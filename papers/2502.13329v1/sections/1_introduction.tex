\section{Introduction}
\label{sec:introduction}
Language Models trained to predict the next token of a sentence on large-scale data~\citep{sutskever2014sequence, devlin-etal-2019-bert} are unrivalled in general Natural Language Understanding (NLU). During inference, these models generate remarkably high-quality outputs by sequentially predicting the most appropriate output tokens to generate. Frontier LM systems of today build on this by scaling inference i.e. increasing the number of tokens or sequences generated~\citep{wei2022chain, yao2023tree, OpenAIO1, guo2025deepseek}, to provide even better responses. 

In this paper we ask whether LMs have learned representations that contain information, not just about the next token the model will predict, but the entire sequence as a whole. We conclude with an affirmative answer, showing that \textbf{the internal states of the LM can often be used to preemptively identify how an LM will behave}. This information becomes accessible before the LM has generated a single token, potentially obviating the need for further inference on those instances. 

Concretely (Figure~\ref{fig:method}), we train linear classifiers (probes)~\citep{alain2017understanding} that use the internal representation of the input tokens to predict the eventual behavior of the LM (e.g. whether an LM will comply with a request or abstain). We calibrate the probes using methods from conformal prediction~\citep{shafer2008tutorial}, allowing them to estimate behavior only when confident. During inference, probes predict only when there is a provable guarantee on the estimation error, ensuring precise early warning signals for various model behaviors. 

%During inference, if the probes are sufficiently confident, we halt all computation and return the estimated behavior instead.

We apply this method to an LM that tackles text classification by generating several explanation tokens before a class prediction (CoT). The probes estimate the final CoT prediction and exit early if confident in their estimate. On 27 datasets, spanning the tasks of MCQA, Sentiment Analysis, Topic Classification, Toxicity Detection and Fact Verification, our method reduces inference costs by 65\% on average, while suffering accuracy losses of no more than 1.4\%. Furthermore, on 14/27 of the datasets, our method reduces inference costs by 63\% on average with \textbf{no cost to accuracy}. The probes generalize to unseen datasets, reducing the inference cost of CoT on OpenBookQA~\citep{mihaylov2018can} by 68\% with minimal loss to accuracy (0.4\%), despite only training on other MCQA datasets. 

Beyond accelerating text classification, probes can pre-emptive identify several behaviors of interest. We apply our method to a QA system that verbalizes some reasoning about the input question and then either answers or abstains if the question is deemed to be unanswerable. The probes, which are trained to estimate whether the LM will abstain, make confident predictions on over 15\% of the test instances and achieve an estimation consistency of over 90\% when doing so.

Probes can provide precise early warning signals for degenerate behavior (whether the model will fail to follow a specified output format) and even `self-reflective" properties which cannot be inferred from the text output alone (whether the model will output a high perplexity answer to a question).
While the number of instances where the probes give confident predictions varies significantly across datasets and tasks, the probe maintains high consistency on confident predictions.  

We show that often fewer than a thousand training instances are sufficient for the probes to attain high estimation consistency, while ablations on the probing layer present a more complicated, task-specific story. Encouragingly, increasing the scale of the LM improves the performance of the probe, suggesting the method may scale favorably with ever-increasing model sizes~\citep{brown2020language, hoffmann2022an,  chowdhery2023palm, dubey2024llama}. 

The probes are limited in the behaviors they can preemptively detect. While probes can identify various model behaviors, they fail on those that cannot be identified without knowledge external to the model (e.g. identifying whether the model is likely to output an incorrect answer to an MCQ instance). Additionally, probes are less confident on instances which trigger longer outputs. 

We hope our findings enable practitioners to build efficient early warning systems on Language Models and enable further research into the information encoded in their hidden states~\citep{petroni2019language, azaria-mitchell-2023-internal, nylund-etal-2024-time}. 

\input{figures/methodfigure}