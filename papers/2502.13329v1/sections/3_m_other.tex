\section{Creating Early Warning Systems for Behaviors of Interest}
\label{sec:warning}

\input{figures/otherstack}

The phenomenon of probes containing information on the eventual output sequence of the LM is not limited to predicting text classification answers but applies to a wide range of behaviors that can be represented with discrete labels. In this section, we showcase several other use cases of this finding. We describe the key experiment designs below, with exact prompts in the Appendix~\ref{sec:appendix_experiment}.

\noindent\textbf{LM Abstention:} We collect two datasets (SelfAware~\citep{yin-etal-2023-large}, KnownUnknown~\citep{amayuelas2023knowledge}) which have both answerable and unanswerable questions. The LM is guided, via few-shot examples, to first reason out loud with respect to the question content and then either answer it or refuse to answer if the question seems to be unanswerable. Separately, we collect malicious and benign prompts from WildJailbreak~\citep{wildteaming2024}, and prompt the LM to refuse to comply with a request if it is malicious. In both cases, the conformal probes try to estimate whether the LM will give an answer or refuse. 

Results show that probes confidently estimate (Figure~\ref{fig:unanswerable})  the abstention decision on over 15\% of KnownUnknown, 44\% of SelfAware and 83\% of WildJailbreak, with over 90\% across datasets. 

\noindent\textbf{Format Following Errors:} We collect 3 QA datasets (NaturalQA~\citep{kwiatkowski-etal-2019-natural}, MSMarco~\citep{DBLP:journals/corr/NguyenRSGTMD16}, TriviaQA~\citep{joshi-etal-2017-triviaqa}) and prompt the LM to an answer in a specific format. One format requires the answer to be organized in exactly 3 bullet points, and the other requires a JSON output which contains pre-specified fields. The probes estimate whether the output will adhere to the format.

Predicting format following errors in a JSON format proves more challenging than the bullet point format(~\ref{fig:bullet}). However, even on the JSON task, the probes maintain a minimum consistency of 84\% and have confidence on over 8\% of instances on 2/3 datasets while deferring completely on the other. 

\noindent\textbf{Low Confidence Output:} On the same 3 QA datasets, we try to identify when the LM is less confident in its response. Following recent work~\citep{kadavath2022language, jiang-etal-2021-know}, we consider one setting where we explicitly prompt the model to verbalize its confidence, and another where we use the per-token perplexity of the output as a proxy for the confidence. In the case of the perplexity measure, we consider the bottom $25\%$ of scores to be `high confidence', and the top $25\%$ to be `low confidence' (discarding the rest). In both cases, the probes attempt to identify whether the model will be confident in its output. 

This is the most challenging task, probes typically make predictions on around 4\% of test instances. However, consistency remains high, making the early warning system feasible in practice.   

The conformal probe-based early warning systems vary in their confidence across tasks and datasets. However, on a wide range of LM behaviors, they predict on sizable portions of the test set with consistently high consistency. 
