\section{Experiment Details}
\label{sec:appendix_experiment}

\subsection{Datasets:}
We used the following datasets in our experiments, all usage is in accordance with their respective licenses. For each dataset, we select a maximum of 50,000 training instances to train (and validate) our probes, using the full test set to measure all metrics. 


\subsubsection{Multiple Choice Question Answering:}
To test this, we collect 8 MCQ datasets---MMLU~\citep{hendrycksmeasuring}, CosmoQA~\citep{huang-etal-2019-cosmos}, PiQA~\citep{Bisk2020}, ARC~\citep{allenai:arc}, MedMCQA~\citep{pmlr-v174-pal22a}, CommonsenseQA~\citep{talmor-etal-2019-commonsenseqa}, OpenbookQA~\citep{mihaylov2018can} and QASC~\citep{allenai:qasc} and use CoT to generate outputs with explanations before the answer (for more prompts see Appendix~\ref{sec:appendix_experiment}). 

\noindent\textbf{ARC:} The AI2 Reasoning Challenge (ARC)~\citep{allenai:arc} is a knowledge and reasoning challenge that contains 7,787 natural, grade-school science questions (authored for human tests).

\noindent\textbf{CommonsenseQA:} The CommonsenseQA~\citep{talmor-etal-2019-commonsenseqa} dataset contains 12,247 questions with complex semantics that often require prior knowledge. 

\noindent\textbf{MedMCQA:} A large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address realworld medical entrance exam questions, MedMCQA~\citep{pmlr-v174-pal22a} has 194k AIIMS and NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects.

\noindent\textbf{MMLU:} The Massive Multitask Language Understanding Benchmark~\citep{hendrycksmeasuring} is a test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. 

\noindent\textbf{OpenBookQA:} Modeled after an open book examination, OpenBOOkQA~\citep{mihaylov2018can}, OpenBookQA consists of 6000 questions that probe an understanding of elementary level science facts and their application to novel situations.

\noindent\textbf{QASC:} Question Answering via Sentence Composition~\citep{allenai:qasc} tests a models ability to compose knowledge from multiple pieces of texts. The dataset consists of 9,980 multiple-choice questions from elementary and middle school level science, with a focus on fact composition;


\noindent\textbf{PiQA:} Physical Interaction: Question Answering~\citep{Bisk2020} is a dataset that tests whether AI systems can learn to reliably answer physical common-sense questions without experiencing the physical world. The dataset consists of 16,000 training QA pairs with 2,000 and 3,000 examples held out for validation and training. 

\noindent\textbf{CosmoQA:} Commonsense Machine Comprehension Question Answering~\citep{huang-etal-2019-cosmos} is a dataset of 35,600 problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. The dataset focuses on reading between the lines with questions that require reasoning beyond the exact text spans in the context.

\subsubsection{Sentiment Analysis:}

\noindent\textbf{MTEB:} The following datasets were taken from the Massive Text Embedding Benchmark~\citep{muennighoff2022mteb}. \textbf{AmazonReviews:} a dataset with 1.7 million Amazon product reviews and a sentiment score ranging from 0-5, \textbf{TwitterSentiment:} a dataset with 30,000 tweets and a sentiment class from positive, neutral or negative 

\noindent\textbf{Yelp Polarity:} The Yelp review rating challenge~\citep{asghar2016yelp} consists of nearly 600,000 yelp reviews with sentiment classes from positive or negative.

\noindent\textbf{TwitterFinance:} The Twitterfinance dataset~\citep{ATwitter} is an annotated corpus of 11,000 finance-related tweets. This dataset is used to classify whether the tweet are bullish, bearish, or neutral.

\noindent\textbf{NewsMTC:} A dataset for sentiment analysis (TSC) on news articles reporting on policy issues, NewsMTC~\citep{Hamborg2021b} consists of more than 11,000 labeled sentences.

\noindent\textbf{IMDB Reviews:} A classic sentiment analysis dataset, IMDB Reviews~\citep{maas-EtAl:2011:ACL-HLT2011} consists of 50,000 highly polar movie reviews with binary labels.

\noindent\textbf{Financial Phrasebank:} A dataset that measures the polar sentiment ~\citep{Malo2014GoodDO} of sentences from financial news. The dataset consists of 4840 sentences from English language financial news categorised by sentiment. The dataset is divided by agreement rate of 5-8 annotators.

\noindent\textbf{AuditorSentiment:} Based on Financial Phrasebank, this dataset~\citep{AuditorSentiment} is additionally annotated by auditors to reflect bearish, bullish and neutral labels for accounting related sentences. 

\noindent\textbf{Emotion:} The DAIR-AI Emotion dataset~\citep{saravia-etal-2018-carer} is a dataset of 20,000 Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. 

\noindent\textbf{SST5:} A standard sentiment analysis dataset, SST5~\citep{socher2013recursive} consists of fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences. 

\subsubsection{Fact verification:} 

\noindent\textbf{ClimateFEVER:} A dataset that consists of 1,535 real-world claims regarding climate-change collected on the internet. Each claim In ClimateFEVER~\citep{diggelmann2020climatefever}is accompanied by five manually annotated evidence sentences retrieved from the English Wikipedia that support, refute or do not give enough information to validate the claim totalling in 7,675 claim-evidence pairs.

\noindent\textbf{HealthVER:} A dataset for  evidence-based fact-checking of health-related claims, HealthVER~\citep{sarrouti2021evidence} consists of 14,330 evidence-claim pairs. 

~\noindent\textbf{FEVER:} Fact Extraction and VERification~\citep{thorne2018fever} consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from.

\subsubsection{Topic Identification:}

~\noindent\textbf{AGNews:}  A collection of more than 1 million news articles. AGNews articles have been gathered from more than 2000 news sources and annotated for Topic~\citep{Gulli}.

~\noindent\textbf{BBCNews:} The BBCNews dataset~\citep{greene06icml} is a dataset consisting of 2,225 articles published on the BBC News website corresponding during 2004-2005. Each article is labeled under one of 5 categories: business, entertainment, politics, sport or tech.

~\noindent\textbf{NYTimes:} The New York Times Annotated Corpus~\citep{sandhaus2008new} contains over 1.8 million articles written and published by the New York Times between January 1, 1987 and June 19, 2007 with article metadata provided by the New York Times Newsroom, the New York Times Indexing Service and the online production staff at nytimes.com.

\subsubsection{Toxicity Detection:}

We use two datasets provided by the same organization, JigsawToxicity~\citep{} and a JigsawUnintendedBiasToxicity~\citep{}. Both datasets have scores for toxicity from a chatroom setting. 

\subsubsection{Others:}

\noindent\textbf{Unanswerable Questions:} We use two datasets which contain unanswerable questions, SelfAware~\citep{yin-etal-2023-large} and KnownUnkown~\citep{amayuelas2023knowledge}. Selfaware is a dataset consisting of unanswerable questions from five diverse categories and their answerable counterparts. KnownUnknown is a dataset with questions on known quantities and unknown quantities. We collect jailbreaking prompts (and benign prompts) from WildJailbreak~\citep{wildteaming2024}, a dataset with 262,000 vanilla (direct harmful requests) and adversarial (complex adversarial jailbreaks) prompt-response pairs. The dataset also has benign prompts that should be complied with. 

\noindent\textbf{Format Following, Confidence Elicitation:} Both of these tasks use the same set of datasets - NaturalQA~\citep{kwiatkowski-etal-2019-natural}, TriviaQA~\citep{joshi-etal-2017-triviaqa} and MSMarco~\citep{DBLP:journals/corr/NguyenRSGTMD16}. These datasets were selected as they are large question-answering datasets which often require generative output that can vary in length. 

\subsection{Task Specific Set Up:}

\subsubsection{Text Classification Tasks:}
In all text-classification tasks, the CoT model is first asked to output a reasoning chain and then finally provide a classification answer. The probes are always trained to preemptively identify what the prediction will be using only the input token embeddings. 

\noindent\textbf{MCQ:} For the MCQ tasks, we select two options by randomly sampling an incorrect answer from the provided options along with the correct option. This is because the MCQ datasets vary in terms of number of MCQ options, and for the out-of-distribution experiment we require all the datasets to have the same number of answer classes. The CoT model is asked to identify the correct answer option. 

\textbf{Sentiment:} For the MCQ task, many datasets are in a binarized format, while others include continuous scores, or ternary labels (including neutral sentiment. We make all the datasets have a similar label structure,  by keeping only positive and negative labels. Concretely, the label is 1 if and only if the sentiment is positive (or bullish for finance datasets), and 0 otherwise. The CoT model is asked to identify whether or not the text has a positive sentiment. 

\textbf{Topic:} We keep only a subset of the topics to ensure every topic is fairly represented in the data. After dropping topics, AGNews has World, Sports, Businsess or Science, BBCNews has Business, Tech, Sports or Politics, while NYTimes has Health, Fashion, Real Estate or Television. The CoT model is asked to identify the topic of the article.

\textbf{Fact Verification:} We map all claims to with supported or not supported (includes refuted and neutral). For ClimateFEVER and HealthVer, we provide evidence along with the claim, while for FEVER we provide only the claim. The CoT model is asked to identify whether or not the claim is supported. 

\textbf{Toxicity Detection:} We randomly sample the datasets to ensure a balance of toxic and not toxic comments. The CoT model is asked to detect whether or not the text is toxic

\subsubsection{Other Tasks:}

In these tasks, the probes are trained to predict a variety of other behaviors using the input token embeddings. 

\textbf{LM Abstention:} For the unanswerable question datasets of SelfAware and KnownUnknown, the model is first instructed  to give a reasoning chain about the question, and answer only if the question is answerable (abstaining otherwise). With WildJailbreak, the input need not be a question, but any request. Hence, we instruct the model to comply with the request if it is not malicious, and abstain otherwise. In all cases, the probe is trained to detect whether or not the LM will abstain. 

\textbf{Confidence Estimation:} There are two settings for this task:
\begin{itemize}
    \item Internal Confidence: The LM is prompted to output an answer to the input question, and we record the token-normalized perplexity as a proxy of confidence. We keep only the bottom and top 25\% of instances as per normalized perplexity and train the probes to differentiate between the two. 
    \item Verbal Confidence: The LM is prompted to output an answer, along with a confidence score (either confident or unsure). The probe is trained to identify what the confidence score will be. 
\end{itemize}


\textbf{Format Following:} Using an input prompt, we specify two different output formats a LM must obey. In both cases, the probe learns whether the LM will fail to follow format specifications or comply with them. 
\begin{itemize}
    \item Bullets: The output should be presented in 3 numbered bullet points, no fewer and no more
    \item JSON: The output should be presented in a JSON string with the following structure: \{[`short\_answer']: <str>, `entities': List<str>, `references': List<str>\}
\end{itemize}

% justify why MCQ is 2 ops but say look at topic classification

\subsection{Prompts:}

We have written separate prompts for each dataset to ensure the LM follows the instructions and outputs the text in a way that guarantees we can parse it and infer the behavior we seek to pre-emptively identify with the probes. For example, a prompt for the MMLU dataset for MCQA is :

\begin{verbatim}
 Question: What is true for a type-Ia supernova?
 Option A: This type occurs in young galaxies
 Option B: This type occurs in binary systems
 Give an explanation and then the answer:
 Explanation: Type Ia supernova is a type of supernova that occurs when 
 two stars orbit one another in which one of the stars is a white dwarf
 Answer: B
 Question: <NEW QUESTION>
 Give an explanation and then the answer:
 Explanation: 
\end{verbatim}

For the JSON task, one such prompt is:

\begin{verbatim}
Answer the following questions by giving a short_answer, entities list 
 and references list. Give the output in JSON format
 Question: What is the capital of France?
 Answer: { "short_answer": "Paris",
 "entities": ["France"], "references": ["https://en.wikipedia.org/wiki/Paris"]}
 Question: <NEW QUESTION>
 Answer: 
\end{verbatim}

Each task and dataset has a different set of prompts, we have provided a link to the code \url{https://github.com/DhananjayAshok/LMBehaviorEstimation}. All prompts can be seen in the file data.py
% refer to repository for all

\subsection{Hardware:}

All of our experiments were run on a compute cluster with 8 NVIDIA A40 GPUs (approx 46068 MiB of memory) on CUDA version 12.6. The CPU on the cluster is an AMD EPYC 7502 32-Core Processor. Most experiments could be conducted with less than 16GB of RAM.

\subsection{Replicability:}

To ensure that our code is easy to replicate and our method is easy to extend, we have provided open access to our code at \url{https://github.com/DhananjayAshok/LMBehaviorEstimation}