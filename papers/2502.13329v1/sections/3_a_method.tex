\section{Can Internal States Predict Eventual Behavior?}
\input{figures/mcqaboxstack}
In this section, we ask whether the internal states of Language Models contain information on the properties of the output tokens the model will eventually generate. We explain our methodology with the running example of a Chain-of-Thought prompted model answering a multiple choice question. 

The CoT model is expected to reason about the input question before giving its final answer, a paradigm that has been shown to enable superior performance on a wide range of reasoning tasks~\citep{wei2022chain, kojima2022large, wangself}. 
Suppose we are given an input prompt of a question with two answer options:
\begin{verbatim}
   Question: Osteoclasts
   Option A: deposit bone and differentiate from monocytes.
   Option B: resorb bone and differentiate from monocytes.
   Explanation: 
\end{verbatim}

Llama3.1-8B~\citep{dubey2024llama} responds to this with
\begin{verbatim}
Osteoclasts are cells that break down bone tissue and are responsible for 
bone reabsorption. They differentiate from monocytes and macrophages. 
Answer: B
\end{verbatim}

We can reasonably infer that the model will select option B by the time it generates the token `reabsorption', but the tokens before that are generic enough that they do not eliminate either option. 
However, is it possible that the model contains signals that can identify the answer by the time it generates its initial output token `Osteoclasts', and simply takes multiple tokens to express it in an interpretable way?  

To test this, we collect 8 MCQ datasets---MMLU~\citep{hendrycksmeasuring}, CosmoQA~\citep{huang-etal-2019-cosmos}, PiQA~\citep{Bisk2020}, ARC~\citep{allenai:arc}, MedMCQA~\citep{pmlr-v174-pal22a}, CommonsenseQA~\citep{talmor-etal-2019-commonsenseqa}, OpenbookQA~\citep{mihaylov2018can} and QASC~\citep{allenai:qasc} and use CoT to generate outputs with explanations before the answer (for more prompts see Appendix~\ref{sec:appendix_experiment}).  

We collect the output of the 18th (18/32) Transformer layer of Llama3.1-8B when processing the final token of the input (i.e. the final `:'), and train linear classifiers to predict what the \textbf{eventual} answer prediction will be. We ablate layers in Section~\ref{sec:analysis} and reproduce results on other LMs in Appendix~\ref{sec:appendix_robustness}. 

The probes achieve (Figure~\ref{fig:mcqaboxstack}) higher than random consistency (accuracy on prediction of model behavior) on all datasets, suggesting that input embeddings contain information on the eventual answer the LM will output. However, the performance varies significantly across datasets, suggesting that a naive attempt to exit early with these estimations could result in catastrophic failure.  

\subsection{Creating robust behavior estimators with conformal prediction}
Unless one believes that the explanation generated by the LM has no causal influence on the eventual answer, there will always be instances where there is not enough information in the internal state of the input tokens to conclude what the future model behavior will be. The ideal behavior estimation system handles such cases, making consistent predictions when it is confident and deferring otherwise. 

Hoping to impute such a capability to our probes, we use the probes we learned above in a conformal prediction framework. Specifically, we use a held-out validation set $\mathbb{D}_\text{valid}$ to calibrate the probe after training. We compute the probes' prediction probabilities $\mathbf{\hat{y}}\in[0, 1]^{|c|}$ for each class with true label $y\in\{1,\ldots c\}$ on the validation set, and find the lowest threshold quantile $q$ that satisfies: 

\begin{equation}
    \frac{\sum_{ \mathbf{y_i}\in \mathbb{D}_\text{val}}\mathbb{I}[(\max{(\mathbf{\hat{y_i}})\geq q) \land (\text{argmax}(\mathbf{\hat{y}_i})=\mathbf{y_i})}]}{\sum_{ \mathbf{y_i}\in\mathbb{D}_\text{valid}}\mathbb{I}[\max{(\mathbf{\hat{y_i}})\geq q}]} \geq \alpha
\end{equation}

During inference, when the probe predicts a behavior with probability vector $\mathbf{\hat{y}}_{\text{test}}$, we return the prediction if and only if $\max{(\mathbf{\hat{y}}_\text{test})} \geq q$, otherwise we defer to the LM. In all experiments we set $\alpha=0.9$. If no satisfying $q$ can be found we defer on all test instances. We ablate $\alpha$ in Section~\ref{sec:analysis}.  

Using the conformal probes (Figure~\ref{fig:mcqaboxstack}) significantly increases the performance across all datasets. For datasets where the total test consistency was already high (ARC with $88\%$ total test consistency), the conformal probes predict on large portions of the test set with consistency close to 90\%. On datasets where total test consistency was poor (MedMCQA, MMLU, PiQA), the probes predict on fewer instances. When they do predict, they do so with higher consistencies than on the full test set. 

\section{Accelerating Inference-Scaling with Conformal Probes}
\input{figures/tradeoff}

We extend the method described above to 18 other datasets spanning the tasks of Sentiment Analysis, Topic Analysis, Toxicity Detection and Fact Verification. Details on dataset setup are in Appendix~\ref{sec:appendix_experiment}. 

For all of these, we use Llama3.1-8B under CoT prompting (outputting an explanation before the final answer). We train a linear probe that uses the internal representation of the final input token at the 18th layer to predict the class that the CoT model will eventually output, and then perform conformal calibration. During inference, if the conformal probe is confident we use the probe estimation as the final answer. If not, we allow the model to continue its CoT generation and provide the final answer. We compare this to the vanilla CoT on two metrics---Accuracy Loss (Accuracy of CoT - Accuracy of Method) and Inference Cost Reduction ($\frac{\#\text{CoT Forward Passes - \#Method Forward Passes}}{\text{\# CoT Forward Passes}}$). 

The results (Figure~\ref{fig:tradeoff}) show that the method is highly effective at reducing the inference cost with minimal cost to accuracy. The minimum inference cost reduction is $4.7\%$, with an average reduction of $65\%$ across all datasets. Despite this significant speedup, the average accuracy loss is near zero ($\mathbf{-0.46\%}$), with the worst loss at $1.34\%$. Surprisingly, accuracy \textbf{increases} on several datasets. 

Finally, we investigate the out-of-distribution generalization capabilities of the conformal probes. For each test dataset of the MCQ and Sentiment tasks, we train and calibrate using data from every \textbf{other} dataset. The results (Figure~\ref{fig:tradeoff}) show that the probes do exhibit OOD generalization, suggesting the method may be applicable even when there are slight shifts between training and test distributions.