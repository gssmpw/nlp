\section{Analysis and Ablations}
\label{sec:analysis}
\input{figures/detail_mc_pc_corr}

\noindent\textbf{Why does accuracy improve when using probes?} Seeking to explain the surprising increase in accuracy when accelerating CoT models (Figure~\ref{fig:tradeoff}), we plot (Figure~\ref{fig:detail_mc_pc_corr}) the correlation between the CoT models accuracy and the probes estimation consistency. It is generally a positive correlation, suggesting that on the instances where the probe estimates incorrectly, the CoT model is also more likely to output an incorrect answer. This minimizes the harm of an inconsistent estimation as sometimes making an inaccurate estimate is actively beneficial for overall accuracy.
 



\noindent\textbf{How early in the computation does the model show signs of the final behavior?} We have shown that the final input token often contains sufficient information to predict output behavior, but at which token does this information start becoming clear and accessible? To investigate this, we collect the activations of the mid-layer from \textbf{every} input token (after the FewShot example tokens) and train a linear classifier to map any of these internal activations to the desired output behavior. 

Probing (Figure~\ref{fig:detail_mc_pc_corr}) using the embeddings of tokens in the first quarter of the question tokens leads to near-random performance. While consistency increases steadily as we use later tokens, a considerable jump is noticed when shifting to the final tokens. Surprisingly, confidence declines marginally as we use later tokens for probing. This suggests that using the earliest tokens for probing could result in inconsistent and poorly calibrated models, making it a poor design decision for this method. 

\noindent\textbf{How does output length affect estimation accuracy?} We measure the correlation between the estimation consistency of the probe and the token count of both the input (-0.41\%) and output (-6.1\%). While input length seems to have little effect, a higher output length is correlated with worse accuracy. This suggests probes struggle more with inputs that evoke longer outputs. 

%We test this by writing a fresh set of prompts with particularly verbose explanation examples for the TwitterFinance Sentiment Analysis dataset. The CoT output for the version with the longer prompts are longer on average (), and the probes perform \textbf{MENTION WHAT HAPPENS HERE.}

\input{figures/n_datapoints}

\noindent\textbf{How much data does a probe need?} We ablate the amount of data used to train the probe. We find (Figure~\ref{fig:n_datapoints}) that often, fewer than 1000 datapoints is sufficient for the probe to achieve high estimation accuracy, suggesting that the method is particularly data efficient. 

\input{figures/layer}

\noindent\textbf{Which layer should we use to probe?} We explore (Figure~\ref{fig:layer}) the specific layer being probed affected the performance of the probe. The most general finding is that the early layers offer poor estimation consistency. However, whether mid or late layers perform better is task and dataset-specific, and cannot be described generally. For more details see Appendix~\ref{sec:appendix_analysis}. 

\input{figures/confidence}

\noindent\textbf{How does varying $\alpha$ change performance?} Increasing the confidence required from the probes has predictable effects (Figure~\ref{fig:confidence}). Increasing the confidence threshold has no effect until a point, after which conformal consistency increases while the coverage tends to zero.

\input{figures/scale}

\noindent\textbf{How does model scale affect performance?} We varied the size of the LM used (Llama3 3B, 8B and 70B) and measure probe consistency across several layers. Encouragingly (Figure~\ref{fig:scale}), the performance of the larger models are consistently better, suggesting that the information encoded in the internal activations is easier for the probe to `read' when the model is more powerful. This bodes well for the methods' ability to scale with models of increasing size and capability. 

%\noindent\textbf{Are probes predicting the label or the LM behavior?} A natural question is whether the probes are truly predicting the behavior of the model, or simply predicting what the `correct label', which ends up coinciding with the model's decision often. Perhaps the datasets have been leaked to the LMs and this is what enables the probes to work? 

%On the question of data leakage, we would note first that the test set of multiple datasets used in this study (MMLU, \textbf{FIND MORE}) are not in the LM training corpus according to the Llama3 documentation. More conceptually, even if the data itself has been accidentally leaked, the LM has not trained on it's \textbf{own} completions, which is the target variable of the probe. For example, the labels and outputs related to the format following and confidence estimation tasks are not available in the form of text anywhere on the internet. These tasks also alleviate the concern that the probe is estimating the text classification label directly as opposed to the model output, since in these cases there is no classification `label'.  Another piece of evidence is that the probe performs with higher than random estimation accuracy even on instances where the model \textbf{incorrectly} classifies the output. (\textbf{NUMBERS}). 

\textbf{What are the limits of the probes?} Using the MCQ task, we tried to estimate whether the LM would output an \textbf{incorrect} answer. The probe accuracies were consistently poor and near random. This shows that, intuitively, the probes have limitations in the behaviors they can estimate. We hypothesize that since probes are simple linear classifiers, they can only detect patterns and use `knowledge' that is well encoded in the internal activations being probed. This suggests that probes struggle with output properties that cannot be identified by the LM itself (without external knowledge). %For completeness, the failed tasks and results are shown in  Appendix~\ref{sec:appendix_analysis}. 