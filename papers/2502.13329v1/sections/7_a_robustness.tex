\section{Confirming Robustness of Results}
\label{sec:appendix_robustness}

\subsection{Confirming Robustness to Choice of Model}
We conduct experiments on two other Language Models, to ensure that our results hold on Language Model families outside of the Llama3 series. We use Mistral-7B-Instruct-v0.3 from MistralAI~\citep{jiang2023mistral} and DeepSeek-R1-Distill-Qwen-14B from DeepSeek~\citep{guo2025deepseek}. The results show that the phenomenon shown in the main text can be detected robustly across various model families.

\input{figures/othermodels}

%Also study transferability of the probe

%\subsection{Confirming Robustness to Dataset Exposure}

\subsection{Data Leakage}
Several datasets used in this study have been released before the creation of Llama3, and as such might have been pre-exposed to the model during training. We detail our reasons as to why data leakage is not a concern for the conclusions that we draw from this study:

\begin{itemize}
    \item There are datasets in our study that have not been leaked. For example, the MMLU test set is used as an official test benchmark by LLama3\citep{dubey2024llama} and WildJailbreak was released to the public only after the initial release of Llama3 models~\citep{wildteaming2024}. The results are strong on such datasets as well, showing that data leakage is not behind the performance of the probes.
    \item Several tasks require the probes to identify behavior that fundamentally does not exist in text form on the internet. The bullets, JSON and Confidence tasks require the probes to determine how the model will behave, regardless of whether the model has seen the input text or not. For example, whether or not the model was pretrained on MSMarco questions is immaterial for predicting whether or not it will output answers in a specific JSON format.  
\end{itemize}


%\subsection{Unrelated Next Token(s)} % Other Twtitter experiment


