\section{Background}
\label{sec:background}
\textbf{Hidden State Probing:} Lightweight probes have long been used to interpret the internal activations of neural networks~\citep{alain2017understanding} and language models~\citep{petroni2019language, azaria-mitchell-2023-internal}. Given a set of input prompts, we compute the generated outputs and store the internal activations computed during the forward passes of the LM (e.g. the outputs of a specific Transformer~\citep{vaswani2017attention} layer). The instances are then assigned a classification label meant to capture some property of the input and output. The probes are then trained to predict the label using only the internal activations as input. For example, \citet{azaria-mitchell-2023-internal} collect the activations of the middle layer of an LM when processing the output tokens, and manually label the generations as true or false. They then train a small neural network which can identify whether the model output is truthful given the internal activations of the generated tokens. Our work focuses on behaviors that can be trivially inferred from the output text alone (e.g. infer the answer from the output, infer if the output follows a specified format) and hence \textbf{does not require manual labelling}. Standard practice in probing uses the internal states of all tokens, even generated ones. We take an alternate approach, training probes on the internal states of the input tokens alone, allowing them to predict properties of the output tokens \textbf{before} they have been generated.


\textbf{Conformal Prediction:} Given a validation set, conformal prediction can be used to precisely calibrate the model's confidence~\citep{gammerman2013learning, kumar2023conformal}. During inference, the conformal system makes a prediction if and only if the probability of the prediction being accurate is above a user-specified probability~\citep{shafer2008tutorial}. 

In the classification setting, we are given a validation set $\{({\bf x_1}, \ldots  {\bf x_n}), (y_1, \ldots y_n)\}, \mathbf{x}_i\in\mathbb{R}^d, y_i\in\{1, 2, \ldots, c\}$ (where $d$ is the dimension of the input vector and $c$ is the number of classes), a classifier which maps the input to scores for each class $f:\mathbb{R}^d\to\mathbb{R}^{c}$ and a user-defined confidence level $\alpha\in(0, 1)$. To form a well-calibrated prediction set, we define a score function $S: \mathbb{R}^d\times \{1, 2, \ldots c\}\to\mathbb{R}$ that measures how poor a prediction is e.g. $S(\mathbf{x}, y) = 1-(\frac{\exp{f(\mathbf{x})}}{\sum_i{\exp{f(\mathbf{x})}_i}})_y$. Using scores on the validation set $(s_1, \ldots s_n)$ we calibrate by calculating threshold $\hat{q}_\alpha$, the $1-\alpha$ quantile of the scores:
\begin{equation}
    \hat{q}_\alpha = \text{Quantile}(\{s_1, s_2, \ldots s_n\}, \frac{\lceil(n+1)(1-\alpha)\rceil}{n})
\end{equation}

At inference time, the prediction set for a test instance $\mathbf{x}$ is simply $\{\hat{c}| S(\mathbf{x}, \hat{c}) \leq q_\alpha\}$. 

For the score function provided above, no two classes can achieve a value greater than 0.5 simultaneously, implying that with $\alpha>0.5$ the prediction set at inference time is either empty (deferral/abstention) or consists of a single class label.

If the validation set is \textit{exchangeable} (a slightly weaker assumption than the typical I.I.D assumption~\citep{bernardo1996concept}) with the test set, then a prediction at inference time is provably guaranteed~\citep{shafer2008tutorial} to satisfy
\begin{equation}
    \mathbb{P}[y_\text{test}=\hat{c}_{\text{test}}] \geq 1-\alpha
\end{equation}