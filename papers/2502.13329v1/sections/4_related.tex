\section{Related Works}
\label{sec:related}
This work takes inspiration from recent methods that probe the hidden states of LMs to observe interpretable patterns~\citep{alain2017understanding,Kim2017InterpretabilityBF, petroni2019language, hewitt-liang-2019-designing, akyrek2023what} identify false statements~\citep{azaria-mitchell-2023-internal, li2023inferencetime, liu-etal-2024-universal, yuksekgonul2024attention} and hallucinations~\citep{chuang-etal-2024-lookback, su-etal-2024-unsupervised, jiang2024large}. Our work also has connections to literature on early exiting during the forward pass of NN models, with works often using signals from the hidden states to prematurely exit with a prediction on the next token~\citep{xin-etal-2020-deebert, zhou2020bert, xin-etal-2021-berxit, schuster-etal-2021-consistent, jazbec2023towards}. Notably, ~\citet{schuster-etal-2021-consistent} also uses a conformal prediction framework to give a provable error bound on the next token approximation. 

We are the first to show that the internal states can predict a range of behaviors, uninferable from the next token alone, \textbf{before any} of the output tokens are generated. Additionally, we are the first to show that conformal prediction can be used to create early warning systems for a wide range of behaviors like question abstention to format following errors. Our work advances research on understanding the nature of the information contained in the hidden states of LMs~\citep{petroni2019language, anonymous2023does, nylund-etal-2024-time, liu-etal-2024-probing, tighidet-etal-2024-probing}. Specifically, we show that the information contained in the hidden states is relevant not just to the next token, but to behaviors that manifest several tokens later during the LMs generation. 