\section{Conclusion}
\label{sec:conclusion}
We show that a Language Model's internal representation of input tokens alone contains vital information on the behavior of the LM over the entire output sequence. These signals can be used to create probes that serve as precise early warning or exit systems for LMs on a wide range of behaviors. On 27 text classification datasets across 5 different tasks, the method can accelerate Chain-of-Thought prompting by 65\% with little accuracy loss. The method can be used to create high-precision early warning systems that identify if an LM will refuse to answer a question, fall victim to jailbreaking, fail to follow output format specifications or give low-confidence responses. We show that the probes generalize to out-of-distribution test sets, and scale favorably to larger LMs. Finally, we explore the limitations of the method, showing that the behavior of longer output sequences is harder to estimate and that tasks that require knowledge external to the model are particularly challenging. With the rising popularity of inference-time scaling methods, we hope our work can help ameliorate the growing computational cost of running LMs and provide more insight into the nature of the information contained in the hidden states of LMs.  