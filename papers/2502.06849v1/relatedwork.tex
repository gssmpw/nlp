\section{Related Work}
% (https://paperswithcode.com/paper/structural-pruning-for-diffusion-models-1 hat paar mathematische Formalia zu structural pruning (ansonsten gehts nicht um ensemble pruning))

% "nur" ensembles: 
% https://papers.nips.cc/paper/2020/file/b86e8d03fe992d1b0e19656875ee557c-Paper.pdf (Ensemble aus diversen modellen), aber kein downsizen danach
% (https://bmvc2022.mpi-inf.mpg.de/0798.pdf
% feature difference loss - ensemble member sollen andere feature lernen --> loss einführen dafür)

% https://paperswithcode.com/paper/deep-model-fusion-a-survey - survey zu model fusion
% es gibt 4 klassen an Ansätzen für fusion:
% 1) model connectivity - mehrere Lösungen durch Pfad verbinden und samplen. (gibt linear, non-linear, subspace - varianten , zB. fast geometric ensembling (https://arxiv.org/pdf/1802.10026.pdf))
% 2) alignment (before averaging) (zB. https://arxiv.org/abs/2009.02439 , https://arxiv.org/pdf/1910.05653v6.pdf). Damit kann man wohl auch "rebasin" machen (parameter in low-loss region transportieren, aber das soll speicheraufwändig sein) 
%" Alignment requires additional computational overhead to adjust the model’s parameters, which can lead to a more complex and
%time-consuming training process, especially in large depth model"
% 3) weight average , einfach averagen. Es gibt dazu noch "model soup" (mit anderen hyperparametern trainierte modelle averagen), "model arithmetic", "stochastic weight averaging". WA ist einfach und effizient --> mainstream. 
% 4) ensemble learning (alle modelle nutzen und nicht fusionieren)



% TODO etwas zu allgemeinen Alternativen wie im paper zu optimal transport? 
% TODO etwas zu federated learning?
% etwa eine ganze Seite
The field of model fusion has been categorized by Li et al.~\cite{li2023deep} into four categories, of which ``weight averaging'', and ``alignment'' is of relevance to our work.

\paragraph{Weight Averaging.}
Weight averaging of trained models~\cite{DBLP:journals/corr/abs-2002-06440} is the process of combining models in a defined space. The outputs of the individual models are weighted and summed. Weight averaging has the problem of a loss barrier which increases the loss for fused models through averaging. For large pre-trained models, it has been shown that they lie in a single low error basin~\cite{DBLP:journals/corr/abs-2008-11687, wortsman2022model}, rendering vanilla averaging feasible. Another such scenario is the case of merging similar models~\cite{li2023deep}. In such fusion settings~\cite{inproceedings, Leontev_2019}, no loss-barrier seems to be present making average-based methods viable.
%In the case of merging similar models, the loss-barrier may also not exist, making averaging and similar methods viable~\cite{inproceedings, Leontev_2019}.
%A greedy and guaranteed successful strategy was proposed by Wortsman et al. \cite{wortsman2022model} called "model soups" in which they iterate through ensemble members (ordered by descending validation accuracy) and only take new models into account if the validation accuracy of the fused model is higher than the current one.
Unlike the approaches detailed here, we do not interpolate between different models, but instead transplant all of them into the fused model to create a new initialization that is able to reach near-ensemble performance with the memory and speed of a single member.

\paragraph{Alignment.} %\paragraph{Mode Connectivity.}
% linear mode connectivity, non-linear, mc in subspace
Entezari et al. conjecture that models can be permuted in such a way that there is no loss barrier on the linear interpolation between them~\cite{entezari2022role}. This solves the problem of the loss barrier in weight averaging, but the search space to probe all possible permutations in a reasonable time is too large. To solve this, Ainsworth et al. propose activation matching, weight matching and using a straight-through estimator to find good permutations~\cite{ainsworth2023git}. Singh and Jaggi use Optimal Transport (OT) to align the neurons (layerwise, in a greedy way) minimizing the total transportation cost between them~\cite{singh2023model}. In both works, a certain loss barrier remains when the actual averaging is done, especially when layers with small widths are fused. In contrast to these methods, we do not try to ``align and average'' neurons, but to ``select'' the most relevant ones from the ensemble members, hence avoiding the averaging-induced loss.

%https://arxiv.org/pdf/2002.06440.pdf , matching und dann averagen... also schon wieder alignment
% https://arxiv.org/pdf/2111.09832.pdf Merging Models with Fisher-Weighted Averaging

%\cite{singh2023model} % https://arxiv.org/pdf/1910.05653v6.pdf Model Fusion via Optimal Transport
% wichtig! paper hat identische Ziele wie meins! 

\paragraph{Distillation.}
Knowledge distillation~\cite{hinton2015distilling} is a compression method that can be used as a fusion method~\cite{DBLP:journals/corr/abs-2011-07449, DBLP:journals/corr/abs-1812-02425, DBLP:journals/corr/abs-2012-09816}. It is orthogonal to averaging or transplanting and can be used alongside other fusion methods such as OT~\cite{singh2023model} or ours. In knowledge distillation, knowledge from the teacher is distilled into the student by having access to the logits of the teacher model as soft targets, in addition to the original labels as hard targets. Sun et al. use knowledge distillation on a diverse ensemble as a synchronization method for SGD~\cite{sun2017ensemblecompression}. After each fusion, workers train independently until the next synchronization step. To ensure diversity of the ensemble members, they add a similarity term to the loss function. We use distillation to further study the behaviour of our method in experiments.

\paragraph{Pruning.}
Pruning is the method of removing elements from a model, the smallest unit being a single weight, which can be set to zero to decrease model complexity while retaining predictive performance. The field of pruning is large~\cite{mishra2020survey}. Structured pruning considers compositions of elements to be pruned concurrently, for linear layers these might be neurons, their equivalent for CNNs might be filters~\cite{10.1145/3005348}. Units to be removed are selected by some measure, e.g. magnitude pruning uses the $L_p$-norm as a simple indicator whether a units contribution is of relevance~\cite{248452}. As structured pruning can get complex when using different architectures, we resort to \emph{torch-pruning}~\cite{fang2023depgraph} which is a pruning framework that can handle many structural couplings in a general way.
% Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning - ensemble distillation.. 

% (https://arxiv.org/pdf/2011.07449v1.pdf ensemble (teacher) und jedes einzelne modell (student) lernen.
% Die haben aber nur mit baseline verglichen.
% Man kann sich Studenten aussuchen als Endmodell (die werden immer kleiner (etwa halbe größe) und etwas ungenauer mit jedem weiteren studenten) oder das ganze ensemble nehmen (je nach speicherbedarf))

% (https://arxiv.org/pdf/1812.02425v2.pdf compression von ensembles in ein modell mit knowledge distillation "MEAL". kommt dem nah, was ich mache nur mit knowledge distillation und extra tricks statt concatenation)