\section{Experiments}
\input{tables/ablation_experiment_results}
We evaluate \tool on three code efficiency evaluation benchmarks: EvalPerf \cite{liu2024evaluatinglanguagemodelsefficient}, Mercury \cite{du2024mercury} and ENAMEL \cite{qiu2024efficientllmgeneratedcoderigorous}. \textbf{\uline{EvalPerf}} focuses on performance-challenging tasks and uses Differential Performance Evaluation to assess efficiency across different LLMs and solutions. Its efficiency metric, DPS\_norm, is calculated by determining the cumulative ratio of the reference solution that is immediately slower than the new solution, normalized by the total number of solutions. This ensures a fair comparison of code efficiency based on reference solutions with varying performance levels. \textbf{\uline{Mercury}} introduces the Beyond metric to evaluate both functional correctness and code efficiency. The Beyond metric is calculated by normalizing the runtime percentiles of LLM solution samples over the runtime distribution for each task, ensuring consistent runtime comparisons across different environments and hardware configurations. \textbf{\uline{ENAMEL}} evaluates code efficiency using the eff@1 metric. This efficiency score is determined by measuring the worst execution time of the code sample across test cases of varying difficulty levels. The score is then adjusted using a weighted average across these levels to account for hardware fluctuations. The eff@1 metric ranges from 0 to 1, with higher values indicating greater code efficiency. A value exceeding 1 signifies that the generated code is more efficient than the expert-level solution.

% It divides tasks into different difficulty levels for a more detailed evaluation
%ENAMEL also includes expert-designed reference solutions to assess algorithmic efficiency, revealing that LLMs often fail to generate code that meets expert-level efficiency standards.



\subsection{Compared Methods.}
We evaluate the direct instruction of generating correct and efficient code as the \textbf{Instruct} baseline. We compare \tool with two recent proposed methods \textbf{ECCO} \cite{waghjale-etal-2024-ecco} and \textbf{Effi-Learner} \cite{EffiLearner} for code efficiency. 
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=0pt,parsep=0pt]
\item \textbf{ECCO:} A self-refine with NL feedback approach that prompts the LLM to generate code, then asks if improvements in correctness or efficiency can be made, and finally refines the solution based on optimization suggestions.
\item \textbf{Effi-Learner:} First generates code using instruction prompts same as \textbf{Instruct} baseline, then executes the code with test cases to collect performance profiles, including runtime and memory usage. These profiles are fed back into the LLM along with the code, prompting the LLM to refine the code for efficiency based on the profile. It is worth noting that Effi-Learner relies on test case oracles, and in this study, we use the visible test cases from the task. In contrast, \tool does not rely on any test case oracles; all test cases are synthetically generated by \tool itself.
\end{itemize}

\subsection{Experiment Setup.}
To comprehensively evaluate \tool, we selected five different LLM backbones: two proprietary models, GPT-4o \cite{openai2024gpt4ocard} and GPT-4o-mini, and three open-source models, including DeepSeek-V3  \cite{deepseekai2024deepseekv3technicalreport}, Qwen2.5-72B-Instruct \cite{qwen2.5}, and Qwen2.5-Coder-32B-Instruct \cite{hui2024qwen2}. During the \tool process, we set the number of algorithm plans to 5 and the number of synthetic test cases to 20, followed by one iteration to refine the code for correctness. All prompts used in \tool are detailed in Appendix \ref{sec:ap-method}. To ensure consistency and a fair comparison, all experiments were conducted with the temperature set to 0, and each experiment was repeated three times to compute an average, thereby eliminating any potential disruptions.

\input{tables/special_experiment_results}

\subsection{Main Results.}
We compare \tool with the other methods on the EvalPerf, Mercury, and ENAMEL benchmarks, and present the results in Table \ref{tab:complete_results}. First, we observe that direct instruction prompts yield good performance, indicating that LLMs have a reasonable understanding of correct and efficient code.  Then, through ECCO, we observe a slight improvement in efficiency on EvalPerf and Mercury. However, this improvement often comes at the cost of correctness. Particularly in more complex benchmarks like ENAMEL, the approach results in a decline in both efficiency and correctness. This suggests that relying solely on code understanding to generate optimization suggestions is insufficient. When optimization strategies are based purely on code-level analysis, they often fail to align with the broader logical requirements of the task. The mismatch between the code domain and the logic strategy domain makes such methods less effective.

Moreover, Effi-Learner shows some gains in efficiency and correctness on specific benchmarks, such as when using GPT-4o on the Mercury benchmark. However, its performance varies significantly across different LLM backbones and benchmarks, often falling short of the direct Instruct baseline. More importantly, Effi-Learner faces a recurring issue: both efficiency and correctness suffer simultaneously. This stems from its feedback mechanism, which focuses solely on performance metrics like execution time, neglecting the codeâ€™s functionality and correctness. Additionally, the lack of a comprehensive algorithmic strategy leads to an over-prioritization of execution time, often sacrificing code accuracy and resulting in a decline in both efficiency and correctness.

In comparison, \tool achieves a simultaneous improvement in both correctness and efficiency through efficiency optimizations at the logical and code implementation levels, followed by refinement of correctness using "checked" test case feedback. The results demonstrate that \tool delivers robust and consistent performance improvements across various benchmarks and different LLM backbones, with the gains highlighted in color. For example, using DeepSeek-V3 as the backbone on EvalPerf, \tool improved the efficiency metric DPS\_norm by 6.63\%, while on ENAMEL, eff@1 increased by 9.27\%, and Pass@1 improved by 2.82\%.

\subsection{Ablation Study.}
\tool incorporates several unique design choices, such as separating efficiency optimization into the logic domain and code implementation level. To better understand the impact of each component, we conduct the following ablation study:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=0pt,parsep=0pt]
    \item \textbf{Variant-1:} (Without Algorithmic Exploration in the Logic Domain): In this variant, no algorithmic exploration is performed for efficiency optimization in the logic domain. Instead, the LLM directly generates the same count efficient code solution, followed by implementation-level optimization (based on the formalized task and the generated code solution). All other steps remain the same as in \tool.
    \item \textbf{Variant-2:} (Without Implementation Optimization in the Code Domain): This variant omits the implementation optimization step in the code domain, while all other processes are identical to those in \tool.
    \item \textbf{Variant-3:} (Without Code Correctness Refinement): In this variant, after generating the efficiency-optimized code solutions, the LLM independently selects the most efficient and correct code as the final output.
\end{itemize}
We conduct the ablation study using Qwen2.5-Coder-32B-Instruct and DeepSeek-V3 as LLM backbones, with the results presented in Table \ref{tab:ablation_results}. The results show that removing any component significantly impacts both efficiency and correctness. Specifically, omitting Algorithmic Exploration in the Logic Domain (\textbf{Variant-1}) or Implementation Optimization in the Code Domain (\textbf{Variant-2}) leads to a marked decline in efficiency metrics across all three benchmarks. Additionally, removing Code Correctness Refinement (\textbf{Variant-3}) results in a significant drop in Pass@1. These results align with our expectations, as both Algorithmic Exploration and Implementation Optimization are designed for efficiency, while Code Correctness Refinement ensures the final code retains functional correctness after efficiency-driven steps.
