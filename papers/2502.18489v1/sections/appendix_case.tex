\section{Case Study.}
\label{appendix-casestudy}
\subsection{The Execution Details of Each Process of \tool.}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/prcess_case.pdf}
    \caption{The figure illustrates the specific output of each subtask process of \tool in solving algorithm problems.}
    \label{fig:process_case}
\end{figure*}

As shown in Figure~\ref{fig:process_case}, \tool firstly analyzes the algorithm problem, "returns the n-th number that is both a Fibonacci number and a prime number", providing a detailed explanation of key aspects, including the entry point, expected behavior, and edge cases. Based on this analysis and the problem description, \tool explores potential algorithms and generates five efficient solutions, such as using the Fibonacci sequence generation method and Binet’s formula. Next, \tool examines the implementation details of these algorithms and identifies the optimal practical approaches. For example, it uses Python’s built-in pow() function for efficient exponentiation and applies the Miller-Rabin primality test (based on the Monte Carlo method) to enhance the efficiency of prime number detection for large numbers.

Then, \tool combines the explored algorithms and practical operations to generate five distinct code implementations. To validate the correctness of these codes, \tool generates 20 test cases based on the algorithm description and outputs them in the format "assert prime\_fib(3) == 5". Each code is then executed with these 20 test cases, recording the number of passed test cases ($Pass_{\text{t}} \leq 20$) and the number of successful executions for each test case ($Pass_{\text{c}} \leq 5$). Subsequently, \tool checks the test cases that are not passed by the code implementations, ensuring that correct test cases are not excluded due to code errors and preventing incorrect test cases from being misused in subsequent iterations.

After filtering, \tool obtains a new batch of test cases and executes them again to gather new results. For the failed test cases, an iterative feedback mechanism is applied to optimize the code. Then, the code, enhanced with the iterative feedback, is executed once more, and the final passing results are recorded. All codes are then ranked in descending order based on their correctness, and the most accurate code is selected. 
% If multiple codes have the same pass rate, \tool uses a large model to evaluate efficiency, ultimately selecting the code that is both efficient and correct.

This process ensures the identification of the most optimal solution while maintaining both high efficiency and accuracy in code implementation.
\subsection{Comparison of Methods.}
In Figures~\ref{fig:case2} and Figures~\ref{fig:case1}, we compare the code efficiency optimization processes of the three tools.
\begin{figure*}[htbp]
    \centering

    \includegraphics[width=1.0\textwidth]{figures/case2.pdf}
    \caption{The diagram demonstrates how \tool, Effi-Learner, and ECCO generate code. \tool, through deep exploration of the algorithm domain, generates a set of efficient and high-quality algorithm candidates. However, the time complexity of these algorithms is similar, and there is no significant difference from the original code generated by Effi-Learner and ECCO. Subsequently, \tool identifies key optimization suggestions in its practical recommendations, such as replacing list with bytearray, among others. As a result, although the final code has a similar time complexity to the other two tools, it significantly outperforms them in the final ENAMEL efficiency evaluation metrics.}
    \label{fig:case2}
\end{figure*}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/case1.pdf}
    \caption{The figure illustrates the code generation process of \tool, Effi-Learner, and ECCO. \tool, through deep exploration of the algorithm domain, generates a set of efficient and high-quality algorithm candidates. By incorporating practical optimization suggestions, it ultimately produces an algorithm with a time complexity of only $\mathcal{O}(n \cdot k\log n)$, achieving a high score of 1.28 on the ENAMEL test set. In contrast, Effi-Learner and ECCO, constrained by the $\mathcal{O}(n \cdot \sqrt{F_n})$ time complexity of their code algorithms, can only perform local optimizations on certain implementations, resulting in minimal improvements, with the final efficiency index reaching only 0.34.}
    \label{fig:case1}
\end{figure*}


