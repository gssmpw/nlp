\section{Introduction.}
Large Language Models (LLMs), particularly those specialized in code, are revolutionizing the field of software engineering at an unprecedented pace. A significant area of advancement lies in automated code generation \cite{evalplus}, where LLMs such as GPT-4o \cite{openai2024gpt4ocard}, Gemini \cite{team2023gemini}, the DeepSeek Series \cite{deepseekv2}, and the Qwen Series \cite{qwen2, qwen2.5} demonstrating remarkable capabilities. These models have attracted considerable attention from both academia and industry, consistently breaking new ground on code completion and generation benchmarks, including HumanEval \cite{chen2021evaluating}, MBPP \cite{austin2021program}, and LiveCodeBench \cite{jain2024livecodebench}.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/comparison.pdf}
    \caption{Comparison of \tool with existing methods. Existing methods generate code first, then optimize it using strategy and execution profiles. In contrast, \tool starts with the task, focusing on efficiency through algorithm exploration and implementation, followed by correctness refinement.}
    \label{fig:comparison}
\end{figure}

While these LLMs achieve impressive accuracy in automatic code generation, practical software engineering applications require more than just correct code—they also demand efficiency \cite{shi2024efficientgreenlargelanguage,niu2024evaluatingefficiencysourcecode}. In real-world scenarios, even correct code often requires manual optimization by engineers before it can be used, which undermines the goal of "out-of-the-box" automated code generation. Therefore, generating code that is both correct and efficient is essential, yet automating this process has not been widely explored.

\begin{figure*}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/workflow.pdf}
    \caption{The workflow of \tool. Given a programming task, \tool formalizes it into a code-oriented description, generates optimal algorithms and pseudocode in logic domain, and then produces implementation suggestions in code domain. \tool synthesizes test cases and uses a verification-based adaptive framework to evaluate candidate solutions. The final code is selected based on the highest pass rate of the "checked" test cases.}
    \label{fig:workflow}
\end{figure*}

Recent preliminary works \cite{EffiLearner,waghjale-etal-2024-ecco} have explored feedback-based approaches to optimize generated code and enhance its efficiency. As illustrated in Figure \ref{fig:comparison}, these methods typically involve profiling code execution time and incorporating reflective feedback into the optimization process. However, the "generate-then-optimize" paradigm is constrained by the algorithmic design and overall structure of the initial code, leading to only incremental improvements. We provide detailed examples in Figure \ref{fig:case1} and \ref{fig:case2} in Appendix B. In contrast, when human developers write high-quality code, whether in practical software development or algorithmic teaching scenarios, they typically start by designing multiple potential solutions at a logical level. For example, when tackling a sorting problem, a developer might consider Quicksort for its average-case efficiency of $\mathcal{O}(N*\log N)$, while also factoring in its worst-case time complexity of $\mathcal{O}(N^2)$. By carefully analyzing the problem’s constraints and evaluating various algorithms along with their complexities, they then proceed to implement the solution, applying various coding techniques to optimize it. Finally, they debug and refine the code to achieve a high-quality implementation.

Inspired by this thought, we propose \tool, as shown in Figure \ref{fig:workflow}, a novel paradigm that enables LLMs to generate both efficient and correct code. Specifically, for a given programming task described in natural language, \tool first "formalizes" it into a code-oriented problem description. In other words, it converts the broad natural language statement into a clear, concrete, and well-defined coding problem, ensuring that the LLM can accurately interpret it. Next, \tool prompts the LLM for logic-level reasoning and exploration, considering various algorithmic approaches, providing corresponding complexity analyses, and generating relevant pseudocode. Based on these different algorithm designs and their associated pseudocode, \tool suggests code implementation strategies, followed by code generation and optimization at the implementation level, as high-quality code also requires careful consideration during the practical implementation stage. To ensure the functional correctness of the generated code while targeting efficiency, \tool introduces a bidirectional verification-based adaptive testing framework to check synthetic test cases. Finally, the code solutions are executed on the "checked" test cases and iterated upon for correctness. The solution with highest pass rate across the "checked" test cases is selected as the final generated code.

The \tool has two distinctive uniqueness:
\noindent\textbf{\underline{Uniqueness 1:}} Separation of Efficiency Optimization into Logic and Code Domains. \tool divides efficiency optimization into two distinct domains: the "logic domain" and the "code domain". In the logic domain, efficiency optimization focuses on exploring the optimal algorithmic approaches, while in the code domain, optimization deals with the practical implementation details. This separation effectively breaks down the challenge of optimizing code efficiency into manageable steps, making the overall efficiency optimization process more systematic and targeted.

\noindent\textbf{\underline{Uniqueness 2:}} The Order of Correctness and Efficiency. The order in which correctness and efficiency are optimized plays a critical role. By prioritizing efficiency first, a wider range of algorithmic solutions can be explored, leading to the discovery of multiple efficient approaches. Correctness is then incrementally ensured across these solutions. This approach avoids prematurely constraining efficiency optimization by focusing too early on correctness. Prioritizing efficiency first allows for greater room for improvement and significantly enhances the potential for efficiency gains.

We validate \tool on three recently proposed code efficiency benchmarks: EvalPerf \cite{liu2024evaluatinglanguagemodelsefficient}, ENAMEL \cite{qiu2024efficientllmgeneratedcoderigorous}, and Mercury \cite{du2024mercury}. Experimental results show that \tool consistently enhances both code correctness and efficiency across various LLM backbones, achieving state-of-the-art performance in efficiency metrics. Specifically, using the DeepSeek-V3 backbone, \tool improves eff@1 by 9.27\% on ENAMEL and boosts DPS\_norm by 6.63\% on Mercury.

Overall, we summarize our contributions as follows, with corresponding code available at link\footnote{\url{https://anonymous.4open.science/r/LLM4EFFI-04B2}}:
\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
\item We propose \tool, the first framework that simultaneously optimizes both code efficiency and correctness.
\item We introduce two key features: \textit{Separation of Efficiency Optimization into Logic and Code Domains} and \textit{Order of Correctness and Efficiency}. We hope these unique features will contribute to the advancement of the code efficiency community.
\item Extensive experiments and analysis on three benchmarks across different LLM backbones demonstrate the effectiveness and robustness of \tool in efficient code generation.
\end{itemize}