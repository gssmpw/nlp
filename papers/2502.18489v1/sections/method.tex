\section{Methodology}
\paragraph{Problem Formulation.}
In the code efficiency task, each sample is represented as a pair $(Q, T_h)$, where $Q$ denotes the task description, and $T_h$ corresponds to the hidden test cases. Our goal is to generate the corresponding code solution $S$ that passes the hidden test cases and achieves the highest efficiency (i.e., the shortest execution time). Notably, to better simulate real-world scenarios, we assume there are no public test cases. $T_h$ is only used during the evaluation stage and is not visible during efficiency and correctness optimization stages.

\subsection{Overview.}
We present the framework of \tool in Figure \ref{fig:workflow}. For a given programming task described in natural language, \tool first "formalizes" it into a code-oriented problem description (\textbf{Section \ref{method-1}}). Next, \tool queries the LLM for logic-domain reasoning and exploration, generating multiple optimal algorithmic solutions along with their corresponding pseudocode (\textbf{Section \ref{method-2}}). Based on these algorithm designs and their associated pseudocode, \tool analyzes and generates code implementation suggestions, followed by the generation and optimization of the corresponding code at the implementation level (\textbf{Section \ref{method-3}}). To further refine the solutions for correctness, \tool synthesizes a large number of test cases and utilizes a bidirectional verification-based adaptive testing framework to "check" these synthetic test cases. The "checked" test cases are then used to evaluate the candidate code solutions (\textbf{Section \ref{method-4}}). The solution with highest pass rate across the "checked" test cases is selected as the final generated code.

\subsection{Task Formalization.}
\label{method-1}
In the initial task formalization stage, \tool ensures that the task description is clear and unambiguous, which is crucial for the success of subsequent stages. As highlighted by \citet{han-etal-2024-archcode}, errors in LLM-generated code often arise from an insufficient or unclear understanding of the task. Therefore, \tool prompts the LLM to comprehend the task from four key dimensions: \uline{entry point function name}, \uline{input/output conditions and parameter types}, \uline{edge cases}, and \uline{expected behavior}. Based on these dimensions, the LLM is further encouraged to engage in self-reflection to confirm whether it has fully grasped all aspects of the task, thus laying a solid foundation for the subsequent stages. Formally, $Q \to Q_{formal} \overset{\text{check}} {\longleftrightarrow} Q$.

\subsection{Algorithmic Exploration in Logic Domain.}
\label{method-2}
For the formalized task defined in the first stage, \tool prompts the LLM to engage in algorithmic reasoning at the logical level, rather than immediately generating code. This approach mirrors that of human programmers, who first perform abstract and high-level reasoning before implementation. The LLM is prompted to explore multiple potential optimal algorithms, analyze their corresponding complexities, and represent the entire logical process with pseudocode. Formally, $Q_{formal} \to \{Algo, Cplx, Pseudo\}$, where $Algo$ refers to the algorithm plan, $Cplx$ refers to the complexity analysis, and $Pseudo$ refers to the corresponding pseudocode.

\subsection{Implementation Optimization in Code Domain.}
\label{method-3}
Excellent code not only requires careful algorithm design but also necessitates optimization at the implementation level. Even when the same algorithm is used, different implementation approaches can lead to significant variations in code efficiency \cite{shypula2024learning, Coignion_2024}. When implementing code based on the algorithm plan and corresponding pseudocode, \tool prompts the LLM to provide practical suggestions derived from $Algo$ and $Pseudo$, such as replacing a manual binary exponentiation implementation with Pythonâ€™s built-in pow function, among other optimizations. We provide three detailed examples in the appendix to illustrate this process. Subsequently, \tool generates the corresponding code based on the $Algo$, $Pseudo$, and implementation suggestions, while also checking for further optimization opportunities. Formally, 
$\{Algo, Pseudo\}\to\{Suggs\}$, and $\{Algo, Pseudo, Suggs\}\to\{Code \ Candidates\}$.

\input{tables/fully_results}

\subsection{Code Correctness.}
\label{method-4}
To ensure the functional correctness of generated code while targeting efficiency, \tool introduces a bidirectional verification-based adaptive testing framework. The process works as follows: First, \tool automatically synthesizes a large number of test cases based on the formalized task description $Q_{formal}$. These test cases are designed to cover a wide range of edge cases, thoroughly testing the robustness and reliability of the generated code. However, since the synthesized test cases may not be entirely correct, \tool performs bidirectional verification to validate them.

\noindent \textbf{Forward Verification:} If all candidate code implementations pass a specific test case, the test case is marked as trusted. Otherwise, \textbf{Reverse Review:} For test cases that cause failures in any candidate code, \tool performs a $Q_{formal}$-based review. It checks whether the test case aligns with the intent of the formal task description and conducts a semantic consistency check, which is similar to Test-Driven Development \cite{Erdogmus2010TestDrivenD} in software engineering. If the test case passes the reverse review, it is retained; otherwise, it is discarded. Finally, the retained test cases are marked as "checked". These "checked" test cases are then used to evaluate the generated code candidates, with any failures triggering further refinements. The code candidate that passes the most "checked" test cases is ultimately selected as the final solution. Formally,
$Q_{formal} \to \{Synth. \ test \ cases\} $$\overset{\text{check}} \to \{Checked\ test\ cases\} \overset{\text{select}} \to \{final \ solution\}$.

