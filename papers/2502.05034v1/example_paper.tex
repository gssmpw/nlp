%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
\usepackage{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{float}

% 添加的包:
\usepackage{bbding} %首先在导言区调用bbding包

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% 添加的包
\usepackage{booktabs} % 用于更美观的表格样式
\usepackage{multirow} % 导入 multirow 包
\usepackage{footnote}

% \usepackage[table]{xcolor} % 引入 xcolor 包以支持表格颜色

\definecolor{yellow1}{RGB}{255, 245, 200}
\definecolor{yellow1}{RGB}{255, 253, 208}
\definecolor{yellow}{RGB}{252, 236, 172}
\definecolor{yellow}{RGB}{242, 219, 150}
\definecolor{yellow}{RGB}{247, 234, 187}
\definecolor{yellow}{RGB}{252, 228, 129}
\definecolor{yellow}{RGB}{250, 217, 122}
\definecolor{yellow}{RGB}{255, 225, 195}
\definecolor{yellow}{RGB}{250, 229, 184}
% \definecolor{yellow}{RGB}{250, 210, 150}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{MindAligner: Explicit Brain Functional Alignment for Cross-Subject Visual Decoding from Limited fMRI Data}

\begin{document}




\twocolumn[
\icmltitle{MindAligner: Explicit Brain Functional Alignment for Cross-Subject Visual Decoding from Limited fMRI Data}


\icmlsetsymbol{equal}{*}

\vspace{-0.3cm}
\begin{center}
\textbf{ 
Yuqin Dai$^{\star1,2,4}$ \quad
Zhouheng Yao$^{\star1}$ \quad
Chunfeng Song$^{1}$ \quad
Qihao Zheng$^{1}$ \quad
Weijian Mai$^{1}$ \\ 
Kunyu Peng$^{5}$  \quad
Shuai Lu$^{4}$ \quad
Wanli Ouyang$^{1,3}$ \quad
Jian Yang$^{\dagger2}$ \quad
Jiamin Wu$^{\dagger1,3}$}\\ \vspace{0.1cm}
{
$^1$Shanghai Artificial Intelligence Laboratory \quad
$^2$Nanjing University of Science and Technology \\ \vspace{0.05cm}
$^3$The Chinese University of Hong Kong \quad  
$^4$Tsinghua University \quad
$^5$Karlsruher Institut f\"ur Technologie \\ \vspace{0.05cm}
$^\star$Equal contribution  \quad  
$^\dagger$ Correspondence to: 
\{jiaminwu@cuhk.edu.hk\}, \{csjyang@njust.edu.cn\}
}\\ \vspace{0.2cm}

%Code: \url{github.com/lm-similarity} \quad Data: \url{huggingface-spaces.com} 
% \raisebox{-1pt}{\faDatabase} \href{https://huggingface.co/datasets/bethgelab/lm-similarity}{\texttt{Sample-wise Predictions}} \quad 
% \raisebox{-1pt}{\faGlobe} \href{https://model-similarity.github.io/}{\texttt{model-similarity.github.io}} \quad 
% \raisebox{-1pt}{\faGithub} \href{https://github.com/model-similarity/lm-similarity}{\texttt{lm-similarity}}
\end{center}

% % origin
% % 机构间不能有逗号
% \begin{icmlauthorlist}
% \icmlauthor{Yuqin Dai}{equal,ailab,njust,thu}
% \icmlauthor{Zhouheng Yao}{equal,ailab}
% \icmlauthor{Chunfeng Song}{ailab}
% \icmlauthor{Qihao Zheng}{ailab}
% \icmlauthor{Weijian Mai}{ailab}
% \icmlauthor{Kunyu Peng}{kit}
% \icmlauthor{Shuai Lu}{thu}
% %\icmlauthor{}{sch}
% \icmlauthor{Wanli Ouyang}{ailab,cuhk}
% \icmlauthor{Jian Yang \textsuperscript{\dagger}}{njust}
% \icmlauthor{Jiamin Wu \textsuperscript{\dagger}}{ailab,cuhk}
% \end{icmlauthorlist}
% % \headerfigure{}
% % 不能有空行
% \icmlaffiliation{ailab}{Shanghai Artificial Intelligence Laboratory}
% \icmlaffiliation{njust}{Nanjing University of Science and Technology}
% \icmlaffiliation{cuhk}{The Chinese University of Hong Kong}
% \icmlaffiliation{thu}{Tsinghua University}
% \icmlaffiliation{kit}{Karlsruher Institut für Technologie}
% \icmlcorrespondingauthor{Jian Yang}{csjyang@njust.edu.cn}
% \icmlcorrespondingauthor{Jiamin Wu}{jiaminwu@cuhk.edu.hk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.2in
]



% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=0.97\linewidth, height=60mm]{images/MainFig.pdf}}
\vspace{-2mm}
\caption{
% 我们采用了一种同现有隐式对齐不同的显式对齐策略, 能在跨受试者间进行 region-level 的关联, 获得更好的解码效果.
% Different ways of performing brain decoding. 
% Prior works...
% Differently, ...
Different approaches to functional alignment in brain decoding: Prior works~\cite{mindeyev2, eccv25} adopt implicit alignment approach that aligns all subjects into a single latent space, which may lead to suboptimal alignment. Differently, MindAligner employs an \textbf{explicit alignment strategy}, mapping novel subject signals to seen ones by establishing fine-grained functional correspondences. MindAligner not only enables high-quality visual reconstruction from fMRI signals but also facilitates brain functional analysis across subjects. 
}
\label{fig:mainfig}
\end{center}
\vskip -0.3in
\end{figure*}


% ver0
% Different approaches for functional alignment in brain decoding.
% Prior works~\cite{mindeyev2, eccv25} perform implicit alignment approaches, enforcing all subjects to be simultaneously aligned to a single latent space prone to suboptimal alignment and compromised
% representation.
% MindAligner adopts an \textbf{explicit alignment strategy} that maps the novel subject's signal to a seen subject.
% The aligned fMRI signal can not only seamlessly be integrated into the pre-trained decoding model of seen subjects but also reveals brain region-level cross-subject variability, offering valuable insights for neuroscience analysis.

\begin{abstract}
{
Brain decoding aims to reconstruct visual perception of human subject from fMRI signals, which is crucial for understanding brain's perception mechanisms.  
Existing methods are confined to the single-subject paradigm due to substantial brain variability, which leads to weak generalization across individuals and incurs high training costs, exacerbated by limited availability of fMRI data.
To address these challenges, we propose MindAligner, an explicit functional alignment framework for cross-subject brain decoding from limited fMRI data.
The proposed MindAligner enjoys several merits. First,
we learn a Brain Transfer Matrix (BTM) that projects the brain signals of an arbitrary new subject to one of the known subjects, enabling seamless use of pre-trained decoding models.
Second, to facilitate reliable BTM learning, a Brain Functional Alignment module is proposed to perform soft cross-subject brain alignment under different visual stimuli with a multi-level brain alignment loss, uncovering fine-grained functional correspondences with high interpretability.
%
Experiments indicate that MindAligner not only outperforms existing methods in visual decoding under data-limited conditions, but also provides valuable neuroscience insights in cross-subject functional analysis. The code will be made publicly available.
% 
% 
%
%
%Existing methods rely on functional alignment to address inter-subject variability, yet they face two major challenges: insufficient alignment and a lack of functional interpretability. Moreover, the high cost of acquiring fMRI data limits the availability of data from new subjects, with few studies tackling this issue under limited data conditions.
%To address these issues, we propose MindAligner, an explicit brain functional alignment method for  cross-subject brain decoding and brain functional analysis in the data limited setting. The proposed MindAligner enjoys several merits. First, a cross-subject brain transfer matrix is learned for xxxxxx
% Second, To learn reliable brain transfer matrix, a brain functional alignment module is proposed to achieve flexible, soft cross-subject alignment without the need for paired fMRI data. 
% Extensive experimental results xxxxxxx
% Neuroscience discoveryxxxxx

% ver0
% Brain decoding is crucial for understanding the brain's perception mechanisms. 
% Due to inter-subject brain differences, current methods rely on functional alignment to reduce variability across subjects. 
% % 现有方法存在的问题
% However, current alignment methods suffer from both insufficient brain alignment and a lack of functional interpretability.
% Additionally, the high cost of acquiring fMRI data limits availability for new subjects, with only a few studies addressing this challenge in scenarios with limited data.
% % 我们的方法
% To address these issues, we propose MindAligner, an explicit functional alignment framework for cross-subject visual decoding with limited fMRI data. The MindAligner uses a Brain Transfer Matrix (BTM) that projects the brain signals of a new subject to one of the known subjects to achieve seemly use of the pre-trained decoding model. We design a Brain Functional Alignment module to train the BTM, which mitigates the differences in both brain structures and stimuli, reducing the difficulty of alignment and thereby achieving better results.
% Additionally, the resulting brain transfer matrix reveals cross-subject
% differences in region-level for brain functional analysis.
% % 实验的废话
% Experimental results show that MindAligner outperforms baselines in functional alignment and provides enhanced data analysis value. 
}
\end{abstract}

\section{Introduction}
\label{introduction}
% 先引入 fMRI 信号
%\textcolor{gray}{ cognition, and unraveling its intricate mechanisms holds profound academic importance~\cite{Naselaris2011}. A critical aspect of this pursuit is the study of visual perception, which is crucial for understanding how the brain processes and interprets complex visual stimuli. Visual decoding tasks, in particular, aim to reconstruct visual experiences from brain activity, offering a window into the neural representation of perception. Owing to its noninvasive nature and ability to precisely localize functional brain regions, functional magnetic resonance imaging (fMRI)~\cite{fmri} has emerged as a pivotal tool in this domain. By leveraging fMRI signals, researchers have made significant strides in brain visual decoding, a field that not only drives advancements in brain-computer interface (BCI) technologies~\cite{Qian2020,Horikawa2017}, but also enriches our understanding of cognitive mechanisms~\cite{Wen2018,Kietzmann2019,Horikawa2017}.}

The brain serves as the center of human cognition and unraveling its underlying mechanisms holds profound academic significance~\cite{Naselaris2011}.
To investigate the brain's perceptual mechanisms, functional magnetic resonance imaging (fMRI)~\cite{Naselaris2011} has been widely used due to its noninvasive acquisition and precise localization of the functional regions. 
The advances in fMRI facilitate the research on brain visual decoding,
which aims to recover visual stimuli seen by humans from their brain activity, contributing to the progress of cognitive science research and Brain-Computer Interfaces (BCI)~\cite{Qian2020,Horikawa2017}.
%Brain visual decoding has not only advanced the field of cognitive science, but also contributed to the progress of brain-computer interfaces (BCI)~\cite{Qian2020,Horikawa2017}.
    % fMRI 具有广泛前景
% Among them, functional Magnetic Resonance Imaging (fMRI) decoding~\cite{fmri}, which interprets visual cortex responses to external stimuli and reconstructs images from brain activity has garnered significant attention as a promising approach~\cite{Naselaris2011}.

% 现有方法存在的问题
% 因为无法克服脑固有差异(e.g., 脑大小, 认知差异), 单受试者模型需要 follow follow per-subject-per-model paradigm~\cite{mindshot}.
%Due to brain's inherent variability (e.g., volume)~\cite{mindtuner}, 


Despite the success in fMRI-based visual decoding, the majority  methods~\cite{seeliger2018generative, Minddiffuser, Ozcelik2023} are confined to the less practical single-subject paradigm, where a customized decoding model is trained for each person subject.
Due to substantial brain differences among subjects, the decoding model trained on one subject cannot be effectively transferred to others, 
%leading to exceptionally high training costs as the number of subjects grows and 
limiting its practicality in BCI and clinical applications. In fact, variations in individual cognitive patterns and brain structures, 
%shaped by genetic and environmental factors, 
result in significant fMRI differences~\cite{Naselaris2011}.
%, even under identical visual stimuli. 
Moreover, the high acquisition cost of fMRI limits the data availability for new subjects. Thus, adapting brain decoding models to new subjects with limited data is crucial.
%



%In this paradigm, each individual subject requires a customized decoding model that captures subject-specific brain patterns. Even trained on multiple subjects' brain recordings, the decoding model still struggles in generalizing to novel subjects, restricting its application in wider clinical and BCI scenarios. xxx numbers. The primary reason for this limitation is the substantial variability in brain function among individuals. 
%fMRI signals exhibit considerable differences across subjects.

%%%%%%%%%%%%
%However, as they are customized for individual subjects, these methods are difficult to apply in practice.
% 跨受试者研究引入功能对齐


To address the cross-subject issue, several methods~\cite{mindeyev2, eccv25, mindbridge} adopt brain alignment techniques in an implicit manner. They align fMRI signals from different subjects to a latent space that is assumed to capture common cognition patterns across subjects by learning subject-specific parameters.
%For example, MindEye2~\cite{mindeyev2} leverages subject-specific regression layers to achieve this latent space alignment. 
However, this implicit alignment approach has two limitations.
%
(1)~\textbf{Insufficient brain alignment}: learning a shared space that effectively aligns all subjects remains challenging due to noisy and limited fMRI data. As individuals have vast brain differences even when viewing identical stimuli, enforcing all subjects to be simultaneously aligned to a single latent space is prone to suboptimal alignment and compromised representation.
Even with extensive multi-subject fMRI training, the shared latent space shows limited generalizability to unseen subjects. 
For instance, Unibrain~\cite{unibrain} shows $~50 \%$ performance drop when transferring the decoding model to new subjects. %compared to supervised training.
%
(2) \textbf{Lack of functional interpretability}: 
%to align fMRI signals across subjects. , either align in anatomical brain space~\cite{willsaligner, mindshot} or a common latent space~\cite{mindeyev2, eccv25, mindbridge}.
existing latent alignment methods~\cite{mindeyev2,mindbridge,eccv25} fail to explicitly account for cognitive pattern relationships between subjects. Their alignment is incapable of localizing the brain regions for functional differences and commonalities.
This lack of interpretability not only limits cross-subject knowledge transfer in new subject adaptation, but also hinders analysis of neural functional mechanisms underlying human perception process.
%
%Given these limitations, an intriguing question arises, can we establish an explicit brain alignment framework that enables both effective new subject adaptation and brain functional analysis in the low-sample setting.
Given these limitations, an important question arises: \textbf{\textit{can we create a brain alignment framework enabling effective new-subject adaptation and brain functional analysis under data scarcity?}}

% %To address this issue, current cross-subject approaches leverage the brain alignment, either align in anatomical brain space~\cite{willsaligner, mindshot} or a common latent space~\cite{mindeyev2, eccv25, mindbridge}.
% % 现在的方法怎么做的, 引出暴露的问题
% % 现有模型使用的功能对齐策略省去了跨受试者间脑功能信号的对比过程, 完全依赖于图像重建质量得到的反馈来衡量拉齐质量. 
% However, compared with the standard approach~\cite{difumo, Bazeille2019, Thual2023}, 
% their alignment omits the comparison of brain functional signals across subjects, instead relying solely on image reconstruction quality as a proxy for evaluating alignment performance.
% % 主流方法对齐存在的问题
% This simplified strategy:
% %
% (1) struggles to generalize to new subjects, as it requires aligning new subjects to all prior subjects in a one-to-many manner, requires substantial computational costs, and
% (2) unable to capture differences in cognitive patterns, which may uncover the neural mechanisms underlying visual encoding.
%     % fMRI 采样成本高、费时, 很难采集到大规模的数据。
%     % 提一嘴不太适合用太多的数据进行实验, 衔接下文+ mindeye 1h 的策略
% In addition, 
% current fMRI sampling is both costly and time-consuming, making large-scale data collection challenging~\cite{mindtuner}. 
% 
% Thus, it is essential to propose an alignment paradigm that can be adapted to new subjects with limited data, while enabling the capture of brain differences, as shown in {Figure 1}.


To answer this question, our motivation is to establish an explicit brain functional alignment framework that maps the novel subject's signal to a seen subject's signal.
% 该方法是一种显示对齐，不但能通过关联跨收受试者间具有相似功能的的分区实现跨受试者解码, 害能得到一套具体的对齐策略, 进而具有神经科学价值. 
%%%%%%%%%%%%%%%%%%
%The alignment can simulate the functional information transfer in the original brain space, instead of in the latent space.
% 相比起隐式对齐, which 在同时拉齐多个受试者时需要 compromise 多个语义，造成 insufficient alignment, 显式对齐更关注跨受试者间的 functional atlas 之间的功能对应，which 对应关系更加明确，因而更容易找到最优解. 
% Compared to implicit alignment, which requires compromising multiple subjects' same semantics when aligning multiple subjects simultaneously and leading to insufficient alignment, explicit alignment focuses more on the functional atlases correspondences between subjects. 
%Compared to implicit alignment approach
%that may compromise the same semantics of multiple subjects, 
%Therefore, given a new subject, our explicit alignment is able to establish fine-grained functional correspondences between atlases of a new subject and any previously observed subject.
Given an arbitrary new subject, our explicit alignment can establish fine-grained functional correspondences between the new subject and seen subjects in the original brain space, as shown in Fig.~\ref{fig:mainfig}. 
%This strategy requires almost no compromises 
%and thus is easier to find an optimal solution without compromise, as shown in 
The aligned fMRI signal can not only seamlessly be integrated into the pre-trained decoding model of seen subjects but also reveals brain region-level cross-subject variability.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Moreover, it also reveals associations and connectivity between specific brain regions during the brain transfer process, providing valuable insights into cross-subject differences in neuroscience.
% the aligned fMRI of new subject can be seamlessly connected to the pre-trained decoding model of the seen subjects.
% Leveraging its explicit nature, it can reveal associations between specific brain regions across arbitrary subjects and is capable of illustrating the degree of connectivity between these regions.
% This connectivity information can offer valuable insights into research on cross-subject differences in neuroscience.
%However, the fine-grained brain alignment between subjects is difficult, as it requires paired fMRI signals from subjects under the same task state, i.e., viewing identical visual stimuli.
However, achieving such brain alignment is challenging, as it requires paired fMRI from subjects performing the same task (\textit{i.e.}, viewing identical visual stimuli~\cite{Bazeille2021}), a condition not met by the existing dataset~\cite{nsd}.
% —a condition not met by the existing dataset~\cite{nsd}.
%usually performs reconstructions on data pairs where two subjects view identical visual stimuli.
%However, this condition is not present 
% In addition, compared with the latent space, the original fMRI is with low Signal-to-Noise Ratio (SNR), therefore  
% Instead of using the low Signal-to-Noise Ratio (SNR) fMRI data, we move to a subject-shared latent space, which is more compact and consistent across subjects.
% However, the training of such an explicit model requires reconstruction on paired same-stimulus data, where two subjects view identical visual stimuli. 
% However, if subjects are not viewing identical images and can not provide paird fMRI data, the training of this transfer model is challenging for unable to perform explicit reconstruction.


% 我们的方法的解决思路
% 我们提出的方法 + 优势
Based on the above observations, we propose \textbf{MindAligner}, an explicit functional alignment framework for cross-subject visual decoding with limited fMRI data.
The core of our method is to train a cross-subject \textbf{Brain Transfer Matrix} (BTM) that projects the brain signals of a new subject to one of the known subjects in the voxel-level.
% 概述: 软对齐策略 - Functional Aligner Embeder
%in addition to the visual decoding model training, 
To overcome the lack of strictly paired fMRI signals, we propose a \textbf{Brain Functional Alignment module} (BFA) to perform soft cross-subject alignment between fMRI signals from different but similar visual stimuli, facilitating the mapping of functionally equivalent cortical areas.
%%%%
Specifically, BFA first decomposes the brain transfer matrix into two low-rank linear layers, enhancing parameter efficiency to facilitate effective adaptation with limited data.
%
In the latent space of the BTM,
a cross-stimulus neural mapper is designed to transform the fMRI under different visual stimuli, with stimulus differences as mapping condition. To achieve sufficient and fine-grained alignment, we design a multi-level brain alignment losses that incorporates a signal-level reconstruction loss and a latent alignment loss guided by visual semantic similarities.
In this way, the resulting brain transfer matrix not only facilitates fine-grained alignment without shared stimuli constraint, but also encodes cross-subject brain relations for enhanced functional interpretability and neuroscience analysis.

%
% 如何进行差异性分析
%Experimental results demonstrate our model's superiority.
In summary, our contributions are as follows:
\begin{itemize}
    \item We propose MindAligner, the first explicit brain alignment framework that enables cross-subject visual decoding and brain functional analysis in the data-limited setting.
    \item We propose a brain transfer matrix to establish fine-grained functional correspondences between arbitrary subjects. This matrix is optimized through a brain functional alignment module, which employs a multi-level alignment loss to enable soft cross-subject mapping.
    %without the need for paired fMRI data. A cross-stimulus neural mapper is designed for transferring functional representations under different stimuli, facilitating brain alignment between arbitrary subjects.
    \item Experiments demonstrate that MindAligner outperforms state-of-the-art methods in visual decoding with only 6\% of the whole model's parameters learned.
    \item We conduct cross-subject brain functional visualization and discover that the early visual cortex shows similar activities across subjects, while the higher visual cortex related to memory and spatial navigation exhibits significant inter-subject variability.
% enhanced interpretation in uncovering neural differences between subjects' brains 
    % 提出 mindAligner,
    % \item We propose MindAligner, an explicit alignment framework that allows functional region mapping between subjects, therefore enabling better knowledge transfer and interpretation.
    % % 提出 Neuro Difference Aligner,
    % \item We introduce the brain functional alignment module, which performs a more flexible alignment by regulating the extent of differences between brain regions.
    % \item We propose the cross-stimulus neural mapper, a module that transforms similar brain activities across subjects into equivalent activities, thereby facilitating alignment.
    % Providing valuable insights for brain functional analysis.
    % % 提出 Functional Aligner Embeder, 
    % \item Experimental results demonstrate that our method not only outperforms existing baseline but also provides enhanced data analysis potential.
    % 可解释性 + 实验结果 
\end{itemize}

\section{Related Work}
\subsection{fMRI-Based Brain Decoding}
% 任务定义
% Brain Visual decoding 目的是从脑活动中还原出受试者得到的对应视觉刺激. Naselaris2011
Brain decoding seeks to reconstruct the visual stimuli perceived by subjects based on their brain activity, offering a deeper understanding of the brain's mechanisms for processing external information~\cite{Naselaris2011}.
% Early
Earlier work~\cite{Horikawa2017} reveals a correlation between Deep Neural Networks (DNNs) image representations and neural activity in the visual cortex using sparse linear regression.
% 生成模型入场: GAN 入场 -> Diffusion + CLIP
With the advent of generative models~\cite{gan, diffusion} and extensive fMRI datasets~\cite{nsd}, visual decoding has shifted towards mapping brain signals to the latent spaces of large models, facilitating the reconstruction of diverse visual stimuli~\cite{gu2022decoding, ozcelik2022reconstruction, Shen2019, gao2023mind, maisurvey, gao2024fmri}.
% Diffusion + CLIP: % 现有数据集缺乏配对数据集,大多依赖于拉到潜空间的策略
This approach has proven effective in utilizing latent diffusion models for image reconstruction~\cite{Lin2022, Takagi2023, maiunibrain, mindeyev1, chen2023seeing}, addressing inter-subject differences by either training separate models for individual subjects or employing partially unified models with subject-specific parameters.
% 我们的优势
% However, such models focus solely on individual latent patterns and, limit their potential to uncover inter-subject neural variability.
However, influenced by neural variability, cross-subject brain signals in the latent space are prone to semantic conflicts, which can lead to convergence at suboptimal points.
% MindAligner introduces a region-level explicit alignment method designed to enhance the interpretation of inter-subject neural variability. 
MindAligner addresses this by leveraging an explicit functional alignment framework across brains, this approach more effectively utilizes shared functionalities among subjects, thereby mitigating semantic conflicts.

% 框架图
\begin{figure*}[!t]
\begin{center}
\centerline{\includegraphics[width=0.97\textwidth]{images/Framework.pdf}}
\vspace{-2mm}
\caption{
Overview of MindAligner. 
To achieve explicit brain functional alignment, given a pre-trained brain decoding model, we design a \textbf{Brain Functional Alignment Module (BFA)} that learns a Brain Transfer Matrix (BTM) $\boldsymbol{\mathcal{M}}$ for fMRI mapping between the known and novel subjects. BTM is decomposed into two low-rank matrices $\boldsymbol{\mathcal{A}}$ and $\boldsymbol{\mathcal{B}}$ to create latent space for further alignment. 
The Cross-Stimulus Neural Mapper is proposed to create fMRI pairs under shared stimuli. In addition to the alignment losses $\boldsymbol{\mathcal{L}}_{rec}$ and $\boldsymbol{\mathcal{L}}_{KL}$ between generated and real fMRI, a latent alignment loss $\boldsymbol{\mathcal{L}}_{latent}$ guides functional alignment based on stimulus similarities.
In the inference stage, only the BTM is utilized for functional mapping, enabling cross-subject brain decoding.
}
\label{fig:framework}
\end{center}
\vskip -0.3in
\end{figure*}




\subsection{Cross-Subject Functional Alignment}
% 文件夹清空, 
% 问题: 为什么要研究跨受试功能对齐
As brains differ both in size and processing mechanisms \cite{nsd, finn2017brain}, the resulting variability in fMRI signals has spurred research into brain alignment methods.
% 促使了功能对齐的产生, 传统功能对齐往往依赖于共享刺激，即需要提供不同受试者在相同视觉刺激下的数据对，通过优化重建损失完成对齐.
The ideal condition for functional alignment methods often depends on \textbf{shared stimuli}, 
requiring paired data from multiple subjects exposed to identical visual inputs, with alignment achieved through reconstruction loss optimization \cite{difumo, rastegarnia2023brain}.
A new paradigm~\cite{Bazeille2019, fgwu, Thual2023, througtheireye} has emerged, enabling explicit alignment by transforming one subject's signal to that of another, thereby preserving functionality and facilitating knowledge transfer across subjects.
To enable cross-subject visual decoding on the Natural Scenes Dataset (NSD)~\cite{nsd}, the largest open-source dataset available, which lacks shared stimuli, 
current methods focus on either anatomical alignment~\cite{Bao2024, mindshot, shenneuro} or functional alignment in latent space~\cite{mindeyev1, mindeyev2, mindbridge, mindtuner}.
Benefiting from a focus on region-level functional differences, functional alignment outperforms anatomical alignment in effectiveness.
MindEye2~\cite{mindeyev2} employs ridge regression to align different subjects into a shared latent space, followed by a shared decoding module.
%, subsequently using the resulting latent vectors in a common module for visual decoding.
MindBridge~\cite{mindbridge} generates pseudo stimuli to create shared stimuli for brain alignment.
% 现有方法存在的问题, %  我们方法的优越性
However, these alignment methods either rely on shared stimuli, restricting their applicability to datasets without such conditions, or utilize latent space alignment, which impedes their ability to uncover inter-subject neural variability.
We introduce an explicit brain functional alignment model to conquer the restriction of the shared stimuli and enhance the interpretation of inter-subject neural variability.
%and design a region-level explicit alignment method to enhance the interpretation of inter-subject neural variability. 


\section{Preliminary}

We begin with the illustration of the problem definition, and preliminary on cross-subject brain decoding baseline that reconstructs visual stimuli in the data-limited setting.


\noindent \textbf{Problem Definition.}
%Brain Visual Decoding in Low-Sample Setting.
The acquisition of fMRI data is both time-intensive and costly, leading to brain decoding scenarios frequently constrained by limited data. Therefore, this study focuses on investigating cross-subject brain decoding in a data-limited setting.
%
We follow MindEye2~\cite{mindeyev2} to build this setting on the Natural Scenes Dataset (NSD)~\cite{nsd}. Specifically, the decoding model is first pre-trained for one or several subjects $S_K$ using their full 40 scanning sessions of fMRI signals. Subsequently, the pre-trained decoding model is transferred to a new subject $S_N$, using only a single session of scanned fMRI  (approximately 1 hour of data, representing just 2.5\% of the full dataset). Finally, the adapted model is tested on $1000$ images shared across all subjects for subject $S_N$.


\noindent \textbf{Cross-subject Brain Decoding Baseline.}
Here we introduce our cross-subject decoding baseline method~\cite{mindeyev2}. 
% $L = \{L_i\}_{i=1}^C$ 
To reduce inter-subject differences, the baseline model first employs linear layers to map brain signals from different subjects into a shared latent space, where $C$ is the number of subjects. 
Then the fMRI embeddings are aligned with the latent space of a CLIP model~\cite{clip} through a diffusion prior~\cite{diffusionprior}, thereby leveraging generative models' capabilities for visual reconstruction.
%%%%%%%%%%
%%%%%%%%
The output embeddings are then fed through a low-level submodule and a retrieval submodule.  Two corresponding losses are utilized: a low-level reconstruction loss $\mathcal{L}_{low-level}$ between the blurry images generated by the low-level submodule and the ground truth, and a bidirectional MixCo loss $\mathcal{L}_{BiMixCo}$ to
perform contrastive optimization between the retrieval module's output and the CLIP image embeddings.
% Losses
%Two losses are used to ensure the reconstruction quality.
% To train the above modules, the MindEye2 utilizes three end-to-end training losses: the low-level module loss $\mathcal{L}_{lowlevel}$, the diffusion prior loss $\mathcal{L}_{prior}$, and the retrieval BiMixco loss $\mathcal{L}_{BiMixCo}$.
%$\mathcal{L}_{lowlevel}$ and
% The a 计算 low-level submodules 生成的 图片和真实值之间的重建损失,
%while $\mathcal{L}_{prior}$ incorporates the CLIP alignment reconstruction loss, measuring the alignment between the true image CLIP embedding and the outputs produced by the diffusion prior.
%
The final loss for the decoding model's training is formulated as:
$
\boldsymbol{\mathcal{L}}_{Dec} = \boldsymbol{\mathcal{L}}_{prior} + \alpha_1 \boldsymbol{\mathcal{L}}_{low-level} + \alpha_2 \boldsymbol{\mathcal{L}}_{BiMixCo},
$
where $\boldsymbol{\mathcal{L}}_{prior}$ denotes the diffusion prior loss that measures discrepancies between the CLIP image embedding and the outputs produced by the diffusion prior. More details can be found in~\cite{mindeyev2}. 
%, $\alpha_1$ and $\alpha_2$ are loss coefficients.
%%%%%%%%%%%%%%%%%%%%
%When introducing a new subject, MindEye2 adds a new ridge regression layer to map the new subject's fMRI into the shared latent space and trains the entire model with 1 session data. 
%The entire model is then trained for functional alignment and knowledge transfer. 
%In contrast, our approach only fine-tunes the functional alignment module, with far fewer parameters, to achieve effective alignment and stronger cross-subject decoding performance.


\vspace{-2mm}
\section{MindAligner}

\subsection{Overview}
Building on the pre-trained brain decoding model, MindAligner utilizes a \textbf{Brain Transfer Matrix} (BTM) to transform signals from novel subjects into the signal space of a known subject with limited data, thereby enabling cross-subject brain decoding.
To achieve reliable and fine-grained brain alignment, we design the \textbf{Brain Functional Alignment Module} (BFA). 
Notably, the alignment module is utilized only during the training phase to assist BTM learning; during the inference phase, only the lightweight BTM is retained. Next, we provide a detailed illustration of each module of MindAligner.
%, explain their training strategies, and describe the inference process.



\subsection{Brain Transfer Matrix}
Given the fMRI signal $\boldsymbol{\mathcal{F}}_N$ of an arbitrary novel subject $S_N$ as input, the Brain Transfer Matrix (BTM) $\boldsymbol{\mathcal{M}}$ aims to transform it into the fMRI signal of a subject $S_K$ seen during pre-training through a linear transformation:
\begin{equation}
\label{eq:matrix}
\hat{\boldsymbol{\mathcal{F}}_K} = \boldsymbol{\mathcal{M}} \times \boldsymbol{\mathcal{F}}_N,
\end{equation}
where $\hat{\boldsymbol{\mathcal{F}}_K}$ denotes the fMRI signal projected to the brain space of the seen subject $S_K$.
To improve parameter efficiency in the data-limited setting, we decompose $\boldsymbol{\mathcal{M}}$ into two low-rank matrices $\boldsymbol{\mathcal{A}}$ and $\boldsymbol{\mathcal{B}}$,
\begin{equation}
\boldsymbol{\mathcal{M}} = \boldsymbol{\mathcal{A}} \times \boldsymbol{\mathcal{B}},
\end{equation}
where $\boldsymbol{\mathcal{A}} \in\mathbb{R}^{n\times h}$ and $\boldsymbol{\mathcal{B}}\in\mathbb{R}^{h\times k}$,
$n$ and $k$ are the voxel dimensions of the novel and known subjects' fMRI, and $h$ is the hidden dimension. 
The matrix decomposition creates a shared latent space between two subjects for subsequent alignment.
% \begin{equation}
%     \boldsymbol{z}_N= \boldsymbol{\mathcal{A}} \times \boldsymbol{\mathcal{F}}_N
% \end{equation}
$\boldsymbol{\mathcal{M}}$ 
encodes transfer weights that capture region-level inter-subject brain correlations and can be utilized for functional alignment during inference. Moreover, these correlations provide valuable insights for analyzing inter-subject variability.


\subsection{Brain Functional Alignment Module}
% 作用
To learn a reliable brain transfer matrix, the Brain Functional Alignment Module (BFA) conducts soft cross-subject alignment in both the brain space and the shared latent space of the BTM.
As no strictly-paired fMRI data under identical stimuli is provided,
%, thereby enhancing the brain transfer matrix’s capability to project fMRI signals across subjects.
% 包含的模块
we employ a cross-stimulus neural mapper to facilitate stimulus transformation, rendering fMRI-pairs under visually similar stimuli. 
%Subsequently, the brain transfer matrix recovers the stimulus information from the latent space back to the brain space, generating fMRI signals aligned with the known subject.
Based on these fMRI-pairs, a multi-level brain alignment loss is employed to achieve final alignment.
%align the generated fMRI signals with the real ones of the known subject
%Finally, a brain alignment loss is utilized to 

% 我们的性能对比结果如 
\begin{table*}[!t]
\centering
\caption{Visual decoding performance comparison. 1h means 1 hour of data. \textbf{Bold} indicates the best performance.}
\resizebox{0.89\textwidth}{!}{
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
\textbf{Method} & \multicolumn{4}{c}{\textbf{Low-Level}} & \multicolumn{4}{c}{\textbf{High-Level}} & \multicolumn{2}{c}{\textbf{Retrieval}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-11}
 & \textbf{PixCorr$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{Alex(2)$\uparrow$} & \textbf{Alex(5)$\uparrow$} & \textbf{Incep$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{Eff$\downarrow$} & \textbf{SwAV$\downarrow$} & \textbf{Image$\uparrow$} & \textbf{Brain$\uparrow$} \\
\midrule
MindEye2(1 h)   & 0.195 & \textbf{0.419} & 84.2\% & 90.6\% & 81.2\% & 79.2\% & 0.810 & 0.468 & 79.0\% & 57.4\% \\
MindBridge(1 h) & 0.112 & 0.229 & 79.6\% & 89.0\% & 82.3\% & \textbf{86.7\%} & 0.840 & 0.521 & -      & -      \\
Ours(1 h)       & \textbf{0.206} & 0.414 & \textbf{85.6\%} & \textbf{91.6\%} & \textbf{83.0\%} & 81.2\% & \textbf{0.802} & \textbf{0.463} & \textbf{79.0\%} & \textbf{75.3\%} \\
\midrule
MindEye2(subj1) & \textbf{0.235} & \textbf{0.428} & 88.02\% & \textbf{93.33\%} & \textbf{83.56\%} & 80.75\% & \textbf{0.798} & 0.459 & \textbf{93.96\%} & 77.63\% \\
Ours(subj1)     & 0.226 & 0.415 & \textbf{88.19\%} & 93.26\% & 83.48\% & \textbf{81.76\%} & 0.800 & \textbf{0.459} & 90.90\% & \textbf{86.88\%} \\
\midrule
MindEye2(subj2) & 0.200 & \textbf{0.433} & 85.00\% & 92.13\% & 81.86\% & 79.39\% & 0.807 & 0.467 & 90.53\% & 67.18\% \\
Ours(subj2)     & \textbf{0.218} & 0.426 & \textbf{88.08\%} & \textbf{93.33\%} & \textbf{84.13\%} & \textbf{82.47\%} & \textbf{0.791} & \textbf{0.452} & 90.04\% & \textbf{85.61\%} \\
\midrule
MindEye2(subj5) & 0.175 & 0.405 & 83.11\% & 91.00\% & 84.33\% & 82.53\% & \textbf{0.781} & \textbf{0.444} & 66.94\% & 46.96\% \\
Ours(subj5)     & \textbf{0.197} & \textbf{0.409} & \textbf{84.69\%} & \textbf{91.61\%} & \textbf{84.63\%} & \textbf{82.76\%} & 0.784 & 0.454 & \textbf{70.62\%} & \textbf{65.95\%} \\
\midrule
MindEye2(subj7) & 0.170 & \textbf{0.408} & 80.70\% & 85.90\% & 74.90\% & 74.29\% & 0.854 & 0.504 & \textbf{64.44\%} & 37.77\% \\
Ours(subj7)     & \textbf{0.183} & 0.407 & \textbf{81.45\%} & \textbf{88.31\%} & \textbf{79.92\%} & \textbf{77.82\%} & \textbf{0.834} & \textbf{0.487} & 64.18\% & \textbf{62.58\%} \\
\bottomrule
\end{tabular}
}
\label{tab:method_comparison}
\vspace{-3mm}
\end{table*}



% The Cross-stimulus Neural Mapper
\noindent \textbf{Cross-stimulus Neural Mapper.}
% Due to the lack of stimuli-pair where two subjects view the same image, we aim to create pseudo data to facilitate alignment between two subjects.
%After obtaining the fMRI embedding $\boldsymbol{z}_N=$ 
%
Due to the lack of stimuli-pair where two subjects view the same image, we turn to select cross-subject fMRI pairs with similar stimuli $\boldsymbol{I}_N$ and $\boldsymbol{I}_K$ viewed by two subjects. 
The cross-stimulus neural mapper aims to transform the fMRI embedding $\boldsymbol{z}_N$ under stimuli $\boldsymbol{I}_N$ into those corresponding to stimuli $\boldsymbol{I}_K$ of the known subject $S_K$.
%thereby rendering cross-subject fMRI-pairs $(\hat{\boldsymbol{\mathcal{F}}_K}, \boldsymbol{\mathcal{F}}_N)$ under visually similar stimuli.
However, due to the absence of brain prior knowledge, directly generating fMRI signals is still challenging. Therefore, we leverage the differences between $\boldsymbol{I}_N$ and $\boldsymbol{I}_K$ as conditions for generating fMRI.
%As we have created a subject-shared latent space in the BTM $\boldsymbol{\mathcal{M}}$, we can achieve cross-subject cross-stimuli mapping by performing stimulus transfer in the latent space. 
Based on the fMRI embedding $\boldsymbol{z}_N$ projected by the low-rank matrix $\boldsymbol{\mathcal{A}}$, i.e., 
$\boldsymbol{z}_N= \boldsymbol{\mathcal{A}} \times \boldsymbol{\mathcal{F}}_N
$,
we use the visual stimuli difference \( \boldsymbol{E}_{\text{diff}} \) viewed by the two subjects as a condition to perform linear modulation to generate the stimuli embedding $\boldsymbol{z}_K$ corresponding to the stimuli $\boldsymbol{I}_K$ of the known subject: 
%To be more specific, we calculate the differences $\boldsymbol{E}_{\text{diff}}$ in the image stimulus embedding:
% \begin{equation}
% \boldsymbol{E}_{\text{diff}} = \mathcal{E}_{image}(\mathcal{I}_N) - \mathcal{E}_{image}(\mathcal{I}_K),
% \end{equation}
% \begin{equation}
% \boldsymbol{z}_{\text{diff}} = \boldsymbol{E}_{\text{diff}} \times\boldsymbol{\mathcal{M}}_{\text{diff}},
% \end{equation}
% \begin{equation}
% \boldsymbol{z}_K =  \boldsymbol{\mathcal{M}}_{C}(\boldsymbol{z}_N,\boldsymbol{z}_{\text{diff}}) 
% =\boldsymbol{z}_{\text{diff}}^{[:h]}\boldsymbol{z}_N + \boldsymbol{z}_{\text{diff}}^{[h:2h]},
% \end{equation}
\begin{align}
\boldsymbol{E}_{\text{diff}} &= \mathcal{E}_{image}(\mathcal{I}_N) - \mathcal{E}_{image}(\mathcal{I}_K), \\
\boldsymbol{z}_{\text{diff}} &= \boldsymbol{E}_{\text{diff}} \times \boldsymbol{\mathcal{M}}_{\text{diff}}, \\
\boldsymbol{z}_K &= \boldsymbol{\mathcal{M}}_{C}(\boldsymbol{z}_N, \boldsymbol{z}_{\text{diff}}),
% \boldsymbol{z}_{\text{diff}}^{[:h]}\boldsymbol{z}_N + \boldsymbol{z}_{\text{diff}}^{[h:2h]},
\end{align}
where $\mathcal{E}_{image}$ is the image encoder of pretrained CLIP~\cite{clip}. 
% The cross-stimulus neural mapper $\mathcal{M}_{C}(\cdot)$ is a linear modulation layer with condition \textcolor{blue}{$\boldsymbol{z}_{\text{diff}}$ generated by  $\boldsymbol{\mathcal{M}}_{\text{diff}} \in \mathbb{R}^{a\times 2h}$. $a$ is the clip embedding's dimension. 
% based on differences in image semantic stimuli
% \textcolor{red}{The cross-stimulus neural mapper $\mathcal{M}_{C}(\cdot)$ is in fact a linear modulation layer with modulation parameters generated by  $\mathcal{M}_{\text{diff}}(\cdot)$. After the stimulus transformation, we can obtain paired fMRI embedding $(\boldsymbol{z}_N, \boldsymbol{z}_K)$ under similar stimulus for subsequent brain alignment.}
The cross-stimulus neural mapper $\mathcal{M}_{C}(\cdot)$ is a linear modulation
that splits the condition $\boldsymbol{z}_{\text{diff}}$ to scale and shift parameters using $\boldsymbol{\mathcal{M}}_{\text{diff}} \in \mathbb{R}^{a\times 2h}$. $a$ is the clip embedding's dimension. These parameters can be used to modulate the input $\boldsymbol{z}_N$, thereby facilitating the cross-subject stimulus transformation in the latent space. The transformed embedding $\boldsymbol{z}_K$ is then projected to the known subject's space by the low-rank matrix $\boldsymbol{\mathcal{B}}$, rendering a synthesized fMRI embedding $\hat{\boldsymbol{\mathcal{F}}}_K$ to be aligned with the known subject's real fMRI embedding $\boldsymbol{F}_K$:
\begin{equation}
\hat{\boldsymbol{\mathcal{F}}}_K= \boldsymbol{z}_K  \times \boldsymbol{\mathcal{B}}.
\end{equation}
%The $\mathcal{M}_{CSN}(\cdot)$ facilitates a cross-subject stimulus transformation and the generation of pseudo data, providing data support for subsequent model optimization.
% After the stimulus transformation, we can obtain paired fMRI embedding $(\boldsymbol{z}_N, \boldsymbol{z}_K)$ under similar stimulus for subsequent brain alignment.

\noindent \textbf{Multi-level Brain Alignment Loss.}
The brain alignment loss integrates both signal-level reconstruction loss between the generated and real fMRI signals and an embedding-level alignment loss to achieve more refined alignment across different visual stimuli.
% to prevent overfitting to noise.
% Note that NSD does not present the same images to different subjects, 
To ensure the quality of synthesized fMRI $\hat{\boldsymbol{\mathcal{F}}_K}$, an fMRI reconstruction loss is designed to enforce the consistency between $\hat{\boldsymbol{\mathcal{F}}_K}$ and the real fMRI $\boldsymbol{\mathcal{F}}_K$ of the known subject:
\begin{equation}
\boldsymbol{\mathcal{L}}_{rec} = ||\hat{\boldsymbol{\mathcal{F}}_K}-\boldsymbol{\mathcal{F}}_K||_2^2. 
\end{equation}
To further enhance the alignment performance, we use the Kullback-Leibler (KL) Divergence loss to enforce the consistency between distributions of the generated and real fMRI signals:
% 两个 loss 的公式
% overall loss
\begin{equation}
\boldsymbol{\mathcal{L}}_{KL} = \mathcal{KL}(\hat{\boldsymbol{\mathcal{F}}_K},\boldsymbol{\mathcal{F}}_K).
\end{equation}
To enable fine-grained functional mapping under different stimuli,
we leverage intrinsic correlations between visual semantics to guide the alignment of the corresponding brain activities in the latent space. Specifically, we design a latent alignment loss \(\boldsymbol{\mathcal{L}}_{latent} \) by enforcing the consistency between fMRI embedding pairs and stimuli pairs:
%enhance the correlation between the fMRI embeddings \( \{\boldsymbol{z}_K, \boldsymbol{z}_N\} \) and image semantics:
\begin{equation}
\boldsymbol{\mathcal{L}}_{latent} = ||(\mathcal{R}(\mathcal{E}_{f}(\boldsymbol{z}_N), \mathcal{E}_{f}(\boldsymbol{z}_K)) - \mathcal{R}(\boldsymbol{E}_N, \boldsymbol{E}_K)||_2^2,
\end{equation}
where ${E}_N$ and ${E}_K$ denote the CLIP embeddings of $\mathcal{I}_N$ and $\mathcal{I}_K$. 
$\mathcal{R}(\cdot)$ calculates the  dissimilarity matrix between embedding pairs. $\mathcal{E}_{f}$ denotes a functional embedder for fMRI embeddings for better dissimilarity calculation.
%
% \textcolor{red}{The constraint on the latent features supports the model's optimization by enhancing the stimulus-brain correlation.}
Hence, the final brain alignment loss $\boldsymbol{\mathcal{L}}_{\text{Align}}$ is formulated as:
\begin{equation}
\boldsymbol{\mathcal{L}}_{\text{Align}} = \boldsymbol{\mathcal{L}}_{Dec} + \alpha_{rec}\boldsymbol{\mathcal{L}}_{rec}+\alpha_{KL}\boldsymbol{\mathcal{L}}_{KL}+\alpha_{la}\boldsymbol{\mathcal{L}}_{latent},
\end{equation}
% Mindeye + Brain Alignment Loss
where $\alpha_{rec}, \alpha_{kl}, \alpha_{la}$ are the loss coefficients, and $\boldsymbol{\mathcal{L}}_{Dec}$ denotes the decoding loss in the baseline method.
The combination of these losses can improve the semantic accuracy of visual reconstruction and also reduce the reliance on same-stimulus data.

\subsection{Inference} 
% TODO:
% During the inference phase, only the trained brain transfer matrix is utilized for functional alignment, thereby facilitating cross-subject visual decoding. 
% TODO:模型的 subject-specific 性.
% To be more specific, given a new subject's fMRI, the MindAligner transforms it to match a specific known subject's fMRI, allowing the signal to be directly utilized by the pretrained decoding model. Here, our MindAligner is a lightweight functional alignment plug-in that does not significantly increase the original model’s parameters, facilitating efficient cross-subject visual decoding.
During inference, only the trained BTM is used for functional alignment (Eq.~\ref{eq:matrix}). 
The generated $\hat{\boldsymbol{\mathcal{F}}}_K$ is then directly fed into the pre-trained model for brain decoding. MindAligner is a lightweight functional alignment module that enables efficient cross-subject visual decoding.
% ver1
% During inference, only the trained BTM is used for functional alignment (Eq.~\ref{eq:matrix}), enabling cross-subject visual decoding. The generated $\hat{\boldsymbol{\mathcal{F}}}_K$ can be directly fed into the pre-trained model for brain decoding. MindAligner is a lightweight functional alignment plug-in that doesn’t significantly increase the original model's parameters, ensuring efficient cross-subject visual decoding.
% ver0
% During the inference phase, only the trained brain transfer matrix is utilized for functional alignment, as in Eq.~\ref{eq:matrix}, thereby facilitating cross-subject visual decoding.
% The generated $\hat{\boldsymbol{\mathcal{F}}}_K$ can be then directly input to the pre-trained decoding model to perform brain decoding. Here, our MindAligner is a lightweight functional alignment plug-in that does not significantly increase the original model’s parameters, facilitating efficient cross-subject visual decoding.



\section{Experiments}
In this section, we present the implementation details, followed by fMRI-to-image reconstruction results and brain functional alignment analysis. The Appendix includes additional metrics, qualitative and quantitative results, model efficiency, and further visualizations.
% overview
% APPENDIX 中包含哪些东西，不用加具体的索引。

\subsection{Implementation Details}
The BTM is composed of two bias-free linear layers with a hidden size $h = 4096$. The input dimension \( n \) and output dimension \( k \) of BTM are determined by the specific subject transfer pairs.  
For subjects 1, 2, 5, and 7, the dimensions are 15,724, 14,278, 13,039, and 12,682, respectively.
The cross-stimulus neural mapper is implemented using the Feature-wise Linear Modulation model~\cite{film}, where the input dimension of \( \mathcal{M}_{\text{diff}} \) is \( a = 768 \), matching the CLIP embedding dimension, and the output is $2h = 8192$.  
The functional embedder is a linear layer with input and output sizes of \( h = 4096 \).
% loss
The loss coefficients are set to $\alpha_{rec} = 1$, $\alpha_{la} =\alpha_{KL} = 0.001$, $\alpha_1 = 0.033$, and $\alpha_2 = 0.016$. The learning rates for the brain transfer matrix, cross-stimulus neural mapper, and functional embedder are all set to $1\text{e}{-5}$.
% , while the learning rate for $\mathcal{M}_{C}$ is set to $1\text{e}{-5}$. 
We use a batch size of 16 and optimize using Adam. Training on a single NVIDIA A100 GPU achieves convergence in approximately 12 hours.


\subsection{Dataset}
We use the Natural Scenes Dataset (NSD)~\cite{nsd}, the largest publicly available set, widely used for brain visual decoding. It includes neural responses from subjects viewing complex images from the MSCOCO-2017 dataset\cite{coco}. In line with MindEye2's data-limited setting, our approach uses only a single session of neural recordings, corresponding to one hour of data.


\subsection{Metrics}
To evaluate fMRI-to-image reconstruction performance, we assess both low- and high-level properties of the reconstructed images. Low-level properties capture fundamental visual elements like pixel similarity and edges, while high-level properties reflect semantic information. 
Following previous works~\cite{mindeyev1, mindeyev2}, we adopt the PixCorr, SSIM, AlexNet(2), and AlexNet(5)~\cite{alex} to evaluate low-level properties and use Inception~\cite{inception}, CLIP~\cite{clip}, EffNet-B~\cite{effi} and SwAV~\cite{swav} to evaluate high-level properties. 
These metrics assess the fidelity of the reconstructed images by comparing them with the ground truth. Metric details can be found in Appendix~\ref{ap:metrics}.


To evaluate functional alignment, we use two metrics: fMRI Spatial Correlation (fSC)~\cite{corr} and Transfer Quantity (TQ). fSC measures the Pearson correlation between corresponding brain regions of two subjects ($i \neq j$), assessing global alignment consistency.
TQ captures voxel-level differences by analyzing the weights of the BTM $\boldsymbol{\mathcal{M}}$, which maps voxels between subjects. For a source voxel indexed by $i$, TQ is defined as $\text{TQ}_i =\sum_{0 \leq j < p} ||\boldsymbol{\mathcal{M}}_{i, j}||$, where $p$ is the number of voxels in the target brain. High TQ values indicate regions with greater activation differences and more intricate functional alignment requirements.


\subsection{fMRI-based Visual Decoding}

We evaluate the visual decoding performance of MindAligner in both qualitative and quantitative manners. The compared state-of-the-art methods include our baseline MindEye2~\cite{mindeyev2} and MindBridge~\cite{mindbridge}.



\begin{figure*}[!t]
\centering
\centerline{\includegraphics[width=0.90\textwidth]{images/recon_compare.pdf}}
\vspace{-3mm}
\caption{Visualization of MindAligner's decoding results from training on one hour of data.}
\vspace{-4mm}
\label{fig:1hvis}
\end{figure*}


% 鲁棒性结果
\begin{figure}[!t]
\begin{center}
\centerline{\includegraphics[width=0.9\linewidth]{images/recon5-13.pdf}}
\vspace{-3mm}
\caption{
Visualization results of aligning a new subject with different known subjects.
% Visualization of alignment between different subjects. 
% The first column represents the original visual stimuli, while subsequent columns show reconstructed images from aligned fMRI data across pairs of subjects.
}
% \caption{Aignment between different subjects}
\label{fig:crosssubjalign}
\end{center}
\vskip -0.3in
\end{figure}



\noindent \textbf{Qualitative Comparison.}
% We trained our model using data from a single session and visualized the results in Fig.~\ref{fig:1hvis}. As observed, our MindAligner decodes visual signals with greater semantic accuracy, demonstrating its effectiveness in reconstructing stimuli with higher fidelity compared to existing methods. \textcolor{blue}{This advantage stems from our approach, which, compared to previous methods, not only leverages cross-subject image semantic information to facilitate fMRI reconstruction but also incorporates semantic differences between subjects. By associating brain signals with the semantic information of image stimuli, our method achieves semantically cohesive alignment.}
We train our model using data from a single session (1 hour of data) and visualize the results in Fig.~\ref{fig:1hvis}. 
MindAligner delivers decoding results that are more consistent with the original visual stimuli semantics compared to the baseline, highlighting its effectiveness. 
The performance improvement can be attributed to the effective learning of brain transfer matrix that accurately aligns the novel subject with the known subject, thus well transferring the pre-trained decoding model to the new subject with limited data.




\noindent \textbf{Quantitative Comparison.}
As summarized in Tab.~\ref{tab:method_comparison}, MindAligner surpasses state-of-the-art methods across almost all metrics. Notably, it achieves a 17.9\% improvement in brain retrieval performance. The observed improvement can be attributed to the inherent challenges in implicit alignment strategies employed in previous methods. Aligning multiple subjects with substantial individual differences remains a complex task that may result in information loss during the alignment process. Our method addresses this challenge by adopting an explicit alignment strategy that aligns one subject at a time, avoid conflicts arising from multi-subject alignment. Our model focuses on fine-grained cross-subject brain mapping, thereby achieving better decoding performance with high fidelity.




\noindent \textbf{Ablation Study.}
To evaluate the effectiveness of each model design in MindAligner, we perform an ablation study using Subject 2 as the novel subject and Subject 1 as the known subject. The results exclude the refinement step of MindEye2 for generated images. As shown in Tab.~\ref{tab:ablation_study}, training MindAligner with only the visual decoding loss $\boldsymbol{\mathcal{L}}_{dec}$ yields suboptimal cross-subject reconstruction performance, underscoring the difficulty of directly generalizing pre-trained models to new subjects without effective alignment.
Adding signal reconstruction loss $\boldsymbol{\mathcal{L}}_{rec}$ significantly enhances performance as it leads to more accurate brain activity reconstructions.
%by aligning the synthesized fMRI with the target fMRI, leading to more accurate and realistic brain activity reconstructions.
% kl
The incorporation of $\boldsymbol{\mathcal{L}}_{KL}$ further strengthens alignment by enforcing consistency between the distributions of the generated and real signals. 
%by mitigating overfitting to noise. 
%Furthermore, due to the low signal-to-noise ratio (SNR) of fMRI data, incorporating 
% latent
Lastly, $\boldsymbol{\mathcal{L}}_{latent}$ exploits the correlation of visual stimuli and fMRI embeddings to guide the brain alignment in the latent space, thereby improving model's ability to capture visual semantics in brain activity and enhancing low-level reconstruction performance. These losses together work in synergy to refine alignment and improve cross-subject decoding fidelity.



\begin{table*}[!t]
\centering
\caption{Ablation study results. The combination of $\boldsymbol{\mathcal{L}}_{dec}$+$\boldsymbol{\mathcal{L}}_{rec}$+$\boldsymbol{\mathcal{L}}_{KL}$+$\boldsymbol{\mathcal{L}}_{latent}$ is our final model setting.}
\resizebox{0.78\textwidth}{!}{
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Method} & \textbf{PixCorr$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{Alex(2)$\uparrow$} & \textbf{Alex(5)$\uparrow$} & \textbf{Incep$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{Eff$\downarrow$} & \textbf{SwAV$\downarrow$} \\
\midrule
+$\boldsymbol{\mathcal{L}}_{dec}$ & 0.072 & 0.318 & 63.50\% & 71.44\% & 66.07\% & 62.59\% & 0.905 & 0.550 \\
+$\boldsymbol{\mathcal{L}}_{dec}$+$\boldsymbol{\mathcal{L}}_{rec}$ & 0.186 & 0.340 & 86.83\% & 93.51\% & 84.55\% & 82.42\% & 0.811 & 0.465 \\
+$\boldsymbol{\mathcal{L}}_{dec}$+$\boldsymbol{\mathcal{L}}_{rec}$+$\boldsymbol{\mathcal{L}}_{KL}$ & 0.191 & 0.407 & 87.98\% & 92.99\% & \textbf{86.61\%} & 82.16\% & \textbf{0.780} & \textbf{0.453} \\
+$\boldsymbol{\mathcal{L}}_{dec}$+$\boldsymbol{\mathcal{L}}_{rec}$+$\boldsymbol{\mathcal{L}}_{KL}$+$\boldsymbol{\mathcal{L}}_{latent}$ & \textbf{0.195} & \textbf{0.408} & \textbf{88.25\%} & \textbf{93.51\%} & 86.24\% & \textbf{82.72\%} & 0.782 & 0.454 \\
\bottomrule
\end{tabular}%
}
\label{tab:ablation_study}
\vspace{-3mm}
\end{table*}




\begin{figure}[!t]
\begin{center}
\centerline{\includegraphics[width=0.95\linewidth]{images/TransportMassCompare.pdf}}
\vspace{-3mm}
\caption{
Visualization of transfer quantity in brain heatmaps.}
\label{fig:transprotmasscompare}
\end{center}
\vskip -0.3in
\end{figure}
% ver0
% Brain heatmap of TQ when different source subjects are transferred to the same target subject.
% the TQ metric on MindAligner’s brain transfer matrix to assess cross-subject associations and visualize the results through brain heatmaps

\begin{figure}[!t]
    \begin{center}
    \centerline{\includegraphics[width=0.97\linewidth]{images/corr_1sess.png}}
    \vspace{-4mm}
    \caption{
    % The fSC between two subjects' brains after different functional alignment methods. 
    Comparison of fSC results between MindAligner and the baseline.
    }
    \label{fig:corr}
    \end{center}
    \vskip -0.3in
\end{figure}



\noindent \textbf{Impact of Aligning to Different Subjects.}
% \textcolor{gray}{In Fig.~\ref{fig:crosssubjalign}, we visualized the results of aligning a specific new subject with different known subjects. The results demonstrate that our MindAligner exhibits robustness when aligning with different known subjects, enabling faithful decoding of the stimuli from the novel subject.}
We visualize the results of fixing a new subject and aligning it with different known subjects in Fig.~\ref{fig:crosssubjalign}. The reconstruction performance are stable when aligned with different known subjects, as the generated images are nearly identical. This demonstrates the robustness of our brain functional alignment. The choice of the known subject has minimal impact on visual decoding performance. 
%
This is because MindAligner
%not only employs reconstruction between the generated fMRI and real fMRI under similar stimuli to ensure high fidelity of the results but also 
%
leverages the intrinsic correlation between visual semantics to guide the alignment of corresponding brain activities, thereby facilitating robust alignment performance. We provide more detailed cross-subject results in Appendix~\ref{app:rob}.




\begin{table}[h]
\centering
\caption{Efficiency comparison results. ``Tr. Param." refers to the model's trainable parameters when adding a new subject.}
\resizebox{0.75\linewidth}{!}{
\begin{tabular}{@{}lllc@{}}
    \toprule
    Method & Tr. Param. & Total Param. & Inference \\
    \midrule
    MindEye2    & 2.21G  & 2.21G & 5.000 s \\
    MindAligner & 139.23M & 2.21G  & 5.056 s \\
    \bottomrule
\end{tabular}
}
\label{tab:effi}
\end{table}



\noindent \textbf{Computational Efficiency.}
We compare the computational efficiency between our model and baseline MindEye2 w.r.t. parameter count and inference time per image. 
As shown in Tab.~\ref{tab:effi}, MindAligner achieves superior decoding performance while significantly reduces the fine-tuning requirement, with just 6\% of MindEye2's learnable parameters, demonstrating its efficiency. Moreover, the addition of our alignment module only slightly increases the inference time.




\subsection{Brain Functional Alignment Analysis}

To deepen the understanding of the brain functional alignment process, we provide detailed visualizations of brain regions along with corresponding functional analysis, offering insights into cross-subject variability and underlying neuroscience mechanisms. 







\noindent \textbf{Region-level Functional Mapping.} 
We apply the Transfer Quantity (TQ) metric on MindAligner’s brain transfer matrix to assess cross-subject associations and visualize the results through brain heatmaps. As shown in Fig.~\ref{fig:transprotmasscompare},
the visualization results highlight two key neuroscience findings. 
1)~\textit{\textbf{The visual system exhibits a hierarchical pattern of inter-subject variability.}} 
The early visual region (labeled as "EarlyVis" in Fig.~\ref{fig:transprotmasscompare}) presents lower inter-subject variability while higher visual regions (including OPA, FFA, PPA, and EBA) show larger variability.
This graded variability aligns with established neuroscientific principles. The early visual region processing fundamental features like lines/textures show more conserved neural mechanisms, sharing larger commonality across subjects. In contrast, higher visual regions handle more complex cognitive processes, including categorical perception and semantic understanding, leading to higher variability across individuals.
%
2)~\textit{\textbf{The ventral pathway exhibits the greatest inter-subject variability.}} The ventral pathway - anatomically positioned on the brain's ventral surface (lower section in Fig.~\ref{fig:transprotmasscompare}) and encompassing functional regions like PPA and FFA -  demonstrates the highest variability among visual pathways. This variability arises from its important role in high-level visual processing, such as object recognition, face perception, and semantic interpretation. 
The ventral stream integrates sensory input with prior knowledge, experiences, and cognitive biases. This results in greater individual differences, as factors like familiarity, attention, and personal experiences shape how visual information is interpreted and understood.



\begin{figure}[!t]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.97\linewidth, height=53mm]{images/TransportMass_1s40s.pdf}}
\vspace{-3mm}
\caption{Visualization of transfer quantity in brain heatmaps from MindAligner training using 1 hour and full datasets.}
\label{fig:1s40s}
\end{center}
\vskip -0.4in
\end{figure}




% 定量对比
% \noindent \textbf{Quantitative Comparison.}
\noindent \textbf{Cross-subject Correlation Analysis.}
To assess the alignment effect of MindAligner, we measure the fMRI Spatial Correlation (fSC) for different subject pairs, comparing our functional alignment strategy with the baseline in reducing inter-subject differences, as shown in Fig.~\ref{fig:corr}.
We use $1 \rightarrow 2$ to denote aligning Subject 1 to Subject 2.
The results demonstrate that our method significantly outperforms the existing baseline in fSC in all transfer configurations, demonstrating the superiority of our explicit alignment manner against the implicit alignment. By establishing fine-grained voxel correspondences between subjects, MindAligner significantly enhances alignment performance even without paired fMRI signals under shared stimuli, leading to a better visual decoding performance.


\noindent{\textbf{Brain Alignment with More Data.}}
Furthermore, to evaluate MindAligner's robustness in limited data scenarios, we compare its performance using only 1 hour of fMRI data (2.5\% of the full dataset) to using the full scanning sessions. As shown in Fig.~\ref{fig:1s40s}, even with limited data, the TQ distribution closely resembles that of the full dataset, effectively identifying regions with significant inter-subject variability. This highlights the robustness of our explicit brain alignment strategy under data scarcity. 


\section{Conclusion}
We present MindAligner, a functional alignment framework for cross-subject brain visual decoding. Unlike existing methods, it addresses insufficient alignment and lack of interpretability by learning a brain transfer matrix for voxel-level correspondences and proposing a brain functional alignment module for cross-subject mapping. Experiments validate the effectiveness of our method.



\section*{Impact Statement}
MindAligner enables high-quality visual perception reconstruction from a single fMRI session, potentially advancing the clinical diagnosis and brain computer interface applications. 
This approach holds significant potential for enabling alignment across diverse data formats and uncovering commonalities in brain organization across species, such as between humans and monkeys. Moreover, it could play a pivotal role in advancing the creation of unified brain atlases.
% 
The datasets used are publicly available, ensuring transparency and participant privacy.


\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn % 可选要不要

\section{Explanation of Metrics}
\label{ap:metrics}
% 这是之前删减后的版本
% 看这：https://aicarrier.feishu.cn/wiki/WiInwiinAijqEKkED93c8q4On7f
Following prior work~\cite{mindeyev1, mindeyev2}, we evaluate the image reconstruction results based on eight metrics, which are categorized into low-level and high-level groups. Low-level metrics, including Pixelwise Correlation (PixCorr), Structural Similarity Index (SSIM)~\cite{ssmi}, AlexNet(2) (Alex(2)), and AlexNet(5) (Alex(5))~\cite{alex}, focus on textural and structural details. High-level metrics—Inception (Incep)~\cite{inception}, CLIP~\cite{clip}, EfficientNet-B (Eff)~\cite{effi}, and SwAV-ResNet50 (SwAV)~\cite{swav} —assess semantic fidelity. Alex(2), Alex(5), Incep, and CLIP metrics are derived by calculating Pearson correlation between the embeddings of the ground truth and reconstructed images, following the two-way identification framework of Ozcelik and VanRullen~\cite{braindiffuser}. Eff and SwAV scores are based on the average distance between feature embeddings.


In addition to the aforementioned metrics, we also evaluate the model using retrieval-based metrics to quantify the fine-grained image information in the fMRI embeddings, following the methodology in MindEye2~\cite{mindeyev2}. Specifically, for image retrieval, each test fMRI scan is first transformed into its corresponding fMRI representation. We then compute the cosine similarity between this representation and the CLIP-derived image representations of 300 randomly selected images from the test set. Retrieval success is defined as the maximization of cosine similarity between the fMRI embedding and its ground truth CLIP embedding (top-1 retrieval, with random chance at 1/300). To mitigate variability from random batch sampling, the evaluation is repeated 30 times per test sample. The same procedure is applied for brain retrieval, with fMRI and image representations swapped.



\section{Details on Model Parameters}
% 1->2 版本的，不同的受试者大小会有区别
\begin{table}[h]
\centering
\caption{Parameter counts of different modules.}
\begin{tabular}{llc}
    \toprule
    Module & Parameter & Used during inference \\
    \midrule
    BTM & 122,888,192 & \Checkmark  \\
    FE & 6,299,648 & \XSolidBrush \\
    CNM & 16,781,312 & \XSolidBrush  \\
    MindEye2 & 2,227,290,748 & \Checkmark \\
    MindEye2.ridge\_regression & 64,405,504 & \Checkmark \\
    MindEye2.backbone         & 1,903,020,028 & \Checkmark \\
    MindEye2.diffusion\_prior & 259,865,216 & \Checkmark \\
    % \midrule
    % MindBridge\_voxel2clip & 609,586,176 & \Checkmark \\
    \bottomrule
\end{tabular}
\label{tab:paramcount}
\end{table}
% 百分比
\begin{table}[h]
\centering
\caption{Trainable parameter share of different modules.}
\begin{tabular}{llc}
    \toprule
    Module &  Parameter \%  & Used during inference \\
    \midrule
    MindEye2 & 100\% & \Checkmark \\
    \textbf{MindAligner (Ours)} & 6.2\% & - \\
    \midrule
    MindAligner.BTM & 5.2\% & \Checkmark  \\
    MindAligner.FE & 0.3\% & \XSolidBrush \\
    MindAligner.CNM & 0.7\% & \XSolidBrush  \\
    
    
    \bottomrule
\end{tabular}
\label{tab:paramshare}
\end{table}
We list the parameter counts of different modules in the pipeline. 
MindAligner comprises the Brain Transfer Matrix (BTM), Functional Embedder (FE), and Cross-Stimulus Neural Mapper (CNM).
Tab.~\ref{tab:paramcount} shows the number of parameters for each module, while Tab.~\ref{tab:paramshare} displays the trainalble parameter share for each module, helping us understand their relative contributions to the overall model.
The results show that our model has a relatively small parameter count, accounting for only 6\% of the parameter size of the visual decoding model. 
Moreover, the introduction of the FE and CNM modules during the training phase does not significantly increase the model's parameters, contributing to only 1\% of the total parameter count. The BTM only accounts for 5\% of the parameter size of the visual decoding model. 

% \textcolor{blue}{Despite this slight increase in parameters, the inference time remains comparable. MindAligner requires 12,512 ms to reconstruct an image from brain signals, only marginally higher than MindEye2’s 11,319 ms. This demonstrates that the added modules enhance cross-subject alignment with negligible computational overhead.}

% origin names
% \begin{table}[h]
% \centering
% \caption{Parameter counts comparison.}
% \begin{tabular}{ll}
%     \toprule
%     Module &  Parameter \\
%     \midrule
%     align\_model & 122,888,192 \\
%     A\_layer & 6,299,648 \\
%     B\_model & 16,781,312 \\
%     MindEye2 & 2,227,290,748 \\
%     MindEye2.ridge & 64,405,504 \\
%     MindEye2.backbone & 1,903,020,028 \\
%     MindEye2.diffusion\_prior & 259,865,216 \\
%     \bottomrule
% \end{tabular}
% \end{table}


\section{More Results of Aligning to Different Subjects}
\label{app:rob}
% 图片
\begin{figure*}[!t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.97\textwidth]{images/appendix_recon.pdf}}
\caption{More visualization of brain decoding results under different novel and known subjects.}
\label{fig:apcrosssubjalign}
\end{center}
\vskip -0.2in
\end{figure*}
We visualized more results of fixing a new subject and aligning it with different known subjects in Fig.~\ref{fig:apcrosssubjalign}. 
MindAligner demonstrates robustness, as the generated images remain nearly identical when the novel subject is fixed.
This is because MindAligner combines fMRI reconstruction between generated and real data under similar stimuli to ensure result fidelity, while also utilizing intrinsic correlations in visual semantics to guide the alignment of corresponding brain activities, enabling robust optimization.


\begin{table*}[!t]
\centering
\caption{Performance of MindAligner with different hidden sizes.}
\resizebox{0.91\textwidth}{!}{
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
\textbf{Hidden size} & \multicolumn{4}{c}{\textbf{Low-Level}} & \multicolumn{4}{c}{\textbf{High-Level}} & \multicolumn{2}{c}{\textbf{Retrieval}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-11}
 & \textbf{PixCorr$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{Alex(2)$\uparrow$} & \textbf{Alex(5)$\uparrow$} & \textbf{Incep$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{Eff$\downarrow$} & \textbf{SwAV$\downarrow$} & \textbf{Image$\uparrow$} & \textbf{Brain$\uparrow$} \\
\midrule
64   & 0.144 & 0.384 & 76.33\% & 85.34\% & 74.67\% & 73.99\% & 0.876 & 0.512 & 79.13\% & 64.00\% \\
256  & 0.166 & 0.395 & 80.83\% & 87.81\% & 76.89\% & 76.47\% & 0.858 & 0.498 & 86.67\% & 77.83\% \\
512 & 0.185 & 0.405 & 83.85\% & 90.95\% & 80.74\% & 78.55\% & 0.839 & 0.481 & 89.06\% & 82.97\% \\
1024 & 0.204 & 0.415 & 87.01\% & 93.30\% & 83.51\% & 80.40\% & 0.811 & 0.463 & 90.30\% & 86.19\% \\
2048 & 0.215 & 0.422 & 88.30\% & 93.30\% & 83.94\% & 82.75\% & 0.798 & 0.458 & 90.16\% & 85.96\% \\
4096 & 0.218 & 0.425 & 88.36\% & 93.55\% & 84.17\% & 82.57\% & 0.794 & 0.455 & 90.09\% & 86.19\% \\
\bottomrule
\end{tabular}
}
\label{tab:performance_comparison}
\end{table*}


\section{Ablation Study on Hidden Size}
To investigate the potential for further reducing the model size, we adjusted the hidden size and evaluated the model's performance at different values. The experiments showed that when the hidden size is set to 1024, the model delivers comparable performance, while its size is reduced to one-quarter of the original. Compared to the $d=4096$ configuration, which only accounts for 6\% of the framework, the 
$d=1024$ setting accounts for just 2\%, further highlighting the efficiency performance advantages of our model.


\section{More Detialed MindAligner Reconstruction Performance}
We provide more detailed MindAligner reconstruction evaluation results, as shown in Tab.~\ref{tab:apmethod_comparison}.
MindAligner surpasses the baseline in almost all metrics, even when applying the same novel subject to different known subjects. 
Notably, our method achieves a 17.9\% improvement in brain retrieval performance. This improvement stems from addressing the limitations of implicit alignment strategies used in prior methods. Aligning multiple subjects with significant individual differences is inherently challenging and often leads to functional information loss during the alignment process. To overcome this, our approach employs an explicit alignment strategy, aligning one subject at a time, which effectively mitigates the conflicts arising from multi-subject alignment. By focusing on region-level cross-subject brain mapping, our model not only achieves superior visual performance but also captures more comprehensive brain region features for functional representation.

% Full Sessions
% \begin{table*}[!t]
% \centering
% \caption{\textcolor{blue}{Additional Full Session Results}}
% \begin{tabular}{@{}lcccccccccc@{}}
% \toprule
% \textbf{Method} & \multicolumn{4}{c}{\textbf{Low-Level}} & \multicolumn{4}{c}{\textbf{High-Level}} & \multicolumn{2}{c}{\textbf{Retrieval}} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-11}
%  & \textbf{PixCorr$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{Alex(2)$\uparrow$} & \textbf{Alex(5)$\uparrow$} & \textbf{Incep$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{Eff$\downarrow$} & \textbf{SwAV$\downarrow$} & \textbf{Image$\uparrow$} & \textbf{Brain$\uparrow$} \\
% \midrule
% Takagi... & 0.246 & 0.410 & 78.9\% & 85.6\% & 83.8\% & 82.1\% & 0.811 & 0.504 & - & - \\
% Ozcelik... & 0.273 & 0.365 & 94.4\% & 96.6\% & 91.3\% & 90.9\% & 0.728 & 0.422 & 18.8\% & 26.3\%  \\
% MindEye1 & 0.319 & 0.360 & 92.8\% & 96.9\% & 94.6\% & 93.3\% & 0.648 & 0.377 & 90.0\% & 84.1\%  \\
% MindEye2 & 0.322 & 0.431 & 96.1\% & 98.6\% & 95.4\% & 93.0\% & 0.619 & 0.344 & 98.8\% & 98.3\%  \\
% Ours & 0.322 & 0.421 & 95.8\% & 98.8\% & 95.6\% & 93.8\% & 0.612 & 0.340 & 98.9\% & 98.3\% \\
% \midrule
% MindEye2(1 hour) & 0.195 & 0.419 & 84.2\% & 85.6\% & 81.2\% & 79.2\% & 0.810 & 0.468 & 79.0\% & 57.4\%  \\
% Ours(1 hour) & \textbf{0.224} & \textbf{0.420} & \textbf{87.8\%} & \textbf{93.6\%} & \textbf{84.8\%} & \textbf{83.5\%} & \textbf{0.780} & \textbf{0.440} & \textbf{83.1\%} & \textbf{76.0\%} \\
% \bottomrule
% \end{tabular}
% \end{table*}

\begin{table*}[!t]
\centering
\caption{Detailed performance of our model compared with the baseline. \textbf{Bold} means our results outperform the baseline.}
\resizebox{0.91\textwidth}{!}{
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
\textbf{Method} & \multicolumn{4}{c}{\textbf{Low-Level}} & \multicolumn{4}{c}{\textbf{High-Level}} & \multicolumn{2}{c}{\textbf{Retrieval}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-11}
 & \textbf{PixCorr$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{Alex(2)$\uparrow$} & \textbf{Alex(5)$\uparrow$} & \textbf{Incep$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{Eff$\downarrow$} & \textbf{SwAV$\downarrow$} & \textbf{Image$\uparrow$} & \textbf{Brain$\uparrow$} \\
\midrule
MindEye2(subj1)   & 0.235          & 0.428          & 88.02\%          & 93.33\%          & 83.56\%          & 80.75\%          & 0.798          & 0.459          & 93.96\%          & 77.63\%          \\
Ours(subj1)       & 0.226          & 0.415          & \textbf{88.19\%} & 93.26\%          & 83.48\%          & \textbf{81.76\%} & 0.800          & \textbf{0.459} & 90.90\%          & \textbf{86.88\%} \\
1$\rightarrow$2   & 0.222          & 0.413          & \textbf{88.09\%} & 93.28\%          & \textbf{84.01\%} & \textbf{81.82\%} & \textbf{0.796} & \textbf{0.457} & 91.56\%          & \textbf{87.49\%} \\
1$\rightarrow$5   & 0.227          & 0.416          & \textbf{88.29\%} & \textbf{93.36\%} & 83.54\%          & \textbf{80.94\%} & 0.803          & 0.461          & 89.76\%          & \textbf{85.78\%} \\
1$\rightarrow$7   & 0.229          & 0.416          & \textbf{88.18\%} & 93.13\%          & 82.90\%          & \textbf{82.52\%} & 0.800          & \textbf{0.458} & 91.37\%          & \textbf{87.36\%} \\
\midrule
MindEye2(subj2)   & 0.200          & 0.433          & 85.00\%          & 92.13\%          & 81.86\%          & 79.39\%          & 0.807          & 0.467          & 90.53\%          & 67.18\% \\
Ours(subj2)       & \textbf{0.218} & 0.426          & \textbf{88.08\%} & \textbf{93.33\%} & \textbf{84.13\%} & \textbf{82.47\%} & \textbf{0.791} & \textbf{0.452} & 90.04\%          & \textbf{85.61\%} \\
2$\rightarrow$1   & \textbf{0.218} & 0.425          & \textbf{88.36\%} & \textbf{93.55\%} & \textbf{84.17\%} & \textbf{82.57\%} & \textbf{0.794} & \textbf{0.455} & 90.09\%          & \textbf{86.19\%} \\
2$\rightarrow$5   & \textbf{0.218} & 0.426          & \textbf{87.88\%} & \textbf{93.13\%} & \textbf{83.39\%} & \textbf{82.05\%} & \textbf{0.793} & \textbf{0.454} & 90.34\%          & \textbf{85.67\%} \\
2$\rightarrow$7   & \textbf{0.217} & 0.427          & \textbf{88.00\%} & \textbf{93.32\%} & \textbf{84.83\%} & \textbf{82.78\%} & \textbf{0.785} & \textbf{0.449} & 89.70\%          & \textbf{84.98\%} \\
\midrule
MindEye2(subj5)   & 0.175          & 0.405          &  83.11\%         & 91.00\%          & 84.33\%          & 82.53\%          & 0.781          & 0.444          & 66.94\%          & 46.96\% \\
Ours(subj5)       & \textbf{0.197} & \textbf{0.409} & \textbf{84.69\%} & \textbf{91.61\%} & \textbf{84.63\%} & \textbf{82.76\%} & 0.784          & 0.454          & \textbf{70.62\%} & \textbf{65.95\%} \\
5$\rightarrow$1   & \textbf{0.196} & 0.405          & \textbf{84.23\%} & \textbf{91.28\%} & \textbf{84.66\%} & \textbf{82.93\%} & 0.787          & 0.459          & \textbf{69.67\%} & \textbf{65.14\%} \\
5$\rightarrow$2   & \textbf{0.196} & \textbf{0.409} & \textbf{84.71\%} & \textbf{91.88\%} & \textbf{84.56\%} & \textbf{82.88\%} & 0.783          & 0.455          & \textbf{70.78\%} & \textbf{66.38\%} \\
5$\rightarrow$7   & \textbf{0.198} & \textbf{0.412} & \textbf{85.12\%} & \textbf{91.67\%} & \textbf{84.66\%} & 82.47\%          & \textbf{0.781} & 0.450          & \textbf{71.41\%} & \textbf{66.32\%} \\
\midrule
MindEye2(subj7)   & 0.170          & 0.408          & 80.70\%          & 85.90\%          & 74.90\%          & 74.29\%          & 0.854          & 0.504          & 64.44\%          & 37.77\% \\
Ours(subj7)       & \textbf{0.183} & 0.407          & \textbf{81.45\%} & \textbf{88.31\%} & \textbf{79.92\%} & \textbf{77.82\%} & \textbf{0.834} & \textbf{0.487} & 64.18\%          & \textbf{62.58\%} \\
7$\rightarrow$1   & \textbf{0.180} & 0.404          & \textbf{80.86\%} & \textbf{87.47\%} & \textbf{78.94\%} & \textbf{77.05\%} & \textbf{0.840} & \textbf{0.492} & \textbf{65.62\%} & \textbf{63.69\%} \\
7$\rightarrow$2   & \textbf{0.185} & 0.406          & \textbf{82.11\%} & \textbf{89.01\%} & \textbf{80.47\%} & \textbf{77.53\%} & \textbf{0.835} & \textbf{0.486} & 63.26\%          & \textbf{61.06\%} \\
7$\rightarrow$5   & \textbf{0.183} & \textbf{0.411} & \textbf{81.38\%} & \textbf{88.46\%} & \textbf{80.36\%} & \textbf{78.87\%} & \textbf{0.828} & \textbf{0.482} & 63.67\%          & \textbf{62.98\%} \\
\midrule
MindEye2(1 h)     & 0.195          & 0.419          & 84.2\%           & 90.6\%           & 81.2\%           & 79.2\%           & 0.810          & 0.468          & 79.0\%           & 57.4\% \\
Ours(1 h)         & \textbf{0.206} & 0.414          & \textbf{85.6\%}  & \textbf{91.6\%}  & \textbf{83.0\%}  & \textbf{81.2\%}  & \textbf{0.802} & \textbf{0.463} & \textbf{78.9\%}  & \textbf{75.3\%} \\
\bottomrule
\end{tabular}
}
\label{tab:apmethod_comparison}
\end{table*}


% \begin{table*}[!t]
% \centering
% \caption{Performance Comparison Across Methods using 1 sessions}
% \begin{tabular}{@{}lccccccccccc@{}}
% \toprule
% \textbf{Method} & \multicolumn{4}{c}{\textbf{Low-Level}} & \multicolumn{4}{c}{\textbf{High-Level}} & \multicolumn{2}{c}{\textbf{Retrieval}} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-11}
%  & \textbf{PixCorr$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{Alex(2)$\uparrow$} & \textbf{Alex(5)$\uparrow$} & \textbf{Incep$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{Eff$\downarrow$} & \textbf{SwAV$\downarrow$} & \textbf{Image$\uparrow$} & \textbf{Brain$\uparrow$} \\
% \midrule
% MindEye(subj01) & 0.235 & 0.428 & 88.02\% & 93.33\% & 83.56\% & 80.75\% & 0.798 & 0.459 & 93.96\% & 77.63\% \\
% average & 0.226 & 0.415 & \cellcolor{yellow} \textbf{88.19\%} & 93.26\% & 83.48\% & \cellcolor{yellow} \textbf{81.76\%} & 0.800 & \cellcolor{yellow} \textbf{0.459} & 90.90\% & \cellcolor{yellow} \textbf{86.88\%} \\
% 1-\>2 & 0.222 & 0.413 & \cellcolor{yellow} \textbf{88.09\%} & 93.28\% & \cellcolor{yellow} \textbf{84.01\%} & \cellcolor{yellow} \textbf{81.82\%} & \cellcolor{yellow} \textbf{0.796} & \cellcolor{yellow} \textbf{0.457} & 91.56\% & \cellcolor{yellow} \textbf{87.49\%} \\
% 1-\>5 & 0.227 & 0.416 & \cellcolor{yellow} \textbf{88.29\%} & \cellcolor{yellow} \textbf{93.36\%} & 83.54\% & \cellcolor{yellow} \textbf{80.94\%} & 0.803 & 0.461 & 89.76\% & \cellcolor{yellow} \textbf{85.78\%} \\
% 1-\>7 & 0.229 & 0.416 & \cellcolor{yellow} \textbf{88.18\%} & 93.13\% & 82.90\% & \cellcolor{yellow} \textbf{82.52\%} & 0.800 & \cellcolor{yellow} \textbf{0.458} & 91.37\% & \cellcolor{yellow} \textbf{87.36\%} \\
% \midrule
% MindEye(subj02) & 0.200 & 0.433 & 85.00\% & 92.13\% & 81.86\% & 79.39\% & 0.807 & 0.467 & 90.53\% & 67.18\% \\
% average & \cellcolor{yellow} \textbf{0.218} & 0.426 & \cellcolor{yellow} \textbf{88.08\%} & \cellcolor{yellow} \textbf{93.33\%} & \cellcolor{yellow} \textbf{84.13\%} & \cellcolor{yellow} \textbf{82.47\%} & \cellcolor{yellow} \textbf{0.791} & \cellcolor{yellow} \textbf{0.452} & 90.04\% & \cellcolor{yellow} \textbf{85.61\%} \\
% 2-\>1 & \cellcolor{yellow} \textbf{0.218} & 0.425 & \cellcolor{yellow} \textbf{88.36\%} & \cellcolor{yellow} \textbf{93.55\%} & \cellcolor{yellow} \textbf{84.17\%} & \cellcolor{yellow} \textbf{82.57\%} & \cellcolor{yellow} \textbf{0.794} & \cellcolor{yellow} \textbf{0.455} & 90.09\% & \cellcolor{yellow} \textbf{86.19\%} \\
% 2-\>5 & \cellcolor{yellow} \textbf{0.218} & 0.426 & \cellcolor{yellow} \textbf{87.88\%} & \cellcolor{yellow} \textbf{93.13\%} & \cellcolor{yellow} \textbf{83.39\%} & \cellcolor{yellow} \textbf{82.05\%} & \cellcolor{yellow} \textbf{0.793} & \cellcolor{yellow} \textbf{0.454} & 90.34\% & \cellcolor{yellow} \textbf{85.67\%} \\
% 2-\>7 & \cellcolor{yellow} \textbf{0.217} & 0.427 & \cellcolor{yellow} \textbf{88.00\%} & \cellcolor{yellow} \textbf{93.32\%} & \cellcolor{yellow} \textbf{84.83\%} & \cellcolor{yellow} \textbf{82.78\%} & \cellcolor{yellow} \textbf{0.785} & \cellcolor{yellow} \textbf{0.449} & 89.70\% & \cellcolor{yellow} \textbf{84.98\%} \\
% \midrule
% MindEye(subj05) & 0.175 & 0.405 & 83.11\% & 91.00\% & 84.33\% & 82.53\% & 0.781 & 0.444 & 66.94\% & 46.96\% \\
% average & \cellcolor{yellow} \textbf{0.197} & \cellcolor{yellow} \textbf{0.409} & \cellcolor{yellow} \textbf{84.69\%} & \cellcolor{yellow} \textbf{91.61\%} & \cellcolor{yellow} \textbf{84.63\%} & \cellcolor{yellow} \textbf{82.76\%} & 0.784 & 0.454 & \cellcolor{yellow} \textbf{70.62\%} & \cellcolor{yellow} \textbf{65.95\%} \\
% 5-\>1 & \cellcolor{yellow} \textbf{0.196} & \cellcolor{yellow} \textbf{0.405} & \cellcolor{yellow} \textbf{84.23\%} & \cellcolor{yellow} \textbf{91.28\%} & \cellcolor{yellow} \textbf{84.66\%} & \cellcolor{yellow} \textbf{82.93\%} & 0.787 & 0.459 & \cellcolor{yellow} \textbf{69.67\%} & \cellcolor{yellow} \textbf{65.14\%} \\
% 5-\>2 & \cellcolor{yellow} \textbf{0.196} & \cellcolor{yellow} \textbf{0.409} & \cellcolor{yellow} \textbf{84.71\%} & \cellcolor{yellow} \textbf{91.88\%} & \cellcolor{yellow} \textbf{84.56\%} & \cellcolor{yellow} \textbf{82.88\%} & 0.783 & 0.455 & \cellcolor{yellow} \textbf{70.78\%} & \cellcolor{yellow} \textbf{66.38\%} \\
% 5-\>7 & \cellcolor{yellow} \textbf{0.198} & \cellcolor{yellow} \textbf{0.412} & \cellcolor{yellow} \textbf{85.12\%} & \cellcolor{yellow} \textbf{91.67\%} & \cellcolor{yellow} \textbf{84.66\%} & 82.47\% & \cellcolor{yellow} \textbf{0.781} & 0.450 & \cellcolor{yellow} \textbf{71.41\%} & \cellcolor{yellow} \textbf{66.32\%} \\
% \midrule
% MindEye(subj07) & 0.170 & 0.408 & 80.70\% & 85.90\% & 74.90\% & 74.29\% & 0.854 & 0.504 & 64.44\% & 37.77\% \\
% average & \cellcolor{yellow} \textbf{0.183} & 0.407 & \cellcolor{yellow} \textbf{81.45\%} & \cellcolor{yellow} \textbf{88.31\%} & \cellcolor{yellow} \textbf{79.92\%} & \cellcolor{yellow} \textbf{77.82\%} & \cellcolor{yellow} \textbf{0.834} & \cellcolor{yellow} \textbf{0.487} & 64.18\% & \cellcolor{yellow} \textbf{62.58\%} \\
% 7-\>1 & \cellcolor{yellow} \textbf{0.180} & 0.404 & \cellcolor{yellow} \textbf{80.86\%} & \cellcolor{yellow} \textbf{87.47\%} & \cellcolor{yellow} \textbf{78.94\%} & \cellcolor{yellow} \textbf{77.05\%} & \cellcolor{yellow} \textbf{0.840} & \cellcolor{yellow} \textbf{0.492} & \cellcolor{yellow} \textbf{65.62\%} & \cellcolor{yellow} \textbf{63.69\%} \\
% 7-\>2 & \cellcolor{yellow} \textbf{0.185} & 0.406 & \cellcolor{yellow} \textbf{82.11\%} & \cellcolor{yellow} \textbf{89.01\%} & \cellcolor{yellow} \textbf{80.47\%} & \cellcolor{yellow} \textbf{77.53\%} & \cellcolor{yellow} \textbf{0.835} & \cellcolor{yellow} \textbf{0.486} & 63.26\% & \cellcolor{yellow} \textbf{61.06\%} \\
% 7-\>5 & \cellcolor{yellow} \textbf{0.183} & \cellcolor{yellow} \textbf{0.411} & \cellcolor{yellow} \textbf{81.38\%} & \cellcolor{yellow} \textbf{88.46\%} & \cellcolor{yellow} \textbf{80.36\%} & \cellcolor{yellow} \textbf{78.87\%} & \cellcolor{yellow} \textbf{0.828} & \cellcolor{yellow} \textbf{0.482} & 63.67\% & \cellcolor{yellow} \textbf{62.98\%} \\
% \midrule
% MindEye2(1h) & 0.195 & 0.419 & 84.2\% & 90.6\% & 81.2\% & 79.2\% & 0.810 & 0.468 & 79.0\% & 57.4\% \\
% Ours(1h) & \cellcolor{yellow} \textbf{0.206} & 0.414 & \cellcolor{yellow} \textbf{85.6\%} & \cellcolor{yellow} \textbf{91.6\%} & \cellcolor{yellow} \textbf{83.0\%} & \cellcolor{yellow} \textbf{81.2\%} & \cellcolor{yellow} \textbf{0.802} & \cellcolor{yellow} \textbf{0.463} & \cellcolor{yellow} \textbf{78.9\%} & \cellcolor{yellow} \textbf{75.3\%} \\
% \bottomrule
% \end{tabular}
% \label{tab:comparison_1session2}
% \end{table*}

% 涂黄版
% \begin{table*}[!t]
% \centering
% \caption{Performance Comparison Across Methods using 1 sessions}
% \begin{tabular}{@{}lccccccccccc@{}}
% \toprule
% \textbf{Method} & \multicolumn{4}{c}{\textbf{Low-Level}} & \multicolumn{4}{c}{\textbf{High-Level}} & \multicolumn{2}{c}{\textbf{Retrieval}} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-11}
%  & \textbf{PixCorr$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{Alex(2)$\uparrow$} & \textbf{Alex(5)$\uparrow$} & \textbf{Incep$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{Eff$\downarrow$} & \textbf{SwAV$\downarrow$} & \textbf{Image$\uparrow$} & \textbf{Brain$\uparrow$} \\
% \midrule
% MindEye(subj01) & 0.235 & 0.428 & 88.02\% & 93.33\% & 83.56\% & 80.75\% & 0.798 & 0.459 & 93.96\% & 77.63\% \\
% average & 0.226 & 0.415 & \cellcolor{yellow} 88.19\% & 93.26\% & 83.48\% & \cellcolor{yellow} 81.76\% & 0.800 & \cellcolor{yellow} 0.459 & 90.90\% & \cellcolor{yellow} 86.88\% \\
% 1-\>2 & 0.222 & 0.413 & \cellcolor{yellow} 88.09\% & 93.28\% & \cellcolor{yellow} 84.01\% & \cellcolor{yellow} 81.82\% & \cellcolor{yellow} 0.796 & \cellcolor{yellow} 0.457 & 91.56\% & \cellcolor{yellow} 87.49\% \\
% 1-\>5 & 0.227 & 0.416 & \cellcolor{yellow} 88.29\% & \cellcolor{yellow} 93.36\% & 83.54\% & \cellcolor{yellow} 80.94\% & 0.803 & 0.461 & 89.76\% & \cellcolor{yellow} 85.78\% \\
% 1-\>7 & 0.229 & 0.416 & \cellcolor{yellow} 88.18\% & 93.13\% & 82.90\% & \cellcolor{yellow} 82.52\% & 0.800 & \cellcolor{yellow} 0.458 & 91.37\% & \cellcolor{yellow} 87.36\% \\
% \midrule
% MindEye(subj02) & 0.200 & 0.433 & 85.00\% & 92.13\% & 81.86\% & 79.39\% & 0.807 & 0.467 & 90.53\% & 67.18\% \\
% average & \cellcolor{yellow} 0.218 & 0.426 & \cellcolor{yellow} 88.08\% & \cellcolor{yellow} 93.33\% & \cellcolor{yellow} 84.13\% & \cellcolor{yellow} 82.47\% & \cellcolor{yellow} 0.791 & \cellcolor{yellow} 0.452 & 90.04\% & \cellcolor{yellow} 85.61\% \\
% 2-\>1 & \cellcolor{yellow} 0.218 & 0.425 & \cellcolor{yellow} 88.36\% & \cellcolor{yellow} 93.55\% & \cellcolor{yellow} 84.17\% & \cellcolor{yellow} 82.57\% & \cellcolor{yellow} 0.794 & \cellcolor{yellow} 0.455 & 90.09\% & \cellcolor{yellow} 86.19\% \\
% 2-\>5 & \cellcolor{yellow} 0.218 & 0.426 & \cellcolor{yellow} 87.88\% & \cellcolor{yellow} 93.13\% & \cellcolor{yellow} 83.39\% & \cellcolor{yellow} 82.05\% & \cellcolor{yellow} 0.793 & \cellcolor{yellow} 0.454 & 90.34\% & \cellcolor{yellow} 85.67\% \\
% 2-\>7 & \cellcolor{yellow} 0.217 & 0.427 & \cellcolor{yellow} 88.00\% & \cellcolor{yellow} 93.32\% & \cellcolor{yellow} 84.83\% & \cellcolor{yellow} 82.78\% & \cellcolor{yellow} 0.785 & \cellcolor{yellow} 0.449 & 89.70\% & \cellcolor{yellow} 84.98\% \\
% \midrule
% MindEye(subj05) & 0.175 & 0.405 & 83.11\% & 91.00\% & 84.33\% & 82.53\% & 0.781 & 0.444 & 66.94\% & 46.96\% \\
% average & \cellcolor{yellow} 0.197 & \cellcolor{yellow} 0.409 & \cellcolor{yellow} 84.69\% & \cellcolor{yellow} 91.61\% & \cellcolor{yellow} 84.63\% & \cellcolor{yellow} 82.76\% & 0.784 & 0.454 & \cellcolor{yellow} 70.62\% & \cellcolor{yellow} 65.95\% \\
% 5-\>1 & \cellcolor{yellow} 0.196 & \cellcolor{yellow} 0.405 & \cellcolor{yellow} 84.23\% & \cellcolor{yellow} 91.28\% & \cellcolor{yellow} 84.66\% & \cellcolor{yellow} 82.93\% & 0.787 & 0.459 & \cellcolor{yellow} 69.67\% & \cellcolor{yellow} 65.14\% \\
% 5-\>2 & \cellcolor{yellow} 0.196 & \cellcolor{yellow} 0.409 & \cellcolor{yellow} 84.71\% & \cellcolor{yellow} 91.88\% & \cellcolor{yellow} 84.56\% & \cellcolor{yellow} 82.88\% & 0.783 & 0.455 & \cellcolor{yellow} 70.78\% & \cellcolor{yellow} 66.38\% \\
% 5-\>7 & \cellcolor{yellow} 0.198 & \cellcolor{yellow} 0.412 & \cellcolor{yellow} 85.12\% & \cellcolor{yellow} 91.67\% & \cellcolor{yellow} 84.66\% & 82.47\% & \cellcolor{yellow} 0.781 & 0.450 & \cellcolor{yellow} 71.41\% & \cellcolor{yellow} 66.32\% \\
% \midrule
% MindEye(subj07) & 0.170 & 0.408 & 80.70\% & 85.90\% & 74.90\% & 74.29\% & 0.854 & 0.504 & 64.44\% & 37.77\% \\
% average & \cellcolor{yellow} 0.183 & 0.407 & \cellcolor{yellow} 81.45\% & \cellcolor{yellow} 88.31\% & \cellcolor{yellow} 79.92\% & \cellcolor{yellow} 77.82\% & \cellcolor{yellow} 0.834 & \cellcolor{yellow} 0.487 & 64.18\% & \cellcolor{yellow} 62.58\% \\
% 7-\>1 & \cellcolor{yellow} 0.180 & 0.404 & \cellcolor{yellow} 80.86\% & \cellcolor{yellow} 87.47\% & \cellcolor{yellow} 78.94\% & \cellcolor{yellow} 77.05\% & \cellcolor{yellow} 0.840 & \cellcolor{yellow} 0.492 & \cellcolor{yellow} 65.62\% & \cellcolor{yellow} 63.69\% \\
% 7-\>2 & \cellcolor{yellow} 0.185 & 0.406 & \cellcolor{yellow} 82.11\% & \cellcolor{yellow} 89.01\% & \cellcolor{yellow} 80.47\% & \cellcolor{yellow} 77.53\% & \cellcolor{yellow} 0.835 & \cellcolor{yellow} 0.486 & 63.26\% & \cellcolor{yellow} 61.06\% \\
% 7-\>5 & \cellcolor{yellow} 0.183 & \cellcolor{yellow} 0.411 & \cellcolor{yellow} 81.38\% & \cellcolor{yellow} 88.46\% & \cellcolor{yellow} 80.36\% & \cellcolor{yellow} 78.87\% & \cellcolor{yellow} 0.828 & \cellcolor{yellow} 0.482 & 63.67\% & \cellcolor{yellow} 62.98\% \\
% \midrule
% MindEye2(1h) & 0.195 & 0.419 & 84.2\% & 90.6\% & 81.2\% & 79.2\% & 0.810 & 0.468 & 79.0\% & 57.4\% \\
% Ours(1h) & \cellcolor{yellow} 0.206 & 0.414 & \cellcolor{yellow} 85.6\% & \cellcolor{yellow} 91.6\% & \cellcolor{yellow} 83.0\% & \cellcolor{yellow} 81.2\% & \cellcolor{yellow} 0.802 & \cellcolor{yellow} 0.463 & \cellcolor{yellow} 78.9\% & \cellcolor{yellow} 75.3\% \\
% \bottomrule
% \end{tabular}
% \label{tab:comparison_1session1}
% \end{table*}


% \begin{table*}[!t]
% \centering
% \caption{Performance of our model using 40 sessions}
% \begin{tabular}{@{}lccccccccccc@{}}
% \toprule
% \textbf{Method} & \multicolumn{4}{c}{\textbf{Low-Level}} & \multicolumn{4}{c}{\textbf{High-Level}} & \multicolumn{2}{c}{\textbf{Retrieval}} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-11}
%  & \textbf{PixCorr$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{Alex(2)$\uparrow$} & \textbf{Alex(5)$\uparrow$} & \textbf{Incep$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{Eff$\downarrow$} & \textbf{SwAV$\downarrow$} & \textbf{Image$\uparrow$} & \textbf{Brain$\uparrow$} \\
% \midrule
% average & 0.268 & 0.416 & 91.07\% & 95.81\% & 90.25\% & 87.72\% & 0.713 & 0.407 & 98.84\% & 97.82\% \\
% 1-\>2   & 0.262 & 0.414 & 90.77\% & 95.58\% & 90.13\% & 87.73\% & 0.712 & 0.408 & 98.98\% & 98.19\% \\
% 1-\>5   & 0.273 & 0.417 & 91.63\% & 96.29\% & 90.62\% & 87.81\% & 0.709 & 0.401 & 98.84\% & 97.24\% \\
% 1-\>7   & 0.270 & 0.417 & 90.82\% & 95.57\% & 89.99\% & 87.61\% & 0.717 & 0.413 & 98.70\% & 98.02\% \\
% \midrule
% average & 0.259 & 0.413 & 90.21\% & 95.21\% & 89.82\% & 86.89\% & 0.720 & 0.417 & 98.19\% & 97.35\% \\
% 2-\>1   & 0.260 & 0.412 & 90.20\% & 95.42\% & 89.29\% & 87.12\% & 0.719 & 0.417 & 98.20\% & 97.00\% \\
% 2-\>5   & 0.260 & 0.412 & 89.65\% & 94.61\% & 90.12\% & 86.66\% & 0.720 & 0.421 & 98.54\% & 98.13\% \\
% 2-\>7   & 0.256 & 0.415 & 90.77\% & 95.59\% & 90.05\% & 86.90\% & 0.720 & 0.414 & 97.83\% & 96.93\% \\
% \midrule
% average & 0.221 & 0.403 & 87.64\% & 94.59\% & 89.36\% & 87.38\% & 0.728 & 0.419 & 89.64\% & 86.61\% \\
% 5-\>1   & 0.223 & 0.406 & 88.09\% & 94.90\% & 89.17\% & 87.07\% & 0.732 & 0.421 & 89.54\% & 85.29\% \\
% 5-\>2   & 0.221 & 0.401 & 87.36\% & 94.57\% & 89.09\% & 87.70\% & 0.725 & 0.417 & 89.36\% & 86.73\% \\
% 5-\>7   & 0.218 & 0.402 & 87.46\% & 94.29\% & 89.83\% & 87.36\% & 0.729 & 0.420 & 90.03\% & 87.82\% \\
% \midrule
% average & 0.222 & 0.401 & 87.00\% & 92.76\% & 87.21\% & 85.86\% & 0.739 & 0.429 & 88.64\% & 88.03\% \\
% 7-\>1   & 0.221 & 0.400 & 87.44\% & 93.09\% & 87.99\% & 86.10\% & 0.736 & 0.427 & 89.40\% & 89.37\% \\
% 7-\>2   & 0.224 & 0.401 & 88.37\% & 94.08\% & 88.72\% & 86.96\% & 0.733 & 0.420 & 88.30\% & 86.70\% \\
% 7-\>5   & 0.221 & 0.403 & 85.20\% & 91.10\% & 84.93\% & 84.53\% & 0.749 & 0.441 & 88.23\% & 88.02\% \\
% \bottomrule
% \end{tabular}
% \label{tab:comparison_40session}
% \end{table*}






% \begin{table*}[ht]
% \centering
% \caption{\textcolor{blue}{Few-shot Performance Comparison}}
% \begin{tabular}{@{}llccccccccc@{}}
% \toprule
% \textbf{Method}  & \textbf{Data} & \multicolumn{4}{c}{\textbf{Low-Level}} & \multicolumn{4}{c}{\textbf{High-Level}}\\
% \cmidrule(lr){3-6} \cmidrule(lr){7-10}
%  & & \textbf{PixCorr$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{Alex(2)$\uparrow$} & \textbf{Alex(5)$\uparrow$} & \textbf{Incep$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{Eff$\downarrow$} & \textbf{SwAV$\downarrow$} \\
% \midrule
% MindEye & 1-shot & .047 & .213 & 62.3\% & 68.9\% & 61.5\% & 66.4\% & .927 & .601 \\
% MindShot (Ours) & 1-shot & .097 & .260 & 73.8\% & 81.9\% & 73.7\% & 77.1\% & .853 & .601 \\
% \midrule
% MindEye & 2-shot & .069 & .237 & 68.5\% & 75.8\% & 67.5\% & 72.1\% & .896 & .574\\
% MindShot (Ours) & 2-shot & .117 & .262 & 76.9\% & 85.9\% & 78.8\% & .816\% & .499 & .601 \\
% \midrule
% MindEye & 3-shot & .079 & .249 & 71.1\% & 78.9\% & 70.2\% & 75.0\% & .879 & .555\\
% MindShot (Ours) & 3-shot & .122 & .267 & 78.8\% & 87.9\% & 81.6\% & 83.5\% & .794 & .482\\
% \bottomrule
% \end{tabular}
% \label{tab:mindeye_comparison}
% \end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

% 暂时用不到的实验结果
% 40 session 微调结果


\begin{table*}[!t]
\centering
\caption{Performance of our model using 40 sessions}
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
\textbf{Method} & \multicolumn{4}{c}{\textbf{Low-Level}} & \multicolumn{4}{c}{\textbf{High-Level}} & \multicolumn{2}{c}{\textbf{Retrieval}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-11}
 & \textbf{PixCorr$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{Alex(2)$\uparrow$} & \textbf{Alex(5)$\uparrow$} & \textbf{Incep$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{Eff$\downarrow$} & \textbf{SwAV$\downarrow$} & \textbf{Image$\uparrow$} & \textbf{Brain$\uparrow$} \\
\midrule
average & 0.268 & 0.416 & 91.07\% & 95.81\% & 90.25\% & 87.72\% & 0.713 & 0.407 & 98.84\% & 97.82\% \\
1-\>2   & 0.262 & 0.414 & 90.77\% & 95.58\% & 90.13\% & 87.73\% & 0.712 & 0.408 & 98.98\% & 98.19\% \\
1-\>5   & 0.273 & 0.417 & 91.63\% & 96.29\% & 90.62\% & 87.81\% & 0.709 & 0.401 & 98.84\% & 97.24\% \\
1-\>7   & 0.270 & 0.417 & 90.82\% & 95.57\% & 89.99\% & 87.61\% & 0.717 & 0.413 & 98.70\% & 98.02\% \\
\midrule
average & 0.259 & 0.413 & 90.21\% & 95.21\% & 89.82\% & 86.89\% & 0.720 & 0.417 & 98.19\% & 97.35\% \\
2-\>1   & 0.260 & 0.412 & 90.20\% & 95.42\% & 89.29\% & 87.12\% & 0.719 & 0.417 & 98.20\% & 97.00\% \\
2-\>5   & 0.260 & 0.412 & 89.65\% & 94.61\% & 90.12\% & 86.66\% & 0.720 & 0.421 & 98.54\% & 98.13\% \\
2-\>7   & 0.256 & 0.415 & 90.77\% & 95.59\% & 90.05\% & 86.90\% & 0.720 & 0.414 & 97.83\% & 96.93\% \\
\midrule
average & 0.221 & 0.403 & 87.64\% & 94.59\% & 89.36\% & 87.38\% & 0.728 & 0.419 & 89.64\% & 86.61\% \\
5-\>1   & 0.223 & 0.406 & 88.09\% & 94.90\% & 89.17\% & 87.07\% & 0.732 & 0.421 & 89.54\% & 85.29\% \\
5-\>2   & 0.221 & 0.401 & 87.36\% & 94.57\% & 89.09\% & 87.70\% & 0.725 & 0.417 & 89.36\% & 86.73\% \\
5-\>7   & 0.218 & 0.402 & 87.46\% & 94.29\% & 89.83\% & 87.36\% & 0.729 & 0.420 & 90.03\% & 87.82\% \\
\midrule
average & 0.222 & 0.401 & 87.00\% & 92.76\% & 87.21\% & 85.86\% & 0.739 & 0.429 & 88.64\% & 88.03\% \\
7-\>1   & 0.221 & 0.400 & 87.44\% & 93.09\% & 87.99\% & 86.10\% & 0.736 & 0.427 & 89.40\% & 89.37\% \\
7-\>2   & 0.224 & 0.401 & 88.37\% & 94.08\% & 88.72\% & 86.96\% & 0.733 & 0.420 & 88.30\% & 86.70\% \\
7-\>5   & 0.221 & 0.403 & 85.20\% & 91.10\% & 84.93\% & 84.53\% & 0.749 & 0.441 & 88.23\% & 88.02\% \\
\bottomrule
\end{tabular}
\label{tab:comparison_40session}
\end{table*}

% MindTuner 的对比结果
% \begin{table*}[!t]
% \centering
% \caption{\textcolor{blue}{Additional Full Session Results}}
% \begin{tabular}{@{}lcccccccccc@{}}
% \toprule
% \textbf{Method} & \multicolumn{4}{c}{\textbf{Low-Level}} & \multicolumn{4}{c}{\textbf{High-Level}} & \multicolumn{2}{c}{\textbf{Retrieval}} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-11}
%  & \textbf{PixCorr$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{Alex(2)$\uparrow$} & \textbf{Alex(5)$\uparrow$} & \textbf{Incep$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{Eff$\downarrow$} & \textbf{SwAV$\downarrow$} & \textbf{Image$\uparrow$} & \textbf{Brain$\uparrow$} \\
% \midrule
% Takagi... & 0.246 & 0.410 & 78.9\% & 85.6\% & 83.8\% & 82.1\% & 0.811 & 0.504 & - & - \\
% Ozcelik... & 0.273 & 0.365 & 94.4\% & 96.6\% & 91.3\% & 90.9\% & 0.728 & 0.422 & 18.8\% & 26.3\%  \\
% MindEye1 & 0.319 & 0.360 & 92.8\% & 96.9\% & 94.6\% & 93.3\% & 0.648 & 0.377 & 90.0\% & 84.1\%  \\
% MindEye2 & 0.322 & 0.431 & 96.1\% & 98.6\% & 95.4\% & 93.0\% & 0.619 & 0.344 & 98.8\% & 98.3\%  \\
% Ours & 0.322 & 0.421 & 95.8\% & 98.8\% & 95.6\% & 93.8\% & 0.612 & 0.340 & 98.9\% & 98.3\% \\
% \midrule
% MindEye2(1 hour) & 0.195 & 0.419 & 84.2\% & 85.6\% & 81.2\% & 79.2\% & 0.810 & 0.468 & 79.0\% & 57.4\%  \\
% Ours(1 hour) & \textbf{0.224} & \textbf{0.420} & \textbf{87.8\%} & \textbf{93.6\%} & \textbf{84.8\%} & \textbf{83.5\%} & \textbf{0.780} & \textbf{0.440} & \textbf{83.1\%} & \textbf{76.0\%} \\
% \bottomrule
% \end{tabular}
% \end{table*}