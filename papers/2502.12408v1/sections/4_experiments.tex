\section{Experiments}\label{sec:experiments}
In this section, we present the experimental setup to evaluate our ASR metrics approximation tool. We describe the datasets, models, and regression pipeline used in our experiments, highlighting the diversity of ASR systems and testing conditions. 

\subsection{Datasets}\label{subsec:datasets}
To evaluate the robustness and generalizability of our ASR metrics approximation tool, we use datasets sourced from multiple distributions, divided into two types: \textbf{Standard Benchmark} and \textbf{Wild Challenge} datasets. Below we describe the datasets and provide additional details in Appendix~\ref{appsubsec:datasets} Table~\ref{tab:list_of_datasets}.


\noindent\textbf{Standard Benchmark Datasets.} We include widely used datasets representing diverse domains and acoustic conditions. \textit{LibriSpeech}~\cite{panayotov2015librispeech} provides 1,000 hours of English read audiobooks, covering both clean and noisy conditions. \textit{TED-LIUM}~\cite{rousseau2014tedlium} consists of TED talks from 2,000 speakers. \textit{GigaSpeech}~\cite{chen2021gigaspeech} spans audiobooks, podcasts, and YouTube, incorporating both read and spontaneous speech. \textit{SPGISpeech}~\cite{kensho2021spgispeech} features 5,000 hours of earnings calls with a focus on orthographic accuracy. \textit{Common Voice}~\cite{ardila2020common} is a multilingual, crowdsourced corpus with diverse accents. \textit{Earnings22}~\cite{rio2022earnings} provides 119 hours of accented, real-world earnings calls. Additional datasets include \textit{AMI (IHM)}~\cite{ami2005corpus}, with 100 hours of English meeting recordings from non-native speakers, and \textit{People's Speech}~\cite{galvez2021peoplesspeechlargescalediverse}, emphasizing inclusivity and linguistic diversity. \textit{SLUE-VoXCeleb}~\cite{shon2022slue} contains conversational voice snippets, capturing diverse speaking styles and emotions.

\noindent\textbf{Wild Datasets.} The wild set focuses on real-world variability and challenging scenarios. \textit{Primock57}~\cite{papadopoulos-korfiatis-etal-2022-primock57} includes telemedicine consultations with diverse accents, ages, and scenarios, recorded by clinicians and actors. \textit{VoxPopuli Accented}~\cite{wang-etal-2021-voxpopuli} contains multilingual speeches from European Parliament recordings, rich in named entities. \textit{ATCOsim}~\cite{jan_van_doorn_2023} features 10 hours of non-native English speech from air traffic control simulations with clean utterance-level transcriptions. Additionally, we include a noisy subset of \textit{LibriSpeech}~\cite{panayotov2015librispeech}, which reflects challenging real-world conditions.


\subsection{Models}\label{subsec:models}

We evaluate our approximation framework for a range of state-of-the-art ASR models, put into three categories based on their architecture and functionality. Below we describe the datasets and provide additional details in Appendix~\ref{appsubsec:datasets} and in Table~\ref{tab:list_of_models}.

\noindent\textbf{Encoder-Decoder Models.}  
We include multiple encoder-decoder families of models capable of performing ASR tasks in a zero-shot setting. More specifically, we include \texttt{whisper}~\cite{radford2023robust} and \texttt{distil-whisper}~\cite{gandhi2023distilwhisper} models which perform really well across diverse testing settings. We also include \texttt{seamless }~\cite{communication2023seamlessm4tmassivelymultilingual, seamless2023, Barrault2025}, \texttt{SpeechT5}~\cite{ao2022speecht5unifiedmodalencoderdecoderpretraining} which are unified encoder-decoder framework for tasks such as ASR, speech synthesis, translation, and voice conversion. \texttt{MMS}~\cite{pratap2023scalingspeechtechnology1000} supports hundreds of languages and excels in resource-constrained scenarios. \textit{Moonshine (2)}~\cite{jeffries2024moonshinespeechrecognitionlive}, a lightweight and efficient model, is designed for edge deployments with strong performance.


\noindent\textbf{NeMo-ASR Models.}  
We use multiple models from the \texttt{NeMo-ASR}~\cite{gulati2020conformerconvolutionaugmentedtransformerspeech, variani2020hybridautoregressivetransducerhat, noroozi2024statefulconformercachebasedinference, tang2023hybridtransducerattentionbased, Harper_NeMo_a_toolkit} toolkit by NVIDIA. We include models such as Canary and Parakeet, which use highly efficient speech encoders like Fast-Conformer~\cite{rekesh2023fastconformerlinearlyscalable}. In addition to that, we use models based on various encoders, and decoders (CTC, RNN-T, TDT, Conformer-CTC~\cite{guo2021multispeakerasrcombiningnonautoregressive}. In our work, we evaluate 11 models from the NeMo-ASR toolkit.




\noindent\textbf{Encoder-Only and Decoder-Only Models.}  
We include self-supervised encoder-only models and their derivatives, as well as decoder-only models like SpeechLLM. Specifically, we use \texttt{Wav2Vec2}~\cite{schneider2019wav2vecunsupervisedpretrainingspeech, baevski2020wav2vec20frameworkselfsupervised}, \textit{HuBERT}~\cite{hsu2021hubertselfsupervisedspeechrepresentation}, and \textit{Data2Vec}~\cite{baevski2022data2vecgeneralframeworkselfsupervised}. Additionally, we include speech language models like \textit{SpeechLLM}~\cite{Rajaa_SpeechLLM_Multi-Modal_LLM}, which combines speech embeddings with language models to predict metadata such as speaker attributes, emotions, and accents, offering robust multimodal capabilities.




\subsection{Experimental Setup}

We evaluate all models listed in Section~\ref{subsec:models} on $1000$ examples sampled randomly from the $test$ split of each dataset, as described in Section~\ref{subsec:datasets}. Since all models are trained at a 16 kHz sampling rate, we (re)sample the speech accordingly. For ASR, we use greedy decoding and all other parameters are default unless otherwise specified. We apply basic text post-processing~\footnote{\url{https://bit.ly/enormwhisper}} before computing ASR metrics. We obtain all models from \texttt{Huggingface Hub}~\footnote{\url{https://huggingface.co/models}} and implement the ASR pipeline using the Transformers~\cite{wolf2020huggingfacestransformersstateoftheartnatural} library.

For multimodal embeddings, we use SONAR~\cite{duquenne2023sonar}, a 1024-dimensional sentence-level multilingual embedding model. Specifically, we utilize \texttt{text\_sonar\_basic\_encoder} for text encoding and \texttt{speech\_sonar\_basic\_encoder} for speech encoding. These encoders provide unified representations.

The regression framework uses a stacking ensemble with base regressors and a final estimator. Hyperparameter tuning is performed with \texttt{RandomizedSearchCV} to minimize MAE. The model is trained on 9 benchmark datasets and evaluated on the remaining benchmark dataset and {\nwilds}~in-the-wild datasets. This process is repeated for all 10 benchmark datasets. Additional details of the regression pipeline are provided in Section~\ref{sec:methodology} and low-level details in Appendix~\ref{appsubsubsec:regression_pipeline}. 

We conduct ASR experiments on a single A100/H100 GPU, while the regression model training runs on CPUs. Although ASR time and memory consumption depend on the model size, embedding extraction for $1000$ audio-text pairs takes approximately one minute on a single consumer-grade GPU without parallelization or additional efficiency measures. Appendix~\ref{appsubsec:experiments} provides further experimental setup details.



\noindent\textbf{Baselines.} 
Recent studies directly aligned with our approach are limited. For instance, eWER~\cite{ali-renals-2018-word} and eWER2~\cite{Ali2020WordER} estimate error rates based on the input signal, which differs from our approach. In contrast, we incorporate the model's output transcript into the error rate approximation function. The most closely related recent works are WERBERT~\cite{sheshadri-etal-2021-wer} and eWER3~\cite{chowdhury2023multilingualworderrorrate}, which share a similar pipeline. Both use encoders for text, speech, and other data, followed by a regression model trained in an end-to-end setting. Since eWER3 is the more recent of the two, we use it as our baseline. In eWER3, the speech encoder is \emph{wav2vec2}~\cite{baevski2020wav2vec20frameworkselfsupervised}, and the text encoder is \emph{roberta-base}~\cite{liu2019robertarobustlyoptimizedbert}, with a regression model trained on top while both encoders remain frozen. Given the unavailability of public code or pretrained models for evaluation, we implement eWER3 with some modifications to ensure a fair comparison. Specifically, we extract features from both encoders and apply PCA for dimensionality reduction on each modality before training our regression pipeline. For both speech and text, we experiment with 32 and 64 PCA components (referred to as $nc$ in Table~\ref{tab:ablation_baseline_results}).

