\section{Related Work}\label{sec:related_work}
Automatic speech recognition (ASR) has seen remarkable progress in recent years, driven by advances in deep learning and the availability of extensive training datasets~\cite{radford2022robustspeechrecognitionlargescale, communication2023seamlessm4tmassivelymultilingual}. Transformer~\cite{vaswani2023attentionneed} based models, in particular, have significantly contributed to these developments by effectively capturing long-range dependencies and contextual nuances in speech, achieving state-of-the-art (SOTA) performance across diverse benchmarks~\cite{kheddar2024automatic,dhanjal2024comprehensive, zimerman2023long}. While traditional evaluation metrics like Word Error Rate (WER) and Character Error Rate (CER) are de-facto evaluation metrics in benchmarking ASR systems~\cite{lin2021speech, park2024character}, scenarios where ground truth transcriptions are unavailable have caught interest in reference-free ASR evaluation methods~\cite{karbasi2022asr, wang2024no, kuhn2024measuring}.

Reference-free ASR evaluation methods aim to estimate ASR performance without requiring ground truth transcriptions~\cite{ospanov2024towards}. Earlier approaches rely on heuristic features or metadata such as speaker demographics, background noise, and linguistic characteristics \cite{litman-etal-2000-predicting, yoon10b_interspeech}, limiting their applicability across varied contexts. However, recent advancements focus on deep learning-based frameworks, such as convolutional neural networks (CNNs)~\cite{elloumi2018asrperformancepredictionunseen} and contrastive learning methhods~\cite{yuksel2023referencelessqualitymetricautomatic}, to predict ASR quality directly from encoded speech and text. For instance, methods like NoRefER~\cite{yuksel2023noreferreferencelessqualitymetric} employ Siamese architectures fine-tuned on ASR hypotheses, achieving high correlation with traditional metrics and improving WER by optimizing hypothesis ensembling \cite{park2024character}.

Efforts to approximate ASR metrics explore hybrid approaches that combine traditional and reference-free methods, such as leveraging word confidence scores, linguistic embeddings, or post-processing adaptations to estimate WER and CER without explicit references~\cite{Ali2020WordER, ali-renals-2018-word, kuhn2024measuring, negri-etal-2014-quality}. However, these approaches often suffer from reliance on specific ASR models or domain characteristics, limiting their generalizability. Unlike existing methods, our work addresses these limitations by introducing a robust, model and data-agnostic framework that evaluates ASR outputs across diverse datasets and configurations, emphasizing adaptability to unseen domains and variations. 

% In summary, while reference-free evaluation and metric approximation methods have advanced significantly, they remain constrained by domain specificity and heuristic dependencies. Our work bridges these critical gaps by proposing a generalized, scalable approach to ASR performance estimation, enabling reliable evaluation in diverse and resource-limited settings.



