\section{Introduction}

Automatic Speech Recognition (ASR) models have made significant advancements in recent years, achieving near-human performance on several standard evaluation benchmarks~\citep[\textit{inter alia}]{radford2022robustspeechrecognitionlargescale, seamless2023, communication2023seamlessm4tmassivelymultilingual, Harper_NeMo_a_toolkit}. These models are typically evaluated using metrics like Word Error Rate (WER) and Character Error Rate (CER)~\cite{likhomanenko2020rethinking}, which are essential for assessing model performance.

However, these metrics are dependent on ground truths, which are often scarce in resource-constrained environments, and human labeling is both costly and time-consuming. To mitigate this challenge, several reference-free evaluation methods are proposed~\cite{yuksel2023noreferreferencelessqualitymetric, 7178922, Swarup2019, 9413966, 8325512, 5947648}. While these approaches eliminate the reliance on labeled data, they primarily offer relative assessments of transcription quality, rather than providing precise error counts or rates. As a result, their applicability in real-world scenarios, where actionable performance metrics are crucial for further model refinement and deployment, is limited.

Given the limitations of both methods, approximating ASR metrics has emerged as a promising alternative for label-free evaluation~\cite{chowdhury2023multilingualworderrorrate, sheshadri2021werbertautomaticwerestimation, ali-renals-2018-word}. This approach typically involves training regression~\cite{jalalvand-etal-2016-transcrater} and/or classification models~\cite{sheshadri-etal-2021-wer} on top of speech and text encoders. While this method offers a close approximation of error metrics, several important questions remain unresolved. Specifically, an approximation model trained on dataset sampled from \(D\) to predict ASR metrics for a source model \(M\) must be evaluated under diverse conditions: 1) on test data that is IID (independent and identically distributed) sampled from \(D\); 2) on out-of-distribution (OOD) data representing diverse domains and recording conditions; 3) on IID data, but transcription from a target model \(T\); and 4) on OOD data with transcriptions from a target model \(T\). Most prior works~\cite{chowdhury2023multilingualworderrorrate, sheshadri2021werbertautomaticwerestimation} focus primarily on the first condition. Moreover, recent advancements in multimodal foundation models offer new opportunities to directly train regression models on unified speech and text embeddings.

To address these critical research gaps, we propose a novel framework for approximating the performance of a wide range of ASR models, both on standard benchmarks and in-the-wild scenarios. Specifically, we compute the similarity between speech and text embeddings in a unified space, capturing the semantic alignment between the two modalities. Additionally, we incorporate a high-quality reference model as a proxy, based on the intuition that agreement with a reliable proxy correlates with transcription quality, as shown in prior works~\cite{waheed2025udistilwhisperlabelfreedatafiltering}. These features are then used to train a regression model to predict key ASR metrics, such as WER, CER, and absolute word and character error counts.

In summary, our work represents one of the most comprehensive studies to date on approximating ASR metrics at scale, in terms of both data and model coverage. Our proposed approach serves as a reference-free evaluation particularly suited for label-scarce scenarios. Beyond evaluation, our method is especially valuable for tasks such as pseudo-labeling, where high-quality transcriptions are essential for downstream applications like knowledge distillation~\cite{waheed-etal-2024-distill, gandhi2023distilwhisper}.

Our contributions are as follows:
\begin{itemize}
    \item We evaluate over 40 ASR models across 14 diverse evaluation setups, including both standard benchmarks and domain-specific, unseen conditions followed by training regression models to approximate ASR metrics.
    \item We compare our approach with the most recent work on approximating ASR metrics and demonstrate over a 100\% improvement against the strong baseline.
    \item We conduct a rigorous ablation study to analyze the impact of different experimental configurations, providing deeper insights into the robustness of our approach. Our findings show that our method is resilient to diverse evaluation setups and requires only a small amount of training data.
\end{itemize}

\noindent\textbf{Outline.} The remainder of this paper is organized as follows: Section~\ref{sec:related_work} reviews related work. Section~\ref{sec:methodology} presents our proposed methodology. Sections~\ref{sec:experiments} and~\ref{sec:results} detail our experimental setup, results, and ablation study, respectively. Section~\ref{sec:conclusion} concludes the paper and outlines future directions.

\noindent\textbf{Reproducibility.} We are committed to making all code, data, and logs available upon acceptance.