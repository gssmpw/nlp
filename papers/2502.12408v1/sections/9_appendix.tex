\clearpage
\section{Appendix}\label{appsec:appendix}
\subsection{Methodology}\label{appsubsec:methodology}




\begin{align}
    \text{pWER}(x_{\text{text}}, x_{\text{proxy}}) &= \frac{\text{EditDistance}(x_{\text{text}}, x_{\text{proxy}})}{\text{WordCount}(x_{\text{proxy}})} \\
    \text{pCER}(x_{\text{text}}, x_{\text{proxy}}) &= \frac{\text{EditDistance}(x_{\text{text}}, x_{\text{proxy}})}{\text{CharCount}(x_{\text{proxy}})}
\end{align}

\subsection{Datasets}\label{appsubsec:datasets}

To evaluate the robustness and generalizability of our ASR metrics approximation tool, data were sourced from multiple repositories, which we divided into two distinct groups: Standard Benchmark and Wild Challenge dataset. 
\subsubsection{Standard Benchmark Datasets} 

There are six datasets in total that fall under the benchmark group. These datasets are categorized based on their frequent use in ASR model training and their representation of commonly encountered domains in real-world applications. 

\noindent\textbf{LibriSpeech~\cite{panayotov2015librispeech}.} prioritized speaker and content balance over explicit consideration of speech characteristics. It comprises approximately 1000 hours of English read audiobooks, with subsets featuring both clean and noisy speech conditions to simulate different acoustic environments. While the dataset covers diverse subject matter, its focus on formal, clear speech from public domain books means it lacks the natural variability of spontaneous speech, limiting its representation of conversational or informal dialogue.  \\
\noindent\textbf{TED-LIUM~\cite{rousseau2014tedlium}.} contains TED Talks totaling 452 hours of English speech data from approximately 2,000 speakers, recorded in close-talk microphone conditions. The corpus features narrated speaking styles, capturing clear and articulate speech. While it provides non-orthographic transcriptions, lacking formatting such as capitalization and punctuation, it remains a valuable resource for training and benchmarking automatic speech recognition (ASR) models.\\
\noindent\textbf{GigaSpeech~\cite{chen2021gigaspeech}.} is a multi-domain, multi-style speech recognition corpus incorporating diverse acoustic and linguistic conditions. It sources audio from three primary domains: audiobooks, podcasts, and YouTube, covering a wide range of speaking styles, including both read and spontaneous speech. The dataset covers a broad spectrum of topics, such as arts, science, sports, and more, making it highly versatile for training robust speech recognition models. \\
\noindent\textbf{SPGISpeech~\cite{kensho2021spgispeech}.} contains 5,000 hours of professionally transcribed audio from corporate earnings calls, featuring both spontaneous and narrated speaking styles. It emphasizes orthographic accuracy, providing fully formatted text with capitalization, punctuation, and denormalization of non-standard words. \\
\noindent\textbf{Common Voice~\cite{ardila2020common}.} (a multilingual corpus of narrated prompts built through crowdsourcing. Recorded in teleconference conditions, the corpus features narrated speaking styles and emphasizes inclusivity by covering a wide range of accents and languages, including low-resource ones. \\
\noindent\textbf{Earnings22~\cite{rio2022earnings}.} is a 119-hour corpus of English-language earnings calls from global companies, designed to address the lack of real-world, accented speech data in ASR benchmarking\\

\noindent\textbf{AMI (IHM)~\cite{ami2005corpus}.} The AMI Meeting Corpus is a 100-hour dataset of English meeting recordings, featuring multimodal data synchronized across close-talking and far-field microphones, room-view and individual cameras, slide projectors, and whiteboards. It includes mostly non-native speakers recorded in three rooms with varying acoustics. Digital pens capture unsynchronized handwritten notes, supporting research in speech recognition, diarization, and multimodal interaction. Available under edinburghcstr/ami, it is widely used for meeting analysis and speech processing studies. \\

\noindent\textbf{People's Speech~\cite{galvez2021peoplesspeechlargescalediverse}.}  Thousands of hours of labeled speech data collected from diverse speakers, covering a wide range of topics, accents, and speaking styles. The dataset emphasizes inclusivity and linguistic diversity, making it suitable for developing robust and generalized speech models. It is widely used in academic and industrial research to advance the state-of-the-art in automatic speech recognition (ASR) and other speech-related applications. \\

\noindent\textbf{SLUE - VolxCeleb~\cite{shon2022slue}.}consists of single-sided conversational voice snippets extracted from YouTube videos, originally designed for speaker recognition. The dataset represents natural, unscripted speech in diverse real-world settings, capturing a wide range of speaking styles, emotions, and acoustic conditions. Utterances containing slurs were excluded, and partial words were trimmed using a forced aligner to ensure clean, usable segments. \\

\subsubsection{Wild Challenge Set}


\noindent\textbf{Primock57~\cite{papadopoulos-korfiatis-etal-2022-primock57}.} 
contains mock consultations conducted by seven clinicians and 57 actors posing as patients, representing a diverse range of ethnicities, accents, and ages. Each actor was provided with a detailed case card outlining a primary care scenario, such as urinary tract infections, cardiovascular issues, or mental health concerns, ensuring the conversations were realistic and clinically relevant. The consultations were recorded using telemedicine software, capturing separate audio channels for clinicians and patients, and transcribed by experienced professionals to ensure accuracy.\\
\noindent\textbf{VoxPopuli Accented~\cite{wang-etal-2021-voxpopuli}.} is a comprehensive multilingual speech corpus derived from European Parliament event recordings. It includes audio, transcripts, and timestamps sourced directly from the official Parliament website. Due to its origin, the dataset features a rich collection of named entities, making it particularly suitable for tasks like Named Entity Recognition (NER). \\

% \noindent\textbf{BERSt~\cite{?}.} is a collection of speech data recorded in home environments using various smartphone microphones, with phone models included as metadata. The dataset features participants from diverse regions, including native English speakers from the UK, Canada, the USA (multi-state), and Australia, as well as non-native English speakers with accents such as French, Russian, and Hindi. Participants were instructed to speak, raise their voice, and shout 13 non-sense phrases, designed to be robust to linguistic context and high surprisal, while moving their phones to different distances and locations within their homes. Recordings also include scenarios with obstructions, such as placing the phone in a backpack, to simulate real-world conditions.\\
\noindent\textbf{ATCOsim~}\cite{jan_van_doorn_2023}.is a specialized database containing ten hours of English speech from ten non-native speakers, recorded during real-time ATC simulations using close-talk headset microphones. It features orthographic transcriptions, speaker metadata, and session details. With a 32 kHz sampling frequency and 10,078 clean, utterance-level recordings. \\

\input{tables/datasets}
\subsection{Models}
\noindent\textbf{Whisper Models \cite{radford2023robust}.}
is a transformer-based model that processes 80-dimensional log-mel filter bank features from 16 kHz audio, utilizing a 2D CNN stack followed by a transformer encoder-decoder architecture. Trained on a vast multilingual dataset of 680,000 hours, it incorporates timestamp tokens into its vocabulary and operates on 30-second audio windows during inference, auto-regressively generating text sequences while leveraging encoder outputs as context. Variants of Whisper, such as Distilled, Large, Base, and Medium, offer different trade-offs in model size and performance, catering to diverse computational and accuracy requirements.

\noindent\textbf{Seamless Models~\cite{communication2023seamlessm4tmassivelymultilingual, seamless2023, Barrault2025}.}
is a cutting-edge multilingual and multitask model for speech and text translation. Built on the UnitY architecture, it uses w2v-BERT 2.0 for speech encoding and NLLB for text encoding, supporting nearly 100 languages. A text decoder handles ASR and translation, while a text-to-unit (T2U) model and multilingual HiFi-GAN vocoder generate speech. Leveraging SONAR embeddings and SeamlessAlign (443,000 hours of aligned speech/text data), it achieves SOTA results in ASR, speech-to-text, speech-to-speech, and text-to-text translation, excelling in low-resource languages. It introduces BLASER 2.0 for robust evaluation and outperforms competitors in noisy environments. \\

\noindent\textbf{Nemo-ASR-Models~\cite{gulati2020conformerconvolutionaugmentedtransformerspeech, variani2020hybridautoregressivetransducerhat, rekesh2023fastconformerlinearlyscalable, noroozi2024statefulconformercachebasedinference, tang2023hybridtransducerattentionbased,  Harper_NeMo_a_toolkit}} We included several NVIDIAâ€™s NeMo advanced automatic speech recognition (ASR) models, including Canary, Parakeet (110M, 0.6B, and 1.1b), Conformer-CTC, and Fast-Conformer, as each is designed for specific use cases and optimized for performance. Canary-1B is a state-of-the-art multilingual, multitask model featuring a FastConformer encoder and Transformer decoder. The Parakeet family includes models with a FastConformer encoder paired with different decoders: CTC, RNN-T, or TDT. Conformer-CTC is a non-autoregressive model based on the Conformer architecture, combining self-attention and convolution for global and local feature extraction. It uses CTC loss and a linear decoder, supporting both sub-word (BPE) and character-level encodings. While Fast-Conformer is an optimized version of the Conformer architecture, offering significant speed improvements (2.4x faster) with minimal quality degradation. It uses 8x depthwise convolutional subsampling and reduced kernel sizes for efficiency. 

\noindent\textbf{Wav2Vec2 Models~\cite{schneider2019wav2vecunsupervisedpretrainingspeech, baevski2020wav2vec20frameworkselfsupervised}.} is a self-supervised pre-trained model designed to process raw audio inputs and generate speech representations. The model architecture consists of three key components: a convolutional feature encoder, a context network, and a quantization module.  The convolutional feature encoder converts raw waveforms into latent representations, which are then processed by the context network a transformer based stack with 24 blocks, a hidden size of 1024, 16 attention heads, and a feed-forward dimension of 4096 to capture contextual information.The quantization module maps these latent representations to quantized forms. 

\noindent\textbf{HuBERT Models~\cite{hsu2021hubertselfsupervisedspeechrepresentation}.}
 is a self-supervised learning framework designed for speech representation learning where CNN-encoded audio features are randomly masked. During training, the model predicts cluster assignments for masked regions of the input speech, forcing it to learn both acoustic and language models from continuous inputs.
 
\noindent\textbf{Audio/Speech Language Models 1.5B and 2B ~\cite{Rajaa_SpeechLLM_Multi-Modal_LLM}} is a multi-modal Language Model designed to analyze and predict metadata from a speaker's turn in a conversation. It integrates a speech encoder to convert speech signals into meaningful embeddings, which are then processed alongside text instructions by TinyLlama-1.1B-Chat-v1.0 to generate predictions. The model accepts 16 KHz audio inputs and predicts metadata such as SpeechActivity, Transcript, Gender, Age, Accent, and Emotion. 

\noindent\textbf{SpeechT5~\cite{ao2022speecht5unifiedmodalencoderdecoderpretraining}.}
unified modal framework capable of handling a wide range of tasks, including automatic speech recognition (ASR), speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.Its audio post-net, which can incorporate speaker embeddings to enable prosody transfer, making it effective for tasks like voice conversion and speech synthesis. By leveraging its encoder-decoder architecture, SpeechT5 can generate high-quality mel-spectrograms from text input while preserving speaker-specific characteristics like emotion and gender.
\input{tables/models}



\subsection{Experiments}\label{appsubsec:experiments}

\subsubsection{Regression Pipeline.}\label{appsubsubsec:regression_pipeline}
The regression framework is a stacking ensemble comprising multiple base regressors and a final estimator. We perform basic hyperparameter tuning using \texttt{RandomizedSearchCV} with 5-fold cross-validation, with the objective to minimize \textit{mean absolute error (MAE)}. The search explores key hyperparameters such as \texttt{n\_estimators}, \texttt{max\_depth}, \texttt{learning\_rate}, and \texttt{min\_samples\_split}, balancing model complexity and generalization. We provide hyperparameter and other details in~\ref{tab:hyperparameters}. The model is trained on 14 datasets divided into two groups: \textit{bench} (10 standard benchmark datasets) and \textit{in-the-wild} (4 diverse, real-world datasets). A leave-one-out strategy is applied to the \textit{bench} set, where the model is trained on 9 datasets and evaluated on the remaining one. All trained models are also evaluated on the \textit{in-the-wild} set, which remains isolated during training to assess out-of-domain generalization.
\input{tables/hyperparameters}

\subsection{Results}\label{appsubsec:results}
\input{tables/ablation_results_appendix}
\input{tables/results_in_the_wild_cer}

\input{figures/all_models}
\input{tables/results_benchmark}