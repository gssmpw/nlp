\section{Results}\label{sec:results}


We conduct experiments using two dataset categories: standard benchmarks and in-the-wild, as described in Section~\ref{subsec:datasets}. For each ASR model, a leave-one-out strategy is used, training the regression model on 9 benchmark datasets and testing it on the remaining benchmark dataset and all {\nwilds}~in-the-wild datasets to ensure comprehensive evaluation exclusively on out-of-domain data. Additionally, in-domain testing is included in ablation studies, as detailed in Section~\ref{subsec:ablation}. The regression model is trained to predict absolute error counts (word and character levels), which are normalized by the reference length to compute approximate error rates (\(aWER\) and \(aCER\)). We also train regression models to directly predict WER and CER. 




\subsection{Evaluation on In-the-Wild Datasets}\label{subsec:wild_eval}



The wild datasets provide a realistic testbed for evaluating the regression model's ability to approximate error rates under real-world conditions. The results are presented in Table~\ref{tab:main_results_in_the_wild}. High-performing models, like \textit{canary-1b}, demonstrate strong agreement between predicted and actual error rates. For example, on VP\_Accented, \textit{canary-1b} achieves a WER of \(23.2\%\) and an \(aWER\) of \(12.1\%\), with a minimal difference of \(1.1\%\). On Primock57, a clinical consultation dataset, the model shows robustness with a WER of \(16.2\%\) and an \(aWER\) of \(13.4\%\), highlighting its effective generalization across diverse and domain-specific contexts.



For models like \textit{data2vec-audio-large-960h} our approximation is pretty close to actual error rates with difference consistently under \(2\%\) on various datasets. For example, on LibriSpeech-test-noise, the model's actual WER is \(7.2\%\) while the approximated \(aWER\) is \(8.6\%\), showcasing its reliability in noisy conditions. Even on acoustically complex datasets like ATCOsim, where the WER is \(44.0\%\) and the \(aWER\) is \(51.1\%\), the model exhibits a reasonable alignment between approximated and actual error rates.




In contrast, models with high actual error rates, such as \textit{mms-1b-fl102}, show slightly larger deviations, particularly on datasets with challenging conditions. For instance, on ATCOsim, the WER is \(93.4\%\) and the \(aWER\) is \(99.0\%\), resulting in a significant deviation of \(5.6\%\), the highest observed across all in-the-wild datasets. Similarly, on Primock57, where the WER is \(70.2\%\) and the \(aWER\) is \(67.8\%\), the approximation also struggles to align due to the inherently high error rates. This highlights that extreme error cases often correspond to semantically nonsensical outputs, where the distinction between high and extremely high error rates becomes less relevant. 

\input{tables/results_in_the_wild}



\subsection{Evaluation on Benchmark Datasets}\label{subsec:benchmark_eval}
We summarize results on 10 standard benchmark datasets in Appendix~\ref{appsubsec:results} Tables~\ref{tab:results_benchmark_part1} and~\ref{tab:results_benchmark_part2}. Each table reports actual WER/CER alongside the approximated WER/CER (denoted by aWER/aCER). 

\input{figures/results-four-models}


Overall, models such as \textit{parakeet-tdt-1.1b} and \textit{whisper-large-v3} show relatively small differences between WER and aWER, indicating reliable approximations. For instance,  the actual WER for \textit{whisper-large-v3} on \textbf{AMI\_IHM} is 19.0\% compared to aWER 17.1\%, 1.9\% gap. Conversely, some challenging datasets (e.g., \textbf{CV11} and \textbf{Earnings22}) reveal larger discrepancies for specific models, particularly those with higher overall error rates. For example, \textit{mms-1b-fl102} exhibits a wide WER/aWER gap on \textbf{Earnings22}, suggesting difficulty handling accented or domain-specific speech. 




In general, high-performing ASR models demonstrate small WERâ€“aWER gaps, indicating that it's easy to approximate when error rates are low. However, models with higher WERs or faced with more acoustically or linguistically challenging test sets tend to show wider divergences. Despite these variations, most results remain within a reasonable margin, highlighting the robustness of our approximation model on diverse out-of-distribution data.

These results underscore the critical role of model quality in achieving reliable approximations. The approximation framework remains effective for high-performing models, while deviations tend to increase in cases of semantically divergent or poorly structured outputs, reflecting the inherent challenges in approximating errors for low-performing systems.







\subsection{Ablation}\label{subsec:ablation}
We conduct ablation experiments to evaluate the robustness of the approximation model and the contributions of its individual components. Using the evaluation setup outlined in Section~\ref{subsec:evaluation}, we select \textit{data2vec-audio-base-960h} as the source model (\(S\)) and \textit{wav2vec2-base-960h} as the target model (\(T\)). The results are summarized in Table~\ref{tab:ablation_baseline_results}, where IID results correspond to Case-I~\ref{para:case_1_data_iid_eval}, and \(D\), \(M\), and \(D+M\) under OOD represent Case II~\ref{para:case_2_data_iid_eval}, Case-III~\ref{para:case_3_data_ood_eval}, and Case-IV~\ref{para:case_4_data_ood_eval}, respectively. The reference model's $r$ value represents the average WER across all datasets. We include reference models with varying $r$ values, such as \textit{whisper-large-v3}~($r=17.8$), \textit{whisper-medium.en}~($r=20.1$), \textit{whisper-tiny}~($r=33.4$), and \textit{mms-1b-fl102}~($r=51.0$).


The results in Table~\ref{tab:ablation_baseline_results} demonstrate the importance of proxy references in improving the regression model's performance. Training without proxy references (\textit{w/o PR}) significantly increases the mean absolute error (MAE) across all conditions. For instance, the IID MAE increases from \(1.03\) (Base) to \(3.13\), and the OOD \(D+M\) MAE rises from \(1.07\) (Base) to \(3.33\), highlighting the essential role of proxy references in approximation.


Increasing the number of high-quality proxy references (\textit{MPR}) further reduces errors. Under IID conditions, the MAE decreases from \(1.00\) with \(n=2\) to \(0.93\) with \(n=5\). Similarly, in OOD \(D+M\), the error drops from \(1.06\) (\textit{MPR}, \(n=2\)) to \(0.95\) (\textit{MPR}, \(n=5\)), demonstrating that multiple high-quality references enhance model robustness.

% \input{tables/results_benchmark}
\input{tables/ablation_baseline}

The quality of references, quantified by the \(r\)-value, also plays a critical role. For example, in IID conditions, the MAE increases from \(1.31\) for \(r=17.8\) to \(2.03\) for \(r=51.0\). A similar trend is observed in OOD \(D+M\), where the MAE rises from \(1.40\) (\(r=17.8\)) to \(2.09\) (\(r=51.0\)). The absence of similarity (\textit{w/o S}) combined with low-quality proxies further degrades performance, underscoring the importance of both high-quality references and similarity measures. These trends are similarly observed for character-level error count approximation, as detailed in Appendix Table~\ref{tab:ablation_baseline_results_cer}.

\input{figures/scaling}

\paragraph{Scaling Training Data for Regression.}\label{effect_of_scaling_training_data} To evaluate the impact of training data size on the regression model, we scale the data from 1K to 10K examples in increments of 1K. As shown in Figure~\ref{fig:scaling}, the model's performance does not exhibit a clear trend with increasing training data size. Some datasets show slight improvements with more data; others show minimal improvement. This suggests that the regression model is largely agnostic to the size of the training data. In fact, it appears that a relatively small dataset of just 1,000 examples is sufficient to train a robust approximation model. This underscores the model's ability to generalize effectively with limited data, making it an efficient choice for scenarios with constrained datasets.



% \subsection{Qualitative Analysis}
% \input{tables/qualitative-examples}