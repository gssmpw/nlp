
\section{Methodology}\label{sec:methodology}


We present a scalable and robust method to approximate ASR performance metrics using multimodal unified embeddings, proxy references, and regression models. The primary goal is to eliminate reliance on ground-truth labels, enabling performance evaluation in label-scarce scenarios. The pipeline consists of three components: representation similarity in a unified speech-text embedding space, agreement with a \texttt{high-quality} proxy reference, and a regression model trained on these features to predict ASR metrics. Our pipeline diagram is shown in Figure~\ref{fig:pipeline_diagram}.


\subsection{Similarity in Unified Representation Space}
The foundation of our approach is the SONAR model~\cite{duquenne2023sonar}, a state-of-the-art multimodal (speech-text) model trained to produce unified embeddings for both speech and text inputs. Let $x_{\text{speech}}$ represent the input speech signal and $x_{\text{text}}$ denote the corresponding ASR-generated transcription. SONAR maps these inputs to a shared embedding space, generating $e_{\text{speech}}$ and $e_{\text{text}}$:
\begin{equation}
    e_{\text{speech}} = f_{\text{SONAR}}(x_{\text{speech}}),\quad e_{\text{text}} = f_{\text{SONAR}}(x_{\text{text}})
\end{equation}
where $f_{\text{SONAR}}$ represents the embedding model. The alignment between these embeddings is quantified using cosine similarity:
\begin{equation}
    \text{Similarity}(x_{\text{speech}}, x_{\text{text}}) = \frac{e_{\text{speech}} \cdot e_{\text{text}}}{\|e_{\text{speech}}\| \|e_{\text{text}}\|}
\end{equation}
The similarity metric serves as an indicator of transcription quality, with higher values suggest better alignment between speech and text representations.


\subsection{Agreement with a Proxy Reference}
To complement the similarity score, we utilize proxy references generated by a high-quality ASR model, denoted as $x_{\text{proxy}}$. The comparison between the ASR-generated transcription $x_{\text{text}}$ and the proxy reference $x_{\text{proxy}}$ is quantified using Word Error Rate (pWER) and Character Error Rate (pCER) as defined in Appendix~\ref{appsubsec:methodology}.

% \begin{align}
%     \text{pWER}(x_{\text{text}}, x_{\text{proxy}}) &= \frac{\text{EditDistance}(x_{\text{text}}, x_{\text{proxy}})}{\text{WordCount}(x_{\text{proxy}})} \\
%     \text{pCER}(x_{\text{text}}, x_{\text{proxy}}) &= \frac{\text{EditDistance}(x_{\text{text}}, x_{\text{proxy}})}{\text{CharCount}(x_{\text{proxy}})}
% \end{align}


These metrics assess transcription quality by comparing it with a reliable proxy reference, without using ground-truth labels at any stage. Proxy references are dynamically selected by profiling 41 models across datasets and ranking them by average performance. For each target ASR model, the reference is the highest-ranking model other than the target itself. For example, if \texttt{whisper-large-v3} ranks highest, the reference for \texttt{whisper} will be the second-best model. This ensures the proxy reference is both relevant and reliable for evaluating the target model.


\input{figures/pipeline}


\subsection{Regression Model for Metric Prediction}
The extracted features, including similarity scores and proxy metrics, are concatenated to form the input to a regression model. Let $z = [\text{Similarity}, \text{pWER}/\text{pCER}]$ represent the feature vector. The regression model $g$ estimates the ASR metrics $\hat{y}$, denoted as aWER and/or aCER:
\begin{equation}
    \hat{y} = g(z)
\end{equation}

The regression model is an ensemble of Random Forest, Gradient Boosting, and Histogram-based Gradient Boosting regressors. Each base model is fine-tuned via grid search for hyperparameter optimization. The ensemble is trained to minimize the mean absolute error between predicted and ground-truth metrics. Additionally, a ridge regression model with non-negativity constraints is included in the ensemble to ensure predictions remain within valid ranges. Additional details of our regression pipeline are provided in Section~\ref{sec:experiments}, with hyperparameter details in Appendix~\ref{appsubsec:experiments}.


\subsection{Evaluation}\label{subsec:evaluation}
We evaluate the regression model's performance across four setups, including IID and OOD data and different model configurations. Specifically, we train our regression model on one ASR system (source) on one dataset and evaluate it on both IID and OOD data for the source and target models.


Let $\mathcal{D}_{M,B}$ denote the 10 benchmark datasets, and $\mathcal{D}_{M, W}$ represent the {\nwilds}~in-the-wild datasets, as described in Section~\ref{subsec:datasets}, where $M \in \{S, T\}$ refers to either the source model $S$ or the target model $T$. 

The regression model is trained on data $\mathcal{D}_{S,B}^{\text{train}} \sim \mathcal{D}_{S,B}$ and evaluated on the IID test set $\mathcal{D}_{S,B}^{\text{test-IID}} \sim \mathcal{D}_{S,B}$, consisting of $80\%$ and $20\%$ of the data, respectively. Additionally, the model is evaluated on $\mathcal{D}_{T,B}^{\text{test-IID}}$, $\mathcal{D}_{S, W}$, and $\mathcal{D}_{T, W}$. Below, we detail the formulation of each evaluation setup.

\paragraph{Case 1: IID Evaluation (Source \(S\))}\label{para:case_1_data_iid_eval}
The regression model is trained on $\mathcal{D}_{S,B}^{\text{train}}$ and evaluated on $\mathcal{D}_{S,B}^{\text{test-IID}}$. Let \(x_1^S = f(s, o^S)\) represent the similarity between input speech \(s\) and the ASR output \(o^S\), and \(x_2^S = g(o^S, r)\) represent the agreement with the proxy reference \(r\), where \(o^S\) is the ASR output produced by the source model \(S\). The evaluation is formulated as:
\begin{equation}
\mathcal{L}_{\text{IID}}^S = \mathbb{E}_{(x_1^S, x_2^S, y) \sim \mathcal{D}_{S,B}^{\text{test-IID}}} \big[\mathcal{L}(h(x_1^S, x_2^S), y)\big]
\end{equation}
 
\paragraph{Case 2: IID Evaluation (Target \(T\))}\label{para:case_2_data_iid_eval}
The regression model trained on $\mathcal{D}_{S,B}^{\text{train}}$ is evaluated on the IID test set $\mathcal{D}_{T,B}^{\text{test-IID}}$. Let \(x_1^T = f(s, o^T)\) represent the similarity between input speech \(s\) and the ASR output \(o^T\), and \(x_2^T = g(o^T, r)\) represent the agreement with the proxy reference \(r\), where \(o^T\) is the ASR output produced by the target model \(T\). The evaluation is expressed as:
\begin{equation}
\mathcal{L}_{\text{IID}}^T = \mathbb{E}_{(x_1^T, x_2^T, y) \sim \mathcal{D}_{T,B}^{\text{test-IID}}} \big[\mathcal{L}(h(x_1^T, x_2^T), y)\big]
\end{equation}

\paragraph{Case 3: OOD Evaluation (Source \(S\))}\label{para:case_3_data_ood_eval}
The regression model trained on $\mathcal{D}_{S,B}^{\text{train}}$ is evaluated on the out-of-distribution set $\mathcal{D}_{S, W}$. Let \(x_1^S = f(s, o^S)\) represent the similarity between the input speech \(s\) and the ASR output \(o^S\), and \(x_2^S = g(o^S, r)\) represent the agreement with the proxy reference \(r\), where \(o^S\) is the ASR output produced by the source model \(S\). The evaluation is defined as:
\begin{equation}
\mathcal{L}_{\text{OOD}}^S = \mathbb{E}_{(x_1^S, x_2^S, y) \sim \mathcal{D}_{S, W}} \big[\mathcal{L}(h(x_1^S, x_2^S), y)\big]
\end{equation}

\paragraph{Case 4: OOD Evaluation (Target \(T\))}\label{para:case_4_data_ood_eval}
The regression model trained on $\mathcal{D}_{S,B}^{\text{train}}$ is evaluated on the out-of-distribution set $\mathcal{D}_{T, W}$, using the ASR output produced by the target model \(T\). Let \(x_1^T = f(s, o^T)\) represent the similarity between the input speech \(s\) and the ASR output \(o^T\), and \(x_2^T = g(o^T, r)\) represent the agreement with the proxy reference \(r\), where \(o^T\) is the ASR output produced by the target model \(T\). The evaluation is expressed as:
\begin{equation}
\mathcal{L}_{\text{OOD}}^T = \mathbb{E}_{(x_1^T, x_2^T, y) \sim \mathcal{D}_{T, W}} \big[\mathcal{L}(h(x_1^T, x_2^T), y)\big]
\end{equation}


\noindent\textbf{Note.} For computational feasibility, the primary experiments train the regression model on 9 out of the 10 datasets in $\mathcal{D}_{S,B}^{\text{train}}$ and evaluate it on the remaining dataset, as well as on all {\nwilds} datasets in $\mathcal{D}_{S,B}^{\text{OOD}}$. This process is repeated for each dataset in $\mathcal{D}_{S,B}^{\text{train}}$, ensuring robust evaluation across various testing conditions. No examples from $\mathcal{D}_{M,\text{OOD}}$ are used at any stage for training the regression model.


