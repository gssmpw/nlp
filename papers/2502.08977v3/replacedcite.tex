\section{RELATED WORK}
\label{2}
\subsection{Text-to-3D Generation}\label{2.1}
With the rapid advancements in text-to-image generation models, researchers increasingly focus on extending these capabilities to text-to-3D generation ____. Early approaches ____ leverage CLIP-based loss functions to supervise 3D representation learning, achieving promising results. However, these methods often struggle with realism and fine-grained geometric details. A significant breakthrough comes with DreamFusion____, which replaces CLIP supervision with a text-to-image diffusion model and introduces SDS to optimize NeRF-based 3D representations. This innovation lays the foundation for numerous follow-up methods.
Magic3D ____ improves computational efficiency through a two-stage pipeline, incorporating a more effective DMET ____ representation. ProlificDreamer ____ addresses over-saturation and over-smoothing issues by introducing a variational SDS objective, while MVDream ____ employs a multi-view diffusion model to enhance 3D consistency across different viewpoints. Fantasia3D ____ introduces a decoupled geometry and appearance modeling strategy, optimizing surface normals independently using SDS loss. More recently, DreamGaussian ____ replaces NeRF with 3DGS, significantly reducing inference time. MVGaussian ____ further refines this approach by introducing a densification algorithm that aligns Gaussian distributions with the surface, thereby improving structural fidelity.

Further advancements target structural coherence and optimization efficiency. DreamCouple ____ introduces a correction flow model to identify coupled noise and incorporates a unique pair matching (UCM) loss to mitigate over-smoothing effects. DreamMapping ____ proposes Variational Distribution Mapping (VDM) and timestep-dependent Distribution Coefficient Annealing (DCA) to enhance distillation precision. ISD refines the SDS framework by replacing its reconstruction term with an invariant score term derived from DDIM sampling, enabling a moderate classifier-free guidance scale. This modification reduces reconstruction errors while alleviating over-smoothing and over-saturation issues.

Despite these advances, current methods still face significant challenges in high-quality 3D human generation, particularly in mitigating Janus artifacts and geometric inconsistencies, highlighting the need for further refinement in fine-grained semantic alignment and structural fidelity.
\subsection{Text-driven 3D Human Generation}\label{2.2}
Existing methods for text-driven 3D human generation leverage various representations and optimization strategies, yet they still face challenges in realism, detail preservation, and alignment with complex textual descriptions. AvatarCLIP ____ integrates SMPL ____ and NeuS ____ with CLIP supervision to generate 3D avatars, but the results often lack realism and appear overly simplified. DreamAvatar ____ and AvatarCraft ____ incorporate SMPL priors for pose and shape constraints, yet they tend to produce blurry 3D avatars. DreamWaltz ____ enhances generation quality by utilizing 3D-consistent occlusion-aware SDS with 3D-aware skeletal conditions, while DreamWaltz-G ____ further refines this approach by introducing a hybrid 3D Gaussian avatar representation, combining neural implicit fields and parametric 3D meshes to enable real-time rendering and stable SDS optimization.

Other methods focus on pose conditioning and structured diffusion models. DreamHuman ____ generates animatable 3D humans by incorporating pose-conditioned NeRF trained on imGHUM ____. AvatarBooth ____ employs dual fine-tuning diffusion models to separately refine facial and body details, improving the personalization of avatars from casual images. AvatarVerse ____ enhances view consistency by training ControlNet ____ conditioned on DensePose ____. TADA ____ utilizes SMPL-X with displacement layers and texture maps, introducing hierarchical rendering with SDS loss to improve 3D human synthesis. Building upon this framework, X-Oscar ____ proposes avatar-aware score distillation sampling (ASDS), which injects avatar-specific noise into rendered images to enhance optimization and improve generation quality. GAvatar ____ introduces an SDF-based implicit mesh learning approach, capturing fine facial geometries and enabling large-scale animatable avatar synthesis from textual descriptions.

More recently, fine-tuned text-to-image methods have been explored to enhance text-driven 3D human generation. HumanNorm ____ fine-tunes a text-to-image model to predict normal and depth maps from text, which are subsequently used for SDS-based geometry refinement and texture optimization via normal-conditioned ControlNet ____. HumanGaussian ____ introduces structure-aware SDS, jointly optimizing geometry and appearance by fine-tuning a text-to-image model to predict depth maps, which guide Gaussian densification and pruning to enhance human body synthesis. While these methods improve the structural and appearance quality of generated avatars, they struggle with semantic alignment for long textual descriptions, limiting their ability to capture complex, fine-grained attributes.
To overcome these limitations, we propose a novel framework that enhances long-text alignment in text-to-3D human generation, ensuring more precise and semantically consistent 3D outputs.

\begin{figure*}[!htbp]
	\centering{
		\includegraphics[width=2.0\columnwidth]{pipelines.eps}
		\caption{Overview of the Proposed Framework. We first leverage a LLM to generate complex long-text prompts along with dynamic negative prompts, which guide the optimization process and enhance the quality of 3D human generation. Next, we employ the representation of 3DGS ____ to synthesize accurate 3D human models from the generated textual descriptions. The process begins with densely sampling Gaussian functions on the SMPL-X ____ human mesh, serving as the initialization for model generation. To improve semantic alignment between the long-text descriptions and the 3D human models, we introduce a preference optimization module, which integrates preference models such as ImageReward ____ and PickScore ____. These models capture fine-grained semantic nuances across different text segments, enhancing overall alignment accuracy. Additionally, we propose a negation preference optimization module, which incorporates both static and dynamic negative prompts to mitigate over-optimization in preference models. This mechanism prevents excessive emphasis on certain attributes while ensuring balanced semantic representation. The entire framework is optimized through SDS, where preference models refine the SDS process to achieve more precise alignment between long-text descriptions and 3D human representations.}
		\label{fig:pipelines}
	}
\end{figure*}


\subsection{Text-to-Image Generative Models Alignment}\label{2.3}
Recent advancements in text-to-image generation have significantly improved the quality of synthesized 2D images. However, these images often fail to align precisely with human preferences, making preference alignment a critical research challenge. To address this issue, various studies ____ have focused on constructing large-scale text-image preference datasets and fine-tuning visual models as evaluation functions, thereby enhancing the alignment between generated outputs and human expectations.

Several notable approaches have been proposed to improve preference alignment. ImageReward ____ introduces a reward-feedback learning framework to optimize diffusion models and presents the first universal text-image human preference reward model, effectively encoding human visual preferences. PickScore ____ constructs a large-scale open dataset for text-to-image generation and employs a fine-tuned CLIP model to enhance preference alignment. HPS\_V2 ____ curates a high-quality preference dataset with carefully selected text-image pairs to mitigate potential biases. VisionPrefer ____ utilizes a multimodal large-scale language model to generate a fine-grained preference dataset, incorporating key factors such as prompt adherence, aesthetics, fidelity, and harmlessness to enhance visual preference modeling. To further improve alignment in long-text-driven generation, LongAlign ____ introduces a segment-level encoding approach, facilitating the processing of complex textual descriptions and leveraging a CLIP-based DenseScore preference model for efficient long-text alignment training.

Despite these advancements, existing preference models still exhibit semantic bias issues. For instance, ImageReward effectively recognizes categorical attributes (e.g., color and position) but struggles to capture fine-grained textual attributes (e.g., specific hat types). In contrast, PickScore demonstrates superior attribute perception, accurately identifying fine-grained object properties such as different types of hats. To address these limitations, we propose an integrated preference modeling approach that combines multiple preference models, enabling a more comprehensive understanding of textual semantics and improving the robustness and accuracy of preference alignment.