\section{Related Work}
% \textbf{Offline Restless Bandit\;\;} 
The offline restless bandit problem, with known transition matrices and reward functions, is 
% a 
PSPACE-hard to solve due to the combinatorial nature of the budget constraint 
% combinatorial optimization problem 
____.
% When the transition matrix and the reward function of the MDP are given, the restless bandit problem is a combinatorial optimization problem, that is 
% % Since this optimization problem is shown to be 
% PSPACE-hard to solve
% exactly 
% ____.
Many approximation algorithms have been proposed,
including Whittle's index policy____ and linear programming-based methods____, both of which are proven to be optimal as the number of agents approaches infinity.
Whittle's index policy is commonly used as a \emph{benchmark} for evaluating online restless bandit algorithms____. We provide a brief overview in Section~\ref{subsec:restless_bandits} and a detailed explanation in Appendix~\ref{app:whittle}.
In this paper, we focus on the online setting, where transitions and rewards must be learned from data.
% The hardness behind the optimization problem lies in the budget constraint.
% One line of research focuses on the Lagrangian relaxation of the constraint. Based on this, Whittle first introduces Whittle's index policy ____. The asymptotic optimality of Whittle's index policy is established in ____. Meanwhile, it is also shown that Whittle's index could provide a scalable solution to real-life applications of restless bandit problems ____. 
% Another line of research focuses on relaxing the original optimization problem of linear programming. Based on this relaxation, fluid linear programming-based policies are proposed ____ and asymptotic optimality has been established.
% An recent paper ____ establishes a new type of policy under an additional constraint that all arms could only be pulled once.w

% This paper, on the other hand, is trying to find the optimal policy without assuming knowing the environment of the MDP.

\textbf{Online Restless Bandit\;\;} 
Online restless bandit algorithms typically fall into two categories: model-based and model-free methods. The former directly estimates the transition matrix in both finite-____ and infinite-horizon ____ settings, whereas the latter focuses on learning the Q-function specifically in the infinite-horizon setting ____.
% This paper is closely related to the online restless bandits problem, where the transition matrix and the reward of the MDP is unknown and has to be learned sequentially.
% \kyra{here}
Our problem focuses on the finite horizon setting, and we contrast our results with prior finite-horizon regret guarantees in Section~\ref{subsec:online-restless-bandit}.

% There are two major lines of work. The first line of research is the model-based method, where the unknown transition matrix is estimated directly using the data collected. Among model-based methods, the works can be further divided into two groups based on their objective: maximizing the average reward in an infinite horizon or maximizing the cumulative reward in a finite horizon. For maximizing average reward, ____ provides an algorithm that has $\mathcal{O}\left(\sqrt{T}\right)$ regret. However, the constant is exponential in the number of arms and states. Meanwhile, the constant is also dependent on the ergodicity coefficient of the underlying MDP. ____ improved the dependence of regret on the number of arms and states, but they have worse dependence on the horizon $T$, which is $\mathcal{O}(T^{2/3})$. However, they assumed that the transition matrix is not dependent on action. After that, ____ improved the results and reduce the dependence on instance-dependent constants. To be specific, their algorithms had $\mathcal{O}\left(\sqrt{T}\right)$ regret. However, their regret is still dependent on the ergodicity coefficient and they require an offline generator that could sample a certain number of state-action pairs. ____ focused on learning specifically the Whittle's index policy and provided an algorithm with regret $\mathcal{O}\left(\sqrt{T}\right)$. Again their result is dependent on the ergodicity coefficient. This paper focuses on maximizing the cumulative reward within a fixed horizon, which distinguishes us from this line of research.

% For maximizing the finite-horizon cumulative reward, the problem involves a fixed horizon $T$ and a fixed number of episodes $K$. In this setting, an episode contains a run of restless bandit for $T$ steps and the goal is to maximize the cumulative reward over multiple episodes. Based on Thompson sampling, ____ designed algorithms that had good performance compared with Whittle's index policy, where ____ achieved $\mathcal{O}\left(\sqrt{T^4K}\right)$ regret. Based on UCB, ____ developed an algorithm with $\mathcal{O}\left(\sqrt{T^3K}\right)$ regret upper bound. The relation between this paper and their works will be further discussed in Section \ref{subsec:online-restless-bandit}.


% Another line of research focuses on developing algorithms leveraging Q-learning and Whittle's index policy. In this line of research, maximizing the infinite horzion discounted reward is the objective. ____ derived the first model-free approach for restless bandits and showed that as the number of iteration grows, their algorithm will converge to Whittle's index policy. Moving on, ____ designed algorithms that could have $\mathcal{O}(1/k^{2/3})$ convergence rate, where $k$ is the number of iterations. Recently, ____ revised the algorithm and showed a faster convergence speed numerically.

\textbf{Thresholding Bandit/MDP\;\;} 
Our paper is closely related to regret-minimizing thresholding bandit problems, where regret is defined by the gap between the arm's mean and a predefined threshold when the mean falls below the threshold.
% Regret-minimization thresholding bandit problems, whose goal is minimizing the regret characterized by the gap between the arm below some predetermined threshold and the threshold is closely related to this paper. 
It is shown that under this setting, minimax optimal (max is taken over all possible bandit instances and min taken over all possible learning algorithms) constant regret is achievable ____. 
% \kyra{what do we mean by minimax constant regret is achievable? it is optimal?}
Our problem setup differs from the traditional thresholding bandit framework by incorporating context, where the arrival of contexts follows an MDP.
% \kyra{check, maybe missing another point here}
% Instead of focusing on multi-armed bandit setting, this paper focuses on contextual bandit problem. 
% Furthermore, 
In contrast, ____ study threshold MDPs under an infinite horizon setting, seeking policies with average rewards exceeding a predefined threshold. We, however, focus on the finite-horizon setting.
% ____ study threshold MDP problems where the goal is to find a policy whose average reward is above some predetermined threshold.
 %____ provides a constant regret bound when there is exactly one arm whose reward is above the threshold. Leveraging UCB algorithms, ____ provides a regret upper bound that is only dependent on the gap between the arm below the threshold and the threshold. Recently, ____ improved the result and provided a upper bound that is only dependent on the gap between the arm above the threshold and the threshold. In their paper, they leveraged the lower confidence bound. However, their works are focusing on multi-armed bandit setting while ours is focusing on contextual bandit problems. 
 % We note that our problem differs from theirs since we are considering the finite-horizon setting instead. 

% \kyra{here, and need to make the connection between our problem and EMDP more clear}
\textbf{Episodic MDP\;\;} When both the budget and the number of arms in an online restless bandit are set to one, the problem reduces to a standard reinforcement learning problem. Thus, our work also relates to episodic MDP, which focuses on maximizing cumulative rewards across 
% learning Q-functions
a finite horizon. Let $K$ be the number of episodes, each containing $T$ time steps. 
____ establishes a minimax regret of $\mathcal{O}\left(\sqrt{T^2K}\right)$ for stationary transitions, and ____ derives an upper bound of $\mathcal{O}\left(\sqrt{T^4K}\right)$ for nonstationary transitions. 
These results highlight the difficulty of the learning problem under a finite horizon, emphasizing the need for a reformulation to simplify the learning objective.
% the number of budget and the number of arms are all one in restless bandit problem, then it is reduced to the reinforcement learning problem. 
% Therefore, our work is related to Episodic Markov Decision Process where it consists of $K$ episodes and within each episode, it contains a run of MDP for $T$ steps.
% The goal is to maximize the cumulative reward across $K$ episodes.
% Assuming that the transition matrix is stationary, ____ achieves a minimax $\mathcal{O}\left(\sqrt{T^2K}\right)$ regret bound. Moving on, ____ achieves a $\mathcal{O}\left(\sqrt{H^4K}\right)$ regret upper bound when the transition matrix is non-stationary.
%