\section{Related Work}
% \textbf{Offline Restless Bandit\;\;} 
The offline restless bandit problem, with known transition matrices and reward functions, is 
% a 
PSPACE-hard to solve due to the combinatorial nature of the budget constraint 
% combinatorial optimization problem 
\citep{b0e74184-2114-3e45-b092-dfbc8fefcf91}.
% When the transition matrix and the reward function of the MDP are given, the restless bandit problem is a combinatorial optimization problem, that is 
% % Since this optimization problem is shown to be 
% PSPACE-hard to solve
% exactly 
% \citep{b0e74184-2114-3e45-b092-dfbc8fefcf91}.
Many approximation algorithms have been proposed,
including Whittle's index policy~\citep{whittle1988restless, Weber_Weiss_1990, 10.1109/ISIT.2018.8437712,7852321} and linear programming-based methods~\citep{verloop2016asymptotically,zhang2021restless,brown2023fluid,hong2024achievingexponentialasymptoticoptimality,gast2024linear}, both of which are proven to be optimal as the number of agents approaches infinity.
Whittle's index policy is commonly used as a \emph{benchmark} for evaluating online restless bandit algorithms~\citep{jung2019regret,wang2020restless, wang2023optimistic,akbarzadeh2023learning}. We provide a brief overview in Section~\ref{subsec:restless_bandits} and a detailed explanation in Appendix~\ref{app:whittle}.
In this paper, we focus on the online setting, where transitions and rewards must be learned from data.
% The hardness behind the optimization problem lies in the budget constraint.
% One line of research focuses on the Lagrangian relaxation of the constraint. Based on this, Whittle first introduces Whittle's index policy \citep{whittle1988restless}. The asymptotic optimality of Whittle's index policy is established in \citealt{Weber_Weiss_1990}. Meanwhile, it is also shown that Whittle's index could provide a scalable solution to real-life applications of restless bandit problems \citep{10.1109/ISIT.2018.8437712,7852321}. 
% Another line of research focuses on relaxing the original optimization problem of linear programming. Based on this relaxation, fluid linear programming-based policies are proposed \citep{zhang2021restless,hong2024achievingexponentialasymptoticoptimality,brown2023fluid,gast2024linear,verloop2016asymptotically} and asymptotic optimality has been established.
% An recent paper \citep{xiong2025finite} establishes a new type of policy under an additional constraint that all arms could only be pulled once.w

% This paper, on the other hand, is trying to find the optimal policy without assuming knowing the environment of the MDP.

\textbf{Online Restless Bandit\;\;} 
Online restless bandit algorithms typically fall into two categories: model-based and model-free methods. The former directly estimates the transition matrix in both finite-\citep{jung2019regret,wang2023optimistic,liang2024bayesian} and infinite-horizon \citep{10.1007/978-3-642-34106-9_19, wang2020restless, xiong2022learning,xiong2022reinforcement, akbarzadeh2023learning} settings, whereas the latter focuses on learning the Q-function specifically in the infinite-horizon setting \citep{killian2021q, xiong2023finite,xiong2024whittle, kakarapalli2024faster}.
% This paper is closely related to the online restless bandits problem, where the transition matrix and the reward of the MDP is unknown and has to be learned sequentially.
% \kyra{here}
Our problem focuses on the finite horizon setting, and we contrast our results with prior finite-horizon regret guarantees in Section~\ref{subsec:online-restless-bandit}.

% There are two major lines of work. The first line of research is the model-based method, where the unknown transition matrix is estimated directly using the data collected. Among model-based methods, the works can be further divided into two groups based on their objective: maximizing the average reward in an infinite horizon or maximizing the cumulative reward in a finite horizon. For maximizing average reward, \citealt{10.1007/978-3-642-34106-9_19} provides an algorithm that has $\mathcal{O}\left(\sqrt{T}\right)$ regret. However, the constant is exponential in the number of arms and states. Meanwhile, the constant is also dependent on the ergodicity coefficient of the underlying MDP. \citealt{wang2020restless} improved the dependence of regret on the number of arms and states, but they have worse dependence on the horizon $T$, which is $\mathcal{O}(T^{2/3})$. However, they assumed that the transition matrix is not dependent on action. After that, \citealt{xiong2022learning,xiong2022reinforcement} improved the results and reduce the dependence on instance-dependent constants. To be specific, their algorithms had $\mathcal{O}\left(\sqrt{T}\right)$ regret. However, their regret is still dependent on the ergodicity coefficient and they require an offline generator that could sample a certain number of state-action pairs. \citealt{akbarzadeh2023learning} focused on learning specifically the Whittle's index policy and provided an algorithm with regret $\mathcal{O}\left(\sqrt{T}\right)$. Again their result is dependent on the ergodicity coefficient. This paper focuses on maximizing the cumulative reward within a fixed horizon, which distinguishes us from this line of research.

% For maximizing the finite-horizon cumulative reward, the problem involves a fixed horizon $T$ and a fixed number of episodes $K$. In this setting, an episode contains a run of restless bandit for $T$ steps and the goal is to maximize the cumulative reward over multiple episodes. Based on Thompson sampling, \citealt{jung2019regret,liang2024bayesian} designed algorithms that had good performance compared with Whittle's index policy, where \citealt{jung2019regret} achieved $\mathcal{O}\left(\sqrt{T^4K}\right)$ regret. Based on UCB, \citealt{wang2023optimistic} developed an algorithm with $\mathcal{O}\left(\sqrt{T^3K}\right)$ regret upper bound. The relation between this paper and their works will be further discussed in Section \ref{subsec:online-restless-bandit}.


% Another line of research focuses on developing algorithms leveraging Q-learning and Whittle's index policy. In this line of research, maximizing the infinite horzion discounted reward is the objective. \citealt{killian2021q} derived the first model-free approach for restless bandits and showed that as the number of iteration grows, their algorithm will converge to Whittle's index policy. Moving on, \citealt{xiong2023finite,xiong2024whittle} designed algorithms that could have $\mathcal{O}(1/k^{2/3})$ convergence rate, where $k$ is the number of iterations. Recently, \citealt{kakarapalli2024faster} revised the algorithm and showed a faster convergence speed numerically.

\textbf{Thresholding Bandit/MDP\;\;} 
Our paper is closely related to regret-minimizing thresholding bandit problems, where regret is defined by the gap between the arm's mean and a predefined threshold when the mean falls below the threshold.
% Regret-minimization thresholding bandit problems, whose goal is minimizing the regret characterized by the gap between the arm below some predetermined threshold and the threshold is closely related to this paper. 
It is shown that under this setting, minimax optimal (max is taken over all possible bandit instances and min taken over all possible learning algorithms) constant regret is achievable \citep{tamatsukuri2019guaranteed,michel2022regret,feng2024satisficing}. 
% \kyra{what do we mean by minimax constant regret is achievable? it is optimal?}
Our problem setup differs from the traditional thresholding bandit framework by incorporating context, where the arrival of contexts follows an MDP.
% \kyra{check, maybe missing another point here}
% Instead of focusing on multi-armed bandit setting, this paper focuses on contextual bandit problem. 
% Furthermore, 
In contrast, \citet{hajiabolhassan2023online} study threshold MDPs under an infinite horizon setting, seeking policies with average rewards exceeding a predefined threshold. We, however, focus on the finite-horizon setting.
% \citealt{hajiabolhassan2023online} study threshold MDP problems where the goal is to find a policy whose average reward is above some predetermined threshold.
 %\citealt{tamatsukuri2019guaranteed} provides a constant regret bound when there is exactly one arm whose reward is above the threshold. Leveraging UCB algorithms, \citealt{michel2022regret} provides a regret upper bound that is only dependent on the gap between the arm below the threshold and the threshold. Recently, \citealt{feng2024satisficing} improved the result and provided a upper bound that is only dependent on the gap between the arm above the threshold and the threshold. In their paper, they leveraged the lower confidence bound. However, their works are focusing on multi-armed bandit setting while ours is focusing on contextual bandit problems. 
 % We note that our problem differs from theirs since we are considering the finite-horizon setting instead. 

% \kyra{here, and need to make the connection between our problem and EMDP more clear}
\textbf{Episodic MDP\;\;} When both the budget and the number of arms in an online restless bandit are set to one, the problem reduces to a standard reinforcement learning problem. Thus, our work also relates to episodic MDP, which focuses on maximizing cumulative rewards across 
% learning Q-functions
a finite horizon. Let $K$ be the number of episodes, each containing $T$ time steps. 
\citet{azar2017minimax} establishes a minimax regret of $\mathcal{O}\left(\sqrt{T^2K}\right)$ for stationary transitions, and \citet{jin2018q} derives an upper bound of $\mathcal{O}\left(\sqrt{T^4K}\right)$ for nonstationary transitions. 
These results highlight the difficulty of the learning problem under a finite horizon, emphasizing the need for a reformulation to simplify the learning objective.
% the number of budget and the number of arms are all one in restless bandit problem, then it is reduced to the reinforcement learning problem. 
% Therefore, our work is related to Episodic Markov Decision Process where it consists of $K$ episodes and within each episode, it contains a run of MDP for $T$ steps.
% The goal is to maximize the cumulative reward across $K$ episodes.
% Assuming that the transition matrix is stationary, \citealt{azar2017minimax} achieves a minimax $\mathcal{O}\left(\sqrt{T^2K}\right)$ regret bound. Moving on, \citealt{jin2018q} achieves a $\mathcal{O}\left(\sqrt{H^4K}\right)$ regret upper bound when the transition matrix is non-stationary.
%