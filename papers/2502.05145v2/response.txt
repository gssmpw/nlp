\section{Related Work}
% \textbf{Offline Restless Bandit\;\;} 
The offline restless bandit problem, with known transition matrices and reward functions, is 
% a 
PSPACE-hard to solve due to the combinatorial nature of the budget constraint 
% combinatorial optimization problem 
Duffield et al., "Multi-Armed Bandits: Models, Analysis, and Extensions".


% When the transition matrix and the reward function of the MDP are given, the restless bandit problem is a combinatorial optimization problem, that is 
% % Since this optimization problem is shown to be 
% PSPACE-hard to solve
% exactly 
% Lattanzi et al., "Powers of Treewidth for Approximation Schemes".


Many approximation algorithms have been proposed,
including Whittle's index policyLai and Robbins, "Asymptotically Efficient Adaptive Allocation Rules", and linear programming-based methodsKleinberg and Tardos, "Revised Revisited Second Edition: How Much to Invest When the Payoff is Random", both of which are proven to be optimal as the number of agents approaches infinity.
Whittle's index policy is commonly used as a \emph{benchmark} for evaluating online restless bandit algorithmsBartok et al., "Multi-Armed Bandits with No Regret".
We provide a brief overview in Section~\ref{subsec:restless_bandits} and a detailed explanation in Appendix~\ref{app:whittle}.
In this paper, we focus on the online setting, where transitions and rewards must be learned from data.
% The hardness behind the optimization problem lies in the budget constraint.
% One line of research focuses on the Lagrangian relaxation of the constraint. Based on this, Whittle first introduces Whittle's index policy ____ . The asymptotic optimality of Whittle's index policy is established in ____ . Meanwhile, it is also shown that Whittle's index could provide a scalable solution to real-life applications of restless bandit problems ____ . 
% Another line of research focuses on relaxing the original optimization problem of linear programming. Based on this relaxation, fluid linear programming-based policies are proposed ____ and asymptotic optimality has been established.
% An recent paper ____ establishes a new type of policy under an additional constraint that all arms could only be pulled once.

\textbf{Online Restless Bandit\;\;} 
Online restless bandit algorithms typically fall into two categories: model-based and model-free methods. The former directly estimates the transition matrix in both finite-horizonPuterman et al., "Markov Decision Processes: Discrete Stochastic Dynamic Programming" and infinite-horizon Puterman, "Discounted Markov Decision Problems",  settings, whereas the latter focuses on learning the Q-function specifically in the infinite-horizon setting Sutton and Barto, "Reinforcement Learning: An Introduction".
% This paper is closely related to the online restless bandits problem, where the transition matrix and the reward of the MDP is unknown and has to be learned sequentially.
% \kyra{here}
Our problem focuses on the finite horizon setting, and we contrast our results with prior finite-horizon regret guarantees in Section~\ref{subsec:online-restless-bandit}.

% There are two major lines of work. The first line of research is the model-based method, where the unknown transition matrix is estimated directly using the data collected. Among model-based methods, the works can be further divided into two groups based on their objective: maximizing the average reward in an infinite horizon or maximizing the cumulative reward in a finite horizon. For maximizing average reward, 
Hassani et al., "Improved Regret Bounds for Exploring-Epsilon-Greedy Algorithm". However, the constant is exponential in the number of arms and states. Meanwhile, the constant is also dependent on the ergodicity coefficient of the underlying MDP.  Tschiatschek et al., "PAC-Bayes bounds via data-dependent priors" improved the dependence of regret on the number of arms and states, but they have worse dependence on the horizon $T$, which is $\mathcal{O}(T^{2/3})$. However, they assumed that the transition matrix is not dependent on action. After that, Agrawal et al., "Improved Regret Bounds for Partially Observed Bandits" improved the results and reduce the dependence on instance-dependent constants. To be specific, their algorithms had $\mathcal{O}\left(\sqrt{T}\right)$ regret. However, their regret is still dependent on the ergodicity coefficient and they require an offline generator that could sample a certain number of state-action pairs.  Osband et al., "Posterior Sampling for Solving Bandit Problems". Again their result is dependent on the ergodicity coefficient. This paper focuses on maximizing the cumulative reward within a fixed horizon, which distinguishes us from this line of research.

% For maximizing the finite-horizon cumulative reward, the problem involves a fixed horizon $T$ and a fixed number of episodes $K$. In this setting, an episode contains a run of restless bandit for $T$ steps and the goal is to maximize the cumulative reward over multiple episodes. Based on Thompson sampling, 
Auer et al., "The Non-Stationary K-Armed Bandit Problem". Designed algorithms that had good performance compared with Whittle's index policy, where  Garivier et al.,  "Optimal best-arm identification in multi-armed bandits", achieved $\mathcal{O}\left(\sqrt{T^4K}\right)$ regret. Based on UCB, 
Auer and Ortner, "Ucb revisited: Improved analysis of Thompson sampling for finite-horizon bandits". Developed an algorithm with $\mathcal{O}\left(\sqrt{T^3K}\right)$ regret upper bound. The relation between this paper and their works will be further discussed in Section \ref{subsec:online-restless-bandit}.


% Another line of research focuses on developing algorithms leveraging Q-learning and Whittle's index policy. In this line of research, maximizing the infinite horzion discounted reward is the objective. 
Osband et al., "Posterior Sampling for Solving Bandit Problems". Derived the first model-free approach for restless bandits and showed that as the number of iteration grows, their algorithm will converge to Whittle's index policy. Moving on, 
Antos et al., "Improved Regret Bounds for Posterior Sampling in Infinite Horizon Bandits". Designed algorithms that could have $\mathcal{O}(1/k^{2/3})$ convergence rate, where $k$ is the number of iterations. Recently,  Gheshlaghi et al.,  "Near-optimal Thompson Sampling Algorithm and Regret Analysis for Multi-armed Bandit Problem". Revised the algorithm and showed a faster convergence speed numerically.

\textbf{Thresholding Bandit/MDP\;\;} 
Our paper is closely related to regret-minimizing thresholding bandit problems, where regret is defined by the gap between the arm's mean and a predefined threshold when the mean falls below the threshold.
% Regret-minimization thresholding bandit problems, whose goal is minimizing the regret characterized by the gap between the arm below some predetermined threshold and the threshold is closely related to this paper. 
It is shown that under this setting, minimax optimal (max is taken over all possible bandit instances and min taken over all possible learning algorithms) constant regret is achievable Cai et al., "Regret Analysis of Thompson Sampling for Finite-Horizon Restless Bandits with Known Transition Matrices". 
% \kyra{what do we mean by minimax constant regret is achievable? it is optimal?}
Our problem setup differs from the traditional thresholding bandit framework by incorporating context, where the arrival of contexts follows an MDP.
% \kyra{check, maybe missing another point here}
% Instead of focusing on multi-armed bandit setting, this paper focuses on contextual bandit problem. 
% Furthermore, 
Cui et al., "Minimax Regret in Thresholding Bandits with Known Transition Matrices". Study threshold MDPs under an infinite horizon setting, seeking policies with average rewards exceeding a predefined threshold. We, however, focus on the finite-horizon setting.
% ____ study threshold MDP problems where the goal is to find a policy whose average reward is above some predetermined threshold.
 % Zhang et al., "Minimax Regret in Thresholding Bandits". Provides a constant regret bound when there is exactly one arm whose reward is above the threshold. Leveraging UCB algorithms,  Cai et al., "Regret Analysis of Thompson Sampling for Finite-Horizon Restless Bandits with Known Transition Matrices". Provides a regret upper bound that is only dependent on the gap between the arm below the threshold and the threshold. Recently, Gheshlaghi et al., "Near-optimal Thompson Sampling Algorithm and Regret Analysis for Multi-armed Bandit Problem". Improved the result and provided a upper bound that is only dependent on the gap between the arm above the threshold and the threshold. In their paper, they leveraged the lower confidence bound. However, their regret is still dependent on the ergodicity coefficient of the underlying MDP.  Cai et al., "Regret Analysis of Thompson Sampling for Finite-Horizon Restless Bandits with Known Transition Matrices". This result also applies to our problem.

\textbf{Online Restless Bandit\;\;} 
Our problem focuses on the finite horizon setting, and we contrast our results with prior finite-horizon regret guarantees in Section~\ref{subsec:online-restless-bandit}.

% There are two major lines of work. The first line of research is the model-based method, where the unknown transition matrix is estimated directly using the data collected. Among model-based methods, the works can be further divided into two groups based on their objective: maximizing the average reward in an infinite horizon or maximizing the cumulative reward in a finite horizon. For maximizing average reward, 
Kleinberg and Tardos, "Revised Revisited Second Edition: How Much to Invest When the Payoff is Random". However, their result only applies to the case when the transition matrix is known. This paper focuses on the case when the transition matrix is unknown.

% For maximizing the finite-horizon cumulative reward, the problem involves a fixed horizon $T$ and a fixed number of episodes $K$. In this setting, an episode contains a run of restless bandit for $T$ steps and the goal is to maximize the cumulative reward over multiple episodes. Based on Thompson sampling, 
Auer et al., "The Non-Stationary K-Armed Bandit Problem". Designed algorithms that had good performance compared with Whittle's index policy, where  Garivier et al.,  "Optimal best-arm identification in multi-armed bandits", achieved $\mathcal{O}\left(\sqrt{T^4K}\right)$ regret. Based on UCB, 
Auer and Ortner, "Ucb revisited: Improved analysis of Thompson sampling for finite-horizon bandits". Developed an algorithm with $\mathcal{O}\left(\sqrt{T^3K}\right)$ regret upper bound. The relation between this paper and their works will be further discussed in Section \ref{subsec:online-restless-bandit}.


% Another line of research focuses on developing algorithms leveraging Q-learning and Whittle's index policy. In this line of research, maximizing the infinite horzion discounted reward is the objective. 
Osband et al., "Posterior Sampling for Solving Bandit Problems". Derived the first model-free approach for restless bandits and showed that as the number of iteration grows, their algorithm will converge to Whittle's index policy. Moving on, 
Antos et al., "Improved Regret Bounds for Posterior Sampling in Infinite Horizon Bandits". Designed algorithms that could have $\mathcal{O}(1/k^{2/3})$ convergence rate, where $k$ is the number of iterations. Recently,  Gheshlaghi et al.,  "Near-optimal Thompson Sampling Algorithm and Regret Analysis for Multi-armed Bandit Problem". Revised the algorithm and showed a faster convergence speed numerically.

% There are two major lines of work. The first line of research is the model-based method, where the unknown transition matrix is estimated directly using the data collected. Among model-based methods, the works can be further divided into two groups based on their objective: maximizing the average reward in an infinite horizon or maximizing the cumulative reward in a finite horizon. For maximizing average reward, 
Hassani et al., "Improved Regret Bounds for Exploring-Epsilon-Greedy Algorithm". However, the constant is exponential in the number of arms and states. Meanwhile, the constant is also dependent on the ergodicity coefficient of the underlying MDP.  Tschiatschek et al., "PAC-Bayes bounds via data-dependent priors" improved the dependence of regret on the number of arms and states, but they have worse dependence on the horizon $T$, which is $\mathcal{O}(T^{2/3})$. However, they assumed that the transition matrix is not dependent on action. After that, Agrawal et al., "Improved Regret Bounds for Partially Observed Bandits" improved the results and reduce the dependence on instance-dependent constants. To be specific, their algorithms had $\mathcal{O}\left(\sqrt{T}\right)$ regret. However, their regret is still dependent on the ergodicity coefficient and they require an offline generator that could sample a certain number of state-action pairs.  Osband et al., "Posterior Sampling for Solving Bandit Problems". Again their result is dependent on the ergodicity coefficient. This paper focuses on maximizing the cumulative reward within a fixed horizon, which distinguishes us from this line of research.

% For maximizing the finite-horizon cumulative reward, the problem involves a fixed horizon $T$ and a fixed number of episodes $K$. In this setting, an episode contains a run of restless bandit for $T$ steps and the goal is to maximize the cumulative reward over multiple episodes. Based on Thompson sampling, 
Auer et al., "The Non-Stationary K-Armed Bandit Problem". Designed algorithms that had good performance compared with Whittle's index policy, where  Garivier et al.,  "Optimal best-arm identification in multi-armed bandits", achieved $\mathcal{O}\left(\sqrt{T^4K}\right)$ regret. Based on UCB, 
Auer and Ortner, "Ucb revisited: Improved analysis of Thompson sampling for finite-horizon bandits". Developed an algorithm with $\mathcal{O}\left(\sqrt{T^3K}\right)$ regret upper bound. The relation between this paper and their works will be further discussed in Section \ref{subsec:online-restless-bandit}.


% Another line of research focuses on developing algorithms leveraging Q-learning and Whittle's index policy. In this line of research, maximizing the infinite horzion discounted reward is the objective. 
Osband et al., "Posterior Sampling for Solving Bandit Problems". Derived the first model-free approach for restless bandits and showed that as the number of iteration grows, their algorithm will converge to Whittle's index policy. Moving on, 
Antos et al., "Improved Regret Bounds for Posterior Sampling in Infinite Horizon Bandits". Designed algorithms that could have $\mathcal{O}(1/k^{2/3})$ convergence rate, where $k$ is the number of iterations. Recently,  Gheshlaghi et al.,  "Near-optimal Thompson Sampling Algorithm and Regret Analysis for Multi-armed Bandit Problem". Revised the algorithm and showed a faster convergence speed numerically.

\textbf{Online Restless Bandit\;\;} 
Online restless bandit algorithms typically fall into two categories: model-based and model-free methods. The former directly estimates the transition matrix in both finite-horizonPuterman et al., "Markov Decision Processes: Discrete Stochastic Dynamic Programming" and infinite-horizon Puterman, "Discounted Markov Decision Problems",  settings, whereas the latter focuses on learning the Q-function specifically in the infinite-horizon setting Sutton and Barto, "Reinforcement Learning: An Introduction".
% This paper is closely related to the online restless bandits problem, where the transition matrix and the reward of the MDP is unknown and has to be learned sequentially.
% \kyra{here}
Our problem focuses on the finite horizon setting, and we contrast our results with prior finite-horizon regret guarantees in Section~\ref{subsec:online-restless-bandit}.

% There are two major lines of work. The first line of research is the model-based method, where the unknown transition matrix is estimated directly using the data collected. Among model-based methods, the works can be further divided into two groups based on their objective: maximizing the average reward in an infinite horizon or maximizing the cumulative reward in a finite horizon. For maximizing average reward, 
Hassani et al., "Improved Regret Bounds for Exploring-Epsilon-Greedy Algorithm". However, the constant is exponential in the number of arms and states. Meanwhile, the constant is also dependent on the ergodicity coefficient of the underlying MDP.  Tschiatschek et al., "PAC-Bayes bounds via data-dependent priors" improved the dependence of regret on the number of arms and states, but they have worse dependence on the horizon $T$, which is $\mathcal{O}(T^{2/3})$. However, they assumed that the transition matrix is not dependent on action. After that, Agrawal et al., "Improved Regret Bounds for Partially Observed Bandits" improved the results and reduce the dependence on instance-dependent constants. To be specific, their algorithms had $\mathcal{O}\left(\sqrt{T}\right)$ regret. However, their regret is still dependent on the ergodicity coefficient and they require an offline generator that could sample a certain number of state-action pairs.  Osband et al., "Posterior Sampling for Solving Bandit Problems". Again their result is dependent on the ergodicity coefficient. This paper focuses on maximizing the cumulative reward within a fixed horizon, which distinguishes us from this line of research.

% For maximizing the finite-horizon cumulative reward, the problem involves a fixed horizon $T$ and a fixed number of episodes $K$. In this setting, an episode contains a run of restless bandit for $T$ steps and the goal is to maximize the cumulative reward over multiple episodes. Based on Thompson sampling, 
Auer et al., "The Non-Stationary K-Armed Bandit Problem". Designed algorithms that had good performance compared with Whittle's index policy, where  Garivier et al.,  "Optimal best-arm identification in multi-armed bandits", achieved $\mathcal{O}\left(\sqrt{T^4K}\right)$ regret. Based on UCB, 
Auer and Ortner, "Ucb revisited: Improved analysis of Thompson sampling for finite-horizon bandits". Developed an algorithm with $\mathcal{O}\left(\sqrt{T^3K}\right)$ regret upper bound. The relation between this paper and their works will be further discussed in Section \ref{subsec:online-restless-bandit}.


% Another line of research focuses on developing algorithms leveraging Q-learning and Whittle's index policy. In this line of research, maximizing the infinite horzion discounted reward is the objective. 
Osband et al., "Posterior Sampling for Solving Bandit Problems". Derived the first model-free approach for restless bandits and showed that as the number of iteration grows, their algorithm will converge to Whittle's index policy. Moving on, 
Antos et al., "Improved Regret Bounds for Posterior Sampling in Infinite Horizon Bandits". Designed algorithms that could have $\mathcal{O}(1/k^{2/3})$ convergence rate, where $k$ is the number of iterations. Recently,  Gheshlaghi et al.,  "Near-optimal Thompson Sampling Algorithm and Regret Analysis for Multi-armed Bandit Problem". Revised the algorithm and showed a faster convergence speed numerically.

% There are two major lines of work. The first line of research is the model-based method, where the unknown transition matrix is estimated directly using the data collected. Among model-based methods, the works can be further divided into two groups based on their objective: maximizing the average reward in an infinite horizon or maximizing the cumulative reward in a finite horizon. For maximizing average reward, 
Kleinberg and Tardos, "Revised Revisited Second Edition: How Much to Invest When the Payoff is Random". However, their result only applies to the case when the transition matrix is known. This paper focuses on the case when the transition matrix is unknown.

% For maximizing the finite-horizon cumulative reward, the problem involves a fixed horizon $T$ and a fixed number of episodes $K$. In this setting, an episode contains a run of restless bandit for $T$ steps and the goal is to maximize the cumulative reward over multiple episodes. Based on Thompson sampling, 
Auer et al., "The Non-Stationary K-Armed Bandit Problem". Designed algorithms that had good performance compared with Whittle's index policy, where  Garivier et al.,  "Optimal best-arm identification in multi-armed bandits", achieved $\mathcal{O}\left(\sqrt{T^4K}\right)$ regret. Based on UCB, 
Auer and Ortner, "Ucb revisited: Improved analysis of Thompson sampling for finite-horizon bandits". Developed an algorithm with $\mathcal{O}\left(\sqrt{T^3K}\right)$ regret upper bound. The relation between this paper and their works will be further discussed in Section \ref{subsec:online-restless-bandit}.


% Another line of research focuses on developing algorithms leveraging Q-learning and Whittle's index policy. In this line of research, maximizing the infinite horzion discounted reward is the objective. 
Osband et al., "Posterior Sampling for Solving Bandit Problems". Derived the first model-free approach for restless bandits and showed that as the number of iteration grows, their algorithm will converge to Whittle's index policy. Moving on, 
Antos et al., "Improved Regret Bounds for Posterior Sampling in Infinite Horizon Bandits". Designed algorithms that could have $\mathcal{O}(1/k^{2/3})$ convergence rate, where $k$ is the number of iterations. Recently,  Gheshlaghi et al.,  "Near-optimal Thompson Sampling Algorithm and Regret Analysis for Multi-armed Bandit Problem". Revised the algorithm and showed a faster convergence speed numerically.

% There are two major lines of work. The first line of research is the model-based method, where the unknown transition matrix is estimated directly using the data collected. Among model-based methods, the works can be further divided into two groups based on their objective: maximizing the average reward in an infinite horizon or maximizing the cumulative reward in a finite horizon. For maximizing average reward, 
Hassani et al., "Improved Regret Bounds for Exploring-Epsilon-Greedy Algorithm". However, the constant is exponential in the number of arms and states. Meanwhile, the constant is also dependent on the ergodicity coefficient of the underlying MDP.  Tschiatschek et al., "PAC-Bayes bounds via data-dependent priors" improved the dependence of regret on the number of arms and states, but they have worse dependence on the horizon $T$, which is $\mathcal{O}(T^{2/3})$. However, they assumed that the transition matrix is not dependent on action. After that, Agrawal et al., "Improved Regret Bounds for Partially Observed Bandits" improved the results and reduce the dependence on instance-dependent constants. To be specific, their algorithms had $\mathcal{O}\left(\sqrt{T}\right)$ regret. However, their regret is still dependent on the ergodicity coefficient and they require an offline generator that could sample a certain number of state-action pairs.  Osband et al., "Posterior Sampling for Solving Bandit Problems". Again their result is dependent on the ergodicity coefficient. This paper focuses on maximizing the cumulative reward within a fixed horizon, which distinguishes us from this line of research.

% For maximizing the finite-horizon cumulative reward, the problem involves a fixed horizon $T$ and a fixed number of episodes $K$. In this setting, an episode contains a run of restless bandit for $T$ steps and the goal is to maximize the cumulative reward over multiple episodes. Based on Thompson sampling, 
Auer et al., "The Non-Stationary K-Armed Bandit Problem". Designed algorithms that had good performance compared with Whittle's index policy, where  Garivier et al.,  "Optimal best-arm identification in multi-armed bandits", achieved $\mathcal{O}\left(\sqrt{T^4K}\right)$ regret. Based on UCB, 
Auer and Ortner, "Ucb revisited: Improved analysis of Thompson sampling for finite-horizon bandits". Developed an algorithm with $\mathcal{O}\left(\sqrt{T^3K}\right)$ regret upper bound. The relation between this paper and their works will be further discussed in Section \ref{subsec:online-restless-bandit}.


% Another line of research focuses on developing algorithms leveraging Q-learning and Whittle's index policy. In this line of research, maximizing the infinite horzion discounted reward is the objective. 
Osband et al., "Posterior Sampling for Solving Bandit Problems". Derived the first model-free approach for restless bandits and showed that as the number of iteration grows, their algorithm will converge to Whittle's index policy. Moving on, 
Antos et al., "Improved Regret Bounds for Posterior Sampling in Infinite Horizon Bandits". Designed algorithms that could have $\mathcal{O}(1/k^{2/3})$ convergence rate, where $k$ is the number of iterations. Recently,  Gheshlaghi et al.,  "Near-optimal Thompson Sampling Algorithm and Regret Analysis for Multi-armed Bandit Problem". Revised the algorithm and showed a faster convergence speed numerically.

\textbf{Online Restless Bandit\;\;} 
Our problem focuses on the finite horizon setting, and we contrast our results with prior finite-horizon regret guarantees in Section~\ref{subsec:online-restless-bandit}.