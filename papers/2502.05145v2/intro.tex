Restless bandits extend classic contextual bandits by modeling state transitions and incorporating budget constraints, providing a framework for dynamic decision-making in nonstationary environments. In this framework, a set of independent arms (agents) evolves over discrete time steps according to a Markov Decision Process (MDP), regardless of whether they are actively played or passively rested. The goal is to dynamically allocate a fixed number of active plays to maximize cumulative rewards over time. While much of the existing literature focuses on offline settings, where transitions and rewards are assumed known \citep{whittle1988restless,hong2024achievingexponentialasymptoticoptimality}, online restless bandits, where these parameters must be learned, have gained traction due to their practical relevance \citep{jung2019regret}.


% Restless bandits 
% provides a powerful framework for dynamic decision-making under resource constraints,
% \citep{brown2020index},

% with applications spanning 
% % extend
% % classic contextual bandit problems by
% % modeling transitions and uncertainty in states under budget constraints. This framework captures long-term behavior, making it non-myopic, and accounts for the system's nonstationarity, making it well-suited to real-world applications with resource constraints,
% % such as
% healthcare, 
% % \citep{villar2015multi,biswas2021learn}, 
% resource allocation,
% % \citep{borkar2017opportunistic}, 
% and beyond.
% Unlike classic contextual bandits, restless bandits model state transitions and capture long-term behavior. This makes them particularly well-suited for real-world problems where interventions are costly, resources are limited, and outcomes evolve over time.
% other domains requiring dynamic decision-making \citep{brown2020index}.
This framework is well-suited for real-world applications where interventions are costly, resources are limited, and outcomes evolve over time.
For example, in healthcare, restless bandits have been used to optimize targeted interventions~\citep{villar2015multi,biswas2021learn}, such as guidance calls in mobile health programs to improve engagement in automated prenatal care \citep{mate2021}.
% Similarly, t
They have also been applied to resource allocation problems \citep{borkar2017opportunistic} and other 
% domains requiring
adaptive decision-making \citep{brown2020index}.  In practice, interventions are often rolled out gradually, or there is abundant observational data from periods without intervention~\citep{Liu2014, Shaikh2019, mate2021,perianez2024reform},
enabling the estimation of
% This provides a rich foundation for estimating 
state evolution dynamics in the absence of intervention.
Further, these settings often operate under finite horizons, where the experimentation period for new interventions is constrained by factors such as budget limitations, client-imposed timeframes, evolving market conditions, or changing health guidelines~\citep{delrío2024, delrío2024Pharmacy}.
Table~\ref{tab:examples} provides a broader overview of real-world applications where restless bandits naturally arise.
\begin{table*}[!th]
\centering
\renewcommand{\arraystretch}{1.2} % Adjusts row height for better readability
\caption{Examples of good and bad states and corresponding budgeted interventions in various domains}
\label{tab:fixedwidth}
\vspace{5pt}
{\hyphenpenalty=10000\exhyphenpenalty=10000
\begin{tabular}{
K{0.105\textwidth} 
K{0.26\textwidth} 
K{0.25\textwidth} 
K{0.28\textwidth}
}
\hline
\textbf{Domain} & \textbf{Good state} & \textbf{Bad state} & \textbf{Budgeted Intervention Example} \\
\hline

General Usage & 
Frequent logins, feature exploration, meaningful use & 
Rare/brief logins, session abandonment & 
Reach out, offer help, gather feedback \\

E-commerce & 
High purchase frequency, browsing multiple categories & 
Low/no purchases, high cart abandonment & 
Provide a personalized discount or free shipping \\

E-learning & 
Completes modules, passes quizzes, revisits content & 
Seldom opens modules, fails quizzes & 
Progress‐based reward (finish by deadline for free certificate) \\

Loyalty & 
Completes surveys, redeems rewards, advances loyalty & 
Not collecting points, ignoring surveys, no rewards & 
Special bonus for feedback (earn points by reviewing new features) \\

Patient management & 
Updates records, completes follow‐ups, tracks adherence & 
Rarely logs updates, misses assessments or follow‐ups & 
Points or bonus for timely follow‐ups \\

\hline
\end{tabular}}
\label{tab:examples}
\end{table*}
% applied restless bandits in a mobile health program in India to incorporate limited targeted guidance calls from health workers
% % to pregnant women 
% to improve engagement in automated
% prenatal care. 
% Additional real-world applications where restless bandit naturally arises are included in Table~\ref{tab:examples}.
% preventive care information delivery programs. 

% \cf{\citep{mate2021} deployed restless bandits in a mobile health program to deliver automated preventive care information to pregnant women in India. The program incorporates targeted health worker interventions, where a limited subset of beneficiaries receive personalized guidance calls each week to enhance engagement. Other applications in healthcare emerged through controlled simulation environments using real-world data \citep{Lee2019,Prins2020,baek2024}. Digital interventions, in particular, represent a critical application area where restless and contextual bandits have demonstrated significant utility. 
% Table~\ref{tab:examples} illustrates the relationship between domain-specific states and corresponding budgeted interventions.
% }
% \kyra{citations for each application area.}
% \kyra{@africa: do you think we should add the motivating example that causal foundry has here?}
% \kyra{need to rewrite the intro to justify that the information under no intervention is fully known}


% \cf{Restless and contextual bandits are a powerful framework for modeling and optimizing sequential decision-making processes across diverse domains. The model's ability to capture state transitions and balance exploration-exploitation trade-offs has led to numerous real-world applications. In personalized recommendation systems, bandits have been successfully employed to model user preferences and engagement patterns (\citep{Lihong2010, bouneffouf2012, delrío2024, delrío2024Pharmacy}. In online education, restless bandits have proven valuable for adaptive learning and student engagement (\citep{Liu2014, Shaikh2019}. The healthcare sector has leveraged this framework for patient monitoring and treatment optimization (\citep{Yom-Tov2017, Liao2020}) and optimization of calls by health workers to support patients (\citep{mate2021}, this framework was applied to ARMMAN, an Indian NGO's mobile health program that delivers automated preventive care information to pregnant women and new mothers. The program incorporates targeted health worker interventions, where a limited subset of beneficiaries receive personalized guidance calls each week to enhance engagement.}



% In classical restless bandits, a set of independent arms (agents) evolves over discrete time steps according to an MDP, regardless of whether they are actively played or passively rested. 
% The goal is to dynamically allocate a fixed number of active plays to maximize a cumulative reward over time. 
% Most existing literature focuses 
% % Much of the majority of the existing literature focuses
% on the offline setting, where the transitions and rewards are assumed to be known \citep{whittle1988restless,hong2024achievingexponentialasymptoticoptimality}. 
% % \kyra{cite},
% However, online restless bandits, where these parameters must be learned, 
% have gained popularity recently due to their practicality in real-world applications \citep{jung2019regret}.

% In these applications, 

% Further, these applications often operates under a finite horizon setting where
% the experimentation period for new intervention is limited. This could potentially be caused by limited budget for costly intervention, time constraints imposed by clients, market condition or health guidelines that change over time. 
% Notably, online Whittle index algorithms have shown better performance among these approaches \citep{wang2023optimistic,akbarzadeh2023learning,killian2021q,kakarapalli2024faster}. 
% Specifically, this line of work is using the data collected to formulate Whittle's index policy.


% \cf{
% % Modern digital intervention tools continuously collect data as opposed to traditional single-point data collection methods \citep{tamura2024cardiology}. Moreover, interventions are rolled out gradually, and to prevent habituation, more users remain uninvolved at any given time than those receiving interventions \citep{perianez2024reform}. As a result, these tools typically have ample historical data to estimate state evolution dynamics in the absence of intervention.

% While interventions are carefully designed by behavioral scientists to increase the likelihood of users transitioning to a good state compared to control, the exact magnitude of their impact remains uncertain unless separate experiments to accurately learn state evolution dynamics are run. Given this uncertainty, the challenge is to allocate a finite intervention budget to maximize transitions into good states. Restless bandits provide an ideal framework for this problem because they account for the fact that agents continue to evolve even when no intervention is applied.

% Moreover, focusing on finite-horizon performance is also crucial in real-world applications. Firstly, many real-world interventions operate under strict deadlines and fixed budgets. Secondly, user behavior, market conditions, or health guidelines can change over time e.g., holiday seasons for e-commerce, disease seasonality in healthcare. These shifts mean that by focusing on a finite horizon, we conclude an intervention campaign before the scenario resets or evolves. And lastly, in e-learning or loyalty based interventions, there is often a finite life to each incentive. Users might tire of repeated prompts \citep{shani2017jit}, or resources may need to be reallocated to new programs.}

Despite their potential, solving online restless bandits in finite-horizon settings is
% are notoriously 
challenging, 
% to solve
as they require detailed knowledge of the underlying MDP for each agent \citep{jung2019regret,wang2020restless,wang2023optimistic}. 
In such settings, this 
% challenge 
% Given a finite horizon, this 
leads to an 
% As will be discussed later, this requirement leads to an 
unavoidable linear regret  
when maximizing cumulative rewards \citep{azar2017minimax}.
% when we are maximizing the finite-horizon cumulative reward. 
% \kyra{make this slightly more formal and add citation, and exactly why the existing solution methods fail}
% This inherent learning challenge 
These difficulties call for a reformulation of the problem to \emph{simplify the learning objective}, enabling the development of solutions that are not only optimal for the reformulated problem but also demonstrate improved empirical performance in finite samples, even in the online restless bandit setting.

% \kyra{the rest needs to be structured as "our contributions are xx fold"}
\textbf{Our contributions are three-fold.} {First},
% to ease learning, 
we reformulate
% provide a novel  \emph{reformulation} of 
% the
online restless bandits as a \emph{budgeted thresholding contextual bandit}, simplifying learning through carefully designed rewards. 
% Specifically, 
% the objective is to
This reformulation focuses on
identifying agents whose action benefits
% benefit from taking action
exceed a predefined threshold at each time step (Section~\ref{section:restless-bandit}).
% to address this challenge and improve performance in the online problem mentioned above, we propose reformulating the learning task into a contextual thresholding bandit problem. Specifically, the objective becomes identifying agents at each time step for which the benefit of taking action exceeds a predefined threshold. 
We establish the optimality of this reformulation in a simple 2-state, \emph{finite horizon} setting (Theorem \ref{th:optimality-greedy-homogeneous}) where agents are homogeneous and giving intervention is always better than no intervention (Assumption \ref{asssumption:stochastic-dominance}).  
% We verify the optimality of this reformulation for some choices of the threshold under the stochastic dominance assumption (Assumption \ref{asssumption:stochastic-dominance}) and that all agents are homogeneous. This assumption intuitively makes sure that giving intervention is always better. 
We note that this simple setting has been applied to many real-world settings \citep{wang2023scalable,killian2023robust,liang2024bayesian,raman2024global},
% \kyra{cite}, 
highlighting the utility of our reformulation.  
% 
% We note that given we are assuming that giving intervention is more costly by imposing the budget constraint, it is reasonable to further assume that giving intervention is better. 
% This will be discussed in detail in Section \ref{subsec:optimality}. 
% We emphasize that this is the first attempt to reformulate the restless bandit problem into a contextual bandit problem. 
% Furthermore, unlike Whittle's index policy \citep{whittle1988restless} and fluid LP-based methods \citep{zhang2021restless} where only asymptotic optimality has been established, the greedy policy we present is shown to be optimal for any number of agents. 
% Meanwhile, our policy do not need to solve linear programming nor dynamic programming, thus, is easy to implement. 
% 
% \kyra{need to bring in the logic we introduced in each section to state the results and explain their importance, justify why assumptions are reasonable in applications}

{Second}, assuming knowledge of outcomes under no intervention, we propose a novel learning algorithm for heterogeneous agents in multi-state settings: the \emph{lower confidence bound} (LCB)-guided randomized thresholding algorithm (LGRT, Algorithm~\ref{alg:'name'}, Section~\ref{section:thresholding-bandit-algorithm}). We establish that LGRT achieves  optimal minimax constant regret under our reformulation
% the 
% budgeted thresholding contextual bandit
% formulation 
(Theorem \ref{th:sublinearregret}), enabled by
% providing
a novel design of the LCB for the reward empirical mean. 

 % 
% we prove that the regret of the proposed algorithm compared with baseline policy we define in Section \ref{subsec:contextual_bandit} achieves the optimal minimax constant regret. To achieve this minimax regret, we leverage the lower confidence bound and provide a novel design of the lower confidence bound for the empirical mean.
{Lastly}, we numerically demonstrate (in Section \ref{sec:experiments}) that a greedy variant of our proposed algorithm,  Algorithm~\ref{alg:lcb-greedy}, consistently outperforms existing online restless bandit algorithms 
% \kyra{maybe introduce this somewhere before here?}\jm{Before the `` despite the potential''} 
in \emph{finite-horizon online restless bandit settings} for instances satisfying stochastic dominance assumption (Assumption~\ref{asssumption:stochastic-dominance}), validating its effectiveness and practical utility.
We further evaluate the performance of our algorithm in the absence of the stochastic dominance assumption. We modify both our algorithm and the benchmarks, allowing them to allocate actions to fewer than 
B agents when additional actions are expected to yield negative benefits. 
Our numerical results demonstrate that our modified algorithm, Algorithm~\ref{alg:lcb-greedy-modified}, outperforms the adapted benchmarks, highlighting the practicality of our framework.
% Further, we further modify the algorithm to only give action to the agents whose action benefits is positive (Algorithm~\ref{alg:lcb-greedy-modified}). We also numerically demonstrate that Algorithm~\ref{alg:lcb-greedy-modified} consistently outperforms existing online restless bandit algorithms 
% % \kyra{maybe introduce this somewhere before here?}\jm{Before the `` despite the potential''} 
% in \emph{finite-horizon online restless bandit settings} for instances \emph{not} satisfying stochastic dominance assumption, further illustrating its practical utility.
% % \kyra{be more explicit about the improvement once section 5 is done}
% % The results are shown in Section \ref{sec:experiments}.
% % \kyra{point to section number somewhere here}

% % Third, we test our algorithm's performance numerically. Empirically, our algorithm consistently outperforms existing online Whittle index algorithms in finite-horizon scenarios, validating its effectiveness and practical utility.
