\documentclass{article}
\usepackage{tikz,bm,bbm,algorithmic}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\T}{\mathsf{T}} 
\newcommand{\greedy}{\text{greedy}}
\newcommand{\D}{\Delta} 
\newcommand{\HDT}{\hat{\Delta}_t} 
\usepackage{array}    % Enables text wrapping with p{}
\newcolumntype{K}[1]{>{\raggedright\arraybackslash}p{#1}}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs, enumitem} % for professional tables
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,       % Enable colored links
    linkcolor=blue,        % Color for internal links (e.g., sections)
    citecolor=blue,        % Color for citations
    filecolor=blue,        % Color for file links
    urlcolor=blue          % Color for URLs
}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
% Use the following line for the initial blind version submitted for review:
 \usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}
% For theorems and such
\usepackage{amsmath}
\allowdisplaybreaks
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{libertine}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{thm-restate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{From Restless to Contextual}
%\SetKwComment{Comment}{/* }{ */}
\newcommand{\kyra}[1]{{\color{purple}{Kyra: #1}}}
\newcommand{\jm}[1]{{\color{red}{Jiamin: #1}}}
\newcommand{\cf}[1]{{\color{cyan}{#1}}}
\begin{document}
\onecolumn
\icmltitle{From Restless to Contextual: A Thresholding Bandit Approach to Improve Finite-horizon Performance}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jiamin Xu}{yyy}
\icmlauthor{Ivan Nazarov}{comp}
\icmlauthor{Aditya Rastogi}{comp}
\icmlauthor{África Periáñez}{comp}
\icmlauthor{Kyra Gan}{sch}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}
\icmlaffiliation{yyy}{Cornell University, Ithaca, New York}
\icmlaffiliation{comp}{Causal Foundry, Barcelona, Spain}
\icmlaffiliation{sch}{Cornell Tech, Cornell University, New York City, New York}

\icmlcorrespondingauthor{Kyra Gan}{kyragan@cornell.edu}
\icmlcorrespondingauthor{Jiamin Xu}{jx389@cornell.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Online Restless Bandit, Contextual Threshold Bandit, Regret Minimization, Finite-horizon Optimization}

\vskip 0.3in


% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    % 1,reduction to bandit problem;2,give an algorithm that is minimax optimal for threshold bandit problem;3,numerical verification
    % This approach captures long-term behavior and system nonstationarity, making it highly applicable to domains like healthcare and resource allocation.
Online restless bandits extend classic contextual bandits by incorporating state transitions and
% and uncertainty
% under
budget constraints, representing each agent as a Markov Decision Process (MDP). 
This framework is crucial for \emph{finite-horizon} strategic resource allocation, optimizing limited costly interventions for long-term benefits.
% enabling optimal deployment of limited costly interventions for long-term benefits.
However, learning the underlying MDP for each agent poses a major challenge in finite-horizon settings.
% solving such problems
% is challenging due to the need to learn the underlying MDP for each agent.
% To address learning
% under finite horizon, 
To facilitate learning, we reformulate the problem as a scalable 
\emph{budgeted thresholding contextual bandit} problem. We carefully integrate the state transitions into the reward design and
focus on identifying agents with action benefits exceeding 
a threshold. 
We establish the optimality of an oracle greedy solution
% to the reformulation 
in a simple two-state setting, and propose an algorithm that achieves minimax optimal constant regret in
the online multi-state setting with heterogeneous agents and knowledge of outcomes under no intervention.
% , our proposed algorithm achieves optimal minimax constant regret. 
We numerically show that
our algorithm outperforms existing online restless bandit methods, offering significant improvements in finite-horizon performance.

% In a simple two-state setting, we show that an oracle greedy solution to the reformulation 
% optimally solves
% % provides the optimal solution to 
% the offline restless bandit problem under 
% stochastic dominance and homogeneous agents.
% additional assumptions.
% In the offline setting, under stochastic dominance and homogeneous agent assumptions, this approach provides the optimal offline restless bandit solution. 
    % Restless bandit problems have been proven efficient in applications like resource allocation, recommender systems and other areas. We consider online restless bandits problems with the goal of maximizing the cumulative reward within a fixed horizon. Solving restless bandits in an online fashion require knowledge on the unknown underlying MDP environment, which could be very costly when the horizon we are considering is short. Therefore, to improve the finite-horizon performance, we first introduce a way to reformulate the problem into a contextual thresholding bandit. To be specific, we show that under stochastic dominance assumption and that all arms are homogeneous, our reduction to contextual bandit is valid. Then, we provide an algorithm for the contextual thresholding bandit problem, which achieves the sub-linear regret upper bound. Empirically, we demonstrate that our algorithm could outperform the existing online Whittle index algorithms.
\end{abstract}
 \section{Introduction}
 \input{intro}


\subsection{Related Work}
% \textbf{Offline Restless Bandit\;\;} 
The offline restless bandit problem, with known transition matrices and reward functions, is 
% a 
PSPACE-hard to solve due to the combinatorial nature of the budget constraint 
% combinatorial optimization problem 
\citep{b0e74184-2114-3e45-b092-dfbc8fefcf91}.
% When the transition matrix and the reward function of the MDP are given, the restless bandit problem is a combinatorial optimization problem, that is 
% % Since this optimization problem is shown to be 
% PSPACE-hard to solve
% exactly 
% \citep{b0e74184-2114-3e45-b092-dfbc8fefcf91}.
Many approximation algorithms have been proposed,
including Whittle's index policy~\citep{whittle1988restless, Weber_Weiss_1990, 10.1109/ISIT.2018.8437712,7852321} and linear programming-based methods~\citep{verloop2016asymptotically,zhang2021restless,brown2023fluid,hong2024achievingexponentialasymptoticoptimality,gast2024linear}, both of which are proven to be optimal as the number of agents approaches infinity.
Whittle's index policy is commonly used as a \emph{benchmark} for evaluating online restless bandit algorithms~\citep{jung2019regret,wang2020restless, wang2023optimistic,akbarzadeh2023learning}. We provide a brief overview in Section~\ref{subsec:restless_bandits} and a detailed explanation in Appendix~\ref{app:whittle}.
In this paper, we focus on the online setting, where transitions and rewards must be learned from data.
% The hardness behind the optimization problem lies in the budget constraint.
% One line of research focuses on the Lagrangian relaxation of the constraint. Based on this, Whittle first introduces Whittle's index policy \citep{whittle1988restless}. The asymptotic optimality of Whittle's index policy is established in \citealt{Weber_Weiss_1990}. Meanwhile, it is also shown that Whittle's index could provide a scalable solution to real-life applications of restless bandit problems \citep{10.1109/ISIT.2018.8437712,7852321}. 
% Another line of research focuses on relaxing the original optimization problem of linear programming. Based on this relaxation, fluid linear programming-based policies are proposed \citep{zhang2021restless,hong2024achievingexponentialasymptoticoptimality,brown2023fluid,gast2024linear,verloop2016asymptotically} and asymptotic optimality has been established.
% An recent paper \citep{xiong2025finite} establishes a new type of policy under an additional constraint that all arms could only be pulled once.w

% This paper, on the other hand, is trying to find the optimal policy without assuming knowing the environment of the MDP.

\textbf{Online Restless Bandit\;\;} 
Online restless bandit algorithms typically fall into two categories: model-based and model-free methods. The former directly estimates the transition matrix in both finite-\citep{jung2019regret,wang2023optimistic,liang2024bayesian} and infinite-horizon \citep{10.1007/978-3-642-34106-9_19, wang2020restless, xiong2022learning,xiong2022reinforcement, akbarzadeh2023learning} settings, whereas the latter focuses on learning the Q-function specifically in the infinite-horizon setting \citep{killian2021q, xiong2023finite,xiong2024whittle, kakarapalli2024faster}.
% This paper is closely related to the online restless bandits problem, where the transition matrix and the reward of the MDP is unknown and has to be learned sequentially.
% \kyra{here}
Our problem focuses on the finite horizon setting, and we contrast our results with prior finite-horizon regret guarantees in Section~\ref{subsec:online-restless-bandit}.

% There are two major lines of work. The first line of research is the model-based method, where the unknown transition matrix is estimated directly using the data collected. Among model-based methods, the works can be further divided into two groups based on their objective: maximizing the average reward in an infinite horizon or maximizing the cumulative reward in a finite horizon. For maximizing average reward, \citealt{10.1007/978-3-642-34106-9_19} provides an algorithm that has $\mathcal{O}\left(\sqrt{T}\right)$ regret. However, the constant is exponential in the number of arms and states. Meanwhile, the constant is also dependent on the ergodicity coefficient of the underlying MDP. \citealt{wang2020restless} improved the dependence of regret on the number of arms and states, but they have worse dependence on the horizon $T$, which is $\mathcal{O}(T^{2/3})$. However, they assumed that the transition matrix is not dependent on action. After that, \citealt{xiong2022learning,xiong2022reinforcement} improved the results and reduce the dependence on instance-dependent constants. To be specific, their algorithms had $\mathcal{O}\left(\sqrt{T}\right)$ regret. However, their regret is still dependent on the ergodicity coefficient and they require an offline generator that could sample a certain number of state-action pairs. \citealt{akbarzadeh2023learning} focused on learning specifically the Whittle's index policy and provided an algorithm with regret $\mathcal{O}\left(\sqrt{T}\right)$. Again their result is dependent on the ergodicity coefficient. This paper focuses on maximizing the cumulative reward within a fixed horizon, which distinguishes us from this line of research.

% For maximizing the finite-horizon cumulative reward, the problem involves a fixed horizon $T$ and a fixed number of episodes $K$. In this setting, an episode contains a run of restless bandit for $T$ steps and the goal is to maximize the cumulative reward over multiple episodes. Based on Thompson sampling, \citealt{jung2019regret,liang2024bayesian} designed algorithms that had good performance compared with Whittle's index policy, where \citealt{jung2019regret} achieved $\mathcal{O}\left(\sqrt{T^4K}\right)$ regret. Based on UCB, \citealt{wang2023optimistic} developed an algorithm with $\mathcal{O}\left(\sqrt{T^3K}\right)$ regret upper bound. The relation between this paper and their works will be further discussed in Section \ref{subsec:online-restless-bandit}.


% Another line of research focuses on developing algorithms leveraging Q-learning and Whittle's index policy. In this line of research, maximizing the infinite horzion discounted reward is the objective. \citealt{killian2021q} derived the first model-free approach for restless bandits and showed that as the number of iteration grows, their algorithm will converge to Whittle's index policy. Moving on, \citealt{xiong2023finite,xiong2024whittle} designed algorithms that could have $\mathcal{O}(1/k^{2/3})$ convergence rate, where $k$ is the number of iterations. Recently, \citealt{kakarapalli2024faster} revised the algorithm and showed a faster convergence speed numerically.

\textbf{Thresholding Bandit/MDP\;\;} 
Our paper is closely related to regret-minimizing thresholding bandit problems, where regret is defined by the gap between the arm's mean and a predefined threshold when the mean falls below the threshold.
% Regret-minimization thresholding bandit problems, whose goal is minimizing the regret characterized by the gap between the arm below some predetermined threshold and the threshold is closely related to this paper. 
It is shown that under this setting, minimax optimal (max is taken over all possible bandit instances and min taken over all possible learning algorithms) constant regret is achievable \citep{tamatsukuri2019guaranteed,michel2022regret,feng2024satisficing}. 
% \kyra{what do we mean by minimax constant regret is achievable? it is optimal?}
Our problem setup differs from the traditional thresholding bandit framework by incorporating context, where the arrival of contexts follows an MDP.
% \kyra{check, maybe missing another point here}
% Instead of focusing on multi-armed bandit setting, this paper focuses on contextual bandit problem. 
% Furthermore, 
In contrast, \citet{hajiabolhassan2023online} study threshold MDPs under an infinite horizon setting, seeking policies with average rewards exceeding a predefined threshold. We, however, focus on the finite-horizon setting.
% \citealt{hajiabolhassan2023online} study threshold MDP problems where the goal is to find a policy whose average reward is above some predetermined threshold.
 %\citealt{tamatsukuri2019guaranteed} provides a constant regret bound when there is exactly one arm whose reward is above the threshold. Leveraging UCB algorithms, \citealt{michel2022regret} provides a regret upper bound that is only dependent on the gap between the arm below the threshold and the threshold. Recently, \citealt{feng2024satisficing} improved the result and provided a upper bound that is only dependent on the gap between the arm above the threshold and the threshold. In their paper, they leveraged the lower confidence bound. However, their works are focusing on multi-armed bandit setting while ours is focusing on contextual bandit problems. 
 % We note that our problem differs from theirs since we are considering the finite-horizon setting instead. 

% \kyra{here, and need to make the connection between our problem and EMDP more clear}
\textbf{Episodic MDP\;\;} When both the budget and the number of arms in an online restless bandit are set to one, the problem reduces to a standard reinforcement learning problem. Thus, our work also relates to episodic MDP, which focuses on maximizing cumulative rewards across 
% learning Q-functions
a finite horizon. Let $K$ be the number of episodes, each containing $T$ time steps. 
\citet{azar2017minimax} establishes a minimax regret of $\mathcal{O}\left(\sqrt{T^2K}\right)$ for stationary transitions, and \citet{jin2018q} derives an upper bound of $\mathcal{O}\left(\sqrt{T^4K}\right)$ for nonstationary transitions. 
These results highlight the difficulty of the learning problem under a finite horizon, emphasizing the need for a reformulation to simplify the learning objective.
% the number of budget and the number of arms are all one in restless bandit problem, then it is reduced to the reinforcement learning problem. 
% Therefore, our work is related to Episodic Markov Decision Process where it consists of $K$ episodes and within each episode, it contains a run of MDP for $T$ steps.
% The goal is to maximize the cumulative reward across $K$ episodes.
% Assuming that the transition matrix is stationary, \citealt{azar2017minimax} achieves a minimax $\mathcal{O}\left(\sqrt{T^2K}\right)$ regret bound. Moving on, \citealt{jin2018q} achieves a $\mathcal{O}\left(\sqrt{H^4K}\right)$ regret upper bound when the transition matrix is non-stationary.
% \subsection{Contributions}
% Our contributions are threefold:
% \begin{enumerate}[noitemsep, topsep=1pt]
% \item To improve performance under a small horizon, we reduce the restless bandit problem into a contextual bandit problem and show the validity of this reduction under stochastic dominance assumption and homogeneous agents.
% \item We study the regret-minimization thresholding bandit problem in the contextual bandit setting and we present algorithm which achieves a minimax constant regret. To achieve a minimax constant regret, we leverage the lower confidence bound and provide a novel design of the lower confidence bound for the empirical mean. 
% \item We empirically show that our algorithm could achieve higher cumulative reward than existing algorithms.
% \end{enumerate}
\section{Restless Bandits and the Budgeted Thresholding Contextual Bandit Reformulation}\label{section:restless-bandit}
This section begins by formally defining the restless bandit problem with \emph{known transition probabilities and reward function} in Section~\ref{subsec:restless_bandits}.
% , followed by a description of Whittle's index policy, a commonly used solution approach.
In Section~\ref{subsec:online-restless-bandit}, we extend the problem to the online setting, where \emph{transition and reward distributions are unknown} and must be learned from data. We note that online restless bandit learning is data-intensive, making it impractical for budget-constrained settings with a finite horizon. To address this limitation, Section~\ref{subsec:contextual_bandit} presents our problem reformulation, and Section~\ref{subsec:optimality} characterizes the conditions under which this reformulation achieves optimality for the restless bandit problem in the offline setting.

% \textbf{Restless Bandits\;\;} 
\subsection{Restless Bandits}\label{subsec:restless_bandits}
% We consider
In an offline finite-horizon restless bandit problem, the learner is provided with $M$ \emph{independent} arms, each representing one independent agent. At each time $t\in [T] = \{1, \ldots, T\}$, each arm, $m\in[M]$, is associated with a state $s_t^m\in\mathcal{S}$, and an action $a_{t+1}^m\in\mathcal{A}$. 
% 
% Let $\mathcal{M}:=\{m: 1\leq m\leq M\}$ be the set of all agents, $\mathcal{S}$ be the state space, and $\mathcal{A}$ be the action space.  
We 
% will
focus on binary action,
% in this paper, 
% the case where actions are binary, 
i.e., $\mathcal{A}=\{0,1\}$. 
% Throughout the paper, we will use intervention to denote action $1$ and no intervention to denote action $0$.
Each agent $m$ is modeled as an
% independent 
MDP represented by the tuple $\left(\mathcal{S},\mathcal{A},P_a^m,R^m\right)$, where 
% $P_a^m$ is the transition probability matrix for agent $m$ under action $a$, with entries
$P_a^m(s,s'):\mathcal{S}\times \mathcal{S}\to[0,1]$ represents the transition probability from state $s$ to $s'$ under action $a$ for agent $m$,
and $R^m(s):\mathcal{S}\to\R$ represents the \emph{expected reward} that agent $m$ receives at state $s$. We highlight that in a restless bandit problem, the expected reward, $R^m(s)$, is \emph{independent} of the action, and the learner observes the reward \emph{immediately after arriving at} the state.
In the offline setting, both $P_a^m(s,s')$ and $R^m(s)$ are known to the learner.
% \kyra{add subgaussian assumption to theorem statement}. 

At each time $t$, 
the learner observes the set of states of $M$ agents, denoted by $\bm{s}_t:=\left(s_t^1,s_t^2,\cdots,s_t^M\right)$, and receives the corresponding reward $\bm{r}_t:=(r_t^1,\cdots, r_t^M)$. 
% Let $\boldsymbol{\mathcal{A}}$ denote the set of all possible action vectors, i.e.,  $\boldsymbol{\mathcal{A}}=\{0,1\}^M$.
Based on the observed states, the learner determines the action $\bm{a}_{t+1}:=\left(a_{t+1}^1,a_{t+1}^2,\cdots,a_{t+1}^M\right)$,  according to a deterministic policy $\pi^d_{t+1}(\bm{s}):\mathcal{S}^M\rightarrow 
\{0,1\}^M$.
% \kyra{add a sentence here talking about how the states transition to next time step.}
The learner is provided with a budget constraint, allowing at most $B$ arms to be pulled at each time step,
% the learner can pull at most $B$ arms,
i.e., $\sum_{m\in[M]} a_{t+1}^m\leq B$.
After pulling up to $B$ arms, the learner
% After choosing action $\bm{a}_{t+1}$, the learner can
observes the next state $\bm{s}_{t+1}$, 
% which is
generated according to $s_{t+1}^m\sim P_{a_{t+1}^m}^m\left(s_t^m,s_{t+1}^m\right)$, and 
% as well as
receives a feedback $\bm{r}_{t+1}$.
% Let $\bm{s}_t:=\left(s_t^1,s_t^2,\cdots,s_t^M\right)$ denote the observed states of $M$ agents at time $t$, and $\bm{a}_{t+1}:=\left(a_{t+1}^1,a_{t+1}^2,\cdots,a_{t+1}^M\right)$.
% At each time, given the state $\bm{s}_t\in\mathcal{S}^M$, the learner determines the action $\bm{a}_{t+1}\in\{0,1\}^M$, governed by a deterministic policy: $\pi_t(\bm{s}):\mathcal{S}^M\rightarrow\{0,1\}^M$. 
% In this paper, we will consider the setting where there is a fixed horizon $T$ and a fixed horizon $B$.
% At each time step until time $T$, the learner will observe the states of all agents as well as a reward $r_t^m$, which is a random variable defined as $r_t^m:=r^m\left(s_t^m\right)+\epsilon_t$.
% We will assume that $\epsilon_t$ are i.i.d 1-subgaussian random variable. 

Let $\bm s_0$ denote the initial states and $\bm r_0$ their associated rewards. 
% The goal of t
The learner aims to find the policy 
% $\pi$
$\bm{\pi}_d:=\{\pi^d_t\}_{t\in[T]}$
that maximizes the expected cumulative reward,
% 
% Then they will decide how to allocate intervention across all agents. The  learner could only give at most $B$ agents intervention at each time step. The objective of the restless bandit problem is to find the policy that could maximize the cumulative reward until time $T$, which is equivalent to solving the following optimization problem:
% \vspace{-6.48pt}
\begin{align}
    \max_{{\bm\pi}_d}\; &\E\left[\sum_{m=1}^M\sum_{t=0}^TR^m\left(s_t^m\right)\mid\bm{s}_0\right]\notag\\
    \text{s.t.~}&\sum_{m=1}^M a_{t}^m\leq B,\;\forall t\in [T],\label{eq:optimization-problem}
\end{align}
% \vspace{-0.02pt}
where the expectation is taken over ${s}_{t+1}^m \sim P_a^m(s_t^m, s_{t+1}^m)$ for all $m$. We will use $\bm{\pi}^*$ to denote the optimal policy corresponding to Problem~\eqref{eq:optimization-problem}. 
% the maximizer, which is the optimal policy. 

\textbf{Whittle's Index Policy\;\;}
% \kyra{complete}
% As the exact solution to Problem \ref{eq:optimization-problem} is PSPACE-hard to compute \citep{b0e74184-2114-3e45-b092-dfbc8fefcf91}, many approximation algorithms were proposed. Among these, Whittle's index policy \citep{whittle1988restless} is widely used for solving restless bandits in offline settings \kyra{cite}, and served as \emph{a key benchmark} for assessing online restless bandit algorithms \citep{jung2019regret,akbarzadeh2023learning,wang2020restless, wang2023optimistic},\kyra{maybe move some citations here to online} as discussed Section \ref{subsec:online-restless-bandit}.
% (see \citealt{whittle1988restless} for reference), 
% which is the commonly used benchmark for online restless bandits. This will be discussed in detail in Section \ref{subsec:online-restless-bandit}. 
% 
% A detailed description of Whittle's index policy is included in Appendix~\ref{app:whittle}. 
% Whittle's index
This policy provides an approximate solution to Problem~\eqref{eq:optimization-problem} by maximizing the \emph{average expected reward} as the horizon length approaches infinity~\eqref{eq:optimization-average-reward}.\footnote{Alternatively,
% We note that there is also a version of
Whittle's index policy can also be defined
for maximizing the discounted reward as the horizon approaches infinity~\eqref{eq:optimization-discounted-reward}, which we describe in
% we will leave the definition to 
Appendix~\ref{app:whittle}.} It incorporates the budget constraint using Lagrangian relaxation~\eqref{eq:optimization-average-reward-relaxation}, allowing the problem to be decoupled and solved individually for each agent.
% Whittle's index is computed for each agent. 
% At each time $t$, given $\bm s_t$, the top $B$ agents with the highest indices are selected.  
Whittle's index policy achieves the optimal solution to Problem~\eqref{eq:optimization-average-reward}
% ~\eqref{eq:optimization-problem} 
as the number of \emph{homogeneous agents} $M$ approaches infinity (Theorem~\ref{th:optimality-whittle}).
Let $\bm{\pi}^{\text{Whittle}}$ be Whittle's index policy obtained in the offline setting, corresponding to the solution to Eq.~\eqref{eq:optimization-average-reward-relaxation}. In Section~\ref{sec:experiments}, we benchmark the performance of our algorithm against that of $\bm{\pi}^{\text{Whittle}}$ in our problem instances.
% A detailed description of
We describe Whittle's index policy in detail
% is included 
in Appendix~\ref{app:whittle}. 
% $\bm{\pi}^{\text{Whittle}}$ is often used as a benchmark for evaluating the performance of online learning algorithms in online restless bandits as discussed in Section~\ref{subsec:online-restless-bandit}.
% It reformulates Problem~\ref{eq:optimization-problem} into a 
% Whittle's index policy is designed to approximately find the policy $\bm{\pi}_d$ that maximizes the average reward,
% \begin{align}
%     \max_{\bm{\pi}_d}~&\liminf_{T\to\infty}\frac{1}{TM}\E\left[\sum_{m=1}^M\sum_{t=0}^TR^m\left(s_t^m\right)\mid\bm{s}_0
%     % \mid \bm{s}_0=(0,0,\cdots,0)
%     \right]\notag\\
%     \text{s.t.~}&\sum_{m=1}^M a_t^m\leq B,\forall t\in[T]\label{eq:optimization-average-reward}.
% \end{align}
% We will denote $J^*$ to be the optimal value of Problem \ref{eq:optimization-average-reward}. Instead of directly solving Problem \ref{eq:optimization-average-reward}, Whittle deals with the Lagrangian relaxation of Problem \ref{eq:optimization-average-reward}, which is stated in Problem \ref{eq:optimization-average-reward-relaxation}
% \begin{equation}
%     \max_{\bm{\pi}_d}~\liminf_{T\to\infty}\frac{1}{TM}\E\left[\sum_{m=1}^M\sum_{t=0}^TR^m\left(s_t^m\right)+\lambda\left(1-a_t^m\right)\mid\bm{s}_0
%     % \mid \bm{s}_0=(0,0,\cdots,0)
%     \right].\label{eq:optimization-average-reward-relaxation}
% \end{equation}
% The key observation to derive the Whittle's index policy is that Problem \ref{eq:optimization-average-reward-relaxation} can be solved by addressing the following dynamic programming separately for each agent
% \begin{align*}
%     V^m_\lambda(s)&=\max_{a\in\{0,1\}} Q^m(s,a)\\
%     Q^m_\lambda(s,a)+\beta^*_m&=r(s)-\lambda\mathbbm{1}\{a=1\}\\&+\sum_{s'\in\mathcal{S}}P^m_a(s,s')V^m(s'),
% \end{align*}
% where $\beta^*_m$ is the maximized average reward for arm $m$ without any constraint, i.e.
% \begin{equation*}
%     \beta^*_m:=\max~ \liminf_{T\to\infty}\frac{1}{TM}\E\left[\sum_{t=0}^TR^m\left(s_t^m\right)\mid s_0^m
%     % \mid \bm{s}_0=(0,0,\cdots,0)
%     \right].
% \end{equation*}
% The Whittle's index $\lambda^m_s$ is defined as the multiplier $\lambda_s^m$ when it is equally favorable between choosing action $1$ and action $0$, i.e. $Q^m_{\lambda^m_s}(s,1)=Q^m_{\lambda^m_s}(s,0)$. The Whittle's index policy would then give action to $B$ agents with the highest Whittle's index at each time step $t$. Denote this as $\bm{\pi}^{\text{Whittle}}$ and the average reward of Whittle's index policy as $J^{\text{Whittle}}$. The performance of the Whittle's index policy is characterized in Theorem \ref{th:optimality-whittle}.
% \begin{restatable}{theorem}[Theorem 2, \citealt{Weber_Weiss_1990}]\label{th:optimality-whittle}
% Suppose all arms are homogeneous, i.e., $P_a^m=P_a^{m'}, R^m=R^{m'},\forall m\neq m'$, then the Whittle's index policy is asymptotically optimal,
% \begin{equation*}
%     \lim_{M\to\infty}J^{\text{Whittle}}=\lim_{M\to\infty}J^*.
% \end{equation*}
% \end{restatable}

\subsection{Online Restless Bandits}
\label{subsec:online-restless-bandit}
% \textbf{Online Restless Bandits\;\;}
In this setting,
% an online restless bandit, 
the transition matrices (and/or reward functions) need to be learned from the data. As a result,
at each time $t$, given
the state $\bm{s}_t$, 
the learner's decision $\bm{a}_{t+1}$ follows a history-dependent random policy, $\pi_{t+1}$. 
% In this section, we will further define the online restless bandit problems. Similar to the offline restless bandit defined in Section \ref{section:restless-bandit}, the learner is provided with a fixed budget of $B$ per time step and a fixed horizon of $T$. 
Let
$\mathcal{H}_{t}:=\{(\bm{a}_{i}, \bm{s}_i, \bm{r}_i)\}_{i=1}^{t} \cup \{\bm s_0, \bm r_0\}$ be the history observed up to time $t$, and let $\mathcal{H}_{t}^+:=\mathcal{H}_{t} \cup \{\bm a_{t+1}\}$ be the augmented history up to taking action $\bm a_{t+1}$.
% 
% 
% $\mathcal{H}_{t}:=\{(\bm{s}_i, \bm{r}_i,\bm{a}_{i+1})\}_{i=0}^{t-1}$
% % $\mathcal{H}_t:=\left\{\bm{s}_0,\bm{r}_1,\bm{a}_2,\bm{s}_2,\bm{r}_2,\cdots,\bm{s}_t,\bm{r}_t,\bm{a}_{t+1}\right\}$
% be the history observed up to taking action $\bm a_t$, and let 
% % $\mathcal{H}_{t-1}^-:=\mathcal{H}_{t-1}\cup
% % \left\{\bm{s}_t,\bm{r}_t\right\}$
% $\mathcal{H}_{t-1}^-:=\{(\bm{a}_{i}, \bm{s}_i, \bm{r}_i)\}_{i=1}^{t-1} \cup \{\bm s_0, \bm r_0\}$ 
% be the augmented history observed up to time $t-1$, including the current state $\bm{s}_t$ and reward $\bm{r}_t$
% %
% {\color{red}{
% Above it is written that at time $t$ in state $s_t$ ``after pulling $B$ arms, the agent observes $s_{t+1}$ and receives feedback $r_{t+1}$''. Shouldn't we thus attribute the action at $s_t$ to $t$ rather than ${t+1}$? 
% kyra: yes, let me make some changes
% ivan: in my notes i usually write $(s_t, a_{t+1}, r_{t+1})_{t \in [T]}$. because it conveys the notion of time order $s_t, a_{t+1} \overset{\text{env}}{\to} s_{t+1}, r_{t+1}$. but this is could just my quirk, which can be safely ignored if it contradicts the established time attribution.
% agreed: better for the reader to have multiple ways to understand the sequence in which things are happening
% %
% ivan: as for the policy we can write $\omega_{t-1}, s_t \overset{\text{pol}}{\to} \omega_{t+1}, a_{t+1}$ (if $\omega_{t+1}$ is the internal state of the policy). But i want to emphasize that this my notation can be ignored if it is inconvenient
% %
% Wouldn't it make more sense to 
% $\mathcal{H}_t^+  % we list the sars' in time-reversed order
%     := \left\{
%     \bm{s}_{t  },
%     \bm{r}_{t  }, a_{t-1}, \bm{s}_{t-1},  % due to `t-1 \to t` transition:
%     % above you say `after pulling arms, the agent obserserves `s_{t+1}` and receives feedback `r_{t+1}`
%     % 
%     \bm{r}_{t-1}, a_{t-2}, \bm{s}_{t-2},
%     \ldots \right\}$
% }}.
% \jm{I think since our reward only depends on the state, we do observe the reward first before taking the action.}
% %%kyra: i think might be a good idea to shift action index by 1, we start from state s_0, and get r_0, then take a_1?
% %% the reward is only dependent on the state, so we get reward when we ARRIVE AT (emphasis from ivan) the state, the action changes the transition to the states in the next time period
% %yeah, will emphasize this, as this is difference between mdp and restless bandit
% ivan: but we observe the reward for the previous action and use this feedback for proper credit assignment
% ivan: why do we get $r_0$? initial state $s_0$, then we take action $a_0$ and receive $r_1$ for it, and the env now is at state $s_1$...
% \jm{In \citealt{wang2023optimistic}, they did not even define the history and in \citealt{jung2019regret}, their reward is state-action dependent, so that's ths usual way of defining the history.}
% % action dependent reward is a bit strange since the budget constraint is defined separately; 
% % ivan: i disagree: the whittle index derivation introduces the lagrange multiplier used to adjust the reward at the next state and is essentially 0 if a \neq 1 and \lamdba otherwise -- so action -dependent rewards simply incorporate the cost of the taken action
% %yeah, but that's the reformulated problem, in the vanilla problem that we are trying to solve, introducing observed reward as a function of action is a bit strange, that's what i am trying to say
% % ivan: understood
% %okay, let's shift index of action by 1.
% % or is this the established timing from the restless literature? (personally, i do not like dissonance in the notation and nomenclature between bandit literature and RL. imo, restless bandits are especially egregious when compared to multi-agent RL; but if it is established, then i have no problem with it, because the readers will have mostly restless MAB background)
% % yeah this is established, but i do think the notation can be further clarified to highlight this.  the restless bandit literature notation/assumption are a bit all over the place
% % that settles it then
% ivan: i also made some changes to the algo pseudocode\jm{I saw them. Will make the notation consistent.}
% %
Then,
% At each time step $t$, given the state 
% $\bm{s}_t$, 
% the learner decides the action $\bm{a}_{t+1}$ according to a random policy 
$\pi_{t+1}:\mathcal{H}_{t}\to\Delta{\left(\{0,1\}^M\right)}$, where
% $\mathcal{H}_{t-1}^+:=\left\{\bm{s}_0,\bm{r}_1,\bm{a}_2,\bm{s}_2,\bm{r}_2,\cdots,\bm{s}_t,\bm{r}_t\right\}$ and
$\Delta{\left(\{0,1\}^M\right)}$ denotes the probability simplex over all possible actions.
% % Again, the learner could only pull at most $B$ arms. 
% After pulling at most $B$ arms, the learner
% % After choosing action $\bm{a}_{t+1}$, the learner can
% observe the next state $\bm{s}_{t+1}$, 
% % which is
% generated according to $s_{t+1}^m\sim P_{a_{t+1}^m}^m\left(s_t^m,s_{t+1}^m\right)$, and 
% % as well as
% receives a feedback $\bm{r}_{t+1}$.
Suppose that the reward $\bm{r}_{t+1}$ is generated according to a distribution $\mathcal{P}_t$, we will assume stationarity of the \emph{expected} reward received at time $t+1$, $\bm{r}_{t+1}$, when conditioned on history $\mathcal{H}_{t}^+$ and the current state.
% the stationarity of the expectation of $\bm{r}_{t+1}$. To be specific, we will assume that when conditioned on the previous observation $\mathcal{H}_{t}:=\{(\bm{s}_i, \bm{r}_i,\bm{a}_i)\}_{i=1}^t$, and state $\bm{s}_{t+1}$, the expectation of the feedback $r_{t+1}^m$ remains a constant, i.e. 
\begin{assumption}\label{assump:stationarity}
    $\E\left[r_{t+1}^m\mid s_{t+1}^m, \mathcal{H}_t^+\right]=R^m\left(s_{t+1}^m\right).$
\end{assumption}
% \begin{equation}
%     \E\left[r_{t+1}^m\mid s_{t+1}^m, \mathcal{H}_t\right]=R^m\left(s_{t+1}^m\right).
% \label{eq:stationarity}
% \end{equation}
We permit non-stationarity in the rewards, provided that Assumption~\ref{assump:stationarity} is satisfied, offering a relaxation of the standard stationarity assumption typically imposed in online restless bandit problems. 

Similar to the offline setting, the goal of the learner is to find the policy $\bm{\pi}:=\{\pi_t\}_{t\in[T]}$ that maximizes the expected cumulative reward, where the expectation is further taken over the randomized policy ($\bm{a}_{t}\sim \pi_{t}$), in addition to the states ($s_{t+1}^m\sim P^m_{a_{t+1}^m}(s_t^m,s_{t+1}^m)$): 
\begin{align}
\vspace{-5pt}
    \max_{{\bm\pi}}\; &\E_{\bm{\pi}}\left[\sum_{m=1}^M\sum_{t=0}^TR^m\left(s_t^m\right)\mid\bm{s}_0
    % \mid \bm{s}_0=(0,0,\cdots,0)
    \right]\notag\\
    \text{s.t.~}&\sum_{m=1}^M a_{t}^m\leq B,\;\forall t\in [T].\label{eq:optimization-problem-online}
\end{align}
% where the expectation is taken over $\bm{a}_{t+1}\sim \pi_t, s_{t+1}^m\sim P^m_{a_{t+1}^m}(s_t^m,s_{t+1}^m)$. 
\textbf{Online Whittle's Index Policy and Hardness of the Finite-Horizon Problem\;\;}
% Online Restless Bandits Under Finite Horizon\;\;} 
The online Whittle's index policy often involves estimating the uncertainty associated with the transition probabilities from data and using it to obtain an online Whittle's index policy \citep{akbarzadeh2023learning, wang2023optimistic}. 
% \kyra{cite; }
% are they forming CIs or just plugging in? is it true that the rewards are often remain known in this setting? are there benchmarks that we can use that are already implemented unknown reward?} 
% \kyra{complete}
% To evaluate the performance of a fixed policy $\bm{\pi}$, we need to compare the cumulative reward of policy $\bm{\pi}$ with the cumulative reward of the optimal policy $\bm{\pi}^*$. However, as mentioned in Section \ref{subsec:restless_bandits}, $\bm{\pi}^*$ is computationally hard to track. Therefore, Whittle's index policy is often used in literature as the baseline policy (see, i.e., \citealt{jung2019regret,akbarzadeh2023learning,wang2020restless,xiong2022learning,xiong2022reinforcement};\citealt{wang2023optimistic} for reference). 
% Furthermore, the high level idea of existing online algorithms to address restless bandits is estimating the transition probability $P_a^m$ and uses that to calculate the Whittle's index policy. 
However, in a \emph{finite horizon setting}, the challenge of learning transition probabilities becomes even more complex due to the
% imposed
budget constraints. To formally highlight the difficulty of the learning problem within the online restless bandit framework,
% we introduce the concept of regret. 
we examine
% focus on 
the regret of the online Whittle's index policy as an illustrating example. 
% 
Given a horizon length of $T$ and an online Whittle's index policy $\bm\omega$, the regret $\mathcal{R}_{f}(\bm{\omega})$ is defined as follows:\footnote{Oftentimes, in online restless bandits, the horizon is segmented into episodes, each with a fixed horizon length $T$. A generalized definition of regret, along with its discussion, is included in Appendix~\ref{app:generalized_regret}.}
% To evaluate the performance of the online approximation of the Whittle's index policy, the notion of regret is defined, which is directly comparing with the finite-horizon performance of the Whittle index policy and average them among $K$ number of episodes.
$$
\mathcal{R}_{f}(\bm{\omega}):=
% \frac{1}{K}\sum_{k=1}^K
% \Bigg(
\E_{\bm{\pi}^{\text{Whittle}}}\left[\sum_{m=1}^M\sum_{t=0}^TR^m\left(s_t^m\right)\mid\bm{s}_0
    % \mid \bm{s}_0=(0,0,\cdots,0)
    \right]\notag-\E_{\bm{\omega}}\left[\sum_{m=1}^M\sum_{t=0}^TR^m\left(s_t^m\right)\mid\bm{s}_0
    % \mid \bm{s}_0=(0,0,\cdots,0)
    \right].
$$
% \begin{align}\label{eq:regret-finite-horizon}
% \mathcal{R}_{f}(\bm{\omega}):&=
% % \frac{1}{K}\sum_{k=1}^K
% % \Bigg(
% \E_{\bm{\pi}^{\text{Whittle}}}\left[\sum_{m=1}^M\sum_{t=0}^TR^m\left(s_t^m\right)\mid\bm{s}_0
%     % \mid \bm{s}_0=(0,0,\cdots,0)
%     \right]\notag\\&-\E_{\bm{\omega}}\left[\sum_{m=1}^M\sum_{t=0}^TR^m\left(s_t^m\right)\mid\bm{s}_0
%     % \mid \bm{s}_0=(0,0,\cdots,0)
%     \right].
%     % \Bigg).
% \end{align}
% where $\bm{\pi}^{\text{Whittle}}$ is Whittle's index policy obtained in the offline setting. 
% Next, we observe that 

\citet{wang2023optimistic} establishes an upper bound of $\mathcal{O}(T^{3/2})$ on $\mathcal{R}_{f}(\bm{\omega})$, and \citet{azar2017minimax} establishes a lower bound of  $\Omega(T)$ when $B=M=1$. This highlights that a sublinear regret in the online restless bandit problem is impossible to achieve, highlighting the difficulty of this learning problem.
% \kyra{more discussion is needed here to state the optimality is under what assumptions?}

% \jm{here:} Before we end this subsection, we will discuss more on the regret. For any online algorithm, the regret is defined as the difference between the cumulative reward of the algorithm and a fixed baseline policy. Therefore, the regret can be seen as a measure of the convergence rate of the algorithm to the baseline policy. As we said, for online Whittle's index policy, the regret lower bound is linear in $T$, which means that the convergence rate is slow and when the horizon is short, the performance of any online Whittle's index policy is possibly worse than some suboptimal policy. This motivates us to reformulate this problem where it is possible to find an algorithm with sublinear regret in the reformulated problem since this will translates to a faster convergence rate and a better performance in low horizon setting provided that the reformulated problem is close to the original problem in terms of cumulative reward.

% establish that learning in the online restless bandit is fundamentally hard: given a finite horizon, it is impossible to obtain a sublinear regret.

% \textbf{Hardness of Online Restless Bandits Under Finite Horizon\;\;} 
% % \kyra{complete, citing the finite horizon restless bandit result here}
% % \kyra{making the observation that the sample complexity in learning online restless bandits is the same as learning an MDP (could in lit review)}
% For regret bound of existing online Whittle's index policy, it will only be reasonable when the number of episodes is sufficiently large. To illustrate this, \citet{wang2023optimistic} show that there algorithm has a regret bound of the order $\mathcal{O}\left(T\sqrt{TK}\right)$. If we only have $1$ episode, this will be equivalent to saying that the regret bound is $\mathcal{O}(T^{3/2})$. 
% \citet{wang2023optimistic} show that 
% The hardness lies in estimating the MDP, in \citealt{azar2017minimax}, they show that when $B=M=1$, the lower bound of the regret is $\Omega(T\sqrt{K})$. This lower bound indicates that when directly dealing with a MDP, it is impossible to get a regret bound that is sublinear in $T$. 

% In contrast, the contextual bandit 

% Therefore this motivates us to reformulate the problem into a contextual bandit problem.
\subsection{Budgeted Thresholding Contextual Bandit Reformulation}
\label{subsec:contextual_bandit}
% As discussed in Section~\ref{subsec:online-restless-bandit}, the finite horizon setting common in many real-world applications fundamentally constrains the learnability of the optimal policy for the online restless bandit problem. 
% To mitigate this issue, 
Thus motivated, we propose to simplify the learning objective by making the following modifications:
% to the problem formulation:
\begin{enumerate}[label=\textbf{\alph*}),noitemsep, topsep=1pt]
    \item We reformulate the problem as a contextual bandit problem, where the transition probabilities no longer need to be estimated explicitly;\label{modif:contextual}
    \item Instead of finding the optimal policy that maximizes the expected cumulative rewards across $M$ agents over a horizon $T$, we focus on identifying arms (agents) at each time step where taking action (versus not) yields an effect size exceeding a predefined threshold. \label{modif:thr}
\end{enumerate}
We observe that \ref{modif:contextual} (to be introduced below) simplifies the learning problem by embedding the nonstationarity of the state in online restless bandits into the reward structure. Consequently, the resulting policy depends solely on the current context. In general, learning a contextual bandit requires significantly lower sample complexity compared to an MDP \citep{simchi2022bypassing}.  
Additionally, Modification~\ref{modif:thr} further simplifies the problem by reducing reward maximization to a threshold identification task, which is known to have a lower sample complexity
% \kyra{cite}
\citep{feng2024satisficing}.
We establish that
% Further, we note that
under suitable assumptions, our modification, under the knowledge of the transition and reward, recovers the optimal policy of the \emph{offline} restless bandit problem in Section~\ref{subsec:optimality} for any $M$. In contrast, Whittle's index policy achieves optimality only when the number of agents and horizon length tend to infinity in the offline setting.
% \kyra{check}
% \kyra{we should mention the fairness aspect of the second constraint so we are not favoring the agent with the highest reward but randomizing among the agents who are all above the threshold}

% To improve the finite performance of the learning algorithms in our problem setup,
% we reduce this problem to a 
\textbf{Regret-Minimizing Budgeted 
Thresholding Contextual Bandit 
Reduction\;\;}
In the reduced contextual bandit problem, 
% where
the context space is the state space. 
At time $t$, the reward $\phi^m$ that agent $m$ receives at state $s_t^m$ under action $a_{t+1}^m$
is defined as the \emph{expected reward} that the agent will receive in the next time period:
% , if starting at state $s$ under action $a$:
\begin{align}
    &\phi^m(s_t^m,a_{t+1}^m) := \sum_{s_{t+1}^m\in\mathcal{S}}P^m_{a_{t+1}^m}(s_t^m,s_{t+1}^m)R^m\left(s_{t+1}^m\right)\notag \\
    &\quad\quad= \sum_{s_{t+1}^m\in\mathcal{S}}\prob\left(s_{t+1}^m\mid  \mathcal{H}_t^+\right) \E\left[r_{t+1}^m\mid s_{t+1}^m, \mathcal{H}_t^+\right]\label{eq:phi_1}\\
    &\quad\quad= \E\left[r_{t+1}^m\mid\mathcal{H}_t^+\right].
    \label{eq:c_reward}
\end{align}
Eq.~\eqref{eq:phi_1} follows from Assumption~\ref{assump:stationarity} and the Markovian property of the transition probabilities. Eq.~\eqref{eq:c_reward} shows that the immediate reward, \(\phi^m(s_t^m, a_{t+1}^m)\), 
% in the contextual bandit problem
can be estimated without explicitly modeling the transition probabilities.
% This reward corresponds to the average received in the next time step after taking action \(a_{t+1}^m\) in state \(s_t^m\). 
It does not depend on the next state, consistent with the standard bandit reward formulation. 


% % The online procedure is exactly the same as what is defined in Section \ref{section:online-restless-bandit}. 
% Next, we want to verify that we could, indeed, estimate the reward $\phi^m$ that we defined for this reformulation.
% % Again, we have
% % Similar to the restless bandit problem, the learner in this contextual bandit reformulation is provided with a fixed budget of $B$ per time step and a fixed horizon of $T$. 
% % Let $\mathcal{H}_{t-1}^+:=\left\{\bm{s}_0,\bm{r}_1,\bm{a}_2,\bm{s}_2,\bm{r}_2,\cdots,\bm{s}_t,\bm{r}_t\right\}$ be the history observed up to time $t-1$, including the current state $s_t$ and reward $r_t$.
% % At each time step $t$, given the state $\bm{s}_t\in\mathcal{S}^M$, the learner decides the action $\bm{a}_{t+1}\in\{0,1\}^M$ according to a random policy $\pi_t:\mathcal{H}_{t-1}^+\to\Delta{\left(\{0,1\}^M\right)}$, where
% % % $\mathcal{H}_{t-1}^+:=\left\{\bm{s}_0,\bm{r}_1,\bm{a}_2,\bm{s}_2,\bm{r}_2,\cdots,\bm{s}_t,\bm{r}_t\right\}$ and
% % $\Delta{\left(\{0,1\}^M\right)}$ denotes the probability simplex over all possible actions.
% % % Again, the learner could only pull at most $B$ arms. 
% % After pulling at most $B$ arms, the learner
% % % After choosing action $\bm{a}_{t+1}$, the learner can
% % observe the next state $\bm{s}_{t+1}$, 
% % % which is
% % generated according to $s_{t+1}^m\sim P_{a_{t+1}^m}^m\left(s_t^m,s_{t+1}^m\right)$, and 
% % % as well as
% % receives a feedback $\bm{r}_{t+1}$.
% % We will assume that when conditioned on the previous observation $\mathcal{H}_{t}:=\{(\bm{s}_i, \bm{r}_i,\bm{a}_i)\}_{i=1}^t$, and state $\bm{s}_{t+1}$, the expectation of the feedback $r_{t+1}^m$ remains a constant, i.e. 
% % \begin{equation*}
% %     \E\left[r_{t+1}^m\mid s_{t+1}^m, \mathcal{H}_t\right]=R^m\left(s_{t+1}^m\right),
% % \end{equation*}
% % where $\mathcal{H}_t:=\left\{\bm{s}_0,\bm{r}_1,\bm{a}_2,\bm{s}_2,\bm{r}_2,\cdots,\bm{s}_t,\bm{r}_t,\bm{a}_{t+1}\right\}$.
% % Meanwhile, we will also assume that conditioned on the current action $\bm{a}_{t+1}$ and the current state $\bm{s}_{t}$, $\bm{s}_{t+1}^m$ is independent of the previous observation $\mathcal{H}_{t-1}$, i.e.
% % \begin{equation*}
% %     \prob\left(s_{t+1}^m\mid s_t^m,a_{t+1}^m,\mathcal{H}_{t-1}\right)=\prob\left(s_{t+1}^m\mid s_t^m,a_{t+1}^m\right).
% % \end{equation*}
% To be specific, we can have the following:
% \begin{align*}
%     % ivan: i try to write latex eqn code in as sparse and multiline fashion as possible so as not to wait for the doc to render the eqns
%     \E\left[r_{t+1}^m\mid\mathcal{H}_t\right]
%         &
%             = \sum_{s_{t+1}^m \in \mathcal{S}}
%                 \E\left[r_{t+1}^m\mid s_{t+1}^m, \mathcal{H}_t\right]
%                     \prob\left(s_{t+1}^m\mid \mathcal{H}_t\right)
%         \\
%         &
%             = \sum_{s_{t+1}^m \in \mathcal{S}}
%                 R^m\left(s_{t+1}^m\right) P_{a_{t+1}^m}^m\left(s_t^m,s_{t+1}^m\right)
%         \\
%         &
%             = \phi^m(s_{t+1}^m, a_{t+1}^m)
%         \,,
% \end{align*}
% where the first equality holds because of the stationarity of the feedback and the Markov property of the MDP.
% %
% This equation indicates that the conditional expectation of $r_{t+1}^m$ conditioned on $s_t^m$ and $a_{t+1}^m$ is indeed the reward $\phi^m\left(s_t^m,a_{t+1}^m\right)$ that we defined, which aligns with the contextual bandit problem. 
% \kyra{here}

As in the online restless bandits, at each time $t$, the learner observes $\bm s_t$ and the corresponding reward $\bm {r}_t$. Using these observations, the learner updates the estimated uncertainty associated with the estimated $\phi^m(s_{t-1}^m, a_{t}^m) \;\forall m\in[M]$ according to Eq.~\eqref{eq:c_reward}, and uses it to guide the action $\bm a_{t+1}$, with $\sum_{m\in[M]} a_{t+1}^m\leq B$. 
% Note that here $\bm{r}_t$ is used to estimate $\phi(s_{t-1}^m,a_t^m)$, which is a direct result of Eq.~\eqref{eq:c_reward}.

% \kyra{check; might want to explain again why the index is $t-1$ again here.}
%kyra: note for self: it is okay for the reward of the bandit to be defined as the expected reward at the next time period. the incremental reward determines the decision rule


The goal of the learner is to learn a policy that identifies and selects $B$ agents whose expected gain from taking the action (i.e., $a_{t+1}^m=1$) exceeds a predefined threshold in their current state $s_t^m$. To quantify this expected gain
% the expected gain from taking an action
in state $s\in\mathcal{S}$, we define $I^m(s):=\phi^m(s,1)-\phi^m(s,0)$, representing the \emph{incremental reward} an agent receives by taking the action versus not at state $s$.  
Let $\gamma>0$ be a predefined threshold. At time $t$, we say that an agent $m$ is in a good state if $I^m(s_t^m)\geq \gamma$. 

At each time $t$, let $G_t:=\{m\in[M]: I^m(s_t^m)\geq \gamma\}$ denote the set of agents currently in a good state. 
In this budgeted thresholding contextual bandit 
formulation, the optimal policy, $\bm\pi^b$, involves randomly selecting $B$ agents from $G_t$ for intervention 
% \jm{changed here}
when $|G_t|\geq B$ and selecting $B$ agents according to the ordering of incremental reward otherwise. 
% To ensure the validity of the policy, we make the following assumption:
% \begin{assumption}\label{asssumption:enough-good-arm}
%     At each time $t$, there are at least $B$ agents whose incremental reward is above 
%     % the predefined 
%     threshold $\gamma$.
%     % Assume that there exists a policy such that at each time-step, there are more than $B$ agents whose incremental reward is larger than the threshold.
% \end{assumption}
%In practice, Assumption~\ref{asssumption:enough-good-arm} can be satisfied by carefully choosing $\gamma$. 
For any policy $\bm\pi$, potentially learned through an online algorithm, the regret $\mathcal{R}(\bm\pi)$ is defined as: 
% The goal of the learner is to learn a policy that is defined as: at each time $t$, randomly give action to $B$ agents in the set $\{m \in  [M] \colon I^m\left(s_t^m\right)\geq \gamma\}$, where $I^m(s)=\phi^m(s,1)-\phi^m(s,0)$. We will use $\bm{\pi}^b$ to denote this baseline policy. To characterize the performance of a policy $\bm{\pi}$, we define the regret $\mathcal{R}(\bm{\pi})$ as 
\begin{equation}
% \vspace{-5pt}
    \mathcal{R}(\bm{\pi}):=\E_{\bm{\pi}}\left[\sum_{m=1}^M\sum_{t=0}^T c_t^m\left(a_{t+1}^m\right)|\bm{s}_0\right]-\E_{\bm{\pi}^b}\left[\sum_{m=1}^M\sum_{t=0}^T c_t^m\left(a_{t+1}^m\right)|\bm{s}_0\right],\label{eq:cumulative-regret}
\end{equation}
where the expectation is taken with respect to $\bm{a}_{t+1}\sim \pi_t,s_{t+1}^m\sim P_{a_{t+1}^m}^m(s_t^m,s_{t+1}^m)$, and
\begin{equation}
% \vspace{-5pt}
    c_t^m\left(a_{t+1}^m\right):=\left(\gamma-I^m(s_t^m)\right)
    \cdot\mathbbm{1}\left\{I^m(s_t^m)<\gamma\right\}\cdot\mathbbm{1}\left\{a_{t+1}^m=1\right\}.\label{eq:cost}
\end{equation}
Eq.~\eqref{eq:cost} shows that a loss is incurred at each step only when an action is taken on an agent who is \emph{not} in a good state. The incurred cost is determined by the difference between the threshold and the incremental reward, given by \( \gamma - I^m(s_t^m) \).
% This means that at each time step $t$, if we choose an agent $m$ who is at a state $s_t^m$ such that the incremental reward is below the threshold, then a cost $\gamma-s_t^m$ will be incurred. 
The goal of the learner is to minimize the cumulative regret, defined in Eq.~\eqref{eq:cumulative-regret}, within a fixed horizon $T$. 

We note that while learning algorithms can achieve sublinear regret in the reformulated thresholding contextual bandit setting, this does not guarantee 
% necessarily lead to
an improved regret bound for the online restless bandit problem, as the 
% baseline policy
{$\bm\pi^b$} may be suboptimal.
This highlights a fundamental trade-off between learnability and optimality,
% of a policy, which we 
further explored in Remark~\ref{remark:trade-off-gamma}, and
% Additionally, we 
empirically examined
% the finite-horizon performance of this reduction 
in Section~\ref{sec:experiments}.

Here, regret serves as a measure of the speed of convergence for the designed policy. 
As established in Subsection~\ref{subsec:online-restless-bandit}, the online finite-horizon restless bandit problem inherently suffers from linear regret, meaning any algorithm aiming for optimality (in the homogeneous agent setting and asymptotically) will necessarily exhibit slow convergence.
To achieve better finite-sample performance, i.e., faster convergence to a policy, we must relax the requirement for optimality, accepting that the policy might be suboptimal. 
In this context, we hypothesize that an algorithm designed for maximizing reward in the simplified contextual bandit setting could outperform slower-converging algorithms that target near-optimal performance. This is particularly relevant since the optimality of the online Whittle index policy remains unestablished in the heterogeneous agent setting. We numerically validate this hypothesis in Section~\ref{sec:experiments} and confirm that it holds true.

Before establishing the sublinear regret of our proposed algorithm for the thresholding contextual bandit problem in the heterogeneous agent case (Section~\ref{section:thresholding-bandit-algorithm}), we first demonstrate that this thresholding bandit formulation is \emph{optimal} in a 2-state setting with homogeneous agents under the assumption of stochastic dominance in the next section.
% To device an algorithm with better finite sample performance, i.e., convergence to a policy at a faster rate, this policy necessarily needs to be suboptimal.
% in this problem, we hope that an algorithm designed for maximizing reward in the simplified contextual bandit setting could potentially outperform slower-converging algorithms (with linear regret, as indicated in Subsection~\ref{subsec:online-restless-bandit }) that aim for near-optimal performance—especially since the optimality of the online Whittle index policy has not been established in the heterogeneous agent setting. We numerically examine this hypothesis in Section~\ref{sec:experiments } and establish that this is indeed the case.

% So instead of constructing an algorithm that could solve
% At a high level, we hypothesize that
% an algorithm converging to a potentially suboptimal policy at a faster rate than RL-based policies could yield a performance advantage in finite-sample settings. 
% % not 
% % % always
% % be optimal.
% \jm{However, we note that since we are considering maximizing the cumulative reward in a finite horizon setting, an algorithm with faster convergence rate to a suboptimal policy can potentially outperform algorithms with slower convergence rate to an optimal policy. As discussed in subsection~\ref{subsec:online-restless-bandit}, online whittle's index policy has linear regret which means a slow convergence rate. Therefore, the learning algorithm we proposed can be better in a finite horizon setting since it has a faster convergence rate.}
% % for the original problem.
% % Nevertheless, t


% We will further denote good agents in state 
% $s \in S$ by 
% $G_s = \left\{m \in [M] \colon I^m(s) \geq\gamma\right\}$ and $\mathcal{G} := \left\{(m,s): s\in\mathcal{S}\,, m \in G_s\right\}$ as all good agent-states.
% Formally, at each time $t$, let $\{m \in  [M] \colon I^m\left(s_t^m\right)\geq \gamma\}$
% Let $\mathcal{G}$ be the set of good agent-state pairs, i.e., $\mathcal{G}:=\{(m,s): m\in[M], s\in\mathcal{S}, I^m(s)\geq \gamma\}$.
% \kyra{here}
% % We will further denote $\mathcal{G}:=\left\{(m,s): m\in[M],s\in\mathcal{S},I^m(s)\geq\gamma\right\}$ as all good agents.
% We will further denote good agents in state $s \in S$ by $G_s = \left\{m \in [M] \colon I^m(s) \geq\gamma\right\}$ and $\mathcal{G} := \left\{(m,s): s\in\mathcal{S}\,, m \in G_s\right\}$ as all good agent-states.
% %
% {\color{red}{ivan: i changed the term from ``agent'' to ``agent-state'', because we keep pairs in that set.}}


\section{Optimality of Budgeted Thresholding Contextual Bandit
% Model 
Reduction}\label{subsec:optimality}
In this section, we assume \emph{binary states} and we further assume that all agents share a good state with higher $R^m$. Without loss of generality (WLOG), we will use 1 to denote the good state and 0 for the bad state. We establish that \emph{given oracle knowledge} of the transition probabilities and rewards, a natural greedy policy is optimal for the \emph{offline restless bandit problem} described in Section~\ref{subsec:restless_bandits} under two additional assumptions: 1) stochastic dominance (Assumption~\ref{asssumption:stochastic-dominance}) and 2) homogeneity of the agents. 
We note that the optimality of this natural greedy policy indicates that the contextual bandit provides an exact reformulation to the offline restless bandit problem under these conditions. 
% When the greedy policy is optimal, learning this policy aligns with the goal of the contextual bandit problem, which is ``choosing the agent with the highest reward''. In this case, the reward is the incremental reward $I$.

We first describe the greedy policy.
WLOG,
% Without loss of generality, 
suppose that at each time step $t$, the incremental rewards are ordered as $I^1\left(s_t^1\right)\geq I^2\left(s_t^2\right)\geq\cdots\geq I^M\left(s_t^M\right)$. The corresponding greedy policy $\pi_t^{\text{greedy}}: \mathcal{S}\rightarrow\{0,1\}^M$ is to give actions to the first $B$ agents. Formally, with a slight abuse of notation, let $\pi_t^{\text{greedy}}({s}_t^i)$ be the
$i$\textsuperscript{th} entry of $\pi_t^{\text{greedy}}$, defined as:
% , $\pi_t^{\text{greedy}}({s}_t^i)$, is defined as:
%
% In this section, our goal is to verify the validity of our reduction to the threshold bandit problem. To do this, we will first show that greedy policy in terms of the incremental reward $I$ is optimal. To be specific, without losing generality, assume that at time-step $t$, we have $I^1\left(s_t^1\right)\geq I^2\left(s_t^2\right)\geq\cdots\geq I^M\left(s_t^M\right)$. Then the greedy policy $\pi_t^{\text{greedy}}$ will be
\begin{equation*}
 \pi_t^{\text{greedy}}({s}_t^i)=\begin{cases}
        1&i\leq B\\
        0&i>B
    \end{cases}.
   % \left( \pi_t^{\text{greedy}}(\bm{s}_t)\right)_i=\begin{cases}
   %      1&i\leq B\\
   %      0&i>B
   %  \end{cases}.
\end{equation*}
Let $\bm\pi^{\text{greedy}}:=\{\pi_t^{\text{greedy}}\}_{t\in[T]}$.
% \jm{added here} 
We note that $\bm\pi^{\text{greedy}}=\bm{\pi}^b$ with suitable choices of $\gamma$. A trivial choice of $\gamma$ would be $\max_{m,s} I^m(s)$.
Next, to establish the optimality of $\bm \pi^{\text{greedy}}$ in the offline restless bandit setting, we introduce the stochastic dominance assumption below:
% (Assumption~\ref{asssumption:stochastic-dominance}).
% When the greedy policy is optimal, learning this policy aligns with the goal of the contextual bandit problem, which is ``choosing the agent with the highest reward''. In this case, the reward is the incremental reward $I$.
% To do this, we will first introduce Assumption \ref{asssumption:stochastic-dominance}. 
\begin{assumption}[Stochastic Dominance]\label{asssumption:stochastic-dominance}
    Assume that for any non-decreasing function $f(s)$ of $s\in\mathcal{S}$, the following holds for any $s\in\mathcal{S},m\in[M]$:
    \begin{equation}\label{eq:stochastic-dominance}
        \E_{s'\sim P_1^m(s,s')}\left[f(s')\right]\geq \E_{s'\sim P_0^m(s,s')}\left[f(s')\right].
    \end{equation}
\end{assumption}
Before interpreting Assumption \ref{asssumption:stochastic-dominance}, we provide
% we further discuss the interpretation of this assumption, we shall give 
a necessary and sufficient condition for its validity
% for Assumption \ref{asssumption:stochastic-dominance} to hold when there are only 
in the two-state case. 
\begin{restatable}{lemma}{iffDominance}\label{le:iff-for-dominance}
Assumption \ref{asssumption:stochastic-dominance} holds if and only if $P^m_1(s,1)\geq P^m_0(s,1)$ for any $s\in\mathcal{S},m\in[M]$.  
\end{restatable}
Lemma \ref{le:iff-for-dominance} (proof in Appendix~\ref{app:stochastic-dominance}) indicates that when there are two states, 
Assumption \ref{asssumption:stochastic-dominance}
is equivalent to  the condition that
% is equivalent to having 
the transition probability to state 1 under action 1 is always higher (better) than
% the transition probability 
under action 0. 
% \begin{remark}[The Stochastic Dominance Assumption]\label{remark:stochastic-dominance}
%     % We note that 
%     Assumption~\ref{asssumption:stochastic-dominance} holds across a wide range of real-world applications\kyra{@africa, can we make this more explicit here on application examples?}. Meanwhile, it was also adopted by \citet{wang2023optimistic} in their experimental section, which we thoroughly compare against in Section~\ref{sec:experiments}.
%     % \kyra{the need for this assumption for using budget to be optimal in offline setting} 
%     % Further, this assumption is natural, as 
%     Without this assumption, the optimal policy might involve never intervening at all
%     % not giving intervention at all 
%     (see detailed discussion following Lemma~\ref{le:use-all-budget}).
%     % as we will discuss in detail after Lemma \ref{le:use-all-budget}, without this assumption, the optimal policy could be not giving any intervention at all.
% \end{remark}
% \kyra{perhaps more discussion on the strength of the assumption here}
% \kyra{do we have more papers to cite here?}
%
% Furthermore, i

Intuitively, Assumption \ref{asssumption:stochastic-dominance} imposes a structure
% is imposing a certain structure
on the transitions to ensure that giving an intervention is consistently more beneficial than
% will always be better than
not giving one 
% even
in the future. 
This implies that in a system with a single agent and two states, the optimal solution is to \emph{always} provide intervention at every time step, which aligns with \(\bm{\pi}^{\text{greedy}}\).
This observation is formalized in Lemma \ref{le:optimality-one-agent} (proof in Appendix~\ref{app:optimality-one-agent}).
\begin{restatable}{lemma}{optimalOneAgent}\label{le:optimality-one-agent}
    Assuming $B=M=1$,
    % , meaning that there is only one agent. 
    % Then,
    % under
    and under
    Assumption \ref{asssumption:stochastic-dominance}, $\bm{\pi}^{\text{greedy}}=\bm{\pi}^*$.
\end{restatable}
Next, we establish the optimality of the greedy policy
% Now, we will move on to prove that
% $\bm{\pi}^{\text{greedy}}=\bm{\pi}^*$ 
in the case of multiple agents. 
First, in Lemma  \ref{le:use-all-budget} (proof in Appendix~\ref{app:use-all-budget}), we show
% First, we need to show 
that under Assumption~\ref{asssumption:stochastic-dominance},
utilizing the entire budget is always beneficial.
% \kyra{here}
% the stochastic dominance assumption, 
% it is always beneficial to use all the budget.
% , which is formalized in Lemma \ref{le:use-all-budget}.
\begin{restatable}{lemma}{useallbudget}\label{le:use-all-budget}
Given a budget $B\in[M]$, and
assuming 
% under
Assumption \ref{asssumption:stochastic-dominance}, 
the optimal policy $\bm{\pi}^*$ selects exactly $B$ agents to take action $1$,
ensuring full utilization of the budget. 
    % will give $B$ agents action $1$ for any $0<B\leq M$,
    % which means that the optimal policy will always use all of the budgets. 
\end{restatable}
\begin{remark}[The Stochastic Dominance Assumption]\label{remark:stochastic-dominance}
    % We note that 
    Assumption~\ref{asssumption:stochastic-dominance} holds across a wide range of real-world applications, {some of which are illustrated in Table~\ref{tab:examples}}.
    % \kyra{@africa, can we make this more explicit here on application examples?}. 
    Meanwhile, it was also adopted by \citet{wang2023optimistic} in their experimental section, which we thoroughly compare against in Section~\ref{sec:experiments}.
    Further, in the offline restless bandit literature, the budget constraint often directly requires the learner to spend all the budget~\citep{whittle1988restless}, which aligns with Assumption~\ref{asssumption:stochastic-dominance}. When Assumption~\ref{asssumption:stochastic-dominance} is violated—such as when $P_0^m(s, s')$ stochastically dominates $P_1^m(s, s')$—the optimal policy may favor not intervening at all. 
    % \kyra{the need for this assumption for using budget to be optimal in offline setting} 
    % Further, this assumption is natural, as 
    % Without this assumption, the optimal policy might involve never intervening at all
    % % not giving intervention at all 
    % (see detailed discussion following Lemma~\ref{le:use-all-budget}).
    % as we will discuss in detail after Lemma \ref{le:use-all-budget}, without this assumption, the optimal policy could be not giving any intervention at all.
\end{remark}


% We shall note that consider a simple case where Assumption \ref{asssumption:stochastic-dominance} does not hold: $P_0^m(s,s')$ stochastically dominate $P_1^m(s,s')$. In this case, we can use the same proof of Lemma \ref{le:use-all-budget} to show that the optimal policy should not give any intervention at each time step. However, in offline restless bandit literature, the budget constraint directly requires the learner to use all the budget \citep{whittle1988restless}. Therefore, this setting will not be valid for the scenario we created, which further illustrates why we need Assumption \ref{asssumption:stochastic-dominance}.

Building on Lemmas~\ref{le:iff-for-dominance}-\ref{le:use-all-budget}, Theorem~\ref{th:optimality-greedy-homogeneous} (proof in Appendix \ref{app:thm-optimality-greedy-homogeneous}) establishes the optimality of the greedy policy under homogeneous agents. 
% in the general setting.
% Now, we are ready to give the main theorem that shows the optimality of the greedy policy, which is formally stated in Theorem \ref{th:optimality-greedy-homogeneous}. The proof is deferred to Appendix \ref{app:thm-optimality-greedy-homogeneous}.
\begin{restatable}{theorem}{optimalityhomogeneous}\label{th:optimality-greedy-homogeneous}
Assuming all agents are homogeneous, under Assumption  \ref{asssumption:stochastic-dominance},
    % Further assume that all agents are homogeneous, then under Assumption \ref{asssumption:stochastic-dominance}, 
    $\bm{\pi}^{\text{greedy}}=\bm{\pi}^*$ for any $0\leq B\leq M<\infty$.
\end{restatable}
% \begin{remark}
% We further note that 
Theorem \ref{th:optimality-greedy-homogeneous} generally does not extend to heterogeneous agents, as demonstrated by the counterexample in Appendix \ref{sec:counter-example-heterogeneous}. Nevertheless, we conjecture that under some additional assumptions, Theorem \ref{th:optimality-greedy-homogeneous} could be approximately extended to the multi-state case, where
% . To be specific, we conjectured that 
the greedy policy may achieve a constant-factor approximation of the optimal policy.
% in multi-state cases.
% \end{remark}
% \kyra{
% % discuss that the homogeneous assumption is common across restless bandit literature, and that binary state setup is common in applications (add citations to numerical case studies in prior papers).
% State that we conjecture that the extension to multiple states might approximately hold and we leave it to future work. Note that this result does not hold for heterogeneous agents.}
\begin{remark}[Significance of Theorem~\ref{th:optimality-greedy-homogeneous}]\label{remark:significance_thm_optimality}
% We note that, 
%kyra: we are already in a remark environment so we don't need to start with we note that 
To our knowledge, this is the first result that establishes the optimality of a policy that holds for any number of agents in an offline finite-horizon restless bandit. Though we assume that all agents are homogeneous, 
this is common in
% this is a common assumption in
offline restless bandit problems \citep{hong2024achievingexponentialasymptoticoptimality,whittle1988restless,zhang2021restless}. 
Meanwhile,
in real-world scenarios, such as with the ARMMAN dataset, researchers often simplify the problem by treating it as a two-state system
% in common applications to real-life datasets like ARMMAN, researchers will model the problem as a two-state problem 
\citep{liang2024bayesian,killian2023robust,wang2023scalable}.
\end{remark}
Before we end this section, we
outline a proof sketch for
% provide the proof sketch of
Theorem \ref{th:optimality-greedy-homogeneous}. The proof consists of
% proceeds into 
% three
two steps: 1) We reformulate the problem as an MDP where the state space is the Cartesian product of the individual state space, i.e., $\mathcal{S}^M$; the action space is the subset of $\{0,1\}^M$ that
% which 
satisfies the budget constraint, and the transition matrix is the product of $P^m$ as all agents are independent. 2) We show that the proposed greedy policy $\pi_t^\greedy$ 
% we proposed
satisfies the Bellman 
% optimality
equation (Eq.~\eqref{eq:bellman-optimality}) for all $t\leq T$ 
by using backward induction. 
We prove
% The key idea to prove 
the induction hypothesis 
by 
% is
carefully controlling the difference of the value function (Eq.~\eqref{eq:value-function}).
% \kyra{what does the difference of the value function mean?}\jm{or maybe we should say span of the value function. I want to say the difference of value function of different states.}

\section{Budgeted Thresholding Contextual Bandit}\label{section:thresholding-bandit-algorithm}
In this section, we will present a learning algorithm for the 
budgeted thresholding contextual bandit
reformulation described in Section~\ref{subsec:contextual_bandit},
Algorithm \ref{alg:'name'}, in a general setting.
We establish that under this reformulation, our algorithm achieves the optimal minimax constant regret.
To ease the analysis, in this section, we assume that at each time $t$, there are sufficient agents whose ground truth incremental reward is above the threshold (Assumption~\ref{asssumption:enough-good-arm}). This assumption is merely used to simplify the proof. 
In practice, Algorithm~\ref{alg:'name'} can be executed with any chosen value of $\gamma$ and serve as a tunable parameter that can be used to improve the overall algorithm performance. We will discuss this in detail in Remark \ref{remark:trade-off-gamma}.
% we can still choose any $\gamma$ we want.
In Section \ref{sec:experiments}, we do not require $\gamma$ to satisfy
% we will not choose specific $\gamma$ that satisfy 
Assumption~\ref{asssumption:enough-good-arm}.
% \kyra{is this the same as we relax this assumption in 5.2?}\jm{Not quite, since if we truly relax this assumption, it should be like the algorithm also needs to perform well for any $\gamma$?}
%We relax this assumption in Section~\ref{sec:experiments} in Algorithm~\ref{alg:lcb-greedy}.
% \kyra{try to connect this online policy to greedy later}
% \kyra{revisit: do we really need this assumption when stating theoretical results?}
% \kyra{rewrite after rewriting intro}
We assume access to an accurate estimator of the reward $\phi^m$ \emph{under no intervention}, based on the observation that the system's dynamics are more easily observed without intervention. This does not require directly estimating transition probabilities.
% Based on the observation that we have much easier access to the dynamic of the system under no intervention, we will assume that we have an accurate estimator of the reward $\phi^m$ under no intervention. We note again that this does not involve directly estimating the transition probabilities.
% a distribution.
% theoretical guarantee for the performance of the algorithm.
% \subsection{LGRT}
\subsection{LCB-Guided Randomized Thresholding Algorithm}
% In this section, 
We
present our main algorithm: LCB-Guided Randomized Thresholding (LGRT, Algorithm \ref{alg:'name'}), that aims to learn the policy $\bm\pi^b$ that gives actions to agents whose incremental reward exceeds the threshold $\gamma$. 
% \kyra{we just discussed the choice of $\gamma$ before this subsection, should we remove it from here and add to section 5 to algorithm 2?}
% in this section. 



% \begin{algorithm}[H]
% \For {$k=1,2,\cdots$}
% {
% \For{$s=1,2,\cdots,\tau_k$}
% {
% UCB\;
% $t=t+1$\;
% }
% Randomly choose $B$ $(m,s)$ from this epoch\;
% \For{$s=1,2,\cdots$}{
% Sample $\tau_k^\alpha$ $(m,s)$\;
% }
% \While{$\hat{I}_{t}^m(s)-\mathrm{CB}\geq \gamma$}
% {
% Pull $(m,s)$ if it exists\;
% Otherwise, UCB\;
% }
% }
% \end{algorithm}
\begin{algorithm}[tb]
  \caption{LCB-Guided Randomized Thresholding}
    \label{alg:'name'}
    \begin{algorithmic}[1]
        \STATE{\bfseries Input:} $\bm{s},B,g(t),\phi^m(s,0),\gamma$
        \STATE Initialize: $\bm{s}_0=\bm{s}$, $\forall s\in\mathcal{S},\; m\in[M]: N_{s,m}^{(0)}=0$,
        $\hat{I}_0^m(s)=0$,
        $\hat{\phi}^m(s,1)=0$,
        $\mathrm{LCB}_0^m(s)=-\infty$
        \FOR{$t=0, 1,\cdots,T$}
        % \STATE Observe $\bm s_t$ and receive corresponding $\bm r_t$
        \STATE Retrieve $\bm s_t$, and obtain the set of agents in good states $\tilde G_t = \left\{m \colon \mathrm{LCB}_t^m({s}_t^m) \geq \gamma\right\}$. Set $C=\tilde G_t$;
        % for all $t$, any $\bm{s}$\;
        % \STATE  Give actions to all agents in $\tilde G_t$, and set $a_{t+1}^i = 1 \;\forall i\in \tilde G_t$
        \STATE Select $\max(0, B-|\tilde G_t|)$ agents uniformly from $[M]\setminus \tilde G_t$, and append them to $C$
        % Pick random $C \subseteq \{1\ldots M\}$ of size $B$ without replacement, prioritizing agents from $L_{t-1}(\bm{s}_t)$
        \label{alg:check-lcb}\;
            \STATE Uniformly select $B$ agents from $C$, and denote them by $\tilde C$. Set $a_{t+1}^m = 1$ for $m \in \tilde C$, $0$ otherwise\;
            \STATE Observe $(\bm{s}_{t+1}, \bm r_{t+1})$ \label{alg:observe-next-state}\;
            % \STATE Observe $(\bm{s}_{t+1}, r^m_t)$ \label{alg:observe-next-state}\;
            % \Comment*[r]{Update the point estimate of state-action reward}
            % \Comment*[r]{Update the sample mean of incremental reward}
                    \STATE  Update cumulative incremental reward 
                    for each $m\in [M]$: 
        $\hat{\phi}^m(s_t^m, 1) = \hat{\phi}^m(s_t^m, 1) +
        % r^m_t\mathbbm{1}\left\{a_{t}^m = 1\right\}$\;
        r^m_{t+1}\mathbbm{1}\left\{a_{t+1}^m = 1\right\}$
            \STATE For each agent-state pair $(m,s)\in[M]\times\mathcal{S}$, update the lower confidence bound of the incremental rewards as follows:

        \STATE \hspace{1em} 
                $N_{s, m}^{(t+1)} = 
        N_{s, m}^{(t)} + \mathbbm{1}
        \left\{a_{t+1}^m = 1\right\}\mathbbm{1}
        \left\{s = s_{t}^m \right\}$\;
        % $N_{s_t^m, m}(t+1) = 
        % N_{s_t^m, m}(t) + \mathbbm{1}
        % \left\{a_{t+1}^m = 1\right\}$\;
        \STATE  \hspace{1em} 
    $\hat{I}_{t+1}^m(s)=\frac{\hat{\phi}^m(s, 1)}{N_{s, m}^{(t+1)}} - \phi^m(s, 0)$
    % $\hat{I}_{t+1}^m(s_t^m)=\frac{\hat{\phi}^m(s_t^m, 1)}{N_{s_t^m, m}(t+1)} - \phi^m(s_t^m, 0)$
            \label{alg:update-sample-mean}\;
        \STATE  \hspace{1em} $\mathrm{LCB}_{t+1}^m(s) = \hat{I}_{t+1}^m(s) - \sqrt{\frac{g\left(N_{s, m}^{(t+1)} + 2\right)}{N_{s, m}^{(t+1)} + 2}}$\;
        \ENDFOR\;
    \end{algorithmic}
% \begin{algorithmic}[1]
%    \STATE{\bfseries Input:} $\bm{s},B,f(t),g(t),\phi^m(s,0),\gamma$
%    \STATE Initialization $N_{s,m}(t)=0$,
%     $\hat{I}_0^m(s)=0$,
%     $\hat{\phi}^m(s,1)=0$,
%     $\bm{s}_0=\bm{s}$,
%     $\mathrm{UCB}_0^m(s)=1$,
%      $\mathrm{LCB}_0^m(s)=0$
%      % \Comment*[r]{Initialization}
%     \FOR{$t=0,2,\cdots,T$}
%     \IF{$\left|\left\{(m,s):s=s_t^m,\mathrm{LCB}_{t-1}^m(s)\geq \gamma\right\}\right|\geq B$\label{alg:check-lcb}}
%    \STATE  Uniformly choose a subset $M_c$ of $\left\{(m,s):s=s_t^m,\mathrm{LCB}_{t-1}^m(s)\geq \gamma\right\}$ whose size is $B$
%     \STATE$a_{t+1}^m=1$, $\forall (m,s_t^m)\in M_c$
%     \ELSE
%    \STATE $a_{t+1}^m=1,\forall (m,s_t^m)\in \left\{(m,s):s=s_t^m,\mathrm{LCB}_{t-1}^m(s)\geq \gamma\right\}$
%    \STATE Uniformly choose a subset $M_c'$ of $\left\{(m,s):s=s_t^m,\mathrm{LCB}_{t-1}^m(s)< \gamma\right\}$ whose size is $B-\left|\left\{(m,s):s=s_t^m,\mathrm{LCB}_{t-1}^m(s)\geq \gamma\right\}\right|$
%    \STATE $a_{t+1}^m=1$, $\forall (m,s_t^m)\in M_c'$
%     \ENDIF
%     \STATE $N_{s_t^m,m}(t+1)=N_{s_t^m,m}(t)+\mathbbm{1}\left\{a_{t+1}^m=1\right\}$
%   \STATE  Observe $\bm{s}_{t+1}, r^m_t$\label{alg:observe-next-state}\;
%     $\hat{\phi}^m(s_t^m,1)=\hat{\phi}^m(s_t^m,1)+r^m_t\mathbbm{1}\left\{a_{t+1}^m=1\right\}$
%     % \Comment*[r]{Update the point estimate of state-action reward}
%    \STATE $\hat{I}_{t+1}^m(s)=\frac{\hat{\phi}^m(s,1)}{N_{s,m}(t+1)}-\phi^m(s,0)$\label{alg:update-sample-mean}
%     % \Comment*[r]{Update the sample mean of incremental reward}
%    % $\mathrm{UCB}_{t+1}^m(s)=\hat{I}_{t+1}^m(s)+\sqrt{\frac{f(t)}{N_{s,m}(t+1)+2}}$\;
%    \STATE $\mathrm{LCB}_{t+1}^m(s)=\hat{I}_{t+1}^m(s)-\sqrt{\frac{g\left(N_{s,m}(t+1)+2\right)}{N_{s,m}(t+1)+2}}$
%     \ENDFOR
%     \end{algorithmic}
\end{algorithm}
% The idea of the algorithm is that we are trying to learn the policy that will give actions to the agents whose incremental reward exceeds a predetermined threshold $\gamma$ with high probability. The parameter $\gamma$ can be tuned improve the overall performance.
% e.g. converting to anytime using the doubling trick \url{https://inria.hal.science/hal-01736357} and then adapting $\gamma$

Algorithm~\ref{alg:'name'} takes the initial state $\bm{s}$, budget $B$, threshold $\gamma$, and the state-action reward under no intervention as inputs.
% \kyra{introduce all the new notations in algorithm 1 here: $f(t), g(t), N$ and all the hats}
At each time step $t$, Algorithm~\ref{alg:'name'} proceeds as follows:
% We will give the key steps of the algorithm. At each time step $t$, the algorithm will proceed as following:
% \kyra{shouldn't we explain somewhere why the confidence bounds are constructed in this way, and what each notation in the algorithm represents?}
First, the algorithm obtains a set of agents, denoted by $\tilde G_t$,
% will check if there are more than $B$ agents
whose estimated lower confidence bounds
    % lower confidence bound estimators
of the incremental reward under the current state are above the threshold $\gamma$. 
If the cardinality of  $\tilde G_t$ satisfies  $|\tilde G_t|\geq B$,  we uniformly sample $B$ agents from $\tilde G_t$ to give actions. Otherwise, we give all agents in $\tilde G_t$ actions and uniformly sample $B-|\tilde G_t|$ agents from the rest. After taking the action, we observe 
% the next state
$\bm{s}_{t+1}$ and the corresponding reward $\bm r_{t+1}$.
Next, we
% Then, we will use this information to
update the lower confidence bound $\mathrm{LCB}_{t+1}$. 
% To be specific, 
% we will first use
Let $\hat{\phi}^m(s,1)$ be
% record 
the cumulative reward of each agent in each state,
% its current state
and $N_{s,m}^{(t+1)}$ be
% to record 
the number of times giving action 1 to agent $m$ when they are in state $s$. 
% $s_{t}^m$. 
We update the estimated incremental reward $\hat{I}_{t+1}^m(s)$ for each agent and state using the
% Then we will use the
empirical mean of reward under action 1. 
% to update the estimated incremental reward $\hat{I}_{t+1}^m(s)$ for each agent and state. 
The $\mathrm{LCB}_{t+1}^m(s)$ is defined as $\hat{I}_{t+1}^m(s)-\sqrt{\frac{g\left(N_{s,m}^{(t+1)}+2\right)}{N_{s,m}^{(t+1)}+2}}$, where $g(t)=3\log(t)$. 
% \kyra{missing the explanation of the rest of the algorithm}
% \begin{itemize}[noitemsep, topsep=1pt]
%     % \item We need to input the initial state $\bm{s}$ of each agent, the budget $B$, the threshold $\gamma$ and the state-action reward under no intervention.
%     \item First, the algorithm will check if there are more than $B$ agents whose estimated lower confidence bound
%     % lower confidence bound estimators
%     of the incremental reward under the current state are above the threshold $\gamma$. If so, uniformly give actions to $B$ such agents.
%     \item 
%     % If there are enough agents, we uniformly give action to $B$ such agents. 
%     If there are not enough agents, we will give action to all such agents whose lower confidence bound estimators are above the threshold. Next, the algorithm will uniformly choose from the remaining agents to give action. 
% \end{itemize}

% At each time step $t$, Algorithm~\ref{alg:'name'} maintains a lower confidence bound on the incremental reward of each state $s$ for each agent $m$ as follows: \kyra{complete, and discuss the construction of the lower confidence bound here}
Note that this lower confidence bound
% we defined here
is only a function of $N_{s,m}^{(t)}$, the number of times 
action 1 is given 
% that the algorithm gives action 
to agent $m$ in
% when it's in 
state $s$. Compared to the typical
% normal
% choice of 
anytime-valid LCB choice of
% lower confidence bound where 
$g(t)=\Theta(\log(t))$, our choice of $g$ is smaller, ensuring faster threshold crossing for agents with incremental reward above the threshold.
% and makes sure that it will take less time for the LCB of the agent whose incremental reward is larger than the threshold to exceed the threshold.
% \jm{changed here}Meanwhile,
We note that our algorithm shares similarities with Algorithm 1 in \citet{michel2022regret}, with a key distinction being the use of the LCB instead of the empirical mean when defining the candidate set $\tilde{G}_t$. This modification is crucial for achieving a superior regret bound compared to \citet{michel2022regret}. Intuitively, our choice of $g$ 
ensures that the LCB of an agent with an incremental reward smaller than the threshold will fall below the threshold more quickly. This design intuitively allows for faster elimination of ``bad" agents, as demonstrated in the proof.
% \kyra{note inline citation should be citet, within () use citealt to prevent having (author(year))}
% also makes sure that it will take less time for the LCB of the agent whose incremental reward is smaller than the threshold to be below the threshold. This intuitively guarantees that we will eliminate the ``bad'' agents faster as can be seen in the proof.%Intuitively, this will be more conservative than the empirical mean used in \citet{michel2022regret}. Meanwhile, it is less conservative than choosing the confidence bound which will lead to a valid confidence interval, which is important to get a regret bound that only depends on the gap between the incremental reward that is less than the threshold and threshold. \kyra{rewrite to emphasize that this lower bound is not necessarily anytime valid, but provides the desired guarantee as we want in thm 4.4}



\subsection{Theoretical Guarantee}
%\kyra{better to start by saying: in this subsection we establish that we establish that Algorithm~\ref{alg:'name'} achieves a regret upper bound of xxx which matches the lower bound xx under simplifying assumptions xxx, these assumptions are reasonable because (move assumptions up), notation, then lemma, then thm}
Let $\Delta_{s_{\min}}^{m_{\min}}$ denote the smallest gap between the threshold and the incremental reward above the threshold (formally defined later).
In this subsection, we establish that Algorithm~\ref{alg:'name'} achieves a regret upper bound of $\mathcal{O}\left(\frac{1}{\Delta_{s_{\min}}^{m_{\min}}}\right)$ under Assumptions~\ref{assumption:regular-transition-matrix} and \ref{asssumption:enough-good-arm}. This upper bound
% which
matches the lower bound $\Omega\left(\frac{1}{\Delta_{s_{\min}}^{m_{\min}}}\right)$up to some
% absolute
constants.
% , where $\Delta_{s_{\min}}^{m_{\min}}$ represents the smallest gap between the threshold and the incremental reward above the threshold.
% We will formally define this term later. 

% First, we will give the assumptions we need. 
Assumption~\ref{assumption:regular-transition-matrix} ensures that all agents have strictly positive transition probabilities between states, even if these probabilities are arbitrarily small.
\begin{assumption}[Positive Transition]\label{assumption:regular-transition-matrix}
% Assume
\( P_a^m(s, s') > 0 \) for all \( s, s' \in \mathcal{S} \), \( a \in \mathcal{A} \), and \( m \in [M] \).
    % Assume for any $s',s\in\mathcal{S}$, any $a\in\mathcal{A}$ and any $m\in[M]$, we have $P_a^m(s,s')>0$.
\end{assumption}
% Note that we do not need the transition probability be lower bounded by some constant, all we need is that the transition probability is strictly positive. 
% Therefore, this is a reasonable assumption since we still allow the transition probability to be arbitrarily small.
The next assumption ensures the existence of a policy where there are always enough good arms at each time step.
% Besides Assumption \ref{assumption:regular-transition-matrix}, we also need that there exists a policy such that there are always enough good arms at each time step $t$.
\begin{assumption}[Exists A Good Policy]
% [Sufficient Good Agents]
\label{asssumption:enough-good-arm}
% \kyra{check whether we want to say the existence of a policy or the existence of agents here}
    % At each time $t$, there are at least $B$ agents whose incremental reward is above 
    % the predefined 
    % threshold $\gamma$.
    Assume that there exists a policy such that the cumulative cost is 0.
    % at each time step, 
    % % there are 
    % more than $B$ agents have an
    % % whose 
    % incremental reward above
    % % is larger than 
    % the threshold $\gamma$.
\end{assumption}
Assumption~\ref{asssumption:enough-good-arm}
% This assumption
can always be satisfied by appropriately choosing
% for each instance with some 
% choices of
$\gamma$. For example, setting $\gamma=0$ guarantees the assumption holds.
% will always
% ensure it holds.
% be a trivial option. 
Before stating the
% move on to build the 
main theorem (Theorem \ref{th:sublinearregret}), we first introduce some notations. 
Let $\mathcal{G}$ be the set of good agent-state pairs, i.e., $\mathcal{G}:=\{(m,s): m\in[M], s\in\mathcal{S}, I^m(s)\geq \gamma\}$.
Let 
% $\Delta_s^m$ be defined as
$\Delta_s^m:=\gamma-I^m(s)$. Let $\left(m_{\min},s_{\min}\right)$ be $\arg\min_{(m,s)\in\mathcal{G}}\left\{I^m(s)-\gamma\right\}$.
% and $\left(m_{\bm{s}},s_{m_{\bm{s}}}\right)$ be $\arg\min_{(m,s_m)\in\mathcal{G}}\left\{I^m(s_m)-\gamma\right\}$.\kyra{what is $s_m$? though we had superscripts before. somehow the last two argmin notations look redundant to me}

% First,
% Next, Lemma \ref{le:enough-good-arm-all-states}
% % we 
% shows that, under Assumption \ref{assumption:regular-transition-matrix},
% % we can show that 
% Assumption \ref{asssumption:enough-good-arm} is equivalent to 
% requiring that sufficient good agents exist at each time step
% % the condition that there are always enough good agents at each time step 
% under any policy.
Next, Lemma \ref{le:enough-good-arm-all-states}
% we 
establishes that under Assumptions \ref{assumption:regular-transition-matrix} and \ref{asssumption:enough-good-arm}, two mild assumptions as  discussed, we 
can ensure a strong guarantee that sufficient good agents exist for each possible state
% can have a strong guarantee that there are enough agents for each possible state
$\bm{s}\in\mathcal{S}^M$. 
% This is formalized in Lemma \ref{le:enough-good-arm-all-states}.
% This is formalized in Lemma \ref{le:enough-good-arm-all-states}.
\begin{restatable}[Sufficient Good Arms]{lemma}{sufficientgoodarms}\label{le:enough-good-arm-all-states}
    Under Assumptions \ref{asssumption:enough-good-arm} and \ref{assumption:regular-transition-matrix}, 
    $\left|\left\{(m,s^m)\right\}\cap \mathcal{G}\right|\geq B$ for all $\bm{s}\in\mathcal{S}^M$.
    % for any $\bm{s}\in\mathcal{S}^M$, we have $\left|\left\{(m,s^m)\right\}\cap \mathcal{G}\right|\geq B$.
\end{restatable}
% \kyra{Are the results for 2 agents?}
Lemma \ref{le:enough-good-arm-all-states} (proof in Appendix~\ref{proof:enough-good-arm-all-states}) implies that
although selecting a suboptimal agent for action may affect the future state distribution, this effect does not accumulate over time because sufficient good agents are always available. This ensures zero regret is achievable at each step.
% ? \kyra{not sure what cost means. old:}
% though picking a run agent to give action will affect the future distribution of the state, this will not have a cumulative effect on the cost. 
% This is because no matter which state the learner is in, one can always make the immediate cost 0. 
Next, we present our main theorem
% Now we are ready to give the main theorem
on the regret upper bound.
\begin{restatable}[Regret Upperbound]{theorem}{regretbound}\label{th:sublinearregret}
Assume $r_t^m$ is 1-subgaussian random variable for all $t\in[T]$. Let $g(t)=3\log(t)$. 
%$e$ be Euler's number, and c be some absolute constant.
The regret, $\mathcal{R}(\pi)$
% The following holds for the regret 
(Eq.~\eqref{eq:cumulative-regret}), satisfies:
\begin{equation}
    \mathcal{R}(\pi)=\min\left\{\mathcal{O}\left(\frac{|\mathcal{S}|M-|\mathcal{G}|}{\Delta_{s_{\min}}^{m_{\min}}}\right),\mathcal{O}\left(\sqrt{B|\mathcal{G}|(|\mathcal{S}|M-\mathcal{G})T}\right)\right\}.
\end{equation}
%     \begin{align}
%         \vspace{-5pt}
%         \mathcal{R}(\pi)&\leq \min\bigg\{\sum_{\Delta_s^m>0}(2
%         % G
% |\mathcal{G}|+2)\Delta_s^m
% % \log\left(\frac{e}{\left(\Delta_{s_{\min}}^{m_{\min}}\right)^6}\right)
% \\&\;
% +\frac{24|\mathcal{G}|
% % G
% \Delta_s^m}{\left(\Delta_{s_{\min}}^{m_{\min}}\right)^2}\log\left(\frac{e}{\left(\Delta_{s_{\min}}^{m_{\min}}\right)^6}\right),\notag
%         \\& c\sqrt{|\mathcal{G}|
%         % G
%         B(2M-
%         |\mathcal{G}|
%         % G
%         )T}
%         +(2
%        |\mathcal{G}| 
%        % G
%        +2)\sum_{\Delta_s^m>0}\Delta_s^m\bigg\}.\notag
%            \vspace{-5pt}
%     \end{align}
    % for an absolute constant $c$.
\end{restatable}
The specific constant will be formally stated in the proof (Appendix~\ref{app:proof-thresholding}). When the budget is one, Theorem \ref{th:sublinearregret} shows that our algorithm achieves a regret upper bound that is $\mathcal{O}\left(1/\Delta_{s_{\min}}^{m_{\min}}\right)$, which depends only on the gap between the incremental reward above the threshold and the threshold. 
This improves the regret bound of $\mathcal{O}\left(\max_{\Delta_s^m>0}1/\Delta_s^m\right)$ from \citet{michel2022regret}, which depends on the gap between incremental rewards below the threshold and the threshold—typically smaller than $\Delta_{s_{\min}}^{m_{\min}}$. Our result also aligns with the minimax lower bound established in \citet{feng2024satisficing} (Theorem 3).
% This improves the regret bound provided in \citet{michel2022regret}, which is $\mathcal{O}\left(\max_{\Delta_s^m>0}1/\Delta_s^m\right)$. This is dependent on the gap between the incremental reward below the threshold and the threshold, which is typically smaller than $\Delta_{s_{\min}}^{m_{\min}}$. We also note that our result
% matches the minimax lower bound provided in \citet{feng2024satisficing} (Theorem 3).

%%%%%%kyra: read%%%%%%%%%%%%%%%%%
% {\jm{added here}}\kyra{we should start the paragraph by comparing the results that we have to that in feng et al. this paragraph should serve as a discussion point of our result, and you can say the reason why the result is in this way is because of assumption 4.2. but this paragraph is point is not really to discuss 4.2. so the focus appears a bit off}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The optimality result in Theorem~\ref{th:sublinearregret} relies on the presence of a sufficient number of good agents above the threshold. This contrasts with the findings of \citet{feng2024satisficing}, who demonstrate optimal regret regardless of the number of good arms above the threshold in the multi-armed bandit setting. 
Our problem, however, is fundamentally more challenging, making our analysis more difficult. 
Specifically, we address the contextual bandit setting where the context distribution evolves according to an MDP.
In this setup, taking a suboptimal action has a long-term impact on future rewards by influencing the state distribution over subsequent periods. This complexity hinders a direct adaptation of the methods proposed by \citet{feng2024satisficing} to our problem.
While relaxing Assumption~\ref{asssumption:enough-good-arm} may be possible, it falls outside the scope of this study and is left for future exploration.

% Before we end this section, we want to discuss more about Assumption \ref{asssumption:enough-good-arm}. This assumption prevents us from getting a optimal-of-both-world regret upper bound as achieved in \citealt{feng2024satisficing}. 

% However, we should note that instead of considering the multi-armed bandit setting that is the focus of \citealt{feng2024satisficing}, we are considering the contextual bandit setting where the distribution of the context is modeled as an MDP. Therefore, every wrong action taken will have spillover effect, which introduces extra complexity for analyzing the regret. We will leave removing Assumption \ref{asssumption:enough-good-arm} as future work.

% \jm{changed this}
% Also, we would like to note that the algorithm provided in \citealt{feng2024satisficing} could not be trivially adapted to our problem. The reason is that to implement their algorithm in budgeted thresholding contextual bandit, we would need to consistently give action to a fixed state-agent pair. However, since our state is generated according to an MDP, this step is not possible. Therefore, we need to introduce a new algorithm.
\begin{remark}[Sample Complexity]
While our algorithm shares a high-level idea with the online Whittle index algorithm \citep{wang2023optimistic}—constructing confidence intervals for unknown statistics—the intervals in our model are naturally tighter. This is because, with more than two states, constructing confidence intervals for transition probabilities requires sufficient visits to state pairs \((s_t, s_{t+1})\). In contrast, since our reward \(\phi\) depends only on the current state \(s_t\), our algorithm can gather information more efficiently.
    % We note that though the high level idea of our algorithm and the one for online Whittle index \citep{wang2023optimistic} is similar, which is constructing the confidence interval for unknown statistics, the confidence interval will naturally be tighter in our model. The reason behind is that when there are more than 2 states, to construct the confidence interval for the transition probability, it requires sufficient visit to the state pair $(s_t,s_{t+1})$. However, since our reward $\phi$ only depends on the current state $s_t$, it will be easier for our algorithm to get more information. %This is one of the reason why we can get a better regret bound and empirical performance.
\end{remark}
\begin{remark}[How to Select the Threshold $\gamma$?]\label{remark:trade-off-gamma}
In our problem, there is an inherent trade-off between satisfying Assumption~\ref{asssumption:enough-good-arm} and maximizing cumulative reward. While our reformulation focuses on the threshold identification problem, maximizing cumulative reward remains a desirable property in many real-world applications. 
Specifically, a smaller threshold \(\gamma\) makes it easier to satisfy Assumption~\ref{asssumption:enough-good-arm}, as more state-agent pairs qualify as good. However, a low \(\gamma\) also makes any arm above the threshold optimal, potentially leading to lower rewards. 
% Assumption~\ref{asssumption:enough-good-arm} is crucial for providing our regret guarantee. 
In the next section, we modify our algorithm, Algorithm~\ref{alg:lcb-greedy}, to greedily select actions based on the ordering of their confidence-bound estimators. This adjustment aims to enhance the cumulative reward achievable in practical applications.
    % We will further discuss on Assumption \ref{asssumption:enough-good-arm}. Notice that to make sure Assumption \ref{asssumption:enough-good-arm} hold, we need to pick some small $\gamma$ while to make sure the policy $\bm{\pi}^b$ we are learning close to $\bm{\pi}^\greedy$, we need to pick some large $\gamma$. Therefore, there is a trade-off between maximizing the cumulative reward in restless bandit problem and minimizing the cumulative cost in the budgeted thresholding contextual bandit reformulation. To be more specific, by choosing large enough $\gamma$, $\bm\pi^b$ can have larger cumulative reward. However, we are not guaranteed to learn a policy that is close to $\bm\pi^b$ if the $\gamma$ is chosen to be large.
\end{remark}


    % \vspace{-10pt}
\section{Experiments}\label{sec:experiments}
% In this section, 
%(the results are deferred to Appendix~\ref{subsec:regret-experiment-detail})% achieved by Algorithm~\ref{alg:lcb-greedy}, 
% with rewards 
We numerically evaluate the performance of our algorithm, demonstrating that: 1) the regret is sublinear and converges to a constant under the budgeted thresholding contextual bandit reformulation 
, and 2) the 
achieved
average cumulative reward, 
as defined in the finite-horizon restless bandit setting,
outperforms that of existing online restless bandit algorithms. 
To comprehensively assess the performance of our algorithm for reward maximization across diverse scenarios, we conduct experiments under three settings:
\begin{enumerate}
    \item \textbf{Under Stochastic Dominance Assumption (Assumption~\ref{asssumption:stochastic-dominance})}: We test our modified algorithm, Algorithm~\ref{alg:lcb-greedy}, which is adapted from Algorithm~\ref{alg:'name'} to better align with reward maximization objectives. This evaluation is conducted under the stochastic dominance assumption, a common framework in prior work~\citep{wang2023optimistic} and a key foundation for our optimality results.
    \item \textbf{Randomly Generated Instances}: We evaluate our algorithm on randomly generated instances to demonstrate its robustness without relying on the stochastic dominance assumption. 
    We modify both our algorithm (as reflected in Algorithm~\ref{alg:lcb-greedy-modified}) and the benchmark algorithms to make them suitable for this setting and illustrate that our proposed algorithm outperforms existing modified benchmarks.
    % With slight modifications, our algorithm outperforms existing benchmarks designed for restless bandit problems, even compared to the reformulated problem.
    \item \textbf{Adversarial Setting (No Action is  Optimal)}: We test our algorithm in the setting where taking no action is optimal. Here, our algorithm performs comparably to existing benchmarks, further validating its adaptability. Results for this setting are deferred to Appendix~\ref{paragraph:experiments-opposite-stochastic-dominance}.
\end{enumerate}
% \jm{For 2), we will test the performance of all algorithms under three settings: 1) modifying the randomly generated instances to ensure Assumption~\ref{asssumption:stochastic-dominance} holds, 2) directly using randomly generated instances, and 3) modifying the randomly generated instances to ensure that the transition probability under no intervention stochastically dominates the one under intervention (the results are deferred to Appendix~\ref{paragraph:experiments-opposite-stochastic-dominance}).}
% in the finite-horizon setting. 
These results underscore the practicality of the proposed reformulation and algorithms in finite-horizon restless bandit problems.
 % \kyra{maybe just directly describe it here.}

Our modified algorithm, 
Algorithm~\ref{alg:lcb-greedy}, 
is
% a modified algorithm 
designed to improve cumulative reward as well as the convergence rate under the budgeted thresholding contextual bandit reformulation. 
%%%%% better to remind them the rule in alg 1 and make a direct contrast %%%%%
When sufficient good arms are identified, rather than randomly sampling from agents with LCB values above the threshold, we greedily select agents in $\tilde G_t$ in descending order of their LCB values. 
% 
% Besides using the LCB to efficiently eliminate suboptimal agents, we greedily allocate actions to agents whose LCB values exceed the threshold.
% a predefined threshold. 
% Specifically, actions are assigned to agents in $\tilde{G}_t$ based on the descending order of their LCB values.
Further,
when insufficient good arms are identified, we optimistically select agents based on the upper confidence bound (UCB) of the incremental reward rather than randomly sampling those with LCB below the threshold.
%%%i don't really get balance part, might be better just to directly state the difference%%%
% To balance exploration and exploitation, we incorporate the upper confidence bound (UCB) of the incremental reward to optimistically select agents whose LCB values fall below the threshold. 
The UCB at each time step is defined according to the anytime-valid confidence bound in \citet{10.1214/20-AOS1991}:
% To be specific, we will define 
$\mathrm{UCB}_{t+1}^m(s):=\hat{I}_{t+1}^m(s)+\sqrt{\frac{f(t,T)}{N_{s,m}^{(t+1)}+2}}$, where $f(t,T)= 1.714^2\left(\log\log\left(N_{s, m}(t+1) + 2 \log\left(10 T\right)\right)\right)$.
%%% repeating alg 2 is for improving cumulative reward as readers might start reading from sec 5 without reading the remark above %%%%%%
% We first introduce
% describe 
\begin{algorithm}[tb]
  \caption{LCB-Guided Greedy Thresholding}
    \label{alg:lcb-greedy}
    \begin{algorithmic}[1]
        \STATE{\bfseries Input:} $\bm{s},B,g(t),f(t),\phi^m(s,0),\gamma$
        \STATE Initialize: $\bm{s}_0=\bm{s}$, $\forall s\in\mathcal{S},\; m\in[M]: N_{s,m}^{(0)}=0$,
        $\hat{I}_0^m(s)=0$,
        $\hat{\phi}^m(s,1)=0$,
        $\mathrm{LCB}_0^m(s)=-\infty$,
        $\mathrm{UCB}_0^m(s)=\infty$
        \FOR{$t=0, 1,\cdots,T$}
        % \STATE Observe $\bm s_t$ and receive corresponding $\bm r_t$
        \STATE Retrieve $\bm s_t$, and obtain the set of agents in good states $\tilde G_t = \left\{m \colon \mathrm{LCB}_t^m({s}_t^m) \geq \gamma\right\}$. Set $C=\tilde G_t$;
        % for all $t$, any $\bm{s}$\;
        % \STATE  Give actions to all agents in $\tilde G_t$, and set $a_{t+1}^i = 1 \;\forall i\in \tilde G_t$
        \STATE 
        Sort agents in \([M] \setminus \tilde G_t\) in descending order of \(\text{UCB}_t^m(s_t^m)\), select the first \(\max(0, B - |\tilde G_t|)\) agents, and append them to \(C\).
        % Select $\max(0, B-|\tilde G_t|)$ agents  from $[M]\setminus \tilde G_t$ according to the ordering of $\text{UCB}_t^m(s_t^m)$, and append them to $C$
        % Pick random $C \subseteq \{1\ldots M\}$ of size $B$ without replacement, prioritizing agents from $L_{t-1}(\bm{s}_t)$
        %\label{alg:check-lcb}\;
            %\STATE Set $a_{t+1}^m = 1$ for $m \in C$, $0$ otherwise\;
        \STATE Sort agents in $C$ in descending order of $\text{LCB}_{t}^m(s_t^m)$, select the first $B$ agents, and denote them by
        % Select $B$ agents from $C$ according to the rank of $\text{LCB}_{t}^m(s_t^m)$, and denote them by 
        $\tilde C$. Set $a_{t+1}^m = 1$ for $m \in \tilde C$, $0$ otherwise\;
            \STATE Observe $(\bm{s}_{t+1}, \bm r_{t+1})$ \;%\label{alg:observe-next-state}\;
            % \STATE Observe $(\bm{s}_{t+1}, r^m_t)$ \label{alg:observe-next-state}\;
            % \Comment*[r]{Update the point estimate of state-action reward}
            % \Comment*[r]{Update the sample mean of incremental reward}
                    \STATE  Update cumulative incremental reward 
                    for each $m\in [M]$: 
        $\hat{\phi}^m(s_t^m, 1) = \hat{\phi}^m(s_t^m, 1) +
        % r^m_t\mathbbm{1}\left\{a_{t}^m = 1\right\}$\;
        r^m_{t+1}\mathbbm{1}\left\{a_{t+1}^m = 1\right\}$
            \STATE For each agent-state pair $(m,s)\in[M]\times\mathcal{S}$, update the LCB and UCB of the incremental rewards as:
            % follows:

        \STATE \hspace{1em} 
                $N_{s, m}^{(t+1)} = 
        N_{s, m}^{(t)} + \mathbbm{1}
        \left\{a_{t+1}^m = 1\right\}\mathbbm{1}
        \left\{s = s_{t}^m \right\}$\;
        % $N_{s_t^m, m}(t+1) = 
        % N_{s_t^m, m}(t) + \mathbbm{1}
        % \left\{a_{t+1}^m = 1\right\}$\;
        \STATE  \hspace{1em} 
    $\hat{I}_{t+1}^m(s)=\frac{\hat{\phi}^m(s, 1)}{N_{s, m}^{(t+1)}} - \phi^m(s, 0)$
    % $\hat{I}_{t+1}^m(s_t^m)=\frac{\hat{\phi}^m(s_t^m, 1)}{N_{s_t^m, m}(t+1)} - \phi^m(s_t^m, 0)$
            %\label{alg:update-sample-mean}\;
        \STATE  \hspace{1em} $\mathrm{LCB}_{t+1}^m(s) = \hat{I}_{t+1}^m(s) - \sqrt{\frac{g\left(N_{s, m}^{(t+1)} + 2\right)}{N_{s, m}^{(t+1)} + 2}}$\;
        \STATE\hspace{1em}$\mathrm{UCB}_{t+1}^m(s)=\hat{I}_{t+1}^m(s)+\sqrt{\frac{f(t,T)}{N_{s,m}^{(t+1)}+2}}$
        \ENDFOR\;
    \end{algorithmic}
\end{algorithm}
We will use this algorithm for Sections~\ref{subsec:regret-experiment-detail} and \ref{subsec:experiments-with-stochastic-dominance}.
% In this section, 
\subsection{Regret Curve}\label{subsec:regret-experiment-detail}
\begin{figure}[!t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.4\columnwidth]{different-gap-10-100-015-2-1.png}}
\caption{Regret Curve as horizon $T$ grows.}
\label{figure:regret-curve}
\end{center}
% \vskip -0.2in
\end{figure}
% In this section, we will use numerical examples to 
To numerically 
verify Theorem \ref{th:sublinearregret}, 
we set 
% the number of agents
$M=2$, 
% the budget 
$B=1$, and 
% the state space
$\mathcal{S}=\{0,1\}$. We set the expected reward of the state as $R^m(s)=s$. We randomly generated $10$ instances and chose three $\gamma$ values satisfying Assumption~\ref{asssumption:enough-good-arm}, labeling each $\gamma$ by the corresponding gap $\Delta_{s_{\min}}^{m_{\min}}$ between the threshold and the smallest incremental reward. The details of implementation are deferred to Appendix~\ref{subsec:implementation-detail-regret}.

For each $\gamma$, the regret was averaged over $100$ repetitions per instance (Figure~\ref{figure:regret-curve}). 
Figure \ref{figure:regret-curve} demonstrates sublinear regret for all choices of \(\gamma\), with regret stabilizing at a constant as the horizon length increases, consistent with Theorem \ref{th:sublinearregret}. Further, as $\Delta_{s_{\min}}^{m_{\min}}$ decreases, regret increases as it becomes harder to distinguish whether the incremental reward is above the threshold or not. This matches the results in Theorem~\ref{th:sublinearregret}.
% This is the anytime-valid confidence bound derived in \citealt{10.1214/20-AOS1991}.

% In the numerical experiments, we will implement Algorithm \ref{alg:lcb-greedy}, which is a modification of Algorithm \ref{alg:'name'}. The key idea is that instead of randomly choosing an agent within the candidate set, we will give actions greedily to agents according to the order of their confidence bound estimators.

% \begin{algorithm}[tb]
%   \caption{LCB-Guided Greedy Thresholding}
%   \label{alg:lcb-greedy}
% \begin{algorithmic}[1]
%     \STATE{\bfseries Input:} {$\bm{s},B,f(t),g(t),\phi^m(s,0),\gamma$}
%     \STATE Initialize $N_{s,m}(t)=0$,
%     $\hat{I}_0^m(s)=0$,
%     $\hat{\phi}^m(s,1)=0$,
%     $\bm{s}_0=\bm{s}$,
%     $\mathrm{UCB}_0^m(s)=1$,
%     $\mathrm{LCB}_0^m(s)=0$
%     % \Comment*[r]{Initialization}
%     \STATE Define $L_t(\bm{s}) = \left\{m \colon \mathrm{LCB}_t^m(\bm{s}^m) \geq \gamma\right\}$ for all $t$, any $\bm{s}$
%     \FOR{$t=0,1,\cdots,T$}
%         \IF{$\left|L_{t-1}(\bm{s}_t)\right| \geq B$}
%             \STATE Give action to the top $B$ agents in $L_{t-1}(\bm{s}_t)$ in terms of $\mathrm{LCB}_{t-1}^m\left(s_t^m\right)$ \;
%         \ELSE
%             \STATE Put $a_{t+1}^m=1$ for $m \in L_{t-1}(\bm{s}_t)$\;
%             \STATE Assign actions to the top $B-\left|L_{t-1}(\bm{s}_t)\right|$ ranked by $\mathrm{UCB}_{t-1}^m\left(s_t^m\right)$\;
%         \ENDIF
%         \STATE Observe $(\bm{s}_{t+1}, r^m_t)$\;
%         % \Comment*[r]{Update the point estimate of state-action reward}
%         % \Comment*[r]{Update the sample mean of incremental reward}
%         \STATE $N_{s_t^m, m}(t+1) = N_{s_t^m, m}(t) + \mathbbm{1}\left\{a_{t+1}^m = 1\right\}$\;
%         \STATE $\hat{\phi}^m(s_t^m, 1) = \hat{\phi}^m(s_t^m, 1) + r^m_t\mathbbm{1}\left\{a_{t+1}^m = 1\right\}$\;
%         % \Comment*[r] update the upper- (lower-) confidence bounds q-functions at action 1
%         \STATE $\hat{I}_{t+1}^m(s) = \frac{\hat{\phi}^m(s, 1)}{N_{s, m}(t+1)} - \phi^m(s, 0)$\;
%         \STATE $\mathrm{UCB}_{t+1}^m(s) = \hat{I}_{t+1}^m(s) + \sqrt{\frac{f(t)}{N_{s, m}(t+1) + 2}}$\;
%         \STATE $\mathrm{LCB}_{t+1}^m(s) = \hat{I}_{t+1}^m(s) - \sqrt{\frac{g\left(N_{s, m}(t+1) + 2\right)}{N_{s, m}(t+1) + 2}}$\;
%     \ENDFOR
% \end{algorithmic}
%
% \begin{algorithmic}
%      \STATE{\bfseries Input:} {$\bm{s},B,f(t),g(t),\phi^m(s,0),\gamma$}
%     \STATE Initialize $N_{s,m}(t)=0$,
%     $\hat{I}_0^m(s)=0$,
%     $\hat{\phi}^m(s,1)=0$,
%     $\bm{s}_0=\bm{s}$,
%     $\mathrm{UCB}_0^m(s)=1$,
%     $\mathrm{LCB}_0^m(s)=0$
%     % \Comment*[r]{Initialization}
%     \FOR{$t=0,2,\cdots,T$}
%     \IF{$\left|\left\{(m,s):s=s_t^m,\mathrm{LCB}_{t-1}^m(s)\geq \gamma\right\}\right|\geq B$}
%    \STATE Give action to the top $B$ agents in $\left|\left\{(m,s):s=s_t^m,\mathrm{LCB}_{t-1}^m(s)\geq \gamma\right\}\right|\geq B$ in terms of $\mathrm{LCB}_{t-1}^m\left(s_t^m\right)$ 
%     \ELSE
%    \STATE $a_{t+1}^m=1,\forall (m,s_t^m)\in \left\{(m,s):s=s_t^m,\mathrm{LCB}_{t-1}^m(s)\geq \gamma\right\}$
%  \STATE  Give action to the top $B-\left|\left\{(m,s):s=s_t^m,\mathrm{LCB}_{t-1}^m(s)\geq \gamma\right\}\right|$ in terms of $\mathrm{UCB}_{t-1}^m\left(s_t^m\right)$
%     \ENDIF
%    \STATE $N_{s_t^m,m}(t+1)=N_{s_t^m,m}(t)+\mathbbm{1}\left\{a_{t+1}^m=1\right\}$
%     \STATE Observe $\bm{s}_{t+1}, r^m_t$
%    \STATE $\hat{\phi}^m(s_t^m,1)=\hat{\phi}^m(s_t^m,1)+r^m_t\mathbbm{1}\left\{a_{t+1}^m=1\right\}$
%     % \Comment*[r]{Update the point estimate of state-action reward}
%     \STATE$\hat{I}_{t+1}^m(s)=\frac{\hat{\phi}^m(s,1)}{N_{s,m}(t+1)}-\phi^m(s,0)$
%     % \Comment*[r]{Update the sample mean of incremental reward}
%     \STATE$\mathrm{UCB}_{t+1}^m(s)=\hat{I}_{t+1}^m(s)+\sqrt{\frac{f(t)}{N_{s,m}(t+1)+2}}$
%     \STATE$\mathrm{LCB}_{t+1}^m(s)=\hat{I}_{t+1}^m(s)-\sqrt{\frac{g\left(N_{s,m}(t+1)+2\right)}{N_{s,m}(t+1)+2}}$
    
%     \ENDFOR
% \end{algorithmic}

\subsection{Experiments With Stochastic Dominance}\label{subsec:experiments-with-stochastic-dominance}
Next, we examine the average cumulative reward obtained by our algorithm in the online restless bandit setting (Section~\ref{subsec:online-restless-bandit}) under stochastic dominance assumption, and compare it with the following benchmarks: 1) 
WIQL \citep{biswas2021learn}, 2) variants of UCWhittle (UCWhittle-UCB-Privileged, UCWhittlePv-Privileged) \citep{wang2023optimistic}, 3) \emph{Oracle Whittle} \citep{whittle1988restless}, 4) \emph{Oracle Greedy} and 5) \emph{Random Policy}, which aligns with the experimental setting of \citet{wang2023optimistic}.
We emphasize that our experiments involve heterogeneous agents, a setting in which no algorithm has been established as optimal. To evaluate the relative quality of the policies to which the online algorithms converge, we incorporate two benchmark algorithms with full MDP knowledge: \emph{Oracle Whittle} and \emph{Oracle Greedy}. {We note that in this setting, neither algorithm is guaranteed to be optimal.}
\begin{itemize}
    \item Oracle Whittle: A variant of Whittle's index policy with access to the ground truth transitions and rewards, optimized for maximizing discounted reward (as described in Appendix \ref{app:whittle}). 
    \item Oracle Greedy:
    The greedy policy we defined (Algorithm~\ref{alg:lcb-greedy}) with knowledge of the ground truth incremental reward.
    \item WIQL \citep{biswas2021learn}:  A Q-learning based approach to
% uses Q-learning to
learn
the value function of each arm at each state by interacting
with the RMAB instance.
We use the same learning rate ($\alpha$) as in \citet{biswas2021learn}.
% The learning rate, $\alpha$,  in WIQL in our implementation is the same as the one in the original paper. % In our implementation, we choose the learning rate $\alpha$ exactly the same as the one used in original paper. 
\item UCWhittle \citep{wang2023optimistic}: It constructs the confidence interval of the unknown transition probability and then chooses one to calculate Whittle's index policy. We compare with two different choices.
%\item UCWhittle-extreme \citep{wang2023optimistic}: it uses the upper confidence bound of the transition probability under intervention and the lower confidence bound of the transition probability under no intervention to construct the Whittle's index policy. 
\begin{itemize}
\item UCWhittle-UCB \citep{wang2023optimistic}: It uses the upper confidence bounds of transition probabilities.
\item UCWhittle-penalty \citep{wang2023optimistic}: {It constructs a confidence interval and chooses the transition probability that will maximize $V_\lambda^m(s)$ (Eq.~\eqref{eq:dynamic-programming-discounted}) for some adaptively updated $\lambda$.}
\end{itemize}
\end{itemize}

In all of the settings, the discount factor $\beta$ was set to $0.9$.
% $\beta=0.9$ is chosen to the discount factor. 
We note that though $\beta=1$ was chosen in \citet{biswas2021learn}, $\beta=0.9$ has better empirical performance, which aligns with the choice in 
% And $\beta=0.9$ is chosen exactly the same as 
\citet{wang2023optimistic}.\footnote{Code is available at \href{https://github.com/jamie01713/ucblcb/tree/work20250117}{github}. Our results differ from those of \citet{wang2023optimistic} due to detected bugs in their code; a detailed discussion is provided in Appendix~\ref{subsec:exp-detail-cumulative}.}
% Detailed descriptions of the baselines as well as further discussion about the implementation detailed
% % of UCWhittle 
% are included in
% % We will leave the detailed introduction of these two baselines to 
% Appendix~\ref{subsec:exp-detail-cumulative}.
 

Since our algorithm has access to \(\phi^m(s,0)\), for fairness, we assume that UCWhittle algorithms have full knowledge of the transition probability under no intervention. This is a stronger assumption, as accurately estimating transition probabilities would require significantly more offline data than estimating \(\phi^m(s,0)\). 
%We will not give UCWhittle-Extreme such knowledge as giving it this knowledge would make it the same as UCWhittle-UCB-Privileged.
Since it is challenging to determine how Q-learning algorithms could effectively use additional information from the no-intervention setting, we did not provide WIQL with any extra information.
% We would like to mention here first that since our algorithm requires the knowledge of $\phi^m(s,0)$, we also give UCWhittle knowledge of transition probability under no intervention. Nevertheless, we note that the knowledge we pass to UCWhittle is more than what we need.

We set 
% the number of agents to be
$M=50$, $\mathcal{S}=\{0,1\}$, and compare the results under three different budget settings: $B=5,10,20$. 
% Throughout, we focus on binary state and action setting.
The expected reward is set to be 
$R^m(s)=s,\forall m\in[M],s\in\mathcal{S}$
% \{0,1\}$
and 
% We will
randomly generate $50$ instances with different transition matrices.
    \begin{figure}[!t]
       \centering
% \includegraphics[width=0.32\linewidth]{xp2all_fig1__P50__N50__0.0__B5__E11__L11__H500__+ga__-go__random__B76A074C23C703767710E1D756F73AE9.pdf}
% \includegraphics[width=0.32\linewidth]{xp2all_fig1__P50__N50__0.0__B10__E11__L11__H500__+ga__-go__random__B76A074C23C703767710E1D756F73AE9.pdf}
% \includegraphics[width=0.32\linewidth]{xp2all_fig1__P50__N50__0.0__B20__E11__L11__H500__+ga__-go__random__B76A074C23C703767710E1D756F73AE9.pdf}
\includegraphics[width=\linewidth]{dominance.pdf}
        \caption{Average cumulative reward under the stochastic dominance assumption, evaluated across three budget settings: $B=5$ (left column), $10$ (middle column), $20$ (right column). Results are averaged over $50$ instance satisfying Assumption~\ref{asssumption:stochastic-dominance}, each with $13$ repetitions. Top row: noiseless reward. Bottom row: noisy reward.}
        \label{fig:sub4}
         % \vskip -0.1in
    \end{figure}

To enforce the stochastic dominance assumption, we modify randomly generated instances by checking if there exist \(s, m\) such that \(P_1^m(s,1) < P_0^m(s,1)\). If so, we swap these transitions to satisfy Assumption \ref{asssumption:stochastic-dominance}.
% In this paragraph, we will modify the randomly generated instances to make sure that the stochastic dominance assumption holds. After an instance is generated, 
% To ensure that the stochastic dominance assumption holds, we make the following modification to a randomly generated instance: 
% we check whether there exists $s,m$ such that $P_1^m(s,1)<P_0^m(s,1)$. If there does, we will simply swap these two transitions to make sure Assumption \ref{asssumption:stochastic-dominance} hold. 
In Figure~\ref{fig:sub4}, each line is averaged over $50$ instances, each with
% represents one instance, each averaged over
$13$ repetitions.
% and run each instance for 11 times for each policy to get the average cumulative reward curve. 
% \emph{Oracle Whittle} and \emph{Orcale Greedy}, with full MDP knowledge, serve as benchmarks for all online algorithms. 
We tested our algorithm, LGGT, with two threshold values $0.1$ and $0.4$. 
% 
We consider two reward settings: 1) a noiseless case where $r_t\in\{0,1\}$, and 2) a noisy setting where the rewards at state $s\in\mathcal{S}$ follow a normal distribution with mean $R^m(s)$ and variance 1.
% We will do the experiment under two settings: 1) the reward is known and 2) the reward follows a normal distribution with variance 1.

% We first notice that 
Under the \emph{noiseless reward setting} (Figure~\ref{fig:sub4} top row), in all three budget settings, LGGT with $\gamma=0.1$ consistently outperforms online restless bandit algorithms within a horizon of $1000$.
Additionally, as the budget increases, the performance gap between LGGT (\(\gamma=0.1\)) and WIQL \emph{widens}. At first glance, this may seem counterintuitive—since lower budgets reduce action frequency, one might expect learning the Q-function in WIQL to be harder. Indeed, the smallest performance gap occurs at the lowest budget, highlighting the difficulty of learning in low-budget settings.
In resource-limited settings, while our algorithm is more efficient in terms of learning, its potentially larger suboptimality gap compared to benchmark algorithms may negatively impact its overall finite-sample performance.
% However, in resource-scarce settings, more suboptimal decisions may be more detrimental than the challenge of learning effectively.
%%%%kyra: the original sentence is kind of implying that the benchmark algorithms are optimal, which is not desirable%%%%
As shown in Figure~\ref{fig:sub4} (top left), LGGT quickly converges to a stable yet suboptimal policy, whereas WIQL converges more slowly. However, given a sufficiently long horizon, WIQL will eventually converge to the \emph{Oracle Whittle} policy.
Nevertheless, LGGT ($\gamma=0.1$) outperforms WIQL when the horizon is short even when the budget is low, highlighting the practicality of our algorithm in the online restless bandit settings. Similarly, we observe that UCWhittleUCBPriv converges slowly, and it is worse than LGGT ($\gamma=0.1$) under all three budget settings. 
Further, UCWhittlePvPriv performs the worst when
% is almost the same level as random policy under the case where 
the number of agents is large.
% huge
% even with oracle knowledge of no intervention. 
% Similarly, UCWhittleExtreme performs similarly to random.
% is also as bad as random policy.


In the \emph{noisy} reward setting (Figure~\ref{fig:sub4}, bottom row), we excluded UCWhittle policies since constructing Whittle's index requires precise reward knowledge. Handling noise would require carefully calibrated confidence intervals for both rewards and transitions, which is beyond the scope of this work. Additionally, since UCWhittleUCBPriv underperformed WIQL in the noiseless setting, we did not include the UCWhittle family in the noisy setting.
% In the \emph{noiseless} reward setting , we 
% observe that UCWhittleUCBPriv
% % with oracle knowledge
% performs worse than WIQL. 
% Further, knowledge of rewards is also needed to calculate oracle Whittle's index policy. Thus, when the rewards are also unknown, one need to construct another confidence interval on the rewards to implement UCWhittle, which is non-trivial. Also, keeping track of two confidence intervals in the algorithm would require more efforts on balancing between these two, which would make learning harder. Therefore, we will only compare LGGT with WIQL under noisy reward setting. Again, i
In all three budget settings, LGGT with $\gamma=0.1$ consistently outperforms WIQL. 
Moreover, 
% as shown in Figure~\ref{fig:sub4},
the gap between LGGT and WIQL widens when compared to noiseless setting. This indicates that LGGT adapts better to the noise in rewards, demonstrating the robustness of our algorithm.

% \kyra{should we discuss $\gamma=0.4$ somewhere?}

Lastly, we compare the performance of 
\emph{Oracle Whittle} and \emph{Oracle Greedy}.
With the stochastic dominance assumption, \emph{Oracle Whittle} consistently outperforms \emph{Oracle Greedy} across all budget settings, with the performance gap narrowing as the budget increases. 
Despite this gap, LGGT outperforms all online restless bandit benchmarks, demonstrating the benefit of rapid convergence in finite-horizon scenarios.

% We can notice that when the budget is low, there is a gap between Oracle Greedy and Oracle Whittle. However, LGGT, which is learning Oracle Greedy still has better performance. This illustrates that convergence speed is important when we are under finite horizons. Furthermore, we can notice that as the budget grows, the gap between Oracle Greedy and Oracle Whittle becomes smaller.

\subsection{Experiments Without Stochastic Dominance}\label{subsec:experiments-without-stochastic-dominance}
  \begin{figure}[!t]
       \centering
% \includegraphics[width=0.32\linewidth]{xp2all_fig1__P50__N50__0.0__B5__E11__L11__H500__+ga__-go__random__B76A074C23C703767710E1D756F73AE9.pdf}
% \includegraphics[width=0.32\linewidth]{xp2all_fig1__P50__N50__0.0__B10__E11__L11__H500__+ga__-go__random__B76A074C23C703767710E1D756F73AE9.pdf}
% \includegraphics[width=0.32\linewidth]{xp2all_fig1__P50__N50__0.0__B20__E11__L11__H500__+ga__-go__random__B76A074C23C703767710E1D756F73AE9.pdf}
\includegraphics[width=\linewidth]{nodominance.pdf}
        \caption{Average cumulative reward \emph{without} the stochastic dominance assumption, evaluated across three budget settings: $B=5$ (left column), $10$ (middle column), $20$ (right column). Results are averaged over $50$ instances, each with $13$ repetitions. Top row: noiseless reward. Bottom row: noisy reward.}
        \label{fig:sub5}
         % \vskip -0.1in
    \end{figure}
In this subsection,
we evaluate the performance of all algorithms in the absence of the stochastic dominance assumption. 
% we will directly use the randomly generated instances. 
As discussed in Remark \ref{remark:stochastic-dominance}, when stochastic dominance does not hold, we might favor not using the entire budget at each time step. 
Therefore, we adapt all algorithms to accommodate scenarios where the stochastic dominance assumption may not hold.
% need to modify all of the algorithms to adjust them to the setting where stochastic dominance assumption might be violated. 
% 

To adapt LGGT, we restrict interventions to agents whose incremental reward is estimated to be positive, as outlined in Algorithm~\ref{alg:lcb-greedy-modified}.
In particular,  when the incremental reward of an agent-state pair $(m,s^m)$ is negative, no action will be taken by agent $m$ when they are in state $s^m$. We will denote this algorithm as LCB-Guided Greedy Thresholding With UCB Filtering (LGGTWUF).
Similarly, for policies based on Whittle's index, we restrict interventions to agents with positive index values, i.e.,  when Whittle's index of an agent-state pair $(m,s^m)$ is negative, no action will be taken by agent $m$ when they are in state $s^m$.
We note that the optimality of these adaptations in the absence of stochastic dominance has not been established. 
% Below, we outline the specific modifications made to each algorithm:
% Similarly, by the definition of Whittle's index, we hypothesized that when the Whittle's index is negative, giving no intervention is more beneficial. Therefore, we will still compare our algorithm with the same benchmark as in Section~\ref{subsec:experiments-with-stochastic-dominance}, except for the following modification:

% For LGGT, by the definition of incremental reward, we hypothesize that if the incremental reward of a state is negative, this implies giving no intervention is more beneficial. Therefore, we would only give intervention to agents whose incremental reward is believed to be positive. To be specific, we present Algorithm~\ref{alg:lcb-greedy-modified}.
\begin{algorithm}[tb]
  \caption{LCB-Guided Greedy Thresholding With UCB Filtering}
    \label{alg:lcb-greedy-modified}
    \begin{algorithmic}[1]
        \STATE{\bfseries Input:} $\bm{s},B,g(t),f(t),\phi^m(s,0),\gamma$
        \STATE Initialize: $\bm{s}_0=\bm{s}$, $\forall s\in\mathcal{S},\; m\in[M]: N_{s,m}^{(0)}=0$,
        $\hat{I}_0^m(s)=0$,
        $\hat{\phi}^m(s,1)=0$,
        $\mathrm{LCB}_0^m(s)=-\infty$,
        $\mathrm{UCB}_0^m(s)=\infty$,
        $\mathrm{TEST}_0^m(s)=\infty$
        \FOR{$t=0, 1,\cdots,T$}
        % \STATE Observe $\bm s_t$ and receive corresponding $\bm r_t$
        \STATE Retrieve $\bm s_t$, and obtain the set of agents in good states $\tilde G_t = \left\{m \colon \mathrm{LCB}_t^m({s}_t^m) \geq \gamma\right\}$. Set $C=\tilde G_t$;
        % for all $t$, any $\bm{s}$\;
        % \STATE  Give actions to all agents in $\tilde G_t$, and set $a_{t+1}^i = 1 \;\forall i\in \tilde G_t$
        \STATE 
        Sort agents in \([M] \setminus \tilde G_t\) in descending order of \(\text{UCB}_t^m(s_t^m)\), select the first \(\max(0, B - |\tilde G_t|)\) agents, and append them to \(C\).
        % Select $\max(0, B-|\tilde G_t|)$ agents  from $[M]\setminus \tilde G_t$ according to the ordering of $\text{UCB}_t^m(s_t^m)$, and append them to $C$
        % Pick random $C \subseteq \{1\ldots M\}$ of size $B$ without replacement, prioritizing agents from $L_{t-1}(\bm{s}_t)$
        %\label{alg:check-lcb}\;
            %\STATE Set $a_{t+1}^m = 1$ for $m \in C$, $0$ otherwise\;
        \STATE Sort agents in $C$ in descending order of $\text{LCB}_{t}^m(s_t^m)$, select the first $B$ agents, and denote them by
        % Select $B$ agents from $C$ according to the rank of $\text{LCB}_{t}^m(s_t^m)$, and denote them by 
        $\tilde C$. 
        \STATE Obtain the set of agents whose incremental reward is positive $\tilde{L}_t:=\{m: \mathrm{TEST}_t^m(s_t^m)\geq 0\}$. 
        Set $a_{t+1}^m = 1$ for $m \in \tilde C\cap\tilde L_t$, $0$ otherwise\;
            \STATE Observe $(\bm{s}_{t+1}, \bm r_{t+1})$ \;%\label{alg:observe-next-state}\;
            % \STATE Observe $(\bm{s}_{t+1}, r^m_t)$ \label{alg:observe-next-state}\;
            % \Comment*[r]{Update the point estimate of state-action reward}
            % \Comment*[r]{Update the sample mean of incremental reward}
                    \STATE  Update cumulative incremental reward 
                    for each $m\in [M]$: 
        $\hat{\phi}^m(s_t^m, 1) = \hat{\phi}^m(s_t^m, 1) +
        % r^m_t\mathbbm{1}\left\{a_{t}^m = 1\right\}$\;
        r^m_{t+1}\mathbbm{1}\left\{a_{t+1}^m = 1\right\}$
            \STATE For each agent-state pair $(m,s)\in[M]\times\mathcal{S}$, update the LCB and UCB of the incremental rewards as:
            % follows:

        \STATE \hspace{1em} 
                $N_{s, m}^{(t+1)} = 
        N_{s, m}^{(t)} + \mathbbm{1}
        \left\{a_{t+1}^m = 1\right\}\mathbbm{1}
        \left\{s = s_{t}^m \right\}$\;
        % $N_{s_t^m, m}(t+1) = 
        % N_{s_t^m, m}(t) + \mathbbm{1}
        % \left\{a_{t+1}^m = 1\right\}$\;
        \STATE  \hspace{1em} 
    $\hat{I}_{t+1}^m(s)=\frac{\hat{\phi}^m(s, 1)}{N_{s, m}^{(t+1)}} - \phi^m(s, 0)$
    % $\hat{I}_{t+1}^m(s_t^m)=\frac{\hat{\phi}^m(s_t^m, 1)}{N_{s_t^m, m}(t+1)} - \phi^m(s_t^m, 0)$
            %\label{alg:update-sample-mean}\;
        \STATE  \hspace{1em} $\mathrm{LCB}_{t+1}^m(s) = \hat{I}_{t+1}^m(s) - \sqrt{\frac{g\left(N_{s, m}^{(t+1)} + 2\right)}{N_{s, m}^{(t+1)} + 2}}$\;
        \STATE  \hspace{1em} $\mathrm{TEST}_{t+1}^m(s) = \hat{I}_{t+1}^m(s) + \sqrt{\frac{g\left(N_{s, m}^{(t+1)} + 2\right)}{N_{s, m}^{(t+1)} + 2}}$\;
        \STATE\hspace{1em}$\mathrm{UCB}_{t+1}^m(s)=\hat{I}_{t+1}^m(s)+\sqrt{\frac{f(t,T)}{N_{s,m}^{(t+1)}+2}}$
        \ENDFOR\;
    \end{algorithmic}
\end{algorithm}


% \begin{itemize}
%     \item Oracle Whittle: a variant of Whittle's index policy with access to the ground truth transitions and rewards, optimized for maximizing discounted reward (as described in Appendix \ref{app:whittle}).  
%     Additionally, when Whittle's index of an agent-state pair $(m,s^m)$ is negative, no action will be taken by agent $m$ when they are in state $s^m$.
%     \item Oracle Greedy: the greedy policy we defined with knowledge of the incremental reward. Additionally, when the incremental reward of an agent-state pair $(m,s^m)$ is negative, no action will be taken by agent $m$ when they are in state $s^m$.
%     \item WIQL:  a Q-learning based approach to
% % uses Q-learning to
% learn
% the value function of each arm at each state by interacting
% with the RMAB instance. The learning rate, $\alpha$,  in WIQL in our implementation is the same as the one in the original paper. Additionally, when the estimated Whittle's index of some agent-state pair $(m,s^m)$ is negative, action will not be given to that agent $m$ when they are in state $s^m$.% In our implementation, we choose the learning rate $\alpha$ exactly the same as the one used in original paper. 
% \item UCWhittle: it constructs the confidence interval of the unknown transition probability and then use it to calculate the \emph{modified} Whittle's index policy. 
% %\item UCWhittle-extreme \citep{wang2023optimistic}: it uses the upper confidence bound of the transition probability under intervention and the lower confidence bound of the transition probability under no intervention to construct the Whittle's index policy. 
% \item UCWhittle-UCB: it uses the upper confidence bounds of transition probabilities.
% \item UCWhittle-penalty: it solves an optimization problem to choose one transition probability from the confidence interval. 
% \end{itemize}
Similar to the case where stochastic dominance holds, we tested our algorithm, LGGTWUF, with two threshold values $\gamma = -0.9$ and $0$. {Note that when stochastic dominance does not hold, the incremental reward may become negative. Consequently, the cases with $\gamma = -0.9$ and $0$ exhibit similar behavior to those with $\gamma = 0.1$ and $0.4$ under stochastic dominance.}
We consider both noiseless and noisy settings and include \emph{Oracle Whittle} and \emph{Oracle Greedy} as benchmarks. 

In the \emph{noiseless reward setting} (Figure~\ref{fig:sub5} top row), 
% in all three budget settings, 
LGGT with $\gamma=-0.9$ consistently outperforms online restless bandit algorithms across three budget settings within a horizon of $1000$. 
This phenomenon is consistent with the results under the stochastic dominance assumption, demonstrating that the strong performance of our proposed algorithm extends beyond scenarios where stochastic dominance holds. {Furthermore, UCWhittlePvPriv performs close to \emph{RandomPolicy} when the stochastic dominance assumption does not hold and when the number of agents is large. }

In the \emph{noisy reward setting} (Figure~\ref{fig:sub5} bottom row),  LGGT with $\gamma=-0.9$ still consistently outperforms online restless bandit algorithms across all budget settings within a horizon of $1000$. However, the performance gap between LGGT and WIQL shrinks. We attribute this to the use of a naive conservative test (upper confidence bound) in LGGT to determine whether the incremental reward is positive, which limits its performance in the noisy setting. We hypothesize that replacing this test with more sophisticated procedures could potentially improve the convergence rate.

Finally, when stochastic dominance does not hold, the performance gap between the \emph{Oracle Whittle} and \emph{Oracle Greedy} is smaller than in Section~\ref{subsec:experiments-with-stochastic-dominance}. This highlights
the practical utility of our algorithm in general online restless bandit settings.

% that our proposed framework applies to general online restless bandit settings. 
% better adjusts to a broader setting.
% Additionally, as the budget increases, the performance gap between LGGT (\(\gamma=-10\)) and WIQL \emph{widens}. 
% Again, as shown in Figure~\ref{fig:sub5} (top left), LGGT quickly converges to a stable yet suboptimal policy, while WIQL converges more slowly but will eventually reach the \emph{Oracle Whittle} policy given a sufficiently long horizon. Nevertheless, LGGT ($\gamma=-10$) outperforms WIQL when the horizon is short even when the budget is low, highlighting the practicality of our algorithm in the online restless bandit settings. Similarly, we observe that UCWhittleUCBPriv converges slowly, and it is worse than LGGT ($\gamma=0.1$) under all three budget settings. 
% Further, UCWhittlePvPriv perform similarly to \emph{Random Policy} when
% % is almost the same level as random policy under the case where 
% the number of agents is large.
% \kyra{add runtime comparison somewhere}
% \kyra{add discussion on UCwhittle?}

% Additionally, we observe that as the budget increases, the performance gap between LGGT ($\gamma=0.1$) and WIQL increases. Although this result might be counterintuitive at the first glance: as budget decreases, the frequency of taking action decreases, it becomes harder to learn the q-function in WIQL when compared with LGGT,
% this is evident by the fact that the performance gap between the algorithms are the smallest when the budget is the lowest.
% taking non-mycotic decision when the budget is scarce might outweigh the difficulty in learning. As we observe in Figure~\ref{fig:sub3} left, LGGT converges to the a stable policy quickly but it is suboptimal for the online restless bandit problem. On the other hand, WIQL converges slowly, but will eventually converge to the \emph{Oracle Whittle} with a sufficiently long horizon. 

% In the plots we show, yellow line stands for Whittle's index policy, which serves as the optimal policy with full knowledge of the MDP.  Green and red lines are LGGT with different thresholds, purple line is WIQL, blue line is random policy, pink line is UCWhittle-UCB and brown line is UCWhittle-extreme. 
% We notice that in all three settings (Figure~\ref{fig:sub3}) LGGT with threshold $0.1$ will consistently be better than all the online algorithms we listed within 500 time steps. Though WIQL will be close to us when the budget is lower, it's still worse than us within 500 time steps. These results verify our reduction empirically.
% \begin{remark}
%     From our observation of the code \citet{wang2023optimistic} provided, WIQL was wrongly implemented there. We suspect that is the reason that the WIQL in our results performs better than the results shown in \citet{wang2023optimistic}.
% \end{remark}
% Acknowledgements should only appear in the accepted version.
\section{Conclusion}
To the best of our knowledge, this is the first paper to establish the optimality of a greedy policy in finite-horizon restless bandits. 
% that establishes the optimality of greedy policy in finite-horizon restless bandits problem.
Based on this, we reformulate the online restless bandit as a budgeted thresholding contextual bandit and provide an algorithm with a minimax constant regret bound. 
Our algorithm builds on the intuition that a policy converging faster to a suboptimal solution can outperform those converging more slowly to a more optimal one in finite-sample settings.
We further demonstrate the superior performance of our algorithm through numerical experiments.
% We emphasize the improvement of our algorithm numerically.
% \newpage
% \section*{Impact Statement}

% % Authors are \textbf{required} to include a statement of the potential 
% % broader impact of their work, including its ethical aspects and future 
% % societal consequences. This statement should be in an unnumbered 
% % section at the end of the paper (co-located with Acknowledgements -- 
% % the two may appear in either order, but both must be before References), 
% % and does not count toward the paper page limit. In many cases, where 
% % the ethical impacts and expected societal implications are those that 
% % are well established when advancing the field of Machine Learning, 
% % substantial discussion is not required, and a simple statement such 
% % as the following will suffice:
% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite


\bibliography{ref}
\bibliographystyle{icml2025}
\newpage
\appendix
\onecolumn
\input{proof}

\end{document}