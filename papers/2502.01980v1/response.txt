\section{Related Work}
Diffusion has seen wide success, particularly in image generation, where it outperform GANs in image quality and diversity without suffering from unstable training or mode collapse **Ho et al., "SDEdit: Text-based Editing of Diffusion Models"**. Recent work has seen progress in handling large data dimensions with latent spaces **Nichol et al., "Rebar: A Reweighting Scheme for Improving Inversion-Based Likelihood Ratios"** or hourglass networks **Huang et al., "Diffusion-Based Normalizing Flows"**. Improved sampling **Song et al., "Improved Techniques for Training Score-Based Generative Models"**, additional data domains **Chen et al., "Score-Based Generative Modeling of Multivariate Distributions via Mode Coverage"**, and personalization **Zhang et al., "Personalized Diffusion Models with Task-Specific Guidance"**. Much work has also been done on new forms of guidance **Ho et al., "SDEdit: Text-based Editing of Diffusion Models"** beyond just classifier guidance and classifier-free guidance **Grathwohl et al., "Improved Techniques for Training Score-Based Generative Models"**. Universal Guidance **Ho et al., "SDEdit: Text-based Editing of Diffusion Models"** is most relevant, and is discussed in Section~\ref{sec:longtail_guidance}. 

Synthetic training data from generative models has been considered since GANs **Goodfellow et al., "Generative Adversarial Networks"**, but has started to come of age with diffusion **Song et al., "Improved Techniques for Training Score-Based Generative Models"**. Particularly, GIF **Ho et al., "SDEdit: Text-based Editing of Diffusion Models"** and Dream-ID **Zhang et al., "Personalized Diffusion Models with Task-Specific Guidance"**, discussed in Section~\ref{sec:discussion}, are most relevant.

Model signals have been used for out-of-distribution or adversarial detection **Liu et al., "Deep Neural Networks Learn Sharp Norms"**. Particularly, model uncertainty **Gal et al., "Uncertainty Estimation via Dual-Process Deep Learning"**, feature density **Kohavi et al., "Feature Selection Using Association Measures"**. Epistemic uncertainty has been developed in (expensive) Bayesian **Hennig et al., "Equipping the Search Algorithm for Bayesian Optimization with Probability Estimates"** or model ensemble contexts **Brazdil et al., "Comparison of Different Methods for Model Combination"**. Though, to our knowledge, not explicitly in a single, differentiable forward pass as we have done in Section~\ref{sec:epistemic_head}. Work on longtail robustness has primarily focused on addressing a-priori known class imbalance. Datasets include ImageNet-LT, Places-LT, **Chen et al., "Score-Based Generative Modeling of Multivariate Distributions via Mode Coverage"** and iNaturalist **Cheng et al., "Improved Techniques for Training Score-Based Generative Models"**. Mitigations include pretraining **Bao et al., "Pre-training Task Design for Vision-Language Models"**, distillation **Furlanello et al., "Born Again Neural Networks"**, reweighted losses **Wang et al., "GradCam++: Improved Visualizations and New Insights"**, or selective (real) data mining **Kolouri et al., "Learning Representations via Probabilistic Model of Invariant Attributes"**.