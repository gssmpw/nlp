%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{xcolor} % Required for colored text
\usepackage{wrapfig}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% \usepackage{algorithm2e}
\usepackage[ruled,vlined]{algorithm2e}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% colors
\definecolor{warmmarble}{RGB}{221, 208, 188} % Adjust these values as 
\definecolor{faded_bubblegum}{RGB}{251, 221, 253} % Adjust these values as needed



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\TODO}[1]{\textcolor{red}{\textbf{TODO: #1}}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Towards Understanding Predictive Model Longtails with Epistemically-Guided Generative Models}
% \icmltitlerunning{Anticipating Production Model Longtails}
\icmltitlerunning{Generative Data Mining with Longtail-Guided Diffusion}

\begin{document}

\twocolumn[
% \icmltitle{Anticipating Production Model Longtails \\with Epistemically-Guided Generative Models}
\icmltitle{Generative Data Mining with Longtail-Guided Diffusion}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{David~S.~Hayden}{Cruise}
\icmlauthor{Mao~Ye}{Cruise}
\icmlauthor{Timur~Garipov}{OpenAI}
\icmlauthor{Gregory~P.~Meyer}{Cruise}
\icmlauthor{Carl Vondrick}{Colombia}
\icmlauthor{Zhao Chen}{Upwork}
\icmlauthor{Yuning Chai}{Meta}
\icmlauthor{Eric Wolff}{Cruise}
\icmlauthor{Siddhartha~S.~Srinivasa}{Cruise}
\end{icmlauthorlist}

\icmlaffiliation{Cruise}{Cruise, LLC, San Francisco, CA}
\icmlaffiliation{OpenAI}{OpenAI, San Francisco, CA}
\icmlaffiliation{Colombia}{Colombia University, New York, NY}
\icmlaffiliation{Upwork}{Upwork, Palo Alto, CA}
\icmlaffiliation{Meta}{Meta, Menlo Park, CA}



\icmlcorrespondingauthor{David Hayden}{david.hayden@getcruise.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
It is difficult to anticipate the myriad challenges that a predictive model will encounter once deployed. Common practice entails a \emph{reactive}, cyclical approach: model deployment, data mining, and retraining. We instead develop a \emph{proactive} longtail discovery process by imagining additional data during training. In particular, we develop general model-based longtail signals, including a differentiable, single forward pass formulation of epistemic uncertainty that does not impact model parameters or predictive performance but can flag rare or hard inputs. We leverage these signals as guidance to generate additional training data from a latent diffusion model in a process we call Longtail Guidance (LTG). Crucially, we can perform LTG without retraining the diffusion model or the predictive model, and we do not need to expose the predictive model to intermediate diffusion states. Data generated by LTG exhibit semantically meaningful variation, yield significant generalization improvements on image classification benchmarks, and can be analyzed to proactively discover, explain, and address conceptual gaps in a predictive model.
% 
\end{abstract}

\section{Introduction}

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/ltg_examples_by_guidance_001.jpg}
    \caption{Diffusion with Longtail Guidance (LTG) generates synthetic data that are difficult or rare for an existing predictive model. The predictive model can then a) be fine-tuned on this data to improve generalization performance, or b) the synthetic data can be analyzed to understand conceptual gaps in the predictive model. Guided generations exhibit more extreme views and lighting compared to unguided generations.}
    % order: epistemic (top), entropy (middle), energy (bottom)
    \label{fig:longtail_guidance_example_sequence}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/ltg_guidance_by_weight_v002.pdf}
    \vspace{-2em}
    \caption{Predictive model performance (left axis, red) and model-based longtail signals $f_{\phi}^{\text{lt}}$
    (right axis, green) on synthetic data generated by varying Longtail Guidance weights for two types of longtail signals: Epistemic (left plot) and Entropy (right plot). In all cases, guided synthetic data generations exhibit lower correct class probability, lower accuracy, and higher longtail signals compared to unguided generations (guidance weight 0).}
    \label{fig:longtail_guidance_signals}
\end{figure}
% Wide figure at the top of the page
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/ltg_diagram_joint2.pdf}
    \vspace{-2em}
    \caption{In Longtail Guidance, we iteratively fine-tune existing \textcolor{red}{predictive model $f_\phi$} with longtail signal $\textcolor{red}{f_\phi^{\text{lt}}}$ (left) then freeze model weights $\phi$ and generate synthetic data with latent diffusion model $\epsilon_\theta$ guided by $\textcolor{red}{f_\phi^{\text{lt}}}$ that is, by definition, rare or disproportionately hard for $\textcolor{red}{f_\phi}$ (right). Synthetic data are added to $\textcolor{red}{f_\phi}$'s training set, and the process repeats.
    }
    % \caption{In Longtail Guidance, we start with predictive model $f_\phi$ trained on existing, real data (left). We then iteratively fine-tune $f$ by generating synthetic data with diffusion model $\epsilon_\theta$ that is rare or disproportionately hard for $f$ by guiding data generation in the direction of predictive model longtail signal $f_\phi^{\text{lt}}$ (right). We also introduce a differentiable, single forward-pass formulation of epistemic uncertainty that can be grafted onto any predictive model. Attached to a model's backbone (left, top row) are $K$ duplicates of the head (left, bottom row), trained with oracle loss to encourage diversity. Model weights are protected with a stop-grad during training. Outputs of the epistemic heads are used as fixed point samples from the model's posterior predictive to estimate uncertainty. }
    \label{fig:ltg_diagram_joint}
\end{figure*}
Longtail encounters are common in production ML systems but difficult to anticipate and be robust towards. Recently, the answer has been: add more training data and more compute \cite{kaplan2020scaling}. However, the time and cost to acquire additional data, especially longtail data, can be prohibitive (e.g.~train instances occurs 1 time for every $10^4$ car instances in Berkeley DeepDrive \cite{yu2020bdd100k}). Additional compute can be traded for improved performance; see distillation \cite{yu2023dataset, gou2021knowledge} and synthetic data generation \cite{du2024dream,zhang2023expanding} for offline approaches, chain of thought \cite{yao2024tree,wei2022chain} for online approaches. While online approaches show promise, they are not applicable to real-time, safety-critical systems such as autonomous driving, which have strict parameter count and inference latency budgets.

Production ML systems are iteratively developed in a continuous, reactive cycle of: collect data, deploy model, encounter longtail edge cases, repeat. We wish to expedite this cycle and move it towards a proactive approach of longtail discovery. Existing offline approaches appeal to larger models (knowledge distillation, synthetic data generation) or summarize training data (data distillation). Yet, they do not directly account for what is rare or difficult for an existing, currently-deployed predictive model.

We wish to trade additional offline compute for performance by generatively mining additional training data that is hard or rare \emph{from the perspective of an existing predictive model}. Distinct from adversarial training and robustness approaches \cite{bansal2023leaving, zhang2022memo,  gowal2021improving, hendrycks2021many} which imbue models with invariance to distribution shift or small, often imperceptible changes, we wish to generate semantically meaningful, in-distribution examples that cause high model uncertainty.

Foundation generative models such as Stable Diffusion \cite{rombach2022high}, Pixart \cite{chen2025pixart}, GPT \cite{brown2020language, openai2023gpt4}, and Claude \cite{anthropic2024claude3} are exposed to Internet-scale data, but it is not obvious how to use them to generate data that is specifically challenging to an existing predictive model. We seek to explicitly couple the vast knowledge contained in foundation generative models with the specific challenges faced by an \emph{existing} predictive model. But how can we ensure that the generative model produces data that is relevant (has meaningful learning signal) to a given predictive model? Existing approaches, like prompt tuning and reasoning in an embedding space (such as CLIP) \cite{zhang2023expanding, du2024dream} can generate synthetic data that improves model generalization performance up to a point, but they do not condition on the deployed model.

Following, we develop model-based longtail signals (Section~\ref{sec:longtail_signals}), including a lightweight Epistemic Head (Section~\ref{sec:epistemic_head}), that effectively indicate an example as being hard or rare
while keeping the original predictive model intact. We then develop Longtail Guidance (Section~\ref{sec:longtail_guidance}), a simple method for coupling Internet-scale knowledge in diffusion models with the specific struggles of an existing predictive model. We show that synthetic data produced with Longtail Guidance yields outsized generalization improvements over existing synthetic data generation approaches and that these synthetic data exhibit semantically meaningful variations, including occlusion and extreme views, see Figures~\ref{fig:longtail_guidance_example_sequence},~\ref{fig:longtail_guidance_signals},~\ref{fig:longtail_guidance_example_many} and Section~\ref{sec:experiments} for comparisons. In summary, we contribute:
\begin{enumerate}
    \item Epistemic Head: a differentiable, single forward-pass formulation of epistemic uncertainty that does not impact existing model weights or performance, detects rare or hard examples, and guides data generation towards high-value training examples.
    \item Longtail Guidance (LTG): a latent diffusion guidance technique that generates high-value, difficult and/or rare training examples from the perspective of an existing predictive model. It requires no changes in training for the diffusion model or the predictive model.
    \item Longtail Introspection: through VLM analysis of LTG-generated data, we find keywords describing what an existing predictive model struggles with. Keywords can be used to prompt diffusion models for high-value data or to inform future data collection.
\end{enumerate}
%
\section{Model-Based Longtail Signals}
\label{sec:longtail_signals}
%
Longtail is a \emph{suitcase word} \cite{minsky2006emotion} -- it does not have a single definition. Two reasonable definitions include:
\begin{itemize}
    \item \textbf{Rare}: An input is longtail if instances similar to it are rare in training data. This definition naturally captures a data-centric view in which events are longtail if their overall occurrence is rare (independent of any model).
    \item \textbf{Hard}: An input is longtail if it is disproportionately difficult for a given model to correctly reason about.
\end{itemize}
We develop differentiable model-based longtail signals that can flag both rare and hard instances. We then use model-based longtail signals offline to generate additional longtail synthetic training data that provide outsized generalization improvements. Following, we introduce the Epistemic Head, a lightweight addition to any predictive model that does not impact predictive performance but provides superior longtail signals based on detecting hard or rare examples.

We wish to develop longtail signals that are general to loss, architecture (transformer, convolutional), or output space (discrete, continuous). Ideally, they do not affect model performance or require changes to model training. Obvious candidates would be an uncertainty measure of the output, such as entropy or variance. Others can be gathered from the anomaly and out-of-distribution detection literature. One is the Helmholtz free energy, defined as the negative log partition function of an energy model $p(y \mid x)$,
\begin{equation}
    E(x) = -T \log{\int_{y'} e^{E(x,y')/T}} dy'
\end{equation}
where $x$ is some input with energy $y$. This energy can be computed in typical classification models as the negative, temperature-scaled log-sum-exp of the logits, $E(x) = -T \log{ \sum_i e^{f_i(x) / T} }$
where $f_i(x)$ is the $i^{\text{th}}$ logit of a predictive model $f(x)$ that forms a probability distribution by softmax. Energy was shown to be effective in discriminating in-distribution and out-of-distribution data \cite{liu2020energy}. Like entropy and variance, it requires no model adjustment or retraining. We use entropy and energy as baselines.

\subsection{Epistemic Head}
\label{sec:epistemic_head}
%
\begin{figure}[]
    \centering
    \includegraphics[trim=0px 0px 0px 0px, clip, width=0.35\textwidth] {figures/epistemic_manifold.pdf}
    \caption{Epistemic $E(y, \phi)$ vs aleatoric $\mathbb{E}_\phi[\mathcal{U}(y \mid \phi)]$ uncertainty. Epistemic increases with distance from the data manifold. Aleatoric increases with proximity to the decision boundary.}
    \label{fig:epistemic_aleatoric_manifold}
\end{figure}
%
Traditional measures of predictive uncertainty, like entropy, do not distinguish what is rare from what is ambiguous or hard. To address longtail scenarios, we would ideally account for both. To do so, we decompose predictive uncertainty into two components: epistemic and aleatoric (Figure~\ref{fig:epistemic_aleatoric_manifold}). For test input $x$ with unknown label $y$ and predictive model parameters $\phi$ drawn from distribution $\Phi$, epistemic uncertainty is defined as \cite{depeweg2018decomposition},
\begin{equation}
    E(y, \Phi) = \mathcal{U}(y) - \mathbb{E}_\phi\left[ \mathcal{U}(y \mid \phi) \right]
    \label{eqn:epistemic}
\end{equation}
where we implicitly condition on $x$ in each term, and where $\mathcal{U}$ is an uncertainty measure, such as entropy for discrete $y$ or variance for continuous $y$. If entropy, Eqn~\ref{eqn:epistemic} is equivalent to measuring the mutual information between parameter distribution $\Phi$ and unknown target $y$, which informs us about how much knowing about one tells us about the other.

In principle, epistemic uncertainty can be computed by sampling from the posterior predictive distribution:
\begin{equation}
    p(y \mid x) = \int p(y \mid \phi, x)\ p(\phi \mid x_{\text{train}}, y_{\text{train}})\ d\phi.
    \label{eqn:posterior_predictive}
\end{equation}
The first term is a likelihood for test label $y$ and the second term is a posterior distribution for model parameters $\phi$, conditioned on all previous training data $(x_{\text{train}}, y_{\text{train}})$.

Performing the inference required to sample from Eqn~\ref{eqn:posterior_predictive} is intractable for modern neural networks. Nevertheless, it can be approximated. One approach is variational inference, typified by Monte Carlo dropout \cite{gal2016dropout}, where the same input is passed through a network many times (often $50$ or more forward passes), with test time variation in each pass enabled by random Bernoulli dropout. Unfortunately, this would be expensive to compute and is not conveniently differentiable due to discrete sampling \cite{jang2016categorical}. It is also bested in practical terms by a model ensemble, where a small number of models, often $3-5$, are trained (independently \cite{lakshminarayanan2017simple} or otherwise \cite{maddox2019simple}). However, computing a gradient (which we will need to generate longtail synthetic data) across $K$ instances of a model introduces substantial memory overhead and compute latency.

In Figure~\ref{fig:ltg_diagram_joint} (left), we introduce a lightweight ensembling technique called the Epistemic Head, which provides a superior longtail signal with no impact on model performance and negligible changes to model parameter count, training time, and inference time (see Supplement~\ref{sup:section:Computational_Cost}). Inspired by LoRA \cite{hu2021lora} and the Hydra architecture \cite{tran2020hydra}, we duplicate the head of an existing prediction model $K$ times and jointly train with the same loss as the base model but with diversity encouraged through an oracle loss \cite{guzman2012multiple} that only propagates per-example loss through the best-performing head. Head outputs act as fixed-point samples from the posterior predictive (Eqn~\ref{eqn:posterior_predictive}) under a prior determined by weight initialization and permit differentiable computation of longtail signals in a single forward pass, including $E(y, \Phi)$, 

\vspace{-1.5em} % Consider removing this at final layout

{\small
\begin{equation}
    f_\phi^{\text{lt}}(x)
    \approx \mathcal{U}\left( \frac{1}{K} \sum_k p(y \mid \phi_k) \right) - \frac{1}{K} \sum_k \mathcal{U}\left( p(y \mid \phi_k) \right).
    \label{eqn:production_model_longtail}
\end{equation}
}

Base model performance is protected by a stop-grad at training time so that, similar to LoRA \cite{hu2021lora}, the Epistemic Head does not impact existing model weights. In Figure~\ref{fig:longtail_validation_performance}, we demonstrate that longtail signals from the Epistemic Head more effectively indicate rare or hard examples than do entropy or energy. In particular, we train ViT-B ImageNet-LT classifiers according to \cite{xu2023learning} and compare how well our proposed longtail signals (entropy, energy, epistemic) detect rare or hard test examples. Examples are defined as rare if they come from a longtail class. Examples are defined as hard if the model incorrectly predicts the label. In Supplement~\ref{sup:section:hard}, we further show that detected examples are disproportionately hard.
%
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/detect_rare_hard.pdf}
    \vspace{-1em}
    \caption{The Epistemic Head is a better indicator of rare or hard ImageNet-LT validation examples than entropy, energy, or epistemic signals from an independently-trained ensemble.}
    \vspace{-1em}
    \label{fig:longtail_validation_performance}
\end{figure}
%
%
\section{Longtail Guidance}
\label{sec:longtail_guidance}
% Wide figure at the top of the page
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/longtail_guidance_imagenet_example_v002.jpg}
    \vspace{-2em}
    \caption{Synthetic data generated for 100 ImageNet classes with increasing Longtail Guidance strength, guided by a SOTA ViT ImageNet-LT classifier $f_\phi$. Views frequently become more extreme, occluded, or cut off. They also become more difficult for $f_\phi$ (see Figure~\ref{fig:longtail_guidance_signals})}
    \label{fig:longtail_guidance_example_many}
\end{figure*}
%
To motivate Longtail Guidance, we briefly review diffusion. Diffusion models learn a continuous data distribution $p_\theta(x_0) = \int p_\theta(x_{0:T}) dx_{1:T}$ from a finite set of data samples by defining a forward noising process $q(x_{1:T} | x_0)$ over latent states $x_{1:T}$, and learning a reverse denoising process $p_\theta(x_{0:T})$. In the DDPM \cite{ho2020denoising} and DDIM \cite{song2020denoising} formulations, the forward process is a Markov chain that iteratively add noise according to schedule $\alpha_{1:T}$ that decreases over steps $1, \ldots, T$,
\begin{equation}
q(x_t | x_{t-1}) = N(\sqrt{\alpha_t}x_{t-1}, (1 - \alpha_t) I)
\end{equation}
However, they differ in the learned reverse process: DDPM models it as a Markov chain,
\begin{equation}
p_\theta(x_t | x_{t-1}) = N(\mu_\theta(x_t, t), \Sigma_\theta(x_t, t))  
\label{eqn:ddpm_sample}
\end{equation}
where, by reparameterization for numerical stability, the diffusion network $\epsilon_{\theta}(x_t, t)$ learns to predict the previously-sampled noise at each step rather than the process mean. In DDIM, the reverse process is instead modeled as non-Markovian. At each step, it predicts,
\begin{equation}
x_{t-1} = \sqrt{\alpha_{t-1}} \hat{x}_0^{(t)} + v_t + \sigma_t \epsilon_t
\label{eqn:ddim_sample}
\end{equation}
for terminal state estimate $\hat{x}_0^{(t)}$, directional vector $v_t$ pointing towards $x_t$, and sampled noise $\epsilon_t \sim N(0, I)$,
\begin{align}
    \hat{x}_0^{(t)} &= \alpha_t^{-0.5}\left(
    {x_t - \sqrt{1 - \alpha_t} \epsilon_{\theta}(x_t,t)}\right) \label{eqn:terminal_state} \\
    v_t &= (1 - \alpha_{t-1} - \sigma_t^2)^{1/2} \epsilon_{\theta}(x_t, t) .
\end{align}
SDE formulations of diffusion \cite{song2020score} generalize DDPM and DDIM. Denoising models can be learned in one framework (DDPM) and sampled in another (DDIM).

As stated, diffusion models sampled according to Eqns~\ref{eqn:ddpm_sample},~\ref{eqn:ddim_sample} are unconditional; they will sample instances that are distributed approximately according to the data. This can be changed with guidance. In classifier-free guidance \cite{ho2022classifier}, the diffusion model is trained on pairs $(x,c)$ for data $x$ and conditioning vector $c$ (for example, class labels or CLIP-encoded text).
%
% As stated, diffusion models sampled according to Eqn~\ref{eqn:ddpm_sample} or Eqn~\ref{eqn:ddim_sample} are unconditional; they will sample instances that are distributed approximately according to the data. This can be changed with guidance. In classifier-free guidance \cite{ho2022classifier}, the diffusion model is trained on pairs $(x,c)$ for data $x$ and conditioning vector $c$ (for example, class labels or CLIP-encoded text). Conditioning vectors are randomly dropped out at training time so that a conditional model $\epsilon_\theta(x_t, t, c)$ and an unconditional model $\epsilon_\theta(x_t, t)$ are implicitly learned. Sampling works as in the unconditional case, but an extrapolation of the conditional and the unconditional denoising estimates is computed,
% \begin{equation}
% \hat{\epsilon}_t = (1+w)\epsilon_\theta(x_t, t, c) - w\epsilon_\theta(x_t, t)
% \end{equation}
% where guidance weight $w$ modulates the strength of the conditioning signal. Classifier-free guidance is used to great effect by Stable Diffusion, Pixart, and DALL-E to incorporate text conditioning. It is an effective technique whose primary limitations are that it requires conditioning information to be available at training time and that it requires two forward passes at each denoising step.
%
In contrast, classifier guidance \cite{dhariwal2021diffusion} can be used during the sampling process even when no conditioning information was available at diffusion training time. It operates by biasing the denoising estimate in the direction of the gradient of a differentiable signal, $\nabla f(x_t, t)$,
\begin{equation}
\hat{\epsilon}_t = \epsilon_\theta(x_t, t) - w \nabla_{x_t} f(x_t, t) \sigma_t.
\end{equation}
Commonly, $f(x_t,t) = \log{p(y_i \mid x_t, t)}$, the log probability of class $i$ under trained classifier $f_{\phi'}$. But this only works if classifier $f_{\phi'}$ is trained on intermediate noisy diffusion states $x_{1:T}$ and accepts denoising step $t$ (thus, it trains on triples of (noisy data $x_t$, step $t$, and label $y$) instead of a standard formulation of (clean data $x$ and label $y$)). Training $f_{\phi'}$ on intermediate diffusion states is required because the distribution of $x_t$ differ from the original data $x$. Classifier-free guidance can be combined with classifier guidance.

Using classifier guidance to generate synthetic data in the longtail of an existing production model $f_\phi$, as defined by model-based longtail signal $f_\phi^{\text{lt}}(x)$ is appealing because it can be applied to existing diffusion models without retraining. However, as stated, classifier guidance presents a dilemma: we must either fine-tune a noise-aware model $f_{\phi'}(x_t,t)$ from the original model $f_\phi(x)$ on noisy, intermediate diffusion states, at which point $f_{\phi'}$ no longer reflects the production model performance of $f_\phi$, or we must deploy a production model $f_{\phi'}$ that wastes capacity on intermediate diffusion states that it will not encounter in production.

There is an additional challenge with using classifier guidance for longtail data generation: SOTA diffusion models perform training and sampling in a lower dimensional latent space $\mathcal{Z}$ by first encoding the original data using a pretrained VAE, noising (at training) and denoising (at inference) among latent states $z_{0:T}$, then decoding the final result back to data space $\mathcal{X}$. How can we pass latent $z_t$ through production model $f_\phi$ when it operates in data space $\mathcal{X}$?

\begin{algorithm}[h]
\SetKwInOut{Input}{Input}
\caption{Longtail Guidance}
\label{alg:longtail_guidance}
% \KwIn{Latent diffusion model $\epsilon_\theta(z_t, t)$, predictor $f_\phi(x)$
% latent decoder $D$, noise schedule $\sigma_{1:T}$, weight $w$}
\Input{Latent diffusion model $\epsilon_\theta(z_t, t)$, predictor $f_\phi(x)$
decoder $D$, noise schedule $\sigma_{1:T}$, weight $w$}
% \KwOut{Generated sample $x$}
% \BlankLine
\textbf{Initialize:} $z_T \sim \mathcal{N}(0, I)$\; \BlankLine
\For{$t = T-1, \dots, 0$}{
    Estimate terminal latent state $\hat{z}_t^{0} = P(z_t)$ as in Eqn.~\ref{eqn:terminal_state}\;
    
    Decode terminal data state: $\hat{x}_t^0 = D(\hat{z}_t^0)$\;
    
    Compute model longtail signal $f_\phi^{\text{lt}}(\hat{x}_t^{0})$ as in Eqn.~\ref{eqn:production_model_longtail}\;

    Bias denoising estimate as in Eqn.~\ref{eqn:longtail_guidance}
        
    Compute $z_{t-1}$ as in Eq.~\ref{eqn:ddim_sample}\;
}
\Return $x = D(z_0)$\;
\end{algorithm}

With Longtail Guidance, we find a simple diffusion guidance approach that couples longtail signals from an existing production model to the Internet-scale knowledge of a latent diffusion model. Surprisingly, it requires no retraining of either the diffusion model or the production model and, in particular, it does not require the production model to be trained on intermediate diffusion states or time-conditioned inputs, as is common in Classifier Guidance.

The key idea in Longtail Guidance (LTG) is that we can differentiably estimate a terminal latent state $\hat{z}_t^0 = P(z_t)$ with appropriate diffusion samplers (including DDIM), decode to an estimated terminal data state $\hat{x}_t^0 = D(\hat{z}_t^0)$, compute longtail signal $f_\phi^{\text{lt}}(\hat{x}_t^0)$ from the existing production model (that has only ever seen real production data), and then bias the denoising estimate (in latent space) in the direction of higher production model longtail signal (See Figure~\ref{fig:ltg_diagram_joint} and Algorithm~\ref{alg:longtail_guidance} for complete details):
\begin{equation}
    \hat{\epsilon}_t = \epsilon_\theta(z_t, t) - w \nabla_{z_t} f_\phi^{\text{lt}}\left(D(P(z_t))\right) \sigma_t .
    \label{eqn:longtail_guidance}
\end{equation}
It is unintuitive that LTG would work since production model $f_\phi$ has only ever trained on clean (data, label) pairs, not intermediate diffusion states. Empirically, however, we find that we can generate synthetic data for which production model $f_\phi$ exhibits lower probability of the correct class, lower accuracy, and higher longtail signal. And so long as we do not adjust the longtail guidance weight $w$ too high, the diffusion model reliably generates data that adheres to the expected class label. Figures~\ref{fig:longtail_guidance_example_sequence},~\ref{fig:longtail_guidance_example_many} and Supplement~\ref{sup:section:additional_examples} (high-res) show example longtail synthetic ImageNet data using a SOTA ViT model \cite{xu2023learning} for guidance. Figure~\ref{fig:longtail_guidance_signals} quantitatively analyzes model performance and longtail signals on LTG-guided synthetic data generation over multiple runs of $24k$ generations spanning all ImageNet classes. Notably, the probability of the expected class can be decreased to one-half or even one-third of its original value (from the perspective of production model $f_\phi$), while staying in-distribution (as evidenced by generalization improvements in Section~\ref{sec:experiments}) and the model longtail signal can be more than doubled over baseline (unguided) synthetic data generation. Qualitatively, LTG data exhibit semantically meaningful changes, including more extreme, occluded, or cut off views.
% In Supplement~\ref{sup:section:why_ltg}, we analyze why the predictive model does not need to be trained on intermediate diffusion states.

Longtail Guidance is similar to Universal Guidance \cite{bansal2023universal} in that both perform differentiable decoding of latent states (see also \cite{jiang2023motiondiffuser}). However, Universal Guidance is significantly more expensive and fails to remain in-distribution when guided by model-based longtail signals. In particular, Universal Guidance performs so-called recurrent and backward sampling passes, where recurrent sampling permits each diffusion step to refine its denoising estimate (each time computing a costly gradient) and each backward pass optimizes an objective (within each denoising and recurrent iterate, itself requiring multiple optimization iterates). See Figure~\ref{fig:ug_guidance_example} for direct comparison.

Although Longtail Guidance is not the first approach to use the predicted, decoded terminal state $D(P(x_t))$, we uniquely (to our knowledge) show in Supplement~\ref{sup:section:why_ltg} why the predictive model performing guidance does not need to be trained on intermediate diffusion states: $D(P(x_t))$ is closer in distribution to real training data than is $x_t$.

LTG requires no training for either the diffusion or the production model. Each can be used off-the-shelf. It's primary limitation is that the gradient $d\hat{x}_t^0 / d\hat{z}_t^0$ calculated by decoding is expensive (see Supplement~\ref{sup:section:Computational_Cost}).
%
\begin{figure}[]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/ug_comparison.jpg}
    \vspace{-1em}
    \caption{Universal Guidance (top) vs Longtail Guidance (bottom). Universal Guidance, when driven by model-based longtail signals, successfully raises those signals but it does so by generating data that is no longer in-distribution. It is also much more expensive.}
    \label{fig:ug_guidance_example}
\end{figure}

\section{Experiments}
\label{sec:experiments}
In Section~\ref{sec:experiments:benchmarks}, we compare predictive model generalization improvements when training data is augmented with synthetic data generated by Longtail Guidance, when training data is augmented with existing synthetic data generation approaches, and when training data is augmented with traditional data augmentations approaches. In Section~\ref{sec:experiments:interpret}, we reduce synthetic data generated with Longtail Guidance to a set of text descriptions that describe attributes of a predictive model's longtail. We demonstrate that these descriptions are meaningful by showing that they produce higher-value synthetic data than manually prompt-tuned diffusion. In Supplement~\ref{sup:section:longtail_experiment}, we additionally demonstrate that LTG improves the ImageNet-LT generalization performance of SOTA ViT models that compensate for class-imbalance.
%\footnote{We will add Supplement~\ref{sup:section:longtail_experiment} to main paper on acceptance}
In Section~\ref{sec:discussion}, we examine why Longtail Guidance outperforms existing synthetic data generation approaches.

\subsection{LTG Improves Predictive Model Generalization}
\label{sec:experiments:benchmarks}
We compare Longtail Guidance with three measures (Epistemic, Entropy, Energy) to the recent Guided Imagination Framework (GIF) \cite{zhang2023expanding} and Dream the Impossible (Dream-ID) \cite{du2024dream} synthetic data generation approaches. For baselines, we compare to prompt-tuned Stable Diffusion 1.4 (SD), prompt-tuned DALL-E2, and masked autoencoder (MAE) data generation \cite{he2022masked}. We also compare to data augmentation baselines: Cutout \cite{devries2017improved}, GridMask \cite{chen2020gridmask}, RandAugment \cite{cubuk2020randaugment}, AutoAugment \cite{cubuk2019autoaugment}, CutMix \cite{yun2019cutmix}, AugMix \cite{hendrycks2019augmix} and adversarial robustness approaches DeepAugment \cite{hendrycks2021many}, and MEMO \cite{zhang2022memo}. Finally, we report CLIP zero-shot and distillation performance \cite{radford2021learning}.

We evaluate on seven natural image datasets spanning fine-grained, coarse-grained, and mixed coarse/fine classification tasks. Datasets include ImageNet, ImageNet-V2, ImageNet-A, Stanford Cars \cite{krause2013collecting}, Oxford Flowers \cite{nilsback2008automated}, Oxford Pets \cite{parkhi2012cats}, and Caltech101 \cite{fei2004learning}. We reproduce baseline predictive models (Original) that are not exposed to synthetic data, with comparable performance (within $1\%$) of the baselines used in \cite{du2024dream, zhang2023expanding}.\footnote{Baseline model checkpoints were not available so we reproduce them using the reported architecture and training recipe.} We iteratively perform additional fine-tuning with synthetic training data that is equal in quantity across all conditions, use the same augmentations (random rotation, reflection, and center cropping), and report generalization performance as top-1 accuracy. Results are averaged over three runs. Additional experiment details are in Supplement~\ref{sup:section:experiment_details} and dataset details are in Supplement~\ref{supp:sec:dataset_details}. We fix hyperparameters (LTG weight, number of Epistemic Heads) across all datasets according to Supplement~\ref{sup:section:ablations}.

In Table~\ref{tab:results_gif}, we compare to baselines under the Dataset Expansion task as defined in GIF \cite{zhang2023expanding}. For parity, we fine-tune ResNet-50 models for 100 epochs and generate synthetic data equivalent to $20\times$ the original dataset size for Caltech, Cars and Flowers, and $30\times$ the original dataset size for Pets. GIF generates all synthetic data at once whereas we evenly distribute data generation throughout training epochs to better capture evolving model-based longtail challenges, see Supplement~\ref{sup:section:experiment_details} for details. In each expansion, synthetic examples per class are equal in quantity to the original number of real examples.

In Table~\ref{tab:results_imagenet}, we compare to baselines under the in-distribution synthetic data generation task as defined in Dream the Impossible \cite{du2024dream} and discussed in Supplement~\ref{sup:section:experiment_details}. For parity, we train ResNet-34 models for 100 epochs and generate 1k synthetic data per class. Models are trained on ImageNet-100 and evaluated on ImageNet-100, ImageNet-A, and ImageNet-V2 subsets defined in Supplement~\ref{supp:sec:dataset_details}.

In each benchmark, Longtail Guidance provides substantial improvements over existing synthetic data generation methods and data augmentation baselines. In particular, it provides an average $5.6$ points of additional top-1 accuracy compared to GIF or Dream-ID, and an average $15.0$ points of additional top-1 accuracy compared to text-prompted Stable Diffusion. Aggregate improvements from the Epistemic Head over alternative model-based longtail signals (energy, entropy) are mild but significant and come at negligible inference or training time cost as described in Supplement~\ref{sup:section:Computational_Cost}. In Supplement~\ref{sup:section:continuous_mining}, we show that predictive model generalization can be further improved by engaging in more cycles of fine-tuning and synthetic data generation.

\begin{table}[h]
{\small \setlength{\tabcolsep}{4pt}
\centering
\vspace{-0.5em}
\caption{Top-1 Accuracy on coarse-grained (Caltech) and fine-grained (Pets, Cars, Flowers) natural image datasets under the Dataset Expansion task as defined in \cite{zhang2023expanding}. }
\begin{tabular}{lccccc}
\toprule
\textbf{Dataset} & \textbf{Caltech} & \textbf{Cars} & \textbf{Flowers} & \textbf{Pets} & \textbf{Avg} \\
\midrule
Original & 26.3 & 19.8 & 74.1 & 6.8 & 31.8 \\
CLIP  & 82.1 & 55.8 & 71.2 & 85.4 & 72.3 \\
CLIP Distill & 33.2 & 18.9 & 75.1 & 11.1 & 34.6 \\
\midrule
\textbf{Expanded} & & & & & \\
Cutout  & 51.5 & 25.8 & 77.8 & 38.7 & 48.5  \\
GridMask  & 51.6 & 28.4 & 80.7 & 37.6 & 49.6  \\
RandAugment  & 57.8 & 43.2 & 83.8 & 48.0 & 58.2  \\
MAE  & 50.6 & 25.9 & 76.3 & 39.9 & 48.2  \\
DALL-E2  & 61.3 & 48.3 & 84.1 & 61.7 & 63.8  \\
SD  & 51.1 & 51.7 & 78.8 & 57.9 & 59.9  \\
GIF-MAE  & 58.4 & 44.5 & 84.4 & 52.4 & 59.9  \\
GIF-DALLE  & 63.0 & 53.1 & 88.2 & 66.4 & 67.7  \\
GIF-SD  & 65.1 & 75.7 & 88.3 & 73.4 & 75.6  \\
\midrule
LTG (Energy) & 70.4 & \textbf{85.3} & 90.3 & 80.0 & 81.5  \\
LTG (Entropy) & 70.6 & 84.6 & 89.9 & 81.6 & 81.7  \\
LTG (Epistemic) & \textbf{71.5} & 85.1 & \textbf{90.9} & \textbf{82.0} & \textbf{82.4} \\
\bottomrule
\end{tabular}
\vspace{-1.5em}
\label{tab:results_gif}
}
\end{table}

\begin{table}[h]
{\small \setlength{\tabcolsep}{2pt}
\centering
\caption{Top-1 Accuracy on the in-distribution synthetic data generation task for ImageNet variants as defined in \cite{du2024dream}}
\begin{tabular}{lcccc}
\toprule
\textbf{Methods} & \textbf{ImageNet} & \textbf{ImageNet-A} & \textbf{ImageNet-v2} & \textbf{Avg} \\ 
\midrule
Original & 87.28 & 8.69 & 77.80 & 57.92 \\
RandAugment & 88.10 & 11.39 & 78.90 & 59.46 \\
CutMix & 87.98 & 9.67 & 79.70 & 59.12 \\
AutoAugment & 88.00 & 10.85 & 79.70 & 59.52 \\
AugMix & 87.74 & 10.96 & 79.20 & 59.30 \\
DeepAugment & 86.86 & 10.79 & 78.30 & 58.65 \\
MEMO & 88.00 & 10.85 & 78.60 & 59.15 \\
SD & 87.74 & 11.18 & 79.20 & 59.37 \\
DREAM-ID & 88.46 & 12.13 & 80.40 & 60.33 \\
\midrule
LTG (Energy) & 90.00 & 20.20 & 81.05 & 63.75 \\
LTG (Entropy) & 90.04 & \textbf{22.70} & 80.66 & 64.47 \\
LTG (Epistemic) & \textbf{90.30} & 22.09 & \textbf{81.54} & \textbf{64.64} \\
\bottomrule
\end{tabular}
\vspace{-0.5em}
\label{tab:results_imagenet}
}
\end{table}

\subsection{LTG Produces Meaningful Longtail Data}
\label{sec:experiments:interpret}
We examine synthetic data generated by LTG to determine whether they exhibit meaningful variation from baseline synthetic data generated by prompt-tuned diffusion. This is a difficult and fundamentally qualitative task to manually perform at scale. To make it quantitative, we ask: do text descriptions of LTG-generated data lead to predictive model generalization improvements when they are used to generate additional synthetic data (without LTG)?

Starting with the strongest baseline model $f_\phi$ trained on Flowers (Original from Table~\ref{tab:results_gif}), we ask a VLM \cite{liu2024llavanext} to caption real training instances $x_\text{real}$ and also synthetic data $x_\text{ltg}$ generated by LTG as guided by $f_\phi^{\text{lt}}$. For each synthetic instance, $N$ novel keywords are found by computing the token embeddings that are furthest in cosine similarity from all real data token embeddings. An LLM \cite{openai2023gpt4} is then provided with a set of (caption, novel keyword) pairs for each class and asked to summarize them into $P$ refined prompts. We call this Longtail Introspection.

Refined prompts are used to generate additional synthetic data $x_\text{inspect}$ by diffusion (without LTG). We train $f_\phi$ on synthetic data generated by Longtail Introspection and compare it to synthetic data generated by manual prompt tuning (40 unique prompts per class, as in \cite{zhang2023expanding}; details in Supplement~\ref{sup:sec:longtail_introspection}). Table~\ref{tab:results_longtail_introspection} shows that Longtail Introspection significantly outperforms manually prompt-tuned diffusion (defined in Supplement~\ref{sup:section:experiment_details}), quantitatively supporting that data generated by LTG exhibit meaningful variation. Figure~\ref{fig:ltg_introspection_example} shows example longtail keywords.

\begin{figure}[]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/ltg_introspection.jpg}
    \vspace{-1em}
    \caption{Longtail keywords from LTG-generated Flowers data.}
    \vspace{-1em}
    \label{fig:ltg_introspection_example}
\end{figure}

% Table example
\begin{table}[h]
    \centering
    \caption{Synthetic data generated by prompts based on VLM descriptions of LTG data outperform synthetic data generated by manually prompt-tuned diffusion, quantitatively suggesting that data generated by LTG exhibit meaningful and challenging variation from the perspective of predictive model $f_\phi$.
    }
    
    \begin{tabular}{lc}
        \toprule
        \textbf{Condition} & \textbf{Accuracy} \\ \hline
        No Synthetic Data & 74.1 \\
        Manual Prompt Tuning & 78.8 \\
        Longtail Introspection & \textbf{84.3} \\
        % Longtail Guidance & \textbf{90.9} \\
        \bottomrule
    \end{tabular}
    \label{tab:results_longtail_introspection}
\end{table}

\subsection{Discussion}
\label{sec:discussion}
Longtail Guidance significantly outperforms leading synthetic data generation baselines including GIF and Dream-ID even though it requires no prompt tuning. To explain this, we note that GIF and Dream-ID reason in latent CLIP space to create new embedding vectors with which to prompt a diffusion model. For each real data instance, GIF creates $K$ additional synthetic instances by jointly optimizing $K$ embedding vectors (initialized by CLIP image encoding)
such that they are difficult for CLIP zero-shot classification (high entropy), remain close to the target class (high log prob of target class), and diverse (high KL divergence to a mean embedding). In Dream-ID, a separate model is trained to predict class embedding vectors from (real data, CLIP-based class text embedding vector) pairs. From this, a manifold in CLIP space can be sampled along class boundaries to generate new diffusion conditioning vectors.

We posit that synthesizing data that is difficult or rare for a foundation model, like CLIP, is different from synthesizing data that is difficult or rare for a specific, deployed predictive model. Figure~\ref{fig:flowers_clip_prod.png} quantifies this by showing that, even when CLIP zero-shot performance and a production model have similar aggregate performance $74.1\%$ vs $71.2\%$, the classes (and data instances) that are difficult for CLIP are different from the classes that are difficult for the production model--with average per-class accuracy differences of $34\%$!

In both GIF and Dream-ID, reasoning is only conditioned on training data and restricted to CLIP embedding space. In Longtail Guidance, we instead condition on longtail signals coming directly from the predictive model we wish to improve. We know that these signals identify rare or disproportionately hard examples (Figure~\ref{fig:longtail_validation_performance}) and that synthetic data generated by LTG reliably have lower probability of correct classification, lower accuracy, and higher longtail signals (Figure~\ref{fig:longtail_guidance_signals}) under the predictive model. This is most strikingly demonstrated in the more than $80\%$ generalization improvement over Dream-ID on the naturally challenging examples of ImageNet-A.

\begin{figure}[]
    \centering
    \includegraphics[width=0.45\textwidth] {figures/clip_vs_model_caltech_cars_flowers.pdf}
    \vspace{-1em}
    \caption{Reasoning in latent CLIP space for synthetic data generation does not account for differences in what is difficult for CLIP versus what is difficult for a production model. We plot (sorted) absolute difference in per-class accuracy between a CLIP zero-shot classifier and a well-trained production model. Even when aggregate performance is similar, as in Flowers (74.0\% vs 71.2\% top-1 accuracy), average per-class difference is high ($33\%$). 
    }
    \label{fig:flowers_clip_prod.png}
\end{figure}

\section{Related Work}
Diffusion has seen wide success, particularly in image generation, where it outperform GANs in image quality and diversity without suffering from unstable training or mode collapse \cite{dhariwal2021diffusion}. Recent work has seen progress in handling large data dimensions with latent spaces \cite{chen2025pixart, podell2023sdxl, rombach2022high} or hourglass networks \cite{crowson2024scalable}, improved sampling \cite{lu2022dpm, karras2022elucidating, ho2020denoising, song2020denoising}, additional data domains \cite{ran2024towards, pronovost2023scenario, zhong2023language}, and personalization \cite{ruiz2023dreambooth, kumari2023multi}. Much work has also been done on new forms of guidance \cite{wallace2023edict, yu2023freedom, wallace2023end, zhang2023adding} beyond just classifier guidance and classifier-free guidance \cite{dhariwal2021diffusion, ho2022classifier}. Universal Guidance \cite{bansal2023universal} is most relevant, and is discussed in Section~\ref{sec:longtail_guidance}. 

Synthetic training data from generative models has been considered since GANs \cite{li2022bigdatasetgan}, but has started to come of age with diffusion \cite{azizi2023synthetic, zhou2023training}, particularly for high-resolution datasets where fidelity matters. GIF \cite{zhang2023expanding} and Dream-ID \cite{du2024dream}, discussed in Section~\ref{sec:discussion}, are most relevant.

Model signals have been used for out-of-distribution or adversarial detection \cite{huang2021importance, hsu2020generalized, cohen2020detecting}, particularly model uncertainty \cite{van2020uncertainty} or feature density \cite{lee2018simple}. Epistemic uncertainty has been developed in (expensive) Bayesian \cite{gal2016dropout} or model ensemble contexts \cite{liu2019accurate, depeweg2018decomposition, wilson2020bayesian, lakshminarayanan2017simple}, though, to our knowledge, not explicitly in a single, differentiable forward pass as we have done in Section~\ref{sec:epistemic_head}. Work on longtail robustness has primarily focused on addressing a-priori known class imbalance. Datasets include ImageNet-LT, Places-LT, \cite{liu2019large} and iNaturalist \cite{van2018inaturalist}. Mitigations include pretraining \cite{he2022masked, bao2021beit}, distillation \cite{xiang2020learning}, reweighted losses \cite{xu2023learning, ross2017focal}, or selective (real) data mining \cite{jiang2022improving}.

\section*{Conclusion}
We developed model-based longtail signals that do not impact model weights or performance and are leveraged by Longtail Guidance, a synthetic data generation approach that explicitly conditions on an existing predictive model to generate examples that are rare or hard from that model's perspective. We show that training on LTG-generated data provides stronger generalization improvements than synthetic data generation and data augmentation baselines over eight datasets. We further demonstrate that longtail synthetic data generations can be rendered into meaningful text descriptions that can aid future (real or synthetic) data collection priorities.

Predictive models are being deployed more than ever before. Increasingly, they will encounter longtail scenarios that human operators cannot predict in advance. Foundation models can be used to mitigate some risk, but we cannot shoehorn the entirety of Internet-scale knowledge into every deployed model due to capacity and compute constraints. By letting an existing predictive model \emph{speak for itself}, we can expend offline compute to generatively mine high-value synthetic training data. Further reducing these synthetic examples to human- and machine-readable text suggests a future where we can move away from slow, reactive longtail mitigation towards fast, proactive longtail discovery.

% \clearpage

% \section*{Broader Impacts}
% We develop model-based longtail signals, model-conditioned longtail synthetic data generation, and longtail interpretation techniques that have the potential to discover and mitigate production model failure modes before they occur. This has tremendous potential for improving the quality and safety of deployed ML applications. However, they may also have negative impacts if human operators rely too heavily on our approaches and not enough on traditional safeguards. We recommend our methods be used to supplement, not replace, standard deployment practices.

\bibliography{main}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}
\begin{wraptable}{r}{0.5\textwidth}
\vspace{-3.25em}
\centering
{ \small
\begin{tabular}{l r r r r}
\toprule
\textbf{Method} & \textbf{Many} & \textbf{Med.} & \textbf{Few} & \textbf{Acc} \\
\midrule
CE \cite{cui2019class} & 64.0 & 33.8 & 5.8 & 41.6 \\
LDAM \cite{cao2019learning} & 60.4 & 46.9 & 30.7 & 49.8 \\
c-RT \cite{kang2019decoupling} & 61.8 & 46.2 & 27.3 & 49.6 \\
$\tau$-Norm \cite{kang2019decoupling} & 59.1 & 46.9 & 30.7 & 49.4 \\
Causal \cite{tang2020long} & 62.7 & 48.8 & 31.6 & 51.8 \\
Logit Adj. \cite{wang2020long} & 61.1 & 47.5 & 27.6 & 50.1 \\
RIDE(4E) \cite{wang2020long} & 68.3 & 53.5 & 35.9 & 56.8 \\
MiSLAS \cite{zhong2021improving} & 62.9 & 50.7 & 34.3 & 52.7 \\
DisAlign \cite{zhang2021distribution} & 61.3 & 52.2 & 31.4 & 52.9 \\
ACE \cite{cai2021ace} & 71.7 & 54.6 & 23.5 & 56.6 \\
PaCo \cite{cui2021parametric} & 68.0 & 56.4 & 37.2 & 58.2 \\
TADE \cite{zhang2022self} & 66.5 & 57.0 & 43.5 & 58.8 \\
TSC \cite{li2022targeted} & 63.5 & 49.7 & 30.4 & 52.4 \\
GCL \cite{li2022long} & 63.0 & 52.7 & 37.1 & 54.5 \\
TLC \cite{li2022trustworthy} & 68.9 & 55.7 & 40.8 & 55.1 \\
BCL \cite{zhu2022balanced} & 67.6 & 54.6 & 36.6 & 57.2 \\
NCL \cite{li2022nested} & 67.3 & 55.4 & 39.0 & 57.7 \\
SAFA \cite{hong2022safa} & 63.8 & 49.9 & 33.4 & 53.1 \\
DOC \cite{wang2022towards} & 65.1 & 52.8 & 34.2 & 55.0 \\
DLSA \cite{xu2022constructing} & 67.8 & 54.5 & 38.8 & 57.5 \\
\midrule
ViT \cite{dosovitskiy2020image} & 50.5 & 23.5 & 6.9 & 31.6 \\
MAE \cite{he2022masked} & \textbf{74.7} & 48.2 & 19.4 & 54.5 \\
DeiT \cite{touvron2022deit} & 70.4 & 40.9 & 12.8 & 48.4 \\
LiVT (Reproduced) \cite{xu2023learning} & 72.7 & 56.6 & 40.4 & 60.6 \\
\midrule
LiVT SD & 71.8 & \textbf{58.1} & 44.3 & 61.5 \\
\midrule
LiVT LTG (Energy) & 70.5 & 57.9 & \textbf{50.0} & 61.7 \\
LiVT LTG (Entropy) & 70.9 & 57.9 & 49.8 & 61.8 \\
LiVT LTG (Epistemic) & 71.4 & 57.7 & \textbf{50.0} & \textbf{61.9} \\
\bottomrule
\end{tabular}
}
\caption{Top-1 Accuracy on ImageNet-LT by dataset split (Many, Med, Few) and overall (Acc). The validation set is balanced between all 1000 classes but trainig is highly imbalanced: Few classes have $5-19$ examples, Medium classes have $20-99$ examples, Many classes have $100-1200$ examples.}
\label{table:results_imagenet_lt_vit}
\end{wraptable}
%
%
\subsection{LTG Improves Longtail Performance}
\label{sup:section:longtail_experiment}
In Table~\ref{table:results_imagenet_lt_vit}, we train SOTA ViT-based models (LiVT) from scratch on ImageNet-LT according to the longtail-compensation approach of \cite{xu2023learning}. In summary, training includes MAE pretraining followed by 100 epochs of BCE loss with a logit adjustment to account for class imbalance. Initial LiVT training is on real data only and uses many data augmentations including RandAugment, Mixup, and CutMix. Our baseline predictive model, LiVT (Reproduced), nearly matches the generalization performance (top-1 accuracy) of what is reported in \cite{xu2023learning} (LiVT (Reproduced): $60.6$ vs LiVT (Reported): $60.9$). We then fine-tune LiVT on $24000$ additional synthetic data evenly distributed across all $1000$ classes for $100$ epochs with $1e-4$ learning rate. We use the same Stable Diffusion 1.4 baseline and sampling details as in the main paper (LiVT SD), and also compare Longtail Guidance with three types of model-based longtail signals (LiVT LTG \{Energy, Entropy, Epistemic\}). Overall generalization improvements on the balanced validation set are mild but significant ($+1.3$ top-1 accuracy). However, performance for longtail classes (Few) improves by four points for baseline diffusion ($+10\%$) but a marked $10$ points for Longtail Guidance ($+24\%$)! 

Synthetic data always improves Medium and Few performance, which are most impacted by a fixed number of data generations per class. This experiment demonstrates that Longtail Guidance works with multiple architectures (ResNet, ViT), multiple losses (CE, logit-adjusted BCE), and that it disproportionately improves longtail performance. It also demonstrates that LTG can be composed with finely-crafted training recipes to further improve predictive model generalization performance.
%
\subsection{Comparison to GIF and Dream-ID}
\label{sup:section:experiment_details}
In the main paper, Table~\ref{tab:results_gif}, we compare to GIF, the Guided Imagination Framework \cite{zhang2023expanding}. We use the same predictive model architecture (ResNet50 trained from scratch), generate the same quantity of synthetic data ($30\times$ dataset size for Pets, $20\times$ dataset size for Caltech101, Cars, and Flowers), the same diffusion model (Stable Diffusion v1.4), the same diffusion sampler (DDIM), and the same number of sampling iterates ($50$). We reproduce baseline models (not exposed to synthetic data or data augmentation conditions) to match the performance of GIF baselines (Original row in Table~\ref{tab:results_gif}) to within $1\%$ of the generalization performance reported in GIF. We then fine-tune for $100$ epochs, generating synthetic data with Longtail Guidance according to the schedule in Table~\ref{table:ltg_sampling_schedule_for_gif} (e.g. generate synthetic data in epoch 0, fine-tune until epoch 5, generate more synthetic data, fine-tune until epoch 10, \ldots). We experimentally found that synthetic data iteratively generated throughout fine-tuning outperformed synthetic data generated all at once and speculate that this is due to the model-based longtail evolving throughout training.

Synthetic data are distributed across classes according to their frequency in the original training data. Fine-tuning is with the Adam optimizer, cosine annealing learning rate schedule, and $1e-3$ learning rate. As in GIF, we train with random rotations ($\pm 15^\circ$), $224\times 224$ crops, and horizontal flips.

In the main paper, Table~\ref{tab:results_imagenet}, we compare to the in-distribution synthetic data generation task (Dream-ID) from Dream the Impossible \cite{du2024dream}. As in Dream-ID, we start with ImageNet-pretrained ResNet-34 models and fine-tune for $100$ epochs with the same training details as above, except that we match the synthetic data quantity used in Dream-ID by generating a total of $1000$ images per class, evenly distributed every $5$ epochs ($50$ images per class per $5$ epochs).

Different from GIF or Dream-ID is that we build synthetic data over the course of model fine-tuning, to better capture evolving model-based longtail challenges (whereas GIF, Dream-ID, and other synthetic data baselines generate all synthetic data at once since they are not model-conditioned). Also different is that LTG does not need prompt tuning to force synthetic data diversity. LTG text prompts are generic: ``a photo of \texttt{Class}'' whereas GIF prompts (and diffusion baselines) are, ``a \texttt{Noun} \texttt{Adjective} of \texttt{Class}'' where \texttt{Noun} is randomly sampled to be one of (``image'', ``oil painting'', ``cartoon image'', ``sketch'', ``pencil sketch'') and \texttt{Adjective} is randomly sampled to be one of (``'', ``colorful'', ``stylized'', ``bright'', ``sheared'', ``solarized'', ``posterized'', ``high-contrast''), for a total of $40$ unique prompts per class. In LTG, diversity is automatic based on the production model's evolving longtail signals.

All experiments are performed on 8xH100.

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Dataset} & \textbf{Total Expansion Ratio} & \textbf{Synthetic Data Generation Epochs} \\
\midrule
Pets & $30\times$ & $0, 5, \ldots, 45, 50; 52, 54, \ldots, 96, 98$ \\
Caltech101, Cars, Flowers & $20\times$ & $0, 5, \ldots, 90, 95$
\\
\bottomrule
\end{tabular}
\caption{Synthetic Data Generation Schedule by Dataset}
\label{table:ltg_sampling_schedule_for_gif}
\end{table}

\subsection{Longtail Introspection}
\label{sup:sec:longtail_introspection}
We construct refined prompts for each class by prompting LLaVA-1.6 7B \citep{liu2024llavanext} to generate a description for two sets of synthetic images: $100$ generated by Longtail Guidance and $100$ generated by baseline diffusion. VLM prompts are of the form: \textit{This is an image of \texttt{<}Class\texttt{>}. Describe it in detail}. For each output VLM description, we compute per-token BERT embeddings \citep{devlin2018bert}. This provides us with two token embedding distributions: longtail and baseline.

For each token embedding in the longtail distribution, we compute cosine similarity to the nearest token embedding in the baseline distribution. For each longtail image, we define the $K$ tokens that are furthest from the the baseline distribution as longtail keywords. We retain the (description, keyword) pairs of the $P=40$ longtail examples per class whose keyword tokens are furthest from the base distribution.

Following, we create $P=40$ refined prompts per class by prompting GPT-4o \cite{openai2023gpt4} with: \textit{\texttt{<}VLM description of the image\texttt{>} The following keywords describe the key features of the description above: \texttt{<}Keyword 1\texttt{>}, \texttt{<}Keyword 2\texttt{>} ... Use a complete sentence to summarize the key features. The sentence should start with: A photo of \texttt{<}Class\texttt{>} that...}.

Example (image, keyword) pairs are displayed in Figure~\ref{fig:ltg_introspection_example} of the main paper. Example refined prompts are displayed in Table~\ref{tab:refined_prompts}. We emphasize that not all keywords found by this method are immediately meaningful (e.g. satin as in \emph{satin-like pedals}, ether as in \emph{ethereal appearance}, but they can be quickly scanned for longtail themes and, quantitatively, the process improves generalization performance beyond manual prompt tuning.

\begin{table}[h!] % h! ensures the table stays close to the text that references it
\centering
{ \small
\begin{tabular}{p{0.75\textwidth}}
\toprule
A photo of a bearded iris that showcases its deep purple petals with a yellow and brown pattern running through the \textbf{underside}, set against a blurred background to emphasize the flower's texture and color.
\\ \\
A photo of sweet pea that showcases a \textbf{young} plant with a single, unfurled green leaf and a small, green flower bud beginning to open, set against a softly blurred green background, highlighting the delicate texture and promise of the flower to come.
\\ \\
A photo of red ginger that showcases a vibrant red flower with a yellow center, surrounded by slightly \textbf{damaged} green leaves, set against a blurred natural background.
\\ \\
A photo of morning glory that showcases vibrant petals with a gradient from deep red to blue-purple, arranged in a spiral pattern, with smooth texture and \textbf{dew} droplets, set against a blurred background to emphasize the flower's intricate details and colors.
% \bottomrule
\end{tabular}
}
\caption{Example refined prompts generated by Longtail Introspection for Flowers data. Keywords are \textbf{bolded}.}
\label{tab:refined_prompts} 
\end{table}

% Figure \ref{fig:introspection_example} and \ref{fig:manual_prompt_example} show examples of images generated with Longtail Guidance and standard diffusion. The associated keywords are in the caption. Those examples illustrates several patterns of the long-tailness related to the production model, which can be captured by the keywords. Those patterns include: flowers that are partially occluded by some caption, stylish image such as digital art; split image, etc. To make a comparison, we apply the same process as the longtail introspection to select keywords and their associated images. We find that the image generated with standard diffusion tends to be more regular, despite good diversity.

% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}{\textwidth}
%         \centering
%         % \includegraphics[width=\textwidth]{figures/vlm_keyword/ltg_image.jpg}
%         \caption{Generated image using longtail Introspection. Keywords: latin, font, split, half, pest, french, imaginary, speculative, ban, icy, imaginary, speculative, digital, art, presentation, transition. \TODO{Iterate on this figure.}
%         }
%     \end{subfigure}
%     \label{fig:introspection_example}
%     \hfill
%     \begin{subfigure}{\textwidth}
%         \centering
%         % \includegraphics[width=\textwidth]{figures/vlm_keyword/base_image.jpg}
%         \caption{Generated image using diffusion with manual prompt tuning. Keywords: symbolism, medicine, asteroid, bolton, mature, consumption, holding, person, tipped, creating, throat, glow, identification, macro, wall, neutral. \TODO{Iterate on this figure.}}
%     \end{subfigure}
%     \label{fig:manual_prompt_example}
% \end{figure}

\subsection{Dataset Details}
\label{supp:sec:dataset_details}
We summarize each dataset used in Table~\ref{tab:dataset_summary}. Additional details can be found in the \cite{zhang2023expanding} for Caltech, Cars, Pets, and Flowers or \cite{du2024dream} for ImageNet-100, ImageNet-A, and ImageNet-V2 variants. Notably, \cite{du2024dream} creates evaluation subsets of ImageNet-A and ImageNet-V2 that overlap with the training classes of ImageNet-100, listed in Figure~\ref{fig:imagenet100_classes}. For fair comparison, we also train on ImageNet-100 and evaluate on the ImageNet-100 (Eval), ImageNet-A, and ImageNet-V2 subsets as defined in \cite{du2024dream}. We emphasize that these datasets exhibit example counts that are common for longtail data in production settings.

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
\textbf{Dataset} & \textbf{Classes} & \textbf{\#Train} & \textbf{\#Val} & \textbf{\#Synthetic} \\
\midrule
Caltech101 & 102 & 3060 & 6084 & 61200 \\
Cars & 196 & 8144 & 8041 & 162880 \\
Flowers & 102 & 6557 & 1632 & 131140 \\
Pets & 37 & 3680 & 3669 & 110400 \\
ImageNet-100 & 100 & 129860 & 5000 & 100000 \\
ImageNet-A* & 41 & - & 1852 & - \\
ImageNet-V2* & 100 & - & 10000 & - \\
ImageNet-LT & 1000 & 115846 & 20000 & 24000 \\
\bottomrule
\end{tabular}
\caption{Overview of datasets with number of classes, training samples, validation samples, and synthetic samples. *ImageNet-A and ImageNet-V2 are not trained on; they are only used for evaluation.}
\label{tab:dataset_summary}
\end{table}

\begin{figure}[ht]
    \centering
    \fbox{%
        \parbox{0.86\textwidth}{%
            \scriptsize % Set the font size to smaller
            \texttt{n01498041 n01514859 n01582220 n01608432 n01616318 n01687978 n01776313 n01806567 n01833805 n01882714 n01910747 n01944390 n01985128 n02007558 n02071294 n02085620 n02114855 n02123045 n02128385 n02129165 n02129604 n02165456 n02190166 n02219486 n02226429 n02279972 n02317335 n02326432 n02342885 n02363005 n02391049 n02395406 n02403003 n02422699 n02442845 n02444819 n02480855 n02510455 n02640242 n02672831 n02687172 n02701002 n02730930 n02769748 n02782093 n02787622 n02793495 n02799071 n02802426 n02814860 n02840245 n02906734 n02948072 n02980441 n02999410 n03014705 n03028079 n03032252 n03125729 n03160309 n03179701 n03220513 n03249569 n03291819 n03384352 n03388043 n03450230 n03481172 n03594734 n03594945 n03627232 n03642806 n03649909 n03661043 n03676483 n03724870 n03733281 n03759954 n03761084 n03773504 n03804744 n03916031 n03938244 n04004767 n04026417 n04090263 n04133789 n04153751 n04296562 n04330267 n04371774 n04404412 n04465501 n04485082 n04507155 n04536866 n04579432 n04606251 n07714990 n07745940}}
    }
    \caption{List of ImageNet-100 Classes trained and evaluated on as defined by \cite{du2024dream}}
    \label{fig:imagenet100_classes}
\end{figure}

\subsection{Can We Continuously Mine for Additional High-Value Synthetic Data?}
\label{sup:section:continuous_mining}
\begin{table}[h]
{\small \setlength{\tabcolsep}{4pt}
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Dataset} & \textbf{Caltech} & \textbf{Cars} & \textbf{Flowers} & \textbf{Pets} & \textbf{Avg} \\
\midrule
LTG (100 epoch) & 71.5 & 85.1 & 90.9 & 82.0 & 82.4 \\
LTG (200 epoch) & 72.9 & 85.4 & 92.6 & 82.1 & 83.3 \\
LTG (300 epoch) & 72.9 & 85.4 & 92.6 & 82.2 & 83.3 \\
\bottomrule
\end{tabular}
\vspace{-0.5em}
\caption{Top-1 Accuracy when training with Longtail Guidance for many epochs.}
\label{tab:results_ltg_many}
} \vspace{-0.25em}
\end{table}
In Table~\ref{tab:results_ltg_many} we ask: if Longtail Guidance generates high-value training data for predictive model $f_\phi$, can we continuously iterate between fine-tuning $f$ and generating synthetic data for additional generalization improvements? In this experiment, we generate synthetic data according the original schedules defined in Table~\ref{table:ltg_sampling_schedule_for_gif} for the first $100$ epochs. We then continue fine-tuning and generating an additional $1\times$ expansion every $5$ epochs for a total of $300$ epochs and report generalization performance. We observe that in all cases, we can improve performance, but that gains eventually saturate. It remains a question for future work whether higher-capacity generative models could be mined longer periods of time before predictive performance saturates. If so, it suggests an exciting future possibility of continuously exchanging unused offline compute for improved predictive model performance.

\subsection{Computational Costs}
\label{sup:section:Computational_Cost}

\subsubsection*{Epistemic Head}
The Epistemic Head with $K$ heads has a parameter count equal to $K \times d_\text{model} \times C$, for model embedding dimension $d_\text{model}$ and number of output logits $C$. For many predictive models, this leads to negligible parameter count increase, as displayed in Table~\ref{tab:epistemic_head_parameter_count} -- less than 5\% increase for all experiments in this paper. Training and inference times are impacted by less than 2\% for our most expensive (ImageNet-LT) experiments.

\begin{table}[h!]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Classes} & \multicolumn{2}{c}{\textbf{Epistemic Heads}} \\ %\cmidrule(lr){2-4}
                 & \textbf{3} & \textbf{5} \\ \midrule
10               & 0.03\%       & 0.04\%        \\
100              & 0.27\%       & 0.45\%        \\
1000             & 2.67\%       & 4.44\%        \\
\bottomrule
\end{tabular}
\caption{Epistemic Head Parameter Count as a Percentage of ViT-B Parameter Count}
\label{tab:epistemic_head_parameter_count}
\end{table}

\subsubsection*{Longtail Guidance}
We generate $512 \times 512$ resolution synthetic images at an unoptimized rate of $6.32$ image / second for baseline text-prompted diffusion and $1.01$ image / second for diffusion with Longtail Guidance (using Stable Diffusion v1.4 in FP16 with $50$ DDIM sampling steps on 8xH100 GPUs). The majority of the LTG guidance cost is in differentiably decoding through the VAE. In particular, in the gradient
\begin{equation}
\frac{d\hat{z}_t^0}{dz_t} \times
\textcolor{red}{
\frac{d\hat{x}_t^0}{d\hat{z}_t^0}
} \times
\frac{df_\phi^{\text{lt}}}{d\hat{x}_t^0}
\end{equation}
calculated by the main paper's Algorithm~\ref{alg:longtail_guidance}, the second term is VRAM-intensive (45GB for batch size 8 without gradient checkpointing). This cost occurs primarily because the $83.6m$ parameter VAE decodes latent dimensions of $64 \times 64 \times 4$ to data dimension $512 \times 512 \times 3$.

\subsection{Longtail Signals}
\label{sup:section:hard}
In Figure~\ref{fig:detect_hard2} we show that high quantiles $q=90\%$ of model-based longtail signals are all strong indicators of examples that are disproportionately hard for the model (as determined by average accuracy above and below the quantile). However, as shown in Figure~\ref{fig:longtail_validation_performance} of the main paper, the Epistemic Head more effectively \emph{detects} rare or hard examples.

In Figure~\ref{fig:epistemic_aleatoric_samples}, we visualize a toy example of high epistemic (left) and high aleatoric (right) uncertainty for three Epistemic Heads each classifying over three classes. High epistemic uncertainty occurs when each sample from the predictive posterior (i.e. each Epistemic Head) gives mutually incompatible answers (such as all being confident about a different class). High aleatoric uncertainty (which includes high entropy) occurs when each sample has similarly and highly ambiguous (e.g. uniform) belief over classes.

\begin{figure}[h!]
\begin{minipage}[t]{0.47\textwidth} % Adjust width as needed
    \centering
    \includegraphics[width=0.9\textwidth]{figures/detect_hard.pdf}
    \caption{High quantiles of longtail signal indicate examples that are disproportionately hard. }
    \label{fig:detect_hard2}
\end{minipage}
\hfill
\begin{minipage}[t]{0.47\textwidth} % Adjust width as needed
    \centering
    \includegraphics[width=0.9\textwidth]{figures/epistemic_aleatoric_heatmaps.pdf}
    \caption{Probability of classification under high epistemic and high aleatoric uncertainty. There are three samples from the posterior predictive (y-axis). Each maintains a distribution over three classes (x-axis). Epistemic uncertainty (left) goes high when each sample has mutually incompatible beliefs. Aleatoric uncertainty (right) goes high when samples are all highly uncertain.
    }
    \label{fig:epistemic_aleatoric_samples}
\end{minipage}
\end{figure}

\subsection{Why Does Longtail Guidance Work without Training the Predictive Model on Intermediate Diffusion States?}
\label{sup:section:why_ltg}
A key finding and contribution of this work is that an existing predictive model $f_\phi$ does not need to be retrained on intermediate, noisy diffusion states to effectively guide diffusion model $\epsilon_\theta$ towards high-value synthetic training examples that are rare or hard from $f$'s perspective. This realization frees us from the dilemma of having to decide whether to waste predictive model capacity training on intermediate diffusion data it will never see in production or risking divergence from the production model by fine-tuning on intermediate noisy states. But why does this work?

In Figure~~\ref{fig:terminal_state_example}, we visualize the intermediate, noisy diffusion states of two quantities: the decoded, predicted terminal state $D(P(\hat{x}_t^0))$ (top row, what LTG uses as guidance input to predictive model $f_\phi$) and the decoded data state, $D(x_t)$ (bottom row, what classifier guidance would traditionally performs guidance if not in a latent space). We observe that the terminal state predictions much more readily resemble natural image data at a much earlier time in the diffusion process (within the first 10\% of denoising steps) than do the data states. In fact, decoded data states have off-distribution noise artifacts up through the first 90\% of the diffusion denoising steps.

We make this quantitative with the Frechet Inception Distance (FID) \cite{heusel2017gans}, defined as 
\begin{equation}
    \text{FID} = \|\mu_r - \mu_g\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2 (\Sigma_r \Sigma_g)^{\frac{1}{2}})
\end{equation}
where $(\mu_r, \Sigma_r), (\mu_g, \Sigma_g)$ are the mean and covariance of Inception-V3's \texttt{pool3} features for real and synthetic data, respectively. Lower FID indicates that the generated data more closely match and cover the real data. We measure FID for two conditions:
\begin{enumerate}
\item FID between the decoded, predicted terminal state $\hat{x}_t^0 = D(P(x_t))$ and real ImageNet-LT training data, and
\item FID between the decoded data state $x_t$ and real ImageNet-LT training data.
\end{enumerate}
Results are plotted in Figure~\ref{fig:fid_xt_x0}. Observe that the decoded, predicted terminal state $D(P(\hat{x}_t^0))$ are dramatically closer (in distribution) to real training data than are the naively decoded data states, $x_t$ for nearly all denoising iterations.

Figure~\ref{fig:terminal_state_longtail_signal} demonstrates that, because the terminal state estimates $D(P(\hat{x}_t^0))$ are closer real training data, the predictive model is able to effectively guide data generations towards higher longtail signals (model entropy in this case), with clear longtail signal separation between different longtail guidance weights occurring within the first $25\%$ of denoising iterations. In contrast, naively guiding based on data states $x_t$ causes the predictive model to be unable to effectively guide data generation until the last $5\%$ of denoising iterations, when there are no longer many degrees of freedom for the diffusion model to meaningfully change image content.
%
\begin{figure}[ht!]
    \begin{minipage}[b]{\textwidth} % Adjust width as needed
    \centering
    \includegraphics[width=0.95\textwidth]{figures/ltg_terminal_state_example.jpg}
    \caption{Performing Longtail Guidance on predicted, decoded terminal states $\hat{x}_t^0$ (top row) provides production model $f_\phi$ with data that are more in-distribution and less corrupted by intermediate diffusion noise than does Naive guidance performed on each intermediate decoded state $x_t$ (bottom row). See Figures~\ref{fig:terminal_state_longtail_signal},~\ref{fig:fid_xt_x0} for quantitative analysis.
    }
    \vspace{1em}
    \label{fig:terminal_state_example}
    \end{minipage}
    % First figure
    \begin{minipage}[b]{0.47\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/ltg_vs_ltg_naive.pdf}
        \vspace{-2em}
        \caption{Performing Longtail Guidance on the predicted, decoded terminal data state $\hat{x}_t^0$ (top row) provides production model $f_\phi$ with cleaner data much earlier in the diffusion denoising process, enabling it to meaningfully exercise guidance as compared to naively performing guidance on the predicted decoded data state $x_t$. Y-axes are the guiding model longtail signal (in this case, entropy).  See Figure~\ref{fig:terminal_state_example} for a visual depiction.
        }
        \label{fig:terminal_state_longtail_signal}
    \end{minipage}
    \hfill
    % Second figure
    \begin{minipage}[b]{0.47\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/fid_xt_x0_over_diffusion_time_by_guidance_weight.pdf}
        \vspace{-2em}
        \caption{Decoded terminal data states $\hat{x}_t^0$ have better FID with respect to real training data than do intermediate data states $x_t$, suggesting that LTG works with an existing predictive model $f_\phi$, even though it is not trained on intermediate diffusion states, precisely because the states upon which LTG performs guidance better match what the predictive model has already seen.
        }
        \label{fig:fid_xt_x0}
    \end{minipage}
\end{figure}
%
\subsection{Ablations}
\label{sup:section:ablations}
In Table~\ref{tab:ablation_guidance_weight}, we ablate over Longtail Guidance signal (energy, entropy, epistemic) by Longtail Guidance Weight (1.0, 10.0, 50.0, 200.0) for the Pets dataset. In Table~\ref{tab:ablation_epistemic_heads}, we ablate over the number of Epistemic Heads on the same task. In both cases, we report predictive model generalization performance when trained on the $30\times$ synthetic data expansion task as defined in Section~\ref{sec:experiments:benchmarks}. We find highest performance for energy and entropy at guidance weight $10.0$ and highest performance for epistemic at guidance weight $50.0$. Similarly, we find highest performance for $K=5$ Epistemic Heads. Performance declines with additional heads, likely because the oracle loss used to train the Epistemic Head causes each head to be exposed to $\frac{1}{K}$ examples in expectation; too many heads and they lose the ability to represent the predictive model.

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.4\textwidth}
\centering
\begin{tabular}{lcccc}
\toprule
& \multicolumn{4}{c}{\textbf{Longtail Guidance Weight}} \\
\cmidrule(lr){2-5}
\textbf{Longtail Signal} & \textbf{1.0} & \textbf{10.0} & \textbf{50.0} & \textbf{200.0} \\
\midrule
Energy & 78.8 & \textbf{80.0} & 78.9 & 76.7 \\
Entropy & 76.1 & \textbf{81.6} & 80.9 & 74.9 \\
Epistemic & 76.8 & 78.5 & \textbf{82.0} & 79.1 \\
\bottomrule
\end{tabular}
\caption{Ablation over Longtail Guidance Weight on Predictive Model Generalization Performance on the Pets Dataset Expansion Task, holding the number of Epistemic Heads fixed at $K=5$. Entries are bolded for best hyperparameter in each row.}
\label{tab:ablation_guidance_weight}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{c}{\textbf{Epistemic Heads}}
& \textbf{3} & \textbf{5} & \textbf{7} & \textbf{9} \\
\midrule
LTG (Epistemic) & 81.2 & \textbf{82.0} & 80.6 & 79.9 \\
\bottomrule
\vphantom{X}
\end{tabular}
\caption{Ablation over Epistemic Heads on Predictive Model Generalization Performance on the Pets Dataset Expansion Task, holding the guidance weight fixed at $50.0$.}
\label{tab:ablation_epistemic_heads}
\end{minipage}
\end{figure}

\subsection{Additional Examples}
\label{sup:section:additional_examples}
In Figure~\ref{fig:ltg_examples_figure}, we show high-resolution examples of synthetic data generated by baseline diffusion and by Longtail Guidance. In order, classes are: jellyfish, flamingo, lion, monarch butterfly, ant, candle, jeep, forklift, and ambulance. Guidance is performed by a SOTA ViT-based ImageNet-LT classifier described in Supplement~\ref{sup:section:longtail_experiment}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ltg_examples_figure.jpg}
    \caption{Additional example of baseline diffusion (left) and Longtail Guidance (right). Baseline diffusion tends to generate canonical, well-posed views that help a predictive generalize, but only up to a point. In contrast, Longtail Guidance produces more extreme, occluded, or challenging views from the perspective of a predictive model, enabling significantly improved generalization performance.
    }
    \label{fig:ltg_examples_figure}
\end{figure}

% \clearpage
% \bibliography{main}
% \bibliographystyle{icml2024}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
