\section{Related Work}
Diffusion has seen wide success, particularly in image generation, where it outperform GANs in image quality and diversity without suffering from unstable training or mode collapse \cite{dhariwal2021diffusion}. Recent work has seen progress in handling large data dimensions with latent spaces \cite{chen2025pixart, podell2023sdxl, rombach2022high} or hourglass networks \cite{crowson2024scalable}, improved sampling \cite{lu2022dpm, karras2022elucidating, ho2020denoising, song2020denoising}, additional data domains \cite{ran2024towards, pronovost2023scenario, zhong2023language}, and personalization \cite{ruiz2023dreambooth, kumari2023multi}. Much work has also been done on new forms of guidance \cite{wallace2023edict, yu2023freedom, wallace2023end, zhang2023adding} beyond just classifier guidance and classifier-free guidance \cite{dhariwal2021diffusion, ho2022classifier}. Universal Guidance \cite{bansal2023universal} is most relevant, and is discussed in Section~\ref{sec:longtail_guidance}. 

Synthetic training data from generative models has been considered since GANs \cite{li2022bigdatasetgan}, but has started to come of age with diffusion \cite{azizi2023synthetic, zhou2023training}, particularly for high-resolution datasets where fidelity matters. GIF \cite{zhang2023expanding} and Dream-ID \cite{du2024dream}, discussed in Section~\ref{sec:discussion}, are most relevant.

Model signals have been used for out-of-distribution or adversarial detection \cite{huang2021importance, hsu2020generalized, cohen2020detecting}, particularly model uncertainty \cite{van2020uncertainty} or feature density \cite{lee2018simple}. Epistemic uncertainty has been developed in (expensive) Bayesian \cite{gal2016dropout} or model ensemble contexts \cite{liu2019accurate, depeweg2018decomposition, wilson2020bayesian, lakshminarayanan2017simple}, though, to our knowledge, not explicitly in a single, differentiable forward pass as we have done in Section~\ref{sec:epistemic_head}. Work on longtail robustness has primarily focused on addressing a-priori known class imbalance. Datasets include ImageNet-LT, Places-LT, \cite{liu2019large} and iNaturalist \cite{van2018inaturalist}. Mitigations include pretraining \cite{he2022masked, bao2021beit}, distillation \cite{xiang2020learning}, reweighted losses \cite{xu2023learning, ross2017focal}, or selective (real) data mining \cite{jiang2022improving}.