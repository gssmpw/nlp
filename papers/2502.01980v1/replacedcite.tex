\section{Related Work}
Diffusion has seen wide success, particularly in image generation, where it outperform GANs in image quality and diversity without suffering from unstable training or mode collapse ____. Recent work has seen progress in handling large data dimensions with latent spaces ____ or hourglass networks ____, improved sampling ____, additional data domains ____, and personalization ____. Much work has also been done on new forms of guidance ____ beyond just classifier guidance and classifier-free guidance ____. Universal Guidance ____ is most relevant, and is discussed in Section~\ref{sec:longtail_guidance}. 

Synthetic training data from generative models has been considered since GANs ____, but has started to come of age with diffusion ____, particularly for high-resolution datasets where fidelity matters. GIF ____ and Dream-ID ____, discussed in Section~\ref{sec:discussion}, are most relevant.

Model signals have been used for out-of-distribution or adversarial detection ____, particularly model uncertainty ____ or feature density ____. Epistemic uncertainty has been developed in (expensive) Bayesian ____ or model ensemble contexts ____, though, to our knowledge, not explicitly in a single, differentiable forward pass as we have done in Section~\ref{sec:epistemic_head}. Work on longtail robustness has primarily focused on addressing a-priori known class imbalance. Datasets include ImageNet-LT, Places-LT, ____ and iNaturalist ____. Mitigations include pretraining ____, distillation ____, reweighted losses ____, or selective (real) data mining ____.