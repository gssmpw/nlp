%TODO
%1- (temporarily aborted) Summit performance graph (15, 32, and 64 nodes) an average of three runs
%5- Draw the figure related to time-to-solution using four nodes and 2GPUs each
%9- (Sameh) Polish the performance section and see if we can run on Frontier
%10- Tomorrow, we will check together the real dataset problem
%11 - 4 nodes (GSL) -- 64 core each, 4 nodes( refined) -- 2 GPUs each configuration
%12 - Write the sentence telling the rescaling of locations.

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{flushend}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm,algpseudocodex}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{url}
\usepackage{hyperref}
\usepackage{xcolor}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\usepackage[margin=1in]{geometry}
\usepackage{soul} % strike through the texts!
%\renewcommand{\color{red}}{\color{black}}
% Self-defined commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\eps}{\varepsilon}
\newcommand{\PP}{\mathbb{P}}
\newcommand*{\bs}{\boldsymbol}
\newcommand*{\mb}{\mathbf}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\title{
GPU-Accelerated Modified Bessel Function of the Second Kind for Gaussian Processes}

 \author{
 \IEEEauthorblockN{Zipei Geng$^1$, Sameh Abdulah$^2$, Ying Sun$^1$, Hatem Ltaief$^2$, David E. Keyes$^2$, Marc G. Genton$^1$}
 \IEEEauthorblockA{
 $^1$Statistics Program, \\
 $^2$Applied Mathematics and Computational Sciences (AMCS) Program, \\
 $^{1,2}$King Abdullah University of Science and Technology, Thuwal, Saudi Arabia. \\
 $^{1,2}$\{firstname.lastname\}@kaust.edu.sa
 }
}



\maketitle

\begin{abstract}
Modified Bessel functions of the second kind are widely used in physics, engineering, spatial statistics, and machine learning. Since contemporary scientific applications, including machine learning, rely on GPUs for acceleration, providing robust GPU-hosted implementations of special functions, such as the modified Bessel function, is crucial for performance. Existing implementations of the modified Bessel function of the second kind rely on CPUs and have limited coverage of the full range of values needed in some applications. In this work, we present a robust implementation of the modified Bessel function of the second kind on GPUs, eliminating the dependence on the CPU host. We cover a range of values commonly used in real applications, providing high accuracy compared to common libraries like the GNU Scientific Library (GSL) when referenced to Mathematica as the authority. Our GPU-accelerated approach also demonstrates a $2.68$X performance improvement using a single A100 GPU compared to the GSL on 40-core Intel Cascade Lake CPUs.
Our implementation is integrated into \emph{ExaGeoStat}, the HPC framework for Gaussian process modeling, where the modified Bessel function of the second kind is required by the Mat\'ern covariance function in generating covariance matrices. We accelerate the matrix generation process in \emph{ExaGeoStat} by up to $12.62$X with four A100 GPUs while maintaining almost the same accuracy for modeling and prediction operations using synthetic and real datasets.
\end{abstract}

\begin{IEEEkeywords}
Modified Bessel function of the second kind, GPU implementation, Tile-based matrix computations, Gaussian processes.
\end{IEEEkeywords}

\section{Introduction}
The modified Bessel function of the second kind, denoted as \( K_\nu(x) \), where \( \nu \) represents the order and \( x \) the argument, arises in a wide range of applications across mathematics, physics, engineering, spatial statistics, and machine learning. For instance, in physics and engineering, \( K_\nu(x) \) frequently appears in solving PDEs in cylindrical symmetry with radially decaying behavior. Examples include heat conduction, damped wave propagation~\cite{sharma2007damped}, electromagnetic fields in cylindrical waveguides~\cite{karamian2022role}, and Schr\"{o}dinger's equation in axisymmetry~\cite{martin2021quasi}. 
%In machine learning, $K_{\nu}(x)$ can serve as a kernel function, mapping data to higher-dimensional spaces in algorithms such as Support Vector Machines (SVMs)~\cite{ramakrishnan2023learning}. 
Similarly, in Gaussian processes, \( K_\nu(x) \) is used in the Mat\'{e}rn kernel function, which is widely used to construct correlation matrices that capture spatial relationships between locations~\cite{wang2023parameterization}. These diverse applications emphasize the importance of efficient computation of the \( K_\nu(x) \) function.

In the literature, three methods are widely employed to compute the modified Bessel function of the second kind across a broad range of values for \(x\) and \(\nu\)~\cite{bbtakekawa}: series expansions~\cite{bbtemme}, continued fractions~\cite{amos1974computation}, and asymptotic expansions~\cite{olver2009bessel}.  The first expresses \( K_\nu(x) \) as an infinite series that involves powers of \(x\). This approach is particularly effective for small \(x\), where truncating the series to a finite number of terms provides a highly accurate approximation. %However, for large values of \(x\), the series converges more slowly, requiring additional terms and thus increasing computational complexity. 
The continued fractions method represents the Bessel function as an infinite recursive fraction. This method is suitable for larger \(x\) than is the series expansion, but it remains computationally intensive. An asymptotic expansion approximates the Bessel function by capturing its dominant behavior as \(x\) becomes very large. By neglecting lower-order terms, this method efficiently describes the primary growth trend. However, it lacks accuracy for small \(x\), where such approximations fail to converge effectively.  Combining these three methods and other less common approaches can effectively cover a wider range of \(x\) and \(\nu\) values. Nevertheless, these techniques are computationally demanding and inherently sequential, posing challenges for parallelization on modern architectures, such as GPUs.

In this paper, we adopt an integration-based method proposed by~\cite{bbtakekawa} to compute \(K_{\nu}(x)\) for real order \(\nu \in \mathbb{R}\), denoted by \textsc{BesselK}, and optimize it for GPU implementation. This approach, implemented in CUDA, is well suited for parallel execution, as it represents the \textsc{BesselK} function using quadrature over finite intervals. The key advantages of this method, beyond its parallelizability, are its flexibility and its ability to efficiently cover a reasonable range of \(x\) and \(\nu\) values.  %^We also rely on the algorithm proposed by~\cite{bbtakekawa} to efficiently leverage the \textsc{BesselK} function in spatial data modeling and prediction. 
Our implementation has been integrated into \emph{ExaGeoStat}~\cite{abdulah2018exageostat}, a scalable geospatial data modeling and prediction framework optimized for manycore systems, including modern GPUs.   \emph{ExaGeoStat} possesses three functionalities: synthetic data generation, modeling, and prediction. Each of these operations requires, at a minimum, the generation of a covariance matrix of size \(N \times N\), where \(N\) represents the number of spatial locations in its simplest form. The Mat\'{e}rn kernel (or covariance function), which involves the \textsc{BesselK} function, is often used in Gaussian processes to construct this covariance matrix. 
%This work presents an efficient and scalable computation, which leverages the integration-based approach for computing the \textsc{BesselK} function, enabling accurate modeling and prediction over large spatial datasets.


%\section{Contributions}
Our contributions are summarized as follows:

\begin{itemize}
    \item We present a highly efficient GPU implementation of the \textsc{BesselK} function, which addresses a critical performance bottleneck encountered in numerous scientific applications.  
    
    \item {\color{black} We accelerate an integration-based algorithm using a unified integral bound for \textsc{BesselK}, enabling efficient handling of a practical range of \(x\) and \(\nu\) commonly encountered in Gaussian processes on the GPU.}


    \item We combine the accelerated integration-based algorithm with a series expansion to accurately compute the \textsc{BesselK} function for \(x < 0.1\), where the integral-based approach specifically underperforms, making our solution more comprehensive.  

    \item { \color{black} Our refined algorithm combines series expansion and integration, both of which are established methods to approximate BesselK. However, to the best of our knowledge, we are the first to deploy these approaches on massively parallel GPU hardware accelerators.}

        
    \item We integrate the refined algorithm into the covariance matrix generation step of \emph{ExaGeoStat} and evaluate its performance and accuracy against existing CPU-based implementations, such as the GNU Scientific Library.
    
    \item We assess the accuracy of the proposed algorithm to compute the \textsc{BesselK} function against existing approaches. Additionally, we evaluate its accuracy and overall impact within the \emph{ExaGeoStat} software pipeline using synthetic datasets and a real-world wind speed dataset for climate and weather modeling applications.  
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%STOP
%Spatial statistics applications aim to study patterns, distributions, and relationships of variables across different spatial locations in a given spatial region. The abundance of massive spatial data volumes makes High-Performance Computing indispensable for effectively handling the processing of high-resolution maps. Numerous attempts to scale existing modeling and prediction methods in spatial statistics have been proposed in the literature. For Example, \emph{ExaGeoStat} is a high-performance computational framework tailored to address large-scale spatial data challenges in climate and environmental studies. %\emph{ExaGeoStat} abandons the use of Partial Differential Equation (PDE) methods \cite{bolin_spatial_2011}, opting instead for statistical inference grounded in the exact estimation of Maximum Likelihood Estimation (MLE).
%\emph{ExaGeoStat} assumes that spatial measurements reflect a Gaussian spatial random field. The corresponding Gaussian log-likelihood is the most commonly used objective. In other literature related to machine learning, this concept is commonly referred to as Gaussian regression. This function operates with various covariance matrices generated from predefined covariance functions. Given the nature of spatial problems, they typically involve many measurements taken at regular or irregular locations across a geographical area. This necessitates that the synthetic dataset generation process can handle dense exascale matrices. The statistical framework is given by first denoting spatial measurements $(z_1, ..., z_N) := (Z(\bs{s}_1), ..., Z(\bs{s}_N))$, where $Z(\bs{s})$ denotes the realization of the Gaussian random field at location $\bs{s}$. We further assume that the mean function of this random field is $m(\bs{s})$. Often, $m(\bs{s})$ will be constant, which is the prerequisite of a stationary random field. The kernel matrix of the random field is 
%${\boldsymbol \Sigma}(\bs{\theta}) := \big(C(\bs{s}_i - \bs{s}_j; \bs{\theta})\big)_{ij}, i,j \in 1, ..., N,$
%where $N$ is the number of spatial locations, $\bs{s}_i - \bs{s}_j$ is the spatial lag vector of location $\bs{s}_i$ and $\bs{s}_j$, and $C(\cdot\, ; \bs{\theta})$ is a stationary parametric covariance/kernel function w.r.t. spatial lag vectors. Lastly, the frequent use of Gaussian random fields in geospatial studies can be attributed to their unique characteristic. Given a mean function and a kernel matrix, a Gaussian random field can be uniquely defined. This property allows for precise and consistent spatial data modeling, which is crucial in geospatial studies.

%Moreover, \textit{ExaGeoStat} uses a 'black-box' (zero-gradient/derivative-free) optimization method known as BOBYQA. The BOBYQA algorithm fits a quadratic model to the function to minimize (or maximize) the log-likelihood function and obtains a set of statistical parameters that represent the underlying spatial region. To precisely evaluate the function value of log-likelihood, the MLE objective function can be expressed with the following formula:
%\vspace{-1mm}
%\[l(\bs{\theta}) = -\frac{N}{2}\log(2\pi) - \frac{1}{2}\log\left|{\boldsymbol \Sigma}(\bs{\theta})\right| - \frac{1}{2}\bs{Z}^\top{\boldsymbol \Sigma}(\bs{\theta})^{-1}\bs{Z}.\vspace{-1mm}\] 
%As dimensions increase, the computational demand typically grows significantly. The state-of-the-art linear algebra system MAGMA can deal with matrix multiplication, determinant evaluation, and matrix inversion on GPU hardware accelerators. However, we also need fast covariance matrix generation on GPUs. Prior works exclusively relied on the CPU for matrix generation. Instead, the current work focuses on leveraging the power of GPUs to generate the covariance matrix for spatial statistics applications.


\section{Related Work}
%Applications
%\textcolor{red}{Zipei's comment: Isn't this paragraph redundant?}

%\textcolor{red}{The modified Bessel function of the second kind, \( K_\nu(x) \) or \textsc{BesselK}, is a fundamental tool in various fields of science and engineering due to its unique mathematical properties~\cite{weisstein2002modified}. These properties make \textsc{BesselK} well-suited for solving problems in differential equations~\cite{dunster1990bessel}, and physical systems~\cite{zhukovsky2017solving}. A notable characteristic of \textsc{BesselK} is its \textit{rapid decay at infinity}, meaning the function diminishes exponentially as \( x \to \infty \). This property makes it invaluable for modeling boundary conditions or phenomena where the solution must vanish far from the origin. Key applications include heat conduction~\cite{hetnarski2009heat}, which models temperature distributions in cylindrical or spherical systems; wave propagation~\cite{gomez2017physics}, where it describes damped wave behavior in cylindrical waveguides; and stochastic processes~\cite{pogany2013bessel}, which represent probability densities in systems where long-distance contributions are negligible.}

The modified Bessel function of the second kind, \( K_\nu(x) \) or \textsc{BesselK}, arises in the separation of variables solutions of partial differential equations~\cite{weisstein2002modified,dunster1990bessel, zhukovsky2017solving}. A key feature of \textsc{BesselK} is its \textit{rapid decay at infinity}. The function diminishes exponentially as \( x \to \infty \), providing a basis for solutions that vanish at large distances. Applications include heat conduction~\cite{hetnarski2009heat}, wave propagation~\cite{gomez2017physics}, and stochastic processes~\cite{pogany2013bessel}.


\textsc{BesselK} also arises in the widely used Mat\'{e}rn kernel, a covariance function for Gaussian processes. The Mat\'{e}rn kernel finds extensive applications across various fields. In spatial statistics, it is used to model spatial correlations~\cite{wang2023parameterization}. In machine learning, it is particularly valuable for Gaussian process regression and Bayesian optimization~\cite{albahar2021robust}. In signal processing, it helps capture correlations in time series and spectral analysis~\cite{yin2020linear}. 
%These applications underscore the versatility and scientific significance of \textsc{BesselK} in both theoretical and applied domains.
 
At least six different methods for \textsc{BesselK} can be identified in the literature; however, not all of these methods are adopted in existing tools and libraries. (1) \textit{Series Expansions}~\cite{bbtemme}: \( K_\nu(x) \) can be expressed as an infinite series in powers of \( x \), which is truncated to a finite number of terms when \( x \) is small. Most existing libraries utilize series expansions for small \( x \), including MATLAB, Mathematica, GSL, SciPy, and Maple. (2) \textit{Continued Fractions}~\cite{amos1974computation}: \( K_\nu(x) \) can be represented as an infinite fraction with a recursive structure, particularly suitable when \( x \) is moderate or large. This method is also used in most existing libraries for moderate $x$ values. (3) \textit{Asymptotic Expansions}~\cite{olver2009bessel}: This method focuses on the dominant behavior of \( x \) as it becomes large, ignoring lower-order terms, which reduces the accuracy for small \( x \). This method is also used in most existing libraries for large \( x \) values.
%\textcolor{red}{(4) is not a single method, it is a relation and it is used in many methods including (1) and (3).}
%\textcolor{red}{(4) \textit{Recurrence Relations}~\cite{thacher1979new}: Mathematical formulas that relate the values of a function at different orders $\nu$, enabling efficient computation by leveraging previously calculated values, allowing efficient computation for different orders $\nu$ once the function is known for a particular order. Examples of libraries that employ this approach include MATLAB, SciPy, and Boost C++; }
(4) \textit{Integral Representations}~\cite{bbtakekawa,luke2014integrals}: \( K_\nu(x) \) is expressed as integrals. This method is flexible and can work with small or large values of $x$. Some libraries that rely on this method include Mathematica, SciPy, and GSL. (5) \textit{Polynomial Fitting}~\cite{grosswald1978bessel}: Polynomial fitting approximates \( K_\nu(x) \), by fitting polynomials to precomputed values of the function over restricted ranges of \( x \) and \( \nu \). (6) \textit{Differential Equation Solvers}~\cite{carley2013numerical}: Numerical methods for solving differential equations, such as finite difference schemes and Runge-Kutta methods~\cite{butcher2007runge}, can be employed to compute \( K_\nu(x) \). These solvers approximate the solution of \( K_\nu(x) \) by discretizing the domain and applying iterative algorithms to solve the governing differential equation.

% \textcolor{red}{As for the GPU \textsc{BesselK} implementation, Plesner et al.~\cite{bbeth} developed a GPU library for computing logarithms of modified Bessel functions of the second kind, addressing critical precision limitations and numerical stability issues present in existing libraries. Their approach provided more robust results compared to existing libraries such as GSL, however, their implementation showed large limitations for modified Bessel functions of the second kind with small values ($(x, \nu) \in [0, 150] \times (0, 150]$) in accuracy and runtime, which is a strong shortcoming since most real applications cannot reach that kind of smoothness level, the typical range of $\nu$ is in between $0$ and $3.5$~\cite}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1- Give brief about \emph{ExaGeoStat}and the missing of GPU support of matern kernel
% 2- Related work to solve the BesselK function (GNU, takekawa, and ICS24 reference, ...etc)


%TO-DO: reduce the contexts of \emph{ExaGeoStat}but the necesseity of accurate and fast evaluation of modified bessel function of the second kind, with a comprehensive review of existing method, comparing advantages and disadvantages. Also, mentioning the bottleneck of evaluation of derivative of besselk which is not suitable for gradient-based optimization.

%\emph{ExaGeoStat}, a high-performance software package designed for climate and environmental geostatistics on large-scale computing systems. The software implements exact maximum likelihood estimation (MLE) for large spatial datasets using the Mat\'{e}rn covariance function, enabling both parameter estimation and prediction of missing measurements across geographical locations. \emph{ExaGeoStat} leverages state-of-the-art dense linear algebra libraries (Chameleon) and runtime systems (StarPU) to achieve high performance across different hardware architectures, including multicore CPUs, GPUs, and distributed systems. The software can handle unprecedented large-scale exact computations, demonstrated through synthetic and real-world soil moisture datasets. The authors also provide an R interface (ExaGeoStatR) to make the software accessible to statisticians. The software serves as a benchmark for validating various approximation techniques and provides a complete machine learning pipeline for geostatistical applications. Using the state-of-the-art runtime system, we can further parallelize and reduce the computational complexity of GPU.

%The previous work~\cite{geng2023gpu} presents new GPU-accelerated methods for generating dense covariance matrices in spatial statistics applications, specifically enhancing the \emph{ExaGeoStat} software framework. The authors propose two implementation schemes: a pure GPU approach for kernels without special functions like the modified Bessel function and a hybrid CPU-GPU approach for kernels that require special functions (like the Mat\'{e}rn kernel) that cannot be computed directly on GPUs. Performance evaluations show that the pure GPU implementation achieves up to 6X speedup compared to CPU-only implementations for the power exponential kernel, while the hybrid approach achieves up to 1.5X speedup for the Mat\'{e}rn kernel. The implementations were tested on different hardware configurations, including Intel processors (Skylake and Icelake) and NVIDIA GPUs (V100 and A100), demonstrating significant performance improvements, especially as the number of spatial locations increases.

%To prevent the computational bottleneck of CPU-only implementation on Mat\'{e}rn kernel function evaluation, it is necessary to implant the code from CPU-only to GPU-only, which prevents generating the matrix and evaluation using CPU. Recent work introduces new algorithms for computing the logarithm of modified Bessel functions of the first and second kind on GPUs, proposing a pure GPU implementation scheme. The modified Bessel function of the second kind is defined as the solution of

%\[x^2 \frac{d^2y}{dx^2} + x\frac{dy}{dx} - (x^2 + \nu^2)y = 0,\] denoted by $K_{\nu}(x)$, where $\nu$ represents for smoothness. The algorithms demonstrate significant improvements in numerical stability and performance, with no underflow or overflow issues, achieving precision equal to or better than current C++ libraries and runtimes that are typically one to two orders of magnitude faster. The authors validate their approach with both numerical experiments and a real-world case study. Another work presents a novel method for efficiently computing the modified Bessel function of the second kind and its derivatives using a parallel numerical integration approach, addressing the computational challenges in existing methods that use series expansion, continued fraction, or asymptotic expansion. The proposed method pre-computes the integration range and uses a fixed number of intervals for numerical integration, making it more suitable for parallel processing on CPUs and GPUs and multi-core processors while maintaining accuracy comparable to existing methods with the usage of PyTorch. Although it gains certain accuracy using the numerical integration, it does not consider the problem region when $x < 0.1$. In summary, the problem region defined in those papers is either far too large in speaking of the smoothness parameter or limited in speaking of the support of the function, which will be unsuitable for environmental studies. To improve computational performance, we propose a new GPU-accelerated algorithm for evaluating the modified Bessel function of the second kind, which can be integrated with spatial statistics frameworks like \emph{ExaGeoStat}and optimized by focusing on specific study regions.

\section{Background}
This section addresses the challenges in the modeling of Gaussian processes and highlights the importance of accelerating the computations of the \textsc{BesselK} functions to enhance the modeling procedure. Furthermore, we provide an overview of the \emph{ExaGeoStat} software and its implementation, as a testbed to evaluate our novel \textsc{BesselK} implementation.



\subsection{ Gaussian Processes and Mat\'{e}rn Kernel}
 Gaussian process (GP) modeling is a probabilistic machine learning approach that defines distributions over functions and distinguished by its mean function $m(\mathbf{x})$, and covariance function $C(\mathbf{x}, \mathbf{x}')$, where $\mathbf{x}$, $\mathbf{x}'$ are locations. A commonly used method in Gaussian process modeling is the maximum likelihood estimation (MLE). In MLE, an optimization process iterates over a given log-likelihood function to estimate statistical parameters that describe the underlying data. This is achieved by constructing a positive definite covariance matrix that captures the correlation between different observations. The log-likelihood function \( \mathcal{L}( \boldsymbol{\theta}) \) is represented as:
\vspace{-1mm}
\[
\mathcal{L}( \boldsymbol{\theta}) = -\frac{1}{2} \left[ N \log(2\pi) + \log\left(|\mathbf{\Sigma}( \boldsymbol{\theta})|\right) + \mathbf{z}^\top \mathbf{\Sigma}(\boldsymbol{\theta})^{-1} \mathbf{z} \right],
\]
%\vspace{-2mm}
\noindent
 where \( \boldsymbol{\theta} \) is a set of parameters to be fit, \( N \) is the number of observations, \( \mathbf{\Sigma}(\boldsymbol{\theta}) \) is the parameterized covariance matrix, \( |\mathbf{\Sigma}(\boldsymbol{\theta})| \) is its determinant, and \( \mathbf{z} \) is the vector of observed data. The Matérn function serves as a fundamental component in spatial statistics, representing a significant domain where Gaussian processes demonstrate their utility, owing to its adaptability in characterizing spatial correlation structures. Its parametric flexibility enables precise modeling of spatial relationships across diverse applications and datasets~\cite{wang2023parameterization}.

The Mat\'{e}rn function can be represented as:
\[C(\mathbf{x}, \mathbf{x}')={\cal M}(r;\boldsymbol{\theta}) = \frac{\sigma^2}{2^{\nu - 1}\Gamma(\nu)}\left(\frac{r}{\beta}\right)^{\nu}K_{\nu}\left(\frac{r}{\beta}\right),\]
\noindent
where \( r = \| \mathbf{x} - \mathbf{x}' \| \) is the distance between two spatial locations and $\boldsymbol{\theta} = (\sigma^2, \beta, \nu)^\top$, \( \nu > 0 \) is a parameter that controls the smoothness of the function, \( \beta \) is the length scale parameter, \( \sigma^2 \) is the variance, \( \Gamma(\nu) \) is the Gamma function, and \( K_\nu(\cdot) \) is the modified Bessel function of the second kind of order \(\nu\) (\textsc{BesselK($x, \nu$)}). %We can adjust \(\nu\) for different levels of smoothness: \(\nu = 1/2\): Exponential kernel (rough functions), \(\nu \to \infty\): Squared exponential (Gaussian) kernel (infinitely differentiable).

The \textsc{BesselK} is the core in the generation of the covariance matrix when using the Mat\'{e}rn function,
%Therefore, optimizing the computation of the \textsc{BesselK} function is essential to prevent performance bottlenecks during covariance matrix generation. Moreover, the covariance matrix 
which is required not only during the modeling process but also for prediction tasks and the generation of synthetic datasets~\cite{gramacy2020surrogates, leandro2021exploiting}.


%lies in its ability to analyze and interpret spatially-referenced data, providing insights into patterns, relationships, and dependencies across geographic or spatial domains.


\subsection{ExaGeoStat: A Parallel Tile-Based Framework for Geospatial Data Analysis}

  % \begin{figure}
  %       \centering
  %       \includegraphics[width=0.5\textwidth]{figs/dag.pdf} 
  %       \caption{Directed acyclic graph of tasks for Cholesky factorization of a $4 \times 4$ matrix: Nodes represent tasks, edges indicate dependencies. The runtime schedules tasks to hardware while maintaining correlation constraints.}
  %       \label{fig:dag}
  %   \end{figure}

    
\emph{ExaGeoStat} is a high-performance software package for large-scale climate and environmental geostatistics~\cite{abdulah2018exageostat}. It evaluates the log-likelihood function for spatial datasets using a range of covariance models, including the Mat\'{e}rn covariance function. This enables efficient parameter estimation and prediction for large-scale spatial datasets by leveraging state-of-the-art dense linear algebra libraries (e.g., CHAMELEON~\cite{chameleon2024}, and DPLASMA~\cite{bosilca2011flexible}) and runtime systems (e.g., StarPU~\cite{augonnet2009starpu}, and PaRSEC~\cite{hoque2017dynamic}). \emph{ExaGeoStat} achieves high performance across diverse hardware architectures, including multicore CPUs, GPUs, and distributed systems. The software supports both exact and approximate computations~\cite{abdulah2018parallel,abdulah2019geostatistical,cao2023reducing,zhang2024parallel}.

\emph{ExaGeoStat} relies on tile-based algorithms to effectively leverage the underlying runtime system to distribute tasks across available hardware resources. The runtime system efficiently schedules the processing of matrix tiles across computational units, maximizing performance and resource utilization. All executions are conducted exclusively on GPUs to further optimize the performance of \emph{ExaGeoStat}. 

%Figure~\ref{fig:dag} illustrates the concept of tile algorithms using the example of tile-based Cholesky factorization---a core operation in the MLE process---on a $4 \times 4$ tiled matrix, at the stage where the (1,1) tile has been factored and its inverse action applied down the first column of tiles.

\section {Algorithm for \textsc{BesselK}}
Our \textsc{BesselK} algorithm is built to compute a reasonable range of \( x \), and \( \nu \) that covers a wide spectrum of applications builds upon two existing methods: Temme's series expansion and Takekawa's algorithm, reviewed, respectively, in the following two subsections.

%\subsection{\textsc{BesselK}($x$, $\nu$) Function}
%This subsection introduces two key algorithms used to compute the \textsc{BesselK}($x$, $\nu$): the series expansion and Takekawa's methods. These methods serve as the core for implementing our proposed algorithm.

\subsection{Temme's series expansion}

Most existing libraries rely on series expansions to compute the \textsc{BesselK} function when \( x \) is small~\cite{bbtemme}. Series expansions offer an efficient and accurate approximation in this regime, with the series expressed as:%%%%%Sameh
\begin{equation}
    K_\nu(x) = \sum_{k=0}^\infty c_k f_k, \quad K_{\nu+1}(x) = \frac{2}{x} \sum_{k=0}^\infty c_k h_k,
    \label{eq:temme_main}
\end{equation} where
\[
c_k = \frac{(x^2 / 4)^k}{k!}, \quad h_k = -k f_k + p_k,
\]
\noindent
and \( p_k \) and \( q_k \) are recurrence relations defined as:
\[
p_k = \frac{p_{k-1}}{k - \nu}, \quad q_k = \frac{q_{k-1}}{k + \nu}.
\]
\noindent
The term \( f_k \) is computed iteratively:
\[
f_k = \frac{k f_{k-1} + p_{k-1} + q_{k-1}}{k^2 - \nu^2}.
\]
\noindent
The recurrence relations are initialized with:
\begin{equation}
    p_0 = \frac{1}{2} \left(\frac{x}{2}\right)^{-\nu} \Gamma(1 + \nu), \quad q_0 = \frac{1}{2} \left(\frac{x}{2}\right)^\nu \Gamma(1 - \nu),
\label{eq:temme_initpq}
\end{equation}
\begin{equation}
    f_0 = \frac{\nu \pi}{\sin(\nu \pi)} \left[ \cosh(\sigma) \Gamma_1(\nu) + \frac{\sinh(\sigma)}{\sigma} \ln\left(\frac{2}{x}\right) \Gamma_2(\nu) \right],
    \label{eq:temme_initf}
\end{equation}
where \( \sigma \) and \( \Gamma_1(\nu) \), \( \Gamma_2(\nu) \) are precomputed constants for the expansion. 
%This approach ensures accurate computation even for small values of \( x \).

The original formulation of Temme's series expansion in~\cite{bbtemme} does not provide sufficiently stable or accurate results for \( \nu \geq 1.5 \). The direct evaluation of \( p_0, q_0, \text{ and } f_0 \) for large orders of \( \nu \) increases the error of this approximation method. To address these limitations and improve numerical stability and accuracy, Campbell~\cite{campbell1980temme} proposed leveraging the recurrence relation for large-order \( \nu = \mu + M \) (where \( M = \lfloor \nu + 0.5 \rfloor \)) of \( K_\nu(x) \). 

This approach uses starting values \( K_\mu(x) \) and \( K_{\mu+1}(x) \), with \( -\frac{1}{2} \leq \mu < \frac{1}{2} \), and applies the forward recurrence relation:
\begin{equation}
K_{\eta+1}(x) = \left(\frac{2\eta}{x}\right)K_\eta(x) + K_{\eta-1}(x). \label{eq:recurrence}
\end{equation}

We employ Temme's series expansion and the recurrence relation to evaluate the \textsc{BesselK} function in the small-\(x\) regime (\(x < 0.1\)).



%\subsubsection{$x \geq 2, \nu > 0$}

%For \(x > 2\), the Thompson-Barnett-Temme method is employed to approximate the modified Bessel function \(K_\nu(x)\), using the continued fraction (CF2) for \(K'_\nu / K_\nu\). This approach relies on confluent hypergeometric functions and their recurrence relations to compute \(K_\nu(x)\) and \(K_{\nu+1}(x)\), with normalization ensured via a series summation condition. The method combines CF2 evaluation and normalization through efficient recursive algorithms to achieve accurate results.


%For $x > 2$, we use the Thompson-Barnett-Temme method, which utilizes the continued fraction to approximate the function value. This method is unstable when $x$ is small.

%Define the second continued fraction (CF2) as $K'_{\nu}/K_{\nu}$. We aim to use the approximation to CF2 to get the approximate value of $K_{\nu}$ and $K_{\nu + 1}$.

%To get CF2 and an additional normalization condition in a convenient form, consider the sequence of confluent hypergeometric (CH) functions.
%\[
%z_n(x) = U\left(\nu + \frac{1}{2} + n, 2\nu + 1, 2x\right)
%\]
%for fixed \(\nu\). Then
%\begin{equation}
%    K_\nu(x) = \pi^{1/2} (2x)^\nu e^{-x} z_0(x) \label{eq:1}
%\end{equation}
%\begin{equation}
%    \frac{K_{\nu+1}(x)}{K_\nu(x)} = \frac{1}{x} \left[\nu + \frac{1}{2} + x + \left(\nu^2 - \frac{1}{4}\right) \frac{z_1}{z_0}\right] \label{eq:2}
%\end{equation}

%Equation (\ref{eq:1}) is the standard expression for \(K_\nu\) in terms of a confluent hypergeometric function, while equation (\ref{eq:2}) follows from relations between contiguous confluent hypergeometric functions (equations 13.4.16 and 13.4.18 in Abramowitz and Stegun). Now the functions \(z_n\) satisfy the three-term recurrence relation (equation 13.4.15 in Abramowitz and Stegun)
%\begin{equation}
%z_{n-1}(x) = b_n z_n(x) + a_{n+1} z_{n+1}
%\end{equation}
%with
%\[
%b_n = 2(n + x)
%\]
%\[
%a_{n+1} = -\left[(n + 1/2)^2 - \nu^2\right]
%\]
%Through some additional steps, we get the CF2
%\begin{equation}
%\frac{z_1}{z_0} = \cfrac{1}{b_1 + \cfrac{a_2}{b_2 + \cdots}} %\label{eq:3}
%\end{equation}
%from which (\ref{eq:2}) gives \(K_{\nu+1}/K_\nu\), and thus \(K'_\nu/K_\nu\).

%Temme's normalization condition is that
%\begin{equation}
%    \sum_{n=0}^{\infty} C_n z_n = \left(\frac{1}{2x}\right)^{\nu+1/2} \label{eq:4}
%\end{equation}
%where
%\[
%C_n = \frac{(-1)^n \Gamma(\nu + 1/2 + n)}{n! \, \Gamma(\nu + 1/2 - n)}
%\]
%Note that the \(C_n\)s can be determined by recursion:
%\[
%C_0 = 1, \quad C_{n+1} = -\frac{a_{n+1}}{n + 1} C_n
%\]
%We use the condition (\ref{eq:4}) by finding
%\begin{equation}
%    S = \sum_{n=1}^{\infty} C_n \frac{z_n}{z_0} \label{eq:sum}
%\end{equation}
%Then
%\[
%z_0 = \left(\frac{1}{2x}\right)^{\nu+1/2} \frac{1}{1 + S} 
%\]
%and (\ref{eq:1}) gives \(K_\nu\).

%Thompson and Barnett have given a clever method of doing the sum (\ref{eq:sum}) simultaneously with the forward evaluation of the CF2. Suppose the continued fraction is being evaluated as
%\[
%\frac{z_1}{z_0} = \sum_{n=0}^{\infty} \Delta h_n
%\]
%where the increments \(\Delta h_n\) are found by, e.g., Steed’s algorithm. Then the approximation to \(S\) keeping the first \(N\) terms can be found as
%\[
%S_N = \sum_{n=1}^{N} Q_n \Delta h_n
%\]
%Here
%\[
%Q_n = \sum_{k=1}^{n} C_k q_k
%\]
%and \(q_k\) is found by recursion from
%\[
%q_{k+1} = \left(q_{k-1} - b_k q_k\right)/a_{k+1}
%\]
%starting with \(q_0 = 0\), \(q_1 = 1\). For the case at hand, approximately three times as many terms are needed to get \(S\) to converge as are needed simply for CF2 to converge.


\subsection{Integral-based Algorithm (Takekawa's Approach)}

In~\cite{bbtakekawa}, Takekawa introduced a method to compute the \textsc{BesselK} function using an integral approach. \( K_{\nu}(x) \) is represented by the following integral from~\cite{watson1922treatise}:
\begin{equation}
K_\nu(x) = \int_0^\infty e^{-x \cosh(t)} \cosh(\nu t) \, \mbox{d}t \overset{\Delta}{=} \int_0^{\infty} f_{\nu, x}(t) \, \mbox{d}t,
\label{eq:integral}
\end{equation}
where \( x > 0 \) and \( \nu \in \mathbb{R} \). This representation is computationally intensive but provides accurate results, especially for small and moderate \( x \).

%T. Takekawa analyzed the above integral to study the behavior of the Bessel function for different values of $x$ and $\nu$. 
Takekawa worked with the logarithm of the integrand
\(e^{-x \cosh(t)} \cosh(\nu t)  \label{eq:ft}\). Namely,
\begin{equation}
g_{\nu, x}(t) = \log \cosh(\nu t) - x \cosh(t).
\label{eq:logintegrand}
\end{equation}

The first-order, and second-order derivatives of \( g_{\nu, x}(t) \) with respect to \( t \) are given by:
\[
    g'_{v,x}(t) = v \tanh(vt) - x \sinh(t), 
\]
 \[   g''_{v,x}(t) = v^2 \operatorname{sech}^2(vt) - x \cosh(t). \]
   % \ell'''_{v,x}(t) &= -v^3 \operatorname{sech}^2(vt) \tanh(vt) - x \sinh(t).

At \( t = 0 \), we have \( g_{\nu,x}(0) = -x \) and \( g'_{\nu,x}(0) = 0 \). Consequently, if \( \nu^2 \leq x \), the function \( g_{\nu,x}(t) \) will always decrease, implying that the maximum value of the function in Equation~\eqref{eq:logintegrand} occurs at \( t = 0 \), as \( g''_{\nu,x}(0) \leq 0 \). In contrast, if \( \nu^2 > x \), the function in Equation~\eqref{eq:logintegrand} will reach its maximum at some \( t \geq 0 \), since \( g''_{\nu,x}(0) > 0 \). More details are provided in~\cite{bbtakekawa}.

To integrate Equation~\eqref{eq:integral}, the region where the maximum value of \( t \), denoted as \( t_{\text{max}} \), is located, can be defined as the region where \( {f_{\nu,x}(t) \geq \epsilon_{\text{machine}} f_{\nu,x}(t_{\text{max}})} \), where \(\epsilon_{\text{machine}}\) is the machine epsilon. This region can be expressed as interval \( [t_0, t_1] \), defined as:
\vspace{-2mm}
\begin{equation}
    [t_0, t_1] = \{ t \mid g_{\nu,x}(t) \geq \log(\epsilon_{\text{machine}}) + g_{\nu,x}(t_{\text{max}}) \}.
\end{equation}
Takekawa defines the integral range by determining \( t_{\text{max}} \) from Equation~\eqref{eq:logintegrand}. If \( \nu^2 \leq x \), then \( t_{\text{max}} = 0 \). Otherwise, \( t_{\text{max}} \) can be found by searching within a specific range of the function \( g_{\nu,x}(t) \). Since \( g'_{\nu,x}(t_{\text{max}}) = 0 \) and \( g'_{\nu,x}(t) < 0 \) for $t > t_{\text{max}}$, the range can be defined as \( [2^{m-1}, 2^{m}] \), where \( m \) is the smallest value such that \( g'_{\nu,x}(2^m) < 0 \). For a detailed explanation, refer to the {\it FINDRANGE} algorithm in~\cite{bbtakekawa}. Afterward, the $t_{\text{max}}$ value can be obtained using binary search and Newton methods; refer to the {\it FINDZERO} algorithm in~\cite{bbtakekawa}.

The integration range is also determined using the {\it FINDZERO} algorithm. To compute \( t_0 \) (the lower bound of integration), if \( \nu^2 \leq x \), then \( t_0 = 0 \). Otherwise, the {\it FINDZERO} algorithm is applied to find \( t_0 \) within the range \( [t_0, t_{\text{max}}] \). For \( t_1 \), the {\it FINDZERO} algorithm is used similarly to search within the range \( {[t_{\text{max}} + 2^{m-1}, t_{\text{max}} + 2^m]} \).

After determining \( t_0 \) and \( t_1 \), integration is performed over the range using a fixed number \( b\) of bins. To ensure numerical stability, the \texttt{log\_sum\_exp} function is applied to \( g_{\nu,x}(t_m) \), resulting in:
\begin{equation}
\begin{split}
\log K_{\nu}(x) \approx & \\ g_{\nu,x}(t_{\text{max}}) + & \log \sum_{m=0}^{b} h \exp \Big\{ c_m \big( g_{\nu,x}(t_m) - g_{\nu,x}(t_{\text{max}}) \big) \Big\}.
\end{split}
\label{eq:refined}
\vspace{-30mm}
\end{equation}
Here, the parameters are defined as:
\vspace{-1.5mm}
\[
h = \frac{t_1 - t_0}{b}, \quad t_m = t_0 + mh, 
\]
\vspace{-5mm}
\[
c_0 = c_n = \frac{1}{2}, \quad c_m = 1 \quad (m = 1, \ldots, b-1).
\vspace{-1.5mm}
\]
$K_{\nu}(x)$ can be obtained by taking the exponential.

%%%%%%%%%%%%%%%%
%.......... In this method, the integration range is pre-determined, and the number of bins used for numerical integration is fixed. This approach offers robustness even when the integration range is estimated coarsely, ensuring accuracy is not significantly compromised. Moreover, the inherent parallelization potential of this method allows for efficient computation.

%For \( \nu \in \mathbb{R} \), \( x > 0 \), modified Bessel functions of the %second kind can be represented by the evaluation of integrals of the form:
%\begin{equation}
%K_{\nu}(x) = \int_{0}^{\infty} f_{\nu, x}(t) \, dt
%\end{equation}
%where
%\begin{equation}
%f_{\nu, x}(t) = \cosh(\nu t) \exp(-x \cosh t).
%\end{equation}
%Considering the shape of \( f_{\nu, x}(t) \), we examine the increase or %decrease in its logarithmic form as it is more stable in the sense of %numerical integration.
%\begin{equation}
%\log f_{\nu, x}(t) = g_{\nu, x}(t) = \log \cosh(\nu t) - x \cosh(t).
%\end{equation}
%The derivatives of \( g_{\nu, x}(t) \) relative to \( t \) are given by
%\begin{align}
%g'_{\nu, x}(t) &= \nu \tanh(\nu t) - x \sinh(t), \\
%g''_{\nu, x}(t) &= \nu^2 \mathrm{sech}^2(\nu t) - x \cosh(t)
%\end{align}

%We aim to find the lower bound and upper bound of the numerical integration since the machine cannot properly define $\infty$ itself. When integrating \( f_{\nu, x}(t) \) numerically, we solely need to consider the region where \( f_{\nu,x}(t) \geq f_{\nu,x}(t_p) \), in which \( g_{\nu,x}(t) \) takes the maximum value at \( t_p \), and \( \epsilon \) denotes the machine epsilon (double precision usually $10^{-16}$). In fact, from the shape of \( g_{\nu,x}(t) \), the region that satisfies the condition can be defined as a single continuous range as:
%\begin{equation}
%[t_0, t_1] = \{ t \mid f_{\nu, x}(t) \geq \eps f_{\nu, x}(t_p) \}
%\end{equation}
%\begin{equation}
%= \{ t \mid g_{\nu, x}(t) \geq g_{\nu, x}(t_p) + \log \epsilon \}.
%\end{equation}

%In order to find $[t_0, t_1]$, we need to first find $t_p$. To find $t_p$ efficiently, we define the following two algorithms.

%\subsubsection{\textsc{FindRange} Algorithm}
% This algorithm is to shorten the search region of zero point of a function $h(\cdot)$. Note that this algorithm cannot be applied to a generic function but only suitable for our implementation.
 
%\begin{algorithm}[!hbt]
%\caption{$\textsc{FindRange}(h, t_0)$}
%\begin{algorithmic}[1]
%\Require \( h(t_0) \geq 0 \)
%\Function{FindRange}{$h, t_0$} \Comment{return $t_* \geq t_0$, such that $h(t_*) < 0$}
%    \State \( m \leftarrow 0 \)
%    \While{\( h(t_0 + 2^m) \geq 0 \)}
%        \State \( m \leftarrow m + 1 \)
%    \EndWhile
%    \State \textbf{return} \( t_0 + 2^{m-1} \), \( t_0 + 2^m \)
%\EndFunction
%\end{algorithmic}
%\end{algorithm}

%\subsubsection{\textsc{FindZero} Algorithm}
%This algorithm is to find the zero point of a certain function $l(\cdot)$ given start point $t_s$ and end point $t_e$.

%\begin{algorithm}
%\caption{$\textsc{FindZero}(l, t_s, t_e)$}
%\hfill

%Using a combination of bisection and Newton-Raphson method to find zero point of function $l(\cdot)$ between $t_s$ and $t_e$.

%\end{algorithm}

%Using the integration algorithm to find $t_0, t_1$, integrate numerically over the range with the fixed number of divisions $n$:
%\[K_{\nu}(x) \sim h\sum_{m=0}^n c_m f_{\nu, x}(t_m),\] where $$h = (t_1 - t_0)/n,\; t_m = t_0 + mh,$$ $$c_0 = c_n = 1/2,\; c_m = 1 \text{, for }m \neq 0 \;\&\; n.$$
%To stably estimate the numerical integration, the log-sum-exponential technique is applied to $g_{\nu, x}(t_m)$ and enable us to estimate $\log(f_{\nu, x})$ by $\log(g_{\nu, x})$.

%\begin{align*}
%    \log \sum_{i=1}^{N} z_i &= \log \sum_{n=1}^{N} e^{ \log z_i } \\
%    &= \log(e^a \times \sum_{n=1}^{N} e^{ \log z_i - a }) \\
%    &= a + \log \sum_{n=1}^{N} e^{\log z_i - a}
%\end{align*}


%We thus can evaluate the original integration as %the following

%\begin{equation}
%    \log K_v(x) \approx g_{v,x}(t_p) + \log \sum_{m=0}^{n} h \exp\{c_m(g_{v,x}(t_m) - g_{v,x}(t_p))\}.
%    \label{eq:imp}
%\end{equation}


%\begin{algorithm}
%\caption{Main algorithm of \textsc{LogBesselK}}
%\begin{algorithmic}[1]
%\Function{LogBesselK}{$\nu, x$} \Comment{return %$\log K_\nu(x)$}
%    \State $t_p \gets 0$
%    \If{$g''_{\nu, x}(0) > 0$} \Comment{find $t_p > 0$}
%        \State $t_s, t_e \gets \textsc{FindRange}(g'_{\nu,x}(t), 0)$
%        \State $t_p \gets \textsc{FindZero}(g'_{\nu,x}(t), t_s, t_e)$
%    \EndIf
%    \State $t_0 \gets 0$
%    \If{$g_{\nu,x}(0) - g_{\nu,x}(t_p) \leq \log \varepsilon$} \Comment{find $0 < t_0 \leq t_p$}
%        \State $t_0 \gets \textsc{FindZero}(g_{\nu,x}(t) - g_{\nu,x}(t_p) - \log \varepsilon, 0, t_p)$
%    \EndIf
%    \State $t_s, t_e \gets \textsc{FindRange}(g_{\nu,x}(t) - g_{\nu,x}(t_p) - \log \varepsilon, t_p)$ \Comment{find $t_1 > t_p$}
%    \State $t_1 \gets \textsc{FindZero}(g_{\nu,x}(t) - g_{\nu,x}(t_p) - \log \varepsilon, t_s, t_e)$
%    \State \Return $\log K_v(x)$ as defined in Eq. (\ref{eq:imp}).
%\EndFunction
%\end{algorithmic}
%\end{algorithm}

%The value of $K_{\nu}(x)$ is acquired by simple exponential function $K_{\nu}(x) = \exp(\log(K_{\nu}(x)))$
%Sameh
\subsection{The Proposed Refined Algorithm} %and Its GPU-Based Implementation}
Existing numerical libraries, such as MATLAB, SciPy, and Boost C++, do not offer GPU support to evaluate the \textsc{BesselK} function, resulting in a significant performance bottleneck for GPU-accelerated scientific applications that depend on this function. Performing \textsc{BesselK} computations on the CPU while executing other tasks on the GPU is highly time-consuming due to the overhead of data transfers between the CPU and GPU and the GPU's superior parallel processing capabilities compared to the CPU. Although Plesner et al.~\cite{bbeth} recently introduced a GPU-accelerated library for \textsc{BesselK} evaluation, its performance gains are limited. Their implementation outperforms GSL only within the parameter range \((x, \nu) \in (150, 4000] \times (150, 4000]\) and exhibits a relatively limited performance outside of this range compared to GSL.  

To address this gap, we propose a novel approach that advances the state-of-the-art in GPU-accelerated \textsc{BesselK} computation, delivering improved performance across a reasonable parameter space. We adopt the quadrature-based algorithm proposed by Takekawa~\cite{bbtakekawa}. Although Takekawa's algorithm demonstrates substantial accuracy for large values of \( x \) and \( \nu \), it notably lacks discussion of cases where \( x < 0.1 \), a range frequently encountered in applications such as spatial statistics. Our analysis reveals that within this range, the integration algorithm exhibits a significant loss of accuracy. Figure~\ref{fig:Takekawa_heatmap} shows a heatmap of the relative error in Takekawa's approach for \( x < 0.1 \), compared to referential results obtained using Mathematica~\cite{paul2018mathematica}. Following Takekawa in ~\cite{bbtakekawa}, we compute the relative error (RE) at each \( x \) and \( \nu \) as:
\[
\text{RE} = 
\log_{10} \left( 1 + \frac{\lvert \text{Mathematica's output} - \text{output} \rvert}
{\varepsilon_{\text{machine}}} \right)
\]
where $\varepsilon_{\text{machine}} = 2^{-52} \approx 2.22 \times 10^{-16}$ for double-precision numbers.

   \begin{figure}[!hbt]
        \centering
        \includegraphics[width=0.33\textwidth]{figs/logk_prec_takekawanew.png} 
        \caption{Relative error of Takekawa's algorithm vs Mathematica for \((\nu, x) \in [0.001, 5] \times [0.001, 0.1]\).}
        \label{fig:Takekawa_heatmap}
    \end{figure}

We propose a novel algorithm for the \textsc{BesselK} function that extends the range of \( x \) values beyond those supported by Takekawa's method. We refer to this enhanced approach as the \emph{Refined Algorithm}. Furthermore, we provide an efficient GPU-based implementation for generating large matrices using \textsc{BesselK} within the \emph{ExaGeoStat} software that relies on the \emph{StarPU} runtime system. Our key contributions to improving Takekawa's algorithm are summarized as follows:

1) Our algorithm simplifies the integral in Equation~\eqref{eq:integral} by setting the lower bound to \(0\), as reducing the range by computing \(t_0\) and \(t_1\) is more computationally expensive than extending the range on the GPU. 2) The upper bound is set to a maximum value determined through an empirical bound finder, ensuring applicability across all \(x\) and \(\nu\) ranges. 3) Instead of using the \textit{FINDZERO} algorithm to find a global \(t_{\text{max}}\), as in Takekawa's method, we compute a local \(t_{\text{lmax}}\) for each division within the integral range, enabling faster computation. 4) To improve accuracy, we increase the number of bins \(b\) in the integral, which offsets the expanded range \([t_0, t_1]\) and leverages GPU computational power with minimal performance impact. 5) For \(x < 0.1\), we combine the integral algorithm with Temme's expansion, reducing the relative error from \(6.78682\) in Takekawa's method to \(0.73180\) in the refined algorithm, provided a sufficient number of bins.

% \begin{itemize}
%     \item Our algorithm sets the lower bound of the integral in Equation~\eqref{eq:integral} to \(0\) rather than explicitly calculating it. We observed that reducing the integral range by computing \(t_0\) and \(t_1\) is more computationally expensive than extending the integral range on the GPU.

%     \item The refined algorithm sets the upper bound of the integral to the maximum value determined by our empirical analysis (discussed later) on Mathematica, ensuring that it is applicable across all ranges of \( x \) and \( \nu \) encountered in various applications.
%     \item Instead of searching for a global \( t_{\text{max}} \) using the \textit{FINDZERO} algorithm, as done in Takekawa's algorithm, we compute a local \( t_{\text{lmax}} \) for each division (two divisions form a bin) within the integral range. This localized approach enables faster computation compared to identifying a global maximum.
%     \item We increase the number \(b\) of bins in the integral due to expanding the integral range \( [t_0, t_1] \). This enhancement significantly improves the accuracy of the Bessel function in many cases while having minimal impact on performance due to the computational power of GPUs.

%     \item To address the high error in Takekawa's method for \( x < 0.1 \), we combine the integral algorithm with Temme's expansion. This refinement reduces the relative error from \( 6.78682 \) in Takekawa's algorithm to \( 0.73180 \) in the refined algorithm for this range, given a sufficient number of bins.


% \end{itemize}


%\subsection{The Refined Algorithm}
%Building on the results of \cite{bbtakekawa} and \cite{bbtemme}, we have developed a new scheme for evaluating \textsc{BesselK}. Specifically, Temme's series expansion method is employed for \( x \in [0, 0.1) \), while Takekawa's integration method is used for \( x \geq 0.1 \). Combined, these methods effectively cover the entire parameter space \( \mathcal{S} = \mathcal{X} \times \mathcal{V} \), where \( x \in \mathcal{X} \) and \( \nu \in \mathcal{V} \).

\begin{algorithm}
\caption{Empirical Upper Bound Finding}
\begin{algorithmic}[1]
\small
\State \textbf{Given:} $\mathcal{X} \times \mathcal{V} = [0, 140] \times (0, 20]$, $\text{MBK}(x, \nu)$ the reference level of \textsc{logBesselK} using Mathematica, and $\text{RBK}(x, \nu) := \log\left[\int_0^{L} e^{-x \cosh(t)} \cosh(\nu t) \, \mbox{d}t\right]$, where the integration follows the Equation \eqref{eq:refined}.
\ForAll{$(x, \nu) \in \mathcal{X} \times \mathcal{V}$}
\State $\text{AE}(x, \nu) \gets \vert \text{MBK}(x, \nu) - \text{RBK}(x, \nu)\vert$ (Absolute Error for $(x, \nu)$)
\EndFor
\State \(t_1 \gets \min L \;\; s.t. \; \max_{x,\nu} \text{AE}(x, \nu) \leq 10^{-9}\)
\end{algorithmic}
\label{alg:emprical}
\end{algorithm}
For $x \in [0.1, \infty)$, we use a refined version of the integration algorithm proposed in \cite{bbtakekawa} to evaluate \textsc{BesselK}. Instead of determining the integration range $[t_0, t_1]$ dynamically based on $x$ and $\nu$, we fix $t_0 = 0$ and $t_1 = 9$. We set $t_0 = 0$ to match the starting point of the integral, and through the empirical upper bound finding algorithm, we establish $t_1 = 9$ as the optimal endpoint for parameters within $(x, \nu) \in [0, 140] \times (0, 20]$, which corresponds to the expected parameter range in geospatial applications involving the Mat\'{e}rn kernel. To elaborate on, in geospatial and machine learning studies, we can always rescale the 2D plane inside a unit square, whose maximum distance is \(\sqrt{2}\). The typical starting point of optimizing the \(\beta\) parameter in the Mat\'{e}rn kernel is \(0.01\), which gives us the maximum \(x\) we have \(\sqrt{2}/0.01 \approx 140\). We are choosing \(x \in [0, 140]\). The smoothness parameter \(\nu\) controls the smoothness of the data and has mathematical interpretation as the \(m\)-th differentiability for integers $m<\nu$. The case \(\nu > 20\) can be approximated by the squared exponential kernel. Thus, choosing this study region is reasonable. Empirically finding the upper bound is shown in Algorithm~\ref{alg:emprical}.
%Since a larger upper bound leads to better integral approximation under the fixed numerical integration method, this algorithm is guaranteed to lead to a solution.
This broader integration range may slightly increase computational cost but improves GPU efficiency by avoiding performance-degrading conditional branching. Besides, this unified upper bound, no matter what \((x, \nu)\), largely decreased the computational burden compared to the original method, which determines the lower and upper bound for each \((x, \nu)\) pair using Newton-based zero-finders.



%
Algorithm~\ref{alg:refined} provides a detailed explanation of the steps of our proposed algorithm. The inputs are the pair ($x$, $\nu)$ to compute $K_{\nu}(x)$. To define $b$, the number of bins, increasing it can improve accuracy, but at the cost of reduced performance, as the algorithm must identify the local maximum point $t_{\text{lmax}}$ for each division. However, we observed that fixing the number of bins to $40$ provides a balance, achieving an accuracy threshold that ensures computational stability across different values of $x$ and $\nu$. In Algorithm~\ref{alg:refined}, Temme's expansion method is used to compute the Bessel function if $x\leq 0.1$ (lines 3-7); otherwise, use Equation (\ref{eq:refined}) to compute the Bessel function using $b$ bins (lines 8-13).
\begin{algorithm}
\caption{Refined Algorithm (\textsc{BesselK}($x$, $\nu$))} %Computation of \textsc{logBesselK} using Temme's expansion and modified integration method}
\begin{algorithmic}[1]
\small
\State \textbf{Input:} \((x, \nu) \in \mathcal{X} \times \mathcal{V}\), where $\mathcal{X} \times \mathcal{V}$ is the problem region of evaluation of \textsc{BesselK}
\State \textbf{Given:} $b$ number of bins for numerical integration
\BeginBox[draw=black,dashed]
\If{\(0 \leq x < 0.1\) \textbf{and} \(\nu \in \mathcal{V}\)}
    \State Set $M = \lfloor \nu + 0.5 \rfloor$ and use $\mu = \nu - M$
    \State Use Equation \eqref{eq:temme_initpq} and Equation \eqref{eq:temme_initf} to initialize $p_0, q_0, f_0$ for smoothness $\mu$
    \State Set $15000$ instead of $\infty$ for the sum of Temme's series for $K_{\mu}(x)$ in Equation \eqref{eq:temme_main}
    \State Use the recurrence relation 
    $$K_{\eta+1}(x) = \left(\frac{2\eta}{x}\right)K_\eta(x) + K_{\eta-1}(x)$$ until $\eta + 1 = \nu$ to acquire $K_{\nu}(x)$
\EndBox
\BeginBox[draw=red,dashed]
\Else{ \(x \in \mathcal{X} \backslash [0, 0.1)\) \textbf{and} \(\nu \in \mathcal{V}\)}
   % \State 
    \State $t_0 \gets \text{LB}$ (fixed to $0$)
    \State $t_1 \gets \text{UB}$ (empirical upper bound for all $(x, \nu)$)
    \State $t_{\text{lmax}} \gets \max_{i = 0, \ldots, b}t_i$
    \State Use \begin{align*}
       & \log K_{\nu}(x) \gets  \ g_{\nu,x}(t_{\text{lmax}}) + \\ 
& \log \sum_{m=0}^{b} h \exp \Big\{ c_m \big( g_{\nu,x}(t_m) - g_{\nu,x}(t_{\text{lmax}}) \big) \Big\},
    \end{align*} where $h, t_m, c_m$ are given in Equation~\eqref{eq:refined} and $g_{\nu,x}(\cdot)$ given in Equation \eqref{eq:logintegrand}
    % Use Equation \eqref{eq:refined} to compute \(\textsc{logBesselK}(x, \nu)\)
    \State $K_{\nu}(x) \gets \exp(\log(K_{\nu}(x)))$
\EndBox
% \Else
%     \State Return 0
\EndIf
\State \textbf{Output:} \(\textsc{BesselK}(x, \nu) \gets K_{\nu}(x)\)
\end{algorithmic}
\label{alg:refined}
\end{algorithm}

Algorithm~\ref{alg:tile_matern} presents the pseudocode for the CUDA algorithm used to generate a single tile of the covariance matrix based on the Matérn covariance function and the \textsc{BesselK} function, using the refined algorithm. Using the \emph{StarPU} runtime system, the matrix is partitioned into smaller tiles and each tile is assigned to a different GPU to dynamically compute the full covariance matrix, while handling the data transfer from host-to-device and device-to-host. Memory allocation is handled via \emph{StarPU} to optimize data transfer performance between the CPU (host) and the GPU. In lines 5-13, each GPU thread processes a single tile value to compute the \textsc{BesselK} function based on the corresponding \(x\) value, as described in Algorithm~\ref{alg:refined}.





\begin{algorithm}
\small
\caption{GPU Single-Tile Mat\'{e}rn Covariance Generation Algorithm}
\begin{algorithmic}[1]
% \State Split the matrix which are stored in a contiguous data chunk into $mc * nc$ number of submatrices of dimension $m \times n$, $mc$ number of submatrices of dimension $m \times q$, $nc$ number of submatrices of dimension $p \times n$, and a submatrix of dimension $p \times q$
% \Ensure $N = mc \times m + p = nc \times n + q$
\Function{GenerateMat\'{e}rnCovariance}{$\mathbf{\ell}^1_x$, $\mathbf{\ell}^1_y$, $\mathbf{\ell}^2_x$, $\mathbf{\ell}_y^2$, $\sigma$, $\beta$, $\nu, m, n$} where $\ell^1$  and $\ell^2$ represents location vectors.
    \State Initialize CUDA grid with dimension $(m + 8- 1 // m) * (n + 8 - 1 // n)$ and block dimensions $8 \times 8$ (which means $64$ threads for each block)
    \State Use \texttt{starpu\_malloc()} for efficient CUDA memory allocation, enabling fast CPU-GPU and GPU-GPU transfers.
    \State Copy location vectors and parameters to GPU memory
    \For{each thread $(i,j)$ in parallel}
        \If{$i < m$ and $j < n$}
            \State $c \gets \sigma^2 / (2^{\nu-1}\Gamma(\nu))$
            \State $d \gets \sqrt{(\mathbf{\ell}^2_x[j] - \mathbf{\ell}^1_x[i])^2 + (\mathbf{\ell}^2_y[j] - \mathbf{\ell}^1_y[i])^2)}$
            \State $r \gets d/\beta$
            \If{$r = 0$}
                \State $A[i + j\times m] \gets \sigma^2$
            \Else
                \State $A[i + j\times m] \gets c \cdot r^{\nu} \cdot \textsc{BesselK}(r, \nu)$
            \EndIf
        \EndIf
    \EndFor
    \State Transfer back the generated submatrices and parameters to CPU
    \State Synchronize CUDA stream
\EndFunction
\State Free the CUDA memory and destroy the CUDA stream
\end{algorithmic}
\label{alg:tile_matern}
\end{algorithm}

\subsection{GPU Optimizations on Algorithm \ref{alg:refined}}
Algorithm \ref{alg:refined} is written for CUDA where threads are collected in blocks of threads; 64 threads in our case gained good performance. The threads in a block are collected in warps of 32 threads, where each warp is used for the Single Instruction Multiple Threads (SIMT) execution model~\cite{bbeth}. When naively parallelized on GPUs, the \(i\)-th thread computes \(K_{\nu}(x)\) for the \(i\)-th input value \((x_i, \nu_i)\).

Specifically, the covariance matrix we target involves computing the modified Bessel function for \(r/\beta\), where \(r\) represents the distance between two spatial locations, and \(\beta\) is a fixed parameter during a single generation operation. In our implementation, we ordered the spatial locations using Morton's ordering~\cite{ordering2018} to ensure that distances within the same block remain close and consistently follow a single branch of the if-else condition in the CUDA implementation since we are operating in a tile-based manner.


\section{Experimental Results}
In this section, we evaluate the implementation of the refined \textsc{BesselK} algorithm on the GPU within the \emph{ExaGeoStat} software, focusing on its impact in accelerating the computation of individual matrix elements when millions to trillions of elements are computed. The experiments focus on four main objectives: (1) Assessing the accuracy of computing \textsc{BesselK} for specific $x$ and $\nu$ values; (2) Analyzing the overall accuracy of spatial statistical modeling using a synthetic dataset, emphasizing our GPU-based implementation for matrix generation across different spatial correlation levels; (3) Validating the accuracy of the proposed implementation in modeling real datasets within the context of climate and weather applications; (4) Evaluating the performance of full covariance matrix generation within \emph{ExaGeoStat}, using both single and multiple GPUs.

The implementation associated with this research, which functions independently from the \emph{ExaGeoStat} framework and focuses exclusively on optimizing \textsc{BesselK} for the CUDA architecture, is available for public access at: \url{https://github.com/stsds/CuBesselK}


\subsection{Experimental Configuration}
The results for the overall accuracy of the spatial statistical modeling (Figures \ref{fig:mle-combined} -- \ref{fig:iternbins}) are generated using NVIDIA GV100 32GB with Intel Cascade Lake 256GB CPU, each with 100 repetitions. The real dataset application follows the same settings. Performance assessments (Figures \ref{fig:v100comparison} -- \ref{fig:scaling}) use five repetitions for benchmarking. We use NVIDIA GV100 32GB and NVIDIA A100 80GB GPUs with Intel Cascade Lake CPU. For \emph{ExaGeoStat}, we use CUDA 11.8, gcc 11.2.1, CMake 3.24.2, OpenMPI 4.1.4, OneAPI 2022.3, and StarPU 1.3.11.


\subsection{Relative Error Analysis Against Mathematica}

For accuracy assessment, existing work on implementing the \textsc{BesselK} function often uses Mathematica's \texttt{BesselK[nu, x]} as the benchmark. In this study, we compare the performance of the GSL library, Takekawa's algorithm, and the refined algorithm against Mathematica. The relative errors for various values of \( x \) and \( \nu \) are presented as a heatmap. The heatmap spans the region \( (\nu, x) \in [0.001, 20] \times [0.001, 140] \), which adequately covers the parameter range relevant to Gaussian processes.


Figure~\ref{fig:heatmap1} presents three heatmaps for the target region using the three implementations.  As shown, the relative errors of the \textsc{LogBesselK} in Takekawa's algorithm are larger than those of GSL and the refined algorithm, with a maximum error of \( 6.54807 \). This is mainly due to the uncovered region where \( x < 0.1 \). GSL and the refined algorithm exhibit very close relative errors, with values of \( 1.67896 \) and \( 1.65466 \), respectively. %\red{Figure~\ref{fig:figure4} displays the accuracy difference between GSL and our refined algorithm, with red indicating superior refined accuracy, white showing identical results, and blue favoring GSL. The predominant white regions demonstrate that both algorithms achieve comparable accuracy across most parameter combinations.}
To highlight the advantages of our method over Takekawa's method for \( x \leq 0.1 \), we further zoom into the region \( (\nu, x) \in [0.001, 5] \times [0.001, 0.1] \) in Figure~\ref{fig:heatmap2}. %Darker regions in the heatmap indicate higher errors.

\begin{figure}[!hbt]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/logk_prec_takekawa.png} % Replace with your figure file
        \caption{Takekawa's algorithm.}
        \label{fig:figure1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/logk_prec_refined.png} % Replace with your figure file
        \caption{Refined algorithm.}
        \label{fig:figure2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/logk_prec_gsl.png} % Replace with your figure file
        \caption{GNU Scientific Library.}
        \label{fig:figure3}
    \end{subfigure}
 %   \hfill
  %  \begin{subfigure}[b]{0.24\textwidth}
   %     \centering
    %    \includegraphics[width=\textwidth]%{figs/comp_gsl_refined_all.png} % Replace with your figure file
        %\caption{Accuracy comparison GSL vs. Refined algorithm.}
        %\label{fig:figure4}
    %\end{subfigure}
    \caption{\textsc{LogBesselK} accuracy comparisons using heatmap for $(\nu, x) \in [0.001, 20] \times [0.001, 140]$.}
    \label{fig:heatmap1}
\end{figure}

\begin{figure}[!hbt]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/logk_prec_takekawanew.png} % Replace with your figure file
        \caption{Takekawa's algorithm.}
        \label{fig:figure5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/logk_prec_refinednew.png} % Replace with your figure file
        \caption{Refined algorithm. }
        \label{fig:figure6}
    \end{subfigure}
    \hfill
        \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/logk_prec_gslnew.png} % Replace with your figure file
        \caption{GNU Scientific Library.}
        \label{fig:figure7}
    \end{subfigure}
    \hfill
    % \begin{subfigure}[b]{0.24\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figs/comp_gsl_refined_zoom.png} % Replace with your figure file
    %     \caption{Accuracy comparison GSL vs. Refined algorithm.}
    %     \label{fig:figure8}
    % \end{subfigure}
    \caption{\textsc{LogBesselK} accuracy comparisons using heatmap for $(\nu, x) \in [0.001, 5] \times [0.001, 0.1]$.}
    \label{fig:heatmap2}
\end{figure}

\subsection{Spatial Data Modeling Accuracy within ExaGeoStat}

 %These three settings were chosen because they effectively represent the typical spatial correlation patterns in real-world spatial data analysis.

 Estimating the relative error of the Bessel function for each pair $(x, \nu)$ is crucial to evaluate the effectiveness of a given implementation. However, beyond this, the function \textsc{BesselK}, which is buried within a kernel to generate a matrix, can affect the accuracy of subsequent matrix operations if the accumulated error from different calculations becomes significant. We integrate our implementation into \emph{ExaGeoStat} to generate the Mat\'{e}rn kernel covariance matrix to assess this point. We use the modeling process in \emph{ExaGeoStat}, specifically MLE with gradient-free optimization, which requires multiple iterations to converge and generate a set of estimates of the parameters $\sigma^2$, $\beta$ and $\nu$ that effectively describe the underlying field.

%  \begin{figure}[!hbt]
%     \centering
%     \begin{subfigure}[b]{0.47\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/weak_MLE.pdf} % Replace with your figure file
%         \caption{Weak correlation $\beta=0.03$, when $\nu=0.5$.}
%         \label{fig:weak0.5}
%     \end{subfigure}
%  \hfill
%     % \vline
%     \hfill
%     \begin{subfigure}[b]{0.47\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/medium_MLE.pdf} % Replace with your figure file
%         \caption{Medium correlation $\beta=0.1$, when $\nu=0.5$.}
%         \label{fig:medium0.5}
%     \end{subfigure}
%     \vfill
%     \begin{subfigure}[b]{0.47\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/strong_MLE.pdf} % Replace with your figure file
%         \caption{Strong correlation $\beta=0.3$, when $\nu=0.5$.}
%         \label{fig:strong0.5}
%     \end{subfigure}
%     \caption{Boxplots of MLE optimization results over 100 replicas using GSL on CPU and the refined algorithm on GPU when  $\nu=0.5$, comparing estimates of parameters (\(\sigma^2\), \(\beta\), \(\nu\)) and iteration counts. Red dashed lines: true values of parameters.}
%     \label{fig:mle0.5}
% \end{figure}

The accuracy of the MLE operation is typically evaluated using simulations in which synthetic data are generated~\cite{salvana2022parallel}. 
%  In the simplest case, this involves $(x, y, z)$, where $(x, y)$ represents spatial locations, and $z$ denotes the measurement value at $(x, y)$.
We use synthetic data generated within a 2D space, as described in~\cite{sun2016statistically}. We assess the accuracy of the modeling results across three representative scenarios commonly encountered in spatial statistics. These scenarios were characterized by different levels of spatial correlation: weak ($\beta = 0.03$ and $\beta = 0.025$), medium ($\beta = 0.1$ and $\beta = 0.075$) and strong ($\beta = 0.3$ and $\beta = 0.2$), for two different levels of smoothness, rough field ($\nu = 0.5$) and smooth field ($\nu = 1$), respectively. The variance was fixed at $\sigma^2 = 1$ in all experiments.

 %  Estimating the relative error of the Bessel function for each pair $(x, \nu)$ is crucial to evaluating the effectiveness of a given implementation. Beyond this, however, the \textsc{BesselK} function, which is buried within a kernel to generate a matrix, can affect the accuracy of subsequent matrix operations if the accumulated error from different calculations becomes significant. To assess this point, we integrate our implementation into \emph{ExaGeoStat} to generate a Mat\'{e}rm kernel matrix in parallel. We use the modeling process in \emph{ExaGeoStat}, specifically MLE with gradient-free optimization, which requires multiple iterations to converge and generate a set of estimates of the parameters $\sigma^2$, $\beta$, and $\nu$ that effectively describe the underlying field.

%  The accuracy of the MLE operation is typically evaluated using Monte Carlo simulations in which synthetic spatial data are generated~\cite{salvana2022parallel}. In the simplest case, this involves $(x, y, z)$, where $(x, y)$ represents spatial locations, and $z$ denotes the measurement value at $(x, y)$. We use synthetic data generated at irregular locations within a two-dimensional space, as described in~\cite{sun2016statistically}. We evaluated the accuracy of the modeling results across three representative scenarios commonly encountered in spatial statistics. These scenarios were characterized by different levels of spatial correlation: weak  ($\sigma^2 = 1, \beta = 0.03, \nu = 0.5$), medium  ($\sigma^2 = 1, \beta = 0.1, \nu = 0.5$), and strong  ($\sigma^2 = 1, \beta = 0.3, \nu = 0.5$).

Figures \ref{fig:weak0.5} -- \ref{fig:strong1-16} provide a comparison of parameter estimation using MLE, taking advantage of both GSL and the proposed refined algorithm. The analysis, conducted across three correlation levels (weak, medium, and strong), includes boxplots that illustrate the estimation of three key parameters (\(\sigma^2\), \(\beta\), \(\nu\)) and iteration counts.

\begin{figure*}[ht]
    \centering
    % Left column - nu=0.5 cases
    \begin{minipage}[t]{0.48\textwidth}
        \begin{subfigure}[b]{\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figs/weak_MLE.pdf}
            \caption{Weak correlation $\beta=0.03$, when $\nu=0.5$.}
            \label{fig:weak0.5}
        \end{subfigure}
        \vfill       
        \begin{subfigure}[b]{\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figs/medium_MLE.pdf}
            \caption{Medium correlation $\beta=0.1$, when $\nu=0.5$.}
            \label{fig:medium0.5}
        \end{subfigure}
 \vfill 
        \begin{subfigure}[b]{\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figs/strong_MLE.pdf}
            \caption{Strong correlation $\beta=0.3$, when $\nu=0.5$.}
            \label{fig:strong0.5}
        \end{subfigure}
    \end{minipage}
    \hfill
    % Right column - nu=1 cases
    \begin{minipage}[t]{0.48\textwidth}
        \begin{subfigure}[b]{\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figs/weak_MLE_nuone.pdf}
            \caption{Weak correlation $\beta=0.025$, when $\nu=1$.}
            \label{fig:weak1}
        \end{subfigure}
 \vfill 
        \begin{subfigure}[b]{0.93\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figs/medium_MLE_nuone.pdf}
            \caption{Medium correlation $\beta=0.075$, when $\nu=1$.}
            \label{fig:medium1}
        \end{subfigure}
 \vfill 
        \begin{subfigure}[b]{0.985\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figs/strong_MLE_nuone_nbins16.pdf}
            \caption{Strong correlation $\beta=0.2$, when $\nu=1$.}
            \label{fig:strong1-16}
        \end{subfigure}
    \end{minipage}
    \caption{Boxplots of MLE optimization results over 100 replicas comparing GSL (CPU) and refined algorithm (GPU). Left column: $\nu=0.5$ cases. Right column: $\nu=1$ cases. All plots show parameter estimates ($\sigma^2$, $\beta$, $\nu$) and iteration counts with red dashed lines indicating true values of parameters.}
    \label{fig:mle-combined}
\end{figure*}

In the weak correlation scenario (Figure \ref{fig:weak0.5}), both implementations achieve comparable accuracy in parameter estimation. Furthermore, the average iteration count for the refined algorithm is similar to that of GSL, indicating a comparable computational efficiency. The medium correlation scenario (Figure \ref{fig:medium0.5}) demonstrates increased variability in parameter estimation. Both implementations maintain comparable accuracy, but the refined algorithm exhibits slightly more consistent estimates across all parameters. For strong spatial correlation (Figure \ref{fig:strong0.5}), the most challenging scenario, both implementations exhibit wider parameter distributions, reflecting the increased difficulty of the estimation process. The refined algorithm achieves comparable accuracy and number of iterations.

% \begin{figure}[!hbt]
%     \centering
%     \begin{subfigure}[b]{0.47\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/weak_MLE_nuone.pdf} % Replace with your figure file
%         \caption{Weak correlation $\beta=0.025$, when $\nu=1$.}
%         \label{fig:weak1}
%     \end{subfigure}
%  \hfill
%     % \vline
%     \hfill
%     \begin{subfigure}[b]{0.47\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/medium_MLE_nuone.pdf} % Replace with your figure file
%         \caption{Medium correlation $\beta=0.075$, when $\nu=1$.}
%         \label{fig:medium1}
%     \end{subfigure}
%     \vfill
%     \begin{subfigure}[b]{0.47\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/strong_MLE_nuone_nbins16.pdf} % Replace with your figure file
%         \caption{Strong correlation $\beta=0.2$, when $\nu=1$.}
%         \label{fig:strong1-16}
%     \end{subfigure}
%     \caption{Boxplots of MLE optimization results over 100 replicas using GSL on CPU and the refined algorithm on GPU when  $\nu=1$ , comparing estimates of parameters (\(\sigma^2\), \(\beta\), \(\nu\)) and iteration counts. Red dashed lines: true values of parameters.}
%     \label{fig:mle1.0}
% \end{figure}

In the smoother case where $\nu = 1.0$, Figures \ref{fig:weak1}, \ref{fig:medium1}, and \ref{fig:strong1-16} demonstrate that the refined algorithm achieves an accuracy similar to GSL. However, it shows a slightly higher bias between the median and ground truth in the strong correlation scenario, which remains within an acceptable range. This is expected, as a larger $\nu$ makes the \textsc{BesselK} approximation more sensitive to the number of bins used for numerical integration. However, the refined algorithm requires significantly fewer iterations than GSL. We also experimented with a strong correlation scenario using $b = 40$ instead of $16$. The results are shown in Figure \ref{fig:strong1-40}. The refined algorithm achieves a level of accuracy similar to that of GSL, and the bias in parameter estimation is eliminated.
\begin{figure}[!hbt]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/strong_MLE_nuone_nbins40.pdf}
    \caption{Boxplots of MLE optimization results over 100 replicas using GSL on CPU and the refined algorithm on GPU when  $\nu=1$ and using $b=40$ bins.}
    \label{fig:strong1-40}
\end{figure}
These results indicate that the refined algorithm largely preserves accuracy compared to GSL while achieving improved computational efficiency. Despite requiring a similar number of optimization iterations, each iteration is significantly faster, as demonstrated in the next section. Furthermore, consistent performance across varying correlation strengths highlights the robustness of the refined algorithm for spatial statistics and Gaussian process applications. By comparing Figure \ref{fig:strong1-16} and Figure \ref{fig:strong1-40}, it can be concluded that adjusting the number of bins inherently involves a trade-off between accuracy and computational efficiency. Users are advised to carefully consider this balance, particularly in cases where $\nu$ is larger and the spatial correlation is strong.


\subsection{Accuracy Across Different Numbers of Bins}
\begin{figure}[!hbt]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/MLE_boxplot_weak0.5.png} % Replace with your figure file
        \caption{Weak correlation $\beta=0.03$.}
        \label{fig:mleweak0.5}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/MLE_boxplot_medium0.5.png} % Replace with your figure file
        \caption{Medium correlation $\beta=0.1$.}
        \label{fig:mlemedium0.5}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/MLE_boxplot_strong0.5.png} % Replace with your figure file
        \caption{Strong correlation $\beta=0.3$.}
        \label{fig:mlestrong0.5}
    \end{subfigure}
    \caption{Parameter estimation over 100 replicas using the refined algorithm was evaluated for varying bin counts to estimate \(\sigma^2\), \(\beta\), and \(\nu\) across different correlation levels with problem size $51{,}076$. Red dashed lines: true values of parameters.}
    \label{fig:mlenbins}
\end{figure}
%STOP..... Sameh
The numerical approximation accuracy of the \textsc{BesselK} function, as defined in Equation \eqref{eq:refined}, is significantly influenced by the discretization parameter $b$ (number of bins). Following the previous discovery, we vary the number of bins (\(b = 16, 40,\text{ and } 128\)) to assess its impact on the estimated parameters under weak, medium, and strong correlation levels when $\nu = 0.5$ and sample size $N = 51{,}076$, as illustrated in Figure~\ref{fig:mlenbins}. Additionally, we analyze the effect of the number of bins on the iteration count required for convergence across the three correlation levels, as shown in Figure~\ref{fig:iternbins}.

Although a smaller number of bins might introduce larger approximation errors given fixed integration bounds, this does not significantly affect the MLE procedure when $\nu$ is small. 
%\st{This assertion is supported by MLE algorithms that typically employ convergence tolerances $10^{-7}$. Therefore, the choice of bin count plays a less critical role in the overall MLE parameter estimation than expected.}
Through extensive numerical experiments (Figures \ref{fig:mle-combined}, \ref{fig:strong1-40}, \ref{fig:mlenbins}, and \ref{fig:iternbins}), we have empirically demonstrated that parameter estimation remains robust even with a reduced number of bins, supporting our hypothesis that accurate estimation can be achieved without requiring fine-grained discretization, although discretization should be chosen based on the specific application and desired accuracy level according to the aforementioned analysis. Overall, our approach suggests a practical balance between computational efficiency and numerical accuracy in evaluating the Mat\'{e}rn covariance function.

%\subsubsection{CUDA BesselK implementation}

%\begin{itemize}
 %   \item Accuracy: \textbf{\textcolor{red}{The numerical experiments showing that intervals $= 40$ is a relatively good approximation.}}
 %   \item Runtime:
%\end{itemize}
\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/iter_boxplots_weak0.5.png}
        \captionsetup{justification=centering}
        \caption{Weak correlation \\($\beta=0.03$).}
        \label{fig:iterweak}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/iter_boxplots_medium0.5.png}
        \captionsetup{justification=centering}
        \caption{Medium correlation \\($\beta=0.1$).}
        \label{fig:itermedium}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/iter_boxplots_strong0.5.png}
        \captionsetup{justification=centering}
        \caption{Strong correlation \\($\beta=0.3$).}
        \label{fig:iterstrong}
    \end{subfigure}
    \caption{The number of iterations for MLE optimization over 100 replicas using the refined algorithm was evaluated with \(b = 16\), \(40\), and \(128\) bins across different correlation levels with problem size $51{,}076$, when $nu=0.5$.}
    \label{fig:iternbins}
\end{figure}

%\section{Spatial Data Application}
\subsection{Wind Speed Application}

For real data analysis in this study, we use a wind speed dataset of 1M locations generated using the WRF-ARW model for the Arabian Peninsula. The model has a horizontal grid resolution of \(5\) km and \(51\) vertical levels, extending to a maximum altitude of \(10\) hPa. The dataset spans a geographical region from \(20^\circ\)E to \(83^\circ\)E longitude and \(5^\circ\)S to \(36^\circ\)N latitude, covering \(37\) years of daily records. Each file contains hourly wind speed measurements across \(17\) atmospheric layers. Our analysis specifically focused on wind speed data from September 1, 2017, at 00:00 AM, examining measurements at 10 meters above ground level (layer 0). To address the skewed distribution of wind speeds, we plot the square root of wind speed against longitude and latitude, as shown in Figure~\ref{fig:windres}. 
\begin{figure}[!hbt]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/windres.png}
    \caption{Residuals of a wind speed dataset of 1M locations in the Middle East region.}
    \label{fig:windres}
\end{figure}

Starting with an initial dataset of 1M locations, we randomly sampled $160{,}000$ locations for modeling and $25{,}000$ for testing. To improve numerical stability, the location coordinates are preprocessed through normalization. Given the length and width of a squared region $\ell_1, \ell_2$: (1) Compute the scaling factor $\ell := \max(\ell_1, \ell_2)$. (2) Rescale location $(x_0, y_0)$  to $(\{x_0 - \min(x_0)\}/\ell, \{y_0 - \min(y_0)\}/\ell)$. This transformation maps the spatial coordinates into a unit square $[0,1] \times [0,1]$, which helps mitigate numerical issues in subsequent computations.

Table~\ref{tab:comparison_table} presents the estimated parameters and the final log-likelihood values obtained by the GSL and the refined algorithms. The experiments were carried out on a 40-core Intel Cascade Lake CPU with $383$GB of memory and a $80$GB single A100 GPU. Both methods estimated nearly identical parameters, achieving nearly the same maximum log-likelihood value (llh), and produced the same mean square prediction error (MSPE).

However, using the GSL library required $596.61$ minutes of execution time, while the refined algorithm reduced this to $299.89$ minutes. For fairness, only the matrix generation was performed either by the CPU (GSL) or the GPU (refined algorithm), while all other operations were executed on the GPU.



%\usepackage{graphicx} % Include this line in your preamble

\begin{table}[h!]
\caption{Comparison of GSL and refined algorithms on \( 160\text{K} \) random locations from the wind speed dataset.}
\label{tab:comparison_table}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
     & ($\sigma^2$, $\beta$, $\nu$) & llh    &MSPE       & Time\\ \
          & estimates & value    &       & (min) \\ \hline
GSL        & (2.505,      0.178,     0.426)         & 6984.660     &   0.037188    & 596.61       \\ \hline
Refined    & (2.510, 0.179, 0.426)    &  6984.598            &  0.037186    & 299.89       \\ \hline
\end{tabular}
}
\end{table}


%2.51037911,0.17848840,0.42616764
%2.50503967,0.17851527,0.425



\subsection{Performance Assessment}
% \begin{figure*}[!hbt]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=0.8\textwidth]{figs/v100_1.png} % Replace with your figure file
%         \caption{40-core Intel Cascade Lake + 32GB NVIDIA V100 GPU.}
%         \label{fig:v100_1gpu}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.51\textwidth}
%         \centering
%         \includegraphics[width=0.75\textwidth]{figs/v100_2.png} % Replace with your figure file
%         \caption{40-core Intel Cascade Lake + 2$\times$32GB NVIDIA V100 GPUs.}
%         \label{fig:v100_2gpu}
%     \end{subfigure}
% \caption{Performance comparison for single-GPU and dual-GPU (V100) configurations with CPU.}

%     \label{fig:v100comparison}
% \end{figure*}

% \begin{figure*}[!hbt]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=0.8\textwidth]{figs/a100_1.png} % Replace with your figure file
%         \caption{{\color {red}64-core AMD EPYC} + 80GB NVIDIA A100 GPU.}
%         \label{fig:a100_1gpu}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.51\textwidth}
%         \centering
%         \includegraphics[width=0.75\textwidth]{figs/a100_2.png} % Replace with your figure file
%         \caption{{\color {red}64-core AMD EPYC} + 2$\times$80GB NVIDIA A100 GPUs.}
%         \label{fig:a100_2gpu}
%     \end{subfigure}
% \caption{Performance comparison for single-GPU and dual-GPU (A100) configurations with CPU.}
%     \label{fig:a100comparison}
% \end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% START THE MERGED FIG 3 and FIG 4




For single-node performance evaluation, we use a 40-core Intel Cascade Lake CPU to run the GSL version and generate an entire covariance matrix within the \emph{ExaGeoStat} framework for varying numbers of locations. Additionally, we use 1 to 4 V100 (32 GB) or A100 (80 GB) GPUs on a single node to evaluate the implementation of the refined algorithm.

\begin{figure}[!hbt]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/v100_new.png}
    \caption{Execution time for generating an \( N \times N \) covariance matrix using GSL and the refined algorithm on a 40-core Intel Cascade Lake CPU and 1-4 NVIDIA V100 GPUs.}
    \label{fig:v100comparison}
\end{figure}


In the single-GPU (V100) configuration (Figure~\ref{fig:v100comparison}), tests on datasets with a number of locations ranging from \(57{,}137\) to \(202{,}500\) demonstrated that the refined algorithm achieved a $1.16$X speedup compared to the CPU-only implementation. The performance improvements became increasingly pronounced with additional GPUs, reaching a $4.44$X speedup with four GPUs. For NVIDIA A100 GPUs, the single-GPU configuration (Figure~\ref{fig:a100comparison}), tested on datasets with a number of locations ranging from \(57{,}137\) to \(99{,}225\), showed a $2.68$X speedup over the CPU-only implementation. This improvement scaled significantly with additional GPUs, achieving a $12.62$X speedup with four GPUs. These results highlight the effectiveness of GPU acceleration with the refined algorithm for matrix generation, with benefits becoming more pronounced when leveraging multiple GPUs and larger datasets.

\begin{figure}[!hbt]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/a100_new.png}
    \caption{Mean execution time (s) for generating an \( N \times N \) covariance matrix using GSL and the refined algorithm on a 40-core Intel Cascade Lake CPU and 1-4 NVIDIA A100 GPUs.}
    \label{fig:a100comparison}
\end{figure}

%%%%%%
\begin{figure}[!hbt]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/full_MLE.png}
    \caption{Comparison of overall MLE execution time between the GSL library and the refined algorithm on a V100 GPU across various problem sizes. The reported time accounts for multiple iterations of the log-likelihood function, including matrix generation and all associated linear algebra operations within the MLE process~\cite{abdulah2018exageostat}.}
    \label{fig:full_MLE_perf}
\end{figure}

Although the single-matrix generation time highlights the efficiency of the refined algorithm on GPUs compared to GSL, its impact becomes even more significant in operations like MLE. During MLE, the matrix generation function is invoked multiple times while optimizing the log-likelihood function until convergence, amplifying the benefits of the refined algorithm. Figure~\ref{fig:full_MLE_perf} illustrates the performance of executing the full MLE process on five different problem sizes, comparing the GSL implementation with the refined algorithm using up to four A100 GPUs.  Due to the long execution time of the GSL function, the full MLE process was not estimated for \(N = 82,521\) and \(N = 99,225\). For \(N = 71,289\), the complete MLE process took 186.21 minutes with GSL (CPU) and $78.87$, $49.36$, $30.01$, and $26.38$ minutes using the refined algorithm on 1 GPU, 2 GPUs, 3 GPUs, and 4 GPUs, respectively.


%CPU = 11172.81    186.2135
%1 GPU = 4732.42    78.873666666666667
%2 GPU = 2961.32  49.355333333333333
%3 GPU = 1800.346  30.005766666666667
%4 GPU = 1582.96   26.382666666666667


Figure~\ref{fig:scaling} shows the scalability of the refined algorithm across up to six nodes, each equipped with two V100/A100 GPUs. As available memory increases, GPUs can handle larger problem sizes and the performance scales almost linearly with more GPUs.

\begin{figure}[!hbt]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/scaling_perf.png}
    \caption{Mean matrix generation time varying node counts and GPU architectures (V100 and A100).}
    \label{fig:scaling}
\end{figure}





\section{Conclusion}
We present a highly efficient GPU-accelerated implementation of the modified Bessel function of the second kind (\textsc{BesselK}) using CUDA, effectively addressing critical computational bottlenecks in Gaussian processes and various other scientific applications. This approach can also be extended to other modeling frameworks, including non-Gaussian processes, where \textsc{BesselK} functions frequently arise. It integrates Temme’s series expansion for small input values and a refined version of Takekawa's integral-based approach for larger values, ensuring accuracy and computational efficiency across a reasonable parameter space.
Incorporating this optimized algorithm into the \emph{ExaGeoStat} framework demonstrates significant performance improvements in generating covariance matrices for spatial data modeling. The GPU-based implementation achieved substantial speedups compared to traditional CPU-based methods while maintaining high numerical accuracy, validated through synthetic datasets and real-world climate data. These improvements are particularly notable in large-scale applications that require massive matrix computations. Our work improves the computational capabilities of \textsc{BesselK} evaluations on GPUs and establishes the foundation for integrating such optimizations into other domains based on these functions. Future work includes extending the implementation to support the evaluation of derivatives of \textsc{BesselK}, enabling gradient-based optimization techniques, as well as transitioning from single-threaded to a multi-threaded evaluation of $K_{\nu}(x)$ to further improve performance.

\begin{thebibliography}{00}
\bibitem{sharma2007damped}
Sharma, Kal Renganathan, "Damped wave conduction and relaxation in cylindrical and spherical coordinates," \textit{Journal of Thermophysics and Heat Transfer}, vol. 21, no. 4, pp. 688--693, 2007.
\bibitem{karamian2022role}
Karamian, Asghar, Jazi, Bahram, and Najari, Samaneh, "The Role of Ordinary Bessel and Hankel Functions in Simulation of Plasma Valve Mechanism in a Loss-Free Metallic Cylindrical Waveguide," \textit{Mathematics Interdisciplinary Research}, vol. 7, no. 4, pp. 343--355, 2022, University of Kashan.
\bibitem{martin2021quasi}
Martin, Pablo, Rojas, Eduardo, Olivares, Jorge, and Sotomayor, Adrián, "Quasi-Rational Analytic Approximation for the Modified Bessel Function \( I_1(x) \) with High Accuracy," \textit{Symmetry}, vol. 13, no. 5, p. 741, 2021, MDPI.
% \bibitem{ramakrishnan2023learning}
% Ramakrishnan, Rahul O. and Friedrich, Benjamin M., "Learning run-and-tumble chemotaxis with support vector machines," \textit{Europhysics Letters}, vol. 142, no. 4, p. 47001, 2023, IOP Publishing.
\bibitem{wang2023parameterization}
Wang, Kesen, Abdulah, Sameh, Sun, Ying, and Genton, Marc G., "Which parameterization of the Mat\'{e}rn covariance function?," \textit{Spatial Statistics}, vol. 58, p. 100787, 2023, Elsevier.
\bibitem{bbtakekawa} Takekawa, T., 2022. Fast parallel calculation of modified Bessel function of the second kind and its derivatives. \textit{SoftwareX}, 17, p.100923.
\bibitem{bbtemme} Temme, N.M., 1975. On the numerical evaluation of the modified Bessel function of the third kind. \textit{Journal of Computational Physics}, 19(3), pp.324-337.
\bibitem{amos1974computation}
Amos, Donald E., ``Computation of modified Bessel functions and their ratios,'' \emph{Mathematics of Computation}, vol. 28, no. 125, pp. 239--251, 1974.
\bibitem{olver2009bessel}
F.W.J. Olver and L.C. Maximon, "Bessel Functions," in \textit{NIST Handbook of Mathematical Functions}, Cambridge University Press, 2009, pp. 215--286.
\bibitem{abdulah2018exageostat}
S.~Abdulah, H.~Ltaief, Y.~Sun, M.~G.~Genton, and D.~E.~Keyes, ``ExaGeoStat: A high performance unified software for geostatistics on manycore systems,'' \emph{IEEE Transactions on Parallel and Distributed Systems}, vol.~29, no.~12, pp.~2771--2784, 2018.
\bibitem{weisstein2002modified}
Weisstein, Eric W., "Modified Bessel Function of the Second Kind," \textit{MathWorld -- A Wolfram Web Resource}, Wolfram Research, Inc., 2002. Available at: \url{https://mathworld.wolfram.com/}.
\bibitem{dunster1990bessel}
Dunster, T. Mark, "Bessel Functions of Purely Imaginary Order, with an Application to Second-Order Linear Differential Equations Having a Large Parameter," \textit{SIAM Journal on Mathematical Analysis}, vol. 21, no. 4, 1990, pp. 995--1018, SIAM.
\bibitem{zhukovsky2017solving}
Zhukovsky, K. V., "Solving Evolutionary-Type Differential Equations and Physical Problems Using the Operator Method," \textit{Theoretical and Mathematical Physics}, vol. 190, 2017, pp. 52--68, Springer.
\bibitem{hetnarski2009heat}
Hetnarski, Richard B., and Eslami, M. Reza, "Heat Conduction Problems," \textit{Thermal Stresses--Advanced Theory and Applications}, Springer, 2009, pp. 132--218.
\bibitem{gomez2017physics}
G{\'o}mez-Correa, J. E., Balderas-Mata, S. E., Coello, V., Puente, N. P., Rogel-Salazar, J., and Ch{\'a}vez-Cerda, S., "On the Physics of Propagating Bessel Modes in Cylindrical Waveguides," \textit{American Journal of Physics}, vol. 85, no. 5, 2017, pp. 341--345, AIP Publishing.
\bibitem{pogany2013bessel}
Pog{\'a}ny, Tibor K., "Bessel--Sampling Restoration of Stochastic Signals," \textit{Acta Polytechnica Hungarica}, vol. 10, no. 7, 2013, pp. 7--19.
\bibitem{albahar2021robust}
AlBahar, Areej, Kim, Inyoung, and Yue, Xiaowei, "A Robust Asymmetric Kernel Function for Bayesian Optimization, with Application to Image Defect Detection in Manufacturing Systems," \textit{IEEE Transactions on Automation Science and Engineering}, vol. 19, no. 4, 2021, pp. 3222--3233, IEEE.
\bibitem{yin2020linear}
Yin, Feng, Pan, Lishuo, Chen, Tianshi, Theodoridis, Sergios, Luo, Zhi-Quan Tom, and Zoubir, Abdelhak M., "Linear Multiple Low-Rank Kernel Based Stationary Gaussian Processes Regression for Time Series," \textit{IEEE Transactions on Signal Processing}, vol. 68, 2020, pp. 5260--5275, IEEE.
%\bibitem{thacher1979new}
%H.C. Thacher, "New backward recurrences for Bessel functions," \textit{Mathematics of Computation}, vol. 33, no. 146, pp. 744--764, 1979.
\bibitem{luke2014integrals}
Y.L. Luke, \textit{Integrals of Bessel Functions}. Courier Corporation, 2014.
\bibitem{grosswald1978bessel}
E. Grosswald, \textit{Bessel Polynomials}, vol. 698. Springer, 1978.
\bibitem{carley2013numerical}
M. Carley, "Numerical solution of the modified Bessel equation," \textit{IMA Journal of Numerical Analysis}, vol. 33, no. 3, pp. 1048--1062, 2013.
\bibitem{butcher2007runge}
J. Butcher, "Runge-Kutta methods," \textit{Scholarpedia}, vol. 2, no. 9, p. 3147, 2007.
\bibitem{gramacy2020surrogates}
Gramacy, Robert B. \emph{Surrogates: Gaussian Process Modeling, Design, and Optimization for the Applied Sciences}. Chapman and Hall/CRC, 2020.
\bibitem{leandro2021exploiting}
Nesi, Lucas Leandro, Arnaud Legrand, and Lucas Mello Schnorr. 
"Exploiting system-level heterogeneity to improve the performance of a geostatistics multi-phase task-based application." 
In \emph{Proceedings of the 50th International Conference on Parallel Processing}, pp. 1--10, 2021.
\bibitem{chameleon2024}
The Chameleon project, ``Chameleon: A dense linear algebra library for heterogeneous architectures,'' Dec. 2024. [Online]. Available: \url{https://project.inria.fr/chameleon/}
\bibitem{bosilca2011flexible}
G.~Bosilca, A.~Bouteiller, A.~Danalis, M.~Faverge, A.~Haidar, T.~Herault, J.~Kurzak, J.~Langou, P.~Lemarinier, H.~Ltaief, \emph{et al.}, ``Flexible development of dense linear algebra algorithms on massively parallel architectures with DPLASMA,'' in \emph{2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and PhD Forum}, IEEE, 2011, pp.~1432--1441.
\bibitem{augonnet2009starpu}
C.~Augonnet, S.~Thibault, R.~Namyst, and P.~A.~Wacrenier, ``StarPU: a unified platform for task scheduling on heterogeneous multicore architectures,'' in \emph{Euro-Par 2009 Parallel Processing: 15th International Euro-Par Conference, Delft, The Netherlands, August 25-28, 2009. Proceedings 15}, Springer, 2009, pp.~863--874.
\bibitem{hoque2017dynamic}
R.~Hoque, T.~Herault, G.~Bosilca, and J.~Dongarra, ``Dynamic task discovery in PARSEC: A data-flow task-based runtime,'' in \emph{Proceedings of the 8th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems}, 2017, pp.~1--8.
\bibitem{abdulah2018parallel}
S.~Abdulah, H.~Ltaief, Y.~Sun, M.~G.~Genton, and D.~E.~Keyes, ``Parallel approximation of the maximum likelihood estimation for the prediction of large-scale geostatistics simulations,'' in \emph{2018 IEEE International Conference on Cluster Computing (CLUSTER)}, IEEE, 2018, pp.~98--108.
\bibitem{abdulah2019geostatistical}
S. Abdulah, H. Ltaief, Y. Sun, M. G. Genton, and D. E. Keyes, 
"Geostatistical modeling and prediction using mixed precision tile Cholesky factorization," 
in \textit{Proceedings of the 2019 IEEE 26th International Conference on High Performance Computing, Data, and Analytics (HiPC)}, 
2019, pp. 152--162.
\bibitem{cao2023reducing}
Q.~Cao, S.~Abdulah, H.~Ltaief, M.~G.~Genton, D.~Keyes, and G.~Bosilca, ``Reducing data motion and energy consumption of geospatial modeling applications using automated precision conversion,'' in \emph{2023 IEEE International Conference on Cluster Computing (CLUSTER)}, IEEE, 2023, pp.~330--342.
\bibitem{zhang2024parallel}
X. Zhang, S. Abdulah, J. Cao, H. Ltaief, Y. Sun, M. G. Genton, and D. E. Keyes, 
``Parallel Approximations for High-Dimensional Multivariate Normal Probability Computation in Confidence Region Detection Applications,'' 
in \textit{Proceedings of the 2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
pp. 265--276, IEEE, 2024.
\bibitem{campbell1980temme}
J. B. Campbell, “On Temme’s Algorithm for the Modified Bessel Function of the Third Kind,” \textit{ACM Transactions on Mathematical Software}, vol. 6, no. 4, pp. 581–586, Dec. 1980.
\bibitem{watson1922treatise}
Watson, G. N. (1922). \textit{A treatise on the theory of Bessel functions} (Vol. 2). The University Press.
\bibitem{bbeth} Plesner, A., Sørensen, H.H.B. and Hauberg, S., 2024, May. Accurate Computation of the Logarithm of Modified Bessel Functions on GPUs. In \textit{Proceedings of the 38th ACM International Conference on Supercomputing} (pp. 213-224).
\bibitem{paul2018mathematica}
Paul, C. Abbott.
\textit{Mathematica}. 
In \textit{Revival: The Handbook of Software for Engineers and Scientists (1995)}, pages 926--962. CRC Press, 2018.
\bibitem{ordering2018}
Walker, D.W., 2018. Morton ordering of 2D arrays for efficient access to hierarchical memory. The International Journal of High Performance Computing Applications, 32(1), pp.189-203.
\bibitem{salvana2022parallel}
Salva{\~n}a, Mary Lai O., Sameh Abdulah, Hatem Ltaief, Ying Sun, Marc G. Genton, and David E. Keyes. 
"Parallel space-time likelihood optimization for air pollution prediction on large-scale systems." 
\emph{Proceedings of the Platform for Advanced Scientific Computing Conference}, 2022, pp. 1--11.
\bibitem{sun2016statistically}
Sun, Ying, and Michael L. Stein. "Statistically and computationally efficient estimating equations for large spatial datasets."
\emph{Journal of Computational and Graphical Statistics}, vol. 25, no. 1, 2016, pp. 187--208. Taylor \& Francis.



%%%%%%%%%%



%\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
%\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
%\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
%\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
%\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
%\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
%\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.

%\bibitem{bbgsl} Gough, B., 2009. GNU scientific library reference manual. Network Theory Ltd.
%\bibitem{bbmma} Weisstein, E.W., 2002. Modified Bessel function of the first kind. https://mathworld. wolfram.com/.



%\bibitem{geng2023gpu}
%Geng, Z., Abdulah, S., Ltaief, H., Sun, Y., Genton, M. G., \& Keyes, D. E. (2023). 
%GPU-accelerated dense covariance matrix generation for spatial statistics applications.



















\end{thebibliography}
\vspace{12pt}

\end{document}
