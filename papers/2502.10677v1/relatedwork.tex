\section{Related Work}
\subsection{Object Counting}
Object counting plays a crucial role in applications such as public safety, administration, and labor efficiency. Traditional methods~\cite{ranjan2022exemplar, shi2022represent, yang2021class, you2023few} are restricted to fixed categories and require retraining when new categories are introduced. In contrast, class-agnostic counting~\cite{lu2019class, gong2022class, nguyen2022few, lin2024gramformer, du2023domain} offers more flexible solutions, supporting scenarios with limited data and enabling few-shot, reference-less, and zero-shot counting methods.

\vspace{-10pt}
\paragraph{Few-shot object counting} addresses scenarios with limited annotated data. GMN~\cite{lu2019class} formulates class-agnostic counting as a matching task, leading to FamNet~\cite{ranjan2021learning}, which incorporates ROI Pooling. BMNet~\cite{shi2022represent} introduces a bilinear matching network to refine similarity assessments. LOCA~\cite{djukic2023low} enhances feature representation and exemplar adaptation, while CounTR~\cite{liu2022countr} uses transformers to scale counting tasks. CACViT~\cite{WangX0024} integrates Vision Transformers (ViT) into object counting for improved performance.

\vspace{-10pt}
\paragraph{Zero-shot object counting} operates without the need for category-specific training data. CLIP-Count~\cite{jiang2023clip} leverages CLIP to encode text and images separately, enabling semantic associations, while VLCount~\cite{kang2023vlcounter} enhances text-image association learning. PseCo~\cite{huang2023point} introduces a SAM-based framework for segmentation, dot mapping, and detection, offering broad applicability but requiring significant computational resources.

\vspace{-10pt}
\paragraph{Reference-less object counting} do not rely on specific references. ZSC~\cite{xu2023zero} generates prototypes using textual inputs and filters image patches, reducing labeling requirements but facing scalability challenges. While these methods are flexible and scalable, they often struggle with accuracy across diverse categories due to the absence of multi-category density map labels in most datasets.

\subsection{Counting Loss}
The most commonly used loss function in object counting is mean squared error (MSE) loss~\cite{ranjan2021learning}, which effectively measures the difference between predicted and ground truth maps. To capture associations, GMN~\cite{lu2019class} introduces a similarity loss that quantifies the relations between predicted and actual similarities, while FamNet~\cite{ranjan2021learning} incorporates a perturbation loss to increase robustness. LOCA~\cite{djukic2023low} adds an auxiliary loss to support multi-channel learning. Other methods~\cite{jiang2023clip, huang2023point} employ an InfoNCE-based contrastive loss~\cite{oord2018representation} to distinguish target regions from the background, while VA-Count~\cite{zhu2024zero} uses contrastive loss to differentiate between known and unknown classes.

However, these loss functions do not address the imbalance between single-label and multi-label data, which often leads models to indiscriminately count all objects. Inspired by Focal Loss~\cite{lin2017focal}, we propose Focal-MSE, an error-sensitive loss function that enhances sensitivity to specific regions. Focal-MSE promotes intra-class compactness and inter-class distinctiveness, transforming counting into a probability metric for specified categories and ensuring precise counting while overcoming the limitations of traditional loss.


% \subsection{Dirichlet Distribution}