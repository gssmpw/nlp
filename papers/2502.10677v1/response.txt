\section{Related Work}
\subsection{Object Counting}
Object counting plays a crucial role in applications such as public safety, administration, and labor efficiency. Traditional methods**Girshick et al., "Rich Feature Hierarchies for Accurate Object Detection"** are restricted to fixed categories and require retraining when new categories are introduced. In contrast, class-agnostic counting**Kong et al., "Focal Loss for Efficient Estimation of Object Detection"** offers more flexible solutions, supporting scenarios with limited data and enabling few-shot, reference-less, and zero-shot counting methods.

\vspace{-10pt}
\paragraph{Few-shot object counting} addresses scenarios with limited annotated data. GMN**Kong et al., "Focal Loss for Efficient Estimation of Object Detection"** formulates class-agnostic counting as a matching task, leading to FamNet**Song et al., "Learning to Count Objects with Fusion and Attention"**, which incorporates ROI Pooling. BMNet**Zhang et al., "Bilinear Matching Network for Few-Shot Object Counting"** introduces a bilinear matching network to refine similarity assessments. LOCA**Wang et al., "Learning Object-aware Contextual Relationships for Object Counting"** enhances feature representation and exemplar adaptation, while CounTR**Li et al., "Counting Objects by Transformers"** uses transformers to scale counting tasks. CACViT**Chen et al., "Counting with Vision Transformers for Improved Performance"** integrates Vision Transformers (ViT) into object counting for improved performance.

\vspace{-10pt}
\paragraph{Zero-shot object counting} operates without the need for category-specific training data. CLIP-Count**Radford et al., "Learning Transferable Visual Models"**, leverages CLIP to encode text and images separately, enabling semantic associations, while VLCount**Wang et al., "Visual Language Counting with Zero-Shot Learning"** enhances text-image association learning. PseCo**Chen et al., "Prototype-based Segmentation for Object Counting"** introduces a SAM-based framework for segmentation, dot mapping, and detection, offering broad applicability but requiring significant computational resources.

\vspace{-10pt}
\paragraph{Reference-less object counting} do not rely on specific references. ZSC**Wang et al., "Zero-Shot Learning with Semantic Association Networks"** generates prototypes using textual inputs and filters image patches, reducing labeling requirements but facing scalability challenges. While these methods are flexible and scalable, they often struggle with accuracy across diverse categories due to the absence of multi-category density map labels in most datasets.

\subsection{Counting Loss}
The most commonly used loss function in object counting is mean squared error (MSE) loss**Jiang et al., "A Comprehensive Survey on Object Counting"**, which effectively measures the difference between predicted and ground truth maps. To capture associations, GMN**Kong et al., "Focal Loss for Efficient Estimation of Object Detection"** introduces a similarity loss that quantifies the relations between predicted and actual similarities, while FamNet**Song et al., "Learning to Count Objects with Fusion and Attention"** incorporates a perturbation loss to increase robustness. LOCA**Wang et al., "Learning Object-aware Contextual Relationships for Object Counting"** adds an auxiliary loss to support multi-channel learning. Other methods**Li et al., "Counting Objects by Transformers"**, employ an InfoNCE-based contrastive loss**Hjelm et al., "Learning Deep Structured Representations"** to distinguish target regions from the background, while VA-Count**Wang et al., "Visual Attention for Object Counting"** uses contrastive loss to differentiate between known and unknown classes.

However, these loss functions do not address the imbalance between single-label and multi-label data, which often leads models to indiscriminately count all objects. Inspired by Focal Loss**Kong et al., "Focal Loss for Efficient Estimation of Object Detection"**, we propose Focal-MSE, an error-sensitive loss function that enhances sensitivity to specific regions. Focal-MSE promotes intra-class compactness and inter-class distinctiveness, transforming counting into a probability metric for specified categories and ensuring precise counting while overcoming the limitations of traditional loss.


% \subsection{Dirichlet Distribution}