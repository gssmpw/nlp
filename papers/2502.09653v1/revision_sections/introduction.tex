\section{Introduction}
\label{sec:intro}

Surgical video segmentation is crucial in advancing computer-assisted surgery, aiding intraoperative guidance and postoperative assessment. However, modern Deep Learning (DL) solutions require large-scale annotated datasets to be effectively trained. Gathering \textbf{annotations} in the form of \textbf{complete segmentation masks} requires substantial effort since creating full per-pixel annotations is a highly tedious task \cite{sanner2024detection}. This issue is multiplied in surgical process modelling, where DL solutions are often targeted at analysing long video sequences \cite{al2019cataracts,twinanda2016endonet}, significantly increasing the annotation effort along the temporal axis.

Large \textbf{foundation models} have lately emerged, trained on multitudes of publicly available large-scale datasets and often multiple tasks in parallel. These methods have proven to be successful when applied out of the box or fine-tuned to other domains \cite{ma2024segment,yu2024sam,chen2024sam2}. Yet, their application for computer-assisted surgery is either limited to frame-wise segmentation without incorporating temporal information \cite{sheng2024surgical,yue2024surgicalsam,chen2024sam2}, tracking only single tool classes \cite{wu2024real,liu2024surgical} or relying on manual prompting \cite{lou2024zero,yu2024sam}. 

\emph{SAM2} \cite{ravi2024sam} recently emerged as a robust video object tracking and segmentation tool but still relies on \textbf{manual prompting} and can fail to generalise to \textbf{video sections where entities leave the scene or new objects enter}, as visualised in Figure \ref{fig:intro}. Such events happen frequently in surgical video data when other instruments are used in subsequent surgical phases or when the camera moves during laparoscopy. Usually, such moments would require a re-prompting of the new entities to track, again increasing the manual effort of the clinician or machine learning engineer in the loop \cite{wang2023sam}. Further, without external domain knowledge, the method does not model the semantic meanings of tracked entities, rather than just performing consistent segmentation of tracked objects throughout a video.

\begin{figure}[htbp]
    \centering
    \if\usepng1
        \includegraphics[width=\textwidth]{figures/introduction_v3.png}
    \else
        \includegraphics[width=\textwidth]{figures/introduction_v3.eps}
    \fi
    \caption{\textbf{SAM2 Failure Case.} Video segmentation with \emph{SAM2} struggles with objects leaving or entering the scene (middle row; the \emph{electrocautery} is missed and predicted as background). \emph{SASVi} mitigates this issue by leveraging a frame-wise overseer model, producing temporally smooth and complete segmentations from scarce annotation data (bottom row).}
    \label{fig:intro}
\end{figure}

We propose \emph{Segment Any Surgical Video (SASVi)}, a novel video segmentation pipeline including a re-prompting mechanism based on a supportive frame-wise overseer model which runs in parallel to \emph{SAM2}. Precisely, we deploy an object detection model, pre-trained on small-scale surgical segmentation datasets, to monitor the entities currently present in the video. The dual nature of models such as \textit{Mask R-CNN} \cite{he2017mask}, \emph{DETR} \cite{carion2020end} or \emph{Mask2Former} \cite{cheng2022masked} allows us to rely on the object detection part of the model to detect when untracked classes enter the scene or previously tracked entities leave. We can then intercept such time points and use the model's segmentation part to segment the current frame. The obtained segmentation mask is then used to sample new prompting anchors for each currently present entity, including their semantic meaning. These anchor prompts are subsequently utilised to re-prompt \emph{SAM2}, which then continues the segmentation.

With this re-prompting of our overseer model, trained on scarcely available annotations, we can successfully leverage \emph{SAM2}'s excellent temporal properties to segment long video sequences of various surgical modalities with limited available annotation data. We quantitatively and qualitatively demonstrate on three prominent cholecystectomy and cataract surgery datasets that our method generates temporally smooth and consistent semantic segmentations of complete surgical video sequences. This further allows us to provide complete segmentation annotations of large-scale surgical video datasets for the public without additional manual annotation effort. 

\paragraph{Contributions}
\begin{itemize}
    \item We are the first to propose an automated re-prompting mechanism based on an object detector for deploying \emph{SAM2} for temporally smooth and consistent semantic segmentation of arbitrary surgical video domains with scarce annotation data.
    \item We deploy our method to leverage small-scale annotated surgical segmentation datasets into fully annotated publicly available large-scale segmentation annotations of their origin videos, demonstrated for the cholecystectomy dataset \emph{Cholec80} and the cataract surgery datasets \emph{Cataract1k} and  \emph{CATARACTS}. 
\end{itemize}
