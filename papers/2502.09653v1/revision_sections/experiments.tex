\section{Experiments \& Results}
\label{sec:exp}

We start this section by describing the datasets used in our evaluations.
Subsequently, we describe the experimental setup used to train the models.
We then present frame-wise segmentation results before evaluating the temporal smoothness of video segmentation and eventually giving an overview of the large-scale annotations we derive from our method and make available to the general public. 

%
%
%

\subsection{Datasets}

The \textbf{\emph{Cholec80}} dataset \cite{twinanda2016endonet} consists of 80 videos of laparoscopic cholecystectomy performed by 13 surgeons. The videos have an average length of $2,306.27$ seconds, are recorded at 25 FPS, and have a resolution of $854 \times 480$ or $1920 \times 1080$ pixels. They are annotated with one of seven surgical phases for each frame and multi-class multi-label annotations for seven surgical tools at 1 FPS.

Derived from \emph{Cholec80}, the \textbf{\emph{CholeSeg8k}} dataset \cite{hong2020cholecseg8k} contains 8080 frames of laparoscopic cholecystectomy, fully annotated with segmentation masks for 13 semantic labels, including black background, abdominal wall, liver, gastrointestinal tract, fat, grasper, connective tissue, blood, cystic duct, L-hook electrocautery, gallbladder, hepatic vein, and liver ligament.

The \textbf{\emph{CATARACTS}} challenge data \cite{al2019cataracts} was initially introduced as a challenge on surgical tool usage recognition and later on for surgical phase prediction. It consists of 50 video sequences of cataract surgery at 30 FPS, a $1920 \times 1080$ pixels resolution and an average length of $656.29$ seconds. Two experts annotated the tool usage of 21 surgical instruments. 

Introduced as a sub-challenge on semantic segmentation of cataract surgery images, the \textbf{\emph{CaDISv2}} dataset \cite{grammatikopoulou2021cadis} contains 4670 images of the 25 \emph{CATARACTS} training videos, which are fully annotated with segmentation masks. The total count of labels is 36, from which 28 are surgical instruments, four are anatomy classes, and three are miscellaneous objects appearing during the surgery. Our experiments focus on the pre-defined experiment setting II, which groups the instrument classes into ten classes, resulting in 17 semantic labels. 

Lastly, the \textbf{\emph{Cataract-1k}} dataset \cite{ghamsarian2023cataract} consists of over 1000 cataract surgery videos recorded at 60 FPS, from which different subsets are annotated for different tasks, including surgical phase prediction, semantic segmentation and irregularity detection. Here, we focus on the 30 videos from which 2256 frames are annotated with segmentation masks for the surgical instrument, pupil, iris and artificial lens. These frames have a resolution of $512 \times 384$ pixels. 

An analysis of the scarcity of annotations of the respective datasets can be found in Supplementary Section \ref{sec:app_scarce}.


%
%
%

\subsection{Experimental Setup}

We split the available videos in \emph{CholecSeg8k}, \emph{CaDISv2} and \emph{Cataracts1k} for training/validation/testing by 14/2/2, 19/3/3 and 24/3/3, respectively.
Our \emph{Overseer} models are trained for $1\text{e}5$ steps on the small-scale datasets with a batch size of 8. We are using the \emph{AdamW} optimiser \cite{loshchilov2017decoupled} with $(\beta_1=0.5,\beta_2=0.999)$, an initial learning rate of $1\text{e-}4$ and a weight decay of $0.05$. The learning rate is decayed every $2\text{e}4$ steps by a factor of $0.5$. To match the training configurations of the involved backbones, we rescale images to $(299 \times 299)$ pixels for \emph{Mask R-CNN} and \emph{Mask2Former} and $(200 \times 200)$ pixels for \emph{DETR}. The models have been trained on a single Nvidia RTX4090 using PyTorch 2.4.1 and Cuda 12.2.
Further details on the model and training configurations and the code to reproduce our results can be found at \href{https://github.com/MECLabTUDA/SASVi}{https://github.com/MECLabTUDA/SASVi} upon acceptance.

%
%
%

\subsection{Per-Frame Object Detection \& Segmentation Results}
\label{sec:overseer}

This section presents object detection and segmentation results on the small-scale annotated sub-datasets. For \emph{quantitative evaluation} of the bounding boxes, we deploy the IoU metric at a $50\%$ threshold. To evaluate the predicted classes of objects, we use the F1 score at a $50\%$ IoU threshold, and to quantify the per-object segmentation quality, we deploy the Dice metric at $50\%$ IoU. We additionally evaluate the final semantic segmentation quality using the macro-average Dice metric (\emph{Semantic Dice}).

The results of all metrics are displayed in Table \ref{tab:framewise}, and qualitative results for \emph{Mask R-CNN} are shown in Figure \ref{fig:frame_qual}. While \emph{Mask R-CNN} occasionally predicts multiple bounding boxes for the same object, resulting in lower per-object scores, it generally performs well across all datasets, especially regarding the final segmentation masks obtained.
%, underscoring its capability as an \emph{Overseer} model for \emph{SAM2}.
However, the Transformer-based methods \emph{DETR} and \emph{Mask2Former} suffer less from this issue and generally show superior performance. We therefore opt to continue with \emph{Mask2Former} as our main \emph{Overseer} model for \emph{SAM2}

\begin{table}[htbp]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{c|c|cccc}
            \textbf{Dataset} & \textbf{Method} & \textbf{Class F1 $(\uparrow)$} & \textbf{BB IoU $(\uparrow)$}  & \textbf{Mask Dice $(\uparrow)$} & \textbf{Semantic Dice $(\uparrow)$} \\
            \hline
            & Mask R-CNN & 0.957 & 0.887 & 0.834 & 0.937 \\
            CholecSeg8k & DETR & 0.935 & \textbf{0.893} & 0.912 & 0.934 \\
            & Mask2Former & \textbf{0.958} & 0.884 & \textbf{0.913} & \textbf{0.940} \\
            \hline
            & Mask R-CNN & 0.585 & 0.636 & 0.626 & 0.786 \\
            CaDISv2 & DETR & 0.769 & 0.774 & 0.811 & \textbf{0.854} \\
            & Mask2Former & \textbf{0.823} & \textbf{0.824} & \textbf{0.828} & 0.838 \\
            \hline
            & Mask R-CNN & 0.745 & 0.731 & 0.664 & 0.881 \\
            Cataract1k Segm. & DETR & \textbf{0.835} & \textbf{0.777} & \textbf{0.777} & \textbf{0.897} \\
            & Mask2Former & 0.764 & 0.729 & 0.737 & 0.881 \\
        \end{tabular}
    }
    \caption{\textbf{Per-Frame Overseer Object Detection \& Segmentation Results.}}
    \label{tab:framewise}
\end{table}

\begin{figure}[htbp]
    \centering
    \if\usepng1
        \includegraphics[width=0.95\textwidth]{figures/frame_wise_qual.png}
    \else
        \includegraphics[width=0.95\textwidth]{figures/frame_wise_qual.eps}
    \fi
    \caption{\textbf{Qualitative Object Detection \& Segmentation Results.} Object detection methods such as \emph{Mask R-CNN} can serve as a powerful frame-wise \emph{Overseer} model, predicting classes, bounding boxes and segmentation masks of objects in surgical scenes.}
    \label{fig:frame_qual}
\end{figure}

%
%
%

\subsection{Temporally Consistent Video Segmentation}
\label{sec:vid_seg}

Applying frame-wise models of any kind onto sequential images often introduces artefacts of temporal inconsistencies due to ambiguities in predictions and a lack of temporal information \cite{rivoir2021long,frisch2023temporally}. Therefore, and due to the lack of large-scale ground truth annotations, we deploy the following metrics to quantify the quality and temporal consistency of video segmentations:
\begin{enumerate}
    \item Similarly to previous work on evaluating temporal consistency for image-to-image translation \cite{rivoir2021long,frisch2023temporally}, we deploy optical flow warping for evaluating the consistency of segmentations along the temporal axis. More specifically, given two subsequent image frames $v_t$ and $v_{t+1}$, we compute the optical flow $OF(v_t,v_{t+1})$ between them. We then use this optical flow in a warping operation $W$ to warp the previous segmentation mask as $m'_{t+1} := W(m_t,OF(v_t,v_{t+1}))$. We eventually compare the macro-average Dice and IoU scores of the warped segmentation $m'$ to the segmentation of the next frame $m_{t+1}$, denoted as $\text{Dice}_{OF}$ and $\text{IoU}_{OF}$ respectively.
    \item Analogously, we directly compute the macro-average Contour Distance and IoU scores of subsequent mask predictions $m_t$ and $m_{t+1}$, which we denote as $\text{CD}_{T}$ and $\text{IoU}_{T}$ respectively. Here, better scores indicate a better temporal consistency of the masks but disregard the actual image content.
\end{enumerate}
Appendix Section \ref{sec:app_temp} provides auxiliary visualisations for these metrics, and their results are presented in Table \ref{tab:videowise}. Qualitative results are presented in Figure \ref{fig:video_qual} with additional results in Section \ref{sec:app_qual} in the Appendix. For \emph{SAM2}, we prompt the model with the semantic mask predicted by \emph{Mask2Former} from the first frame (\emph{SAM2 ($t_1$)}). Further, we experiment with re-prompting the model with ground truth segmentation masks every time they are available, denoted as $\emph{SAM2 (GT)}$. We additionally compare the approaches to a frame-wise \emph{nnUNet} with the \emph{ResNetEncM} configuration \cite{isensee2021nnu}, trained on $(128 \times 128)$ sized images and an equal number of steps as the \emph{Overseer} models, and to Surgical De-SAM \cite{sheng2024surgical}, trained on $(1024 \times 1024)$ images until convergence.

\begin{table}[htbp]
    \centering
    \caption{\textbf{Quantitative Video Segmentation Results.}}
    \begin{tabular}{c|c|cccc}
         \textbf{Dataset} & \textbf{Method} & \textbf{$\text{Dice}_{OF} (\uparrow)$} & \textbf{$\text{IoU}_{OF} (\uparrow)$} & \textbf{$\text{CD}_{T} (\downarrow)$} & \textbf{$\text{IoU}_{T} (\uparrow)$} \\
         \hline
          & nnUNet & 0.562 & 0.476 & 6.811 & 0.573 \\
          & Mask R-CNN & 0.568 & 0.482 & 7.002 & 0.555 \\
          & Mask2Former & 0.625 & 0.542 & 4.654 & 0.624 \\
          & Surgical-DeSAM & 0.540 & 0.459 & 7.390 & 0.546 \\
         Cholec80 & SAM2 ($t_1$) & 0.451 & 0.398 & 163.98 & 0.475 \\
          & SAM2 (GT) & 0.730 & 0.636 & \textbf{2.879} & 0.769 \\
          & SASVi (Mask R-CNN) & 0.737 & 0.645 & 3.449 & 0.763 \\ 
          & SASVi (Mask2Former) & \textbf{0.754} & \textbf{0.662} & 3.291 & \textbf{0.780} \\
         \hline
          & nnUNet & 0.547 & 0.474 & 5.116 & 0.583 \\
          & Mask R-CNN & 0.375 & 0.308 & 6.134 & 0.501 \\
          & Mask2Former & 0.592 & 0.515 & 3.601 & 0.623 \\
          & Surgical-DeSAM & 0.518 & 0.437 & 4.621 & 0.560 \\
         CATARACTS & SAM2 ($t_1$) & 0.465 & 0.412 & 126.05 & 0.495 \\
          & SAM2 (GT) & 0.652 & 0.568 & \textbf{2.939} & 0.695 \\
          & SASVi (Mask R-CNN)& 0.658 & 0.570 & 3.466 & 0.694 \\ 
          & SASVi (Mask2Former) & \textbf{0.674} & \textbf{0.588} & 3.028 & \textbf{0.715} \\
         \hline
          & nnUNet & 0.662 & 0.570 & 1.951 & 0.690 \\
          & Mask R-CNN & 0.578 & 0.500 & 2.717 & 0.605 \\
          & Mask2Former & 0.665 & 0.575 & \textbf{1.911} & 0.681 \\
          & Surgical-DeSAM & 0.665 & 0.575 & 2.094 & 0.619 \\
         Cataract1k & SAM2 ($t_1$) & 0.329 & 0.292 & 241.53 & 0.339 \\
          & SAM2 (GT) & 0.726 & 0.630 & 1.980 & 0.744 \\
          & SASVi (Mask R-CNN) & \textbf{0.741} & \textbf{0.650} & 1.935 & \textbf{0.756} \\ 
          & SASVi (Mask2Former) & 0.730 & 0.634 & 1.986 & 0.751 \\
    \end{tabular}
    \label{tab:videowise}
\end{table}

\begin{figure}[htbp]
    \centering
    \if\usepng1
        \includegraphics[width=\textwidth]{figures/video_qual.png}
    \else
        \includegraphics[width=\textwidth]{figures/video_qual.eps}
    \fi
    \caption{\textbf{Qualitative Video Segmentation Results.} \emph{SASVi (Mask R-CNN)} predicts smooth and complete annotations for surgical videos of arbitrary domains, here demonstrated for one video of \emph{Cholec80} (top), \emph{CATARACTS} (middle) and Cataract1k (bottom).}
    \label{fig:video_qual}
\end{figure}

Clearly, the re-prompting of \emph{SAM2}, be it from ground truth masks or our \emph{Overseer}, produces segmentations of significantly better temporal consistency. While \emph{SAM2 (GT)} predicts segmentations with lower \emph{Contour Distance} along the temporal axis, this can be explained by the metric's high sensitivity to outliers and not entirely optimal predictions from the \emph{Overseer}, as discussed in Section \ref{sec:overseer}. We are discussing this and other limitations and future improvements in Appendix Section \ref{sec:app_limit}. However, incorporating the actual image movement in the optical-flow-based metrics reveals better performance of \emph{SASVi} over all other considered methods. 

Our method allows us to leverage the \textbf{scarce annotations} available in \emph{CholecSeg8k}, \emph{CadISv2} and \emph{Cataract1k Segm.} and \textbf{produce full annotations of their large-scale video counterpart datasets} \emph{Cholec80}, \emph{CATARACTS} and \emph{Cataract1k}, respectively. Section \ref{sec:app_data} in the Appendix outlines the large-scale data statistics. We make those annotations available to the public, providing extensive annotation data for the future development of surgical analysis models.
