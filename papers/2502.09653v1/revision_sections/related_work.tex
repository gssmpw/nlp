\section{Related Work}
\label{sec:rw}

For \textbf{segmenting surgical videos}, Wang et al. \cite{wang2021efficient} have introduced a dual-memory network to relate local temporal knowledge with global semantic information by incorporating an active learning strategy. Zhao et al. \cite{zhao2021anchor} combine meta-learning with anchor-guided online adaption to improve domain transfer generalisation. COWAL \cite{wu2024correlation} deploys an active learning strategy based on model uncertainty and temporal information to improve video segmentation. However, these approaches require access to large-scale annotated data for their specific target or visually similar source domains.

\textbf{Foundation models}, trained on large-scale computer vision datasets, have been successfully deployed in the recent past to demonstrate generalisation capabilities for segmentation \cite{kirillov2023segment}. This model has found a wide range of applications in medical imaging \cite{ma2024segment,ranem2024uncle}.

In the \textbf{surgical context}, \emph{SurgicalSAM} \cite{yue2024surgicalsam} eliminates the need for explicitly prompting \emph{SAM}\cite{kirillov2023segment} by introducing a prompt encoder that generates prompt embeddings automatically, alongside contrastive prototype learning to distinguish visually similar tools better. \emph{Surgical-DeSAM} \cite{sheng2024surgical} combines \emph{SAM} with a \emph{DETR} model for tool detection and re-prompts SAM using bounding boxes, enabling multi-class segmentation. While these approaches improve frame-wise segmentation, they do not leverage temporal information from videos.

The \emph{Segment Anything Model 2 (SAM2)} \cite{ravi2024sam} extends \emph{SAM} \cite{kirillov2023segment} for \textbf{video segmentation}. It achieves temporally smooth segmentations by introducing a memory buffer of previous information. \emph{SAM2-Adapter} \cite{chen2024sam2} extends \emph{SAM2} by introducing trainable adapter layers to incorporate task-specific knowledge and has been successfully applied to frame-wise polyp segmentation. \emph{Surgical SAM2} \cite{liu2024surgical} implements a frame-pruning mechanism to reduce memory and computation costs, addressing challenges associated with processing long sequences of surgical video frames. Yu et al. \cite{yu2024sam} evaluate \emph{SAM2} on surgical videos using manual point and box prompts. They observe robust results but also point to the method's limitations when dealing with synthetic data, where performance degrades due to image corruptions and perturbations. Similarly, zero-shot segmentation using SAM2 has been explored for surgical tool tracking in endoscopy and microscopy data, proving effective for multi-class tool segmentation \cite{lou2024zero}. However, unlike our proposed approach, these methods still rely heavily on manual prompting and do not implement re-prompting mechanisms, hence suffering from performance decreases when entities leave or enter the scene.