\section{Temporal Consistency Metrics}
\label{sec:app_temp}

This section aids in understanding the metrics introduced in Section \ref{sec:vid_seg} with simplified visualisations, displayed in Figure \ref{fig:app_tc}.

\begin{figure}[htbp]
    \centering
    \if\usepng1
        \includegraphics[width=\textwidth]{figures/tc_metrics.png}
    \else
        \includegraphics[width=\textwidth]{figures/tc_metrics.eps}
    \fi
    \caption{\textbf{Temporal Consistency Metrics.} The metrics $\text{CD}_{T}$ and $\text{IoU}_{T}$ consider the temporal consistency purely in mask space (top row). However, they fail to capture when images are stationary, but the masks transition smoothly.
    Therefore, $\text{Dice}_{OF}$ and $\text{IoU}_{OF}$ take the actual image movement into account, penalising such cases (bottom rows).}
    \label{fig:app_tc}
\end{figure}

\section{Additional Qualitative Results}
\label{sec:app_qual}

This section presents additional qualitative results in Figure \ref{fig:app_qual}. Fully segmented example videos of each of the three datasets can be found at \href{https://hessenbox.tu-darmstadt.de/getlink/fiW6NMDLQ1z8oGsj1PD8Kc81/}{https://hessenbox.tu-darmstadt.de/getlink/fiW6NMDLQ1z8oGsj1PD8Kc81/}. In the videos, we also visually compare \emph{SASVi} to \emph{nnUNet}, a popular meta-learning framework for frame-wise segmentation of medical images.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/sasvi_app_qual.png}
    \caption{\textbf{Additional Qualitative Results.} \emph{SASVi} predicts complete segmentation masks for whole videos (bottom row) only relying on scarcely available annotation data (middle row), here demonstrated for \emph{Video20} of the \emph{Cholec80} dataset (top row).}
    \label{fig:app_qual}
\end{figure}

\section{Limitations \& Future Work}
\label{sec:app_limit}

The performance of \emph{SASVi} naturally depends on the performance of the \emph{Overseer} model, as analysed in Table \ref{tab:sensitivity}. Hence, we will explore other model choices in future work, focusing primarily on models that can be effectively trained on scarcely available ground truth data. Additional techniques for reducing error propagation, such as incorporating model uncertainty estimates, also yield a promising direction for future research. During the late stages of preparing the manuscript, the authors of \emph{SAM2} \cite{ravi2024sam} provided the means to fine-tune the model on custom data, which we will include in the future. Further, we will explore including existing ground truth data during \emph{SASVi} inference. Despite these limitations, our proposed approach can be a strong baseline for smooth and temporally consistent segmentation. The method lets us publicly provide large-scale annotations of complete videos from scarcely available data, as presented in the next section. 

\begin{table}[htbp]
    \centering
    \resizebox{\textwidth}{!}{\begin{tabular}{c|c|cccc}
        \textbf{Overseer (Annotations)} & \textbf{Semantic Dice $(\uparrow)$} & \textbf{$\text{Dice}_{OF} (\uparrow)$} & \textbf{$\text{IoU}_{OF} (\uparrow)$} & \textbf{$\text{CD}_{T} (\downarrow)$} & \textbf{$\text{IoU}_{T} (\uparrow)$} \\
        \hline
        Mask R-CNN (100\%) & 0.881 & 0.741 & 0.650 & 1.935 & 0.756 \\
        Mask R-CNN (50\%) & 0.879 & 0.567 & 0.489 & 2.088 & 0.719 \\
        Mask R-CNN (10\%) & 0.855 & 0.473 & 0.405 & 2.453 & 0.655 \\
        Mask R-CNN (1\%) & 0.756 & 0.352 & 0.299 & 93.153 & 0.447 \\
    \end{tabular}}
    \caption{\textbf{Impact of Overseer Performance on SASVi.} The \emph{Overseer} is trained with fewer training samples to assess \emph{SASVi} performance under data scarcity constraints.}
    \label{tab:sensitivity}
\end{table}

\section{Compute Analysis}
\label{sec:app_compute}

This section analyses the applicability of the methods for real-time segmentation of surgical videos using a single Nvidia RTX4090. We provide their parameter count and FPS for \emph{Cholec80} in Table \ref{tab:efficiency}. The results show that \emph{SASVi} does not introduce a significant computational overhead over \emph{SAM2}, which stems from our choice of lightweight object detection \emph{Overseer} models. These models can monitor surgical scenes more efficiently than traditional surgical segmentation pipelines, such as \emph{nnUNet} \cite{isensee2021nnu}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{c|cc}
         \textbf{Method} & \textbf{Number of Parameters} & \textbf{FPS} \\
         \hline
         nnUNet & $269.4 \times 10^6$ & 4.633 \\
         Mask R-CNN & $45.8 \times 10^6$ & 49.456 \\
         DETR & $42.8 \times 10^6$ & 51.361 \\
         Mask2Former & $106.8 \times 10^6$ & 25.974 \\
         \hline
         SAM2 (t1) & $224.4 \times 10^6$ & 8.064 \\ 
         SASVi (Mask2Former) & $331.2 \times 10^6$ & 6.680
    \end{tabular}
    \caption{\textbf{Model Compute Evaluation for \emph{Cholec80}.}}
    \label{tab:efficiency}
\end{table}

\section{Dataset Annotation Sparsity}
\label{sec:app_scarce}

The three surgical datasets examined in this paper (\emph{CATARACTS} \cite{al2019cataracts}, \emph{Cataract1k} \cite{ghamsarian2023cataract} and \emph{Cholec80} \cite{twinanda2016endonet}) comprise full surgical videos each containing 50, 1000, and 80 videos respectively. We refer to these full videos as "large-scale datasets" or "counterparts". Each dataset only has a small subset of videos with only a few individual frames annotated with semantic segmentation masks: \emph{CaDISv2} \cite{grammatikopoulou2021cadis}, \emph{Cataract1k Segm.} \cite{ghamsarian2023cataract} and \emph{CholecSeg8k} \cite{hong2020cholecseg8k}, respectively. These annotations are scarce and vary significantly in length and distribution, as visualised in Figure \ref{fig:temporal_plot}. 

\begin{itemize}
\item \textbf{CATARACTS}: The videos were recorded at 30 FPS. Only 4670 out of 494,878 frames were annotated in the \emph{CaDISv2} subset \cite{grammatikopoulou2021cadis}, which constitutes just 0.95\% of the total frames. There are gaps as large as 5110 frames ($\approx$ 170 seconds) without annotations. 

\item \textbf{Cataract1k}: The videos were recorded at 60 FPS, with annotations provided at regular intervals of every 276th frame ($\approx$ 4.6 seconds) across 30 videos. This results in 2256 annotated frames, accounting for just 0.34\% of all available frames.

\item \textbf{Cholec80}: The videos were recorded at 25 FPS with an average length of 2306.27 seconds. While the \emph{CholeSeg8k} subset \cite{hong2020cholecseg8k} includes 8080 annotated frames, which is nearly twice as many as \emph{CaDISv2}, the annotations are only marginally denser, containing 1.08\% of annotated frames due to the videos being $\approx 3.5$ times longer on average. The annotations are also heavily concentrated at specific time frames, leaving extensive portions of the videos without any annotations.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figures/temporal_plot.png}
    \caption{\textbf{Visualising Video Annotation Scarcity.} Each vertical bar represents one annotated frame. Multiple concentrated annotated frames blend into darker colours for visualisation.}
    \label{fig:temporal_plot}
\end{figure}

The lack of datasets with continuous segmentation annotations in the surgical domain presents a significant challenge for training video segmentation models. Capturing temporal connections and modelling transitions across frames is difficult without such models. Hence, leveraging foundational models pre-trained on extensive and diverse datasets can help overcome this limitation by providing robust features for video segmentation in the surgical domain. 

\section{Large-Scale Annotation Data for Surgical Video Segmentation}
\label{sec:app_data}%

This section gives an overview of the large-scale annotations generated with \emph{SASVi} for the full video counterparts of the small-scale scarcely annotated data. Upon acceptance, we provide the obtained annotations for the public at \hyperlink{https://github.com/MECLabTUDA/SASVi}{https://github.com/MECLabTUDA/SASVi}, enabling future improvements of surgical data science models.

We provide complete annotations for the 17 videos from \emph{Cholec80}, from which \emph{CholecSeg8k} was created. The left part of Figure \ref{fig:app_data} gives an overview of the available frames per label, comparing the previously available small-scale annotations and our large-scale extension. Analogously, we generate complete annotations for the 25 \emph{CATARACTS} videos from which the \emph{CaDIS} dataset was extracted. The middle part of Figure \ref{fig:app_data} displays the data statistics. Eventually, we also provide complete annotations for the 30 videos from which the \emph{Cataract1k} segmentation subset was extracted. The right part of figure \ref{fig:app_data} gives an overview of the statistics.

\begin{figure}[htbp]
    \centering
    \if\usepng1
        \includegraphics[width=\textwidth]{figures/data_stats.png}
    \else
        \includegraphics[width=\textwidth]{figures/data_stats.eps}
    \fi
    \caption{\textbf{Large-Scale Data Statistics.} Using \emph{SASVi}, we can greatly extend the available annotations for semantic segmentation of various surgical datasets, here demonstrated for \emph{Cholec80} (left), \emph{CATARACTS} (middle) and \emph{Cataract1k} (right). It is best viewed in the digital version.}
    \label{fig:app_data}
\end{figure}

