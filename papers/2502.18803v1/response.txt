\section{Related Work}
\label{sec:relwork}
\subsection{Approximation Query Processing}

Approximate Query Processing (AQP) methods aim to return approximate answers to Online Analytical Processing (OLAP) queries with high accuracy and low computational overhead. AQNNs differ from OLAP queries fundamentally in both scope and operation. 

AQP techniques can generally be categorized into two types: online and offline. Online methods dynamically sample data at query time to compute approximate answers **Zhang et al., "Approximate Query Processing for Big Data"**. Offline methods, on the other hand, rely on precomputed synopses such as wavelets to answer queries efficiently **Chakrabarti et al., "Offline Approximation of Aggregate Queries in OLTP Systems"**. Our method adopts a probabilistic top-k framework that distinguishes itself from these categories, which does not rely on precomputation nor does it dynamically sample data during query execution. 

\subsection{Nearest Neighbor Search}
Nearest neighbor (NN) search methods can be categorized into exact and approximate approaches. Common methods, such as brute force, k-d trees, and cell techniques, are conceptually straightforward **Bentley et al., "Multidimensional Binary Search Trees Used for Associative Searching"**. However, these methods are impractical due to their high computational cost. Exact NN search typically relies on accurate embeddings generated by expensive oracle models and builds indexes over the entire database, leading to significant computational overhead **Andreu et al., "Nearest Neighbor Search in Metric Spaces"**. 

As databases and data dimensionality grow, approximation algorithms are used to strike a balance between accuracy and efficiency. These approaches include hashing-based **Min et al., "Approximate Nearest Neighbor Search by Weighted Sampling"**, tree-based **Wong et al., "A New Indexing Method for High-Dimensional Data"**, and index-based methods **Fagin et al., "Indexing the Universe"**. Additionally, **Zhu et al. proposes a locality-sensitive hashing based approach and delivers sublinear query times and polynomial time processing costs**, while **Kim et al. proposes space-efficient data structures guaranteeing compactness for Euclidean distances under specific accuracy constraints**. FLANN **Muja et al., "Fast Approximate Nearest Neighbor Search in High Dimensions"** is a library leveraging randomized kd-trees and priority search k-means trees for scalable approximate NN finding over large datasets. Unlike our work, these approaches assume that the ground-truth representation of data objects is precomputed and can be efficiently accessed. The challenge in our problem is that the ground-truth representation needs to be computed on demand using expensive models. Recently, there has been a line of works relying cheap proxy models to approximate ground truth oracle labels and find accurate approximations of NN **Lai et al., "Probabilistic Top-K for Approximate Nearest Neighbor Search"**. For instance, Lai et al. propose probabilistic Top-K to train proxy models to generate oracle label distribution and output approximate Top-K solutions for video analytics **Lai et al., "Probabilistic Top-K for Approximate Nearest Neighbor Search in Video Analytics"**. In **Zhu et al., the authors introduce statistical accuracy guarantees, such as meeting a minimum precision or recall target with high probability, for approximate selection queries**. Recently, Ding et al. introduce a  framework for approximate queries that leverages proxy models to minimize the reliance on expensive oracle models **Ding et al., "Efficient Approximate Query Processing using Proxy Models"**. They propose four algorithms to find high-quality answers for both precision-target and recall-target queries. However, these methods prioritize either precision or recall. 

Notice that while \sprintv and \sprintc build upon PQE-PT, one of the algorithms proposed in **Zhu et al., for selecting nearest neighbors, our work is substantially differentiated by two key aspects**. First, PQE-PT assumes a \textit{given} recall target as input and identifies neighbors that satisfy this target. In contrast, our algorithms \textit{dynamically optimize both recall and precision targets}. Second, while PQE-PT focuses on maximizing recall given a precision target, \sprintv is designed to maximize the \(\text{F}{\beta}\) score, and \sprintc aims to equalize recall and precision, which enable more accurate approximation of aggregates. Specifically, none of the algorithms in prior work including PQE-PT can achieve either of these objectives -- max \(\text{F}{\beta}\) score or equal precision and recall.

% \subsection{Hypothesis Testing}
% Hypothesis testing is an established method of statistical analysis, widely applied in many areas, including behavioral sciences and medical research **Talwar et al., "Testing the Factors Influencing Fake News Sharing on Social Media"**. Existing studies focus on designing statistical tests that account for dependencies, distributions, and other characteristics of the data **Wang et al., "Hypothesis Testing for Graph Data"**. Additionally, hypothesis testing has been applied to evaluate relationships between variables, determine causal effects, and validate assumptions in different scenarios **Talwar et al., "Testing the Relationship Between Students' Interest in Learning and Their Academic Outcomes"**. For instance, Talwar et al. test hypotheses about the factors influencing the sharing of fake news on social media **Talwar et al., "Testing the Factors Influencing Fake News Sharing on Social Media"**. In **Wang et al., the authors examine the relationship between students' interest in learning and their academic outcomes in natural sciences**. Wang et al. extend hypothesis testing to graph data, proposing methods to test graph hypotheses based on nodes, edges, and path patterns **Wang et al., "Hypothesis Testing for Graph Data"**. However, little attention has been given to hypothesis testing in the context of modern predictive models.

\subsection{Post-ML-Inference Processing}
General-purpose embeddings generated by machine learning models are typically trained on broader objectives, such as feature extraction or representation learning. These embeddings can be reused across various post-inference querying tasks, including anomaly detection **Chen et al., "Learning Deep Embeddings for Anomaly Detection"**, aggregation **Wang et al., "Aggregation of Deep Features for Image Recognition"**, and nearest neighbor search **Beyer et al., "Fast Approximate Nearest Neighbor Search in High Dimensions"**. Our work leverages these embeddings as proxy models to facilitate a distinct task: the approximate answering of AQNNs.