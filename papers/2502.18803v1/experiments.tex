\begin{table}[t!]
\caption{Datasets, oracle and proxy models}
\label{tab:dataset}
\small
% \resizebox{\columnwidth}{!}
% {%
\begin{tabular}{|c|c|c|}
\hline
Datasets            & Oracle               & Proxy       \\ \hline
Mimic-III, eICU     & Physicians           & LIG-Doctor\cite{DBLP:journals/isci/RodriguesGSBA21} \\ \hline
Jigsaw & User Labels & GLiClass       \\ \hline
Jackson & Mask R-CNN\cite{DBLP:conf/iccv/HeGDG17} & ResNet-50\cite{DBLP:conf/cvpr/HeZRS16}       \\ \hline
\end{tabular}
% }
\end{table}


\section{Experiments}\label{sec:exps}

The purpose of our experiments is to evaluate our solution to AQNN queries w.r.t. objectives (\textbf{O1}) and (\textbf{O2}): error and cost. In \S ~\ref{exp:setup}, we introduce the datasets, baselines, and evaluation measures. \S~\ref{exp:error} evaluates the effectiveness of \sprintv and \sprintc in minimizing aggregation error, as well as the impact of key parameters  including \(\text{F}{\beta}\) score, equalizing precision and recall, and radius \(r\) on the aggregation error. \S ~\ref{exp:cost} examines the efficiency of our methods w.r.t.  execution time and scalability. Finally, in \S ~\ref{exp:HT}, we demonstrate the application of AQNNs in hypothesis testing. % and evaluate the performance of SPRinT-V and SPRinT-C in this context.


\subsection{Experimental Setup}\label{exp:setup}
\subsubsection{Datasets}
We used datasets in three different domains. \\ 
%
\stitle{Medical:} MIMIC-III \cite{johnson2016mimic} and eICU \cite{pollard2018eicu} are publicly available clinical datasets containing de-identified data for patient stays across intensive care units (ICUs) in the United States. We adopt LIG-Doctor \cite{DBLP:journals/isci/RodriguesGSBA21} to learn each patient's embeddings from patient's ICU admissions and regard the physician's diagnoses as the oracle embeddings. The dataset has several patient attributes, including  demographics (e.g., age, weight, height) and physiological measurements (e.g., blood pressure, hemoglobin levels), which are a rich source of AQNNs in medical contexts. After removing records with only one admission, the resulting MIMIC-III and eICU datasets contain 4,245 and 8,234 records respectively.

\stitle{Social media:} The Jigsaw Toxicity Classification Dataset (Jigsaw) \cite{jigsaw} is a large-scale dataset containing user-generated comments labeled across multiple toxicity categories, including toxic, threat, and insult. We employ GLiClass, a zero-shot classifier, to learn comment embeddings, while the annotated toxicity labels serve as the oracle embeddings. To enhance the dataset for AQNN queries in social media, we synthesize additional attributes, such as the number of down-votes, to capture engagement dynamics. After preprocessing, we filter out non-toxic comments, resulting in a final dataset of 16,225 toxic comments.

% Electronics (Amazon-E) and Health and Household (Amazon-HH) are two categories in Amazon Reviews’23 \cite{hou2024bridging} which contain extensive user-generated reviews and metadata for those products on Amazon. We employ TLSAN \cite{DBLP:journals/ijon/ZhangWY21} to learn user embeddings from their historical purchases on Amazon and regard the real customer interactions as the oracle embeddings. The dataset has the rating information to answer AQNNs in e-commerce. We filter for active users with at least 10 reviews and popular products with more than 800 reviews. Electronics and Health and Household contain 10,223 and 15,384 users, respectively.

\stitle{Video:} The Jackson dataset \cite{DBLP:conf/mlsys/CanelKZLLAKD19} is a real-world camera feed dataset capturing urban environments. Each frame is labeled with a boolean value indicating the presence or absence of cars. We synthesize relevant attributes, like car speed. For our experiments, we uniformly sample 10,000 frames from the original 60,000-frame dataset.

\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/distribution/icd9_eICU.png}
        \caption{eICU}
        \label{fig:dist_eICU}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/distribution/icd9_mimic.png}
        \caption{MIMIC-III}
        \label{fig:dist_mimic}
    \end{subfigure}
    \caption{Distribution of the difference between proxy and oracle distances}
    \label{fig:dist}
\end{figure}


The proxy and oracle models used for each dataset are shown in Table \ref{tab:dataset}. The choices are justified by our analysis of sample complexity and pilot sample complexity in \S~\ref{subsubsec:exp_s_s_p}.

\subsubsection{Baselines}
\sprint supports AQNN approximation at a low cost and with low error as our experiments will show, and to our knowledge, is the first framework to do so. As such, there are no directly comparable frameworks. Given this, we evaluate \sprint by comparing its effectiveness with the following approximate nearest neighbor search algorithms:

\stitle{PQE-PT and PQE-RT.}
PQE-PT and PQE-RT are approximate FRNN query algorithms designed to pick nearest neighbors with minimal oracle usage while achieving predefined precision or recall targets, respectively, with high probability \cite{DujianPQA}. These solutions do not require any assumption about the proxy quality but calibrate the correlation between the oracle and the proxy by incurring some oracle calls. Specifically, a sample of objects in \(D\) is drawn to estimate the correlation between oracle and proxy distances for nearest neighbor selection. PQE-PT \cite{DujianPQA} is used as a sub-routine of SPRinT-V and SPRinT-C, although it only focuses on optimizing a single measure, precision. 
% PQA assumes that distances computed from proxy embeddings closely approximate those from oracle embeddings, which holds in many real-world scenarios. 
% Figure \ref{fig:dist} shows the distribution of differences between oracle distance \(dist^O(x_i)\) and proxy distance \(dist^P(x_i)\) for \(x_i \in D\) for the Mimic-III and Amazon-HH datasets. 
% \laks{as far as i checked the notation \(dist^O(x_i)\) and \(dist^P(x_i)\) appears here for the first time. it needs to be defined ahead of time and explained. also by \(dist^O(x_i)\) do you mean the oracle distance from the implicit query object $q \in D$ to object $x_i$? if so, this convention needs to be clearly stated.}  The high frequency of differences close to zero indicates that the proxy model effectively approximates oracle embeddings. 
% \laks{it's not just the mode at 0 deviation that is important here. it's the fact that the distribution of the deviation between the oracle distances and the proxy distances follows a Gaussian is significant. PQE-PT and PQE-RT leverage this to estimate the oracle distance given the proxy distance, where the latter is cheaper to compute.} 
By default, we set the recall and precision targets to 0.95, as suggested in \cite{DujianPQA}. 

\stitle{SUPG-PT and SUPG-RT. } 
SUPG-PT and SUPG-RT transform an FRNN query into a binary classification problem, predicting whether a candidate object lies within a query radius \(r\) using proxy distances as predictors \cite{DBLP:journals/pvldb/KangGBHZ20}. Given a query object \(q\) and a radius \(r\), the FRNN query becomes: ``For each object \(x\), is it a neighbor of \(q\) within distance \(r\)?'' SUPG-PT and SUPG-RT use the proxy model \(P\) to output a high score when \(x\) is a likely nearest neighbor, and the oracle model \(O\) for validation, where \(O(x) = 1\) indicates \(x\) is a true nearest neighbor. By combining proxy predictions with selective oracle validation, SUPG-PT and SUPG-RT ensure that the final query results meet predefined statistical guarantees for precision or recall, respectively.

\stitle{Top-K.}  
Probabilistic Top-K approximates nearest neighbors by returning the top-\(K\) nearest objects to the query, based on statistical guarantees from oracle predictions \cite{DBLP:conf/sigmod/LaiHLZ0K21}. We map the Top-K query directly from the FRNN query, by setting \(K\) to the number of neighbors of \(q\) within radius \(r\). Top-K relies on distributional assumptions over oracle predictions, which hold in many real-world scenarios, as shown in Fig. ~\ref{fig:dist}, where \(dist^X(x_i)\) represents the distance computed by \(X\) embeddings between \(x_i\) and \(q\), with \(X\in \{O,P\}\).

\subsubsection{Evaluation Measures}
\ \\
\stitle{\(\text{F}{\beta}\) score} is used to assess whether the true nearest neighbors have been identified by balancing precision and recall. It is defined as:
\begin{equation}
\text{F}{\beta} = (1+\beta^2)\frac{\text{P} \text{R}}{\beta^2 \text{P} + \text{R}}
\end{equation}
where \(\beta\) is a positive real factor. Two commonly used values for \(\beta\) are 2 which weights recall higher than precision, and 0.5, which weights recall lower than precision.

\stitle{Relative Error (RE)} measures the deviation of the estimated aggregate from the ground truth, calculated as:
\begin{equation}
\text{RE} = \frac{| \widetilde{\text{agg}_S} - \text{agg}_D |}{| \text{agg}_D |} \times 100\%
\end{equation}

\stitle{Cost} represents the overall time and resources required for answering an AQNN, accounting for both (i) the number of oracle and proxy calls made, denoted by \(C_O\) and \(C_P\) respectively, and (ii) the execution time to evaluate the AQNN, given the corresponding embeddings. Each oracle call is at least one order of magnitude more expensive than a proxy call in terms of time, computational resources, or monetary cost. Hence, the total cost is significantly affected by \(C_O\).

% \laks{we let cost mean time or comp. resources or money. correspondingly are we assuming that $C_P$ is an order of magnitude more efficient than $C_O$ w.r.t. each of these ``metrics"?}

\begin{table}[t]
\caption{Default number of oracle and proxy calls.}
% \laks{it seems like you are using $C_O$ and $C_P$ to denote the cost of one oracle or proxy call here. however, in later tables $C_O$ seems to stand for the \textit{number} of oracle calls. please check!!!!}
\label{tab:defult_proxy_oracle_cost}
% \resizebox{\columnwidth}{!}{
\small
\begin{tabular}{|c|c|c|c|c|}
\hline
     & eICU & MIMIC-III & Jigsaw  & Jackson \\ \hline
\(C_P\) & 1000 & 500       & 1000        & 1000    \\ \hline
\(C_O\) & 600  & 150       & 600       & 600     \\ \hline
\end{tabular}
% }
\end{table}

\subsubsection{Default Parameters}
We conduct experiments on the aggregate functions \(\mathtt{AVG}\), \(\mathtt{PCT}\), \(\mathtt{SUM}\), and \(\mathtt{VAR}\). To ensure a fair comparison \eat{of error and execution time among} with the baselines, we fix \(C_P\) and \(C_O\) as specified in Table \ref{tab:defult_proxy_oracle_cost}. One exception is Top-K, which incurs oracle costs \textit{dynamically} rather than using predefined values. Therefore, when comparing our algorithms with Top-K, we use the proxy cost from Table \ref{tab:defult_proxy_oracle_cost} but set the oracle cost to match Top-K's incurred oracle cost.

To better illustrate algorithm effectiveness in answering AQNNs, we modify the attribute values of the true nearest neighbors so that their aggregates stand out from those computed on the global dataset. Specifically, half of the values are generated by bootstrap resampling from the nearest neighbor attribute value distribution with added noise (small standard deviation), while the other half are generated by bootstrap resampling from the global attribute value distribution with added noise (larger standard deviation). To maintain valid attribute ranges, we enforce dataset-specific clipping and shuffle the values to eliminate order bias.

Moreover, guided by empirical findings in Fig. \ref{fig:beta_impact}, we set \(\beta = 0.5\) for \(\mathtt{AVG}\), \(\beta = 2\) for \(\mathtt{VAR}\) and \(\beta = 1\) for \(\mathtt{SUM}\). For \(\mathtt{PCT}\), SPRinT-C equalizes precision and recall so that \(\beta\) is irrelevant to nearest neighbor selection. We conduct experiments 30 times, as guided by the Central Limit Theorem (CLT), which ensures that the sample mean distribution approximates normality \cite{kwak2017central}. All reported measures are averaged over 10 random queries, with the target query \(q\) varying across runs to account for query-specific variability.

\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/beta/beta_impact_avg_eICU.png}
        \caption{eICU \(\mathtt{AVG}\)}
        \label{fig:beta_avg_eICU}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/beta/beta_impact_sum_eICU.png}
        \caption{eICU \(\mathtt{SUM}\)}
        \label{fig:beta_sum_eICU}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/beta/beta_impact_var_eICU.png}
        \caption{eICU \(\mathtt{VAR}\)}
        \label{fig:beta_var_eICU}
    \end{subfigure}
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/beta/beta_impact_avg_jackson.png}
        \caption{Jackson \(\mathtt{AVG}\)}
        \label{fig:beta_avg_jackson}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/beta/beta_impact_sum_jackson.png}
        \caption{Jackson \(\mathtt{SUM}\)}
        \label{fig:beta_sum_jackson}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/beta/beta_impact_var_jackson.png}
        \caption{Jackson \(\mathtt{VAR}\)}
        \label{fig:beta_var_jackson}
    \end{subfigure}
    \caption{Impact of the values of \(\beta\) to RE.}
    \label{fig:beta_impact}
\end{figure}

% \subsection{Summary of results}
% % \carrie{need a summary of the findings}
% \laks{nomally an early summary of results would be helpful. however, in this instance, i had difficulty verifying the claims in the summary. let's see whether we should keep the summary here or move it to the end. \\ 
% given that optimizing f1-score and equalizing precision and recall are just \textbf{a means} for our real objective of minimizing RE, we should get to RE early and open the hood on F1 and |P-R| once we have discussed RE and are ready to drill down and analyze what happened. IOW, let's not give primacy to these means.} 
% Our experiments indicate the superiority of SPRinT-V in terms of F1 score compared to baselines. In terms of RE, SPRinT-V and SPRinT-C consistently achieve smaller RE than PQE-RT and SUPG for \(\mathtt{AVG}\), \(\mathtt{SUM}\) and \(\mathtt{PCT}\). For \(\mathtt{VAR}\), PQE-RT and SUPG sometimes achieve lower RE due to retrieving one order of magnitude more neighbors. This smooths variance estimates but relies on artifacts rather than meaningful neighbor selection. Compared to Top-K, SPRinT-V achieves comparable RE with a consistent speedup of roughly 6-50x.

% With respect to sample size, we observe that the deviation between aggregates in \(S\) and \(D\) decreases: when the pilot sample size increases, SPRinT-V yields a higher F1 score and SPRinT-C a more equal precision and recall, which results in a lower relative error. When the radius increases, the difficulty level and the RE decrease. 

% SPRinT-V and SPRinT-C demonstrate competitive execution times across all datasets, which corroborates our time complexity analysis in previous sections. As expected, we also find that SPRinT-C is faster than SPRinT-V. 
% The stability of RE and scalability wrt \(|D|\) make SPRinT-V and SPRinT-C suitable for large datasets without compromising error or cost.

% We examine the applicability of AQNNs to hypothesis testing on neighborhood. We find that SPRinT-V and SPRinT-C consistently achieve higher accuracy for t-tests and proportion z-tests compared to PQE-RT and SUPG (PQE-PT is excluded because it
% fails to retrieve nearest neighbors for certain datasets). This finding is very promising, since while Top-K achieves comparable results, both SPRinT-V and SPRinT-C are much faster. 

\subsection{Examining Aggregation Error}\label{exp:error}
\subsubsection{Relative Error Performance} \label{subsubsec:RE}

\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/RE/RE_PQE_AVG.png}
        \caption{\(\mathtt{AVG}\)}
        \label{fig:RE_PQE_AVG}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/RE/RE_PQE_SUM.png}
        \caption{\(\mathtt{SUM}\)}
        \label{fig:RE_PQE_SUM}
    \end{subfigure}
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/RE/RE_PQE_VAR.png}
        \caption{\(\mathtt{VAR}\)}
        \label{fig:RE_PQE_VAR}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/RE/RE_PQE_PERCENT.png}
        \caption{\(\mathtt{PCT}\)}
        \label{fig:RE_PQE_PCT}
    \end{subfigure}
    \caption{RE when aggregation function is \(\mathtt{AVG}\) (a), \(\mathtt{SUM}\) (b), \(\mathtt{VAR}\) (c) or \(\mathtt{PCT}\) (d).}
    \label{fig:RE}
\end{figure}

Fig. \ref{fig:RE} shows the RE performance of \sprintv for \(\mathtt{AVG}\), \(\mathtt{SUM}\), and \(\mathtt{VAR}\), as well as \sprintc for \(\mathtt{PCT}\) across different datasets. We set a cut-off of 100 on RE.

\sprintv consistently achieves the lowest RE for all datasets across all baselines for \(\mathtt{AVG}\), \(\mathtt{SUM}\), and \(\mathtt{VAR}\) in Fig. \ref{fig:RE_PQE_AVG}, \ref{fig:RE_PQE_SUM}, and \ref{fig:RE_PQE_VAR}, respectively. For \(\mathtt{AVG}\), which is a normalized statistic, missing a few neighbors has a limited impact, making the advantage of our method relatively less pronounced. However, for \(\mathtt{SUM}\) and \(\mathtt{VAR}\), where both precision and recall are critical, \sprintv's performance demonstrates the effectiveness of our approach in achieving both. 

In Fig. \ref{fig:RE_PQE_PCT}, \sprintc also consistently achieves the lowest RE, demonstrating that our strategy of balancing precision and recall is effective in maintaining the number of nearest neighbors, which is crucial for count-based aggregation functions.


% Discussion: In these datasets, where the number of true nearest neighbors is very small (\(\leq 5\%\)), the pilot sample yields less reliable estimates of precision and recall, which can affect the count approximation. SUPG-PT's conservative nature by ensuring high precision in selecting nearest neighbors helps mitigate these estimation errors.

Unlike the above-mentioned algorithms, which operate under a predefined oracle budget, Top-K incurs oracle calls while searching for the nearest neighbors for \(K=ON_S\). Thus, to ensure a fair comparison, we evaluate \sprintv (resp. \sprintc) independently with Top-K by matching the pilot sample size to Top-K’s incurred oracle calls, and then compare the RE, execution time, and oracle calls (\(C_O\)) for \(\mathtt{AVG}\) (resp. \(\mathtt{PCT}\)) in Table \ref{tab:sprint_vs_topk_avg} (resp. \ref{tab:sprint_vs_topk_pct}). \sprintv and \sprintc achieves nearly the same RE as Top-K with a consistent speedup of roughly 7-746x. For example, on eICU, \sprintv completes in 1.33s with RE of 3.34, compared to Top-K's 73.88s and RE of 3.33.

\begin{table}[t]
\caption{RE of \sprintv and Top-K for \(\mathtt{AVG}\)}
\label{tab:sprint_vs_topk_avg}
% \resizebox{\columnwidth}{!}
% {%
\small
\begin{tabular}{|c|ccc|ccc|}
\hline
          & \multicolumn{3}{c|}{\sprintv}                                       & \multicolumn{3}{c|}{Top-K}                                        \\ \hline
          & \multicolumn{1}{c|}{RE}    & \multicolumn{1}{c|}{Time (s)} & $C_O$ & \multicolumn{1}{c|}{RE}   & \multicolumn{1}{c|}{Time (s)} & $C_O$ \\ \hline
eICU      & \multicolumn{1}{c|}{3.34} & \multicolumn{1}{c|}{1.33}     & 991  & \multicolumn{1}{c|}{3.33} & \multicolumn{1}{c|}{73.88}    & 991  \\ \hline
MIMIC-III & \multicolumn{1}{c|}{3.17} & \multicolumn{1}{c|}{0.51}     & 496  & \multicolumn{1}{c|}{3.23} & \multicolumn{1}{c|}{5.72}     & 496  \\ \hline
Jigsaw    & \multicolumn{1}{c|}{3.64} & \multicolumn{1}{c|}{0.21}     & 995  & \multicolumn{1}{c|}{3.61} & \multicolumn{1}{c|}{6.92}     & 995  \\ \hline
Jackson   & \multicolumn{1}{c|}{2.01} & \multicolumn{1}{c|}{1.77}     & 994  & \multicolumn{1}{c|}{2.00} & \multicolumn{1}{c|}{13.72}    & 994  \\ \hline
\end{tabular}
% }
\end{table}

\begin{table}[t]
\caption{RE of \sprintc and Top-K for \(\mathtt{PCT}\)}
\small
\label{tab:sprint_vs_topk_pct}
% \resizebox{\columnwidth}{!}
% {%
\begin{tabular}{|c|ccc|ccc|}
\hline
          & \multicolumn{3}{c|}{\sprintc}                                       & \multicolumn{3}{c|}{Top-K}                                        \\ \hline
          & \multicolumn{1}{c|}{RE}    & \multicolumn{1}{c|}{Time (s)} & $C_O$ & \multicolumn{1}{c|}{RE}   & \multicolumn{1}{c|}{Time (s)} & $C_O$ \\ \hline
eICU      & \multicolumn{1}{c|}{6.17} & \multicolumn{1}{c|}{0.10}     & 991  & \multicolumn{1}{c|}{6.15} & \multicolumn{1}{c|}{74.31}    & 991  \\ \hline
MIMIC-III & \multicolumn{1}{c|}{7.25} & \multicolumn{1}{c|}{0.04}     & 496  & \multicolumn{1}{c|}{5.67} & \multicolumn{1}{c|}{5.69}     & 496  \\ \hline
Jigsaw    & \multicolumn{1}{c|}{2.49} & \multicolumn{1}{c|}{0.09}     & 995  & \multicolumn{1}{c|}{2.49} & \multicolumn{1}{c|}{6.75}     & 995  \\ \hline
Jackson   & \multicolumn{1}{c|}{3.55} & \multicolumn{1}{c|}{0.20}     & 994  & \multicolumn{1}{c|}{3.56} & \multicolumn{1}{c|}{14.00}    & 994  \\ \hline
\end{tabular}
% }
\end{table}


\subsubsection{Impact of \(\text{F}{\beta}\) Score}\label{subsec:fbetascore}
For value-based aggregation functions, \(\text{F}{\beta}\) score provides valuable insights into the RE performance. Table \ref{tab:fbeta_score_AVG_nonPQA} reports the \(\text{F}{\beta}\) scores achieved by \sprintv and baselines\footnote{Top performance in bold in Tables \ref{tab:fbeta_score_AVG_nonPQA} - \ref{tab:time_PCT_PQE}.}.

\sprintv ranks the highest across all datasets, explaining its superior RE performance. 
However, a higher \(\text{F}{\beta}\) score does not always lead to a lower RE for all value-based aggregation functions. For instance, on Jigsaw, although PQE-RT achieves a higher \(\text{F}{\beta}\) score than PQE-PT, it results in worse RE performance for \(\mathtt{AVG}\) as shown in Fig. \ref{fig:RE_PQE_AVG}. This discrepancy arises because the \(\text{F}{\beta}\) score measures set interaction (precision and recall) but does not account for the distribution of attribute values among the selected nearest neighbors. Consequently, false positives with extreme attribute values or the omission of similar false negatives can significantly impact the aggregate, leading to higher RE despite a high \(\text{F}{\beta}\) score. Nonetheless, in scenarios where the distribution of nearest neighbor attribute values is unknown a priori, maximizing the \(\text{F}{\beta}\) score remains the most effective strategy for reducing RE.

To further illustrate the effectiveness of \sprintv in identifying the optimal \( t^* \) which maximizes \(\text{F}{\beta}\) score, we present two representative cases from the eICU and Jackson datasets, as shown in Fig. ~\ref{fig:fbeta_witht}. We observe that the value of \(t\) corresponding to the peak of the \(\text{F}{\beta}\) score curve is successfully identified as \( t^* \) by \sprintv.

\begin{table}[t]
\caption{\(\text{F}{\beta}_S\) achieved by \sprintv}
\small
\label{tab:fbeta_score_AVG_nonPQA}
% \resizebox{\columnwidth}{!}
% {%
\begin{tabular}{|c|c|c|c|c|c|}
\hline
          & \sprintv         & PQE-PT         & PQE-RT & SUPG-PT        & SUPG-RT \\ \hline
eICU      & \textbf{0.959} & 0.936  & 0.797  & 0.951   & 0.497   \\ \hline
MIMIC-III & \textbf{0.792} & 0.699  & 0.494  & 0.665   & 0.379   \\ \hline
Jigsaw  & \textbf{0.911} & 0.567  & 0.807  & 0.815   & 0.657   \\ \hline
Jackson   & \textbf{0.943} & 0.928  & 0.600  & 0.909   & 0.382   \\ \hline
\end{tabular}
% }
\end{table}

\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/fbetaScore/fbeta_fig_eICU.png}
        \caption{eICU}
        \label{fig:fbeta_eICU_witht}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/fbetaScore/fbeta_fig_Jackson.png}
        \caption{Jackson}
        \label{fig:fbeta_Jackson_witht}
    \end{subfigure}
    \caption{$\text{F}{\beta}$ score curves for eICU (a) and Jackson (b) datasets}
    \label{fig:fbeta_witht}
\end{figure}

\subsubsection{Impact of Equalizing Precision and Recall}\label{subsec:prdifference}

For count-based aggregation functions, such as \(\mathtt{PCT}\), the absolute difference between precision and recall in \(S\) achieved by each algorithm effectively explains their RE performance. Table \ref{tab:PR_Difference_PCT_nonPQA} reports this absolute difference for \sprintc and baselines. \sprintc consistently achieves the lowest precision-recall difference for all datasets, which justifies its superior RE performance.

% Meanwhile, SUPG-PT maintains the smallest precision-recall difference for Amazon-E and Amazon-HH, resulting in its lowest RE in Fig.~\ref{fig:RE_PQE_PCT}. On these datasets, we attribute \sprintc's performance gap to the insufficient number of nearest neighbors in the pilot sample. With too few nearest neighbors in \(S_p\), the estimated precision and recall do not accurately reflect their true values in \(S\), preventing our algorithm from achieving an even smaller precision-recall difference.


\begin{table}[t]
\caption{\(|\text{P}_S-\text{R}_S|\) achieved by \sprintc}
\label{tab:PR_Difference_PCT_nonPQA}
\small
% \resizebox{\columnwidth}{!}
% {%
\begin{tabular}{|c|c|c|c|c|c|}
\hline
          & \sprintc               & PQE-PT         & PQE-RT      & SUPG-PT        & SUPG-RT \\ \hline
eICU      & \textbf{0.021} & 0.208  & 0.216  & 0.182   & 0.511   \\ \hline
MIMIC-III & \textbf{0.042} & 0.552  & 0.499  & 0.709   & 0.666   \\ \hline
Jigsaw  & \textbf{0.019} & 0.580  & 0.219  & 0.533   & 0.377   \\ \hline
Jackson   & \textbf{0.019} & 0.024  & 0.451  & 0.329   & 0.661   \\ \hline
\end{tabular}
% }
\end{table}

\subsubsection{Impact of Sample and Pilot Sample Complexity}\label{subsubsec:exp_s_s_p}

\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/sample_complexity/avg_eICU_s.png}
        \caption{\(s\) vs \(\mathtt{AVG}_S\)}
        \label{fig:avg_s}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/sample_complexity/pct_eICU_s.png}
        \caption{\(s\) vs \(\mathtt{PCT}_S\)}
        \label{fig:pct_s}
    \end{subfigure}
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/sample_complexity/avg_eICU_s_p_fbeta.png}
        \caption{\sprintv: \(s_p\) vs \(\text{F}{\beta}_S\)}
        \label{fig:avg_s_p_fbeta}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/sample_complexity/pct_eICU_s_p_pr.png}
        \caption{\sprintc: \(s_p\) vs \(|\text{P}_S-\text{R}_S|\)}
        \label{fig:pct_s_p_pr}
    \end{subfigure}
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/sample_complexity/avg_eICU_s_p_re.png}
        \caption{\sprintv: \(s_p\) vs RE}
        \label{fig:avg_s_p_re}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/sample_complexity/pct_eICU_s_p_re.png}
        \caption{\sprintc: \(s_p\) vs RE}
        \label{fig:pct_s_p_re}
    \end{subfigure}
    \caption{Impact of \(s_p\) and \(s\) of \sprint for \(\mathtt{AVG}\) (a, c, e) and \(\mathtt{PCT}\) (b, d, f) using the eICU dataset.}
    \label{fig:sample_complexity}
\end{figure}

In \S~\ref{sec:algo}, we present the minimum sample and pilot sample size required for a confidence interval of (\(1-\alpha\)). Here, we empirically illustrate their impact on RE for \(\mathtt{AVG}\) (similar to \(\mathtt{SUM}\) and \(\mathtt{VAR}\)) and \(\mathtt{PCT}\) using the eICU dataset in Fig. \ref{fig:sample_complexity}. For the pilot sample complexity analysis (Fig. \ref{fig:avg_s_p_fbeta}-\ref{fig:pct_s_p_re}), the sample size is fixed at 1000.

In Fig. \ref{fig:avg_s} and \ref{fig:pct_s}, as \(s\) increases, the aggregate in \(S\) converges toward the true aggregate in \(D\). Larger sample sizes reduce sampling bias, leading to more accurate approximations of AQNNs in \(S\). However, beyond a certain threshold (e.g., \(s > 1000\) in Fig. \ref{fig:pct_s}), the improvement diminishes.

Fig. \ref{fig:avg_s_p_fbeta} and \ref{fig:avg_s_p_re} illustrate the impact of \(s_p\) using \sprintv. In Fig. \ref{fig:avg_s_p_fbeta}, increasing \(s_p\) consistently improves \(\text{F}{\beta}_S\), which corresponds to the RE reduction observed in Fig. \ref{fig:avg_s_p_re}. As discussed previously, a higher \(\text{F}{\beta}\) score does not always lead to lower RE; however, we observe a overall decreasing trend. Fig. \ref{fig:pct_s_p_pr} and \ref{fig:pct_s_p_re} show the impact of \(s_p\) using \sprintc. In Fig. \ref{fig:pct_s_p_pr}, the absolute difference between precision and recall in \(S\) decreases as \(s_p\) increases. Fig. \ref{fig:pct_s_p_re} further illustrates the impact of equalizing precision and recall in \(S\) on RE. As \(s_p\) increases, RE decreases, stabilizing once \(s_p\) is sufficiently large. The initial rise in Fig. \ref{fig:pct_s_p_pr} occurs due to an insufficient number of nearest neighbors in the pilot sample, which prevents accurate monitoring of precision and recall in \(S\). In other words, when \(s_p\) is too small, the optimal \(t^*\) for selecting nearest neighbors becomes unstable, leading to inconsistent neighbor selection across different runs. This variation introduces fluctuations in AQNN results, which in turn affect RE. Thus, our analysis aligns with the theoretical findings, indicating that a sufficiently large \(s_p\) is necessary for reliable monitoring of precision and recall in \(S\).


We also ran experiments on Amazon retail datasets (not reported here due to space constraints) where nearest neighborhoods are highly sparse. As a result, the pilot sample yielded less reliable estimates of precision and recall. Our results corroborate that the poor estimation of \( t^* \) prevents \sprintv and \sprintc from consistently achieving lower RE. Interestingly, algorithms that prioritize precision, such as SUPG-PT and PQE-PT, exhibit better RE performance due to their conservative nearest neighbor selection strategy.  This calls for developing adaptive strategies that account for sparsity in neighbor distributions, ensuring robust precision-recall balancing even in highly sparse settings.

\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/level_of_difficulty/avg_eICU_fbeta.png}
        \caption{\sprintv: \(r\) vs \(\text{F}{\beta}_S\)}
        \label{fig:avg_rho_fbeta}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/level_of_difficulty/pct_eICU_pr.png}
        \caption{\sprintc: \(r\) vs \(|\text{P}_S-\text{R}_S|\)}
        \label{fig:pct_rho_pr}
    \end{subfigure}
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/level_of_difficulty/avg_eICU_re.png}
        \caption{\sprintv: \(r\) vs RE}
        \label{fig:avg_rho_re}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/level_of_difficulty/pct_eICU_re.png}
        \caption{\sprintc: \(r\) vs RE}
        \label{fig:pct_rho_re}
    \end{subfigure}
    \caption{Impact of radius on the performance of \sprintv for \(\mathtt{AVG}\) (a, c) and \sprintc for \(\mathtt{PCT}\) (b, d) using the eICU dataset.}
    \label{fig:level_of_difficulty}
\end{figure}

\subsubsection{Impact of Radius}
Fig. \ref{fig:level_of_difficulty} illustrates how varying \(r\) from 0.4 to 0.9 impacts the performance of \sprintv for \(\mathtt{AVG}\) and \sprintc for \(\mathtt{PCT}\) using the eICU dataset. As \(r\) increases, the true proportion of nearest neighbors, denoted by \(\rho\), grows because more data points are included in the set of nearest neighbors. The values of \(\rho\) are visualized as bars in Fig. \ref{fig:avg_rho_fbeta} and \ref{fig:pct_rho_pr}, with the y-axis on the right.

For \sprintv, Fig. \ref{fig:avg_rho_fbeta} shows the impact of \(r\) on \(\text{F}{\beta}_S\). \(\text{F}{\beta}_S\) fluctuates when \(r\) is small but increases as \(r\) grows. Based on our analysis of pilot sample complexity, this initial fluctuation results from the scarcity of nearest neighbors when \(r\) is very small (e.g., \(\rho \leq 5\%\)). As \(r\) increases, \(\text{F}{\beta}_S\) becomes more stable. The impact of \(\text{F}{\beta}_S\) on RE is shown in Fig. \ref{fig:avg_rho_re}. For \sprintc, similarly, Fig. \ref{fig:pct_rho_pr} shows that \(|\text{P}_S - \text{R}_S|\) fluctuates before decreasing as \(r\) increases. Consequently, RE follows a decreasing pattern in \ref{fig:pct_rho_re}.


\subsection{Examining Cost}\label{exp:cost}
\subsubsection{Time}
We compare the execution time of \sprintv and \sprintc with baselines, assuming all embeddings are precomputed. Tables~\ref{tab:time_AVG_PQE} and \ref{tab:time_PCT_PQE} show the results for \(\mathtt{AVG}\) (similar to \(\mathtt{SUM}\) and \(\mathtt{VAR}\)) and \(\mathtt{PCT}\).

For \(\mathtt{AVG}\), \sprintv achieves competitive execution times across datasets. Given the algorithm's complexity of \(O(s^2)\), execution time increases quadratically with \(s\), as observed in MIMIC-III and eICU where \(s\) increases from 500 to 1000. Notably, \sprintv is substantially faster than Top-K. Similarly, for \(\mathtt{PCT}\), \sprintc follows the same performance trends as the baselines.

\begin{table}[t]
\caption{Time (sec) for \(\mathtt{AVG}\) (similar as \(\mathtt{SUM}\), and \(\mathtt{VAR}\)) AQNNs}
\small
\label{tab:time_AVG_PQE}
\resizebox{\columnwidth}{!}
{%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
          & \sprint & PQE-PT         & PQE-RT & SUPG-PT        & SUPG-RT        & Top-K      \\ \hline
eICU      & 0.95   & 0.94   & 0.90   & 0.95          & \textbf{0.81} & 0.95  \\ \hline
MIMIC-III & 0.07   & 0.03   & 0.02   & 0.02          & \textbf{0.02} & 5.72  \\ \hline
Jigsaw  & 0.21   & 0.08   & 0.07   & \textbf{0.01} & 0.02          & 6.92  \\ \hline
Jackson   & 0.35   & 0.07   & 0.09   & \textbf{0.02} & 0.03          & 13.72 \\ \hline
\end{tabular}
}
\end{table}

\begin{table}[t]
\caption{Time (sec) for \(\mathtt{PCT}\) AQNNs}
\small
\label{tab:time_PCT_PQE}
\resizebox{\columnwidth}{!}
{%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
          & \sprint & PQE-PT         & PQE-RT & SUPG-PT & SUPG-RT       & Top-K       \\ \hline
eICU      & 0.90   & 0.76   & 0.73   & 0.78          & \textbf{0.60} & 0.91  \\ \hline
MIMIC-III & 0.03   & 0.03   & 0.03   & 0.02          & \textbf{0.02} & 5.69  \\ \hline
Jigsaw  & 0.11   & 0.09   & 0.08   & \textbf{0.02} & 0.02          & 6.75  \\ \hline
Jackson   & 0.19   & 0.07   & 0.09   & 0.02          & \textbf{0.02} & 14.00 \\ \hline
\end{tabular}
}
\end{table}



\subsubsection{Scalability}
We evaluate the scalability of \sprintv and \sprintc using the semi-synthetic Jackson dataset. The original dataset is scaled from \(10^4\) to \(10^7\), and we assess execution time and RE for \(\mathtt{AVG}\) and \(\mathtt{PCT}\). We fix (\(s\) and \(s_p\)) at their default values in Fig.~\ref{fig:scalability}.

Fig. \ref{fig:avg_Jackson_scalability} and \ref{fig:pct_Jackson_scalability} show that the execution times for \sprintv and \sprintc remain stable as \(|D|\) increases. Both theoretically and empirically, their execution time is independent of \(|D|\). Also, Fig. \ref{fig:avg_Jackson_scalability_re} and \ref{fig:pct_Jackson_scalability_re} demonstrate that RE remains stable across different dataset sizes. Therefore, with sufficiently large \(s\) and \(s_p\), the error performance of our proposed algorithms is also independent of \(|D|\).



\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/scalability/avg_Jackson_scalability.png}
        \caption{\sprintv for \(\mathtt{AVG}\)}
        \label{fig:avg_Jackson_scalability}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/scalability/pct_Jackson_scalability.png}
        \caption{\sprintc for \(\mathtt{PCT}\)}
        \label{fig:pct_Jackson_scalability}
    \end{subfigure}
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/scalability/avg_Jackson_scalability_re.png}
        \caption{\sprintv for \(\mathtt{AVG}\)}
        \label{fig:avg_Jackson_scalability_re}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/scalability/pct_Jackson_scalability_re.png}
        \caption{\sprintc for \(\mathtt{PCT}\)}
        \label{fig:pct_Jackson_scalability_re}
    \end{subfigure}
    \caption{Impact of dataset size \(|D|\) on the execution time and RE of \sprintv for \(\mathtt{AVG}\) (a, c) and \sprintc for \(\mathtt{PCT}\) (b, d) using Jackson dataset.}
    \label{fig:scalability}
\end{figure}



\subsection{Application to Hypothesis Testing}\label{exp:HT}
AQNNs have a natural application in hypothesis testing because they provide aggregate statistics about the local neighborhood of a query object, which are crucial inputs for statistical inference. Hypothesis testing is a statistical method used to evaluate whether there is sufficient evidence in a sample to support or reject a specific claim about a population. When the population of interest is the neighborhood of a query object, this nearest neighbor hypothesis (NNH) is made based on an AQNN. Hence, answering the AQNN is an essential intermediate step. Accurate testing of NNH can help practitioners uncover meaningful patterns, validate assumptions, and make reliable predictions. For example, it allows clinicians to determine whether the average blood pressure of patients similar to a given individual is significantly higher/lower than normal, guiding personalized treatment plans.

To evaluate \sprintv and \sprintc's effectiveness in hypothesis testing, we perform one-sample t-tests and proportion z-tests, respectively. Both tests use a hypothesis of the form:
\[\text{agg}(\text{NN}_D(q, r)[\texttt{attr}]) \ \text{op} \ c\] 
where \(\text{op} \in \{\leq, \geq, \neq\}\) and \(c \in \mathbb{R}\) is a constant. Each hypothesis comes in pairs: a null hypothesis (\(H_0\)) and an alternative hypothesis (\(H_a\)). For instance, \(H_0\) might state that ``\textit{the average systolic blood pressure of patients similar to \(q\) equals 140 mmHg},'' while \(H_a\) asserts that ``\textit{the average is at most 140 mmHg}.'' To measure performance, we use \textbf{accuracy}, defined as the proportion of hypothesis testing results from approximate aggregates that match those derived from the true aggregate:
\[
\text{Accuracy} = \frac{1}{k}\sum_{i=1}^k \mathbbm{1}_{H(\text{agg}_D) == H(\widetilde{\text{agg}_S})}
\]
where \(k = 30\) is the number of samples. The constant \(c\) is set as a factor of the ground truth value, such that \(c = \text{factor} \times \text{ground truth}\), ranging from 0.5 to 1.5 in steps of 0.05. Accuracy is averaged over 10 random queries and evaluated under two operators (\(\geq\) and \(\leq\)).

Fig. \ref{fig:HT} demonstrates the accuracy of hypothesis testing across different datasets and algorithms. \sprintv and \sprintc consistently achieve the highest accuracy for t-tests and proportion z-tests (Fig. \ref{fig:HT_PQE_AVG} and \ref{fig:HT_PQE_PCT}, respectively). This is attributed to their accurate approximation of aggregates. The accuracy hovering around 0.5 imply that the algorithm cannot consistently determine whether the true aggregate supports the null or alternative hypothesis. Instead, their results appear to be random guesses between the two outcomes due to poor aggregate estimation. These results highlight that \sprintv and \sprintc are both cost-effective and efficient, enabling reliable and accurate testing of NNH for practical decision-making.


\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/HT/HT_PQE_AVG.png}
        \caption{t-test}
        \label{fig:HT_PQE_AVG}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/HT/HT_PQE_PCT.png}
        \caption{Proportion z-test}
        \label{fig:HT_PQE_PCT}
    \end{subfigure}
    \caption{Hypothesis testing performance for \(\mathtt{AVG}\) (a) and \(\mathtt{PCT}\) (b) AQNNs.}
    \label{fig:HT}
\end{figure}
