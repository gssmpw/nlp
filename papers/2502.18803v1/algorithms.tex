\section{Our Solution}\label{sec:algo}
In this section, we first discuss our objectives and challenges (\S~\ref{sec:challenges}), then describe the Sampler with Precision-Recall in Target (\sprint) framework for answering AQNNs (\S~\ref{sec:sprint}). For value- and count-based aggregation functions, we introduce \sprintv (\S~\ref{subsec:sprint_v}) and \sprintc (\S~\ref{subsec:sprint_c}), which respectively maximize \(\text{F}{\beta}\) score and equalize recall and precision. We then theoretically analyze our framework (\S~\ref{sec:theory}) where we derive error bounds on approximate aggregates and the minimal sample and pilot sample size required to ensure reliable aggregation approximation. 

\begin{table}[t!]
\begin{small}
\centering
\caption{Table of Notations}
\begin{tabular}{c|c}
\hline
\textbf{Notation} & \textbf{Description} \\ \hline
\( D \) & the dataset \\ 
\( O \) & oracle model \\ 
\( P \) & proxy model \\ 
\( q \) & query point \\ 
\( S \) & random sample of \( D \) with size \(s\) \\ 
% \( s \) & size of \( S \) \\
\( ON_D \) & oracle neighbors in \( D \) \\ 
\( ON_S \) & oracle neighbors in \( S \) \\ 
\( \sprint_S \) & $\sprint$ neighbors in \(S\) \\ 
\( S_{p} \) & pilot sample in \( S \) with size \(s_p\) \\
% \( s_{p} \) & size of \( S_{p} \) \\ 
\( t^* \) & optimal recall target \\ 
\(\text{R}_S\) & recall in \(S\) \\
\(\text{P}_S\) & precision in \(S\) \\
\(\text{F}{\beta}_S\) & \(\text{F}{\beta}\) score in \(S\) \\
% \( \widetilde{\text{prop}_S} \) & approximated proportion by SPRinT in \(S\) \\ 
% \( \widetilde{\text{agg}_S} \) & approximate aggregate by SPRinT in \(S\) \\ 
\hline
\end{tabular}
\end{small}
\end{table}




\subsection{Objectives and Challenges}
\label{sec:challenges}
Recall the  objectives in answering an AQNN: (\textbf{O1}) ensure a low approximation error %between the approximate aggregate and the true aggregate in \(D\); 
and (\textbf{O2}) minimize computational cost. 

To achieve \textbf{O1} for value-based aggregation functions, the key challenge lies in ensuring that the retrieved nearest neighbors are both correct and complete, with high likelihood, formally expressed as achieving high precision and recall. Precision measures the proportion of retrieved neighbors that are correct, while recall measures the proportion of true nearest neighbors that are retrieved, which is essential for completeness. As shown in Fig. \ref{numericalExample}, both low-precision (Case 1) and low-recall (Case 2) scenarios introduce aggregation error. However, precision and recall often conflict. A higher precision may exclude true neighbors, lowering recall, whereas a higher recall may introduce incorrect neighbors, reducing precision.
Additionally, Case 1 results in a significantly larger error for \(\mathtt{AVG}\) and \(\mathtt{VAR}\). Assuming the value distributions of the true nearest neighbors and non-nearest neighbors are consistent within themselves but differ from each other, precision is generally more important than recall for \textit{value-based} aggregation functions, including \(\mathtt{AVG}\), \(\mathtt{VAR}\), and \(\mathtt{SUM}\), as it prevents outliers and noisy values from distorting AQNN results. Therefore, striking the right balance between these two metrics is key to reducing error between the approximate and true aggregate in \(D\). On the other hand, count-based aggregation functions are more sensitive to the imbalances between precision and recall. If the false positives and false negatives are equal, we can prevent systematic over- or under-estimation of the aggregate for count-based aggregation functions.

% In fact, maximization of precision involves stricter criteria for neighbor selection, which excludes incorrect neighbors but risks omitting true neighbors, thus reducing recall. Conversely, maximizing recall tends to relax the selection criteria to capture all possible true neighbors, increasing the likelihood of retrieving incorrect neighbors, hence reducing precision. 
% Therefore, striking the right balance between these two metrics is key to reducing error between the approximate and true aggregate in \(D\). 
% \laks{isn't the tradeoff b/w precision and recall well known? couldn't we just state this in one line and free up the space for sth more important needed elsewhere?}

Achieving \textbf{O2}, however, inherently conflicts with \textbf{O1}. First, distances derived from oracle embeddings are highly accurate, often yielding the highest precision and recall for nearest neighbor selection, but are computationally expensive. In contrast, proxy embeddings are computationally cheaper but reduce the accuracy of neighborhood prediction, leading to lower precision and recall.
Second, verifying precision and recall for selected nearest neighbors requires identifying true nearest neighbors, which depends on expensive oracle embeddings. This exacerbates the trade-off between two objectives.

{\em Our overarching goal is to strike a balance between cost and error by optimizing precision and recall in a manner that is tailored to different aggregation functions.}

To address \textbf{O1} for \textit{value-based} aggregation functions, we propose to optimize \textit{precision and recall} by maximizing \(\text{F}{\beta}\) score, a weighted harmonic mean of precision and recall, for the retrieved nearest neighbors. Here, \(\beta < 1\) weights recall lower than precision while \(\beta > 1\) weights recall higher than precision. On the other hand, \textit{count-based} aggregation functions require a different approach. For these functions, accurately approximating the number of nearest neighbors is critical to ensure low error. Hence, equalizing precision and recall is essential to prevent systematic over- or under-estimation. For instance, when recall is higher than precision, 
false positives are higher than false positives. It results in an over-estimation of the number of nearest neighbors because too many irrelevant neighbors are included. Conversely, when precision is higher than recall, too many true neighbors are missed, leading to an under-estimation. Thus, achieving \textbf{O1} entails optimizing both precision and recall in sync. However, existing literature \cite{DBLP:journals/pvldb/KangGBHZ20,DujianPQA} typically focuses on optimizing either precision or recall in isolation, and does not optimize both or achieve a balance thereof. 

To address \textbf{O2}, we first draw a random sample to reduce the computational cost. Random sampling is effective at preserving the overall distribution of the data by ensuring that each data point in \(D\) has an equal probability of being selected. As a result, the nearest neighbor found in the sample \(S\) can serve as an effective approximation for the nearest neighbor in \(D\), enabling us to approximate the aggregate over \(D\) without processing the entire dataset. To further reduce the cost, 
we additionally draw a pilot sample \(S_p\) from \(S\) and only call the expensive oracle on the data in \(S_p\), while using a computationally efficient proxy on all data in \(S\). The pilot sample serves two key purposes when finding nearest neighbors in \(S\). First, with oracle embeddings for data in \(S_p\), we can directly verify the precision and recall within this subset. Due to the properties of the random sampler (formal proof in Appendix~\ref{subsec:s_p_in_appendix}), the precision and recall measured in \(S_p\) reliably approximate those in the full sample \(S\). Consequently, we can monitor and optimize the precision and recall in \(S\) based on their performance in \(S_p\) (\(\textbf{O1}\)). Second, the true nearest neighbors in \(S_p\) can be used to refine the retrieved neighbor set in \(S\). This refinement further improves the \(\text{F}{\beta}\) score and equalizes precision and recall for value- and count-based aggregation functions, respectively (\(\textbf{O1}\)). 
% \laks{what is missing here is a similar blurb for count-based agg. even if we need to do nothing special for count-based agg to make it work correctly, we need a line here to complete the picture.}

\subsection{The \sprint Framework}
\label{sec:sprint}
Fig. ~\ref{fig:framework} illustrates \sprint, our framework for answering AQNNs. It consists of three main steps: (1) sampling, (2) nearest neighbor refinement, and (3) aggregation. First, given a dataset \(D\) and a desired sample size \(s\), we draw a random sample \(S\) of size \(s\) using a random sampler. Since \(s \ll |D|\), this step significantly reduces the overall computational cost (\(\mathbf{O2}\)). In the second step, we approximate AQNN results on \(S\) using a combination of proxy and oracle embeddings. From \(S\), we randomly draw a smaller pilot sample \(S_p\) of size \(s_p\); while proxy models are applied to all data in \(S\), the expensive oracle models are invoked only on data in \(S_p\). It further reduces the oracle cost (\(\mathbf{O2}\)). By computing precision and recall on \(S_p\), we can monitor the performance of different nearest neighbor selection strategies in \(S\) to verify whether we achieve the maximum \(\text{F}\beta\) score or equal precision and recall (\(\mathbf{O1}\)). In the third step, after selecting the nearest neighbors in \(S\), we apply the specified aggregation function (e.g., \(\mathtt{AVG}\), \(\mathtt{SUM}\), \(\mathtt{PCT}\)) to the target attribute of these neighbors. The final output is the approximate aggregate.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% SPRinT-V %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{\sprintv} \label{subsec:sprint_v}

\begin{algorithm}[t]
\caption{\sprintv}
\label{algo:sprint_v}
\begin{algorithmic}[1] 
\State \textbf{Input:} \( S \), \( S_p \), \( O \), \( P \), \( q \), \( \beta \), \( \omega_V \)
\State \textbf{Output:} \(\sprint_S\)

\State \(t_{\text{min}} \leftarrow 0\), \(t_{\text{max}} \leftarrow 1\) % \Comment{t: recall target}

\While{$t_{\text{max}} - t_{\text{min}} > \omega_V$}
    \State \(t_1 \leftarrow t_{\text{min}} + \frac{t_{\text{max}} - t_{\text{min}}}{3}\), \(t_2 \leftarrow t_{\text{high}} - \frac{t_{\text{max}} - t_{\text{min}}}{3}\)
    
    \State \(NN(t_1) \leftarrow \text{PQE-PT}(S_p, O, P, q, t_1)\)
    \State \(NN(t_2) \leftarrow \text{PQE-PT}(S_p, O, P, q, t_2)\)
    
    % calculation of precision and recall when t1 and t2
    % \State \(\text{R}_{S_p}(t_1) \leftarrow \frac{|NN(t_1) \cap ON_D \cap S_p|}{|ON_D \cap S_p|}\)
    % \State \(\text{P}_{S_p}(t_1) \leftarrow \frac{|NN(t_1) \cap ON_D \cap S_p|}{|NN(t_1) \cap S_p|}\)

    % \State \(\text{R}_{S_p}(t_2) \leftarrow \frac{|NN(t_2) \cap ON_D \cap S_p|}{|ON_D \cap S_p|}\)
    % \State \(\text{P}_{S_p}(t_2) \leftarrow \frac{|NN(t_2) \cap ON_D \cap S_p|}{|NN(t_2) \cap S_p|}\)
    
    \State \(\text{F}{\beta}_{S_p}(t_1) \leftarrow (1+\beta^2)\frac{\text{P}_{S_p}(t_1)  \text{R}_{S_p}(t_1)}{\beta^2\text{P}_{S_p}(t_1) + \text{R}_{S_p}(t_1)}\)
    \State \(\text{F}{\beta}_{S_p}(t_2) \leftarrow (1+\beta^2)\frac{\text{P}_{S_p}(t_2)  \text{R}_{S_p}(t_2)}{\beta^2\text{P}_{S_p}(t_2) + \text{R}_{S_p}(t_2)}\)

    \If{\(\text{F}{\beta}_{S_p}(t_1) > \text{F}{\beta}_{S_p}(t_2)\)}
        \State \(t_{\text{max}} \leftarrow t_2\)
    \Else
        \State \(t_{\text{min}} \leftarrow t_1\)
    \EndIf
\EndWhile
\State \(t^* \leftarrow (t_{\text{min}} + t_{\text{max}})/2\)

\State \Return \(\text{PQE-PT}(S, O, P, q, t^*)\)
\end{algorithmic}
\end{algorithm}

\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/fbetaScore/fbeta_fig_eICU_ori.png}
        \caption{eICU}
        \label{fig:unimodal-eICU}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/fbetaScore/fbeta_fig_Jackson_ori.png}
        \caption{Jackson}
        \label{fig:unimodal-J}
    \end{subfigure}
    \caption{F1 score curves for eICU (a) and Jackson (b) datasets}
    \label{fig:fbeta-empirical}
\end{figure}


For value-based aggregation functions, we propose \sprintv to find nearest neighbors in the second step. 
% To reduce computational cost, \sprintv primarily relies on proxy embeddings of data in \(S\). 
Our goal is to identify neighbors which maximize the \(\text{F}{\beta}\) score in \(S\) with high probability. However, directly optimizing the \(\text{F}{\beta}\) score is challenging because it depends non-linearly on both precision and recall, with improvements in one often coming at the expense of the other. Instead, we leverage the insight that if a nearest neighbor search algorithm can \textit{guarantee} one metric while \textit{maximizing} another, it can effectively navigate this trade-off. PQE-PT and PQE-RT are state-of-the-art approaches that meet this requirement by ensuring that the selected neighbors achieve a given precision (resp. recall) target with high probability while maximizing recall (resp. precision) \cite{DujianPQA}. Since accurate aggregate estimates in value-based functions are more sensitive to precision, where false positives can significantly distort the result, we favor PQE-PT, which prioritizes achieving a precision target before maximizing recall, over PQE-RT. In practical settings, as illustrated in Fig.~\ref{fig:fbeta-empirical}, the \(\text{F}{\beta}\) score is found to be unimodal when using PQE-PT. This unimodality allows us to efficiently search for the optimal \(\text{F}{\beta}\) score via ternary search. Rather than directly specifying an \(\text{F}{\beta}\) score target, we iteratively explore different precision targets until we find the optimal precision target \(t^*\) that maximizes the \(\text{F}{\beta}\) score of the selected nearest neighbors. Consequently, we integrate PQE-PT as a subroutine in \sprintv to identify the nearest neighbors in \(S\).

As shown in Algorithm~\ref{algo:sprint_v}, \sprintv takes as inputs \(S\), \(S_p\), \(O\), \(P\), \(q\), \(\beta\), and a tolerance \(\omega_V\) and outputs the \sprint neighbors in \(S\). The algorithm first initializes the range of precision targets with \(t_{\text{min}} = 0\) and \(t_{\text{max}} = 1\) (Line 3) and iteratively narrows this range until \(t_{\text{max}}-t_{\text{min}} < \omega_V\) (Line 4). In each iteration, two candidate thresholds, \(t_1\) and \(t_2\), are calculated by dividing the range into thirds (Line 5). The PQE-PT subroutine is then called with \(t_1\) and \(t_2\) to retrieve the nearest neighbors in \(S_p\) (Lines 6-7). Using these selected neighbors and the oracle neighbors in \(S_p\), the algorithm computes precision (\(\text{P}_{S_p}\)), recall (\(\text{R}_{S_p}\)), and \(\text{F}{\beta}\) scores for both \(t_1\) and \(t_2\) (Lines 8-9). Next, \sprintv updates the search range by retaining the more promising region of the precision target (Lines 10-14). 
% \laks{in this para references to line numbers seem off. pls fix.} 
Once the search range converges, \(t^*\) is computed as the midpoint of \(t_{\text{min}}\) and \(t_{\text{max}}\) (Line 16). Finally, PQE-PT is called one last time with \(t^*\) to retrieve the \sprint neighbors in \(S\) (Line 17), which are then used to compute the aggregation of an AQNN query. 
% \laks{wonder if we should add a line to sprint-v making it return the aggregate value. similar comment for sprint-c }
% \laks{the mismatch of line numbers continues.}  

\subsubsection{\sprintc} \label{subsec:sprint_c}
For count-based aggregation functions, we propose \sprintc to find nearest neighbors. Similar to \sprintv, \sprintc leverages PQE-PT to iteratively adjust the precision target until the precision of the \sprint neighbors in \(S_p\) matches the recall. The intuition is that balancing false positives and false negatives leads to an unbiased estimation of the proportion of nearest neighbors. Specifically, we define the approximated proportion %\(\widetilde{\mathtt{PCT}_S}\) 
as:
\begin{align}
\hspace*{-2ex} 
\widetilde{\mathtt{PCT}_S} & = \frac{|\sprint_S|}{s} = \frac{|(\sprint_S\cap ON_S)\cup (\sprint_S\setminus ON_S)|}{s} 
\label{eq:approx_PCT_S}
\end{align}
where $\sprint_X$ (resp. $ON_X$) denotes the nearest neighbors found by \sprint (resp. true nearest neighbors) in dataset $X$, $X \in \{S, D\}$. When the number of false positives \((\sprint_S \setminus ON_S)\) equals the number of false negatives \((ON_S \setminus \sprint_S)\), \(\widetilde{\mathtt{PCT}_S}\) accurately reflects \(\mathtt{PCT}_S\). This condition is satisfied when precision equals recall. Formally, the recall and precision of the \sprint neighbors in \(S\) are:
%\begin{align}
    $\text{R}_S = \frac{|\sprint_S \cap ON_D|}{|ON_S|}$, and 
%\end{align}
%\begin{align}
    $\text{P}_S = \frac{|\sprint_S \cap ON_D|}{|\sprint_S|}$.
%\end{align}
By setting \(\text{P}_S = \text{R}_S\), we can infer 
%\begin{align}
    %\frac{|\text{SPRinT}_S \cap ON_D|}{|\text{SPRinT}_S|} & = \frac{|\text{SPRinT}_S \cap ON_D|}{|ON_S|} \\
    $|\sprint_S \setminus ON_S|  = |ON_S \setminus \sprint_S|$,
%\end{align}
i.e., the numbers of false positives and false negatives are balanced. Hence, \(\widetilde{\mathtt{PCT}_S}\) provides an unbiased estimation of \(\mathtt{PCT}_S\). 

As shown in Algorithm \ref{algo:sprint_c}, \sprintc takes the same inputs as \sprintv and outputs the \sprint neighbors in \(S\). Since there is an inverse relationship between precision and recall, where a higher precision target leads to a lower recall, a binary search is employed to find the optimal precision target \(t^*\) which equalizes precision and recall. \sprintc initializes the search range with \(t_{\text{min}} = 0\) and \(t_{\text{max}} = 1\) (Line 3) and iteratively narrows this range until the difference between the recall \(R_{S_p}\) and precision \(P_{S_p}\) in the pilot sample is within a tolerance \(\omega_C\) (Line 4). In each iteration, a candidate precision target is set to the midpoint of the current range (Line 5). The PQE-PT subroutine is then invoked to retrieve the nearest neighbors in \(S_p\) (Line 6). The corresponding \(R_{S_p}\) and \(P_{S_p}\) are then computed based on the oracle-determined true neighbors in \(S_p\) (Line 7), and the search range is updated accordingly (Lines 8–12). Finally, once the optimal \(t^*\) is determined, PQE-PT is called with \(t^*\) to retrieve the \sprint neighbors in \(S\) (Line 14).

\begin{algorithm}[t!]
\caption{\sprintc}
\label{algo:sprint_c}
\begin{algorithmic}[1]
\State \textbf{Input:} \( S \), \( S_p \), \( O \), \( P \), \( q \), \( \omega_C \)
\State \textbf{Output:} \(\sprint_S\)

\State \(t_{\text{min}} \leftarrow 0\), \(t_{\text{max}} \leftarrow 1\) % \Comment{t: recall target}
\While{$|\text{R}_{S_p} - \text{P}_{S_p}| > \omega_C$}
    \State \( t^* \leftarrow \frac{t_{\text{min}} + t_{\text{max}}}{2} \)
    \State \(NN \leftarrow \text{PQE-PT}(S_p, O, P, q, t^*)\)
    \State \(\text{R}_{S_p} \leftarrow \frac{|NN \cap ON_D \cap S_p|}{|ON_D \cap S_p|}\), \(\text{P}_{S_p} \leftarrow \frac{|NN \cap ON_D \cap S_p|}{|NN \cap S_p|}\)
    \If{$\text{P}_{S_p} \leq \text{R}_{S_p}$}
        \State \( t_{\text{min}} \leftarrow t^* \)
    \Else
        \State \( t_{\text{max}} \leftarrow t^* \)
    \EndIf
\EndWhile
\State \Return \(\text{PQE-PT}(S, O, P, q, t^*)\)
\end{algorithmic}
\end{algorithm}

\stitle{Time Complexity.} The time complexity for both algorithms is dominated by the invocation of PQE-PT, which has a time complexity of \(O(s^2\log(s))\) \cite{DujianPQA}, $s = |S|$ being the sample size. During the search for the optimal \(t^*\), \sprintv performs a ternary search and \sprintc performs a binary search, requiring \(O(\log(1/\omega_V))\) and \(O(\log(1/\omega_C))\)iterations \footnote{Ternary search uses \(\log_3\) while binary search uses \(\log_2\). But for asymptotic analysis, these differences are absorbed in the constant factors.}. Assuming \(\omega_C=\omega_V =\omega^*\), the overall time complexity is \(O(\log(1/\omega^*) s^2 \log(s))\). Evaluating precision and recall over \(S_p\) introduces another linear term \(O(s_p)\). But since \(s_p \ll s\), its impact becomes negligible in the asymptotic analysis. Therefore, \sprintv and \sprintc scale quadratically with \(s\) (up to logarithmic factors), and the number of iterations needed for convergence depends only logarithmically on \(\omega^*\).

\subsection{Theoretical Analysis} \label{sec:theory}

In this section, we establish results on the error bounds of \(\mathtt{AVG}\) and \(\mathtt{PCT}\) achieved by \sprintv and \sprintc, respectively. Also, we derive the minimum sample and pilot sample size required to achieve the bound.


% we analyze sampling error and derive the minimum sample size \(s\) needed for accurate estimation of \(\mathtt{AVG}\) and \(\mathtt{PCT}\) (\S \ref{subsec:s_complexity}). Then, we establish results on their error bounds by incorporating the impact of the pilot sampling (\S \ref{subsec:sp_complexity}). We also derive the minimum pilot sample size for reliable precision and recall estimation.



% \subsubsection{Sampling Error Analysis and Sample Complexity}\label{subsec:s_complexity}
% For SPRinT-V, we use \(\mathtt{AVG}\) as an example to illustrate the sampling error analysis. 
% %Our sample \(S\) of size \(s\) is drawn uniformly at random from \(D\). 
% Let \(x[\mathtt{attr}]\) denote the value of attribute $\mathtt{attr}$ for an object $x \in D$. For a set $X \subseteq D$, the average value of $\mathtt{attr}$ is $\mathtt{AVG}_{X} = \frac{1}{|ON_X(q, r)|} \sum_{x \in ON_X(q, r)} x[\mathtt{attr}]$, 
% % 
% where \(ON_X(q, r)\) is the set of oracle neighbors of \(q\) in \(X\) within radius \(r\). When $X$ is $D$ (resp. a random sample $S\subset D$), $\mathtt{AVG}_{X}$ is the true (resp. sample) average. 
% % 
% \eat{ 
% The sample average attribute value of the oracle neighbors in a random sample \(S \subset D\) is 
% %\begin{align}
% $\mathtt{AVG}_{S} = \frac{1}{|ON_S(q, r)|} \sum_{x_i \in ON_S(q, r)} x_i[\mathtt{attr}]$, 
% %\end{align}
% where \(ON_S(q, r)\) is the set of oracle neighbors of \(q\) in \(S\) within radius \(r\).
% } 
% % 
% Suppose that each \( x[\mathtt{attr}] \) is bounded, i.e., \( x[\mathtt{attr}] \in [a, b] \), for some finite $a,b \in \mathbb{R}$.\footnote{E.g., human systolic blood pressure typically falls in the range $[70, 200]$ mmHg. For healthy individuals, the normal range is $[90, 120]$ mmHg.} Applying Hoeffding's inequality~ \cite{hoeffding1994probability, serfling1974probability}, the probability that \( \mathtt{AVG}_{S} \) deviates from \( \mathtt{AVG}_{D} \) by more than a given threshold \( \omega_D \) is 
% \begin{align}
% \Pr\left[ |\mathtt{AVG}_{S} - \mathtt{AVG}_{D}| \geq \omega_D \right] \leq 2 \exp\left(-\frac{ 2 |ON_S| \omega_{D}^2}{(b - a)^2} \right), 
% \label{eq:AVG_D_bound1}
% \end{align}
% %
% \eat{Hoeffding's inequality is a widely-used concentration inequality in probability theory, particularly for analyzing the deviation of sample means from true means when the samples are independent and bounded \cite{hoeffding1994probability, serfling1974probability}. } 
% % 
% showing as \( |ON_S| \) increases, the probability of a significant deviation between the sample and true averages decreases exponentially.

% For a given confidence level \( (1 - \alpha) \) (e.g., \(\alpha = 0.05\) for a 95\% confidence interval), to guarantee $\Pr\left[ |\mathtt{AVG}_{S} - \mathtt{AVG}_{D}| \leq \omega_D \right] \geq 1-\alpha$, by Eq. (\ref{eq:AVG_D_bound1})
% %, we require \(\omega_D = (b-a)\sqrt{\frac{\ln(2/\alpha)}{2|ON_S|}}\).
% % where \(\alpha = 2 \exp\left(-\frac{ 2 |ON_S| \omega_{D}^2}{(b - a)^2}\right)\).
% the minimum sample size required to achieve a desired error margin \(\omega_D\) w.h.p. is:
% \begin{align}
% s \geq \frac{(b - a)^2 \ln\left(\frac{2}{\alpha}\right)}{2 \rho \omega_D^2}
% \end{align}
% where \(\mathbb{E}[|ON_S|] = \rho s\) by the law of large numbers \cite{dekking2006modern}, and \(\rho := \frac{|ON_D|}{|D|}\). 
% % We refer to $\rho$ as the \textit{level of difficulty} of an AQNN. 
% \(\rho\) is affected by factors such as radius and data characteristics. Smaller radius leads to lower \(\rho\) as fewer true nearest neighbors exist in \(D\). A lower \(\rho\) will lead to a higher likelihood of sampling bias and insufficient representation in \(S\). 

% Similarly, for SPRinT-C, to guarantee %by adapting Eq. (\ref{eq:AVG_D_bound1}) for \(\mathtt{PCT}\), we get:
% %\begin{align}
%     $\Pr\left[ |\mathtt{PCT}_{S} - \mathtt{PCT}_{D}| \leq \omega_D' \right] \geq 1-\alpha$ 
%     %\label{eq:PCT_D_bound}
% %\end{align}
% for \(\mathtt{PCT}\), we require \(\omega'_D = \sqrt{\frac{\ln(2/\alpha)}{2s}}\). The minimum sample size required to achieve a desired error margin \(\omega_D'\) is:
% \begin{align}
% s\geq\frac{\ln(2/\alpha)}{2\omega_D'}
% \end{align}


% \subsubsection{Overall Error Analysis and Pilot Sample Complexity}\label{subsec:sp_complexity}
% We next analyze the overall error and establish results on the pilot sample complexity required in order to bound the overall error w.h.p. 

% \begin{theorem}\label{thm:sprint-v}
%     Let \(\widetilde{\mathtt{AVG}_S}\) be the approximate aggregate returned by SPRinT-V. Given a target error bound \(\gamma\) for the deviation between \(\text{F}{\beta}_S\) and \(\text{F}{\beta}_{S_p}\), and a confidence level of \(1 - \alpha\), the approximation error satisfies:
%     \begin{equation}
%     \Pr\left[|\widetilde{\mathtt{AVG}_S} - \mathtt{AVG}_D| \leq \omega_S + \omega_D\right] \geq 1-\alpha,
%     \end{equation}
%     where:
%     \begin{align}
%     \omega_S &= \frac{2(b-a)}{|ON_S|} (1- \text{F1}_{S_p} + \gamma), \\
%     % \omega_D &= \sqrt{\frac{(b-a)^2\ln(2/\alpha)}{2|ON_S|}}, \\
%     s_p &\geq \frac{2L^2\ln(4/\alpha)}{\gamma^2},
%     \end{align}
%     and \(L = \max\{1+\beta^2,\frac{1+\beta^2}{\beta^2}\}\).
% \end{theorem}

\begin{theorem}\label{thm:sprint-v}
    Let \(\widetilde{\mathtt{AVG}_S}\) be the approximate aggregate returned by \sprintv. Given a target error bound \(\omega = \omega_D+\omega_S\), and a confidence level of \(1 - \alpha\), the approximation error satisfies:
    \begin{equation}
    \Pr\left[|\widetilde{\mathtt{AVG}_S} - \mathtt{AVG}_D| \leq \omega\right] \geq 1-\alpha,
    \end{equation}
    provided:
    \begin{align}
    % \omega_S &= \frac{2(b-a)}{|ON_S|} (1- \text{F1}_{S_p} + \gamma), \\
    % \omega_D &= \sqrt{\frac{(b-a)^2\ln(2/\alpha)}{2|ON_S|}}, \\
    s &\geq \frac{(b - a)^2 \ln(2/\alpha)}{2 |ON_S| \omega_D^2}, \\
    s_p &\geq \frac{8L^2(b-a)^2\ln(4/(\alpha+1))}{|ON_S|^2 \omega_S^2},
    \end{align}
    where \(L = \max\{1+\beta^2,\frac{1+\beta^2}{\beta^2}\}\).
\end{theorem}
% \laks{in eq. (3), the RHS which is a bound on $s$ should not depend on $s$!}
% We derive the overall error bound between the approximate aggregate (e.g., \(\widetilde{\mathtt{AVG}_S}\)) and the true aggregate (e.g., \(\mathtt{AVG}_D\)), incorporating both the pilot sampling error analyzed in this section and the sampling error discussed in Section \ref{subsec:s_complexity}. Notice that the pilot sample size affects the precision and recall estimates obtained during the nearest neighbor refinement step, which in turn impacts the error of the approximate aggregates. As part of the pilot sampling error analysis, we also determine the minimum required pilot sample size. Our analysis covers both SPRinT-V and SPRinT-C, using \(\mathtt{AVG}\) for value-based aggregation functions and \(\mathtt{PCT}\) for count-based aggregation functions to illustrate the respective error bounds.

% \stitle{SPRinT-V.} 
\begin{proof}
We begin by bounding the deviation \(|\mathtt{AVG}_D - \mathtt{AVG}_S|\). Let \(x[\mathtt{attr}]\) denote the value of attribute $\mathtt{attr}$ for an object $x \in D$. For a set $X \subseteq D$, the average value of $\mathtt{attr}$ is 
\begin{equation}
\mathtt{AVG}_{X} = \frac{1}{|ON_X(q, r)|} \sum_{x_i \in ON_X(q, r)} x_i[\mathtt{attr}],    
\end{equation}
% $\mathtt{AVG}_{X} = \frac{1}{|ON_X(q, r)|} \sum_{x_i \in ON_X(q, r)} x_i[\mathtt{attr}]$, 
% 
where \(ON_X(q, r)\) is the set of oracle neighbors of \(q\) in \(X\) within radius \(r\). When $X$ is $D$ (resp. a random sample $S\subset D$), $\mathtt{AVG}_{X}$ is the true (resp. sample) average. Suppose that each \( x_i[\mathtt{attr}] \) is bounded, i.e., \( x_i[\mathtt{attr}] \in [a, b] \), for some finite $a,b \in \mathbb{R}$.\footnote{E.g., human systolic blood pressure typically falls in the range $[70, 200]$ mmHg. For healthy individuals, the normal range is $[90, 120]$ mmHg.} Applying Hoeffding's inequality~ \cite{hoeffding1994probability, serfling1974probability}, the probability that \( \mathtt{AVG}_{S} \) deviates from \( \mathtt{AVG}_{D} \) by more than a given error \( \omega_D > 0\) is 
\begin{align}
\Pr\left[ |\mathtt{AVG}_{S} - \mathtt{AVG}_{D}| \geq \omega_D \right] \leq 2 \exp\left(-\frac{ 2 |ON_S| \omega_{D}^2}{(b - a)^2} \right), 
\label{eq:AVG_D_bound1}
\end{align}
showing as \( |ON_S| \) increases, the probability of a significant deviation between the sample and true averages decreases exponentially.

For a given confidence level \( (1 - \alpha) \) (e.g., \(\alpha = 0.05\) for a 95\% confidence interval), to guarantee $\Pr\left[ |\mathtt{AVG}_{S} - \mathtt{AVG}_{D}| \leq \omega_D \right] \geq 1-\alpha$, by Eq. (\ref{eq:AVG_D_bound1}), the minimum sample size required to achieve the desired error margin \(\omega_D\) w.h.p. is:
\begin{align}
s \geq \frac{(b - a)^2 \ln\left(\frac{2}{\alpha}\right)}{2 |ON_S| \omega_D^2}. \label{eq:s-bound}
\end{align}

% \laks{similar comment: above, the RHS which is a bound on $s$ should not depend on $s$!} 

\eat{where \(\mathbb{E}[|ON_S|] = \rho s\) by the law of large numbers \cite{dekking2006modern}, and \(\rho := \frac{|ON_D|}{|D|}\). \(\rho\) is affected by factors such as radius and data characteristics. Smaller radius leads to lower \(\rho\) as fewer true nearest neighbors exist in \(D\). A lower \(\rho\) will lead to a higher likelihood of sampling bias and insufficient representation in \(S\).}

Next, we bound the deviation \(|\widetilde{\mathtt{AVG}_S} - \mathtt{AVG}_S|\), which can be decomposed into two terms:
% 
\begin{align}
|\widetilde{\mathtt{AVG}_S} - \mathtt{AVG}_S| \leq 
& 
| 1/|\sprint_S|\cdot \Sigma_{x_i \in \sprint_S} x_i[\mathtt{attr}]  \nonumber \\
%& \left. 
& - 1/|\sprint_S \cap ON_S| \cdot \Sigma_{x_i \in \sprint_S \cap ON_S} x_i[\mathtt{attr}]| \nonumber \\
& + | 1/|\sprint_S \cap ON_S| \cdot  \Sigma_{x_i \in \sprint_S \cap ON_S} x_i[\mathtt{attr}]  \nonumber \\
&  - 1/|ON_S| \cdot \Sigma_{x_i \in ON_S} x_i[\mathtt{attr}] |,
\end{align}
where the first term measures the error introduced by false positives (\(x_i \in \sprint_S \setminus ON_S\)), while the second term captures the error from false negatives (\(x_i \in ON_S \setminus \sprint_S\)). Since each false positive or false negative contributes at most \((b-a)/|ON_S|\), we bound the total error as:
\begin{align}
|\widetilde{\mathtt{AVG}_S} - \mathtt{AVG}_S| & \leq \frac{(b-a)}{|ON_S|} \left[(1 - \text{P}_S) + (1 - \text{R}_S)\right] \nonumber \\
& \leq \frac{2(b-a)}{|ON_S|}(1-\text{F}{\beta}_S).
\label{eq:AVG_S_bound1}
\end{align} 
The last inequality follows from the fact that for positive numbers the harmonic mean is always upper bounded by the arithmetic mean, i.e., \(\text{F}{\beta}_S\leq(P_S+R_S)/2\).
% \begin{align}
% (P_S - R_S)^2 &\geq 0, \label{eq:P-R-nonneg} \\
% P_S + R_S &\geq \frac{4\,P_S\,R_S}{P_S + R_S}, \label{eq:P+R-bound} \\
% 2 - (P_S + R_S) &\leq 2 - \frac{4\,P_S\,R_S}{P_S + R_S}. \label{eq:2-PR-bound}
% \end{align}

\sprintv maximizes the \(\text{F}{\beta}_{S_p}\) in the refinement step (Algorithm \ref{algo:sprint_v}). Applying Hoeffding's inequality (details in Appendix \S~\ref{subsec:s_p_in_appendix}), we establish probabilistic bounds on the deviations in precision and recall:
\begin{equation}
\Pr\left[ |\text{P}_{S_{p}} - \text{P}_S| \geq \gamma/2 \right] \leq \min\{1, 2 \exp( -2 s_{p} (\gamma/2)^2 )\},
\label{eq:Hoef_precision}
\end{equation}
\begin{equation}
\Pr\left[ |\text{R}_{S_{p}} - \text{R}_S| \geq \gamma/2 \right] \leq \min\{1, 2 \exp( -2 s_{p} (\gamma/2)^2 )\},
\label{eq:Hoef_recall}
\end{equation}
where \(\gamma>0\) is an error term.
Since \(f(\text{P},\text{R}) := \text{F}{\beta}\) is differentiable and continuous, we apply the mean value theorem to bound \(|\text{F}{\beta}_{S_p} - \text{F}{\beta}_S|\):
\begin{align}
|\text{F}{\beta}_{S_p}-\text{F}{\beta}_S| & = |f(\text{P}_{S_p}, R_{\text{S}_p})-f(\text{P}_S, \text{R}_S)| \nonumber \\
& \leq |\pdv{f}{\text{P}}||\text{P}_{S_p}-\text{P}_S|+|\pdv{f}{\text{R}}||\text{R}_{S_p}-\text{R}_S| \nonumber \\
& \leq L(|\text{P}_{S_p}-\text{P}_S| + |\text{R}_{S_p}-\text{R}_S|),
\label{eq:Hoef_fbeta}
\end{align}
where \(L = \max\left(1 + \beta^2, \frac{1 + \beta^2}{\beta^2}\right)\) is the Lipschitz constant of \(F{\beta}\) with respect to precision and recall. By Eq. (\ref{eq:Hoef_precision}) and (\ref{eq:Hoef_recall}), we get:
\begin{align}
    \Pr\left[|\text{P}_{S_p}-\text{P}_S| + |\text{R}_{S_p}-\text{R}_S|\leq \frac{\gamma}{L} \right] & \geq \Pr\left[|\text{P}_{S_p}-\text{P}_{S}|\leq \frac{\gamma}{2L}\right] \nonumber \\
    & + \Pr\left[|\text{R}_{S_p}-\text{R}_{S}|\leq \frac{\gamma}{2L}\right] \nonumber \\
    \text{So,} 
    \Pr\left[|\text{F}{\beta}_{S_p}-\text{F}{\beta}_S|\leq \gamma\right] & \geq 2-2\min\{1, 2\exp(-2s_p(\frac{\gamma}{2L})^2)\}.
    % \Pr\left[|\text{F1}_{S_p}-\text{F1}_S|\leq \omega\right] & \geq 1-\alpha,
\label{eq:fbeta_bound}
\end{align}
Under a \((1-\alpha)\) confidence interval and an error margin \(\omega_S\), by Eq. (\ref{eq:AVG_S_bound1}), for the following guarantee to hold:
\begin{align}
    % \Pr\left[|\widetilde{\mathtt{AVG}_S} - \mathtt{AVG}_S| \leq 2(b-a) (1- \text{F1}_{S_p}+\omega) \right] & \geq 1-\alpha \\
    \Pr\left[|\widetilde{\mathtt{AVG}_S} - \mathtt{AVG}_S| \leq \omega_S \right] & \geq 1-\alpha
\label{eq:AVG_S_bound}
\end{align}
the minimum required pilot sample size should satisfy:
\begin{align}
s_p& \geq \frac{8L^2(b-a)^2\ln(4/(\alpha+1))}{[|ON_S|\omega_S-2(b-a)(1-\text{F}{\beta}_{S_p})]^2} \nonumber \\
&\geq \frac{8L^2(b-a)^2\ln(4/(\alpha+1))}{|ON_S|^2 \omega_S^2}. \label{eq:sp-bound}
\end{align}
The theorem follows from Eq. (\ref{eq:s-bound}) and (\ref{eq:sp-bound}).
%Finally, we obtain the bound and the minimal sample and pilot sample size stated in Theorem \ref{thm:sprint-v}.
\end{proof}


\begin{theorem}\label{thm:sprint-c}
    Let \(\widetilde{\mathtt{PCT}_S}\) be the approximate aggregate returned by \sprintc. Given a target error bound \(\omega=\epsilon + \omega_D'\) and  confidence level  \(1 - \alpha\), the approximation error satisfies:
    \begin{equation}
    \Pr\left[|\widetilde{\mathtt{PCT}_S} - \mathtt{PCT}_D| \leq \omega \right] \geq 1-\alpha,
    \end{equation}
    provided:
    \begin{align}
    % \epsilon &= \omega_C + \sqrt{\frac{2(\ln 4/\alpha)}{s_p}}, \\
    s &\geq\frac{\ln(2/\alpha)}{2\omega_D'}, \\
    % \omega_D' & = \sqrt{\frac{\ln(2/\alpha)}{2s}}, \\
    s_p &\geq \frac{2\ln(4/(\alpha+1))}{(\epsilon-\omega_C)^2},
    \end{align}
    where \(\omega_C \geq |\text{R}_{S_p}-\text{P}_{S_p}|\) is the threshold used by \sprintc.
\end{theorem}


% \stitle{SPRinT-C.} 
\begin{proof}
We first bound the deviation \(|\mathtt{PCT}_D - \mathtt{PCT}_S|\).
Similarly to the proof in Theorem \ref{thm:sprint-v}, given an confidence level \((1-\alpha)\), to guarantee %by adapting Eq. (\ref{eq:AVG_D_bound1}) for \(\mathtt{PCT}\), we get:
%\begin{align}
    $\Pr\left[ |\mathtt{PCT}_{S} - \mathtt{PCT}_{D}| \leq \omega_D' \right] \geq 1-\alpha$,
    %\label{eq:PCT_D_bound}
%\end{align}
% \eat{we require \(\omega'_D = \sqrt{\frac{\ln(2/\alpha)}{2s}}\). T}
the minimum sample size required to achieve a desired error margin \(\omega_D'>0\) is:
\begin{align}
s\geq\frac{\ln(2/\alpha)}{2\omega_D'}. \label{eq:s-bound2}
\end{align}


We then bound the deviation \(|\widetilde{\mathtt{PCT}_S} - \mathtt{PCT}_S|\). From Eq. (\ref{eq:approx_PCT_S}), we have:
\begin{align}
    \left|\widetilde{\mathtt{PCT}_S} - \mathtt{PCT}_S\right| = \frac{\left| |\sprint_S \setminus ON_S| - |ON_S \setminus \sprint_S| \right|}{s}.
\end{align}
The deviation is determined by the count difference between false positives (\(|\sprint_S \setminus ON_S|\)) and false negatives (\(|ON_S \setminus \sprint_S|\)). 

Using the triangle inequality, we can bound \(|\text{P}_S - \text{R}_S|\) as:
\begin{align}
    |\text{P}_S - \text{R}_S| &\leq |\text{P}_S - \text{P}_{S_p}| + |\text{R}_S - \text{R}_{S_p}| + |\text{P}_{S_p} - \text{R}_{S_p}| \\
    |\text{P}_S - \text{R}_S| &\leq |\text{P}_S - \text{P}_{S_p}| + |\text{R}_S - \text{R}_{S_p}| + \omega_C.
\end{align}
The second inequality follows by applying the stopping condition enforced by Algorithm~\ref{algo:sprint_c}, namely \(|\text{P}_{S_p} - \text{R}_{S_p}| \leq \omega_C\).

Applying Hoeffding's inequality, the probability that \(|\text{P}_S - \text{R}_S|\) exceeds an error term \(\epsilon>0\) is bounded by:
\begin{align}
    \Pr\left[ |\text{P}_S - \text{R}_S| \leq \epsilon \right] 
    &\geq \Pr\left[ |\text{P}_S - \text{P}_{S_p}| + |\text{R}_S - \text{R}_{S_p}| \leq \epsilon - \omega_C \right] \\
    % &\leq \Pr\left( |\text{P}_S - \text{P}_{S_p}| \geq \frac{\epsilon - \omega}{2} \right) + \Pr\left( |\text{R}_S - \text{R}_{S_p}| \geq \frac{\epsilon - \omega}{2} \right) \\
    % &\geq 4 \exp\left(- s_p(\epsilon - 
    % \omega_C)^2/2\right). \\
    &\geq 2-2\min\{1, 2\exp(-\frac{s_p(\epsilon - \omega_C)^2}{2})\}.
\end{align}
Since the deviation in \(\text{P}_S\) and \(\text{R}_S\) directly translates to the difference in false positives and false negatives, we obtain:
\begin{align}
    \left| |\sprint_S \setminus ON_S| - |ON_S \setminus \sprint_S| \right| = |\text{P}_S - \text{R}_S| \cdot s,
\end{align}
Thus, when \(|\text{P}_S - \text{R}_S| \leq \epsilon\), it follows that:
\begin{align}
    \left| |\sprint_S \setminus ON_S| - |ON_S \setminus \sprint_S| \right| \leq \epsilon s.
\end{align}
Finally, we obtain the probability bound for \(|\widetilde{\mathtt{PCT}_S} - \mathtt{PCT}_S|\):
\begin{align}
    \Pr[|\widetilde{\mathtt{PCT}_S} - \mathtt{PCT}_S| \leq \epsilon]\geq 1-\alpha,
    \label{eq:PCT_S_bound}
\end{align}
holds provided  the %minimum required 
pilot sample size satisfies:
\begin{equation}
s_p \geq \frac{2\ln(4/(\alpha+1))}{(\epsilon-\omega_C)^2}. \label{eq:sp-bound2}
\end{equation}

% \(\epsilon = \omega_C + \sqrt{\frac{2(\ln 4/\alpha)}{s_p}}\). To achieve the desired confidence level \(1 - \alpha\), the 
With two bounds in Eq.~\ref{eq:s-bound2} and \ref{eq:sp-bound2}, we establish Theorem \ref{thm:sprint-c}. 
\end{proof}


% As \(s_p\) increases, the probability bound becomes tighter, hence improving the reliability of \(\widetilde{\mathtt{PCT}_S}\). To achieve the desired confidence level for \(\mathtt{PCT}\), the minimum pilot sample size required is \(s_p \geq \frac{2\ln(4/\alpha)}{(\epsilon-\omega_C)^2}\). 

% Additionally, by adapting Equation \ref{eq:AVG_D_bound} for the aggregation function \(\mathtt{PCT}\), we bound the deviation between \(\mathtt{PCT}_D\) and \(\mathtt{PCT}_S\) by \(\omega'_D = \sqrt{\frac{\ln(2/\alpha)}{2s}}\). Overall, the answer to an AQNN with a \(\mathtt{PCT}\) aggregation function is \(\widetilde{\mathtt{PCT}_S}\), where \(|\widetilde{\mathtt{PCT}_S}-\mathtt{PCT}_D|\leq \omega'_D + \epsilon\) w.p. \(1-\alpha\). 

% \laks{of course a similar observation to what i said at the end of SPRinT-V section: the main message is better summarized as a theorem whose proof is there in the derivations contained in the section.}

% \stitle{D and S} The deviation between \(\mathtt{PCT}_D\) and \(\mathtt{PCT}_S\) can be expressed as:
% \begin{align}
%     |\mathtt{PCT}_D - \mathtt{PCT}_S| = \left| \frac{|ON_D \cap D|}{|D|} - \frac{|ON_D \cap S|}{|S|} \right|.
% \end{align}
% Since the term \(|ON_D\cap S|\) can be viewed as the sum of an indicator variables for elements in \(S\) that also belong to \(ON_D\), \(\frac{|ON_D\cap_S|}{|S|}\) is the average of these indicator variables. Hence, we apply Hoeffding's inequality to probabilistically bound the deviation between the proportion of nearest neighbors in \(D\) and \(S\):
% \begin{align}
%     \Pr\left( |\mathtt{PCT}_D - \mathtt{PCT}_S| \geq \omega_D \right) \leq 2 \exp\left( -2 |S| \omega_D^2 \right).
% \end{align}
% Under confidence interval \(1-\alpha\), the deviation is bounded by \(\omega_D = \sqrt{\frac{\ln(2/\alpha)}{2|S|}}\) w.p. \(\geq 1-\alpha\).





\subsection{Extensibility of the Framework}
Our \sprint framework accommodates various aggregation functions. The theoretical analysis developed for \(\mathtt{AVG}\) and \(\mathtt{PCT}\) can be extended to other value- and count-based aggregation functions. This involves straightforward modifications to the final aggregation computation while retaining the same sampling-based framework. For example, going from \(\mathtt{AVG}\) to \(\mathtt{SUM}\) involves scaling the sum by the ratio of dataset to the sample size. The error bounds for \(\mathtt{SUM}\) are derived similarly to \(\mathtt{AVG}\), and adjusted to account for the linear scaling factor. For \(\mathtt{VAR}\), additional steps are needed to compute the mean and squared deviations, with error bounds that combine the deviations in both terms. Despite these modifications, the sampling process, pilot sample selection, and precision-recall optimization strategies remain applicable across different aggregation functions. This demonstrates the flexibility and broad applicability of our framework across various aggregation functions.

We note that while our framework is applicable to many aggregation functions, its effectiveness is most evident in a subset of functions, such as \(\mathtt{AVG}\), \(\mathtt{PCT}\), \(\mathtt{SUM}\), and \(\mathtt{VAR}\). In particular, the approximation guarantees for \(\mathtt{AVG}\) and \(\mathtt{PCT}\) can extend directly to \(\mathtt{SUM}\) and \(\mathtt{VAR}\). Functions like \(\mathtt{MIN}\) and \(\mathtt{MAX}\), however, are excluded from our theoretical analysis due to their inherent sensitivity to the sampling process. Indeed, even a single outlier in the sample can drastically alter the results, making sampling less reliable. %Therefore, we focus our experiments in Section \ref{sec:exps} on \(\mathtt{AVG}\), \(\mathtt{PCT}\), \(\mathtt{SUM}\), and \(\mathtt{VAR}\).



% \subsection{SPRinT-SUM}
% The algorithm for \(\mathtt{SUM}\) follows a similar structure to \(\mathtt{SPRinT-V}\), with modifications to calculate and return the sum of attributes of the nearest neighbors instead of the average. Algorithm \ref{algo:sprint_sum} shows the lines of the algorithm that differ.

% \begin{algorithm}
%     \caption{SPRinT-SUM \\ (showing only lines that replace line 19 in Algorithm~\ref{algo:sprint_v})}
%     \label{algo:sprint_sum}
%     \begin{algorithmic}[1]
%         % \State \(\mathtt{SUM}[\text{attr}] \leftarrow \sum_{x_i \in NN_{\text{SPRinT}}} x_i[\text{attr}]\) (replaces line 19 in Algorithm~\ref{algo:sprint_v})
%         \State \Return \(\frac{|D|}{s}\sum_{x_i \in \text{SPRinT}_S} x_i[\text{attr}]\)
%     \end{algorithmic}
% \end{algorithm}

% \subsubsection{Error Bounds and Confidence Guarantees for \(\mathtt{SUM}\)}
% In this section, we demonstrate that the deviation between \(\widetilde{\mathtt{SUM}_S}\) and \(\mathtt{SUM}_D\) is bounded with high probability. 

% \stitle{Bounding Deviation Between \(\mathtt{SUM}_D\) and \(\mathtt{SUM}_S\).}
% Similar to Equation \ref{eq:AVG_D_bound1}, we can bound the probability that \(\mathtt{SUM}_S\) deviates from \(\mathtt{SUM}_D\) by more than \(\omega_D\):
% \begin{align}
% \Pr\left( |\mathtt{SUM}_S - \mathtt{SUM}_D| \geq \omega_D \right) \leq 2 \exp\left( -\frac{2 \omega_D^2}{|ON_S| (b - a)^2} \right).
% \label{eq:SUM_D_bound}
% \end{align}
% That is, under confidence interval \(1-\alpha\), the deviation between \(\mathtt{SUM}_S\) and \(\mathtt{SUM}_S\) is bounded by \(\omega_D=\sqrt{\frac{|ON_S| (b-a)^2 \ln(2/\alpha)}{2}}\) w.p. \(\geq 1-\alpha\).


% \stitle{Bounding Deviation Between \(\widetilde{\mathtt{SUM}_S}\) and \(\mathtt{SUM}_S\).} The deviation arises from false positives and false negatives, as follows:
% \begin{align}
% |\widetilde{\mathtt{SUM}_S} - \mathtt{SUM}_S|  & \leq \sum_{x_i \in \text{SPRinT}_S \setminus ON_S} |x_i[\text{attr}]|  \\ 
% & \quad + \sum_{x_i \in ON_S \setminus \text{SPRinT}_S} |x_i[\text{attr}]|\\
% & \leq b  \left[(1 - \text{P}_S) |\text{SPRinT}_S| + (1 - \text{R}_S) |ON_S|\right] \\
% & \leq b |ON_S| (2 - 2 \text{F1}_S),
% \label{eq:SUM_S_bound1}
% \end{align}
% where we assume \(|{\text{SPRinT}_S}| \approx |ON_S|\) in the last inequality (which holds for high recall and precision). Let \(\omega_S = 2b |ON_S| (1 - \text{F1}_S)\), we have:
% \begin{align}
%     \Pr\left[|\widetilde{\mathtt{SUM}_S}-\mathtt{SUM}_S\leq \omega_S| \right] \geq 1-\alpha.
%     \label{eq:SUM_S_bound}
% \end{align}

% \stitle{Combined Error Bound.} Combining the two bounds, the overall deviation between \(\widetilde{\mathtt{SUM}_S}\) and \(\mathtt{SUM}_D\) can be expressed as:
% \begin{align}
% \Pr\left( |\widetilde{\mathtt{SUM}_S} - \mathtt{SUM}_D| \leq \omega_S +\omega_D \right) \geq 1 - \alpha.
% \label{eq:SUM_bound}
% \end{align}
% That is, the answer to the aggregation query with \(\mathtt{SUM}\) aggregation function is \(\widetilde{\mathtt{SUM}_S}-\mathtt{SUM}_D \leq \omega_S +\omega_D\) w.p. \(\geq 1-\alpha\).

% \subsection{SPRinT-VAR}
% Algorithm \ref{algo:sprint_v} can also be adapted to support the variance aggregation function. The primary difference lies in the computation of the variance once the nearest neighbors are identified. The algorithm for SPRinT-VAR is shown in Algorithm \ref{algo:sprint_variance}, highlighting only the lines that differ from Algorithm~\ref{algo:sprint_v}.

% \begin{algorithm}
%     \caption{SPRinT-VAR \\ (showing only lines that replace line 19 in Algorithm~\ref{algo:sprint_v})}
%     \label{algo:sprint_variance}
%     \begin{algorithmic}[1]
%         \State \(\mathtt{AVG}[\text{attr}] \leftarrow \frac{1}{|\text{SPRinT}_S|} \sum_{x_i \in \text{SPRinT}_S} x_i[\text{attr}]\)
%         \State \Return \(\frac{1}{|\text{SPRinT}_S|} \sum_{x_i \in \text{SPRinT}_S} (x_i[\texttt{attr}] - \texttt{AVG}[\texttt{attr}])^2\) 
%     \end{algorithmic}
% \end{algorithm}

% \stitle{Bounding Deviation Between \(\mathtt{VAR}_D\) and \(\mathtt{VAR}_S\).} The variance for \(D\) is defined as \(\mathtt{VAR}_D = \frac{1}{ON_D}\sum_{x_i\in ON_D}x_i[\mathtt{attr}]^2-\mathtt{AVG}^2_D\). Similarly, we can define \(\mathtt{VAR}_S\). There difference can be split into:
% \begin{align}
% |\mathtt{VAR}_D - \mathtt{VAR}_S| = & \left| \frac{1}{|ON_D|} \sum_{x_i \in ON_D} x_i[\mathtt{attr}]^2 - \frac{1}{|ON_S|} \sum_{x_i \in ON_S} x_i[\mathtt{attr}]^2 \right| \\
% & + \left| \mathtt{AVG}_S^2 - \mathtt{AVG}_D^2 \right|.
% \label{eq:VAR_D_bound1}
% \end{align}
% The second-order term deviation is bounded by \((b^2 - a^2) \sqrt{\frac{\ln(2/\delta)}{2|ON_S|}}\) using Hoeffding's inequality with bounds \([a^2, b^2]\) for \(x_i[\mathtt{attr}]^2\). The second term, representing the mean deviation, can be bounded as follows:
% \begin{align}
% |\mathtt{AVG}_S^2 - \mathtt{AVG}_D^2| &\leq |\mathtt{AVG}_S - \mathtt{AVG}_D| |\mathtt{AVG}_S + \mathtt{AVG}_D| \\
% &\leq 2b (b-a) \sqrt{\frac{\ln(2/\delta)}{2|ON_S|}},
% \label{eq:VAR_D_bound2}
% \end{align}
% where we apply the identity \((a^2 - b^2) = (a-b)(a+b)\) and Equation \ref{eq:AVG_D_bound1}. Therefore, with confidence interval \(1-\alpha\), the deviation is bounded by \(\omega_D = (3b+a) (b-a) \sqrt{\frac{\ln(2/\delta)}{2|ON_S|}}\) w.p. \(\geq 1-\alpha\)

% \stitle{Bounding Deviation Between \(\widetilde{\mathtt{VAR}_S}\) and \(\mathtt{VAR}_S\).} Similar to the splitting in Equation \ref{eq:VAR_D_bound1}, applying Equation \ref{eq:AVG_S_bound1}, we get:
% \begin{align}
% \Pr\left[|\widetilde{\mathtt{VAR}_S} - \mathtt{VAR}_S| \leq \omega_S\right] \geq 1-\alpha,
% \label{eq:VAR_S_bound}
% \end{align}
% where \(\omega_S = 2(b-a)(3b+a)(1-\text{F1}_S)\).


% \stitle{Combined Error Bound.} Combing two bounds, the answer to the aggregation query with \(\mathtt{VAR}\) aggregation function is \(\widetilde{\mathtt{VAR}_S}-\mathtt{VAR}_D \leq \omega_S +\omega_D\) w.p. \(\geq 1-\alpha\).


