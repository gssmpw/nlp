\section{Introduction}\label{sec:intro}

\begin{figure}[t!]
    \centering \includegraphics[width=\linewidth]{img/NumericalExample.pdf}
    \caption{A numerical example showing how precision and recall affects error in AQNN results.}
    \label{numericalExample}
\end{figure}


\begin{figure*}[t!]
    \centering \includegraphics[width=\linewidth]{img/framework.pdf}
    \caption{Overview of the SPRinT framework for answering AQNNs.}
    \label{fig:framework}
\end{figure*}

Many applications today require to compute aggregations efficiently over the neighborhood of a designated object. When the neighborhood is not known, it needs to be predicted. That is the case for queries commonly asked by medical professionals such as computing {\em the average systolic blood pressure of patients whose predicted condition is similar to a designated insomnia patient} \cite{DBLP:journals/isci/RodriguesGSBA21}. We call such queries AQNNs, or Aggregation Queries over Nearest Neighbors. AQNNs extend the well-known Fixed-Radius Near Neighbor (FRNN) queries~\cite{FRNNSurvey} with {\em prediction} and {\em aggregation}. The exact evaluation of AQNNs is costly as it relies on a predictive model, a.k.a. an oracle \cite{DBLP:conf/sigmod/LaiHLZ0K21, DBLP:journals/pvldb/KangGBHZ20, DujianPQA}, to determine the neighborhood of the input object and compute an aggregated value. In this paper, we formulate and solve the problem of evaluating AQNNs by seeking an approximate aggregate result that leverages both an expensive oracle and a cheaper predictive model, namely a proxy model \cite{DBLP:conf/icde/AndersonCRW19, DBLP:conf/sigmod/LuCKC18, DujianPQA, kang2017noscope}, to compute the predictions. \\

\stitle{Objectives and Challenges.} Answering an AQNN approximately requires addressing two key objectives: (\textbf{O1}) ensuring low error between the approximate and true aggregates, and (\textbf{O2}) minimizing computational cost. 
The main challenge for \textbf{O1} is to ensure that the retrieved nearest neighbors are both correct and complete, with high likelihood, formally stated as achieving high precision and recall. For example, as illustrated in Fig. \ref{numericalExample}, both low-precision (Case 1) and low-recall (Case 2) scenarios introduce aggregation error. Additionally, Case 1 results in a significantly larger error for \(\mathtt{AVG}\) and \(\mathtt{VAR}\). Assuming the value distributions of the true nearest neighbors and non-nearest neighbors are consistent within themselves but differ from each other, precision is generally more important than recall for \textit{value-based} aggregation functions, including \(\mathtt{AVG}\), \(\mathtt{VAR}\), and \(\mathtt{SUM}\), as it prevents outliers and noisy values from distorting AQNN results. Therefore, for value-based aggregation functions, we aim to maximize \(\text{F}{\beta}\) score, a weighted harmonic mean of precision and recall, for the retrieved nearest neighbors.
On the other hand, \textit{count-based} aggregation functions, such as percentage (\(\mathtt{PCT}\)), are more sensitive to imbalances between precision and recall. For instance, in Fig. \ref{numericalExample}, missing NN2 while adding False NN4 results in both precision and recall equaling \(2/3\), which does not affect the approximate percentage. Hence, to reduce error in count-based aggregation functions, we aim to equalize precision and recall, i.e. equalizing false positives and false negatives. This prevents systematic over- or under-estimation of the aggregate.

Achieving \textbf{O2}, however, inherently conflicts with \textbf{O1}. First, predicting nearest neighbors relies on distances derived from oracle embeddings, which are highly accurate but computationally expensive. In contrast, proxy embeddings are computationally cheaper but inaccurate, leading to lower precision and recall when predicting nearest neighbors, and hence introducing larger aggregation error~\cite{DBLP:conf/icde/AndersonCRW19, DBLP:conf/sigmod/LuCKC18, DujianPQA, kang2017noscope}. Second, verifying precision and recall for selected nearest neighbors requires identifying true nearest neighbors, which depends on expensive oracle embeddings. This exacerbates the trade-off between \textbf{O1} and \textbf{O2}.
% {\em Our overarching goal is to seek a trade-off between cost and error while ensuring that precision and recall are reliably evaluated. } 
{\em Our overarching goal is to strike a balance between cost and error by optimizing precision and recall in a manner that is tailored to different aggregation functions.}\\


\stitle{Technical Contributions.} 
We design \textit{Sampler with Precision-Recall in Target} (\sprint), our framework for answering AQNNs. \sprint consists of three steps: (1) sampling, (2) nearest neighbor refinement, and (3) aggregation. To address \textbf{O2}, we first reduce computational cost by drawing a random sample \(S\) from \(D\). The random sampler preserves data distribution, which ensures that the approximate aggregates in \(S\) reflect the true aggregates in \(D\). Additionally, we draw a pilot sample \(S_p\) from \(S\) to restrict the use of the expensive oracle to \(S_p\), while employing a computationally efficient proxy for all data in \(S\). The function of the pilot sample is twofold: (1) enabling direct verification of precision and recall using oracle embeddings in \(S_p\), which we can use to monitor the precision and recall performance of the proxy in \(S\) (\textbf{O1}), and (2) refining the retrieved neighbor set. We develop \sprintv and \sprintc, two algorithms that cater to value- and count-based aggregation functions, respectively. \sprintv optimizes the \(\text{F}{\beta}\) score by dynamically adjusting precision and recall targets to minimize the aggregation error. \sprintc equalizes precision and recall targets in \(S\) to ensure the number of selected nearest neighbors is neither over- nor under-estimated. 

Furthermore, we derive theoretical bounds on the minimum required sample size and on the minimum required pilot sample size, showing that they are both independent of the dataset size. This scalability enables our framework to handle very large datasets. In addition, we present two theorems that establish error bounds between the approximate and true aggregates for \(\mathtt{AVG}\) and \(\mathtt{PCT}\).\\

\stitle{Empirical Findings.} We run extensive experiments on a variety of datasets (medical, social media, and video), and we measure relative aggregation error (RE) and response time. Our findings reveal that \sprintv consistently achieves the lowest RE across all datasets compared with baselines under the same predefined oracle cost, owing to its ability to attain the highest \(\text{F}{\beta}\) scores. We also observe that both the sample and pilot sample sizes must be sufficiently large to ensure reliable estimation of precision, recall, and the \(\text{F}{\beta}\) score, which are critical for accurate approximation of aggregates. Furthermore, the radius of an AQNN affects the number of true nearest neighbors and, consequently, the RE. Specifically, when the proportion of true nearest neighbors is \(\leq 5\%\), larger sample and pilot sample sizes are required; otherwise, the RE of our proposed method cannot be reliably guaranteed. Interestingly, the reliance of \sprintv and \sprintc on sampled data makes them highly scalable as they are independent from the dataset size. Our results position the \sprint framework as an excellent solution for large-scale applications requiring neighborhood aggregation where accuracy is critical (e.g., medical). This is further demonstrated on a key application of AQNNs -- one-sample hypothesis testing where \sprintv and \sprintc consistently achieve the highest accuracy for t-tests and proportion z-tests while exhibiting low costs.

% In contrast, on certain datasets, \sprintc sometimes underperforms because a very small proportion of nearest neighbors in \(S_p\) leads to poor estimates of precision and recall, which in turn prevents an accurate nearest neighbor count. 
