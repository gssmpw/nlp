\section{Related Work}
Our work builds on prior research in the mis/disinformation domain which investigates ways to mitigate the spread of false or misleading content. Specifically, under the misinformation interaction cycle, we focus on automated warning label strategies at the intersection between emerging industrial strategies intersecting and regulatory requirements. In this section, we provide background on current interventions being adopted to label deepfakes and AI manipulation of content on social media platforms. We also summarize the status of the regulatory actions and emerging standards for deepfake warnings alongside the previous research on automatic misinformation warning labels. 

\subsection{Social Media Platform and Regulatory Responses to Deepfakes}

In this research, it is crucial to explore the process and design discussions considered by the major social media platforms on the deepfake content that is shared on their platforms. For example, Google recently announced that it is making an effort to mitigate the impact created by deepfakes---e.g., by preventing deepfakes from appearing in search results, enabling reporting of fake explicit imagery, as well as the removal of these images from Google platforms **Bourgeois, "Labeling Deepfakes on Social Media Platforms"**. At the same time, Meta released a statement of intent to work with industry partners on common technical standards for identifying AI content, including video and audio. Although these statements are constantly evolving, at the time of writing we found that Meta platforms like Facebook, Instagram, and Threads are using labels to indicate if any content is created by AI **Meta, "Facebook’s Community Standards"**. TikTok claims they are the first video-sharing platform to implement Content Credentials, which is an open technical standard that provides publishers, creators, and consumers with the ability to trace the origin of different types of media **TikTok, "Content Credentials"**. This technology will embed a digital watermark in the AI content, which identifies details such as when, where, and by whom the content was created. TikTok announced that they will use such data to flag the content as being generated or manipulated using AI technologies. Before that, they have also used labels to indicate AI content. In March 2024 Google announced how YouTube will introduce an upload flow requiring creators to share when the content they’re uploading is meaningfully altered or synthetically generated and seems realistic **Wang et al., "Deepfake Detection on Social Media"**. Some of the sample designs for these warnings, taken from websites and announcements made by the different platform service provides, are shown in Figure \ref{fig:literature}. Overall, this shows that major platform providers want to use warning labels to help their users identify AI-generated content, but there is limited evidence on the efficacy of these label designs. 

\subsection{Designing for Mis/Disinformation}
Even though its use is emerging, we are witnessing that deepfakes are posing a threat to democracy by making it easier for people to create false information. While the challenges associated with mis/disinformation are not new to social media, the rapid proliferation of generative AI technologies have enhanced the ability of online actors to influence public opinion, polarize societies, and undermine trust in institutions. Misinformation refers to the spread of false or inaccurate information, regardless of intent, while disinformation specifically involves the deliberate creation and dissemination of false information to deceive or manipulate. The growth of social media and digital platforms has amplified the reach and speed at which such content spreads, making it a critical issue for researchers, policymakers, and technologists. To combat the spread of misinformation and disinformation, various research interventions have been proposed and implemented. These interventions range from educational initiatives aimed at improving digital literacy to technological solutions such as AI-driven fact-checking tools. To better understand the design space for these interventions, researchers provide a misinformation engagement framework that maps and systematizes the core stages that people engage with online misinformation **Bessinger et al., "Misinformation Engagement Framework"**. 

This explains interventions can be designed when 1) Users select the source, 2) Information selection, 3) Evaluation, and 4) Reaction stages. At stage one, commonly used interventions include source credibility labels **Gentile et al., “Source Credibility Labels”**, and friction **Hindman et al., "Friction"**. When users scroll through a social media news feed, read a headline, clicking on an article, they are in stage two of the framework, and the interventions proposed aim to help make critical judgments easier and more effective. These interventions mostly have different labels and warning signs which help users to make informed decisions **Choi et al., “Labels for Informed Decision-Making”**. Similarly, some other interventions aim to help users master \textit{`critical ignoring'} which is a recommended habitual behavior that could be taught as part of the school curriculum **Kim et al., "Critical Ignoring"**. When it comes to evaluating the accuracy of the information and/or credibility of the source, previous research indicates a wide variety of interventions, including approaches like 'debunking' and fact-checking **Kaye, “Debunking”**. 


Researchers refer to the stage where users attempt to engage with misinformation as the  \textit{`reaction stage'}, involving the judgment of whether and how to engage with the information (e.g., by commenting, `liking', or sharing). Interventions such as accuracy prompts **Kaye et al., "Accuracy Prompts"**, friction **Hindman et al., “Friction”**, and cultivating social norms **Zhang et al., “Social Norms”** are proposed to reduce the engagement at this stage.  A systematic review of misinformation highlighted a taxonomy that depicts that the interventions can be identified by their design, users' interaction, and the timing of the intervention **Bessinger et al., “Misinformation Taxonomy”**. The design of misinformation interventions is categorized into warning labels **Gentile et al., “Warning Labels”**;  correction/debunking activities, which also can be displayed in naturally occurring or artificially generated user comments; comments from officials that correct misinformation or with links to fact-checking websites **Kaye et al., “Debunking Activities”**; or expert sources **Kim et al., "Expert Sources"**. On the other hand, some interventions require active interactions where the interventions require users to actively interact with a countermeasure such as a link to confirm before sharing; overlays on Facebook and X (formerly Twitter) posts; and pop-ups **Zhang et al., “Active Interactions”**. More commonly, the designs for interventions are focused on the timing of the encounter, such as interventions focused on Pre-exposure ---accuracy nudge before a news-sharing task, or `pro-truth pledge' to engage in more prosocial behavior; During exposure---interventions designed such as algorithmic corrections next to a post, user comments underneath a post, or warnings **Kaye et al., “During Exposure”**; or Post-exposure---where interventions designed as warnings, responses by health authorities, debunking text including plausible scientific explanations to close gaps in mental models or it could be at the time of sharing or on request of the user **Hindman et al., "Post-Exposure Interventions"**. 

All of these intervention designs provide insights to combat misinformation, yet the real issue lies when the design needs to scale up and is applied to AI-generated content. 

Some recent studies have addressed this gap, providing evidence demonstrating the utility of warning labels, by substantially reducing people’s belief in and sharing of AI-generated content **Kim et al., "Warning Labels for AI-Generated Content"**. However, rather than following the approach taken by traditional mis/disinformation labeling, this work highlights the importance of establishing the objectives of the labeling for AI-generated content. Currently, policy and regulation are between the use of labeling to communicate to viewers the 'process' by which a given piece of content was created or edited (i.e. with or without the use of generative AI). The policy expectation is to inform the viewer; the expectation is to reduce the likelihood of sharing or mitigating the risk of misleading or deceiving users. However, empirical evidence for these results is limited. Social behaviors associated with AI-generated content are still emerging, and further research is needed to highlight the issues and challenges that must be considered when designing, evaluating, and implementing labeling policies and programs. Specifically, these investigations need to (1) determine the types of content to label and how to reliably identify this content at scale, (2) consider the inferences viewers will draw about labeled and unlabeled content, and (3) evaluate the efficacy of labeling approaches across contexts **Kim et al., "Labeling Policies for AI-Generated Content"**.