\section{Related Work}
Our work builds on prior research in the mis/disinformation domain which investigates ways to mitigate the spread of false or misleading content. Specifically, under the misinformation interaction cycle, we focus on automated warning label strategies at the intersection between emerging industrial strategies intersecting and regulatory requirements. In this section, we provide background on current interventions being adopted to label deepfakes and AI manipulation of content on social media platforms. We also summarize the status of the regulatory actions and emerging standards for deepfake warnings alongside the previous research on automatic misinformation warning labels. 

\subsection{Social Media Platform and Regulatory Responses to Deepfakes}

In this research, it is crucial to explore the process and design discussions considered by the major social media platforms on the deepfake content that is shared on their platforms. For example, Google recently announced that it is making an effort to mitigate the impact created by deepfakes---e.g., by preventing deepfakes from appearing in search results, enabling reporting of fake explicit imagery, as well as the removal of these images from Google platforms ____. At the same time, Meta released a statement of intent to work with industry partners on common technical standards for identifying AI content, including video and audio. Although these statements are constantly evolving, at the time of writing we found that Meta platforms like Facebook, Instagram, and Threads are using labels to indicate if any content is created by AI ____. TikTok claims they are the first video-sharing platform to implement Content Credentials, which is an open technical standard that provides publishers, creators, and consumers with the ability to trace the origin of different types of media ____. This technology will embed a digital watermark in the AI content, which identifies details such as when, where, and by whom the content was created. TikTok announced that they will use such data to flag the content as being generated or manipulated using AI technologies. Before that, they have also used labels to indicate AI content. In March 2024 Google announced how YouTube will introduce an upload flow requiring creators to share when the content they’re uploading is meaningfully altered or synthetically generated and seems realistic ____. Some of the sample designs for these warnings, taken from websites and announcements made by the different platform service provides, are shown in Figure \ref{fig:literature}. Overall, this shows that major platform providers want to use warning labels to help their users identify AI-generated content, but there is limited evidence on the efficacy of these label designs. 

\subsection{Designing for Mis/Disinformation}
Even though its use is emerging, we are witnessing that deepfakes are posing a threat to democracy by making it easier for people to create false information. While the challenges associated with mis/disinformation are not new to social media, the rapid proliferation of generative AI technologies have enhanced the ability of online actors to influence public opinion, polarize societies, and undermine trust in institutions. Misinformation refers to the spread of false or inaccurate information, regardless of intent, while disinformation specifically involves the deliberate creation and dissemination of false information to deceive or manipulate. The growth of social media and digital platforms has amplified the reach and speed at which such content spreads, making it a critical issue for researchers, policymakers, and technologists. To combat the spread of misinformation and disinformation, various research interventions have been proposed and implemented. These interventions range from educational initiatives aimed at improving digital literacy to technological solutions such as AI-driven fact-checking tools. To better understand the design space for these interventions, researchers provide a misinformation engagement framework that maps and systematizes the core stages that people engage with online misinformation ____. 

This explains interventions can be designed when 1) Users select the source,  2) Information selection, 3) Evaluation, and 4) Reaction stages. At stage one, commonly used interventions include source credibility labels  ____, and friction  ____. When users scroll through a social media news feed, read a headline, clicking on an article, they are in stage two of the framework, and the interventions proposed aim to help make critical judgments easier and more effective. These interventions mostly have different labels and warning signs which help users to make informed decisions ____. Similarly, some other interventions aim to help users master \textit{`critical ignoring'} which is a recommended habitual behavior that could be taught as part of the school curriculum ____. When it comes to evaluating the accuracy of the information and/or credibility of the source, previous research indicates a wide variety of interventions, including approaches like 'debunking' and fact-checking ____. 


Researchers refer to the stage where users attempt to engage with misinformation as the  \textit{`reaction stage'}, involving the judgment of whether and how to engage with the information (e.g., by commenting, `liking', or sharing). Interventions such as accuracy prompts ____ , friction ____, and cultivating social norms ____ are proposed to reduce the engagement at this stage.  A systematic review of misinformation highlighted a taxonomy that depicts that the interventions can be identified by their design, users' interaction, and the timing of the intervention ____. The design of misinformation interventions is categorized into warning labels ____;  correction/debunking activities, which also can be displayed in naturally occurring or artificially generated user comments; comments from officials that correct misinformation or with links to fact-checking websites ____; or expert sources ____. On the other hand, some interventions require active interactions where the interventions require users to actively interact with a countermeasure such as a link to confirm before sharing; overlays on Facebook and X (formerly Twitter) posts; and pop-ups ____. More commonly, the designs for interventions are focused on the timing of the encounter, such as interventions focused on Pre-exposure ---accuracy nudge before a news-sharing task, or `pro-truth pledge' to engage in more prosocial behavior; During exposure---interventions designed such as algorithmic corrections next to a post, user comments underneath a post, or warnings; or Post-exposure---where interventions designed as warnings, responses by health authorities, debunking text including plausible scientific explanations to close gaps in mental models or it could be at the time of sharing or on request of the user ____. 

All of these intervention designs provide insights to combat misinformation, yet the real issue lies when the design needs to scale and have high efficacy. Deepfakes are spreading too fast and users find it almost impossible to distinguish them from real content. It is essential to use technologies to detect deepfakes quickly, but this information also needs to be communicated to the user in a trustworthy manner to reduce engagement behaviors that lead to harmful consequences. Labeling the content is much discussed in the GenAI literature, but the efficacy of different label designs is not empirically validated. Therefore, in this research, we focus more on label design and test for impact to understand the scaling effect.  %In general, understanding the effectiveness of these interventions requires a multidisciplinary approach that integrates insights from psychology, communication studies, computer science, and public policy. In this research, we are specific to deepfake content, and our interventions need to align with practical implementation. For this research, we selected labeling as the intervention and we present a design space for labels that warn users about deepfakes. 

\subsection {Misinformation Labels and Effect}
A widely adopted method of countering misinformation is having \textit{``labels"} to communicate an informed warning to users based on fact checkers or system decisions ____. These labels may be designed directly on or alongside the content. Many social media platforms have different label designs---Facebook ____, Instagram  ____, and Twitter  ____ have some fact-checkers taglines or community taglines as labels. As we found for the mis/disinformation intervention designs, the warning labels differ from related informational interventions such as pre-bunks or corrections ____ which are delivered before or after exposure, respectively. One of the prominent tasks of the fake news identification process is how the results of detection should be communicated to the user. Researchers argue its not only the method of delivering the message but also the method used in identifying the mis/disinformation---i.e. credibility also matters. The trust for computation-based detection and prevention of fake news and decision-aid methods (such as using fact-checkers opposed to warnings by ML methods) to warn users when a piece of fake news has been explored ____. Researchers also identify that interventions can be just an indicator of contextual information ____, including the use of different colors for the label  ____; relevant words for misinformation classification ____; generic tips to detect misinformation; or infographics ____. This highlights the importance of research to systematically study the effects of different label design dimensions, e.g.,  their positions, colors, or fonts ____. 

Notably, all of these interventions targeted traditional misinformation, but exploration of intervention design for emerging types of misinformation, such as those using AI-generated content, is significantly limited. Recent ongoing research work examined that if given the warning of the deepfake content, the likelihood of correctly detecting deepfake videos is approximately twice as likely as the inauthentic video in the control group ____. Similarly, it was shown that the text in the label affects users' ability to detect if the content is a deepfake or not, depending on the intended goal of the label ____. In this study,  participants found that the term "AI-generated" was most consistently associated with content generated using AI, but if the goal is to label misleading content, the term "AI-generated" performed poorly. As a solution to this, the terms “Manipulated” and “Not Real” were found to be the most consistent for misleading information. With such combinations, it is clear that the design space for labeling is wide and there is a need for robust evidence to identify effective labels for AI-generated content.

Some recent studies have addressed this gap, providing evidence demonstrating the utility of warning labels, by substantially reducing people’s belief in and sharing of AI-generated content ____. However, rather than following the approach taken by traditional mis/disinformation labeling, this work highlights the importance of establishing the objectives of the labeling for AI-generated content. Currently, policy and regulation are between the use of labeling to communicate to viewers the 'process' by which a given piece of content was created or edited (i.e. with or without the use of generative AI). The policy expectation is to inform the viewer; the expectation is to reduce the likelihood of sharing or mitigating the risk of misleading or deceiving users. However, empirical evidence for these results is limited. Social behaviors associated with AI-generated content are still emerging, and further research is needed to highlight the issues and challenges that must be considered when designing, evaluating, and implementing labeling policies and programs. Specifically, these investigations need to (1) determine the types of content to label and how to reliably identify this content at scale, (2) consider the inferences viewers will draw about labeled and unlabeled content, and (3) evaluate the efficacy of labeling approaches across contexts ____. Contributing to this empirical work, our research provides a design space in which labels can be designed to warn users about AI-generated content. We then conduct experiments to explore the features of warning labels that may enhance their efficacy in understanding user behavior and reducing misleading deepfake content. In the next section, we explain the methods we used to derive the design space for warning labels for AI-generated content and empirically evaluate the efficacy of different warning labels derived from this design space.