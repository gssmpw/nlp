%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}

\usepackage{makecell}
% \usepackage{algpseudocode}
\usepackage{eqparbox}
\usepackage{array}
\usepackage{pifont}
\usepackage{wrapfig}
\usepackage{amsfonts}
\usepackage{colortbl}
\usepackage{fontenc}
% \usepackage{tikz}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{marvosym}

\definecolor{baselinecolor}{rgb}{0.9, 0.9, 1.}
\definecolor{graycolor}{gray}{0.9}
\definecolor{Green}{rgb}{0.0, 0.5, 0.0}
\definecolor{Green}{rgb}{0.0, 0.5, 0.0}
\definecolor{rebuttal}{rgb}{0.6, 0.6, 1.}
\newcommand{\baseline}[1]{\cellcolor{baselinecolor}{#1}}
\newcommand{\graybase}[1]{\cellcolor{graycolor}{#1}}


\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand*{\affaddr}[1]{#1} 
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand{\authormark}[2][]{%
  \begingroup
  \def\@thefnmark{#1}%
  \footnote{#2}%
  \endgroup
}



\author{%
\bf
Tao Zhang\affmark[1,3]~~~ 
Cheng Da\affmark[2]~~~ 
Kun Ding\affmark[1]~~~
Kun Jin\affmark[2]~~~ 
Yan Li\affmark[2]~~~ 
Tingting Gao\affmark[2]~~~ 
Di Zhang\affmark[2]~~~ \\
\bf
Shiming Xiang\affmark[1,3]~~~ 
Chunhong Pan\affmark[1]~~~ \\
\affaddr{\affmark[1]MAIS, CASIA~~~~~~~~~~}
\affaddr{\affmark[2]Kuaishou Technology~~~~~~~~~~} 
\affaddr{\affmark[3]School of Artificial Intelligence, UCAS~~~~~~~~~~} \\
\small
}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}
\icmltitlerunning{Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level Preference Optimization}

\begin{document}

\twocolumn[
\icmltitle{Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level Preference Optimization}

\icmlkeywords{Diffusion Model, Preference Optimization, Reward Model, Image Generation}

\vskip 0.3in

\arxivauthor
]

\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
% \setlength{\arraycolsep}{2pt}

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% 视觉语言模型用于逐步偏好优化时的缺点--> 扩散模型具有这些能力--> 提出LRM，提出LPO --> 效果。

Preference optimization for diffusion models aims to align them with human preferences for images. Previous methods typically leverage Vision-Language Models (VLMs) as pixel-level reward models to approximate human preferences. However, when used for step-level preference optimization, these models face challenges in handling noisy images of different timesteps and require complex transformations into pixel space. In this work, we demonstrate that diffusion models are inherently well-suited for step-level reward modeling in the latent space, as they can naturally extract features from noisy latent images. Accordingly, we propose the \textbf{Latent Reward Model (LRM)}, which repurposes components of diffusion models to predict preferences of latent images at various timesteps. Building on LRM, we introduce \textbf{Latent Preference Optimization (LPO)}, a method designed for step-level preference optimization directly in the latent space. Experimental results indicate that LPO not only significantly enhances performance in aligning diffusion models with general, aesthetic, and text-image alignment preferences, but also achieves 2.5-28$\times$ training speedup compared to existing preference optimization methods. Our code and models are available at
\url{https://github.com/Kwai-Kolors/LPO}.
\end{abstract}

\input{sec/01_intro}

\input{sec/02_related_work}

\input{sec/03_background}

\input{sec/04_method}

\input{sec/05_experiment}

\input{sec/06_discussion}


\section*{Impact Statement}
\setlength{\itemsep}{5pt}
\setlength{\topsep}{0pt}
This work introduces a preference optimization method used for text-to-image diffusion models. The method may have the following impacts:
\begin{itemize}[left=0pt]
    \vspace{-1pt}
    \item The reward model plays a crucial role in shaping the preferences of diffusion models. However, if the training data for the reward model contains biases, the optimized model may inherit or even amplify these biases, leading to the generation of stereotypical or discriminatory content about certain groups. To mitigate this risk, it is essential to ensure that the training data is diverse, representative, and fair.
    \item This method offers a way to enhance the quality of generated images in terms of aesthetics, relevance, and other aspects by leveraging a well-designed reward model. It can also be applied to safety domains, optimizing the model to prevent the generation of negative content.
    \item The optimized model can generate highly realistic images, which may be used to create misinformation or misleading content, thereby impacting public opinion and social trust. Therefore, it is essential to develop effective detection tools and mechanisms to identify synthetic content.
\end{itemize}


\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\input{sec/07_appendix}



\end{document}


