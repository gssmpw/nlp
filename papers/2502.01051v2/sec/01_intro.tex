\section{Introduction}
\label{sec:introduction}

\renewcommand{\thefootnote}{} 
\footnotetext{\textsuperscript{\Letter}Corresponding author: Kun Ding $<$kun.ding@ia.ac.cn$>$.}

Diffusion models \cite{sd3} have achieved significant success in the domain of text-to-image generation. 
Inspired by advancements in preference optimization \cite{llama2, llama3} for Large Language Models (LLMs), several methods are proposed to align the diffusion model with human preferences. Diffusion-DPO \cite{diffusion_dpo} extends Direct Preference Optimization (DPO) \cite{dpo} to diffusion models, leveraging human-annotated data for training without requiring a reward model. However, the reliance on offline sampling introduces a distribution discrepancy between the preference data and the diffusion model, resulting in reduced optimization efficiency and effectiveness. DDPO \cite{ddpo} and D3PO \cite{d3po} employ online sampling and reward models for preference assessment. These methods typically assume that the preference order or reward value remains constant across all timesteps; however, this assumption may not hold in practice. To address this limitation, Step-by-step Preference Optimization (SPO) \cite{spo} introduces the Step-aware Preference Model (SPM) to predict preferences for different steps. Consequently, SPO can align models with aesthetic preference through step-level online sampling and preference assignment.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/illustration_v13.pdf}
    \vspace{-20pt}
    \caption{The comparison between the pixel-level reward model (a) and latent reward model (b) in step-level preference optimization. DMO denotes the diffusion model to be optimized.}
    \label{fig:illustration}
    \vspace{-12pt}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/main_comparison_v3.pdf}
    \vspace{-23pt}
    \caption{Qualitative comparison among different preference optimization methods based on SDXL \cite{sdxl}. LPO excels in both aesthetics and text-image alignment, resulting in improved overall quality. Larger versions are provided in Fig.\;\ref{fig:vis_xl_1} and Fig.\;\ref{fig:vis_xl_2}.}
    \label{fig:main_comparison}
    \vspace{-5pt}
\end{figure*}

The reward models used in the aforementioned methods are mainly implemented by fine-tuning Vision-Language Models (VLMs) like CLIP \cite{clip} on preference datasets. We designate these models as Pixel-level Reward Models (PRMs) since they exclusively accept pixel-level image inputs. When applied to step-level preference optimization, PRMs encounter several common challenges. (1) \textbf{Complex Transformation}: At each timestep $t$, they necessitate additional processes of diffusion denoising and VAE \cite{vae} decoding to transform noisy latent images $x_t$ into clean ones $\hat{x}_{0,t}$ and pixel-level images $I_t$. This results in an overly lengthy inference process, as illustrated in Fig.\;\ref{fig:illustration}\;(a) and Fig.\;\ref{fig:pipeline}\;(a). (2) \textbf{High-Noise Incompatibility}: At large timesteps characterized by high noise, the predicted images $I_t$ are significantly blurred, leading to a severe distribution shift from the training data of VLMs, \ie clear images. Consequently, the predictions of PRMs are unreliable at large timesteps. (3) \textbf{Timestep Insensitivity}: PRMs exhibit limited sensitivity to variations across different timesteps since they usually do not include timesteps as input, complicating the understanding of how different timesteps influence image assessment. These issues hinder the effectiveness of PRMs for step-level reward modeling.

\textit{Is there a reward model that can naturally capture human preferences directly in the latent space while being aware of timesteps and compatible with high noise?}
In this paper, we find that the pre-trained diffusion model for text-to-image generation is an ideal choice because it exhibits several favorable characteristics, as listed in Fig.\;\ref{fig:illustration}\;(b). (1) It possesses inherent text-image alignment capabilities, owing to the pre-training on large-scale text-image pairs. (2) It can directly accept noisy latent images $x_t$ without requiring additional diffusion forwarding and VAE decoding. (3) It is high-noise-compatible since it can extract features from $x_t$ with various noise intensities, as is done during pre-training. (4) It naturally exhibits a strong sensitivity to timesteps, enabling it to effectively grasp the model's attention at different timesteps. These pre-trained abilities make the diffusion model particularly suitable for step-level reward modeling.

Based on the above insights, we propose using the diffusion model as a noise-aware \textbf{Latent Reward Model (LRM)}. For noisy latent images $x_t$, LRM utilizes visual features from the UNet \cite{unet} and textual features from the text encoder to predict step-level preference labels. We further introduce the Visual Feature Enhancement (VFE) module to enhance LRM's focus on the text-image alignment. To address the inconsistent preference issue in LRM's training data, we propose the Multi-Preference Consistent Filtering (MPCF) strategy to ensure that winning images consistently outperform losing ones across multiple preference dimensions, thus enabling LRM to effectively learn human preferences. Finally, we employ LRM for step-level preference optimization, leading to a simple yet effective method termed \textbf{Latent Preference Optimization (LPO)}, where all steps are conducted within the latent space of diffusion models. 
Extensive experiments demonstrate that LPO significantly improves the image quality of various diffusion models and consistently outperforms existing DPO and SPO methods across the general, aesthetic, and alignment preferences, as indicated in Fig.\;\ref{fig:main_comparison}. Moreover, LPO exhibits remarkable training efficiency, achieving a speedup of 10-28$\times$ over Diffuison-DPO and 2.5-3.5$\times$ over SPO.