\section{Experiments}
\label{sec:experiment}

\subsection{Experimental Setup}
\label{sec:exp_setup}
The experiments are mainly conducted on SD1.5 \cite{sd1} and SDXL \cite{sdxl} without refiner. The LRM is first trained on Pick-a-Pic and then used to fine-tune diffusion models through LPO. Unless otherwise specified, we employ \textit{homogeneous optimization}.

\textbf{LRM Training.} We denote the LRM based on SD1.5 and SDXL as LRM-1.5 and LRM-XL, respectively. They are trained on the filtered Pick-a-Pic v1 \cite{pickscore} as clarified in Sec.\;\ref{sec:lrm_train}. The $gs$ in the VFE module is set to 7.5. 
More details are in \cref{sec:experimental_detail}.

\textbf{LPO Training.} The same 4k prompts in SPO are used for the LPO training, randomly sampled from the training set of Pick-a-Pic v1. The DDIM scheduler \cite{ddim} with 20 inference steps is employed. We use all steps for sampling and training, \ie $t\in[0,50,...,900,950]$. The dynamic threshold range $[th_{min}, th_{max}]$ is set to $[0.35, 0.5]$ for SD1.5 and $[0.45, 0.6]$ for SDXL. The $\beta$ in Eqn.\;(\ref{eq:spo_loss}) is set to 500 and the $K$ in the sampling process is set to 4. Further details can be found in \cref{sec:experimental_detail}.

\begin{table}[t]
    \centering
    \vspace{-2.5mm}
    \caption{General and aesthetic preference scores on Pick-a-Pic validation unique set. $^*$ denotes the metrics are copied from \cite{spo}. Others are evaluated using the official model.}
    \vskip 0.05in
    \label{tab:preferenece_eval}
    \scriptsize
    \setlength{\tabcolsep}{1.0mm}{
    \scalebox{1.1}{
    \begin{tabular}{l c c c c c}
         \toprule
         Method & PickScore & ImageReward & HPSv2 & HPSv2.1 & Aesthetic \\
         \midrule
         \textcolor{gray}{SD1.5} & & & & & \\
         \hspace{1pt} Original & 20.56 & 0.0076 & 26.46 & 24.05 & 5.468 \\
         \hspace{1pt} $^*$DDPO & 21.06 & 0.0817 & - & 24.91 & 5.591 \\
         \hspace{1pt} $^*$D3PO & 20.76 & -0.1235 & - & 23.97 & 5.527 \\
         \hspace{1pt} Diff.-DPO & 20.99 & 0.3020 & 27.03 & 25.54 & 5.595 \\
         \hspace{1pt} SPO & 21.22 & 0.1678 & 26.73 & 25.83 & 5.927 \\
         \rowcolor{cyan!15}\hspace{1pt} LPO & \textbf{21.69} & \textbf{0.6588} & \textbf{27.64} & \textbf{27.86} & \textbf{5.945} \\
         \midrule
         \textcolor{gray}{SDXL} & & & & & \\
         \hspace{1pt} Original & 21.65 & 0.4780 & 27.06 & 26.05 & 5.920 \\
         \hspace{1pt} Diff.-DPO & 22.22 & 0.8527 & 28.10 & 28.47 & 5.939 \\
         \hspace{1pt} MaPO & 21.89 & 0.7660 & 27.61 & 27.44 & 6.095 \\
         \hspace{1pt} SPO & 22.70 & 0.9951 & 28.42 & 31.15 & 6.343 \\
         \rowcolor{cyan!15}\hspace{1pt} LPO & \textbf{22.86} & \textbf{1.2166} & \textbf{28.96} & \textbf{31.89} & \textbf{6.360} \\
         \bottomrule
    \end{tabular}}}
    % \vspace{-3mm}
    \vskip -0.15in
\end{table}


\begin{table*}[t]
    \vspace{-2.5mm}
    \caption{Quantitative results on T2I-CompBench++ \cite{t2i_compbench}.}
    \vskip 0.05in
    \label{tab:t2i_eval}
    \centering
    \scriptsize
    \setlength{\tabcolsep}{1.8mm}{
    \scalebox{1.1}{
    \begin{tabular}{c l c c c c c c c c}
         \toprule
         Model & Method & Color & Shape & Texture & 2D-Spatial & 3D-Spatial & Numeracy & Non-Spatial & Complex \\
         \midrule
         \multirow{4}{*}{SD1.5} & Original \cite{sd1} & 0.3783 & 0.3616 & 0.4172 & 0.1230 & 0.2967 & 0.4485 & 0.3104 & 0.2999 \\
         & Diff.-DPO \cite{diffusion_dpo} & 0.4090 & 0.3664 & 0.4253 & 0.1336 & 0.3124 & 0.4543 & \textbf{0.3115} & 0.3042 \\
         & SPO \cite{spo} & 0.4112 & 0.4019 & 0.4044 & 0.1301 & 0.2909 & 0.4372 & 0.3008 & 0.2988 \\
         & \cellcolor{cyan!15}LPO & 
         \cellcolor{cyan!15}\textbf{0.5042} &
         \cellcolor{cyan!15}\textbf{0.4522} & 
         \cellcolor{cyan!15}\textbf{0.5259} & 
         \cellcolor{cyan!15}\textbf{0.1928} & 
         \cellcolor{cyan!15}\textbf{0.3562} & 
         \cellcolor{cyan!15}\textbf{0.4845} & 
         \cellcolor{cyan!15}0.3110 &
         \cellcolor{cyan!15}\textbf{0.3308}\\
         \midrule
         \multirow{5}{*}{SDXL} & Original \cite{sdxl} & 0.5833 & 0.4782 & 0.5211 & 0.1936 & 0.3319 & 0.4874 & 0.3137 & 0.3327 \\
         & Diff.-DPO \cite{diffusion_dpo} & 0.6941 & 0.5311 & 0.6127 & 0.2153 & 0.3686 & 0.5304 & \textbf{0.3178} & 0.3525 \\
         & MaPO \cite{mapo} & 0.6090 & 0.5043 & 0.5485 & 0.1964 & 0.3473 & 0.5015 & 0.3154 & 0.3229 \\
         & SPO \cite{spo} & 0.6410 & 0.4999 & 0.5551 & 0.2096 & 0.3629 & 0.4931 & 0.3098 & 0.3467 \\
         & \cellcolor{cyan!15}LPO & 
         \cellcolor{cyan!15}\textbf{0.7351} & 
         \cellcolor{cyan!15}\textbf{0.5463} & \cellcolor{cyan!15}\textbf{0.6606} &
         \cellcolor{cyan!15}\textbf{0.2414} &
         \cellcolor{cyan!15}\textbf{0.4075} &
         \cellcolor{cyan!15}\textbf{0.5493} &
         \cellcolor{cyan!15}0.3152 &
         \cellcolor{cyan!15}\textbf{0.3801}\\
         \bottomrule
    \end{tabular}}}
    \vspace{-2mm}
    % \vskip -0.1in
\end{table*}


\begin{table*}[t]
    \begin{minipage}{0.63\linewidth}
        \vspace{-2mm}
        \caption{Quantitative results on GenEval \cite{geneval}.}
        \vskip 0.05in
        \label{tab:geneval}
        \centering
        \scriptsize
        \setlength{\tabcolsep}{1.1mm}{
        \scalebox{1.1}{
        \begin{tabular}{l l c c c c c c c}
             \toprule
             Model & Method & \makecell[c]{Single \\ Object} & \makecell[c]{Two \\ Object} & Counting & Colors & Position & \makecell[c]{Color \\ Attribution} & Overall \\
             \midrule
             \multirow{4}{*}{SD1.5} & Original & 97.50 & 37.12 & 34.69 & 75.53 & 3.75 & 6.75 & 42.56 \\
             & Diff.-DPO & \textbf{98.44} & 38.38 & 36.25 & 77.93 & 4.50 & 7.25 & 43.79 \\
             & SPO & 95.00 & 33.84 & 32.50 & 69.95 & 4.25 & 7.25 & 40.46 \\
             & \cellcolor{cyan!15}LPO & \cellcolor{cyan!15}97.81 &
             \cellcolor{cyan!15}\textbf{54.80}&
             \cellcolor{cyan!15}\textbf{40.94}&
             \cellcolor{cyan!15}\textbf{79.52}&
             \cellcolor{cyan!15}\textbf{7.00}& 
             \cellcolor{cyan!15}\textbf{10.25}&
             \cellcolor{cyan!15}\textbf{48.39}\\
             \midrule
             \multirow{5}{*}{SDXL} & Original & 93.75 & 63.38 & 30.94 & 80.05 & 9.25 & 19.00 & 49.40  \\
             & Diff.-DPO & 99.06 & 76.52 & \textbf{45.00} & 88.83 & 11.50 & 25.75 & 57.78 \\
             & MaPO & 95.63 & 68.94 & 32.19 & 83.51 & 11.50 & 17.75 & 51.59 \\
             & SPO & 94.38 & 69.44 & 31.88 & 81.65 & 10.25 & 15.50 & 50.52  \\
             & \cellcolor{cyan!15}LPO & \cellcolor{cyan!15}\textbf{99.69} &
             \cellcolor{cyan!15}\textbf{81.57} &
             \cellcolor{cyan!15}43.75 &
             \cellcolor{cyan!15}\textbf{89.10} &
             \cellcolor{cyan!15}\textbf{14.00} &
             \cellcolor{cyan!15}\textbf{27.50} & 
             \cellcolor{cyan!15}\textbf{59.27}\\
             \bottomrule
        \end{tabular}}}
        \vskip -0.1in
    \end{minipage}
    \hfill
    \begin{minipage}{0.35\linewidth}
        \vspace{-2mm}
        \caption{Comparisons of training speed.}
        \vskip 0.05in
        \label{tab:speed}
        \centering
        % \footnotesize
        \scriptsize
        \setlength{\tabcolsep}{1.1mm}{
        \scalebox{1.1}{
        \begin{tabular}{l c c c}
             \toprule
             Method & \makecell[c]{Reward \\ Modeling} & \makecell[c]{Preference \\ Optimization} & \makecell[c]{Total $\downarrow$ \\ (A100 h)} \\
             \midrule
             \textcolor{gray}{SD1.5} \\
             \hspace{1pt} Diff.-DPO & 0 & 240 & 240 \\
             \hspace{1pt} SPO & 32 & 48 & 80 \\
             \hspace{1pt} \cellcolor{cyan!15}LPO & \cellcolor{cyan!15}\textbf{15} & \cellcolor{cyan!15}\textbf{8} & \cellcolor{cyan!15}\textbf{23} \\
             \midrule
             \textcolor{gray}{SDXL} \\
             \hspace{1pt} Diff.-DPO & 0 & 2,560 & 2,560 \\
             \hspace{1pt} SPO & 116 & 118 & 234 \\
             \hspace{1pt} \cellcolor{cyan!15}LPO & \cellcolor{cyan!15}\textbf{52} & \cellcolor{cyan!15}\textbf{40} & \cellcolor{cyan!15}\textbf{92} \\
             \bottomrule
        \end{tabular}}}
        \vskip -0.1in
    \end{minipage}
    \vspace{-0.8mm}
\end{table*}

\begin{table}[t]
    \centering
    \vspace{-2mm}
    \caption{Heterogeneous optimization based on LRM-SD1.5. P-S and I-R denote the PickScore and ImageReward metrics.}
    \vskip 0.05in
    \label{tab:sd15_for_sd21}
    \scriptsize
    \setlength{\tabcolsep}{1.0mm}{
    \scalebox{1.0}{
    \begin{tabular}{c c c c c c c c}
         \toprule
         Model & Method & Aesthetic & GenEval & P-S & I-R & HPSv2 & HPSv2.1\\
         \midrule
         SD2.1 & Original & 5.673 & 48.59 & 20.92 & 0.3063 & 27.05 & 25.49 \\
         \tiny(Same VAE) & \cellcolor{cyan!15}LPO & \cellcolor{cyan!15}\textbf{5.969} & \cellcolor{cyan!15}\textbf{56.01}  & \cellcolor{cyan!15}\textbf{21.76} & \cellcolor{cyan!15}\textbf{0.7978} & \cellcolor{cyan!15}\textbf{28.05} & \cellcolor{cyan!15}\textbf{28.61} \\
         \midrule
         SDXL & Original & 5.920 & \textbf{49.40} & \textbf{21.65} & \textbf{0.4780} & 27.06 & 26.05\\
         \tiny(Diff. VAE) & \cellcolor{cyan!15}LPO & \cellcolor{cyan!15}\textbf{5.953} & \cellcolor{cyan!15}40.85 & \cellcolor{cyan!15}20.82 & \cellcolor{cyan!15}0.3919 & \cellcolor{cyan!15}\textbf{27.10} & \cellcolor{cyan!15}\textbf{26.69} \\
         \bottomrule
    \end{tabular}}}
    % \vspace{-2mm}
    \vskip -0.15in
\end{table}


\textbf{Baseline Methods.} We compare LPO with DDPO \cite{ddpo}, D3PO \cite{d3po}, Diffusion-DPO \cite{diffusion_dpo}, MaPO \cite{mapo}, and SPO \cite{spo}. These methods are trained on similar datasets, such as Pick-a-Pic v1 and v2, to ensure a fair comparison. Details are provided in \cref{sec:experimental_detail}.


\textbf{Evaluation Protocol.} We evaluate various diffusion models across three dimensions: general preference, aesthetic preference, and text-image alignment. The PickScore \cite{pickscore}, HPSv2 \cite{hpsv2}, HPSv2.1 \cite{hpsv2}, and ImageReward \cite{imagereward} are utilized to assess the general preference. The aesthetic preference is evaluated using the Aesthetic Score \cite{aesthetic}. Consistent with \cite{spo}, both general and aesthetic preferences are assessed on the validation unique split of Pick-a-Pic v1, which has 500 different prompts. For text-image alignment, we employ the GenEval \cite{geneval} and T2I-CompBench++ \cite{t2i_compbench} metrics. All images are generated using the DDIM scheduler with 20 steps. Additionally, to assess the correlations between the LRM and aesthetics as well as text-image alignment, we propose two corresponding metrics. Specifically, we calculate the score gaps $G_*,*\in\{A,C,L\}$ between winning and losing images, where $A$, $C$, $L$ represent Aesthetic, CLIP, and LRM. For LRM, the score is taken at $t=0$. Then the Pearson Correlation Coefficient \cite{pearson} between $G_L$ and $G_A$ is referred to as \textit{Aes-Corr} while that between $G_L$ and $G_C$ is termed \textit{CLIP-Corr}. They are evaluated on the validation unique and test unique splits of Pick-a-Pic v1.

\subsection{Main Results}


\textbf{Quantitative Comparison.} As indicated in Tab.\;\ref{tab:preferenece_eval}, Tab.\;\ref{tab:t2i_eval}, and Tab.\;\ref{tab:geneval}, Diffusion-DPO excels in enhancing the text-image alignment, while SPO focuses more on aesthetics. LPO outperforms both methods across three dimensions, achieving higher Aesthetic Scores and superior performance on T2I-CompBench++ and GenEval metrics, leading to improved general preference scores. The user study results indicate similar findings, as discussed in \cref{sec:add_exp}. Notably, the LPO-optimized SD1.5 even exhibits performance comparable to the original SDXL model across various metrics.  We further validate the effectiveness of \textit{heterogeneous optimization} in Tab.\;\ref{tab:sd15_for_sd21}. SD1.5 and SD2.1 \cite{sd1} share the same VAE encoder, but SD1.5 has a smaller text encoder. Remarkably, fine-tuning SD2.1 using LRM-1.5 still yields significant improvements across various aspects, demonstrating that a smaller and inferior diffusion model can effectively fine-tune a larger and more advanced model as long as they share the same VAE encoder. In contrast, applying LRM-1.5 for the LPO of SDXL is ineffective due to the distribution mismatch in their latent spaces, which arises from differences in their VAE encoders.

\textbf{Qualitative Comparison.} The qualitative comparisons of various methods are illustrated in Fig.\;\ref{fig:main_comparison} and Fig.\;\ref{fig:vis_15_1}-Fig.\;\ref{fig:vis_xl_4}. The images generated by Diffusion-DPO exhibit deficiencies in color and detail, whereas those produced by SPO demonstrate lower semantic relevance. Additionally, SPO's excessive focus on aesthetics may lead to an overabundance of details in some images, making them appear cluttered. In contrast, the images produced by LPO achieve a strong balance between text-image alignment and aesthetic quality, delivering a higher overall image quality.


\textbf{Training Efficiency Comparison.} LPO achieves significantly faster training speed. As shown in Tab.\;\ref{tab:speed}, considering the time required for both reward modeling and preference optimization, LPO requires only 23 A100 hours for SD1.5---just 1/10 of the training time needed for Diffusion-DPO and 1/3.5 of that for SPO. For SDXL, LPO's training time is reduced to 1/28 and 1/2.5 of that for Diffusion-DPO and SPO, respectively. This efficiency is primarily due to LPO performing reward modeling and preference optimization directly in the latent space, avoiding the additional computational overhead of converting to pixel space.

\subsection{Ablation Studies}
\label{sec:ablation_study}
If not specified, ablation experiments are conducted on SD1.5. Due to space limitations, we only use PickScore to reflect general preference in Tab.\;\ref{tab:ablation_data} and Tab.\;\ref{tab:ablation_lrm}.


\textbf{MPCF.} As shown in Tab.\;\ref{tab:ablation_data}, MPCF plays a critical role in LRM training. As discussed in Sec.\;\ref{sec:lrm_train}, the inconsistent preference issue makes training on the full dataset (wo MPCF) ineffective, since it hinders the LRM from adequately focusing on aesthetics or text-image alignment, resulting in inferior LPO performance. On the other hand, different filtering strategies can profoundly impact the preference patterns of both the LRM and LPO-optimized models. The first filtering strategy strictly requires that winning images score higher than losing images across all aspects. However, since the diffusion model lacks explicit text-image alignment pre-training like CLIP, it is prone to overfitting to the visual features of the images, as indicated by a higher Aes-Corr. This overfitting results in reduced attention to alignment, as reflected by lower CLIP-Corr and GenEval scores. The second and third strategies relax the aesthetic constraints to varying degrees. However, excessively lenient constraints (the 3rd strategy) may cause LRM to focus solely on text-image alignment while neglecting image quality, resulting in a negative Aes-Corr. In contrast, the second strategy balances these two aspects better, leading to the highest general preference scores.


\begin{table}[t]
    \centering
    \vspace{-2.5mm}
    \caption{Ablation results on MPCF of LRM's training data. The second strategy balances aesthetics and alignment better.}
    \vskip 0.05in
    \label{tab:ablation_data}
    \scriptsize
    \setlength{\tabcolsep}{1.0mm}{
    \scalebox{1.1}{
    \begin{tabular}{c c c c c c}
         \toprule
         \multirow{2}{*}{Strategy} & \multicolumn{2}{c}{LRM} & \multicolumn{3}{c}{LPO} \\
         \cmidrule(lr){2-3} \cmidrule(lr){4-6}
          & Aes-Corr & CLIP-Corr & Aesthetic & GenEval & PickScore \\
         \midrule
         wo MPCF & 0.1342 & 0.2274 & 5.772 & 45.66 & 21.49 \\
         1 & \textbf{0.4860} & 0.1011 & \textbf{6.390} & 45.77 & \underline{21.61} \\
         \rowcolor{cyan!15}2 & 0.1136 & 0.3588 & \underline{5.945} & \underline{48.39} & \textbf{21.69} \\
         3 & -0.1152 & \textbf{0.4480} & 5.750 & \textbf{48.62} & 21.47 \\
         \bottomrule
    \end{tabular}}}
    % \vspace{-2mm}
    \vskip -0.1in
\end{table}


\begin{table}[t]
    \centering
    \vspace{-2mm}
    \caption{Ablation results on the VFE module of LRM. Introducing VFE leads to better alignment and general preferences.}
    \vskip 0.05in
    \label{tab:ablation_lrm}
    \scriptsize
    \setlength{\tabcolsep}{1.0mm}{
    \scalebox{1.1}{
    \begin{tabular}{c c c c c c c }
         \toprule
         \multirow{2}{*}{VFE} & \multirow{2}{*}{$gs$} & \multicolumn{2}{c}{LRM} & \multicolumn{3}{c}{LPO} \\
         \cmidrule(lr){3-4} \cmidrule(lr){5-7}
          &  & Aes-Corr & CLIP-Corr & Aesthetic & GenEval & PickScore\\
         \midrule
         \xmark & 1.0 & \textbf{0.1712} & 0.3211 & \textbf{6.053} & 46.60 & 21.51  \\
         \cmark & 3.0 & 0.1233 & 0.3441 & 5.923 & 47.35 & 21.53 \\
         \rowcolor{cyan!15}\cmark & 7.5 & 0.1136 & 0.3588 & \underline{5.945} & \textbf{48.39} & \textbf{21.69}\\
         \cmark & 10.0 & 0.1063 & \textbf{0.3592} & 5.937 & \underline{48.13} & \underline{21.56}\\
         \bottomrule
    \end{tabular}}}
    % \vspace{-2mm}
    \vskip -0.1in
\end{table}


\begin{table}[t]
    \centering
    \vspace{-2.5mm}
    \caption{Ablation results on optimization timestep ranges in LPO.}
    \vskip 0.05in
    \label{tab:ablation_timestep}
    \scriptsize
    \setlength{\tabcolsep}{1.0mm}{
    \scalebox{1.1}{
    \begin{tabular}{c c c c c c c}
         \toprule
         Range of $t$ & Aesthetic & GenEval & P-S & I-R & HPSv2 & HPSv2.1 \\
         \midrule
         \texttt{[}0, 200\texttt{]} & 5.434 & 40.11 & 20.46 & -0.0987 & 26.25 & 23.61 \\
         \texttt{[}250, 450\texttt{]} & 5.527 & 43.00 & 20.76 & 0.1430 & 26.90 & 25.37 \\
         \texttt{[}500, 700\texttt{]} & 5.742 & 44.44 & 20.95 & 0.1591 & 26.71 & 25.16\\
         \texttt{[}750, 950\texttt{]} & \underline{5.853} & \underline{48.28} & 
         \underline{21.54} & \underline{0.6337} & \underline{27.47} & \underline{27.64} \\
         \midrule
         \texttt{[}0, 450\texttt{]} & 5.573 & 42.71 & 20.63 & 0.0204 & 26.69 & 24.88 \\
         \texttt{[}0, 700\texttt{]} & 5.765 & 44.93 & 21.02 & 0.3087 &  27.10 & 26.25\\
         \rowcolor{cyan!15}\texttt{[}0, 950\texttt{]} & \textbf{5.945} & \textbf{48.39} & \textbf{21.69} & \textbf{0.6588} & \textbf{27.64} & \textbf{27.86} \\
         \bottomrule
    \end{tabular}}}
    % \vspace{-2mm}
    \vskip -0.1in
\end{table}


\begin{table}[t]
    \centering
    \vspace{-2mm}
    \caption{Ablation results on different threshold strategies.}
    \vskip 0.05in
    \label{tab:ablation_threshold}
    \scriptsize
    \setlength{\tabcolsep}{1.0mm}{
    \scalebox{1.1}{
    \begin{tabular}{c c c c c c c }
         \toprule
          Threshold & Aesthetic & GenEval & P-S & I-R & HPSv2 & HPSv2.1\\
         \midrule
         0.3 & 5.853 & 46.75 & 21.22 & 0.5112  & 27.30 & 27.12 \\ 
         0.4 & 5.832 & 48.32 & 21.32 & 0.4789 & 27.08 & 26.37 \\
         0.5 & 5.900 & 48.39 & 21.57 & 0.6088 & 27.54 & \underline{27.42} \\
         0.6 & 5.877 & 47.97 & 21.35 & 0.5510 & 27.25 & 26.73 \\
         \midrule
         \texttt{[}0.3, 0.45\texttt{]} & \underline{5.916} & \textbf{49.43} & \underline{21.58} & \underline{0.6405} & \underline{27.55} & 27.33\\
         \rowcolor{cyan!15}\texttt{[}0.35, 0.5\texttt{]} & \textbf{5.945} & 48.39 & \textbf{21.69} & \textbf{0.6588} & \textbf{27.64} & \textbf{27.86} \\
         \texttt{[}0.4, 0.55\texttt{]} & 5.882 & \underline{48.77} & 21.48 & 0.4791 & 27.30 & 27.13\\
         \bottomrule
    \end{tabular}}}
    % \vspace{-2mm}
    \vskip -0.1in
\end{table}


\textbf{Structure of LRM.} As illustrated in Tab.\;\ref{tab:ablation_lrm}, the introduction of VFE ($gs>1$) leads to lower Aes-Corr values but higher CLIP-Corr values, indicating an enhanced emphasis on text-image alignment. This results in improvements in both the GenEval score and PickScore, with only a minor decline in the Aesthetic Score. As $gs$ increases, the LRM's correlation with alignment steadily improves, while its correlation with aesthetics decreases. When $gs$ is set to 7.5, the model achieves the best overall performance.

\textbf{Optimization Timesteps.} Tab.\;\ref{tab:ablation_timestep} ablates different optimization timestep ranges, indicating that larger timesteps lead to better performance. The results achieved within the range of $[750, 950]$ are nearly comparable to those achieved through optimization across the entire denoising process, \ie $[0,950]$. We suggest this is because diffusion models focus on low-frequency information, such as image layout and style, during larger timesteps, while emphasizing high-frequency texture details during smaller timesteps. The low-frequency components formed in higher timesteps play a decisive role in determining the overall quality of the generated images. This observation also demonstrates the effectiveness of LRM, even in very large timesteps. The qualitative comparison of different ranges is shown in Fig.\;\ref{fig:vis_timestep}.

\textbf{Dynamic Sampling Threshold.} The standard deviation $\sigma_t$ of samples at smaller timesteps is relatively small according to the DDPM scheduling \cite{ddpm}, making the constant threshold insufficient to accommodate all timesteps. As indicated in Tab.\;\ref{tab:ablation_threshold}, the dynamic threshold strategy generally outperforms the constant threshold across different intervals, effectively alleviating this problem. We further explore other dynamic strategies in \cref{sec:add_exp}.

