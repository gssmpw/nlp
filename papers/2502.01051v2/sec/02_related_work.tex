\section{Related Work}
\label{sec:related_work}

\textbf{Reward Models for Human Preferences.} Evaluating text-to-image generative models is a challenging problem. Several methods leverage Vision-Language Models (VLMs) to assess the alignment of generated images with human preferences. PickScore \cite{pickscore}, HPSv2 \cite{hpsv2}, and ImageReward \cite{imagereward} aim to predict general preference by fine-tuning CLIP \cite{clip} or BLIP \cite{blip} on preference datasets. \citet{mps} proposes CLIP-based MPS to capture multiple preference dimensions. \citet{ddpo} and \citet{comat} utilize LLaVA \cite{llava} and BLIP, respectively, to improve the text-image alignment of generative models. \citet{spo} introduces SPM, based on PickScore, to predict step-level preference labels during the denoising process. 
These reward models, limited to accepting pixel-level images, often face problems like distribution shift and cumbersome inference. In contrast, our proposed LRM can effectively address these issues. To the best of our knowledge, we are the first to employ diffusion models themselves for reward modeling.

\textbf{Preference Optimization for Diffusion Models.} Motivated by improvements in Reinforcement Learning from Human Feedback (RLHF) in LLMs \cite{llama2, llama3}, several preference optimization approaches for diffusion models have been proposed. Differentiable reward fine-tuning methods \cite{draft,drtune} directly adjust diffusion models to maximize the reward of generated images. However, they are susceptible to reward hacking issues and require gradient backpropagation through multiple denoising steps, significantly increasing the computation burden. DPOK \cite{dpok} and DDPO \cite{ddpo} formulate the denoising process as a Markov decision process and employ Reinforcement Learning (RL) techniques to align models with specific preferences. Nonetheless, they exhibit inferior performance on open vocabulary sets. Diffusion-DPO \cite{diffusion_dpo} and D3PO \cite{d3po} apply DPO \cite{dpo} in LLMs to diffusion models, yielding better performance than the aforementioned RL-based methods. These methods optimize the trajectory-level preference; however, the preference order in intermediate steps may not be the same as that of the final generated images. In view of this, SPO \cite{spo} proposes a step-level preference optimization method. In this paper, our proposed LPO optimizes models in the latent space by employing the diffusion model as a powerful and cost-effective reward model, demonstrating significant effectiveness and efficiency.