\section{Experimantal Details}
\label{sec:experimental_detail}

\textbf{LRM Training.} We train LRM-1.5 for 4,000 steps with 500 warmup steps and LRM-XL for 8,000 steps with 1000 warmup steps, both using the learning rate 1e-5. The batch size is 128 for LRM-1.5 and 32 for LRM-XL, respectively. Following CLIP \cite{clip}, the initial value of $\tau$ in Eqn\;(\ref{eq:score}) is set to $e^{2.6592}$. The training resolution is 512 for SD1.5 and 1024 for SDXL. For SDXL, which includes two text encoders, we utilize only the OpenCLIP ViT-bigG \cite{openclip} as the text encoder of LRM-XL.

\textbf{LPO Training.} Both SD1.5 and SDXL are fine-tuned for 5 epochs. Following SPO \cite{spo}, we employ LoRA \cite{lora} to fine-tune models. The LoRA rank is 4 for SD1.5 and 64 for SDXL. All experiments are conducted on 4 A100 GPU. Other hyperparameters are provided in Tab.\;\ref{tab:exp_detail}. The experimental setting of SD2.1 is the same as SD1.5.

\begin{table}[ht]
    \centering
    \vspace{-3mm}
    \caption{Hyperparameters of LPO training.}
    \vskip 0.05in
    \label{tab:exp_detail}
    \footnotesize
    \setlength{\tabcolsep}{1.5mm}{
    \scalebox{1.0}{
    \begin{tabular}{c c c}
         \toprule
         & SD1.5 \cite{sd1} & SDXL \cite{sdxl} \\
         \midrule
         Learning Rate & 5e-5 & 1e-5 \\
         LoRA Rank & 4 & 64 \\
         $\beta$ & 500 & 500 \\
         $K$ & 4 & 4 \\
         Sampling Threshold Range & [0.35, 0.5] & [0.45, 0.6] \\
         Sampling Batch Size & 5 & 4 \\
         Training Resolution & 512$\times$512 & 1024$\times$1024 \\
         Training Epoch & 5 & 5 \\
         Training Batch Size & 10 & 4 \\
         \bottomrule
    \end{tabular}}}
    % \vspace{-2mm}
    % \vskip -0.1in
\end{table}


\textbf{Baseline Methods.} Specifically, SPO and LPO are trained on Pick-a-Pic v1 \cite{pickscore} while Diffusion-DPO and MaPO are trained on Pick-a-Pic v2 \cite{pickscore}, an extended version of Pick-a-Pic v1. Pick-a-Pic v1 consists of 583,737 preference pairs, whereas Pick-a-Pic v2 contains over 950,000 pairs. In the practice of SPO, DDPO \cite{ddpo} and D3PO \cite{d3po} are reproduced on 4k prompts randomly sampled from Pick-a-Pic v1, which are the same as the training data used for SPO and LPO. Therefore, the comparison between these methods can be considered fair.


\section{Additional Experiments}
\label{sec:add_exp}

\begin{figure}[ht]
    \centering
    % \vspace{-10pt}
    \includegraphics[width=1.0\linewidth]{figs/user_study_v1.pdf}
    \vspace{-20pt}
    \caption{User study results on 200 prompts from HPSv2 benchmark \cite{hpsv2} and DPG-Bench \cite{dpg_bench}.}
    \label{fig:user_study}
    % \vspace{-20pt}
\end{figure}

\textbf{User Study.} We conduct human evaluation experiments for LPO, SPO, and Diffusion-DPO. The evaluation set consists of 200 prompts, with 100 randomly sampled from HPSv2 benchmark \cite{hpsv2} and 100 randomly sampled from DPG-Bench \cite{dpg_bench}. For each prompt, the model generates four images. For each image, five expert evaluators score it across three dimensions: general preference, visual appeal, and prompt alignment. Subsequently, votes are cast to determine the winning relationships among the different models. The results are illustrated in Fig.\;\ref{fig:user_study}. Similar to the results of automatic metrics in the main paper, LPO outperforms both SPO and Diffusion-DPO across three dimensions. Compared to T2I-CompBench and GenEval results, the advantage of LPO on text-image alignment in the human evaluation results is not particularly pronounced. We believe this is because the prompts used for human evaluation are not specifically designed to assess text-image alignment, and different models exhibit relatively good alignment.

\begin{table}[ht]
    \centering
    \vspace{-2.5mm}
    \caption{The correlations of reward models with aesthetics and text-image alignment, along with the preference prediction accuracy on the validation and test set of Pick-a-Pic v1.}
    \vskip 0.05in
    \label{tab:ablation_vanilla_reward}
    \footnotesize
    \setlength{\tabcolsep}{1.6mm}{
    \scalebox{1.0}{
    \begin{tabular}{c l c c c c}
         \toprule
         & Model & Aes-Corr & CLIP-Corr & VQA-Corr & Val-Test Accuracy \\
         \midrule
         \multirow{3}{*}{\makecell[c]{Specific Reward Model \\ (VLM-Based)}} & Aesthetic \cite{aesthetic} & - & - & - & 54.03 \\
         & CLIP Score (CLIP-H) \cite{clip} & - & - & - & 61.84 \\
         & VQAScore \cite{vqascore} & - & - & - & 59.16 \\
         \midrule
         \multirow{5}{*}{\makecell[c]{General Reward Model \\ (VLM-Based)}} & ImageReward \cite{imagereward} & 0.108 & 0.425 & 0.417 & 62.66 \\
         & HPSv2 \cite{hpsv2} & 0.007 & 0.602 & 0.406 & 64.76 \\
         & HPSv2.1 \cite{hpsv2} & 0.191 & 0.432 & 0.332 & 65.58 \\
         & PickScore \cite{pickscore} & 0.066 & 0.490 & 0.402 & \textbf{71.93} \\
         & Average & 0.093 & 0.487 & 0.389 & - \\
         \midrule
         \multirow{4}{*}{\makecell[c]{General Reward Model \\ (Diffusion-Based)}} & LRM-1.5 ($t=0$) & 0.115 & 0.359 & 0.339 & 65.46 \\
         & LRM-1.5 ($t=200$) & 0.111 & 0.356 & 0.336 & 67.21 \\
         & LRM-XL ($t=0$) & 0.073 & 0.403 & 0.390 & 67.44 \\
         & LRM-XL ($t=200$) & 0.089 & 0.401 & 0.385 & \underline{69.31} \\
         \bottomrule
    \end{tabular}}}
    % \vspace{-2mm}
    % \vskip -0.1in
\end{table}

\textbf{LRM as Pixel-Wise Reward Model.} The LRM can also serve as a pixel-wise reward model when combined with the corresponding VAE encoder. Tab.\;\ref{tab:ablation_vanilla_reward} shows the correlations of different reward models with aesthetics and text-image alignment. Aes-Corr is employed to assess the correlation with aesthetics, while the CLIP-Corr and VQA-Corr are utilized to measure alignment correlation. The calculation method for VQA-Corr is similar to that of CLIP-Corr, as described in Sec.\;\ref{sec:exp_setup}, with the CLIP Score replaced by VQAScore. It is observed that the LRMs, especially LRM-XL, exhibit aesthetic and alignment correlations comparable to those of VLM-based models, as indicated by similar Aes-Corr and VQA-Corr values. The CLIP-Corr of LRMs is slightly lower than the average value of VLM-based reward models, likely because  VLM-based models are fine-tuned versions of CLIP \cite{clip} or BLIP \cite{blip}, leading to greater similarity to CLIP. The accuracy on the validation and test sets of Pick-a-Pic is also present in Tab.\;\ref{tab:ablation_vanilla_reward}. LRMs demonstrate competitive performance, with larger models (LRM-XL) exhibiting higher accuracy. Moreover, we find that adding slight noise to the latent images ($t=200$) can achieve better accuracy. This can be attributed to the fact that slight noise can better activate the feature extraction capabilities and improve the generalization of diffusion models.

\begin{table}[ht]
    \centering
    \vspace{-2.5mm}
    \caption{Ablataion results on the hyperparamter $\beta$.}
    \vskip 0.05in
    \label{tab:ablation_beta}
    \footnotesize
    \setlength{\tabcolsep}{1.8mm}{
    \scalebox{1.0}{
    \begin{tabular}{c c c c c c c}
         \toprule
         $\beta$ & Aesthetic & GenEval & PickScore & ImageReward & HPSv2 & HPSv2.1 \\
         \midrule
         20 & 5.920 & 46.95 & 21.53 & 0.6407 & 27.54 & \textbf{28.13} \\
         100 & \textbf{6.031} & 48.23 & 21.68 & 0.6443 & 27.47 & 27.52\\
         \rowcolor{cyan!15}500 & 5.945 & 48.39 & \textbf{21.69} & \textbf{0.6588} & \textbf{27.64} & 27.86 \\
         1000 & 5.858 & \textbf{48.44} & 21.39 & 0.4785 & 27.22 & 26.47\\
         5000 & 5.647 & 43.87 & 20.95 & 0.2970 & 27.00 & 25.68\\
         \bottomrule
    \end{tabular}}}
    % \vspace{-2mm}
    % \vskip -0.1in
\end{table}

\textbf{Regularization HyperParameter $\beta$.} $\beta$ is a regularization hyperparameter to control the deviation of the optimized model ($p_\theta$ in Eqn.\;(\ref{eq:spo_loss})) with the reference model $p_{ref}$. As shown in Tab.\;\ref{tab:ablation_beta}, as $\beta$ increases, the regularization effect becomes stronger, which slows down the model's optimization speed and leads to poorer performance. Conversely, if $\beta$ is too small ($\beta=20$), the regularization constraint becomes too weak, potentially reducing the model's generalization ability and decreasing the quality of the generated images. The performance is optimal when $\beta$ is within the range of 100 to 500.



\begin{table}[ht]
    \centering
    \vspace{-2.5mm}
    \caption{Ablataion results on the dynamic threshold strategies.}
    \vskip 0.05in
    \label{tab:ablation_dyn_th}
    \footnotesize
    \setlength{\tabcolsep}{1.8mm}{
    \scalebox{1.0}{
    \begin{tabular}{l c c c c c c}
         \toprule
         Strategy & Aesthetic & GenEval & PickScore & ImageReward & HPSv2 & HPSv2.1 \\
         \midrule
         \rowcolor{cyan!15}Standard Deviation & \textbf{5.945} & \textbf{48.39} & \textbf{21.69} & \textbf{0.6588} & \textbf{27.64} & \textbf{27.86} \\
         Variance & 5.921 & 47.78 & 21.57 & 0.5054 & 27.42 & 26.93 \\
         Timestep & 5.929 & 48.22 & 21.65 & 0.6465 &  27.42 & 27.25\\
         \bottomrule
    \end{tabular}}}
    % \vspace{-2mm}
    % \vskip -0.1in
\end{table}

\textbf{Other Dynamic Threshold Strategy.} In the main paper, we dynamically correlate the threshold in LPO sampling with the standard deviation $\sigma_t$ in DDPM \cite{ddpm}. Here we explore alternative strategies. The first strategy involves replacing $\sigma_t$ with variance $\sigma_t^2$. Consequently, the threshold can be formulated as:
\begin{equation}
    th_t = \frac{\sigma^2_t-\sigma^2_{min}}{\sigma^2_{max}-\sigma^2_{min}}*(th_{max}-th_{min})+th_{min}.
    \label{eq:dyn_thresh_var}
\end{equation}
The second strategy is linearly correlating the threshold with the timestep $t$:
\begin{equation}
    th_t = \frac{t-t_{min}}{t_{max}-t_{min}}*(th_{max}-th_{min})+th_{min},
    \label{eq:dyn_thresh_linear}
\end{equation}
where $t_{min}$ and $t_{max}$ denote the minimum and maximum optimization timesteps, respectively. The values of different strategies are illustrated in Fig.\;\ref{fig:dyn_thresh}. The threshold range is $[0.35, 0.5]$ and the timestep range is $[0, 950]$. It can be observed that the standard deviation strategy sets higher thresholds for middle timesteps, while the variance strategy sets lower thresholds. As shown in Tab.\;\ref{tab:ablation_dyn_th}, the standard deviation strategy achieves the best performance across various metrics. This may be because the standard deviation strategy better accommodates the variations between $x_t$ at different timesteps. 

\begin{figure}[ht]
    \begin{minipage}{0.49\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/dyn_thresh_v2.pdf}
    \vspace{-23pt}
    \caption{The thresholds of different strategies.}
    \label{fig:dyn_thresh}
    \vspace{-5pt}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/sample_num_v1.pdf}
    \vspace{-23pt}
    \caption{The number of valid samples of different timesteps .}
    \label{fig:sample_num}
    \vspace{-5pt}
    \end{minipage}
\end{figure}

Fig.\;\ref{fig:sample_num} illustrates the number of valid samples of different timesteps for both constant and dynamic (standard deviation) threshold strategies. Dynamic threshold strategy significantly increases the number of valid samples, especially at intermediate timesteps.

\section{Additional Visualizations}
\label{sec:add_vis}

\textbf{Predicted Images $I_t$ at Different Timesteps.} Fig.\;\ref{fig:vis_it} illustrates the predicted images $I_t$ generated through the diffusion forward and VAE decoding processes, as shown in Fig.\;\ref{fig:pipeline} (a). As discussed in the main paper, predicted images at large timesteps tend to be very blurred, causing a significant distribution shift from the original images. This makes it challenging for PRMs to adapt to these blurred images without adequate pre-training and extensive datasets, resulting in unreliable predictions at large timesteps.

\textbf{Optimization Timesteps.} The generated images of different optimization timestep ranges are illustrated in Fig.\;\ref{fig:vis_timestep}. Larger timestep ranges result in more pronounced improvements in the quality of the generated images. We think there are two main reasons. Firstly, small timesteps in the denoising process primarily focus on high-frequency details, which do not lead to significant changes in the layout and style of the image. Secondly, as indicated in Fig.\;\ref{fig:sample_num}, the smaller the timestep, the fewer the valid samples, resulting in less optimization at the corresponding timesteps. Furthermore, compared to images in the range $[750,950]$, those in $[0,950]$ exhibit richer details, including both foreground and background, which also demonstrates that optimization at smaller timesteps aids in the enhancement of image details.


\textbf{More Comparison.} Fig.\;\ref{fig:vis_15_1} presents the generated images of different methods based on SD1.5 \cite{sd1}. Fig.\;\ref{fig:vis_xl_1} and Fig.\;\ref{fig:vis_xl_2} show the larger version of images in Fig.\;\ref{fig:main_comparison}. Fig.\;\ref{fig:vis_xl_3} and Fig.\;\ref{fig:vis_xl_4} provide more generated images of various methods based on SDXL \cite{sdxl}. Some keywords in prompts are emphasized using \textbf{bold} formatting.

\begin{figure}[ht]
    \centering
    % \vspace{-10pt}
    \includegraphics[width=1.0\linewidth]{figs/predicted_x0_sd15_v3.pdf}
    \vspace{-23pt}
    \caption{Predicted images $I_t$ in Fig.\;\ref{fig:pipeline}\;(a) at different timesteps. The original images come from the Pick-a-Pic v1 dataset \cite{pickscore}.}
    \label{fig:vis_it}
    % \vspace{-20pt}
\end{figure}

\begin{figure}[t]
    \centering
    % \vspace{-10pt}
    \includegraphics[width=1.0\linewidth]{figs/optimization_timestep_v3.pdf}
    \vspace{-20pt}
    \caption{Qualitative comparison of various optimization timestep ranges based on SDXL.}
    \label{fig:vis_timestep}
    % \vspace{-20pt}
\end{figure}

\begin{figure}[p]
    \centering
    \vspace{-10pt}
    \includegraphics[width=0.92\linewidth]{figs/main_comparison_15_v4.pdf}
    \vspace{-10pt}
    \caption{Qualitative comparison of various preference optimization methods based on SD1.5.}
    \label{fig:vis_15_1}
    % \vspace{-20pt}
\end{figure}

\begin{figure}[p]
    \centering
    % \vspace{-10pt}
    \includegraphics[width=0.85\linewidth]{figs/main_comparison_xl1_v2.pdf}
    \vspace{-6pt}
    \caption{Qualitative comparison of various preference optimization methods based on SDXL.}
    \label{fig:vis_xl_1}
    % \vspace{-20pt}
\end{figure}

\begin{figure}[p]
    \centering
    % \vspace{-10pt}
    \includegraphics[width=0.85\linewidth]{figs/main_comparison_xl2_v3.pdf}
    \vspace{-10pt}
    \caption{Qualitative comparison of various preference optimization methods based on SDXL.}
    \label{fig:vis_xl_2}
    % \vspace{-20pt}
\end{figure}

\begin{figure}[p]
    \centering
    % \vspace{-10pt}
    \includegraphics[width=0.8\linewidth]{figs/main_comparison_xl3_v3.pdf}
    \vspace{-10pt}
    \caption{Qualitative comparison of various preference optimization methods based on SDXL.}
    \label{fig:vis_xl_3}
    % \vspace{-20pt}
\end{figure}


\begin{figure}[p]
    \centering
    % \vspace{-10pt}
    \includegraphics[width=0.8\linewidth]{figs/main_comparison_xl4_v3.pdf}
    \vspace{-10pt}
    \caption{Qualitative comparison of various preference optimization methods based on SDXL.}
    \label{fig:vis_xl_4}
    % \vspace{-20pt}
\end{figure}
