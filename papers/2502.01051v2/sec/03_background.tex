\section{Background}
\label{sec:background}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/pipeline_v9.pdf}
    \vspace{-22pt}
    \caption{The training pipelines of PRM (a) and LRM (b) for step-level reward modeling, and LPO (c). Compared to PRM, LRM can directly process noisy latent images $x_t$. Therefore, LPO is able to perform sampling and training within the latent space.}
    \label{fig:pipeline}
    \vspace{-5pt}
\end{figure*}


\subsection{Latent Diffusion Models}
The forward process of diffusion models gradually adds random Gaussian noise to clean latent images $x_0$ to obtain noisy latent images $x_t$, in the manner of a Markov chain:
\begin{equation}
    q(x_t|x_{t-1})=\mathcal{N}(\sqrt{\alpha_t}x_{t-1}, (1-\alpha_t)\mathbf{I}),
    \label{eq:diffusion_forward}
\end{equation}
where $t$ is the timestep and $\mathcal{N}(\mu,\Sigma)$ denotes the Gaussian distribution. $\alpha_t=1-\beta_t$, where $\beta_t$ is a pre-defined time-dependent variance schedule. $\mathbf{I}$ denotes the identity matrix. The backward process aims to denoise $x_t$, which can be formulated as follows according to DDIM \cite{ddim}:
\begin{equation}
    p_{\theta}(x_{t-1}|x_t)=\mathcal{N}(\mu_t, \sigma_t^2\epsilon_t), \epsilon_t \sim \mathcal{N}(\mathbf{0},\mathbf{I}), \label{eq:diffusion_backward} \\
\end{equation}
\begin{equation}
    \mu_t=\sqrt{\bar{\alpha}_{t-1}}\hat{x}_{0,t}+\sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2}\cdot\epsilon_{\theta}^{(t)}, \\
\end{equation}
\begin{equation}
    \hat{x}_{0,t}=(\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_{\theta}^{(t)}}{\sqrt{\bar{\alpha}_t}}), \label{eq:predicted_x0} \\
\end{equation}
\begin{equation}
    \sigma_t=\eta\sqrt{(1-\bar{\alpha}_{t-1})/(1-\bar{\alpha}_t)}\sqrt{1-\alpha_t},
\end{equation}
where $\bar{\alpha}_t=\prod_{i=1}^t\alpha_t$ and $\eta \in [0,1]$ is a hyperparameter to adjust the standard deviation $\sigma_t$. $\epsilon_{\theta}^{(t)}$ and $\hat{x}_{0,t}$ denote the noise and clean latent images predicted by diffusion models with parameters $\theta$ at timestep $t$.


\subsection{Preference Optimization for Diffusion Models}

Given winning latent images $x_0^w$, losing ones $x_0^l$, and condition $c$, Diffusion-DPO \cite{diffusion_dpo} and D3PO \cite{d3po} propagate the preference order of $(x_0^w, x_0^l)$ to all denoising steps, thereby freely generating intermediate preference pairs $(x_t^w, x_t^l)$. They encourage models $p_{\theta}$ to generate $x_t^w$ rather than $x_t^l$ by minimizing
\vspace{-1pt}
\begin{multline}
    L_{DPO} = -\mathbb{E}_{\substack{x^w_{t,t+1} \sim p_\theta(x^w_{t,t+1}|x^w_0,c), \\
    x^l_{t,t+1} \sim p_\theta(x^l_{t,t+1}|x^l_0,c)}}\biggl[\log\sigma\biggl( \\
    \beta\log\frac{p_\theta(x_{t}^w|x_{t+1}^w,c)}{p_{ref}(x_{t}^w|x_{t+1}^w,c)}-\beta\log\frac{p_{\theta}(x_{t}^l|x_{t+1}^l,c)}{p_{ref}(x_{t}^l|x_{t+1}^l,c)}\biggr)\biggr],
\end{multline}
where $\beta$ is a regularization hyperparameter and $p_{ref}$ denotes the reference model, fixed to the initial value of $p_{\theta}$.

However, the preference order along the denoising process is not always identical. Therefore, SPO \cite{spo} proposes SPM to predict preferences for intermediate steps, and sample the win-lose pairs $(x^w_{t}, x^l_{t})$ from the same $x_{t+1}$ to make the two paths comparable. 
The optimization objective of SPO is to minimize
\vspace{-2pt}
\begin{multline}
    L_{SPO} = -\mathbb{E}_{x^w_{t},x^l_{t} \sim p_\theta(x_{t}|x_{t+1},c)}\biggl[\log\sigma\biggl( \\
    \beta\log\frac{p_\theta(x_{t}^w|x_{t+1},c)}{p_{ref}(x_{t}^w|x_{t+1},c)}-\beta\log\frac{p_{\theta}(x_{t}^l|x_{t+1},c)}{p_{ref}(x_{t}^l|x_{t+1},c)}\biggr)\biggr].
    \label{eq:spo_loss}
\end{multline}
As a Pixel-level Reward Model (PRM), SPM faces some common issues discussed in Sec.\;\ref{sec:introduction}. First, as illustrated in Fig.\;\ref{fig:pipeline} (a), it requires the forward of the diffusion model and VAE decoder. Second, it struggles to process highly blurred $I_t$ when $t$ is large. Third, although the AdaLN block \cite{dit} is introduced in SPM to improve its sensitivity to timesteps, insufficient pre-training makes it challenging to understand the focus of different timesteps on image generation. These issues jointly diminish the effectiveness of SPM in step-level preference optimization.