\section{Discussion and Conclusion}
\label{sec:discussion}


\textbf{Conclusion.} In this paper, we propose LRM to utilize diffusion models for step-level reward modeling, based on the insights that diffusion models possess text-image alignment abilities and can perceive noisy latent images across different timesteps. To facilitate the training of LRM, the MPCF strategy is introduced to address the inconsistent preference issue in LRM's training data. We further propose LPO, a method that employs LRM for step-level preference optimization, operating entirely within the latent space. LPO not only significantly reduces training time but also delivers remarkable performance improvements across various evaluation dimensions, highlighting the effectiveness of employing the diffusion model itself to guide its preference optimization. We hope our findings can open new avenues for research in preference optimization for diffusion models and contribute to advancing the field of visual generation.

\textbf{Limitations and Future Work.} (1) The experiments in this work are conducted on UNet-based models and the DDPM scheduling method. Further research is needed to adapt these findings to larger DiT-based models \cite{sd3} and flow matching methods \cite{flow_match}. (2) The Pick-a-Pic dataset mainly contains images generated by SD1.5 and SDXL, which generally exhibit low image quality. Introducing higher-quality images is expected to enhance the generalization of the LRM. (3) As a step-level reward model, the LRM can be easily applied to reward fine-tuning methods \cite{alignprop, draft}, avoiding lengthy inference chain backpropagation and significantly accelerating the training speed. (4) The LRM can also extend the best-of-N approach to a step-level version, enabling exploration and selection at each step of image generation, thereby achieving inference-time optimization similar to GPT-o1 \cite{gpt_o1}.