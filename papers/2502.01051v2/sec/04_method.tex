\section{Methodology}
\label{sec:method}

To address the limitations of PRMs, we propose the Latent Reward Model (LRM), which leverages diffusion models for reward modeling. To facilitate its training, we analyze the preference dataset and propose a Multi-Preference Consistent Filtering (MPCF) strategy. Finally, we introduce Latent Preference Optimization (LPO), a method that optimizes diffusion models in the latent space using LRM.

\subsection{Latent Reward Model}
\label{sec:lrm}
As described in Sec.\;\ref{sec:introduction}, the pre-trained diffusion model is remarkably suitable for step-level reward modeling due to its inherent ability to align texts and images, extract features directly from $x_t$ at any timestep, and naturally accept timestep inputs. Therefore, we introduce the Latent Reward Model (LRM) to employ diffusion models for reward modeling.
% \textbf{Superority of Diffusion Models.} As described in Sec.\;\ref{sec:introduction}, the pre-trained diffusion model is remarkably suitable for step-level reward modeling, due to the following characteristics. (1) Diffusion models possess cross-modality alignment capabilities between texts and images, thanks to the extensive pre-training dataset of image-text pairs and the pre-training target of text-to-image generation. (2) Diffusion models can extract useful information directly from noisy latent images $x_t$ at any timesteps, as they did during pre-training. (3) Diffusion models can naturally accept timestep inputs and recognize the focus of different denoising steps.


\textbf{Architecture of LRM.} LRM leverages features of the UNet and text encoder in the diffusion model for preference prediction, as depicted in Fig.\;\ref{fig:lrm}. 
% \textbf{Architecture of LRM.} Therefore, we propose the LRM, which utilizes the UNet and text encoder of diffusion models for preference prediction, as depicted in Fig.\;\ref{fig:lrm}. 
Specifically, the textual features $f_p\in\mathbb{R}^{n_l\times n_p}$ are extracted from the prompt $p$ by the text encoder, where $n_l$ and $n_p$ denotes the number of tokens and the dimension of textual features, respectively. Following CLIP \cite{clip}, we use the last token $f_{eos}\in\mathbb{R}^{1\times n_p}$ to represent the entire prompt and incorporate a text projection layer to obtain the final textual features $T\in\mathbb{R}^{1\times n_d}$, where $n_d$ represents the final dimension of textual and visual features. Each noisy latent image $x_t$ is passed through the UNet to interact with textual features $f_p$. The visual features of UNet are averagely pooled along the spatial dimension, resulting in the multiscale down-block features $V_{down}$ and middle-block features $V_{mid}$ as follows
\begin{gather}
    V_{down},V_{mid}=\text{AvgPool}(\text{UNet}(x_t,f_p)), 
\end{gather}
where $V_{down}=\{V_{d_i},i=1,\ldots,L\}$. $L$ is the number of down blocks and $V_{d_i}$ represents features of the $i$-th down blocks. However, it is observed that these features lack sufficient correlations with the textual features. Inspired by the Classifier-Free Guidance \cite{cfg}, we propose the \textbf{Visual Feature Enhancement (VFE)} module to enhance correlations between visual and textual features. It first extracts middle-block features $V_{mid\_ucond}$ that are independent of textual information following a process similar to extracting $V_{mid}$, but with a null prompt. Then the enhanced visual features $V_{enh}\in\mathbb{R}^{1\times n_m}$ can be calculated:
\begin{equation}
    % V_{enh}=V_{ucond}+gs*(V_{cond}-V_{ucond}),
    V_{enh}=V_{mid}+(gs-1)*(V_{mid}-V_{mid\_ucond}),
    \label{eq:cfg}
\end{equation}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/lrm_v4.pdf}
    \vspace{-24pt}
    \caption{The architecture of LRM. The text encoder and UNet of diffusion models are utilized for preference prediction. The VAE encoder is only used in training.}
    \label{fig:lrm}
    \vspace{-14pt}
\end{figure}
where $gs\ge 1$ is a hyperparameter and $n_m$ denotes the dimension of $V_{mid}$. As $gs$ increases, $V_{mid}$ incorporates more features related to the text, leading to an enhanced focus on text-image alignment. When $gs=1$, the VFE module is not utilized. Next, $V_{enh}$ are concatenated with down-block features $V_{down}$ along the channel dimension and projected into the final visual features $V\in\mathbb{R}^{1\times n_d}$ by the visual projection layer. Finally, the preference score of $x_t$ and $p$ is computed as the dot product between textual and visual features after applying $l_2$ normalization:
\begin{equation}
    S(p, x_t)=\tau*(l_2(V)\cdot l_2(T)),
    \label{eq:score}
\end{equation}
where $\tau$ denotes the temperature coefficient following CLIP.


\textbf{Training Loss.} LRM is trained on the public preference dataset Pick-a-Pic v1 \cite{pickscore}, denoted as $D$, which consists of triples $(I^w, I^l, p)$, each containing a preference image pair and a corresponding prompt. As depicted in Fig.\;\ref{fig:pipeline}\;(b), given uniformly sampled timestep $t \sim \mathcal{U}(0,T)$ where $T$ denotes the total denoising steps, we use the VAE encoder of the diffusion model to transform pixel-level images $I$ into latent images $x_0$, and then add $x_0$ with noise $\epsilon_t \sim \mathcal{N}(\mathbf{0},\mathbf{I})$ to simulate $x_t$ in the backward denoising process. Following the Bradley-Terry (BT) model \cite{bt}, the training loss of LRM is formulated as:
\begin{gather}
    L_{LRM} = - \mathbb{E}_{\substack{t\sim\mathcal{U}(0,T),\\ (I^w,I^l,p)\in D}}\log \frac{e^{S(p,x_t^w)}}{e^{S(p,x_t^w)}+e^{S(p,x_t^l)}},
    \label{eq:loss_lrm}  \\
    x_t^*=\sqrt{\bar{\alpha}_t} * \text{VAE}_{\text{encoder}}(I^*)+\sqrt{1-\bar{\alpha}_t}*\epsilon_t^*,
\end{gather}
where $*\in\{w,l\}$. During training, the parameters of VAE are frozen to ensure that the latent space of the LRM remains stable, while the text encoder and UNet are trainable.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/hist_analysis_v2.pdf}
    \vspace{-24pt}
    \caption{The distribution of preference score gaps in Pick-a-Pic.}
    \label{fig:hist_anslysis}
    \vspace{-10pt}
\end{figure}

\subsection{Multi-Preference Consistent Filtering}
\label{sec:lrm_train}

\textbf{Issue of Inconsistent Preferences.} The training loss of LRM in Eqn.\;(\ref{eq:loss_lrm}) involves the assumption that if $x^w_0$ is preferred over $x^l_0$, then after adding noise of equal intensity, the preference order remains unchanged, meaning $x^w_t$ continues to be preferred over $x^l_t$. However, this assumption breaks down when the winning image excels in one aspect but is inferior in other aspects. For example, if the winning image $x^w_0$ has better details but exhibits weaker text-image alignment compared to the losing image $x_0^l$, the advantage in detail may be diminished after introducing significant noise, making the noisy losing image $x_t^l$ potentially more preferred than $x_t^w$. Such situations frequently occur in the Pick-a-Pic \cite{pickscore} dataset. To demonstrate this, we employ the Aesthetic Score $S_A$ \cite{aesthetic} to evaluate the aesthetic quality while using CLIP Score $S_C$ \cite{clip} and VQAScore $S_V$ \cite{vqascore} to assess the text-image alignment. Let $G_*=S_*(I^w)-S_*(I^l), *\in \{A,C,V\}$ denote the preference score gaps between winning and losing images. As illustrated in Fig.\;\ref{fig:hist_anslysis}, nearly half of the winning images have Aesthetic Scores lower than those of losing images. Similarly, around 40\% of the winning images exhibit lower CLIP Scores and VQAScores. When considering both Aesthetic and CLIP scores, over 70\% of the winning images score lower than the losing images in at least one aspect. 

\textbf{Multi-Preference Consistent Filtering.} We hypothesize that if the winning image outperforms the losing image across various aspects, the preference order is more likely to remain consistent after introducing similar noise. Based on this hypothesis, we propose the Multi-Preference Consistent Filtering (MPCF) strategy, which aims to approximate the condition of the above hypothesis by filtering data using various preference score gaps. Pick-a-Pic v1 contains 511,840 win-lose pairs and 71,907 tie pairs. Three strategies are explored for win-lose pairs, as detailed in Tab.\;\ref{tab:filter}. The first strategy is the most strict, but it is observed that it causes the LRM to overfit to the aesthetic aspect while neglecting the text-image alignment. In contrast, the third strategy is the most lenient regarding aesthetics, resulting in the LRM being unable to perceive the aesthetics of the images. It is found that the second strategy effectively balances both aspects, leading us to choose it ultimately. Further investigations and experiments are detailed in Sec.\;\ref{sec:ablation_study}.


\begin{table}[t]
    \centering
    \vspace{-2.5mm}
    \caption{Explored data filter strategies of Pick-a-Pic.}
    \vskip 0.05in
    \label{tab:filter}
    % \vspace{-2mm}
    \centering
    \scriptsize
    % \footnotesize
    \setlength{\tabcolsep}{1.0mm}{
    \scalebox{1.0}{
    \begin{tabular}{l c c c}
        \toprule
        Part & Strategy & Filter Rule & Num  \\
        \midrule
        \multirow{3}{*}{Win-Lose} & 1 & $G_A\ge0, G_C\ge0, G_V\ge0$ & 101,573 \\
        & \cellcolor{cyan!15}2 & \cellcolor{cyan!15}$G_A\ge-0.5, G_C\ge0, G_V\ge0$ & \cellcolor{cyan!15}168,539 \\
        & 3 & $G_A\ge-1, G_C\ge0, G_V\ge0$  & 201,885 \\
        \midrule
        Tie & \cellcolor{cyan!15}- & \cellcolor{cyan!15}$|G_A|\le0.2, |G_C|\le0.03, |G_V|\le0.07$ & \cellcolor{cyan!15}8,537 \\
        \bottomrule
    \end{tabular}}}
    % \vspace{-3mm}
    \vskip -0.15in
\end{table}

\subsection{Latent Preference Optimization}
\label{sec:lpo}

To utilize LRM for step-level preference optimization, we propose the Latent Preference Optimization (LPO) method.

\textbf{Sampling and Training.}  As illustrated in Fig.\;\ref{fig:pipeline}\;(c), at each timestep $t$, LPO samples a set of $x^i_{t},i=1,...,K$ from the same $x_{t+1}$. These noisy latent images are then directly fed into LRM to predict preference scores $S^i_{t},i=1,...,K$. The highest and lowest scores are normalized by the SoftMax function. If their score gap exceeds a predefined threshold $th_{t}$, the $x_{t}$ with the highest score is selected as $x_{t}^w$, while the one with the lowest score is designated as $x_{t}^l$, forming a qualified training sample $(x_{t+1},x_{t}^w,x_{t}^l)$. Finally, these samples are used to optimize the diffusion model, using the same loss with SPO, \ie Eqn.\;(\ref{eq:spo_loss}). As a result, the entire process of LPO is performed within the latent space, eliminating the need to decode into pixel space, which significantly enhances the training efficiency of LPO.

\textbf{Optimization Timesteps.} Due to the inaccuracy of SPM at high-noise levels, SPO samples training data only at low to medium noise levels, specifically when $t\in[0, 750]$. In contrast, owing to the noise-aware capability of LRM, LPO is able to collect training samples throughout the entire denoising process, covering $t\in[0,950]$. Experiments in Sec.\;\ref{sec:ablation_study} demonstrate that training samples collected at high noise levels play a crucial role in preference optimization.

\textbf{Dynamic Threshold.} The sampling threshold $th$ is a crucial hyperparameter in LPO. A higher threshold tends to sample win-lose pairs with more pronounced differences, improving the quality of the training samples but reducing their overall quantity. Conversely, a lower threshold increases the number of training samples but may introduce a certain amount of noisy samples. Given that the standard deviation $\sigma_t$ in Eqn.\;(\ref{eq:diffusion_backward}) decreases with $t$, using a constant threshold across all timesteps can hinder the training effectiveness. To handle this, we implement a dynamic threshold strategy, which sets lower thresholds for smaller timesteps, as follows:
\begin{equation}
    th_t = \frac{\sigma_t-\sigma_{min}}{\sigma_{max}-\sigma_{min}}*(th_{max}-th_{min})+th_{min},
    \label{eq:dyn_thresh}
\end{equation}
where $\sigma_{max}$ and $\sigma_{min}$ are the maximum and minimum values of $\sigma_t$. The hyperparameters $th_{max}$ and $th_{min}$ are used to adjust the range of thresholds. 

\textbf{Selection of LRM in LPO.} To avoid confusion, we refer to the diffusion model optimized in LPO as DMO. Since LPO is performed within the latent space, which is determined by the VAE encoder, the VAE of LRM should be identical to that of DMO. Therefore, the specific architecture of LRM can be initialized from DMO or any other diffusion models that share the same VAE encoder with DMO, as depicted in Fig.\;\ref{fig:illustration}\;(b). The former is termed \textit{homogeneous optimization} because LRM and DMO share the same architecture, whereas the latter is called \textit{heterogeneous optimization}.