
\section{Introduction}

An asynchronous time series (also named \textit{temporal event sequence} or \textit{continuous-time event sequence}) is a temporally ordered set of events that describe the progression of actions or occurrences. Asynchronous time series are ubiquitous in daily life, such as healthcare \citep{Lorch2018Stochastic, Rizoiu2018SirHawkes}, finance \citep{Bacry2015Hawkes, Jin2020Visual}, e-commerce \citep{Hernandez2017Analysis}, and social media \citep{Zhang2022Counterfactual, Kong2023Interval}. In each of those domains, predicting the next events plays a crucial role. %Unlike time series which carry regular time stamps, asynchronous time series data is a sequence of events that do not necessarily follow any time pattern and modeling them has presented new challenges.%  

Unlike regular time series, which consist of values at evenly spaced time intervals (like weather measurements), asynchronous time series consist of multiple types of discrete events occurring sporadically over time. For example, in the context of social media platforms like X (Twitter), user interactions (likes, comments, shares, and follows) happen sporadically and at irregular intervals~\citep{zhao2015seismic}.  Each such type of interaction with a user's profile represents an event type, and together with their timestamps, form an asynchronous time series~ \citep{Xue2023EasyTPP}. Modeling such asynchronous time series is challenging due to the irregular timing and the diversity of event types, which contrasts with the uniformity and regularity of traditional time series data~\citep{schirmer2022modeling,horn2020set,zhangirregular}.

% {Unlike regular time series, which consist of numerical values at evenly spaced time intervals, asynchronous time series involve events occurring at irregular intervals without any consistent time pattern. This irregularity, combined with the complexity of event descriptions, makes modeling asynchronous time series significantly more challenging than regular time series. \cite{schirmer2022modeling} \citep{horn2020set} \citep{zhangirregular} }

Traditionally, to model asynchronous time series, events are grouped into a fixed, small number of categorical types~\citep{Xue2023EasyTPP}. Separate stochastic processes—such as Poisson processes or Hawkes processes—are then modeled for each event type to predict which event will occur next and when~\citep{Mei2022Transformer,Hawkes1971Spectra}. However, this approach presents several significant drawbacks. \textit{Firstly}, it inherently limits research to datasets with a small number of event types because modeling each event type separately becomes increasingly computationally intensive as the number of event types grows \citep{Zuo2020Transformer}. \textit{Secondly}, events can vary widely and may not fit neatly into predefined categories. \textit{Thirdly}, this method leads to the loss of meaningful natural language descriptions associated with the events. \textit{Fourthly}, these methods treat each event type independently, ignoring any interactions between them --- for example, likes and shares of a tweet are not independent events. \textit{Lastly}, extending these methods to other tasks require significant theoretical development~ \citep{shchur2021detecting}. % Therefore, there is a need for modeling approaches that can handle a larger number of event types and leverage the detailed information available in natural language event descriptions. }

Deep learning models have significantly revolutionized techniques for time series modeling, and even more so with the introduction of transformers \citep{Vaswani2017Attention}. However, there are often limitations due to the scarcity of training data, overfitting in specific domains, and the highly specialized architectural designs. In response to those challenges, Large Language Models (LLMs) have emerged as a powerful and promising direction to model time series data. For example,  \citet{Gruver2023Large, Zhou2023One, Xue2023Promptcast, Jin2023TimeLLM} have illustrated how LLMs can be used as time series forecasters when the input time series is encoded as a string of numeric digits, by  casting the time series forecasting problem as a next-token prediction in text, hence unlocking the use of powerful pre-trained models. LLMs have also been explored in other domains like action forecasting from videos \citep{zhaoantgpt, wang2024lifelongmemory}. {However, these approaches focus on regular time series with evenly spaced numerical observations and cannot be directly applied to asynchronous time series due to their irregular intervals and diverse event types described in natural language. While LLMs have recently been explored for action recognition and action forecasting from videos \citep{zhaoantgpt,wang2024lifelongmemory}, applying LLMs to textual asynchronous time series over multiple tasks (like anomaly detection and imputation) remains largely unexplored. }


% Inspired by the successes of pre-trained language foundation models, we asked whether LLMs could be adapted to work on asynchronous time series data.




% \begin{figure}[ht]
% % \centering
% % \begin{subfigure}[b]{0.48\textwidth}
% %     \includegraphics[width=\textwidth]{figures/next_token_prediction}
% %     \caption{\textbf{Next token prediction.} The model is given a sequence of words/tokens with the goal of predicting the next word/token.}
% %     \label{fig:intro_next_token}
% % \end{subfigure}


\begin{figure}[ht]
\centering
\begin{minipage}[b]{\columnwidth}
\centering
    \includegraphics[width=\linewidth]{figures/forecasting}
    % \caption{\textbf{Forecasting} The model is given a sequence of events, encoded as text, with the goal of predicting the next event.}
    % \caption{\textbf{Forecasting}}
    % \textbf{(a) Forecasting}
    \label{fig:intro_forecasting}
\end{minipage} 
\begin{minipage}[b]{\columnwidth}
\centering
    \includegraphics[width=\linewidth]{figures/anomaly_detection_mod.pdf}
    % \caption{\textbf{Anomaly detection} The model is given a sequence of events containing an incorrect event (bold) with the goal of finding the incorrect event.}
    % \textbf{(b) Anomaly detection}
    \label{fig:intro_anomaly_detection}
\end{minipage}
\begin{minipage}[b]{\columnwidth}
\centering
    \includegraphics[width=\linewidth]{figures/imputation}
    % \caption{\textbf{Imputation} The model is given a sequence of events containing a masked event, encoded as text, with the goal of predicting the masked event.}
    % \caption{\textbf{Imputation}}
    % \textbf{(c) Imputation}
    \label{fig:intro_imputation}
\end{minipage}
\caption{We show that our LASTS framework can solve the following tasks on asynchronous time series data: (a)\textbf{Forecasting:} \textit{(top)} The model is given a sequence of events, encoded as text, with the goal of predicting the next event. (b)\textbf{Anomaly detection:} \textit{(middle)} The model is given a sequence of events containing an incorrect event (bold) with the goal of finding the incorrect event. (c)\textbf{Imputation: }\textit{(bottom)} The model is given a sequence of events containing a masked event, encoded as text, with the goal of predicting the masked event.}

\label{fig:overall}
\end{figure}

%\caption{\textbf{LASTS}, the framework we propose for using LLMs to model asynchronous time series by encoding the sequence of events with natural language labels. LLMs have shown great performances to solve NLP tasks by predicting the next token given a sequence of tokens. We show that our model can solve tasks on asynchronous time series data such as  forecasting, anomaly detection and imputation. \lilian{I made the caption more concise.}}

% \caption{We propose a new method, named LASTS, for modeling asynchronous time series with LLMs, by encoding the sequence of events as text.
% (a) LLMs have shown great performances to solve NLP tasks by predicting the next token given a sequence of tokens. We show this property can be used to solve tasks on asynchronous time series data such as forecasting (b), anomaly detection (c) and imputation (d). 
% Each event is represented by the inter-arrival time of its occurrence and its event type. Unlike standard asynchronous time series models, our approach uses natural language event descriptions to extract richer semantic representations, resulting in greater accuracy.}


This paper presents \textbf{LASTS} (\textbf{L}anguage-modeled-\textbf{As}ynchronous \textbf{T}ime \textbf{S}eries), a novel prompting-based framework to adapt LLMs to asynchronous time series data while keeping the backbone model intact. To the best of our knowledge, this is the first work to explore the capabilities of LLMs to process textual asynchronous time series data and works on multiple tasks as shown in \autoref{fig:overall}. {Our framework overcomes the drawbacks presented by traditional approaches for modeling asynchronous time series --- it can handle datasets with numerous event types easily, it does not need to group events into predefined categorical bundles, it retains and utilizes the natural language descriptions of event types, and it is able to leverage the rich interactions between different event types.} Our contributions can be summarized as follows: 
% \begin{itemize}
% \itemsep0em 
% % \item We introduce \textbf{LASTS}, a new LLM-based method on asynchronous times series with actions described in natural language instead of a fixed set of actions classes. We present experimental results to show that our approach expands the adaptability of LLMs to downstream tasks such as next-action predictions, imputations and anomaly detection.
% \item We introduce \textbf{LASTS}, a method for representing asynchronous time series and designing efficient prompts to leverage LLMs for performing various tasks on such data in a zero-shot manner. We present experimental results demonstrating that our approach adapts LLMs to tasks like forecasting, imputation, and anomaly detection.

% \item We propose \textbf{Stochastic Soft Prompt (StoP)}, an interpretable adaptation of soft prompting as a parameter-efficient way to adapt an LLM to asynchronous time series. 
% During training, the soft prompts are randomly truncated to learn more diverse representations, and to enhance our model's adaptability to carry out various downstream tasks.
% % By attaching a learnable tensor to the original text prompt that was randomly truncated during training, StoP enhances our the model's adaptability to carry out various downstream tasks while avoiding overfitting. 
% % \todo{Review the last line} 
% %\thibaut{I made some changes} \lilian{I like the changes. Sounds much more concise.}

% \item  Comprehensive evaluations of the proposed method demonstrate its \textbf{effectiveness to model asynchronous time series, including in zero-shot scenarios.} 
% We show that our model outperforms the leading state-of-the-art methods on multiple tasks and datasets.
% Our strong results and analyses highlight the potential of LLM-based methods to model asynchronous time series.


% %\item Unified model to work on multiple tasks. Do we want to analyze the transfer between datasets or tasks? Most of the existing models are specialized for a task \eg forecasting. \lilian{TODO: Review needed. Sorry i don't understand this point.} 
% \end{itemize}


%\todo{Do we show treating an asynchronous time series as a regularly samples time series with missing values?}

\begin{itemize}
    \item \textbf{We introduce LASTS (Language-modeled Asynchronous Time Series)}, a novel framework that leverages Large Language Models (LLMs) to model asynchronous time series data, while effectively handling datasets with a large number of event types without the need for predefined categorical groupings. To the best of our knowledge, this is the first work to explore the capabilities of LLMs to process textual asynchronous time series data across multiple tasks such as forecasting, anomaly detection, and data imputation.

\item \textbf{We introduce Stochastic Soft Prompting (StoP)} which is an novel prompt-tuning mechanism that serves as a parameter-efficient method to adapt LLMs to asynchronous time series data. StoP learns soft prompts that significantly improve model performance and generalizability by randomly truncating the prompts during training to learn more diverse representations.

% \item \textbf{LASTS extends the scope of asynchronous time series analysis} beyond forecasting. Our framework allows the application of LLMs to tasks like anomaly detection and data imputation in asynchronous time series, which were previously unexplored in this domain, thus demonstrating the versatility of our approach.

\item \textbf{We conduct comprehensive evaluations} on real-world datasets across multiple tasks to demonstrate the effectiveness of our proposed method. Our approach achieves state-of-the-art performance, outperforming existing methods, and highlights the potential of LLM-based models to effectively process and analyze asynchronous time series data.
\end{itemize}

