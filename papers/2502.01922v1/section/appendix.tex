\section{Appendix}

\subsection{Dataset Preparation} 
\label{sec:dataset-preparation}

We remove any sequence in the dataset that is very small ($< 4$ elements). We split the dataset in a random $70/10/20$ train, validation and test split. Each sequence is expanded into multiple sequences based on the task:
\begin{itemize}
    \item \textbf{Forecasting}: We convert a sequence into multiple prediction tasks. For each element of the series, the prediction task is to predict the element given the preceding elements. We impose a minimum and maximum length requirements on the number of preceding elements used.
    \item \textbf{Imputation}: For every element in the series, we replace the element by a mask, and the imputation task is to predict the masked element given the remaining sequence.
    \item \textbf{Anomaly Detection}: For every element in the sequence, we replace the action by a random different action. the anomaly detection task is to identify the element of the sequence that has been tampered with.
\end{itemize}

For the three test based datasets - Breakfast, MultiTHUMOS and EPIC-KITCHENS, the event types are already represented as text. The remaining $5$ datasets from the temporal point processes domain lack a textual component, and the event types are represented by integers. For these datasets, we simply treat each integer event type as a string, allowing the LLM to process it similarly to text-based data.

\subsection{Dataset Class Imbalance}
\label{app:class_imbalance}
We observe significant class imbalance in our datasets, as shown in \autoref{fig:class_imbalance} for the Breakfast and MultiTHUMOS datasets. This imbalance motivates our choice of Macro-F1 as the primary metric, as it treats all classes equally, unlike Accuracy, which is heavily influenced by the dominant class.

\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/dataset_skew_plots/breakfast.pdf}
    \end{minipage}
    \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/dataset_skew_plots/multihumos.pdf}
    \end{minipage}
    
    % % if using 4, use this
    % \begin{minipage}[b]{0.45\textwidth}
    %     \includegraphics[width=\textwidth]{figures/dataset_skew_plots/epic_kitchen.pdf}
    % \end{minipge}
    % \begin{minipage}[b]{0.245\textwidth}
    %     \includegraphics[width=\textwidth]{figures/dataset_skew_plots/amazon.pdf}

    % \end{minipage}
    
    % \begin{minipage}[b]{0.245\textwidth}
    %     \includegraphics[width=\textwidth]{figures/dataset_skew_plots/retweet.pdf}

    % \end{minipage}
    % \begin{minipage}[b]{0.245\textwidth}
    %     \includegraphics[width=\textwidth]{figures/dataset_skew_plots/taxi.pdf}

    % \end{minipage}
    % \begin{minipage}[b]{0.245\textwidth}
    %     \includegraphics[width=\textwidth]{figures/dataset_skew_plots/taobao.pdf}

    % % if using 4, use this
    % \end{minipage}
    % \begin{minipage}[b]{0.45\textwidth}
    %     \includegraphics[width=\textwidth]{figures/dataset_skew_plots/stackoverflow.pdf}

    % \end{minipage}
    
\caption{Normalized event counts (y-axis) vs. event types sorted by count (x-axis) for two datasets - Breakfast and MultiTHUMOS, showing significant class imbalance.}

\label{fig:class_imbalance}
\end{figure}
\subsection{LASTS representation of Asynchronous time series for Zero Shot}
\label{sec:zero_shot_prompts}
Here we present the LASTS prompt structure for use with LLMs for various tasks. The structure of the LASTS prompts is shown in \autoref{fig:last_prompt}.

\paragraph{System Prompt} The system prompt is very similar across tasks, except for the task specific portions of the prompt.  The system prompt used for Forecasting is: 
\begin{tcolorbox}[colback=blue!10, colframe=black!80,]
You are a helpful assistant. Your task is to complete an asynchronous time series. \textit{\textcolor{red}{dataset\_description}}. Each series is given in the format (inter\_arrival\_time, action\_name). This indicates that the action\_name started inter\_arrival\_times milliseconds after the start of the previous action or the beginning of time if it's the first action. The allowable actions are: \textit{\textcolor{red}{valid\_vocab}}. Given the first few elements of an asynchronous time series, your task is to provide  the next action with its inter arrival time as (inter\_arrival\_time, action\_name). You generate all your response as a single python tuple. Be sure to provide only that one python tuple and nothing else.
\end{tcolorbox}

The system prompt used for Imputation is:
\begin{tcolorbox}[colback=blue!10, colframe=black!80,]
You are a helpful assistant. Your task is to find a missing value in an asynchronous time series. \textit{\textcolor{red}{dataset\_description}}. Each series is given in the format (inter\_arrival\_time, action\_name). This indicates that the action\_name started inter\_arrival\_times milliseconds after the start of the previous action or the beginning of time if it's the first action. The allowable actions are: \textit{\textcolor{red}{valid\_vocab}}. One of the elements in the series would be missing, marked by the word 'MISSING'. Provide your answer as a single python tuple (inter\_arrival\_time, action\_name) which is your estimate of the missing element of the series. Be sure to  give me that one missing  python tuple as your response and nothing else.
\end{tcolorbox}
The system prompt for Anomaly Detection is:
\begin{tcolorbox}[colback=blue!10, colframe=black!80,]
You are a helpful assistant. Your task is to find an anomolous value in an asynchronous time series. \textit{\textcolor{red}{dataset\_description}}. Each series is given in the format (inter\_arrival\_time, action\_name). This indicates that the action\_name started inter\_arrival\_times milliseconds after the start of the previous action or the beginning of time if it's the first action. The allowable actions are: \textit{\textcolor{red}{valid\_vocab}}. One of the elements in the series is an anomaly, and your task is to identify this element which doesn't belong in the series. Provide your answer as a single python tuple (inter\_arrival\_time, action\_name) which is an element from the series you think is an anomaly. Just give me that one anomolous python tuple from the series as your answer and nothing else. 
\end{tcolorbox}

Here, \textbf{dataset\_description} is a short one line description of the underlying dataset, for example:  \textit{"The underlying dataset is derived from tagged human actions while cooking/preparing meals"}.

Also, \textbf{valid\_vocab} is a comma separated list of allowable action descriptions, if we choose to provide this list and if this list is small.

\paragraph{User Prompt} The user prompt in all three tasks is a comma separated string of sequence events, for example 
\begin{equation*}
    \textit{(0,wait),(139000,carry\_bowl),(26000,hold\_bowl),}
\end{equation*}
In case of imputation, there would be a missing element marked by the word MISSING, like so:
\begin{equation*}
    \textit{(0,wait),(139000,carry\_bowl),\textcolor{red}{MISSING},(41000,reach\_eggcarton),}
\end{equation*}

\paragraph{Assistant Prompt} This is empty for zero-shot, as it is filled by the LLM as its prediction for the task on the given sequence.


\subsection{Evaluating LLM Interaction with LASTS Components}
\label{ast_rep_zero_shot}

We considered various variants of framing the LASTS prompt and present a few interesting ones here, evaluated on Breakfast dataset. 

\paragraph{Testing LLMs use of world knowledge} We want to test whether LLMs can understand a prompt like LASTS and provide a meaningful response to the task on the sequence using their world knowledge. To this end, we study a variant where each event description is replaced by a uniquely mapped gibberish 4-letter string. This unique mapping ensures that while any semantic meaning in the descriptions is removed, the structure of the time series remains intact.\autoref{tab:scrambled_names} shows that all tracked metrics degrade considerably in the scrambled names variant. This confirms that LLMs  not only understand LASTS properly but also leverage their world knowledge to perform the specified tasks. 


\begin{table}[ht]
\centering
\begin{tabular}{l r r r r r r}
\hline
                & \multicolumn{6}{c}{\textbf{Forecast}} \\ \hline
                & \textbf{M-F1} $\uparrow$ & \multicolumn{1}{c}{\% $\Delta$} & \textbf{Acc} $\uparrow$& \multicolumn{1}{c}{\% $\Delta$} & \textbf{MAE $\downarrow$} & \multicolumn{1}{c}{\% $\Delta$} \\ \hline
Zero Shot       & 0.0432 &       & 0.0866 &       & 37.8030 &       \\
Scrambled Names & 0.0140 & \textcolor{red}{$\downarrow$ -67.63\%} & 0.0397 & \textcolor{red}{$\downarrow$ -54.13\%} & 38.0742 & \textcolor{red}{$\uparrow$ 0.72\%} \\ \hline
                & \multicolumn{6}{c}{\textbf{Imputation}} \\ \hline
Zero Shot       & 0.0248 &       & 0.0338 &       & 33.7669 &       \\
Scrambled Names & 0.0100 & \textcolor{red}{$\downarrow$ -59.73\%} & 0.0224 & \textcolor{red}{$\downarrow$ -33.73\%} & 40.4918 & \textcolor{red}{$\uparrow$ 19.92\%} \\ \hline
                & \multicolumn{6}{c}{\textbf{Anomaly Detection}} \\ \hline
Zero Shot       & 0.0760 &       & 0.0650 &       & NA      &       \\
Scrambled Names & 0.0619 & \textcolor{red}{$\downarrow$ -18.55\%} & 0.0469 & \textcolor{red}{$\downarrow$ -27.88\%} & NA      &       \\ \hline
\end{tabular}
\caption{Comparing LASTS Zero Shot with the Scrambled Names variant across Forecast, Imputation, and Anomaly Detection tasks. Higher values are better for M-F1 and Acc, while lower values are better for MAE. Red indicates negative impact, while green indicates favorable impact.}
\label{tab:scrambled_names}
\end{table}
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}

\paragraph{Sequence Representation} We probe about the right representation for the time series events - should they be represented as $(e_i, t_i)$ or $(t_i, e_i)$. Our results in \autoref{tab:seq_rep} show that its better to have time first, followed by the event description. This is what we adopt in LASTS.
\begin{table}[ht]
\centering
\begin{tabular}{l r r r r r r}
\hline
                & \multicolumn{6}{c}{\textbf{Forecast}} \\ \hline
                & \textbf{M-F1} $\uparrow$ & \multicolumn{1}{c}{\% $\Delta$} & \textbf{Acc} $\uparrow$& \multicolumn{1}{c}{\% $\Delta$} & \textbf{MAE $\downarrow$} & \multicolumn{1}{c}{\% $\Delta$} \\ \hline
Time First  $(t_i, e_i)$     & 0.0432 &       & 0.0866 &       & 37.8030 &       \\
Event First $(e_i, t_i)$ & 0.0409 & \textcolor{red}{$\downarrow$ 5.38\%} & 0.0726 & \textcolor{red}{$\downarrow$ 16.07\%} & 37.5344 & \textcolor{darkgreen}{$\downarrow$ 0.71\%} \\ \hline
                & \multicolumn{6}{c}{\textbf{Imputation}} \\ \hline
Time First  $(t_i, e_i)$       & 0.0248 &       & 0.0338 &       & 33.7669 &       \\
Event First $(e_i, t_i)$ & 0.0071 & \textcolor{red}{$\downarrow$ -71.30\%} & 0.0150 & \textcolor{red}{$\downarrow$ -55.56\%} & 31.8194 & \textcolor{darkgreen}{$\downarrow$ -5.77\%} \\ \hline
                & \multicolumn{6}{c}{\textbf{Anomaly Detection}} \\ \hline
Time First  $(t_i, e_i)$       & 0.0760 &       & 0.0650 &       & NA      &       \\
Event First $(e_i, t_i)$ & 0.0858 & \textcolor{darkgreen}{$\uparrow$ 12.94\%} & 0.0619 & \textcolor{red}{$\downarrow$ -4.81\%} & NA      &       \\ \hline
\end{tabular}
\caption{Comparison of two ways to express events in an asynchronous time series - event first or time first across Forecast, Imputation, and Anomaly Detection tasks. Higher values are better for M-F1 and Acc, while lower values are better for MAE. Red indicates negative impact, while green indicates favorable impact.}
\label{tab:seq_rep}
\end{table}

\paragraph{Time Representation} We investigate if simplifying the series representation would improve LLM performance. For the Breakfast dataset, we replace inter-arrival times with durations, since we hypothesize that most actions occur contiguously for this dataset. We hypothesize that durations may be easier for the LLM to model rather than inter arrival. From the results in \autoref{tab:durations}, we observe that while we have a favourable impact on forecast, both imputation and anomaly detection suffer from this change. This suggests that while durations help with forecasting, more precise inter-arrival times are crucial for more involved tasks like imputation and anomaly detection. 
\begin{table}[ht]
\centering
\begin{tabular}{l r r r r r r}
\hline
                & \multicolumn{6}{c}{\textbf{Forecast}} \\ \hline
                & \textbf{M-F1} $\uparrow$ & \multicolumn{1}{c}{\% $\Delta$} & \textbf{Acc} $\uparrow$& \multicolumn{1}{c}{\% $\Delta$} & \textbf{MAE $\downarrow$} & \multicolumn{1}{c}{\% $\Delta$} \\ \hline
Zero Shot       & 0.0432 &       & 0.0866 &       & 37.8030 &       \\
Durations & 0.0600 & \textcolor{darkgreen}{$\uparrow$ 38.84\%} & 0.0953 & \textcolor{darkgreen}{$\uparrow$ 10.12\%} & 33.781 & \textcolor{darkgreen}{$\downarrow$ 10.62\%} \\ \hline
                & \multicolumn{6}{c}{\textbf{Imputation}} \\ \hline
Zero Shot       & 0.0248 &       & 0.0338 &       & 33.7669 &       \\
Durations & 0.0140 & \textcolor{red}{$\downarrow$ -43.56\%} & 0.0288 & \textcolor{red}{$\downarrow$ -14.81\%} & 29.6881 & \textcolor{darkgreen}{$\downarrow$ -12.09\%} \\ \hline
                & \multicolumn{6}{c}{\textbf{Anomaly Detection}} \\ \hline
Zero Shot       & 0.0760 &       & 0.0650 &       & NA      &       \\
Durations & 0.0767 & \textcolor{darkgreen}{$\uparrow$ 0.96\%} & 0.0532 & \textcolor{red}{$\downarrow$ -18.20\%} & NA      &       \\ \hline
\end{tabular}
\caption{Comparison of LASTS Zero Shot with the variant using durations instead of inter-arrival times across Forecast, Imputation, and Anomaly Detection tasks. Higher values are better for M-F1 and Acc, while lower values are better for MAE. Red indicates negative impact, while green indicates favorable impact.}
\label{tab:durations}
\end{table}



\subsection{LASTS representation used for LLM Adaptation}
\label{sec:lasts_for_peft}
For our experients on LLM adaptation, we keep the LASTS representation very similar to our zero shot experiments: 
\begin{itemize}
    \item \textbf{System prompt} in this case is a very concise description of just the task. We skip any dataset description as we expect the model to learn that during the fine tuning process.
    \item \textbf{User prompt} is represented as a comma separated sequence of tuples of event description and inter arrival times.
    \item \textbf{Assistant prompt}  contains the expected prediction.
\end{itemize}

The exact system prompt used for each of the tasks are as follows:
\begin{itemize}
    \item \textbf{Forecasting}: \textit{"Predict the next element of this asynchronous time series where each element is of the form (inter\_arrival\_time,\ action\_name)."}
    \item \textbf{Imputation}: \textit{"Predict the  element marked 'MISSING' in this asynchronous time series where each element is of the form (inter\_arrival\_time, action\_name)."}
    \item \textbf{Anomaly Detection}: \textit{"One of the element in this asynchronous time series is anomalous, find this element. Each element of the series is of the form (inter\_arrival\_time, action\_name)."}
\end{itemize}

\subsection{Baselines}
\label{sec:llms_for_ts_baselines}

\paragraph{Random Baseline}  To evaluate our methods on the three text-based datasets and the three tasks, we establish a random baseline simulating random guesses. For forecasting and imputation, given an input asynchronous time series, the baseline predicts the inter-arrival time as the average of all inter-arrival times in the sequence and selects a random event type from the valid event descriptions. For anomaly detection, it randomly labels an event from the series as anomalous (see \autoref{tab:performance_comparison}).

\paragraph{Foundation Models for Time Series Baseline}
We adapted Chronos \citep{ansari2024chronos}, a state-of-the-art foundation model designed for zero-shot forecasting on time series data, as a baseline for forecasting and imputation tasks on asynchronous time series datasets. We use the largest model version (\textit{amazon/chronos-t5-large}) available which contains $710M$ model parameters. Since Chronos exclusively handles numerical data, we converted our event descriptions into categorical representations. Each asynchronous time series of length $n$ was transformed into a sequence of $2n$ integers, alternating between inter-arrival times and event categories.

For forecasting, the task was framed as predicting the next two elements in this sequence given the historical context. Adapting Chronos for imputation, however, required additional considerations since it is inherently designed for forecasting. We reformulated the imputation task as a forecasting problem: if the prefix leading up to the missing element is longer than the suffix following it, we treated imputation as forecasting the missing element using the prefix as context. Conversely, if the suffix is longer, we reversed the suffix and used it as context to forecast the missing element. This approach ensures the longest possible context is utilized for predicting the missing value.

It is worth noting that adapting Chronos for anomaly detection is not straightforward, as anomaly detection involves identifying a single anomalous event within the series, which does not align with Chronos' forecasting capabilities. Consequently, Chronos is provided as a baseline exclusively for forecasting and imputation tasks. 

\paragraph{LLMs for Time Series Baselines}
We adapted two LLM-based methods for time series: \textbf{LLMTime} \citep{gruver2024large} and \textbf{LLMProcesses} \citep{requeima2024llm}, as baselines. Since both methods are designed for numerical time series, we converted textual event descriptions into categorical representations.

\subparagraph{LLMTime}
In this method, each data point is represented as a pair: (inter-arrival-time, event-categorical). We modified the default next-token prediction behavior of the model using simple task-specific prompts:
\begin{itemize} \item \textbf{Forecasting}: \textit{Predict the next time and event.} \item \textbf{Imputation}: \textit{Find the element marked as 'MISSING.'} \item \textbf{Anomaly Detection}: \textit{Find the anomalous time and event.} \end{itemize}

\subparagraph{LLMProcesses}
This method uses in-context learning with $(x, y)$ examples derived from a sequence, treating the sequence as a real-valued function on a 2D space as domain. In this setup, $x$ represents a point in 2D space $(x_1, x_2)$, where $x_1$ denotes the sequence position, and $x_2$ indicates the output type: $0$ for inter-arrival time and $1$ for event categorical. For a given sequence, we crafted two distinct prompts: one for predicting the event categorical and another for predicting the inter-arrival time, based on the corresponding value of $x$. We followed the recommended settings from the original paper for prompt construction.

 However, anomaly detection does not align with this framework, as it involves identifying a single anomalous time point where the function output is $0$ everywhere except at the anomaly. This makes it unsuitable for predicting function values at unseen points based on prior observations. Consequently, we adapted this approach exclusively for forecasting and imputation tasks.

% We use two LLM-based time series forecasting methods, LLMTime \citep{Gruver2023Large} and LLMProcesses \citep{requeima2024llm}, as baselines for asynchronous time series. Since LLMProcesses cannot be easily adapted for anomaly detection, it is used only for forecasting and imputation. Our method performs comparably or better than these baselines. Details on baseline implementation are in Appendix 
\paragraph{TPP Models as Baselines}
We compare our best fine-tuned model configuration, $LASTS+StoP$, against current state-of-the-art methods for forecasting on asynchronous time series. These methods are adapted from the benchmark study in \citep{Xue2023EasyTPP}. The evaluation spans eight datasets, five of which—Amazon, Retweet, Taxi, Taobao, and StackOverflow  contain event categoricals without textual descriptions and are regarded as standard benchmarks for asynchronous time series analysis.

We benchmark the TPP models covered in the EasyTPP benchmark \citep{Xue2023EasyTPP} on the three textual datasets considered in our work: Breakfast, MultiTHUMOS, and EPIC KITCHEN. Since these datasets represent events as text and TPP models are not equipped to handle text directly, we converted the event names into event categoricals to make them compatible with these models.

\paragraph{Observations} We summarize our comparison of various baselines with LASTS Zero Shot in \autoref{fig:baseline_performance_plot}. We observe that Chronos performs the weakest among the baselines, yet it remains competitive. This is expected as Chronos, while being a much smaller model compared to LLMs, is highly specialized for time series forecasting, which enables it to achieve decent performance. LLMTime and LLMProcesses also perform competitively, especially on the MultiTHUMOS dataset. We attribute this to the noisy nature of the MultiTHUMOS dataset, which includes non-standard event names (e.g., "OneHandedCatch," "TalkToCamera", etc) and repetitive, less meaningful patterns (e.g., "GolfSwing, Wait, GolfSwing, Wait..."). These characteristics may help event-categorical-based models like LLMTime and LLMProcesses. However, on the other two datasets—Breakfast and EPIC\_KITCHEN—the textual descriptions of events provide a significant advantage, as evident from the comfortable margin by which LASTS Zero Shot outperforms LLMTime and LLMProcesses across all tasks.

Furthermore, we observed that existing TPP-based models struggled with datasets containing a large number of unique event types, often performing poorly, failing to converge, or encountering out-of-memory errors. This highlights the challenges these models face in handling the diversity and complexity of such datasets.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/Baseline_Performance_Histograms_Bold_Spaced_Lasts_Zero_Shot.pdf}
    \caption{
        Comparison of performance metrics: Macro-F1 (M-F1), Mean Absolute Error (MAE), and Accuracy (ACC), averaged across all datasets for Forecast and Imputation tasks. Higher values for M-F1 and ACC indicate better performance, while a lower value of MAE is preferred. It is evident that LASTS Zero Shot (our method) achieves the highest average M-F1 and average ACC among all the baselines and also produces the lowest MAE.
    }
    \label{fig:baseline_performance_plot}
\end{figure}

\subsection{Disentangling Stochasticity and Prefix Picking in StoP}
\label{app:stop_disentangling}

To further analyze the impact of prefix picking in StoP, we compare it with an alternative training paradigm where, instead of selecting a structured prefix, we randomly select $l$ tokens from the prompt during each batch, with $l$ drawn from a uniform distribution. This comparison isolates the effects of introducing stochasticity alone versus the structured prefix picking employed by StoP. \autoref{fig:random_tokens} presents a comparison of Macro-F1 and MAE metrics on the validation set as both prompts are trained for 10 epochs.  These plots show that stochasticity alone is not sufficient for learning good soft prompts, and structured prefix picking is a key component of the StoP training. 

\begin{figure}[]
    \centering
    \begin{minipage}[t]{0.49\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/comparison_plots/f1_comparison.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.49\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/comparison_plots/mae_comparison.pdf}
    \end{minipage}
    
    \caption{Comparison of Macro-F1 and MAE for StoP vs. random token selection during training, evaluated on validation data after 10 epochs. Results show that random token selection (shown in red) fails to learn effective prompts, while StoP's structured prefix selection (shown in blue) achieves significantly better performance.}
    \label{fig:random_tokens}
\end{figure}


\subsection{StoP Prompt Interpretations Through Model Probing} 
\label{sec:prompt_interpretations_word_embeddings}
Prior work such as \cite{Lester2021Power} attempts to interpret learned prompts by mapping them to the closest input embeddings. However, this approach often produces sequences that are difficult to understand or lack meaningful content. For example, \cite{xusoft} observed that the closest words to the learned embeddings are mostly meaningless, several tokens are mapped to the same word, and the cosine similarity between the tokens and their closest word embeddings almost always falls below $0.16$. Instead, we explore probing the LLM using the learned prompt. By appending the learned prompt with a simple instruction, such as \textit{"Tell me in as much detail as possible what task you are supposed to do,"} we encourage the LLM to generate an output that reflects its understanding of the task. This approach allows us to gain meaningful insights into what the model has summarized from the tasks and datasets it has been trained on. Here we present a few interesting interpretations of StoP prompts obtained by model probing.
% \paragraph{StoP of Length $400$, Breakfast Dataset, Forecast } This interpretation contains a general summary of cooking events

% \begin{tcolorbox}[colback=blue!10, colframe=black!80]
% The task appears to be related to preparing a snack or meal by using the facilities provided in the kitchen. Specifically, I was asked to perform a series of actions to accomplish this goal. Here's what I've observed:\n\n1. **Opening the cabinet**: I need to open the cabinet to access the necessary utensils, cookware, or ingredients.\n2. **Grabbing the ingredients**: I will need to pick up the specified ingredients, such as a bowl, a cutting board, or a can opener, to prepare the desired dish.\n3. **Slicing the ingredients**: I will cut the ingredients into the required proportions, shapes, or sizes depending on the recipe being followed.\n4. **Preheating the cooking surface**: If needed, I will turn on the stove, oven, or other cooking device to the desired temperature to start cooking the ingredients.\n5. **Cooking the ingredients**: I will proceed to cook the ingredients according to the required cooking method, such as boiling, sautéing, frying, or baking.\n6. **Adding condiments or seasonings**: If necessary, I will add the required condiments, spices, or seasonings to the cooked ingredients to enhance their flavor.\n7. **Combining the ingredients**: Finally, I will combine the cooked ingredients in the desired manner, whether it's mixing, blending, or assembling the final dish.\n\n Please let me know if I'm correct in my understanding or if there's anything else I can do to help!
% \end{tcolorbox}

\paragraph{StoP of Length $400$, Breakfast Dataset, Forecast } We obtain a very nice summarization of the breakfast dataset: 

\begin{tcolorbox}[colback=blue!10, colframe=black!80]
The task appears to be related to preparing a snack or meal by using the facilities provided in
the kitchen. Specifically, I was asked to perform a series of actions to accomplish this goal.
Here’s what I’ve observed:1. **Opening the cabinet**: I need to open the cabinet to access
the necessary utensils, cookware, or ingredients.2. **Grabbing the ingredients**: I will
need to pick up the specified ingredients, such as a bowl, a cutting board, or a can opener, to
prepare the desired dish.3. **Slicing the ingredients**: I will cut the ingredients into the re-
quired proportions, shapes, or sizes depending on the recipe being followed.4. **Preheating
the cooking surface**: If needed, I will turn on the stove, oven, or other cooking device to
the desired temperature to start cooking the ingredients.5. **Cooking the ingredients**: I
will proceed to cook the ingredients according to the required cooking method, such as boil-
ing, saut´eing, frying, or baking.6. **Adding condiments or seasonings**: If necessary, I
will add the required condiments, spices, or seasonings to the cooked ingredients to enhance
their flavor.7. **Combining the ingredients**: Finally, I will combine the cooked ingredi-
ents in the desired manner, whether it’s mixing, blending, or assembling the final dish.Please
let me know if I’m correct in my understanding or if there’s anything else I can do to help!
\end{tcolorbox}

\paragraph{StoP of Length $5$, Breakfast Dataset, Forecast } We obtain very cooking adjacent prompt interpretations from even a small prompt of length $5$ trained for forecasting on Breakfast. For example:

\begin{tcolorbox}[colback=blue!10, colframe=black!80]
Here is my main task: "Reach for a tablespoon and move in the direction of a plate where I already placed some eggs and a knife. Slowly pour one egg into the plate, take the knife, and carefully cut the egg in halves. Transfer both halves away from the plate, and then carry the remaining plate towards a sink. Turn the tap on, wash the plate, and then move away from the sink, holding the plate still under running water. Remove the plate from the sink, carry it to a countertop, and put it down. Finally, reach for a knife and move towards a bowl containing a knife, carry the knife, and move towards a counter where I already placed a cutting board, where there is a set of utensils lying on it. Take the knife, push towards the board, and reach for a knife. Move away from the cutting board.
\end{tcolorbox}

\paragraph{StoP of length $400$, MultiTHUMOS Dataset, Imputation} The interpretation seems to have mapped the idea of an asynchronous time series to another domain all together, while still keeping elements like walk, run, stand, etc from the original dataset:
\begin{tcolorbox}[colback=blue!10, colframe=black!80]
I'm an AI designed to generate text based on the sequence of events occurring in the events diagram. Specifically, I\'m trained to analyze the sequence of events and identify the corresponding actions or actions sequences in the diagram, and then use that information to generate a textual description of what is happening in the diagram.The main tasks I was designed to perform are:1. **Identify sequence of events**: I analyze the sequence of events in the diagram, identifying the time-stamped data points and the transitions between them.2. **Match events with actions**: I match the identified sequence of events with the corresponding actions or actions sequences. For example, if I see a sequence of time-stamped data points indicating a person sitting, standing, and then walking, I would match those events to the action of "walking".3. **Generate textual description**: Using the matched events and actions, I then generate a textual description of what is happening in the diagram. For example, if I match the sequence "sitting, standing, walking" to the action "walking", I would generate a text describing that sequence as "walking". Additionally, I also make use of some contextual information, such as the diagram\'s caption, to disambiguate and refine my generated textual descriptions. The final textual output is a natural language description of the content of the diagram.
\end{tcolorbox}

\paragraph{StoP of length $400$, MultiTHUMOS Dataset, Anomaly Detection} The interpretations center around games, which is expected as MultiTHUMOS is a gaming dataset. Here are two sampled interpretations:
\begin{tcolorbox}[colback=blue!10, colframe=black!80]
The task is to simulate a volleyball game. The events include:1. Slices or spikes the ball with the palm of the hand (VolleyballSpiking). 2. Blocks the ball with the palm of the hand (VolleyballBlock).3. Sets the ball with the palm of the hand (VolleyballSet). 4. Pounds or slams the ball with the fist (VolleyballSpiking). 5. Jumps or reaches for the ball (VolleyballJumping). 6. Moves sideways or slides on the ground (VolleyballSpikeRun).It's important to remember that volleyball spiking, blocking, setting, and jumping involves movement and body language, which can't be reflected directly in a sequential diagram
\end{tcolorbox}

\begin{tcolorbox}[colback=blue!10, colframe=black!80]
The task you are supposed to do is sports broadcasting, and specifically, you are a golf broadcaster. As a golfer, you are supposed to perform a live broadcast of the golf match, providing commentary and analysis as the golfers participate in the match. The situation is simulated, as there are no external stimuli or real golfers, you have to act as both commentator and golfer, making mock commentary and mimicking golf swings.The task requires you to stay in a fixed position, usually behind a microphone, and maintain a semi-serious, professional tone. Your task is to articulate the events on the course, such as drives, putts, and other golf shots, as if you are actually seeing them happen. You should also describe the terrain, distance, and obstacles faced by the golfer, and offer your expert analysis and opinions on their technique, strategy, and chances of scoring.Your commentary should be smooth, clear, and engaging, drawing the audience into the game and painting a vivid picture of the action on the course. Your performance should exude confidence and authority, as if you are an experienced commentator who has watched many matches before. The audience should feel like they are right there on the course with you, experiencing the excitement and suspense of the tournament.
\end{tcolorbox}



% \textbf{StoP of Length 400, Breakfast Dataset, Forecast} 

% \textit{Prompt:} Write a 5 line poem about your task. \\
% \textit{Model Response}: \\
% \begin{tcolorbox}[colback=blue!10, colframe=black!80, verbatim]
% Whisk, bowl, and cake, a trio so fine \\
% Cream and sugar, a dancer's rhyme \\ 
% Lather and smooth, a frosting divine \\ 
% Sweet delight, a treat so sublime \\ 
% Tartan, the crown, a masterpiece of mine. 
% \end{tcolorbox}

% \begin{tcolorbox}[colback=blue!10, colframe=black!80, verbatim]
% Topping pizza, a careful art \\
% Fresh herbs sprinkle, a savory heart \\
% Fragrance fills, the evening air \\
% Tempting taste buds, beyond compare \\ 
% A culinary delight, beyond despair 
% \end{tcolorbox}

% \begin{tcolorbox}[colback=blue!10, colframe=black!80, verbatim]
% Within the realm of fruitful zest \\
% Wielding knife and gentle quest \\ 
% With each delicate cut, I unfold \\ 
% A tale of taste and weetness to hold \\ 
% And savor the beauty within 
% \end{tcolorbox}

\textbf{StoP of Length 400, Taobao Dataset, Forecast} Note that this dataset is not textual, so all the soft prompt has seen is the task description and a sequence of pairs of numbers.

\textit{Prompt:} Describe your task. \\
\textit{Model Response}: \\

\begin{tcolorbox}[colback=blue!10, colframe=black!80]
I\'m an AI assistant, my task is to generate a sequence based on a rule that I\'ve learned 
from given inputs (or "train" sequences). The rule is inferred by analyzing the pattern of increasing or decreasing values of the input sequence. Each sequence is either called ("silenced") or activated, and if there\'s no change in the sequence, the "silence" is used, unless the input sequence calls the "silence" by itself. The task involves predicting the next output in a sequence, given some input sequences that establish a pattern.
\end{tcolorbox}   

\subsection{Comparison of LASTS + StoP with other PEFT techniques}
\label{app:stop_comparison_with_peft}
In this section, we compare the performance of LASTS + StoP with other PEFT techniques listed in Table \ref{tab:performance_comparison}. Table \ref{tab:compare_stop_and_sp} highlights the percentage improvements observed in various metrics when using Stochastic Soft Prompting compared to standard Soft Prompting. We observe a significant advantage of Stochastic Soft Prompting across all datasets and tasks, with an overall average increase of $12.69\%$ in Macro-F1 across all tasks and datasets. Similarly, Table \ref{tab:compare_stop_and_qlora} demonstrates an average increase of $13.55\%$ in Macro-F1 when using Stochastic Soft Prompting instead of finetuning techniques like QLORA.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\multirow{2}{*}{\textbf{Task}} & \multicolumn{3}{c}{\textbf{Breakfast}} & \multicolumn{3}{c}{\textbf{MultiTHUMOS}} & \multicolumn{3}{c}{\textbf{EPIC\_KITCHEN}} \\ 
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
 & \textbf{M-F1} & \textbf{MAE} & \textbf{ACC} & \textbf{M-F1} & \textbf{MAE} & \textbf{ACC} & \textbf{M-F1} & \textbf{MAE} & \textbf{ACC} \\ 
\midrule
Forecast       & 11.09\% & 0.91\% & 4.87\% & 6.08\% & 0.77\% & 0.04\% & 2.13\% & -4.91\% & 3.52\% \\
Imputation     & 23.40\% & 2.22\% & 17.37\% & 7.64\% & 2.76\% & 10.95\% & 30.66\% & 1.09\% & 10.81\% \\
Anomaly Detection & 10.40\% & \textemdash     & 12.82\% & 15.56\% & \textemdash     & 10.97\% & 7.21\% & \textemdash     & 6.06\% \\
\midrule
\textbf{Avg Gain (Per Task)} & 14.96\% & 1.56\% & 11.69\% & 9.76\% & 1.76\% & 7.32\% & 13.33\% & -1.91\% & 6.80\% \\
\midrule
\textbf{Avg Gain (All Tasks, All Datasets)} & \multicolumn{3}{c}{\textbf{M-F1}: 12.69\%} & \multicolumn{3}{c}{ \textbf{MAE}: 0.47\%} & \multicolumn{3}{c}{ \textbf{ACC}: 8.60\%} \\
\bottomrule
\end{tabular}%
}
\caption{Comparison of LASTS+StoP with LASTS+SP. The table shows the percentage improvement in each metric achieved by using Stochastic Soft Prompting compared to standard Soft Prompting. Significant gains are observed across all datasets and tasks with Stochastic Soft Prompts. On average, across all datasets and tasks, Macro F1 increases by $12.69\%$.}  
\label{tab:compare_stop_and_sp}
\end{table}

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\multirow{2}{*}{\textbf{Task}} & \multicolumn{3}{c}{\textbf{Breakfast}} & \multicolumn{3}{c}{\textbf{MultiTHUMOS}} & \multicolumn{3}{c}{\textbf{EPIC\_KITCHEN}} \\ 
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
 & \textbf{M-F1} & \textbf{MAE} & \textbf{ACC} & \textbf{M-F1} & \textbf{MAE} & \textbf{ACC} & \textbf{M-F1} & \textbf{MAE} & \textbf{ACC} \\ 
\midrule
Forecast       & 2.93\% & 4.39\% & 3.11\% & 22.65\% & 4.71\% & 10.31\% & 4.32\% & -4.47\% & 6.39\% \\
Imputation     & 22.27\% & 1.20\% & 9.60\% & 3.80\% & -5.40\% & 3.46\% & 61.38\% & 0.25\% & 25.24\% \\
Anomaly Detection & 2.67\% & \textemdash     & 3.40\% & 0.70\% & \textemdash     & 1.65\% & 1.27\% & \textemdash     & 0.70\% \\
\midrule
\textbf{Avg Gain (Per Task)} & 9.29\% & 2.79\% & 5.37\% & 9.05\% & -0.34\% & 5.14\% & 22.32\% & -2.11\% & 10.78\% \\
\midrule
\textbf{Avg Gain (All Tasks, All Datasets)} & \multicolumn{3}{c}{\textbf{M-F1}: 13.55\%} & \multicolumn{3}{c}{\textbf{MAE}: 0.11\%} & \multicolumn{3}{c}{\textbf{ACC}: 7.10\%} \\
\bottomrule
\end{tabular}%
}
\caption{Comparison of LASTS+StoP with LASTS+QLORA. The table shows the percentage improvement in each metric achieved by using Stochastic Soft Prompting compared to finetuning via QLORA. Significant gains are observed across all datasets and tasks with Stochastic Soft Prompts. On average, across all datasets and tasks, Macro-F1 increases by $13.55\%$.}
\label{tab:compare_stop_and_qlora}
\end{table}



% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|cc|cc|cc|}\toprule
 & 
  \multicolumn{1}{l|}{} &
  \multicolumn{2}{c|}{Breakfast} &
  \multicolumn{2}{c|}{MultiThumos} &
  \multicolumn{2}{c|}{EPIC KITCHEN} \\ \midrule
 &
  \# Params &
  \multicolumn{1}{c|}{Macro F1 $\uparrow$} &
  MAE $\downarrow$ &
  \multicolumn{1}{c|}{Macro F1 $\uparrow$} &
  MAE $\downarrow$ &
  \multicolumn{1}{c|}{Macro F1 $\uparrow$} &
  MAE $\downarrow$ \\ \midrule

\multicolumn{1}{|c|}{Forecast} &
  1B &
  \multicolumn{1}{c|}{0.2292} &
  33.9309 &
  \multicolumn{1}{c|}{0.3210} &
  1.8013 &
  \multicolumn{1}{c|}{0.0574} &
  3.0859 \\
 &
  3B &
  \multicolumn{1}{c|}{0.2526} &
  33.2541 &
  \multicolumn{1}{c|}{0.3694} &
  1.7259 &
  \multicolumn{1}{c|}{0.0708} &
  3.0169 \\ 
 &
  8B &
  \multicolumn{1}{c|}{0.2633} &
  32.5464 &
  \multicolumn{1}{c|}{0.3947} &
  1.6503 &
  \multicolumn{1}{c|}{0.0797} &
  3.0318 \\ \midrule

\multicolumn{1}{|c|}{Imputation} &
  1B &
  \multicolumn{1}{c|}{0.0256} &
  31.1075 &
  \multicolumn{1}{c|}{0.0907} &
  2.4256 &
  \multicolumn{1}{c|}{0.0102} &
  3.2571 \\
 &
  3B &
  \multicolumn{1}{c|}{0.0966} &
  31.1597 &
  \multicolumn{1}{c|}{0.1329} &
  2.3963 &
  \multicolumn{1}{c|}{0.0280} &
  3.1445 \\
 &
  8B &
  \multicolumn{1}{c|}{0.2064} &
  28.2251 &
  \multicolumn{1}{c|}{0.2213} &
  2.3445 &
  \multicolumn{1}{c|}{0.0610} &
  3.1116 \\ \midrule
 
\multicolumn{1}{|c|}{Anomaly Detection} &
  1B &
  \multicolumn{1}{c|}{0.0688} &
  \textemdash &
  \multicolumn{1}{c|}{0.0954} &
  \textemdash &
  \multicolumn{1}{c|}{0.0318} &
  \textemdash \\
 &
  3B &
  \multicolumn{1}{c|}{0.5726} &
  \textemdash &
  \multicolumn{1}{c|}{0.4777} &
  \textemdash &
  \multicolumn{1}{c|}{0.5793} &
  \textemdash \\
 &
  8B &
  \multicolumn{1}{c|}{0.7198} &
  \textemdash &
  \multicolumn{1}{c|}{0.6045} &
  \textemdash &
  \multicolumn{1}{c|}{0.6603} &
  \textemdash \\ \bottomrule
\end{tabular}%
}
\caption{Comparison of Macro-F1 and MAE across the Breakfast, MultiThumos, and EPIC\_KITCHENS datasets for forecasting, imputation, and anomaly detection as the number of model parameters varies. The results show that Macro-F1 consistently improves with increasing model size across all datasets and tasks. In most cases, MAE decreases as model size increases, confirming that larger models generally lead to better performance.}
\label{tab:scaling_laws}
\end{table}

\subsection{Scaling to different LLM backbone sizes}
\label{app:scaling_laws}
We trained Stochastic Soft Prompts (StoP) across different backbone sizes of large language models and observed consistent improvements in performance as the model size increased. Specifically, we conducted experiments using LLama3.2 models with 1B and 3B parameters, as well as the LLama3-8B Instruct model. These improvements were clear across the Breakfast, MultiThumos, and EPIC\_KITCHENS datasets and applied to all tasks - forecasting, imputation, and anomaly detection.

Notably, \autoref{tab:scaling_laws} and \autoref{fig:avg_macro_f1_mae_by_model_size} show that macro-F1 scores consistently improve with larger model sizes across all datasets and tasks. Additionally, Mean Absolute Error (MAE) decreased in most cases as the model size increased, further confirming that larger models help Stochastic Soft Prompts perform better by utilizing their enhanced representational power. The performance difference between model sizes is smaller for forecasting tasks since these align with the next-token prediction that LLMs are trained on. However, for harder tasks like imputation and anomaly detection, the improvements are much larger as model size increases.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Average_MacroF1_MAE_Histograms_Model_sizes.pdf}
    \caption{
        Comparison of average Macro F1 and MAE across all datasets and tasks for different model sizes. 
        The left histogram shows the average Macro F1 scores, while the right histogram depicts the average MAE values. 
        We see a clear trend of improvement in both metrics as model sizes increase.
    }
    \label{fig:avg_macro_f1_mae_by_model_size}
\end{figure}

\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccc|ccc|ccc}
        \toprule
        \textbf{Few-Shot (k)} & \multicolumn{3}{c|}{\textbf{Breakfast}} & \multicolumn{3}{c|}{\textbf{MultiTHUMOS}} & \multicolumn{3}{c}{\textbf{EPIC-KITCHENS}} \\
        \midrule
        & M-F1 $\uparrow$ & MAE $\downarrow$  & ACC $\uparrow$  & M-F1 $\uparrow$  & MAE 
        $\downarrow$ & ACC $\uparrow$  & M-F1 $\uparrow$ & MAE $\downarrow$ & ACC $\uparrow$ \\
        \midrule
        & \multicolumn{9}{c}{Forecast}  \\
        \midrule
        \( k=0 \) & 0.0604 & 38.1630 & 0.0969 & 0.1361 & 1.8868 & 0.1826 & 0.0105 & 3.1566 & 0.0920 \\
        \( k=1 \) & 0.1312 & 37.6239 & 0.1808 & 0.1393 & 1.7913 & 0.2381 & 0.0144 & 3.2606 & 0.1123 \\
        \( k=2 \) & 0.1257 & 36.4688 & 0.1870 & 0.1622 & 1.7960 & 0.2505 &\textbf{ 0.0151} & 3.2266 & \textbf{0.1180 }\\
        \( k=5 \) & 0.1518 & \textbf{35.5605} & 0.2133 & 0.1676 & 1.8114 & 0.2581 & 0.0149 & 3.3092 & 0.1150 \\
        \( k=7 \) & 0.1491 & 35.6785 & 0.2107 & \textbf{0.1991} & \textbf{1.7810 }& \textbf{0.2828} & 0.0138 & 3.2177 & 0.1002 \\
        \( k=10 \) & \textbf{0.1667} & 37.6084 & \textbf{0.2442} & 0.1807 & 1.7820 & 0.2397 & 0.0124 & \textbf{3.0904} & 0.0901 \\
        \midrule
        & \multicolumn{9}{c}{Imputation}  \\
        \midrule
        \( k=0 \) & 0.0263 & 33.0097 & 0.0594 & 0.0915 & 2.6696 & 0.1210 & 0.0015 & 3.6527 & 0.0446 \\
        \( k=1 \) & 0.0419 & 33.1403 & 0.0738 & 0.1165 & 2.5106 & 0.1490 & 0.0018 & 3.6402 & \textbf{0.0569} \\
        \( k=2 \) & \textbf{0.0527} & \textbf{31.1138} & 0.0826 & 0.1102 & \textbf{2.3576} & 0.1486 & 0.0022 & 3.5375 & 0.0527 \\
        \( k=5 \) & 0.0520 & 33.3440 & 0.1001 & 0.1013 & 2.3982 & \textbf{0.1569} & \textbf{0.0023} & \textbf{3.2528} & 0.0547 \\
        \( k=7 \) & 0.0509 & 34.0198 & 0.0994 & 0.1001 & 2.4228 & 0.1462 & 0.0019 & 3.3447 & 0.0475 \\
        \( k=10 \) & 0.0474 & 31.2001 & \textbf{0.1069} & \textbf{0.1219} & 2.3771 & 0.1546 & 0.0015 & 3.2552 & 0.0406 \\
        \midrule
        & \multicolumn{9}{c}{Anomaly Detection}  \\
        \midrule
        \( k=0 \) & 0.0923 & \textemdash & \textbf{0.0763} & 0.2755 & \textemdash & 0.1949 & 0.0159 & \textemdash & 0.0777 \\
        \( k=1 \) & 0.1002 & \textemdash & 0.0681 & 0.2809 & \textemdash & 0.1961 & 0.0172 & \textemdash & 0.0854 \\
        \( k=2 \) & 0.0739 & \textemdash & 0.0569 & 0.3361 & \textemdash & \textbf{0.2891} & 0.0213 & \textemdash & 0.1062 \\
        \( k=5 \) & 0.0837 & \textemdash & 0.0563 &\textbf{ 0.3535} & \textemdash & 0.2720 & \textbf{0.0337} & \textemdash & \textbf{0.1637} \\
        \( k=7 \) & 0.0705 & \textemdash & 0.0469 & 0.3436 & \textemdash & 0.2516 & 0.0278 & \textemdash & 0.1369 \\
        \( k=10 \) & \textbf{0.1026} & \textemdash & 0.0700 & 0.2340 & \textemdash & 0.1629 & 0.0222 & \textemdash & 0.1097 \\
        \bottomrule
    \end{tabular}}
    \caption{Comparison of performance metrics (M-F1, MAE, and ACC) across Breakfast, MultiTHUMOS and EPIC\_KITCHEN datasets over forecast, imputation and anomaly detection tasks for different few-shot values \( k \) given as in context examples. $k = 0$ indicates Zero Shot. Higher M-F1 and ACC values indicate better performance, while lower MAE values are better. MAE computation is not applicable for anomaly detection. Best metric values are indicated in \textbf{bold}.}
    \label{tab:few_shot_comparison_adjusted}
\end{table}


\subsection{LASTS Few Shot}
\label{sec:few-shot-analysis}

We study the impact of varying the number of examples (\(k\)) in the few-shot setting to determine the optimal value of \(k\) for our method. Specifically, we evaluate the performance of LASTS Few Shot on all datasets and tasks using different \(k\) values, ranging from \(k=0\) (Zero Shot) to \(k=10\). As shown in \autoref{fig:performance_vs_k} and detailed in \autoref{tab:few_shot_comparison_adjusted}, the performance metrics—Macro-F1, MAE, and ACC—improve significantly as \(k\) increases from 0 to 5. However, further increases in \(k\) beyond 5 do not consistently yield improvements and, in some cases, result in marginal performance degradation.

On average, \(k=5\) achieves the best balance across all metrics and datasets. Therefore, we adopt \(k=5\) as the default value for LASTS Few Shot and include it as the entry for "LASTS Few Shot" in \autoref{tab:performance_comparison}. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/Average_Performance_LinePlots_FinerOffset_MacroF1.pdf}
    \caption{Average values of Macro-F1, MAE, and ACC across all datasets and tasks for different values of \(k\) (number of few-shot examples). Higher values indicate better performance for Macro-F1 and ACC, while lower values indicate better performance for MAE. The results indicate that on an average, $k=5$ works best.}
    \label{fig:performance_vs_k}
\end{figure}




\subsection{Further analysis on Stochastic Soft Prompts (StoP)}
\label{sec:stop_analysis}

In this section, we comment on the structure learned by StoP prompts and discuss the practical benefits of Stochastic Soft Prompts.

\paragraph{Evidence for Coarse-to-Fine Structure}
The prompts learned through Stochastic Soft Prompts (StoP) suggest the presence of a structured coarse-to-fine hierarchy. In this structure, the first few tokens appear to encode broader task-level information, while later tokens may refine predictions by adding more detailed nuances. Below, we provide observations that support this behavior:


\begin{enumerate}
    \item \textbf{t-SNE Projections:} Visualizations of t-SNE projections (see \autoref{fig:coarse-to-fine}) suggest that the first few tokens in StoP prompts may encode more diverse or independent representations, as indicated by their wider spread in the projection space. In contrast, the later tokens tend to cluster more closely together, potentially reflecting the refinement of previously encoded information.

    \item \textbf{Cosine Similarity:} Adjacent tokens at the beginning of the StoP prompt tend to exhibit lower cosine similarity compared to tokens later in the prompt (see \autoref{fig:coarse-to-fine}). This pattern suggests more diverse information being captured at the beginning of the prompt. Standard soft prompts, however, show uniformly high cosine similarities across all tokens, lacking this structure. 

    \item \textbf{Prefix Validity:} \autoref{fig:vallid_porefixes} indicate that any prefix of a StoP prompt serves as a valid standalone prompt, with additional tokens refining the predictions. This behavior suggests that early tokens convey broad task-level information, while later tokens refine and add finer-grained details.
\end{enumerate}


% \begin{figure}[t]
%     \centering
%     % First figure with cropping
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[clip, trim=0cm 0cm 0cm 2.2cm, width=\linewidth]{figures/50_hsp_forecast_breakfast.png}
%         \caption{Forecast on Breakfast Dataset}
%         \label{fig:forecast_breakfast}
%     \end{subfigure}
%     \hfill
%     % Second figure
%     \begin{subfigure}[b]{0.6\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Final_Adjusted_Cosine_Similarity_StoP_SP.pdf}
%         \caption{Adjusted Cosine Similarity}
%         \label{fig:cosine_similarity}
%     \end{subfigure}
%     \caption{Comparison of Forecasting and Adjusted Cosine Similarity.}
%     \label{fig:side_by_side_figures}
% \end{figure}


\begin{figure}[]
    \centering
    \begin{minipage}[t]{0.34\textwidth}
        \centering
        \includegraphics[clip, trim=0cm 0cm 0cm 2.3cm, width=\linewidth]{figures/50_hsp_forecast_breakfast.png}
    \end{minipage}
    \begin{minipage}[t]{0.55\textwidth}
        \centering
        \includegraphics[clip, trim=0cm 0cm 0cm 0cm, width=\linewidth]{figures/Final_Adjusted_Cosine_Similarity_StoP_SP.pdf}
    \end{minipage}
    
    \caption{\textit{Left}: t-SNE projections of Stochastic Soft Prompt (StoP) tokens with a prompt length of $50$ on the Breakfast dataset for the forecasting task. Adjacent tokens are connected by a line, and the color darkens as the token index increases. The presence of lighter tokens on the periphery and darker tokens in the center indicates that the initial tokens learn very diverse information, while this diversity diminishes as the token index increases. \textit{Right}: Pairwise cosine similarity of the first $350$ tokens of a stochastic soft prompt and a soft prompt learned on the Breakfast dataset for forecasting. We observe that in StoP, the initial cosine similarities are smaller and increase as the token index increases, while no such variation by token index is present in a normal soft prompt.}
    \label{fig:coarse-to-fine}
\end{figure}
\vspace{1in}
\paragraph{Practical Benefits of StoP} We observe that StoP offers many benefits over standard soft prompting:

\begin{enumerate}
    \item \textbf{Improved Generalization:} StoP prompts achieve better generalization compared to standard soft prompts, with an average improvement of \textbf{12.69\%} in Macro-F1 across all datasets (Breakfast, MultiTHUMOS, and EPIC\_KITCHENS) and tasks (Forecast, Imputation, Anomaly Detection) (see \autoref{tab:compare_stop_and_sp})

    \item \textbf{Faster Training:} The stochastic nature of StoP reduces training time by approximately \textbf{25\%}, making it more efficient than standard soft prompting.

    \item \textbf{Resource Efficiency:} StoP enables flexible deployment in resource-constrained environments. Longer trained StoP prompts can be truncated to prefixes as needed, allowing for adaptable inference without compromising performance.
\end{enumerate}


\subsection{Complete Evaluation on Textual Datasets}
\label{sec:accuracy_numbers}
Here we reproduce the main table from our paper, along with accuracy numbers for the interested readers.

\begin{table}[ht]
    \centering
        \caption{\textit{Performance of our models on three textual datasets for forecasting, imputation, and anomaly detection tasks. Metrics are macro F1, and accuracy (ACC) for event type prediction and MAE for event time prediction. The \textbf{best result} in each class is highlighted in bold, and the \underline{second-best result} is underlined. Note that for anomaly detection, since the task involves identifying only the anomalous event, the MAE metric is not applicable and Chronos and LLMProcesses are not adaptable (see  \ref{sec:llms_for_ts_baselines}). A \textsuperscript{*} indicates our method. We use $5$ examples for few shot results (see \ref{sec:few-shot-analysis}). }} 
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|ccc|ccc|ccc}
        \toprule
        \textbf{Model} & \multicolumn{3}{c|}{\textbf{Breakfast}} & \multicolumn{3}{c|}{\textbf{MultiTHUMOS}} & \multicolumn{3}{c}{\textbf{EPIC-KITCHENS}} \\
        \midrule
        & M-F1 ($\uparrow$) & MAE ($\downarrow$) & ACC ($\uparrow$) & M-F1 ($\uparrow$) & MAE ($\downarrow$) & ACC ($\uparrow$) & M-F1 ($\uparrow$) & MAE ($\downarrow$) & ACC ($\uparrow$) \\
        \midrule
        & \multicolumn{9}{c}{Forecast}  \\
        \midrule
        Random & 0.0162 & 40.1513 & 0.0201 & 0.0417 & 1.8803 & 0.0382 & 0.0000 & 3.2001 & 0.0001 \\
        Chronos & 0.0011 & 43.0502	& 0.0021 & 0.0265 & 1.9805 & 0.0279 &	0.0000 & 3.5925	& 0.0005\\
        LLMTime & 0.0240 & 37.3902 &	0.0288	& 0.1280 & 2.2060 & 0.1235 & 0.0040 & 4.8948 & 0.0458\\
        LLMProcesses & 0.0337 & 44.9856 &	0.0845 & 0.1278	& 2.0471 &	0.0970	& 0.0049 &	4.3843 & 0.0703\\
        LASTS Zero Shot\textsuperscript{*} & 0.0604 & 38.1630 & 0.0969 & 0.1361 & 1.8868 & 0.1826 & 0.0105 & 3.1566 & 0.0920 \\
        LASTS Few Shot\textsuperscript{*} & 0.1518 & 35.5605 &	0.2133 & 0.1676 & 1.8114 & 0.2581 &	0.0149 & 3.3092 & 0.1150 \\
        LASTS + QLORA\textsuperscript{*} & \underline{0.2558} & 33.9737 & \underline{0.3763} & 0.3218 & 1.7281 & 0.4337 & 0.0764 & \underline{2.8964} & 0.2160 \\
        LASTS + SP\textsuperscript{*} & 0.2341 & \underline{32.8417} & 0.3691 & \underline{0.3707} & \underline{1.6630} & \underline{0.4782} & \underline{0.0780} & \textbf{2.8830} & \underline{0.2217} \\
        LASTS + StoP\textsuperscript{*} & \textbf{0.2633} & \textbf{32.5464} & \textbf{0.3880} & \textbf{0.3947} & \textbf{1.6503} & \textbf{0.4784} & \textbf{0.0797} & 3.0318 & \textbf{0.2298} \\
        \midrule
        & \multicolumn{9}{c}{Imputation}  \\
        % & M-F1 & MAE & ACC & M-F1 & MAE & ACC & M-F1 & MAE & ACC \\
        \midrule
        Random & 0.0168 & 37.7029 & 0.0214 & 0.0435 & 2.3622 & 0.0416 & 0.0000 & 3.4269 & 0.0001 \\
        Chronos & 0.0013 & 38.4039 & 0.0044 & 0.0294 & 2.3971 & 0.0312 &	0.0000 & 3.6955	& 0.0000\\
        LLMTime & 0.0137 & 35.9899	& 0.0381 & 0.0968	& 2.6998 & 0.1330 & 0.0005 &	3.6750 & 0.0314 \\
        LLMProcesses & 0.0156 & 34.7117 & 0.0488 & 0.1123 & 2.3786 & 0.1430 & 0.0008 & 4.2600 & 0.0371\\
        LASTS Zero Shot\textsuperscript{*} & 0.0263 & 33.0097 & 0.0594	& 0.0915 & 2.6696 & 0.1210 & 0.0015 & 3.6527 & 0.0446 \\
        LASTS Few Shot\textsuperscript{*} & 0.0520 & 33.3440 & 0.1001 & 0.1013	& 2.3982 & 0.1569 & 0.0023 & 3.2528	& 0.0547 \\
        LASTS + QLORA\textsuperscript{*} & 0.1688 & \underline{28.5638} & \underline{0.2500} & \underline{0.2132} & \textbf{2.2179} & \underline{0.2744} & 0.0378 & \underline{3.1194} & 0.1137 \\
        LASTS + SP\textsuperscript{*} & \underline{0.1581} & 28.8503 & 0.2264 & 0.2044 & 2.4092 & 0.2528 & \underline{0.0423} & 3.1456 & \underline{0.1270} \\
        LASTS + StoP\textsuperscript{*} & \textbf{0.2064} & \textbf{28.2251} & \textbf{0.2740} & \textbf{0.2213} & \underline{2.3445} & \textbf{0.2839} & \textbf{0.0610} & \textbf{3.1116} & \textbf{0.1424} \\
        \midrule
        & \multicolumn{9}{c}{Anomaly Detection}  \\
        % & M-F1 & MAE & ACC & M-F1 & MAE & ACC & M-F1 & MAE & ACC \\
        \midrule
        Random & 0.0349 & \textemdash & 0.0396 & 0.0381 & \textemdash & 0.0552 & 0.0238 & \textemdash & 0.0307 \\
        LLMTime & 0.0240 & \textemdash & 0.0288 &	0.0415 & \textemdash & 0.0639 & 0.0048	& \textemdash & 0.0650\\
        LASTS Zero Shot\textsuperscript{*} & 0.0923 & \textemdash & 0.0763 & 0.2755 & \textemdash & 0.1949 & 0.0159 & \textemdash & 0.0777 \\
        LASTS Few Shot\textsuperscript{*} & 0.0837 & \textemdash	& 0.0563	& 0.3535 & \textemdash & 0.2720 & 0.0337 & \textemdash & 0.1637 \\
        LASTS + QLORA\textsuperscript{*} & \underline{0.7011} & \textemdash & \underline{0.6478} & \underline{0.6003} & \textemdash & \underline{0.5084} & \underline{0.6520} & \textemdash & \underline{0.6988} \\
        LASTS + SP\textsuperscript{*} & 0.6520 & \textemdash & 0.5937 & 0.5231 & \textemdash & 0.4657 & 0.6159 & \textemdash & 0.6635 \\
        LASTS + StoP\textsuperscript{*} &\textbf{0.7198} & \textemdash & \textbf{0.6698} & \textbf{0.6045} & \textemdash & \textbf{0.5168} & \textbf{0.6603} & \textemdash & \textbf{0.7037}\\
        \bottomrule
    \end{tabular}
     }

    \label{tab:performance_comparison}
\end{table}


% \subsection{Future Work}

% Some interesting directions of extension for our Stochastic Soft Prompts are as follows:
% \begin{itemize}
%     \item \textbf{Finding equivalances to StoP training} In Stochastic soft prompts, during each training batch, we do a hard cut of the prompt. This results in following things implicitly:
%     \begin{itemize}
%         \item \textbf{Importance assignment to tokens:} Can this be achieved without hard splicing? Its a bit tricky given that we are working with multi layer transformer architecture, so figuring out how to do token importances in such a setting could help with this.
%         \item \textbf{FLow of Gradients from LASTS to StoP takes a shortcut} - This is a bit reminiscent of skip connections, but only a bit tangentially. Skipo connection would allow for us to have the gradienst from LASTS flow back to the desired locations in StoP but in that setting we are not turning off gradients from all other paths that are created due to these skip connections. 
%     \end{itemize}
%     \item \textbf{Are the prompts learned properly? } Using the analysis we did with t-SNE, is there a way to analyse  a given prmopt and figure out if its saturated or if it can hold more information, without doing extra training - this would guide when to stop training or decision to add extra  fine tokens dynamically.
%     \item \textbf{How many tokens are needed for an instance} - Its possible that not all tokens are neded for inference for all instance, given that we have a coarse to fine token structure, easier instances may just require a few tokens from the earlier part of the prompt for inference, and could safely ignore the fine tokens. Can this decisoin be made a priori inference as a function of the instance? Can this be a part of the training dynamic? Does this require the soft prompt to be at the end of the text prompt instead of at the beginning? How does this latter setting work with next token generation?
%     \item \textbf{Multitasking} - Since the tokens are organized as coarse to fine, we need the coarse tokens to be learned properly to do the task at hand properly. However, the fine tokens are probably not doing much for simple tasks and are not using their capacity well. For multitasking, we could have different set of coarse tokens assigned to each task - such that say if we have $n$ tasks - each token acts as a coarse token for atmost one task, and may act as a fine token for multiple tasks. If we assume that the tasks dont learn from each other and they are very disjoint, this mechanism could give us more juice out of same prompt length on multiple tasks for multi tasking. However, if we assume that tasks learn from each other too, this mechanism may be counter productive for such inter task interactions?
%     \item \textbf{Trojan horse} - Does this kind of training open some kind of a backdoor entry - in that while training a prompt, we train it on multi tasks, and each task has its  own  unique token importance order. For inference, only those tasks can be inferred for which this token importance is known. 
%     \item \textbf{Interpretability}: The prompt interpretations kind of suggest that the model is getting a generic signal from the prompts about the tasks they are trained on. What does the interpretation look like if we mix the prompts or just their coarse portions together and then interpret this through an LLM? Does interpretability improve if the soft prompt is after the text prompt?
% \end{itemize}




