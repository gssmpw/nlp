\section{Conclusion and Future Work}
\label{sec:conclusion}

We presented a novel approach to modeling asynchronous time series with an LLM, introducing a flexible alternative to  traditional TPP methods. By encoding an asynchronous time series in a prompt, our approach enables LLMs to leverage their world knowledge for various downstream tasks, including forecasting, anomaly detection, and imputation.

Additionally, we proposed Stochastic Soft Prompt (StoP), an efficient PEFT technique for adapting LLMs to asynchronous time series data. This approach not only improves adaptability but also suggests broader applicability to other data modalities such as image or natural language sequences.

Our findings highlight the potential of LLM-based representations for asynchronous time series and suggest new directions for future research, including refining LLM adaptation strategies and exploring hybrid approaches that combine neural architectures with prompt-based modeling.
