
\section{Background} 
% \begin{comment}

\paragraph{Notations.} 
% We first introduce some notations and the problem definition for the tasks that are used in this paper. 
%Following the notations in \cite{Mei2022Transformer},
We observe $n$ events over a fixed time interval $[0, T)$, with each event being denoted as $(e, t)$, where $e \in \mathcal{E}$ is the event type (or attributes) and $\mathcal{E}$ represents the space of event types. An \textit{asynchronous time series} is a sequence of events $x_{1:n} = ((e_1,t_1), (e_2, t_2), \ldots, (e_n, t_n))$ where $t_i$ is an increasing sequence in $[0, T)$ that does not necessarily observe any periodicity. A common alternative to the event time $t_i$ is the inter-arrival time $\tau_j:= t_j - t_{j-1}$; they are considered isomorphic and often used interchangeably. In our work there is very little constraint on $\mathcal{E}$ and in principle, our model still works even if $\mathcal{E}$ is infinite. We only need to be able to compute a vectorial representation of the event type/attributes, which is achieved through the LLM's learned input embeddings in our work.
\vspace{-0.1in}
\paragraph{Language modeling.} 
Language modeling is a widely used task to train LLMs where the goal is predicting the next word or character in a document. Language models are designed to work on a sequence of $m$ tokens, where each token belongs to a vocabulary. 
%$U=(u_1, u_2, \ldots, u_m)$, where $u_i$ is the $i$-th token and belongs to a vocabulary $\mathcal{V}$.
A tokenizer transforms the input text data into a sequence of tokens. The tokenization process is important and can impact performance significantly, for it directly influences how patterns form within tokenized sequences and the types of operations that language models can learn.

%Language models use various statistical and probabilistic techniques to determine the probability of the next token in the sequence, $p_\theta(U) = \prod_{i=1}^m p_\theta(u_i | u_{1:i-1})$.

%\end{comment}

%\todo{Indicate somewhere the tokenizer used for the experiments} \shubham{The tokenizer we use is teh one that comes with Llama3B- Instruct model already, does it need a mention? }

% trained on a collection of sequences, where each sequence is and each token, $u_i$ belongs to a vocabulary. 
\vspace{-0.1in}
\paragraph{Tasks} We propose a new approach to model asynchronous time series with LLMs, which solves three different tasks (see \autoref{fig:overall}).
% \begin{itemize}
% \itemsep0em 
% \item \textbf{Forecasting} (also known as \textit{next event prediction}) Given a history of events $x_{1:m}$ from an asynchronous time series, the model is tasked with predicting the next event $x_{m+1}$.
% %$p(x_{m+1:m+p}|x_{1:m})$. 
% %This definition covers both single even forecasting (when $p=1$) and multi-events forecasting ($p > 1$).
% %The goal of the forecasting task is to predict the future events of the series.
% \item \textbf{Data imputation.} One of the events $x_j$ of the series is randomly chosen and masked, the model is tasked with filling in the gap.
% \item \textbf{Anomaly detection.} One event $x_j$ of the series is randomly chosen and its event type $e_j$ is replaced randomly by another event type $e'$. The model is tasked with identifying this out-of-place element.
% \end{itemize}
\textbf{Forecasting} (also known as \textit{next event prediction}): Given a history of events $x_{1:m}$ from an asynchronous time series, the model is tasked with predicting the next event $x_{m+1}$.
%$p(x_{m+1:m+p}|x_{1:m})$. 
%This definition covers both single even forecasting (when $p=1$) and multi-events forecasting ($p > 1$).
%The goal of the forecasting task is to predict the future events of the series.
\textbf{Data imputation:} One of the events $x_j$ of the series is randomly chosen and masked, and the model is tasked with filling in the gap.
\textbf{Anomaly detection:} One event $x_j$ of the series is randomly chosen and its event type $e_j$ is replaced randomly by another event type $e'$. The model must identify the out-of-place element without knowing the position of the replaced element.


To find the right recipe for the model to solve these tasks, we innovated in two major directions: first, we  studied various representations of the asynchronous time series as inputs to LLMs (Section \ref{ATSrepresentation}) for zero shot completion of these tasks; and secondly, we study different parameter efficient techniques to adapt an LLM backbone for working with asynchronous time series, while leveraging its knowledge of the world and its understanding of natural language (Section \ref{sec:peft_with_lasts}). %We call this method \textit{Stochastic Soft Prompt (StoP)}.


\section{Proposed Method}
% In this paper, we propose 
% \begin{itemize}
%     \item LASTS: A representation of asynchronous time series for LLMs. 
%     \item Parameter-efficient LLM adaptation: Using LoRA, soft prompts, and our novel stochastic soft prompts (StoP), which, in combination with the LASTS representation, achieve state-of-the-art performance across several downstream tasks. \end{itemize}


\subsection{\textbf{LASTS} - Prompting LLMs with Asynchronous Time Series data} \label{ATSrepresentation} 

\begin{comment}
One of our key contributions is the exploration of different methods to represent asynchronous time series data for various tasks.    
\end{comment} 
Unlike ordinary time series, often represented as sequences of numerical values \citep{Gruver2023Large},  asynchronous time series are represented as sequences of events $x_i = (e_i, t_i)$, where $e_i$ is the event type, and $t_i$ is a representation of the timestamp of this event.  Normally, $t_i$ is expressed as an inter-arrival time, which is the time elapsed between event $x_{i-1}$ and $x_i$.

% We will demonstrate that labels in natural language, as opposed to a fixed set of action classes in existing methods, enable the LLM to better understand the context of the problem, leading to better predictions. They also instill a new level of flexibility into the prediction problem, as the input actions are no longer restricted to a particular subset.



% \paragraph{Event representation.}
% We also explore different ways to add dataset description to the prompt.\lilian{TODO: please elaborate}


% \paragraph{Dataset description.}
% \lilian{TODO: please elaborate}
% Understand what is the right recipe/configuration. 


In prior work on modeling asynchronous time series \citep{duTPP, Mehrasa2019Variational, Zhang2020Self, Mei2022Transformer}, events are typically reduced to categories from a small set of options. In contrast, we retain the event types $e_i$ as natural language descriptions. We introduce LASTS, which specifies how to input an asynchronous time series as part of a prompt to effectively leverage LLMs for various tasks on such data.

\begin{comment}
\paragraph{Prompt Structure} The prompts we use follow a generic structure designed for easy understanding and effective task performance by the LLM:

\begin{itemize}
\itemsep0em 
    \item \textbf{System Prompt}: Begins with "You are a helpful assistant," followed by a brief description of the task at hand and a concise description of the dataset being used. This part provides essential context to the LLM about what the task is and what type of data it will encounter.
    \item \textbf{Valid Event Types}: Since we treat the task as predicting a correct answer from a set of possible event types, we optionally include a list of valid event type names in the prompt. This helps guide the model and gives it inspiration for possible responses, effectively constraining its output to valid choices.
    \item \textbf{User Message}: Provides the input series, describing the events in the format discussed earlier.
\end{itemize}


A visual structure of this prompt is present in \ref{fig:ssp} as 'text prompt'. We conducted various studies to determine the optimal structure for the system prompt and the best representation of the input series for maximizing LLM performance.

\paragraph{Prompt Design and Variants} In our work, we also investigated the impact of different prompt configurations:

\begin{itemize}\itemsep0em 
    \item \textbf{Dataset Description}: We explored how providing a brief dataset description within the system prompt could improve the model's understanding of the data it was handling.
    \item \textbf{Valid Vocabulary} We experimented with including a list of valid event type names to help guide the model's predictions, aiming to see if explicitly mentioning the possible options lead to better performance.
\end{itemize}


\paragraph{Event Representation} We experimented with multiple ways to represent asynchronous time series data in the prompt, aiming to understand the best structure for effective LLM interaction. Specifically, we tested the following variations:
\begin{itemize} \itemsep0em 
    \item Representing each event as either (inter\_arrival\_time $t_i$, event\_type $e_i$) or (event\_type $e_i$, inter\_arrival\_time $t_i$). We wanted to test here if predicting the event type first helps the model get better time predictions.
    \item Using scrambled names for event types to assess the model's reliance on world knowledge in understanding the event descriptions.
    \item Using durations instead of inter arrival time as a time representation $t_i$
\end{itemize}
\end{comment}

\paragraph{LASTS Prompt Structure} The LASTS prompt consists of three parts that can be mapped to the system-user-assistant structure when using an instruction fine-tuned LLM (see \autoref{fig:last_prompt}). The \textbf{system prompt} introduces what an asynchronous time series is, provides a description of the task to be performed, and includes details about the underlying dataset. The \textbf{user prompt} represents the input series as a comma-separated sequence of tuples \((e_i, t_i)\), where \(e_i\) is the textual description of the event type and \(t_i\) is the inter-arrival time. The \textbf{assistant prompt} contains the correct event if performing LLM adaptation training, or is left to be generated by the LLM during inference. More details about the exact prompts used in our experiments can be found in Appendix \ref{sec:zero_shot_prompts}.

\begin{figure}[t]
\centering
\includegraphics[width=0.99\columnwidth]{figures/LASTS_prompt.pdf}
\caption{Components of a LASTS prompt: A concise task description is included in the system prompt, while asynchronous time series is provided as an input in the user prompt.}
\label{fig:last_prompt}
\end{figure} 
% \begin{itemize}

%     \item The \textbf{system prompt} introduces what an asynchronous time series is, provides a description of the task to be performed, and includes details about the underlying dataset;
%     \item The \textbf{user prompt} represents the input series as a comma-separated sequence of tuples \((e_i, t_i)\), where \(e_i\) is the textual description of the event type and \(t_i\) is the inter-arrival time;
%     \item The \textbf{assistant prompt} contains the correct event if performing LLM adaptation training, or is left to be generated by the LLM during inference.
% \end{itemize}
\begin{figure*}[t]
\centering
\includegraphics[width=0.99\textwidth]{figures/SP_vs_StoP_2.pdf}
\caption{Comparison of Soft Prompt (SP) and Stochastic Soft Prompt (StoP) training. For illustration, the soft prompt $P$ is of length $50$. In SP, the entire prompt is used during both training and inference. In StoP, a random prefix of $P$ is used per training batch, while the full prompt is used for inference. Fire marks the soft prompt, which is the trainable prompt portion, while snowflake represents the frozen LASTS text prompt.}
\label{fig:ssp_vs_sp}
\end{figure*} 




% \paragraph{Use of Prior World Knowledge by LLMs}


\subsection{Parameter Efficient LLM Adaptation with LASTS representation}
\label{sec:peft_with_lasts}

% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.99\textwidth]{figures/ICLR_Model_2.pdf}
% \caption{Overview of LASTS and Prompt Tuning with LASTS: (a) Structure of a LASTS prompt for adapting an asynchronous time series for use as input for an LLM. (b) $SP$ and $StoP$ training setup - the LLM backbone is frozen and the soft prompt is fine-tuned via gradients computed through the standard next token prediction loss. (c) Comparison of SP and StoP training. In StoP, a random prefix of the trainable prompt is used during each training batch.
% }
% \label{fig:ssp}
% \end{figure*} 


Having established a representation of asynchronous time series for use with LLMs via LASTS, we further enhance the model's adaptability to various tasks using three different adaptation techniques: 

\paragraph{Low Rank Adaption} LoRA is a family of low-rank adaptation techniques that reduce the number of trainable parameters by learning low-rank updates to selected model weights, allowing for efficient fine-tuning of large models. We adapt the LLM backbone for our tasks by applying low-rank adaptations using the LASTS representation as inputs to encode both the task and the input asynchronous time series.

\paragraph{Soft Prompting (SP)} SP involves prepending a continuous prompt to the LASTS representation, which is trained through gradients from next token prediction loss. This guides the model towards task-specific behavior without altering the model weights directly. (See \autoref{fig:ssp_vs_sp})

\paragraph{Stochastic Soft Prompting (StoP)} We propose Stochastic Soft Prompts - an enhancement of SP which learns more robust prompts by imposing a coarse-to-fine structure on the prompt tokens. (See \autoref{sp_vs_stop}).  Similar to SP, we prepend a continuous prompt to the LASTS representation which is trained through gradients from a next-token prediction loss. However, in SP, the entire soft prompt $P$
of length  $L$ is used during training, while in StoP, we randomly select a prefix of the prompt $P$ for each training batch. Specifically, for each batch, we sample a prefix length $l$ from a probability distribution $p(l)$, where $l \leq L$. The soft prompt used for that batch is then represented by $P_{batch} = P[:l] \ \ \text{with} \ \ l \sim p(l)$. In our experiments, we use a uniform distribution as $p$. Both the forward pass and the backward pass are conducted using only the selected prefix $P_{\text{batch}}$. During inference, we use the entire learned soft prompt of length $L$:  $ P_{\text{inference}} = P[1:L] $
See Figure \ref{fig:ssp_vs_sp} for more details. {Our approach is inspired by techniques like dropout \citep{Srivastava2014Dropout} and stochastic depth \citep{huang2016deep}, as well as audio models like SoundStream \citep{Zeghidour2021SoundStream}, where randomly selecting the first $k$ codebooks during training enables better generalization. Similarly, we draw inspiration from Matryoshka Representations \citep{kusupati2022matryoshka}, which learn representations such that predefined prefix lengths remain valid representations.}

These adaptation techniques enable an LLM backbone to handle a variety of asynchronous time series tasks, including forecasting, imputation, and anomaly detection, while maintaining parameter efficiency. Details on the exact prompt representation are provided in Appendix \ref{sec:lasts_for_peft}.



% \subsection{\textbf{StoP} - \textbf{Sto}chastic \textbf{S}oft \textbf{P}rompt}  \label{stochSoftPrompt}

% \lilian{I commented out the following and relocated it to "Section 2: Related Works" - just want to bring out StoP asap to keep the discussion focused.}
% \begin{comment} Soft prompts have emerged as an efficient prompt tuning large language models (LLMs) without the need for manually crafted input prompts. Instead of modifying the entire model or coming up with adapters for model layers, soft prompting introduces a trainable tensor to be concatenated to the prompt. Soft prompts are effective in adapting a pre-trained model to a specific task domain in various fields such as \shubham{ Give some examples here}. Soft prompts have emerged as a parameter efficient method of adapting LLMs to new domains, giving better generalization performance metrics than those obtained by methods like QLoRA with similar number of trainable parameters.

% Stochastic approaches have consistently demonstrated both empirical and theoretical advantages across various machine learning domains, especially in avoiding overfitting. Techniques such as dropout \citep{srivastava2014dropout} and stochastic depth \citep{huang2016deep}  introduce randomness into the training process to encourage more generalized representations. Outside of deep learning, randomized strategies are also widely successful. For example, Thompson Sampling in bandit problems \citep{pmlr-v23-agrawal12} %(Agrawal and Goyal, 2011) 
% outperforms deterministic strategies by balancing exploration and exploitation, while randomized confidence bounds in UCB algorithms improve decision-making \citep{pmlr-v108-vaswani20a}. Recent work in data pruning \citep{qin2024infobatch} further highlights the superiority of randomized over deterministic methods, leading to better generalization and empirical performance. \lilian{TODO: I updated the bib references in this section based on our Slack discussion, please check if they are right. I will try to read those papers later.}

% \end{comment}
% We further propose Stochastic Soft Prompting (StoP), an enhancement of soft prompting-based prompt tuning that incorporates stochasticity to promote robustness during training while also learning prompts with enforced desired structures. Soft prompting involves prepending a trainable vector $P$ to the input, enabling large language models (LLMs) to adapt to specific tasks without modifying their core weights. In our work, we use soft prompting as a prompt-tuning mechanism to adapt the LLM backbone for asynchronous time series data and introduce StoP as a more robust version of Soft Prompt (SP) training.

% \paragraph {Prompt setup} We adapt soft prompts for AST tasks as follows. We prepend the trainable soft prompt $P$ to the standard instruction text prompt $T$ introduced in the previous section. We make the system message in $T$ a very succinct representation of the task, excluding any details about the dataset or vocabulary, while keeping the user messgae and the assistant messages same. Our goal is for the soft prompt to learn the specifics of the dataset during training, as well as the possible allowed responses, thereby allowing flexibility without adhering to a strict vocabulary. We use following task descriptions for each task:

% See figure \ref{fig:ssp} for prompt structure.

% \paragraph{Traditional Soft Prompt Training (SP)} We keep the model parameters fixed and use gradients from the cross-entropy loss function to train the soft prompts. Specifically, we use cross-entropy loss to represent the next token prediction loss on portions of the textual prompt. This includes both the user message containing the input series and the assistant answer containing the prediction.We observe that applying cross-entropy loss to both the user input and the assistant response enhances the soft prompt representations, allowing them to better understand the structure of the input sequences, which leads to improved performance. We explore this difference further in Appendix \ref{app:Loss}.During inference, we prompt the model with the learned soft prompt $P$ followed by the text prompt $T$ up to the assistant header. The model is expected to generate a prediction, which is parsed using a regular expression to yield the model's prediction of the next element. See figure \ref{fig:ssp} for more details.

% \paragraph{Stochastic Soft Prompt Training (SToP)} In SP, we use the entire soft prompt $P$
% of length  $L$ in each batch, ensuring consistent use of all tokens of 
% $P$. However, in StoP, we randomly select a prefix of the soft prompt in each training batch. Specifically, for each batch, we choose a prefix length $l$ from a probability distribution 
% $p(l)$, where $l \leq L$. The soft prompt used for that batch is then represented by:
% \begin{equation*}
%     P_{batch} = P[:l] \ \ \text{with} \ \ l \sim p(l)
% \end{equation*}
% In our experiments, we use a uniform distribution as $p$.
% Both the forward pass and the backward pass are conducted using only the selected prefix $P_{\text{batch}}$.
% During inference, we use the entire learned soft prompt of length $L$:   $ P_{\text{inference}} = P[1:L] $.  See figure \ref{fig:ssp} for more details.


\begin{comment}
We show the following results:

\begin{itemize}
    \item HSP acts as a regularizer over SP
    \item prompts learnt through HSP results in better performance over the time series tasks as compared to SP
    \item every prefix in the prompt learnt through HSP is a valip learnt prompt, while the same is not true for SP
    \item HSP trains much faster than SP
    \item HSP possibly allows us to dynamically add more tokens at the end of the learnt prompt if there feels like a need to add more parameters
    \item Both HSP and SP perform similar or better to QLoRA finetuning of the LLM, where we finetune only the attention alyers, while using much less learnable parameters
    \item Both HSP and SP perform much better on forecasting on the tasks from EasyTPP paper especially on the macro F1 metric which is the right metric to look at as these are multi class classification problems.
\end{itemize}
\end{comment}

% \paragraph{Prompt Setup} For our experiemnts with Soft prompting (SP) and Stochastic Soft Prompting (SSP), we prefix a regular LLM text prompt with the trainable tokens as shown in figure \ref{fig:prompt}. We found that using a small text description of the task needed to be accomplished helped the model learn better prompts. We include a very concise text instructions in our experiments based on the task as follows:
% \begin{itemize}
%     \item \textbf{Forecasting}: \textit{"Predict the next element of this asynchronous time series where each element is of the form (inter\_arrival\_time,\ action\_name)."}
%     \item \textbf{Imputation}: \textit{"Predict the  element marked 'MISSING' in this asynchronous time series where each element is of the form (inter\_arrival\_time, action\_name)."}
%     \item \textbf{Anomaly Detection}: \textit{"One of the element in this asynchronous time series is anomalous, find this element. Each element of the series is of the form (inter\_arrival\_time, action\_name)."}
% \end{itemize}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.99\textwidth, trim={0cm 9.5cm 0cm 10.5cm}, clip]{figures/Prompt.pdf}
% \caption{Prompt Structure - The trainable soft prompt portion is prefixed to a standard text prompt for the LLM backbone used. This figure shows the prompt structure used for Llama3 Instruct version, as applied to the forecasting task. }
% \label{fig:prompt}
% \end{figure}


% \begin{figure}[h]
% \centering
% \begin{subfigure}[b]{0.32\textwidth}
%     \centering
%     \includegraphics[width=\textwidth, trim={2.5cm 8cm 2.5cm 8cm}, clip]{figures/length_ablations_acc.pdf}
%     \caption{Accuracy}
%     \label{fig:length_ablation_acc}
% \end{subfigure}
% \hfill
% \begin{subfigure}[b]{0.32\textwidth}
%     \centering
%     \includegraphics[width=\textwidth, trim={2.5cm 8cm 2.5cm 8cm}, clip]{figures/length_ablations_macro_f1.pdf}
%     \caption{Macro F1}
%     \label{fig:length_ablation_f1}
% \end{subfigure}
% \begin{subfigure}[b]{0.32\textwidth}
%     \centering
%     \includegraphics[width=\textwidth, trim={2.5cm 8cm 2.5cm 8cm}, clip]{figures/length_ablations_mae.pdf}
%     \caption{MAE}
%     \label{fig:length_ablation_mae}
% \end{subfigure}
% \caption{Comparison of training SSP for different prompt lengths for forecasting on the breakfast dataset. Here we train all models for 10 epochs and report metrics on the validation set.We note that as the prompt length increases, we get faster convergence and better validation metrics.}
% \label{fig:length_ablation}
% \end{figure}



