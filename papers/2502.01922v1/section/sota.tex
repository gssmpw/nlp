\section{Related Work}

\paragraph{Temporal Point Processes (TPPs).} 
TPPs \citep{Hawkes1971Spectra, daley2007introduction} have emerged as the standard method to model asynchronous time series data. 
% It is a mathematical framework for modeling sequences of events  and autoregressively generate events one after another. 
Over the last decade, a large number of neural temporal point processes have been proposed to capture complex dynamics of stochastic processes in time by using neural networks.
\citet{duTPP, MeiEisner} proposed to use models based on Recurrent Neural Networks (RNNs) to model the sequence of events. Then, more advanced models \citep{Mehrasa2019Variational, DiffusionTPP} were proposed to better model uncertainty when predicting the future. Recently, several neural TPP models incorporate Transformers in order to improve performance by using attention to better model long-term dependencies. These include the Self-attentive Hawkes process (SAHP) \citep{Zhang2020Self}, Transformer Hawkes process (THP) \citep{Zuo2020Transformer}, and Attentive Neural Hawkes Process (Att-NHP) \citep{Mei2022Transformer}.


\paragraph{Transformers for Time Series.} 
Transformers \citep{Vaswani2017Attention} have become popular to model regularly-sampled time series because of their ability to capture long-range dependencies and to extract semantic correlations among the elements of a long sequence. Informer \citep{Zhou2021Informer} introduced a novel self-attention architecture to reduce the quadratic complexity of the original self-attention. Autoformer \citep{Wu2021Autoformer} used a novel decomposition architecture with an auto-correlation mechanism to identify more reliable temporal patterns.  Crossformer \citep{Zhang2023Crossformer} proposed a novel architecture to model both  the cross-time and cross-dimension dependencies multivariate time series forecasting. PatchTST \citep{Yuqi2023PatchTST} tokenizes the time series in patches, and proposes a channel-independent patch time series Transformer to improve the long-term forecasting accuracy.

Due to space limitations, we only review some popular models and invite the reader to \citep{Wen2023Transformers, Zeng2023Transformers} for a more complete literature reviews of Transformer models for regularly-sampled time series.
Most of the time series Transformer models are designed for specific tasks, and cannot be easily extended to asynchronous time series data or other tasks like anomaly detection or imputation.

% Most of these Transformer models are designed for the forecasting task, and cannot be easily extended to other tasks like anomaly detection or imputation.
% Similarly, these models are designed for regularly-sampled time series and cannot be easily extended to asynchronous time series due to the different nature of the data.


\paragraph{Foundation Models (FMs) for Time Series.}
FMs \citep{Bommasani2021Opportunities} are a family of deep models that are pretrained on vast amounts of data, and have caused a paradigm shift due to their unprecedented capabilities for zero-shot and few-shot generalization.
FMs have revolutionized natural language processing \citep{Brown2020Language, BigScience2023Bloom, Wu2024Empirical, Dubey2024Llama3} and computer vision \citep{Radford2021Learning, Kirillov2023Segment}.
% Recently, several methods have been developed to build FMs for time series and can be organized in three main categories based on the pretraining data: text, time series, and image.
The availability of large-scale time series datasets has opened the door to pretrain a large model on time series data.
ForecastPFN \citep{Dooley2024ForecastPFN} proposed the first zero-shot forecasting method trained purely on synthetic data.
Lag-Llama \citep{Rasul2023LagLlama} introduced a univariate probabilistic forecasting model that was pretrained on a large corpus of diverse time series data.
TimeFM \citep{Das2024TimesFM} pretrained a decoder style attention model with input patching, using a large time series corpus comprising both real-world and synthetic
datasets. Chronos \citep{ansari2024chronos} introduced a framework for pretraining on tokenized time series data, achieving state-of-the-art zero-shot forecasting performance and simplifying forecasting workflows.
MOIRAI \citep{Woo2024Unified} is an enhanced Transformer architecture pretrained in the Large-scale Open Time Series Archive, that achieves competitive performance as a zero-shot forecaster.
% \subparagraph{Text-based FMs.} LLMs pretrained on large amounts of text data have emerged as a promising direction to model time series data.
% GPT4TS \citep{Zhou2023One}, LLM4TS \citep{Chang2024Llm4ts}, and TEMPO \citep{Cao2024Tempo} fine-tuned a pretrained GPT2 \citep{Radford2019Language} on some time series downstream tasks to capture intrinsic dynamic properties.
% TimeLLM \citep{Jin2023TimeLLM} proposed a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact.
% PromptCast \citep{Xue2023Promptcast} introduced  a new prompt-based forecasting paradigm, where the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner.
% LLMTime \citep{Gruver2023Large} showed that LLMs can zero-shot extrapolate time series if the numerical values of the time series are well represented.
\paragraph{{LLMs for Time Series}} LLMs pretrained on large amounts of text data have emerged as a promising direction to model time series data.
GPT4TS \citep{Zhou2023One}, LLM4TS \citep{Chang2024Llm4ts}, and TEMPO \citep{Cao2024Tempo} fine-tuned a pretrained GPT2 \citep{Radford2019Language} on some time series downstream tasks to capture intrinsic dynamic properties.
TimeLLM \citep{Jin2023TimeLLM} proposed a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact.
PromptCast \citep{Xue2023Promptcast} introduced  a new prompt-based forecasting paradigm, where the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner.
LLMTime \citep{Gruver2023Large} showed that LLMs can zero-shot extrapolate time series if the numerical values of the time series are well represented. LLM Processes \citep{requeima2024llm} explores various prompt configurations for using LLMs for time series forecasting condiitoned on a textual context. We refer the reader to \citep{zhang2024large} for a more detailed survey on the topic.


% \subparagraph{Time series-based FMs.} The availability of large-scale time series datasets has opened the door to pretrain a large model on time series data.
% ForecastPFN \citep{Dooley2024ForecastPFN} proposed the first zero-shot forecasting method trained purely on synthetic data.
% Lag-Llama \citep{Rasul2023LagLlama} introduced a univariate probabilistic forecasting model that was pretrained on a large corpus of diverse time series data.
% TimeFM \citep{Das2024TimesFM} pretrained a decoder style attention model with input patching, using a large time series corpus comprising both real-world and synthetic
% datasets. 
% MOIRAI \citep{Woo2024Unified} is an enhanced Transformer architecture pretrained in the Large-scale Open Time Series Archive, that achieves competitive performance as a zero-shot forecaster.
% \subparagraph{Image-based FMs.} Several works started to explore the use of FMs pretrained on images because of the better intrinsic similarities between images and time series such as trend, stationarity, seasonality/periodicity, and sudden change. 
% \citep{Zhou2023One} tried to fine-tune a BEiT \citep{Bao2021BEiT} trained on images for time series forecasting, but it falls short of the leading text-based and time series-based FMs.
% Recently, VisionTS \citep{Chen2024Visionts} proposes to use a vision Transformer pretrained on ImageNet to reduce the cross-domain gap or in-domain heterogeneity between time series and text.

\paragraph{Vision Models for Time Series.} Several works started to explore the use of FMs pretrained on images because of the better intrinsic similarities between images and time series such as trend, stationarity, seasonality/periodicity, and sudden change. 
\citet{Zhou2023One} tried to fine-tune a BEiT \citep{Bao2021BEiT} trained on images for time series forecasting, but it falls short of the leading text-based and time series-based FMs.
Recently, VisionTS \citep{Chen2024Visionts} proposes to use a vision Transformer pretrained on ImageNet to reduce the cross-domain gap or in-domain heterogeneity between time series and text.





\paragraph{Parameter Efficient Fine Tuning (PEFT).} 
PEFT \citep{peft} is a paradigm to adapt pretrained LLMs to various domains without fine-tuning all of a modelâ€™s parameters, which can be costly and require large amounts of training data. % The reader may refer to \cite{PEFTreview} for a comprehensive survey.
% Common PEFT methods include Low Rank Adapters (LoRA) and prompt-based methods. 
LoRA \citep{LoRA} methods freeze the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. 
QLoRA \citep{QLoRA} advances finetuning by significantly reducing memory usage while preserving task performance.

\paragraph{Soft Prompt Tuning.}
Soft prompts have emerged as a compute efficient method for adapting a pretrained LLMs to new domains without altering their core architectures.
\citet{Brown2020Language} were among the first to demonstrate the power of prompting for task adaption of pretrained language models, but automatically finding suitable sets of
text prompts remains an open challenge.
\citet{Li2021Prefix, qin-eisner-2021-learning} proposed the prefix tuning technique that preprends a few task specific soft tokens to the input and hidden states of each Transformer layer. During training, the parameters of soft prompts are updated by gradient descent while the model parameters keep frozen.
\citet{Liu2021Ptuning} showed the prefix tuning technique could be effectively applied to
natural language understanding with different scales of models.
\citet{Lester2021Power} simplified the prefix tuning technique such that it only adds soft prompts to the input layer and is now considered the standard soft prompt-tuning.
% where they optimize sequences of continuous-valued embeddings prepended to the real embeddings of the input tokens.
