\subsection{Experimental setup}
\label{sec:datasets}
\paragraph{Datasets.} We perform experiments on two different sets of datasets: three text-based action datasets and five standard temporal point process datasets.
The main difference is that actions are represented by words in the action datasets, whereas they are represented by indices in temporal point process datasets.
The \textbf{text-based action datasets} are built from the action annotations of activity videos.  
\textit{Breakfast} \citep{Kuehne2014Language} contains 1712 videos with 177 action classes related to breakfast preparation. 
Each video has a sequence of events to prepare breakfast, with each event containing the timestamp and the action.
\textit{EPIC-KITCHENS-100} \citep{Damen2022Rescaling} is a large-scale dataset in egocentric vision capturing daily activities in the kitchen over multiple days with a total of 100 hours of recording.
It presents more complex activity than the Breakfast dataset, with rich annotations of sequences of actions comprising 97 verb classes and 300 noun classes, with 20k unique narrations. 
\textit{MultiTHUMOS} \citep{Yeung2018Every} contains 400 videos with 65 action classes related to  human activities.
Each video has a sequence of human activity events, with each event containing the timestamp and the activity.
For the \textbf{temporal point process datasets}, we use the five benchmarks introduced in \citep{Xue2023EasyTPP}: 
\textit{Amazon} \citep{Ni2019Justifying} where the goal is to predict the timestamp and category (among 16 categories) of the next reviewed product, 
\textit{Retweet} \citep{Zhou2013Learning} where the goal is to predict the timestamp and category (among 3 categories) of the next user to retweet a post, 
\textit{Taxi} \citep{TaxiData} where the goal is to predict the timestamp and category (among 10 categories) of the next pick-up or drop-off of a taxi driver, 
\textit{Taobao} \citep{Xue2022Hypro} where the goal is to predict the timestamp and category (among 20 categories) of the item clicked by a user, 
and \textit{StackOverflow}\footnote{\scriptsize\url{https://snap.stanford.edu/data/}} where the goal is to predict the timestamp and category (among 22 categories) of the next badges assigned to a given user.
We follow the same data preprocessing as in \citep{Xue2023EasyTPP}.
For each of these datasets, the semantic meaning of the event type is unknown, and only the index of the event type is available.
We use the index of the event type as input to our model.
% \textit{Amazon} \citep{amazon-2018} contains \todo{N} examples, where each example is 

% \paragraph{Datasets} Our main results are derived from three text-based datasets, each rooted in natural language and describing events that occur in common scenarios, such as cooking or games. These datasets represent asynchronous time series with elements $x_i = e_i@t_i$  where $e_i$ is a short text describing the action happening at time  $t_i$
% \begin{itemize}
% \item \textbf{Breakfast} \citep{Kuehne2014Language} contains \todo{1712} videos with \todo{177} action classes related to breakfast preparation. 
% Each video has a sequence of events to prepare breakfast, with each event containing the timestamp and the action.

% \item \textbf{EPIC-KITCHENS-100} \citep{Damen2022Rescaling} is a large-scale dataset in first-person (egocentric) vision capturing daily activities in the kitchen over multiple days.
% It presents more complex activity than Breakfast dataset, with rich annotations of sequences of actions. 
% \todo{Describe statistics on data}

% \item \textbf{MultiTHUMOS} \citep{Yeung2018Every} contains 400 videos with 65 action classes related to  human activities.
% Each video has a sequence of human activity events, with each event containing the timestamp and the activity.
% \todo{Examples from dataset in appendix}
% \end{itemize}
% In addition, we work with 5 datasets benchmarked in \cite{Xue2023EasyTPP} for forecasting on temporal point processes. These are $5$ real-world datasets from popular works that contain diverse characteristics in terms of their application domains and temporal statistics. These datasets do not have a textual component to them. The events here belong to a relatively small number of classes and the forecasting task is to predict the next event type and its inter arrival time given the preceding elements of the series. \todo{move bulk of this to Appendix}
% \begin{itemize}

%       \item {\textbf{Amazon}} \citep{amazon-2018}. This dataset includes time-stamped user product reviews behavior from January, 2008 to October, 2018. 
% Each user has a sequence of produce review events with each event containing the timestamp and category of the reviewed product, with each category corresponding to an event type. We work on a subset of $5200$ most active users with an average sequence length of $70$ and then end up with $K=16$ event types. 

%     \item \textbf{Retweet} 
%     \textnormal{\citep{zhou-2013}}. This dataset contains time-stamped user retweet event sequences.  The events are categorized into $K = 3$ types: retweets by “small,” “medium” and “large” users. Small users have fewer than $120$ followers, medium users have fewer than $1363$, and the rest are large users. We work on a subset of $5200$ active users with an average sequence length of $70$.

%     \item \textbf{Taxi} \textnormal{\citep{whong-14-taxi}}. This dataset tracks the time-stamped taxi pick-up and drop-off events across the five boroughs of the New York City; each (borough, pick-up or drop-off) combination defines an event type, so there are $K=10$ event types in total. We work on a randomly sampled subset of $2000$ drivers with an average sequence length of $39$.
% %average length is $39$. 


%     \item \textbf{Taobao} \citep{xue2022hypro}. This dataset contains time-stamped user click behaviors on Taobao shopping pages from November 25 to December 03, 2017. Each user has a sequence of item click events with each event containing the timestamp and the category of the item. The categories of all items are first ranked by frequencies and the top $19$ are kept while the rest are merged into one category, with each category corresponding to an event type. We work on a subset of $4800$ most active users with an average sequence length of $150$ and then end up with $K=20$ event types. 


%     \item {\textbf{StackOverflow} \textnormal{\citep{snapnets}}.} This dataset has two years of user awards on a question-answering website: each user received a sequence of badges and there are $K=22$ different kinds of badges in total.  We work on a subset of $2200$ active users with an average sequence length of $65$.
% \end{itemize}

% \thibaut{I moved the data preparation to the appendix \autoref{sec:dataset-preparation} as it is important only to reproduce the results. It does not add a lot of value to the paper.}

% \begin{figure}[t]
%     \centering
%     \begin{minipage}[b]{0.45\columnwidth}
%         \includegraphics[width=\columnwidth]{figures/dataset_skew_plots/breakfast.pdf}
%     \end{minipage}
%     \begin{minipage}[b]{0.45\columnwidth}
%         \includegraphics[width=\columnwidth]{figures/dataset_skew_plots/multihumos.pdf}
%     \end{minipage}
    
%     % % if using 4, use this
%     % \begin{minipage}[b]{0.45\columnwidth}
%     %     \includegraphics[width=\columnwidth]{figures/dataset_skew_plots/epic_kitchen.pdf}

%     % \end{minipage}
%     % \begin{minipage}[b]{0.245\textwidth}
%     %     \includegraphics[width=\textwidth]{figures/dataset_skew_plots/amazon.pdf}

%     % \end{minipage}
    
%     % \begin{minipage}[b]{0.245\textwidth}
%     %     \includegraphics[width=\textwidth]{figures/dataset_skew_plots/retweet.pdf}

%     % \end{minipage}
%     % \begin{minipage}[b]{0.245\textwidth}
%     %     \includegraphics[width=\textwidth]{figures/dataset_skew_plots/taxi.pdf}

%     % \end{minipage}
%     % \begin{minipage}[b]{0.245\textwidth}
%     %     \includegraphics[width=\textwidth]{figures/dataset_skew_plots/taobao.pdf}

%     % % if using 4, use this
%     % \end{minipage}
%     % \begin{minipage}[b]{0.45\columnwidth}
%     %     \includegraphics[width=\columnwidth]{figures/dataset_skew_plots/stackoverflow.pdf}

%     % \end{minipage}
    
% \caption{Normalized event counts (y-axis) vs. event types sorted by count (x-axis) for two datasets, showing class imbalance.}

% \label{fig:class_imbalance}
% \end{figure}
%  % that are often easier to predict because they have more training examples

\paragraph{Metrics.} 
Due to the bi-modal nature of the asynchronous time series, we report separate metrics for the event type and time.
We report the Macro-F1 (M-F1) \citep{Yang1999Evaluation} for event type prediction as Macro-F1 is better suited for multi-class classification tasks with skewed class distributions (Appendix \ref{app:class_imbalance}) than accuracy because Macro-F1 gives equal importance to all the classes. We also report accuracy numbers in Appendix \ref{sec:accuracy_numbers}.
% As shown in \autoref{fig:class_imbalance}, most of the datasets used in this paper exhibit class imbalances, which makes accuracy a less reliable metric. 
% Macro-F1 treats all classes equally by computing the average F1 score across all classes, ensuring that no class is given undue weight, regardless of its frequency in the dataset. This approach provides a more balanced evaluation of model performance across all event types.
We report either the Mean Absolute Error (MAE) or Root Mean Square Error (RMSE) for time prediction.

% \paragraph{Metrics.} 
% Our main evaluation metrics are Macro-F1 for event type prediction, and Mean Absolute Error (MAE) or Root Mean Square Error (RMSE) for time prediction. we use Macro-F1 instead of accuracy for event type prediction, as Macro-F1 is well suited for multi-class classification tasks with potentially skewed class distributions. As shown in Figure \ref{fig:dataset_skew}, the datasets exhibit class imbalances, which makes accuracy a less reliable metric. Macro-F1 treats all classes equally by computing the average F1 score across all classes, ensuring that no class is given undue weight, regardless of its frequency in the dataset. This approach provides a more balanced evaluation of model performance across all event types.

\paragraph{Implementation details} We use Llama-3-8B-Instruct \citep{Dubey2024Llama3} as our LLM backbone. For zero-shot experiments, we disable sampling during response generation, ensuring deterministic outputs. For LLM adaptation experiments, we use QLoRA as the low rank adaptation algorithm, Adam as the optimizer, and a constant learning rate of $2e^{-4}$ for QLoRA and $1e^{-4}$ for prompt tuning. Following \cite{Xue2023EasyTPP}, we split our datasets into a train/validation/test ratio of 70/10/20. Both SP and StoP training are conducted for the same number of epochs. We employ early stopping based on the Macro-F1 on the validation set. We report  performance on the test set.

We use a prompt length of $400$ for prompt tuning in both SP and StoP experiments. This value was selected through hyperparameter tuning across all datasets and tasks, striking a balance between model capacity, performance, and the compute resources available to us. Given that Llama-3-8B-Instruct has a hidden dimension of $4096$, this configuration results in approximately $1.6M$ trainable parameters, which corresponds to only $0.02\%$ of the LLM parameters. For QLoRA, we use a rank of $4$, resulting in a comparable number of trainable parameters ($1.7M$).


\subsection{Experiment Results}

\paragraph{Baselines} We evaluate our methods using four sets of baselines. See Appendix  \ref{sec:llms_for_ts_baselines} for details.
\begin{itemize}
    \item \textbf{Random baseline}: We establish a random baseline simulating random guesses to evaluate our methods on the three text-based datasets and tasks~ (\autoref{tab:performance_evaluation_new}, \autoref{fig:zero_shot_comparison}).
    \item \textbf{Foundation models for time series}: We use a state-of-the-art pretrained foundation model for time series forecasting, \textbf{Chronos} \cite{ansari2024chronos}, as a baseline for forecasting and imputation tasks on asynchronous time series~ (\autoref{tab:performance_evaluation_new}).
    \item \textbf{LLM for time series}:  We adapt two LLM-based time series forecasting methods, \textbf{LLMTime} \citep{Gruver2023Large} and \textbf{LLMProcesses} \citep{requeima2024llm}, as baselines for zero-shot LASTS prompting on asynchronous time series~ (\autoref{tab:performance_evaluation_new}, \autoref{fig:zero_shot_comparison}).
    \item \textbf{TPP models}: We compare our model with state-of-the-art TPP models for asynchronous time series \citep{Xue2023EasyTPP}. We report the results for two popular RNN-based models: Recurrent marked temporal point process (RMTPP) \citep{duTPP} and neural Hawkes Process (NHP) \citep{MeiEisner}. We also compare with three attention-based models: self-attentive Hawkes process (SAHP) \citep{Zhang2020Self}, Transformer Hawkes process (THP) \citep{Zuo2020Transformer}, attentive neural Hawkes process (AttNHP) \citep{Yang2022Large}~ (\autoref{tab:main_results_pred_main_text}).
\end{itemize}
\vspace{-0.1in}
\paragraph{Results} Our results on the the three tasks (forecast, imputation, anomaly detection) and the three  text datasets (Breakfast, MultiTHUMOS, EPIC-KITCHENS) are presented in \autoref{tab:performance_evaluation_new}. Based on our results, we make five main observations. Firstly, {LASTS proves to be an effective and robust representation for asynchronous time series data across multiple datasets. LASTS Zero Shot consistently outperforms the Time Series Foundation Model Chronos and LLM-based methods (LLMTime and LLM Processes) in most evaluations, highlighting the advantage of using textual event descriptions enabled by LASTS.}  Secondly, our results demonstrate that the LASTS representation can be applied across multiple tasks without any investment needed in designing custom models for each task. Thirdly, LASTS work effectively with multiple LLM adaptation techniques without algorithm-specific modifications. Fourthly, we observe that StoP as an adaptation technique outperforms other techniques for most time prediction evaluations, and in all event type prediction evaluations. Finally, we highlight our results on the EPIC-KITCHENS dataset, which features very rich textual event descriptions (approximately 20,000). While traditional TPP modeling methods struggle to handle such a large set of classes, our approach effectively models various tasks on this complex dataset.

\begin{table}[htb]
    \centering
    % \caption{\textit{Performance evaluation on three textual datasets across forecasting, imputation, and anomaly detection tasks. Metrics are macro F1 (M-F1) and Mean Absolute Error (MAE) where applicable. The \textbf{best result} in each category is highlighted in bold, and the \underline{second-best result} is underlined. Note that the TPP based methods NHP, SAHP and AttNHP are only designed for forecasting. Note that for anomaly detection, since the task involves identifying only the anomalous event, the MAE metric is not applicable and Chronos and LLMProcesses are not adaptable (see  \ref{sec:llms_for_ts_baselines}). A $^*$ indicates our method. We use $5$ examples for few shot results (see \ref{sec:few-shot-analysis}).}}
    \caption{\textit{Performance evaluation on three textual datasets for forecasting, imputation, and anomaly detection. Metrics: Macro F1 (M-F1) and Mean Absolute Error (MAE) where applicable. \textbf{Best} results are in bold, \underline{second-best} are underlined. For anomaly detection, MAE is inapplicable, and Chronos/LLMProcesses are non-adaptable (see \ref{sec:llms_for_ts_baselines}). A $^*$ indicates our method. Few-shot results use five examples (see \ref{sec:few-shot-analysis}).}
}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cc|cc|cc}
        \toprule
        \multicolumn{7}{c}{\textbf{Forecasting}} \\
        \midrule
        \textbf{Model} 
        & \multicolumn{2}{c|}{\textbf{Breakfast}} 
        & \multicolumn{2}{c|}{\textbf{MultiTHUMOS}} 
        & \multicolumn{2}{c}{\textbf{EPIC-KITCHENS}} \\
        & M-F1 $\uparrow$ & MAE $\downarrow$ 
        & M-F1 $\uparrow$ & MAE $\downarrow$ 
        & M-F1 $\uparrow$ & MAE $\downarrow$ \\
        \midrule
        Random & 0.0162 & 40.1513 & 0.0417 & 1.8803 & 0.0000 & 3.2001 \\
        Chronos & 0.0011 & 43.0502 & 0.0265 & 1.9805 & 0.0000 & 3.5925 \\
        LLMTime & 0.0240 & 37.3902 & 0.1280 & 2.2060 & 0.0040 & 4.8948 \\
        LLMProcesses & 0.0337 & 44.9856 & 0.1278 & 2.0471 & 0.0049 & 4.3843 \\
        LASTS Zero Shot$^*$ & 0.0604 & 38.1630 & 0.1361 & 1.8868 & 0.0105 & 3.1566 \\
        LASTS Few Shot$^*$ & 0.1518 & 35.5605 & 0.1676 & 1.8114 & 0.0149 & 3.3092 \\
        LASTS + QLORA$^*$ & \underline{0.2558} & 33.9737 & 0.3218 & 1.7281 & 0.0764 & \underline{2.8964} \\
        LASTS + SP$^*$ & 0.2341 & \underline{32.8417} & \underline{0.3707} & \underline{1.6630} & \underline{0.0780} & \textbf{2.8830} \\
        LASTS + StoP$^*$ & \textbf{0.2633} & \textbf{32.5464} & \textbf{0.3947} & \textbf{1.6503} & \textbf{0.0797} & 3.0318 \\
        \midrule
        \multicolumn{7}{c}{\textbf{Imputation}} \\
        \midrule
        \textbf{Model} 
        & \multicolumn{2}{c|}{\textbf{Breakfast}} 
        & \multicolumn{2}{c|}{\textbf{MultiTHUMOS}} 
        & \multicolumn{2}{c}{\textbf{EPIC-KITCHENS}} \\
        & M-F1 $\uparrow$ & MAE $\downarrow$ 
        & M-F1 $\uparrow$ & MAE $\downarrow$ 
        & M-F1 $\uparrow$ & MAE $\downarrow$ \\
        \midrule
        Random & 0.0168 & 37.7029 & 0.0435 & 2.3622 & 0.0000 & 3.4269 \\
        Chronos & 0.0013 & 38.4039 & 0.0294 & 2.3971 & 0.0000 & 3.6955 \\
        LLMTime & 0.0137 & 35.9899 & 0.0968 & 2.6998 & 0.0005 & 3.6750 \\
        LLMProcesses & 0.0156 & 34.7117 & 0.1123 & 2.3786 & 0.0008 & 4.2600 \\
        LASTS Zero Shot$^*$ & 0.0263 & 33.0097 & 0.0915 & 2.6696 & 0.0015 & 3.6527 \\
        LASTS Few Shot$^*$ & 0.0520 & 33.3440 & 0.1013 & 2.3982 & 0.0023 & 3.2528 \\
        LASTS + QLORA$^*$ & 0.1688 & \underline{28.5638} & \underline{0.2132} & \textbf{2.2179} & 0.0378 & \underline{3.1194} \\
        LASTS + SP$^*$ & \underline{0.1581} & 28.8503 & 0.2044 & 2.4092 & \underline{0.0423} & 3.1456 \\
        LASTS + StoP$^*$ & \textbf{0.2064} & \textbf{28.2251} & \textbf{0.2213} & \underline{2.3445} &  \textbf{0.0610} & \textbf{3.1116} \\
        \midrule
        \multicolumn{7}{c}{\textbf{Anomaly Detection}} \\
        \midrule
        \textbf{Model} 
        & \multicolumn{2}{c|}{\textbf{Breakfast}} 
        & \multicolumn{2}{c|}{\textbf{MultiTHUMOS}} 
        & \multicolumn{2}{c}{\textbf{EPIC-KITCHENS}} \\
        & M-F1 $\uparrow$ & MAE $\downarrow$ 
        & M-F1 $\uparrow$ & MAE $\downarrow$ 
        & M-F1 $\uparrow$ & MAE $\downarrow$ \\
        \midrule
        Random & 0.0349 & \textemdash & 0.0381 & \textemdash & 0.0238 &  \textemdash \\
        LLMTime & 0.0240 &  \textemdash & 0.0415 &  \textemdash & 0.0048 &  \textemdash\\
        LASTS Zero Shot$^*$ & 0.0923 & \textemdash & 0.2755 &  \textemdash & 0.0159 &  \textemdash \\
        LASTS Few Shot$^*$ & 0.0837 &  \textemdash  & 0.3535 &  \textemdash & 0.0337 &  \textemdash \\
        LASTS + QLORA$^*$ & \underline{0.7011} &  \textemdash & \underline{0.6003} &  \textemdash & \underline{0.6520} &  \textemdash \\
        LASTS + SP$^*$ & 0.6520 &  \textemdash & 0.5231 &  \textemdash & 0.6159 &  \textemdash \\
        LASTS + StoP$^*$ & \textbf{0.7198} &  \textemdash & \textbf{0.6045} &  \textemdash & \textbf{0.6603} &  \textemdash \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:performance_evaluation_new}
\end{table}



% \subparagraph{Random Baseline} To evaluate our methods on the three text-based datasets and the three tasks, we establish a random baseline simulating random guesses. For forecasting and imputation, given an input asynchronous time series, the baseline predicts the inter-arrival time as the average of all inter-arrival times in the sequence and selects a random event type from the valid event descriptions. For anomaly detection, it randomly labels an event from the series as anomalous (see \autoref{tab:performance_comparison}).

% \subparagraph{LLMs for TS Baselines} We use two LLM-based time series forecasting methods, LLMTime \citep{Gruver2023Large} and LLM Processes \citep{requeima2024llm}, as baselines for asynchronous time series. Since LLM Processes cannot be easily adapted for anomaly detection, it is used only for forecasting and imputation. Our method performs comparably or better than these baselines. Details on baseline implementation are in Appendix \ref{sec:llmtime_baseline}.




% \begin{table*}[ht]
%     \caption{Performance of our models on three textual datasets for forecasting, imputation, and anomaly detection tasks. Metrics are macro F1 for event type prediction and MAE for event time prediction. The \textbf{best result} in each class is highlighted in bold, and the \underline{second-best result} is underlined. Note that for anomaly detection, since the task involves identifying only the anomalous event, the MAE metric is not applicable and Chronos and LLM Processes are not adaptable (see \ref{sec:llms_for_ts_baselines}). A $^*$ indicates our method. We use $5$ examples for few-shot results (see \ref{sec:few-shot-analysis}).}
%     \label{tab:performance_comparison_transposed}
%     \centering
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{c|l|ccccccccc}
%         \toprule
%         \textbf{Dataset} & \textbf{Metric} & \textbf{Random} & \textbf{Chronos} & \textbf{LLMTime} & \textbf{LLM Processes} & \textbf{LASTS Zero Shot$^*$} & \textbf{LASTS Few Shot$^*$} & \textbf{LASTS + QLORA$^*$} & \textbf{LASTS + SP$^*$} & \textbf{LASTS + StoP$^*$} \\
%         \midrule
%         \multirow{6}{*}{\rotatebox{90}{\textbf{Breakfast}}}
%         & M-F1 $\uparrow$ & 0.0162 & 0.0011 & 0.0240 & 0.0337 & 0.0604 & 0.1518 & \underline{0.2558} & 0.2341 & \textbf{0.2633} \\
%         & MAE $\downarrow$ & 40.1513 & 43.0502 & 37.3902 & 44.9856 & 38.1630 & 35.5605 & \underline{33.9737} & \underline{32.8417} & \textbf{32.5464} \\
%         & M-F1 (Imputation) $\uparrow$ & 0.0168 & 0.0013 & 0.0137 & 0.0156 & 0.0263 & 0.0520 & \underline{0.1688} & \underline{0.1581} & \textbf{0.2064} \\
%         & MAE (Imputation) $\downarrow$ & 37.7029 & 38.4039 & 35.9899 & 34.7117 & 33.0097 & 33.3440 & \underline{28.5638} & 28.8503 & \textbf{28.2251} \\
%         & M-F1 (Anomaly) $\uparrow$ & 0.0349 & \textemdash & 0.0240 & \textemdash & 0.0923 & 0.0837 & \underline{0.7011} & 0.6520 & \textbf{0.7198} \\
%         & MAE (Anomaly) $\downarrow$ & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash \\
%         \midrule
%         \multirow{6}{*}{\rotatebox{90}{\textbf{MultiTHUMOS}}}
%         & M-F1 $\uparrow$ & 0.0417 & 0.0265 & 0.1280 & 0.1278 & 0.1361 & 0.1676 & 0.3218 & \underline{0.3707} & \textbf{0.3947} \\
%         & MAE $\downarrow$ & 1.8803 & 1.9805 & 2.2060 & 2.0471 & 1.8868 & 1.8114 & \textbf{1.7281} & \underline{1.6630} & \textbf{1.6503} \\
%         & M-F1 (Imputation) $\uparrow$ & 0.0435 & 0.0294 & 0.0968 & 0.1123 & 0.0915 & 0.1013 & \underline{0.2132} & 0.2044 & \textbf{0.2213} \\
%         & MAE (Imputation) $\downarrow$ & 2.3622 & 2.3971 & 2.6998 & 2.3786 & 2.6696 & 2.3982 & \textbf{2.2179} & \underline{2.4092} & 2.3445 \\
%         & M-F1 (Anomaly) $\uparrow$ & 0.0381 & \textemdash & 0.0415 & \textemdash & 0.2755 & 0.3535 & \underline{0.6003} & 0.5231 & \textbf{0.6045} \\
%         & MAE (Anomaly) $\downarrow$ & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash \\
%         \midrule
%         \multirow{6}{*}{\rotatebox{90}{\textbf{EPIC-KITCHENS}}}
%         & M-F1 $\uparrow$ & 0.0000 & 0.0000 & 0.0040 & 0.0049 & 0.0105 & 0.0149 & 0.0764 & \underline{0.0780} & \textbf{0.0797} \\
%         & MAE $\downarrow$ & 3.2001 & 3.5925 & 4.8948 & 4.3843 & 3.1566 & 3.3092 & \underline{2.8964} & \textbf{2.8830} & 3.0318 \\
%         & M-F1 (Imputation) $\uparrow$ & 0.0000 & 0.0000 & 0.0005 & 0.0008 & 0.0015 & 0.0023 & \underline{0.0378} & \underline{0.0423} & \textbf{0.0610} \\
%         & MAE (Imputation) $\downarrow$ & 3.4269 & 3.6955 & 3.6750 & 4.2600 & 3.6527 & 3.2528 & \underline{3.1194} & 3.1456 & \textbf{3.1116} \\
%         & M-F1 (Anomaly) $\uparrow$ & 0.0238 & \textemdash & 0.0048 & \textemdash & 0.0159 & 0.0337 & \underline{0.6520} & 0.6159 & \textbf{0.6603} \\
%         & MAE (Anomaly) $\downarrow$ & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash & \textemdash \\
%         \bottomrule
%     \end{tabular}
%     }
% \end{table*}


% \begin{table*}[ht]
%     \caption{Performance of our models on three textual datasets for forecasting, imputation, and anomaly detection tasks. The \textbf{best result} in each class is highlighted in bold, and the \underline{second-best result} is underlined. Note that for anomaly detection, the MAE metric is not applicable.}
%     \label{tab:performance_comparison_split}
%     \centering
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{c|l|ccccccccc}
%         \toprule
%         \multicolumn{10}{c}{\textbf{(a) M-F1 Results}} \\
%         \toprule
%         \textbf{Dataset} & \textbf{Task} & \textbf{Random} & \textbf{Chronos} & \textbf{LLMTime} & \textbf{LLM Processes} & \textbf{LASTS Zero Shot$^*$} & \textbf{LASTS Few Shot$^*$} & \textbf{LASTS + QLORA$^*$} & \textbf{LASTS + SP$^*$} & \textbf{LASTS + StoP$^*$} \\
%         \midrule
%         \multirow{3}{*}{Breakfast}
%         & Forecast $\uparrow$ & 0.0162 & 0.0011 & 0.0240 & 0.0337 & 0.0604 & 0.1518 & \underline{0.2558} & 0.2341 & \textbf{0.2633} \\
%         & Imputation $\uparrow$ & 0.0168 & 0.0013 & 0.0137 & 0.0156 & 0.0263 & 0.0520 & \underline{0.1688} & \underline{0.1581} & \textbf{0.2064} \\
%         & Anomaly $\uparrow$ & 0.0349 & \textemdash & 0.0240 & \textemdash & 0.0923 & 0.0837 & \underline{0.7011} & 0.6520 & \textbf{0.7198} \\
%         \midrule
%         \multirow{3}{*}{MultiTHUMOS}
%         & Forecast $\uparrow$ & 0.0417 & 0.0265 & 0.1280 & 0.1278 & 0.1361 & 0.1676 & 0.3218 & \underline{0.3707} & \textbf{0.3947} \\
%         & Imputation $\uparrow$ & 0.0435 & 0.0294 & 0.0968 & 0.1123 & 0.0915 & 0.1013 & \underline{0.2132} & 0.2044 & \textbf{0.2213} \\
%         & Anomaly $\uparrow$ & 0.0381 & \textemdash & 0.0415 & \textemdash & 0.2755 & 0.3535 & \underline{0.6003} & 0.5231 & \textbf{0.6045} \\
%         \midrule
%         \multirow{3}{*}{EPIC-KITCHENS}
%         & Forecast $\uparrow$ & 0.0000 & 0.0000 & 0.0040 & 0.0049 & 0.0105 & 0.0149 & 0.0764 & \underline{0.0780} & \textbf{0.0797} \\
%         & Imputation $\uparrow$ & 0.0000 & 0.0000 & 0.0005 & 0.0008 & 0.0015 & 0.0023 & \underline{0.0378} & \underline{0.0423} & \textbf{0.0610} \\
%         & Anomaly $\uparrow$ & 0.0238 & \textemdash & 0.0048 & \textemdash & 0.0159 & 0.0337 & \underline{0.6520} & 0.6159 & \textbf{0.6603} \\
%         \bottomrule
%     \end{tabular}
%     }

%     \vspace{0.5cm}

%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{c|l|ccccccccc}
%         \toprule
%         \multicolumn{10}{c}{\textbf{(b) MAE Results}} \\
%         \toprule
%         \textbf{Dataset} & \textbf{Task} & \textbf{Random} & \textbf{Chronos} & \textbf{LLMTime} & \textbf{LLM Processes} & \textbf{LASTS Zero Shot$^*$} & \textbf{LASTS Few Shot$^*$} & \textbf{LASTS + QLORA$^*$} & \textbf{LASTS + SP$^*$} & \textbf{LASTS + StoP$^*$} \\
%         \midrule
%         \multirow{2}{*}{Breakfast}
%         & Forecast $\downarrow$ & 40.1513 & 43.0502 & 37.3902 & 44.9856 & 38.1630 & 35.5605 & \underline{33.9737} & \underline{32.8417} & \textbf{32.5464} \\
%         & Imputation $\downarrow$ & 37.7029 & 38.4039 & 35.9899 & 34.7117 & 33.0097 & 33.3440 & \underline{28.5638} & 28.8503 & \textbf{28.2251} \\
%         \midrule
%         \multirow{2}{*}{MultiTHUMOS}
%         & Forecast $\downarrow$ & 1.8803 & 1.9805 & 2.2060 & 2.0471 & 1.8868 & 1.8114 & \textbf{1.7281} & \underline{1.6630} & \textbf{1.6503} \\
%         & Imputation $\downarrow$ & 2.3622 & 2.3971 & 2.6998 & 2.3786 & 2.6696 & 2.3982 & \textbf{2.2179} & \underline{2.4092} & 2.3445 \\
%         \midrule
%         \multirow{2}{*}{EPIC-KITCHENS}
%         & Forecast $\downarrow$ & 3.2001 & 3.5925 & 4.8948 & 4.3843 & 3.1566 & 3.3092 & \underline{2.8964} & \textbf{2.8830} & 3.0318 \\
%         & Imputation $\downarrow$ & 3.4269 & 3.6955 & 3.6750 & 4.2600 & 3.6527 & 3.2528 & \underline{3.1194} & 3.1456 & \textbf{3.1116} \\
%         \bottomrule
%     \end{tabular}
%     }
% \end{table*}



\begin{table*}[h]
\caption{\textit{Performance of models on next-event's type and type prediction across five real datasets.  The \textbf{best result} is shown in bold, and the \underline{second best result} is underlined. OOM indicates an Out Of Memory error. A missing entry indicates the model diverged. We tried optimizing these baselines for the three textual datasets—MultiTHUMOS (65 classes), Breakfast (177 classes), and EPIC-KITCHENS ($\sim$ 20K classes)—but these models either diverged, performed poorly, or ran out of memory due to the large number of classes.}}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|cc|cc|cc|cc|cc|cc|cc|cc|}
\toprule
Model & \multicolumn{2}{c|}{Amazon} & \multicolumn{2}{c|}{Retweet} & \multicolumn{2}{c|}{Taxi} & \multicolumn{2}{c|}{Taobao} & \multicolumn{2}{c|}{StackOverflow} & \multicolumn{2}{c|}{Breakfast} & \multicolumn{2}{c|}{MultiTHUMOS} & \multicolumn{2}{c|}{EPIC-KITCHENS}\\
\cmidrule(lr){2-17}
& M-F1 $\uparrow$ & RMSE $\downarrow$ & M-F1 $\uparrow$ & RMSE $\downarrow$ & M-F1 $\uparrow$ & RMSE $\downarrow$ & M-F1 $\uparrow$ & RMSE $\downarrow$ & M-F1 $\uparrow$ & RMSE $\downarrow$ & M-F1 $\uparrow$ & RMSE $\downarrow$ & M-F1 $\uparrow$ & RMSE $\downarrow$ & M-F1 $\uparrow$ & RMSE $\downarrow$\\
\midrule
RMTPP    & 0.0988 & \underline{0.4780} & 0.3110 & 16.5849 & 0.2969 & 0.3761 & \underline{0.4495} & \underline{0.1338} & 0.0277 & 1.3727  & - & - & - & - & OOM & OOM \\	
NHP      & 0.1266 & \textbf{0.4489} & 0.4128 & \textbf{15.6233} & \underline{0.3667} & 0.3995 & 0.4287 & 0.1822 & 0.0559 & 1.3960   & 0.0167 & 116.23 & \underline{0.2861} & 4.8583 & OOM & OOM\\
SAHP     &  0.0846 & 0.5491 & 0.2772 & 16.6451 & 0.2780 & \textbf{0.3193} & 0.1816 & 0.1347 & 0.0322 & \underline{1.3326}   & 0.0023 &  112.85& 0.0 &  \underline{4.5908} & OOM & OOM\\
THP      &  \underline{0.1414} & 0.4911 & 0.2114 & 16.6440 & 0.3451 & 0.3736 & 0.2734 & 0.1340 & \underline{0.0661} & 1.4054   & - & - & - & - & OOM & OOM\\
AttNHP   &  0.1270 & 0.7054 & \underline{0.4210} & 16.8278 & 0.2167 & 0.4072 & 0.1048 & 0.1350 & 0.0475 & 1.3661  & \underline{0.0478} & \underline{108.41} & 0.0809 & 5.2113 & OOM & OOM \\
    \midrule
% \textbf{SP}       &  0.143 & 0.601 & \textbf{0.4331} & 16.4713 & 0.3204 & 0.3314 & \textbf{0.4652} & \textbf{0.1321} & 0.0943 & 1.2657 & & & & & & \\
\textbf{LASTS + StoP}      & \textbf{0.1520} & 0.6000 & \textbf{0.4299} & \underline{16.4981} &\textbf{0.4174} & \underline{0.3278} & \textbf{0.4633} & \textbf{0.1321} & \textbf{0.0983} & \textbf{1.2596} & \textbf{0.2633} & \textbf{102.02} & \textbf{0.3947} & \textbf{3.6722} & \textbf{0.0797} & \textbf{7.3724} \\
\bottomrule
\end{tabular}
}
\label{tab:main_results_pred_main_text}
\end{table*}
\vspace{-0.1in}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{figures/Baseline_Performance_Histograms_Bold_Spaced_Lasts_Zero_Shot.pdf}
    \caption{
        Macro-F1 $\uparrow$, MAE $\downarrow$, and Accuracy $\uparrow$, averaged across all datasets for Forecast and Imputation for Zero Shot methods.
    }
    \label{fig:zero_shot_comparison}
\end{figure}
\vspace{-0.1in}
\paragraph{Comparison with TPP models.} 
\autoref{tab:main_results_pred_main_text} shows experimental results that compare our model with existing TPP models on standard TPP datasets.
% We report the results for two popular RNN-based models: Recurrent marked temporal point process (RMTPP) \citep{duTPP} and neural Hawkes Process (NHP) \citep{MeiEisner}.
% We also compare with three attention-based models: self-attentive Hawkes process (SAHP) \citep{Zhang2020Self}, Transformer Hawkes process (THP) \citep{Zuo2020Transformer}, attentive neural Hawkes process (AttNHP) \citep{Yang2022Large}.
% To the best our knowledge, our model is the first LLM-based model for asynchronous time series so we cannot compare with other LLM-based models.
TPP models are designed for forecasting so we only show the results for the forecasting task.
% \todo{I assume our model is the best on Breakfast, MH, and EK. We will need to adapt the description if it is not true.} \shubham{It  is true. These methods dont give baseline for EPIC-KITCHENS due to large category space and on MH and BK our method is best.}
We observe that our model has competitive results w.r.t.~TPP models, 
outperforming existing TPP models on 13 of the 18 evaluations, and is in the top-2 best models on 17 of the 18 evaluations. 
Our model has the best performance for all the event type evaluations, which shows that our model is more accurate to predict the next event type. 
On three of the eight datasets, our model is less accurate than TPP models to predict the time.
We think that our model is not performing as well as the TPP models, because our model does not have an explicit prior about the time distribution whereas TPP models (e.g.~Poisson process or Hawkes process) make strong assumptions about the time distribution. {In the case of the Amazon dataset, the performance gap is more pronounced because this dataset groups a large number of diverse event types into a single event category, making it harder to model inter-arrival times.} 
These results show that our model is able to outperform existing TPP models on most of the datasets without explicit modeling of the time distribution.
% We think it may be possible to improve the performance of our model by adding a distribution prior in the prompt, and leave it as future work. 
It also shows that our model is performing well even when only the index of the event type is provided instead of its textual description, making it a more generally applicable method~(See Appendix \ref{sec:llms_for_ts_baselines}).
% \vspace{-0.2in}
\paragraph{Comparison with Zero Shot Methods} \autoref{fig:zero_shot_comparison} shows LASTS Zero Shot outperforms other zero shot techniques over all metrics when averaged over all tasks and datasets. See Appendix \ref{sec:llms_for_ts_baselines} for details.
\vspace{-0.1in}
\paragraph{Comparison with PEFT Techniques.} As detailed in Appendix \ref{app:stop_comparison_with_peft}, Stochastic Soft Prompting provides a significant advantage, achieving an average Macro-F1 improvement of $12.69\%$ over vanilla Soft Prompting and $13.55\%$ over QLoRA across all tasks and datasets.
% We compare our model with existing TPP models on standard TPPs datasets and the results are summarized in \autoref{tab:main_results_pred_main_text}.

\begin{figure}[h]
    \centering
    % First Row: t-SNE Projections
    \begin{minipage}[t]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/tsne_plots_for_paper/hsp.pdf}
    \end{minipage}
    \begin{minipage}[t]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/tsne_plots_for_paper/sp.pdf}
    \end{minipage}
    \begin{minipage}[t]{\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/tsne_plots_for_paper/cosine.pdf}

    \end{minipage}
    
\caption{Learned token representations of StoP and SP. The first two plots show t-SNE projections of 100 tokens from 400-length prompts (Breakfast dataset, forecasting)—StoP tokens are more dispersed, while SP tokens cluster closely. The third plot shows lower adjacent token cosine similarity for StoP (blue) than SP (red), indicating greater diversity.}


\label{fig:tsne}
\end{figure}
\vspace{-0.15in}
\subsection{Model analysis} 
% \vspace{-0.05in}
\paragraph{Comparison of SP and StoP learned token representations.} \label{sp_vs_stop}
% \paragraph{Understanding differences in token representations learned by SP and StoP.} 
% The tokens learned by Stochastic Soft Prompt (StoP) and Soft Prompt (SP) have distinct characteristics due to differences in their training paradigms. 
% To illustrate this difference, we plot the t-SNE projections of the first 100 tokens from a prompt of length 400 for both StoP and SP in \autoref{fig:tsne}. 
% We observe that the tokens learned through StoP training are more spread out, indicating greater diversity, while those learned through SP training tend to cluster more closely. 
% StoP uses a coarse-to-fine approach, where the first embeddings are more diverse to cover a large part of the space than the first embeddings trained with SP.
% This difference is further highlighted by the cosine similarity between adjacent tokens in the last plot of \autoref{fig:tsne}: the adjacent tokens in StoP prompts have lower similarity compared to SP. It allows StoP to work better than SP, even when only the first soft tokens are used (see \autoref{fig:vallid_porefixes}). Using more soft tokens further improves StoP, as it gains access to more fine-grained information.
Stochastic Soft Prompt (StoP) and Soft Prompt (SP) learn distinct token distributions due to differences in training. Figure~\ref{fig:tsne} shows t-SNE projections of the first 100 tokens from 400-length prompts. We observe that the tokens learned through StoP training are more spread out, indicating greater diversity, while those learned through SP training tend to cluster more closely. StoP follows a coarse-to-fine approach, with early embeddings that are diverse and cover a larger space. This difference is further highlighted in the last plot of Figure~\ref{fig:tsne}, where StoP tokens have lower adjacent cosine similarity than SP. As a result, StoP outperforms SP even when using only the first few tokens, with further improvements as more tokens are utilized (Figure~\ref{fig:vallid_porefixes}).


% Figure \ref{fig:tsne} shows that adjacent tokens in StoP prompts have lower similarity compared to SP. 
% The StoP training paradigm introduces not only stochasticity but also the selection of a prefix of tokens during training, forcing the tokens to spread apart, localizing learned information to specific positions, possibly making it easier for LLMs to retrieve it.
% \todo{Thibaut, Lilian - does this look ok? Would love animproved language here around whats happening.}
\vspace{-0.05in}
\paragraph{All prefixes are valid prompts in StoP} The training paradigm of StoP forces all prefixes of StoP to act as valid standalone prompts, as they are used as prompts during training for some batches (if trained for long enough). (see \autoref{fig:vallid_porefixes}). This further strengthens our belief that tokens in StoP are arranged from coarse, independent tokens at the beginning to tokens with tokens containing finer information towards the end. See Appendix \ref{sec:stop_analysis} for further discussion.

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pdf_plots/f1_comparison.pdf}
    \end{minipage}
    \hfill
    % \begin{minipage}[t]{0.31\columnwidth}
    %     \centering
    %     \includegraphics[width=\linewidth]{figures/pdf_plots/acc_comparison.pdf}
    % \end{minipage}\hfill
    \begin{minipage}[t]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pdf_plots/mae_comparison.pdf}
    \end{minipage}
    
    \caption{StoP-trained prefixes function as standalone prompts, unlike SP. Testing 400-length prompts (Breakfast, imputation) shows StoP prefixes remain effective, while SP prefixes do not.}
    \label{fig:vallid_porefixes}
\end{figure}

\vspace{-0.05in}
\paragraph{Disentangling Stochasticity and Prefix Picking in StoP.}  
To highlight the impact of structured prefix picking in StoP, we compare it with an alternative approach where, instead of selecting a prefix, we randomly select $l$ tokens from the prompt per batch, with $l$ drawn from a uniform distribution. We find that stochasticity alone is insufficient for learning effective soft prompts, and structured prefix picking plays a crucial role in StoP’s performance gains~(Appendix \ref{app:stop_disentangling}).

% \paragraph{Disentangling stochasticity and prefix picking in StoP.} 
% To further emphasize that prefix picking during the training regime of StoP is a key contributing factor to the performance improvement, we compare StoP with an alternative training paradigm where, instead of selecting a prefix, we randomly select $l$ tokens from the prompt during each batch, with $l$ drawn from a uniform distribution. 
% This comparison helps to distinguish the effects of introducing stochasticity alone from the structured prefix picking employed by StoP. \autoref{fig:random_tokens} shows a comparison of the macro F1 and MAE metrics on the validation data as both prompts are trained for 10 epochs.  These plots show that stochasticity alone is not sufficient for learning good soft prompts, and structured prefix picking is a key component of the StoP training.

% \begin{figure}[]
%     \centering
%     \begin{minipage}[t]{0.49\columnwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/comparison_plots/f1_comparison.pdf}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[t]{0.49\columnwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/comparison_plots/mae_comparison.pdf}
%     \end{minipage}
    
%     \caption{Comparison of Macro-F1 and MAE for StoP vs. random token selection during training, evaluated on validation data after 10 epochs. Results show that random token selection (shown in red) fails to learn effective prompts, while StoP's structured prefix selection (shown in blue) achieves significantly better performance.}
%     \label{fig:random_tokens}
% \end{figure}
\vspace{-0.07in}
\paragraph{Training speed.} 
Another dimension on which to compare SP and StoP is the training speed. 
Due to differences in training paradigms, StoP trains significantly faster than SP for the same prompt length, as many training batches use only a subset of the full prompt in StoP.
In our experiments with 400 soft prompts, we observed that StoP trains approximately \textbf{$25\%$} faster than SP.
\vspace{-0.07in}
\paragraph{Understanding StoP prompts through probing.} 
% \cite{Lester2021Power} attempts to interpret learned prompts by mapping them to the closest input embeddings, but this often yields incoherent results.  Instead, we treat prompt interpretability as a probing task for the LLM itself, appending the learned prompt with a simple instruction: \textit{"Tell me in as much detail as possible what task you are supposed to do."} This approach allows the LLM to articulate its interpretation directly, providing a more human-understandable explanation of the learned task. For example, this interpretation of a prompt learned on forecasting for breakfast dataset, indicates the high level dataset and task information is learned by the prompt:
While prior work such as \cite{Lester2021Power} attempts to interpret learned prompts by mapping them to the closest input embeddings—often yielding incoherent results—we instead explore probing the LLM using the learned prompt. By appending the learned prompt with a simple instruction, such as \textit{``Tell me in as much detail as possible what task you are supposed to do,''} we encourage the LLM to generate an output that reflects its understanding of the task. This approach allows us to gain some insight into what the model has summarized from the tasks and datasets it has been trained on. We present multiple model responses when probed like this in Appendix \ref{sec:prompt_interpretations_word_embeddings}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/Average_MacroF1_MAE_Histograms_Model_sizes.pdf}
    \caption{
        Macro-F1 $\uparrow$ and MAE $\downarrow$ across all datasets and tasks for different model sizes.
    }
    \label{fig:avg_macro_f1_mae_by_model_size_paper}
\end{figure}
\vspace{-0.2in}
\paragraph{Scaling Laws.}  
We evaluate Stochastic Soft Prompts (StoP) across different LLM backbone sizes (1B, 3B, and 8B) and observe consistent performance gains with larger models, indicating that StoP benefits from improvements in the underlying LLMs and is expected to scale accordingly~ (\autoref{fig:avg_macro_f1_mae_by_model_size_paper}, Appendix \ref{app:scaling_laws}). 



% \vspace{0.1cm}
% \begin{tcolorbox}[colback=blue!10, colframe=black!80]
% The task appears to be related to preparing a snack or meal by using the facilities provided in
% the kitchen. Specifically, I was asked to perform a series of actions to accomplish this goal.
% Here’s what I’ve observed:1. **Opening the cabinet**: I need to open the cabinet to access
% the necessary utensils, cookware, or ingredients.2. **Grabbing the ingredients**: I will
% need to pick up the specified ingredients, such as a bowl, a cutting board, or a can opener, to
% prepare the desired dish.3. **Slicing the ingredients**: I will cut the ingredients into the re-
% quired proportions, shapes, or sizes depending on the recipe being followed.4. **Preheating
% the cooking surface**: If needed, I will turn on the stove, oven, or other cooking device to
% the desired temperature to start cooking the ingredients.5. **Cooking the ingredients**: I
% will proceed to cook the ingredients according to the required cooking method, such as boil-
% ing, saut´eing, frying, or baking.6. **Adding condiments or seasonings**: If necessary, I
% will add the required condiments, spices, or seasonings to the cooked ingredients to enhance
% their flavor.7. **Combining the ingredients**: Finally, I will combine the cooked ingredi-
% ents in the desired manner, whether it’s mixing, blending, or assembling the final dish.Please
% let me know if I’m correct in my understanding or if there’s anything else I can do to help!
% \end{tcolorbox}
% We present additional interpretability examples, is provided in Appendix \ref{sec:prompt_interpretations_word_embeddings}.



% \begin{figure}[htbp]
%     \centering
%     \begin{minipage}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/tsne_plots_for_paper/hsp.pdf}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/tsne_plots_for_paper/sp.pdf}
%     \end{minipage}
%     \caption{\textit{Illustration of the tokens learned via Stochastic Soft Prompt training versus Soft Prompt training. The t-SNE projections of the first 100 tokens from a 400-length prompt are plotted for each case. Tokens learned through Stochastic Soft Prompt training are spread widely, indicating greater diversity, whereas tokens learned through traditional Soft Prompt training tend to cluster closely together.}}
%     \label{fig:tsne}

% \end{figure}



% \begin{figure}[htbp]
%     \centering
%     % First Plot: t-SNE Projection for Stochastic Soft Prompt
%     \begin{minipage}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/tsne_plots_for_paper/hsp.pdf}
%     \end{minipage}
%     \hfill
%     % Second Plot: t-SNE Projection for Soft Prompt
%     \begin{minipage}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/tsne_plots_for_paper/sp.pdf}
%     \end{minipage}
%     \hfill
%     % Third Plot: Cosine Similarity for SP and StoP
%     \begin{minipage}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/tsne_plots_for_paper/cosine.pdf}

%     \end{minipage}
    
%     \caption{\textit{\textbf{Comparison between tokens learned via Stochastic Soft Prompt (StoP) training and Soft Prompt (SP) training}. The first plot shows the t-SNE projection of the first 100 tokens from a 400-length prompt for StoP training, indicating greater dispersion and diversity. The second plot shows the t-SNE projection for SP training, where tokens are more closely clustered. The third plot illustrates cosine similarity between adjacent tokens for SP and StoP training across multiple learned prompts and datasets, showing that StoP yields lower cosine similarity, indicating greater diversity in learned representations. }}
%     \label{fig:tsne}
% \end{figure}


% \subsection{Training Setup} We train our soft prompts using gradient descent on valid instructions to the LLM backbones and their intended correct responses. Following \cite{Xue2023EasyTPP}, we split our datasets into a train/val/test split of 70/10/20, and train our SP and SSP prompts for same number of epochs for a fair comparison. We employ early stopping on the macro F1 metric on validation split and report the performance on the test split. As a training objective, we perform cross entropy not just on the intended output desired from the the LLM backbone, but also the portion of the time series provided as an input to the LLM in the user prompt portion. We found that including the series as a part of the training objective significantly improves model performance. We suspect that including the series as a part of the training objective enables the prompts to learn underlying latent representations for the composition of the series themselves, thereby helping the model do much better at the intended tasks. \shubham{Include ablation here. Is there a quick experiment we can do that shows not just the performance change but also some sense of series understanding by the model with and without this cross entorpy on series portion?}

% \subsection{Prompt Length} The right token length to use for a dataset-task pair is a hyperparameter that can be tuned. due to constrains on computational resources, all our results reported here, unless otherwise stated, are obtained using prompt lengths of $400$. For Llama3, which has hidden dimension of $4096$, this amounts to around $1.6M$  trainable parameters. We emphasize, however, that some tasks like imputation, are harder than tasks like forecasting, as the later falls neatly in the next token prediction paradigm our LLM backbone is trained on, but imputation requires a deeper understanding of series and remembering the location of the missing element, and its left and right contexts to make a valid prediction. Hence, while a token length of $400$ may be needed for imputation, it may be a overkill for forecasting.\shubham{Is it controversial to mention this? :( Reviewers may say that a larger token length is not necessarily good, and may cause problems in learning... should i skip this?} We explore an ablation on token length for forecasting on the breakfast dataset further in appendix. \shubham{Provide an ablation in appendix on token length}
% \shubham{RESOLVE WHAT METRIC TO USE - MAE OR RMSE}

% \subsection{Datasets} We work with two different families of datasets, one with the event labels in natural language (Breakfast, EPIC-KITCHENS-100 and MultiTHUMOS (see Section \ref{textualDataset}) and the other with non-textual event labels (

% \subsubsection{Non Textual Datasets}
% The first set of datasets are the ones benchmarked in \cite{Xue2023EasyTPP} for forecasting on temporal point processes. These are $5$ real-world datasets from popular works that contain diverse characteristics in terms of their application domains and temporal statistics. These datasets do not have a textual component to them. The events here belong to a relatively small number of classes and the forecasting task is to predict the next event type and its inter arrival time given the preceding elements of the series. These datasets are: \shubham{Copied from EasyTPP, needs rephrasing}
% \begin{itemize}

%       \item {\textbf{Amazon}} \citep{amazon-2018}. This dataset includes time-stamped user product reviews behavior from January, 2008 to October, 2018. 
% Each user has a sequence of produce review events with each event containing the timestamp and category of the reviewed product, with each category corresponding to an event type. We work on a subset of $5200$ most active users with an average sequence length of $70$ and then end up with $K=16$ event types. 

%     \item \textbf{Retweet} 
%     \textnormal{\citep{zhou-2013}}. This dataset contains time-stamped user retweet event sequences.  The events are categorized into $K = 3$ types: retweets by “small,” “medium” and “large” users. Small users have fewer than $120$ followers, medium users have fewer than $1363$, and the rest are large users. We work on a subset of $5200$ active users with an average sequence length of $70$.

%     \item \textbf{Taxi} \textnormal{\citep{whong-14-taxi}}. This dataset tracks the time-stamped taxi pick-up and drop-off events across the five boroughs of the New York City; each (borough, pick-up or drop-off) combination defines an event type, so there are $K=10$ event types in total. We work on a randomly sampled subset of $2000$ drivers with an average sequence length of $39$.
% %average length is $39$. 


%     \item \textbf{Taobao} \citep{xue2022hypro}. This dataset contains time-stamped user click behaviors on Taobao shopping pages from November 25 to December 03, 2017. Each user has a sequence of item click events with each event containing the timestamp and the category of the item. The categories of all items are first ranked by frequencies and the top $19$ are kept while the rest are merged into one category, with each category corresponding to an event type. We work on a subset of $4800$ most active users with an average sequence length of $150$ and then end up with $K=20$ event types. 


%     \item {\textbf{StackOverflow} \textnormal{\citep{snapnets}}.} This dataset has two years of user awards on a question-answering website: each user received a sequence of badges and there are $K=22$ different kinds of badges in total.  We work on a subset of $2200$ active users with an average sequence length of $65$.
% \end{itemize}

% We use the codebase provided by \cite{Xue2023EasyTPP} to reproduce their results on forecasting for these datasets and compare them with  forecasting results obtained via SP and $SSP$ in \ref{tab:main_results_pred_main_text}

% \subsubsection{Textual Datasets} \label{textualDataset}
% In addition, we work with $3$ textual datasets. These datasets are more rooted in natural language, and describe events happening in common place scenarios like cooking and games in a timestamped but free flowing natural language format. These are more challenging datasets from the previous section, as they are intended to be english based datasets. When viewed as action classes, the number of actions in these datasets can be order of magnitude more than those in the previous section. 
% \begin{itemize}
% \item \textbf{Breakfast} \citep{Kuehne2014Language} contains \todo{1712} videos with \todo{48} action classes related to breakfast preparation. 
% Each video has a sequence of events to prepare breakfast, with each event containing the timestamp and the action.
% \todo{Describe the data preparation}

% \item \textbf{EPIC-KITCHENS-100} \citep{Damen2022Rescaling} is a large-scale dataset in first-person (egocentric) vision capturing daily activities in the kitchen over multiple days.
% It contains more complex activity than Breakfast dataset. 
% \todo{Describe the data preparation}

% \item \textbf{MultiTHUMOS} \citep{Yeung2018Every} contains 400 videos with 65 action classes related to  human activities.
% Each video has a sequence of human activity events, with each event containing the timestamp and the activity.
% \todo{Describe the data preparation}

% \end{itemize}

% For these datasets, we obtain forecasting baselines using the code provided by \cite{Xue2023EasyTPP} and compare them to our methods. Note that as these datasets have a much larger number of classes, the algorithms in the benchmark had to be scaled up significantly to yield meaningful results, and even then we were unable to close a big gap in performance for breakfast dataset and we weren't able to run any of the benchmark methods for EPIC-KITCHENS dataset due to the massive number of classes. These results are in table \ref{tab:compare_baselines}

% We also produce imputation and anomaly detection results for these datasets and compare them against a random baseline, a zero shot and a few shot baseline and finally a QLORA baseline. These results are in Table \ref{tab:performance_comparison} \shubham{Should i also be doing imputation and anomaly detection for the other 5 datasets? Seems weird why we do not do that :( }
% \subsection{Dataset Preparation}

% We throw away any sequence in the dataset that is very small ($< 4$ elements). We split the dataset in a random $70/10/20$ train, validation and test split. Each sequence is then expanded into multiple sequences based on the task:
% \begin{itemize}
%     \item \textbf{Forecasting}: We convert a sequence into multiple prediction tasks - for each element of the series, the prediction task is to predict the element given the preceding elements. We impose a minimum and maximum length requirements on the number of preceding elements used.
%     \item \textbf{Imputation}: For every element in the series, we replace the element by a mask, and the imputation task is to predict the masked element given the remaining sequence.
%     \item \textbf{Anomaly Detection}: For every element in the sequence, we replace the action by a random different action. the anomaly detection task is to identify the element of the sequence that has been tampered with.
% \end{itemize}










% \section{Benchmarking} Our benchmarking process is based on EasyTPP \citep{Xue2023EasyTPP}. The algorithms used in this paper for benchmarking purposes are \lilian{TODO: list to be updated + briefly describe each of them}

% \paragraph{Metrics.} \lilian{TODO: add any justification for MacroF1?}
% To evaluate the event type/attributes, we report the accuracy, the balanced accuracy (i.e., average of the recall from each class), and the Macro F1 ("M-F1", average of the F1 scores from each class). Recall that F-score is the harmonic mean of precision and recall. When expanded, it can be expressed as
% \begin{equation}
%  0 \leq F_1:=\ds \frac{2\cdot \text{TP}}{2 \text{TP} + \text{FP}+ \text{FN}} \leq 1.
% \label{F1Def}
% \end{equation} 
% The F1 score is a better choice \lilian{Q: better over ??? } for imbalanced data because it strikes a balance between precision and recall.


% To evaluate the time, we report the mean absolute error (MAE) or the root mean square error (RMSE). Compared to RMSE, MAE is less susceptible to outliers.

% \lilian{TODO: discuss in relation of imbalance problem}

% \lilian{TODO: mention this in DATA section.
% Much larger classes in our data: 180 breakfast, 70 multihumos, 1800 EPIC-KITCHENS}

% We combine the metrics used in \citep{Xue2023EasyTPP}
% We report accuracy of action category prediction and mean absolute error (MAE) of
% inter-arrival time prediction.
% \begin{itemize}
% \item MAE
% \item Macro F1
% \item balanced accuracy
% \item accuracy
% \end{itemize}


% Backbone:
% \begin{itemize}
% \item Llama3 7B
% \item Mistral 7B
% \end{itemize}


% Tasks
% \begin{itemize}
% \item Forecasting
% \item Data imputation
% \item Anomaly detection
% \end{itemize}


% Settings
% \begin{itemize}
% \item Zero-shot learning
% \begin{enumerate}
%     \item Sensitivity to prompt used
%     \item using durations instead of inter arrival time for breakfast
%     \item using non english names to show impact of english world knowledge
%     \item reversing the order time and action in the tuple
% \end{enumerate}
% \item Few-shot learning
% \item Adaptation/Fine-tuning
% \end{itemize}

% \section{Discussion}
% \subsection{SSP as a regularization technique}  \todo{complete the experiments here}. We demonstrate that SSP acts a s aregularizer and we compare SSP as a regularizer with other regularization techniques like dropout and weight decay and show that SSP is a much more robust regulariation technique. \todo{Do these experiments }

% \subsection{Interpretatibility of Soft Prompts} We demonstrate that its possible to interpret the soft prompts by asking the LLM backbone to make sense of the learned soft prompt. We compare this to the more well accepted methodology of mapping each token in soft promopt to an english token and trying to make sense of the obtained english tokens. \todo{Do this comparison, examples of interpretability}

% \subsection{Sampling Distribution} Depending on what distribution  is used to sample teh prefix lengths during training, can significantly imapact the model convergence speed and performance. Here we explore a few distributions \todo{Do this!!}

% \subsection{Speed} We demonstrate here that SSP is much faster than SP and results in a training speedup of $25\%$

% \subsection{Adaptive Learning - Curriculum learning} We demonstarte that we can adaptively add more tokens to the prompts learned via SSP, if needed to give more capacity to the model without significanlty impacting model perofrmance 

% \subsection{SSP prefixes are valid prefixes} We demonsrate that the prefixes of the trained SSP are valid prompts themselves, while this property is absent in SP. 