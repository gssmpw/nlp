% \vspace{-0.5em}
\section{Related Work}
\label{sec:related}

\noindent\textbf{Approximate Nearest Neighbor Search.} To address the ANNS problem, various methods~\cite{LSHDatarIIM04,liuSKLSHEfficientIndex2014, DBLP:journals/pami/GeHK014, jegouProductQuantizationNearest2011, gongIterativeQuantizationProcrustean2013, ite_matsui_2018, fu2019fast, malkovEfficientRobustApproximate2020, wang2021comprehensive} have been proposed, which can be classified into four categories: tree-based methods~\cite{arora2018hdindex, BeygelzimerKL06Covertree, RamS19kdtree}, hashing-based methods~\cite{LSHDatarIIM04, wangSurveyLearningHash2018, hash2014survey, DBLP:journals/pami/HeoLHCY15, liuSKLSHEfficientIndex2014, DBLP:journals/vldb/ZhengZWNLJ22,DBLP:journals/tods/TaoYSK10, DBLP:journals/tkde/TianZZ24, DBLP:journals/pvldb/LuWWK20, DBLP:journals/pvldb/HuangFZFN15,DBLP:conf/compgeom/DatarIIM04,DBLP:conf/mm/TuncelFR02,DBLP:conf/sigmod/GanFFN12,DBLP:conf/sigmod/LeiHKT20, DBLP:conf/sigmod/LiYZXCLNC18, DBLP:journals/pvldb/GongWOX20}, quantization-based methods~\cite{jegouProductQuantizationNearest2011, gongIterativeQuantizationProcrustean2013, DBLP:journals/pami/GeHK014,ite_matsui_2018,gao2024rabitq}, and graph-based methods~\cite{wang2021comprehensive,fu2019fast,liApproximateNearestNeighbor2020,DBLP:journals/pami/FuWC22,DBLP:journals/is/MalkovPLK14,DBLP:conf/cvpr/HarwoodD16,malkovEfficientRobustApproximate2020,harwood2016fanng,chen2018sptag,iwasaki2015neighborhood,DBLP:journals/pami/FuWC22}. According to experimental evaluations~\cite{wang2021comprehensive,liApproximateNearestNeighbor2020}, graph-based methods demonstrate superior performance compared with other methods. %for in-memory ANNS algorithms. 
This is because 
%non-graph-based 
other methods typically attempt to partition vectors into buckets and route queries to a small number of close buckets for fast retrieval, which is challenging in high-dimensional space~\cite{curse1,curse2}. 
%For example, Tree-based methods like kd-tree~\cite{RamS19kdtree} recursively partition the data by choosing one dimension to divide the data at each step. It performs well in low-dimensional spaces but poorly in high-dimensional spaces due to the curse of dimensionality~\cite{curse1,curse2}. 
%For example, Hashing-based methods like LSH~\cite{LSHDatarIIM04} map high-dimensional vectors to hash buckets but only provide a loose theoretical guarantee that closer vectors are more likely to fall into the same bucket, which results in poor empirical performance, as reported~\cite{liApproximateNearestNeighbor2020}. Quantization-based methods such as PQ~\cite{jegouProductQuantizationNearest2011} decompose the original vector space into $M$ subspaces and perform vector quantization~\cite{gray1998quantization} in each subspace separately. However, due to the high quantization error, it fails to match the performance of graph-based ANNS. 
To the best of our knowledge, none of these ANNS techniques have been %specifically 
designed 
%or adapted 
for the \hvq problem.

%Quantization-based methods such as PQ~\cite{jegouProductQuantizationNearest2011} split vectors into sub-vectors, perform clustering in each sub-vector space and assign each sub-vector to the nearest centroid. The centroids form a quantization codebook, with each data vector represented by the ID of the assigned centroids. During the query phase, the query computes distances only with the codebook and efficiently estimates the distance for each data vector by looking up the codebook's distance. However, due to the high quantization error, it fails to match the performance of graph-based ANNS. To the best of our knowledge, none of these techniques have been adapted for the \hvq problem.

\noindent\textbf{Graph-based ANNS Indexes.} 
%Recently, graph-based ANNS indexes have emerged as state-of-the-art ANNS algorithms~\cite{liApproximateNearestNeighbor2020,dobson2023scaling}. 
Most existing graph-based indexes~\cite{wang2021comprehensive,fu2019fast,liApproximateNearestNeighbor2020,DBLP:journals/pami/FuWC22,DBLP:journals/is/MalkovPLK14,DBLP:conf/cvpr/HarwoodD16,malkovEfficientRobustApproximate2020,harwood2016fanng,chen2018sptag,iwasaki2015neighborhood,DBLP:journals/pami/FuWC22} are derived from four fundamental types of graphs: Delaunay Graph (DG)~\cite{dgraph}, Relative Neighborhood Graph (RNG)~\cite{rng}, K-Nearest Neighbor Graph (KNNG)~\cite{knngraph}, and Minimum Spanning Tree (MST)~\cite{mst}. 
%We refer readers to the recent survey and experimental evaluation~\cite{wang2021comprehensive} for details. 
According to the experimental evaluation~\cite{wang2021comprehensive}, RNG-based ANNS indexes deliver state-of-the-art performance 
%compared to others 
due to its pruning strategy. 
%However, none of these %graphs or the 
%graph-based ANNS indexes %built upon them 
%have been specifically adapted to 
%are designed for the \hvq problem.

%(e.g., HNSW~[xx] and NSG~[xx]) 
% are the state-of-the-art ANNS indexes. Then we give a formal description of the RNG graph.

% \noindent\textbf{Multi-Vector Search.} 
\noindent\textbf{Variants of ANNS Query.} To address more complex real-world scenarios, several variants of the ANNS query have been proposed~\cite{pan2024vector}. These variants of ANNS queries have been identified as a promising future research direction in recent vector database studies~\cite{milvus2021,guo2022manu}. For instance, \cite{wei2020analyticdbv} \revision{has introduced a %\annotation{R2O2}
hybrid ANNS query} with attribute filtering to retrieve similar vectors that satisfy boolean predicates over their attributes. \cite{milvus2021} introduces the multi-vector query, from which we define HVQ, and develops two solutions (as detailed in Section~\ref{sec:preliminary-motivations}). % To the best of our knowledge, very few studies have attempted to tackle the multi-vector query. 
%\cite{milvus2021}~propose two naive solutions based on existing ANNS indexes (as detailed in Section~\ref{sec:preliminary-motivations}). 
However, both of these solutions have limitations in handling queries with different $\alpha$ (as shown in Section~\ref{sec:preliminary-motivations}). 
To the best of our knowledge, none of the existing studies have identified or attempted to address these limitations, and answering multi-vector query is an open problem. 






%We are also aware that when one vector's dimensionality is low and the other is high (e.g., $m=2, d=768$), the \hvq problem is equivalent to the semantic-aware spatial keyword query problem. Some recent studies~\cite{chenS2RtreePivotbasedIndexing2020,qianSemanticawareTopkSpatial2018,DBLP:conf/edbt/TheodoropoulosN24} aim to solve this problem either exactly or with very high accuracy (e.g., recall = 0.99). However, due to the curse of dimensionality, these methods have time costs similar to brute-force search. Due to the page limit, we leave the details of these methods in the appendix.

\noindent\revision{\textbf{Hybrid Search.} HVQ is 
a variant of hybrid search, which has been researched extensively. Existing studies %often consider that each object has $m$ attributes, such as price, and 
aim to efficiently identify  top-$k$ objects ranked by monotone aggregation functions. %such as attribute sums. %This problem is also known as the multi-metric search problem. 
%\annotation{R2O1, R2O2}
The Threshold Algorithm (TA) and the No Random Access (NRA) algorithm~\cite{fagin2001optimal} use pre-sorted lists for each attribute, but they face challenges with multi-dimensional data, %, where attributes are represented as vectors and ranking involves vector distance sums. 
as pre-computing distances and generating sorted lists is computationally expensive. To overcome this limitation, RR$^*$-tree~\cite{franzke2016indexing} uses reference objects to generate low-dimensional embeddings for each data object, which are then indexed by R-tree for efficient query processing.
%selects reference objects for each metric and generates low-dimensional embeddings for each data object based on distances to these reference objects. The generated embeddings are then indexed by R-tree to support efficient query processing. 
However, these approaches are limited to low-dimensional spaces and degrade to no better than a linear scan in high-dimensional settings. 
%due to the curse of dimensionality~\cite{curse1,curse2}.
}

\revision{There exist some attempts that investigate the integration of existing hybrid search techniques with hybrid vector search. Specifically, \cite{budikova2013towards} develops a distributed system based on the \textsf{Fusion} baseline, employing a modified TA algorithm to filter vectors fetched from distributed machines. %Similarly, 
\cite{milvus2021} uses the NRA algorithm%\annotation{R2O1} 
to determine when the \textsf{Fusion} baseline should stop increasing $k'$ in order to obtain exact top-$k$ results. However, the experimental results~\cite{milvus2021} show that integrating  NRA  with the \textsf{Fusion} baseline has extremely high computational cost, %In scenarios where approximate results are sufficient, 
%According to experimental findings~\cite{milvus2021}, 
and iteratively increasing the $k'$ in the \textsf{Fusion} baseline is the most effective strategy.%the main challenge is determining when to stop the NRA algorithm early to reduce computational expenses.
}


\revision{%\annotation{R1O1}
There also exists recent work on exploring hybrid vector search for special cases, such as semantic-aware spatial keyword queries~\cite{chenS2RtreePivotbasedIndexing2020,qianSemanticawareTopkSpatial2018, DBLP:conf/edbt/TheodoropoulosN24}, where one attribute is a geographical coordinate and the other is a high-dimensional embedding. 
%Some studies have explored hybrid search on high-dimensional data, such as the semantic-aware spatial keyword query~\cite{chenS2RtreePivotbasedIndexing2020,qianSemanticawareTopkSpatial2018, DBLP:conf/edbt/TheodoropoulosN24}, which 
It takes geo-location and %high-dimensional 
embedding as inputs to find top-$k$ objects based on a weighted sum of geographical and embedding distances. However, these methods often focus on exact top-$k$ results, and it is still an open problem to design efficient querying algorithms. 
%leading to high computational costs due to the curse of dimensionality~\cite{curse1,curse2}. 
Details on these methods are provided in the appendix~\footnote{\url{https://github.com/Heisenberg-Yin/DEG/blob/main/SIGMOD2024_DEG_ready.pdf}} due to the page limitation.}



%\noindent\revision{\textbf{Multi-metric Indexing.} 
% We are aware that our research problem is relevant to the multi-metric indexing and search problem~[xx], which also involves searching over multiple metric spaces. For example, %M$^2$-tree~[xx] employs a specific promotion algorithm to select representative objects as internal nodes. For each internal node, the radii and the distance between entry feature values and parent node feature values are stored to facilitate efficient pruning of sub-trees. 
