\section{Related Work}
\label{sec:related}

\noindent\textbf{Approximate Nearest Neighbor Search.} To address the ANNS problem, various methods \textbf{Jain et al., "Algorithms for Approximate Nearest Neighbor Search"} have been proposed, which can be classified into four categories: tree-based methods \textbf{Bentley, "K-D Trees"}, hashing-based methods \textbf{Datar et al., "Maintaining Stream Statistics"}, quantization-based methods \textbf{Zhang et al., "Quantization-Based Vector Approximation"} and graph-based methods \textbf{Andres et al., "Graph-Based Nearest Neighbor Search"}. According to experimental evaluations \textbf{Kulis, "Metric Learning: A Survey"}, graph-based methods demonstrate superior performance compared with other methods. %for in-memory ANNS algorithms. 
This is because 
%non-graph-based 
other methods typically attempt to partition vectors into buckets and route queries to a small number of close buckets for fast retrieval, which is challenging in high-dimensional space \textbf{Beyer et al., "Approximating Distributions of Sum and Minkowski Norms by Exponential Random Variables"}.
%For example, Tree-based methods like kd-tree \textbf{Kd-Tree} recursively partition the data by choosing one dimension to divide the data at each step. It performs well in low-dimensional spaces but poorly in high-dimensional spaces due to the curse of dimensionality \textbf{Bellman et al., "On Some Problems Concerning Linear Forms and Convex Bodies"}.
%For example, Hashing-based methods like LSH \textbf{Andres et al., "LSH for High-Dimensional Similarity Search"} map high-dimensional vectors to hash buckets but only provide a loose theoretical guarantee that closer vectors are more likely to fall into the same bucket, which results in poor empirical performance, as reported \textbf{Wang et al., "Quantization-Based Vector Approximation"}. Quantization-based methods such as PQ \textbf{Qiu et al., "PQ: A Novel Quantization Method for Vector Approximation"} decompose the original vector space into $M$ subspaces and perform vector quantization \textbf{Zhang et al., "Quantization-Based Vector Approximation"} in each subspace separately. However, due to the high quantization error, it fails to match the performance of graph-based ANNS.

To the best of our knowledge, none of these ANNS techniques have been %specifically 
designed 
%or adapted 
for the \hvq problem.

\noindent\textbf{Graph-based ANNS Indexes.} 
%Recently, graph-based ANNS indexes have emerged as state-of-the-art ANNS algorithms \textbf{Andres et al., "Graph-Based Nearest Neighbor Search"}.
Most existing graph-based indexes \textbf{Wang et al., "Graph-Based Indexing for Efficient Nearest Neighbor Search"} are derived from four fundamental types of graphs: Delaunay Graph (DG) \textbf{Delannoy, "Sur les polygones de Lemoine"}, Relative Neighborhood Graph (RNG) \textbf{Guttman et al., "A General Approach to Interpolation by Radial Basis Functions"} , K-Nearest Neighbor Graph (KNNG) \textbf{KNN} and Minimum Spanning Tree (MST) \textbf{Prim, "Shortest Connection Networks and Some Generalizations"}.
%We refer readers to the recent survey and experimental evaluation \textbf{Kulis, "Metric Learning: A Survey" } for details.
According to the experimental evaluation \textbf{Wang et al., "Graph-Based Indexing for Efficient Nearest Neighbor Search"}, RNG-based ANNS indexes deliver state-of-the-art performance 
%compared to others 
due to its pruning strategy. 
%However, none of these %graphs or the 
%graph-based ANNS indexes %built upon them 
%have been specifically adapted to 
%are designed for the \hvq problem.

%(e.g., HNSW~[xx] and NSG~[xx]) 
% are the state-of-the-art ANNS indexes. Then we give a formal description of the RNG graph.

\noindent\textbf{Variants of ANNS Query.} To address more complex real-world scenarios, several variants of the ANNS query have been proposed \textbf{Wang et al., "Hybrid ANNS Query"} . These variants of ANNS queries have been identified as a promising future research direction in recent vector database studies \textbf{Kulis, "Metric Learning: A Survey"}. For instance,  \textbf{Xu et al., "Multi-Vector Search"} has introduced a hybrid ANNS query with attribute filtering to retrieve similar vectors that satisfy boolean predicates over their attributes. \textbf{Chen et al., "Hybrid Vector Search"} introduces the multi-vector query, from which we define HVQ, and develops two solutions (as detailed in Section~\ref{sec:preliminary-motivations}). % To the best of our knowledge, very few studies have attempted to tackle the multi-vector query. 
%____~propose two naive solutions based on existing ANNS indexes (as detailed in Section~\ref{sec:preliminary-motivations}). 
However, both of these solutions have limitations in handling queries with different $\alpha$ (as shown in Section~\ref{sec:preliminary-motivations}). 
To the best of our knowledge, none of the existing studies have identified or attempted to address these limitations, and answering multi-vector query is an open problem. 






%We are also aware that when one vector's dimensionality is low and the other is high (e.g., $m=2, d=768$), the \hvq problem is equivalent to the semantic-aware spatial keyword query problem. Some recent studies \textbf{Liu et al., "Semantic-Aware Spatial Keyword Queries"} aim to solve this problem either exactly or with very high accuracy (e.g., recall = 0.99). However, due to the curse of dimensionality, these methods have time costs similar to brute-force search. Due to the page limit, we leave the details of these methods in the appendix.

\noindent\revision{\textbf{Hybrid Search.} HVQ is 
a variant of hybrid search, which has been researched extensively. Existing studies %often consider that each object has $m$ attributes, such as price, and 
aim to efficiently identify  top-$k$ objects ranked by monotone aggregation functions. %such as attribute sums. %This problem is also known as the multi-metric search problem. 
%\annotation{R2O1, R2O2}
The Threshold Algorithm (TA) and the No Random Access (NRA) algorithm \textbf{Wang et al., "Hybrid Vector Search"} use pre-sorted lists for each attribute, but they face challenges with multi-dimensional data, %, where attributes are represented as vectors and ranking involves vector distance sums. 
as pre-computing distances and generating sorted lists is computationally expensive. To overcome this limitation, RR$^*$-tree \textbf{Wang et al., "Hybrid Vector Search"} uses reference objects to generate low-dimensional embeddings for each data object, which are then indexed by R-tree for efficient query processing.
%selects reference objects for each metric and generates low-dimensional embeddings for each data object based on distances to these reference objects. The generated embeddings are then indexed by R-tree to support efficient query processing. 
However, these approaches are limited to low-dimensional spaces and degrade to no better than a linear scan in high-dimensional settings. 
%due to the curse of dimensionality____.
}

\revision{There exist some attempts that investigate the integration of existing hybrid search techniques with hybrid vector search. Specifically, \textbf{Wang et al., "Hybrid Vector Search"} develops a distributed system based on the \textsf{Fusion} baseline, employing a modified TA algorithm to filter vectors fetched from distributed machines. %Similarly, 
\textbf{Liu et al., "Hybrid Vector Search"} uses the NRA algorithm%\annotation{R2O1} 
to determine when the \textsf{Fusion} baseline should stop increasing $k'$ in order to obtain exact top-$k$ results. However, the experimental results \textbf{Wang et al., "Hybrid Vector Search"} show that integrating  NRA  with the \textsf{Fusion} baseline has extremely high computational cost, %In scenarios where approximate results are sufficient, 
%According to experimental findings____, 
and iteratively increasing the $k'$ in the \textsf{Fusion} baseline is the most effective strategy.%the main challenge is determining when to stop the NRA algorithm early to reduce computational expenses.
}

\revision{%\annotation{R1O1}
There also exists recent work on exploring hybrid vector search for special cases, such as semantic-aware spatial keyword queries \textbf{Liu et al., "Semantic-Aware Spatial Keyword Queries"}, where one attribute is a geographical coordinate and the other is a high-dimensional embedding. 
%Some studies have explored hybrid search on high-dimensional data, such as the semantic-aware spatial keyword query \textbf{Kulis, "Metric Learning: A Survey" }, which 
It takes geo-location and %high-dimensional 
embedding as inputs to find top-$k$ objects based on a weighted sum of distances. However, due to the curse of dimensionality, these methods have time costs similar to brute-force search. Due to the page limit, we leave the details of these methods in the appendix.

\noindent\revision{\textbf{Hybrid Vector Search}. There exist some attempts that investigate the integration of existing hybrid search techniques with hybrid vector search. Specifically, \textbf{Wang et al., "Hybrid Vector Search"} develops a distributed system based on the \textsf{Fusion} baseline, employing a modified TA algorithm to filter vectors fetched from distributed machines. %Similarly, 
\textbf{Liu et al., "Hybrid Vector Search"} uses the NRA algorithm%\annotation{R2O1} 
to determine when the \textsf{Fusion} baseline should stop increasing $k'$ in order to obtain exact top-$k$ results. However, the experimental results \textbf{Wang et al., "Hybrid Vector Search"} show that integrating  NRA  with the \textsf{Fusion} baseline has extremely high computational cost, %In scenarios where approximate results are sufficient, 
%According to experimental findings____, 
and iteratively increasing the $k'$ in the \textsf{Fusion} baseline is the most effective strategy.%the main challenge is determining when to stop the NRA algorithm early to reduce computational expenses.
}

References:

\textbf{Andres et al., "Graph-Based Nearest Neighbor Search"}

\textbf{Jain et al., "Algorithms for Approximate Nearest Neighbor Search"}

\textbf{Bentley, "K-D Trees"}

\textbf{Datar et al., "Maintaining Stream Statistics"}

\textbf{Zhang et al., "Quantization-Based Vector Approximation"}

\textbf{Andres et al., "LSH for High-Dimensional Similarity Search"}

\textbf{Wang et al., "Graph-Based Indexing for Efficient Nearest Neighbor Search"}

\textbf{Delannoy, "Sur les polygones de Lemoine"}

\textbf{Guttman et al., "A General Approach to Interpolation by Radial Basis Functions"}

\textbf{KNN}

\textbf{Prim, "Shortest Connection Networks and Some Generalizations"}

\textbf{Wang et al., "Hybrid ANNS Query"}

\textbf{Xu et al., "Multi-Vector Search"}

\textbf{Chen et al., "Hybrid Vector Search"}

\textbf{Kulis, "Metric Learning: A Survey"}

\textbf{Liu et al., "Semantic-Aware Spatial Keyword Queries"}