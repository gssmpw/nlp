% \vspace*{-1em}
\section{Experiments}
\label{sec:exp}
% We evaluate the following aspects of our proposed solution: (1) search performance (Section~\ref{sec:exp-search-performance}), (2) index cost (Section~\ref{sec:exp-index-cost}), (3) parameter sensitivity (Section~\ref{sec:exp-para}), (4) ablation study (Section~\ref{sec:exp-ablation}).


% It is worth mentioning that the dataset we used is an order of magnitude larger than those used by other memory-based ANNS index studies~[xx]. Therefore, we do not conduct a scalability study in this work.

\subsection{Evaluation Setup}
\label{sec:exp-setup}

\begin{table}[!t]
    \centering
    \small
    % \vspace{-1em}
    \caption{Datasets Statistics.}
    \vspace*{-1em}
    \label{tab:dataset}
    \renewcommand{\arraystretch}{1.2} % 控制行高
    \setlength{\arrayrulewidth}{1pt}
    \begin{tabularx}{0.9\textwidth}
    {|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{1.5cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}X|}
    % {|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|}    
    \hline
    Dataset & $|D|$ & $d$ & $m$ & $|Q|$ & Type  \\
    \hline
    % \textsf{OpenImage} & 1,746,073 &  768 & 768 & 1,000 & Text, Image \\
    \textsf{OpenImage} & 507,444 & 768 & 768 & 1,000 & Text, Image \\
    \hline
    \textsf{Ins-SG}  & 1,000,000 &  768 & 2 & 1,000 & Text, Coordinate \\ 
    \hline
    \textsf{Howto100M} & 1,238,875& 1,024 & 768 & 1,000 & Text, Video \\    
    \hline
    \textsf{CC3M} & 3,131,153 &  768 & 768 & 1,000 & Text, Image \\
    \hline
    \textsf{Twitter-US} & 10,000,000 &  768 & 2 & 1,000 & Text, Coordinate \\
    \hline
    \end{tabularx}
    \vspace*{-1em}
\end{table}

\noindent\textbf{Datasets.} Our experiments are conducted on five real-world datasets: \textsf{OpenImage}, \textsf{Ins-SG}, 
\textsf{Howto100M}, \textsf{CC3M}, and \textsf{Twitter-US}. The statistics of datasets are listed in Table~\ref{tab:dataset}. Details of each dataset are stated as follows.
\begin{itemize}[leftmargin=*, topsep=0pt]
\item \textsf{OpenImage}~\cite{OpenImagesLocNarr}: The \textsf{OpenImage} dataset\footnote{\url{https://google.github.io/localized-narratives/}} is an open benchmark for object detection, image classification, and visual relationship detection. %and available online\footnote{https://storage.googleapis.com/openimages/web/index.html}. 
%It comprises 1.7M training images and 41K validation images. Each image is annotated with bounding boxes and corresponding labels. Experts manually annotate 90\% of the boxes and labels, while the remaining 10\% are machine-annotated and human-verified. Images are converted into 768-dimensional vectors using ViT~[xx], and labels are concatenated and transformed into 768-dimensional vectors using BERT~[xx]. The training set is used as the base data. The query set $Q$ is 1,000 images randomly selected from the validation set.
It comprises 500K training images and 41K validation images. Each image is paired with localized narratives provided by annotators. Images are converted into 768-dimensional vectors ($o.e$) using ViT~\cite{dosovitskiy2020image}, and localized narratives are transformed into 768-dimensional vectors ($o.s$) using BERT~\cite{devlin2018bert}. The training set is used as the 
%base data. 
database. The query set $Q$ ($|Q|$ = 1,000 ) is 
%images 
randomly selected from the validation set.

\item \textsf{Ins-SG}: The \textsf{Ins-SG} dataset is a real dataset that contains 1 million Instagram posts from Singapore. Each post has a  geo-location and image. The images are transformed into %768-dimensional 
vectors using ViT. The query set $Q$ consists of another 1,000 collected posts. 
%together with the dataset.

\item \textsf{Howto100M}~\cite{miech19howto100m}: The \textsf{Howto100M} dataset\footnote{\url{https://www.di.ens.fr/willow/research/howto100m/}} is an open benchmark for learning text-video embeddings.
%from narrated instructional videos. 
It includes 136 million video clips,
%from 1.2 million YouTube videos, 
covering 23,000 activities in areas such as cooking, handcrafting, and fitness. Each video is paired with subtitles automatically downloaded from YouTube. We downloaded S3D~\cite{miech2020end} video embeddings from its official website and transformed textual descriptions into embeddings using BERT. We randomly sampled 1,000 video-text pairs 
%from the dataset 
to form the query set $Q$.


%Each video clip is associated with natural language annotations in the form of automatically transcribed subtitles.



\item \textsf{CC3M}~\cite{sharma2018conceptual}: The Conceptual Captions dataset\footnote{\url{https://github.com/google-research-datasets/conceptual-captions}} is an open benchmark for training and evaluating visual-language models. It comprises 3.3M image-text pairs. 
%Each image and corresponding textual description are first crawled from the internet and then further extracted, filtered, and transformed by a well-developed pipeline. We 
%excluded the images that could not be downloaded, 
randomly selected 1,000 image-text pairs as the query set, and treated the remaining image-text pairs as the database. Each Image and its attached text are %separately 
transformed into vectors using ViT and BERT, respectively.

\item \textsf{Twitter-US}: The \textsf{Twitter-US} dataset is generated from 10 million real tweets in the USA. All of them contain geo-locations and text descriptions. The text descriptions are transformed into vectors using BERT. The query set $Q$ consists of 1,000 real tweets collected together with the data. This dataset is an order of magnitude larger than those used by other memory-based graph ANNS index studies~\cite{wang2021comprehensive}. Therefore, we conduct the scalability study on this dataset.

\end{itemize}
% It is worth mentioning that the largest dataset we use is an order of magnitude larger than those used by other memory-based ANNS index studies~[xx]. Therefore, we do not conduct a scalability study in this work.





\noindent\textbf{Evaluation metrics.} Exiting ANNS studies~\cite{liApproximateNearestNeighbor2020,wang2021comprehensive} typically use recall rate $Recall@k=\frac{\mathcal{R}\cap \tilde{\mathcal{R}}}{k}$ to evaluate the accuracy of search results and queries per second QPS $= \frac{\#q}{t}$ to evaluate the search's efficiency. Here, $\mathcal{R}$ represents the result set retrieved by the index, $\tilde{\mathcal{R}}$ denotes the 
%real 
result set returned by a brute-force search, and $|\mathcal{R}| = |\tilde{\mathcal{R}}| = k$. QPS is the ratio of number of queries ($|Q|$) to search time ($t$); i.e., QPS$ = \frac{|Q|}{t}$~\cite{fu2019fast}. In this work, we use recall@10 and QPS as the evaluation metrics.

%in this study.

\noindent\revision{%\annotation{MRO1, R3O1, R3O3}
\textbf{Baselines.} In addition to the two existing baselines discussed in Section~\ref{sec:motivations}, we %further propose 
consider a baseline called Overlay, and an ideal method called Oracle~\footnote{We would like to thank the anonymous reviewers for suggesting the two methods}.
%two graph-based ANNS index baselines that are specifically designed for the hybrid vector search problem, which are detailed below: 
}
\begin{itemize}[leftmargin=*, topsep=0pt]
    \item \revision{\textsf{Overlay (abbr. O)}: This method constructs five different graph-based ANNS indexes by setting $\alpha$ in the hybrid distance to $0.1$, $0.3$, $0.5$, $0.7$, $0.9$, respectively. These separate graph-based indexes are then overlaid into a single graph-based index by merging their edges accordingly, with each edge assigned a value representing the $\alpha$ under which it was constructed. During the query phase, 
    %the search is performed on the overlaid graph-based ANNS index, but 
    rather than traversing all edges, it only routes the edges with values closest to the query’s $\alpha$ and ignores others. This actually restricts the search to the corresponding sub-index, i.e., we use the sub-index built with $q.\alpha=0.1$ to handle queries with $\alpha \in [0, 0.2]$, the sub-index with $\alpha=0.3$ for queries with $q.\alpha \in [0.2, 0.4]$, etc. The overlay operation eliminates the need to store five separate indexes, thereby reducing memory usage. The accuracy-efficiency trade-off is controlled by the search algorithm's hyperparameter (e.g., $ef_{search}$). We integrate this method with the HNSW index, denoted as \textsf{HNSW}$_{\textsf{O}}$. }  
    \item \revision{\textsf{Oracle (abbr. Or)}: This method represents the ideal scenario where a separate graph-based ANNS is built for every possible $\alpha$ value, with searches conducted on the corresponding index. Although this method is not feasible in practical scenarios since $q.\alpha$ is unknown beforehand and can be an arbitrary value within $[0, 1]$, we use it to illustrate the performance gap between our proposed method and the ideal case. Specifically, we implement this approach using the HNSW index, denoted as \textsf{HNSW}$_{\textsf{Or}}$. To make the comparison feasible, we select five $q.\alpha$ values (0.1, 0.3, 0.5, 0.7, and 0.9), constructing a separate HNSW index for each, with $M = 40$ and $ef_{construction} = 200$. These five $\alpha$ values are used to generate five test query sets.
    %, with \textsf{HNSW}$_{\textsf{Or}}$ performing searches on the corresponding HNSW index. 
    We compare \method with \textsf{HNSW}$_{\textsf{Or}}$ and other baselines on these query sets.}
    
    %to compare our proposed \method with \textsf{HNSW}$_{\textsf{Or}}$.}
\end{itemize}


%A key issue is determining the number of edges per node for each separate graph-based index. If each node in these indexes has $M$ edges, the construction time and memory costs could be approximately five times higher than those of other methods, leading to an unfair comparison. To address this, we set the maximum number of edges per node in each separate graph-based index to 10 by default,  then the overlaid HNSW index will have a maximum of 50 edges for each node, ensuring that the total number of edges in the overlaid graph index is comparable to ours. The experiments involving changes to $M$ will be presented later in Section~\ref{sec:exp-para-sen}.  

\begin{figure*}[!t]
\centering
\vspace*{-1.5em}
\subcaptionbox{\revision{OpenImage}\label{fig:exp-openimage}}{
\includegraphics[width=1.0\textwidth]{fig/exp_qps_recall_openimage.pdf}
\vspace*{-0.5em}
}
\subcaptionbox{\revision{Ins-SG}\label{fig:exp-sg-ins}}{
\includegraphics[width=1.0\textwidth]{fig/exp_qps_recall_ins_sg.pdf}
\vspace*{-0.5em}
}
\subcaptionbox{\revision{Howto100M}\label{fig:exp-howto100m}}{
\includegraphics[width=1.0\textwidth]{fig/exp_qps_recall_howto100m.pdf}
\vspace*{-0.5em}
}
\subcaptionbox{\revision{CC3M}\label{fig:exp-cc3m}}{
\includegraphics[width=1.0\textwidth]{fig/exp_qps_recall_cc3m.pdf}
\vspace*{-0.5em}
}
\vspace*{-1em}
% \revision{\marginnote{MRO1, R3O1, R3O3}}
\caption{\revision{The accuracy-efficiency trade-off results (upper and right is better).}}
\vspace*{-1.5em}
\label{fig:exp-acc-eff}
\end{figure*}



\begin{figure*}[!t]
\centering
% \vspace*{-1.5em}
\subcaptionbox{\revision{OpenImage}\label{fig:exp-openimage-oracle}}{
\includegraphics[width=1.0\textwidth]{fig/exp_qps_recall_oracle_openimage.pdf}
\vspace*{-0.5em}
}
\vspace*{-1em}
% \revision{\marginnote{MRO1, R3O3}}
\caption{\revision{The accuracy-efficiency trade-off results compared to \textsf{HNSW}$_{\textsf{Or}}$ (upper and right is better).}}
\vspace*{-1.5em}
\label{fig:exp-acc-eff-oracle}
\end{figure*}


\noindent\textbf{Parameter Settings.} %We first evaluate \textsf{HNSW}$_{\textsf{F}}$ and \textsf{HNSW}$_{\textsf{M}}$ on the \textsf{Openimage} dataset. 
% Following the previous study~[xx], 
The three key parameters of HNSW, namely the candidate set size $ef_{construction}$, the maximum number of edges per node $M$, and the search set size $ef_{search}$, are set to 200, 40, and 10 by default for baselines \textsf{HNSW}$_{\textsf{F}}$ and \textsf{HNSW}$_{\textsf{M}}$, with other parameters set as recommended in the previous study~\cite{malkovEfficientRobustApproximate2020}. \revision{For baseline \textsf{HNSW}$_{\textsf{O}}$, if the maximum number of edges per sub-index node is set to be $M$, the construction time and memory costs can be up to five times higher than other methods, %could be approximately five times higher than those of other methods, 
leading to unfair comparison. %\annotation{MRO1, R3O3}
Therefore, we set the maximum number of edges per node in each sub-index to 10 by default, so that each node in the overlaid HNSW index has a maximum of 50 edges, and thus the total number of edges in the overlaid graph index is comparable to others. Accordingly, we set the default value of $ef_{construction}$ for each sub-index to 50, which is five times the value of $M$, as applied previously. All other parameters for \textsf{HNSW}$_{\textsf{O}}$ are set the same as other baselines. Experiments on varying $M$ and $ef_{construction}$ will be presented later in Section~\ref{sec:exp-para-sen}. } %and our proposed method \method. 
For \textsf{HNSW}$_{\textsf{F}}$ \revision{and \textsf{HNSW}$_{\textsf{O}}$}, we vary the hyperparameter $ef_{search}$ from 10 to 200 in steps of 10 to control the accuracy-efficiency trade-off. For \textsf{HNSW}$_{\textsf{M}}$, we vary the hyperparameter $k'$, the number of objects to be retrieved from each index, and then merged and reranked, from 10 to 200 in steps of 10 to control the accuracy-efficiency trade-off, with $ef_{search}$ always set equal to $k'$. %Our method \method's hyperparameters $ef_{construction}$, $M$, and $ef_{search}$ are tuned in the same way as \textsf{HNSW}$_{\textsf{F}}$ to ensure a fair comparison. 
{By default, we set $M$ to 40 and $ef_{construction}$ to 200 for \method, aligning with \textsf{HNSW}$_{\textsf{F}}$. When \textsf{HNSW}$_{\textsf{F}}$'s hyperparameters are adjusted, \method is modified to the same value accordingly to ensure consistency and fair comparison unless otherwise specified.} The threshold value $th$ is consistently set to $0.1$. The parameter $\alpha$ in Equation~\ref{eq:hybird_distance} allows to set preferences between different modalities. \revision{%\annotation{R3O1, R3O4}
To evaluate the capability of methods to adapt to different query $\alpha$ values, we divide the $\alpha$ range into five intervals: [0, 0.2], [0.2, 0.4], [0.4, 0.6], [0.6, 0.8], and [0.8, 1]. We generate five random $\alpha$ values for each test query, one from each interval, producing five different query sets. Each set includes all test queries with $\alpha$ values from one specified interval. %Intuitively, when $q.\alpha$ approaches 0 or 1, the search will be dominated by one of the feature vectors, making the problem simpler. In contrast, when $q.\alpha$ is close to 0.5, the problem becomes more complex. 
We report the overall average results of all sets for a comprehensive comparison. We also report the average results for each set for a detailed comparison. %For each interval, we calculate the average results of all queries within the corresponding set for comparison. 
%This approach allows us to evaluate performance across all alpha intervals systematically. 
}
%We vary $\alpha$ in $\{0.1, 0.3, 0.5, 0.7, 0.9\}$ to evaluate the capability of 
%baselines 
%methods to adapt to different input $\alpha$ values. 


\noindent\textbf{Implementations.} %The baselines and our proposed \method are all implemented in C+, %The ANNS indexes (e.g., HNSW and \method) and the R-tree are all implemented in C++, 
%with the implementations of HNSW\footnote{\url{https://github.com/Lsyhprum/WEAVESS/}} and R-tree\footnote{\url{https://github.com/nushoin/RTree}} sourced from publicly available code repositories. It is worth noting that our proposed \method and the baselines are implemented within the same framework\footnote{\url{https://github.com/Lsyhprum/WEAVESS/}}, thereby ensuring a fair comparison. Our default experimental environment consists of an AMD Ryzen Threadripper PRO 5965WX CPU @ 7.00 GHz and 128GB of memory. All experiments are conducted on the CPU. 
The baselines and  \method\footnote{The code is available at \url{https://github.com/Heisenberg-Yin/DEG}.} are all implemented in C++. The implementations of HNSW\footnote{\url{https://github.com/Lsyhprum/WEAVESS/}} and R-tree\footnote{\url{https://github.com/nushoin/RTree}} are sourced from publicly available code repositories. \revision{%\annotation{R1O3}
\method and the baselines are implemented by following previous experimental evaluations\footnotemark[5], excluding SIMD, pre-fetching, and other hardware-specific optimizations. Although these optimizations can speed up index construction significantly, they are hardware-specific and could introduce unfairness in comparison. }%It is worth noting that our proposed \method and the baselines are implemented within the same framework\footnotemark[4], thereby ensuring a fair comparison. 
Our default experimental environment consists of an AMD Ryzen Threadripper PRO 5965WX CPU @ 7.00 GHz and 128GB of memory. %All experiments are conducted on the CPU.





% \vspace{-1em}
\subsection{Experimental Results}
\label{sec:exp-results}
\subsubsection{\textbf{Search Performance}}
\label{sec:exp-search-performance}
The accuracy-efficiency trade-off results over the four datasets are shown in Figure~\ref{fig:exp-acc-eff}. We have the following observations. 


\noindent%\annotation{MRO1, R3O3}
\revision{\textbf{(1) \method demonstrates the best performance compared to the baselines %across different $\alpha$ settings 
on all the datasets.} %For example, in the \textsf{OpenImage} dataset, among the baselines, \textsf{HNSW}$_{\textsf{F}}$ performs better at $\alpha = 0.3, 0.5, 0.7$ and \textsf{HNSW}$_{\textsf{M}}$ excels at $\alpha = 0.1, 0.9$, which is consistent with our analysis in Section~\ref{sec:motivations}. 
Specifically, compared to the baselines, \method consistently achieves the best overall performance across the four datasets. Additionally, across different $\alpha$ settings, \method consistently delivers the best performance. For example, in the \textsf{OpenImage} dataset, among the baselines, \textsf{HNSW}$_{\textsf{F}}$ performs better when $\alpha \in [0.2, 1]$ and \textsf{HNSW}$_{\textsf{O}}$ excels when $\alpha \in [0, 0.2]$. Our proposed \method matches the performance of \textsf{HNSW}$_{\textsf{F}}$ for $\alpha \in [0.4, 0.6]$ and outperforms the best baseline in all other query $\alpha$ settings. } It is worth noting that \method aims to maintain high performance across different $\alpha$ values rather than outperforming existing state-of-the-art graph-based ANNS indexes. It is as expected that \method has similar performance as \textsf{HNSW}$_{\textsf{F}}$ when $q.\alpha$ \revision{is close to} $0.5$ because \textsf{HNSW}$_{\textsf{F}}$ is constructed with $\alpha=0.5$. %Additionally, across all five query $\alpha$ ranges, our method achieves a higher average performance compared to the baselines.} 
%Similar results are observed in other datasets.

% When searching with other $\alpha$ values (e.g. 0.1, 0.9), \textsf{HNSW}$_{\textsf{F}}$ starts to degrade, whereas our proposed \method does not. Compared with \textsf{HNSW}$_{\textsf{M}}$, \method significantly outperforms it across different $\alpha$ settings. 

\noindent\revision{\textbf{(2) The baselines perform well in certain $q.\alpha$ settings but poorly in others, while \method consistently achieves high performance across all $q.\alpha$ settings without significant degradation.} %\annotation{MRO1, R3O3}
The results show that when $q.\alpha$ is close to $0.1$, \textsf{HNSW}$_{\textsf{F}}$ faces significant performance degradation across the four datasets. Similarly, \textsf{HNSW}$_{\textsf{M}}$ shows comparable degradation across the four datasets when $\alpha$ is close to $0.5$. These results validate our analysis in Section~\ref{sec:motivations}. A similar pattern is observed with \textsf{HNSW}$_{\textsf{O}}$, which performs better at $\alpha \in [0, 0.2]$ on the \textsf{OpenImage}, \textsf{Ins-SG}, and \textsf{CC3M} datasets, and excels at $\alpha \in [0.8, 1.0]$ on the \textsf{Howto100M} dataset. %Although \textsf{HNSW}$_{\textsf{O}}$ was designed to perform consistently across different $q.\alpha$ settings by building five separate indexes, performance degradation is still observed. 
%We believe this occurs 
This pattern may occur because when $q.\alpha$ is close to 0 or 1, the search becomes dominated by a single feature vector. If this modality's feature vector is easier for the graph-based ANNS to capture, the performance will improve. This reasoning is also supported by the performance of \textsf{HNSW}$_{\textsf{M}}$. In settings where \textsf{HNSW}$_{\textsf{O}}$ performs better, \textsf{HNSW}$_{\textsf{M}}$ also shows relatively better performance. For example, on the \textsf{OpenImage} and \textsf{CCM} datasets, for $\alpha$ in [0, 0.2], \textsf{HNSW}$_{\textsf{M}}$ outperforms \textsf{HNSW}$_{\textsf{F}}$, but 
%is outperformed 
performs worse in other $\alpha$ settings. The \textsf{Ins-SG} dataset is an exception and the reason for this will be explained later. %\annotation{R3O1, R3O4}
Moreover, we also evaluate the performance of \method %in normal vector query scenarios 
when $\alpha = 0$ or $1$, which reduces to normal vector queries. Experimental results show that \method still delivers %comparable 
the best performance compared to the %best-performing 
baselines for \hvq. Detailed results are provided in the appendix due to page limitations.
%and makes the problem easier, as explained in Section~\ref{sec:exp-setup}, leading to %comparatively a better performance.
} 

%The results show that the baselines perform well for some $\alpha$ settings but degrade significantly for others, which validates our analysis in Section~\ref{sec:motivations}. For instance, when $\alpha = 0.1$, \textsf{HNSW}$_{\textsf{F}}$ faces significant performance degradation across the four datasets. Similarly, \textsf{HNSW}$_{\textsf{M}}$ shows comparable degradation across the four datasets when $\alpha = 0.5$. \method addresses this issue, maintaining strong performance across different $\alpha$ values, without noticeable degradation on the four datasets.

\noindent{\textbf{(3) \method maintains similar advantages on larger datasets.} Specifically, the \textsf{CC3M} dataset employs the same embedding techniques as the \textsf{OpenImage} dataset, but it is much larger. \method shows similar advantages over the baselines on both datasets, validating that  \method also performs well on larger datasets.}

\noindent\textbf{(4) \textsf{HNSW}$_{\textsf{M}}$ performs worse on the \textsf{Ins-SG} datasets.} As shown in Figure~\ref{fig:exp-sg-ins}, \textsf{HNSW}$_{\textsf{M}}$ shows significantly worse performance than \textsf{HNSW}$_{\textsf{F}}$ when $\alpha = 0.1, 0.3, 0.5, 0.7$ on the \textsf{Ins-SG} dataset. This is because the \textsf{HNSW}$_{\textsf{M}}$ struggles to retrieve high-quality candidates for reranking. Due to the inherently dense distribution of geographic coordinates compared to high-dimensional vectors, 
hundreds of objects can coexist within a small spatial scale,
%there may still be hundreds of objects with very similar geographic similarity (e.g., 0.01), %making embedding similarity the dominating factor in ranking. 
making geographically close objects difficult to distinguish from each other, and requiring embedding similarity to further determine the ranking results. \revision{However, both the R-Tree and HNSW used in \textsf{HNSW}$_{\textsf{M}}$ fails to retrieve objects that exhibit similarity across both modalities, thereby resulting in notable performance degradation. %\annotation{R1O2, R3O4}
It is worth mentioning that the throughput of R-Tree and HNSW is comparable, with 1,777 and 1,906 queries per second, respectively, when $k' = 10$. Therefore, the relatively low performance of \textsf{HNSW}$_{\textsf{F}}$ is not due to HNSW having a slower querying speed. %compared to R-Tree.
}

\subsubsection{\revision{\textbf{Performance Gap Analysis with \textsf{HNSW}$_{\textsf{Or}}$.}}} \revision{Here, we compare \method with the \textsf{HNSW}$_{\textsf{Or}}$ and other baselines %, which represents the ideal scenario, 
on the \textsf{OpenImage} dataset for $\alpha = 0.1$, $0.3$, $0.5$, $0.7$, and $0.9$. %\annotation{MRO1, R3O3}
The experimental results are shown in Figure~\ref{fig:exp-acc-eff-oracle}. %We have the following findings: 
\textbf{The results demonstrate that the overall performance of \method is comparable to that of \textsf{HNSW}$_{\textsf{Or}}$ and significantly outperforms other baselines.} %Specifically, \method deliver similar overall performance compared to the \textsf{HNSW}$_{\textsf{Or}}$ baseline matches the performance of \textsf{HNSW}$_{\textsf{Or}}$ at $\alpha = 0.5$, performs slightly lower at $\alpha = 0.1$, and slightly higher at $\alpha = 0.3$. The \method's slightly higher performance at $\alpha = 0.3$ can be attributed to our new components, such as the edge seed method. 
This suggests that the performance of \method 
%has reached the optimal level, 
is on par with \textsf{HNSW}$_{\textsf{Or}}$.
%while the construction cost remains comparable to other baselines (as to be detailed later).
}


% However, \textsf{HNSW}$_{\textsf{M}}$ fails to retrieve objects that exhibit similarity across both modalities, thereby resulting in notable performance degradation.


% \noindent\textbf{(5) For different values of $\alpha$, there are discrepancies in the extent of performance degradation.} Specifically, for the \textsf{HNSW}$_{\textsf{F}}$ method, when $q.\alpha = 0.7$, its performance is similar to when $q.\alpha=0.5$, and the performance degradation at $q.\alpha = 0.9$ is not as severe as at $q.\alpha = 0.1, 0.3$. Similar results are observed in the \textsf{HNSW}$_{\textsf{M}}$ method as well. The reason lies in the difference in the distribution of the two vectors. When the variance of one vector (e.g., $o.e$) is larger, the average distance between these vectors is greater than that of the other vector, causing the experimental results to skew.

\begin{figure}[!htbp]
\vspace*{-1em}
\centering
\includegraphics[width=0.9\textwidth]{fig/exp_qps_recall_twitter.pdf}
\vspace*{-1em}
\caption{\revision{Scalability study.}}
\vspace*{-1em}
\label{fig:scalibility-study}
\end{figure}

\subsubsection{\textbf{Scalability Study}}
\label{sec:exp-scalability}
Here, we investigate the index construction cost and search performance of our proposed \method and the baselines on the \textsf{Twitter-us} dataset. The index construction time for \method is 44,492 seconds, approximately 12 hours. However, neither of the baselines completed the index construction within two days. This is consistent with the experimental results of \cite{fu2019fast}, where HNSW could not be constructed on larger datasets and raised an Out-Of-Memory error. We infer that this is due to HNSW's multi-layer mechanism, which causes its construction cost to increase exponentially with the size of the dataset. Therefore, we only report the results for \method, which are shown in Figure~\ref{fig:scalibility-study}. The results show that %our proposed method
\method exhibits stable performance for varying $\alpha$, consistent with our observations in previous experiments.





\subsubsection{\textbf{Index Cost}}
\label{sec:exp-index-cost}

% \begin{figure}[!htbp]
% \begin{center}
% \includegraphics[width=0.5\columnwidth]{fig/memory.pdf}
% \label{fig:memory}
% \caption{The index size.}
% \end{center}
% \end{figure}



Figure~\ref{fig:constrution_time} illustrates the construction time of \method and the baselines on the four datasets with default parameter settings. \revision{\textbf{
%\annotation{MRO1, R3O3}
The results show that \method's indexing time is comparable to those of \textsf{HNSW}$_{\textsf{M}}$ and \textsf{HNSW}$_{\textsf{F}}$, and is significantly faster than that of \textsf{HNSW}$_{\textsf{O}}$.}} %For instance, 
For instance, on the largest dataset \textsf{CC3M}, \method has a comparable time cost to \textsf{HNSW}$_{\textsf{F}}$, \revision{while being 1.6 times faster than \textsf{HNSW}$_{\textsf{M}}$ and 2.3 times faster than \textsf{HNSW}$_{\textsf{O}}$. \textsf{HNSW}$_{\textsf{M}}$ and \textsf{HNSW}$_{\textsf{O}}$ are slower because they  build multiple indexes.} This validates our %conclusion 
analysis in Section~\ref{sec:search} that the \method's construction time is comparable to previous graph-based ANNS indexes. %\annotation{R3O4}
On the \textsf{Ins-SG} dataset, HNSW$_M$ has a slightly shorter construction time than HNSW$_F$. {The reason is two-fold: (1) HNSW$_M$ builds indexes separately, %allowing it to compute distances between individual vectors rather than hybrid vectors, thereby leading to similar construction times for both baselines across all datasets. 
computing distances for individual vectors rather than hybrid vectors, resulting in similar construction times across datasets; (2) The construction of R-Tree is faster, leading to slightly quicker times on the \textsf{Ins-SG} dataset but slightly slower on others.} %\annotation{R1O2, R1O3} 
\revision{Note that the 
%relatively 
high construction time is a result of our setting, as discussed in Section~\ref{sec:exp-setup}, where hardware-specific optimizations such as SIMD and pre-fetching instructions have been removed. With these optimizations, the construction time for million-scale datasets can be reduced to several minutes.}  Figure~\ref{fig:memory} shows the memory usage of \method and the baselines. \textbf{The results show that \method's memory usage is slightly higher than baseline methods due to its more complex index structure.} However, the high-dimensional vectors still dominate the memory consumption, making the difference negligible 
%and not impacting 
in its real-world application. {Notably, maintaining identical construction time or memory consumption between baselines and \method for a fair comparison is impractical. Therefore, we provide further comparisons in the appendix.}


\begin{figure}[!htbp]
\begin{center}
% \vspace*{-1em}
\subcaptionbox{Construction Time.\label{fig:constrution_time}}{
\includegraphics[width=0.45\columnwidth]{fig/construction_time.pdf}
}
% \revision{\marginnote{R3O4}}
\subcaptionbox{Index Size.\label{fig:memory}}{
\includegraphics[width=0.45\columnwidth]{fig/memory.pdf}
}
% \vspace*{-1em}
\caption{\revision{The index size and construction time.}}
% \vspace*{-1em}
\label{fig:memory-construction}
\end{center}
\end{figure}
% \begin{table}[!htbp]
%     % \vspace*{-1em}
%     \centering
%     \caption{The Indexing Time for the \textsf{CC3M} Dataset.}
%     \vspace*{-1em}
%     \label{tab:index-time}
%     \renewcommand{\arraystretch}{1.2} % 控制行高
%     \setlength{\arrayrulewidth}{0.5pt}
%     %\small
%     \begin{tabularx}{0.45\textwidth}{>{\centering}m{2cm}*{3}{X}}
%     \toprule[1pt]
%      & \textsf{HNSW}$_{\textsf{F}}$ & \textsf{HNSW}$_{\textsf{M}}$ & \method\\
%     \midrule[1pt]
%     Time (s) & 27735.8 & 42426.4 & 27258.9 \\
%     %OpenImage &  &  & 4501.54\\   
%     % \midrule6
%     % \textbf{\method} &  &  &  \\    
%     \bottomrule[1pt]
%   \end{tabularx}
%   \vspace*{-1em}
% \end{table}

% In Table~\ref{tab:index-time}, we report the index construction times of \method and other baseline methods on the largest dataset \textsf{CC3M} with default parameter settings. The results show that 
% %indexing time is not a bottleneck for \method. 
% \method's indexing time is comparable to \textsf{HNSW}$_{\textsf{F}}$ and significantly faster than \textsf{HNSW}$_{\textsf{M}}$, which is slower because it requires building two separate indexes. This validates our %conclusion 
% analysis in Section~\ref{sec:search} that the \method's construction time is comparable to previous graph-based ANNS indexes.











\subsubsection{\textbf{Parameter Sensitivity Study}}
\label{sec:exp-para-sen} \revision{Here, we examine how the performance of \method and the baselines change is affected by the two key hyperparameters, $M$ and $ef_{construction}$. %The three key hyperparameters are the candidate set size $ef_{construction}$, the maximum number of edges per node $M$, and the search set size $ef_{search}$, which is set to 40, 200, and 10 by default. $ef_{search}$ controls the accuracy-efficiency tradeoff during the query phase and has been adjusted from 10 to 200 in Section~\ref{sec:exp-search-performance} to demonstrate the search performance of the built index. Therefore, we investigate how $M$ and $ef_{construction}$ impacts the built index's quality. 
Specifically, we increase $M$ and $ef_{construction}$ from 40, 200 to 50, 250, then to 60, 300 for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$, and  \method. For \textsf{HNSW}$_{\textsf{O}}$, we adjust the $M$ and $ef_{construction}$ of each sub-index from 10, 50 to 12, 60, then to 14, 70, ensuring that they have comparative edges and allow for a fair comparison. %We maintain the same hyperparameters across both the baselines and our proposed \method for fair comparisons. The experimental results are shown in Figure~\ref{fig:exp-acc-eff-para-sensitivity}. 
Due to the page limitation, we only report the results for $\alpha \in [0, 0.2], [0.4, 0.6], [0.8, 1.0]$ when $M=40, 60$, with the other results in the appendix.} %\annotation{MRO1, R3O3}
The results demonstrate that: \revision{(1) For the different number of edges $M$ and candidate set size $ef_{construction}$,  \method maintains the best performance among the baselines. (2) With varying $M$ and $ef_{construction}$, both baselines \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$, and  \method exhibit similar performance. This indicates that for graph-based ANNS indexes, once $M$ and $ef_{construction}$ are fine-tuned to relatively large values, the key factors determining performance are no longer the hyperparameters, but rather the edge selection strategy, as to be shown later. (3) The performance of the \textsf{HNSW}$_{\textsf{O}}$ improves as $M$ and $ef_{construction}$ increase, but but this leads to significantly higher index construction costs. For example, when $M = 14$ and $ef_{construction}=70$, the overall performance of \textsf{HNSW}$_{\textsf{O}}$ becomes comparable to that of \textsf{HNSW}$_{\textsf{F}}$, %. However, under the same parameters
but the build time for \textsf{HNSW}$_{\textsf{O}}$ is %67 minutes, 
twice as long as that of \textsf{HNSW}$_{\textsf{F}}$. This indicates that although \textsf{HNSW}$_{\textsf{F}}$ offers better performance as $M$ and $ef_{construction}$ increase, it comes at the cost of significantly higher indexing overhead. }

%\textbf{(1) For the different number of edges $M$ and candidate set size $ef_{construction}$, \revision{our proposed \method maintains the best performance among the baselines.}}  \textbf{(2) Both baselines and our proposed \method exhibit similar performance for varying $M$ and $ef_{construction}$.} This indicates that for graph-based ANNS indexes, the key factors determining performance are not the hyperparameters but the edge selection strategy, as to be shown later.

% \begin{figure}[!t]
% \centering
% \vspace*{-1.5em}
% \subcaptionbox{$M=40, ef_{constrution}=200$\label{fig:exp-openimage-40-200}}{
% \includegraphics[width=0.45\textwidth]{fig/exp_qps_recall_40_200_openimage.pdf}
% \vspace*{-0.5em}
% }
% % \subcaptionbox{$M=50, ef_{constrution}=250$\label{fig:exp-openimage-50-250}}{
% % \includegraphics[width=0.45\textwidth]{fig/exp_qps_recall_50_250_openimage.pdf}
% % \vspace{-0.5em}
% % }
% \subcaptionbox{$M=60, ef_{constrution}=300$\label{fig:exp-openimage-60-300}}{
% \includegraphics[width=0.45\textwidth]{fig/exp_qps_recall_60_300_openimage.pdf}
% \vspace*{-0.5em}
% }
% \vspace*{-1em}
% \caption{\revision{The accuracy-efficiency trade-off results %on \textsf{OpenImage} dataset 
% with varying $M$ and $ef_{constrution}$.}}
% \vspace*{-2em}
% \label{fig:exp-acc-eff-para-sensitivity}
% \end{figure}

\begin{figure}[!t]
\centering
% \vspace{-2em}
\subcaptionbox{\revision{$M=40, ef_{constrution}=200$ ($M=10, ef_{construction}=50$ for \textsf{HNSW}$_{\textsf{O}}$)}\label{fig:exp-openimage-40-200-full}}{
\includegraphics[width=0.7\textwidth]{fig/exp_qps_recall_40_200_openimage_half.pdf}
}
% \subcaptionbox{$M=50, ef_{constrution}=250$\label{fig:exp-openimage-50-250-full}}{
% \includegraphics[width=0.9\textwidth]{fig/exp_qps_recall_50_250_openimage_full.pdf}
% }
\subcaptionbox{\revision{$M=60, ef_{constrution}=300$ ($M=14, ef_{construction}=70$ for \textsf{HNSW}$_{\textsf{O}}$)}\label{fig:exp-openimage-60-300-full}}{
\includegraphics[width=0.7\textwidth]{fig/exp_qps_recall_60_300_openimage_half.pdf}
}
% \vspace{-1em}
\caption{\revision{The accuracy-efficiency trade-off results on \textsf{OpenImage} dataset with varying $M$ and $ef_{constrution}$.}}
% \vspace{-2em}
\label{fig:exp-acc-eff-para-sensitivity-full}
\end{figure}




%The main differences between D-RNG and the RNG can be summarized as it includes different edges to adapt to different values of $q.\alpha$. To validate the superiority of D-RNG, we propose a naive method called \textsf{HNSW}$_{\textsf{vary}}$ that attempts to cover different edges. Specifically, instead of fixing $\alpha = 0.5$ during index construction, it constructs five different HNSW indexes by setting $\alpha = 0.1, 0.3, 0.5, 0.7, 0.9$ and merges these indexes. Note that each index's maximum edge number for each node is $\frac{M}{5}$, given that the merged index's maximum edge number for each node is $M$. During the query phase, it traverses all the edges using a greedy search algorithm.

% First, D-RNG includes more edges to adapt to different values of $q.\alpha$. Second, each edge in D-RNG is assigned a use range, allowing for dynamic edge routing during the search process based on the specific value of $q.\alpha$. In the following, We verify the effectiveness of the D-RNG from the aforementioned two perspectives.

% To validate the effectiveness of the first difference point,



% To validate the effectiveness of the dynamic use range, we propose an alternative method called DEG$_{static}$. Instead of dynamically choosing edges to route, DEG$_{static}$ routes through all the edges during the search phase. DEG$_{static}$ maintains the same edges and other components as \method. The experimental results on the \textsf{CC3M} datasets are shown in Figure~\ref{fig:exp-use-range-ablation}. The results indicate that the dynamic use range can enhance search performance.

% Note that DEG$_{static}$ shares exactly the same edges as \method.




\subsubsection{\textbf{Ablation study}} 
\label{sec:exp-ablation-study} Here, we investigate how the proposed components contribute to \method's performance. Due to page limitations, we present results when \revision{$\alpha \in [0, 0.2], [0.4, 0.6], [0.8, 1]$}, with full results in the appendix.

\noindent\textbf{The \method's pruning strategy.}  To verify the effectiveness of the \method's pruning strategy, we compared \method with DEG$_{None}$, which does not apply any pruning strategy but uses the approximate %$l$-layer 
Pareto frontiers obtained by the \GPS algorithm as edges directly and assigns each edge an active range $u=[0, 1]$. %Note that DEG$_{None}$ shares the same other components with \method. 
The experimental results are shown in Figure~\ref{fig:exp-drng-ablation}, which %The results 
show that \method consistently outperforms DEG$_{None}$ by a large margin across varying $\alpha$. This validates the effectiveness of the \method's pruning strategy and confirms that the edge selection strategy is the key factor in enhancing the search performance of graph-based ANNS indexes.%}





% we replaced the D-RNG's pruning algorithm in \method with two comparison algorithms. One is called DEG$_{RNG}$, which simply applies the RNG pruning strategy to the edge candidate set obtained by the \GPS algorithm and assigns each edge an active range $u=[0, 1]$. The other is called DEG$_{None}$, which does not apply any pruning strategy but uses the nearest $l$-layer Pareto frontiers as edges and assigns each edge an active range.

%\subsubsection
%Ablation Study of t

\noindent\textbf{The active range.} We further explore how the active range enhances \method's search performance by proposing an alternative method, DEG$_{static}$. DEG$_{static}$ uses the same index as \method but routes through all edges during the search phase, ignoring the active range. The experimental results on the \textsf{OpenImage} dataset are shown in Figure~\ref{fig:exp-active-range-ablation}. {The results show that the \method consistently outperforms the DEG$_{static}$ for varying $\alpha$, which indicates the active range can enhance search performance.}


\noindent\textbf{The candidate acquisition method.}
To verify the effectiveness of the GPS algorithm, %the Pareto frontiers based \GPS algorithm, 
we replaced the GPS algorithm in \method with the greedy search algorithm, fixing $\alpha = 0.5$ during the index construction phase, which we call \textsf{DEG}$_{\textsf{greedy}}$. %Specifically, during the index construction phase, for each inserted node, instead of using the \GPS algorithm to acquire the edge candidate set, \textsf{DEG}$_{\textsf{greedy}}$ uses a greedy search algorithm with fixing $\alpha = 0.5$ to obtain edge candidate set while keeping other components including the pruning strategy and search algorithm the same with \method. 
The experiment results on the \textsf{OpenImage} dataset are shown in Figure~\ref{fig:exp-gps-ablation}. {The results validate that \method consistently outperforms the \method$_{\textsf{greedy}}$ for varying $q.\alpha$, proving that the \GPS algorithm can acquire better candidates than the greedy search algorithm.}




\noindent\textbf{The edge seed method.} To verify the effectiveness of the edge seed acquisition method, we 
%propose 
consider two alternative methods, called DEG$_{\textsf{centroid}}$, which selects the group of points closest to the centroid as the seed. Specifically, it maintains the Pareto frontier of the graph center during the index construction phase and uses it as the starting point during the search phase. Another method is called DEG$_{\textsf{random}}$, which randomly selects some nodes, approximately the same size as the edge seed, as the starting point. The experimental results are shown in Figure~\ref{fig:exp-seed-ablation}. {The results show that \method consistently outperforms the two alternatives for varying $\alpha$, which validates the edge seed method's superiority.}

\begin{figure}[!t]
\centering
\vspace*{-1em}
\includegraphics[width=0.7\textwidth]{fig/exp_qps_recall_openimage_drng_albation_half.pdf}
\vspace*{-1em}
\caption{\revision{Ablation Study for the DEG's pruning strategy.}}
\vspace*{-1em}
\label{fig:exp-drng-ablation}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=0.7\textwidth]{fig/exp_qps_recall_openimage_active_albation_half.pdf}
\vspace*{-1em}
\caption{\revision{Ablation Study for the Active Range.}}
\vspace*{-1em}
\label{fig:exp-active-range-ablation}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=0.7\textwidth]{fig/exp_qps_recall_openimage_gps_albation_half.pdf}
\vspace*{-1em}
\caption{\revision{Ablation Study for candidate acquisition method.}}
\vspace*{-1em}
\label{fig:exp-gps-ablation}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=0.7\textwidth]{fig/exp_qps_recall_openimage_seed_albation_half.pdf}
\vspace{-1em}
\caption{\revision{Ablation Study for Edge Seed Acquisition method.}}
\vspace*{-1em}    
\label{fig:exp-seed-ablation}
\end{figure}



% \subsubsection{\textbf{Parameter Sensitivity}}
% \label{sec:exp-para}



% \subsubsection{\textbf{Ablation Study}}
% \label{sec:exp-ablation}

