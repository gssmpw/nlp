\newpage
\appendix
\revision{\section{Proof of Theorem 3.1}}

\begin{proof}
\label{theorem:nearest_proof} 
{For any $y \in D$ but $y \notin PF(D, p)$, there must exist $x \in PF(D, p)$ that dominates $y$; otherwise, $y$ would have been added to $PF(D, p)$ as well according to the definition. Consequently, we have $\delta_e(p, x) \leq \delta_e(p, y)$ and $\delta_s(p, x) \leq \delta_s(p, y)$, with at least one inequality being strict. Given that $\alpha \in [0,1]$ and the Euclidean distance is non-negative, we can derive that for any $\alpha$, $Dist(p, x) < Dist(p, y)$.}
\end{proof}

\revision{\section{Proof of Lemma 3.2}}
\begin{proof}
\label{lemma:drng_proof} 
{Since both Equation~\ref{eq:rng-hvq-1} and Equation~\ref{eq:rng-hvq-2} hold for any $\alpha \in r^z_1 \cap r^z_2$, then we can derive that for varying $\alpha \in r^z_1 \cap r^z_2$, the edge length of $(x, y)$, i.e., the distance between $x$ and $y$, is consistently larger than the edge lengths of both $(x, z)$ and $(y, z)$ as the right sides of the two inequalities represent the hybrid distance formulas for $(x, y)$, while the left sides represent the hybrid distance formulas for $(x, z)$ and $(y, z)$, respectively. Therefore, for any $\alpha \in r^z_1 \cap r^z_2$, the edge $(x, y)$ is consistently the longest edge in the triangle $(x, y, z)$, and will be pruned according to the RNG's pruning strategy.}
\end{proof}

\revision{\section{Proof of Lemma 3.3}}
\begin{proof}
\label{lemma:drng_nearest_neighbor_proof} 
%Firstly, for the graph constructed using the dynamic edge pruning strategy on the entire graph, it inevitably satisfies the RNG property for any $\alpha$ value; otherwise, the edge will be pruned or assigned an active range that does not include that $\alpha$ value. Secondly, 
{Given a specific $\alpha$, the graph formed by the active edges constructed using the above method will always be an exact RNG. This is because if any edge can be inserted into the graph without violating the RNG property for a specific $\alpha$ value, it would be included in the graph with an active range that includes $\alpha$ according to our dynamic edge pruning strategy. This ensures that no edges are omitted, thus guaranteeing that the constructed graph is an exact RNG. According to~\cite{fu2019fast, wang2021comprehensive}, exact RNG is equivalent to exact MRNG, thereby guaranteeing that the nearest neighbor can always be found for any query using the greedy search algorithm.}


% According to~\cite{wang2021comprehensive}, the MRNG shares the same pruning strategy as the RNG, making the MRNG equivalent to the RNG. Given a specific $\alpha$, the graph formed by the active edges we construct will always be an RNG. If any node could violate the RNG property, it would be inserted into the graph with an active range that includes $\alpha$ according to our dynamic pruning strategy. Additionally, as stated in~\cite{fu2019fast}, the MRNG guarantees that the nearest neighbor can always be found for the query using the greedy search algorithm.
\end{proof}


\section{Finding $l$-layer Pareto frontiers}
\label{sec:appendix-l-skyline}
\begin{algorithm}[t]
    \caption{Finding Pareto Frontiers}
    \small
    \label{alg:find-pf}
    \KwIn{A candidate pool $CS$ for node $p$, candidate pool size $ef_{construction}$}    
    \KwOut{$l$ layer Pareto frontiers $\{PF^{1}(p), \cdots, PF^{l}(p)\}$ for node $p$, where $\sum_{i = 1}^l |PF^{i}(p)| \leq ef_{construction}$}
    \BlankLine
    
    % Define the procedure
    \SetKwFunction{FindPF}{\textsc{FindPF}}
    \SetKwProg{Fn}{Procedure}{:}{}
    \Fn{\FindPF{$CS, ef_{construction}$}}{
        % Call the procedure
        \text{Sort} $x \in CS$ \text{in ascending order of} $\delta_s(x, p)$ \text{to} $p$\;
        $Res \leftarrow \emptyset$ \tcp*{\textsf{result set}}
        \While{$|Res| < ef_{construction}$}{
            $PF \leftarrow \emptyset$ \tcp*{\textsf{current pareto frontier}}
            $Remain \leftarrow \emptyset$ \tcp*{\textsf{remaining candidates}}
            $PrevEmbDist \leftarrow \infty$ \tcp*{\textsf{smallest $\delta_e(x, p)$, $x \in PF$}}   
            \ForEach{$x \in CS$} {
                \If{$\delta_e(x, p) < \text{PrevEmbDist}$} {
                    % \PFs[\Layer].\text{add}(point)\;
                    $PF.add(x)$\;
                    $PrevEmbDist \leftarrow \delta_e(x, p)$\;
                }
                \Else {
                    $Remain.add(x)$\;
                }
            }
            \If{$|PF| + |Res| < ef_{construction}$} {
                $Res.add(PF)$\;
                $CS \leftarrow Remain$\;
            }
            \Else {
                \textbf{break}\;
            }        
        }
        \Return $Res$ \;
    }
\end{algorithm}


Algorithm~\ref{alg:find-pf} summarizes the procedure of finding $l$-layer $PF(D, p)$ using the existing algorithm~\cite{borzsony2001skyline}. Initially, the entire set is sorted in ascending order of $\delta_s(x, p)$, and the result set $Res$ is initialized (lines 2-3). Then, it repeatedly finds the Pareto frontier and removes it from the candidate set until the result set reaches the size bound (lines 4-18). 
Specifically, in each iteration, two empty sets, $PF$, and $Remain$, are initialized to store the Pareto frontier and remaining nodes, respectively. A variable $PrevEmbDist$ is also initialized to store the $\delta_e(p, x)$ of the last added element in $PF$ (lines 5-7). The candidate set $CS$ is scanned to check if $\delta_e(p, x)$ is smaller than $PrevEmbDist$, determining if it should be added to $PF$ (lines 8-9). If added, $PrevEmbDist$ is updated; otherwise, the element is added to $Remain$ (lines 10-13). This process, proven correct in~\cite{borzsony2001skyline}, ensures that the next element added to $PF$ will have a smaller $\delta_e(p, x)$ value; otherwise, it is dominated by the previous element in $PF$ due to the sorted order. % Only $\delta_e(p, x)$ needs to be checked for dominance by existing points in $PF$. The correctness of this process has been proven in~\cite{borzsony2001skyline}. Since the elements are sorted in ascending order of $\delta_s(p, x)$, the next element in $CS$ will have a larger $\delta_s(p, x)$ value than the elements already added to $PF$. Therefore, we only need to check $\delta_e(p, x)$ to determine whether it is dominated by existing points in $PF$. 
Finally, we check if the result set size exceeds the candidate set size bound $ef_{construction}$ (line 14). If it does, we break the loop and return $Res$ (lines 17-18). Otherwise, we add $PF$ to $Res$, update the candidate set $CS$ with $Remain$ (lines 15-16), and continue finding Pareto frontiers within $Remain$. The time complexity of Algorithm~\ref{alg:find-pf} is $O(|CS|\log(|CS|)+l|CS|)$, where $l$ is a constant. %given below. 
The proof is straightforward and therefore omitted.

\revision{\section{Normal Vector Query Study}}
\noindent\revision{Here, we present the experimental results for the normal vector query case, where $q.\alpha = 0$ and $1$. It is worth noting that such a case is not a common scenario for \hvq. For example, \cite{liu2023} finds that the optimal weight for most queries ranges from 0.1 to 0.4. Given that our proposed \method is tailored for \hvq, it is as expected that \method performs worse than the ideal method, \textsf{oracle}, which constructs separate HNSW indexes for each modality and conducts searches within the corresponding index (as detailed in Section 5.1). Our expectation is that it should outperform the existing baselines for \hvq. %Our expectation is to ensure \method's robustness under extreme conditions, i.e., it should not perform worse than other practical baselines. 
The experimental results in Figure~\ref{fig:exp-01}
show that \method
outperforms the existing baselines of \hvq. 
The results demonstrate that our proposed DEG achieves the best performance compared to the baselines. %comparable to the best-performing baselines. 
Specifically, \textsf{HNSW}$_\textsf{F}$ shows competitive performance at $\alpha = 1$ but declines significantly when $\alpha = 0$. A similar pattern is observed for \textsf{HNSW}$_\textsf{O}$, aligning with our observation in Section 4.2. The explanation can be found in Section 4.2.1, observation (2). \textsf{HNSW}$_\textsf{M}$ performs well at both $\alpha = 0$ and $\alpha = 1$ because its two indexes are constructed specifically under the two values. DEG consistently outperforms the performance of \textsf{HNSW}$_\textsf{M}$, demonstrating its capability to handle normal vector queries. %extreme query cases. %Moreover, when 
As expected, the ideal method \textsf{HNSW}$_\textsf{Or}$ outperforms \method, along with all other baselines.}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.45\textwidth]{fig/exp_qps_recall_01_openimage.pdf}
\caption{\revision{The accuracy-efficiency trade-off results for the extreme query.}}
\label{fig:exp-01}
\end{figure}




\section{Sensitivity Study}
The complete experimental results of the sensitivity study are presented in Figure~\ref{fig:exp-acc-eff-para-sensitivity-full}, confirming our previous statement.



\begin{table}[!htbp]
    \centering
    \caption{\ready{The Indexing Time for the \textsf{OpenImage} Dataset (Min).}}
    \label{tab:index-time}
    \renewcommand{\arraystretch}{1.2} % 控制行高
    \setlength{\arrayrulewidth}{0.5pt}
    \begin{tabularx}{1.0\textwidth}{>{\centering}m{8cm}*{4}{X}}
    \toprule[1pt]
    $M, ef_{constrution}$ & \textsf{HNSW}$_{\textsf{F}}$ & \textsf{HNSW}$_{\textsf{M}}$ & \textsf{HNSW}$_{\textsf{O}}$ & \method\\
    \midrule[1pt]
    $40, 200$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $10, 50$ for \textsf{HNSW}$_{\textsf{O}}$ & 40 & 45 & 47 & 75 \\
    $50, 250$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $12, 60$ for \textsf{HNSW}$_{\textsf{O}}$ & 46 & 51 & 58 & N/A %120 
    \\
    $60, 300$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $14, 70$ for \textsf{HNSW}$_{\textsf{O}}$ & 52 & 58 & 70 & N/A %166
    \\ 
    $70, 350$ ffor \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $16, 80$ for \textsf{HNSW}$_{\textsf{O}}$ & 58 & 62 & 78 & N/A \\ 
    $80, 400$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $18, 90$ for \textsf{HNSW}$_{\textsf{O}}$ & 62 & 66 & 82 & N/A \\ 
    $20, 100$ for \textsf{HNSW}$_{\textsf{O}}$ & N/A & N/A & 90 & N/A \\ 
    $30, 150$ for \textsf{HNSW}$_{\textsf{O}}$ & N/A & N/A & 120 & N/A \\ 
    $40, 200$ for \textsf{HNSW}$_{\textsf{O}}$ & N/A & N/A & 176 & N/A \\ 
    \bottomrule[1pt]
  \end{tabularx}
\end{table}



\begin{table}[!htbp]
    \centering
    \caption{\ready{The Memory Consumption for the \textsf{OpenImage} Dataset (MB).}}
    \label{tab:index-memory}
    \renewcommand{\arraystretch}{1.2} % 控制行高
    \setlength{\arrayrulewidth}{0.5pt}
    \begin{tabularx}{1.0\textwidth}{>{\centering}m{8cm}*{4}{X}}
    \toprule[1pt]
    $M, ef_{constrution}$ & \textsf{HNSW}$_{\textsf{F}}$ & \textsf{HNSW}$_{\textsf{M}}$ & \textsf{HNSW}$_{\textsf{O}}$ & \method\\
    \midrule[1pt]
    $40, 200$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $10, 50$ for \textsf{HNSW}$_{\textsf{O}}$ & 3220 & 3412 & 3614 & 4203 \\
    $50, 250$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $12, 60$ for \textsf{HNSW}$_{\textsf{O}}$ & 3238 & 3418 & 3639 & N/A %4419 
    \\
    $60, 300$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $14, 70$ for \textsf{HNSW}$_{\textsf{O}}$ & 3253 & 3420 & 3654 & N/A %4571 
    \\ 
    $70, 350$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $16, 80$ for \textsf{HNSW}$_{\textsf{O}}$  & 3271 & 3443 & 3663 & N/A \\ 
    $80, 400$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $18, 90$ for \textsf{HNSW}$_{\textsf{O}}$ & N/A%3290 
    & N/A%3450 
    & 3759 & N/A \\ 
    $20, 100$ for \textsf{HNSW}$_{\textsf{O}}$ & N/A & N/A & 3793 & N/A \\ 
    $30, 150$ for \textsf{HNSW}$_{\textsf{O}}$ & N/A & N/A & 3879 & N/A \\ 
    $40, 200$ for \textsf{HNSW}$_{\textsf{O}}$ & N/A & N/A & 4080 & N/A \\
    \bottomrule[1pt]
  \end{tabularx}
\end{table}



\begin{table}[!htbp]
    \centering
    \caption{\ready{Average number of edges per node for each index in the \textsf{OpenImage} dataset.}}
    \label{tab:index-edge-number}
    \renewcommand{\arraystretch}{1.2} % 控制行高
    \setlength{\arrayrulewidth}{0.5pt}
    \begin{tabularx}{1.0\textwidth}{>{\centering}m{8cm}*{4}{X}}
    \toprule[1pt]
    $M, ef_{constrution}$ & \textsf{HNSW}$_{\textsf{F}}$ & \textsf{HNSW}$_{\textsf{M}}$ & \textsf{HNSW}$_{\textsf{O}}$ & \method\\
    \midrule[1pt]
    $40, 200$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $10, 50$ for \textsf{HNSW}$_{\textsf{O}}$ & 24 & 42 & 35 & 37 \\
    $50, 250$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $12, 60$ for \textsf{HNSW}$_{\textsf{O}}$ & 31 & 47 & 44 & N/A %4419 
    \\
    $60, 300$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $14, 70$ for \textsf{HNSW}$_{\textsf{O}}$ & 32 & 51 & 51 & N/A %4571 
    \\ 
    $70, 350$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $16, 80$ for \textsf{HNSW}$_{\textsf{O}}$  & 32 & 51 & 63 & N/A \\ 
    % $80, 400$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $18, 90$ for \textsf{HNSW}$_{\textsf{O}}$ & 38 & 56 & 72 & N/A \\ 
    $80, 400$ for \textsf{HNSW}$_{\textsf{F}}$, \textsf{HNSW}$_{\textsf{M}}$ and $18, 90$ for \textsf{HNSW}$_{\textsf{O}}$ & N/A & N/A & 72 & N/A \\     
    $20, 100$ for \textsf{HNSW}$_{\textsf{O}}$ & N/A & N/A & 84 & N/A \\ 
    $30, 150$ for \textsf{HNSW}$_{\textsf{O}}$ & N/A & N/A & 95 & N/A \\ 
    $40, 200$ for \textsf{HNSW}$_{\textsf{O}}$ & N/A & N/A & 117 & N/A \\ 
    \bottomrule[1pt]
  \end{tabularx}
\end{table}





\section{{Different Hyperparameter Study}}
\ready{In Table~\ref{tab:index-time}, Table~\ref{tab:index-memory}, and Table~\ref{tab:index-edge-number}, we report the construction time, memory usage, and average number of edges per node for each baseline index under various hyperparameter settings on the \textsf{OpenImage} dataset, and compare these results with those of our proposed \method configured with $M = 40$ and $ef_{constrution} = 200$. Notably, we optimize the implementation of our proposed \method by using integers instead of floats to store the active range for each edge, thereby reducing memory consumption. We have the following findings: (1) For the baselines \textsf{HNSW}$_{\textsf{F}}$ and \textsf{HNSW}$_{\textsf{M}}$, increasing $M$ and $ef_{construction}$ does not lead to a significant increase in memory consumption. This is because the memory usage is primarily dominated by the high-dimensional vectors, and the average number of edges per node for these indexes remains relatively stable. We do not increase the hyperparameters $M$ and $ef_{construction}$ further, as search performance does not show any additional improvement. We attribute this phenomenon to the lack of increase in the average number of edges.
%The additional memory overhead of the proposed \method arises from its more complex index structure, primarily due to multiple active ranges associated with each edge. 
(2) For baseline \textsf{HNSW}$_{\textsf{O}}$, as $M$ and $ef_{construction}$ increase, the construction time and memory consumption of this baseline increase significantly. %We observe that the construction time of \textsf{HNSW}$_\textsf{M}$ and \textsf{HNSW}$_\textsf{F}$ at $M = 80$ and $ef_{construction} = 400$ is comparable to that of our proposed \method at $M = 40$ and $ef_{construction} = 200$. We do not further increase $M$ and $ef_{construction}$, as the search performance shows no additional improvement. 
We increase the hyperparameters of \textsf{HNSW}$_\textsf{O}$ to $M = 40$ and $ef_{construction} = 200$, but do not raise them further, as its memory consumption is comparable to our method, while construction time has more than doubled compared to our method. }

\ready{In Figure~\ref{fig:exp-acc-eff-para-different-full}, we compare our proposed \method, configured with $M = 40$ and $ef_{construction} = 200$, to \textsf{HNSW}$_\textsf{M}$ and \textsf{HNSW}$_\textsf{F}$, both set to $M = 70$ and $ef_{construction} = 350$, as well as \textsf{HNSW}$_\textsf{O}$, configured with $M = 40$ and $ef_{construction} = 200$. %For \textsf{HNSW}$_\textsf{O}$, we increase the hyperparameters of \textsf{HNSW}$_\textsf{O}$ to $M = 30$ and $ef_{construction} = 150$. We do not further increase them as the construction time is already significantly higher than that of our proposed \method. The experimental results are shown in Figure~\ref{fig:exp-acc-eff-para-different-full}. 
The results demonstrate that our proposed \method consistently achieves superior performance under different hyperparameters. Specifically, our proposed \method consistently outperforms \textsf{HNSW}$_\textsf{M}$ and \textsf{HNSW}$_\textsf{F}$. Compared to \textsf{HNSW}$_\textsf{O}$, our proposed \method achieves similar performance while requiring only half the time for index construction.} 




\begin{figure*}[!htbp]
\centering
\subcaptionbox{\revision{$M=40, ef_{constrution}=200$ ($M=10, ef_{construction}=50$ for \textsf{HNSW}$_{\textsf{O}}$)}\label{fig:exp-openimage-40-200-full}}{
\includegraphics[width=1.0\textwidth]{fig/exp_qps_recall_40_200_openimage_full.pdf}
}
\subcaptionbox{\revision{$M=50, ef_{constrution}=250$ ($M=12, ef_{construction}=60$ for \textsf{HNSW}$_{\textsf{O}}$)\label{fig:exp-openimage-50-250-full}}}{
\includegraphics[width=1.0\textwidth]{fig/exp_qps_recall_50_250_openimage_full.pdf}
}
\subcaptionbox{\revision{$M=60, ef_{constrution}=300$ ($M=14, ef_{construction}=70$ for \textsf{HNSW}$_{\textsf{O}}$)}\label{fig:exp-openimage-60-300-full}}{
\includegraphics[width=1.0\textwidth]{fig/exp_qps_recall_60_300_openimage_full.pdf}
}
% \vspace{-1em}
\caption{\revision{The accuracy-efficiency trade-off results on \textsf{OpenImage} dataset with varying $M$ and $ef_{constrution}$.}}
% \vspace{-1em}
\label{fig:exp-acc-eff-para-sensitivity-full}
\end{figure*}



\begin{figure*}[!htbp]
\centering
\subcaptionbox{\revision{$M=40, ef_{constrution}=200$ for \method, $M=80, ef_{construction}=400$ for \textsf{HNSW}$_{\textsf{F}}$ and \textsf{HNSW}$_{\textsf{M}}$, and $M=30, ef_{construction}=150$ for \textsf{HNSW}$_{\textsf{O}}$}\label{fig:exp-openimage-40-200-full}}{
\includegraphics[width=1.0\textwidth]{fig/exp_qps_recall_diff_openimage.pdf}
}
\caption{\ready{The accuracy-efficiency trade-off results on \textsf{OpenImage} dataset with different $M$ and $ef_{constrution}$.}}
% \vspace{-1em}
\label{fig:exp-acc-eff-para-different-full}
\end{figure*}


\begin{figure*}[!htbp]
\centering
\includegraphics[width=1.0\textwidth]{fig/exp_qps_recall_openimage_drng_albation_full.pdf}
% \vspace{-1em}
\caption{\revision{Ablation Study for the D-RNG's pruning strategy on the \textsf{OpenImage} dataset.}}
\label{fig:exp-drng-ablation-full}
\end{figure*}

\begin{figure*}[!htbp]
% \vspace{-1em}
\centering
\includegraphics[width=1.0\textwidth]{fig/exp_qps_recall_openimage_active_albation_full.pdf}
% \vspace{-1em}
\caption{\revision{Ablation Study for the Active Range on the \textsf{OpenImage} dataset.}}
\label{fig:exp-active-range-ablation-full}
\end{figure*}

\begin{figure*}[!htbp]
% \vspace{-1em}
\centering
\includegraphics[width=1.0\textwidth]{fig/exp_qps_recall_openimage_gps_albation_full.pdf}
% \vspace{-1em}
\caption{\revision{Ablation Study for the candidate acquisition method on the \textsf{OpenImage} dataset.}}
\label{fig:exp-gps-ablation-full}
\end{figure*}

\begin{figure*}[!htbp]
% \vspace{-1em}
\centering
\includegraphics[width=1.0\textwidth]{fig/exp_qps_recall_openimage_seed_albation_full.pdf}
% \vspace{-1em}
\caption{\revision{Ablation Study for the Edge Seed Acquisition method on the \textsf{OpenImage} dataset.}}
% \vspace{-1em}
\label{fig:exp-seed-ablation-full}
\end{figure*}

\section{Ablation Study}
The complete experimental results of the ablation study are presented from Figure~\ref{fig:exp-drng-ablation-full} to Figure~\ref{fig:exp-seed-ablation-full}, which is consistent with our previous statement.



% % Here, we examine the performance changes of \method and the baselines as hyperparameters vary. The three key hyperparameters are the candidate set size $ef_{construction}$, the maximum number of edges per node $M$, and the search set size $ef_{search}$, which is set to 40, 200, and 10 by default. $ef_{search}$ controls the accuracy-efficiency tradeoff during the query phase and has been adjusted from 10 to 200 in Section~\ref{sec:exp-search-performance} to demonstrate the search performance of the built index. Therefore, we investigate how $M$ and $ef_{construction}$ impacts the built index's quality. Specifically, we increase $M$ and $ef_{constrution}$ from 40, 200 to 50, 250, then to 60, 300. Note that we maintain the same hyperparameters across both the baselines and our proposed \method for fair comparisons. 



% % The results demonstrate that: 
% % \textbf{(1) For different number of edges $M$ and candidate set size $ef_{construction}$, our proposed \method shows similar advantages as in previous experiments.} This validates the robustness of our proposed \method regarding different hyperparameters. 
% % \textbf{(2) Both baselines and our proposed \method exhibit similar performance for varying $M$ and $ef_{construction}$.} This indicates that for graph-based ANNS indexes, the key factors determining performance are not the hyperparameters but the edge selection strategy, as to be shown later.





\section{Semantic-aware Spatial keyword query}
\label{sec:appendix-stkq}
Recently, some studies~\cite{chenS2RtreePivotbasedIndexing2020, qianSemanticawareTopkSpatial2018, DBLP:conf/edbt/TheodoropoulosN24} have attempted to address the special case of the \hvq problem when one of the feature vectors has a small dimensionality (e.g., m = 2 or 3). However, these methods all suffer from significant efficiency issues. Details of these methods are discussed below.


\begin{itemize}[leftmargin=*]
    \item \textsf{NIQ~\cite{qianSemanticawareTopkSpatial2018}}: This method employs a Quadtree~\cite{finkelQuadTreesData1974} to index low-dimensional feature vectors ($o.s$) and an iDistance index~\cite{Jagadishidistance2005} to organize high-dimensional feature vectors ($o.e$) within each Quadtree leaf node. During the querying phase, it first traverses the Quadtree to find nearby leaf nodes and then uses the iDistance index within the leaf nodes to find exact results.
    \item \textsf{S2R~\cite{chenS2RtreePivotbasedIndexing2020}}: This method uses an R-tree~\cite{beckmann1990r} to index low-dimensional feature vectors and a pivot-based mechanism to map high-dimensional feature vectors to lower-dimensional vectors, which are then organized by another R-tree. During the querying phase, similar to the NIQ-Tree, it hierarchically traverses R-trees to locate exact results.
    \item \textsf{CSSI~\cite{DBLP:conf/edbt/TheodoropoulosN24}}: This method uses PCA to project high-dimensional feature vectors, clusters the low-dimensional and PCA-projected vectors separately with K-means, and forms hybrid clusters by combining the resulting clusters. The centroids and radii of the clusters are computed from the mean of the objects' feature vectors. During the querying phase, it routes and prunes clusters based on the distance between the query and cluster centroids, and produces exact results by reranking the visited objects.
    \item \textsf{CSSIA~\cite{DBLP:conf/edbt/TheodoropoulosN24}}: This method modifies CSSI by using the mean of PCA vectors for cluster centroids, while still using original vectors for reranking within clusters, thereby producing approximate results. It stops the search only when it believes no better candidates remain based on the distance to the PCA centroids, thereby lacking the accuracy-efficiency trade-off.
\end{itemize}
However, none of these methods can handle cases where both vectors are high-dimensional. Furthermore, our experiments show that these methods have time costs similar to those of brute-force search, making them unsuitable for retrieval tasks. 