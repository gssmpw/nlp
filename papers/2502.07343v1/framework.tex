\begin{figure*}[!t]
\begin{center}
% \vspace*{-1.5em}
\includegraphics[width=\textwidth]{fig/framework.pdf}
% \vspace*{-2.5em}
\caption{The framework of the \method, including the index construction phase and index search phase.}
% \vspace*{-1.5em}
\label{fig:framework}
\end{center}
\end{figure*}

\section{The \method Method}
\label{sec:method}
\subsection{Overview}
\label{src:overview}


To overcome the limitations of baselines, and address the three challenges discussed in Section~\ref{sec:intro}, we develop a new graph-based ANNS index, namely Dynamic Edge Navigation Graph (\method), which includes three components, each addressing a challenge. We proceed to give an overview of the three components. 

%: (1)  How to compute the candidate neighbor set for each node; (2) How to determine edges from the
% candidate set; and (3) How to select the seed for the graph index, respectively. 

%To address the first challenge of computing an edge candidate set for each node, which comprises the approximate nearest neighbors at varying $\alpha$ values, we propose a greedy Pareto frontier search algorithm, called the GPS algorithm (%will be detailed in 
%Section~\ref{sec:candidate-acquisition}). It treats the distance of each individual vector as an objective function and then proposes to use the Pareto frontier of this multi-objective optimization problem as the edge candidate set.
% \pasq{


To tackle the first challenge of computing a candidate neighbor set for each node, %which includes its approximate nearest neighbors at different $\alpha$ values, 
we propose %a greedy Pareto frontier search algorithm, called 
the GPS algorithm. The high-level idea of the GPS algorithm can be summarized in two points: 1) Finding the nearest neighbor of a node at any $\alpha$ value is equivalent to solving the problem of minimizing the distance function (Equation~\ref{eq:hybird_distance}) at that $\alpha$ value. Finding solutions for each possible $\alpha \in [0, 1]$ is unfeasible. Then, we treat it as a multi-objective optimization problem, with each vector distance as an objective. % Since the distance function depends on $\alpha$ and 
%each 
% individual vector distances, it is a multi-objective optimization problem, with each vector distance as an objective.
%function. 
To solve this, we 
%use 
compute the Pareto frontier as the solution, i.e., the neighbor candidate set, ensuring the nearest neighbor for any $\alpha$ value is included; 2) Finding the exact Pareto frontier is expensive.
%, with a time complexity of $O(N\log(N)+N(d+m))$ for each node. 
Therefore, the GPS algorithm aims to find approximate Pareto frontiers to reduce index construction costs. The core idea of the GPS algorithm is that a neighborâ€™s neighbor is likely to be a neighbor. By iteratively exploring neighbors of neighbors, it continuously optimizes the approximate Pareto frontiers.
%with a time complexity of $O(\log(N))$.

To address the second challenge of determining edges from the candidate set, we propose a dynamic edge pruning strategy that utilizes the RNG's pruning strategy to dynamically prune edges in \method.
%, enabling the RNG pruning strategy to be utilized to dynamically prune edges in \method. 
%This strategy involves two steps: 
Our key idea is as follows: 1) We assign an active range to each edge, covering the suitable $\alpha$ values for that edge, within which that edge will not be pruned by the RNG's property; and 2) at query time, we use an edge for routing only if the query's $\alpha$ value intersects with its active range; otherwise, we ignore it. To achieve this, we take $\alpha$ into account in the pruning strategy of RNG and compute an active range for each edge such that for any $\alpha$ value, the remaining graph formed by the activated edges satisfies the RNG's property, thereby ensuring high performance. 

To address the third challenge of finding the appropriate seed for the graph index, we propose a new edge seed method. This method uses nodes that are farthest from the center for varying $\alpha$ values as seed, i.e., edge seed. Compared to using multiple approximate centroids, edge seed ensures that they are far from each other. In greedy search, only edge nodes close to the query are activated, while others are ignored due to their large distance, thus avoiding the efficiency issue caused by multiple start nodes.


Figure~\ref{fig:framework} illustrates the framework of \method, comprising the index construction phase and the index search phase. In the index construction phase, \method builds the graph index by continuously inserting new nodes, similar to HNSW. It performs the GPS algorithm over the partially built graph to obtain candidate neighbors of the inserted node (shown as green nodes). Then, it utilizes the dynamic pruning strategy to prune some candidate edges, deriving the final dynamic edges (shown as dashed lines), where each edge is assigned an active range. Finally, it updates the edge seed (shown as red nodes). In the index search phase, the edge seed is used as the starting node, and a variant of the greedy search algorithm is employed, which dynamically skips some edges based on their active ranges and the query's $\alpha$ value. %For instance, the dashed line with an active range of $[0, 0.3]$ is skipped since $q.\alpha = 0.9$.

In the rest of this section, we first present the three main components of \method, namely (1) a greedy Pareto frontier search algorithm, called the GPS algorithm (Section~\ref{sec:candidate-acquisition}); (2) a dynamic edge pruning strategy (Section~\ref{sec:D-RNG}); and (3) an edge seed method (Section~\ref{sec:construction}). Then we present the search algorithm in Section~\ref{sec:search}.



%resulting in a multi-objective optimization problem where the candidate set is defined by the Pareto frontier.
% }
% In section~\ref{sec:D-RNG}, we present a novel Dynamic Relative Neighborhood Graph (D-RNG), which maintains the properties of the Relative Neighborhood Graph under varying $\alpha$. %In section~\ref{sec:candidate-acquisition}, we first propose to acquire the Pareto frontiers of each node as the edge candidate set. To address the efficiency issue of finding exact Pareto frontiers, we develop a novel greedy Pareto frontiers search algorithm, called GPS algorithm, to find approximate Pareto frontiers. 
% \pasq{In section \ref{sec:candidate-acquisition}, we show how to acquire the Pareto frontiers of each node to use as the edge candidate set. To achieve this in an efficient way, we introduce a novel Greedy Pareto Frontiers search algorithm, or GPS algorithm for short.}
% In section~\ref{sec:construction}, based on the proposed D-RNG and the GPS algorithm, we formulate the \method's pruning strategy. Additionally, we propose a new edge seed acquisition method, which uses the nodes furthest from the graph center as the seed. Finally, we summarize the index phase of \method based on the aforementioned modules. In section~\ref{sec:search}, we detail the search algorithm of \method. In the end, we analyze the time and space complexity of \method.



% \begin{figure}[!htbp]
% \begin{center}
% \includegraphics[width=0.45\columnwidth]{fig/pareto-frontier.pdf}
% \vspace{-1em}
% \caption{The illustration of the Pareto Frontier.}
% \vspace{-1em}
% \label{fig:pareto}
% \end{center}
% \end{figure}
\vspace*{-0.5em}
\subsection{Candidate Neighbor Acquisition}
\label{sec:candidate-acquisition}
% \pasq{
As previously discussed, finding the nearest neighbor of a node at any $\alpha$ value is equivalent to solving a multi-objective optimization problem.
% }
%\ziqi{%As discussed before, finding the nearest neighbor of a point at any $\alpha$ value is equivalent to a multi-objective optimization problem.
We formalize this problem as follows: Given an object $p \in D$, for other objects $x \in D\setminus \{p\}$, the multi-objective function is defined as $f(p, x): x \rightarrow \mathbb{R}^2$, where $f_1(p, x) = \delta_e(p, x)$ and $f_2(p, x) = \delta_s(p, x)$. 
We propose using the Pareto frontier as the solution to this problem. Next, we introduce the concept of the Pareto frontier and explain why it is used as the 
%solution and 
candidate set.
%}

% As discussed in Section~\ref{sec:motivations}, one challenge is how to acquire the candidate set for each node, where the candidate set should contain the approximate nearest neighbors for varying $\alpha$. Here, we propose using the Pareto Frontiers of each node as the edge candidate set, ensuring that the nearest neighbors for varying $\alpha$ are always included within the set. We first introduce the concept of the Pareto Frontier.


%existing RNG-based ANNS typically acquire the $ef_{construction}$ approximate nearest neighbors for each node as the edge candidate set, and then choose the final edges from this set to approximate the RNG and built ANNS index. However, in our context, as $\alpha$ changes, the nearest neighbors of each node will also change, making it challenging to determine an edge candidate set that adapts to the varying $\alpha$. Therefore, we propose a novel index construction method based on the Pareto Frontier. We first introduce the concept of the Pareto Frontier.

\noindent\textbf{Pareto Frontier~\cite{ma2020efficient}.} Consider a dataset $D$ and a multi-objective optimization problem described by $f(x)\colon x \rightarrow \mathbb{R}^c$, where $x \in D$. Each function $f_i(x) \rightarrow \mathbb{R}$ represents the objective function of the $i$-th task to be minimized, with $i\in C$ and $C = \{1, 2, \cdots, c\}$. For any $x, y \in D$, $x$ dominates $y$ if and only if (1) $\forall i \in C$, $f_i(x) \leq f_i(y)$ and (2) $\exists j \in C$, $f_j(x) < f_j(y)$. An object $x \in D$ is considered Pareto optimal if no other object in $D$ dominates $x$. The Pareto frontier, also called the Pareto set, comprises all Pareto optimal objects in $D$.

% \ziqi{
Figure~\ref{fig:pareto} illustrates the Pareto frontier. For a node $p$, $\delta_s(x, p)$ and $\delta_e(x, p)$ denote the distance of each individual vector. 
The nodes within layer 1 lie at the bottom of the graph and constitute the Pareto frontier described above. Nodes in other layers will also be collected into the candidate neighbor set, as will be explained later. %The role of the other layers will be explained later. 
Next, we present the theorem that explains why the Pareto frontier can be used as the candidate set.
%Then we have the following theorem, which explains why the Pareto frontier can be used as the solution and candidate set.
% }

% \vspace*{-0.5em}
\begin{theorem}
\label{theorem:nearest}
% Given an object $p \in D$, for other object $x \in D\setminus \{p\}$, we formulate a multi-objective problem $f(p, x): x \rightarrow \mathbb{R}^2$, where $f_1(p, x) = \delta_e(p, x)$ and $f_2(p, x) = \delta_s(p, x)$. 
We denote the Pareto Frontier of the multi-objective function $f(p, x)$ as $PF(D, p) \subset D\setminus\{p\}$. For any $\alpha \in [0,1]$, %in Equation~\ref{eq:hybird_distance}, 
the nearest neighbor of $p$ is contained in $PF(D, p)$. 
\end{theorem}
\vspace*{-0.5em}
% \begin{proof}
% \label{theorem:nearest_proof} 
% For any $y \in D$ but $y \notin PF(D, p)$, there must exist $x \in PF(D, p)$ that dominates $y$; otherwise, $y$ would have been added to $PF(D, p)$ as well according to the definition. Consequently, we have $\delta_e(p, x) \leq \delta_e(p, y)$ and $\delta_s(p, x) \leq \delta_s(p, y)$, with at least one inequality being strict. Given that $\alpha \in [0,1]$ and the Euclidean distance is non-negative, we can derive that for any $\alpha$, $Dist(p, x) < Dist(p, y)$.
% \end{proof}
Due to page limitations, the proof is provided in the appendix. Therefore, we can compute $PF(D, p)$ for each object $p \in D$ as the 
%solution and 
candidate set, ensuring the nearest neighbors are always within $PF(D, p)$ when $\alpha$ varies. However, in a dataset with millions of objects, $PF(D, p)$ may contain only a dozen objects (e.g., 10), while we usually need hundreds of objects (e.g., 200) as the edge candidates to select the final edges. To address this, we can choose multiple layers of $PF(D, p)$. Specifically, after finding the $PF(D, p)$, we remove the objects within $PF(D, p)$ from the dataset and search for a new $PF(D, p)$ until we obtain enough edge candidates. %\ziqi{
Figure~\ref{fig:pareto} illustrates this, where layers 2 and 3 will also be collected as part of the candidate set.
%}

The positive news is that for a given node $p$, with the distances $\delta_e(p, x)$ and $\delta_s(p, x)$ from other nodes to $p$ already calculated, finding $PF(D, p)$ is equivalent to %solving a long-standing problem in the database community: 
finding the two-dimensional skyline~\cite{borzsony2001skyline}, which has efficient solutions~\cite{khalefa2008skyline,zhiyonghuang2006continuous,papadias2005progressive,kalyvas2017survey}. %\ziqi{
However, these methods cannot be efficiently applied to our problem directly. 
%In our context, the attributes of each object are dynamic since the distances from each inserted node to other nodes are different. 
%This makes traditional skyline methods that rely on static data attributes unsuitable. 
%Although dynamic skyline queries exist~\cite{kalyvas2017survey}, which treats the dynamic distance between queries and objects as attributes, these methods are designed for low-dimensional coordinates and use indexes like R-Tree to handle such queries, making them unsuitable for high-dimensional cases. 
% In our case, the distances from each inserted node to other nodes are completely different, and computing the high-dimensional vector distances from one node to all other nodes is very expensive. This characteristic makes most solutions, such as index-based methods and divide-and-conquer methods, unsuitable. 
%For instance, \cite{borzsony2001skyline} proposes a sorting-based method, demonstrating superior performance on the two-dimensional skyline problem. However, the sorting process along with the computation cost in our context has a time complexity of $O(N(d+m) + N\log(N))$, which is expensive.
% }

%Then we demonstrate how to find the $l$-layer $PF(D, p)$ using existing algorithms.

%Using the existing algorithm, we can efficiently find the $l$-layer $PF(D, p)$ given the precomputed $\delta_e(p, x)$ and $\delta_s(p, x)$.

\begin{figure}[!t]
\begin{center}
% \vspace*{-1em}
\subcaptionbox{Pareto Frontier.\label{fig:pareto}}{
\includegraphics[width=0.35\columnwidth]{fig/pareto-frontier.pdf}
}
% \subcaptionbox{DEG.\label{fig:arng}}{
% \includegraphics[width=0.45\columnwidth]{fig/A-RNG.pdf}
% }
% \vspace*{-1em}
\subcaptionbox{Edge seed method.\label{fig:seed}}{
\includegraphics[width=0.35\columnwidth]{fig/seed.pdf}
}
% \vspace*{-1em}
\caption{Figure~\ref{fig:pareto}~illustrates the Pareto frontiers and Figure~\ref{fig:seed}~illustrates the difference between the edge seed acquisition method and the multiple centroids method.}
% \vspace*{-2em}
\end{center}
\end{figure}

% \begin{algorithm}[t]
%     \caption{Finding Pareto Frontiers}
%     \small
%     \label{alg:find-pf}
%     \KwIn{A candidate pool $CS$ for node $p$, candidate pool size $ef_{construction}$}    
%     \KwOut{$l$ layer Pareto frontiers $\{PF^{1}(p), \cdots, PF^{l}(p)\}$ for node $p$, where $\sum_{i = 1}^l |PF^{i}(p)| \leq ef_{construction}$}
%     \BlankLine
    
%     % Define the procedure
%     \SetKwFunction{FindPF}{\textsc{FindPF}}
%     \SetKwProg{Fn}{Procedure}{:}{}
%     \Fn{\FindPF{$CS, ef_{construction}$}}{
%         % Call the procedure
%         \text{Sort} $x \in CS$ \text{in ascending order of} $\delta_s(x, p)$ \text{to} $p$\;
%         $Res \leftarrow \emptyset$ \tcp*{\textsf{result set}}
%         \While{$|Res| < ef_{construction}$}{
%             $PF \leftarrow \emptyset$ \tcp*{\textsf{current pareto frontier}}
%             $Remain \leftarrow \emptyset$ \tcp*{\textsf{remaining candidates}}
%             $PrevEmbDist \leftarrow \infty$ \tcp*{\textsf{smallest $\delta_e(x, p)$, $x \in PF$}}   
%             \ForEach{$x \in CS$} {
%                 \If{$\delta_e(x, p) < \text{PrevEmbDist}$} {
%                     % \PFs[\Layer].\text{add}(point)\;
%                     $PF.add(x)$\;
%                     $PrevEmbDist \leftarrow \delta_e(x, p)$\;
%                 }
%                 \Else {
%                     $Remain.add(x)$\;
%                 }
%             }
%             \If{$|PF| + |Res| < ef_{construction}$} {
%                 $Res.add(PF)$\;
%                 $CS \leftarrow Remain$\;
%             }
%             \Else {
%                 \textbf{break}\;
%             }        
%         }
%         \Return $Res$ \;
%     }
% \end{algorithm}

% Algorithm~\ref{alg:find-pf} summarizes the procedure of finding $l$-layer $PF(D, p)$ using the existing algorithm~\cite{borzsony2001skyline}. Initially, the entire set is sorted in ascending order of $\delta_s(x, p)$, and the result set $Res$ is initialized (lines 2-3). Then, it repeatedly finds the Pareto frontier and removes it from the candidate set until the result set reaches the size bound (lines 4-18). 
% Specifically, in each iteration, two empty sets, $PF$, and $Remain$, are initialized to store the Pareto frontier and remaining nodes, respectively. A variable $PrevEmbDist$ is also initialized to store the $\delta_e(p, x)$ of the last added element in $PF$ (lines 5-7). The candidate set $CS$ is scanned to check if $\delta_e(p, x)$ is smaller than $PrevEmbDist$, determining if it should be added to $PF$ (lines 8-9). If added, $PrevEmbDist$ is updated; otherwise, the element is added to $Remain$ (lines 10-13). This process, proven correct in~\cite{borzsony2001skyline}, ensures that the next element added to $PF$ will have a smaller $\delta_e(p, x)$ value; otherwise, it is dominated by the previous element in $PF$ due to the sorted order. % Only $\delta_e(p, x)$ needs to be checked for dominance by existing points in $PF$. The correctness of this process has been proven in~\cite{borzsony2001skyline}. Since the elements are sorted in ascending order of $\delta_s(p, x)$, the next element in $CS$ will have a larger $\delta_s(p, x)$ value than the elements already added to $PF$. Therefore, we only need to check $\delta_e(p, x)$ to determine whether it is dominated by existing points in $PF$. 
% Finally, we check if the result set size exceeds the candidate set size bound $ef_{construction}$ (line 14). If it does, we break the loop and return $Res$ (lines 17-18). Otherwise, we add $PF$ to $Res$, update the candidate set $CS$ with $Remain$ (lines 15-16), and continue finding Pareto frontiers within $Remain$. The time complexity of Algorithm~\ref{alg:find-pf} is $O(|CS|\log(|CS|)+l|CS|)$, where $l$ is a constant. %given below. 
% The proof is straightforward and therefore omitted.

% \begin{lemma}
% \label{lem:skyline}
% The time complexity of Algorithm~\ref{alg:find-pf} is $O(|CS|\log(|CS|)+l|CS|)$, where $l$ is a constant. %approximately equal to $\frac{ef_{construction}}{10}.
% \end{lemma}

% However, %even with Algorithm~\ref{alg:find-pf}, 
% finding the exact $PF(D, p)$ %for every object $p \in D$ 
% is not practical. For each node, calculating its distance to other nodes and using Algorithm~\ref{alg:find-pf} to obtain the exact Pareto frontiers has a time complexity of $O(N(d+m+l)+N\log(N))$, which is too expensive. %Consequently, finding exact Pareto frontiers for the entire graph has a time complexity of $O(N^2(d+m+l)+N^2\log(N))$, which is too expensive.
% To address this, %following previous studies~[xx], 
% we %propose to build the graph incrementally and 
% introduce a novel Greedy Pareto Frontier Search algorithm (called GPS algorithm) to search for approximate Pareto frontiers on a partially built graph.


%This method also alleviates the data discrepancy issue discussed in Section~\ref{sec:motivations}, as objects with small $\delta_e(p, x)$ and $\delta_s(p, x)$ will be both included in the edge candidate set will both be included in the edge candidate set rather than being dominated by one vector.

% \noindent\textbf{Candidate Set Acquisition.} As discussed in Section~\ref{sec:motivations}, one challenge in index construction is acquiring the edge candidate set $CS$ for each node. Existing RNG-based indexes typically acquire the $ef_{construction}$ approximate nearest neighbors for each node as the edge candidate set, and then choose the final edges from this set to approximate the RNG. %This is because nodes that are farther away from the given node are more likely to be pruned by the RNG's pruning strategy (with a larger red lune in Figure~\ref{fig:rng}). 
% However, in our context, as $\alpha$ changes, the distances from neighbors to nodes may vary significantly, making it challenging to determine an edge candidate set that adapts to the varying $\alpha$. Therefore, we propose a novel index construction method based on the Pareto Frontier. We first introduce the concept of the Pareto Frontier.



\noindent\textbf{Greedy Pareto Frontier Search.} %To address the aforementioned efficiency issue, we propose a novel Greedy Pareto Frontier Search algorithm (Algorithm~\ref{alg:pareto-frontier-search}), called the GPS algorithm below, to find approximate multi-layer Pareto-frontiers. 
% \ziqi{
To address this, we introduce a novel Greedy Pareto Frontier Search algorithm (called GPS algorithm) for searching approximate Pareto frontiers on a partially built graph. The core idea of the GPS algorithm is that the neighbor of a neighbor is more likely to be a neighbor. It continuously explores the neighbors of neighbors and searches for the multi-layer Pareto frontier (skylines) within this small set. This approach allows us to obtain a high-quality approximate Pareto frontier and improve efficiency.
%a concept widely used and proven effective in the graph ANNS domain~\cite{DBLP:conf/www/DongCL11, fu2019fast}. 
% }
\begin{algorithm}[t]
    % \small
    \caption{\revision{GPS($G$, $q$, $ep$, $ef_{constrution}$)}}
    \small
    \label{alg:pareto-frontier-search}
    \KwIn{A partially built graph $G$, a query $q$, start node set $ep$, candidate pool size $ef_{construction}$}
    \KwOut{Multi-layer Pareto frontiers $\{PF^{1}(q), \cdots, PF^{l}(q)\}$ for $q$, where 
    $\sum_{i = 1}^l |PF^{i}(q)| \leq ef_{construction}$}
    
    \BlankLine
    
    % Define the procedure
    \SetKwFunction{GPS}{\textsc{GPS}}
    \SetKwProg{Fn}{Procedure}{:}{}
    \Fn{\GPS{$G, q, ep, ef_{constrution}$}}{
    
        $Res \leftarrow \emptyset$ \tcp*{\textsf{set of the pareto frontiers}}
        $Vis \leftarrow \emptyset$ \tcp*{\textsf{set of the visited nodes}}
        $Flag \leftarrow \emptyset$ \tcp*{\textsf{set of the explored nodes}}    
        % $Res.\text{add}(ep)$\;
        % $Vis.\text{add}(ep)$\;
        \ForEach{$v \in ep$}{
            $Res.\text{add}(v)$\; 
            $Vis.\text{add}(v)$\;
        }
        $Res \leftarrow FindPF(Res, ef_{constrution})$\;  
        % \ForEach{$v \in ep$} {
        %     % $dist_{e} \leftarrow  \delta_e(q, v)$\;
        %     % $dist_{s} \leftarrow \delta_s(q, v)$\; %\tcc*[r]{Compute $\delta_e(q, v)$ and $ \delta_s(q, v)$}
        %     % Push start nodes into the priority queue with distances and initial layer  
        %     $v.layer \leftarrow 1$ \tcp*[r]{\textsf{Initialize each start node's layer to 1}}
        %     $\mathcal{S}.\text{add}(v)$\; 
        %     %\; %\tcc*[r]{Initialize each start node with layer = 1.}
        %     $Vis.\text{add}(v)$ 
        % }
    
    
    \While{$|Res| < ef_{construction}$}{        
            % $k \leftarrow -1$ \;
            $NNS \leftarrow \emptyset$ \tcp*[r]{\textsf{set of unexplored new nodes}} 
            % \tcp{\textsf{find the nearest Pareto frontier with new node}}
                        %\tcp{\textsf{scan each layer of pareto frontier}}
    
            \ForEach{$S_k \in Res$} 
            {
                $NNS \leftarrow \{v \in \mathcal{S}_k \mid  v \notin Flag\}$\;                 
                \If{$NNS \neq \emptyset$}{
                    \textbf{break}\;        
                }
            }
            % $NNS \leftarrow \{v \in \mathcal{S} \mid  v \notin Flag\}$\; %\tcp*[r]{\textsf{set of unexplored new nodes}}      
            \If{$NNS = \emptyset$} 
            {
                \textbf{break}\;  
            }
            
            % \tcp{\textsf{The new node set within layer $k$ Pareto frontier}}       
    
            \ForEach{$u \in NNS$}{
                $Flag.\text{add}(u)$  \tcp*{\textsf{mark $u$ as old node}}
                \ForEach{$v \in neighbour(u)$}{
                    \If{$v \notin vis$}{
                        $Res.\text{add}(v)$\;
                        $Vis.\text{add}(v)$\;
                    }
                }
            }
            \tcp{\textsf{update the pareto frontiers using existing algorithm}} 
            % $\mathcal{S}.\text{update}()$ \;
            $Res \leftarrow FindPF(Res, ef_{constrution})$\;
        }
        \Return $Res$\;
        % \Return $\{PF^{1}(q), \cdots, PF^{l}(q)\}$\;
    }
\end{algorithm}

The detailed procedures of the GPS algorithm are summarized at Algorithm~\ref{alg:pareto-frontier-search}. %The GPS algorithm takes as input a query $q$, a constructed graph $G$, %where each node's neighbors are its approximate $l$-layer Pareto frontiers the start node set $ep$ of graph $G$, and a hyperparameter $ef_{constrution}$ to determine the size of the result set. 
It maintains a candidate set $Res$ to store the \revision{discovered approximate} Pareto frontiers, an indicator $Vis$ to mark whether a node was added to $Res$, and another indicator $Flag$ to mark whether a node's neighbors have been evaluated and added to $Res$ (lines 2-4).  \revision{If a node's neighbor is not evaluated,} we call it a new node in $Res$. The GPS algorithm first adds all the start nodes (the seed) into $Res$ as the  \revision{initial candidates} %Pareto frontier 
and marks them as visited (lines 5-7).  \revision{Then it organizes these initial candidates into multi-layer Pareto frontiers using a modification of existing algorithm~\cite{borzsony2001skyline}, referred to as $FindPF$ (line 8). Specifically, $FindPF$ repeatedly finds the Pareto frontier (skyline) within the candidate set $Res$ using the existing algorithm~\cite{borzsony2001skyline}, adds it to the result set, and removes it from the candidate set. When the result set reaches the size bound, it returns the first $l$ layer Pareto frontiers that satisfy the size limit.
%it discards the last Pareto frontier layer and returns the remaining multi-layer Pareto frontiers. %it returns the discovered multi-layer Pareto frontiers. 
Due to page limitations, the details of the algorithm are provided in the appendix.}
%Note that the size of the seed node $|ep|$ is typically much smaller than $ef_{construction}$. 
\revision{Next}, the GPS algorithm iteratively performs a greedy search over graph $G$ to optimize the discovered approximate %multi-layer 
Pareto frontiers (lines 9-23). Specifically, it scans each layer of the discovered approximate Pareto frontiers (line 11), collects the new nodes within that layer,  \revision{denoted as $\mathcal{S}_k$} (line 12), and breaks if the set is not empty (lines 13-14). %The reason 
We only collect new nodes from the nearest Pareto frontier rather than from the entire set to avoid the neighbor explosion issue and improve efficiency. If no new nodes exist in $Res$, then we break the loop (lines 15-16). This is because no new nodes within $Res$ can be used to improve the results further. Otherwise, we add the neighbors of the new nodes to $Res$, mark these new nodes as old nodes, and mark their neighbors as visited (lines 17-22). After that, we  \revision{optimize} the candidate set $Res$ by finding the multi-layer Pareto frontiers within $Res$ using $FindPF$,  \revision{and then updating $Res$ accordingly} %a modification of existing algorithm. %~\ref{alg:find-pf} \
(line 23). %is modified from existing algorithm~\cite{borzsony2001skyline}. 
% Specifically, it repeatedly finds the Pareto frontier (skyline) using the existing algorithm~\cite{borzsony2001skyline}, adds it to the result set, and removes it from the candidate set. When the result set reaches the size bound, it returns the multi-layer Pareto frontiers. Due to page limitations, the details of the algorithm are provided in the appendix.


% Through this heuristic method, we can evaluate a small subset of $G$ to derive high-quality approximate Pareto frontiers.

\noindent\textbf{Complexity Analysis.} %The build time complexity mainly comes from the incremental insertion of new nodes into the graph, where the \method uses the GPS algorithm and existing methods use the greedy search algorithm. 
The dominant factor in the GPS algorithm's time complexity is the search path length, which determines the number of evaluated objects. In an extreme scenario where each layer of the Pareto frontier contains only one node, the GPS algorithm becomes equivalent to the greedy search algorithm with the longest search path length. In other cases, the GPS algorithm performs a greedy search for different $\alpha$ values. Given the bounded size of the candidate set, the search path length is shorter. Therefore, the time complexity of the GPS algorithm is $O(\log(N))$, the same as the greedy search.




%This is confirmed in the experiments (Section~\ref{sec:exp-index-cost}), where \method shows comparable construction time to the baselines.

% \begin{table}[!t]
% \caption{Examples.} %to illustrate the challenge of applying RNG's pruning strategy in the HVQ problem.}
% \vspace{-1em}
% \centering
% \begin{tabular}{|p{0.07\textwidth}|p{0.05\textwidth}|p{0.05\textwidth}|p{0.05\textwidth}|p{0.05\textwidth}|p{0.05\textwidth}|p{0.05\textwidth}|}
% \hline
% Examples & $\delta_s(x, y)$ & $\delta_e(x, y)$ & $\delta_s(x, z)$ & $\delta_e(x, z)$ & $\delta_s(y, z)$ & $\delta_e(y, z)$  \\ 
% \hline
% $(x_1, y_1, z_1)$ & 0.3 & 0.4 & 0.8 & 0.9 & 0.1 & 0.7 \\ 
% $(x_2, y_2, z_2)$ & 0.5 & 0.7 & 0.2 & 0.4 & 0.3 & 0.5 \\ 
% $(x_3, y_3, z_3)$ & 0.2 & 0.6 & 0.4 & 0.5 & 0.3 & 0.4 \\ 
% \hline
% \end{tabular}
% \vspace{-1em}
% \label{tab:example}
% \end{table}

% \vspace{-1em}



\subsection{Dynamic Edge Pruning Strategy}
\label{sec:D-RNG}
% As discussed in Section~\ref{sec:motivations}, designing a graph that can maintain the Relative Neighborhood Graph's properties under varying $\alpha$, remains an open problem. A straightforward approach is to construct separate graph-based ANNS indexes for each different $q.\alpha$ and then merge these separate graphs into a single graph. However, this approach not only incurs very high time and space costs but also makes the entire graph very dense, as edges suitable for different $\alpha$ values vary. This density leads to a significant decrease in efficiency during the query phase if all these edges are navigated.

% To address this, we propose a novel idea: constructing a single graph that contains edges for different $q.\alpha$ values and assigning each edge an active range $u$, where $u \subseteq [0, 1]$. Only when the $q.\alpha$ specified by the user falls into this active range $u$, the edge is activated and routed for searching, otherwise, the edge will be ignored during the query phase. Next, we will illustrate how this idea is implemented in practice. Specifically, we take $\alpha$ into account in the pruning strategy of the Relative Neighborhood Graph, thereby proposing the Dynamic Relative Neighborhood Graph (D-RNG). 

% \ziqi{

As discussed before, we aim to dynamically utilize the RNGâ€™s pruning strategy to prune edges. % at query time. 
To achieve this, we take $\alpha$ into account the pruning strategy of the RNG, transforming the pruning condition of edge $(x, y)$ into the following two formulas:
% }

% Firstly, let's recall the pruning condition of the RNG, which is formulated as below:



% \noindent\textbf{RNG's Edge Pruning Strategy}: An edge $(x, y)$ is pruned if and only if there exists a node $z \in V$ such that both $\delta(x, z) < \delta(x, y) $ and $\delta(y, z) < \delta(x, y)$ hold.

% \noindent\textbf{D-RNG's Edge Pruning Strategy}: In our context, we substitute $\delta(x, y)$ with $Dist(x, y)$, which transforms the pruning condition of edge $(x, y)$ into the following two formulas:
\begin{equation}
\begin{aligned}
\alpha \times \delta_e(x, z) + (1 - \alpha) \times \delta_s(x, z) < \\ 
\alpha \times \delta_e(x, y) + (1 - \alpha) \times \delta_s(x, y)
\end{aligned}
\label{eq:rng-hvq-1}
\end{equation}
\begin{equation}
\begin{aligned}
\alpha \times \delta_e(y, z) + (1 - \alpha) \times \delta_s(y, z) < \\
\alpha \times \delta_e(x, y) + (1 - \alpha) \times \delta_s(x, y)
\end{aligned}
\label{eq:rng-hvq-2}
\end{equation}
% \ziqi{
If both Equation~\ref{eq:rng-hvq-1} and Equation~\ref{eq:rng-hvq-2} hold for any $\alpha \in [0, 1]$, then the edge $(x, y)$ will be pruned due to the presence of node $z$ according to the pruning strategy of RNG, as it becomes the longest edge in the triangle $(x, y, z)$. However, the two equations may be satisfied for some values of $\alpha$ and not for others, which makes the pruning process challenging.
% }
% leading to the edge being pruned depending on the specific value of $\alpha$.

\begin{table}[!t]
% \vspace*{-1em}
\caption{Examples.} %to illustrate the challenge of applying RNG's pruning strategy in the HVQ problem.}
% \vspace*{-1em}
\centering
% \raggedright
\begin{tabular}{p{0.1\textwidth} C{0.1\textwidth} C{0.1\textwidth} C{0.1\textwidth} C{0.1\textwidth} C{0.1\textwidth} C{0.1\textwidth}}
\toprule
\scalebox{0.95}{Examples} & \scalebox{0.9}{$\delta_s(x, y)$} & \scalebox{0.9}{$\delta_e(x, y)$} & \scalebox{0.9}{$\delta_s(x, z)$} & \scalebox{0.9}{$\delta_e(x, z)$} & \scalebox{0.9}{$\delta_s(y, z)$} & \scalebox{0.9}{$\delta_e(y, z)$}  \\ 
\midrule
\scalebox{0.9}{$(x_1, y_1, z_1)$} & 0.3 & 0.4 & 0.8 & 0.9 & 0.1 & 0.7 \\ 
\scalebox{0.9}{$(x_2, y_2, z_2)$} & 0.5 & 0.7 & 0.2 & 0.4 & 0.3 & 0.5 \\ 
\scalebox{0.9}{$(x_3, y_3, z_3)$} & 0.2 & 0.6 & 0.4 & 0.5 & 0.3 & 0.4 \\ 
\bottomrule
\end{tabular}
% \vspace*{-2em}
\label{tab:example}
\end{table}

\noindent{\textbf{Example 1:}} Table~\ref{tab:example} presents several examples to illustrate this challenge. In the first example, $(x_1, y_1, z_1)$, Equation~\ref{eq:rng-hvq-1} does not hold for any $\alpha$ value. This means that the edge $(x, y)$ will not be pruned due to the presence of node $z$ according to RNG's pruning strategy. In the second example, $(x_2, y_2, z_2)$, both equations are satisfied for any $\alpha \in [0, 1]$, indicating that the edge $(x_2, y_2)$ will be consistently pruned by node $z_2$, regardless of the $\alpha$ value. In the third example, $(x_3, y_3, z_3)$, the first equation holds for $\alpha \in [\frac{2}{3}, 1]$ and the second equation holds for $\alpha \in [\frac{1}{3}, 1]$. This means that the edge $(x_3, y_3)$ will be pruned due to the presence of node $z$ when $\alpha \in [\frac{2}{3}, 1]$, as both conditions are satisfied in this range.

 
Example~1 shows that the RNG's pruning strategy is effective in some $\alpha$ cases, but not in others. We formalize this property into the following lemma.
\vspace*{-0.5em}
\begin{lemma}
\label{lemma:drng}
If Equation~\ref{eq:rng-hvq-1} holds for $\alpha \in r^z_1$ and Equation~\ref{eq:rng-hvq-2} holds for $\alpha \in r^z_2$, where $r^z_1, r^z_2 \subseteq [0, 1]$, then the edge $(x, y)$ will be pruned due to the presence of node $z$ according to RNG's pruning strategy when $\alpha \in r^z_1 \cap r^z_2$. 
\end{lemma}
\vspace*{-0.5em}
%Then we prove the correctness of Lemma~\ref{lemma:drng} as follows:
% \vspace*{-0.5em}
% \begin{proof}
% \label{lemma:drng_proof} 
% Since both Equation~\ref{eq:rng-hvq-1} and Equation~\ref{eq:rng-hvq-2} hold for any $\alpha \in r^z_1 \cap r^z_2$, then we can derive that for varying $\alpha \in r^z_1 \cap r^z_2$, the edge length of $(x, y)$, i.e., the distance between $x$ and $y$, is consistently larger than the edge lengths of both $(x, z)$ and $(y, z)$ as the right sides of the two inequalities represent the hybrid distance formulas for $(x, y)$, while the left sides represent the hybrid distance formulas for $(x, z)$ and $(y, z)$, respectively. Therefore, for any $\alpha \in r^z_1 \cap r^z_2$, the edge $(x, y)$ is consistently the longest edge in the triangle $(x, y, z)$, and will be pruned according to the RNG's pruning strategy.
% \end{proof}
% \vspace*{-0.5em}
Due to page limitations, the proof is provided in the appendix. Next, we demonstrate how to compute $r^z_1$ and $r^z_2$. First, 
%the above two formulas 
Equations~2 and 3
can be transformed into the following two formulas, respectively:%}

% \ziqi{Therefore, we propose a novel idea: for each edge, we calculate an active range within which the edge will not be pruned by node $z$ according to the RNG's pruning strategy, and outside of which it will be pruned. This range is then assigned to the edge, determining whether it will be activated at query time based on the query's $\alpha$ value. This strategy ensures that the remaining graph at query time consistently satisfies the RNG's property. Next, we show how to calculate the active range for each edge. Firstly, the above two formulas can be transformed into the following two formulas, respectively:}

% \ziqi{Then we show how to calculate the active range. %in the following. 
% The above two formulas can be %easily 
% transformed into the following:}

\begin{equation}
\begin{aligned}
\alpha \times (\delta_e(x, z) - \delta_e(x, y) + \delta_s(x, y) - \delta_s(x, z)) < \\
\delta_s(x, y) - \delta_s(x, z)
\end{aligned}
\label{eq:rng-hvq-3}
\end{equation}
\begin{equation}
\begin{aligned}
\alpha \times (\delta_e(y, z) - \delta_e(x, y) + \delta_s(x, y) - \delta_s(y, z)) < \\
\delta_s(x, y) - \delta_s(y, z)
\end{aligned}
\label{eq:rng-hvq-4}
\end{equation}
% \ziqi{
%Then we detail how to compute the active range for Equation~\ref{eq:rng-hvq-3}, and Equation~\ref{eq:rng-hvq-4} can be computed similarly. In Equation~\ref{eq:rng-hvq-3}, the value of $\alpha$ that satisfies the condition %can be computed as follows, which 
In Equation~\ref{eq:rng-hvq-3}, the range of $\alpha$ that satisfies the inequality, denoted as $r_1^z$, is determined by the value and sign of the distance differences, which can be categorized into four cases based on their signs:
% }

% Clearly, for each inequality, we can derive a range within which the inequality holds, denoted as $r_1^z$ and $r_2^z$ respectively. Taking $r_1^z$ as an example, there are four cases for computing it, and $r_2^z$ can be computed similarly.



\noindent\textbf{Case 1:} If $\delta_s(x, y)-\delta_s(x, z) > 0$ and $\delta_e(x, z)-\delta_e(x, y)+\delta_s(x, y)-\delta_s(x, z) > 0$, then the inequality is satisfied for the range $r_1^z = \left[0, \min\left(1, \frac{\delta_s(x, y) - \delta_s(x, z)}{\delta_e(x, z) - \delta_e(x, y) + \delta_s(x, y) - \delta_s(x, z)}\right)\right]$.

\noindent\textbf{Case 2:} If $\delta_s(x, y) - \delta_s(x, z) < 0$ and $\delta_e(x, z) - \delta_e(x, y) + \delta_s(x, y) - \delta_s(x, z) \geq 0$, then the inequality cannot be satisfied for any $\alpha \in [0, 1]$, resulting in $r_1^z = \emptyset$.

\noindent\textbf{Case 3:} If $\delta_s(x, y) - \delta_s(x, z) > 0$ and $\delta_e(x, z) - \delta_e(x, y) + \delta_s(x, y) - \delta_s(x, z) \leq 0$, then the inequality can be satisfied for any $\alpha \in [0, 1]$, resulting in $r_1^z = [0, 1]$. 

\noindent\textbf{Case 4:} If $\delta_s(x,y) - \delta_s(x,z) < 0$ and $\delta_e(x,z) - \delta_e(x,y) + \delta_s(x,y) - \delta_s(x,z) < 0$, then the inequality is satisfied for the range $r_1^z = \left[\min\left(1, \frac{\delta_s(x,y) - \delta_s(x,z)}{\delta_e(x,z) - \delta_e(x,y) + \delta_s(x,y) - \delta_s(x,z)}\right), 1\right]$.


\noindent\textbf{Example 2:} The example $(x_1, y_1, z_1)$ falls under case 2, resulting in $r_1^z = \emptyset$. The example $(x_2, y_2, z_2)$ falls under case 3, leading to $r_1^z = [0, 1]$. The example $(x_3, y_3, z_3)$ falls under case 4, $r_1^z = [\frac{2}{3}, 1]$. %where $\delta_s(x, y) - \delta_s(x, z) = -0.5$ and $\delta_e(x, z) - \delta_e(x, y) + \delta_s(x, y) - \delta_s(x, z) = 0$. The example $(x_2, y_2, z_2)$ falls under case 3, where $\delta_s(x, y) - \delta_s(x, z) = 0.3$ and $\delta_e(x, z) - \delta_e(x, y) + \delta_s(x, y) - \delta_s(x, z) = 0$. The example $(x_3, y_3, z_3)$ falls under case 4, where $\delta_s(x, y) - \delta_s(x, z) = -0.2$ and $\delta_e(x, z) - \delta_e(x, y) + \delta_s(x, y) - \delta_s(x, z) = -0.3$.

For Equation~\ref{eq:rng-hvq-4}, such range can computed similarly, denoted as $r_2^z$. %The edge $(x, y)$ will be pruned by node $z$ if both inequalities are simultaneously satisfied. 
Thus, the pruning range of edge $(x, y)$ due to the presence of node $z$ is the intersection of $r_1^z$ and $r_2^z$, denoted as $r^z = r_1^z \cap r_2^z$. The active range $u$ of edge $(x, y)$ for node $z$ is then the complement of $r^z$, given by $u = [0, 1] \setminus r^z$.


%$u = [0, 1] \setminus \bigcup_{z \in D r^z$.


% According to the definition of RNG, in order to add the edge $(x, y)$ to the graph $G(V, E)$, we need to compute $r^z$ for all $z \in V$ and take their union $\bigcup_{z \in D} r^z$ to obtain the pruning range. The active range $u$ of edge $(x, y)$ is then the complement of this union, given by $u = [0, 1] \setminus \bigcup_{z \in D} r^z$.


% Through the above edge pruning strategy, the D-RNG assigns each edge an active range $u$. For a given $q.\alpha$, the D-RNG will skip the edges whose active ranges $u$ do not intersect with $q.\alpha$. Consequently, the D-RNG can maintain more edges in the graph while only routing a subset of unskipped edges, ensuring that the remaining edges satisfy the RNGâ€™s property. This ensures that the ANNS index built upon the D-RNG is both efficient and accurate under varying query $\alpha$ values. Figure~\ref{fig:arng} illustrates the D-RNG. When $q.\alpha = 0.9$, edges with use ranges that do not intersect with $q.\alpha$ are neglected, as shown by the dashed lines in the graph.

% However, the time complexity of constructing an exact D-RNG graph, like that of RNG~\cite{jaromczyk1991constructing}, is $O(N^3)$. The proof is straightforward, so we omit it here. %Therefore, we need to develop an efficient method to approximate D-RNG.
% Because of this, we develop an efficient method to approximate the D-RNG.

\begin{algorithm}[!t]
    \caption{\revision{Dynamic Edge Pruning Strategy}}
    \small
    \label{alg:DRNG-prune}
    \KwIn{A candidate set $CS$ consisting of approximate Pareto frontiers, maximum edges $M$, a threshold value $th$ to prune edges with a small use range}  
    \KwOut{Neighbor Set $NS$}
    \BlankLine    
    % Define the procedure
    \SetKwFunction{DRNGPrune}{\textsc{DRNGPrune}}
    \SetKwProg{Fn}{Procedure}{:}{}
    \Fn{\DRNGPrune{$CS$, $M$, $th$}}{
        Initialize the Neighbor Set $NS = \emptyset$\;        
        \ForEach{$PF_i \in CS$}{
            \ForEach{$x \in PF_i$}{
                $r^x \leftarrow \emptyset$\;
                \ForEach{$y \in NS$}{
                    % Compute $r^x$\; %according to Section~\ref{sec:D-RNG}\;
                    Compute $r^y$\;
                    $r[x] \leftarrow r[x] \cup r^y$\;
                }
                $u \leftarrow [0, 1] \setminus r[x]$ \;
                \If{$|u| \geq th$}{
                    $NS.\text{add}({x, u})$\;
                }                
            }
            \If{$|NS|\geq M$}{
                \textbf{break}\;  
            }
        }
        \Return $NS$\;
    }
\end{algorithm}


\noindent\textbf{\method's Pruning Strategy.} Using the approximate Pareto frontiers derived by the GPS algorithm as the candidate set $CS$, we can apply this dynamic edge pruning strategy to it. As shown in Algorithm~\ref{alg:DRNG-prune}, we first initialize the neighbor set $NS$ as an empty set (line 2). Then we sequentially traverse each layer of the Pareto frontier from nearest to farthest, gradually adding nodes to the $NS$ (lines 3--13). Specifically, for each new node $x$, we compute its pruning range $r^y$ with respect to each previously added node $y$ and take their union $r[x]$ as the final pruning range (lines 6--8), ensuring that the new edge will not be pruned by any previously added edges. The active range $u$ for this edge is the complement of $r[x]$ (line 9). To avoid maintaining edges that are not useful in most cases (e.g., $u = [0, 0.05]$), we set a threshold value $th$ to further prune such edges (lines 10--11). Once we obtain $M$ edges, we break the loop and return $NS$ as the final neighbor set (lines 12--13).


\revision{%\annotation{R2O3, R3O2}
However, our pruning strategy does not apply to multi-vector queries with more than two vectors, as the active range becomes a hyperplane in 2D space in these cases, which is challenging to compute, store, and utilize. We leave this 
%extension 
for future work.}
\vspace*{-0.5em}
\begin{lemma}
\label{lemma:drng-nearest-neighbor}
% If we use all objects in the dataset as candidate neighbors for each node and apply the dynamic edge pruning strategy to obtain dynamic edges, the constructed graph will be an MRNG graph~\cite{fu2019fast} for any $\alpha$ value. This guarantees that the nearest neighbor can always be found for the query using the greedy search algorithm.
By considering all objects in the dataset as candidate neighbors for each node and applying the dynamic edge pruning strategy to obtain dynamic edges, the constructed graph becomes a Relative Neighborhood Graph~\cite{fu2019fast} for any $\alpha$ value. This ensures that the nearest neighbor can always be found for the query using the greedy search algorithm. 
\end{lemma}
\vspace*{-0.5em}
Due to page limitations, the proof is provided in the appendix.
% \begin{proof}
% \label{lemma:drng_nearest_neighbor_proof} 
% %Firstly, for the graph constructed using the dynamic edge pruning strategy on the entire graph, it inevitably satisfies the RNG property for any $\alpha$ value; otherwise, the edge will be pruned or assigned an active range that does not include that $\alpha$ value. Secondly, 
% Given a specific $\alpha$, the graph formed by the active edges constructed using the above method will always be an exact RNG. This is because if any edge can be inserted into the graph without violating the RNG property for a specific $\alpha$ value, it would be included in the graph with an active range that includes $\alpha$ according to our dynamic edge pruning strategy. This ensures that no edges are omitted, thus guaranteeing that the constructed graph is an exact RNG. According to~\cite{fu2019fast, wang2021comprehensive}, exact RNG is equivalent to exact MRNG, thereby guaranteeing that the nearest neighbor can always be found for any query using the greedy search algorithm.


% % According to~\cite{wang2021comprehensive}, the MRNG shares the same pruning strategy as the RNG, making the MRNG equivalent to the RNG. Given a specific $\alpha$, the graph formed by the active edges we construct will always be an RNG. If any node could violate the RNG property, it would be inserted into the graph with an active range that includes $\alpha$ according to our dynamic pruning strategy. Additionally, as stated in~\cite{fu2019fast}, the MRNG guarantees that the nearest neighbor can always be found for the query using the greedy search algorithm.
% \end{proof}




% \vspace*{-2em}
\subsection{Index Construction}
\label{sec:construction}
%For instance, HNSW~[xx] treats each inserted node as a query to perform a greedy search~[xx] on the partially built graph, using the search results as the candidate set. 



% \noindent\textbf{Skyline~[xx].} Given a $c$-dimensional data space $C$ and a dataset $D$, an object $p \in D$ can be represented by a $c$-dimensional vector $p = \{p_1, p_2, \cdots, p_c\}$, where $p_i$ is the value on dimension $i$. A point $p \in D$ dominates another point $o \in D$, denoted as $p \prec o$, if (1) on every dimension $i$, $p_i \leq o_i$, and (2) on at least one dimension $j$, $p_j < o_j$. The skyline is a set of points $Sky(D) \subset D$ that are not dominated by any other point.

% For the \hvq problem, given an object $p \in D$, we define the data space $C$, where $c=2$ and each dimension represents the Euclidean distance space between the object $p$ and other objects $o \in D$ based on the feature vectors $e$ and $s$, respectively. Specifically, for a given object $p$, for any other object $o \in D$, $o$ is calculated as $o = \{\delta(p.e, o.e), \delta(p.s, o.s)\}$. Then we can derive the following theorem.









%Then we show how to efficiently find the $l$-layer $PF(D,p)$ given the precomputed $\delta_e(p, x)$ and $\delta_s(p, x)$ using an existing algorithm for finding 2-dimensional skylines~[xx]. 









% \begin{proof}
% \label{theorem:skyline_proof} 
% The two main time-consuming steps in Algorithm~\ref{alg:find-pf} are sorting the set (line 1) and iteratively finding the $l$-layer Pareto frontiers (lines 3-17). The time complexity of this sorting step is $O(|CS|log(|CS|)$. The second step involves iteratively scanning the entire set $l$ times, so the time complexity is $O(l|CS|)$. %Typically, $l$ is approximately equal to $\frac{ef_{construction}}{10}$ since each Pareto frontier usually contains dozens of elements.
% \end{proof}



% search on a built graph, referred to as the GPS algorithm, to  multi-layer .

%Subsequently, if we use Algorithm~\ref{alg:find-pf} for each node to find Pareto frontiers, the total time complexity will be $O(N^2(d+m)+N^2\log(N))$, which is too expensive. 

%other nodes to node $p$. The time complexity of computing the distances from all other points to point $p$ is $O(N(d+m))$. Performing this computation for each node $x \in D$ results in a time complexity of $O(N^2(d+m))$. Subsequently, if we use Algorithm~\ref{alg:find-pf} for each node to find Pareto frontiers, the total time complexity will be $O(N^2(d+m)+N^2\log(N))$, which is too expensive. 

% The issue of computing an exact edge candidate set being too expensive also exists in previous graph-based ANNS indexes, whose candidate sets are supposed to be the exact $ef_{construction}$ nearest neighbors. To tackle this, they~[xx] choose to construct the graph incrementally, performing a greedy search over the partially constructed graph to obtain approximate neighbors as the candidate set. Following previous studies~[xx], we propose a novel Greedy Pareto Frontier Search algorithm (Algorithm~\ref{alg:pareto-frontier-search}) to search on a built graph, referred to as the GPS algorithm, to find approximate multi-layer Pareto frontiers.


% not to mention the additional time cost of finding Pareto frontiers using Algirithm~\ref{alg:find-pf}, which is unacceptable.


%We are aware that for a given point $p$, and with the two distances from other points to $p$, $\delta_e(p, x)$ and $\delta_s(p, x)$, already calculated, finding the exact $PF(D, p)$ is equivalent to a long-standing problem in database community: finding the two-dimensional skyline~[xx], which has many efficient solutions~[xx]. However, computing the distances from all other points to point $p$ brutely is $O(N(d+m))$, and conducting such computation for each node $x \in D$ leads to a time complexity of $O(N^2(d+m))$, not to mention the time cost of finding skylines, which is unacceptable. 





% \begin{lemma}
% \label{lem:skyline}
% The time complexity of Algorithm~\ref{alg:pareto-frontier-search} is $O(|CS|log(|CS|)+l|CS|)$.
% \end{lemma}

% \begin{proof}
% \label{theorem:skyline_proof} 
% The two main time-consuming steps in Algorithm~\ref{alg:find-pf} are sorting the set (line 1) and iteratively finding the $l$-layer Pareto frontiers (lines 3-17). The time complexity of this sorting step is $O(|CS|log(|CS|)$. The second step involves iteratively scanning the entire set $l$ times, so the time complexity is $O(l|CS|)$. Typically, $l$ is approximately equal to $\frac{ef_{construction}}{10}$ since each Pareto frontier usually contains dozens of elements.
% \end{proof}



\noindent\textbf{Edge Seed Acquisition.} %Another challenge discussed in Section~\ref{sec:motivations} is how to choose the start nodes (also called the seed). %The seed not only impacts the search performance and also influence the quality of the approximate Pareto frontiers in the GPS algorithm. Therefore, selecting the appropriate seeds is essential. 
% \ziqi{
Another challenge discussed in Section~\ref{sec:intro} is how to choose the start nodes (also known as the seed) for the graph index. 
% } 
Existing graph-based ANNS methods, such as HNSW~\cite{malkovEfficientRobustApproximate2020}, randomly select a subset of nodes as the seed set, which forms the upper layers. To reduce search path length and improve search efficiency, one approach~\cite{fu2019fast} sets the node closest to the graph's center as the seed. %For each query, it first searches in the upper sparse layer to locate the nearest node, which then serves as the seed in the base layer, as discussed in Section~\ref{sec:motivations}. 
%However, this approach results in additional time costs due to the search in the upper layers. To address this, one approach~\cite{fu2019fast} sets the node closest to the graph's center as the seed, thereby reducing the search path length. 
However, for the \hvq problem, as $\alpha$ changes, using a single centroid or a small number of random nodes as the seed results in reduced performance (as to be shown in Section~\ref{sec:exp-ablation-study}). A straightforward method to maintain performance is to sample more random nodes or choose multiple centroids as seeds for different $\alpha$ values, but this reduces the efficiency during the query phase due to the multiple start nodes.

% \ziqi{
Figure~\ref{fig:seed} illustrates this issue, where the red nodes represent the potential multiple centers. At query time, the greedy search algorithm used by graph-based ANNS indexes maintains two sets: a min-heap candidate set $\mathcal{S}$ and a max-heap result set $\mathcal{R}$, as detailed in Section~\ref{sec:graph-anns}. The algorithm iteratively fetches the object with the minimum distance in $\mathcal{S}$ and adds its neighbors into $\mathcal{S}$ and $\mathcal{R}$. When we use multiple centroids as seeds, they all have similar distances to the query, as shown in Figure~\ref{fig:seed} by $\mathcal{S}_2$. This can lead to multiple parallel searches, causing many intermediate nodes to be visited repeatedly, thus reducing efficiency.
% }


% Therefore, we propose a novel edge seed acquisition method in this study. The core idea of this method is to choose the nodes farthest from the center as the seeds, which are located at the edge of the graph. Figure~\ref{fig:seed} illustrates this method, where the green nodes indicate the edge seed and the red nodes represent the potential multiple centers. 

% Now we analyze the advantages of the edge seed acquisition method. Let's recall the greedy search algorithm used by most graph-based ANNS indexes, as detailed in Section~\ref{sec:graph-anns}, it maintains two sets a min-heap candidate set $\mathcal{S}$ and a max-heap result set $\mathcal{R}$. The algorithm iteratively fetches the object with the minimum distance in $\mathcal{S}$, adds its neighbors into $\mathcal{S}$ and $\mathcal{R}$, and stops if the minimum distance in $\mathcal{S}$ is larger than the maximum distance in $\mathcal{R}$. The main advantage of the edge seed acquisition method is that it can reduce redundant calculations in the greedy search algorithm. As shown in Figure~\ref{fig:seed}, $\mathcal{S}_1$ is the candidate set for the edge seed method, and $\mathcal{S}_2$ is the candidate set for the multiple centroids method. Since the multiple centroids all lie in the center of the graph, they have similar distances to the query. Consequently, during the greedy search, all of their neighbors will be considered, evaluated, and added into $\mathcal{S}_2$, resulting in many redundant calculations. In contrast, for the edge seed method, some edge seeds are much farther from the query. Therefore, during the greedy search process, these distant seeds will remain at the bottom of the candidate set $\mathcal{S}_1$, and their neighbors will not be considered and evaluated. From a high-level perspective, the edge seed method allows us to adaptively start the search from the edge point closest to the query while ignoring other distant edge nodes.

% \ziqi{
To address this, we propose a novel edge seed acquisition method. The core idea of this method is to choose the nodes farthest from the center under varying $\alpha$ value as the seeds, which are located at the edge of the graph. Figure~\ref{fig:seed} illustrates this method, where the green nodes indicate the edge seed. These edge seeds are much farther from the query. Therefore, during the greedy search process, these distant seeds will remain at the bottom of the candidate set $\mathcal{S}_1$, and their neighbors will not be considered and evaluated. From a high-level perspective, the edge seed method allows us to adaptively start the search from the edge node closest to the query while ignoring other distant edge nodes.
% }


% Next, we discuss how to obtain the edge nodes. One solution is based on the number of hops. We randomly select a start node and perform a BFS search over the graph. The nodes with the largest hop count from the start point are chosen as the edge nodes. However, this method is influenced by the choice of the random start node. Additionally, performing the BFS algorithm has a time complexity of $O(N + M)$. Dynamically updating the seed during the graph construction phase results in a time complexity of $O(N^2 + MN)$, which is expensive. 

% Therefore, we employ a more efficient yet highly effective method. Specifically, we first calculate the centroid $c$ of $D$, where $c.e = \text{avg}(x.e)$ and $c.s = \text{avg}(x.s)$ for all $x \in D$. Then we maintain the inverse Pareto frontier of the centroid $c$, which consists of the nodes that do not dominate any other points based on the distance from the centroid, i.e., the most distant nodes from the centroid for varying $\alpha$. The algorithm can be easily adapted from Algorithm~\ref{alg:find-pf}, so the details are not provided here. The time complexity of updating the seed set $ep$ is $O(|ep|\log(|ep|) + |ep|)$ since we only need to maintain one layer of inverse Pareto frontier. The inverse Pareto frontier often contains only a dozen nodes, so the time complexity for updating is negligible.

To obtain the edge seed, we employ an efficient yet highly effective method. Specifically, we first calculate the centroid $c$ of $D$, where $c.e = \text{avg}(x.e)$ and $c.s = \text{avg}(x.s)$ for all $x \in D$. Then we maintain the inverse Pareto frontier of the centroid $c$, which consists of the nodes that do not dominate any other nodes based on the distance from the centroid, i.e., the most distant nodes from the centroid under varying $\alpha$. The algorithm can be easily adapted from existing algorithm for finding two-dimensional skyline~\cite{borzsony2001skyline}%~\ref{alg:find-pf}
, so the details are not provided here. The time complexity of updating the seed set $ep$ is $O(|ep|\log(|ep|) + |ep|)$ since we only need to maintain one layer of inverse Pareto frontier. The inverse Pareto frontier often contains only a dozen nodes, so the time complexity for updating is negligible.

\begin{algorithm}[t]
    \caption{Index Construction}
    \small
    \label{alg:index-construction}
    \KwIn{A dataset $D$, candidate pool size $ef_{construction}$, maximum edges per node $M$, a threshold value $th$ to prune edges with a small use range}    
    \KwOut{$G(V, E, ep)$, where $ep$ denotes the seed set}
    \BlankLine    
    % Define the procedure
    \SetKwFunction{DEGBuild}{\textsc{DEGBuild}}
    \SetKwProg{Fn}{Procedure}{:}{}
    \Fn{\DEGBuild{$D$, $ef_{construction}$, $M$, $th$}}{
        calculate the centroid $c$ of $D$\;        
        Initialize graph $G(V, E, ep)$, $V = \{x_0\}$, $E = \emptyset$, $ep = \{x_0\}$\;        
        \ForEach{$x \in D \setminus \{x_0\}$}{
            $V \leftarrow V \cup \{x\}$\;
            $CS \leftarrow \GPS(G, x, ep, ef_{construction})$\;
            $NS(x) \leftarrow \DRNGPrune(CS, M, th)$\;
            $E(x) \leftarrow NS(x)$\;                                 
            \ForEach{$y \in NS(x)$ }{
                $E(y) \leftarrow \DRNGPrune(E(y)\cup \{x\}, M, th)$\; %\tcp*{\textsf{add reverse edges}}
            }
            update $ep$ as the inverse Pareto frontier of $c$\;
        }
        \Return $G$\;
    }
\end{algorithm}




% \subsection{Index Construction}
\noindent\textbf{Summary.} Based on the modules proposed above, we summarize the index construction process. The details of the index construction phase are provided in Algorithm~\ref{alg:index-construction}. Specifically, we first calculate the centroid $c$ of $D$ (line 2). %The centroid $c$ will then be used to calculate the edge seed. Following previous studies~[xx], 
Then we initialize the start node as the first node $x_0$ (line 3) and construct the graph by iteratively inserting new nodes (lines 4-11). For each newly inserted node $x$, we employ the GPS algorithm to search for approximate Pareto frontiers (line 6) and obtain the final neighbor set $NS(x)$ of node $x$ among the approximate Pareto frontiers using Algorithm~\ref{alg:DRNG-prune} (line 7). The derived neighbor set $NS(x)$ is treated as $x$'s edges in $G$ (line 8). Similar to previous studies~\cite{malkovEfficientRobustApproximate2020}, we also attempt to add reverse edges by trying to include $x$ in $E(y)$, where $y \in NS(x)$ (lines 9-10). Finally, we update the edge seed set by determining whether $x$ can be added to the inverse Pareto frontier of the centroid (line 11). 

% \begin{algorithm}[t]
%     \caption{Search Algorithm}
%     \small
%     \label{alg:search}
%     \KwIn{A \method $G(V, E, ep)$, a query $q = \langle e, s, \alpha, k \rangle$, a hyperparameter $ef_{search}$}    
%     \KwOut{$k$ nearest neighbors of $q$}
%     \BlankLine    
%     % Define the procedure
%     \SetKwFunction{DEGSearch}{\textsc{DEGSearch}}
%     \SetKwProg{Fn}{Procedure}{:}{}
%     \Fn{\DEGSearch{$G$, $q$, $ef_{search}$}}{
%         $\mathcal{S} \leftarrow \emptyset$ \tcp*{\textsf{Initialize the candidate set (min-heap)}}    
%         $\mathcal{R} \leftarrow \emptyset$ \tcp*{\textsf{Initialize the result set (max-heap)}}    
%         $Vis \leftarrow \emptyset$  \tcp*{\textsf{set of visited set}}
%         $\mathcal{S}.\text{add}(ep)$\;
%         $\mathcal{R}.\text{add}(ep)$\;
%         $Vis.\text{add}(ep)$\;
%         \While{$\mathcal{S} \neq \emptyset$}{
%             $x \leftarrow \mathcal{S}.\text{pop}()$\;
%             \If{$x.distance > \mathcal{R}.top().distance$}{
%                \textbf{break}\;
%             }
%             \ForEach{$v \in neighbour(x)$}{
%                 \If{$q.\alpha \notin v.u$}{
%                     \textbf{continue}\;                
%                 }
%                 \If{$v \notin Vis$}{
%                     $Vis.\text{add}(v)$\;
%                 }\Else{
%                     \textbf{continue}\;                                
%                 }
%                 \If{$\alpha \leq 0.5$}{
%                     $dist_s \leftarrow \delta_s(q, v)$\;
%                     \If{$(1 - q.\alpha) \times dist_s > \mathcal{R}.top().dist$}{
%                         \textbf{continue}\;  
%                     }
%                     $dist_e \leftarrow \delta_e(q, v)$\;           
%                     $dist \leftarrow q.\alpha \times dist_e + (1 - q.\alpha) \times dist_s$\;                    
%                 }\Else{
%                     $dist_e \leftarrow \delta_e(q, v)$\;
%                     \If{$q.\alpha \times dist_e > \mathcal{R}.top().dist$}{
%                         \textbf{continue}\;  
%                     }
%                     $dist_s \leftarrow \delta_s(q, v)$\;           
%                     $dist \leftarrow q.\alpha \times dist_e + (1 - q.\alpha) \times dist_s$\;                                
%                 }
                
%                 \If{$dist < \mathcal{R}.top().distance$}{
%                     $\mathcal{S}.\text{add}(v)$\;
%                     $\mathcal{R}.\text{add}(v)$\;
%                     \If{$|\mathcal{R}| > ef_{search}$}{
%                         $\mathcal{R}.\text{pop}()$\;
%                     }
%                 }                
%              }
%         }
%         Return the first $q.k$ nodes in $\mathcal{S}$\;
%     }
% \end{algorithm}

% \begin{algorithm}[t]
%     \caption{Search Algorithm}
%     \small
%     \label{alg:search}
%     \KwIn{A \method $G(V, E, ep)$, a query $q = \langle e, s, \alpha, k \rangle$, a hyperparameter $ef_{search}$}    
%     \KwOut{$k$ nearest neighbors of $q$}
%     \BlankLine    
%     % Define the procedure
%     \SetKwFunction{DEGSearch}{\textsc{DEGSearch}}
%     \SetKwProg{Fn}{Procedure}{:}{}
%     \Fn{\DEGSearch{$G$, $q$, $ef_{search}$}}{
%         $\mathcal{S} \leftarrow \emptyset, \mathcal{R} \leftarrow \emptyset, Vis \leftarrow \emptyset$\;
%         $\mathcal{S}.\text{add}(ep), \mathcal{R}.\text{add}(ep), Vis.\text{add}(ep)$\;
%         \While{$\mathcal{S} \neq \emptyset $}{
%             $x \leftarrow \mathcal{S}.\text{pop}()$\;
%             \If{$x.distance > \mathcal{R}.top().distance$}{
%                \textbf{break}\;
%             }
%             \ForEach{$v \in neighbour(x) \land v \notin Vis$}{
%                 \If{$q.\alpha \notin v.u$}{
%                     \textbf{continue}\;                
%                 }
%                 $Vis.\text{add}(v)$\;
%                 % \If{$v \notin Vis$}{
%                 %     $Vis.\text{add}(v)$\;
%                 % }\Else{
%                 %     \textbf{continue}\;                                
%                 % }
%                 $dist_s \leftarrow \delta_s(q, v)$\;
%                     \If{$(1 - q.\alpha) \times dist_s > \mathcal{R}.top().distance$}{
%                         \textbf{continue}\;  
%                     }           
%                 $dist_e \leftarrow \delta_e(q, v)$\;         
%                 $dist \leftarrow q.\alpha \times dist_e + (1 - q.\alpha) \times dist_s$\;
%                 % \If{$\alpha \leq 0.5$}{
%                 %     $dist_s \leftarrow \delta_s(q, v)$\;
%                 %     \If{$(1 - q.\alpha) \times dist_s > \mathcal{R}.top().dist$}{
%                 %         \textbf{continue}\;  
%                 %     }
%                 %     $dist_e \leftarrow \delta_e(q, v)$\;           
%                 %     $dist \leftarrow q.\alpha \times dist_e + (1 - q.\alpha) \times dist_s$\;                    
%                 % }\Else{
%                 %     $dist_e \leftarrow \delta_e(q, v)$\;
%                 %     \If{$q.\alpha \times dist_e > \mathcal{R}.top().dist$}{
%                 %         \textbf{continue}\;  
%                 %     }
%                 %     $dist_s \leftarrow \delta_s(q, v)$\;           
%                 %     $dist \leftarrow q.\alpha \times dist_e + (1 - q.\alpha) \times dist_s$\;                                
%                 % }
                
%                 \If{$dist < \mathcal{R}.top().distance$}{
%                     $\mathcal{S}.\text{add}(v), \mathcal{R}.\text{add}(v)$\;
%                     \If{$|\mathcal{R}| > ef_{search}$}{
%                         $\mathcal{R}.\text{pop}()$\;
%                     }
%                 }                
%              }
%         }
%         Return the first $q.k$ nodes in $\mathcal{S}$\;
%     }
% \end{algorithm}

\subsection{Search Algorithm}
\label{sec:search}
We next present the search algorithm of \method. It is based on the greedy search algorithm~\cite{wang2021comprehensive}, with two modifications. Firstly, it dynamically skips edges whose active ranges do not intersect with the query's $\alpha$ value. Secondly, we introduce an early stopping mechanism. When calculating the hybrid distance, we first compute one of the individual vector distances and compare it to the threshold distance in the greedy search algorithm to determine whether to skip this node early.

Specifically, the algorithm first initializes an empty min-heap $\mathcal{S}$ as the candidate set, an empty max-heap $\mathcal{R}$ as the result set. Next, the seed are added to $\mathcal{S}$, $\mathcal{R}$. In each iteration, the object with the smallest distance in $\mathcal{S}$ is fetched. If its distance to the query is larger than the maximum distance in $\mathcal{R}$, the loop breaks, and the top $k$ objects in $\mathcal{R}$ are returned. Otherwise, its neighbors are checked, with some being skipped based on the active ranges and the query $\alpha$ value. If the distance of a neighbor to the query is smaller than the maximum distance in $\mathcal{R}$, the neighbor is pushed into $\mathcal{S}$ and $\mathcal{R}$. Here, the early termination mechanism is used for acceleration. If the size of $\mathcal{R}$ exceeds $ef_{search}$, the object with the maximum distance is popped from $\mathcal{R}$.

%, which is based on the widely used greedy search algorithm~\cite{wang2021comprehensive}. %, with modifications to accommodate the D-RNG and the \hvq problem, skipping some edges and avoiding redundant calculations early.
% Algorithm~\ref{alg:search} summarizes the search algorithm process. It begins by initializing an empty min-heap $\mathcal{S}$ as the candidate set, an empty max-heap $\mathcal{R}$ as the result set, and an indicator $Vis$ to mark evaluated nodes (line 2). Next, the seed are added to $\mathcal{S}$, $\mathcal{R}$, and $Vis$ (line 3), and a greedy search is conducted (lines 4-20). Specifically, it fetches the object with the smallest distance in $\mathcal{S}$ (line 5). If its distance to the query is larger than the maximum distance in $\mathcal{R}$, the loop breaks (lines 6-7), and the top $k$ objects are returned (line 21). Otherwise, its unvisited neighbors are checked (line 8). Edges whose active ranges $v.u$ do not intersect with $q.\alpha$ are skipped (lines 9-10). The unskipped neighbor is then added to $Vis$ (line 11). Instead of directly computing the hybrid distance $dist$, we introduce an early stopping mechanism. First, a vector distance, such as $dist_e$, is computed (line 12). If this distance is already larger than the maximum distance in $\mathcal{R}$, the algorithm stops and moves to the next neighbor (lines 13-14); otherwise, the computation continues (lines 15-16). If $dist$ is smaller than the maximum distance in $\mathcal{R}$, the neighbor $v$ is pushed into $\mathcal{S}$ and $\mathcal{R}$. If the size of $\mathcal{R}$ exceeds $ef_{search}$, the object with the maximum distance is popped from $\mathcal{R}$ (lines 17-20).




% and begin to compute the distance $dist$ and skip the nodes that are far from the query early in the process (lines 19-30). Specifically, when $\alpha \leq 0.5$, $\delta_s(q, v)$ has a larger weight than $\delta_e(q, v)$, so we compute $\delta_s(q, v)$ for each unvisited node first (line 20). If $(1-\alpha) \times \delta_s(q, v)$ is larger than the maximum distance in $\mathcal{R}$, we skip this node early without computing $\delta_e(q, v)$ (lines 21-22). Similarly, when $\alpha > 0.5$, we prioritize $\delta_e(q, v)$ (lines 25-30). This is another difference from the traditional greedy search. %Otherwise, we compute $\delta_e(q, v)$ and the hybird distance from the query, $dist$ (lines 22-23). 
% If the hybrid distance $dist$ is smaller than the maximum distance in $\mathcal{R}$, we push the neighbor $v$ into $\mathcal{S}$ and $\mathcal{R}$, and if the size of $\mathcal{R}$ exceeds $ef_{search}$, we pop the object with the maximum distance from $\mathcal{R}$ (lines 31-35). 

% \subsection{Complexity Analysis}
% \label{sec:complexity-analysis}

\noindent\textbf{Complexity Analysis.} Here, we analyze the time and space complexity of \method. For space complexity, the main difference between \method and previous graph-based ANNS indexes is the active range $u$ stored for each edge. Compared to the high-dimensional vectors stored in memory, this additional storage is negligible. As for time complexity, %there are two parts to consider: the time complexity of the index construction and the time complexity of the search algorithm. 
during the query phase, %for different $q.\alpha$, 
\method dynamically skips some edges, while the remaining edges satisfy the RNG property. Therefore, the search time complexity remains the same as that of previous RNG-based ANNS indexes~\cite{wang2021comprehensive}, which is $O(\log(N))$.

% The build time complexity mainly comes from the incremental insertion of new nodes into the graph, where the \method uses the GPS algorithm and existing methods use the greedy search algorithm. The dominant factor in the time complexity of both methods is the search path length, which determines the number of evaluated objects. In an extreme scenario where each layer of the Pareto frontier contains only one node, the GPS algorithm becomes equivalent to the greedy search algorithm and has the longest search path length. In other cases, the GPS algorithm can be considered as performing a greedy search for different $\alpha$ values. Given the bounded size of the candidate set, the search path length will be even shorter. Therefore, the build time complexity of \method is equivalent to previous methods at $O(N\log(N))$. This is verified in the experiment (as to be shown in Section~\ref{sec:exp-index-cost}), where \method has a similar construction time to the baselines.

%The main factor determining the time complexity of the GPS algorithm is the number of evaluated objects, i.e., the number of nodes for which the distance is computed, which is largely influenced by the search path. If we consider an extreme scenario where each layer of the Pareto frontier contains only one node, then the GPS algorithm is equivalent to the greedy search algorithm and will have the same search path of the greedy search.


% The main difference between the GPS algorithm and the greedy search lies in how they maintain the candidate set. 


%The greedy search uses a heap to pop the object farthest from the query, while the GPS algorithm requires finding the $l$-layer Pareto frontiers each time (line 21 in Algorithm~\ref{alg:pareto-frontier-search}). The time complexity of popping an object from the candidate set $CS$ is $O(\log(|CS|))$ while finding the $l$-layer Pareto frontiers requires $O(|CS|\log(|CS|)+l|CS|)$. 



% In the extreme scenario, if each layer of the Pareto frontier in the GPS algorithm contains only one node, resulting in the longest search path, the GPS algorithm degenerates into a greedy search algorithm. Then the main difference between the GPS algorithm and the greedy search is that it needs to update the pareto forniters each 


% The build time complexity mainly comes from the incremental insertion of new nodes into the graph by the GPS algorithm, so we need to analyze the time complexity of the GPS algorithm. In an optimal scenario, each layer of the Pareto frontier in the GPS algorithm consists of $pf$ points, where $pf$ is approximately a dozen. Since the neighbors' neighbors are very similar to the neighbors, these neighbors will form the next layer of the Pareto frontier, and their positions within the Pareto frontier will also be similar. Then the GPS algorithm essentially becomes a greedy search for different $\alpha$ values, where each node in the Pareto frontier searches for its most similar nodes that perform best for certain $\alpha$ values. The time complexity of the GPS algorithm becomes $O(pf\log(N))$. The construction time complexity is $O(pfN\log(N))$, where $pf$ is approximately a dozen.

% Due to the similarity of neighbors' neighbors, these neighbors will form the next layer of the Pareto frontier, and their positions within the Pareto frontier will also be similar.


%The GPS algorithm maintains approximate Pareto frontiers and finds new nodes based on the neighbors of these frontiers. 

%Essentially, it can be considered as performing greedy searches for different $\alpha$ values on a hybrid graph composed of RNG graphs under different $\alpha$ conditions.


%The GPS algorithm can be considered as a greedy search for different $\alpha$, which makes its time complexity Thus, its time complexity remains $O(\log(N))$.

%Therefore, the building time complexity 


%A straightforward method is to traverse the entire graph to find the edge node. However, this method is time-consuming with a time complexity of $O(N+M)$, where $N$ is the dataset size and $M$ is the edge set size. Therefore, we employ a more efficient yet highly effective method. That is, we maintain the inverse Pareto frontier of the centroid $c$, which consists of the most distant nodes from the centroid. The algorithm can be easily adapted from Algorithm~\ref{alg:find-pf}, so the details are not provided here. The time complexity of updating the seed set $ep$ is $O(|ep|\log(|ep|) + |ep|)$ since we only maintain one layer of inverse Pareto frontier. Since the inverse Pareto frontier often contains only a dozen nodes, the time complexity for updating is negligible.

%Firstly, we develop a D-RNG pruning strategy based on the approximate Pareto frontiers $CS$. 

%Based on the Algorithm~\ref{alg:DRNG-prune}, we propose our index construction method. 






















% Existing state-of-the-art graph-based ANNS indexes either randomly select a node as the start node~[xx] or choose the node closest to the center of all points as the start node~[xx]. However, these approach are less effective for our task. Randomly selecting a node as the seed node does not guarantee good performance across different queries, especially for those that are distant from the seed.










% \noindent\textbf{Time Complexity.} Now we analyze the time complexity of the GPS algorithm. The total time complexity of GPS consists of two parts: the time complexity of aggregating new nodes into $\mathcal{S}$ (lines 9-22) and the time complexity of updating the $l$-layer Pareto frontiers (line 23). As the maximum degree of each node is $M$, the maximum number of newly added nodes is $M \times |\mathcal{S}| \leq M \times ef_{construction}$. The time complexity mainly arises from computing $\delta_s(p, x)$ and $\delta_e(p, x)$, which is $O(Mef_{construction}(d + m))$. 














 


%For a given query $q$ on a constructed graph $G$, where each node's neighbors are its local optimal $l$-layer Pareto frontiers, the neighbors of the already found local optimal $l$-layer Pareto frontiers for $q$ are more likely to be its global $l$-layer Pareto frontiers. 

% Here, the local optimal Pareto frontiers indicate that we do not evaluate the entire graph but only a partial graph to determine the $l$ layer Pareto frontiers.

% The core idea of the GPS algorithm is that the neighbors of existing local optimal Pareto frontiers are more likely to be the global Pareto frontier. This is because the neighbors of each point, being the local optimal Pareto frontiers for that point, are likely to be closer to the query point in both optimization functions rather than farther away.










% The core idea of the GPS Algorithm is to continually visit the neighbors of the current local optimal Pareto Frontiers, add them to the candidate set, and then evaluate whether they belong to the local optimal solutions. 



