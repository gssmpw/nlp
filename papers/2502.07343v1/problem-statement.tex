\section{Preliminary and MOTIVATIONS}
\label{sec:preliminary-motivations}

\subsection{Problem Statement}
\label{sec:preliminary}
We 
%first describe the dataset used in this study, and then formally 
proceed to define the Hybrid Vector Query (\hvq).

\noindent\textbf{Dataset.} We consider a dataset $D$ consisting of $N$ objects. Each object $o\in D$ is characterized by two features: (1) {\cheng one} feature vector, denoted by $o.e$,  defined in a $d$-dimensional Euclidean space $E^{d}$, where $d$ is typically hundreds, and (2) {\cheng the other} feature vector, denoted {\cheng by} $o.s$, defined in an $m$-dimensional Euclidean space $E^{m}$, where $m$ 
%can 
may vary from two to hundreds, depending on applications.


\noindent\textbf{Hybrid Vector Query (\hvq):} Given a dataset $D$, a hybrid vector query $q=\langle e, s, \alpha, k\rangle$ consists of 
% a feature vector $q.e \in E^{d}$, a feature vector $q.s \in E^{m}$, 
{\cheng two feature vectors $q.e \in E^{d}$ and $q.s \in E^{m}$,}
a hyperparameter $q.\alpha\in [0,1]$, and the number of objects to be returned $q.k$. 
%The 
\hvq aims to retrieve $k$ objects with the {\cheng minimum} 
%hybrid 
distance $Dist(q,o)$:
\begin{equation}
\begin{aligned}
    Dist(q, o) = q.\alpha \times \delta_e(q, o) + (1-q.\alpha) \times \delta_s(q, o),
\end{aligned}
\label{eq:hybird_distance}
\end{equation}
where $\delta_e(q, o)=\frac{\delta(q.e, \ o.e)}{e_{max}}$ and $\delta_s(q, o)=\frac{\delta(q.s,\ o.s)}{s_{max}}$. Here, $\delta(x, y)$ represents the Euclidean distance between vectors $x$ and $y$, and $e_{max}$ (resp. $s_{max}$) denotes the maximum Euclidean distance between $o.e$ (resp. $o.s$) of any two objects in the dataset and is used as a normalization factor. %$\alpha \in [0, 1]$ is a user-defined parameter used to balance the importance between two feature vectors. 
Following 
%existing research
previous work~\cite{milvus2021}, we use the weighted aggregated score as the similarity metric, and $q.\alpha \in [0, 1]$ is a %user-defined 
parameter that allows to set preferences between the two vectors at query time. 

\noindent\textbf{Problem Statement.} In this study, we aim to develop a graph-based ANNS index for the \hvq problem that maintains both accuracy and efficiency under %varying 
different query $\alpha$ values.


\noindent\revision{%\annotation{R1O1, R2O1, R2O2}
\textbf{Comparison of HVQ and Hybrid Queries.} Several types of hybrid queries are related to \hvq, and we next discuss their differences. %On the one hand, 
Early studies on hybrid queries~\cite{fagin2001optimal,franzke2016indexing} typically assume that each object has multiple attributes, such as price, and aim to identify the top-k objects ranked by monotone aggregation functions, e.g., the sum of these attributes. These studies %are designed for 
focus on numeric or low-dimensional attributes (e.g., geo-location) and aim to find the exact top-k results (as to be detailed in Section~\ref{sec:related}). Due to the curse of dimensionality~\cite{curse1,curse2}, they are not suitable for the \hvq problem. 
%we study. %On the other hand, 
When one of the vectors is low-dimensional coordinates, \hvq can function as semantic-aware spatial keyword queries~\cite{qianSemanticawareTopkSpatial2018}, which differ from traditional spatial keyword queries. However, existing methods for semantic-aware spatial keyword queries~\cite{chenS2RtreePivotbasedIndexing2020,qianSemanticawareTopkSpatial2018, DBLP:conf/edbt/TheodoropoulosN24} often suffer from severe efficiency challenges (as to be detailed in Section~\ref{sec:related}).}

\subsection{Graph-based ANNS methods}
\label{sec:graph-anns}

% \pasq{You wrote the exact same paragraph in the introduction, it does not look good to me.}


% To address the ANNS problem, various methods have been proposed~[xx], such as tree-based methods~[xx], hashing-based methods~[xx], quantization-based methods~[xx], and graph-based methods~[xx]. 
According to experimental evaluations~\cite{liApproximateNearestNeighbor2020, wang2021comprehensive}, graph-based approaches have demonstrated superior performance compared to other ANNS techniques. 
%Such methods 
They build a graph $G(V, E)$ where each node $x \in V$ corresponds to an object $o \in D$. The Hierarchical Navigable Small World graph (HNSW)~\cite{malkovEfficientRobustApproximate2020}, as illustrated in Figure~\ref{fig:hnsw}, is a %representative
state-of-the-art method. 
%well-known example. 
It comprises several layers, where layer 0 contains all objects, and each upper layer $i\ge1$, randomly retains a subset of the objects from layer $i-1$. Within each layer, a vertex is connected to several approximate nearest neighbors, while across adjacent layers, two vertices are connected only if they represent the same vector data.


\begin{figure}[!t]
\begin{center}
% \vspace*{-1em}
\subcaptionbox{HNSW.\label{fig:hnsw}}{
\includegraphics[width=0.35\columnwidth]{fig/HNSW.pdf}
}
% \vspace*{-1em}
\subcaptionbox{Pruning Strategy of RNG.\label{fig:rng}}{
\includegraphics[width=0.35\columnwidth]{fig/RNG-Lune.pdf}
}
% \vspace{-0.5em}
\caption{Figure~\ref{fig:hnsw} illustrates the HNSW index. Figure~\ref{fig:rng}~illustrates the pruning strategy of the Relative Neighborhood Graph (RNG).}
\label{fig:hnsw-rng}
% \vspace*{-2em}
\end{center}
\end{figure}

At %the 
query time, the HNSW algorithm starts the search from the vertex in the top layer, as shown in Figure~\ref{fig:hnsw}. %\ziqi
{At each layer, the HNSW algorithm accesses all neighbors of the current vertex and checks whether there exists a neighbor closer to the query than the current vertex. If so, it moves to the neighbor nearest to the query. This process continues within the layer until no closer neighbor to the query can be found.
%of the current vertex closer to the query. 
It then proceeds to the next layer and repeats the same procedure, using the current vertex as the starting vertex. This process continues until reaching layer 0.} 
%In the subsequent layer, it iteratively moves to the neighbor closest to the query until no neighbor closer than the current node exists. The reached node is used as the new starting point and the algorithm proceeds to the next layer until reaching layer 0. 
At layer 0, it conducts a greedy search algorithm, a method commonly used by %most 
graph-based ANNS indexes~\cite{wang2021comprehensive,fu2019fast,liApproximateNearestNeighbor2020,DBLP:journals/pami/FuWC22,DBLP:journals/is/MalkovPLK14,DBLP:conf/cvpr/HarwoodD16}. Specifically, it maintains two sets, a candidate set $\mathcal{S}$ (a min-heap) and a result set $\mathcal{R}$ (a max-heap), where $R$ stores the approximate nearest neighbors that have currently been found for the query, and $S$ stores candidates that can potentially improve $R$. At each iteration, the object with the smallest distance in $\mathcal{S}$ is popped, and its neighbors are evaluated. If a neighbor has not been visited before and its distance from the query is smaller than the maximum distance in $\mathcal{R}$, the neighbor is added to $\mathcal{S}$ and $\mathcal{R}$. Here, new elements are inserted into $\mathcal{R}$ continuously.
%without popping. 
To avoid the high maintenance cost when $\mathcal{R}$ becomes large, which would reduce search efficiency, a hyperparameter $ef_{search}$ is used to control its size. If the size of $\mathcal{R}$ exceeds $ef_{search}$, the object with the maximum distance in $\mathcal{R}$ is removed. The candidate set $\mathcal{S}$ is unbounded in size, as empirically it does not significantly reduce efficiency. The search stops and returns the $k$ nearest objects in $\mathcal{R}$ when the minimum distance in $\mathcal{S}$ is larger than the maximum distance in $\mathcal{R}$. The hyperparameter $ef_{search}$ controls the accuracy-efficiency trade-off at query time.


% The candidate set $\mathcal{S}$ is unbounded in size, while the result set $\mathcal{R}$ has its size bounded by a hyperparameter $ef_{search}$, where $ef_{search}$ controls the accuracy-efficiency trade-off.
% %during the query phase. 
% At each iteration, the object with the smallest distance in $\mathcal{S}$ is popped, and its neighbors are evaluated. If a neighbor's distance from the query is smaller than the maximum distance in $\mathcal{R}$, the neighbor is added to $\mathcal{S}$ and $\mathcal{R}$. If $\mathcal{R}$ exceeds $ef_{search}$, the object with the maximum distance in $\mathcal{R}$ is removed. The search stops and returns the $k$ nearest objects in $\mathcal{R}$ when the minimum distance in $\mathcal{S}$ is larger than the maximum distance in $\mathcal{R}$. 


% Clearly, the graph structure plays an important role %in HNSW and similarly for other graph-based indexes
Clearly, the graph structure plays a key role in graph-based ANNS indexes, as verified in the work~\cite{wang2021comprehensive}. According to~\cite{wang2021comprehensive}, 
%most 
state-of-the-art graph-based ANNS indexes (e.g., HNSW) are mostly constructed by approximating the Relative Neighborhood Graph (RNG)~\cite{rng}%typically based on the Relative Neighborhood Graph (RNG)~\cite{rng}
, and we next explain the RNG. 
%for which we provide a formal definition.

\noindent\textbf{Relative Neighborhood Graph (RNG).} The RNG $G(V, E)$ constructed on a dataset $D$ has the following property: For $x, y \in V$, if $x$ and $y$ are connected by an edge $e \in E$, then for $\forall z \in V$, either $\delta(x, y) < \delta(x, z)$ or $\delta(x, y) < \delta(y, z)$. In other words, the longest edge of a triangle in RNG will be pruned. \revision{%\annotation{R2O3}
Note that the RNG property applies only to metric distances, as non-metric or non-linear distances do not satisfy the triangle inequality.}

Figure~\ref{fig:rng} illustrates this property of RNG. The dashed edge $(x, y)$ represents a potential edge and is pruned from RNG because it is the longest edge in the triangle $(x, y, z)$.
%, this edge is pruned. %The other dashed lines in the graph are also pruned for the same reason. 
This pruning strategy removes some redundant neighbors and makes the neighbors distribute omnidirectionally, thereby reducing 
%some 
redundant search on the ANNS index~\cite{wang2021comprehensive}. For example, in the triangle $(x, y, z)$, if all three edges are preserved, there exist two paths from $x$ to $z$: $\{(x, z)\}$ and $\{(x, y), (y, z)\}$, which result in redundant calculations. According to the experimental evaluation~\cite{wang2021comprehensive}, the pruning strategy of RNG can improve search performance significantly. %at query time.
%The red lune in the graph is formed by the intersection of two circles, each centered at $x$ and $y$ with a radius equal to the edge length of $(x, y)$. There exists a point $z$ within this red lune, so $(x, z) < (x, y)$ and $(y, z) < (x, y)$.}


% In other words, if there exists a point $z$ in the red lune in Figure~\ref{fig:rng}, then edge $(x, y)$ becomes the longest edge in the triangle and is pruned. %the RNG removes the longest edge in any triangle in the graph, as illustrated in Figure~\ref{fig:rng}, where the dashed line represents the longest edge in the triangle and the pruned edge. 
% {This strategy removes some redundant neighbors and makes the neighbors distribute omnidirectionally, thereby reducing some redundant calculations of the ANNS index.}

However, the time complexity of constructing the exact RNG on dataset $D$ is $O(N^3)$~\cite{jaromczyk1991constructing}. Therefore, many techniques~\cite{fu2019fast,malkovEfficientRobustApproximate2020,harwood2016fanng,chen2018sptag,iwasaki2015neighborhood,DBLP:journals/pami/FuWC22,liApproximateNearestNeighbor2020} have been developed to approximate RNG and construct an ANNS index. Take HNSW as an example, which constructs 
%the graph 
RNG by continuously inserting nodes. For each node, the key is how to determine edges for each node. The neighbor selection strategy in HNSW consists of two steps, which is also widely used by other graph-based ANNS indexes~\cite{fu2019fast,malkovEfficientRobustApproximate2020,harwood2016fanng,iwasaki2015neighborhood, DBLP:journals/is/MalkovPLK14}: (1) Obtaining $ef_{construction}$ approximate nearest neighbors as the candidate neighbor set, which avoids using all nodes in the graph as candidates, thus improving index construction efficiency. To obtain the candidate set, it performs a greedy search over the partially built graph; (2) Using the RNG's pruning strategy on the candidate set to prune some redundant candidate set and obtain the final $M$ neighbors, thereby achieving a better accuracy-efficiency trade-off. %at query time. 
Here, $ef_{construction}$ is a hyperparameter that controls the candidate set size, and $M$ is a hyperparameter that determines the maximum number of neighbors per node. 

%For example, HNSW constructs the graph by continuously inserting nodes. Each point is treated as a query to perform a greedy search over the partially built graph, obtaining $ef_{construction}$ approximate nearest neighbors as the candidate set $CS$, where $ef_{construction}$ is a hyperparameter that determines the graph's quality. From the candidate set $CS$, $M$ edges are selected by choosing the nearest neighbors, while some other edges are pruned, according to the RNG's pruning strategy. Here, $M$ is a hyperparameter that determines the maximum number of edges per node and controls the graph's density.
% \vspace*{-1em}
\subsection{Motivations}
\label{sec:motivations}
\noindent\textbf{Baselines.} To the best of our knowledge, very little work has been done for the \hvq problem and previous work only presents two simple solutions by adapting existing ANNS indexes~\cite{milvus2021}, which are detailed below:

\begin{itemize}[leftmargin=*, topsep=0pt]
    \item \textsf{Fusion (abbr. F)}: This method builds an ANNS index based on the hybrid distance by fixing $\alpha = 0.5$ in Equation~\ref{eq:hybird_distance}. During the query phase, it searches the built 
    %hybrid 
    ANNS index %using the corresponding search algorithm 
    to obtain the approximate result. The accuracy-efficiency trade-off is controlled by the search algorithm's hyperparameter (e.g., $ef_{search}$).
    \item \textsf{Merging (abbr. M)}: This method builds an index for each
    %
    %type of 
    feature vector separately. During the query %ing 
    phase, it issues a top-$k'$ query for each query feature 
    %vector 
    on the corresponding index, where $k' \geq k$. All the returned 
    %$2\times k'$ 
    objects from every query feature are then reranked based on Equation~\ref{eq:hybird_distance} to find the approximate top-$k$ results. The accuracy-efficiency trade-off is largely affected by the hyperparameter $k'$.
\end{itemize}
%For the \textsf{Fusion} method, we integrate it
We integrate the \textsf{Fusion} method with the state-of-the-art ANNS index, \textsf{HNSW}~\cite{malkovEfficientRobustApproximate2020}. %, but replace the distance function with Equation~\ref{eq:hybird_distance} and set $\alpha$ to 0.5 during the indexing phase. 
This method is denoted as \textsf{HNSW}$_{\textsf{F}}$. For the \textsf{Merging} method, we use \textsf{HNSW} to build an ANNS index for each modality separately. When one of the feature vector's dimensions is low, 
%(e.g., 2), 
we use the R-Tree~\cite{beckmann1990r} to index this modality and issue an exact top-$k'$ query instead 
%
as the R-tree will perform better. This method is denoted as \textsf{HNSW}$_{\textsf{M}}$.

\begin{figure}[!t]
\centering
% \vspace*{-2em}
\includegraphics[width=0.7\columnwidth]{fig/baseline_qps_recall_openimage.pdf}
\vspace*{-1em}
\caption{The experiment results of \textsf{HNSW}$_{\textsf{M}}$ and 
\textsf{HNSW}$_{\textsf{F}}$ on the OpenImage dataset (up and right is better).}
\label{fig:exp-motivation}
\vspace*{-1em}
\end{figure}

\noindent\textbf{Limitations of Baselines.} Both baselines share a common idea: fixing the $\alpha$ value (e.g., 0, 0.5, 1)
%
to build the index
during the 
%index 
construction phase. \textbf{This is because existing ANNS indexes are 
%all constructed based on the assumption 
designed for the setting 
%that the 
in which
distances between objects 
%remain static.
are certain and pre-determined.} Take graph-based ANNS indexes as an example, they typically %require selecting 
selects approximate nearest neighbors %(nearby objects based on distance) 
for each node as the edge candidate set and pruning candidate edges based on edge length (distance between nodes). However, for the \hvq problem, as the query's $\alpha$ changes, the distances between objects may also change. The 
dynamic nature of
%property of 
distances in the \hvq problem 
%is beyond 
exceeds
the capabilities of existing ANNS techniques to handle%. Then
, and a straightforward approach is to fix the value of $\alpha$ during the indexing phase.

\textbf{The limitation of fixing $\alpha$ values to build ANNS indexes in the \hvq problem is that while this approach performs well for some query $\alpha$ values, it faces significant performance degradation for others.} For example, %for the 
\textsf{HNSW}$_{\textsf{F}}$ 
%baseline, we 
constructs the index by fixing $\alpha$ at 0.5, ensuring that the edges satisfy the RNG's properties when $\alpha$ is 0.5. However, during the query phase, when $q.\alpha$ deviates
%
significantly from 0.5, the properties of RNG quickly
become ineffective, leading to a significant degradation in performance (as to be shown later). 
%For the 
\textsf{HNSW}$_{\textsf{M}}$ 
%baseline, we 
constructs separate indexes, 
%that 
each of which considers only one feature vector while neglecting the other. Therefore, each index can only retrieve similar objects within a modality. When $q.\alpha$ is around 0.5 during the query time, the separate indexes struggle to fetch high-quality candidates that are similar in both modalities for reranking, resulting in severe performance degradation (as to be shown later).



To illustrate the baselines' limitations, We evaluate \textsf{HNSW}$_{\textsf{F}}$ and \textsf{HNSW}${_\textsf{M}}$ on the \textsf{OpenImage} dataset, which contains 500K images and %corresponding 
textual descriptions. The textual content and images are transformed into embeddings using BERT~\cite{devlin2018bert} and ViT~\cite{dosovitskiy2020image}, respectively. The efficiency-accuracy trade-off results are shown in Figure~\ref{fig:exp-motivation}
%where the hyperparameters are detailed in Section~\ref{sec:exp-setup}. 
(hyperparameters details can be found in Section~\ref{sec:exp-setup}).
We make the following observations: 
%\textbf{
(1) \textsf{HNSW}$_{\textsf{F}}$ 
%shows optimal performance 
performs the best when $q.\alpha$ is %close to 0.5 
(e.g., 0.5) 
and degrades significantly when $q.\alpha$ deviates from 0.5 (e.g., 0.1 and 0.9);
%} \textbf{
(2) \textsf{HNSW}$_{\textsf{M}}$ 
%shows optimal performance 
performs better when $q.\alpha$ is close to 0 and 1 (e.g., 0.1 and 0.9) and degrades significantly when $q.\alpha$ is close to 0.5. 
%deviates from 0 (e.g., 0.3 and 0.5).
%} 
This validates our analysis that adopting existing graph-based ANNS methods as baselines can perform well only for certain values of $q.\alpha$, but they exhibit significant performance degradation under different $q.\alpha$ values. These limitations motivate us to develop a new graph-based ANNS index that maintains high performance across varying $\alpha$ values.
%This validates our %previous 
%analysis. In summary, 
%simply adopting existing graph-based ANNS methods 
%baselines can only perform well for certain values of $q.\alpha$, but exhibit significant performance degradation under 
%varying 
%other $q.\alpha$.

% \noindent\textbf{Challenges.} 
% To overcome the limitations of baselines, we aim to develop a new graph-based ANNS index. The key lies in how to construct an index that is capable of handling varying $\alpha$ values, which presents two main challenges. {\textbf{(1) How to compute the candidate neighbor set for each node?}} Existing RNG-based indexes typically use hundreds of approximate nearest neighbors for each node as the edge candidate set to avoid considering all nodes, thereby improving construction efficiency. However, as $\alpha$ changes, the distance between objects changes, and the approximate nearest neighbors of each node may vary significantly. Therefore, it becomes challenging to compute a candidate set for each node that comprises the approximate nearest neighbors at varying $\alpha$ values. 
% {\textbf{(2) How to determine edges from the candidate set?}} Existing RNG-based indexes use the RNG's pruning strategy to determine the final edges from the candidate set, which prunes some candidate edges based on the edge lengths. However, for the \hvq problem, $q.\alpha$ may vary at query time, making the edge lengths dynamic, and the RNG pruning strategy does not work. A straightforward solution is to fix an  $\alpha$ value to prune edges during the index construction phase. However, as the query $\alpha$ vary, the RNG's properties 
% %quickly 
% become ineffective, thereby leading to significant performance degradation, as shown in Figure~\ref{fig:exp-motivation}. How to design a new edge pruning strategy that can maintain the RNG's property under varying $\alpha$ values is an open problem.

% Another issue is how to determine the start node (also known as the seed) of the index graph. According to~\cite{fu2019fast}, the seed of the graph impacts the search path length, which reflects the search efficiency. To reduce search path length, existing methods~\cite{malkovEfficientRobustApproximate2020, fu2019fast} either randomly select a subset of nodes or choose the graph's approximate center as the seed. To handle changing query $\alpha$ values, an intuitive solution is to select more random nodes or multiple approximate centroids for different $\alpha$. However, these solutions result in too many start nodes during the search, thereby decreasing search efficiency. How to select the seed for the \hvq problem remains an open question.






 





 

