@misc{proofnet,
      title={ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics}, 
      author={Zhangir Azerbayev and Bartosz Piotrowski and Hailey Schoelkopf and Edward W. Ayers and Dragomir Radev and Jeremy Avigad},
      year={2023},
      eprint={2302.12433},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.12433}, 
}

@article{zheng2021minif2f,
  title={Minif2f: a cross-system benchmark for formal olympiad-level mathematics},
  author={Zheng, Kunhao and Han, Jesse Michael and Polu, Stanislas},
  journal={arXiv preprint arXiv:2109.00110},
  year={2021}
}

@article{gemma2-2024,
  title  = {Gemma 2: Improving Open Language Models at a Practical Size},
  author = {Gemma Team},
  year   = {2024}
}

@article{Riviere2024Gemma2I,
  title={Gemma 2: Improving Open Language Models at a Practical Size},
  author={Gemma Team and Morgane Riviere and Shreya Pathak and Pier Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and Léonard Hussenot and Thomas Mesnard and Bobak Shahriari and Alexandre Ramé and Johan Ferret and Peter Liu and Pouya Tafti  and others},
  journal={ArXiv},
  year={2024},
  volume={abs/2408.00118},
  url={https://api.semanticscholar.org/CorpusID:270843326}
}

@misc{gemmateam2024gemma2improvingopen,
      title={Gemma 2: Improving Open Language Models at a Practical Size}, 
      author={Gemma Team and Morgane Riviere and Shreya Pathak and Pier Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and Léonard Hussenot and Thomas Mesnard and Bobak Shahriari and Alexandre Ramé and Johan Ferret and Peter Liu and Pouya Tafti and Abe Friesen and Michelle Casbon and Sabela Ramos and Ravin Kumar and Charline Le Lan and Sammy Jerome and Anton Tsitsulin and Nino Vieillard and Piotr Stanczyk and Sertan Girgin and Nikola Momchev and Matt Hoffman and Shantanu Thakoor and Jean-Bastien Grill and Behnam Neyshabur and Olivier Bachem and Alanna Walton and Aliaksei Severyn and Alicia Parrish and Aliya Ahmad and Allen Hutchison and Alvin Abdagic and Amanda Carl and Amy Shen and Andy Brock and Andy Coenen and Anthony Laforge and Antonia Paterson and Ben Bastian and Bilal Piot and Bo Wu and Brandon Royal and Charlie Chen and Chintu Kumar and Chris Perry and Chris Welty and Christopher A. Choquette-Choo and Danila Sinopalnikov and David Weinberger and Dimple Vijaykumar and Dominika Rogozińska and Dustin Herbison and Elisa Bandy and Emma Wang and Eric Noland and Erica Moreira and Evan Senter and Evgenii Eltyshev and Francesco Visin and Gabriel Rasskin and Gary Wei and Glenn Cameron and Gus Martins and Hadi Hashemi and Hanna Klimczak-Plucińska and Harleen Batra and Harsh Dhand and Ivan Nardini and Jacinda Mein and Jack Zhou and James Svensson and Jeff Stanway and Jetha Chan and Jin Peng Zhou and Joana Carrasqueira and Joana Iljazi and Jocelyn Becker and Joe Fernandez and Joost van Amersfoort and Josh Gordon and Josh Lipschultz and Josh Newlan and Ju-yeong Ji and Kareem Mohamed and Kartikeya Badola and Kat Black and Katie Millican and Keelin McDonell and Kelvin Nguyen and Kiranbir Sodhia and Kish Greene and Lars Lowe Sjoesund and Lauren Usui and Laurent Sifre and Lena Heuermann and Leticia Lago and Lilly McNealus and Livio Baldini Soares and Logan Kilpatrick and Lucas Dixon and Luciano Martins and Machel Reid and Manvinder Singh and Mark Iverson and Martin Görner and Mat Velloso and Mateo Wirth and Matt Davidow and Matt Miller and Matthew Rahtz and Matthew Watson and Meg Risdal and Mehran Kazemi and Michael Moynihan and Ming Zhang and Minsuk Kahng and Minwoo Park and Mofi Rahman and Mohit Khatwani and Natalie Dao and Nenshad Bardoliwalla and Nesh Devanathan and Neta Dumai and Nilay Chauhan and Oscar Wahltinez and Pankil Botarda and Parker Barnes and Paul Barham and Paul Michel and Pengchong Jin and Petko Georgiev and Phil Culliton and Pradeep Kuppala and Ramona Comanescu and Ramona Merhej and Reena Jana and Reza Ardeshir Rokni and Rishabh Agarwal and Ryan Mullins and Samaneh Saadat and Sara Mc Carthy and Sarah Cogan and Sarah Perrin and Sébastien M. R. Arnold and Sebastian Krause and Shengyang Dai and Shruti Garg and Shruti Sheth and Sue Ronstrom and Susan Chan and Timothy Jordan and Ting Yu and Tom Eccles and Tom Hennigan and Tomas Kocisky and Tulsee Doshi and Vihan Jain and Vikas Yadav and Vilobh Meshram and Vishal Dharmadhikari and Warren Barkley and Wei Wei and Wenming Ye and Woohyun Han and Woosuk Kwon and Xiang Xu and Zhe Shen and Zhitao Gong and Zichuan Wei and Victor Cotruta and Phoebe Kirk and Anand Rao and Minh Giang and Ludovic Peran and Tris Warkentin and Eli Collins and Joelle Barral and Zoubin Ghahramani and Raia Hadsell and D. Sculley and Jeanine Banks and Anca Dragan and Slav Petrov and Oriol Vinyals and Jeff Dean and Demis Hassabis and Koray Kavukcuoglu and Clement Farabet and Elena Buchatskaya and Sebastian Borgeaud and Noah Fiedel and Armand Joulin and Kathleen Kenealy and Robert Dadashi and Alek Andreev},
      year={2024},
      eprint={2408.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.00118}, 
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{abdin2024phi3technicalreporthighly,
      title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
      author={Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and others},
      year={2024},
      eprint={2404.14219},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14219}, 
}


@misc{dpo2024,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2024},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.18290}, 
}

@article{christiano2017RLHF,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{lean2015lean,
  title={The Lean theorem prover (system description)},
  author={De Moura, Leonardo and Kong, Soonho and Avigad, Jeremy and Van Doorn, Floris and von Raumer, Jakob},
  booktitle={Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25},
  pages={378--388},
  year={2015},
  organization={Springer}
}

@inproceedings{moura2021lean4,
  title={The Lean 4 theorem prover and programming language},
  author={Moura, Leonardo de and Ullrich, Sebastian},
  booktitle={Automated Deduction--CADE 28: 28th International Conference on Automated Deduction, Virtual Event, July 12--15, 2021, Proceedings 28},
  pages={625--635},
  year={2021},
  organization={Springer}
}

@article{hendrycks2021MATH,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{schulman2017PPO,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{radford2018GPT,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}

@article{radford2019lGPT,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020GPT,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}


@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{lai2024stepdpo,
  title={Step-dpo: Step-wise preference optimization for long-chain reasoning of llms},
  author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Yang, Senqiao and Peng, Xiangru and Jia, Jiaya},
  journal={arXiv preprint arXiv:2406.18629},
  year={2024}
}

@inproceedings{azar2024dpo_variant,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4447--4455},
  year={2024},
  organization={PMLR}
}

@inproceedings{hong2024orpo,
  title={Orpo: Monolithic preference optimization without reference model},
  author={Hong, Jiwoo and Lee, Noah and Thorne, James},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={11170--11189},
  year={2024}
}

@article{xu2024contrastive,
  title={Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation},
  author={Xu, Haoran and Sharaf, Amr and Chen, Yunmo and Tan, Weiting and Shen, Lingfeng and Van Durme, Benjamin and Murray, Kenton and Kim, Young Jin},
  journal={arXiv preprint arXiv:2401.08417},
  year={2024}
}

@article{ethayarajh2024kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@article{xu2024chatglm,
  title={ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline},
  author={Xu, Yifan and Liu, Xiao and Liu, Xinghan and Hou, Zhenyu and Li, Yueyan and Zhang, Xiaohan and Wang, Zihan and Zeng, Aohan and Du, Zhengxiao and Zhao, Wenyi and others},
  journal={arXiv preprint arXiv:2404.02893},
  year={2024}
}

@article{jiao2024learning,
  title={Learning planning-based reasoning by trajectories collection and process reward synthesizing},
  author={Jiao, Fangkai and Qin, Chengwei and Liu, Zhengyuan and Chen, Nancy F and Joty, Shafiq},
  journal={arXiv preprint arXiv:2402.00658},
  year={2024}
}

@article{lu2024step-controlled-dpo,
  title={Step-controlled dpo: Leveraging stepwise error for enhanced mathematical reasoning},
  author={Lu, Zimu and Zhou, Aojun and Wang, Ke and Ren, Houxing and Shi, Weikang and Pan, Junting and Zhan, Mingjie and Li, Hongsheng},
  journal={arXiv preprint arXiv:2407.00782},
  year={2024}
}

@article{zeng2024token,
  title={Token-level Direct Preference Optimization},
  author={Zeng, Yongcheng and Liu, Guoqing and Ma, Weiyu and Yang, Ning and Zhang, Haifeng and Wang, Jun},
  journal={arXiv preprint arXiv:2404.11999},
  year={2024}
}

@article{polu2020gpt-f,
  title={Generative language modeling for automated theorem proving},
  author={Polu, Stanislas and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2009.03393},
  year={2020}
}

@article{han2021PACT,
  title={Proof artifact co-training for theorem proving with language models},
  author={Han, Jesse Michael and Rute, Jason and Wu, Yuhuai and Ayers, Edward W and Polu, Stanislas},
  journal={arXiv preprint arXiv:2102.06203},
  year={2021}
}

@inproceedings{de2015lean,
  title={The Lean theorem prover (system description)},
  author={De Moura, Leonardo and Kong, Soonho and Avigad, Jeremy and Van Doorn, Floris and von Raumer, Jakob},
  booktitle={Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25},
  pages={378--388},
  year={2015},
  organization={Springer}
}

@inproceedings{paulson1990isabelle,
  title={Isabelle: The next 700 theorem provers},
  author={Paulson, Lawrence C},
  booktitle={Logic and computer science},
  volume={31},
  pages={361--386},
  year={1990},
  organization={Citeseer}
}

@article{lin2024leanstar,
  title={Lean-star: Learning to interleave thinking and proving},
  author={Lin, Haohan and Sun, Zhiqing and Yang, Yiming and Welleck, Sean},
  journal={arXiv preprint arXiv:2407.10040},
  year={2024}
}

@article{lample2022hypertree,
  title={Hypertree proof search for neural theorem proving},
  author={Lample, Guillaume and Lacroix, Timothee and Lachaux, Marie-Anne and Rodriguez, Aurelien and Hayat, Amaury and Lavril, Thibaut and Ebner, Gabriel and Martinet, Xavier},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={26337--26349},
  year={2022}
}

@article{jiang2022thor,
  title={Thor: Wielding hammers to integrate language models and automated theorem provers},
  author={Jiang, Albert Qiaochu and Li, Wenda and Tworkowski, Szymon and Czechowski, Konrad and Odrzyg{\'o}{\'z}d{\'z}, Tomasz and Mi{\l}o{\'s}, Piotr and Wu, Yuhuai and Jamnik, Mateja},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8360--8373},
  year={2022}
}

@article{jiang2022dsp,
  title={Draft, sketch, and prove: Guiding formal theorem provers with informal proofs},
  author={Jiang, Albert Q and Welleck, Sean and Zhou, Jin Peng and Li, Wenda and Liu, Jiacheng and Jamnik, Mateja and Lacroix, Timoth{\'e}e and Wu, Yuhuai and Lample, Guillaume},
  journal={arXiv preprint arXiv:2210.12283},
  year={2022}
}

@article{wang2023lego,
  title={Lego-prover: Neural theorem proving with growing libraries},
  author={Wang, Haiming and Xin, Huajian and Zheng, Chuanyang and Li, Lin and Liu, Zhengying and Cao, Qingxing and Huang, Yinya and Xiong, Jing and Shi, Han and Xie, Enze and others},
  journal={arXiv preprint arXiv:2310.00656},
  year={2023}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}

@article{chang2021does,
  title={Does the order of training samples matter? improving neural data-to-text generation with curriculum learning},
  author={Chang, Ernie and Yeh, Hui-Syuan and Demberg, Vera},
  journal={arXiv preprint arXiv:2102.03554},
  year={2021}
}

@article{polu2022formalCL,
  title={Formal mathematics statement curriculum learning},
  author={Polu, Stanislas and Han, Jesse Michael and Zheng, Kunhao and Baksys, Mantas and Babuschkin, Igor and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2202.01344},
  year={2022}
}

@inproceedings{zombori2019curriculumTP,
  title={Curriculum learning and theorem proving},
  author={Zombori, Zsolt and Csisz{\'a}rik, Adri{\'a}n and Michalewski, Henryk and Kaliszyk, Cezary and Urban, Josef},
  booktitle={Conference on Artificial Intelligence and Theorem Proving},
  pages={37},
  year={2019}
}

@article{kumarappan2024leanagent,
  title={LeanAgent: Lifelong Learning for Formal Theorem Proving},
  author={Kumarappan, Adarsh and Tiwari, Mo and Song, Peiyang and George, Robert Joseph and Xiao, Chaowei and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2410.06209},
  year={2024}
}

@article{narvekar2020curriculumSurvey,
  title={Curriculum learning for reinforcement learning domains: A framework and survey},
  author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={181},
  pages={1--50},
  year={2020}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@inproceedings{wang2024mathshepherd,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9426--9439},
  year={2024}
}



@article{guo2024deepseekCODER,
  title={DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence},
  author={Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Yu and Li, YK and others},
  journal={arXiv preprint arXiv:2401.14196},
  year={2024}
}\

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@article{xin2024deepseek,
  title={DeepSeek-Prover-V1. 5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search},
  author={Xin, Huajian and Ren, ZZ and Song, Junxiao and Shao, Zhihong and Zhao, Wanjia and Wang, Haocheng and Liu, Bo and Zhang, Liyue and Lu, Xuan and Du, Qiushi and others},
  journal={arXiv preprint arXiv:2408.08152},
  year={2024}
}

@article{lu2024step,
  title={Step-controlled dpo: Leveraging stepwise error for enhanced mathematical reasoning},
  author={Lu, Zimu and Zhou, Aojun and Wang, Ke and Ren, Houxing and Shi, Weikang and Pan, Junting and Zhan, Mingjie and Li, Hongsheng},
  journal={arXiv preprint arXiv:2407.00782},
  year={2024}
}

@article{pal2024smaug,
  title={Smaug: Fixing failure modes of preference optimisation with dpo-positive},
  author={Pal, Arka and Karkhanis, Deep and Dooley, Samuel and Roberts, Manley and Naidu, Siddartha and White, Colin},
  journal={arXiv preprint arXiv:2402.13228},
  year={2024}
}

@book{megill2019metamath,
  title={Metamath: a computer language for mathematical proofs},
  author={Megill, Norman and Wheeler, David A},
  year={2019},
  publisher={Lulu. com}
}



@inproceedings{majumder2024tango,
  title={Tango 2: Aligning diffusion-based text-to-audio generations through direct preference optimization},
  author={Majumder, Navonil and Hung, Chia-Yu and Ghosal, Deepanway and Hsu, Wei-Ning and Mihalcea, Rada and Poria, Soujanya},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={564--572},
  year={2024}
}

@inproceedings{zheng-etal-2024-llamafactory,
    title = "{L}lama{F}actory: Unified Efficient Fine-Tuning of 100+ Language Models",
    author = "Zheng, Yaowei  and
      Zhang, Richong  and
      Zhang, Junhao  and
      Ye, Yanhan  and
      Luo, Zheyan",
    editor = "Cao, Yixin  and
      Feng, Yang  and
      Xiong, Deyi",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-demos.38/",
    doi = "10.18653/v1/2024.acl-demos.38",
    pages = "400--410",
    abstract = "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and 3,000 forks."
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{DBLP:journals/corr/abs-2102-06203,
  author       = {Jesse Michael Han and
                  Jason Rute and
                  Yuhuai Wu and
                  Edward W. Ayers and
                  Stanislas Polu},
  title        = {Proof Artifact Co-training for Theorem Proving with Language Models},
  journal      = {CoRR},
  volume       = {abs/2102.06203},
  year         = {2021},
  url          = {https://arxiv.org/abs/2102.06203},
  eprinttype    = {arXiv},
  eprint       = {2102.06203},
  timestamp    = {Thu, 18 Feb 2021 15:26:00 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2102-06203.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NEURIPS2023_44414694,
 author = {Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan J and Anandkumar, Animashree},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {21573--21612},
 publisher = {Curran Associates, Inc.},
 title = {LeanDojo: Theorem Proving with Retrieval-Augmented Language Models},
 
 volume = {36},
 year = {2023}
}
