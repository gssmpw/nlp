\documentclass[10pt,english]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % 支持\toprule、\midrule、\bottomrule
\usepackage{multirow} % 支持多行合并
\usepackage{array}    % 提供更灵活的列格式
\usepackage{caption}  % 控制caption样式
\usepackage{algorithm}  % 用于算法环境
% \usepackage{algorithmic} % 用于算法步骤的排版
\usepackage{algorithmicx} % 算法支持
\usepackage{algpseudocode} % 用于伪代码支持
\usepackage{pifont}
% \usepackage[section]{placeins} %%%图片段内浮动
\usepackage{stfloats}  %% 表格浮动

% For theorems and such
\usepackage{amsmath}  
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% \usepackage{xcolor}
\usepackage{tikz}
\usepackage[most]{tcolorbox}
% \usepackage[style=authoryear,maxnames=5]{biblatex}
% \usepackage[style=numeric, maxbibnames=5]{biblatex}

\usepackage{hyperref}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


%%% PACKAGES
\usepackage{fullpage}
\usepackage{amsmath,amssymb,amsthm,graphicx,subfigure,hyperref,algorithm,algpseudocode,bbm,mathtools,caption,multirow}
\usepackage[noadjust]{cite}
% \usepackage[table,xcdraw]{xcolor}
\hypersetup{colorlinks=true, citecolor=blue, urlcolor=blue}
\usepackage{parskip}

%%% THEOREM ENVIRONMENTS
\newtheorem{theorem}{Theorem}
\newtheorem{defi}{Definition}
\newtheorem{lem}{Lemma}

%%% COMMANDS
\newcommand{\bfs}{\textbf{BFS}}
\newcommand{\mcts}{\textbf{MCTS}}
\newcommand{\dpo}{\textbf{DPO}}
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}

\usepackage{algorithm}
\usepackage{algpseudocode}
% \usepackage{minted}

% Custom styles for keywords and comments
\newcommand{\keyword}[1]{\textbf{\textcolor{blue}{#1}}}
% \newcommand{\comment}[1]{\hfill\textcolor{green!50!black}{// #1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Reasoning Enhancement for Large Language Models via Counterexample Guidance}
% \icmltitlerunning{Reasoning Enhancement for Large Language Models via  Iterative Curriculum Learning}
% \icmltitlerunning{CuDIP:Enhancing theorem proving in LLMs via  Iterative Curriculum Learning}

% \icmltitlerunning{FPICL: Enhancing Theorem Proving in LLMs via Fine-Grained Preference Optimization and Iterative Curriculum Learning}
% \usepackage[maxbibnames=5]{biblatex}
% \addbibresource{reference.bib}


\title{\textbf{CuDIP: Enhancing Theorem Proving in LLMs via Curriculum Learning-based Direct Preference Optimization}}

\author{
    \textbf{Shuming Shi\textsuperscript{1}} \quad 
    \textbf{Ruobing Zuo\textsuperscript{1}} \quad 
    \textbf{Gaolei He\textsuperscript{2}} \quad 
    \textbf{Jianlin Wang\textsuperscript{2}} \quad \\
    \textbf{Chenyang Xu\textsuperscript{1}} \quad 
    \textbf{Zhengfeng Yang\textsuperscript{1}} \quad 
    \\[0.5em]
    {\textbf{\textsuperscript{1}East China Normal University,}}
    {\textbf{\textsuperscript{2}Henan University}}
    \\[0.5em]
    {\{smshi, rbzuo\}@stu.ecnu.edu.cn},
    {\{hegaolei, jlwang\}@henu.edu.cn},\\
    {\{cyxu, zfyang\}@sei.ecnu.edu.cn} \quad
}

% \author{
%     \textbf{Shuming Shi\textsuperscript{1}} \quad 
%     \textbf{Ruobing Zuo\textsuperscript{1}} \quad 
%     \textbf{Gaolei He\textsuperscript{2}} \quad 
%     \textbf{Jianlin Wang\textsuperscript{2}} \quad \\[0.3em]
%     \textbf{Chenyang Xu\textsuperscript{1}} \quad 
%     \textbf{Zhengfeng Yang\textsuperscript{1}} \quad 
%     \\[1.5em]
%     {\textbf{\textsuperscript{1}East China Normal University,}}
%     {\textbf{\textsuperscript{2}Henan University}}  \\[1.5em]
    % {\{smshi, \}@stu.ecnu.edu.cn} \quad
    % {\{hegaolei, jlwang\}@henu.edu.cn} \quad
    % {\{cyxu, zfyang\}@sei.ecnu.edu.cn} \quad

% }





\date{}
\begin{document}
\maketitle

% \icmltitle{ICL: Enhancing Theorem Proving in LLMs \\
% via  Iterative Curriculum Learning}
% \icmltitle{CVPI: Enhancing Theorem Proving in LLMs via \\
% Curriculum Learning-Based Prover-Verifier Iteration}
% \icmltitle{CPVI: Enhancing Theorem Proving in LLMs via \\
% Curriculum Learning-Based P-V Iteration}
% \icmltitle{Enhancing Theorem Proving in LLMs via Granular Preference Optimization and Curriculum Iteration Learning}

% \icmltitle{FPICL: Enhancing Theorem Proving in LLMs via Fine-Grained Preference Optimization and Iterative Curriculum Learning}


% \icmltitle{CuDIP: Enhancing Theorem Proving in LLMs via Curriculum Learning-based DPO Iteration}

% CuDIP: Curriculum learning-based DPO Iteration for Theorem Proving
% CuDIP: Enhancing theorem proving in LLMs via Curriculum-based DPO Iteration
% CuDIP: Enhancing theorem proving in LLMs via Direct Preference Optimization



%%%% 细粒度偏好优化 迭代课程学习







%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% Formal 
Automated theorem proving (ATP) is one of the most challenging mathematical reasoning tasks for Large Language Models (LLMs). 
Most existing LLM-based ATP methods rely on supervised fine-tuning, 
which results in a limited alignment between the theorem proving process and human preferences. 
% and lacks generalization ability. %%%%%%%%
% the theorem proving capability of LLMs still being insufficient.
% In fact, preference optimization methods 
% such as 
% Direct Preference Optimization (DPO) 
% , which align LLMs with human preferences, can be beneficial  for certain LLM tasks. 
% Preference optimization methods, such as Direct Preference Optimization (DPO), which align LLMs with human preferences, can be beneficial for certain tasks. 
Direct Preference Optimization (DPO), which aligns LLMs with human preferences, has shown positive effects for certain tasks. 
%%%%%?
% However, these preference optimization methods require high-quality preference data.
However, the lack of high-quality preference data for theorem proving presents a significant challenge. 
% In this paper, we innovatively introduce curriculum learning-based DPO iterative theorem proving (CuDIP). 
In this paper, we innovatively apply DPO to formal automated theorem proving and introduces a Curriculum Learning-based DPO Iterative Theorem Proving (CuDIP) method. % 大小写t
Specifically, we propose a method for constructing preference data which utilizes LLMs and existing theorem proving data to enhance the diversity of the preference data 
while reducing the reliance on human preference annotations. 
% We then combine this preference data construction method with curriculum learning to iteratively build and fine-tune DPO data. 
We then integrate this preference data construction method with curriculum learning to iteratively fine-tune the theorem proving model through DPO. 
Experimental results on the MiniF2F and ProofNet datasets demonstrate the effectiveness of the proposed method.

% TODO:
%偏好数据构建方法优点：
% （1）结合LLM构建偏好数据，减少对人类标注的依赖，
% （2）结合细粒度偏好评分，提高所生成偏好数据中positive sample的多样性（偏好数据的多样性）
%基于CL的DPO迭代训练方法优点：
% 1）DPO微调方式使得定理证明过程更能与人类偏好对齐，且相比于RLHF，采用DPO更加简单高效  
% 2）采用课程学习的方式对定理证明问题进行由易到难地迭代学习，通过这一过程提高LLM解决困难问题的能力，提高LLM解决定理证明问题的能力。


\end{abstract}

\section{Introduction}
\label{Instruction}
% 研究背景
% 研究目标
% 现有的方法问题
% Despite their success in various domains \cite{}, LLMs exhibit significant limitations in mathematical reasoning, particularly in tasks like theorem proving. 
Despite the success of LLMs in various fields 
\cite{ziegler2019fine,lightman2023let,wang2024mathshepherd,guo2024deepseekCODER},  
they still exhibit significant limitations in mathematical reasoning tasks, particularly in theorem proving. 
Enhancing the performance of LLMs in these tasks remains a challenging problem, especially for automated theorem proving in strictly formalized interactive theorem provers (ITPs) such as Lean \cite{lean2015lean} and Isabelle \cite{paulson1990isabelle}. Many studies integrate LLMs with ITPs for automated theorem proving. Existing approaches generally fall into two categories: using LLMs to provide stepwise proof strategies combined with search algorithms to complete proofs \cite{polu2020gpt-f,han2021PACT,lample2022hypertree}, 
and leveraging LLMs to independently or informally generate complete proofs \cite{jiang2022dsp,wang2023lego}. 
While these methods have achieved some success, most rely on supervised fine-tuning, resulting in limited alignment with human preferences, 
and the effectiveness of theorem proving remains constrained.
% and a lack of generalization capability. %  且缺少泛化能力 ?

In recent years, LLM fine-tuning methods aligned with human preferences have gained significant attention. 
Recent studies demonstrate that preference-based optimization for LLM fine-tuning can yield substantial benefits \cite{ziegler2019fine,stiennon2020learning}. 
Reinforcement Learning with Human Feedback (RLHF) \cite{christiano2017RLHF} is a well-established and effective method for aligning models with human preferences, typically consisting of two steps: (1) training a reward model, and (2) optimizing it using reinforcement learning methods such as PPO \cite{schulman2017PPO}. 
% Recent research \cite{} %Deepseek-Prover v1.5 TODO
% employed GRPO, a variant of PPO, to optimize theorem prover.
% Recent research \cite{xin2024deepseek} %Deepseek-Prover v1.5 TODO
% employed GRPO \cite{shao2024deepseekmath} to optimize theorem prover.  %%%  TODO: ADD to Relarted work
However, this approach has two primary limitations: 1) it is relatively complex; and 2) it entails significant computational overhead.

To address the aforementioned issue, 
Direct Preference Optimization (DPO) \cite{dpo2024} offers a simpler and more efficient alternative. 
DPO eliminates the need for training a reward model, directly optimizing from preferences, which makes it both straightforward and effective. 
DPO has shown promising results in certain mathematical reasoning tasks \cite{lu2024step,pal2024smaug}, 
but has yet to be applied to theorem proving tasks. 
However, a key challenge for preference-based optimization methods in LLM fine-tuning is the requirement for high-quality human preference labels, which are difficult to obtain.  %%% TODO ADD 还有数据多样性的问题（不使用细粒度评分构建偏好数据的方式 ） 

% To address the aforementioned issues, 
To overcome the challenges mentioned above, 
this paper proposes a method for automated theorem proving that combines DPO and curriculum learning, named \textbf{C}urriculum {L}earning-based \textbf{D}PO \textbf{I}terative Theorem \textbf{P}roving (CuDIP). 
To the best of our knowledge, this is the first work to integrate DPO with automated theorem proving, filling a gap in DPO-based automated theorem proving.
% To the best of our knowledge, this is the first work to integrate DPO for automated theorem proving. 
% In response to the lack of preference data for theorem proving, we 
To address the challenge of insufficient theorem proving preference data, this paper proposes a method for constructing preference data based on fine-grained scoring by LLMs, 
which reduces the reliance on human annotations by utilizing LLMs for preference data construction, and enhances the diversity of positive samples in the generated preference data through the incorporation of fine-grained preference scoring.
% with the aim of enhancing the diversity of the preference data. %%5
% Using the constructed preference data, and in conjunction with a curriculum learning, we propose a process for iteratively training a prover based on DPO within the framework of curriculum learning. 
Using the constructed preference data in conjunction with curriculum learning, we propose a process for iteratively training a prover based on DPO, which enhances the LLM's ability to solve difficult problems through progressively challenging DPO iterations. 
Specifically, we 
% employ the mainstream and reliable theorem proving assistant Lean, %%% LEAN 在哪说？
categorizing automated theorem proving problems into curriculum data based on predefined difficulty levels, and iteratively construct and fine-tune the DPO preference data following the principles of curriculum learning.
Experimental results indicate that the pass rate reaches 38.5\% on the MiniF2F dataset and 12.7\% on the ProofNet dataset, 
surpassing the baseline methods and highlighting the superior performance of the proposed method. %%% 添加实验结果----
% The experimental results (with pass@k reaching ** on the Minif2f dataset and pass@k reaching ** on the ProofNet dataset) demonstrate the superior performance of the proposed method.

Our contributions can be summarized as follows:
\begin{itemize}
    \item We propose a method for constructing DPO preference data based on fine-grained preference scoring by Large Language Models (LLMs), 
    which facilitates the generation of diverse preference data while reducing reliance on human annotations.
    % which enhances the diversity of preference data while building on existing datasets.
    \item Based on the proposed preference data construction method and curriculum learning, we introduce Curriculum Learning-based DPO Iterative Theorem Proving (CuDIP) method, which enhances the capability of LLMs in autumated theorem proving.
    % solving complex problems.
    % To the best of our knowledge, this is the first work to combine DPO with automated theorem proving, filling a gap in DPO-based automated theorem proving.%%%%%%%%
    \item Experimental results 
    % on Minif2f and ProofNet 
    demonstrate that the proposed method outperforms all baseline approaches, achieving a maximum improvement of 7.4\% on the MiniF2F-valid benchmark, thereby effectively enhancing the theorem proving capabilities of LLMs.
    % Experimental results on Minif2f and ProofNet 
    % demonstrate that the proposed method outperforms all baseline models and approaches, 
    % effectively enhancing the theorem proving capabilities of LLMs.
\end{itemize}


%%%% 基于LLM的定理证明  LLM reasoning 的困难性
% Lean 
% 自然语言FT发展
%%%% RLHF 方法的困难性
%%%% DPO数学推理 已有应用
% 尚未 应用在定理证明 %%%% 绝大多数？使用SFT
%%%% 在定理证明等专家数据下 使用DPO的问题 %%%% DPO 在 RLHF 基础上去掉了reward model，更加简单，但是其偏好数据是二进制分布的 ， 应用在具体问题上的绝对正例和绝对反例的偏好 for 高质量数据 仍然难以界定
%%%% 仍然需要高质量偏好数据。 
%%（Q: 细粒度评分后仍然回归了二进制。不能diss二进制。 优点在于除了专家数据之外，使用LM扩大了正例的多样性

%% 优点在于： 通过先使用LM生成示例，再使用LM对生成的示例更加细粒度地打分再构建DPO偏好数据。基于有限的人类专家数据， 使用LLM提供了更多的正例，扩充了偏好数据的多样性，并一定程度上弥补DPO细粒度控制的不足
% 
% 课程学习 困难  证明长度 数据划分

% 打分器自动替换搜索 更泛化  通用  
 
%%%% RLAIF 属于同一种 %%常规的 AI 给偏好是什么样的？

%%%% 现在基于 LLM 做judge model 等等工作 需要注意

% （针对创新点的几个部分分别提出现有的问题
% 本文研究内容
% 本文贡献
% 由于缺少定理证明人类偏好数据  mathlib等大数据库构建偏好数据耗时耗力，给出了一种基于LM生成示例并进行细粒度评分构建DPO偏好数据的方法。在给定正例的基础上，生成其他候选策略并评分，按照 细粒度的评分，划分为新添加的正例和反例。由此扩充了正例的多样性。TODO
%课程学习迭代

\section{Related Works} %%% 待精简
\textbf{Automated Theorem Proving with LLMs.}
% 两种范式： 1. LLM单步 + Search；2.LLM 利用非形式化语言得到完整证明过程
% 范式1： GPT-f  PACT HTPS  Lean-STaR Thor 
% 范式2： LEGO-Prover DSP 
% With the acceleration of Large Language Models (LLMs), automated theorem proving based on LLMs has been extensively applied, demonstrating significant progress. 
The rapid advancement of Large Language Models (LLMs) 
has spurred significant progress in automated theorem proving.
% has led to widespread applications of LLM-based automated theorem proving, achieving notable progress.
Various approaches integrate interactive theorem provers (ITP) such as 
Lean \cite{de2015lean}, Isabelle \cite{paulson1990isabelle}, and Metamath \cite{megill2019metamath} with language models. 
A prominent method, exemplified by GPT-f \cite{polu2020gpt-f}, involves the language model generating the next proof step based on the current proof state, followed by tree search to find a complete proof for the theorem. 
PACT \cite{han2021PACT} jointly trains a language model with a strategy prediction objective for interactive theorem proving, where the auxiliary task for extracting self-supervised signals is used. 
% utilizing self-supervised signals. 
HTPS \cite{lample2022hypertree} improves the automated theorem proving process by enhancing the MCTS-based proof search strategy. 
% Thor \cite{jiang2022thor}
% Lean-STaR \cite{lin2024leanstar} 
% Another approach uses LLMs to automatically generate the entire proof of a theorem, 
% Another approach utilizes informal proof languages to derive the complete proof of a theorem, such as DSP \cite{jiang2022dsp}, Lean-STaR \cite{lin2024leanstar}, and LEGO-Prover \cite{wang2023lego}.  
Another approach employs LLMs to derive the complete proof of a theorem, either directly or with the assistance of informal proof languages \cite{jiang2022dsp,lin2024leanstar,wang2023lego}. %%%%% ?TODO
However, most approaches rely on supervised fine-tuning (SFT), which may not align well with human preferences, potentially limiting their proving capabilities.

% or 本文遵循第一种范式
% In this paper, we introduce an innovative DPO-based automated theorem proving method that utilizes preference data to enhance the theorem proving capabilities of LLMs.

% Deepseek-Prover v1.5 GRPO ?
% or 现有方法效果仍然不够好 因此探索与人类偏好对齐的方法
% 然而现有的大部分方法使用SFT微调模型，与人类的偏好对齐程度不高），可能是限制现有方法证明能力的原因。我们给出了一种基于DPO的自动定理证明方法，利用偏好数据提高LLM定理证明能力。
% （基于偏好的方法）（提高了答案多样性？ 使得定理证明过程更与人类偏好对齐）

\textbf{Aligning LLMs with Human Preference.} %%% 部分内容可删掉/放Introduction 有点多
Reinforcement Learning from Human Feedback (RLHF) \cite{christiano2017RLHF} is a classical and effective method for aligning LLMs with human preferences. This approach involves initially training a reward model, followed by optimization using reinforcement learning algorithms such as 
PPO \cite{schulman2017PPO}, 
% Proximal Policy Optimization Algorithms (PPO) \cite{schulman2017PPO}, 
leading to substantial success in models like 
ChatGPT %\cite{achiam2023gpt}, 
\cite{radford2019lGPT,brown2020GPT,achiam2023gpt}, % GPT? ChatGPT
% \cite{radford2018GPT,radford2019lGPT,brown2020GPT,achiam2023gpt}, % GPT? ChatGPT
LLaMA \cite{touvron2023llama,dubey2024llama}, 
% Gemini \cite{}, 
and Claude \cite{bai2022training}. 
However, using PPO for RLHF is a complex and computationally expensive process. Motivated by this, Direct Preference Optimization (DPO) \cite{dpo2024} has emerged as an effective alternative. 
DPO directly optimizes using preference data, without the need to train a reward model. 
Several works \cite{ethayarajh2024kto,hong2024orpo,lai2024stepdpo,xu2024contrastive,zeng2024token,azar2024dpo_variant}  %KTO,  CPO,, ORPO ,Step-DPO,Token-level DPO 可删 %xu2024contrastive,zeng2024token,azar2024dpo_variant,
have proposed variations based on DPO. 
The simplicity and efficiency of DPO have enabled its application in various downstream tasks, such as 
mathematical reasoning \cite{xu2024chatglm,jiao2024learning,lu2024step-controlled-dpo}, %Chat-GLM & %,lu2024step-controlled-dpo
multimodal tasks \cite{majumder2024tango}, 
and 
summarization \cite{dpo2024}.  %%% 只介绍数学推理?
However, DPO still relies on high-quality preference labels to generate preference data, which requires costly manual annotation. 
%%%% 表述 DPO性能依赖
In this paper, we propose a method for constructing DPO preference data and 
% utilize the generated data to improve DPO's performance in mathematical theorem proving.
the generated preference data is utilized in DPO iterative training to enhance the theorem proving capabilities of LLMs.
% This paper presents a preference data construction method based on fine-grained LLM scoring, which reduces the reliance on human annotations. The generated preference data is utilized in DPO iterative training to enhance the theorem proving capabilities of LLMs.
%   TODO Deepseek-Prover v1.5 GRPO 成本高?
%   CL+PPO）Curriculum Learning and Theorem Proving Zsolt Zombori

%% *** DPO的 具体 应用
%% DPO在数学推理等多个领域的应用 尚未应用在定理证明 需要高质量的偏好数据

\textbf{Curriculum Learning.} %提出根据证明难度优化学习轨迹的CL LeanAgent 
Curriculum learning \cite{bengio2009curriculum} 
is a method that simulates the human learning process by progressing from simpler to more complex tasks. 
Chang et al. \cite{chang2021does} applied curriculum learning to data-to-text generation, achieving improvements in generation quality. %%% 这句可删
By strategically organizing the learning trajectory, curriculum learning enhances model performance or training efficiency and has shown promise in mathematical reasoning tasks such as theorem proving. %%% 可删
% In theorem proving tasks, curriculum learning has also demonstrated significant potential. 
Polu et al. \cite{polu2022formalCL} introduced a curriculum of progressively harder statements by synthesizing inequalities with increasing difficulty, 
%  \cite{ zombori2019curriculumTP} 使用证明的长度确定强化学习的奖励。
while LeanAgent \cite{kumarappan2024leanagent} categorized theorems into three levels of complexity and utilized curriculum learning to train on mathematical repositories. 
% These studies highlight the effectiveness of curriculum learning methods. 
In this paper, we propose a curriculum learning iterative training method based on the difficulty of theorem proof data and demonstrate its effectiveness.
% \cite{narvekar2020curriculumSurvey} TODO 可以参考这篇再改 & methodology说明

%% 我们的方法独特地结合了CL DPO  
%%% Iteration learning
% \textbf{Curiculum learning based}
% \textbf{LLM as judge.} %%or Reeard Model ? 表述待修改 


\section{Preliminaries and Notation}% 可以放附录
\subsection{Preliminaries}
\label{section_3.1}
\textbf{Theorem Proving in Lean.} 
Lean \cite{lean2015lean,moura2021lean4} is a reliable interactive theorem proving assistant. Unlike natural language-based theorem proving, Lean requires a more rigorous proof process. Specifically, the theorem proving in Lean is interactive, where each step is carefully checked. 
% to ensure the proof's formalization and accuracy. %%%whether
% rigorously checks each step, ensuring a formal and precise proof. 
In Lean, the theorem statement serves as the initial goal, and each formalized proof strategy, called a "tactic", is applied to reach new subgoals. 
The proof is considered complete when the "no goals" state is reached.
% When the "no goals" state is reached, it signifies the successful completion of the proof. %%%% Search-based Theorem Proving


\textbf{Direct Preference Optimization (DPO).} DPO \cite{dpo2024} is an offline reinforcement learning method that aligns the outputs of Large Language Model (LLM) with human preferences. 
Unlike Reinforcement Learning from Human Feedback (RLHF) \cite{christiano2017RLHF}, which first trains a reward model and then applies reinforcement learning, DPO directly optimizes based on preference data. Each sample in the DPO preference dataset is a triplet $\left(x, y_w, y_l\right)$, where $y_w$ is the preferred response and $y_l$ is the dispreferred response, both corresponding to the input prompt $x$. The training objective is to minimize the following loss:

\label{eq_loss}
\begin{equation}
\mathcal{L}_{DPO}({\pi}_\theta;{\pi}_{\text{ref}}) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \Big[ \log \sigma \Big( \beta \log \frac{\pi_\theta(y_w | x)}{ \pi _{\text{ref}}(y_w | x)} 
- \beta \log \frac{\pi_\theta(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \Big) \Big],
\end{equation}

where $\mathcal{D}$ is the preference dataset, $\sigma$ is the sigmoid function, $\pi_{\theta}$ is the model to be optimized, initialized as $\pi_{\text{ref}}$, and $\beta$ is a hyperparameter used to control the degree of divergence between $\pi_{\theta}$ and $\pi_{\text{ref}}$.
% \begin{equation}
%     \mathcal{L}_{DPO}({\pi}_\theta;{\pi}_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}_{DPO}} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_{\theta}(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right]
% \end{equation}

% \textbf{Curriculum Learning.} 


%%% 或者抽象成 MDP. ？
\subsection{Problem Definition}  %%%% 形式化定理证明问题
\label{Section:3.2}
This paper investigates the problem of automated theorem proving in the Lean language. The theorem proving process can be abstracted as a tree search process, where the root node represents the theorem statement to be proven. 
Each node in the tree corresponds to a proof state $s$ 
(i.e., a proof subgoal and corresponding premises), 
and each edge represents a proof tactic $t$. 
The initial state $s_0$ represents the target theorem statement to be proved. Finding a valid proof for $s_0$ entails identifying a complete path 
% in the theorem proving tree 
starting from $s_0$ that successfully resolves the proof goals along the path.%% LLM提供候选策略


\subsection{Notation}  %%% 这里不需要有大括号里面的内容  %% [可以删掉]有点多余
In this paper, we define $\mathcal{D}$ as the existing formalized theorem dataset with complete proof steps, 
% available theorem proving dataset, 
and $\mathcal{C}= \{\mathcal{C}_1,\mathcal{C}_2,...,\mathcal{C}_n\}$ as the curriculum dataset derived from $\mathcal{D}$.  %n 表示问题困难程度
%%%% $\mathcal{C}$  = \{C_1,C_2,...,C_n\}
$\mathcal{P}$ represents the prover model, where $\mathcal{P}_{n}$ denotes the prover model at the $n$-th iteration, and $\mathcal{P}_{0}$ refers to the base prover obtained through supervised fine-tuning (SFT) of the base model. 
$\mathcal{G}$ represents the score generator model used to produce preference data, %% score 
% . Similarly, 
and $\mathcal{G}_{n}$ denotes the score generator model at the $n$-th iteration.

The theorem proving state at step $i$ is denoted as $s_i$. 
$d_i$ denotes the \textit{distance} from $s_i$ to completing the proof, and $DIF_i$ represents the \textit{difficulty} of $s_i$. %% 是否需要脚注
$\mathcal{T}$ represents the set of all candidate tactics for a given proof goal. %, where $k$ is the limit on the number of candidate tactics. % = \{t_1, t_2, \dots, t_k\}
$\mathcal{SC}$ denotes the dataset of corresponding preference scores for the candidate tactics in $\mathcal{T}$. 
% SC^0
$\mathcal{D}_P$ represents the preference dataset for DPO.
% $\mathcal{D}_P = \{p_1, p_2, \dots, p_n\}$ represents the preference dataset for DPO, where $p_i = \{t_{\text{accept}}, t_{\text{reject}}\}$ denotes a preference pair consisting of a more preferred tactic $t_{\text{accept}}$ and a less preferred tactic $t_{\text{reject}}$.


% 

%%%% FG-DPO 对于专家数据 01  更加细粒度地评分，并扩大绝对正例的多样性
%%%% FG-DPO 和 记忆增强课程学习迭代 分成两章 或者合在一章
%%%% 基于FG-DPO 和 互监督的记忆增强课程学习迭代 

% \section{Fine-Grained Direct Preference Optimization (FG-DPO)}
%%%% Q: FG-DPO?  RL AI F?
\section{Methodology}   %%%%
% \section{Curriculum Learning-Based Prover-Verifier Iteration} %% Memory-enhancing Curriculum Learning-Based P-V Iteration


%%%%%%% overview 待修改
\begin{figure*}[t]%[htbp]%[htbp]
\centering
\includegraphics[width=0.9\linewidth]{overview_25.1.31_v1.pdf}%{figures/overview/overview_25.1.30_v2.pdf}%{figures/overview/overview_25.1.15_v1_no_symbols.pdf}
\caption{Overview of \textbf{C}urriculum {L}earning-based \textbf{D}PO \textbf{I}terative Theorem \textbf{P}roving  (\textbf{CuDIP}). %%%说明
% The three colored arrows represent the processes of the three stages. 
\textbf{Stage 1: Preparation.} 
The original data is partitioned to create curriculum data, and the base prover is trained using Supervised Fine-Tuning (SFT).
% In this stage, the original data is partitioned to form curriculum data, and the base prover is obtained through Supervised Fine-Tuning (SFT). 
\textbf{Stage 2: Preference Data Generation.} 
% A scoring data generator is trained, which uses the scoring generator to produce fine-grained scores for candidate strategies corresponding to a given proof state. Based on these scores, filtering and pairing are performed to obtain preference data.
A score generator is trained to assign fine-grained scores to candidate strategies for a given proof state, followed by filtering and pairing to generate preference data. %%%TODO SG合并到FGPS里面
\textbf{Stage 3: Curriculum-based DPO Iteration.} 
The prover model undergoes iterative DPO fine-tuning based on the generated preference dataset, refining both the preference data and the prover model through successive iterations.
% Based on the generated preference data set P, the prover model undergoes DPO fine-tuning, and the process of constructing DPO preference data and fine-tuning the prover model is iteratively refined.
}
\label{overview_1}
\end{figure*}
In this section, we introduce the proposed \textbf{C}urriculum {L}earning-based \textbf{D}PO \textbf{I}terative Theorem \textbf{P}roving (\textbf{CuDIP}). 
First, we present our key insights and an overview of the framework of the proposed CuDIP method, which consists of three stages: 
(1) Preparation, 
(2) Preference Data Generation, 
and (3) Curriculum-based DPO Iteration. % on Prover. %Iterative DPO on Prover. 
Subsequently, we provide a detailed introduction to each of these stages. %% 一种偏好数据构建方式和一种DPO迭代微调范式
%%% 基于细粒度评分的偏好数据构建方法
%%% & 基于课程学习的DPO迭代
%%%% 表述： Course-based iterative DPO ？
%%%%  Curriculum-based DPO iteration 
%%%% Curriculum learning-based DPO Iteration (CLI)

\subsection{Key Insights and Framework Overview}
% \subsection{Key Insights and Method Overview}

Our proposed approach is based on the following observations and assumptions.
% \begin{itemize}
\begin{enumerate}
% \item For theorem proving tasks based on LLMs, incorporating multi-step proof sequences with \textit{memory} into the training data, rather than using a single-step proof state with the corresponding next-step strategy, may enhance the LLM's ability to solve theorem proving problems.%%%% 去掉multi-step memory
% \item Compared to RLHF, 
% % and other LM fine-tuning methods, 
% using the DPO fine-tuning method for preference alignment requires less human and computational resources while achieving better results than SFT.

\item The preference-based LLM fine-tuning method DPO can be applied to theorem-proving tasks in LLMs to enhance reasoning capabilities and align better with human preferences. %%% to modify  preference-based
%%%

\item Similar to the human learning process, where knowledge is acquired progressively from simple to complex, training LLMs in a stepwise manner—from easy to difficult—may enhance their learning process and improve their ability to tackle more complex problems.

% \item Previous studies \cite{bengio2009curriculum} have shown that curriculum learning can help agents find better local minima in non-convex optimization problems. The task of theorem proving involves navigating highly non-convex optimization landscapes. \cite{kumarappan2024leanagent}
% particularly when dealing with complex mathematical propositions. \cite{}
% TODO: 改成课程学习 优化
% 智能体可以利用课程学习来减少处理复杂问题时进行广泛探索的需要。\cite
% 课程学习可以引导智能体在非凸优化问题中达到更好的局部最小值\cite。而定理证明涉及导航高度非凸的优化环境，特别是对于复杂语句。 （LeanAgent

\item The off-the-shelf expert data from the theorem proving process can serve as positive examples for DPO training. Additionally, existing language models can generate counterexamples and more diverse DPO training data based on the available theorem-proving data.
\item When constructing DPO preference data, applying more fine-grained preference scoring to the data prior to forming preference pairs enhances the diversity of positive examples in the preference data.  %%% TO BE MODIFIED
% \end{itemize}
\end{enumerate}

Based on the aforementioned key insights, we propose 
a curriculum learning-based DPO iterative theorem proving method (CuDIP) 
for the theorem proving problem defined in Section \ref{Section:3.2}. 
To address the lack of preference data in theorem proving, 
we propose a fine-grained scoring-based preference data generation method, which utilizes existing theorem proving data and LLMs to generate more diverse preference data, thereby reducing the reliance on human annotations. 
% we introduce a method for generating DPO preference data using LLMs and existing datasets. 
We then apply curriculum learning-based DPO iteration to the prover, leveraging the proposed preference data generation method and curriculum learning.
% Subsequently, we apply the proposed preference data generation method to perform curriculum-based DPO iterations on the prover.  %%% 改表述  结合课程学习

% For the theorem proving problem defined %in the Lean language 
% in Section \ref{Section:3.2}, we propose a theorem proving method based on curriculum learning and fine-grained preference scoring for iterative training. First, we present a method for constructing DPO preference data pairs using fine-grained preference scoring with LLMs. Then, combining curriculum learning, we introduce an iterative training approach based on curriculum learning and fine-grained preference scoring. 
% Figure \ref{overview_1} gives an overview of our proposed iterative training method. 
Figure \ref{overview_1} gives an overview of our proposed CuDIP method. 
Specifically, the overall algorithm can be divided into the following three stages: 
% (1) Preparation: Construct memory-augmented curriculum data and obtain the base prover; (2) Construct preference data pairs for DPO based on fine-grained preference scoring; (3) Perform DPO fine-tuning on the prover.
%%%三小段 分别叙述

\textbf{Stage 1: Preparation.} 
The existing formal theorem proving dataset  $\mathcal{D}$ is partitioned based on the \textit{difficulty} of the tasks to construct a curriculum learning dataset $\mathcal{C}$. The base model of the prover is then fine-tuned in a supervised manner using the entire curriculum learning dataset $\mathcal{C}$, resulting in the base prover $\mathcal{P}_{0}$. %% 

\textbf{Stage 2: Preference Data Genertion.} (1)Based on the current prover model and a subset of %proof state data 
%from 
the new round of curriculum learning data $\mathcal{C}_{n}$, the preference score data $\mathcal{SC}_{0}$ for training the score generator is derived through Fine-Grained Preference Scoring (FGPS) process. %%%%%%
(2)Supervised fine-tuning (SFT) is applied to the current generator model using $\mathcal{SC}_{0}$, %and the prompts from the curriculum learning data $\mathcal{C}_{n}$, 
resulting in a new score generator model $\mathcal{G}_{n}$. %%有点冗余
(3)$\mathcal{G}_{n}$ is then used to score the candidate tactics across all states in $\mathcal{C}_{n}$, and the preference dataset $\mathcal{D}_P$ is obtained through \textit{filtering and pairing}.

% \textbf{Stage 2: DPO Preference Data Generation.} 
% % \textbf{Stage 2: Construct preference data pairs for DPO based on fine-grained preference scoring.}
% Based on the current prover model and a subset of proof state data from the next-round curriculum learning dataset %,  
% $\mathcal{C}_{n}$, 
% preference scoring data 
% $\mathcal{S}_0$ 
% is generated using a tactic scoring mechanism to train the generator. Then, the current generator model is fine-tuned (SFT) with $\mathcal{S}_0$ and prompts from $\mathcal{C}_{n}$, resulting in an updated generator model $\mathcal{G}_n$. Finally, $\mathcal{G}_n$ is used to score candidate tactics for all proof states in the current curriculum dataset, and preference pairing is performed.

% \textbf{Stage 3: Perform Iterative DPO Fine-Tuning on the Prover.} 
% \textbf{Stage 3: Iterative DPO on Prover.}
\textbf{Stage 3: Curriculum-based DPO Iteration.}
The current prover model is fine-tuned using the generated preference data $\mathcal{D}_P$ through the DPO method. \textbf{Stage 2} and \textbf{3} are then repeated iteratively until no further performance improvement is observed. 

Next, we will provide a detailed technical description of the three stages of the proposed method. 

%%%%% Work Flow
% Summerize: 
% Step 1 and Step 2: Preparation %% (建立memory课程数据和base prover)
% Step 3 and Step 4:  SFT on Verifier %%   (细粒度评分的verifier建模 得到评分数据)
% Step 5 and Step 6:  Fine-Grained Preference Optimization  %%(FG-DPO) 细粒度偏好的DPO   （对Prover 进行DPO）
% Step 7 and Step 8:  Curriculum Learning-Based Prover-Verifier Iterative Training %%(基于课程学习的P-V迭代训练)

% \subsection{Stage 1: Preparation}
\subsection{Stage 1: Preparation}
\label{Stage1}
Before introducing the process of constructing curriculum data, we first define two concepts. 

\newtheorem{definition}{Definition}
\begin{definition}
\label{def:distance}
 (Distance). For a given proof tree $T$ of a theorem, the \textit{distance} of a proof state $s$ is defined as the number of proof tactics that need to be executed from $s$ 
 % to obtain a complete proof for the theorem. 
 to the completion of the proof (i.e., reach the "no goals" state in Lean).  
\end{definition}

% \newtheorem{definition}{Definition}
\begin{definition}
\label{def:difficulty}
(Difficulty). The \textit{difficulty} of resolving a proof goal $s$ is defined as the \textit{distance} in the proof tree from $s$ to the final proof goal.  %%%or \textit{difficulty coefficient}  %%% 
\end{definition}

% \textbf{Curriculum Learning Data Construction.} 
\textbf{Curriculum Data Construction.} 
The curriculum dataset $\mathcal{C}$ is derived by partitioning the original theorem-proving dataset $\mathcal{D}$ based on  increasing \textit{difficulty} values. 
Specifically, for each theorem and its corresponding proof process in $\mathcal{D}$, we extract the proof state $s$ at each step of the proof process and the corresponding subsequent proof tactic $t$. 
We then calculate the \textit{difficulty} $DIF_i$ of each proof state $s_i$. %to the completion of the proof (i.e., the "no goals" state).  
According to definition 
\cref{def:difficulty}
% \ref{def:difficulty}
, $DIF_i$ of $s_i$ is defined as the \textit{distance} $d_i$. % and obtain the difficulty $DIF_i$ corresponding to each proof state. 
This process results in intermediate data composed of triplets, $\mathcal{D}_{\text{triplet}}$, which can be formally expressed as:

% By Definition 3.2, the difficulty \textit{difficulty} $DIF_i=d_i$. 
\begin{equation}
\mathcal{D}_{triplet} = \{ (s_i, t_i, DIF_i) \mid i = 1, 2, \dots, n \},
\end{equation}
where $s_i$ represents the current theorem proving state, $t_i$ denotes the next proving tactic of $s_i$, and $DIF_i$ indicates the \textit{difficulty} of $s_i$. %%%%% goal or state??

Then, divide the dataset $\mathcal{D}_{\text{triplet}}$ into subsets in ascending order of $DIF_i$. This results in a curriculum dataset $\mathcal{C}$ organized from easy to hard, which can be formally expressed as:

\begin{equation}
    \mathcal{C} = \{ \mathcal{C}_n \mid n \in \{DIF_i \mid (s_i, t_i, DIF_i) \in \mathcal{D}_{\text{triplet}} \} \},
\end{equation}
\begin{equation}
    \mathcal{C}_n = \{ (s_i, t_i, DIF_i) \mid DIF_i = n \},
\end{equation}  %%% DIF_i做准则但不在C里面

where $\mathcal{C}$ is the set of all subsets $\mathcal{C}_n$, each corresponding to a unique \textit{difficulty} level $n$, ensuring that triplets with the same \textit{difficulty} are grouped together. In the subsequent training process, further processing is performed based on dataset $\mathcal{C}$. %%有点冗余

\textbf{Base Prover Training.} To achieve better proving performance, 
% during the iterative process, 
we first train the base model used as the prover model before the subsequent iterations. The entire curriculum learning dataset $\mathcal{C}$ is used to perform supervised fine-tuning (SFT) on the base model, resulting in the base prover $\mathcal{P}_{0}$.

After obtaining the curriculum learning data %$\mathcal{C}$
and the base prover, we next construct a preference dataset for DPO fine-tuning.

% \subsection{Stage 2: Preference Data Generation}
\subsection{Stage 2: Preference Data Generation}
\label{Stage2}
 We propose a method for constructing preference data by leveraging LLM and existing formal theorem-proving datasets.  %% benefits 
 First, we define a scoring rule and assign fine-grained preference scores to each theorem proof state and its corresponding candidate tactics in curriculum dataset acheived in \cref{Stage1}.
 % section \ref{Stage1}. 
 % $\mathcal{C}_n$. 
 Next, the data is filtered and paired based on the scores to form preference pairs, resulting in the final DPO preference dataset. To enhance the efficiency of the data generation process, we introduce a score generator during the preference scoring stage, replacing manually defined scoring rules. 
 % thereby improving the overall efficiency of data construction. 
 The proposed method offers the following benefits: (1) reducing reliance on human annotations by utilizing LLMs for preference data construction, and (2) enhancing diversity of positive samples through fine-grained preference scoring. 
 % The proposed method offers the following advantages: (1) By leveraging LLMs to construct preference data, the reliance on human annotations is reduced; (2) By incorporating fine-grained preference scoring, the diversity of positive samples in the generated preference data is enhanced. 
 The method proceeds as follows.%%%% 冗余


% To fine-tune the prover using Direct Preference Optimization (DPO), high-quality preference data is essential. 
% We propose a method for constructing such data by leveraging Large Language Models (LLMs) and existing formal theorem-proving datasets. %% 冗余
% First, we assign fine-grained preference scores to each theorem proving state and its corresponding candidate tactics. Then, the data is filtered and paired to form preference pairs, ultimately resulting in the DPO preference dataset. To enhance the efficiency of the data generation process, we introduce a generator which can generate preference scores that replaces manually defined scoring rules for preference evaluation. The method proceeds as follows.   %%% to be modified
% Specifically, the proposed preference data generation method consists of three steps: 1) fine-grained preference scoring; 2) training the scoring generator; 3) filtering and pairing.

%%%%  符号表示可以删掉 非必要?
\textbf{Tactic Samples Generation.}  %%%sample
% For each data entry  $(s_i, t_i)$ 
For each proof state $s$ in the formalized course dataset $\mathcal{C}_n$, $k$ candidate tactics are generated using the current prover model $\mathcal{P}_{n-1}$, forming a candidate tactic set 
% $\mathcal{T}_i$.  %%%% 生成k个候选策略
% $\mathcal{T}=\left\{ t_{i}\right\}_{i=1}^{k}$
% $\mathcal{T} = \{ t_{i} \mid i = 1, 2, \dots, k \} $ . 
$\mathcal{T} = \{ t_{1},t_{2},...,t_{k} \} $ . 
% which can be expressed as:
% \begin{equation}
%     \mathcal{T} = \{ t_{j} \mid j = 1, 2, \dots, k \} 
% \end{equation}
% = {(t_{i1},t_{i2},...,t_{ik})}$.  %% t_ij,j from 1 to k 
%%%% 这里可以不细写T_i


\textbf{Fine-Grained Preference Scoring (FGPS).} 
%%细粒度的偏好评分  评分机制  TODO具体MCTS搜索可以放附录
% \textbf{Search and Scoring}
% To obtain preference scores for each proof step in the formalized course dataset $\mathcal{C}_n$, we employ the following scoring mechanism: 
For the candidate tactic set $\mathcal{T}$ originating from proof state $s$, all tactics in $\mathcal{T}$ are evaluated using the following scoring process.
% \begin{itemize}
%     \item 
% \end{itemize}
\begin{enumerate}
    \item Execute each candidate tactic from $s$ to reach a new proof state and perform $n_{\text{attempt}}$ subsequent proof search attempts using the current prover $\mathcal{P}_{n-1}$ and Monte Carlo Tree Search (MCTS). 
    % Execute each candidate tactic $t_{ij}$ based on $s_i$ to reach a new proof state $s_{ij}$. From $s_{ij}$, perform $n_{\text{attempt}}$ subsequent proof search attempts using the current prover $\mathcal{P}_{n-1}$ and Monte Carlo Tree Search (MCTS). 
    %Execute each candidate tactic in $\mathcal{T}_i$ to reach a new set of proof states $\mathcal{S}_i$. Using the current prover $\mathcal{P}_{n-1}$ and Monte Carlo Tree Search (MCTS), perform $n_\text{attempt}$ subsequent proof search attempts for each state in $\mathcal{S}_i$.
    \item The score for a given candidate tactic $t$ of proof state $s$ is calculated using the following equation:
        \begin{equation}
            \text{Score} \left(t\mid s\right) = \frac{n_\text{success}}{n_\text{attempt}},
        \end{equation}
        where $n_\text{success}$ represents the number of successful proofs found after executing the proof tactic $t$, and $n_\text{attempt}$ is the number of subsequent proof search attempts for each candidate tactic.
\end{enumerate}

\begin{figure}[tbp]%[htbp]%[htbp]
\centering
\includegraphics[width=0.6\linewidth]{FG_preference_scoring_v4.pdf}%{figures/preference_scoring/FG_preference_scoring_v3.1.pdf}%{figures/preference_scoring/FG_preference_scoring_v2_conv.pdf}
\caption{
% Fine-Grained Preference Scoring (FGPS). 
An example of fine-grained preference scoring of all candidate tactics for the initial state of the Lean theorem (a b c: $\mathbb{N}$): a + b + c = c + b + a.
}%%%%%%[todo]
\label{FGPS}
\end{figure}

As shown in \cref{FGPS}, an example of the proposed fine-grained scoring process is provided. %%%%%%[todo]
However, The above scoring process necessitates multiple MCTS search attempts for each proof state, incurring significant time costs. Therefore, we randomly select a subset of 
% $s_i$ and corresponding $\mathcal{T}_i$ from 
the course data $\mathcal{C}_n$ for preference scoring to obtain the original score data $\mathcal{SC}^0$, and introduce a \textit{score generator} to score the remaining data in $\mathcal{C}_n$. %This language model-based scoring replaces the proof search process, thereby enhancing the efficiency of preference data construction.  %%%%% 声明 随机选择 k 个证明状态




\textit{REMARK}. For the remaining unscored data, language model-based scoring replaces the proof search scoring process, improving the efficiency of preference data construction.  %%%% TO BE MODIFIED

\textbf{Score Generator Training.} %% 训练Generator ： 构造偏好评分训练数据；SFT
% Using $\mathcal{SC}_0$, the existing language model $\mathcal{G}_{n-1}$ is trained to produce the score generator $\mathcal{G}_n$ for preference scoring. The score generator is then used to evaluate the remaining data, yielding the final scoring dataset $\mathcal{SC}$.
Using $\mathcal{SC}^0$, the existing language model $\mathcal{G}_{n-1}$ undergoes SFT to produce the score generator $\mathcal{G}_n$. The score generator is then used to evaluate the remaining data, resulting in the final scoring dataset $\mathcal{SC}$, which can be expressed as:
\begin{equation}
% \begin{aligned}
%     &\mathcal{SC} = \left\{ (s_i, T_i) \mid i = 1, 2, \dots, x \right\}, \quad \\&\text{where} \quad T_i = \left\{ (t_{ij}, \text{Score}(t_{ij} | s_i)) \mid j = 1, 2, \dots, k \right\}
% \end{aligned}
\mathcal{SC} = \{ (s_i, T_i) \mid i = 1, 2, \ldots, x \},
\end{equation}
\begin{equation}
% T_i = \{ (t_{ij}, \text{Score}_{ij}) \mid j = 1, 2, \ldots, k \}.
T_i = \{ (t_{i}^{(j)}, \text{Score}\left(t_{i}^{(j)}\mid s_i\right)) \mid j = 1, 2, \ldots, k \}.
\end{equation}
Where \( s_i \) represents the \( i \)-th proof state in $\mathcal{C}_n$, 
\( T_i \) is the set of candidate proof tactics of $s_i$ and their corresponding preference scores, 
\( t_{i}^{(j)} \) denotes the \( j \)-th candidate tactic for \( s_i \), 
\( x \) is the total number of proof states in $\mathcal{C}_n$, and \( k \) is the number of candidate tactics for a given state. 
% Here:     %%%%%%  删掉或放在notetion ？
% \begin{itemize}
%     \item \( s_i \) represents the \( i \)-th proof state in $\mathcal{C}_n$.
%     \item \( T_i \) is the set of candidate proof tactics of $s_i$ and their corresponding preference scores.
%     \item \( t_{i}^j \) denotes the \( j \)-th candidate tactic for \( s_i \).
%     % \item \( \text{Score}_{ij} \) is the preference score for \( t_{ij} \).
%     \item \( x \) is the total number of proof states in $\mathcal{C}_n$, and \( k \) is the number of candidate tactics for a given state.  %%% 或者放在Notation里面一起介绍
% \end{itemize}

% \begin{equation}
%     \mathcal{SC} = \{ (s_i, \{ (t_{ij}, \text{Score}_{ij}) \ |\ j = 1, 2, \dots, k \}) \ |\ i = 1, 2, \dots, x \}
% \end{equation}
%%%% $\mathcal{SC}$=\{(s_i,t_i,score_i)\}其中s_i,C_n   %%%% 具体形式需要重新表示



%%%% TODO 上面两个部分应该合在一起=========================  得到细粒度评分数据

% \textbf{Generate Fine-Grained Preference Scoring Data.}

\textbf{Filtering and Paring.} 
Perform data filtering and pairing on the dataset $\mathcal{SC}$, which contains all states along with their corresponding candidate tactics and tactic preference scores.
Specifically, for a given proof state $s$, candidate tactics from its tactic set $\mathcal{T}$ are selected for pairwise comparison if the difference in their preference scores exceeds a certain threshold $\text{Th}$. %The resulting preference pair $\{s, t_w, t_l\}$ satisfies $\text{Score}(t_w | s) > \text{Score}(t_l | s) $ and $\text{Score}(t_w | s) - \text{Score}(t_l | s) > \text{Th}$.
A preference pair $\{s, t_w, t_l\}$ is considered valid if the following conditions are met:
\begin{equation}
    \text{Score}(t_w | s) > \text{Score}(t_l | s) ,
    % \text{Score}(t_w | s) - \text{Score}(t_l | s) > \text{Th}
     \Delta_{Score_{t_w,t_l}} > \text{Th},
\end{equation}
where $\Delta_{Score_{t_w,t_l}}=\left| \text{Score}(t_w | s) - \text{Score}(t_l | s) \right|$.

After the filtering and pairing process, all the preference data $ \left(s, t_w, t_l \right)$  satisfying the conditions constitute the preference data set $\mathcal{D}_P$, which can be expressed as: 
\begin{equation}
    \mathcal{D}_P = \left\{ s^{(i)}, t_w^{(i)}, t_l^{(i)} \right\}_{i=1}^{N},
\end{equation}
% $\mathcal{P}$ 
% All such preference data points constitute the preference dataset 
% $\mathcal{D}_P = \left\{ s^{(i)}, t_w^{(i)}, t_l^{(i)} \right\}_{i=1}^{N}$, 
where $N$ is the number of all qualified preference pairs in $\mathcal{C}_n$, and $s_i$ is the state corresponding to the preference candidate tactic pair.  %%%%P需要写一个单独的等式

% Specifically, for a given proof state $s$, candidate tactics with a preference score difference exceeding a certain threshold $Th$ are paired, resulting in the final preference dataset $\mathcal{D}_P$ 
% , which can be expressed by the following equation:
% \begin{equation}
%     \mathcal{D}_P =  \left\{ (s_i, (t^+, t^-)) \mid i = 1, 2, \ldots, x, \, \exists t_{ij}, t_{ik} \in T_i, \, |\text{Score}_{ij} - \text{Score}_{ik}| > \text{Th} \right\} 
% \end{equation}

% \begin{equation}
% \begin{aligned}
%    \mathcal{D}_P = \left\{ &(s_i, (t^+, t^-)) \mid i = 1, 2, \ldots, x, \\&\exists t_{ij}, t_{ik} \in T_i, \Delta_{ij, ik} > \text{Th} \right\} 
% \end{aligned}
% \end{equation}

% \subsection{Perform Iterative DPO Fine-Tuning on the
% Prover}

% \subsection{Stage 3: DPO on Prover}
% \subsection{Stage 3: Iterative DPO on Prover}
\subsection{Stage 3: Curriculum-based DPO Iteration}
\label{Stage3}
\textbf{DPO Fine-tuning.} After obtaining the preference dataset $\mathcal{D}_P$, we fine-tune the prover model 
% $\mathcal{P}_\theta$ 
using DPO. 
For each triplet $ \left(s, t_w, t_l \right)$ in $\mathcal{D}_P$, it corresponds to the triplet $ \left(x, y_w, y_l \right)$ in the DPO preference data discussed in \cref{section_3.1}, 
where s corresponds to an input, $t_w$ denotes the preferred response, and $t_l$ denotes the dispreferred response. 
We then minimize the optimization objective of \cref{eq_loss}, where $\pi_{ref}$ corresponds to the prover model before DPO fine-tuning.   
% We minimize the following optimization objective:
% \begin{multline}
% \mathcal{L}_{DPO}(\mathcal{P}_\theta; \mathcal{P}_{\text{ref}}) =  - \mathbb{E}_{(s, t_w, t_l) \sim \mathcal{D}_P} \\\Big[ \log \sigma \Big( \beta \log \frac{\mathcal{P}_\theta(t_w | s)}{\mathcal{P}_{\text{ref}}(t_w | s)} 
% - \beta \log \frac{\mathcal{P}_\theta(t_l | s)}{\mathcal{P}_{\text{ref}}(t_l | s)} \Big) \Big],
% \end{multline}
% where $\mathcal{P}_{\text{ref}}$ denotes the prover before DPO fine-tuning, and $\beta$ is a hyperparameter that controls the degree of deviation between $\mathcal{P}_{\text{ref}}$ and $\mathcal{P}_{\theta}$.
% \begin{equation}
    % \mathcal{L}_{DPO}(\mathcal{P}_n; \mathcal{P}_{n-1}) = - \mathcal{E}_{(s, t_w, t_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\mathcal{P}_n(t_w | s)}{\mathcal{P}_{n-1}(t_w | s)} - \beta \log \frac{\mathcal{P}_n(t_l | s)}{\mathcal{P}_{n-1}(t_l | s)} \right) \right]
% \end{equation}




\textbf{Curriculum-based Iteration (CI).} After obtaining the new prover model, the process enters a new round of curriculum-based DPO iteration, as outlined in Algorithm 1. %\ref{}. 
Specifically, for the $n$-th iteration, the dataset $\mathcal{C}_n$, with \textit{difficulty} value n, is used as the curriculum data. Tactic sampling is performed using the prover model $\mathcal{P}_{n-1}$ from the previous iteration, and fine-grained scoring is applied to a subset of the data in $\mathcal{C}_n$ to obtain the scoring dataset $\mathcal{SC}_n^0$. This dataset is used to fine-tune the score generator $\mathcal{G}_{n-1}$ from the previous round through supervised learning, yielding the new score generator $\mathcal{G}_n$. Next, $\mathcal{G}_n$ is used to score the remaining data in $\mathcal{C}_n$, resulting in the scoring dataset $\mathcal{SC}_n$. Data filtering and pairing are then performed on $\mathcal{SC}_n$ to obtain the preference pair dataset, followed by a new round of DPO. In the first iteration, $\mathcal{P}_{\text{ref}}$ is the base prover $\mathcal{P}_0$ obtained after SFT. In the $n$-th iteration, $\mathcal{P}_{\text{ref}}$ is $\mathcal{P}_{n-1}$. %%%有点冗余


\begin{algorithm}[t]
    \caption{Curriculum-based DPO Iteration}
    \label{alg:dpo_iteration}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}

    \begin{algorithmic}[1]
        \Require {Curriculum dataset $\mathcal{C} = \{ \mathcal{C}_1, \mathcal{C}_2, \dots, \mathcal{C}_I \}$, the base prover model $\mathcal{P}_0$ obtained from SFT, Base score generator model $\mathcal{G}_0$}  
        \Ensure $\mathcal{P}_I$: The final prover model 
        
        \State \textbf{Initialize:}  $\mathcal{P}_\textbf{ref} \gets \mathcal{P}_0$, $\mathcal{P}_\theta \gets \mathcal{P}_\textbf{ref}$
        \For{$n = 1$ \textbf{to} $I$}
            \State Use $\mathcal{C}_n$ as curriculum data.
            \State $\mathcal{T}$ $\gets$ Sample candidate tactics from $\mathcal{C}_n$
            \State $\mathcal{SC}_n^0$ $\gets$ Apply fine-grained scoring to a subset of $\mathcal{C}_n$  to obtain scoring dataset
            \State $\mathcal{G}_n$ $\gets$ SFT on score generator $\mathcal{G}_{n-1}$ with $\mathcal{SC}_0^n$ to obtain new score generator
            \State $\mathcal{SC}_n$ $\gets$ Use $\mathcal{G}_n$ to score the remaining data in $\mathcal{C}_n$ and get scoring dataset
            \State Filter and pair data in $\mathcal{SC}$ to form preference pair dataset $\mathcal{D}_P$
            \State Perform DPO on $\mathcal{P}_{n-1}$:  $\mathcal{P}_{\text{ref}} \gets \mathcal{P}_{n-1}$, $\mathcal{P}_\theta \gets \mathcal{P}_{\text{ref}}$
            \State $\mathcal{P}_n$ $\gets$ Minimize the loss $\mathcal{L}_{DPO}$($\mathcal{P}_\theta$, $\mathcal{P}_{\text{ref}}$) to update $\mathcal{P}_\theta$
        \EndFor
    \State \textbf{return} $\mathcal{P}_n$
        % \ENSURE $\mathcal{P}_I$
    \end{algorithmic}
\end{algorithm} %%%% TODO 是否要加条件 若连续两轮没有性能提升 就提前结束循环



% \begin{algorithm}[t]
%     \caption{Curriculum-based DPO Iteration}
%     \label{alg:dpo_iteration}
%     \renewcommand{\algorithmicrequire}{\textbf{Input:}}
%     \renewcommand{\algorithmicensure}{\textbf{Output:}}

%     \begin{algorithmic}[1]
%         \REQUIRE {Curriculum dataset $\mathcal{C} = \{ \mathcal{C}_1, \mathcal{C}_2, \dots, \mathcal{C}_I \}$, the base prover model $\mathcal{P}_0$ obtained from SFT, Base score generator model $\mathcal{G}_0$}  
%         \ENSURE $\mathcal{P}_I$: The final prover model 
        
%         \STATE \textbf{Initialize:}  $\mathcal{P}_\textbf{ref} \gets \mathcal{P}_0$, $\mathcal{P}_\theta \gets \mathcal{P}_\textbf{ref}$
%         \FOR{$n = 1$ \textbf{to} $I$}
%             \STATE Use $\mathcal{C}_n$ as curriculum data.
%             \STATE $\mathcal{T}$ $\gets$ Sample candidate tactics from $\mathcal{C}_n$
%             \STATE $\mathcal{SC}_n^0$ $\gets$ Apply fine-grained scoring to a subset of $\mathcal{C}_n$  to obtain scoring dataset
%             \STATE $\mathcal{G}_n$ $\gets$ SFT on score generator $\mathcal{G}_{n-1}$ with $\mathcal{SC}_0^n$ to obtain new score generator
%             \STATE $\mathcal{SC}_n$ $\gets$ Use $\mathcal{G}_n$ to score the remaining data in $\mathcal{C}_n$ and get scoring dataset
%             \STATE Filter and pair data in $\mathcal{SC}$ to form preference pair dataset $\mathcal{D}_P$
%             \STATE Perform DPO on $\mathcal{P}_{n-1}$:  $\mathcal{P}_{\text{ref}} \gets \mathcal{P}_{n-1}$, $\mathcal{P}_\theta \gets \mathcal{P}_{\text{ref}}$
%             \STATE $\mathcal{P}_n$ $\gets$ Minimize the loss $\mathcal{L}_{DPO}$($\mathcal{P}_\theta$, $\mathcal{P}_{\text{ref}}$) to update $\mathcal{P}_\theta$
%         \ENDFOR
%     \STATE \textbf{return} $\mathcal{P}_n$
%         % \ENSURE $\mathcal{P}_I$
%     \end{algorithmic}
% \end{algorithm} %%%% TODO 是否要加条件 若连续两轮没有性能提升 就提前结束循环


% \begin{algorithm}[t]
%     \caption{Curriculum-based DPO Iteration}
%     \label{alg:dpo_iteration}
%     \renewcommand{\algorithmicrequire}{\textbf{Input:}}
%     \renewcommand{\algorithmicensure}{\textbf{Output:}}
    
%     \begin{algorithmic}[1]
%         \REQUIRE {Curriculum dataset $\mathcal{C} = \{ \mathcal{C}_1, \mathcal{C}_2, \dots, \mathcal{C}_I \}$,  the base prover model $\mathcal{P}_0$ obtained from SFT, Base score generator model $\mathcal{G}_0$}  %(This is Inputs)  %%input
%         \ENSURE $\mathcal{P}_n$:%XXX(This is Outputs)    %%output
%         % \textbf{Input:} 
%         \STATE \textbf{Initialize:}  $\mathcal{P}_\textbf{ref} \gets \mathcal{P}_0$,$\mathcal{P}_\theta \gets \mathcal{P}_\textbf{ref}$
%         % \WHILE{$A=B$}
%         %     \STATE xxxxx
%         % \ENDWHILE
%         \FOR{$n = 1$ \textbf{to} $I$}
        
%         % \State \textbf{Step 1:} Tactic Samples Generation. 
%         \State Use $\mathcal{C}_n$ as curriculum data.
%         % \State Sample candidate tactics set $\mathcal{T}$ using the prover model $\mathcal{P}_{n-1}$
%         \State  %\text{Candidate tactics set} 
%         $\mathcal{T} \gets \text{Sample candidate tactics from} \: \mathcal{C}_n$
%         \State %$\text{scoring dataset} 
%         \mathcal{SC}_0^n$ \gets \text {Apply fine-grained scoring to a subset of}\: \mathcal{C}_n$  \text{to obtain scoring dataset}
%         \State  $\mathcal{G}_n \gets \text{SFT on score generator} \: \mathcal{G}_{n-1}\: \text{with} \:\mathcal{SC}_0^n$  \text{ to obtain new score generator }
%         \State $\mathcal{SC} \gets \text{Use} \: \mathcal{G}_n \text{to score the remaining data in} \:\mathcal{C}_n$ \text{and get scoring dataset }

%         \State \text{Filter and pair data in} \:$\mathcal{SC}$ to form preference pair dataset \:$\mathcal{D}_P $

%         \State Perform DPO on $\mathcal{P}_{n-1}$. $\mathcal{P}_{\text{ref}} \gets \mathcal{P}_{n-1}$, $\mathcal{P}_\theta \gets \mathcal{P}_{\text{ref}}$ 
        
%         \State $\mathcal{P}_n \gets \text{Minimize the loss} \:\mathcal{L}_{DPO}\left(\mathcal{P}_\theta, \mathcal{P}_\text{ref} \right)$ to update $\mathcal{P}_{\theta}$
        
%         % \State \textbf{Step 2:} Fine-Grained Preference Scoring.
%             % \State \textbf{Step 1:} Use $\mathcal{C}_n$ as curriculum data.
%             % \State Sample candidate tactics set $T$ using the prover model $\mathcal{P}_{n-1}$.
%             % \State \textbf{Step 2:} Apply fine-grained scoring to a subset of $\mathcal{C}_n$ to obtain scoring dataset $\mathcal{SC}_0^n$.
%             % \State \textbf{Step 3:} Fine-tune score generator $\mathcal{G}_{n-1}$ with $\mathcal{SC}_0^n$ to obtain new score generator $\mathcal{G}_n$.
%             % \State \textbf{Step 4:} Use $\mathcal{G}_n$ to score the remaining data in $\mathcal{C}_n$ and get scoring dataset $\mathcal{SC}$.
%             % \State \textbf{Step 5:} Filter and pair data in $\mathcal{SC}$ to form preference pair dataset $\mathcal{D}_P$.
%             % \State \textbf{Step 6:} Perform DPO with $\mathcal{P}_{n-1}$ as $\mathcal{P}_{\text{ref}}$ and $\mathcal{P}_\theta$ as $\mathcal{P}_n$.
%             % \State Minimize the loss $\mathcal{L}_{DPO}$ to update $\mathcal{P}_\theta$
%             % \IF {}
%             %     \STATE 
%             % \ELSE
%             %     \STATE 
%             % \ENDIF
%         \ENDFOR
        
%        \State \textbf{return} $\mathcal{P}_n$
        
%     \end{algorithmic}
% \end{algorithm}



%%%%伪代码 明确输入输出


% \subsection{Building Curriculum Dataset with Memory Enhancement}
% \subsection{Base Prover} %% 内容少就和上节合并为 一节 数据和base prover准备
% % \subsection{Dataset and Base Prover Preparation}
% % \textbf{Building a Memory-Enhanced Curriculum Dataset}
% \subsection{Supervised Fine-Tuning for a Verifier}
% % \subsection{Fine-Grained DPO (FG-DPO) on Prover } %%%%(FG-DPO) 细粒度偏好的DPO   （对Prover 进行DPO）
% % \subsection{Fine-Grained Direct Preference Optimization on Prover}
% \subsection{FG-DPO on Prover}
% \textbf{Fine-Grained Preference Scoring} %%%细粒度偏好评分
% % \subsection{Curriculum Learning-Based Iteration}
% % \subsection{Curriculum Learning-Based Prover-Verifier Iterative Training}
% \subsection{Curriculum Learning-Based Prover-Verifier Iteration} %%(基于课程学习的P-V迭代 
% % \subsection{Fine-Grained Preference Optimization} %%%细粒度偏好优化
% %%%% 形成偏好数据
% % \subsection{Curriculum Learning-Based Prover-Verifier Iteration} %%%%基于课程学习的P-V迭代
% %%%% Prover-Verifier Iteration Based on Curriculum Learning

\medskip
\section{Experiments}
% To validate the effectiveness of the proposed CuDIP method, we conducted extensive experiments on the Minif2f \cite{zheng2021minif2f} and ProofNet \cite{proofnet} datasets within the Lean4 environment. Additionally, we performed ablation studies to further assess the contributions of different components of the proposed method.

\subsection{Experimental Setup}
\textbf{Datasets and Metric.} We use the Lean 4 formalized theorem-proving dataset Mathlib4\footnote{https://github.com/leanprover-community/mathlib4} %, which %% 训练集介绍
as the training set $\mathcal{D}$. %, with Minif2f \cite{zheng2021minif2f} and ProofNet \cite{proofnet} as the test sets. 
To evaluate the performance of the proposed method, we used datasets MiniF2F \cite{zheng2021minif2f} and ProofNet \cite{proofnet} as evaluation benchmarks.%test sets.
MiniF2F consists of 488 problems derived from high school mathematics competitions, with 244 problems in both the test and validation sets. These problems primarily include those sourced from the MATH \cite{hendrycks2021MATH} dataset, high school mathematics competitions such as AMC, AIME, and IMO, as well as a set of carefully crafted problems designed to match the difficulty level of those in the competitions. 
ProofNet includes 371 examples, each comprising a formal theorem statement, a natural language theorem statement, and a natural language proof in Lean 3. %%%%%%%
We manually translated the corresponding Lean 3 proofs in ProofNet into Lean 4, resulting in 360 examples that are suitable for interaction with Lean 4. 
% We selected 360 examples from ProofNet and manually translated the corresponding Lean 3 proofs into Lean 4. %%%得到可用于与Lean4交互的360个
% % \textbf{MiniF2F} 
% \begin{itemize}
%     \item \textbf{MiniF2F.} 
%     MiniF2F consists of 488 problems derived from high school mathematics competitions, with 244 problems in both the test and validation sets. These problems primarily include those sourced from the MATH \cite{hendrycks2021MATH} dataset, high school mathematics competitions such as AMC, AIME, and IMO, as well as a set of carefully crafted problems designed to match the difficulty level of those in the competitions.
% %MiniF2F is composed of problems from three different source categories. It includes 260 problems sampled from the MATH \cite{hendrycks2021MATH} dataset, 160 problems derived from real-world high school mathematics competitions AMC, AIME, and IMO, and 68 carefully crafted problems designed to match the difficulty level of those in the competitions. 
%     \item \textbf{ProofNet.}  
%     ProofNet includes 371 examples, each comprising a formal theorem statement, a natural language theorem statement, and a natural language proof in Lean 3. 
%     We selected 360 examples from ProofNet and manually translated the corresponding Lean 3 proofs into Lean 4.
%     % The problems primarily originate from popular undergraduate pure mathematics textbooks, covering topics such as real and complex analysis, linear algebra, abstract algebra, and topology.
% \end{itemize}
We use pass@k as the evaluation metric, which represents the probability that the model successfully finds at least one valid proof within k proof attempts. %%% 是否要这样介绍pass@k? or pass@1?


% Minif2f consists of 488 problems from high school math competitions, with 244 problems each in the test and validation sets, drawn from three categories: (1) 260 problems sampled from the MATH dataset; (2) 160 problems from actual high school math competitions (AMC, AIME, IMO); and (3) 68 procedural problems of similar difficulty to those in (2).
% \textbf{ProofNet} 

% \begin{table*}[htbp]%[htbp]
% % \centering
% \caption{
% Proving success rates (Pass@1) on the MiniF2F and ProofNet datasets. The superscript \textbf{*} denotes the model as a prover, and \textbf{\dag} indicates it as a score generator.
% % The highest success rates are \textbf{bolded}. %%% ?是否
% % Proving success rates (Pass@32) on the miniF2F dataset with Lean. The superscript \textbf{*} in the upper right corner indicates that the model is used as a prover, while the \textbf{\dag} superscript indicates that the model is used as a verifier.
% }
% \label{tab:main_results}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \renewcommand{\arraystretch}{1.2} % 调整行高  %%%%% 注意
% \begin{tabular}{l c c c r}
% \toprule
% \textbf{Method} &  \textbf{miniF2F-valid} & \textbf{miniF2F-test} & \textbf{ProofNet}\\ 
% \midrule
%         \textit{\textbf{Baselines}} \\ \hline
%         Mathstral &23.3\%  &27.0\%  &7.\%\\ %  
%         Llama-3.1&20.1\%  &22.5\% &1.7\%\\ %  
%         Phi-3.5 &3.7\% &3.7\% & 0\% \\ %  
%         Mathstral + SFT &34.4\% &34.4\% &11.1\%\\ %  
%         Llama-3.1 + SFT &28.6\% &32.3\% &9.4\%\\ %  
%         Phi-3.5 + SFT  &29.1\%  &32.7\%  & 7.7\%\\ %  
%         \hline
%         \textit{\textbf{Ours}} \\ \hline
%         Mathstral\textsuperscript{\textbf{*}} + Llama-3.1\textsuperscript{\textbf{\dag}} + CuDIP & 38.5\%(+4.1\%) & 38.5\%(+4.1\%) & 12.7\%(+1.6\%) \\
%         Llama-3.1\textsuperscript{\textbf{*}} + gemma2\textsuperscript{\textbf{\dag}} + CuDIP  &36.0\%(+7.4\%) &36.8\%(+4.5\%) &11.6\%(+2.2\%) \\
%          % \hline 
%         Phi-3.5\textsuperscript{\textbf{*}} + gemma2\textsuperscript{\textbf{\dag}} + CuDIP 
%          &33.2\%(+4.1\%) &36.4\%+(3.7\%) &9.7\%(+2.0\%) \\
%         \bottomrule
%     \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}



 
\textbf{Baselines} We selected different models as the prover model and score generator model to evaluate the effectiveness of the proposed method. Specifically, we chose Mathstral-7B-v0.1\cite{jiang2023mistral7b}, 
Llama3.1-8B-Instruct\cite{dubey2024llama}, 
and Phi-3.5-mini-instruct \cite{abdin2024phi3technicalreporthighly}
as the prover models, and Llama3.1-8B-Instruct, 
gemma-2-9b-it\cite{Riviere2024Gemma2I},  %%% 作者太多换成了and others, % \cite{gemmateam2024gemma2improvingopen} 
and gemma-2-2b-it 
% \cite{Riviere2024Gemma2I}  %%% 作者太多换成了and others
% \cite{gemmateam2024gemma2improvingopen} 
as the score generator models for the experiments. We used the entire Mathlib4 dataset for SFT and directly tested the base model, with these two approaches serving as baselines for comparison. Both baseline methods and the proposed method were tested in the Lean 4 %\footnote{v} 
environment to assess their performance.
% 有点冗余


% \textbf{Implementation details} 
% % During the training phase,
% The hyperparameter $\beta$ in the loss function of DPO is set to 0.1 and temperature of the prove model $\mathcal{P}$ is set to 1.0, with 32 attempts across all procedures.
% In the Stage 2 process, for each curriculum learning data $C_n$, we randomly selected 2K data from it to interact with the Lean4. We set the number of search attempts at $n_\text{attempt}$ = 10. When using score generator $\mathcal{G}_{n}$ to score the remaining data in $C_n$, we set the temperature parameter to 0.1 and sampled 10 times. The average of the 10 sampling results is used as the score of the tactic. During the evaluation, we used the BFS algorithm and limited the search time for each theorem to 300 seconds. The prompt templates for prove model and score generator are shown in the appendix.

\textbf{Implementation details} 
The prover’s sampling parameters remain consistent throughout the entire process. We set its temperature to 1.0 and the number of candidate candidate tactics $k$ to 32 for each proof state. In the Preference Data Generation stage, for each curriculum dataset \(C_n\), we randomly select 2000 data points to interact with the Lean environment. After applying the candidate tactic to the current proof state, we obtain a new proof state. Starting from this updated state, we perform 10 search attempts (\(n_{\text{attempt}} = 10\)) and use the frequency of successful proofs as the score of the tactic. For the remaining unscored data, we use the trained score generator \( \mathcal{G}_n \) to evaluate the candidate tactic, setting the temperature to 0.1. For each candidate tactic in each proof state, we perform 10 sampling iterations and compute the average score of these samples as the tactic’s final score. During the DPO fine-tuning process, the hyperparameter \( \beta \) in the loss function is set to 0.1. In the evaluation phase, we limit the search time for each theorem to 300 seconds and employ standard best-first search algorithm to find proofs\cite{NEURIPS2023_44414694, DBLP:journals/corr/abs-2102-06203}. For further details, please refer to Appendix \ref{training details}.





% In the Preference Data Generation stage, for each curriculum data $C_n$, we randomly select 2000 data to interact with the Lean environment. After applying the candidate tactic to the current proof state, we get a new proof state. Starting from this new state, we perform $n_{attempt}$=10 searches and use the frequency of successful proofs as the score of the tactic. For the remaining unscored dataset, we let the trained score generator $\mathcal{G}_n$ score the candidate tactic with a temperature of 0.1. For each candidate tactic in each proof state, we sample 10 times and take the average score of these 10 samples as the score of the tactic. The prover parameters are kept consistent throughout the process. We set its temperature to 1.0 and sample 32 for each input. In the evaluation phase, we limit the search time for each theorem to 300 seconds and use the BFS search algorithm. For more details, see Appendix \ref{training details}.

% During the preference data construction process, for each set of course learning data $C_n$, we randomly select 2k data points to undergo a manual fine-grained rating procedure, where the number of proof attempts $n_{attempt}$ is set to 10. 
% After obtaining the score generator $\mathcal{G}_n$, we set its temperature to 0.1. The score generator $\mathcal{G}_n$ is then used to rate each of the remaining data points in $C_n$ 10 times, with the final preference score being the average of these 10 ratings. During the DPO fine-tuning process, the hyperparameter $\beta$ in the loss function is set to 0.1, the temperature of the prover model is set to 1, and the number of proof attempts, k, is set to 32. In the testing phase, the prover model provides the proof tactic, and a BFS algorithm is used for proof search, with a time limit of 300 seconds for the search. Detailed prompt templates for the proof model and the score generator model are provided in Appendix \ref{prompt_templates}.

% During the experiment, we used LlamaFactory to train LLM, which is a convenient and easy-to-use LLM training tool that can help us focus more on dataset construction. In the Preparation process of Stage 1, we used LeanDojo Benchmark 4 v10, which is a training corpus extracted from the Lean4.10 version of Mathlib4. We processed the corpus and constructed about 250K training data. The data format of each pair is as introduced in 4.2, which is a pair consisting of proofstate and tactic. For the specific input prompt of the model, see Appendix B. When performing SFT training, we set the number of training epochs to 3 and the learning rate to $2.0 \times 10^{-5}$. 

% \textbf{Score Training} In the Stage2 process, for each divided dataset $C_i$, we randomly selected 2K data from it to interact with the actual Lean4 environment. For each proof state in the data, we let the prover P generate 32 candidate tactics, try to apply each tactic to the current state, and use the subsequent proof state as the root node of the Monte Carlo tree to search 10 times. Then the frequency of successful search is used as the score of each corresponding tactic. 
% During the Monte Carlo tree search, we set the number of simulations to 1000. After we obtain the specific scores for each strategy, we use these data to train the scorer, where we use a supervised fine-tuning approach.

% \textbf{Preference Data Generation} For the data that are not selected in Ci, we will also use the prover P to generate 32 candidate strategies. The difference is that we will use the trained scorer to score these strategies instead of searching in the actual interactive environment to score them. When scoring, in order to reduce the randomness of the scoring, we let the scorer score each strategy 10 times, and then use the average of the scores as the actual score of the strategy. When scoring, we set the temperature parameter of the model inference to 0.1 to ensure the stability of the model output. When we have all the strategies and the corresponding scores, we set a pair of strategies whose score difference exceeds a fixed value as positive and negative examples in the training data. In order to ensure the stability of dpo training, the score difference threshold is set to 0.5 here, which can ensure that when constructing the dpo data set, for the same input, there is no answer that is used as both a positive example and a negative example. When performing DPO training, we set the penalty coefficient of DPO to 0.1, the learning rate to $5.0 \times 10^{-6}$ , and trained for 1 epoch.

\subsection{Main Results}
To validate the effectiveness of the proposed method, we apply the CuDIP method to three different model groups 
% (serving as provers and score generators, respectively) 
and 
compare the results with existing baseline methods 
% (where the model is used solely as a prover) 
as well as the corresponding models after fine-tuning with SFT. 
% compare the results with %existing 
% baseline methods. 
The proof pass rates on the MiniF2F and ProofNet are presented in 
\cref{tab:main_results}, 
% Table \ref{tab:main_results}, 
where our method corresponds to the results of the fourth iteration. 
As shown in \cref{tab:main_results}, the use of the CuDIP method leads to significant improvements over the baseline results. For instance, when using Mathstral as the prover and Llama-3.1 as the score generator with CuDIP, the pass@1 on MiniF2F reaches 38.5\%, representing a 4.1\% improvement over the baseline method after SFT. 
Additionally, when using Llama-3.1 as the prover and Gemma2 as the score generator, the highest performance improvements were observed on both benchmarks, with a 7.4\% increase on MiniF2F and a 2.2\% increase on ProofNet. 
% Additionally, on the ProofNet benchmark, the proof success rate increases from 11.1\% after SFT to 12.7\%. %%%%%
In the experiments with the remaining group of models, the proof pass rate also showed notable improvements. 
% The accuracy of the Llama3.1 model on the MiniF2F valid set increased from 28.6\% to 36.0\%. Similarly, the Phi3.5-mini model, which has fewer parameters, also demonstrated a 3.7\% improvement on the MiniF2F-test benchmark. 
These results demonstrate the effectiveness of the proposed CuDIP method.

\begin{table*}[htbp]%[htbp]
% \centering
\caption{
Proving success rates (Pass@1) on the MiniF2F and ProofNet datasets. The superscript \textbf{*} denotes the model as a prover, and \textbf{\dag} indicates it as a score generator.
The highest success rates are \textbf{bolded}. %%% ?是否
% Proving success rates (Pass@32) on the miniF2F dataset with Lean. The superscript \textbf{*} in the upper right corner indicates that the model is used as a prover, while the \textbf{\dag} superscript indicates that the model is used as a verifier.
}
\label{tab:main_results}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\renewcommand{\arraystretch}{1.2} % 调整行高  %%%%% 注意
\begin{tabular}{l c c c r}
\toprule
\textbf{Method} &  \textbf{MiniF2F-valid} & \textbf{MiniF2F-test} & \textbf{ProofNet}\\ 
\midrule
        % \textit{\textbf{Baselines}} \\ \hline
        Mathstral &23.3\%  &27.0\%  &7.2\%\\ %  
        Mathstral + SFT &34.4\% &34.4\% &11.1\%\\ %  
        % \textbf{Mathstral\textsuperscript{\textbf{*}} + Llama-3.1\textsuperscript{\textbf{\dag}} + CuDIP(ours)} & \textbf{38.5\%(+4.1\%)} & 
        % \textbf{38.5\%(+4.1\%)} & \textbf{12.7\%(+1.6\%)} \\
        \textbf{Mathstral\textsuperscript{\textbf{*}} + Llama-3.1\textsuperscript{\textbf{\dag}} + CuDIP(ours)} & \textbf{38.5\%} & 
        \textbf{38.5\%} & \textbf{12.7\%} \\ \hline
        
        Llama-3.1&20.1\%  &22.5\% &1.7\%\\ %  
        Llama-3.1 + SFT &28.6\% &32.3\% &9.4\%\\ %  
        % \textbf{Llama-3.1\textsuperscript{\textbf{*}} + gemma2\textsuperscript{\textbf{\dag}} + CuDIP(ours)}  & \textbf{36.0\%(+7.4\%)} & \textbf{36.8\%(+4.5\%)} & \textbf{11.6\%(+2.2\%)} \\ \hline
        \textbf{Llama-3.1\textsuperscript{\textbf{*}} + gemma2\textsuperscript{\textbf{\dag}} + CuDIP(ours)}  & \textbf{36.0\%} & \textbf{36.8\%} & \textbf{11.6\%} \\ \hline
        
        Phi-3.5 &3.7\% &3.7\% & 0\% \\ %  
        Phi-3.5 + SFT  &29.1\%  &32.7\%  & 7.7\%\\ %  
        % \textbf{Phi-3.5\textsuperscript{\textbf{*}} + gemma2\textsuperscript{\textbf{\dag}} + CuDIP(ours)} 
        %  & \textbf{33.2\%(+4.1\%)} & \textbf{36.4\%(+3.7\%)} & \textbf{9.7\%(+2.0\%)} \\
        \textbf{Phi-3.5\textsuperscript{\textbf{*}} + gemma2\textsuperscript{\textbf{\dag}} + CuDIP(ours)} 
         & \textbf{33.2\%} & \textbf{36.4\%} & \textbf{9.7\%} \\
        % \textit{\textbf{Ours}} \\ \hline
        \bottomrule
    \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


In order to validate the effect of different iteration counts using the CuDIP method, we performed four rounds of DPO iteration on three sets of models. The testing results of the models after each iteration 
% on the MiniF2F-valid, MiniF2F-test, and ProofNet benchmarks 
are shown in Figure \ref{iteration}. As illustrated in Figure \ref{iteration}, during the first four iterations, the model's proof capabilities increase with the number of iterations, thereby confirming the effectiveness of the proposed iterative method. 
The specific test results of these three groups of models after four iterations can be found in Appendix \ref{appendixA}.

\begin{figure*}[tbp]%[htbp]%[htbp]
\centering
\includegraphics[width=0.9\linewidth]{accuracy_all_plots_25.1.31_15_6.pdf}%{figures/accuracy_all_plots_15_6.pdf}%{figures/accuracy_all_plots.pdf}
\caption{Comparison curves of different iteration rounds 
using the CuDIP method. 
The performance curves depicting the proof success rates of the three model groups after four rounds of DPO iteration based on curriculum learning, evaluated on the MiniF2F-valid, MiniF2F-test, and ProofNet benchmarks.
}
\label{iteration}
\end{figure*}




% \textbf{Results on MiniF2f.}
% whether or not ?
% \textbf{Results on ProofNet.}

% We conducted four rounds of iterative training on these three groups of models. The test results of each round of these three groups of models are shown in Appendix A. The experimental results show that the pass rates of the models on both the Minif2f and Proofnet datasets have improved, and the improvement on the Minif2f dataset is more obvious. When we use Mathstral as the proof model and Llama3.1 as the score generator, the proof pass rate of MiniF2F Benckmark increases the most, from 34.4\% to 37.7\%, an increase of 3.3\%. At the same time, it also increases from 11.1\% to 12.4\% on the ProofNet dataset, an increase of 1.3\%. In the experiments of the other two groups of models, the model's test accuracy has also made progress. The accuracy of the Llama3.1 model on the validation set of MiniF2F has increased from 28.6\% to 31.9\%, and it has achieved a 2.9\% improvement on the test set of MiniF2F. For the Phi3.5-mini model with a smaller number of parameters, it has also achieved a 2\% improvement on the test set.



% In addition, we also tested the three final versions of the model for Pass@k. The experimental results are shown in Table ? In the Pass@4 test of the Mathstral model, the accuracy on the validation set and test set of Minif2f reached 39.7\% and 40.1\% respectively. For the remaining two groups of models, their accuracy on the Minif2f and proofnet datasets has improved.


% \textbf{Fine-Grained Direct Preference Optimization (FG-DPO).}%Effect of Fine-Grained Direct Preference Optimization (FG-DPO).



\subsection{Ablation Studies}
To separately investigate the effects of different components of the proposed CuDIP, we conducted ablation experiments, testing: 
1) the effectiveness of the fine-grained preference scoring based on the score generator, 
% during the preference data construction process,
and 2) the effectiveness of DPO iteration based on curriculum learning.

% \textbf{The Effectiveness of Score Generator.} 
% \textbf{Score Generator.} % Ablation on Score Generator
\textbf{Fine-Grained Preference Scoring (FGPS).}
In order to verify the effectiveness of fine-grained preference scoring during the construction of preference data, we used Mathstral as the prover model and tested two conditions: one where Llama3.1 was used as the score generator in the preference data construction process, and another where no score generator or scoring rules were applied for fine-grained scoring. The test results on the MiniF2F and ProofNet benchmarks are presented in the 
\cref{tab:ablation_result_FGPS}
% Table \ref{tab:ablation_result_FGPS}. %%%%%%
The use of the score generator for fine-grained scoring significantly improved the model's proof success rate, thereby demonstrating the effectiveness of the proposed fine-grained preference scoring process.
% In order to verify the effectiveness of fine-grained scoring, we do not use a score generator to score the proof tactics generated by the model. When constructing the DPO dataset, we use the tactics in the original dataset $C_n$ as positive examples and the tactics generated by the model as negative examples. The results show that when the DPO dataset is constructed without fine-grained scoring, the accuracy of the prove model is almost not improved. Compared with the fine-grained scoring method, only using the labels in the supervised fine-tuning as positive examples in the DPO dataset cannot make the model produce more high-quality outputs, which further verifies the importance of using the score generator for fine-grained scoring.


\begin{table*}[t]%[htbp]
\centering
\caption{
% Ablation results.
Improvement in pass rates on MiniF2F and ProofNet at Pass@1 with %after 
% scoring with a score generator 
Fine-grained Preference Scoring (FGPS). 
% during preference data generation. 
% "SG" in the table refers to the score generator.
}
\label{tab:ablation_result_FGPS}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l c c c r}
        \toprule
        \textbf{Model} &  \textbf{FGPS}&
        % \textbf{Verifier Rating} & 
        \textbf{MiniF2F-valid} & \textbf{MiniF2F-test} & \textbf{ProofNet}\\ 
    \hline
    Mathstral& \ding{55} &35.2\% &35.6\% &11.6\% \\ 
    Mathstral& \ding{51} &36.0\% &36.0\% &11.6\% \\
    % & \ding{51} & \ding{51} & \ding{55}  & &\\
    \bottomrule
    \end{tabular}
\vskip -0.1in
\end{sc}
\end{small}
\end{center}
\end{table*}

\textbf{Curriculum-based Iteration (CI).} 
% \textbf{Iterative Curriculum Learning (ICL).} % % Ablation on Iterative Curriculum Learning 
% To verify the effectiveness of curriculum learning, we merged the datasets from the first four rounds and conducted a round of training. Similarly, we used the score generator to perform fine-grained scoring on each tactic and then constructed dpo training data. We tested the trained model on Minif2f, and the accuracy of the model increased from ? to ?, an increase of ?. Compared with the curriculum learning method, the accuracy improvement was lower, which further verified the effectiveness of curriculum learning.
To validate the effect of Curriculum-based Iteration (CI), we combined the curriculum data from the first four iterations and conducted one round of training. The results were then compared with those obtained after four iterations of training. As shown in 
\cref{tab:ablation_result_CL}
% Table \ref{tab:ablation_result_CL}
, the test results on MiniF2F after training with CI were 4.1\%, showing an improvement of 2.1\% compared to the results without CI, which further verified the effectiveness of curriculum-based iteration. 




\begin{table*}[t]%[htbp]
\centering
\caption{Improvement in pass rates for MiniF2F and ProofNet at Pass@1 with %after 
Curriculum-based Iteration (CI).
% Iterative Curriculum Learning (ICL).
}
\label{tab:ablation_result_CL}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l c c c r}
        \toprule
        \textbf{Model} &  \textbf{CI}%\textbf{ICL} 
        & 
        \textbf{MiniF2F-valid} & \textbf{MiniF2F-test} & \textbf{ProofNet}\\ 
    \hline
    Mathstral&  \ding{55} &35.6\% &36.4\% &12.2\%\\ 
   Mathstral(4 iterations) & \ding{51} & 38.5\% & 38.5\% & 12.7\% \\
    % & \ding{51} & \ding{51} & \ding{55}  & &\\
    \bottomrule
    \end{tabular}
\vskip -0.1in
\end{sc}
\end{small}
\end{center}
\end{table*}
% \medskip
% % \begin{figure*}[t]%[htbp]%[htbp]
% \includegraphics[width=\linewidth]{figures/MiniF2F-valid.pdf}
% \includegraphics[width=\linewidth]{figures/MiniF2F-test.pdf}
% \includegraphics[width=\linewidth]{figures/ProofNet.pdf}


\section{Conclusion}
In this paper, we present a 
% curve-based iterative theorem proving method for Direct Preference Optimization (CuDIP).
curriculum learning-based DPO iterative theorem proving method (CuDIP).
Specifically, we first introduce an effective preference data construction approach based on 
fine-grained preference scoring by 
large language models (LLMs), 
which reduces the reliance on human annotations and enhances the diversity of the preference data. 
Subsequently, we integrate the proposed preference data construction method with curriculum learning to conduct iterative DPO training for the theorem proving model. 
Experimental results demonstrate that the method proposed herein significantly improves the performance of LLMs in theorem proving tasks and enhances their reasoning capabilities in the context of theorem proving.



% accept再加
% \section*{Acknowledgements}
% This work was supported by the GPU computing platform of the Academy of Mathematics and Systems Science, Chinese Academy of Sciences.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}
\bibliographystyle{plain}
\bibliography{reference}
% \bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

\section{More Experimental Results}
\label{appendixA}
In this paper, we used three groups of models to verify our proposed method. For each group of models, we performed 4 rounds of iterations. After each round of iteration, we tested the model. Table 4 shows the test results of the model on the MiniF2F and ProofNet datasets. The results show that the accuracy of the model gradually increases during the iteration process.
%%% 
%%%%不同课程学习迭代次数下各模型在minif2f的lean语言上的证明通过率
\begin{table*}[htbp]
% \centering
\caption{Proof success rates for Curriculum Learning-based DPO Iterative Theorem Proving (CuDIP) on MiniF2F and ProofNet with Lean at different iterations. Results of our CuDIP method after 4 iterations are shown.}%%%%%%
\label{tab:ablation_result_1}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l c c c r}
        \toprule
        \textbf{Model} &  \textbf{Iterations}& 
        \textbf{MiniF2F-valid} & \textbf{MiniF2F-test} 
        & \textbf{ProofNet}\\  
    \hline
    \multirow{4}{*}
        {Mathstral\textsuperscript{\textbf{*}} + Llama-3.1\textsuperscript{\textbf{\dag}} + CuDIP
        % (multi-step prompts)
        } 
        & 1   & 36.0\% & 36.0\% & 11.6\%  \\ 
        & 2   & 36.4\% & 36.8\% & 12.5\%  \\ 
        & 3   & 37.7\% & 37.7\% & 12.7\%  \\ 
        & 4   & 38.5\% & 38.5\% & 12.7\%  \\
        % & 5 \\
        % & 6 \\
        % & 7 \\
        % & 8 \\
        % & 9 \\
        % & 10 \\
        \hline 
        \multirow{4}{*}{Llama-3.1\textsuperscript{\textbf{*}} + gemma2\textsuperscript{\textbf{\dag}} + CuDIP   
        % (multi-step prompts)
        } 
        & 1 &31.9\% &34.0\% &10.8\% \\ 
        & 2 &34.0\% &35.2\% &11.1\% \\ 
        & 3 &35.6\% &36.4\% &11.6\% \\ 
        & 4 &36.0\% &36.8\% &11.6\% \\
        % & 5 \\
        % & 6 \\
        % & 7 \\
        % & 8 \\
        % & 9 \\
        % & 10 \\
        \hline 
        \multirow{4}{*}{Phi-3.5\textsuperscript{\textbf{*}} + gemma2\textsuperscript{\textbf{\dag}} + CuDIP
        % (multi-step prompts)
        } 
        & 1 &30.3\% &33.1\% &8.6\% \\ 
        & 2 &31.5\% &34.8\% &9.4\% \\ 
        & 3 &32.7\% &36.4\% &9.7\% \\ 
        & 4 &33.2\% &36.4\% &9.7\% \\
        % & 5 \\
        % & 6 \\
        % & 7 \\
        % & 8 \\
        % & 9 \\
        % & 10 \\
    \bottomrule
    \end{tabular}
\vskip -0.1in
\end{sc}
\end{small}
\end{center}
\end{table*}

\section{Training Details}
\label{training details}
\subsection{Training Tools}
In this paper, we used supervised fine-tuning (SFT) and direct preference optimization (DPO) to train the model. In terms of training tools, we used LlamaFactory\cite{zheng-etal-2024-llamafactory}, a unified framework that integrates a suite of cutting-edge efficient training methods. It can help us focus more on the construction of the dataset without worrying about the implementation of the model training code.
\subsection{Base Prover Training}
We used LlamaFactory to train the three basic models with SFT. The three models used the same prompt template, see Appendix \ref{prompt_templates} %%%%
for details. During training, we used the LoRA\cite{hu2021lora} method, which can help us save training resources. We set the learning rate to $2.0 \times 10^{-5}$, the learning rate scheduler to cosine, the warmup ratio to 0.03, and trained for 3 epochs. We set the floating point precision to bfloat16 and batch size to 4.
\subsection{Score Generator Training}
When generating training data for the score generator, we performed a Monte Carlo Tree Search (MCTS) on each proof state in the subset of the curriculum learning data $C_n$. For each search, we set the number of simulations to 1000 and limited the search depth to 10. We did not use a fixed value for the search time limit, because as the number of iterations increases, the model's ability gradually increases and the model's search time increases. We observed that when the model performs automatic theorem proving, it usually succeeds within 60 seconds, and the probability of the model proving the theorem after that is very low. This phenomenon is caused by the model's insufficient long-chain reasoning ability, which is also one of the challenges faced by large language models (LLMs) at this stage. In order to balance the quality and quantity of training data when computing resources are limited, we limited the search time limit to 60 seconds in the first round of iterations. For each subsequent round, we increased the search time limit by 30 seconds each time, hoping that the model can perform a deeper search.
\subsection{Filtering and Paring}
After we score each tactic in the candidate tactic set for a proof state, we need to consider how to construct a preferred data set for DPO training. For those tactics with non-zero scores, it means that after the current proof state applies the tactic, the model has the probability of completing the subsequent proof. We use a pair of tactics with a score difference of more than 0.5 as positive and counterexample in a dpo training data set. The advantage of this method is that for any proof tactic, when constructing the DPO training data set, it will not be used as both a positive example and a counterexample, which can be beneficial to the training of the model. When conducting dpo training, we set $\beta$ in the DPO loss function to 0.1, set the learning rate to $5.0 \times 10^{-6}$, and trained for 1 epoch.


\section{Prompt Templates}
\label{prompt_templates}

\subsection{Prove Model Prompt Template}
\begin{tcolorbox}[colframe=brown!40!black, colback=brown!5,title=\textbf{Prompt Template}]
\texttt{<s>You are using Lean4 for theorem proving[INST] You will be provided with a json-formatted data describing the intermediate proof status of the Lean4 theorem. \\ 
You need to give a prediction of a proof tactic based on the data description and your knowledge and experience about Lean4 proofs.\\
The input json format description is as follows: \\ 
\{\\ 
\hspace*{2em}"current\_theorem\_state": <current theorem state description> \\ 
\}\\
Your output must also be json-formatted data in the following format: \\
\{ \\
\hspace*{2em}"predict\_tactic": <A proof tactic that complies with Lean4 syntax. The type is a string and the content cannot contain "sorry"> \\
\} \\
Your input: \\
\{ \\
\hspace*{2em}"current\_theorem\_state": "\$current\_theorem\_state" \\
\} \\
Your output:\\
}
\end{tcolorbox}
We apply this template to an actual proof state and replace the placeholders in the template about the theorem proof state with specific values. Considering that the proof state of a theorem may contain multiple proof goals, we set the upper limit of the number of model input tokens to 4096 and the upper limit of the number of output tokens to 2048, which can meet the needs of the current task. It is worth mentioning that in order to better help us extract the desired tactics from the model output, we limit the input and output of the model to json format. \\
The following is a actual examples of prompts and the responses of the model:


\begin{tcolorbox}[colframe=brown!40!black, colback=brown!5, breakable, title=\textbf{Input Example}]
\texttt{<s>You are using Lean4 for theorem proving[INST] You will be provided with a json-formatted data describing the intermediate proof status of the Lean4 theorem. \\ 
You need to give a prediction of a proof tactic based on the data description and your knowledge and experience about Lean4 proofs.\\
The input json format description is as follows: \\ 
\{\\ 
\hspace*{2em}"current\_theorem\_state": <current theorem state description> \\ 
\}\\
Your output must also be json-formatted data in the following format: \\
\{ \\
\hspace*{2em}"predict\_tactic": <A proof tactic that complies with Lean4 syntax. The type is a string and the content cannot contain "sorry"> \\
\} \\
Your input: \\
\{ \\
\hspace*{2em}"current\_theorem\_state": "a b c : $\mathbb{N}$\texttt{\textbackslash n} $h_1$ : b = 0\texttt{\textbackslash n} $\mathbb{\vdash}$ a + b + c = c + a" \\
\} \\
Your output:\\
}
\end{tcolorbox}

For the above input, LLM's response is as follows
\begin{tcolorbox}[colframe=brown!40!black, colback=brown!5, title=\textbf{Output Example}]
\texttt{\{ \\
\hspace*{2em}"predict\_tactic": "norm\_num [$h_1$]" \\
\} \\
}
\end{tcolorbox}



\subsection{Score Generator Prompt Template}
% Single-step prompt
\begin{tcolorbox}[colframe=brown!40!black, colback=brown!5, breakable,title=\textbf{Prompt Template}]
\texttt{
\#\#\# Task Description: \\
You will be given a `tactic` generated by a model in the context of a Lean4 theorem proving process. Please score the `tactic` based on the following criteria, where: \\
- 0 means the `tactic` cannot run in Lean4 syntax or does not affect the current proof state.\\
- 1 means the `tactic` fully complies with Lean4 syntax and successfully completes the proof.\\
- A value between 0 and 1 indicates the probability of the `tactic` completing the proof or significantly advancing the proof process.\\
Do not provide any additional text, explanations, or responses. Only output the score which is a number between 0 and 1.\\
\\
\#\#\# Output Format: \\
- A score (floating-point value): A number between 0 and 1.\\
\\
\#\#\#\# Input: \\
\{\\
\hspace*{2em}"current\_theorem\_state": "\$current\_theorem\_state"\\
\}\\
\\
\#\#\#\# Generated `tactic`: \\
\{\\
\hspace*{2em}"predict\_tactic": "\$predict\_tactic"\\
\}\\
\\
\texttt{---}\\
\\
\#\#\# Score:\\
}
\end{tcolorbox}

\section{Case Studies}
\subsection{mathd\_algebra\_109}
Theorem mathd\_algebra\_109 in the valid validation set of Minif2f is defined as follows:
\begin{tcolorbox}[colframe=brown!40!black, colback=brown!5,title=\textbf{Theorem Define}]
theorem mathd\_algebra\_109 (a b : $\mathbb{R}$) ($h_0$ : 3 * a + 2 * b = 12) ($h_1$ : a = 4) : b = 0
\end{tcolorbox}

Each time we replace the theorem state placeholder in the prompt with the intermediate state when proving the theorem, and then use the proof tactic given by the model to prove the theorem.

 

\begin{tcolorbox}[colframe=brown!40!black, colback=brown!5, breakable,title=\textbf{Example}]
\texttt{<s>You are using Lean4 for theorem proving[INST] You will be provided with a json-formatted data describing the intermediate proof status of the Lean4 theorem. \\ 
You need to give a prediction of a proof tactic based on the data description and your knowledge and experience about Lean4 proofs.\\
The input json format description is as follows: \\ 
\{\\ 
\hspace*{2em}"current\_theorem\_state": <current theorem state description> \\ 
\}\\
Your output must also be json-formatted data in the following format: \\
\{ \\
\hspace*{2em}"predict\_tactic": <A proof tactic that complies with Lean4 syntax. The type is a string and the content cannot contain "sorry"> \\
\} \\
Your input: \\
\{ \\
\hspace*{2em}"current\_theorem\_state": "b : $\mathbb{R}$\texttt{\textbackslash n} $h_0$ : 3 * 4 + 2 * b = 12\texttt{\textbackslash n} b = 0" \\
\} \\
Your output:\\
}
\tikz \draw[dashed] (0,0) -- (\linewidth,0);
\texttt{
\{ \\
\hspace*{2em}"predict\_tactic": " linarith" \\
\} \\
}
\end{tcolorbox}

Theorem mathd\_algebra\_109 can be proved using two proof tactics, \texttt{"}subst $h_1$\texttt{"} and \texttt{"}linarith\texttt{"}, but this does not mean that there is only one way to prove the theorem. When the model predicts the tactics for the first proof state, it will not only generate the tactic shown above, but also other proof tactics. If the proof tactic of the first step, \texttt{"}subst $h_1$\texttt{"}, is replaced by \texttt{"}rw [$h_1$] at $h_0$\textbf{\texttt{"}}, and the proof tactic of the second step is not changed, the theorem can also be proved.


\subsection{mathd\_numbertheory\_109}
The theorem is defined as follows
\begin{tcolorbox}[colframe=brown!40!black, colback=brown!5,title=\textbf{Theorem Define}]
theorem mathd\_algebra\_109 (v : $\mathbb{N}$ → $\mathbb{N}$) ($h_0$ : $\forall$ n, v n = 2 * n - 1) :
  ($\sum$ k in Finset.Icc 1 100, v k) \% 7 = 4
\end{tcolorbox}

\begin{tcolorbox}[colframe=brown!40!black, colback=brown!5, breakable,title=\textbf{Step 1}]
\texttt{<s>You are using Lean4 for theorem proving[INST] You will be provided with a json-formatted data describing the intermediate proof status of the Lean4 theorem. \\ 
You need to give a prediction of a proof tactic based on the data description and your knowledge and experience about Lean4 proofs.\\
The input json format description is as follows: \\ 
\{\\ 
\hspace*{2em}"current\_theorem\_state": <current theorem state description> \\ 
\}\\
Your output must also be json-formatted data in the following format: \\
\{ \\
\hspace*{2em}"predict\_tactic": <A proof tactic that complies with Lean4 syntax. The type is a string and the content cannot contain "sorry"> \\
\} \\
Your input: \\
\{ \\
\hspace*{2em}"current\_theorem\_state": "v : $\mathbb{N}$ → $\mathbb{N}$ $h_0$ : $\forall$ (n : $\mathbb{N}$), v n = 2 * n - 1 $\mathbb{\vdash}$ ($\sum$ k $\in$ Finset.Icc 1 100, v k) \% 7 = 4" \\
\} \\
Your output:\\
}
\tikz \draw[dashed] (0,0) -- (\linewidth,0);
\texttt{
\{ \\
\hspace*{2em}"predict\_tactic": "norm\_num $h_0$" \\
\} \\
}
\end{tcolorbox}

\begin{tcolorbox}[colframe=brown!40!black, colback=brown!5, breakable,title=\textbf{Step 2}]
\texttt{<s>You are using Lean4 for theorem proving[INST] You will be provided with a json-formatted data describing the intermediate proof status of the Lean4 theorem. \\ 
You need to give a prediction of a proof tactic based on the data description and your knowledge and experience about Lean4 proofs.\\
The input json format description is as follows: \\ 
\{\\ 
\hspace*{2em}"current\_theorem\_state": <current theorem state description> \\ 
\}\\
Your output must also be json-formatted data in the following format: \\
\{ \\
\hspace*{2em}"predict\_tactic": <A proof tactic that complies with Lean4 syntax. The type is a string and the content cannot contain "sorry"> \\
\} \\
Your input: \\
\{ \\
\hspace*{2em}"current\_theorem\_state": "v : $\mathbb{N}$ → $\mathbb{N}$ $h_0$ :$\forall$ (n : $\mathbb{N}$), v n = 2 * n - 1 $\mathbb{\vdash}$ ($\sum$ x $\in$ Finset.Icc 1 100, 2 * x - 1) \% 7 = 4" \\
\} \\
Your output:\\
}
\tikz \draw[dashed] (0,0) -- (\linewidth,0);
\texttt{
\{ \\
\hspace*{2em}"predict\_tactic": "rfl" \\
\} \\
}
\end{tcolorbox}





% \section{Limitations}
% In Stage 2 of the proposed CuDIP method, during the process of constructing preference data, Monte Carlo Tree Search (MCTS) was employed for proof search to generate fine-grained scoring data for training the score generator. The data produced during this search process can also be utilized as part of the raw data for constructing preference data. 
% \section{More details}

% \section{MCTS Scoring Process}


\end{document}
