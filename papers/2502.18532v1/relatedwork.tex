\section{Related Works}
%%% 待精简
\textbf{Automated Theorem Proving with LLMs.}
% 两种范式： 1. LLM单步 + Search；2.LLM 利用非形式化语言得到完整证明过程
% 范式1： GPT-f  PACT HTPS  Lean-STaR Thor 
% 范式2： LEGO-Prover DSP 
% With the acceleration of Large Language Models (LLMs), automated theorem proving based on LLMs has been extensively applied, demonstrating significant progress. 
The rapid advancement of Large Language Models (LLMs) 
has spurred significant progress in automated theorem proving.
% has led to widespread applications of LLM-based automated theorem proving, achieving notable progress.
Various approaches integrate interactive theorem provers (ITP) such as 
Lean \cite{de2015lean}, Isabelle \cite{paulson1990isabelle}, and Metamath \cite{megill2019metamath} with language models. 
A prominent method, exemplified by GPT-f \cite{polu2020gpt-f}, involves the language model generating the next proof step based on the current proof state, followed by tree search to find a complete proof for the theorem. 
PACT \cite{han2021PACT} jointly trains a language model with a strategy prediction objective for interactive theorem proving, where the auxiliary task for extracting self-supervised signals is used. 
% utilizing self-supervised signals. 
HTPS \cite{lample2022hypertree} improves the automated theorem proving process by enhancing the MCTS-based proof search strategy. 
% Thor \cite{jiang2022thor}
% Lean-STaR \cite{lin2024leanstar} 
% Another approach uses LLMs to automatically generate the entire proof of a theorem, 
% Another approach utilizes informal proof languages to derive the complete proof of a theorem, such as DSP \cite{jiang2022dsp}, Lean-STaR \cite{lin2024leanstar}, and LEGO-Prover \cite{wang2023lego}.  
Another approach employs LLMs to derive the complete proof of a theorem, either directly or with the assistance of informal proof languages \cite{jiang2022dsp,lin2024leanstar,wang2023lego}. %%%%% ?TODO
However, most approaches rely on supervised fine-tuning (SFT), which may not align well with human preferences, potentially limiting their proving capabilities.

% or 本文遵循第一种范式
% In this paper, we introduce an innovative DPO-based automated theorem proving method that utilizes preference data to enhance the theorem proving capabilities of LLMs.

% Deepseek-Prover v1.5 GRPO ?
% or 现有方法效果仍然不够好 因此探索与人类偏好对齐的方法
% 然而现有的大部分方法使用SFT微调模型，与人类的偏好对齐程度不高），可能是限制现有方法证明能力的原因。我们给出了一种基于DPO的自动定理证明方法，利用偏好数据提高LLM定理证明能力。
% （基于偏好的方法）（提高了答案多样性？ 使得定理证明过程更与人类偏好对齐）

\textbf{Aligning LLMs with Human Preference.} %%% 部分内容可删掉/放Introduction 有点多
Reinforcement Learning from Human Feedback (RLHF) \cite{christiano2017RLHF} is a classical and effective method for aligning LLMs with human preferences. This approach involves initially training a reward model, followed by optimization using reinforcement learning algorithms such as 
PPO \cite{schulman2017PPO}, 
% Proximal Policy Optimization Algorithms (PPO) \cite{schulman2017PPO}, 
leading to substantial success in models like 
ChatGPT %\cite{achiam2023gpt}, 
\cite{radford2019lGPT,brown2020GPT,achiam2023gpt}, % GPT? ChatGPT
% \cite{radford2018GPT,radford2019lGPT,brown2020GPT,achiam2023gpt}, % GPT? ChatGPT
LLaMA \cite{touvron2023llama,dubey2024llama}, 
% Gemini \cite{}, 
and Claude \cite{bai2022training}. 
However, using PPO for RLHF is a complex and computationally expensive process. Motivated by this, Direct Preference Optimization (DPO) \cite{dpo2024} has emerged as an effective alternative. 
DPO directly optimizes using preference data, without the need to train a reward model. 
Several works \cite{ethayarajh2024kto,hong2024orpo,lai2024stepdpo,xu2024contrastive,zeng2024token,azar2024dpo_variant}  %KTO,  CPO,, ORPO ,Step-DPO,Token-level DPO 可删 %xu2024contrastive,zeng2024token,azar2024dpo_variant,
have proposed variations based on DPO. 
The simplicity and efficiency of DPO have enabled its application in various downstream tasks, such as 
mathematical reasoning \cite{xu2024chatglm,jiao2024learning,lu2024step-controlled-dpo}, %Chat-GLM & %,lu2024step-controlled-dpo
multimodal tasks \cite{majumder2024tango}, 
and 
summarization \cite{dpo2024}.  %%% 只介绍数学推理?
However, DPO still relies on high-quality preference labels to generate preference data, which requires costly manual annotation. 
%%%% 表述 DPO性能依赖
In this paper, we propose a method for constructing DPO preference data and 
% utilize the generated data to improve DPO's performance in mathematical theorem proving.
the generated preference data is utilized in DPO iterative training to enhance the theorem proving capabilities of LLMs.
% This paper presents a preference data construction method based on fine-grained LLM scoring, which reduces the reliance on human annotations. The generated preference data is utilized in DPO iterative training to enhance the theorem proving capabilities of LLMs.
%   TODO Deepseek-Prover v1.5 GRPO 成本高?
%   CL+PPO）Curriculum Learning and Theorem Proving Zsolt Zombori

%% *** DPO的 具体 应用
%% DPO在数学推理等多个领域的应用 尚未应用在定理证明 需要高质量的偏好数据

\textbf{Curriculum Learning.} %提出根据证明难度优化学习轨迹的CL LeanAgent 
Curriculum learning \cite{bengio2009curriculum} 
is a method that simulates the human learning process by progressing from simpler to more complex tasks. 
Chang et al. \cite{chang2021does} applied curriculum learning to data-to-text generation, achieving improvements in generation quality. %%% 这句可删
By strategically organizing the learning trajectory, curriculum learning enhances model performance or training efficiency and has shown promise in mathematical reasoning tasks such as theorem proving. %%% 可删
% In theorem proving tasks, curriculum learning has also demonstrated significant potential. 
Polu et al. \cite{polu2022formalCL} introduced a curriculum of progressively harder statements by synthesizing inequalities with increasing difficulty, 
%  \cite{ zombori2019curriculumTP} 使用证明的长度确定强化学习的奖励。
while LeanAgent \cite{kumarappan2024leanagent} categorized theorems into three levels of complexity and utilized curriculum learning to train on mathematical repositories. 
% These studies highlight the effectiveness of curriculum learning methods. 
In this paper, we propose a curriculum learning iterative training method based on the difficulty of theorem proof data and demonstrate its effectiveness.
% \cite{narvekar2020curriculumSurvey} TODO 可以参考这篇再改 & methodology说明

%% 我们的方法独特地结合了CL DPO  
%%% Iteration learning
% \textbf{Curiculum learning based}
% \textbf{LLM as judge.} %%or Reeard Model ? 表述待修改