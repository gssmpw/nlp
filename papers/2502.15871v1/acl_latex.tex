% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
%\usepackage{colortbl} % Adds table-specific coloring commands % this cause a problem remove and add everytime to fix
%\usepackage{longtable}
%\usepackage{hyperref}
\usepackage{float}


%\usepackage[a4paper, left=1in, right=1in]{geometry}
\usepackage{booktabs}
\usepackage{tikz}
\usetikzlibrary{external}
\tikzexternalize % activate!
\usepackage{ textcomp }
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{nicefrac}
\usepackage{xurl}
\usepackage{tabularray}
\newcommand{\xsub}[1]{%
  \mbox{\scriptsize\begin{tabular}{@{}c@{}}#1\end{tabular}}%
}
\usepackage{hyperref}

%\hypersetup{
    %colorlinks=true,
    %linkcolor=blue,
    %urlcolor=magenta,
    %citecolor=cyan
%}
\usepackage{epstopdf}
\usepackage{makecell}
\usepackage{threeparttable}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{tabulary}
\usepackage{tablefootnote}
\usepackage{array}
%\usepackage{hyperref}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\renewcommand\arraystretch{1.3}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}
%\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{epstopdf}
\usepackage{subfigure}
\setcounter{tocdepth}{3}
%\usepackage{graphicx}
% \usepackage{subfig}
\usepackage{caption}
\usepackage{color}
\usepackage{mathtools}
\usepackage{url}
\usepackage{bm}
% \usepackage{algorithmic}
%\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage[edges]{forest}
\usepackage{fontawesome5}
\usepackage{natbib}
\usepackage{afterpage}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\definecolor{hidden-draw}{RGB}{20,68,106}
\definecolor{hidden-pink}{RGB}{255,245,247}

% Red Cross Mark
\definecolor{mattered}{RGB}{214, 26, 60}
\definecolor{mattegreen}{HTML}{369F39}


\newcommand{\redcross}{{{\color{mattered}\faIcon{times-circle}}}}
% Green Check Mark
\newcommand{\greencheck}{{\color{mattegreen}\faIcon{check-circle}}}


% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
% \SetCommentSty{mycommfont}
\newcommand{\black}[1]{\textcolor{black}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\zhongwei}[1]{\textcolor{blue}{#1}}
\newcommand{\yuzheng}[1]{\textcolor{blue}{Yu: #1}}
\newcommand{\xin}[1]{\textcolor{blue}{#1}}  % 
\newcommand{\mz}[1]{\textcolor{red}{[MZ: #1]}}
\newcommand{\zq}[1]{\textcolor{green}{[ZQ: #1]}}
\newcommand{\quanlu}[1]{\textcolor{orange}{[Quanlu: #1]}}
\newcommand{\samiul}[1]{\textcolor{orange}{[Samiul: #1]}}
\newcommand{\yz}[1]{\textcolor{red}{[YiZhu: #1]}}
\newcommand{\sy}[1]{\textcolor{red}{[ShenYan: #1]}}

% \newcommand\figref[1]{Fig.~\ref{#1}}
\newcommand\figsubref[1]{Fig.~\subref{#1}}
\newcommand\tabref[1]{Tab.~\ref{#1}}
% \newcommand\secref[1]{Sec.~\ref{#1}}
\newcommand\equref[1]{Eq.(\ref{#1})}
\newcommand\appref[1]{(App.~\ref{#1})}
\newcommand{\fakeparagraph}[1]{\vspace{1mm}\noindent\textbf{#1}}
\newcommand{\etal}{\emph{et~al.}\xspace}
\newcommand{\eg}{\emph{e.g.},\xspace}
\newcommand{\ie}{\emph{i.e.},\xspace}
\newcommand{\etc}{etc.\xspace}
\newcommand{\iconText}[2]{
    % \includegraphics[height=2ex]{#1} % Display the icon
    \,\texttt{#2} % Display the text next to the icon, with a small space in between
}

\newcommand{\ours}{\textsc{MedTrust}\xspace}

\input{sk_Jun_command}
\newcommand{\xw}[1]{{\small\color{blue}{\bf xw:} #1}}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{Manar Aljohani \\
 % Virginia Tech / Address line 1 \\
  %Affiliation / Address line 2 \\
 % Affiliation / Address line 3 \\
  %\texttt{email@domain} \\\And
  %Xuan Wang \\
  %Virginia Tech / Address line 1 \\
  %Affiliation / Address line 2 \\
  %Affiliation / Address line 3 \\
  %\texttt{email@domain} \\}

\author{
  \textbf{Manar Aljohani\textsuperscript{1}},
  \textbf{Jun Hou\textsuperscript{1}},
  \textbf{Sindhura Kommu\textsuperscript{1}},
  \textbf{Xuan Wang\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
\\
  \textsuperscript{1}Department of Computer Science, Virginia Tech, Blacksburg, VA, USA
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
\\
  \normalsize{
   \texttt{(\href{mailto:manara@vt.edu}{manara}, \href{mailto:junh@vt.edu}{junh}, \href{mailto:sindhura@vt.edu}{sindhura}, \href{mailto:xuanw@vt.edu}{xuanw})@vt.edu}
  }
}
%\geometry{a4paper}


\begin{document}
\maketitle
\begin{abstract}
The application of large language models (LLMs) in healthcare has the potential to revolutionize clinical decision-making, medical research, and patient care. As LLMs are increasingly integrated into healthcare systems, several critical challenges must be addressed to ensure their reliable and ethical deployment. These challenges include truthfulness, where models generate misleading information; privacy, with risks of unintentional data retention; robustness, requiring defenses against adversarial attacks; fairness, addressing biases in clinical outcomes; explainability, ensuring transparent decision-making; and safety, mitigating risks of misinformation and medical errors. Recently, researchers have begun developing benchmarks and evaluation frameworks to systematically assess the trustworthiness of LLMs. However, the \textbf{trustworthiness of LLMs in healthcare} remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights into this area. This \textbf{survey} bridges this gap by providing a comprehensive overview of the recent research of existing methodologies and solutions aimed at mitigating the above risks in healthcare. By focusing on key trustworthiness dimensions including truthfulness, privacy and safety, robustness, fairness and bias, and explainability, we present a thorough analysis of how these issues impact the reliability and ethical use of LLMs in healthcare. This paper highlights ongoing efforts and offers insights into future research directions to ensure the safe and trustworthy deployment of LLMs in healthcare.
\end{abstract}

\section{Introduction}
The application of LLMs in healthcare is advancing rapidly, with the potential to transform clinical decision-making, medical research, and patient care. However, incorporating them into healthcare systems poses several key challenges that need to be addressed to ensure their reliable and ethical use. As highlighted in \citet{bi2024ai}, a major concern is the trustworthiness of AI-enhanced biomedical insights. This encompasses improving model explainability and interpretability, enhancing robustness against adversarial attacks, mitigating biases across diverse populations, and ensuring strong data privacy protections. Key concerns include truthfulness, privacy, safety, robustness, fairness, and explainability, each of which plays a vital role in the reliability and trustworthiness of AI-driven healthcare solutions. 

\emph{Truthfulness}, defined as "the accurate representation of information, facts, and results by an AI system" \cite{huang2024position}, is critical in healthcare, as inaccuracies can lead to misdiagnoses or inappropriate treatment recommendations. Ensuring that generated information is both accurate and aligned with verified medical knowledge is essential. Additionally, \emph{privacy} concerns arise from the risk of exposing sensitive patient data during model training and usage, potentially leading to breaches or violations of regulations such as HIPAA (Health Insurance Portability and Accountability Act) and GDPR (General Data Protection Regulation). Ensuring patient confidentiality while leveraging LLMs for diagnostics and treatment recommendations is a critical challenge. \emph{Safety}, defined as “ensuring that LLMs do not answer questions that can harm patients or healthcare providers in healthcare settings” \cite{han2024medsafetybench}, further underscores the necessity of implementing stringent safeguards to mitigate harm. \emph{Robustness} refers to an LLM’s ability to consistently generate accurate, reliable, and unbiased outputs across diverse clinical scenarios while minimizing errors, hallucinations, and biases. It also encompasses the model’s resilience against adversarial attacks, ensuring that external manipulations do not compromise its integrity. A truly robust LLM in healthcare must demonstrate stability, reliability, and fairness, even when faced with noisy, ambiguous, or adversarial inputs, thereby safeguarding patient safety and supporting clinical decision-making. Similarly, \emph{fairness and bias} must be addressed to prevent discriminatory patterns in model predictions, which could lead to unequal treatment recommendations and exacerbate healthcare disparities. Furthermore, the \emph{explainability} of LLMs, which ensures that model outputs are interpretable and transparent, plays a vital role in fostering trust and allowing informed decision-making by healthcare professionals. The lack of transparency in model reasoning complicates clinical adoption and raises concerns about accountability.

Tackling these challenges is essential for the trustworthy and ethical implementation of LLMs in healthcare. Recently, researchers have begun developing benchmarks and evaluation frameworks to systematically assess the trustworthiness of LLMs \cite{huang2024position}. The \textbf{trustworthiness of LLMs in healthcare} is gaining increasing attention due to its significant social impact. However, there is currently no systematic review that provides a comprehensive understanding and future insights into this area. To bridge this gap, we present a comprehensive \textbf{survey} that explores these trust-related dimensions in detail, reviewing existing datasets, solutions, and methodologies aimed at improving the trustworthiness of LLMs in healthcare.

\section{Datasets, Models, and Tasks}
We first conducted an extensive search for papers on the trustworthiness of LLMs in healthcare. Our search utilized a range of keyword combinations, including terms such as `large language models,' `foundation model,' `medical,' `clinical,' `explainability,' `truthfulness,' `trustworthiness,' `safety,' `fairness,' `robustness,' and `privacy.' We explored several reputable venues, including Arxiv, PubMed, ACL, EMNLP, NAACL, ICML, NeurIPS, ICLR, KDD, Nature, Science, AAAI, and IJCAI, with a focus on recent publications post-2021. After reviewing the search results, we identified a total of 30,595 papers. Following the removal of duplicates, we narrowed the focus to 69 papers that specifically addressed the truthfulness, privacy, safety, robustness, fairness, bias, and explainability of LLMs in the healthcare domain.

We then summarized all the datasets, models, and tasks relevant to research on trust in LLMs for healthcare, providing a comprehensive overview of their applications and contributions to this domain. \textbf{The datasets} used in studies of trust in LLMs for healthcare are categorized by the dimensions of trustworthiness they address in Appendix~\ref{app:datasets}, where we highlight key details such as data type, content, task, and dimensions of trustworthiness. The content of each dataset specifies its composition, while the task refers to the primary purpose for which the dataset is utilized. The data type varies across datasets and includes web-scraped data, curated domain-specific datasets, public text corpora, synthetic data, real-world data, and private datasets, providing a comprehensive overview of their relevance to healthcare applications. \textbf{The models} assessed in studies on trust in LLMs for the healthcare domain are outlined, along with their trustworthiness dimensions, in Appendix~\ref{app:gpts}, where we summarized key details such as the model name, release year, task, and the institution responsible for its development. \textbf{The tasks} covered various primary focuses of LLMs in healthcare. Based on insights from the survey by \citet{liu2024survey}, these tasks are outlined as follows:

\paragraph{Medical Information Extraction (Med-IE)} Med-IE extracts structured medical data from unstructured sources such as EHRs, clinical notes, and research articles. Key tasks include entity recognition (identifying diseases, symptoms, and treatments), relationship extraction (understanding entity connections), event extraction (detecting clinical events and attributes), information summarization (condensing medical records), and adverse drug event detection (identifying medication-related risks).

\paragraph{Medical Question Answering (Med-QA)} Med-QA systems interpret and respond to complex medical queries from patients, clinicians, and researchers. Their core functions include query understanding (interpreting user questions), information retrieval (finding relevant data in medical databases), and inference and reasoning (drawing conclusions, inferring relationships, and predicting outcomes based on retrieved data).

\tikzstyle{my-box}=[
    rectangle,
    draw=hidden-draw,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=2pt,
    align=center,
    fill opacity=.5,
    line width=0.8pt,
]
\tikzstyle{leaf}=[my-box, minimum height=1.5em,
    fill=hidden-pink!80, text=black, align=left,font=\normalsize,
    inner xsep=2pt,
    inner ysep=4pt,
    line width=0.8pt,
]



\begin{figure*}[t!]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{forest}
            forked edges,
            for tree={
                grow=east,
                reversed=true,
                anchor=base west,
                parent anchor=east,
                child anchor=west,
                base=center,
                font=\large,
                rectangle,
                draw=hidden-draw,
                rounded corners,
                align=left,
                text centered,
                minimum width=4em,
                edge+={darkgray, line width=1pt},
                s sep=3pt,
                inner xsep=2pt,
                inner ysep=3pt,
                line width=0.8pt,
                ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},
            },
            where level=1{text width=14em,font=\normalsize,}{},
            where level=2{text width=18em,font=\normalsize,}{},
            where level=3{text width=18em,font=\normalsize,}{},
            [
                \textbf{Trustworthiness of LLMs in Healthcare}, ver
                                 [
                                \textbf{Truthfulness}, fill=blue!10
                            [
                        Med-HALT~\cite{pal-etal-2023-med}{,} Med-HVL~\cite{yan2024med}{,} Med-HallMark~\cite{chen2024detecting}{,} Semantic Entropy~\cite{farquhar2024detecting}{,} \\ 
                        SEPs~\cite{han2024semantic}{,} Survey~\cite{ahmad2023creating}{,} Self Reflection~\cite{ji-etal-2023-towards}{,}  Model-Agnostic Hallucination Post-Processing~\cite{li-etal-2024-better}{,} \\ 
                        Faithful Reasoning~\cite{tan2024faithful}{,} PubHealthTab~\cite{akhtar-etal-2022-pubhealthtab}{,} HEALTHVER~\cite{sarrouti-etal-2021-evidence-based}{,} CRITIC~\cite{gou2024critic}{,}\\ 
                        Cross-Examination~\cite{cohen2023lm}{,} KnowledgeEditor~\cite{de-cao-etal-2021-editing}, leaf, text width=66em
                            ]
                                ]
                            [
                            \textbf{Privacy}, fill=blue!10
                            [
                        Survey~\cite{das2024security}{,} Survey~\cite{yan2024protecting}{,}
                        Privacy Risks~\cite{pan2020privacy}{,}
                        Federated Learning~\cite{zhao2024llm}{,}\\ 
                        Differential Privacy~\cite{singh2024whispered}{,}
                        SecureSQL~\cite{song-etal-2024-securesql}{,} Memorize Fine-tuning Data~\cite{yang2024memorization}{,}\\ clinical Note De-identification~\cite{altalla2025evaluating}{,} De-identification~\cite{liu2023deid}, leaf, text width=66em
                            ]
                         ]
                         [
                            \textbf{Safety}, fill=blue!10
                            [
                        Med-harm~\cite{han2024towards}{,} Medsafetybench~\cite{han2024medsafetybench}{,} UNIWIZ~\cite{das2024uniwiz}{,}
                        Misinformation Attacks~\cite{han2024medical}{,}\\ MEDIC~\cite{kanithi2024mediccomprehensiveframeworkevaluating}, leaf, text width=66em
                            ]
                        ]
                        [
                        \textbf{Robustness}, fill=blue!10
                        [
                        Survey~\cite{yuan2023revisiting}{,} Survey~\cite{alberts2023large}{,} Stumbling Blocks~\cite{wang2024stumbling}{,} LLM-TTA~\cite{o2024improving}{,} MedFuzz~\cite{ness2024medfuzz}{,}\\ Detecting Anomalies~\cite{rahman2024incorporating}{,} Instruction Phrasings~\cite{ceballos-arroyo-etal-2024-open}{,} Adversarial Attacks~\cite{yang2024adversarial}{,}\\ Secure Your Model~\cite{tang-etal-2024-secure}, leaf, text width=66em
                        ]
                        ]
                        [
                        \textbf{Fairness and Bias}, fill=blue!10
                                 [
                                Evaluation Study\cite{zack2024assessing}{,} BiasMedQA~\cite{schmidgall2024evaluation}{,} Survey~\cite{parray2023chatgpt}{,} Instruction Fine-tuning~\cite{singhal2023large}{,}\\ Hurtful Words~\cite{zhang2020hurtful}{,} Race-based Medicine~\cite{omiye2023large}{,} Detect Debunked Stereotypes~\cite{swaminathan2024feasibility}{,} \\ EquityMedQA~\cite{pfohl2024toolbox}{,} Superficial Fairness Alignment~\cite{wei2024actions}{,} Examines Biased AI~\cite{adam2022just}{,}\\ Identify Biases~\cite{Yang_2024}{,} Quantifying Cognitive Biases~\cite{lin-ng-2023-mind}{,} Mitigate Cognitive Biases~\cite{ke2024mitigating}{,} CI4MRC~\cite{zhu2023causal}, leaf, text width=66em
                                 ]  
                        ]
                        [
                        \textbf{Explanability}, fill=blue!10
                                 [
                                Knowledge Graphs~\cite{shariatmadari2024harnessing}{,} LLMs and Explainable ML~\cite{elsborg2023using}{,}  Medical Imaging Explainability~\cite{ghosh2023bridging}{,}\\ MedViLaM~\cite{xu2024medvilam}{,} MedThink~\cite{gai2024medthink}{,} MedExQA~\cite{kim-etal-2024-medexqa}{,} Causal Graphs Meet Thoughts~\cite{luo2025causal}{,}\\ Retrieval and Reasoning on KGs~\cite{ji-etal-2024-retrieval}{,} TOSRR~\cite{liu2025improvingtcmquestionanswering}{,} DDCoT~\cite{zheng2023ddcot}{,} Layered Chain-of-Thought Prompting~\cite{sanwal2025layered}{,}\\ A ChatGPT Aided Explainable Framework~\cite{liu2023a},  leaf, text width=66em
                                 ]       
                        ]
                        ]
        \end{forest}
        }
    \caption{Summary of the recent research across various dimensions of trustworthiness of LLMs in healthcare.}
    \label{fig:Dimensions}
\end{figure*}
                         

\paragraph{Medical Natural Language Inference (Med-NLI)} Med-NLI analyzes the logical relationships between medical texts. Key tasks include textual entailment (determining if one statement logically follows another), contradiction detection (identifying conflicting statements), neutral relationship identification (recognizing unrelated statements), and causality recognition (inferring cause-and-effect relationships).

\paragraph{Medical Text Generation (Med-Gen)} Med-Gen focuses on generating and summarizing medical content. Its key applications include text summarization (condensing lengthy documents into concise summaries) and content generation (producing new medical descriptions or knowledge based on input data).

\section{Trustworthiness of LLMs in Healthcare}
We examine the challenges related to the trustworthiness of LLMs in healthcare, outlining key strategies for identifying and mitigating these concerns. From our literature review screening, we identified truthfulness, privacy and safety, robustness, fairness and bias, and explainability as key trustworthiness dimensions of LLMs as highlighted in TrustLLM \cite{huang2024position}, particularly in healthcare. Figure \ref{fig:Dimensions} provides a summary of the recent research on trust in LLMs for healthcare across key dimensions of trustworthiness.

\subsection{Truthfulness}
Ensuring the \emph{truthfulness} of LLMs in healthcare is vital, as inaccurate information can significantly impact patient care and clinical outcomes. Given their influence on diagnoses and treatment decisions, it is essential to develop effective methods to detect and mitigate hallucinations and factual inaccuracies.

Hallucinations in medical LLMs arise from reliance on unverified sources, biases in training data, and limitations in contextual understanding and sequential reasoning \cite{ahmad2023creating}. Addressing these issues requires robust evaluation frameworks, self-correction mechanisms, and uncertainty quantification techniques.

The Med-HALT benchmark \cite{pal-etal-2023-med} is designed to evaluate hallucinations in medical LLMs by using reasoning-based tests like ‘False Confidence’ and ‘None of the Above,’ as well as memory-based tests to assess how well the model recalls medical knowledge. On the other hand, the interactive self-reflection methodology \cite{ji-etal-2023-towards} aims to reduce hallucinations in medical question-answering tasks by introducing an iterative feedback loop where the model refines its responses through self-evaluation and knowledge adjustment.

In the context of multimodal models, Med-HVL \cite{yan2024med} introduces two key metrics—Object Hallucination and Domain Knowledge Hallucination—to quantify hallucinations in Large Vision-Language Models (LVLMs). This framework also uses the CHAIR (Caption Hallucination Assessment with Image Relevance) metric to assess object hallucinations in image captioning. Med-HallMark \cite{chen2024detecting} takes it a step further by providing a multi-task evaluation framework with a hierarchical categorization of hallucinations, introducing the MediHall Score for assessing hallucination severity and the MediHallDetector, a multitask-trained LVLM for hallucination detection.

Researchers have also investigated semantic entropy, a probabilistic measure of uncertainty, to detect hallucinations in LLMs. For example, \citet{farquhar2024detecting} leverage semantic entropy to identify confabulations—hallucinations where the model generates arbitrary or incorrect outputs. While effective, this approach is computationally expensive, limiting its scalability. To overcome this, \citet{han2024semantic} introduce Semantic Entropy Probes (SEPs), which approximate semantic entropy directly from hidden states. By eliminating the need for multiple output samples, SEPs significantly reduce computational overhead. Both methods have been successfully applied to biomedical datasets, such as BioASQ.

Although these techniques offer valuable contributions, hallucination mitigation methods often lack adaptability, being either task-specific or requiring expensive retraining. To address this gap, MEDAL \cite{li-etal-2024-better} introduces a model-agnostic post-processing framework that integrates with any medical summarization model. MEDAL uses a self-examining correction model to improve factual accuracy without adding extra computational costs, providing a practical solution to the issue of medical hallucinations.

Collectively, these studies underscore the multifaceted challenge of ensuring truthfulness in medical LLMs. By leveraging benchmarking frameworks, self-correction mechanisms, and entropy-based uncertainty measures, researchers can develop complementary strategies for detecting, mitigating, and quantifying hallucinations. A key focus of these efforts is the development of quantifiable scoring methods, enabling systematic assessment and comparison across different models. These evaluation techniques not only help identify the most reliable and effective LLMs for healthcare applications but also provide actionable insights for further improvements. 

Factual accuracy is fundamental to building trust in LLMs, especially in healthcare, where reliable and verifiable information is critical. However, current LLMs lack effective mechanisms to trace claims back to their original sources, underscoring the urgent need for improved validation techniques to ensure safe and trustworthy medical applications. To address these challenges, several studies have introduced innovative approaches to enhance the transparency, accuracy, and reliability of healthcare LLMs.

\citet{tan2024faithful} propose an approach that integrates multiple perspectives from scientific literature to evaluate conflicting arguments, thereby improving LLM reasoning. Similarly, \citet{akhtar-etal-2022-pubhealthtab} introduce PubHealthTab, a table-based dataset designed for validating public health claims against noisy evidence, while \citet{sarrouti-etal-2021-evidence-based} present HEALTHVER, a dataset tailored for evidence-based fact-checking of health-related claims. These structured benchmarks provide a foundation for assessing and refining the reliability of LLM-generated medical information.

Beyond dataset-driven validation, self-correction mechanisms have been explored to improve LLM truthfulness. \citet{gou2024critic} introduce CRITIC, a framework inspired by human fact-checking practices, enabling LLMs to validate and refine their responses through iterative feedback and evaluation. Expanding on automated fact-checking, \citet{cohen2023lm} propose a cross-examination framework, where an examiner LLM identifies inconsistencies through multi-turn interactions with the original model. Unlike fully automated verification pipelines, CRITIC incorporates human-like evaluation strategies, enhancing the trustworthiness of fact-checking in medical contexts.

Overall, these studies advance the factual accuracy and transparency of LLMs in healthcare by introducing structured benchmarks, iterative validation processes, and automated fact-checking strategies. By incorporating these approaches, researchers can enhance the reliability of medical LLMs, ensuring they deliver more accurate, evidence-based insights to support clinical decision-making.

\subsection{Privacy}
LLM-based healthcare applications pose significant \emph{privacy} risks due to their ability to memorize and reproduce sensitive patient data \cite{das2024security}. Unauthorized data exposure can lead to confidentiality breaches, ethical concerns, and compliance violations \cite{pan2020privacy}. Addressing these risks requires privacy safeguards at different stages of model development.

A major challenge is unintended data retention and leakage, where LLMs memorize fine-tuning data, increasing re-identification risks. Studies show that domain-specific LLMs, such as Medalpaca, can retain sensitive data, making privacy breaches more likely \cite{yang2024memorization}. Additionally, adversarial attacks like prompt injection and inference attacks can further exploit these vulnerabilities, as demonstrated by the SecureSQL benchmark \cite{song-etal-2024-securesql}.

To mitigate these risks, pre-training privacy safeguards focus on de-identification. \citet{altalla2025evaluating} assess GPT-3.5 and GPT-4 in clinical note de-identification and synthetic data generation. Similarly, \citet{liu2023deid} propose a GPT-4-enabled framework for masking private information while maintaining text structure. However, de-identification remains imperfect, as attackers may infer sensitive details from anonymized text.

During fine-tuning, techniques such as federated learning \cite{zhao2024llm} and differential privacy \cite{singh2024whispered}, as highlighted by \citet{liu2024survey}, play a crucial role in safeguarding patient data. Federated learning enables decentralized training without sharing raw data, but it demands high computational resources. Differential privacy adds noise to protect sensitive information but can reduce model accuracy.

Adversarial defenses remain limited. The SecureSQL benchmark \cite{song-etal-2024-securesql} highlights LLM vulnerabilities to structured query attacks. While chain-of-thought (COT) prompting offers partial mitigation, it does not eliminate the risk of data exposure.

Researchers address privacy concerns in healthcare LLMs through two primary approaches: de-identification, which alters real data to prevent re-identification, and synthetic data generation, which creates artificial data to eliminate reliance on sensitive patient information. While these strategies enhance privacy protection and maintain model effectiveness, challenges remain in long-term memorization control and adversarial robustness, requiring further research to strengthen data security and prevent unintended information retention.

\subsection{Safety}
Ensuring the \emph{safety} of LLMs in healthcare is critical, as these models must not generate harmful responses. A key safety concern, noted by \citet{han2024medical}, is that modifying just 1.1 \% of a model’s weights can embed persistent biomedical inaccuracies while maintaining overall performance. This highlights the need for rigorous validation mechanisms and safety assessments to prevent misleading medical information before clinical use.

To systematically assess safety, MedSafetyBench \cite{han2024medsafetybench} was introduced as the first benchmark designed to evaluate LLM safety in medical contexts. It includes 1,800 harmful medical queries alongside safety-optimized responses generated using advanced LLMs and adversarial techniques. Results indicate that publicly available medical LLMs fail to meet safety standards, but fine-tuning with MedSafetyBench significantly improves safety without compromising performance.

A major challenge is that adversarial actors can manipulate LLMs to generate unsafe outputs, while excessive safety alignment may induce hallucinations. To address this, \citet{das2024uniwiz} propose UNIWIZ, a two-step framework that unifies safety alignment and factual knowledge retrieval. Their safety-priming approach synthesizes safety-focused training data, while a retrieval mechanism ensures that model outputs remain factually accurate. Models fine-tuned on UNIWIZ outperform larger state-of-the-art instruction-tuned models across multiple safety and accuracy metrics.

Another key contribution to safety alignment is from \citet{han2024towards}, who provide the first comprehensive safety evaluation for medical LLMs (MedLLMs). They define key concepts of medical safety and alignment and introduce Med-Harm, a dataset designed to evaluate both general and medical-specific risks. This dataset assesses how well LLMs handle harmful medical questions, ensuring they adhere to safety and ethical standards in medical AI.

Further advancing safety assessments, \citet{kanithi2024mediccomprehensiveframeworkevaluating} introduce MEDIC, a multi-dimensional trustworthiness evaluation framework that systematically assesses medical LLMs across critical dimensions of clinical competence including medical reasoning and clinical safety.

These studies collectively offer a multi-faceted approach to LLM safety in healthcare. MedSafetyBench \cite{han2024medsafetybench} provides a standardized benchmark for safety evaluation and fine-tuning, while UNIWIZ \cite{das2024uniwiz} introduces a structured framework that prevents hallucinations while reinforcing safety. \citet{han2024towards} focus on comprehensive safety alignment, establishing Med-Harm to evaluate domain-specific risks. Lastly, MEDIC \cite{kanithi2024mediccomprehensiveframeworkevaluating} offers a holistic evaluation franmework for improving practical application of LLMs in clinical settings. Together, these efforts contribute to a more rigorous and systematic framework for assessing and improving LLM safety in medical applications.

\subsection{Robustness}
Enhancing the \emph{robustness} of LLMs is crucial for their reliability in healthcare applications. A key approach involves developing adversarial test samples tailored to the medical domain, such as synthetic anomaly cases \cite{yuan2023revisiting} and boundary stress testing \cite{wang2024stumbling}, to assess model resilience. However, creating clinically meaningful adversarial samples presents unique challenges, as \citet{alberts2023large} highlight the need to align adversarial testing methods with the complexities of real-world medical data, where medical dependencies must be accounted for.

In addition to adversarial testing, uncertainty quantification is another important avenue to improve robustness. LLM-TTA \cite{o2024improving} explores test-time adaptation techniques to enhance model performance on rare or unfamiliar cases, which are common in medical diagnostics. Unlike adversarial robustness, which focuses on resistance to manipulated inputs, uncertainty quantification aims to identify when models are likely to be incorrect, providing a complementary safety mechanism.

Another critical question is whether benchmark performance truly reflects medical robustness. MedFuzz \cite{ness2024medfuzz} challenges assumptions in MedQA by modifying questions to test if models rely on rigid, dataset-specific patterns rather than genuine clinical reasoning. This research exposes vulnerabilities in LLMs, revealing that subtle changes in input can significantly impact performance, raising concerns about their reliability in dynamic medical environments.

Instruction robustness is also a growing concern. \citet{ceballos-arroyo-etal-2024-open} examine how variations in medical instructions affect performance across different LLMs, finding that specialized medical models may be more fragile than general-purpose models when instructions are reworded. This counterintuitive result suggests that excessive domain adaptation may decrease flexibility and reduce robustness.

Adversarial vulnerabilities also pose direct security risks. \citet{yang2024adversarial} investigate two adversarial attack strategies across medical tasks using real patient data, demonstrating that fine-tuned models are especially vulnerable to poisoning attacks that subtly alter learned weights. While adversarial data does not always degrade general performance, it can introduce dangerous biases into specific medical predictions, making early attack detection a priority.

To protect against adversarial manipulations, \citet{tang-etal-2024-secure} introduce Secure Your Model, a framework that strengthens LLM robustness with cryptographic prompt authentication. This mechanism ensures that only verified and secure prompts are processed, mitigating vulnerabilities associated with prompt injections and adversarial attacks, and reducing the risk of model exploitation in medical contexts.

While all these studies address LLM robustness, they differ in their primary focus. MedFuzz \cite{ness2024medfuzz} and \citet{ceballos-arroyo-etal-2024-open} expose vulnerabilities in existing benchmarks and task instructions, questioning whether current evaluation methods truly measure robustness or merely reflect dataset biases. In contrast, \citet{yang2024adversarial} and \citet{alberts2023large} highlight adversarial threats, demonstrating how medical LLMs can be subtly manipulated, raising concerns about their security in real-world applications. Meanwhile, LLM-TTA \cite{o2024improving} takes a different approach, focusing on uncertainty quantification rather than adversarial resistance to enhance reliability in handling unfamiliar cases. Secure Your Model \cite{tang-etal-2024-secure} provides an additional security layer by introducing proactive adversarial defenses through prompt protection mechanisms, ensuring resilience against manipulation risks.

These studies highlight that robustness is a multifaceted challenge, requiring advancements in evaluation methods and defensive mechanisms. Ensuring LLMs can handle adversarial scenarios, integrate domain knowledge, and adapt to language variations is crucial for their safe deployment in healthcare. Strengthening robustness through testing and resilience-building enhances the trustworthiness of medical LLMs, making them more reliable in complex clinical settings.

\subsection{Fairness and Bias}
Ensuring \emph{fairness} in LLMs is crucial in healthcare, where biased models can result in unequal treatment outcomes. Research has highlighted biases in clinical data and practice related to race, gender, and disability. For example, \citet{omiye2023large} examine the potential for harmful or inaccurate race-based content in LLMs, while \citet{zack2024assessing} discusse how language models encode societal biases that can affect healthcare outcomes. These studies underscore the need for fairness in LLM development to ensure equitable healthcare delivery.

Efforts to address bias focus on both detection and mitigation. \citet{swaminathan2024feasibility} offers an automated method for detecting race-based medicine stereotypes, while BiasMedQA \cite{schmidgall2024evaluation} benchmarks cognitive biases in medical tasks across multiple models, revealing varying bias resilience. Mitigation strategies, such as bias education, and one-shot and few-shot bias demonstrations, are proposed to reduce but not fully eliminate bias. \citet{pfohl2024toolbox} introduce frameworks for assessing health equity-related harms in LLMs, including EquityMedQA, a dataset for equity-focused testing. Additionally, \citet{wei2024actions} distinguishe between intrinsic fairness, rooted in model training, and behavioral fairness, which relates to model operation in real-world applications, advocating for both to ensure equitable outcomes.

Bias is also explored in closed-source models. \citet{zack2024assessing} evaluate racial and gender biases in clinical scenarios, finding that models often amplify societal biases. Similarly, \citet{adam2022just} show that biased AI recommendations can affect emergency decisions, while \citet{Yang_2024} identify healthcare disparities in model predictions based on patient demographics. For open-source LLMs, techniques such as reinforcement learning with clinician feedback \cite{zack2024assessing} and data augmentation during pre-training \cite{parray2023chatgpt} enhance training data quality to reduce bias. In contrast, for closed-source models, where internal representations are inaccessible, strategies like instruction fine-tuning \cite{singhal2023large} and prompt engineering \cite{schmidgall2024evaluation} are employed to improve fairness in outputs.

Further, \citet{zhang2020hurtful} investigate how LLM embeddings can encode biases, particularly in clinical tasks, and applies adversarial debiasing to mitigate disparities. \citet{lin-ng-2023-mind} identify cognitive biases in BERT, while \citet{ke2024mitigating} use a multi-agent framework to explore how LLMs can mitigate biases in clinical decision-making. Additionally, \citet{zhu2023causal} introduce CI4MRC, a method that addresses name-related bias in Machine Reading Comprehension (MRC) tasks by applying a causal interventional paradigm. 

In addressing bias, solutions vary depending on whether the bias is at the individual level (e.g., name-related information) or dataset level (e.g., biased racial distributions). Open-sourced LLMs benefit from direct accessibility, enabling robust interventions to mitigate bias. In contrast, closed-sourced models, due to their inaccessibility, rely on methods such as instruction fine-tuning and external post-processing tools to refine model outputs and reduce bias.

\subsection{Explanability}
The lack of \emph{explainability} in LLMs poses a significant barrier to building trust with clinical practitioners, thereby restricting their adoption in real-world healthcare systems. To address this challenge, efforts in clinical practice and research have prioritized improving the transparency of LLMs by developing more effective explanation mechanisms and incorporating human oversight. For instance, \citet{shariatmadari2024harnessing} enhance biomedical applications by integrating knowledge graphs with language models and visualizing attention probabilities to provide clear and interpretable explanations for model predictions. \citet{elsborg2023using} employ local explanation models to generate intuitive, case-specific insights, further advancing the explainability of LLMs and fostering trust in their medical applications.

Another critical area of research focuses on medical imaging explainability. \citet{ghosh2023bridging} propose a method that iteratively decomposes a black-box (BB) model into interpretable expert models and a residual network, where expert models specialize in specific data subsets and explain their reasoning using First-Order Logic (FOL).

Efforts to improve reasoning and multimodal integration in LLMs have also gained momentum. MedViLaM \cite{xu2024medvilam} and MedThink \cite{gai2024medthink} focus on improving medical understanding and reasoning by integrating both textual and visual data for complex medical tasks such as question answering and medical image classification, aiming to provide a more holistic view for decision-making. In contrast, MedExQA \cite{kim-etal-2024-medexqa} prioritizes explainability in medical QA systems by providing multiple explanations for its responses. Causal Graphs Meet Thoughts \cite{luo2025causal} and Retrieval and Reasoning on KGs \cite{ji-etal-2024-retrieval} explore how to enhance complex reasoning in LLMs through knowledge graphs (KGs), helping these models retrieve relevant information from structured sources to answer questions more effectively and with better justification. 

For more specialized applications, TOSRR \cite{liu2025improvingtcmquestionanswering} introduce tree-organized self-reflective retrieval, which aims to improve performance in the niche area of Traditional Chinese Medicine (TCM), combining LLMs with self-reflection techniques for more accurate and contextually relevant responses. Studies like DDCoT \cite{zheng2023ddcot} and Layered Chain-of-Thought Prompting \cite{sanwal2025layered} develop methods for multimodal reasoning across multiple agents, with DDCoT focusing on chain-of-thought prompting for structured reasoning across tasks. Lastly, frameworks like A ChatGPT Aided Explainable Framework \cite{liu2023a} aim for zero-shot diagnosis and interpretation, ensuring more accessible AI support in real-world medical settings.

The trend towards explainability enhancement encompasses both intrinsic and post-hoc approaches. Intrinsic methods, such as knowledge graphs and decomposed expert models, integrate explainability directly into the models, offering natural transparency. Post-hoc methods, including local explanations and textual justifications, elucidate model decisions after predictions, improving user comprehension and trust.

\section{Future Directions}
We have reviewed key trust challenges in LLMs and existing solutions. This section highlights current limitations and proposes future directions.

While efforts to enhance LLM \emph{truthfulness} in healthcare have advanced, gaps remain, including limited adaptability in hallucination mitigation and weak source attribution. Future research should prioritize model-agnostic post-processing, improved self-correction, enhanced uncertainty quantification, and real-time fact-checking via structured knowledge integration.

Existing \emph{privacy} safeguards, such as de-identification and federated learning, remain imperfect. Strengthening de-identification methods, reinforcing federated learning defenses, refining differential privacy, and exploring homomorphic encryption and real-time audits are crucial next steps.

\emph{Safety} remains a pressing concern due to adversarial attacks and excessive safety alignment-induced hallucinations. Future work should focus on robust validation mechanisms, refined safety alignment strategies, and comprehensive evaluation frameworks.

Improving \emph{robustness} remains critical, with adversarial testing and uncertainty quantification methods needing to better handle medical data complexities. Future research should focus on clinically relevant adversarial tests, enhancing uncertainty techniques, and improving instruction robustness.

Despite progress in \emph{fairness}, key gaps persist, including the need for comprehensive bias assessments and real-world testing of mitigation strategies. Standardizing fairness metrics, conducting real-world evaluations, and assessing long-term impacts on healthcare equity are critical for progress.

Advancements in \emph{explainability} have yet to bridge significant gaps. Future research should focus on integrating explainability into clinical workflows, developing interactive explanations, and improving multimodal integration to enhance transparency and trust.

Recently, multi-agent frameworks like TriageAgent \cite{lu-etal-2024-triageagent} have been introduced to streamline complex clinical tasks through agent collaboration. By harnessing their capabilities, we can embed trustworthiness mitigation and evaluation within a multi-agent system, enabling proactive monitoring and intervention across key areas such as truthfulness, privacy, robustness, fairness, and explainability in healthcare LLMs.

Addressing these gaps will ensure that LLMs can be effectively integrated into healthcare systems, improving their reliability, privacy, safety, fairness, and transparency.

\section{Conclusion}
The integration of LLMs into healthcare holds great promise, but realizing their full potential requires addressing the critical challenges outlined in this survey. A key concern is truthfulness, as inaccuracies in medical LLMs pose serious risks to patient safety, making their detection and mitigation an ongoing research priority. Equally vital are privacy, safety, robustness, fairness, and explainability, ensuring responsible deployment in real-world clinical settings.

While existing solutions show progress, much work remains to enhance the reliability, transparency, and ethical implications of LLMs in healthcare. Future research must refine these areas, balancing performance with trustworthiness while preventing LLM-based systems systems from worsening healthcare disparities. Comprehensive benchmarks, cross-disciplinary collaboration, and model accountability frameworks will be essential. Additionally, regulatory oversight and ethical guidelines must ensure LLM applications align with medical standards and patient rights.

Ultimately, achieving safe and equitable AI-driven healthcare will require ongoing efforts to improve both technical capabilities and societal frameworks.

\clearpage
\section*{Limitations}
This survey provides a comprehensive overview of the challenges associated with LLMs in healthcare, but it primarily focuses on existing methodologies, leaving out emerging technologies that could address these issues in new ways. It also lacks practical insights into the real-world implementation of these solutions, such as deployment challenges, cost considerations, and system integration, which would make the findings more applicable to healthcare settings.

While the paper addresses privacy and safety, it does not fully explore broader ethical issues like informed consent, patient autonomy, and human oversight. Additionally, the survey focuses on current research without delving into the long-term societal and health impacts of LLM deployment, such as changes in doctor-patient relationships, patient trust, and healthcare workflows.
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl_latex}

\clearpage
\appendix

\section{Comparison of Datasets}
\label{app:datasets}
We systematically collected and analyzed 41 datasets relevant to the study of trust in LLMs for healthcare. Table 1 provides a comprehensive summary, highlighting key attributes such as data type, content, associated tasks, and the specific trustworthiness dimensions they address. These datasets vary widely, including web-scraped data, curated domain-specific datasets, public text corpora, synthetic data, real-world data, and private datasets. Each dataset's content specifies its composition, while its associated task defines its primary research application. Additionally, we categorize the datasets based on critical trustworthiness dimensions—truthfulness, privacy and safety, robustness, fairness and bias, and explainability—offering a structured evaluation of their contributions to building reliable and trustworthy healthcare AI.

\begin{table*}[htbp]
        \centering
        \small
        \renewcommand{\arraystretch}{1.2}
        \setlength{\tabcolsep}{6pt}
        \begin{tabular}{p{2cm} p{2.5cm} p{4cm} p{3.5cm} p{1.5cm}}
            \toprule
            \textbf{Datasets} & \textbf{Data Type} & \textbf{Content} & \textbf{Task} & \textbf{Dimensions} \\
            \midrule
        \href{https://huggingface.co/datasets/openlifescienceai/multimedqa}{MultiMedQA} &
          Combination of Public and Synthetic Data, Curated Domain-Specific Dataset &
          A benchmark combining six existing medical questions answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. &
          Tasks including Medical Question Answering, Clinical Reasoning, Evidence-Based Medicine, Multilingual and Multimodal Support, Bias and Safety Analysis &
          Fairness and Bias \\
          \hline
            \href{https://pure.johnshopkins.edu/en/publications/evaluation-and-mitigation-of-cognitive-biases-in-medical-language}{BiasMedQA} &
          Curated Domain-Specific Datasets &
          1273 USMLE questions &
          Replicate common clinically relevant cognitive biases &
          Fairness and Bias \\
          \hline
            \href{https://ai.nejm.org/browse/ai-article-type/datasets-benchmarks-protocols}{NEJM Healer} &
          Real Data and Curated Domain-Specific Dataset, &
          Consists of Clinical Cases, Diagnostic Pathways, Educational Materials, Interactive Learning Modules &
          Tasks including Diagnostic Skill Development, Medical Education, Simulated Decision-Making, Feedback and Improvement &
          Fairness and Bias \\
          \hline
            \href{https://arxiv.org/abs/2403.12025?utm_source=chatgpt.com}{EquityMedQA} &
          Curated domain-specific datasets and synthetic data &
          Cover a wide range of medical topics to surface biases that could harm health equity, including implicit and explicit adversarial questions addressing biases like stereotypes, lack of structural explanations, and withholding information. &
          Evaluate the performance of LLMs in generating unbiased, equitable medical responses. &
          Fairness and Bias \\
          \hline
            \href{(https://www.science.org/doi/10.1126/science.aax2342)}{”bias” medical dataset} &
          curated, domain-specific dataset comprising real-world healthcare data from a large U.S. health system &
          Includes patient demographics, medical histories, and healthcare utilization records. &
          Evaluate and identify racial biases in algorithms used for healthcare management. &
          Fairness and Bias \\
          \hline
            \href{https://rajpurkar.github.io/SQuAD-explorer/}{SQuAD} &
          Curated Domain-Specific Dataset &
          Consists of over 100,000 question-answer pairs derived from more than 500 articles from Wikipedia. Each question is paired with a segment of text from the corresponding article, serving as the answer. &
          To develop models that can read a passage and answer questions about it, assessing the model's ability to understand and extract information from the text. &
          Fairness and Bias \\
          \hline
            \href{https://physionet.org/content/mimicdb/1.0.0/}{MIMIC} &
          Real Data and Curated Domain-Specific Datasets &
          Consists of electronic health records include patient Demographics, Clinical Data, Medical Notes, Treatment Records, Time-series Data &
          Various medical and machine-learning tasks, including Clinical Decision Support, Disease Modeling, Natural Language Processing, Time-series Analysis, Education and Research &
          Fairness and Bias, Explainability \\
          \hline
            \href{https://github.com/jind11/MedQA}{MedQA} &
          Curated Domain-Specific Datasets &
          A benchmark that includes questions drawn from the United States Medical License Exam (USMLE). &
          Exam the physicians to test their ability to make clinical decisions &
          Fairness and Bias, Robustness, Explainability \\
          \hline
            \href{https://www.nature.com/articles/s41597-023-02814-8}{PMC-Patients} &
          Curated dataset derived from public text corpora. &
          Contains 167,000 patient summaries extracted from 141,000 PMC articles &
          Designed to benchmark ReCDS systems through two primary tasks: Patient-to-Article Retrieval (PAR), Patient-to-Patient Retrieval (PPR) &
          Robustness \\
          \hline
            \href{https://www.nature.com/articles/sdata201635}{MIMIC- III} &
          Public text corpora, real-world data &
          De-identified health-related data from over 40,000 critical care patients, including demographics, vital signs, laboratory tests, medications, and caregiver notes. &
          Epidemiological studies, clinical decision-rule improvement, machine learning in healthcare. &
          Robustness \\
          \hline
            \bottomrule
        \end{tabular}
        \label{tab:Datasets1}\end{table*}


\begin{table*}[htbp]
        \centering
        \small
        \renewcommand{\arraystretch}{1.2}
        \setlength{\tabcolsep}{6pt}
        \begin{tabular}{p{2cm} p{2.5cm} p{4cm} p{3.5cm} p{1.5cm}}
            \toprule
            \textbf{Datasets} & \textbf{Data Type} & \textbf{Content} & \textbf{Task} & \textbf{Dimensions} \\
            \midrule
        \href{(https://arxiv.org/html/2403.03744v4)}{MedSafetyBench} &
          Curated domain-specific dataset and synthetic (generated using GPT-4, Llama-2-7b-chat, and adversarial techniques). &
          1,800 harmful medical requests violating medical ethics, along with 900 corresponding safe responses. The dataset is structured based on the Principles of Medical Ethics from the American Medical Association (AMA). &
          Assess the medical safety of LLMs by testing whether they refuse to comply with harmful medical requests. Fine-tune LLMs using medical safety demonstrations to enhance their alignment with ethical medical guidelines. &
          Safety \\
          \hline
            \href{https://aclanthology.org/2024.findings-acl.102.pdf}{UNIWIZ} &
          Synthetic and curated data, including: 17,638 quality-controlled conversations, and 10,000 augmented preference data &
          Features conversations that integrate safety and knowledge alignment. A "safety-priming" method was employed to generate synthetic safety data, and factual information was injected into conversations by retrieving content from curated sources. &
          Fine-tune large language models to enhance their performance in generating safe and knowledge-grounded conversations. &
          Safety \\
          \hline
            \href{https://github.com/allenai/scifact?utm_source=chatgpt.com}{SciFact} &
          Curated Domain-Specific Dataset. &
          Includes claims and corresponding evidence abstracts, each annotated with labels indicating whether the claim is supported or refuted, along with rationales justifying the decision. &
          To verify the veracity of scientific claims by identifying supporting or refuting evidence within abstracts and providing justifications for these decisions. &
          Truthfulness \\
          \hline
            \href{https://aclanthology.org/2022.findings-naacl.1/?utm_source=chatgpt.com}{PubHealthTab} &
          Curated Domain-Specific Dataset &
          Contains 1,942 real-world public health claims, each paired with evidence tables extracted from over 300 websites. &
          Facilitates evidence-based fact-checking by providing claims and corresponding evidence tables for verification. &
          Truthfulness \\
          \hline
            \href{https://github.com/facebookresearch/LAMA?utm_source=chatgpt.com}{LAMA} &
          Curated Domain-Specific Dataset. &
          Comprises a set of knowledge sources, each containing a collection of facts. &
          To probe pretrained language models to determine the extent of their factual and commonsense knowledge. &
          Truthfulness \\
          \hline
            \href{https://aclanthology.org/P17-1147/}{TriviaQA} &
          Curated Domain-Specific Dataset. &
          Consists of over 650,000 question-answer pairs, each linked to a set of supporting documents. The questions are sourced from trivia websites, and the answers are derived from the corresponding documents. &
          Training and evaluating models on reading comprehension, specifically focusing on the ability to extract and reason over information from provided documents to answer questions. &
          Truthfulness \\
          \hline
            \href{https://aclanthology.org/Q19-1026/}{Natural Questions (NQ)} &
          Real data &
          consists of real anonymized queries from Google's search engine users, paired with answers derived from entire Wikipedia articles. &
          To develop and evaluate question-answering systems that can read and comprehend entire Wikipedia articles to find answers to user queries. &
          Truthfulness \\
          \hline
            \href{https://arxiv.org/abs/2212.10511}{PopQA} &
          Curated Domain-Specific Dataset. &
          consists of 14,000 QA pairs, each associated with fine-grained Wikidata entity IDs, Wikipedia page views, and relationship type information. &
          Designed for open-domain question answering tasks, focusing on evaluating the effectiveness of language models in retrieving and utilizing factual knowledge. &
          Truthfulness \\
          \hline
            \href{https://fever.ai/dataset/fever.html}{FEVER} &
          Curated Domain-Specific Dataset. &
          comprises 185,000 claims, each paired with evidence from Wikipedia articles. These claims are categorized as supported, refuted, or not verifiable. &
          Fact extraction and verification, where models are trained to determine the veracity of claims based on provided evidence. &
          Truthfulness \\
          \hline
            \href{https://aclanthology.org/2021.findings-emnlp.297/}{HEALTHVER} &
          Curated Domain-Specific Dataset. &
          Contains 14,330 evidence-claim pairs (SUPPORTS, REFUTES, NEUTRAL) on health claims, mainly COVID-19, verified against scientific articles. &
          Used to train and evaluate models in verifying health claims by classifying them based on scientific evidence. &
          Truthfulness \\
          \hline
            \bottomrule
        \end{tabular}
        \label{tab:Datasets2}\end{table*}


\begin{table*}[htbp]
        \centering
        \small
        \renewcommand{\arraystretch}{1.2}
        \setlength{\tabcolsep}{6pt}
        \begin{tabular}{p{2cm} p{2.5cm} p{4cm} p{3.5cm} p{1.5cm}}
            \toprule
            \textbf{Datasets} & \textbf{Data Type} & \textbf{Content} & \textbf{Task} & \textbf{Dimensions} \\
            \midrule
        \href{https://medhalt.github.io/}{Med-HALT} &
          Synthetic and Real Data, Curated Domain-Specific Dataset, and Public Dataset &
          Consist of Reasoning-Based Assessments, Memory-Based Assessments, Medical Scenarios, Evaluation Metrics &
          Tasks including Evaluation of Hallucination in Medical AI, Reliability Benchmarking, Error Analysis, Mitigation Development &
          Truthfulness \\
          \hline
            \href{https://github.com/allenai/medicat}{MedICaT} &
          Public Text Corpora And Real Data (curated from publicly available biomedical literature) &
          Contains medical images (e.g., radiographs, charts, and diagrams) paired with captions extracted from biomedical literature. Also, includes metadata about the source and context of the images. &
          Task including Medical Image Captioning, Text-Image Retrieval, Medical Reasoning &
          Truthfulness \\
          \hline
            \href{https://arxiv.org/html/2406.10185v1}{Med-HallMark} &
          Curated Domain-Specific Dataset, , Synthetic and Real Data (includes a mix of real-world medical data and synthetically generated hallucination scenarios) &
          Diverse medical multimodal data, including text, images, and paired annotations. Hierarchically categorized hallucination data, addressing both structural (e.g., object-level) and contextual (e.g., domain knowledge) hallucinations. &
          Task including Hallucination Detection, Hallucination Evaluation, Mitigation Analysis &
          Truthfulness \\
          \hline
            \href{https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0564-6}{BioASQ} &
          Curated Domain-Specific Dataset; Real Data. &
          The dataset comprises English-language biomedical questions, each accompanied by reference answers and related materials. These questions are designed to reflect real information needs of biomedical experts, making the dataset both realistic and challenging. &
          The primary task is Biomedical Question Answering (QA), which involves systems providing accurate answers to questions based on biomedical data. The dataset supports various QA tasks, including yes/no, factoid, list, and summary questions. &
          Truthfulness \\
          \hline
            \href{https://pmc.ncbi.nlm.nih.gov/articles/PMC11186750/}{FactualBio} &
          Synthetic Data; Public Text Corpora. &
          collection of biographies of individuals notable enough to have Wikipedia pages but lacking extensive detailed coverage. The dataset was generated using GPT-4 and includes biographies of 21 individuals randomly sampled from the WikiBio dataset. &
          Evaluating the factual accuracy of language models, particularly in the context of biography generation. It serves as a benchmark for detecting hallucinations and assessing the factual consistency of generated text. &
          Truthfulness \\
          \hline
            \href{https://pubmedqa.github.io/}{PubMedQA} &
          Curated Domain-Specific Dataset. &
          Consists of over 1,000 question-answer pairs derived from PubMed abstracts, focusing on various biomedical topics. &
          Evaluates the ability of models to comprehend and extract information from biomedical texts to answer specific questions. &
          Truthfulness \\
          \hline
            \href{https://github.com/abachaa/MedQuAD}{MedQuAD} &
          Curated Domain-Specific Dataset. &
          The dataset encompasses 37 question types, such as Treatment, Diagnosis, and Side Effects, associated with diseases, drugs, and other medical entities like tests. &
          Designed for medical question answering, the dataset aids in developing and evaluating systems that can understand and respond to medical inquiries. &
          Truthfulness \\
          \hline
            \href{https://github.com/abachaa/LiveQA_MedicalTask_TREC2017}{LiveMedQA2017} &
          Curated Domain-Specific Dataset &
          Consists of 634 question-answer pairs corresponding to National Library of Medicine (NLM) questions &
          Medical question answering, focusing on consumer health questions received by the U.S. National Library of Medicine. &
          Truthfulness \\
          \hline
            \href{https://aclanthology.org/2020.findings-emnlp.342/}{MASH-QA} &
          Curated Domain-Specific Dataset. &
          Approximately 25,000 question-answer pairs sourced from WebMD, covering a wide range of healthcare topics. &
          Designed for multiple-answer span extraction in healthcare question answering. &
          Truthfulness \\
          \hline
            \href{https://aclanthology.org/2024.findings-emnlp.346.pdf}{SecureSQL} &
          Curated domain-specific dataset &
          Comprises meticulously annotated samples, including both positive and negative instances. The dataset encompasses 57 databases across 34 diverse domains, each associated with specific security conditions. &
          Evaluate and analyze data leakage risks in LLMs, particularly concerning SQL query generation and execution. &
          Privacy \\
          \hline
            \bottomrule
        \end{tabular}
        \label{tab:Datasets3}\end{table*}


\begin{table*}[htbp]
        \centering
        \small
        \renewcommand{\arraystretch}{1.2}
        \setlength{\tabcolsep}{6pt}
        \begin{tabular}{p{2cm} p{2.5cm} p{4cm} p{3.5cm} p{1.5cm}}
            \toprule
            \textbf{Datasets} & \textbf{Data Type} & \textbf{Content} & \textbf{Task} & \textbf{Dimensions} \\
            \midrule
        \href{https://arxiv.org/abs/2304.08247}{Medical Meadow} &
          curated domain-specific dataset &
          It comprises approximately 1.5 million data points across various tasks, including question-answer pairs generated from openly available medical data using models like OpenAI's &
          Designed to enhance large language models (LLMs) for medical applications &
          Privacy \\
          \hline
            \href{https://www.nature.com/articles/s41598-025-86890-3}{Electronic Health Records (EHR) at (KHCC)} &
          Private dataset &
          gpt-3.5-turbo &
          Clinical research, outcome analysis. &
          Privacy \\
          \hline
            \href{https://huggingface.co/datasets/agupte/MedVQA}{MedVQA} &
          Curated domain-specific dataset &
          A collection of medical visual question answering pairs, designed to train and evaluate models that interpret medical images and answer related questions. &
          Visual question answering, medical image understanding. &
          Explainability \\
          \hline
            \href{https://aclanthology.org/2024.bionlp-1.14/}{MedExQA} &
          Curated domain-specific dataset &
          A dataset focused on medical examination questions and answers, intended to aid in the development of AI models for medical exam preparation and assessment. &
          Question answering, educational assessment. &
          Explainability \\
          \hline
            \href{https://medmcqa.github.io}{MedMCQA} &
          Curated domain-specific dataset &
          A multiple-choice question-answering dataset in the medical domain, aimed at training models to handle medical examinations and practice questions. &
          Multiple-choice question answering, medical education. &
          Explainability \\
          \hline
            \href{https://arxiv.org/abs/2502.09156}{TCM Medical Licensing Examination(MLE)} &
          Curated domain-specific dataset &
          A dataset comprising questions and answers from Traditional Chinese Medicine licensing examinations. &
          Educational assessment, question answering. &
          Explainability \\
          \hline
            \href{https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia}{Pneumonia Dataset} &
          Curated domain-specific dataset &
          Medical images (such as chest X-rays) labeled for the presence or absence of pneumonia, used for training diagnostic models. &
          Image classification, disease detection. &
          Explainability \\
          \hline
            \href{https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-montgomery}{Montgomery Dataset} &
          Curated domain-specific dataset &
          Chest X-ray images with manual segmentations of the lung fields, useful for pulmonary research. &
          Image segmentation, tuberculosis detection. &
          Explainability \\
          \hline
            \href{https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-shenzhen}{Shenzhen Dataset} &
          Curated domain-specific dataset &
          Chest X-ray images collected in Shenzhen, China, with annotations for tuberculosis manifestations. &
          Disease classification, image analysis. &
          Explainability \\
          \hline
            \href{https://ieee-dataport.org/open-access/indian-diabetic-retinopathy-image-dataset-idrid}{IDRID Dataset} &
          Curated domain-specific dataset &
          Retinal images with annotations for diabetic retinopathy lesions, intended for retinal image analysis. &
          Image segmentation, disease grading. &
          Explainability \\
          \hline
\href{https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset}{BrainTumor Dataset} &
          Curated domain-specific dataset &
          MRI images of brain tumors with corresponding labels, used for developing diagnostic and segmentation models. &
          Tumor detection, image segmentation. &
          Explainability \\
          \hline
            \bottomrule
        \end{tabular}
        \caption{This table provides a structured comparison of datasets used in studies on trust in LLMs for healthcare. The datasets are categorized by data type (e.g., web-scraped, curated domain-specific, synthetic, real-world, or private datasets), content (e.g., medical literature, patient records, clinical guidelines, QA pairs), task (e.g., clinical decision support, medical question-answering, document summarization, biomedical fact-checking, chatbot training), and dimensions of trustworthiness (e.g., truthfulness, privacy, safety, robustness, fairness, bias, explainability). This comparison highlights how each dataset contributes to the development of trustworthy LLMs in medical AI.}
        \label{tab:Datasets4}\end{table*}


\clearpage
\section{Comparison of Models}
\label{app:gpts}
We systematically gathered and analyzed 78 models relevant to studies on trust in LLMs for healthcare. Table 2 provides a comprehensive summary of the LLMs evaluated in these studies, detailing key aspects such as model name, release year, and the institution responsible for its development. Additionally, it specifies the primary task each model is designed for, including medical question-answering, clinical decision support, and biomedical text summarization. To further assess their reliability, we categorize the models based on the dimensions of trustworthiness they address, such as truthfulness, privacy, safety, robustness, fairness and bias, and explainability. This structured overview offers valuable insights into how different LLMs are designed and evaluated to enhance trust in healthcare AI applications.

\begin{table*}[htbp]
        \centering
        \small
        \renewcommand{\arraystretch}{1.2}
        \setlength{\tabcolsep}{6pt}
        \begin{tabular}{p{2cm} p{1.5cm} p{2cm} p{5cm} p{2cm}}
            \toprule
            \textbf{Models} & \textbf{Release Year} & \textbf{Institution} & \textbf{Primary Task} & \textbf{Dimensions} \\
            \midrule
        \href{https://github.com/allenai/scibert}{SciBERT} &
          2019 &
          Allen Institute for AI &
          Pre-trained language model specialized for scientific text, particularly biomedical and computer science literature. &
          Fairness and Bias \\
          \hline
            \href{https://github.com/conceptofmind/PaLM}{PaLM-2} &
          2023 &
          Google &
          Multilingual language understanding and generation, with a focus on reasoning and coding tasks. &
          Fairness and Bias \\
          \hline
            \href{https://huggingface.co/mistralai/Mixtral-8x7B-v0.1}{Mixtral-8x70B} &
          2023 &
          Mistral AI &
          Ensemble of language models aimed at improving performance across diverse language tasks. &
          Fairness and Bias, Safety \\
          \hline
            \href{https://github.com/kyegomez/Med-PaLM}{Med-PaLM} &
          2023 &
          Google Health &
          Specializing in healthcare-related question answering, clinical diagnosis support, and medical literature interpretation. &
          Fairness and Bias \\
          \hline
            \href{https://github.com/kyegomez/Med-PaLM}{Med-PaLM 2} &
          2024 &
          Google Health &
          Updated version of Med-PaLM, further improving healthcare-related tasks with enhanced accuracy and reliability in medical information retrieval, clinical reasoning, and decision support. &
          Fairness and Bias \\
          \hline
            \href{https://github.com/meta-llama/llama}{Llama-13B} &
          2023 &
          Meta &
          Designed for natural language understanding and generation tasks, such as text summarization, machine translation, and conversational AI. &
          Fairness and Bias \\
          \hline
            \href{https://github.com/zihangdai/xlnet}{XLNet} &
          2019 &
          Google Research &
          It is used for text classification, question answering, and language modeling tasks. &
          Fairness and Bias \\
          \hline
            \href{https://github.com/microsoft/DeBERTa}{DeBERTa} &
          2020 &
          Microsoft Research &
          Improves BERT and RoBERTa by enhancing the attention mechanism. It performs well in a variety of NLP tasks, such as sentence classification, question answering, and named entity recognition. &
          Fairness and Bias \\
          \hline
            \href{https://github.com/meta-llama/llama}{Llama-7B} &
          2023 &
          Meta &
          Focused on general-purpose natural language understanding and generation, with potential fine-tuning for specific domains like medicine, law, and technology. &
          Fairness and Bias, Truthfulness \\
          \hline
            \href{https://huggingface.co/meta-llama/Llama-2-70b-chat-hf}{Llama 2 70Bchat} &
          2023 &
          Meta Platforms &
          Open-source conversational AI model designed for dialogue and instruction-following tasks. &
          Fairness and Bias, Truthfulness, Safety, Robustness, \\
          \hline
            \href{https://github.com/kydycode/chatgpt-3.5-turbo}{GPT-3.5} &
          2022 &
          OpenAI &
          Enhanced language processing capabilities, building upon GPT-3. &
          Fairness and Bias, Truthfulness, Safety, Robustness, Privacy \\
          \hline
            \href{https://github.com/openai/gpt-2}{GPT2} &
          2019 &
          OpenAI &
          Text generation &
          Fairness and Bias, Robustness \\
          \hline
            \href{https://github.com/chaoyi-wu/PMC-LLaMA}{PMC Llama 13B} &
          2023 &
          Allen Institute for AI &
          Specialized in medical literature understanding and generation. &
          Fairness and Bias, Robustness \\
          \hline
            \bottomrule
        \end{tabular}
        \label{tab:Models1}\end{table*}


\begin{table*}[htbp]
        \centering
        \small
        \renewcommand{\arraystretch}{1.2}
        \setlength{\tabcolsep}{6pt}
        \begin{tabular}{p{2cm} p{1.5cm} p{2cm} p{5cm} p{2cm}}
            \toprule
            \textbf{Models} & \textbf{Release Year} & \textbf{Institution} & \textbf{Primary Task} & \textbf{Dimensions} \\
            \midrule
        \href{https://openai.com/index/gpt-4/}{GPT-4} &
          2023 &
          OpenAI &
          Advanced language generation and understanding across various domains. &
          Fairness and Bias, Safety, Robustness, Explainability, Privacy \\
          \hline
            \href{https://huggingface.co/docs/transformers/en/model_doc/bert}{BERT} &
          2018 &
          Google AI Language &
          Pre-trained Transformer model for a wide range of NLP tasks, such as text classification, NER, QA, etc. &
          Fairness and Bias, Safety, Robustness, Truthfulness \\
          \hline
            \href{https://github.com/dataprofessor/llama2}{LLAMA 2 CHAT} &
          2023 &
          Meta AI &
          Language modeling &
          Robustness, Explainability \\
          \hline
            \href{https://huggingface.co/medalpaca/medalpaca-7b}{MEDALPACA (7B)} &
          2023 &
          medalpaca &
          Medical domain language model fine-tuned for question-answering and medical dialogue tasks. &
          Robustness, Privacy \\
          \hline
            \href{https://github.com/bowang-lab/clinical-camel}{CLINICAL CAMEL (13B)} &
          2023 &
          the AI and healthcare community &
          Fine-tuned for clinical applications. It is designed to assist with tasks like medical text classification, clinical decision support, information extraction from medical records, and answering clinical questions. &
          Robustness \\
          \hline
            \href{https://huggingface.co/openai-community/gpt2-xl}{GPT-2 XL} &
          2019 &
          OpenAI &
          Large-scale language model for text generation and understanding. &
          Robustness \\
          \hline
            \href{https://huggingface.co/google-t5/t5-large}{T5-Large} &
          2020 &
          Google Research &
          It treats all NLP tasks as text-to-text tasks, meaning both the input and output are in the form of text, and it's used for tasks like translation, summarization, and question answering. &
          Robustness \\
          \hline
            \href{https://www.anthropic.com/news/claude-3-5-sonnet}{claude-3.5-sonnet} &
          2024 &
          Anthropic &
          It is a variant of Claude, specialized in tasks such as conversational AI, creative writing, poetry generation, and other text-based applications. &
          Robustness \\
          \hline
            \href{https://huggingface.co/aaditya/Llama3-OpenBioLLM-70B}{OpenBioLLM-70B} &
          2024 &
          OpenBioAI &
          It is designed to handle tasks such as biological information extraction, gene sequence analysis, protein folding predictions, and other bioinformatics applications. &
          Robustness \\
          \hline
            \href{https://huggingface.co/BioMistral/BioMistral-7B}{BioMistral-7B} &
          2023 &
          Mistral AI &
          Focused on biomedical and healthcare-related text. Its tasks include medical question answering, clinical document analysis, and medical text summarization. &
          Robustness \\
          \hline
            \href{https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20}{Medllama3-v20} &
          2024 &
          MedAI Labs &
          Designed to assist in healthcare tasks like clinical reasoning, medical question answering, and patient record analysis. &
          Robustness \\
          \hline
            \href{https://huggingface.co/starmpcc/Asclepius-Llama2-7B}{ASCLEPIUS (7B)} &
          2023 &
          Asclepius AI &
          Developed for clinical and medical applications, specializing in tasks like diagnosing medical conditions from symptoms, medical text summarization, and extracting structured information from clinical documents. &
          Robustness, Explainability \\
          \hline
            \href{https://github.com/tatsu-lab/stanford_alpaca}{ALPACA (7B)} &
          2023 &
          Stanford University &
          Fine-tuned version of the LLaMA model aimed at providing high-quality responses to questions, with an emphasis on maintaining ethical and accurate conversational capabilities in diverse domains. &
          Robustness \\
          \hline
            \bottomrule
        \end{tabular}
        \label{tab:Models2}\end{table*}


\begin{table*}[htbp]
        \centering
        \small
        \renewcommand{\arraystretch}{1.2}
        \setlength{\tabcolsep}{6pt}
        \begin{tabular}{p{2cm} p{1.5cm} p{2cm} p{5cm} p{2cm}}
            \toprule
            \textbf{Models} & \textbf{Release Year} & \textbf{Institution} & \textbf{Primary Task} & \textbf{Dimensions} \\
            \midrule
        \href{https://github.com/ra83205/google-bard-api}{Google’s Bard} &
          2023 &
          Google &
          Conversational AI tool, focused on providing detailed, accurate, and creative responses to user queries. It can handle a variety of tasks, including web search, content generation, and complex QA. &
          Robustness \\
          \hline
            \href{https://github.com/gabrielsants/openai-davinci-003}{Text- Davinci-003} &
          2022 &
          OpenAI &
          It is an advanced variant of GPT-3. It is designed for a wide range of natural language understanding and generation tasks, such as answering questions, summarizing text, creative writing, translation, and code generation. &
          Robustness, Truthfulness \\
          \hline
            \href{https://huggingface.co/meta-llama/Llama-2-7b}{LLaMa 2-7B} &
          2023 &
          Meta (formerly Facebook AI Research) &
          Designed to be a general-purpose AI for a wide range of tasks such as text generation, question answering, and summarization, with specific fine-tuning for medical and technical domains. &
          Robustness, Truthfulness, Privacy \\
          \hline
            \href{https://openai.com/index/chatgpt/}{ChatGPT} &
          2022 &
          OpenAI &
          Conversational AI &
          Robustness, Truthfulness, Explainability, Privacy \\
          \hline
            \href{https://github.com/meta-llama/llama3}{Llama-3.1} &
          2024 &
          Meta AI &
          Multilingual large language model designed for a variety of natural language processing tasks. &
          Safety \\
          \hline
            \href{https://huggingface.co/wanglab/ClinicalCamel-70B}{ClinicalCamel-70b} &
          2023 &
          the AI and healthcare community &
          Medical language model designed for clinical research applications. &
          Safety, Explainability \\
          \hline
            \href{https://github.com/m42-health/med42}{Med42-70b} &
          2023 &
          M42 Health &
          Clinical large language model providing high-quality answers to medical questions. &
          Safety, Explainability \\
          \hline
            \href{https://openai.com/index/hello-gpt-4o/}{GPT-4o} &
          2024 &
          OpenAI &
          Multimodal large language model capable of processing and generating text, audio, and images in real time. &
          Safety, Privacy, Explainability \\
          \hline
            \href{https://github.com/mistralai/mistral-inference}{Mistral} &
          2023 &
          Mistral AI &
          Language model optimized for code generation and reasoning tasks. &
          Safety, Robustness, Explainability \\
          \hline
            \href{https://github.com/epfLLM/meditron}{Meditron (7) (70b)} &
          2023 &
          École Polytechnique Fédérale de Lausanne (EPFL) &
          Medical language model fine-tuned for clinical decision support and medical reasoning. &
          Safety, Robustness, Explainability \\
          \hline
            \href{https://www.anthropic.com/news/claude-2-1}{Claude-2.1} &
          2023 &
          Anthropic &
          General-purpose language model for a wide range of natural language understanding and generation tasks. &
          Safety, Robustness \\
          \hline
            \href{https://huggingface.co/docs/transformers/en/model_doc/gptj}{GPT-J} &
          2021 &
          EleutherAI &
          Open-source language model for text generation and understanding. &
          Safety, Robustness \\
          \hline
            \href{https://github.com/eddieali/Vicuna-AI-LLM}{Vicuna} &
          2023 &
          UC Berkeley and Microsoft Research &
          Conversational AI &
          Safety, Robustness, Truthfulness \\
          \hline
            \bottomrule
        \end{tabular}
        \label{tab:Models3}\end{table*}


\begin{table*}[htbp]
        \centering
        \small
        \renewcommand{\arraystretch}{1.2}
        \setlength{\tabcolsep}{6pt}
        \begin{tabular}{p{2cm} p{1.5cm} p{2cm} p{5cm} p{2cm}}
            \toprule
            \textbf{Models} & \textbf{Release Year} & \textbf{Institution} & \textbf{Primary Task} & \textbf{Dimensions} \\
            \midrule
        \href{https://huggingface.co/medalpaca/medalpaca-13b}{Medalpaca-13b} &
          2023 &
          medalpaca &
          Medical domain language model fine-tuned for question-answering and medical dialogue tasks. &
          Safety, Truthfulness, Privacy \\
          \hline
            \href{https://openai.com/index/gpt-3-apps/}{GPT-3} &
          2020 &
          OpenAI &
          Natural language understanding and generation &
          Truthfulness, Explainability \\
          \hline
            \href{https://github.com/google-research/albert}{ALBERT} &
          2019 &
          Google Research &
          Lighter version of BERT that reduces parameters for efficiency while maintaining performance. It excels in tasks such as text classification, named entity recognition, and question answering. &
          Truthfulness \\
          \hline
            \href{https://huggingface.co/docs/transformers/en/model_doc/roberta}{RoBERTa} &
          2019 &
          Facebook AI Research &
          Optimized variant of BERT that removes the Next Sentence Prediction task and trains with more data and for longer periods. It is used for tasks like question answering, sentiment analysis, and text classification. &
          Truthfulness \\
          \hline
            \href{https://github.com/ncbi-nlp/bluebert}{BlueBERT} &
          2019 &
          NIH and Stanford University &
          BERT-based model pre-trained on clinical and biomedical text. It is designed for healthcare-related tasks, including clinical text classification, named entity recognition, and medical question answering. &
          Truthfulness \\
          \hline
            \href{https://github.com/kexinhuang12345/clinicalBERT}{ClinicalBERT} &
          2019 &
          University of Pennsylvania &
          Variant of BERT fine-tuned on clinical texts, tailored for clinical NLP tasks like named entity recognition, clinical event extraction, and question answering in the medical domain. &
          Truthfulness \\
          \hline
            \href{https://github.com/google-research/tapas}{TAPAS} &
          2020 &
          Google Research &
          Designed for answering questions based on tabular data. It is used for tasks like extracting structured information from tables and processing queries in tabular datasets. &
          Truthfulness \\
          \hline
            \href{https://huggingface.co/meta-llama/Llama-2-13b}{LLaMA-2 13B} &
          2023 &
          Meta &
          Advanced variant of Meta's LLaMA series, designed for text generation, question answering, summarization, and other NLP tasks. &
          Truthfulness, Explainability, Privacy \\
          \hline
            \href{https://github.com/mosaicml/llm-foundry}{MPT} &
          2023 &
          MosaicML &
          General-purpose LLM for text generation, summarization, language understanding, and reasoning tasks. Fine-tuned for downstream applications such as chatbot development, code generation, and other NLP tasks. &
          Truthfulness \\
          \hline
            \href{https://huggingface.co/docs/transformers/main/model_doc/blip-2}{BLIP2} &
          2023 &
          Salesforce &
          Bootstrapping language-image pre-training, designed to bridge vision-language models with large language models for improved visual understanding and generation. &
          Truthfulness \\
          \hline
            \href{https://huggingface.co/Salesforce/instructblip-vicuna-7b?utm_source=chatgpt.com}{InstructBLIP-7b/13b} &
          2023 &
          Salesforce &
          Visual instruction-tuned versions of BLIP-2, utilizing Vicuna-7B and Vicuna-13B language models, respectively, to enhance vision-language understanding through instruction tuning. &
          Truthfulness \\
          \hline
            \href{https://github.com/haotian-liu/LLaVA}{LLaVA1.5-7b/13b} &
          2023 &
          Microsoft &
          Large language and vision assistant models with 7B and 13B parameters, respectively, designed for multimodal tasks by integrating visual information into language models. &
          Truthfulness \\
          \hline
            \href{https://github.com/X-PLUG/mPLUG-Owl?tab=readme-ov-file}{mPLUGOwl2} &
          2023 &
          Zhejiang University &
          Multimodal pre-trained language model designed to handle various vision-language tasks, including image captioning and visual question answering. &
          Truthfulness \\
          \hline
            \bottomrule
        \end{tabular}
        \label{tab:Models4}\end{table*}


\begin{table*}[htbp]
        \centering
        \small
        \renewcommand{\arraystretch}{1.2}
        \setlength{\tabcolsep}{6pt}
        \begin{tabular}{p{2cm} p{1.5cm} p{2cm} p{5cm} p{2cm}}
            \toprule
            \textbf{Models} & \textbf{Release Year} & \textbf{Institution} & \textbf{Primary Task} & \textbf{Dimensions} \\
            \midrule
        \href{https://github.com/mbzuai-oryx/XrayGPT}{XrayGPT} &
          2023 &
          University of Toronto &
          Specialized model for generating radiology reports from chest X-ray images, aiming to assist in medical image interpretation. &
          Truthfulness \\
          \hline
            \href{https://github.com/Vision-CAIR/MiniGPT-4}{MiniGPT4} &
          2023 &
          King Abdullah University of Science and Technology &
          A lightweight multimodal model designed to align vision and language models efficiently, facilitating tasks like image captioning and visual question answering. &
          Truthfulness \\
          \hline
            \href{https://github.com/chaoyi-wu/RadFM}{RadFM} &
          2023 &
          Stanford University &
          Foundation model tailored for radiology, focusing on interpreting medical images and integrating findings with clinical language models. &
          Truthfulness \\
          \hline
            \href{https://github.com/tloen/alpaca-lora}{Alpaca-LoRA} &
          2023 &
          Stanford University &
          It focuses on achieving good performance in tasks such as question answering and personalized dialogue. &
          Truthfulness \\
          \hline
            \href{https://github.com/Integral-Healthcare/robin-ai-reviewer}{Robin- medical} &
          2023 &
          Robin Health &
          Fine-tuned for medical applications, including clinical decision support, medical question answering, and health record analysis. &
          Truthfulness \\
          \hline
            \href{https://huggingface.co/docs/transformers/en/model_doc/flan-t5}{Flan-T5} &
          2021 &
          Google Research &
          Optimized for tasks like question answering, text summarization, and sentence classification, across a variety of domains. &
          Truthfulness \\
          \hline
            \href{https://github.com/dmis-lab/biobert?tab=readme-ov-file}{BioBERT} &
          2019 &
          Korea University &
          Biomedical language representation learning, enhancing performance on tasks like named entity recognition, relation extraction, and question answering within the biomedical domain. &
          Truthfulness \\
          \hline
            \href{https://github.com/falconry/falcon}{Falcon Instruct (7B and 40B)} &
          2023 &
          Technology Innovation Institute (TII), UAE. &
          Instruction-tuned language model designed to follow user instructions effectively. &
          Truthfulness, Robustness \\
          \hline
            \href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}{Mistral Instruct (7B)} &
          2023 &
          Mistral AI &
          Instruction-tuned language model designed to follow user instructions effectively. &
          Truthfulness, Robustness \\
          \hline
            \href{https://github.com/falconry/falcon}{Falcon} &
          2023 &
          Technology Innovation Institute (TII), UAE. &
          General-purpose language model optimized for text understanding, generation, question answering, and reasoning tasks. Focused on efficient deployment for industry-scale applications. &
          Truthfulness, Robustness \\
          \hline
            \href{https://github.com/microsoft/LLaVA-Med?utm_source=chatgpt.com}{LLaVA-Med} &
          2024 &
          Microsoft &
          Large language and vision assistant for biomedicine, trained to handle visual instruction tasks in the biomedical field, aiming for capabilities similar to GPT-4. &
          Truthfulness, Explainability \\
          \hline
            \href{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}{GPT-4o-mini} &
          2024 &
          OpenAI &
          Natural language processing (NLP), text generation, and understanding. &
          Explainability \\
          \hline
            \href{https://huggingface.co/starmpcc/Asclepius-13B}{ASCLEPIUS (13B)} &
          2023 &
          Asclepius AI &
          Medical NLP, clinical text analysis, and healthcare-related tasks. &
          Explainability \\
          \hline
            \bottomrule
        \end{tabular}
        \label{tab:Models5}\end{table*}


\begin{table*}[htbp]
        \centering
        \small
        \renewcommand{\arraystretch}{1.2}
        \setlength{\tabcolsep}{6pt}
        \begin{tabular}{p{2cm} p{1.5cm} p{2cm} p{5cm} p{2cm}}
            \toprule
            \textbf{Models} & \textbf{Release Year} & \textbf{Institution} & \textbf{Primary Task} & \textbf{Dimensions} \\
            \midrule
        \href{https://github.com/MedHK23/MedViLaM}{MedViLaM} &
          2023 &
          \citet{xu2024medvilam} &
          Medical vision-language tasks, combining image and text analysis for healthcare. &
          Explainability \\
          \hline
            \href{https://github.com/jiangsongtao/Med-MoE}{Med-MoE} &
          2023 &
          \citet{jiang2024med} &
          Medical NLP, leveraging Mixture of Experts (MoE) for specialized healthcare tasks. &
          Explainability \\
          \hline
            \href{https://deepmind.google/technologies/gemini/pro/}{Gemini Pro} &
          2023 &
          Google DeepMind &
          Multi-modal NLP, combining text, image, and other data types for advanced AI tasks &
          Explainability \\
          \hline
            \href{https://github.com/XZhang97666/AlpaCare}{AlpaCare (7B) (13B)} &
          2023 &
          \citet{zhang2023alpacare} &
          Healthcare-focused NLP, clinical text analysis, and medical decision support &
          Explainability \\
          \hline
            \href{https://huggingface.co/01-ai/Yi-6B}{Yi (6B)} &
          2023 &
          01.AI (China) &
          General-purpose NLP, text generation, and fine-tuning for specific applications. &
          Explainability \\
          \hline
            \href{https://huggingface.co/microsoft/phi-2}{Phi-2 (2.7B)} &
          2023 &
          Microsoft &
          Lightweight NLP, text generation, and fine-tuning for specific tasks. &
          Explainability \\
          \hline
            \href{https://huggingface.co/upstage/SOLAR-10.7B-v1.0}{SOLAR (10.7B)} &
          2023 &
          Upstage AI &
          General-purpose NLP, text generation, and fine-tuning for specific domains. &
          Explainability \\
          \hline
            \href{https://github.com/InternLM/InternLM}{InternLM2 (7B)} &
          2023 &
          Shanghai AI Laboratory (China) &
          General-purpose NLP, text generation, and fine-tuning for specific applications. &
          Explainability \\
          \hline
            \href{https://ai.meta.com/blog/meta-llama-3/}{Llama3-( 8B and 70B)} &
          2024 &
          Meta &
          General-purpose NLP, text generation, and fine-tuning for specific applications. &
          Privacy \\
          \hline
            \href{https://huggingface.co/codellama}{CodeLlama-( 7B, 13B, and 34B)} &
          2023 &
          Meta &
          Code generation, code completion, and programming assistance. &
          Privacy \\
          \hline
            \href{https://mistral.ai/en/news/mixtral-8x22b}{Mixtral-8x7B and 8x22B} &
          2023 &
          Mistral AI &
          General-purpose NLP, text generation, and fine-tuning for specific domains. &
          Privacy \\
          \hline
            \href{https://huggingface.co/Qwen/Qwen1.5-72B-Chat}{Qwen-(7B, 14B, 32B, 72B)-Chat} &
          2023 &
          Alibaba &
          Chat-oriented NLP, conversational AI, and text generation. &
          Privacy \\
          \hline
            \href{https://open.bigmodel.cn/dev/api/normal-model/glm-4}{GLM-4} &
          2024 &
          Tsinghua University &
          Advanced NLP, text generation, and multi-modal tasks. &
          Privacy \\
          \hline
            \bottomrule
        \end{tabular}
                \caption{Detailed Comparison of GPT Models Evaluated for Trust in Healthcare LLMs, Including Model Name, Release Year, Institution, Primary Tasks (e.g., Medical Question-Answering, Clinical Decision Support, Biomedical Text Summarization, Medical Report Generation), and Key Trustworthiness Dimensions (Truthfulness, Privacy, Safety, Robustness, Fairness and Bias, Explainability).}
            \label{tab:Models6}\end{table*}



%\end{landscape}

\end{document}
