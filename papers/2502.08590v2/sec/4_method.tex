\section{Light-A-Video}
\label{sec:method}


% In this section, we aim to take advantage of existing state-of-the-art image relighting models (IC-Light~\cite{iclight})
% to achieve a consistent appearance of video relighting results in a training-free manner.

Section~\ref{sec:PF} defines the objectives of the video relighting task. Section~\ref{sec:CLA} reveals that per-frame image relighting for video sequences suffers from lighting source inconsistency
and accordingly proposes the Consistent Lighting Attention \textbf{(CLA)} module for enhanced stability in generated lighting source across frames. Section~\ref{sec:PLF} represents the Progressive Light Fusion \textbf{(PLF)} strategy for temporally consistent video
appearance generation.
\subsection{Problem Formulation}
\label{sec:PF}
Given a source video and a lighting condition $c$, 
the objective of video relighting is to
render the source video into the relighted video that maintains the motion in the source video while aligning the lighting condition $c$.
Unlike image relighting that solely concentrates on appearance, video relighting raises extra challenges in
maintaining temporal consistency and motion preservation, necessitating high-quality visual coherence across frames.
\subsection{Consistent Light Attention}
\label{sec:CLA}
Given the achievement in image relighting model~\cite{zhang2025scaling}, a straightforward approach for video relighting is to
directly perform frame-by-frame image relighting under the same lighting condition. 
However, as illustrated in Fig.~\ref{fig:cla}, this naive method fails to maintain appearance coherence across frames,
resulting in frequent flickering of the generated light source
and inconsistent temporal illumination.

To improve inter-frame information integration and generate a stable light source,
we propose a Consistent Light Attention \textbf{(CLA)} module.
Specifically, for each self-attention layer in the IC-Light model, 
a video feature map $\mathbf{h} \in \mathbb{R}^{(b \times f) \times (h \times w) \times d}$ serves as 
the input, where $b$ is the batch size and $f$ is the number of video frames, $h$ and $w$ denote the height and width of the feature map, 
with $h \times w$ representing the number of tokens for attention computation.
With linearly projections, $\mathbf{h}$ is projected into query, 
key and value features $Q, K, V \in \mathbb{R}^{ (b \times f) \times (h \times w) \times d}$.
The attention computation is defined as follows:
\begin{equation}
\operatorname{Self-Attn}(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^T}{\sqrt{d}}\right) V.
\end{equation}
Note that the naive method treats the frame dimension as the batch size, 
performing self-attention frame by frame with the image relighting model, 
which results in each frame attending only to its features.
For the CLA module, as shown in Fig.~\ref{fig:pipe},
a dual-stream attention fusion strategy is applied.
Given the input feature $\mathbf{h}$, the original stream directly feeds the feature map into the attention module to 
compute frame-by-frame attention, resulting in the output $\mathbf{h}^{\prime}_1$.
The average stream first reshapes $\mathbf{h}$ into $ \mathbb{R}^{b \times f \times (h \times w) \times d}$, averages
it along the temporal dimension, then expands it $f$ times to obtain $\mathbf{\bar{h}}$.
Specifically, the average stream mitigates high-frequency temporal fluctuations,
thereby facilitating the generation of a stable background light source across frames.
Meanwhile, the original stream
retains the original high-frequency details, 
thereby compensating for the detail loss incurred by the averaging process.
Then,
$\mathbf{\bar{h}}$ is input into the self-attention module and the output is $\mathbf{\bar{h}}^{\prime}_2$.
The final output $\mathbf{h}_o^{\prime}$ of the CLA module is a weighted average between two streams, with the trade-off parameter $\gamma$,
\begin{equation}\label{eq:attn_fusion}
\mathbf{h}_o^{\prime} = (1-\gamma) \mathbf{h}^{\prime}_1 + \gamma \mathbf{\bar{h}}^{\prime}_2.
\end{equation}
With the help of CLA, 
the result can capture global context across the entire video
and generate a more stable lighting source, as shown in Fig.~\ref{fig:cla}.

\subsection{Progressive Light Fusion}
\label{sec:PLF}

CLA module improves cross-frame consistency but lacks pixel-level constraints, 
leading to inconsistencies in appearance details.
To address this, we leverage motion priors in the Video Diffusion Model (VDM), 
which are trained on large-scale video datasets and use a temporal attention module
to ensure consistent motion and lighting changes.
The novelty of our Light-A-Video lies in 
progressively injecting the relighting results
as guidance into the denoising process.

In the pipeline as shown in Fig~\ref{fig:pipe},
a source video is first encoded into latent space, 
and then add $T_m$ step noise to acquire the noisy latent $\mathbf{z}_m$.
At each denoising step $t$,
the noise-free component $\hat{\mathbf{z}}_{0 \gets t}$ 
in Eq.~\ref{eq: z0_pred} is predicted, which serves as the denoising target for the current step.
Prior work demonstrated the potential of applying tailored manipulation in denoising targets for guided generation, 
with significant achievements observed in high-resolution image synthesis~\cite{kim2024diffusehigh} 
and text-based image editing~\cite{rout2024rfinversion}.

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{figs/plf.pdf}
\vspace{-0.5em}
\caption{{\bf Visualization of the PLF strategy}. 
During the denoising process of the VDM, the PLF strategy progressively
replaces the original Consistent Target $\mathbf{z}^{v}_{0 \gets t}$ with the Fusion Target $\tilde{\mathbf{z}}_{0 \gets t}$,
guiding the denoising direction from  $ \mathbf{v}_t$ to $\tilde{\mathbf{v}}_t$.
}
\label{fig:plf}
\vspace{-0.5em}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figs/detail.pdf}
\vspace{-1.9em}
\caption{{\bf Visualization of the detail compensation}. 
$\Delta d_m$ records the difference between $\hat{\mathbf{z}}_{0 \gets m}$ and the source video in the first denoising step, which is used
as a detail compensation component for detail preservation in the consistent target.
% ensuring detail consistency between the relighted video and the source video at each step.
}
\label{fig:detail}
\vspace{-1.5em}
\end{figure}


Driven by the motion priors in the VDM, the denoising process encourage 
$\hat{\mathbf{z}}_{0 \gets t}$ to be temporally consistent.
Thus, we define this target as the video \textbf{Consistent Target $\mathbf{z}^{v}_{0 \gets t}$} with environment illumination $L^{v}_t$.
However, discrepancies still exist between the predicted $\hat{\mathbf{z}}_{0 \gets t}$ and the original video,
resulting in detail loss in the relighted video.
To address this issue, as shown in Fig.~\ref{fig:detail},
details compensation $\Delta d_m$ is incorporated into the $\mathbf{z}^{v}_{0 \gets t}$ at each step.
Then, $\mathbf{z}^{v}_{0 \gets t}$ is sent into the CLA module to
obtain the relighted latent, which serves 
as the \textbf{Relight Target $\mathbf{z}^{r}_{0 \gets t}$} with the illumination $L^{r}_t$ for the $t$-th denoising step.
Aligning with the light transport theory in Section~\ref{sec:lightTrans},
a pre-trained VAE $\{ {\mathcal{E}(\cdot)}, {\mathcal{D}(\cdot)} \}$ is used to
decode the two targets into pixel level,
yielding the image appearances 
$\mathbf{I}^{v}_{t} = \mathcal{D}(\mathbf{z}^{v}_{0 \gets t})$ 
and $ \mathbf{I}^{r}_{t} = \mathcal{D}(\mathbf{z}^{r}_{0 \gets t})$, respectively.
Refer to Eq.~\ref{eq:light_fusion},
the fusing appearance $\mathbf{I}^{f}_{t}$ can be formulated as:
\begin{equation} \label{eq:target_fusion}
   \mathbf{I}^{f}_{t} =  \mathbf{T}(L^{v}_t +L^{r}_t) = \mathbf{I}^{v}_{t} + \mathbf{I}^{r}_{t}.
\end{equation}
It is observed that directly using encoded latent $\mathcal{E}(\mathbf{I}^{f}_{t})$
as the new target at each step results in suboptimal performance.
This is attributed to the excessively large gap between the two targets, 
which exceeds the refinement capability of the VDM and 
consequently causes visible temporal lighting jitter.
To mitigate this gap, a progressive lighting fusion strategy is proposed.
Specifically, a fusion weight $\lambda_t$ is introduced, which 
decreases as denoising progresses, thereby 
gradually reducing the influence of the relight target.
The progressive light fusion appearance is defined as $\mathbf{I}^{p}_{t}$, i.e.,
\begin{equation} \label{eq:pro_target_fusion}
   \mathbf{I}^{p}_{t} = \mathbf{I}^{v}_{t} + \lambda_t (\mathbf{I}^{r}_{t}-\mathbf{I}^{v}_{t}).
\end{equation}
The encoded latent $ \tilde{\mathbf{z}}_{0 \gets t} = \mathcal{E}(\mathbf{I}^{p}_{t})$ is utilized as the \textbf{Fusion Target} for step $t$,
replacing the original $\mathbf{z}^{v}_{0 \gets t}$.
Based on the fusion target, the less noisy latent $\mathbf{z}_{t-1}$ can be computed with DDIM scheduler with $v$-prediction:
\begin{equation}
    a_t=\sqrt{\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}},\\
    b_t=\sqrt{\bar{\alpha}_{t-1}}-\sqrt{\bar{\alpha}_t}  a_t,
\end{equation}
\begin{equation}
\mathbf{z}_{t-1}=a_t \mathbf{z}_t+b_t  \tilde{\mathbf{z}}_{0 \gets t}.
\end{equation}
From Eq.~\ref{eq: z0_pred}, the fusion target $ \tilde{\mathbf{z}}_{0 \gets t} $ determines a new denoising direction,
denoted as $\tilde{\mathbf{v}}_t$,
\begin{equation}
\tilde{\mathbf{v}}_t=\frac{\sqrt{\bar{\alpha}_t} \mathbf{z}_t-\tilde{\mathbf{z}}_{0 \gets t}}{\sqrt{1-\bar{\alpha}_t}},
\end{equation}
which means PLF essentially refines $\mathbf{v}_t$ iteratively and
guides the denoising process towards the relighting direction, as shown in Fig~\ref{fig:plf}.
Other schedulers capable of modeling the denoising direction, such as Euler Scheduler~\cite{karras2022elucidating} and Rectified Flow~\cite{lipman2022flow}, are also applicable.
As the denoising progresses, smooth and consistent illumination injection is achieved, ensuring coherent video relighting.