\vspace{-0.5em}
\section{Experiments}
\label{sec:experiment}

\begin{figure*}[htp]
\centering
\includegraphics[width=1.0\linewidth]{figs/lav_comparison.pdf}
\vspace{-2em}
\caption{\textbf{Qualitative comparison of baseline methods}.
Given a source video and guidance text prompt, 
Light-A-Video achieves high temporal consistency and fidelity to the light condition, 
outperforming other methods in avoiding flickering, jitter, and identity shifts.
VDM used: AnimateDiff (Left), CogVideoX (Right).
}
\vspace{-0.5em}
\label{fig:comparison}
\end{figure*}

\input{./tabs/tab_1}

\subsection{Experimental Details}
\label{sec:details}

\noindent \textbf{Baselines.} 
Given the lack of established video relighting methods, we adopt the state-of-the-art image 
relighting technique to perform frame-by-frame relighting on videos as a baseline. 
To verify the temporal smoothing effect of illumination using a VDM,
we construct two comparative methods by applying SDEdit~\citep{meng2021sdedit} to the per-frame results of IC-Light~\citep{zhang2025scaling}.
These two methods are named IC-Light + SDEdit-0.2 and IC-Light + SDEdit-0.6, 
corresponding to noise levels of 20\% and 60\%.
Finally, IC-Light + AnyV2V~\citep{ku2024anyv2v} serves as another baseline. 
Specifically, IC-Light relights the first frame, and AnyV2V propagates the appearance information
from the first frame to all subsequent frames, preserving the content of the source video.

\noindent \textbf{Evaluation metrics.} Three widely adopted metrics are reported for quantitative evaluation.
First, the temporal consistency of the generated video is assessed using the
average CLIP~\citep{radford2021learning} score across consecutive video frames. 
Second, the optical flow for each baseline video is estimated using RAFT~\citep{teed2020raft},
and the motion preservation score of each method is assessed by calculating the optical flow discrepancy with the source video.
Third, to evaluate the quality of relighted image, a video test dataset is collected.
The FID~\cite{Seitzer2020FID} score is then calculated between the results of each method
and the frame-by-frame IC-Light results, serving as the metric for relight quality evaluation.
Finally, 52 volunteers are invited to conduct a user study across three aspects: 
\textbf{L}ighting \textbf{P}rompt \textbf{A}lignment (alignment between video content and lighting prompt), 
\textbf{V}ideo \textbf{S}moothness (temporal consistency of the relighted video), and \textbf{I}D-\textbf{P}reservation 
(consistency of the objectâ€™s identity and albedo
before and after relighting). 
The volunteers rank the results of five methods,
and the average user ranking is used as a preference metric.

\noindent \textbf{Datasets.} 
We constructed a video test dataset consisting of 73 videos.
The majority of these videos are selected from the DAVIS~\citep{pont20172017} public dataset, 
which contains a diverse collection of semantically rich videos with pronounced motion.
Additionally, some videos are collected from Pixabay~\cite{pixabay}, 
featuring high-quality videos with significant motion.
All quantitative metrics are evaluated on our collected dataset.
For each video, two lighting prompts are applied, and three lighting directions are randomly chosen. 

\noindent \textbf{Implementation details.} Unless otherwise specified,
the default models employed for image relighting and 
VDM in the subsequent experiments are IC-Light~\citep{zhang2025scaling} 
and AnimateDiff~\citep{guo2023animatediff} Motion-Adapter-v3, respectively.
In the IC-Light model, the lighting conditions $c$ for image relighting are derived from two components:
First, a text prompt that describes the characteristics of the light source (e.g., neon light, sunshine, etc.).
Second, a lighting map is utilized to represent the light intensity across the scene.
This lighting map is then encoded by a VAE and serves as the initial latent for the denoising process.
During the inference stage, the source video is added with 50\% noise. 
Subsequently, the VDM employs a denoising process with $T_m = 25$ steps to 
progressively fuse the relight target.
For the parameters in the pipeline,
$\gamma=0.5$ in the CLA module is used to balance the original attention feature
and the cross-frame averaged feature.
In the PLF strategy, the fusion weight $\lambda_t$ decreases as denoising progresses,
and we set $\lambda_t = 1 - t/T_m$. 
\vspace{-0.5em}
\subsection{Qualitative Results}
\label{sec:QR}
As depicted in Fig.~\ref{fig:comparison},
the frame-by-frame IC-Light method ensures high single-frame quality.
However, the lack of consistency design and VDM temporal priors leads to
significant flickering of the light source and overall appearance. 
By introducing VDM priors, IC-Light + SDEdit-0.2 maintains content consistent with the source video, 
but still exhibits noticeable relight appearance jitter. 
IC-Light + SDEdit-0.6 further enhances temporal smoothness, 
yet object identity shifts occur.  
AnyV2V transfers the appearance of the first relight
frame to subsequent frames, 
but this pixel-level migration method, 
lacking perception of the given light source,
results in unreasonable illumination changes.
In contrast, Light-A-Video achieves high-quality video relighting, 
demonstrating strong temporal consistency and high fidelity to the light source.

\subsection{Video Relighting with Background Generation}
\label{sec:APP}
\vspace{-0.5em}
As depicted in Fig.~\ref{fig:inpaint}, Light-A-Video can accept a video foreground sequence and
a user-provided text prompt as input, generating a corresponding video background 
and illumination that aligns with the prompt descriptions.
Specifically, the input foreground sequence is initially processed
with IC-Light for frame-by-frame relighting,
while the background is entirely noised to serve as the initialization latent for the VDM.
From step $T$ to $T_m$, the background is progressively denoised,
leveraging the VDM's inpainting capability to generate the background.
Subsequently, from step $T_m$ to 0, the CLA module and PLF strategy are employed to achieve a temporally consistent  
relighting appearance of the video. 
These results illustrate that our pipeline
can produce high-quality video relighting results with consistent background generation. \footnote{More examples and ablation experiments are provided in the supplementary material.}

% \footnote{More examples and ablation experiments are provided in the supplementary material.}

\begin{figure}[htp]
\centering
\includegraphics[width=\linewidth]{figs/inpaint.pdf}
\vspace{-2.5em}
\caption{{\bf Text-conditioned video illumination modifying with background generation}. 
Given a video foreground sequence and a text description of the target illumination, our method synthesizes suitable backgrounds and harmonious illumination.
}
\label{fig:inpaint}
\vspace{-1.5em}
\end{figure}

\subsection{Quantitative Evaluation}
\label{sec:QE}
The quantitative comparison of our method with various baselines is presented in Tab.~\ref{tab:quant}.
Given the addition of only 20\% noise, IC-Light + SDEdit-0.2 exhibits performance in video relighting 
that is nearly identical to IC-Light, resulting in significant temporal flickering in both methods. 
IC-Light + SDEdit-0.6 provides enhanced temporal consistency but suffers from object identity shifts due to the introduction of excessive noise.
For AnyV2V, the appearance of the first frame aligns well with the IC-Light results.
However, its inability to perceive the light source, 
combined with inherent quality degradation in subsequent frames, 
leads to a low motion preservation score. 
In contrast, Light-A-Video achieves a low FID score while maintaining high temporal consistency,
demonstrating its effectiveness in both relighted image quality and temporal stability.

\begin{figure}[htp]
\centering
\includegraphics[width=\linewidth]{figs/ablation.pdf}
\vspace{-2.5em}
\caption{{\bf Ablation Study}. 
Results of video relighting with the CLA module or the PLF strategy removed.
}
\label{fig:ablation}
\vspace{-1.5em}
\end{figure}

\vspace{-0.5em}
\subsection{Ablation Study}
\label{sec:abla}
An ablation study is conducted to assess the importance of our CLA and PLF modules.
As illustrated in Fig.~\ref{fig:ablation}, for the video relighting task involving background generation,
frame-by-frame IC-Light provides high-quality single-frame relighting but lacks control over temporal consistency.
This results in inconsistencies in lighting sources and relighted appearances across frames.
The CLA module enables cross-frame information exchange, which stabilizes the generation of background lighting sources.
Additionally, by introducing VDM motion priors and employing PLF's strategy for progressive fusion of relight targets 
into the original denoising target,
Light-A-Video ensures temporally smooth relighting.
The overall video quality is also significantly improved with the aid of VDM priors.

\vspace{-0.5em}
\subsection{Limitation and Future Work}
\label{sec:limitation}
Despite the impressive results achieved by our training-free method,
its performance is inherently constrained by the capabilities of the underlying image-relighting model and the VDM. 
While Light-A-Video demonstrates remarkable proficiency in ensuring stable lighting and temporal consistency, 
the CLA module, which is designed to stabilize background lighting,
exhibits limitations when it comes to modeling dynamic lighting changes.
To address this limitation, future work will focus on
developing novel methods that can more effectively handle dynamic lighting conditions. 