\vspace{-0.5em}
\section{Preliminary}
\label{sec:preliminary}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{figs/pipeline.pdf}
\vspace{-1.0em}
\caption{{\bf The pipeline of Light-A-Video}. A source video is first noised and processed through the VDM for denoising across $T_m$
steps. At each step, the predicted noise-free component with details compensation serves as the Consistent Target $\mathbf{z}^{v}_{0 \gets t}$, 
inherently representing the VDM's denoising direction. 
Consistent Light Attention infuses $\mathbf{z}^{v}_{0 \gets t}$ with unique lighting information,
transforming it into the Relight Target $\mathbf{z}^{r}_{0 \gets t}$.
The Progressive Light Fusion strategy then merges two targets to form the Fusion Target $\tilde{\mathbf{z}}_{0 \gets t}$, 
which provides a refined direction for the current step.The bottom-right part illustrates the iterative evolution of $\mathbf{z}^{v}_{0 \gets t}$.}
\label{fig:pipe}
\vspace{-1.2em}
\end{figure*}



\subsection{Diffusion Model}
\label{sec:dm}
Given an image $\mathbf{x}_0$ that follows the real-world data distribution, we first 
encode $\mathbf{x}_0$ into latent space $\mathbf{z}_0 = \mathcal{E}(\mathbf{x}_0)$
using a pretrained autoencoder $\{ {\mathcal{E}(\cdot)},{\mathcal{D}(\cdot)} \}$.
The forward diffusion process is a $T$ steps Markov chain~\cite{ho2020denoising},
corresponding to the iterative introduction of Gaussian noise $\epsilon$, which can be expressed as:
\begin{equation}
    \mathbf{z}_t =  \sqrt{1-\beta _t}\mathbf{z}_{t-1} + \sqrt{\beta _t} \epsilon, 
\end{equation}
where $\beta _t \in (0,1)$ determines the amount of Gaussian noise introduced at time step $t$. Mathematically, the above cumulative noise adding has the following closed-form:
\begin{equation}
    \mathbf{z}_t = \sqrt{\bar{\alpha}_t}  \mathbf{z}_{0} + \sqrt{1-\bar{\alpha}_t}\epsilon, 
\end{equation}
where $\bar{\alpha}_t =  {\textstyle \prod_{1}^{t}} (1-\beta _t)$. 

For numerical stability, $\mathbf{v}$-prediction~\cite{salimans2022progressive} approach is employed, 
where the diffusion model outputs a predicted velocity $\mathbf{v}$ to represent the denoising direction.
Defined as:
\begin{equation}\label{eq: v_pred}
    \mathbf{v}=\sqrt{\bar{\alpha}_t} \epsilon-\sqrt{1-\bar{\alpha}_t} \mathbf{z}_0.
\end{equation}
During inference, the noise-free component $\hat{\mathbf{z}}_{0 \gets t}$ can be recovered from the model's output $\mathbf{v}_t$ as follows:
\begin{equation}\label{eq: z0_pred}
    \hat{\mathbf{z}}_{0 \gets t}=\sqrt{\bar{\alpha}_t} \mathbf{z}_t-\sqrt{1-\bar{\alpha}_t} \mathbf{v}_t.
\end{equation}
$ \hat{\mathbf{z}}_{0 \gets t}$ represents the denoising target at time step $t$.
% based on $\mathbf{v}_t$.


\subsection{ Light Transport}
\label{sec:lightTrans}
Light transport theory~\cite{debevec2000acquiring,zhang2025scaling} demonstrates that arbitrary image appearance
$\mathbf{I}_L$ can be decomposed by the product of a light transport matrix $\mathbf{T}$  and environment illumination $L$, which can be expressed as:
\begin{equation}\label{eq:decomposition}
    \mathbf{I}_L = \mathbf{T}L,
\end{equation}
where $\mathbf{T}$ is a single matrix for linear light transform~\cite{debevec2000acquiring} and 
$L$ denotes variable environment illumination.
Given the linearity of $\mathbf{T}$, the merging between environment illumination $L$ 
is equal to the fusion of image appearance $\mathbf{I}_L$, i.e., 
\begin{equation}\label{eq:light_fusion}
    \mathbf{I}_{L_1 + L_2} =  \mathbf{T}(L_1 +L_2) = \mathbf{I}_{L_1} + \mathbf{I}_{L_2}.
\end{equation}
Such characteristic suggests the feasibility of lighting control by indirectly constraining image appearance, 
i.e., the consistent image light constraint in IC-Light~\citep{zhang2025scaling}. 


\label{sec:iclight}