\section{Introduction}
\label{sec:intro}
Illumination plays a crucial role in shaping our perception of visual content,
impacting both its aesthetic quality and human interpretation of scenes. 
Relighting tasks~\citep{sun2019single, nestmeyer2020learning, pandey2021total, zhou2019deep, sengupta2021light, hou2021towards, wang2023sunstage, zhou2023relightable, kocsis2024lightit}, which focus on adjusting lighting conditions in 2D and 3D visual content,
have long been a key area of research in computer graphics due to their broad practical applications,
such as film production, gaming, and virtual environments.
% Traditional image relighting methods rely on physical illumination models, 
% which struggle with accurately estimating real-world lighting and materials.
Traditional image relighting methods, which rely on physical illumination models, are typically confined to controlled laboratory settings and struggle to generalize to complex, unconstrained real-world lighting and material estimation.

In order to address these limitations, data-driven approaches~\citep{deng2025flashtex, ren2024relightful, kim2024switchlight, zhang2024lumisculpt, zeng2024dilightnet, jin2024neural} have emerged, 
leveraging large-scale, diverse relighting datasets combined with pre-trained diffusion models. 
As the state-of-the-art image relighting model, IC-Light~\citep{zhang2025scaling} modifies only the illumination of an image 
while maintaining its albedo unchanged.
Based on the physical principle of light transport independence, 
IC-Light allows for controllable and stable illumination editing, 
such as adjusting lighting effects and simulating complex lighting scenarios.
However, video relighting is significantly more challenging
as it needs to maintain temporal consistency across frames. 
The scarcity of video lighting datasets and the high training costs further complicate the task. 
Thus, existing video relighting methods~\citep{zhang2024lumisculpt}
struggle to deliver consistently high-quality results or
are limited to specific domains, such as portraits.



\begin{figure}
\centering
\includegraphics[width=\linewidth]{figs/CLA.pdf}
\vspace{-2em}
\caption{{\bf Relighted frames of vanilla IC-Light and ``IC-Light + CLA" }. 
The line chart depicts the average optical flow intensity between adjacent frames. Since IC-Light performs image relighting based on each independent frame, its results show a noticeable jitter between frames, especially in the generated background lighting. Conversely, the proposed CLA facilitates consistent lighting generation by forcing interaction between frames.
}
\label{fig:cla}
\vspace{-1.5em}
\end{figure}
In this work, we propose a training-free approach for video relighting,
named \textbf{Light-A-Video}, which enables the generation of smooth, high-quality relighted videos
without any additional training or optimization. 
As shown in Fig.~\ref{fig:teaser}, given a text prompt that provides a general description of the video and 
specified illumination conditions, 
our Light-A-Video pipeline can relight the input video in a zero-shot manner,
fully leveraging the relighting capabilities of image-based models and the motion priors of the video diffusion model.
To achieve this goal, we initially apply an image-relighting model to video-relighting tasks on a frame-by-frame basis,
and observe that the generated lighting source is unstable across the video frames.
This instability leads to inconsistencies in the relighting of the objects' appearances and significant flickering across frames.
To stabilize the generated lighting source and ensure consistent results, we design a
Consistent Light Attention \textbf{(CLA)} module within the self-attention layers of the image relighting model. 
As shown in Fig.~\ref{fig:cla},
by incorporating additional temporally averaged features into the attention computation,
CLA facilitates cross-frame interactions, producing a structurally stable lighting source.
To further enhance the appearance stability across frames,
we utilize the motion priors of the video diffusion model with a novel Progressive Light Fusion \textbf{(PLF)} strategy.
Adhering to the physical principles of light transport,
PLF progressively employs linear blending to integrate relighted appearances from the CLA 
into each original denoising target, 
which gradually guides the video denoising process toward the desired relighting direction.
Finally, Light-A-Video serves as a complete end-to-end pipeline, effectively achieving smooth and consistent video relighting.
As a training-free approach, Light-A-Video is not restricted to specific video diffusion models,
making it highly compatible with a range of popular video generation backbones, 
including UNet-based and DiT-based models such as AnimateDiff~\citep{guo2023animatediff} and
CogVideoX~\citep{yang2024cogvideox}.
% \tong{Do we have results for LTX-video in the paper?}
Our contributions are summarized as follows:
\begin{itemize}

\item We propose Light-A-Video, a novel training-free video relighting framework that
generalizes the capabilities of image relighting models to the video domain, enabling flexible and temporally consistent video relighting.

% \item We propose Light-A-Video, a novel training-free video relighting framework that
% leverages the capabilities of existing image relighting models to generate high-quality, 
% temporally consistent relighted videos.

\item We introduce two key designs: a consistent light attention module to enhance the stability of lighting sources across frames,
and a progressive light fusion strategy gradually injects lighting information to facilitate temporal consistency in video appearance.

\item Extensive experiments under various scenarios demonstrate the effectiveness and versatility of the proposed method, which not only supports relighting the entire video sequences
but also enables relighting for given foreground sequences.

\end{itemize}
\vspace{-0.5em}