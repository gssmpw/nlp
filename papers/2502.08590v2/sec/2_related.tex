
\section{Related Work}
\label{sec:related}

% \subsection{}
\noindent{\textbf{Video Diffusion Models.}}
Video diffusion models~\citep{blattmann2023align, chen2023videocrafter1, chen2024videocrafter2, guo2023animatediff, wang2023modelscope, wang2023lavie, hong2022cogvideo, yang2024cogvideox, blattmann2023stable, zhang2024show, bu2024broadway} aim to synthesize temporally consistent image frames based on provided conditions, such as a text prompt or an image prompt. In the realm of text-to-video (T2V) generation, the majority of methods~\citep{wang2023modelscope, guo2023animatediff, chen2023videocrafter1, chen2024videocrafter2, blattmann2023align, zhang2024show} train additional motion modeling modules from existing text-to-image architectures to model the correlation between video frames, while others~\citep{hong2022cogvideo, yang2024cogvideox} train from scratch to learn video priors. For image-to-video (I2V) tasks that enhance still images with reasonable motions, a line of research~\citep {xing2025dynamicrafter, chen2025livephoto} proposes novel frameworks dedicated to image animation. Some approaches~\citep{guo2023i2v, zhang2024pia, guo2025sparsectrl} serve as plug-to-play adapters. Stable Video Diffusion~\citep{blattmann2023stable} fine-tune pre-trained T2V models for I2V generation, achieving impressive performance. Numerous works~\cite {niu2025mofa, ma2024follow,ling2024motionclone} focus on controllable generation, providing more controllability for users. Video diffusion models, due to their inherent video priors, are capable of synthesizing smooth and consistent video frames that are both content-rich and temporally harmonious.

\noindent{\textbf{Learning-based Illumination Control.}}
Over the past few years, a variety of lighting control techniques~\citep{sun2019single, nestmeyer2020learning, pandey2021total} for 2D and 3D visual content based on deep neural networks have been proposed, especially in the field of portrait lighting~\citep{shu2017portrait, barron2014shape, sengupta2018sfsnet, shih2014style, kim2024switchlight}, along with a range of baselines~\citep{zhou2019deep, sengupta2021light, hou2021towards, wang2023sunstage, zhou2023relightable} aimed at improving the effectiveness, accuracy, and theoretical foundation of illumination modeling. Recently, owing to the rapid development of diffusion-based generative models, a number of lighting control methods~\citep{ren2024relightful, zeng2024dilightnet, deng2025flashtex, jin2024neural} utilizing diffusion models have also been introduced. Relightful Harmonization~\citep{ren2024relightful} focuses on harmonizing sophisticated lighting effects for the foreground portrait conditioning on a given background image.
SwitchLight~\citep{kim2024switchlight} suggests training a physically co-designed framework for human portrait relighting. IC-Light~\citep{zhang2025scaling} is a state-of-the-art approach for image relighting. LumiSculpt~\citep{zhang2024lumisculpt} enables consistent lighting control in T2V generation models for the first time. However, in the domain of video lighting, the aforementioned approaches fail to simultaneously ensure precise lighting control and exceptional visual quality. This work incorporates a pre-trained image lighting control model into the denoising process of a T2V model through progressive guidance, leveraging the latter's video priors to facilitate the smooth transfer of image lighting control knowledge, thereby enabling accurate and harmonized control of video lighting.

\noindent{\textbf{Video Editing with Diffusion Models.}}
In recent years, diffusion-based video editing has undergone significant advancements. 
Some researches~\citep{liu2024video, wang2023zero, wu2023tune, ma2024follow, mokady2023null} 
adopt pre-trained text-to-image (T2I) backbones for video editing. Another line of approaches~\citep{yang2024fresco, yang2023rerender, cong2023flatten, hu2023videocontrolnet} leverages pre-trained optical flow models to enhance the temporal consistency of output video. Numerous studies~\citep{qi2023fatezero, geyer2023tokenflow, kara2024rave} have concentrated on exploring zero-shot video editing approaches. COVE~\citep{wang2024cove} leverages the inherent diffusion feature correspondence proposed by DIFT~\citep{tang2023emergent} to achieve consistent video editing. SDEdit~\citep{meng2021sdedit} utilizes the intrinsic capability of diffusion models to refine details based on a given layout, enabling efficient editing for both image and video. Despite the remarkable performance of existing video editing techniques in various settings, there remains a lack of approaches specifically designed for controlling the lighting of videos. 