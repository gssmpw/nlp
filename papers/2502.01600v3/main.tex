%

\documentclass{article}

%
\usepackage[dvipsnames]{xcolor}      %
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{booktabs} %
\usepackage{tcolorbox}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}

%
%
%
%
%
\definecolor{liketransactionscolor}{HTML}{8fa255}%{c8dc8d}
\definecolor{showrelationshipscolor}{HTML}{ff8215}
\definecolor{showfeedcolor}{HTML}{e74c3c}
\definecolor{agentcolor}{HTML}{328AFF}
\definecolor{environmentcolor}{HTML}{FEB300}
\definecolor{searchcolor}{HTML}{9b59b6}
%
%
%
%
%
%

\newtcolorbox{assistant_message_box}[1]{colback=white,colframe=agentcolor,fonttitle=\bfseries,title=#1}
\newtcolorbox{user_message_box}[1]{colback=white,colframe=environmentcolor,fonttitle=\bfseries,title=#1}
\usepackage{fancyvrb,fvextra}
\definecolor{codebgcolor}{rgb}{0.95, 0.95, 0.95}
\definecolor{textbgcolor}{rgb}{0.95, 0.95, 0.95}

\usepackage{textcomp}  %
\usepackage{scalerel}  %
%


%

\newcommand{\vladlen}[1]{\textcolor{blue}{Vladlen says: #1}}

\newcommand{\textbad}[1]{\textcolor{red}{\emph{#1}}}
\newcommand{\textblue}[1]{\textcolor{Black}{#1}}
\newcommand{\textcall}[1]{\textcolor{Black}{#1}}
\newcommand{\textcallerror}[1]{\textcolor{Black}{#1}}

\newcommand{\iconcallfail}{%
  \scalerel*{\includegraphics{images/emojis/warning.pdf}}{\bigcirc}%
}
\newcommand{\iconcall}{%
  \scalerel*{\includegraphics{images/emojis/lightning.pdf}}{\bigcirc}%
}
%
  %
%
\newcommand{\icondocs}{%
  \scalerel*{\includegraphics{images/emojis/book.pdf}}{\bigcirc}%
}
\newcommand{\iconbad}{%
  \scalerel*{\includegraphics{images/emojis/prohibited.pdf}}{\bigcirc}%
}
\newcommand{\icongear}{%
  \scalerel*{\includegraphics{images/emojis/gear.pdf}}{\bigcirc}%
}
\newcommand{\iconplan}{%
  \scalerel*{\includegraphics{images/emojis/brain.pdf}}{\bigcirc}%
}
\newcommand{\iconempty}{\hspace{2.7mm}}
\newcommand{\iconspacer}{\hspace{1mm}}

%
%
%
%
\usepackage{hyperref}

%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%

%
%
\usepackage[arxiv]{icml2025}
%

\usepackage{algpseudocodex}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{xspace}
\usepackage{xparse}

\usepackage{comment}

%
%
%
\newcommand{\x}{\mathbf{x}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\c}{\mathbf{c}}


%
\usepackage{cleveref}
\newcommand{\lbleq}[1]{\label{eq:#1}}
\newcommand{\lblsec}[1]{\label{sec:#1}}
\newcommand{\lblfig}[1]{\label{fig:#1}}
\newcommand{\lbltbl}[1]{\label{tbl:#1}}
\newcommand{\lblalg}[1]{\label{alg:#1}}
%
\renewcommand{\refeq}[1]{Eq.~\ref{eq:#1}}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%
%
%
%

\NewDocumentCommand{\todo}{om}{\textcolor{red}{Todo\IfValueT{#1}{ (#1)}: #2}}

%
\def\ours{LOOP\xspace}
%

%
%
%
%
%

\begin{document}

\twocolumn[
%
%
%
\icmltitle{Reinforcement Learning for Long-Horizon Interactive LLM Agents}
%
%
%
%
%
%



%
%
%
%

%
%
%
%

%
%
%
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Kevin Chen}{equal,apple}
\icmlauthor{Marco Cusumano-Towner}{equal,apple}
\icmlauthor{Brody Huval}{equal,apple}
\icmlauthor{Aleksei Petrenko}{equal,apple}\\
\icmlauthor{Jackson Hamburger}{apple}
\icmlauthor{Vladlen Koltun}{apple}
\icmlauthor{Philipp Kr채henb체hl}{apple}
\end{icmlauthorlist}

\icmlaffiliation{apple}{Apple Inc.}

\icmlcorrespondingauthor{Kevin Chen}{kchen29@apple.com}
\icmlcorrespondingauthor{Philipp Kr채henb체hl}{philkr@apple.com}

%
%
%
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

%

%
%
%
%
%

%
\printAffiliationsAndNotice{\icmlEqualContribution} %

%
%
\newcommand{\iid}{\stackrel{\text{i.i.d.}}{\sim}}

\begin{abstract}
Interactive digital agents (IDAs) leverage APIs of stateful digital environments to perform tasks in response to user requests. While IDAs powered by instruction-tuned large language models (LLMs) can react to feedback from interface invocations in multi-step exchanges, they have not been trained in their respective digital environments. Prior methods accomplish less than half of tasks in sophisticated benchmarks such as AppWorld. We present a reinforcement learning (RL) approach that trains IDAs directly in their target environments. We formalize this training as a partially observable Markov decision process and derive \ours, a data- and memory-efficient variant of proximal policy optimization. \ours uses no value network and maintains exactly one copy of the underlying LLM in memory, making its implementation straightforward and as memory-efficient as fine-tuning a single LLM. A 32-billion-parameter agent trained with \ours in the AppWorld environment outperforms the much larger OpenAI o1 agent by 9 percentage points (15\% relative). To our knowledge, this is the first reported application of RL to IDAs that interact with a stateful, multi-domain, multi-app environment via direct API calls. Our analysis sheds light on the effectiveness of RL in this area, showing that the agent learns to consult the API documentation, avoid unwarranted assumptions, minimize confabulation, and recover from setbacks.
\end{abstract}
%
%
%
%
%
%


\section{Introduction}
\lblsec{intro}

\begin{figure}[t]
  %
  %
  \includegraphics[width=\linewidth]{figures/example3-crop.pdf}
  \vspace{-1.5em}
  \caption{
  An interactive digital agent receives a user request and iteratively interacts with APIs through a Python read-eval-print loop (REPL) to accomplish the task.
  }
  \vspace{-1.5em}
  \lblfig{interactive-agent-intro}
\end{figure}

%
%
Consider an interactive digital agent (IDA)
faced with %
the task illustrated in \cref{fig:interactive-agent-intro}.
The task spans multiple software apps and requires common-sense knowledge about human life and language. Successful completion requires chains of information-gathering and state-changing actions, and potential replanning in response to information that is obtained along the way.
The best open-weights models~\cite{qwen2.5} have a success rate below 40\% in these kinds of tasks, while top-of-the line reasoning models~\cite{openai2024o1} succeed barely more than half the time as measured by the AppWorld benchmark~\cite{trivedi2024appworld}.
This is not surprising.
Solving a task can take up to \textit{40 interactions} between the agent and the Python read-eval-print loop (REPL), using up to \textit{32K tokens}.
The AppWorld environment state comprises up to 30M text tokens, making thoughtful management of context a necessity.

In this paper, we demonstrate that reinforcement learning (RL) is an effective approach for training long-horizon interactive LLM agents.
Our approach does not require either expensive-to-gather ground-truth action sequences or large datasets of training scenarios.
With a simple task completion reward on only 24 training scenarios, %
our agent learns behaviors that generalize to diverse held-out tasks resembling the one illustrated in \cref{fig:interactive-agent-intro}.

%

We present a framework for RL with LLM-based IDAs and provide a systematic evaluation of various design choices for policy gradient in this domain.
Our best approach, \ours, combines PPO~\cite{schulman2017proximal} with a leave-one-out baseline estimate~\cite{kool2019buy,ahmadian2024back,shao2024deepseekmath} and per-token clipping.
This enables reuse of off-policy samples (making the method sample-efficient) while storing only a single backbone LLM in memory (making it memory-efficient) and without requiring a value network (implementation-efficient). %

%
Starting from an instruction-tuned base LLM with only 32 billion parameters~\cite{qwen2.5}, \ours achieves new state-of-the-art (SOTA) performance on both AppWorld test splits (test-normal and test-challenge), improving upon the previous open-weight SOTA~\cite{dubey2024llama} by 47 percentage points (pp) and upon closed-source SOTA (GPT-4o) by 23 pp, respectively.
We also evaluate other fine-tuned and non-fine-tuned models on AppWorld and show that \ours outperforms all of them, including outperforming the much larger OpenAI o1 agent by 9 pp.
To our knowledge, this is the first reported application of RL to IDAs that interact with a stateful, multi-domain, multi-app environment via direct API calls.

%
Our analysis reveals a variety of behavioral patterns that emerge in the course of training with a simple task-completion reward. The agent learns to avoid suboptimal open-loop control (unnecessary batching of multiple code cells decreases by $\sim$6x over the course of training), consistently consult API documentation before invoking an app or a specific function (API documentation queries increase by $\sim$60\%), avoid unwarranted assumptions ($\sim$30x reduction of `assuming' and related words in the agent's chain of thought), avoid confabulating placeholder values for important information such as passwords ($\sim$6x reduction of the word `dummy', commonly encountered in the agent's thoughts prior to RL training to indicate the use of a placeholder instead of looking up the actual information), and recover from setbacks ($\sim$3x reduction in capitulations after failed API calls).
%


%
%
%
%





%

%

%

%
%

%

%
%
%
%
%
%

%

%

%

%

%

%
%
%

\section{Related Work}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%

%

\textbf{LLM agents.}
Pretrained, instruction-tuned LLMs have demonstrated an ability to interact with external software environments by invoking structured APIs, both for information retrieval~\cite{nakano2021webgpt,schick2023toolformer} and for acting in stateful external environments~\cite{yao2020keep,yao2022webshop}.
To improve performance in this domain, further works introduced structured workflows that combine reasoning, acting, and reflection steps~\cite{yao2022react,shinn2024reflexion,kim2024language}, and  interaction with code interpreters~\cite{wang2024executable,yang2024intercode}. Others apply supervised fine-tuning on datasets of agent trajectories~\cite{chen2023fireact,qin2023toolllm,mitra2024agentinstruct}.

\textbf{Reinforcement learning for LLMs.}
RL was first used to train LLMs in the setting of reinforcement learning from human feedback (RLHF)~\cite{ziegler2019fine,stiennon2020learning,ouyang2022training}.
These works used proximal policy optimization (PPO)~\cite{schulman2017proximal} to train an LLM policy based on a reward model inferred from human preferences.
RLHF with PPO uses up to four separate LLMs during training: a reward model, trained policy, reference policy, and critic.
%
\citet{ahmadian2024back} showed that the much simpler REINFORCE Leave-One-Out (RLOO) algorithm~\cite{kool2019buy} performs competitively.
RLOO avoids the need for the reference and critic LLMs using on-policy updates and using multiple rollouts from the same query for a sampling-based advantage estimate instead of a learned critic.
Our method, \ours, is a generalization of RLOO that allows for policy updates to drift off-policy using the trust region defined in PPO.
This enables reusing rollouts and a looser loop between rollout collection and policy updates.

PPO and its variants have been used to train LLMs to perform reasoning and static code generation using programmatic reward functions. %
GRPO~\cite{shao2024deepseekmath,deepseekai2025deepseekr1} replaces the critic in PPO with baselines computed from a collection of samples from the same query.
VinePPO~\cite{kazemnejad2024vineppo} estimates per-step advantages via rollouts branched from each reasoning step of the training rollout.
Other RL algorithms for training reasoning LLMs depart from the policy gradient framework, alternating between generating datasets of filtered rollouts and supervised training on these datasets~\cite{singh2023beyond,havrilla2024teaching,yuan2023scaling}.
We compare to these methods and show that a straightforward combination of PPO with a leave-one-out estimate performs significantly better in training IDAs.
%

\textbf{Reinforcement learning for LLM agents.}
RL has been used to train stateful multi-turn agents in text-based games~\cite{narasimhan2015language,yao2020keep,carta2023grounding}, web shopping and navigation  environments~\cite{yao2022webshop}, mobile device control~\cite{bai2024digirl}, and embodied environments~\cite{zhai2024fine}.
Most closely related to our approach are several works that train LLM policies with RL in WebShop:
\citet{yao2022webshop} apply REINFORCE with a learned value baseline,
ArCHer~\cite{zhou2024archer} uses a hierarchical approach that combines off-policy and on-policy training, and
AgentQ~\cite{putta2024agent} combines tree search with direct policy optimization (DPO).
Our work targets AppWorld, which is substantially more complex than the WebShop environment.
While the goal of all WebShop scenarios is to purchase a described item from a simulated site with 8 actions (with at most 1 parameter per turn), AppWorld tasks leverage 9 apps, 457 API endpoints with up to 17 parameters, and require non-trivial logic.
\ours outperforms both REINFORCE-based and DPO-based baselines in this more challenging environment.
\ours was discovered independently in the context of Text-to-Image Diffusion Fine-tuning by \citet{gupta2025simple}.
%
%
%


%
%
%
%
%
%

%
%


%

%
%
%
%
%
%
%
%

%

%
%

%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%

%
%

%
%
%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%

%
%


%
%
%
%

%


%
%
%
%
%

%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

%

%
%
%

%
%
%

%
%

%
%

%



%
%

%
%

%
%
%
%
%

%
%

%
%

%

%
%

%
%
%
%
%
%



\section{Preliminaries}
\lblsec{preliminaries}

\textbf{Autoregressive language models.}
An autoregressive language model (LM) $p_{\theta}$ maps a sequence of tokens $x_{1:t}$ to a probability distribution over the next token $p_{\theta}(x_{t+1} | x_{1:t})$.
%
Generation with an autoregressive LM proceeds token-by-token by sampling the next token ${x_{t+1} \sim p_\theta(\cdot | x_{1:t})}$ until a stop token $\bot$ is reached.
Conditional generation takes a starting sequence $\c = [c_1\ldots c_m]$ of $m$ tokens and generates a sequence of new tokens from
${p_{\theta}(x_{1:n} | \c) = \prod_{t=1}^{n} p_\theta(x_{t} | \c, x_{1:t-1})}$.
We denote the sampling processes as $\x \sim p_\theta(\cdot)$ and $\x \sim p_\theta(\cdot | \c)$.
Unless otherwise noted, all samples $\x$ end with the stop token $\bot$.

%
%

%
%
%
%
%
%
%

%
%
%
%
%
%

%
%

\textbf{Language modeling as a Markov decision process.}
%
%
Language generation can be cast as a Markov decision process (MDP).
The state $[\c, x_{1:t}]$ of the MDP is the context $\c$ and the generation thus far $x_{1:t}$.
Actions produce the next token $x_{t+1} \sim p_\theta(\cdot | \c, x_{1:t})$, transitioning to the new state by appending the token $[\c, x_{1:t}, x_{t+1}]$.
Language modeling tasks often assume a terminal reward, leading to a trajectory-based return $R(\c, \x)$.
The MDP optimization objective is
\begin{equation}
L_\theta(\c) = \mathbb{E}_{\x \sim p_\theta(\cdot | \c)}\left[R(\c, \x)\right] . \lbleq{MDP}
\end{equation}
The terminal reward structure and deterministic state transitions reduce this MDP to a contextual bandit~\citep{ahmadian2024back}.
In the bandit formulation, an entire generation $\x$ is an action, which simplifies the problem significantly.

\textbf{REINFORCE}~\cite{williams1992simple} provides a sampling-based gradient estimate of the above objective~\eqref{eq:MDP}:
\begin{equation}
  \nabla_\theta L_\theta(\c) = \mathbb{E}_{\x \sim p_\theta(\cdot | \c)}\big[A(\c, \x)\nabla_\theta \log p_\theta(\x | \c)\big] \lbleq{REINFORCE},
\end{equation}
where $A(\c,\x)$ is an advantage estimate that lowers the variance of the gradient estimate~\cite{schulman2015high}.
Leave-one-out~\cite{kool2019buy} estimates the advantage using sampling.
Specifically, REINFORCE Leave-One-Out (RLOO) generates $K$ independent samples $\x_1, \ldots, \x_K \sim p_\theta(\cdot | \c)$ and uses all other samples to compute a baseline for the current return.
${A(\c,\x_k) = R(\c, \x_k) - \frac{1}{K-1}\sum_{i=1, i\ne k}^K R(\c, \x_i)}$.
An equivalent form of this objective estimates the advantage by subtracting the average return baseline~\cite{kool2019buy}:
\begin{equation}
  A(\c,\x_k) = \frac{K}{K-1} \Bigg(R(\c, \x_k) - \frac{1}{K}\sum_{i=1}^K R(\c, \x_i)\Bigg) . \lbleq{lno}
\end{equation}
This results in a simple, unbiased, low-variance advantage estimate, which has been successfully applied to large language models~\cite{ahmadian2024back}.
However, the gradient estimate needs to be on-policy.
(For each gradient step, samples need to be drawn from the current policy.)
In practice, on-policy methods can be inefficient because they do not amortize the relatively high cost of generating rollouts across multiple gradient steps.
%
%
%
Reinforcement learning from human feedback (RLHF) with LLMs
%
thus often uses
PPO% proximal policy optimization (PPO)~\cite{schulman2017proximal}
%
, which is designed to perform multiple gradient updates per training iteration.


\textbf{Proximal policy optimization (PPO)} is a policy-gradient algorithm that allows an updated policy to deviate from the sampling policy within a certain trust region~\cite{schulman2017proximal}.
It relies on an importance-weighted estimate of the MDP objective:
\begin{equation}
    L_\theta^\text{bandit}\!(\c)\!=\!\mathbb{E}_{\x \sim p_\psi\!(\cdot | \c)}\!\!\!\left[\!\min\!\!\left(\!\frac{p_\theta\!(\x | \c)}{p_\psi\!(\x | \c)}\!A\!(\c,\!\x)\!, g_\epsilon\!(\!A\!(\c,\!\x)\!)\!\!\right)\!\!\right]\!\!\!\! \lbleq{PPO_bandit}
\end{equation}
where $g_\epsilon(A) = A + \epsilon |A|$ and $p_\psi$ is a sampling policy.
In the on-policy setting, $p_\psi = p_\theta$, the PPO objective reduces to the original MDP objective in~\refeq{MDP}, and its gradient estimate is REINFORCE (\refeq{REINFORCE}).
%
However, PPO allows for samples to drift off policy via updates of $p_\theta$.
PPO gradient updates ignore any samples that fall outside a trust region:  $p_\theta(\x | \c) > (1+\epsilon)p_\psi\!(\x | \c)$ for positive advantages or $p_\theta(\x | \c) < (1-\epsilon)p_\psi\!(\x | \c)$ for negative advantages.
For PPO, the choice between the MDP and contextual bandit interpretations matters.
For a contextual bandit, PPO uses a per-trajectory importance weight as in \refeq{PPO_bandit}.
For an MDP, PPO uses a per-action importance weight derived from policy improvement theory~\cite{kakade2002approximately}:
%
\begin{align}
    & L_\theta^\text{MDP}(\c)=\notag\\
    & \mathbb{E}_{\x \sim p_\psi\!(\cdot|\c)}\!\!\!\left[\!\!\frac{1}{|\x|}\!\!\sum_{t\!=\!1}^{|\x|}\!\min\!\!\left(\!\frac{p_\theta\!(x_t | \c,\!x_{1:t\!-\!1})}{p_\psi\!(x_t | \c,\!x_{1:t\!-\!1})}\!A\!(\c,\!\x)\!, g_\epsilon\!(\!A\!(\c,\!\x)\!)\!\!\right)\!\!\right]\!\!.\!\! \lbleq{PPO_MDP}
\end{align}
Per-action (per-token) importance weights are generally more stable as the learned policy $p_\theta$ drifts from the sampling policy $p_\psi$.
With a per-trajectory importance weight (\refeq{PPO_bandit}), a change in a single token's probability may stop the gradient update for the entire trajectory.
With a per-token importance weight (\refeq{PPO_MDP}), a change in a single token's probability only influences its own gradient.
PPO generally uses a value function network to estimate advantages~\cite{schulman2017proximal}.
However, for language models this value network can be slow, memory intensive, and inaccurate~\cite{ahmadian2024back,kazemnejad2024vineppo}.
We instead present a variant of PPO with a leave one out estimator~\cite{kool2019buy}: Leave-One-Out PPO (\ours).

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%

\section{\ours}

We start by describing a partially observable Markov decision processes (POMDP, \citet{kaelbling1998planning}) for interactive digital agents.
%
We then present a variant of Proximal Policy Optimization with a Leave-One-Out advantage estimate.
We show that GRPO~\cite{shao2024deepseekmath,deepseekai2025deepseekr1} and RLOO~\cite{kool2019buy,ahmadian2024back} correspond to specific variants of \ours.

%
%
%


\subsection{POMDPs for interactive digital agents}

%

%

\begin{figure}[t]
    \includegraphics[width=\columnwidth, page=1]{images/observations-crop.pdf}
    \caption{
    Top: modeling interactive agents as a POMDP.
    States take the form $[\s_0, \c, x_{1:t}]$, where $\s_0$ is the hidden state (containing the initial state of the coding environment), $\c$ is the initial context for the task (green), and $x_{1:t}$ combines tokens emitted by the LLM (blue) with tokens emitted by the environment (red).
    Bottom: PPO with a per-token, -turn, or -trajectory importance weights.
    %
    %
    %
    }
    \lblfig{histories}
    \vspace{-1em}
\end{figure}


%
IDAs interact with a read-eval-print loop (REPL) environment that is initialized with variable bindings to a predefined API.
Given a task instruction, the agent interacts with the environment to complete the task, invoking API functions when applicable.
Each interaction consists of the agent executing a code block in the environment and receiving a response string.
The interactions continue until the agent indicates completion
(e.g.\ by calling `complete\_task()').
Importantly, the environment is stateful.
%
In AppWorld, the environment's state is (i) a relational database that can be queried or modified via various API functions (such as sending a message or appending text to a SimpleNote note)
and (ii) the state of the REPL itself (e.g.\ Python global variables defined by the agent).

We formulate IDA tasks as a POMDP.
The state $[\s_0, \c, x_{1:t}]$ of the POMDP
is the initial state of the environment $\s_0$ (e.g. Python REPL and database),
the task context $\c$ (i.e. a user prompt),
and the generation thus far $x_{1:t}$.
Actions produce the next token $x_{t+1} \sim p_\theta(\cdot|\c, x_{1:t})$.
Most transitions again simply append a single token $[\s_0, \c, x_{1:t}] \to [\s_0, \c, x_{1:t+1}]$.
However, some transitions (when the agent emits a stop token) trigger code execution\footnote{Mathematically, the environment replays all code from the entire interaction history in the initial environment state, and responds with the output of the code cells most recently submitted.} by the environment.
These transitions append both the generated token $x_{t+1}$ and tokenized environment response $x_{t+2:t+1+k}$ to the state $[\s_0, \c, x_{1:t+1+k}]$.
In consecutive turns, the LLM observes both its generation $x_{1:t+1}$ and the environment's response $x_{t+2:t+1+k}$.
\cref{fig:histories} depicts this process.
Let $a(\x) \subseteq \{1, \ldots, T\}$ denote the subset of tokens $x_t$ in trajectory $\x$ that were emitted by the LLM (versus part of the environment response).
Let $\mathbb{I}(\s_0, \x) \in \{0, 1\}$ denote an indicator of trajectories with consistent API responses for an initial state $\s_0$.
The distribution over trajectories is then
\begin{equation}
    \rho_{\theta}(\x | \s_0, \c) := \mathbb{I}(\s_0, \x) \prod_{t \in a(\x)}^T p_{\theta}(x_t | \c, x_{1:t-1}). \lbleq{pomdp_prob}
\end{equation}
The goal of an agent $p_\theta$ is to maximize its expected return:% $R(\s_0, \c, \x)$:
\begin{equation}
L_\theta(\s_0, \c) = \mathbb{E}_{\x \sim \rho_\theta(\cdot | \s_0, \c)}\left[R(\s_0, \c, \x)\right].\lbleq{POMDP}
\end{equation}
%
When rollouts are sampled from $\rho_{\psi}$ (i.e. using a different LLM $p_{\psi}$), the importance weight of PPO (\refeq{PPO_bandit}) reduces to
\begin{equation}
    \frac{\rho_{\theta}(\x | \s_0, \c)}{\rho_{\psi}(\x | \s_0, \c)}
    = \prod_{t \in a(\x)} \frac{p_{\theta}(x_t | \c, x_{1:t-1})}{p_{\psi}(x_t | \c, x_{1:t-1})}.\lbleq{trajectory_importance_weight}
\end{equation}

%

%
%
%

The token-level PPO gradient (\refeq{PPO_MDP}) reduces to a sum over tokens $x_t$ produced by the LLM.
%
Full equations are provided in \cref{sec:ppo_pomdp}.

%
%

%
%
%
%
%
%
%

%

%
%
%
%

%

%

\begin{algorithm}[t]
\caption{Leave-One-Out Proximal Policy Optimization}
\lblalg{mppo}
%
\begin{algorithmic}[1]
\Require{Policy $p_{\theta}$, dataset of tasks and initial states $\mathcal{D}$}
\Ensure{Policy $p_{\theta}$ maximizing $\mathbb{E}_{\s_0, \c \sim \mathcal{D}}\left[L_\theta(\s_0, \c)\right]$ (\refeq{POMDP}})
\For{$\text{iteration} = 1, 2, \ldots$}
  \State $\B \gets \{\}$ \Comment{Initialize rollout buffer}
  \For{$(\s_0, \c) \sim \mathcal{D}$}\Comment{Rollout collection}
    \State Collect $K$ rollouts $\x_1, \ldots, \x_K \iid \rho_{\theta} (\cdot | \s_0, \c)$ \lblalg{mppo:rollout}
    \State Estimate advantages $A_1, \ldots, A_K$ using \refeq{lno}
    \State $\B \gets \B \cup \{(\x_1, A_1), \ldots, (\x_K, A_K)\}$
  \EndFor
  \For{$\text{epoch} = 1, \ldots, N_\text{epoch}$}\Comment{Policy update}
    %
    \For{mini-batch $\{(\x_i, A_i)\}_{i=1}^M \sim \B$}
      \State Update policy using PPO gradient (\refeq{PPO_MDP}) \lblalg{mppo:update}
    \EndFor
  \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Leave-One-Out Proximal Policy Optimization}
\lblsec{LOOP-subsec}

\ours combines PPO with a Leave-One-Out estimate of the advantage~\cite{kool2019buy}.
The algorithm proceeds in two phases: rollout collection and policy update.
During rollout collection, we sample $K$ samples from the POMDP for each initial state and context pair $(\s_0,\c)$ in the dataset $\mathcal{D}$.
We then directly compute the advantage of each rollout using the leave-one-out estimator (\refeq{lno}).
During updates, we iterate over all collected rollouts for $N_\text{epoch}$ epochs.
%
Each epoch iterates over random mini-batches to update the policy using the PPO objective (\refeq{PPO_MDP}).
We randomly shuffle trajectories irrespective of their initial state-context pair $(\s_0, \c)$.
This process is summarized in \cref{alg:mppo}.
%

\textbf{PPO with tokens, trajectories, and turns.}
In \cref{alg:mppo:update} of \cref{alg:mppo}, we experiment with per-token (\refeq{PPO_MDP}), per-trajectory (\refeq{PPO_bandit}), or per-turn importance weights (\cref{fig:histories}).
%
%
%
%
%

\textbf{\ours as REINFORCE Leave-One-Out~\cite{ahmadian2024back}.}
If \ours is run purely on-policy ($N_\text{epoch}=1$, no mini-batches), the algorithm reduces to REINFORCE Leave-One-Out (RLOO), since the policy update in PPO (\refeq{PPO_MDP}) reduces to REINFORCE (\refeq{REINFORCE}).

\textbf{\ours as GRPO~\cite{shao2024deepseekmath}.}
\ours and GRPO differ principally in their advantage estimation.
GRPO uses a leave-one-out estimate (\refeq{lno}), normalized by the standard deviation of returns.
This disproportionally favors low-standard-deviation trajectories from which the LLM receives a consistent return.
Experimentally, we found forgoing this normalization to be beneficial.

%
%
%
%
%
%

%

%

%
%

%
%
%
%
%
%
%
%
%
%


%
%


%
%
%
%
%
%
%
%
%
%

%


%
%
%
%
%
%
%

%
%
%
%

%
%



%

%
%
%
%
%
%
%

%
%

%

%
%


%
%
%


\section{Evaluation}
\lblsec{evaluation}

%
%
%
%
%
%
%
%

\subsection{AppWorld}
\lblsec{appworld}

The AppWorld benchmark~\cite{trivedi2024appworld} tests an IDA's ability to interact with the APIs of nine feature-rich simulated consumer apps (including email, payments, music, shopping, phone, and file system) on a user's behalf.
The agent interacts with a stateful Python interpreter to complete tasks of varying difficulty (difficulty levels 1 to 3).
Each task contains a series of unit tests that ensure that (1) the requested changes to the environment state were successfully made, (2) there were no extraneous changes to the environment or app states, and (3) the final answer produced by the agent matches the ground truth (where applicable).

AppWorld includes a total of 250 task templates, or scenarios, and each scenario has 3 task variants for a total of 750 tasks.
Tasks are split into train (35 scenarios, 105 tasks)\footnote{\citet{trivedi2024appworld} reports 105 train tasks and 60 dev tasks but only 90 and 57 tasks are available to use, respectively.}, dev (20 scenarios, 60 tasks), test-normal (Test-N) (56 scenarios, 168 tasks), and test-challenge (Test-C) (139 scenarios, 417 tasks).
Test-C features tasks that require more complex sequences of interactions and involve new apps not seen during training.
%
%
We report task goal completion (TGC) and scenario goal completion (SGC) metrics, the per-task and per-scenario success rates, respectively.
A scenario is only considered successful if all corresponding tasks succeed.

\subsection{Implementation}
We use Qwen2.5-32B-Instruct~\cite{qwen2.5} as our base model and fine-tune with LoRA~\cite{hu2022lora}.
We train on all difficulty 1 and 2 training scenarios (24 out of 30 scenarios, listed in \cref{sec:train_tasks}), and limit the agent to 40 interactions during training and 50 interactions during evaluation.
We use $K=6$ rollouts per task with a reward ${R(\s_0, \c, \x)\in [0,1]}$ corresponding to the fraction of unit tests passed for the task.
See \cref{sec:appendix-training-details} for additional training details.

    %
    %
    %

\begin{table*}[t]
    \small
    \begin{tabular}{llccccccc}
    \toprule
    %
     & &  & \textbf{Strictly} & \textbf{Normalized} & \multicolumn{2}{c}{\textbf{Test Normal (Test-N)}} & \multicolumn{2}{c}{\textbf{Test Challenge (Test-C)}} \\ \cline{6-7} \cline{8-9}
     %
    \textbf{Type} & \textbf{Algorithm} & \textbf{Action} & \textbf{on-policy} & \textbf{reward} & \raisebox{-2pt}{TGC} & \raisebox{-2pt}{SGC} & \raisebox{-2pt}{TGC} & \raisebox{-2pt}{SGC} \\
    \midrule
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %

    %
    %
    %
    %
    %
    NFT & GPT-4o & -- & -- & -- & 48.8 & 32.1 & 30.2 & 13 \\
    NFT & OpenAI o1 & -- & -- & -- & 61.9 & 41.1 & 36.7 & 19.4\\
    NFT & Llama 3 70B & -- & -- & -- & 24.4 & 17.9 & 7.0 & 4.3 \\
    NFT & Qwen 2.5 32B & -- & -- & -- & 39.2 $\pm$ 3.5 & 18.6 $\pm$ 2.0 & 21.0 $\pm$ 1.4 & 7.5 $\pm$ 1.2 \\
    \midrule
    SFT & SFT-GT & -- & -- & -- & 6.2 $\pm$ 0.7 & 1.8 $\pm$ 0.0 & 0.8 $\pm$ 0.2 & 0.1 $\pm$ 0.3\\
    SFT & RFT & -- & -- & -- & 47.9 $\pm$ 3.7 & 26.4 $\pm$ 2.3 & 26.4 $\pm$ 1.8 & 11.4 $\pm$ 2.3 \\
    SFT & EI & -- & -- & --  & 58.3 $\pm$ 2.8 & 36.8 $\pm$ 6.0 & 32.8 $\pm$ 0.7 & 17.6 $\pm$ 1.3 \\
    %
    \midrule
    DPO & DPO-MCTS & -- & -- & --  & 57.0 $\pm$ 1.5 & 31.8 $\pm$ 4.2 & 31.8 $\pm$ 1.3 & 13.7 $\pm$ 1.5 \\
    DPO & DMPO & -- & -- & --  & 59.0 $\pm$ 1.2 & 36.6 $\pm$ 4.7 & 36.3 $\pm$ 1.8 & 18.4 $\pm$ 2.3 \\
    \midrule
    RL & PPO (learned critic) & token & & & 50.8 $\pm$ 3.7 & 28.9 $\pm$ 7.9 & 26.4 $\pm$ 0.5 & 10.5 $\pm$ 2.1 \\
    RL & RLOO & traj & \checkmark &  & 57.2 $\pm$ 2.6 & 35.7 $\pm$ 2.9 & 36.7 $\pm$ 1.6 & 17.4 $\pm$ 1.4 \\
    %
    RL & GRPO & token  & \checkmark\footnotemark[3] & \checkmark & 58.0 $\pm$ 1.8 & 36.8 $\pm$ 3.9 & 39.5 $\pm$ 1.9 & 22.4 $\pm$ 0.8 \\ %
    RL & GRPO no kl & token & \checkmark\footnotemark[3] & \checkmark & 59.0 $\pm$ 1.4 & 35.7 $\pm$ 2.9 & 42.7 $\pm$ 1.3 & 21.3 $\pm$ 1.7 \\
    %
     %
    RL & \ours (bandit) & traj & & & 53.3 $\pm$ 3.4 & 33.6 $\pm$ 3.2 & 27.7 $\pm$ 1.5 & 13.0 $\pm$ 0.9 \\  %
    RL & \ours (turn) & turn & & & 64.1 $\pm$ 2.2 & 43.5 $\pm$ 3.5 & 40.8 $\pm$ 1.5 & 26.5 $\pm$ 2.4 \\ %
    %
    RL & \textbf{\ours (token)} & token & & & \bf{71.3 $\pm$ 1.3} & \bf{53.6 $\pm$ 2.2} & \bf{45.7 $\pm$ 1.3} & \bf{26.6 $\pm$ 1.5} \\ %
    RL & \ours RwNorm (token) & token  & & \checkmark & 61.9 $\pm$ 4.0 & 44.1 $\pm$ 7.8 & 39.8 $\pm$ 1.3 & 20.4 $\pm$ 2.1 \\ %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    \bottomrule
    \end{tabular}

    \caption{Evaluation results on AppWorld normal and challenge test sets with respect to Task Goal Completion (TGC) and Scenario Goal Completion (SGC). Each number is computed over 5 evaluation runs of the same model checkpoint, selected based on best dev set performance. Train paradigm is defined on the left (NFT = off-the-shelf model, SFT = supervised fine-tuned, DPO = direct preference optimization, RL = reinforcement learning). For RL, an action can be a token, turn, or trajectory (traj). Strictly on-policy implies only a single gradient update between rollout generation stages rather than multiple. Reward normalization is per group (rollouts which share the same task) using the group mean and std. Our approach, \ours (token), outperforms all other methods.}
\vspace{-3mm}
    \lbltbl{eval}
\end{table*}




\subsection{Evaluated Methods}
%
%
\lblsec{baselines}

We compare our approach with methods outlined below.
For each fine-tuned method, we select the highest performing checkpoint according to the validation (dev) set.

\textbf{No fine-tuning (NFT)} baselines evaluate on AppWorld zero-shot.
We include current open-weight and closed-source SOTA models (Llama 3 70B~\cite{dubey2024llama} and GPT-4o) from \citet{trivedi2024appworld}, as well as two others: Qwen2.5-32B-Instruct~\cite{qwen2.5}, which is used as the base LLM in our setup, and OpenAI o1~\cite{openai2024o1}.

\textbf{Ground truth supervised fine-tuning (SFT-GT).}
We transform the AppWorld solution code into a ReAct-style dataset and apply supervised fine-tuning.
Details in \cref{sec:sft-gt}.

\textbf{Rejection sampling fine-tuning (RFT)~\cite{yuan2023scaling}} collects rollouts generated with the base model and fine-tunes on successful ones ($R(\s_0, \c, \x)=1$).
Details in \cref{sec:rft}.

\textbf{Expert iteration (EI)~\cite{Anthony2017}} runs multiple smaller iterations of RFT.
It uses the current best model for rollout collection and fine-tunes on many smaller collections of rollouts~\cite{zelikman2022star,singh2023beyond,havrilla2024teaching}.
Details in \cref{sec:ei}.

\textbf{Direct Preference Optimization + MCTS (DPO-MCTS)~\cite{putta2024agent}.}
We implement a simplified version of Agent Q.
It collects preference pairs into a replay buffer using Monte Carlo Tree Search (MCTS).
Unlike Agent Q, we rely only on an upper confidence bound (UCB) to expand nodes and do not use an LLM critic heuristic.
Details in \cref{sec:dpo-mcts}.

\textbf{Direct multi-turn preference optimization (DMPO)~\cite{shi2024direct}.} We sample a pair of rollouts per task and form a preference pair if the return difference is greater than a threshold. We use the DMPO loss on the winning and losing rollout, and treat the interactions as turns within the loss. We run this on-policy without an offline replay buffer
%
(the reference policy is updated after every iteration).

\textbf{PPO with a learned critic~\cite{schulman2017proximal}.}
We implement a version of PPO with a learned baseline and Generalized Advantage Estimation~\cite{schulman2015high}. See \cref{sec:ppo-critic} for details.

\textbf{REINFORCE leave-one-out (RLOO)~\cite{ahmadian2024back}} is the on-policy equivalent to per-trajectory LOOP.
RLOO and LOOP share the same experimental setup and all hyperparameters.

\textbf{Group relative policy optimization (GRPO)~\cite{shao2024deepseekmath}.}
We implement GRPO strictly on-policy as described in \citet[Sec. 4.2]{shao2024deepseekmath}.
We evaluate GRPO with and without the KL penalty.
Finally, we compare to off-policy PPO with a GRPO advantage estimate (LOOP RwNorm).
All versions of GRPO and LOOP share the same experimental setup and all hyperparameters.

%
%
%
%
%

\textbf{\ours (ours).} We evaluate variants of \ours with importance weighting on trajectories (`bandit'), turns, or individual tokens (see \cref{eq:PPO_bandit,eq:PPO_MDP,sec:LOOP-subsec}).
%

\begin{figure*}[t]
\begin{subfigure}{0.43\linewidth}
\small
  \input{figures/analysis_bar_dev_eval_settings}
  \caption{Change in agent behavior}
\end{subfigure}%
\begin{subfigure}{0.57\linewidth}
  \input{figures/analysis_figure_with_bar_no_heatmap}
  \caption{Summaries of representative rollouts}
\end{subfigure}
\caption{
(a) Aggregate changes in agent behavior between the base model and \ours, averaged over three i.i.d.\ rollouts per dev (validation) task.
(b) Rollouts of the base and \ours agents on a single dev task.
The agent must identify the user's roommates (via phone.search\_contacts) and pending requests to them (via venmo.show\_sent\_payment\_requests), and then call venmo.remind\_payment\_request on each.
The base agent assumes the identities of the roommates, leading to task failure.
The \ours agent correctly gathers this information and consistently reads API documentation.
}
\vspace{-3mm}
\lblfig{analysis_combined}
\end{figure*}


\subsection{Results}
\lblsec{results}

The results are summarized in \cref{tbl:eval}, including off-the-shelf models (no fine-tuning, `NFT'), supervised fine-tuning (`SFT'), direct preference optimization (`DPO'), and reinforcement learning (`RL').
For each RL algorithm in the table, we highlight the following attributes:
\begin{itemize}[noitemsep, topsep=0pt, parsep=4pt, partopsep=0pt]
    \item \textbf{Action definition.} A trajectory (traj), turn, or token.
    \item \textbf{Strictly on-policy.} On-policy methods perform rollout generation followed by a \textit{single} gradient update. Otherwise, we allow multiple gradient updates between each rollout generation stage.
    \item \textbf{Reward normalization} divides by the standard deviation of returns~\citep{shao2024deepseekmath}.
\end{itemize}

\footnotetext[3]{GRPO allows for off-policy updates in theory but is implemented strictly on-policy (Sec.4.2 of \citet{shao2024deepseekmath}).}


\ours (token) outperforms all other methods, achieving 71.3 TGC on test-normal (Test-N) and 45.7 TGC on test-challenge (Test-C).
%
%
%
%
Our method improves over the performance of our base LLM (Qwen2.5-32B-Instruct) by 81\% relative on Test-N and by 117\% on Test-C. We improve over the much larger OpenAI o1 model by 15\% on Test-N and by 24\% on Test-C.

All fine-tuning methods improve performance considerably over the base model.
One or more variants of each SFT, DPO, and RL are able to achieve a TGC of 58 or higher on Test-N, outperforming GPT-4o and only 4 percentage points (pp) lower than OpenAI o1.
However, performance seems to saturate around 59 TGC with the exception of \ours (turn, token), which achieves \textgreater 5 pp improvement over those approaches.
All RL methods with Monte Carlo baselines outperform PPO with a learned value function. This PPO variant requires additional hyperparameter tuning and showed inferior training stability, consistent with previous findings~\cite{kazemnejad2024vineppo}.
%

%



Reward normalization generally hurts performance in AppWorld.
It causes a 9 pp drop in performance for \ours (token).
A potential reason is that reward normalization disproportionally favors trajectories from which the LLM receives a consistent return (and which therefore have a low standard deviation).
The largest training signal thus comes from scenarios that the LLM either fully solves or fails on.
For AppWorld this seems less beneficial than considering scenarios that can sometimes be solved.
This is the likely reason GRPO performs worse.
A minor drop (2 pp) further comes from the KL penalty.
%
%
%

%
%

Lastly, we observe that defining actions as tokens works considerably better than turns or trajectories, as evidenced by \ours bandit/turn/token. We found that trajectories and turns as actions led to unstable training, and that the importance weights were often clipped (\refeq{PPO_bandit}).
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%

%
%
%
%

%
%

%

%

%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%



%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%

%


\subsection{What does the policy learn?} \lblsec{behaviors}

To analyze changes in behavior due to reinforcement learning, we compared rollouts collected from the base model with rollouts collected after training with \ours.
\cref{fig:analysis_combined}(a) shows changes in rollout features, aggregated over three i.i.d.\ rollouts for every dev task.
We also analyzed per-task changes in behavior:
\cref{fig:analysis_combined}(b) shows summaries of two rollouts for a single task. (See \cref{sec:appendix-analysis} for more rollout summaries and rollout transcripts.)
%
%
We identified several changes in behavior due to RL that contribute to the overall improvement in score.
Many of these changes involve avoiding anti-patterns.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
\begin{figure*}[t]
    \includegraphics[width=1.0\textwidth,trim={0 8.5in 0 0},clip]{figures/variability/figure_keynote_v11.pdf}
    \caption{
    A visualization of 100 i.i.d.\ rollouts of an agent on the same task after LOOP training.
    Each column represents a rollout, containing 12 to 33 color-coded API calls.
    The majority of rollouts (98 out of 100) successfully complete the task, yet 94 out of 98 exhibit unique sequences of API calls.
    Even at the high level, behavior does not collapse into a single mode but instead exhibits multiple distinct strategies.
    The heatmap summarizes the task 2a163ab\_1 (\cref{sec:rollout_2a163ab_1_late}) where the agent needs to `like' transactions involving their roommates using the \textbf{\textcolor{liketransactionscolor}{venmo.like\_transaction}} API. Solutions follow four distinct strategies. \textbf{Mode 1}: Directly searching for roommate contacts via \textbf{\textcolor{searchcolor}{phone.search\_contacts}}. \textbf{Mode 2}: Browsing the Venmo social feed first via \textbf{\textcolor{showfeedcolor}{venmo.show\_social\_feed}} to identify relevant transactions. \textbf{Mode 3}: Querying all contact relationships in the phonebook first with \textbf{\textcolor{showrelationshipscolor}{phone.contact\_relationships}}. \textbf{Mode 4}: Utilizing \textbf{\textcolor{showfeedcolor}{venmo.show\_social\_feed}} followed by \textbf{\textcolor{showrelationshipscolor}{phone.contact\_relationships}}.
    This diversity is likely key to the success of RL in this domain. It fosters exploration during early training, and prevents severe overfitting late in training.}
    \lblfig{heatmap}
    \vspace{-4mm}
\end{figure*}

\textbf{Agent learns to avoid open-loop control.}
Submitting multiple Python code cells for execution all at once amounts to decision-theoretically suboptimal open-loop control.
It is better to use intermediate results to inform the next code cell.
Early in training the agent optimistically submits multiple code cells (interleaved with chain-of-thought tokens) per turn, assuming that each cell will succeed.
The prevalence of multiple code cells per turn decreases by $\sim$6x over the course of training, even though the total amount of code submitted does not significantly decrease (\cref{fig:analysis_combined}).

\textbf{Agent learns to consistently read API documentation.}
Reading API documentation is a critical information-gathering action in AppWorld because of the large number of API functions (457) and their individual complexity (1470 total function parameters).
Early in training, the agent looks up API documentation, but infrequently and inconsistently.
Later in training, the agent looks up API documentation for an app or a specific function more consistently before attempting to call it.
Training increases the frequency of the `show\_api\_doc' call that is used to look up API documentation by $\sim$1.6x  (\cref{fig:analysis_combined}).

\textbf{Agent learns to makes fewer assumptions.}
Early in training, the agent often makes assumptions in its chain of thought that simplify its task but are brittle or wrong, like equating the user's list of roommates with the user's list of Venmo friends (e.g.\ `\emph{Get the list of roommates (assuming roommates are friends in Venmo)}'). %
Assumptions made early are not revisited and can cause mistakes, sometimes far downstream in the rollout (`\emph{Let's now get the list of my roommates (friends) and then fetch transactions ... that involve any of my roommates}' %
and `\emph{We need to call the `search\_friends' API to get the list of roommates (friends)}').
%
Later in training, the agent explicitly searches for the `roommate' relationship in the phone app instead (compare rollouts in \cref{sec:rollout_2a163ab_1_early,sec:rollout_2a163ab_1_late}).
%
Training reduces the frequency of `assuming' and related words by $\sim$30x on dev tasks (\cref{fig:analysis_combined}).

\textbf{Agent learns to use fewer placeholder values.}
Early in training, the agent frequently uses `dummy' or placeholder values for passwords or other information that may take some work to acquire.
For example, the agent often tries to use a dummy value for passwords like `\emph{dummy\_venmo\_pass}'
(\cref{sec:rollout_2a163ab_1_early_1}).
%
Training reduces the frequency of the word `dummy' by $\sim$6x (\cref{fig:analysis_combined}).
%

%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

\textbf{Agent learns not to give up after an API error.}
Early in training, when the agent encounters an API error,
it often gives up on its subtask and starts to work on other subtasks.
%
For example, when the call `\emph{phone.search\_contacts(query=``roommate'')}' fails because the phone app's access token was not provided as an argument, the agent states `\emph{It seems there's an issue with accessing the `phone' app... Since we can't currently use this app to retrieve the roommates...}' (\cref{sec:rollout_2a163ab_1_early_1}).
%
%
In the course of training, the agent learns to persevere and debug occasional errors.
%
Training reduces the frequency of giving up on a failed API call by $\sim$3x (\cref{fig:analysis_combined}).
%

\subsection{Why does RL on small data work?}
\lblsec{analysis-variability}
One striking conclusion from \cref{tbl:eval} is that all reinforcement-learning-like methods (EI, DPO-MCTS, DMPO, RLOO, GRPO, LOOP) work much better than supervised fine-tuning (RFT, SFT-GT) or prompt engineering.
This is surprising, considering the small amount of training data: 24 scenarios, 72 tasks.
Surely, capable RL algorithms should overfit to this data.
Why don't they?

One of the answers lies in the diversity of rollouts produced by the LLM.
Even in late stages of training, a LOOP LLM policy produces a great variety of rollouts.
At a micro-level, sampling from an LLM policy encourages small variations and rarely produces the same solution twice.
At a macro-level, the LLM maintains multiple distinct solution ``phenotypes'' and jointly improves all of them~(\cref{fig:heatmap}).
%

This variety of rollouts has two benefits. Early in training, it fosters exploration that discovers solutions that improve over the base model.
Late in training, it prevents the model from collapsing onto a single solution and thus fosters generalization.
%


%
%
%

%
%


%
%

%
%
%

\section{Discussion}

%
%
%

%
We formalized training interactive digital agents (IDAs) as a reinforcement learning (RL) problem.
We presented a simple and effective learning algorithm (\ours) for IDAs.
%
%
Experimental results indicate that \ours substantially improves agent performance.
%
%
%
In particular, we showed that RL can produce meaningfully better IDAs after training on only a small number of scenarios.

Much remains to be done to realize the dream of broadly effective IDAs.
Even our best agents succeed on $\sim$7 out of 10 tasks in AppWorld.
This may be acceptable for agents with close human supervision, but is below the level of robustness required for broader autonomy.
%
%
%
%
%
%
%
%
Additionally, while AppWorld is the literature's most advanced multi-turn IDA benchmark~\cite{trivedi2024appworld}, it still lacks some key features of everyday environments:
non-determinism, transient failures, unsolvable and ambiguous tasks, adversarial scenarios (e.g.\ scams), user clarification and confirmation steps, and interactive counterparties such as customer service representatives.
Notwithstanding these challenges, our findings, along with other recent work, demonstrate the transformative potential of applying RL to LLMs.


%
%
%
%
%
%

%
%
%


%



%
%
%
%
%
%
%
%
%

%
%
%

%
%
%


%
%
%
%
%

\clearpage

\bibliography{main}
\bibliographystyle{icml2025}


%
%
%
%
%
\newpage
\appendix


%
%
%
%
%
%
%
%


%
%
%
%
%

\onecolumn

\section{Emergent behavior after training} \lblsec{appendix-analysis}
\lblsec{early-late-training-analysis}

%

%
%

\subsection{Quantitative comparison on training tasks}

The emergent behaviors described in \cref{sec:behaviors} are reflected not only in the dev tasks (\cref{fig:analysis_combined}(a)) but also in the train tasks.
The fine-tuned agent becomes more interactive (fewer turns with multiple code cells), more persistent (reduced give up rate), more careful (frequently checking the documentation), and more serious (fewer assumptions and dummy variables).

\begin{figure*}[ht]
\centering
    \input{figures/analysis_bar_train_eval_settings_new}
    \caption{Changes in behaviors, aggregated over all 72 \textbf{tasks used in reinforcement learning training} (3 i.i.d. rollouts per task). Showing that the emergent behaviors observed on dev tasks (\cref{fig:analysis_combined}) also apply to the train tasks.}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\lblfig{analysis_combined_train}
\end{figure*}

The following are the specific definitions for metrics used in \cref{sec:behaviors}, \cref{fig:analysis_combined}(a), and \cref{fig:analysis_combined_train}:

\textbf{Number of turns} is the number of agent submissions followed by environment responses in a given rollout.

\textbf{Number of code characters per rollout} is calculated by extracting the content of all \texttt{```python|py} ... \texttt{```} delineated blocks from agent turns and counting the characters within these blocks.
This total number of characters is then normalized by the number of analyzed rollouts.

\textbf{Multiple code cell per turn} is calculated by counting the number of \texttt{```python|py} ... \texttt{```} delineated blocks from agent turns and tracking the number of turns with more than one block.
This total number of turns with multiple code cells is then normalized by the number of turns across all analyzed rollouts.

\textbf{Execution errors per turn} is calculated by counting the number of environment responses denoting \texttt{"Execution failed."} and then dividing by the number of turns across all analyzed rollouts.

\textbf{Failed API call give up rate} is computed via the following algorithm:
\begin{enumerate}
    \item Track an overall count of failed API calls (initially 0)
    \item Track an overall count of recovered API calls (initially 0)
    \item For each rollout:
    \begin{enumerate}
        \item Track the set of all failed AppWorld API endpoints that have not been successfully retried (initially the empty set)
        \item Track a count of failed API calls for this rollout (initially 0)
        \item Track a count of recovered API calls for this rollout (initially 0)
        \item For each sequential turn:
        \begin{enumerate}
            \item If the turn results in an execution error (as communicated by the environment response)
            \begin{enumerate}
                \item Add all attempted AppWorld API endpoints from that turn to the set if not already present.
                \item Add the number of endpoints newly added to the tracked set to the count of the rollout's failed API calls.
            \end{enumerate}
            \item Otherwise, if the turn did not result in an execution error
            \begin{enumerate}
                \item Remove all AppWorld API endpoints from that turn from the tracked set
                \item Add the number of removals from the tracked set to the count of successfully retried API endpoints.
            \end{enumerate}
        \end{enumerate}
    \end{enumerate}
    \item Add the rollout's count of failed API calls to the overall count of failed API calls
    \item Add the rollout's count of recovered API calls to the overall count of recovered API calls
    \item \textit{The failed API call give up rate} is then computed as
    \[
        \frac{\text{(overall \# failed API calls)} - \text{(overall \# recovered API calls)}}
             {\text{(overall \# failed API calls)}}
    \]
\end{enumerate}

\textit{Note: This rate is not normalized by the number of turns.}

\textbf{Frequency of calls to \textit{show\_api\_doc} per rollout} is calculated by counting the number of calls to the function \textit{apis.api\_docs.show\_api\_doc} in the code blocks of the agent's output.
This total number of occurrences is then normalized by the number of analyzed rollouts.


\textbf{Frequency of \textit{`assum(e,ed,ing)'} per rollout} is calculated by counting the number of occurrences of the strings \textit{`assume'}, \textit{`assumed'}, \textit{`assuming'} in the content of each agent turn output.
This total number of occurrences is then normalized by the number of analyzed rollouts.

Note: the string \textit{`assumed'} is only counted once rather than twice for both the substring \textit{`assume'} and the string \textit{`assumed'}.

\textbf{Frequency of \textit{`dummy'} per rollout} is calculated by counting the number of occurrences of the string \textit{`dummy'} in the content of each agent turn output.
This total number of occurrences is then normalized by the number of analyzed rollouts.



\subsection{Qualitative comparison (early vs. late in training) via summarized rollouts}

\cref{fig:before_after_summaries} analyzes three independent and identically distributed (i.i.d.) rollouts of a task early in training (example in \cref{sec:rollout_2a163ab_1_early}) and compared them to three i.i.d. rollouts late in \ours training (\cref{sec:rollout_2a163ab_1_late}).
The results are remarkably consistent even across multiple rollouts: without extensive training, the agent frequently makes poor assumptions, uses dummy values, and makes erroneous API calls.
After training, the agent learns to correct for all of these behaviors, significantly reducing the frequency of each of them.
%

%

\input{figures/before_after_rollout_summaries}

\clearpage
\section{PPO in a POMDP}
\lblsec{ppo_pomdp}
Following the POMDP definition in \refeq{pomdp_prob} with objective \refeq{POMDP}, the equivalent PPO objective for \refeq{PPO_bandit} is
\begin{equation}
    L_\theta^\text{bandit}(\s_0, \c) = \mathbb{E}_{\x \sim \rho_\psi(\cdot | \s_0, \c)}\left[\min\left(\prod_{t \in a(\x)}\frac{p_\theta(x_t | \c, x_{1:t-1})}{p_\psi(x_t | \c, x_{1:t-1})}A(\s_0, \c, \x), g_\epsilon(A(\s_0, \c, \x))\right)\right], \lbleq{PPO_POMDP_bandit}
\end{equation}
and for \refeq{PPO_MDP}
\begin{equation}
    L_\theta^\text{POMDP}(\s_0, \c)=\mathbb{E}_{\x \sim \rho_\psi(\cdot | \s_0, \c)}\left[\frac{1}{|a(\x)|}\sum_{t \in a(\x)}\min\left(\frac{p_\theta(x_t | \c,x_{1:t-1})}{p_\psi(x_t | \c,x_{1:t-1})}A(\s_0,\c,\x), g_\epsilon(A(\s_0,\c,\x))\right)\right]. \lbleq{PPO_POMDP_MDP}
\end{equation}

\section{Evaluated Methods: Details}
\subsection{Ground Truth Supervised Fine-Tuning (SFT-GT)}
\lblsec{sft-gt}
The AppWorld Engine environment provides fully programmatic solutions to train and dev set tasks for their stated purpose of ensuring that tasks are solvable. We generated a  ReAct-style dataset from these ground truth solutions for supervised fine-tuning and subsequent model evaluation.

\textbf{Dataset construction.}
These solutions consist of a series of code blocks, each described by comments, that sequentially solve the task. We used each top-level statement or loop and it's associated comments as a single ReAct message from the agent. The comments are concatenated and serve as the `thinking' portion of the agent output while the comment-stripped code is included in the formatted python code-block. Since each ground truth code block is successful, an ``Execution successful.'' reply from the AppWorld environment is included after each code block. For each task in the train split, there was exactly one example in the SFT-GT dataset.

\textbf{Fine-tuning and checkpoint selection.}
When fine-tuning with SFT-GT, performance deteriorates for all splits other than train. Notably, initially during the course of the fine-tuning run, performance on all splits including train initially deteriorates. While the train set performance ultimately recovers to near perfect, the dev and test split performance do not meaningfully improve after the initial degradation. Performance for dev, test\_normal, and test\_challenge are reported on an epoch after the train-performance recovery.

\textbf{Performance commentary.}
Notably, the construction of some of these solutions is intentionally not possible without interaction with the AppWorld environment. For example, in one task the content of a text message must be read and interpreted in natural language to send the proper list of movie recommendations in reply. This means that fully-specified, open control programmatic solutions must use a priori knowledge of AppWorld state to be constructed. As such, it is reasonable we would not see performance improvement outside of the train set when fine-tuning on this dataset.

Qualitatively, the train set's performance fall-off and subsequent improvement seems to be driven by the agent switching from attempting to use available environment interaction to solve a task to simple memorization of solution steps. On train tasks, the interim agent during the interpolation between these two model states dysfunctionally tries to apply an imperfect memorization of the task with insufficient environment discovery and recovery capabilities.

\subsection{Rejection-sampling Fine-Tuning (RFT)}
\lblsec{rft}

\textbf{Dataset rejection sampling.}
To generate a rejection-sampled dataset for fine-tuning, we ran train-set rollouts using the base Qwen32B model over a 20-step temperature sweep from 0.05 to 1.0 incrementing by 0.05. For each temperature and each of the 90 examples in the train set, we repeatedly ran rollouts until either the rollout attained reward of 1 (i.e. successfully completed the task) or 8-hours of attempts had passed. This totaled 1,613 successful rollouts across 87 of the 90 (96.7 \%) of training tasks. (Note: the 3 tasks that could not be completed by the base model, IDs 29caf6f\_1, 29caf6f\_2, 29caf6f\_3, were all from the same AppWorld scenario, which from the authors' qualitative review is potentially under-specified). 70 tasks had 20 distinct examples with the remaining 17 represented tasks having between 1 and 17 examples each.

\textbf{Fine-tuning and checkpoint selection.}
During the fine-tuning step on this dataset the training loss approaches 0. For a selection of epochs, 5 full rounds of evaluation were performed on the validation (dev) split and the epoch with the highest mean dev set performance was chosen for Test Normal and Test Challenge evaluation (epoch 30 out of 100). In epochs after 30, loss converges terminally towards 0 and we see over-fitting to the training set as the validation loss starts to go down.

\textbf{Multi-iteration RFT.}
In line with {RFT~\cite{yuan2023scaling}}, we attempted a further iteration of rejection sampling and fine-tuning using the epoch 30 model to create a new rejection sampling dataset. Notably, while this model was able to generate a rejection sampling dataset with 88 rather than 87 of the train tasks solved, continuing to training the model did not result in significantly improved model performance.

\textbf{Performance commentary.}
Notably, after one pass of RFT a 32B parameter model attains approximately the previous SOTA performance from GPT-4o, although a meaningful performance gap still remains relative to the explored RL methods.

\subsection{Expert Iteration (EI)}
\lblsec{ei}

\textbf{EI training process.}
The EI training process was meant to mirror as many of the hyperparameter choices from the RL evaluation as possible. The number of rollouts per task and number of tasks sampled at each iteration were held consistent with those in the RL training runs. The two key changes made for EI were: 1) Only rollouts that obtained return $1.0$ were retained for training and 2) cross entropy fine-tuning loss was used instead of the policy gradient loss.

\textbf{Performance commentary.}
The EI algorithm demonstrated notable stability, achieving results comparable to certain RL variants, such as GRPO. Beyond its simplicity, EI offers increased robustness to off-policy data. In our experiments, EI remained stable even when training on stale experience (e.g., asynchronous rollout collection), conditions that often cause RL methods to diverge.
This suggests that versions of EI augmented with aggressive rollout filtering and search (to maximize the probability of finding successful trajectories) can achieve even better performance.
At the same time, in our experiments EI methods showed higher degree of overfitting to the training data compared to RL approaches.

\subsection{DPO-MCTS}
\lblsec{dpo-mcts}
Our implementation of DPO using MCTS is a simplified version of Agent Q~\cite{putta2024agent}. Similar to Agent Q we collect action preference pairs using MCTS; however we do not rely on a separate LLM critic heuristic for node expansion. Specifically, for AppWorld we select a node (i.e. interaction) to expand based only on an Upper Confidence Bound (UCB), sample two possible policy interactions, roll out one out to completion, and back propagate the return through the tree. We continue this process for up to 30 iterations, then collect preference pairs where the Q value exceeds a given threshold and add them to a replay buffer.

Due to the long horizon of AppWorld tasks and the resources required for each rollout, we only attempted an offline approach where preference pairs are stored in a persistent offline replay buffer.

\subsection{PPO with a Learned Critic}
\lblsec{ppo-critic}
We implemented and evaluated PPO algorithm, which can be seen as a version of \ours with a learned critic. Per-token value estimates predicted by the parameterized value function $V_{\psi}(\c, x_{1:t})$ replace the rollout-wise Monte Carlo baseline estimates (e.g. as in \refeq{lno}). We use Generalized Advantage Estimation (GAE)~\cite{schulman2015high} to compute per-token advantages.

The design space for PPO implementations in LLM post-training is extensive. The learned value function can be implemented in multiple ways:
\begin{itemize}[noitemsep, topsep=0pt, parsep=4pt, partopsep=0pt]
    \item A separate copy of the base model with a value head,
    \item A smaller LLM, potentially bootstrapped from an existing reward model, as in Tulu 3~\cite{lambert2024t},
    \item A second set of LoRA weights on top of the base model (requiring an additional full forward/backward pass), or
    \item A value head added to the policy parameterization.
\end{itemize}

We opted for the latter option due to memory and throughput considerations, although exploration of different design choices is an interesting research direction.

Our PPO implementation uses the same base hyperparameters as \ours. We use non-discounted returns ($\gamma=1.0$).
Additionally, we found that variance reduction provided by the discounting factor $\lambda_{\text{GAE}} < 1.0$ in GAE is counterproductive. Training runs with $\lambda_{\text{GAE}} \in \{0.95,0.99,0.999\}$ quickly diverged, whereas runs with $\lambda_{\text{GAE}}=1.0$ were the most stable.
Note that without discounting, the GAE estimator simply reduces to $A(\c,x_{1:t}) = R(\c, \x) - V_{\psi}(\c,x_{1:t})$.

With $\lambda_{\text{GAE}} \ll 1$ the advantage estimates bootstrap from the value predictions later in the (potentially very long) rollout, thus amplifying the critic's errors and introducing additional bias. On the other hand, non-discounted GAE estimates have large variance. In our experiments, the mean squared error (MSE) for value predictions mostly stayed above $0.01$, highlighting the inherent difficulty of value estimation further discussed in~\cite{kazemnejad2024vineppo}.

Additional parameters and tweaks:
\begin{itemize}[noitemsep, topsep=0pt, parsep=4pt, partopsep=0pt]
    \item $V_{\psi}$ is parameterized by a 3-layer MLP with layer sizes $[(5120\times3072), (3072\times2048), (2048\times1)]$ and ReLU activations and receives the last hidden state $h$ ($\text{dim}=5120$) of the policy network  as input.
    \item $V_{\psi}$ is trained with L2 loss with the coefficient that linearly decays from $0.1$ to $0.001$ over 200 iterations.
    \item We pre-train (warmup) the critic on the trajectories of the base model for 10 iterations (2400 rollouts) in order to obtain better advantage estimates at the start of training.
    \item Value loss gradients are not propagated to the policys LoRA weights, as doing so destabilizes training.
    \item Value predictions were clipped to $[0.0, 1.0]$ in advantage calculations, as returns in AppWorld always fall within this range.
\end{itemize}

Value function-based PPO suffers from biased advantage estimates, training instability, and hyperparameter sensitivity, ultimately underperforming Monte Carlo-based methods in our tests. On the other hand, the critic allows us to obtain token-level advantages, potentially improving credit assignment. With relaxed memory and compute constraints these methods might offer significant advantages and warrant further exploration.

%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%


\section{Additional training details}
\lblsec{appendix-training-details}

\textbf{Interactive agent configuration.}
For all fine-tune experiments, we use Qwen2.5-32B-Instruct~\cite{qwen2.5} as the base model and fine-tune with LoRA~\cite{hu2022lora}.
We apply LoRA to the self-attention modules (query, key, value, output) and MLP.
We use a LoRA rank $r = 16$ and $\alpha = 32$. To reduce memory footprint while training, we use Cut Cross-Entropy (CCE) to prevent materializing the logits for all tokens into global memory~\cite{Wijmans2024xentropy}. We use the same ReAct style prompt~\cite{yao2022react} for all evaluated agents, which includes one in-context example of a successful task execution.
Agent observations $x_{i \notin a(\x)}$ consist of the code execution results, such as API call outputs and any exception traces, followed by the original task instruction.
For each turn, the agent is limited to 1500 output tokens, including chain-of-thought tokens and code. API responses exceeding 3K tokens are truncated, with a brief note indicating the truncation.

%

\textbf{Training reward.}
%
Each AppWorld task has a set of unit tests that evaluate whether the correct state changes were made without introducing any undesired changes. Our reward $R(\c, \x)\in [0,1]$ is simply the fraction of successfully passed unit tests.
%

\textbf{Training setup.}
We train on two NVIDIA H100 8-GPU nodes: one node for rollout generation and one for training steps.
For rollout generation, we use two vLLM~\cite{kwon2023efficient} servers (4 GPUs each).
For learning, we use a custom RL algorithm implementation based on torchtune~\cite{torchtune} and FSDP2~\cite{zhao2023pytorch}.
When computing the PPO gradient, we recompute the per-token log-probabilities under the generating policy instead of using values reported by vLLM.
\cref{fig:traincurve} shows a complete training run of LOOP.

\begin{figure*}[t]
    \includegraphics[width=1.0\textwidth]{figures/train_curves/training_curve_combined.pdf}
    \caption{
    Training curves for the best-performing training run with LOOP. In this experiment, training took place over 42 hours on two NVIDIA H100 8-GPU nodes.
    }
    \lblfig{traincurve}
\end{figure*}

\textbf{Training and evaluation parameters.}
We use a constant learning rate of $5 \times 10^{-5}$
%
and clip the gradient norm to $1$ in all experiments.
As a performance optimization, \ours removes rollouts with low advantage ($|\hat{A}^{(i,j)}| < 0.01$) before computing the gradient.

Each iteration starts with the generation of $K=6$ rollouts with temperature $1.0$ for 40 randomly sampled tasks, for a total of 240 rollouts.
%
We use only difficulty-1 and difficulty-2 tasks for training (including difficulty-3 tasks in training did not help and even hurt performance).
Since a few straggler rollouts can slow down rollout generation, we early-stop rollout collection when at least 4 rollouts for each task and 90\% of the total number of rollouts have been collected, similar to \citet{wijmans2020ddppo}.

We allow up to 40 interactions between the agent and the environment during training and up to 50 for evaluation.
Any episode that does not complete within this budget is considered a failure.

%
\subsection{Training Tasks}
\lblsec{train_tasks}
We train on a subset of the AppWorld train set, excluding difficulty 3 tasks.
This subset consists of 24 scenarios, with 3 minor variations (tasks) per scenario.
The complete set of our train scenarios is given below:

\begin{tabular}{cccccccc}
07b42fd &
229360a&
27e1026&
287e338&
692c77d&
82e2fac&
aa8502b&
 b7a9ee9\\
 c901732&
 ccb4494&
 ce359b5&
 e7a10f8&
 e85d92a&
 e3d6c94&
 d0b1f43&
 2a163ab\\
 60d0b5b&
6ea6792&
 29caf6f&
 cf6abd2&
 771d8fc&
 7d7fbf6&
 76f2c72&
 302c169
\end{tabular}

%
\section{Evaluation over multiple training runs for each method}

\begin{table*}
        \small
    \begin{tabular}{llccccccc}
    \toprule
    %
     & &  & \textbf{Strictly} & \textbf{Normalized} & \multicolumn{2}{c}{\textbf{Test Normal (Test-N)}} & \multicolumn{2}{c}{\textbf{Test Challenge (Test-C)}} \\ \cline{6-7} \cline{8-9}
     %
    \textbf{Type} & \textbf{Algorithm} & \textbf{Action} & \textbf{on-policy} & \textbf{reward} & \raisebox{-2pt}{TGC} & \raisebox{-2pt}{SGC} & \raisebox{-2pt}{TGC} & \raisebox{-2pt}{SGC} \\
    \midrule
    RL & RLOO & traj & \checkmark &  & 56.3 $\pm$ 3.1 & 33.2 $\pm$ 5.0 & 31.9 $\pm$ 3.9 & 15.2 $\pm$ 3.0 \\
    %
    RL & GRPO & token  & \checkmark\footnotemark[3] & \checkmark & 58.0 $\pm$ 2.3 & 36.6 $\pm$ 5.1 & 38.1 $\pm$ 2.2 & 20.2 $\pm$ 2.6 \\ %
    RL & GRPO no kl & token & \checkmark\footnotemark[3] & \checkmark & 60.2 $\pm$ 2.8 & 36.7 $\pm$ 4.2 & 39.3 $\pm$ 3.2 & 20.5 $\pm$ 2.0 \\
    %
     %
    RL & \ours (bandit) & traj & & & 48.9 $\pm$ 7.6 & 27.5 $\pm$ 6.9 & 27.3 $\pm$ 5.9 & 11.5 $\pm$ 4.8 \\  %
    RL & \ours (turn) & turn & & & 61.7 $\pm$ 2.6 & 41.2 $\pm$ 4.1 & 38.0 $\pm$ 3.0 & 22.2 $\pm$ 4.3 \\ %
    %
    RL & \textbf{\ours (token)} & token & & & \bf{66.4 $\pm$ 4.8} & \bf{47.5 $\pm$ 5.3} & \bf{41.7 $\pm$ 3.4} & \bf{24.4 $\pm$ 2.5} \\ %
    RL & \ours RwNorm (token) & token  & & \checkmark & 59.6 $\pm$ 3.8 & 42.1 $\pm$ 5.4 & 35.0 $\pm$ 5.0 & 18.6 $\pm$ 3.5 \\ %
    \bottomrule
    \end{tabular}
    \caption{Evaluation results \textit{averaged over three training runs} per algorithm, with mean and standard deviation reported. The relative performance trends align with \cref{tbl:eval}, which shows performance for the best checkpoint of the best training run. \ours (bandit) shows high variance due to training instability, while \ours (token) significantly outperforms other methods.}
    \lbltbl{eval-over-train-runs}
\end{table*}

Results presented in \cref{tbl:eval} reflect performance for the best checkpoint of the best training run (per algorithm) across repeated evaluation runs.
In contrast, the results presented in \cref{tbl:eval-over-train-runs} show the performance for each algorithm across three training runs.
For each training run, we select the best checkpoint according to the dev set performance and evaluate the checkpoint 5 times for a total of 15 evaluation runs per algorithm.
While the numbers are overall lower in \cref{tbl:eval-over-train-runs} than in \cref{tbl:eval} as we are no longer selecting from the best training run, the relative performance is largely the same.
Notably, \ours (bandit) has particularly high variance across training runs because training was unstable, would result in clipped importance weights (\cref{sec:preliminaries}), and would diverge before the performance could otherwise peak.

\clearpage
\section{Additional rollout examples}

In this section, we visualize rollouts from dev set tasks generated using our agent (\ours).

\subsection{Example rollout: success}
\lblsec{example-rollout-success}

\begin{minipage}[t]{0.65\textwidth}
    \vspace{-8ex}
    \textbf{Task instruction}: \textit{Kristin paid for my grocery recently as my payment cards were not working at the time. Send them the owed money with a description note ``Groceries'' as per my phone text conversation, and then send them a phone text message, ``It is done.''.}

    \textbf{Full rollout}: \cref{sec:example-rollout-success:rollout}
\end{minipage}
\hfill
\begin{minipage}[t]{0.29\textwidth}
    \begin{tabular}{lr}
        \toprule
        \textbf{Statistic} & \textbf{Value} \\
        \midrule
        Data split & Dev \\
        Difficulty & 3 \\
        Success & Yes \\
        Tests passed / total & 10 / 10 \\
        Num interactions & 17 \\
        \bottomrule
    \end{tabular}
\end{minipage}

\begin{tabular}{ll}
Turn & Action\\
\hline
1 & \icondocs \iconspacer \textblue{creates high level task execution plan, looks up docs for Phone app to get the text conversation}\\
2-3 & \icondocs \iconspacer \textblue{looks up docs for phone.search\_text\_messages, phone.login}\\
4 & \iconcallfail \iconspacer  \textcall{logs into Phone using phone.login but misspells the phone number}\\
5 & \iconcall \iconspacer  \textcall{logs into Venmo using venmo.login with correct phone number}\\
6 & \iconcallfail \iconspacer  \textcall{searches for text messages involving phone number ``Kristin''}\\
7 & \icondocs \iconspacer  \textcall{looks for Kristin's phone number, looks up docs for search\_contacts}\\
8 & \iconcall \iconspacer  \textcall{searches for contact with query ``Kristin'' using phone.search\_contacts}\\
9 & \iconcall \iconspacer  \textcall{searches text messages with Kristin's phone number using phone.search\_text\_messages, sees \$54 grocery payment}\\
10-11 & \icondocs \iconspacer \textblue{looks up docs for Venmo app, venmo.search\_users}\\
12 & \iconcall \iconspacer  \textcall{calls venmo.search\_users to search for Kristin's account in Venmo}\\
13 & \icondocs \iconspacer \textblue{looks up docs for venmo.create\_transaction}\\
14 & \iconcall \iconspacer  \textcall{pays Kristin \$54 with the description ``Groceries'' using venmo.create\_transaction}\\
15 & \icondocs \iconspacer \textblue{looks up docs for phone.send\_text\_message}\\
16 & \iconcall \iconspacer  \textcall{sends text message ``It is done.'' to Kristin using phone.send\_text\_message}\\
\end{tabular}

\clearpage
\subsection{Example rollout: failure}
%
\lblsec{example-rollout-failure:description}

\begin{minipage}[t]{0.65\textwidth}
    \vspace{-8ex}
    \textbf{Task instruction}: \textit{Download all the songs from my Spotify song library that I have liked.}

    \textbf{Full rollout}: \cref{sec:example-rollout-failure:rollout}

    \textbf{Failure reason}: Fails to download only the liked songs in the users library, and instead downloads all liked songs including those not in the user's library.
\end{minipage}
\hfill
\begin{minipage}[t]{0.29\textwidth}
    \begin{tabular}{lr}
        \toprule
        \textbf{Statistic} & \textbf{Value} \\
        \midrule
        Data split & Dev \\
        Difficulty & 1 \\
        Success & No \\
        %
        Tests passed / total & 4 / 6 \\
        %
        Num interactions & 10 \\
        \bottomrule
    \end{tabular}
\end{minipage}

\begin{tabular}{ll}
Turn & Action\\
\hline
1 & \icondocs \iconspacer \textblue{creates plan on how to start, then looks up docs for spotify.login}\\
2 & \iconcall \iconspacer \textblue{calls spotify.login}\\
3 & \icondocs \iconspacer  \textcall{looks up apis for spotify}\\
4 & \icondocs \iconspacer  \textcall{looks up docs for spotify.show\_liked\_songs}\\
5 & \iconcall \iconspacer \textblue{calls spotify.show\_liked\_songs and gathers all liked songs with credentials}\\
6 & \icondocs \iconspacer \textblue{looks up docs for spotify.download\_song}\\
7 & \iconcallfail \iconspacer calls spotify.download\_song for all liked songs, but included already downloaded songs \\
8 & \icondocs \iconspacer \textblue{looks up docs for spotify.show\_downloaded\_songs}\\
9 & \icongear \iconspacer gets all downloaded songs and downloads songs that are not already downloaded \\
10 & \iconcall \iconspacer \textblue{calls supervisor.complete\_task}\\
\end{tabular}

\newpage

\section{Full Rollouts (\ours)}

The remainder of this document contains \textit{only} printouts of full trajectories.

\subsection{Rollout 0 of task \texttt{2a163ab\_1}early in training} \lblsec{rollout_2a163ab_1_early}
The following is a rollout on AppWorld training task \texttt{2a163ab\_1} at gradient step 38 (near the beginning of training):

%
\input{rollouts/xjt8ymm7gd.step000038.2.3.0.new}

\subsection{Rollout 1 of task \texttt{2a163ab\_1}early in training} \lblsec{rollout_2a163ab_1_early_1}
The following is a rollout on AppWorld training task \texttt{2a163ab\_1} at gradient step 38 (near the beginning of training):

%
\input{rollouts/xjt8ymm7gd.step000038.2.3.1.new}

\subsection{Rollout 0 of training task \texttt{2a163ab\_1} after training} \lblsec{rollout_2a163ab_1_late}
The following is a rollout on AppWorld task \texttt{2a163ab\_1} at
at gradient step 530 (near the best-performing checkpoint):


\input{rollouts/xjt8ymm7gd.step000530.0.0.0.new}

\subsection{Rollout of dev task \texttt{530b157\_1} after training}
\lblsec{example-rollout-success:rollout}
\input{rollouts/4ashm99hq6.530b157_1}

\subsection{Rollout of dev task \texttt{3ab5b8b\_2} after training. Fails to download only the liked songs in the users library, and instead downloads all liked songs including those not in the user's library.}
\lblsec{example-rollout-failure:rollout}
\input{rollouts/h5kjbn42sw.3ab5b8b_2}

%
%


%

%

%
%

\end{document}


%
%
%
%
%
%
%
%
%
%
%
%
%

