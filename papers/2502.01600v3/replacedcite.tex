\section{Related Work}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%

%

\textbf{LLM agents.}
Pretrained, instruction-tuned LLMs have demonstrated an ability to interact with external software environments by invoking structured APIs, both for information retrieval____ and for acting in stateful external environments____.
To improve performance in this domain, further works introduced structured workflows that combine reasoning, acting, and reflection steps____, and  interaction with code interpreters____. Others apply supervised fine-tuning on datasets of agent trajectories____.

\textbf{Reinforcement learning for LLMs.}
RL was first used to train LLMs in the setting of reinforcement learning from human feedback (RLHF)____.
These works used proximal policy optimization (PPO)____ to train an LLM policy based on a reward model inferred from human preferences.
RLHF with PPO uses up to four separate LLMs during training: a reward model, trained policy, reference policy, and critic.
%
____ showed that the much simpler REINFORCE Leave-One-Out (RLOO) algorithm____ performs competitively.
RLOO avoids the need for the reference and critic LLMs using on-policy updates and using multiple rollouts from the same query for a sampling-based advantage estimate instead of a learned critic.
Our method, \ours, is a generalization of RLOO that allows for policy updates to drift off-policy using the trust region defined in PPO.
This enables reusing rollouts and a looser loop between rollout collection and policy updates.

PPO and its variants have been used to train LLMs to perform reasoning and static code generation using programmatic reward functions. %
GRPO____ replaces the critic in PPO with baselines computed from a collection of samples from the same query.
VinePPO____ estimates per-step advantages via rollouts branched from each reasoning step of the training rollout.
Other RL algorithms for training reasoning LLMs depart from the policy gradient framework, alternating between generating datasets of filtered rollouts and supervised training on these datasets____.
We compare to these methods and show that a straightforward combination of PPO with a leave-one-out estimate performs significantly better in training IDAs.
%

\textbf{Reinforcement learning for LLM agents.}
RL has been used to train stateful multi-turn agents in text-based games____, web shopping and navigation  environments____, mobile device control____, and embodied environments____.
Most closely related to our approach are several works that train LLM policies with RL in WebShop:
____ apply REINFORCE with a learned value baseline,
ArCHer____ uses a hierarchical approach that combines off-policy and on-policy training, and
AgentQ____ combines tree search with direct policy optimization (DPO).
Our work targets AppWorld, which is substantially more complex than the WebShop environment.
While the goal of all WebShop scenarios is to purchase a described item from a simulated site with 8 actions (with at most 1 parameter per turn), AppWorld tasks leverage 9 apps, 457 API endpoints with up to 17 parameters, and require non-trivial logic.
\ours outperforms both REINFORCE-based and DPO-based baselines in this more challenging environment.
\ours was discovered independently in the context of Text-to-Image Diffusion Fine-tuning by ____.
%
%
%


%
%
%
%
%
%

%
%


%

%
%
%
%
%
%
%
%

%

%
%

%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%

%
%

%
%
%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%

%
%


%
%
%
%

%


%
%
%
%
%

%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

%

%
%
%

%
%
%

%
%

%
%

%



%
%

%
%

%
%
%
%
%

%
%

%
%

%

%
%

%
%
%
%
%
%