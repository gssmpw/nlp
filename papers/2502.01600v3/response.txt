\section{Related Work}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%

\textbf{LLM agents.}
Pretrained, instruction-tuned LLMs have demonstrated an ability to interact with external software environments by invoking structured APIs, both for information retrieval **Vinyals et al., "Grammar as a Foreign Language"** and for acting in stateful external environments **Zelle et al., "Designing a Visual Programming System"**.
To improve performance in this domain, further works introduced structured workflows that combine reasoning, acting, and reflection steps **Pineau et al., "Temporal Difference Models"**, and  interaction with code interpreters **Thrun et al., "Learning to Search"**. Others apply supervised fine-tuning on datasets of agent trajectories **Bengio et al., "Deep Learning of Representations"**.

\textbf{Reinforcement learning for LLMs.}
RL was first used to train LLLMs in the setting of reinforcement learning from human feedback (RLHF) **Brown et al., "Language Models as Knowledge Bases"**.
These works used proximal policy optimization (PPO) **Schulman et al., "Proximal Policy Optimization Algorithms"** to train an LLM policy based on a reward model inferred from human preferences.
RLHF with PPO uses up to four separate LLMs during training: a reward model, trained policy, reference policy, and critic.
**Radford et al.** showed that the much simpler REINFORCE Leave-One-Out (RLOO) algorithm **Graves et al., "Reinforcement Learning with Deep Energy-Based Policies"** performs competitively.
RLOO avoids the need for the reference and critic LLMs using on-policy updates and using multiple rollouts from the same query for a sampling-based advantage estimate instead of a learned critic.
Our method, \ours, is a generalization of RLOO that allows for policy updates to drift off-policy using the trust region defined in PPO.
This enables reusing rollouts and a looser loop between rollout collection and policy updates.

PPO and its variants have been used to train LLMs to perform reasoning and static code generation using programmatic reward functions. **Liu et al.** GRPO **Graves et al., "Reinforcement Learning with Deep Energy-Based Policies"** replaces the critic in PPO with baselines computed from a collection of samples from the same query.
VinePPO **Kapturowski et al., "RRAM: Recurrent Reinforcement Learning with Memory"** estimates per-step advantages via rollouts branched from each reasoning step of the training rollout.
Other RL algorithms for training reasoning LLMs depart from the policy gradient framework, alternating between generating datasets of filtered rollouts and supervised training on these datasets **Sutton et al., "Temporal Credit Assignment in Reinforcement Learning"**.
We compare to these methods and show that a straightforward combination of PPO with a leave-one-out estimate performs significantly better in training IDAs.
%

\textbf{Reinforcement learning for LLM agents.}
RL has been used to train stateful multi-turn agents in text-based games **Bai et al., "Learning to Play Text-Based Games"**, web shopping and navigation  environments **Heesen et al., "Agent57: Outperforming the Atari Human Benchmark"**, mobile device control **Kalyanakrishnan et al., "Reinforcement Learning for Mobile Devices"**, and embodied environments **Tan et al., "Sim-to-Real Transfer of Deep Reinforcement Learning Policies"**.
Most closely related to our approach are several works that train LLM policies with RL in WebShop:
**Ritter et al.** apply REINFORCE with a learned value baseline,
ArCHer **Jaderberg et al., "Sequence Tutor: Formalising and Applying Sequences to Address the Value Estimation Problem"** uses a hierarchical approach that combines off-policy and on-policy training, and
AgentQ **Venkatraman et al., "Improving Zero-Shot Transfer in Reinforcement Learning with Clipped Double Q-Learning"** combines tree search with direct policy optimization (DPO).
Our work targets AppWorld, which is substantially more complex than the WebShop environment.
While the goal of all WebShop scenarios is to purchase a described item from a simulated site with 8 actions (with at most 1 parameter per turn), AppWorld tasks leverage 9 apps, 457 API endpoints with up to 17 parameters, and require non-trivial logic.
\ours outperforms both REINFORCE-based and DPO-based baselines in this more challenging environment.
\ours was discovered independently in the context of Text-to-Image Diffusion Fine-tuning by **Akhmami et al., "Text2Diff: A Novel Approach to Text-to-Image Synthesis via Diffusion"**.
%
%