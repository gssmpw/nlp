\section{Related Work}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%

%

\textbf{LLM agents.}
Pretrained, instruction-tuned LLMs have demonstrated an ability to interact with external software environments by invoking structured APIs, both for information retrieval~\cite{nakano2021webgpt,schick2023toolformer} and for acting in stateful external environments~\cite{yao2020keep,yao2022webshop}.
To improve performance in this domain, further works introduced structured workflows that combine reasoning, acting, and reflection steps~\cite{yao2022react,shinn2024reflexion,kim2024language}, and  interaction with code interpreters~\cite{wang2024executable,yang2024intercode}. Others apply supervised fine-tuning on datasets of agent trajectories~\cite{chen2023fireact,qin2023toolllm,mitra2024agentinstruct}.

\textbf{Reinforcement learning for LLMs.}
RL was first used to train LLMs in the setting of reinforcement learning from human feedback (RLHF)~\cite{ziegler2019fine,stiennon2020learning,ouyang2022training}.
These works used proximal policy optimization (PPO)~\cite{schulman2017proximal} to train an LLM policy based on a reward model inferred from human preferences.
RLHF with PPO uses up to four separate LLMs during training: a reward model, trained policy, reference policy, and critic.
%
\citet{ahmadian2024back} showed that the much simpler REINFORCE Leave-One-Out (RLOO) algorithm~\cite{kool2019buy} performs competitively.
RLOO avoids the need for the reference and critic LLMs using on-policy updates and using multiple rollouts from the same query for a sampling-based advantage estimate instead of a learned critic.
Our method, \ours, is a generalization of RLOO that allows for policy updates to drift off-policy using the trust region defined in PPO.
This enables reusing rollouts and a looser loop between rollout collection and policy updates.

PPO and its variants have been used to train LLMs to perform reasoning and static code generation using programmatic reward functions. %
GRPO~\cite{shao2024deepseekmath,deepseekai2025deepseekr1} replaces the critic in PPO with baselines computed from a collection of samples from the same query.
VinePPO~\cite{kazemnejad2024vineppo} estimates per-step advantages via rollouts branched from each reasoning step of the training rollout.
Other RL algorithms for training reasoning LLMs depart from the policy gradient framework, alternating between generating datasets of filtered rollouts and supervised training on these datasets~\cite{singh2023beyond,havrilla2024teaching,yuan2023scaling}.
We compare to these methods and show that a straightforward combination of PPO with a leave-one-out estimate performs significantly better in training IDAs.
%

\textbf{Reinforcement learning for LLM agents.}
RL has been used to train stateful multi-turn agents in text-based games~\cite{narasimhan2015language,yao2020keep,carta2023grounding}, web shopping and navigation  environments~\cite{yao2022webshop}, mobile device control~\cite{bai2024digirl}, and embodied environments~\cite{zhai2024fine}.
Most closely related to our approach are several works that train LLM policies with RL in WebShop:
\citet{yao2022webshop} apply REINFORCE with a learned value baseline,
ArCHer~\cite{zhou2024archer} uses a hierarchical approach that combines off-policy and on-policy training, and
AgentQ~\cite{putta2024agent} combines tree search with direct policy optimization (DPO).
Our work targets AppWorld, which is substantially more complex than the WebShop environment.
While the goal of all WebShop scenarios is to purchase a described item from a simulated site with 8 actions (with at most 1 parameter per turn), AppWorld tasks leverage 9 apps, 457 API endpoints with up to 17 parameters, and require non-trivial logic.
\ours outperforms both REINFORCE-based and DPO-based baselines in this more challenging environment.
\ours was discovered independently in the context of Text-to-Image Diffusion Fine-tuning by \citet{gupta2025simple}.
%
%
%


%
%
%
%
%
%

%
%


%

%
%
%
%
%
%
%
%

%

%
%

%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%

%
%

%
%
%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%

%
%


%
%
%
%

%


%
%
%
%
%

%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

%

%
%
%

%
%
%

%
%

%
%

%



%
%

%
%

%
%
%
%
%

%
%

%
%

%

%
%

%
%
%
%
%
%