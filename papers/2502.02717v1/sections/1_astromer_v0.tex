\section{Astromer 1}\label{sec:astromer0}
The initial version of Astromer \citep{astromer} adapted the BERT text model from natural language processing \citep{devlin2018bert}. While both light curves and text are sequential data, light curves pose a unique challenge due to the inherent irregularities in their sampling. Moreoever,  instead of a discrete vocabulary, we work with continuous magnitudes for each token. 

Despite the differences between BERT and Astromer, the high-level approach to training remains similar, as it leverages a self-supervised task. 
Specifically, we employ a masking strategy that obscures portions of the light curve, allowing the model to predict the missing magnitudes. This technique, inspired by BERT’s word masking in sentences, enables the model to learn meaningful representations without relying on human-annotated labels.

This section revisits the pipeline previously introduced in Astromer 1. While much of the content has been explained before, we present it here with a more refined and clearer explanation for enhanced understanding.


% =============================================================
% DATA PREPARATION ============================================
% =============================================================
\subsection{Data preparation}\label{sec:data_preparation}
Astromer uses single-band light curves $\{x_i\}_{i=0}^{N-1}$ with $N$ as the number of samples. Each sample is represented as a set of tuples $x_i = \{(t_l, m_l, e_l)\}_{l=0}^{L_i-1}$. Here, $t_l$ denotes the observation time in modified Julian date (MJD), $m_l$ represents the magnitude, and $e_l$ corresponds to the magnitude uncertainty. The maximum number of observations $L_i$ varies across samples, resulting in a variable-length dataset. 

We fixed a maximum length of 200 observations to create the network’s input. During pretraining, we sample different windows of 200 observations per epoch, allowing the model to see most of the light curve sequence in small, fixed chunks. Shorter light curves are zero-padded to a fixed length of 200 observations.

After constructing the windows, we normalize their values. Specifically, we subtract $\bar{x_i} = (\bar{t}, \bar{m}, \bar{e})$ the mean value of each light curve, producing zero-mean samples with non-scaled amplitude. Our experiments have shown that this normalization step is essential for the model to converge effectively. Other options may be insufficient to produce valuable embeddings.


% =============================================================
% INPUT EMBEDDING  ============================================
% =============================================================
\subsection{Input Embedding}\label{sec:input_embedding}
Unlike language models, Astromer does not have a fixed vocabulary of tokens. Instead, the input consists of a sequence of continuous magnitude values, each paired with its corresponding observation time in MJD. We do not consider the uncertainties in Astromer's input.

To create a single input embedding, we transform each time and magnitude scalar into vectors. To encode observation times, we apply an adapted positional encoder (PE) that scales the angular frequencies $\omega_j$ using the observation time $t_l$, capturing the irregular sampling in the temporal representation.
\begin{equation}\label{eq:pe}
\text{PE}_{\textit{j}, t_{l}} = \left\{
        \begin{array}{ll}
            \sin{(t_{l}\cdot\omega_{\textit{j}})} & \quad j \text{ is even} \\
            \cos{(t_{l}\cdot\omega_{\textit{j}})} & \quad j \text{ is odd}
        \end{array}
    \right.
\end{equation}
In Eq. \ref{eq:pe}, $\textit{j} \in [0,...,d_{pe}-1] $, where $d_{pe}=256$ is the PE dimensionality and $\omega_\textit{j}$ is the angular frequency defined as,
\begin{equation}\label{eq:angular_freq}
    \omega_{\textit{j}} = \frac{1}{1000^{2\textit{j}/d_{pe}}}.
\end{equation}

For the magnitudes, we linearly project each magnitude value into a vector of size $d_{pe}=256$. The weights used to transform magnitudes are initialized from a Normal distribution and subsequently learned during training. This shared set of weights is applied across all observations.

The final input embedding $\rm{X}\in\mathbb{R}^{L\times 256}$ is the sum of the PE and transformed magnitudes, $\rm{X} = PE + \boldsymbol{m}\rm{W}^{\top}$, where $\boldsymbol{m} \in \mathbb{R}^{L\times 1}$ is the magnitudes vector and $\rm{W} \in \mathbb{R}^{256\times 1}$ are the weights for transforming scalars into vectors.

% =============================================================
% PROBED AND MASKING ==========================================
% =============================================================
\subsection{Probed and Masking}
The key to Astromer learning a good representation is to pretrain it to predict unseen observations along the light curve sequence. The probed subset consists of magnitudes designated for the model to predict. However, these values are excluded when calculating attention weights. We randomly select $50\%$ of the total observations per window to constitute the probed subset. This subset is denoted by a binary mask vector, where the 1's correspond to the probed magnitudes, and zero otherwise.

In the self-attention mechanism, the attention weights for the probed subset are set to zero using masking. This design encourages the model to leverage the surrounding context to predict the probed magnitudes. During inference, however, masking is not applied. To prevent the model from over-relying on the masked observations during training, we adopt a the following strategy. We assign $10\%$ of visible observations and $10\%$ of random observations in the probed subset. As a result, the actual masked portion is reduced to $30\%$, while the probed subset still corresponds to the initial $50\%$. This approach mitigates the risk of the model learning a direct identity mapping and improves its robustness to noise. Figure \ref{fig:probed_masking} illustrates the composition of the final subsets,  with the probed subset size fixed at $50\%$ of the observations.
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.30]{figures/astromer_0/masking_process.pdf}
    \caption{Observation subsets. In our pretraining strategy, $50\%$ of the observations are designated as probed. The probed subset is evaluated in the loss function to optimize Astromer's parameters. Its composition emphasizes predicting hidden elements (with $30\%$ masked overall) while mitigating the risk of the model over-relying on the masked observations ($10\%$ visible and $10\%$ with random magnitudes). This approach ensures the model pays attention to current observations without overfitting to them.}
    \label{fig:probed_masking}
\end{figure}

% =====================================================
% ENCODER  ============================================
% =====================================================
\subsection{Encoder}
The encoder comprises a sequence of attention blocks connected in series. The first block processes the input embeddings described in Sect. \ref{sec:input_embedding} and a binary mask matrix that specifies which observations to exclude from the attention mechanism. Subsequent blocks take as input the output of the preceding attention block as shown in Fig. \ref{fig:astromer_0}.
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.45]{figures/astromer_0/astromer_0.pdf}
    \caption{Astromer 1 architecture diagram. The input embedding is the sum of PEs for times and linearly transformed magnitudes. In Astromer 1, the encoder comprises $M=2$ blocks with $H=4$ attention heads, each having $64$ units. The outputs of the attention heads are concatenated and processed through a feed-forward network. The final embedding is derived from the output of the last attention block.}
    \label{fig:astromer_0}
\end{figure}

Astromer 1 has two attention blocks, each containing four heads with 64 units. The outputs of the heads are concatenated, normalized, and combined through a fully connected layer, one hidden layer of 128 units, and a hyperbolic tangent activation. 

Within each head, the attention values are computed from the similarity matrix derived using the Query (Q), Key (K), and Value (V) matrices, normalized by the square root of $d_k=256$, the model’s embedding size. Note that Q, K, and V come from a linear transformation of the input embedding and have different values for each head and block.
\begin{eqnarray}\label{eq:att_weights}
    \label{eq:mask_selfatt}
    \rm{Q}&=& \rm{X}\rm{W}_{query}^{\top}\hspace{6mm} \nonumber
    \rm{K} \hspace{2mm}=\hspace{2mm} 
    XW_{key}^{\top}\hspace{6mm}\rm{V} \hspace{2mm}=\hspace{2mm} \rm{X}\rm{W}_{value}^{\top}\\
    \rm{W}_{att} &=& \text{Softmax} \left ( \frac{\rm{Q}\rm{K}^{\top} -\infty\text{M}}{\sqrt{d_{k}}}\right )\\
    \rm{Z} &=& \rm{W}_{att}\rm{V}\nonumber
\end{eqnarray}
In Eq. \ref{eq:att_weights}, the mask matrix $\rm{M}$ prevents the masked subset of probed observations from contributing to the attention values. When $\rm{M}=1$, the argument of the softmax function is effectively negative infinity, resulting in a zero attention weight.

In Astromer 1, the output of the last block serves as the final embedding. This matrix has two functions: reconstructing magnitudes during pretraining and serving as the embedding for downstream tasks.

% =====================================================
% PRETRAINING TASK ====================================
% ===================================================== 
\subsection{Pretraining Task}
We pretrain the model to predict the magnitudes of the probed subset in each input sequence. This is achieved by passing %$\boldsymbol{z}\in\mathbb{R}^{200\times256}$, 
the output embedding from the last attention block, through a fully connected network with no hidden layers or activation. The result is a vector of estimated magnitudes, $\boldsymbol{\hat{x}} \in \mathbb{R}^{200\times 1}$, providing the reconstruction for each time point according to its related embedding.

We constraint the loss function to compute the root-mean-square error on the probed subset only:
\begin{eqnarray}\label{eq:loss}
    \mathcal{L}oss = \sqrt{\frac{1}{N-1}\sum_{i=0}^{N-1}\sum_{l=0}^{L-1} m_{il}(x_{il} - \hat{x}_{il})^2}.
\end{eqnarray}
In Eq. \ref{eq:loss}, $N$ represents the number of training samples, and $L=200$ represents the length of the windows. Thus, the masking vector $\boldsymbol{m}_{i}$ selectively includes errors from the probed subset.
