\section{Introduction}
%Light curve analysis has been a cornerstone in astronomy for characterizing stellar objects \citep{deb2009light}.
Light curve analysis is a cornerstone in astronomy for characterizing stellar objects \citep{deb2009light}. By analyzing the time-series data of luminosity variations, astronomers can extract statistical features that enable classification and identification tasks \citep{richards2011machine}.

Although traditional methods show success \citep{sanchez2021alert, chaini2024light}, the advent of foundational models presents fresh opportunities to gain insights into cosmic variability. Foundational models are deep neural networks trained using self-supervised techniques on extensive datasets \citep{bommasani2021opportunities, awais2023foundational}. These models acquire a thorough grasp of their domain, allowing for the creation of versatile representations applicable to various downstream tasks.

Classical techniques rely on manually engineered features \citep{debosscher2007automated, nun2015fats}, which may introduce biases or fail to capture intricate patterns \citep{2022MNRAS.517.3660P}. Foundational models, by processing large volumes of data, have the potential to reveal novel, precise structures in the data. However, this gain in representational power comes at the expense of reduced interpretability. %Automated embeddings should not be employed without scrutiny, and it is critical to ensure that the results generated by these models are well-understood \citep{bommasani2021opportunities}.

In 2023, we introduced Astromer, a self-supervised model designed to extract general-purpose embeddings from light curves 
\citep{astromer}. Trained on 1.5 million light curves, Astromer demonstrated consistent improvements in classification tasks compared to models trained directly on labeled datasets.

Other foundational models in astronomy, such as those employing contrastive learning \citep{lanusse2023astroclip, rizhko2024self, parker2024astroclip}, integrate multiple data modalities to create richer and more complex representations. While multi-modal learning is a promising avenue, it introduces additional complexity in model training and interpretation \citep{wang2024comprehensive}.

Astromer, by contrast, focuses solely on single-modality light curve data, leveraging its temporal structure without requiring alignment or integration steps across modalities. Instead of contrastive learning, Astromer employs magnitude imputation to handle missing values in time series, resulting in a simpler, yet highly effective model that achieves state-of-the-art performance without incurring high computational costs.

In this paper, we present Astromer 2, an improved version of our original model. For consistency, we use the same dataset from our initial publication and compare our latest model in classification task. Additionally, we delve into the embedding vectors and attention weights to better understand the modelâ€™s capabilities and performance as used in other works \citep{10.1093/mnras/stab2588}.

In this work, Sect. \ref{sec:astromer0} describes the main characteristics of the original version of Astromer, namely Astromer 1. Section \ref{sec:astromer1} introduces the improvements to the model. Sections \ref{sec:data} and \ref{sec:results} describe the data and the main results of this work, respectively. Finally, Sect. \ref{sec:conclusion} outlines the main findings and conclusions.