\section{Results}\label{sec:results}
The pretraining task of reconstructing the probed magnitudes is an essential component that allows the model to learn meaningful representations. Reconstructing probed magnitudes can be evaluated as a downstream regression task on labeled datasets, where the model's ability to predict the magnitudes can be assessed. Here we evaluate the potential of the representation in terms of regression.

Figure \ref{fig:trastromerv1} shows the learning curves from the pretraining of Astromer 2. The training took approximately 3 days using 4 A5000 GPUs. The model achieved $0.73$ R2 with a root mean squared error (RMSE) of 0.113 on the probed subset. Astromer 1 had an RMSE of 0.148, making Astromer 2 0.035 better in terms of reconstruction error.  

\begin{figure}
    \centering
    \includegraphics[scale=0.8]{figures/results/tr_astromer_2.pdf}
    \caption{Training and validation learning curves from the pretraining of Astromer 2. The dotted line represents the evaluation on the test set.}
    \label{fig:trastromerv1}
\end{figure}

\subsection{Downstream setup}
Similar to Astromer 1, this work evaluates the embeddings across various scenarios, while controlling the number of samples per class (SPC). When there are few SPC, the quality of the embeddings becomes crucial, as they must capture fundamental features that enable the model to make predictions based solely on the shape of the light curves, without relying on additional information.

Using the labeled datasets, we construct three training scenarios with limited data by randomly sampling 20, 100, and 500 SPC to assess performance on downstream tasks. When the number of available labels is insufficient, sampling is performed with replacement. To account for variability in our experiments, we generate three folds for each scenario. To ensure fairness across scenarios, we evaluate all models on a shared 3-fold test dataset consisting of 1000 SPC per fold.
Hence, the models trained on 20, 100, and 500 SPC were evaluated against this common dataset.
% Specifically, 100 SPC were selected for testing.

In the initial stage of the downstream pipeline, we performed finetuning. This involves loading the pretrained weights and adapting the encoder parameters to the target domain. Finetuning followed the same pretraining setup, predicting a random probed subset containing $50\%$ of the magnitudes per sequence.



\subsection{Finetuning}
Figure \ref{fig:ft_metrics} presents the RMSE results for each scenario. The reported values are calculated across the total number of observations without masking, allowing for a fair comparison as the error could be biased by the random masking selection. 
\begin{figure}
    \centering
    \includegraphics[scale=0.6]{figures/results/ft_metrics.pdf}
    \caption{[Left y-axis] Reconstruction error (dashed bars) measured by RMSE. [Right y-axis] Elapsed finetuning time in minutes. The x-axis represents each downstream scenario with training data of 20, 100, and 500 SPC.}
    \label{fig:ft_metrics}
\end{figure}
As shown in Fig. \ref{fig:ft_metrics}, finetuning the model on the Alcock dataset does not result in significant improvements, indicating that the pretrained model already captures most of the relevant information, despite the out-of-distribution modality discussed in Sect. \ref{sec:machovsatlas}. In contrast, finetuning on ATLAS leads to a notable improvement. Specifically, with 100 SPC, we observe a $23\%$ reduction in RMSE compared to the pretrained model. However, the performance improvement between 100 and 500 SPC is minimal, with only small variations.

The most computationally intensive scenario takes approximately three minutes to finetune, which is significantly faster than the days required for pretraining. While the time for MACHO increases with more samples, in ATLAS, we observe almost no variation between 20 and 100 SPC. This is because MACHO samples are longer than ATLAS samples, resulting in a more substantial increase in the number of windows as the number of SPC grows. This explains the more significant rise in computing time when going from 20 to 100 MACHO SPC.

\subsection{Visualizing reconstruction}
To gain insight into Astromer's representations, we visualize the attention values after finetuning. Figure \ref{fig:attention_vis} presents two examples showing the mean attention weights from each attention head, along with the mean across all attention heads. For visualization purposes, light curves associated with the average between heads were folded; however, Astromer does not receive folded inputs during processing. We display only the first attention block, as it is more intuitive. This contrasts with intermediate layers, where attention is computed over abstract embeddings.
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{figures/reconstruction/lightcurves_weights.pdf}
    \caption{Average attention per head for a random sample from each class of the Alcock dataset. The light curves were folded for better visualization. The final plot represents the mean across all attention heads. The top bar shows the normalized attention value, with higher values indicating better attention.}
    \label{fig:attention_vis}    
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{figures/reconstruction/lightcurves_weights-atlas.pdf}
    \caption{Analogous to Figure \ref{fig:attention_vis}, but showing results for ATLAS.}
    \label{fig:attention_vis_altas}    
\end{figure}
Figures \ref{fig:attention_vis} and \ref{fig:attention_vis_altas} show that each attention head focuses on different parts of the sequence. In particular, attention appears to focus most strongly at maximum and minimum brightness points suggesting these are key features for reconstruction. 


% \subsection{Forecasting}

\subsection{Classification}
Evaluating classification performance is crucial for assessing the overall effectiveness of Astromer, as it serves as a common benchmark for evaluating the quality of embeddings. After fine-tuning on labeled subsets of 20, 100, and 500 SPC, the encoder is frozen, meaning its weights are no longer updated

Astromer is used to extract the representation, which is fed to another classifier model. The same labeled data is used to train a classifier. In this setup, only the classifier section receives label-based gradients, while the encoder focuses exclusively on capturing dependencies between observationns.

Per-sample embeddings were generated by averaging the attention vectors from the encoder, with trainable parameters $\gamma_0,...,\gamma_m$ weighting the outputs of each block. The resulting embedding is then passed through a feed-forward network consisting of three hidden layers with 1024, 512, and 256 units, respectively, each using ReLU activation. A fully connected layer without activation predicts the final label, as shown in Figure \ref{fig:clf-arch}.
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{figures/astromer_1/skip_avg_clf.pdf}
    \caption{Classifier architecture used in Astromer 2. The input is a weighted embedding, and the classifier consists of a feed-forward network with three hidden layers and ReLU activation.}
    \label{fig:clf-arch}
\end{figure}

Figure \ref{fig:macho-clf} presents the F1 scores for the Alcock dataset across different scenarios. For comparison, it also includes F1 scores from Astromer 1, evaluated on the same dataset, using both weighted per-sample embeddings (v0) and non-weighted embeddings as detailed in \citet{astromer}.
\begin{figure}
    \centering
    \includegraphics[scale=0.65]{figures/astromer_1/alcock.pdf}
    \caption{Classification results on the MACHO-labeled test dataset. The x-axis labels represent the encoder architectures used. For a fair comparison, both Astromer 1 (A1) and Astromer 2 (A2) employ the same classifier, which uses the weighted average embedding. In contrast, the results for \cite{astromer} are based on a classifier trained using the average embedding from the last block, as detailed in our previous publication.}
    \label{fig:macho-clf}
\end{figure}

The improvements are most pronounced when evaluating classification performance on the ATLAS dataset. As depicted in Fig. \ref{fig:atlas-clf}, the new version of Astromer exhibits a significant advantage in generalizing to other datasets. With just 20 SPC, Astromer achieves a F1-score improvement of over $15\%$. Thus, Astromer's performance with 20 SPC surpasses the results previously reported 500 SPC.
\begin{figure}
    \centering
    \includegraphics[scale=0.65]{figures/astromer_1/atlas.pdf}
    \caption{Classification results on the ATLAS labeled test dataset. The description and methodology are consistent with those provided in Figure \ref{fig:macho-clf}.}
    \label{fig:atlas-clf}
\end{figure}

Weighted per-sample embeddings play a critical role by allowing the model to use intermediate representations instead of depending exclusively on the final one. During pretraining, the encoder focuses on reconstructing magnitudes from the last embedding, which could result in representations tailored to the reconstruction task rather than optimized for discrimination.

To examine the role of intermediate embeddings, we plot the gamma parameters after training the classifier on both the Alcock and ATLAS datasets. As shown in Fig. \ref{fig:gammaweights}, the weights assigned to intermediate embeddings are higher than those for the initial or final ones. This disparity becomes more pronounced as the number of training samples increases.
\begin{figure}
    \centering
    \includegraphics[scale=0.8]{figures/astromer_1/gamma_weights.pdf}
    \caption{Values of the trainable parameters that scale the contribution of intermediate embeddings. $\gamma_0$ represents the weight of the input embedding, while $\gamma_1, \ldots, \gamma_6$ correspond to the weights for the outputs of the attention blocks. These weights are optimized during the training of the classifier.}
    \label{fig:gammaweights}
\end{figure}

In Figures \ref{fig:alcock-emb} and \ref{fig:atlas-emb}, we present t-SNE projections of the embeddings associated with the first fold of the test set. These projections are computed from the average output of each attention block. Each output consists of a set of $200 \times 256$ vectors, which we reduce to a single $1 \times 256$ vector by averaging.
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{figures/results/tsne_alcock.png}
    \caption{t-SNE projection (perplexity=$30$ and learning rate=$1000$) of MACHO embeddings. Each plot visualizes the output of each attention block from Astromer 2.}
    \label{fig:alcock-emb}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.7]{figures/results/tsne_atlas.png}
    \caption{t-SNE projection (perplexity=$30$ and learning rate=$1000$) of ATLAS embeddings. Each plot visualizes the output of each attention block from Astromer 2.}
    \label{fig:atlas-emb}
\end{figure}

The first thing to notice is that there is evidence that Astromer properly separates classes in both the Alcock and ATLAS datasets. Recall that these embeddings were trained solely on light curve reconstruction, without any information about the labels.

Classes are separated in different ways depending on the block. For both Alcock and ATLAS, the first block does not seem to separate classes at all, while other blocks exhibit better discrimination. This aligns with the $\gamma$ parameters we introduced to weight each block's output during classifier training (see Figure \ref{fig:gammaweights}).

Alternatively, we can observe the effect of class separation in the confusion matrices in Figures \ref{fig:cm_alcock} and \ref{fig:cm_atlas}. Specifically, for the Alcock dataset, the main confusion occurs between RR Lyrae types ab and c. A similar pattern is observed in the projection of Figure \ref{fig:alcock-emb}, where the blue (RRab) and purple (RRc) points are mixed together. In ATLAS, there is an \textit{Other} class, which can be particularly confusing for the model. As observed in Figure \ref{fig:atlas-emb}, the blue points corresponding to the \textit{Other} class are sparsely distributed in the embedding space.
\begin{figure}
    \centering
    \includegraphics[scale=0.58]{figures/results/alcock_cm.pdf}
    \caption{Confusion matrices for the test set of the Alcock dataset, evaluated on classifiers trained with 20, 100, and 500 samples per class, following our methodology. }
    \label{fig:cm_alcock}
\end{figure}
 \begin{figure}
    \centering
    \includegraphics[scale=0.58]{figures/results/atlas_cm.pdf}
    \caption{Similar to Figure \ref{fig:cm_alcock} but with ATLAS.}
    \label{fig:cm_atlas}
\end{figure}



