\section{Astromer 2}\label{sec:astromer1}
Astromer 2 incorporates features that were not included in the initial version due to resource and time constraints. While Astromer 1 served as a proof of concept for generating effective embeddings, Astromer 2 builds on this foundation, introducing iterative enhancements to optimize performance at each stage.

Building upon the foundation of Astromer 1 discussed in Sect. \ref{sec:astromer0}, this section is dedicated solely to the enhancements that distinguish the principal features of Astromer 2. Fig. \ref{fig:astromer_1} shows a visual representation of the updated architecture of Astromer 2.


\begin{figure}
    \centering
    \includegraphics[scale=0.45]{figures/astromer_1/astromer_1.pdf}
    \caption{Astromer 2 Architecture. The primary difference from its predecessor is the input layer, where a trainable MASK token is appended to observations designated as [MASK] during pre-training. The magnitude vector is subsequently projected into a higher-dimensional space matching the PE dimensionality via a fully connected layer without activation. We generalize over $H$, the number of heads and $M$ the number of layers. Note that the final embedding $Z$ corresponds to the output from the last attention layer. It contains vectors of size $M d_k$, where $d_k$ is the size of the head for each $l$-th observation in the input.}
    \label{fig:astromer_1}
\end{figure}

% ====================================================================================
% INPUT EMBEDDING ====================================================================
% ====================================================================================
\subsection{Input embedding}
The process for creating the input embedding for Astromer 2 remains the same as in the initial version. However, we replace the magnitudes targeted for masking with a trainable token that is zero-initialized and shared across all samples.

While the contribution of masked tokens is zero after the attention weight calculation, adding a mask token to replace the actual magnitude allows the model to recognize which tokens are masked, which can be helpful during training. We also avoid potential information leaks that could arise from the all-to-all computation within the similarity matrix.

% ====================================================================================
% ENCODER  ===========================================================================
% ====================================================================================
\subsection{Encoder}
The encoder of Astromer 2 has a significantly larger number of parameters, increasing from \num{661505} to \num{5432129}. An eight fold increase. This growth is due to the inclusion of six attention blocks, with each block containing four heads and 64 units. Additionally, we have incorporated a dropout layer after the self-attention calculation, as depicted in Fig. \ref{fig:astromer_1}.

% ====================================================================================
% PRETRAINING TASK ===================================================================
% ====================================================================================
\subsection{Pretraining task}
Like Astromer 1, we use the root-mean-square error as the loss function. In Astromer 2, however, the losses are scaled based on observational uncertainties. These uncertainties are normalized to a range of 0 to 1, and their reciprocals are used as weights. Incorporating this scaling term into the error calculation enhances performance compared to Astromer 1.
\begin{eqnarray}\label{eq:loss_a1}
    \mathcal{L}oss = \sqrt{\frac{1}{N-1}\sum_{i=0}^{N-1}\sum_{l=0}^{L-1} \frac{m_{il}}{e_{il}}(x_{il} - \hat{x}_{il})^2}.
\end{eqnarray}
In Eq. \ref{eq:loss_a1}, $e_{il} \neq 0$ denotes the observation uncertainty associated with step $l$ in window $i$.