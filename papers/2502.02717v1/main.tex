%                                                                 aa.tex
% AA vers. 9.2, LaTeX class for Astronomy & Astrophysics
% Demonstration file
%                                                       (c) EDP Sciences
%-----------------------------------------------------------------------
%
%\documentclass[referee]{aa}    % for a referee version
% \documentclass[onecolumn]{aa}  % for a paper on 1 column  
%\documentclass[longauth]{aa}   % for the long lists of affiliations
%\documentclass[letter]{aa}     % for the letters
%\documentclass[bibyear]{aa}    % if the references are not structured
                                % according to the author-year natbib style


\documentclass{aa}
\usepackage{graphicx}
\usepackage{txfonts}
\usepackage{lipsum}
\usepackage{subfiles}
\usepackage{subcaption}         % necessary for continued figures, example in section 3
                                % and appendix
\usepackage{lscape}             % to rotate a single page table, example in appendix.
                                % For landscape tables, see the longtable examples.
\usepackage{placeins}           % useful with \FloatBarrier, to keep 
                                % onecolumn floats from drifting to the next section
\usepackage{siunitx}   

% \newcommand{\PP}[1]{\textcolor{blue}{[{\bf PP}: #1]}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage[options]{hyperref}
% To add links in your PDF file, use the package "hyperref"
% with options according to your LaTeX or PDFLaTeX drivers.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

   \title{Astromer 2}

   \author{Cristobal Donoso-Oliva\inst{1, 3, 5}
        \and Ignacio Becker\inst{2, 7}
        \and Pavlos Protopapas\inst{2}
        \and Guillermo Cabrera-Vives\inst{1,3,4,5,6}\fnmsep
        \and Martina Cádiz-Leyton\inst{1, 3}
        \and Daniel Moreno-Cartagena\inst{1, 3}
        }

   \institute{
Department of Computer Science, Universidad de Concepción, Edmundo Larenas 219, Concepción, Chile\and
John A. Paulson School of Engineering and Applied Science, Harvard University, Cambridge, MA, 02138\and
Center for Data and Artificial Intelligence, Universidad de Concepción, Edmundo Larenas 310, Concepción, Chile\and
Millennium Institute of Astrophysics (MAS), Nuncio Monseñor Sotero Sanz 100, Of. 104, Providencia, Santiago, Chile\and
Millennium Nucleus on Young Exoplanets and their Moons (YEMS), Chile\and
Heidelberg Institute for Theoretical Studies, Heidelberg, Baden-Württemberg, Germany\and
Department of Computer Science, Pontificia Universidad Catolica de Chile, Macul, Santiago 7820436, Chile
   }

   \date{Received Novemember xx, 2024}

% \abstract{}{}{}{}{}
% 5 {} token are mandatory
 
  \abstract{
  %context heading (optional)
  % {} leave it empty if necessary  
   Foundational models have emerged as a powerful paradigm within the deep learning field. Their capacity relies on the ability to learn robust representations from large-scale datasets and generalize to diverse downstream applications, such as classification. In this paper, we present Astromer 2, a foundational model designed for extracting light curve embeddings.}
  % aims heading (mandatory)
   {We introduce Astromer 2, an enhanced iteration of our self-supervised model for light curve analysis. This paper highlights the advantages of its pre-trained embeddings, compares its performance with that of its predecessor, Astromer 1, and provides a detailed empirical analysis of its capabilities, offering deeper insights into the model’s representations.} 
  % methods heading (mandatory)
   {Astromer 2 is pretrained on 1.5 million single-band light curves from the MACHO survey using a self-supervised learning task that predicts randomly masked observations within sequences. Fine-tuning on a smaller labeled dataset allows us to assess its performance in classification tasks. The quality of the embeddings is measured by the F1 score of an MLP classifier trained on Astromer-generated embeddings.}
  % results heading (mandatory)
   {Our results demonstrate that Astromer 2 significantly outperforms Astromer 1 across all evaluated scenarios, including limited datasets of 20, 100, and 500 samples per class. The use of weighted per-sample embeddings, which integrate intermediate representations from Astromer’s attention blocks, is particularly impactful. Notably, Astromer 2 achieves a 15\% improvement in F1 score on the ATLAS dataset compared to prior models, showcasing robust generalization to new datasets. This enhanced performance, especially with minimal labeled data, underscores the potential of Astromer 2 for more efficient and scalable light curve analysis.}
    {}
% Foundational models have emerged as a powerful paradigm in deep learning field, leveraging their capacity to learn robust representations from large-scale datasets and effectively to diverse downstream applications such as classification. In this paper, we present \textbf{Astromer 2}, a foundational model specifically designed for extracting light curve embeddings.

% We introduce Astromer 2 as an enhanced iteration of our self-supervised model for light curve analysis. This paper highlights the advantages of its pre-trained embeddings, compares its performance with that of its predecessor, Astromer 1, and provides a detailed empirical analysis of its capabilities, offering deeper insights into the model’s representations.

% Astromer 2 is pretrained on 1.5 million single-band light curves from the MACHO survey using a self-supervised learning task that predicts randomly masked observations within sequences. Fine-tuning on a smaller labeled dataset allows us to assess its performance in classification tasks. The quality of the embeddings is measured by the F1 score of an MLP classifier trained on Astromer-generated embeddings.

% Our results demonstrate that Astromer 2 significantly outperforms Astromer 1 across all evaluated scenarios, including limited datasets of 20, 100, and 500 samples per class. The use of weighted per-sample embeddings, which integrate intermediate representations from Astromer’s attention blocks, is particularly impactful. Notably, Astromer 2 achieves a 15\% improvement in F1 score on the ATLAS dataset compared to prior models, showcasing robust generalization to new datasets. This enhanced performance, especially with minimal labeled data, underscores the potential of Astromer 2 for more efficient and scalable light curve analysis.


   \keywords{Representation Learning --
             Light Curves --
             Foundational Models
               }

   \maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subfile{sections/0_introduction.tex}
\subfile{sections/1_astromer_v0.tex}
\subfile{sections/2_astromer_v1.tex}
\subfile{sections/3_data.tex}
\subfile{sections/4_results.tex}
\subfile{sections/5_conclusion.tex}

\section{Ethical and Practical Considerations}
Large-scale models require significant computational resources, highlighting the need for optimization. According to \cite{lannelongue2021green}, training Astromer 2 resulted in $32.29$ kg of $\text{CO}_2$ emissions, which is equivalent to a 195.96 km trip in a passenger car.

To mitigate this environmental impact, we provide pre-trained weights in our repository, enabling users to build upon our model without the need for full retraining. If you have new data, we encourage you to share your pre-trained models as a pull request in our repository.

Code, weights, and data can be found in our official organization: \url{https://github.com/astromer-science}. Additionally, we provide a website with user practical information \url{https://www.stellardnn.org/projects/astromer/index.html}.

% \begin{acknowledgements}
% This work ....      
% \end{acknowledgements}
\bibliographystyle{aa} % style aa.bst
\bibliography{references} % your references Yourfile.bib

\end{document}