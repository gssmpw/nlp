\section{Preliminaries}
\label{sec:background}
\subsection{CFD for automotive aerodynamics}
\label{subsec:cfd_for_automotive}
\paragraph{Computational fluid dynamics}
Automotive aerodynamics is centered around \acf{CFD}~\cite{versteeg2007introduction, hirsch2007numerical,pletcher2012computational}, which is deeply connected to solving the \acf{NS} equations. 
For automotive aerodynamics simulations, the assumptions of incompressible fluids due to low local Mach numbers, i.e., low ratio of flow velocity to the speed of sound, are justified~\cite{ashton2024drivaerml}.
Thus, the simplified incompressible form of the \ac{NS} equations~\citep{Temam:01} are applicable, which conserve momentum and mass of the flow field 
$\Bu(t,x,y,z): [0,T] \times \dR^3 \rightarrow \dR^3$ via:
\begin{align}
\frac{\partial \Bu}{\partial t} = -\Bu \cdot \nabla \Bu + \mu \nabla^2 \Bu - \nabla p + \Bf \ , \quad
\nabla \cdot \Bu = 0 \ .
\end{align}
To compute a numerical solution, it is essential to discretize the computational domain. 
In \ac{CFD}, the \ac{FVM}  is one of the most widely used discretization techniques. 
\Ac{FVM} partitions the computational domain into discrete control volumes using a structured or unstructured mesh. 
The initial geometric representation, typically provided as a \ac{CAD} model in formats such as STL, must be transformed into a simulation mesh. 
This meshing process precisely defines the simulation domain, allowing the representation of complex flow conditions, such as those in wind tunnel configurations or open-street environments \footnote{We emphasize the differentiation between raw \emph{geometry mesh} and the re-meshed \emph{simulation mesh} for CFD modeling.}. 
Note that meshing highly depends on the turbulence modeling and the flow conditions.

\paragraph{Turbulence modeling in CFD} 
Turbulence arises when the convective forces $\Bu \cdot \nabla \Bu$ dominate over viscous forces $\mu \nabla^2 \Bu$, typically quantified by the Reynolds number. 
Turbulent flows are characterized by a wide range of vortices across scales, with energy cascading from larger structures to smaller ones until viscous dissipation converts it into thermal energy at the Kolmogorov length scale.
Although \ac{DNS} can theoretically resolve the turbulent flow field by directly solving the \ac{NS} equations, it requires capturing all scales of motion down to the Kolmogorov scale. 
This implies extremely high requirements on the discretization mesh, which results in infeasible compute costs for full industrial cases.

 Therefore, engineering applications rely on turbulence modeling approaches that balance accuracy and computational efficiency. 
 \ac{RANS} \cite{reynolds1895iv, alfonsi2009reynolds} and \ac{LES} \cite{lesieur2005large} are two methods for modeling turbulent flows, each with distinct characteristics. 
 \ac{RANS} decomposes flow variables into mean and fluctuating components and solves the time-averaged equations, using turbulence models like k-$\epsilon$ \cite{LAUNDER1974kepsilon} to account for unresolved fluctuations. 
 While computationally efficient, \ac{RANS} may lack accuracy in capturing complex or unsteady flows, particularly in cases involving flow separation, where turbulence models are often less effective.
 In contrast, \ac{LES} resolves eddies down to a cut-off length and models sub-grid scale effects and their impact on the larger scales. 
 \ac{LES} offers higher accuracy in capturing unsteady behavior and separation phenomena at the cost of more compute. 
 In cases where \ac{LES} is too costly, hybrid models like, 
 \Acfi{HRLES} models~\cite{spalart2006new, chaouat2017state, heinz2020review, ashton2022hlpw} are an alternative. 
 These models reduce computational demand by using \ac{LES} away from boundary layers, and \ac{RANS} near surfaces, where sufficiently resolving the flow in \ac{LES} would require very high resolution.
The DrivAerNet~\cite{elrefaie2024drivaernet, elrefaie2024drivaernet++} dataset runs \ac{CFD} simulations on 8 to 16 million volumetric mesh cells with low-fidelity \ac{RANS} methods.
On the other hand, the DrivAerML~\cite{ashton2024drivaerml} dataset utilizes a \ac{HRLES} turbulence model and runs \ac{CFD} simulations on 160 million volumetric cells. 
 
\paragraph{Quantities of interest}
Interesting quantities for automotive aerodynamics comprise quantities on the surface of the car, in the volume around the car, as well as integral quantities such as drag and lift coefficient. 
The force acting on an object in an airflow is given by
\begin{align}
    \BF = \oint_S -(p-p_\infty) \bm{n} + \bm{\tau}_w dS \ ,
\end{align}
with the aerodynamic contribution, consisting of surface pressure $p$ and pressure far away from the surface $p_\infty$ times surface normals $\Bn$, and the surface friction contribution $\bm{\tau}_w$.
For comparability between designs, dimensionless numbers as drag and lift coefficients
\begin{equation}
    C_\text{d} =   \frac{2\, \bm{F} \cdot \bm{e}_{\text{flow}}}{\rho\, v^2 A_\text{ref}}, \; C_\text{l} =   \frac{2\, \bm{F} \cdot \bm{e}_\text{lift}}{\rho\, v^2 A_\text{ref}}
\end{equation}
 are used~\citep{ashton2024drivaerml}, where $\bm{e}_{flow}$ is a unit vector into the free stream direction, $\bm{e}_{lift}$ a unit vector into the lift direction perpendicular to the free stream direction, $\rho$ the density, $v$ the free stream velocity, and $A_{ref}$ a characteristic reference area.
Predicting these surface integrals allows for an efficient estimation when using deep learning surrogates,
since these surrogates can directly predict the surface values without the need to model the full 3D volume field,
as required by numerical CFD simulations.

\subsection{Transformer blocks for building neural operators}
\label{sec:prelim-transformer}
\textbf{Neural operators}~\cite{Lu:21,Li:20graph,Li:20} are formulated with the aim of learning a mapping between function spaces, 
usually defined as Banach spaces $\mathcal{I}$, $\mathcal{O}$ of input and output functions defined on compact input and output domains $\cX$ and $\cY$, respectively. 
Neural operators enable continuous outputs that remain consistent across varying input sampling resolutions.
A neural operator $\hat{\gG} : \mathcal{I} \rightarrow \mathcal{O}$ approximates the ground truth operator $\gG: \mathcal{I} \rightarrow \mathcal{O}$, and is often composed of three maps $\hat{\gG} \coloneqq \cD \circ \cA \circ \ \cE
$~\cite{Seidman:22,alkin2024universal,alkin2024neuraldem}, comprising encoder $\cE$, approximator $\cA$, 
and decoder $\cD$. 
Training a neural operator involves constructing a dataset of input-output function pairs evaluated at discrete spatial locations. %

\paragraph{Self-attention and cross-attention} Scaled dot-product attention~\cite{Vaswani:17} is defined upon three sets of vectors $\{\Bq_i \}$, $\{\Bk_i \}$, $\{\Bv_i \}$, written in matrix representation as $\mathbf{Z} = \text{softmax} (\BQ \BK^T/\sqrt{d})\BV$,  where $\Bz_i$, $\Bq_i$, $\Bk_i$, $\Bv_i$ are the $i$-th row vectors of the matrices $\BZ$, $\BQ$, $\BK$, $\BV$, respectively, and $d$ is the hidden dimension of the row vectors. 
Due to the row-wise application of the softmax operation, the multiplication $\BQ\BK^T$ must be evaluated explicitly, resulting in an overall complexity of $O(n^2d)$, which is prohibitively expensive when applying to a large number of tokens $n$. 
In self-attention, the $i$-th row vector of $\BQ$, $\BK$, $\BV$ is viewed as the latent embedding of a token, e.g., a word. 
\citet{Cao:21} proposes that each column in $\BQ$, $\BK$, $\BV$ can be interpreted as the evaluation of a learned basis function at each point.  
E.g., $\BV_{ij}$ can be viewed as the evaluation of the $j$-th basis function on the $i$-th grid point $x_i$, i.e., $\BV_{ij} = \Bv_j (\Bx_i)$. In contrast to self-attention, in cross-attention, the query matrix $\BQ$ is encoded from a different input. 
Cross-attention is probably most prominently used in perceiver-style architectures~\cite{jaegle2021perceiver}, where inputs with $N$ tokens are projected into a fixed-dimensional latent bottleneck of $M$ tokens ($M \ll N$), before processing it via self-attention. 

\paragraph{Neural field output decoding via cross-attention}  
Following the basis function interpretation of \citet{Cao:21}, when choosing the $i$-th row vector $\Bq_i$
in the query matrix $\BQ$ to be an encoding of query point $\By_i$, we can query at arbitrary locations which are independent of the input grid points~\cite{Li:OFormer,alkin2024universal,wang2024cvit}. The number of rows $m$ in the query matrix $\BQ$ corresponds to the number of output query points. Concretely, the $N$ latent tokens which are projected into the key and value matrices $\BK$ and $\BV$, result in $m$ output tokens. The case $m=1$ yields point-wise decoding.
It is to note that if no self-attention operation between query points is applied, the decoding happens point-wise, and is independent of the number of query points. I.e., the decoded output value at coordinate $\By_i$ is independent of the number of points used for decoding. Such a decoding scheme can be seen as a neural field~\cite{wang2024cvit}, which is conditioned on the global context, i.e., the encoded latent state, and on local coordinate information. Point-wise decoding is a desired property when building neural operators, i.e., continuous outputs that remain consistent across varying input sampling resolutions.

