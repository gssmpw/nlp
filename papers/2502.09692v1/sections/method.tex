\section{Geometry-preserving UPT}
\label{sec:methodology}
We start with a simple observation: in the DrivAerNet++ paper, \citet{elrefaie2024drivaernet++} state that 27 design parameters are enough to specify a wide range of conventional car geometries.
However, the output surface/volumetric meshes often correspond to (hundreds of) millions of mesh cells. 
In other words, inputs are rather simple, but outputs are diverse and complicated, and require huge meshes. 
Given this, we formulate the following model requirements:
\begin{enumerate}[label={(\Roman*)}, noitemsep,topsep=0pt]
    \itemsep0em 
    \item \label{req1} \textbf{Reduced latent space modeling.} The potentially vast number of output mesh cells requires an encoder-approximator-decoder modeling paradigm, where the decoder often functions as point-wise conditional neural fields. 
    Such approaches are introduced in AROMA~\citep{serrano2024aroma}, CViT~\citep{wang2024cvit}, \citet{knigge2024space}, and \ac{UPT}~\citep{alkin2024universal}. Moreover, \ac{UPT} additionally introduces a patch-embedding analogue for general geometries via supernode pooling. Alternatively, Transolver~\citep{wu2024transolver} implements a sliced and efficient attention.
    \item \label{req2} \textbf{Decoupling of encoder and decoder.} For various tasks, e.g., predicting 3D flow fields directly from geometry inputs, a decoupling of the encoder/approximator and decoder is favorable (see e.g., \ac{UPT}, OFormer~\citep{Li:OFormer}, \ac{GINO}~\citep{Li:23}). Additionally, a decoupled latent space representation allows for scalable decoding to a large number of output mesh cells since such models can cache encoded latents states and decode output queries in parallel. Finally, decoupling geometry-mesh encodings and model outputs is a fundamental requirement for transfer learning, especially when input geometry and output mesh get decoupled, see transfer learning experiments in Section~\ref{sec:experiments}.
    \item \label{req3} \textbf{Geometry-aware latent space.} In general, this requirement is only fulfilled for models that map input points directly to output predictions. Such methods comprise models such as Reg-DGCNN~\citep{wang2019dynamic, elrefaie2024drivaernet, elrefaie2024drivaernet++}, PointNet~\citep{qi2017pointnet}, and Transformer models, such as Transolver~\citep{wu2024transolver}, FactFormer~\citep{li2024scalable}, or ONO~\citep{xiao2023improved}. Preserving geometry-awareness and decoupling encoder and decoder are somewhat orthogonal requirements, yet important to build scalable and generalizable models. Recent attempts map locations and physics quantities to an embedding via cross-attention~\citep{wang2024latent}, or update query points using a heterogeneous normalized cross-attention layer~\citep{hao2023gnot}.
    Methods like \ac{GINO}~\citep{Li:23} can also be seen as geometry-aware due to the regularly-structured latent space.
\end{enumerate}

Given the methodological requirements stated above,
we design \ac{GP-UPT} as shown in Figure~\ref{fig:gp-upt-architecture}.
\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/gp_upt.pdf}
    \caption{Architecture overview of the \acl{GP-UPT}.}
    \label{fig:gp-upt-architecture}
\end{figure*}

\paragraph{Geometry-aware encoder} In our experiments, the input geometry $\cM_{i}$ for data sample $i$ is represented as a finite discretized point cloud, consisting of $N$ points in a 3D space, i.e., $\BX^{N}_{i} \in \dR^{N \times 3}$.
We embed the 3D coordinates using the transformer positional encoding \cite{Vaswani:17}. 
In this paper we do not use additional input features.
However, it is possible to tie additional input features (e.g., surface normals, etc.) to the coordinates of the input representation.
Following the approach of~\citet{alkin2024universal}, a supernode pooling block $\cS$ maps the input geometry $\cM_{i}$ into a set of supernode representations $\BS_{i}$, which capture information within radius $r_{sn}$ of the supernode: 
$\cS: \BX^{N}_{i} \in \cM_{i} \xrightarrow[]{\text{embed}} \BX^{N}_{i} \in \dR^{k\times d_{hidden}} \xrightarrow[]{\text{supernode pooling}} \BS_{i} \in \dR^{S \times d_{hidden}}$, where $d_{hidden}$ is the hidden dimensionality of the model's latent representations. 
First, we randomly select a subset of $S$ \emph{supernodes}, where typically $S \ll N$, from the coordinates $\BX^{N}_{i}$.
Through a \ac{GNO} layer~\citep{Li:20graph}, we aggregate information from neighboring coordinates within a radius $r_{sn}$.
The supernode pooling block $\cS$ fulfills requirement~\ref{req1}
by reducing the input tokens to the encoder from $N$ to $S$. 
This reduces the workload of the quadratic self-attention layers, thereby simplifying the latent space modeling by decreasing computational cost and memory requirements. 

The supernode representation $\BS_{i} = \BZ^0_{i} \in \dR^{S \times d_{hidden}}$ is then passed as input to the \emph{geometry-aware encoder} $\cE$, which consists of $K$ encoding blocks. 
Each block consists of one (multi-head) cross-attention layer followed by a (multi-head) self-attention layer. 
The encoder maps the supernode representations $\BS_{i}$ in $S$ latent token representations: $\cE :\BZ_{i}^0 \in \dR^{S \times d_{hidden}} \xrightarrow[]{\text{geometry-aware encoding}} \BZ^K_{i} \in \dR^{S \times d_{hidden}}$.
For each block $j$, the cross-attention layer uses the latent representation of the previous block $\BZ^{j-1}$,  as input for the query representation, i.e., $\BZ^{j-1} \rightarrow \BQ^{j}$, while the supernode representations $\BS_{i}$ are repeatedly used as input for the key and value representations, i.e., $\BS_{i} \rightarrow \BK_{i}^{j},\BV_{i}^{j}$. 
This means that the inputs for the keys and values for each cross-attention layer are fixed across all $\{1, \dots, j\} \in K$ blocks, while the input to the self-attention transformer layer in each block is the output of the previous cross-attention layer.
Our proposed geometry-preserving encoder $\cE$ partly fulfills requirement~\ref{req3}. 
Since the cross-attention layers in each block attend to the original supernode representations $\BS_{i}$, the encoder is able to maintain the latent representations $\BZ^{j}$ grounded w.r.t. the original input geometry.
As discussed in Section \ref{sec:prelim-transformer}, neural operators consist of three mapping functions: an encoder, an approximator, and a decoder. 
Usually, the approximator is used to model the transition from state $t$ into $t+1$. 
However, since we are working with stationary \ac{CFD} simulations, there is no temporal component. 
Therefore, we do not require the approximator in our model.

\paragraph{Field decoder} 
Similarly to the original~\ac{UPT}~\cite{alkin2024universal}, we employ a \emph{field-based} decoder $\cD_{field}$ to predict an output signal conditioned on $M$ arbitrary coordinates on the surface mesh and the encoded latent tokens $\BZ_{i}^{K}$: $\cD_{field}: (\BZ_{i}^{K}, \{\mathbf{y}^{1}_{i}, \dots, \mathbf{y}^{M}_{i}\}) \xrightarrow[]{\text{cross-attention}} \BO^{M}_{i} \in \cR^{M \times d_{out}}$ where $\BO^{M}_{i}$ contains $M$ samples of the output function $\Bo_i \in \cO$.
We embed the 3D coordinate $\By_{i}^{k}$ analogously to the input point cloud, i.e., $\By_{i}^{k} \in \dR^{3} \xrightarrow[]{\text{embed}} \By_{i}^{k} \in \cR^{d_{hidden}}$, and map the embedded coordinate through a stack of $C$ cross-attention layers to obtain $\Bv_{i}^{k} \in \cR^{d_{out}}$, where $d_{out}$ is the dimensionality of the prediction of the output signal. If we only predict a single quantity (e.g., surface pressure), then $d_{out} = 1$.
The input to this stack of cross-attention layers is the embedded query coordinate, i.e., $\By^{k}_{i} = \Bz^{0}_{ki}$. 
The query representation for each cross-attention layer $\{1, \dots, C\}$ is the output of the previous layer, i.e., $\Bz_{ki}^{c-1} = \Bq_{ki}^{c}$. 
For all cross-attention layers, we use the latent tokens $\BZ_{i}^{K}$ as input for the key a value representations, i.e., $\BZ^{K}_{i} \rightarrow \BK^{K}_{i}, \BV^{K}_{i}$.
It is important to note that each query point is decoded independently, meaning there is no attention between query points. 
Since the decoder is conditioned on both the latent tokens and individual query coordinates, we can use a different set of query coordinates for decoding than those as input for the encoder.
As a result, the field-based decoder $\cD_{field}$ satisfies requirement~\ref{req2}: the decoupling of encoder en decoder. 
In other words, the output queries are independent of the geometry representation used as input for the encoder, and hence, the encoder and decoder are decoupled.

To ensure that the latent tokens $\BZ^{K}_{i}$ remain grounded w.r.t. the input geometry (requirement~\ref{req3}), we introduce an auxiliary decoder for training only, i.e., a \emph{point-based} decoder $\cD_{point}$. 
We map the set of latent tokens $\BZ_{i}^{K}$ through a linear projection and predict the output signal (e.g., pressure value) tied to the coordinates of the supernodes to which the tokens belong in the input space: $\cD_{point}: \BZ_{i}^K \in \dR^{S \times h} \xrightarrow[]{\text{Linear projection}} \BV^{S}_{i} \in \dR^{S \times d_{out}}$.
By using $\cD_{point}$ we ensure that the latent tokens retain the information tied to the radius they represent in the input space $\cM_{i}$. 

\paragraph{Optimization}
Both decoders, $\cD_{point}$ and $\cD_{field}$, are optimized by minimizing the \ac{MSE} w.r.t. the predicted outputs (e.g., pressure or \ac{WSS}).
Therefore, the \emph{multi-task} optimization objective is defined as $
    \mathcal{L}_{multi} = w_{point} \cdot \mathcal{L}_{point} +  w_{field} \cdot \mathcal{L}_{field} \ ,
$
where $w_{point}=w_{field}=0.5$ are the weight coefficients for the respective loss terms. 
Unless otherwise stated, during inference, we only use the field-based decoder $\cD_{field}$.
