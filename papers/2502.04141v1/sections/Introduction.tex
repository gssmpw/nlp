\section{Introduction}

Reinforcement learning (RL) methods can successfully solve challenging tasks in complex environments, even outperforming humans in a variety of cases \citep{mnih2015human, silver2018general}. However, due to the online nature of standard RL algorithms and their reliance on informative, often hand-engineered reward signals, RL methods are typically sample-inefficient and lack generalizability to new tasks. Offline RL \citep{levine2020offline, prudencio2023survey} is an alternative approach that applies RL-based techniques to train policies entirely offline using static datasets of trajectories collected from the target domain. The key innovation is that a single offline dataset can be relabeled with a variety of different reward functions, enabling reuse of datasets to learn a variety of downstream tasks. This paradigm magnifies the importance of learning to generate datasets with diverse coverage of the state space in hopes of covering regions that correspond to a wide a variety of downstream tasks \cite{yarats2022don}. The design of existing algorithms for dataset generation \citep{pathak2017curiosity, eysenbach2018diversity, lee2019efficient, burda2019exploration, liu2021behavior, yarats2021reinforcement} relies on uncertainty metrics, such as entropy, to quantify the quality and control the variety of state space coverage. This renders the choice of uncertainty metrics critical to the diversity of datasets that can be achieved. While Shannon entropy (SE) and R\'{e}nyi entropy (RE) have been widely used as exploration objectives in RL \cite{hazan2019provably, liu2021behavior, yarats2021reinforcement, zhang2021exploration, yuan2022renyi}, the variety of datasets that can be achieved using them is limited. In the SE case this is due to the fact that SE provides just a single objective and thus a single notion of optimal coverage (see Figure \ref{fig:entropy}). In the RE case, though its parametric form provides a variety of notions of coverage, this coverage remains partial (see Figure \ref{fig:entropy}) and comes at the cost of instability arising due to its inherent discontinuity in parameter space \citep{suresh2024robotic} and poor coverage for many parameter values (see Figure \ref{fig:intro}). Developing a family of exploration objectives overcoming these issues remains an open question.

% {Recently, \cite{suresh2024robotic} proposed \textit{behavioral entropy} (BE), a novel generalization of classical entropies that composes SE with the probability weighting functions widely used in behavioral economics to model human decision making \citep{dhami2016foundations}. They proved $i)$ BE belongs to a class of \emph{admissible generalized} entropies and $ii)$ BE is the most \emph{perceptive} entropy formulation to date. The first property guarantees BE is smooth and  maximizes the uniform distribution uniquely, which is necessary for exploration and data generation applications aiming for uniform coverage. 
% % The second property guarantees that objectives maximizing BE can produce the widest range of policies maximizing uniform coverage. 
% BE's superior \emph{perceptiveness} stems from the smooth tuning parameter (inspired from human behaviors~\citep{prelec1998probability}) that reweighs the probability distribution, to generate the widest range of entropies (See Fig~\ref{fig:intro}), producing the most diverse exploration policies ranging from coarse and wider coverage to dense and focused coverage. \cite{suresh2024robotic} showed BE inspired exploration objective induces the best performance and the best variety of coverage in discrete probability spaces as compared with SE and RE. Thus, BE can be a principled alternative measure for existing exploration objectives to generate rich variety of datasets for RL. However, the current formulation of BE is restricted to discrete probability distributions and the exploration objective used in \cite{suresh2024robotic} is limited to discretized, two-dimensional robotic exploration tasks.}

\new{Recently, \cite{suresh2024robotic} proposed \textit{behavioral entropy} (BE), a novel generalization of classical entropies that composes SE with the probability weighting functions widely used in behavioral economics to model human decision making \citep{dhami2016foundations}.
%
In \cite{suresh2024robotic}, the authors rigorously established two key facts: (i) BE is a valid generalization of the classical notion of entropy, and (ii) BE provides the most general notion of entropy to date in the sense that its parametric form captures a wider range of valid generalized entropies than existing parametric families of entropies, such as RE.
%
The first property guarantees BE is smooth in its parameter and attains its maximum on the uniform distribution, which is critical for exploration and data generation applications aiming for uniform coverage. The second property stems from BE's definition in terms of probability weighting functions \citep{prelec1998probability}, the use of which allows it to achieve a broader range of entropies than comparable methods (see Figure \ref{fig:intro}). As \cite{suresh2024robotic} experimentally demonstrated on robotic exploration problems, when BE is used as an objective this flexibility induces diverse exploration policies ranging from those providing coarse and widespread coverage of the environment to dense and focused coverage, achieving the best performance and the widest variety of coverages in discrete probability spaces compared with SE and RE.
%
Together, these results establish BE as a principled alternative to existing exploration objectives that provides a rich variety of exploration behaviors and state space coverage. However, the current formulation of BE is restricted to discrete probability distributions and the experimental evaluation provided in \cite{suresh2024robotic} is limited to classical, planning-based solutions to discretized, two-dimensional robotic exploration tasks. This impedes the applicability of BE to more complex problems.}

% As \cite{suresh2024robotic} showed, by varying the parameters of the probability weighting function, BE induces the best variety of coverage for two-dimensional map exploration problems in robotics compared with SE and RE. In addition, BE was shown to vary smoothly over its parametric domain and to satisfy the Shannon-Khinchin axioms and thus belong to the class of admissible generalized entropies \citep{shannon1948entropy, Amigo2018entropy}. Together, these results establish BE as a principled alternative to existing exploration objectives that provides a rich variety of exploration behaviors and state space coverage. However, the current formulation of BE is restricted to discrete probability distributions and the experimental evaluation provided in \cite{suresh2024robotic} limited to classical, planning-based solutions to discretized, two-dimensional robotic exploration tasks. This impedes the applicability of BE to more complex problems.

In this paper we propose using BE as a principled exploration objective for systematically generating datasets that provide diverse state space coverage in complex, continuous, potentially high-dimensional domains. We hypothesize that using BE in this way will lead to superior offline RL performance on BE-generated datasets compared with objectives providing an inferior diversity of coverage, particularly SE and RE, \new{but also the widely used Random Network Distillation (RND) \citep{burda2019exploration} and State Marginal Matching (SMM) \citep{lee2019efficient} algorithms}. There are three primary challenges to using BE in this way: (i) a continuous-spaces version of BE must be formulated; (ii) principled estimators for BE that enjoy theoretical guarantees while remaining tractable in continuous, potentially high-dimensional settings must be developed; (iii) practical RL methods for training BE-maximizing policies must be derived. We address all three of these challenges in this work, then use the resulting RL method to generate datasets providing a rich variety of state space coverage for subsequent offline RL. We experimentally confirm our hypothesis, demonstrating that BE-generated datasets lead to superior offline RL performance over SE, RE, \new{RND, and SMM} datasets, and that offline RL methods enjoy better data- and sample-efficiency when applied to BE- and RE-generated datasets compared with existing benchmarks.
%
%
%
% Reinforcement learning (RL) algorithms have been largely successful in solving complex task and outperforming humans in specific tasks\cite{mnih2015human,silver2018general}. However, these algorithms lack generalizability and also require labelled data and access to frequent environmental interactions~\cite{westheider2023multi} which may not be possible or feasible for all applications. 
% Offline RL is an emerging field that tries to use available data collected for certain tasks to adapt to new downstream tasks~\cite{eysenbach2018diversity,agarwal2020optimistic}, thereby improving generalizability. Thus diverse data generation becomes as important as improving the offline-RL algorithms to adapt to downstream tasks\cite{yarats2022don}. 
% An important aspect of RL data generation algorithms~\cite{pathak2017curiosity,yarats2021reinforcement,liu2021behavior,liu2021aps,lee2019efficient,burda2018exploration}is to enhance diversity of the data collected~\cite{panaganti2022robust} in the hopes of covering areas of the state space that correspond to different downstream tasks. 
% The fundamental design of such algorithms relies on the measure of uncertainty/entropy to ascertain the quality of coverage and to guide sampling future data points. To promote diversity of data collection we also need diverse exploration and sampling strategies to adapt to the variety of RL application landscapes.
% The Shannon and R\'{e}nyi entropies have been widely studied as exploration objectives in general machine learning and reinforcement learning (RL), in particular \citep{hazan2019provably, liu2021behavior, yarats2021reinforcement, zhang2021exploration}. Despite their popularity, Shannon's entropy employs a singular reasoning and evaluation (expected information gain) that limits diversity, while Renyi's entropy is parametrized with the $q-norm$, enhancing diversity but at the cost of stability arizing due to its inherent discontinuity in the parameter space~\cite{suresh2024robotic}.  
%
% Recently~\cite{suresh2024robotic} proposed a novel \textit{generalized behavioral entropy} (GBE) using Prelec's probability weighting~\citep{prelec1998probability} to quantify uncertainty in a tunable and human-intuitive manner. They show that the proposed GBE can generate the highest variety of exploration policies for 2D physical spaces and also produces the most diverse valuations from an information theory perspective, while also being smooth across the parametric domain.
% In addition, GBE is also admissible as a generalized entropy (Shannon-kinchin Axioms)~\citep{Amigo2018entropy}, has tunable intuitive reasoning, generalizes Shannon~\citep{shannon1948entropy} and R\'{e}nyi~\citep{renyi1961measures} entropies and experimentally demonstrates the superiority of GBE as a metric in the objective for robotic exploration problems.
% This diverse entropy measure can also be applied to exploratory data generation schemes of RL to generate rich and representative data in an efficient manner. However, the GBE proposed in \cite{suresh2024robotic} is restricted to discrete probability distributions and the experimental evaluation provided is limited to classical planning-based solution approaches of discretized, two-dimensional robotic exploration problems.
%
% In the present paper we first extend the definition of GBE to continuous high dimensional spaces, derive it's k-nearest neighbor (k-nn) estimator inspired Shannon and Renyi estimators\citep{kozachenko1987sample, singh2003nearest, leonenko2008class, sricharan2012estimation, singh2016finite},and characterize the bias of the estimate as a probabilistic bound. We then investigate the use of GBE and its k-nn estimator as an exploration objective for learning-based solution approaches to continuous, high-dimensional problems. As a motivating use case, we focus on the employment of GBE as the exploration objective during the pretraining phase of unsupervised reinforcement learning (URL), and we experimentally demonstrate that starting with pretrained policies learned using GBE results in greater sample-efficiency on a variety of downstream tasks during the URL fine-tuning phase.
%
% Reinforcement learning (RL) methods have been successful at solving challenging tasks in complex environments, even outperforming humans in a variety of cases \citep{mnih2015human, silver2018general}. However, due to the online nature of standard RL algorithms and their reliance on informative, often hand-engineered reward signals, RL methods are typically sample-inefficient and lack generalizability to new tasks. Offline RL \citep{levine2020offline, prudencio2023survey} is an alternative approach that applies RL-based techniques to train policies entirely offline using static datasets of trajectories collected from the target domain. The key innovation is that a single offline dataset can be relabeled with a variety of different reward functions, enabling reuse of datasets to learn a variety of downstream tasks, mitigating sample inefficiency and improving generalizability over standard RL methods. This paradigm magnifies the importance of learning to generate datasets with diverse coverage of the state space, in hopes of covering regions that correspond to as wide a variety of downstream tasks as possible \cite{yarats2022don}. The design of existing algorithms for dataset generation \citep{pathak2017curiosity, eysenbach2018diversity, lee2019efficient, liu2021behavior, yarats2021reinforcement} fundamentally relies on uncertainty metrics, such as entropy, to quantify the quality and control the variety of state space coverage. This renders the choice of uncertainty metrics critical to the diversity of datasets that can be achieved. While the Shannon and R\'{e}nyi entropies have been widely used as exploration objectives in RL \cite{hazan2019provably, liu2021behavior, yarats2021reinforcement, zhang2021exploration, yuan2022renyi}, the variety of datasets that can be achieved using them is limited. In the Shannon case this is due to the fact that SE only provides a single objective and thus a single notion of optimal coverage (see Figure \ref{fig:entropy}). In the RE case, though its parametric form provides a variety of notions of coverage, this comes at the cost of instability arising due to its inherent discontinuity in parameter space \citep{suresh2024robotic} and resulting poor coverage for many parameter values (see Figure \ref{fig:intro}).
%
%
%
\begin{figure}[tp]
    \includegraphics[width=\textwidth]{figs/summary_plots_both.png}
    \caption{\small \textbf{(Left)} Comparison of Shannon entropy, R\'{e}nyi entropy, and behavioral entropy (ours) and their effects on dataset generation, shown in PHATE plots, when used as an exploration objective. \textbf{(Right)} Performance comparison of an offline RL algorithm (CQL) for three downstream tasks on datasets generated using Shannon, behavioral entropy (ours), and R\'{e}nyi entropy for the parameter $q = 1.1$ shown in the left-hand figure.}
    \label{fig:intro}
    % \vspace{-0.6cm}
\end{figure}
%
%
%

Our main contributions are:
%
\begin{itemize}[noitemsep,topsep=1pt,leftmargin=*]
    \item \textbf{Behavioral entropy estimation in continuous spaces.} We propose a version of BE applicable to continuous probability distributions, derive $k$-nearest neighbor ($k$-NN) estimators for BE with general probability weighting functions, and provide convergence guarantees and probabilistic bounds characterizing the bias and variance of these estimators.
    %
    %
    \item \textbf{Exploration and data generation via RL-based BE maximization.} \new{ We derive practical BE-maximizing exploration objectives and experimentally illustrate their effectiveness to generate datasets with diverse levels of state space coverage in unsupervised RL settings.}
    %
    %
    \item \textbf{Offline RL performance on BE-generated data.} \new{ We experimentally evaluate the performance of offline RL algorithms for a variety of downstream tasks on BE, RE, SE, RND, and SMM datasets. We find that BE datasets lead to superior offline RL performance over SE, RE, RND, and SMM, and that offline RL methods enjoy better data- and sample-efficiency when applied to BE- and RE-generated datasets compared with existing benchmarks. }
    %
\end{itemize}