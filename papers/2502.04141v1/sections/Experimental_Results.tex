\section{Experimental Results}
%
%
%
%
\begin{wraptable}{r}{0.75\textwidth}
    \vspace{-4mm}
    %
    \centering
    \begin{tabular}{|cc|ccccc|}
        \hline
        %
        Environment & Task & BE & RE & SE & RND & SMM \\
        %
        \hline
        %
        Walker & Stand & \textbf{990.38} & 988.93 & 954.93 & 947.89 & 496.09 \\
        %
        & Walk & \textbf{904.66} & 878.20 & 895.89 & 735.77 & 409.46  \\
        %
        & Run & 385.07 & \textbf{440.53} & 360.64 & 341.03 & 140.29  \\
        %
        \hline
        %
        Quadruped & Walk & \textbf{845.31} & 776.64 & 755.79 & 699.22 & 425.11  \\
        %
        & Run & \textbf{522.32} & 490.75 & 490.46 & 490.66 & 275.38  \\
        %
        \hline
    \end{tabular}
    \caption{\small \new{ Comparison of best offline RL performance across all datasets, training seeds, and offline RL algorithms.} }
    %
    %
    \label{tab:orl_performance}
    %
    \vspace{-3mm}
\end{wraptable}
%
% \vspace{5mm}
%
The experiments presented in this section (i) provide qualitative insights into the state space coverage achieved by policies that maximize BE, RE, and SE, and (ii) examine the utility of BE-maximizing policies for performing offline dataset generation for subsequent offline RL \new{compared with datasets generated using the SE and RE objectives and datasets generated using the RND and SMM algorithms}. The state space coverage visualizations that we present suggest that BE-generated datasets achieve a wider variety of coverage than RE- and SE-generated datasets, and that the RE objective is unstable as a function of $q$ and provides poor coverage for $q > 1$. \new{We provide coverage visualizations for RND and SMM in the appendix.} In our offline RL experiments, we demonstrate that offline learning on \new{BE datasets leads to superior performance over SE, RND, and SMM datasets on all five tasks,} and superior performance to RE datasets on four out of five tasks (see Table \ref{tab:orl_performance}).



\textbf{Experimental Setup.} For our experiments, we generated BE, RE, SE, RND, and SMM datasets for the Walker and Quadruped environments using the Unsupervised Reinforcement Learning Benchmark (URLB) framework \citep{laskin2021urlb}\footnote{\url{https://github.com/rll-research/url_benchmark}}. We subsequently generated t-SNE plots \citep{hinton2002stochastic} and PHATE plots \citep{moon2019visualizing} from the BE, RE, SE, RND, and SMM datasets to visualize their varying state space coverage. Finally, we performed offline RL training on all datasets using the Exploratory Data for Offline RL (ExORL) framework \citep{yarats2022don}\footnote{\url{https://github.com/denisyarats/exorl}}. We emphasize that the datasets we generated contained just 500K elements, only 5\% as many as the 10M-element datasets considered in the ExORL framework \citep{yarats2022don}, and that we performed just 100K offline training steps, only 20\% of the 500K performed in ExORL. Despite these limitations, we achieved comparable performance to that achieved in \citep{yarats2022don}, indicating that using BE-generated data for subsequent offline RL leads to significant improvements in both data- and sample-efficiency.



\begin{wrapfigure}{r}{0.45\textwidth}
    %
    \vspace{-3mm}
    % \begin{figure}[]
    \begin{subfigure}[b]{0.98\linewidth}
        \includegraphics[width=\linewidth, height=0.9\linewidth]{figs/phate_gbe_walker_full_v2.png}
        \caption{PHATE plots for BE for Walker.}
        \label{fig:phate_gbe_walker_full}
    \end{subfigure}
    \centering

    \begin{subfigure}[b]{0.98\linewidth}
        \includegraphics[width=\linewidth, height=0.6\linewidth]{figs/phate_renyi_walker_full_v2.png}
        \caption{PHATE plots for Renyi for Walker. Coverage for $q = 3.0, 5.0$ similar to $q = 2.0$.}
        \label{fig:phate_renyi_walker_full}
    \end{subfigure}
    \caption{\small PHATE plots for Walker tasks.}
    \label{fig:phate_walker_full}
    % \end{figure}
    \vspace{-5mm}
\end{wrapfigure}
%
\noindent\textbf{Dataset Generation and Visualization.} For dataset generation, we used the Active Pre-Training (APT) algorithm \citep{liu2021behavior} implemented in the URLB framework to maximize BE using the reward proposed in \eqref{eqn:r_final} for various values of $\alpha$, RE using the reward proposed in \cite{zhang2021exploration} for various values of $q$, and SE using the default reward from \cite{liu2021behavior}. Specifically, for behavioral and RE we considered $\alpha \in \{ 0.2, 0.5, 0.7, 0.9, 1.5, 2.0, 3.0, 5.0 \}$ and $q \in \{ 0.2, 0.5, 0.7, 0.9, 1.1, 2.0, 3.0, 5.0 \}$.
%
\new{To ensure admissibility of the behavioral entropies we considered, we used the conditioning $\beta=e^{(1-\alpha)\log(\log(M))}$ from \cite{suresh2024robotic}, where $M$ is the dimensions of the representation space. See the discussion following \eqref{eqn:behavioral_entropy} in Section \ref{sec:be_in_continuous_spaces} for details.}
%
%
For each of the $\alpha$ and $q$ values, as well as for SE, we trained APT on the corresponding reward for 500K pretraining steps on both the Walker and Quadruped environments, collecting the resulting trajectories to form our datasets. This resulted in 17 datasets for each environment, for a total of 34 datasets. To ensure a fair comparison across entropies, for the APT hyperparameters we used the default URLB pretraining hyperparameter values across all datasets (see Table \ref{tab:data_gen_hyperparams} in the appendix). \new{For the datasets generated using the Random Network Distillation (RND) \citep{burda2019exploration} and State Marginal Matching (SMM) \cite{lee2019efficient} algorithms, we similarly trained for 500K pretraining steps and for consistency we used the same RND and SMM hyperparameters considered in URLB.}

%
To provide qualitative insight into the state space coverage of the SE, RE, and BE datasets, we generated two-dimensional $t$-SNE \citep{hinton2002stochastic} and PHATE \citep{moon2019visualizing} plots of the trajectories they contain.\footnote{ \new{ 10K representative samples from each dataset were gathered uniformly, totaling 170K samples for each domain from $17$ datasets. $t$-SNE and PHATE were implemented on this aggregated 170K-element dataset to ensure uniformity in projections for each dataset and then correspondingly represented individually for clarity.} } \new{ We also generated $t$-SNE and PHATE plots for the RND and SMM datasets, pictured in Figure \ref{fig:smm_rnd_full} in the appendix.}
%
% The $t$-SNE and PHATE techniques are well-known non-linear dimensionality reduction techniques for visualizing high-dimensional data.
%
While $t$-SNE has been previously used to visualize RL trajectory data \citep{zhang2021exploration}, to our knowledge this is the first time PHATE plots have been used to visualize such data. PHATE tends to better retain global structure such as the temporal nature of trajectory data, while $t$-SNE obscures it \citep{moon2019visualizing}. Figure \ref{fig:phate_walker_full} indicates that while both BE and RE-generated datasets provide more flexible levels of state space coverage than SE, BE-generated datasets achieve a wider variety of coverage than RE. See the appendix for $t$-SNE and PHATE plots for the remaining datasets. Importantly, these plots indicate that the RE objective is unstable as a function of $q$ and provides poor coverage for $q > 1$, while the level of coverage provided by BE varies smoothly in $\alpha$. We provide experimental support for this in the following section, where highly unstable offline RL performance on RE datasets and the contrasting stability on BE datasets is illustrated in Figure \ref{fig:orl_performance}.
%



\begin{figure}[!htp]
    %
    \begin{subfigure}{.95\textwidth}
    % \captionsetup{font=footnotesize,labelfont=scriptsize,textfont=scriptsize}
    \includegraphics[width=\textwidth]{figs/quadruped_walk_barplot_with_shannon3M_SMM_RND.png}
    % \caption{TEST}
    \label{fig:orl:quad_walk}
    \end{subfigure}
    %
    \vspace{-4mm}
    %
    \begin{subfigure}{.95\textwidth}
    % \captionsetup{font=footnotesize,labelfont=scriptsize,textfont=scriptsize}
    \includegraphics[width=\textwidth]{figs/quadruped_run_barplot_with_shannon3M_SMM_RND.png}
    % \caption{TEST}
    \label{fig:orl:quad_run}
    \end{subfigure}
    %
    \vspace{-4mm}
    %
    \begin{subfigure}{.95\textwidth}
    % \captionsetup{font=footnotesize,labelfont=scriptsize,textfont=scriptsize}
    \includegraphics[width=\textwidth]{figs/walker_stand_barplot_with_shannon3M_SMM_RND.png}
    % \caption{TEST}
    \label{fig:orl:walker_stand}
    \end{subfigure}
    %
    \vspace{-4mm}
    %
    \begin{subfigure}{.95\textwidth}
    % \captionsetup{font=footnotesize,labelfont=scriptsize,textfont=scriptsize}
    \includegraphics[width=\textwidth]{figs/walker_walk_barplot_with_shannon3M_SMM_RND.png}
    % \caption{TEST}
    \label{fig:orl:walker_walk}
    \end{subfigure}
    %
    \vspace{-4mm}
    %
    \begin{subfigure}{.95\textwidth}
    % \captionsetup{font=footnotesize,labelfont=scriptsize,textfont=scriptsize}
    \includegraphics[width=\textwidth]{figs/walker_run_barplot_with_shannon3M_SMM_RND.png}
    % \caption{TEST}
    \label{fig:orl:walker_run}
    \end{subfigure}
    %
    \vspace{-4mm}
    %
    \caption{\small \new{Comparison of offline RL performance over the entropy objectives used in dataset generation. Plots show mean and standard deviation over five seeds. Dotted line shows performance of RL policy trained online until approximate optimality.} }
    \label{fig:orl_performance}
\end{figure} 




%
\noindent\textbf{Offline RL Experiments.} We compared the TD3, CQL, and CRR offline RL algorithms \citep{fujimoto2018addressing, kumar2020conservative, wang2020critic} implemented in the ExORL framework on the datasets generated as described above for all eight $\alpha$ values and for $q \in \{ 0.2, 0.5, 0.7, 0.9, 1.1\}$. We omitted offline RL training for RE datasets with $q \in \{2.0, 3.0, 5.0\}$ after observing in initial trials that performance was no better than for $q = 1.1$ and typically worse (see poor performance on $q = 1.1$ datasets in Figure \ref{fig:orl_performance}). \new{To gain insight into the effect of using larger datasets, we also considered a 3M-element SE-generated dataset. Altogether we considered 17 datasets: eight BE, five BE, two SE, and one each for RND and SMM.} % We omitted the behavioral cloning (BC) and BC+TD3 algorithms implemented in ExORL due to the poor performance reported by \cite{yarats2022don}.
%
%
On the Walker datasets we considered the Stand, Walk, and Run tasks, while on Quadruped we considered the Walk and Run tasks. \new{ For each of the $5 \times 17 = 85$ task-dataset combinations, we trained each of TD3, CQL, and CRR for 100K offline training steps, evaluating performance every 10K training steps. For each of the $255$ task-dataset-algorithm combinations, we repeated this training process for a total of 5 different seeds, resulting in our training $1275$ offline RL policies altogether. To ensure a fair comparison across all entropies, we used default ExORL hyperparameter values across all datasets (see Table \ref{tab:orl_hyperparams} in the appendix). }

As summarized in Table \ref{tab:orl_performance}, offline RL training on BE-generated datasets leads to superior performance over SE-, RND-, and SMM-generated datasets on all five tasks we considered, and superior performance to RE-generated datasets on four out of five tasks. Figure \ref{fig:orl_performance} provides a detailed overview of the experimental results for $\alpha \in \{0.2, 0.5, 0.7, 0.9, 1.5\}$ and $q \in \{0.2, 0.5, 0.7, 0.9, 1.1\}$ (complete results for all $\alpha$ values are shown in the appendix). This figure illustrates that BE-generated datasets lead to significantly better performance over the other methods on Quadruped Walk and Walker Walk for the best-performing $\alpha$ values, while offline RL performance on RE datasets for the best-performing values of $q$ is only slightly below that of BE datasets in Quadruped Run and Walker Stand. These trends hold across all algorithms for each of the tasks. On Walker Run, the best-performing RE parameter clearly leads to superior offline RL performance over both BE and SE datasets in the TD3 and CQL trials, but performance on BE datasets is again better than on RE datasets in the CRR trials. \new{Performance on SMM datasets is clearly inferior across all tasks considered. Performance on RND datasets is inferior on Walker tasks, but is almost competitive with BE on Quadruped tasks. Interestingly, the 3M-element SE datasets lead to strong downstream performance on Walker Stand and improved downstream performance over the 500K-element SE datasets on the Walker Stand and Run tasks, but the 3M-element SE datasets actually lead to worse performance compared with the 500K-element SE datasets on both Quadruped tasks and TD3 performance on Walker Walk.} Overall, best offline RL performance on BE-generated datasets clearly exceeds best performance on RE datasets on 13 out of 15 task-algorithm combinations and best performance on SE, RND, and SMM datasets on 15 out of 15 task-algorithm combinations.

We observed sensitivity of performance to parameters $\alpha$ and $q$ as well as choice of offline RL algorithm. Regarding the latter, notice in Figure \ref{fig:orl_average_appendix} in the appendix that on Walker Walk the SE-generated datasets are competitive with the average and best-performing BE datasets in the TD3 trials, while BE datasets significantly outperform SE ones in both the CQL and CRR trials. On Quadruped Run, on the other hand, performance on BE and SE datasets remains roughly the same across all algorithms, while average RE performance is significantly worse (see appendix for average performance plots for all tasks). These results suggest that offline RL algorithm performance depends in a complex way on the choice of exploration objective used in dataset generation. Well-performing, flexible objectives such as BE -- and to a lesser but still significant extent, RE -- therefore merit additional study as tools for dataset generation for offline RL.