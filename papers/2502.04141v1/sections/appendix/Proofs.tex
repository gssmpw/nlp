\subsection{Proofs}

Fix a probability weighting function $w$ and let $g(y) = -\frac{1}{y} \log(w(y)) w(y)$. Fix a p.d.f. $f \in \Delta(\mc{X})$, where $\mc{X} \subset \mathbb{R}^d$ is compact. Fix $n, k \in \mathbb{N}$, and let $X_1, \ldots, X_n \sim f(\cdot)$. Recall the definition of $\hat{f}$ from \eqref{eqn:knn_f}. Let $\mu$ denote the Lebesgue measure and $B_r(x) = \{ x' \in \mathbb{R}^d \ | \ \norm{x' - x} < r \}$. Define
%
\begin{align}
    H^{B,w}(f) &= - \int_{\mc{X}} \log(w(f(x))) w(f(x)) dx \int_{\mc{X}} g(f(x)) f(x) dx, \\
    %
    H^{B,w}_n(f) &= - \sum_{i=1}^n \frac{1}{f(x)} \log(w(f(X_i))) w(f(X_i)) = \frac{1}{n} \sum_{i=1}^n g(f(X_i)), \\
    %
    \widehat{H}^{B,w}_{k,n}(f) &= - \sum_{i=1}^n \frac{1}{\hat{f}(x)} \log(w(\hat{f}(X_i))) w(\hat{f}(X_i)) = \frac{1}{n} \sum_{i=1}^n g(\hat{f}(X_i)).
\end{align}
%
Our goal is to establish a bound on the error
%
\begin{equation} \label{eqn:goal_to_bound}
    \left| \mathbb{E} \left[ \widehat{H}^{B,w}_{k,n}(f) \right] - H^{B,w}(f) \right|.
\end{equation}
%
In general, for finite $k$, even as $n \rightarrow \infty$ the approximator $\widehat{H}^{B,w}_{k,n}(f)$ will remain biased due to the biasedness of $\hat{f}$ for fixed $k$ and the lack of a known bias correction procedure for our BE approximator $\widehat{H}^{B,w}_{k,n}(f)$. This contrasts with the situation for simpler estimators like Shannon and R\'{e}nyi entropies, for which explicit bias correction terms are known (see \cite{singh2003nearest, leonenko2008class, singh2016finite}). Nonetheless, in Theorem \ref{thm:main_bound} we are able to build on existing results to establish a probabilistic bound on \eqref{eqn:goal_to_bound}. We first recall the following result.
%
\begin{lemma}[\citep{singh2016finite}] \label{lem:singh_poczos}
    Suppose that, for some $\xi \in (0, 2]$, $f$ is $\xi$-H\"{o}lder continuous and strictly positive on $\mc{X}$. Suppose furthermore that there exists a function $f_* : \mc{X} \rightarrow \mathbb{R}^+$ and a constant $f^*$ such that $0 < f_*(x) \leq \int_{B_r(x)} f(y) dy / \mu(B_r(x)) \leq f^* < \infty$, for all $x \in \mc{X}, r \in (0, \sqrt{d}]$, and assume that $\int_0^{\infty} e^{-x} x^k f(x) dx < \infty$. Then
    %
    % \begin{align}
    %     \left| \mathbb{E} \left[ \widehat{H}^{B,w}_{k,n}(f) \right] - H^{B,w}(f) \right| &= \mc{O}\left( \frac{k}{n} \right)^{\frac{\xi}{d}}, \\
    %     %
    %     \text{Var} \left( \widehat{H}^{B,w}_{k,n}(f) \right) &= \mc{O}\left( \frac{C_V}{n} \right).
    % \end{align}
    %
    \begin{multicols}{2}
        \noindent
        \small
        \vspace{-8mm}
        \begin{equation} \label{eqn:bias_bound}
            \left| \mathbb{E} \left[ H^{B,w}_n(f) \right] - H^{B,w}(f) \right| = \mc{O}\left( \frac{k}{n} \right)^{\frac{\xi}{d}},
        \end{equation}
        \normalsize
        %
        % \break
        %
        \noindent
        \small
        \begin{equation} \label{eqn:var_bound}
            \text{Var} \left( H^{B,w}_n(f) \right) = \mc{O}\left( \frac{1}{n} \right).
        \end{equation}
        \normalsize
    \end{multicols}
\end{lemma}
%
The proof of this result follows directly from that of \citep[Thm. 5]{singh2016finite} due to the fact that $H^{B,w}_n(f)$ is an unbiased estimator of $H^{B,w}(f)$. Also note that the variance bound can be trivially strengthened to apply to $\widehat{H}^{B,w}_{k,n}(f)$ due to the fact that the latter is simply the sample average of $n$ i.i.d., bounded random variables:
%
\begin{corollary}
    Under the conditions of Lemma \ref{lem:singh_poczos}, $\text{Var} \left( \widehat{H}^{B,w}_{k,n}(f) \right) = \mc{O}\left( \frac{1}{n} \right).$
\end{corollary}

It remains to characterize \eqref{eqn:goal_to_bound}. We first recall another useful result from the literature. For a given set $S \subset \mc{X}$, radius $r$, and $m > 0$, let $\mc{N} \left( S, r \right)$ denote the covering number, the minimum number of balls of radius $r$ needed to cover $S$. Let $\normop{\cdot}$ denote the operator norm.
%
\begin{lemma}[\citep{zhao2022analysis}] \label{lem:zhao_lai}
    Suppose there exist $C_1, C_2, C_3, \mc{N}_0 > 0$ and $\beta \in (0, 1]$ such that the following conditions hold:
    \begin{align*}
        &(i) \quad \frac{\norm{ \nabla f(x) }}{f(x)} \leq C_1; \quad (ii) \quad \frac{ \normop{ \nabla^2 f(x) } }{f(x)} \leq C_2; \quad (iii) \quad \forall t > 0, P(f(x) < t) \leq C_3 t^{\beta}; \\
        %
        &(iv) \quad \mc{N} \left( \{ x | f(x) > m \}, r \right) \leq \frac{\mc{N}_0}{m^{\gamma} r^d}, \text{ for some } \gamma > 0 \text{ and all } m > 0.
    \end{align*} 
    %
    Then, for $\varepsilon > 0$, it holds with probability (w.p.) $1 - \varepsilon$ that
    %
    \begin{equation} \sup_x \left| \hat{f}(x) - f(x) \right| =
        \begin{cases}
            \mc{O}\left( \left( \frac{k}{n} \right)^{\frac{2}{d}} \log n + \sqrt{ \frac{ \log ( n / \varepsilon ) }{k} } \right) & \text{ if } d > 2, \\
            %
            \mc{O}\left( \frac{k}{n} \log n + \sqrt{ \frac{ \log ( n / \varepsilon ) }{k} } \right) & \text{ if } d = 1, 2.
        \end{cases}
    \end{equation}
\end{lemma}

We are now in a position to prove our main result.
%
\begin{customthm}{2} \label{eqn:main_bound}
    Suppose $f$ satisfies the conditions of Lemmas \ref{lem:singh_poczos} and \ref{lem:zhao_lai}. Assume $w$ is Lipschitz continuous. Then, for $\varepsilon > 0$, it holds w.p. $1 - \varepsilon$ that
    %
    \begin{equation} \left| \mathbb{E} \left[ \widehat{H}^{B,w}_{k,n}(f) \right] - H^{B,w}(f) \right| = 
        \begin{cases}
            \mc{O}\left( \frac{k}{n} \right)^{\frac{\xi}{d}} + \mc{O}\left( \left( \frac{k}{n} \right)^{\frac{2}{d}} \log n + \sqrt{ \frac{ \log ( n / \varepsilon ) }{k} } \right) & \text{ if } d > 2, \\
            %
            \mc{O}\left( \frac{k}{n} \right)^{\frac{\xi}{d}} + \mc{O}\left( \frac{k}{n} \log n + \sqrt{ \frac{ \log ( n / \varepsilon ) }{k} } \right) & \text{ if } d = 1, 2.
        \end{cases}
    \end{equation}
\end{customthm}
%
\begin{proof}
    First notice that
    %
    \begin{equation} \label{eq:0}
        \left| \mathbb{E} \left[ \widehat{H}^{B,w}_{k,n}(f) \right] - H^{B,w}(f) \right| \leq \left| \mathbb{E} \left[ \widehat{H}^{B,w}_{k,n}(f) - H^{B,w}_n(f) \right] \right| + \left| \mathbb{E} \left[ H^{B,w}_n(f) \right] - H^{B,w}(f) \right|.
    \end{equation}
    %
    The second term can be bounded using Lemma \ref{lem:singh_poczos}, so it just remains to bound the first term. Recall that $\mc{X}$ is compact, $f$ is bounded strictly away from 0 on $\mc{X}$, and $w$ is Lipschitz. We therefore have that $g$ is the product of Lipschitz, bounded functions and is therefore itself Lipschitz on its domain. Let $K$ denote the minimal Lipschitz parameter of $g$. Rewriting \eqref{eq:0} in terms of $g$, we obtain
    %
    \begin{align}
        \left| \mathbb{E} \left[ \widehat{H}^{B,w}_{k,n}(f) - H^{B,w}_n(f) \right] \right| &= \left| \frac{1}{n} \sum_{i=1}^n \mathbb{E} \left[ g(\hat{f}(X_i)) - g(f(X_i)) \right] \right| \\
        %
        &\leq \frac{1}{n} \sum_{i=1}^n \mathbb{E} \left[ \left| g(\hat{f}(X_i)) - g(f(X_i)) \right| \right] \\
        %
        &\labelrel={eq:2} \mathbb{E} \left[ \left| g(\hat{f}(X_1)) - g(f(X_1)) \right| \right] \\
        %
        &\leq K \mathbb{E} \left[ \left| \hat{f}(X_1) - f(X_1) \right| \right] \\
        %
        &\leq K \mathbb{E} \left[ \sup_x \left| \hat{f}(x) - f(x) \right| \right] % \\
        %
        % &\leq K \sup_x \left| \hat{f}(x) - f(x) \right|,
    \end{align}
    %
    where \eqref{eq:2} follows from the fact that the $X_1, \ldots, X_n$ are i.i.d. An application of the law of total probability and Lemma \ref{lem:zhao_lai} to the last term completes the proof.
\end{proof}