\section{Conclusion}

In this work we developed the theory and practice of behavioral entropy in continuous spaces, enabling the incorporation of human cognitive and perceptual biases into uncertainty perception in complex, real-world scenarios. We first developed and analyzed $k$-nearest neighbor estimators for BE with general probability weightings. We subsequently derived practical reinforcement learning-based methods for maximizing BE under Prelec's probability weighting in sequential decision-making problems, enabling the use of BE as a state space exploration objective in the RL context. Leveraging these algorithmic developments, we experimentally investigated the utility of BE for dataset generation for offline RL. Our experiments demonstrate that BE-generated datasets lead to superior offline RL performance over both Shannon and R\'{e}nyi entropy-generated datasets, that BE is stabler and therefore easier to use as an exploration objective compared with R\'{e}nyi entropy, and that BE-generated datasets lead to improved data- and sample-efficiency for offline RL over existing methods.
%
As a limitation, due to the computational burden of dataset generation and offline RL training for a nontrivial variety of $\alpha$ and $q$ values additional environments and offline RL algorithms were not considered. These limitations are important directions for future work.