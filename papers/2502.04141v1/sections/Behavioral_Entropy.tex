% \section{Differential Behavioral Entropy and its Estimation}

\section{Behavioral Entropy in Continuous Spaces} \label{sec:be_in_continuous_spaces}

\begin{figure}[tp]
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\linewidth]{figs/prelec_cividis.png}
        \caption{\small \textbf{Perception of normal distribution ($\mathcal{N}(0,1)$) under Prelec weighting function.} Black dotted line shows standard normal density, $f$. For $\alpha \approx 0$ the perceived density is uniform, indicating \textit{over-weighting} of uncertainty over entire support of $f$. For $\alpha \gg 0$ the perceived density approaches a step function around the mean, indicating \textit{under-weighting} of uncertainty near $f$'s tails. $\alpha = 1$ recovers original $f$.
        %
        }
        \label{fig:prelec_perception}
    \end{subfigure}
    \centering
    ~
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/bernoulli.png}
        \caption{\small\textbf{Diversity of Shannon, R\'{e}nyi and BE values as a function of Bernoulli trial parameter $p$.} Behavioral entropy $H^B$ captures entire behavior spectrum from overvaluing uncertainty (light blue, $\alpha \approx 0$) to highly undervaluing uncertainty (dark blue, $\alpha \gg 0$). R\'{e}nyi, $H^R$, captures the former (light red, $q \approx 0$), but cannot capture the latter. Dotted red curve shows $H^R$ as $q \rightarrow \infty$. Shannon, $\supscr{H}{S}$, is dotted black curve.
        %
        %
        }
    \label{fig:entropy}
    \end{subfigure}
    %
    \caption{\small Visualizations of probability weightings (left) and superior expressiveness of BE (right).}
    \label{fig:prob_ent}\
    
\vspace{-4mm}
\end{figure}



\textbf{Behavioral Entropy.} The various notions of entropy that have been studied in the literature since the initial work by \cite{shannon1948entropy} quantify the uncertainty inherent in a random variable by measuring how evenly distributed its associated probability density is over its support. Let $X$ be a discrete random variable over a finite set of $M$ elements, and let $p$ denote its probability mass function (p.m.f.). Two classical and widely used entropies are the Shannon and R\'{e}nyi entropies \citep{shannon1948entropy, renyi1961measures}, given respectively by
%
\begin{multicols}{2}
    \noindent
    \begin{equation} \label{eqn:shannon}
        H^S(X) = - \sum_{i=1}^M \log(p_i) p_i,
    \end{equation}
    %
    % \break
    %
    \noindent
    \begin{equation} \label{eqn:renyi}
        H^R_q(X) = \frac{1}{1-q} \log \sum_{i=1}^M p_i^q, \ q > 0, q \neq 1.
    \end{equation}
\end{multicols}
%
These entropy functionals, along with others such as Tsallis entropy \citep{tsallis1988possible}, belong to the class of \textit{admissible generalized entropies} satisfying the first three Shannon-Khinchin axioms (see \citep{Amigo2018entropy} for details). These axioms ensure that an entropy functional is well-behaved by ensuring their continuity, stability with respect to addition or removal of known outcomes, and maximality of the uniform distribution.

As mentioned in the introduction, the recent work \citep{suresh2024robotic} proposed composing Shannon's entropy with the probability weighting functions widely used in behavioral economics to encode human cognitive and perceptual biases. The composition of Shannon's entropy with Prelec's probability weighting function \cite{prelec1998probability} yielded BE, an admissible generalized entropy that provides a tractable mathematical formalism for incorporating helpful human biases into uncertainty quantification via entropy. The probability weighting functions used to encode human biases are defined as follows (see \citep[Ch. 2.2]{dhami2016foundations}).
%
\begin{definition} \label{def:prob_weighting}
    A function $w : [0, 1] \rightarrow [0, 1]$ is said to be a \textbf{probability weighting function} if it is continuous, strictly increasing, satisfies $w(0) = 0$ and $w(1) = 1$, and has a unique, continuous, and strictly increasing inverse.
\end{definition}
%
The most widely used probability weighting function, and that which was the focus of \cite{suresh2024robotic}, is Prelec's probability weighting, given by
%
\begin{equation} \label{eqn:prelec_w}
    w(x) = e^{-\beta(-\log x)^{\alpha}}, \quad \alpha, \beta > 0.
\end{equation}
%
Prelec's function is smooth in both the probability and parameter space and has the ability to control the fixed point and shape of the weighting function (see \citep{prelec1998probability, dhami2016foundations} for details). Figure \ref{fig:prelec_perception} illustrates its effect on a Gaussian density.
%
Equipped with \eqref{eqn:prelec_w}, \cite{suresh2024robotic} proposed and studied the following.
%
\begin{definition} Letting $w$ be as in \eqref{eqn:prelec_w}, \textbf{behavioral entropy} is given by
    \begin{equation} \label{eqn:behavioral_entropy}
        \supscr{H}{B}(X) = -\sum_{i=1}^M w(p_i) \log(w(p_i)).
    \end{equation}
\end{definition}
%
The parameter $\alpha$ controls the shape of the perceived probability curve, enabling a wide range of probability perceptions (see Figure \ref{fig:prelec_perception}) and the resulting perceived entropies, as illustrated in Figure \ref{fig:entropy}. Under the condition that $\beta = e^{(1-\alpha)\log(\log(M))}$, \eqref{eqn:behavioral_entropy} was shown in \cite{suresh2024robotic} to belong to the class of admissible generalized entropies. 
%
With this conditioning, \eqref{eqn:prelec_w} allows control of the third fixed point of $w$, which is critical for ensuring BE remains an admissible entropy, whereas other probability weighting functions lack this property \citep{dhami2016foundations} and do not generate meaningful, admissible generalized entropies.

Interestingly, it was shown in \citep{suresh2024robotic} that using BE over other approaches led to significant acceleration of robotic exploration tasks as well as emergent search behaviors similar to breadth-first and depth-first search, depending on choice of $\alpha$. These results indicate that BE holds promise as an objective for a broad range of exploration tasks in complex environments, yet \cite{suresh2024robotic} only applied BE to discrete, binary random variables. To pave the way for the application of BE to more complex problems, we now extend it to continuous settings.
%



\textbf{Differential Behavioral Entropy.} In this subsection we extend the definition of BE to that of differential BE, providing a novel entropy functional that is applicable in continuous, potentially high-dimensional spaces.
%
Let $f \in \Delta(\mc{X})$ be a p.d.f. over $\mc{X} \subset \mathbb{R}^d$, where $d \in \mathbb{N}^+$. We first recall the differential versions of Shannon's entropy and R\'{e}nyi entropy of order $q$, where $q > 0, q \neq 1$, which are defined, respectively, by
%
\begin{multicols}{2}
    \noindent
    \begin{equation} \label{eqn:diff_shannon}
        H^S(f) = - \int_{\mc{X}} \log (f(x)) f(x) dx,
    \end{equation}
    %
    % \break
    %
    \noindent
    \begin{equation} \label{eqn:diff_renyi}
        H^R_q(f) = \frac{1}{1 - q} \log \int_{\mc{X}} f^q(x) dx.
    \end{equation}
\end{multicols}
%
We will use these expressions in our experimental comparisons below and thus include them here for easy reference. We next state the definitions of our continuous-spaces analogues of \eqref{eqn:behavioral_entropy}.
%
\begin{definition}
    For an arbitrary probability weighting function $w$, \textbf{differential generalized behavioral entropy} is given by
    %
    \begin{equation} \label{eqn:diff_gbe}
        H^{B,w}(f) = - \int_{\mc{X}} \log(w(f(x))) w(f(x)) dx.
    \end{equation}
% \end{definition}
%
%
%
In particular, substituting Prelec's probability weighting from \eqref{eqn:prelec_w} into \eqref{eqn:diff_gbe} yields \textbf{differential behavioral entropy}, given by
%
\begin{equation} \label{eqn:diff_be}
    H^{B,\alpha,\beta}(f) = \beta \int_{\mc{X}} e^{-\beta ( - \log (f(x)))^{\alpha}} (-\log f(x))^{\alpha} dx.
\end{equation}
\end{definition}
%
It is important to note that, unlike Definition \ref{def:prob_weighting} for probability weightings in the discrete setting, where $w : [0, 1] \rightarrow [0, 1]$ in the continuous setting $w$ must be generalized to $w : [0, \infty) \rightarrow [0, \infty)$ to accommodate arbitrary densities, $f$. Desirable structural properties of $w$ are described in the detailed statement of Theorem \ref{thm:main_bound} in the appendix. 
%
We will henceforth abuse both terminology and notation by omitting ``differential'' when referring to differential entropies and by suppressing the dependence of \eqref{eqn:diff_be} on $\alpha, \beta$ when these are clear from context.



\section{$k$-Nearest Neighbor Behavioral Entropy Estimation}

We next turn to the problem of BE estimation in continuous, potentially high-dimensional spaces. To accomplish this, we derive $k$-nearest neighbor ($k$-NN) estimates of BE along the lines of the estimates studied in \citep{kozachenko1987sample, singh2003nearest, leonenko2008class, sricharan2012estimation, singh2016finite} and others for Shannon and R\'{e}nyi entropy. The $k$-NN family of nonparametric estimators enables estimation of arbitrary densities from a finite number of i.i.d. samples in continuous and potentially high-dimensional settings, making them particularly well-suited to the RL context considered in the following section.
%
%
Let $f \in \Delta(\mathbb{R}^d)$ be a probability density function (p.d.f.) over $\mathbb{R}^d$, where $d \in \mathbb{R}^+$. Let $X_1, X_2, \ldots, X_n \sim f(\cdot)$ be $n \in \mathbb{N}^+$ i.i.d. samples drawn from $f$. In \citep{loftsgaarden1965nonparametric, devroye1977strong} and subsequent works it was established that, for suitably chosen $n$ and $k$, a reasonable approximation of $f$ is provided by the $k$-NN density estimator
%
\begin{equation} \label{eqn:knn_f}
    \hat{f}(x) = \frac{ k \Gamma(d / 2 + 1) }{ n \pi^{d / 2} R^d_{k, n}(x) },
\end{equation}
%
where $R_{k, n}(x) = \norm{x - NN_k(x)}$ is the Euclidean distance between $x$ and its $k$th nearest neighbor among $\{ X_1, \ldots, X_n \}$ and $\Gamma(x) = \int_0^{\infty} t^{x-1} e^{-t} dt$ is the gamma function. When $x = X_i$, we will write $R_{i, k, n} = R_{k, n}(X_i)$ for simplicity. A natural first approximation to Shannon entropy of $f$ is given by the plug-in estimator
%
\begin{equation} \label{eqn:shannon_knn_approx_1}
    \widehat{H}^{S}_{k,n}(f) = - \frac{1}{n} \sum_{i=1}^n \log \hat{f}(X_i) \approx \mathbb{E}_{X_i \sim f(\cdot)} \left[ - \log \hat{f}(X_i) \right] = - \int_{\mathbb{R}^n} \log \hat{f}(x) \cdot f(x) dx.
\end{equation}
%
With this in mind, the na\"{i}ve approach to estimating \eqref{eqn:diff_gbe} for general $w$ is via
%
\begin{equation} \label{eqn:be_knn_wrong_1}
    \widetilde{H}^{B,w}_{k, n}(f) = - \frac{1}{n} \sum_{i=1}^n w(\hat{f}(X_i)) \log w(\hat{f}(X_i)).
\end{equation}
%
Since $X_i \sim f(\cdot)$, however, the estimator of \eqref{eqn:be_knn_wrong_1} is biased, since
%
\begin{equation} \label{eqn:be_knn_wrong_2}
    \widetilde{H}^{B,w}_{k,n}(f) \approx \mathbb{E}_{X_i \sim f(\cdot)} \left[ - w(\hat{f}(X_i)) \log w(\hat{f}(X_i)) \right] = - \int_{\mathbb{R}^n} w(\hat{f}(x)) \log w(\hat{f}(x)) \cdot f(x) dx.
\end{equation}
%
Dividing by the approximation $\hat{f}$ yields the alternative, importance sampling-corrected estimator
%
\begin{align}
    \widehat{H}^{B,w}_{k,n}(f) &= - \frac{1}{n} \sum_{i=1}^n \frac{1}{\hat{f}(X_i)} w(\hat{f}(X_i)) \log w(\hat{f}(X_i)) \label{eqn:be_knn_approx} \\
    %
    &\approx \mathbb{E}_{X_i \sim f(\cdot)} \left[ \frac{1}{\hat{f}(X_i)} w(\hat{f}(X_i)) \log w(\hat{f}(X_i)) \right] = - \int_{\mathbb{R}^n} w(\hat{f}(x)) \log w(\hat{f}(x)) \frac{f(x)}{\hat{f}(x)} dx. \nonumber
\end{align}
%
Though this importance sampling-corrected plug-in estimator will be biased for the same reasons detailed in \cite[Thm. 8]{singh2003nearest} for $\widehat{H}^S_{k,n}(f)$, for large $n$ and suitable $k$ \eqref{eqn:be_knn_approx} provides a reasonable estimator of \eqref{eqn:diff_be}, as characterized by the following results.

\begin{theorem} \label{thm:convergence}
    Suppose that $k := k_n \rightarrow \infty, \frac{k_n}{n} \rightarrow 0$, and $\frac{k_n}{\log n} \rightarrow \infty$ as $n \rightarrow \infty$. Assume that $w$ is Lipschitz, that $f$ is absolutely continuous, and that there exist $c_1, c_2 > 0$ such that $0 < c_1 \leq f(x) \leq c_2 < \infty$, for all $x \in \mc{X}$. Then $\widehat{H}^{B,w}_{k,n}(f) \rightarrow H^{B,w}(f)$ both uniformly and in probability.
\end{theorem}
%
The result follows from the strong uniform consistency of $\hat{f}$ \citep{devroye1977strong}.

Though asymptotic guarantees like Theorem \ref{thm:convergence} are somewhat reassuring, in practice we will use finite $k$ in our $k$-NN estimators and therefore need a more fine-grained characterization of the bias and variance.
%
Unfortunately, for finite $k$, the approximator $\widehat{H}^{B,w}_{k,n}(f)$ remains biased as $n \rightarrow \infty$ due to the biasedness of $\hat{f}$ for fixed $k$ and the lack of a known bias correction procedure for our BE approximator $\widehat{H}^{B,w}_{k,n}(f)$. This contrasts with the situation for simpler estimators like those for Shannon and R\'{e}nyi entropies, for which explicit bias correction terms are known (see \cite{singh2003nearest, leonenko2008class, singh2016finite}). Nonetheless, we establish probabilistic guarantees on the bias and variance of our proposed BE estimator in the following main result.
%
\begin{theorem} \label{thm:main_bound}
    Suppose $f$ and $w$ satisfy suitable differentiability and continuity conditions. Then, for $\varepsilon > 0$, it holds with probability $1 - \varepsilon$ that
    %
    \begin{align} \left| \mathbb{E} \left[ \widehat{H}^{B,w}_{k,n}(f) \right] - H^{B,w}(f) \right| &= 
        \begin{cases}
            \mc{O}\left( \frac{k}{n} \right)^{\frac{\xi}{d}} + \mc{O}\left( \left( \frac{k}{n} \right)^{\frac{2}{d}} \log n + \sqrt{ \frac{ \log ( n / \varepsilon ) }{k} } \right) & \text{ if } d > 2, \\
            %
            \mc{O}\left( \frac{k}{n} \right)^{\frac{\xi}{d}} + \mc{O}\left( \frac{k}{n} \log n + \sqrt{ \frac{ \log ( n / \varepsilon ) }{k} } \right) & \text{ if } d = 1, 2,
        \end{cases}
        %
        \\
        %
        \text{Var} \left( \widehat{H}^{B,w}_{k,n}(f) \right) &= \mc{O}\left( \frac{1}{n} \right).
    \end{align}
    %
\end{theorem}
%
The proof and a precise statement are provided in the appendix. Equipped with the $k$-NN estimators and theoretical guarantees derived above, we next turn to their application in the RL setting.