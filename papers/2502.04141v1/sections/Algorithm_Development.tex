\section{Behavioral Entropy in the Reinforcement Learning Context}

In this section we leverage the $k$-nearest neighbor generalized behavioral entropy estimates developed in the previous section for general probability weightings to derive a practical reward function that can be used in conjunction with standard RL methods to maximize behavioral entropy of an RL agent's state occupancy measure.



\textbf{Behavioral Entropy as an RL Objective.} State occupancy measure entropy has been used as an exploration objective for RL in a wide range of previous works \citep{hazan2019provably, liu2021behavior, yarats2021reinforcement, zhang2021exploration, yuan2022renyi}. In order to formally define behavioral entropy for state occupancy measures in this context, we first provide preliminary background on Markov decision processes (MDPs).
%
Let an average-reward MDP $\mathcal{M} = (\mc{S}, \mc{A}, p, r)$ be given, where $\mc{S}$ is the state space, $\mc{A}$ is the action space, $p: \mc{S} \times \mc{A} \rightarrow \Delta(\mc{S})$ is the transition probability kernel mapping state-action pairs $(s, a) \in \mc{S} \times \mc{A}$ to probability distributions $p(\cdot | s, a) \in \Delta(\mc{S})$ over the state space, and $r : \mc{S} \times \mc{A} \rightarrow \mathbb{R}$ is the reward function. Given a policy $\pi : \mc{S} \rightarrow \Delta(\mc{A})$ mapping states to probability distributions over the action space, $\mc{M}$ evolves as follows: at timestep $t \in \mathbb{N}$, the system is in state $s_t$, action $a_t \sim \pi(\cdot | s_t)$ is selected and executed, a reward $r_t = r(s_t, a_t)$ is received, the state transitions according to $s_{t+1} \sim p(\cdot | s_t, a_t)$, and the process repeats. To each policy $\pi$ is associated the long-run average reward $J(\pi) = \lim_{T \rightarrow \infty} \frac{1}{T} \mathbb{E}_{\pi} [ \sum_{i=0}^{T-1} r_i ]$, and the goal is to determine an optimal policy $\pi^* = \argmax_{\pi} J(\pi)$.

Under mild conditions on the transition kernel and policy (see \citep{puterman2014markov}), each policy $\pi$ induces a state occupancy measure $d_{\pi}(\cdot) \in \Delta(\mc{S})$ capturing the long-run state visitation behavior induced by $\pi$ over $\mc{S}$.
%
When $\mc{S}$ is continuous, for a measurable subset $B \subset \mc{S}$ we have $d_{\pi}(B) = \lim_{t \rightarrow \infty} P(s_t \in B)$.
%
We will henceforth assume that each measure $d_{\pi}$ has a corresponding p.d.f. and abuse notation by denoting the value of this p.d.f at $s$ by $d_{\pi}(s)$.
%
\footnote{Similarly, $\pi$ induces a state-action occupancy measure $\lambda_{\pi}(s, a) = d_{\pi}(s) \pi(a | s)$. In this work we focus on state occupancy measures, but all results and methods can be extended to apply to state-action occupancy measures in a straightforward manner.}
%
State occupancy measure entropies can be obtained by directly substituting $f = d_{\pi}$ and $\mc{X} = \mc{S}$ in the entropy definitions in \eqref{eqn:diff_shannon}, \eqref{eqn:diff_renyi}, and \eqref{eqn:diff_be}. In particular, the behavioral entropy induced by $\pi$ resulting from this substitution in \eqref{eqn:diff_be} is given by
%
\begin{equation} \label{eqn:be_occ_meas}
    H^{B,\alpha,\beta}(d_{\pi}) = \beta \int_{\mc{S}} e^{-\beta (-\log(d_{\pi}(s)))^\alpha} (-\log d_{\pi}(s))^{\alpha} ds.
\end{equation}
%
We propose to use \eqref{eqn:be_occ_meas} as an exploration objective. To achieve this, we leverage the $k$-NN estimator of \eqref{eqn:be_knn_approx} developed in the preceding section to derive a reward function $r$ such that $J(\pi) \approx H^{B,\alpha,\beta}(d_{\pi})$ in the following subsection.



\textbf{Behavioral Entropy Reward Derivation.} We next build on the $k$-NN estimator of \eqref{eqn:be_knn_approx} to derive a practical reward function $r$ such that $J(\pi) \approx H^{B,\alpha,\beta}(d_{\pi})$. Our derivation is similar to the reward derivations followed in \citep{liu2021behavior, yarats2021reinforcement} for Shannon entropy and \citep{yuan2022renyi} for R\'{e}nyi entropy. Once we are equipped with this reward, we can leverage existing RL methods to learn behavioral entropy-maximizing exploration policies.
%
Let $s_1, \ldots, s_n \sim d_{\pi}(\cdot)$. Substituting $f = d_{\pi}$ and $s_i = X_i$ into \eqref{eqn:knn_f}, for $i = 1, \ldots, n$, we have that $\widehat{d}_{\pi}(s_i) = k \Gamma(d/2 + 1) / n \pi^{d/2} R^d_{i,k,n}$, where $R_{i, k, n} = \norm{s_i - NN_k(s_i)}$ and $NN_k(s)$ denotes the $k$-NN of $s_i$ within $\{s_i\}_{i = 1, \ldots, n}$. Recalling that $w(x) = e^{-\beta(-\log(x))^{\alpha}}$, we can write
%
\begin{align}
    w(\widehat{d}_{\pi}(s_i)) &= e^{-\beta \left( -\left[ \log(k \Gamma(d/2 + 1)) - \log(n \pi^{d/2} R_{i, k, n}^d) \right] \right)^{\alpha}} \label{eqn:r_deriv:1} \\
    %
    &= e^{-\beta \left( d \log R_{i, k, n} + D_{k, n} \right)^\alpha}, \label{eqn:r_deriv:2}
\end{align}
%
where $D_{k,n} = - \log (k \Gamma(d/2 + 1)) + \log(n \pi^{d/2}) = \log \left( (n \pi^{d/2}) / (k \Gamma(d/2 + 1)) \right)$. Substituting \eqref{eqn:r_deriv:2} into \eqref{eqn:be_knn_approx} gives
%
\begin{align}
    \widehat{H}^{B,\alpha,\beta}_{k,n}(d_{\pi}) &= - \frac{1}{n} \sum_{i=1}^n \frac{1}{\widehat{d}_{\pi}(s_i)} w(\widehat{d}_{\pi}(s_i)) \log w(\widehat{d}_{\pi}(s_i)) \label{eqn:r_deriv:3} \\
    %
    &= \frac{\beta}{n} \sum_{i=1}^n \frac{n \pi^{d/2} R^d_{i,k,n}}{k \Gamma(d/2 + 1)} e^{-\beta (d \log R_{i,k,n} + D_{k,n})^{\alpha}} \left( d \log R_{i,k,n} + D_{k,n} \right)^{\alpha} \label{eqn:r_deriv:4} \\
    %
    &\propto \frac{1}{n} \sum_{i=1}^n R_{i,k,n}^d e^{-\beta ( d \log R_{i,k,n} + D_{k,n} )^{\alpha}} \left( d \log R_{i,k,n} + D_{k,n} \right)^{\alpha} \label{eqn:r_deriv:5} \\
    %
    &\appropto \frac{1}{n} \sum_{i=1}^n R_{i,k,n}^d e^{-\beta ( d \log R_{i,k,n} )^{\alpha}} \left( d \log R_{i,k,n} \right)^{\alpha}.\label{eqn:r_deriv:6}
\end{align}
%
where the approximate proportionality in \eqref{eqn:r_deriv:6} follows from the fact that, under suitable conditions on $n, k$ (see, e.g., Theorem \ref{thm:convergence}) the contribution of $D_{k,n}$ to the value of \eqref{eqn:r_deriv:5} is negligible.
%
Since $\mathbb{E}_{\pi} [ \widehat{H}^{B,\alpha,\beta}_{k,n}(d_{\pi}) ] \approx H^{B, \alpha, \beta} (d_{\pi})$ by Theorems \ref{thm:convergence} and \ref{thm:main_bound}, and since \eqref{eqn:r_deriv:6} is approximately proportional to $\widehat{H}^{B,\alpha,\beta}_{k,n}(d_{\pi})$, \eqref{eqn:r_deriv:6} suggests
%
\begin{equation} \label{eqn:r_deriv:7}
    \widetilde{r}(s, a) = \norm{s - NN_k(s)}^d e^{-\beta(d \log \norm{s - NN_k(s)})^{\alpha}} \left( d \log \norm{s - NN_k(s)} \right)^\alpha
\end{equation}
%
as a suitable proxy reward for maximizing behavioral entropy in an RL context. For numerical stability, we follow \citep{yarats2021reinforcement, liu2021behavior} by making the additional simplification of setting $d=1$ and adding a constant $c > 0$ inside the logarithms to obtain
%
\begin{equation} \label{eqn:r_final}
    r(s, a) = \norm{s - NN_k(s)} e^{-\beta(\log( \norm{s - NN_k(s)} + c ) )^{\alpha}} \left( \log( \norm{s - NN_k(s)} + c ) \right)^\alpha.
\end{equation}
%
A visualization of \eqref{eqn:r_final} with a comparison to the SE reward function is provided in Fig.~\ref{fig:be_rwrd_fn} in the appendix. Armed with this reward, any standard RL method can be applied to learn exploration policies approximately maximizing BE using \eqref{eqn:be_occ_meas}. We illustrate its application in data generation for offline RL in the next section.
%
\new{We note that implementing the $k$-NN estimator in \eqref{eqn:r_final} can be computationally challenging in high dimensions for large $k$ values due to the well-known curse of dimensionality of suffered by $k$-NN estimators \cite{beyer1999nearest}.
%
%
To address this, in practice $k \leq 15$ is selected and dimension reduction to a feature space of manageable dimensions is performed before $k$-NN estimation is carried out, thereby limiting computational costs \cite{liu2021behavior, yarats2021reinforcement}. 
%
}