\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{style}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}  
\usepackage{graphicx}
\usepackage{amsmath} 
\usepackage{amssymb}
\newcommand{\mypara}[1]{\medskip\noindent{\bf {#1}.}~}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

\date{}

\title{\bf PDA: Generalizable Detection of AI-Generated Images via Post-hoc Distribution Alignment}

\author{
Li Wang\ \ \
Wenyu Chen\ \ \
Zheng Li\ \ \
Shanqing Guo\ \ \
\\
\\
\textit{Shandong University} \ \ \ 
}

\maketitle

%-------------------------------------------------------------------------------
\begin{abstract} 
The rapid advancement of generative models has led to the proliferation of highly realistic AI-generated images, posing significant challenges for detection methods to generalize across diverse and evolving generative techniques. Existing approaches often fail to adapt to unknown models without costly retraining, limiting their practicability. To fill this gap, we propose Post-hoc Distribution Alignment (PDA), a novel approach for the generalizable detection for AI-generated images. The key idea is to use the known generative model to regenerate undifferentiated test images. This process aligns the distributions of the re-generated real images with the known fake images, enabling effective distinction from unknown fake images. PDA employs a two-step detection framework: 1) evaluating whether a test image aligns with the known fake distribution based on deep k-nearest neighbor (KNN) distance, and 2) re-generating test images using known generative models to create pseudo-fake images for further classification. This alignment strategy allows PDA to effectively detect fake images without relying on unseen data or requiring retraining. Extensive experiments demonstrate the superiority of PDA, achieving 96.73\% average accuracy across six state-of-the-art generative models, including GANs, diffusion models, and text-to-image models, and improving by 16.07\% over the best baseline. Through t-SNE visualizations and KNN distance analysis, we provide insights into PDA's effectiveness in separating real and fake images. Our work provides a flexible and effective solution for real-world fake image detection, advancing the generalization ability of detection systems.
\end{abstract}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------------------------

The rapid advancement of generative models, such as GANs~\cite{goodfellow2014generative} and diffusion models~\cite{rombach2022high,kim2022diffusionclip}, has led to a surge in the creation of highly realistic AI-generated images. 
These images present significant challenges for identity authentication, content verification, and media forensics~\cite{wang2024dp,zhang2023text,kang2023scaling}. 
As the ability to create highly convincing fake images becomes more widespread, there is an urgent need for reliable and generalized detection methods, the need for reliable, generalized detection methods is becoming more urgent particularly in real-world scenarios where models must adapt to various types of generated images~\cite{ojha2023towards,corvi2023intriguing,barrett2023identifying}.

Existing detection methods typically rely on binary classifiers trained using features extracted from specific generative models~\cite{heidari2024deepfake}. Although these approaches achieve high accuracy on known datasets (see \autoref{fig:illustration}(a)), they often fail to generalize to images generated by unseen or emerging models~\cite{tan2024rethinking, zhang2024boosting}. For instance, \autoref{fig:illustration}(b) demonstrates that a detector trained on fake images from Stable Diffusion V1.4~\cite{rombach2022high} may fail to distinguish between real images and unknown fake images, such as those generated by VQDM~\cite{gu2022vector} and BigGAN~\cite{brock2018large}, due to artifact differences and distribution shifts~\cite{liang2024comprehensive}.
This reveals a critical limitation in current detection paradigms: the detection performance degrades significantly when faced with fake images from unseen generative models, often leading to high-confidence misclassifications of out-of-distribution fake images as real~\cite{sun2021react,djurisic2022extremely}.
Additionally, existing methods attempt to improve their performance by retraining or fine-tuning unseen data~\cite{chen2022ost}. However, retraining or fine-tuning models is computationally expensive, and accessing labeled generated data is often difficult and impractical, especially in real-world scenarios where generative models are evolving.

To address these challenges, we propose PDA, a novel generalizable detection of AI-generated images through post-hoc distribution alignment. 
PAD leverages the distinct feature distributions of real and fake images generated by known models.
The key idea is to re-generate the indistinguishable test images using the same generative model. In this process, real images are transformed into known fake images, replicating the same artifacts and features as the previously known fake images. In contrast, the unknown fake images, when regenerated, continue to exhibit different artifacts and distribution shifts compared to the known fake images (as shown in \autoref{fig:illustration}(c)). At this point, the distributions of these re-generated real images are aligned with the known fake image and can thus be effectively distinguished from the unknown fake images.
Specifically, PDA employs a two-step detection framework: 1) evaluating whether a test image aligns using the known fake distribution with deep KNN distances and a threshold-based criterion, and 2) regenerating test images using known generative models to create pseudo-fake images for further classification. This alignment strategy enables PDA to detect fake images without relying on unseen data or retraining for new generative models.

Overall, PDA performs secondary forgery of the test images using a known generative model and efficiently classifies the data based on deep k-nearest neighbor (KNN) distance~\cite{sun2022out,bergman2020deep}. This generalized approach can effectively detect fake images even when trained on a single generative model, making it highly adaptable and scalable for real-world applications.
PDA offers several key advantages: a) independence from unknown data. The thresholds are estimated solely based on known fake data, ensuring reliable classification without reliance on unseen samples; b) model-agnostic detection. It is compatible with various detection architectures and loss functions (as shown in Section \ref{backbone}), making the method flexible and adaptable; c) efficiency and simplicity. The detection process requires no fine-tuning or complex processing during testing, relying on a straightforward threshold-based classification.
Extensive experiments demonstrate the strong generalization of our PDA, achieving an average accuracy of 96.73\% across six state-of-the-art generative models, including GANs, diffusion models, and text-to-image models, which is 16.07\% higher than the best baseline. Furthermore, through t-SNE visualizations and KNN distance analysis, we provide insights into the effectiveness of PDA in distinguishing real from fake images. 
Additionally, we investigate key factors that may influence PDA and evaluate its robustness against various image transformations.

\begin{figure}[!t]
\centering
\begin{minipage}{0.32\linewidth}
    \centerline{\includegraphics[width=\textwidth]{./figure/t-sne/raw/sne_origin_fake_real_20.pdf}}
    \centerline{(a)}
\end{minipage}
\begin{minipage}{0.32\linewidth}
    \centerline{\includegraphics[width=\textwidth]{./figure/t-sne/raw/sne_origin_biggan_vqdm_20.pdf}}
    \centerline{(b)}
\end{minipage}
\begin{minipage}{0.32\linewidth}
    \centerline{\includegraphics[width=\textwidth]{./figure/t-sne/re-generated/sne_generate_biggan_vqdm_20.pdf}}
    \centerline{(c)}
\end{minipage}
\caption{t-SNE visualization. The feature representations from the penultimate layer of a detector trained on Stable Diffusion V1.4-generated and real images.}
\label{fig:illustration}
\end{figure}

The main contributions of this paper are as follows:
\begin{itemize}
\item  We propose PDA, a generalizable framework for detecting AI-generated images. PDA leverages post-hoc distribution alignment to effectively distinguish real images from fake ones, including those generated by unknown models, without requiring retraining or access to unknown data.
\item PDA introduces a two-step detection strategy: 1) aligning test images with known fake distributions using deep KNN distances and a threshold-based criterion, and 2) generating pseudo-fake samples for further classification, ensuring efficient and robust detection across diverse generative models.
\item PDA achieves superior performance, outperforming strong baselines by 16.07\% and achieving  96.73\% average detection accuracy across six generative models. Through t-SNE visualizations and KNN distance analysis, we provide insights into PDA's effectiveness in separating real and fake images. Additionally, we evaluate we investigate key factors that may influence PDA and evaluate its robustness against various image transformations, demonstrating its practicality for real-world applications.
\end{itemize}

%-------------------------------------------------------------------------------
\section{Related Works}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\subsection{Evolution of AI-generated Images}
%-------------------------------------------------------------------------------
The rapid advancement of generative models has significantly improved the quality and diversity of AI-generated images.
Early approaches, such as Variational Autoencoders (VAEs)~\cite{kingma2013auto}, focused on learning latent representations of data distributions. 
Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative}, through their adversarial training framework, enabled high-fidelity image generation, with subsequent models like BigGAN~\cite{brock2018large} and StyleGAN~\cite{karras2019style} further enhancing controllability and realism.
Recently, diffusion models \cite{ho2020denoising} have emerged as a powerful alternative to GANs, achieving state-of-the-art results in image generation. These models gradually denoise a random distribution to generate highly detailed and diverse images~\cite{corvi2023detection}.
Additionally, autoregressive models such as DALL·E~\cite{ramesh2022hierarchical}, along with diffusion-based models like Stable Diffusion 1.4~\cite{rombach2022high}, GLIDE~\cite{nichol2021glide}, and ADM~\cite{dhariwal2021diffusion}, have further pushed the boundaries by enabling highly realistic text-to-image generation. These models have significantly broadened the applicability of AI-generated images across a variety of domains, including creative content generation, artistic design, and virtual reality ~\cite{gu2022vector,ricker2022towards}.

However, the increasing realism of AI-generated images has raised concerns about their potential misuse, such as creating deepfakes or spreading misinformation. This has motivated research into robust detection techniques capable of generalizing across diverse generative models, as the subtle artifacts of generated images make them increasingly difficult to distinguish from real ones.

%-------------------------------------------------------------------------------
\subsection{Detection of AI-generated Images}
%-------------------------------------------------------------------------------
The detection of AI-generated images has become a critical research area due to the potential misuse of generative technologies~\cite{yan2023deepfakebench}. 
Early methods focused on identifying GAN-specific artifacts, such as pixel-level inconsistencies, using handcrafted features or traditional classifiers~\cite{marra2019gans,li2020face,mccloskey2019detecting}. While effective for low-quality or early-generation fake images, these approaches struggled to generalize to more advanced models. With the rise of deep learning, Convolutional Neural Networks (CNNs) have been widely adopted for their ability to automatically learn discriminative features~\cite{wang2024deepfake,rossler2019faceforensics++}. For example, Wang et al.~\cite{wang2020cnn} find that pre-trained CNNs on ProGAN-generated images can generalize to other GAN-based images, benefiting from large-scale training on diverse LSUN~\cite{yu2015lsun} object categories.

However, as generative models such as diffusion models and text-to-image models have gained traction, new challenges have emerged for detection tasks~\cite{stein2024exposing}. Unlike GANs, which often introduce visible artifacts, diffusion models produce highly realistic images with minimal visual discrepancies~\cite{corvi2023intriguing}. Zhu et al.~\cite{zhu2024genimage} highlight that classifiers trained solely on GAN-based images struggle to generalize to diffusion-based generated images, as the two types of synthetic images exhibit distinct fingerprints.
To address this, Wang et al.~\cite{wang2023dire} proposed the Diffusion Reconstruction Error (DIRE), which leverages the insight that real images cannot be accurately reconstructed by diffusion models. While effective for diffusion-generated images, DIRE fails to generalize to text-to-image generation models, as demonstrated by Sha et al. in ZeroFake~\cite{sha2024zerofake}. ZeroFake~\cite{sha2024zerofake} improves upon DIRE by exploiting the differential response of fake and real images to the adversary prompts during the inversion and reconstruction process, showing superior performance in detecting fake images generated by text-to-image models. While ZeroFake does not require retraining, its adversarial prompt and reconstruction process are computationally expensive. Moreover, its reliance on case-specific thresholds (e.g., fake image detection and fake artwork detection), further limiting its practicality in open-world scenarios.

In summary, existing detection methods face significant challenges in generalizing to unseen generative models~\cite{ojha2023towards}. As the landscape of generative models continues to diversify, there is a growing need for a universal detection method that can detect fake images generated by any model, even when only a single generative model is used during training. 

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.98\linewidth]{./figure/framework1.pdf}
  \caption{ The overall framework of our PAD. It consists of two key steps: (1) aligning test images with known fake distributions, and (2) generating pseudo-fake samples for further classification using deep KNN distances and a threshold-based criterion.}
\label{fig: framework}
\end{figure*}
%-------------------------------------------------------------------------------
\section{PDA for Generalizable Detection}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\subsection{Design Intuition}
%-------------------------------------------------------------------------------
As aforementioned, existing detection methods typically train binary classifiers to distinguish between real and fake images. These classifiers perform well when tested on fake images from the same distribution as the training data; however, they often misclassify unknown fake images as real due to a distribution shift. This shift occurs because different generative models introduce distinct artifacts and features.

Our approach builds on a previously trained classifier that can already distinguish known fake images from real and unknown ones. The key idea is to use the same generative model to regenerate these undifferentiated images. In this process, real images are converted to known fake images, replicating the same artifacts and features as the previously known fake images. In contrast, the unknown fake images, when regenerated, continue to exhibit different artifacts and distribution shifts compared to the known fake images. 
At this point, the distributions of these re-generated real images are aligned with the known fake image and can thus be effectively distinguished from the unknown fake images.

%-------------------------------------------------------------------------------
\subsection{Overview}
%-------------------------------------------------------------------------------
Following the above idea, we propose a generalize detection for AI-generated images based on Post-hoc Distribution Alignment (PDA). 
Our approach leverages the distinct feature distributions of real and fake images generated by known models. 
The overall framework of our PAD method is shown in \autoref{fig: framework}.
Specifically, the framework starts by training a CNN extractor using fake images generated by any known generative model, along with real images. This detector effectively learns the distribution of the known fake samples and is used to extract feature representations. For each test image, the PDA component first checks its alignment with the distribution of known fake images. If aligned, the image is classified as fake. If not, the image undergoes a secondary forgery process using the known generative model to create a pseudo-fake sample. If this re-generated image aligns with the known fake distribution, it is classified as real; otherwise, it is classified as fake.
The key idea is that real images, when re-generated using a known generative model, align with the distribution of fake images from the same model, whereas unknown fake images exhibit distinct shifts in their distribution. 

%-------------------------------------------------------------------------------
\subsection{Methodology}
%-------------------------------------------------------------------------------
We propose a generalizable detection method for AI-generated images based on Post-hoc Distribution Alignment (PDA), as illustrated in \autoref{fig: framework}.
Our approach involves four steps: \textbf{feature extractor training, reference feature representation, threshold determination} and \textbf{fake image detection}.

%-------------------------------------------------------------------------------
\subsubsection{Feature Extractor Training}
%-------------------------------------------------------------------------------
The first step is to train the feature extractor to capture high-level artifacts and features. To this end, we train a binary classifier using a mixture of real images and fake images generated by a known generative model \(G( \cdot ) \). The generative models, such as Stable Diffusion V1.4~\cite{rombach2022high} from the HuggingFace Hub, are readily accessible. The binary classifier minimizes the classification loss to effectively distinguish between real and generated fake images, based on their learned feature distributions. The loss function \( L \) can be written as:
\begin{equation}
L = \sum_{i=1}^{N} \text{loss}(f_{\theta}(I_i), y_i)
\label{eq:extractor}
\end{equation}
where \( I_i \) denotes the input image and \( y_i \) is the corresponding label (real or fake), and \( N \) is the number of training images. 

After training, the classifier—excluding the final layer—can be used as the feature extractor  \( f_{\theta} \).

%-------------------------------------------------------------------------------
\subsubsection{Reference Feature Representation}
%-------------------------------------------------------------------------------
After training the feature extractor, we use it to generate feature representations for the known fake images from training data, which are then stored in a nearest neighbor reference set \( \mathbb{Z} \). 
Let \( I \) be the known fake image, and \( f_{\theta} \) the feature extractor with parameters \( \theta \). The feature vector \( \mathbf{x} \) is given by:
\begin{equation}
\mathbf{x} = f_{\theta}(I)
\label{eq:feature1}
\end{equation}
Here, \( \mathbf{x} \in \mathbb{R}^d \) represents the extracted feature vector in a high-dimensional space, where \( d \) denotes the dimensionality of the feature space. These representations are further refined through activation pruning and dimensionality reduction to improve discriminative power and eliminate irrelevant features.

\mypara{Activation Pruning}
we remove activations that contribute minimally to distinguishing real from fake images. This is accomplished through a rectification operation, which truncates activations exceeding a threshold \(c\). Let \( \mathbf{x}_{pruned} \) represent the pruned feature vector:
    \begin{equation}
\mathbf{x}_{pruned} = \mathcal{P}(\mathbf{x; c}) 
\label{eq:feature2-1}
\end{equation}
\begin{equation}
\mathcal{P}(\mathbf{x} )= min(\mathbf{h}_{i}(\mathbf{x}), c )
\label{eq:feature2-2}
\end{equation}
Here, \( \mathcal{P}(\cdot) \) denotes the pruning operation, and \( \mathbf{h}_{i}(\mathbf{x}) \) represents the activation value of the feature vector \( \mathbf{x} \). The threshold \( c \) is determined as the \( p \)-th percentile of activations for each sample. Following~\cite{sun2021react}, \( p = 90 \) is selected, meaning \( c \) is set so that 90\% of activations lie below the threshold. This approach retains the most informative activations while minimizing noise.

\mypara{Dimensionality Reduction}
To reduce computational complexity and preserve local data structures, we use t-SNE~\cite{van2008visualizing} for dimensionality reduction. t-SNE maps high-dimensional feature vectors into a 2D space:
\begin{equation}
\mathbf{z} = \text{t-SNE}(\mathbf{x}_{pruned})
\label{eq:feature3}
\end{equation}
where \( \mathbf{z} \in \mathbb{R}^2 \) represents the reduced feature vector. This step is essential for efficiently computing KNN distances, which underpin our distribution alignment framework. By maintaining local data structures, t-SNE ensures that nearest neighbor relationships in the original feature space are accurately preserved in the reduced space, improving the robustness and interpretability of our detection method.

In this way, we construct a  nearest neighbor reference set \( \mathbb{Z} \) containing the feature representations of known fake images:
\begin{equation}
\mathbb{Z} = \{\mathbf{z}_1, \mathbf{z}_2, \dots, \mathbf{z}_n\}
\label{eq:knn1}
\end{equation}
where \( n \) denotes the number of known fake images, and \(\mathbf{z}_i\) represents the feature vector of the \(i\)-th fake image. 
This reference set is used to evaluate the similarity of test images, determining how closely they align with the distribution of known fake images.

\subsubsection{Threshold Determination}
We input another subset of real images into the generative model to produce a new set of fake images. Using the same feature extraction process, we obtain a new reference feature set, \( \mathbb{Z}^{'} \). Since both feature sets follow the same distribution, we here aim to determine a threshold that can identify that \( \mathbb{Z}^{'} \) and \( \mathbb{Z} \) follow the same distribution. This threshold is later used to detect unknown images in real-world scenarios.  

Specifically, for each \( \mathbf{z}^{'} \in \mathbb{Z}^{'} \), we compute the Euclidean distance \( ||\mathbf{z}^{'} - \mathbf{z}_i||_2 \) for all \( \mathbf{z}_i \in \mathbb{Z} \). We then sort \( \mathbb{Z} \) in ascending order of distance and obtain the \( k \)-NN distance of \( \mathbf{z}^{'} \) by:
\begin{equation}
d_k(\mathbf{z}^{'}) = ||\mathbf{z}^{'} - \mathbf{z}_k||_2 
\end{equation}
where \( \mathbf{z}_k \) is the \( k \)-th nearest neighbor.  
Repeating this process for all elements in \( \mathbb{Z}^{'} \), we obtain a set of \( k \)-NN distances. Finally, we sort these distances in descending order and set the threshold $\tau$ at the 95th percentile of the distances.
This ensures that at least 95\% of the \( k \)-NN distances are below this value, in which case we consider that the feature distribution in \( \mathbb{Z}^{'} \) matches \( \mathbb{Z} \).
Note that 95\% is a commonly used~\cite{sun2022out,djurisic2022extremely} to confidently determine distribution alignment.
Additionally, the thresholds do not rely on any unseen test data and are independent of the generative model, making them generalizable across different models.

%-------------------------------------------------------------------------------
\subsubsection{Fake Image Detection}
%-------------------------------------------------------------------------------
Finally, given a set of unknown test images in the real world, we follow a two-step strategy to distinguish between real and fake images and further classify fake images as known or unknown.  

First, we apply the same process as described earlier to obtain the feature representation \( \mathbf{z}^{*} \) of unknown test images and compute the \( k \)-NN distance \( d_k(\mathbf{z}^{*})\) for each image. We then compare \( d_k(\mathbf{z}^{*}) \) to the threshold \( \tau \). If the distance is smaller than \( \tau \), the image is classified as a known fake since it follows the distribution of the reference feature set \( \mathbb{Z} \), which represents previously known fake images.  

Second, for the remaining images—a mix of real and unknown fake images—we feed them to the known generative model \(G( \cdot ) \) to obtain the re-generate versions, denoted as pseudo-fake samples \(G(\mathbf{I}_{pseudo})\). We then repeat the feature extraction process and compute the \( k \)-NN distance as follows:
\begin{equation}
\begin{aligned}
    &\mathbf{x}_{pseudo} = f_{\theta}(G(\mathbf{I}_{pseudo})) \\
    &\mathbf{x}_{pseudo\_pruned} = \mathcal{P}(\mathbf{x}_{pseudo}; c) \\
    & \mathbf{z}^{*}_{pseudo} = \text{t-SNE}(\mathbf{x}_{pseudo\_pruned}) \\
    &d_k(\mathbf{z}^{*}_{pseudo})  = ||\mathbf{z}^{*}_{pseudo} - \mathbf{z}_k||_2 
\end{aligned}
\label{eq:pseudo_all}
\end{equation}
The \( d_k(\mathbf{z}^{*}_{pseudo}) \) is compared to \( \tau \). If the distance is smaller than \( \tau \), the image is classified as real, as only real images produce pseudo-fake samples with the same artifacts and features as known fake images. Conversely, if the distance is larger than \( \tau \), the image is classified as an unknown fake.

The two-step detection procedure is formalized as follows:
\begin{equation}
\hat{y} =
\begin{cases}
\text{Fake}, & d_k(\mathbf{z}^{*} \leq \tau_1 \\
\text{Real}, & d_k(\mathbf{z}^{*} > \tau_1 \text{ and }  d_k(\mathbf{z}^{*}_{pseudo})  \leq \tau_1 \\
\text{Fake}, &   d_k(\mathbf{z}^{*}_{pseudo})   > \tau_1
\end{cases}
\label{eq:knn3}
\end{equation}

The detailed steps of this method are outlined in Algorithm \ref{alg1}. By leveraging post-hoc distribution alignment, this approach enables effective detection of AI-generated images without requiring retraining for each new generative model. 

\begin{algorithm}[ht]
\caption{PDA for AI-Generated Image Detection}
\label{alg1}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithmic}[1]
\REQUIRE Test image \( I_{test} \), fake images from training set \( I \), pre-trained feature extractor \( f_{\theta} \), known generative model \( G(\cdot) \)   
\ENSURE Predicted label \( \hat{y} \) (Real or Fake)

\STATE \textbf{Reference Feature Representation:} 
\STATE Extract feature representation of fake image \( \mathbf{x} = f_{\theta}(I) \)
\STATE Activation pruning \( \mathbf{x}_{pruned} = \mathcal{P}(\mathbf{x}; c) \)
\STATE Project into 2D space \( \mathbf{z} = \text{t-SNE}(\mathbf{x}_{pruned}) \)
\STATE Obtain nearest neighbor reference set \( \mathbb{Z} \)

\STATE \textbf{Threshold Determination:} 
\STATE Compute a set of \( k \)-NN distances \( d_k(\mathbf{z}') = ||\mathbf{z}' - \mathbf{z}_k||_2 \)
\STATE Determine threshold \( \tau_1 \) at the 95th percentile of the distances

\STATE \textbf{Fake Image Detection:}
\STATE Obtain the feature representation \( \mathbf{z}^* \) of \( I_{test} \)
\STATE Compute the \( k \)-NN distance \( d_k(\mathbf{z}^*) = ||\mathbf{z}^* - \mathbf{z}_k||_2 \)
\IF{\( d_k(\mathbf{z}^*) \leq \tau_1 \)}
    \STATE Classify as Fake
\ELSE
    \STATE \textbf{Secondary Forgery Process:}
    \STATE Generate pseudo-fake image \( I_{pseudo} = G(I_{test}) \)
    \STATE Obtain feature representation \( \mathbf{z}_{pseudo} \)
    \STATE Compute KNN distance \( d_k(\mathbf{z}_{pseudo}) = ||\mathbf{z}_{pseudo} - \mathbf{z}_k||_2 \)
    \IF{\( d_k(\mathbf{z}_{pseudo}) \leq \tau_1 \)}
        \STATE Classify as Real
    \ELSE
        \STATE Classify as Fake
    \ENDIF
\ENDIF
\end{algorithmic}
\end{algorithm}

%-------------------------------------------------------------------------------
\section{Experiments}
%-------------------------------------------------------------------------------

\begin{table*}[htbp]
  \centering
  \caption{Effectiveness Evaluation on Different Generative Models}
    \begin{tabular}{c|ccccccc}
    \toprule
    \textbf{Method} & \textbf{SD} & \textbf{GLIDE} & \textbf{VQDM} & \textbf{ADM} & \textbf{BigGAN} & \textbf{Wukong} & \textbf{AP} \\
    \midrule
    Benchmark & \textbf{98.53}\% & 64.27\% & 59.00\% & 54.70\% & 63.42\% & \textbf{94.65\%} & 72.43\% \\
    CNNDetection & 50.15\% & 52.62\% & 52.17\% & 51.17\% & 73.70\% & 50.15\% & 54.99\% \\
    DIRE  & 51.22\% & 60.18\% & 52.32\% & 54.97\% & 59.72\% & 52.75\% & 55.19\% \\
    ZeroFake & 85.63\% & 85.00\% & 69.38\% & 83.00\% & 82.55\% & 78.37\% & 80.66\% \\
    \textbf{PDA (Our)} & 96.00\% & \textbf{98.09\%} & \textbf{97.87\%} & \textbf{98.12\%} & \textbf{98.19\%} & 92.10\% & \textbf{96.73\%} \\
    \bottomrule
    \end{tabular}%
  \label{tab:effectiveness}
\end{table*}%
\subsection{Experimental Setup}
\subsubsection{Generation Models and Datasets}
We evaluate our method on images generated by various generative models, including Generative Adversarial Networks (GANs), diffusion models, and text-to-image models. These models provide a diverse set of fake images for testing the robustness and generalization of our method, including \textbf{Stable Diffusion V1.4 (SD)~\cite{rombach2022high}, GLIDE~\cite{nichol2021glide}, VQDM ~\cite{gu2022vector}, ADM~\cite{dhariwal2021diffusion}, BigGAN~\cite{brock2018large}} and \textbf{Wukong~\cite{wukong2022}}. The details of generation models are in \autoref{appendix_models}.

Our experiments are conducted on the GenImage dataset~\cite{zhu2024genimage}, a large-scale benchmark specifically designed for evaluating the detection of AI-generated images. GenImage comprises over 1.33 million real images and 1.35 million fake images, totaling 2.68 million images, making it one of the most comprehensive datasets for this task. The real images are sourced from ImageNet~\cite{deng2009imagenet}, a widely recognized and high-quality dataset, ensuring the diversity and representativeness of the real image distribution. In experiments, we evaluate the generalization ability
of detection methods using 3,000 fake images generated by each generative model and 3,000 real images.

%-------------------------------------------------------------------------------
\subsubsection{Evaluation Metrics}
%-------------------------------------------------------------------------------
Following existing generated-image detection methods~\cite{sha2023fake,wang2023dire,ojha2023towards,tan2024rethinking}, we report \textbf{accuracy (ACC)} and \textbf{average precision (AP)} to evaluate the detectors. Since our method applies a two-step threshold-based classification strategy, ACC is calculated as:
\begin{equation}
\text{ACC} \left ( \% \right ) = 100\times  \frac{N_{\text{correct1}} + N_{\text{correct2}}}{N_{\text{total}}}
\label{eq:acc}
\end{equation}
where \( N_{\text{correct1}} \) is the number of correct classifications in the first step. \( N_{\text{correct2}} \) is the number of correct classifications in the second step. \( N_{\text{total}} \) is the total number of test samples.

In addition to the quantitative metrics, we provide \textbf{qualitative visualizations} of the feature space using t-SNE and KNN distance distributions. These visualizations demonstrate how well our method separates real and fake images, offering insights into the effectiveness of the distribution alignment process.

%-------------------------------------------------------------------------------
\subsubsection{Baselines}
%-------------------------------------------------------------------------------
To evaluate the effectiveness of our proposed method, we compare it against the following state-of-the-art baselines: 1) \textbf{Benchmark} (pre-trained CNN feature extractor~\cite{he2016deep}) trained on fake images generated by SD and real images. This baseline serves as a reference to illustrate the generalization capability of a detector trained on a single generative model.
2) \textbf{CNNDetection~\cite{wang2020cnn}} uses a CNN classifier trained on ProGAN-generated images with simple data augmentation techniques. The classifier has been shown to generalize well to other GAN-based generated images.
3) \textbf{DIRE~\cite{wang2023dire}} distinguishes real and fake images by using the reconstruction error from a pre-trained diffusion model. It leverages the observation that fake images generated by diffusion models have smaller reconstruction errors compared to real images.
4) \textbf{ZeroFake~\cite{sha2024zerofake}} uses a perturbation-based DDIM inversion technique with prompt guidance to distinguish real from fake images, achieving significantly better performance than DIRE.

%-------------------------------------------------------------------------------
\subsubsection{Implementation Details}
\label{appendix_implement}
%-------------------------------------------------------------------------------
The feature extractor is a ResNet-50~\cite{he2016deep} trained on real and fake images generated by SD. 
The nearest neighbor reference set is built using the feature representations of 3,000 SD-generated images from training set. 
In experiments, we evaluate the generalization ability of our PAD using 3,000 fake images generated by each generative model (e.g., SD, GLIDE, VQDM, BigGAN, Wukong, and ADM) and 3,000 real images.
We set the number of nearest neighbors \(k = 20\) for the KNN-based detection step. 
For baseline methods, we use their open-source implementations with recommended pre-trained models and default hyperparameters to ensure a fair comparison.

%-------------------------------------------------------------------------------
\subsection{Effectiveness Evaluation}
%-------------------------------------------------------------------------------
In this section, we evaluate the effectiveness of our Post-hoc Distribution Alignment (PAD) method for detecting AI-generated images. 

\textbf{Comparison to Existing Methods.} As shown in \autoref{tab:effectiveness}, which presents the ACC and AP results of different detection methods, our PDA method demonstrates superior performance across various generative models. 
The Benchmark method, which trains a classifier on SD-generated images, exhibits poor generalization to other models, particularly for ADM, where the detection accuracy drops to 54.7\%. In contrast, our method achieves 98.12\% accuracy by leveraging distribution alignment to efficiently identify unknown fake images.
DIRE requires training a separate model for each unknown fake image type to learn the reconstruction error, which limits its generalization ability. As a result, when using a single pre-trained model, its AP is only 55.19\%.
ZeroFake, designed for text-to-image models, improves detection efficiency by utilizing prompt guidance during the DDIM inversion process. However, its AP is still 16.07\% lower than our method, particularly on VQDM-generated image,  where our ACC is 97.87\% compared to ZeroFake's 69.38\%. Additionally, while ZeroFake does not require retraining, its adversarial prompt and reconstruction process are computationally expensive. Moreover, it requires different thresholds for varying scenarios (e.g., fake images vs. fake artwork), further limiting its practical applicability.

\begin{figure}[!t]
    \centering
        \includegraphics[width=0.48\linewidth]{./figure/t-sne/raw/sne_origin_wukong_20.pdf}
    \includegraphics[width=0.48\linewidth]{./figure/t-sne/raw/sne_origin_adm_20.pdf} \\ 
     \includegraphics[width=0.48\linewidth]{./figure/t-sne/re-generated/sne_generate_wukong_20.pdf}
     \includegraphics[width=0.48\linewidth]{./figure/t-sne/re-generated/sne_generate_adm_20.pdf}  \\
\caption{T-SNE visualization. Rows correspond to feature spaces: original and re-generated images (orange denotes the known fake distribution).}
\label{fig:T-SNE visualization}
\end{figure}

\textbf{Visualization Results.} 
In addition to quantitative accuracy evaluations, we also present qualitative results through t-SNE visualizations in \autoref{fig:T-SNE visualization} and KNN distance distributions in \autoref{fig:KNN visualization} (more results are in  \autoref{appendix_t-SNE}). 
We have the following observations:
1) \textbf{Alignment of known fake samples}. The first row in \autoref{fig:T-SNE visualization} clearly shows that the Wukong-genenrated images are aligned with the distribution of known fake samples from the training set in the raw feature space. This indicates that Wukong shares similar artifacts with SD, and the feature extractor, trained on SD, successfully learns these artifact features. Consequently, fake samples like  those from Wukong and SD are easily detected in the first step of our threshold-based classification. 
2) \textbf{Separation between real and known fake samples}. The first row in \autoref{fig:KNN visualization} shows a clear boundary between the distributions of real and known fake images in the raw feature space. However, other unknown fake images such as ADM, while not fitting the known fake distribution, exhibit significant overlap with real images, making them indistinguishable. This reveals why detectors trained on a single generative model struggle to generalize to unknown fake samples, as they often misclassify unknown fake images with different artifact patterns as real with high confidence.
3) \textbf{Discrepancy in real and unknown fake distributions.} The second row in \autoref{fig:T-SNE visualization} and \autoref{fig:KNN visualization} demonstrates that the pseudo-fake versions of real images align with the known fake distribution (with smaller KNN distances) due to shared artifacts. In contrast, ADM-generated images exhibit persistent and more pronounced distribution shifts. This distinction allows us to effectively distinguish between real and unknown fake images.

\begin{figure}[!t]
    \centering
        \includegraphics[width=0.48\linewidth]{./figure/KNN/raw/wukong_20.pdf} 
    \includegraphics[width=0.48\linewidth]{./figure/KNN/raw/adm_20.pdf} 
  \\

    \includegraphics[width=0.48\linewidth]{./figure/KNN/re-generated/wukong_20.pdf} 
    \includegraphics[width=0.48\linewidth]{./figure/KNN/re-generated/adm_20.pdf} 
  \\
    \caption{KNN distance distributions. Rows correspond to feature spaces: original and re-generated images (orange denotes the known fake distribution).}
\label{fig:KNN visualization}
\end{figure}

\begin{figure}[!t]
\centering
\begin{minipage}{0.32\linewidth}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/raw/average_spectrum_sd_origin.png}}
    \vspace{6pt}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/generate/average_spectrum_sd_origin.png}}
    \centerline{SD}
\end{minipage}
\begin{minipage}{0.32\linewidth}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/raw/average_spectrum_nature.png}}
    \vspace{6pt}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/generate/average_spectrum_nature.png}}
    \centerline{Real}
\end{minipage}
\begin{minipage}{0.32\linewidth}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/raw/average_spectrum_adm.png}}
    \vspace{6pt}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/generate/average_spectrum_adm.png}}
    \centerline{ADM}
\end{minipage}
\caption{The visualization of frequency analysis on the test samples (first row) and the pseudo-fake samples (second row).}
\label{fig:frequency}
\end{figure}
%-------------------------------------------------------------------------------
\subsection{Ablation Studies}
%-------------------------------------------------------------------------------
\begin{table}[htbp]
\centering
\caption{Performance comparison with and without activations pruning}
\setlength{\tabcolsep}{3pt}
\scalebox{0.75}{
   \begin{tabular}{cccccccc}
    \toprule
     & \textbf{SD} & \textbf{GLIDE} & \textbf{VQDM} & \textbf{ADM} & \textbf{BigGAN} & \textbf{Wukong} & \textbf{AP} \\
    \midrule
    \textbf{Without} & 95.14\% & 97.04\% & 96.41\% & 97.05\% & 97.92\% & 90.15\% & 95.62\% \\
    \textbf{With} & \textbf{96.00\%} & \textbf{98.09\%} & \textbf{97.87\%} & \textbf{98.12\%} & \textbf{98.19\%} & \textbf{92.10\%} & \textbf{96.73\%} \\
    \bottomrule
    \end{tabular}%
}
\label{tab:activate}
\end{table}

\begin{table*}[htbp]
\centering
\caption{The impact of different Network Backbone.}
    \begin{tabular}{cccccccc}
    \toprule
          & \textbf{SD} & \textbf{GLIDE} & \textbf{VQDM} & \textbf{ADM} & \textbf{BigGAN} & \textbf{Wukong} & \textbf{AP} \\
    \midrule
    \textbf{Resnet-18} & 95.67\% & 97.28\% & 96.89\% & 97.36\% & 97.48\% & 92.35\% & 96.17\% \\
    \textbf{Restnet-50} & 96.00\% & 98.09\% & 97.87\% & 98.12\% & 98.19\% & 92.10\% & 96.73\% \\
    \textbf{VGG-19}    &  96.93\%   & 98.43\% & 98.46\% & 98.46\% & 98.46\% & 94.53\% & 97.55\% \\
    \bottomrule
    \end{tabular}
    \label{tab:backbone}
\end{table*}

\begin{figure*}[t]
\centering
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./figure/curve_K2.pdf}
  \caption{K value}
  \label{k_values}
\end{subfigure}%
\hfill
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./figure/t-sne/re-generated/test1.jpg}
  \caption{Outlier analysis}
  \label{outlies}
\end{subfigure}%
\hfill
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./figure/bar_T.pdf}
  \caption{Training set size}
  \label{training set}
\end{subfigure}
\caption{Impact of different key factors on PDA performance}
\label{fig:factors}
\end{figure*}

%-------------------------------------------------------------------------------
\subsubsection{Impact of Activations Pruning}
%-------------------------------------------------------------------------------
In this experiment, we explore the impact of activation pruning on the performance of our method. Recent works~\cite{sun2021react,djurisic2022extremely} have shown that out-of-distribution (OOD) data often exhibit high activations in certain feature dimensions, which can negatively impact detection performance. We apply activation pruning to suppress these high activation values and evaluate its effect on our method. As shown in \autoref{tab:activate}, we observe that activation rectification improves detection performance by mitigating the impact of spurious activations. The rectified activations help refine decision boundaries and enhance model robustness against distribution shifts in the test images.

%-------------------------------------------------------------------------------
\subsubsection{Impact of KNN}
%-------------------------------------------------------------------------------
We systematically analyze the effect of the number of neighbors \( k \) on the performance of our KNN-based detection approach. In this experiment, we vary \( k \) across the values \( \{1, 10, 20, 30, 40, 50\} \) and evaluate the performance of the method in terms of detection accuracy. As shown in Figure \autoref{k_values}, our method achieves superior performance and remains stable across different values of \( k \), except when \( k = 1 \). The performance for other values of \( k \) shows consistent results and has little impact on the detection performance across different generative models. In Figure \autoref{outlies}, we demonstrate why the performance is poor when \( k = 1 \). Specifically, the nearest neighbor reference set may contain outliers, such as the red-circled data points that fall outside the distribution (anomalous points). These outliers can affect the KNN distance distribution, causing fake samples (e.g., those generated by VQDM) to have small distances to the nearest neighbors after secondary generation, making it difficult to distinguish them from real samples. By using a larger \( k = 10 \) (or other values), we effectively mitigate the impact of these outliers, leading to more accurate detection.

%-------------------------------------------------------------------------------
\subsubsection{Impact of Training Dataset Size}
%-------------------------------------------------------------------------------
In this section, we investigate the impact of the training dataset size on the performance of our PAD-based fake image detection. We vary the number of fake images from 20,000 to 120,000, with equal numbers of real images in the dataset. As shown in Figure \autoref{training set}, increasing the size of the training dataset initially improves detection performance, with AP rising from 81.6\% at 20,000 samples to 94.15\% at 60,000 samples. However, as the dataset size continues to increase, the detection performance stabilizes at a high level. This highlights the efficiency and scalability of our PAD method in real-world scenarios, where labeled data may be limited.

%-------------------------------------------------------------------------------
\subsubsection{ Impact of Network Backbone}
\label{backbone}
%-------------------------------------------------------------------------------
We also evaluate the effect of the network backbone of feature extractors on the performance of our detection method. Specifically, we test our method with different network architectures, including ResNet-18~\cite{he2016deep}, ResNet-50~\cite{he2016deep}, VGG-19~\cite{simonyan2014very}, which are suitable for various applications. As in previous experiments, we apply our method to the penultimate layer output and perform activation rectification. The results in \autoref{tab:backbone} show that, even with different network backbones, our method consistently achieves superior detection performance with an AP of above 96\%.
This demonstrates that our approach is effective across various network architectures and can be seamlessly integrated with existing binary classifiers to further enhance their performance, using only fake images generated from a single known generative model.

\begin{table*}[htbp]
  \centering
  \caption{Robustness Evaluation Under Different Image Transformations}
    \begin{tabular}{c|c|ccccccc}
    \toprule
    \textbf{Transformation} & \textbf{Factor} & \textbf{SD} & \textbf{GLIDE} & \textbf{VQDM} & \textbf{ADM} & \textbf{BigGAN} & \textbf{Wukong} & \textbf{AP} \\
    \midrule
    \multirow{3}[2]{*}{Gaussian Blurring } & Kernel Size = 3 & 95.73\% & 97.38\% & 97.10\% & 97.57\% & 97.69\% & 92.13\% & 96.27\% \\
          & Kernel Size = 5 & 95.90\% & 97.29\% & 97.24\% & 97.61\% & 97.77\% & 93.88\% & 96.62\% \\
          & Kernel Size = 7 & 95.76\% & 97.17\% & 96.87\% & 97.29\% & 97.56\% & 92.76\% & 96.24\% \\
    \midrule
    \multirow{3}[2]{*}{Image Compression} & QF = 90 & 96.17\% & 98.13\% & 97.60\% & 98.20\% & 98.27\% & 92.25\% & 96.77\% \\
          & QF = 70 & 96.33\% & 98.25\% & 97.91\% & 98.33\% & 98.40\% & 93.80\% & 97.17\% \\
          & QF = 50 & 96.06\% & 98.06\% & 97.71\% & 98.16\% & 98.25\% & 92.20\% & 96.74\% \\
    \midrule
    \textbf{PDA (Our)} &       & \textbf{96.00\%} & \textbf{98.09\%} & \textbf{97.87\%} & \textbf{98.12\%} & \textbf{98.19\%} & \textbf{92.10\%} & \textbf{96.73\%} \\
    \bottomrule
    \end{tabular}%
  \label{tab:robustness}
\end{table*}%

%-------------------------------------------------------------------------------
\subsubsection{Frequency Analysis}
%-------------------------------------------------------------------------------
To gain insights into the distinctive artifacts produced by different generative models, we conduct a frequency analysis of real and fake images. Following the approach used in~\cite{sha2023fake,zhang2019detecting}, we calculate the average of Fourier transform outputs of 3,000 real images and 3,000 fake images from each generative model, respectively. 
We utilize the Fourier transform for its ability to reveal latent features within the images.

As shown in \autoref{fig:frequency}, the first row displays the results of unprocessed original images. Fake images generated by different models show distinct artifacts in the frequency spectrum, which are different from the real images. The second row shows the corresponding pseudo-fake images generated by the known generative model SD. The pseudo-fake images of real images exhibit the same artifact patterns as SD-generated images, while images generated by ADM show a more distinct artifact patterns (more visualization results are in \autoref{appendix_frequency}). This observation supports our hypothesis that real images, when re-generated using a known generative model, align with the distribution of known fake images due to sharing the same artifact features, whereas unseen fake images from other generative models exhibit persistent distribution shifts.

%-------------------------------------------------------------------------------
\subsection{Robustness Evaluation}
%-------------------------------------------------------------------------------
In this section, we evaluate the robustness of our PAD method under different image transformations with varying parameters, including Gaussian blurring~\cite{kurakin2018adversarial}, image compression. 
\begin{itemize}
\item \textbf{Gaussian Blurring} is commonly used to reduce image detail and noise by averaging pixels in a local neighborhood, resulting in a smoother image. We apply Gaussian blur with different kernel sizes to assess the effect on detection performance. 
\item \textbf{Image compression}  reduces image size by removing less important visual details. We test the effect of compression using different quality factors (QF), where lower QF values indicate lower image quality.
\end{itemize}

As shown in \autoref{tab:robustness}, our PAD method demonstrates strong robustness across various image transformations. The performance, in terms of ACC and AP, remains high even under challenging conditions. Notably, our method achieves AP scores of over 96\% despite the introduction of noise and image distortions. These results highlight the effectiveness and resilience of the PAD method in real-world applications, where images are often subject to various image transformations.

%-------------------------------------------------------------------------------
\section{Conclusion \& Future Work}
%-------------------------------------------------------------------------------
We introduced Post-hoc Distribution Alignment (PDA), a novel method for the generalizable detection of AI-generated images. Unlike prior methods, PDA is model-agnostic and does not require retraining on new generative models. It employs a two-step detection strategy that uses distribution alignment to separate real and fake images, including those generated by unknown models. 
Extensive experiments show that PDA outperforms existing methods such as DIRE and ZeroFake, achieving 96.73\% average accuracy across various generative models. PDA offers a scalable and efficient solution for fake image detection, with potential for real-world applications where new generative models are constantly emerging.

In future work, we will explore the impact of adversarial examples and other types of distribution shifts, as well as explore the integration of PDA into real-time detection systems. Additionally, we aim to assess PDA's performance in detecting AI-generated images from emerging generative models, particularly in real-world applications such as social media platforms and large-scale generative model APIs, to validate its practical utility and scalability.

%-------------------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{normal_generated}
%-------------------------------------------------------------------------------
\clearpage
\newpage
\appendix
%-------------------------------------------------------------------------------
\section{Description of Generation Models}
\label{appendix_models}
%-------------------------------------------------------------------------------
In this section, we provide detailed descriptions of the generative models evaluated in our experiments. These models represent a variety of approaches to image generation, including diffusion-based models, text-to-image models, and GAN-based models.

\begin{itemize}
    \item \textbf{Stable Diffusion V1.4 (SD)~\cite{rombach2022high}.} A latent diffusion model that generates high-quality images by iteratively denoising a random latent vector. It is trained on large-scale datasets and is known for its ability to produce highly realistic images with fine details.
    
    \item \textbf{GLIDE~\cite{nichol2021glide}.} A text-to-image diffusion model that leverages guided diffusion to generate images conditioned on textual descriptions. GLIDE is notable for its ability to synthesize diverse and semantically meaningful images based on complex prompts.

    \item \textbf{VQDM~\cite{gu2022vector}.} A variant of diffusion models that combines vector quantization with diffusion processes.  VQDM performs image generation by discretizing the data distribution, which allows it to efficiently generate high-resolution images with more diversity. 

    \item \textbf{ADM~\cite{dhariwal2021diffusion}.}  A class of diffusion models that systematically removes components (e.g., attention mechanisms) to study their impact on generation quality. ADM is widely used for benchmarking due to its modular design and strong performance.
    
    \item \textbf{BigGAN~\cite{brock2018large}.}  A large-scale GAN model that generates high-resolution images by leveraging a combination of class-conditional batch normalization and orthogonal regularization. BigGAN is known for its ability to produce diverse and realistic images across multiple categories. 

    \item \textbf{Wukong~\cite{wukong2022}.}  A state-of-the-art text-to-image generation model trained on a massive dataset of Chinese text-image pairs. Wukong is particularly challenging for detection tasks due to its high-quality outputs and strong generalization capabilities. 
\end{itemize}

\section{More Visualization Results.}
\label{appendix_t-SNE}
In addition to the results for ADM and Wukong presented in the main text, we provide t-SNE visualizations and KNN distance distributions for SD, VQDM, GLIDE, and BigGAN in \autoref{fig:T-SNE_more} and \autoref{fig:KNN_more}. These results further validate the effectiveness of our proposed method across diverse generative models. 

Key observations include: 1) similar to Wukong, SD-generated images align well with the distribution of known fake samples in the raw feature space, as shown in the first row of \autoref{fig:T-SNE_more}. This alignment confirms that the feature extractor, trained on SD, effectively captures shared artifacts, enabling reliable detection of SD-generated images in the first step of our threshold-based classification; 2) the first row of \autoref{fig:T-SNE_more} demonstrates unknown fake samples from VQDM, GLIDE, and BigGAN exhibit significant overlap with real images, highlighting the challenge of generalizing detection to unseen generative models; 3) The second row of \autoref{fig:T-SNE_more} and \autoref{fig:KNN_more} reveals that pseudo-fake versions of real images align with the known fake distribution (with smaller KNN distances), while unknown fake samples from VQDM, GLIDE, and BigGAN show persistent and more pronounced distribution shifts. This distinction enables our method to effectively separate real images from unknown fake samples.

\begin{figure*}[htbp]
    \centering
        \includegraphics[width=0.23\linewidth]{./figure/t-sne/raw/sne_origin_sd_origin_20.pdf}
      \includegraphics[width=0.23\linewidth]{./figure/t-sne/raw/sne_origin_vqdm_20.pdf}
    \includegraphics[width=0.23\linewidth]{./figure/t-sne/raw/sne_origin_glide_20.pdf}
       \includegraphics[width=0.23\linewidth]{./figure/t-sne/raw/sne_origin_biggan_20.pdf}
  \\
     \includegraphics[width=0.23\linewidth]{./figure/t-sne/re-generated/sne_generate_sd_origin_20.pdf}
    \includegraphics[width=0.23\linewidth]{./figure/t-sne/re-generated/sne_generate_vqdm_20.pdf}
    \includegraphics[width=0.23\linewidth]{./figure/t-sne/re-generated/sne_generate_glide_20.pdf}
     \includegraphics[width=0.23\linewidth]{./figure/t-sne/re-generated/sne_generate_biggan_20.pdf}  \\
\caption{T-SNE visualization. Rows correspond to feature spaces: original and re-generated images (orange denotes the known fake distribution).}
\label{fig:T-SNE_more}
\end{figure*}

\begin{figure*}[htbp]
    \centering
        \includegraphics[width=0.23\linewidth]{./figure/KNN/raw/sd_origin_20.pdf} 
    \includegraphics[width=0.23\linewidth]{./figure/KNN/raw/vqdm_20.pdf} 
            \includegraphics[width=0.23\linewidth]{./figure/KNN/raw/glide_20.pdf} 
    \includegraphics[width=0.23\linewidth]{./figure/KNN/raw/biggan_20.pdf} 
  \\

    \includegraphics[width=0.23\linewidth]{./figure/KNN/re-generated/sd_origin_20.pdf} 
    \includegraphics[width=0.23\linewidth]{./figure/KNN/re-generated/vqdm_20.pdf} 
            \includegraphics[width=0.23\linewidth]{./figure/KNN/re-generated/glide_20.pdf} 
    \includegraphics[width=0.23\linewidth]{./figure/KNN/re-generated/biggan_20.pdf} 
  \\
    \caption{KNN distance distributions. Rows correspond to feature spaces: original and re-generated images (orange denotes the known fake distribution).}
\label{fig:KNN_more}
\end{figure*}
%-------------------------------------------------------------------------------
\section{More Frequency Analysis.}
\label{appendix_frequency}
%-------------------------------------------------------------------------------
We provide visualization frequency analysis for GLIDE, VQDM, BigGAN, and Wukong in \autoref{figapp}. These results further validate our hypothesis and demonstrate the distinct artifact patterns introduced by different generative models. The first row of \autoref{figapp} shows the results of original test images. Fake images generated by GLIDE, VQDM, BigGAN exhibit distinct artifact patterns in the frequency domain, which differ from the natural spectral distribution of real images. Wukong shares similar artifact patterns with SD, allowing it to be easily identified in the first stage with our PAD-based detection method.

The second row of \autoref{figapp} illustrates the spectral distributions of pseudo-fake images generated by the known generative model SD. These pseudo-fake images from real align closely with the artifact patterns of SD-generated images, confirming that real images, when re-generated by a known model, adopt the same artifact features. In contrast, images generated by GLIDE, VQDM, and BigGAN exhibit distinct and persistent distribution shifts, further highlighting the unique artifact patterns of each generative model.

\begin{figure*}[t]
\centering
\begin{minipage}{0.15\linewidth}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/raw/average_spectrum_sd_origin.png}}
    \vspace{6pt}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/generate/average_spectrum_sd_origin.png}}
    \centerline{SD}
\end{minipage}
\begin{minipage}{0.15\linewidth}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/raw/average_spectrum_nature.png}}
    \vspace{6pt}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/generate/average_spectrum_nature.png}}
    \centerline{Real}
\end{minipage}
\begin{minipage}{0.15\linewidth}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/raw/average_spectrum_vqdm.png}}
    \vspace{6pt}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/generate/average_spectrum_vqdm.png}}
    \centerline{VQDM}
\end{minipage}
\begin{minipage}{0.15\linewidth}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/raw/average_spectrum_glide.png}}
    \vspace{6pt}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/generate/average_spectrum_glide.png}}
    \centerline{GLIDE}
\end{minipage}
\begin{minipage}{0.15\linewidth}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/raw/average_spectrum_biggan.png}}
    \vspace{6pt}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/generate/average_spectrum_biggan.png}}
    \centerline{BigGAN}
\end{minipage}
\begin{minipage}{0.15\linewidth}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/raw/average_spectrum_wukong.png}}
    \vspace{6pt}
    \centerline{\includegraphics[width=\textwidth]{./figure/frequency/generate/average_spectrum_wukong.png}}
    \centerline{Wukong}
\end{minipage}
\caption{The visualization of frequency analysis on the test samples (first row) and the pseudo-fake samples (second row).}
\label{figapp}
\end{figure*}
%-------------------------------------------------------------------------------
\end{document}
%-------------------------------------------------------------------------------