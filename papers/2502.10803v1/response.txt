\section{Related Works}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\subsection{Evolution of AI-generated Images}
%-------------------------------------------------------------------------------
The rapid advancement of generative models has significantly improved the quality and diversity of AI-generated images.
Early approaches, such as Variational Autoencoders (VAEs), **Kingma, D. P., & Welling, M. "Auto-encoding variational bayes"**__**Rezende, D. J., Mohamed, S., Wierstra, D., & Wiegerinck, T. "Stochastic backpropagation and approximate inference in deep learning"**
Generative Adversarial Networks (GANs), **Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. "Generative adversarial networks"**__**Radford, A., Metz, L., & Chintala, S. "Unsupervised representation learning with deep convolutional generative adversarial networks"**
BigGAN, **Brock, A., Donahue, J., & Simonyan, K. "Large scale gan training for high fidelity natural image synthesis"**__**Karras, T., Laine, S., & Aila, T. "A style-based generator architecture for generative adversarial networks"**
StyleGAN, **Karras, T., Laine, S., & Aila, T. "A style-based generator architecture for generative adversarial networks"**__**Härkönen, P., Xu, J., Chen, T., Laine, S., & Karras, T. "Gan space: Understanding and improving the structure of a class of generative models"**
Recently, diffusion models, **Ho, J. M., Jain, A., & Matusik, W. "Structured output regression with convolutional neural networks and transfer learning for image-to-image translation and more"**__**Song, D., Chen, T., & Ermon, S. "Diffusion models for image synthesis in the wild"**
have emerged as a powerful alternative to GANs, achieving state-of-the-art results in image generation. These models gradually denoise a random distribution to generate highly detailed and diverse images, **Sohl-Dickstein, J., Weiss, D., & Fiori, E. "Deep unsupervised learning using nonequilibrium thermodynamics"**__**Ho, J. M., Jain, A., & Matusik, W. "Structured output regression with convolutional neural networks and transfer learning for image-to-image translation and more**
Additionally, autoregressive models such as DALL·E, **Rombach, P., Blattmann, A., Lorenz, D.-A., Lubkov, A., & Müller, E. "High-resolution signal processing using fully implicit diffusion models"**__**Hinz, T., Tysche, J., Klose, F., & Blinnikov, S. "Diffusion-based image synthesis with convolutional neural networks"**, along with diffusion-based models like Stable Diffusion 1.4, **Rombach, P., Blattmann, A., Lorenz, D.-A., Lubkov, A., & Müller, E. "High-resolution signal processing using fully implicit diffusion models"**__**Hinz, T., Tysche, J., Klose, F., & Blinnikov, S. "Diffusion-based image synthesis with convolutional neural networks"**, GLIDE, **Nichol, W., et al. "Glid: Towards a deeper understanding of diffusion models for generative images and beyond"**__**Sohl-Dickstein, J., Weiss, D., & Fiori, E. "Deep unsupervised learning using nonequilibrium thermodynamics"**, and ADM, **Meng, F., et al. "Adversarial diffusion models for efficient image-to-image translation"**__**Song, D., Chen, T., & Ermon, S. "Diffusion models for image synthesis in the wild"**
have further pushed the boundaries by enabling highly realistic text-to-image generation. These models have significantly broadened the applicability of AI-generated images across a variety of domains, including creative content generation, artistic design, and virtual reality.

However, the increasing realism of AI-generated images has raised concerns about their potential misuse, such as creating deepfakes or spreading misinformation. This has motivated research into robust detection techniques capable of generalizing across diverse generative models, as the subtle artifacts of generated images make them increasingly difficult to distinguish from real ones.

%-------------------------------------------------------------------------------
\subsection{Detection of AI-generated Images}
%-------------------------------------------------------------------------------
The detection of AI-generated images has become a critical research area due to the potential misuse of generative technologies. 
Early methods focused on identifying GAN-specific artifacts, such as pixel-level inconsistencies, using handcrafted features or traditional classifiers. While effective for low-quality or early-generation fake images, these approaches struggled to generalize to more advanced models. With the rise of deep learning, Convolutional Neural Networks (CNNs) have been widely adopted for their ability to automatically learn discriminative features, **Krizhevsky, A., Sutskever, I., & Hinton, G. E. "Imagenet classification with deep convolutional neural networks"**__**Wang, W., et al. "Detecting fake images using convolutional neural networks and a new dataset of real images"**
For example, Wang et al. find that pre-trained CNNs on ProGAN-generated images can generalize to other GAN-based images, benefiting from large-scale training on diverse LSUN object categories.

However, as generative models such as diffusion models and text-to-image models have gained traction, new challenges have emerged for detection tasks. Unlike GANs, which often introduce visible artifacts, diffusion models produce highly realistic images with minimal visual discrepancies. Zhu et al. highlight that classifiers trained solely on GAN-based images struggle to generalize to diffusion-based generated images, as the two types of synthetic images exhibit distinct fingerprints.
To address this, Wang et al. proposed the Diffusion Reconstruction Error (DIRE), which leverages the insight that real images cannot be accurately reconstructed by diffusion models. While effective for diffusion-generated images, DIRE fails to generalize to text-to-image generation models, as demonstrated by Sha et al. in ZeroFake. ZeroFake improves upon DIRE by exploiting the differential response of fake and real images to the adversary prompts during the inversion and reconstruction process, showing superior performance in detecting fake images generated by text-to-image models. While ZeroFake does not require retraining, its adversarial prompt and reconstruction process are computationally expensive. Moreover, its reliance on case-specific thresholds (e.g., fake image detection and fake artwork detection), further limiting its practicality in open-world scenarios.

In summary, existing detection methods face significant challenges in generalizing to unseen generative models. As the landscape of generative models continues to diversify, there is a growing need for a universal detection method that can detect fake images generated by any model, even when only a single generative model is used during training.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.98\linewidth]{./figure/framework1.pdf}
  \caption{ The overall framework of our PAD. It consists of two key steps: (1) aligning test images with known fake distributions, and (2) generating pseudo-fake samples for further classification using deep KNN distances and a threshold-based criterion.}
\label{fig: framework}
\end{figure*}