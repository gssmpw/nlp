\section{Related Works}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\subsection{Evolution of AI-generated Images}
%-------------------------------------------------------------------------------
The rapid advancement of generative models has significantly improved the quality and diversity of AI-generated images.
Early approaches, such as Variational Autoencoders (VAEs)~\cite{kingma2013auto}, focused on learning latent representations of data distributions. 
Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative}, through their adversarial training framework, enabled high-fidelity image generation, with subsequent models like BigGAN~\cite{brock2018large} and StyleGAN~\cite{karras2019style} further enhancing controllability and realism.
Recently, diffusion models \cite{ho2020denoising} have emerged as a powerful alternative to GANs, achieving state-of-the-art results in image generation. These models gradually denoise a random distribution to generate highly detailed and diverse images~\cite{corvi2023detection}.
Additionally, autoregressive models such as DALLÂ·E~\cite{ramesh2022hierarchical}, along with diffusion-based models like Stable Diffusion 1.4~\cite{rombach2022high}, GLIDE~\cite{nichol2021glide}, and ADM~\cite{dhariwal2021diffusion}, have further pushed the boundaries by enabling highly realistic text-to-image generation. These models have significantly broadened the applicability of AI-generated images across a variety of domains, including creative content generation, artistic design, and virtual reality ~\cite{gu2022vector,ricker2022towards}.

However, the increasing realism of AI-generated images has raised concerns about their potential misuse, such as creating deepfakes or spreading misinformation. This has motivated research into robust detection techniques capable of generalizing across diverse generative models, as the subtle artifacts of generated images make them increasingly difficult to distinguish from real ones.

%-------------------------------------------------------------------------------
\subsection{Detection of AI-generated Images}
%-------------------------------------------------------------------------------
The detection of AI-generated images has become a critical research area due to the potential misuse of generative technologies~\cite{yan2023deepfakebench}. 
Early methods focused on identifying GAN-specific artifacts, such as pixel-level inconsistencies, using handcrafted features or traditional classifiers~\cite{marra2019gans,li2020face,mccloskey2019detecting}. While effective for low-quality or early-generation fake images, these approaches struggled to generalize to more advanced models. With the rise of deep learning, Convolutional Neural Networks (CNNs) have been widely adopted for their ability to automatically learn discriminative features~\cite{wang2024deepfake,rossler2019faceforensics++}. For example, Wang et al.~\cite{wang2020cnn} find that pre-trained CNNs on ProGAN-generated images can generalize to other GAN-based images, benefiting from large-scale training on diverse LSUN~\cite{yu2015lsun} object categories.

However, as generative models such as diffusion models and text-to-image models have gained traction, new challenges have emerged for detection tasks~\cite{stein2024exposing}. Unlike GANs, which often introduce visible artifacts, diffusion models produce highly realistic images with minimal visual discrepancies~\cite{corvi2023intriguing}. Zhu et al.~\cite{zhu2024genimage} highlight that classifiers trained solely on GAN-based images struggle to generalize to diffusion-based generated images, as the two types of synthetic images exhibit distinct fingerprints.
To address this, Wang et al.~\cite{wang2023dire} proposed the Diffusion Reconstruction Error (DIRE), which leverages the insight that real images cannot be accurately reconstructed by diffusion models. While effective for diffusion-generated images, DIRE fails to generalize to text-to-image generation models, as demonstrated by Sha et al. in ZeroFake~\cite{sha2024zerofake}. ZeroFake~\cite{sha2024zerofake} improves upon DIRE by exploiting the differential response of fake and real images to the adversary prompts during the inversion and reconstruction process, showing superior performance in detecting fake images generated by text-to-image models. While ZeroFake does not require retraining, its adversarial prompt and reconstruction process are computationally expensive. Moreover, its reliance on case-specific thresholds (e.g., fake image detection and fake artwork detection), further limiting its practicality in open-world scenarios.

In summary, existing detection methods face significant challenges in generalizing to unseen generative models~\cite{ojha2023towards}. As the landscape of generative models continues to diversify, there is a growing need for a universal detection method that can detect fake images generated by any model, even when only a single generative model is used during training. 

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.98\linewidth]{./figure/framework1.pdf}
  \caption{ The overall framework of our PAD. It consists of two key steps: (1) aligning test images with known fake distributions, and (2) generating pseudo-fake samples for further classification using deep KNN distances and a threshold-based criterion.}
\label{fig: framework}
\end{figure*}
%-------------------------------------------------------------------------------