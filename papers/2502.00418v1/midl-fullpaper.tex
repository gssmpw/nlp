\documentclass{midl} % Include author names

% The following packages will be automatically loaded:
% jmlr, amsmath, amssymb, natbib, graphicx, url, algorithm2e
% ifoddpage, relsize and probably more
% make sure they are installed with your latex distribution
\usepackage{placeins}
\usepackage{hyperref}

% \jmlrvolume{-- Under Review}
% \jmlryear{2025}
% \jmlrworkshop{Full Paper -- MIDL 2025 submission}
% \editors{Under Review for MIDL 2025}

% https://2025.midl.io/call-for-papers
\title[PEFT-SAM]{Parameter Efficient Fine-Tuning of Segment Anything Model}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
 % \midlauthor{\Name{Author Name1} \Email{abc@sample.edu}\and
 %  \Name{Author Name2} \Email{xyz@sample.edu}\\
 %  \addr Address}

 % Three or more authors with the same address:
 % \midlauthor{\Name{Author Name1} \Email{an1@sample.edu}\\
 %  \Name{Author Name2} \Email{an2@sample.edu}\\
 %  \Name{Author Name3} \Email{an3@sample.edu}\\
 %  \addr Address}


% Authors with different addresses:
% \midlauthor{\Name{Author Name1} \Email{abc@sample.edu}\\
% \addr Address 1
% \AND
% \Name{Author Name2} \Email{xyz@sample.edu}\\
% \addr Address 2
% }

%\footnotetext[1]{Contributed equally}

% More complicate cases, e.g. with dual affiliations and joint authorship
% \midlauthor{\Name{Author Name1\midljointauthortext{Contributed equally}\nametag{$^{1,2}$}} \Email{abc@sample.edu}\\
% \addr $^{1}$ Address 1 \\
% \addr $^{2}$ Address 2 \AND
% \Name{Author Name2\midlotherjointauthor\nametag{$^{1}$}} \Email{xyz@sample.edu}\\
% \Name{Author Name3\nametag{$^{2}$}} \Email{alphabeta@example.edu}\\
% \Name{Author Name4\midljointauthortext{Contributed equally}\nametag{$^{3}$}} \Email{uvw@foo.ac.uk}\\
% \addr $^{3}$ Address 3 \AND
% \Name{Author Name5\midlotherjointauthor\nametag{$^{4}$}} \Email{fgh@bar.com}\\
% \addr $^{4}$ Address 4
% }

 % Three or more authors with the different address:
\midlauthor{\Name{Carolin Teuber\midljointauthortext{Contributed Equally}\nametag{$^{1,2}$}} \orcid{0009-0004-3866-191X} \Email{carolin.teuber@stud.uni-goettingen.de}\\
  \Name{Anwai Archit\midlotherjointauthor\nametag{$^{2}$}} \orcid{0009-0002-9533-8620} \Email{anwai.archit@uni-goettingen.de}\\
  \Name{Constantin Pape\nametag{$^{2}$}} \orcid{0000-0001-6562-7187} \Email{constantin.pape@informatik.uni-goettingen.de}\\
  \addr $^{1}$ Georg-August-University Göttingen, Institute of Physics \\
  \addr $^{2}$ Georg-August-University Göttingen, Institute of Computer Science
}

\begin{document}

\maketitle

\begin{abstract}
Segmentation is an important analysis task for biomedical images, enabling the study of individual organelles, cells or organs. Deep learning has massively improved segmentation methods, but challenges remain in generalization to new conditions, requiring costly data annotation.
Vision foundation models, such as Segment Anything Model (SAM), address this issue through broad segmentation capabilities.
However, these models still require finetuning on annotated data, although with less annotations, to achieve optimal results for new conditions.
As a downside, they require more computational resources. This makes parameter-efficient finetuning (PEFT) relevant for their application. We contribute the first comprehensive study of PEFT for SAM applied to biomedical segmentation by evaluating 9 PEFT methods on diverse datasets. We also provide an implementation of QLoRA for vision transformers and a new approach for resource-efficient finetuning of SAM.
\end{abstract}
\begin{keywords}
segment anything, peft, biomedical image segmentation
\end{keywords}

\section{Introduction}
Segmentation is a fundamental analysis task for biomedical images.
It enables the study of individual objects, such as cells and organelles in microscopy, or organs and lesions in medical imaging.
Deep learning has massively advanced the field.
However, the large diversity of modalities and tasks so far required different methods for specific applications, such as CellPose \cite{Pachitariu:2022:cellpose2} and Stardist \cite{Weigert:2022:stardist} for cell and nucleus segmentation, or nnU-Net \cite{MaierHein:2021:nnunet} and TotalSegmentator \cite{wasserthal2023totalsegmentator} for segmentation in radiology.
Adapting any of these models to a new modality or a new segmentation task requires further data annotation for training due to limited generalisation. Annotations can only be provided by experts and is time-consuming, making adaptation costly and preventing the wider use of automatic segmentation.

Vision foundation models for image segmentation, e.g. Segment Anything Model (SAM) \cite{Kirrilov:arXiv:2023:SAM} or SEEM \cite{zou:2023:seem}, promise a more unified solution.
These models have been trained on large annotated datasets and address both interactive and automatic segmentation.
They were recently adapted to biomedical images, resulting in foundation models for microscopy \cite{Archit:arXiv:2023:MicroSAM, israel2024foundation} and medical imaging \cite{ma2024segment,zhao2024foundation,archit2025medico}.
These models are applicable to many tasks in their respective domain without adaptation, but specific finetuning can further improve them \cite{Archit:arXiv:2023:MicroSAM}.
Notably, they need fewer annotated data compared to other models, requiring as few as a single labeled image \cite{zhou:2024:cellseg1}.

Most vision foundation models use a vision transformer (ViT) \cite{dosovitskiy_arxiv:2021:vit} as encoder. Hence, they typically have more parameters than previous architectures, making training more resource demanding and requiring a high-end GPU, even for small training sets.
To enable efficient adaptation, parameter-efficient finetuning (PEFT) has emerged, for example through low rank adaptation of attention layers (LoRA) \cite{hu:2021:lora}.
Instead of updating all model parameters, these methods either update only a small subset of parameters, or they introduce a few new parameters that are updated, while freezing the rest of the model. While PEFT has been extensively studied for large language models \cite{pu:2023:empiricalanalysisstrengthsweaknesses, xu:2023:parameterefficientfinetuningmethodspretrained, balne:2024:parameterefficientfinetuning}, its application in computer vision, particularly for segmentation tasks, is less explored. 
Several approaches that adapt foundation models to biomedical segmentation use PEFT, e.g. \cite{Archit:arXiv:2023:MicroSAM,zhou:2024:cellseg1} for microscopy and \cite{gu:arxiv:2024:bestmedicalmodel,zhang2023customizedsegmentmodelmedical,wei:2024:imedsam} for medical imaging. However, they primarily use LoRA without investigating hyperparameters or alternatives.
Prior work has studied PEFT for classification in natural images \cite{xin2024parameterefficientfinetuningpretrainedvision} as well as classification and text-to-image generation in medical images \cite{dutt:openreview:2024:parameterefficient}.

% Notes CP: I am reviewing related work, leaving summaries here
% - CellSeg1 (https://arxiv.org/pdf/2412.01410): Interesting, but I am quite skeptical of their claims that AMG works better than CellPose. My guess is that they report extremely unfavorable numbers because they screw up both CellPose and StarDist by their training / finetuning, using the pretrained models they would likely get much better scores.
% - Dutt et al., PEFT for MIA (https://openreview.net/forum?id=LVRhXa0q5r): PEFT for image classification and text-to-image generation in medical images
% - Xin et al., 20204: PEFT survey (https://arxiv.org/abs/2402.02242): Natural images and only classification tasks.
% - I-MedSAM (https://arxiv.org/abs/2311.17081): This combines SAM with an implicit decoder. It uses LoRA in the image encoder. Quite interesting work, but not super relevant here. I have added it to "examples of work that use LoRA but don't really evaluate it thoroughly".

We address this gap by studying PEFT for biomedical segmentation; for SAM and its variants $\mu$SAM \cite{Archit:arXiv:2023:MicroSAM} and MedicoSAM \cite{archit2025medico}. We contribute:
\begin{itemize}
    \item An evaluation of 9 PEFT methods on 6 microscopy and 6 medical imaging datasets. 
    \item An implementation of quantized LoRA (QLoRA) \cite{dettmers:2023:qlora} for ViTs.
    \item Recommendations for the use of PEFT and a workflow for efficient adaptation of SAM to new segmentation tasks.
    \item A detailed ablation of hyperparameters for several PEFT methods, including LoRA.
\end{itemize}
Our workflow and improvements from efficient adaptation are shown in Fig.~\ref{fig:peft_sam}.
We believe that our study will facilitate the use of foundation models for biomedical image segmentation. Further, it will inform future developments of PEFT for computer vision.
Our code is available at \url{https://github.com/computational-cell-analytics/peft-sam}.

\begin{figure}[h!]
 % conceptual image should go here
\floatconts
  {fig:peft_sam}
  {\caption{Overview of PEFT-SAM. a) We study PEFT of domain-specific SAMs to enable interactive correction and efficient finetuning for improved segmentation. b) Segmentation results for three datasets from $\mu$SAM, $\mu$SAM finetuned on two images with LoRA and CellSeg1 \cite{zhou:2024:cellseg1}.}}
{\includegraphics[width=0.8\linewidth]{fig/figure_1.png}}
\end{figure}

\section{Methods}

\subsection{Additive and Selective Finetuning}\label{sec:peft}

\begin{figure}[htbp]
 % conceptual image should go here
\floatconts
  {fig:architecture}
  {\caption{The original architecture of SAM, comprising image encoder, mask decoder, and prompt encoder. The image encoder is fine-tuned through additive PEFT, which introduces a few additional parameters, or selective PEFT, which updates only selected parameters. We study 9 methods; freezing the image encoder not shown.}}
{\includegraphics[width=0.8\linewidth]{fig/figure1b_horizontal_v2.png}}
\end{figure}

We distinguish two types of methods: additive and selective PEFT.
Selective methods update a subset of the model parameters, while freezing the rest. 
A simple example is freezing parts of the model, for example SAM's image encoder, mask decoder, or prompt encoder. Freezing the image encoder, which contains most parameters, yields the greatest efficiency gain. \cite{Archit:arXiv:2023:MicroSAM} have shown that this approach does not have a large negative impact on segmentation quality after finetuning.
We refer to this approach as Freeze Encoder.
The other selective PEFT methods we study are LayerNorm Tuning (LN Tune) \cite{basu:2023:layernormtune}, Bias Tuning (Bias Tune) \cite{cai:2021:biastune}, and Attention Tuning (Attn Tune). \cite{touvron:2022:attntune}, which update only the parameters of the normalization layers, the biases, or the attention weights, respectively.

Additive PEFT methods introduce a few additional parameters, while freezing the existing ones. Among these methods, LoRA \cite{hu:2021:lora} reduces the number of parameters via a low rank decomposition of the attention weight matrix:
\begin{equation} \label{eq:lora}
\bold{W} = \bold{W}_{pretrained} + \alpha \bold{AB}\,,
\end{equation}
where $\bold{A} \in \mathbb{R}^{d\times r}$, $\bold{B} \in \mathbb{R}^{r\times d}$. Here, the rank $r$ is much smaller than the original dimension $d$ of $\bold{W}$. $\alpha$ scales the learned weights. Further efficiency gains can be obtained by quantizing the frozen weights, i.e. $W_{pretrained}$ and others, to 4 bit during training, as in QLoRA \cite{dettmers:2023:qlora}.
We adopt this approach for ViTs, providing, to our knowledge, the first application in computer vision.
AdaptFormer \cite{chen:arxiv:2022:adaptformer} adds a lightweight module, consisting of two fully connected layers, in parallel to the MLP layers of the ViT.
The outputs of the adapter are scaled by a factor $\alpha$ and added to the original MLP outputs.
Scale-Shift Features (SSF) \cite{lian:2023:arxiv:ssf} performs a linear transformation, introducing a scale and a shift parameter to the output of each layer in the transformer block.
Factor Tuning (FacT) \cite{jie:arxiv:2023:fact} combines multiple weight matrices into a single tensor and then applies a low rank decomposition to it. Here, we adopt the implementation of \cite{chen2024ma}, which tensorizes the attention weight increment matrices (corresponding to $A, B$ in Eq.~\ref{eq:lora}) and then decomposes the tensor according to:
\begin{equation}
    \Delta \bold{W} = \bold{U} \bold{\Sigma} \bold{V}^T\,,
\end{equation}
where $\bold{U} \in \mathbb{R}^{d \times r}$, $\bold{V} \in \mathbb{R}^{d \times r}$ and $\Sigma \in \mathbb{R}^{r\times r}$. The factors $\bold{U}$ and $\bold{V}$ are shared across layers.

%OLD Fact text, that was correct for the original implementation, but not for the MaSam implementation we use. I am leaving it here for reference.
%Factor Tuning (FacT) \cite{jie:arxiv:2023:fact} tensorizes all attention weight matrices of the ViT and then finds a low rank decomposition of the entire tensor, similar to LoRA. The stacked attention weights can be written in one tensor $\bold{W} \in \mathbb{R}^{12L\times d\times d}$, where $L$ refers to the number of layers of the transformer. Then the weight update $\Delta \bold{W}$ can be decomposed into three matrices $\bold{U} \in \mathbb{R}^{d \times r_1}$, $\bold{V} \in \mathbb{R}^{d \times r_2}$ and $\bold{\Sigma} \in \mathbb{R}^{12L \times r_1}$ as 
%\begin{equation*}
%    \Delta \bold{W} = \bold{\Sigma} \times_2 \bold{U}^T \times_3 \bold{V}^T\,,
%\end{equation*}
%with $\times_i$ referring to the mode-i product.

Fig.~\ref{fig:architecture} shows an overview of these 9 PEFT methods. We apply them only to the image encoder; prompt encoder and mask decoder are always updated.
We also compare these methods with normal parameter updates (Full FT).
We use SAM \cite{Kirrilov:arXiv:2023:SAM} and two domain-specific models, $\mu$SAM \cite{Archit:arXiv:2023:MicroSAM} for microscopy, and MedicoSAM \cite{archit2025medico} for medical imaging, to initialize weights.
The latter two models have finetuned SAM on a large annotated dataset from the respective domain.

\subsection{Interactive Segmentation} \label{sec:SAM}
SAM has introduced a novel formulation for interactive segmentation, where users can provide input prompts, points (positive or negative), a bounding box or a rough mask, to identify an object.
The model then predicts the corresponding mask by processing the image with its image encoder, a ViT, the prompts with its prompt encoder, and the outputs of image encoder and prompt encoder with its mask decoder.
Model predictions can be corrected by providing further prompts.

SAM is trained on a large dataset with annotations using an objective that simulates interactive segmentation. In each iteration, this objective first samples prompts from an annotated mask, predicts the object mask, and then iteratively corrects the prediction with additional prompts sampled from the annotation.
Predictions and annotations are compared with a loss function for each iteration, and the average loss is used to update parameters.
We use the implementation of this procedure from \cite{Archit:arXiv:2023:MicroSAM}. 

To evaluate interactive segmentation, we automatically derive prompts from annotated masks to segment the object and then iteratively correct the segmentation, similar to the training objective. We perform 7 correction iterations.
We compute the segmentation metric (App.~\ref{app:metric}) between annotations and predictions for the initial prompt and corrections. 
We report the results for an initial point prompt, an initial box prompt, and the last correction iteration, when starting from a point ($I_{P}$), and when starting from a box ($I_{B}$).

\subsection{Automatic Instance Segmentation} \label{sec:AIS}
For automatic instance segmentation (AIS) we use the implementation of $\mu$SAM, which adds a UNETR decoder \cite{hatamizadeh:2021:unetr} to SAM to predict outputs for instance segmentation.
The decoder consists of four blocks, with two convolutional layers and a transposed convolution for upsampling each. The blocks also receive the image encoder's output as input. The decoder predicts three channels: the distance to the object center, the distance to the object boundary, and foreground probabilities.
During training, it is updated jointly with the rest of SAM, using the annotations to derive targets for its outputs.

During inference, the segmentation is computed by deriving seeds from the distance predictions, a mask from foreground predictions, and applying a seeded watershed inside this mask.
For evaluation, the instance segmentation is compared with annotations using the mean segmentation accuracy, see App.~\ref{app:metric}.

\subsection{Data} \label{sec:data}
We use 6 microscopy and 6 medical imaging datasets.
For light microscopy (LM), we use Covid-IF \cite{Pape:BioEssays:2021:CovidIF} with 49 immunofluorescence microscopy images and cell annotations, HPA \cite{Ouyang:nm:2019:hpa} with 276 confocal microscopy images and cell annotations; using the channel that stains cytosol.
We use GoNuclear \cite{Vijayan:dev:2024:gonuclear} with 5  fluorescence microscopy and nucleus annotations.
We use OrgaSegment \cite{Lefferts:CB:2024:orgasegment} with 231 brightfield microscopy images and organoid annotations.
We use two electron microscopy (EM) datasets: Platynereis \cite{Vergara:cell:2021:platynereis} with 3 EM volumes and cilia annotations, and a subset of MitoLab \cite{Conrad:cs:2023:mitolab} with a volume of glycolytic muscles and mitochondria annotations.
For the 3D datasets (GoNuclear, Platynereis, MitoLab), we perform 2D segmentation by treating slices as individual images.

For medical imaging, we use AMD-SD \cite{amd-sd} with 3049 optical coherence tomograms and annotations for lesions. We use JSRT \cite{jsrt} with 247 images and lung and heart annotations. We use Mice TumSeg \cite{mice-tumseg} with 452 micro-CT volumes and tumor annotations. We use Papila \cite{papila} with 488 fundus images and optic cup annotations. We use MOTUM \cite{motum} with 64 brain MRI volumes and tumor annotations.
We use PSFHS \cite{psfhs} with 1358 ultrasound images and fetal head and pubic sympysis annotations.
For the 3D datasets (MOTUM and Mice TumSeg), we perform 2D segmentation (see above).

\section{Results}

\subsection{Comparison of PEFT methods} \label{sec:res_peft}
We evaluate the PEFT methods (Sec.~\ref{sec:peft}), on 6 microscopy datasets (Sec.~\ref{sec:data}) for interactive and automatic segmentation. We use SAM and $\mu$SAM (either LM or EM model) as base models.
Models are trained with a batch size of 2 and 25 objects per image, using a A100 GPU with 80GB of VRAM. We use separate train / test splits for all datasets.

\begin{figure}[ht!]
\floatconts
  {fig:microscopy_result}
  {\caption{PEFT results for automatic and interactive microscopy segmentation. Methods are ordered by parameter count, SAM and $\mu$SAM are used as base models. Circles show the best three results per dataset and task. See Fig.~\ref{fig:qualitative_iterative_prompting} for examples.} \label{fig:microscopy_results}}  {\includegraphics[width=0.9\linewidth]{fig/results_microscopy.png}}
\end{figure} %\vspace{-2.5cm}

The results are shown in Fig.~\ref{fig:microscopy_results}. Finetuning improves over the base models, depending on the task and the base model. Iterative segmentation ($I_{B}$, $I_{P}$) shows the smallest improvements, other tasks improve more. The effectiveness of PEFT methods differs across datasets. Full finetuning typically improves most, LoRA is the best overall PEFT method. QLoRA shows worse performance with SAM as base model, but not for $\mu$SAM.
The results are summarized in Tab.~\ref{tab:avg_results_sam} (SAM) and Tab.~\ref{tab:avg_results_microsam} ($\mu$SAM).
We perform ablation studies for LoRA, Adaptformer and FacT in App.~\ref{sec:ablation}.
We also perform experiments for the 6 medical datasets. Here, we use SAM and MedicoSAM as base models and do not evaluate AIS, otherwise using the same settings. The results are shown in Fig.~\ref{fig:medical_results} and summarized in Tabs~\ref{tab:avg_results_medical_images_sam},\ref{tab:avg_results_medical_images_medicosam}. We see the same trends as for microscopy.

\begin{figure}[ht!]
\floatconts
  {fig:medical_result}
  {\caption{PEFT results for medical imaging. MedicoSAM is used as domain-specific base model and we do not evaluate AIS, otherwise same settings as for microscopy.} \label{fig:medical_results}}
{\includegraphics[width=0.9\linewidth]{fig/results_medical.png}}
\end{figure}

We also evaluate the computational efficiency of PEFT methods. Tabs.~\ref{tab:combined_peft_methods_memory}, \ref{tab:combined_peft_methods_memory_vit_l}, \ref{tab:combined_peft_methods_memory_vit_h} report the parameter count, memory efficiency, and training times for ViT-B, ViT-L and ViT-H. Fig.~\ref{fig:training_times} shows the training times for microscopy.
Overall, PEFT methods only yield marginal efficiency gains for ViT-B and ViT-L, with the exception of Freeze Encoder, and small gains for ViT-H.
As a preliminary conclusion, PEFT methods generally don't lead to better segmentation quality than full finetuning. Their efficiency gains are minor, except for Freeze Encoder. LoRA and, for small domain gaps, QLoRA provide relatively good quality-efficiency trade-offs.

\subsection{Resource-efficient Finetuning}
Building on the evaluation of PEFT methods, we propose resource-efficient finetuning by adopting a similar workflow to CellSeg1 \cite{zhou:2024:cellseg1}, who finetune SAM for automatic segmentation on a single image using LoRA. We compare their method with our finetuning approach for automatic \emph{and} interactive segmentation, using Freeze Encoder, QLoRA, LoRA, and Full Finetuning, which offer the best efficiency-quality trade-offs.
For more efficiency, we lower the number of objects per image to 5, otherwise using the same settings as in Sec.~\ref{sec:res_peft}.
We use one image for training, one for validation, and the same test sets as before.
Fig.~\ref{fig:single_img_training} shows the results. SAM improves for all tasks, $\mu$SAM mainly for AIS. Our approach is on par with CellSeg1, for which we also use both base models, while improving interactive segmentation.
Interestingly, full finetuning is on par with PEFT, despite the small training data and more trainable parameters. For large domain shifts (Platynereis) it outperforms PEFT.
We also benchmark efficiency, see Tabs.~\ref{tab:memory_table}, \ref{tab:single_img_train_time}, \ref{tab:single_img_time_per_it}. Here, we again see major gains when freezing the encoder, and small gains from other PEFT.

\begin{figure}[ht!]
 % conceptual image should go here
\floatconts
  {fig:single_img_training}
  {\caption{Resource-efficient training for microscopy segmentation. Our methods use one image for training, one for validation, CellSeg1 uses only a single image.} \label{fig:single_img_training}}
  {\includegraphics[width=0.9\linewidth]{fig/single_img_training.png}}
\end{figure}

\section{Discussion}
We conduct a thorough study of PEFT for segmentation in biomedical images with SAM.
Our experiments show that PEFT achieves a similar quality as full finetuning. Contrary to observations made by others, e.g. \cite{dutt:openreview:2024:parameterefficient}, it does not lead to better results for limited training data.
The performance of PEFT methods varies across datasets, LoRA is the best overall, QLoRA is good for small domain gaps.
Efficiency gains are marginal for small models (ViT-B, ViT-L) and small for larger models (ViT-H). An exception is encoder freezing, which leads to clear gains, without a strong negative impact on quality.
Given these results, we recommend freezing the encoder in limited resource settings, exploring QLoRA or LoRA with medium resources, which may enable training despite only small efficiency benefits, and otherwise using full finetuning.

We propose an approach for resource-efficient finetuning, which can improve SAM with only two annotated image, similar to \cite{zhou:2024:cellseg1}, but also improving interactive segmentation.
We believe that this approach will speed up many practical segmentation tasks, by enabling finetuning SAM previously hindered by resource demands.
To this end, we will implement our approach in the $\mu$SAM tool, which provides interactive data annotation, enabling efficient human-in-the-loop annotation and training in one tool.

Finally, we believe that our work will benefit future work on PEFT for vision foundation models, e.g. for SAM2 \cite{ravi2024sam}.
We also show a need to improve PEFT for segmentation; current methods do not yield clear advantages in the quality-efficiency trade-off.
Such improvements may be achieved by simple strategies, such as using adapters in a subset of layers, or through more fundamental advances.

%\noindent In this work, we demonstrated the potential of parameter-efficient fine-tuning (PEFT) methods for microscopy image segmentation. Through our experiments, we highlighted that the choice of the optimal fine-tuning method is highly dataset-dependent. However, LoRA emerged as a particularly robust approach, consistently achieving competitive performance, while significantly reducing memory and parameter requirements. This balance between efficiency and effectiveness positions LoRA as a promising candidate for resource-constrained scenarios.
%\noindent Building on the insights presented in \cite{Archit:arXiv:2023:MicroSAM}, which investigated fine-tuning in the context of the low data regime, we aim to extend our studies to explore the performance of LoRA, when training on extremely limited data, such as a single image with qualitative annotations. This will help establish whether LoRA can maintain its effectiveness under minimal data conditions, further enhancing its applicability in domains where annotated data is expensive to obtain.
%\noindent In addition to refining our understanding of LoRA’s performance in the low data regime, we will extend our investigations to more practical setups. 
%Despite the memory savings achieved by LoRA, as outlined in Table \ref{tab:combined_peft_methods_memory}, further reductions in memory usage are still possible. To address this, we turn to QLoRA \cite{dettmers:2023:qlora}, a method that combines LoRA with 4-bit model quantization. Given its significant potential for memory reduction without compromising performance, QLoRA could be a key enabler for extending fine-tuning capabilities to consumer-grade hardware.
%\noindent Using QLoRA, we aim to evaluate the feasibility of training models using LoRA CPUs and standard GPUs. This will allow us to validate the utility of PEFT methods in settings with limited computational resources, bringing their benefits closer to real-world laboratory environments where access to high-performance hardware may be restricted.

\clearpage  % Acknowledgements, references, and appendix do not count toward the page limit (if any)
% Acknowledgments---Will not appear in anonymized version
\midlacknowledgments{
The work of Anwai Archit was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - PA 4341/2-1. 
This work was also supported by the Google Research Scholarship “Vision Foundation Models for Bioimage Segmentation”.
We gratefully acknowledge the computing time granted by the Resource Allocation Board and provided on the supercomputer Emmy at NHR@G{\"o}ttingen as part of the NHR infrastructure, under the project nim00007.
We acknowledge the use of icons from Bioicons~\cite{bioicons} in Fig.~\ref{fig:peft_sam}. In particular, the microscope icon by DBCLS~\cite{dbcls} is licensed under CC-BY 4.0.
}

\bibliography{midl-samplebibliography}

\appendix

\section{Evaluation Metric} \label{app:metric}

We use the mean segmentation accuracy, following the definition of \cite{Caicedo:2019:dsb}, to evaluate instance segmentation results for the microscopy datasets. 
It is computed based on true positives ($TP$), false negatives ($FN$), and false positives ($FP$), derived from the intersection over union (IoU) of predicted and true objects.
Specifically, a $TP(t)$ is defined as the number of matches between predicted and true objects with an IoU above threshold $t$, $FP(t)$ correspond to the number of predicted objects minus $TP(t)$, and $FN(t)$ to the number of true objects minus $TP(t)$.
The mean segmentation accuracy is computed over multiple thresholds:
\begin{equation*}
    \text{Mean Segmentation Accuracy} = \frac{1}{|\text{\# thresholds}|} \sum_{t} \frac{TP(t)}{TP(t) + FP(t) + FN(t)}\,.
\end{equation*}
Here, we use thresholds $t \in [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]$. For each dataset, we report the average mean segmentation accuracy over images in the test set.

We use the dice coefficient to evaluate segmentation results for medical imaging because the segmentation tasks we evaluate typically only contain a single object, or objects belonging to different classes, per image.
It is defined as
\begin{equation*}
    \text{Dice Coefficient} = \frac{2 * \sum{p_i \, t_i}}{\sum p_i + \sum t_i},
\end{equation*}
for per-pixel prediction values $p_i$ and per pixel target values $t_i$.
For each dataset, we report the average dice coefficient over images in the test set.

\section{PEFT Evaluation}

\subsection{Segmentation Quality}

Fig.~\ref{fig:microscopy_results} and Fig.~\ref{fig:medical_results} present a comparison of various PEFT methods across microscopy and medical datasets respectively. 
The PEFT methods are arranged from left to right in the order of the number of trainable parameters, with full finetuning having the most parameters. Table \ref{tab:combined_peft_methods_memory} lists the number of parameters for each method. We evaluate the methods using training from the default SAM \cite{Kirrilov:arXiv:2023:SAM} and from the generalist $\mu$-SAM models \cite{Archit:arXiv:2023:MicroSAM}, where we use either the electron microscopy or light microscopy model, and from MedicoSAM \cite{archit2025medico} for medical imaging.
The models are evaluated on automatic instance segmentation (AIS) for microscopy experiments, and initial prompts (point and box prompts), and iterative prompting, beginning either from a box prompt ($I_B$) or a point prompt ($I_P$) for both microscopy and medical imaging experiments.

As an overall trend, we observe that as the number of trainable parameters increases, the segmentation quality also tends to increase across the different datasets, demonstrating a tradeoff between model complexity, capacity, and performance. 
The relative performance of the PEFT methods varies between datasets, indicating that the suitability of different PEFT methods may depend on the dataset's characteristics, such as complexity, size, and the model's initial performance. Despite this variation, full finetuning, Attention Tuning, and LoRA consistently show good performance;
QLoRA shows good performance for the domain specific foundation models ($\mu$SAM, MedicoSAM), but performs bad for default SAM, which has a larger domain gap.
Among the listed PEFT methods, Attention Tuning has the highest number of trainable parameters, requiring 32.5M parameters. In contrast, LoRA significantly reduces the parameter space to just 5.4M parameters, amounting to only 5.6\% of the parameters used in Full Finetuning (FFT). Despite this substantial reduction, LoRA achieves performance that is on par with Attention Tuning and Full Finetuning in most cases. 
To simplify these observations, we averaged the performance of the different PEFT methods over all datasets in Tabs.~\ref{tab:avg_results_sam} (microscopy, SAM), \ref{tab:avg_results_microsam} (microscopy, $\mu$SAM), \ref{tab:avg_results_medical_images_sam} (medical imaging, SAM), and \ref{tab:avg_results_medical_images_medicosam} (medical imaging, MedicoSAM).
LoRA consistently ranks among the top three methods, frequently alongside full fine-tuning and attention tuning, despite these methods having significantly more parameters. It is notable that for the iterative prompting task, FacT performs very well alongside LoRA. This is especially interesting because FacT is essentially an extension of LoRA, applying the low-rank decomposition to a tensorized vision transformer.
Moreover, while some PEFT methods excel for specific tasks - such as Bias Tuning for Mitolab - LoRA consistently ranks among the top methods across diverse scenarios, including AIS and single point and box prompt. This makes LoRA the best PEFT choice from a segmentation quality perspective.

The strong performance of both LoRA and Attention Tuning suggests that tuning the attention weight matrices is particularly important for domain adaptation. This highlights the potential benefit of focusing on attention-based techniques when fine-tuning models for specialized tasks.

\begin{table}[ht]
\caption{The mean segmentation accuracy of different PEFT methods, trained from default SAM, for microscopy segmentation and averaged over all 6 datasets. (*): 1st Place; (**): 2nd Place; (***): 3rd Place.}
\centering
\begin{tabular}{l|r|r|r|r|r}
\textbf{PEFT Method} & \textbf{AIS} & \textbf{Point} & \textbf{Box} & $\mathbf{I_p}$ & $\mathbf{I_b}$ \\
\hline
Full FT & **0.352 & **0.303 & ***0.665 & 0.727  & 0.796  \\
Attn Tune & *0.352  & *0.308  & *0.673  & **0.735 & **0.809 \\
AdaptFormer & 0.291   & 0.255   & 0.636   & 0.718  & 0.795  \\
LoRA & ***0.343 & ***0.296 & **0.667  & *0.741  & *0.810  \\
QLoRA & 0.281   & 0.218   & 0.529   & 0.206  & 0.288  \\
FacT & 0.328   & 0.270   & 0.644   & ***0.730 & ***0.797 \\
SSF & 0.291   & 0.245   & 0.625   & 0.710  & 0.779  \\
Bias Tune & 0.334   & 0.279   & 0.644   & 0.724  & 0.788  \\
LN Tune & 0.329   & 0.281   & 0.635   & 0.717  & 0.789  \\
Freeze Encoder & 0.309  & 0.267   & 0.620   & 0.690  & 0.768  \\
\end{tabular}
\label{tab:avg_results_sam}
\end{table}

\begin{table}[ht]
\caption{The mean segmentation accuracy of different PEFT methods, trained from $\mu$SAM, for microscopy segmentation and averaged over all 6 datasets. (*): 1st Place; (**): 2nd Place; (***): 3rd Place.}
\centering
\begin{tabular}{l|r|r|r|r|r}
\textbf{PEFT Method} & \textbf{AIS} & \textbf{Point} & \textbf{Box} & $\mathbf{I_p}$ & $\mathbf{I_b}$ \\
\hline
Full FT & **0.406 & *0.351  & **0.696  & 0.793  & 0.849  \\
Attn Tune & *0.414 & **0.337 & ***0.695  & **0.803 & **0.852 \\
AdaptFormer & 0.334 & 0.287 & 0.672 & 0.788  & 0.837  \\
LoRA & **0.390 & ***0.336 & *0.702  & *0.804  & ***0.852 \\
QLoRA & 0.334 & 0.302 & 0.657 & 0.761  & 0.823  \\
FacT & 0.379 & 0.317 & 0.693 & ***0.799 & *0.854 \\
SSF & 0.324 & 0.274 & 0.670 & 0.789  & 0.842  \\
Bias Tune & 0.357 & 0.307 & 0.690 & 0.796  & 0.843  \\
LN Tune & 0.366 & 0.318 & 0.684 & 0.789  & 0.842  \\
Freeze Encoder & 0.330  & 0.268 & 0.655 & 0.763  & 0.824  \\
\end{tabular}
\label{tab:avg_results_microsam}
\end{table}

\begin{table}[ht]
\caption{The dice coefficient of different PEFT methods, trained from default SAM, for medical segmentation and averaged over all 6 datasets. (*): 1st Place; (**): 2nd Place; (***): 3rd Place.}
\centering
\begin{tabular}{l|r|r|r|r|r}
\textbf{PEFT Method} & \textbf{Point} & \textbf{Box} & $\mathbf{I_p}$ & $\mathbf{I_b}$ \\
\hline
Full FT & *0.723 & **0.830 & 0.811 & 0.850  \\
Attn Tune & **0.721 & *0.810 & **0.825 & 0.855  \\
AdaptFormer & 0.612 & 0.814 & 0.789 & ***0.858 \\
LoRA & ***0.673 & ***0.826 & *0.838  & *0.862  \\
QLoRA & 0.386 & 0.757 & 0.394  & 0.508  \\
FacT & 0.633 & 0.812 & ***0.815 & **0.862 \\
SSF & 0.592 & 0.802 & 0.803  & 0.850  \\
Bias Tune & 0.615 & 0.810 & 0.802  & 0.847  \\
LN Tune & 0.596 & 0.818 & 0.805 & 0.843  \\
Freeze Encoder & 0.521 & 0.791 & 0.784  & 0.833  \\
\end{tabular}
\label{tab:avg_results_medical_images_sam}
\end{table}

\begin{table}[ht]
\caption{The dice coefficient of different PEFT methods, trained from MedicoSAM, for medical segmentation and averaged over all 6 datasets. (*): 1st Place; (**): 2nd Place; (***): 3rd Place.}
\centering
\begin{tabular}{l|r|r|r|r|r}
\textbf{PEFT Method} & \textbf{Point} & \textbf{Box} & $\mathbf{I_p}$ & $\mathbf{I_b}$ \\
\hline
Full FT & *0.709 & 0.813 & 0.826  & 0.868  \\
Attn Tune & ***0.687 & 0.824 & **0.862 & 0.879  \\
AdaptFormer & 0.670 & 0.842 & 0.856  & 0.880  \\
LoRA & **0.694 & 0.836  & *0.883 & *0.901  \\
QLoRA & 0.639 & ***0.846 & ***0.856 & **0.890 \\
FacT & 0.645 & **0.849 & 0.852 & 0.885  \\
SSF & 0.658 & 0.845 & 0.853 & 0.883  \\
Bias Tune & 0.643 & 0.832 & 0.845 & 0.881  \\
LN Tune & 0.645 & *0.850 & 0.853 & 0.879  \\
Freeze Encoder & 0.583 & 0.812 & 0.830  & 0.879  \\
\end{tabular}
\label{tab:avg_results_medical_images_medicosam}
\end{table}

\begin{figure}[htbp]
\floatconts
  {fig:qualitative_iterative_prompting}
  {\caption{Qualitative comparison of interactive segmentation for the default $\mu$SAM model and the finetuned model with LoRA and Full Finetuning. For all three models ViT-B was used. The yellow outlines show the ground truth, red the model prediction and cyan shows the input prompts.}}
  {\includegraphics[width=0.9\linewidth]{fig/qualitative_iterative_prompting.png}}\label{fig:qualitative_iterative_prompting}
\end{figure}

\begin{figure}[htbp]
\floatconts
  {fig:qualitative_iterative_prompting}
  {\caption{Qualitative comparison of interactive segmentation for the default MedicoSAM and the finetuned model with LoRA and Full Finetuning. For all three models ViT-B was used. The yellow outlines show the ground truth, red the model prediction and cyan shows the input prompts.}}
  {\includegraphics[width=0.9\linewidth]{fig/qualitative_iterative_prompting_medical.png}}\label{fig:qualitative_iterative_prompting_medical}
\end{figure}

\FloatBarrier
\subsection{Computational Efficiency}

We measure the efficiency of PEFT methods, in terms of number of trainable parameters, training time per iteration and in total, and VRAM memory usage during training. Tab.~\ref{tab:combined_peft_methods_memory} reports these measures for ViT-B and all PEFT methods, Tab.~\ref{tab:combined_peft_methods_memory_vit_l} and Tab.~\ref{tab:combined_peft_methods_memory_vit_h} report them for ViT-L and ViT-H respectively, for only a subset of PEFT methods.
These results were obtained from finetuning default SAM on the LIVECell \cite{Edlund:nature:2021:LIVECell} dataset, a large dataset with annotations for cell segmentation in phase-contrast microscopy. Note that we perform all experiments for segmentation quality with the smallest model, ViT-B, because it was shown e.g. in \cite{gu:arxiv:2024:bestmedicalmodel,archit2025medico} that using it does not have a noticeable impact compared to the larger models ViT-L and ViT-H for segmentation quality in biomedical imaging.
However, we evaluate the computational efficiency for all model sizes, as this observation may not hold true for other domains, for which finetuning the larger models may be beneficial.
We report efficiencies only for a single dataset, as these measurements are largely independent of the data specifics.
The exception is the overall training time because we use early stopping.
Specifically, we stop the training after 10 epochs without an improvement in the validation score. Hence, the overall training time depends on dataset characteristics. We report the training time across datasets in Fig.~\ref{fig:training_times}.

We see that all PEFT methods lead to a clear reduction in the number of trainable parameters, from 30\% (Attn Tune) to less than ca. 5\% (all others).
However, this reduction in trainable parameters does not result in a clear reduction of memory requirements or training time per iteration.
This fact is especially noticeable for ViT-B, for the larger models, especially ViT-H, the reduction in memory becomes a bit more pronounced.
The exception is freezing the encoder, which clearly reduces the memory demand and also reduces training time per iteration, for all model sizes.
It is unclear to us why the reduction in trainable parameters is not accompanied by a similar reduction in memory consumption for other methods.

\begin{table}[ht]
\caption{Number of trainable parameters, training times, and allocated VRAM during training for different PEFT methods. Here, Train Time refers to the time until the best epoch is reached during training. Training times and memory are reported for finetuning default SAM with ViT-B on LIVECell. The number of parameters is shown as \# Params w/o decoder $|$ \# Params with decoder. (*): selective PEFT; (**): additive PEFT}
\centering
\begin{tabular}{l|r|r|r|r}
PEFT Method & \#Params [M] & Time / it [s] & Train Time [h] & Memory [GB] \\
\hline
Full FT & 93.7 $|\,$ 104.8 & 1.44 & 14.33 & 52.1 \\
Attn Tune (*) & 32.5 $|\,$ 43.5 & 1.42 & 19.72 & 51.4 \\
AdaptFormer (**) & 5.3 $|\,$ 16.3 & 1.43 & 18.75 & 50.8 \\
LoRA (**) & 5.3 $|\,$ 16.3 & 1.44 & 17.59 & 51.2 \\
QLoRA (**) & 5.3 $|\,$ 16.3 & 1.50 & 18.34 & 51.1 \\
FacT (**) & 4.4 $|\,$ 15.4 & 1.45 & 19.60 & 51.2 \\
SSF (**) & 4.3 $|\,$ 15.3 & 1.49 & 16.12 & 53.3 \\
Bias Tune (*) & 4.2 $|\,$ 15.2 & 1.42 & 8.34  & 50.1 \\
LN Tune (*) & 4.1 $|\,$ 15.1 & 1.41 & 15.29 & 50.9 \\
Freeze Encoder (*) & 4.1 $|\,$ 15.1 & 1.24 & 16.79 & 35.5 \\
\end{tabular}
\label{tab:combined_peft_methods_memory}
\end{table}

\begin{table}[ht]
\caption{Number of trainable parameters, training times, and allocated VRAM during training for different PEFT methods. Here, Train Time refers to the time until the best epoch is reached during training. Training times and memory are reported for finetuning default SAM with ViT-L on LIVECell. The number of parameters is shown as \# Params w/o decoder $|$ \# Params with decoder.}
\centering
\begin{tabular}{l|r|r|r|r}
PEFT Method & \#Params [M] & Time / it [s] & Train Time [h] & Memory [GB] \\
\hline
Full FT & 312.3 $|\,$ 323.4 & 1.92 & 21.68 & 65.8 \\
LoRA & 7.2 $|\,$ 18.2 & 1.84 & 7.49 & 63.4 \\
QLoRA & 7.2 $|\,$ 18.2 & 2.06 & 7.46 & 63.3 \\
Freeze Encoder & 4.1 $|\,$ 15.1 & 1.37 & 18.55 & 36.3 \\
\end{tabular}
\label{tab:combined_peft_methods_memory_vit_l}
\end{table}

\begin{table}[ht]
\caption{Number of trainable parameters, training times, and allocated VRAM during training for different PEFT methods. Here, Train Time refers to the time until the best epoch is reached during training. Training times and memory are reported for finetuning default SAM with ViT-H on LIVECell. The number of parameters is shown as \# Params w/o decoder $|$ \# Params with decoder.}
\centering
\begin{tabular}{l|r|r|r|r}
PEFT Method & \#Params [M] & Time / it [s] & Train Time [h] & Memory [GB] \\
\hline
Full FT & 641.1 $|\,$ 652.1 & 2.20 & 27.86 & 76.9 \\
LoRA & 9.3 $|\,$ 20.3 & 2.21 & 5.98 & 71.2 \\
QLoRA & 9.3 $|\,$ 20.3 & 2.49 & 7.89 & 69.7 \\
Freeze Encoder & 4.1 $|\,$ 15.1 & 1.51 & 14.29 & 37.8 \\
\end{tabular}
\label{tab:combined_peft_methods_memory_vit_h}
\end{table}

\begin{figure}[ht]
\floatconts
  {fig:training_times}
  {\caption{Time until convergence (early stopping) when training $\mu$SAM and SAM on the microscopy datasets, including LIVECell, for the PEFT methods.}}
  {\includegraphics[width=\linewidth]{fig/training_times.png}}
  \label{fig:training_times}
\end{figure}

\begin{figure}[ht]
\floatconts
  {fig:medical_training_times}
  {\caption{Time until convergence (early stopping) when training MedicoSAM and SAM on the 6 medical datasets for the PEFT methods.}}
  {\includegraphics[width=\linewidth]{fig/medical_training_times.png}}
  \label{fig:medical_training_times}
\end{figure}


\FloatBarrier
\section{Resource-efficient Finetuning}

Fig.~\ref{fig:single_img_training} shows the results for training on a single annotated image (with another used for validation).
These results show that the performance of LoRA, when trained on a single image is highly dataset-dependent. % This supports the claim made in \cite{cellseg1} that the quality of annotations can outweigh the quantity of training data. 
For datasets like Platynereis \cite{Vergara:cell:2021:platynereis}, which feature sparse instances within a single image, single-image training shows some limitations.
Similarly, in the case of 3D datasets such as Platynereis, MitoLab, and GoNuclear, training on a single slice of one volume poses the additional challenge of selecting a suitable slice.
However, the performance achieved for MitoLab in this setting was notably strong. Here, when trained from the $\mu$SAM model, both LoRA and full fine-tuning achieved a mean segmentation accuracy of 0.54 when trained on a single image. Using all available images outperformed single-image training by only 6\% for full finetuning and 2\% for LoRA. 
As shown in Tab.~\ref{tab:memory_table}, LoRA consistently reduces memory usage by approximately 500 MB compared to full fine-tuning. Thus, in resource constrained training settings, LoRA can be a practical solution, potentially enabling training scenarios that would otherwise be infeasible.

\begin{table}[h!]
\caption{Allocated memory (in gigabytes) for each microscopy dataset during training, comparing full fine-tuning, LoRA, QLoRA and freezing the encoder when training from SAM and $\mu$SAM.}
\centering
\begin{tabular}{l|r|r|r|r|r|r}
Allocated Memory [GB] & \rotatebox{90}{\textbf{Covid-IF}} & \rotatebox{90}{\textbf{Platynereis}} & \rotatebox{90}{\textbf{MitoLab}} & \rotatebox{90}{\textbf{OrgaSegment}} & \rotatebox{90}{\textbf{GoNuclear}} & \rotatebox{90}{\textbf{HPA}} \\
\hline
\multicolumn{7}{l}{\textbf{SAM}} \\
\hline
Full FT         & 13.3 & 13.3 & 13.3 & 13.3 & 13.3 & 13.3 \\
LoRA            & 12.8 & 12.8 & 12.8 & 12.8 & 12.8 & 12.8 \\
QLoRA           & 12.5 & 12.5 & 12.5 & 12.5 & 12.5 & 12.5 \\
Freeze Encoder  & 5.8  & 5.8  & 5.2  & 5.8  & 5.8  & 5.8  \\
CellSeg1        & 7.0  & 7.7  & 8.4  & 11.3 & 9.6  & 8.0  \\
\hline
\multicolumn{7}{l}{\textbf{$\mu$SAM}} \\
\hline
Full FT         & 13.3 & 13.3 & 13.3 & 13.3 & 13.3 & 13.3 \\
LoRA            & 12.8 & 12.8 & 12.8 & 12.8 & 12.8 & 12.8 \\
QLoRA           & 12.6 & 12.5 & 12.6 & 12.5 & 12.6 & 12.5 \\
Freeze Encoder  & 5.8  & 5.8  & 5.8  & 5.8  & 5.8  & 5.8  \\
CellSeg1        & 7.0  & 7.7  & 8.3  & 11.3 & 9.7  & 8.0  \\
\end{tabular}
\label{tab:memory_table}
\end{table}

\begin{table}[h!]
\caption{Training time until convergence in minutes for each microscopy dataset, comparing full fine-tuning, LoRA, QLoRA and freezing the encoder when training from SAM and $\mu$SAM. For CellSeg1 the training time amounts to approximately 30 minutes, for 300 epochs, without early stopping.}
\centering
\begin{tabular}{l|r|r|r|r|r|r}

Train Time [min] & \rotatebox{90}{\textbf{Covid-IF}} & \rotatebox{90}{\textbf{Platynereis}} & \rotatebox{90}{\textbf{MitoLab}} & \rotatebox{90}{\textbf{OrgaSegment}} & \rotatebox{90}{\textbf{GoNuclear}} & \rotatebox{90}{\textbf{HPA}} \\
\hline
\multicolumn{7}{l}{\textbf{SAM}} \\
\hline
Full FT         & 3.10 & 11.32 & 18.90 & 16.63 & 7.50 & 3.18 \\
LoRA            & 18.42 & 10.97 & 13.65 & 19.61 & 10.59 & 12.55 \\
QLoRA           & 9.73 & 10.99 & 9.79 & 16.48 & 12.15 & 20.69 \\
Freeze Encoder  &  2.38 & 19.81  & 12.77  & 16.56  & 10.73  & 9.38  \\
\hline
\multicolumn{7}{l}{\textbf{$\mu$SAM}} \\
\hline
Full FT         & 5.41 & 8.33 & 2.35 & 1.18 & 5.57 & 1.04 \\
LoRA            & 5.34 & 18.48 & 3.39 & 5.55 & 2.26 & 3.11 \\
QLoRA           & 13.20 & 25.36 & 2.32 & 7.04 & 3.54 & 2.14 \\
Freeze Encoder  & 4.67  & 15.32  & 0.93  & 3.67  & 5.94  & 6.54  \\

\end{tabular}
\label{tab:single_img_train_time}
\end{table}

\begin{table}[h!]
\caption{Time per iteration in seconds for each microscopy dataset, comparing full fine-tuning, LoRA, QLoRA and freezing the encoder when training from SAM and $\mu$SAM.}
\centering
\begin{tabular}{l|r|r|r|r|r|r}

Train Time per Iteration [s] & \rotatebox{90}{\textbf{Covid-IF}} & \rotatebox{90}{\textbf{Platynereis}} & \rotatebox{90}{\textbf{MitoLab}} & \rotatebox{90}{\textbf{OrgaSegment}} & \rotatebox{90}{\textbf{GoNuclear}} & \rotatebox{90}{\textbf{HPA}} \\
\hline
\multicolumn{7}{l}{\textbf{SAM}} \\
\hline
Full FT         & 0.74 & 0.71 & 0.69 & 0.71 & 0.69 & 1.27 \\
LoRA            & 0.71 & 0.73 & 0.68 & 0.69 & 0.67 & 1.25 \\
QLoRA           & 0.73 & 0.73 & 0.73 & 0.73 & 0.69 & 1.31 \\
Freeze Encoder  & 0.57 & 0.55  & 0.55  & 0.54  & 0.56  & 1.13  \\
\hline
\multicolumn{7}{l}{\textbf{$\mu$SAM}} \\
\hline
Full FT         & 0.72 & 0.71 & 0.71 & 0.71 & 0.67 & 1.25 \\
LoRA            & 0.71 & 0.69 & 0.68 & 0.67 & 0.68 & 1.24 \\
QLoRA           & 0.75 & 0.74 & 0.70 & 0.70 & 0.71 & 1.28 \\
Freeze Encoder  & 0.56  & 0.54  & 0.56  & 0.55  & 0.55  & 1.12  \\

\end{tabular}
\label{tab:single_img_time_per_it}
\end{table}

\FloatBarrier
\section{Ablation Study} \label{sec:ablation}

\subsection{LoRA}

\begin{figure}[htbp]
\floatconts
  {fig:lora_1}
  {\caption{Inference results on OrgaSegment for LoRA with different combinations in $\alpha$ and rank. Results are shown in mean segmentation accuracy and evaluated across different tasks.}}
  {\includegraphics[width=0.9\linewidth]{fig/lora_1.png}}
\end{figure}
\begin{figure}[htbp]
\floatconts
  {fig:lora_2}
  {\caption{Inference results on OrgaSegment for LoRA with different learning rates and scaling factors $\alpha$. The colorbars represent mean segmentation accuracy.}}
  {\includegraphics[width=0.7\linewidth]{fig/lora_2.png}}
\end{figure}

\begin{figure}[htbp]
\floatconts
  {fig:lora_3}
  {\caption{Inference results for LoRA with different scaling factors $\alpha$ on four datasets. Circles highlight he best results per dataset and task}}
  {\includegraphics[width=0.9\linewidth]{fig/lora_3.png}}
\end{figure}

In this section, we conduct a series of experiments to investigate the influence of two key hyperparameters in LoRA: the rank and the scaling factor $\alpha$, as described in Section \ref{sec:peft}. Specifically, we aim to understand how these parameters impact segmentation accuracy and model performance.

\cite{hu:2021:lora} suggest that the scaling factor $\alpha$ behaves similarly to the learning rate, with both influencing the optimization process and model convergence. We explore this relationship through a series of experiments that vary  $\alpha$ and rank in conjunction with the learning rate.

Figure \ref{fig:lora_1} presents the impact of rank on segmentation accuracy for various values of $\alpha$. Intuitively, one might expect that increasing the rank would lead to improved accuracy, as it expands the parameter space. However, our experiments show that rank has only a marginal influence on iterative prompting. However, there is a noticeable improvement in the AIS and single point metrics as rank increases. Based on these findings, we choose a rank of 32 for all other experiments that use LoRA.

Next, we investigate the relationship between the scaling factor $\alpha$ and the learning rate. Figure \ref{fig:lora_2} illustrates that smaller learning rates tend to yield better performance across various values of  $\alpha$. From this experiment, we select a learning rate of $ 1 \times 10^{-5} $ for all further experiments, as it provides the best trade-off between convergence and accuracy.

Our findings also suggest that using both a high learning rate and a large $\alpha$ simultaneously is detrimental to performance, potentially leading to instability in optimization. However, the precise influence of  $\alpha$ remains ambiguous, as its impact is not as pronounced as that of the learning rate.

To further investigate the role of $\alpha$, we run experiments on four different datasets, as shown in Figure \ref{fig:lora_3}. The optimal value for $\alpha$ varies across datasets, and no consistent pattern emerges. However, we observe that using $\alpha = 1$ does not harm performance in any case, and we recommend using this default value for practical applications. In general, the scaling factor $\alpha$ appears to have a limited impact on the model's performance, and tuning it is not always necessary for achieving good results.
We conclude: 
\begin{itemize}
    \item Rank has a marginal effect on iterative prompting but improves AIS and weak prompt performance, so we opt for rank 32.
    \item Learning rate is a more critical factor, with smaller learning rates being more favorable. A high learning rate and high  $\alpha$ should be avoided.
    \item The optimal $\alpha$ value is dataset-dependent, but setting  $\alpha = 1$ is a safe and effective choice across all datasets.
\end{itemize}

\subsection{AdaptFormer}
\begin{figure}[htbp]
 % conceptual image should go here
\floatconts
  {fig:adaptformer_1}
  {\caption{Inference results on OrgaSegment for different projection sizes and scaling factors and dropout values for AdaptFormer.}}
  {\includegraphics[width=0.8\linewidth]{fig/adaptformer_1.png}}
\end{figure}

\begin{figure}[htbp]
 % conceptual image should go here
\floatconts
  {fig:adaptformer_2}
  {\caption{AdaptFormer inference results, trained on OrgaSegment for different scaling factors and projection sizes. The results are averaged over different dropout factor experiments.}}
  {\includegraphics[width=0.8\linewidth]{fig/adaptformer_2.png}}
\end{figure}

To find the best hyperparameters for AdaptFormer a grid-search was run on 3 parameters. The scaling factor, which scales the output of the Adapter module and therefore, similarly to LoRA, balances the impact of the features from the frozen branch with the task-specific features from the tunable parameters. Secondly, we try different projection sizes, meaning the middle dimension of the AdaptFormer module, which controls the number of learnable parameters that are introduced by this finetuning method. Lastly we introduce an optional dropout layer, between the down and up projection of the AdaptFormer branch. 

Fig.~\ref{fig:adaptformer_1} indicates that the dropout factor does not exhibit a consistent pattern in its impact on the inference results. Considering this lack of clarity and the additional stochasticity introduced by dropout during inference, we recommend setting the dropout to None by default, effectively excluding the dropout layer altogether. 
Assuming that the dropout value does not significantly influence the results, we average across the different dropout settings to achieve a more stable analysis of the alpha values and projection sizes. This is visualized in a heatmap for various inference tasks in figure \ref{fig:adaptformer_2}. The results suggest a slight preference for larger projection sizes. However, the performance margin is minimal. Thus, we recommend a smaller projection size of 64 to reduce the number of parameters, balancing performance and complexity.

For the alpha parameter, the best results are observed when it is either learned by the model or set to one. Notably, the alpha values learned by the model are consistently close to one. To maintain flexibility for diverse datasets, we suggest using alpha as a learnable parameter. However, if reducing the number of parameters is a priority, setting alpha to 1 is a suitable alternative. 

\subsection{FacT}

\begin{table}[ht]
\centering
\caption{Mean segmentation accuracy for dropout factor and rank parameter search for FacT on LIVECell. }
\begin{tabular}{l|r|r|r|r|r}
 & ais & ip & ib & point & box \\
\hline
\multicolumn{6}{l}{\textbf{Dropout Factor (Rank = 4)}} \\
\hline
0.25 & 0.386388 & 0.783651 & 0.828160 & 0.408460 & 0.646480 \\
None & 0.383379& 0.779622 & 0.822782 & 0.400679 & 0.64233 \\
0.1  & \textbf{0.389440} & \textbf{0.787556} & \textbf{0.830561} & \textbf{0.413252} & \textbf{0.649592} \\
0.5  & 0.385142  & 0.779190  & 0.821817 & 0.397363 & 0.640852 \\
\hline
\multicolumn{6}{l}{\textbf{Rank (dropout=0.1)}} \\
\hline
1   & 0.379699  & 0.774329 & 0.822471 & 0.382696 & 0.630324 \\
2   & 0.383812 & 0.775398 & 0.822041 & 0.387330  & 0.638050  \\
4   & 0.389440   & 0.787556 & 0.830561 & 0.413252 & 0.649592 \\
8   & 0.393855 & 0.783390  & 0.826328 & 0.415375 & 0.648752 \\
16  & \textbf{0.398600}    & \textbf{0.792422} & \textbf{0.833112} & 0.426964 & 0.655786 \\
32  & 0.397353 & 0.782903 & 0.831328 & \textbf{0.431270}  & \textbf{0.657162} \\
\end{tabular}

\label{tab:combined_study}
\end{table}

To analyze the impact of hyperparameter tuning in the FacT parameter-efficient fine-tuning method, we conducted an extensive search over two parameters: dropout factor and rank, using the LIVECell dataset. The dropout factor controls an optional dropout layer in the forward function of the FacT methods. We tested dropout factors of 0.1. 0.25, 0.5 and no dropout, finding that a dropout factor of 0.1 achieved the best overall performance. It led to consistent improvements across all tasks. For rank, we observed that increasing its value improved performance up to a rank of 16, which achieved the highest results across most tasks. Further increasing the rank to 32 provided only marginal improvements for weak prompts, while for AIS and iterative prompting, rank 16 remain superior. We therefore recommend using rank 16 for this method. 

\end{document}