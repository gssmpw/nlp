% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\usepackage{xspace}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{comment}
\usepackage{amsmath}

\newcommand{\model}[0]{\textsc{SentenceSmith}\xspace}

\newcommand{\edge}[3]{\texttt{<#1,#2,#3>}}
\newcommand{\embedder}[1]{\texttt{#1}}
\newcommand{\insertX}[0]{\textcolor{red}{\textbf{Insert}\xspace}}
\newcommand{\insertC}[1]{\textcolor{red}{\textbf{[JO: #1]}\xspace}}

%\title{Fine-grained Evaluation of Text Embeddings with the Sentence Smith}
\title{Sentence Smith: Formally Controllable Text Transformation and its Application to Evaluation of Text Embedding Models}

% Author information can be set in various styles:
% For several authors from the same institution:
 \author{Hongji Li ~~~ Andrianos Michail ~~~ Reto Gubelmann ~~~ Simon Clematide ~~~ Juri Opitz
 \\
         University of Zurich \\
         \texttt{\{hongji.li,andrianos.michail,reto.gubelmann,simon.clematide\}@uzh.ch}
         \\ \texttt{opitz.sci@gmail.com}}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\}

%\author{
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
We propose the \model framework that enables controlled and specified manipulation of text meaning. It consists of three main steps: 1.\ Parsing a sentence into a semantic graph, 2.\ Applying human-designed semantic manipulation rules, and 3.\ Generating text from the manipulated graph. A final filtering step (4.) ensures the validity of the applied transformation. To demonstrate the utility of \model in an application study, we use it to generate hard negative pairs that challenge text embedding models. Since the controllable generation makes it possible to clearly isolate different types of semantic shifts, we can gain deeper insights into the specific strengths and weaknesses of widely used text embedding models, also addressing an issue in current benchmarking where linguistic phenomena remain opaque. Human validation confirms that the generations produced by \model are highly accurate.  
\end{abstract}

\section{Introduction}

Transforming a sentence’s meaning through computational methods is a challenging task with numerous applications, including style transfer, data augmentation, and controlled text generation. Large language models (LLMs) offer a promising approach via prompting but lack fine-grained control \citep{greenblatt2024ai, 10.1162/tacl_a_00681}. Conversely, symbolic methods, e.g., word replacement using taxonomy lookups \citep{bolshakov2004synonymous, huang2009using}, provide control but often produce unnatural sentences with limited variation.

\begin{figure}[ht]
\centering
\includegraphics[width=1.0\linewidth]{img/srl-crop.pdf}
\caption{Example application of two controlled text transformations. \textbf{Left}: Switching semantic roles in a sentence, where the agent (ARG0: the biter) becomes the patient (ARG1: the bitten), and vice versa. \textbf{Right}: Deleting the patient and negating the main predicate.}
\label{fig:srl-switch}
\end{figure}

To bridge the gap between these paradigms, we propose \model, a neuro-symbolic framework for text manipulation. The process begins with a parser that maps a sentence onto a symbolic graph-based meaning representation, such as an Abstract Meaning Representation (AMR) \citep{banarescu2013abstract}. We posit that such a representation provides an effective interface for applying well-defined, precise, and targeted meaning-altering operations. Once the graph is updated, a generator converts it back into natural text. 

This process is illustrated in Figure \ref{fig:srl-switch}, where a single sentence is transformed into two new sentences with distinct meanings. On the left, we reverse semantic roles (e.g., \textit{tiger}, \textit{snake}), modifying the event structure of the sentence by swapping patient and agent roles. On the right, we apply a two-step process: removing the patient (\textit{tiger}) and negating the main predicate (\textit{bite-01}), resulting in a new sentence that describes a “harmless” snake (the ``\textit{-01}'' in \textit{bite-01} indicates a specific sense in PropBank \citep{palmer-etal-2005-proposition}). We also introduce an optional faithfulness check as a post-processing step to evaluate the consistency between the transformed graph and the generated sentence, filtering out outputs affected by parsing or generation errors.

We argue that this combination of symbolic control, neural fluency, and post-hoc verification provides a powerful and flexible approach to sentence meaning transformation, offering a more controllable alternative to prompting LLMs. 

To illustrate the practical value of \model, we conduct a detailed demonstration study, leveraging our framework to generate hard negative pairs that pose significant challenges for text embedding models. By applying well-defined transformation rules targeting specific semantic phenomena, \model creates nuanced sentence pairs that go beyond superficial lexical differences, enabling a more fine-grained evaluation of embedding models. Through this application study, we shed light on their strengths and weaknesses in handling particular linguistic phenomena, addressing a limitation in current benchmarking practices, where such intricacies are often obscured.

Broadly speaking, this study contributes to the ongoing effort to pinpoint the limits of Transformer-Based Neural Networks' understanding of sentence meaning. It has been established that even the latest State-of-the-Art LLMs struggle with important sentence-level meaning phenomena such as negation \citep{parmarLogicBenchSystematicEvaluation2024}, and \citet{xu2025large} finds that LLMs still struggle with exact, syntax-based modes of reasoning, sometimes being outperformed by BART \citep{lewis-etal-2020-bart}, an early sequence-to-sequence transformer. The fine-grained and dynamic experimental settings enabled by \model shed new light on this central, but still open question regarding the abilities of Transformers.

The remainder of this paper is structured as follows: Section \S \ref{sec:rw} reviews background and related work. Section \S \ref{sec:ss} introduces our neuro-symbolic \model framework, detailing its components, including the parser, symbolic graph transformations, generator, and optional faithfulness checker. Section \S \ref{sec:app} presents an application demonstration, focusing on generating hard negative pairs to challenge state-of-the-art text embedding models and reveal potential weaknesses in their linguistic understanding. Finally, Section \S \ref{sec:concl} concludes the paper, discussing the broader implications of our work and potential directions for future research. We release code and data under a public license.\footnote{\url{https://github.com/1160300921/SentenceSmith}}





\section{Related Work and Background}
\label{sec:rw}

\paragraph{Method: Semantic graph as an intermediate representation.} The value of using semantic graphs as an intermediate representation, particularly AMR, has been highlighted in two recent surveys \citep{sadeddine-etal-2024-survey, wein-opitz-2024-survey}. This approach allows the fusion of neural network power with the expressivity and explicitness of meaning representations, which is especially useful when \textit{interpretability} and \textit{control} are required in an application. Related work applies this principle to style transfer \citep{jangra-etal-2022-star}, translationese reduction\footnote{Translationese reduction aims at attempting to reduce ``language weirdness'' that can arise when translating texts, the authors show that post-processing with parsing and generation can mitigate these artifacts.} \citep{wein-schneider-2024-lost}, and data augmentation \citep{shou-etal-2022-amr, shou-lin-2023-evaluate, ghosh-etal-2024-abex}. Our work generalizes this principle further and imposes an additional check for validating the faithfulness of the generation. Notably, the idea of planning and controlling sentence generation through meaning representation dates decades back \citep[i.a.,][]{sondheimer-nebel-1986-logical, mann1986systemic, kasper-1989-flexible, wijnen1990development, bateman1990interfacing}, but was limited by inaccuracies parsing and generation systems. With stronger parsing and generation systems now at hand, we showcase the usefulness of this way of controllable text generation.

\paragraph{Application: Embedding models and benchmarking.} Text embedding models are crucial for a wide range of NLP tasks, including semantic search, information retrieval, and NLG evaluation \citep{clark2019sentence, muennighoff2022sgpt, gao2023retrieval}. Since \citet{reimers-gurevych-2019-sentence}'s foundational "SBERT" work, multiple branches of embedding model research have emerged. These include enhancing model performance through scaling parameters \citep{wang2023improving} or training data \citep{wang2022text}, as well as exploring unsupervised embeddings \citep{gao-etal-2021-simcse} and interpretable embeddings \citep{opitz-frank-2022-sbert}. However, key questions remain: \textit{What is the accuracy of such embeddings? What level of linguistic understanding resides in those vectors?} Towards being better able to answer such questions, we employ \model and demonstrate its capacity to generate fine evaluation data that test embedding models' ability to assess different linguistic phenomena. In posing such questions towards better understanding model's similariy decisions through assessing behavior on tailored datasets, our goal is related to a recent/concurrent line of work from Nastase et al. \citep{nastase-merlo-2024-tracking, nastase2024exploring, nastase2024exploringsy}.

Then, we would also like to learn more about different notions of similarity: Even for long-established datasets like SICK \citep{marelli-etal-2014-sick} or STS \citep{agirre-etal-2016-semeval}, it remains unclear what specific aspects human annotations elicit. For instance, \citet{budanitsky-hirst-2006-evaluating} point out that while relatedness (SICK) and similarity (STS) are somewhat related notions, similarity is ``not an adequate proxy (for relatedness).'' Recent empirical research investigates sentence meaning through descriptions, assessing the similarity of texts through these descriptions \citep{hoyle-etal-2023-natural, ravfogel2024descriptionbased}. Likewise, paraphrases vary widely and are often difficult to formally distinguish \citep{michail-etal-2025-paraphrasus}. In our work, we formalize relatedness as a confounder of similarity by inducing pairs with varying degree of \textit{structural surface similarity}, while their potential \textit{meaning differences are formally distinguishable}. Furthermore, the stasis of most benchmarks has also drawn upon criticism \citep{fan-etal-2024-nphardeval}, and limits evaluation to the available data, with potential ramifications for the trustworthiness of results. By demonstrating how \model can generate challenging, trustworthy, and interpretable test sets, we pave the way for more customizable, dynamic and interpretable testing of models.





\section{The \model Framework}
\label{sec:ss}

\subsection{Model Overview}

The formal description of \model is straightforward. Given $s$ as an input sentence, a parser $p$ and a generator $g$, \model conducts a controlled transformation, resulting in a changed sentence $s'$, that is:
\begin{equation}
    s' =g \circ t_n \circ ... \circ t_1 \circ p ~|~ s,
\end{equation}
where $\{t_0, ... t_n\}$ are graph transformations that we apply on the graph representation of the input sentence $s$, output of $p$. Finally, $g$ is used to generate a text $s'$ from the manipulated graph.

\subsection{Parameterization}

\paragraph{Graph model.} We rely on Abstract Meaning Representation \citep[AMR,][]{banarescu2013abstract} as our graph framework. AMR represents text as directed acyclic graphs, where nodes denote entities and edges capture semantic relations. The overarching goal of AMR is to explicitly encode ``Who does what to whom?''. This explicitness is a key motivation for our work, as it enables targeted meaning changes within the semantic structure.

\paragraph{Parsing and generation.} As is standard in most AMR applications, we employ off-the-shelf parsing and generation models.\footnote{\url{https://github.com/bjascob/amrlib.}} These models, based on pre-trained BART \citep{lewis2019bart}, produce linearized sequence graphs from text (parsing) or text from linearized sequence graphs (generation). To facilitate manipulation, we convert the linearized graph into triples of the form \edge{u}{r}{v}, where \texttt{u} and \texttt{v} are graph nodes, and \texttt{r} is a relation. 

\paragraph{Graph transformation.} This is the interface where a human (or other controller) can influence the machine-based sentence transformation. Beyond various graph operations, we may wish to, for example, modify truth values by adding a negation to a select predicate, swap semantic roles, or add new information in a controlled manner. Since different use cases of \model may require distinct transformations, the specific rules applied in our demonstration study are detailed later in \S \ref{sec:app}. 

\paragraph{Validation.} Once a transformed sentence is generated, we would like to validate whether the intended transformation was successfully executed. Potential failure cases include noise introduced by the parsing and generation process. We employ a faithfulness check function $check(s, s') \in \{-1, 0, 1\}$, where $-1$ denotes contradiction between $s$ and $s'$, $0$ implies a neutral relation, and $1$ indicates that $s'$ is entailed by $s$. 

The criteria for discarding output sentences depend on the application: If the goal is meaning alteration, we should filter out cases where $s$ and $s'$ mutually entail each other. Conversely, if we seek paraphrases, we should discard cases where contradiction or neutrality is detected. \model parameterizes $check$ with an efficient NLI-based model from \citet{steen-etal-2023-little}. This system is robustness-enhanced through data augmentation and achieves strong results on the TRUE faithfulness benchmark \citep{honovich-etal-2022-true-evaluating}.\footnote{We use \url{https://huggingface.co/juliussteen/DeBERTa-v3-FaithAug}.}





\section{Application Study}
\label{sec:app}

As an important application to demonstrate the usefulness of \model, we focus on fine-grained linguistic testing of embedding models. 

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/data_creation-crop.pdf}
\caption{Breaking the paraphrase relation and creating a challenging test case: The new sentence has a high surface similarity to the input, but it is not a paraphrase. By contrast, the actual paraphrases have a lower superfifical overlap, thus posing a challenge to any model, especially those that are merely driven by surface cues.}
\label{fig:foilgen}
\end{figure}



\paragraph{Foil idea.} The foil concept is illustrated in Figure \ref{fig:foilgen}. The objective is to generate a new sentence that is structurally highly similar to an existing one but semantically distinct. If this transformation is applied to one part of a paraphrase pair, the resulting sentence will not maintain a paraphrase relation with either of the original sentences. In the example, we start with the paraphrases \textit{a) A tiger is being bitten by a serpent.} and its paraphrastic variant \textit{b) The snake bites the tiger.} Our parser processes sentence b), and within the generated meaning space, we perform a \textit{semantic role confusion}, swapping the agent and patient of the \textit{bite} event. Finally, our generator produces \textit{c) The tiger bites the snake}, which is structurally more similar to a) than a) and b) are to each other (i.e., higher lexical overlap). However, the event’s semantics have \textit{fundamentally} changed, thereby breaking the paraphrase relation.



\subsection{Setup}

\subsubsection{Defining Semantic Manipulations}

We define five semantic manipulations designed to generate foils from a given sentence: Polarity Negation, Role Swap, Underspecification, Antonym Replacement, and Hypernym Substitution. Examples of all five manipulation types are displayed in Figure \ref{fig:five-rules}. Below, we present a more detailed description of each manipulation.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{img/five_rules-crop.pdf}
    \caption{Five aspectual semantic manipulations.}
    \label{fig:five-rules}
\end{figure}

\paragraph{1.\ Polarity Negation (PN).} This fundamental manipulation assesses embedding models' sensitivity to negation. We define negation as altering the polarity of a predicate within an AMR graph. Specifically, this transformation attaches a \edge{n}{:polarity}{-} edge (in AMR notation, a negation) to a randomly selected node \texttt{n}, excluding pronouns. For example, modifying the predicate \textit{approve} by adding this edge results in its opposite, \textit{disapprove} (i.e., in AMR: \edge{n}{:polarity}{-} is added to node \texttt{n}, where \edge{n}{:instance}{approve} already exists). The manipulated AMR is then used to generate the negated text. Also in the example of Figure \ref{fig:five-rules}, the truth value of the sentence changes.

\paragraph{2.\ Role Swap (RS)} evaluates how well embedding models distinguish fine-grained semantic relations. It randomly swaps two graph nodes \texttt{u} and \texttt{v}, generalizing the semantic role switch illustrated in Figure \ref{fig:srl-switch} and Figure \ref{fig:five-rules}, where \texttt{s} (instance of snake) and \texttt{t} (instance of tiger) were swapped. 

\paragraph{3.\ Underspecification (US).} This transformation removes a randomly selected leaf node \texttt{n} from the AMR graph, reducing semantic content, thereby breaking a paraphrase relation. The introduced ambiguity or incompleteness  tests how robust embedding models are to partial information.\footnote{Note that it can also introduce a polarity change, in case a negation node in the AMR is removed. The notion of ``Underspecification'' refers to the AMR structure being reduced.} In Figure \ref{fig:five-rules}, the patient specification (a snake) has disappeared. 

\paragraph{4.\ Antonym Replacement (AR)} inverts the meaning of a selected node while maintaining the context of the surrounding graph. The system identifies a random non-pronoun node, extracts its lexical stem (e.g., happy in happy-01), and queries WordNet \citep{miller1995wordnet} for antonyms. If a suitable antonym is found, the node label is updated while preserving any sense-suffix (e.g., -01). This transformation alters meaning more explicitly than polarity negation by directly substituting an opposite term. In Figure \ref{fig:five-rules}, same as in PN, the truth value changes ---while still a \textit{biting} is happening, this time the agent is a small rather than big. 

\paragraph{5.\ Hypernym Substition (HS).} This transformation replaces a word with its hypernym, distorting inferences and breaking paraphrase relationships. Consider ``Penguins can’t fly''. If Penguin is replaced with its hypernym ``Bird'', the sentence becomes nonsensical (``Birds can’t fly''), demonstrating how hypernym substitution alters meaning while preserving surface structure. Following the same WordNet-based approach as in the  antonym replacement, non-pronoun node labels are substituted with randomly selected hypernyms. The node’s graph position and semantic roles remain unchanged, but abstraction increases, disrupting logical inferences. In Figure \ref{fig:five-rules}, we observe how snake is replaced with animal. This is also some form of underspecification, but speaking through the AMR, it is achieved by replacing (not removing) a concept (which can also trigger a changed truth value, as in the aforementioned bird-example).

\subsubsection{Discussion}

While introducing more control in changing text meaning, it is likely that our manipulations not always yield a desired outcome. Clearly, we want to prevent the generation of a paraphrase, or near-paraphrase. We do not expect that this happens often, since all manipulations are bound to change the meaning in some way or another; and even parsing or generation errors are much more likely to generate something that is not a paraphrase of the input than otherwise. But generally, there may be several less desirable outcomes. One of these would be the generation of weird sentence meanings (e.g., swapping the roles of \textit{horse} and \textit{leaves} in \textit{the horse eats leaves} would yield such a weird meaning). Another less desirable outcome may be very similar sentence meanings (e.g., if a hypernym is semantically very close to its hyponym). While an argument can be made for the former that these weird sentences are still fully valid foils for embedding models (weird meaning or not, the paraphrase relation is broken), the latter may be the least desirable type (since they may be close to actual paraphrases). This is where our entailment check proves valuable, since it allows us to fish out those foils that we can trust in the most (e.g., the contradictory ones).

To assess these and more possible difficulties in more detail, we will conduct a human evaluation of the generated foils next in \S \ref{ssec:humaneval}, discuss a full running example for a full and noisy processing in Appendix \ref{app:running_example}, Figure \ref{fig:manipulations}, and show examples of our final dataset in Appendix \ref{app:examples}, Table \ref{tab:foilexamples}.

\subsubsection{Data Instantiation}

\paragraph{Initial datasets.} We use two paraphrase datasets as our base data. The first is PAWS \citep{yang-etal-2019-paws}\footnote{\url{https://huggingface.co/datasets/google-research-datasets/paws-x/tree/main}}, from which we take the positive pairs. While PAWS provides structurally similar paraphrases—originally designed to challenge paraphrase detection models with adversarial positive and negative pairs—it is not fully ideal for our purpose. Due to the high structural similarity, we anticipate slightly lower difficulty in the challenge pairs generated from this dataset.

To obtain a more comprehensive benchmark, we additionally leverage a more diverse set of ChatGPT-generated paraphrases \citep{chatgpt_paraphrases_dataset}\footnote{\url{https://huggingface.co/datasets/humarin/chatgpt-paraphrases}}, henceforth denoted as GPTP. This dataset consists solely of positive paraphrase pairs. Also unlike PAWS, GPTP exhibits greater structural variability, making our resulting challenge set inherently more difficult. \label{par:statvalidationdifference} We validate this statistically by computing bag-of-words similarity between paraphrase pairs in each dataset: PAWS yields an average similarity of 0.9, whereas GPTP has a much lower average similarity of 0.4 (see Appendix \ref{ssec:bowdivergence} for details). 

\paragraph{Data induction and filtering.} After employing our parser $parse$, a graph transformation $manip$, and the generator $gen$ on every pair, we end up with triples $(s, p, f)$, where $s$ is the original sentence, $p$ a paraphrase, and $f=gen(manip(parse(s)))$ our generated foil (structurally highly similar to $s$, but of another meaning). For post-processing, to maximize the integrity of the generated benchmark, we apply a harsh quality filtering process through our NLI-based validation module: We only retain those ``contradiction'' sentence pairs with a semantic relationship confidence score greater than 90\%. This ensures that the final dataset focuses on maximally validated paraphrase foils. However, we will later also experiment with a different filtering criterion. Final dataset statistics, including distribution of transformations, is shown in Table \ref{tab:datastats}. 

\begin{table}
    \centering
    \adjustbox{width=\linewidth}{\begin{tabular}{lrrrrrr}
    \toprule
         & total & PN \% & RS \% & US \% & AR \% & HS \% \\
         \midrule
       PAWS & 1,737 & 39.5  & 18.5  & 9.7  & 27.2  & 5.0  \\
       GPTP & 11,456 & 36.5  & 15.3  & 9.6  & 30.4  & 8.2  \\
       \bottomrule
    \end{tabular}}
    \caption{Final data set statistics.}
    \label{tab:datastats}
\end{table}

\paragraph{Human evaluation of generation.}\label{ssec:humaneval} We conduct a human annotation to assess the quality of our generations. Specifically, we are interested in two variables. First, \textit{Does the meaning of sentence B differ from that of sentence A (no, or yes)?} This lets us assess how faithfully the final output of \model has achieved our main goal: The manipulation of text meaning. Second, we wonder about the fluency of the final generation: \textit{Given sentence A, is the fluency of sentence B worse, about the same, or improved?} We task two annotators to annotate 100 randomly sampled pairs from each of PAWS and GPTP, so 200 in total. 

Table \ref{tab:humananno} presents the results. Our primary goal—meaning manipulation—is achieved to an overwhelming degree, with only one or two exceptions across both samples. While fluency tends to degrade on average, a notable portion of outputs remain equally fluent or even improve in fluency.

\begin{table}[]
    \centering
   \adjustbox{width=\linewidth}{ \begin{tabular}{lrr|rrrrrr}
   %\cmidrule{2-9}
         & \multicolumn{2}{c|}{Meaning}  & \multicolumn{6}{c}{Fluency}\\
         & \multicolumn{2}{c|}{accuracy}    & \multicolumn{2}{c}{worse} & \multicolumn{2}{c}{same} & \multicolumn{2}{c}{better} \\
             \cmidrule{2-9}
             & A1 & A2 & A1 & A2 & A1 & A2 & A1 & A2 \\
        \midrule
        PAWS & 98 & 99& 34 & 41 &  30 & 33 &  36 & 26 \\
        GPTP & 100 & 99 & 42 & 45 & 35 & 37 & 23 & 18   \\
        \bottomrule
    \end{tabular}}
    \caption{Manual assessment of the generation quality. All numbers are \%s. A1 and A2 are human annotators.}
    \label{tab:humananno}
\end{table}

Examples of our generated foils are shown in Appendix \ref{app:examples}, Table \ref{tab:foilexamples}. E.g., through adding a negative polarity to the node ``need'', the statement \textit{But you need to get somebody like Warren to do it} becomes \textit{But you don't need to get somebody like Warren to do it}, which is superficially highly similar, but has the opposite meaning of the original paraphrase \textit{You should find someone similar to Warren to handle it} that is structurally much more different on the surface.

\subsubsection{Evaluation Measures} 

Given any text embedding model $\mathcal{E}$ that maps a pair of texts to a real-valued similarity score, we compute two key evaluation metrics. For \textbf{triplet accuracy} $(TACC)$, we consider a set with triplets $T = \{(s_i, p_i, f_i)\}_{i=1}^n$, where $s_i$ is a text, $p_i$ its paraphrase, and $f_i$ our generated foil that differs from $s$ in meaning (but not as much in structure). Formally, we then define triplet accuracy as
\begin{align*}
    TACC = \frac{1}{|T|}\sum_{(s, p, f) \in T}\mathcal{I}[\mathcal{E}(s,p) > \mathcal{E}(s, f)],
\end{align*}

\noindent where $\mathcal{I}[c]$ returns 1 if condition $c$ is true, and 0 else. As a more standard metric, we also measure the \textbf{Area Under the Receiver Operator Curve} ($AUC$) that represents a softer score directly based on the real valued output scores of embedding models. For this, we dichotomize the dataset into positive pairs $\{(s_i, p_i)\}_{i=1}^n$ and negative pairs $\{(s_i, f_i)\}_{i=1}^n$, essentially inducing a binary paraphrase classification task. Since, it is hard to say which of the two metrics is more apt for selecting models, models should excel in both and thus in our main results we show averages \citep[Rec.\ 3 in Section 8]{10.1162/tacl_a_00675} using an harmonic mean. 

\subsection{Evaluating Embedding Models}


\subsubsection{Embedding Model Selection}

Now with our benchmark fully set up, we are ready to evaluate an off-the shelf text embedding model. We select a range of models that have a good balance of performance and efficiency, with some of them ranking among the top models at the time of writing this paper, and others well known. 

Specifically, we evaluate 29 embedding models on our novel datasets, among them quite popular ones like LaBSE \cite{feng-etal-2022-languagelabse}, SBERT \citep{reimers-gurevych-2019-sentence}, Jina \citep{gunther-etal-2023-jina} and E5 \citep{wang2022texte5}, others are picked from the MTEB public leaderboard\footnote{\url{https://huggingface.co/spaces/mteb/leaderboard}} according to a performance-efficiency balance.

It is important to note that names like ``SBERT'' or ``E5'' represent techniques rather than specific model instances. Therefore, from here on, we reference specific models by their Hugging Face identifiers. For instance, \embedder{all-mpnet-base-v2} refers to a widely used embedding model that applies the SBERT training methodology to a self-supervised pre-trained MPNET model \citep{NEURIPS2020_c3a690bempnet}.

\subsection{Results}
\begin{table}[ht]
    \centering
    \adjustbox{width=\linewidth}{%
    \begin{tabular}{lrr|r}
        \toprule
        \textbf{Model Name} & \textbf{PAWS} & \textbf{GPTP} & \textbf{AVG} \\
        \midrule
\embedder{sentence-t5-large} & 0.9506 & 0.8026 & 0.8704 \\
\embedder{ember-v1} & 0.9562 & 0.7513 & 0.8415 \\
\embedder{bge-base-en-v1.5} & 0.9506 & 0.6910 & 0.8003 \\
\embedder{e5-base-v2} & 0.9378 & 0.6930 & 0.7970 \\
\embedder{GIST-Embedding-v0} & 0.9297 & 0.6954 & 0.7957 \\
\embedder{FAB-Ramy-v1} & 0.9190 & 0.6928 & 0.7900 \\
\embedder{gte-base-en-v1.5} & 0.9062 & 0.6947 & 0.7865 \\
\embedder{all-mpnet-base-v2} & 0.9046 & 0.6920 & 0.7841 \\
\embedder{instructor-base} & 0.9242 & 0.6503 & 0.7634 \\
\embedder{paraphrase-MiniLM-L12-v2} & 0.9509 & 0.6286 & 0.7569 \\
\embedder{nomic-embed-text-v1.5} & 0.8896 & 0.6519 & 0.7524 \\
\embedder{jina-embeddings-v2-base-en} & 0.9314 & 0.5965 & 0.7272 \\
\embedder{MedEmbed-small-v0.1} & 0.9237 & 0.5980 & 0.7260 \\
\embedder{stella-base-en-v2} & 0.9508 & 0.5724 & 0.7146 \\
\embedder{Wartortle} & 0.9556 & 0.5639 & 0.7093 \\
\embedder{LaBSE} & 0.9657 & 0.5444 & 0.6963 \\
\embedder{all-MiniLM-L12-v2} & 0.9234 & 0.5586 & 0.6961 \\
\embedder{gtr-t5-large} & 0.8501 & 0.5762 & 0.6869 \\
\embedder{cde-small-v1} & 0.8507 & 0.5653 & 0.6792 \\
\embedder{gte-micro} & 0.9320 & 0.5315 & 0.6769 \\
\embedder{msmarco-bert-co-condensor} & 0.8724 & 0.5474 & 0.6727 \\
\embedder{contriever-base-msmarco} & 0.8880 & 0.5214 & 0.6570 \\
\embedder{snowflake} & 0.8852 & 0.5104 & 0.6475 \\
\embedder{Ivysaur} & 0.9194 & 0.4889 & 0.6384 \\
\embedder{Venusaur} & 0.9323 & 0.4459 & 0.6033 \\
\embedder{distiluse-base-v2} & 0.9475 & 0.4393 & 0.6003 \\
\embedder{allenai-specter} & 0.8143 & 0.3908 & 0.5281 \\
\embedder{SGPT-125M} & 0.7491 & 0.3982 & 0.5200 \\
\embedder{komninos} & 0.8905 & 0.3431 & 0.4953 \\
        \bottomrule
    \end{tabular}}
    \caption{Main results. Models are ranked according average performance on our two data sets.}
    \label{tab:mainres}
\end{table}


Table \ref{tab:mainres} shows the main result. The top ranked model is \embedder{sentence-t5-large}, showing an AVG score of 0.87, outperforming the worst ranked model \embedder{SGPT-125M} by about 30 percentage points (pp.). On PAWS, several models show strong performance of more than 0.90. This is likely due to the higher initial structural similarity of the paraphrases (for statistics see earlier in \S \ref{app:otherstats}), reducing the effectivness of some foils. On GPTP, with its more varied structural differences, the differences between models grows, and even the best performing model only barely exceeds an  AVG of 0.8. For individual metrics on the two datasets, we refer to Appendix \ref{app:otherstats}: Table \ref{tab:gpt} shows evaluation on GPTP; Table \ref{tab:pawsx} shows evaluation on PAWS.

\subsection{Analysis}

We investigate some additional interesting research questions through our novel datasets.

\begin{table}[ht]
    \centering
    \adjustbox{width=\linewidth}{
    \begin{tabular}{lr}
        \toprule
        \textbf{Manipulation} & \textbf{Percentage} $\mathcal{E}(s , f) \geq \mathcal{E}(s, p)$ \\
        \midrule
        Underspecification (US)      & 10.33\% \\
        Role Swap (RS)          & 20.84\% \\
        Antonym Replacement (AR)   & 30.96\% \\
        Hypernym Substitution (HS) & 8.69\%  \\
        Polarity Negation (PN)    & 29.17\% \\
        \bottomrule
    \end{tabular}}
    \caption{Percentage of cases where a type of foil is confusing an embedding model.}
    \label{tab:manipulation-stats}
\end{table}

\paragraph{Which semantic aspects are most challenging for embedding models?} In Table \ref{tab:manipulation-stats} we show the overall tendency of embedding models to get tricked by a particular foil type. Specifically, we find that negation phenomena are not captured well by the models: Either replacing a word with its antonym (AS), or otherwise changing the factuality of the sentence (PN), an embedding model is not unlikely to be foiled. On the other hand, hypernym substitution is least likely to fail a model, accounting for about 9\% of embedding model confusions. This is plausible, since changing a word to its hypernym does cause a meaning change, but it does so more subtly than AS, or PN. 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{img/fg4_gptp_rank.png}
    \caption{Fine-grained linguistic benchmarking of text embedding models.}
    \label{fig:fine-view-performance}
\end{figure*}

A more fine-grained picture is shown in Figure \ref{fig:fine-view-performance}. For each individual embedding model we plot the $TACC$ accuracy across different manipulation types within the GPTP dataset. We make several interesting observations: 1.\ \embedder{sentence-t5-large} excels in all categories. 2.\ Many embedding models are moderattely robust to polarity changes (purple bars), an important property for models of which we expect finer linguistic understanding. However, this doesn't hold true for all models, with a few of them exhibiting a $TACC$ accuracy of equal or below 40\%. Notably, the (less recent) \embedder{all-mpnet-base-v2} performs the most robust across all manipulations, making it seem apt for use-cases where sensitivity across the full breadth of possible linguistic manipulations is paramount.

\paragraph{NLI filtering ablation study.} To ensure the highest faithfulness of our automatically induced challenge cases, we selected only pairs where our NLI-based validation scored high for contradiction. In this experiment, we induce a dataset with subtle pairs and less assured pairs, and thus we fish only those examples that score the \textit{neutral} label with a probability in between 50\% and 80\%. Specifically, we are interested how much the final ranking deviates in between the two induced datasets.

The results are shown in Table \ref{tab:filtered-gpt}, Appendix \ref{app:otherstats}. Interestingly, \embedder{sentence-t5-large}, same as in all other experiments before, achieves the top rank, even though the intersection of the two datasets is zero. Overall,  the ranking stays mostly stable (Spearmanr correlation coefficient: 0.91), and there are no drastic changes. 

\paragraph{Comparison against \textit{MTEB} ranking.} Our created testing challenge focuses on \textit{linguistic understanding} and can be dynamically generated and adapted. On the other hand, static and large benchmarks like MTEB cover a large spectrum of \textit{tasks}, including STS, but also IR, argument mining, and so on. Therefore, we might expect a different ranking of models. Models that would score high in our benchmark but lower on MTEB could point at unleveraged linguistic strengths of a model that might not be fully reflected by MTEB. On the other hand, a relatively lower ranking in our challenge could signal a `red flag' in a model that could suggest vulnerabilities in difficult cases. 

In our Appendix, Table \ref{tab:compare_ranking_mteb}, we compare the ranking of our models against the relative rankings on MTEB. Notably, the relatively best ranked model on MTEB (\embedder{cde-small-v1}, actually, as of October 1st, 2024 the best small model under 400M params) only reaches place 20 in our challenge, suggesting poorer fine-level semantic understanding than \embedder{sentence-t5-large} a model of similar size, but based on a sequence-to-sequence pre-trained transformer. Thus, our results point at potential vulnerabilities of \embedder{cde-small-v1}, especially towards negation and polarity. To assess the overall difference from our ranking versus the MTEB ranking, we again calculate the Spearmanr. The result 0.54, suggesting a significant but only moderate alignment of the two benchmarks. While the massively-scaled MTEB contains many datasets, our datasets have the advantage that they contain \textit{fresh, unseen benchmarking data} that can be dynamically generated, and the models can be diagnosed in \textit{linguisticunderstanding}.





\section{Conclusion}
\label{sec:concl}

Our \model framework facilitates controlled meaning transformations. We first project a text to an abstract graph-based meaning representation, allowing us to change meaning aspects explicitly (e.g., negation). A generator then produces text from the manipulated graph structure. Finally, we leverage an NLI-based to verify an expected outcome. Manual analysis of the generation quality shows high accuracy, with slightly reduced fluency. We showcased \model's utility in an NLP task: benchmarking and analysis of text embedding models. We used it to produce challenging foils from paraphrase pairs, carefully breaking the paraphrase relation while keeping superficial similarity at maximum. Through this, we shed light on the robustness of text embedding models, as well as their facility to distinguish fine-grained linguistic phenomena.

\section*{Limitations}

Our main limitations are directly tied to the model of meaning representation. Indeed, while AMR has the great advantage of large data sets as well as reasonably robust parsers and generators, it also comes with limitations and drawbacks that have been, over the years, carefully outlined by research. There is the mono-linguality \citep{banarescu2013abstract}, then there may be meaning non-isomorphisms due to ambiguity \citep{wein-2025-ambiguity}, and lack of some scope \citep{pustejovsky-etal-2019-modeling} and tense aspects \citep{donatelli-etal-2018-annotation}. Therefore, it is very important to note that \model directly scales with improvements in the area of meaning representations, be it their formal improvement, and further improvement of parsers and generators --- indeed, while AMR parsers and generators score high on benchmarks according to a metric \citep[e.g.,][]{vasylenko-etal-2023-incorporating}, these tasks are by far not solved  \citep{manning-etal-2020-human, opitz-frank-2022-better, yang-schneider-2024-relative}, as is also supported by our output investigation where we observed some fluency issues. Further, with an eye towards multi-linguality, we see some interesting current developments in cross-lingual parsing in the form of ``Universal Representations'' \citep{van2021designing} or highly abstract representations that are taxonomy-aligned \citep{10.1162/coli_a_00542_tax_rich_parsing}. Exploiting the wealth of AMR tools, we might effectively use an off-the-shelf MT model to wrap the English parsing and generation process \citep{uhrig-etal-2021-translate}, or use cross-lingual AMR parsing for certain languages \citep{vanroy-van-de-cruys-2024-less, kang-etal-2024-cross-lingual}. Given sufficient quality of the MT system, this simple approach could already work for some cross-lingual use-cases of our \model. 

\section*{Acknowledgments}

Three authors received funding from the Swiss National Science Foundation (SNSF 213585) and the Luxembourg National Research Fund (17498891) through the \textit{Impresso} research project.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Appendix}
\label{sec:appendix}

\subsection{Example of Data Generation Process}
\label{app:running_example}
A running example of a complete data generation and selection process with \model is shown in Figure \ref{fig:manipulations}.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{img/fg4_updated.png}
    \caption{Running example of the full data generation process with \model. If we apply our strict quality filtering criterion of $>$ 90\% contradition, only PN and AR are fished from the five candidate generations. Later, in our ablation with the alternative and more subtle neutral filter, we retain only the RS.}
    \label{fig:manipulations}
\end{figure*}



\textbf{PN}: the original sentence \textit{I'm happy that things are going so well} transforms into \textit{I'm happy things aren't going so well for me}. Here, we added \edge{w}{:polarity}{-} to node \texttt{w}, where \edge{w}{:instance}{well-09} represents ``well-being.'' While the resulting text may sound counterintuitive (``Happy things are not going well''), the goal is successfully achieved: the meaning is strictly altered (paraphrase relation is broken), while the surface structure remains highly similar with many overlapping tokens.

\textbf{RS}:  In our example, \textit{I'm happy that things are going so well} becomes \textit{So happy things are going so well for me}. In the AMR graph, \texttt{i} (instance of ``I'') and \texttt{s} (instance of ``so'') were swapped. While the original sentence implies an unspecified referent (\textit{who} is it that things are going well for?), the transformed sentence explicitly assigns this role to an entity (``me'') and also shifts emphasis (``so happy'' vs. ``so well''), altering meaning while preserving structural similarity.

\textbf{US}: \textit{I’m happy that things are going so well} is transformed into \textit{Happy things are going so well}. Here, the first-person subject (``I'') is removed, which could also lead to an alternative interpretation: that things themselves are happy. Arguably, this change could still be considered a paraphrase in some pragmatic contexts, and we would thus like to filter it out from our final data. This is exactly where the NLI validation module proves valuable—it assigns an entailment probability of 96.2\%, indicating that this particular instance might not be suitable as a foil.

\textbf{AR}: \textit{I’m happy that things are going so well} is transformed into \textit{I’m happy things are stopping so well}, where the predicate go-01 is replaced with stop-01 in the AMR graph, that is, \edge{g}{:instance}{go-01} becomes \edge{g}{:instance}{stop-01}. This results in a fundamental semantic shift: the original sentence conveys progression (things are going), while the transformed (slightly weird) sentence implies cessation (things are stopping), disrupting the paraphrase relation but maintains high lexical overlap.

\textbf{HS}: \textit{I’m happy that things are going so well} becomes\textit{I’m happy the attributes are going so well}. Here,``thing'', an already quite abstract term, is replaced with an even more abstract alternative, ``attribute'', leading to a grammatically valid but semantically odd sentence. In AMR, this corresponds to the change \edge{t}{:instance}{thing} $\rightarrow$ \edge{t}{:instance}{attribute}, demonstrating how even subtle lexical shifts can significantly impact meaning. The NLI validator assigns it a ``neutral''-label, with very high confidence. Thus, if wished for high strictness, this concrete example can also be filtered out.

\subsection{Bag-of-words Divergence Analysis}
\label{ssec:bowdivergence}
To further investigate the semantic divergence between the original sentences and their paraphrases in both datasets, we employ a simple bag-of-words approach. Specifically, we convert each sentence to lowercase, remove punctuation, split whitespace, optionally filter out English stopwords, and treat the remaining tokens as sets. We compute the \emph{Jaccard similarity} between the original and paraphrased token sets, defined as
\[
J(A, B) = \frac{|A \cap B|}{|A \cup B|},
\]
and use \(1 - J(A,B)\) as a simple measure of divergence. We compare two datasets: one from ChatGPT and one from PAWS. For the ChatGPT dataset, which consists of 11,457 sentence-paraphrase pairs, the average Jaccard similarity is approximately 0.3971. In contrast, the PAWS dataset (1,738 pairs) exhibits a much higher average Jaccard similarity of about 0.9002, indicating a much smaller structural divergence that leads to less challenging foils. This indicates that the ChatGPT paraphrases are, on average, much more lexically varied from the original sentences than those in PAWS. From a practical standpoint, this suggests that the ChatGPT-based paraphrasing process produces more diverse rewordings, potentially offering broader coverage for downstream tasks.

\subsection{Examples of Manipulations}
\label{app:examples}

Examples of some of our generated foils are shown in Table \ref{tab:foilexamples}.

\begin{table*}
    \centering
    \adjustbox{width=\linewidth}{\begin{tabular}{llr}
    \toprule
        Text & Choices & Type \\

        \midrule
        The majority of Havocs served with the Soviets, but the US and Great Britain also used the planes. & T: Most Havocs were utilized by the Soviets, although the US and Great Britain also employed them. \\
         & F: Havocs also serve with the Soviet Union but the majority of their use is by the US and Britain. & RS \\
                 \midrule
        Universities minister David Willetts said all universities can do is ask students if they have booked a flight home. & T: According to David Willetts, universities can only inquire about students' flight reservations for their return home.\\
       &  F: University Minister David Blankts said the university could no longer do the asking whether students had booked flights home. & AR \\
         \midrule
         It was known as a relatively easy plane to fly with good handling during takeoff and landing & T:  The aircraft was recognized for its ease of operation and smooth handling during takeoff and landing \\
         & F: It is known that it is not a relatively easy flying plane that is well-handled when taking off and landing. & PN\\
         \midrule
         The reason for the crash is not known & T: The cause of the accident is uncertain. \\ 
         & F: The reason for the crash was known & US\\
         \midrule
         carcasses were also found near the contaminated watering holes. & T: The polluted watering holes were also discovered to have nearby carcasses.\\
         & F: The carcass was also found far from the contaminated water hole. & AR \\
         \midrule
         The auction house also sold one of the two gun belts owned by Jesse James at the time of his death. & T: One of Jesse James' two gun belts at the time of his death was also sold by the auction house. \\
         & F: The auction house also gave up one of Jesse James's two gun belts when he died. & HS \\
         \midrule
         Jean-Marc Wenger, who lives in Klingau, found the gold. & T: The gold was discovered by Jean-Marc Wenger, a resident of Klingau. \\
         & F: Jean-Marc Klonau a Wenger resident found gold&  RS \\
         \midrule
         Jean-Marc Wenger, who lives in Klingau, found the gold. & T: The gold was discovered by Jean-Marc Wenger, a resident of Klingau. \\
         & F: Jean-Marc Wenger who lives in Klanau has lost gold. & AR \\
         \midrule
         But at the moment it is a complete mystery. & T: Currently, it remains an enigma.\\
         &F: But it is not a complete mystery at the moment. & PN  \\
         \midrule
        Thibaut Courtois was out quickly to thwart Sterling as Liverpool looked to get back in the game. & T:  Liverpool attempted to make a comeback, but Thibaut Courtois swiftly prevented Sterling from scoring\\ 
        & F: When Liverpool looked to get back in the game Thibaut Sterling was quick out to thwart Courttois. & RS \\
        \midrule
        Willis visited the Neon Museum in 2013 to celebrate her 90th birthday. & T: In 2013, Willis marked her 90th birthday by visiting the Neon Museum. \\
        & F: In 2013 to celebrate his 90th birthday he visited the Neo Museum. & US \\ 
        \midrule
        AT least 3,000 Brussels bureaucrats earn more than David Cameron, it emerged yesterday. & T: Yesterday, it was revealed that over 3,000 Brussels bureaucrats earn a higher salary than David Cameron. \\
        & F: It emerged yesterday that at least 3000 bureaucrats in Brussels earn less than David Cameron. & AR \\
         \midrule
        We were going in water until hit the hill and spun. & T: We were traveling through water until we hit the hill and spun.\\
        & F: We went up the hill until the water hit and we spun. & RS \\
         \midrule
        But you need to get somebody like Warren to do it. & T: You should find someone similar to Warren to handle it.\\
        & F: But you don't need to get somebody like Warren to do it. & PN\\
        \midrule
        Julian E. Zelizer says Democrats should be questioning themselves on several key points. & T: Democrats ought to be reflecting on various crucial aspects, according to Julian E. Zelizer. \\
        & F: Julian E. Zelizer said Democrats should question him on several key points. & RS \\
             \midrule
         \bottomrule
    \end{tabular}}
    \caption{Example cases from our automatically induced challenge benchmark. T: The actual paraphrase. F: A generated foil, close to the input text in surface form, but different in meaning.}
    \label{tab:foilexamples}
\end{table*}

\subsection{Additional Tables}
\label{app:otherstats}

Table \ref{tab:gpt} (Table \ref{tab:pawsx}) shows individual embedding model rankings on our GPTP (PAWS) dataset. On the other hand, Table \ref{tab:filtered-gpt} shows results on GPTP, when we apply another quality filtering criterion within our NLI quality validator, namely, getting more subtle examples that lie in between a probability of 50\% and 80\% neutral.

\begin{table}[ht]
    \centering
    \adjustbox{width=\linewidth}{%
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Model Name} & \textbf{TACC} & \textbf{AUC} & \textbf{AVG} \\
        \midrule
\embedder{sentence-t5-large} & 0.8197 & 0.7862 & 0.8026 \\
\embedder{ember-v1} & 0.7600 & 0.7427 & 0.7513 \\
\embedder{GIST-Embedding-v0} & 0.7050 & 0.6860 & 0.6954 \\
\embedder{gte-base-en-v1.5} & 0.7041 & 0.6856 & 0.6947 \\
\embedder{e5-base-v2} & 0.7111 & 0.6758 & 0.6930 \\
\embedder{FAB-Ramy-v1} & 0.7086 & 0.6776 & 0.6928 \\
\embedder{all-mpnet-base-v2} & 0.7049 & 0.6795 & 0.6920 \\
\embedder{bge-base-en-v1.5} & 0.6957 & 0.6864 & 0.6910 \\
\embedder{nomic-embed-text-v1.5} & 0.6622 & 0.6420 & 0.6519 \\
\embedder{instructor-base} & 0.6609 & 0.6401 & 0.6503 \\
\embedder{paraphrase-MiniLM-v2} & 0.6347 & 0.6227 & 0.6286 \\
\embedder{MedEmbed-small-v0.1} & 0.5991 & 0.5969 & 0.5980 \\
\embedder{jina-embeddings-v2-base-en} & 0.5988 & 0.5943 & 0.5965 \\
\embedder{gtr-t5-large} & 0.5780 & 0.5744 & 0.5762 \\
\embedder{stella-base-en-v2} & 0.5756 & 0.5693 & 0.5724 \\
\embedder{cde-small-v1} & 0.5717 & 0.5590 & 0.5653 \\
\embedder{Wartortle} & 0.5604 & 0.5674 & 0.5639 \\
\embedder{all-MiniLM-L12-v2} & 0.5570 & 0.5603 & 0.5586 \\
\embedder{msmarco-bert-co-condensor} & 0.5437 & 0.5511 & 0.5474 \\
\embedder{LaBSE} & 0.5529 & 0.5362 & 0.5444 \\
\embedder{gte-micro} & 0.5257 & 0.5375 & 0.5315 \\
\embedder{contriever-base-msmarco} & 0.5163 & 0.5265 & 0.5214 \\
\embedder{snowflake} & 0.5074 & 0.5135 & 0.5104 \\
\embedder{Ivysaur} & 0.4815 & 0.4966 & 0.4889 \\
\embedder{Venusaur} & 0.4362 & 0.4561 & 0.4459 \\
\embedder{distiluse-base-v2} & 0.4348 & 0.4439 & 0.4393 \\
\embedder{SGPT-125M} & 0.3889 & 0.4079 & 0.3982 \\
\embedder{allenai-specter} & 0.3818 & 0.4003 & 0.3908 \\
\embedder{komninos} & 0.3263 & 0.3617 & 0.3431 \\
        \bottomrule
    \end{tabular}}
    \caption[GPT Table]{Performance of models on GPTP dataset.}
    \label{tab:gpt}
\end{table}

\begin{table}[ht]
    \centering
    \adjustbox{width=\linewidth}{%
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Model Name} & \textbf{TACC} & \textbf{AUC} & \textbf{AVG} \\
        \midrule
\embedder{LaBSE} & 0.9730 & 0.9586 & 0.9657 \\
\embedder{ember-v1} & 0.9597 & 0.9527 & 0.9562 \\
\embedder{Wartortle} & 0.9591 & 0.9521 & 0.9556 \\
\embedder{paraphrase-MiniLM-v2} & 0.9563 & 0.9455 & 0.9509 \\
\embedder{stella-base-en-v2} & 0.9534 & 0.9482 & 0.9508 \\
\embedder{sentence-t5-large} & 0.9551 & 0.9462 & 0.9506 \\
\embedder{bge-base-en-v1.5} & 0.9545 & 0.9467 & 0.9506 \\
\embedder{distiluse-base-v2} & 0.9522 & 0.9429 & 0.9475 \\
\embedder{e5-base-v2} & 0.9419 & 0.9337 & 0.9378 \\
\embedder{Venusaur} & 0.9396 & 0.9251 & 0.9323 \\
\embedder{gte-micro} & 0.9356 & 0.9284 & 0.9320 \\
\embedder{jina-embeddings-v2-base-en} & 0.9356 & 0.9273 & 0.9314 \\
\embedder{GIST-Embedding-v0} & 0.9361 & 0.9233 & 0.9297 \\
\embedder{instructor-base} & 0.9287 & 0.9198 & 0.9242 \\
\embedder{MedEmbed-small-v0.1} & 0.9321 & 0.9155 & 0.9237 \\
\embedder{all-MiniLM-L12-v2} & 0.9315 & 0.9155 & 0.9234 \\
\embedder{Ivysaur} & 0.9252 & 0.9137 & 0.9194 \\
\embedder{FAB-Ramy-v1} & 0.9298 & 0.9084 & 0.9190 \\
\embedder{gte-base-en-v1.5} & 0.9114 & 0.9010 & 0.9062 \\
\embedder{all-mpnet-base-v2} & 0.9131 & 0.8963 & 0.9046 \\
\embedder{komninos} & 0.9143 & 0.8680 & 0.8905 \\
\embedder{nomic-embed-text-v1.5} & 0.8982 & 0.8812 & 0.8896 \\
\embedder{contriever-base-msmarco} & 0.8964 & 0.8797 & 0.8880 \\
\embedder{snowflake} & 0.8930 & 0.8775 & 0.8852 \\
\embedder{msmarco-bert-co-condensor} & 0.8774 & 0.8674 & 0.8724 \\
\embedder{cde-small-v1} & 0.8843 & 0.8196 & 0.8507 \\
\embedder{gtr-t5-large} & 0.8619 & 0.8386 & 0.8501 \\
\embedder{allenai-specter} & 0.8228 & 0.8060 & 0.8143 \\
\embedder{SGPT-125M} & 0.7664 & 0.7326 & 0.7491 \\
        \bottomrule
    \end{tabular}}
    \caption[PAWS Table]{Performance of models on our PAWS dataset.}
    \label{tab:pawsx}
\end{table}


\begin{table}[ht]
    \centering
    \adjustbox{width=\linewidth}{%
    \begin{tabular}{lrrrrr}
        \toprule
        \textbf{Model Name} & \textbf{TACC} & \textbf{AUC} & \textbf{AVG} & \textbf{RANK} & \textbf{GROUP} \\
        \midrule
\embedder{sentence-t5-large} & 0.7260 & 0.6945 & 0.7099 & 1 & 1 \\
\embedder{FAB-Ramy-v1} & 0.6939 & 0.6638 & 0.6785 & 6& 1\\
\embedder{all-mpnet-base-v2} & 0.6691 & 0.6463 & 0.6575 & 7& 1 \\
\embedder{ember-v1} & 0.6635 & 0.6505 & 0.6569 & 2 & 1\\
\embedder{nomic-embed-text-v1.5} & 0.6454 & 0.6239 & 0.6345 & 9& 1 \\
\embedder{e5-base-v2} & 0.6449 & 0.6198 & 0.6321 & 5& 1\\
\embedder{GIST-Embedding-v0} & 0.6375 & 0.6238 & 0.6306 & 3& 1 \\
\embedder{gte-base-en-v1.5} & 0.6218 & 0.6144 & 0.6181 & 4& 1 \\
\embedder{bge-base-en-v1.5} & 0.6156 & 0.6007 & 0.6081 & 8& 1\\
\embedder{instructor-base} & 0.6009 & 0.5853 & 0.5930 & 10& 1 \\
\embedder{gtr-t5-large} & 0.5975 & 0.5795 & 0.5884 & 14 & 1 \\
\embedder{stella-base-en-v2} & 0.5862 & 0.5769 & 0.5815 & 15& 1 \\
\embedder{cde-small-v1} & 0.5795 & 0.5622 & 0.5707 & 16& 2 \\
\embedder{msmarco-bert-co-condensor} & 0.5609 & 0.5644 & 0.5626 & 19& 2 \\
\embedder{MedEmbed-small-v0.1} & 0.5490 & 0.5488 & 0.5489 & 12& 1 \\
\embedder{jina-embeddings-v2-base-en} & 0.5445 & 0.5500 & 0.5472 & 13 & 1\\
\embedder{snowflake} & 0.5479 & 0.5416 & 0.5447 & 23& 2 \\
\embedder{paraphrase-MiniLM-L12-v2} & 0.5457 & 0.5390 & 0.5423 & 11 & 1\\
\embedder{contriever-base-msmarco} & 0.5299 & 0.5346 & 0.5322 & 22& 2 \\
\embedder{LaBSE} & 0.5389 & 0.5211 & 0.5299 & 20& 2 \\
\embedder{all-MiniLM-L12-v2} & 0.5287 & 0.5310 & 0.5298 & 18& 2 \\
\embedder{gte-micro} & 0.4543 & 0.4702 & 0.4621 & 21& 2 \\
\embedder{Ivysaur} & 0.4476 & 0.4674 & 0.4573 & 24& 2 \\
\embedder{allenai-specter} & 0.4510 & 0.4532 & 0.4521 & 28& 2 \\
\embedder{Wartortle} & 0.4391 & 0.4411 & 0.4401 & 17& 2 \\
\embedder{distiluse-base-v2} & 0.4397 & 0.4377 & 0.4387 & 26& 2 \\
\embedder{SGPT-125M} & 0.4036 & 0.4219 & 0.4125 & 27& 2 \\
\embedder{komninos} & 0.3596 & 0.4050 & 0.3810 & 29& 2\\
\embedder{Venusaur} & 0.3579 & 0.3914 & 0.3739 & 25& 2 \\
        \bottomrule
    \end{tabular}}
    \caption[Filtered GPT Dataset Results]{Performance of models on GPTP dataset, when data for examples that are in between 50\% and 80\% neutral. RANK: The rank of the model when GPTP has been filtered by our main criterion (contradiction), compare with Table \ref{tab:gpt}. A more coarse view is the GROUP, it shows the binary (better, worse) group that a model is assigned to in the main data (again, c.f., Table \ref{tab:gpt}).}
    \label{tab:filtered-gpt}
\end{table}

\begin{table}[]
    \centering
    \adjustbox{width=\linewidth}{\begin{tabular}{lrr}
    \toprule
         & \textbf{RANK} & \textbf{MTEB rank (relative)} \\
         \midrule
\embedder{sentence-t5-large} & 1 & 16 \\
\embedder{ember-v1} & 2 & 5  \\
\embedder{bge-base-en-v1.5} & 3 & 4  \\
\embedder{e5-base-v2} & 4 & 9  \\
\embedder{GIST-Embedding-v0} & 5 & 3  \\
\embedder{FAB-Ramy-v1} & 6 & 28 \\
\embedder{gte-base-en-v1.5} & 7 & 2  \\
\embedder{all-mpnet-base-v2} & 8 & 15 \\
\embedder{instructor-base} & 9 & 12 \\
\embedder{paraphrase-MiniLM-L12-v2} & 10 & 19 \\
\embedder{nomic-embed-text-v1.5} & 11 & 7  \\
\embedder{jina-embeddings-v2-base-en} & 12 & 10 \\
\embedder{MedEmbed-small-v0.1} & 13 & 8  \\
\embedder{stella-base-en-v2} & 14 & 6  \\
\embedder{Wartortle} & 15 & 22 \\
\embedder{LaBSE} & 16 & 24 \\
\embedder{all-MiniLM-L12-v2} & 17 & 17 \\
\embedder{gtr-t5-large} & 18 & 13 \\
\embedder{cde-small-v1} & 19 & 1  \\
\embedder{gte-micro} & 20 & 27 \\
\embedder{msmarco-bert-co-condensor} & 21 & 20 \\
\embedder{contriever-base-msmarco} & 22 & 18 \\
\embedder{snowflake} & 23 & 11 \\
\embedder{Ivysaur} & 24 & 14 \\
\embedder{Venusaur} & 25 & 23 \\
\embedder{distiluse-base-v2} & 26 & 29 \\
\embedder{allenai-specter} & 27 & 26 \\
\embedder{SGPT-125M} & 28 & 21 \\
\embedder{komninos} & 29 & 25 \\
         \bottomrule
    \end{tabular}}
    \caption{Comparing our obtained main ranking (Table \ref{tab:mainres}) against the relative ranking on MTEB.}
    \label{tab:compare_ranking_mteb}
\end{table}



\end{document}
