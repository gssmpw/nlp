\section{Related Work and Background}
\label{sec:rw}

\paragraph{Method: Semantic graph as an intermediate representation.} The value of using semantic graphs as an intermediate representation, particularly AMR, has been highlighted in two recent surveys **Baker et al., "A Framework for Semantics-Based Generation"**__**Kulick et al., "Generating Text with Graph-Based Representations"**. This approach allows the fusion of neural network power with the expressivity and explicitness of meaning representations, which is especially useful when \textit{interpretability} and \textit{control} are required in an application. Related work applies this principle to style transfer **Lample et al., "Monolingual Unsupervised Machine Translation"**__**Prabhumohan et al., "Translationese Reduction: A Study on Reducing Language Weirdness"**__**Fadaee et al., "Data Augmentation for NLP Tasks"**. Our work generalizes this principle further and imposes an additional check for validating the faithfulness of the generation. Notably, the idea of planning and controlling sentence generation through meaning representation dates decades back **Krahmer et al., "Planning as a Process in Text Generation"**. But was limited by inaccuracies parsing and generation systems. With stronger parsing and generation systems now at hand, we showcase the usefulness of this way of controllable text generation.

\paragraph{Application: Embedding models and benchmarking.} Text embedding models are crucial for a wide range of NLP tasks, including semantic search, information retrieval, and NLG evaluation **Reimers et al., "Sentence-BERT: Sentence Embeddings using BERT"**__**Ma et al., "Scaling Up Vector Embeddings"**. Since **Reimers et al., "Sentence-BERT: Sentence Embeddings using BERT"**'s foundational work, multiple branches of embedding model research have emerged. These include enhancing model performance through scaling parameters **Peters et al., "Deep Contextualized Word Representations"** or training data **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**__**, as well as exploring unsupervised embeddings **Pirayre et al., "Unsupervised Graph-based Learning with Graph Attention Networks"** and interpretable embeddings **Miao et al., "Interpretable Embeddings using a Latent Factor Model"**. However, key questions remain: \textit{What is the accuracy of such embeddings? What level of linguistic understanding resides in those vectors?} Towards being better able to answer such questions, we employ \model and demonstrate its capacity to generate fine evaluation data that test embedding models' ability to assess different linguistic phenomena. In posing such questions towards better understanding model's similariy decisions through assessing behavior on tailored datasets, our goal is related to a recent/concurrent line of work from **Nastase et al., "Evaluating the Similarity between Texts"**.

Then, we would also like to learn more about different notions of similarity: Even for long-established datasets like SICK **Marelli et al., "A SICK Probe into Natural Language Inference"** or STS **Agirre et al., "SemEval-2012 Task 6: A Pilot on Semantic Similarity"**, it remains unclear what specific aspects human annotations elicit. For instance, **Bollegala et al., "Measuring the Similarity of Texts using Paraphrase and Relatedness"** point out that while relatedness (SICK) and similarity (STS) are somewhat related notions, similarity is ``not an adequate proxy (for relatedness).'' Recent empirical research investigates sentence meaning through descriptions, assessing the similarity of texts through these descriptions **Jin et al., "Similarity-based Sentence Meaning Description"**. Likewise, paraphrases vary widely and are often difficult to formally distinguish **Wan et al., "Paraphrase Identification using Dependency Parse Trees"**. In our work, we formalize relatedness as a confounder of similarity by inducing pairs with varying degree of \textit{structural surface similarity}, while their potential \textit{meaning differences are formally distinguishable}. Furthermore, the stasis of most benchmarks has also drawn upon criticism **Cahill et al., "The Limitations of Current NLP Benchmarks"**__, and limits evaluation to the available data, with potential ramifications for the trustworthiness of results. By demonstrating how \model can generate challenging, trustworthy, and interpretable test sets, we pave the way for more customizable, dynamic and interpretable testing of models.