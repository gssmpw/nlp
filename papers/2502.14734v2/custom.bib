@article{xu2025large,
  title={Are large language models really good logical reasoners? A comprehensive evaluation and beyond},
  author={Xu, Fangzhi and Lin, Qika and Han, Jiawei and Zhao, Tianzhe and Liu, Jun and Cambria, Erik},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2025},
  publisher={IEEE}
}

@inproceedings{parmarLogicBenchSystematicEvaluation2024,
  title = {{{LogicBench}}: {{Towards Systematic Evaluation}} of {{Logical Reasoning Ability}} of {{Large Language Models}}},
  shorttitle = {{{LogicBench}}},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Parmar, Mihir and Patel, Nisarg and Varshney, Neeraj and Nakamura, Mutsumi and Luo, Man and Mashetty, Santosh and Mitra, Arindam and Baral, Chitta},
  year = {2024},
  pages = {13679--13707},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.739},
  urldate = {2025-02-13},
  abstract = {Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really ``reason'' over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to `logical reasoning' has remained underexplored. Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes overlook contextual information necessary for reasoning to arrive at the correct conclusion. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs1.},
  langid = {english},
  file = {/Users/retog/Zotero/storage/HURGDGC4/Parmar et al. - 2024 - LogicBench Towards Systematic Evaluation of Logic.pdf}
}

@inproceedings{uhrig-etal-2021-translate,
    title = "Translate, then Parse! A Strong Baseline for Cross-Lingual {AMR} Parsing",
    author = "Uhrig, Sarah  and
      Garcia, Yoalli  and
      Opitz, Juri  and
      Frank, Anette",
    editor = "Oepen, Stephan  and
      Sagae, Kenji  and
      Tsarfaty, Reut  and
      Bouma, Gosse  and
      Seddah, Djam{\'e}  and
      Zeman, Daniel",
    booktitle = "Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.iwpt-1.6",
    doi = "10.18653/v1/2021.iwpt-1.6",
    pages = "58--64",
    abstract = "In cross-lingual Abstract Meaning Representation (AMR) parsing, researchers develop models that project sentences from various languages onto their AMRs to capture their essential semantic structures: given a sentence in any language, we aim to capture its core semantic content through concepts connected by manifold types of semantic relations. Methods typically leverage large silver training data to learn a single model that is able to project non-English sentences to AMRs. However, we find that a simple baseline tends to be overlooked: translating the sentences to English and projecting their AMR with a monolingual AMR parser (translate+parse,T+P). In this paper, we revisit this simple two-step base-line, and enhance it with a strong NMT system and a strong AMR parser. Our experiments show that T+P outperforms a recent state-of-the-art system across all tested languages: German, Italian, Spanish and Mandarin with +14.6, +12.6, +14.3 and +16.0 Smatch points",
}

@article{palmer-etal-2005-proposition,
    title = "The {P}roposition {B}ank: An Annotated Corpus of Semantic Roles",
    author = "Palmer, Martha  and
      Gildea, Daniel  and
      Kingsbury, Paul",
    journal = "Computational Linguistics",
    volume = "31",
    number = "1",
    year = "2005",
    url = "https://aclanthology.org/J05-1004/",
    doi = "10.1162/0891201053630264",
    pages = "71--106"
}

@inproceedings{nastase-merlo-2024-tracking,
    title = "Tracking linguistic information in transformer-based sentence embeddings through targeted sparsification",
    author = "Nastase, Vivi  and
      Merlo, Paola",
    editor = "Zhao, Chen  and
      Mosbach, Marius  and
      Atanasova, Pepa  and
      Goldfarb-Tarrent, Seraphina  and
      Hase, Peter  and
      Hosseini, Arian  and
      Elbayad, Maha  and
      Pezzelle, Sandro  and
      Mozes, Maximilian",
    booktitle = "Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP-2024)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.repl4nlp-1.15/",
    pages = "203--214",
    abstract = "Analyses of transformer-based models have shown that they encode a variety of linguistic information from their textual input. While these analyses have shed a light on the relation between linguistic information on one side, and internal architecture and parameters on the other, a question remains unanswered: how is this linguistic information reflected in sentence embeddings? Using datasets consisting of sentences with known structure, we test to what degree information about chunks (in particular noun, verb or prepositional phrases), such as grammatical number, or semantic role, can be localized in sentence embeddings. Our results show that such information is not distributed over the entire sentence embedding, but rather it is encoded in specific regions. Understanding how the information from an input text is compressed into sentence embeddings helps understand current transformer models and help build future explainable neural models."
}

@article{nastase2024exploringsy,
  title={Exploring syntactic information in sentence embeddings through multilingual subject-verb agreement},
  author={Nastase, Vivi and Jiang, Chunyang and Samo, Giuseppe and Merlo, Paola},
  journal={arXiv preprint arXiv:2409.06567},
  year={2024}
}

@article{nastase2024exploring,
  title={Exploring Italian sentence embeddings properties through multi-tasking},
  author={Nastase, Vivi and Samo, Giuseppe and Jiang, Chunyang and Merlo, Paola},
  journal={arXiv preprint arXiv:2409.06622},
  year={2024}
}



@inproceedings{donatelli-etal-2018-annotation,
    title = "Annotation of Tense and Aspect Semantics for Sentential {AMR}",
    author = "Donatelli, Lucia  and
      Regan, Michael  and
      Croft, William  and
      Schneider, Nathan",
    editor = "Savary, Agata  and
      Ramisch, Carlos  and
      Hwang, Jena D.  and
      Schneider, Nathan  and
      Andresen, Melanie  and
      Pradhan, Sameer  and
      Petruck, Miriam R. L.",
    booktitle = "Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-4912/",
    pages = "96--108",
    abstract = "Although English grammar encodes a number of semantic contrasts with tense and aspect marking, these semantics are currently ignored by Abstract Meaning Representation (AMR) annotations. This paper extends sentence-level AMR to include a coarse-grained treatment of tense and aspect semantics. The proposed framework augments the representation of finite predications to include a four-way temporal distinction (event time before, up to, at, or after speech time) and several aspectual distinctions (including static vs. dynamic, habitual vs. episodic, and telic vs. atelic). This will enable AMR to be used for NLP tasks and applications that require sophisticated reasoning about time and event structure."
}

@article{wijnen1990development,
  title={The development of sentence planning},
  author={Wijnen, Frank},
  journal={Journal of Child Language},
  volume={17},
  number={3},
  pages={651--675},
  year={1990},
  publisher={Cambridge University Press}
}

@inproceedings{kasper-1989-flexible,
    title = "A Flexible Interface for Linking Applications to {P}enman`s Sentence Generator",
    author = "Kasper, Robert T.",
    booktitle = "Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",
    year = "1989",
    url = "https://aclanthology.org/H89-1022/"
}

@inproceedings{
ravfogel2024descriptionbased,
title={Description-Based Text Similarity},
author={Shauli Ravfogel and Valentina Pyatkin and Amir David Nissan Cohen and Avshalom Manevich and Yoav Goldberg},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=W8Rv1jVycX}
}

@inproceedings{hoyle-etal-2023-natural,
    title = "Natural Language Decompositions of Implicit Content Enable Better Text Representations",
    author = "Hoyle, Alexander  and
      Sarkar, Rupak  and
      Goel, Pranav  and
      Resnik, Philip",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.815/",
    doi = "10.18653/v1/2023.emnlp-main.815",
    pages = "13188--13214",
    abstract = "When people interpret text, they rely on inferences that go beyond the observed language itself. Inspired by this observation, we introduce a method for the analysis of text that takes implicitly communicated content explicitly into account. We use a large language model to produce sets of propositions that are inferentially related to the text that has been observed, then validate the plausibility of the generated content via human judgments. Incorporating these explicit representations of implicit content proves useful in multiple problem settings that involve the human interpretation of utterances: assessing the similarity of arguments, making sense of a body of opinion data, and modeling legislative behavior. Our results suggest that modeling the meanings behind observed language, rather than the literal text alone, is a valuable direction for NLP and particularly its applications to social science."
}

@inproceedings{fan-etal-2024-nphardeval,
    title = "{NPH}ard{E}val: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
    author = "Fan, Lizhou  and
      Hua, Wenyue  and
      Li, Lingyao  and
      Ling, Haoyang  and
      Zhang, Yongfeng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.225/",
    doi = "10.18653/v1/2024.acl-long.225",
    pages = "4092--4114",
    abstract = "Complex reasoning ability is one of the most important features of Large Language Models (LLMs). Numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, they are inadequate in offering a rigorous evaluation and prone to the risk of overfitting, as these publicly accessible and static benchmarks allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, we introduce a new benchmark NPHardEval. It contains a broad spectrum of 900 algorithmic questions belonging up to the NP-Hard complexity class, offering a rigorous measure of the reasoning ability of LLMs utilizing computational complexity. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at https://github.com/casmlab/NPHardEval."
}

@inproceedings{bateman1990interfacing,
  title={Interfacing an English text generator with a German MT analysis},
  author={Bateman, John and Kasper, Robert and Sch{\"u}tz, J{\"o}rg and Steiner, Erich},
  booktitle={Interaktion und Kommunikation mit dem Computer: Jahrestagung der Gesellschaft f{\"u}r Linguistische Datenverarbeitung (GLDV). Ulm, 8.-10. M{\"a}rz 1989 Proceedings},
  pages={155--163},
  year={1990},
  organization={Springer}
}

@article{mann1986systemic,
  title={A systemic grammar for text generation},
  author={Mann, WC and Nigel, Matthiessen CM},
  journal={ISI-Report},
  year={1986}
}

@inproceedings{sondheimer-nebel-1986-logical,
    title = "A Logical-Form and Knowledge-Base Design for Natural Language Generation",
    author = "Sondheimer, Norman K.  and
      Nebel, Bernhard",
    booktitle = "Strategic Computing - Natural Language Workshop: Proceedings of a Workshop Held at Marina del Rey, California, May 1-2, 1986",
    year = "1986",
    url = "https://aclanthology.org/H86-1022/"
}


@inproceedings{opitz-frank-2022-better,
    title = "Better {S}match = Better Parser? {AMR} evaluation is not so simple anymore",
    author = "Opitz, Juri  and
      Frank, Anette",
    editor = "Deutsch, Daniel  and
      Udomcharoenchaikit, Can  and
      Opitz, Juri  and
      Gao, Yang  and
      Fomicheva, Marina  and
      Eger, Steffen",
    booktitle = "Proceedings of the 3rd Workshop on Evaluation and Comparison of NLP Systems",
    month = nov,
    year = "2022",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.eval4nlp-1.4/",
    doi = "10.18653/v1/2022.eval4nlp-1.4",
    pages = "32--43"
}

@inproceedings{manning-etal-2020-human,
    title = "A Human Evaluation of {AMR}-to-{E}nglish Generation Systems",
    author = "Manning, Emma  and
      Wein, Shira  and
      Schneider, Nathan",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.420/",
    doi = "10.18653/v1/2020.coling-main.420",
    pages = "4773--4786",
    abstract = "Most current state-of-the art systems for generating English text from Abstract Meaning Representation (AMR) have been evaluated only using automated metrics, such as BLEU, which are known to be problematic for natural language generation. In this work, we present the results of a new human evaluation which collects fluency and adequacy scores, as well as categorization of error types, for several recent AMR generation systems. We discuss the relative quality of these systems and how our results compare to those of automatic metrics, finding that while the metrics are mostly successful in ranking systems overall, collecting human judgments allows for more nuanced comparisons. We also analyze common errors made by these systems."
}

@inproceedings{pustejovsky-etal-2019-modeling,
    title = "Modeling Quantification and Scope in {A}bstract {M}eaning {R}epresentations",
    author = "Pustejovsky, James  and
      Lai, Ken  and
      Xue, Nianwen",
    editor = "Xue, Nianwen  and
      Croft, William  and
      Hajic, Jan  and
      Huang, Chu-Ren  and
      Oepen, Stephan  and
      Palmer, Martha  and
      Pustejovksy, James",
    booktitle = "Proceedings of the First International Workshop on Designing Meaning Representations",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3303/",
    doi = "10.18653/v1/W19-3303",
    pages = "28--33",
    abstract = "In this paper, we propose an extension to Abstract Meaning Representations (AMRs) to encode scope information of quantifiers and negation, in a way that overcomes the semantic gaps of the schema while maintaining its cognitive simplicity. Specifically, we address three phenomena not previously part of the AMR specification: quantification, negation (generally), and modality. The resulting representation, which we call {\textquotedblleft}Uniform Meaning Representation{\textquotedblright} (UMR), adopts the predicative core of AMR and embeds it under a {\textquotedblleft}scope{\textquotedblright} graph when appropriate. UMR representations differ from other treatments of quantification and modal scope phenomena in two ways: (a) they are more transparent; and (b) they specify default scope when possible.{\textquoteleft}"
}

@inproceedings{wein-2025-ambiguity,
    title = "Ambiguity and Disagreement in {A}bstract {M}eaning {R}epresentation",
    author = "Wein, Shira",
    editor = "Roth, Michael  and
      Schlechtweg, Dominik",
    booktitle = "Proceedings of Context and Meaning: Navigating Disagreements in NLP Annotation",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2025.comedi-1.14/",
    pages = "145--154",
    abstract = "Abstract Meaning Representation (AMR) is a graph-based semantic formalism which has been incorporated into a number of downstream tasks related to natural language understanding. Recent work has highlighted the key, yet often ignored, role of ambiguity and implicit information in natural language understanding. As such, in order to effectively leverage AMR in downstream applications, it is imperative to understand to what extent and in what ways ambiguity affects AMR graphs and causes disagreement in AMR annotation. In this work, we examine the role of ambiguity in AMR graph structure by employing a taxonomy of ambiguity types and producing AMRs affected by each type. Additionally, we investigate how various AMR parsers handle the presence of ambiguity in sentences. Finally, we quantify the impact of ambiguity on AMR using disambiguating paraphrases at a larger scale, and compare this to the measurable impact of ambiguity in vector semantics."
}

@inproceedings{michail-etal-2025-paraphrasus,
    title = "{PARAPHRASUS}: A Comprehensive Benchmark for Evaluating Paraphrase Detection Models",
    author = "Michail, Andrianos  and
      Clematide, Simon  and
      Opitz, Juri",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.585/",
    pages = "8749--8762",
    abstract = "The task of determining whether two texts are paraphrases has long been a challenge in NLP. However, the prevailing notion of paraphrase is often quite simplistic, offering only a limited view of the vast spectrum of paraphrase phenomena. Indeed, we find that evaluating models in a paraphrase dataset can leave uncertainty about their true semantic understanding. To alleviate this, we create PARAPHRASUS, a benchmark designed for multi-dimensional assessment, benchmarking and selection of paraphrase detection models. We find that paraphrase detection models under our fine-grained evaluation lens exhibit trade-offs that cannot be captured through a single classification dataset. Furthermore, PARAPHRASUS allows prompt calibration for different use cases, tailoring LLM models to specific strictness levels. PARAPHRASUS includes 3 challenges spanning over 10 datasets, including 8 repurposed and 2 newly annotated; we release it along with a benchmarking library at https://github.com/impresso/paraphrasus"
}

@article{spearman1904proof,
  title={The Proof and Measurement of Association between Two Things},
  author={Spearman, C},
  journal={The American Journal of Psychology},
  volume={15},
  number={1},
  pages={72--101},
  year={1904}
}


@inproceedings{NEURIPS2020_c3a690bempnet,
 author = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {16857--16867},
 publisher = {Curran Associates, Inc.},
 title = {MPNet: Masked and Permuted Pre-training for Language Understanding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{chatgpt_paraphrases_dataset,
  author={Vladimir Vorobev and Maxim Kuznetsov},
  title={ChatGPT paraphrases dataset},
  year={2023}
}


@article{wang2022texte5,
  title={Text embeddings by weakly-supervised contrastive pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}


@inproceedings{gunther-etal-2023-jina,
    title = "{J}ina Embeddings: A Novel Set of High-Performance Sentence Embedding Models",
    author = {G{\"u}nther, Michael  and
      Milliken, Louis  and
      Geuter, Jonathan  and
      Mastrapas, Georgios  and
      Wang, Bo  and
      Xiao, Han},
    editor = "Tan, Liling  and
      Milajevs, Dmitrijs  and
      Chauhan, Geeticka  and
      Gwinnup, Jeremy  and
      Rippeth, Elijah",
    booktitle = "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.nlposs-1.2/",
    doi = "10.18653/v1/2023.nlposs-1.2",
    pages = "8--18",
    abstract = "Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating textual inputs into numerical representations, capturing the semantics of the text. These models excel in applications like dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of high-quality pairwise and triplet datasets.It underlines the crucial role of data cleaning in dataset preparation, offers in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Text Embedding Benchmark (MTEB). Furthermore, to increase the model`s awareness of grammatical negation, we construct a novel training and evaluation dataset of negated and non-negated statements, which we make publicly available to the community."
}

@inproceedings{feng-etal-2022-languagelabse,
    title = "Language-agnostic {BERT} Sentence Embedding",
    author = "Feng, Fangxiaoyu  and
      Yang, Yinfei  and
      Cer, Daniel  and
      Arivazhagan, Naveen  and
      Wang, Wei",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.62/",
    doi = "10.18653/v1/2022.acl-long.62",
    pages = "878--891",
    abstract = "While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM), dual encoder translation ranking, and additive margin softmax. We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80{\%}. Composing the best of these methods produces a model that achieves 83.7{\%} bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5{\%} achieved by LASER, while still performing competitively on monolingual transfer learning benchmarks. Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de. We publicly release our best multilingual sentence embedding model for 109+ languages at \url{https://tfhub.dev/google/LaBSE}."
}

@article{10.1162/tacl_a_00675,
    author = {Opitz, Juri},
    title = {A Closer Look at Classification Evaluation Metrics and a Critical
                    Reflection of Common Evaluation Practice},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {12},
    pages = {820-836},
    year = {2024},
    month = {06},
    abstract = {Classification systems are evaluated in a countless number of papers. However, we
                    find that evaluation practice is often nebulous. Frequently, metrics are
                    selected without arguments, and blurry terminology invites misconceptions. For
                    instance, many works use so-called ‘macro’ metrics to rank systems
                    (e.g., ‘macro F1’) but do not clearly specify what they would
                    expect from such a ‘macro’ metric. This is problematic, since
                    picking a metric can affect research findings and thus any clarity in the
                    process should be maximized. Starting from the intuitive concepts of bias and prevalence, we perform an
                    analysis of common evaluation metrics. The analysis helps us understand the
                    metrics’ underlying properties, and how they align with expectations as
                    found expressed in papers. Then we reflect on the practical situation in the
                    field, and survey evaluation practice in recent shared tasks. We find that
                    metric selection is often not supported with convincing arguments, an issue that
                    can make a system ranking seem arbitrary. Our work aims at providing overview
                    and guidance for more informed and transparent metric selection, fostering
                    meaningful evaluation.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00675},
    url = {https://doi.org/10.1162/tacl\_a\_00675},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00675/2465598/tacl\_a\_00675.pdf},
}


@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@inproceedings{yang-etal-2019-paws,
    title = "{PAWS}-{X}: A Cross-lingual Adversarial Dataset for Paraphrase Identification",
    author = "Yang, Yinfei  and
      Zhang, Yuan  and
      Tar, Chris  and
      Baldridge, Jason",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1382/",
    doi = "10.18653/v1/D19-1382",
    pages = "3687--3692",
    abstract = "Most existing work on adversarial data generation focuses on English. For example, PAWS (Paraphrase Adversaries from Word Scrambling) consists of challenging English paraphrase identification pairs from Wikipedia and Quora. We remedy this gap with PAWS-X, a new dataset of 23,659 human translated PAWS evaluation pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. We provide baseline numbers for three models with different capacity to capture non-local context and sentence structure, and using different multilingual training and evaluation regimes. Multilingual BERT fine-tuned on PAWS English plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy across the non-English languages and an average accuracy gain of 23{\%} over the next best model. PAWS-X shows the effectiveness of deep, multilingual pre-training while also leaving considerable headroom as a new challenge to drive multilingual research that better captures structure and contextual information."
}

@inproceedings{vasylenko-etal-2023-incorporating,
    title = "Incorporating Graph Information in Transformer-based {AMR} Parsing",
    author = "Vasylenko, Pavlo  and
      Huguet Cabot, Pere Llu{\'i}s  and
      Mart{\'i}nez Lorenzo, Abelardo Carlos  and
      Navigli, Roberto",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.125/",
    doi = "10.18653/v1/2023.findings-acl.125",
    pages = "1995--2011",
    abstract = "Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence. In this paper, we present LeakDistill, a model and method that explores a modification to the Transformer architecture, using structural adapters to explicitly incorporate graph information into the learned representations and improve AMR parsing performance. Our experiments show how, by employing word-to-node alignment to embed graph structural information into the encoder at training time, we can obtain state-of-the-art AMR parsing through self-knowledge distillation, even without the use of additional data. We release the code at [\url{http://www.github.com/sapienzanlp/LeakDistill}](\url{http://www.github.com/sapienzanlp/LeakDistill})."
}

@inproceedings{kang-etal-2024-cross-lingual,
    title = "Should Cross-Lingual {AMR} Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning {AMR} Parsing",
    author = "Kang, Jeongwoo  and
      Coavoux, Maximin  and
      Lopez, C{\'e}dric  and
      Schwab, Didier",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.3/",
    doi = "10.18653/v1/2024.findings-emnlp.3",
    pages = "43--51",
    abstract = "Cross-lingual AMR parsing is the task of predicting AMR graphs in a target language when training data is available only in a source language. Due to the small size of AMR training data and evaluation data, cross-lingual AMR parsing has only been explored in a small set of languages such as English, Spanish, German, Chinese, and Italian. Taking inspiration from Langedijk et al. (2022), who apply meta-learning to tackle cross-lingual syntactic parsing, we investigate the use of meta-learning for cross-lingual AMR parsing. We evaluate our models in $k$-shot scenarios (including 0-shot) and assess their effectiveness in Croatian, Farsi, Korean, Chinese, and French. Notably, Korean and Croatian test sets are developed as part of our work, based on the existing The Little Prince English AMR corpus, and made publicly available. We empirically study our method by comparing it to classical joint learning. Our findings suggest that while the meta-learning model performs slightly better in 0-shot evaluation for certain languages, the performance gain is minimal or absent when $k$ is higher than 0."
}

@inproceedings{vanroy-van-de-cruys-2024-less,
    title = "Less is Enough: Less-Resourced Multilingual {AMR} Parsing",
    author = "Vanroy, Bram  and
      Van de Cruys, Tim",
    editor = "Bunt, Harry  and
      Ide, Nancy  and
      Lee, Kiyong  and
      Petukhova, Volha  and
      Pustejovsky, James  and
      Romary, Laurent",
    booktitle = "Proceedings of the 20th Joint ACL - ISO Workshop on Interoperable Semantic Annotation @ LREC-COLING 2024",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.isa-1.11/",
    pages = "82--92",
    abstract = "This paper investigates the efficacy of multilingual models for the task of text-to-AMR parsing, focusing on English, Spanish, and Dutch. We train and evaluate models under various configurations, including monolingual and multilingual settings, both in full and reduced data scenarios. Our empirical results reveal that while monolingual models exhibit superior performance, multilingual models are competitive across all languages, offering a more resource-efficient alternative for training and deployment. Crucially, our findings demonstrate that AMR parsing benefits from transfer learning across languages even when having access to significantly smaller datasets. As a tangible contribution, we provide text-to-AMR parsing models for the aforementioned languages as well as multilingual variants, and make available the large corpora of translated data for Dutch, Spanish (and Irish) that we used for training them in order to foster AMR research in non-English languages. Additionally, we open-source the training code and offer an interactive interface for parsing AMR graphs from text."
}

@inproceedings{yang-schneider-2024-relative,
    title = "The Relative Clauses {AMR} Parsers Hate Most",
    author = "Yang, Xiulin  and
      Schneider, Nathan",
    editor = "Bonial, Claire  and
      Bonn, Julia  and
      Hwang, Jena D.",
    booktitle = "Proceedings of the Fifth International Workshop on Designing Meaning Representations @ LREC-COLING 2024",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.dmr-1.16/",
    pages = "151--161",
    abstract = "This paper evaluates how well English Abstract Meaning Representation parsers process an important and frequent kind of Long-Distance Dependency construction, namely, relative clauses (RCs). On two syntactically parsed datasets, we evaluate five AMR parsers at recovering the semantic reentrancies triggered by different syntactic subtypes of relative clauses. Our findings reveal a general difficulty among parsers at predicting such reentrancies, with recall below 64{\%} on the EWT corpus. The sequence-to-sequence models (regardless of whether structural biases were included in training) outperform the compositional model. An analysis by relative clause subtype shows that passive subject RCs are the easiest, and oblique and reduced RCs the most challenging, for AMR parsers."
}

@inproceedings{ghosh-etal-2024-abex,
    title = "{ABEX}: Data Augmentation for Low-Resource {NLU} via Expanding Abstract Descriptions",
    author = "Ghosh, Sreyan  and
      Tyagi, Utkarsh  and
      Kumar, Sonal  and
      Evuru, Chandra Kiran  and
      S, Ramaneswaran  and
      Sakshi, S  and
      Manocha, Dinesh",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.43/",
    doi = "10.18653/v1/2024.acl-long.43",
    pages = "726--748",
    abstract = "We present ABEX, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU) tasks. ABEX is based on ABstract-and-EXpand, a novel paradigm for generating diverse forms of an input document {--} we first convert a document into its concise, abstract description and then generate new documents based on expanding the resultant abstraction. To learn the task of expanding abstract descriptions, we first train BART on a large-scale synthetic dataset with abstract-document pairs. Next, to generate abstract descriptions for a document, we propose a simple, controllable, and training-free method based on editing AMR graphs. ABEX brings the best of both worlds: by expanding from abstract representations, it preserves the original semantic properties of the documents, like style and meaning, thereby maintaining alignment with the original label and data distribution. At the same time, the fundamental process of elaborating on abstract descriptions facilitates diverse generations. We demonstrate the effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource settings. ABEX outperforms all our baselines qualitatively with improvements of 0.04{\%} - 38.8{\%}. Qualitatively, ABEX outperforms all prior methods from literature in terms of context and length diversity."
}


@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}



@inproceedings{
greenblatt2024ai,
title={{AI} Control: Improving Safety Despite Intentional Subversion},
author={Ryan Greenblatt and Buck Shlegeris and Kshitij Sachan and Fabien Roger},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=KviM5k8pcP}
}

@inproceedings{sadeddine-etal-2024-survey,
    title = "A Survey of Meaning Representations {--} From Theory to Practical Utility",
    author = "Sadeddine, Zacchary  and
      Opitz, Juri  and
      Suchanek, Fabian",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.159",
    doi = "10.18653/v1/2024.naacl-long.159",
    pages = "2877--2892",
    abstract = "Symbolic meaning representations of natural language text have been studied since at least the 1960s. With the availability of large annotated corpora, and more powerful machine learning tools, the field has recently seen several new developments. In this survey, we study today{'}s most prominent Meaning Representation Frameworks. We shed light on their theoretical properties, as well as on their practical research environment, i.e., on datasets, parsers, applications, and future challenges.",
}

@article{wang2022text,
  title={Text embeddings by weakly-supervised contrastive pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}

@inproceedings{clark2019sentence,
  title={Sentence mover’s similarity: Automatic evaluation for multi-sentence texts},
  author={Clark, Elizabeth and Celikyilmaz, Asli and Smith, Noah A},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2748--2760},
  year={2019}
}

@inproceedings{marelli-etal-2014-sick,
    title = "A {SICK} cure for the evaluation of compositional distributional semantic models",
    author = "Marelli, Marco  and
      Menini, Stefano  and
      Baroni, Marco  and
      Bentivogli, Luisa  and
      Bernardi, Raffaella  and
      Zamparelli, Roberto",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf",
    pages = "216--223",
    abstract = "Shared and internationally recognized benchmarks are fundamental for the development of any computational system. We aim to help the research community working on compositional distributional semantic models (CDSMs) by providing SICK (Sentences Involving Compositional Knowldedge), a large size English benchmark tailored for them. SICK consists of about 10,000 English sentence pairs that include many examples of the lexical, syntactic and semantic phenomena that CDSMs are expected to account for, but do not require dealing with other aspects of existing sentential data sets (idiomatic multiword expressions, named entities, telegraphic language) that are not within the scope of CDSMs. By means of crowdsourcing techniques, each pair was annotated for two crucial semantic tasks: relatedness in meaning (with a 5-point rating scale as gold score) and entailment relation between the two elements (with three possible gold labels: entailment, contradiction, and neutral). The SICK data set was used in SemEval-2014 Task 1, and it freely available for research purposes.",
}

@inproceedings{conneau-kiela-2018-senteval,
    title = "{S}ent{E}val: An Evaluation Toolkit for Universal Sentence Representations",
    author = "Conneau, Alexis  and
      Kiela, Douwe",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Hasida, Koiti  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tokunaga, Takenobu",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1269",
}

@article{budanitsky-hirst-2006-evaluating,
    title = "Evaluating {W}ord{N}et-based Measures of Lexical Semantic Relatedness",
    author = "Budanitsky, Alexander  and
      Hirst, Graeme",
    journal = "Computational Linguistics",
    volume = "32",
    number = "1",
    year = "2006",
    url = "https://aclanthology.org/J06-1003",
    doi = "10.1162/coli.2006.32.1.13",
    pages = "13--47",
}

@inproceedings{muennighoff-etal-2023-mteb,
    title = "{MTEB}: Massive Text Embedding Benchmark",
    author = "Muennighoff, Niklas  and
      Tazi, Nouamane  and
      Magne, Loic  and
      Reimers, Nils",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.148",
    doi = "10.18653/v1/2023.eacl-main.148",
    pages = "2014--2037",
    abstract = "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings todate. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-theart results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at \url{https://github.com/embeddings-benchmark/mteb}.",
}

@inproceedings{agirre-etal-2016-semeval,
    title = "{S}em{E}val-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation",
    author = "Agirre, Eneko  and
      Banea, Carmen  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor  and
      Mihalcea, Rada  and
      Rigau, German  and
      Wiebe, Janyce",
    editor = "Bethard, Steven  and
      Carpuat, Marine  and
      Cer, Daniel  and
      Jurgens, David  and
      Nakov, Preslav  and
      Zesch, Torsten",
    booktitle = "Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S16-1081",
    doi = "10.18653/v1/S16-1081",
    pages = "497--511",
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}


@article{muennighoff2022sgpt,
  title={Sgpt: Gpt sentence embeddings for semantic search},
  author={Muennighoff, Niklas},
  journal={arXiv preprint arXiv:2202.08904},
  year={2022}
}



@inproceedings{gao-etal-2021-simcse,
    title = "{S}im{CSE}: Simple Contrastive Learning of Sentence Embeddings",
    author = "Gao, Tianyu  and
      Yao, Xingcheng  and
      Chen, Danqi",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.552",
    doi = "10.18653/v1/2021.emnlp-main.552",
    pages = "6894--6910",
    abstract = "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using {``}entailment{''} pairs as positives and {``}contradiction{''} pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3{\%} and 81.6{\%} Spearman{'}s correlation respectively, a 4.2{\%} and 2.2{\%} improvement compared to previous best results. We also show{---}both theoretically and empirically{---}that contrastive learning objective regularizes pre-trained embeddings{'} anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",
}

@article{wang2023improving,
  title={Improving text embeddings with large language models},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2401.00368},
  year={2023}
}



@inproceedings{reimers-gurevych-2019-sentence,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}

@inproceedings{opitz-frank-2022-sbert,
    title = "{SBERT} studies Meaning Representations: Decomposing Sentence Embeddings into Explainable Semantic Features",
    author = "Opitz, Juri  and
      Frank, Anette",
    editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-main.48",
    pages = "625--638",
    abstract = "Models based on large-pretrained language models, such as S(entence)BERT, provide effective and efficient sentence embeddings that show high correlation to human similarity ratings, but lack interpretability. On the other hand, graph metrics for graph-based meaning representations (e.g., Abstract Meaning Representation, AMR) can make explicit the semantic aspects in which two sentences are similar. However, such metrics tend to be slow, rely on parsers, and do not reach state-of-the-art performance when rating sentence similarity. In this work, we aim at the best of both worlds, by learning to induce Semantically Structured Sentence BERT embeddings (S$^3$BERT). Our S$^3$BERT embeddings are composed of explainable sub-embeddings that emphasize various sentence meaning features (e.g., semantic roles, negation, or quantification). We show how to i) learn a decomposition of the sentence embeddings into meaning features, through approximation of a suite of interpretable semantic AMR graph metrics, and how to ii) preserve the overall power of the neural embeddings by controlling the decomposition learning process with a second objective that enforces consistency with the similarity ratings of an SBERT teacher model. In our experimental studies, we show that our approach offers interpretability {--} while preserving the effectiveness and efficiency of the neural sentence embeddings.",
}

@inproceedings{shou-etal-2022-amr,
    title = "{AMR-DA}: {D}ata Augmentation by {A}bstract {M}eaning {R}epresentation",
    author = "Shou, Ziyi  and
      Jiang, Yuxin  and
      Lin, Fangzhen",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.244",
    doi = "10.18653/v1/2022.findings-acl.244",
    pages = "3082--3098",
    abstract = "Abstract Meaning Representation (AMR) is a semantic representation for NLP/NLU. In this paper, we propose to use it for data augmentation in NLP. Our proposed data augmentation technique, called AMR-DA, converts a sample sentence to an AMR graph, modifies the graph according to various data augmentation policies, and then generates augmentations from graphs. Our method combines both sentence-level techniques like back translation and token-level techniques like EDA (Easy Data Augmentation). To evaluate the effectiveness of our method, we apply it to the tasks of semantic textual similarity (STS) and text classification. For STS, our experiments show that AMR-DA boosts the performance of the state-of-the-art models on several STS benchmarks. For text classification, AMR-DA outperforms EDA and AEDA and leads to more robust improvements.",
}

@inproceedings{shou-lin-2023-evaluate,
    title = "Evaluate {AMR} Graph Similarity via Self-supervised Learning",
    author = "Shou, Ziyi  and
      Lin, Fangzhen",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.892",
    doi = "10.18653/v1/2023.acl-long.892",
    pages = "16112--16123",
    abstract = "In work on AMR (Abstract Meaning Representation), similarity metrics are crucial as they are used to evaluate AMR systems such as AMR parsers. Current AMR metrics are all based on nodes or triples matching without considering the entire structures of AMR graphs. To address this problem, and inspired by learned similarity evaluation on plain text, we propose AMRSim, an automatic AMR graph similarity evaluation metric. To overcome the high cost of collecting human-annotated data, AMRSim automatically generates silver AMR graphs and utilizes self-supervised learning methods. We evaluated AMRSim on various datasets and found that AMRSim significantly improves the correlations with human semantic scores and remains robust under diverse challenges. We also discuss how AMRSim can be extended to multilingual cases.",
}

@inproceedings{wein-schneider-2024-lost,
    title = "Lost in Translationese? Reducing Translation Effect Using {A}bstract {M}eaning {R}epresentation",
    author = "Wein, Shira  and
      Schneider, Nathan",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.45",
    pages = "753--765",
    abstract = "Translated texts bear several hallmarks distinct from texts originating in the language ({``}translationese{''}). Though individual translated texts are often fluent and preserve meaning, at a large scale, translated texts have statistical tendencies which distinguish them from text originally written in the language and can affect model performance. We frame the novel task of translationese reduction and hypothesize that Abstract Meaning Representation (AMR), a graph-based semantic representation which abstracts away from the surface form, can be used as an interlingua to reduce the amount of translationese in translated texts. By parsing English translations into an AMR and then generating text from that AMR, the result more closely resembles originally English text across three quantitative macro-level measures, without severely compromising fluency or adequacy. We compare our AMR-based approach against three other techniques based on machine translation or paraphrase generation. This work represents the first approach to reducing translationese in text and highlights the promise of AMR, given that our AMR-based approach outperforms more computationally intensive methods.",
}

@inproceedings{jangra-etal-2022-star,
    title = "{T}-{STAR}: Truthful Style Transfer using {AMR} Graph as Intermediate Representation",
    author = "Jangra, Anubhav  and
      Nema, Preksha  and
      Raghuveer, Aravindan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.602",
    doi = "10.18653/v1/2022.emnlp-main.602",
    pages = "8805--8825",
    abstract = "Unavailability of parallel corpora for training text style transfer (TST) models is a very challenging yet common scenario. Also, TST models implicitly need to preserve the content while transforming a source sentence into the target style. To tackle these problems, an intermediate representation is often constructed that is devoid of style while still preserving the meaning of the source sentence. In this work, we study the usefulness of Abstract Meaning Representation (AMR) graph as the intermediate style agnostic representation. We posit that semantic notations like AMR are a natural choice for an intermediate representation. Hence, we propose T-STAR: a model comprising of two components, text-to-AMR encoder and a AMR-to-text decoder. We propose several modeling improvements to enhance the style agnosticity of the generated AMR. To the best of our knowledge, T-STAR is the first work that uses AMR as an intermediate representation for TST. With thorough experimental evaluation we show T-STAR significantly outperforms state of the art techniques by achieving on an average 15.2{\%} higher content preservation with negligible loss ({\textasciitilde}3{\%}) in style accuracy. Through detailed human evaluation with 90,000 ratings, we also show that T-STAR has upto 50{\%} lesser hallucinations compared to state of the art TST models.",
}

@inproceedings{wein-opitz-2024-survey,
    title = "A Survey of {AMR} Applications",
    author = "Wein, Shira  and
      Opitz, Juri",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.390",
    pages = "6856--6875",
    abstract = "In the ten years since the development of the Abstract Meaning Representation (AMR) formalism, substantial progress has been made on AMR-related tasks such as parsing and alignment. Still, the engineering applications of AMR are not fully understood. In this survey, we categorize and characterize more than 100 papers which use AMR for downstream tasks{---} the first survey of this kind for AMR. Specifically, we highlight (1) the range of applications for which AMR has been harnessed, and (2) the techniques for incorporating AMR into those applications. We also detect broader AMR engineering patterns and outline areas of future work that seem ripe for AMR incorporation. We hope that this survey will be useful to those interested in using AMR and that it sparks discussion on the role of symbolic representations in the age of neural-focused NLP research.",
}

@article{huang2009using,
  title={Using WordNet synonym substitution to enhance UMLS source integration},
  author={Huang, Kuo-Chuan and Geller, James and Halper, Michael and Perl, Yehoshua and Xu, Junchuan},
  journal={Artificial intelligence in medicine},
  volume={46},
  number={2},
  pages={97--109},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{bolshakov2004synonymous,
  title={Synonymous paraphrasing using wordnet and internet},
  author={Bolshakov, Igor A and Gelbukh, Alexander},
  booktitle={International conference on application of natural language to information systems},
  pages={312--323},
  year={2004},
  organization={Springer}
}


@article{10.1162/tacl_a_00681,
    author = {Mizrahi, Moran and Kaplan, Guy and Malkin, Dan and Dror, Rotem and Shahaf, Dafna and Stanovsky, Gabriel},
    title = {State of What Art? A Call for Multi-Prompt LLM Evaluation},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {12},
    pages = {933-949},
    year = {2024},
    month = {08},
    abstract = {Recent advances in LLMs have led to an abundance of evaluation benchmarks, which typically rely on a single instruction template per task. We create a large-scale collection of instruction paraphrases and comprehensively analyze the brittleness introduced by single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We find that different instruction templates lead to very different performance, both absolute and relative. Instead, we propose a set of diverse metrics on multiple instruction paraphrases, specifically tailored for different use cases (e.g., LLM vs. downstream development), ensuring a more reliable and meaningful assessment of LLM capabilities. We show that our metrics provide new insights into the strengths and limitations of current LLMs.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00681},
    url = {https://doi.org/10.1162/tacl\_a\_00681},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00681/2464098/tacl\_a\_00681.pdf},
}



@inproceedings{banarescu2013abstract,
  title={Abstract meaning representation for sembanking},
  author={Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan},
  booktitle={Proceedings of the 7th linguistic annotation workshop and interoperability with discourse},
  pages={178--186},
  year={2013}
}



@inproceedings{honovich-etal-2022-true-evaluating,
    title = "{TRUE}: Re-evaluating Factual Consistency Evaluation",
    author = "Honovich, Or  and
      Aharoni, Roee  and
      Herzig, Jonathan  and
      Taitelbaum, Hagai  and
      Kukliansy, Doron  and
      Cohen, Vered  and
      Scialom, Thomas  and
      Szpektor, Idan  and
      Hassidim, Avinatan  and
      Matias, Yossi",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.287",
    doi = "10.18653/v1/2022.naacl-main.287",
    pages = "3905--3920",
    abstract = "Grounded text generation systems often generate text that contains factual inconsistencies, hindering their real-world applicability. Automatic factual consistency evaluation may help alleviate this limitation by accelerating evaluation cycles, filtering inconsistent outputs and augmenting training data. While attracting increasing attention, such evaluation metrics are usually developed and evaluated in silo for a single task or dataset, slowing their adoption. Moreover, previous meta-evaluation protocols focused on system-level correlations with human annotations, which leave the example-level accuracy of such metrics unclear. In this work, we introduce TRUE: a comprehensive survey and assessment of factual consistency metrics on a standardized collection of existing texts from diverse tasks, manually annotated for factual consistency. Our standardization enables an example-level meta-evaluation protocol that is more actionable and interpretable than previously reported correlations, yielding clearer quality measures. Across diverse state-of-the-art metrics and 11 datasets we find that large-scale NLI and question generation-and-answering-based approaches achieve strong and complementary results. We recommend those methods as a starting point for model and metric developers, and hope TRUE will foster progress towards even better evaluation methods.",
}



@inproceedings{steen-etal-2023-little,
    title = "With a Little Push, {NLI} Models can Robustly and Efficiently Predict Faithfulness",
    author = "Steen, Julius  and
      Opitz, Juri  and
      Frank, Anette  and
      Markert, Katja",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.79",
    doi = "10.18653/v1/2023.acl-short.79",
    pages = "914--924",
    abstract = "Conditional language models still generate unfaithful output that is not supported by their input. These unfaithful generations jeopardize trust in real-world applications such as summarization or human-machine interaction, motivating a need for automatic faithfulness metrics. To implement such metrics, NLI models seem attractive, since they solve a strongly related task that comes with a wealth of prior research and data. But recent research suggests that NLI models require costly additional machinery to perform reliably across datasets, e.g., by running inference on a cartesian product of input and generated sentences, or supporting them with a question-generation/answering step. In this work we show that pure NLI models {\_}can{\_} outperform more complex metrics when combining task-adaptive data augmentation with robust inference procedures. We propose: (1) Augmenting NLI training data toadapt NL inferences to the specificities of faithfulness prediction in dialogue;(2) Making use of both entailment and contradiction probabilities in NLI, and(3) Using Monte-Carlo dropout during inference. Applied to the TRUE benchmark, which combines faithfulness datasets across diverse domains and tasks, our approach strongly improves a vanilla NLI model and significantly outperforms previous work, while showing favourable computational cost.",
}

@article{van2021designing,
  title={Designing a uniform meaning representation for natural language processing},
  author={Van Gysel, Jens EL and Vigus, Meagan and Chun, Jayeol and Lai, Kenneth and Moeller, Sarah and Yao, Jiarui and O’Gorman, Tim and Cowell, Andrew and Croft, William and Huang, Chu-Ren and others},
  journal={KI-K{\"u}nstliche Intelligenz},
  volume={35},
  number={3},
  pages={343--360},
  year={2021},
  publisher={Springer}
}

@inproceedings{lorenzo2024efficient,
  title={Efficient AMR Parsing with CLAP: Compact Linearization with an Adaptable Parser},
  author={Lorenzo, Abelardo Carlos Mart{\'\i}nez and Navigli, Roberto},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={5578--5584},
  year={2024}
}



@article{10.1162/coli_a_00542_tax_rich_parsing,
    author = {Zhang, Xiao and Bouma, Gosse and Bos, Johan},
    title = {Neural Semantic Parsing with Extremely Rich Symbolic Meaning Representations},
    journal = {Computational Linguistics},
    pages = {1-40},
    year = {2024},
    month = {10},
    abstract = {Current open-domain neural semantics parsers show impressive performance. However, closer inspection of the symbolic meaning representations they produce reveals significant weaknesses: sometimes they tend to merely copy character sequences from the source text to form symbolic concepts, defaulting to the most frequent word sense based in the training distribution. By leveraging the hierarchical structure of a lexical ontology, we introduce a novel compositional symbolic representation for concepts based on their position in the taxonomical hierarchy. This representation provides richer semantic information and enhances interpretability.We introduce a neural “taxonomical” semantic parser to utilize this new representation system of predicates, and compare it with a standard neural semantic parser trained on the traditional meaning representation format, employing a novel challenge set and evaluation metric for evaluation. Our experimental findings demonstrate that the taxonomical model, trained on much richer and complex meaning representations, is slightly subordinate in performance to the traditional model using the standard metrics for evaluation, but outperforms it when dealing with out-of-vocabulary concepts. We further showed through neural model probing that training on a taxonomic representation enhances the model’s ability to learn the taxonomical hierarchy. This finding is encouraging for research in computational semantics that aims to combine data-driven distributional meanings with knowledge-based symbolic representations.},
    issn = {0891-2017},
    doi = {10.1162/coli_a_00542},
    url = {https://doi.org/10.1162/coli\_a\_00542},
    eprint = {https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli\_a\_00542/2474131/coli\_a\_00542.pdf},
}

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703/",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
}


