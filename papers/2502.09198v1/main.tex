
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %
\usepackage{acronym}
\usepackage{parskip}
\usepackage{nicefrac}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{placeins}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{bm}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{csvsimple}
\usepackage{numprint}
\usepackage[]{mdframed}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage{siunitx}
\usepackage{xspace}

\hyphenation{hy-per-pa-ra-me-ter}

\acrodef{BO}[BO]{Bayesian optimization}
\acrodef{HDBO}[HDBO]{high-dimensional Bayesian optimization}
\acrodef{MLE}[MLE]{maximum likelihood estimation}
\acrodef{GP}[GP]{Gaussian process}
\acrodefplural{GP}[GPs]{Gaussian processes}
\acrodef{EI}[EI]{expected improvement}
\acrodef{HP}[HP]{hyperparameter}
\acrodef{RBF}[RBF]{radial basis function}
\acrodef{VBO}[VBO]{Vanilla Bayesian Optimization}
\acrodef{PDF}[PDF]{probability density function}
\acrodef{CDF}[CDF]{cumulative distribution function}
\acrodef{DSP}[DSP]{dimensionality-scaled length scale prior}
\acrodef{ARD}[ARD]{automated relevance determination}
\acrodef{MAP}[MAP]{maximum a-posteriori estimation}
\acrodef{VAE}[VAE]{variational autoencoder}
\acrodef{TR}[TR]{trust region}
\acrodef{TS}[TS]{Thompson sampling}
\acrodef{RMSE}[RMSE]{root mean square error}
\acrodef{UCB}[UCB]{upper confidence bound}
\acrodef{PI}[PI]{probability of improvement}
\acrodef{RAASP}[RAASP]{random axis-aligned subspace perturbation}
\acrodef{OTSD}[OTSD]{observation traveling salesperson distance}
\acrodef{GD}[GD]{gradient descent}
\acrodef{MLL}[MLL]{marginal log-likelihood}
\acrodef{COD}[COD]{curse of dimensionality}
\acrodef{AF}[AF]{acquisition function}
\acrodef{DOE}[DOE]{design of experiments}

\newcommand{\baxus}{\texttt{BAxUS}\xspace}
\newcommand{\bounce}{\texttt{Bounce}\xspace}
\newcommand{\bodi}{\texttt{BODi}\xspace}
\newcommand{\combo}{\texttt{COMBO}\xspace}
\newcommand{\hesbo}{\texttt{HeSBO}\xspace}
\newcommand{\turbo}{\texttt{TuRBO}\xspace}
\newcommand{\dsp}{\texttt{DSP}\xspace}
\newcommand{\logei}{\texttt{LogEI}\xspace}
\newcommand{\alebo}{\texttt{Alebo}\xspace}
\newcommand{\rembo}{\texttt{REMBO}\xspace}
\newcommand{\saasbo}{\texttt{SAASBO}\xspace}
\newcommand{\botorch}{\texttt{BoTorch}\xspace}
\newcommand{\gpytorch}{\texttt{GPyTorch}\xspace}
\newcommand{\cmaes}{\texttt{CMA-ES}\xspace}
\newcommand{\lassobench}{\texttt{LassoBench}\xspace}
\newcommand{\T}{{\intercal}}

\newcommand{\rowgroup}[1]{\hspace{-1em}#1}


\usepackage{hyperref}


\newcommand{\thealgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\icmltitlerunning{Understanding High-Dimensional Bayesian Optimization}


\newcommand{\lp}[1]{{\color{magenta}\textbf{Leonard:~}#1}}

\begin{document}

\twocolumn[
\icmltitle{Understanding High-Dimensional Bayesian Optimization}




\begin{icmlauthorlist}
\icmlauthor{Leonard Papenmeier}{lund}
\icmlauthor{Matthias Poloczek}{amazon}
\icmlauthor{Luigi Nardi}{lund,dbtune}
\end{icmlauthorlist}

\icmlaffiliation{lund}{Department of Computer Science, Lund University, Lund, Sweden}
\icmlaffiliation{amazon}{Amazon (This research does not relate to Matthias' work at Amazon.)}
\icmlaffiliation{dbtune}{DBtune}

\icmlcorrespondingauthor{Leonard Papenmeier}{leonard.papenmeier@cs.lth.se}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{} %

\begin{abstract}
Recent work reported that simple \acl{BO} methods perform well for high-dimensional real-world tasks, seemingly contradicting prior work and tribal knowledge. 
This paper investigates the `why'.
We identify fundamental challenges that arise in high-dimensional \acl{BO} and explain why recent methods succeed. 
Our analysis shows that vanishing gradients caused by \acl{GP} initialization schemes play a major role in the failures of high-dimensional Bayesian optimization and that methods that promote local search behaviors are better suited for the task. 
We find that \acl{MLE} of \acl{GP} length scales suffices for state-of-the-art performance. Based on this, we propose a simple variant of \acl{MLE} called MSR that leverages these findings to achieve state-of-the-art performance on a comprehensive set of real-world applications. 
We also present targeted experiments to illustrate and confirm our findings. 
\end{abstract} 


\section{Introduction}
\ac{BO} has found wide-spread adoption for optimizing expensive-to-evaluate black-box functions that appear in aerospace engineering~\cite{lukaczyk2014active,lam2018advances},  drug discovery~\cite{negoescu2011the},
robotics~\cite{lizotte2007automatic,calandra2016bayesian,rai2018bayesian,mayr2022skill} or finance~\cite{baudi2014online}.

While \ac{BO} has proven reliable in low-dimensional settings, high-dimensional spaces are challenging due to the \ac{COD} that demands exponentially more data points to maintain the same precision with increasing problem dimensionality.
Several approaches have extended \ac{BO} to high-dimensional spaces under additional assumptions on the objective function that lower the data demand, such as additivity~\cite{duvenaud2011additive,kandasamy2015high,han2021high,ziomek2023random} or the existence of a low-dimensional active subspace~\cite{wang2016bayesian,nayebi2019framework,letham2020re,papenmeier2022increasing}.
Without such assumptions, it was widely believed that \ac{BO} with a \ac{GP} surrogate is limited to approximately 20 dimensions for common evaluation budgets~\cite{frazier2018tutorial,moriconi2020high}.
Recently, \citet{hvarfner2024vanilla} and \citet{xu2024standard} reported that simple \ac{BO} methods perform well on high-dimensional real-world benchmarks, often surpassing the performance of more sophisticated algorithms.

Due to its many impactful applications, \ac{HDBO} has seen active research in recent years~\cite{binois2022survey,papenmeier2023bounce}.
While the boundaries have been pushed significantly, the causes of performance gains have not always been thoroughly identified.
For example, in the case of the \bodi~\cite{deshwal2023bayesian} and \combo~\cite{oh2019combinatorial} algorithms, later work found that the methods benefited from specific benchmark structures prevalent in their evaluation~\cite{papenmeier2023bounce}.
Similarly, an evaluation of~\citet{nayebi2019framework} showed that the performance of some prior methods is sensitive to the location of the optimum in the search space.

Thus, answering the recent call for more scrutiny and exploratory research~\cite{herrmannposition}, we first identify fundamental challenges arising in high dimensions and then examine state-of-the-art \ac{HDBO} methods to understand how they mitigate these obstacles.
Equipped with these insights, we propose a simpler approach that uses \ac{MLE} of the \ac{GP} length scales called MLE Scaled with RAASP (MSR). 
We demonstrate that MSR is sufficient for state-of-the-art \ac{HDBO} performance without the need for specifying a prior belief on length scales as in \ac{MAP}.
We note that practitioners usually do not possess such priors and instead rely on empirical performances on benchmarks.%
In particular, we change the initialization of length scales to avoid vanishing gradients of the \ac{GP} likelihood function that easily occur in high-dimensional spaces but, so far, have been overlooked for \ac{BO}.
Furthermore, we provide empirical evidence suggesting that good \ac{BO} performance on extremely high-dimensional problems (on the order of \num{1000} dimensions) is due to local search behavior and not to a well-fit surrogate model.
In summary, we make the following contributions. 
\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]
    \item 
    We identify fundamental challenges that arise in high-dimensional Bayesian optimization and explain why recent methods succeed. We show that vanishing gradients and local search behaviors are important in HDBO.  
    \item We find that MLE of GP length scales suffices for state-of-the-art performance. We propose a simple variant of MLE called \texttt{MSR}. 
    \item 
    We evaluate \texttt{MSR} on a comprehensive set of real-world applications and a series of targeted experiments that illustrate and confirm our findings.
\end{enumerate}

\section{Problem Statement and Related Work}\label{sec:background}
We aim to find $\bm{x}^* \in \argmin_{\bm{x}\in\mathcal{X}} f(\bm{x})$, where $f: \mathcal{X} \rightarrow \mathbb{R}$ is an unknown, only point-wise observable, and expensive-to-evaluate black-box function and $\mathcal{X}=[0,1]^d$ is the $d$-dimensional search space, sometimes called ``input space''.
\ac{BO} is a popular approach to optimize problems with the above characteristics.
We give a summary of \ac{BO} and \acp{GP} in Appendix~\ref{app:background_bo_and_gps} and restrict the discussion to high-dimensional \ac{BO}.

Extending the scope of \ac{BO} to high-dimensional problems was, for a long time, considered ``as one of the holy grails''~\cite{wang2016bayesian} or ``one of the most important goals''~\cite{nayebi2019framework} of the field.
Several contributions extended the scope of \ac{BO} to specific high-dimensional problems.
However, for the longest time, no fully scalable method has been found to extend in arbitrarily high-dimensional spaces without making additional assumptions about the problem structure.
The root problem of extending \acf{BO} to high dimensions is the \acf{COD}~\cite{binois2022survey} that not only requires exponentially many more data points to model $f$ with the same precision but also complicates the fitting of the \ac{GP} hyperparameters and the maximization of the \ac{AF}.
The growing demand for training samples stems from increasing point distances in high dimensions, where the average distance in a $d$-dimensional hypercube is $\sqrt{d}$~\cite{koppen2000curse}.

This paper focuses on \ac{HDBO} operating directly in the search space.
Other methods for \ac{HDBO} include linear~\cite{nayebi2019framework,wang2016bayesian,letham2020re,papenmeier2022increasing,papenmeier2023bounce} or non-linear~\cite{tripp2020sample,moriconi2020high,maus2022local,bouhlel2018efficient,chen2020semi} embeddings to map from a low-dimensional subspace to the input space.

\paragraph{High-dimensional BO in the input space.}
In the literature, \ac{HDBO} operating directly in the high-dimensional search space $\mathcal{X}$ is often considered infeasible due to the \ac{COD}.
Numerous approaches have been proposed, often leveraging assumptions made on the objective function $f$ such as additivity~\cite{kandasamy2015high,gardner2017discovering,wang2018batched,mutny2018efficient,ziomek2023random} or axis-alignment~\cite{eriksson2021high,hellsten2023high,song2022monte}, which simplify the problem and improve sample efficiency if they are met.
Other methods identify regions in the search space relevant for the optimization, for example, using \acp{TR}~\cite{regis2016trust,pedrielli2016g,eriksson2019scalable} or by partitioning the space~\cite{wang2020learning}.

Recently, multiple works re-evaluated basic \ac{BO} setups for high-dimensional problems, presenting state-of-the-art performance on various high-dimensional benchmarks with only small changes to basic \ac{BO} strategies.
\citet{hvarfner2024vanilla} use a dimensionality-scaled log-normal length scale hyperprior that shifts the mode and mean of the log-normal distribution by a factor of $\sqrt{d}$, designed to counteract the increased distance between randomly sampled points.
To optimize the \ac{AF}, they change \botorch's~\cite{balandat2020botorch} default strategy of performing Boltzmann sampling on a set of quasi-randomly generated points by sampling over both a set of quasi-randomly generated points and a set of points that are generated by perturbing the 5\% best-performing points.
By perturbing 20 dimensions on average, this strategy creates candidates closer to the incumbent observations and enforces a more exploitative behavior~\cite{regis2013combining,regis2016trust,eriksson2019scalable}.
The effect of the sampling strategy was recently revisited by \citet{rashidi2024cylindrical}.
They argue that \turbo's \acp{RAASP} are crucial to performance on high-dimensional benchmarks and, motivated by this observation, derive the cylindrical \ac{TS} strategy that maintains locality but drops the requirement of axis alignment.
Independently of \citet{hvarfner2024vanilla}, \citet{xu2024standard} reported that ``standard \acp{GP} can be excellent for \ac{HDBO}'' which they show empirically on several high-dimensional benchmarks.
They use a uniform $\mathcal{U}(\num{e-3},30)$ length scale hyperprior, which, in their experiments, performs superior to \botorch's Gamma $\Gamma(3,6)$ length scale hyperprior.

\section{Facets of the Curse of Dimensionality}
\label{sec:facets-of-cod}

This section discusses how the \acl{COD} impacts high-dimensional Bayesian optimization and techniques to mitigate these challenges.

\subsection{Vanishing Gradients at Model Fitting}\label{subsec:problem-vanishing-gradients}

\acl{BO} uses a probabilistic surrogate model of $f$ to guide the optimization.
\acp{GP} are the most popular surrogates due to their analytical tractability.
They allow for different likelihoods, mean, and covariance functions, each often exposing several hyperparameters, including the function variance $\sigma_f^2$, the noise variance $\sigma_n^2$, and the $d$ model length scales $\bm{\ell}$ that need to be fitted to the task at hand.
In the absence of prior information about the objective function~$f$, \acf{MLE} is commonly used to fit the model hyperparameters by maximizing the \ac{GP} \ac{MLL}:
\begin{align}
\bm{\theta}_{\textrm{MLE}}^* = \argmax_{\bm{\theta}} \log p(\bm{y}\vert X,\bm{\theta}), \label{eq:mle}
\end{align}
Here, $X$ are points in the search space $\mathcal{X}$, $\bm{y}$ are the associated function values, and $\bm{\theta}$ is the vector of \ac{GP} hyperparameters.
See Appendix~\ref{app:background_bo_and_gps} for more information.

The \ac{MLL} is usually maximized using a multi-start \ac{GD} approach.
A crucial component of fitting a \ac{GP} is choosing starting points for the \ac{MLE} hyperparameters.
In this section, we show that an ill-suited length scale initialization scheme can cause the gradient of the \ac{MLL} function with respect to the \ac{GP} length scales to vanish for high-dimensional problems.
Thus, the length scales remain at the numerical values that they have been initialized to and will not be fitted to the objective function.

\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{figures/vanishing_grads.pdf}
    \vspace{-0.35em}
    \caption{Maximum \ac{MLE} gradient magnitude for the 50 first gradient steps initialized with different initial length scales ($y$-axis) and problem dimensionalities ($x$-axis). With short initial length scales, the gradients vanish even for low dimensions.}
    \label{fig:vanishing_grads}
\end{figure}
Fig.~\ref{fig:vanishing_grads} shows the severity of the vanishing-gradients phenomenon.
We plot the maximum magnitude (element-wise) of the length scale gradient across \num{50} gradient updates of an isotropic \ac{GP} with a $\nicefrac{5}{2}$-Mat\'ern kernel as a function of the input dimensionality of the objective function ($x$-axis) and the initial value for the length scale hyperparameter with which the gradient-based optimizer starts when maximizing the \ac{MLE} ($y$-axis).
The objective function is sampled from a \ac{GP} with a $\nicefrac{5}{2}$-Mat\'ern kernel and $\ell=0.5$, i.e., a \ac{GP} prior sample.
The dashed line shows the default initial length scale of $\ln 2$ used in \gpytorch.
We consider gradients smaller than the machine precision for single floating point numbers `vanished'. 
The reason is that even after 500 gradient updates, the length scale would change at most by $\approx\num{6e-5}$ from the value with which the gradient-based optimizer was initialized.

\paragraph{Methods to Mitigate Vanishing Gradients.}

One strategy to counteract the vanishing gradients is to replace \ac{MLE} with \acf{MAP} by choosing a hyperprior on the length scales  that prefers long length scales:
\begin{align}
\bm{\theta}_{\textrm{MAP}}^* = \argmax_{\bm{\theta}} \underbrace{\log p(\bm{y}\vert X,\bm{\theta})}_{\textrm{evidence}} + \underbrace{\log p(\bm{\theta})}_{\textrm{prior}}. \label{eq:map}
\end{align}
\ac{MAP} maximizes the unnormalized log posterior, which is the sum of the \ac{MLL} and the log prior.
We sometimes use the terms `\ac{MLL}' and `unnormalized log-posterior' interchangeably if what is meant is clear from the context.
The gradient of the recently popularized dimensionality-scaled log-normal hyperprior of~\citep{hvarfner2024vanilla} directs the optimizer toward the mode of the hyperprior  $\log p(\bm{\theta})$ that corresponds to long length scales. 

If the gradient-based optimizer of the \ac{MLE} reaches sufficiently long length scales, the \ac{MLL} $\log p(\bm{y}\vert X,\bm{\theta})$ no longer vanishes.
Fig.~\ref{fig:mll-map-progression} shows the length scales of a \ac{GP} conditioned on random observations of a realization drawn from an isotropic 1000-dimensional \ac{GP} prior with length scale $\ell=0.5$ and a $\nicefrac{5}{2}$-Mat\'ern kernel when fitting with \ac{MLE} and \ac{MAP}.
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/mle_map_progression_ls}
    \vspace{-0.35em}
    \caption{Gradient-based optimization of a GP length scale with \ac{MAP} and \ac{MLE}. When conditioning the \ac{GP} on only one random observation, \ac{MAP} converges to the prior mode. We plot $\pm$ one standard error, which is too small to be visible.}
    \label{fig:mll-map-progression}
\end{figure}
We initialize the length scales with $\ln 2\approx 0.69$, draw \num{1} and \num{10} observations uniformly at random, and maximize the \ac{MLL} for \num{500} iterations with \ac{MLE} and \ac{MAP}, using a log-normal hyperprior.
We average over \num{10} random restarts.
When conditioning on only \num{1} observation, the \ac{MAP} optimization starts at the initialization  $\ln 2$ and converges to the hyperprior mode.
For \num{10} observations, the length scales first move toward the mode but then converge to a different point, which trades off the attraction of the prior mode and the ground truth length scale.
With \ac{MLE}, the length scale does not change because of the vanishing gradients issue.

To ``ensure meaningful correlation'' in increasingly high-dimensional space, \citet{hvarfner2024vanilla} scale a log-normal hyperprior for the \ac{GP} length scales with the problem's dimensionality.
\citet{xu2024standard} pursue a different approach of initializing the gradient-based optimizer of the length scales with large values. 
Specifically, they posit a uniform $\mathcal{U}(\num{e-3},\num{30})$ hyperprior to the length scales and initialize the gradient-based optimization of the \ac{ARD} kernel with $d$ samples from the hyperprior.
Although their method does not scale the hyperprior mode with the problem's dimensionality, the expected length scale of $\approx 15$ is sufficiently long to avoid vanishing gradients for the problems studied by the authors.

While these methods mitigate the issue of vanishing gradients, increasing length scales by any constant factor does not always solve it, as Fig.~\ref{fig:vanishing_grads} indicates.
Whether scaling the hyperprior of the length scale with the problem's dimensionality achieves a good fit of the surrogate model depends on the properties of~$f$.
The success of the two methods described above is related to the effect of the increased length scales on mitigating the problem of vanishing gradients.
If the underlying function varies quickly, the \ac{GP} needs to use a short length scale to model $f$.
In such cases, the \ac{GP} cannot model the function globally, as shown in Appendix~\ref{subsec:hard-problems}.

\subsection{Vanishing Gradients of the Acquisition Function}\label{subsec:problem-flat-acquisition}

Several popular \aclp{AF} for \ac{BO}, such as \ac{UCB}~\cite{srinivas2010gaussian}, \ac{EI}~\cite{jones1998efficient}, and \ac{PI}~\cite{jones2001taxonomy}, rely solely on the \ac{GP} posterior, exhibiting only small variation when the posterior itself changes only moderately.
In high-dimensional spaces, the expected distance between two points sampled uniformly at random increases with~$\sqrt{d}$~\cite{koppen2000curse}.
Thus, there are typically large `unexplored regions' in the search space where the algorithm has not sampled.
Suppose a commonly used \ac{GP} with constant prior mean function and a stationary kernel.
The \ac{GP} posterior corresponds to the prior in those regions unless the kernel has sufficiently long length scales.
Therefore, the \ac{GP} posterior and the acquisition surface are flat in those vast unexplored regions.
\Acp{AF} are usually optimized with gradient-based approaches. 
Thus, these `flat' areas of the \ac{AF} also lead to vanishing gradients, but this time of the \ac{AF}.
This affects the selection of the next sample in the \ac{BO} procedure.

The \logei~\cite{ament2024unexpected} \ac{AF} provides a numerically more stable implementation of \ac{EI}, mitigating the problem of vanishing gradients but not solving it, as we demonstrate next.
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/candidates_dist_sab_False.pdf}
    \vspace{-0.35em}
    \caption{Average distances between the initial and the final candidates of \logei for various model length scales and dimensionalities without RAASP sampling. Values in the gray region are numerically zero. In high dimensions, the gradient of the \ac{AF} vanishes, causing no movement of the gradient-based optimizer.}
\label{fig:acq_cands_sab_False}
\end{figure}
Fig.~\ref{fig:acq_cands_sab_False} shows the average distances the gradient-based optimizer travels for \logei. 
The surrogate is a \ac{GP} with \num{20} observations drawn uniformly at random. 
The unknown objective function is a realization of the same \ac{GP}.
We run \botorch's multi-start \ac{GD} \acl{AF} optimizer, initialized with \num{512} random samples and \num{5} random restarts, and measure the distances between the starting and end points of the gradient-based optimization of the \ac{AF}.
Average distances increase with~$\sqrt{d}$; thus the plot shows average distances between the starting and endpoints of the gradient-based optimization that are normalized by~$d^{-\frac{1}{2}}$.
Gray regions correspond to a numerically zero average distance, indicating vanishing gradients.
Vanishing gradients of the \ac{AF} remain a problem, even when using \logei and operating in moderate dimensions. 

\paragraph{Methods to Mitigate Vanishing Gradients of the \ac{AF}.}

One technique for handling vanishing gradients in the \ac{AF} optimization is \emph{locality}.
\turbo~\cite{eriksson2019scalable}, for example, uses \acp{TR} to constrain the optimization to a subregion of the search space.
Even if the \ac{GP} surrogate is uninformative, \turbo performs local search and can optimize high-dimensional problems even if data is scarce.
\turbo also uses \ac{RAASP} sampling~\cite{rashidi2024cylindrical,regis2013combining}, i.e., with a probability of $\min\left(1,\frac{20}{d}\right)$, it replaces the value of each dimension of the incumbent solution with a value drawn uniformly at random within the \ac{TR} bounds.
This process is repeated multiple times to create several candidates evaluated on a realization of the \ac{GP} posterior, a process known as \acl{TS}.
The point of maximum value is then chosen for evaluation in the next iteration of the \ac{BO} loop.
This design choice further enforces locality as new candidates only differ on average from the incumbent in \num{20} dimensions for $d\geq 20$.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/candidates_dist_sab_True_and_local_idxs}
    \vspace{-0.35em}
    \caption{\textbf{Left}: Average distances between the initial and the final candidates of \logei with \ac{RAASP} sampling. The vanishing gradient issue decreases. \textbf{Right:} Fraction of multi-start \ac{GD} candidates originating from the \ac{RAASP} samples when evaluating \logei on random samples. In high dimensions, \ac{RAASP} samples are increasingly more likely to get picked, even for longer length scales.}
    \label{fig:acq_cands_sab_True_and_raasp_candidates}
\end{figure}
Differing slightly from \turbo's approach, \ac{RAASP} sampling has been implemented in \botorch's \ac{AF} maximization and can optionally be enabled with the parameter \texttt{sample\_around\_best}.
\botorch augments a set of globally sampled candidates with the \ac{RAASP} samples, resulting in twice as many initial candidates.
It perturbs the best-performing points by replacing the value of each dimension with a probability of $\min\left(1,\frac{20}{d}\right)$ by a value drawn from a Gaussian distribution, truncated to stay within the bounds of the search space.
\botorch then chooses the points of maximum initial acquisition value to start the \ac{GD} optimization of the acquisition function.

With increasing dimensionality or descending length scale, the starting points for the multi-start \ac{GD} routine chosen by the \ac{AF} maximizer are increasingly more likely to originate from the \ac{RAASP} samples.
Fig.~\ref{fig:acq_cands_sab_True_and_raasp_candidates} (right panel) illustrates this.
Here, we draw realizations from \acp{GP}, initialized with different dimensionalities ($x$-axis) and length scales ($y$-axis).
For each realization, we maximize the acquisition function with \ac{RAASP} sampling and plot the percentage of candidates of maximum acquisition value originating from the \ac{RAASP} samples across all candidates. 
A higher percentage indicates a more `local' sampling.
We further average across five random restarts.
The percentage of \ac{RAASP} candidates with maximum acquisition value increases with the input dimensionality and decreases with the length scale.
\begin{figure}[tb]
    \centering
    \includegraphics[width=.75\linewidth]{figures/otsd_sab_d_100}
    \vspace{-0.35em}
    \caption{\acs{OTSD} for \botorch's \ac{AF} maximizer operating on a 100-dimensional space and \acp{GP} of various length scales with and without \ac{RAASP} sampling. The behavior of short-length-scale \acp{GP} reverts to local search ($\ell=0.05$ in the left panel) with \ac{RAASP} sampling and to local search without \ac{RAASP} sampling ($\ell=0.05$ and $\ell=0.28$ in the right panel). Shaded areas show the standard error of the mean obtained by 10 random repetitions. In the right panel, the blue line masks the black line.}
    \label{fig:otsd-sab}
\end{figure}
At the same time, those candidates stay close to the initial candidates.
This is shown in the bottom right of Fig.~\ref{fig:acq_cands_sab_True_and_raasp_candidates} (left panel), which shows the average distance traveled by candidate points of the \ac{AF} maximizer when using \ac{RAASP} sampling.
With \ac{RAASP} sampling, candidates travel a positive distance, visualized by the lack of gray color.
This indicates a reduction of the vanishing-gradient issues.
We attribute this to candidates being created close to the incumbent observations where the \ac{AF} is less likely to be flat. 

In general, when the length scales of the \ac{GP} are short and the dimensionality is high, \ac{BO} shows a local-search-like behavior with  \ac{RAASP} sampling and a random search behavior without it.
We demonstrate this using the \ac{OTSD}~\cite{cheng2025variational}, which quantifies an algorithm's exploration by finding the shortest path connecting all observations made up to a certain iteration.
The \ac{OTSD} curve of an algorithm $A$ consistently lying above the curve of algorithm $B$ indicates that algorithm $A$ is more explorative as its observations are more spread more evenly across $\mathcal{X}$.
The \ac{OTSD} is always monotonic increasing.
Fig.~\ref{fig:otsd-sab} shows the \acp{OTSD} for \num{100}-dimensional \acp{GP} with different length scales, each initialized with \num{10} random samples in the \ac{DOE} phase and subsequently optimized with \logei and \ac{RAASP} sampling for \num{20} iterations.
Unless the model length scale is sufficiently long for the \ac{AF} gradient not to vanish (as for $\ell=0.5$ in the right panel of Fig.~\ref{fig:otsd-sab}), the \ac{AF} maximizer picks one of the initial random candidates without further optimizing it.
This is supported by the trajectories for the \ac{BO} phase (iteration $\geq 9$) following the trajectory of the \ac{DOE} phase (iteration $< 9$) for $\ell=0.05$ and $\ell=0.28$ in the right panel of Fig.~\ref{fig:otsd-sab}.
We generally recommend the \ac{RAASP} sampling method as it improves \ac{BO} by automatically reverting to local search when encountering flat \acp{AF}.

\subsection{Bias-Variance Trade-off for fitting the GP}\label{subsec:problem-poor-model-fit}
\ac{GP} models are commonly fitted by maximizing the \ac{MLL}, either using unbiased \ac{MLE} estimation or using \ac{MAP}, which places a hyperprior on one or several \ac{GP} hyperparameters.
\ac{MLE} exhibits a higher variance and sensitivity to noise, particularly when fitting a model in high-dimensional spaces with scarce data.
\ac{MAP}, on the other hand, has a lower variance in the length-scale estimates but comes at the cost of bias unless accurate prior information is available.
The \ac{MLL} is given by
\begin{align}
    \begin{split}\log p(\bm{y} \vert X, \bm{\theta})  = \underbrace{-\frac{1}{2}\bm{y}^\intercal \left (K(X,X)+\sigma_n^2I \right )^{-1}\bm{y}}_{\text{data fit}}\\
    -\underbrace{\frac{1}{2}\log \lvert K(X,X)+\sigma_n^2I \rvert}_{\text{complexity penalty}} - \frac{n}{2}\log 2\pi 
    \end{split}\label{eq:marginal_likelihood_maintext}
\end{align}
The first and second terms are often called data fit and complexity penalty~\cite{williams2006gaussian}. For more details, see Appendix~\ref{app:background_bo_and_gps}.
This section explores the bias-variance trade-off between these two popular approaches for \ac{GP} model fitting.

\paragraph{\ac{MLE}.}
Fig.~\ref{fig:mle-brittle} shows the length scale obtained when using  (blue) or \ac{MAP} (orange) to fit a \ac{GP} surrogate model with an $\nicefrac{5}{2}$-ARD-Mat\'ern to a realization drawn from an isotropic \ac{GP} prior with length scale $\ell=1$ and noise term \num{e-8}.
We examine a~$10$ and a~$50$-dimensional GP and repeat each experiment \num{50}-times.
As before, the gradient-based optimizer starts with an initial length scale of $\frac{\sqrt{d}}{10}$.
We observe that the length scales estimated by \ac{MLE} vary significantly less for the 10-dimensional function than for the~$50$-dimensional one.
As we increase the number of observations on which the GP surrogate is conditioned, the variance of the estimated length scales decreases.
\begin{figure}[tb]
    \centering
    \includegraphics[width=.9\linewidth]{figures/map_mle_ls_spread_annotated}
    \vspace{-0.35em}
    \caption{Average length scales ($y$-axis) obtained by \ac{MLE} (blue) and \ac{MAP} (orange) for different numbers of randomly sampled observations ($x$-axis) for a 10- and for a 50-dimensional \ac{GP} prior sample. The obtained length scales differ substantially for the higher dimensional function if few points have been observed.}
    \label{fig:mle-brittle}
\end{figure}
\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figures/mll_plot}
    \vspace{-0.35em}
    \caption{The \acs{MLL} surface (bottom), the penalty (top row), and data fit terms (center) for various length scales and numbers of observations. Fewer observations lead to more erratic changes in the data fit term, leading to higher variance in the length scale estimates unless a prior gives additional shape to the surface.}
    \label{fig:mll_plot}
\end{figure}

The dotted curves in the bottom row of Fig.~\ref{fig:mll_plot} show the likelihood surface for \ac{MLE} as specified in Eq.~\eqref{eq:marginal_likelihood_maintext}.
The penalty term $\frac{1}{2}\log \vert K\vert$ and the data fit term $-\frac{1}{2}\bm{y}^\intercal \left (K(X,X)+\sigma_n^2I \right )^{-1}\bm{y}$ are shown in the first two rows, respectively;
the constant term $-\frac{n}{2}\log 2\pi$ is omitted from the figure. We account for the different number of samples between the left and right figures by scaling the penalty, data fit, MLE, and MAP terms with $s^{-1}$.
We show the surface for $s=5$ (left) and $s=50$ (right) data points.
As the length scales increase, the entries of the kernel matrix increase.
The determinant $\vert K \vert$ decays more quickly, and the penalty term decreases, adding a more distinct global trend to the likelihood surface.
In the limit, $\ell\rightarrow\infty$, the kernel matrix becomes a matrix of ones, and the determinant becomes zero.
In low dimensions, the data fit term decreases for long length scales, but the decreasing penalty compensates for this, resulting in a relatively flat \ac{MLL} surface.
The fast decay of the data fit term increases the ``signal-to-noise'' ratio, making it easier for the optimizer to converge in \num{10} than \num{100} dimensions. 
This can be seen by comparing the green and brown \ac{MLL} curves in Fig.~\ref{fig:mll_plot} for $s=50$ samples.
For more observations, \ac{MLL} becomes smoother in all dimensions, as indicated by the left vs. right panel in Fig.~\ref{fig:mll_plot}.



\paragraph{\ac{MAP}.}
\ac{MAP} allows for incorporating prior beliefs about reasonable values for hyperparameters.
However, practitioners often do not possess such prior information and hence resort to hyperpriors that reportedly perform well in benchmarks.
\citet{karvonen2023maximum} criticized this as an `arbitrary' determination of hyperparameters.

The orange distributions in Fig.~\ref{fig:mle-brittle} show the average length scales obtained by \ac{MAP} with a Gamma$(3,6)$ prior, which has been the default in \botorch before version \texttt{12.0}.
We use this prior as it has a substantial mass around its mode~$\nicefrac{1}{3}$, simplifying our analysis compared to wider priors, which reduce the difference between \ac{MLE} and \ac{MAP}.
Compared to \ac{MLE}, the \ac{MAP} estimates vary less but exhibit significant bias.
This is pronounced for the 50-dimensional \ac{GP} sample, where the \ac{MAP} estimates for the length scales revert to the prior mode for 100, 200, 500, and 1000 initial samples.
The solid lines in the lower row of Fig.~\ref{fig:mll_plot} show the surface for the \ac{MAP} estimation, using the same \ac{GP} sample as for \ac{MLE}.
The log prior term adds additional curvature, resulting in length scale estimates of lower variance.
This is particularly noticeable for little data (left column of Fig.~\ref{fig:mll_plot}) and consistent with Fig.~\ref{fig:mle-brittle}.
With more data, \ac{MLE} and \ac{MAP} become increasingly more similar, with \ac{MAP}'s log posterior decreasing faster for longer length scales due to the Gamma prior.



\section{Discussion}
\label{sec:discussion}
\paragraph{Experimental Setup and Benchmarks.}
\label{subsec:mle-enough-for-hdbo}
\begin{figure}[tb]
    \centering
    \includegraphics[width=.8\linewidth]{figures/res_mle_enough.pdf}
    \vspace{-0.35em}
    \caption{\ac{BO} with the `scaled' initialization of \ac{MLE} performs comparably to the state-of-the-art in \ac{HDBO}. 
    }
    \label{fig:mle_is_enough}
\end{figure}
We propose a simple initialization for the gradient-based optimizer used for fitting the length scales of the \ac{GP} surrogate \emph{via} \ac{MLE} and evaluate its performance for \ac{BO} tasks. 
In what follows, we suppose a \ac{BO} algorithm with a $\nicefrac{5}{2}$-ARD-Mat\'ern kernel and \logei~\cite{ament2024unexpected}.
To address the issue of vanishing gradients at the start of the \ac{MLE} optimization, we choose the initial length scale as 0.1 and scale with $\sqrt{d}$ to account for the increasing distances of the randomly sampled \ac{DOE} points.
Thus, the initial length scale used in the optimization is~$\frac{\sqrt{d}}{10}$, and we refer to this new Bayesian optimization method as `MLE scaled'.
The second method in the evaluation is the same BO method, but now using the default value $\ell = \ln 2$ of \texttt{GPyTorch} as initial length scale in \ac{MLE}. This value is not scaled with~$d$.
Based on our analysis of the impact of \ac{RAASP} sampling when optimizing the \ac{AF}, we combine `MLE scaled' with \ac{RAASP} sampling and call the method `MLE Scaled with RAASP' (\texttt{MSR}).  
The fifth method \dsp is the new default in \texttt{BoTorch}~\cite{balandat2020botorch}, which uses a \ac{MAP} estimate of length scales and initializes the optimization with the mode of the \ac{DSP}~\citep{hvarfner2024vanilla}. 
We also compare against \saasbo~\cite{eriksson2021high} and \bounce~\cite{papenmeier2023bounce}.
\saasbo has a large computational runtime. Hence, we run it only for~500 iterations and terminate runs that exceed 72 hours. 
That is why \saasbo has fewer evaluations for \texttt{Ant} and \texttt{Humanoid}.

Our benchmarks are the $124$-dimensional soft-constrained version of the \texttt{Mopta08} benchmark~\cite{jones2008large} introduced by \citet{eriksson2021high}, the 180-dimensional \texttt{Lasso-DNA} benchmark~\cite{vsehic2022lassobench}, as well as two 888- and 6392-dimensional \texttt{Mujoco} benchmarks used by~\citet{hvarfner2024vanilla}.
The first two benchmarks are noise-free, while the others exhibit observational noise.

\paragraph{MLE Works Well for HDBO.}
Fig.~\ref{fig:mle_is_enough} shows the performance of the BO methods on the four real-world applications. 
Each plot gives the average objective value of the best solution found so far in terms of the number of iterations. 
We show the ranking of the methods according to the final performance in Table~\ref{tab:optimizer_ranking} in Appendix~\ref{app:ranking}.
The confidence bands indicate the standard error of the mean.
MSR achieves competitive performance across all benchmarks, matching the SOTA \texttt{DSP}. 
Notably, MSR outperforms \ac{DSP} on $124d$ \texttt{Mopta08} and $888d$ \texttt{Ant}, and performs slightly worse than \texttt{DSP} other benchmarks. 
\bounce~\cite{papenmeier2023bounce} is consistently outperformed by MSR, \ac{MLE}, and \ac{DSP} but surpasses \saasbo~\citep{eriksson2021high}, especially on \texttt{Ant}.
Although the constant length scale initialization ($\ell = \ln 2$) without \ac{RAASP} achieves satisfactory results on lower-dimensional benchmarks such as $124d$ \texttt{Mopta08} and $180d$ \texttt{Lasso-DNA}, it fails on higher-dimensional benchmarks like $888d$ \texttt{Ant} and $6392d$ \texttt{Humanoid}. 
We attribute this breakdown to vanishing gradients as shown in Fig.~\ref{fig:mean_ls_grads} in Appendix~\ref{subsec:mean_grads_mle}. 

Fig.~\ref{fig:mean_length_scales} compares the mean length scales per BO iteration, averaged over dimensions and 15 repetitions.
\begin{figure}[tb]
    \centering
    \includegraphics[width=.8\linewidth]{figures/mean_length_scales}
    \vspace{-0.35em}
    \caption{Average length scales of \texttt{MSR} and the other methods. \ac{RAASP} sampling gives more consistent length scale estimates. 
    }
    \label{fig:mean_length_scales}
\end{figure}
For the 124-dimensional \texttt{Mopta08} and 180-dimensional \texttt{Lasso-DNA} applications, `MLE ($\ell=\ln 2$)' learns length scales similar to the other methods.
However, this constant initialization strategy fails to make progress from the initial value for the more high-dimensional problems in the bottom row of Fig.~\ref{fig:mean_length_scales}.
We attribute this to vanishing gradients of the \ac{MLL} as discussed in Sec.~\ref{sec:facets-of-cod} and highlighted in Fig.~\ref{fig:mean_ls_grads} in Appendix~\ref{subsec:mean_grads_mle}.


\paragraph{RAASP Reduces Variance.}
Fig.~\ref{fig:mean_length_scales} shows a surprising behavior for \texttt{DSP}.
As one would anticipate, the estimated length scales are typically close to the mode of the hyperprior at the start of the optimization.
However, they then converge to an even higher value on all benchmarks but the \num{6392}-dimensional \texttt{Humanoid} benchmark.
Furthermore, the deviation from the prior mode is more pronounced for the lower-dimensional benchmarks, being in line with our analysis of the bias-variance trade-off in Sec.~\ref{subsec:problem-poor-model-fit}.
\ac{MLE} with scaled initial length scales (`MLE (scaled)') initially has higher length scale values than all other methods. 
The resulting estimates vary significantly for the high-dimensional \texttt{Ant} and \texttt{Humanoid} problems, supporting our analysis in Sec.~\ref{subsec:problem-poor-model-fit} where we study the comparatively high variance of \ac{MLE} compared to \ac{MAP}.

The length scales obtained by \texttt{MSR} lie between the values of \texttt{DSP} and of `\ac{MLE} without \ac{RAASP} sampling' (green dots, `MLE (scaled)') for most benchmarks. 
An exception is \texttt{Ant} where \texttt{MSR} sometimes results in shorter length scales than~\texttt{DSP}.
Overall, the \ac{RAASP} sampling, which is the only difference between \texttt{MSR} and `MLE (scaled)', obtains more consistent length scale estimates.



\paragraph{RAASP Promotes Locality.}
\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{figures/otsd}
    \vspace{-0.35em}
    \caption{\texttt{DSP} exhibits the least exploration. \ac{MLE} with fixed initial length scales performs like random search on \texttt{Ant} and \texttt{Humanoid}. 
    }
    \label{fig:otsd-real-world}
\end{figure}
Fig.~\ref{fig:otsd-real-world} compares the amount of exploration that the algorithms perform through the lens of the \ac{OTSD} metric; see Section~\ref{subsec:problem-flat-acquisition} for details on \ac{OTSD}.
We observe that \texttt{DSP} (blue curves) is the most exploitative method on all benchmarks, being in line with the fact that, after \ac{MLE} with constant length scale initialization (`MLE ($\ell=\ln 2$)'), \texttt{DSP} has the shortest length scales on most benchmarks.
The fact that `MLE ($\ell=\ln 2$)' is the most \emph{explorative} method, coinciding with the `random search' line in Fig.~\ref{fig:otsd-real-world}, despite having the \emph{shortest} length scales is explained by our analysis in Sec.~\ref{subsec:problem-flat-acquisition}:
the random initial points for the \ac{AF} optimization are not further optimized if the gradient of the \ac{AF} vanishes, because the method does not employ \ac{RAASP} sampling. 
We observe that `MLE ($\ell=\ln 2$)' does not learn longer length scales during the optimization for \texttt{Ant} and \texttt{Humanoid}, as indicated by the horizontal brown line in in Fig.~\ref{fig:mean_length_scales}. 
Thus, it does not recover later. 
For \texttt{Mopta08} and \texttt{Lasso-DNA}, which have a lower input dimension, the effect is less pronounced because `MLE ($\ell=\ln 2$)' sometimes learns longer length scales that avoid vanishing gradients of the \ac{AF}.
\ac{MLE} with scaled initial length scale values (green curves in Fig.~\ref{fig:otsd-real-world}) is the second-most explorative method.
However, this is not due to vanishing gradients but caused by overly long length scales, shown as green dots in Fig.~\ref{fig:mean_length_scales}.
\texttt{MSR} (red curves) is more explorative than \texttt{DSP} and more exploitative than `MLE (scaled)', which does not use \ac{RAASP} sampling.
This is consistent with the shorter length scales of \texttt{MSR} (red dots in Fig.~\ref{fig:mean_length_scales}), confirming that \texttt{MSR} not only yields more consistent length scale estimates but also acts more local than its \ac{RAASP}-sampling-free equivalent.

\paragraph{Notes on Popular HDBO Benchmarks.}
In Fig.~\ref{fig:mean_length_scales}, all methods converge to similarly long length scales on the \texttt{Mopta08} and \texttt{Lasso-DNA} benchmarks.
This is likely attributable to a specific property of these popular benchmarks, as we discuss in Appendix~\ref{app:simple-benchmarks}.
In short, these benchmarks have a simple structure that benefits models with long length scales, posing the risk of algorithms being `overfitted' to these benchmarks.



\section{Conclusions and Future Work}
Our analysis reveals fundamental challenges in HDBO while offering practical insights for improving HDBO methods.
We demonstrate that common approaches for fitting Gaussian processes cause vanishing gradients in high dimensions. We propose an initialization for \ac{MLE} that achieves state-of-the-art performance on established \ac{HDBO} benchmarks without requiring the assumptions of maximum a-posteriori estimation. 
Finally, we provide empirical evidence that combining \ac{MLE} with \ac{RAASP} sampling reduces the variance of the length scale estimates and yields values closer to the ones learned by \dsp, providing a fresh argument for the inclusion of \ac{RAASP} sampling in \ac{HDBO}.

In future work, we will continue to carefully vet popular benchmarks and propose novel, challenging benchmarks that preserve the traits of real-world applications.
Furthermore, this work emphasizes the importance of numerical stability for the performance of Bayesian optimization in high dimensions.
Thus, we propose approaching the development of models and acquisition functions from this perspective.


\paragraph{Impact.} This paper presents work that aims to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\section*{Acknowledgements}
This project was partly supported by the Wallenberg AI, Autonomous Systems, and Software program (WASP) funded by the Knut and Alice Wallenberg Foundation.
The computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS), partially funded by the Swedish Research Council through grant agreement no. 2022-06725


\bibliography{references}
\bibliographystyle{icml2025}


\newpage
\appendix
\acresetall
\onecolumn
\section{A Review of Gaussian Processes and Bayesian Optimization}\label{app:background_bo_and_gps}

\subsection{Gaussian Processes}
\acp{GP} model a distribution over functions, i.e., assume that $f$ is drawn from a \ac{GP}: $f\sim \mathcal{GP}(m(\bm{x}), k(\bm{x},\bm{x}')+\sigma_n^2\mathds{1}_{[\bm{x}=\bm{x}']})$ where $m$ and $k$ are the mean and covariance function of the \ac{GP}, respectively~\cite{williams2006gaussian}.
Common kernel functions include the \ac{RBF} kernel: 
\begin{align}
    k_{\text{RBF}}(\bm{x},\bm{x}')=\sigma_f^2\exp\left (-\frac{r}{2} \right ), \label{eq:rbf_kernel}
\end{align} or the $\nicefrac{5}{2}$\ac{ARD}-Mat\'ern kernel: 
\begin{align}
    k_{\text{Mat\nicefrac{5}{2}}}(\bm{x},\bm{x}')= \sigma_f^2\left (1+\sqrt{5r}+\frac{5r}{3} \right )\exp \left (-\sqrt{5r} \right) \label{eq:matern_kernel}
\end{align} with $r=\sum_{i=1}^d\frac{(x_i-x_i')^2}{\ell_i^2}$.
Here, $\bm{\ell}$ is a $d$-dimensional vector of component-wise length scales.
Thus, the kernel's number of \acp{HP} in Eq.~\eqref{eq:matern_kernel} is $d+1$.

Given some training data $\mathcal{D}\coloneqq\{(\bm{x}_1,y_1),\ldots,(\bm{x}_N,y_N)\},\, X\coloneqq (\bm{x}_1^\intercal,\ldots,\bm{x}_N^\intercal)^\intercal,\, \bm{y}=(y_1\ldots,y_N)^\intercal$, the function values $\bm{y}_*$ of a set of query points $X_*$ is normally distributed as
\begin{align}
    \bm{y}_* \vert X,\bm{y},X_* \sim\, &\mathcal{N}(K(X_*,X)(K(X,X)+\sigma_n^2I)^{-1}\bm{y},\\
    & K(X_*,X_*)-K(X_*,X)(K(X,X)^{-1}+\sigma_n^2I)K(X,X_*))
\end{align}

Let $\bm{\theta}=\{\sigma_n^2, \sigma_f^2, \bm{\ell}\}$ be the set of \ac{GP} hyperparameters.
The \ac{GP} surrogate is then typically fitted by maximizing the marginal log-likelihood w.r.t. $\bm{\theta}$, also known as \ac{MLE}, i.e.
\begin{align}
    \bm{\theta}^*&\in\argmax_{\bm{\theta}} \log p(\bm{y} \vert X, \bm{\theta})\\
    \log p(\bm{y} \vert X, \bm{\theta})  &= -\frac{1}{2}\bm{y}^\intercal \left (K(X,X)+\sigma_n^2I \right )^{-1}\bm{y}-\frac{1}{2}\log \lvert K(X,X)+\sigma_n^2I \rvert - \frac{n}{2}\log 2\pi \label{eq:marginal_likelihood}
\end{align}

With a gradient-based approach, this is done by maximizing Eq.~\eqref{eq:marginal_likelihood}, which is usually multi-modal and difficult to optimize.
The gradient of the marginal log-likelihood w.r.t. is given by
\begin{align}
    \frac{\partial}{\partial\theta_i}\log p(\bm{y} \vert X, \bm{\theta}) &= \frac{1}{2}\bm{y}^\intercal \left (K+\sigma_n^2I\right )^{-1}\frac{\partial K}{\partial \theta_i}\left (K+\sigma_n^2I\right )^{-1}\bm{y}-\frac{1}{2}\text{tr}\left (\left (K+\sigma_n^2I\right )^{-1}\frac{\partial K}{\partial \theta_i} \right ),\label{eq:grad-likelihood}
\end{align}
where $\frac{\partial K}{\partial \theta_i}$ is the symmetric Jacobian matrix of partial derivatives w.r.t. $\theta_i$.

One often endows the \ac{GP} hyperparameters with hyperpriors and seeks the mode of the posterior distribution, known as \ac{MAP}:
\begin{align}
    \bm{\theta}^*\in\argmax_{\bm{\theta}} \log p(\bm{y} \vert X, \bm{\theta})+\log p(\bm{\theta}).\label{eq:map1}
\end{align}

\subsection{Bayesian Optimization}\label{subsec:bo}

\ac{BO} is an iterative approach, alternating between fitting the model and choosing query points.
Query points are found by maximizing an \ac{AF}, e.g., \ac{EI}~\cite{mockus2005bayesian}.
The \ac{EI} \ac{AF} measures how much observing a point $\bm{x}$ is expected to improve over the best function value observed thus far.
It is defined as 
\begin{align}
    \text{EI}(\bm{x}) = \mathbb{E}_{f(\bm{x})}\left [ \left [ f(\bm{x})-y^\star\right ]_+ \right ] = \left (\mu_N(\bm{x}) - y^\star \right )\Phi(Z) + \sigma_N(\bm{x})\phi(Z)\label{eq:ei}
\end{align}
with $Z=\frac{\mu(\bm{x})-y^\star}{\sigma(\bm{x})}$, $\Phi$ and $\phi$ being the standard normal \ac{CDF} and \ac{PDF}, and $\mu_N$ and $\sigma^2_N$ being the posterior mean and posterior variance at $\bm{x}$, i.e.,
\begin{align}
    \mu_N(\bm{x}) &= K(\bm{x},X)(K(X,X)+\sigma_n^2I)^{-1}\bm{y}\\
    \sigma_N^2(\bm{x}) &= (k(\bm{x},\bm{x})+\sigma_n^2)(K(X,X)+\sigma_n^2I)^{-1}K(X,\bm{x})
\end{align}

As discussed by~\cite{ament2024unexpected}, \ac{EI} often suffers from vanishing gradients, which only worsens in high-dimensional spaces due to the plethora of flat regions.
\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{figures/ei_flat_regions.pdf}
    \caption{Distribution of \ac{EI} values for \acp{GP} in various dimensionalities. When conditioning on the same amount of data points and maintaining the length scale as the dimensionality grows, the distribution of \ac{EI} values becomes more peaked.}
    \label{fig:ei_flat_with_dims}
\end{figure}
This is shown in Fig.~\ref{fig:ei_flat_with_dims}.
Here, we condition a \ac{GP} on \num{100} random points in $[0,1]^d$ for which we obtain function values by drawing from a \ac{GP} prior with an isotropic \ac{RBF} kernel with $\ell=10$.
We evaluate the \ac{AF} on \num{2000} points drawn from a scrambled Sobol sequence and plot the histograms for various dimensionalities. 
As the dimensionality grows, there are more equal \ac{EI} values, indicating flat regions in the \ac{AF} and possible problems with vanishing gradients of the \ac{EI} function.
\cite{ament2024unexpected} propose \logei, a numerically more stable version of \ac{EI} that solves many of the numerical issues with \ac{EI}.

\FloatBarrier

\section{Additional Experiments}

\subsection{Ranking of Optimization Algorithms}
\label{app:ranking}

\begin{table}[H]
    \centering
    \begin{tabular}{lccccc}
        \toprule
         &  MSR & DSP & Bounce & MLE (scaled) & MLE $(\ell=\ln 2)$ \\
         \midrule
         \texttt{Mopta08} $(d=124)$ & 1 & 2 & 5 & 3 & 4\\
         \texttt{Lasso-DNA} $(d=180)$ & 4 & 1 & 5 & 3 & 2\\
         \texttt{Ant} $(d=888)$ & 2 & 4 & 3 & 1 & 5 \\
         \texttt{Humanoid} $(d=6392)$ & 2 & 1 & - & 3 & 4 \\
         \bottomrule
         & 
    \end{tabular}
    \caption{Ranking for the different optimizers on the benchmark problems according to their final performance. \texttt{SAASBO} is excluded from the comparison as it was not run for the entirety of the optimization; \texttt{Bounce} ran into memory issues on \texttt{Humanoid} and, therefore, does not have a rank on this benchmark.}
    \label{tab:optimizer_ranking}
\end{table}

\subsection{MLE Gradients for Real-World Experiments}
\label{subsec:mean_grads_mle}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/ls_grads.pdf}
    \caption{Mean absolute value of the gradients for the different MLE methods, including the proposed \texttt{MSR}. The constant length scale initialization exhibits vanishing gradients for the high-dimensional \texttt{Ant} and \texttt{Humanoid} problems.}
    \label{fig:mean_ls_grads}
\end{figure}

Complementing our analysis in Sec.~\ref{sec:discussion}, Fig.~\ref{fig:mean_ls_grads} shows the average absolute gradients of the different \ac{MLE} methods, including our proposed \texttt{MSR} method.
The constant length scale initialization (`MLE ($\ell=\ln 2$)') is the only method consistently exhibiting vanishing gradients on the $888$-dimensional \texttt{Ant} and the $6392$-dimensional \texttt{Humanoid} problems as depicted by the solid orange lines for those benchmarks. 
On \texttt{Mopta08} and \texttt{Lasso DNA}, all methods have non-vanishing gradients.
Furthermore, both \ac{MLE} methods scaling the initial length scale do not suffer from vanishing gradients on \texttt{Ant} and \texttt{Humanoid}.



\FloatBarrier

\subsection{High-dimensional Benchmark Functions}
\label{subsec:highdim-synth-results}

By the no-free-lunch theorem~\cite{wolpert1997no}, the relative performance of an optimization algorithm depends on the properties of the problems it operates on.
Here, we show that for several benchmark problems, no state-of-the-art algorithm strictly dominates the other methods.

\begin{table}[tb]
    \centering
    \begin{tabular}{llll}
         \toprule 
         Benchmark & Rank 1 & Rank 2 & Rank 3\\
         \midrule
         \texttt{Levy100} & \turbo & \dsp & \cmaes\\
         \texttt{Schwefel100} & \cmaes & \dsp & \turbo\\
         \texttt{Griewank100} & \dsp & \turbo & \cmaes\\
         \bottomrule
    \end{tabular}
    \caption{Relative performances of \cmaes, \dsp, and \turbo after optimizing for 1000 iterations averaged over 10 repetitions. No algorithm performs best for all benchmarks.}
    \label{tab:no-free-lunch}
\end{table}
Table~\ref{tab:no-free-lunch} shows the relative performances of \cmaes, \dsp, and \turbo after 1000 optimization steps on the 100-dimensional versions of the \texttt{Levy}, \texttt{Schwefel}, and \texttt{Griewank} benchmarks.
We evaluate \texttt{Levy} in the bounds $[-10,10]$, \texttt{Schwefel} in the bounds $[-500,500]$, and \texttt{Griewank} in the bounds $[-600,600]$ by scaling from the unit hypercube in which the \ac{GP} operates to the respective bounds before evaluating the function.

\begin{figure}[tb]
    \centering
    \includegraphics[width=.5\linewidth]{figures/compare_opt_perf_schwefel100}
    \caption{\acs{OTSD} (solid lines) and performance curves (dashed lines) of the 100-dimensional \texttt{Schwefel} function}
    \label{fig:tsp-schwefel100}
\end{figure}

To better understand the reason for the performance differences, we study the \ac{OTSD} for the different functions, shown in Fig.~\ref{fig:tsp-schwefel100}. %
Plots of the 2-dimensional versions of all three benchmarks are shown in Fig.~\ref{fig:levy-griewank-schwefel} and the performance and \ac{OTSD} plot of 100-dimensional \texttt{Levy} figure can be found in Fig.~\ref{fig:100d-synth-results}.
\cmaes shows the lowest level of exploration and has the lowest \ac{OTSD} on the \texttt{Schwefel} function, where it outperforms the two other algorithms.
On \texttt{Griewank} (see Fig.~\ref{fig:tsp-griewank100}), \dsp has the highest average \ac{OTSD} performs best while the less explorative \cmaes shows the worst performance.
For \texttt{Levy}, both \turbo and \dsp are relatively explorative and outperform \cmaes by a considerable margin.
We conclude that more explorative algorithms are advantageous on the benchmarks with a clear global trend like \texttt{Griewank}, which resembles a paraboloid, and \texttt{Levy}, which has a parabolic shape along the $x_1$ dimension (see Fig.~\ref{fig:levy-griewank-schwefel}).
In contrast, the \texttt{Schwefel} benchmark is more ``stationary'' in that a point's function value depends less on that point's absolute position in the space.
Noteworthy, stationarity as assumed by \ac{GP} models with a stationary covariance function, which, by far, are the most common covariance functions for \ac{HDBO}.
A more local approach such as \cmaes is beneficial on this highly multi-modal benchmark.



\begin{figure}[H]
    \centering
    \includegraphics[width=.5\textwidth]{figures/compare_opt_perf_levy100.pdf}
    \caption{\acs{OTSD} (solid lines) and performance curves (dashed lines) of the 100-dimensional \texttt{Levy} function}
    \label{fig:100d-synth-results}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=.5\linewidth]{figures/compare_opt_perf_griewank100}
    \caption{\acs{OTSD} (solid lines) and performance curves (dashed lines) of the 100-dimensional \texttt{Griewank} function}
    \label{fig:tsp-griewank100}
\end{figure}

Figs.~\ref{fig:100d-synth-results} and~\ref{fig:tsp-griewank100} show the \ac{OTSD} and performance plots for the 100-dimensional \texttt{Levy} and \texttt{Griewank} functions.
Fig.~\ref{fig:levy-griewank-schwefel} shows the two-dimensional versions of the \texttt{Levy}, \texttt{Griewank}, and \texttt{Schwefel} benchmark functions.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/levy_griewank_schwefel.pdf}
    \caption{The two-dimensional versions of the \texttt{Levy}, \texttt{Griewank}, and \texttt{Schwefel} benchmark functions used above.}
    \label{fig:levy-griewank-schwefel}
\end{figure}

\FloatBarrier

\subsection{Hard Optimization Problems}\label{subsec:hard-problems}
\begin{figure}
    \centering
    \includegraphics[width=.5\linewidth]{figures/gp_prior/GP_prior_0.025_sab_True_8_raw_samples}
    \caption{\logei run on a two-dimensional \ac{GP} prior sample for \num{100} evaluations. The right panel shows the posterior mean at the end of the optimization. For highly multimodal benchmarks, \ac{EI} reverts to a local search behavior and does not obtain a global optimum (red cross).}
    \label{fig:2d_gp_prior_miss_optimum}
\end{figure}
We reiterate that the \ac{COD} remains a reality and exists even for low-dimensional problems.
Fig.~\ref{fig:2d_gp_prior_miss_optimum} shows 100 evaluations made by \ac{EI} on a 2-dimensional \ac{GP} prior sample as the benchmark function.
There is no model mismatch; the length scales of the surrogate model are set to the correct value ($\ell=0.025$).
However, \ac{EI} operates locally and fails to find the global optimum (marked by a red cross).

\FloatBarrier

\section{Popular Benchmarks Seem Simpler Than Expected}\label{app:simple-benchmarks}
This section examines two popular high-dimensional benchmarks, the $180d$ \texttt{Lasso-DNA} and $124d$ \texttt{Mopta08}. 
In what follows, we will demonstrate that many input variables seem to have little influence on the objective value.
These empirical findings suggest that these benchmarks are not truly as high-dimensional as their nominal number of input variables might suggest, potentially misleading the perceived difficulty of these benchmarks and confounding the assessment of what state-of-the-art algorithms will be able to accomplish in practice.

We begin by collecting the top 10\% best points identified by the SOTA algorithm using \ac{DSP}~\cite{hvarfner2024vanilla} for each benchmark. 
For each dimension~$x_i$ separately, we then count how often these points lie on the boundary of the search space. 
If over half of the points place~$x_i$ on the boundary, we label~$x_i$ as \textit{secondary}, and as \textit{dominant} otherwise. 
To ensure consistency across runs, we perform \num{15} repetitions and consider those dimensions that have been identified as dominant in eight or more of the repetitions.

Table~\ref{tab:sparseness} reports the number of dominant and secondary dimensions. 
In both benchmarks, a large fraction of the dimensions are classified as \emph{secondary}, meaning that the best solutions obtained by \ac{DSP} method set the corresponding input variables to value near the boundary of the search space.
Note that our finding is directionally aligned with the $43$ active dimensions that \citet{vsehic2022lassobench} reported for \texttt{Lasso-DNA}, using a different method to estimate the number of relevant dimensions.

We further investigate the impact of each set of dimensions by replacing, at each iteration, either the dominant dimensions ($f_{\textrm{dominant}}$) or the secondary dimensions ($f_{\textrm{secondary}}$) with randomly chosen binary values. 
The rows $\bar{f}_{\textrm{dominant}}$ and $\bar{f}_{\textrm{secondary}}$ in Table~\ref{tab:sparseness} indicate the corresponding average objective values (with standard errors in parentheses). Replacing secondary dimensions impairs the result only slightly, whereas randomizing the dominant dimensions yields a markedly larger performance drop. A two-sided Wilcoxon test shows that all differences are statistically significant at $p < 0.001$.

\begin{table}[]
    \centering
    \begin{tabular}{lcc}
         \toprule
         & \texttt{Lasso-DNA} (d=$180$) & \texttt{Mopta08} (d=$124$)\\
         \midrule
         \# dominant & 69.33 & 30.80\\
         \# secondary & 110.67 & 93.20\\
         $\bar{f}_{\textrm{dominant}}$ & 0.315 ($\pm \num{4e-4}$) & 328.824 ($\pm 1.920 )$\\
         $\bar{f}_{\textrm{secondary}}$ & 0.445 ($\pm \num{4e-3}$) & 430.474 ($\pm 8.164$)\\
         $\bar{f}_{\textrm{rand}}$ & 0.410 ($\pm \num{3.3e-2}$) & 403.428 ($\pm 48.976$)\\
         \bottomrule
    \end{tabular}
    \caption{The number of dimensions identified as dominant and secondary and the average function values obtained when replacing the dominant ($\bar{f}_1$) or secondary ($\bar{f}_2$) parameters with uniformly random values. The average values and standard deviations for uniformly random points are shown as $\bar{f}_{\textrm{rand}}$. Replacing secondary parameters harms performance considerably less than replacing dominant parameters.}
    \label{tab:sparseness}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=.5\linewidth]{figures/active_inactive_dims_mopta08_lasso.pdf}
    \caption{The mean average length scales of ``dominant'' and ``secondary'' dimensions for the \texttt{Mopta08} (left) and \texttt{Lasso-DNA} (right) benchmarks for \ac{DSP}.}
    \label{fig:active_inactive}
\end{figure}

Next, we examine how Gaussian process (\ac{GP}) models account for this structure during \ac{HDBO}. 
As illustrated in Fig.~\ref{fig:active_inactive}, the estimated length scales of secondary dimensions are typically large, which is consistent with the observation that they influence the (predicted) objective value less. 
Intuitively, if a dimension’s optimal value often lies at the boundary, the \ac{GP} can assign it a very large length scale, learning the trend toward the boundary with few evaluations and leaving more of the budget to explore the truly dominant dimensions.
In effect, the \ac{BO} loop focuses on those dimensions that genuinely drive performance.

Below, we demonstrate that for these problems, BO will set most of the input variables to exactly zero or one --that is, to the boundary of the search space-- regardless of whether the GP is fit by~\ac{MLE} or~\ac{MAP}.
Thus, we conclude that the task effectively has a lower dimensionality than its nominal number of input variables.
While this property was already recognized for \texttt{Lasso-DNA}, our results demonstrate for the first time that it also applies to \texttt{Mopta08}. 
Furthermore, it is interesting to note that a simple GP surrogate fitted via \ac{MLE} or \ac{MAP} can leverage this structure, a behavior that had not been previously observed.

Above, we showed that 1) dimensions of the best points observed by MLE and MAP that predominantly lie on the border have significantly longer length scales than the ``dominant'' dimensions, and 2) most of the dimensions have marginal impact.
\ac{MLE} shows a similar behavior and is omitted for brevity.
We further omit the 388-dimensional \texttt{SVM} benchmark as it is known to have a low-dimensional effective subspace~\cite{eriksson2021high}.

To complement our analysis, we show that \ac{MLE} and \ac{MAP} assign dominant dimensions mainly values at the boundary.
This indicates that the GP model actually makes use of the specific characteristics of these benchmarks. 

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/n_border_params_vbo/mopta08_frac_border.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/n_border_params_vbo/lasso-dna_frac_border.pdf}
    \end{subfigure}
    \caption{Fraction of dimensions set to a value at the border (0 or 1) by \ac{DSP}. The shaded area shows the standard error of the mean across 15 repetitions.}
    \label{fig:frac_dims_at_border}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/mopta08_ones_mle.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/lasso_ones_mle.pdf}
    \end{subfigure}
    \caption{Fraction of dimensions set to a value at the border (0 or 1) by our \ac{MLE} method. The shaded area shows the standard error of the mean across 15 repetitions.}
    \label{fig:frac_dims_at_border_mle}
\end{figure*}

Figs.~\ref{fig:frac_dims_at_border} and~\ref{fig:frac_dims_at_border_mle} further show that \ac{BO} consistently evaluates a large share of the parameters at the border.
Fig.~\ref{fig:frac_dims_at_border} shows this for \ac{DSP} by \cite{hvarfner2024vanilla} whereas Fig.~\ref{fig:frac_dims_at_border_mle} uses \ac{MLE} as proposed in Section~\ref{subsec:mle-enough-for-hdbo}.
The general trend is that, during the course of the optimization, increasingly many parameters are evaluated at the border, which is consistent with Fig.~\ref{fig:active_inactive}.

We thus argue that two \ac{HDBO} benchmarks do not fully capture the complexity of \ac{HDBO} because of this simple \ structure.
While this is to be expected for \texttt{Lasso-DNA} and \texttt{SVM}, this property has not been discussed for the soft-constrained \texttt{Mopta08} benchmark introduced in~\cite{eriksson2019scalable} to the best of our knowledge.



\end{document}
