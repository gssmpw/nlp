\section{Literature Review}
This section provides existing literature on the origins of digital twins and their technologies.
Recent years have shown significant developmental progress in digital twins in academia and industry (\cite{wu2020}). The digitization concept has also helped address issues related to improved manufacturing quality, operation objectives, and conditions during the production of machinery energy. The potential of digital twin technology to revolutionize these areas is a cause for optimism. Countries, institutes, and industries worldwide, such as Massachusetts Institute of Technology (MIT) in the United States, Siemens in Germany, and China Nuclear Power Research and Design Institute, have also applied digital twin technology (\cite{mengyan2024digitaltwin}).\\
What is a digital twin? The Cambridge dictionary defines digital as "to record" or "information storage as digits of 1's and 0's, to show the presence of signals", while "Twin," as it is generally known, is one of two things containing or consisting of two matching or corresponding parts linked together. 
Digital Twin started relatively as 3D Computer-Aided Design (CAD) geometry and all design requirements for a product (including notation and parts lists). These Digital Mock-Ups are no longer mere digital copies but now exchange information with their real-life counterparts via a series of attached sensorsâ€”making them digital twins. Current experience from domestic and foreign manufacturing industries has made it evident that the product model defined by 3D digitalisation has grown and evolved, and its benefits have been repeatedly verified (\cite{xiong2022}).
\cite{jeong2022} in his paper, defined a digital twin as a replica of "physical objects (e.g., people, objects, spaces, systems, and processes) in the real world into digital objects in the digital world" to address various real-world problems and optimise the natural world through simulation or prediction of situations that can occur in the future. This concept of the digital twin, which was a paradigm shift in technological advancement, was introduced when the National Aeronautics and Space Administration (NASA) decided to create the physical twin of a space aircraft within the Apollo Program to reproduce its behaviour in space (\cite{wang2020digital}). The work of Michael Grieves with John Vickers of NASA presented the concept of the product lifecycle, which is from the physical product, a virtual representation of that product, and the bi-directional data connection that feeds data from the physical to the virtual and vice versa (\cite{jones2020}), (\cite{macias2024}). The digital twin concept refers to a digital twin instead of a physical one, composed of a physical part, a digital part, and interconnectivity for data transfers. 
The digital twin requires high integration between the digital and physical parts through data transfer. The level of integration consists of a digital model representing a physical entity, a digital shadow representing a uni-directional inflow of information (Physical-to-Virtual), and a digital twin representing a bi-directional inflow of information (Virtual to physical). This integration is possible by using sensors installed in the physical object parts to reflect the digital objects. Similarly, the digital object can change the physical state through these sensors and actuators. According to \cite{kritzinger2018}, the level of integration of a digital twin with its physical counterpart has all to do with the level of data integration, which he also identified in three levels: the digital model, digital shadow, and the digital twin. 
\subsection{Digital Twin Technologies}
For a digital twin to exist, data must flow in and out between the digital and physical objects in real-time. Sensors and actuators make this flow possible, and various technologies collect and store data in real time. \cite{attaran2023}, gave four different types of technologies. These are The Internet of Things (IoT), Artificial Intelligence (AI), Extended Reality (XR), and the Cloud. A particular technology depends on the digital twin use case, including industrial public and personal areas. These technologies can used for insightful information in visualisation and operation technology, analysis technology, multidimensional modeling and simulation technology, connection technology, data and security technology, and synchronisation technology. 

\subsection{Data Fusion}
Data fusion, also known as information fusion, is a process that combines data or information from various sources to enhanced decision-making. In the context of data fusion architectures, the terms' information,' 'data,' and 'knowledge' describe the hierarchical order levels of a data fusion process. Whether referred to as information fusion or data fusion, the primary focus is on the fundamental questions related to fusion. These questions include the fusion's objectives, the pieces of information or data to be fused, their characteristics and level of uncertainty, the available fusion methods, and the associated difficulties and challenges.  
In Digital Twins, data fusion and information are used interchangeably. Data Fusion, in this context, is a Crucial process. It combines data from multiple sensors to accurately and reliably represent the physical and digital systems. This enhanced representation is pivotal in improving decision-making, underscoring the significance of data fusion in the digital twin domain and its potential impact on your work.  
Data fusion is a multidisciplinary research area that draws ideas from various fields. It is defined as the study of efficient methods for transforming information from different sources and points in time into a representation that effectively supports humans or automated quality. With data fusion in the digital twin, we can produce a refined digital representation of our physical object with characteristics that will enhance decision-making and control-related activities such as environmental mapping, object recognition, forecasting, and prevention. Data fusion is familiar and has been used since the 1960s, notably by the US Department of Defense and the Joint Directors of Laboratories (JDL). Data fusion has applications in diverse fields, such as robotics, defence, and healthcare. However, the application of data fusion in digital twins is relatively new and holds great potential to enhance the accuracy of a digital twin.  
Direct fusion involves combining sensor data from heterogeneous or homogeneous sources. In contrast, Indirect fusion combines the outputs of heterogeneous information deduced from sensor data. Some researchers often interchange 'data fusion' with 'information fusion.' 
Direct data fusion, categorised as low-level fusion, involves combining sensor data from heterogeneous or homogeneous sources. Indirect data fusion, on the other hand, is classified as high-level fusion since it is performed after some analysis. There is also mixed data fusion, low-level and high-level fusion. Understanding these different levels of data fusion can make you feel more informed and knowledgeable about the topic.
\begin{table}[htbp]
\caption{Types of data fusion and their level of fusion.\label{tab1}}
\begin{tabularx}{\textwidth}{|l|l|X|l|}
\hline
\textbf{S/N} & \textbf{DATA FUSION TYPES} & \textbf{STAGES OF FUSION} & \textbf{Level of Fusion} \\
\hline
1 & Direct Fusion & Fusion of data from sensors before analysis & Low-level fusion \\
2 & Indirect Fusion & Fusion after analysis & High-level fusion \\
\hline
\multirow{2}{*}{3} & \multirow{2}{*}{Complex Fusion} & \raggedright Fusion of multiple sensor data and advanced processing & \multirow{2}{*}{High-level fusion} \\
& & \raggedright (e.g., Kalman Filter, Bayesian networks) & \\
\hline
\end{tabularx}
\end{table}
\subsection{Methods And Techniques of Data Fusion}
Data fusion's main objective is to increase the reliability of the decisions made using collected data from sensors. Researchers generally categorise data fusion methods into four main methods. 	  
Probabilistic method, Statistical method, Knowledge base theory method and  Reasoning method. The probabilistic Data fusion method, which follows Bayes' Rule, is the heart of most data fusion methods. The Probabilistic Data Fusion method has practical applications in establishing a joint probability distribution P(y,z) between the relationship y and z for discrete and continuous variables. This practical relevance makes it a crucial method for understanding data fusion. 
Statistical methods include the cross-covariance intersection. Knowledge base theory, a widely popular method for handling uncertainty in data fusion, includes intelligence aggregation methods, such as Fuzzy logic. Evidence reasoning, often called the Dempster-Shafter evidence theory, is used in areas related to automated reasoning applications and recursive operators.  
The models we shall analyse in this research will be based on some essential characteristics of the data and the type of architecture. 

\subsection{Relation Between Input Data Sources}
Data generated from different sources could have relationships with each other about a target. Durrant-Whyte proposes that these relationships be defined as complementary, redundant, or cooperative.\\ 
The information provided by the input sources represents different parts of the scene and could thus be used to obtain generally accepted information. Such data types are complementary in nature. For instance, when two separate sensors provide information on a particular target.\\
Data that can be overlapped or superimposed on each other are redundant. Fusing redundant data can improve target confidence. Redundant data are mainly observed when two or more sources provide individual information on a particular target.\\
Cooperative Data combines provided information with new data, a key concept in data fusion. This process typically results in more complex information than the original data, such as \textbf{multi-modal (audio and video) data fusion}.\\
The type of Data input/output is a classification system that breaks data fusion into five categories based on their nature.\\
Data in - Data out (DAI-DAO):
This fusion process inputs data to make them more accurate. It is the most elementary data fusion method in classification. This data fusion process inputs and outputs raw data, but the results are typically more reliable or accurate.\\

Data in - Feature out (DAI-FEO):
At this level, the data fusion process employs raw data from the source inputs to extract features or characteristics that describe an entity in the environment.\\
Feature in - Feature out (FEI-FEO):
    In FEI-FEO, the inputs and outputs of the data fusion process are features. This data fusion process addresses feature improvement and is regarded as information fusion.\\
Feature in - Decision out (FEI-DEO):Most algorithms fall into this category because of their feature purpose classification. FEI-FEO obtains a set of features as input and provides a set of decisions as output.
Decision in - Decision Out (DEI-DEO): This is the highest level in this classification. DEI-FEO fusion transforms some decisions at the low level into global decisions for decision-making. It fuses input decisions to obtain better or new choices. 

\subsection{JDL Data Fusion Classification}
\begin{figure}[htpb]  % The [H] ensures it stays in place on this page
   \centering
    \includegraphics[width=\textwidth]{Screenshot_2024-08-16_120328.png}  % Image name uploaded to Overleaf
    \caption{JDL Architecture}
    \label{fig:datafusionclassification}
\end{figure}
This classification is the most famous conceptual model in the data fusion community. It was initially proposed by JDL and the American Department of Defense (DoD) for use in the military. These organisations classified the data fusion process into five processing levels.\\ 
Level 0- (Sources Preprocessing) provides the input data (lowest level). Different sources, such as sensors, a priori information (references or geographic data), databases, and human inputs, can be employed. The primary aim of level 0 is data transformation and transfer to the proper level for further processing.\\ 
Level 1-(Object Refinement) employs the processed data from the previous level. The main aim is to identify entities and relations. Standard procedures at this level include spatiotemporal alignment, association, correlation, clustering or grouping techniques, state estimation, the removal of false positives, identity fusion, and combining features extracted from images. The output results of this stage are object discrimination (classification and identification) and object tracking (state of the object and orientation). This stage transforms the input information into consistent data structures.\\
Level 2 focuses on a higher level of inference than Level 1. The relationship information gained from the previous level broadens the scope of the investigation into the entire environment of the entity. Situation assessment aims to identify the likely situations given the observed events and obtained data. It establishes relationships between the objects. Relations (i.e., proximity and communication) are valued to determine the significance of the entities or objects in a specific environment. The aim of this level includes performing high level inferences and identifying significant activities and events (patterns in general). The output is a set of high-level inferences.     
Level 3 evaluates the impact and threats of the detected activities in Level 2 to obtain a proper perspective. The current situation is assessed by predicting a logical outcome's risks, vulnerabilities, and probabilities of operation. 
Level 4 -(Process Refinement), improves the process from level 0 to level 3, a managed part of the entire process. The aim is to achieve efficient resource management by monitoring other levels in real-time while accounting for task priorities, scheduling, and controlling available resources. The supporting components of the JDL architecture are;
Human-Computer Interaction (HCI), %\\HCI is an interface that allows human inputs to the system from the operators and produces outputs to the operators. HCI includes queries, commands, and information on the obtained results and alarms%
Database Management System, %\\This system stores the provided information and the fused results. It is a critical component because it stores highly diverse information.
 and Sources, which is the base of the whole system.\\ %\\This is the base of the whole system. A \textit{source} is the medium by which information/data are captured, harnessed, accessed, or transferred. Sources, such as databases, prior knowledge, and sensors, could take different forms.
 
One of the limitations of the JDL method is how the uncertainty about previous or subsequent results could be employed to enhance the fusion process (feedback loop). \cite{liggins2009handbook}, propose several refinements and extensions to the JDL model. \cite{blasch2010high} proposed to add a new level (user refinement) to support a human user in the data fusion loop. The JDL model represents the first effort to provide a detailed model and a common terminology for the data fusion domain. However, because their roots originate in the military domain, the employed terms are oriented to the risks commonly occurring in these scenarios.  
The Dasarathy model differs from the JDL model in terms of the terminology and approach employed. The former is oriented toward the differences between the input and output results, independent of the employed fusion method. The Dasarathy model provides a method for understanding the relations between the fusion tasks and employed data. In contrast, the JDL model presents an appropriate fusion perspective for designing data fusion systems. 
\subsection{Classification Based On The Type Of Architecture}
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{Screenshot_2024-08-16_123428.png}  % Image name uploaded to Overleaf
    \caption{Types of Data Fusion Architectures}
    \label{fig:landscapeimage}
\end{figure}
%\end{landscape}
One of the main questions when designing a data fusion system is where the data fusion process will be performed. Based on this criterion, the following types of architectures could be identified by research
\begin{enumerate}
\item\textbf{Centralised Architecture} \\In a centralised architecture, the fusion node resides in the central processor that receives the information from all the input sources, which implies that all the fusion processes will be executed in a central processor. In this schema, the sources obtain only the observational measurements and transmit them to a central processor, where the data fusion process is performed. If data alignment and association are performed correctly and the required data transfer time is insignificant, then the centralised scheme can provide significant benefits with its theoretical optimality.

The disadvantage of this kind of architecture is that time synchronisation with the various sensors is a significant challenge in real time. Additionally, the bandwidth cost of transferring data to a central processor is expensive and can lead to information loss, highlighting the need for further research and development in these areas.
\item\textbf{Decentralised Architecture} \\A decentralised architecture comprises a network of nodes. Each node has its processing capabilities, implying no single data fusion point. Therefore, for the fusion process, each node uses its local information in conjunction with the information received from its peers. Data fusion is performed autonomously, with each node accounting for its local information and the information received from its peers. Thus, this type of architecture could suffer scalability problems when the number of nodes increases. 
\item\textbf{Distributed architecture} \\In a distributed architecture, each generated data is analysed independently from the local node before the information is sent to the fusion node. Machine learning model methods like the K-NN, Multiple Hypothesis Testing (MHT), and Probabilistic Data Association (PDA) are methods used to associate and estimate at the source nodes. In other words, data association and state estimations are done only with their local analysis, and the analysed information becomes input for the fusion process, providing a fused global analysis.
\item\textbf{Hierarchical architecture}
\\This architecture combines decentralised and distributed nodes, creating hierarchical schemes where the data fusion process is executed at different levels in the hierarchy. Implementing a decentralised data fusion system can be challenging due to the significant computation and communication requirements. However, it is essential to note that there is no single best architecture existence yet known.  
According to research and studies, selecting the most appropriate architecture should be based on the requirements, existing networks, if there are any, data availability, node processing capabilities or hardware, and the organisation of the data fusion system.  
According to \cite{castanedo2013}, the methods for data fusion are classified into three basic categories which are:
\begin{itemize}  % Nested list for sub-items
    \item\textbf{Data Association} \\Data association is a complex task that aims to establish the set of observations/measurements generated by the same target over time. It involves intricate methods such as the K-NN, Probabilistic Data Association (PDA), Joint Probabilistic Data Association (JPDA), and Multiple Hypothesis Test (MHT).    
    \item\textbf{State Estimation} \\State estimation is a precise process that considers the state of a target under movement (i.e. position), given the observation or measurement. It relies on accurate methods such as Maximum Likelihood Estimation (MLE), Kalman Filter, Distributed Filter, and covariance consistency methods. 
    \item\textbf{Decision Fusion} \\Decision fusion is crucial in making high-level inferences about an event and its activities. It does so by analyzing the detected targets provided by many sources, highlighting their importance.
    \end{itemize}
\end{enumerate}
%\begin{landscape}  % Start landscape page
% Include the image, you may need to adjust the width to fit the page


\subsection{Data Fusion Challenges}
We must implicitly examine the methods outlined above, which try to address challenges in data fusion. 
%Data Imperfection% 
Understanding data imperfection is crucial as it forms the basis of all data fusion methods. Sensor data, often imprecise, uncertain, ambiguous, vague, and incomplete, presents a significant challenge. Although we can improve data quality by modeling its imperfection and using other available information and powerful mathematical tools, the severity of data imperfection can significantly affect fusion quality if data fusion fails to extract precise and valuable data (\cite{khaleghi2013multisensor}).\\

%Data Inconsistency%
Some data uncertainties are caused by inherent noise in measurement, sensors, and environments. These noises lead to data outliers or disorder, collectively called data inconsistency.  Data inconsistency introduces a terrible effect on data fusion if the fusion model cannot distinguish the techniques necessary to overcome this problem by eliminating the influence it creates. There are some other problems caused by lasting or dynamic failures, which are challenging to model and predict in the usual way (\cite{bakr2017distributed}), (\cite{khaleghi2013multisensor}).\\

Data-related challenges often appear in a system that applies belief functions or Dempster-Shafer theory \cite{meng2020survey} when some problems that should be treated independently are erroneously integrated.\\

%Data Correlation%   
 Data captured from different sensors with different frames must undergo a crucial step before fusion alignment into a standard frame. This process, known as data alignment, is of utmost importance in data fusion. If the data fusion algorithm fails to address this, the lack of alignment or correlation can lead to over/under confidence or biased estimation, underscoring the necessity of this step (\cite{meng2020survey}).\\
 
%Data Heterogeneity% 
Data captured by different sensors could have different structures. The data fusion method should be able to integrate the different types to describe the environment better.\\

Another challenge is that of Fusion Location. To determine the fusion location, we must consider a trade-off between fusion cost and quality. This is a problem with respect to wireless sensors. When data are generated from a central node, the expense is bandwidth and time, but if data are from a local node, the cost is data accuracy.\\

In addition, the complexity of data fusion is caused by timeliness, where the significance of a data point might be limited to a limited period, especially for a time-varying system. The fusion node must distinguish the correct order of the data and its validation. 


\begin{table}[H]
\centering
\caption{General Types Of Sensors For Data Acquisition Within A Home Environment.}
\begin{tabular}{|p{3.5cm}|p{5cm}|p{5cm}|} 
\hline
\textbf{CATEGORIES} & \textbf{EXTERNAL SENSORS} & \textbf{MOBILE SENSORS} \\
\hline
\parbox[t]{4.5cm}{ENVIRONMENTAL \\ SENSORS} & Barometer, Humidity, Light sensor, Thermal sensors. & Ambient air temperature and pressure, Barometer, Photometer, Thermal sensor. \\
\hline
\parbox[t]{4.5cm}{LOCATION \\ SENSORS} & GPS receiver, Wi-Fi. & GPS receiver, Wi-Fi location. \\
\hline
\parbox[t]{4.5cm}{MOTION \\ SENSORS} & Accelerometer, Pressure sensor, Gravity sensor, Rotational sensor. & Accelerometer, Orientation sensor, Gravity sensor. \\
\hline
\parbox[t]{4.5cm}{IMAGE AND \\ VIDEO \\ SENSORS} & Digital camera, 3D camera, Optical sensor, Infrared sensor & Digital camera, Infrared sensor. \\
\hline
\parbox[t]{4.5cm}{PROXIMITY \\ SENSORS} & Touch sensor, Proximity sensor, RFID, Tactile sensor & RFID, Touch sensor, Proximity sensor. \\
\hline
\parbox[t]{4.5cm}{ACOUSTIC \\ SENSORS} & Microphone, Silicon wave device, Silicon microphone. & Microphone. \\
\hline
\parbox[t]{4.5cm}{MEDICAL \\ SENSORS} & Blood pressure, Dosage control, Stress sensor, Heart rate sensor, Electrodermal activity sensor. & - \\
\hline
\parbox[t]{4.5cm}{CHEMICAL \\ SENSORS} & Oxygen saturation, Electrochemical gas sensor, Aroma sensor. & - \\
\hline
\parbox[t]{4.5cm}{OPTICAL \\ SENSORS} & Fibre optic sensors, Infrared sensor, Radio frequency sensor. & Infrared sensors, Radio frequency sensors. \\
\hline
\parbox[t]{4.5cm}{FORCE \\ SENSORS} & Force-sensitive resistor, Mass sensor, Fingerprint sensor. & Fingerprint sensor. \\
\hline
\end{tabular}
\end{table}
\subsection{Fusion Quality Assessment}
 Okolie and Smit, (2022), proposed three approaches that are commonly used for Digital fusion estimation Models.
\begin{enumerate}
    \item{A qualitative approach which involves an inspection and comparison of results from the physical entity with the digital twin.}
    \item{The quantitative approach, a practical and widely used method, involves using statistical metrics to measure the relative and absolute quality of the fused Digital Elevation Model (DEM).}
    \item{The performance-based approach offers a versatile set of quality assessment criteria, including the Mean Absolute Error (MAE), Coefficient of Determination \( R^{2} \) and Model Loss, allowing for a comprehensive evaluation of a fused digital twin model.}
\end{enumerate}