To train and evaluate the proposed model, we generated training, validation, and test datasets that replicate realistic acoustic scenes. These scenes \secor{consist of music listened to by a user through headphones or earphones and ambient noise as perceived by the user through their earphones}{simulate a user listening to music through headphones or earphones while being in a noisy environment}.  

We defined several \textit{environments} to represent a variety of realistic acoustic scenes with different ambient noise levels: urban, indoor office, construction site, beach, transportation (train/plane/boat), and restaurant/bar. 
\se{We chose} the noise recordings from the DNS Challenge dataset \cite{dubeyIcassp2022Deep2022}.
Each environment is \se{created using} the labels from the noise dataset and a realistic noise level distribution in dBA. Noise samples are evenly selected per environment and normalized to levels sampled from the corresponding noise distribution. A pre-processing step is applied to drop the audio signals composed mainly of silence (which are therefore isolated, impulsive noises). After that, all those samples are filtered with one of three headphone frequency responses to reproduce their passive attenuation. 
Each noise sample is then paired with a music track \se{chosen from the FMA dataset \cite{defferrardFMADATASETMUSIC2017}} which covers a large diversity of music genres (Pop, Rock, Classical, Jazz, Hip-Hop, etc.).
Each music is normalized to a dBA level derived from a Signal-to-Noise Ratio (SNR) value, itself sampled from a defined SNR distribution. The resulting music dBA level is constrained within the \secor{45 dBA to 100 dBA range}{range $[45, 100]$ dBA,} reflecting the typical range offered by standard headphones.
\se{We thus generate} 50h of training data, 20h of validation data, and 10h of test data, composed of pairs of 10s mono music and noise excerpts sampled at 44100 Hz.