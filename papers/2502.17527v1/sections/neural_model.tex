In this work, we tackle the task of raising the listened music's masking thresholds to better mask the listener's surrounding noise.
The proposed model Deep Perceptual Noise Masking with Music (DPNMM) is based on a deep neural network that predicts the frequency responses of filters to apply to the music to achieve the desired masking effect. A schematic overview of the system is shown in Fig. \ref{fig:system_overview}. The neural network takes as input frequency features derived from both the noise and the music. Their respective power spectral densities (PSDs) are computed using sliding Hanning windows of length $N_{win}=$ 2048 points with a 75\% overlap and a sampling rate $f_s=$ 44100 Hz.  
Both are mapped to the psychoacoustic Bark scale \cite{zwickerPsychoacousticsFactsModels2010} resulting in the PSDs $P^{noise}(n, \nu)$ and $P^{music}(n, \nu)$, where $n$ denotes the frame index and $\nu$ the Bark band. 

The music masking thresholds $T(n, \nu)$ per Bark band are computed using Johnston's model \cite{johnstonTransformCodingAudio1988} as in Estreder's system \cite{estrederPerceptualAudioEqualization2018}. The calculation involves applying a \textit{spreading function} to the PSD in each critical band, which indicates how it masks signals within the same band and in adjacent bands. The contributions from all Bark bands are then summed, following the additivity property of simultaneous masking \cite{lutfiAdditivitySimultaneousMasking1983, johnstonTransformCodingAudio1988, humesModelsAdditivityMasking1989}, and adjusted downward by an offset that depends on whether the music is more tonal or noise-like.

\subsection{Architecture}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/system_overview.png}
    \caption{Overview of the proposed system. Bark features from the music and noise signals are computed: PSD per Bark band for both music and noise and music masking thresholds. The features are fed to the U-Net that outputs gains in dB used to scale filter frequency responses. They are applied in the spectral domain to the music and a processed version is generated by inverse STFT.}\vspace{-0.3cm}
    \label{fig:system_overview}
\end{figure}

Our model's architecture is illustrated in Fig. \ref{fig:u-net_architecture}. We design a U-Net encoder-decoder model, inspired by the DeepFilterNet architecture \cite{schroterDeepfilternetLowComplexity2022}, especially its Equivalent Rectangular Bandwidth (ERB) gains prediction branch. The encoder consists of 4 convolutional blocks (separable convolution + BatchNorm + ReLU) followed by a linear layer, designed to process frequency information between the Bark features. 
%while preserving the temporal structure. 
This frequency information is then passed through a Gated Recurrent Unit (GRU) layer to handle the temporal dynamics. The decoder mirrors the encoder and incorporates skip connections, outputting the predicted gains in dB, $g(n,\nu)$, per frame, and Bark band (up to the 24th band $\sim$ 16500 Hz). 
Additionally, a gain smoothing filter is applied to prevent too rapid variations in the filters' frequency responses over time, and the gains are finally clamped as in \cite{estrederPerceptualAudioEqualization2018} with the following threshold values:
\begin{equation}
    g(n, \nu) \leftarrow \max \big( \min( g(n, \nu); -5 \text{ dB} ) ; 10 \text{ dB} \big).
\end{equation} 
\vspace{-0.5cm}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/u-net_architecture.png}
    \caption{U-Net architecture of the proposed model. $N$ is the number of time frames, and 26 the number of Bark bands. The encoder is composed of 4 convolutional layers (Conv), a linear layer and a Gated Recurrent Unit (GRU) layer. The decoder follows the inverse path with transposed convolutional layers (TConv) and pathway convolutions (PConv) as add-skip connections.}%\vspace{-0.3cm}
    \label{fig:u-net_architecture}
\end{figure}

\subsection{Filters frequency responses}

The obtained gains are converted into filter frequency responses
that are generated using a pattern $W_{dB}^\nu(f)$ centered on the corresponding Bark band $\nu$ (see Fig. \ref{fig:pattern_filter}). In each band, the pattern is constant and equal to 1. 
To simulate the spreading effect of masking across the Bark bands, the pattern transitions smoothly with a cosine shape across the two adjacent bands below (from 0 to 1) and above (from 1 to 0). For the lowest frequency band, the pattern level is set to 2 instead of 1 at the center. This approach accounts for the fact that adjacent bands also contribute to raising the masking threshold of a given band, thereby distributing the required gain across multiple bands and minimizing the boost needed on the central band. %alone.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/pattern_filter.pdf}
    \caption{Pattern $W_{dB}^\nu(f)$ used to shape the filters' frequency responses, for $\nu =$ 5.}\vspace{-0.3cm}
    \label{fig:pattern_filter}
\end{figure}
For each audio frame the overall frequency response $\mathcal{W}_{dB}(n,k)$ is obtained as:
\begin{equation}
    \mathcal{W}_{dB}(n,k) = \sum_{\nu=1}^{B} g(n,\nu) \cdot W_{dB}^\nu(k).
\end{equation}
The input music is then filtered, in the frequency domain, using those frequency responses frame by frame to produce a processed music whose masking properties are enhanced.

\subsection{Loss functions}
Our model DPNMM is trained with a primary loss function designed to raise the music's masking threshold above the noise level in the critical bands where needed. These Bark bands are constrained to exceed specific thresholds, while the energy in other bands is free to change as long as their masking threshold remains above the noise level, thereby supporting masking in adjacent bands: 
\begin{equation}
    \mathcal{L}_0 (\theta_t) = \frac{1}{N} \frac{1}{B}  \sum_{n,\nu} \text{ReLU}\left( P_{dB}^{noise}(n,\nu) - \hat{T}_{dB}(n,\nu) \right) ,
\end{equation}
where $\hat{T}_{dB}(n,\nu)$ is the masking threshold computed with the processed music, $N$ the number of time frames and $B$ the number of Bark bands.
By using a ReLU function to express the constraint, we only set a minimum threshold level for the network to reach, allowing it greater flexibility to find solutions for masking noise across all frequency bands. However, while this choice aims to provide the network with more freedom, it also brings the challenge that the system is not required to output zero gains when no amplification of the music is needed. To address this, we use the knowledge of the masking spreading effect  to compute a mask that identifies, for each band, whether it is close enough to another band (including itself) where the threshold needs to be raised to have an impact on it. If it is not, the gain for that band at the network's output is set to zero.

Even with this gain masking, the system still has an infinite number of potential solutions. To guide the learning process in a desired direction, we add a secondary constraint aimed at preserving the naturalness of the original music by limiting the average power variation: 
\begin{equation}
    \mathcal{L}_{power}(\theta_t) = \frac{1}{N} \sum_n \vert \hat{\mathcal{P}}_{dBA}^{music}(n) - \mathcal{P}_{dBA}^{music}(n) \vert \; ; 
\end{equation}
where the initial music mean power $\mathcal{P}$ of frame $n$ is evaluated in dBA \cite{a_weights}, as well as the processed music mean power $\hat{\mathcal{P}}$. To include this constraint in the training process we use a strategy inspired by the method of multipliers \cite{jonasdegraveHowWeCan2021, plattConstrainedDifferentialOptimization1987}. The goal is to ensure that during training the power variation does not exceed a given value $\Delta \mathcal{P}_{max}$ using a dynamic weight $\lambda_t$ to scale the loss. The overall loss function for this constrained optimization problem is:
\begin{equation}
    L(\theta_t, \lambda_t) = \mathcal{L}_0 - \lambda_t \cdot (\Delta \mathcal{P}_{max} - \mathcal{L}_{power}(\theta_t)).
\end{equation}
At the start of the training, $\lambda_t = 0$. While gradient descent is applied to the overall loss $L(\lambda_t)$, gradient ascent is simultaneously performed on $\lambda_t$ using the gradient $\frac{\partial L(\theta, \lambda_t)}{\partial \lambda_t} = \mathcal{L}_{power}(\theta_t) - \Delta \mathcal{P}_{max}$ with a specific learning rate of $10^{-3}$, keeping $\lambda_t$ always positive. When $\mathcal{L}_{power}(\theta_t)$ exceeds the threshold $\Delta \mathcal{P}_{max}$, $\lambda_t$ increases, giving more weight to the power constraints in the total loss. Conversely, when $\mathcal{L}_{power}(\theta_t)$ drops below $\Delta \mathcal{P}_{max}$, $\lambda_t$ decreases.
This approach allows us to guide the training in a direction that satisfies both constraints without requiring a tedious search for an optimal fixed weight. The degree of compromise between the two constraints is directly controlled by the choice of parameter $\Delta \mathcal{P}_{max}$. In the rest of the article, we explore several values for this parameter. 

The code for the implementation of DPNMM is available on github.\footnote{\href{https://github.com/ClementineBerger/DPNMM}{https://github.com/ClementineBerger/DPNMM}}