Gene expression classification \cite{ahmed2019gene,huynh2018random,do2024enhancing,do2023ensemble,huynh2019novel} has emerged as a fundamental tool in bioinformatics, enabling the identification of disease subtypes, prediction of patient outcomes, and discovery of potential therapeutic targets. With the advent of high-throughput sequencing technologies, researchers can now analyze vast gene expression profiles across thousands of genes simultaneously. However, this progress comes with a significant computational and analytical challenge: the \textit{curse of dimensionality} \cite{koppen2000curse,bach2017breaking}. In typical gene expression datasets, the number of genes (\( p \)) far exceeds the number of samples (\( n \)), often by several orders of magnitude. This imbalance leads to severe overfitting in machine learning models, where classifiers struggle to generalize due to the overwhelming presence of irrelevant or redundant features. Additionally, the high dimensionality increases computational complexity, making conventional classification models inefficient and impractical for real-world applications.

To address these challenges, \textit{feature selection} \cite{li2017feature,miao2016survey} plays a critical role in gene expression classification. By identifying and retaining only the most informative genes, feature selection not only improves model generalization but also enhances interpretability, allowing researchers to derive biologically meaningful insights from machine learning predictions \cite{do2024enhancing,karim2019onconetexplainer}. Furthermore, reducing the feature space significantly lowers computational costs, enabling faster model training and inference. More importantly, gene selection aligns with the biological reality that only a subset of genes actively contributes to disease mechanisms, making feature selection a crucial step in biomedical analysis. Without an effective selection strategy, classifiers are prone to noise, reduced accuracy, and difficulty in extracting meaningful biomarkers for clinical applications.

Given the importance of feature selection, extensive research has been conducted to develop robust selection techniques tailored for gene expression data. Traditional methods include filter-based approaches \cite{duch2006filter,cervante2012binary,cherrington2019feature,lee2011filter}, which rely on statistical measures such as mutual information, correlation, and entropy to rank features independently of the classifier. Although computationally efficient, filter methods often fail to capture complex gene interactions. Wrapper-based methods \cite{bajer2020wrapper,wang2014stability,kasongo2020deep,mufassirin2018novel}, such as recursive feature elimination (RFE) \cite{chen2007enhanced,yan2015feature} and genetic algorithms \cite{leardi1992genetic}, iteratively refine feature subsets based on classifier performance but tend to be computationally expensive for high-dimensional data. Meanwhile, embedded techniques \cite{imani2013novel,stanczyk2015feature,hamed2014accurate}, such as LASSO \cite{fonti2017feature,muthukrishnan2016lasso} and tree-based models \cite{freeman2013feature,liu2019embedded}, integrate feature selection within the classification process but may not always provide optimal feature subsets for different classifiers. Despite these advancements, no single approach consistently outperforms others across all gene expression datasets, necessitating hybrid strategies that leverage the strengths of multiple selection paradigms.

To bridge this gap, we propose BOLIMES (Borutaâ€“LIME Enhanced eXtraction), a novel feature selection algorithm designed to enhance gene expression classification by systematically refining the feature subset. Unlike conventional methods that rely solely on statistical ranking or classifier-specific selection, BOLIMES integrates the robustness of Boruta \cite{kursa2010boruta,zhou2023diabetes} with the interpretability of LIME \cite{ribeiro2016should}, ensuring that only the most relevant and influential genes are retained. BOLIMES begins by applying Boruta, safeguarding against the premature elimination of any informative genes. Next, LIME (Local Interpretable Model-agnostic Explanations) \cite{ribeiro2016should,kumarakulasinghe2020evaluating} is employed to assess the local importance of each selected gene, offering an interpretable ranking that highlights the contributions of individual features to the model. Finally, an iterative classification evaluation identifies the optimal subset of features by selecting the number of genes that maximize predictive accuracy. We pay attention at Boruta and LIME for the following reasons: (1) Due to the large number of dimensions, LIME would face scalability issues when it comes to interpretation. (2) Boruta is viewed as a global feature selection method, whereas LIME focuses on local feature selection. Therefore, we will first perform global filtering using Boruta, followed by local refinement with LIME. (3) Our primary objective is to determine the optimal number of features for selection. As a result, we have proposed the Bolimex algorithm to select the most appropriate set of features, ensuring the best possible accuracy.








The remainder of this paper\footnote{This paper will be condensed to meet the page limit requirements of the conference upon acceptance. We aim to retain all the core ideas and details from this version to ensure the integrity of our research.} is structured as follows: we briefly presents a fundamental background in Section 2. Next, Section 3 describes our proposal algorithm. Then, Section 4 represents the experiment, the results of the models and an discussion. Finally, Section 5 shows the conclusions and future work.




