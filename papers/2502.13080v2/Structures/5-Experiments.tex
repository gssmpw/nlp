This section offers a brief description of the gene expression datasets while delivering a detailed comparative analysis of the classification models. Furthermore, we also provide the results of Boruta. Our source code has been made publicly accessible on GitHub\footnote{\url{https://github.com/Pbchung75/BOLIMES/}}.

 \subsection{Dataset and Configurations}
We carry out a series of experiments using 14 gene expression datasets, all meticulously sourced from the reputable ArrayExpress repository \cite{brazma2003arrayexpress}. The gene expression datasets \cite{do2024enhancing} summarized in Table~\ref{tab:datasets} exemplify the inherent challenges of high-dimensional biomedical data. With sample sizes ranging from $53$ to $575$ and feature counts spanning from approximately $11,950$ to over $54,600$, these datasets present a significant imbalance between the number of available samples and the vast dimensionality of gene expression profiles. Additionally, the variability in the number of classes—from as few as $3$ to as many as $10$—further complicates the classification task by introducing diverse and complex biological signals. 
This high dimensionality coupled with limited sample sizes accentuates the risk of overfitting and underscores the critical need for effective feature selection. 
%Robust feature selection methods are essential to isolate the most informative genes, thereby enhancing model interpretability and predictive accuracy in gene expression classification.
In this study, our feature selection strategy is specifically designed to address these challenges, ensuring that only the most relevant features are retained for subsequent classification tasks.

\begin{table}[ht]
    \centering
    \caption{Dataset Characteristics}
    \label{tab:datasets}
    \begin{tabular}{cccccc}
        \toprule
        ID & Dataset        & \#Datapoints & \#Dimensions & \#Classes \\
        \midrule
        1  & E-GEOD-20685   & 327          & 54627        & 6  \\
        2  & E-GEOD-20711   & 90           & 54675        & 5  \\
        3  & E-GEOD-21050   & 310          & 54613        & 4  \\
        4  & E-GEOD-21122   & 158          & 22283        & 7  \\
        5  & E-GEOD-29354   & 53           & 22215        & 3  \\
        6  & E-GEOD-30784   & 229          & 54675        & 3  \\
        7  & E-GEOD-31312   & 498          & 54630        & 3  \\
        8  & E-GEOD-31552   & 111          & 33297        & 3  \\
        9  & E-GEOD-32537   & 217          & 11950        & 7  \\
        10 & E-GEOD-33315   & 575          & 22283        & 10 \\
        % 11 & E-GEOD-36895   & 76           & 54675        & 14 \\
        11 & E-GEOD-37364   & 94           & 54675        & 4  \\
        12 & E-GEOD-39582   & 566          & 54675        & 6  \\
        13 & E-GEOD-39716   & 53           & 33297        & 3  \\
        14 & E-GEOD-44077   & 226          & 33252        & 4  \\
        \bottomrule
    \end{tabular}
\end{table}

To evaluate our approach and the training model, we implement the AI libraries \textit{(i.e., Pandas (version 2.2.2),
Scikit-learn (version 1.6.1))} and run the experiments on the computer with the following configuration: \textit{Intel Core i5-12400, 2.50 GHz, 24 GB RAM, Windows 11 Pro OS}.

%===========================================
 \subsection{Boruta's Feature Selection}
In this study, we adopt Boruta as our primary feature selection method to substantially reduce the dimensionality of our gene expression datasets for the \textit{global} problem. The rationale behind this choice is to prevent an explosion in computational complexity and potential loss of interpretability when LIME is applied to an excessively high-dimensional feature space. By using Boruta, we are able to effectively eliminate irrelevant features (tentative and rejected in Table~\ref{tab:feature_selection_results}) while retaining those that are truly informative for classification. As evidenced in Table~\ref{tab:feature_selection_results}, datasets such as \textit{E-GEOD-20685} are reduced from 54,627 dimensions to only 545 confirmed features, thereby rendering the subsequent LIME analysis both feasible and efficient. To achieve this, Boruta is executed with the following parameters via the BorutaPy library: a random forest classifier (`rf`) with 300 estimators, a maximum of 200 iterations, an alpha value of $0.01$, a strict percentile threshold of 100, two-step feature selection enabled, and a fixed random state of 42, with verbose output enabled. These parameter settings are meticulously selected to ensure a rigorous and robust selection process, ultimately facilitating a more interpretable and high-performing classification model.

\begin{table}[ht]
    \centering
    \caption{Feature Selection Results}
    \label{tab:feature_selection_results}
    \begin{tabular}{cccccc}
        \toprule
        ID & Dataset & & Features & & Feature Selection \\
        & E-GEOD-* & Confirmed & Tentative  & Rejected & Time (s) \\
        \midrule
        1  & 20685 & 545  & 262 & 53820 & 633,0205021 \\
        2  & 20711 & 111  & 33  & 54531 & 140,0120337 \\
        3  & 21050 & 72   & 17  & 54524 & 350,3323104 \\
        4  & 21122 & 271  & 124 & 21888 & 219,2371044 \\
        5  & 29354 & 28   & 3   & 22184 & 91,59759808 \\
        6  & 30784 & 171  & 64  & 54440 & 279,9282582 \\
        7  & 31312 & 213  & 42  & 54375 & 777,7057292 \\
        8  & 31552 & 79   & 48  & 33170 & 136,9728532 \\
        9  & 32537 & 96   & 17  & 11837 & 232,6542134 \\
        10 & 33315 & 483  & 108 & 21692 & 1176,811348  \\
        % 11 & 36895 & 39   & 14  & 54622 & 122,578902   \\
        11 & 37364 & 59   & 43  & 54573 & 129,0980539  \\
        12 & 39582 & 640  & 253 & 53782 & 1306,720685  \\
        13 & 39716 & 124  & 29  & 33144 & 99,24336505  \\
        14 & 44077 & 227  & 231 & 32794 & 221,3678348  \\
        \bottomrule
    \end{tabular}
\end{table}

%===========================================
 \subsection{Classification results}
In this study, we conduct experiments with four ML algorithms (using SVM, Random Forest (RF), XGBoost (XGB), and Gradient Boosting (GB)) to determine the method yielding the highest accuracy across various gene expression datasets (see Table \ref{tab:svm_results}, \ref{tab:RF_results}, \ref{tab:XGB_results}, and \ref{tab:gb_results}). We configure an \textit{SVM} using SVC with a radial basis function kernel, setting C to $100,000$, gamma to $0.001$, ensuring reproducibility with a random state of $42$. In parallel, we employe a Random Forest classifier with $200$ estimators, a maximum depth of $10$. Additionally, we implemented both an \textit{XGBClassifier} and a \textit{GradientBoostingClassifier}, each configured with $50$ estimators, a maximum depth of $10$, and a learning rate of $0.01$. Our comparative evaluation reveales that while the SVM excelled on datasets with fewer samples, the ensemble methods—particularly Random Forest and XGBoost—demonstrated more robust performance on high-dimensional data. %This comprehensive analysis highlights the critical role of algorithm selection and fine-tuning in achieving optimal classification performance in gene expression studies.

In Table \ref{tab:svm_results}, \ref{tab:RF_results}, \ref{tab:XGB_results}, and \ref{tab:gb_results}, we conduct SVM, RF, and Decision Tree (DT) methodology to identify the importance of features and the number of selected features ($k$ selected dimensions) following the methodology outlined in \cite{do2024enhancing} i.e., Top $k$ selected feature of E-GEOD-20685 (ID 1) is $85$, E-GEOD-20711 (ID 2) is $43$, E-GEOD-21050 (ID 3) is $275$, and others. Overall, our proposed algorithm demonstrated superior performance, achieving higher accuracy with respect to the F1-score. In certain instances, it reached $100\%$, while other solutions (SVM, RF, DT) only achieved $80-90\%$. The feature selection algorithms, such as SVM, RF, and DT, with a smaller number of features, appeared to underperform in covering the data (e.g., IDs 1, 2, 4). Our approach, however, seemed to be more effective in determining the optimal number of features needed.


\begin{table}[htbp]
  \centering
  \caption{SVM Classification Results with Top $k$ Features}
  \label{tab:svm_results}
  \scriptsize
  \begin{tabular}{cccccccccccc}
    \toprule
    ID & Dataset & Class & Method & Samples & \multicolumn{1}{c}{Top k} & \multicolumn{4}{c}{Classification Results} & {Training} \\
    \cmidrule(lr){7-10} 
        &         &       &         &   & Features & Acc & Prec & Rec & F1 Score & Time (s)  \\
  \midrule
        1  & 20685 & 6 & ours & 327 & 107 & 0.955 & 0.956 & 0.955 & 0.955 & 112.137 \\
           &       &   & SVM  &     & 85  & 0.859 & 0.872 & 0.859 & 0.859 & 0.135 \\
           &       &   & RF   &     & 85  & 0.890 & 0.902 & 0.890 & 0.889 & 0.147 \\
           &       &   & DT   &     & 85  & 0.872 & 0.884 & 0.872 & 0.868 & 0.192 \\
        \midrule
        2  & 20711 & 5 & ours & 90  & 67  & 0.944 & 0.952 & 0.944 & 0.943 & 17.754 \\
           &       &   & SVM  &     & 43  & 0.811 & 1.000 & 0.811 & 0.811 & 0.818 \\
           &       &   & RF   &     & 43  & 0.778 & 1.000 & 0.778 & 0.778 & 0.806 \\
           &       &   & DT   &     & 43  & 0.778 & 1.000 & 0.778 & 0.778 & 0.788 \\
        \midrule
        3  & 21050 & 4 & ours & 310 & 70  & 0.677 & 0.709 & 0.677 & 0.678 & 295.982\\
           &       &   & SVM  &     & 275 & 0.684 & 0.690 & 0.684 & 0.676 & 0.218 \\
           &       &   & RF   &     & 275 & 0.681 & 0.681 & 0.681 & 0.666 & 0.227 \\
           &       &   & DT   &     & 275 & 0.652 & 0.674 & 0.652 & 0.622 & 0.250 \\
        \midrule
        4  & 21122 & 7 & ours & 158 & 170 & 0.938 & 0.915 & 0.938 & 0.922 & 82.092 \\
           &       &   & SVM  &     & 78  & 0.886 & 1.000 & 0.886 & 0.886 & 1.489 \\
           &       &   & RF   &     & 78  & 0.880 & 1.000 & 0.880 & 0.880 & 1.483 \\
           &       &   & DT   &     & 78  & 0.842 & 1.000 & 0.842 & 0.842 & 1.744 \\
        \midrule
        5  & 29354 & 3 & ours & 53  & 10  & 1.000 & 1.000 & 1.000 & 1.000 & 0.165 \\
           &       &   & SVM  &     & 35  & 0.868 & 1.000 & 0.868 & 0.868 & 0.415 \\
           &       &   & RF   &     & 35  & 0.830 & 1.000 & 0.830 & 0.830 & 0.422 \\
           &       &   & DT   &     & 35  & 0.660 & 1.000 & 0.660 & 0.660 & 0.397 \\
        \midrule
        6  & 30784 & 3 & ours & 229 & 14  & 1.000 & 1.000 & 1.000 & 1.000 & 3.476 \\
           &       &   & SVM  &     & 42  & 0.904 & 1.000 & 0.904 & 0.904 & 2.277 \\
           &       &   & RF   &     & 42  & 0.908 & 1.000 & 0.908 & 0.908 & 2.075 \\
           &       &   & DT   &     & 42  & 0.908 & 1.000 & 0.908 & 0.908 & 2.175 \\
        \midrule
        7  & 31312 & 3 & ours & 498 & 43  & 0.900 & 0.900 & 0.900 & 0.900 & 2.144 \\
           &       &   & SVM  &     & 195 & 0.878 & 0.872 & 0.878 & 0.859 & 0.236 \\
           &       &   & RF   &     & 195 & 0.878 & 0.864 & 0.878 & 0.857 & 0.225 \\
           &       &   & DT   &     & 195 & 0.809 & 0.796 & 0.809 & 0.775 & 0.345 \\
\midrule
        8  & 31552 & 3 & ours & 111 & 15  & 0.957 & 0.917 & 0.957 & 0.936 & 1.816 \\
           &       &   & SVM  &     & 44  & 0.820 & 1.000 & 0.820 & 0.820 & 0.931 \\
           &       &   & RF   &     & 44  & 0.883 & 1.000 & 0.883 & 0.883 & 0.870 \\
           &       &   & DT   &     & 44  & 0.847 & 1.000 & 0.847 & 0.847 & 0.890 \\
        \midrule
        9  & 32537 & 7 & ours & 217 & 40  & 0.795 & 0.730 & 0.795 & 0.747 & 21.201 \\
           &       &   & SVM  &     & 171 & 0.760 & 1.000 & 0.760 & 0.760 & 3.054 \\
           &       &   & RF   &     & 171 & 0.779 & 1.000 & 0.779 & 0.779 & 2.910 \\
           &       &   & DT   &     & 171 & 0.756 & 1.000 & 0.756 & 0.756 & 3.450 \\
        \midrule
        10 & 33315 & 10 & ours & 575 & 273 & 0.878 & 0.860 & 0.878 & 0.867 & 33.250 \\
           &       &    & SVM  &     & 2321 & 0.873 & 0.893 & 0.873 & 0.855 & 4.174 \\
           &       &    & RF   &     & 2321 & 0.870 & 0.893 & 0.870 & 0.851 & 4.667 \\
           &       &    & DT   &     & 2321 & 0.781 & 0.845 & 0.781 & 0.752 & 5.534 \\
        \midrule
        11 & 37364 & 4 & ours & 94  & 17  & 0.947 & 0.965 & 0.947 & 0.947 & 2.083 \\
           &       &   & SVM  &     & 140 & 0.809 & 1.000 & 0.809 & 0.809 & 0.793 \\
           &       &   & RF   &     & 140 & 0.755 & 1.000 & 0.755 & 0.755 & 0.750 \\
           &       &   & DT   &     & 140 & 0.713 & 1.000 & 0.713 & 0.713 & 0.798 \\
        \midrule
        12 & 39582 & 6 & ours & 566 & 251 & 0.877 & 0.889 & 0.877 & 0.880 & 25.058\\
           &       &   & SVM  &     & 441 & 0.859 & 0.873 & 0.859 & 0.859 & 0.466 \\
           &       &   & RF   &     & 441 & 0.862 & 0.874 & 0.862 & 0.861 & 0.480 \\
           &       &   & DT   &     & 441 & 0.790 & 0.812 & 0.790 & 0.786 & 0.764 \\
        \midrule
        13 & 39716 & 3 & ours & 53  & 12  & 1.000 & 1.000 & 1.000 & 1.000 & 0.454 \\
           &       &   & SVM  &     & 118 & 0.887 & 1.000 & 0.887 & 0.887 & 0.408 \\
           &       &   & RF   &     & 118 & 0.925 & 1.000 & 0.925 & 0.925 & 0.408 \\
           &       &   & DT   &     & 118 & 0.774 & 1.000 & 0.774 & 0.774 & 0.401 \\
        \midrule
        14 & 44077 & 4 & ours & 226 & 18  & 1.000 & 1.000 & 1.000 & 1.000 & 5.220 \\
           &       &   & SVM  &     & 23  & 0.991 & 1.000 & 0.991 & 0.991 & 1.803 \\
           &       &   & RF   &     & 23  & 0.996 & 1.000 & 0.996 & 0.996 & 1.790 \\
           &       &   & DT   &     & 23  & 0.987 & 1.000 & 0.987 & 0.987 & 2.005 \\
    
    \midrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%Ramdom foresst


%%%%%%%%RF mới

\begin{table}[htbp]
  \centering
  \caption{Random Forest Classification Results with Top $k$ Features}
  \label{tab:RF_results}
  \scriptsize
  \begin{tabular}{cccccccccccc}
    \toprule
    ID & Dataset & Class & Method & Samples & \multicolumn{1}{c}{Top k} & \multicolumn{4}{c}{Classification Results} & {Training} \\
    \cmidrule(lr){7-10} 
        &         &       &         &   & Features & Acc & Prec & Rec & F1 Score & Time (s)  \\
  \midrule
        1  & 20685 & 6 & Ours & 327 & 48 & 0.970 & 0.971 & 0.970 & 0.970 & 16306.229 \\
           &       &   & SVM  &     & 85  & 0.838 & 0.852 & 0.838 & 0.831 & 500.484 \\
           &       &   & RF   &     & 85  & 0.874 & 0.890 & 0.874 & 0.872 & 4.706 \\
           &       &   & DT   &     & 85  & 0.887 & 0.904 & 0.887 & 0.885 & 5.175 \\
        \midrule
        2  & 20711 & 5 & Ours & 90  & 10  & 0.833 & 0.841 & 0.833 & 0.830 & 5.396 \\
           &       &   & SVM  &     & 43  & 0.767 & 1.000 & 0.767 & 0.767 & 25.742 \\
           &       &   & RF   &     & 43  & 0.833 & 1.000 & 0.833 & 0.833 & 2593.662 \\
           &       &   & DT   &     & 43  & 0.833 & 1.000 & 0.833 & 0.833 & 2588.751 \\
        \midrule
        3  & 21050 & 4 & Ours & 310 & 72  & 0.726 & 0.727 & 0.726 & 0.711 & 1507.624 \\
           &       &   & SVM  &     & 275 & 0.732 & 0.776 & 0.732 & 0.696 & 6.371 \\
           &       &   & RF   &     & 275 & 0.735 & 0.773 & 0.735 & 0.700 & 6.617 \\
           &       &   & DT   &     & 275 & 0.739 & 0.793 & 0.739 & 0.701 & 6.441 \\
        \midrule
        4  & 21122 & 7 & Ours & 158 & 104 & 0.938 & 0.922 & 0.938 & 0.925 & 1039.209 \\
           &       &   & SVM  &     & 78  & 0.880 & 1.000 & 0.880 & 0.880 & 50.215 \\
           &       &   & RF   &     & 78  & 0.886 & 1.000 & 0.886 & 0.886 & 50.446 \\
           &       &   & DT   &     & 78  & 0.873 & 1.000 & 0.873 & 0.873 & 53.083 \\
        \midrule
        5  & 29354 & 3 & Ours & 53  & 10  & 1.000 & 1.000 & 1.000 & 1.000 & 3.325 \\
           &       &   & SVM  &     & 35  & 0.887 & 1.000 & 0.887 & 0.887 & 13.873 \\
           &       &   & RF   &     & 35  & 0.811 & 1.000 & 0.811 & 0.811 & 12.891 \\
           &       &   & DT   &     & 35  & 0.774 & 1.000 & 0.774 & 0.774 & 12.654 \\
        \midrule
        6  & 30784 & 3 & Ours & 229 & 12  & 1.000 & 1.000 & 1.000 & 1.000 & 43.917 \\
           &       &   & SVM  &     & 42  & 0.913 & 1.000 & 0.913 & 0.913 & 80.499 \\
           &       &   & RF   &     & 42  & 0.895 & 1.000 & 0.895 & 0.895 & 75.487 \\
           &       &   & DT   &     & 42  & 0.908 & 1.000 & 0.908 & 0.908 & 76.522 \\
        \midrule
        7  & 31312 & 3 & Ours & 498 & 161 & 0.900 & 0.821 & 0.900 & 0.858 & 221.322 \\
           &       &   & SVM  &     & 195 & 0.871 & 0.885 & 0.871 & 0.828 & 8.221 \\
           &       &   & RF   &     & 195 & 0.872 & 0.885 & 0.872 & 0.828 & 7.968 \\
           &       &   & DT   &     & 195 & 0.841 & 0.859 & 0.841 & 0.800 & 8.715 \\
     \midrule
        8  & 31552 & 3 & Ours & 111 & 12 & 1.000 & 1.000 & 1.000 & 1.000 & 20.318 \\
           &       &   & SVM  &     & 44  & 0.865 & 1.000 & 0.865 & 0.865 & 31.843 \\
           &       &   & RF   &     & 44  & 0.901 & 1.000 & 0.901 & 0.901 & 29.352 \\
           &       &   & DT   &     & 44  & 0.865 & 1.000 & 0.865 & 0.865 & 30.185 \\
        \midrule
        9  & 32537 & 7 & Ours & 217 & 87  & 0.773 & 0.679 & 0.773 & 0.707 & 1238.228 \\
           &       &   & SVM  &     & 171 & 0.779 & 1.000 & 0.779 & 0.779 & 113.423 \\
           &       &   & RF   &     & 171 & 0.793 & 1.000 & 0.793 & 0.793 & 104.033 \\
           &       &   & DT   &     & 171 & 0.765 & 1.000 & 0.765 & 0.765 & 110.804 \\
        \midrule
        10 & 33315 & 10 & Ours & 575 & 444 & 0.887 & 0.854 & 0.887 & 0.868 & 985.966 \\
           &       &    & SVM  &     & 2321 & 0.835 & 0.866 & 0.835 & 0.808 & 31.634 \\
           &       &    & RF   &     & 2321 & 0.833 & 0.868 & 0.833 & 0.800 & 31.258 \\
           &       &    & DT   &     & 2321 & 0.797 & 0.840 & 0.797 & 0.756 & 31.688 \\
        \midrule
        11 & 37364 & 4 & Ours & 94  & 10  & 0.947 & 0.953 & 0.947 & 0.947 & 5.579 \\
           &       &   & SVM  &     & 140 & 0.798 & 1.000 & 0.798 & 0.798 & 26.410 \\
           &       &   & RF   &     & 140 & 0.809 & 1.000 & 0.809 & 0.809 & 25.745 \\
           &       &   & DT   &     & 140 & 0.734 & 1.000 & 0.734 & 0.734 & 27.083 \\
        \midrule
        12 & 39582 & 6 & Ours & 566 & 189 & 0.877 & 0.890 & 0.877 & 0.876 & 304.137 \\
           &       &   & SVM  &     & 441 & 0.820 & 0.836 & 0.820 & 0.819 & 13.926 \\
           &       &   & RF   &     & 441 & 0.832 & 0.847 & 0.832 & 0.831 & 13.629 \\
           &       &   & DT   &     & 441 & 0.807 & 0.831 & 0.807 & 0.805 & 14.261 \\
        \midrule
        13 & 39716 & 3 & Ours & 53  & 15  & 1.000 & 1.000 & 1.000 & 1.000 & 19.338 \\
           &       &   & SVM  &     & 118 & 0.925 & 1.000 & 0.925 & 0.925 & 13.133 \\
           &       &   & RF   &     & 118 & 0.925 & 1.000 & 0.925 & 0.925 & 13.049 \\
           &       &   & DT   &     & 118 & 0.830 & 1.000 & 0.830 & 0.830 & 13.391 \\
        \midrule
        14 & 44077 & 4 & Ours & 226 & 52  & 1.000 & 1.000 & 1.000 & 1.000 & 642.643 \\
           &       &   & SVM  &     & 23  & 0.991 & 1.000 & 0.991 & 0.991 & 61.789 \\
           &       &   & RF   &     & 23  & 0.996 & 1.000 & 0.996 & 0.996 & 60.206 \\
           &       &   & DT   &     & 23  & 0.996 & 1.000 & 0.996 & 0.996 & 63.950 \\
    
    \bottomrule
  \end{tabular}
\end{table}

%%%%%%% Này của XGB
\begin{table}[htbp]
  \centering
  \caption{XGB Classification Results with Top $k$ Features}
  \label{tab:XGB_results}
  \scriptsize
  \begin{tabular}{cccccccccccc}
    \toprule
    ID & Dataset & Class & Method & Samples & \multicolumn{1}{c}{Top k} & \multicolumn{4}{c}{Classification Results} & {Training} \\
    \cmidrule(lr){7-10} 
        &         &       &         &   & Features & Acc & Prec & Rec & F1 Score & Time (s)  \\
  \midrule
        1  & 20685 & 6 & Ours & 327 & 114 & 0.955 & 0.961 & 0.955 & 0.955 & 40.015 \\
           &       &   & SVM  &     & 85  & 0.804 & 0.839 & 0.804 & 0.793 & 10.932 \\
           &       &   & RF   &     & 85  & 0.829 & 0.854 & 0.829 & 0.822 & 11.592 \\
           &       &   & DT   &     & 85  & 0.851 & 0.874 & 0.851 & 0.843 & 11.531 \\
        \midrule
        2  & 20711 & 5 & Ours & 90  & 10  & 0.833 & 0.841 & 0.833 & 0.830 & 0.328 \\
           &       &   & SVM  &     & 43  & 0.744 & 1.000 & 0.744 & 0.744 & 62.459 \\
           &       &   & RF   &     & 43  & 0.811 & 1.000 & 0.811 & 0.811 & 63.819 \\
           &       &   & DT   &     & 43  & 0.811 & 1.000 & 0.811 & 0.811 & 63.947 \\
        \midrule
        3  & 21050 & 4 & Ours & 310 & 60  & 0.790 & 0.791 & 0.790 & 0.784 & 16.183 \\
           &       &   & SVM  &     & 275 & 0.732 & 0.769 & 0.732 & 0.701 & 9.478 \\
           &       &   & RF   &     & 275 & 0.748 & 0.782 & 0.748 & 0.724 & 9.977 \\
           &       &   & DT   &     & 275 & 0.752 & 0.796 & 0.752 & 0.720 & 9.622 \\
        \midrule
        4  & 21122 & 7 & Ours & 158 & 159 & 0.938 & 0.921 & 0.938 & 0.924 & 44.087 \\
           &       &   & SVM  &     & 78  & 0.848 & 1.000 & 0.848 & 0.848 & 162.044 \\
           &       &   & RF   &     & 78  & 0.854 & 1.000 & 0.854 & 0.854 & 164.271 \\
           &       &   & DT   &     & 78  & 0.867 & 1.000 & 0.867 & 0.867 & 166.522 \\
        \midrule
        5  & 29354 & 3 & Ours & 53  & 10  & 1.000 & 1.000 & 1.000 & 1.000 & 0.271 \\
           &       &   & SVM  &     & 35  & 0.906 & 1.000 & 0.906 & 0.906 & 23.871 \\
           &       &   & RF   &     & 35  & 0.774 & 1.000 & 0.774 & 0.774 & 23.896 \\
           &       &   & DT   &     & 35  & 0.830 & 1.000 & 0.830 & 0.830 & 23.359 \\
        \midrule
        6  & 30784 & 3 & Ours & 229 & 50  & 0.978 & 0.979 & 0.978 & 0.976 & 6.344 \\
           &       &   & SVM  &     & 42  & 0.921 & 1.000 & 0.921 & 0.921 & 121.816 \\
           &       &   & RF   &     & 42  & 0.913 & 1.000 & 0.913 & 0.913 & 119.124 \\
           &       &   & DT   &     & 42  & 0.921 & 1.000 & 0.921 & 0.921 & 123.405 \\
        \midrule
        7  & 31312 & 3 & Ours & 498 & 10  & 0.860 & 0.848 & 0.860 & 0.843 & 0.388 \\
           &       &   & SVM  &     & 195 & 0.851 & 0.863 & 0.851 & 0.810 & 7.677 \\
           &       &   & RF   &     & 195 & 0.833 & 0.828 & 0.833 & 0.797 & 7.432 \\
           &       &   & DT   &     & 195 & 0.831 & 0.850 & 0.831 & 0.790 & 7.762 \\
        \midrule
        8  & 31552 & 3 & ours & 111 & 10  & 1.000 & 1.000 & 1.000 & 1.000 & 0.557 \\
           &       &   & SVM  &     & 44  & 0.856 & 1.000 & 0.856 & 0.856 & 53.782 \\
           &       &   & RF   &     & 44  & 0.892 & 1.000 & 0.892 & 0.892 & 52.092 \\
           &       &   & DT   &     & 44  & 0.892 & 1.000 & 0.892 & 0.892 & 53.220 \\
        \midrule
        9  & 32537 & 7 & ours & 217 & 95  & 0.795 & 0.693 & 0.795 & 0.736 & 26.388 \\
           &       &   & SVM  &     & 171 & 0.783 & 1.000 & 0.783 & 0.783 & 282.386 \\
           &       &   & RF   &     & 171 & 0.802 & 1.000 & 0.802 & 0.802 & 283.478 \\
           &       &   & DT   &     & 171 & 0.770 & 1.000 & 0.770 & 0.770 & 283.282 \\
        \midrule
        10 & 33315 & 10 & ours & 575 & 425 & 0.870 & 0.842 & 0.870 & 0.854 & 389.225 \\
           &       &    & SVM  &     & 2321 & 0.861 & 0.883 & 0.861 & 0.838 & 94.119 \\
           &       &    & RF   &     & 2321 & 0.859 & 0.879 & 0.859 & 0.839 & 94.236 \\
           &       &    & DT   &     & 2321 & 0.852 & 0.874 & 0.852 & 0.827 & 96.417 \\
        \midrule
        11 & 37364 & 4 & ours & 94  & 21  & 1.000 & 1.000 & 1.000 & 1.000 & 1.862 \\
           &       &   & SVM  &     & 140 & 0.777 & 1.000 & 0.777 & 0.777 & 59.093 \\
           &       &   & RF   &     & 140 & 0.798 & 1.000 & 0.798 & 0.798 & 59.041 \\
           &       &   & DT   &     & 140 & 0.745 & 1.000 & 0.745 & 0.745 & 60.416 \\
        \midrule
        12 & 39582 & 6 & ours & 566 & 556 & 0.825 & 0.833 & 0.825 & 0.824 & 554.762 \\
           &       &   & SVM  &     & 441 & 0.797 & 0.810 & 0.797 & 0.793 & 21.655 \\
           &       &   & RF   &     & 441 & 0.804 & 0.816 & 0.804 & 0.799 & 22.184 \\
           &       &   & DT   &     & 441 & 0.788 & 0.791 & 0.788 & 0.784 & 22.569 \\
        \midrule
        13 & 39716 & 3 & ours & 53  & 44  & 1.000 & 1.000 & 1.000 & 1.000 & 3.746 \\
           &       &   & SVM  &     & 118 & 0.897 & 1.000 & 0.887 & 0.887 & 23.631 \\
           &       &   & RF   &     & 118 & 0.925 & 1.000 & 0.925 & 0.925 & 22.742 \\
           &       &   & DT   &     & 118 & 0.943 & 1.000 & 0.943 & 0.943 & 24.227 \\
        \midrule
        14 & 44077 & 4 & ours & 226 & 57  & 0.978 & 0.980 & 0.978 & 0.978 & 8.332 \\
           &       &   & SVM  &     & 23  & 0.978 & 1.000 & 0.978 & 0.978 & 132.205 \\
           &       &   & RF   &     & 23  & 0.987 & 1.000 & 0.987 & 0.987 & 133.960 \\
           &       &   & DT   &     & 23  & 0.991 & 1.000 & 0.991 & 0.991 & 133.546 \\

        
    \bottomrule
  \end{tabular}
\end{table}
%%%%%%%%%%%%này của Gradient

\begin{table}[htbp]
  \centering
  \caption{Gradient Boosting Classification Results with Top $k$ Features}
  \label{tab:gb_results}
  \scriptsize
  \begin{tabular}{cccccccccccc}
    \toprule
    ID & Dataset & Class & Method & Samples & \multicolumn{1}{c}{Top k} & \multicolumn{4}{c}{Classification Results} & {Training} \\
    \cmidrule(lr){7-10} 
        &         &       &         &   & Features & Acc & Prec & Rec & F1 Score & Time (s)  \\
  \midrule
        1  & 20685 & 6 & ours & 327 & 468 & 0.939 & 0.947 & 0.939 & 0.936 & 17.957 \\
           &       &   & SVM  &     & 85  & 0.795 & 0.820 & 0.795 & 0.788 & 53.840 \\
           &       &   & RF   &     & 85  & 0.856 & 0.876 & 0.856 & 0.855 & 53.966 \\
           &       &   & DT   &     & 85  & 0.844 & 0.865 & 0.844 & 0.834 & 53.381 \\
        \midrule
        2  & 20711 & 5 & ours & 90  & 38  & 0.889 & 0.921 & 0.889 & 0.892 & 0.664 \\
           &       &   & SVM  &     & 43  & 0.733 & 1.000 & 0.733 & 0.733 & 91.394 \\
           &       &   & RF   &     & 43  & 0.711 & 1.000 & 0.711 & 0.711 & 90.162 \\
           &       &   & DT   &     & 43  & 0.711 & 1.000 & 0.711 & 0.711 & 89.505 \\
        \midrule
        3  & 21050 & 4 & ours & 310 & 65  & 0.806 & 0.802 & 0.806 & 0.804 & 1.844 \\
           &       &   & SVM  &     & 275 & 0.703 & 0.733 & 0.703 & 0.685 & 72.745 \\
           &       &   & RF   &     & 275 & 0.735 & 0.745 & 0.735 & 0.720 & 78.348 \\
           &       &   & DT   &     & 275 & 0.758 & 0.792 & 0.758 & 0.742 & 76.235 \\
        \midrule
        4  & 21122 & 7 & ours & 158 & 257 & 0.875 & 0.872 & 0.875 & 0.871 & 5.544 \\
           &       &   & SVM  &     & 78  & 0.804 & 1.000 & 0.804 & 0.804 & 511.805 \\
           &       &   & RF   &     & 78  & 0.835 & 1.000 & 0.835 & 0.835 & 507.856 \\
           &       &   & DT   &     & 78  & 0.861 & 1.000 & 0.861 & 0.861 & 513.399 \\
        \midrule
        5  & 29354 & 3 & ours & 53  & 12  & 1.000 & 1.000 & 1.000 & 1.000 & 0.387 \\
           &       &   & SVM  &     & 35  & 0.849 & 1.000 & 0.849 & 0.849 & 22.807 \\
           &       &   & RF   &     & 35  & 0.717 & 1.000 & 0.717 & 0.717 & 20.737 \\
           &       &   & DT   &     & 35  & 0.811 & 1.000 & 0.811 & 0.811 & 20.797 \\
        \midrule
        6  & 30784 & 3 & ours & 229 & 39  & 0.957 & 0.957 & 0.957 & 0.957 & 0.747 \\
           &       &   & SVM  &     & 42  & 0.913 & 1.000 & 0.913 & 0.913 & 272.929 \\
           &       &   & RF   &     & 42  & 0.900 & 1.000 & 0.900 & 0.900 & 269.238 \\
           &       &   & DT   &     & 42  & 0.943 & 1.000 & 0.943 & 0.943 & 273.324 \\
        \midrule
        7  & 31312 & 3 & ours & 498 & 169 & 0.880 & 0.853 & 0.880 & 0.854 & 4.961 \\
           &       &   & SVM  &     & 195 & 0.853 & 0.840 & 0.853 & 0.830 & 89.338 \\
           &       &   & RF   &     & 195 & 0.853 & 0.828 & 0.853 & 0.824 & 90.304 \\
           &       &   & DT   &     & 195 & 0.825 & 0.805 & 0.825 & 0.795 & 90.659 \\
       \midrule
        8  & 31552 & 3 & ours & 111 & 13  & 0.957 & 0.917 & 0.957 & 0.936 & 0.310 \\
           &       &   & SVM  &     & 44  & 0.802 & 1.000 & 0.802 & 0.802 & 77.917 \\
           &       &   & RF   &     & 44  & 0.883 & 1.000 & 0.883 & 0.883 & 74.657 \\
           &       &   & DT   &     & 44  & 0.892 & 1.000 & 0.892 & 0.892 & 70.156 \\
        \midrule
        9  & 32537 & 7 & ours & 217 & 54  & 0.773 & 0.705 & 0.773 & 0.734 & 2.249 \\
           &       &   & SVM  &     & 171 & 0.747 & 1.000 & 0.747 & 0.747 & 1942.324 \\
           &       &   & RF   &     & 171 & 0.742 & 1.000 & 0.742 & 0.742 & 1928.460 \\
           &       &   & DT   &     & 171 & 0.747 & 1.000 & 0.747 & 0.747 & 1943.035 \\
        \midrule
        10 & 33315 & 10 & ours & 575 & 351 & 0.896 & 0.865 & 0.896 & 0.877 & 40.149 \\
           &       &    & SVM  &     & 2321 & 0.833 & 0.847 & 0.833 & 0.822 & 4118.864 \\
           &       &    & RF   &     & 2321 & 0.856 & 0.873 & 0.856 & 0.839 & 4055.604 \\
           &       &    & DT   &     & 2321 & 0.840 & 0.852 & 0.840 & 0.824 & 3923.718 \\
        \midrule
        11 & 37364 & 4 & ours & 94  & 24  & 0.947 & 0.953 & 0.947 & 0.947 & 0.480 \\
           &       &   & SVM  &     & 140 & 0.787 & 1.000 & 0.787 & 0.787 & 173.044 \\
           &       &   & RF   &     & 140 & 0.745 & 1.000 & 0.745 & 0.745 & 172.476 \\
           &       &   & DT   &     & 140 & 0.745 & 1.000 & 0.745 & 0.745 & 172.407 \\
        \midrule
        12 & 39582 & 6 & ours & 566 & 352 & 0.816 & 0.828 & 0.816 & 0.816 & 24.821 \\
           &       &   & SVM  &     & 441 & 0.802 & 0.819 & 0.802 & 0.799 & 465.967 \\
           &       &   & RF   &     & 441 & 0.813 & 0.819 & 0.813 & 0.806 & 465.414 \\
           &       &   & DT   &     & 441 & 0.776 & 0.794 & 0.776 & 0.768 & 470.792 \\
        \midrule
        13 & 39716 & 3 & ours & 53  & 24  & 1.000 & 1.000 & 1.000 & 1.000 & 0.299 \\
           &       &   & SVM  &     & 118 & 0.906 & 1.000 & 0.906 & 0.906 & 38.331 \\
           &       &   & RF   &     & 118 & 0.868 & 1.000 & 0.868 & 0.868 & 34.458 \\
           &       &   & DT   &     & 118 & 0.811 & 1.000 & 0.811 & 0.811 & 38.443 \\
        \midrule
        14 & 44077 & 4 & ours & 226 & 18  & 1.000 & 1.000 & 1.000 & 1.000 & 0.711 \\
           &       &   & SVM  &     & 23  & 0.973 & 1.000 & 0.973 & 0.973 & 220.643 \\
           &       &   & RF   &     & 23  & 0.982 & 1.000 & 0.982 & 0.982 & 210.123 \\
           &       &   & DT   &     & 23  & 0.987 & 1.000 & 0.987 & 0.987 & 224.643 \\
     \bottomrule
  \end{tabular}
\end{table}



% The classification results obtained using SVM, Random Forest, XGBoost, and Gradient Boosting reveal distinct performance and computational trade-offs. SVM generally delivers high accuracy and balanced precision, recall, and F1 scores across most datasets, with relatively low training times in some cases; however, its preprocessing time can be substantial when handling a large number of features. In contrast, Random Forest exhibits competitive accuracy, often outperforming SVM on certain datasets, yet it tends to incur significantly higher training times, particularly when the number of top features increases—as observed in dataset 1 where training time was notably high. XGBoost demonstrates robust performance with competitive accuracy and precision; its moderate training and preprocessing times suggest that it strikes a balance between model complexity and computational efficiency, though its performance can be dataset-dependent (e.g., lower accuracy on dataset 13). Meanwhile, Gradient Boosting achieves near-perfect accuracy on several datasets, indicating high robustness in classification, yet its training and total processing times vary considerably across datasets. 


% %=====================================
% \begin{table}[htbp]
%   \centering
%   \caption{Random Forest Classification Results with Top K Features}
%   \label{tab:rf_results}
%   \scriptsize
%   \begin{tabular}{cccccccccccc}
%     \toprule
%     ID & Dataset & Class & Samples & \multicolumn{1}{c}{Top N} & \multicolumn{4}{c}{Classification Results} & \multicolumn{3}{c}{Time (s)} \\
%     \cmidrule(lr){5-5} \cmidrule(lr){6-9} \cmidrule(lr){10-12}
%         &         &       &         & Features & Acc & Prec & Rec & F1 Score & Training & Preprocess & Total \\
%     \midrule
%     1  & 20685  & 6  & 327  & 487 & 0,970 & 0,971 & 0,970 & 0,970 & 40,762  & 13,055  & 16306,229 \\
%     2  & 20711  & 5  & 90   & 10  & 0,833 & 0,841 & 0,833 & 0,830 & 5,396   & 3,703   & 5,396 \\
%     3  & 21050  & 4  & 310  & 72  & 0,726 & 0,727 & 0,726 & 0,711 & 25,093  & 10,252  & 1507,624 \\
%     4  & 21122  & 7  & 158  & 104 & 0,938 & 0,922 & 0,938 & 0,925 & 11,577  & 1,971   & 1039,209 \\
%     5  & 29354  & 3  & 53   & 10  & 1,000 & 1,000 & 1,000 & 1,000 & 3,325   & 1,059   & 3,325 \\
%     6  & 30784  & 3  & 229  & 12  & 1,000 & 1,000 & 1,000 & 1,000 & 14,531  & 8,278   & 43,917 \\
%     7  & 31312  & 3  & 498  & 161 & 0,900 & 0,821 & 0,900 & 0,858 & 1,594   & 19,739  & 221,322 \\
%     8  & 31552  & 3  & 111  & 12  & 1,000 & 1,000 & 1,000 & 1,000 & 6,667   & 2,608   & 20,318 \\
%     9  & 32537  & 7  & 217  & 87  & 0,773 & 0,680 & 0,773 & 0,707 & 17,146  & 0,935   & 1238,228 \\
%     10 & 33315  & 10 & 575  & 444 & 0,887 & 0,854 & 0,887 & 0,868 & 3,215   & 4,947   & 985,866 \\
%     11 & 36895  & 14 & 76   & 20  & 0,929 & 0,929 & 0,929 & 0,929 & 4,347   & 3,242   & 46,355 \\
%     12 & 37364  & 4  & 94   & 10  & 0,947 & 0,953 & 0,947 & 0,947 & 5,579   & 3,380   & 5,579 \\
%     13 & 39582  & 6  & 566  & 189 & 0,877 & 0,890 & 0,877 & 0,876 & 1,908   & 20,970  & 304,137 \\
%     14 & 39716  & 3  & 53   & 15  & 1,000 & 1,000 & 1,000 & 1,000 & 3,172   & 1,528   & 19,338 \\
%     15 & 44077  & 4  & 226  & 52  & 1,000 & 1,000 & 1,000 & 1,000 & 15,408  & 6,125   & 642,643 \\
%     \bottomrule
%   \end{tabular}
% \end{table}


% %=====================================
% \begin{table}[htbp]
%   \centering
%   \caption{XGB Classification Results with Top K Features}
%   \label{tab:xgb_results}
%   \scriptsize
%   \begin{tabular}{cccccccccccc}
%     \toprule
%     IID & Dataset & Class & Samples & \multicolumn{1}{c}{Top N} & \multicolumn{4}{c}{Classification Results} & \multicolumn{3}{c}{Time (s)} \\
%     \cmidrule(lr){5-5} \cmidrule(lr){6-9} \cmidrule(lr){10-12}
%         &         &       &         & Features & Acc & Prec & Rec & F1 Score & Training & Preprocess & Total \\
%     \midrule
%     1  & 20685  & 6  & 327  & 114 & 0,955 & 0,962 & 0,955 & 0,955 & 0,395  & 14,439  & 40,015 \\
%     2  & 20711  & 5  & 90   & 10  & 0,833 & 0,841 & 0,833 & 0,830 & 0,328  & 3,800   & 0,328 \\
%     3  & 21050  & 4  & 310  & 60  & 0,790 & 0,791 & 0,790 & 0,784 & 0,286  & 11,975  & 16,186 \\
%     4  & 21122  & 7  & 158  & 159 & 0,938 & 0,921 & 0,938 & 0,925 & 0,293  & 4,028   & 44,087 \\
%     5  & 29354  & 3  & 53   & 10  & 1,000 & 1,000 & 1,000 & 1,000 & 0,271  & 2,541   & 0,271 \\
%     6  & 30784  & 3  & 229  & 50  & 0,978 & 0,979 & 0,978 & 0,976 & 0,127  & 10,126  & 6,344 \\
%     7  & 31312  & 3  & 498  & 10  & 0,860 & 0,848 & 0,860 & 0,843 & 0,388  & 21,554  & 0,388 \\
%     8  & 31552  & 3  & 111  & 10  & 1,000 & 1,000 & 1,000 & 1,000 & 0,557  & 4,239   & 0,557 \\
%     9  & 32537  & 7  & 217  & 95  & 0,796 & 0,693 & 0,796 & 0,736 & 0,299  & 2,478   & 26,388 \\
%     10 & 33315  & 10 & 575  & 425 & 0,870 & 0,842 & 0,870 & 0,855 & 1,341  & 6,495   & 389,225 \\
%     11 & 36895  & 14 & 76   & 29  & 0,929 & 0,929 & 0,929 & 0,929 & 0,250  & 3,364   & 6,509 \\
%     12 & 37364  & 4  & 94   & 21  & 1,000 & 1,000 & 1,000 & 1,000 & 0,142  & 4,804   & 1,862 \\
%     13 & 39582  & 6  & 566  & 556 & 0,825 & 0,833 & 0,825 & 0,824 & 1,564  & 22,414  & 554,762 \\
%     14 & 39716  & 3  & 53   & 44  & 1,000 & 1,000 & 1,000 & 1,000 & 0,101  & 3,074   & 3,746 \\
%     15 & 44077  & 4  & 226  & 57  & 0,978 & 0,980 & 0,978 & 0,979 & 0,139  & 6,925   & 8,332 \\
%     \bottomrule
%   \end{tabular}
% \end{table}


% \begin{table}[htbp]
%   \centering
%   \caption{Gradient Boosting Classification Results with Top N Features}
%   \label{tab:gb_results}
%   \scriptsize
%   \begin{tabular}{cccccccccccc}
%     \toprule
%     IID & Dataset & Class & Samples & \multicolumn{1}{c}{Top N} & \multicolumn{4}{c}{Classification Results} & \multicolumn{3}{c}{Time (s)} \\
%     \cmidrule(lr){5-5} \cmidrule(lr){6-9} \cmidrule(lr){10-12}
%         &         &       &         & Features & Acc & Prec & Rec & F1 Score & Training & Preprocess & Total \\
%     \midrule
%     1  & 20685  & 6  & 327  & 468 & 0,939 & 0,947 & 0,939 & 0,936 & 17,957 & 12,795 & 17,957 \\
%     2  & 20711  & 5  & 90   & 38  & 0,889 & 0,921 & 0,889 & 0,892 & 0,664  & 3,832  & 0,664 \\
%     3  & 21050  & 4  & 310  & 65  & 0,807 & 0,802 & 0,807 & 0,804 & 1,844  & 9,923  & 1,844 \\
%     4  & 21122  & 7  & 158  & 257 & 0,875 & 0,872 & 0,875 & 0,871 & 5,544  & 1,890  & 5,544 \\
%     5  & 29354  & 3  & 53   & 12  & 1,000 & 1,000 & 1,000 & 1,000 & 0,387  & 0,822  & 0,387 \\
%     6  & 30784  & 3  & 229  & 39  & 0,957 & 0,957 & 0,957 & 0,957 & 0,747  & 8,048  & 0,747 \\
%     7  & 31312  & 3  & 498  & 169 & 0,880 & 0,853 & 0,880 & 0,854 & 4,961  & 18,680 & 4,961 \\
%     8  & 31552  & 3  & 111  & 13  & 0,957 & 0,917 & 0,957 & 0,936 & 0,310  & 2,594  & 0,310 \\
%     9  & 32537  & 7  & 217  & 54  & 0,773 & 0,705 & 0,773 & 0,734 & 2,249  & 0,906  & 2,249 \\
%     10 & 33315  & 10 & 575  & 351 & 0,896 & 0,865 & 0,896 & 0,877 & 40,149 & 4,819  & 40,149 \\
%     11 & 36895  & 14 & 76   & 11  & 0,857 & 0,857 & 0,857 & 0,857 & 0,826  & 3,260  & 1,645 \\
%     12 & 37364  & 4  & 94   & 24  & 0,947 & 0,953 & 0,947 & 0,947 & 0,480  & 3,327  & 0,480 \\
%     13 & 39582  & 6  & 566  & 352 & 0,816 & 0,828 & 0,816 & 0,816 & 24,821 & 20,419 & 24,821 \\
%     14 & 39716  & 3  & 53   & 24  & 1,000 & 1,000 & 1,000 & 1,000 & 0,299  & 1,468  & 0,299 \\
%     15 & 44077  & 4  & 226  & 18  & 1,000 & 1,000 & 1,000 & 1,000 & 0,711  & 5,468  & 0,711 \\
%     \bottomrule
%   \end{tabular}
% \end{table}



The four classification algorithms reveal distinct trade-offs between predictive performance and computational efficiency when applied to high-dimensional gene expression data. For instance, on dataset E-GEOD-20685 (ID 1), SVM attained an accuracy of 95.5\% with 107 selected features and a total processing time of 112.137 seconds, while Random Forest achieved a marginally higher accuracy of 97.0\% but at a prohibitive total time of 16,306.229 seconds. In contrast, XGBoost matched SVM's accuracy (95.5\%) with a considerably lower training time of 0.395 seconds and an overall time of 40.015 seconds, suggesting a more balanced performance. Gradient Boosting, on the other hand, reached an accuracy of 93.9\% using 468 features, with a total time of 17.957 seconds—indicating competitive speed but slightly reduced accuracy compared to the other methods. Similar trends are observed in other datasets; for example, in E-GEOD-29354 (ID 5), all algorithms achieved perfect accuracy, yet their feature counts and processing times varied substantially. These differences underscore the critical need for an approach that not only maintains high classification performance but also minimizes computational overhead. Although Random Forest often produces high accuracy, its excessive time cost can limit its practicality in real-world applications. Conversely, XGBoost offers a compelling balance between accuracy and efficiency, making it particularly well-suited for gene expression classification tasks.

In general, the results of our proposed method outperformed feature selection techniques in terms of accuracy. Moreover, the computational time for the training process was significantly reduced, as our approach identified fewer but more crucial and promising features compared to previous studies. This reduction in the number of features also led to a faster training time. In some cases, although more features were selected, the accuracy achieved was considerably higher than that of other solutions



% \subsection{Discussion}
% % \subsection{Interface of AquaNetCT}

% The AquaNetCT system interface is designed using the Streamlit platform\footnote{\url{https://streamlit.io/}} with Python as the core programming language, offering a user-friendly and interactive experience. The mapping component of the interface leverages OpenCage\footnote{\url{https://opencagedata.com/}}, providing geospatial visualization capabilities.  Libraries utilized in the system include networkx, osmnx, paddleocr, ultralytics, among others, ensuring robust functionality and performance.


% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.9\textwidth]{Images/Interface.png}
% \includegraphics[width=0.9\textwidth]{Images/ChatbotAquaNetCT.png}
% \caption{Interface of Our System.}
% \label{fig:interfaceAquaNetCT}
% \end{figure}

% The system consists of two main components: graph representation and a chatbot. The graph representation section follows the workflow defined in the Framework, covering steps 1 through 5, while the chatbot operates as step 6, facilitating user interactions and queries. Users can search for specific data using graph types, street names, or manhole IDs. Additionally, the system computes and displays the distance between two manholes using the Haversine formula, ensuring accurate geospatial calculations. This combination of features and tools creates a powerful platform for efficient data analysis and visualization in urban infrastructure management.


