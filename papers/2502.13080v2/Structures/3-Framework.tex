Our methodology begins with applying the Boruta algorithm to sift through the high-dimensional gene expression dataset, effectively filtering out irrelevant features and isolating those that are truly significant. This initial reduction is critical because LIME, our subsequent interpretability tool, involves generating numerous perturbed samples and calculating distances—a computationally intensive process, especially in large feature spaces. By narrowing the feature set, we improve LIME's precision and enhance the model's interpretability, ultimately boosting classification performance.

However, the key question remains: how many features should be selected for optimal classification? In our proposed BOLIMES algorithm, which integrates Boruta and LIME, we first reduce the high-dimensional feature set by eliminating irrelevant variables with Boruta. Then, we further refine this subset using LIME to assess the local importance of each feature. Finally, we determine the optimal number of features by evaluating classification performance—selecting the subset that yields the highest accuracy for model training. In general, our model is both efficient and robust, relying only on the most informative features for gene expression classification.

\begin{algorithm}[h]
\scriptsize
    \DontPrintSemicolon
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    
    \Input{Dataset \( D = (X, y) \) with \( X \subseteq \mathbb{R}^p \) and class labels \( y \).}
    \Output{Optimal feature subset \( X_{\text{opt}} \) and trained classifier \( f_{\text{opt}} \).}
    
    \Begin{
        \( X^* \leftarrow \text{Boruta}(D) \) \tcp*{\scriptsize Identify relevant features from \( X \)}
        
        \( \mathcal{I} \leftarrow \text{LIME}(f, X^*) \) \tcp*{\scriptsize Compute local importance scores on \( X^* \)}
        
        \( X^{*}_R \gets \{ x^*_{(i)} \}_{i=1}^{|X^*|} \) \quad  \( \mathcal{I}(x^*_{(1)}) \geq \dots \geq \mathcal{I}(x^*_{(|X^*|)}) \) \tcp*{\tiny \textbf{Rank} features in \( X^* \) in descending order of \( \mathcal{I} \)}
        
        \( \text{best\_acc} \leftarrow 0 \), \( k^* \leftarrow 0 \)\;
        
        \For{\( k = 10 \) \KwTo \( |X^*| \)}{
            \( S_k \gets \{ x^*_{(i)} \}_{i=1}^{k} \) \tcp*{\scriptsize Select top-\( k \) features from \( X^*_R \)}
            \( f_k \leftarrow \text{TrainClassifier}(D_{S_k}) \) \tcp*{\scriptsize Train classifier on \( D_{S_k} \)}
            \( \text{acc} \leftarrow \text{Evaluate}(f_k, D_{S_k}) \) \tcp*{\scriptsize Compute classification accuracy}
            
            \If{\( \text{acc} > \text{best\_acc} \)}{
                \( \text{best\_acc} \leftarrow \text{acc} \)\;
                \( k^* \leftarrow k \)\;
                \( f_{\text{opt}} \leftarrow f_k \)\;
            }
        }
        
        \( X_{\text{opt}} \gets \{ x^*_{(i)} \}_{i=1}^{k^*} \) \tcp*{\scriptsize Select top-\( k^* \) features from \( X^*_R \)}
        
        \Return{\( X_{\text{opt}}, f_{\text{opt}} \)}
    }
    
    \caption{Optimal Feature Selection for Classification using Boruta and LIME}
    \label{alg:BOLIMES}
\end{algorithm}

% The algorithm optimally selects features for classification by integrating Boruta and LIME. First, Boruta extracts a relevant subset \( X^* \) from \( X \). LIME then assigns importance scores \( \mathcal{I} \) to \( X^* \), producing a ranked set \( X^*_R \) where \( \mathcal{I}(x^*_{(1)}) \geq \dots \geq \mathcal{I}(x^*_{(|X^*|)}) \).  For \( k = 1 \) to \( |X^*| \), the algorithm selects the top-\( k \) features \( S_k \), trains a classifier \( f_k \), and evaluates its accuracy. The best-performing model determines the optimal feature count \( k^* \), yielding \( X_{\text{opt}} = \{ x^*_{(i)} \}_{i=1}^{k^*} \) and classifier \( f_{\text{opt}} \). This method ensures feature selection maximizes classification accuracy while maintaining interpretability.


Algorithm \ref{alg:BOLIMES} presents an optimal feature selection framework by integrating Boruta and LIME to refine feature subsets for classification. Given a dataset \( D = (X, y) \), it first applies Boruta to extract a subset \( X^* \) of relevant features. LIME then computes local importance scores \( \mathcal{I} \) for \( X^* \), producing a ranked set \( X^*_R \) where
\[
X^*_R \gets \{ x^*_{(i)} \}_{i=1}^{|X^*|}, \quad \mathcal{I}(x^*_{(1)}) \geq \dots \geq \mathcal{I}(x^*_{(|X^*|)}).
\]
An iterative search determines the optimal number of features \( k^* \) by selecting the top-\( k \) ranked features, training a classifier \( f_k \), and evaluating its accuracy:
\[
S_k \gets \{ x^*_{(i)} \}_{i=1}^{k}, \quad f_k \leftarrow \text{TrainClassifier}(D_{S_k}).
\]
Note that we begin with 
$k=10$ and increase it until 
$|X^*|$, as using a smaller number of vectors would be inefficient. The best-performing classifier defines \( k^* \), yielding the final subset \( X_{\text{opt}} \) and trained model \( f_{\text{opt}} \). 


The algorithm's complexity is driven by three key stages: Boruta for feature selection, LIME for ranking, and iterative classification. Boruta, relying on multiple iterations of Random Forest, has a worst-case complexity of \( O(T \cdot p^2 \log p) \). LIME, which perturbs \( m \) samples per feature, contributes \( O(m \cdot p) \). The final stage trains classifiers iteratively over \( p^* \) ranked features, leading to an overhead of \( O(n p^*{}^2) \) assuming a model with \( O(n k) \) complexity. Thus, the total complexity is:
\(
O(T \cdot p^2 \log p) + O(m \cdot p) + O(n p^*{}^2),
\)
where \( p^* \ll p \) in practice, making the approach feasible for moderate-dimensional data but computationally intensive for extremely large \( p \).




