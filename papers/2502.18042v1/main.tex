\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array,soul}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage{xcolor}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\newcommand{\model}{VLM-E2E}
\newcommand{\nuScenes}{nuScenes}
\newcommand{\carla}{Carla}

\begin{document}

% \title{\model: Enhancing Autonomous Driving with Vision-Language Models and Learnable BEV-Text Fusion for Driver Attention Understanding}
\title{{\model}: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion}
% \title{{\model}: Enhancing End-to-end Autonomous Driving with Attention-Aware Semantic Fusion}

\author{Pei Liu, Haipeng Liu, Haichao Liu, Xin Liu, Jinxin Ni, Jun Ma, \textit{Senior Member, IEEE}
        % <-this % stops a space
\thanks{Pei Liu, Haichao Liu, and Xin Liu are with  The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511453, China (e-mail: pliu061@connect.hkust-gz.edu.cn; hliu369@connect.hkust-gz.edu.cn; xliu969@connect.hkust-gz.edu.cn).}% <-this % stops a space
\thanks{Haipeng Liu is with Li Auto Inc., Shanghai 201800, China (e-mail: liuhaipeng2012@live.com).}
\thanks{Jinxin Ni is with the School of Aeronautics and Astronautics, Xiamen University, Xiamen 361102, China (e-mail: nijinxinlxq@outlook.com).}
\thanks{Jun Ma is with The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511453, China, and also with The Hong Kong University of Science and Technology, Hong Kong SAR, China (e-mail: jun.ma@ust.hk).}
}


% \author{Pei Liu,~\IEEEmembership{Staff,~IEEE,}
%         % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}
% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell Shell \MakeLowercase{\textit{et al.et al.}}: A Sample Article Using : A Sample Article Using IEEEtran..cls for IEEE Journals for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

% 1. 知识蒸馏，VLM能力蒸馏到E2E
% 2. 多模态融合，BEV fuse text
% 3. Text + E2E框架(inference无需text)

% 人类驾驶员依靠驾驶关注信息来应对多样化和动态的现实世界场景。现有的端到端自动驾驶通常 将2D 透视观察序列映射到3D BEV空间。虽然很简单，但它们忽略了 3D 环境的几何信息和对象语义，尤其是驾驶员驾驶关注语义的理解。这种限制限制了它们处理具有挑战的驾驶场景的能力，为了弥补这一差距，我们提出了ST-UAD，这种方法利用视觉语言模型 (VLM) 作为教师，通过提供额外的环境理解来增强训练，增强了对驾驶关注环境信息的语义和动作等非结构化数据的理解。它将多模态大语言模型特征整合到鸟瞰图 (BEV) 特征以进行语义监督。这种监督增强了模型学习更丰富的特征表示的能力，这些特征表示可以捕捉驾驶员当前的注意力。此外，考虑到模态重要性不平衡问题，在AD场景，BEV特征可能text特征重要，我们还提出了一种BEV-Text可学习加权融合策略。重要的是，我们的方法在推理过程中不需要VLM，因此可以实时部署。我们在nuScenes 数据集将我们的方法与之前的最新技术进行了对比。结果证明了我们方法的有效性。

\begin{abstract}
% Human drivers rely on attentional cues to navigate diverse and dynamic real-world driving scenarios. Current autonomous driving approaches typically map 2D perspective observation sequences into 3D space, but this process often results in the loss of critical semantic information, particularly the rich contextual and attentional semantics that human drivers naturally perceive. This limitation hinders their ability to effectively handle complex and challenging driving scenarios. In contrast, Vision-Language Models (VLMs) excel in scene understanding and reasoning, offering a promising solution.
Human drivers adeptly navigate complex scenarios by utilizing rich attentional semantics, but the current autonomous systems struggle to replicate this ability, as they often lose critical semantic information when converting 2D observations into 3D space. In this sense, it hinders their effective deployment in dynamic and complex environments.
Leveraging the superior scene understanding and reasoning abilities of Vision-Language Models (VLMs), we propose {\model}, a novel framework that uses the VLMs to enhance training by providing attentional cues. Our method integrates textual representations into Bird's-Eye-View (BEV) features for semantic supervision, which enables the model to learn richer feature representations that explicitly capture the driver's attentional semantics. By focusing on attentional semantics, {\model} better aligns with human-like driving behavior, which is critical for navigating dynamic and complex environments.
Furthermore, we introduce a BEV-Text learnable weighted fusion strategy to address the issue of modality importance imbalance in fusing multimodal information. This approach dynamically balances the contributions of BEV and text features, ensuring that the complementary information from visual and textual modality is effectively utilized. 
By explicitly addressing the imbalance in multimodal fusion, our method facilitates a more holistic and robust representation of driving environments.
We evaluate {\model} on the {\nuScenes} dataset and demonstrate its superiority over state-of-the-art approaches, showcasing significant improvements in performance. 
\end{abstract}

\begin{IEEEkeywords}
Bird's eye view, multimodal information fusion, end-to-end autonomous driving, vision language models.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:intro}
%1. 自动驾驶与端到端背景，VLM背景， VLM+E2E重要性
%2. 当前问题
%3. contribution： 如何集成VLM与端到端模型？ 如何融合视觉和语言特征？ 如何选择driver attention input? 
% \textbf{How to integrate VLMs with E2E models?}
% \textbf{How to fuse vision and text representations?}
% \textbf{How to represent driver's attentional environment}

Autonomous driving has witnessed remarkable progress in recent years \cite{jiang2023vad, hu2023planning, wang2022detr3d}, with significant advancements in key areas such as perception \cite{philion2020lift, li2022bevformer, liao2022maptr}, motion prediction \cite{chai2019multipath, gu2023vip3d, jiang2022perceive}, and planning \cite{toromanoff2020end, prakash2021multi}. These developments have laid a solid foundation for achieving more accurate and safer driving decisions. Among these, end-to-end (E2E) autonomous driving has emerged as a transformative paradigm, leveraging large-scale data to demonstrate impressive planning capabilities. By directly mapping raw sensor inputs to driving actions, E2E approaches bypass the need for handcrafted intermediate modules, enabling more flexible and scalable solutions.
However, 
Despite these advancements, traditional end-to-end autonomous driving approaches predominantly predict future trajectories or control signals directly, without explicitly considering the driver’s attention to critical information such as traffic dynamics and navigation cues. E2E systems often struggle in complex and ambiguous scenarios due to their limited ability to reason about high-level semantics and contextual cues, such as traffic rules, driver attentions, and dynamic interactions. In contrast, human drivers rely on an attentional decision-making process, where attention to both the surrounding traffic environment and navigation guidance plays a critical role \cite{botvinick2008hierarchical, koechlin2003architecture, badre2008cognitive}. For instance, when approaching an intersection, human drivers naturally prioritize traffic signals, pedestrian movements, and lane markings, dynamically adjusting their focus based on the evolving scene.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/model/model.jpg}
    \caption{{\model} augments the end-to-end driving model with semantic textual descriptions during training. These descriptions extract driver attention from VLMs to encourage the model to learn richer attentional semantics.}
    \label{fig:fig1}
\end{figure}

This limitation has spurred the integration of Vision-Language Models (VLMs) \cite{liu2023visual, wang2025cogvlm, chen2024internvl, achiam2023gpt} into autonomous driving frameworks. Trained on vast multimodal datasets, VLMs excel at tasks requiring high-level semantic reasoning, such as interpreting complex scenes, predicting dynamic interactions, and generating contextual descriptions. Their ability to leverage commonsense knowledge makes them particularly well-suited for addressing challenges in autonomous driving, such as understanding traffic rules, identifying vulnerable road users, and making safe decisions in ambiguous scenarios. By generating text-based descriptions of critical driving cues, VLMs can explicitly capture and prioritize regions of interest that align with human driver attention. This capability enables more human-like decision-making, particularly in safety-critical scenarios where attentional focus is paramount.

% End-to-end autonomous driving approaches predominantly predict future trajectories or control signals directly, without explicitly considering the driver’s attention to critical information such as traffic dynamics and navigation cues. However, such methods often increase the difficulty of model learning and suffer from a lack of interpretability. In contrast, when the human brain performs detailed decision-making, attention to both the surrounding traffic environment and navigation guidance plays a critical role \cite{botvinick2008hierarchical, koechlin2003architecture, badre2008cognitive}. Furthermore, end-to-end models are commonly limited by their lack of driver-like common sense, leading to failures in even simple scenarios. For example, these models might mistakenly interpret a truck carrying traffic cones as a roadblock, resulting in unnecessary braking. These limitations hinder the deployment of end-to-end models in complex real-world driving scenarios.



Motivated by these challenges, we propose {\model} (illustrated in Fig. \ref{fig:fig1}), a novel framework designed to enhance autonomous driving systems by incorporating a deeper understanding of driver attentional semantics. Our approach addresses three key questions:

\textbf{How to integrate VLMs with E2E models?}
While most existing methods integrate VLMs with decision-making modules \cite{xu2024vlm} or other high-level components \cite{jiang2024senna, ma2025leapvad,zhai2024world}, leveraging their semantic understanding capabilities to enhance decision processes, our approach introduces a novel integration strategy. Instead of limiting VLMs to decision modules, we combine them directly with the BEV module, which is widely used to represent and process spatial information from multiple perspectives in autonomous driving. By integrating VLMs into the BEV module, we enable BEV representations to incorporate both visual and textual features, resulting in richer and more semantic-aware spatial understanding. This integration allows the model to not only perceive geometric structures but also reason about high-level driver attentional semantics.

\textbf{How to fuse vision and text representations?}
Existing methods for fusing vision and text representations predominantly rely on attention-based mechanisms \cite{vaswani2017attention, han2022survey}, such as cross-attention or co-attention modules, to align and enhance interactions between modalities. While effective, these approaches often rely on predefined attention mechanisms that lack flexibility in adapting to varying task requirements and time and memory consuming. To address this issue, we propose a BEV-Text learnable weighted fusion strategy, where the importance of each modality is dynamically determined through a learnable weighting mechanism. This approach allows the model to adaptively emphasize visual or textual features based on their relevance to the task, leading to a more robust and context-aware multimodal representation. For instance, in scenarios requiring precise localization such as lane keeping, the model can prioritize BEV features, while in scenarios requiring high-level reasoning such as red lights, it can emphasize text features.

\textbf{How to represent driver attentional environment?}
To effectively model driver attentional environment, we propose a multimodal framework that leverages vision-language representations. First, we utilize front-view images captured from the driving scene as input to BLIP-2 \cite{li2023blip} to generate initial textual descriptions of the environment. These descriptions provide a semantic understanding of key objects and events within the driver's vision scope. To address the hallucination problem of VLMs, we further refine these textual representations using ground truth annotations and high-level maneuvering intentions. This refinement ensures that the generated text is not only accurate but also contextually aligned with the driving task. Finally, the refined text is encoded into a dense representation using a pre-trained CLIP \cite{radford2021learning} model, which aligns the textual information with visual features in a shared embedding space. This textual representation enables the model to capture driver attentional cues, such as focusing on pedestrians near crosswalks or traffic signals at intersections, leading to more human-like decision-making and better safety performance.

We evaluate {\model} on the widely used {\nuScenes} dataset, a comprehensive benchmark for autonomous driving research. Our experimental results demonstrate significant improvements over the baseline methods, highlighting the effectiveness of our approach in enhancing perception, decision-making, and overall driving performance. The key contributions of this work can be summarized as follows:

\begin{itemize} 
\item We propose {\model}, a novel framework that leverages VLMs to enrich the training process with attentional understanding. By integrating semantic and contextual information, {\model} explicitly captures driver attentional semantics, which enables more human-like decision-making in complex driving scenarios.

\item We introduce a BEV-Text learnable weighted fusion strategy that dynamically balances the contributions of BEV and textual modalities. This adaptive fusion mechanism is computationally efficient, which requires minimal additional overhead while significantly enhancing the model’s adaptability and robustness. 

\item To address the hallucination problem of VLMs, we incorporate semantic refinement of text descriptions generated from front-view images. By leveraging ground truth (GT) labels and high-level maneuvering intentions, we ensure that the textual representations are both accurate and highly relevant to the driving task, enhancing the model’s ability to reason about critical driving cues.

\item Extensive experiments on the {\nuScenes} dataset demonstrate the superiority of {\model} over existing methods. Our framework achieves significant improvements in handling complex driving scenarios, showcasing its ability to integrate geometric precision with high-level semantic reasoning for safer and more interpretable autonomous driving.
\end{itemize}

\section{Related Work}
\label{sec:related}
\subsection{BEV Representation from Multi-view Cameras}
BEV representation has emerged as a natural and ideal choice for planning and control tasks in autonomous driving \cite{ng2020bev, zhang2021end, xu2024vlm, chitta2021neat}. Unlike perspective views, BEV avoids issues such as occlusion and scale distortion while preserving the 3D spatial layout of the scene, making it highly suitable for tasks like path planning, object detection, and motion prediction. While LiDAR and HD maps can be easily represented in BEV, projecting vision inputs from camera views into BEV space remains a challenging problem due to the inherent complexity of perspective-to-BEV transformation \cite{zhu2018generative}.

Early approaches to BEV generation relied on geometric methods \cite{mu2023inverse, kim2019deep}, such as IPM \cite{zhuravlev2023towards}, which assumes a flat ground plane to project 2D image pixels into BEV space. However, these methods struggle in complex environments where the ground plane assumption does not hold, such as uneven terrains or dynamic scenes. To address these limitations, learning-based methods have gained prominence. For instance, some works \cite{huang2021bevdet, pan2020cross} implicitly project image inputs into BEV using neural networks. However, the quality of these projections is often limited due to the lack of ground truth BEV data for supervision. Loukkal et al. \cite{loukkal2021driving} proposed an explicit projection method using homography between the image and BEV plane, but this approach is sensitive to calibration errors and environmental variations.

Recent advancements have introduced more sophisticated techniques for BEV generation. Methods like \cite{chen2022persformer,li2022bevformer} acquire BEV features through spatial cross-attention with pre-defined BEV queries, enabling end-to-end learning of perspective-to-BEV transformations. Notably, \cite{philion2020lift} and \cite{hu2021fiery} have demonstrated impressive performance by leveraging estimated depth and camera intrinsics to perform the projection. These methods explicitly model the 3D geometry of the scene, resulting in more accurate and robust BEV representations.

\subsection{Multi-modal Information Fusion Mechanism}
In recent years, attention-based fusion mechanisms and learnable fusion strategies have emerged as dominant paradigms for multi-modal information fusion, addressing the challenges of modality heterogeneity and imbalance. These approaches have demonstrated remarkable success in capturing cross-modal interactions and dynamically adapting to the relevance of each modality, making them particularly suitable for complex tasks such as autonomous driving and robotics.

Attention-based fusion mechanisms leverage the power of attention to model dependencies between modalities, enabling the model to focus on the most informative features. Transformer-based architectures \cite{vaswani2017attention, han2022survey} have become a cornerstone of this approach, utilizing self-attention and cross-attention mechanisms to fuse features from different modalities. For instance, TransFuser \cite{chitta2022transfuser} employs transformers to integrate visual and LiDAR features, achieving state-of-the-art performance in 3D object detection and scene understanding. Similarly, cross-modal attention networks \cite{xu2020cross} use attention to weigh the importance of visual and textual features, enhancing tasks such as image-text matching and visual question answering. These methods excel at capturing long-range dependencies and complex interactions between modalities. However, they often require significant computational resources, limiting their applicability in real-time systems.

On the other hand, learnable fusion mechanisms have gained traction for their ability to dynamically adjust the contribution of each modality based on task-specific requirements. These methods introduce learnable parameters, such as weights or coefficients, to adaptively fuse features during training. For example, Modality-Aware Fusion \cite{liu2024cross} proposes learnable coefficients to balance the importance of visual and LiDAR features, improving robustness in autonomous driving tasks. Another notable approach is Dynamic Fusion Networks \cite{xue2023dynamic}, which use gating mechanisms to selectively combine modalities based on their relevance to the current context. These strategies are particularly effective in handling modality imbalance, where one modality may dominate due to its inherent information richness or task-specific importance. By dynamically adjusting the fusion process, learnable mechanisms ensure that all modalities contribute meaningfully to the final output, enhancing both performance and interpretability.

\subsection{End-to-end Autonomous Driving} 
End-to-end autonomous driving systems have demonstrated significant improvements in overall performance by jointly training all modules under a unified objective, thereby minimizing information loss across the pipeline. In recent years, unified frameworks such as ST-P3 \cite{hu2022st} and UniAD \cite{hu2023planning} have pioneered vision-based E2E systems that seamlessly integrate perception, prediction, and planning modules, achieving state-of-the-art results in complex driving scenarios. Building on these advancements, subsequent research such as VAD \cite{jiang2023vad} and VADv2 \cite{chen2024vadv2} introduced vectorized encoding methods to enhance the efficiency and scalability of scene representation, enabling more robust handling of dynamic environments.

More recently, methods such as Ego-MLP \cite{zhai2023rethinking}, BEV-Planner \cite{li2024ego}, and PARA-Drive \cite{weng2024drive} have explored novel design spaces within modular stacks, focusing on self-state modeling and innovative architectural designs to further enhance driving performance. These approaches have pushed the boundaries of E2E systems by incorporating richer representations of the ego vehicle’s state and its interactions with the environment.

In this work, we build upon ST-P3 by integrating driver attentional text information into the framework. By leveraging natural language descriptions of critical driving cues such as pedestrian crossing ahead or red traffic light, we enable the model to explicitly capture and prioritize regions of interest that align with human driver attention. This enhancement not only improves the interpretability of the system but also ensures that the model’s decisions are more closely aligned with human-like reasoning, particularly in safety-critical scenarios.

\subsection{Vision Language Models in Autonomous Driving}
The integration of VLMs into autonomous driving systems has garnered significant attention due to their inherent capabilities in common sense knowledge, advanced reasoning, and interpretability. These attributes effectively address the limitations of traditional E2E models, making VLMs a promising avenue for enhancing driving systems. Recent research has explored various methodologies to harness VLMs for driving tasks, demonstrating substantial progress in this domain.
For instance, Drive-with-LLMs \cite{chen2024driving} employs a Transformer network to encode ground-truth perception data into a latent space, which is subsequently processed by a Large Language Model (LLM) to predict future trajectories. Similarly, DriveGPT4 \cite{xu2024drivegpt4} leverages VLMs to interpret front-camera video inputs, generating planning control signals and providing natural language explanations for decision-making processes. Further advancements include DriveMLM \cite{wang2023drivemlm}, which validates the efficacy of VLM-based planning in closed-loop simulation environments \cite{dosovitskiy2017carla}, and ELM \cite{zhou2024embodied}, which introduces large-scale pre-training of VLMs using cross-domain video data. These studies collectively underscore that the incorporation of diverse data sources and task-specific training significantly enhances VLM performance in driving-related tasks.

Moreover, several works have proposed specialized data collection strategies and datasets tailored for autonomous driving \cite{sima2024drivelm, qian2024nuscenes,wu2023referring,kim2018textual}, further accelerating the development and application of VLMs in this field. A notable contribution is DriveVLM \cite{tian2024drivevlm}, the first framework to seamlessly integrate VLMs with E2E models. In this approach, VLMs predict low-frequency trajectories, which are subsequently refined by the E2E model to generate the final planning trajectory. Additionally, Senna \cite{jiang2024senna} generates high-level planning decisions in natural language, circumventing the need for VLMs to predict precise numerical values, thereby simplifying the decision-making process.

Despite these advancements, existing methods often overlook the design of driving-specific text prompts that capture the unique semantics of driving scenarios, particularly those incorporating driver attentional cues. These cues are crucial for achieving human-like decision-making in autonomous driving systems.




\section{Methodology}
\label{sec:method}
In this section, we provide a detailed introduction to VLM-E2E, as illustrated in Fig. \ref{fig:architect}. The input scene information includes multi-view image sequences, GT, maneuvering, and user prompts. The front-view image, maneuvering, and user prompts are fed into the VLM-based Text Annotation Generation module to generate descriptive text annotations, while the multi-view images are processed by the visual encoding layer to produce BEV features. These text annotations are then passed to the Text Interaction Guidance Module, where they are encoded into text features using a pre-trained CLIP model. Subsequently, the BEV and text features are fused to support downstream tasks such as perception, prediction, and decision-making. In Section \ref{sec:vlm}, we introduce the design of VLM-based Text Annotation Generation in detail. Sections \ref{sec:e2e} and \ref{sec:text-e2e} focus on the design of the Text Interaction Guidance Module and Vision-based End-to-End Architecture, respectively.

\begin{figure*}
    \centering
    \includegraphics[width=0.85\linewidth]{fig/model/model_all.png}
    \caption{We present {\model}, a driver attention enhanced end-to-end vision-based framework. {\model} consists of three modules: VLM-based Text Annotation Generation, Text Interaction Guidance Module, and Vision-based End-to-end Model.}
    \label{fig:architect}
\end{figure*}

\subsection{VLM-based Text Annotation Generation}
\label{sec:vlm}
\subsubsection{Text Annotation}
Fig. 2 depicts the proposed pipeline for extracting driver attentional information from visual inputs, leveraging the reasoning capabilities of a pre-trained VLM. The semantic annotation extraction process can be formulated as follows:

\begin{equation}
    T = \mathcal{BLIP}_{2}(P, I_{front})
\end{equation}
where $\mathcal{BLIP}_{2}(\cdot)$ denotes the visual language model BLIP-2, $P$ represents the task-specific prompts, $I_{front}$ is the visual input from the ego vehicle's front camera, and $T$ is the generated textual description providing detailed environment-related information. The goal of this process is to utilize task-specific prompts alongside real-time visual inputs to extract actionable and attentional information from BLIP-2. This approach not only emphasizes critical elements such as pedestrians, traffic signals, and dynamic obstacles but also filters out irrelevant scene details, ensuring that the outputs directly support driving decisions.

In our work, we employ a state-of-the-art vision language model BLIP-2 \cite{li2023blip}, capable of performing complex reasoning over visual contexts, to generate precise and contextually relevant descriptions. The model interprets visual scenarios guided by prompts and outputs textual descriptions. This method enhances the dataset's richness by providing driver attentional annotations, thereby improving the understanding and decision-making capabilities of downstream driving models.

We encountered a challenge in determining the visual input. That is, selecting the right images from multiple cameras that can cover 360 degrees of the ego vehicle. Considering that we want to capture the driver’s attentional semantics when driving, the front view images usually contain the most relevant information required for most driving tasks. All-view images contain more distracting information that affects the system’s decision-making, so we choose to use only the front-view images to extract the attentional information. In addition, considering that the ego vehicle and its surroundings are in dynamic motion and the hallucination problem inherent in large models, we use the GT and maneuvering to
refine the annotations of dynamic objects.

\subsection{Text Interaction Guidance Module}
\label{sec:text-e2e}
The driver's attentional text descriptions preserve rich visual semantic cues. It is complementary to the BEV features that mainly represent the 3D geometric information. Hence, BEV-Text fusion is proposed for comprehensive scene understanding from the BEV perspective. 

\subsubsection{Text Encoder}
Given a text input $T$ that provides semantic features to guide the BEV-Text fusion network toward achieving a specified fusion result, the text encoder and embedding within the text interaction guidance architecture are responsible for transforming this input into a text embedding. Among various VLMs, we adopt CLIP \cite{radford2021learning} due to its lightweight architecture and efficient text feature extraction capabilities. Compared to other VLMs, CLIP is computationally less demanding and produces text embeddings with a relatively small feature dimension of $77$, which significantly enhances the efficiency of subsequent BEV-Text feature fusion.
We freeze the text encoder from CLIP to preserve its consistency and leverage its pretrained knowledge. This process can be formally expressed as:
\begin{equation}
    f_{t} = \mathcal{CLIP}_{e}(T)
\end{equation}
where $\mathcal{CLIP} \in \mathbb{R}^{N \times L}$ denotes the CLIP model with wights frozen. $f_{t}$ is the text semantic representations.

In different but semantically similar texts, the extracted features should be close in the reduced Euclidean space. Furthermore, we use the MLP $F_{m}^i$ to mine this connection and further map the text semantic information and the semantic parameters. Therefore, it can be obtained:
\begin{equation}
    \gamma_m = F_m^1(f_{t}), \beta_m=F_m^2(f_{t})
\end{equation}
where $F_m^1$ and $F_m^2$ are the chunk operations to form the text semantic parameters.

\subsubsection{BEV-Text Fusion} 
In the semantic interaction guidance module, semantic parameters interact through feature modulation and fusion features $s_t$, to obtain the effect of guidance. The feature modulation consists of scale scaling and bias control, which adjust the features from two perspectives, respectively. In particular, a residual connection is used to reduce the difficulty of network fitting, inspired by \cite{yi2024text}. For simplicity, it can be described as:
\begin{equation}
    x_t = (1 + \gamma_m) \odot s_t + \beta_m
\end{equation}
where $\odot$ denotes the Hadamard product. $x_t$ denotes the BEV feature of BEV-Text fusion, and $s_t$ denotes the BEV feature defined in Section \ref{subsec:bev}.

\subsection{Vision-based End-to-end Model}
\label{sec:e2e}
\subsubsection{Spatial Temporal BEV Perception}
\label{subsec:bev}
In our framework, the BEV representation is constructed from multi-camera images. The input multi-camera images $\{I_t^1, \cdots, I_t^n\}, n = 6$ at time $t$ are first passed through a shared backbone network, EfficientNet-b4 \cite{tan1905rethinking}, to extract high-dimensional feature maps. For each camera image $k$ at time $t$, we get its encoder features $e_t^k \in \mathbb{R}^{C \times H_e \times W_e}$ and depth estimation $d_t^k \in \mathbb{R}^{D \times H_e \times W_e}$ with $C$ denotes the number of feature channels, $D$ is the number of discrete depth values and $(H_e, W_e)$ depicts the spatial feature size. Implicit depth estimation is applied to infer the depth information for each pixel, enabling the construction of a 3D feature volume. Since the depth values are estimated, we take the outer product of the features with the depth estimation. 
\begin{equation}
    \hat{e}_t^k = e_t^k \otimes d_t^k
\end{equation}
where $\hat{e}_t^k \in \mathbb{R}^{C \times D \times H_e \times W_e}$. Then, to transform the 2D perspective features into a 3D space, we employ a feature lifting module. This module uses camera intrinsic and extrinsic parameters to project the 2D features into a 3D voxel space. The 3D feature volume is then collapsed into a 2D BEV representation by aggregating features along the vertical axis to form the BEV view features $b_t \in \mathbb{R}^{C \times H \times W}$, with $(H, W)$ denotes the spatial size of BEV feature. This is achieved through attention-based aggregation, which preserves the most salient features while maintaining spatial consistency. The resulting BEV map provides a top-down view of the scene, encapsulating both geometric and semantic information.

In addition to the BEV construction pipeline described above, we further incorporate temporal modeling to enhance the dynamic understanding of the scene. Specifically, given the current timestamp $t$ and its $h$ historical BEV features $\{b_{t-h}, \cdots, b_{t-1}, b_t\}$, we first align the historical features to the current frame's coordinate system using a temporal alignment module. This process leverages the relative transformation and rotation matrix $M_{t-i \to t} \in \mathbb{R}^{4 \times 4}$ between adjacent frames. The past BEV feature $b_{t-i}$ is then spatially transformed as: 
\begin{equation}
\hat{b}_{t-i} = \mathcal{W}(b_{t-i}, M_{t-i \to t}), \quad \ i=1,2
\end{equation}
where $\mathcal{W}(\cdot)$ denotes the pose-based BEV feature warping operation, and $\hat{b}_{t-i}$ represents the aligned historical features. Subsequently, the aligned BEV features from the $h$ frames are concatenated to form the spatiotemporal input $\hat{b}=[\hat{b}_{t-h}, \cdots, \hat{b}_{t-1}, \hat{b}_t] \in \mathbb{R}^{h \times C \times H \times W}$. To capture long-term dependencies in dynamic scenes, we use a spatiotemporal transformer module $F_s$. 
\begin{equation}
    s_t = F_s(\hat{b}_{t-h}, \cdots, \hat{b}_{t-1}, \hat{b}_t)
\end{equation}
where $s_t \in \mathbb{R}^{h \times C \times H \times W}$ is the spatiotemporally fused BEV feature. $F_s$ is a spatiotemporal convolutional unit with cross-frame self-attention. Our spatial-temporal BEV representation explicitly models the static and dynamic evolution of the scene, enabling the BEV representation to encode geometric structures and temporal continuity simultaneously.

\subsubsection{Semantic Occupancy Prediction}
The future prediction model is a convolutional gated recurrent unit network taking as input the current state $s_t$ and the latent variable $\eta_t$ sampled from the future distribution during training, or the present distribution $P$ for inference. It recursively predicts future states $(y_{t+1}, \cdots, y_{t+l})$ with $l$ denotes the prediction horizon.

% \textbf{Probabilistic Future Prediction Module.} 
To model the inherent uncertainty in multi-modal future trajectories, we employ a conditional variational framework inspired by \cite{hu2020probabilistic}. Present distribution $P(z|x_t)$ is conditioned solely on the current state $x_t$. Future distribution $P_f(z|x_t, y_{t+1:t+l})$ is augmented with ground-truth future observations $(y_{t+1}, \cdots, y_{t+l})$. This distribution is parameterized as diagonal Gaussian with learnable mean $\mu \in \mathbb{R}^M$ and variance $\sigma^2 \in \mathbb{R}^M$, where $M$ is the latent dimension.
\begin{equation}
    P(z|x_t) = \mathcal{N}(\mu_{pres}, \sigma^2_{press}), 
\end{equation}
\begin{equation}
    P_f(z|x_t, y_{t+1:t+l}) = \mathcal{N}(\mu_{fut}, \sigma^2_{fut})
\end{equation}
In the training phase, to ensure prediction consistency with observed futures while preserving multi-modal diversity, we sample $\eta_t$ from $P_f(z|x_t, y_{t+1:t+l})$ and then optimize a mode-covering KL divergence loss.
\begin{equation}
    \mathcal{L}_{KL} = D_{KL}(P_f(z|x_t, y_{t+1:t+F}) || P(z|x_t))
\end{equation}
which encourages $P(z|x_t)$ to encompass all plausible futures encoded in $P_f$. In the inference phase, future trajectories are generated by sampling from the present distribution $\eta_t \sim P(z|x_t)$, where each sample $\eta_t$ represents a distinct future hypothesis. 

This probabilistic formulation enables our model to generate diverse yet physically plausible futures while maintaining temporal consistency, crucial for handling ambiguous scenarios like unprotected left turns or pedestrian interactions.

% \textbf{Multi-task Decoder.} 
The fusion features $x_t$ are processed by a multi-task decoder $D_p$ to generate instance-aware segmentation masks and motion predictions. The decoder outputs four key predictions: semantic segmentation, instance centerness, instance offset, and future instance flow, which collectively enable robust instance detection, segmentation, and tracking. The semantic segmentation head predicts pixel-wise semantic categories through a convolutional classifier. This provides a dense understanding of the scene layout and object categories. For instance segmentation, we adopt a hybrid center-offset formulation \cite{cheng2020panoptic}. The instance centerness head outputs a heatmap $\mathcal{H}_t \in \mathbb{R}^{H \times W}$ indicating the likelihood of instance centers. During training, a Gaussian kernel is applied to suppress ambiguous regions and focus on high-confidence centers. The instance offset head predicts a vector field $\mathcal{O}_t \in \mathbb{R}^{2 \times H \times W}$, where each vector points to its corresponding instance center. At inference, instance centers are extracted via non-maximum suppression (NMS) on $\mathcal{H}_t$. The future instance flow head predicts a displacement vector field $\mathcal{F}_t \in \mathbb{R}^{2 \times H \times W}$ encoding the motion of dynamic agents over a future horizon $l$. This flow field is used to propagate instance centers across timesteps, ensuring temporal consistency. Specifically, detected instance centers $\{c_i^t\}$ are flow-warped to $t+1$ via $\hat{c}_i^{t+1}=c_i^t + \mathcal{F}_t(c_i^t)$. The warped centers $\{\hat{c}_i^{t+1}\}$ are then matched to detected centers $c_j^{t+1}$ at $t+1$ using the Hungarian algorithm \cite{kuhn1955hungarian}, which solves for optimal assignments based on pairwise IoU. This flow-based matching enables robust cross-frame association even under occlusions or abrupt motion changes.


\subsection{Attention Guided Future Planning}
The primary objective of the proposed motion planner is to generate trajectories that ensure safety, comfort, and efficient progress toward the goal. To achieve this, we employ a motion planner that generates a set of kinematically feasible trajectories, each of which is evaluated using a learned scoring function, inspired by \cite{casas2021mp3, sadat2020perceive, zeng2019end, hu2022st}.

Our scoring function incorporates a probabilistic dynamic occupancy field, which is crucial for encoding the safety of the potential maneuvers. This field encourages cautious driving behaviors by penalizing trajectories that enter occupied regions or get too close to these regions, thus maintaining a safe distance from surrounding obstacles. Additionally, we utilize the probabilistic layers from our online map to inform the scoring function. These layers provide important information, ensuring that the self-driving vehicle (SDV) remains within the drivable area, stays close to the center of the lane, and moves in the correct direction.
Particularly in regions of uncertainty, where occupancy and road structure are less predictable, the planner takes extra care to drive cautiously. Moreover, the planner ensures that the vehicle progresses toward the goal specified by the input high-level command, whether it is to continue forward, make a turn, or navigate other maneuvers.

%\textbf{Objective Cost Function.} 
The planner evaluates all sampled trajectories in parallel. Each trajectory $\tau$ is assessed based on the scoring function $f$, which considers several input factors, including the map $\mathcal{M}$, occupancy $\mathcal{O}$, and the motion $\mathcal{V}$. The trajectory selection process is formulated as:
\begin{equation}
    \tau^* = \underset{\tau}{\text{argmin}} f(\tau, \mathcal{M}, \mathcal{O}, \mathcal{V}, w)
    \label{eq:plan}
\end{equation}
where $\tau^*$ denotes the optimal trajectory, $f(\tau, \mathcal{M}, \mathcal{O}, \mathcal{V}, w)$ is the learned scoring function, $w$ are the learnable parameters of the model.

The scoring function evaluates each trajectory concerning multiple criteria, such as the safety of the maneuver avoiding obstacles, the comfort of the ride such as maintaining smooth motion, and progress toward the goal, as guided by the high-level command. By combining these factors, the motion planner efficiently selects the trajectory that best satisfies all safety, comfort, and progress criteria, ensuring the SDV navigates complex environments in a manner that is both effective and cautious.

%\textbf{Trajectory Sampling.} 
The output of the motion planner is a sequence of vehicle states, which defines the desired motion of the SDV within the planning horizon. In each iteration of the planning process, a set of candidate trajectories is generated and evaluated using the cost function described in  (\ref{eq:plan}). The output of the motion planner is a sequence of vehicle states, which defines the desired motion of the SDV within the planning horizon. The trajectory with the minimum cost is then selected for execution.

To ensure real-time performance, the set of sampled trajectories must remain sufficiently small. However, this set must also represent various possible maneuvers and actions to avoid encroaching obstacles. To strike this balance, we employ a sampling strategy that is aware of the lane structure, ensuring that the sampled trajectories effectively capture a diverse range of driving behaviors while remaining computationally feasible.

In particular, we follow the trajectory sampling method proposed in \cite{sadat2019jointly, werling2010optimal}, where trajectories are generated by combining longitudinal motion with lateral deviations relative to specific lanes, such as the current SDV lane or adjacent lanes. This approach allows the planner to sample trajectories that adhere to lane-based driving principles while incorporating variations in lateral motion. These variations enable the motion planner to handle a wide array of traffic scenarios.

%\textbf{Attentional Refinement.} 
To ensure the planned trajectory adheres to driver attention on traffic regulations and route, we utilize a temporal refinement module that dynamically integrates traffic regulations. Leveraging front-view camera features $e_{front}$ from the encoder, we initialize a GRU-based refinement network to iteratively adjust the initially selected trajectory. The front-view features explicitly encode traffic regulations semantics, enabling the model to halt at red lights or proceed through green signals. 
The recurrent architecture ensures smooth transitions between trajectory points, mitigating abrupt steering or acceleration changes.




\section{Experimental Settings}
\label{sec:experiments}
\subsection{Dataset}
We evaluate our method on the nuScenes dataset \cite{caesar2020nuscenes}, a large-scale autonomous driving benchmark comprising 1,000 diverse driving scenes, each spanning 20 seconds with annotations provided at 2\,Hz. The dataset features a 360° multi-camera rig composed of six synchronized cameras (front, front-left, front-right, back, back-left, back-right) with minimal field-of-view overlap. Precise camera intrinsic and extrinsic are provided for each frame to ensure accurate spatial alignment.

The BEV occupancy labels $\{y_{t+1}, \cdots, y_{t+l}\}$ are generated by projecting 3D bounding boxes of dynamic agents onto the BEV plane, creating a spatiotemporal occupancy grid. All labels are transformed into the ego vehicle’s reference frame using GT future ego-motion, ensuring temporal consistency across frames.

\subsection{Metrics}
The perception performance is assessed using the Intersection over Union (IoU), which quantifies the overlap between predicted and GT object bounding boxes. This metric is commonly used to evaluate the accuracy of object detection and tracking in autonomous driving systems.

\begin{equation}
    IoU = \frac{A \cap B}{A \cup B}
\end{equation}
where $A$ represents the predicted segmentation, $B$ represents the ground truth segmentation.

For prediction evaluation, we employ three key metrics. Panoptic Quality (PQ) evaluates the overall quality of both semantic segmentation and instance detection, accounting for both the accuracy of object classification and the correctness of instance segmentation.
Recognition Quality (RQ) measures the ability of the model to correctly recognize and classify objects within the scene.
Segmentation Quality (SQ) focuses on the accuracy of the predicted segmentation masks, comparing them with the GT to evaluate the precision of the object segmentation.

\begin{equation}
    PQ = \underbrace{\frac{\sum_{(p,g)\in TP}IoU(p,g)}{|TP|}}_{\text{Segmentation Quality (SQ)}} \times \underbrace{\frac{|TP|}{|TP|+\frac{1}{2}|FP|+\frac{1}{2}|FN|}}_{\text{Recognition Quality (RQ)}}
\end{equation}
where $TP$, $FP$, and $FN$ refer to true positives, false positives, and false negatives, respectively. $p, g$ are the predicted and GT instances.

For evaluating the planning performance, we consider two primary metrics. L2 Distance measures the Euclidean distance between the SDV's planned trajectory and real human-driven trajectories. A lower L2 distance indicates that the model is better able to replicate human-like driving behavior, which is important for ensuring natural and comfortable motion. Collision Rate quantifies the percentage of time steps during a trajectory in which the SDV's predicted path collides with the ground truth bounding box of other agents. It provides an indication of the safety of the planner's decisions.
\begin{equation}
    CR = \frac{1}{T}\sum_{t=1}^{T}\mathrm{1}(\tau_t^* \cap o_t \neq \emptyset)
\end{equation}
where $\tau^*$ is the planned trajectory and $o$ is the other agents’ ground-truth occupancy.

\begin{equation}
    L2 = \frac{1}{T}\sum_{t=1}^{T}|| \tau_t^*-\tau_t^{human}||_2
\end{equation}
where $|| \cdot ||$ is the Euclidean distance between the SDV’s planned trajectory at time $t$ and the corresponding human trajectory at the same time.

\subsection{Implementation Details}
Our model utilizes a temporal context of 1.0 seconds of past information to predict the future trajectory over a 2.0-second horizon. In the nuScenes dataset, this corresponds to 3 frames of past context and 4 frames into the future, operating at a frequency of 2\,Hz.

At each past timestep, the model processes 6 camera images, each with a resolution of 224 × 480 pixels. The BEV spatial is $100m \times 100m$ area at a pixel resolution of 50cm in both the x and y directions. This results in a BEV video with spatial dimensions of $200 \times 200$ pixels.

Training is performed using the Adam optimizer with a constant learning rate of $2.0 \times 10^{-3}$. The model is trained for 20 epochs with a batch size of 6, distributed across 4 Tesla A6000 GPUs. To optimize memory usage and accelerate computation, mixed precision training is employed. Additionally, both our model and ST-P3 are trained without depth guidance, ensuring a fair comparison and highlighting the effectiveness of our approach in leveraging semantic and attentional cues for improved performance.

\section{Results}
\subsection{Quantitative Results}
\subsubsection{Perception}
Table \ref{tab:Perception} presents the perception performance of various methods across four key categories: Drivable Area, Lane, Vehicle, and Pedestrian. Our proposed {\model} model demonstrates significant improvements over existing approaches, achieving best results in three out of four categories. Specifically, {\model} outperforms ST-P3 in lane detection with a $2.24\%$ relative improvement, vehicle detection with an $0.75\%$ increase, and Pedestrian detection with a $24.40\%$ boost on the nuScenes validation set. While IVMP achieves the highest score in drivable area detection, {\model} closely follows with a score of 74.69, demonstrating competitive performance. These results highlight the effectiveness of our end-to-end approach in enhancing perception accuracy, particularly in critical tasks such as lane and vehicle detection, which are essential for safe and reliable autonomous driving.

\begin{table}
    \centering
    \caption{Perception results. We report the semantic segmentation IoU (\%) in BEV.}
    \begin{tabular}{c|cccc}
    \toprule
    Method &  Drivable Area & Lane & Vehicle & Pedestrian\\
    \midrule
    VED \cite{lu2019monocular} &60.82 & 16.74 & 23.28 & 11.93 \\
    VPN \cite{pan2020cross} & 65.97 &17.05 &28.17 &10.26 \\
    PON \cite{roddick2020predicting} & 63.05 &17.19 &27.91 &13.93 \\
    Lift-Splat \cite{philion2020lift} & 72.23 &19.98 &31.22 &15.02 \\
    IVMP \cite{wang2021learning} & \textbf{74.70} &20.94 &34.03 &17.38 \\
    FIERY \cite{hu2021fiery}& 71.97 &33.58 &38.00 &17.15 \\
    ST-P3 \cite{hu2022st} & 74.38 &   38.47&  38.79& 14.06 \\
    \midrule
    \model & 74.69 & \textbf{39.33} & \textbf{39.08} & \textbf{17.49}\\
    \bottomrule
    \end{tabular}
    \label{tab:Perception}
\end{table}

\subsubsection{Prediction}
Table \ref{tab:Prediction} presents the prediction performance of various methods on the task of semantic and instance segmentations in BEV for a future horizon of 2.0 seconds. We evaluate the methods using IoU, PQ, SQ, and RQ.
Our proposed method, {\model}, achieves best performance across IoU, PQ, and RQ. Specifically, {\model} attains an IoU of 38.54, representing a $4.47\%$ improvement over ST-P3. In terms of PQ, {\model} achieves a PQ of 29.83, demonstrating superior instance segmentation capabilities compared to ST-P3 and FIERY. 
These results highlight the effectiveness of {\model} in capturing both semantic and instance-level information in dynamic driving scenarios.

\begin{table}
    \centering
    \caption{Prediction results. We report the semantic and instance segmentations in BEV for 2s in the future.}
    \begin{tabular}{c|cccc}
    \hline
    Method & IoU & PQ & SQ & RQ \\
    \hline
    % Static & 32.20 & 27.64 & 70.05 & 39.08 \\
    FIERY \cite{hu2021fiery} &36.20 & 27.80 & - & -\\
    ST-P3 \cite{hu2022st} &36.89 & 29.10 & \textbf{69.77} & 41.71\\
    \model & \textbf{38.54} & \textbf{29.83} & 69.56  & \textbf{42.88}\\ 
    \hline
    \end{tabular}
    \label{tab:Prediction}
\end{table}

\subsubsection{Planning}
The results presented in Table \ref{tab:Planning} illustrate the performance of various methods in planning tasks, specifically focusing on L2 displacement error and collision rate across 1s, 2s, and 3s time horizons. Notably, {\model} exhibits significant improvements, particularly in longer-term planning scenarios.
For the L2 displacement error, {\model} achieves the best performance at the 3s horizon with a score of 2.68, outperforming all other methods. While Vanilla demonstrates superior performance at the 1s and 2s horizons, {\model}'s notable reduction in long-term prediction errors is critical for ensuring reliable autonomous driving.

In terms of collision rates, {\model} achieves state-of-the-art performance, recording the lowest rates of 0.60\% at the 2-second horizon and 1.17\% at the 3-second horizon, surpassing all other methods. Although ST-P3 achieves a slightly lower collision rate at the 1-second horizon, the consistent improvements demonstrated by {\model} over longer time underscore its robustness in minimizing collision risks during extended planning periods.
Overall, {\model} demonstrates substantial advancements in long-term planning accuracy and safety, particularly in reducing both L2 displacement errors and collision rates at the 2s and 3s horizons. These results highlight {\model}'s potential to enhance planning systems in autonomous driving, especially in scenarios that demand reliable long-term predictions and effective collision avoidance.
\begin{table}
    \centering
    \caption{Planning results. We report the L2 (m) and CR (\%) across 1s, 2s, 3s.}
    \begin{tabular}{c|ccc|ccc}
    \hline
    \multirow{2}{*}{Method}  & \multicolumn{3}{c|}{L2 (m)} & \multicolumn{3}{c}{CR (\%)}\\
    ~  & 1s & 2s & 3s & 1s & 2s & 3s \\
    \hline
    Vanilla \cite{codevilla2019exploring} & \textbf{0.50} & \textbf{1.25} & 2.80 & 0.68 &0.98 &2.76 \\
    NMP \cite{chen2020learning} & 0.61 & 1.44 & 3.18 & 0.66 & 0.90 & 2.34 \\
    Freespace \cite{prakash2021multi}& 0.56 & 1.27 & 3.08 & 0.65 & 0.86 & 1.64 \\
    ST-P3 \cite{hu2022st} & 1.33 & 2.11 & 2.90 & \textbf{0.23} & 0.62 & 1.27 \\
    \model & 1.22 & 1.94 & \textbf{2.68} & 0.26 & \textbf{0.60} & \textbf{1.17}\\
    \hline
    \end{tabular}
    \label{tab:Planning}
\end{table}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{fig/results/prediction_1640.png}
    \caption{Qualitative analysis on prediction. (a) shows the multi-view input images. (b) shows the heatmap (blue to red)  which illustrates the probability distribution of instance centers within the scene, with warmer colors indicating higher confidence regions. (c) represents the vehicles segmentation which effectively distinguishes individual instances in the complex traffic scenario. (d) reveals the directional vectors pointing towards the corresponding instance centers for each pixel, demonstrating the model's understanding of spatial relationships. (e) exhibits consistency within each instance, reflecting the characteristic rigid-body motion of vehicles. } 
    \label{fig:prediction}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{fig/results/planning_res.png}
    \caption{Qualitative analysis on planning. (a) shows the multi-view input images. (b) shows the planned trajectory (blue). (d) presents the learned costmap with a warmer color indicates a lower cost.}
    \label{fig:planning}
\end{figure*}

\subsection{Qualitative Analysis}
Fig. \ref{fig:prediction} demonstrates the generated outputs, including instance segmentation, instance center, instance offset, and future flow. Fig. \ref{fig:prediction}(b) features a heatmap highlighting detected objects, while Fig. \ref{fig:prediction}(c) displays the instance segmentation results, where each segment is color-coded to represent different objects. The offset is a vector pointing to the center of the instance in Fig. \ref{fig:prediction}(d). The future flow Fig. \ref{fig:prediction}(e) is a displacement vector field of the dynamic agents. These visualizations enhance the understanding of spatial relationships and the distribution of elements within the environment, underscoring the model's capability to accurately perceive and segment critical features essential for autonomous driving applications.

Fig. \ref{fig:planning} illustrates examples of planning scenarios. In the upper scene, the model accurately predicts the route when provided with turning instructions, effectively navigating through crowded environments in a manner similar to human demonstrations. The bottom scene demonstrates the model's predictions when instructed to proceed straight at an intersection, further highlighting its ability to handle diverse driving scenarios with precision. These examples emphasize the model's advanced planning capabilities in complex and dynamic environments.

% \subsection{Ablation Studies}
% \begin{table}
%     \centering
%     \begin{tabular}{ccc|cc}
%     \hline
%         Transformation & Temporal & Text & V.IoU Long& V.IoU Short\\
%         \hline
%         &&&38.00& \\
%         \checkmark &&& 38.79&\\
%         \checkmark & \checkmark && & \\  
%         \checkmark & \checkmark &\checkmark&39.08 &\\ 
%     \hline
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:ab_per}
% \end{table}

% \begin{table}
%     \centering
%     \begin{tabular}{ccc|cccc}
%     \hline
%     \multirow{2}{*}{Future Flow}  & \multirow{2}{*}{Temporal Loss} & \multirow{2}{*}{probabilistic} & \multicolumn{2}{c}{V.IoU} & \multicolumn{2}{c}{V.PQ} \\
%         ~ & ~ & ~ & Short & Long& Short & Long\\
%         \hline
%         &&&&&& \\

%         \checkmark &&& &&&\\
%         \checkmark & \checkmark && &&& \\  
%         \checkmark & \checkmark &\checkmark& &&&\\ 
%     \hline
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:ab_pre}
% \end{table}


% \begin{table}
%     \centering
%     \begin{tabular}{ccc|cc}
%     \hline
%         Occ & Sampler & Refinement & L2 & CR\\
%         \hline
%         \checkmark &&& &\\
%         \checkmark & \checkmark && &\\
%         \checkmark & \checkmark & \checkmark & &\\        
%     \hline
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:ab_plan}
% \end{table}


\section{Conclusion}
In this paper, we propose {\model}, a novel end-to-end autonomous driving framework that leverages VLMs to enhance semantic understanding of driver attention. Our approach is motivated by the need to address key limitations in existing systems, such as modality imbalance in multi-sensor fusion, insufficient utilization of high-level semantic context, and the lack of interpretability in trajectory planning. To this end, we introduce a BEV-Text learnable weighted fusion strategy to dynamically balance geometric and semantic features, a spatiotemporal module to ensure temporal coherence in dynamic scenes and a probabilistic future prediction module with attention guiede trajectory refinement. These components collectively enable our framework to achieve robust and interpretable performance across perception, prediction, and planning tasks. Future work will focus on extending the framework to incorporate VLMs and E2E into a unified framework and utilize the lidar and radar modalities in our framework to generalize our model in long tail scenarios.



% \section*{Acknowledgments}
% This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.



% {\appendix[Proof of the Zonklar Equations]
% Use $\backslash${\tt{appendix}} if you have a single appendix:
% Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
% If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
% You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
%  starts a section numbered zero.)}


\newpage
\bibliographystyle{IEEEtran}
\bibliography{main}



% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}




\vfill

\end{document}


