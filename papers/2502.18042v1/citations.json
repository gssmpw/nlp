[
  {
    "index": 0,
    "papers": [
      {
        "key": "ng2020bev",
        "author": "Ng, Mong H and Radia, Kaahan and Chen, Jianfei and Wang, Dequan and Gog, Ionel and Gonzalez, Joseph E",
        "title": "{BEV-Seg: Bird's Eye View Semantic Segmentation Using Geometry and Semantic Point Cloud}"
      },
      {
        "key": "zhang2021end",
        "author": "Zhang, Zhejun and Liniger, Alexander and Dai, Dengxin and Yu, Fisher and Van Gool, Luc",
        "title": "{End-to-End Urban Driving by Imitating a Reinforcement Learning Coach}"
      },
      {
        "key": "xu2024vlm",
        "author": "Xu, Yi and Hu, Yuxin and Zhang, Zaiwei and Meyer, Gregory P and Mustikovela, Siva Karthik and Srinivasa, Siddhartha and Wolff, Eric M and Huang, Xin",
        "title": "{VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision}"
      },
      {
        "key": "chitta2021neat",
        "author": "Chitta, Kashyap and Prakash, Aditya and Geiger, Andreas",
        "title": "{NEAT: Neural Attention Fields for End-to-End Autonomous Driving}"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhu2018generative",
        "author": "Zhu, Xinge and Yin, Zhichao and Shi, Jianping and Li, Hongsheng and Lin, Dahua",
        "title": "{Generative Adversarial Frontal View to Bird View Synthesis}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "mu2023inverse",
        "author": "Mu, Xiangru and Ye, Haoyang and Zhu, Daojun and Chen, Tongqing and Qin, Tong",
        "title": "{Inverse Perspective Mapping-Based Neural Occupancy Grid Map for Visual Parking}"
      },
      {
        "key": "kim2019deep",
        "author": "Kim, Youngseok and Kum, Dongsuk",
        "title": "{Deep Learning based Vehicle Position and Orientation Estimation via Inverse Perspective Mapping Image}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhuravlev2023towards",
        "author": "Zhuravlev, Dmitriy",
        "title": "{Towards Real-Time 3D Object Detection Through Inverse Perspective Mapping}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "huang2021bevdet",
        "author": "Huang, Junjie and Huang, Guan and Zhu, Zheng and Ye, Yun and Du, Dalong",
        "title": "{BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View}"
      },
      {
        "key": "pan2020cross",
        "author": "Pan, Bowen and Sun, Jiankai and Leung, Ho Yin Tiga and Andonian, Alex and Zhou, Bolei",
        "title": "{Cross-View Semantic Segmentation for Sensing Surroundings}"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "loukkal2021driving",
        "author": "Loukkal, Abdelhak and Grandvalet, Yves and Drummond, Tom and Li, You",
        "title": "{Driving Among Flatmobiles: Bird-Eye-View Occupancy Grids From a Monocular Camera for Holistic Trajectory Planning}"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chen2022persformer",
        "author": "Chen, Li and Sima, Chonghao and Li, Yang and Zheng, Zehan and Xu, Jiajie and Geng, Xiangwei and Li, Hongyang and He, Conghui and Shi, Jianping and Qiao, Yu and others",
        "title": "{PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark}"
      },
      {
        "key": "li2022bevformer",
        "author": "Li, Z and Wang, W and Li, H and Xie, E and Sima, C and Lu, T and others",
        "title": "{BEVFormer: Learning Bird\u2019s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers}"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "philion2020lift",
        "author": "Philion, Jonah and Fidler, Sanja",
        "title": "{Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "hu2021fiery",
        "author": "Hu, Anthony and Murez, Zak and Mohan, Nikhil and Dudas, Sof{\\'\\i}a and Hawke, Jeffrey and Badrinarayanan, Vijay and Cipolla, Roberto and Kendall, Alex",
        "title": "{FIERY: Future Instance Prediction in Bird's-Eye View From Surround Monocular Cameras}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "{Attention is All you Need}"
      },
      {
        "key": "han2022survey",
        "author": "Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and others",
        "title": "{A Survey on Vision Transformer}"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "chitta2022transfuser",
        "author": "Chitta, Kashyap and Prakash, Aditya and Jaeger, Bernhard and Yu, Zehao and Renz, Katrin and Geiger, Andreas",
        "title": "{TransFuser: Imitation With Transformer-Based Sensor Fusion for Autonomous Driving}"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "xu2020cross",
        "author": "Xu, Xing and Wang, Tan and Yang, Yang and Zuo, Lin and Shen, Fumin and Shen, Heng Tao",
        "title": "{Cross-Modal Attention With Semantic Consistence for Image\u2013Text Matching}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "liu2024cross",
        "author": "Liu, Lei and Zhang, Mengya and Li, Cheng and Li, Chenglong and Tang, Jin",
        "title": "{Cross-Modal Object Tracking via Modality-Aware Fusion Network and a Large-Scale Dataset}"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "xue2023dynamic",
        "author": "Xue, Zihui and Marculescu, Radu",
        "title": "{Dynamic Multimodal Fusion}"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "hu2022st",
        "author": "Hu, Shengchao and Chen, Li and Wu, Penghao and Li, Hongyang and Yan, Junchi and Tao, Dacheng",
        "title": "{ST-P3: End-to-End Vision-Based Autonomous Driving via Spatial-Temporal Feature Learning}"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "hu2023planning",
        "author": "Hu, Yihan and Yang, Jiazhi and Chen, Li and Li, Keyu and Sima, Chonghao and Zhu, Xizhou and Chai, Siqi and Du, Senyao and Lin, Tianwei and Wang, Wenhai and others",
        "title": "{Planning-Oriented Autonomous Driving}"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "jiang2023vad",
        "author": "Jiang, Bo and Chen, Shaoyu and Xu, Qing and Liao, Bencheng and Chen, Jiajie and Zhou, Helong and Zhang, Qian and Liu, Wenyu and Huang, Chang and Wang, Xinggang",
        "title": "{VAD: Vectorized Scene Representation for Efficient Autonomous Driving}"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "chen2024vadv2",
        "author": "Chen, Shaoyu and Jiang, Bo and Gao, Hao and Liao, Bencheng and Xu, Qing and Zhang, Qian and Huang, Chang and Liu, Wenyu and Wang, Xinggang",
        "title": "{VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning}"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "zhai2023rethinking",
        "author": "Zhai, Jiang-Tian and Feng, Ze and Du, Jinhao and Mao, Yongqiang and Liu, Jiang-Jiang and Tan, Zichang and Zhang, Yifu and Ye, Xiaoqing and Wang, Jingdong",
        "title": "{Rethinking the Open-Loop Evaluation of End-to-End Autonomous Driving in nuScenes}"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "li2024ego",
        "author": "Li, Zhiqi and Yu, Zhiding and Lan, Shiyi and Li, Jiahan and Kautz, Jan and Lu, Tong and Alvarez, Jose M",
        "title": "{Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?}"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "weng2024drive",
        "author": "Weng, Xinshuo and Ivanovic, Boris and Wang, Yan and Wang, Yue and Pavone, Marco",
        "title": "{PARA-Drive: Parallelized Architecture for Real-time Autonomous Driving}"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "chen2024driving",
        "author": "Chen, Long and Sinavski, Oleg and H{\\\"u}nermann, Jan and Karnsund, Alice and Willmott, Andrew James and Birch, Danny and Maund, Daniel and Shotton, Jamie",
        "title": "{Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving}"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "xu2024drivegpt4",
        "author": "Xu, Zhenhua and Zhang, Yujia and Xie, Enze and Zhao, Zhen and Guo, Yong and Wong, Kwan-Yee K and Li, Zhenguo and Zhao, Hengshuang",
        "title": "{DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model}"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "wang2023drivemlm",
        "author": "Wang, Wenhai and Xie, Jiangwei and Hu, ChuanYang and Zou, Haoming and Fan, Jianan and Tong, Wenwen and Wen, Yang and Wu, Silei and Deng, Hanming and Li, Zhiqi and others",
        "title": "{DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving}"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "dosovitskiy2017carla",
        "author": "Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen",
        "title": "{CARLA: An Open Urban Driving Simulator}"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "zhou2024embodied",
        "author": "Zhou, Yunsong and Huang, Linyan and Bu, Qingwen and Zeng, Jia and Li, Tianyu and Qiu, Hang and Zhu, Hongzi and Guo, Minyi and Qiao, Yu and Li, Hongyang",
        "title": "{Embodied Understanding of Driving Scenarios}"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "sima2024drivelm",
        "author": "Sima, Chonghao and Renz, Katrin and Chitta, Kashyap and Chen, Li and Zhang, Hanxue and Xie, Chengen and Bei{\\ss}wenger, Jens and Luo, Ping and Geiger, Andreas and Li, Hongyang",
        "title": "{DriveLM: Driving with Graph Visual Question Answering}"
      },
      {
        "key": "qian2024nuscenes",
        "author": "Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang",
        "title": "{NuScenes-QA: A Multi-Modal Visual Question Answering Benchmark for Autonomous Driving Scenario}"
      },
      {
        "key": "wu2023referring",
        "author": "Wu, Dongming and Han, Wencheng and Wang, Tiancai and Dong, Xingping and Zhang, Xiangyu and Shen, Jianbing",
        "title": "{Referring Multi-Object Tracking}"
      },
      {
        "key": "kim2018textual",
        "author": "Kim, Jinkyu and Rohrbach, Anna and Darrell, Trevor and Canny, John and Akata, Zeynep",
        "title": "{Textual Explanations for Self-Driving Vehicles}"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "tian2024drivevlm",
        "author": "Tian, Xiaoyu and Gu, Junru and Li, Bailin and Liu, Yicheng and Wang, Yang and Zhao, Zhiyong and Zhan, Kun and Jia, Peng and Lang, Xianpeng and Zhao, Hang",
        "title": "{DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models}"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "jiang2024senna",
        "author": "Jiang, Bo and Chen, Shaoyu and Liao, Bencheng and Zhang, Xingyu and Yin, Wei and Zhang, Qian and Huang, Chang and Liu, Wenyu and Wang, Xinggang",
        "title": "{Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving}"
      }
    ]
  }
]