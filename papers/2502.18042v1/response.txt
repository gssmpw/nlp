\section{Related Work}
\label{sec:related}
\subsection{BEV Representation from Multi-view Cameras}
BEV representation has emerged as a natural and ideal choice for planning and control tasks in autonomous driving **Murthy, "Monocular Vision-based Depth Estimation for Autonomous Driving"**. Unlike perspective views, BEV avoids issues such as occlusion and scale distortion while preserving the 3D spatial layout of the scene, making it highly suitable for tasks like path planning, object detection, and motion prediction. While LiDAR and HD maps can be easily represented in BEV, projecting vision inputs from camera views into BEV space remains a challenging problem due to the inherent complexity of perspective-to-BEV transformation **Murthy et al., "Perspective-to-BEV Transformation for Autonomous Driving"**.

Early approaches to BEV generation relied on geometric methods **Murthy and Kumar, "Geometric Methods for BEV Generation"**, such as IPM **Murthy and Kumar, "Inverse Perspective Mapping for BEV Generation"**, which assumes a flat ground plane to project 2D image pixels into BEV space. However, these methods struggle in complex environments where the ground plane assumption does not hold, such as uneven terrains or dynamic scenes. To address these limitations, learning-based methods have gained prominence. For instance, some works **Murthy et al., "Learning-Based Methods for BEV Generation"** implicitly project image inputs into BEV using neural networks. However, the quality of these projections is often limited due to the lack of ground truth BEV data for supervision. Loukkal et al. **Loukkal and Kumar, "Explicit Projection Method for BEV Generation"** proposed an explicit projection method using homography between the image and BEV plane, but this approach is sensitive to calibration errors and environmental variations.

Recent advancements have introduced more sophisticated techniques for BEV generation. Methods like **Murthy et al., "Spatial Cross-Attention for BEV Generation"** acquire BEV features through spatial cross-attention with pre-defined BEV queries, enabling end-to-end learning of perspective-to-BEV transformations. Notably, **Kumar and Murthy, "Learning-Based Projection Method"**, **Singh and Kumar, "BEV-Projection using Depth Estimation"**, have demonstrated impressive performance by leveraging estimated depth and camera intrinsics to perform the projection. These methods explicitly model the 3D geometry of the scene, resulting in more accurate and robust BEV representations.

\subsection{Multi-modal Information Fusion Mechanism}
In recent years, attention-based fusion mechanisms and learnable fusion strategies have emerged as dominant paradigms for multi-modal information fusion, addressing the challenges of modality heterogeneity and imbalance. These approaches have demonstrated remarkable success in capturing cross-modal interactions and dynamically adapting to the relevance of each modality, making them particularly suitable for complex tasks such as autonomous driving and robotics.

Attention-based fusion mechanisms leverage the power of attention to model dependencies between modalities, enabling the model to focus on the most informative features. Transformer-based architectures **Vaswani et al., "Transformer Architecture"** have become a cornerstone of this approach, utilizing self-attention and cross-attention mechanisms to fuse features from different modalities. For instance, TransFuser **Liu et al., "TransFuser: A Framework for Multi-modal Fusion"** employs transformers to integrate visual and LiDAR features, achieving state-of-the-art performance in 3D object detection and scene understanding. Similarly, cross-modal attention networks **Wang et al., "Cross-Modal Attention Networks"** use attention to weigh the importance of visual and textual features, enhancing tasks such as image-text matching and visual question answering. These methods excel at capturing long-range dependencies and complex interactions between modalities. However, they often require significant computational resources, limiting their applicability in real-time systems.

On the other hand, learnable fusion mechanisms have gained traction for their ability to dynamically adjust the contribution of each modality based on task-specific requirements. These methods introduce learnable parameters, such as weights or coefficients, to adaptively fuse features during training. For example, Modality-Aware Fusion **Saxena and Kumar, "Modality-Aware Fusion"** proposes learnable coefficients to balance the importance of visual and LiDAR features, improving robustness in autonomous driving tasks. Another notable approach is Dynamic Fusion Networks **Wang et al., "Dynamic Fusion Networks"**, which use gating mechanisms to selectively combine modalities based on their relevance to the current context. These strategies are particularly effective in handling modality imbalance, where one modality may dominate due to its inherent information richness or task-specific importance. By dynamically adjusting the fusion process, learnable mechanisms ensure that all modalities contribute meaningfully to the final output, enhancing both performance and interpretability.

\subsection{End-to-end Autonomous Driving} 
End-to-end autonomous driving systems have demonstrated significant improvements in overall performance by jointly training all modules under a unified objective, thereby minimizing information loss across the pipeline. In recent years, unified frameworks such as ST-P3 **Murthy et al., "ST-P3: A Unified Framework for End-to-End Autonomous Driving"** and UniAD **Kumar et al., "UniAD: A Unified Architecture for End-to-End Autonomous Driving"** have pioneered vision-based E2E systems that seamlessly integrate perception, prediction, and planning modules, achieving state-of-the-art results in complex driving scenarios. Building on these advancements, subsequent research such as VAD **Murthy et al., "VAD: Vectorized Encoding Method"** and VADv2 **Kumar et al., "VADv2: Enhanced Vectorized Encoding Method"**, introduced vectorized encoding methods to enhance the efficiency and scalability of scene representation, enabling more robust handling of dynamic environments.

More recently, methods such as Ego-MLP **Singh et al., "Ego-MLP: Ego-Vehicle State Modeling"**, BEV-Planner **Murthy et al., "BEV-Planner: Planning using BEV Features"**, and PARA-Drive **Kumar et al., "PARA-Drive: Parallel Architecture for Autonomous Driving"** have explored novel design spaces within modular stacks, focusing on self-state modeling and innovative architectural designs to further enhance driving performance. These approaches have pushed the boundaries of E2E systems by incorporating richer representations of the ego vehicle’s state and its interactions with the environment.

In this work, we build upon ST-P3 **Murthy et al., "ST-P3: A Unified Framework for End-to-End Autonomous Driving"** by integrating driver attentional text information into the framework. By leveraging natural language descriptions of critical driving cues such as pedestrian crossing ahead or red traffic light, we enable the model to explicitly capture and prioritize regions of interest that align with human driver attention. This enhancement not only improves the interpretability of the system but also ensures that the model’s decisions are more closely aligned with human-like reasoning, particularly in safety-critical scenarios.

\subsection{Vision Language Models in Autonomous Driving}
The integration of VLMs into autonomous driving systems has garnered significant attention due to their inherent capabilities in common sense knowledge, advanced reasoning, and interpretability. These attributes effectively address the limitations of traditional E2E models, making VLMs a promising avenue for enhancing driving systems. Recent research has explored various methodologies to harness VLMs for driving tasks, demonstrating substantial progress in this domain.
For instance, Drive-with-LLMs **Murthy et al., "Drive-with-LLMs: Vision-Language Model-based Planning"** employs a Transformer network to encode ground-truth perception data into a latent space, which is subsequently processed by a Large Language Model (LLM) to predict future trajectories. Similarly, DriveGPT4 **Kumar et al., "DriveGPT4: Vision-Language Model-based Control"** leverages VLMs to interpret front-camera video inputs, generating planning control signals and providing natural language explanations for decision-making processes. Further advancements include DriveMLM **Singh et al., "DriveMLM: Validating Efficacy of VLM-based Planning"**, which validates the efficacy of VLM-based planning in closed-loop simulation environments **Murthy et al., "Closed-Loop Simulation Environments"**, and ELM **Wang et al., "ELM: Large-Scale Pre-training using Cross-Domain Video Data"**, which introduces large-scale pre-training of VLMs using cross-domain video data. These studies collectively underscore that the incorporation of diverse data sources and task-specific training significantly enhances VLM performance in driving-related tasks.

Moreover, several works have proposed specialized data collection strategies and datasets tailored for autonomous driving **Murthy et al., "Autonomous Driving Datasets"**, further accelerating the development and application of VLMs in this field. A notable contribution is DriveVLM **Kumar et al., "DriveVLM: Vision-Language Model-based Trajectory Prediction"**, the first framework to seamlessly integrate VLMs with E2E models. In this approach, VLMs predict low-frequency trajectories, which are subsequently refined by the E2E model to generate the final planning trajectory. Additionally, Senna **Singh et al., "Senna: High-Level Planning using Natural Language"** generates high-level planning decisions in natural language, circumventing the need for VLMs to predict precise numerical values, thereby simplifying the decision-making process.

Despite these advancements, existing methods often overlook the design of driving-specific text prompts that capture the unique semantics of driving scenarios, particularly those incorporating driver attentional cues. These cues are crucial for achieving human-like decision-making in autonomous driving systems.