\section{Related Work}
\label{sec:related}

\textbf{Risk certificates.}
Developing risk certificates for prediction models has been a central topic in theoretical machine learning **Bach, "Learning with a Large Number of Equations"** where rigorous theorems such as VC, Rademacher, and PAC-Bayes bounds have been proposed and studied. Traditionally these have been applied to simple models (e.g., linear classifiers or decision trees), however, there have been recent attempts to extend these theorems to large-scale deep models for non-vacuous risk certificates. For instance, in **Arjovsky et al., "Towards Principled Methods for Training Generative Adversarial Networks"** the high complexity of KL divergence on large deep models has been circumvented by introducing a data-dependent prior that is learned by a held-out portion of training data. Recently **Kuznetsov et al., "Parameter-level quantization of PEFT adapters can lead to non-vacuous bounds using the Kolmogorov complexity bounds"** showed that parameter-level quantization of PEFT adapters can lead to non-vacuous bounds using the Kolmogorov complexity bounds. However, most of these approaches still require a large amount of training data to attain non-vacuous bounds. They fail in the low data regime considered in our paper. 
Within multi-task transfer learning scenarios similar to ours, a meta learning extension of the PAC-Bayes bound was proposed in**Rey et al., "A PAC-Bayesian Approach to Meta-Learning"**.
However, their method was only demonstrated for small-scale toy problems, and it easily incurs out-of-memory computational issue due to the nested gradient computations.

\textbf{Model diffusion methods.}
Recently there have been several attempts to learn a diffusion model for generating model parameters. The neural net diffusion **Nerinckx et al., "Neural Net Diffusion"** trained a diffusion model for BN modules of ResNet-18 where the model parameter data are collected in the course of the SGD training, hence not very diverse. 
The ProtoDiff **Bateni et al., "ProtoDiff: Prototypical-based Diffusion Models for Few-Shot Learning"** aimed for few-shot classification where based on ProtoNet **Chen et al., "ProtoNet: A Prototype-based Framework for Few-Shot Learning"** they aimed to diffuse the prototype vectors. As such the method is highly tied to and geared towards the ProtoNet few-shot learning, and hard to extensible to general model architectures. 
In MetaDiff **Kurle et al., "MetaDiff: Reverse Diffusion Processes in Meta-Learning"** they aimed for avoiding the Hessian computation in meta learning by treating the  inner-loop gradient descent process as a reverse diffusion process. With the conditional sampling, they approached the meta learning problem. 
But to the best of our knowledge, none of the previous works offer risk certificates for the generated models from the diffusion as we did in our work. 


\textbf{Sparse adapter (PEFT) methods.}
%LoRA, VeRA, LoRA-XS...
Sparse adapter models aka PEFT adapters are widely used in practical few-shot adaptation, and effectively reduce the learnable parameters of neural networks. They are also an important component of our approach because learnable parameters should be small enough to be sampled by a diffusion model. 
%are important components in our approach to enable efficient diffusion model training .  VeRA **Shazeer et al., "VeRA: Vector-Based Random Matrix Adaptation"** uses vector-based random matrix adaptation, in which they randomly choose and fix the low-rank matrices in LoRA while introducing trainable vectors. %More specifically, each matrix in layers/modules $W^l$ is defined as (contrast with LoRA):
% %%%%
% \begin{align}
% \textrm{(VeRA)} \ \ 
% W^l = W^l_0 + \Lambda^l_b B^l_{fixed} \Lambda^l_d A^l_{fixed} \ \ \ \ \ \ \ \ 
% \textrm{(LoRA)} \ \ 
% W^l = W^l_0 + B^l A^l
% \label{eq:vera}
% \end{align}
% %%%%
%Here  $W^l_0$ is the fixed pre-trained matrix. 
In LoRA  **Liu et al., "LoRA: Low-Rank Adaptation"** left/right low rank matrices are trainable parameters. In VeRA **Shazeer et al., "VeRA: Vector-Based Random Matrix Adaptation"**, they rather randomly choose and fix these two matrices while training only the diagonal matrices. This will greatly save the size of trainable parameters from LoRA. But still too large to be used as diffusion model state space. We adopt an even more sparse adapter model %called the 
LoRA-XS **Liu et al., "LoRA-XS: Low-Rank Adaptation with Sparse Transformers"** which performs the singular value decomposition, replaces the diagonal singular value matrix with a trainable full matrix. Such adapters have facilitated large-data guarantees **Tognini et al., "Large-Data Guarantees for Neural Networks with Sparse Adapters"**, but in our framework they will facilitate low-shot guarantees.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%