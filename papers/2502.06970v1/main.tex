 %%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{color,colortbl,tabularx}
\definecolor{Gray}{gray}{0.9}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{comment}
\usepackage{color,soul}

% for tables
\usepackage{longtable}
\usepackage{soul}
\usepackage{multirow}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\doublecheck}[1]{\textcolor{blue}{#1}}
\newcommand{\ourShort}{STEEL}
\newcommand{\ourLong}{Sample ThEn Evaluate Learner}



% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% for svg figures
\usepackage{svg}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Provable Risk Certificates via Gradient-Free Diffusion Adaptation}
\icmltitlerunning{Model Diffusion for Certifiable Few-shot Transfer Learning}
\begin{document}

\twocolumn[
%\icmltitle{Tight Provable Risk Certificates for Cross-task Low-shot Transfer Learning\\ via Gradient-Free Diffusion PEFT Adaptation}
\icmltitle{Model Diffusion for Certifiable Few-shot Transfer Learning}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Fady Rezk}{ed,saic}
\icmlauthor{Royson Lee}{saic}
\icmlauthor{Henry Gouk}{ed}
\icmlauthor{Timothy Hospedales}{ed,saic}
\icmlauthor{Minyoung Kim}{saic}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{ed}{School of Informatics, Univesity of Edinburgh, Scotland, UK}
\icmlaffiliation{saic}{Samsung AI Research Cambridge, England, UK}

\icmlcorrespondingauthor{Fady Rezk}{s1985200@ed.ac.uk}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In modern large-scale deep learning, a prevalent and effective workflow for solving low-data problems is adapting powerful pre-trained foundation models (FMs) to new tasks via parameter-efficient fine-tuning (PEFT). However, while empirically effective, the resulting solutions lack generalisation guarantees to certify their accuracy - which may be required for ethical or legal reasons prior to deployment in high-importance applications. In this paper we develop a novel transfer learning approach that is designed to facilitate non-vacuous learning theoretic generalisation guarantees for downstream tasks, even in the low-shot regime. Specifically, we first use upstream tasks to train a {\em  distribution over PEFT parameters}. We then learn the downstream task by a {\em sample-and-evaluate} procedure -- sampling plausible PEFTs from the trained diffusion model and selecting the one with the highest likelihood on the downstream data. Crucially, this confines our model hypothesis to a {\em finite} set of PEFT samples. In contrast to learning in the typical continuous hypothesis spaces of neural network weights, this facilitates tighter risk certificates. We instantiate our bound and show non-trivial generalization guarantees compared to existing learning approaches which lead to vacuous bounds in the low-shot regime. 
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{pdf_figures/support_error_vs_complexity_svg-tex}}
\centerline{\includegraphics[width=2\columnwidth]{pdf_figures/query_error_vs_complexity_svg-tex}}
\caption{Generalisation bounds for adapting CLIP to novel tasks (5-way classification with from 1 to 16 examples per class). Plots show error (y-axis) vs log-complexity term of the bound of Equation \ref{eq:bound} (x-axis). Top/Bottom: The mean support/query (train/test) error for new tasks. The unshaded area corresponds to non-vacuous guarantees, while in the shaded area the total sum of both axes is vacuous. Other approaches are not even close to providing non-vacuous guarantees, while our approach can do so without sacrificing too much fit quality (top), or empirical test accuracy (bottom).}
\label{fig:teaser_clip}
\end{center}
\vskip -0.2in
\end{figure*}

Generalisation certificates are crucial for high-importance applications where accuracy should be guaranteed for legal or ethical reasons. Guarantees should certify the minimum testing accuracy expected on unseen data drawn from the training distribution. However, it is hard to establish non-trivial guarantees for large neural networks, since  large learning capacity tends to produce looser guarantees. As such, there have only been a few successful demonstrations of non-vacuous guarantees for contemporary neural networks, even in the large-data regime \cite{dziugaite2017computing,ortiz2021tighter,lotfi2024nonvacuousLLM}. 

What about learning with sparse rather than large data? The problem of low-data learning is highly topical, due to the plethora of important limited-data applications \cite{wang2019fewShotSurvey}, but challenging due to the difficulty of learning a large number neural network parameters without overfitting. This need has inspired several lines of research that make use of different forms of knowledge transfer, including meta-learning \cite{hospedales20201metaSurveyPAMI} and parameter-efficient transfer learning (PEFT) \citep{hu2021lora} from foundation models. While PEFT methods have recently been more empirically effective, neither family of approach has produced methods that can provide low-shot generalisation guarantees, to our knowledge. From a learning theoretic perspective this is because existing algorithms still search a hypothesis space (e.g., all neural network weights $\theta\in\mathcal{R}^N$) large enough to make known bounds vacuous when instantiated. 

This paper introduces a novel approach to knowledge transfer that ultimately learns downstream tasks by \emph{picking from a finite set of hypothesis}, where the set of hypothesis is fit to the upstream tasks. Our method, \ourShort{} (\ourLong), facilitates using classic finite-hypothesis bounds, which are simple and tight, but not typically used in contemporary machine learning -- which focuses on learning continuous value neural network parameters. 

More specifically, in the upstream phase, we fit PEFT modules to available source tasks, and then train a parameter diffusion model to generate PEFTs according to this task distribution. In the downstream phase, we learn by sample-then-evaluate instead of traditional gradient descent. PEFT modules, unconditionally generated by the diffusion model, are scored using the target task training set, and learning is to choose the highest scoring module. Compared to the original set of upstream models, the diffusion model can be more compact, and can interpolate between the original model set to achieve higher accuracy. This procedure is gradient-free, which has some scalability benefits \cite{malladi2023finetuning,rezk2024liouna}, but more importantly it facilitates the use of PAC-Bayes finite-hypothesis bounds to provide non-vacuous guarantees, all while maintaining similar empirical accuracy to mainstream few-shot learning approaches. Figure~\ref{fig:teaser_clip} shows some illustrative results, demonstrating our learner's ability to maintain practical efficacy while being constrained to low enough complexity to provide non-vacuous guarantees (white zone).

In summary, our contributions are: (1) Introducing a novel learning paradigm for gradient-free transfer learning designed to facilitate accuracy guarantees for downstream tasks, even in the low-shot regime. (2) The first practical demonstration of non-vacuous generalization bounds for low-shot learning in large language and vision architectures. 

% , by adapting large language (LaMP/T5) and (Vision/CLIP) architectures 

% Meaningful generalisation guarantees are hard to establish for large neural networks, since increased learning capacity tends to make them loser. However, non-vacuous bounds 

% Many high-stakes applications (medical, security, etc) are also low-data applications, which makes practically relevant generalisation bounds even hard to come by as normally millions of data are required to achieve non-vacuous results.

% Several areas of research (meta-learning, transfer learning, etc) aim to improve empirical test performance of low-data applications by transferring knowledge from related source tasks. However, to the best of our knowledge, they are all far from being able to provide non-vacuous bounds in low-data regime.

% In this paper we provide a novel approach that for the first time enables modern architectures to achieve non-vacuous bounds in a low-shot regime through transfer learning.

% Specifically, we work with a set of PEFT-compressed models pre-trained on source tasks, and further compress this model set via training a diffusion model over the source neural network parameters. The diffusion model can then be used to sample candidate models for the low-shot target tasks, and learning proceeds in a gradient-free way by simply selecting the diffusion sample with the lowest loss on the target task training set. This approach expresses learning the target task as selection within a finite hypothesis class (of diffusion model samples), and thus allows efficient finite set PAC-Bayes bounds to be applied.  
% -- \textbf{1) finite hypothesis space built from diffusion generative model for task distribution; 2) gradient-free evaluate-then-select learning algorithm; 1 and 2 when combined with the PAC-Bayes finite-hypothesis bound leads to non-vacuous bounds}

% Some highlights on empirical results: We report non-vacuous generalization bounds for large-scale LLM and vision tasks: LaMP/T5 and Vision/CLIP; compared to the recent bounds for deep models including existing parameter-discretization bound~\citep{lotfi2024nonvacuousLLM} and meta learning PAC-Bayes bound~\citep{meta_pac_bayes} which lead to vacuous bounds most of the time.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{%Background: 
Risk Certificates for Deep Models}\label{sec:background}

%Before we describe the detailed problem setup and our proposed approach, in this section we discuss two important background concepts: risk certificates (Sec.~\ref{sec:risk}) and diffusion generative models (Sec.~\ref{sec:diffusion}). 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Risk Certificates for Deep Models}\label{sec:risk}

%\textbf{Key idea.} 
Certifying the generalization performance of a prediction model has been a central topic and studied rigorously in theoretical machine learning~\citep{vapnik95,shalev2014understanding,fmm18}.
The popular Vapnikâ€“Chervonenkis (VC), Rademacher, and PAC-Bayes bounds all aim to relate the empirical risk (which is computable) to the generalization risk (which is impossible to compute) via some inequality relations. 
In this section we discuss, with less mathematical rigor, the key ideas of risk certificates and main bottlenecks that hinder naive application of the existing learning theory methods to deep models. 

Formally, let $h\in\mathcal{H}$ be a {\em hypothesis}, a prediction function $y=h(x)$ that returns output $y$ (e.g., class label) for a given input $x$, and let $\mathcal{H}$ be the {\em hypothesis space}, a set of  hypotheses from which a learning algorithm can select a hypothesis. In modern deep learning practice $h$ corresponds to a deep model with parameters (weights and biases) $\theta$, where we denote by $\Theta$ ($\ni\theta$) the set of all possible parameter values that the deep model can take. So $\Theta$ can be seen as the hypothesis space. A {\em learning algorithm} (e.g., SGD) is a specific recipe to choose $\theta$ from $\Theta$ for a given empirical data $S=\{(x_i,y_i)\}_{i=1}^n$ which is assumed to be i.i.d.~samples from some underlying but unknown distribution $T$. %So there is a clear correspondence between $h$ and $\theta$ as well as $\mathcal{H}$ and $\Theta$, and we will use both interchangeably. 

%(Eisk and ERM) 
The ultimate goal is to find a hypothesis $\theta$ that minimizes the {\em generalization error} or {\em risk} $R(\theta) = \mathbb{E}_{(x,y)\sim T}[l(\theta; x,y)]$ where $l(\theta; x,y)$ is the instance risk (error or loss) (e.g., the cross-entropy loss or 0/1 error) of the hypothesis $\theta$ on the instance $(x,y)$. Since it is not possible to access the population $T$ in the expectation, even computing $R(\theta)$ is impossible. Hence one can rather minimize the {\em empirical risk} $r(\theta) = \frac{1}{n} \sum_{(x,y)\in S} l(\theta; x, y)$, instead. 
But $r(\theta)$ is just a surrogate for $R(\theta)$, and it does not give any certificate about the true generalization risk. In theoretical machine learning, several theorems have been proposed to relate $R$ and $r$, mostly via upper bounding $R$ in terms of $r$. That is, they typically follow the following form.  For any $\theta\in\Theta$:
\begin{equation}
R(\theta) \leq r(\theta) + \textrm{ComplexityTerm}(\dim(\Theta), n)
\label{eq:risk_bound}
\end{equation}
where the complexity term is %independent of the hypothesis $\theta$ itself, and only 
determined by the empirical data size $n=|S|$ and the so-called the {\em hypothesis space complexity} $\dim(\Theta)$ which quantifies how large/small your hypothesis space is. Roughly saying, the complexity term decreases as $n$ goes large and $\dim({\Theta})$ becomes small. 
The well-known VC, Rademacher, and PAC-Bayes bounds all follow the above form\footnote{
In the PAC-Bayes bounds, the risks are measured as expected risks over some (posterior) distribution over $\theta$ rather than point estimates. Also the complexity term is expressed in terms of divergence from a prior distribution. But if we confine the posterior to be sharply concentrated at a single point $\theta$ and use a flat prior, this roughly follows the form of (\ref{eq:risk_bound}). Also certain PAC-Bayes bounds have nonlinear relation between $R$ and $r$, however, they can be approximated as (\ref{eq:risk_bound}), where this simplification does not affect the reasoning in our paper.
}, while they are differentiated (roughly) by how the complexity term is defined precisely. 


\begin{comment}
In this paper we particularly focus on the PAC-Bayes bound since it is known to be tighter than the others with less regularity conditions (e.g., Lipschitz loss functions required in the Rademacher bound)~\citep{shalev2014understanding,fmm18,user_friendly}. There are several versions of PAC-Bayes bounds, but a popular one is the Cantoni's bound that can be written as follows. With probability at least $1-\epsilon$, the following holds:
%%%%
\begin{align}
\mathbb{E}_{Q(\theta)}[R(\theta)] \leq \mathbb{E}_{Q(\theta)}[r(\theta)] + \sqrt{\frac{\textrm{KL}(Q||\pi)\!+\!\log(\frac{1}{\epsilon})}{2n}}
\label{eq:vanilla_pac_bayes}
\end{align}
%%%%
Here $n$ is the test support size. 
We can set $\pi = \mathcal{N}(0, \kappa^2 I)$ for some fixed $\kappa_\pi$ and $Q(\theta) = \mathcal{N}(\mu, \Sigma^2)$ where the parameters $(\mu,\Sigma)$ can be learned by minimizing the right hand side. The sampled version $\theta^z = \mu + \sqrt{\Sigma}\cdot z$, $z\sim\mathcal{N}(0,I)$ can be used during the optimization. Once optimized, the minimum value of the right hand side serves as the error bound for the test task.
\end{comment}

Note that the right hand side of (\ref{eq:risk_bound}) is always computable (both $r$ and the complexity term), and called the {\em risk certificate}. It is called so since the generalization risk is {\em guaranteed} to be no greater than the computed value (i.e., upper bounding). However, when the computed risk certificate is greater than 1, assuming bounded loss $l\in[0,1]$, then it is said {\em vacuous} since saying the generalization risk is less than 1 is useless. However, if it is less than 1 (or sometimes less than the random guess risk in the classification setting), then we say that it is {\em non-vacuous}. Several traditional models and algorithms (e.g., linear SVM) were shown to have non-vacuous risk certificates when proper regularization is imposed to control the complexity term~\citep{vapnik95,shalev2014understanding}. 

Then the follow-up question is whether or not deep neural networks conventionally trained by gradient descent have non-vacuous bounds. 
The answer is no if we apply the traditional learning theory techniques directly, unless the data size $n$ is huge. It is mainly due to the large number of parameters in a deep model that leads to high $\dim(\Theta)$. Although recent sparse adapter (aka PEFT) tricks in deep learning (e.g., LoRA~\citep{hu2021lora} and LoRA-XS~\citep{loraxs}) can be considered, the gradient-based learning still suffers from the large hypothesis space complexity due to the inherently continuous space of $\Theta$. To our knowledge, no existing deep learning approach can achieve meaningful generalization bounds in the low-shot regime.  

\begin{comment}
    \doublecheck{Recently, \cite{lotfi2024nonvacuousLLM} proposed non-vacuous bounds by doing quantization at the parameter level with PEFT adapters even sparser than LoRA, however, their non-vacuous bounds rely heavily on a large amount of empirical data, and can become easily vacuous with low-shot data (\hl{SEE OUR EMPIRICAL RESULTS...}).  
In the low-shot (i.e., small $n$) data regime, to the best of our knowledge, most existing methods based on gradient descent failed to yield non-vacuous bounds. 
\hl{JMLR's data-dependent prior might be a remedy, but it requires data split, one for prior training, the other for model/posterior training, which is not feasible for low-shot learning cases. SHOULD WE MENTION THIS??}}
\end{comment}

%However, PAC-Bayes bound for deep models are challenging; even for PEFT the bound, since the bound is dominated by the KL term and the bound can be easily vacuous

% Recent treatment is parameter quantization below: \textbf{Non-vacuous bounds by Quantization.} Wilson with SubLoRAs (Give brief ideas only; details to appendix)




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\subsection{Background on Diffusion Generative Models}\label{sec:diffusion}

\hl{DDPM derivation here...}
    
\end{comment}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{A New Narrative and Background}\label{sec:background}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Approach}\label{sec:main}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Our Approach at High Level}\label{sec:our_approach_high_level}

In this paper, we approach the problem in a completely different perspective. Our key idea is to avoid the continuous hypothesis space and gradient-based learning algorithms by devising a {\em finite} hypothesis space and adopting an (gradient-free) {\em evaluate-then-select} learning algorithm.
First, we focus on the multi-task transfer learning setup where a large number of training tasks are available. We learn a task distribution from these tasks using a diffusion generative model, where we build a finite hypothesis space $\Theta$ comprised of a large number of samples $\theta$ generated from the learned diffusion model. The diffusion models are known to have strong interpolation capability in density estimation, and our hope is that the constructed $\Theta$ is large enough (but still finite) to cover any downstream tasks successfully as long as they come from the same underlying task distribution.

Our choice of learning algorithm is as simple as: {\em evaluate each $\theta\in\Theta$ and choose the one with the minimum empirical risk}. In the large cardinality $\Theta$ cases, we make use of efficient and smart heuristic search algorithms to reduce the forward-pass overhead (as detailed in Sec.~\ref{sec:main}). 
Our diffusion-based finite hypothesis space and the gradient-free learning algorithm, when combined with existing finite-hypothesis PAC-Bayes bounds, surprisingly leads to tight non-vacuous risk certificates on large-scale LLM/vision benchmarks with large-scale FLAN-T5/CLIP models. In addition the actual test performance of our approach on these benchmarks is comparable to standard learning algorithms. We do not pay a huge price for these guarantees. 
%are significantly better than strong baselines and other competitors. 
We also note that our evaluate-then-select learning algorithm essentially performs PAC-Bayes bound minimization since the complexity term is fixed/constant (details in Sec.~\ref{sec:main}). This turns out to be the optimal learning strategy in terms of risk certificates. 


%However, for the multi-task learning scenario we do care about in this paper, there is little work on how to bound the transferred model on downstream task. A straightforward method is to follow any multi-task learning and downstream adaptation (e.g., SGD). One recent approach below is exception but incur several issues in the deep learning (incl. peft) cases.

%One potential competitor to our approach, under the same meta/multi-task learning setup, is the recent PAC-Bayes meta learning bounds~\citep{meta_pac_bayes}. They extended the vanilla (non-meta learning) PAC-Bayes bound for the meta learning setup by introducing a new concept of hyper-prior and hyper-posterior distributions. However, their concrete methods were only demonstrated for MNIST/MLP-level toy scale problems, and our own in-house implementation of their approach for T5/CLIP-scale models simply failed to run due to the memory overhead (OOM). \hl{Furthermore, although we implemented a Reptile-like modification of their algorithm to address the memory issue, it showed nearly vacuous bounds in all our benchmarks (HOPEFULLY!!).}

Before we jump into our main approach in greater detail in Sec.~\ref{sec:main_detail}, we will formally define the problem setup.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Setup and Notation}\label{sec:setup}

%In this section we state the problem that we aim to solve, specifically the {\em low-shot cross-task transfer learning} problem, and discuss related challenges. 
We describe the {\em low-shot cross-task transfer learning} problem that we aim to solve in the paper. 
Consider that we have a training pool of {\em related} tasks $T_1,\dots,T_N$, i.i.d.~sampled from some unknown but true task distribution $p_{true}(T)$. 
%We can learn some model $M$ with these training tasks in various different ways. For instance, it could be a single deep network trained with the unioned data from these tasks, or some ensemble of trained networks, or even a pool of trained networks with one for each task, and so on. Once trained, our main interest is {\em test-time low-shot adaptation}. 
At test time, we are given a new test task $T^*$ sampled from the same $p_{true}(T)$, but only in the form of low-shot data samples $S^*=\{(x_i,x_y)\}_{i=1}^n$ (aka support data) from $T^*$. By low shot, we mean the number of support samples $n$ is small. 
Our goal is to find a way to ensure tight generalization bounds for the underlying deep models. As discussed in Sec.~\ref{sec:background}, a main challenge here is that we have low-shot data $S^*$ and a large number of model parameters, where the latter immediately translates into high hypothesis space complexity. Hence, applying the traditional learning theories directly to this problem leads to vacuous error bounds. We come up with a new method that exploits the training tasks to transfer the knowledge to unseen tasks so that it can offer tight non-vacuous risk certificates.  

%There are several options. One option is to adapt the trained model $M$ to the new task by taking some gradient descent steps with $S^*$. But since $S^*$ consists of just a few data samples, there is a risk of {\em overfitting}, possibly losing/forgetting useful information stored in the trained $M$. To reduce the risk of overfitting, another option is to consider taking a few gradient steps. But it is not clear in principle exactly how many steps we should take. Alternatively, one may opt to ignore $S^*$ and just use the (unadapted) model $M$ in the downstream prediction for $T^*$. But this may not be ideal since we would give up the only hint about the new task, i.e., $S^*$, that is available to us.

%To address these issues in a principled manner, it is essential to come up with a risk certificate, an upper bound of the generalization error of the adapted model on the test distribution $T^*$, beyond just the empirical error on $S^*$. Although traditionally in machine learning there have been rigorous studies on deriving generalization bounds for prediction models (aka hypotheses), a main challenge here is that we have low-shot data $S^*$ and a large number of model parameters where the latter immediately translates into high hypothesis space complexity. Hence, it is likely that applying the traditional learning theories directly to this problem leads to vacuous error bounds. The main goal in this paper is to come up with a new methodology that exploits the training tasks for adaptation or knowledge transfer to unseen tasks while offering tight non-vacuous risk certificates. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Details of Our Approach}\label{sec:main_detail}

%{\em Note: PEFT model zoo and diffusion-based sampling are our two proposed methods.}

Our first observation is that the gradient-based model adaptation to low-shot data samples $S^*$ needs to avoided to reduce the hypothesis space complexity as discussed in Sec.~\ref{sec:background}. 
%can be risky due to the underfitting-overfitting trade off that we discussed in Sec.~\ref{sec:setup}. There must be some sort of strong inductive bias to be taken into account. 
Our key intuition is that we can learn the task distribution $p_{true}(T)$ using the training tasks $\{T_i\}_{i=1}^N$, but while doing so, we will introduce some strong inductive bias or regularization. 
Let $\theta_i$ be the learned neural network parameters for task $T_i$. 
(Throughout the paper, we deal with PEFT adapter parameters as $\theta$, leaving the pre-trained main backbone parameters fixed.)
We can view $\theta_i$ as the best description for the task $T_i$. We can collect task-wise trained network parameters $\{\theta_i\}_{i=1}^N$, and learning the task distribution $p_{true}(T)$ can be regarded as a {\em density estimation problem}, namely estimating the density $p(\theta)$ from the i.i.d.~samples $\{\theta_i\}_{i=1}^N$. In other words, we treat $p(\theta)$ as a parametric description or surrogate for $p_{true}(T)$. 

%We employ the recent diffusion modeling techniques~[CITE] as our work horse for density estimation. That is, 
We learn a diffusion model $p(\theta)$ with $\{\theta_i\}_{i=1}^N$ as training data following~\citep{ddpm}. 
%as described in Sec.~\ref{sec:diffusion}. 
The estimated $p(\theta)$, as a proxy for $p_{true}(T)$, can be used at test time: We generate many plausible candidate samples $\theta$ (i.e., tasks $T$), among which we select the one that is closest to $T^*$. Since we only have the support data set $S^*$ of $T^*$ available, we find the candidate sample that has the least discrepancy with $S^*$. 
%If our network is of the prediction form, say $p_\theta(y|x)$, %and let $S^*=\{(x_i,x_y)\}_{i=1}^n$, then the 
The least discrepancy criterion can be defined as the %maximum likelihood (equivalently, minimum loss) 
minimum loss selection rule, i.e., %$\arg\max_{\theta\in \Theta} \sum_{(x,y)\in S^*} \log p_\theta(y|x)$ 
$\arg\min_{\theta\in \Theta} \sum_{(x,y)\in S^*} l(\theta; x,y)$ where $\Theta$ is the set of candidate samples from the diffusion model $p(\theta)$. Importantly, this is the empirical risk minimization where the candidate diffusion sample set $\Theta$ serves as our {\em hypothesis space}.

It is important to note that this learning strategy is nothing but {\em selection from a finite hypothesis space}, instead of searching a continuous hypothesis space as in gradient-based parameter-level fine-tuning. The choice of the finite hypothesis set (diffusion model samples) prior to learning  provides an opportunity for a strong regularizing inductive bias that can be learned from the upstream tasks (by training the diffusion model). 
%Noticeably, this does not incur any gradient-based adaptation. Gradient-free adaptation is promising since we can enforce strong regularization during adaptation, where the regularizer (in this case the diffusion model $p(\theta)$) is learned from training data. Also we can aggressively explore the regularized search space via intensive sampling. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Risk Certificate with Finite Hypothesis Space}\label{sec:setup}

%{\em Our bounds derived from finite-hypothesis PAC-Bayes bounds with the uniform prior...}

Suppose we have a well-trained zoo of (PEFT adapter) parameters $\overline{\Theta} = \{\theta_i\}_{i=1}^N$, where each $\theta_i$ is the optimal parameters for the $i$-th training task ($i=1,\dots,N$). 
%
Without the diffusion training with $\overline\Theta$, nothing can prevent us from using the the model zoo $\overline{\Theta}$ itself as our hypothesis space, i.e., $\Theta = \overline\Theta$. This becomes more reasonable as $N$ goes larger since $\overline{\Theta}$ would be richer and closer to the true $p_{true}(T)$. 
We will call this strategy of $\Theta = \overline\Theta$ the {\em model zoo} strategy. In contrast, another proposal of ours is to train a diffusion model $p(\theta)$ with $\overline{\Theta}$, and build $\Theta$ using the samples from it, which we call the \ourShort\footnote{Alternatively, we can union model zoo and diffusion samples together to build a larger hypothesis space. This is practically effective, but in this paper we focus only on model zoo and diffusion strategies so as to contrast their behaviors more carefully.}.
Whereas both are our proposals, the diffusion is our main strategy as it is more attractive in the following aspects: i) the model zoo strategy requires a large amount of space to store the $N$ models while the diffusion strategy is more scalable, only needing to store the diffusion model itself; ii) The diffusion model is widely known to have strong interpolation capability to approximate the target density better, which in practice can provide improved test accuracy


Few-shot adaptation is done by evaluate-then-select: %Formally, % as follows: For a new test task (of a distribution $T^*$) with a small number of support samples $S^*\!\sim\!T^*$, we can choose the model from the hypothesis space $\Theta$ that yields the smallest loss on $S^*$:
%%%%
\begin{align}
\theta^* = \arg\min_{\theta\in\Theta} %\ l(\theta; S^*). 
\ r(\theta) = \frac{1}{n} \sum_{(x,y)\in S^*} l(\theta; x, y)
\label{eq:strategy}
\end{align}
%%%%
We expect that $\Theta$ is rich enough to represent the true task distribution $p_{true}(T)$ faithfully, and the adapted (``selected'') $\theta^*$ will generalize well on unseen samples from $T^*$. 

A crucial benefit of our test-time adaptation strategy (\ref{eq:strategy}) is that we have a tight provable generalization error bound that can serve as a risk certificate for its test-time prediction quality. This mainly originates from the {\em finite} hypothesis space $\Theta$. More specifically, using the PAC-Bayes theorems (e.g., Sec.~2.1.3 in~\citep{user_friendly}), we can show that with probability at least $1-\epsilon$,
%%%%
\begin{align}
R(\theta) \leq r(\theta) + C \cdot \sqrt{\frac{\log\frac{|\Theta|}{\epsilon}}{2n}} \ \ \ \ \textrm{for any $\theta\in\Theta$} 
\label{eq:bound}
\end{align}
%%%%
where $R(\theta) = \mathbb{E}_{(x,y)\sim T^*}[l(\theta; x,y)]$ is the generalization error of $\theta$, %$r(\theta) = \frac{1}{n} \sum_{z\in S^*} l(\theta; z)$ is the empirical error on the support data with size $n = |S^*|$, 
and $C$ is the maximal loss value (i.e., $0\leq l \leq C)$. The bound immediately comes from the PAC-Bayes theorem with the (data-independent) uniform prior over $\Theta$ and the Dirac's delta posterior choice. 
Since the size of the hypothesis space $|\Theta|$ only appears in the $\log$ term, a massively large $\Theta$ is allowable while retaining a tight bound. Furthermore, the bound can be minimized with the smallest empirical error $r(\theta)$, i.e., $\theta=\theta^*$, which justifies our evaluate-then-select strategy $(\ref{eq:strategy})$. 

However, the computational question naturally arises: {\em How do we solve (\ref{eq:strategy}) efficiently?}
We consider two %computationally viable 
solutions:.
%%%%
\begin{itemize}
%
\item \textbf{Exhaustive search.}
We go through every $\theta$ in $\Theta$, evaluate the loss $r(\theta)$, and choose the minimum one. Even though we guarantee to find $\theta^*$ always with the minimal empirical $r(\theta)$, this is computationally very expensive (often intractable for the LLM cases due to prohibitive $|\Theta|$ forward passes or text generation).
%
\item \textbf{Hierarchical search.}
This is the well-known tree search strategy to find an approximate solution. For instance, we can do hierarchical clustering of $\theta$s in $\Theta$, evaluate the losses of the top level clusters (either cluster centroids or medoids), and choose the best cluster. Then we focus only on the $\theta$s that belong to the selected cluster, discarding the rest, and go on recursively. This may find a good approximate solution $\theta$ close to $\theta^*$ in $O(\log |\Theta|)$ time. However, we may possibly end up with suboptimal (underfit) empirical error $r(\theta)$. %Note that this approximation does not compromise the theoretical guarantee, but may lead to looser than optimal certificates from  (\ref{eq:bound}) if $r(\theta)$ underfits. 
%
\end{itemize}
%%%%
We emphasize again that in all these three strategies, the generalization error bound (\ref{eq:bound}) holds true, but with possibly differently/suboptimally selected $\theta$s in the latter case, which may imply (slight) increase in the empirical loss $r(\theta)$, and hence a slight loosening of the obtained certificate. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}\label{sec:related}

\textbf{Risk certificates.}
Developing risk certificates for prediction models has been a central topic in theoretical machine learning~\citep{vapnik95,shalev2014understanding,fmm18} where rigorous theorems such as VC, Rademacher, and PAC-Bayes bounds have been proposed and studied. Traditionally these have been applied to simple models (e.g., linear classifiers or decision trees), however, there have been recent attempts to extend these theorems to large-scale deep models for non-vacuous risk certificates. For instance, in ~\citet{ortiz2021tighter} the high complexity of KL divergence on large deep models has been circumvented by introducing a data-dependent prior that is learned by a held-out portion of training data. Recently ~\citet{lotfi2024nonvacuousLLM} showed that parameter-level quantization of PEFT adapters can lead to non-vacuous bounds using the Kolmogorov complexity bounds. However, most of these approaches still require a large amount of training data to attain non-vacuous bounds. They fail in the low data regime considered in our paper. 
Within multi-task transfer learning scenarios similar to ours, a meta learning extension of the PAC-Bayes bound was proposed in~\citet{meta_pac_bayes}.
However, their method was only demonstrated for small-scale toy problems, and it easily incurs out-of-memory computational issue due to the nested gradient computations.

\textbf{Model diffusion methods.}
Recently there have been several attempts to learn a diffusion model for generating model parameters. The neural net diffusion~\citep{nndiffu} trained a diffusion model for BN modules of ResNet-18 where the model parameter data are collected in the course of the SGD training, hence not very diverse. 
The ProtoDiff~\citep{protodiff} aimed for few-shot classification where based on ProtoNet~\citep{protonet} they aimed to diffuse the prototype vectors. As such the method is highly tied to and geared towards the ProtoNet few-shot learning, and hard to extensible to general model architectures. 
In MetaDiff~\citep{metadiff} they aimed for avoiding the Hessian computation in meta learning by treating the  inner-loop gradient descent process as a reverse diffusion process. With the conditional sampling, they approached the meta learning problem. 
But to the best of our knowledge, none of the previous works offer risk certificates for the generated models from the diffusion as we did in our work. 


\textbf{Sparse adapter (PEFT) methods.}
%LoRA, VeRA, LoRA-XS...
Sparse adapter models aka PEFT adapters are widely used in practical few-shot adaptation, and effectively reduce the learnable parameters of neural networks. They are also an important component of our approach because learnable parameters should be small enough to be sampled by a diffusion model. 
%are important components in our approach to enable efficient diffusion model training .  VeRA~\citep{vera} uses vector-based random matrix adaptation, in which they randomly choose and fix the low-rank matrices in LoRA while introducing trainable vectors. %More specifically, each matrix in layers/modules $W^l$ is defined as (contrast with LoRA):
% %%%%
% \begin{align}
% \textrm{(VeRA)} \ \ 
% W^l = W^l_0 + \Lambda^l_b B^l_{fixed} \Lambda^l_d A^l_{fixed} \ \ \ \ \ \ \ \ 
% \textrm{(LoRA)} \ \ 
% W^l = W^l_0 + B^l A^l
% \label{eq:vera}
% \end{align}
% %%%%
%Here  $W^l_0$ is the fixed pre-trained matrix. 
In LoRA  \cite{hu2021lora} left/right low rank matrices are trainable parameters. In VeRA~\citep{vera}, they rather randomly choose and fix these two matrices while training only the diagonal matrices. This will greatly save the size of trainable parameters from LoRA. But still too large to be used as diffusion model state space. We adopt an even more sparse adapter model %called the 
LoRA-XS~\citep{loraxs} which performs the singular value decomposition, replaces the diagonal singular value matrix with a trainable full matrix. Such adapters have facilitated large-data guarantees \cite{lotfi2024nonvacuousLLM}, but in our framework they will facilitate low-shot guarantees.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}\label{sec:expmts}
We defer all architectural details, and hyperparameters to Appendix \ref{app:hyperparams}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{(Synthetic) SineLine Few-shot Regression??}\label{sec:expmts_sineline}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Guarantees for few-shot LLM adaptation}\label{sec:expmts_lamp}
\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{pdf_figures/lamp_bound_dist_svg-tex}}
\caption{Distribution of generalisation guarantees (x-axis, log scale) obtained over few-shot LLM adaptation episodes. Vertical lines indicate the vacuous bound threshold. Our \ourShort{} learner provides a dramatically better distribution of outcomes in terms of provable generalisation compared to alternatives.}
\label{fig:lamp_bound_distribution}
\end{center}
\vskip -0.2in
\end{figure*}


\begin{table}[h]
    \centering
    \caption{Results on the LaMP LLM adaptation benchmark.}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ll>{\columncolor{Gray}}rrrr}
         \toprule
           & & \textbf{SGD} & \textbf{LoRA-Hub} & \textbf{MeZO} & \textbf{STEEL}\\
         \midrule
           \multirow{6}{*}{ \rotatebox[origin=c]{90}{\textbf{LaMP-2}} }
            &\% Non-Vacuous Tasks & 0.00\% & \underline{32.13\%} & 0.00\% & \textbf{65.12\%} \\
            &Median Gap & 8.12 & \underline{0.68} & 8.45 & \textbf{0.43} \\
            &Min Bound & 3.32 & \underline{0.59} & 3.60 & \textbf{0.47} \\
            &Median Bound & 8.52 & \underline{1.12} & 8.85 & \textbf{0.80} \\
            &Max Bound & 20.86 & \underline{2.90} & 21.36 & \textbf{1.99} \\
            &Accuracy$\uparrow$ & 63.25\% & 57.51\% & \underline{63.30\%} & \textbf{63.74\%} \\
            &F1$\uparrow$ & 56.15\% & 50.84\% & \underline{57.03\%} & \textbf{55.69\%} \\

            \midrule
            \multirow{6}{*}{ \rotatebox[origin=c]{90}{\textbf{LaMP-3}} }
            &\% Non-Vacuous Tasks & 0.00 & \underline{5.00} & 0.00 &\textbf{15.48} \\
            &Median Gap  & 3.09 & \underline{0.23} & 3.10 & \textbf{0.14} \\
            &Min Bound & 1.35 & \underline{0.51} & 2.54 & \textbf{0.43} \\
            &Median Bound & 3.56 & \underline{1.04} & 3.76 & \textbf{0.93} \\
            &Max Bound & 4.75 & \underline{1.30} & 4.53 & \textbf{1.17} \\
            &MAE$\downarrow$ & 0.217 & \textbf{0.230} & 0.242 &\underline{0.231} \\
            &RMSE$\downarrow$ & 0.511 & \underline{0.526} & 0.531 & \textbf{0.524} \\
            &Cross-Entropy$\downarrow$ & 0.479 & 0.739 & \textbf{0.626} & \underline{0.693} \\
        
            \midrule
            \multirow{6}{*}{ \rotatebox[origin=c]{90}{\textbf{LaMP-5}} }
            &\% Non-Vacuous Tasks & 0.00 & \underline{63.84} & 0.00 & \textbf{99.16} \\
            &Median Gap & 4.04 & \underline{0.41} & 4.04 & \textbf{0.26} \\
            &Min Bound & 1.61 & \underline{0.52} & 2.58 & \textbf{0.40} \\
            &Median Bound & 4.57 & \underline{0.96} & 4.57 & \textbf{0.81} \\
            &Max Bound & 5.86 & \underline{1.25} & 5.88 & \textbf{1.06} \\
            &ROUGE-1$\uparrow$ & 47.04\% & \underline{47.05\%} & 47.03\% & \textbf{47.22\%} \\
            &ROUGE-L$\uparrow$ & 42.79\% & \underline{42.75\%} & 42.73\% & \textbf{42.89\%} \\
         \bottomrule
    \end{tabular}
    }
    \label{tab:lamp_results}
\end{table}

\textbf{Datasets:} For low-shot LLM adaptation, we use the LaMP personalization benchmark \cite{salemi2024lamplargelanguagemodels}. LaMP contains a fixed number of users per dataset that is split over training (seen) and evaluation (unseen) clients. Training and evaluation clients are mutually exclusive. Each client has their own support data and query data. We choose three datasets from the benchmark, namely LaMP-2 (Personalized Movie Tagging), LaMP-3 (Personalized Product Rating), and LaMP-5 (Personalized Scholarly Title Generation). These make nominal classification, ordinal classification, and text generation tasks respectively.

\textbf{Setup:} Following recent work on LaMP \cite{tan-etal-2024-personalized,salemi2024lamplargelanguagemodels}, we first build a base model with task-specific capabilities by end-to-end fine-tuning of Flan-T5 base \cite{flant5} on the user support data seen. Subsequently, to personalize the base model for a user, we train one LoRA-XS module on the support data of each training client \cite{baazy2024loraxs}. LoRA-XS rank of 6 and an alpha of 16 was used, producing a total of 2592 tunable parameters. We build the model zoo by collecting the LoRA-XS modules trained using seen users support data; same data that was aggregated to finetune the base model.

\textbf{Baselines:} We compare our proposal of using the model zoo directly, and our diffusion samples against two gradient-free methods, namely LoRA-Hub \cite{huang2024lorahub}, MeZO \cite{malladi2023finetuning}, and include SGD baseline for completeness. Please note that LoRA-Hub randomly samples from the model zoo and learns a new adapter as a linear combination of the sampled adapters. MeZO and SGD do not use the zoo. We refer the reader to Appendix \ref{app:lamp_hyperparams} for detailed hyperparameters. \ourShort{} uses the bound described in Equation \ref{eq:bound}. Meanwhile, MeZO, SGD and LoRA-Hub use the quantization bound from Equation \ref{eq:qbound}. Finally, for computational efficiency, we use Hierarchical search as described in Section \ref{sec:setup}.

\textbf{Results:} Our main contribution relates to the ability to provably certify the generalisation of low-shot learning. In terms of low-shot LLM adaptation, Figure~\ref{fig:lamp_bound_distribution} visualises the distribution of certification outcomes over a large number of episodes for the three LAMP benchmarks. Taking note of the log-scale on the x-axis for generalisation guarantee strength, we can see that our \ourShort{} learner provides dramatically better guarantees than conventional continuous-parameter learner alternatives, thanks to its discrete hypothesis space. The vertical lines indicate the threshold for vacuous bounds. Standard learners such as SGD and MeZO have no mass left of the threshold, while a substantial number of \ourShort{} learning episodes are non-vacuously guaranteed. Table~\ref{tab:lamp_results} provides more detailed quantitative results in terms of various metrics. To assess provable generalisation, the data visualised in Figure~\ref{fig:lamp_bound_distribution} is summarised as the \% non-vacuous metric (the fraction of episodes which have guarantees with a strength above chance-level), and the median guarantee strength across episodes. To asses practical performance we report relevant metrics for each task such as Accuracy, RMSE, and ROGUE score. 

From these results we can see that: (1) Standard supervised learning approaches such as (gradient-based) SGD and (gradient-free) MeZO have no non-vacuous episodes - no few-shot learning task can be guaranteed. (2) Our \ourShort{} model has the most non-vacuous episodes for each benchmark, with almost every few-shot learning episode being guaranteed in the LAMP-5 benchmarks. And the median \ourShort{} episode has a substantially non-vacuous guarantee for all three benchmarks. (3) Interesingly, LoraHUB combined with \citet{lotfi2024nonvacuousLLM}'s discretization bound also has some non-vacuous episodes, but less than \ourShort{}. (3) \ourShort{} has comparable or better empirical test accuracy compared to existing approaches such as SGD and MeZO, while providing a huge improvement in certifiability. 




% Models:
% \begin{itemize}
% \item Our Diffusion/Zoo-PEFT -- Finite Set PAC-Bayes
% \item LoraHUB $+$ Discretisation -- with about 20-ish parameters
% \item LoraHUB $+$ vanilla PAC-Bayes bounds (assume no data splitting; uninformative prior for simplicity)
% \item MeZo-FT/PEFT $+$ FP-Discretization
% \item SGD-FT/PEFT $+$ FP-Discretization
% \item PAC-Bayes meta-learners (ICML24 Lampert)
% \item Linear readouts
% \item ICL/FID ???
% \end{itemize}
    


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Guarantees for few-shot visual recognition}\label{sec:expmts_clip}
\begin{figure*}[ht]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{pdf_figures/shots_vs_bound_svg-tex}}
\caption{Dependence of generalisation guarantee on training set size. Our finite-hypothesis class learner \ourShort{} achieves non-vacuous guarantees from 4-shot onward. Standard approaches provide no guarantees anywhere in this low-shot range.}
\label{fig:clip_shots_vs_bound}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{pdf_figures/diffusion_sample_size_analysis_svg-tex}}
\caption{``Learning curves" illustrating empirical and certified learning dynamics of \ourShort{} with respect to samples/iterations, which is equivalent to hypothesis space size. More samples improves the training (support) error, while increasing the complexity penalty. The sum of these two terms instantiates the generalisation guarantee (Eq.~\ref{eq:bound}) achieved for a given number of samples.}
\label{fig:diffusion_samples_analysis}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{table}[h]
    \centering
    \resizebox{0.75\columnwidth}{!}{
    \begin{tabular}{l>{\columncolor{Gray}}rrr}
        \toprule
        Method & SGD & BBPT & \ourShort \\
        \midrule
        \multicolumn{4}{c}{\textbf{CUBirds}}\\
        \midrule
        Non-Vacuous Ratio & 0.00\% & 0.00\% & \textbf{100.00\%} \\
        Average Gap & 2.49 & 2.48 & \textbf{0.24} \\
        Min Bound & 2.55 & 2.55 & \textbf{0.30} \\
        Median Bound & 2.58 & 2.59 & \textbf{0.36} \\
        Max Bound & 2.64 & 2.68 & \textbf{0.47} \\
        Average Accuracy & 90.32 & \textbf{89.27} & \underline{88.40} \\

        \midrule
        \multicolumn{4}{c}{\textbf{Describable Textures}}\\
        \midrule        
        Non-Vacuous Ratio & 0.00\% & 0.00\% & \textbf{100.00\%} \\
        Average Gap & 2.43 & 2.46 & \textbf{0.24} \\
        Min Bound & 2.55 & 2.55 & \textbf{0.36} \\
        Median Bound & 2.55 & 2.63 & \textbf{0.42} \\
        Max Bound & 2.58 & 2.78 & \textbf{0.53} \\
        Average Accuracy & 87.95 & \textbf{83.20} & \underline{81.50} \\

        \midrule
        \multicolumn{4}{c}{\textbf{FGVCAircrafts}}\\
        \midrule        
        Non-Vacuous Ratio & 0.00\% & 0.00\% & \textbf{97.50\%} \\
        Average Gap & 2.31 & 2.45 & \textbf{0.22} \\
        Min Bound & 2.58 & 2.64 & \textbf{0.45} \\
        Median Bound & 2.64 & 2.80 & \textbf{0.61} \\
        Max Bound & 2.78 & 3.01 & \textbf{0.85} \\
        Average Accuracy & 65.57 & \textbf{62.37} & \underline{61.37} \\

        \midrule
        \multicolumn{4}{c}{\textbf{Flowers-101}}\\
        \midrule
        Non-Vacuous Ratio & 0.00\% & 0.00\% & \textbf{100.00\%} \\
        Average Gap & 2.51 & 2.50 & \textbf{0.27} \\
        Min Bound & 2.55 & 2.55 & \textbf{0.31} \\
        Median Bound & 2.55 & 2.59 & \textbf{0.40} \\
        Max Bound & 2.55 & 2.70 & \textbf{0.61} \\
        Average Accuracy & 95.90 & \textbf{90.15} & \underline{84.90} \\
        \bottomrule
    \end{tabular}
    }
    \caption{CLIP+CoOp-based few-shot learning. Aggregates over multiple 16-Shots 5-way learning episodes.}
    \label{tab:vision_bound_stats}
\end{table}

\textbf{Datasets:} For vision, we use fine-grained classification datasets that have readily available training (seen) and novel (unseen) classes split. We choose CUBirds \cite{cubirds}, FGVC-Aircraft \cite{aircrafts}, Describable Textures \cite{dtd} and Flowers-101 \cite{flowers}. We use the split offered by learn2learn for the first three datasets  \cite{arnold2020learn2learnlibrarymetalearningresearch}, and the Flowers-101 split from Meta-Dataset \cite{metadataset}.

\textbf{Setup:} We sample random few-shot $n$-shot $k$-way learning tasks as per meta-learning literature \cite{metadataset}. We randomly select 5 classes (5-way) from a given split and for each class we randomly sample $n$-shots. We evaluate 1, 2, 4, 8 and 16-shots. The model zoo is built by sampling from the training classes of the aforementioned datasets. Meanwhile, we evaluate on unseen classes and samples. Please note that training and evaluation classes are mutually exclusive. For a given task, we use CoOp for adaptation \cite{zhou2022coop} which is simply prompt tuning for CLIP. We finetune a 2-token prompt appended in front of the class name for every task to build a model zoo. This results in total tunable parameters of length 1024. For vision experiments, we found that the forward passes were fast enough to conduct exhaustive search over sampled prompts from the diffusion model.

\textbf{Baselines:}  We compare our proposal of using the model zoo directly, and our diffusion samples against Black-Box Prompt Tuning (BBPT) \cite{bbpt} which is a gradient-free version of CoOp, optimizing the prompt using evolutionary optimization. We also include SGD based CoOp for completeness. For details on methods hyperparameters, we refer the reader to Appendix \ref{app:vision_hyperparams}. \ourShort uses the bound described in Equation \ref{eq:bound}. Meanwhile, BBPT and SGD use the quantization bound from Equation \ref{eq:qbound}.

\textbf{Results:} The results in terms of mean training and testing error versus complexity are summarised for three datasets in Figure~\ref{fig:teaser_clip}. The dots for each learner reflect the training set sizes of 1, 2, 4, 8, and 16-examples per-class, and the white/grey zone separation delineates the space of non-vacuous vs vacuous bound outcomes. The main message is that only our finite hypothesis class approaches achieve any non-vacuous guarantees across this whole range of training set sizes. Every result for the standard SGD and BBPT approaches is vaccuous and cannot be guaranteed. 

For the 16-shot case, these experiments are quantified in more detail in Table~\ref{tab:vision_bound_stats}. Similarly to the results for LLM adaptation, we can see the dramatic difference in \% of non-vacuous episodes, and dramatic improvement in the min, median and max bound obtained over episodes. Compared to the LLM case, our \ourShort{}  pays a slighly higher price here in terms of empirical test accuracy compared to SGD for some benchmarks, however this is still small compared to the stark difference in certification performance. 

Figure~\ref{fig:clip_shots_vs_bound} highlights the evolution of the median generalisation bound as a function of the training set size. For \ourShort{} it becomes non-vacuous from 4-shots onward, and the standard approaches never become non-vacuous\footnote{Note their bound is substantially worse than 0.8, but for simple visualisation, we plot it as chance-level for 5-way classification.}

%Meanwhile, bound statistics for the 16-shot scenario and a visualization over all shots can be found in Table \ref{tab:vision_bound_stats} and Figure \ref{fig:teaser_clip} respectively. We defer comparison of generalization performance across shots and methods to Table \ref{tab:vision_accuracies} in Appendix \ref{app:extra_vision}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Vision Domain II: Meta-Dataset??}\label{sec:expmts_metadataset}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\textbf{Further Analysis: } We finally discuss and provide some insight into the learning process of our discrete hypothesis class learner. Standard gradient-descent takes repeated update steps to find a model that better fits a training set. By analogy, our \ourShort{} gradient-free learner would draw more  samples as it attempts to iteratively sample a model that better fits the training set. Our main experiments use a fixed number of 20,000 samples on all vision datasets, but Figure~\ref{fig:diffusion_samples_analysis} illustrates our learner's behaviour by showing the equivalent of a learning curve for our model. The x-axis is the number of samples drawn, and equivalently the learning theoretic hypothesis space size. Unlike SGD, this means that there is a direct dependence of hypothesis class complexity ($|\Theta|$ in Eq.~\ref{eq:bound}) and the number of iterations/samples. This is reflected in the steadily increasing red complexity curve in Figure~\ref{fig:diffusion_samples_analysis}(left, middle). We can also see that the training/support error goes down consistently over iterations/samples as the sampler progressively discovers better models. The generalisation bound (black line) is given by the sum of the training error and complexity. The figure illustrates one case (Flowers, middle, right) where the bound continues to improve up to a large number of samples/hypothesis size, because the continued improvement in training error outweighs the complexity gain. It also illustrates a case (DTD, left) where the training error improvement is slower and quite rapidly outweighed by the complexity gain, so that the best bound is actually achieved after quite a small number of samples.

\textbf{Sampler vs Zoo:} Our approach compresses the upstream set of pre-trained models into a learned model generator. Selecting among the upstream models using downstream task performance as a criterion provides an alternative approach to learning that also corresponds to a finite hypothesis space. Our generator approach was motivated by ensuring scalability with respect to a large number of upstream models, and also to improve accuracy by enabling interpolation between upstream models rather than solely being limited to selecting one of them. Figure~\ref{fig:teaser_clip} shows that \ourShort{}'s diffusion sampler tends to provide improved accuracy compared to its raw model zoo, especially for Flowers and DTD. On average across all datasets STEEL consistently outperforms Model Zoo. Detailed per-dataset per-shot performance is deferred to Appendix \ref{app:extra_vision}, Table \ref{tab:vision_accuracies}.

\section{Conclusion}\label{sec:conclusion}
We have introduced a novel Sample-Then-Evaluate approach to transfer learning. Our \ourShort{} is designed to facilitate non-trivial performance guarantees, even in the low-shot regime, through use of a discrete hypothesis space. Our results instantiating the bound demonstrate that for both LLM and visual transfer learning \ourShort{}  performs comparably to alternatives it terms of test accuracy, while being dramatically better in terms of ability to provably guarantee this performance level.

\section*{Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, and we mention a non-exhaustive concerns here. First, for LLM experiments, please note that the diffusion model is trained on adapters trained on user-specific data. We can use the diffusion model to interpolate between adapters to potentially avoid distributing the private model zoo among clients, but this does have potential for data leakage by memorization \cite{staab2024beyond}. 
%and the fact that LoRA-XS adapters are too tiny to memorize user-specific data, the base model finetuning can potentially introduce data leakage by memorization \cite{staab2024beyond}. 
It is worth investigating privacy preserving methods for such concerns \cite{miranda2024preservingprivacylargelanguage}.

On the vision side of our experiments, despite our benchmark being on fine-grained classification tasks of publicly available datasets, we necessitate the reminder of ethical concerns in computer vision as our method is easily applicable across domains and applications \cite{Waelen2023}.

\clearpage
\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Existing Risk Bounds for Deep Models}\label{appsec:other_bounds}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Vanilla PAC-Bayes Bound}\label{appsec:vanilla_pb}

This is the vanilla, non-transfer learning bound.
As a baseline, one can also contrast with the vanilla PAC-Bayes bound (i.e., non-meta learning bound). This essentially follows the Cantoni's bound, and can be written as follows. With probability at least $1-\epsilon$, the following holds:
\begin{align}
\mathbb{E}_{Q(\theta)}[R(\theta)] \ \leq \ \mathbb{E}_{Q(\theta)}[r(\theta)] + \sqrt{\frac{\textrm{KL}(Q||\pi)+\log(1/\epsilon)}{2n}}
\label{eq:vanilla_pac_bayes}
\end{align}
Here $n$ is the test support size. 
We can set $\pi = \mathcal{N}(0, \kappa^2 I)$ for some fixed $\kappa_\pi$ and $Q(\theta) = \mathcal{N}(\mu, \Sigma^2)$ where the parameters $(\mu,\Sigma)$ can be learned by minimizing the right hand side. The sampled version $\theta^z = \mu + \sqrt{\Sigma}\cdot z$, $z\sim\mathcal{N}(0,I)$ can be used during the optimization. Once optimized, the minimum value of the right hand side serves as the error bound for the test task.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bound with Parameter-Level Quantization}\label{appsec:quantized_bound}

In~\citep{lotfi2024nonvacuousLLM}, they proposed a non-vacuous bound for the LLM based on the model parameter quantization (e.g., fixed-length floating point machine representation). There are several differences to our approach:
\begin{enumerate}
\item The paper is about LLM pre-training setup with large training data, and the bound would be vacuous if training data size is not large enough (e.g., $\geq$ 10K). 
\item They derive the same finite hypothesis space PAC-Bayes bound, but replace the $\log |H|$ term by $\log(1/p(h))$ where $p(h)$ is the prior likelihood, and $\log(1/p(h))$ is approximated and upper-bounded by $C(h)$ which is the number of bits for representing the hypothesis $h$.
\item The finite hypothesis space comes from the fixed-size floating point representation for real numbers (e.g., if there are $d$ trainable parameters, then $C(h) = d\cdot 32$), but to reduce it further, they propose what is called the SubLoRA, which is a random subspace representation (i.e., $\theta = P w$, $P =$ random subspace basis, $w$ = coefficients) of the LoRA $A$/$B$ matrices.
\item Also, instead of 32 bit for each of $d$ params, they do some clustering to reduce it to shorter coding, more precisely the arithmetic coding.
\end{enumerate}

The followings are some details of their bound derivation.
With probability at least $1-\epsilon$,
%%%%
\begin{align}
R(\theta) \leq r(\theta) + C \cdot \sqrt{\frac{K(\theta) + 2\log K(\theta) + \log(1/\epsilon)}{2n}} 
\label{eq:qbound}
\end{align}
%%%%
where $K(\theta)$ is the Kolmogorov complexity bound that can be estimated as:
%%%%
\begin{align}
K(\theta) = \sum_{i=1}^d (\textrm{$\#$ of bits in the arithmetic coding of $\theta_i$})
\end{align}
%%%%
where $d=\dim(\theta)$ for the PEFT parameters $\theta$. 
The arithmetic coding requires clustering of parameters $\theta_i$s, thus being dependent on the particular $\theta$ used. 
However, we can consider the best (i.e., the tightest) bound possible. That is, even if we have 1 bit for every $\theta_i$ (the minimal code length possible), $K(\theta)=d$, and plugging this into (\ref{eq:qbound}) yields:
%%%%
\begin{align}
R(\theta) \leq r(\theta) + C \cdot \sqrt{\frac{d + 2\log d + \log(1/\epsilon)}{2n}} 
\label{eq:qbound_best}
\end{align}
%%%%
which is the {\em best} scenario.

In (\ref{eq:qbound}), as before, $R(\theta) = \mathbb{E}_{z\sim T^*}[l(\theta; z)]$ is the generalization error of $\theta$, 
$r(\theta) = \frac{1}{n} \sum_{z\in S^*} l(\theta; z)$ is the empirical error on the support data with size $n = |S^*|$, and $C$ is the maximal loss value (i.e., $0\leq l \leq C)$. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Architecture and Training Recipes}\label{app:hyperparams}
\subsection{Diffusion Architecture and Training Recipe}
First, the diffusion model forwad encoder uses a 1000 timesteps with a linear scheduler over noise between 1e-4 to 2e-2. For the decoder, we use an MLP network for the diffusion model with 3 hidden-layers. The hidden layer dimension is $4\times$ the size of it's input. This is 10,240 for LaMP (divisible by 512 for parallelization concerns) and 4096 for vision. A layer conditioned time embedding of the diffusion step is added (summed with) the hidden layer's hidden representation.

The time embedding is generated from the diffusion timestep using sinsusoidal embeddings as per the original DDPM model \cite{ddpm}. The dimensionality of the sinusoidal embedding is equal to the Diffusion MLP hidden dimension. Subsequently, the embedding is transformed using a two-layer MLP with first layer expanding the dimension to $4\times$ the network's hidden dimension and the second layer downscaling again to original hidden dimension. For example, on the vision experiments, the time embedding network has hidden dimension of $4098\times4$. Finally, to condition the time embedding computed by the two-layer MLP time embedding network for each layer, we apply a different linear transformation per diffusion hidden layer.

We train the diffusion model for 30K epochs for all experiments with a batch size of 1,024. We use the LAMB optimizer \cite{you2020largebatchoptimizationdeep} with a learning rate of 0.01. For vision experiments, we found that we can continue improving performance if we continue training for a second stage of 10K more epochs. For the second stage, we use a one-cycle learning rate scheduler \cite{smith2018superconvergencefasttrainingneural} with default hyperparameters (as found in pytorch). The maximum learning rate starts from 0.0004 reaching a maximum of 0.001 over a 1000 steps. We keep an exponential-moving average of the network weights throughout training with a decaying rate of 0.9999.

\subsection{Flan-T5 + LaMP Hyperparameters}\label{app:lamp_hyperparams}
For training the base model across all datasets, we use LaMP's original recipe \cite{salemi2024lamplargelanguagemodels}. We use a batch size of 64, AdamW optimizer with a learning rate of 5e-5 and weight decay of size 0.0001. We use a linear warmup for the learning rate over 5\% of the total number of training steps.

To build the model zoo, we found that we required to tune Adam optimizer learning rate and per-dataset epochs per-dataset. We optimizer the hyperparameters to improve performance on the training split (seen users) query data. We use learning rates of 0.01, 0.01, 0.0001 and 20, 10, 10 epochs for LaMP-2, LaMP-3 and LaMP-5 respectively. For all datasets, we used a linear warmup for the learning rate over 5\% of the total number of training steps. We used the same recipe to train a per-user LoRA-XS model on the unseen users/novel tasks.

For LoRA-Hub, we used default hyperparameters as proposed by the original authors. First, we sample 20 random adapters from the model zoo. The weights of the linear combination is initialized with zeros and truncate min/max weights to -1.5/1.5. We do maximum inference steps of 40 with NeverGrad default hyperparameters.

Finally, we transform the SGD number of epochs to MeZO. The authors used 32$\times$ as many epochs as SGD in the original paper \cite{malladi2023finetuning}. This translates to 640, 320, 320 epochs for LaMP-2, LaMP-3, and LaMP-5. We tested the three learning rates proposed to search over by the authors. We fixed a learning rate of 1e-3 across all datasets because we consistently found 1e-4 to not learn and 1e-2 to be unstable.

For LaMP, we sample 10K LoRA-XS modules from the diffusion model. We use k-means clustering on the diffusion samples to produce N clusters where N is chosen as the minimum Silhouette score. We evaluate N between [2,150] inclusive. For each cluster, we find the medoid; the adapter closest to the centroid of the cluster.  During evaluation, we choose a cluster and evaluate all adapters therein. From the cluster, we short-list the best 15 adapters using the Flan-T5 training loss. On the best 15 adapters, we use text generation to produce an answer with greedy sampling. Using the LaMP benchmark proposed model selection metric for each dataset, we select the ``winning" adapter.

\subsubsection{Bound Metrics}
For support errors, we use 1-Accuracy for LaMP-2, and ROUGE-1 for LaMP-5. For LaMP-3, both RMSE and MAE are not bounded. Therefore, we devise a cross-entropy like metric for the dataset. First, we convert the ordinal vectors to one-hot encodings. Subsequently, we calculate the absolute error between the labels one-hot encoding and Flan-T5 model logits, and divide by 2. This guarantees the error to be bounded in the [0-1] range inclusive. We use this metric as support error term.

\textbf{LaMP-2 Intricacies:} LaMP benchmark has one query sample per user. For LaMP-3 and LaMP-5, this suffices since the generated support error is continuous. Nevertheless, for LaMP-2, the accuracy term, which we use to derive the support error, becomes the 0/1 loss. Therefore, we split the support data in novel tasks to support and query data with ratio 80\% and 20\% respectively. If the split generates only 1 query samples, we move one sample from support to query to have a minimum of two-samples in query. We use the same split for all evaluations across SGD, MeZO, LoRA-Hub and our proposed methods. For reproducibility, all splits were done deterministically. Furthermore, we did not constrain the split to have same classes across both support and query. LaMP classification tasks are long-tailed. Therefore, for a novel task, a user might have classes X and Y in support but the query ends up with classes A and B making it a more challenging benchmark for all methods.

Finally, we truncate the support sizes of LaMP-3 and LaMP-5 to 256 samples across all methods. This is done deterministically for reproducibility. The reason for truncating the dataset is pure computational concerns.

\subsubsection{Model Zoo Size}
For LaMP, we build a model zoo by training one PEFT adapter per-task in the dataset. Each user is treated as one task. This yields 3820, 20,000, and 9,682 total adapters/tasks in LaMP-2, LaMP-3, and LaMP-5 respectively.

\subsection{CLIP + CoOP Hyperparameters}\label{app:vision_hyperparams}
To build the model zoo, we used the authors original hyperparameters to train CoOP because we found them to work the best. This is SGD with a learning rate of 0.002. For Flowers-101, we train for 200 epochs. For DTD, FGVCAircraft and CUBirds, we trained for 300 epochs and found a One Cycle learning rate useful to stabilize training. These same hyperparameters were used to evaluate SGD on novel tasks. For BBPT, we use the authors default hyperparameters \cite{bbpt}. We found that the method converges within 8000 ``API call". We attempted to run for a budget of 20K as our diffusion model offers but found that performance did not improve. Please note that the original authors reduce the dimensionality of the prompt using a small network because evolutionary optimization struggles in high-dimension. They reduce dimensionality to 512. Nevertheless, since we train only 2-tokens (dimensionality=1,024), then we do not use the small dimensionality reduction network.

For vision experiments, we found that exhaustive search was fast enough even though we sample 20K adapters from the diffusion model.

\textbf{Bound Metrics:} we use $1-\text{Accuracy}$ as the support error term in our bound calculation for all vision experiments.

\textbf{Model Zoo Size:} We randomly sample 16-shot 5-way tasks for building the model zoo from each respective dataset. We train 10,000 total tasks per-dataset for the model zoo. The diffusion model is trained on this model zoo. Once trained, we sample the diffusion model once and fix the samples across all downstream evaluation for 1, 2, 4, 8 and 16 shots.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Extra Vision Results}\label{app:extra_vision}
\begin{table*}[h]
    \centering
    \begin{tabular}{lc>{\columncolor{Gray}}cccc}
        \toprule
        \textbf{Dataset} & \textbf{Zero Shot} & \textbf{SGD} & \textbf{BBPT} & \textbf{Model Zoo} & \textbf{Diffusion} \\
        \midrule
        \multicolumn{6}{c}{1-Shots}\\
        \midrule
        CUBirds & 83.82\% & 81.77\% & 85.80\% & 86.00\% & 86.72\% \\
        DescribableTextures & 67.29\% & 69.97\% & 74.10\% & 72.12\% & 73.97\% \\
        FGVCAircraft &  47.44\% & 53.00\% & 55.05\% & 54.37\% & 55.40\% \\
        Flowers-101 & 81.14\% & 83.55\% & 80.40\% & 74.25\% & 76.75\% \\
        \midrule
        Avg & 69.92\% & 72.07\% & 73.84\% & 71.69\% & 73.21\% \\

        \midrule
        \multicolumn{6}{c}{2-Shots}\\
        \midrule
        CUBirds & 83.82 & 85.82\% & 86.92\% & 86.77\% & 86.50\% \\
        DescribableTextures & 67.29 & 75.95\% & 76.57\% & 75.22\% & 76.17\% \\
        FGVCAircrafts & 47.44 & 52.30\% & 57.90\% & 55.82\% & 57.30\% \\
        Flowers-101 & 81.14 & 86.92\% & 85.05\% & 77.17\% & 79.50\% \\
        \midrule
        Avg & 69.92 & 75.25\% & 76.61\% & 73.75\% & 74.87\% \\
        
        \midrule
        \multicolumn{6}{c}{4-Shots}\\
        \midrule
        CUBirds & 83.82 & 88.30\% & 88.42\% & 87.50\% & 87.47\% \\
        DescribableTextures & 67.29 & 81.02\% & 77.90\% & 77.40\% & 79.82\% \\
        FGVCAircrafts & 47.44 & 58.65\% & 60.55\% & 57.55\% & 60.10\% \\
        Flowers-101 & 81.14 & 91.95\% & 87.07\% & 79.82\% & 81.42\% \\
        \midrule
        Avg & 69.92\% & 79.98\% & 78.49\% & 75.57\% & 77.21\% \\

        \midrule
        \multicolumn{6}{c}{8-Shots}\\
        \midrule
        CUBirds & 83.82 & 89.75\% & 88.40\% & 87.42\% & 87.32\% \\
        DescribableTextures & 67.29 & 85.07\% & 81.47\% & 79.90\% & 81.77\% \\
        FGVCAircrafts & 47.44 & 62.07\% & 61.55\% & 58.77\% & 61.72\% \\
        Flowers-101 & 81.14 & 94.30\% & 88.67\% & 80.32\% & 82.52\% \\
        \midrule
        Avg & 69.92\% & 82.80\% & 80.02\% & 76.61\% & 78.34\% \\

        \midrule
        \multicolumn{6}{c}{16-Shots}\\
        \midrule
        CUBirds & 83.82 & 90.32\% & 89.27\% & 87.97\% & 88.40\% \\
        DescribableTextures & 67.29 & 87.95\% & 83.20\% & 79.25\% & 81.50\% \\
        FGVCAircrafts & 47.44 & 65.57\% & 62.37\% & 61.02\% & 61.37\% \\
        Flowers-101 & 81.14 & 95.90\% & 90.15\% & 82.92\% & 84.90\% \\
        \midrule
        Avg & 69.92\% & 84.94\% & 81.25\% & 77.79\% & 79.04\% \\
        
    \bottomrule
    \end{tabular}
    \caption{CLIP+CoOp few-shot learning. Accuracies over different number of shots.}
    \label{tab:vision_accuracies}
\end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{lc>{\columncolor{Gray}}cccc}
%         \toprule
%         \textbf{Dataset} & \textbf{Zero Shot} & \textbf{SGD} & \textbf{BBPT} & \textbf{Model Zoo} & \textbf{Diffusion} \\
%         \midrule
%         \multicolumn{6}{c}{1-Shots}\\
%         \midrule
%         CUBirds & 83.82\% & 81.77\% & 85.80\% & 86.00\% & 86.72\% \\
%         DescribableTextures & 67.29\% & 69.97\% & 74.10\% & 72.12\% & 73.97\% \\
%         FGVCAircraft &  47.44\% & 53.00\% & 55.05\% & 54.37\% & 55.40\% \\
%         Flowers-101 & 81.14\% & 83.55\% & 80.40\% & 74.25\% & 76.75\% \\
%         \midrule
%         Avg & 69.92\% & 72.07\% & 73.84\% & 71.69\% & 73.21\% \\

%         \midrule
%         \multicolumn{6}{c}{2-Shots}\\
%         \midrule
%         CUBirds & 83.82 & 85.82\% & 86.92\% & 86.77\% & 86.50\% \\
%         DescribableTextures & 67.29 & 75.95\% & 76.57\% & 75.22\% & 76.17\% \\
%         FGVCAircrafts & 47.44 & 52.30\% & 57.90\% & 55.82\% & 57.30\% \\
%         Flowers-101 & 81.14 & 86.92\% & 85.05\% & 77.17\% & 79.50\% \\
%         \midrule
%         Avg & 69.92 & 75.25\% & 76.61\% & 73.75\% & 74.87\% \\
        
%         \midrule
%         \multicolumn{6}{c}{4-Shots}\\
%         \midrule
%         CUBirds & 83.82 & 88.30\% & 88.42\% & 87.50\% & 87.47\% \\
%         DescribableTextures & 67.29 & 81.02\% & 77.90\% & 77.40\% & 79.82\% \\
%         FGVCAircrafts & 47.44 & 58.65\% & 60.55\% & 57.55\% & 60.10\% \\
%         Flowers-101 & 81.14 & 91.95\% & 87.07\% & 79.82\% & 81.42\% \\
%         \midrule
%         Avg & 69.92\% & 79.98\% & 78.49\% & 75.57\% & 77.21\% \\

%         \midrule
%         \multicolumn{6}{c}{8-Shots}\\
%         \midrule
%         CUBirds & 83.82 & 89.75\% & 88.40\% & 87.42\% & 87.32\% \\
%         DescribableTextures & 67.29 & 85.07\% & 81.47\% & 79.90\% & 81.77\% \\
%         FGVCAircrafts & 47.44 & 62.07\% & 61.55\% & 58.77\% & 61.72\% \\
%         Flowers-101 & 81.14 & 94.30\% & 88.67\% & 80.32\% & 82.52\% \\
%         \midrule
%         Avg & 69.92\% & 82.80\% & 80.02\% & 76.61\% & 78.34\% \\

%         \midrule
%         \multicolumn{6}{c}{16-Shots}\\
%         \midrule
%         CUBirds & 83.82 & 90.32\% & 89.27\% & 87.97\% & 88.40\% \\
%         DescribableTextures & 67.29 & 87.95\% & 83.20\% & 79.25\% & 81.50\% \\
%         FGVCAircrafts & 47.44 & 65.57\% & 62.37\% & 61.02\% & 61.37\% \\
%         Flowers-101 & 81.14 & 95.90\% & 90.15\% & 82.92\% & 84.90\% \\
%         \midrule
%         Avg & 69.92\% & 84.94\% & 81.25\% & 77.79\% & 79.04\% \\
        
%     \bottomrule
%     \end{tabular}
%     \caption{CLIP+CoOp few-shot learning. Accuracies over different number of shots.}
%     \label{tab:vision_accuracies}
% \end{table*}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
