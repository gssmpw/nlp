% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{chen2021fatnn,
  title={FATNN: Fast and accurate ternary neural networks},
  author={Chen, Peng and Zhuang, Bohan and Shen, Chunhua},
  booktitle={ICCV},
  pages={5219--5228},
  year={2021}
}

@article{zhu2016trained,
  title={Trained ternary quantization},
  author={Zhu, Chenzhuo and Han, Song and Mao, Huizi and Dally, William J},
  journal={Preprint},
    volume={arXiv:1612.01064},
    url={https://arxiv.org/1612.01064},
  year={2016}
}

@article{li2016ternary,
  title={Ternary weight networks},
  author={Li, Fengfu and Liu, Bin and Wang, Xiaoxing and Zhang, Bo and Yan, Junchi},
  journal={Preprint},
    volume={arXiv:1605.04711},
url={https://arxiv.org/abs/1605.04711},
  year={2016}
}

% === DOWNSTREAM DATASETS  ===
@article{clark2018think,
  title={Think you have solved question answering? {T}ry {ARC}, the {AI}2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={Preprint},
    volume={arXiv:1803.05457},
    url={https://arxiv.org/abs/1803.05457},
  year={2018}
}
@inproceedings{de2019commitmentbank,
  title={The commitmentbank: Investigating projection in naturally occurring discourse},
  author={De Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
  booktitle={proceedings of Sinn und Bedeutung},
  volume={23},
  issue={2},
  pages={107--124},
  year={2019}
}

@inproceedings{roemmele2011choice,
  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={2011 AAAI spring symposium series},
  year={2011}
}

@article{zellers2019hellaswag,
  title={Hella{S}wag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={Preprint},
volume={arXiv:1905.07830},
    url={https://arxiv.org/abs/1905.07830},
  year={2019}
}

@inproceedings{dolan2005automatically,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, Bill and Brockett, Chris},
  booktitle={Third international workshop on paraphrasing (IWP2005)},
  year={2005}
}

@inproceedings{OpenBookQA2018,
 title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
 booktitle={EMNLP},
 year={2018}
}


@inproceedings{Bisk2020,
  author = {Yonatan Bisk and Rowan Zellers and
            Ronan Le Bras and Jianfeng Gao
            and Yejin Choi},
  title = {{PIQA}: Reasoning about Physical Commonsense in
           Natural Language},
  booktitle = {AAAI},
  year = {2020},
}

@inproceedings{dagan2005pascal,
  title={The pascal recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine learning challenges workshop},
  pages={177--190},
  year={2005},
  organization={Springer}
}

@article{SciQ,
    title={Crowdsourcing Multiple Choice Science Questions},
    author={Johannes Welbl, Nelson F. Liu, Matt Gardner},
    year={2017},
    journal={Preprint},
    volume={arXiv:1707.06209v1}
}

@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "EMNLP",
    no_month = oct,
    year = "2013",
    no_address = "Seattle, Washington, USA",
    publisher = "ACL",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}

@article{ai2:winogrande,
author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
title = {WinoGrande: {A}n adversarial winograd schema challenge at scale},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/3474381},
doi = {10.1145/3474381},
abstract = {Commonsense reasoning remains a major challenge in AI, and yet, recent progresses on benchmarks may seem to suggest otherwise. In particular, the recent neural language models have reported above 90\% accuracy on the Winograd Schema Challenge (WSC), a commonsense benchmark originally designed to be unsolvable for statistical models that rely simply on word associations. This raises an important question---whether these models have truly acquired robust commonsense capabilities or they rely on spurious biases in the dataset that lead to an overestimation of the true capabilities of machine commonsense.To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) large-scale crowdsourcing, followed by (2) systematic bias reduction using a novel AFLITE algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. Our experiments demonstrate that state-of-the-art models achieve considerably lower accuracy (59.4\%-79.1\%) on WINOGRANDE compared to humans (94\%), confirming that the high performance on the original WSC was inflated by spurious biases in the dataset.Furthermore, we report new state-of-the-art results on five related benchmarks with emphasis on their dual implications. On the one hand, they demonstrate the effectiveness of WINOGRANDE when used as a resource for transfer learning. On the other hand, the high performance on all these benchmarks suggests the extent to which spurious biases are prevalent in all such datasets, which motivates further research on algorithmic bias reduction.},
journal = {Commun. ACM},
month = aug,
pages = {99–106},
numpages = {8}
}
% ===


@article{guo2025deepseek,
  title={Deepseek-{R1}: Incentivizing reasoning capability in {LLMs} via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={Preprint},
  volume={arXiv:2501.12948},
  year={2025},
    url={https://arxiv.org/abs/2501.12948}
}

@article{jaech2024openai,
  title={Open{AI} o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={Preprint},
volume={arXiv:2412.16720},
  year={2024},
url={https://arxiv.org/abs/2412.16720}
}

@misc{bitsenoughbottomup,
      title={When are 1.58 bits enough? A Bottom-up Exploration of BitNet Quantization}, 
      author={Jacob Nielsen and Lukas Galke and Peter Schneider-Kamp},
      year={2024},
      eprint={2411.05882},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.05882}, 
}

@misc{huggingfaceFinetuningLLMs,
	author = {Mohamed Mekkouri and Marc Sun and Leandro von Werra and Predo Cuenca and Omar Sanseviero and Thomas Wolf},
	title = {{F}ine-tuning {L}{L}{M}s to 1.58bit: extreme quantization made easy --- huggingface.co},
	howpublished = {\url{https://huggingface.co/blog/1_58_llm_extreme_quantization}},
	year = {2024},
	note = {[Accessed 04-02-2025]},
}

@InProceedings{pmlr-v202-liu23ao,
  title = 	 {Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models},
  author =       {Liu, Hong and Xie, Sang Michael and Li, Zhiyuan and Ma, Tengyu},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {22188--22214},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/liu23ao/liu23ao.pdf},
  url = 	 {https://proceedings.mlr.press/v202/liu23ao.html},
  abstract = 	 {Language modeling on large-scale datasets improves performance of various downstream tasks. The validation pre-training loss is often used as the evaluation metric for language models since the pre-training loss tends to be well-correlated with downstream performance (which is itself hard to evaluate comprehensively). Contrary to the conventional wisdom, this paper shows that 1) pre-training loss cannot fully explain downstream performance and 2) flatness of the model is well-correlated with downstream performance where pre-training loss is not. We identify three ways to produce models with the same pre-training loss but different downstream performance: continue pre-training after convergence, increasing the model size, and changing the pre-training algorithms. These experiments demonstrate the existence of implicit bias of pre-training algorithms—among models with the same minimal pre-training loss, they implicitly prefer more transferable ones. Toward understanding this implicit bias, we prove that SGD with standard mini-batch noise implicitly prefers flatter minima of pre-training loss in language models, and empirically observe a strong correlation between flatness (measured by the trace of Hessian) and downstream performance among models with the same pre-training loss. We also prove in a synthetic language setting that among models with the minimal pre-training loss, the flattest model transfers to downstream tasks.}
}

@misc{xu2023qaloraquantizationawarelowrankadaptation,
      title={{QA-LoRA}: Quantization-Aware Low-Rank Adaptation of Large Language Models}, 
      author={Yuhui Xu and Lingxi Xie and Xiaotao Gu and Xin Chen and Heng Chang and Hengheng Zhang and Zhengsu Chen and Xiaopeng Zhang and Qi Tian},
      year={2023},
      eprint={2309.14717},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.14717}, 
}

@article{OLMo,
  title={OLMo: Accelerating the Science of Language Models},
  author={Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and A. Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Daniel Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hanna Hajishirzi},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:267365485},
  journal={Preprint},
 volume={arXiv:2402.00838},
}

@article{dolma,
  title = {{Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}},
  author={Luca Soldaini and Rodney Kinney and Akshita Bhagia and Dustin Schwenk and David Atkinson and Russell Authur and Ben Bogin and Khyathi Chandu and Jennifer Dumas and Yanai Elazar and Valentin Hofmann and Ananya Harsh Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and Nathan Lambert and Ian Magnusson and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and Emma Strubell and Nishant Subramani and Oyvind Tafjord and Pete Walsh and Luke Zettlemoyer and Noah A. Smith and Hannaneh Hajishirzi and Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo},
  year={2024},
  journal={arXiv preprint arXiv:2402.00159},
  url={https://arxiv.org/abs/2402.00159}
}


@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{geiping2023cramming,
  title={Cramming: Training a Language Model on a single GPU in one day.},
  author={Geiping, Jonas and Goldstein, Tom},
  booktitle={ICML},
  pages={11117--11143},
  year={2023},
  organization={PMLR}
}

@inproceedings{nawrot-2023-nanot5,
	title        = {nano{T}5: Fast {\&} Simple Pre-training and Fine-tuning of {T}5 Models with Limited Resources},
	author       = {Nawrot, Piotr},
	year         = 2023,
	month        = dec,
	booktitle    = {Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)},
	publisher    = {Association for Computational Linguistics},
	doi          = {10.18653/v1/2023.nlposs-1.11},
	url          = {https://aclanthology.org/2023.nlposs-1.11}
}

@article{bal2024exploring,
  title={Exploring Extreme Quantization in Spiking Language Models},
  author={Bal, Malyaban and Jiang, Yi and Sengupta, Abhronil},
  journal={Preprint},
  volume={arXiv:2405.02543},
    url={https://arxiv.org/abs/2405.02543},
  year={2024}
}

@article{sundaram2024llavaolmobitnet1b,
  title={{LLaVaOLMoBitnet1B}: {T}ernary {LLM} goes Multimodal!},
  author={Sundaram, Jainaveen and Iyer, Ravishankar},
  journal={Preprint},
volume={arXiv:2408.13402},
url={https://arxiv.org/abs/2408.13402},
  year={2024}
}

@inproceedings{nielsen2024bitnetb158reloadedstateoftheart,
  title={BitNet B1. 58 Reloaded: State-of-the-Art Performance Also on Smaller Networks},
  author={Nielsen, Jacob and Schneider-Kamp, Peter},
  booktitle={International Conference on Deep Learning Theory and Applications},
  pages={301--315},
  year={2024},
  organization={Springer}
}

@misc{ashkboos2024slicegpt,
      title={SliceGPT: Compress Large Language Models by Deleting Rows and Columns}, 
      author={Saleh Ashkboos and Maximilian L. Croci and Marcelo Gennari do Nascimento and Torsten Hoefler and James Hensman},
      year={2024},
      eprint={2401.15024},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{frantar2023gptq,
      title={{GPTQ}: Accurate Post-Training Quantization for Generative Pre-trained Transformers}, 
      author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
      year={2023},
      eprint={2210.17323},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{li2023vit,
  title={{I-ViT}: {I}nteger-only quantization for efficient vision transformer inference},
  author={Li, Zhikai and Gu, Qingyi},
  booktitle={ICCV},
  pages={17065--17075},
  year={2023}
}

@misc{lin2024awq,
      title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration}, 
      author={Ji Lin and Jiaming Tang and Haotian Tang and Shang Yang and Wei-Ming Chen and Wei-Chen Wang and Guangxuan Xiao and Xingyu Dang and Chuang Gan and Song Han},
      year={2024},
      eprint={2306.00978},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{he2024matterstransformersattentionneeded,
      title={What Matters in Transformers? Not All Attention is Needed}, 
      author={Shwai He and Guoheng Sun and Zheyu Shen and Ang Li},
      year={2024},
      eprint={2406.15786},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.15786}, 
}

@article{greenwade93,
    author  = "George D. Greenwade",
    title   = "The {C}omprehensive {T}ex {A}rchive {N}etwork ({CTAN})",
    year    = "1993",
    journal = "TUGBoat",
    volume  = "14",
    number  = "3",
    pages   = "342--351"
}

@article{wang2023bitnet,
      title={{BitNet}: Scaling 1-bit Transformers for Large Language Models}, 
      author={Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei},
      year={2023},
      journal={Preprint},
        volume={arXiV:2310.11453},
    url={https://arxiv.org/abs/2310.11453}
}


@misc{liu2023llmqat,
      title={{LLM-QAT}: Data-Free Quantization Aware Training for Large Language Models}, 
      author={Zechun Liu and Barlas Oguz and Changsheng Zhao and Ernie Chang and Pierre Stock and Yashar Mehdad and Yangyang Shi and Raghuraman Krishnamoorthi and Vikas Chandra},
      year={2023},
      eprint={2305.17888},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{158bit_tips_code,
title ={The Era of 1-bit LLMs: Training Tips, Code and FAQ},
howpublished="\url{https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf}"
}

@misc{ma2024era,
      title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits}, 
      author={Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei},
      year={2024},
      eprint={2402.17764},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{kingma2014adam,
  title={{Adam: A method for stochastic optimization}},
  author={Kingma, Diederik P and Ba, Jimmy},
 booktitle={ICLR},
  year={2015}
}

%adamW
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

% mnist
@article{lecun1998mnist,
  title={The MNIST database of handwritten digits},
  journal={http://yann. lecun. com/exdb/mnist/}
}

% CIFAR10 + CIFAR100
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

% short gpt
@article{men2024shortgpt,
  title={Shortgpt: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

% WideMLP for text classification
@inproceedings{galke-scherp-2022-bag,
    title = "Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide {MLP}",
    author = "Galke, Lukas  and
      Scherp, Ansgar",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.279",
    doi = "10.18653/v1/2022.acl-long.279",
    pages = "4038--4051",
    abstract = "Graph neural networks have triggered a resurgence of graph-based text classification methods, defining today{'}s state of the art. We show that a wide multi-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent graph-based models TextGCN and HeteGCN in an inductive text classification setting and is comparable with HyperGAT. Moreover, we fine-tune a sequence-based BERT and a lightweight DistilBERT model, which both outperform all state-of-the-art models. These results question the importance of synthetic graphs used in modern text classifiers. In terms of efficiency, DistilBERT is still twice as large as our BoW-based wide MLP, while graph-based models like TextGCN require setting up an $\mathcal{O}(N^2)$ graph, where $N$ is the vocabulary plus corpus size. Finally, since Transformers need to compute $\mathcal{O}(L^2)$ attention weights with sequence length $L$, the MLP models show higher training and inference speeds on datasets with long sequences.",
}

% Simplified Graph Convolution
@InProceedings{sgc,
  title = 	 {Simplifying Graph Convolutional Networks},
  author =       {Wu, Felix and Souza, Amauri and Zhang, Tianyi and Fifty, Christopher and Yu, Tao and Weinberger, Kilian},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6861--6871},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/wu19e/wu19e.pdf},
  url = 	 {https://proceedings.mlr.press/v97/wu19e.html},
  abstract = 	 {Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.}
}

% Kipf-GCN
@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}


@inproceedings{chen2024sudden,
title={Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in {MLM}s},
author={Angelica Chen and Ravid Shwartz-Ziv and Kyunghyun Cho and Matthew L Leavitt and Naomi Saphra},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=MO5PiKHELW}
}


@article{kudo2018sentencepiece,
  title={Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author={Kudo, T},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}

%wordpiece
@misc{wu2016googlesneuralmachinetranslation,
      title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}, 
      author={Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
      year={2016},
      eprint={1609.08144},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1609.08144}, 
}


@InProceedings{pmlr-v48-yanga16,
  title = 	 {Revisiting Semi-Supervised Learning with Graph Embeddings},
  author = 	 {Yang, Zhilin and Cohen, William and Salakhudinov, Ruslan},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {40--48},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/yanga16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/yanga16.html},
  abstract = 	 {We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.}
}


@article{bommasani2021,
  author       = {Rishi Bommasani and
                  Drew A. Hudson and
                  Ehsan Adeli and
                  Russ B. Altman and
                  Simran Arora and
                  Sydney von Arx and
                  Michael S. Bernstein and
                  Jeannette Bohg and
                  Antoine Bosselut and
                  Emma Brunskill and
                  Erik Brynjolfsson and
                  Shyamal Buch and
                  Dallas Card and
                  Rodrigo Castellon and
                  Niladri S. Chatterji and
                  Annie S. Chen and
                  Kathleen Creel and
                  Jared Quincy Davis and
                  Dorottya Demszky and
                  Chris Donahue and
                  Moussa Doumbouya and
                  Esin Durmus and
                  Stefano Ermon and
                  John Etchemendy and
                  Kawin Ethayarajh and
                  Li Fei{-}Fei and
                  Chelsea Finn and
                  Trevor Gale and
                  Lauren E. Gillespie and
                  Karan Goel and
                  Noah D. Goodman and
                  Shelby Grossman and
                  Neel Guha and
                  Tatsunori Hashimoto and
                  Peter Henderson and
                  John Hewitt and
                  Daniel E. Ho and
                  Jenny Hong and
                  Kyle Hsu and
                  Jing Huang and
                  Thomas Icard and
                  Saahil Jain and
                  Dan Jurafsky and
                  Pratyusha Kalluri and
                  Siddharth Karamcheti and
                  Geoff Keeling and
                  Fereshte Khani and
                  Omar Khattab and
                  Pang Wei Koh and
                  Mark S. Krass and
                  Ranjay Krishna and
                  Rohith Kuditipudi and
                  et al.},
  title        = {On the Opportunities and Risks of Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2108.07258},
  year         = {2021},
  url          = {https://arxiv.org/abs/2108.07258},
  eprinttype    = {arXiv},
  eprint       = {2108.07258},
  timestamp    = {Fri, 08 Nov 2024 20:52:57 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2108-07258.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  no_editor       = {Isabelle Guyon and
                  Ulrike von Luxburg and
                  Samy Bengio and
                  Hanna M. Wallach and
                  Rob Fergus and
                  S. V. N. Vishwanathan and
                  Roman Garnett},
  title        = {Attention is All you Need},
  booktitle    = {Advances in Neural Information Processing Systems 30},
  pages        = {5998--6008},
  year         = {2017},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/BrownMRSKDNSSAA20,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  no_editor       = {Hugo Larochelle and
                  Marc'Aurelio Ranzato and
                  Raia Hadsell and
                  Maria{-}Florina Balcan and
                  Hsuan{-}Tien Lin},
  title        = {Language Models are Few-Shot Learners},
  booktitle    = {Advances in Neural Information Processing Systems 33},
  year         = {2020},
  url          = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/tmlr/WeiTBRZBYBZMCHVLDF22,
  author       = {Jason Wei and
                  Yi Tay and
                  Rishi Bommasani and
                  Colin Raffel and
                  Barret Zoph and
                  Sebastian Borgeaud and
                  Dani Yogatama and
                  Maarten Bosma and
                  Denny Zhou and
                  Donald Metzler and
                  Ed H. Chi and
                  Tatsunori Hashimoto and
                  Oriol Vinyals and
                  Percy Liang and
                  Jeff Dean and
                  William Fedus},
  title        = {Emergent Abilities of Large Language Models},
  journal      = {Trans. Mach. Learn. Res.},
  volume       = {2022},
  year         = {2022},
  url          = {https://openreview.net/forum?id=yzkSU5zdwD},
  timestamp    = {Fri, 19 May 2023 11:20:41 +0200},
  biburl       = {https://dblp.org/rec/journals/tmlr/WeiTBRZBYBZMCHVLDF22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/cacm/SchwartzDSE20,
  author       = {Roy Schwartz and
                  Jesse Dodge and
                  Noah A. Smith and
                  Oren Etzioni},
  title        = {Green {AI}},
  journal      = {Commun. {ACM}},
  volume       = {63},
  number       = {12},
  pages        = {54--63},
  year         = {2020},
  url          = {https://doi.org/10.1145/3381831},
  doi          = {10.1145/3381831},
  timestamp    = {Fri, 25 Dec 2020 01:10:05 +0100},
  biburl       = {https://dblp.org/rec/journals/cacm/SchwartzDSE20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{DBLP:conf/naacl/DevlinCLT19,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  no_editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {4171--4186},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1423},
  doi          = {10.18653/V1/N19-1423},
  timestamp    = {Mon, 26 Sep 2022 12:21:55 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/LiuXL023,
  author       = {Hong Liu and
                  Sang Michael Xie and
                  Zhiyuan Li and
                  Tengyu Ma},
  title        = {Same Pre-training Loss, Better Downstream: Implicit Bias Matters for
                  Language Models},
  booktitle    = {{ICML}},
  no_series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {22188--22214},
  publisher    = {{PMLR}},
  year         = {2023}
}

@inproceedings{
kumar2025scaling,
title={Scaling Laws for Precision},
author={Tanishq Kumar and Zachary Ankner and Benjamin Frederick Spector and Blake Bordelon and Niklas Muennighoff and Mansheej Paul and Cengiz Pehlevan and Christopher Re and Aditi Raghunathan},
booktitle={ICLR},
year={2025},
url={https://openreview.net/forum?id=wg1PCg3CUP}
}


@article{muennighoff2025s1simpletesttimescaling,
      title={s1: Simple test-time scaling}, 
      author={Niklas Muennighoff and Zitong Yang and Weijia Shi and Xiang Lisa Li and Li Fei-Fei and Hannaneh Hajishirzi and Luke Zettlemoyer and Percy Liang and Emmanuel Candès and Tatsunori Hashimoto},
      year={2025},
      journal={Preprint},
      volume={arXiv:2501.19393},
      url={https://arxiv.org/abs/2501.19393}
}

@article{schwartz2020green,
  title={Green {AI}},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={Commun. ACM},
  volume={63},
  number={12},
  pages={54--63},
  year={2020},
  publisher={ACM New York, NY, USA}
}
@inproceedings{DBLP:conf/aaai/StrubellGM20,
  author       = {Emma Strubell and
                  Ananya Ganesh and
                  Andrew McCallum},
  title        = {Energy and Policy Considerations for Modern Deep Learning Research},
  booktitle    = {{AAAI}},
  pages        = {13693--13696},
  publisher    = {{AAAI} Press},
  year         = {2020}
}


@InProceedings{pmlr-v202-xiao23c,
  title = 	 {{S}mooth{Q}uant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author =       {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle = 	 {ICML},
  pages = 	 {38087--38099},
  year = 	 {2023},
  no_editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  no_series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf},
  url = 	 {https://proceedings.mlr.press/v202/xiao23c.html},
  abstract = 	 {Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56$\times$ speedup and 2$\times$ memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs.}
}


@article{bengio2025international,
  title={International {AI} Safety Report},
  author={Bengio, Yoshua and Mindermann, S{\"o}ren and Privitera, Daniel and Besiroglu, Tamay and Bommasani, Rishi and Casper, Stephen and Choi, Yejin and Fox, Philip and Garfinkel, Ben and Goldfarb, Danielle and others},
  journal={Preprint},
    volume={arXiv:2501.17805},
url={https://arxiv.org/2501.17805},
  year={2025}
}

@article{straightthrough,
  author       = {Yoshua Bengio and
                  Nicholas L{\'{e}}onard and
                  Aaron C. Courville},
  title        = {Estimating or Propagating Gradients Through Stochastic Neurons for
                  Conditional Computation},
  journal      = {Preprint},
    volume={arXiv:1308.3432},
url={https://arxiv.org/abs/1308.3432},
  year         = {2013}
}