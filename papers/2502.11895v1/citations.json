[
  {
    "index": 0,
    "papers": [
      {
        "key": "chen2021fatnn",
        "author": "Chen, Peng and Zhuang, Bohan and Shen, Chunhua",
        "title": "FATNN: Fast and accurate ternary neural networks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "li2016ternary",
        "author": "Li, Fengfu and Liu, Bin and Wang, Xiaoxing and Zhang, Bo and Yan, Junchi",
        "title": "Ternary weight networks"
      },
      {
        "key": "zhu2016trained",
        "author": "Zhu, Chenzhuo and Han, Song and Mao, Huizi and Dally, William J",
        "title": "Trained ternary quantization"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "frantar2023gptq",
        "author": "Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh",
        "title": "{GPTQ}: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "lin2024awq",
        "author": "Ji Lin and Jiaming Tang and Haotian Tang and Shang Yang and Wei-Ming Chen and Wei-Chen Wang and Guangxuan Xiao and Xingyu Dang and Chuang Gan and Song Han",
        "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2023vit",
        "author": "Li, Zhikai and Gu, Qingyi",
        "title": "{I-ViT}: {I}nteger-only quantization for efficient vision transformer inference"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "kumar2025scaling",
        "author": "Tanishq Kumar and Zachary Ankner and Benjamin Frederick Spector and Blake Bordelon and Niklas Muennighoff and Mansheej Paul and Cengiz Pehlevan and Christopher Re and Aditi Raghunathan",
        "title": "Scaling Laws for Precision"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "liu2023llmqat",
        "author": "Zechun Liu and Barlas Oguz and Changsheng Zhao and Ernie Chang and Pierre Stock and Yashar Mehdad and Yangyang Shi and Raghuraman Krishnamoorthi and Vikas Chandra",
        "title": "{LLM-QAT}: Data-Free Quantization Aware Training for Large Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "xu2023qaloraquantizationawarelowrankadaptation",
        "author": "Yuhui Xu and Lingxi Xie and Xiaotao Gu and Xin Chen and Heng Chang and Hengheng Zhang and Zhengsu Chen and Xiaopeng Zhang and Qi Tian",
        "title": "{QA-LoRA}: Quantization-Aware Low-Rank Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wang2023bitnet",
        "author": "Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei",
        "title": "{BitNet}: Scaling 1-bit Transformers for Large Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ma2024era",
        "author": "Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei",
        "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "sundaram2024llavaolmobitnet1b",
        "author": "Sundaram, Jainaveen and Iyer, Ravishankar",
        "title": "{LLaVaOLMoBitnet1B}: {T}ernary {LLM} goes Multimodal!"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "bal2024exploring",
        "author": "Bal, Malyaban and Jiang, Yi and Sengupta, Abhronil",
        "title": "Exploring Extreme Quantization in Spiking Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "nielsen2024bitnetb158reloadedstateoftheart",
        "author": "Nielsen, Jacob and Schneider-Kamp, Peter",
        "title": "BitNet B1. 58 Reloaded: State-of-the-Art Performance Also on Smaller Networks"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "bitsenoughbottomup",
        "author": "Jacob Nielsen and Lukas Galke and Peter Schneider-Kamp",
        "title": "When are 1.58 bits enough? A Bottom-up Exploration of BitNet Quantization"
      }
    ]
  }
]