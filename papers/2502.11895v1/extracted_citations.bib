@article{bal2024exploring,
  title={Exploring Extreme Quantization in Spiking Language Models},
  author={Bal, Malyaban and Jiang, Yi and Sengupta, Abhronil},
  journal={Preprint},
  volume={arXiv:2405.02543},
    url={https://arxiv.org/abs/2405.02543},
  year={2024}
}

@misc{bitsenoughbottomup,
      title={When are 1.58 bits enough? A Bottom-up Exploration of BitNet Quantization}, 
      author={Jacob Nielsen and Lukas Galke and Peter Schneider-Kamp},
      year={2024},
      eprint={2411.05882},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.05882}, 
}

@inproceedings{chen2021fatnn,
  title={FATNN: Fast and accurate ternary neural networks},
  author={Chen, Peng and Zhuang, Bohan and Shen, Chunhua},
  booktitle={ICCV},
  pages={5219--5228},
  year={2021}
}

@misc{frantar2023gptq,
      title={{GPTQ}: Accurate Post-Training Quantization for Generative Pre-trained Transformers}, 
      author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
      year={2023},
      eprint={2210.17323},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{li2016ternary,
  title={Ternary weight networks},
  author={Li, Fengfu and Liu, Bin and Wang, Xiaoxing and Zhang, Bo and Yan, Junchi},
  journal={Preprint},
    volume={arXiv:1605.04711},
url={https://arxiv.org/abs/1605.04711},
  year={2016}
}

@inproceedings{li2023vit,
  title={{I-ViT}: {I}nteger-only quantization for efficient vision transformer inference},
  author={Li, Zhikai and Gu, Qingyi},
  booktitle={ICCV},
  pages={17065--17075},
  year={2023}
}

@misc{lin2024awq,
      title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration}, 
      author={Ji Lin and Jiaming Tang and Haotian Tang and Shang Yang and Wei-Ming Chen and Wei-Chen Wang and Guangxuan Xiao and Xingyu Dang and Chuang Gan and Song Han},
      year={2024},
      eprint={2306.00978},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2023llmqat,
      title={{LLM-QAT}: Data-Free Quantization Aware Training for Large Language Models}, 
      author={Zechun Liu and Barlas Oguz and Changsheng Zhao and Ernie Chang and Pierre Stock and Yashar Mehdad and Yangyang Shi and Raghuraman Krishnamoorthi and Vikas Chandra},
      year={2023},
      eprint={2305.17888},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ma2024era,
      title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits}, 
      author={Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei},
      year={2024},
      eprint={2402.17764},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{nielsen2024bitnetb158reloadedstateoftheart,
  title={BitNet B1. 58 Reloaded: State-of-the-Art Performance Also on Smaller Networks},
  author={Nielsen, Jacob and Schneider-Kamp, Peter},
  booktitle={International Conference on Deep Learning Theory and Applications},
  pages={301--315},
  year={2024},
  organization={Springer}
}

@article{sundaram2024llavaolmobitnet1b,
  title={{LLaVaOLMoBitnet1B}: {T}ernary {LLM} goes Multimodal!},
  author={Sundaram, Jainaveen and Iyer, Ravishankar},
  journal={Preprint},
volume={arXiv:2408.13402},
url={https://arxiv.org/abs/2408.13402},
  year={2024}
}

@article{wang2023bitnet,
      title={{BitNet}: Scaling 1-bit Transformers for Large Language Models}, 
      author={Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei},
      year={2023},
      journal={Preprint},
        volume={arXiV:2310.11453},
    url={https://arxiv.org/abs/2310.11453}
}

@misc{xu2023qaloraquantizationawarelowrankadaptation,
      title={{QA-LoRA}: Quantization-Aware Low-Rank Adaptation of Large Language Models}, 
      author={Yuhui Xu and Lingxi Xie and Xiaotao Gu and Xin Chen and Heng Chang and Hengheng Zhang and Zhengsu Chen and Xiaopeng Zhang and Qi Tian},
      year={2023},
      eprint={2309.14717},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.14717}, 
}

@article{zhu2016trained,
  title={Trained ternary quantization},
  author={Zhu, Chenzhuo and Han, Song and Mao, Huizi and Dally, William J},
  journal={Preprint},
    volume={arXiv:1612.01064},
    url={https://arxiv.org/1612.01064},
  year={2016}
}

