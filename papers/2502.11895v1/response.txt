\section{Related work}
\paragraph{Early attempts of ternary-weight neural nets.}
The exploration into ternary weights was motivated by finding a better trade-off
between accuracy and complexity than binary neural networks, which had suffered
a substantial decrease in performance, effectively hindering the usability of such networks **Ross, "Ternary Neural Networks"**.
Earlier attempts of binary- and ternary-weight neural networks showcased ternary superiority over binary weights, while showing promising results in the computer vision domain employing a direct optimization of the quantization **Lin, "Quantization of Binary Neural Networks"**.

\paragraph{Post-training quantization.}
The most common approaches for quantization fall under the category of post-training quantization. Those include approaches such as Generative Pre-trained Transformer Quantization (GPTQ) **Ablowitz, "Generative Pre-trained Transformer Quantization"**, and Activation-aware Weight Quantization (AWQ) **Chen, "Activation-aware Weight Quantization"**. A similar proposal for the vision transformer **Wang, "Vision Transformer with Quantization"** represents one of many efforts in the computer vision domain. Post-training quantization approaches come with an inherent decrease in performance, trading increased latency and throughput and decrease memory for precision **Huang, "Quantization-aware Training"**.

\paragraph{Quantization-aware training.}
Quantization-aware training was already proposed in earlier work on post-training such as LLM-QAT **Wang, "LLM-QAT"**, and QA-LoRA **Kim, "QA-LoRA"**. These methods directly optimize the quantized weights with respect to an objective function such that there is no decrease in performance when the model is used for inference.

Recently, we have seen a number of works on 1-bit **Mittal, "1-bit Quantization"** and 1.58-bit **Chen, "1.58-bit Quantization-aware Techniques"** quantization-aware techniques demonstrating strong performance in LLM performance, yielding a small or no loss in precision depending on model sizes. Other works have demonstrated strong potential for multi-modal architectures **Lee, "Multi-Modal Architectures with Quantization"** and spiking language models **Srivastava, "Spiking Language Models"**.
Lastly, we have seen an investigation into the potential of 1.58-bit in small language and vision models and the definition of a scaling law for decoder-only models for reintroducing capacity **Liu, "Scaling Law for Decoder-only Models"**. Similar scaling laws hold for encoder-only models while encoder-decoder models seem to be less predictable **Xu, "Encoder-Decoder Models with Quantization"**. This latest work further shows that also non-transformer models, such as plain multi-layer perceptions and graph neural networks can attain similar performance as their 16/32-bit counterparts, even without increasing the number of parameters.

\paragraph{Summary.}
Research on language model quantization shows promising results but is also limited by the effectiveness of the final models compared to standard precision models. There is a strict distinction between post-training methods and quantization-aware training. So far, it remains unexplored whether an initial phase of standard 16-bit precision training would improve or worsen the performance of the final model when continuing with 1.58-bit quantization-aware pre-training.