\section{Related work \label{sec2}
}
\subsection{Dynamic Convolution \label{sec2.1}}
DyConv ____ is an effective method to enhance the representation capability of models with negligible additional computational complexity. It employs input-dependent dynamic kernels instead of static counterparts, making the kernels nonlinear functions of the input features. The dynamic kernels are generated by aggregating a set of parallel candidate kernels with identical kernel size $K_\text{h} \times K_\text{w}$ and input/output channels  $C_{\text{in/out}}$. The kernel aggregation is guided by the attention weights and can be expressed as
\begin{equation}
    W = \sum_{k=1}^{K} A_k W_k,
    \label{eq.1}
\end{equation}
where $W$ represents the resulting dynamic kernel, $K$ is the number of candidate kernels, $W_k, k=1,2,\cdots,K$ denotes the $k$-th candidate kernel consisting of $C_{\text{out}}$ filters $W_k^m \in \mathbb{R}^{K_\text{h} \times K_\text{w} \times C_{\text{in}}}, m=1,2,\cdots,C_{\text{out}}$, and $A_k \in \mathbb{R}$ is the attention weight assigned to $W_k$. The attention weights are computed using a squeeze-and-excitation mechanism. Specifically, global spatial information is first compressed through global average pooling (GAP), followed by a fully connected (FC) layer, a rectified linear unit (ReLU) activation, another FC layer, and a softmax function. Due to the small size of the convolution kernels and the lightweight nature of the attention mechanism, DyConv is computationally efficient at the cost of an increased number of parameters. Dynamic convolution similarly handles bias, but for simplicity, this detail is not included in the formulas.

The softmax function ensures that the attention weights sum to one, as expressed by $\sum_{k=1}^{K} A_k = 1$. This normalization helps constrain the kernel space, facilitating the learning of the attention model. During training, candidate kernels with low attention weights are often difficult to optimize. To address this, DyConv incorporates temperature annealing in the softmax function, encouraging a near-uniform attention distribution in the early training stages. Specifically, the features are scaled by a temperature coefficient, which varies during training, before applying the softmax function. A higher temperature produces a flatter distribution of attention weights, facilitating the learning of all kernels. As training progresses, the temperature is gradually reduced, allowing the model to assign more distinct attention weights and improving its ability to select the most relevant kernels.

\subsection{Omni-dimensional Dynamic Convolution \label{sec2.2}}
ODConv ____ introduces a multi-dimensional attention mechanism to enhance the performance of DyConv. It employs a parallel strategy to learn complementary attention weights for convolutional kernels across four dimensions of the kernel space: spatial, channel (input channel), filter (output channel), and candidate kernel dimensions. Specifically, in addition to kernel attention, ODConv assigns distinct attention weights, denoted as $A_k^\text{s} \in \mathbb{R}^{K_\text{h} \times K_\text{w}}$, $A_k^\text{c} \in \mathbb{R}^{C_{\text{in}}}$, and $A_k^\text{f} \in \mathbb{R}^{C_{\text{out}}}$, to different spatial locations, input channels, and output channels of the $k$-th candidate kernel. It can be represented as
\begin{equation}
    W = \sum_{k=1}^{K} A_k \left( A_k^\text{s} * A_k^\text{c} * A_k^\text{f} * W_k \right),
    \label{eq.2}
\end{equation}
where $*$ denotes element-wise multiplication across different dimensions. The attention weights for each dimension are computed using a multi-head mechanism. The additional spatial, channel, and filter attention weights are all generated using the sigmoid function. For simple implementation, these weights can be shared across all candidate kernels, allowing the subscript $k$ of the corresponding attention descriptor in Eq.~\ref{eq.2} to be omitted. By leveraging this refined kernel adjustment mechanism, ODConv achieves comparable or superior performance to standard DyConv while requiring fewer convolutional kernels and significantly fewer extra parameters.