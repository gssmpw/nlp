\section{Related Work}
% CHI

% \rev{The Human-Computer Interaction (CHI) community has long been invested in promoting greater global solidarity and in increasing the diversity of researchers and research subjects**Zickuhr, "Promoting Global Solidarity through Human-Computer Interaction"** ____. 
% Early calls for the CHI community to focus on war and peace have revolved around education, using new technologies to connect opposing factions, and exposing the horrors of war**Liu et al., "War and Peace in the Digital Age"**. 
% In the light of these educational efforts, various tools have been proposed to tackle the impact of low-quality content on the Web, including browser extensions that provide additional information about the viewed content **Sharma et al., "Browser Extensions for Quality Content"**, LLM-based explainers about potential propaganda techniques **Fang and Zhang, "LLM-Based Propaganda Detection"** , and visual indicators highlighting elements associated with misinformation **Gupta et al., "Visual Indicators of Misinformation"**. 
% However, these interventions many not be necessary if the platform itself provides moderation, especially when the misinformation has already been identified by reputable fact-checkers.
% Few studies have reported a thorough overview of the known misinformation circulating on Twitter, its character, and especially the extent to which the platform itself has moderated it. 
% This study aims to fill this literature gap by examining the interaction of Twitter users with the known misinformation during the first year of the Russian invasion of Ukraine starting late February 2022.}

% Datasets

Social media has exacerbated the ``fog of war'' surrounding modern-day conflicts.
Since Russia's annexation of Crimea in 2014, and especially since its invasion of Ukraine's territory in 2022, social media has been a parallel battleground for the public discourse around the conflict **Katz et al., "The Role of Social Media in Modern Conflicts"**. 
Immediately after the invasion, communications and media researchers began compiling datasets capturing the media coverage and social media posts around the conflict.
**Pierri et al. created a dataset of relevant news articles from media outlets in Ukraine, Russia, Europe, Asia, and the US around nine events.**
%The articles are compared in terms of the semantic similarity of their text and sentiment scores, with the aim of aiding future research on journalistic disagreements and possible misinformation within the mainstream media coverage of the war. 
Similarly, there exist datasets from Twitter **Lai et al., "Twitter Dataset for Misinformation"**, TikTok **Alieva et al., "TikTok Dataset for Misinformation"**, Reddit **Toraman et al., "Reddit Dataset for Misinformation"**, Telegram **Ferdush et al., "Telegram Dataset for Misinformation"** and Weibo **Darwish et al., "Weibo Dataset for Misinformation"**.
The first year after the invasion is also the last year of Twitter API's broad availability,\footnote{\url{https://9to5mac.com/2023/04/06/twitter-shuts-down-free-api/}} allowing for potentially some of the last large-scale analyses of public discourse on the platform around the conflict. 


% Twitter

Within this discourse, propaganda and misinformation pose a serious danger to the health of the public sphere. 
To track these, researchers employed a variety of fully and partially automated approaches. 
Researchers in the misinformation space have used manually annotated tweets about the war in the design and evaluation of ML classifiers **Chen et al., "Misinformation Detection with Manual Annotation"**.
**La Gatta et al. used a strategy of retrieving tweets based on \emph{claims}, some of which may match debunked misinformation, to compile a manually-cleaned list of \num{83} claims matching \num{2359} tweets concerning the Russo-Ukrainian conflict.**
However, as the authors point out, tracking claims instead of particular posts introduces a difficulty, as some claims may become true or false over time (for instance, whether NATO members supplied Ukraine with military weapons). 
Instead, **Aguerri et al. create a Twitter dataset about the war in English, Japanese, Spanish, French, German, and Korean languages** %between February 24 and March 12, 2022 
and group tweets into clusters %using text embeddings and UMAP **Gupta et al., "Twitter Dataset for Misinformation"** 
and manually assign misinformation labels to clusters which contain a tweet that can be linked to a fact-checked article that debunks it. 
%They found that misinformation had different topical foci in different countries, and the misinformation topics were over-represented in some linguistic groups in proportion to their user base (such as in Spanish).
A study on misinformation on Facebook and Twitter was performed using keyword-based search and annotation based on source credibility of news website mentioned in tweets **Ferdush et al., "Misinformation Detection with Source Credibility"**. 
Such an approach can be considered ``distant supervision'', as the veracity of the tweet itself is not annotated.
Network properties of the datasets have also been utilized: **Gupta et al. used the retweet network for tagging users who share misinformation URLs in their tweets, and performed explanatory analysis of tweets**. 
Even before the invasion, some of the misinformation has been attributed to the Russian state actors such as Russia's Internet Research Agency **Kosseim et al., "Russian State Actors and Misinformation"** and Russian Ministry of Defense **Katz et al., "Russian Propaganda and Misinformation"**, but a variety of topics spanning global and local politics and many kinds of misinformation exist. 
Comparing their activity to that of non-bot (human) accounts using Granger causality test, the authors find evidence of causality in both directions. 
A popular tool for such research, Botometer has unfortunately been unable to update its scores since the closure of the public Twitter API, however historical data is available up to May 2023.\footnote{\url{https://botometer.osome.iu.edu/faq}}
We use this functionality in our study.

% Moderation 

Twitter content moderation is an important enforcement mechanism of the platform's terms of service. 
Flagging individual tweets, suppressing their spread, and deleting content have all been possible actions the platform could take to limit the spread of unwanted materials **Gupta et al., "Content Moderation on Twitter"**. 
However, suspension of entire accounts has been the most visible and controversial action the platform can take, often referred to as ``deplatforming'' **Katz et al., "Deplatforming on Twitter"**.
Studies of such suspended accounts have shown this approach to be effective in decreasing the overall activity and the toxicity levels of their supporters **Gupta et al., "Effectiveness of Deplatforming"**.
On the other hand, fine-grained tools have been proposed for personalized moderation wherein the user is able to control what is shown in their feed **Katz et al., "Personalized Moderation on Twitter"**, however these were envisioned for moderating toxic speech, instead of its factual accuracy.
Recently, **Aguerri et al. have examined the accounts suspended in a Twitter dataset around the Russo-Ukrainian conflict in the first two months of the conflict.**
They found that Twitter tends to be more proactive in suspending accounts that were created more recently, as well as those that use toxic language and have a higher level of activity. 
However, it is up to the platform whether it chooses to exercise this option.
**Katz et al. track Russian propaganda and low-credibility content on Facebook and Twitter and find that only about 8-15\% of the posts and tweets sharing links to Russian propaganda or untrustworthy sources were removed.**
In this study, we focus on misinformation specifically (as opposed to any toxic or spam content), to measure the extent of moderation Twitter exercised.



% % CHI-specific literature
% cited **Zickuhr, "Digital Literacy and Human-Computer Interaction"** - propose a browser extension that helps users assess the accuracy of content on the web by showing the user assessments from trusted sources.

% cited **Fang et al., "AI-Misinfo Detection"** - interviewed information professionals who promote digital literacy, who say that there is "a temporal mismatch, whereby digital literacy requires time-consuming processes that cannot be accelerated, but institutional and societal pressures demand speed."

% cited **Chen et al., "Human-Misinfo Detection"** - "We observed four expression patterns differentiating AI-misinfo from Human-misinfo in that AI-misinfo tended to enhance details, communicate uncertainties and limitations, draw conclusions, and simulate personal tones. Existing detection models, while still able to reasonably classify AI-misinfo, showed a significant drop in performance compared with Human-misinfo."

% ____ - Studied tweets at the beginning of the Ukraine war that provide security and privacy advice. Find some misinformation and conflicting information in these tweets: ``potentially life-threatening consequences of following misinformation in times of war''. 

% ____ - Exploring the Use of Personalized AI for Identifying Misinformation on Social Media

% ____ - Keyboard Fighters: The Use of ICTs by Activists in Times of Military Coup in Myanmar

% cited ____ - Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool

% ____ -  Dungeons \& Deepfakes: Using scenario-based role-play to study journalists' behavior towards using AI-based verification tools for video content

% cited ____ - From Adolescents' Eyes: Assessing an Indicator-Based Intervention to Combat Misinformation on TikTok

% ____ - Cognitive Biases in Fact-Checking and Their Countermeasures: A Review (not chi)