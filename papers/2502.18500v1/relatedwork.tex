\section{Related Work}
% CHI

% \rev{The Human-Computer Interaction (CHI) community has long been invested in promoting greater global solidarity and in increasing the diversity of researchers and research subjects~\cite{cannanure2023chi}. 
% Early calls for the CHI community to focus on war and peace have revolved around education, using new technologies to connect opposing factions, and exposing the horrors of war~\cite{hourcade2011chi}. 
% In the light of these educational efforts, various tools have been proposed to tackle the impact of low-quality content on the Web, including browser extensions that provide additional information about the viewed content \cite{jahanbakhsh2024browser}, LLM-based explainers about potential propaganda techniques \cite{zavolokina2024think}, and visual indicators highlighting elements associated with misinformation \cite{hartwig2024adolescents}. 
% However, these interventions many not be necessary if the platform itself provides moderation, especially when the misinformation has already been identified by reputable fact-checkers.
% Few studies have reported a thorough overview of the known misinformation circulating on Twitter, its character, and especially the extent to which the platform itself has moderated it. 
% This study aims to fill this literature gap by examining the interaction of Twitter users with the known misinformation during the first year of the Russian invasion of Ukraine starting late February 2022.}

% Datasets

Social media has exacerbated the ``fog of war'' surrounding modern-day conflicts.
Since Russia's annexation of Crimea in 2014, and especially since its invasion of Ukraine's territory in 2022, social media has been a parallel battleground for the public discourse around the conflict \cite{freeman2023seeing}. 
Immediately after the invasion, communications and media researchers began compiling datasets capturing the media coverage and social media posts around the conflict.
\citet{khairova2023first} created a dataset of relevant news articles from media outlets in Ukraine, Russia, Europe, Asia, and the US around nine events. 
%The articles are compared in terms of the semantic similarity of their text and sentiment scores, with the aim of aiding future research on journalistic disagreements and possible misinformation within the mainstream media coverage of the war. 
Similarly, there exist datasets from Twitter \cite{chen2023tweets,pohl2023invasion}, TikTok \cite{primig2023remixing}, Reddit \cite{albota2022war}, Telegram \cite{ronzhyn2023collective} and Weibo \cite{hanley2023special}.
The first year after the invasion is also the last year of Twitter API's broad availability,\footnote{\url{https://9to5mac.com/2023/04/06/twitter-shuts-down-free-api/}} allowing for potentially some of the last large-scale analyses of public discourse on the platform around the conflict. 


% Twitter

Within this discourse, propaganda and misinformation pose a serious danger to the health of the public sphere. 
To track these, researchers employed a variety of fully and partially automated approaches. 
Researchers in the misinformation space have used manually annotated tweets about the war in the design and evaluation of ML classifiers \citet{darwish2023identifying,toraman2024mide22}.
\citet{la2023retrieving} used a strategy of retrieving tweets based on \emph{claims}, some of which may match debunked misinformation, to compile a manually-cleaned list of \num{83} claims matching \num{2359} tweets concerning the Russo-Ukrainian conflict.
However, as the authors point out, tracking claims instead of particular posts introduces a difficulty, as some claims may become true or false over time (for instance, whether NATO members supplied Ukraine with military weapons). 
Instead, \citet{lai2024multilingual} create a Twitter dataset about the war in English, Japanese, Spanish, French, German, and Korean languages %between February 24 and March 12, 2022 
and group tweets into clusters %using text embeddings and UMAP \cite{mcinnes2018umap} 
and manually assign misinformation labels to clusters which contain a tweet that can be linked to a fact-checked article that debunks it. 
%They found that misinformation had different topical foci in different countries, and the misinformation topics were over-represented in some linguistic groups in proportion to their user base (such as in Spanish).
A study on misinformation on Facebook and Twitter was performed using keyword-based search and annotation based on source credibility of news website mentioned in tweets \cite{pierri2023propaganda}. 
Such an approach can be considered ``distant supervision'', as the veracity of the tweet itself is not annotated.
Network properties of the datasets have also been utilized: \citet{lai2024multilingual} used the retweet network for tagging users who share misinformation URLs in their tweets, and performed explanatory analysis of tweets. 
Even before the invasion, some of the misinformation has been attributed to the Russian state actors such as Russia's Internet Research Agency \cite{linvill2020troll,boyte2017analysis} and Russian Ministry of Defense \cite{alieva2022investigating}, but a variety of topics spanning global and local politics and many kinds of sources of inaccuracy are possible \cite{lai2024multilingual}. 
During the conflict, it was found that several Twitter accounts related to Russian-based media were spreading misinformation: \citet{aguerri2024fight} filtered 90 Twitter accounts of media outlets and individual journalists and analyzed possible misinformation therein. 

%\citet{toraman2024mide22} use the information from fact-checked articles to formulate a search query for gathering tweets relevant to several events, including the Russo-Ukrainian conflict, which are then manually annotated for falsehood. 
%While building a misinformation classifier, the \citet{darwish2023identifying} randomly sampled 500 tweets and annotated misinformation tweets based on the input from fact-checking websites. 
%Following the same approach, another study annotated \num{6041} tweets based on the source credibility to train ``fake news'' detection models. 
%\citet{toraman2024mide22} derives keywords from titles of fact-checked articles on Russo-Ukrainian conflict and gathered tweets from Twitter API using keywords. The gathered tweets are manually annotated by two annotators as misinformation tweets. 

A summary of Twitter misinformation datasets concerning the war can be found in Table \ref{ex:misinfodata}. 
Overall, the key challenge with source-based annotation (distant supervision) is that not all tweets are misinformation. 
For manual annotation, none of them provided the misinformation data by individually debunking tweets with background truth. 
In this study, we track a large set of misinformation tweets identified explicitly by various fact-checkers to examine their reach and the platform's response systematically. This approach allows us to examine data annotated by experts with requisite background knowledge. 
Although it limits the coverage of all potential instances of the misinformation content, it minimizes the bias in the identification of misinformation and provides clarity in the information available to the platform about specific posts. 
%Overall, the field of misinformation tracking and analysis is large \cite{aimeur2023fake}


\begin{table*}[!htbp]
    \centering
    \small
    \caption{An Overview of Textual Datasets on misinformation on Twitter.} % (* considering Russian-Ukraine conflict)
        \renewcommand{\familydefault}{\ttdefault}
        %\normalfont
\begin{tabular}{ p{2.2cm}rlp{5cm}  }  % p{7cm}p{1cm}p{1.5cm}p{1.8cm}p{4.8cm}
 \toprule

Dataset & Data Collection Approach & Timeline & Annotation Technique\\ \midrule
%1577248641663705088 &  
Pierri et al. \cite{pierri2023propaganda} & Keyword based & 01/01/2022-24/04/2024 & Source Credibility \\ 

Lai et al. \cite{lai2024multilingual} & Keywords based & 24/02/2022-12/03/2024 & User who shares misinformation URLs \\
%users from retweet network shares misinformation URLs \\

Alieva et al. \cite{alieva2022investigating}& Keyword based & 24/02/2022-08/08/2022 & Open qualitative analysis \\

La Gatta et al. \cite{la2023retrieving} & Claim based filtering & 24/02/2022-08/03/2024 & Text (claim) similarity\\

Aguerri et al. \cite{aguerri2024fight} & Account Filtering & 05/02/2022-15/03/2022 & -- \\

Toraman et al. \cite{toraman2024mide22} & Event-based keyword search & 24/02/2022-21/03/2022 & Manual annotation\\
% https://github.com/metunlp/MiDe22 2275

Darwish et al. \cite{darwish2023identifying} & Random filter of tweets & not specified & Fact-based annotation \\

Ferdush et al. \cite{ferdush2023detecting} & Keyword Search & 02/2022-05/2023 & Source credibility based annotation \\
\bottomrule 

    \end{tabular}%
    \label{ex:misinfodata}%
\end{table*}%


% Bots

Bots, or automated accounts, are of particular interest in the dissemination of information, as they distort the perception of public support (or disapproval).
\citet{de2023twitter} used Botometer (previously, BotOrNot) \cite{davis2016botornot} to detect bots in tweets posted by politicians in six major parties in Italy in 2022 after Russia's invasion of Ukraine. 
They discovered that around 12\% of the commenters on these posts were bots, with Giorgia Meloni (who became the Prime Minister of Italy on October 22, 2022) showing the higher percentage of bots (at 15.08\%).
Focusing on pro-Russian tweets, \citet{geissler2023russian} apply Botometer to posters and retweeters of such content, and conclude that ``bots played a disproportionate role in the dissemination of pro-Russian messages'', with ``20.28\% of the spreaders'' being identified as bots, ``most of which were created at the beginning of the invasion''.
Also using Botometer, \citet{zhao2024manufacturing} study a general dataset about the war, and label 23.14\% of the accounts as bots. 
Comparing their activity to that of non-bot (human) accounts using Granger causality test, the authors find evidence of causality in both directions. 
A popular tool for such research, Botometer has unfortunately been unable to update its scores since the closure of the public Twitter API, however historical data is available up to May 2023.\footnote{\url{https://botometer.osome.iu.edu/faq}}
We use this functionality in our study.

% Moderation 

Twitter content moderation is an important enforcement mechanism of the platform's terms of service. 
Flagging individual tweets, suppressing their spread, and deleting content have all been possible actions the platform could take to limit the spread of unwanted materials \cite{zannettou2021won}. 
However, suspension of entire accounts has been the most visible and controversial action the platform can take, often referred to as ``deplatforming'' \cite{jhaver2021evaluating}.
Studies of such suspended accounts have shown this approach to be effective in decreasing the overall activity and the toxicity levels of their supporters \cite{jhaver2021evaluating}.
On the other hand, fine-grained tools have been proposed for personalized moderation wherein the user is able to control what is shown in their feed \cite{jhaver2023personalizing}, however these were envisioned for moderating toxic speech, instead of its factual accuracy.
Recently, \citet{pierri2023does} have examined the accounts suspended in a Twitter dataset around the Russo-Ukrainian conflict in the first two months of the conflict. 
They found that Twitter tends to be more proactive in suspending accounts that were created more recently, as well as those that use toxic language and have a higher level of activity. 
However, it is up to the platform whether it chooses to exercise this option.
\citet{pierri2023propaganda} track Russian propaganda and low-credibility content on Facebook and Twitter and find that only about 8-15\% of the posts and tweets sharing links to Russian propaganda or untrustworthy sources were removed.
In this study, we focus on misinformation specifically (as opposed to any toxic or spam content), to measure the extent of moderation Twitter exercised.



% % CHI-specific literature
% cited \cite{jahanbakhsh2024browser} - propose a browser extension that helps users assess the accuracy of content on the web by showing the user assessments from trusted sources.

% cited \cite{wilner2023s} - interviewed information professionals who promote digital literacy, who say that there is "a temporal mismatch, whereby digital literacy requires time-consuming processes that cannot be accelerated, but institutional and societal pressures demand speed."

% cited \cite{zhou2023synthetic} - "We observed four expression patterns differentiating AI-misinfo from Human-misinfo in that AI-misinfo tended to enhance details, communicate uncertainties and limitations, draw conclusions, and simulate personal tones. Existing detection models, while still able to reasonably classify AI-misinfo, showed a significant drop in performance compared with Human-misinfo."

% \cite{schmuser2024analyzing} - Studied tweets at the beginning of the Ukraine war that provide security and privacy advice. Find some misinformation and conflicting information in these tweets: ``potentially life-threatening consequences of following misinformation in times of war''. 

% \cite{jahanbakhsh2023exploring} - Exploring the Use of Personalized AI for Identifying Misinformation on Social Media

% \cite{guntrum2024keyboard} - Keyboard Fighters: The Use of ICTs by Activists in Times of Military Coup in Myanmar

% cited \cite{zavolokina2024think} - Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool

% \cite{sohrawardi2024dungeons} -  Dungeons \& Deepfakes: Using scenario-based role-play to study journalists' behavior towards using AI-based verification tools for video content

% cited \cite{hartwig2024adolescents} - From Adolescents' Eyes: Assessing an Indicator-Based Intervention to Combat Misinformation on TikTok

% \cite{soprano2024cognitive} - Cognitive Biases in Fact-Checking and Their Countermeasures: A Review (not chi)



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%