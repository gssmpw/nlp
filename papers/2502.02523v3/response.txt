\section{Related Work of Note}
These aren’t the only notable innovations to come out of China in recent weeks, on the 22nd of January, ByteDance (the company behind TikTok – at time of writing), released their Dai et al., "Doubao-1.5-pro"**,** which out-performs GPT 4o, and is 50x cheaper**.  It also uses MoE, and a highly optimised architecture that balances performance with reduced computational demands.  Doubao is one of the most popular AI Chatbots in China, with 60 million active users**.  The company focuses on building AI models that balance intelligence with communication, looking for more emotionally aware, natural sounding interactions. It is likely that Duobao incorporates improved prompt optimisation techniques** and a communication efficient MoE training via locality-sensitive hashing**.  The latter aimed at tackling latency challenges inherent in training sparse-gated MoE models; results in 2.2 times quicker inferences.

\paragraph{}On the 15th of January, iFlytek, launched its own deep reasoning large model, training on fully domestic computing platform; **Chen et al., "Spark Deep Reasoning X1"**.  It demonstrates characteristics similar to “slow thinking” during problem solving, whilst achieving ‘industry-leading’ results with relatively low computing power.  It is particularly strong in Chinese mathematical capabilities and has already been successfully applied in the education sector, as an intelligent teaching assistant**.

\paragraph{}On the 20th of January, **Kotti et al., "Kimi k1.5"** was released by Chinese research company Moonshot AI, reporting o1 equivalent performance on reasoning tasks (i.e. 77.5\% on AIME and 96.2\% on MATH).  This model also reports the use of RL in post-training**.  From the technical press, Kimi is multimodal, text/code and images.  It has a context length of 128k, meaning whole novels can be read in via the prompt.  Their simplified RL framework balances exploration and exploitation, and penalised the model for generating overly verbose responses.  They also encouraged shorter/faster responses by blended the weights from both long and short CoT models**.

\paragraph{}At the end of January, Qwen released a new family of models, **Wang et al., "Qwen2.5-VL"**. This multi-modal (visual and text) model has had several improvements over Qwen2, including better text recognition (including handwriting, multilingual and tables), improved object detection and spatial reasoning, improved agent functionality and better video functionality

\paragraph{}On 2nd February OpenAI announced **Bommasani et al., "Deep Research"**, claiming “It accomplishes in tens of minutes what would take a human many hours.”. After the DeepSeek models were released, it was conjectured that this might force OpenAI to rush their next release to maintain market dominance. It is too early to determine if this is the case, or the impact it has had on the model.