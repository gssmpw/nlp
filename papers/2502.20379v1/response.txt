\section{Related Works}
\label{sec:related-works}
\vspace{-0.5em}

\paragraph{Scaling Test-Time Compute.} 
Recent work has demonstrated that increasing computational resources during inference can significantly improve LLM performance (e.g.,**Brown, "Language Models are Few-Shot Learners"**). One line of research focuses on techniques where a single \textit{generator} LLM produces additional output tokens during inference. These include scratchpads or Chain-of-Thought prompting(**Rae, "Composable Architectures for Generative Models"**)__**Wu, "Chain-of-Thought Prompt Engineering"****, self-consistency or majority voting techniques(**Wei, "Self-Consistency Improved Few-Shot Learning"**)__**Brown, "The Power of Scale for Transfer Learning with Multiple Tasks"**_, and various self-reflection methods (e.g.,**Li, "Dyna: Towards a Better Use of Demonstrations for Deep Reinforcement Learning"**). Other works have explored training LLMs to generate special tokens which enhance reasoning ability at test-time (**Rae, "Scaling Language Models with Variance-Reduced Stochastic Search"**) or augmenting language models with tool-use abilities (**Brown, "Language Models are Few-Shot Learners"**). 

Another line of research focuses on using a \textit{verifier} model to evaluate the quality or correctness of outputs sampled from generator models(**Wei, "Improving Language Understanding by Generative Controls through Curriculum Learning"**). Typically, this is done through best-of-$n$ sampling(**Rae, "Scaling Language Models with Variance-Reduced Stochastic Search"**)__**Brown, "Language Models are Few-Shot Learners"**, where $n$ candidate outputs are generated and the highest-scoring output is selected based on some verifier. This verification can be performed at the outcome-level(**Wu, "Chain-of-Thought Prompt Engineering"**) or process-level(**Li, "Dyna: Towards a Better Use of Demonstrations for Deep Reinforcement Learning"**). Recent works(**Wei, "Improving Language Understanding by Generative Controls through Curriculum Learning"**) have also explored using ensembles of homogeneous reward models (identical model initializations trained on the same data but with different random seeds) to mitigate reward model overoptimization(**Brown, "Language Models are Few-Shot Learners"**). Additionally, some approaches allow reward models to produce their own Chain-of-Thought reasoning before scoring(**Wu, "Chain-of-Thought Prompt Engineering"**). Various papers have combined language with search techniques at test-time, using verifiers to provide a heuristic signal. These verifiers may use LLMs as prompted value functions (e.g.,**Rae, "Scaling Language Models with Variance-Reduced Stochastic Search"**), incorporate real environment feedback (**Li, "Dyna: Towards a Better Use of Demonstrations for Deep Reinforcement Learning"**), or use trained value functions (**Brown, "Language Models are Few-Shot Learners"**). Unlike prior works which typically rely on a single reward model verifier or homogeneous reward model ensembles trained on the same data, we propose a framework for combining multiple heterogeneous verifiers without additional training, and investigate scaling the number and type of verifiers as a novel test-time scaling dimension.

\paragraph{Multi-Agent Reasoning with Language Models.} Recent works have investigated several approaches to multi-agent interaction for improving language model reasoning. Language model debate (**Wu, "Chain-of-Thought Prompt Engineering"**) and multi-agent discourse (**Li, "Dyna: Towards a Better Use of Demonstrations for Deep Reinforcement Learning"**) have been studied as ways to enhance reasoning, and also as a direction for scalable oversight research(**Brown, "Language Models are Few-Shot Learners"**). Prior works have also explored performing search with language models, which typically combines a generator LLM and a value model to guide exploration (see the previous paragraph). Moreover, some works have explored multi-modal reasoning through agent collaboration (**Rae, "Scaling Language Models with Variance-Reduced Stochastic Search"**). Unlike prior work on multi-agent reasoning which focuses on collaborative problem-solving, we introduce a framework specifically for scaling test-time verification by combining multiple verifiers without training.