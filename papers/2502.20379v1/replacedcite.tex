\section{Related Works}
\label{sec:related-works}
\vspace{-0.5em}

\paragraph{Scaling Test-Time Compute.} 
Recent work has demonstrated that increasing computational resources during inference can significantly improve LLM performance (e.g.,____). One line of research focuses on techniques where a single \textit{generator} LLM produces additional output tokens during inference. These include scratchpads or Chain-of-Thought prompting____, self-consistency or majority voting techniques____, and various self-reflection methods (e.g.,____). Other works have explored training LLMs to generate special tokens which enhance reasoning ability at test-time (e.g.,____) or augmenting language models with tool-use abilities (e.g.,____). 

Another line of research focuses on using a \textit{verifier} model to evaluate the quality or correctness of outputs sampled from generator models____. Typically, this is done through best-of-$n$ sampling____, where $n$ candidate outputs are generated and the highest-scoring output is selected based on some verifier. This verification can be performed at the outcome-level____ or process-level____. Recent works____ have also explored using ensembles of homogeneous reward models (identical model initializations trained on the same data but with different random seeds) to mitigate reward model overoptimization____. Additionally, some approaches allow reward models to produce their own Chain-of-Thought reasoning before scoring____. Various papers have combined language with search techniques at test-time, using verifiers to provide a heuristic signal. These verifiers may use LLMs as prompted value functions (e.g.,____), incorporate real environment feedback (e.g.,____), or use trained value functions (e.g.,____). Unlike prior works which typically rely on a single reward model verifier or homogeneous reward model ensembles trained on the same data, we propose a framework for combining multiple heterogeneous verifiers without additional training, and investigate scaling the number and type of verifiers as a novel test-time scaling dimension.

\paragraph{Multi-Agent Reasoning with Language Models.} Recent works have investigated several approaches to multi-agent interaction for improving language model reasoning. Language model debate (e.g.,____) and multi-agent discourse (e.g.,____) have been studied as ways to enhance reasoning, and also as a direction for scalable oversight research____. Prior works have also explored performing search with language models, which typically combines a generator LLM and a value model to guide exploration (see the previous paragraph). Moreover, some works have explored multi-modal reasoning through agent collaboration (e.g.,____). Unlike prior work on multi-agent reasoning which focuses on collaborative problem-solving, we introduce a framework specifically for scaling test-time verification by combining multiple verifiers without training.