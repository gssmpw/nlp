[
  {
    "index": 0,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      },
      {
        "key": "snell2024scaling",
        "author": "Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral",
        "title": "Scaling {LLM} test-time compute optimally can be more effective than scaling model parameters"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "nye2021show",
        "author": "Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others",
        "title": "Show your work: Scratchpads for intermediate computation with language models"
      },
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      },
      {
        "key": "li2022competition",
        "author": "Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others",
        "title": "Competition-level code generation with alphacode"
      },
      {
        "key": "thoppilan2022lamda",
        "author": "Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others",
        "title": "Lamda: Language models for dialog applications"
      },
      {
        "key": "lewkowycz2022solving",
        "author": "Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others",
        "title": "Solving quantitative reasoning problems with language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "shinn2024reflexion",
        "author": "Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu",
        "title": "Reflexion: Language agents with verbal reinforcement learning"
      },
      {
        "key": "qu2024recursive",
        "author": "Qu, Yuxiao and Zhang, Tianjun and Garg, Naman and Kumar, Aviral",
        "title": "Recursive introspection: Teaching language model agents how to self-improve"
      },
      {
        "key": "madaan2024self",
        "author": "Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others",
        "title": "Self-refine: Iterative refinement with self-feedback"
      },
      {
        "key": "saunders2022self",
        "author": "Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan",
        "title": "Self-critiquing models for assisting human evaluators"
      },
      {
        "key": "bai2022constitutional",
        "author": "Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others",
        "title": "Constitutional ai: Harmlessness from ai feedback"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "goyal2023think",
        "author": "Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh",
        "title": "Think before you speak: Training language models with pause tokens"
      },
      {
        "key": "wang2023guiding",
        "author": "Wang, Xinyi and Caccia, Lucas and Ostapenko, Oleksiy and Yuan, Xingdi and Wang, William Yang and Sordoni, Alessandro",
        "title": "Guiding language model reasoning with planning tokens"
      },
      {
        "key": "herel2024thinking",
        "author": "Herel, David and Mikolov, Tomas",
        "title": "Thinking Tokens for Language Modeling"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "schick2023toolformer",
        "author": "Schick, Timo and Dwivedi-Yu, Jane and Dess{\\`\\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas",
        "title": "Toolformer: Language models can teach themselves to use tools"
      },
      {
        "key": "gao2023pal",
        "author": "Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham",
        "title": "Pal: Program-aided language models"
      },
      {
        "key": "qin2023toolllm",
        "author": "Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others",
        "title": "Tool{LLM}: Facilitating large language models to master 16000+ real-world apis"
      },
      {
        "key": "qu2025tool",
        "author": "Qu, Changle and Dai, Sunhao and Wei, Xiaochi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Xu, Jun and Wen, Ji-Rong",
        "title": "Tool learning with large language models: A survey"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "cobbe2021training",
        "author": "Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others",
        "title": "Training verifiers to solve math word problems"
      },
      {
        "key": "zheng2023judging",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",
        "title": "Judging {LLM}-as-a-judge with MT-Bench and Chatbot Arena"
      },
      {
        "key": "snell2024scaling",
        "author": "Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral",
        "title": "Scaling {LLM} test-time compute optimally can be more effective than scaling model parameters"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "cobbe2021training",
        "author": "Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others",
        "title": "Training verifiers to solve math word problems"
      },
      {
        "key": "nakano2021webgpt",
        "author": "Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others",
        "title": "Webgpt: Browser-assisted question-answering with human feedback"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "cobbe2021training",
        "author": "Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others",
        "title": "Training verifiers to solve math word problems"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lightman2023let",
        "author": "Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl",
        "title": "Let's verify step by step"
      },
      {
        "key": "wang2024math",
        "author": "Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang",
        "title": "Math-shepherd: Verify and reinforce {LLM}s step-by-step without human annotations"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "coste2023reward",
        "author": "Coste, Thomas and Anwar, Usman and Kirk, Robert and Krueger, David",
        "title": "Reward model ensembles help mitigate overoptimization"
      },
      {
        "key": "eisenstein2023helping",
        "author": "Eisenstein, Jacob and Nagpal, Chirag and Agarwal, Alekh and Beirami, Ahmad and D'Amour, Alex and Dvijotham, DJ and Fisch, Adam and Heller, Katherine and Pfohl, Stephen and Ramachandran, Deepak and others",
        "title": "Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "gao2023scaling",
        "author": "Gao, Leo and Schulman, John and Hilton, Jacob",
        "title": "Scaling laws for reward model overoptimization"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhang2024generative",
        "author": "Zhang, Lunjun and Hosseini, Arian and Bansal, Hritik and Kazemi, Mehran and Kumar, Aviral and Agarwal, Rishabh",
        "title": "Generative verifiers: Reward modeling as next-token prediction"
      },
      {
        "key": "mahan2024generative",
        "author": "Mahan, Dakota and Van Phung, Duy and Rafailov, Rafael and Blagden, Chase and Lile, Nathan and Castricato, Louis and Fr{\\\"a}nken, Jan-Philipp and Finn, Chelsea and Albalak, Alon",
        "title": "Generative reward models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "yu2023prompt",
        "author": "Yu, Xiao and Chen, Maximillian and Yu, Zhou",
        "title": "Prompt-Based Monte-Carlo Tree Search for Goal-oriented Dialogue Policy Planning"
      },
      {
        "key": "yao2024tree",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "title": "Tree of thoughts: Deliberate problem solving with large language models"
      },
      {
        "key": "xie2024self",
        "author": "Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, James Xu and Kan, Min-Yen and He, Junxian and Xie, Michael",
        "title": "Self-evaluation guided beam search for reasoning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhou2023language",
        "author": "Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong",
        "title": "Language agent tree search unifies reasoning acting and planning in language models"
      },
      {
        "key": "koh2024tree",
        "author": "Koh, Jing Yu and McAleer, Stephen and Fried, Daniel and Salakhutdinov, Ruslan",
        "title": "Tree search for language model agents"
      },
      {
        "key": "putta2024agent",
        "author": "Putta, Pranav and Mills, Edmund and Garg, Naman and Motwani, Sumeet and Finn, Chelsea and Garg, Divyansh and Rafailov, Rafael",
        "title": "Agent q: Advanced reasoning and learning for autonomous ai agents"
      },
      {
        "key": "long2023large",
        "author": "Long, Jieyi",
        "title": "Large language model guided tree-of-thought"
      },
      {
        "key": "besta2024graph",
        "author": "Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others",
        "title": "Graph of thoughts: Solving elaborate problems with large language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "feng2023alphazero",
        "author": "Feng, Xidong and Wan, Ziyu and Wen, Muning and McAleer, Stephen Marcus and Wen, Ying and Zhang, Weinan and Wang, Jun",
        "title": "Alphazero-like tree-search can guide large language model decoding and training"
      },
      {
        "key": "zhang2024rest",
        "author": "Zhang, Dan and Zhoubian, Sining and Hu, Ziniu and Yue, Yisong and Dong, Yuxiao and Tang, Jie",
        "title": "Rest-mcts*: {LLM} self-training via process reward guided tree search"
      },
      {
        "key": "chen2024alphamath",
        "author": "Chen, Guoxin and Liao, Minpeng and Li, Chengxi and Fan, Kai",
        "title": "AlphaMath Almost Zero: process Supervision without process"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "du2023improving",
        "author": "Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor",
        "title": "Improving factuality and reasoning in language models through multiagent debate"
      },
      {
        "key": "chan2023chateval",
        "author": "Chan, Chi-Min and Chen, Weize and Su, Yusheng and Yu, Jianxuan and Xue, Wei and Zhang, Shanghang and Fu, Jie and Liu, Zhiyuan",
        "title": "Chateval: Towards better {LLM}-based evaluators through multi-agent debate"
      },
      {
        "key": "pham2023let",
        "author": "Pham, Chau and Liu, Boyi and Yang, Yingxiang and Chen, Zhengyu and Liu, Tianyi and Yuan, Jianbo and Plummer, Bryan A and Wang, Zhaoran and Yang, Hongxia",
        "title": "Let models speak ciphers: Multiagent debate through embeddings"
      },
      {
        "key": "liang2023encouraging",
        "author": "Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Tu, Zhaopeng and Shi, Shuming",
        "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate"
      },
      {
        "key": "subramaniam2025multiagent",
        "author": "Subramaniam, Vighnesh and Du, Yilun and Tenenbaum, Joshua B and Torralba, Antonio and Li, Shuang and Mordatch, Igor",
        "title": "Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains"
      },
      {
        "key": "li2023camel",
        "author": "Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard",
        "title": "CAMEL: Communicative Agents for\" Mind\" Exploration of Large Language Model Society"
      },
      {
        "key": "cohen2023lm",
        "author": "Cohen, Roi and Hamri, May and Geva, Mor and Globerson, Amir",
        "title": "Lm vs lm: Detecting factual errors via cross examination"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "chen2023reconcile",
        "author": "Chen, Justin Chih-Yao and Saha, Swarnadeep and Bansal, Mohit",
        "title": "Reconcile: Round-table conference improves reasoning via consensus among diverse {LLM}s"
      },
      {
        "key": "wang2023unleashing",
        "author": "Wang, Zhenhailong and Mao, Shaoguang and Wu, Wenshan and Ge, Tao and Wei, Furu and Ji, Heng",
        "title": "Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration"
      },
      {
        "key": "wang2024rethinking",
        "author": "Wang, Qineng and Wang, Zihao and Su, Ying and Tong, Hanghang and Song, Yangqiu",
        "title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?"
      },
      {
        "key": "xu2023towards",
        "author": "Xu, Zhenran and Shi, Senbao and Hu, Baotian and Yu, Jindi and Li, Dongfang and Zhang, Min and Wu, Yuxiang",
        "title": "Towards reasoning in large language models via multi-agent peer review collaboration"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "irving2018ai",
        "author": "Irving, Geoffrey and Christiano, Paul and Amodei, Dario",
        "title": "AI safety via debate"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zeng2022socratic",
        "author": "Zeng, Andy and Attarian, Maria and Ichter, Brian and Choromanski, Krzysztof and Wong, Adrian and Welker, Stefan and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and others",
        "title": "Socratic models: Composing zero-shot multimodal reasoning with language"
      },
      {
        "key": "li2022composing",
        "author": "Li, Shuang and Du, Yilun and Tenenbaum, Joshua B and Torralba, Antonio and Mordatch, Igor",
        "title": "Composing ensembles of pre-trained models via iterative consensus"
      },
      {
        "key": "ajay2023compositional",
        "author": "Ajay, Anurag and Han, Seungwook and Du, Yilun and Li, Shuang and Gupta, Abhi and Jaakkola, Tommi and Tenenbaum, Josh and Kaelbling, Leslie and Srivastava, Akash and Agrawal, Pulkit",
        "title": "Compositional foundation models for hierarchical planning"
      },
      {
        "key": "jiang2024multi",
        "author": "Jiang, Bowen and Xie, Yangxinyu and Wang, Xiaomeng and Su, Weijie J and Taylor, Camillo Jose and Mallick, Tanwi",
        "title": "Multi-modal and multi-agent systems meet rationality: A survey"
      }
    ]
  }
]