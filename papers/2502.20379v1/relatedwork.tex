\section{Related Works}
\label{sec:related-works}
\vspace{-0.5em}

\paragraph{Scaling Test-Time Compute.} 
Recent work has demonstrated that increasing computational resources during inference can significantly improve LLM performance (e.g.,~\citealt{wei2022chain, snell2024scaling}). One line of research focuses on techniques where a single \textit{generator} LLM produces additional output tokens during inference. These include scratchpads or Chain-of-Thought prompting~\citep{nye2021show,wei2022chain}, self-consistency or majority voting techniques~\citep{wang2022self,li2022competition,thoppilan2022lamda,lewkowycz2022solving}, and various self-reflection methods (e.g.,~\citealt{shinn2024reflexion, qu2024recursive, madaan2024self, saunders2022self, bai2022constitutional}). Other works have explored training LLMs to generate special tokens which enhance reasoning ability at test-time (e.g.,~\citealt{goyal2023think, wang2023guiding,herel2024thinking}) or augmenting language models with tool-use abilities (e.g.,~\citealt{schick2023toolformer, gao2023pal, qin2023toolllm, qu2025tool}). 

Another line of research focuses on using a \textit{verifier} model to evaluate the quality or correctness of outputs sampled from generator models~\citep{cobbe2021training,zheng2023judging,snell2024scaling}. Typically, this is done through best-of-$n$ sampling~\citep{stiennon2020learning,cobbe2021training,nakano2021webgpt}, where $n$ candidate outputs are generated and the highest-scoring output is selected based on some verifier. This verification can be performed at the outcome-level~\citep{stiennon2020learning,cobbe2021training} or process-level~\citep{lightman2023let, wang2024math}. Recent works~\citep{coste2023reward,eisenstein2023helping} have also explored using ensembles of homogeneous reward models (identical model initializations trained on the same data but with different random seeds) to mitigate reward model overoptimization~\citep{gao2023scaling}. Additionally, some approaches allow reward models to produce their own Chain-of-Thought reasoning before scoring~\citep{zhang2024generative, mahan2024generative}. Various papers have combined language with search techniques at test-time, using verifiers to provide a heuristic signal. These verifiers may use LLMs as prompted value functions (e.g.,~\citealt{yu2023prompt, yao2024tree, xie2024self}), incorporate real environment feedback (e.g.,~\citealt{zhou2023language, koh2024tree, putta2024agent, long2023large, besta2024graph}), or use trained value functions (e.g.,~\citealt{feng2023alphazero, zhang2024rest, chen2024alphamath}). Unlike prior works which typically rely on a single reward model verifier or homogeneous reward model ensembles trained on the same data, we propose a framework for combining multiple heterogeneous verifiers without additional training, and investigate scaling the number and type of verifiers as a novel test-time scaling dimension.

\paragraph{Multi-Agent Reasoning with Language Models.} Recent works have investigated several approaches to multi-agent interaction for improving language model reasoning. Language model debate (e.g.,~\citealt{du2023improving, chan2023chateval, pham2023let, liang2023encouraging, subramaniam2025multiagent, li2023camel,cohen2023lm}) and multi-agent discourse (e.g.,~\citealt{chen2023reconcile, wang2023unleashing,wang2024rethinking,xu2023towards}) have been studied as ways to enhance reasoning, and also as a direction for scalable oversight research~\citep{irving2018ai}. Prior works have also explored performing search with language models, which typically combines a generator LLM and a value model to guide exploration (see the previous paragraph). Moreover, some works have explored multi-modal reasoning through agent collaboration (e.g.,~\citealt{zeng2022socratic,li2022composing, ajay2023compositional,jiang2024multi}). Unlike prior work on multi-agent reasoning which focuses on collaborative problem-solving, we introduce a framework specifically for scaling test-time verification by combining multiple verifiers without training.