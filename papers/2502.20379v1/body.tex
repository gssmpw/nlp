\section{Introduction}
\label{sec:introduction}
\vspace{-0.75em}

Scaling the size of large language models (LLMs) and their training datasets has driven remarkable progress in artificial intelligence~\citep{brown2020language,chowdhery2023palm,hoffmann2022training}. However, the growing cost of scaling model size and obtaining unseen high-quality pretraining data has sparked growing interest in methods that improve LLM performance without simply scaling parameters or data. Among these, a promising new direction has emerged: \textit{scaling test-time compute}, where models spend more computational resources during inference---much like humans spend more time thinking through harder problems. 

A common strategy for scaling test-time compute is \textit{best-of-$n$ sampling}~\citep{stiennon2020learning,cobbe2021training,nakano2021webgpt}, where $n$ candidate outputs are sampled from a \textit{generator} LLM and a \textit{verifier} model scores each candidate output based on its quality or correctness. The highest-scoring output is then selected. Under this strategy, the amount of test-time compute can be scaled up by increasing the number of sampled outputs. However, in this work, we propose a new orthogonal scaling dimension: \textit{scaling the number of verifiers}. We introduce \textbf{Multi-Agent Verification (MAV)}, a test-time compute paradigm that combines multiple verifiers to improve performance. 

Typically, verifiers are implemented as reward models which are trained using reinforcement learning from human feedback~\citep{christiano2017deep,stiennon2020learning, ouyang2022training,bai2022training}. However, relying on reward models as verifiers introduces two crucial limitations for multi-agent verification: (1) each reward model has to be trained on expensive curated preference data, and (2) there is no straightforward way to combine scores generated by heterogeneous reward models trained on different datasets (they produce uncalibrated scores). These limitations make reward models poorly suited for multi-agent verification and restrict our ability to simply scale up the number and type of verifiers at test-time.

To address these limitations and enable scalable multi-agent verification, we propose using \textbf{Aspect Verifiers (AVs)} --- off-the-shelf LLMs prompted to verify specific aspects of candidate outputs through binary True/False approvals. This approach is motivated by the observation that internet data contains abundant examples of humans providing binary evaluations with feedback (e.g., educational assessments, academic peer reviews, online forums, and automated code tests), which suggests that language models may be naturally suited for binary verification. Unlike reward models, AVs do not require  additional training since producing binary approvals falls naturally within the training distribution of their base LLMs, and their binary outputs can be easily combined across multiple models through simple voting mechanisms. Thus, the number and type of aspect verifiers can be easily scaled up without additional training. We note that aspect verifiers are just one possible implementation choice for the verifiers in a MAV system, which address the two key limitations of typical reward model verifiers.

By aggregating binary signals across a diverse set of aspect verifiers, we can leverage the growing ecosystem of language models to produce a more robust verification signal. Each verifier can focus on different aspects of outputs like mathematical correctness or logical soundness, and employ different verification strategies such as direct yes/no approval, step-by-step analysis, solution rephrasing, or edge case checking. Thus, a diverse set of aspect verifiers can be obtained by varying three key axes: the base LLM, the aspect to verify, and the verification strategy.

To investigate scaling multi-agent verification, we introduce BoN-MAV as a specific algorithm which combines best-of-$n$ sampling with aspect verifiers. This is one implementation of a MAV algorithm, combining traditional best-of-$n$ sampling with multiple verifiers. Given an input, BoN-MAV (1) samples $n$ outputs from a generator LLM, (2) collects binary approvals from a set of $m$ aspect verifiers, and (3) selects the output with the most approvals. We investigate scaling test-time compute with this approach along two orthogonal dimensions: the traditional dimension of increasing the number of sampled candidate outputs $n$, and our novel test-time scaling dimension of increasing the number of verifiers $m$. We find that using multiple diverse verifiers to select between candidate outputs is an effective strategy, and that performance improves as we use more verifiers. 

More specifically, across multiple domains and LLMs, BoN-MAV demonstrates more effective scaling patterns when we increase the number of sampled outputs, compared to best-of-$n$ with reward model verification~\citep{stiennon2020learning,cobbe2021training} and self-consistency~\citep{wang2022self,li2022competition,thoppilan2022lamda,lewkowycz2022solving}. We also demonstrate \textit{weak-to-strong generalization}~\citep{burns2023weak}, whereby combining many small aspect verifiers can improve the performance of even stronger generator LLMs, and we show that BoN-MAV enables \textit{self-improvement} by using the same base LLM for both the generator and set of aspect verifiers. Since  BoN-MAV is just one simple approach to multi-agent verification, we expect that substantial improvements can be achieved using alternative methods.

Overall, our paper makes the following contributions: 
\vspace{-0.25em}
\begin{itemize}[leftmargin=2em, nosep]
    \item[\textbf{(1)}] We introduce Multi-Agent Verification (MAV) as a new test-time paradigm that combines multiple verifiers at test-time, opening a novel scaling dimension: \textit{scaling the number of verifiers}.
    \item[\textbf{(2)}] We propose Aspect Verifiers (AVs), off-the-shelf LLMs which require no additional training and naturally support combining verification signals from multiple heterogeneous verifiers using voting mechanisms.
    \item[\textbf{(3)}] We demonstrate that BoN-MAV, a simple multi-agent verification algorithm which combines best-of-$n$ with aspect verifiers, improves the performance of various generator LLMs as we scale up the number and type of aspect verifiers.
\end{itemize}



\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/mav_illustration_single_solution_MATH_1.pdf}
\caption{
\textbf{Multi-agent verification for a single solution.} An illustration of how combining multiple aspect verifiers can produce a more robust verification signal. Five different aspect verifiers evaluate an incorrect MATH~\citep{hendrycks2021measuring} solution sampled from Gemini-1.5-Pro. The verifiers vary across three dimensions: base models (Gemini-1.5-Flash, GPT-4o-mini), aspects to verify (e.g., general correctness, mathematical correctness, unit conversions), and verification strategies (e.g., direct yes/no approval, step-by-step verification, summarization). Two verifiers miss the error: one using direct approval without step-by-step thinking, and another tasked with checking unit conversions (the problem contains no units to convert, so the verifier finds no errors and incorrectly approves the solution). The remaining three verifiers each identify the mistake through careful analysis. This demonstrates how combining diverse verification methods can produce a robust signal despite individual verifier failures, as the majority correctly identify the error.
}
% NOTE: This is problem 0 from the MATH test questions for Gemini-1.5-Pro.
\label{fig:mav-illustration-single-solution}
\vspace{-10pt}
\end{figure*}

\section{Multi-Agent Verification}
\label{sec:mav}
\vspace{-0.25em}

Multi-Agent Verification (MAV) is a test-time compute paradigm where multiple verifiers are combined to evaluate outputs from a generator LLM. To implement a MAV algorithm, we must address two questions: \textbf{(1)} What type of verifiers can be easily combined and scaled up in number without additional training? \textbf{(2)} How should we aggregate verification signals from multiple verifiers? In this section, we propose answers to these questions and describe one simple implementation of a multi-agent verification algorithm called BoN-MAV. We discuss future directions for alternative multi-agent verification algorithms in \Cref{sec:discussion}.

In \Cref{subsec:aspect-verifiers}, we propose Aspect Verifiers (AVs) as a convenient building block for MAV, since they require no additional training and naturally support combining multiple verification signals. In \Cref{subsec:aggregation}, we describe our approach to aggregating signals across multiple AVs. In \Cref{subsec:bon-mav}, we outline the BoN-MAV algorithm, which combines best-of-$n$ sampling with aspect verifiers. Finally, \Cref{sec:vera-eng} proposes verifier engineering as a method to select relevant verifiers for specific domains or tasks.

\begin{figure*}[th]
\centering
\includegraphics[width=\linewidth]{figures/mav_illustration_multi_solution.pdf}
\caption{
\textbf{Illustration of the BoN-MAV algorithm.} BoN-MAV combines best-of-$n$ sampling with multi-agent verification: First, $n$ candidate outputs are sampled from a generator LLM. Then, each output is evaluated by a set of Aspect Verifiers (AVs) that produce binary approvals. Finally, the candidate with the most approvals is selected as the final answer. See \Cref{subsec:bon-mav} for algorithm details.
}
\vspace{-10pt}
\label{fig:mav-illustration-multi-solution}
\end{figure*}

\subsection{Aspect Verifiers}
\label{subsec:aspect-verifiers}
\vspace{-0.5em}

In the context of test-time computation with LLMs, a \textit{verifier} typically refers to a model that evaluates the quality or correctness of an output sampled from a generator LLM. Here, we ask: \textit{What type of verifiers can be easily combined and scaled up in number without additional training?}

Prior works have largely focused on using neural reward models as verifiers~\citep{stiennon2020learning,cobbe2021training,snell2024scaling}. However, these models present key challenges for scaling multi-agent verification. First, each reward model requires training on expensive curated preference data to produce reliable reward scores~\citep{stiennon2020learning}. Second, while ensembles of homogeneous reward models (identical model initializations trained on the same data but with different random seeds) have been proposed as a way to mitigate overoptimization~\citep{coste2023reward,eisenstein2023helping,gao2023scaling}, there is no straightforward way to combine scores from heterogeneous reward models trained on different datasets. This second limitation arises because scores from different reward models are uncalibrated—--they operate on different numerical scales based on their distinct training setups. These limitations make reward models poorly suited for multi-agent verification, where we wish to simply scale up the number and type of verifiers without additional training.

We propose Aspect Verifiers (AVs) as one possible implementation choice for the verifiers in a MAV system, which address the two key limitations of typical reward model verifiers. AVs are off-the-shelf LLMs prompted to evaluate specific aspects of candidate outputs and produce binary True/False approvals. Unlike reward models, they require no additional training since binary evaluation is a natural task for LLMs (the internet contains abundant examples of humans providing binary approvals with explanation, such as educational assessments, academic peer reviews, online forums, and automated code tests), and their binary approvals can be easily combined through simple voting mechanisms, even when AVs are based on completely different models or training data. Moreover, since AVs are based on LLMs, they can produce Chain-of-Thought reasoning~\citep{wei2021finetuned} to analyze outputs step-by-step before producing an approval, similar to recent work on generative reward models~\citep{zhang2024generative, mahan2024generative}. Using aspect verifiers, we can easily scale up the number and type of verifiers which may be based on different LLMs, training algorithms, architectures, data, or prompts.

Aspect Verifiers can be configured along three axes:
\begin{itemize}[leftmargin=2em, nosep]
    \item[1.] The base LLM---which model acts as the verifier (e.g., GPT-4o-mini or Gemini-1.5-Flash)
    \item[2.] The aspects to verify---what qualities of the candidate output the verifier is prompted to evaluate (e.g., mathematical correctness, logical soundness, factuality, etc.)
    \item[3.] The verification strategy---how the verifier reaches its decision (e.g., direct approval, going over the output step-by-step, rephrasing, checking edge cases, etc.)
\end{itemize}
For example, an aspect verifier could be implemented as GPT-4o-mini evaluating the mathematical correctness of an output from a generator LLM by going over it step-by-step. By varying these three axes, we can create a diverse set of aspect verifiers with differing capabilities. \autoref{fig:mav-illustration-single-solution} illustrates how multiple aspect verifiers can evaluate a single candidate output, and \autoref{appx:experiment-details} contains a full list of the verifiers used in this work and the prompts for each.



\begin{table*}[t]\centering
\ra{1.3}
\begin{tabular}{@{}lc@{\hspace{10pt}}c@{\hspace{10pt}}cc@{\hspace{10pt}}c@{\hspace{10pt}}cc@{\hspace{10pt}}c@{\hspace{10pt}}cc@{\hspace{10pt}}c@{\hspace{10pt}}c@{}}
% \begin{tabular}{@{}lcccccccccccc@{}}
\toprule
 & \multicolumn{3}{c}{MATH} & \multicolumn{3}{c}{MMLU-Pro} & \multicolumn{3}{c}{GPQA (diamond)} & \multicolumn{3}{c}{HumanEval} \\
\cmidrule(r){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(l){11-13}
\textbf{{Generator LLM}} & B-MAV & Cons & RM & B-MAV & Cons & RM & B-MAV & Cons & RM & B-MAV & Cons & RM\\ \midrule
\textbf{Gemini-1.5-Flash} & \underline{\textbf{66.0}} & 59.0 & 61.7 & \underline{\textbf{66.7}} & 63.3 & 60.7 & 42.0 & 40.0 & \underline{\textbf{46.0}} & \underline{\textbf{80.0}} & 79.0 & 79.0 \\
\textbf{Gemini-1.5-Pro} & \underline{\textbf{72.7}} & 70.3 & 71.0 & \underline{\textbf{72.3}} & 71.7 & 69.3 & \underline{\textbf{49.0}} & 45.0 & \underline{\textbf{49.0}} & \underline{\textbf{88.0}} & 84.0 & \underline{\textbf{88.0}} \\
\textbf{GPT-4o-mini} & 73.0 & \underline{\textbf{74.7}} & 72.3 & \underline{\textbf{67.0}} & 63.7 & 62.7 & \underline{\textbf{50.0}} & 48.0 & 44.0 & 84.0 & \underline{\textbf{87.0}} & 85.0 \\
\textbf{GPT-4o} & 76.3 & 77.3 & \underline{\textbf{80.7}} & 75.7 & \underline{\textbf{76.3}} & 72.7 & \underline{\textbf{59.0}} & \underline{\textbf{59.0}} & 58.0 & 92.0 & \underline{\textbf{95.0}} & 92.0 \\
\textbf{Mistral-7B} & \underline{\textbf{26.0}} & 22.0 & 21.7 & \underline{\textbf{36.7}} & 25.7 & 31.0 & 36.0 & 32.0 & \underline{\textbf{37.0}} & \underline{\textbf{59.0}} & 46.0 & 52.0 \\
\textbf{Llama-3.1-8B} & \underline{\textbf{61.7}} & 61.0 & 54.7 & \underline{\textbf{59.3}} & 55.3 & 51.3 & \underline{\textbf{43.0}} & 36.0 & 41.0 & \underline{\textbf{75.0}} & 62.0 & 64.0 \\
\textbf{Gemma-2-9B} & \underline{\textbf{58.7}} & 51.7 & 55.0 & \underline{\textbf{57.7}} & 54.3 & 54.7 & 34.0 & 36.0 & \underline{\textbf{38.0}} & 32.0 & 25.0 & \underline{\textbf{51.0}} \\
\textbf{Gemma-2-27B} & \underline{\textbf{62.3}} & 55.7 & 59.3 & \underline{\textbf{62.0}} & 58.3 & 60.0 & \underline{\textbf{41.0}} & 40.0 & \underline{\textbf{41.0}} & \underline{\textbf{76.0}} & 66.0 & \underline{\textbf{76.0}} \\
\bottomrule
\end{tabular}
\caption{
\textbf{Best-of-$n$ with Multi-Agent Verification (BoN-MAV) across models and domains.} Performance (accuracy \%) comparison of three test-time verification methods using $n=16$ candidate outputs: the BoN-MAV algorithm (labeled as B-MAV in the table), reward model verification (RM), and self-consistency (Cons). Results are shown for eight generator LLMs across four domains, with BoN-MAV on each domain using the domain-specific aspect verifier subset $\mathcal{M}^d$. BoN-MAV outperforms self-consistency in nearly all cases, and generally outperforms RM except on GPQA (diamond) and HumanEval, where BoN-MAV and RM achieve comparable results.
}
\label{tab:main}
\vspace{-10pt}
\end{table*}



\subsection{Combining Aspect Verifiers}
\label{subsec:aggregation}

With aspect verifiers as our building block, we ask: \textit{How can we effectively aggregate verification signals across multiple AVs?} We take the simplest possible approach in our experiments: each binary True/False approval is a single vote, and the aggregated score for a candidate output is the sum of the positive votes from all AVs. That is, the aggregated verification score is the sum of the individual binary scores from each verifier: 
\vspace{-0.2em}
\begin{equation}
\text{AggScore}(o^{(i)}) = \frac{1}{|\mathcal{M}|} \sum_{v \in \mathcal{M}} \text{BinaryScore}_v(o^{(i)}),
\label{eqn:aggregated-score}
\end{equation}
\vspace{-1.25em}

where $o^{(i)} \in \mathcal{O}$ is the $i$th candidate output from the set of sampled outputs $\mathcal{O}$, $\mathcal{M}$ is the set of aspect verifiers, and $\text{BinaryScore}_v : \mathcal{O} \to \{0, 1\}$ maps a candidate output from $\mathcal{O}$ to the binary approval produced by verifier $v \in \mathcal{M}$ for that output. This voting strategy gives equal weight to all verifiers in the final aggregated score, and it proves remarkably effective in our experiments (see \Cref{sec:exps-main}).  
However, future works could investigate more sophisticated aggregation strategies such as grouping verifiers by aspect and then voting across aspects, or having aspect verifiers debate with each other~\citep{du2023improving} before producing an approval. We discuss these and other potential directions for future work in \Cref{sec:discussion}.

\subsection{BoN-MAV}
\label{subsec:bon-mav}
Best-of-$n$ (BoN) sampling is a test-time optimization technique~\citep{stiennon2020learning,cobbe2021training,nakano2021webgpt} where $n$ candidate outputs are sampled from a generator LLM, each candidate is scored by a verifier model, and the highest-scoring output is selected. We introduce BoN-MAV as a simple multi-agent verification algorithm that combines best-of-$n$ sampling with aspect verifiers. It uses the simple aggregation strategy from \autoref{eqn:aggregated-score} and consists of three steps: (1) sampling $n$ candidate outputs from a generator LLM, (2) collecting binary approvals from a set of $m$ aspect verifiers, and (3) selecting the output with the most approvals. That is,
\vspace{-0.2em}
\begin{equation}
    \hat{i} = \argmax_{0 < i < n} \left( \text{AggScore}(o^{(i)}) \right)
\end{equation}
\vspace{-1.25em}

where $o^{(i)}$ is the $i$th candidate output, $n$ is the total number of sampled candidate outputs, and $\hat{i}$ is the index of the output with the highest aggregated score (the selected output). \autoref{fig:mav-illustration-multi-solution} illustrates how BoN-MAV can be used to select between a set of candidates. 

Using BoN-MAV, we can increase test-time computation by sampling more candidate outputs (increasing $n$) and by querying more verifiers (increasing $m = |\mathcal{M}|$), where test-time computation can be easily parallelized during generation as well as verification. In addition, BoN-MAV represents just one specific approach to multi-agent verification, and more nuanced aggregation algorithms or alternatives to aspect verifiers could further enhance performance.

\subsection{Verifier Engineering}
\label{sec:vera-eng}
Using aspect verifiers, we can create a diverse pool of verifiers with different capabilities. However, not all verifiers are equally relevant for every domain (e.g., math, coding, general knowledge). Thus, we propose \textit{verifier engineering} as a process to select a subset of verifiers most effective for a particular domain (similar to prompt engineering, where prompts are engineered for specific domains or tasks). 

We engineer domain-specific sets of verifiers by first creating a diverse initial set $\mathcal{M}$ and then selecting the subset $\mathcal{M}^d \subseteq \mathcal{M}$ which contains the most relevant verifiers for domain $d$. Specifically, for each domain $d$, we select the subset $\mathcal{M}^d \subseteq \mathcal{M}$ which maximizes the average performance across all generator LLMs evaluated on a validation set. Our current approach keeps the engineered set of verifiers fixed for all questions in a domain, but future works could explore dynamically customizing verifiers for particular questions, as we discuss in \Cref{sec:discussion}. 


\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/plot_multiScaling_valOnTest_paperSubset.pdf}
\vspace{-18pt}
\caption{
\textbf{Scaling the number of candidate outputs.} Performance (accuracy \%) of test-time compute methods as we increase the number of sampled candidate outputs ($n$), shown for four generator LLMs (Gemini-1.5-Flash, Gemini-1.5-Pro, Mistral-7B, and Llama-3.1-8B) across all evaluation domains. The BoN-MAV algorithm demonstrates more effective scaling patterns than self-consistency (cons@n) across all domains, and stronger scaling than reward model verification (BoN-RM@n) except on GPQA (diamond).
}
\vspace{-10pt}
\label{fig:scaling-solutions}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/plot_verifierScaling_SHADING_valOnTest_paperSubset.pdf}
\vspace{-15pt}
\caption{
\textbf{Scaling the number of verifiers.} Performance (accuracy \%) of BoN-MAV as we increase the number of verifiers ($m$) up to domain-specific subsets $\mathcal{M}^d$ (detailed in \Cref{sec:exps-main}). For each $m$, we plot the performance of BoN-MAV averaged across all possible combinations of $m$ verifiers drawn from $\mathcal{M}^d$, with shading indicating the spread of observed values --- dark blue shows the 25-75th percentile range (middle 50\%) while light blue shows the 5-95th percentile range (90\% of outcomes). The leftmost point ($m=0$) represents pass@1 accuracy without verification while the rightmost point ($m=|\mathcal{M}^d|$) uses all verifiers in the domain-specific set. Results demonstrate that increasing the number of verifiers is a promising test-time scaling dimension, with gains of up to 10\% for large LLMs and up to 20\% for small ones, even when stronger generator LLMs (Gemini-1.5-Pro, GPT-4o) are verified by our weaker aspect verifiers. We observe domain and model-dependent variation and some diminishing returns at higher verifier counts, and we expect better-engineered verifiers to unlock even stronger scaling patterns.
}
\vspace{-10pt}
\label{fig:scaling-verifiers}
\end{figure*}


\vspace{-0.1em}
\section{Experiments}
\label{sec:exps}
\vspace{-0.25em}
In our experiments, we investigate scaling test-time compute along two orthogonal dimensions: the traditional dimension of increasing the number of sampled candidate outputs $n$, and our novel test-time scaling dimension of increasing the number of verifiers $m$. We aim to address the following questions: \textbf{(1)} How well does multi-agent verification improve performance across diverse domains and various generator LLMs? \textbf{(2)} Can multi-agent verification facilitate weak-to-strong generalization and self-improvement? \textbf{(3)} How important is engineering a domain-specific set of verifiers and what are the important design choices?

To address these questions, we evaluate the BoN-MAV algorithm described in \Cref{sec:mav} on the following four domains:

\begin{itemize}[leftmargin=2em, nosep]
\item{\textbf{Mathematics.}} The MATH dataset~\citep{hendrycks2021measuring}  consists of competition-level math questions at five difficulty levels. For our experiments, we randomly sample 400 questions from the test set across all five levels: 100 for validation and 300 for testing. 

\item{\textbf{General Knowledge \& Reasoning.}} MMLU-Pro ~\citep{wang2024mmlu} is an enhanced version of the popular MMLU benchmark~\citep{hendrycks2020measuring} which features more challenging, reasoning-focused questions and expands the multiple-choice set from four to ten options. As with MATH, we sample 100 questions for validation and 300 for testing. 

\item{\textbf{Graduate-Level Reasoning.}} The GPQA dataset~\citep{rein2023gpqa} consists of graduate-level, multiple-choice questions in biology, physics, and chemistry. For our experiments, we utilize GPQA's ``diamond'' subset --- a collection of 198 high-quality and extremely challenging questions. We sample 98 questions for validation and 100 for testing. 

\item{\textbf{Coding.}} HumanEval~\citep{chen2021evaluating} is a widely-used benchmark consisting of 164 Python programming questions. We sample 64 questions for validation and 100 for testing.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MAV Enables Scaling Along Two Dimensions}
\label{sec:exps-main}
\vspace{-0.5em}

\begin{figure*}[t]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=\linewidth]{figures/solutionScalingPlot_perDataset_math_gemini-1.5-flash_valOnTest_bigScale_OUTPUTS.pdf}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=\linewidth]{figures/solutionScalingPlot_perDataset_math_gemini-1.5-flash_valOnTest_bigScale_BUDGET.pdf}
\end{minipage}
\vspace{-0.5em}
\caption{
\textbf{Scaling to 256 candidate outputs.} Comparison of different test-time verification methods on MATH~\citep{hendrycks2021measuring} as we increase the number of candidate outputs sampled from Gemini-1.5-Flash up to 256 (x-axes shown in log scale.). \textit{Left:} Accuracy (\%) versus number of sampled outputs $n$. BoN-MAV consistently improves with additional samples while reward model verification (BoN-RM) and self-consistency (cons) plateau much earlier. \textit{Right:} Accuracy (\%) versus total compute budget (number of queries to both generator LLM and each verifier). While BoN-MAV initially underperforms due to the overhead of querying multiple verifiers limiting the number of candidate outputs we can sample, it significantly outperforms both baseline methods once given sufficient compute to leverage multiple verifiers effectively. Note that both x-axes are on a log scale.
}
% \vspace{-0.25em}
\label{fig:large-scale-analysis}
\end{figure*}

\begin{table*}\centering
\ra{1.3}
\begin{tabular}{@{}cl@{}c@{\hspace{5pt}}cc@{\hspace{5pt}}cc@{\hspace{5pt}}cc@{\hspace{5pt}}c@{}}
\toprule
& & \multicolumn{2}{c}{MATH} & \multicolumn{2}{c}{MMLU-Pro} & \multicolumn{2}{c}{GPQA (diamond)} & \multicolumn{2}{c}{HumanEval} \\
\cmidrule(r){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(l){9-10}
& \textbf{{Generator LLM}} & B-MAV  & pass@1 & B-MAV  & pass@1 & B-MAV  & pass@1 & B-MAV  & pass@1\\ 
\cmidrule(r){1-10} 
\multirow{2}{*}{\textbf{Weak-to-Strong}} & \multicolumn{1}{|l}{\textbf{Gemini-1.5-Pro}} & \underline{\textbf{72.7}} & 64.7 & \underline{\textbf{72.3}} & 68.0 & \underline{\textbf{49.0}} & 45.0 & \underline{\textbf{88.0}} & 84.0 \\
& \multicolumn{1}{|l}{\textbf{GPT-4o}} & \underline{\textbf{76.3}} & 68.3 & \underline{\textbf{75.7}} & 73.3 & \underline{\textbf{59.0}} & 54.0 & 92.0 & \underline{\textbf{94.0}} \\
\midrule
% [1.5pt]
\multirow{2}{*}{\textbf{Self-Improvement}} & \multicolumn{1}{|l}{\textbf{Gemini-1.5-Flash}} 
& \underline{\textbf{59.0}} & 52.7 
& \underline{\textbf{64.0}} & 59.3 
& \underline{\textbf{43.0}} & 42.0 
& 78.0 & \underline{\textbf{79.0}} \\
& \multicolumn{1}{|l}{\textbf{GPT-4o-mini}} 
& \underline{\textbf{76.0}} & 69.0 
& \underline{\textbf{65.7}} & 62.3 
& \underline{\textbf{46.0}} & 38.0 
& \underline{\textbf{86.0}} & \underline{\textbf{86.0}} \\ 
\bottomrule
\end{tabular}
\caption{
\textbf{Weak-to-strong generalization and self-improvement.} Performance (accuracy \%) using the BoN-MAV algorithm (labeled as B-MAV in the table) compared to base pass@1 accuracy. For weak-to-strong generalization (top), we use aspect verifiers based on weaker models (Gemini-1.5-Flash and GPT-4o-mini) to improve stronger generator LLMs. For self-improvement (bottom), we use aspect verifiers based on the same model as the generator. BoN-MAV improves performance in nearly all cases.
\label{tab:wts-and-self-imp}
}
\vspace{-15pt}
\end{table*}

Here, we investigate how BoN-MAV scales with the number of candidate outputs and number of verifiers.

\paragraph{Baselines.} We compare Best-of-$n$ sampling with Multi-Agent Verification (BoN-MAV) against two established test-time compute methods: (1) best-of-$n$ sampling with reward model verification~\citep{stiennon2020learning,cobbe2021training,nakano2021webgpt}, where we use a trained neural reward model as the external verifier to select the highest-scoring candidate output, and (2) self-consistency~\citep{wang2022self,li2022competition,thoppilan2022lamda,lewkowycz2022solving}, which selects the most common answer from the set of candidates outputs. For reward model verification, we use the current top-performing open-source 8B reward model on RewardBench~\citep{lambert2024rewardbench}. See \Cref{appx:experiment-details:reward-model-baseline} for more details.

\paragraph{Verifier Engineering} For our experiments, we implement the verifier engineering method described in \Cref{sec:vera-eng}. To create our initial diverse pool $\mathcal{M}$ of 20 aspect verifiers, we vary the three key axes that define aspect verifiers:
\begin{enumerate}[leftmargin=2em, nosep]
    \item[1.] Base model: Gemini-1.5-Flash or GPT-4o-mini
    \item[2.] Aspect to verify: Mathematical correctness, logical soundness, factuality, etc.
    \item[3.] Verification strategy: Direct approval, step-by-step verification, solution rephrasing, edge case checking, etc.
\end{enumerate}
From this pool, we then select domain-specific subsets $\mathcal{M}^d \subseteq \mathcal{M}$ that maximize average performance across all generator LLMs on the corresponding validation sets. The complete list of verifiers and the domain-specific subsets are detailed in \autoref{tab:verifier-subsets}. We choose Gemini-1.5-Flash and GPT-4o-mini as the base LLMs for our aspect verifiers since they are cost-effective for large-scale verification and enable us to demonstrate that combining multiple weaker verifiers can improve the performance of even stronger generator LLMs (\Cref{sec:exps-wts-and-self-imp}).

\paragraph{Quantitative Results.} We evaluate BoN-MAV across four domains using eight generator LLMs (four closed-source and four open-source). For each model, we sample $n=16$ candidate outputs per question and compare between best-of-$n$ with Multi-Agent Verification (BoN-MAV), best-of-$n$ with reward model verification (BoN-RM), and self-consistency (cons). As shown in \autoref{tab:main}, BoN-MAV outperforms self-consistency in nearly all cases, and outperforms reward model verification on MATH and MMLU-Pro, while achieving comparable results on GPQA (diamond) and HumanEval.

\paragraph{Qualitative Examples.} \autoref{fig:mav-illustration-single-solution} illustrates how multiple aspect verifiers can be used to evaluate a single candidate output. The first aspect verifier uses direct yes/no approval without step-by-step thinking and incorrectly approves the output while additional aspect verifiers, using the same base model but with more thorough verification strategies, successfully identify the error. Additional examples are provided in \autoref{appx:extra-mav-illustrations}. Note that for the purposes of illustration, we visualize slightly different sets of verifiers than the final domain-specific sets used in our experiments.


\paragraph{Scaling the Number of Candidate Outputs.} In \autoref{fig:scaling-solutions}, we show the scaling patterns for various generator LLMs as we increase the number of sampled candidate outputs ($n$). Matching the results in \autoref{tab:main}, BoN-MAV demonstrates more effective scaling patterns than self-consistency across all domains, and stronger scaling than reward model verification on MATH and MMLU-Pro while achieving comparable scaling patterns on GPQA (diamond) and HumanEval. 

\paragraph{Scaling the Number of Verifiers.} Multi-Agent Verification introduces a powerful new dimension for scaling test-time compute: \textit{scaling the number of verifiers}. In  \autoref{fig:scaling-verifiers}, we show how accuracy tends to improve as we increase the number of verifiers $m$ from zero verifiers up to the full domain-specific subset $\mathcal{M}^d$. For each value of $m \in \{0, 1, 2, ..., |\mathcal{M}^d|\}$, we plot the average accuracy across all possible combinations of $m$ verifiers drawn from $\mathcal{M}^d$, with the shaded regions indicating the spread of observed values (dark blue shows the middle 50\% range, and light blue captures 90\% of outcomes). The leftmost point ($m=0$) represents the base pass@1 accuracy without  verification, while the rightmost point ($m=|\mathcal{M}^d|$) shows the accuracy when using all verifiers in the domain-specific subset, matching the values reported in \autoref{tab:main}. Note that \autoref{fig:teaser} shows just one randomly selected sequence of verifiers for illustration, rather than averaging across all possible combinations like in \autoref{fig:scaling-verifiers}.

Our results demonstrate that scaling verifier count is a promising new dimension for improving model performance at test-time. In most cases, accuracy improves as we add verifiers, with performance gains of up to 10\% for large LLMs and up to 20\% for small ones. Notably, performance gains persist even when strong generator LLMs (Gemini-1.5-Pro, GPT-4o) are verified by combinations of our weaker verifiers (Gemini-1.5-Flash, GPT-4o-mini), supporting our findings about weak-to-strong generalization in \Cref{sec:exps-wts-and-self-imp}. However, the magnitude and pattern of improvement varies and, in some cases, accuracy initially decreases before improving with additional verifiers. We expect better-engineered verifiers to unlock even stronger scaling patterns.

\paragraph{Scaling Up to 256 Candidate Outputs.} We extend our analysis to even larger scales by sampling 256 candidate outputs from Gemini-1.5-Flash on MATH. In \autoref{fig:large-scale-analysis}, we plot accuracy as a function of both the number of sampled candidate outputs $n$ (left) and the total compute budget (right). The left plot demonstrates that BoN-MAV consistently improves with additional samples, while reward model verification and self-consistency plateau early on. Starting at 52.7\% base accuracy, the baselines plateau around 61\% while BoN-MAV continues to 69\%---nearly double the improvement. The right plot shows computational efficiency by comparing accuracy against the total compute budget, measured as the combined number of queries to both the generator and verifier models. At low compute budgets, the overhead of querying multiple verifiers with BoN-MAV means we can sample fewer candidate solutions, leading to initially worse performance than the baselines. However, once we have sufficient compute, BoN-MAV significantly outperforms both baseline methods. These results demonstrate that multi-agent verification can significantly improve model performance at test-time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MAV Enables Weak-to-Strong ~~~~~~~~~~~~Generalization and Self-Improvement}
\label{sec:exps-wts-and-self-imp}
\vspace{-0.5em}

We now explore two important capabilities of multi-agent verification: improving strong models using only weaker verifiers (weak-to-strong generalization) and improving models using only self-verification (self-improvement).

\paragraph{Weak-to-Strong Generalization.} Prior work has shown that weak supervisors can improve the performance of strong pretrained models~\citep{burns2023weak}. Here, we show that multi-agent verification can be used to enhance the performance of strong generator LLMs by combining weaker verifiers. As shown in \autoref{tab:wts-and-self-imp}, our strongest generators (Gemini-1.5-Pro and GPT-4o) show substantial improvements over their base pass@1 accuracy when using verifiers based on weaker models (Gemini-1.5-Flash and GPT-4o-mini), and \autoref{fig:scaling-verifiers} shows how the performance of Gemini-1.5-Pro and GPT-4o changes as we scale the number of verifiers. These results suggest that the diverse perspectives of multiple smaller models can collectively produce a verification signal robust enough to improve even state-of-the-art generators. It also shows that effective verification can be achieved using computationally cheaper and faster models, rather than requiring large verifiers which match the performance of the generator---a promising result for deploying multi-agent verification at scale. 

\paragraph{Self-Improvement.} Multi-agent verification can also enable models to improve their own performance through self-verification. To demonstrate, we configure BoN-MAV to use the same base LLM for both generation and verification. That is, we sample candidate outputs from a generator LLM (Gemini-1.5-Flash or GPT-4o-mini) and create multiple aspect verifiers derived from the same LLM. Following the verifier engineering procedure from \Cref{sec:exps-main}, we select the best subset of self-verifiers based on validation performance. As shown in \autoref{tab:wts-and-self-imp}, this self-verification approach yields substantial improvements over base pass@1 accuracy across all domains except HumanEval. For instance, GPT-4o-mini shows particularly strong self-improvement on MATH (+7\%) and GPQA diamond (+8\%). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis: Understanding Multi-Agent Verification}
\label{sec:exps-ablations}
% \vspace{-0.5em}

To better understand the key design choices that impact multi-agent verification, we conduct two ablation studies on MMLU-Pro and GPQA (diamond)---the two most challenging domains in our evaluation. We investigate: \textbf{(1)} how performance depends on engineering domain-specific sets of verifiers, and \textbf{(2)} whether using diverse verifiers outperforms repeatedly querying the single best verifier.

\paragraph{Effect of Verifier Engineering.} In \Cref{sec:exps-main}, we introduced verifier engineering as an approach for selecting a relevant subset of verifiers $\mathcal{M}^d \subseteq \mathcal{M}$ for each domain $d$. Here, we compare our engineered verifier subsets $\mathcal{M}^d$ against a simple baseline that uses all available aspect verifiers in $\mathcal{M}$ (see \Cref{appx:experiment-details:verifier-subsets} for a full list) without any domain-specific tuning. \autoref{tab:ablations-eng} shows that engineering the set of verifiers is a more effective strategy. However, \autoref{appx:tab:all-vs-baselines} in the Appendix shows that even the simple strategy of combining all verifiers in $\mathcal{M}$ remains competitive with both self-consistency and reward model verification baselines.

\begin{table}[h!]\centering
\ra{1.3}
\begin{tabular}{@{}lcccc@{}}
\toprule
 & \multicolumn{2}{c}{MMLU-Pro} & \multicolumn{2}{c}{GPQA (diamond)} \\
\cmidrule(r){2-3} \cmidrule(l){4-5}
\textbf{{Generator LLM}} & Eng & All & Eng & All\\ \midrule
\textbf{Gemini-1.5-Flash} & \underline{\textbf{66.7}} & 65.7 & \underline{\textbf{42.0}} & 41.0 \\
\textbf{Gemini-1.5-Pro} & \underline{\textbf{72.3}} & 70.3 & \underline{\textbf{49.0}} & \underline{\textbf{49.0}} \\
\textbf{GPT-4o-mini} & \underline{\textbf{67.0}} & 65.3 & \underline{\textbf{50.0}} & 49.0 \\
\textbf{GPT-4o} & \underline{\textbf{75.7}} & 75.3 & \underline{\textbf{59.0}} & 55.0 \\
\bottomrule
\end{tabular}
\caption{
\textbf{Ablation: Verifier-Engineering.} Performance comparison between using the engineered domain-specific verifier subsets (Eng) versus using all verifiers without tuning (All).
}
\vspace{-1em}
\label{tab:ablations-eng}
\end{table}


\paragraph{Effect of Verifier Diversity.} Here, we investigate whether using diverse verifiers outperforms repeatedly querying a single verifier. Specifically, we compare the performance of our diverse domain-specific subsets $\mathcal{M}^d$ versus repeatedly querying the single best-performing verifier $v^* \in \mathcal{M}^d$ for domain $d$  (where the number of queries to $v^*$ equals $|\mathcal{M}^d|$). As shown in \autoref{tab:ablations-div}, using diverse sets of verifiers generally outperforms querying the same verifier multiple times. 


\begin{table}[h!]\centering
\ra{1.3}
\begin{tabular}{@{}lcccc@{}}
\toprule
 & \multicolumn{2}{c}{MMLU-Pro} & \multicolumn{2}{c}{GPQA (diamond)} \\
\cmidrule(r){2-3} \cmidrule(l){4-5}
\textbf{{Generator LLM}} & Diverse & Same & Diverse & Same\\ \midrule
\textbf{Gemini-1.5-Flash} & \underline{\textbf{66.7}} & 66.3 & \underline{\textbf{42.0}} & 39.0 \\
\textbf{Gemini-1.5-Pro} & \underline{\textbf{72.3}} & 71.0 & 49.0 & \underline{\textbf{55.0}} \\
\textbf{GPT-4o-mini} & \underline{\textbf{67.0}} & 64.7 & \underline{\textbf{50.0}} & 42.0 \\
\textbf{GPT-4o} & \underline{\textbf{75.7}} & 75.0 & \underline{\textbf{59.0}} & 58.0 \\
\bottomrule
\end{tabular}
\caption{
\textbf{Ablation: Verifier Diversity.} Performance comparison between using diverse verifiers from $\mathcal{M}^d$ (Diverse) versus querying the single best-performing verifier multiple times (Same).
}
\label{tab:ablations-div}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.5em}
\section{Discussion}
\label{sec:discussion}
\vspace{-0.2em}
Multi-Agent Verification (MAV) introduces a promising dimension for scaling test-time compute: scaling the number of verifiers. In \Cref{sec:exps}, we demonstrated that combining multiple verifiers enables more effective evaluation of candidate outputs, facilitates weak-to-strong generalization, and allows for self-improvement. However, our approach has important limitations and there are several opportunities for future work to explore.

First, our investigation is limited to a pool of 20 aspect verifiers based on just two base LLMs, and the design of our verifiers is constrained by our ability to come up with diverse verification strategies and relevant aspects. Future work could explore scaling to many more verifiers and try a more systematic exploration of the space of verifiers, potentially using LLMs themselves to generate diverse verification strategies and identify relevant aspects to verify. With better-engineered verifiers and more systematic exploration, we expect to observe stronger scaling patterns.

Second, our aggregation technique described in \Cref{subsec:aggregation} uses a simple voting mechanism that directly sums the individual binary approvals from each verifier. This approach does not account for the confidence or relevance of each verifier, and verifiers do not observe each other's decisions or feedback. Future works could explore more sophisticated aggregation methods such as confidence-weighted voting or allowing verifiers to engage in debate~\citep{du2023improving} before producing an approval. Moreover, our current approach uses a static engineered set of verifiers $\mathcal{M}^d$ for all questions in a domain $d$, even though it may be best to use fewer or different verifiers for specific questions. Future works could investigate dynamically selecting the best set of verifiers for particular problems or adaptively choosing additional verifiers based on the results of the first few verification queries. Additionally, the field of social choice theory~\citep{arrow2012social,fishburn2015theory,kelly2013social,brandt2016handbook} is concerned with procedures for collective decision-making and might offer insights for aggregating the perspectives of diverse verifiers. Although, our setting differs in that we care more about verifier capabilities than preferences. 

Next, our implementation of BoN-MAV is limited to only a single generator LLM. Thus, an interesting direction would be to explore sampling from multiple generators in addition to evaluating with multiple verifiers. Since different models may excel at solving different types of problems, this approach could make even better use of the growing ecosystem of LLMs and their diverse capabilities.

Furthermore, while our results show that BoN-MAV can improve language model performance at test-time, we did not investigate finetuning the generator LLM on the outputs selected by our verifiers. Similar to how prior works have finetuned on outputs selected through self-consistency~\citep{huang2022large} or reward models~\citep{dong2023raft}, training on outputs selected by MAV systems could be explored as a method to improve  the generator LLM and also each of the LLM-based verifiers. Moreover, an interesting direction for future work is to directly use reinforcement learning to train both the generator and verifier models. That is, generator LLMs can be trained to maximize the scores across multiple verifiers, and the verifiers can simultaneously be trained to accurately verify individual aspects of responses.

Finally, multi-agent verification offers interesting opportunities for AI safety and oversight. The ability to combine multiple verifiers checking different aspects of model outputs aligns with recent efforts towards safety checking the outputs of language models. That is, different verifiers can be engineered to check various safety and alignment properties, from basic constraints like avoiding harmful content to more nuanced properties like reasoning transparency. Our results on weak-to-strong generalization also align with recent work on scalable oversight, where weaker systems supervise stronger ones~\citep{amodei2016concrete, saunders2022self,burns2023weak}. In general, our work connects to broader ideas in AI alignment about using multiple models to improve safety~\citep{irving2018ai}.

An underlying thread throughout our work and discussion is the vision of a growing ecosystem of diverse language models that generate, verify, and learn from each other. Our work on multi-agent verification represents one step in this direction, and each of the future directions we have discussed offers a potential avenue for additional progress. We look forward to seeing how the research community advances these ideas.

\vspace{-0.25em}
\section{Related Works}
\label{sec:related-works}
\vspace{-0.5em}

\paragraph{Scaling Test-Time Compute.} 
Recent work has demonstrated that increasing computational resources during inference can significantly improve LLM performance (e.g.,~\citealt{wei2022chain, snell2024scaling}). One line of research focuses on techniques where a single \textit{generator} LLM produces additional output tokens during inference. These include scratchpads or Chain-of-Thought prompting~\citep{nye2021show,wei2022chain}, self-consistency or majority voting techniques~\citep{wang2022self,li2022competition,thoppilan2022lamda,lewkowycz2022solving}, and various self-reflection methods (e.g.,~\citealt{shinn2024reflexion, qu2024recursive, madaan2024self, saunders2022self, bai2022constitutional}). Other works have explored training LLMs to generate special tokens which enhance reasoning ability at test-time (e.g.,~\citealt{goyal2023think, wang2023guiding,herel2024thinking}) or augmenting language models with tool-use abilities (e.g.,~\citealt{schick2023toolformer, gao2023pal, qin2023toolllm, qu2025tool}). 

Another line of research focuses on using a \textit{verifier} model to evaluate the quality or correctness of outputs sampled from generator models~\citep{cobbe2021training,zheng2023judging,snell2024scaling}. Typically, this is done through best-of-$n$ sampling~\citep{stiennon2020learning,cobbe2021training,nakano2021webgpt}, where $n$ candidate outputs are generated and the highest-scoring output is selected based on some verifier. This verification can be performed at the outcome-level~\citep{stiennon2020learning,cobbe2021training} or process-level~\citep{lightman2023let, wang2024math}. Recent works~\citep{coste2023reward,eisenstein2023helping} have also explored using ensembles of homogeneous reward models (identical model initializations trained on the same data but with different random seeds) to mitigate reward model overoptimization~\citep{gao2023scaling}. Additionally, some approaches allow reward models to produce their own Chain-of-Thought reasoning before scoring~\citep{zhang2024generative, mahan2024generative}. Various papers have combined language with search techniques at test-time, using verifiers to provide a heuristic signal. These verifiers may use LLMs as prompted value functions (e.g.,~\citealt{yu2023prompt, yao2024tree, xie2024self}), incorporate real environment feedback (e.g.,~\citealt{zhou2023language, koh2024tree, putta2024agent, long2023large, besta2024graph}), or use trained value functions (e.g.,~\citealt{feng2023alphazero, zhang2024rest, chen2024alphamath}). Unlike prior works which typically rely on a single reward model verifier or homogeneous reward model ensembles trained on the same data, we propose a framework for combining multiple heterogeneous verifiers without additional training, and investigate scaling the number and type of verifiers as a novel test-time scaling dimension.

\paragraph{Multi-Agent Reasoning with Language Models.} Recent works have investigated several approaches to multi-agent interaction for improving language model reasoning. Language model debate (e.g.,~\citealt{du2023improving, chan2023chateval, pham2023let, liang2023encouraging, subramaniam2025multiagent, li2023camel,cohen2023lm}) and multi-agent discourse (e.g.,~\citealt{chen2023reconcile, wang2023unleashing,wang2024rethinking,xu2023towards}) have been studied as ways to enhance reasoning, and also as a direction for scalable oversight research~\citep{irving2018ai}. Prior works have also explored performing search with language models, which typically combines a generator LLM and a value model to guide exploration (see the previous paragraph). Moreover, some works have explored multi-modal reasoning through agent collaboration (e.g.,~\citealt{zeng2022socratic,li2022composing, ajay2023compositional,jiang2024multi}). Unlike prior work on multi-agent reasoning which focuses on collaborative problem-solving, we introduce a framework specifically for scaling test-time verification by combining multiple verifiers without training.

\section{Conclusion}
\label{sec:conclusion}
We have introduced Multi-Agent Verification (MAV), a test-time compute paradigm that combines multiple verifiers to improve performance. MAV enables test-time scaling along two orthogonal dimensions: (1) the traditional dimension of increasing the number of candidate outputs sampled from a \textit{generator} LLM, and (2) our novel test-time scaling dimension of increasing the number of \textit{verifiers} evaluating each output. We propose Aspect Verifiers (AVs) as one possible implementation choice for the verifiers in a MAV system. AVs are off-the-shelf LLMs that require no additional training and naturally support combining verification signals from models based on different LLMs, training algorithms, architectures, data, or prompts. Thus, AVs are a convenient building block for multi-agent verification, allowing us to leverage the growing ecosystem of language models and their diverse capabilities. We introduce BoN-MAV as a simple multi-agent verification algorithm and  our results indicate that increasing the number of diverse verifiers is a promising dimension for scaling test-time compute. Specifically, we demonstrate that this approach improves test-time performance across multiple domains and generator LLMs, enables weak-to-strong generalization by combining multiple weak verifiers to improve stronger generators, and facilitates self-improvement when the generator LLM is also used as the base LLM for each of the aspect verifiers. Moreover, BoN-MAV represents just one approach to multi-agent verification and we expect better-engineered verifiers and more nuanced aggregation strategies to unlock even stronger scaling patterns. We hope that our work inspires future research into multi-agent verification algorithms and further exploration of scaling the number of verifiers as a powerful new dimension for test-time compute. 

\section*{Acknowledgments}
We thank Romi Lifshitz, Andrew Li, Parand Alamdari, Claas Voelcker, Lev McKinney, Toryn Klassen, and David Glukhov for their helpful comments. We thank ArdaLabs for providing funding and compute resources for this project (\url{https://ardalabs.ai/}). The second author gratefully acknowledges funding from the Natural Sciences and
Engineering Research Council of Canada (NSERC) and the Canada CIFAR AI Chairs
Program. 
%
Resources used in preparing this research
were provided, in part, by the Province of Ontario, the Government of
Canada through CIFAR, and companies sponsoring the Vector Institute for
Artificial Intelligence (\url{https://vectorinstitute.ai/partnerships/}). 
