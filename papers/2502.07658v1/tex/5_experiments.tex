\section{Experiments}



\begin{table*}[]
\caption{
Performance of constructed interest units, where CTR Improvement is compared with normal product. The real CTR is omitted due to privacy issue.
}
\label{tab:iuanalysis}
\begin{tabular}{ccccccc}
\midrule
\textbf{IU Type\textbackslash Metric} & \textbf{Numbers} & \textbf{Product Coverage} & \textbf{Exposure Ratio} & \textbf{Click Ratio} & 
\textbf{Bills Ratio} & \textbf{CTR Improvement} \\ \toprule
\textbf{SPU}                          & 45.9k            & 23.0\%                    & 9.28\%                  & 10.4\%       & 23.9\%           & +21.92\%                                     \\
\textbf{Image}                        & 287k            & 5.11\%                    & 13.7\%                  & 12.5\%        & 24.3\%        & +34.75\%                                     \\
\textbf{Semantic}                     & 615k            & 78.8\%                    & 77.0\%                  & 77.1\%      & 51.8\%            & +12.85\%                                     \\ \midrule
\end{tabular}
\end{table*}



\begin{table*}[]
\caption{
Model comparison on the production dataset, where "RI" is short for "RelaImpr". The improvements are statistically significant (i.e. two-side t-test with $p < 0.05$) over the original model.
}
\label{tab:offline_result}
\begin{tabular}{cc|cccc|cc|cc}
\midrule
\multicolumn{2}{c|}{\multirow{2}{*}{}}                                                                                                             & \multicolumn{4}{c|}{\textbf{Overall}}                 & \multicolumn{2}{c|}{\textbf{Intereste Unit}} & \multicolumn{2}{c}{\textbf{Normal Product }} \\ \cline{3-10} 
\multicolumn{2}{c|}{  \diagbox{\textbf{Method}}{\textbf{Metric}}  }                                                                                                                      & \textbf{AUC}    & \textbf{RI}   & \textbf{GAUC}   & \textbf{RI}   & \textbf{AUC}                & \textbf{RI}                & \textbf{AUC}                   & \textbf{RI}                       \\ \midrule
\multicolumn{1}{c|}{\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Common \\ DNNs\end{tabular}}} & DNN                                                 & 0.7348          & -0.75\%        & 0.6509          & -0.46\%        & 0.6972                      & -1.12\%                   & 0.7428                         & -0.53\%               \\
\multicolumn{1}{c|}{}                                                                        & Wide\&Deep\cite{wideanddeep} & 0.7346          & -0.83\%       & 0.6506          & -0.66\%     & 0.6970                       & -1.22\%                   & 0.7425                         & -0.66\%               \\
\multicolumn{1}{c|}{}                                                                        & DeepFM\cite{deepfm}          & 0.7335          & -1.30\%         & 0.6504          & -0.79\%         & 0.6965                      & -1.47\%                   & 0.7415                         & -1.07\%               \\
\multicolumn{1}{c|}{}                                                                        & DIN\cite{din}                & 0.7366          & 0.00\%        & 0.6516          & 0.00\%        & 0.6994                      & 0.00\%                    & 0.7441                         & 0.00\%                \\ \midrule
\multicolumn{1}{c|}{\multirow{3}{*}{Gate Methods}}                                            & GateNet\cite{gatenet}              & 0.7356          & -0.41\%        & 0.6512          & -0.26\%         & 0.6984                      & -0.52\%                    & 0.7432                         & -0.37\%               \\
\multicolumn{1}{c|}{}                                                                        & FiBiNet\cite{senet}                & 0.7355          & -0.45\%         & 0.651           & -0.40\%         & 0.6982                      & -0.62\%                    & 0.7431                         & -0.41\%               \\
\multicolumn{1}{c|}{}                                                                        & POSO\cite{poso}                    & 0.7365          & -0.03\%         & 0.6515          & -0.07\%         & 0.6988                      & -0.32\%                    & 0.744                          & -0.04\%               \\ \midrule
\multicolumn{1}{c|}{\multirow{2}{*}{Graph Methods}}                                           & GroupEmb\cite{groupid}             & 0.7367          & 0.05\%          & 0.6517          & 0.07\%          & 0.6995                      & 0.04\%                    & 0.7443                         & 0.08\%                \\
\multicolumn{1}{c|}{}                                                                        & GIFT\cite{gift}                    & 0.7368          & 0.10\%          & 0.652           & 0.26\%          & 0.6999                      & 0.24\%                    & 0.7445                         & 0.16\%                \\ \midrule
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Former \\ Baseline\end{tabular}}              & MSNet\cite{wu2024metasplit}        & 0.7392          & 1.11\%          & 0.6544          & 1.85\%         & 0.7039                      & 2.25\%                    & 0.7468                         & 1.11\%                \\ \midrule
\multicolumn{1}{c|}{\textbf{Ours}}                                                           & IU-Boosted Network                                               & \textbf{0.7411} & \textbf{1.91\%} & \textbf{0.6549} & \textbf{2.17\%} & \textbf{0.7082}             & \textbf{4.39\%}            & \textbf{0.7484}                & \textbf{1.76\%}                 \\ \midrule
\end{tabular}
\end{table*}

\begin{table}[]
\caption{Comparison between production model and IU-Boosted Model in two domains during online A/B tests followed by industry standards. 
}
\begin{tabular}{cccc}
\toprule
  \diagbox{\textbf{Domain}}{\textbf{Metric}}     & \textbf{CTR} & \textbf{Clicks} & \textbf{Bills}  
                     \\ \toprule
\textbf{Overall} & +2.90\%      & +2.62\%         & +1.02\%                     \\
\textbf{Interest Unit Rec} & +11.76\%      & +10.22\%         & +5.61\%                    \\
\textbf{General Product Rec} & +1.58\%      & +1.49\%         & +0.33\%        \\ \toprule
\end{tabular}
\label{tab:online result}
\end{table}

In this section, we conduct extensive experiments on the offline
dataset and online A/B testing to evaluate the performance of the proposed method. The following research questions (RQs) are addressed:
\begin{itemize}
    \item \textbf{RQ1}: How do the constructed interest units perform, and have they achieved the desired outcomes in terms of efficiency and coverage?  (Section 5.2)
    \item \textbf{RQ2}: How does the new product interaction interface perform and how do the metics compared with former product interface? (Section 5.2)
    \item \textbf{RQ3}: Does your proposed IU-Boosted Network outperform state-of-the-art performance? (Section 5.3)
    \item \textbf{RQ4}: Does your proposed model work in real large-scale online recommendation scenarios? (Section 5.2 and 5.4)
\end{itemize}
\subsection{Experiment Setup}

\subsubsection{\textbf{Xianyu Dataset}}
We collect click traffic logs of 8 days from Alibabaâ€™s Xianyu Recommendation Platform\footnote{The proposed paradigm and the proposed method are implemented from a industrial practice and there are no suitable public data-set.} to build the production data-set with 10 billion samples (1.5 billion sample per day), with numbers of items and people, 209 features (e.g., user and item features). 
The samples in the first 7 days and 8th day are employed for training and testing respectively, and we randomly partition the testing set into 10 parts and report average evaluation results. 

\subsubsection{\textbf{Baselines}}
We compare the proposed IU-Boosted Network with three types of baselines: the first type are common DNN-based methods, including:
\begin{itemize}
    \item \textbf{DNN}: a method proposed by Youtube, and has been been implemented in various industrial applications.
    \item \textbf{Wide \& Deep} \cite{wideanddeep}:  a method proposed by Google that consists of a wide model and a deep model and has gained significant adoption in various applications.
    \item \textbf{DeepFM} \cite{deepfm}: a method proposed by Google that combines a wide model with a deep model and has been widely adopted in various applications.
    \item \textbf{DIN} \cite{din}: our base model, as described in Section 4, which uses attention mechanism to  aggregate historical behaviors sequence information based on the similarity between historical behaviors item and the target item.
\end{itemize}
The second type are some gate network methods.
\begin{itemize}
    \item \textbf{GateNet} \cite{gatenet}: a method proposed by Baidu, using gate layer to generate weights for the embedding layer and the hidden layer.
    \item \textbf{FiBiNet} \cite{senet}: a method proposed by Sinanet, filtering the information of embedding based on their importance using the SENet module.
    \item \textbf{POSO} \cite{poso}: a method proposed by KuaiShou, a short-video application. We implement it as described in the paper with personal MLP and personal MHA.
\end{itemize}
The third type are graph learning methods.
\begin{itemize}
    \item \textbf{GroupID}~\cite{groupid}: a meta-learning approach proposed by Airbnb to generate new item embeddings by calculating the average of similar item embeddings. 
    \item \textbf{GIFT}~\cite{meta_emb}: a meta-learning approach to address the cold-start problem by generating ID embeddings for cold items using other available features.
\end{itemize}



\subsubsection{\textbf{Evaluation Metric}}
Following previous works \cite{din,dien,star,clid}, we adopt some widely used metrics, AUC, GAUC and RelaImpr, to evaluate the performance of our proposed method. 
The calculation of AUC, GAUC, RelaImpr can be expressed as follow:
\begin{equation}
\mathrm{AUC}=\frac{1}{|P||N|} \Sigma_{p \in P} \Sigma_{n \in N} I(\Theta(p)>\Theta(n)),
\end{equation}
where $P$ and $N$ denote positive sample set and negative sample set, respectively. $\Theta$ is the estimator function and $I$ is the indicator function.

\begin{equation}
\mathrm{GAUC}=\frac{\sum_{i=1}^n \# \text { impression }_i \times \mathrm{AUC}_i}{\sum_{i=1}^n \text { \#impression }_i},
\end{equation}
where the AUC is first calculated within samples of each user, and averaged w.r.t sample count, where $n$ is the number of users, \#impression $i$ and $\mathrm{AUC}_i$ are the number of impressions and AUC corresponding to the $i$-th user.

\begin{equation}
\text { RelaImpr }=\left(\frac{\text { AUC }(\text { measured model })-0.5}{\text { AUC }(\text { base model })-0.5}-1\right) \times 100 \%
\end{equation}
where RelaImpr metric is used to measure relative improvement over models and 0.5 stands for the AUC of a random guesser. Generally, higher values for AUC and GAUC indicate better performance.


\subsubsection{\textbf{Implementation Details.}}
We implement the DNN part of DeepFM, Wide \& Deep, DNN, DIN all the same architecture, i.e., a three-layer MLP Network with 512, 256 and 128 hidden units. For all attention layers in above models, the number of hidden units are all set to 128. AdagradDecay optimizer is adopted in all the methods, the learning
rate of 1e-4 is set. Batch size is set to 4096 for all DNNs. And the historical click sequence is collected within the last 30 days and max length is 150. The max length of the click sequence inside IU click sequence is 5.

\subsection{RQ1 \& RQ2: Product Format Comparison and Analysis of Constructed Interest Unit}
After the launch of the two-stage recommendation and distribution based on interest units, we chose to integrate it into the homepage in the form of a workflow. This means the homepage has two workflows, one for distributing normal product recommendations and the other for IU product recommendations (workflow in Figure~\ref{fig:iu4rec_overview} and product format in Figure~\ref{fig:new_product}). Both are merged into a single queue based on certain rules~\footnote{These rules are mainly established from a business decision perspective} and presented to users. To verify the effectiveness of this product format, we conducted an A/B test with 10\% of the traffic. The experimental group used the aforementioned product format, while the control group used the original product recommendation format. The experiment lasted for 7 days, and the new product format achieved a 5\% improvement in bills, a 10\% improvement in GMV, and even in exposure, there was a minor decrease in clicks. Using 13\% of the exposure, it contributed to 18\% of the GMV, proving the effectiveness of this product format. We rapidly expanded this product format to a larger traffic volume, leaving a reversal bucket for observation.
The exposure ratio of IU on the homepage exceeded 10\%. The constructed types of interest units are presented in Table~\ref{tab:iuanalysis}, and we can see the click-through rate for different IUs have certain advantages compared to the overall product efficiency.

\subsection{RQ3: Model Comparison With Baselines}
To demonstrate the effectiveness and superiority of the proposed method, we conduct offline experiments on the Xianyu dataset and compare it with state-of- the-art (SOTA) approaches. The experimental results are presented in Table~\ref{tab:offline_result}. 
Compared with previous approaches, our method achieved the best overall performance. Specifically, when compared to the baseline model DIN, our approach delivered a 1.91\% enhancement in AUC and a 2.17\% increase in GAUC~\footnote{Note that the 0.1\% AUC gain is already considerable in large-scale industrial recommendation system \cite{chang2023pepnet}}
. Additionally, our method achieved the highest AUC metrics for both interest units and normal products recommendation.


\subsection{RQ4: Online A/B test}
In Xianyu's recommendation system, we compared a new model's effectiveness against a highly-optimized baseline (DIN architecture) through an online A/B test.
The 7-day experiment period provided statistically validated, reliable results. We used CTR, Clicks, and Bills  as metrics to evaluate the online effectiveness.
The results are shown in Table~\ref{tab:online result}.
The IU-Boosted model demonstrated significant improvements in both interest unit and normal product recommendations. Specifically, for interest unit recommendations, there was an 11.76\% increase in CTR, a 10.22\% rise in Clicks, and a 5.61\% boost in Bills. For normal product recommendations, CTR improved by 3.72\% and Clicks by 5.20\%. Overall, the model achieved a 2.90\% increase in CTR, a 2.62\% rise in Clicks, and a 1.02\% increase in Bills, highlighting a notable performance enhancement.

