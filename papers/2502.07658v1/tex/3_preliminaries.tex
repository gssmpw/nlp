\section{Preliminaries}

\begin{figure}[tbp]
    \centerline{\includegraphics[width=8cm]{base_model.png}}
    \caption{An illustration of Embedding and MLP structure with sequence modeling for the deep CTR prediction model}
    \label{fig:din_model}
    % \vspace{-0.4cm}
\end{figure}


In this section, we formulate the problem and briefly introduce the existing production CTR prediction model on the large-scale recommender system of Xianyu.

% \textit{\textbf{Problem Formulation}}: 
\subsection{Problem Formulation}
Given a dataset $\mathcal{D}=\{ \mathbf{x}, y \}^N,(\mathbf{x}, y)$ marks a sample and $N$ is the number of samples, where $\mathbf{x}$ denotes the high dimensional feature vector consisting of multi-fields (e.g., user and item field), and $y$ is the binary label with $y = 1$ indicating the sample is clicked. Our task is to accurately predict the probability of CTR $p_{ctr} = p(y=1|x)$ for the testing sample $x$.

% 多样性打散
\subsection{Production CTR Prediction Model}

We select Deep Interest Network \cite{din} as our base model due to its online efficiency and effectiveness, which follows the conventional Embedding and MLP paradigm and utilizes an attention mechanism to model user behavior sequences, as depicted in Figure~\ref{fig:din_model}.

\textit{\textbf{Embedding Layer}}: 
The inputs are composed of non-sequential features (e.g., user ID) and sequential features (e.g., user’s history clicked items). The embedding layer is employed to convert each discrete feature from the raw input into a vector of lower dimensions, by using an embedding look-up table. The embedding of non-sequential features is simply concatenated, whereas for the embedding of sequential features, a sequence information modeling module is used to assemble them into a fixed-size representation.


\textit{\textbf{Sequence Information Modeling}}: 
Deep Interest Network utilizes a local attention mechanism to dynamically capture the user's interests based on the similarity between their historical clicked items and the target item. 
This mechanism allows for the acquisition of a personalized representation of the user's interests since it enables weight pooling of the information in the sequence of varying length.
Additionally, DIN could be further optimized by leveraging the target attention mechanism equations~\cite{BST,transformer,pssa} to replace the original local attention mechanism. In the existing Multi-head Target-attention (MHTA) implementation, the target item $Item_t$ is consider as query $(Q)$ and the history click sequence $\boldsymbol{S}_u$ is considered both as keys $(K)$ and values $(V)$, where $\boldsymbol{S}_u=\left\{Item_1, Item_2, \ldots, Item_H\right\}$ is the set of embedding vectors of items in the user behaviors with length of $H$. 
% Then, they are produced by a linear projection layer. $Q =W^Q Item_t$, $K=W^K X^{\text {seq }}$, and $V=W^V X^{\text {seq }}$, where ($W^Q$, $W^K$ and $W^V$ are trainable matrices)

Specifically, the output of MHTA can be formalized as follows :


% \begin{equation}
% \begin{gathered}
% \boldsymbol{h}_u=\text { TargetAttention }\left(\boldsymbol{h}_t \boldsymbol{W}^Q, \boldsymbol{H}_u \boldsymbol{W}^K, \boldsymbol{H}_u \boldsymbol{W}^V\right) \\
% \end{gathered}
% \end{equation}
\begin{equation}
\text { TargetAttention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{\top}}{\sqrt{d}}\right) V,
\end{equation}
where $Q =W^Q Item_t$, $K=W^K X^{\text {seq }}$, and $V=W^V X^{\text {seq }}$, the linear projections matrices $W^Q \in \mathbb{R}^{d \times d}, W^K \in \mathbb{R}^{d \times d}, W^V \in$ $\mathbb{R}^{d \times d}$ are learn-able parameters and $d$ stands for the dimension of hidden space. The temperature $\sqrt{d}$ is introduced to produce a softer attention distribution for avoiding extremely small gradients. Finally, all non-sequential embedding and transformed user sequential embedding is concatenated with other continuous features together to generate the overall embedding and pass through a deep network to get the final prediction.



\textit{\textbf{Loss}}: 
The objective function used in DIN is the negative log-likelihood function defined as:
\begin{equation}
L=-\frac{1}{N} \sum_{(x, y) \in \mathcal{D}}(y \log f(x)+(1-y) \log (1-f(x))),
\end{equation}
where $D$ is the training set, each sample $x$ is associated with a ground-truth label $y$. The output of the model, denoted as $f(x)$, represents the predicted probability of sample $x$ being clicked.

