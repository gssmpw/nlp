%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\input{preamble/preamble} 
\input{preamble/preamble_math}
\input{preamble/preamble_acronym}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Preference learning made easy: Everything should be understood through win rate}

\begin{document}

\twocolumn[
\icmltitle{Preference learning made easy:\\Everything should be understood through win rate}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lily H. Zhang}{cds}
\icmlauthor{Rajesh Ranganath}{cds,courant}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{cds}{Center for Data Science, New York University, New York, USA}
\icmlaffiliation{courant}{Courant Institute, New York University, New York, USA}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Lily H. Zhang}{lily.h.zhang@nyu.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML, preference learning, RLHF, DPO, SFT, language model}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Preference learning, or the task of aligning generative models to preference comparison data, has yet to reach the conceptual maturity of classification, density estimation, etc. To close this gap, this work presents a framework to understand preference learning starting from the sampling distribution of pairwise preference data. First, we prove that the only evaluation of a generative model that respects both preferences and prevalences in the data distribution is a form of win rate, justifying win rate as the focal point to understand preference learning. We then analyze preference learning methods as win rate optimization (WRO) or non-WRO. We present novel instances of WRO beyond existing examples (RLHF, NLHF) and identify two key theoretical benefits of all such methods. We prove that common non-WRO methods like DPO and SFT on preferred samples lack these properties and suggest ways to mitigate such theoretical limitations. We also show that WRO underperforms in practice due optimization difficulties and that optimization success predicts performance better than choices which affect the objective's solution. Our analysis highlights best practices for existing methods and provides recommendations for future research, guided by the principle that one should either align non-WRO methods more closely with WRO or improve the optimization of WRO objectives.
\end{abstract}

\section{Introduction}
Learning from preference data, often referred to as human feedback, has emerged as a key step in training large language models, particularly given the success of reinforcement learning from human feedback (RLHF) \citep{Christiano2017DeepRL} on state-of-the-art and high-profile language models such as GPT-4 \citep{openai2024gpt4technicalreport}. 
The goal 
of learning from preference data
is to finetune powerful base language models to output generations more in line with human preferences \citep{Stiennon2020LearningTS,Ouyang2022TrainingLM},
motivated by the fact that pretraining on internet-scale data has enabled large language models to exhibit fluent generations of text \citep{Minaee2024LargeLM}
but not necessarily responses aligned with what humans prefer to see.

In recent years, the landscape of algorithms and evaluations for preference learning has grown significantly \citep{Kaufmann2023ASO,jiang2024surveyhumanpreferencelearning}, resulting in a complex and often fragmented field that can pose challenges for practitioners and researchers deciding where to focus their efforts. Whereas well-established machine learning tasks such as classification are grounded in principles such as maximum likelihood, preference learning lacks such a unifying conceptual foundation. To advance the field, we thus first ask: what underlying principles underpin preference learning?

We address this question 
by developing a 
framework 
starting from the sampling distribution implied by pairwise preference data. 
We first tackle the question of evaluating a model under the preference learning paradigm. We show that the only evaluation of a generative model rooted in the preference data sampling distribution itself is a form of win rate we call $h$-win rate; any other notions of good either do not evaluate the model or respect the preference data, or are based on assumptions outside of either (\Cref{sec:setup}). 
From this insight, we introduce a win rate-centric framework for understanding the landscape of preference learning methods.
Given $h$-win rate is the only relevant evaluation without additional assumptions, 
we relate common preference learning algorithms 
to directly optimizing for win rate, dividing the preference learning space into win rate optimization (WRO) and non-WRO objectives. 
In \Cref{sec:framework}, we generalize the space of WRO beyond existing methods in the literature and identify two theoretical benefits of such methods:
(1) win rate-correspondence, i.e., optimizing the objective corresponds to optimizing for $h$-win rate; and
(2) win rate-consistency, i.e., the solution can achieve the maximum win rate possible over a competitor as regularization strength goes to zero.
In \Cref{sec:not_wro}, we show that direct preference optimization (DPO) and other direct alignment algorithms \cite{rafailov2024scalinglawsrewardmodel} fail win rate-correspondence, and that supervised finetuning (SFT) on preferred samples additionally fails win rate-consistency. 
In \Cref{sec:experiments}, we show that despite their theoretical benefits, WRO methods underperform relative to expectations due to difficulties in optimization, which has a much more significant effect on performance than other design choices that distinguish WRO methods from each other. 
We conclude with takeaways 
for current practice 
and future research in preference learning (\Cref{sec:discussion}). 

Our contributions can be summarized as follows:
\begin{enumerate}
    \item We prove that the only evaluation of a generative model grounded in the preference data distribution is win rate. This result justifies using win rate as the focal point to understand the landscape of preference learning.
    \item We present a win rate-centric framework to understand preference learning. 
    Under this framework, we:
    \begin{enumerate}
        \item introduce win rate-correspondence and consistency and show that WRO methods satisfy both while non-WRO do not;
        \item highlight the central role of optimization success for the performance;
        \item motivate best practices for existing methods (e.g., win rate checkpointing with direct alignment algorithms, multiple seeds with WRO algorithms, inducing diversity for SFT, combining different approaches); and 
        \item provide recommendations for future research---namely, to focus on better surrogates for or improved optimization of WRO.
    \end{enumerate}
\end{enumerate}

\section{Related Work}
\label{sec:related}
Our work is most closely related to previous work in win rate evaluation and optimization as well as analysis of RLHF and preference learning objectives.

\paragraph{Win rate evaluation and optimization.} 
Win rate is already a central evaluation in preference learning \citep{alpaca_eval,zheng2024judging}; however, our work goes further and proves that it is the only evaluation grounded in the sampling distribution itself, thus motivating its use as the central object to understand the rest of the preference learning landscape.
Several works have proposed methods that perform some form of win rate optimization \citep{munos2023nash, swamy2024minimaximalistapproachreinforcementlearning, rosset2024directnashoptimizationteaching}, including as a minimax optimization with a dynamic (rather than fixed) opponent model. Our work provides a more general framework to characterize the landscape of win rate optimization (WRO) methods beyond the specific instances that currently exist in the literature. Moreover, our analysis points to the theoretical merit of these approaches, while our experiments demonstrate the implementation bottlenecks of them.


\paragraph{Analyzing RLHF, DPO, and other preference learning methods.}
Our work is related to work that seeks to 
% Many works seek to improve upon or 
better understand RLHF, DPO, and other existing methods in preference learning (e.g., best-of-n). 
Examples include benchmarking generalization and diversity \citep{kirk2024understanding}, comparing on- vs off-policy approaches \citep{tajwar2024preferencefinetuningllmsleverage}, investigating length bias \citep{singhal2024longwaygoinvestigating}, and disentangling design choices empirically \citep{ivison2024unpackingdpoppodisentangling}. 
For RLHF in particular, 
existing works consider
the complexity of proximal policy optimization
\citep{ahmadian2024back},
vanishing gradients \citep{Razin2023VanishingGI},
reward model overoptimization \citep{zhu2024iterative}, or
limitations of the Bradley-Terry assumption to relate preferences to rewards
\citep{wang2024transforming,azar2023general,munos2023nash}.
For DPO, there exists not only large space of alternative direct alignment algorithms (e.g., \citep{zhao2023slichf,azar2023general, xu2024contrastivepreferenceoptimizationpushing,  huang2024correctingmythosklregularizationdirect,pal2024smaugfixingfailuremodes,xu2024thingscringeothersiterative}) but also works which analyze its limited ability to flip rankings 
\citep{Chen2024PreferenceLA} or the decrease in chosen and rejected log probabilities \citep{razin2024unintentionalunalignmentlikelihooddisplacement}.
\citet{gui2024bonbonalignmentlargelanguage} show that the target distribution of best-of-n for different choices of $n$ yields a win rate vs. KL divergence curve close to that of RLHF's for different choices of divergence constraint; \Cref{thm:sft_gap} of this work generalizes their win rate result for best-of-2 from a deterministic to an arbitrary preference environment.
\citet{azar2023general} present a family of $\Psi$-Preference Optimization objectives,
show that RLHF falls within this family, and propose Identity Preference Optimization (IPO); this work presents a larger family of WRO objectives beyond $\Psi$-Preference Optimization and categorizes $\Psi$-Preference Optimization and IPO separately into WRO and non-WRO objectives, highlighting the 
theoretical benefits of the former and theoretical limitations of the latter. 

\section{What evaluations make sense for preference learning?}
\label{sec:setup}
In this section, we analyze how to evaluate a generative model under the preference learning landscape. 
Various evaluations have been used in the preference learning literature, from win rate \citep{alpaca_eval} and reward \citep{ahmadian2024back} to implicit reward accuracy \citep{rafailov2024scalinglawsrewardmodel} and ranking accuracy \citep{Chen2024PreferenceLA}. 
Here, we critically assess what evaluations are meaningful under the pairwise preference comparisons data itself.

We first describe the underlying sampling distribution of the pariwise comparison data used in preference learning (\Cref{sec:sampling_dist}). Then we define and motivate two important conditions for an evaluation function, preference-consistency and prevalence-consistency, and prove that the only evaluations that satisfy both conditions is a form of win rate we call $h$-win rate
(\Cref{sec:win_rate}). This result justifies win rate as the singular focus in preference learning.

\subsection{The Sampling Distribution}
\label{sec:sampling_dist}
The goal of preference learning is to learn a generative model that performs well in a given context.  However, whereas typical maximum likelihood training employs samples from the distribution of interest, the setup of preference learning does not: only samples from generative model competitors (sometimes the same model) and their relative preference within a pair are available. 

The sampling distribution for preferences consists of input $\mbx$, candidate outputs $\mby_0$ and $\mby_1$, and a label $\ell \in \{0, 1\}$ denoting which of $\mby_0$ or $\mby_1$ is preferred. Let $\ell=1$ denote that $\mby_1$ is preferred, and $\ell=0$ denote that $\mby_0$ is preferred. Then, the overall sampling distribution can be defined as follows:

\begin{definition}
\label{def:sampling_dist}
A \textbf{sampling distribution for (pairwise) preference learning} is a distribution over input $\mbx \in \mathcal{X}$, candidate outputs $\mby_0, \mby_1 \in \mathcal{Y}$, and preference label $\ell \in \{0, 1\}$ defined by:
    \begin{enumerate}
    \itemsep0em
    \item Query distribution: $p(\mbx)$
    \item Generation competitor 0: $p(\mby_0 \g \mbx)$
    \item Generation competitor 1: $p(\mby_1 \g \mbx)$
    \item Preference classifier: $p(\ell \g \mbx, \mby_0, \mby_1)$.
\end{enumerate}
1, 2, and 3 are \textit{user-specified} distributions; 1 denotes the inputs of interest, and 2 and 3 are the candidate competitors one chooses to evaluate. 4 is only distribution that cannot be directly specified; rather, it is defined by the environment in which the user chooses to collect the preferences.
\end{definition}

Generation competitor 0 and 1 can be the same distribution and often are in existing open-source preference datasets \citep{h4stackexchange,Ethayarajh2021UnderstandingDD}.

\subsection{Win Rate is the Only Evaluation that can Matter}
\label{sec:win_rate}
The goal of preference learning is to learn some generative model $p^*(\mby \g \mbx)$ that performs well under the preference environment for a given query distribution (we refer to this as the \textit{query-preference environment}). 
Learning such a generative model requires a definition of what is good.

Consider 
an evaluation function $\phi$ which maps
a generative model $p(\mby \g \mbx)$, query-preference environment $\mathcal{E} = (p(\mbx), p(\ell\g\mbx, \mby_0, \mby_1))$, and anchor distribution $p(\mby_0 \g \mbx)$ to a scalar: $\phi_{p(\mby_0 \g \mbx)}(p(\mby\g \mbx), \mathcal{E}) \in \mathbb{R}$.\footnote{ 
We will optionally write $\phi_{p(\mby_0 \g \mbx)}$ as $\phi$ when the anchor is clear from context.}
Intuitively,
$\phi$ should respect properties of the preference sampling distribution.
We formalize criterion for $\phi$ below.
\begin{definition}
\label{def:grounded}
    Any evaluation function $\phi$ is \textbf{grounded} in a given preference distribution if 
    \begin{enumerate}[itemsep=0em]
        % \itemsep0em
        \item \label{condition-1}($\phi$ is preference-consistent): given query environment $p(\mbx) = \indicator{\mbx=\mbx'}$, generative model $p(\mby\g \mbx) = \indicator{\mby=\mby'}$, anchor distribution $p(\mby_0\g \mbx) = \indicator{\mby_0=\mby_0'}$, and strictly increasing function $h$: 
        \begin{align*}
        \phi(p(\mby\g \mbx), \mathcal{E}) = h \cdot p(\ell=1 \g \mbx', \mby_0', \mby')\text{; and}
        \end{align*}
        \item \label{condition-2}($\phi$ is prevalence-consistent): for $a, b \geq 0$ and $a + b = 1$:
        \begin{enumerate}
        \item \label{condition_2_generator} for generator $p(\mby\g \mbx) = a p_1(\mby\g \mbx) + b p_2(\mby\g \mbx)$: \begin{align*}
            &\phi(p(\mby\g \mbx), \mathcal{E}) = a \phi(p_1(\mby\g \mbx), \mathcal{E}) + b \phi(p_2(\mby\g \mbx), \mathcal{E});
        \end{align*}
        \item \label{condition_2_query} for query distribution $p(\mbx) = a p_1(\mbx) + b p_2(\mbx)$, letting $\mathcal{E}_i = (p_i(\mbx), p(\ell=1\g\mbx, \mby_0, \mby_1))$:
        \begin{align*}
            &\phi(p(\mby\g \mbx), \mathcal{E}) = a \phi(p(\mby\g \mbx), \mathcal{E}_1) + b \phi(p(\mby\g \mbx), \mathcal{E}_2);
        \end{align*}
        \item \label{condition_2_anchor} for anchor distribution $p(\mby_0\g \mbx) = a p_1(\mby_0\g \mbx) + b p_2(\mby_0\g \mbx):$
        \begin{align*}
            &\phi_{p(\mby_0\g \mbx)}(p(\mby\g \mbx), \mathcal{E}) = a \phi_{p_1(\mby_0\g \mbx)}(p(\mby\g \mbx), \mathcal{E}) \nonumber \\
        &\qquad \qquad \qquad \qquad  \;\;\;+ b \phi_{p_2(\mby_0\g \mbx)}(p(\mby\g \mbx), \mathcal{E}).
        \end{align*}
        \end{enumerate}
    \end{enumerate}
\end{definition}
Preference-consistency ensures that 
the evaluation 
can be reduced to an increasing function of the
preference classifier in the base case where 
query, model, and anchor distributions are all a single deterministic value. The transformation function $h$ dictates that the evaluation need not exactly equal the preference probability, just that the evaluation should be higher when the preference probability is higher.
Prevalence-consistency ensures that the contribution of $p(\ell=1 \g \mbx, \mby_0, \mby_1)$
for a given $(\mbx, \mby_0, \mby_1)$
gets weighted appropriately by its prevalence in the query environment, anchor distribution, and generative model, respectively. 

What are the consequences if these properties are not met? If preference-consistency is not satisfied, a model can be assigned a high value under $\phi$ even if it only outputs a dispreferred response according to the ground truth preference. For prevalence-consistency, if \ref{condition_2_generator} is not satisfied, then $\phi$ weights preferences according to a distribution that is \emph{not} how the model generates; if \ref{condition_2_query} is not satisfied, then $\phi$ does not reflect the chosen queries; and if \ref{condition_2_anchor} is not satisfied, then $\phi$ does not compare to the chosen anchor.

The only evaluation that satisfies \Cref{def:grounded} is a form of win rate:
\begin{proposition}\label{prop:win_rate}
    $\phi$ is grounded, as defined in \Cref{def:grounded}, if and only if 
    \vskip -.3in
    \begin{align}\label{eq:win_rate}
        &\phi_{p(\mby_0 \g \mbx)}(p(\mby\g \mbx), \mathcal{E}) \nonumber \\= &\E_{p(\mbx)}\E_{p(\mby \g \mbx)}\E_{p(\mby_0 \g \mbx)}[h \cdot p(\ell = 1 \g \mbx, \mby_0, \mby)]
    \end{align}
    for choice of strictly increasing function h.
\end{proposition}

See \Cref{sec:win_rate_proof} for proof. \Cref{eq:win_rate} is the win rate of the generator $p(\mby \g \mbx)$ against an anchor distribution $p(\mby_0 \g \mbx)$ under the query-preference environment $\mathcal{E}$, for some choice of order-preserving transformation function $h$. 
When $h$ is the identity, we have vanilla win rate, which we will refer to as win rate or $\text{Win Rate}_{p(\mby_0 \g \mbx)}[p(\mby \g \mbx)]$. Often it is estimated via samples from the preference classifier, i.e., $\E_{p(\mbx,\mby,\mby_0)}\ell,$ where $\ell \sim p(\ell \g \mbx, \mby_0, \mby)$ \citep{alpaca_eval}. When $h$ is any other strictly increasing function, we have an $h$-variant of win rate, which we will refer to as $h$-Win Rate. 

Different $h$-variants 
can stretch preference probabilities in some regions while contracting them in others, effectively prioritizing certain preference probability differences over others in the evaluation of generator vs. anchor.
% (e.g., specifying that .6 vs .5 matters much more than .9 vs .8). 
Note that placing non-identity functions $f, g$ in any other position, i.e., $\E_{p(\mbx)}f \cdot \E_{p(\mby \g \mbx)}g \cdot \E_{p(\mby_0 \g \mbx)}[h \cdot p(\ell = 1 \g \mbx, \mby_0, \mby)]$, breaks prevalence-consistency of query and generator respectively.

What about other evaluations? \Cref{prop:win_rate} proves that all other evaluations either break preference- or prevalance-consistency.
For instance, reward and ranking accuracy, typically computed on offline data, do not respect model prevalences given the pairs in the computation may be unlikely overall under the model; empirically, they have also been shown to correlate little with win rate \citep{rafailov2024scalinglawsrewardmodel}.
Average reward $\E_{p(\mbx, \mby)}r(\mbx, \mby)$ is equivalent to a translated $\text{logit}$-Win Rate under the Bradley-Terry assumption $p(\ell=1\g\mbx, \mby_0, \mby) = \sigma[r(\mbx, \mby) - r(\mbx, \mby_0)]$ \citep{azar2023general}, but the underlying preference environment need not satisfy this assumption.\footnote{Note that the Bradley Terry assumption suggests not only transitivity but a functional relationship in the preference probabilities, e.g., if $A$ is preferred over $B$ with probability .6 and $B$ is preferred over $C$ with probability $.7$, then the Bradley-Terry assumption states that $A$ must be preferred over $C$ with probability $\approx 0.778$.}

Given the insight that the only evaluation grounded in preference data alone is $h$-win rate with respect to some anchor distribution, we next analyze common preference learning algorithms based on how they relate to optimizing for win rate. In \Cref{sec:framework}, we characterize the space of Win Rate Optimization (WRO) objectives, highlighting existing and new methods that fall under this family, and discuss their theoretical benefits. In \Cref{sec:not_wro}, we analyze instances of non-WRO methods and their limitations.

\section{Preference Learning through the Lens of Win Rate Optimization}
\label{sec:framework}

In this section, we characterize the space of objectives which directly optimize for win rate, discussing both
Win Rate Optimization (WRO) objectives (\Cref{sec:wro_objectives}) and games (\Cref{sec:wro_games}).
Then, we identify the theoretical benefits shared by all methods in the WRO family (\Cref{sec:wro_benefits}). 

\subsection{Win Rate Optimization Objectives}
\label{sec:wro_objectives}
The fact that \Cref{eq:win_rate} is the only grounded evaluation immediately provides an objective ${\mathcal O}_h(\theta)$ to optimize:
\begin{align}\label{eq:wro}
    % \minus\mathcal{L}_\text{WRO}(\theta) =
    &\max_\theta {\mathcal O}_h(\theta) = \max_\theta h\text{-Win Rate}_{p(\mby_0 \g \mbx)}[p_\theta(\mby_1 \g \mbx)] \nonumber \\
    = &\max_\theta \E_{p(\mbx) p_\theta(\mby_1 \g \mbx) p(\mby_0 \g \mbx)}[h \cdot p(\ell = 1 \g \mbx, \mby_0, \mby_1)].
\end{align}
WRO can be optimized by using score function/policy gradients \citep{Mohamed2019MonteCG, weng2018PG}. The solution to any objective in this family is the generator that maximizes the $h$-win rate over a given anchor. 

KL-constrained RLHF is KL-regularized WRO with $h$=logit and the Bradley-Terry assumption to estimate the preference classifier via a reward function \cite{azar2023general}. However, many other WRO instances exist, e.g., for any combination of $h$, anchor distribution, and estimate of $p(\ell = 1 \g \mbx, \mby_0, \mby_1)$ or $\E_{p(\mby_0 \g \mbx)}[h \cdot p(\ell = 1 \g \mbx, \mby_0, \mby_1)]$ yields a different WRO objective.\footnote{For instance, $\E_{p(\mby_0 \g \mbx)}[h \cdot p(\ell = 1 \g \mbx, \mby_0, \mby_1)]$ can estimated by learning the preference distribution $p(\ell = 1 \g \mbx, \mby_0, \mby_1)$ from data and estimating the expectation with samples from the anchor or
training a model to estimate this expectation directly, removing the need to additionally sample from $p(\mby_0 \g \mbx)$ during the policy optimization step. The RLHF setting of WRO in particular (namely, choice of $h$ = logit and BT assumption) also allows the expectation over the anchor $p(\mby_0\g\mbx)$ to be dropped, i.e., $\max_{p(\mby_1\g\mbx)} \E_{p(\mby_1\g\mbx)}\E_{p(\mby_0\g\mbx)}[h \cdot p(\ell=1\g\mbx,\mby_0,\mby_1)] = \max_{p(\mby_1\g\mbx)} \E_{p(\mby_1\g\mbx)}[r(\mbx, \mby)]$, thus requiring only that a reward model be estimated.} 
Different choices of $h$ and anchor yield different optimal solutions in general. When the BT assumption holds, however,
all objectives in \Cref{eq:wro} maximize all $h$-variants of win rate for all anchors.
\begin{proposition}
    (informal) Under the Bradley-Terry assumption, all  objectives in \Cref{eq:wro} with strictly increasing $h$ share the same optimal solution. This solution maximizes win rate overall all possible anchor distributions.
\end{proposition}
\vskip -.1in
See \Cref{sec:reward_max_proof} for proof. 

\subsection{Win Rate Optimization Games}
\label{sec:wro_games}
The choice of anchor need not be a fixed up front. Instead, the anchor distribution could also optimize for its own win rate, resulting in a family of WRO games:
\begin{align*}
    \max_\theta \mathbb{E}_{p(\mbx)} \mathbb{E}_{p_\theta(\mathbf{y}|\mbx)} \mathbb{E}_{p_\gamma(\mathbf{y}'|\mbx)}[h \circ p(\ell=1|\mbx, \mathbf{y}', \mathbf{y})], \\
    \max_\gamma \mathbb{E}_{p(\mathbf{x})} \mathbb{E}_{p_\theta(\mathbf{y}|\mathbf{x})} \mathbb{E}_{p_\gamma(\mathbf{y}'|\mathbf{x})}[h \circ p(\ell=1|\mathbf{x}, \mathbf{y}, \mathbf{y}')].
\end{align*}
Nash Learning from Human Feedback \cite{munos2023nash} is one instance of a WRO game with $h$=identity (and additional KL regularization); the choice of $h$ leads to an
antisymmetric, constant-sum game.
However, WRO games need not be anti-symmetric or constant sum; for instance, a WRO game with $h=\log$ is neither. Different choices of $h$ dictate a different prioritization of preference probabilities to target, and it is not obvious that $h$=identity, the specific setting considered in \citet{munos2023nash}, should be the only WRO game to consider. After all, aligning to preferences is not inherently a constant sum endeavor.

\subsection{Benefits of Win Rate Optimization}
\label{sec:wro_benefits}
Letting $\phi_h(\theta)$ denote the $h$-win rate, $\phi_h(p_\theta(\mby\g\mbx))$, two benefits of WRO objectives $\mathcal{O}_h(\theta)$ are:
\begin{enumerate}
    \itemsep0em
    \item \label{property-1} (Win rate-correspondence): Improving the objective improves $h$-win rate:
    \begin{align*}
        \mathcal{O}(\theta) > \mathcal{O}(\theta') \Rightarrow \phi(\theta) > \phi(\theta').
    \end{align*}
    \item \label{property-2} (Win rate-consistency): 
    The optimum of the objective maximizes $h$-win rate:
    \begin{align*}
        \text{For }\theta^* = \arg\max_\theta \mathcal{O}(\theta):
        \phi(\theta^*) = \max_\theta \phi(\theta).
    \end{align*}
\end{enumerate}
Win rate-correspondence makes gradient-based optimization practical, while win rate-consistency ensures that the loss itself does not exhibit fundamental limitations with respect to its optimal solution.\footnote{For readers interested in the precedence for these properties, win rate-correspondence and consistency are closely linked to Question 2 in \citep{Steinwart2007} and $\mathcal{H}$-consistency \citep{awasthi2021calibration} respectively.}
Both properties follow trivially  for WRO from the fact that $\mathcal{O} = \phi$. 
These properties need not hold for non-WRO (see \Cref{sec:not_wro}). 

\paragraph{Regularization.} 
As $p(\ell = 1 \g \mbx, \mby_0, \mby_1)$ is typically estimated from data,
to prevent overfitting to it one can regularize with 
with a divergence penalty $D(p_\theta,  p_\text{ref})$
with regularization parameter $\beta$:
\begin{equation*}\label{eq:wro_reg}
    \max_\theta h\text{-Win Rate}_{p(\mby_0 \g \mbx)}[p_\theta(\mby_1 \g \mbx)] - \beta \E_{p(\mbx)}D(p_\theta,  p_\text{ref}).
\end{equation*}
Options for $D$ include sequence-level reverse KL, chi-sq, or a sum of token-level divergences~\cite{azar2023general,  huang2024correctingmythosklregularizationdirect,rafailov2024from}.

Optimizing WRO with reverse KL regularization is equivalent to minimizing the reverse KL divergence between the model and the target distribution $p^*_\text{WRO-KL}(\mby \g \mbx) \propto p_\text{ref}(\mby \g \mbx)\exp(\frac{1}{\beta}\mathbb{E}_{p(\mby_0|\mbx)}[h \cdot p(\ell=1|\mbx, \mby_0, \mby)])$; see \citet{azar2023general} or \Cref{sec:wro-kl-target-derivation} for derivation. As such, 
the reverse-KL regularized WRO objective 
is a form of black-box variational inference \citep{ranganath2014black}.

Regularized-WRO $\mathcal{O}_{h, \beta}(\theta)$ does not satisfy win rate-correspondence nor consistency in general but modified versions of both: 1. (regularized win rate-correspondence): Improving the objective implies either an improvement in $h$-win rate or divergence to the reference, i.e., the regularization term; and 2. (regularized win rate-consistency): $\theta^* = \arg\max_\theta \mathcal{O}_{h,\beta}(\theta)$ maximizes $h$-win rate for all distributions with divergence within $M(\beta) = D(p_{\theta^*}, p_\text{ref})$.  

\section{Preference Algorithms that are not WRO}
\label{sec:not_wro}

\subsection{DPO fails win rate-correspondence}
\label{sec:dpo}
Direct Preference Optimization (DPO) \citep{rafailov2024direct} shares the same target distribution as RLHF but employs a different objective to optimize for that target. Denoting $p_\theta(\ell\g\mbx,\mby_0,\mby_1)$ as $p_{\ell, \theta}$ and $q(\mbx,\mby_0,\mby_1)$ as some distribution of offline data, DPO is
\begin{equation}\label{eq:dpo_objective}
    \min_\theta \mathcal{L}_\text{DPO}(\theta) = 
    \min_\theta \minus\mathbb{E}_{q(\mbx,\mby_0,\mby_1)p(\ell\g\mbx,\mby_0,\mby_1)}[\log p_{\ell,\theta}],
\end{equation}
where $p_{\ell,\theta}$ is parametrized to include a language model inside of it. Namely, $p_{\ell,\theta}(\ell=1\g\mbx,\mby_0,\mby_1)$ is
\begin{equation}\label{eq:dpo_param}
    \sigma \Big[\beta \log \frac{p_\theta(\mby_1\g\mbx)}{p_\text{ref}(\mby_1\g\mbx)} - \beta \log \frac{p_\theta(\mby_0\g\mbx)}{p_\text{ref}(\mby_0\g\mbx)}\Big].
\end{equation}

DPO
fails win rate-correspondence, as better (i.e., lower) DPO loss does not necessarily mean better (i.e., higher) win rate. 
This correspondence fails for two reasons:
\begin{enumerate}
    \item Using offline data breaks prevalence-consistency. For instance, a model change that improves the ranking of responses $y_A, y_B$ in the offline data but moves more mass overall to generate dispreferred responses not covered in the offline data will improve DPO loss but hurt win rate. Even sampling from the initial model is not sufficient (see \Cref{sec:dpo_preferences} for an example), as  prevalence-consistency of the generator does not hold throughout training.
    \item Even if we satisfy prevalence-consistency by switching to a fully online setting\footnote{This would consist of sampling from the appropriate query, generator, and anchor distributions throughout training. \citet{guo2024directlanguagemodelalignment,calandriello2024humanalignmentlargelanguage} do this but use $\E_{\text{stop-gradient}(p_\theta(\mby\g\mbx))}$ instead of $\E_{p_\theta(\mby\g\mbx)}$ as a loss; as evaluations both are the same.}, 
    win rate correspondence still does not hold; see \Cref{fig:dpo_mismatch_main} for a visualization.
    This is because the likelihood of preference distribution as in \Cref{eq:dpo_objective} is not an $h$-win rate:
    $\minus\E_{p_\theta(\mbx, \mby_0, \mby_1)}[\E_{p_\ell}\log p_{\ell, \theta}] \neq \E_{p_\theta(\mbx, \mby_0, \mby_1)}[h \cdot p_\ell]$. Since prevalence-consistency holds, preference consistency of 
    this online objective must not hold, which can be seen by noting that the argument inside $\E_{p_\theta(\mbx, \mby_0, \mby_1)}$ is not a fixed function of 
    $p_\ell$.
\end{enumerate}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/skewed_init_skewed_pref_identity.pdf}
    \caption{Plot of win rate vs. DPO loss for different $p_\theta(\mby\g\mbx)$ for a given choice of anchor and preference classifier (see \Cref{sec:dpo_mismatch} for details). Any pair of points that would form a line segment with a positive slope form a setting of $\theta, \theta'$ such that win rate correspondence does not hold. Grey lines denote the win rate and DPO loss at initialization. See \Cref{sec:dpo_mismatch} for additional discussion and examples.}
    \label{fig:dpo_mismatch_main}
\end{figure}

Note that even though DPO is derived to match the target distribution of a regularized-WRO objective,
DPO does not satisfy regularized win rate-correspondence either. Namely, it is possible to improve (i.e., decrease) the DPO loss without improving win rate or divergence to the reference model (see \Cref{sec:dpo_mismatch_reg} for example).

The insight that
DPO fails win rate-correspondence
offers an explanation for empirical results such as the loss vs. win rate misalignment observed in \citet{Chen2024PreferenceLA, rafailov2024scalinglawsrewardmodel}, as well as for why it can helpful to perform very early stopping (e.g., one epoch, far from any evidence of overfitting) rather than model selection with DPO validation loss \citep{rafailov2024direct}, or to choose checkpoints using win rate directly \citep{yuan2024self}. 

The above also applies to direct alignment algorithms (DAA) in general
\cite{azar2023general,pmlr-v235-tang24b,huang2024correctingmythosklregularizationdirect,ji2024towards}: the use of offline data means that prevalence-consistency does not hold, and an inner objective $f(\mbx,\mby_0,
\mby_1)$ in $\E_{q(\mbx,\mby_0,
\mby_1)}f(\mbx,\mby_0,
\mby_1)$ other than the preference classifier (including divergence to the true classifier, distance to some function of the true classifier) means that preference-consistency does not hold.

\paragraph{Takeaways.} Even though DPO and other DAA often share the same optimal solution as a regularized-WRO objective (i.e., regularized-win rate consistency holds), these alternative objectives fail win rate-correspondence, complicating their use in training and model selection.

\subsection{SFT fails win rate-correspondence and consistency}
\label{sec:sft}

Supervised finetuning (SFT)
is often viewed as an initial step for or necessary precursor to other preference learning algorithms \citep{Wang2023EnablingLM,Razin2023VanishingGI}, but it is also a preference learning algorithm itself based on filtering and maximum likelihood estimation.

Supervised finetuning on preferred samples
(sometimes denoted $\mby_w$) 
seeks to maximize the likelihood of sample $\mby_1$ when $\ell=1$ and $\mby_0$ when $\ell=0$. When $p(\mby_1\g\mbx) = p(\mby_0\g\mbx)$, then $p(\mby_w\g \mbx) = p(\mby_1 \g \mbx, \ell=1) = p(\mby_0 \g \mbx, \ell=0)$ and the SFT loss can be written as follows:
\begin{align*}
    \mathcal{L}_{\text{SFT}}(\theta) = \min_\theta \minus\mathbb{E}_{p(\mbx,\mby_0,\mby_1,\ell)} \log p_\theta(\mby_1 \g \mbx, \ell=1).
\end{align*}
Like DPO and direct alignment algorithms, this fails win rate-correspondence, as the above expression is not an $h$-win rate. 
What about win rate consistency?
The target distribution
can be written as
\vskip -.3in
\begin{equation}\label{eq:sft_target}
    p^*_\text{SFT}(\mby\g\mbx) \propto p(\mby\g\mbx)\mathbb{E}_{p(\mby_0\g\mbx)}[p(\ell=1\g\mbx,\mby_0,\mby)],
\end{equation}
which tilts the original distribution towards sequences with higher average preference probabilities over the anchor. However, there are limits to the amount of improvement \Cref{eq:sft_target} can achieve
(see \Cref{sec:target_dist_viz} for visualization).
In fact, we can characterize the exact win rate expected from the SFT target distribution
over the original model:
\begin{theorem}[Win rate improvement of SFT on preferred]\label{thm:sft_gap}
Let $p(\mby_0\g\mbx)$ be the initial generative model, and $p_\text{SFT}(\mby\g\mbx)$ be the target distribution of SFT on preferred samples ($p(\mby_1\g\mbx, \ell=1)$, $p(\mby_0\g\mbx) = p(\mby_1\g\mbx)$). Then, 
\begin{align}
&\text{Win Rate}_{p(\mby_0\g\mbx)}[p_\text{SFT}(\mby\g\mbx)] = 0.5 \nonumber \\
&+ 2 \mathbb{E}_{p(\mathbf{x})}\text{Var}_{p(\mby_1 \g \mbx)}\left[
\E_{p(\mby_0 \g \mbx)}\left[p(\ell = 1 \g \mbx, \mby_0, \mby_1)\right]\right].
\label{eq:sft_gap}
\end{align}
\end{theorem}
The theorem states that
variance in the average preference probability under the initial model dictates the extent of win rate improvement. 
Intuitively, one should not expect any possible improvement in win rate if the existing model only outputs sequences which are equally preferred to each other; on the other hand, there is more room for improvement in win rate the more differentially preferred some of the model's sequences are to others. As long as the model has support over more than two sequences, however, the win rate cannot be maximized. See \Cref{sec:thm2_proof} for proof. 

We can generalize \Cref{thm:sft_gap} to other filtering strategies:

\begin{theorem}[Win rate improvement of filter + SFT]\label{thm:general_gap}
Let $p(f \g \mbx, \mby_1, \mby_0, \ell)$ be a filter that selects data ($f=1$) for supervised finetuning, and $p(\mby_0\g\mbx) = p(\mby_1\g\mbx)$). Then, denoting $\E_{p(\ell, \mby_0 \g \mbx)}\left[p(f = 1 \g \mbx, \mby_0, \mby_1, \ell)\right]$ as $\text{AvgFilter}(\mbx,\mby_1)$ and $\E_{p(\mby_0 \g \mbx)}\left[p(\ell = 1 \g \mbx, \mby_0, \mby_1)\right]$ as $\text{AvgPref}(\mbx,\mby_1)$,
\begin{align}
&\text{Win Rate}_{p(\mby_0\g\mbx)}[p_\text{SFT}(\mby\g\mbx)] = 0.5 \nonumber \\
&+ \mathbb{E}_{p(\mathbf{x})}\frac{\text{Cov}_{p(\mby_1 \g \mbx)}[\text{AvgFilter}(\mbx,\mby_1), \text{AvgPref}(\mbx,\mby_1)]}{\E_{p(\mby_1\g\mbx)}\text{AvgFilter}(\mbx,\mby_1)}.
\label{eq:sft_gap}
\end{align}
\end{theorem}

See \Cref{sec:thm2_proof} for proof. Doing SFT on preferred sets the filter as
$p(f=1 \g \mbx, \mby_1, \mby_0, \ell) = \mathbf{1}[\ell=1]$. %for SFT on preferred. 
%
Win rate increases as 
% $\E_{p(\mby_1\g\mbx)}\text{AvgFilter}(\mbx,\mby_1)$
the denominator of the second term, measuring how often a response is selected,
decreases and/or 
the numerator, measuring how closely the average filter and preference co-vary increases.
In other words, both the threshold for filtering as well as the diversity (in preference)\footnote{Recall that $\text{Cov}_{p(\mby_1 \g \mbx)}[\text{AvgFilter}(\mbx,\mby_1), \text{AvgPref}(\mbx,\mby_1)] \leq \sqrt{\text{Var}_{p(\mby_1 \g \mbx)}[\text{AvgFilter}(\mbx,\mby_1)]\text{Var}_{p(\mby_1 \g \mbx)}[\text{AvgPref}(\mbx,\mby_1)]}$.} of the initial model matter for improvement possible in a filter + SFT algorithm.
This could explain the success of works such as \citet{li2023self}, who use backtranslation to generate a larger set of responses than the model's original generations and filter a subset for SFT, and \citet{brandfonbrener2021quantile}, who filter based on quantile.

\paragraph{Takeaways.} SFT on preferred samples fails win rate-correspondence and consistency. 
The improvement possible 
is a function of the initial model's diversity in preference. Other filtering strategies with stricter criteria can yield further win rate improvements.

\section{Investigating the Empirical Impact of Different Design Choices}
\label{sec:experiments}

We next compare 
WRO and non-WRO
experimentally to complement the above theoretical analysis with an empirical one. First, we compare SFT, DPO, and RLHF on equal footing based on how much they are able to improve win rate over the starting model.
Moreover, we also compare different variants of KL-regularized WRO,
varying $h$ (identity, log, and logit), $\beta$ (1, 0.1, 0.01, 0.001), and the estimation of the preference classifier (perfect and estimated with and without the Bradley Terry assumption). 

\subsection{Experimental Setup}
\label{sec:experimental_setup}

We employ Pythia-2.8b \citep{Biderman2023PythiaAS} as our base model
and the OpenAssistant (OASST) \citep{Kopf2023OpenAssistantC}
and Anthropic Helpfulness and Harmlessness (HH) \citep{bai2022training} as datasets.
We train the base model on the data outputs and use these finetuned models as our initial models.
We train an oracle judge model per dataset to estimate $p(\ell=1 \g \mbx, \mby_0, \mby_1)$ and relabel the preference annotations using this judge model. We use the same oracle to evaluate win rate after training. See \Cref{sec:judge} for further details on the judge model. We additionally train 1. a reward model on the oracle-labeled judge annotations (accuracy is 82.8 for OASST
and 81.36 for HH) and 2. an imperfect judge model (accuracy is 80.47 for OASST and 85.16 for HH). As expected, the BT assumption is helpful for preference classifier estimation in OASST but not in HH, as the OASST directly abides by this assumption (outputs are globally ranked) whereas HH does not explicitly. 
See \Cref{sec:training_appendix} for additional training and evaluation details.

\paragraph{Implementing WRO-KL.}
Variants of WRO-KL can be implemented via different $\psi$ 
in $\max_\theta \mathbb{E}_{p(\mbx)}\mathbb{E}_{p_\theta(\mby\g\mbx)}[\psi(\mbx, \mby) - \beta \log p_\theta(\mby\g\mbx) + \beta\log p_\text{ref}(\mby\g\mbx)]$, e.g., $\psi(\mbx, \mby) = \E_{p(\mby_0\g\mbx)}[p(\ell=1\g\mbx,\mby_0,\mby)]$. We approximate expectations over generator distributions with a single-sample Monte-Carlo estimate. Concretely, for a given query we sample one response each from the current and original model to compute $\psi$, which is either a function of 1. the preference probability under the judge model which takes in the pair as input, or 2. the BT preference probability under the reward model which takes each sequence in as input separately. The one exception is WRO-KL-logit-BT (i.e., RLHF), where we drop $\mathbb{E}_{p(\mby_0|\mbx)}[r(\mbx, \mby_0)]$ and only optimize $r(\mbx, \mby)$ (see \Cref{sec:rlhf_variance} for a comparison between keeping and dropping this term). For optimization, we use the PPO algorithm from the TRL library \citep{vonwerra2022trl}. 

\begin{figure}
% \vspace{-.7cm}
    \centering
    \begin{subfigure}{.45\linewidth}
        \includegraphics[width=\linewidth,trim={0 -5mm 0 -1cm},clip]{figures/prediction.pdf}
        \caption{Expected win rates}
    \end{subfigure}
    \begin{subfigure}{.54\linewidth}
        \includegraphics[width=\linewidth,trim={0 0 0 -1cm}]{figures/reality1.pdf}
        \caption{Actual win rates}
    \end{subfigure}
    \caption{Expected versus observed win rates of RLHF, DPO, and SFT over the original model for OASST. RLHF notably underperforms relative to expectations. Moreover, substituting the learned reward model for the oracle preference classifier (logit win rate (oracle)) does not improve performance, suggesting other factors are more important.}
    \label{fig:comparison}
    \vskip -.1in
\end{figure}
\subsection{Results: Comparing RLHF, DPO, and SFT}
\Cref{fig:comparison} compares methods.
While SFT performance aligns with expectations, as does DPO performance with more regularization, RLHF substantially underperforms relative to expected given the aforementioned analysis, as does DPO with less regularization. The non-monotonic nature of both point to the influence of factors beyond target distribution for win rate improvements. To test if RLHF underperformance is due to error from using an estimated reward model instead of the true preference classifier, we additionally run a WRO-logit variant with oracle preference classifier, but this performs even worse, suggesting another factor at play.
\subsection{Results: Comparing WRO-KL variants}
\label{sec:results}

\Cref{tab:wro_results} compares different WRO-KL objectives with different choices of $h$, $\beta$, and the estimation of the classifier. Notably, no WRO-KL method (i.e., choice of $h$) outperforms the others systematically across settings. Moreover, the settings that correspond to better target distributions (using a perfect preference classifier, low $\beta$) do not necessarily yield better win rates empirically. These results suggest that there is a more important consideration than the target distribution implied by the objective, namely the success of optimization. Indeed, we see that training loss correlates more with test win rate across WRO-KL than any of the target distribution design choices $\hat{p}_\ell, h, \text{ or } \beta$: the p-value of Spearman rank correlation test is 8.27e-5 for train loss vs. win rate, compared to 0.968
and 0.133 for $\hat{p}_\ell
\text{ and } \beta$, and 0.54/0.87/0.87 for all pairs of $h$ choices under Kolmogorov-Smirnov test.
\Cref{fig:win_rate_vs_loss} plots train loss vs. test win rate for all WRO-KL variants. Interestingly, even though one would expect the losses for different WRO variants to differ in scale under different choices in $h$ and $\beta$, there still exists a surprisingly noticeable global trend of better loss corresponding to better win rate.
\begin{table}[]
\caption{Win rate of WRO-KL variants over the reference model. Row corresponding to RLHF is shaded grey. No choice of $\hat{p}_\ell$, $h$, or $\beta$ systematically outperforms all others.}
    \centering
    \resizebox{\columnwidth}{!}{%
\begin{tabular}{lllllll}
    \toprule
          Dataset & $\hat{p}_\ell$ & $h$ &                                    $\beta=$ 0.001 &                                    $\beta=$ 0.01 &                                     $\beta=$ 0.1 &                                       $\beta=$ 1 \\
    \midrule
    HH & non-BT & log &       \cellcolor{salmonred} 38.36 (1.71) &       \cellcolor{lightpink} 46.38 (1.34) &  \cellcolor{lightmintgreen} 63.94 (0.94) &       \cellcolor{mintgreen} 67.00 (0.83) \\
          &         & logit &       \cellcolor{mintgreen} 69.15 (0.91) &       \cellcolor{peachpink} 41.98 (1.45) &       \cellcolor{lightpink} 48.74 (0.97) &       \cellcolor{mintgreen} 69.15 (0.93) \\
          &         & identity &       \cellcolor{mintgreen} 69.94 (0.99) &       \cellcolor{mintgreen} 69.94 (0.99) &       \cellcolor{mintgreen} 65.92 (0.95) &       \cellcolor{peachpink} 43.21 (0.54) \\
          & BT & log &       \cellcolor{salmonred} 35.41 (1.98) &       \cellcolor{salmonred} 33.80 (1.59) &       \cellcolor{peachpink} 41.76 (1.49) &       \cellcolor{lightpink} 49.49 (0.60) \\
          \cellcolor{mylightestgray}  &  \cellcolor{mylightestgray}  & \cellcolor{mylightestgray}logit &       \cellcolor{mintgreen} 70.46 (0.87) &  \cellcolor{lightmintgreen} 63.93 (1.24) &       \cellcolor{mintgreen} 69.44 (0.90) &       \cellcolor{palegreen} 54.60 (0.67) \\
          &         & identity &       \cellcolor{mintgreen} 65.71 (0.95) &  \cellcolor{lightmintgreen} 64.59 (0.95) &       \cellcolor{mintgreen} 69.94 (0.99) &       \cellcolor{palegreen} 51.72 (0.59) \\
          & oracle & log &       \cellcolor{salmonred} 34.98 (1.74) &       \cellcolor{palegreen} 52.74 (0.26) &       \cellcolor{lightpink} 47.66 (1.09) &       \cellcolor{peachpink} 43.29 (0.65) \\
          &         & logit &       \cellcolor{peachpink} 41.99 (1.65) &       \cellcolor{softgreen} 55.13 (1.07) &       \cellcolor{palegreen} 50.28 (1.21) &       \cellcolor{lightpink} 48.55 (0.44) \\
          &         & identity &       \cellcolor{mintgreen} 65.34 (0.93) &       \cellcolor{mintgreen} 66.42 (1.01) &       \cellcolor{mintgreen} 68.39 (0.94) &       \cellcolor{lightpink} 45.14 (0.44) \\
   \hline
    OASST & non-BT & log &       \cellcolor{softgreen} 59.51 (3.88) &       \cellcolor{softgreen} 59.02 (3.96) &  \cellcolor{lightmintgreen} 61.98 (1.40) &       \cellcolor{softgreen} 58.63 (1.98) \\
          &         & logit &  \cellcolor{lightmintgreen} 61.98 (3.53) &       \cellcolor{palegreen} 54.15 (3.20) &       \cellcolor{mintgreen} 65.14 (1.20) &       \cellcolor{lightpink} 48.69 (1.33) \\
          &         & identity &       \cellcolor{softgreen} 56.65 (3.26) &       \cellcolor{palegreen} 54.20 (2.28) &  \cellcolor{lightmintgreen} 64.90 (1.28) &       \cellcolor{palegreen} 53.64 (2.10) \\
          & BT & log &  \cellcolor{lightmintgreen} 63.10 (3.48) &  \cellcolor{lightmintgreen} 62.80 (3.55) &  \cellcolor{lightmintgreen} 60.94 (3.46) &       \cellcolor{palegreen} 54.64 (1.26) \\
          \cellcolor{mylightestgray}& \cellcolor{mylightestgray} & \cellcolor{mylightestgray}logit &  \cellcolor{lightmintgreen} 62.05 (3.65) &  \cellcolor{lightmintgreen} 63.95 (3.38) &       \cellcolor{mintgreen} 66.72 (1.16) &       \cellcolor{softgreen} 58.45 (1.82) \\
          &         & identity &  \cellcolor{lightmintgreen} 62.00 (3.55) &       \cellcolor{palegreen} 51.98 (2.96) &       \cellcolor{palegreen} 54.29 (2.16) &       \cellcolor{palegreen} 50.42 (1.38) \\
          & oracle & log &       \cellcolor{softgreen} 59.32 (3.79) &  \cellcolor{lightmintgreen} 60.19 (3.69) &       \cellcolor{softgreen} 56.09 (1.65) &  \cellcolor{lightmintgreen} 60.09 (1.21) \\
          &         & logit &       \cellcolor{lightpink} 49.55 (3.06) &       \cellcolor{peachpink} 44.34 (2.61) &       \cellcolor{palegreen} 53.66 (2.87) &       \cellcolor{lightpink} 46.15 (1.55) \\
          &         & identity &       \cellcolor{palegreen} 52.13 (2.47) &       \cellcolor{mintgreen} 66.69 (1.29) &       \cellcolor{mintgreen} 66.21 (1.22) &       \cellcolor{palegreen} 52.31 (1.25) \\
    \bottomrule
\end{tabular}
}
    \label{tab:wro_results}
\end{table}


\begin{figure}[t]
    \vskip -.2in
    \centering
\begin{subfigure}[t]{.495\linewidth}
    \includegraphics[width=\linewidth, trim={0 0 0 -1cm}, clip]{figures/hh_loss_vs_win_rate.pdf}
\end{subfigure}
\begin{subfigure}[t]{.495\linewidth}
    \includegraphics[width=\linewidth, trim={0 0 0 -1cm}, clip]{figures/oasst_loss_vs_win_rate.pdf}
\end{subfigure}
    \caption{Train loss vs. test win rate across all settings tested in \Cref{tab:wro_results}. 
    }
    \label{fig:win_rate_vs_loss}
    \vskip -.2in
\end{figure}

\section{Discussion}
\label{sec:discussion}
Amidst the complex landscape of aligning language models to human preferences, this work offers a simplifying insight:
win rate is the only evaluation that can matter based on preference data alone, and thus all of preference learning should be understood in relation to it. This work takes the first steps to do that (see \Cref{sec:summary} for summary).
High level takeaways from the analysis are that WRO is the theoretical ideal (as existing non-WRO fail win rate-correspondence or consistency), but optimization difficulties can lead to its underperformance in practice. Thus, optimization is a first-order factor in the practical success of WRO, while achieving / improving win rate-correspondence and consistency is important for advancing non-WRO.

\paragraph{What does this mean for current practice?} The importance and difficulty of optimization for WRO suggests that strategies to improve optimization success could help, e.g., multiple seeds. The lack of win rate-correspondence in DPO/DAA clarifies the importance of checkpointing with metrics other than loss. And SFT's dependence on variance (in preference) suggests that increasing preference diversity of generations to annotate could yield gains.

\paragraph{What's the value of so many methods?} 
Interestingly, theory and experiments yield opposing trends: based on win rate-correspondence and consistency, RLHF $>$ DPO $>$ SFT; but 
based on ease of optimization, 
SFT $>$ DPO $>$ RLHF.\footnote{SFT $>$ DPO given failures at low $\beta$ and DPO training issues mentioned in \citet{pal2024smaugfixingfailuremodes,Chen2024PreferenceLA}}
The fact that
no existing method is optimal with respect to both the objective being optimized and ease of optimization suggests why it might be useful to combine methods (to take advantage of different strengths) and develop new ones (to strike a better overall balance).

\paragraph{What's next for preference learning?} 
Our analysis suggests that the most important improvements in preference learning will likely fall under the umbrella of moving closer to directly optimizing for win rate, either in the objective itself (i.e., coming up with better surrogates for win rate optimization) or the practical optimization of it (e.g., beyond curent policy gradient approaches). How might we make progress? For one, the connection between RLHF / WRO-KL objectives and variational inference
suggests the possibility of drawing upon the rich field of probabilistic inference to improve preference learning,
from variance reduction techniques \citep{mohamed2020montecarlogradientestimation} to alternative optimization objectives and algorithms altogether \citep{naesseth2021markovianscoreclimbingvariational}. Moreover, the strategies and theoretical tools used to understand and develop surrogate objectives for other machine learning tasks \cite{bridle1990probabilistic, Steinwart2007, Awasthi2022MultiClassH} could provide a useful starting point for developing better objectives for preference learning.

Several concrete directions appear in \cref{sec:framework} and \cref{sec:not_wro} including non-constant sum WRO games to reflect the fact that aligning to huamn preferences need not be a constant-sum endeavor; fully online versions of DAA or contrastive post-training algorithms to help satisfy prevalence consistency (and fewer degrees of freedom for win rate correspondence failures); and alternative filter-plus-SFT style algorithms that take diversity and filter strength into account for solutions with better win rates.
There are probably many more promising ideas to explore---as long as win rate remains the guide. 
% \section{Impact Statement}
% This paper presents work whose goal is to advance methods for aligning generative models (especially language models) to human preferences. There are many potential societal consequences of this work, the most critical being that, as preference learning methodology improves, the question of whose preferences are being represented becomes increasingly important.
\section*{Acknowledgements}
This work was partly supported by the NIH/NHLBI Award R01HL148248, NSF Award 1922658 NRT-HDR:
FUTURE Foundations, Translation, and Responsibility for Data Science, NSF CAREER Award 2145542,
ONR N00014-23-1-2634,  NSF Award 2404476, Apple, Google, and Meta.
\bibliography{example_paper}
\bibliographystyle{icml2025}
\newpage
\onecolumn
\appendix
\section{Summary of theoretical contributions}
\label{sec:summary}
Below in \Cref{tab:summary}, we present a table summarizing the win rate-centric framework introduced in this work, as well as the original source for each insight within the overall framework:
\begin{table}[H]
    \centering
    \begin{tabular}{l|llll}
         & WRO membership & Generalization & Win rate-correspondence & Win rate-consistency \\
         \hline
        RLHF & \checkmark \cite{azar2023general} & $h$-win rate \cite{azar2023general} & \checkmark (\Cref{sec:wro_benefits})\textsuperscript{\textdagger} & \checkmark (\Cref{sec:wro_benefits})\textsuperscript{\textdagger} \\
        NLHF & \checkmark \cite{munos2023nash} &$h$-win rate games (\Cref{sec:wro_games}) & \checkmark (\Cref{sec:wro_benefits})\textsuperscript{\textdagger} & \checkmark (\Cref{sec:wro_benefits})\textsuperscript{\textdagger} \\
        DPO & \xmark (\Cref{sec:dpo}) & direct alignment algorithms (*) & \xmark (\Cref{sec:dpo}) & \checkmark \cite{rafailov2024direct}\textsuperscript{\textdaggerdbl}\\
        SFT & \xmark (\Cref{sec:sft}) & best-of-n, RAFT, ReST (*) & \xmark (\Cref{sec:sft}) & \xmark (\Cref{sec:sft})
    \end{tabular}
    \caption{Summary of the win-rate centric framework introduced in this work. (*) means that the given insight can be attributed to multiple works. Works which generalize DPO and propose alternative direct alignment algorithms: \citet{azar2023general,huang2024correctingmythosklregularizationdirect,pmlr-v235-tang24b,han2024f}. Works that generalize SFT on the preferred sample to more samples include \citet{dong2023raft,gulcehre2023reinforced}. \textsuperscript{\textdagger}RLHF and NLHF typically include a regularization term and satisfy regularized win rate-correspondence and consistency. Without regularization, win rate-correspondence and consistency are satisfied. \textsuperscript{\textdaggerdbl}DPO only satisfies regularized win rate-consistency ($\beta$ must be greater than zero for a meaningful objective) and was constructed specifically with this goal in mind.}
    \label{tab:summary}
\end{table}

\section{Proposition 3.3 proof}
\label{sec:win_rate_proof}
\paragraph{Proposition 3.3.}
\textit{    Under Definition 2, $\phi$ must be
    \begin{align}
        \phi_{p(\mby_0 \g \mbx)}(p(\mby\g \mbx), \mathcal{E}) = \E_{p(\mbx)}\E_{p(\mby \g \mbx)}\E_{p(\mby_0 \g \mbx)}[h \cdot p(\ell = 1 \g \mbx, \mby_0, \mby)]
    \end{align}
    for some choice of $p(\mby_0 \g \mbx)$.}
\begin{proof}
    Property 1 forces $\phi$ to be a functional of the preference classifier, while Property 2 forces the preference classifications to be aggregated linearly across query environment, generator, and anchor. Starting from the base case implied by Property 1, we can define $\phi$ for query, generator, and anchor distributions which are each a linear combination of two Dirac deltas. Let $p(\mbx) = a p_1(\mbx) + b p_2(\mbx)$, $p(\mby\g \mbx) = c_\mbx p_1(\mby\g \mbx) + d_\mbx p_2(\mby\g \mbx)$ for each $\mbx$, and $p(\mby_0\g \mbx) = e_\mbx p_1(\mby_0\g \mbx) + f_\mbx p_2(\mby_0\g \mbx)$; assume all constants yield normalized probability distributions. Then,
    \begin{align}
        \phi_{p(\mby_0 \g \mbx)}(p(\mby\g \mbx), \mathcal{E}) &= a \phi_{p(\mby_0 \g \mbx)}(p(\mby\g \mbx), \mathcal{E}_1) + b \phi_{p(\mby_0 \g \mbx)}(p(\mby\g \mbx), \mathcal{E}_2) \\
        &= a [c_{\mbx_a }\phi_{p(\mby_0 \g \mbx)}(p_1(\mby\g \mbx), \mathcal{E}_1) + d_{\mbx_a} \phi_{p(\mby_0 \g \mbx)}(p_2(\mby\g \mbx), \mathcal{E}_1)] \nonumber\\
        &\;+ b [c_{\mbx_b} \phi_{p(\mby_0 \g \mbx)}(p_1(\mby\g \mbx), \mathcal{E}_2) + d_{\mbx_b} \phi_{p(\mby_0 \g \mbx)}(p_1(\mby\g \mbx), \mathcal{E}_2)] \\
        &= a [c_{\mbx_a }[e_{\mbx_a}\phi_{p_1(\mby_0 \g \mbx)}(p_1(\mby\g \mbx), \mathcal{E}_1) + f_{\mbx_a}\phi_{p_2(\mby_0 \g \mbx)}(p_1(\mby\g \mbx), \mathcal{E}_1)]\nonumber \\
        &\;\;\;+ d_{\mbx_a} [e_{\mbx_a}\phi_{p(\mby_0 \g \mbx)}(p_2(\mby\g \mbx), \mathcal{E}_1) + f_{\mbx_a}\phi_{p(\mby_0 \g \mbx)}(p_2(\mby\g \mbx), \mathcal{E}_2)]] \nonumber\\
        &\;+ b [c_{\mbx_b }[e_{\mbx_b}\phi_{p_1(\mby_0 \g \mbx)}(p_1(\mby\g \mbx), \mathcal{E}_1) + f_{\mbx_b}\phi_{p_2(\mby_0 \g \mbx)}(p_1(\mby\g \mbx), \mathcal{E}_1)] \nonumber \\
        &\;\;\;+ d_{\mbx_b} [e_{\mbx_b}\phi_{p(\mby_0 \g \mbx)}(p_2(\mby\g \mbx), \mathcal{E}_1) + f_{\mbx_b}\phi_{p_2(\mby_0 \g \mbx)}(p_2(\mby\g \mbx), \mathcal{E}_2)]] \nonumber\\
        &= \sum_{i \in \{a, b\}} \sum_{j \in \{c_i, d_i\}}\sum_{k \in \{e_i, f_i\}} i j k (h \cdot \phi(\ell=1 \g \mbx_i, \mby_j, \mby_k)).
    \end{align}
    Generalizing to any discrete distribution for query, generator, and anchor, we have \Cref{eq:win_rate}.
\end{proof}

\section{Proposition 4.1 Proof}
\label{sec:reward_max_proof}

We provide both the informal statement in the main paper as well as its formal version.
\paragraph{Proposition 4.1.} (informal)
    \textit{(informal) Under the Bradley-Terry assumption, all objectives in \Cref{eq:wro} with strictly increasing $h$ share the same optimal solution. This solution maximizes win rate overall all possible anchor distributions.} 

\paragraph{Proposition 4.1.} (formal)
    \textit{Denote by $\mathcal{P}^*_{h}$ the set of distributions $p(\mby\g\mbx)$ that optimize the WRO-$h$ objective in \Cref{eq:wro}. Assume a hypothesis class induced by $\theta \in \Theta$ such that all optima are realizable.
    Then, for a given anchor distribution $p(\mby_0\g\mbx)$, 
    $\mathcal{P}^*_{h} =  \mathcal{P}^*_{h'}$ for any strictly increasing $h$ under the Bradley-Terry assumption with finite rewards. }

\begin{proof}
We first introduce the $\mathcal{P}^*_\text{reward}$, the set of all distributions $p(\mby\g\mbx)$ which for each $\mbx$ place all their probability mass over only the highest-reward sequences or some subset of them. Then, we show that 
$\mathcal{P}^*_\text{reward} = \mathcal{P}^*_h$ for any strictly increasing $h$ and any anchor distribution $p(\mby_0\g\mbx)$. In other words, any maximum-reward distribution $p^*_\text{reward}(\mby\g\mbx) \in \mathcal{P}^*_\text{reward}$ maximizes any WRO objective and vice versa:
\begin{align}
    &\E_{p(\mbx)}\E_{p^*_\text{reward}(\mby\g\mbx)}\E_{p(\mby_0 \g \mbx)}[h \circ  p(\ell = 1 \g \mbx, \mby_0, \mby)] \\
    =&\E_{p(\mbx)}\E_{p^*_\text{reward}(\mby\g\mbx)}\E_{p(\mby_0 \g \mbx)}[h \circ \sigma \left( r(\mbx,\mby) - r(\mbx, \mby_0)\right)] \\
    = &\max_\theta \E_{p(\mbx)}\E_{p_\theta(\mby_1 \g \mbx)}\E_{p(\mby_0 \g \mbx)}[h \circ \sigma \left( r(\mbx,\mby_1) - r(\mbx, \mby_0)\right)] \\
    =&\max_\theta \E_{p(\mbx)}\E_{p_\theta(\mby_1 \g \mbx)}\E_{p(\mby_0 \g \mbx)}[h \circ p(\ell = 1 \g \mbx, \mby_0, \mby_1)].
\end{align}
It follows that $\mathcal{P}^*_h = \mathcal{P}^*_{h'}$ for any strictly increasing $h$.
\end{proof}
\section{Target distribution derivations}
\label{sec:derivations}

\subsection{WRO-KL}
\label{sec:wro-kl-target-derivation}
Here, we show that the WRO-KL objective is equivalent to minimizing the reverse KL divergence of the model and following target distribution:
\begin{align}
    p^*_\text{WRO-KL}(\mby \g \mbx) &\propto p_\text{ref}(\mby \g \mbx)\exp(\frac{1}{\beta}\mathbb{E}_{p(\mby_0|\mbx)}[h \cdot p(\ell=1|\mbx, \mby_0, \mby)]).
\end{align}
Derivation:
\begin{align}
    &\max_\theta \E_{p(\mbx)}\big[\E_{p_\theta(\mby_1 \g \mbx)}\E_{p(\mby_0 \g \mbx)}[h \cdot p(\ell = 1 \g \mbx, \mby_0, \mby_1)] - \beta\text{KL}(p_\theta(\mby \g \mbx) \parallel  p_\text{ref}(\mby \g \mbx))\big] \\
    = &\min_\theta \minus\E_{p(\mbx)}\E_{p_\theta(\mby_1 \g \mbx)}\Bigg[\E_{p(\mby_0 \g \mbx)}[h \cdot p(\ell = 1 \g \mbx, \mby_0, \mby_1)]-\beta\log \frac{p_\theta(\mby \g \mbx)}{p_\text{ref}(\mby \g \mbx)}\Bigg] \\
    = &\min_\theta \E_{p(\mbx)}\E_{p_\theta(\mby_1 \g \mbx)}\Bigg[\log \frac{p_\theta(\mby \g \mbx)}{p_\text{ref}(\mby \g \mbx)}-\frac{1}{\beta}\E_{p(\mby_0 \g \mbx)}[h \cdot p(\ell = 1 \g \mbx, \mby_0, \mby_1)]\Bigg] \\
    = &\min_\theta \E_{p(\mbx)}\E_{p_\theta(\mby_1 \g \mbx)}\Bigg[\log \frac{p_\theta(\mby \g \mbx)}{p_\text{ref}(\mby \g \mbx)\exp(\frac{1}{\beta}\E_{p(\mby_0 \g \mbx)}[h \cdot p(\ell = 1 \g \mbx, \mby_0, \mby_1)])}\Bigg] \\
    = &\min_\theta \E_{p(\mbx)}\E_{p_\theta(\mby_1 \g \mbx)}\Bigg[\log \frac{p_\theta(\mby \g \mbx)}{\frac{1}{Z(\mbx)}p_\text{ref}(\mby \g \mbx)\exp(\frac{1}{\beta}\E_{p(\mby_0 \g \mbx)}[h \cdot p(\ell = 1 \g \mbx, \mby_0, \mby_1)])} - \log Z(\mbx)\Bigg] \\
    = &\min_\theta \mathbb{E}_{p(\mbx)}\big[\text{KL}(p_\theta(\mby\g\mbx) \parallel  p^*_\text{WRO-KL}(\mby \g \mbx))\big], \\
    &p^*_\text{WRO-KL}(\mby \g \mbx) \propto p_\text{ref}(\mby \g \mbx)\exp\Big(\frac{1}{\beta}\mathbb{E}_{p(\mby_0|\mbx)}[h \cdot p(\ell=1|\mbx, \mby_0, \mby)]\Big).
\end{align}


\subsection{RLHF}
\label{sec:rlhf-target-derivation}
Here, we show that, under the BT assumption, the RLHF objective is equivalent to minimizing the reverse KL divergence with the following target distribution:
\begin{align}
      p^*_\text{RLHF}(\mby \g \mbx) &\propto p_\text{ref}(\mby \g \mbx)\exp(\frac{1}{\beta}\mathbb{E}_{p(\mby_0|\mbx)}[\text{logit } p(\ell=1|\mbx, \mby_0, \mby)]).
\end{align}
\begin{align}
&\max_{\theta} \mathbb{E}_{p(\mbx)} \big[\E_{p_\theta(\mby \g \mbx)}  [r(\mbx, \mby)] - \beta\text{KL}(p_\theta(\mby \g \mbx) \parallel  p_\text{ref}(\mby \g \mbx))\big] \\
=& \max_\theta \mathbb{E}_{p(\mbx)} \big[\E_{p_\theta(\mby \g \mbx)} \E_{p(\mby_0 \g \mbx)} [\text{logit }p(\ell=1\g\mbx,\mby_0, \mby)]- \beta\text{KL}(p_\theta(\mby \g \mbx) \parallel  p_\text{ref}(\mby \g \mbx))\big] \\
=& \min_\theta \mathbb{E}_{p(\mbx)} \E_{p_\theta(\mby \g \mbx)} \Bigg[\log\frac{(p_\theta(\mby \g \mbx)}{p_\text{ref}(\mby \g \mbx))} - \frac{1}{\beta}\E_{p(\mby_0 \g \mbx)} [\text{logit }p(\ell=1\g\mbx,\mby_0, \mby)]\Bigg] \\
=& \min_\theta \mathbb{E}_{p(\mbx)}\big[\text{KL}(p_\theta(\mby\g\mbx) \parallel  p^*_\text{RLHF}(\mby \g \mbx))\big],\\
&p^*_\text{RLHF}(\mby\g\mbx)\propto p(\mby\g\mbx)\exp\Big(\frac{1}{\beta} \mathbb{E}_{p(\mby_0|\mbx)}[\text{logit } p(\ell=1|\mbx, \mby_0, \mby)])\Big).   
\end{align}


\section{Comparing target distributions}
\label{sec:target_dist_viz}

In \Cref{fig:sharp}, we present a visualization of the target distributions of different preference learning objectives. Panel (a) shows an example preference environment defined by the true preference probabilities between responses (left) and initial starting model (right). Panels (b), (c), and (d) visualize the resulting target distribution of different objectives. In each, the figure on the left shows how the average preference probabilities $\E_{p(\mby_0\g\mbx)}[p(\ell=1\g\mbx,\mby_0,\mby_1]$ (red dots) are translated into the tilt applied to the starting model, i.e. $g(\mby,\mbx)$ in $p^*(\mby \g\mbx) \propto p(\mby \g\mbx)g(\mby,\mbx)$ (green dots). The figure on the right shows the optimal distribution under the objective. Notably, among the WRO-KL family, the choice of $h$ can make a substantial difference in the target distribution of the objective (panels b vs. c). Moreover, SFT is limited in how much mass it can put on the preferred sample A, as $g(\mby,\mbx)$ is only the average preference probabilities themselves (panel d).

\begin{figure}[t]
    \centering
    \begin{subfigure}{.48\textwidth}
        \includegraphics[width=\linewidth]{figures/fig_sharp_a.pdf}
        \caption{Initial generator and preference probabilities}
    \end{subfigure}
    \begin{subfigure}{.48\textwidth}
        \includegraphics[width=\linewidth]{figures/fig_sharp_b.pdf}
        \caption{WRO-KL, $h$= identity, $\beta=0.1$}
    \end{subfigure}
    \begin{subfigure}{.48\textwidth}

    \end{subfigure}
    \begin{subfigure}{.48\textwidth}
        \includegraphics[width=\linewidth]{figures/fig_sharp_c.pdf}
        \caption{RLHF / DPO, $h$= logit, $\beta=0.1$}
    \end{subfigure}
    \begin{subfigure}{.48\textwidth}
        \includegraphics[width=\linewidth]{figures/fig_sharp_d.pdf}
        \caption{SFT on preferred samples}
    \end{subfigure}
    \caption{
    Different preference learning objectives have different target distributions.
    % A primary difference between these distributions is the extent to which their solutions place more weight on preferred samples. 
    Consider the initial setting in (a). WRO-KL with $h$= identity, $\beta=0.1$ yields the target distribution depicted in (b). RLHF ($h$= logit, $\beta=0.1$) yields a sharper distribution given that average logit probabilities can differ more from each other than average probabilities in (c). SFT yields the least sharp distribution as it can only apply weights between 0 and 1 in (d).
    }
    \label{fig:sharp}
\end{figure}

% \section{Example where DPO does not respect preferences / prevalences}
\section{DPO win rate-correspondence counterexamples}
\label{sec:dpo_preferences}
\subsection{Example where data is sampled from original model}
Consider a query environment $p(\mbx) = \indicator{1}[\mbx=\mbx']$ and a sample space of three responses, $\mby_a, \mby_b, \mby_c$. Under the preference environment of interest, $\mby_a$ is preferred over $\mby_b$ and both are very preferred over $\mby_c$, i.e., $\mby_a \succ \mby_b \succ\succ\succ \mby_c$. Let starting anchor distribution $p_\text{ref}(\mby\g\mbx)$ be supported over $\mby_a$ and $\mby_b$ only, where $p_\text{ref}(\mby_a\g\mbx') > p_\text{ref}(\mby_b\g\mbx')$. The dataset is generated from the initial model itself. 

Consider a $p_\theta(\mby\g\mbx)$ that ranks $\mby_a$ and $\mby_b$ correctly but places most of its mass on $\mby_c$. Then, $\minus\mathcal{L}_{DPO}( p_\theta(\mby\g\mbx))$ will be high, higher than the $\mathcal{L}_\text{DPO}( p_\text{ref}(\mby\g\mbx))$, yet $p_\theta(\mby\g\mbx)$ will generally output a response that is dispreferred over one generated from $p_\text{ref}(\mby\g\mbx)$, i.e., $\mby_c$ vs. $\mby_a$ or $\mby_b$. Note that this example even considers online data (i.e., from the initial model) and no estimation error from finite samples; however, even so, the DPO objective does not respect prevalences from the generator itself, which can also be seen as a failing condition \ref{condition-2} \Cref{condition_2_generator}.

\subsection{Example where data is sampled in a fully online manner}
\label{sec:dpo_mismatch}
We now consider the DPO objective for each under a fully online setting, i.e., one response sampled from the anchor, another sampled from the generator, and the ability to obtain the oracle preference probability for every response pair. We show below that even then, DPO does not obey win rate-correspondence.

Consider a query environment $p(\mbx) = \indicator{1}[\mbx=\mbx']$ and a sample space of three responses, $\mby_a, \mby_b, \mby_c$, where $\mby_a \succ\succ\succ \mby_b \succ \mby_c$ under the preference environment. Let starting anchor distribution $p_\text{ref}(\mby\g\mbx)$ rank the responses as $\mby_b \succ \mby_c \succ\succ\succ \mby_a$, i.e., the rank ordering of $\mby_b$ and  $\mby_c$ is correct, but $\mby_a$ is very unlikely relative to them even though it is most preferred. 
In particular, assume a Bradley-Terry preference classifier that greatly prefers $\mby_a$ to $\mby_b$, i.e., $p(\ell=1\g\mbx,\mby_b,\mby_a) = .9$, and only mildly prefers $\mby_b$ to $\mby_c$, i.e., $p(\ell=1\g\mbx,\mby_c,\mby_b) = .6$. By the BT assumption, $p(\ell=1\g\mbx,\mby_c,\mby_a) \approx .93$. Let the original starting model assign probabilities .1, .5, and .4 respectively to $\mby_a, \mby_b, \mby_c$---notably, it's ranking for pairs with $\mby_a$ is very wrong but its ranking for $\mby_b$ vs. $\mby_c$ is correct. 

This is the setting used to generate the plot in \Cref{fig:dpo_mismatch_main} in the main paper.
We generate points on the plot by sampling $p(\theta(\mby\g\mbx)$ uniformly from the probability simplex. We only plot points whose loss improves upon the initial model (as those are the practical model instances to consider during training), and sample until the plot contains 5000 points.

We now provide a concrete example of $p_\theta(\mby\g\mbx)$ and $q_\theta(\mby\g\mbx)$ where $\minus \mathcal{L}_\text{DPO}(p_\theta) > \minus \mathcal{L}_\text{DPO}(q_\theta)$ yet $\phi(p_\theta) < \phi(q_\theta)$---in words, $p_\theta$ has a better DPO loss but worse win rate than $q_\theta$.
Namely, we construct $p_\theta$ such that it continues to increase the correct probability gap between $\mby_b$ and $\mby_c$ without adding probability mass to $\mby_a$, while we construct $q_\theta$ to improve the probability of $\mby_a$ but at the expense of a worse probability gap between $\mby_b$ and $\mby_c$. As a concrete example, set $p_\theta(\mby_a\g\mbx) = .1$, $p_\theta(\mby_b\g\mbx) = .6$, $p_\theta(\mby_c\g\mbx) = .3$, and  $q_\theta(\mby_a\g\mbx) = .8$, $q_\theta(\mby_b\g\mbx) = .001$, $q_\theta(\mby_c\g\mbx) = .199$. 
The online DPO loss for $p_\theta$ and $q_\theta$ is .51 and .78 respectively. The win rate over the original model, however, is .54 and .67 respectively. The logit-win rate (i.e., $h$=logit in \Cref{eq:win_rate}) is .26 and 1.7 respectively.

The intuition for large mismatch in this example comes from the differential impact to the DPO loss between improving the ranking of the initially incorrect $\mby_a$ vs. $\mby_b$ vs. the initially correct $\mby_b$ vs. $\mby_c$---due to the role of the initial model log probability margins on the loss itself---compared to the differential impact to the win rate between increasing the probability over $\mby_a$ versus $\mby_b$. 

However, this example is just one instance of the mismatch between DPO and win rate; there exist mismatched pairs even when the initial model is uniform over responses, as well as when the BT preference classifier maintains the same probability gap between neighboring examples in the rank order; see \Cref{fig:dpo_mismatch} for a visualization.

\begin{figure}
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \includegraphics[width=\linewidth,trim=0 0 -2cm 0, clip]{figures/uniform_init_skewed_pref_identity.pdf}
        \caption{$p(\ell=1\g\mbx,\mby_b,\mby_a) = .9$, $p(\ell=1\g\mbx,\mby_c,\mby_b) = .6$,\\ $p(\ell=1\g\mbx,\mby_c,\mby_a) \approx .93$. \\$p_0(\mby_a\g\mbx) = p_0(\mby_b\g\mbx) = p_0(\mby_c\g\mbx) = 1/3$.}
    \end{subfigure}
    \begin{subfigure}[t]{.48\textwidth}
        \includegraphics[width=\linewidth,trim=0 0 -2cm 0, clip]{figures/uniform_init_uniform_pref_identity.pdf}
        \caption{$p(\ell=1\g\mbx,\mby_b,\mby_a) = .9$, $p(\ell=1\g\mbx,\mby_c,\mby_b) = .9$, \\$p(\ell=1\g\mbx,\mby_c,\mby_a) \approx .99$. \\$p_0(\mby_a\g\mbx) = p_0(\mby_b\g\mbx) = p_0(\mby_c\g\mbx) = 1/3$.}
    \end{subfigure}
    \caption{Plot of win rate vs. DPO loss for different $p_\theta(\mby\g\mbx)$ under a given setting of Bradley-Terry preference classifier $p(\ell=1\g\mbx,\mby_0,\mby_1)$ and initial and anchor distribution $p_0(\mby\g\mbx)$. These plots are analogous to \Cref{fig:dpo_mismatch_main} but with different experimental settings. Each point on the plot is a different instantiation of $p_\theta(\mby\g\mbx)$, sampled from a Dirichlet with $\alpha=1$ uniformly over all three responses. $\beta=1$. Grey dotted lines are the values of the win rate (horizontal) and DPO loss (vertical) at initialization. Any two pairs of points that would form a line segment with a positive slope is a pair of generator settings $\theta, \theta'$ such that win rate-correspondence does not hold.}
    \label{fig:dpo_mismatch}
\end{figure}

The more fundamental issue is the fact that \Cref{eq:dpo_param} inside the DPO loss is meant to approximate the preference classifier, but this approximation is only correct when the model is exactly the target distribution for the objective. Then, even in the online setting where the expectations consider the correct distributions, the estimate of the preference classifier is incorrect, i.e., $p_\theta(\ell=1\g\mbx,\mby',\mby) \neq p(\ell=1\g\mbx,\mby',\mby)$, meaning the DPO objective is not the same as win rate under the preference environment of interest. Namely,
\begin{align*}
    \minus\mathcal{L}_\text{DPO}(p_\theta(\mby\g\mbx)) &= \mathbb{E}_{p(\mbx)p_\theta(\mby\g\mbx)p_0(\mby'\g\mbx)}[p(\ell=1\g\mbx,\mby',\mby) \log p_\theta(\ell=1\g\mbx,\mby',\mby) + p(\ell=0\g\mbx,\mby',\mby)\log p_\theta(\ell=0\g\mbx,\mby',\mby)] \\
    &\neq \mathbb{E}_{p(\mbx)p_\theta(\mby\g\mbx)p_0(\mby'\g\mbx)}[p(\ell=1\g\mbx,\mby',\mby)] = h\text{-Win Rate}_{p(\mby_0 \g \mbx)}[p_\theta(\mby_1 \g \mbx)].
\end{align*}

\subsection{Example that also fails regularized win rate-correspondence}
\label{sec:dpo_mismatch_reg}
Below is a counterexample to prove that DPO does not obey regularized win rate-correspondence, which is the property that an improvement in the objective implies win rate improvement or a decrease in the divergence to the reference model. 

Consider the setting described in the previous section: query distribution is deterministic with a single prompt $\mbx$, universe of responses to the query are $\{\mby_a,\mby_b,\mby_c\}$, preference classifier follows Bradley-Terry assumption with $p(\ell=1\g\mbx,\mby_a,\mby_b)=.9$ and $p(\ell=1\g\mbx,\mby_b,\mby_c)=.6$, and initial model $p_0(\mby\g\mbx)$ has probabilities .1, .5, and .4 over $\mby_a,\mby_b, \text{ and }\mby_c$ respectively. Let $p_\theta(\mby\g\mbx)$ be and .6, .07, and .33 over $\mby_a,\mby_b, \text{ and }\mby_c$, vs. $q_\theta(\mby\g\mbx)$ be .56, .43, and .01 over $\mby_a,\mby_b, \text{ and }\mby_c$. Then, $\mathcal{L}(p_\theta) \approx .59 < \mathcal{L}(q_\theta) \approx .64$, i.e., $p_\theta$ has better DPO loss. However, $p_\theta$ has both worse win rate and larger divergence from the initial model, with win rate and logit-win rate of .69 and 1.2 (vs. .70 and 1.3 for $q_\theta$) and reverse-KL divergence of .87 (vs. .86 for $q_\theta$). 

The intuition for this counterexample is that  $p_\theta$ has slightly more mass on the best response $\mby_a$ than $q_\theta$ does, but has  $q_\theta$ has much more mass on the second best $\mby_b$ than $p_\theta$, which places the remaining mass on $\mby_c$. In other words, $p_\theta$ and $q_\theta$ have different strengths. In this specific setting, $p_\theta$ gets better DPO loss whereas $q_\theta$ gets better win rate over the original model as the small difference in $\mby_a$ mass matters less than larger mass difference between $\mby_b$ and $\mby_c$. Ultimately, $q_\theta$ is slightly closer to the original model, which had most of its mass on $\mby_b$. 

The broader takeaway is that although DPO has the same target distribution as a regularized WRO objective, the loss is not the same as a linear combination of 
win rate and explicit regularization term, such that it is possible to improve the DPO objective in a way that does not improve win rate or the regularization.
\section{SFT proofs}
\label{sec:thm2_proof}
We provide results for SFT on the preferred sample in a pair, as well as SFT with arbitrary filtering based on a pair and its preference. 

We first provide a more general setting of the theorems in the main paper (\Cref{lemma:gap} and \Cref{lemma:general-gap}); then we specialize to the setting in the main paper, where $p(\mby_0\g\mbx)=p(\mby_1\g\mbx)$, i.e., both samples come from the same distribution.


\begin{lemma}\label{lemma:gap}
Let $p(\mby_0\g\mbx)$ be the initial generative model, and $p_\text{SFT}(\mby\g\mbx)$ be the target distribution of supervised finetuning on preferred samples $\mby_0, \mby_1 \sim p(\mby_0, \mby_1 \g \mbx) = p(\mby_0\g\mbx)p(\mby_1 \g \mbx)$. Then, 
\begin{align}
&\text{Win Rate}_{p(\mby_0\g\mbx)}[p_\text{SFT}(\mby\g\mbx)] =  \\
&\text{Win Rate}_{p(\mby_0\g\mbx)}[p(\mby_1\g\mbx)] + \int p(\mbx) \left[\frac{\text{Variance}_{p(\mby_1 \g \mbx)}\left[\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0)\right]}{\int p(\mby_1' \g \mbx) \int p(\ell = 1 | \mbx, \mby_0'', \mby_1') p(\mby_0 \g \mbx) d\mby_0'' d\mby_1'}
\right]d\mbx.
\end{align}
\end{lemma}
\begin{proof}
    \begin{align}
&\text{Win Rate}_{p(\mby\g\mbx)}[p_\text{SFT}(\mby\g\mbx)] = \int p(\mbx) p(\mby_0 \g \mbx) p_\text{SFT}(\mby\g\mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) \mby_0 \mby_1 d\mbx
\\
=&  \int p(\mbx) p(\mby_0 \g \mbx) 
\frac{p(\mby_1 \g \mbx) \int p(\ell = 1 | \mbx, \mby_0', \mby_1) p(\mby_0' \g \mbx) d\mby_0'}{\int p(\mby_1' \g \mbx) \int p(\ell = 1 | \mbx, \mby_0'', \mby_1') p(\mby_0'' \g \mbx) d\mby_0'' d\mby_1'}
p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0 d\mby_1 d\mbx
\\
=&  \int p(\mbx) \left[p(\mby_0 \g \mbx) 
\frac{p(\mby_1 \g \mbx) \int p(\ell = 1 | \mbx, \mby_0', \mby_1) p(\mby_0' \g \mbx) d\mby_0'}{\int p(\mby_1' \g \mbx) \int p(\ell = 1 | \mbx, \mby_0'', \mby_1') p(\mby_0'' \g \mbx) d\mby_0'' d\mby_1'}
p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0 d\mby_1\right] d\mbx
\\
=&  \int p(\mbx) \left[ \int p(\mby_0 \g \mbx) 
\frac{p(\mby_1 \g \mbx) \int p(\ell = 1 | \mbx, \mby_0', \mby_1) p(\mby_0' \g \mbx) d\mby_0'}{\int p(\mby_1' \g \mbx) \int p(\ell = 1 | \mbx, \mby_0'', \mby_1') p(\mby_0'' \g \mbx) d\mby_0'' d\mby_1'}
p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0 d\mby_1\right] d\mbx
\\
=&  \int p(\mbx) \left[ \int
\frac{\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0  p(\mby_1 \g \mbx) \int p(\ell = 1 | \mbx, \mby_0', \mby_1) p(\mby_0' \g \mbx) d\mby_0'}{\int p(\mby_1' \g \mbx) \int p(\ell = 1 | \mbx, \mby_0'', \mby_1') p(\mby_0 \g \mbx) d\mby_0'' d\mby_1'}
d\mby_1\right] d\mbx
\\
=&  \int p(\mbx) \left[
\frac{ \int(\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0)^2  p(\mby_1 \g \mbx)d\mby_1}{\int p(\mby_1' \g \mbx) \int p(\ell = 1 | \mbx, \mby_0'', \mby_1') p(\mby_0 \g \mbx) d\mby_0'' d\mby_1'}
\right] d\mbx
\\
=& \int p(\mbx) \left[
\frac{ \left(\int(\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0)  p(\mby_1 \g \mbx)d\mby_1\right)^2}{\int p(\mby_1' \g \mbx) \int p(\ell = 1 | \mbx, \mby_0'', \mby_1') p(\mby_0 \g \mbx) d\mby_0'' d\mby_1'}
\right] d\mbx 
\\
&+ \int p(\mbx) \left[\frac{\text{Variance}_{p(\mby_1 \g \mbx)}\left[\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0)\right]}{\int p(\mby_1' \g \mbx) \int p(\ell = 1 | \mbx, \mby_0'', \mby_1') p(\mby_0 \g \mbx) d\mby_0'' d\mby_1'}
\right] d\mbx
\\
=& \int p(\mbx) (\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0)  p(\mby_1 \g \mbx)d\mby_1 d\mbx
\\
&+ \int p(\mbx) \left[\frac{\text{Variance}_{p(\mby_1 \g \mbx)}\left[\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0)\right]}{\int p(\mby_1' \g \mbx) \int p(\ell = 1 | \mbx, \mby_0'', \mby_1') p(\mby_0 \g \mbx) d\mby_0'' d\mby_1'}
\right] d\mbx \\
= &\text{Win Rate}_{p(\mby_0\g\mbx)}[p(\mby_1\g\mbx)] + \int p(\mbx) \left[\frac{\text{Variance}_{p(\mby_1 \g \mbx)}\left[\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0)\right]}{\int p(\mby_1' \g \mbx) \int p(\ell = 1 | \mbx, \mby_0'', \mby_1') p(\mby_0 \g \mbx) d\mby_0'' d\mby_1'}
\right]d\mbx.
\end{align}
\end{proof}



\begin{lemma}\label{lemma:general-gap}
    Let $p(\mby_0\g\mbx)$ be the initial generative model, and $p_\text{filter}(\mby\g\mbx)$ be the target distribution of supervised finetuning on samples where the filter $f=1$ is a potentially randomized data point filter $p(f \g \mbx, \mby_1, \mby_0, \ell)$ that selects data for supervised finetuning based on the any of the context observed $\mbx$, generations $\mby_1, \mby_0$ and  preference $\ell$.
    \begin{align*}
        p_\text{filter}(\mby\g\mbx) = p(\mby_1 \g \mbx, f = 1) = \frac{p(\mby_1 \g \mbx) p(f = 1 \g \mbx, \mby_1)}{p(f = 1 \g \mbx)} 
        = \frac{p(\mby_1 \g \mbx) \E_{p(\ell, \mby_0 \g \mbx, \mby_1)}[p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)]}{\E_{p(\mby_1, \mby_0, \ell \g \mbx)}[ p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)]}
    \end{align*}
     Then, 
    \begin{align*}
    &\text{Win Rate}_{p(\mby_0\g\mbx)}[p_\text{filter}(\mby\g\mbx)] =  \\
    &\text{Win Rate}_{p(\mby_0\g\mbx)}[p(\mby_1\g\mbx)] + \int p(\mbx) \left[ \frac{\text{Cov}_{p(\mby_1 \g \mbx)}(\E_{p(\ell,\mby_0 \g \mbx)}[p(f = 1 \g \mbx, \mby_0, \mby_1)], \E_{p(\mby_0' \g \mbx)}[p(\ell = 1 | \mbx, \mby_0', \mby_1)])}{\E_{p(\mby_1, \mby_0, \ell \g \mbx)}[p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)]} \right] d\mbx.
    \end{align*}
    \end{lemma}
    \begin{proof}
        \begin{align*}
    &\text{Win Rate}_{p(\mby\g\mbx)}[p_\text{filter}(\mby\g\mbx)] = \int p(\mbx) p(\mby_0 \g \mbx) p_\text{filter}(\mby\g\mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) \mby_0 \mby_1 d\mbx
    \\
    =&  \int p(\mbx) p(\mby_0 \g \mbx)
    \frac{p(\mby_1 \g \mbx) \E_{p(\ell, \mby_0 \g \mbx, \mby_1)}[p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)]}{\E_{p(\mby_1, \mby_0, \ell \g \mbx)}[p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)]}
    p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0 d\mby_1 d\mbx
    \\
    =&  \int p(\mbx)\left[\int p(\mby_0 \g \mbx)
    \frac{p(\mby_1 \g \mbx) \E_{p(\ell, \mby_0 \g \mbx, \mby_1)}[p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)]}{\E_{p(\mby_1, \mby_0, \ell \g \mbx)}[p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)]}
    p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0 d\mby_1 \right]d\mbx
    \\
    =&  \int p(\mbx)\left[\int
    \frac{p(\mby_1 \g \mbx) \E_{p(\ell, \mby_0 \g \mbx, \mby_1)}[p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)] \E_{p(\mby_0 \g \mbx)} [p(\ell = 1 \g \mbx, \mby_0, \mby_1)]}{\E_{p(\mby_1, \mby_0, \ell \g \mbx)}[ p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)]}  d\mby_1 \right]d\mbx
    \\
    =&  \int p(\mbx) \left[
    \frac{\E_{p(\mby_1 \g \mbx)}\left[\E_{p(\ell, \mby_0 \g \mbx, \mby_1)}[p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)] \E_{p(\mby_0' \g \mbx)}[p(\ell = 1 | \mbx, \mby_0', \mby_1)]\right]}{\E_{p(\mby_1, \mby_0, \ell \g \mbx)}[p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)]}
    d\mby_1\right] d\mbx
    \\
    =&  \int p(\mbx) \left[ \frac{\text{Cov}_{p(\mby_1 \g \mbx)}(\E_{p(\ell, \mby_0 \g \mbx, \mby_1)}[p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)], \E_{p(\mby_0' \g \mbx)}[p(\ell = 1 | \mbx, \mby_0', \mby_1)])}{\E_{p(\mby_1, \mby_0, \ell \g \mbx)}[p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)]} \right] d\mbx
    \\
    &+ \int p(\mbx) \left[ \frac{\E_{p(\ell, \mby_0, \mby_1 \g \mbx)}[p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)] \E_{p(\mby_1 \mby_0 \g \mbx)}[p(\ell = 1 | \mbx, \mby_0, \mby_1)]}
    {\E_{p(\mby_1, \mby_0, \ell \g \mbx)}[ p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)]}\right] d\mbx
    \\
     =&  \int p(\mbx) \left[ \frac{\text{Cov}_{p(\mby_1 \g \mbx)}(\E_{p(\ell,\mby_0 \g \mbx)}[p(f = 1 \g \mbx, \mby_0, \mby_1)], \E_{p(\mby_0' \g \mbx)}[p(\ell = 1 | \mbx, \mby_0', \mby_1)])}{\E_{p(\mby_1, \mby_0, \ell \g \mbx)}[ p(f = 1 \g \mbx, \mby_1,\mby_0, \ell)]} \right] d\mbx
    + \text{Win Rate}_{p(\mby_0\g\mbx)}[p(\mby_1\g\mbx)]
    \end{align*}
    \end{proof}
For SFT on the preferred sample, $p(f=1 \g \mbx, \mby_1, \mby_0, \ell) = \mathbf{1}[\ell = 1]$. Other options which decrease the denominator in the second term while maintaining covariance (e.g., quantile filtering based on $p(\ell=1\g\mbx,\mby,\mby_1)$) have the potential to enable further win rate improvements.

\paragraph{Theorem 5.1.}
(Win rate improvement of SFT on preferred sample)
\textit{Let $p(\mby_0\g\mbx)$ be the initial generative model, and $p_\text{SFT}(\mby\g\mbx)$ be the target distribution of supervised finetuning on preferred samples ($p(\mby_1\g\mbx, \ell=1)$, $p(\mby_0\g\mbx) = p(\mby_1\g\mbx)$). Then, 
\begin{align}
\text{Win Rate}_{p(\mby_0\g\mbx)}[p_\text{SFT}(\mby\g\mbx)] = 0.5 + 2 \mathbb{E}_{p(\mathbf{x})}\text{Var}_{p(\mby_1 \g \mbx)}\left[\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0\right],\label{eq:sft_gap}
\end{align}
which is less than 1.0 as long as there exist at least three responses in $\text{supp}(p(\mby\g\mbx))$ for any $\mbx$.
}
\begin{proof}
We first use the result of \Cref{lemma:gap} and plug in the condition $p(\mby_0\g\mbx) = p(\mby_1\g\mbx)$:
\begin{align}
&\text{Win Rate}_{p(\mby_0\g\mbx)}[p_\text{SFT}(\mby\g\mbx)] \\
&= 0.5 + \int p(\mbx) \left[\frac{\text{Variance}_{p(\mby_1 \g \mbx)}\left[\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0)\right]}{\int p(\mby_1' \g \mbx) \int p(\ell = 1 | \mbx, \mby_0'', \mby_1') p(\mby_0 \g \mbx) d\mby_0'' d\mby_1'}
\right]d\mbx \\
&= 0.5 + 2 \int p(\mbx) \text{Variance}_{p(\mby_1 \g \mbx)}\left[\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0)\right]d\mbx.
\end{align}

$\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0$ can only take values between 0 and 1. Win rate is maximized for a random variable taking values between zero and one when the variance is .25, but \Cref{prop:max_var} proves that this is unachievable as long as $\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0 \in (0, 1)$ for any $\mby_1$. \Cref{prop:three_responses} shows that $\int p(\mby_0 \g \mbx) p(\ell = 1 \g \mbx, \mby_0, \mby_1) d\mby_0 \in (0, 1)$ as long as there exist three or more elements in $\text{supp}(p(\mby\g\mbx))$. Thus, as long as there exist at least three responses in $\text{supp}(p(\mby\g\mbx))$ for any $\mbx$, win rate ove the initial model is less than 1.
\end{proof}

Note that under deterministic and transitive preferences that are evenly spaced apart,  \Cref{thm:sft_gap} matches the win rate result in Theorem 2 reported in \citep{gui2024bonbonalignmentlargelanguage} for best-of-n when $n=2$. In other words, \Cref{thm:sft_gap} generalizes the best-of-n result to general preference probabilities.
\paragraph{Theorem 5.2.}(Win rate improvement of filter + SFT)
Let $p(f \g \mbx, \mby_1, \mby_0, \ell)$ be a filter that selects data ($f=1$) for supervised finetuning, and $p(\mby_0\g\mbx) = p(\mby_1\g\mbx)$). Then, denoting $\E_{p(\ell, \mby_0 \g \mbx)}\left[p(f = 1 \g \mbx, \mby_0, \mby_1, \ell)\right]$ as $\text{AvgFilter}(\mbx,\mby_1)$ and $\E_{p(\mby_0 \g \mbx)}\left[p(\ell = 1 \g \mbx, \mby_0, \mby_1)\right]$ as $\text{AvgPref}(\mbx,\mby_1)$,
\begin{align}
&\text{Win Rate}_{p(\mby_0\g\mbx)}[p_\text{SFT}(\mby\g\mbx)] = 0.5 
+ \mathbb{E}_{p(\mathbf{x})}\frac{\text{Cov}_{p(\mby_1 \g \mbx)}[\text{AvgFilter}(\mbx,\mby_1), \text{AvgPref}(\mbx,\mby_1)]}{\E_{p(\mby_1\g\mbx)}\text{AvgFilter}(\mbx,\mby_1)}.
\label{eq:sft_gap}
\end{align}
\begin{proof}
    We use the result of \Cref{lemma:general-gap} with the condition $p(\mby_0 \g \mbx) = p(\mby_1 \g \mbx).$
\end{proof}

\section{Additional SFT Propositions}
Below, \Cref{prop:max_var} and \Cref{prop:three_responses} support Theorem 5.1, the result that the win rate improvement for SFT on preferred samples is limited.

\label{sec:sft_max_var}
\begin{proposition}\label{prop:max_var}
    Let $X$ be a random variable over [0, 1]. Then, $\text{Var}(X) < .25$ if $\Pr(0 < X < 1) > 0$.
\end{proposition}
\begin{proof}
First, recall that for convex function $f$, $f(x) = f((1-x) \cdot 0 + x \cdot 1) \leq (1-x)\cdot f(0) + x\cdot f(1)$. Then, letting $f(x) = (x - \mu)^2$, $\mu = \E X$, the variance of a random variable is upper bounded as follows:
\begin{align*}
    \text{Var}(X) &= \mathbb{E}[(X - \mu)^2] \\
     &\leq \mathbb{E}[(1-X)\cdot f(0) + X\cdot f(1)] \\
     &= \mathbb{E}[(1-X)\mu^2 + X(1-\mu)^2] \\
     &=(1-\mu)\mu^2 + \mu(1-\mu)^2 \\
     &=\mu^2 - \mu^3 + \mu - 2\mu^2 + \mu^3 \\
     &= \mu - \mu^2 = \mu(1 - \mu).
\end{align*}
For a distribution over [0, 1], this quantity is maximized when $\mu=.5$, resulting in a variance of .25. This variance is achieved when $X$ places mass on only 0 and 1 symmetrically: $\text{Var}(X) = .5(0 - .5)^2 + .5(1 - .5)^2 = .25$. Any change to this distribution that changes $\mu$ will have a lower upper bound and thus not achieve variance .25, and any other change that does not change $\mu$ but moves probability mass away from the endpoints will only decrease variance.
\end{proof}

Since the win-rate is 
$\text{Win Rate}_{p(\mby_0\g\mbx)}[p_\text{SFT}(\mby\g\mbx)] = 0.5 + 2 \mathbb{E}_{p(\mathbf{x})}\text{Var}_{p(\mby_1 \g \mbx)}\left[
\E_{p(\mby_0 \g \mbx)}\left[p(\ell = 1 \g \mbx, \mby_0, \mby_1)\right]\right]$,
the former proposition establishes that $\E_{p(\mby_0 \g \mbx)}\left[p(\ell = 1 \g \mbx, \mby_0, \mby_1)\right]$ has to be zero or one in order for win rate to be 1.  For a fixed $\mby_1$, this expectation being one implies
\begin{align*}
\E_{p(\mby_0 \g \mbx)}\left[p(\ell = 1 \g \mbx, \mby_0, \mby_1)\right] = 1 \rightarrow p(\ell = 1 \g \mbx, \mby_0, \mby_1) = 1, \{\forall \mby_0 \in A\}, \text{for an $A$ such that } \E_{p(\mby_0 \g \mbx)}[\mathbbm{1}[\mby_0 \in A]]=1
\end{align*}
More plainly, $\mby_1$ must always be preferred over every generation with probability one. By symmetry, the expectation being zero implies that $\mby_1$ must always be dispreferred with probability one over every generation.

Being always dispreferred or preferred is impossible due to self-comparison, e.g., for a fixed $\mby_1$, the expectation over $p(\mby_0\g\mbx)$ will include $\mby_1$ in the support when samples are generated from the same model. 
Omitting self-comparisons, as long as the model has support over more than two generations, $\E_{p(\mby_0 \g \mbx)}\left[p(\ell = 1 \g \mbx, \mby_0, \mby_1)\right]$ must be less than one for at least one $\mby_1 \in \text{supp}(p(\mby_1\g\mbx))$.

\begin{proposition}\label{prop:three_responses}
Let $p(\mby \g \mbx)$ have support on three or more elements. Then there exists a $\beta$ in the support of $p(\mby \g \mbx)$ that is neither always preferred or dispreferred over every element in the support of $p(\mby \g \mbx)$.
\end{proposition}
\begin{proof}

Let $\alpha$ be an element that is deterministically preferred over every element in the support of $p(\mby \g \mbx)$, then by symmetry of $(\ell = 1 \g \mbx, \mby_0, \mby_1)$ no other element can be deterministically preferred over every other element.  Preferences for $\alpha$: $p(\ell= 1 \g \mbx, \mby_1 = \alpha, \mby_0=\beta) = 1$ and $p(\ell= 1 \g \mbx, \mby_1 = \alpha, \mby_0=\gamma) = 1$. 


Then let $\gamma$ be an element that is deterministically dispreferred over every element in the support of $p(\mby \g \mbx)$. Preferences for $\gamma$: $p(\ell= 1 \g \mbx, \mby_1 = \gamma, \mby_0=\alpha) = 0$ and $p(\ell= 1 \g \mbx, \mby_1 = \gamma, \mby_0=\beta) = 0$. 
The preference probabilities for $\beta$ are fixed by symmetry from the above assumptions
    \begin{itemize}
        \item Since $p(\ell= 1 \g \mbx, \mby_1 = \alpha, \mby_0=\beta) = 1$, then $p(\ell= 1 \g \mbx, \mby_1 = \alpha, \mby_1=\gamma) = 0$
        \item Since $p(\ell= 1 \g \mbx, \mby_1 = \gamma, \mby_0=\beta) = 0$, $p(\ell= 1 \g \mbx, \mby_1 = \beta, \mby_0=\gamma) = 1$ 
    \end{itemize}
    Thus, the $\beta$ generation beta is not deterministically preferred or dispreferred.
\end{proof}
Since being always determinstically preferred or dispreferred is generally not possible, SFT's improvement is limited.

\section{Expected Win Rate Improvement Expressions}
\label{sec:gaps}
Below, we present the expected win rate improvement over the original model for WRO-KL objectives. Letting $p(\mby_0 \g \mbx) = p(\mby'_0 \g \mbx) = p(\mby''_0 \g \mbx)$ and $p(\mby_1 \g \mbx) = p(\mby'_1 \g \mbx)$, we have:
\begin{align}
\begin{aligned}
&\text{Win Rate}_{p(\mby_0\g\mbx)}[p_\text{WRO-KL}(\mby\g\mbx)] \\ = &\int p(\mbx) \left[\frac{\mathbb{E}_{p(\mby_1 \g \mbx)}\left[\E_{p(\mby_0 \g \mbx)} p(\ell = 1 \g \mbx, \mby_0, \mby_1) \exp\Big(\frac{1}{\beta}\E_{p(\mby'_0 \g \mbx)} h \cdot p(\ell = 1 \g \mbx, \mby'_0, \mby_1)\Big)\right]}{\E_{p(\mby_1' \g \mbx)} \exp\Big(\frac{1}{\beta}\E_{p(\mby''_0 \g \mbx)} h \cdot p(\ell = 1 \g \mbx, \mby''_0, \mby'_1)\Big)}
\right]d\mbx.\label{eq:wro_wr}
\end{aligned}
\end{align}
% \end{theorem}

Consequently,  assuming the BT assumption holds in the preference environment, the expected win rate improvement for the target of RLHF/DPO is as follows:
\begin{align}
\begin{aligned}
&\text{Win Rate}_{p(\mby_0\g\mbx)}[p_\text{RLHF/DPO}(\mby\g\mbx)] \\ = &\int p(\mbx) \left[\frac{\mathbb{E}_{p(\mby_1 \g \mbx)}\left[\E_{p(\mby_0 \g \mbx)} p(\ell = 1 \g \mbx, \mby_0, \mby_1) \exp\Big(\frac{1}{\beta}\E_{p(\mby'_0 \g \mbx)} [\text{logit }p(\ell = 1 \g \mbx, \mby'_0, \mby_1)]\Big)\right]}{\E_{p(\mby_1' \g \mbx)} \exp\Big(\frac{1}{\beta}\E_{p(\mby''_0 \g \mbx)} [\text{logit } p(\ell = 1 \g \mbx, \mby''_0, \mby'_1)]\Big)}
\right]d\mbx.
\end{aligned}
\end{align}

For completion, we write the expected win rate improvement for SFT in the same form, connecting the expressions in this section to the result of \Cref{thm:sft_gap}:
\begin{align}
    \begin{aligned}
&\text{Win Rate}_{p(\mby_0\g\mbx)}[p_\text{SFT}(\mby\g\mbx)] \\ = &\int p(\mbx) \left[\frac{\mathbb{E}_{p(\mby_1 \g \mbx)}\left[\E_{p(\mby_0 \g \mbx)} p(\ell = 1 \g \mbx, \mby_0, \mby_1) \E_{p(\mby'_0 \g \mbx)} [p(\ell = 1 \g \mbx, \mby'_0, \mby_1)]\right]}{\E_{p(\mby_1' \g \mbx)} \E_{p(\mby''_0 \g \mbx)} [p(\ell = 1 \g \mbx, \mby''_0, \mby'_1)]}
\right]d\mbx.
\end{aligned}
\end{align}
\section{Comparing methods}

In \Cref{tab:overall}, we compare various preference learning methods along the axes of target distribution, objective, and the method of estimating the preference classifier. Ideally, we want a method's target solution is able to put more probability mass over the more preferred sequences; its objective directly seeks to approximate that target over some other goal; and its estimate of the preference classifier is as accurate as possible.

\begin{table}[H]
    \centering
    \caption{Comparison of preference learning algorithms along three dimensions of design choices: target distribution, objective, and estimation of the preference classifier. For readability, we substitute the preference classifier distribution $p(\ell=1|\mbx, \mby_0, \mby)$ with $p_\text{clf}$.}
    \begin{tabular}{rlll}
    \toprule
         & Target (unnormalized) & Objective (per-query) & Preference Classifier \\
         \midrule
         WRO-KL & $p(\mby\g\mbx)\exp(\frac{1}{\beta}\mathbb{E}_{p(\mby_0|\mbx)}[h \cdot p_\text{clf}])$ & $\text{KL}(p_\theta(\mby\g\mbx) \parallel   p^*(\mby \g \mbx))$ & $\hat{p}_\text{clf}$ \\
         RLHF & $p(\mby\g\mbx)\exp(\frac{1}{\beta}\mathbb{E}_{p(\mby_0|\mbx)}[\text{logit }p_\text{clf}])$ & $\text{KL}(p_\theta(\mby\g\mbx) \parallel  p^*(\mby \g \mbx))$ & $\hat{r}(\mbx, \mby)$, BT \\
         DPO & $p(\mby\g\mbx)\exp(\frac{1}{\beta}\mathbb{E}_{p(\mby_0|\mbx)}[\text{logit }p_\text{clf}])$ & $\text{KL}(p^*_\text{clf}\parallel p_{\theta, \text{clf}})$ & \Cref{eq:dpo_param}, BT \\
         SFT & $p(\mby\g\mbx)\mathbb{E}_{p(\mby_0|\mbx)}[p_\text{clf}]$ & $\text{KL}(p^*(\mby\g\mbx) \parallel  p_\theta(\mby \g \mbx))$ & Bypass \\
         \bottomrule
    \end{tabular}
    \label{tab:overall}
\end{table}


\section{Judge model}
\label{sec:judge}

To train the judge model, we finetune a Pythia-2.8b base model using the same pairwise training set used to train all SFT models by simply modifying the prompt to take in $\mbx, \mby_0, \mby_1$ into account. The prompt template is as follows: 

\indent \texttt{\textbackslash n\textbackslash n Human: + <Instruction> + \\\textbackslash n\textbackslash n Candidate Response A: + <response a> + 
\\\textbackslash n\textbackslash n Candidate Response B: + <response b> + \\
\textbackslash n\textbackslash n The better answer is Candidate Response }

The model is trained with a language modeling loss on the output, which is either `A` or `B`. For each pair of responses $\mby_0, \mby_1$, the training set includes two rows, one where $\mby_0$ is Response A, and one where $\mby_1$ is Response A. 

The model is trained using RMSProp with a learning rate of 5e-7 and batch size of 64. The model is trained with a maximum sequence length of 512 and a maximum input length of 511. On the evaluation dataset, the model achieves a per-row classification accuracy of 68.8.
% from win_rates2
(Training the same judge model with a  sequence length of 1024 achieves the same accuracy, so we choose to stick with 512 for efficiency.)
% (This setting achieves similar results as setting sequence length to  1024, so we choose to use the former for efficiency.)

To obtain the preference probability of a pair of outputs, we run a forward pass through the judge model twice, once with each order of output pairs, and average the results.
% and score each pair of outputs with a binary \{0, 1\} score denoting whether the output from the distribution of interest is preferred over that of the anchor.

To simulate more opinionated preferences in this preference environment, we sharpen the judge preference probabilities with temperature scaling ($T=0.2$) on the logit-transformed probabilities:
    $p(\ell=1\g\mbx,\mby_0,\mby_1) = \sigma\left(\text{logit }\hat{p}_\text{judge}(\ell=1\g\mbx,\mby_0,\mby_1) / T \right).$

% All evaluations use a held-out set of 100 prompts.

\section{Experiment details}
\label{sec:training_appendix}

\paragraph{Dataset processing.} For the Open Assistant dataset \citep{Kopf2023OpenAssistantC}, we keep only the first turn in each conversation and English-only examples, following \citet{yuan2024self}. The dataset only has a train and validation split, so we split the original train set into a train and validation set and leave the validation set for testing / evaluation. The dataset includes multiple candidate responses for each input, all ranked, and to match the pairwise preference learning setup, we create a dataset of all possible pairs for each input. For instance, for a given input with three candidate responses (A, B, C), our paired dataset includes all three pairs (AB, BC, AC). For SHP, we keep only pairs where the score ratio is $> 2$, following \citet{rafailov2024direct}.

\paragraph{Training the initial model.} For each dataset, we finetune the base Pythia-2.8b models on all outputs, preferred and dispreferred. The resulting finetuned models serve as our initial models for preference learning. To train these models, we utilize a batch size of 64 and learning rate of 5e-7 chosen based on hyperparameter sweep between [1e-8, 5e-8, 1e-7, 5e-7, 1e-6] on OASST. Following \citet{rafailov2024direct}, we use the RMSProp optimizer with a learning rate warm up of 150 steps and constant learning rate schedule otherwise. We evaluate every 100 steps and choose the best checkpoint based on validation loss.

\paragraph{SFT and DPO experiments.} 
SFT and DPO experiments follow the same training configuration as the initial model.

\paragraph{RL experiments.} 
We use the implementation of reward model training and PPO from the TRL library \citep{vonwerra2022trl}. For reward model training, we use a batch size of 64, learning rate of 5e-7 for Pythia2.8b, and checkpoint every 100 steps, matching the SFT and DPO experiments. For PPO, we use a learning rate=1e-6 (obtained through a hyperparameter sweep of [1e-7, 5e-7, 1e-6] on OASST), batch size=128, and PPOConfig defaults for all other hyperparameters. We checkpoint every five steps and choose checkpoint with the best policy loss (namely, ignoring the estimation of the value head). 

\paragraph{Win rate evaluations.}
We sample a set of 100 input prompts from the test set of a given dataset (same 100 prompts for all models) and perform win rate evaluation using the oracle judge for the dataset. 

\section{Exploring optimization benefits of RLHF}
\label{sec:rlhf_variance}

Below, we compare the win rate results when optimizing the RLHF objective versus a non-optimized WRO-KL-BT-logit objective that keeps the $\mathbb{E}_{p(\mby_0\g\mbx)}r(\mbx, \mby_0)$ term. Namely, the RLHF objective we optimize is
\begin{align}
    \minus\mathcal{L}_\text{RLHF}(\theta) =\max_{\theta} \mathbb{E}_{p(\mbx)} \big[\E_{p_\theta(\mby \g \mbx)}  [r(\mbx, \mby)] - \beta\text{KL}(p_\theta(\mby \g \mbx) \parallel  p_\text{ref}(\mby \g \mbx))\big].
\end{align}

The non-optimized WRO-KL-BT-logit objective we optimize is
\begin{align}
    \minus\mathcal{L}_\text{RLHF}(\theta) =\max_{\theta} \mathbb{E}_{p(\mbx)} \big[\E_{p_\theta(\mby \g \mbx)}  [r(\mbx, \mby) - \mathbb{E}_{p(\mby_0\g\mbx)}r(\mbx, \mby_0)] - \beta\text{KL}(p_\theta(\mby \g \mbx) \parallel  p_\text{ref}(\mby \g \mbx))\big].
\end{align}

Win rate results can be found in \Cref{tab:rlhf_variance}. All experimental details match that of the main paper.

\begin{table}[h]
    \centering
    \caption{Comparison of win rates between RLHF (optimized) and WRO-KL-BT-logit without additional optimization (non-optimized). RLHF's dropping of the constant in the objective decreases variance in the gradient estimates and generally improves results with small enough $\beta$. However, this change does not yield systematic benefits for every setting, suggesting that there is still room for improvement.}
    \begin{tabular}{llllll}
    \toprule
          & &         $\beta=0.001$ &          $\beta=0.01$ &           $\beta=0.1$ &             $\beta=1$ \\
    Dataset &  &               &               &               &               \\
    \midrule
    HH & non-optimized &  51.79 (1.12) &  54.59 (0.90) &  68.68 (0.80) &  \textbf{56.88 (0.46) }\\
          & optimized &  \textbf{70.46 (0.87)} &  \textbf{63.93 (1.24)} &  69.44 (0.90) &  54.60 (0.67) \\
    OASST & non-optimized &  60.74 (3.65) &  61.79 (3.64) &  \textbf{73.11 (1.16)} &  60.41 (2.05) \\
          & optimized &  62.05 (3.65) &  63.95 (3.38) &  66.72 (1.16) &  58.45 (1.82) \\
    \bottomrule
\end{tabular}
    \label{tab:rlhf_variance}
\end{table}

\end{document}
