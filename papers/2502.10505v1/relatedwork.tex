\section{Related Work}
\label{sec:related}
Our work is most closely related to previous work in win rate evaluation and optimization as well as analysis of RLHF and preference learning objectives.

\paragraph{Win rate evaluation and optimization.} 
Win rate is already a central evaluation in preference learning \citep{alpaca_eval,zheng2024judging}; however, our work goes further and proves that it is the only evaluation grounded in the sampling distribution itself, thus motivating its use as the central object to understand the rest of the preference learning landscape.
Several works have proposed methods that perform some form of win rate optimization \citep{munos2023nash, swamy2024minimaximalistapproachreinforcementlearning, rosset2024directnashoptimizationteaching}, including as a minimax optimization with a dynamic (rather than fixed) opponent model. Our work provides a more general framework to characterize the landscape of win rate optimization (WRO) methods beyond the specific instances that currently exist in the literature. Moreover, our analysis points to the theoretical merit of these approaches, while our experiments demonstrate the implementation bottlenecks of them.


\paragraph{Analyzing RLHF, DPO, and other preference learning methods.}
Our work is related to work that seeks to 
% Many works seek to improve upon or 
better understand RLHF, DPO, and other existing methods in preference learning (e.g., best-of-n). 
Examples include benchmarking generalization and diversity \citep{kirk2024understanding}, comparing on- vs off-policy approaches \citep{tajwar2024preferencefinetuningllmsleverage}, investigating length bias \citep{singhal2024longwaygoinvestigating}, and disentangling design choices empirically \citep{ivison2024unpackingdpoppodisentangling}. 
For RLHF in particular, 
existing works consider
the complexity of proximal policy optimization
\citep{ahmadian2024back},
vanishing gradients \citep{Razin2023VanishingGI},
reward model overoptimization \citep{zhu2024iterative}, or
limitations of the Bradley-Terry assumption to relate preferences to rewards
\citep{wang2024transforming,azar2023general,munos2023nash}.
For DPO, there exists not only large space of alternative direct alignment algorithms (e.g., \citep{zhao2023slichf,azar2023general, xu2024contrastivepreferenceoptimizationpushing,  huang2024correctingmythosklregularizationdirect,pal2024smaugfixingfailuremodes,xu2024thingscringeothersiterative}) but also works which analyze its limited ability to flip rankings 
\citep{Chen2024PreferenceLA} or the decrease in chosen and rejected log probabilities \citep{razin2024unintentionalunalignmentlikelihooddisplacement}.
\citet{gui2024bonbonalignmentlargelanguage} show that the target distribution of best-of-n for different choices of $n$ yields a win rate vs. KL divergence curve close to that of RLHF's for different choices of divergence constraint; \Cref{thm:sft_gap} of this work generalizes their win rate result for best-of-2 from a deterministic to an arbitrary preference environment.
\citet{azar2023general} present a family of $\Psi$-Preference Optimization objectives,
show that RLHF falls within this family, and propose Identity Preference Optimization (IPO); this work presents a larger family of WRO objectives beyond $\Psi$-Preference Optimization and categorizes $\Psi$-Preference Optimization and IPO separately into WRO and non-WRO objectives, highlighting the 
theoretical benefits of the former and theoretical limitations of the latter.