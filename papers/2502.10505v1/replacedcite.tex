\section{Related Work}
\label{sec:related}
Our work is most closely related to previous work in win rate evaluation and optimization as well as analysis of RLHF and preference learning objectives.

\paragraph{Win rate evaluation and optimization.} 
Win rate is already a central evaluation in preference learning ____; however, our work goes further and proves that it is the only evaluation grounded in the sampling distribution itself, thus motivating its use as the central object to understand the rest of the preference learning landscape.
Several works have proposed methods that perform some form of win rate optimization ____, including as a minimax optimization with a dynamic (rather than fixed) opponent model. Our work provides a more general framework to characterize the landscape of win rate optimization (WRO) methods beyond the specific instances that currently exist in the literature. Moreover, our analysis points to the theoretical merit of these approaches, while our experiments demonstrate the implementation bottlenecks of them.


\paragraph{Analyzing RLHF, DPO, and other preference learning methods.}
Our work is related to work that seeks to 
% Many works seek to improve upon or 
better understand RLHF, DPO, and other existing methods in preference learning (e.g., best-of-n). 
Examples include benchmarking generalization and diversity ____, comparing on- vs off-policy approaches ____, investigating length bias ____, and disentangling design choices empirically ____. 
For RLHF in particular, 
existing works consider
the complexity of proximal policy optimization
____,
vanishing gradients ____,
reward model overoptimization ____, or
limitations of the Bradley-Terry assumption to relate preferences to rewards
____.
For DPO, there exists not only large space of alternative direct alignment algorithms (e.g., ____) but also works which analyze its limited ability to flip rankings 
____ or the decrease in chosen and rejected log probabilities ____.
____ show that the target distribution of best-of-n for different choices of $n$ yields a win rate vs. KL divergence curve close to that of RLHF's for different choices of divergence constraint; \Cref{thm:sft_gap} of this work generalizes their win rate result for best-of-2 from a deterministic to an arbitrary preference environment.
____ present a family of $\Psi$-Preference Optimization objectives,
show that RLHF falls within this family, and propose Identity Preference Optimization (IPO); this work presents a larger family of WRO objectives beyond $\Psi$-Preference Optimization and categorizes $\Psi$-Preference Optimization and IPO separately into WRO and non-WRO objectives, highlighting the 
theoretical benefits of the former and theoretical limitations of the latter.