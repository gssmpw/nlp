[
  {
    "index": 0,
    "papers": [
      {
        "key": "alpaca_eval",
        "author": "Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto ",
        "title": "AlpacaEval: An Automatic Evaluator of Instruction-following Models"
      },
      {
        "key": "zheng2024judging",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",
        "title": "Judging llm-as-a-judge with mt-bench and chatbot arena"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "munos2023nash",
        "author": "R\u00e9mi Munos and Michal Valko and Daniele Calandriello and Mohammad Gheshlaghi Azar and Mark Rowland and Zhaohan Daniel Guo and Yunhao Tang and Matthieu Geist and Thomas Mesnard and Andrea Michi and Marco Selvi and Sertan Girgin and Nikola Momchev and Olivier Bachem and Daniel J. Mankowitz and Doina Precup and Bilal Piot",
        "title": "Nash Learning from Human Feedback"
      },
      {
        "key": "swamy2024minimaximalistapproachreinforcementlearning",
        "author": "Gokul Swamy and Christoph Dann and Rahul Kidambi and Zhiwei Steven Wu and Alekh Agarwal",
        "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback"
      },
      {
        "key": "rosset2024directnashoptimizationteaching",
        "author": "Corby Rosset and Ching-An Cheng and Arindam Mitra and Michael Santacroce and Ahmed Awadallah and Tengyang Xie",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "kirk2024understanding",
        "author": "Robert Kirk and Ishita Mediratta and Christoforos Nalmpantis and Jelena Luketina and Eric Hambro and Edward Grefenstette and Roberta Raileanu",
        "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "tajwar2024preferencefinetuningllmsleverage",
        "author": "Fahim Tajwar and Anikait Singh and Archit Sharma and Rafael Rafailov and Jeff Schneider and Tengyang Xie and Stefano Ermon and Chelsea Finn and Aviral Kumar",
        "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "singhal2024longwaygoinvestigating",
        "author": "Prasann Singhal and Tanya Goyal and Jiacheng Xu and Greg Durrett",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "ivison2024unpackingdpoppodisentangling",
        "author": "Hamish Ivison and Yizhong Wang and Jiacheng Liu and Zeqiu Wu and Valentina Pyatkin and Nathan Lambert and Noah A. Smith and Yejin Choi and Hannaneh Hajishirzi",
        "title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ahmadian2024back",
        "author": "Ahmadian, Arash and Cremer, Chris and Gall{\\'e}, Matthias and Fadaee, Marzieh and Kreutzer, Julia and {\\\"U}st{\\\"u}n, Ahmet and Hooker, Sara",
        "title": "Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "Razin2023VanishingGI",
        "author": "Noam Razin and Hattie Zhou and Omid Saremi and Vimal Thilak and Arwen Bradley and Preetum Nakkiran and Josh Susskind and Etai Littwin",
        "title": "Vanishing Gradients in Reinforcement Finetuning of Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhu2024iterative",
        "author": "Banghua Zhu and Michael I. Jordan and Jiantao Jiao",
        "title": "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wang2024transforming",
        "author": "Wang, Zihao and Nagpal, Chirag and Berant, Jonathan and Eisenstein, Jacob and D'Amour, Alex and Koyejo, Sanmi and Veitch, Victor",
        "title": "Transforming and Combining Rewards for Aligning Large Language Models"
      },
      {
        "key": "azar2023general",
        "author": "Mohammad Gheshlaghi Azar and Mark Rowland and Bilal Piot and Daniel Guo and Daniele Calandriello and Michal Valko and R\u00e9mi Munos",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "key": "munos2023nash",
        "author": "R\u00e9mi Munos and Michal Valko and Daniele Calandriello and Mohammad Gheshlaghi Azar and Mark Rowland and Zhaohan Daniel Guo and Yunhao Tang and Matthieu Geist and Thomas Mesnard and Andrea Michi and Marco Selvi and Sertan Girgin and Nikola Momchev and Olivier Bachem and Daniel J. Mankowitz and Doina Precup and Bilal Piot",
        "title": "Nash Learning from Human Feedback"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhao2023slichf",
        "author": "Yao Zhao and Rishabh Joshi and Tianqi Liu and Misha Khalman and Mohammad Saleh and Peter J. Liu",
        "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"
      },
      {
        "key": "azar2023general",
        "author": "Mohammad Gheshlaghi Azar and Mark Rowland and Bilal Piot and Daniel Guo and Daniele Calandriello and Michal Valko and R\u00e9mi Munos",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      },
      {
        "key": "xu2024contrastivepreferenceoptimizationpushing",
        "author": "Haoran Xu and Amr Sharaf and Yunmo Chen and Weiting Tan and Lingfeng Shen and Benjamin Van Durme and Kenton Murray and Young Jin Kim",
        "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation"
      },
      {
        "key": "huang2024correctingmythosklregularizationdirect",
        "author": "Audrey Huang and Wenhao Zhan and Tengyang Xie and Jason D. Lee and Wen Sun and Akshay Krishnamurthy and Dylan J. Foster",
        "title": "Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization"
      },
      {
        "key": "pal2024smaugfixingfailuremodes",
        "author": "Arka Pal and Deep Karkhanis and Samuel Dooley and Manley Roberts and Siddartha Naidu and Colin White",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive"
      },
      {
        "key": "xu2024thingscringeothersiterative",
        "author": "Jing Xu and Andrew Lee and Sainbayar Sukhbaatar and Jason Weston",
        "title": "Some things are more CRINGE than others: Iterative Preference Optimization with the Pairwise Cringe Loss"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "Chen2024PreferenceLA",
        "author": "Angelica Chen and Sadhika Malladi and Lily H. Zhang and Xinyi Chen and Qiuyi Zhang and Rajesh Ranganath and Kyunghyun Cho",
        "title": "Preference Learning Algorithms Do Not Learn Preference Rankings"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "razin2024unintentionalunalignmentlikelihooddisplacement",
        "author": "Noam Razin and Sadhika Malladi and Adithya Bhaskar and Danqi Chen and Sanjeev Arora and Boris Hanin",
        "title": "Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "gui2024bonbonalignmentlargelanguage",
        "author": "Lin Gui and Cristina G\u00e2rbacea and Victor Veitch",
        "title": "BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "azar2023general",
        "author": "Mohammad Gheshlaghi Azar and Mark Rowland and Bilal Piot and Daniel Guo and Daniele Calandriello and Michal Valko and R\u00e9mi Munos",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      }
    ]
  }
]