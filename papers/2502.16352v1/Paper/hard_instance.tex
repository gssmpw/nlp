%\subsection{Hard Small Margin Instances}\label{sec:hard}

\begin{lemma}[Constructing Skew-Obtuse Family of Vectors when margin $\gamma\leq 1/3$]
\label{lemma:constructing_skob_family}
    For any $\gamma \leq 1/3$, there exists an instance that can be separated by a linear classifier with margin $\gamma$ such that all negatives must be revealed to achieve recall $1$. 
\end{lemma}

The proof of Lemma~\ref{lemma:constructing_skob_family} is in Appendix~\ref{apx:constructing_skob_family}.

% \begin{proof}
%      Let $\varepsilon = 1/\sqrt{3}$.
%     We first consider the case when $\gamma =1/3$.
%    The dataset $X$ contains $n$ data points $x_1,\dots,x_n$ in $\bbR^{n+1}$ such that 
%     $$
%     x_i = \varepsilon \cdot e_1 + \sqrt{1-\varepsilon^2} \cdot e_{i+1},
%     $$
%     where $e_i$ is the $i$-th standard basis vector. Each data point $x_i$ has a unit length. 

%     Consider the following $n$ different label functions $f_1,\dots, f_n$ on this dataset $X$. The function $f_i$ labels the point $x_i$ as positive and all other points as negative. 
%     Let $h_i$ be the linear classifier with the weight vector $w_i = -\varepsilon \cdot e_1 + \sqrt{1-\varepsilon^2} \cdot e_{i+1}$. Then, we have $\inner{h_i}{x_i} = 1-2\varepsilon^2  \geq \gamma$ and $\inner{h_i}{x_j} = -\varepsilon^2  \leq -\gamma$ for all $j \neq i$. 
%     Thus, each instance $(X,f_i)$ is linear separable by margin $\gamma$ and $h_i$ is a $\gamma$-margin classifier for this instance $(X,f_i)$. Therefore, to distinguish these instances and achieve recall $1$, any protocol must reveal all data points for verification.

%     Next we consider the case when $\gamma = 1/3 - \eta$ for some $\eta >0$.
%     Let $\beta = (3/2)\eta$.
%     It is well-known that by picking unit vectors randomly in $\R^d$ we can construct a collection of vectors $v_1, v_2, \ldots v_k$ such that $\inner{v_i}{v_j} \leq \beta$ if $i\neq j$ and $k\geq \exp{(\Omega(\beta^2 d))}$ (see \Cref{lem:exp_many_neearly_orthogoanl_vectors} in the appendix for a proof). 
%     WLOG we can assume that $v_1,\ldots,v_k$ are in the span of $e_2,\ldots,e_{d+1}$. Now, for all $i\in [k]$ we set 
%     $$
%     x_i = \varepsilon \cdot e_1 + \sqrt{1-\varepsilon^2} \cdot v_{i}.
%     $$
%     Further, by setting $w_i = -\varepsilon \cdot e_1 + \sqrt{1-\varepsilon^2} \cdot v_i$ we have that 
%     \[
%     \inner{v_i}{w_i} = -\varepsilon^2 + 1 - \varepsilon^2 = 1/3 \geq \gamma
%     \]
%     and for $i\neq j$
%     \[
%     \inner{v_i}{w_j} \leq -\varepsilon^2 + (1 - \varepsilon^2)\eta = -1/3 + \eta = -\gamma.
%     \]
% \end{proof}