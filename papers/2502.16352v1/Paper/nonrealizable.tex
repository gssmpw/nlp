\section{Nonrealizable Setting}
\label{sec:non_realizable}

% In this section, we consider the nonrealizable setting in which there is no classifier in the hypothesis class that can perfectly classify the instance. 
% In this setting, we consider instances that can be classified by a classifier $h$ in a hypothesis class $\calH$ not necessarily perefectly but with a small number of errors. 
In this setting, we consider instances that can be classified by a classifier $h$ in a hypothesis class $\calH$, not necessarily \emph{perfectly}, but with a small number of errors.
We first introduce the \emph{robust Leave-One-Out dimension} and provide a robust protocol for general hypothesis classes that achieves high recall and nonresponsive disclosure at most the robust Leave-One-Out dimension of $\calH$.
Then, we characterize the robust Leave-One-Out dimension of linear classifiers with a margin. 

\begin{definition}[robust Leave-One-Out dimension]
\label{defn:robust_leave_one_out}
     Given a set system $(X,\calS)$ and a slack $L \in \N$, the \emph{robust Leave-One-Out} dimension is the cardinality of the largest set $C\subseteq X$ such that for each element $c \in C$, there exists $S \in \calS$ with $c \in S \cap C$ and $|S\cap C| \leq L+1$.
\end{definition}

\begin{theorem}\label{thm:robust_protocol_general}
    Let $\calH$ be a hypothesis class of binary classifiers on a set $X$ with robust Leave-One-Out dimension $k$ when the slack is $L$.  
    Suppose $f: X \to \{+1,-1\}$ represents the true labels for responsive and nonresponsive documents, and let $(X,f)$ be an instance classified by a classifier in $\calH$ with at most $L$ errors. Then, \Cref{alg:robust_abstract_critical_points_protocol} (in Appendix~\ref{apx:robust_protocol}) defines a multi-party verification protocol with the following properties:
    \begin{enumerate}
        \item \textbf{(Recall)} The recall is at least $1-L/n^+$, where $n^+$ is the cardinality of the true responsive documents.
        \item \textbf{(Nonresponsive Disclosure)} If Alice reports all labels correctly, the number of disclosed nonresponsive documents is at most $k$.
        \item \textbf{(Efficiency)} Given an empirical risk minimization (ERM) oracle $\calO$ for $\calH$ (see \Cref{asp:oracle}), the protocol runs in time $O(|X|)$.
    \end{enumerate}
\end{theorem}

%\subsection{Robust Critical Points Protocol}\label{sec:robust_protocol_general}

% We now describe the robust critical points protocol for the nonrealizable setting. 
The proof of \Cref{thm:robust_protocol_general} appears in Appendix~\ref{apx:robust_protocol} where we also describe \Cref{alg:robust_abstract_critical_points_protocol} in detail. 
The protocol follows the same framework as Algorithm~\ref{alg:abstract_critical_points_protocol}, but instead of using Algorithm~\ref{alg:abstract_critical_points_computation}, it uses Algorithm~\ref{alg:robust_critical} as a subroutine to compute robust critical points.
We also assume access to the following empirical risk minimization (ERM) oracle.

\begin{assumption}\label{asp:oracle}
    Suppose there is an oracle $\calO$ that given any instance $(X,f)$, a hypothesis class $\calH$, a point $x \in X$, finds a classifier in $\calH$ that classifies $x$ as positive and minimizes the total error among all classifiers in $\calH$ that label $x$ as positive.
\end{assumption}

%\input{Algorithm/Algo_Robust_Protocol}
\input{Algorithm/Algo_Robust_Critical}

%\subsection{Robust Leave-One-Out Dimension of Linear Classifiers with Margin}\label{sec:robust_leave_one_out}

We now characterize the robust Leave-One-Out dimension of linear classifiers with a fixed margin. 
In the nonrealizable setting, a large-margin classifier may incur two types of errors: (1) margin error; and (2) classification error. The margin errors occur when points are classified correctly but do not satisfy the margin condition. 
The classification errors occur when points are misclassified by the classifier. 
We consider instances that can be classified by a large margin classifier with a small total error, where the total error contains both the margin errors and the classification errors. 

\begin{definition}
    An instance $(X,f)$ is classified by a linear classifier with margin $\gamma \in [0,1]$ and total error $L \in \N$ if the number of total error is at most
    $$
    |\{x: \inner{\bar{x}}{w} \cdot f(x) < \gamma\}|  \leq L,
    $$
    where $w$ is the weight vector of this classifier and $\bar{x} = x / \|x\|$ be the normalized vector for point $x$.
\end{definition}

For any instance $(X,f)$, any margin $\gamma \in [0,1]$, and any error slack $L \in \N$, let $\calH_{\gamma,L}(X,f)$ be the class of linear classifiers that classifies $(X,f)$ with margin $\gamma$ and total error at most $L$. We then characterize the robust Leave-One-Out dimension of this class $\calH_{\gamma,L}(X,f)$ for a large margin $\gamma > 1/3$. 
% The proof is deferred to Appendix~\ref{apx:robust_protocol}.

\begin{theorem}\label{thm:robust_leave-one-out}
    For any instance $(X,f)$, any $1/3 < \gamma \leq 1$, and any error slack $L \in \N$, the robust Leave-One-Out dimension of the class $\calH_{\gamma, L}(X,f)$ is 
    $$
        \frac{(2+2L)(\gamma+1)}{3\gamma -1}.
    $$
\end{theorem}

% \begin{theorem}\label{thm:robust_protocol}
%     Consider any instance with $n$ points that can be classified by a classifier with margin $\gamma > 1/3$ and total error $L \in \N$.
%     there is a protocol that takes $\gamma$ and $L$ as input and satisfies
%     \begin{enumerate}
%         \item \textbf{(Recall)} The recall is at least $1 - L/n$.
%         \item \textbf{(Non-responsive disclosure)} If Alice reports all labels truthfully, the non-responsive disclosure is at most 
%         $$
%         \frac{(2+2L)(\gamma+1)}{3\gamma -1}.
%         $$
%         %\item (Truthful) Alice's best strategy is to truthfully report all labels. 
%     \end{enumerate}    
% \end{theorem}









% \begin{lemma}[Robust Skew-Obtuse Family of Vectors]
% \label{lemma:robust-skew-obtuse_lemma}
%     Let $V= \bar{v}_1,\bar{v}_2,\cdots, \bar{v}_k$ be a $k$ unit vectors in $\R^d$. Further, let  $W = w_1,w_2,\cdots, w_k$ be $k$ other unit vectors in $\R^d$. Suppose there exists a $\gamma\in [0,1]$ and $L < k$ such that the following holds: 
%     For each $i \in [k]$
%     \begin{enumerate}
%         \item $\inner{\bar{v}_i}{w_i} \geq \gamma$ and
%         \item $\inner{\bar{v}_j}{w_i} \leq -\gamma$ for at least $k - 1 - L$ vectors $\bar v_j$.
%     \end{enumerate}
%      Then, if $\gamma>1/3$ we have $k \leq \frac{(2+2L)(\gamma+1)}{3\gamma-1}.$ Further if $\gamma=1/3$, there exists a skew-obtuse family of vectors with $k\geq \Omega(d)$ and if $\gamma<1/3$ then with $k\geq \exp{(\Omega_{1/3-\gamma}(d))}$.
% \end{lemma}

% \begin{proof}
%     Let $V= (\bar{v}_1,\bar{v}_2,\cdots, \bar{v}_k)^T$ be a $k\times d$ matrix. 
%     Similarly, we define $W = (w_1,w_2,\cdots, w_k)^T$ be a $k\times d$ matrix. Then, we have $VW^T$ is a $k\times k$ matrix, where each entry is $\inner{\bar{v}_i}{w_j}$. 
%     Let $\{a_1, a_2, \dots, a_d\}$ be the $d$ column vectors of matrix $V$. Let $\{b_1,b_2,\dots, b_d\}$ be the $d$ column vectors of $W$.

%     We first consider the trace of matrix $VW^T$. Since each diagonal entry of $VW^T$ satisfies $\inner{\bar{v}_i}{w_j} \geq \gamma$, we have 
%     $$
%     \Tr(VW^T) = \sum_{i=1}^k \inner{\bar{v}_i}{w_i} \geq k \gamma.
%     $$
%     Since $\bar{v}_i$ and $w_i$ are all unit vectors, we have $\Tr(VW^T) \leq k$.
%     We also have 
%     $$
%     \Tr(VW^T) = \Tr(WV^T) = \sum_{i=1}^d \inner{a_i}{b_i} = \sum_{i=1}^d \|a_i\|\|b_i\|\cos(\theta_i),
%     $$
%     where $\theta_i$ is the angle between vectors $a_i$ and $b_i$. Hence, we get
%     \begin{equation}\label{eq:robust-trace}
%     \sum_{i=1}^d \|a_i\|\|b_i\|\cos(\theta_i) = \Tr(VW^T) \geq k\gamma.
%     \end{equation}

%     We now consider the sum of all entries in matrix $VW^T$. 
%     For each column $i \in [k]$ of the matrix $VW^T$, we know that there are at least $k-1-L$ off-diagonal entries with value at most $\inner{\bar{v}_j}{w_i} \leq -\gamma$.
%     For other off-diagonal entries in that column, we upper bound them by one since $\bar{v}_j$ and $w_i$ are unit vectors.
%     We use $\one$ to denote all one vector. Then, we have the sum of all entries is
%     $$
%     \one^T VW^T \one = \Tr(VW^T) + \sum_{i\neq j} \inner{\bar{v}_j}{w_i} \leq \Tr(VW^T) - k(k-1 -L)\gamma + kL.
%     $$
%     Note that $VW^T = \sum_{i=1}^d a_i b_i^T$. Thus, we have $\one^T VW^T \one = \sum_{i=1}^d \inner{\one}{a_i} \inner{b_i}{\one}$. Let $\alpha_i$ be the angle between $\one$ and $-a_i$ and $\beta_i$ be the angle between $\one$ and $b_i$. Then, we have 
%     $$
%     - \one^T VW^T \one = \sum_{i=1}^d \inner{\one}{-a_i} \inner{b_i}{\one} = \sum_{i=1}^d \|\one\|^2 \|a_i\|\|b_i\| \cos(\alpha_i) \cos(\beta_i). 
%     $$
%     Since the angle between $-a_i$ and $b_i$ is $\pi - \theta_i$, we have $\alpha_i + \beta_i \geq \pi - \theta_i$. Since the cosine function is log-concave on $[0,\pi]$, we have 
%     $$
%     \cos(\alpha_i) \cos(\beta_i) \leq \cos^2\left(\frac{\pi-\theta_i}{2}\right).
%     $$
%     By combining the equations above, we have
%     \begin{align*}
%     k \sum_{i=1}^d \|a_i\|\|b_i\| \cos^2\left(\frac{\pi-\theta_i}{2}\right) \geq - \one^T VW^T \one \geq - \Tr(VW^T) + k(k-1-L)\gamma - kL.
%     \end{align*}
%     Since $\Tr(VW^T) \leq k$, we have
%     \begin{equation}\label{eq:robust-all-sum}
%         k \sum_{i=1}^d \|a_i\|\|b_i\| \cos^2\left(\frac{\pi-\theta_i}{2}\right) \geq -k+k(k-1-L)\gamma - kL.
%     \end{equation}

%     By combining Equations~(\ref{eq:robust-trace}) and~(\ref{eq:robust-all-sum}), we have 
%     $$
%     \sum_{i=1}^d \|a_i\|\|b_i\| = \sum_{i=1}^d \|a_i\|\|b_i\| \left(2\cos^2\left(\frac{\pi-\theta_i}{2}\right) + \cos(\theta_i)\right) \geq (3k-2-2L)\gamma -2 - 2L.
%     $$
%     By Cauchy-Schwarz inequality, we have 
%     $$
%     \left(\sum_{i=1}^d \|a_i\|\|b_i\| \right)^2 \leq \sum_{i=1}^d \|a_i\|^2 \cdot \sum_{i=1}^d \|b_i\|^2 = k^2,
%     $$
%     where the last equality is from $\sum_{i=1}^d \|a_i\|^2 = \sum_{i=1}^d \|b_i\|^2 = k$ since matrices $V$ and $W$ are both consists of $k$ unit row vectors. Therefore, we have 
%     $$
%     k \leq \frac{(2+2L)(\gamma+1)}{3\gamma-1}.
%     $$

%     For the lower bounds see \Cref{theorem:constructing_skob_family}.
% \end{proof}

\input{Paper/generalized_skew-obtuse_lemma}