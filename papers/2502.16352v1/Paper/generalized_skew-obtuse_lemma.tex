%\subsection{Generalized Skew-Obtuse Family of Vectors}\label{sec:general_skew-obtuse}

% The proof of Theorem~\ref{thm:robust_leave-one-out} follows from the following generalized version of Lemma~\ref{lemma:skew-obtuse_lemma} for the geometric properties of the skew-obtuse family of vectors, in the same way \Cref{thm:robust_leave-one-out} followed from \Cref{lemma:skew-obtuse_lemma}. 
% The proof of the lemma is deferred to Appendix~\ref{apx:proof_general_skew_obtuse}.
% The proof of Theorem~\ref{thm:robust_leave-one-out} follows from a generalized version of Lemma~\ref{lemma:skew-obtuse_lemma}, which extends the geometric properties of a \emph{skew-obtuse family of vectors}. 
% The argument mirrors the way \Cref{thm:Leave-One-Out-linear-realizable} follows from \Cref{lemma:skew-obtuse_lemma}.  
% We will use \Cref{lemma:generalized-skew-obtuse_lemma} with $\alpha = \beta = \gamma$.
% Consider any set $C\subseteq X$ witnessing the robust Leave-One-Out dimension for $\calH_{\gamma,L}(X,f)$. Now, for any point $c\in C$ there is a hypothesis $h_c \in \calH_{\gamma,L}(X,f)$ such that it classifies $c$ as positive and at most $L$ other points in $C$ as positive. Besides these points there can also be other points $C$ classified as negative by $h_c$ but with low margin.
% However, by definition of $\calH_{\gamma,L}(X,f)$ there are at most $L$ total points which are either misclassified or have low margin. Hence, for every $c\in C$ there is unit vector $w_c$ (the weight vector of $h_c$) which satisfies the two properties mentioned in \Cref{lemma:generalized-skew-obtuse_lemma}.  
% The proof of this lemma is deferred to Appendix~\ref{apx:proof_general_skew_obtuse}.
The proof of Theorem~\ref{thm:robust_leave-one-out} follows from a generalized version of Lemma~\ref{lemma:skew-obtuse_lemma}, which extends the geometric properties of a \emph{skew-obtuse family of vectors}. The argument mirrors the way \Cref{thm:Leave-One-Out-linear-realizable} follows from \Cref{lemma:skew-obtuse_lemma}.  
We apply \Cref{lemma:generalized-skew-obtuse_lemma} with $\alpha = \beta = \gamma$. Let $C \subseteq X$ be a set witnessing the robust Leave-One-Out dimension for $\calH_{\gamma,L}(X,f)$. For any $c \in C$, there exists a hypothesis $h_c \in \calH_{\gamma,L}(X,f)$ that classifies $c$ as positive and at most $L$ other points in $C$ as positive. Additionally, $h_c$ may classify some points in $C$ as negative but with low margin.  
By definition of $\calH_{\gamma,L}(X,f)$, at most $L$ points are either misclassified or have low margin. Thus, for each $c \in C$, there exists a unit vector $w_c$ (the weight vector of $h_c$) satisfying the two properties in \Cref{lemma:generalized-skew-obtuse_lemma}.  
The proof of this lemma is deferred to Appendix~\ref{apx:proof_general_skew_obtuse}.

\begin{lemma}[Robust Skew-Obtuse Family of Vectors]
\label{lemma:generalized-skew-obtuse_lemma}
    Let $V= \bar{v}_1,\bar{v}_2,\cdots, \bar{v}_k$ be a $k$ unit vectors in $\R^d$. Further, let  $W = w_1,w_2,\cdots, w_k$ be $k$ other unit vectors in $\R^d$. Suppose there exists $\alpha,\beta\in [0,1]$ and an error parameter $L$ such that the following holds:
    \begin{enumerate}
        \item $\inner{\bar{v}_i}{w_i} \geq \alpha$ and
        \item for all $j\in [k]$ we have $\left|i\in[k]\colon  \inner{\bar{v}_i}{w_j} \leq -\beta\right|\geq k-1-L$. 
    \end{enumerate}
     Then, if $\alpha+2\beta>1$ we have $ k \leq \frac{(2+2L)(1+\beta)}{\alpha+2\beta-1}.$
     % Further if $\gamma=1/3$, there exists a skew-obtuse family of vectors with $k\geq \Omega(d)$ and if $\gamma<1/3$ then with $k\geq \exp{(\Omega_{1/3-\gamma}(d))}$.
\end{lemma}

% \begin{proof}
%     Let $V= (\bar{v}_1,\bar{v}_2,\cdots, \bar{v}_k)^T$ be a $k\times d$ matrix. 
%     Similarly, we define $W = (w_1,w_2,\cdots, w_k)^T$ be a $k\times d$ matrix. Then, we have $VW^T$ is a $k\times k$ matrix, where each entry is $\inner{\bar{v}_i}{w_j}$. 
%     Let $\{a_1, a_2, \dots, a_d\}$ be the $d$ column vectors of matrix $V$. Let $\{b_1,b_2,\dots, b_d\}$ be the $d$ column vectors of $W$.

%     We first consider the trace of matrix $VW^T$. Since each diagonal entry of $VW^T$ satisfies $\inner{\bar{v}_i}{w_j} \geq \alpha$, we have 
%     $$
%     \Tr(VW^T) = \sum_{i=1}^k \inner{\bar{v}_i}{w_i} \geq k \alpha.
%     $$
%     Since $\bar{v}_i$ and $w_i$ are all unit vectors, we have $\Tr(VW^T) \leq k$.
%     We also have 
%     $$
%     \Tr(VW^T) = \Tr(WV^T) = \sum_{i=1}^d \inner{a_i}{b_i} = \sum_{i=1}^d \|a_i\|\|b_i\|\cos(\theta_i),
%     $$
%     where $\theta_i$ is the angle between vectors $a_i$ and $b_i$. Hence, we get
%     \begin{equation}\label{eq:trace2}
%     \sum_{i=1}^d \|a_i\|\|b_i\|\cos(\theta_i) = \Tr(VW^T) \geq k\alpha.
%     \end{equation}

%     We now consider the sum of all entries in matrix $VW^T$. Since all off-diagonal entries of matrix $VW^T$ satisfies $\inner{\bar{v}_i}{w_j} \leq -\beta$. We use $\one$ to denote all one vector. Then, we have the sum of all entries is
%     $$
%     \one^T VW^T \one = \Tr(VW^T) + \sum_{i\neq j} \inner{\bar{v}_i}{w_j} \leq \Tr(VW^T) - k(k-1)\beta.
%     $$
%     Note that $VW^T = \sum_{i=1}^d a_i b_i^T$. Thus, we have $\one^T VW^T \one = \sum_{i=1}^d \inner{\one}{a_i} \inner{b_i}{\one}$. Let $\alpha_i$ be the angle between $\one$ and $-a_i$ and $\beta_i$ be the angle between $\one$ and $b_i$. Then, we have 
%     $$
%     - \one^T VW^T \one = \sum_{i=1}^d \inner{\one}{-a_i} \inner{b_i}{\one} = \sum_{i=1}^d \|\one\|^2 \|a_i\|\|b_i\| \cos(\alpha_i) \cos(\beta_i). 
%     $$
%     Since the angle between $-a_i$ and $b_i$ is $\pi - \theta_i$, we have $\alpha_i + \beta_i \geq \pi - \theta_i$. Since the cosine function is log-concave on $[0,\pi]$, we have 
%     $$
%     \cos(\alpha_i) \cos(\beta_i) \leq \cos^2\left(\frac{\pi-\theta_i}{2}\right).
%     $$
%     By combining the equations above, we have
%     \begin{equation}\label{eq:all_sum2}
%     k \sum_{i=1}^d \|a_i\|\|b_i\| \cos^2\left(\frac{\pi-\theta_i}{2}\right) \geq - \one^T VW^T \one \geq - \Tr(VW^T) + k(k-1)\beta \geq - k + k(k-1)\beta,
%     \end{equation}
%     where the last inequality is due to $\Tr(VW^T) \leq k$.

%     By combining Equations~(\ref{eq:trace2}) and~(\ref{eq:all_sum2}), we have 
%     $$
%     \sum_{i=1}^d \|a_i\|\|b_i\| = \sum_{i=1}^d \|a_i\|\|b_i\| \left(2\cos^2\left(\frac{\pi-\theta_i}{2}\right) + \cos(\theta_i)\right) \geq k(\alpha+2\beta) -2 - 2\beta.
%     $$
%     By Cauchy-Schwarz inequality, we have 
%     $$
%     \left(\sum_{i=1}^d \|a_i\|\|b_i\| \right)^2 \leq \sum_{i=1}^d \|a_i\|^2 \cdot \sum_{i=1}^d \|b_i\|^2 = k^2,
%     $$
%     where the last equality is from $\sum_{i=1}^d \|a_i\|^2 = \sum_{i=1}^d \|b_i\|^2 = k$ since matrices $V$ and $W$ are both consists of $k$ unit row vectors. Therefore, we have 
%     $$
%     k \leq \frac{2+2\beta}{\alpha+2\beta-1}.
%     $$

% \end{proof}

% \begin{proof}
%     Let $V= (\bar{v}_1,\bar{v}_2,\cdots, \bar{v}_k)^T$ be a $k\times d$ matrix. 
%     Similarly, we define $W = (w_1,w_2,\cdots, w_k)^T$ be a $k\times d$ matrix. Then, we have $VW^T$ is a $k\times k$ matrix, where each entry is $\inner{\bar{v}_i}{w_j}$. 
%     Let $\{a_1, a_2, \dots, a_d\}$ be the $d$ column vectors of matrix $V$. Let $\{b_1,b_2,\dots, b_d\}$ be the $d$ column vectors of $W$.

%     We first consider the trace of matrix $VW^T$. Since each diagonal entry of $VW^T$ satisfies $\inner{\bar{v}_i}{w_j} \geq \alpha$, we have 
%     $$
%     \Tr(VW^T) = \sum_{i=1}^k \inner{\bar{v}_i}{w_i} \geq k \alpha.
%     $$
%     Since $\bar{v}_i$ and $w_i$ are all unit vectors, we have $\Tr(VW^T) \leq k$.
%     We also have 
%     $$
%     \Tr(VW^T) = \Tr(WV^T) = \sum_{i=1}^d \inner{a_i}{b_i} = \sum_{i=1}^d \|a_i\|\|b_i\|\cos(\theta_i),
%     $$
%     where $\theta_i$ is the angle between vectors $a_i$ and $b_i$. Hence, we get
%     \begin{equation}\label{eq:robust-trace}
%     \sum_{i=1}^d \|a_i\|\|b_i\|\cos(\theta_i) = \Tr(VW^T) \geq k\alpha.
%     \end{equation}

%     We now consider the sum of all entries in matrix $VW^T$. 
%     For each column $i \in [k]$ of the matrix $VW^T$, we know that there are at least $k-1-L$ off-diagonal entries with value at most $\inner{\bar{v}_j}{w_i} \leq -\beta$.
%     For other off-diagonal entries in that column, we upper bound them by one since $\bar{v}_j$ and $w_i$ are unit vectors.
%     We use $\one$ to denote all one vector. Then, we have the sum of all entries is
%     $$
%     \one^T VW^T \one = \Tr(VW^T) + \sum_{i\neq j} \inner{\bar{v}_j}{w_i} \leq \Tr(VW^T) - k(k-1 -L)\beta + kL.
%     $$
%     Note that $VW^T = \sum_{i=1}^d a_i b_i^T$. Thus, we have $\one^T VW^T \one = \sum_{i=1}^d \inner{\one}{a_i} \inner{b_i}{\one}$. Let $\phi_i$ be the angle between $\one$ and $-a_i$ and $\psi_i$ be the angle between $\one$ and $b_i$. Then, we have 
%     $$
%     - \one^T VW^T \one = \sum_{i=1}^d \inner{\one}{-a_i} \inner{b_i}{\one} = \sum_{i=1}^d \|\one\|^2 \|a_i\|\|b_i\| \cos(\phi_i) \cos(\psi_i). 
%     $$
%     Since the angle between $-a_i$ and $b_i$ is $\pi - \theta_i$, we have $\phi_i + \psi_i \geq \pi - \theta_i$. Since the cosine function is log-concave on $[0,\pi]$, we have 
%     $$
%     \cos(\phi_i) \cos(\psi_i) \leq \cos^2\left(\frac{\pi-\theta_i}{2}\right).
%     $$
%     By combining the equations above, we have
%     \begin{align*}
%     k \sum_{i=1}^d \|a_i\|\|b_i\| \cos^2\left(\frac{\pi-\theta_i}{2}\right) \geq - \one^T VW^T \one \geq - \Tr(VW^T) + k(k-1-L)\beta - kL.
%     \end{align*}
%     Since $\Tr(VW^T) \leq k$, we have
%     \begin{equation}\label{eq:robust-all-sum}
%         k \sum_{i=1}^d \|a_i\|\|b_i\| \cos^2\left(\frac{\pi-\theta_i}{2}\right) \geq -k+k(k-1-L)\beta - kL.
%     \end{equation}

%     By combining Equations~(\ref{eq:robust-trace}) and~(\ref{eq:robust-all-sum}), we have 
%     $$
%     \sum_{i=1}^d \|a_i\|\|b_i\| = \sum_{i=1}^d \|a_i\|\|b_i\| \left(2\cos^2\left(\frac{\pi-\theta_i}{2}\right) + \cos(\theta_i)\right) \geq k\alpha+(2k-2-2L)\beta -2 - 2L.
%     $$
%     By Cauchy-Schwarz inequality, we have 
%     $$
%     \left(\sum_{i=1}^d \|a_i\|\|b_i\| \right)^2 \leq \sum_{i=1}^d \|a_i\|^2 \cdot \sum_{i=1}^d \|b_i\|^2 = k^2,
%     $$
%     where the last equality is from $\sum_{i=1}^d \|a_i\|^2 = \sum_{i=1}^d \|b_i\|^2 = k$ since matrices $V$ and $W$ are both consists of $k$ unit row vectors. Therefore, we have 
%     $$
%     k \leq \frac{(2+2L)(\beta+1)}{\alpha + 2\beta-1}.
%     $$

%     For the lower bounds see Lemma~\ref{lemma:constructing_skob_family}.
% \end{proof}