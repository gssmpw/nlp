\subsection{Leave-One-Out Dimension of Linear Classifiers with Margin}\label{sec:realizable_linear_margin}

In this section, we characterize the Leave-One-Out dimension of the linear classifiers with a margin.

Without loss of generality, we assume that the linear classifier passes through the origin in $\R^d$, as one can add an extra dimension for the bias if needed. Let $w \in \R^d$ be the unit-length weight vector of a linear classifier $h$, so that for any $x \in X$, we have $h(x) = \operatorname{sign}(\langle w, x \rangle)$.

We define the margin of a linear classifier as follows. 
\begin{definition}
\label{defn:margin}
    Given a set of points $X \subset \R^d$, the \emph{margin} of a linear classifier $h$ with unit-length weight vector $w$ on $X$ is defined as 
    $$
    \gamma(w,X) = \min_{x \in X} \frac{|\inner{w}{x}|}{\|x\|_2}.
    $$
\end{definition}
Then, for any $\gamma \in [0,1]$ and $X\subseteq \R^d$, we use $\calH_\gamma(X)$ to denote the hypothesis class of all linear classifiers with a margin at least $\gamma$ on $X$.

We show the following trichotomy of the Leave-One-Out dimension of the hypothesis class $\calH_\gamma(X)$.

\begin{theorem}\label{thm:Leave-One-Out-linear-realizable}
    For any set of points $X \subset \R^d$, any $\gamma \in [0,1]$, the Leave-One-Out dimension $k$, of the class $\calH_\gamma(X)$ is 
    \begin{enumerate}
        \item $k \leq \frac{2+2\gamma}{3\gamma -1}$,  if $\gamma > \frac{1}{3}$.
    \end{enumerate}
    Further, there are sets $X$ such that 
    % the the Leave-One-Out dimension of the class $\calH_\gamma(X)$ is 
    \begin{enumerate}
        \item $k \geq \Omega(d)$, if $\gamma = \frac{1}{3}$;
        \item $k \geq \exp(\Omega((1/3 - \gamma)^2d))$, if $\gamma < \frac{1}{3}$.
    \end{enumerate}
    
\end{theorem}
% \todosid{merge with corollary 5 and add efficiency of oracle claim...and statment changed for accuracy}

\begin{corollary}
    Armed with Theorems~\ref{thm:protocol_realizable} and \ref{thm:Leave-One-Out-linear-realizable}, we analyze instances that are linearly separable with a large margin $\gamma > 1/3$, i.e., there exists $w \in \mathbb{R}^d$ such that for all $x \in X$:
\[
\frac{\langle x, w \rangle \cdot f(x)}{\|x\|_2} \geq \gamma.
\]
For such instances, \Cref{alg:abstract_critical_points_protocol} achieves a nonresponsive disclosure of at most $O(1)$, independent of the instance size $|X|$ and ambient dimension $d$.  
% Furthermore, there is an efficient oracle $\calO$ that verifies the label membership in this class using the hard support vector machine (SVM) (See Theorem 15.8 in~\citet*{shalev2014understanding}). Hence,  \Cref{alg:abstract_critical_points_protocol} is overall efficient.
Furthermore, an efficient oracle $\calO$ verifies label membership in this class using the hard support vector machine (SVM) (see Theorem 15.8 in~\citet*{shalev2014understanding}). Thus, \Cref{alg:abstract_critical_points_protocol} is overall efficient.

For instances with smaller margins, however, any protocol ensuring $\mathrm{Recall} = 1$ must incur a nonresponsive disclosure that scales either linearly with $d$ when $\gamma = 1/3$, or exponentially with $d$ when $\gamma < 1/3$.

\end{corollary}
% \begin{corollary}
%     Armed with Theorem~\ref{thm:protocol_realizable} and Theorem~\ref{thm:Leave-One-Out-linear-realizable}, for the instances that are linearly separable by a large margin $\gamma > 1/3$, ie, there exists  $w \in \bbR^d$ such that for any $x \in X$:
%     $
%     \frac{\inner{x}{w} \cdot f(x)}{\|x\|_2}  \geq \gamma
%     $
%     \Cref{alg:abstract_critical_points_protocol} achieves a nonresponsive disclosure at most $O(1)$, which does not depend on the size of instance $|x|$ and the ambient dimension $d$. 
%     While there are instances with smaller margins, for any protocol with $\mathrm{Recall}=1$ the nonresponsive disclosure could be dependent linearly ($\gamma = 1/3$) or exponentially ($\gamma < 1/3$) on the ambient dimension.
% \end{corollary}

% We prove this theorem using Lemma~\ref{lemma:skew-obtuse_lemma} .

% We now consider the instance $(X,f)$ that is realizable by the hypothesis class $\calH_{\gamma}(X)$. 
% This means the instance $(X,f)$ is linearly separable by margin $\gamma$ since there exists a linear classifier with unit-length weight vector $w \in \bbR^d$ such that for any $x \in X$
%     $$
%     \frac{\inner{x}{w} \cdot f(x)}{\|x\|_2}  \geq \gamma.
%     $$
% By Theorem~\ref{thm:protocol_realizable} and Theorem~\ref{thm:Leave-One-Out-linear-realizable}, for the instances that are linearly separable by a large margin $\gamma > 1/3$, our protocol achieves the nonresponsive disclosure at most $O(1)$, which does not depend on the size of instance $n$ and the dimension $d$. While, in the worst-case instance, the minimal nonresponsive disclosure should contain all nonresponsive documents (See Lemma~\ref{lemma:constructing_skob_family}).

% \begin{corollary}\label{corollay:protocol_realizable}
%     For any instance linearly separable by margin $\gamma > 1/3$, there is a protocol that takes $\gamma$ as input and satisfies
%     \begin{enumerate}
%         \item (Recall) The recall is $1$.
%         \item (Non-responsive disclosure) If Alice reports all labels truthfully, the non-responsive disclosure is at most 
%         $$
%         \frac{2+2\gamma}{3\gamma -1}.
%         $$
%         \item (Truthful) Alice's best strategy is to truthfully report all labels. 
%     \end{enumerate}
% \end{corollary}

