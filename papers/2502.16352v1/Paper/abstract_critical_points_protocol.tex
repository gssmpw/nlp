\subsection{Protocol for General Hypothesis Class}\label{sec:realizable_general}

Consider the  multi-party verification protocol \Cref{alg:abstract_critical_points_protocol} when the input is an instance $(X,f)$, realizable by a general hypothesis class $\calH$. The protocol contains a subroutine for computing critical points as shown in Algorithm~\ref{alg:abstract_critical_points_computation}. 
This protocol makes $O(|X|)$ queries to an oracle $\calO$ that checks whether a set of labeled points is realizable by the class $\calH$.
This protocol works as follows.
First, Alice provides Trent with the entire set of documents $X$ along with her labels. Next, Trent applies Algorithm~\ref{alg:abstract_critical_points_computation} to identify critical points based on Alice’s labeling. 
In this algorithm, Trent iterates over each document labeled by Alice as negative, temporarily flips its label to positive, and checks—using an oracle—whether any classifier in $\calH$ can perfectly classify the resulting dataset. If no such classifier exists, that document is removed from consideration. All remaining negatives after this procedure are deemed critical points. Finally, the protocol sends all documents labeled as positive and all critical points to Bob for verification.

There is a key difference between our protocol and the critical points protocol from~\cite{dong2022classification}. In our protocol, if no classifier in $\calH$ perfectly classifies $T_1$ and $T_2$, we remove a point in Step 5. 
By contrast, the protocol in~\cite{dong2022classification} does not remove any points. 
This distinction allows our protocol to handle a general hypothesis class $\calH$. 
To illustrate, consider a classifier $h \in \calH$ that labels $X_A^+$ and two additional points $x_1,x_2 \in X_A^-$ as positive and labels all remaining points $X \setminus (X_A^+\cup \{x_1,x_2\})$ as negatives. 
If there is no classifier in $\calH$ that labels exactly $X_A^+ \cup \{x_1\}$ as positive or exactly $X_A^+ \cup \{x_2\}$ as positive, then the protocol in~\cite{dong2022classification} will not mark $x_1$ or $x_2$ as critical points. Consequently, it can not distinguish $h$ from the classifier that aligns perfectly with Alice’s report.

We now prove that our protocol satisfies Theorem~\ref{thm:protocol_realizable}. 

\input{Algorithm/abstract_hypothesis_class_realizable_protocol}

\input{Algorithm/abstract_critical_points_computation}

\begin{proof}[Proof of Theorem~\ref{thm:protocol_realizable}]
    \textbf{$\mathrm{Recall} = 1$:} Let $X_A^+$ and $X_A^-$ be the set of all positives and the set of all negatives reported by Alice, respectively. 
    Without loss of generality, we assume that all points in $X_A^+$ are true positive. 
    (If this is false, Bob will identify any misclassified negative points within $X_A^+$, as these points are always sent to him for verification.)
    Then, let $\calH(X_A^+)$ be the set of all classifiers in $\calH$ that satisfy the following conditions: (1) all points in $X_A^+$ are still classified as positive; and (2) at least one point in $X_A^-$ is classified as positive. 
    
    We first show that using $C(X_A^+)$ we can distinguish two cases: (1) the labels reported by Alice are correct; and (2) Alice labels some true positive points as negative, i.e. the true classifier is in $\calH(X_A^+)$. 
    Specifically, we show that for any classifier $h \in \calH(X_A^+)$, there exists a critical point in $C(X_A^+)$ classified as positive by $h$. 
    Note that critical points $C(X_A^+) \subseteq X_A^-$. 
    If all critical points $C(X_A^+)$ are true negative, then we are in case (1); otherwise, we are in case (2).
    
    We show this by contradiction. Suppose there is no point in $C(X_A^+)$ classified as positive by $h$. 
    Consider any classifier $h \in \calH(X_A^+)$ that classifies at least a point in $X_A^-$ as positive. We use $X_h^+ = \{x \in X: h(x) = 1\}$ to denote the points classified as positive by $h$. Then, we have $X_h^+ \cap X_A^- \neq \varnothing$. 
    Since no point in $C(X_A^+)$ is classified as positive by $h$, all points in $X_h^+ \cap X_A^-$ are removed in Algorithm~\ref{alg:abstract_critical_points_computation}. 
    Now, consider the last point $x_i$ in $X_h^+ \cap X_A^-$ that is removed in Algorithm~\ref{alg:abstract_critical_points_computation}. Let $M_i$ be the set $M$ at the beginning of the iteration at point $x_i$. Since $x_i$ is the last point in $X_h^+ \cap X_A^-$, we have $M_i \setminus \{x_i\} \cap X_h^+ = \varnothing$. Thus, $T_1 = X_A^+ \cup \{x_i\}$ and $T_2 = M_i \setminus \{x_i\}$ are perfectly classified by the classifier $h \in \calH$, which implies that $x_i$ is not removed. 

    Hence, the protocol always guarantees perfect recall, $\mathrm{Recall} = 1$ since if Alice hides any true responsive documents, then Bob will detect such a document and then the court or Trent will reveal all documents to Bob. 

    % \textbf{$\mathrm{Nonresponsive~Disclosure} \leq k$:} Next, we show that the number of critical points is at most the Leave-One-Out dimension $k$.
    % Suppose $C(X_A^+)$ contains $m$ points $v_1,v_2, \dots, v_m \in X$. 
    % We claim that for each $v_i$, there exists a classifier $h_i \in \calH$ that satisfies $h_i(v_i) = 1$ and $h_i(v_j) = -1$ for all $j \neq i$; (In fact $h_i\in \calH(X_A^+)$.) Consider the iteration corresponding to the point $v_i$ in Algorithm~\ref{alg:abstract_critical_points_computation}. Let $M_i$ be the set $M$ at the beginning of this iteration. Since $v_i$ is not removed, there is a classifier $h \in \calH$ that separates $\{v_i\}$ from $M_i \setminus \{v_i\}$. Note that the critical points $C(X_A^+)$ is a subset of $M_i$. This classifier $h$ satisfies the required property.
    % Thus, for each point $v_i$ in $C(X_A^+)$, there exists a classifier in $\calH$ that classifies $v_i$ as positive and other points in $C(X_A^+)$ as negative. By Definition~\ref{defn:leave_one_out}, the number of critical points is at most the Leave-One-Out dimension of $\calH$, i.e. $|C(X_A^+)| \leq k$.

    \textbf{$\mathrm{Nonresponsive~Disclosure} \leq k$:} We now show that the number of critical points is at most the Leave-One-Out dimension $k$.
    Suppose $C(X_A^+)$ contains $m$ points, denoted as $v_1, v_2, \dots, v_m \in X$. We claim that for each $v_i$, there exists a classifier $h_i \in \calH$ such that $h_i(v_i) = 1$ and $h_i(v_j) = -1$ for all $j \neq i$ (in fact, $h_i \in \calH(X_A^+)$). 
    Consider the iteration corresponding to the point $v_i$ in Algorithm~\ref{alg:abstract_critical_points_computation}. Let $M_i$ be the set $M$ at the beginning of this iteration. Since $v_i$ is not removed, there exists a classifier $h \in \calH$ that separates $\{v_i\}$ from $M_i \setminus \{v_i\}$. Noting that the critical points $C(X_A^+)$ form a subset of $M_i$, this classifier $h$ satisfies the desired property.
    Thus, for each point $v_i$ in $C(X_A^+)$, there exists a classifier in $\calH$ that classifies $v_i$ as positive while classifying all other points in $C(X_A^+)$ as negative. By Definition~\ref{defn:leave_one_out}, it follows that the number of critical points is at most the Leave-One-Out dimension of $\calH$, i.e., $|C(X_A^+)| \leq k$.

    Overall, if Alice labels all documents of $X$ correctly, then the protocol only discloses nonresponsive documents in $C(X_A^+)$ to Bob, which is at most $k$ in size. 


    % Finally, we bound the recall and nonresponsive disclosure. This protocol always guarantees perfect recall, $\mathrm{Recall} = 1$ since if Alice hides any true responsive documents, then Bob will detect such a document and then the court or Trent will reveal all documents to Bob. 
    % If Alice labels all documents $X$ correctly, then the protocol only discloses nonresponsive documents in $C(X_A^+)$ to Bob, which is at most $k$. 
    \emph{Truthfulness and Runtime:} Since reporting false labels is guaranteed to reveal all the documents to Bob, it is in Alice's best interest to be truthful. 
    Further, as we only call the membership oracle $\calO$ one per document in $X$ the runtime is bounded by $O(|X|)$.
    
    \emph{Lower Bound:} Let $C \subseteq X$ be the subset witnessing the Leave-One-Out dimension of $X$, so that $|C| = k$ and for each $c \in C$, there exists a classifier $h_c \in \calH$ satisfying $h_c(c) = 1$ and $h_c(c') \neq 1$ for all $c' \in C \setminus \{c\}$. 
    Choose an arbitrary $c \in C$ with its corresponding hypothesis $h_c$, and define $X' = C \setminus \{c\}$. Further, set $f(x) = -1$ for all $x \in X'$. Clearly, $(X', f)$ is realized by $h_c$ when restricted to $X'$. 
    Now, consider any protocol with recall $1$. If the protocol discloses fewer than $k-1$ nonresponsive documents, then there exists some $c' \in X'$ that is not revealed. However, the ground truth could instead be given by $h_{c'}$, which labels $c'$ as $+1$ and all other points in $X'$ as $-1$. In this case, the protocol fails to reveal all relevant documents to Bob, leading to a contradiction.
    
    % \emph{Lower Bound:} Let $C\subseteq X$ be the subset that witnesses the Leave-One-Out dimension of $X$: so $|C|=k$ and for all $c\in C$ there exists a classifier $h_c\in \calH$ such that $h(c) = 1$ and $h(c') \neq 1$ for all $c' \in C\setminus \{c\}$. 
    % Now choose an arbitrary $c\in C$ with the corresponding hypothesis $h_c$, and let $X' = C\setminus\set{c}$. 
    % Further, let $f(x) = -1$ for all $x \in X'$. Then, clearly $(X',f)$ is realized by $h_c$ when restricted to $X'$. 
    % Now, consider any protocol with recall being $1$.
    % If this protocol has nonresponsive disclore lesser than $k-1$, then there is a certain document in $X'$, say $c'$ which isn't revelead by the protcol on input $(X',f)$. However, in this case it is possible that the ground truth was actually given by $h_{c'}$, which lables $c'$ as $+1$ and everthing else in $X'$ as $-1$. Thus, we will not be able to reveal all the relevant documents to Bob, and hence we get a contradiction.
    
    % Let $S \subseteq X_A^-$ be the smallest set that satisfies for any classifier $h \in \calH_\gamma$, if $h$ classifies any point in $X_A^-$ as positive, then there exists a point in $S$ classified as positive by $h$. Suppose $S$ contains $k$ points $v_1,v_2, \dots, v_k \in \bbR^d$. For each $v_i$, there exists a classifier $h_i \in \calH_\gamma$ such that $h_i(v_i) = 1$ and $h_i(v_j) = -1$ for all $j \neq i$. If there is no such $h_i$, then we can remove $v_i$ from $S$ and get a smaller set satisfying the condition. 
    
    % Let $w_i \in \bbR^d$ be the unit-length weight vector of the classifier $h_i(x) = \sgn(\inner{w_i}{x})$. Let $\bar{v}_i = v_i/\|v_i\|_2$. Since $h_i$ has margin at least $\gamma$, we have $\inner{\bar{v}_i}{w_i} \geq \gamma$ and $\inner{\bar{v}_j}{w_i} \leq -\gamma$ for any $j \neq i$. 

    
    % Therefore by Lemma~\ref{lemma:skew-obtuse_lemma}, we have the number of points in $C_\gamma(X_A^+)$ is at most 
    % $$
    % k \leq \frac{2+2\gamma}{3\gamma-1}.
    % $$
\end{proof}



