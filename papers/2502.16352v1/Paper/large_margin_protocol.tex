\subsection{Linear Classification with Margin}

In this section, we provide an E-Discovery protocol for realizable instances with large margins. 

\begin{theorem}\label{thm:protocol_realizable}
    For any instance linearly separable by margin $\gamma > 1/3$, there is a protocol that takes $\gamma$ as input and satisfies
    \begin{enumerate}
        \item (Recall) The recall is $1$.
        \item (Non-responsive disclosure) If Alice reports all labels truthfully, the non-responsive disclosure is at most 
        $$
        \frac{2+2\gamma}{3\gamma -1}.
        $$
        \item (Truthful) Alice's best strategy is to truthfully report all labels. 
    \end{enumerate}
\end{theorem}

\input{Algorithm/Algo_Protocol}
\input{Algorithm/Algo_Critical}

\begin{proof}[Proof of Theorem~\ref{thm:protocol_realizable}]
    The protocol first asks Alice to report labels of all documents in $X$ to Trent. Then, Trent checks whether the labels reported by Alice are linear separable by a $\gamma$-margin classifier. If the labels reported by Alice are not linear separable by margin $\gamma$, then reveal all documents to Bob. 
    
    Now suppose the labels reported by Alice are linear separable by margin $\gamma$. Let $X_A^+$ and $X_A^-$ be the set of all positives and the set of all negatives reported by Alice, respectively. Then, let $\calH_\gamma$ be the set of all linear classifiers that satisfy the following conditions: (1) all points in $X_A^+$ are still classified as positive; and (2) the margin of the classifier is at least $\gamma$; and (3) at least one point in $X_A^-$ is classified as positive. 
    
    We first show that $C_\gamma(X_A^+)$ can distinguish two cases: (1) the labels reported by Alice are correct; and (2) Alice labels some true positive points as negative, i.e. the true classifier is in $\calH_\gamma$. 
    Specifically, we show that for any classifier $h \in \calH_\gamma$, there exists a critical point in $C_\gamma(X_A^+)$ classified as positive by $h$. 
    Note that critical points $C_\gamma(X_A^+) \subseteq X_A^-$. 
    If all critical points $C_\gamma(X_A^+)$ are true negative, then we have case (1); otherwise, we are in case (2).
    We show this by contradiction. Suppose there is no point in $C_\gamma(X_A^+)$ classified as positive by $h$. 
    Consider any classifier $h \in \calH_\gamma$ that classifies at least a point in $X_A^-$ as positive. We use $X_h^+ = \{x \in X: h(x) = 1\}$ to denote the points classified as positive by $h$. Then, we have $X_h^+ \cap X_A^- \neq \varnothing$. Since no point in $C_\gamma(X_A^+)$ is classified as positive by $h$, all points in $X_h^+ \cap X_A^-$ are removed in Algorithm~\ref{alg:large_margin_critical}. Now, consider the last point $x_i$ in $X_h^+ \cap X_A^-$ that are removed in Algorithm~\ref{alg:large_margin_critical}. Let $M_i$ be the set $M$ at the beginning of the iteration at point $x_i$. Since $x_i$ is the last point in $X_h^+ \cap X_A^-$, we have $M_i \setminus \{x_i\} \cap X_h^+ = \varnothing$. Thus, $T_1 = X_A^+ \cup \{x_i\}$ and $T_2 = M_i \setminus \{x_i\}$ are separated by a $\gamma$-margin linear classifier $h$, which implies that $x_i$ is not removed. 

    Suppose $C_\gamma(X_A^+)$ contains $k$ points $v_1,v_2, \dots, v_k \in X$. We now show that for each $v_i$, there exists a linear classifier $h_i$ satisfies two properties: (1) $h_i(v_i) = 1$ and $h_i(v_j) = -1$ for all $j \neq i$; and (2) $h_i$ has a margin $\gamma$ on $C_\gamma(X_A^+)$. ($h_i$ is not required to be in $\calH_\gamma$.) Consider the iteration corresponding to the point $v_i$ in Algorithm~\ref{alg:large_margin_critical}. Let $M_i$ be the set $M$ at the beginning of this iteration. Since $v_i$ is not removed, there is a $\gamma$-margin linear classifier $h$ that separates $\{v_i\}$ from $M_i \setminus \{v_i\}$. Note that the critical points $C_\gamma(X_A^+)$ is a subset of $M_i$. This classifier $h$ satisfies two properties for the point $v_i$.
    
    % Let $S \subseteq X_A^-$ be the smallest set that satisfies for any classifier $h \in \calH_\gamma$, if $h$ classifies any point in $X_A^-$ as positive, then there exists a point in $S$ classified as positive by $h$. Suppose $S$ contains $k$ points $v_1,v_2, \dots, v_k \in \bbR^d$. For each $v_i$, there exists a classifier $h_i \in \calH_\gamma$ such that $h_i(v_i) = 1$ and $h_i(v_j) = -1$ for all $j \neq i$. If there is no such $h_i$, then we can remove $v_i$ from $S$ and get a smaller set satisfying the condition. 
    
    Let $w_i \in \bbR^d$ be the unit-length weight vector of the classifier $h_i(x) = \sgn(\inner{w_i}{x})$. Let $\bar{v}_i = v_i/\|v_i\|_2$. Since $h_i$ has margin at least $\gamma$, we have $\inner{\bar{v}_i}{w_i} \geq \gamma$ and $\inner{\bar{v}_j}{w_i} \leq -\gamma$ for any $j \neq i$. 

    
    Therefore by Lemma~\ref{lemma:skew-obtuse_lemma}, we have the number of points in $C_\gamma(X_A^+)$ is at most 
    $$
    k \leq \frac{2+2\gamma}{3\gamma-1}.
    $$
\end{proof}

