\section{Proof of Theorem~\ref{thm:robust_protocol_general}}\label{apx:robust_protocol}

In this section, we prove the guarantees of our robust critical points protocol given below.

\input{Appendix/apx_robust_critical_points_protocol}
% \input{Algorithm/Algo_Robust_Protocol}
\begin{proof}[Proof of \Cref{thm:robust_protocol_general}]
    Let $h^*$ be the best classifier in the hypothesis class $\calH$ on the true labels $(X,f)$. We know that $h^*$ classifies $(X,f)$ with at most $L$ errors.
    Let $X_A^+$ and $X_A^-$ be the set of all positives and the set of all negatives reported by Alice, respectively. Let $f_A$ be this labeling function reported by Alice.
    Without loss of generality, we assume that all points in $X_A^+$ are true positive. 
    (If this assumption is false, Bob will identify any misclassified negative points within $X_A^+$, as these points are always sent to him for verification.)
    Next, define $\calH_{L}(X_A^+)$ be the set of all classifiers in $\calH$ that satisfies the following conditions: (a) the instance $(X,f_A)$ are not classified by this classifier with error at most $L$; and (b)
    there is a labeling $f'$ of $X$ such that all points in $X_A^+$ are labeled as positive, i.e.,  $f'(x) = +1$ for all $x\in X_A^+$ and the labeled dataset $(X,f')$ is classified by this classifier with error at most $L$. 
    Without ambiguity, we use $\calH_L$ to denote this hypothesis set. 
    
    We first show that the critical points $C_{L}(X_A^+)$ computed by Algorithm~\ref{alg:robust_critical} can distinguish two cases: (1) Alice's report $(X,f_A)$ is classified by the best classifier $h^*$ with error at most $L$; and (2) Alice labels some true positive points as negative. 
    Specifically, we show that there exists a true positive point in  $C_{L}(X_A^+)$ if the best classifier $h^*$ is in $\calH_L$. If there is a true positive in $C_{L}(X_A^+)$, then we are in case (2); otherwise, the best classifier $h^*$ is not in $\calH_L$, which means we are in case (1) since the condition (b) of $\calH_L$ is always satisfied by the best classifier $h^*$.
    
    Suppose the best classifier $h^*$ is $h \in \calH_L$. Let $E^- = \{x \in X_A^+: h(x) = -1 \}$ be the points in $X_A^+$ that are classified by $h$ as negative. Since $h \in \calH_L$, by the definition of $\calH_L$, we have $|E^-| \leq L$ since errors in $E^-$ can not be avoided by relabeling $X_A^-$.
    Let $E^+ = \{x \in X_A^-: h(x) = +1\}$ be the points in $X_A^-$ that are classified by $h$ as positive. We must have $|E^+| > L-|E^-| \geq 0$ since if $|E^+| \leq L-|E^-|$, then $(X,f_A)$ is classified by $h$ with error at most $L$, which contradicts $h \in \calH_L$. 
    Consider the last $L-|E^-|+1$ points in $E^+$ visited in Algorithm~\ref{alg:robust_critical}, denoted by $C_h$.
    When Algorithm~\ref{alg:robust_critical} visits these points, by flipping the label of the visited point, the classifier $h$ can classify the new instance with error at most $L$. Thus, all these $L-|E^-|+1$ points $C_h$ are not removed and are contained in critical points $C_{L}(X_A^+)$. 
    Since $h$ is the best classifier, there exists at least a true positive in these $L-|E^-|+1$ points $C_h$, otherwise, $h$ makes more than $L$ errors on the true labels. Thus, this implies there exists a true positive point in $C_{L}(X_A^+)$.

    We now show that the number of robust critical points in $C_{L}(X_A^+)$ is at most the robust Leave-One-Out dimension of the hypothesis class $\calH$. 
    Suppose $C_{L}(X_A^+)$ contains $k$ points $v_1,v_2, \dots, v_k \in X$. 
    We now show that for each $v_i$, there exists a classifier $h_i \in \calH$ satisfies two properties: (1) $v_i$ is classified as positive, i.e. $h(v_i) = +1$; and (2) there are at least $k-1-L$ points in $\{v_i\}$ are classified as negative, $\left|\{j\in[k]\colon  h(v_j) = -1\}\right|\geq k-1-L$.
    Consider the iteration corresponding to the point $v_i$ in Algorithm~\ref{alg:robust_critical}. Let $M_i$ be the set $M$ at the beginning of this iteration. Since $v_i$ is not removed, there is a classifier $h \in \calH$ that classifies $v_i$ as positive and has at most $L$ errors. Note that the critical points $C_L(X_A^+)$ is a subset of $M_i$. This classifier $h$ satisfies two properties for the point $v_i$.
    Therefore, the number of robust critical points is at most the robust Leave-One-Out dimension of the hypothesis class $\calH$.

    Finally, we bound the recall and nonresponsive disclosure. 
    We first show that the recall is at least $1-L/n^+$. 
    If Bob detects any documents misclassified by Alice, then all documents are disclosed to him, which implies a perfect recall, $\mathrm{Recall} = 1$. 
    If Bob does not detect any misclassified document, then by the above analysis, we are in case (1) Alice's report $(X,f_A)$ is classified by the best classifier with error at most $L$.
    Since points in $X_A^+$ are always sent to Bob, Alice can only hide true positive points as negative.
    We now show that Alice can hide at most $L$ true positives.
    Since the true labels are classified by the best classifier with at most $L$ errors, there are at most $L$ true positive points classified as negative by the best classifier.
    If Alice hides more than $L$ true positives as negative, then there exists a true positive point $x$ that is labeled as negative by Alice and is classified as positive by the best classifier.
    When Algorithm~\ref{alg:robust_critical} visits this point $x$, this point is considered a critical point in $C_L(X_A^+)$ since the best classifier satisfies the condition.
    Thus, Alice can hide at most $L$ true positives as negative.
    If Alice labels all documents correctly, then Bob will not detect any misclassified documents. Thus, the nonresponsive disclosure is the number of robust critical points, which is at most the robust Leave-One-Out dimension of the hypothesis class $\calH$. 
    % Let $S \subseteq X_A^-$ be the smallest set that satisfies for any classifier $h \in \calH_\gamma$, if $h$ classifies any point in $X_A^-$ as positive, then there exists a point in $S$ classified as positive by $h$. Suppose $S$ contains $k$ points $v_1,v_2, \dots, v_k \in \bbR^d$. For each $v_i$, there exists a classifier $h_i \in \calH_\gamma$ such that $h_i(v_i) = 1$ and $h_i(v_j) = -1$ for all $j \neq i$. If there is no such $h_i$, then we can remove $v_i$ from $S$ and get a smaller set satisfying the condition. 
    
    %Let $w_i \in \bbR^d$ be the unit-length weight vector of the classifier $h_i(x) = \sgn(\inner{w_i}{x})$. Let $\bar{v}_i = v_i/\|v_i\|_2$. Since $h_i$ has margin at least $\gamma$, we have $\inner{\bar{v}_i}{w_i} \geq \gamma$ and $\inner{\bar{v}_j}{w_i} \leq -\gamma$ for any $j \neq i$. 
    % Therefore by Lemma~\ref{lemma:generalized-skew-obtuse_lemma}, we have the number of points in $C_\gamma(X_A^+)$ is at most 
    % $$
    % k \leq \frac{(2+2L)(\gamma+1)}{3\gamma-1}.
    % $$
\end{proof}


% \begin{proof}[Proof of Theorem~\ref{thm:robust_protocol}]
%     The protocol first asks Alice to report labels of all documents in $X$ to Trent. Then, Trent checks whether the labels reported by Alice can be classified by a linear classifier with margin $\gamma$ and total error $L$. 
%     If there is no such linear classifier, then Trent reveals all documents to Bob. 
    
%     Now suppose the labels reported by Alice can be classified by a linear classifier with margin $\gamma$ and total error $L$. 
%     Let $X_A^+$ and $X_A^-$ be the set of all positives and the set of all negatives reported by Alice, respectively. Let $f_A$ be this labeling function reported by Alice.
%     Without loss of generality, we assume that all points in $X_A^+$ are true positive. 
%     (If this assumption is false, Bob will identify any misclassified negative points within $X_A^+$, as these points are always sent to him for verification.)
%     Next, define $\calH_{\gamma,L}$ be the set of all linear classifiers that satisfies the following conditions: (a) the instance $(X,f_A)$ are not classified by this linear classifier with margin $\gamma$ and total error $L$; and (b)
%     there is a labeling $f'$ of $X$ such that all points in $X_A^+$ are labeled as positive, i.e.,  $f'(x) = +1$ for all $x\in X_A^+$ and the labeled dataset $(X,f')$ is classified with margin $\gamma$ and total error $L$. 
%     Without ambiguity, we use $\calH$ to denote this hypothesis set. 
    
%     We first show that the critical points $C_{\gamma,L}(X_A^+)$ computed by Algorithm~\ref{alg:robust_critical} can distinguish two cases: (1) Alice's report $(X,f_A)$ is classified by the true classifier with margin $\gamma$ and total error $L$; and (2) Alice labels some true positive points as negative. 
%     Specifically, we show that there exists a true positive point in  $C_{\gamma,L}(X_A^+)$ if the true classifier is in $\calH$. If there is a true positive in $C_{\gamma,L}(X_A^+)$, then we are in case (2); otherwise, the true classifier is not in $\calH$, which means we are in case (1) since the condition (b) of $\calH$ is always satisfied by the true classifier.
    
%     Suppose the true classifier is $h \in \calH$. Let $w$ be the weight vector of this classifier $h$. We denote the margin error points of this classifier $h$ by $E_{\gamma} = \{x \in X: |\inner{\bar{x}}{w}| < \gamma \}$, where $\bar{x} = x/\|x\|$ is the normalized vector for $x$. Note that all margin errors do not depend on the labels of points. Let $E_{\gamma}^- = \{x \in X_A^+: \inner{\bar{x}}{w} \leq -\gamma \}$ be the points in $X_A^+$ that are classified by $h$ as negative and satisfy the margin condition. Since $h \in \calH$, by the definition of $\calH$, we have $|E_{\gamma} \cup E_{\gamma}^-| \leq L$ since errors in $E_{\gamma} \cup E_{\gamma}^-$ can not be avoided by relabeling $X_A^-$.
%     Let $E_{\gamma}^+ = \{x \in X_A^-: \inner{\bar{x}}{w} \geq \gamma\}$ be the points in $X_A^-$ that are classified by $h$ as positive and satisfy the margin condition. We must have $|E_{\gamma}^+| > L-|E_{\gamma} \cup E_{\gamma}^-| \geq 0$ since if $|E_{\gamma}^+| \leq L-|E_{\gamma} \cup E_{\gamma}^-|$, then $(X,f_A)$ is classified by $h$ with margin $\gamma$ and error $L$, which contradicts $h \in \calH$. 
%     Consider the last $L-|E_{\gamma} \cup E_{\gamma}^-|+1$ points in $E_{\gamma}^+$ visited in Algorithm~\ref{alg:robust_critical}, denoted by $C_h$.
%     When Algorithm~\ref{alg:robust_critical} visits these points, by flipping the label of the visited point, the classifier $h$ can classify the new instance with margin $\gamma$ and error $L$. Thus, all these $L-|E_{\gamma} \cup E_{\gamma}^-|+1$ points $C_h$ are not removed and are contained in critical points $C_{\gamma,L}(X_A^+)$. 
%     Since $h$ is the true classifier, there exists at least a true positive in these $L-|E_{\gamma} \cup E_{\gamma}^-|+1$ points $C_h$, which implies there exists a true positive point in $C_{\gamma,L}(X_A^+)$.

%     We now show that critical points in $C_{\gamma,L}(X_A^+)$ have a special structure, which forms a robust generalized skew-obtuse family of vectors. Suppose $C_{\gamma,L}(X_A^+)$ contains $k$ points $v_1,v_2, \dots, v_k \in X$. 
%     Let $\bar{v}_i = v_i / \|v_i\|$ be the normalized vector of $v_i$.
%     We now show that for each $v_i$, there exists a linear classifier $h_i$ with unit-length weight vector 
%     $w_i$ satisfies two properties: (1) $v_i$ is classified as positive and satisfies the margin condition, i.e. $\inner{\bar{v}_i}{w_i} \geq \gamma$; and (2) there are at least $k-1-L$ points in $\{v_i\}$ are classified as negative and satisfy the margin condition, $\left|j\in[k]\colon  \inner{\bar{v}_j}{w_i} \leq -\gamma\right|\geq k-1-L$.
%     Consider the iteration corresponding to the point $v_i$ in Algorithm~\ref{alg:abstract_critical_points_computation}. Let $M_i$ be the set $M$ at the beginning of this iteration. Since $v_i$ is not removed, there is a linear classifier $h$ that classifies $v_i$ as positive and has margin $\gamma$ and error $L$. Note that the critical points $C_\gamma(X_A^+)$ is a subset of $M_i$. This classifier $h$ satisfies two properties for the point $v_i$.
    
%     % Let $S \subseteq X_A^-$ be the smallest set that satisfies for any classifier $h \in \calH_\gamma$, if $h$ classifies any point in $X_A^-$ as positive, then there exists a point in $S$ classified as positive by $h$. Suppose $S$ contains $k$ points $v_1,v_2, \dots, v_k \in \bbR^d$. For each $v_i$, there exists a classifier $h_i \in \calH_\gamma$ such that $h_i(v_i) = 1$ and $h_i(v_j) = -1$ for all $j \neq i$. If there is no such $h_i$, then we can remove $v_i$ from $S$ and get a smaller set satisfying the condition. 
    
%     %Let $w_i \in \bbR^d$ be the unit-length weight vector of the classifier $h_i(x) = \sgn(\inner{w_i}{x})$. Let $\bar{v}_i = v_i/\|v_i\|_2$. Since $h_i$ has margin at least $\gamma$, we have $\inner{\bar{v}_i}{w_i} \geq \gamma$ and $\inner{\bar{v}_j}{w_i} \leq -\gamma$ for any $j \neq i$. 

    
%     Therefore by Lemma~\ref{lemma:generalized-skew-obtuse_lemma}, we have the number of points in $C_\gamma(X_A^+)$ is at most 
%     $$
%     k \leq \frac{(2+2L)(\gamma+1)}{3\gamma-1}.
%     $$
% \end{proof}