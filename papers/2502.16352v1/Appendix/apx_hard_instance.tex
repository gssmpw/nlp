\section{Proof of Lemma~\ref{lemma:skew-obtuse_lemma}}\label{apx:constructing_skob_family}

% In this section, we prove the Lemma~\ref{lemma:constructing_skob_family}

\begin{proof}
    Let $V= (\bar{v}_1,\bar{v}_2,\cdots, \bar{v}_k)^T$ be a $k\times d$ matrix. 
    Similarly, we define $W = (w_1,w_2,\cdots, w_k)^T$ be a $k\times d$ matrix. Then, we have $VW^T$ is a $k\times k$ matrix, where each entry is $\inner{\bar{v}_i}{w_j}$. 
    Let $\{a_1, a_2, \dots, a_d\}$ be the $d$ column vectors of matrix $V$. Let $\{b_1,b_2,\dots, b_d\}$ be the $d$ column vectors of $W$.

    We first consider the trace of matrix $VW^T$. Since each diagonal entry of $VW^T$ satisfies $\inner{\bar{v}_i}{w_j} \geq \gamma$, we have 
    $$
    \Tr(VW^T) = \sum_{i=1}^k \inner{\bar{v}_i}{w_i} \geq k \gamma.
    $$
    Since $\bar{v}_i$ and $w_i$ are all unit vectors, we have $\Tr(VW^T) \leq k$.
    We also have 
    $$
    \Tr(VW^T) = \Tr(WV^T) = \sum_{i=1}^d \inner{a_i}{b_i} = \sum_{i=1}^d \|a_i\|\|b_i\|\cos(\theta_i),
    $$
    where $\theta_i$ is the angle between vectors $a_i$ and $b_i$. Hence, we get
    \begin{equation}\label{eq:trace}
    \sum_{i=1}^d \|a_i\|\|b_i\|\cos(\theta_i) = \Tr(VW^T) \geq k\gamma.
    \end{equation}

    We now consider the sum of all entries in matrix $VW^T$. Since all off-diagonal entries of matrix $VW^T$ satisfies $\inner{\bar{v}_i}{w_j} \leq -\gamma$. We use $\one$ to denote all one vector. Then, we have the sum of all entries is
    $$
    \one^T VW^T \one = \Tr(VW^T) + \sum_{i\neq j} \inner{\bar{v}_i}{w_j} \leq \Tr(VW^T) - k(k-1)\gamma.
    $$
    Note that $VW^T = \sum_{i=1}^d a_i b_i^T$. Thus, we have $\one^T VW^T \one = \sum_{i=1}^d \inner{\one}{a_i} \inner{b_i}{\one}$. Let $\alpha_i$ be the angle between $\one$ and $-a_i$ and $\beta_i$ be the angle between $\one$ and $b_i$. Then, we have 
    $$
    - \one^T VW^T \one = \sum_{i=1}^d \inner{\one}{-a_i} \inner{b_i}{\one} = \sum_{i=1}^d \|\one\|^2 \|a_i\|\|b_i\| \cos(\alpha_i) \cos(\beta_i). 
    $$
    Since the angle between $-a_i$ and $b_i$ is $\pi - \theta_i$, we have $\alpha_i + \beta_i \geq \pi - \theta_i$. Since the cosine function is decreasing and log-concave on $[0,\pi/2]$, and negative in $[\pi/2, \pi]$, we have 
    $$
    \cos(\alpha_i) \cos(\beta_i) \leq \cos^2\left(\frac{\pi-\theta_i}{2}\right).
    $$
    By combining the equations above, we have
    \begin{equation}\label{eq:all_sum}
    k \sum_{i=1}^d \|a_i\|\|b_i\| \cos^2\left(\frac{\pi-\theta_i}{2}\right) \geq - \one^T VW^T \one \geq - \Tr(VW^T) + k(k-1)\gamma \geq - k + k(k-1)\gamma,
    \end{equation}
    where the last inequality is due to $\Tr(VW^T) \leq k$.

    By combining Equations~(\ref{eq:trace}) and~(\ref{eq:all_sum}), we have 
    $$
    \sum_{i=1}^d \|a_i\|\|b_i\| = \sum_{i=1}^d \|a_i\|\|b_i\| \left(2\cos^2\left(\frac{\pi-\theta_i}{2}\right) + \cos(\theta_i)\right) \geq 3k\gamma -2 - 2\gamma.
    $$
    By Cauchy-Schwarz inequality, we have 
    $$
    \left(\sum_{i=1}^d \|a_i\|\|b_i\| \right)^2 \leq \sum_{i=1}^d \|a_i\|^2 \cdot \sum_{i=1}^d \|b_i\|^2 = k^2,
    $$
    where the last equality is from $\sum_{i=1}^d \|a_i\|^2 = \sum_{i=1}^d \|b_i\|^2 = k$ since matrices $V$ and $W$ are both consists of $k$ unit row vectors. Therefore, we have 
    $$
    k \leq \frac{2+2\gamma}{3\gamma-1}.
    $$

    For the lower bounds see below.
\end{proof}

\begin{proof}[Construction of skew-obtuse family of vectors]
     Let $\varepsilon = 1/\sqrt{3}$.
    We first consider the case when $\gamma =1/3$.
   The dataset $X$ contains $n$ data points $x_1,\dots,x_n$ in $\bbR^{n+1}$ such that 
    $$
    x_i = \varepsilon \cdot e_1 + \sqrt{1-\varepsilon^2} \cdot e_{i+1},
    $$
    where $e_i$ is the $i$-th standard basis vector. Each data point $x_i$ has a unit length. 

    Consider the following $n$ different label functions $f_1,\dots, f_n$ on this dataset $X$. The function $f_i$ labels the point $x_i$ as positive and all other points as negative. 
    Let $h_i$ be the linear classifier with the weight vector $w_i = -\varepsilon \cdot e_1 + \sqrt{1-\varepsilon^2} \cdot e_{i+1}$. Then, we have $\inner{h_i}{x_i} = 1-2\varepsilon^2  \geq \gamma$ and $\inner{h_i}{x_j} = -\varepsilon^2  \leq -\gamma$ for all $j \neq i$. 
    Thus, each instance $(X,f_i)$ is linear separable by margin $\gamma$ and $h_i$ is a $\gamma$-margin classifier for this instance $(X,f_i)$. Therefore, to distinguish these instances and achieve recall $1$, any protocol must reveal all data points for verification.

    Next we consider the case when $\gamma = 1/3 - \eta$ for some $\eta >0$.
    Let $\beta = (3/2)\eta$.
    It is well-known that by picking unit vectors randomly in $\R^d$ we can construct a collection of vectors $v_1, v_2, \ldots v_k$ such that $\inner{v_i}{v_j} \leq \beta$ if $i\neq j$ and $k\geq \exp{(\Omega(\beta^2 d))}$ (see \Cref{lem:exp_many_neearly_orthogoanl_vectors} below for a proof). 
    WLOG we can assume that $v_1,\ldots,v_k$ are in the span of $e_2,\ldots,e_{d+1}$. Now, for all $i\in [k]$ we set 
    $$
    x_i = \varepsilon \cdot e_1 + \sqrt{1-\varepsilon^2} \cdot v_{i}.
    $$
    Further, by setting $w_i = -\varepsilon \cdot e_1 + \sqrt{1-\varepsilon^2} \cdot v_i$ we have that 
    \[
    \inner{v_i}{w_i} = -\varepsilon^2 + 1 - \varepsilon^2 = 1/3 \geq \gamma
    \]
    and for $i\neq j$
    \[
    \inner{v_i}{w_j} \leq -\varepsilon^2 + (1 - \varepsilon^2)\eta = -1/3 + \eta = -\gamma.
    \]
\end{proof}