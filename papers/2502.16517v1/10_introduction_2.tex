% This is rephrased section 1. The original is good, but I wanted to see if it read better if it was a bit more verbose. I'd like to hear what you think of this version

In modern supercomputers, data movement, rather than raw computation, is often the dominant factor influencing runtime. On contemporary compute nodes, transferring data through the cache hierarchy frequently incurs greater latency than performing arithmetic operations on that data. Similarly, offloading computations to accelerators is routinely bottlenecked by data transfer overheads. These penalties are further exacerbated by irregular or scattered memory access patterns, which reduce cache efficiency and increase latency.

Despite advances in modern algorithm design—such as the adoption of higher-order schemes aimed at reducing memory pressure—many scientific codes remain constrained by memory bandwidth rather than compute throughput. Computational paradigms such as multiscale modelling and adaptive meshing amplify this effect, as they inherently increase memory footprint while often remaining arithmetically "lightweight."

Crucially, these trends do not align with the evolution of memory architectures, where bandwidth and latency constraints remain a fundamental challenge. Moreover, the longstanding gap between data movement speed and computational throughput is not narrowing—it is widening~\cite{Dongarra:2011:ExascaleSoftwareRoadmap}. Understanding and mitigating memory access inefficiencies is therefore critical for improving the performance of modern high-performance computing applications.

Modern high-level languages, particularly C and C++, risk exacerbating memory-related performance issues due to their approach to data modelling. These languages provide structs (or classes in an object-oriented context) as fundamental building blocks for representing entities such as particles, mesh cells, or entries in multiphysics partial differential equations. Structs are a natural abstraction for many algorithms, offering a convenient way to group logically related attributes while maintaining a clean and modular software design. They also facilitate data organisation in key operations such as message passing, sorting, and adaptive mesh refinement~\cite{Hirzel:2007:DataLayoutForOO,Homann:2018:SoAx,Reinders:2016:XeonPhi,Jubertie:2018:DataLayoutAbstractionLayers}.

Because structs encapsulate multiple attributes within a single entity, C++ naturally encourages an array-of-structs (AoS) data layout when handling sequences of objects. This layout is advantageous in scenarios where entire entities need to be processed as a unit. Sorting algorithms, for example, benefit from AoS since comparisons and swaps operate on complete objects, preventing excessive pointer dereferencing and reducing cache misses. Message passing between distributed processes also favours AoS layouts, as each struct can be treated as a contiguous block of memory, allowing efficient packing and transmission of complete data structures without the need for complex scatter-gather operations. Similarly, adaptive mesh refinement and load balancing workflows often require relocating entire entities between processors, which is more straightforward when using an AoS layout.

However, the advantages of AoS come at a cost in compute-intensive kernels. If only a subset of a struct’s fields is accessed within a tight numerical loop, cache efficiency is reduced since each cache line fetch brings in unused data. This effect is particularly detrimental on architectures where memory bandwidth is a bottleneck, and caches are small. Additionally, AoS layouts tend to produce non-coalesced memory accesses, which challenge vectorisation and reduce throughput on SIMD (Single Instruction, Multiple Data) architectures.

Thus, while AoS is well-suited for sorting, communication, and data redistribution, it is often suboptimal for numerical computations. Addressing these inefficiencies frequently requires explicit restructuring of data layouts—such as transitioning to a struct-of-arrays (SoA) representation—where memory access patterns and compute efficiency are critical considerations.

Since developers cannot alter hardware constraints and typically prefer not to rewrite existing AoS-based code, we propose a temporal data reorganisation strategy: data is dynamically transformed between array-of-structs (AoS) and struct-of-arrays (SoA) representations at runtime, depending on the computational task at hand. Just before entering a compute-intensive phase, the relevant data is extracted from AoS into a temporary SoA representation, allowing more efficient vectorisation and better cache utilisation. Once computations are completed, the data is converted back to AoS. This approach is applied adaptively across mesh traversals, particle systems, and other hierarchical data structures, where compute kernels process data in portions over multiple iterations.

At first glance, this strategy seems counterintuitive—introducing additional memory movement in a performance-critical regime. However, implementations using SoA often outperform their pure AoS counterparts once they facilitate vector instruction usage~\cite{Gallard:2020:ExaHyPEVectorisation,Intel:MemoryLayoutTransformations,Springer:2018:SoALayout} and become less sensitive to cache inefficiencies~\cite{Homann:2018:SoAx,Hundt:2006:StructureLayoutOptimisation,Springer:2018:SoALayout}. The effectiveness of transformation is problem-dependent; it does not universally improve performance. Instead, we provide a language-level mechanism that allows developers to trigger transformations manually, enabling fine-tuned performance optimisations based on empirical measurements.

To help guide these optimisations, we identify key heuristics: performance gains are likely when a kernel exhibits nested loops, cannot be efficiently vectorised, or operates on only a subset of struct attributes. To further reduce transformation overheads, we introduce the concept of data views: instead of reordering entire struct instances, we extract and reorder only the attributes accessed within a specific computational kernel. This minimises cache pollution and reduces unnecessary memory transfers. While views are introduced as a conceptual tool in this work, their implementation details will be discussed later.

The significance of this approach increases as the fraction of struct attributes accessed by a kernel decreases. In our most performance-critical kernels, computations typically utilise only 30–50\% of the available struct data, making selective reordering via views particularly beneficial. By enabling adaptive, user-controlled data layout transformations, we aim to bridge the gap between the software-centric preference for AoS and the hardware-driven performance benefits of SoA, offering a flexible solution to memory access inefficiencies in high-performance computing.

To make data layout transformations practical and easy to adopt, they must integrate smoothly into existing codebases. The approach should be minimally invasive, avoiding disruptive code rewrites that ripple through the software stack. At the same time, core computational routines—particularly those encoding domain-specific knowledge, such as physical models—should remain agnostic to data layout, ensuring they can be used in both AoS and SoA contexts without modification. Ideally, the entire transformation process should be transparent to the programmer, handling data reorganisation automatically rather than requiring manual intervention. Finally, any transformation must be performance-optimal, ensuring that unnecessary data movements are avoided and the cost of reordering remains lower than the computational benefits it enables.

To achieve this, we generalise the concept of data views, distinguishing between read and write attributes while abstracting away the underlying storage format. Rather than burdening the programmer with explicit data transformations, we delegate these operations to the compiler. The compiler is instructed via lightweight user annotations in C/C++, which indicate where transformations should be applied. Once specified, the compiler constructs and manages views automatically, ensuring that data conversions occur efficiently without requiring additional programmer effort.

These annotations are designed to apply only to local blocks of (nested) loops, where data layout transformations are most beneficial. By keeping the scope limited, we ensure that optimisations remain targeted and context-aware, preserving the clarity and maintainability of the code while improving performance where it matters most.

Our approach of guided data layout conversion via annotations differs from existing techniques that rely on either fully manual (user-driven) transformations or fully automatic heuristics-based workflows~\cite{Jubertie:2018:DataLayoutAbstractionLayers,Xu:2014:SemiAutomaticComposition}. Unlike fully automatic methods, which face the difficult challenge of determining when a transformation is beneficial, we leave this decision to the developer while automating how the conversion is performed. This strikes a balance between flexibility and ease of use, allowing developers to apply their domain knowledge while offloading the technical complexities of data layout transformation to the compiler.

By leveraging compiler-driven automatic conversion, our approach ensures that developers can write memory-layout-agnostic code, where the underlying data structure can even vary dynamically across different execution contexts. One alternative to achieve such separation between programming and memory layout is C++ template metaprogramming combined with specialised containers~\cite{Homann:2018:SoAx,Reinders:2016:XeonPhi,Jubertie:2018:DataLayoutAbstractionLayers,Springer:2018:SoALayout,Strzodka:2011:AbstractionSoA}. However, most of these approaches impose a static, global data layout—once the storage format is determined, it remains fixed throughout execution.

In contrast, our approach introduces a temporal, local data layout transformation. Here, data layout changes only within the lifespan of a loop, avoiding complications such as persistent storage state tracking and aliasing issues. Similar dynamic data structure transformations within a single program have been explored in prior work~\cite{Gallard:2020:ExaHyPEVectorisation,Vikram:2014:LLVM}. However, no existing language-centric approach has demonstrated the ability to automatically identify data views—that is, to determine which struct attributes are relevant to a given computational kernel and selectively reorder only those.

This automatic view identification is a major advantage of our compiler-based approach and extends beyond preparatory work~\cite{Radtke:2024:AoStoSoA}. Traditional solutions require either explicit manual selection of attributes (placing the burden on the developer) or rely on static, global transformations that do not adapt to the fine-grained needs of individual loops. By contrast, our method allows the compiler to analyze access patterns within computational kernels, automatically construct minimal, optimised views, and apply transformations only to the necessary data, reducing memory traffic and improving cache efficiency. Crucially, this process requires no user interaction and does not increase the syntactic complexity of the code, as it is realised purely through compiler passes~\cite{Xu:2014:SemiAutomaticComposition}.

Furthermore, while many prior works focus on regular AoS-to-SoA transformations, our technique extends beyond pure AoS arrays to handle fully scattered data structures, such as arrays of pointers to structs. This generality allows our approach to be applicable across a wider range of high-performance computing scenarios, including irregular and unstructured data layouts.

Although this transformation is primarily motivated by vectorisation, we demonstrate for the first time that guided data layout conversion is also essential for efficient GPU offloading. By ensuring that memory layouts align with the access patterns required for accelerator-friendly execution, our approach enables substantial performance improvements on GPUs while maintaining a clean and portable programming model.

By separating storage format concerns from algorithm development and pushing all data conversions into the compiler, we eliminate the need for developers to modify their baseline code while still enabling performance-critical transformations. This compiler-assisted approach allows for fine-grained, context-aware transformations without requiring programmers to explicitly manage data layout changes. As a result, we provide an intuitive and flexible mechanism for improving memory access efficiency while preserving the structure and readability of scientific code.

To demonstrate the potential of our approach, we apply it to selected compute kernels from a Smoothed Particle Hydrodynamics (SPH) code~\cite{Schaller:2024:Swift}. SPH is an ideal candidate for this analysis, as its individual interactions can be either memory-bound or compute-intensive, and it frequently requires reordering and exchanging particles between nodes. While SPH algorithms rely heavily on AoS for simplicity and clarity, this can result in inefficiencies when memory access patterns do not align with modern hardware capabilities. Notably, we ensure that our transformations do not interfere with critical algorithmic phases, such as particle boundary exchange or sorting, which rely on AoS layout.

In typical SPH simulations, a large portion of runtime is spent in a few computationally intensive kernels. Given that these kernels are often memory-bound, we would expect the automatic data layout transformations to deliver immediate performance benefits. However, our observations challenge this assumption. In some cases, the automatic transformations require the developer to carefully investigate which code segments vectorise effectively. In fact, a naive or unoptimised code version might fail to benefit from either SoA or AoS layouts.

Nonetheless, once developers are willing to explore the transformations in more depth, our results defy the traditional belief that data reorganisation only yields benefits for Stream-like kernels~\cite{McCalpin:1995:Stream} or large arrays~\cite{Strzodka:2011:AbstractionSoA}. Additionally, our findings contradict the notion that temporal data reordering before loops or computational kernels is always detrimental~\cite{Hundt:2006:StructureLayoutOptimisation,Intel:MemoryLayoutTransformations}. In fact, we demonstrate that guided temporal data transformations can yield significant speedups, even for complex, non-trivial kernels.

Despite its simplicity, our approach does not replace manual performance engineering but rather enables more systematic experimentation with data layouts. By separating algorithm development from performance tuning, we provide developers with a mechanism to explore different memory layouts without permanently restructuring their code~\cite{Gallard:2020:Roles}. However, the responsibility remains with the developer to identify where transformations should be applied, making annotation placement a key performance-tuning task.

Our SPH studies suggest that the decision to apply AoS-to-SoA transformations cannot be made in isolation—it must be considered in the broader context of other code properties. While our transformations can facilitate vectorisation, they do not automatically guarantee optimal performance without additional manual adjustments. This underscores the reality that performance tuning in high-performance computing is multi-faceted, requiring developers to analyse memory access patterns, vectorisation potential, and overall computational intensity in tandem.

Beyond SPH, our findings are relevant to a wide range of performance-critical applications. More broadly, our approach prototypes how future C/C++ annotations could redefine performance engineering. Instead of intrusive code refactoring~\cite{Fowler}, performance-critical transformations could be integrated into the compilation process, allowing developers to focus their efforts on algorithmic development and high-level analysis rather than low-level memory optimisations. This aligns with the spirit of modern C++ annotations, where compiler directives help bridge the gap between performance concerns and maintainable software design.

The remainder is organised as follows:
We start with a use case (Section~\ref{section:demonstrator}) that motivates our work but also allows us to formalise the concepts employed from a developer's perspective. Next we introduce our code annotations, and discuss their semantics (Section~\ref{section:annotations}). This allows us to describe the realisation of the annotations in Section~\ref{section:realisation}, before we finally study their impact in Section~\ref{section:results}.
A brief discussion and outlook in Section~\ref{section:conclusion} close the discussion.
