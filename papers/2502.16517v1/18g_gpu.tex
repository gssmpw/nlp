\subsection{GPU offloading}

We conclude our studies with offloading kernels to the GPU.
As our SPH kernels are well-defined and atomic, this can be done through annotations only, i.e.~we do not have to employ additional OpenMP pragmas.
With the Grace-Hopper superchip, we have two variants available how to manage the data transfer:
We can copy over data explicitly via map clauses, or we can use the hardware's shared memory paradigm which translates into a lazy memory transfer similar to a cache miss.


\begin{figure}[htb]
\centering
    \includegraphics[width=1.00\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soatrue-cell0064-predfalse-symfalse-gputrue-legend.pdf}
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soatrue-cell0064-predfalse-symfalse-gputrue.pdf}
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soatrue-cell0128-predfalse-symfalse-gputrue.pdf}     
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soatrue-cell0256-predfalse-symfalse-gputrue.pdf}     
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soatrue-cell1024-predfalse-symfalse-gputrue.pdf}     
  \caption{
   	Cost per particle update for various kernels on Hopper GPU. 
   	All GPU kernels are generated through attribute annotations and transfer their data explicitly to the accelerator prior to the kernel start. 
   	In lexicographic order: $ppc=64$, $ppc=128$, $ppc=256$, and $ppc=1024$.
   \label{figure:results:gpu}
  }
\end{figure}



\begin{figure}[htb]
\centering
    \includegraphics[width=1.00\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soatrue-cell0064-predfalse-symfalse-gputrue-legend.pdf}
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soatrue-cell0064-predfalse-symfalse-gputrue-shmem.pdf}
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soatrue-cell0128-predfalse-symfalse-gputrue-shmem.pdf}     
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soatrue-cell0256-predfalse-symfalse-gputrue-shmem.pdf}     
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soatrue-cell1024-predfalse-symfalse-gputrue-shmem.pdf}     
  \caption{
    Experiments from Figure~\ref{figure:results:gpu} repeated with shared memory, i.e.~without any explicit memory transfer.
   \label{figure:results:gpu-shmem}
  }
\end{figure}

%
% What we see
%
The GPU throughput is low compared to the theoretical capability of the device, but the data is qualitatively similar to the results on the Intel and ARM CPUs (Figure~\ref{figure:results:gpu}) in that larger total particle counts give us a better throughput.
We see a less pronounced relation of throughput to $ppc$, but there is an inversion of the previously stated trend:
Bigger $ppc$ give us superior time-to-solution.
The Grace Hopper's shared memory capabilities give us another factor of two in performance (Figure~\ref{figure:results:gpu-shmem}).




%
% Interpretation
%
GPUs require high concurrency levels in the compute kernels to unfold their full potential.
In return, they are not as vulnerable to the branching due to the distance checks as the vectorised CPU kernels:
The masking facilities on the accelerator can compensate for the thread divergence.
Therefore, picking larger $ppc$ becomes reasonable.

Despite the tight memory connection on the Grace-Hopper superchip, we see a limited performance improvement in our kernels, which is due to the low concurrency.
It is not clear from the present setups, to which degree our data suffers from a lack of accelerator-specific tuning, such as the tailoring of warp sizes or simply too many, too small kernels.
Unless the GPU software stack or the hardware start to facilitate very small kernels, a direct mapping of compute kernels onto GPU kernels does not deliver satisfying results.

Our vanilla offloading in the compiler issues a sequence of three tasks: map, compute, map back.
They are connected through dependencies yet launched asynchronously.
After the final map, we wait.
If we enable the superchip's shared memory, the three tasks degenerate to one single blocking tasks.
Nevertheless, it seems that the chip delivers better performance in this mode.
We assume that OpenMP struggles to overlap the data transfer of all CPU threads firing map-compute-map sequences simultaneously.
Contrary, the hardware seems to succeed to let shared memory tasks run ahead and to bring in all required data lazily upon demand.

Overall, our annotations let the kernels on the CPU outperform their GPU counterpart. 
All data conversation resides on the CPU.
We may therefore conclude that any improvement of compute speed on the accelerator is (partially) consumed by data transfers or on-GPU overheads.
It is also not clear what role the inferior support for FP64 on the GPU plays for out experiments.
It is reasonable to believe that the insight might change qualitatively if we were able to convert our kernels to FP32---an endeavour which requires care as it tends to introduce numerical long-term instabilities.



\begin{observation}
 Our annotations streamline GPU offloading as they tackle the memory bottleneck, but they do not free the developer from generating tailored compute kernels with a sufficiently high concurrency level. 
\end{observation}


\noindent
It is well-know that GPUs benefit from gathering multiple compute tasks into one larger GPU kernel \cite{Nasar:2024:GPU}.
Similar techniques have been proposed in a more general way for task-based systems~\cite{LiShultzWeinzierl:2022:Tasking}.
Our data suggest that using such techniques are absolutely essential to write fast GPU code, and that the naive offloading of individual, tiny compute kernels does not yield satisfying performance.
However, the core idea of reordering data through compiler annotations streamlines the writing of more complex (meta-)kernels as well.


