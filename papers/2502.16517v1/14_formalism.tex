\section{Formal code transformations and algorithm description}
\label{section:formalism}


Let $D = [S_0,S_1,S_2,\ldots]$ be an array of objects, i.e.~instances of structs.
As we store data as AoS, the memory holds $[S_0.a_0,S_0.a_1,S_0.a_2,\ldots,S_1.a_0,S_1.a_1],\ldots$, where the data members $a_i$ span the set $\mathbb{A}$.
$ D \gets f(D)$ represents the loop body of interest.
Function and surrounding loop together define the overall compute kernel.
Without loss of generality, we encounter

\begin{eqnarray*}
  S_j = [S_j.a_0, S_j.a_1, \ldots] & \gets & f([S_j.a_0, S_j.a_1, \ldots]) \qquad \text{and} \\
  S_j = [S_j.a_0, S_j.a_1, \ldots] & \gets & \sum _{i\not=j} f([S_j.a_0, S_j.a_1, \ldots],[S_i.a_0, S_i.a_1, \ldots])
\end{eqnarray*}

\noindent
for our compute kernels of linear complexity or in $\mathcal{O}(N^2)$, respectively.
The loop body itself hosts exclusively unary and binary operations.


\subsection{Data subset construction}

Let $f$ of a loop body be completely visible, i.e. no external functions are called, either directly or indirectly, in the loop body.
We construct the sets $\mathbb{A}_{\text{in}} \subseteq \mathbb{A}$ and $\mathbb{A}_{\text{out}} \subseteq \mathbb{A}$ by running through the loop body line by line and inspecting all read/write operations on the AoS data.
For any operation

\begin{equation}
 x \gets \psi(y_0,y_1,\ldots)
 \label{equation:formalism:assignment}
\end{equation}

\noindent
with $y_i \in \mathbb{A}$ from the traversal container, we add the data members to the set 
\[
 \mathbb{A}_{\text{in}} \gets \mathbb{A}_{\text{in}} \cup \{ y_i \}.
\]

\noindent
Analogously, $x$ is added to the out set if it refers to one of the data members of the container:
\[
 \mathbb{A}_{\text{out}} \gets \mathbb{A}_{\text{out}} \cup \{ x \} \quad \text{if} \quad x \in \mathbb{A}.
\]


\noindent
In practice, a less strict representation of the loop body can be used, i.e.~we do not have to restrict outselves to expressions of the form (\ref{equation:formalism:assignment}).

Let $f$ invoke a subroutine $f_{\text{sub}}$ with $f_{\text{sub}}$ being available (visible), too.
In this case, we can recursively apply our analysis to $f_{\text{sub}}$ to construct a $\mathbb{A}_{\text{in}}^{f_{\text{sub}}}$ and a $\mathbb{A}_{\text{out}}^{f_{\text{sub}}}$.
Once complete, we fuse the sets

\begin{eqnarray}
\mathbb{A}_{\text{in}} & \gets & \mathbb{A}_{\text{in}} \cup \mathbb{A}_{\text{in}}^{f_{\text{sub}}}
\quad \text{and} 
\label{equation:formalism:union-over-sets}
\\
\mathbb{A}_{\text{out}} & \gets & \mathbb{A}_{\text{out}} \cup \mathbb{A}_{\text{out}}^{f_{\text{sub}}}.
\nonumber
\end{eqnarray}

\noindent
We exhaustively traverse the call graph.
Recursive or re-occuring function calls are covered by this formalism once we recursively analyse if and only if the resulting union sets in 
(\ref{equation:formalism:union-over-sets}) continue to grow.



\subsection{Operations}

Let $N_{\mathbb{A}_{\text{in}} \cup \mathbb{A}_{\text{out}}}$ be the operation that narrows down the struct members to the ones read and written by a loop body.
For example, if $f$ reads $\mathbb{A}_{\text{in}} = \{ S_i.a_7 \}$ only and writes into $\mathbb{A}_{\text{out}} = \{S_i.a_3\}$, then 
\[
  [S_i.a_3,S_i.a_7] = N_{\mathbb{A}_{\text{in}} \cup \mathbb{A}_{\text{out}}}([S_i.a_0,S_i.a_1,\ldots])
\]


\noindent
for a single struct.
We can canonically apply it to a sequence of objects.


The operation $W_{\mathbb{A}_{\text{out}}}$ widens the object again. 
We use the notation such that 

\[
  D = \left( W_{\mathbb{A}_{\text{out}}} \circ N_{\mathbb{A}_{\text{in}} \cup \mathbb{A}_{\text{out}}} \right) (D),
\]

\noindent
and our optimisation is based upon the idea that 

\begin{equation}
  f(D) = \left( W_{\mathbb{A}_{\text{out}}} \circ f \circ N_{\mathbb{A}_{\text{in}} \cup \mathbb{A}_{\text{out}}} \right) (D),
    \label{equations:narrowing-widening}
\end{equation}

\noindent
i.e.~that we can pick out all the read and written data from $D$, apply (a rewritten) $f$ to these temporary values, and eventually mirror the changes back into the container holding $D$.
Let

\begin{eqnarray}
  \lbrack S_0.a_0, S_1.a_0, S_3.a_0, \ldots , S_1.a_0, \ldots \rbrack 
    & \gets & 
    C_{AoS}^{SoA}( D ) 
    \label{equations:AoStoSoA}
    \\
  \lbrack S_0.a_0,S_0.a_1,S_0.a_2,\ldots,S_1.a_0,\ldots \rbrack 
    & \gets & (C_{SoA}^{AoS} \circ C_{AoS}^{SoA})( D ). \nonumber 
\end{eqnarray}


\noindent
formalise the conversion from AoS into SoA with 
$ C_{SoA}^{AoS} = \left( C_{AoS}^{SoA} \right)^T$.
The realisation of $f$ over the whole array can now be broken down into three steps:
A conversion into SoA, then the explicit evaluation of the loop body, followed by a conversion back into AoS.
The notion of a view combines these operations (\ref{equations:narrowing-widening}) and (\ref{equations:AoStoSoA}) into

\begin{eqnarray}
  f(D) = \left( W_{\mathbb{A}_{\text{out}}} \circ C_{SoA}^{AoS} \circ \hat f \circ C_{AoS}^{SoA} \circ N_{\mathbb{A}_{\text{in}} \cup \mathbb{A}_{\text{out}}} \right) (D).
  \label{equation:view}
\end{eqnarray}
  


\noindent
The right-hand side replaces the original user code $f(D)$.
While permuting the narrowing and SoA conversion in (\ref{equation:view}) would preserve semantics, it makes no sense from an implementation perspective.
In practice, these two functions collapse into a single operation requiring memory gathering, and we would always only work on the data ``left over'' by the narrowing:
it is unreasonable first to convert all data into SoA and then to pick from the outcome.
The epilogue $W_{\mathbb{A}_{\text{out}}} \circ C_{SoA}^{AoS}$ introduces scattered memory accesses.
Again, it makes no sense to permute the operators.
Indeed, widening is not properly defined over data formats of differing types.
While $C_{SoA}^{AoS}$ and $\left( C_{SoA}^{AoS} \right) ^T$ can be realised in-situ, the conversion in combination with the narrowing and widening requires us to hold the converted data in some explicit storage $\hat D$.


\subsection{GPU offloading}

If we deploy a loop onto the accelerator with memory address translation as we find it in OpenMP, we either have to copy the data onto the accelerator prior to the loop or we rely on shared memory.
We obtain

\begin{eqnarray}
  f(D) = \left( W_{\mathbb{A}_{\text{out}}} \circ C_{SoA}^{AoS} \circ M_{GPU}^{CPU} \circ \hat f_{GPU} \circ M_{CPU}^{GPU} \circ C_{AoS}^{SoA} \circ N_{\mathbb{A}_{\text{in}} \cup \mathbb{A}_{\text{out}}} \right) (D),
  \label{equation:gpu}
\end{eqnarray}

\noindent
where $M_{GPU}^{CPU}$ moves data from the host to the GPU.
We explicitly use OpenMP routines.
If we use shared memory, these moves are implicitly triggered by the hardware.
We can omit $M_{GPU}^{CPU}$ and $M_{CPU}^{GPU}$.


$M_{CPU}^{GPU} = \left( M_{GPU}^{CPU} \right)$ holds if and only if $\mathbb{A}_{\text{in}} = \mathbb{A}_{\text{out}}$.
Otherwise, each operation works only on its relevant subset.
We note that $M_{CPU}^{GPU} \circ C_{AoS}^{SoA} \circ N_{\mathbb{A}_{\text{in}}}$ provides space for optimisation, as we can parameterise the device moves over the narrowed set:
Only data in $\mathbb{A}_{\text{in}}$ has to be copied to the accelerator, while we can allocate $\mathbb{A}_{\text{out}} \setminus \mathbb{A}_{\text{in}}$ on the accelerator without any additional data movements.


Given the observation that memory movements become a limiting factor in GPU utilisation, the order of operations in (\ref{equation:gpu}) is natural.
However, it is not clear that this order is optimal in all cases.
For very tightly integrated future GPU-CPU solutions,


\begin{eqnarray*}
  f(D) & = & \left( M_{GPU}^{CPU} \circ W_{\mathbb{A}_{\text{out}}} \circ C_{SoA}^{AoS} \circ \hat f  \circ C_{AoS}^{SoA} \circ N_{\mathbb{A}_{\text{in}} \cup \mathbb{A}_{\text{out}}} \circ M_{CPU}^{GPU} \right) (D)
  \qquad \text{or}
  \\
       & = & \left( W_{\mathbb{A}_{\text{out}}} \circ M_{GPU}^{CPU} \circ C_{SoA}^{AoS} \circ \hat f  \circ C_{AoS}^{SoA} \circ M_{CPU}^{GPU} \circ N_{\mathbb{A}_{\text{in}} \cup \mathbb{A}_{\text{out}}} \right) (D)
\end{eqnarray*}

\noindent
might be reasonable alternatives to explore---in particular if the AoS--to--SoA conversions can be done in-situ by the GPU.


Kernel memory mapping in OpenMP requires three key ingredients: the buffer, its size, and the mapping type, i.e.~in, out or inout. 
$M_{CPU}^{GPU}$ and $M_{GPU}^{CPU}$ have direct access to these three properties.
The buffer's location is given by the container, 
its size is known due to C++ container \texttt{size()}, 
and the mapping type results from the access pattern analysis. 
