\section{Introduction}
\label{section:introduction}
 
 
%
% Context and bigger picture: Hardware
%
Data movements dominate the runtime on supercomputers.
On a state-of-the-art compute node, moving data through the cache hierarchy often lasts significantly longer than the actual arithmetics, while accelerator offloading notoriously suffers from data transfer overheads.
Data movement penalties are amplified by irregular and scattered memory access.
Despite efforts in modern algorithmics to mitigate this problem (e.g.~by switching to higher-order schemes), many codes remain memory-bound.
Computational paradigms such as multiscale modelling and adaptive meshing amplify this effect.
Their core ideas do not align well with the trends in memory architectures. 
There is no sign that the memory gap between data movement and compute speed closes any time soon.
It is widening~\cite{Dongarra:2011:ExascaleSoftwareRoadmap}.




%
% Context and bigger picture: What does software look like
%
\added[id=copy]{Modern high-level languages} such as C/C++ run risk exacerbating the arising problems:
They \added[id=copy]{provide us with the concept of structures (structs) to model data such as particles, mesh cells, or entries of a multiphysics partial differential equations. 
Structs are the natural modelling vocabulary for many algorithms.
Due to \texttt{struct}, the C++ language leans towards an array-of-structs (AoS) storage for sequences of objects, aka instances of structs.
The struct---or class in object-oriented terminology---is the primary modelling entity for the programmer}~\cite{Hirzel:2007:DataLayoutForOO,Homann:2018:SoAx,Reinders:2016:XeonPhi,Jubertie:2018:DataLayoutAbstractionLayers}.
In many algorithms, structs are not only the natural modelling langugae, they are also convenient to organise (partial) data exchange accross process boundaries through message passing, sorting, or to deliver maintainable and extendable code bases.
Unfortunately, thinking and programming in structs tends to add further data scattering---if not all elements of a struct are required within a compute-heavy subprogram (computational kernel), each cache line load brings data into the cache that is not required at this point, i.e.~the cache's capability to hold relevant data is reduced---and non-coalesced memory accesses which in turn challenge vectorisation. 


%
% Big Idea
%
As developers can not alter the hardware and typically do not want to rewrite AoS code, they can resort to a \emph{temporary data reorganisation in memory}:
Data are converted back and forth between AoS and structures of arrays (SoA), depending on the compute task ahead.
This is a counter-intuitive strategy:
We introduce further memory movements in a regime already constrained by them.
\added[id=copy]{Yet, as implementations over SoA often outperform their AoS counterparts---once they enable efficient usage of vector instructions~\cite{Gallard:2020:ExaHyPEVectorisation,Intel:MemoryLayoutTransformations,Springer:2018:SoALayout} and become less sensitive to cache effects than  AoS~\cite{Homann:2018:SoAx,Hundt:2006:StructureLayoutOptimisation,Springer:2018:SoALayout}---}many codes have a sweet spot, e.g.~in terms of problem sizes, where a reordering pays off.
To bring the sweetspot forward, we introduce data \emph{views} (see preparatory work in \cite{Radtke:2024:AoS2SoA}): 
Any temporary code data reordering includes exclusively those struct members that are read from or written to.
Other struct members remain untouched. 
Logically, we work with a view over the data type that exposes only specific members.


%
% Requirements on solution
%
To streamline programming following this implementation idiom, four properties are desirable: 
First, all transformations must be minimally invasive, avoiding major code changes that ripple through the codebase.
Second, they should leave core routines with domain knowledge (e.g., physical models) written in AoS untouched.
Third, data conversions should be hidden from the programmer and the code hence remain usable in both AoS and SoA contexts.
Finally, the result should be optimal with minimal data movements.
We therefore generalise the notion of a view, distinguishing read from write struct members and abstracting from the underlying data storage. 
We propose delegating all conversions and view construction to the C/C++ compiler. 
The compiler receives instructions by the programmer through C/C++ annotations to construct and use views, handling all complexity behind the scenes. 
These annotations apply exclusively to local blocks of (nested) loops.



%
% Related work
%
A \emph{guided} conversion \cite{Jubertie:2018:DataLayoutAbstractionLayers,Xu:2014:SemiAutomaticComposition} through annotations differs from completely manual (user-driven) or automatic workflows.
The thorny challenge of finding good heuristics when to convert remains out of scope.
We leave the decision ``when'' to convert with the developer but hide the ``how''.
Our \emph{automatic} conversion through compiler techniques ensures that \added[id=copy]{developers can write their algorithms in a memory layout-agnostic way, and the underlying memory layout can even differ from context to context.
C++ template meta programming combined with specialised containers would be an alternative to achieve this
\cite{Homann:2018:SoAx,Reinders:2016:XeonPhi,Jubertie:2018:DataLayoutAbstractionLayers,Springer:2018:SoALayout,Strzodka:2011:AbstractionSoA}.
Yet, most of these techniques provide a \emph{static, global} approach. 
The data layout of a struct remains fixed over time.}
Our temporary, local conversion---where the data layout changes only over a loop's lifespan---frees us from complicated storage state tracking. 
Similar \added[id=copy]{dynamic data structure transformations within a code have been used by several groups successfully \cite{Gallard:2020:ExaHyPEVectorisation,Vikram:2014:LLVM}.
Our contribution is that we clearly separate storage format considerations from programming, and move all data conversions into the compiler.
}
We accomplish this without requiring developers to alter their original baseline code.




Guided temporary and local views could also be realised through C++ reflections~\cite{Childers:2024:CPPReflections}:
In theory, compiler optimisation passes could use reflection information to reorder data out-of-place, i.e.~copying into a temporary buffer similar to our technique, narrow down these conversions only to data used (dead data access elimination), and ultimately vectorise aggressively.
However, it remains uncertain whether compilers will be able to offer such a complex, multi-step code transformation solely through reflections in the near future.
At the moment, there are, to the best of our knowledge, no automatic view constructions available.
Additionally, reflections require code rewrites, contradicting our goal of minimal invasiveness.
Our approach preserves existing code structure.



Implemented as compiler passes~\cite{Xu:2014:SemiAutomaticComposition}, our approach maintains the code's syntax.
We do not increase the syntactic complexity of the code.
We advance beyond preparatory work \cite{Radtke:2024:AoS2SoA} by identifying views automatically without user interaction. 
Finally, our technique applies not only to genuine AoS arrays but also to scattered data like arrays of pointers to structs.
A guided transformation gathering scattered data into continuous buffers is essential for GPU offloading.


%
% Use case
% 
\added[id=copy]{
We demonstrate the potential of our concepts by means of selected compute kernels from a larger smoothed particle hydrodynamics (SPH) code~\cite{Schaller:2024:Swift}.
Individual SPH interactions can either be strongly memory-bound or rely on compute-intense kernels, while SPH notoriously has to reorder and exchange particles between nodes.
Since we allow the code base to stick to AoS overall, we do not negatively impact algorithmic phases such as the particle boundary exchange or any sorting}, but would expect the automatic conversions to pay off for the compute-intense kernels dominating the total runtime.
% With most of the typical SPH runtime spent in few computational kernels, we would expect the automatic conversions to pay off immediately.
% Our observations do not support this assumption:
% Instead, the automatic conversions might force the developer to investigate in-depth which code parts vectorise in which way:
% A vanilla version might be written in a way that it neither benefits from SoA nor AoS.
Demonstrating that this holds in many cases
\added[id=copy]{
challenges the common knowledge that data conversions pay off only for Stream-like \cite{McCalpin:1995:Stream} kernels \cite{Homann:2018:SoAx} or large arrays \cite{Strzodka:2011:AbstractionSoA}.
It also is not in agreement with temporary data reordering prior to loops or computational kernels is always problematic~\cite{Hundt:2006:StructureLayoutOptimisation,Intel:MemoryLayoutTransformations}.
It can yield significant speedups.
}
% Despite its simplicity, our approach hence leaves data arrangement optimisations with manual performance engineering.
% \added[id=copy]{We facilitate experimenting with different data layouts and separate algorithm development from performance tuning \cite{Gallard:2020:Roles}}, but the developer still is responsible to investigate where to use which annotations.
% The SPH studies also suggest that the decision for or against AoS to SoA conversions has to be made in the context of studying other code properties:
% The transformations might enable vectorisation but not without further manual work.
As such, our studies and attribute suggestions are relevant for many codes beyond SPH, but they notably provide a protoype how future C/C++ annotations can change the character of performance optimisation, where more code technical refeactorings \cite{Fowler:2019:Refactoring} are deployed into the translation chain freeing human time for analysis and high-level tasks.
This is in the spirit of C++ annotations.


The remainder is organised as follows:
We start with a use case (Section~\ref{section:demonstrator}) that motivates our work but also allows us to introduce our ideas from a developer's perspective. 
Next we introduce our code annotations (Section~\ref{section:annotations}), we discuss their semantics, and we formalise key concepts such that our work can have an impact of future compiler developments (Section~\ref{section:formalism}). 
In Section~\ref{section:realisation-variants}, the annotations are used to implement the core use case ingredients in different ways, 
before we describe the realisation of the annotations in our compiler prototype (Section~\ref{section:realisation}).
We continue with case studies on their impact in Section~\ref{section:results}.
A brief summary and outlook in Section~\ref{section:conclusion} close the discussion.
