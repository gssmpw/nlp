\section{Results}
\label{section:results}

We assess the impact of our compiler prototype on two architectures.
Our first system is an Intel Xeon Platinum 8480+ (Sapphire Rapids) testbed. 
It features $2 \times 56$ cores over $2 \times 1$ NUMA domains spread over
two sockets, hosts
an L2 cache of 2,048 KByte per core and a shared L3 cache with 105 MByte per
socket.
Our second testbed is an Nvidia GH200 (Grace Hopper) system.
It features 72 cores in a single socket configuration.
Each core has an L2 cache of 1,024 KByte, and a shared L3 cache with 114 MByte for the entire
socket. The system contains the Nvidia H200 GPGPU chip boasting 96GB HBM3 memory at 4TB/s.

\begin{table}[htb]
  \caption{
    Overview of benchmarked SPH kernels. We report on the total contribution to the runtime, the cell-local compute complexity, and the size of the active sets.
    \label{table:results:kernel-overview}
  }
  \begin{center}
    \input{experiments/kernel-overview}
  \end{center}
\end{table}


We benchmark the core SPH algorithm and split up all measurements into data for the individual compute kernels that dominate the runtime (Table~\ref{table:results:kernel-overview}).
Meshing overhead, sorting, I/O and other effects are excluded.
In line with previous discussions, our SPH code does not refine the underlying spacetree down to the finest admissible level~(cmp.~related design decisions in \cite{Schaller:2024:Swift}).
Instead, we impose a cut-off:
If the number of particles per cell ($ppc$) underruns a certain threshold, the underlying mesh is not decomposed further.
This way, codes can balance between meshing overhead and algorithmic complexity.
In our result, we benchmark setups with average $ppc \in \{64,128,256,512,1024\}$.
Further to that, we assess configurations where the particles are scattered over the heap in main memory against configurations where the particles are stored in chunks of continuous memory and hence allow for coalesced memory access. 



Throughout the presentation, all data are normalised as cost per particle update, which might comprise interactions with all cell-local neighbours (density, force) or simply denote the runtime divided by the loop count.
All data are always employing all cores of the compute node, i.e.~the underlying code base is parallelised.
The data are hence node benchmarks or employ the whole GPU respectively.


\input{18a_baseline}
\input{18b_transformation-overhead}
\input{18c_cpu}
\input{18g_gpu}

