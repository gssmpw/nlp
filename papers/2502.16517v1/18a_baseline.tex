\subsection{Baseline code}

%
% What we do next and why
%
We start our studies with a simple assessment of the baseline kernel realisations without our compiler-guided transformations.
It is important to obtain a sound performance baseline for fair comparisons.
Besides this, it is not clear in the context of data format transformations if certain realisation patterns are particularly advantageous for the data re-organisation or challenge them.
Therefore, we distinguish various different kernel variants:
\begin{itemize}
  \item In the scattered generic kernel, we stick to the vanilla blueprint of Algorithm~\ref{section:demonstrator} and let the particles be scattered all accross the heap. Each cell hosts a sequence of pointers to addresses all over the place.
  \item A second variant sticks to the generic loop over C++ containers yet ensures that the particles of one cell are stored continuously in memory. However, this knowledge is not exploited explicitly.
  \item Finally, we use this knowledge about continuous chunks and split up each loop into a loop over chunks. Per chunk, we then employ a plain parallel for with pointer arithmetics, i.e.~we expose the fact that coalesced memory access is possible explicitly within the source code.
  \item For the $\mathcal{O}(N^2)$ kernels, we distinguish two kernel variants of the latter approach: In the variant labelled \texttt{local-active}, we first run over the local particles in the outer loop before we traverse the active particles. In the variant with the label \texttt{active-local}, we permute these two outer loops.
\end{itemize}




%
% Predicate is connected through a locical or, i.e. true disables the actual distance check
% Symmetry checks are connected via a locical and, i.e. false kicks them out
%
\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=1.00\textwidth]{experiments/kernel-throughput/gi001/out-x64-soafalse-cell0064-predfalse-symtrue-legend.pdf}
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/out-x64-soafalse-cell0064-predfalse-symtrue.pdf}
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/out-x64-soafalse-cell1024-predfalse-symtrue.pdf}     
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soafalse-cell0064-predfalse-symtrue.pdf}
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soafalse-cell1024-predfalse-symtrue.pdf}     
  \end{center}
  \caption{
   	Cost per particle update for various kernels on the Intel (top) and Grace (bottom) CPU node without any data transformations. 
   	We distinguish $ppc=64$ (left) from $ppc=1024$ (right). The vertical line denotes the sum of all L2 caches.
   \label{figure:results:baseline}
  }
\end{figure}


%
% What we see
% 
For all setups, the throughput improves with increasing number of particles (Figure~\ref{figure:results:baseline}) unless we have a very small total particle count.
The different kernel realisation variants make no significant difference, while the kernels with local $\mathcal{O}(N^2)$ complexity are significantly more expensive than their linear counterparts.
Notably, the fact that data are available as continuous chunks makes no major difference no matter if we try to exploit this fact or not.
The ARM CPU yields slightly noisier data than the x86 counterpart.
A small workload per kernel improves the throughput of kernels with quadratic complexity, but larger $ppc$ counts favour the linear kernels.
Falling out of the L2 caches has no performance impact and hence cannot explain any runtime behaviour.


%
% Interpretation
%
The plateauing for larger particle counts suggests that there is some compute management overhead which is amortised as the overall workload increases.
We pick the minimal total particle count such that all CPU cores are always busy, i.e.~there are always enough kernel invocations to employ all threads.
However, workload imbalances between the threads can better be smoothed out with a larger total set of compute kernels.
The overhead stems merely from a load distribution argument.
The local $\mathcal{O}(N^2)$ character of the force and density calculation explains why they benefit from small $ppc$ (we effectively have a complexity of $\mathcal{O}(ppc^2)$), while the kick and drifts are streaming kernels and hence do benefit from larger loops which pipe data through the system.
As the kernels with quadratic local complexity dominate the runtime (Table~\ref{table:results:kernel-overview}), the simulation code benefits from small $ppc$.
We assume that noise in the measurements and notably cost peaks stem from situations where we fall out of caches, or setups where a lot of the guard predicates that determine if a particle is to be updated return false.


\begin{observation}
 A better utilisation of the hardware is inferior to a reduction of the local compute complexity of the kernels as long as there are enough of these kernels in flight. 
\end{observation}

%
% Implication
%

\noindent
The classic HPC metric MFlop/s suggests to use reasonably large $ppc$ to balance for the throughput between the different phases.
Once we take the characteristic distribution of the compute phases into account and weight the improvements accordingly, it becomes clear that, from a science per time point of view, we have to work with as small $ppc$ as possible, which implies using aggressive AMR in our case;
even though this challenges the kernel efficiency.


The lack of impact of continuous data seems to be surprising, but is reasonable once we take into account that our baseline data structure is AoS. 
The particles might be continuous in memory, but each kernel picks only few data members $\mathbb{A}_{\text{in}}$ to read and very few $\mathbb{A}_{\text{out}}$ to write (Table~\ref{table:results:kernel-overview}).
Consequently, the data access pattern is scattered even though the underlying data are continuous.
There is no coalesced memory access.

