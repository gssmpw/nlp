\subsection{Kernel modifications on CPUs}

Our SPH compute kernels are heavy on comparisons yielding branching and even involve some internal atomic operations.
Therefore, we have to assume that they do not vectorise accross multiple particle--particle evaluations.
It is a straightforward code modification to remove the handling of long-range interactions and instead only to evaluate a boolean within the loop that flags ``is there an interaction requiring an atomic access''.
If this flag is set, we can loop over the particle pairs again and evaluate the long-range interaction.
Our assumption is that most kernels do not enter any branch with an atomic.
Eliminating or masking out the neighbour predicate (such as \texttt{DensityPredicate} in Listing~\ref{algorithm:demonstrator:blueprint}) is more difficult.
We however can artificially remove the if statement and study the impact of such a tweak on the performance.


\begin{figure}[H]
  \begin{center}
%    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/out-x64-soatrue-cell0064-predfalse-symtrue-legend.pdf}
    \includegraphics[width=1.00\textwidth]{experiments/kernel-throughput/gi001/out-x64-soatrue-cell0064-predfalse-symfalse-legend.pdf}
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/out-x64-soatrue-cell0064-predfalse-symfalse.pdf}
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/out-x64-soatrue-cell0064-predtrue-symfalse.pdf}
  \end{center}
  \caption{
    Measurements on Intel with $ppc=64$ where we mask out all multiscale interactions (left) or multiscale and predicate evaluations (right).
   \label{figure:results:cpu-ppc64}
  }
\end{figure}


%
% What we see
%
The removal of the atomic entries yields a speedup of around a factor of two on Intel systems (Figure~\ref{figure:results:cpu-ppc64}) for the $\mathcal{O}(N^2)$ kernels.
The ARM system showcases a similar effect.
Eliminating the (physically required) distance checks however does not give us a faster code.
The elimination of the atomics helps the local-active version over coalesced memory accesses to almost close the gap to the generic code base.
However, once we normalise all speedups (Figures~\ref{figure:cpu:overhead:Grace} and \ref{figure:cpu:overhead:Intel}), it becomes clear that a generic implementation not trying to exploit any memory insight performs best---although an advantageous memory layout can help (cmp.~differing observations in \cite{Hundt:2006:StructureLayoutOptimisation,Intel:MemoryLayoutTransformations}).
Yet, this statement is only true for the kernels with quadratic compute complexity. 
The linear kernels with a Stream-like access pattern do not benefit from the view concept~\cite{Homann:2018:SoAx,Strzodka:2011:AbstractionSoA}.




\begin{figure}[H]
\centering
 \includegraphics[width=1.00\textwidth]{experiments/kernel-throughput/gh003/plot-1-aarch64-density-legend.pdf}
 \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/plot-1-aarch64-density.pdf}
 \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/plot-1-aarch64-force.pdf}
 \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/plot-1-aarch64-drift.pdf}
 \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/plot-1-aarch64-kick1.pdf}
%  \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/plot-1-aarch64-kick2.pdf}
 \caption{
   Normalised speedups for the density, force, drift and the first kick kernels on the Nvidia Grace CPU using $ppc=1,024$.
   \texttt{pred.~on} denotes the manual elimination of the loop body predicate, i.e.~all particle pairs are evaluated or all particles are updated, respectively.
   \label{figure:cpu:overhead:Grace}
 }
\end{figure}



\begin{figure}[htb]
\centering
 \includegraphics[width=1.00\textwidth]{experiments/kernel-throughput/gi001/plot-1-x64-density-legend.pdf}
 \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/plot-1-x64-density.pdf}
 \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/plot-1-x64-force.pdf}
 \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/plot-1-x64-drift.pdf}
 \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/plot-1-x64-kick1.pdf}
% \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/plot-1-x64-kick2.pdf}
  \caption{
    Data from Figure~\ref{figure:cpu:overhead:Grace} for the Intel system.
    \label{figure:cpu:overhead:Intel}
  }
\end{figure}





%
% Interpretation
%
A speedup of a factor of two is not what we would hope to see on a modern vector architecture for a compute-intense kernel.
However, the SPH interactions are complex and still contain branching, and our implementation does not dive deeper into a kernel assessment and optimisation.
The data showcases that it is reasonable to remove complex atomic accesses from the core loops if these are infrequently encountered.
The data also highlights that it is important to avoid accummulation steps:
Fixing one particle and gathering the impact from all surrounding particles is not fast.
Instead, it is better to realise a spread-out paradigm, i.e.~scattered writes outperform accumulation once the data is converted into SoA.
However, small subchunks---with an average $ppc \approx 64$ and continuous particle chunks being assigned to the $2^d=8$ vertices, each chunk has only around eight particles---still hamper perforance and it remains advantageous to gather all data into a temporary large SoA memory block.


The removal (or masking out) of interactions improves the vector efficiency quite significantly.
Yet, it also increases the compute load, and eventually fails to compensate for the increased amount of work. 


\begin{observation}
 The SoA conversion with views helps the compiler to generate vector instructions. 
\end{observation}

\begin{observation}
 A temporary, local conversion into SoA is beneficial for kernels with high computational complexity but problematic for streaming kernels. 
\end{observation}

%
% Implication, contextualisation
%
\noindent
While the views help the compiler to vectorise better, it is not a free lunch.
It is merely a first step to facilitate better vectorisation and to kickstart more in-depth kernel analysis how to improve the performance further. 
