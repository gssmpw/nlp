\section{Realisation of the code transformations through a compiler modification}
\label{section:realisation}


%  - Clang is the C/C++-specific part of the compiler. It translates the C/C++ source code into the LLVM IR. It focusses on correctness and usability (e.g. user-friendly erros), while leaving any performance optimisation to the downstream processor.
%  - LLVM is an optimising compiler that works with LLVM IR and outputs highly optimised machine code for a range of architectures.
\added[id=copy]{
We implement our proposed techniques as a Clang LLVM front-end prototype, such that we can highlight their potential impact for the SPH demonstrator.
Clang takes C++ code and emits LLVM IR (intermediate representation).
% As we work within Clang, our modifications propagate into the (unoptimised) intermediate representation (IR) output, and we do not compromise  any downstream steps within the translation pipeline. 
This generated IR code still benefits from all LLVM-internal subsequent optimisation passes.
}


\added[id=copy]{
Clang's lexer, parser and semantic analyser yield an abstract syntax tree (AST).
The AST then is subsequently consumed to produce LLVM IR.
These steps constitute the compiler's \texttt{FrontendAction}.
We realise our functionality with a new \texttt{FrontendAction}.
}



\subsection{Data flow}

\added[id=copy]{
Our \texttt{FrontendAction} traverses the AST top down and searches for our annotations.
When it encounters a convert, it inserts allocation statements for the out-of-place memory allocations, it issues the actual data copying from (\ref{equations:AoStoSoA}), modifies the loop body $f$ into $\hat f$ according to (\ref{equation:view}), and eventually adds an epilogue that synchronises the data and frees the temporary buffers.
}
% For all of these steps, the complete information on the data view has to be available.


We need to identify all read and write accesses to struct members subject to the conversion.
Without major modifications to LLVM IR, these modifications are language-dependent.
Therefore, it is reasonable to collate all annotation handling exclusively within the \texttt{FrontendAction} rather than to scatter it among multiple compile stages.


Our automatic identification of views requires us to read whole code blocks before we can actually construct a tailored view representation.
Despite C++'s strict typing, information on the logical data types resulting from the narrowing is not yet available when we encounter a loop with annotations for the first time.
The notion of an automatic view identification violates the principle that the complete data information has to be at hand prior to the first usage.
Therefore, we need a preparatory pass through the source code to collect access data before we pass through the source code once again to apply all required modifications.
We have to analyse the AST before we start to enact any transformations.
The arising two-pass realisation of the front-end resembles preprocessor macros whose handling is realised as separate stage in the front-end, transparent to other components within the front-end pipeline.


By giving up on a single-read paradigm, i.e.~by running through the code multiple times, it becomes a natural choice to lower the annotated code onto plain OpenMP-augmented source code rather than to mangle GPU offloading into the translation process.
We apply a source-to-source translation followed by an additional OpenMP processing realised within the native, original Clang \texttt{FrontendAction}.


Despite being a logically separate pass through the AST, our \texttt{FrontendAction}'s mapping onto native, OpenMP-annotated C++ code can be realised as in-memory replacements of the source code.
This accommodates the fact that the LLVM AST itself is almost immutable.
\added[id=copy]{
We modify the in-memory buffers that hold the original files that make up the translation unit after they are parsed once, using the additional annotations to guide all rewrites.
Subsequently, the compiler re-parses these in-memory buffers, and feeds that new AST into the compiler pipeline.
This realisation reuses the preprocessor, lexer, parser and semantic analysis in both passes.
% We can read the realisation as a concatenation of two instances of the whole front-end pipeline bare the code generation in the middle.
}





\added[id=copy]{
Within our intermediate code generation, the names of all temporary variables are mangled, using the original container identifiers, to avoid variable re-declarations.
For the actual loop body, we then redirect the memory accesses to $\mathbb{A} \cup \mathbb{\hat A}$ to an intentionally non-mangled helper variable that masks the original AoS data accesses.
}

\added[id=copy]{
As we precede Clang's original \texttt{FrontendAction}, it is possible to dump
our output into source files explicitly instead of implicitly passing it on into
the subsequent \texttt{FrontendAction}.
While originally designed for debugging, this feature is particularly appealing
in environments where the modified compiler is only available locally,
while the compilers on the target production platform cannot be
modified.
}


\added[id=copy]{
As we navigate the source code using the AST, the effect of our annotations is scoped at the translation-unit level:
The data transformations cannot propagate into user translation units (object files) and notably fail to propagate into libraries.
}




\subsection{A minimally invasive data structure transformation}

As we realise all data transformations within the compiler, it is an option to let the compiler rewrite the functions' loop bodies explicitly:
We parse the body $f$ and generate a modified $\hat f$ over the view.
This modified loop body then is subject to further optimisation passes.
This strategy is efficant and useful \cite{Radtke:2024:AoS2SoA}, but it can involve significant code rewrites, making is difficult to maintain.
A further disadvantage of a genuine rewrite is that it becomes more difficult to debug and profile the resulting code, as it might become unclear which code parts correspond to which original source code instruction. 
This is a common property of any precompiler and code-generation approach. 

%In our prior work, the AoS-to-SoA transformation consisted of three main stages: the prologue, which performed memory copies and layout transformations; the loop rewriting phase, which modified memory accesses to interact with SoA data structures; and the epilogue, which restored modified data to the original AoS buffers. The loop transformation was designed to ensure compatibility with compiler optimisations, particularly auto-vectorisation.

For our present compiler realisation, we propose to use proxy objects~\cite{Gamma:1994:DesignPatterns}.
In this implementation, a proxy object preserves the structure and method signatures of the original AoS type, i.e.~it includes all static methods, type aliases, typedefs, using declarations, and constexpr values from the original type, as well as provides the storage and member functions required to maintain syntactic backwards-compatibility with the original AoS data structures. Yet, the proxy object replaces all value-holding struct members (fields) with corresponding references to locations within the SoA buffers.
Since static struct members and methods do not affect the memory layout, we employ a blanket approach, i.e.~make them plain forwards to the proxied class.

Once the proxy object type is available, we can instantiate a proxy object at the start of the loop body using the original loop ``counter'' (iterator's) variable name.
This shadows all accesses to the underlying data and instead redirects them through the proxy.
It is a very simple transformation boiling down to one command inserted into $\hat f$.
The remainder of the loop body remains syntactically invariant.
The prologue now creates the SoA data representation, i.e.~the view, while the proxy object redirects data accesses via references into the out-of-place converted data rather than the original container for all data mambers from $\mathbb{A}_{\text{in}} \cup \mathbb{A}_{\text{out}}$. The remaining data members and non-static methods are omitted from the proxy object entirely.


LLVMâ€™s optimisation passes eliminate the indirect data accesses through the proxy object, generating optimised assembly equivalent to that produced by whole-loop rewriting.
From hereon, LLVM can also identify vectorisation potential.
The overall approach is a lightweight code-generation rather than a heavy compiler-based rewrite of major code parts (cmp.~alternative proposed in \cite{Radtke:2024:AoS2SoA}).
Indeed, we delegate the heavy lifting behind code optimisation to subsequent compiler passes and realise a strict separation-of-concern that frees the developer from the cumbersome and error-prone manual creation and management of proxy objects.

\input{code-snippets/soa-conversion-under-the-hood_vanilla}


The two-stage realisation of this concept---attribute realisation from Section~\ref{section:annotations} and code generation---hosts the complicated logic in the first source code pass, while the second pass is a rather mechnical code rewrite and source code generation (cmp.~Listing~\ref{algorithm:rewrite-demo:vanilla} against Listing~\ref{algorithm:rewrite-demo:rewritten}). 
The first analysis pass has to identify all fields and methods of the AoS type accessed within a kernel. 
For this, all source code employed within a loop kernel has to be visible (cmp.~notes in Section~\ref{section:annotations}).
As many scientific codes, our SPH benchmark is heavily templated-based.
For template-based kernels, th view analysis is deferred until the compiler instantiates the template. 
A recursive, queue-driven algorithm then tracks all method calls originating from the kernel, enqueuing methods where the structure escapes and detecting cycles to avoid infinite recursion. 

% \subsection{Transformation output example}


% In any but the most trivial applications, the rewriting process yields an amount of code that is unpractical for presentation or any didactic purpose. For this reason we retreat to a radically simplified example loop that operates on a single AoS buffer (Listing~\ref{algorithm:rewrite-demo:vanilla}).

\input{code-snippets/soa-conversion-under-the-hood_transformed}

% The rewritten loop is shown in Listing~\ref{algorithm:rewrite-demo:rewritten}. Comments inside the listing describe each new element introduced by the transformation process. Introduced types and local variables are suffixed with a unique sequence of characters to avoid unintended naming conflicts.

\subsection{Data management}

Out-of-place transformations for containers typically require heap data allocations if the containers' size is unknown at compile time.
Heap allocations can become expensive if issued very frequently in high-concurrency codes \cite{Hager:2011:HPCIntro}.
However, our prologue-epilogue paradigm does not require strictly dynamic data structures.
We know the size of the underlying container by the time we hit the prologue while the struct member set $\mathbb{A}_{\text{in}} \cup \mathbb{A}_{\text{out}}$ is a compile-time property.
Since the size of the container does not change while we traverse the loop, we work with variable-length arrays (VLAs)~\cite{Cheng:1995:VariableLengthArrays}. This approach lets us avoid making memory allocator calls entirely (cmp.~Listing~\ref{algorithm:rewrite-demo:rewritten}).

\subsection{OpenMP}

The transformation pass itself does not modify the loop beyond the standard SoA transformation.
If offloading annotations are used, it decorates these transformed loops with the appropriate OpenMP target offloading pragmas.
It leverages standard OpenMP. 
This is possible as we keep all data conversions on the host, work with an out-of-place data transformation, and make the generation of plain C++ code follow a complete view analysis.
The compiler ``knows'' exactly how much data in which format is held in the temporary views' data structures, and that all of these data are held in plain SoA.


Due to the ownership over the temporary data, it is possible to deploy the responsibility for all correct data movements to the OpenMP subsystem.
The compiler then has to generate offloading-capable binaries, and it is up to the user to correctly configure the compilation environment. 
This includes setting up the necessary paths and environment variables specified by hardware vendors for both compilation and runtime execution.
Our solution remains hardware-agnostic.



\subsection{GPU data handling}

Issuing device memory allocation commands on a critical path can lead to substantial performance degradataion~\cite{Wille:2023:GPU} which necessitates implementing a memory management scheme that safely reuses accelerator allocations. Our implementation lazily allocates per-thread, per-transformed-loop buffers that grow depending on the sizes of the working sets. To minimise synchronisation overheads, we schedule all accelerator-related operations, including memory transfers and kernel executions asynchronously using the standard OpenMP \texttt{nowait depend(...)} constructs. Since the prologue-loop-epilogue triad is a logically synchronous operation, we may not introduce asynchronicity that is observable beyond the immediate context of the transformed loop. This necessitates at least one host-device synchronisation for every invocation of the loop which we implement using the standard OpenMP \texttt{taskwait} construct. The synchronisation, together with the per-thread, per-loop buffer allocation guarantees data safety.

\subsection{Interplay with other translation passes and shortcomings}

By confining our transformation entirely within Clang, we ensure that all optimisations performed by the LLVM optimiser as part of subsequent passes are applied to the transformed kernels. 
At this point, our transformations are already out of the way. 
This guarantees that each kernel is processed in a single, consistent form, eliminating potential conflicts arising from multiple interpretations of the same code:
If functions are used within views and then also without views, we logically have already replicated the functions' realisation and the optimisations handle the replica independently.



Such a design not only avoids unfortunate side-effects.
It notably ensures that HPC-critical optimisations, such as auto-vectorisation, are applied independently and in a bespoke, tailored way to the vectorisation-friendly SoA representations.


A limitation of our approach is its dependence on complete visibility of all functions that interact with the transformed AoS data. 
We need access to the whole loop body's call graph to identify all write and read accesses to AoS data members and to be able to produce a modified loop kernel.
Making the whole call hierarchy visible becomes challenging when external functions, either directly or indirectly referenced by the kernel, reside outside the analysed translation unit. 
In such cases, our transformations break down.


A potential remedy involves leveraging link-time optimisation (LTO) to extend the transformationâ€™s reach beyond a single translation unit. However, this approach introduces several practical challenges. 
Firstly, it is only viable for external functions with accessible source code.
Precompiled library functions remain beyond reach. 
Secondly, full LTO (fat LTO) is required to provide a comprehensive view of all compiled code, whereas many large codebases favour thin LTO due to its superior compilation speed.
