\subsection{Data transformation impact}


%
% What we do next and why
%
We next enable the data compiler-guided transformations and compare the outcomes to the CPU baseline.
This allows us to quantify the transformation overhead.

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=1.00\textwidth]{experiments/kernel-throughput/gi001/out-x64-soatrue-cell0064-predfalse-symtrue-legend.pdf}
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/out-x64-soatrue-cell0064-predfalse-symtrue.pdf}
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/out-x64-soatrue-cell1024-predfalse-symtrue.pdf}     
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soatrue-cell0064-predfalse-symtrue.pdf}
    \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/out-aarch64-soatrue-cell1024-predfalse-symtrue.pdf}     
  \end{center}
  \caption{
    Repetition of experiments from Figure~\ref{figure:results:baseline} with the compiler transformations enabled.
    Intel (top) and Grace (bottom) CPU data with $ppc=64$ (left) from $ppc=1,024$ (right).
   \label{figure:results:overhead}
  }
\end{figure}

%
% What we see
%
The introduction of the compiler transformations smoothes out the measurements and leads to very stable plateaus (Figure~\ref{figure:results:overhead}).
For the kernels with linear compute complexity, they introduce an overhead which can result in a performance degradation of up to an order of magnitude on the x86 chip.
This overhead penalty is less pronounced on the ARM chip.
For the quadratic kernels, we gain an order of magnitude in speed on the Intel platform, while the ARM chip yields even more significant speed improvements.

\begin{figure}[htb]
\centering
 \includegraphics[width=1.00\textwidth]{experiments/kernel-throughput/gh003/plot-2-aarch64-density-legend.pdf}
 \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/plot-2-aarch64-density.pdf}
 \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/plot-2-aarch64-force.pdf}
 \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/plot-2-x64-density.pdf}
 \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gi001/plot-2-x64-force.pdf}
%  \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/plot-2-aarch64-drift.pdf}
%  \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/plot-2-aarch64-kick1.pdf}
%  \includegraphics[width=0.49\textwidth]{experiments/kernel-throughput/gh003/plot-2-aarch64-kick2.pdf}
 \caption{
  Normalised speedups for the density (left) and force (right) kernel on the Nvidia Grace CPU (top) and on Intel (bottom) over multiple different choices of $ppc$.
  \label{figure:transformation-overhead:normalised-speedups}
 }
\end{figure}


Having containers over coalesced memory seems to make no big difference, but trying to exploit this fact makes the throughput deteriorate.
Once we zoom into different kernel variants and plot their relative speedup over the vanilla baseline version without any views (Figure~\ref{figure:transformation-overhead:normalised-speedups}), it becomes clear that the coalesced memory organistion does help.
It just is not relevant for all $ppc$ and kernel choices, i.e.~notably makes a difference for smaller $ppc$.



%
% Interpretation
%
Cache blocking is implicitly triggered by our out-of-place reordering.
We therefore eliminate a lot of memory transfer noise.
As each kernel picks only few attributes from the baseline container, our prologues and epilogues all yield scattered memory access and hence fail to benefit from a continuous arrangement of the data in memory.
However, they do benefit from very large, uninterrupted loops:
On both architectures, the linear kernels benefit from larger $ppc$, as they can stream data through the chip.
The same effect materialises for the quadratic kernel once we enable our transformations:
If we reduce $ppc$ as we split the loop iteration ranges into chunks, we reduce the efficiency of the transformations.


The ARM chip is slow and weak compared to its Intel cousin yet come up with a better balanced memory bandwidth relative to the compute capability.
Hence, the conversion have a smaller impact.
Overall, we obtain impressive speedups exceeding one order of magnitude.


\begin{observation}
 Continuous ordering of the input does not consistently have a major positive impact on the runtime once our annotations are used. In return, the split into tiny subchunks trying to exploit continuous data access is counterproductive, i.e.~the arising loop blocking makes the performance deterioriate. 
\end{observation}



%
% Discussion
%
\noindent
Loop blocking is a classic performance engineering strategy, notably if combined with a continuous arrangement of the underlying memory.
Our data suggests that these empiric lessons learned have to be rethought or even turned around as we introduce compiler-based temporary reordering with views.
Loop ordering is disadvantageous.
Sorting data is disadvantageous.


However, it is not clear if we can uphold this observation for all particle types, i.e.~number of struct members, deeper memory hierarchies or other algorithms.
Notably SPH algorithm flavours with a lot of long-range interactions might behave different if the ``long-range data'' falls out of L3 cache. 
