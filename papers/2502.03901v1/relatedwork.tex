\section{Related work}
{\begin{figure*}
\includegraphics[width=\textwidth]{figures/2dlabels.png}
\caption{The process of generating 2D pseudo-labels. Using unlabeled images and a list of classes, we use Grounding Dino~\cite{liu_grounding_2023} features to obtain regions with soft labels. Segment Anything~\cite{kirillov_segment_2023} converts these to detailed masks and allows us to obtain per-pixel soft labels.}
\vspace*{-\baselineskip}
\label{2dlabels}
\end{figure*}}

Supervised methods (e.g. ~\cite{maturana_voxnet_2015, graham_3d_2018, vedaldi_searching_2020, zhu_cylindrical_2021, zhao_svaseg_2022, lai_spherical_2023, zhang_occformer_2023, cao_monoscene_2022, wu_squeezesegv2_2018, milioto_rangenet_2019, cortinhal_salsanext_2020, zhang_polarnet_2020, kong_rethinking_2023, roldao_lmscnet_2020, cheng_s3cnet_2020, zuo_pointocc_2023, huang_tri-perspective_2023, qi_pointnet_2017, charles_pointnet_2017, thomas_kpconv_2019, hu_randla-net_2020, wu_point_2022, cheng_af_2021, xu_rpvnet_2021, ye_lidarmultinet_2023, lu_lidar-camera_2024}) for the online prediction of 3D semantics require large amounts of labeled 3D data which is often not available for novel application domains. 
Some approaches ~\cite{zhang_growsp_2023, liu_u3ds3_nodate} attempt unsupervised 3D \gls{ss}. However, the lack of semantic information in 3D features limits performance. Others~\cite{xie_pointcontrast_2020, zhang_self-supervised_2021, nunes_segcontrast_2022, zhu_ponderv2_2023} instead focus on unsupervised representation learning by pre-training on unlabeled LiDAR data, but these methods still require labels for effective fine-tuning. 
Previous research has used the extensive body of work in 2D \gls{cv} to overcome these issues.


\subsection{2D supervised 3D semantic understanding}
Various approaches~\cite{hayler_s4c_2024, zhang_occnerf_2023, genova_learning_2021, bultmann_real-time_2023, sautier_image--lidar_2022, mahmoud_self-supervised_2023, liu_segment_2023, vora_pointpainting_2020} use the well-researched 2D domain to enhance performance on 3D semantic tasks without relying on 3D labels.
Some works~\cite{genova_learning_2021, bultmann_real-time_2023, hayler_s4c_2024, reichardt_360_2023} use off-the-shelf pre-trained 2D semantic segmentation networks to supervise the training of 3D networks (so-called 'shelf-supervised'). PointPainting~\cite{vora_pointpainting_2020} uses 2D-3D projection to apply labels obtained from images to 3D points, however, projected labels are limited to the camera frame and can be noisy due to small errors in projection and masking. \cite{genova_learning_2021, reichardt_360_2023} nevertheless show that it is possible to effectively train 3D models using noisy projected labels as pseudo-labels by label filtering. Alternatively, ~\cite{hayler_s4c_2024, zhang_occnerf_2023} use \glspl{nerf} instead of projection to bridge the gap from 2D to 3D.  They utilize pre-trained 2D semantic segmentation networks and temporal consistency for semantic and depth supervision to train unsupervised \gls{ssc} models. 
However, these methods are constrained by their dependence on pre-trained, closed-set 2D models, which are often not available for novel domains.
Other studies~\cite{sautier_image--lidar_2022, mahmoud_self-supervised_2023} focus on representation learning with unlabeled camera-LiDAR data. The camera is used to group visually similar regions into superpixels. This knowledge is transferred to 3D, improving representations for 3D semantic tasks, though labels are still required for fine-tuning. 

\subsection{Foundation models for 3D semantics}
In recent years, various \glspl{vfm} have been introduced. Models like CLIP~\cite{radford_learning_2021} and Grounding Dino~\cite{liu_grounding_2023} combine language and vision for open-vocabulary image labeling and object detection respectively. However, these do not provide detailed pixel-wise labels and generally output only a single class per image or region. Other models like SAM~\cite{kirillov_segment_2023} (image segmentation) and Depth Anything~\cite{yang_depth_2024} (depth estimation) give per-pixel labels but lack semantics. Additionally, foundation models for 3D data are largely absent. Still, various methods exploit the open-set capabilities of 2D \glspl{vfm} for 3D semantic tasks.
Recent work~\cite{liu_segment_2023} based on~\cite{sautier_image--lidar_2022, mahmoud_self-supervised_2023} has employed segmentation \glspl{vfm} like SAM~\cite{kirillov_segment_2023} to improve representation learning by generating more consistent superpixels. However, the lack of semantic labels in the superpixels means labeled data is required for fine-tuning.
Some works~\cite{zhang_pointclip_2022, peng_openscene_2023, chen_clip2scene_2023, tan_ovo_2023, vobecky_pop-3d_2024, hess_lidarclip_2024} use CLIP~\cite{radford_learning_2021} to distill language features into 3D segmentation networks, enabling open-vocabulary capabilities in 3D applications. 
Although this allows these models to be highly flexible and predict any semantic class at test time, their universality also limits performance. Trained for image captioning, CLIPâ€™s general language features are less suited for precise segmentation. Aggregating these features in 3D is also non-trivial which reduces temporal and geometric consistency. This limits the usefulness of these models for providing high quality labels. In contrast, we use \glspl{vfm} specialized for segmentation, using the statistically grounded Bayesian update in combination with our \gls{cmst} to improve label consistency.
Other works~\cite{khurana_shelf-supervised_2024, zhang_sam3d_2024, najibi_unsupervised_2023} use foundation models for pseudo-labeling to improve 3D object detection. However, as opposed to \gls{ss}, object detection does not require per-point labels, as it only uses course bounding boxes.
Most comparable to our work,~\cite{zhou_openannotate3d_2024, zhou_openannotate2_2024} also use open-vocabulary models for 3D semantic pseudo-labeling. However, the focus of their work is to simplify the workflow of human annotators by using \glspl{llm} to enable frame-by-frame annotation based on voice or text-prompts, requiring a human in-the-loop for supervision and label corrections. Hence, automatic temporal consistency is not considered, and evaluations on label quality across various domains are limited. We instead use a Bayesian voxel update to combine semantic labels and ensure temporal consistency, and evaluate label quality quantitatively across diverse domains.