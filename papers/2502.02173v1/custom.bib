% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@misc{LanguageAnisotropic,
      title={Language Anisotropic Cross-Lingual Model Editing}, 
      author={Yang Xu and Yutai Hou and Wanxiang Che and Min Zhang},
      year={2023},
      eprint={2205.12677},
      archivePrefix={arXiv},
}

@misc{MEMIT,
      title={Mass-Editing Memory in a Transformer}, 
      author={Kevin Meng and Arnab Sen Sharma and Alex Andonian and Yonatan Belinkov and David Bau},
      year={2023},
      eprint={2210.07229},
      archivePrefix={arXiv},
}

@inproceedings{mela-etal-2024-mass,
    title = "Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge",
    author = "Tamayo, Daniel  and
      Gonzalez-Agirre, Aitor  and
      Hernando, Javier  and
      Villegas, Marta",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.347",
    doi = "10.18653/v1/2024.findings-acl.347",
    pages = "5831--5847",
    abstract = "Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks. This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process. Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improvements in all metrics while requiring minimal parameter modifications. MEMAT delivers a remarkable 10{\%} increase in magnitude metrics, benefits languages not included in the training data and also demonstrates a high degree of portability. Our code and data are at https://github.com/dtamayo-nlp/MEMAT.",
}

@inproceedings{FLOR,
  title={FLOR: On the Effectiveness of Language Adaptation},
  author={Da Dalt, Severino and Llop, Joan and Baucells, Irene and P{\`a}mies, Marc and Xu, Yishi and Gonz{\'a}lez-Agirre, Aitor and Villegas, Marta},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={7377--7388},
  year={2024}
}

@misc{ROME,
      title={Locating and Editing Factual Associations in GPT}, 
      author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
      year={2023},
      eprint={2202.05262},
      archivePrefix={arXiv},
}


@article{Artetxe_2019,
   title={Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond},
   volume={7},
   ISSN={2307-387X},
   url={http://dx.doi.org/10.1162/tacl_a_00288},
   DOI={10.1162/tacl_a_00288},
   journal={Transactions of the Association for Computational Linguistics},
   publisher={MIT Press - Journals},
   author={Artetxe, Mikel and Schwenk, Holger},
   year={2019},
   month=nov, pages={597–610} }

@misc{adversarial_examples,
      title={Adversarial Examples for Evaluating Reading Comprehension Systems}, 
      author={Robin Jia and Percy Liang},
      year={2017},
      eprint={1707.07328},
      archivePrefix={arXiv},
}

@article{FunSearch,
  author = {Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M. Pawan and Dupont, Emilien and Ruiz, Francisco J. R. and Ellenberg, Jordan S. and Wang, Pengming and Fawzi, Omar and Kohli, Pushmeet and Fawzi, Alhussein},
  title = {Mathematical discoveries from program search with large language models},
  journal = {Nature},
  year = {2023},
  volume = {},
  number = {},
  pages = {},
  month = {},
  day = {14},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06924-6},
  url = {https://doi.org/10.1038/s41586-023-06924-6},
}


@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@misc{Rope,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
}

@misc{gradientClipping,
      title={Why gradient clipping accelerates training: A theoretical justification for adaptivity}, 
      author={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
      year={2020},
      eprint={1905.11881},
      archivePrefix={arXiv},
}

@misc{scikitlearn,
      title={Scikit-learn: Machine Learning in Python}, 
      author={Fabian Pedregosa and Gaël Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Andreas Müller and Joel Nothman and Gilles Louppe and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jake Vanderplas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and Édouard Duchesnay},
      year={2018},
      eprint={1201.0490},
      archivePrefix={arXiv},
}

@book{python,
author = {Van Rossum, Guido and Drake, Fred L.},
title = {Python 3 Reference Manual},
year = {2009},
isbn = {1441412697},
publisher = {CreateSpace},
address = {Scotts Valley, CA},
abstract = {PYTHON 3 Reference Manual (Python Documentation MANUAL Part 2).Python is an easy to learn object-oriented programming language, which combines power with clear syntax. It has modules, classes, exceptions, very high level data types, and dynamic typing. Python is free software. It can be used with GNU (GNU/Linux), Unix, Microsoft Windows and many other systems.This is a printed softcover copy of the official Python documentation from the latest Python 3.0 distribution. For each copy sold $1 will be donated to the Python Software Foundation by the publisher.This book is part of a brand new six-part series of Python documentation books. Searching for "Python Documentation Manual" will show all six available books.ABOUT THE AUTHOR: Guido van Rossum, is the inventor of Python. Fred L. Drake, Jr. is the official editor of the Python documentation.}
}

@article{DOLA,
  title={Dola: Decoding by contrasting layers improves factuality in large language models},
  author={Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James and He, Pengcheng},
  journal={arXiv preprint arXiv:2309.03883},
  year={2023}
}

@article{BM25,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}


@article{Chen_multimodal,
  title={Murag: Multimodal retrieval-augmented generator for open question answering over images and text},
  author={Chen, Wenhu and Hu, Hexiang and Chen, Xi and Verga, Pat and Cohen, William W},
  journal={arXiv preprint arXiv:2210.02928},
  year={2022}
}

@article{Chen_multimodal2,
  title={Re-imagen: Retrieval-augmented text-to-image generator},
  author={Chen, Wenhu and Hu, Hexiang and Saharia, Chitwan and Cohen, William W},
  journal={arXiv preprint arXiv:2209.14491},
  year={2022}
}

@inproceedings{RETRO,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR}
}

@article{dense_passage,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2004.04906},
  year={2020}
}

@article{RAGmultimodal,
  title={Retrieval-augmented multimodal language modeling},
  author={Yasunaga, Michihiro and Aghajanyan, Armen and Shi, Weijia and James, Rich and Leskovec, Jure and Liang, Percy and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2211.12561},
  year={2022}
}

@article{GAR,
  title={Generation-augmented retrieval for open-domain question answering},
  author={Mao, Yuning and He, Pengcheng and Liu, Xiaodong and Shen, Yelong and Gao, Jianfeng and Han, Jiawei and Chen, Weizhu},
  journal={arXiv preprint arXiv:2009.08553},
  year={2020}
}

@misc{annotationArt,
      title={Annotation Artifacts in Natural Language Inference Data}, 
      author={Suchin Gururangan and Swabha Swayamdipta and Omer Levy and Roy Schwartz and Samuel R. Bowman and Noah A. Smith},
      year={2018},
      eprint={1803.02324},
      archivePrefix={arXiv},
      
}

@article{Circuits,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@misc{serac,
      title={Memory-Based Model Editing at Scale}, 
      author={Eric Mitchell and Charles Lin and Antoine Bosselut and Christopher D. Manning and Chelsea Finn},
      year={2022},
      eprint={2206.06520},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{mLAMA,
  title={Multilingual LAMA: Investigating knowledge in multilingual pretrained language models},
  author={Kassner, Nora and Dufter, Philipp and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2102.00894},
  year={2021}
}

@article{clc,
  title={Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models},
  author={Qi, Jirui and Fern{\'a}ndez, Raquel and Bisazza, Arianna},
  journal={arXiv preprint arXiv:2310.10378},
  year={2023}
}
@inproceedings{Cross_lingual_comb,
    title = "Cross-lingual Continual Learning",
    author = "M{'}hamdi, Meryem  and
      Ren, Xiang  and
      May, Jonathan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.217",
    doi = "10.18653/v1/2023.acl-long.217",
    pages = "3908--3943",
    abstract = "The longstanding goal of multi-lingual learning has been to develop a universal cross-lingual model that can withstand the changes in multi-lingual data distributions. There has been a large amount of work to adapt such multi-lingual models to unseen target languages. However, the majority of work in this direction focuses on the standard one-hop transfer learning pipeline from source to target languages, whereas in realistic scenarios, new languages can be incorporated at any time in a sequential manner. In this paper, we present a principled Cross-lingual Continual Learning (CCL) evaluation paradigm, where we analyze different categories of approaches used to continually adapt to emerging data from different languages. We provide insights into what makes multilingual sequential learning particularly challenging. To surmount such challenges, we benchmark a representative set of cross-lingual continual learning algorithms and analyze their knowledge preservation, accumulation, and generalization capabilities compared to baselines on carefully curated datastreams. The implications of this analysis include a recipe for how to measure and balance different cross-lingual continual learning desiderata, which go beyond conventional transfer learning.",
}

@article{Aging,
  title={Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors},
  author={Hartvigsen, Thomas and Sankaranarayanan, Swami and Palangi, Hamid and Kim, Yoon and Ghassemi, Marzyeh},
  journal={arXiv preprint arXiv:2211.11031},
  year={2022}
}


@article{attention_satisfies,
  title={Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models},
  author={Yuksekgonul, Mert and Chandrasekaran, Varun and Jones, Erik and Gunasekar, Suriya and Naik, Ranjita and Palangi, Hamid and Kamar, Ece and Nushi, Besmira},
  journal={arXiv preprint arXiv:2309.15098},
  year={2023}
}

@misc{overthinking,
      title={Overthinking the Truth: Understanding how Language Models Process False Demonstrations}, 
      author={Danny Halawi and Jean-Stanislas Denain and Jacob Steinhardt},
      year={2023},
      eprint={2307.09476},
      archivePrefix={arXiv},
}

@article{multihop,
  title={Memory injections: Correcting multi-hop reasoning failures during inference in transformer-based language models},
  author={Sakarvadia, Mansi and Ajith, Aswathy and Khan, Arham and Grzenda, Daniel and Hudson, Nathaniel and Bauer, Andr{\'e} and Chard, Kyle and Foster, Ian},
  journal={arXiv preprint arXiv:2309.05605},
  year={2023}
}

@article{ripple_effects,
  title={Evaluating the ripple effects of knowledge editing in language models},
  author={Cohen, Roi and Biran, Eden and Yoran, Ori and Globerson, Amir and Geva, Mor},
  journal={arXiv preprint arXiv:2307.12976},
  year={2023}
}

@misc{OverviewOpt,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
}

@misc{Adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
}

@misc{TruthfulQA,
      title={TruthfulQA: Measuring How Models Mimic Human Falsehoods}, 
      author={Stephanie Lin and Jacob Hilton and Owain Evans},
      year={2022},
      eprint={2109.07958},
      archivePrefix={arXiv},
}

@misc{ITI,
      title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model}, 
      author={Kenneth Li and Oam Patel and Fernanda Viégas and Hanspeter Pfister and Martin Wattenberg},
      year={2023},
      eprint={2306.03341},
      archivePrefix={arXiv},
}

@misc{PolyglotOrNot,
      title={Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge Retrieval from Foundation Language Models}, 
      author={Tim Schott and Daniel Furman and Shreshta Bhat},
      year={2023},
      eprint={2305.13675},
      archivePrefix={arXiv},
}

@misc{DissectingRecall,
      title={Dissecting Recall of Factual Associations in Auto-Regressive Language Models}, 
      author={Mor Geva and Jasmijn Bastings and Katja Filippova and Amir Globerson},
      year={2023},
      eprint={2304.14767},
      archivePrefix={arXiv},
}


@article{MTStudy,
  title={Machine translation and its evaluation: a study},
  author={Mondal, Subrota Kumar and Zhang, Haoxi and Kabir, HM Dipu and Ni, Kan and Dai, Hong-Ning},
  journal={Artificial Intelligence Review},
  pages={1--90},
  year={2023},
  publisher={Springer}
}

@misc{TextSumm,
      title={The Current State of Summarization}, 
      author={Fabian Retkowski},
      year={2023},
      eprint={2305.04853},
      archivePrefix={arXiv},
}

@article{whyInContextLearning,
  title={Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Sui, Zhifang and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10559},
  year={2022}
}

@misc{AttIsAllYouNeed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
}

@misc{RAG,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ConsistencyBERT,
      title={Measuring and Improving Consistency in Pretrained Language Models}, 
      author={Yanai Elazar and Nora Kassner and Shauli Ravfogel and Abhilasha Ravichander and Eduard Hovy and Hinrich Schütze and Yoav Goldberg},
      year={2021},
      eprint={2102.01017},
      archivePrefix={arXiv},
}

@misc{JourneyCenterKnowledge,
      title={Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons}, 
      author={Yuheng Chen and Pengfei Cao and Yubo Chen and Kang Liu and Jun Zhao},
      year={2023},
      eprint={2308.13198},
      archivePrefix={arXiv},
}

@misc{hallucination,
      title={A Survey of Hallucination in Large Foundation Models}, 
      author={Vipula Rawte and Amit Sheth and Amitava Das},
      year={2023},
      eprint={2309.05922},
      archivePrefix={arXiv},
}

@misc{CatastrophicForgettingLLMs,
      title={An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning}, 
      author={Yun Luo and Zhen Yang and Fandong Meng and Yafu Li and Jie Zhou and Yue Zhang},
      year={2023},
      eprint={2308.08747},
      archivePrefix={arXiv},
}

@misc{BPE,
      title={Neural Machine Translation of Rare Words with Subword Units}, 
      author={Rico Sennrich and Barry Haddow and Alexandra Birch},
      year={2016},
      eprint={1508.07909},
      archivePrefix={arXiv},
}


@misc{ngram,
      title={Enriching Word Vectors with Subword Information}, 
      author={Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
      year={2017},
      eprint={1607.04606},
      archivePrefix={arXiv},
      
}

@misc{WordBasedNMT,
      title={Neural Machine Translation: A Review and Survey}, 
      author={Felix Stahlberg},
      year={2020},
      eprint={1912.02047},
      archivePrefix={arXiv},
}

@misc{Llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
}

@article{softcatala,
  title={Softcatal{\`a}: nous reptes per garantir la vitalitat del catal{\`a} a les tecnologies},
  author={Irigoyen, Marc Riera and Ribes, Xavier Ivars and Esteve, Pere Orga and Camacho, Joan Montan{\'e} and Hern{\'a}ndez, Jordi Mas and Cremades, Artur Vicedo},
  journal={Revista de llengua i dret},
  number={73},
  pages={146--153},
  year={2020}
}

@misc{commonsense,
      title={Editing Common Sense in Transformers}, 
      author={Anshita Gupta and Debanjan Mondal and Akshay Krishna Sheshadri and Wenlong Zhao and Xiang Lorraine Li and Sarah Wiegreffe and Niket Tandon},
      year={2023},
      eprint={2305.14956},
      archivePrefix={arXiv},
}

@inproceedings{BBPE,
  title={Neural machine translation with byte-level subwords},
  author={Wang, Changhan and Cho, Kyunghyun and Gu, Jiatao},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={9154--9160},
  year={2020}
}

@article{BLOOM,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@misc{BERT,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
}

@misc{ALiBi,
      title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation}, 
      author={Ofir Press and Noah A. Smith and Mike Lewis},
      year={2022},
      eprint={2108.12409},
      archivePrefix={arXiv},
}

@misc{falcon,
      title={The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only}, 
      author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
      year={2023},
      eprint={2306.01116},
      archivePrefix={arXiv},
}

@misc{bau_key_value,
      title={Rewriting a Deep Generative Model}, 
      author={David Bau and Steven Liu and Tongzhou Wang and Jun-Yan Zhu and Antonio Torralba},
      year={2020},
      eprint={2007.15646},
      archivePrefix={arXiv},
}

@misc{hase_doesLoc,
      title={Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models}, 
      author={Peter Hase and Mohit Bansal and Been Kim and Asma Ghandeharioun},
      year={2023},
      eprint={2301.04213},
      archivePrefix={arXiv},
}

@misc{attention_works,
      title={The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention}, 
      author={Kazuki Irie and Róbert Csordás and Jürgen Schmidhuber},
      year={2022},
      eprint={2202.05798},
      archivePrefix={arXiv},
}

@misc{ICL_survey,
      title={A Survey on In-context Learning}, 
      author={Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Zhiyong Wu and Baobao Chang and Xu Sun and Jingjing Xu and Lei Li and Zhifang Sui},
      year={2023},
      eprint={2301.00234},
      archivePrefix={arXiv},
}

@misc{CausalLMNotOptimal,
      title={CausalLM is not optimal for in-context learning}, 
      author={Nan Ding and Tomer Levinboim and Jialin Wu and Sebastian Goodman and Radu Soricut},
      year={2023},
      eprint={2308.06912},
      archivePrefix={arXiv},
}

@inproceedings{zeroShotCL,
  title={What language model architecture and pretraining objective works best for zero-shot generalization?},
  author={Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Le Scao, Teven and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={22964--22984},
  year={2022},
  organization={PMLR}
}

@article{DOvsED,
  title={Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder},
  author={Fu, Zihao and Lam, Wai and Yu, Qian and So, Anthony Man-Cho and Hu, Shengding and Liu, Zhiyuan and Collier, Nigel},
  journal={arXiv preprint arXiv:2304.04052},
  year={2023}
}

@inproceedings{Comp_EE_EO_DO,
  title={Compare Encoder-Decoder, Encoder-Only, and Decoder-Only Architectures for Text Generation on Low-Resource Datasets},
  author={Cai, Pei-Xuan and Fan, Yao-Chung and Leu, Fang-Yie},
  booktitle={Advances on Broad-Band Wireless Computing, Communication and Applications: Proceedings of the 16th International Conference on Broad-Band Wireless Computing, Communication and Applications (BWCCA-2021)},
  pages={216--225},
  year={2022},
  organization={Springer}
}

@article{FT_normal,
  title={Modifying memories in transformer models},
  author={Zhu, Chen and Rawat, Ankit Singh and Zaheer, Manzil and Bhojanapalli, Srinadh and Li, Daliang and Yu, Felix and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2012.00363},
  year={2020}
}

@inproceedings{geva-key-value,
    title = "Transformer Feed-Forward Layers Are Key-Value Memories",
    author = "Geva, Mor  and
      Schuster, Roei  and
      Berant, Jonathan  and
      Levy, Omer",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.446",
    doi = "10.18653/v1/2021.emnlp-main.446",
    pages = "5484--5495",
    abstract = "Feed-forward layers constitute two-thirds of a transformer model{'}s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys{'} input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model{'}s layers via residual connections to produce the final output distribution.",
}

@article{actFunctions,
title = {Activation functions in deep learning: A comprehensive survey and benchmark},
journal = {Neurocomputing},
volume = {503},
pages = {92-108},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.06.111},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222008426},
author = {Shiv Ram Dubey and Satish Kumar Singh and Bidyut Baran Chaudhuri},
keywords = {Activation Functions, Neural networks, Convolutional neural networks, Deep learning, Overview, Recurrent Neural Networks},
abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: https://github.com/shivram1987/ActivationFunctions.}
}

@misc{MultiQueryAttention,
      title={Fast Transformer Decoding: One Write-Head is All You Need}, 
      author={Noam Shazeer},
      year={2019},
      eprint={1911.02150},
      archivePrefix={arXiv},
}

@misc{llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
}

@article{gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

@misc{rotaryposEmb,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2022},
      eprint={2104.09864},
      archivePrefix={arXiv},
}

@misc{DAIknowledge,
      title={Knowledge Neurons in Pretrained Transformers}, 
      author={Damai Dai and Li Dong and Yaru Hao and Zhifang Sui and Baobao Chang and Furu Wei},
      year={2022},
      eprint={2104.08696},
      archivePrefix={arXiv},
}

@misc{knowledgeBASES,
      title={Language Models as Knowledge Bases?}, 
      author={Fabio Petroni and Tim Rocktäschel and Patrick Lewis and Anton Bakhtin and Yuxiang Wu and Alexander H. Miller and Sebastian Riedel},
      year={2019},
      eprint={1909.01066},
      archivePrefix={arXiv},
}


@misc{MEND,
      title={Fast Model Editing at Scale}, 
      author={Eric Mitchell and Charles Lin and Antoine Bosselut and Chelsea Finn and Christopher D. Manning},
      year={2022},
      eprint={2110.11309},
      archivePrefix={arXiv},
}

@article{DPO,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@article{Contrastive_L,
  title={Contrastive decoding: Open-ended text generation as optimization},
  author={Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:2210.15097},
  year={2022}
}

@article{GRATH,
  title={GRATH: Gradual Self-Truthifying for Large Language Models},
  author={Chen, Weixin and Li, Bo},
  journal={arXiv preprint arXiv:2401.12292},
  year={2024}
}

@article{RLHF,
  title={Training language models to follow instructions with human feedback, 2022},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={URL https://arxiv. org/abs/2203.02155},
  volume={13},
  year={2022}
}

@article{RealRLHF,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{RLHF2,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{RAG_surv,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@article{untyingBIDI,
  title={Untying the Reversal Curse via Bidirectional Language Model Editing},
  author={Ma, Jun-Yu and Gu, Jia-Chen and Ling, Zhen-Hua and Liu, Quan and Liu, Cong},
  journal={arXiv preprint arXiv:2310.10322},
  year={2023}
}

@article{retrievalMulti,
  title={Retrieval-augmented Multilingual Knowledge Editing},
  author={Wang, Weixuan and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:2312.13040},
  year={2023}
}

@misc{IKE,
      title={Can We Edit Factual Knowledge by In-Context Learning?}, 
      author={Ce Zheng and Lei Li and Qingxiu Dong and Yuxuan Fan and Zhiyong Wu and Jingjing Xu and Baobao Chang},
      year={2023},
      eprint={2305.12740},
      archivePrefix={arXiv},
}

@misc{huggingfaces,
      title={HuggingFace's Transformers: State-of-the-art Natural Language Processing}, 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
}

@article{emptying,
  title={Emptying the Ocean with a Spoon: Should We Edit Models?},
  author={Pinter, Yuval and Elhadad, Michael},
  journal={arXiv preprint arXiv:2310.11958},
  year={2023}
}

@misc{pytorch,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
}

@misc{GPT-NeoX,
      title={GPT-NeoX-20B: An Open-Source Autoregressive Language Model}, 
      author={Sid Black and Stella Biderman and Eric Hallahan and Quentin Anthony and Leo Gao and Laurence Golding and Horace He and Connor Leahy and Kyle McDonell and Jason Phang and Michael Pieler and USVSN Sai Prashanth and Shivanshu Purohit and Laria Reynolds and Jonathan Tow and Ben Wang and Samuel Weinbach},
      year={2022},
      eprint={2204.06745},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{GPT-J,
  title={GPT-J-6B: A 6 billion parameter autoregressive language model},
  author={Wang, Ben and Komatsuzaki, Aran},
  year={2021}
}

@misc{PMET,
      title={PMET: Precise Model Editing in a Transformer}, 
      author={Xiaopeng Li and Shasha Li and Shezheng Song and Jing Yang and Jun Ma and Jie Yu},
      year={2023},
      eprint={2308.08742},
      archivePrefix={arXiv},
}

@misc{CrossLingual,
      title={Cross-Lingual Knowledge Editing in Large Language Models}, 
      author={Jiaan Wang and Yunlong Liang and Zengkui Sun and Yuxuan Cao and Jiarong Xu},
      year={2023},
      eprint={2309.08952},
      archivePrefix={arXiv},
}

@misc{layerNorm,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
}

@misc{REMEDI,
      title={Inspecting and Editing Knowledge Representations in Language Models}, 
      author={Evan Hernandez and Belinda Z. Li and Jacob Andreas},
      year={2023},
      eprint={2304.00740},
      archivePrefix={arXiv},
}

@misc{simalign,
      title={SimAlign: High Quality Word Alignments without Parallel Training Data using Static and Contextualized Embeddings}, 
      author={Masoud Jalili Sabet and Philipp Dufter and François Yvon and Hinrich Schütze},
      year={2021},
      eprint={2004.08728},
      archivePrefix={arXiv},
}


@misc{pararel,
      title={A Review on Language Models as Knowledge Bases}, 
      author={Badr AlKhamissi and Millicent Li and Asli Celikyilmaz and Mona Diab and Marjan Ghazvininejad},
      year={2022},
      eprint={2204.06031},
      archivePrefix={arXiv},
}

@misc{logit_lens,
  title = {{interpreting gpt: the logit lens} Less-Wrong, 2020},
  author={nostalgebraist},
  howpublished = {\url{https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}},
}

@article{formalAlgo,
  title={Formal algorithms for transformers},
  author={Phuong, Mary and Hutter, Marcus},
  journal={arXiv preprint arXiv:2207.09238},
  year={2022}
}

@article{Bitfit,
  title={Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models},
  author={Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2106.10199},
  year={2021}
}

@InProceedings{integrals_ref,
  title = 	 {Axiomatic Attribution for Deep Networks},
  author =       {Mukund Sundararajan and Ankur Taly and Qiqi Yan},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3319--3328},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/sundararajan17a.html},
  abstract = 	 {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.}
}






@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}