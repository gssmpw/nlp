\section{Related Work}
\textbf{Retrieval Methods.} Rather than directly relying in LLMs for specific queries, open-domain question answering systems has historically been driven by the development of algorithms aligning queries with external database sources \cite{BM25}. Recent advancements in aligning retrieval-based methods with LLMs have demonstrated promise in this domain \cite{dense_passage,GAR,RETRO}, with retrieval augmented generation \cite{RAG} showing capabilities in both multimodal \cite{Chen_multimodal,Chen_multimodal2,RAGmultimodal} and multilingual \cite{retrievalMulti} contexts. However, while the use of external sources avoids the need for fine-tuning, challenges still persist in precisely identifying the relevant context for a given query \cite{RAG_surv}.

\textbf{Truthfulness.} Efforts to enhance the reliability of LLMs without depending on external sources have been a focal point of recent research. Aligning LLMs with human feedback has been explored through Reinforcement Learning from Human Feedback \cite{RLHF2,RLHF} and Direct Preference Optimization \cite{DPO}, offering valuable insights for veracity alignment \cite{GRATH}. Additionally, approaches contrasting hidden representations of these models have also yielded significant results \cite{Contrastive_L,DOLA} in this direction.

\textbf{Factual Knowledge Editors.}
This research builds upon MEMIT, a method adept at efficiently introducing knowledge by modifying the internal weights of decoder-only architectures, surpassing the effectiveness of earlier meta-learning techniques like MEND \cite{MEND} and constrained fine-tuning \cite{FT_normal}. Nevertheless, less intrusive alternatives, which selectively modify specific hidden states of the model during inference according to the provided prompt, have also demonstrated remarkable efficacy in knowledge editing. Notable examples include REMEDI \cite{REMEDI}, GRACE \cite{Aging}, and SERAC \cite{serac}. 

\textbf{Multilingual Domain.}
The emergence of knowledge editors and multilingual models raises questions about whether the information is being inserted from a cross-lingual perspective. Current findings suggest that these methods are not entirely language-independent \cite{PolyglotOrNot,CrossLingual}, with approaches based on prompting and retrieval yielding stronger results \cite{IKE,retrievalMulti}. 

% Nevertheless, concurrent works that point to the existence of language-independent knowledge neurons \cite{JourneyCenterKnowledge} and masking-based approaches \cite{LanguageAnisotropic} hint at potential solutions to the cross-lingual knowledge editing problem.

% \cite{ripple_effects}
% \cite{DOLA} (Contrastive)