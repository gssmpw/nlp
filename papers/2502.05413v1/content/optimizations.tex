\section{Lightweight Selective Tracing}
To collect sufficient real-time data for anomaly diagnosis in the LLM training cluster, \sysname{}’s tracing daemon offers backbone-agnostic and lightweight full-spectrum tracing. Its design focuses on two key aspects: determining what information to collect and establishing how to collect it efficiently.

% To gather sufficient real-time data for anomaly detection within LLM training clusters, \sysname{}’s tracing daemon provides backbone-agnostic and lightweight full-spectrum tracing.
% Its design focuses on two aspects: identifying what information to collect and determining how to collect it efficiently.

\subsection{Key Segment Instrumentation}
% Collecting all infor

Since profiling APIs like CUPTI\cite{cupti2024} can operate in a background thread, the runtime data collection overhead primarily stems from high memory usage rather than interference with computing resources. For instance, profiling a Llama-70B model trained on 512 H800 GPUs using PyTorch’s built-in profiler produces a log file of $5.5\text{GB}$ (in JSON format, compressed to $451\text{MB}$) for each training step. This substantial memory overhead renders such arbitrary profiling methods impractical for continuously collecting real-time data to support anomaly diagnostics.

% Collecting comprehensive real-time information, including data from all launched GPU kernels, introduces significant overhead. Profiling APIs, such as CUPTI~\cite{cupti2024}, can operate in a background thread, meaning the primary overhead arises not from the slowdown of the running training but from the high memory usage incurred during real-time data collection.
% For instance, profiling a Llama-70B model trained on 512 A100-80GB GPUs using PyTorch’s built-in profiler generates a log file of $5.5\text{GB}$ (in JSON format, compressed to $451\text{MB}$) for each training step, while the profiling process only causes noticeable slowdowns when the profiler dumps the log file after the final profiling step.
% This substantial memory overhead renders such arbitrary profiling methods impractical for continuously collecting real-time data to support anomaly detection.

% \weihao{impact on cluster}
\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figure/2-instrument.pdf}
    \caption{Instrumented key code segments in \sysname{}.}
    \label{fig:key-segment}
    \vspace{-4mm}
\end{figure}

Therefore, \sysname{} selectively instruments code segments of key APIs and kernels to collect real-time information. This design is based on an insight into LLM training on large-scale GPUs: LLM training is predominantly dominated by a limited set of deep learning operators. These operators mainly include matrix multiplication and cross-GPU communication operators. \autoref{fig:key-segment} presents the specific code segments instrumented by \sysname{} for efficient anomaly diagnostics.

% Therefore, \sysname{} turns to selectively instrument code segment of key APIs and kernels for collecting real-time information.
% This design is grounded in the insight regarding the training of LLMs on large-scale GPUs:
% LLM training is predominantly dominated by a limited set of deep learning operators, with matrix multiplication and cross-GPU communication being particularly critical.
% Exhaust profiling, such as tracing all function calls across Python and C++ runtime, is unnecessary and would introduce significant overhead without corresponding benefits.
% \autoref{fig:key-segment} highlights the specific code segments instrumented by \sysname{} for efficient anomaly detection.

As shown in the figure, the instrumented code segments can be broadly categorized into two groups. The first category involves intercepting key API calls, including those related to Python’s garbage collection (GC), PyTorch’s dataloader, and GPU synchronization. These APIs are carefully selected based on empirical insights into performance issues and optimization opportunities. These insights are detailed in \S\ref{sec:slowdown-anomalies}, with corresponding cases discussed in \S\ref{sec:case_study}.

The second category focuses on intercepting critical GPU computation and communication kernels executed at the C++ runtime level. These kernels, primarily provided by optimized libraries\cite{cublas2024,daoFlashAttentionFast,daoFlashAttention2Faster,nccl2024}, account for the majority of the workload during large-scale training. Additionally, there are customized kernels developed by the infrastructure team.

Notably, the above design enables \sysname{} to support backbone-agnostic and extensible tracing capabilities. Extending tracing capabilities for Python-related APIs is straightforward, requiring only the configuration of the specific environment variables in the training scripts, as shown below.
\begin{minted}[
    frame=none,
    obeytabs=true,
    framesep=0mm,
    baselinestretch=0.8,
    fontsize=\footnotesize,
    xleftmargin=1.6em,
    breaklines,
    escapeinside=||,
]{shell}
export TRACED_PYTHON_API="torch.cuda@synchronize"
\end{minted}
Meanwhile, intercepting C++ kernels necessitates explicit registration through a C++ interface. This requirement is feasible, as the infrastructure team takes charge of the development of both these customized operators and \sysname{}, ensuring seamless integration and functionality.


% The code segment instrumentation also enables \sysname{} to support backbone-agnostic and extensible tracing capabilities.
% The instrumented code segments can be broadly categorized into two groups.
% The first category involves intercepting key API calls, including those related to Python’s garbage collection (GC), PyTorch’s dataloader, and GPU synchronization.
% These APIs are carefully chosen based on empirical insights into performance issues and optimization opportunities.
% For example, GC operations and explicit GPU synchronization may introduce stalls of kernel issue during the training, as elaborated in \S\ref{sec:case:stall-free}.
% Extending the tracing capabilities for Python-related APIs is straightforward and only requires setting specific environment variables in the training scripts.

% In the aforementioned example, tracing GPU synchronization in PyTorch is enabled through this approach.
% The second category focuses on intercepting critical GPU computation and communication kernels executed at the C++ runtime level. These kernels, mainly provided by optimized libraries~\cite{cublas2024,daoFlashAttentionFast,daoFlashAttention2Faster,nccl2024}, constitute the majority of the workload during large-scale training.
% For customized kernels, interception requires explicit registration through a C++ interface.
% This requirement is feasible, as the infrastructure team takes charge of the development of both these customized operators and \sysname{}, ensuring seamless integration and functionality.

% Modeling teams develop new modeling algorithms for improving the LLM performance, which are pure python APIs.
% These APIs are often the attributions that incur the slowdown of training jobs compared to the original version of LLM without the new modeling algorithms.


\subsection{Timing in the Background}

With intercepted Python APIs and GPU kernels, FLARE measures their elapsed latencies, as shown in \autoref{fig:timing}. Specifically, a dedicated tracing thread runs in the background to efficiently manage timing data. It employs different timing mechanisms for Python APIs and GPU kernels.

For synchronous Python API calls, FLARE directly records their start and end timestamps and forwards them to the timing manager. For GPU kernels, which execute asynchronously, FLARE injects CUDA events\cite{cudaevents2024} after an interception to record execution status. These events are enqueued for further processing. The timing manager queries the status of the queued events in the background, avoiding any disruption to the training thread. Additionally, during GPU kernel interception, FLARE extracts input specifications, such as memory layout, to support subsequent anomaly diagnostics in \S\ref{sec:diagnose_obvious}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figure/3-timing.pdf}
    \caption{Intercepting and timing the training in the background.}
    \label{fig:timing}
    \vspace{-4mm}
\end{figure}

% With intercepted Python APIs and GPU kernels, \sysname{} measures their elapsed latencies, as depicted in \autoref{fig:timing}. A dedicated tracing thread operates in the background of the main training thread to manage timing data efficiently.
% The timing mechanisms for Python APIs and GPU kernels are implemented differently. For synchronous Python API calls, \sysname{} directly records their start and end timestamps and forwards them to the timing manager.
% For GPU kernels, which execute asynchronously, \sysname{} injects CUDA events~\cite{cudaevents2024} after interception to record execution status.
% These events are enqueued for further processing.
% The timing manager queries the status of the queued events in the background, avoiding any disruption to the training thread.
% To reduce overhead, the events are reused.
% Additionally, during GPU kernel interception, \sysname{} extracts input specifications, such as memory layout, from kernel arguments to support subsequent anomaly detection in \S\ref{sec:slowdown-anomalies}.

As training progresses, the timing manager proactively transmits all real-time data to \sysname{}’s diagnosis engine.
By employing key segment instrumentation and running timing tasks in the background, \sysname{} minimizes both computing resource and memory overhead, ensuring efficient data collection for real-time anomaly diagnostics.
A detailed evaluation of \sysname{}’s real-time overhead is provided in \S\ref{sec:eval:overhead}.

\section{Anomaly Detection and Diagnosis}
Using the real-time data collected by the tracing daemon, \sysname{}’s diagnostic engine identifies and analyzes anomalies encountered during distributed LLM training.
In this section, we present \sysname{}’s holistic diagnostic workflow for addressing two common anomaly symptoms: errors and slowdowns.

\subsection{Fast Runtime Error Diagnosis}

As shown in the left of \autoref{tb:anomaly-analysis}, errors encountered at the beginning of a training job are typically caused by bugs in the training scripts, which can often be addressed easily by the algorithm teams and infrastructure team.
However, diagnosing errors that occur during the training progress is more challenging and critical.
Such errors often stem from issues like operating system crashes, GPU failures, or network disruptions, which can generally be resolved by isolating the problematic machines and restarting the training job.

A typical symptom associated with these errors is the hanging of the training job.
Training LLMs across numerous GPUs in a distributed manner inherently relies on the coordination of training processes.
When the aforementioned errors occur, they rarely affect all training processes simultaneously.
% , leading to a ``hang'' state where training progress stalls.
In this context, \sysname{} focuses on rapidly diagnosing hang errors by identifying faulty machines. Then, \sysname{} routes this information to the operations team, enabling the training job to restart with healthy machines.

Specifically, \sysname{}’s diagnostic engine first detects hang errors by examining the status of tracing daemons.
The tracing daemon operates in the background of the training thread and continuously queries events recorded during job execution.
If it fails to confirm the completion of an event within a predefined timeout interval, it proactively reports a potential hang error to the diagnostic engine.
Similarly, if a tracing daemon does not transmit any real-time data within the specified timeout interval, the diagnostic engine also interprets this as an indication of a hang error.

After hang errors are reported, they are classified as either communication or non-communication errors. \sysname{} diagnoses these errors in two steps: first, a coarse-grained diagnosis through call stack analysis; and second, a fine-grained diagnosis using intra-kernel tracing

\paragraph{Diagnosis using call stack analysis.} 
This diagnosis is used to identify problematic machines encountering non-communication errors. 
% This diagnosis using call stack analysis is coarse-grained and can only identify problematic machines encountering non-communication errors.
\autoref{fig:hang-non-comm} illustrates an example of hang-error diagnosis via call stack analysis.
% involving $N$ training processes.
As shown in the left of \autoref{fig:hang-non-comm}, when the training process of rank-$0$ crashes or is suspended due to a non-communication error, it halts at a call stack corresponding to a non-communication function.
In contrast, the training processes of other ranks continue executing correctly and eventually stop at a call stack associated with a communication-related function that depends on coordination with rank-$0$.
In this scenario, the machine associated with rank-$0$ is identified as the source of the error.
It should be noted that, although these non-communication errors may cause direct crashes, the call stack analysis could also locate the faulty machine. 

% It should be noted that, at times, low-level issues directly cause crashes, which can also be identified through stack analysis.

However, communication hang errors cannot be identified through call stack analysis.
As shown in the right of \autoref{fig:hang-non-comm}, the training processes of all ranks terminate at the same call stack corresponding to a communication function, such as allreduce or allgather.
In this scenario, there are no distinct differences between ranks based on call stack analysis.

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figure/4-hang-stack.pdf}
    \caption{Diagnosing hang errors via call stack analysis.}
    \label{fig:hang-non-comm}
    \vspace{-4mm}
\end{figure}

We further investigate the symptoms of communication hang errors and obtain two observations. Firstly, some communication hang errors generate error logs. For instance, if the link between RDMA NICs breaks, an error code of \texttt{12} is produced. Secondly, more hang errors result in an endless loop within the launched communication kernels, ultimately leading to job termination after a predefined timeout. To identify the unhealthy machine responsible for such errors, a straightforward solution is to perform a binary search by executing communication tests across all involved GPUs. This approach has a complexity of $O(\log N)$ and requires hours to pinpoint the faulty machine among thousands of GPUs~\cite{wuFALCONPinpointing}.

\paragraph{Diagnosis using intra-kernel tracing.}
Faced with this problem, \sysname{} introduces a minute-level diagnostic approach using intra-kernel tracing. This intra-kernel tracing leverages CUDA-GDB, the debugging tool for CUDA programming.

Specifically, \sysname{}'s diagnostic engine instructs the tracing daemon to attach the halted training processes with CUDA-GDB before terminating them.
Once attached, the tracing daemon executes a script capable of automatically extracting detailed communication statuses to identify unhealthy machines.
\autoref{fig:hang-comm} depicts an example of diagnosing communication hang errors in a hanging ring-allreduce kernel.

In the ring-allreduce kernel, each thread block is responsible for transmitting data between linked adjacent ranks within the kernel’s constructed ring.
The data are split into chunks and thread blocks of adjacent ranks work together to transmit the chunks step by step.
Thus, \sysname{} could retrieve the register values corresponding to the loop steps used for data transmission between linked ranks. 
Theoretically, the connection with the minimum step reveals the related GPUs experiencing errors.
This intra-kernel tracing process is performed in parallel across all involved GPUs.
As a result, its complexity is $O(1)$, enabling completion within a few minutes.

\sysname{} then routes the diagnostic information for detected errors to the operations team, assisting with tasks such as isolating faulty machines and restarting the training job.

\subsection{Aggregation for Slowdown Diagnosis}


\label{sec:slowdown-anomalies}
As demonstrated in \S\ref{sec:anomalies}, slowdowns can be attributed to changes across the entire training stack.
Meantime, slowdowns caused by software changes introduced by the algorithm and infrastructure teams are often subtle and challenging to detect.
Identifying these changes typically requires comparisons across historical training jobs and prior training steps.
In contrast, hardware changes, such as GPU underclocking or network jitter, are more apparent and can be detected solely through comparisons across training steps.

To holistically identify these anomalies, \sysname{} aggregates real-time data collected from the tracing daemon into five primary metrics, shown in \autoref{fig:aggregated-metric}.
\textit{
These metrics are based on the consensus that a “healthy” training pipeline should exhibit a timeline saturated with GPU kernels dedicated to either computation or communication.}
Computation kernels should achieve high FLOPS, while communication kernels are expected to utilize high bandwidth.
Any deviations from these characteristics point to idle GPU resources, signaling potential slowdowns in training jobs.
Of the five metrics, three are commonly used in existing works~\cite{jiangMegaScaleScaling,wuFALCONPinpointing}, while the other two are newly introduced by \sysname{}.
% to assess training efficiency.

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figure/5-hang-intra-kernel.pdf}
    \caption{An example of diagnosing communication hang errors in a ring-allreduce kernel using intra-kernel tracing.}
    \label{fig:hang-comm}
    \vspace{-4mm}
\end{figure}
\begin{figure*}
    \centering
    \includegraphics[width=.85\linewidth]{figure/6-aggregated-metric.pdf}
    \vspace{-2mm}
    \caption{A timeline of a distributed training job annotated with aggregated metrics used for diagnosing slowdowns in \sysname{}.}
    \label{fig:aggregated-metric}
    \vspace{-4mm}
\end{figure*}
\subsubsection{Diagnosing Obvious Slowdown}
\label{sec:diagnose_obvious}
\paragraph{\protect\circlenumber{1} Training throughput for detecting slowdown.}
Training throughput is the most straightforward metric for detecting slowdowns.
\sysname{} measures training throughput by timing the rate at which input data is consumed by the training pipeline.
This is achieved by instrumenting the dataloader API of Pytorch.
As a macro performance metric, training throughput directly reflects slowdowns in training efficiency through comparison to historical training jobs and between training steps of the same job.
However, \sysname{} cannot diagnose the specific factors contributing to the slowdown.
To address this, \sysname{} relies on the following four micro metrics to further investigate the underlying causes.

\paragraph{\protect\circlenumber{2} FLOPS for slow critical kernel.}
\sysname{} monitors the FLOPS of instrumented critical computation kernels, leveraging timing data and input layout.
By comparing the FLOPS of identical kernels across different ranks, \sysname{} diagnoses GPUs that exhibit poor computational performance, often caused by issues like GPU underclocking.
Machines affected by GPU underclocking are then routed to the operations team for isolation.
Additionally, by analyzing FLOPS, \sysname{} identifies un-optimized kernels in training jobs, particularly those with large input sizes but low FLOPS.
These anomalies are detected without interrupting training jobs and are subsequently routed to the infrastructure team for further investigation.
Notably, when analyzing FLOPS data, \sysname{} accounts for the impact of communication kernels that overlap with computation kernels.%\weihao{real data}
This ensures that computation kernels with falsely low FLOPS are not mistakenly flagged. 

\paragraph{\protect\circlenumber{3} Bandwidth for slow connection.}
\sysname{} monitors the bandwidth of communication kernels. A communication operator requires launching the communication kernels on all ranks. Since variations in kernel-issue timestamps exist across different ranks, \sysname{} calculates the communication bandwidth by utilizing the start and end timestamps of the final communication kernels issued across all participating ranks.
The captured communication bandwidth is compared with offline profiled data.
If low-bandwidth communication is detected, \sysname{} conducts a communication test using binary search to pinpoint machines experiencing issues such as network congestion.
These slowdowns are then identified and routed to the operations team for resolution.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figure/7-issue-distribution.pdf}
%     \caption{Typical issue distribution patterns in communication kernels for both healthy and unhealthy LLM training jobs.}
%     \label{fig:issue-distribution}
% \end{figure}


\subsubsection{Diagnosing Obscured Slowdown}
While the above three metrics ensure that both critical computation and communication GPU kernels operate at high performance, they do not cover the less critical operations, such as various CPU operations and element-wise activation GPU kernels. Meantime, \sysname{}’s selective key segment instrumentation also omits the monitoring of these operations. 
% Metric-\circlenumber{1},\circlenumber{2} and \circlenumber{3} primarily ensure that both critical computation and communication GPU kernels operate at high performance.
% Furthermore, \sysname{}’s selective key segment instrumentation omits the monitoring of less critical operations, such as various CPU operations and element-wise activation GPU kernels.
% Their impact on the training efficiency is obscured, but this does not mean that \sysname{} disregards their potential contributions to slowdowns.

To diagnose their potential contributions to slowdowns, we further classify these not-instrumented operations into three categories: intra-step CPU operations, inter-step CPU operations, and minority GPU kernels.
Intra-step CPU operations and inter-step CPU operations differ due to their occurrences within the timeline of training steps.
Minority GPU kernels refer to those GPU kernels that often occupy little GPU computation resources. 
Specifically, two metrics are introduced for the diagnostics: issue latency distribution for intra-step CPU operations and void percentage for inter-step CPU operations and minority GPU kernels.

\begin{figure*}
    \centering
    \includegraphics[width=.95\linewidth]{figure/9-latency-overhead.pdf}
    \vspace{-2mm}
    \caption{Runtime overhead in terms of latency with various models, backbones, and number of GPUs.}
    \label{fig:eval:latency-overhead}
    \vspace{-4mm}
\end{figure*}

\paragraph{\protect\circlenumber{4} Issue latency distribution for kernel-issue stall.}\label{sec:diagnose:issue}
In a well-optimized parallel backbone, only the necessary intra-step CPU operations for launching GPU kernels or coordinating the training processes are expected. However, algorithm teams may inadvertently introduce unnecessary GPU synchronizations when modifying the LLM model. Meantime, certain function calls, such as GC~\cite{jiangMegaScaleScaling,shoeybiMegatronLMTraining}, may be implicitly triggered by the Python runtime. These intra-step CPU operations can occur repeatedly during the model’s forward pass, bringing considerable overhead. In such cases, these operations cause an anomaly known as a kernel-issue stall, leading to GPU idle time within the training step.

\protect\circlenumber{4}--1 in \autoref{fig:aggregated-metric} shows the example of Python runtime GC.
In the figure, the Python runtime GC stalls the CPU thread and causes the lagging of GPU kernels on rank-1.
Although the communication kernel on rank-0 is issued without stalling, it simply waits for the one on rank-1, ultimately causing the overall training speed to decline.
\protect\circlenumber{4}--2 in \autoref{fig:aggregated-metric} shows an example of unnecessary GPU synchronization introduced by the developers from the algorithm teams.
As all ranks wait for the completion of communication kernels, the kernel issue of follow-up kernels is stalled and not overlapped with GPU computation.
When such unnecessary synchronization occurs repeatedly across the model’s forward pass, it ultimately results in a slowdown of the training speed.

Originally, detecting these anomalies of kernel-issue stall requires investigating the aggregated timeline with much human effort.
Faced with this issue, \sysname{} proposes a new metric, named issue latency distribution, for diagnosing this issue without human intervention.
Kernel-issue latency is defined as the time elapsed between the kernel’s issue timestamp and the start timestamp of its execution on the GPU.
% \autoref{fig:issue-distribution} illustrates the pattern differences in kernel issue latency distribution between healthy and unhealthy LLM training jobs. 
% In the figure, two cases of kernel issue stall are: one caused by unmanaged Python runtime GC, and the other by unnecessary GPU synchronization introduced by algorithm teams.
Based on our observation of anomalies of kernel-issue stall, the kernel-issue latencies of unhealthy training jobs should be much shorter than those of a healthy training job.

By monitoring runtime issue latency distribution, and comparing it with the historical data, \sysname{} could identify the anomaly of kernel-issue stall. Then, \sysname{} routes them to algorithm and infrastructure teams for resolution, as they are commonly software issues.
\S\ref{sec:eval:issue} and \S\ref{sec:case:stall-free} demonstrate the effectiveness of issue latency distribution in diagnosing kernel-issue stalls, even when the slowdown is minimal.



\paragraph{\protect\circlenumber{5} Void percentage for other un-covered operations.}
While the tracing daemon only instruments the critical operators, inter-step CPU operations and minority GPU kernels both manifest as empty time slots in the visualized timeline, as shown in \autoref{fig:aggregated-metric}.
Consequently, \sysname{} introduces a metric, termed the void percentage, to identify slowdowns caused by these factors.

% Analyzing the GPU timeline of a visualized training job reveals that inter-step CPU operations and minority GPU kernels both manifest as empty GPU slots, as shown in \autoref{fig:aggregated-metric}. Consequently, \sysname{} introduces a metric, termed the void percentage, to identify slowdowns caused by these factors.

As for inter-step CPU operations, as depicted by \circlenumber{5}–2 in \autoref{fig:aggregated-metric}, \sysname{} measures the latency between the last kernel preceding the dataloader and the first kernel following the same dataloader. \sysname{} then computes the void percentage for inter-step CPU operations using the following equation:
\begin{equation}
V_{inter} = T_{inter}\ /\ T_{step} 
\end{equation}
where $T_{inter}$ represents the latency associated with inter-step CPU operations, and $T_{step}$ denotes the total latency of the training step.

As for minority GPU kernels, as shown by \circlenumber{5}–1 in \autoref{fig:aggregated-metric}, \sysname{} first automatically detects empty slots where GPU kernels are launched but remain un-executed. These empty slots signify that the GPUs are occupied by kernels outside the scope of \sysname{}’s tracing mechanism. \sysname{} subsequently accumulates these slots for each training step and computes the void percentage using the following equation:
\begin{equation}
V_{minority} = T_{minority}\ /\ (T_{step} - T_{inter})
\end{equation}
where $T_{minority}$ is the latency of all minority GPU kernels.

When the void percentages ($V_{inter}$ and $V_{minority}$) surpass the predefined thresholds for a specific parallel backbone, \sysname{} annotates the training job with potential slowdowns attributed to inter-step CPU operations or minority GPU kernels. Then, \sysname{} notifies the algorithm and infrastructure team for further investigation.

% \subsection{Diagnostic Summary}
% \weihao{we may give a brief summary here.}