

\section{Evaluation}
In this section, we present experiments to demonstrate \sysname{}’s effectiveness from various perspectives.
Specifically, we first evaluate \sysname{}’s runtime overhead in terms of latency and memory, showcasing its lightweight nature for runtime anomaly diagnosis.
% This experiment also spans various models and commonly used parallel backbones.
Then, we assess the effectiveness of \sysname{}’s novel diagnostic mechanisms: intra-kernel tracing for communication hang error diagnosis and issue distribution metric for kernel-issue stall diagnosis.

\subsection{Runtime Overhead}
\label{sec:eval:overhead}

We evaluate \sysname{} on four parallel backbones: Megatron~\cite{shoeybiMegatronLMTraining}, FSDP~\cite{zhaoPyTorchFSDP}, DeepSpeed~\cite{rajbhandariZeROMemory}, and TorchRec~\cite{ivchenkoTorchRecPyTorch}. Among these, Megatron, FSDP, and DeepSpeed are widely used for LLM training, while TorchRec is employed for training large recommendation models within Ant Group. Four models are benchmarked, spanning language, vision, and recommendation tasks: two large language models (Llama 18B and 70B), one large vision model (Llama Vision 40B), and one recommendation model (DLRM 72M).

The latency overhead experiment is conducted on 1,024 H800 GPUs deployed across 128 servers with RoCE connectivity. \autoref{fig:eval:latency-overhead} presents the latency overhead introduced by \sysname{} with various models, backbones, and number of GPUs. As shown, \sysname{} incurs a latency overhead of $0.43\%$ for three LLM training backbones and $1.02\%$ for TorchRec.

% The runtime overhead experiment is conducted on 1,024 H800 GPUs deployed across 128 servers with InfiniBand connectivity.
% We evaluate \sysname{} on four parallel backbones: Megatron~\cite{shoeybiMegatronLMTraining}, FSDP~\cite{zhaoPyTorchFSDP}, DeepSpeed~\cite{rajbhandariZeROMemory}, and TorchRec~\cite{ivchenkoTorchRecPyTorch}. Among these, Megatron and FSDP are extensively used for LLM training, while TorchRec is employed for training large recommendation models within \groupname{}. DeepSpeed, on the other hand, is a widely adopted backbone for LLM training across the broader community.
% Four models are benchmarked, spanning language, vision, and recommendation tasks: two large language models (Llama 18B and 70B), one large vision model (Llama Vision 40B), and one recommendation model (DLRM 72M).



The memory overhead experiment is conducted on two setups, which are 16 A100 GPUs on 2 nodes and 1536 H800 GPUs on 192 nodes. We compare \sysname{} with \texttt{Torch Full}, \texttt{Torch w/o Stack}, \texttt{Torch w/o Layout\&Stack}, and \sysname{}. \texttt{Torch w/o Stack} refers to using the PyTorch builtin profiler with stack tracing disabled, while \texttt{Torch w/o Layout\&Stack} further disables matrix layout tracing.

\autoref{fig:log-overhead} shows the memory overhead results on 16 A100 GPUs. \sysname{} consumes only $0.39\%$, $1.76\%$, and $2.48\%$ of memory overhead for the respective configurations of PyTorch profiler. Specifically, \sysname{} generates a maximum of $0.78\text{MB}$ of tracing logs per GPU. Besides, in a real-world Llama-20B training job on 1536 H800 GPUs, \sysname{} generated only a $1.5\text{MB}$ tracing log per GPU.
TorchRec is omitted from this experiment, as monitoring a recommendation model generates minimal logs.
% —much smaller than an LLM—

From the above results, we can conclude that, regardless of the GPU scale, parallel backbone, parallel strategy, or model type used, \sysname{} consistently maintains an extremely low runtime overhead in terms of both latency and memory. The lightweight selective tracing facilitates \sysname{}'s deployment within our large training cluster, serving as a diagnostic framework for diverse training jobs.


% \autoref{fig:log-overhead} also compares the memory consumption of \texttt{Torch Full}, \texttt{Torch w/o Stack}, \texttt{Torch w/o Layout\&Stack}, and \sysname{}.
% \texttt{Torch w/o Stack} refers to using the PyTorch built-in profiler with stack tracing disabled, while \texttt{Torch w/o Layout\&Stack} further disables matrix layout tracing.
% Compared to the PyTorch profiler, \sysname{} consumes only $0.39\%$, $1.76\%$, and $2.48\%$ of memory for the respective configurations.
% In a real-world Llama-20B training job on 1536 H800 GPUs, \sysname{} generated only a $1.5\text{MB}$ file per GPU when instructed to dump the logs.

% Regardless of the GPU scale, parallel backbone, parallel strategy, or model type used, \sysname{} consistently maintains an extremely low runtime overhead in terms of both latency and memory.
% \sysname{}’s lightweight full-spectrum tracing facilitates its deployment within our large training cluster as a diagnostic framework for diverse training jobs, ranging from large-scale LLM training to smaller recommendation model training jobs.

\begin{figure}
    \centering
    \includegraphics[width=0.92\linewidth]{figure/10-log-memory.pdf}
    \vspace{-2mm}
    \caption{Memory consumption of dumped logs per GPU per step using the PyTorch profiler and \sysname{} while training a Llama-70B model on 16 A100 GPUs.}
    \label{fig:log-overhead}
    \vspace{-4mm}
\end{figure}


\subsection{Effectiveness of Intra-kernel Tracing}

We evaluate the intra-kernel tracing mechanism on 16 A100 GPUs across two servers with RoCE connectivity. Given that most communication kernels are ring-based, this experiment focuses on evaluating ring-allreduce. Specifically, we customize the training script composed solely of communication kernels, with one GPU intentionally suspended to simulate a hang error caused by communication issues. 

\autoref{fig:eval:intra-kernel} illustrates the pinpointing latencies for intra- and inter-server communication.
% Since communication kernels vary with the underlying GPU interconnects and protocols~\cite{NVIDIA_NCCL_User_Guide}, 
The figure presents the latency results for three communication protocols~\cite{NVIDIA_NCCL_User_Guide} and cross-node configurations.
As shown, \sysname{} requires $29.4\text{\textasciitilde}309.2s$ to detect erroneous GPUs across different scenarios. Among the protocols, \sysname{} performs best when the SIMPLE protocol is used for communication. This is because, with the SIMPLE protocol, \sysname{} only needs to scan the first thread of each thread block to check the steps, whereas the other two protocols require scanning the entire thread block.

% The effectiveness of intra-kernel tracing is evaluated on 16 A100 GPUs across two servers with RoCE connectivity, as its complexity is $O(1)$ and performance remains consistent regardless of scale. Furthermore, since most communication kernels are ring-based, this experiment focuses on evaluating ring-allreduce.
% However, diagnosis using intra-kernel tracing may vary depending on the underlying GPU interconnect mediums and the communication protocols employed~\cite{NVIDIA_NCCL_User_Guide}.
% \autoref{fig:eval:intra-kernel} illustrates the pinpointing latencies for intra/inter-machine communication across three protocols~\cite{NVIDIA_NCCL_User_Guide}. In this experiment, the training scripts consist solely of communication kernels, and a GPU is intentionally suspended to simulate a hang error caused by communication issues.

When comparing intra-server and inter-server results, \sysname{} performs better when the ring-allreduce operation spans multiple servers. This is because intra-server GPUs are connected via NVLink, whereas inter-server GPUs communicate through NICs. Communication kernels launched over NICs involve fewer thread blocks, as NICs have fewer internal links compared to NVLink. As a result, \sysname{} scans fewer thread blocks for error diagnosis in inter-server scenarios.

In summary, the intra-kernel tracing mechanism can detect erroneous GPUs in a maximum of $309.2s$. Notably, as the complexity of intra-kernel tracing is $O(1)$, these results remain consistent regardless of scale.

% As shown in the figure, \sysname{} requires $29.c\text{\textasciitilde}309.2ms$ to detect the erroneous GPUs in different cases. Comparing the results of different protocols, \sysname{} performs best when the SIMPLE protocol is used for communication. This is because, with the SIMPLE protocol, \sysname{} only needs to scan the first thread of each thread block to check the steps, whereas the other two protocols require scanning the entire thread block. When comparing intra-server and inter-server results, \sysname{} performs better when the ring-allreduce spans multiple machines. This is due to intra-server GPUs being connected via NVLink, while inter-server GPUs are connected through NICs. Communication kernels launched over NICs involve fewer thread blocks, as NICs have fewer internal links than NVLink. Consequently, \sysname{} scans fewer thread blocks for hang-error diagnosis in inter-server scenarios.
\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{figure/11-gdb-latency.pdf}
    \vspace{-2mm}
    \caption{Latency for pinpointing the erroneous GPUs causing a hang error in ring-allreduce with different protocols.}
    \label{fig:eval:intra-kernel}
    \vspace{-4mm}
\end{figure}
\begin{figure*}
    \centering
    \includegraphics[width=.9\linewidth]{figure/12-cdfs.pdf}
    \vspace{-2mm}
    \caption{Issue distribution across all communication kernels for a Llama-20B model trained with Megatron and 256 GPUs, including the overall CDF and the CDFs for each type of communication kernel, respectively.}
    \label{fig:eval:issue-distribution}
    \vspace{-4mm}
\end{figure*}
\subsection{Effectiveness of Issue Latency Distribution}
\label{sec:eval:issue}

In this experiment, we evaluate the issue latency distribution using Llama-20B running on 256 H800 GPUs across 32 servers connected via RoCE. \autoref{fig:eval:issue-distribution} illustrates the issue latency distribution for all communication kernels in the \texttt{Unhealthy-GC}, \texttt{Unhealthy-Sync}, and \texttt{Healthy} scenarios. In the \texttt{Unhealthy-GC} scenario, GC is implicitly triggered by the Python runtime. In the \texttt{Unhealthy-Sync} scenario, an unintended GPU synchronization call is added within the transformer block, leading to repetitive GPU synchronizations during the model’s forward pass. In the \texttt{Healthy} scenario, GC is efficiently managed by the parallel backbone, and no unnecessary synchronizations are introduced.

% The effectiveness of the issue distribution analysis is demonstrated using a training job of Llama-70B running on 256 H800 GPUs across 32 servers with InfiniBand connectivity. 
% In this experiment, we collect issue latencies for all communication kernels, as kernel-issue stalls significantly impact communication performance, which requires coordination across kernels of different ranks.
% \autoref{fig:eval:issue-distribution} shows the issue distribution for the \texttt{Unhealthy-GC}, \texttt{Unhealthy-Sync}, and \texttt{Healthy} cases. 
% For \texttt{Unhealthy-GC}, garbage collection (GC) is automatically managed by the Python runtime.
% For \texttt{Unhealthy-Sync}, a GPU synchronization call is added within the transformer block, resulting in repetitive and unnecessary GPU synchronizations during the model’s forward pass.
% In the \texttt{Healthy} case, GC is effectively managed by the parallel backbone, and no unnecessary synchronizations are introduced.

As shown in the figure, the issue latency distribution patterns align with our claim in \S\ref{sec:diagnose:issue}. The issue latency CDF of a healthy LLM training job increases linearly, whereas the issue latency CDFs for \texttt{Unhealthy-GC} and \texttt{Unhealthy-Sync} exhibit a much steeper rise. This is because the issue latencies of different ranks in the healthy scenario are solely influenced by the collective communication operator, resulting in a uniform distribution. In contrast, in the cases of \texttt{Unhealthy-GC} and \texttt{Unhealthy-Sync}, while some ranks are affected, their latencies become very short due to the delayed start of the issue time.

Since both GC and GPU synchronizations span the entire model forward pass, all communication kernels are affected, as illustrated in \autoref{fig:eval:issue-distribution}. Furthermore, each training process triggers GC independently, and the GC operation for a single process is more time-consuming than GPU synchronization. Consequently, the issue latency distribution for \texttt{Unhealthy-GC} is worse than that of \texttt{Unhealthy-Sync}.

% As shown in the figure, the issue distribution pattern aligns with our claim in \S\ref{sec:diagnose:issue}. The issue latency CDF of a healthy LLM training job increases linearly, while the issue latency CDFs for \texttt{Unhealthy-GC} and \texttt{Unhealthy-Sync} rise much more steeply. This indicates that many GPU kernels in these cases are stalled by CPU operations. Since both GC and GPU synchronizations span the entire model forward pass, all communication kernels are impacted, as shown in \autoref{fig:eval:issue-distribution}. Additionally, each training process triggers GC independently, and the GC of a single process is more time-consuming than synchronization.
% As a result, the issue distribution for \texttt{Unhealthy-GC} is worse than that for \texttt{Unhealthy-Sync}. This issue distribution analysis has been instrumental in identifying numerous anomalies contributing to training slowdowns in our cluster, as detailed in \S\ref{sec:case:stall-free}.

\section{Deployment \& Case Studies}
\label{sec:case_study}
In this section, we further demonstrate \sysname{}’s effectiveness in diagnosing LLM training anomalies through statistical analysis of its cluster-wide deployment and detailed case studies of various anomalies.

\subsection{Cluster-wide Deployment}

\sysname{} has been deployed within a training cluster with 6,000 GPUs for over 8 months.
During this period, it is responsible for monitoring, detecting, and diagnosing training jobs for various deep learning models, especially large-scale distributed LLM training.
Its diagnostic capabilities enable the algorithm teams, infrastructure team, and operations team to seamlessly enhance model training efficiency and improve training cluster utilization.

% \sysname{} has been deployed within a training cluster with more than \textcolor{red}{6,000} GPUs for over \textcolor{red}{8} months for  monitoring, detecting, diagnosing training jobs of various deep learning models, especially large-scale distributed LLM training.
% It helps our algorithm teams, infrastructure team, and operations team seamlessly to enhancing model training efficiency and improving the training cluster utilization.

\begin{table}
\centering
\footnotesize
\setlength{\tabcolsep}{2pt}
\caption{Typical errors detected by \sysname{}.}
\label{tb:case:errors}
\begin{tabular}{c|c|c|c}
\hline
\textbf{Taxonomy} & \textbf{Details} & \textbf{Numbers} & \textbf{Mechanism} \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}OS\\ errors\end{tabular}} & Checkpoint storage & 10 & \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Stack\\ analysis\end{tabular}} \\ \cline{2-3}
 & OS crash & 1 &  \\ \cline{1-3}
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}GPU\\ errors\end{tabular}} & GPU Driver & 26 &  \\ \cline{2-3}
 & Faulty GPU (Unknown) & 37 &  \\ \cline{2-4} 
 & \multirow{2}{*}{NCCL hang} & \multirow{2}{*}{36} & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Intra-kernel\\ tracing\end{tabular}} \\ \cline{1-1}
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Network\\ errors\end{tabular}} &  &  &  \\ \cline{2-3}
 & RoCE issue & 17 &  \\ \hline
\end{tabular}
\vspace{-4mm}
\end{table}

\autoref{tb:case:errors} presents a subset of error anomalies detected by \sysname{}.
As shown in the table, \sysname{} effectively identifies OS- and hardware-related anomalies, including crashes, hangs, and slowdowns caused by such issues.
While these issues are typically conspicuous and could be detected by existing methods based on noticeable training interruptions~\cite{shoeybiMegatronLMTraining,wuFALCONPinpointing}, \sysname{}’s novelty lies in providing richer error information through techniques like intra-kernel tracing.
Such runtime information helps ease and accelerate the attribution process of low-level issues for operations teams.

Moreover, the true value of \sysname{} lies in its ability to detect anomalies caused by both algorithm teams and the infrastructure team. \autoref{tb:case:slowdowns} summarizes slowdowns diagnosed by \sysname{} using its aggregated metrics.
In the table, anomalies newly identified by \sysname{}, as compared to prior works~\cite{jiangMegaScaleScaling}, are highlighted in bold. The following subsections provide a detailed discussion of typical cases.

% \autoref{tb:case:statistics} presents the partial anomalies detected by \sysname{} during its cluster-wide deployment.
% Firstly, as shown in the table, \sysname{} effectively identifies anomalies commonly attributed to operations teams, such as crashes, hangs, and slowdowns caused by low-level issues, consistent with findings in existing works~\cite{shoeybiMegatronLMTraining,wuFALCONPinpointing}. 
% These OS- and hardware-related issues are typically evident, as they result in noticeable training progress interruptions or speed fluctuations in ongoing training jobs.
% What sets \sysname{} apart is providing richer error information through techniques such as intra-kernel tracing.
% Such runtime information helps ease and accelerate the attribution process for operations teams in tasks like OS error fixes, unhealthy server isolation, network optimization, and training job restarts.

% More importantly, \sysname{}’s true value lies in its ability to detect anomalies attributable to various algorithm teams and the infrastructure team. In the following subsections, we discuss typical cases detected and diagnosed during \sysname{}’s daily deployments, leveraging its proposed metrics.

\begin{table}
\centering
\footnotesize
\setlength{\tabcolsep}{1pt}
\caption{Slowdowns diagnosed by \sysname{}, with ``\textbf{Details}'' showing training job specifics and associated MFU decline.}
\label{tb:case:slowdowns}
\begin{tabular}{c|c|c}
\hline
\textbf{Metric} & \textbf{Attribution} & \textbf{Details} \\ \hline
\multirow{2}{*}{FLOPS} & GPU underclocking & 480 GPUs, Llama-65B, $14\%\downarrow$ \\ \cline{2-3} 
 & \textbf{Backbone migration} & \textbf{1856 GPUs, Llama-80B, $33.3\%\downarrow$} \\ \hline
\multirow{3}{*}{Bandwidth} & \begin{tabular}[c]{@{}c@{}}Network jitter with\\ increased CRC\end{tabular} & 928GPUs, Llama-65B, $10\text{\textasciitilde}20\%\downarrow$ \\ \cline{2-3} 
 & \begin{tabular}[c]{@{}c@{}}Down of\\ GDR module\end{tabular} & \begin{tabular}[c]{@{}c@{}}32GPUs, Llama-10B, $80\%\downarrow$\\ 128GPUs, Llama-10B, $62.5\%\downarrow$\\ ...\end{tabular} \\ \cline{2-3} 
 & \textbf{\begin{tabular}[c]{@{}c@{}}Host-side hugepage\\ caused high sysload\end{tabular}} & \textbf{128GPUs, LlamaVision-11B, $20\%\downarrow$} \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Issue latency\\ distribution\end{tabular}} & Python GC & \begin{tabular}[c]{@{}c@{}}2048GPUs, Llama-80B, $10\%\downarrow$\\ 280GPUs, LlamaVision-11B, $60\%\downarrow$\\ ...\end{tabular} \\ \cline{2-3} 
 & \textbf{\begin{tabular}[c]{@{}c@{}}Unnecessary\\ GPU Sync\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}256GPUs, Llama-20B, $2.66\%\downarrow$\\ ...\end{tabular}} \\ \cline{2-3} 
 & \textbf{Package chcecking} & \textbf{280GPUs, LlamaVision-20B, $30\%\downarrow$} \\ \cline{2-3} 
 & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Frequent GPU\\ mem. management\end{tabular}}} & \multirow{2}{*}{\textbf{1344GPUs, Llama-176B, $19\%\downarrow$}} \\ \cline{1-1}
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Void\\ percentage\end{tabular}} &  &  \\ \cline{2-3} 
 & \textbf{Dataloader} & \textbf{512GPUs, Llama-80B, $41\%\downarrow$} \\ \hline
\end{tabular}
\vspace{-4mm}
\end{table}

\subsection{Case-1: Towards Stall-free Kernel Issuing}
\label{sec:case:stall-free}

% Kernel-issue stalls are among the most frequent causes of slowdowns in training jobs. Among these stalls, Python runtime GC is a well-known case, which now have been carefully managed by the parallel backbone. Additionally, another common case arises from code modifications made by algorithm teams, which are originally intended to enhance LLM performance in downstream tasks. 

Kernel-issue stalls are among the most frequent causes of slowdowns encountered in a training cluster not dedicated exclusively to a single pre-training task.
While Python runtime GC is well-known and now carefully managed by the parallel backbone in most cases, most encountered kernel-issue stalls arise from code introduced by algorithm teams to enhance LLM performance in downstream tasks.

A typical case encountered by \sysname{} is a training job of Llama20B running on 256 H800 GPUs. The developer from the algorithm team mistakenly enables the timer provided by Megatron for performance profiling of several key code segments. This profiling incurs kernel-issue stalls because it requires GPU synchronizations to obtain accurate timestamps.

Although no significant slowdown was observed in training throughput, \sysname{} successfully detects the abnormal issue latency distribution. After removing these unnecessary synchronizations by disabling the timer, the MFU of the training job improves from $41.4\%$ to $42.5\%$, representing a $2.66\%$ increase.
While this slowdown is much smaller than that caused by Python runtime GC (typically exceeding $10\%$), \sysname{} could still uncover it, ensuring the training job’s performance.

% A typical case encountered involved a training job of Llama-20B running on 256 H100 GPUs. The developer from the algorithm team mistakenly enabled the timer provided by Megatron for performance profiling of several key code segments. Performance profiling incurs kernel-issue stalls because it requires GPU synchronizations to obtain accurate timestamps. Although no significant slowdown was observed in training throughput, \sysname{} successfully detected the abnormal issue distribution when monitoring the training progress. After removing these unnecessary synchronizations by disabling the timer, the MFU of the training job improved from $41.4\%$ to $42.5\%$, representing a $2.66\%$ increase. While this slowdown is much smaller than that caused by Python runtime GC (typically exceeding $10\%$), \sysname{} was still able to uncover it through issue distribution analysis.

In addition to the above two cases, \sysname{} also detects other kernel-issue stalls, such as unnecessary package version checking, frequent CUDA memory management within the PyTorch runtime, and others. These cases are listed in \autoref{tb:case:slowdowns}.

% While these issues are often minor, such code changes can still impact training efficiency to varying degrees.

% More kernel-issue stalls are detected by \sysname{} during daily deployments such as unnecessary package version checking, frequent CUDA memory management within PyTorch runtime,etc.
% We list these cases in \autoref{tb:case:kernel-issue}.
% While often minor, these code changes can indeed affect training efficiency to varying degrees.

% \textcolor{blue}{unnecessary explicit synchronization}
% 256GPU megatron Llama20B
% MFU 2.5\%
% 420 -> 410

% \textcolor{red}{check package version}
% 400GPUs 

% \textcolor{red}{cuda memory management}
% 16

\subsection{Case-2: Migration between Backbones}

Different parallel backbones are suited to varying hardware conditions. While FSDP typically delivers ease of use and good efficiency on fewer than 1000 GPUs or relatively short sequences for LLMs (e.g., 4k), Megatron demonstrates superior scalability as the scale increases beyond 1000 GPUs or when handling extremely large sequences (e.g., 64k). In this context, algorithm teams may migrate an LLM between backbones to meet their specific demands. However, this migration process can potentially introduce anomalies.

% Different parallel backbones are suited to varying hardware conditions.
% While FSDP typically delivers ease of use and good efficiency on fewer than 1000 GPUs or relatively short sequences for LLMs (e.g., 4k), Megatron demonstrates superior scalability as the scale increases beyond 1000 GPUs or when handling extremely large sequences (e.g., 64k).
% In this context, algorithm teams may migrate an LLM between backbones to meet their specific demands, a process during which anomalies can arise.



A typical scenario encountered by \sysname{} is an anomalous MFU decline when migrating a Llama-like 80B model from FSDP (1888 H800 GPUs) to Megatron (1586 H800 GPUs with a data-parallel degree of 58, pipeline-parallel degree of 8, and tensor-parallel degree of 4). This anomaly specifically stems from a matrix layout change. The weight dimension of the LLM’s FFN layer, initially configured as $[8192 \times 33936]$ during training on FSDP, changes to $[8192 \times 8484]$ after migration to Megatron with a tensor parallelism degree of 4.

After migration to Megatron, this operator exhibits significantly lower FLOPS due to smaller batch size and the unfavorable 8484 layout for Tensor Cores, which require alignment to 128 bytes. In contrast, the dimension 33936 and larger batch size on FSDP meet this alignment requirement. Following \sysname{}’s diagnosis, our infrastructure team customizes a kernel that pads 8484 to 8512. \autoref{fig:layout} further illustrates the FLOPS of the same operator before migration, after migration, and post-optimization guided by \sysname{}. As shown, the operator experiences a $65.3\%$ decline in FLOPS after migration, and \sysname{} successfully facilitates the performance diagnosis. From the perspective of the training job, the overall MFU increases from $27\%$ to $36\%$, reflecting a $33.3\%$ improvement.

% A typical case encountered and diagnosed by \sysname{} is an anomalous MFU decline caused by poor matrix layout when a Llama-like 80B model is migrated from FSDP (1888 H800 GPUs) to Megatron (1586 H800 GPUs with data parallel degree of 58, pipeline parallel degree of 8, and tensor parallel degree of 4).
% The weight dimension of the LLM’s FFN layer, originally configured as $[8192 \times 33936]$ when trained on FSDP, changed to $[8192 \times 8484]$ after migration to Megatron with a tensor parallelism degree of 4.
% On FSDP, the dimension 33936 is larger, and the input batch size is also substantial.
% After migration to Megatron, this operator exhibits significantly lower FLOPS due to a smaller batch size and the unfavorable 8484 layout for Tensor Cores, which require alignment to 128 bytes.
% \autoref{fig:layout} shows the FLOPS comparison of the same operator before migration, after migration, and following \sysname{}’s guided optimization, as collected by \sysname{} at runtime. The figure reveals a FLOPS decline of $65.3\%$ after migration. To address this, our infrastructure team achieved a $2.75\times$ speedup by providing a dedicated kernel that pads 8484 to 8512.
% With this optimization, the overall MFU of the training job increased from $27\%$ to $36\%$, representing a $33.3\%$ improvement.



\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{figure/13-layout.pdf}
    \vspace{-2mm}
    \caption{The change in computation TFLOPS when migrating Llama-80B from FSDP to Megatron.}
    \label{fig:layout}
    \vspace{-4mm}
\end{figure}
\subsection{Case-3: New Algorithms and Data}
Algorithm teams continually strive to enhance model performance by modifying the LLM architecture and incorporating new training data. However, this process often introduces anomalies.
% Algorithm teams are making efforts to enhancing model performance including new algorithms and training data.

Firstly, algorithm teams generally modify position embeddings (PE), activation functions (ACT), and normalization operators (NORM), while preserving the core structure of the transformer. \autoref{tb:void-minority} illustrates the detected changes in $V_{minority}$ caused by these operator modifications during daily deployments. In this table, the parallel backbone is Megatron. The “Healthy” column represents a fully optimized training job, whereas the “-PE” column reflects changes in modifying the position embeddings. Similarly, the other columns correspond to modifications of the respective operators.

Since these modified operators are less critical and not instrumented by \sysname{}, $V_{minority}$ increases proportionally with their computational complexity.
Our infrastructure team leverages \sysname{}’s detection of high $V_{minority}$ to develop targeted kernel implementations. Once optimized through techniques like kernel fusion, the job’s $V_{minority}$ returns to a normal level. In this process, \sysname{} eliminates the need for manual identification, thereby accelerating anomaly diagnosis.

% Firstly, new algorithms commonly involve modifications to position embeddings (PE), activation functions (ACT), and normalization operators (NORM), while maintaining the main structure of the transformer.
% \autoref{tb:void-minority} highlights the detected changes in $V_{minority}$ caused by the introduction of new versions of these operators during daily deployments.
% The parallel backbone used is Megatron.
% In the table, \texttt{Healthy} represents a fully-optimized training job, while $-$ represents that specific operators are not optimized.
% Since these new operators are composed of PyTorch’s smaller operators and are not instrumented by \sysname{}, $V_{minority}$ increases proportionally with their computational complexity.
% Our infrastructure team relies on \sysname{}’s detection of high $V_{minority}$ to identify these training jobs for introducing targeted kernel implementations, eliminating the need for manual effort.
% Once optimized through techniques such as kernel fusion, the job’s $V_{minority}$ returns to a normal level.

\begin{table}
\footnotesize
\centering
\caption{Changes in detected $V_{minority}$ and corresponding TFLOPS when different minority kernels are not optimized.}
\label{tb:void-minority}
\begin{tabular}{c|c|c|c|c}
\hline
                        & \textbf{Healthy} & \textbf{-PE} & \textbf{-PE-ACT} & \textbf{-PE-ACT-NORM} \\ \hline
$\mathbf{V_{minority}}$ & $9\%$           & $14\%$       & $15\%$           & $28\%$                \\ \hline
\textbf{N. TFLOPS}      & 1               & 0.95         & 0.93             & 0.83                  \\ \hline
\end{tabular}
\vspace{-4mm}
\end{table}

Secondly, newly filtered data is continuously incorporated into the LLM for training. \sysname{} has successfully diagnosed anomalies arising from variance in the training data. In one specific case, the algorithm team attempts to train a Llama-80B model with data containing a sequence length of 64k, while the original training script is designed for sequence lengths of 4k. \sysname{} identifies a significant anomalous decline in MFU ($41\%$) on 512 H800 GPUs, accompanied by an increase in $V_{inter}$.

After routing this anomaly to the infrastructure team, the root cause was identified in the dataloader, specifically in the attention mask generation process. When the sequence length is short, the latency incurred by mask generation is minimal. However, the complexity of mask generation scales as $O(L^2)$, where $L$ represents the sequence length. As a result, the dataloader experiences extremely poor performance when the sequence length increases to 64k.

% Secondly, newly filtered data is continuously fed into the LLM for training.
% \sysname{} has also successfully diagnosed anomalies caused by the variance in training data.
% Recently, a particularly noticeable trend has been the dramatic improvement in the model’s ability to handle sequences with long context lengths.
% In one diagnosed case, the algorithm team attempted to train a Llama-80B model with data featuring a sequence length of 64k, while the original training script was designed for 4k sequence lengths.
% \sysname{} detected a significant anomalous MFU decline ($41\%$)  accompanied by an increase in $V_{inter}$. After routing this anomaly to the infrastructure team, we identified the root cause in the dataloader, which was generating attention masks.
% When the sequence length was short, the elapsed latency of the mask generation was minimal. However, the complexity of mask generation scales as $O(L^2)$, where $L$ represents the sequence length. Consequently, the dataloader exhibited extremely poor performance when the sequence length increased to 64k.


% 64k sft 512GPUs MFU 24 34 80B

% list increasing

\section{Lessons Learned and Future Work}
\label{sec:practical}
In this section, we share the practical lessons learned during the development and deployment of \sysname{}, along with directions for future work.

\subsection{Practical Usages}
\paragraph{Algorithm teams.}
Algorithm teams are dedicated to continuously integrating new innovations into the existing LLM training pipeline. They employ \sysname{} to verify that submitted training jobs meet the expected training throughput. By leveraging \sysname{}, algorithm teams can identify and resolve slowdown anomalies caused by elementary inefficient code, without requiring intervention from the infrastructure team.
\paragraph{Infrastructure team.}
The infrastructure team is committed to optimizing GPU kernel libraries, training frameworks, and parallel backbones. With \sysname{}’s lightweight logging system, the infrastructure team can gather sufficient runtime data to analyze submitted jobs and identify new optimization opportunities. Anomalies caused by codebase modifications that cannot be resolved by the algorithm team are ultimately routed to the infrastructure team.
\paragraph{Operations team.}
The operations team is responsible for maintaining the stability of all low-level resources. When a training job is terminated due to low-level issues, the operations team queries intra-kernel tracing information provided by \sysname{}. This allows them to identify the faulty machine without requiring a fully comprehensive test.

\subsection{Holistic Diagnostics}
The training stack for advancing AIGC is inherently complex. Previously, in the absence of a holistic diagnostic framework like \sysname{}, significant effort was wasted on communication among algorithm, infrastructure, and operations teams. Algorithm teams often required collaboration with the infrastructure team to address slowdowns caused by minor but inefficient codebase modifications. When low-level issues arose, the operations team had to conduct extensive low-level benchmarking tests to identify the problem, resulting in delays to the algorithm teams’ training jobs. The holistic diagnostics provided by \sysname{} streamline this process by pinpointing elementary inefficiencies in the codebase for algorithm teams, only routing complex anomalies with aggregated metrics to the infrastructure team, and supplying the operations team with valuable runtime low-level data.

% Our clouds have built mature monitoring systems with clear labor of division. Existing monitors focus on their target domain, e.g., PingSys [22] targets physical networks and Zoonet [57] targets virtual networks. When applications encounter problems, operators check each system and try to correlate anomalies with network events. With so many systems and metrics, the whole process is inefficient in finding the root cause. A unified diagnostic platform facilitates this process, which integrates multiple scenarios like physical and virtual networks and performs fast anomaly routing to target teams. AND enables minute-level anomaly detection and routing.

\subsection{Using Historical Data}
Historical traces are essential for enhancing the effectiveness of anomaly detection. While runtime data is crucial, it is insufficient on its own for accurately identifying anomalies. \sysname{} detects various issues by comparing real-time data against historical data. For instance, when using issue latency distributions to detect kernel-issue stalls, \sysname{} relies on historical data from specific backbones operating on specific hardware. By collecting historical data from healthy jobs, \sysname{} can efficiently identify anomalies in newly submitted LLM training jobs. In the future, as deployment data grows, we aim to release relevant datasets to further streamline the diagnosis of LLM training.

\subsection{Hardware Extensibility}
Currently, \sysname{} does support the extensibility of additional hardware, particularly NPUs dedicated to DL training. Its tracing mechanism is not only backbone-agnostic but also designed for seamless extension to other NPUs. Since \sysname{} directly instruments key code segments at the Python and C++ runtime levels, extending it is straightforward, provided the relevant computation kernel is supplied by the NPU vendor.

% For instance, \sysname{} successfully detected a NPU underclocking for a training job.

\subsection{Diagnosing Communication Slowdown}
Diagnosing communication slowdowns caused by issues such as network jitter is challenging, as runtime tracing can introduce significant overhead or lack accuracy. Currently, \sysname{} utilizes a method similar to MegaScale~\cite{jiangMegaScaleScaling}, leveraging NCCL tests to identify faulty machines or switches. We are in the process of developing an eBPF-based~\cite{ebpf} tracing tool to accurately monitor bandwidth across RDMA NICs with minimal overhead. In the future, \sysname{} will be further enhanced with these fine-grained tracing capabilities.