\begin{table*}
\centering
\footnotesize
\caption{A comprehensive analysis of common anomalies encountered in \groupname{}, annotated with \sysname{}’s primary target.}
\label{tb:anomaly-analysis}
% \def\arraystretch{1.2}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c|ccccccccccc}
\hline
\diagbox{}{}&
  \multicolumn{11}{c}{\textbf{Anomalies}} \\ \hline
\textbf{Type} &
  \multicolumn{5}{c|}{Error} &
  \multicolumn{6}{c}{Slowdown} \\ \hline
\textbf{Taxonomy} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Algorithm\\ bugs\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Infrastructure\\ bugs\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}OS\\ errors\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}GPU\\ errors\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Network\\ errors\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}New\\
  algorithms\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Unnecessary\\ synchronization\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Un-optimized\\ kernels\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Memory\\ management\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}GPU\\ underlocking\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}Network\\ jitter\end{tabular} \\ \hline
\textbf{Symptom} &
  \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Startup crash or\\  hang error\end{tabular}} &
  \multicolumn{3}{c|}{\begin{tabular}[c]{@{}c@{}}Runtime hang\\  or crash error\end{tabular}} &
  \multicolumn{3}{c|}{\begin{tabular}[c]{@{}c@{}}Slowdown compared to historical\\ jobs and prior training steps\end{tabular}} &
  \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Slowdown compared to\\prior training steps\end{tabular}} \\ \hline
\textbf{Team} &
  \multicolumn{1}{c|}{Algorithm} &
  \multicolumn{1}{c|}{Intrastructure} &
  \multicolumn{3}{c|}{Operations} &
  \multicolumn{2}{c|}{Algorithm} &
  \multicolumn{2}{c|}{Intrastructure} &
  \multicolumn{2}{c}{Operations} \\ \hline
\textbf{Attribution} &
  \multicolumn{2}{c|}{Obvious} &
  \multicolumn{7}{c|}{Obscure}& 
  \multicolumn{2}{c}{Obvious}
  \\ \hline
\textbf{Comment} &
  \multicolumn{2}{c|}{No need to diagnose} &
  \multicolumn{9}{c}{\textbf{\sysname{}'s main target}} \\ \hline
\end{tabular}
\vspace{-4mm}
\end{table*}


\section{Background and Motivation}
In this section, we introduce the training of LLMs within corporations like \groupname{}, and motivate the design of \sysname{} for anomaly diagnostics of LLM training. 

\subsection{Large-Scale LLM Training Stack}
The pursuit of more powerful large language models~\cite{gpt4,deepseekmoe,claude3,qwen2,gemini,mistral,touvronLLaMAOpen,touvronLlama2,dubeyLlama3,llama3-1} has sparked an intense competition between leading corporations.
Referring to the software-hardware stack for training LLMs in \autoref{fig:training-stack}, we delve into how LLMs are reshaping corresponding teams.
% , focusing on the following key aspects.

\paragraph{Advancing of LLM applications.}
With the advancing of LLM algorithms~\cite{deepseekmoe}, LLMs excel not just in natural language understanding, but also in tackling advanced tasks such as multimodal tasks ~\cite{vit,sora}, reasoning tasks~\cite{openaio1,rstar}. For instance, customer service teams fine-tune LLMs to develop chatbots that generate accurate responses to complicated questions. Similarly, product development teams leverage multimodal LLMs to generate comprehensive product descriptions by integrating diverse inputs, such as product images and textual data. Consequently, different algorithm teams continue to innovate LLM models~\cite{touvronLLaMAOpen,touvronLlama2,dubeyLlama3,llama3-1,deepseekmoe,lepikhinGshardScaling,mixtralmoe,geminimoe,palm,fedusSwitchTransformers,shengHybridFlowFlexible,zhongRLHFuseEfficient} tailored for diverse application scenarios.

% \paragraph{Advancing of LLM applications.}
% With the advancing of LLM algorithms~\cite{deepseekmoe}, LLMs excel not just in natural language understanding, but also in tackling more advanced tasks such as multimodal tasks~\cite{vit,sora}, reasoning tasks~\cite{openaio1,rstar}.
% For instance, customer service teams fine-tune LLMs to develop chatbots that can resolve user queries in real-time or draft personalized responses for more complex scenarios. Similarly, product development teams leverage multimodal LLMs to generate comprehensive product descriptions by integrating diverse inputs, such as product images and textual data.
% Consequently, different modeling teams within our corporation design innovative LLM algorithms~\cite{touvronLLaMAOpen,touvronLlama2,dubeyLlama3,llama3-1,deepseekmoe,lepikhinGshardScaling,mixtralmoe,geminimoe,palm,fedusSwitchTransformers,shengHybridFlowFlexible,zhongRLHFuseEfficient} and deliver performant LLMs tailored to diverse application scenarios.

\paragraph{Advancing of training cluster.}
Training these LLMs is computationally intensive, requiring large-scale GPU clusters of thousand-plus scale~\cite{llama3-1}.
Therefore, leading corporations are continuously investing in large training clusters powered by state-of-the-art hardware~\cite{xai_colossus_training_cluster,meta_ai_training_infrastructure}.
These clusters not only expand in scale but also incorporate the latest GPUs~\cite{nvidia_ampere_architecture,nvidia_hopper_architecture} with higher computational performance and advanced interconnect~\cite{nvidia_nvswitch,nvidia_nvl72}.
Efficient operation of these large-scale clusters is critical to the corporations, which requires the operations team to ensure uninterrupted training performance.
Its responsibilities include job scheduling, driver updates, hardware maintenance, etc. 

% \paragraph{Advancing of training cluster.}
% Training these LLMs is computationally intensive, requiring large-scale distributed systems equipped with thousands of GPUs~\cite{llama3-1}.
% We are increasingly investing in large training clusters powered by state-of-the-art hardware~\cite{xai_colossus_training_cluster,meta_ai_training_infrastructure}.
% These clusters not only expand in scale but also incorporate next-generation GPUs~\cite{nvidia_ampere_architecture,nvidia_hopper_architecture} with higher computational performance and advanced interconnect technologies~\cite{nvidia_nvswitch,nvidia_nvl72} to ensure high training efficiency.
% Efficient operation of these large-scale clusters is entrusted to the operations teams, who manage hardware resources at the operating system level.
% Their responsibilities include job scheduling, driver maintenance, and related tasks to ensure uninterrupted training performance.

\paragraph{Advancing of training infrastructure.}
To enable easy, efficient, and scalable LLM training within large-scale clusters, the infrastructure teams build the dedicated software stack. This stack bridges the gap between algorithm teams and operations team. Specifically, the infrastructure team focuses on optimizing the software stack by integrating advanced operator libraries~\cite{cublas2024,cutlass2024,nccl2024}, training framework~\cite{paszkePyTorchImperative}, and state-of-the-art model parallel backbones~\cite{zhaoPyTorchFSDP,shoeybiMegatronLMTraining,liangTorchTitanOnestop,ivchenkoTorchRecPyTorch}.
By leveraging this highly optimized training infrastructure, we unlock the full potential of LLMs, enabling innovation and scalability across our various services. 


% \paragraph{Advancing of training infrastructure.}
% To enable concurrent training of diverse LLMs from various algorithm teams in training cluster, the infrastructure teams build bottom-up software training stack that promote easy, efficient, and scalable LLM training within large-scale clusters.
% To bridge the gap between algorithm and operations teams, the infrastructure team focuses on optimizing the software stack by integrating advanced operator libraries~\cite{cublas2024,cutlass2024,nccl2024}, training framework~\cite{paszkePyTorchImperativea}, and state-of-the-art model parallel backbones~\cite{zhaoPyTorchFSDP,shoeybiMegatronLMTraining,liangTorchTitanOnestop}.
% By leveraging this highly optimized training infrastructure, built on large training clusters, we unlock the full potential of LLMs, enabling innovation and scalability across our various services.

\subsection{Anomalies of Large-scale LLM Training}
\label{sec:anomalies}

Due to the complexity of the entire training stack, various issues can easily occur.
These issues affect both the training speed of individual training jobs and the overall utilization of training clusters, collectively termed as anomalies.
\autoref{tb:anomaly-analysis} presents a distilled analysis of the common anomalies in our real-world cluster, broadly categorized into two primary types: errors and slowdowns.
These anomalies often span responsibilities across multiple teams ~\cite{jiangMegaScaleScaling,dongBoostingLargescale,wuFALCONPinpointing}, making them notoriously difficult to diagnose.
Thus, resolving these anomalies demands a diagnostic framework that can effectively identify their attributions. However, designing such a framework is far from trivial, posing three key challenges.

% However, due to the complexity of the entire training stack, various issues can easily occur, affecting both individual training jobs and the overall utilization of training clusters.
% We term these issues anomalies in large-scale LLM training.
% % which occur across the training stack and are notoriously difficult to diagnose.
% \autoref{tb:anomaly-analysis} provides a distilled analysis of the commonly encountered anomalies in our organization, broadly categorized into two primary types: errors and slowdowns.
% These anomalies are frequently observed in training jobs across the training cluster and often span responsibilities across multiple teams~\cite{jiangMegaScaleScaling,dongBoostingLargescale,wuFALCONPinpointing}.
% Resolving these anomalies demands a diagnostic framework that can effectively identify their attributions and direct them to the appropriate teams.
% However, designing such a framework is far from trivial, posing three key challenges.
\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figure/underclocking.pdf}
    \caption{Slowdown of a 1024-GPU training job with Llama2-70B under various single-GPU underclocking configurations. Results are evaluated using both Megatron~\cite{shoeybiMegatronLMTraining} and FSDP~\cite{zhaoPyTorchFSDP}.}
    \label{fig:frequency}
    \vspace{-4mm}
\end{figure}


\paragraph{The pain of triggering condition.}
Anomalies in large-scale LLM training can arise at any stage and time, as shown in the fourth row of \autoref{tb:anomaly-analysis}. First, low-level OS or hardware issues, often attributed to the operations team, may occur randomly, causing runtime hangs or slowdowns. For example, \autoref{fig:frequency} illustrates a significant MFU decline resulting from single-GPU underclocking. Second, training slowdowns caused by codebase modifications may also occur randomly. Specifically, codebase updates on the LLM algorithm typically cause slowdowns at the start, while modifications related to the training infrastructure can result in mid-training slowdowns. A notable example is the slowdown caused by on-demand garbage collection (GC)~\cite{jiangMegaScaleScaling} of Python runtime, as detailed in \S\ref{sec:diagnose:issue}.
In such cases, a long-term background diagnostic mechanism is required to identify these anomalies. However, given the variety of anomalies across the stack, it is challenging to design a lightweight diagnostic framework capable of efficiently collecting key metrics.


% \paragraph{The pain of triggering condition.}
% Anomalies in large-scale LLM training are triggered at any stage when training LLMs within a cluster, as shown in the fourth row in \autoref{tb:anomaly-analysis}.
% Firstly, low-level OS or hardware issues, often attributed to the operations team, can trigger runtime hang errors.
% Secondly, slowdowns require comparing different training steps within the same training job to identify variations in training efficiency.
% \autoref{fig:frequency} illustrates a training job utilizing 512 GPUs, demonstrating how model FLOPs utilization (MFU) can be affected by the underclocking of a single GPU, a common runtime-specific factor contributing to LLM training anomalies.
% Consequently, the primary challenge in designing a diagnostic framework lies in ensuring it operates as a cluster-wide solution and continuously monitors training jobs.
% Failure to achieve this may lead to missed anomalies caused by changes introduced by various teams.


\paragraph{The pain of obscured attribution.}
The attribution of encountered anomalies is often obscured by similar symptoms. For example, many large-scale training jobs suffer from hang errors, in which the failure of a single training process results in all participating processes becoming unresponsive.
These errors can arise from specific faulty machines due to computation operators, GPU communication operators, or issues within the operating system~\cite{jiangMegaScaleScaling}.
Besides, identifying the root causes of training slowdowns is inherently challenging. This is because the slowdowns cannot be identified using a single metric.
Specifically, previous researches often rely on identifying computation operators with low FLOPS (floating point operations per second) for optimization~\cite{cutlass2024,cublas2024}. However, when matrix multiplication overlaps with communication operators, it is expected to naturally exhibit lower FLOPS.
For example, in a model trained on an A100 GPU, the FLOPS of the same matrix multiplication kernel can decrease from $283.9\text{TFLOPS}$ to $155.5\text{TFLOPS}$.
Consequently, it is challenging for diagnostic frameworks to precisely attribute anomalies and assign them to the appropriate resolution teams. 


% \paragraph{The pain of obscured attribution.}
% The attribution of encountered anomalies is often obscured by similar symptoms.
% Most large-scale training jobs are terminated by the cluster scheduler due to hang errors, as the failure of a single training process results in all participating processes becoming unresponsive.
% These errors can arise from computation operators, GPU communication operators, or even issues within the operating system~\cite{jiangMegaScaleScaling}.
% Moreover, slowdowns are difficult to diagnose through the monitoring of a single metric.
% Researchers often identify computation operators, such as matrix multiplication with low floating point operations per second (FLOPS), as potential opportunities for optimization~\cite{cutlass2024,cublas2024}.
% However, when matrix multiplication overlaps with communication operators, it is expected to naturally exhibit lower FLOPS.
% These symptoms, though observable, are superficial and fail to reveal the underlying causes.
% Consequently, it becomes challenging for diagnostic frameworks to precisely attribute anomalies and assign them to the appropriate resolution teams.

\paragraph{The pain of backbone extensibility.}
With decades of advancements in model parallelism for large-scale training, a variety of divergent backbones have been developed to parallelize LLMs.
Most LLMs are pre-trained using Megatron~\cite{shoeybiMegatronLMTraining}, a specialized parallel backbone designed for transformer-based LLMs. However, researchers continue to investigate alternative backbones~\cite{zhaoPyTorchFSDP,rajbhandariZeROMemory,liangTorchTitanOnestop} for fine-tuning pre-trained LLMs on downstream tasks or for training more complex large models, such as multimodal models.
For instance, fully sharded data parallelism (FSDP)\cite{zhaoPyTorchFSDP} and DeepSpeed\cite{rajbhandariZeROMemory} are widely utilized within our cluster for training multimodal LLMs. These alternatives are often selected to better align with specific training requirements.
However, integrating the diagnostic framework into divergent parallel backbones remains challenging, as their runtime environments and programming interfaces vary significantly.


Existing works have made efforts to build a more robust training stack for large-scale LLM training~\cite{jiangMegaScaleScaling,dongBoostingLargescale}.
They generally operate under several assumptions:
(1) training tasks rely on limited backbones like Megatron,
(2) all teams work in close collaboration to pre-train a single LLM,
or (3) The focus is on specific anomalies, such as communication-related issues.
These assumptions fail to reflect reality and these works cannot solve the problems across the full stack. To this end, we propose \sysname{}, a cluster-wide solution deployed in \groupname{}’s training cluster.
It monitors the real-time progress of LLM training jobs and diagnoses various anomalies precisely.

% These assumptions fail to reflect reality, particularly when addressing critical post-training tasks instead of only pre-trianing, such as adapting pre-trained LLMs for downstream applications, multimodal tasks, and other specialized scenarios.
% This motivates the proposal of \sysname{}, a cluster-wide solution deployed in \groupname{}’s training cluster.
% It monitors the real-time progress of LLM training jobs to diagnose anomalies precisely attributed to various teams.