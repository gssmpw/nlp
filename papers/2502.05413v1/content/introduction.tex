\section{Introduction}

The advent of large language models (LLMs) has revolutionized the deep learning training community, driving substantial advancements in artificial intelligence-generated content (AIGC). Recognizing their transformative potential to enhance user experiences, leading corporations are proactively leveraging LLMs to enhance a wide array of user-oriented services~\cite{antgroup, jiangMegaScaleScaling,dongBoostingLargescale}.
To meet the computational demands of LLM training, they construct large-scale training clusters comprising the latest GPUs interconnected via high-bandwidth links.

\autoref{fig:training-stack} depicts the general training stack of the large-scale training cluster in modern corporations. As shown, the operations team manages low-level resources, and the infrastructure team delivers training optimizations\cite{cublas2024,nccl2024,daoFlashAttentionFast,paszkePyTorchImperative}, with particular emphasis on parallel backbones\cite{zhaoPyTorchFSDP,shoeybiMegatronLMTraining,rajbhandariZeROMemory}.
Supported by these two teams, multiple algorithm teams focus on adapting LLMs for user-facing applications through various training methods~\cite{shengHybridFlowFlexible,hanParameterEfficientFineTuning}. Notably, since LLMs do not solve all problems, the training cluster also supports other deep learning jobs, such as recommendation models and their specific parallel backbone, TorchRec~\cite{ivchenkoTorchRecPyTorch}.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figure/0-training-stack.pdf}
    \caption{The summarized training stack of large-scale training cluster in \groupname{}, highlighting \sysname{}’s position.}
    \label{fig:training-stack}
    \vspace{-4mm}
\end{figure}

Efficient distributed LLM training of large scale requires delicate collaboration among the teams across the stack. However, due to the involvement of thousands of software and hardware components, training anomalies frequently arise from various layers. These anomalies include obvious job failures and non-obvious training slowdowns, which should be resolved by different teams. 

Specifically, the algorithm teams may unintentionally use incorrect training configurations, leading to training slowdowns. Meanwhile, the slowdowns may also come from unoptimized operators, which fall under the responsibility of the infrastructure team. Moreover, a significant number of job failures are caused by hardware errors. However, the current training stack lacks a dedicated diagnostic framework to address these anomalies. In its absence, resolving such issues requires cross-team investigations, resulting in high communication overhead and low resolution efficiency.


In order to design a deployable diagnostic framework in the training cluster, we identify three main challenges based on real-world operational experiences. \textit{\textbf{C-1}: Designing a long-running and lightweight diagnostic framework is challenging.} Addressing non-obvious training slowdowns and low-level communication errors requires long-term and in-depth monitoring. However, exhaustive tracing mechanisms, such as PyTorch’s built-in profiler, impose a high runtime memory overhead, making it unsuitable for meeting long-running requirements. \textit{\textbf{C-2}: Detecting and diagnosing the underlying root cause is challenging.} Errors or slowdowns in LLM training often manifest with similar symptoms, such as process hangs or decreased training speed, which obscure the actual problematic machine or code. \textit{\textbf{C-3}: Providing a backbone-extensible diagnostic mechanism is challenging.} As shown in \autoref{fig:training-stack}, at least three backbones are commonly used across the LLM community, not to mention those supporting other deep learning models. Making diagnostic mechanisms extensible to multiple backbones is a significant challenge.


Faced with these challenges, previous  efforts\cite{wuFALCONPinpointing,jiangMegaScaleScaling,dongBoostingLargescale} targeting distributed LLM training fail to provide comprehensive solutions. This is because these approaches are narrowly designed for specific problems or scenarios, making them inadequate for addressing issues across the software-hardware stack. For example, FALCON~\cite{wuFALCONPinpointing} and C4D~\cite{dongBoostingLargescale} are capable of diagnosing low-level network issues. However, our observations suggest that many slowdowns are caused by upper-level teams, as shown in \autoref{tb:anomaly-analysis}. Likewise, tools such as MegaScale~\cite{jiangMegaScaleScaling} are optimized for pre-training scenarios where teams collaborate closely and use only the Megatron backbone, limiting their adaptability to the advancing training stack.


To this end, we present \sysname{}, a real-time and holistic anomaly diagnostic framework designed for efficient distributed LLM training in GPU clusters of thousand-plus scale.
\sysname{} consists of two components: a per-training-process tracing daemon and a diagnostic engine.
To collect runtime data with low overhead (\textbf{C-1}), the tracing daemon instruments only key code segments rather than blindly tracing all runtime data.
Furthermore, as the code segments are carefully selected and intercepted at the level of both Python and C++ runtime, the instrumentation provides sufficient critical information for diagnosis (\textbf{C-2}) while remaining backbone-agnostic (\textbf{C-3}).
Notably, \sysname{} is also hardware-extensible, which could provide seamless support for other NPUs.

With the real-time data collected by the tracing daemon, the diagnostic engine detects and diagnoses anomalies, including errors and slowdowns(\textbf{C-2}).
For error diagnostics, \sysname{} introduces a novel intra-kernel tracing mechanism, providing fine-grained diagnostics specifically targeting communication-related hang errors.
Compared to a binary search using NCCL tests~\cite{nccl2024,jiangMegaScaleScaling}, it reduces the complexity of faulty machine diagnostics from $O(\log N)$ to $O(1)$.
For slowdown diagnostics, \sysname{} proposes holistic aggregated metrics that encompass not only commonly used macro metrics like training throughput but also novel micro metrics, such as issue latency distribution.
With these metrics, \sysname{} can diagnose non-obvious slowdowns (e.g., $2.66\%$) in real-world workloads.

We extensively evaluated \sysname{} in terms of its runtime overhead. \sysname{} incurs an average latency overhead of only $0.43\%$ across various LLMs and backbones on 1024 H800 GPUs. Meantime, \sysname{} only generates just $1.5\text{MB}$ of tracing logs per GPU in a real-world model trained on 1536 H800 GPUs. In addition, \sysname{} has been deployed in our training cluster in Ant Group\cite{antgroup}, utilizing more than $6,000$ GPUs over the span of 8 months.
\sysname{} has helped to optimize the whole stack of LLM training, in terms of model designing, infrastructure optimization, and cluster operations.
We present detailed case studies of diagnosed anomalies in \S\ref{sec:case_study} and practical insights in \S\ref{sec:practical} derived from \sysname{}’s daily deployment.



\sysname{} has been open-sourced at \url{https://github.com/intelligent-machine-learning/dlrover/tree/master/xpu_timer}.
\sysname{} serves as a core component of DLRover~\cite{dlrover}, an automated distributed DL system deployed within \groupname{} and supported by the LF AI \& Data Foundation~\cite{lfaidata}. 

% \weihao{talk about reduction of communication cost when encountering slowdown.}

Our contributions are as follows.
\begin{itemize}
[leftmargin=*,topsep=0.2em,itemsep=-0.2em]
    \item We highlight the urgent need for a real-time, holistic diagnostic framework capable of identifying LLM training anomalies across the entire training stack.
    
    \item We introduce \sysname{}, a diagnostic framework specifically designed to tackle the critical challenges of long-term monitoring, root cause diagnosis, and backbone extensibility in LLM training diagnostics.

    \item We deploy \sysname{} across more than 6,000 GPUs over an 8-month period, deriving typical case studies and practical insights from its daily operations.
    
\end{itemize}
% \item We propose a novel intra-kernel tracing mechanism that enables fine-grained diagnostics of communication-related hang errors, reducing the complexity of faulty machine diagnostics from $O(\log N)$ to $O(1)$.
    % \item We propose the holistic aggregated metrics, combining standard macro metrics like training throughput with novel micro metrics, such as issue latency distribution, to address and diagnose non-obvious slowdowns.


% \autoref{fig:training-stack} depicts the summarized training stack of the large-scale training cluster in modern corporations. The bottom-up software-hardware stack follows a multi-layer architecture.
% The operations team manages low-level resources, while the infrastructure team delivers training optimizations~\cite{cublas2024,nccl2024,daoFlashAttentionFast,paszkePyTorchImperative}, especially for highly optimized parallel backbones~\cite{zhaoPyTorchFSDP,shoeybiMegatronLMTraining,rajbhandariZeROMemory}. Algorithm teams, dedicated to specific services, focus on adapting LLMs to their applications using various training methods~\cite{shengHybridFlowFlexible,hanParameterEfficientFineTuning}. Notably, since LLMs do not solve all problems, the training cluster also supports other deep learning (DL) workloads, such as recommendation models and their parallel backbone, TorchRec~\cite{ivchenkoTorchRecPyTorch}.



% When supporting training jobs for various algorithm teams, particularly for LLMs, we encounter numerous issues such as errors and slowdowns. We define these as anomalies in LLM training, as they are often not obvious code bugs but arise from many unclear and complex problems.
% We observe that the current training stack lacks a dedicated diagnostic framework to address these emerging anomalies.
% In its absence, resolving such issues requires cross-team investigations due to the stack’s complexity, leading to high communication costs.


% In order to design a deployable diagnostic framework in the training cluster, we identify three main challenges based on our operational experiences.
% \textit{\textbf{C-1}: it is challenging to design a long-running diagnostic framework.}
% Anomalies caused by low-level issues attributed to operations are runtime errors and slowdowns, which require long-term monitoring.
% However, exhaustive tracing mechanisms, such as those provided by PyTorch’s built-in profiler, incur a high runtime memory overhead.
% \textit{\textbf{C-2}: it is challenging to detect and diagnose the underlying root cause.}
% Errors or slowdowns would exhibit similar symptoms, like training process hang, or training speed decrease.
% In this case, the actual problematic machine or code are obscured.
% \textit{\textbf{C-3}: it is challenging to provide backbone-extensible diagnostic mechanism.}
% Parallel backbone is essential in large-scale distributed LLM training.
% As shown in \autoref{fig:training-stack}, currently, at least three backbones are still commonly used across the LLM algorithm community, not to mention the backbones for other DL models.


% Several works~\cite{wuFALCONPinpointing,jiangMegaScaleScaling,dongBoostingLargescale} have been proposed specifically for distributed LLM training.
% Frameworks like FALCON~\cite{wuFALCONPinpointing} and C4D~\cite{dongBoostingLargescale} are specialized for pinpointing issues arising from low-level network problems.
% While the network plays a crucial role in distributed training, our experiences reveal that considerable slowdowns are caused by upper-level teams, as outlined in \autoref{tb:anomaly-analysis}.
% Similarly, works like MegaScale~\cite{jiangMegaScaleScaling} focus on pre-training scenarios where all teams collaborate closely and a single backbone is utilized.
% In such cases, the environment is constrained, making diagnosis more straightforward.


% We first extensively evaluated \sysname{} in terms of its runtime overhead in terms of latency and memory.
% Results show that \sysname{} incurs an average latency overhead of only $0.43\%$ across various LLMs and backbones on 1024 H800 GPUs, while generating just $1.5\text{MB}$ of tracing logs per GPU in a real-world model trained on 1526 H800 GPUs.
% \sysname{} has been deployed in our training cluster, utilizing more than $6,000$ GPUs over the span of 8 months.
% \sysname{} has also helped to optimize the whole stack of LLM training, in terms of cluster operations, infrastructure optimization, and model designing.
% We present detailed case studies of anomalies diagnosed during \sysname{}'s daily deployment in \S\ref{sec:case_study}.