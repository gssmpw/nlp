\subsection{RQ2: Understanding High Data Leakage in SE Benchmarks}


\subsubsection{Which Types of Software Repositories Are Likely Contributing to Data Leakage?}


\revised{
We conducted an investigation into which software repositories in the pre-training data of the LLM contribute most significantly to data leakage in SE benchmarks. To do this, we grouped all identified duplicate pairs, where the SE benchmark samples are the leaked benchmark data, and the pre-training data in these pairs indicate the source repositories in pre-training data. By analyzing the characteristics of these repositories, we can offer actionable recommendations. 
}

\revised{
Table~\ref{tab:pair_in_repos} highlights the software repositories associated with the most duplicate pairs. One key finding is that 92 duplicate pairs originated from the inclusion of the ``PatrickShaw/QuixBugs'' repository. This accounts for the high data leakage ratio observed in the QuixBugs benchmark, as the pre-training data seems to contain a cloned version of the QuixBugs repository. Additionally, we observed a high frequency of algorithm-related keywords, such as ``leetcode,'' ``programming," and ``data-structures," in repository names. For instance, ``leetcode'' appears 8 times in the top 20 repositories. 
}

\revised{
Based on these observations, we offer several actionable recommendations. For LLM developers, it is crucial to scan repository names during the preparation of pre-training data and filter out repositories that include common SE benchmark names, such as ``PatrickShaw/QuixBugs'' listed in Table~\ref{tab:pair_in_repos}. This will help prevent the inadvertent inclusion of benchmarks through cloned repositories, thereby introducing benchmark data into the pre-training data. Additionally, repositories containing common coding website names, such as 'leetcode,' or algorithm-related keywords like "data-structures," should be excluded. Many benchmarks are sourced from these platforms or algorithm-related repositories, which increases the likelihood of data leakage.
For LLM users and researchers evaluating LLMs, we recommend avoiding the direct use of benchmark data primarily sourced from coding websites like LeetCode or from algorithm-related tutorial repositories. This approach will help reduce the risk of data leakage during evaluations.
}




\begin{table}[t]
\centering
\caption{Software repositories used in LLM pre-training that have a high number of identified duplicate pairs after manual labeling.}
\label{tab:pair_in_repos}
\scalebox{0.9}{
\rotatebox{0}{
\begin{tabular}{lr}
\hline
\textbf{Project Name} & \textbf{\begin{tabular}[c]{@{}r@{}}\#Manual Labeled\\ Duplicate Pair\end{tabular}} \\ \hline
cragkhit/elasticsearch & 509 \\
sgholamian/log-aware-clone-detection & 229 \\
PatrickShaw/QuixBugs & 92 \\
devangi2000/Data-Structures-Algorithms-Handbook & 37 \\
RafaelHuang87/Leet-Code-Practice & 20 \\
khushi-411/LeetCode & 17 \\
sugia/leetcode & 16 \\
kppw99/enVAS & 15 \\
naddym/competitive-programming & 14 \\
VinceW0/Leetcode\_Python\_solutions & 13 \\
AvadheshChamola/LeetCode & 13 \\
NikolayVaklinov10/Python\_Challenges & 10 \\
tirthbharatiya/interview\_questions & 10 \\
wingkwong/competitive-programming & 9 \\
abdzitter/Daily-Coding-DS-ALGO-Practice & 9 \\
vedantc6/LCode & 9 \\
Taewan-P/LeetCode\_Repository & 9 \\
apoorvkk/LeetCodeSolutions & 9 \\
jen-sjen/data-structures-basics-leetcode & 9 \\
chaosWsF/Python-Practice   & 9 \\
\hline
\end{tabular}
}
} \vspace{-0.4cm}
\end{table}



\subsubsection{Why Do Certain SE Benchmarks Exhibit High Data Leakage?}


This subsection explores benchmarks with high data leakage rates, aiming to investigate the potential underlying causes. While most benchmarks exhibit minimal data leakage, this research question focuses specifically on the top 10 benchmarks with the highest leakage. By identifying the reasons and characteristics behind their high leakage rates, we aim to equip researchers with insights to assess whether their chosen benchmarks may face similar issues.

It is important to clarify that the issue does not stem from the benchmarks themselves. Many of these benchmarks were developed and published prior to the advent of LLMs. As a result, the high leakage rates are not due to any flaws in the benchmarks, but rather an unintended consequence of the overlap between the historical data used to create the benchmarks and the data included in LLM training.

Table~\ref{tab:causes} provides an overview of the top 10 benchmarks with the highest leakage rates, detailing their data sources, basic descriptions, and the potential causes behind their high leakage. Specifically, QuixBugs exhibits significant leakage due to the inclusion of the ``PatrickShaw/QuixBugs'' repository in the pre-training dataset. BigCloneBench's high leakage is attributed to the overlap between its source data and the ``cragkhit/elasticsearch'' repository included in the pre-training data. This overlap likely results from BigCloneBenchâ€™s reliance on IJaDataset 2.0~\cite{IJaDataset}, which contains the Elasticsearch project. A similar issue affects BugsInPy, where six source projects used in its dataset construction were also identified in LLM's pre-training data.
For other benchmarks, the primary cause of high data leakage lies in their dependence on widely used coding platforms. Benchmarks such as APPS, CodeEditorBench\textit{-switch-java}, CodeEditorBench\textit{-debug-py}, CodeEditorBench\textit{-switch-c}, and CodeEditorBench\textit{-switch-py} are mainly derived from LeetCode. Since data from LeetCode is included in LLM pre-training datasets, these benchmarks inherently exhibit relatively high leakage rates. Finally, SWE-Bench and SWE-Bench\textit{-verified} also experience substantial overlap because they use GitHub issues as their primary data source, many of which are already incorporated into LLM pre-training datasets.


These findings highlight four primary causes of high data leakage: 1) the direct inclusion of benchmark data in the pre-training dataset (e.g., QuixBugs), 2) overlap with the repositories used to create benchmarks (e.g., BigCloneBench and BugsInPy), 3) dependence on coding platforms like LeetCode (e.g., APPS and CodeEditorBench variants), or 4) shared use of data sources such as GitHub issues (e.g., SWE-Bench).



To address these challenges, 
careful selection of benchmarks is essential for LLM users. They should be cautious about using benchmarks derived from popular coding websites or repositories with known overlaps, and follow the approach in this paper to compare their chosen benchmarks against pre-training datasets to assess the risk of benchmark leakage. 
Doing so helps ensure more reliable evaluations while minimizing the impact of data leakage on benchmark results.





\input{Tables/RQ2_table2}


\vspace{0.2cm}
\noindent
\begin{tcolorbox} [boxrule=0.8pt,
                top=0.2pt,
                  bottom=0.2pt]
    \textbf{Answer to RQ2}: 
    \revised{
     We identified four potential causes of high data leakage: 1) the direct inclusion of benchmark data in the pre-training dataset, 2) overlap between the repositories used to create the benchmarks and the pre-training dataset, 3) reliance on coding platforms like LeetCode to construct the benchmarks, and 4) the shared use of data sources, such as GitHub issues, in both the benchmarks and the pre-training dataset. We also offer several actionable recommendations to reduce the risk of benchmark leakage in future studies.}
\end{tcolorbox}
