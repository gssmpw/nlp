

\subsection{Implications}
Our study presents the first large-scale investigation into data leakage in software engineering benchmarks involving LLMs. Based on our findings, we highlight several implications for the SE research community to further explore and address the challenge of data leakage in future research.

\vspace{0.1cm}
\noindent
\textbf{Researchers need to be cautious about the potential data leakage in SE benchmarks.} Many LLMs are developed after the creation of widely used SE benchmarks, increasing the likelihood of data leakage. Such leakage can result in inflated performance metrics for LLMs, leading to unfair comparisons with non-LLM approaches. For example, our study shows that QuixBugs is entirely leaked to StarCoder, making it unsuitable for evaluating LLMs trained on StarCoder or its pre-training data. As a widely used open-source LLM family, StarCoder and its pre-training data serve as the foundation for many other LLMs, such as PanGu-Coder2~\cite{shen2023pangu}, WizardCoder~\cite{luo2023wizardcoder}, OctoPack~\cite{muennighoff2023octopack}, CodeShell~\cite{xie2024codeshell}, and DeepSeek-Coder~\cite{deepseekcoder}. Our findings on data leakage are applicable to these related LLMs as well.
While many researchers have been aware of this issue, the lack of dedicated studies and tools has often left them unable to verify whether their data was leaked. Our work introduces an effective framework, \textbf{DetectLeak}, to help mitigate the risk of data leakage in benchmark datasets. We believe that future researchers can leverage our framework to enhance the reliability of their LLM evaluations, particularly in addressing data leakage concerns. 
If the used LLMs do not disclose their pre-training data (like ChatGPT), researchers can instead perform leakage detection using the largest open-source LLM pre-training corpus available. While this approach cannot guarantee complete elimination of leakage, it can still significantly reduce the risk to the best extent possible.


\vspace{0.1cm}
\noindent
\textbf{Researchers are encouraged to use our cleaned version of SE benchmarks, LessLeak-Bench, in future studies.}
In this work, we identified many leaked benchmark samples that are no longer suitable for LLM evaluation. By removing these leaked samples, we created a cleaned version of the SE benchmarks, named \textbf{LessLeak-Bench}. For researchers currently using or planning to use the benchmarks analyzed in our study, we recommend adopting LessLeak-Bench to ensure more reliable and trustworthy evaluation results.





\vspace{0.1cm}
\noindent
\textbf{Developing more advanced methods to effectively detect leaked benchmark samples without access to LLM pre-training data remains an essential need.} In our study, we explored the use of Perplexity scores to detect leaked samples (RQ4). While this approach showed potential, there remains significant room for improvement. To support progress in this area, we introduce the \textbf{AutoDetectLeak-Bench} dataset, which contains manually verified labels for both leaked and non-leaked samples. This benchmark serves as a foundation for future research aimed at developing more effective leakage detection methods, particularly in scenarios where LLM pre-training data is unavailable.
As the first large-scale study on data leakage in SE benchmarks, our focus was on LLMs with publicly available pre-training data. This choice was necessary due to the current lack of highly effective automated tools capable of accurately identifying data leakage without knowledge of pre-training data in the SE community. To ensure the reliability of our findings, we selected LLMs that openly shared their pre-training data. Using this information, we constructed AutoDetectLeak-Bench.
Before this work, the absence of a reliable, manually verified dataset made it challenging to assess the effectiveness of automated leakage detection tools. By providing AutoDetectLeak-Bench, we address this gap and offer a valuable resource for advancing research in this area. This dataset can support the development of accurate leakage detection methods, facilitating the identification and removal of leaked data even for models like Llama3 that do not disclose their pretraining data. Such advancements will help ensure more robust and fair evaluations in the future.





\subsection{Actionable Suggestions}
% \vspace{0.1cm}
% \noindent
\subsubsection{\textbf{Suggestions for LLM Providers}}
LLM providers like OpenAI and Meta have developed exceptional and powerful models. To address potential data leakage issues, a key measure is for providers to carefully curate their pre-training data, ensuring it does not contain duplicates or overlaps with SE benchmark samples. 

While some LLM providers such as Huggingface~\cite{starcoder_one} have performed de-duplication for well-known SE benchmarks such as HumanEval and MBPP, many other SE benchmarks have been overlooked in this process.
We recommend that LLM providers adopt the following practices:
\begin{itemize}[leftmargin=*]
    \item Remove pre-training data samples that overlap with a wider range of SE benchmark datasets, rather than limiting removal to popular code generation benchmarks like HumanEval and MBPP. For example, LLM providers could consider the studied benchmarks in this work. 
    \item Carefully inspect GitHub repositories used as part of the pre-training data. If a repository's name or README file explicitly indicates that it was cloned from a benchmark or matches the name of a known benchmark, providers should avoid including such repositories in their pre-training data. This precaution helps prevent severe data leakage, especially when entire cloned versions of benchmark data are used in pre-training.
    \item Be cautious about repositories related to coding platforms like LeetCode. Additionally, repositories containing common coding website names, such as ``leetcode,'' or algorithm-related keywords like ``data-structures,'' should be excluded. Many benchmarks are sourced from these platforms or algorithm-related repositories, which increases the likelihood of data leakage.
    \item Consider leveraging our approach to identify and remove duplicates. Utilize our methodology to detect and exclude pre-training data samples that overlap with the expanded set of SE benchmarks, ensuring cleaner and more reliable pre-training datasets.
    \item Apply leakage detection to instruction-tuning datasets as well. Instruction-tuning datasets, often generated by stronger LLMs to enhance other models, may also introduce data leakage. These datasets should undergo thorough checks to identify and remove overlapping samples with SE benchmarks to prevent inadvertent leakage.
\end{itemize}


 
\subsubsection{\textbf{Suggestions for LLM Users}}
Many LLMs have already been developed and may not have addressed data leakage from various SE benchmarks. Re-training these models to fix data leakage issues can be both costly and impractical. A more cost-effective approach is for LLM users to remove the leaked benchmark samples, thus reducing the risk of data leakage without the need for re-training the models.
Based on our findings in this study, we offer the following suggestions for LLM users:

\begin{itemize}[leftmargin=*]
    \item \textbf{Existing Benchmark Usage:} 
    For benchmarks that are included in LessLeak-Bench, it is recommended to prioritize using the cleaned versions to reduce the risk of data leakage. For benchmarks that are not included, researchers can follow our DetectLeak approach to identify and remove any potential leaked samples.
    
    
    \item \textbf{New Benchmark Construction:} 
    We recommend being cautious about using coding websites like LeetCode or algorithm-focused tutorial repositories to construct new benchmarks. These sources are commonly included in LLM pre-training datasets, which increases the risk of data leakage.   
    One potential solution is to incorporate new coding tasks introduced after the release of the LLMs on coding websites. For example, Liu et al.~\cite{DBLP:journals/tosem/LiuLWTLLL24} found that ChatGPT’s performance drops by up to five times on LeetCode coding tasks introduced after January 2022, likely because these new tasks are less susceptible to leakage in LLM training data.
 

    
    \item \textbf{Community Effort:}
    With the vast number of SE benchmarks available for studying various SE tasks, it is impractical for a single research group to examine data leakage across all of them. This paper provides an analysis of the data leakage status for a relatively large set of benchmarks. We encourage the SE community to build upon our work, such as by using our framework to identify and eliminate leaked samples in more SE benchmarks. Furthermore, we hope this initiative will raise greater awareness of data leakage issues and motivate the development of more sophisticated detection tools, regardless of whether access to LLMs’ pre-training data is available.
\end{itemize}









\subsection{Threats to Validity}
Our findings are limited to the specific SE benchmarks and LLMs studied, and therefore, may not generalize to all SE benchmarks and LLMs. To mitigate this limitation, we selected 83 diverse SE benchmarks to cover a wide range of popular software engineering tasks, including code generation, program repair, vulnerability detection, and more. For the LLMs, we chose StarCoder~\cite{starcoder_one}, a representative model that is fully open-sourced, including its model parameters and pre-training data.
We consider StarCoder a representative choice for this study because it demonstrates competitive performance among fully open-source models and serves as the foundation for derivative LLMs such as WizardCoder~\cite{luo2023wizardcoder}, OctoPack~\cite{muennighoff2023octopack}, CodeShell~\cite{xie2024codeshell}, and DeepSeek-Coder~\cite{deepseekcoder}.
Due to the immense size of LLM pre-training data, manual verification of data leakage is impractical. Instead, we adopted an efficient automated technique to identify potentially leaked data, followed by manual labeling of the flagged samples to confirm leaked SE benchmarks. However, we acknowledge that this approach may not detect all leaked SE benchmark samples, as the automated technique could miss some cases. Despite this limitation, our study identified a substantial number of leaked samples, which can help researchers avoid using these confirmed leaked samples for evaluation. Additionally, our findings provide an overview of the current state of benchmark leakage within the SE community.
To further support the community, we have offered actionable recommendations for both LLM providers and users and shared the implications of our work.

