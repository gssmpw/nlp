In this section, we discuss the selection process for the studied LLM and SE benchmarks. Additionally, we outline the implementation details and define the research questions in this study.

\vspace{-0.2cm}
\subsection{LLM Selection} 
\label{llm_selection}

 



 \textbf{Selection Criteria.} Selecting an appropriate target LLM was a crucial step in this study due to the extensive manual annotation involved, which makes repeated experiments across multiple models infeasible. To guide our selection, we established the following selection criteria:
 

\noindent
\begin{enumerate}[left=0pt]
    \item \textit{Full Open-Source Availability.} The selected LLM must be fully open-source, encompassing both model parameters and pre-training data. However, since re-training the model is outside the scope of our study, open-source pre-training algorithms and scripts are not required.
    \item \textit{High Effectiveness.} The model should demonstrate strong effectiveness on widely used SE benchmarks, such as HumanEval~\cite{chen2021evaluating} and MBPP~\cite{MBPP_1}.
    \item \textit{Influence and Adoption.} 
We prioritized models with significant influence, particularly those that have inspired or laid the groundwork for the development of newer/better LLMs.
\end{enumerate}

\input{Tables/llm_selection}

\vspace{0.1cm}
\noindent
 \textbf{Target LLM Selection Process.}
 \revised{
Table~\ref{tab:model_selection} presents our analysis of various code-related LLMs in chronological order, till April 2024, when this study was initiated. Models highlighted in light green meet our full open-source criteria, while those marked in light red represent the most suitable candidates based on our selection criteria. After thorough analysis, we selected \textbf{StarCoder}~\cite{starcoder_one} as the research LLM for this study.}


In accordance with our first criterion, we excluded proprietary models such as GPT-3.5~\cite{gpt-3.5-turbo} and GPT-4~\cite{gpt4}, along with recent open-source models like LLaMA3~\cite{llama3}, CodeQwen~\cite{codeqwen}, and DeepSeek-Coder~\cite{deepseekcoder}, which do not fully comply with open-source data requirements. Among the fully open-source options, StarCoder~\cite{starcoder_one} and StarCoder2~\cite{starcoder2} ranked highest on two widely used code generation benchmarks, HumanEval~\cite{chen2021evaluating} and MBPP~\cite{MBPP_1}, thereby fulfilling our second criterion.



When comparing StarCoder~\cite{starcoder_one} with StarCoder2~\cite{starcoder2}, we found that StarCoder’s earlier release positioned it as a foundational model in the field, serving as the basis for several other strong/newer LLMs after fine-tuning, such as PanGu-Coder2~\cite{shen2023pangu}, WizardCoder~\cite{luo2023wizardcoder}, and OctoPack~\cite{muennighoff2023octopack}. This suggests that findings from StarCoder extend to these derivative models. Specifically, if SE benchmark data has leaked to StarCoder, it also affects its descendant LLMs as well, thereby enhancing the broader applicability of our findings and conclusions. Consequently, we opted to select StarCoder over StarCoder2.

Lastly, while some other LLMs do not directly build upon StarCoder, they utilize the pre-training dataset of StarCoder or adopt data curation techniques inspired by StarCoder’s pre-training practices. For instance, CodeShell~\cite{xie2024codeshell} incorporates StarCoder’s pre-training data, while DeepSeek-Coder~\cite{deepseekcoder} employs data-filtering methods akin to those used by StarCoder to gather more recent GitHub data, extending to February 2023. This suggests that findings from StarCoder can be generalized to CodeShell and are likely applicable to DeepSeek-Coder as well, as DeepSeek-Coder follows a similar methodology to extend its datasets. 
In summary, we believe StarCoder is a suitable LLM for us to better understand the SE benchmark leakage status.


\vspace{0.2cm}
\noindent
\textbf{StarCoder's Pre-training Data.} 
StarCoder’s pre-training data is sourced from \textit{The Stack~\cite{stack}} dataset. \textit{The Stack~\cite{stack}} dataset comprises over 6TB of permissively licensed code spanning 358 programming languages. The Stack is collected from public GitHub repositories between 2015 and 2022. For pre-training, StarCoder focused on the 86 programming languages that either contained more than 500MB of data or ranked in the top 50 on popularity indices such as \textit{Githut 2.0}\footnote{\url{https://githut.info/}} or the \textit{December 2022 TIOBE Index}\footnote{\url{https://web.archive.org/web/20221229040526/https://www.tiobe.com/tiobe-index/}}. GitHub repositories for these popular programming languages are recognized as valuable data sources within the SE community, ensuring the representativeness of StarCoder’s pre-training data.



Specifically, StarCoder was pre-trained on a dataset of 305M files, totaling 800+ GB.
In this study, we focus on SE benchmarks in three popular programming languages: Python, Java, and C/C++. Therefore, we restrict our analysis to the corresponding subsets within StarCoder's pre-training data. 
These subsets include 12M files for Python, 20M files for Java, and 14M files for C/C++. 
Despite narrowing our scope to these languages, the data volume remains substantial, supporting our decision to use an efficient automated tool to identify potential duplicate pairs before proceeding with manual labeling. 


\vspace{-0.2cm}
 \subsection{Benchmark Data Selection}
 \label{benchmark_selection}


The whole SE community has developed numerous high-quality benchmarks. However, the extensive manual annotation required for each makes it impractical to cover all valuable SE benchmarks. To investigate the leakage status of SE benchmarks in relation to LLMs, we focus on selecting benchmarks that have been previously used for LLM evaluation. Our selection is further constrained to benchmarks within three widely used PLs: Python, Java, and C/C++.



\vspace{0.1cm}
\noindent
\textbf{Studied SE Benchmark Selection.} 
To identify relevant benchmarks, we leverage a recent and comprehensive survey on LLMs for SE tasks~\cite{hou2024large}, which analyzed 395 research papers from January 2017 to January 2024. 
From the papers referenced in this survey, we selected benchmarks and datasets used for evaluating LLMs. We further expanded our selection by examining the citations and related works of these papers, excluding those lacking replication packages or inaccessible datasets.
In building our benchmark collection, we prioritized including datasets across a variety of SE tasks, rather than concentrating on any one task with extensive datasets. 



To ensure clarity in our analysis, benchmarks with multiple variants—such as those involving different programming languages or scenarios—were assigned distinguishing tags appended to their original names.
For example, the CodeEditorBench~\cite{CodeEditorBench} benchmark includes variants in three programming languages: Python, Java, and C++. Additionally, it features four scenarios: Code Debug, Code Translate, Code Polish, and Code Requirement Switch. Since each variant contains distinct benchmark data samples, we assign a unique name to each variant.
The naming convention combines the following components:
(1) benchmark original name, (2) scenario name, and (3) programming language name. Name tags for the scenario and programming language are included only if there are multiple variations across different scenarios or programming languages.
For example, the CodeEditorBench~\cite{CodeEditorBench} variant containing Python data samples for the Code Debug scenario is renamed as ``CodeEditorBench-\textit{debug-py}'', following the naming convention. Here, ``\textit{debug}'' indicates the Code Debug scenario, and ``\textit{py}'' represents the Python data.




Through this process, we ultimately compiled a set of 83 SE datasets covering a broad range of SE tasks, including code generation, program repair, code editing, code translation, code review, debugging, code execution, test output prediction, secure code generation, GitHub issue fixing, clone detection, log generation, vulnerability repair, and vulnerability detection.
A brief overview of the benchmarks we studied is provided in Tables~\ref{tab:main_results_py}, ~\ref{tab:main_results_java}, and ~\ref{tab:main_results_c}.








\vspace{-0.2cm}
\subsection{Implementation Details}
We retrieve StarCoder's pre-trained data from its official HuggingFace homepage.\footnote{\url{https://huggingface.co/bigcode/starcoder}} We execute computations on an NVIDIA GeForce A5000 GPU with 24 GB of memory. We acquire the data for each SE benchmark dataset directly from its replication packages or official websites. For the Minhash+LSH method, we adopt the BigCode Team’s implementation~\cite{bigcode}, which leverages the \textit{\textsf{DataSketches}}~\cite{datasketch} library.



\vspace{-0.2cm}
\subsection{Research Questions}
Our work aims to mainly answer four Research Questions (RQs). 
\begin{itemize}[leftmargin=*]
\item \textbf{RQ1: To what extent does data leakage exist in the studied SE benchmarks?} 
In RQ1, we evaluate an extensive set of 83 SE benchmarks to investigate potential data leakage into an advanced LLM StarCoder~\cite{starcoder_one}.
\item \textbf{RQ2: What factors contribute to high leakage rates in SE benchmarks?}
In RQ2, we analyze the top benchmarks with high leakage rates, discussing the potential reasons behind their high leakage ratios. 
\item \textbf{RQ3: How does the leakage of benchmark data affect the effectiveness of LLMs?} 
In RQ3, we measure the effectiveness differences between leaked and non-leaked portions of the data to explore the impact caused by data leakage.
\item \textbf{RQ4: How effective is the automated metric in detecting data leakage when lacking access to LLM pre-training data?} 
Since pre-training data for many LLMs is inaccessible, this RQ investigates whether SE benchmark leakage can be inferred solely from LLM behaviors (i.e., without access to pre-training data), such as the Perplexity scores of LLMs. 
\end{itemize}



