
Yang et al.~\cite{DBLP:conf/icse/0003ZWS0H024} investigates memorization in Code LLMs, focusing on CodeParrot and CodeParrot-small. The authors use code clone detection tools to identify duplicates in model-generated outputs and analyze patterns of memorization. Our work differs in three key ways. First, we focus on the StarCoder series, a larger and more advanced family of Code LLMs. Second, while Yang et al.~\cite{DBLP:conf/icse/0003ZWS0H024} examines general memorization capabilities using pre-training data not specifically tied to SE benchmarks, our study directly addresses data leakage in SE benchmarks, offering insights into the risks and biases such leakage introduces. Third, unlike their reliance on clone detection tools, which can yield false positives, our approach incorporates large-scale manual labeling to ensure the accuracy and reliability of identified duplicates and leaked samples.

Lopez et al.~\cite{l√≥pez2024interdatasetcodeduplicationdata} investigated dataset leakage in lightweight LLMs, such as CodeBERT, by leveraging an automatic clone detection tool to identify duplicates between pre-training data and three SE benchmarks.
Compared to Lopez et al., our study investigates larger and more advanced LLMs, such as StarCoder, which are approximately 125 times larger than the models analyzed by Lopez et al. and significantly more capable in generation tasks like code generation and program repair. Additionally, rather than focusing on only three SE benchmarks, our work evaluates a much broader set of 83 SE benchmarks across three popular programming languages, providing a more comprehensive analysis of benchmark leakage. To ensure the accuracy of identified leakage, our study also integrates large-scale manual labeling.


In addition to academic literature, several studies in the gray (non-peer-reviewed) literature have explored related topics.
Matton et al.~\cite{matton2024leakagecodegenerationevaluation} analyzed potential data leakage in the HumanEval and MBPP benchmarks by checking for occurrences of their prompts in public GitHub repositories. Similarly, Riddell et al.~\cite{riddell2024quantifyingcontaminationevaluatingcode} studied leakage in these benchmarks using a multi-step approach. They calculated Levenshtein similarity scores to identify overlaps between the benchmarks and LLM pre-training data, selected the top 500 pre-training samples with the highest similarity for each test case, and used an automatic code plagiarism detection tool to identify duplicates. 
In contrast to these gray literature studies, which focus on only two or three SE benchmarks, our work evaluates a significantly broader set of 83 SE benchmarks across three popular programming languages, enabling a more comprehensive analysis of benchmark leakage. Furthermore, our approach incorporates large-scale manual labeling to enhance the accuracy and reliability of leakage detection. We also provide actionable recommendations for LLM developers and users to effectively mitigate data leakage.
