\subsection{RQ3: Impact of Data Leakage for LLM Evaluation}



\noindent
\textbf{Motivation.}
This research question investigates whether data leakage leads to unfair evaluation of LLMs, specifically whether LLMs perform better on leaked data compared to non-leaked data. To address this, we conduct experiments using the APPS~\cite{hendrycks2021measuring} dataset, a widely used benchmark for code generation. We chose code generation as the focus of our analysis because it is a critical SE task where LLMs are frequently applied. We specifically selected the APPS dataset because it includes comprehensive test cases designed to evaluate the functional correctness of generated code. Moreover, the APPS dataset contains a sufficient number of leaked samples, as identified in our study, while many other code generation benchmarks lack enough leaked samples for a similar analysis.


\begin{table*}[b]
\centering
\vspace{-0.2cm}
\caption{Effectiveness of LLMs on Leaked and Non-Leaked Subsets of APPS}
\vspace{-0.2cm}
\label{table:rq3}
\scalebox{0.95}{
\rotatebox{0}{
\begin{tabular}{l|cll|cll}
% \hline
\toprule
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}APPS\\ (zero-shot)\end{tabular}}} & \multicolumn{3}{c|}{\textbf{Leaked test samples}} & \multicolumn{3}{c}{\textbf{Non-leaked test samples}} \\ \cline{2-7} 
 & \multicolumn{1}{c|}{\textbf{pass@1}} & \multicolumn{1}{c|}{\textbf{pass@2}} & \multicolumn{1}{c|}{\textbf{pass@3}} & \multicolumn{1}{c|}{\textbf{pass@1}} & \multicolumn{1}{c|}{\textbf{pass@2}} & \multicolumn{1}{c}{\textbf{pass@3}} \\ 
 % \hline
 \toprule
\textbf{StarCoder-7b} & \multicolumn{1}{c|}{4.4\%} & \multicolumn{1}{c|}{8.9\%} & \multicolumn{1}{c|}{13.3\%} & \multicolumn{1}{c|}{0.9\%} & \multicolumn{1}{c|}{1.6\%} & \multicolumn{1}{c}{2.2\%} \\ \hline
\textbf{StarCoder-3b} & \multicolumn{1}{c|}{3.7\%} & \multicolumn{1}{c|}{7.4\%} &   \multicolumn{1}{c|}{11.1\%}  & \multicolumn{1}{c|}{0.4\%} & \multicolumn{1}{c|}{0.7\%} & \multicolumn{1}{c}{0.9\%}  \\ \hline
\textbf{StarCoder-1b} & \multicolumn{1}{c|}{3.7\%} & \multicolumn{1}{c|}{7.4\%} &   \multicolumn{1}{c|}{11.1\%} & \multicolumn{1}{c|}{0.3\%} & \multicolumn{1}{c|}{0.6\%} &  \multicolumn{1}{c}{0.9\%} \\ 
% \hline
\bottomrule
\end{tabular}
}}
\end{table*}


\vspace{0.2cm}
\noindent
\textbf{Method and Setup.}
The APPS dataset consists of 10,000 code generation problems/data samples, each paired with Python solutions. These problems are categorized into three difficulty levels: introductory, interview, and competition, with solutions ranging from simple one-liners to complex algorithms. On average, each problem includes 21.2 test cases designed to assess the functional correctness of the generated code. The dataset is split into 5,000 samples for training and 5,000 samples for testing.

For our experiments, we first divide the APPS test set into two subsets: leaked and non-leaked. We then generate predictions for both subsets using LLMs of varying sizes: StarCoder-7b, StarCoder-3b, and StarCoder-1b. The models are prompted with a simple instruction format: \textit{``$\#\#\#$ Instruction: [instruction input from APPS data sample] $\#\#\#$ Response:''}, and the model generates a prediction accordingly. The models are prompted in a zero-shot manner, i.e., no labeled data is provided in the prompt. This approach allows us to capture the model's behavior without any influence from labeled data. In other words, the performance in a zero-shot setting more accurately reflects the model's direct outputs based on its pre-training.
For evaluation, we use the Pass@k metric, which is used in the original APPS paper. Pass@k measures the percentage of problems for which at least one of the top-k generated code samples can pass all test cases. This metric reflects the model's ability to produce fully correct solutions within k attempts.

\vspace{0.2cm}
\noindent
\textbf{Results.}
As shown in Table~\ref{table:rq3}, StarCoder-7b, StarCoder-3b, and StarCoder-1b consistently demonstrate superior performance on the leaked samples compared to non-leaked samples. Specifically, StarCoder-7b achieves Pass@1, Pass@2, and Pass@3 scores that are 4.9, 5.6, and 6.0 times higher, respectively, on the leaked samples compared to the corresponding metrics for the non-leaked samples. This indicates that data leakage results in an unfair evaluation of LLMs, with the models performing significantly better on leaked data than on non-leaked data.
In addition, the reason LLMs do not achieve perfect performance on the leaked samples could be attributed to the complexity of the benchmark data, which makes it difficult for the LLMs to fully memorize those data samples. 



\vspace{0.2cm}
\noindent
\begin{tcolorbox} [boxrule=0.8pt,
                top=0.2pt,
                  bottom=0.2pt]
    \textbf{Answer to RQ3}: 
    \revised{
    The experimental results show that data leakage has a significant impact on the evaluation of LLMs, with models performing better on leaked data compared to non-leaked data. For example, StarCoder-7b achieves a Pass@1 score that is 4.9 times higher on the leaked samples than on the non-leaked samples. This underscores how the presence of leaked benchmark samples can lead to significantly inflated metrics, emphasizing the critical need to address the data leakage issue in SE benchmarks to ensure fair evaluation.}
\end{tcolorbox}

