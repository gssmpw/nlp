Recently, Large Language Models (LLMs) have been extensively applied to software engineering (SE) tasks~\cite{10.1145/3695988}, such as code generation~\cite{DBLP:conf/icse/YuSRZZMLLWX24} and automated program repair~\cite{DBLP:conf/icse/XiaWZ23}, leading to notable advancements in the SE domain. The remarkable capabilities of LLMs arise from the extensive knowledge within their large pre-training data, which encompasses various data types, including code, texts, or other modalities.


Despite their effectiveness, many advanced LLM providers—both open-source and commercial—often do not disclose their pre-training datasets~\cite{llama3, gpt-3.5-turbo}. This lack of transparency raises a critical concern in evaluating LLM-related approaches: the risk of data leakage. Data leakage occurs when LLMs are exposed to SE benchmark datasets during their pre-training phase, compromising the evaluation's validity.
The impact of data leakage is twofold. First, it complicates the assessment of whether the notable performance of LLM-based approaches arises from true innovation or inflated effectiveness metrics due to prior exposure to SE benchmark data. Second, it leads to an unfair comparison between LLM-based methods and non-learning-based techniques, such as traditional program analysis approaches, which do not rely on training data and have no opportunity to learn from leaked data.
The data leakage issue in LLMs is becoming increasingly recognized and significant due to LLMs' growing usage~\cite{lópez2024interdatasetcodeduplicationdata}. However, there is a lack of comprehensive studies investigating whether, and to what extent, SE benchmarks have been leaked into the pre-training data of LLMs.



To address this gap, we conduct the first large-scale investigation into data leakage of SE benchmarks. Our study investigates \textbf{83} diverse SE benchmarks\footnote{If a benchmark has multiple variants, we treat each variant as a separate dataset or benchmark.} across three widely used programming languages: Java, C/C++, and Python. Our study covers various SE tasks, such as code generation, code editing, program repair, fault localization, API recommendation, code translation, clone detection, test generation, vulnerability detection, vulnerability repair, debugging, log statement generation, and automatic code review. 
By conducting an extensive analysis of these benchmarks, we aim to understand the comprehensive landscape of data leakage issues within the SE domain. 
Regarding the studied LLMs, we focus on data leakage within the \textbf{StarCoder}~\cite{starcoder_one} family. We choose StarCoder for three key reasons: (1) we require fully open-source LLMs—with publicly available pre-training data—to identify concrete evidence of data leakage; (2) StarCoder demonstrates competitive performance among fully open-source models; and (3) it serves as the foundation for derivative LLMs such as WizardCoder~\cite{luo2023wizardcoder}, OctoPack~\cite{muennighoff2023octopack}, CodeShell~\cite{xie2024codeshell}, and DeepSeek-Coder~\cite{deepseekcoder}.




\begin{figure*}[t] 
    \centering
    \includegraphics[width=14cm]{Figures/lessleak_framework.pdf} 
    \vspace{-0.5cm}
    \caption{Overview of Our Study} 
    \vspace{-0.3cm}
    \label{fig:framework}
\end{figure*}

Studying data leakage in SE benchmarks requires detecting duplicate data between the SE benchmark datasets and the pre-training data of LLMs. To rigorously examine data leakage in SE benchmarks, we proposed a multi-phase approach named \textbf{DetectLeak}, which combines automated techniques with manual labeling to identify overlaps between LLM pre-training data and SE benchmark datasets.
Initially, DetectLeak employs an automated near-duplication detection tool namely MinHash+LSH, to identify potential duplicate pairs. This involves comparing approximately 1.7 trillion pairs of LLM pre-training and SE benchmark data.
Next, several experienced developers collaboratively label the potential duplicate pairs flagged by the automated tool, to accurately identify true duplicates and thus confirm the existence of data leakage of SE benchmark data. 



By applying DetectLeak, we identified and confirmed 606, 816, and 108 leaked data samples in Python, Java, and C/C++ SE benchmarks, respectively.
In general, data leakage in SE benchmarks is minimal, with average leakage ratios of only 4.8\%, 2.8\%, and 0.7\% for Python, Java, and C/C++ benchmarks, respectively. However, some benchmarks exhibit relatively high leakage ratios, which raises concerns about evaluation bias. For instance, QuixBugs~\cite{QuixBugs} has the highest leakage rate at 100.0\%, followed by BigCloneBench~\cite{BigCloneBench} with a significant leakage rate of 55.7\%. Other notable benchmarks, such as APPS~\cite{hendrycks2021measuring}, BugsInPy~\cite{BugsInPy}, SWE-Bench-verified~\cite{SWE-bench-verified}, and CodeEditorBench~\cite{CodeEditorBench}, also show leakage rates ranging from 8–10\%.

To address the identified data leakage, we introduce \textbf{LessLeak-Bench}, a new benchmark that provides cleaned versions of 83 studied SE benchmarks with all identified leaked samples removed. We believe LessLeak-Bench will enable more reliable evaluations of LLMs in future research.



In this work, we structure our study by answering the following research
questions, with an overview illustrated in Figure~\ref{fig:framework}:
\begin{itemize}[leftmargin=*]
\item[-]  \textbf{RQ1. To what extent does data leakage exist in the studied SE benchmarks?}
\item[-]  \textbf{RQ2. What factors contribute to high leakage rates in SE benchmarks?} 
\item[-]  \textbf{RQ3. How does the leakage of benchmark data affect the effectiveness of LLMs?}
\item[-]  \textbf{RQ4. How effective is the automated metric in detecting data leakage when lacking access to LLM pre-training data?}

\end{itemize}


Based on our results, we present the following insightful findings:
1) Overall, data leakage in SE benchmarks is minimal; however, some benchmarks show relatively higher leakage ratios, such as QuixBugs (100\%), BigCloneBench(55.7\%), APPS (10.8\%), and SWE-Bench-verified (10.6\%). Researchers should be cautious of potential data leakage in the benchmarks they use.
2) We identified several potential causes of high data leakage, such as the direct inclusion of benchmark data in the pre-training dataset or overlap between the repositories used to create the benchmarks and those included in the pre-training dataset.
3) Data leakage has a significant impact on the evaluation of LLMs. For example, StarCoder-7b achieves a Pass@1 score that is 4.9 times higher on the leaked samples than on the non-leaked samples on the APPS benchmark. This underscores how the presence of leaked benchmark samples can lead to significantly inflated metrics, emphasizing the critical need to address the data leakage issue in SE benchmarks to ensure fair evaluation.
4) Developing more advanced methods to effectively detect leaked benchmark samples, especially without access to LLM pre-training data, remains a significant challenge and a promising direction for future research. 



In summary, our contributions are as follows:
\begin{itemize}[leftmargin=*]
\item To the best of our knowledge, this is the first extensive investigation into data leakage across a comprehensive set of 83 SE benchmarks. Our study spans three widely used programming languages—Java, C, and Python—and covers diverse popular tasks in software engineering.
\item We reveal and confirm that data leakage is notably prevalent in certain SE tasks/benchmarks while less significant in others. Additionally, we analyze the potential reasons that contribute to the relatively high leakage and we offer actionable recommendations based on our findings.
\item We developed a framework namely DetectLeak, which can identify and quantify the extent of data leakage. In addition, we offer a cleaned set of SE benchmarks, LessLeak-bench, which removes identified data leaks across diverse Java, C/C++, and Python SE benchmarks. 
\end{itemize}

The remainder of this paper is organized as follows. Section \ref{section2} formalizes the data leakage problem. In Section \ref{section3}, we describe the details of our proposed approach to detect data leakage.
Section \ref{section4} explains our experimental settings. 
Section \ref{section5} presents experimental results and the answers to research questions. 
We present the discussion and related work in Section \ref{section6} and Section \ref{section7}, respectively. Finally, Section \ref{section8} concludes the paper and highlights directions for future research.



