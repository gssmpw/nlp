\subsection{RQ4: Automatic Data Leakage Detection Without Knowledge of Pre-training Data}


\noindent
\textbf{Motivation and Setup.}
Since pre-training data for many LLMs is inaccessible, another key question is whether SE benchmark leakage can be inferred solely from LLM behaviors, such as the LLM's confidence in its generated content, without knowledge of the LLM's pre-training data.



\begin{figure}[b] 
    \centering
    \vspace{-0.6cm}
    \includegraphics[width=10cm]{Figures/one_ppl_figure_dedup_balance.png} 
     \vspace{-0.3cm}
    \caption{Perplexity Distribution of Leaked and Non-leaked SE Benchmark Samples Using StarCoder-7B} 
     % \vspace{-0.7cm}
    \label{fig:ppl}
\end{figure}

A benchmark dataset with reliable labels (i.e., leaked or non-leaked) is crucial for evaluating how effectively data leakage in LLMs can be detected when their pre-training data is unavailable. 
To address this, we utilize our manually labeled dataset created through our data leakage detection approach DetectLeak (as shown in Figure~\ref{fig:method_minhash}). In RQ1 and RQ2, we focus exclusively on leaked samples. In contrast, in RQ4, we include both leaked and non-leaked SE benchmark data samples to form a new dataset, named \textit{AutoDetectLeak-Bench}. Specifically, we extract the manually labeled leaked and non-leaked samples and perform a deduplication process. Since the dataset is imbalanced, we apply random under-sampling to equalize the number of leaked and non-leaked samples.
As a result, AutoDetectLeak-Bench consists of 1,300 samples and serves as a test set to evaluate the effectiveness of automated metrics in detecting data leakage.


\vspace{0.2cm}
\noindent
\textbf{Method.}
We investigate the automatic metric \textit{Perplexity}~\cite{Perplexity} to infer whether benchmark data has been leaked. Specifically, Perplexity measures the confidence of a language model when generating an output, with a lower Perplexity value indicating higher confidence in the model's predictions. The underlying assumption is that an LLM is more confident in examples it has encountered during pre-training. Therefore, intuitively, leaked data samples should exhibit lower Perplexity compared to non-leaked samples.


\vspace{0.2cm}
\noindent
\textbf{Results.}
Figure~\ref{fig:ppl} illustrates the Perplexity distribution of leaked and non-leaked SE benchmark samples, evaluated using the StarCoder-7b~\cite{starcoder_one} model. We removed the top 2\% of outliers (data samples with excessively high Perplexity) from both subsets to enhance the clarity of the visualization.
From Figure~\ref{fig:ppl}, we observe that the Perplexity of leaked samples is not necessarily lower than that of non-leaked samples. On the contrary, in the higher Perplexity range (e.g., Perplexity > 10), the number of leaked samples significantly exceeds that of non-leaked samples. 






\begin{figure}[t] 
    \centering
    \includegraphics[width=10cm]{Figures/ppl_accuracy_dedup_balance.png} 
    \vspace{-0.3cm}
    \caption{Detection Accuracy (Proportion of Leaked Samples in Top-k) of Ranking Perplexity in Ascending Order} 
     \vspace{-0.7cm}
    \label{fig:ranking_ppl_accuracy}
\end{figure}


To quantitatively evaluate the effectiveness of using perplexity to infer benchmark data leakage, we rank all labeled samples in our dataset based on ascending Perplexity scores. The top-k samples, which have the lowest Perplexity, suggest higher LLM confidence and are hypothesized to be more likely leaked. We use a simple detection approach that classifies the top-k samples as leaked, while all remaining samples are classified as non-leaked. The value of k is a tunable hyperparameter.
Figure~\ref{fig:ranking_ppl_accuracy} presents the proportion of leaked samples (i.e., accuracy) within the top-k samples, ranked in ascending order, with results from LLMs of varying sizes: StarCoder-7b, StarCoder-3b, and StarCoder-1b. We evaluate multiple k-values, ranging from 100 to 1,000 in increments of 100.
As shown in Figure~\ref{fig:ranking_ppl_accuracy}, the accuracy of detecting leaked samples based on Perplexity ranking is relatively low across all three models.
Specifically, accuracy typically ranges between 40\% and 50\%, highlighting the challenges of using an automated metric like perplexity for reliable data leakage detection.










\vspace{0.2cm}
\noindent
\begin{tcolorbox} [boxrule=0.8pt,
                top=0.2pt,
                  bottom=0.2pt]
    \textbf{Answer to RQ4}: 
     Our experimental results indicate that detecting data leakage using an automated metric like Perplexity is challenging, with accuracy ranging from only 40\% to 50\% in most cases. We encourage the research community to explore more effective automated methods for identifying data leakage, especially in scenarios where the model's pre-training data is unknown.  
     Future research can use our manually labeled \textit{AutoDetectLeak-Bench} dataset to evaluate newly proposed methods for detecting data leakage when assuming the LLM pre-training data is not available.
\end{tcolorbox}





