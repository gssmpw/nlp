


In this study, we performed the first large-scale analysis of data leakage across 83 software engineering (SE) benchmarks, covering three popular programming languagesâ€”Python, Java, and C/C++. By combining an efficient near-duplicate detection algorithm with extensive manual labeling, we ensured the accurate identification of leaked data.



Our findings show that while data leakage is generally low, with average leakage ratios of 4.8\%, 2.8\%, and 0.7\% for Python, Java, and C/C++ benchmarks respectively, some benchmarks exhibit higher leakage that requires attention. We identified four main causes of leakage: direct inclusion of benchmark data in pre-training datasets, overlap between source repositories, reliance on platforms like LeetCode, and shared data sources such as GitHub issues.
We also found that automatic detection methods, like Perplexity-based metrics, struggle to distinguish between leaked and non-leaked samples. Additionally, our experiments reveal that data leakage inflates evaluation metrics, with models performing significantly better on leaked samples. For instance, StarCoder-7b achieved a Pass@1 score 4.9 times higher on leaked samples, underlining the need to address leakage to ensure fair evaluations.
This study offers insights into data leakage status in SE benchmarks and its impact on LLM evaluation.


In the future, we aim to expand the analysis to additional benchmarks and explore new methods to prevent or further reduce data leakage.





\vspace{0.2cm}
\noindent \textbf{Acknowledgement.}  This research / project is supported by the National Research Foundation, under its Investigatorship Grant (NRF-NRFI08-2022-0002). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.

