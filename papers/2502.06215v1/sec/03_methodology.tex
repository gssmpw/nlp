

\begin{figure}[t] 
    \centering
    \includegraphics[width=14cm]{Figures/lessleak_minhash_lsh-cropped.pdf} 
     \vspace{-0.3cm}
    \caption{Our Data Leakage Detection Pipeline: DetectLeak} 
     \vspace{-0.6cm}
    \label{fig:method_minhash}
\end{figure}



This section describes our framework, \textbf{DetectLeak}, for detecting duplicate data between SE benchmarks and the LLM’s pre-training corpus.
Figure~\ref{fig:method_minhash} presents our DetectLeak framework, which consists of three main steps:

\vspace{0.1cm}
\noindent
\textbf{Step 1: Automatic Detection of Potential Duplicates (\textcircled{1} of Figure~\ref{fig:method_minhash}):} Given an LLM pre-training corpus and an SE benchmark, we use an automated near-duplicate detection tool to identify potential duplicate pairs between the SE benchmark data and the LLM's training data.


\vspace{0.1cm}
\noindent
\textbf{Step 2: Manual Verification on Potential Duplicates (\textcircled{2} of Figure~\ref{fig:method_minhash}:)} Experienced developers collaboratively review and annotate the potential duplicates identified in Step 1. This manual validation ensures the accurate identification of true duplicates.


\vspace{0.1cm}
\noindent
\textbf{Step 3: Quantifying the Extent of Data Leakage (\textcircled{3} of Figure~\ref{fig:method_minhash}):}  In this step, we first identify the leaked benchmark data samples from the manually verified duplicate pairs between the SE benchmark data and the LLM's training data. Then, we quantify the extent of data leakage for each studied SE benchmark and compute their corresponding leakage ratios.




\subsection{Automated Data Leakage Detection}



While automated methods may not achieve the precision of manual labeling, manually reviewing extensive pre-training datasets containing millions of samples is both impractical and cost-prohibitive. Therefore, in the first phase of DetectLeak, we adopt a cost-effective and efficient automated approach to detect potential data leakage. For instance, in this study, we conduct the comparison of 1.7 trillion pairs of LLM pre-training and SE benchmark data,  based on our studied LLM and SE benchmarks, which will be introduced in   Sections~\ref{llm_selection} and~\ref{benchmark_selection}. The sheer scale of this computational effort highlights the challenge posed by the extensive size of LLM pre-training datasets, emphasizing the complexity and resource-intensive nature of studying data leakage regarding LLMs.
This also supports our decision to use the efficient automated tool as the initial step before manual labeling.


\vspace{0.2cm}
\noindent\textbf{Automated Duplicate Detection Tool Selection.}
We specifically choose \textbf{MinHash+LSH}~\cite{SantaCoder, CodeParrot}, a scalable technique for identifying potential duplicates in large datasets. This tool has been widely adopted by many LLMs such as Qwen~\cite{bai2023qwen}, CodeParrot~\cite{CodeParrot}, SantaCoder~\cite{SantaCoder}, StarCoder~\cite{starcoder_one}, and StarCoder v2~\cite{starcoder2} to detect and filter potential duplicate data within their pre-training corpora. 
While the authors of these LLMs use MinHash+LSH for filtering duplicates in their pre-training datasets, we adopt it for a different purpose: detecting potential duplicates between LLM pre-training data and SE benchmark datasets.
We also considered other automated tools for duplicate data detection except for MinHash+LSH but decided against using them for several reasons. 

First, Exact Match is effective for detecting exact duplicates but fails to identify semantic duplicates (i.e., those that are semantically equivalent but not identical), which are crucial for our analysis. 
Second, while many code clone detection approaches are available in the literature~\cite{DBLP:journals/jss/NasrabadiPRRE23}, many effective tools are learning-based and rely on training with data that closely matches the distribution of the test set. These tools are also typically evaluated on Java-based benchmarks, (e.g., BigCloneBench~\cite{BigCloneBench}), and focus primarily on code snippets. 
In contrast, our study spans three different programming languages, and we lack training data for duplicates between SE benchmarks and LLM pre-training data. 
Additionally, benchmarks such as SWE-Bench~\cite{SWE-bench} contain not only code but also other types of content, such as GitHub issue reports (similar to bug reports), which require a tool that can handle both code and texts.
MinHash+LSH, on the other hand, does not require training data, supports multiple programming languages, and is capable of handling both code and text. These features make it a more suitable choice for our study.
Furthermore, we have observed the widespread use of MinHash+LSH in deduplicating LLM pre-training data for models such as Qwen, CodeParrot, SantaCoder, StarCoder, and StarCoder v2. However, we have not noticed code clone detection tools applied to LLM' huge pre-training data to detect duplicates yet. For these reasons, we chose to follow the established practice of many LLM developers to select MinHash+LSH for our analysis.





\vspace{0.2cm}
\noindent\textbf{MinHash+LSH Method Details.}
As shown in \textcircled{1} of Figure~\ref{fig:method_minhash}, the method consists of several steps to detect potential duplicate pairs. For a given pair of data samples, the method first decomposes each sample into n-gram tokens. Then, the MinHash~\cite{DBLP:conf/cpm/Broder00} module is applied to generate hashes for each sample. Next, Locality-Sensitive Hashing (LSH)~\cite{LSH} is used to group these hashes into bands and hash the bands. By comparing the hashes of bands, the method identifies similar data samples: if a pair of data samples share the same band hash, they are considered candidate duplicate pairs.
For each candidate pair, the method calculates the Jaccard similarity coefficient~\cite{Jaccard_index} between the two data samples in this pair. If the coefficient exceeds the threshold, the pair is flagged as a potential duplicate by the automated tool. In this study, we set the n-gram size to 2 and the Jaccard similarity coefficient threshold to 0.7, based on initial small-scale trials.








\subsection{Manual Labeling of Potential Duplicates} 


To improve detection accuracy, our approach includes a manual verification step of potential duplicate pairs flagged by the automated tool. 
This large-scale manual labeling process ensures accurate classification of duplicate pairs, confirming the presence of leaked SE benchmark data samples and revealing the extent of data leakage in software engineering benchmarks.
This annotation process is performed by a team of eight experienced data annotators, including three post-doctoral researchers, four PhD students, and one Master's student, each with a minimum of four years of experience in programming. Figure~\ref{fig:method_minhash} (\textcircled{2}) illustrates the annotation process.



\vspace{0.1cm}
\noindent \textbf{Annotation Scheme.} 
Annotators are tasked with categorizing potential duplicate pairs into one of the following four classifications:
\noindent
\begin{itemize} [left=0pt]
\setlength{\itemsep}{0pt} % 调整 item 内部的间距
\setlength{\parskip}{0pt} % 调整段落间距
\setlength{\parsep}{0pt}  % 调整段落与列表之间的间距
\item [-] \textit{Not Related}: The two data samples in the pair are dissimilar. 
\item [-] \textit{Related but Not Duplicates}:  
The two data samples address similar tasks but are designed for distinct purposes.
\item [-] \textit{Semantically Equivalent}: The code may feature renamed identifiers (e.g., variables, constants) while preserving the same logic. The structure may remain largely unchanged with minor additions or deletions. Essentially, the code achieves the same functionality but uses different syntax or implementation. 
\item [-] \textit{Exact Copies}: The pairs are identical except for variations in formatting or comments. 
\end{itemize}
\noindent
In this categorization scheme, the first two categories represent the ``\textit{Non-duplicate}'' class, while the latter two correspond to the ``\textit{Duplicate}'' class. We adopt this four-class categorization to gain more detailed insights from annotators, compared to a simple binary classification (non-duplicate or duplicate).

\vspace{0.1cm}
\noindent \textbf{Annotation Process.} 
In our study, the automated tool MinHash+LSH identified 6,643 potential duplicate pairs, based on the selected LLM and SE benchmarks, which will be discussed in Sections~\ref{llm_selection} and~\ref{benchmark_selection}. Given the large volume of data to label, it is impractical for all annotators to review every pair.
To address this, each potential duplicate pair is independently reviewed by two annotators, who evaluate the similarity between the data samples and assign one of the four annotation categories (ranging from ``\textit{Not Related}'' to ``\textit{Exact Copies}'').
After completing the labeling, we calculate the Cohen's Kappa Score~\cite{Cohens_kappa} between the two annotators based on their labels. The average Cohen's Kappa Score is 0.9424, indicating almost perfect agreement~\cite{landis1977measurement}. Notably, when we simplify the four-class categorization (ranging from ``\textit{Not Related}'' to ``\textit{Exact Copies}'') into a binary classification (``\textit{Duplicate}'' or ``\textit{Non-duplicate}''), the Cohen's Kappa Score increases to 0.9591, reflecting even higher agreement on determining whether pairs are duplicates or not.
Although the agreement between the two annotators is high, there are still some pairs where they disagree on the label. In such cases, a third annotator will resolve the conflict by reviewing the labels assigned by the previous two annotators and re-assigning the most appropriate label for the pair.
Finally, after applying our manual labeling method, we identified 1,950 real duplicate pairs and 4,741 non-duplicate pairs out of the 6,691 pairs flagged by the automated tool. These statistics are based on the selected LLM and SE benchmarks, which will be later discussed in Sections~\ref{llm_selection} and~\ref{benchmark_selection}.


\subsection{Quantifying the Extent of Data Leakage}
In the previous steps, we identified the actual duplicate pairs between the SE benchmark data and the LLM’s training data. Using these duplicate pairs, we can easily identify the leaked benchmark data samples. If a pair is marked as a duplicate, the SE benchmark data sample in that pair is considered leaked, as it has been found to have a semantically equivalent or identical counterpart in the LLM’s pre-training data.

Using the confirmed leaked samples, we quantify the extent of data leakage for each studied SE benchmark. To achieve this, we employ two simple and clear metrics: leakage count and leakage ratio. These metrics are defined as follows:
(1) Leakage Count: the total number of leaked benchmark data samples, i.e., $N_{\text{\textit{leak}}}$ ; (2) Leakage Ratio: the proportion of leaked data samples within the benchmark, calculated as: $N_{\text{\textit{leak}}}/N_{\text{\textit{total}}}$.
Here, \(N_{\text{\textit{leak}}}\) denotes the number of identified leaked benchmark samples, and \(N_{\text{\textit{total}}}\) represents the total number of data samples in the benchmark.
A higher leakage count or ratio indicates a more significant degree of data leakage within the benchmark, raising concerns about its reliability for evaluating LLMs.



\subsection{LessLeak-Bench}
After identifying leaked samples in SE benchmarks using our DetectLeak approach, we introduce \textbf{LessLeak-Bench} to mitigate data leakage risks. Specifically, we remove all identified leaked samples from each studied SE benchmark, producing cleaned versions that are free from known data leakage. This curated collection, LessLeak-Bench, serves as a more reliable and comprehensive benchmark for evaluating LLMs across diverse SE tasks.