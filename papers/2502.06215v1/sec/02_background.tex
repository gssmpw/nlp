

\subsection{Data Leakage Definition}






Data leakage (also called Data Contamination) in LLMs refers to the unintentional inclusion of evaluation data during the model's construction phase (e.g., pre-training or training)~\cite{balloccu2024leak}. This issue can lead to an inaccurate assessment of a model's true capabilities. To formally define data leakage, we denote \( D_{construct} \) as the pre-training/training dataset used for model construction and \( D_{eval} \) as the evaluation dataset. Data leakage occurs if either of the following two conditions is met:




\vspace{0.1cm}
\noindent
1. \textbf{Exact Leakage.} At least one evaluation sample exactly matches a data sample in the pre-training or training set: \[D_{eval} \cap D_{construct} \neq \emptyset \]  




\vspace{0.1cm}
\noindent
2. \textbf{Semantic Leakage.} At least one evaluation sample is semantically equivalent to a data sample in the pre-training or training set:
\[
   \exists x_{eval} \in D_{eval}, \exists x_{construct} \in D_{construct} \colon
   f(x_{eval}, x_{construct}) = 1
\]
Here, \( f(x_{\text{eval}}, x_{\text{construct}}) \) is a function that measures the semantic equivalence between two data samples. If the samples are semantically equivalent, the function returns 1; otherwise, it returns 0. This condition indicates that at least one evaluation sample is semantically equivalent to a pre-training/training sample. 




\vspace{0.2cm}
\noindent\textbf{Data Leakage Regarding LLMs.}
For approaches based on LLMs, leakage can occur in two distinct phases: (1) during pre-training, where evaluation data is inadvertently included in the pre-training dataset, and (2) during fine-tuning or prompting, where evaluation data is accidentally incorporated into the training set.
Detecting the second type of data leakage, which occurs between training and evaluation data, is straightforward. Since the training data is usually available, researchers can easily check for any overlap with the evaluation data (or test set). In contrast, identifying the first type of data leakage, which involves the relationship between pre-training and evaluation data, presents significant challenges. This difficulty arises from two main factors: 1) pre-training datasets for LLMs are often undisclosed, making it difficult to ascertain their contents, and 2) the sheer size of pre-training datasets renders it computationally expensive to examine all data for potential overlaps.
In this study, we focus on investigating data leakage between the pre-training dataset of LLMs and diverse SE benchmarks that serve as evaluation data for various SE tasks.



\subsection{The Relationship Between Data Leakage Detection and Code Clone Detection}
\revised{
Data leakage detection and code clone detection both seek to identify duplicate data pairs but serve distinct purposes. Clone detection focuses on identifying semantically equivalent or identical code snippets, aiming to reduce redundancy and improve maintainability. In contrast, data leakage detection examines the overlap between benchmark data and an LLM's pre-training corpus to determine whether the model has encountered test data during pre-training or training. Furthermore, while code clone detection is usually limited to duplicate source code, data leakage detection applies to a wider array of data formats, including code, natural language texts, and structured data. 
Finally, the actions taken after detection also differ. Code clone detection typically leads to the removal or merging of duplicates to optimize the codebase, while data leakage detection focuses on eliminating overlaps between the benchmark dataset and pre-training/training data to preserve the validity of LLM evaluations.
}

