\subsection{RQ1: Data Leakage Status in SE Benchmarks}




\begin{figure}[t] 
    \centering
    \includegraphics[width=12cm]{Figures/leakless_data_overview-cropped.pdf} 
    \vspace{-0.2cm}
    \caption{Data Leakage Overall Results} 
    \vspace{-0.3cm}
    \label{fig:data_review}
\end{figure}

\input{Tables/RQ1_py}
\input{Tables/RQ1_java}
\input{Tables/RQ1_c}




Table~\ref{tab:main_results_py} to Table~\ref{tab:main_results_c} present the data leakage status of the studied SE benchmarks for Python, Java, and C/C++, respectively. The column ``\textit{\#Auto}'' represents the number of potentially duplicate pairs identified by the automated tool MinHash+LSH. The column ``\textit{\#Manual}'' shows the number of actual duplicate pairs verified through manual labeling. The column ``\textit{Leaked Count}'' refers to the Leakage Count, which is the number of actual leaked SE benchmark samples. Lastly, the column ``\textit{Leaked Ratio}'' refers to the Leakage Ratio, which shows the proportion of benchmark data identified as leaked to an LLM.
Both the Leakage Count and Leakage Ratio serve as key metrics for assessing the extent of data leakage in the benchmarks. A higher value for these metrics indicates a more severe data leakage problem in the given SE benchmark.
It is important to note that a single SE benchmark sample may appear multiple times in an LLM's pre-training dataset. For instance, an identical function may be present in different software repositories used in LLMs's pre-training. These occurrences can result in multiple duplicate pairs being identified, even though they correspond to a single leaked SE benchmark sample. As a result, we may observe that the Leakage Count is occasionally smaller than the number of duplicate pairs identified during manual labeling.







 \vspace{-0.1cm}
\subsubsection{Data Leakage in Python Benchmarks}
\revised{
Table~\ref{tab:main_results_py} reveals that data leakage in Python benchmarks is generally minimal, with an average leakage ratio of only 4.8\%. Our results indicate that \textbf{many Python benchmarks have low or negligible leakage rates}, making them suitable for reliable evaluation of LLMs. As shown in Table~\ref{tab:main_results_py}, several benchmarks, such as BigCodeBench\textit{-py}, BioCoder, CanItEdit, ClassEval, CodeBenchGen, CodeEditorBench\textit{-translate-py}, CodeReview\textit{-py}, CodeReviewNew\textit{-py}, CodeScope\textit{-py}, DS-1000, G-TransEval\textit{-py}, LiveCodeBench series, Mconala\textit{-es}, Mconala\textit{-ja}, PythonSaga, and SVEN\textit{-py}, exhibit no evidence of the leakage issue (i.e., Leakage Ratio=0\%) in our study, highlighting that the threat of data leakage is relatively small when using those benchmarks to evaluate LLMs.}

\revised{
\textbf{However, a few Python benchmarks demonstrate much higher leakage rates, raising concerns about their bias in evaluation.} Notable benchmarks include 
APPS (10.8\%), BugsInPy (11.0\%), CodeEditorBench\textit{-debug-py} (10.7\%), CodeEditorBench\textit{-switch-py} (7.2\%),
EvoCodeBench (6.5\%),
QuixBugs (100\%), SWE-Bench (8.7\%), and SWE-Bench\textit{-verified} (10.6\%). These elevated leakage ratios suggest that a relatively large portion of the data samples in these benchmarks was encountered during LLM pre-training, which could compromise the fairness and validity of evaluation results. 
This underscores the importance of removing identified duplicate benchmark data in future work to ensure unbiased evaluation when utilizing these benchmarks.
}



 \vspace{-0.1cm}
\subsubsection{Data Leakage in Java Benchmarks}
Data leakage in Java benchmarks is also generally low, with an average leakage ratio of just 2.8\%, as shown in Table~\ref{tab:main_results_java}. This is notably lower than the average leakage ratio observed in Python benchmarks. The results reveal that \textbf{many Java benchmarks exhibit low or negligible leakage rates}, making them suitable for reliable evaluation of LLMs. Many Java benchmarks, including AixBench\textit{-auto}, AixBench\textit{-manual}, and several others, show no evidence of leakage issues (i.e., Leakage Ratio=0\%). This finding highlights that the risk of data leakage is relatively small when using these benchmarks. 


\textbf{However, two Java benchmarks exhibit much higher leakage rates.
}
BigCloneBench has a leakage ratio of 55.7\%, while CodeEditorBench\textit{-switch-java} has a leakage ratio of 9.9\%. 
Thus, it is important to remove those identified leaked benchmark samples before using these two benchmarks in future research related to LLMs.

 \vspace{-0.1cm}
\subsubsection{Data Leakage in C/C++ Benchmarks}
\revised{
Data leakage in C/C++ benchmarks is minimal, with an average leakage ratio of only 0.7\%, as shown in Table~\ref{tab:main_results_c}. This represents the lowest average leakage ratio among the Python, Java, and C/C++ benchmarks analyzed. The results indicate that \textbf{most C/C++ benchmarks exhibit low or negligible leakage rates}, making them suitable for reliable evaluation of LLMs.
\textbf{However, two C/C++ benchmarks demonstrate relatively higher leakage rates}: CodeEditorBench\textit{-debug-c} (4.5\%) and CodeEditorBench\textit{-switch-c} (8.3\%). These elevated leakage rates emphasize the importance of addressing data leakage by removing identified duplicate benchmark data when using these two benchmarks. In contrast, the threat of data leakage for other C/C++ benchmarks remains significantly lower.
}



 \vspace{-0.1cm}
\subsubsection{Data Leakage Status in Specific SE Tasks}



\revised{
We will now discuss the leakage rates across different tasks. We first focus on Code Generation and Program Repair, as these are both highly popular tasks and constitute the majority of the benchmarks analyzed.
For Code Generation, out of 33 benchmarks, 26 have a leakage rate of 0\%, and the average leakage rate across all benchmarks is just 0.62\%. This indicates that the issue of data leakage in the Code Generation benchmarks we studied is relatively minor.
In contrast, for Program Repair, only 1 out of 9 benchmarks has a leakage rate of 0\%, while the average leakage rate reaches 12.5\%. This suggests that data leakage poses a significant challenge for Program Repair benchmarks.}

\revised{
Furthermore, when examining the top 10 benchmarks with the highest leakage rates, the task distribution is as follows: Code Editing (4 benchmarks), Program Repair (2 benchmarks), Issue Fix (2 benchmarks), Clone Detection (1 benchmark), and Code Generation (1 benchmark). 
This distribution indicates that Program Repair, Code Editing, and Issue Fix tasks are relatively more vulnerable to higher leakage rates, as they contain more benchmarks with top leakage compared to other tasks.
}



 \vspace{-0.1cm}
\subsubsection{LLM Evaluation with LessLeak-Bench}
Our experimental results indicate that while the average leakage rate in SE benchmarks is low, certain benchmarks have been significantly impacted by data leakage. To support researchers, we have removed the leaked samples and developed LessLeak-Bench, a cleaned version of SE benchmarks. We recommend using LessLeak-Bench in future studies to ensure more reliable LLM evaluations, as many leaked benchmark samples are no longer suitable for this purpose.


\vspace{0.3cm}
\noindent
\begin{tcolorbox} [boxrule=0.8pt,
                top=0.2pt,
                  bottom=0.2pt]
    \textbf{Answer to RQ1}: 
    \revised{
   In general, data leakage in SE benchmarks is minimal, with average leakage ratios of only 4.8\%, 2.8\%, and 0.7\% for Python, Java, and C/C++ benchmarks, respectively. However, some benchmarks exhibit relatively higher leakage ratios. For instance, QuixBugs and BigCloneBench have leakage ratios of 100.0\% and 55.7\%, respectively.
   Additionally, the Program Repair task generally shows higher leakage ratios, with an average leakage rate of 12.5\%. Researchers should exercise caution when applying LLMs to program repair tasks, as these benchmarks may require the removal of duplicate data samples to ensure unbiased evaluations.}
\end{tcolorbox}



