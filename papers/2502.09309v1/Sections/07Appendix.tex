\appendices
\section{Proof of Theorem \ref{Th. Theorem1}} \label{App: H_b}
\begin{proof}
 Using the approach from \cite[Proposition 4.5]{banos2012reset}, to demonstrate the quadratic stability of the system in \eqref{eq.SS closed loop}, we show the existence of a matrix $P > 0$ such that the quadratic Lyapunov function \( V(x(t)) = x^\top(t) P x(t) \) decreases over the entire state space along the system trajectories and is non-increasing at the reset jumps (see \cite[Theorem 1]{beker2004fundamental}). This leads to the following:
\begin{equation}
    \label{eq: Lyapunov A}
    \bar{A}^\top P+P\bar{A}<0, 
\end{equation}
and
\begin{equation}
    \label{eq: reset jumps lyapunov}
    x^\top (t)\left(A_\rho^\top P A_\rho -P \right)x(t)\leq 0, \: \forall x(t)\in \mathcal{F}, 
\end{equation}
where the reset surface $\mathcal{F}$ for the zero input case ($w(t)=0$) gives $\{x(t) \in \mathbb{R}^{n_l+1}:\bar{C}_ex(t)=C_e x_l(t)=0 \wedge (I-A_\rho)x(t)\neq0\}$ with $\bar{C}_e=\begin{bmatrix}
    0 & C_e
\end{bmatrix}$.\\
Considering $P=P^\top=\begin{bmatrix}
    P_1 & P_2 \\
    P_2^\top & P_3
\end{bmatrix}$, $A_\rho = \begin{bmatrix}
    \gamma & 0_{1\times n_l}\\
    
    0_{n_l\times 1} & I_{n_l\times n_l}
\end{bmatrix}$, and $x(t)=\begin{bmatrix}
    x_r(t)^T & x_l(t)^T
\end{bmatrix}^T$, for \eqref{eq: reset jumps lyapunov} we have:
\begin{equation}
\begin{split}
    \label{eq: extended lyapunov}
     &x^\top (t)\left(A_\rho^\top P A_\rho -P \right)x(t)\\
     &=(\gamma^2-1)x_r^2(t)P_1+(\gamma-1)x_r(t)x_l^\top(t)P_2^\top\\
     &+(\gamma-1)x_r(t)P_2x_l(t).
     \end{split}
\end{equation}
By selecting $P_2=\beta C_e$ where $\beta \in \mathbb{R}$, we have:
\begin{equation}
\begin{split}
    \label{eq: extended lyapunov 2}
     &x^\top (t)\left(A_\rho^\top P A_\rho -P \right)x(t)\\
     &=(\gamma^2-1)x_r^2(t)P_1+(\gamma-1)x_r(t)\beta x_l^\top(t) C_e^\top\\
     &+(\gamma-1)x_r(t)\beta C_ex_l(t).
     \end{split}
\end{equation}
Given that $C_ex_l(t)=0$ at the reset jumps, the expression simplifies to:
\begin{equation}
    \label{eq: extended lyapunov 3}
     x^\top (t)\left(A_\rho^\top P A_\rho -P \right)x(t)\\
     =(\gamma^2-1)x_r^2(t)P_1,
\end{equation}
where for $P_1>0$ and $(\gamma^2-1)\leq 0$ (or $-1\leq \gamma \leq 1$) it results
\begin{equation}
    \label{eq: reset jumps lyapunov 2}
    x^\top (t)\left(A_\rho^\top P A_\rho -P \right)x(t)\leq 0, \: \forall x(t)\in \mathcal{F}.
\end{equation}
This implies that the inequality in \eqref{eq: reset jumps lyapunov} is satisfied for every \(-1 \leq \gamma \leq 1\) and for matrices \(P > 0\) of the following form:
\begin{equation}
    \label{eq: P matrix}
    B_0^\top P=C_0,
\end{equation}
where
\begin{equation}
B_0= \begin{bmatrix}
    1 \\
    0_{n_l \times 1}
\end{bmatrix}, \quad
 C_0=\begin{bmatrix}
\varrho & \beta C_e
\end{bmatrix}.
\label{eq. C0B0 proof}
\end{equation}
with $\varrho=P_1 \in \mathbb{R}^{+}$. Thus, for \eqref{eq: Lyapunov A} and \eqref{eq: P matrix} we can write:
\begin{equation}
    \label{eq: KYP}
\begin{bmatrix}
    \bar{A}^\top P+P\bar{A}+2\varepsilon P & PB_0-C_0^\top \\
     B_0^\top P-C_0 & 0
\end{bmatrix}\leq0,
\end{equation}
with $\exists\, P>0$ and $\exists \,\varepsilon>0$. The remainder of the proof follows that of \cite[Proposition 4.5]{banos2012reset} (from equation (4.30) onward), utilizing the generalized Kalman–Yakubovich–Popov (KYP) lemma \cite{rantzer1996kalman}.





\end{proof}




\section{Proof of Lemma \ref{lem: Hb transfer}}\label{App: I}
\begin{proof}
In this proof, the goal is to show that the transfer function of $H_\beta(s)$ in (\ref{eq.H beta}) is equal to the transfer function in (\ref{eq: H_b FRF}). By starting from (\ref{eq.H beta}) for $(sI-\bar{A})$ we have
\begin{equation}
\label{eq: sI-A}
(sI - \bar{A}) = \begin{bmatrix}
s-A_r & -B_rC_u \\
-B_uC_r & sI-A-B_uD_rC_u \\
\end{bmatrix}=\begin{bmatrix}
Q_1 & Q_2 \\
Q_3 & Q_4 \\
\end{bmatrix}.
\end{equation}
Defining
\begin{equation}
\label{eq: M-1}
(sI - \bar{A})^{-1} = \begin{bmatrix}
W & X \\
Y & Z
\end{bmatrix},
\end{equation}
and substituting (\ref{eq: M-1}) in (\ref{eq.H beta}), gives
\begin{equation}
H_\beta(s) = \begin{bmatrix}
\varrho & \beta C_e
\end{bmatrix}
\begin{bmatrix}
W & X \\
Y & Z
\end{bmatrix}
\begin{bmatrix}
1 \\
0
\end{bmatrix}
= \varrho W + \beta C_e Y.
\label{eq: Hb WY}
\end{equation}
Therefore, only $W$ and $Y$ are needed to be calculated.
By using Lemma \ref{lem: Mat inv} we have
\begin{equation}
W = \left(Q_1 - Q_2Q_4^{-1}Q_3\right)^{-1},
\label{eq: W}
\end{equation}
and
\begin{equation}
Y = -Q_4^{-1}Q_3W.
\label{eq: WY}
\end{equation}
For $W$, using \eqref{eq: sI-A} gives
\begin{equation}
\begin{split}
W &= \left(Q_1 - Q_2 Q_4^{-1} Q_3\right)^{-1} \\ &= \left(s - A_r - B_r C_u (sI - A - B_u D_r C_u)^{-1} B_u C_r\right)^{-1}.
\end{split}
\label{eq: W expand}
\end{equation}
Now considering $K=Q_1$, $U=Q_2$, $J=-Q_4^{-1}$, and $V=Q_3$, by applying Lemma \ref{lem: Woodbury} we have
 \begin{multline}
W = \left(s - A_r\right)^{-1} - \left(s - A_r\right)^{-1} B_r C_u \\ ...\left(-(sI - A) + B_u D_r C_u + B_u C_r \left(s - A_r\right)^{-1}B_r C_u\right)^{-1}\\...B_u C_r \left(s - A_r\right)^{-1}.
\label{eq: W expand 2}
 \end{multline}
Having $R(s)=C_r(sI-A_r)^{-1}B_r+D_r$, the part
\begin{equation}
\left(-(sI - A) + B_u D_r C_u + B_u C_r \left(s - A_r\right)^{-1}B_r C_u\right)^{-1},
\end{equation}
can be rewritten as
\begin{equation}
\label{dfr}
\begin{split}
&\Big(-(sI - A) + B_u D_r C_u + B_u (R(s)-D_r) C_u\Big)^{-1} \\ &= \Big(-(sI - A) + B_u \left(D_r + R(s) - D_r\right) C_u\Big)^{-1}  \\
&= \left(-(sI - A) + B_u R(s) C_u\right)^{-1}.
\end{split}
\end{equation}
Lemma \ref{lem: Woodbury} can also be applied to \eqref{dfr} as follows
\begin{equation}
\begin{split}
&\left(-(sI - A) + B_u R(s) C_u\right)^{-1} \\
&=-\left(sI - A\right)^{-1} - \left(sI - A\right)^{-1} B_u\\... &\left(\frac{1}{{R(s)}} - C_u \left(sI - A\right)^{-1} B_u\right)^{-1} C_u \left(sI - A\right)^{-1}.
\end{split}
\label{eq: w expand}
\end{equation}
For the part $C_u (sI - A)^{-1} B_u$, from system description in (\ref{eq.SS linear}) when $r=d=0$, we have
\begin{equation}
\begin{split}
C_u (sI - A)^{-1} B_u = \frac{U_1(s)}{U_r(s)} = \frac{-C_1(s) C_2(s) G(s)}{1 + C_1(s) C_2(s) G(s) C_3(s)},
\label{eq: P}
\end{split}
\end{equation}
where capitalized variables $U_1$ and $U_r$ are the Laplace transforms of the respective non-capitalized time-domain signals. For simplicity, $\frac{U_1(s)}{U_r(s)}$ is considered as $P(s)$. Therefore, (\ref{eq: w expand}) can be rewritten as follows
\begin{equation}
\begin{split}
&\left(-(sI - A) + B_u R(s) C_u\right)^{-1}=-(sI - A)^{-1}-\\
&(sI - A)^{-1} B_u \left(\frac{R(s)}{1 - P(s)R(s)}\right)C_u (sI - A)^{-1}.
\label{eq: w expand 2}
\end{split}
\end{equation}
Now by substituting (\ref{eq: w expand 2}) in (\ref{eq: W expand 2}), and considering $(s - A_r)^{-1} = \frac{R(s) - D_r}{C_r B_r}$, it gives
\begin{equation}
\begin{split}
W =& \frac{R(s) - D_r}{C_r B_r} - \frac{R(s) - D_r}{C_r B_r} B_r  \Bigg(-C_u(sI - A)^{-1}B_u\\ &- C_u(sI - A)^{-1} B_u \left(\frac{R(s)}{1 - P(s) R(s)}\right)\\...&C_u(sI - A)^{-1}B_u\Bigg)C_r \frac{R(s) - D_r}{C_r B_r},
\end{split}
\end{equation}
where again by considering $C_u (sI - A)^{-1} B_u=P(s)$, it leads to
\begin{equation}
\begin{split}
W =& \frac{R(s) - D_r}{C_r B_r} - \frac{R(s) - D_r}{C_r} \Bigg(-P(s)\\ &-P(s) \Big(\frac{R(s)}{1 - P(s) R(s)}\Big) P(s)\Bigg) \frac{R(s) - D_r}{B_r} \\
=& \left(\frac{R(s) - D_r}{C_r B_r}\right) \left(\frac{1 - P(s) D_r}{1 - P(s) R(s)}\right).
\end{split}
\label{eq: W main}
\end{equation}
Now $W$ is calculated based on known parameters and transfer functions.

For $Y$ from \eqref{eq: WY} we have 
\begin{equation}
\begin{split}
Y = -Q_4^{-1} Q_3 W = (sI - A - B_u D_r C_u)^{-1} B_u C_r W,
\end{split}
\end{equation}
by applying Lemma \ref{lem: Woodbury}, it gives
\begin{equation}
\begin{split}
Y =& (sI - A)^{-1} B_u C_r W - (sI - A)^{-1} B_u \left(\frac{D_r}{P(s) D_r - 1}\right)\\... &C_u (sI - A)^{-1} B_u C_r W.
\end{split}
\end{equation}
Thus, based on \eqref{eq: Hb WY}, for $\beta C_u Y$ we have
\begin{equation}
\begin{split}
\beta C_e Y =& \beta C_e (sI - A)^{-1} B_u C_r W - \beta C_e (sI - A)^{-1} B_u\\... &\left(\frac{D_r}{P(s) D_r - 1}\right) C_u (sI - A)^{-1} B_u C_r W,
\label{eq: Y expand}
\end{split}
\end{equation}
where again from the state-space description of the LTI part of the system in (\ref{eq.SS linear}), for $C_e (sI - A)^{-1} B_u$ we have
\begin{equation}
\begin{split}
&C_e (sI - A)^{-1} B_u = \frac{E_r(s)}{U_r(s)} \\ &= \frac{-C_1(s) C_2(s) G(s)C_s(s)}{1 + C_1(s) C_2(s) G(s) C_3(s)}=P(s)C_s(s).
\label{eq: PCs}
\end{split}
\end{equation}
Thus, by replacing (\ref{eq: P}) in (\ref{eq: Y expand}), it gives
\begin{equation}
\beta C_e Y = \beta C_r W \left(\frac{-P(s)C_s(s)}{P(s) D_r - 1}\right).\\
\end{equation}
Therefore, $H_\beta(s)$ in \eqref{eq: Hb WY} can be written as
\begin{equation}
\begin{split}
H_\beta(s)=&\varrho W + \beta C_e Y=\beta^{'} (R(s) - D_r)\left(\frac{P(s)C_s(s)}{1 - P(s) R(s)}\right) \\ &+\varrho^{'} (R(s) - D_r)\left(\frac{1 - P(s) D_r}{1 - P(s) R(s)}\right),
\label{eq: Hb semifinal}
\end{split}
\end{equation}
where $\beta^{'}=-\frac{\beta}{B_r}$, and $\varrho^{'}=\frac{\varrho}{B_rC_r}$ (we assume $B_rC_r>0$, which is a relavant assumption for GFORE, CI and PCI elements). By replacing $P(s)=\frac{-L(s)}{1 + L(s) C_3(s)}$ and $L(s)= C_1(s) C_2(s) G(s)$ , it yields
\begin{equation}
    \label{eq: H_b main}
    \resizebox{1\hsize}{!}{
    $H_\beta(s)=\frac{\beta^{'}L(s)C_s(s)\Big(R(s)-D_r\Big)+\varrho^{'}\bigg(1+L(s)\Big(C_3(s)+D_r\Big)\bigg)\Bigl(R(s)-D_r\Bigl)}{1+L(s)\Bigl(R(s)+C_3(s)\Bigl)},$
    }
    \end{equation}
which is equal to the transfer function in \eqref{eq: H_b FRF}.
\end{proof}

\section{proof of Theorem \ref{lem:Hb frequency}}\label{App: II}
\begin{proof}
    According to Theorem \ref{Th. Theorem1} to ensure that the reset control system (\ref{eq.SS closed loop}) is globally uniformly asymptotically stable, it must be shown that the $H_\beta(s)$ is SPR, $(\bar{A},B_0)$ is controllable, $(\bar{A},C_0)$ is observable, and $-1<\gamma<1$. The required conditions for a transfer function ($p\times p$) to be SPR are presented in Lemma \ref{lem: SPR}. Regarding the first condition in Lemma \ref{lem: SPR}, the transfer function \( H_\beta \) must satisfy the Horowitz criterion. Since \( H_\beta \) and the base linear transfer function share the same denominator, it follows from the expression for \( H_\beta \) in \eqref{eq: H_b FRF} that if both the base linear system and the shaping filter \( C_s(s) \) are stable, then the first condition in Lemma \ref{lem: SPR} is satisfied.
    
    Furthermore, because the $H_\beta$ transfer function is a single-input and single-output transfer function, the second and third conditions in Lemma \ref{lem: SPR} can be expressed as steps 1 and 2, respectively. Also, controllability and observability of $(\bar{A},B_0)$ and $(\bar{A},C_0)$ will investigate in step 3.
\begin{itemize}
    \item Step 1: It is shown that there is a $\beta$ and $\varrho>0$ such that $\mathfrak{R}(H_\beta(j\omega)) > 0$ for all $\omega \in \mathbb{R}$.
    \item Step 2: It is shown that either $\lim_{s \to \infty}H_\beta (s) >0$ or $\lim_{s \to \infty}H_\beta (s) =0$ and $\lim_{\omega \to \infty} \omega^2\mathfrak{R}(H_\beta(j\omega))>0$.
    \item Step 3: It is shown that $(\bar{A},B_0)$ and $(\bar{A},C_0)$ are controllable and observable respectively.\\
    \label{th. theorem2}
\end{itemize}

\textbf{Step 1:}
To do this, first, it is necessary to calculate the real part of $H_\beta(j\omega)$ in (\ref{eq: H_b FRF}). By utilizing the notation in (\ref{eq.M}) for $H_\beta(j\omega)$, we can then proceed as follows
\begin{equation}
    \label{eq: nsv p 1}
    H_\beta(j\omega)=\frac{\beta^{'}M_2+\varrho^{'}M_3}{M_1},
\end{equation}
multiplying both the numerator and the denominator by the complex conjugate of the denominator ($M_1^{*}$), yields
\begin{equation}
    \label{eq: nsv p 2}
    H_\beta(j\omega)=\frac{\beta^{'}M_2M_1^{*}+\varrho^{'}M_3M_1^{*}}{M_1M_1^{*}},
\end{equation}
where $\mathfrak{R}(M_1M_1^{*})>0$, and $I(M_1M_1^{*})=0$ ($I(.)$ means the imaginary part). Thus 
\begin{equation}
    \label{eq: nsv p 25}
\mathfrak{R}\Big(H_\beta(j\omega)\Big)=\frac{\beta^{'}\mathfrak{R}\Big(M_2M_1^{*}\Big)+\varrho^{'}\mathfrak{R}\Big(M_3M_1^{*}\Big)}{M_1M_1^{*}}.
\end{equation}
To have $\mathfrak{R}\Big(H_\beta(j\omega)\Big)>0$ it is needed to show that 
\begin{equation}
    \label{eq: nsv p 3}
\beta^{'}\mathfrak{R}\Big(M_2M_1^{*}\Big)+\varrho^{'}\mathfrak{R}\Big(M_3M_1^{*}\Big)>0.
\end{equation}
Considering $\overrightarrow{\mathcal{N}}(\omega)$ as (\ref{eq.NSV}), and defining $\overrightarrow{\xi}=\begin{bmatrix}
    \beta^{'} & \varrho^{'}
\end{bmatrix}$, the equation (\ref{eq: nsv p 3}) can be rewritten as
\begin{equation}
    \label{eq: N xi}
    \overrightarrow{\xi}.\overrightarrow{\mathcal{N}}(\omega)>0, \quad \forall \, \omega \in [0,\infty).
\end{equation}
Having $\theta_{\xi}=\phase{\overrightarrow{\xi}}$, and $\theta_{\mathcal{N}}(\omega)=\phase{\overrightarrow{\mathcal{N}}(\omega)}$, we can write \eqref{eq: N xi} as
\begin{equation}
    \label{eq: N xi 2}
    |\overrightarrow{\xi}||\overrightarrow{\mathcal{N}}(\omega)| \cos{\Big(\theta_{\xi}-\theta_{\mathcal{N}}(\omega)\Big)}>0, \;|\stackrel{\rightarrow}{\xi}|\neq0, |\stackrel{\rightarrow}{\mathcal{N}}|\neq0.\\
\end{equation}
Thus, to have $\overrightarrow{\xi}.\overrightarrow{\mathcal{N}}(\omega)>0$, we should have
\begin{equation}
    \label{eq: N xi 3}
    \cos{\Big(\theta_{\xi}-\theta_{\mathcal{N}}(\omega)\Big)}>0, \quad \text{for all}\quad \omega \in [0,\infty).
\end{equation}
Since $\cos{(x)}>0$ yields $-\frac{\pi}{2}<x<\frac{\pi}{2}$, we should have
\begin{equation}
   -\frac{\pi}{2}<\theta_{\xi}-\theta_{\mathcal{N}}(\omega)<\frac{\pi}{2}, \quad \forall \omega \in [0,\infty).
   \label{eq: zettaN 2}
\end{equation}
Thus, knowing $-\infty<\beta^{'}<\infty$ and $\varrho^{'}>0$, it gives
\begin{equation}
    0<\theta_{\xi}<\pi.
    \label{eq: zetta}
\end{equation}
Let $\theta_1 = \smash{\displaystyle\min_{\forall \omega \in \mathbb{R}}} \theta_{\mathcal{N}}(\omega)$ and $\theta_2 = \smash{\displaystyle\max_{\forall \omega \in \mathbb{R}}} \theta_{\mathcal{N}}(\omega)$ (see Definition \ref{def:Types}). In reference to \eqref{eq: zetta}, it follows that $(\theta_2 - \theta_1) < \pi$ must hold to satisfy \eqref{eq: zettaN 2}.

Furthermore, if $\theta_{\mathcal{N}}(\omega)$ lies in both intervals $[\pi, \frac{3\pi}{2})$ and $[-\frac{\pi}{2}, 0)$, it is evident that no $0 < \theta_{\xi} < \pi$ can satisfy \eqref{eq: zettaN 2}.
\\

Hence, the conditions for $\overrightarrow{\mathcal{N}}(\omega)$ to satisfy (\ref{eq: N xi}) are as follows
\begin{itemize}
    \item $-\frac{\pi}{2}<\theta_{\mathcal{N}}(\omega)<\pi$ and/or $0<\theta_{\mathcal{N}}(\omega)<\frac{3\pi}{2}$, $\quad \forall \, \omega \in [0,\infty)$.
    \item $(\theta_2-\theta_1)<\pi$. \\
\end{itemize}

\textbf{Step 2:} Regarding the $H_\beta$ transfer function in \eqref{eq: H_b FRF}, we have
\begin{equation}
\begin{split}
    \label{ Hb infty}
    \lim_{s \to \infty} H_\beta(s)=\beta^{'}L(s)C_s(s)\big(R(s)-D_r\big)\\+\varrho^{'}\big(R(s)-D_r\big),
    \end{split}
\end{equation}
where either $\lim_{s \to \infty}H_\beta (s) >0$ or $\lim_{s \to \infty}H_\beta (s) =0$ and $\lim_{\omega \to \infty}$ need to be satisfied for both cases with $\omega_r\neq 0$ and $\omega_r=0$.\\

\begin{itemize}
    \item $\omega_r\neq0$ ($R(s)=\frac{\omega_k}{s+\omega_r}+D_r$) \\
    \begin{itemize}
        \item[*] $n - m = 1$ (the relative degree of ${L}(s)C_s(s)$)\\
        \begin{equation}
    \lim_{\omega \to \infty} \omega^2\mathfrak{R}(H_\beta(j\omega))=-\beta^{'}K+\varrho^{'}\omega_r \omega_k=\overrightarrow{\xi} . \overrightarrow{\mathcal{N'}},
\end{equation}
by setting $\overrightarrow{\mathcal{N'}}=\begin{bmatrix}
     -K & \omega_r \omega_k
    \end{bmatrix}^{T}$, we have
    \begin{equation}
        \phase{\overrightarrow{\mathcal{N'}}}=\lim_{\omega \to \infty}\phase{\overrightarrow{\mathcal{N}}(\omega)}\xRightarrow{(\ref{eq: teta})}\theta_1 \leq \phase{\stackrel{\rightarrow}{\mathcal{N'}}} \leq \theta_2,
    \label{eq: N_theta}
    \end{equation}
where, by using step 1, and starting from (\ref{eq: N xi}) for $\overrightarrow{\xi} .  \overrightarrow{\mathcal{N'}}$, it gives
\begin{equation}
    \lim_{\omega \to \infty} \omega^2\mathfrak{R}(H_\beta(j\omega))=\overrightarrow{\xi} . \overrightarrow{\mathcal{N'}}>0.
\end{equation}
\item[*] $n-m> 1$ \\
\begin{equation}
    \lim_{\omega \to \infty} \omega^2\mathfrak{R}(H_\beta(j\omega))=\varrho^{'}\omega_r \omega_k>0.
    \end{equation}
    \end{itemize}
\item $\omega_r=0$ \\
    \begin{itemize}
        \item[*] $n-m>1$ \\
        \begin{equation}
            \lim_{\omega \to \infty} \omega^2\mathfrak{R}(H_\beta(j\omega))=0,
        \end{equation}
        This implies that $H(j\omega)$ is not SPR when $n - m > 1$.
        \item[*] $n - m = 1$ \\
        \begin{equation}
    \lim_{\omega \to \infty} \omega^2\mathfrak{R}(H_\beta(j\omega))=-\beta^{'}\frac{K_{n}}{K_{m}}>0,
    \end{equation}
    \end{itemize}
\end{itemize}
means that in this case, the relative degree can only be 1, and $-\beta^{'}\frac{K_{n}}{K_{m}}>0$. Regarding the transfer function in \eqref{eq: L(s)Cs}, for $n-m=1$ we have $L(\infty)C_s(\infty)=\frac{K_{n}}{K_{m}s}$. Which leads to the following conditions:
    %\begin{itemize}
     %   \item The relative degree of the transfer function $L(s)C_s(s)=C_1(s)C_2(s)G(s)C_s(s)$ must always be 1.
     %   \item if $\phase{L(\infty)C_s(\infty)}=-90$ ($\frac{K_{n}}{K_{m}>0$), then $0<\theta_{\mathcal{N}}(\omega)<\frac{3\pi}{2}$.
    %    \item if $\phase{L(\infty)C_s(\infty)}=-270$, then $-\frac{\pi}{2}<\theta_{\mathcal{N}}(\omega)<\pi$.\\
  %  \end{itemize}

    \begin{itemize}
    \item The relative degree of the transfer function $L(s)  C_s(s) = C_1(s)  C_2(s)  G(s) C_s(s)$ must be 1.
    \item If $\lim_{s \to \infty} \operatorname{phase}\left(L(s) C_s(s)\right) = -90$ ($\frac{K_{n}}{K_{m}} > 0$), then $0 < \theta_{\mathcal{N}}(\omega) < \frac{3\pi}{2}$.
    \item If $\lim_{s \to \infty} \operatorname{phase}\left(L(s) C_s(s)\right) = -270$ ($\frac{K_{n}}{K_{m}} < 0$), then $-\frac{\pi}{2} < \theta_{\mathcal{N}}(\omega) < \pi$.
\end{itemize}

\textbf{Step 3:}
In order to show that the pairs $(\bar{A},C_0)$ and $(\bar{A},B_0)$ are observable and controllable, respectively, it is sufficient to show that the denominator and the numerator of $H_\beta (j\omega)$ do not have any common root. Let $a_0 + jb_0$ be a root of the denominator. Then considering the $H_\beta(j\omega)=\frac{\beta^{'}M_2+\varrho^{'}M_3}{M_1}$ in \eqref{eq: nsv p 1}, we should have
\begin{equation}
    M_1(a_0,b_0)=0,
\end{equation}
where for the controllability and observability, the numerator must not have any root at $a_0 + jb_0$, which means
\begin{equation}
   \beta^{'} M_2(a_0,b_0)+\varrho^{'}M_3(a_0,b_0)\neq0.
\end{equation}
Here we assume there is a pair of ($\beta_1 ^{'}, \varrho_1 ^{'}$) such that
 \begin{equation}
 \label{eq: old pair}
    \beta_1 ' M_2(a_0,b_0)+\varrho_1 'M_3(a_0,b_0)=0.
\end{equation}
First, we consider the case that $M_2(a_0,b_0)\neq0$, and/or $M_3(a_0,b_0)\neq0$. From step 1 it can be shown that the eligible pairs for ($\beta^{'}, \varrho^{'}$) always have the following property
\begin{equation}
   \phase{(\beta ^{'}, \varrho ^{'})} \in \begin{bmatrix}
    \theta_2-\frac{\pi}{2}, \theta_1+\frac{\pi}{2}
\end{bmatrix},
\end{equation}
thus,
\begin{equation}
   \phase{(\beta_1 ^{'}, \varrho_1 ^{'})} \in \begin{bmatrix}
    \theta_2-\frac{\pi}{2}, \theta_1+\frac{\pi}{2}
\end{bmatrix}.
\end{equation}
Regarding the condition $(\theta_2-\theta_1)<\pi$ in step 1, it conclude that
\begin{equation}
    \begin{bmatrix}
    \theta_2-\frac{\pi}{2}, \theta_1+\frac{\pi}{2}
\end{bmatrix}\neq \varnothing.
\end{equation}
Therefore, a new pair $(\beta_2^{'}, \varrho_1^{'})=(\beta_1^{'}+\varepsilon, \varrho_1^{'})$ for $\varepsilon>0$ can be found that
\begin{equation}
   \phase{(\beta_2 ^{'}, \varrho_1 ^{'})} \in \begin{bmatrix}
    \theta_2-\frac{\pi}{2}, \theta_1+\frac{\pi}{2}
\end{bmatrix}.
\end{equation}
Substituting the new pairs in (\ref{eq: old pair}) yields,
\begin{align}
\label{eq: neq}
    &\beta_2 ' M_2(a_0,b_0)+\varrho_1 'M_3(a_0,b_0) \\ \nonumber
    &=(\beta_1^{'}+\varepsilon)M_2(a_0,b_0)+\varrho_1 'M_3(a_0,b_0) \\ \nonumber
     &=\beta_1^{'}M_2(a_0,b_0)+\varrho_1 'M_3(a_0,b_0)+\varepsilon M_2(a_0,b_0) \\ \nonumber
      &=\varepsilon M_2(a_0,b_0)\neq0.
\end{align}
Thus, for the case that $M_2(a_0,b_0)\neq0$, and/or $M_3(a_0,b_0)\neq0$, it is possible to find a pair ($\beta^{'}, \varrho^{'}$) such that $H(j\omega)$ be SPR and does not have any pole-zero cancellation.\\

Now considering the case that $M_2(a_0,b_0)=M_3(a_0,b_0)=M_1(a_0,b_0)=0$. Therefore, if we show that when $M_1(a_0,b_0)=0$, always one of the transfer functions $M_2(a_0,b_0)$ or $M_3(a_0,b_0)$ is non zero, then there is not any pole-zero cancellation and the proof is done. Thus, consider
\begin{equation}
M_1(a_0,b_0)=1+L(a_0,b_0)\Big(C_3(a_0,b_0)+R(a_0,b_0)\Big)=0
\label{eq: M1=0}
\end{equation}
\begin{equation}
\begin{split}
M_3(a_0,b_0)=&\bigg(1+L(a_0,b_0)\Big(C_3(a_0,b_0)+D_r\Big)\bigg)\\...&\Bigl(R(a_0,b_0)-D_r\Bigl)=0,
\label{eq: M3=0}
    \end{split}
\end{equation}
where (\ref{eq: M3=0}) yields three cases,
\begin{equation}
    \label{eq: case I}
   \text{I}:
    \begin{cases}
        1+L(a_0,b_0)\Big(C_3(a_0,b_0)+D_r\Big)=0,\\
        R(a_0,b_0)-D_r\neq0,
    \end{cases}
\end{equation}
\begin{equation}
    \label{eq: case II}
    \text{II}:
    \begin{cases}
        1+L(a_0,b_0)\Big(C_3(a_0,b_0)+D_r\Big)\neq0,\\
        R(a_0,b_0)-D_r=0,
    \end{cases}
\end{equation}
\begin{equation}
    \label{eq: case III}
    \text{III}:
    \begin{cases}
        1+L(a_0,b_0)\Big(C_3(a_0,b_0)+D_r\Big)=0,\\
        R(a_0,b_0)-D_r=0.
    \end{cases}
\end{equation}
For the case I, we have $ 1+L(a_0,b_0)\Big(C_3(a_0,b_0)+D_r\Big)=0$, where regarding $M_1$ in \eqref{eq: M1=0}, it yields
\begin{equation}
    R(a_0,b_0)=D_r
\end{equation}
which is not possible to have $R(a_0, b_0) - D_r = 0$ in case I. For cases II and III, $R(a_0, b_0) - D_r = 0$, implying
\begin{equation}
    \frac{\omega_k}{s+\omega_r}+D_r=D_r
\end{equation}
where $ \frac{\omega_k}{s+\omega_r}$ is a strictly proper first-order transfer function and can not be zero. Thus, it is also not possible to have $M_3(a_0, b_0) = 0$ and $M_1(a_0, b_0) = 0$ in these cases. Consequently, it is concluded that there is no pole-zero cancellation, and the pairs $(\bar{A}, C_0)$ and $(\bar{A}, B_0)$ are observable and controllable, respectively. \\

Therefore, regarding the Theorem \ref{Th. Theorem1} and steps 1 and 2, which involve the assessment of the strict positive realness of $H_\beta (s)$, and step 3, which pertains to the controllability and observability of $(\bar{A},B_0)$ and $(\bar{A},C_0)$, the $H_\beta$ condition is satisfied for the reset control system in (\ref{eq.SS closed loop}) by setting $-1<\gamma<1$. Consequently, the zero equilibrium of the reset control system in (\ref{eq.SS closed loop}) achieves global uniform asymptotic stability when $w = 0$. 
\end{proof}
\section{proof of Lemma \ref{lemma: sum_mult Zl}}\label{App: III}
\begin{proof}
    First we show that if $K_1(x_1)$ and $K_2(x_2)$ are $\mathcal{Z}_L$ functions then $K_1(x_1)+ K_2(x_2)$ is also a $\mathcal{Z}_L$ function. Consider $K_1(x_1)=g_1(x_1)h_1(x_1)$ and $K_2(x_2)=g_2(x_2)h_2(x_2)$ where
    \begin{equation}
    h_1(x_1)=\sin{(x_1)},
    \end{equation}
    and
    \begin{equation}
    h_2(x_2)=\sin{(x_2)}.
    \end{equation}
Thus,
\begin{equation}
    K_1(x_1)+ K_2(x_2)=g_1(x_1)\sin{(x_1)}+g_2(x_2)\sin{(x_2)}.
\end{equation}
Using the identity in \cite{SINaSINb}, we have
\begin{equation}
\label{eq k1+k2}
    K_1(x_1)+ K_2(x_2)=\sqrt{Q^2+R^2}\sin{(P+\Phi)},
\end{equation}
where
\begin{equation}
\label{eq: QRP}
    \begin{split}
    Q=&\left(g_1(x_1)+g_2(x_2)\right)\sin{\left(\frac{x_1+x_2}{2}\right)},\\
        R=&\left(g_1(x_1)-g_2(x_2)\right)\cos{\left(\frac{x_1+x_2}{2}\right)},\\
        P=&\frac{x_1-x_2}{2},\\
        \Phi=&\tan^{-1}{\left(\frac{Q}{R}\right)}.
    \end{split}
    \end{equation}
Then from \eqref{eq: QRP}, it can be shown $\lim_{x_1,x_2 \to \infty} \sqrt{Q^2+R^2}=0$. Thus the function $ K_1(x_1)+ K_2(x_2)$ in \eqref{eq k1+k2} is a $\mathcal{Z}_L$ function.\\
For $ K_1(x_1)\times K_2(x_2)=g_1(x_1)h_1(x_1)g_2(x_2)h_2(x_2)$ we can write
\begin{equation}
    K_1(x_1)\times K_2(x_2)=g_1(x_1)g_2(x_2)\sin{(x_1)}\sin{(x_2)},
\end{equation}
where 
\begin{equation}
    \sin{(x_1)}\sin{(x_2)}=\frac{1}{2}\left(\cos{(x_1-x_2)}-\cos{(x_1+x_2)}\right).
\end{equation}
Since every $\cos(y)$ function can be written in $\sin(y+\Phi_y)$ form by a shift in phase, the rest of the proof for $K_1(x_1)\times K_2(x_2)$ is the same as $K_1(x_1)+ K_2(x_2)$. Then, we conclude that if $K_1(x_1)$ and $K_2(x_2)$ are $\mathcal{Z}_L$ functions then $K_1(x_1)+ K_2(x_2)$ and $K_1(x_1)\times K_2(x_2)$ are also $\mathcal{Z}_L$ functions.
\end{proof}