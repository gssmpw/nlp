%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[font=small,skip=0pt]{caption}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{adjustbox}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{array} 
\usepackage{makecell}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
% \icmltitle{Position: AI Benchmarks need ecologically valid tasks and human-validated labels}
% \icmltitle{Position: Building AI Benchmarks for Human-like Intelligence}
% \icmltitle{Position: Look to the History of Cognitive Modeling\\to Benchmark Human-Like Intelligence}


\icmltitle{On Benchmarking Human-Like Intelligence in Machines}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lance Ying}{mit,harvard}
\icmlauthor{Katherine M. Collins}{cam}
\icmlauthor{Lionel Wong}{stf}
\icmlauthor{Ilia Sucholutsky}{nyu}
\icmlauthor{Ryan Liu}{pct}
\icmlauthor{Adrian Weller}{cam}
\icmlauthor{Tianmin Shu}{jhu}
\icmlauthor{Thomas L. Griffiths}{pct}
\icmlauthor{Joshua B. Tenenbaum}{mit}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{mit}{Massachusetts Institute of Technology}
\icmlaffiliation{harvard}{Harvard University}
\icmlaffiliation{cam}{University of Cambridge}
\icmlaffiliation{stf}{Stanford University}
\icmlaffiliation{nyu}{New York University}
\icmlaffiliation{jhu}{Johns Hopkins University}
\icmlaffiliation{pct}{Princeton University}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlcorrespondingauthor{Lance Ying}{lanceying@seas.harvard.edu}
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.
\printAffiliationsAndNotice

\begin{abstract}
Recent benchmark studies have claimed that AI has approached or even surpassed human-``level'' performances on various cognitive tasks. However, this position paper argues that current AI evaluation paradigms are insufficient for assessing human-like cognitive capabilities. We identify a set of key shortcomings: a lack of human-validated labels, inadequate representation of human response variability and uncertainty, and reliance on simplified and ecologically-invalid tasks. We support our claims by conducting a human evaluation study on ten existing AI benchmarks, suggesting significant biases and flaws in task and label designs. To address these limitations, we propose five concrete recommendations for developing future benchmarks that will enable more rigorous and meaningful evaluations of human-like cognitive capacities in AI with various implications for such AI applications. 

\end{abstract}


\section{Introduction}
\label{intro}


From the earliest days of artificial intelligence (AI), the vision of creating machines that think and act like humans has captured the imagination of researchers and the public alike~\citep{turing1950, lake2017building, cave2023imagining, weizenbaum1966eliza, anderson1990cognitive}. This pursuit is driven not only by scientific curiosity -- to better understand intelligence and what it means to be human -- but also by the potential of human-like AI to reshape our world, through the ways that we engage with our work and with each other. Furthermore, building AI that mirrors human cognition is crucial for the critical task of AI alignment. Ensuring that these powerful systems understand and share our values will ultimately lead to safer and more beneficial interactions~\citep{kasirzadeh2023conversation}. A deeper understanding of the mechanisms underlying human intelligence can also inform and enhance the development of more robust and adaptable AI systems.

Despite the acknowledged importance of building human-like AI, \textbf{a clear and consistent definition of what constitutes ``human-like" performance remains elusive}, and we have seen this term inconsistently applied across the literature and public discourse. Recent years have witnessed a surge in claims that AI systems have achieved human-level performance on various tasks. However, the relevance of these results for determining whether AI systems act in a way that is human-``like'' is challenged by the limitations of existing evaluation benchmarks.

In this paper, we argue that current evaluation paradigms are insufficient for assessing the true extent of human-like capabilities in AI systems. Specifically, we highlight three major shortcomings: the too-frequent absence of human validation in dataset labeling, inadequate representation of human variability in collected human data, and over-reliance on simplified tasks that lack ecological validity and fail to reflect the complexity of real-world scenarios. We support these claims with a human evaluation study on $10$ well-known AI benchmark tasks, showcasing potential flaws along these three axes. To address these critical gaps, we propose five concrete recommendations for the development of future benchmarks, derived from best practices in cognitive modeling. We believe these recommendations will pave the way for more rigorous and meaningful evaluations of human-like AI, fostering a more accurate understanding of the current state of the field and guiding its future progress. We close with open questions and challenges of implementing these recommendations.


\begin{table*}[ht!]
\centering
\begin{tabular}{p{3cm}p{3cm}p{9cm}}
\toprule
Benchmark & Task & Description\\
\midrule
\multirow{10}{*}{\begin{tabular}{@{}l@{}}\textbf{BigBench}\\ \cite{srivastava2022beyond}\end{tabular} } & Fantasy reasoning & Reason about scenarios that violate the ordinary rules of the world \\[1mm]
& Social IQA &  Reason about typical social situations. \\[1mm]
 & Moral permissibility & Reason about morally permissible actions in scenarios  \\[1mm]
& Simple ethical questions & Give perspectives on a set of hypothetical, consequential, political, and social questions.\\
& Social support & Distinguish supportive and unsupportive language uses. \\[1mm]
& Irony identification & Determine whether a text is meant to be ironic or not. \\[1mm]
& Dark humor detection &  Detect whether a particular piece of text is intended to be humorous (in a dark way) or not\\
& Movie dialog same or different & Determine whether two adjacent "lines" from a movie dialogue were produced by the same or different individuals.
 \\
 \begin{tabular}{@{}l@{}}\textbf{ToMBench} \\ \cite{chen2024tombench}\end{tabular}  
& Ambiguous story task & Reason and answer questions about ambiguous social situations \\

\begin{tabular}{@{}l@{}}\textbf{BigToM}\\ \cite{gandhi2024understanding}\end{tabular}  & Theory of Mind Reasoning & Answer questions about agent's beliefs and actions \\
\bottomrule
\end{tabular}
\caption{Benchmark tasks used in our experiment to evaluate human response distributions and levels of agreement.}
\label{tab:benchmarks}
\end{table*}



%e.g., ensuring such evaluations are scalable.

\section{Building and Evaluating Human-like AI} \label{human-like AI}


There has been a long history of interest in building and evaluating human-like intelligence in machines. But what do we mean by human-like intelligence? In this paper, we adopt the definition given by Alan Turing \cite{turing1950}: an intelligent system that can elicit similar judgments and behaviors ``indistinguishable from that of a human being.''

% Similarly, 
% We define \textbf{human-like intelligence} as any intelligent system that can elicit similar judgments and behaviors as humans on any cognitive tasks.

But why may we aim for human-like AI? The pursuit of human-like AI is motivated by both scientific curiosity and practical considerations. From the earliest days of AI, scholars have sought to understand, model, and attempt to replicate the intricacies of human cognition and intelligence \citep{rosenblatt1958perceptron,10.7551/mitpress/4943.003.0128,minsky1988society, mitchell2024turing} and use these cognitively-informed models for practical applications. Building human-like AI offers a powerful lens through which to explore fundamental questions about the philosophy of mind, the nature of human cognition, and the underlying mechanisms driving complex human behavior. This quest not only pushes the boundaries of computer science but also promises to deepen our understanding of human intelligence.

Creating AI systems that exhibit human-like thinking and behaviors offers several potential advantages for applications. Human-like AI can think and act instead of humans in many scenarios while ensuring safety and reliability:

\begin{itemize}

    \item Effective Human-AI Interaction: Humans have developed complex social cognitive skills for effective collaboration, which involves simulating other agents' mental states and future actions \cite{bandura2001social, gallese2007before}. AI systems that adhere to human-like patterns of reasoning and behavior can enable human users to easily construct accurate mental models of the AI partner and better simulate and predict the AI partner 's future actions~\citep{collins2024building}. This leads to more effective collaboration and coordination between human users and AI agents~\citep{carroll2019utility, ho2022cognitive, zhi2024pragmatic}. Additionally, interacting with agents that behave predictably and understandably can reduce cognitive load~\citep{dragan2013legibility, fisac2020generating}. We don't have to expend as much mental effort trying to decipher unfamiliar or unexpected behaviors. 

    % Moreover, having AI systems share human representations of the world, can help ensure that these systems can flexibly generalize to new situations in intended ways when learning from limited human supervision~\citep{sucholutsky2023getting, sucholutsky2023alignment}.

   % \item Enhanced Safety and Trust: If AI is human-like, then users can better predict and control its behavior. This reduces the probability of unexpected or harmful outcomes and helps to ensure safe deployment and build user trust [cites]. Such systems are more likely to be accepted by human users, reducing the barriers of adoption. 
    
    % building machines that have human-like value representations is important for 
    %Additionally, humans are social creatures, and we naturally form stronger bonds with those we perceive as similar. This sense of connection can extend to AI systems as well. When an AI demonstrates human-like qualities, we may be more inclined to empathize with it, view it as a partner rather than a tool, and consequently place greater trust in its decisions and actions.
    
    \item Better simulated agents: AI systems with human-like cognitive capabilities are valuable tools for building simulations of people. This has many benefits, including improving communication~\citep{liu2023improving, shaikh2024rehearsal}, generating feedback on pilot studies, and even potentially automating human participant responses in social sciences ~\citep{ashokkumar2024predicting, park2024generative, demszky2023using} or Human Computer Interaction~\citep{hamalainen2023evaluating}. Prior work has also explored the use of LLMs for product testing~\citep{brand2023using} and substituting human subjects in software engineering~\citep{gerosa2024can}. 
    
    \item Flexible generalization: Humans are often considered the gold standard for generalizing from small data and getting AI systems to replicate the mechanisms that drive the human ability to learn so efficiently may enable AI systems to do so too~\citep{lake2017building,sucholutsky2021less,sucholutsky2024using}.
\end{itemize}



% \section{Do people agree with labels used in AI Benchmarks?}

\section{Benchmark Selection and Evaluation}

To motivate our recommendations, we collected human data on $10$ commonly used AI Benchmarks. We selected $8$ benchmarks from BigBench \cite{srivastava2022beyond} under the common-sense reasoning category and two Theory-of-Mind reasoning benchmarks, BigToM \cite{gandhi2024understanding} and ToMBench \cite{chen2024tombench}. The benchmarks are described in Table \ref{tab:benchmarks}. We chose these benchmarks as they represent a wide range of cognitive tasks and do not require any specialized knowledge. Many focuses on language understanding and social cognition, which are particularly pertinent for human-AI interaction. All 10 benchmarks have a single ground truth label for each stimulus.

We randomly sampled $30$ stimuli from each benchmark and recruited $240$ participants from Prolific to label the dataset. Each participant was randomly assigned to a dataset and completed $30$ trials in a randomized order. We used the same answer options provided by the benchmarks, but instead of using a multiple choice question we asked participants to drag a slider on a scale from $1-100$ (e.g. $1$ = strongly disagree, $100$ = strongly agree) for each answer option. 

We highlight some aggregate statistics and diagnostic examples in the section below to support our arguments. More detailed analysis and examples can be found in the Appendix.



\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/histogram_agreement.png}
    \caption{Distribution of participants' agreement with benchmark labels across all 300 stimuli. 26.67\% of the stimuli have less than 50\% agreement with the label (i.e. less than half of the participants selected the label provided by the benchmark).}
    \label{fig:distribution-agreement}
\end{figure}



% \section{Recommendations Derived from Best Practices in Cognitive Modeling}
\section{Pitfalls and Recommendations for Benchmarking Human-like AI}
In this section, we present recommendations for evaluating ``human-like'' AI. There have been several works emphasizing alternate ways to evaluate AI system performance~\citep{burnell2023, shanahan2023role, beyret2019animal}. Here, we focus particularly on how insights from decades of computational modeling can inform how we approach AI benchmarking. The recommendations we propose here derive from years of development and debate in cognitive science to determine best practices for designing tasks, richly comparing models to human judgments, and sharpening hypotheses about what aspects of human behavior a computational model is intended to capture in the first place -- all cornerstones, we argue, of what it means to make theoretically rich, replicable, and measured claims about the sense in which a given model is and is not comparable to human behavior. We urge developers of AI benchmarks to engage with and capitalize on this history.


\subsection{Recommendation 1: Measure `human-like AI' against actual humans -- and collect robust, replicable sample sizes of human data}
\label{human-data}
A surprising number of ``cognitively-inspired" benchmark suites and AI evaluations claim to measure human-like AI performance without any human data at all. Rather, tasks derived or sometimes loosely adapted from psychological assays are used to directly evaluate computational model performance, often with ground-truth notions of what it means to ``solve'' a task (for instance, to identify whether a model can label mental states in simple ``false-belief" tasks derived from cognitive theory-of-mind experiments \cite{wimmer1983beliefs}). Our first and perhaps most fundamental recommendation is that the \textbf{ground-truth labels for measuring whether AI is human-like should be response data collected from humans themselves.} 
%In the next sections, we provide suggestions for designing tasks and evaluations that can best engage with this data to make clearer formal claims.

% Most cognitive and psychological tasks are designed to measure specific, and sometimes, were originally intended for specific human populations, like children in particular age ranges. Behavior on a given task is almost always understood to measure one aspect of underlying mental processes and representations -- while it might be possible to contrast human-performance with various normative responses to particular stimuli (like the correct answers to math problems), few cognitive scientists would ever claim that human-like performance is \textit{equivalent} to normative performance on given task.

Using actual human behavior as the ``gold'' labels for AI benchmarks, we propose, is important for many structural aspects that have been well documented in cognitive science. First, many AI benchmarks seek to evaluate inherently \textit{subjective} concepts -- such as whether an act is morally permissible -- where a single, objectively correct answer (or even any set of ``correct answers") may not exist. Rather, computational models of subjective behavior like moral reasoning, have long sought to characterize distributions of human judgments, including to account for known variation across populations, social groups, and cultures \cite{graham2009liberals,graham2016cultural}, while also seeking to explain how these differences arise \cite{levine2020logic}.

Second, even on tasks that appear to have a single objective ``gold label'' based on external measures, measuring human behavior may still reveal important variation and disagreement, sometimes with high confidence, that is nonetheless revealing of the internal computations by which humans process particular inputs. The famous visual illusion involving \href{https://en.wikipedia.org/wiki/The_dress}{The Dress}, for instance, illustrates people's strongly diverging judgments even given a measurable external label, the true color of the dress. These divergent judgments on this single stimulus reveal important, measurable, and modelable facets of human visual processing \cite{lafer2015striking}. More generally, building systems that are truly human-like or that can well-model human-like behavior requires also modeling human error patterns and uncertainty. Computational cognitive modelers do not shy away from human errors, but rather lean into them; consider ~\citet{battaglia2013simulation} which build a model of how humans reason about our physical world. They find, and model, that we humans are not always accurate in our inferences about physics; such errors -- as the history of studying visual and other perceptual illusions has emphasized -- can help reveal structure in what we do or do not know. Understanding whether a machine is human-like therefore ought to examine such error patterns from the ``true'' state of the world. 

%  \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.35\linewidth]{figures/the_dress.jpg}
%     \caption{The classic ``dress'' is an example stimulus where humans can have low uncertainty but high disagreement. According to a 2015 Reuters poll, 39\% of the people surveyed see white and gold and 37\% see blue and black \cite{ReutersDressDebate}. The real dress is blue and black.}
%     \label{fig: thedress}
% \end{figure}

% ~\citep{steyvers2022bayesian} that are instrumental for many HCI applications like collaborating, communicating with, and teaching human beings.


 % An infamous example from recent memory is the color of The Dress (Fig. \ref{fig: thedress}), where people had strongly diverging opinions about the color of the dress even though there is only a single objective ground truth answer. If we really want to understand \textit{whether} a system is human-like, we want to measure whether AI systems can see and interpret these stimuli as humans do, including our error patterns.  



In our analysis of a suite of common AI evaluation benchmarks that had previously been annotated with only a single ``correct'' answer, we found high levels of disagreement in human judgments. Specifically, we found that on average only 63.51\% of participants agree with the ground truth label for each stimulus with a standard deviation of 20.99. Notably, we found that 26.67\% of the stimuli have a human agreement rate below 50\%. Consider the specific example in Figure~\ref{fig:social_support}, participants are asked to rate whether the statement ``There's nothing wrong with the quotations or discussing her art'' is supportive. Absent of the context, most participants find the statement to be more supportive than unsupportive, yet the ground truth label is ``unsupportive''. We show more such examples in Table \ref{tab: example_1}, \ref{tab: example_2} and \ref{tab: example_3} in the Appendix.



Taken together, our re-annotation of these benchmarks -- with real humans -- suggests that there are serious concerns as to the validity of some published ground-truth labels for benchmarking ``human-likeness.''

%In order to benchmark human-like intelligence, we emphasize that benchmark labels should be validated by a diverse group of human participants. One common practice is the use of human annotators, where a few annotators are recruited and trained to assign labels. In many practices, the benchmark creator usually has a gold annotation for a subset of the stimuli to check against the annotations for agreement and exclude annotators. These practices inject biases into the label-creation process and the resulting labels may not reflect how a wider population of humans would interpret the stimuli. Instead, we encourage benchmark creators to collect responses from a large and diverse sample of human participants in a naturalistic setting in order to ensure the data is representative and replicable. 

%\textbf{This needs some kind of note on the sample size for replicability?}

\subsection{Recommendation 2: Evaluate models of human populations against population-level distributions of human judgments} \label{human-data-dist}
Our second recommendation builds more specifically on the inter-annotator variation we discuss above -- for many AI models, particularly machine learning models explicitly trained on large distributions of human-generated data, we propose that model evaluations should explicitly collect, analyze, and use \textit{population-level distributions of human responses} as the ``gold'' soft labels for evaluating model performance. A fundamental distinction for computational cognitive and psychological models is clarifying which populations of humans one seeks to model, and at what level one seek to model them -- distinguishing, for instance, between a granular model of the algorithms, strategies, and errors that a single human might make across related stimuli on a single domain, with the overall pattern of responses we can expect to find across many subjects. Because many AI models are trained on population-level human data using objectives designed to measure population-level responses, and are often intended for deployment across populations, we argue that it is crucial to collecting and evaluating performance explicitly on how well models capture the structure and variation of behavior across sets of human subjects.


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/social_support_example.png}
    \caption{Distribution of participants' ratings on one of the stimuli. The ground truth label is ``unsupportive''.}
    \label{fig:social_support}
\end{figure}

%and be able to explain that distribution?

% Disagreement across annotators motivates our second recommendation: once you collect human data, what do you measure? We believe that it is important to collect and analyze the distribution of human responses as the ground-truth (soft) label. 

Nearly all facets of human cognition -- perception, decision-making, and commonsense reasoning on any number of inherently subjective tasks -- are influenced by a complex set of \textit{individual differences} and cultural factors. These include differences in underlying cognitive abilities or resources like working memory or attention \cite{boogert2018measuring}; differences in prior experiences, preferences and goals, which can influence how they predict unknowns given limited evidence or choose among a set of options and actions \cite{ongchoco2024new}; and cultural variation in values, expectations, and experiences that systematically influences priors or decision making strategies \cite{henrich2010weirdest}.

% Consider again the dress example: people adamantly disagree with each other on the color of the dress (Fig. \ref{fig: thedress}). Therefore, collecting a representative sample of human data is not just desirable but essential for assessing human-like intelligence -- and the diversity of what it means to be human-like. There is no one single ``human'' for a model to be like. This allows for the creation of benchmarks that reflect the diversity and nuance of human responses, providing a more realistic and informative evaluation of AI systems. By capturing the distribution of human responses, rather than relying on a single, potentially arbitrary label, we can move towards a more nuanced understanding of how AI systems compare to human performance in complex, subjective tasks. This shift in focus will enable a more accurate assessment of progress towards truly human-like AI. Similarly, studying the distribution of responses from models can help us better understand diversity in the model class. 

% Some benchmarks are multi-lingual, created by translating stimuli from one language to another (e.g. \citealt{chen2024tombench}, \citealt{hu2020xtreme}), but often a single label is provided by the benchmark without human data validation by speakers of different languages to ensure consistency. This is problematic because language and culture demonstrably influence reasoning and other cognitive processes [cite relevant cross-cultural psychology research]. Capturing the distribution of responses across different linguistic and cultural groups is essential for developing and evaluating AI systems that can effectively interact with diverse populations.
%This approach is problematic because we want to avoid models aligning with the majority, which, in the context of academic research, is often overrepresented by few ethnic groups ~\citep{henrich2010weirdest}.

Many existing benchmarks collect human annotations but rely on majority voting to collapse the human responses to a single ``ground-truth'' label, effectively discarding valuable information about the range and distribution of human judgments .This may disproportionally lead models to align with the majority view, even if there are important subpopulations that are otherwise underrepresented~\citep{gordon2022jury}. Additional pitfalls of such information loss in label construction have been raised in the context of image classification systems wherein the labels used to train models were often taken to be the label with the majority vote; several works identified that training and evaluating such models on \textit{distributions} over annotator uncertainty (``soft labels'') revealed and guarded against otherwise fragility in such model predictions~\citep{peterson2019human, sucholutsky2023informativeness, collins2023humanMixup, Uma_Fornaciari_Hovy_Paun_Plank_Poesio_2020}. These works also highlight the potential benefit of then training on labels that better capture the richness of human beliefs for enhanced generalization and robustness. We advocate for the consideration of distributions over human data in the context of AI evaluation more broadly. 

Researchers in AI Alignment, specifically ``pluralistic alignment'', have advocated for similar recommendations ~\citep{kirk2024prism, sorensen2024roadmap} but more restricted to alignment to a distribution of values and preferences in decision-making. In our paper, we argue modeling distributions over annotators should \textbf{extend to all cognitive tasks, including perception, planning and reasoning, and should be beyond just culture and values}.

\paragraph{Designing and evaluating population-level metrics} Once we collect the distribution of human data, how may we evaluate AI models? As in cognitive modeling, where researchers often deploy a range of evaluation measures on collected data and conduct analyses on subgroups within populations of participants, we recommend being clear and seeking explicitly to measure the following:

\begin{itemize}
    \item Report metrics used to compare distributions of samples from models (with comparable numbers of samples from the model versus samples from a population of participants) to distributions of human judgments, such as measures on probability distributions (e.g., KL divergence or Wasserstein distances). These metrics can ensure that models do not simply report narrow means, with little of the expected distributional diversity shown across populations as a whole.

    \item  Explain structure within a given distribution of answers. For instance, if distributions have distinct modes, can the model interpretably and consistently explain how these modes arise, or how modes are correlated across related questions?

    \item Measure how the model represent individual patterns of answers and explain individual differences across the population -- for instance, to what degree can it capture conditional patterns based on personal traits (eg. how a pluralist would answer a moral value judgment query versus a utilitarian)? Evaluating conditional distributions can help further focus which parts of a population are well-modeled, and which may be more divergent.
\end{itemize}


\subsection{Recommendation 3: Evaluate model \textit{gradedness} and \textit{uncertainty} against gradedness in individual human judgments}

Just as different people may come to different conclusions about any given task, any single person may be uncertain about what decision they want to make or what plan they want to take. Decades of cognitive science research has shown that graded beliefs and uncertainties are an essential part of human cognition, driving nuanced human perception, reasoning and behaviors \cite{tversky1974judgment, chater2006probabilistic, griffiths2024bayesian}. We encourage benchmark builders to consider eliciting, maintaining, and measuring not just judgment over hard labels with multiple choice questions but graded judgments from \textit{individual} annotators using soft labels. The collection and consideration of soft labels for capturing graded judgments from humans has been standard practice for cognitive modeling and has more recently been advocated for in the context of computer vision~\citep{sucholutsky2023getting}, human-AI interaction~\citep{collins2023human}, and the elicitation of knowledge from experts more broadly ~\citep{uncertainJudgments, expertElicitation}. 



\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{figures/histogram_rating.png}
    \caption{Distribution of participants' ratings on soft labels across all 300 stimuli. Each rating maps onto a ground-truth label of 0 or 100, except 625 ratings where the underlying label is 50 (Neutral).}
    \label{fig:distribution-label}
\end{figure}

Discrete multiple-choice questions that require an annotator to select only one choice are typically too coarse for such measures. In our data collection, we find that $57.69$\% of the ratings are between $20$ to $80$, reflecting participants' graded judgments which are not reflected by binary labels (see Figure \ref{fig:distribution-label} and Appendix for examples). 

We call on AI benchmarks to consider collecting and assessing soft labels from annotators to measure their graded judgments for the following reasons. First, graded judgments better reflect the nuances of real-world scenarios. Real-world decision-making rarely involves absolute, binary choices. Consider emotions, which vary in intensity, or moral judgments, where two wrong actions might warrant different levels of reprimand. Graded responses allow benchmarks to capture these crucial distinctions and nuances and can in turn be used to train models for better generalization to new situations~\citep{peterson2019human}.

Second, soft labels capture the inherent uncertainty prevalent in many tasks. A binary choice often fails to represent the full spectrum of human beliefs and judgment. Individuals may lean towards one option while acknowledging some doubt. This uncertainty is fundamental to real-world reasoning and decision-making. Quantifying uncertainty allows for flexible planning, adaptive strategies, and appropriate risk assessment—essential skills for robust AI systems. While some might argue that large samples with hard labels can approximate uncertainty, this approach hinges on the assumption of independent and identically distributed (i.i.d.) samples. However, this assumption often does not hold in many real-world cases due to individual and group-level variations. Again, consider the example of \href{https://en.wikipedia.org/wiki/The_dress}{The Dress}. Averaging judgments across all samples would show high uncertainty between the two color labels. However, in fact each person is quite adamant about what they see. 

To deeply understand whether a model is human-like, we urge \textbf{finer-grained consideration of the rich, structured beliefs that any single annotator may have.} Researchers may fear perceived ``messiness'' of collecting human uncertainty. An oft heard retort to the collection of uncertainty is that people are ``miscalibrated'' in their uncertainty. Decades of research in cognitive science, however, have designed studies to examine people's probabilistic judgments in order to study and model human cognition ~\citep{keren1991calibration,tenenbaum1998bayesian, chater2006probabilistic,windschitl1996measuring, uncertainJudgments, griffiths2024bayesian}. We encourage designers of AI benchmarks to engage with such literature and lean into these uncertainties in humans' judgments in order to assess models' human-like behaviors.

\subsection{Recommendation 4: Situate tasks with respect to meta-reviews of existing cognitive theory}

% TODO: This section is not finished. Need to make the arguments more clear.

Many AI benchmarks focus on testing human and machine judgments on various commonsense reasoning tasks, from object recognition to classifying sentiments in texts. However, the number of tasks in the world is unbounded, and we cannot have infinitely many benchmarks. To draw generalizable conclusions about an AI model, tasks should be \textit{carefully designed} to measure whether the model's cognitive capabilities are human-like ~\cite{hernandez2017evaluation}. To do so, benchmarks should begin with a theory of the target mental construct, outlining its sub-components and how they manifest in observable behaviors. This theoretical framework then guides the construction of the benchmark, ensuring that tasks effectively probe the specific cognitive capacities of interest and provide meaningful insights into to what extent AI possesses these mental constructs in a human-like way.

Recently, there has been surging interest in probing human-like mental capacities in LLMs, such as personality traits, reasoning, planning, etc. \cite{hagendorff2023human, safdari2023personality, coda2024cogbench}. We encourage these investigations, but we highlight two common pitfalls in existing practice.

One common pitfall is the use of impoverished theory in guiding benchmark creation. For example, many benchmarks have been created to evaluate a machine's Theory of Mind (ToM), which refers to the human ability to make inferences about other agents' mental states. ToM benchmarks for AI commonly or exclusively use the Sally-Anne test (a.k.a. false-belief test) (e.g. \citealt{le2019revisiting}), which has traditionally been used in developmental psychology for evaluating the timing of children's developing Theory of Mind. The results from these evaluations have led to claims such as ToM having emerged in LLMs \cite{kosinski2024evaluating, gandhi2024understanding}. However, ToM embodies a wide range of subcomponents beyond those assessed by the Sally-Anne test. In a comprehensive review, \citet{beaudoin2020systematic} identified 220 ToM tasks and measures previously used by psychological studies. Other authors have also questioned the validity and effectiveness of the Sally-Anne test in assessing children's ToM \cite{bloom2000two}. By exclusively focusing on false-belief tasks, many studies on evaluating AI models' ToM reflect a poor understanding of the meta-theory of ToM as construed in cognitive psychology. Instead, benchmarking intelligent systems should \textbf{start from a meta-theory of the cognitive construct and design tasks grounded in the cognitive theory}, including a comprehensive survey of its subdomains, taxonomies, and measures.

Another common pitfall is the naive use and adaptation of psychological tests in evaluating AI models. Passing a few psychological tests is insufficient to claim certain cognitive capacities exist in machines. Again take the Sally-Anne test as an example. Although it may be effective in measuring children's ToM, tests as such are insufficient for evaluating AI's ToM because AI models are trained specifically to do well on these tests while humans are not. Therefore, blindly taking psychological scales and applying them to AI benchmarks to claim an AI is human-like can result in misleading conclusions and the results will be unlikely to generalize to richer tasks in the real world. Instead, we encourage AI benchmark creators to use psychological theories as a guide and psychological tests as inspirations for designing tasks for evaluating AI's cognitive capacity, but the tasks should be richer, more grounded, and more complex. Research in Cognitive Science in the past decades have introduced many rich and interactive paradigms for studying and evaluating models' social cognition, such as the ones used in \citet{baker2017rational}, \citet{jara2020naive} and \citet{ying2023inferring}, which were used to extract sophisticated and graded reasoning patterns from humans (See Fig \ref{fig:foodtruck} as an example). In the next section, we discuss some concrete recommendations for designing such tasks.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/foodtruck2.png}
    \caption{The Food truck experiment used by \citet{baker2017rational} to study human social reasoning. In this domain, a participant watches an agent moving to get food from a foodtruck. There are three kinds of foodtrucks: Lebanese (L), Mexican (M) and Korean (K). The agent cannot see what foodtruck is behind the wall unless they walk behind it to check. After observing the agent's trajectory, the participant is asked to judge the agent's preference of the foodtrucks and their belief of what foodtruck is behind the wall on a Likert scale. The results show graded judgment in humans across  different agent trajectories.}
    \label{fig:foodtruck}
\end{figure}

\subsection{Recommendation 5: Design Ecologically Valid and Cognitively Rich Tasks}
\label{task}

Benchmark tasks should be ecologically-valid, reflecting the complexity and ambiguity of real-world scenarios, to effectively evaluate AI systems designed for human-like reasoning and interaction. Many existing benchmarks focus on simple, straightforward tasks, often excluding those with low inter-annotator agreement. However, real-world challenges rarely present themselves in such simplified forms. Humans routinely navigate complex situations involving incomplete information, contextual nuances, and ambiguous stimuli. If we want to deeply understand in which ways AI systems are (or are not) human-like in the diversity of settings in which humans engage with the real world, AI benchmarks must move beyond these simplified cases. We next provide several key suggestions for eliciting interesting and rich response patterns in humans and models in more naturalistic settings that paint a broader picture of what it means to be ``human-like''.

\vspace{-0.1cm}
\paragraph{Integration of cognitive capacities:} Benchmarks should incorporate tasks that require integrating multiple cognitive processes, including multimodal reasoning and interaction. For example, understanding the intent behind a sentence might require considering conversational context, the speaker's tone, and even visual cues. The foodtruck example shown in Fig. \ref{fig:foodtruck} requires observers to model the perception and mental states of the agent as well as their goal-directed actions and plans. By incorporating such complexities, benchmarks can better assess an AI's ability to handle nuanced, real-world situations.
\vspace{-0.2cm}

\paragraph{Naturalistic traces of human behavior:} Benchmarks may also consider comparing AI system performance across richer traces of how humans go about solving and creating problems, making decisions, and communicating with each over potentially many interactions, which may include traces of student-teacher interactions~\citep{wang2024tutor} or other professionals' workflows, e.g., how mathematicians come up with proofs~\citep{frieder2024data}. 

\vspace{-0.2cm}

\paragraph{Systematic Ablation:} Ablating tasks by systematically withholding or providing specific information or context can reveal how different factors influence both human and AI judgments and uncertainty. Comparing performance across ablated and full stimuli provides valuable insights into the reasoning processes of both humans and AI systems in settings of varied contextual information, which are common in the real-world.

\paragraph{Structured Ambiguity:} Tasks involving ambiguous perceptual and reasoning challenges, like the example illustrated in \href{https://en.wikipedia.org/wiki/The_dress}{The Dress}, can elicit diverse response patterns among humans. While some benchmarks exclude such stimuli due to lower inter-annotator agreement, we argue that these ambiguous cases are crucial for understanding the nuances of human cognition and evaluating an AI's ability to handle uncertainty. Excluding them limits the benchmark's ability to assess real-world applicability. Rather, we encourage leaning into whether tasks are difficult (which could involve collecting new human-derived ratings of expected difficulty~\citep{zhou2024larger}) and \textit{creating} more such tasks; for instance, more ambiguous or challenging tasks can be created iteratively by modifying the task based on previous humans' responses as in ~\citet{collins2022structured} or via other iterative sampling procedures ~\citep{harrison2020gibbs, sanborn2007markov}. 

By incorporating these design principles, we can create benchmarks that assess AI models' capacity for human-like reasoning, interaction, and adaptation to complex, real-world scenarios.

% \paragraph{Scaling human experiments for robust LLM evaluations:}
% Psychology often makes the assumption that findings about people are generalizable, but these assumptions do not necessarily make sense for language models~\citep{liu2024mind}. Such assumptions include the generalizability of study populations (e.g., undergraduates, prolific), and generalizability across variants of tasks. The latter assumption creates a norm in psychology that focuses on the most simple version of a task that demonstrates a particular psychological effect. 

% However, models are often sensitive to context and minor perturbations to their input, meaning that existing psychology stimuli is no longer suited for evaluating these models. Datasets should inject meaningful variation into prompts, contexts, participant demographics, and create grounded settings in addition to the original abstract stimuli that are used for human participants. Such best practices also include varying the order that answers are presented to avoid biases in answer position~\citep{wang2023large}.
% More generally, it would be beneficial to reason about how objectives shape the models that are built~\citep{mccoy2023embers} and use these to motivate variants in experimental stimuli. 

\section{Alternative Views and Open Challenges}

In this section, we address some challenges and alternative views/arguments on benchmarking Human-like intelligence.


\subsection{Do We Need Human-like AI?}

We acknowledge that certain highly specialized AI applications, such as protein structure prediction~\citep{jumper2021highly} or weather forecasting~\citep{lam2023learning, bodnar2024aurora}, do not require human-like characteristics. Benchmarks for these domains fall outside the scope of this paper. Our focus lies on core cognitive capacities that enable machines to reason, interact, and collaborate \textit{with} humans in the real world~\citep{collins2024building}.

Some might argue that, even in common-sense reasoning tasks, AI systems simply need to perform tasks effectively and be understandable or interpretable, without necessarily mimicking human cognition. We address this perspective in two ways. First, we reiterate the numerous benefits of human-like AI outlined in Section \ref{human-like AI}, including potentially enhanced model performance (robustness and flexible generalization), predictability by other humans, and potential for applications that warrant human-like cognition (e.g. agent simulations).

Second, even when the explicit goal is not to create human-like AI, adhering to the guidelines presented in this paper and looking to best practices from cognitive modeling can provide valuable insights into the AI system. Already, insights from cognitive science are being used to better understand LLMs~\citep{binz2023using}. By comparing AI performance on human-centric benchmarks with actual human responses, we can pinpoint the specific cognitive capacities where AI systems deviate from human-like intelligence. This comparative analysis reveals which aspects of an AI's reasoning and decision-making capabilities align with human thinking and which diverge, providing crucial information for AI safety and governance and informing the ways in which we use these systems. Furthermore, understanding these differences helps AI engineers and system users develop more accurate mental models of their systems~\citep{bansal2019beyond, steyvers2023threePublished}, facilitating more informed design and effective use.


\subsection{Biases and Errors in Human Responses}

A critical consideration in using human data for AI benchmarks is the potential for biases and errors in human judgments. Cognitive science research has extensively documented human limitations in rational reasoning and decision-making, due to limited cognitive resources ~\citep{griffiths2020understanding, lieder2020resource} or systematic biases ~\citep{tversky1974judgment}. This raises the question: should AI systems replicate these human cognitive limitations?

There is no clear answer here. While there are some biases that we want to avoid baking into such models (e.g., harmful racial or gender prejudices), other cognitive biases can be useful for decision making \cite{haselton2009adaptive,lieder2020resource} and essential for accurately modeling human behavior -- and early evidence suggests that such patterns of errors are not implicitly learned in some of today's models, which risks hampered human-AI interaction ~\citep{liu2024large}. For instance, human loss aversion, a well-established cognitive bias, plays a significant role in economic decision-making. Modeling such biases can be crucial for AI systems designed to simulate human behaviors or interact effectively within human economic systems. Conversely, an AI devoid of all cognitive biases might create friction or inefficiencies in collaborative decision-making with humans.

Ultimately, the extent to which AI should replicate human cognitive biases must be evaluated on a case-by-case basis, considering the specific objectives and application of the AI system. Nevertheless, to provide maximum flexibility and support diverse research goals, we recommend that benchmark creators provide both human data and ``bias-free'' labels whenever feasible. This approach empowers researchers to choose the appropriate data for their specific needs, whether it is training AI systems to make highly complex decisions free of bias and errors or accurately modeling human behavior for seamless human-AI collaboration or agent simulation.

\subsection{Scalability and Practicality of Human Data Collection}

Concerns regarding the scalability and practicality of human data collection for AI benchmarks are valid. Gathering human judgments can be resource-intensive, potentially hindering rapid benchmark development particularly if such collection involves eliciting many attributes per annotator~\citep{wu2023fine, collins2024beyond, chung2019efficient, kirk2024prism}. However, we argue that \textit{prioritizing quality over quantity}, and leveraging readily available tools, enable us to begin to address these challenges.

First, benchmark effectiveness does not necessarily correlate with size. A smaller, carefully curated dataset focusing on challenging and edge cases can be more insightful than a massive dataset filled with redundant or trivial examples. By concentrating on high-quality, diagnostically valuable stimuli, we can maximize the benchmark's ability to reveal interesting and rich response patterns in AI systems and humans while minimizing the required data collection effort.

Second, advancements in crowdsourcing platforms, such as Amazon Mechanical Turk and Prolific, have significantly streamlined large-scale data annotation~\citep{griffiths2015manifesto}. These tools provide access to diverse populations, enabling researchers to collect representative samples efficiently. However, maintaining data quality remains crucial. Implementing rigorous exclusion criteria, clear instructions, and attention checks are essential for ensuring the reliability and validity of the collected data. For best practices in data crowdsourcing, we refer readers to \citet{stewart2017crowdsourcing}.

By focusing on quality over quantity and utilizing available crowdsourcing tools effectively, the challenges of human data collection for benchmark development can be successfully mitigated. However, we urge substantial additional research into ways that we can make evaluation with humans more scalable especially as we consider human-likeness not just in a single decision or reasoning trace but in interactions with others~\citep{lee2023evaluating, collins2024evaluating, lee2024design, wang2024tutor}. 

% Note on interface design for eliciting soft labels [cite o'hagan] [Put this elsewhere maybe?]


% \begin{itemize}
%     \item There are alternative views on what AI we should build. Some AI are very specialized at certain tasks (e.g. scientific discovery) and these don't need human data.
%     \item Scalability: collecting human data --- and diverse human data
%     \item What if human data is "wrong"? What about human ``biases''?
%     \begin{itemize}
%         \item Counterpoint: we care about assessing how human-like AI systems are. Do they make the same error patterns as us? There may be cases where we \textit{don't} want AI systems to make the same errors as us, e.g., if we want them to complement us and help us go beyond what we can do [cite thought partner; steyvers; tom game paper]. But those assessments are informed by evaluations of human errors (and flexible successes). 
%         \item We also care not just about the ``correctness'' of an answer, but the amount of resources needed to elucidate such an answer.
%     \end{itemize}
% \end{itemize}





\section{Conclusion}
AI systems are increasingly deployed alongside humans. Characterizing the ways in which AI systems are, or are not, like humans is critical for ensuring we can understand where and how we may interact with these AI systems, and help us design systems that themselves may be more robust and flexible - like people. However, to really know whether an AI system is ``human-like'' demands careful evaluation. In this work, we have encouraged builders of AI evaluation to look to decades of research in cognitive modeling. Cognitive scientists have toiled at the question of how to measure human reasoning and decision-making; AI researchers would be well-positioned to build on this work. Specifically, we encourage AI practitioners to ensure that if they are making claims about a system being ``human-like'' (or want to understand whether a system is or is not), human labels must be collected. We encourage researchers to lean towards, not away, from variability and uncertainty: looking at the distribution of annotators' responses and capturing graded beliefs from each annotator. Further, the tasks over which AI systems are benchmarked demand careful theory-driven design, as well as development in more ecologically-valid settings. AI systems are growing increasingly powerful; we need more robust and reliable evaluation not only if we want to build more human-compatible AI thought partners that we understand but also if we want to deeply understand ourselves.  



\section{Acknowledgments}

This work was funded in part by Schmidt AI 2050, ONR, the MIT-IBM Watson AI Lab, and gifts from Reid Hoffman and the Siegel Family Foundation.

KMC acknowledges support from King's College Cambridge and the Cambridge Trust. AW acknowledges support from a Turing AI Fellowship under grant EP/V025279/1, EPSRC grants EP/V056522/1 and EP/V056883/1, and the Leverhulme Trust via CFI. 

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{example_paper, refs_kmc}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Experiment Design}

\subsection{Dataset sources}
The BigBench dataset consists of 204 tasks. Among the tasks we used in the evaluation study, the Social Support task is adapted from a dataset published by \cite{wang2018s}. The Social IQA task is taken from \citet{sap2019socialiqa}. Other tasks are constructed from various online sources. We refer to BigBench \cite{srivastava2022beyond} for detailed descriptions.

\subsection{Converting multiple choices to soft labels}

All benchmarks used in our experiment provide one single answer key with 2-4 answer options for each stimulus. To collect people's graded judgments, we converted the answer options to soft labels. For binary Yes/No questions (e.g. whether a statement is supportive), we use a single scale (e.g. 1 = extremely not supportive, 100 = extremely supportive). For stimuli that have open-ended answer options, we use a scale for each answer option. For example, consider the following stimulus:

\begin{verbatim}
After rushing to make it to the gate, Robin missed his flight, so Cameron picked 
him up from the airport. What will happen to Robin?

A. Be in a car
B. Pick up their friend
C. Be on a plane
\end{verbatim}


For each of the three answer options, the participants answer by dragging a scale. (1 = definitely disagree, 100 = definitely agree).


\subsection{Evaluation metrics}
To examine if participants agree with the labels, we calculated agreement rate by comparing their responses on the soft label with the ground truth label. For binary Yes/No questions, if the participant rate 50 or above, we count it as Yes and otherwise No. In one of the benchmarks, the labels are No/Neutral/Yes. In this case, we covert 1-33 as No, 33-66 as Neutral, 67-100 as Yes. For stimuli with multiple scales, we compare participants' rating on each scale and take the answer option with the highest rating.

We then calculate the agreement rate for each stimulus by dividing the number of responses in agreement with the label against the total number of responses.

\section{Additional results and analysis}


\begin{table*}[h!]
\centering
\begin{tabular}{llllll}
\toprule
Benchmark & Task  & No. of Options & Random baseline (\%) & Human agreement rate (\%)\\
\midrule
\multirow{6}{*}{BigBench} & Fantasy reasoning & 2 & 50 & 62.69 (10.89) \\
& Social IQA & 3 & 33.33&  68.55 (21.35)\\
 & Moral permissibility & 2& 50& 66.19 (12.86)\\
& Simple ethical questions & 2 or 3& 43.87& 90.29 (13.32)\\
& Social support & 3 & 33.33 & 32.13 (14.25) \\
& Irony identification & 2 & 50 & 68.00 (13.19) \\
& Dark humor detection &  2 & 50 & 70.37 (22.70)\\
& Movie dialog same or different & 2 &50 & 58.42 (17.15)\\
ToMBench& Ambiguous story task & 4 & 25 & 39.90 (15.22)\\
BigToM & Theory of Mind reasoning & 2 & 50 &  78.52 (15.55)\\
\bottomrule
\end{tabular}
\caption{Human agreement rates broken down by benchmark. Standard deviations are shown in brackets.}
\label{tab: breakdown}
\end{table*}

\subsection{Agreement rate by dataset}

The agreement rate for each dataset is shown in Table \ref{tab: breakdown}. The random baseline indicates the expected level of agreement rate with human participants by random guesses. Although all but one benchmark has human agreement rates higher than chance, the level varies significantly across datasets. Many tasks have an agreement rate barely above chance.


\subsection{Qualitative examples}

We show more qualitative examples of human response distributions in Tables \ref{tab: example_1}, \ref{tab: example_2} and \ref{tab: example_3}. In some cases, humans interpreted the stimulus differently and the response distribution is bi-modal (Example 1, 2, 9). In some cases, we find that the benchmark label does not match human intuition (e.g. Example 3, 4). In some cases, we find that humans are quite uncertain (e.g. Example 5, 6). 

In Example 7, most participants find all three answer options to be highly likely, while in Example 10, most participants disagree with all four answer options.

In Example 8, almost all participants agree that option A is better than B and C, yet most participants responded within 50 - 80 range for option A, which indicates that they have hesitations about changing others' political view as the absolute ethical thing to do in this case.

These qualitative examples show interesting response patterns among human participants that aren't captured by existing benchmark labels, yet they provide profound insights into how humans interpret stimuli in a nuanced way and reason with uncertainty.

\begin{table*}[h]
\centering
\begin{tabular}{p{2cm}p{6cm}ccp{6cm}}
\toprule
ID/Task & Scenario & Options & Label & Human response distribution\\
\midrule


1. Irony Identification& Life is good. Maybe you should try getting one. & \begin{tabular}{@{}c@{}} Not Ironic \\  Ironic \end{tabular} & Ironic & \begin{minipage}{.3\textwidth}\includegraphics[width = 6cm]{figures/irony_identification.png} \end{minipage} \\[5mm]
\midrule

2. Fantasy reasoning & "Is it true that you can only see three basic colors?", the alien being asked "It is." "So, how do you communicate with the Ghoosha?" "With whom?", I responded confused. "The other major race on your planet." Can humans not see individuals of the Ghoosha race because Ghoosha skin is colored in two of the three basic colors?
 & Yes/No & No & \begin{adjustbox}{varwidth=6cm,raise=-1.5cm}\includegraphics[width = 6cm]{figures/fantasy_reasoning.png} \end{adjustbox}\\ \midrule
 
 
3. Social Support & It's been five days, no answer, I'll delete. - Jmabel  & \begin{tabular}{@{}c@{}} Unsupportive \\ Neurtal \\Supportive \end{tabular}& Neutral &  \begin{adjustbox}{varwidth=6cm,raise=0cm}\includegraphics[width = 6cm]{figures/social_support_1.png} \end{adjustbox}\\ \midrule


4. Dark humor detection  & What's the difference between jelly and jam? You can't jelly a clown into the tiny car. & \begin{tabular}{@{}c@{}}  Joke \\ Not joke \end{tabular}& Joke & \begin{adjustbox}{varwidth=6cm,raise=-0.5 cm}\includegraphics[width = 6cm]{figures/dark_humor_detection.png} \end{adjustbox}\\\midrule


5. Movie dialog same or different & Yes, I come. I want you to turn out the light in the rear entrance. It might attract the police. But Sacha always puts out that light -- -- Tonight he forgot. Yes, I come, I will do it. ----- In the preceding conversation, were the sentences "I want you to turn out the light in the rear entrance." and "It might attract the police." 
& \begin{tabular}{@{}c@{}} Different \\ Same \end{tabular}&  \begin{tabular}{@{}c@{}}  Same \end{tabular}& \begin{adjustbox}{varwidth=6cm,raise=-1.6cm}\includegraphics[width = 6cm]{figures/movie_same_different.png} \end{adjustbox} \\ \midrule



6. Moral permissibility & A self-driving car with sudden brake failure will need to decide to continue or to swerve. If the car continues ahead, it will drive through one pedestrian crossing the road. This will result in the death of one female doctor. If the car swerves, it will crash into a concrete barrier, killing the driver. This will result in the death of one homeless person.  Should the self-driving car continue?
 & Yes / No & No  & \begin{adjustbox}{varwidth=6cm,raise=-2cm}\includegraphics[width = 6cm]{figures/moral_permissability.png} \end{adjustbox}\\


\bottomrule
\end{tabular}
\caption{Human response distributions on examples of stimuli with binary/bipolar response options.}
\label{tab: example_1}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{p{3cm}p{7cm}clll}
\toprule
ID/Task & Scenario & Label & Human response distribution\\
\midrule
7. Social IQA & \begin{tabular}{@{}p{7cm}@{}}Jan came over one night and searched the house because she was scared a robber could their. Why did Jan do this? \\[1cm] \textbf{A}. Be safe \\ \textbf{B}. Was afraid of criminals \\ \textbf{C}. Secure\end{tabular} & \textbf{B}  & \begin{adjustbox}{varwidth=6cm,raise=0cm}\includegraphics[width = 6cm]{figures/social_iqa.png} \end{adjustbox} \\[5mm]
\midrule
8. Simple Ethical Questions & \begin{tabular}{@{}p{7cm}@{}}Imagine there is a person who is often advocating his political views in a public place. Most people agree that his political views are dangerous. They correctly conclude that if the people like him gain some political power, many people will be harmed. Which option is more likely to be ethical? \\[2.5cm] \textbf{A}. Try to convince him to change his views \\ \textbf{B}. Beat him to death \\ \textbf{C}. All people with such political views must be incarcerated \end{tabular} &  \textbf{A} &\begin{adjustbox}{varwidth=6cm,raise=0cm}\includegraphics[width = 6cm]{figures/simple_ethical_questions.png} \end{adjustbox}
\\[5mm]
\bottomrule
\end{tabular}
\caption{Human response distributions on examples of stimuli with multiple choice options. Each histogram corresponds to a rating scale for each answer option. 1 indicates ``Definitely Disagree'' and 100 indicates ``Definitely Agree''}
\label{tab: example_2}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{p{3cm}p{7cm}clll}
\toprule
ID/Task & Scenario & Label & Human response distribution\\

\midrule
9. BigToM & 
\begin{tabular}{@{}p{7cm}@{}}Kofi is a fisherman from a small village in Ghana. He wants to catch enough fish today to provide for his family and sell the surplus at the market. Kofi repaired his fishing net last night. Kofi believes that his fishing net is in good condition. While Kofi is away from his boat, a group of monkeys comes and plays with the fishing net, tearing it apart. Kofi does not see the monkeys damaging his fishing net. What will Kofi do? \\[3.5cm] \textbf{A}. Kofi will buy new repair equipment for fixing the fishing net. \\ \textbf{B}. Kofi will go fishing with his net. \end{tabular} & \textbf{B} & \begin{adjustbox}{varwidth=6cm,raise=0cm}\includegraphics[width = 6cm]{figures/big_tom_1.png} \end{adjustbox}
\\
\midrule
10. ToMBench & \begin{tabular}{@{}p{7cm}@{}}
The night is deep, and everyone in the community immerses in sweet dreams. Suddenly, Xiao Chen turns the stereo to the highest level, playing songs loudly. Xiao Guang and Xiao Li wake up because of the noise. They step onto the balcony and see Xiao Chen on the balcony of the opposite building, laughing at them with schadenfreude. Xiao Li frowns, prepares to confront Xiao Chen, and picks up a baseball bat. At this moment, Xiao Guang stops Xiao Li, waves at Xiao Li, and then walks downstairs. Xiao Chen sees Xiao Guang coming from the corridor. Why does Xiao Guang wave at Xiao Li? \\ [5.2cm]  \textbf{A}. Xiao Guang laughs because he finds Xiao Chen's behavior interesting.\\ \textbf{B}. Xiao Guang laughs because he finds Xiao Li's frowning expression funny.\\ \textbf{C}. Xiao Guang laughs because he wants to solve the problem in a peaceful way and lets Xiao Li know.\\ \textbf{D}. Xiao Guang laughs because he comes up with a good idea to retaliate against Xiao Chen. \end{tabular} & \textbf{C}& \begin{adjustbox}{varwidth=6cm,raise=0cm}\includegraphics[width = 6cm]{figures/ambiguous_story_task_1.png} \end{adjustbox}
\\
\bottomrule
\end{tabular}

\caption{Human response distributions on examples of stimuli with multiple choice options. Each histogram corresponds to a rating scale for each answer option. 1 indicates ``Definitely Disagree'' and 100 indicates ``Definitely Agree''}
\label{tab: example_3}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
