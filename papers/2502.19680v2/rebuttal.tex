\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{relsize}

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}
\makeatletter
\newcommand\maintitle[1]{%
    \quitvmode
    \hb@xt@\linewidth{%
        \dimen@=1ex
        \advance\dimen@-2pt
        \leaders\hrule \@height1ex \@depth-\dimen@\hfill
        \enskip
        \textbf{#1}%
        \enskip
        \leaders\hrule \@height1ex \@depth-\dimen@\hfill
    }%
}
\makeatother

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter value to the
% number of references you have in the main paper (here, 100).
%\makeatletter
%\apptocmd{\thebibliography}{\global\c@NAT@ctr 100\relax}{}{}
%\makeatother

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{5020} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

\input{alias}

\title{M-LLM Based Video Frame Selection for Efficient Video Understanding}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}


\begin{document}
\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
We thank all reviewers for acknowledging the \textbf{practicality, effectiveness, overall sufficient experiments and good readability} of our work. Particularly, Reviewer v1cR highlights the importance of our work on long video QA. Reviewer 1rKf emphasizes the novelty and flexibility; Reviewer iVGn appreciates our sound ablation studies.
~
\maintitle{Reviewer v1cR}
~
\noindent\textbf{Q: Why not using video-LLM to better capture temporal context such as action/motion? A:} We refer to \textit{Table 3} in [1] for justification. In the temporal reasoning tasks, although video-LLMs are trained with frames sampled from the same video, they fall behind image-trained M-LLMs. It implies image-trained M-LLMs could lead to better temporal pseudo-labels for our method. We tried to use LLaVA-NeXT-Video to generate pseudo-labels. It is more time-consuming but worse pseudo-labels quality. On NExT-QA, using frames labeled by Qwen2-VL and LLaVA-NeXT-Video are 63.9\% and 62.8\% respectively. %We promise to incorporate above results into the camera-ready version.

\noindent\textbf{Q: Evaluating on more very long video benchmarks. A:} We kindly point out that we evaluate our method on two long video benchmarks, i.e. \textbf{EgoSchema} in Table 3 and \textbf{LongVideoBench} in Table 8. Per request, we show results of our method on video-MME in below table. %(some videos in the benchmark are no longer available on YouTube):

\vspace{-9pt}
\begin{table}[h!]
    \centering
    \small\addtolength{\tabcolsep}{-1.4pt}
    \begin{tabular}{@{}lllll@{}}
    \toprule
    VideoMME            & short & medium & long & avg  \\ \midrule
    Qwen2-VL (baseline) & 69.1  & 53.0   & 51.6 & 58.1 \\
    Qwen2-VL + our selector & 69.6  & 54.1   & 51.9 & 58.7 \\ \bottomrule
    \end{tabular}
    \vspace{-12pt}
    \label{tab:rebuttal_longvideo_result}
\end{table}

\noindent\textbf{Q: Evaluate on video grounding benchmark to better understand the quality of video frame selection. A:} 
We compare the moment retrieval performance with SeViLa on QVHighlights. Ours achieves 43.9\% R1@5 and 32.3\% R1@7 while SeViLa achieves 54.5\% R1@5 and 36.5\% R1@7. We think it is comparable since SeViLa is tuned on this datase while our method is zero-shot. We will include above results in the main text to strengthen our contribution.

\noindent\textbf{Q: Do we need to train different selector when applying different backbones? A:} No. Once the selector is trained, it is compatible with various downstream video-LLMs.
~
\maintitle{Reviewer 1rKf}
~
\noindent \textbf{Q: Why integrating spatial and temporal pseudo-labels enhances frame selection performance. A:} Each spatial pseudo-label only considers one frame without taking care of temporal context. On the other hand, for computational efficiency, our temporal pseudo-labels are generated via a captioning and then text reasoning process. Captioning generally loses spatial information, therefore we combine both types of pseudo-labels to leverage their advantages.

\noindent \textbf{Q: Visual token reduction lose information in frames? A:}  We highlight \textit{Table 6} in which we show the performance impacts when using different number of tokens to represent per frame features. It shows that \textbf{3x3} token reduction has the best performance balance on a 7B M-LLM backbone.

\noindent \textbf{Q: Why selecting from 128 frames? A:} The biggest concern is the trade-off between efficiency and performance. We kindly ask to refer to \textit{Table 12} in the appendix. We show the empirical results of selecting from different number of frames, ranging from 16 to 128. Limited by our GPUs, although experiments shows that more candidate frames is better, 128 is the largest number of frames we can achieve. 

\noindent \textbf{Q: Why NMS is more preferable? A:} We discuss the motivation in \textbf{Line 277-282}. Here are more clarifications: (i) some questions require to gather frames from different time period in a video. NMS avoids to sample frames from one video clip with global maximum scores; (ii) consecutive frames contain very similar information, which may be redundant to answer a question. NMS could provide multiple sources of information in a video.

\noindent \textbf{Q: More details for better reproducibility? A:} 
% Details of the pesudo-label generation in including prompt templates are illustrated in appendix. We promise to add more training related details in the camera-ready version.
The details of pseudo-label generation are provided in the appendix. We will further expand on training-related details in the camera-ready version to enhance reproducibility.
~
\maintitle{Reviewer iVGn}
~
\noindent \textbf{Q: More literature? A:} We appreciate pointing them out. [4,5] are not end-to-end methods. [2,3] scores video frames independently without considering the temporal relationship. We will include above discussion in the final version. 

\noindent \textbf{Q: Pre-assigned number of key frames may lead to sub-optimal results across various videos? A:} First, \textit{Table 5, 8} show the performance impact when sampling different number of frames. More selected frames usually leads to better performances. However, it is limited by the context length of the downstream video-LLM. Second, our frame selector ranks the frames based on scores. Unfortunately, it is very difficult to obtain the score threshold for supervising our selector. Therefore, we pre-assign a number of key frames for simplicity and efficiency trade-off.

\noindent \textbf{Q: Is the upper-bound of the proposed method lagged behind because of the lack of quality of the pseudo label? A:} Yes, we also tried to use LLaVA1.5, a weaker M-LLM labeler, to generate pseudo-labels, resulting LLaVA-NeXT-Video on EgoSchema drops more than 1\%. %does not remember the actuall data

% 128 frames from each video, regardless of its length. In general, extracting more frames improves performance. However, we select 128 frames to ensure the model fits within a single GPU for training.%, making training easier.
\noindent \textbf{Q: Different videos extract same number of frames? A:} Yes. Following the common practice in CV, we extract fixed number of frames. Limited by our GPUs' VRAM, 128 is the max we can pick though \textit{Table 12} shows the more input frames the better.
~
\maintitle{Rebuttal Reference}
~
{
    \small
    \renewcommand{\baselinestretch}{0.87}\selectfont
    \noindent  [1] Fang et al: MMBench-Video: A long-form multi-shot benchmark for holistic video understanding \\
    \noindent  [2] Han et al: Self-adaptive sampling for accurate video question answering on image text models \\
    \noindent  [3] Liang et al: End-to-end video question answering with frame scoring mechanisms and adaptive sampling \\
    \noindent [4] Wang et al: Videoagent: Long-form video understanding with large language model as agent. \\
    \noindent [5] Wang et al: Adaptive tree-based video representation for llm reasoning on long videos.

}

\end{document}
