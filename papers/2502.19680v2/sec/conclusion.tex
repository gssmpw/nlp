\section{Conclusion}
In this paper, we propose a lightweight \ac{mllm}-based frame selector to improve both performances and efficiency in video QA. This selector is question-aware and takes dense video frames and the question as input, selecting the most relevant frames. We can then use any multi-image or video M-LLMs with the selected frames to complete the question-answering. To train the frame selector, we introduce spatial and temporal pseudo-labeling due to the limited public annotations for video frame-level importance. Our experiments on two medium-length video QA benchmarks (ActivityNet QA and NExt-QA) and two long-video QA benchmarks (EgoSchema and LongVideoBench, VideoMME) demonstrate the effectiveness of our proposed method.