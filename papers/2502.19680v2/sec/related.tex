\vspace{-3pt}
\section{Related Work}
\vspace{-3pt}
\noindent \textbf{\acf{mllm}} As \acf{llms} continue to demonstrate impressive abilities in language comprehension and reasoning~\citep{achiam2023gpt,team2023gemini,claude_technical_report, touvron2023llama2}, interest is growing within the computer vision community to explore their potential for handling multi-modal inputs. Flamingo~\citep{alayrac2022flamingo} demonstrates the capacity to process image and text inputs for a wide range of multi-modal tasks. BLIP-2~\citep{li2023blip2} introduces a Q-Former to map learned image features into the text embedding space of LLMs, while LLaVA~\citep{liu2023llava} employs simple \acf{mlp} projector to align visual and textual features. Further research has focused on best practices for \ac{mllm}, including areas such as dynamic high-resolution~\citep{liu2024llavanext, chen2024far}, instruction-tuning data~\citep{li2023mvbench, liu2024points}, and different visual encoders~\cite{tong2024cambrian, wei2025vary}. MM1~\citep{mckinzie2024mm1} and Idefics2~\citep{laurenccon2024matters} provide comprehensive ablation studies on the design space of \ac{mllm}.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.87\linewidth]{sec/figures/figure2_new.pdf}
    \caption{An illustration of the conventional \emph{n}-frame video \ac{mllm} framework and our video \ac{mllm} framework with frame selection.}
    \vspace{-9pt}
    \label{fig:pipe}
\end{figure*}

\noindent \textbf{Video \ac{mllm}s} As image-based \ac{mllm} become more mature, research naturally extends to the video modality. Video-ChatGPT~\citep{Maaz2023VideoChatGPT} and Valley~\citep{luo2023valley} use pooling over video features to generate compact visual tokens for downstream \ac{mllm}. Video-LLaVA~\citep{lin2023video} aligns images and videos before projection, allowing \ac{llm} to learn from a unified visual representation. Video-Teller~\citep{liu2023video} points out the importance of modality alignment in pre-training. PLLaVA~\citep{xu2024pllava} studies how different pooling of visual features affects downstream video question answering performance. Regarding long video inputs, LLaMA-VID~\citep{li2025llama} represents each frame with two tokens to reduce the overload of long videos while preserving critical information. MovieChat~\cite{song2023moviechat} propose an effective memory management mechanism to reduce the computation complexity and memory cost, enabling very long video with over 10K frames understanding. \citet{weng2025longvlm} proposes to extract video representations as sequences of short-term local features, and integrate global semantics into each short-term segment feature.

\noindent \textbf{Video Frame Selection} Before the rise of video \ac{mllm}, language-aware video key-frame selection and localization had attracted great interest~\citep{gao2022mist,liu2022ts2,wang2022contrastive,cui2022video}. \citet{buch2022revisiting} optimized an end2end pipeline that uses ground truth question-answering labels to select a single key frame for downstream tasks. \citet{lu2022lgdn} uses a CLIP-like paradigm to train a transformer for visual and text feature alignment. \citet{qian2022locate} trains a video clip proposal model and the downstream QA model in a iterative manner. \citet{kim2023semi} employs a semi-parametric retriever to obtain the key frames by comparing similarities between frame and language features.

\noindent \textbf{Video \ac{mllm} Frame Selection} Several works propose to select key frames to improve video QA performance. SeViLA~\citep{yu2024self} prompts an \ac{mllm} to obtain a relevance score to each frame, and uses these scores to localize important frames. MVU~\citep{ranasinghe2024understanding} and adopts a similar approach as SeViLA. However, a critical limitation of this kind of methods \citep{han2024self,liang2024end} is the absence of temporal reasoning. Each frame is assessed independently without contextual information from other frames. Furthermore, this method is expensive during inference. Importance score of every frame is estimated via a \ac{mllm}. The longer the video the higher the cost. In contrast, our method outputs importance scores for all frames in a single pass with the compressed visual tokens, which reduces the computational costs and enables temporal reasoning. Koala~\citep{tan2024koala} uses sparsely sampled key frames as conditions for processing subsequent visual tokens. ViLA~\citep{wang2024vila} trains an end-to-end frame selection module to mask input frames. However, both of these methods lack text awareness: during inference, Koala's subsequent visual token processing and ViLA's frame masking do not account for the specific question posed about the video. Recent work such \citep{wang2024videotree, wang2024videoagent} are not end-to-end methods. ~\citet{han2024videoespresso} finds frame selection is helpful for video chain-of-thought reasoning.