\vspace{-3pt}
\section{Method}
\vspace{-3pt}

This section introduces our video frame selector designed for efficient video-LLM QA. Section~\ref{sec:method.1} disccuss our motivation. Section~\ref{sec:method.2} outlines the design details of the frame selector. Section~\ref{sec:method.3} explains the generation of pseudo labels for training the frame selector. Section~\ref{sec:method.4} describes the training process of the frame selector.

\vspace{-3pt}
\subsection{Rethinking Uniform Sampling in Video LLMs}~\label{sec:method.1}

\vspace{-15pt}
\noindent\textbf{A typical framework for video LLM} \; An \emph{n-frame} framework is widely adopted in existing research in video \ac{mllm}~\cite{li2023videochat, xu2024pllava, zhang2024llavanext-video}. The number of frames in the input video, denoted by $T$, is variable. For example, a 3-minute video at 30 \acf{fps} contains $T = 5400$ frames. The \emph{n-frame} framework uniformly samples a fixed number of frames, $[x_1, x_2, \cdots, x_n]$, from the total $T$ frames, where each frame $x_i \in \mathbb{R}^{H \times W \times 3}$, with $H \times W$ representing the frame resolution, and typically $n \ll T$. A pre-trained visual encoder $f_v$ extracts visual features from \emph{n} frames. These features are subsequently projected into the \ac{llm}'s space using an alignment projector $g_a$ and then flattened. Spatial pooling may also be applied to reduce the number of output tokens:

\vspace{-6pt}
\begin{equation}
    h_i = \textit{AvgPooling}(g_a(f_v(x_i))), h_i\in\mathbb{R}^{m\times d}
\end{equation}
where $m$ is the number of tokens to represent a frame and $d$ is the hidden dimension of the \ac{llm}. Let $Q \in \mathbb{R}^{l \times d}$ denote the input embedding of the input text question. The \emph{n-frame} framework generates a response $r$ as following:

\vspace{-6pt}
\begin{equation}
    r = \textit{LLM}(h_1, \cdots, h_n, Q).\label{eq:n-frame}
\end{equation}

\noindent \textbf{Uniform sampling is not optimal}\; In the \emph{n-frame} framework, as shown in Figure~\ref{fig:pipe} (a), the input video is represented by $n \times m$ tokens where $m$ is the number of visual tokens per frame. For example, in LLaVA-NeXT-Video~\citep{zhang2024llavanext-video}, where $n=32$ and $m=12\times12$, this results in 4608 tokens. To reduce the computational cost of \ac{llm} inference, previous work has either chosen to reduce $n$ with sliding Q-Former~\citep{li2023videochat} or $m$~\cite{xu2024pllava, li2025llama} by spatial pooling, in other words, reducing $m$. However, both of them ignore the importance of reducing $n$, the number of intake frames, before encoding and $n$ could be large especially in long videos. Denser uniform sampling makes a larger $n$, thereby reducing the efficiency of the video \ac{mllm}. On the other hand, sampling fewer frames risks omitting crucial information. For example, sampling 32 frames from a 3-minute video means taking one frame every 6 seconds, potentially missing actions that occur within shorter time windows. In fact, most questions about a video can be answered using only a limited number of key frames. Inspired by this factor, we argue that adaptive frame selection according to question is more efficient and effective than uniform frame sampling.

\vspace{-3pt}
\subsection{Design of the Frame Selector}\label{sec:method.2}
\vspace{-3pt}
An ideal frame selector should be able to understand complex questions and analyze temporal relevance in the video input. To achieve this, we fine-tune an \ac{llm} to function as the frame selector. The frame selector utilizes the base \ac{llm}â€™s strong language comprehension and reasoning abilities to identify key frames in a video.

Similar to existing video \ac{mllm}, our \ac{mllm}-based frame selection method takes as inputs $n$ sampled video frames along with the corresponding text-based question. Instead of generating an answer to the question, the frame selector identifies the most relevant frames for answering the question. Specifically, we make use of well-trained decoder only \ac{llm} as the frame selector to output an $n$-dimensional vector $s$ as follows:

\vspace{-6pt}
\begin{equation}
    s = \textit{FrameSelector}(x_1, \cdots, x_n, Q)\in \mathbb{R}^n
\end{equation}
where the $i^\text{th}$ element in the vector $s$ indicates the importance score of the $i^\text{th}$ input frame. To achieve this, we append a learnable score query $q\in\mathbb{R}^{1\times d}$ to the end of all input tokens and use the concatenation of the visual tokens, text tokens and the score token as the LLM as input:
\begin{equation}
    e_1, \cdots, e_n, e^Q, e^q = \textit{LLM}(x_1, \cdots, x_n, Q, q_{score})
\end{equation}
where $e_i$ and $e^q$ denote the intermediate output of $x_i$  and $q_{score}$ from the penultimate transformer block. Because the nature of causal attention, $e^q$ aggregates information from all visual and text tokens. We then employ an \ac{mlp} to exact frame importance information from the intermediate output of the score query from the penultimate transformer block:

\vspace{-6pt}
\begin{equation}
    s = \textit{MLP}(e^q), s\in\mathbb{R}^n
    \vspace{-12pt}
\end{equation} \label{eq:selector}

Figure~\ref{fig:pipe} (b) presents an overview of the proposed architecture for the \ac{mllm}-based frame selector. Instead of generating tokens that represent the frame selection, we append a learnable query vector at the end of the input sequence and supervisedly learn this query token. The hidden vector of this query token, $e^q$, then serves as the input to generate the $n$-dimensional importance vector $s$.

\begin{algorithm}[t]
    \caption{Greedy NMS sampling}\label{alg:1}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Importance score $s\in\mathbb{R}^n$, Number of frames selected $k$
        \State Initialize the neighbor gap $\delta\gets\text{integer}(\nicefrac{n}{4k})$.
        \State Initialize the selected index list $I_s=[]$.
        \For{step in $1\cdots, k$} 
        \State (Greedy) Selected frame index $i\gets \arg\max s.$ Append index $i$ to $I_s$.
        \State (NMS) Update the importance score:\\ $s[j] = -1 \;\text{if}\; |i-j|\leq \delta$
        \EndFor
        \State $I_s\gets \text{sort}(I_s)$
        \State \textbf{Return:} $I_s$
    \end{algorithmic}\label{alg2}
\end{algorithm}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.83\linewidth]{sec/figures/pseduo_label.pdf}
    \caption{An illustration of the spatial and temporal pseudo labeling for the importance scores}
    \vspace{-12pt}
    \label{fig:pseduo_label}
\end{figure*}

\noindent \textbf{Select frames from the importance score} After obtaining per-frame importance scores, we need to sample $k$ frames for downstream video question-answering via a video \ac{mllm}. Naively selecting the top $k$ frames with the highest importance scores is suboptimal because neighboring frames within short time intervals often have similar scores. For example, if frame $i$ has the highest importance score, the $i+1$ or $i-1$ frames usually have a closely high scores. The adjacent frame adds little additional information if frame $i$ is already selected. To address this issue, we use a greedy algorithm combined with non-maximum suppression (NMS) to select the most informative frames. The ``Greedy'' approach involves sequentially selecting the top k frames, without replacement, by choosing the frame with the highest importance score from the remaining set of frames. With ``NMS'', once a frame is selected, its neighboring frames are excluded as they contain similar information. The ``NMS-Greedy'' procedure is detailed in Algorithm~\ref{alg:1}. In practice, when selecting $k$ frames from $n$ frames, frame $j$ is considered a neighboring frame of frame $i$ if $|i-j| \leq (\nicefrac{n}{4k})$.

\noindent \textbf{Efficiency of the frame selector}\; As discussed in Section~\ref{sec:method.1}, dense uniform sampling is inefficient for video \ac{mllm}. However, we still employ dense uniform sampling for the frame selector.

Although dense uniform sampling is inefficient for video \ac{mllm}, we keep using it at the beginning to maximally preserve the video information. To overcome the drawback of dense uniform sampling, we apply spatial pooling to reduces the token count per video frame before feeding the frames into the selector. In particular, the spatial pooling reduce the number of visual token to a smaller value, e.g., $m = 3 \times 3$ (9 tokens), which is substantially fewer than the tokens per frame used in existing video LLMs, e.g., $m = 12 \times 12$ (144 tokens). This design follows such an assumption: for video question answering, the model requires a substantial number of tokens per frame to capture visual details, but far fewer tokens are needed to determine whether a frame is important. A rough outline of the frame is sufficient. Our empirical results Table~\ref{tab:ab5.1} also justify this assumption.

\vspace{-3pt}
\subsection{Pseudo Labels for the Frame Selector}\label{sec:method.3}
\vspace{-3pt}

To train the frame selector, we need supervision signals for the output importance scores $s\in\mathbb{R}^n$. Unfortunately, there is no existing dataset to label the frame level importance score for video QA, we propose two methods to generate pseudo labels for training the frame selector.

\noindent \textbf{Spatial Pseudo Labels} 
Previous works use \ac{mllms} to evaluate whether a video frame is relevant to a question~\citep{yu2024self, ranasinghe2024understanding}. A common practice is to prompt an \ac{mllm} and ask if the video frame provides useful information to answer the question. The relevant score of the frame is the probability that the \ac{mllm} generates the ``Yes'' token. In our experiments, we observe that this method does not always provide a reasonable estimation. Even if the model agrees the frame is relevant, the \ac{mllm} may not generate the ``Yes'' token but other expressions depending on its text generation style.
To address this issue, we apply chain-of-thoughts (CoT), asking the \ac{mllm} to explain first then generate a Boolean evaluation (check appendix for the detailed prompt). This prompts allows the \ac{mllm} to improve the evaluation quality by extra inference time reasoning. An ideal \ac{mllm} should adhere to the instruction by generating either ``True'' or ``False''. In a few cases the model may fail to follow the instruction, and we manually append the text ``Evaluation: True'' to the end of the generated response. Thus we can always compute the probabilities of generating ``True'' and ``False'', denoted as $p_{\text{True}}$ and $p_{\text{False}}$ respectively. The importance score of the input frame is determined by

\vspace{-9pt}
\begin{equation}
    s = p_{\text{True}}/(p_{\text{True}} + p_{\text{False}})\label{eq:spl}
\end{equation}

In our experimental setup, we uniformly sample $n=128$ frames from the video and obtain the spatial pseudo labels for each frame independently. Let $s_i$ denote the score for the $i^\text{th}$ frame, we normalize the score vector as $ \nicefrac{s_i}{\max_j s_j}$. Figure~\ref{fig:pseduo_label} (a) shows the pipeline.

\vspace{3pt}
\noindent \textbf{Temporal Pseudo Labels} A significant limitation of single-frame evaluation is the lack of temporal reasoning. For example, considering the question: ``\textbf{What did the man do after picking up his hat?}'', the video content following the action of picking up the hat is crucial for answering this question. However, when generating the spatial pseudo labels, it only considers one frame without taking care of the temporal context. Therefore, the spatial labels don't know what occurs after the action.

To address this issue, we propose the temporal pseudo labeling. Since most of the publicly available \ac{mllm}s are not able to consume a large number of image tokens, we alter to take advantages of the frame captions and use a \ac{llm} to reason over all captions. Specifically, we first obtain detail captions of all $n$ frames by prompting a \ac{mllm}. Second, we feed the captions of all frames together as well as the question to a strong text-only \ac{llm}. Then the \ac{llm} can temporally reason the helpfulness of all frames.


We find it challenging to generate floating-point scores for an extensive list of frames for an \ac{llm}. Consequently, we ask the model to produce a list of the index of most helpful frames (see appendix Section \ref{supp:sec:prompt} for details). Frames included in this list are assigned a score of 1, while those excluded receive a score of 0. Figure~\ref{fig:pseduo_label} (b) shows the pipeline.

While temporal pseudo labels can capture temporal relations in videos, it may suffer from information loss and model hallucination due to its two-stage evaluation process. Therefore, we combine two methods into the final pseudo-labels by averaging the scores obtained from spatial and temporal pseudo labels.

\vspace{-3pt}
\subsection{Training of the Frame Selector}\label{sec:method.4}
\vspace{-3pt}
We consider a two-stage training procedure. In stage 1, we freeze the pre-trained vision and LLM backbones and train the parameters of the alignment projector $g_a$, the learned score query $X_\text{score}$ and the score projector $g_s$. Figure~\ref{fig:pipe} (b) shows the trainable modules in the red boxes and the frozen modules in the blue boxes. Stage 1 training is optimized over the two tasks alternatively. We use below two tasks:

\begin{itemize}
    \item \textbf{Visual instruction following}  Recall $r$ in Equation~\ref{eq:n-frame} is the generated response from the LLM, the objective is the cross entropy loss between $r$ and the  ground truth response. This tasks trains the projector $g_a$ to aligns the visual features with the pre-trained LLM embedding space.
    \item \textbf{Importance score prediction} Recall $s$ in Equation~\ref{eq:selector} is the importance score for $n$ frames, the objective is the binary cross entropy loss between $s$ and the pseudo labels generated from Section~\ref{sec:method.3}. This task provides a good initialization for the score query and the score projector.
\end{itemize}

In stage 2, we only train the model with the importance score prediction task. Besides the alignment projector $g_a$, the learned score query $q$ and the score projector $g_s$, we also include the \ac{lora} weights of the \ac{llm} as the trainable parameters to adapt the \ac{llm} to the frame selection task.



