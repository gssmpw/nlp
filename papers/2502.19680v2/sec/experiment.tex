\vspace{-6pt}
\section{Experiments}
\vspace{-3pt}

We begin by outlining our experimental setup in Section~\ref{sec:exp.1}. Then we demonstrate that our frame selector improves the performance of well-trained \ac{mllms} without changing their parameters by selecting better frames in Section~\ref{sec:exp.2}. We also conduct ablation studies to demonstrate the effectiveness and efficiency of our frame selection framework and results are in Section~\ref{sec:exp.3}. At the end, we showcase some qualitative examples of frames selected from the video according to the question in Section~\ref{sec:exp.4}.

\vspace{-3pt}
\subsection{Experiment Setup}~\label{sec:exp.1}
\vspace{-15pt}

\noindent\textbf{Training data} We compile the training dataset from three sources: 1) 800K data from VideoChat2~\citep{li2024mvbench}, 2) 125K data from the TimeIT dataset~\citep{ren2024timechat}, and 3) 178K data from the LLaVA-Video-178K dataset~\citep{zhang2024videoinstructiontuningsynthetic}. For the visual instruction tuning task, we use the entire training dataset. For the importance score prediction task, we use 400K video QA data where the video length exceeds 5 seconds.
\noindent\textbf{Pseudo label generation} we utilize Qwen2-VL-7B~\citep{Qwen2VL} to generate importance scores for each frame in the  spatial pseudo labeling and concise captions for all frames in the temporal pseudo labeling. We use GPT-4o mini to propose the most helpful frames given the concise captions in the multi-frame evaluation.
\noindent\textbf{Evaluation benchmarks} Since our framework selects frames for video question answering, we evaluated its performance on benchmarks consisting of relatively longer videos, including open-ended video QA on ActivityNet-QA~\citep{yu2019activitynet}, and multi-choice QA on NExT-QA~\citep{xiao2021next}, VideoMME~\citep{fu2024video}, EgoSchema~\citep{mangalam2023egoschema}, LongVideoBench~\citep{wu2024longvideobench}.

\noindent \textbf{Implementation details} We use the pre-trained SigLIP ViT-Large~\citep{zhai2023sigmoid} as the visual encoder and Qwen2.5 1.5B~\citep{qwen2.5} as the backbone of the \ac{llm}. We uniformly sample 128 frames from the video as the input for the visual encoder. The output from the visual encoder have a size of $128 \times 16 \times 16$. After the alignment projector and the spatial pooling layer, the size of the visual tokens is $128 \times 3 \times 3$. In Stage 1, we use a batch size of 128, a learning rate of $10^{-3}$, and a warm-up ratio of 0.03 to train the model for two epochs. In Stage 2, a batch size of 128, a learning rate of $10^{-5}$, a warm-up ratio of the first 3\% iterations, and a cosine learning rate scheduler are used to train the model for five epochs. 

\begin{table}[htbp]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lrr}
        \toprule
        Model          & Model Size &ActivityNet-QA \\ \midrule
        Video-ChatGPT \citep{Maaz2023VideoChatGPT}& 7B   & 35.2 / 2.8  \\
        Chat-UniVi~\citep{jin2024chat}& 7B   &     46.1 / 3.3  \\
        LLaMA-VID~\citep{li2025llama}& 7B   &      47.4 / 3.3  \\
        LLaMA-VID~\citep{li2025llama} & 13B  &      47.5 / 3.3 \\
        Video-LLaVA \citep{lin2023video} & 7B   &    45.3 / 3.3  \\
        MiniGPT4-Video~\citep{ataallah2024minigpt4}& 7B   &  46.3 / 3.4   \\
        SlowFast-LLaVA~\citep{xu2024slowfast}& 7B   &    55.5 / 3.4  \\
        SlowFast-LLaVA~\citep{xu2024slowfast}& 34B  &     59.2 / 3.5 \\
        Tarsier~\citep{wang2024tarsier}& 7B   & 59.5 / 3.6\\
        Tarsier~\citep{wang2024tarsier}& 34B  & 61.6 / 3.7  \\
        PLLaVA~\cite{xu2024pllava}         & 7B   & 56.3 / 3.5 \\
        PLLaVA~\cite{xu2024pllava}         & 34B  & 60.9 / 3.7  \\
        LLaVA-NeXT-Video~\citep{zhang2024llavanextvideo} & 7B        &  53.5 / 3.2   \\
        LLaVA-NeXT-Video~\citep{zhang2024llavanextvideo} & 34B        &  58.8 / 3.4   \\
        \midrule
        PLLaVA + Selector & 7B + 1.5B &  57.6 (1.3$\uparrow$) / 3.5  \\
        PLLaVA + Selector & 34B + 1.5B & 62.3 (1.4$\uparrow$) / 3.6  \\
        LLaVA-NeXT-Video + Selector & 7B + 1.5B &  55.1 (1.6$\uparrow$) / 3.4\\
        LLaVA-NeXT-Video + Selector & 34B + 1.5B & 60.2 (1.4$\uparrow$) / 3.5  \\
        \bottomrule
        \end{tabular}
    }
    \vspace{-6pt}
    \caption{Comparison of open-ended question answering evaluation on ActivityNet QA. Results with the ``\textbf{+ Selector}'' are ours.}
    \label{tab:anetqa}
    \vspace{-9pt}
\end{table}

\vspace{-3pt}
\subsection{Comparison with SOTA Video-LLMs}~\label{sec:exp.2}
We choose two strong video \ac{mllm}s, PLLaVA~\citep{xu2024pllava} and LLaVA-NeXT-video~\citep{zhang2024llavanextvideo} and two (multi-)image based \ac{mllm} Idefics~\citep{laurenccon2024matters} and Qwen2-VL~\citep{Qwen2VL}, to be the baselines to illustrate how our frame selector enhances the video question-answering (QA) performance of these models. For each model, we compare the performance of using uniformly sampled frames as inputs versus using frames selected by our frame selector. The number of frames seen by the video \ac{mllm} is the same during the comparison.

\begin{table}[htbp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lrr}
\toprule
Model          & Model Size & NExT-QA\\ \midrule
SlowFast-LLaVA~\citep{xu2024slowfast}& 7B   &    64.2 \\
SlowFast-LLaVA~\citep{xu2024slowfast}& 34B  &     72.0 \\
Tarsier~\citep{wang2024tarsier}& 7B   & 71.6 \\
Tarsier~\citep{wang2024tarsier}& 34B  & 79.2 \\
LLaVA-NeXT-Video~\citep{zhang2024llavanextvideo} & 7B        &  62.4 \\
LLaVA-NeXT-Video~\citep{zhang2024llavanextvideo} & 34B   & 68.1\\ 
Idefics2~\citep{laurenccon2024matters} & 8B & 68.0 \\
Qwen2-VL~\cite{Qwen2VL} & 7B  & 77.6   \\\midrule
LLaVA-NeXT-Video + Selector & 7B + 1.5B &  63.4 (1.0$\uparrow$) \\
LLaVA-NeXT-Video + Selector & 34B + 1.5B & 69.3 (1.2$\uparrow$) \\
Idefics2 + Selector  & 8B + 1.5B & 69.1 (1.1$\uparrow$) \\
Qwen2-VL + Selector & 7B + 1.5B  &78.4 (0.8$\uparrow$)    \\
\bottomrule
\end{tabular}
}
\vspace{-6pt}
\caption{Comparison of multi-choice question answering evaluation on NExT-QA. Results with the ``\textbf{+ Selector}'' are ours.}
\vspace{-10pt}
\label{tab:nextqa}
\end{table}

\begin{table}[htbp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lrr}
\toprule
Model          & Model Size &  EgoSchema\\ \midrule
SlowFast-LLaVA~\citep{xu2024slowfast}& 7B   &    47.2 \\
SlowFast-LLaVA~\citep{xu2024slowfast}& 34B  &      55.8 \\
Tarsier~\citep{wang2024tarsier}& 7B   & 56 \\
Tarsier~\citep{wang2024tarsier}& 34B  & 68.6 \\
LLaVA-NeXT-Video~\citep{zhang2024llavanextvideo} & 7B        &  45.8 \\
LLaVA-NeXT-Video~\citep{zhang2024llavanextvideo} & 34B   &  48.6 \\ 
Idefics2~\citep{laurenccon2024matters} & 8B& 56.6 \\
Qwen2-VL~\cite{Qwen2VL} & 7B  &  64.6  \\\midrule
LLaVA-NeXT-Video + Selector & 7B + 1.5B  & 47.2 (1.3$\uparrow$) \\
LLaVA-NeXT-Video + Selector & 34B + 1.5B& 50.6 (2.0$\uparrow$)  \\
Idefics2 + Selector  & 8B + 1.5B & 57.9 (1.3$\uparrow$) \\
Qwen2-VL + Selector & 7B + 1.5B & 65.9 (1.1$\uparrow$)   \\
\bottomrule
\end{tabular}
}
\vspace{-6pt}
\caption{Comparison of multi-choice question answering evaluation on EgoSchema. Results with the ``\textbf{+ Selector}'' are ours.}
\label{tab:egoscheaeq}
\end{table}

\begin{table}[htbp]
    \centering
    \small\addtolength{\tabcolsep}{-3pt}
    \begin{tabular}{@{}lllll@{}}
    \toprule
    Model            & short & medium & long & average  \\ \midrule
    Qwen2-VL (baseline) & 69.1  & 53.0   & 51.6 & 58.1 \\
    Qwen2-VL + our selector & 69.6 & 54.1 & 51.9 & 58.7 (0.6$\uparrow$) \\ \bottomrule
    \end{tabular}
    \vspace{-6pt}
    \caption{Performances on VideoMME. \textbf{``+ Selector''} are ours.}
    \vspace{-12pt}
    \label{tab:rebuttal_longvideo_result}
\end{table}

Table~\ref{tab:anetqa}, Table~\ref{tab:nextqa}, Table~\ref{tab:egoscheaeq} and Table~\ref{tab:rebuttal_longvideo_result} present the comparison on ActivityNet-QA~\citep{yu2019activitynet}, NExT-QA~\citep{xiao2021next} and EgoSchema~\citep{mangalam2023egoschema}, VideoMME~\citep{fu2024video} respectively. Following existing practice~\citep{Maaz2023VideoChatGPT}, performance on ActivityNet-QA is measured as ``accuracy/correctness'' metrics, and both metrics are evaluated using GPT-3.5, with higher values indicating better performance. Performances on NExT-QA and EgoSchema are the accuracy of of multi-choice questions where each question has 5 options. We prefill the word ``Option'' as the initial token to generate, and then use the next generated token as the prediction result.

\subsection{Ablation Studies}~\label{sec:exp.3}
We conduct ablation studies on several components of the frame selection framework. First, we analyze the performance of the following frame selection methods in the video QA tasks:

\begin{itemize}
    \item \textbf{Uniform sampling}: the default uniform sampling.
    \item \textbf{Pseudo labels from CLIP similarity}: define the importance score of a frame as the image-text similarity between the frame and the text question, computed using a CLIP model. Then sample frames using Algorithm~\ref{alg:1}.
    \item \textbf{Pseudo labels from SeViLA}: define the importance score of a frame as in SeViLA~\citep{yu2024self} to sample frames.
    \item \textbf{Spatial pseudo labels}: define the importance score using the spatial pseudo labels only.
    \item \textbf{Spatial \& temporal pseudo}: define the importance score as the average of the spatial and temporal pseudo labels.
    \item \textbf{Trained selector}: define the importance score of a frame using the output of our trained frame selector.
\end{itemize}

\noindent\textbf{Not using video-LLM due to worse pseudo-label quality.} We refer to \textit{Table 3} in \cite{fang2024mmbench} for justification. In the temporal reasoning tasks, although video-LLMs are trained with frames sampled from the same video, they fall behind image-trained M-LLMs. It implies image-trained M-LLMs could lead to better temporal pseudo-labels for our method. We tried to use LLaVA-NeXT-Video to generate pseudo-labels. It is more time-consuming but worse pseudo-labels quality. On NExT-QA, using frames labeled by Qwen2-VL and LLaVA-NeXT-Video are 63.9\% and 62.8\% respectively.

\begin{table}[htbp]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lrr}
    \hline
    Selection Method & ANet-QA  &  NExT-QA \\
    \hline
    Uniform sampling                & 53.5 & 62.4 \\
    Scores from CLIP similarity     & 53.7 & 62.2 \\
    Pseudo labels from SeViLA       & 54.0 & 63.2 \\
    Spatial pseudo labels & 54.2 & 63.6 \\
    Spatial \& temporal pseudo  & 55.5 & 63.9 \\
    Scores from trained selector    & 55.1 & 63.4 \\
    \hline
    \end{tabular}
}
\vspace{-6pt}
\caption{Performance of LLaVA-NeXT-Video 7B on ActivityNet (ANet) and NExT QA with different frame selection methods}
\vspace{-6pt}
\label{tab:ab2}
\end{table}

Table~\ref{tab:ab2} presents the video QA performance of LLaVA-NeXT-Video 7B on two benchmarks, respectively. \textbf{Uniform} and \textbf{CLIP} sampling serve as simple baselines for frame selection, while other methods utilize \ac{mllm} reasoning. Both \textbf{SeViLA} and our \textbf{Spatial} are single-frame-based selection methods, with the latter achieving superior performance due to enhanced reasoning during multimodal LLM inference. \textbf{Spatial \& Temporal} further improves upon \textbf{Spatial}, demonstrating the importance of temporal reasoning in frame selection. However, the computational cost to generate such pseudo labels are extremely high as it needs to prompt an \ac{mllm} densely. The light-weight selector's performance matches the performance of frame selection using pseudo labels, validating the effectiveness of the selector architecture. 


\begin{table}[]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
\# frames          & Acc@C & Acc@T & Acc@D & Acc  & Speed (s) \\ \midrule
$4$                & 67.2  & 61.2  &  73.9 & 66.4 & 0.56  \\
$8$                & 68.7  & 62.5  &  76.9 & 68.1 & 0.92  \\
$16$               & 69.1  & 63.6  &  76.8 & 68.7 & 1.71  \\
$32$               & 69.5  & 64.3  &  78.4 & 69.3 & 3.40  \\ \midrule
$128\rightarrow4$  & 68.5  & 64.5  &  75.7 & 68.5 & 0.76  \\
$128\rightarrow8$  & 69.3  & 64.9  &  77.5 & 69.3 & 1.12  \\
$128\rightarrow16$ & 69.4  & 64.8  &  78.5 & 69.5 & 1.91  \\
$128\rightarrow32$ & 69.2  & 65.6  &  78.7 & 69.6 & 3.50  \\ \bottomrule
\end{tabular}}
\vspace{-6pt}
\caption{Performance and inference speed of LLaVA-NeXT-Video 34B on NExT-QA with different number of input frames. First 4 rows: uniform sampling, last 4 rows: sampling using the selector.}
\label{tab:ab3}
\vspace{-9pt}
\end{table}


We further show that the video QA system can use fewer frames to reason videos with the help of our frame selector. Table~\ref{tab:ab3} shows the performance of LLaVA-NeXT-Video on NExT-QA taking different number of frames as input and the inference speed of LLaVA-NeXT-Video with different number of input frames. The inference speed was measured with float16 precision using a batch size of 1 on a single A100 GPU, employing the Hugging Face implementation. When taking the same number of frames as input, our framework incurs additional inference costs to select frames using the \ac{mllm} selector. However, this increase in inference time is not significant thanks to the efficient design of the frame selector. Moreover, the frames selected by the \ac{mllm} selector are more useful for answering the question. Therefore, \textbf{the model using a selector with ${n}$-frame input can achieve similar video QA performance as the model without a selector using ${2n}$-frame input}. For example, the configuration $128\rightarrow4$ outperforms 8-frame uniform sampling with a faster inference speed.

\begin{table}[htbp]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccc}
    \toprule
    \# tokens / frame  & ActivityNet-QA  &  NExT-QA & EgoSchema\\
    \midrule
    no selector & 53.5 & 62.4 & 45.8 \\
    1     & 53.2 & 62.7 & 46.6\\
    9     & 55.1 & 63.4 & 47.2\\
    25    & 55.3 & 63.6 & 47.3\\
    \bottomrule
    \end{tabular}
    }
    \vspace{-3pt}
    \caption{Performance of \textbf{LLaVA-NeXT-Video 7B} with different number of tokens per frame used in the selector.}
    \label{tab:ab5.1}
    \vspace{-8pt}
\end{table}

\begin{table}[htbp]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccc}
    \toprule
    Backbone size & ActivityNet-QA  &  NExT-QA & EgoSchema\\
    \hline
    no selector & 53.5 & 62.4 & 45.8 \\
    0.5 B  & 53.8 & 62.8  & 46.4\\
    1.5 B  & 55.1 & 63.4  & 47.2\\
    7 B    & 55.5 & 64.0  & 47.9\\
    \bottomrule
    \end{tabular}
    }
    \vspace{-3pt}
    \caption{Performance of LLaVA-NeXT-Video 7B with different size of the selector's base LLM.}
    \label{tab:ab5.2}
    \vspace{-6pt}
\end{table}

As discussed in Section~\ref{sec:method.2}, one advantage of our framework is that the frame selector features a lightweight design. We examines two key hyperparameters that influence the computational efficiency: the number 
of tokens to represent a frame and the size of the base LLM. By default, we use Qwen2.5 1.5b as the LLM backbone and use 9 tokens for video frame representation. Table~\ref{tab:ab5.1} and Tabel~\ref{tab:ab5.2} present the ablation studies examining these two factors. ``no selector'' indicates sample the video frames uniformly.

We further demonstrate the effectiveness of our frame selector for long video question answering (QA). LongVideoBench~\citep{weng2025longvlm} is a recently introduced benchmark for long-context video-language understanding, with an average video duration of 473 seconds. In Table~\ref{abl:long}, we report the performance of LLaVA-Next-Video 34B and Qwen2-VL 7B with different numbers of input frames, using uniform sampling and sampling with our frame selector. For both \ac{mllms}, the performance with $n$ input frames sampled using the selector surpasses that of $2n$ input frames with uniform sampling, demonstrating the effectiveness of the frame selector.

\begin{table}[]
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{@{}llll@{}}
    \toprule
    model                                 & \# frames & Uniform & Selector \\ \midrule
    \multirow{4}{*}{LLaVA-Next Video 34B} & 4         & 45.3    & 49.5     \\
                                          & 8         & 46.9    & 49.9     \\
                                          & 16        & 48.1    & 49.8     \\
                                          & 32        & 49.7    & 50.0     \\ \midrule
    \multirow{4}{*}{Qwen2-VL 7B}          & 4         & 48.0    & 55.0     \\
                                          & 8         & 50.9    & 56.0     \\
                                          & 16        & 53.6    & 56.5     \\
                                          & 32        & 53.3    & 57.0     \\ \bottomrule
    \end{tabular}}
    \caption{Performance of LLaVA-NeXT-Video 34B Qwen2-VL 7B on LongVideoBench  with different input frames.}
    \label{abl:long}
    \vspace{-12pt}
\end{table}

\subsection{Visualization of selected frames}\label{sec:exp.4}
\noindent\textbf{Qualitative Results:} Figure~\ref{fig:visualazation} presents two examples of selected frames from the video conditioned the question. Each question involves two events and thus requires temporal reasoning. Estimating the importance of a single frame is challenging without reference to prior or subsequent frames. The frame selector effectively identifies frames containing the answers to the questions. More results in Appendix Section \ref{supp:sec:viz} .

\noindent\textbf{Evaluate on Video Grounding Benchmarks:} We compare the moment retrieval performance with SeViLa on QVHighlights. Ours achieves 43.9\% R1@5 and 32.3\% R1@7 while SeViLa achieves 54.5\% R1@5 and 36.5\% R1@7. Although our frame selector has lower performance than SeViLa, it is not specifically trained on QVHighlights. In other words, the comparable performance proves the overall correctness of the frame selection.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{sec/figures/visual/v1.pdf}
    \includegraphics[width=\linewidth]{sec/figures/visual/v4.pdf}
    \vspace{-9pt}
    \caption{Visualization of the frame selection results.}
    \vspace{-12pt}
    \label{fig:visualazation}
\end{figure}

