\vspace{-9pt}
\section{Introduction}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.95\linewidth]{sec/figures/figure1.pdf}
    \caption{An example of our video frame selection for video QA. Compared to uniform sampling, ours has higher hit rate.}
    \label{fig:head}
    \vspace{-12pt}
\end{figure}

\acf{llm} has revolutionized numerous domains in \acf{ai}~\citep{openai2022chatgpt, claude_technical_report, team2023gemini, touvron2023llama2}. In the past year, \acf{mllm} has significantly improved the performances of \acf{vlm} to an unprecedented level in tasks such as image captioning and \acf{vqa}~\citep{alayrac2022flamingo, li2024llava, ataallah2024minigpt4, Qwen2VL}. To extend \ac{vqa} to the temporal domain, video QA task requires a model to understand consecutive images along the time to answer a question, raising new challenges to the current \ac{mllm}. One of the most critical challenges is the length of the video context, where a model needs to comprehend all frames in a video. 


To balance the trade-off between the capability of understanding all video frames and the available context length in a specific \ac{mllm}, conventional practices~\citep{xu2024pllava, zhang2024llavanext-video} rely on uniform sampling of frames. Frames are extracted at pre-defined intervals regardless of their relevance to the specific questions. Although such methods maximize the coverage of a video in time-axis, it introduces insufficient visual information. The uniformly sampled frames may be irrelevant or redundant, meanwhile some important frames are ignored. This fact prevents the model from focusing on key events in the video and sometimes increases the computational costs. Generally in video QA~\citep{yu2019activityqa, liu2024llavanext}, some specific frames are more likely to contain information relevant to the question. The conventional one-size-fits-all approach limits both the performance and practicality of \ac{mllms}, especially when working with long videos or resource-limited environments.

To address these limitations, a straightforward idea is to select frames instead of uniform sampling~\citep{yu2024self, park2024too}. Focusing on the frames that are most helpful for the question, we can use significantly less number of frames without sacrificing the quality of video understanding. Illustrated as an example in Figure \ref{fig:head}, frames that contains the most informative visuals can help downstream model to answer the question. To implement the above idea, we propose a light weight frame selector that employs fine-tuned version of an \ac{llm}. It makes use of \ac{mllm}â€™s capability of multi-modality understanding to effectively capture the relevance between video frames and the question. We introduce two design choices to make the framework lightweight: (i) small \ac{llm}s are capable to understand complicated user questions; (ii) compress per video frame tokens to balance the long context trade-off. 

Although video QA tasks often require a large number of tokens per frame to capture content details, we hypothesize that determining frame importance does not require excessive tokens. Instead of leveraging all visual tokens of the frames, we apply an aggressive pooling-based token reduction on each video frame, which significantly improves the computational efficiency and increases the number of frames that the lightweight \ac{llm} selector can ingest.

%Besides the advantage of computation cost,
We also propose a training method for our \ac{llm}-based video frame selector. Since well-maintained video frame selection datasets for video QA are scarce, supervised training alone is not feasible. To address this issue, we adopt two pseudo-labeling strategies to estimate frame importance. The first strategy focuses on spatial understanding: we prompt a well-trained \ac{mllm} using a video QA question, which then assigns importance scores to each frame. However, due to the limited context length of a \ac{mllm}, it cannot evaluate all frames together from a temporal viewpoint. Therefore, our second strategy leverages an \ac{llm} to identify the top-k relevant frames using their captions. Instead of visual tokens, the \ac{llm} can analyze more frames simultaneously with caption. We integrate these two approaches to generate pseudo-labels that reflect frame importance relative to a specific question for effective training of the frame selector.

Our frame-selector employs a plug-and-play design that requires no additional fine-tuning of the downstream \ac{mllm}. It reduces noise caused by irrelevant frames, allowing the video-LLM to focus more effectively on the relevant content. Additionally, the selector only needs to be trained once and can subsequently enhance the video QA performance of multiple \ac{mllm}s. We demonstrate significant improvements with several popular \ac{mllms} on video question-answering tasks across various benchmarks, including short-to-medium context (ActivityNet, NExt-QA) and long context (EgoSchema, VideoMME) scenarios. In summary, our contributions are threefold:
\begin{itemize}
    \item We propose a lightweight \ac{mllm}-based adaptive video frame selector to for both efficient and stronger video QA performances of \ac{mllms}.
    \item We propose spatial and temporal pseudo-labeling to generate importance scores for video frame selector training.
    \item Our proposed method is plug-and-play friendly. We demonstrates comprehensive video QA improvements across popular \ac{mllms} with further fine-tuning.
\end{itemize}