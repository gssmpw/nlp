\section*{Limitations}

\paragraph{Scope of the Dataset Search and Number of Annotators}
While we conducted a comprehensive and systematic search for datasets, we acknowledge that further datasets may be viewed as relevant that we did not cover. For example, the concepts of offensive or toxic language overlap with uncivil communication \cite{pachinger-etal-2023-toward}; or fallacies in propaganda detection. Moreover, while we believe that the validation on the 10\% sub-sample demonstrates the reliability of AQ annotations, carried out by a single reviewer on the remaining datasets, we cannot exclude the impact of subjectivity and potential errors. For both cases (scope of the datasets collected and potential categorization errors), however, we believe that the fact that the collection will be publicly available in form of a website will allow authors to reach out to us for updates.

\paragraph{Assumed Fit of the Selected Taxonomies to the Whole Dataset Collection} To categorize existing datasets, we have selected and applied two specific taxonomies \cite{wachsmuth-etal-2017-computational, friess2015deliberation}, one for argument quality and one for deliberation quality. While our motivations for this choice are strong, as discussed in Section \ref{sec:relatedwork}, we cannot in principle exclude that a different categorization would have had a better fit to the papers we collected. We believe, however, that our categorization approach, strictly based on consulting descriptions provided in papers and annotation guidelines (and contacting the authors directly when guidelines were not available) alleviates this limitation.

\paragraph{Disagreements: Between Valid Human Label Variation and Annotation Errors}
Disagreement among annotators can arise from various factors, among them subjectivity, but also annotation errors and ambiguity in the items to be labeled. Working with non-aggregated datasets thus always comes with the question of annotation reliability and how to distinguish potential annotation errors from valid label variations. First approaches are being developed to eventually complement perspectivist machine learning workflows \cite{weber-genzel-etal-2024-varierr}. Disentangling the different types of disagreements can improve data quality, which would not only benefit the perspectivist turn, but also the well-established approach of aggregated ground-truth. In both cases, the release of non-aggregated annotations is crucial.

While our pilot experiments in Section \ref{sec:pilot} exemplify the general consequences of pronounced differences in annotations across and within annotator groups, in this paper we do not specifically tackle the question of how much of these differences can be attributed to annotation error and how much to legitimately varied, subjective labeling decisions. However, the low to moderate IAA, even among the Dagstuhl experts --- who themselves developed the underlying taxonomy --- clearly indicates a significant degree of subjectivity in AQ perception.