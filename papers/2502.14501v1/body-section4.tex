\section{Datasets: Annotations \& Annotators}
\label{sec:analysis}

The 98 identified papers introduce 103 AQ datasets in total. A complete list including the mapped AQ categories is in Appendix \ref{sec:relevant-datasets}. In what follows, we provide an overview of the quantitative properties arising from the comparison of their \textit{annotation}, and we then focus on  \textit{annotator} meta-information.

\subsection{Annotations: What Argument Quality?}

Figure \ref{fig:overview-aq} shows the distribution of datasets among the categories of AQ. Particular interest can be observed for the rhetorical effectiveness of arguments, likely driven by its practical relevance and the availability of pre-annotated resources, such as the Reddit forum ChangeMyView and other online debate platforms where users rate the persuasiveness of each other's arguments. Increased attention has also been paid to the logical validity of arguments. However, the two dialectically-driven dimensions of reasonableness and deliberative norms received less attention, a finding that coincides with the CA community's constant call for a greater focus on dialogical argumentation \cite{ruiz-dolz-etal-2024-overview}.

\begin{figure}[t]
  \includegraphics[scale=1]{stacked-bar-chart}
  \caption{Frequency and distribution of AQ categories (major and sub-categories) as assigned to datasets, grouped by the four major categories and overall AQ.}
  \label{fig:overview-aq}
\end{figure}

Looking into the individual sub-categories of AQ, we find that almost all of them are covered. The only exception is \emph{equality}. However, we acknowledge that measuring whether participants have equal opportunities in a deliberation is challenging, as it extends beyond simply assessing active participation to the potential for participation depending on the socioeconomic capital that the participants hold \cite{friess2015deliberation}.

\begin{table}[t]
\centering
\small\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{2pt}
\scriptsize
\begin{tabular}{P{2cm}p{5.4cm}}
\toprule
\bf Meta-Categories & \bf Specifications (Counts, Sorted Descending)\\
\midrule
Genre	&  social media (20), online debate portal	(19), persuasive essays	(12), crowd-sourced	(10), public participation (8), news articles (7), political debate	(6), web (5), collaborative online discussions (5), news comments (4), reviews (3), educational debate (2), fact-checking portals (2), QA forums (2), e-mail communication (2), Wikipedia (2), online educational material (1), classroom discussions (1), business model pitches (1), LLM-sourced (1) \\
Modality & text (100), multimodal (3)\\
Language & en (89), de (13), fr (4), jp (2), es (1), it (1), nl (1), pt-br (1), zh (1)\\
\midrule
Manual annotation & manual (82), automatic (18), manual+automatic (3)\\
Selection method of annotators & students/available	(22), consistency with experts (18), expertise (16), language competence (16), reliability checks (11), performance in prior tasks (8), educational level (5), balanced sample wrt. to some property (5), consistency with fellow annotators (3)\\
In-house annotators or crowd-workers & in-house (39: experts 13, novice 11, mixed 5, n.a. 10), crowd (27; task expertise unknown), in-house+crowd (5: in-house experts 4, in-house mixed 1), n.a. (14)\\
Annotator attributes (across in-house and crowd-sourcing  datasets) & no indication at all (45); education (25), age (18), native language (14), gender (11), profession (9), professional background (8), stance (5), country of origin (4), country of residence (3), occupation (3), political view (3), nationality (2), personality traits (2), annotation time (1), civic engagement (1), competence (1), employment status (1), ethnicity (1), income (1), race (1), religion (1),  role (1), spirituality (1)\\
\bottomrule
\end{tabular}
\caption{Counts of specifications for different meta-categories on datasets (top) and annotations (bottom).}
\label{tab:dataset-stats}
\end{table}

The upper part of Table \ref{tab:dataset-stats} provides an overview of selected dataset properties. In terms of genre, the datasets cover a great variety, with social media, online debate portals, and persuasive essays being the most prominent. Seven datasets draw from multiple genres \cite{xu2014identifying, napoles-etal-2017-finding, lauscher-etal-2020-rhetoric, ziegenbein-etal-2023-modeling, falk-lapesa-2023-storyarg, helwe-etal-2024-mafalda, li-etal-2024-side}. While most of the datasets focus solely on text, three are multimodal \cite{liu-etal-2022-imagearg, liu-etal-2023-overview, mancini-etal-2024-multimodal}. With respect to languages, we observe a very imbalanced situation with English accounting for over 85\% of the datasets. Four of the datasets contain multiple languages \cite{Gerber_Baechtiger_Shikano_Reber_Rohr_2018, toledo-ronen-etal-2020-multilingual, falk-lapesa-2022-scaling, reveilhac2023comparing}.


\subsection{Annotators: Whose Perspectives?}
\label{sec:perceptions}

We now take a closer look at the individuals that provide the AQ assessments, in order to understand whose perspectives current datasets cover. Table~\ref{tab:dataset-stats} shows the statistics on manual annotation, annotator selection and attributes. The majority of datasets were created through coordinated manual data annotation; fewer than 20\% of datasets were generated automatically by parsing existing internet resources, with the ground truth labels derived from a natural sample of platform users.

In manual annotation, authors indicated a variety of reasons for the selection of annotators, among them predominantly consistency with experts, expertise of the annotators themselves, and language competence. Students were also a common choice (in one quarter of the datasets), which might be an indicator for selection upon availability. Only five datasets had annotators selected with the aim of weighting socio-demographic characteristics according to certain standards, such as the representation of a country's population \cite{lukin-etal-2017-argument, brenneis-etal-2021-will}, balancing political ideologies or gender \cite{el-baff-etal-2018-challenge, falk-etal-2024-moderation}, and annotators from diverse debating circuits  \cite{joshi-etal-2023-arganalysis35k}. Also noteworthy is that in three datasets, annotators were excluded if inconsistent with fellow annotators' label decisions.

Looking more closely at the socio-demographic background of AQ annotators, we find that authors only occasionally provide information (in the papers or datasets). In case of \textit{in-house annotators}, we find a higher education in all cases indicated and often a background in NLP and related fields. A key differentiator is expertise in AQ, which separates in-house annotators into two groups: \textit{experts} and \textit{novice} annotators (usually students). Additionally, we find seven explicit mentions of gender (two datasets include both binary genders without specifying proportions, two use balanced samples, two have significantly more male annotators, and one includes two female and one male annotator). Age was reported twice, with ranges of 18--53 and 18--22 years. In case of \textit{annotators recruited on crowd-sourcing platforms}, socio-demographic information is reported sparsely, only for 8 datasets. In these cases, it is either used to draw a more representative sample or serves to narrow down the selection of annotators to the language of data.

On a more general note, characteristics that may invoke some bias in assessment such as political views (and related stances) were rarely collected, and there is similarly little information on cultural diversity among annotators. Individual characteristics that go beyond socio-demographic features are hardly at issue, except from \citet{lukin-etal-2017-argument} and \citet{el-baff-etal-2018-challenge} who collect personality traits, recognizing the potential impact on AQ perception.
