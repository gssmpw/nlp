\section{Towards Perspectivist Argument Quality Assessment}
\label{section-towards_perspectivist-quality-assessment}

\begin{table*}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{0.90}
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{p{0.31\linewidth}@{}rrrrp{0.43\linewidth}}
\toprule
\bf Dataset & \bf Size & \bf Per-item & \bf Total & \bf Category & \bf Annotators' Attributes Provided in Dataset\\ 
\midrule
\multicolumn{6}{c}{\em Introduced as non-aggregated to facilitate perspectivist machine learning or to promote diversity in annotations}\\
\midrule
CrowDEA Ideas \cite{Baba_Li_Kashima_2020} & 16,000 & 20 & 257 & crowd & - \\
Argument Concreteness \cite{romberg-etal-2022-corpus} & 1,127 & 5 & 5 & novice & - \\
TYPIC \cite{naito-etal-2022-typic} & 197 & 1--2 & 4 & in-house & - \\
Argument Validity Novelty \cite{heinisch-etal-2023-architectural} & 1,474 & 3 & 5 & expert & - \\
MAFALDA \cite{helwe-etal-2024-mafalda} & 268 & 1--4 & 4 & expert & - \\
UMOD \cite{falk-etal-2024-moderation} & 1,000 & 9 & 90 & crowd & race, gender, age, annotation time, role, competence, stance\\
\midrule
\multicolumn{6}{c}{\em Built to explore how argument perception differs between groups and individuals}\\
\midrule
Persuasion \& Personality \cite{lukin-etal-2017-argument} & 100 & 20 & 637 & crowd & personality traits, age, gender, political view, education, civic engagement, religion, spirituality, employment status, income, stance \\
Webis-Editorial-Quality-18 \cite{el-baff-etal-2018-challenge} & 1,000 & 6 & 24 & crowd & political view, personality traits \\
\midrule
\multicolumn{6}{c}{\em Personalization}\\
\midrule
n.a. \cite{hunter2017empirical} & 30 & 50 & 50 & crowd & - \\
SIGIR-19 \cite{potthast2019argument} & 494 & 1 & 40 & in-house & age, gender, stance \\
argumentation-attitude \cite{brenneis-etal-2021-will} & 946 & 1--147 & 674 & crowd & stance \\
\midrule
\multicolumn{6}{c}{\em Aggregated ground truth datasets that were released together with the individual labeling decisions}\\
\midrule
Dagstuhl-ArgQuality \cite{wachsmuth-etal-2017-computational} & 320 & 3 & 3 & expert & - \\
n.a. \cite{wachsmuth-etal-2017-argumentation} & 320 & 10 & 102 & crowd & - \\
n.a. \cite{mirzakhmedova2024reliable} & 320 & $\leq$ 10 & 108 & novice & - \\
GAQCorpus \cite{lauscher-etal-2020-rhetoric} & 5,285 & 1--13 & 27 & exp, crowd & - \\
EuropolisAQ \cite{falk-lapesa-2022-scaling} & 513 & 1--2 & 2 & expert & - \\
ArgQ! \citet{silva2021quality} & 352 & 4 & 4 & expert & - \\
UKPConvArg1 \cite{habernal-gurevych-2016-argument}  & 16,000 & 5 & 3,900 & crowd & stance \\
UKPConvArg2 \cite{habernal-gurevych-2016-makes} & 70,000 & 5 & 776 & crowd & - \\
Essay Argument Organization \cite{persing-etal-2010-modeling} & 1,003 & 1-6 & 6 & novice & - \\
Appropriateness Corpus \cite{ziegenbein-etal-2023-modeling} & 2,191 & 3 & 3 & crowd & - \\
UKP-InsufficientArgs \cite{stab-gurevych-2017-recognizing} & 433 & 3 & 3 & expert & - \\
Webis-ArgRank-17 \cite{wachsmuth-etal-2017-pagerank} & 110 & 7 & 7 & expert & - \\
StoryARG \cite{falk-lapesa-2023-storyarg} & 2,451 & 1--4 & 4 & in-house & - \\
\bottomrule
\end{tabular}
\caption{Overview of AQ datasets that come with non-aggregated annotations. In each case, we provide annotator counts \emph{per-item} and \emph{total}, \emph{categorize} them as in-house (experts, novice, or in-house; if expertise is unspecified) or crowd workers, and specify the \emph{annotators' attributes} contained directly in the datasets at the individual level.}
\label{tab:dataset_sizes}
\end{table*}

Developing perspectivist models requires the existence of multiple assessment perspectives. Among the 103 datasets we found, only 24 come with non-aggregated annotations. We detail these datasets, before we exemplify the potential impact of annotator groups on AQ assessment.

\subsection{Non-Aggregated Datasets}
\label{sec:non-aggr-datasets}

Table \ref{tab:dataset_sizes} lists the datasets with properties relevant to perspectivist model development. We identify four conceptual blocks: Six datasets were exclusively introduced as {non-aggregated for perspectivist approaches or to promote annotation diversity}. Two were developed to study {how argument perception varies based on group-level or individual characteristics}. Three stem from {personalization in argument retrieval}, and 13 are {aggregated datasets released together with the individual labeling decisions}. An extensive description of all 24 datasets is provided in Appendix~\ref{sec:description-non-aggr-datasets}. Here, we focus on those that we deem most useful for the perspectivist turn.

\paragraph{Populations}

For developing well-generalizable models, it is integral that the datasets represent a specific population, whose composition of perspectives can be learned. The annotations of four datasets were collected in a controlled setup in this regard: The \textit{Persuasion \& Personality} corpus \cite{lukin-etal-2017-argument} was created to study differences in the perception of argument effectiveness. Stance changes elicited by social media arguments were recorded from a representative sample of the US population. The \textit{argumentation-attitude} dataset \cite{brenneis-etal-2021-will} covers personalized views of strong arguments from a political opinion platform, rated by a representative sample of the German online population, in terms of age, gender, and education. \textit{Webis-Editorial-Quality-18} \cite{el-baff-etal-2018-challenge} captures differing perceptions of effectiveness in US news editorials on a balanced sample of liberals and conservatives, \textit{UMOD} \cite{falk-etal-2024-moderation} annotates characteristics of user-driven online moderation (including comment constructiveness), using a gender-balanced population.

Crowd-platform annotations sourced from a sufficiently large group of workers may also be assumed to approximate the broader population from the respective platform to a certain extent: \textit{UKPConvArg1} and \textit{UKPConvArg2} \cite{habernal-gurevych-2016-makes,habernal-gurevych-2016-argument} capture argument convincingness and the AQ reasons behind, with 16k and 70k items, respectively, and over 4k crowd annotators from the US. \textit{CrowDEA Ideas} \cite{Baba_Li_Kashima_2020} contains preference labels of 257 crowd workers for 16k solutions to an issue. These three datasets also stand out in their size; they are the only datasets with tens of thousands of annotated items.

Lastly, we highlight two datasets that bring together different groups of annotators for the same data. The \textit{Dagstuhl-ArgQuality} dataset \cite{wachsmuth-etal-2017-computational} of online debate arguments was rated by experts, novice student annotators (with no prior experience in CA) \cite{mirzakhmedova2024reliable},%
\footnote{The official release includes only a subset of annotators, but the authors kindly provided the full set upon request.} 
and crowd workers \cite{wachsmuth-etal-2017-argumentation} across the 15 dimensions of their taxonomy. The \textit{GAQCorpus} \cite{lauscher-etal-2020-rhetoric}, which includes diverse arguments annotated for cogency, effectiveness, reasonableness, and overall AQ, was annotated by a mix of 3 expert annotators and 24 crowd workers. Some items were annotated only by the crowd, others by the experts, and part of the dataset was jointly annotated by both groups. The different groups can be regarded as different types of populations and thus represent an interesting testing ground for group-specific analysis.

\paragraph{Individuals}

To model the perspectives of individuals more accurately, meta-information about them is needed. Such attributes can also help to build perspectivist models at the group level. The Persuasion \& Personality corpus, the argument-attitude dataset, WEBIS-Editorial-Quality-18, UMOD, and UKPConvArg1 provide several relevant attributes, including socio-demographics, stances on certain topics, and personality traits. In addition, the \textit{SIGIR-19} dataset \cite{potthast2019argument}, which codes logical, rhetorical, and dialectical AQ, includes gender, age, and stance for each annotator.

Besides, two datasets from our overview have already been used successfully in modeling human label variation, the non-aggregated version of the \textit{Argument Validity and Novelty} dataset \cite{heinisch-etal-2023-architectural} and the \textit{Argument Concreteness} corpus \cite{romberg-etal-2022-corpus}. Both lack background information on the annotators, but this limitation was initially secondary to the goal of developing perspectivist models for personalization \cite{heinisch-etal-2023-architectural}.


\subsection{Potential Impact of Annotator Groups}
\label{sec:pilot}

In Section \ref{sec:analysis}, we found that in-house annotators form a relatively homogeneous group concerning education and work background, with expertise being a key differentiator between experts and novices. In contrast, annotators recruited via crowdsourcing platforms can be assumed to represent a much more diverse sample in terms of socio-demographic attributes and lived experiences. We thus study two research questions on the impact of annotator groups on AQ annotation and prediction:
%
\begin{enumerate}[leftmargin=1cm]
\setlength{\itemsep}{0pt}
\item[RQ1.]
How comparable are annotations across annotator groups, how stable within them?
\item[RQ2.] 
How does this impact the performance bounds of models trained on group-specific annotations when transferred across groups?
\end{enumerate}

\paragraph{Data}
We use the two mentioned non-aggregated datasets with a mix of annotator groups: Dagstuhl-ArgQuality \citep{wachsmuth-etal-2017-computational} and its extensions \cite{mirzakhmedova2024reliable, wachsmuth-etal-2017-argumentation} (summarized as \textit{Dagstuhl}; annotated by experts, novice student annotators and crowd-platform workers), and the GAQCorpus \cite{lauscher-etal-2020-rhetoric} (\textit{GAQ}; annotated by experts and crowd-platform workers). For Dagstuhl, we resort to the 304 arguments deemed argumentative in the original corpus. For GAQ, we use the 538 arguments annotated by both experts and crowd annotators. We focus on cogency, effectiveness, reasonableness, and overall AQ as categories.

\paragraph{Experimental Setup}
For RQ1, we compute inter-annotator agreement (IAA) within and across annotator groups, using Krippendorff's~$\alpha$. For a full picture, we include all annotators per group.

To answer RQ2, we assume a situation in which a \textit{perfect model}, trained on the annotations of one group, is evaluated on another group, effectively performing a {population transfer}. We opted for simulation rather than real training of a model in order to minimize confounding factors, such as model deficits due to limited training data. This way, we can clearly illustrate the discrepancy that arises when population characteristics, and the differences in perspectives they encode, are ignored. We investigate the actual upper performance bounds in two scenarios, a traditional \textit{aggregated approach} with a single regression output per argument, and a \textit{perspectivist approach} in which we assume to obtain a learned regression label distribution per argument as the system output.

\begin{table}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{4.2pt}
\begin{tabular}{p{0.5cm}lrrrr}
\toprule
&\bf Group  & \bf Cogency & \bf Effectiveness & \bf Reasonableness & \bf Overall\\
\midrule
\multirow{3}{*}{\parbox{0.6cm}{\textbf{Dag-\\stuhl}}} 
& E & \bf .372 & \bf .314 & \bf .437 & \bf .443 \\
& N & .230 & .208 & .197 & .233 \\
& C & .099 & .107 & .111 & .140 \\
& E, N & .114 & .098 & .134 & .126 \\
& E, C & .129 & .121 & .143 & .180 \\
& N, C& .060 & .072 & .071 & .083 \\
& E, N, C & .083 & .085 & .098 & .115 \\
\midrule
\textbf{GAQ} 
& E & \bf .175 & \bf .272 & \bf .258 & \bf .254 \\
& C & .156 & .148 & .154 & .173 \\
& E, C & .142 & .142 & .150 & .165 \\
\bottomrule
\end{tabular}
\caption{Krippendorff's $\alpha$ for different groups of annotators; experts (E), novice (N), and crowd workers (C).}
\label{tab:iaa}
\end{table}

\begin{table}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{3.1pt}
\begin{tabular}{p{0.5cm}lrrrrrrrr}
\toprule
&& \multicolumn{2}{c}{\bf Cogency} & \multicolumn{2}{c}{\bf  Effectiven.} & \multicolumn{2}{c}{\bf  Reasonab.} & \multicolumn{2}{c}{\bf  Overall} \\ 
\cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}
&\bf  Transfer& \bf MAE &\bf WS& \bf MAE& \bf WS &\bf MAE& \bf WS &\bf MAE & \bf WS \\
\midrule
\multirow{3}{*}{\parbox{0.6cm}{\textbf{Dag-\\stuhl}}} &E, N & .697&  .159 & .714 & .155& .605& .161 & .686 & .158\\
&E, C & .499&.195 & .530&.194 & .463&.184 & .430&.179 \\
&N, C & .507&.164 &.470&.169 & .454&.162 & .480&.163 \\
\midrule
\textbf{GAQ} & E, C &.697& .111&.751& .122& .629& .110 &  .659&\ .109\\
\bottomrule
\end{tabular}
\caption{Group transfer evaluation for the aggregated approach (MAE) and the perspectivist approach (mean WS); experts (E), novice (N), and crowd workers (C).}
\label{tab:cross-group-dist}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[scale=1]{scatter-overall-quality-dagstuhl}
    \caption{Instance-based aggregation of label decisions for overall AQ, assessed on a scale from 1 (low) to 3 (high), between two annotator groups on Dagstuhl, with a fitted linear regression model highlighting their relationship.}
    \label{fig:oq-aggregated}
\end{figure*}

To compare label distributions (i.e., the perfectly predicted one and that of a target population), we calculate the Wasserstein distance (WS) between the label distributions per item. We report the mean across the whole dataset, respectively. For aggregated regression outputs (i.e., mean ratings), we calculate the mean average error (MAE). We report results per dataset and quality dimension.

\paragraph{Results}
Regarding RQ1, Table \ref{tab:iaa} shows the IAA for each annotator group and their different combinations for both corpora. For in-group, across corpora and quality dimensions, we find that expert annotators have the highest agreement, though still comparably low. For Dagstuhl, crowd annotators have the lowest IAA. Across groups, IAAs drop compared to the involved group with the highest agreement for all combinations of groups and both corpora. This indicates varied annotations within groups and high disagreement across groups.

Regarding RQ2, Table \ref{tab:cross-group-dist} shows the results for both the aggregated and the perspectivist approach. For the former, we find that even if a model perfectly learns one group's aggregated annotation behavior, the minimum MAE achievable, in the worst case, is $0.714$ for Dagstuhl and $0.751$ for GAQ (both effectiveness). Figure \ref{fig:oq-aggregated} exemplifies what this means on Dagstuhl for predictions of overall AQ: while transfer from experts to crowd annotators and vice versa retains the same general trend, the other combinations behave effectively at random. This is in line with IAA per group combination.

The perspectivist evaluation, using label distributions, shows that the highest Wasserstein distances occur for cases of group transfer in which the MAE is lower. For both group combinations in Dagstuhl with the lowest IAAs (E,N and N,C), the Wasserstein distances are consistently lower than for E,C. This indicates systematic patterns of disagreement present to a different extent between groups.

In sum, we find considerable annotation variation within and across groups (RQ1), which causes limited transferability between groups (RQ2). 
