\section{Systematic Review of Datasets}
\label{sec:review}

\paragraph{Search Methodology}
We searched in all major publication organs that, according to our experience in the field, cover the topic of CA. We included the leading conferences in computational linguistics (the entire ACL anthology, including the  Argument Mining Workshop), artificial intelligence (all from AAAI.org, IJCAI, and ICAIL), information retrieval (SIGIR and ECIR), and the specialized computational argumentation series COMMA.

To pre-filter a set of candidates, we used all pairs of search terms from \textit{\{argument, argumentation, argumentative, debate, deliberation, deliberative\} $\times$ \{quality, strength, persuasiveness, fallacies\}}. The retrieval was carried out with the Google site search including all papers that had been published until August 20, 2024, resulting in 238 candidate papers of which we found 42 to be relevant. Additionally, we employed a snowballing method \cite{wohlin2014snowballing} to ensure that the field is covered as completely as possible: looking at studies that either cite one of the previously identified papers (forward snowballing, with Google Scholar) or are cited by those papers (backward snowballing) led to further 56 relevant papers. In total, we identify 98 relevant papers, distributed among research communities as follows: NLP (73), artificial intelligence (6), information retrieval (7), CA (1), and further venues from computer science (4) and the social sciences (7). Appendix \ref{sec:review-details} describes the process in detail.

\paragraph{Categorization Taxonomy}
We assess the relevance of datasets by drawing from the taxonomies of \citet{wachsmuth-etal-2017-computational} and \citet{friess2015deliberation}. A paper is considered relevant if it introduces a new dataset (or extends an existing one) that at least loosely matches one or more of the following categories (whose theoretical background has been introduced in Section \ref{subsec:AQ}): i) \textit{logical cogency} with subcategories \textit{local acceptability}, \textit{local relevance}, and \textit{local sufficiency}; ii) \textit{rhetorical effectiveness} with subcategories \textit{clarity}, \textit{appropriateness}, \textit{arrangement}, \textit{credibility}, and \textit{emotional appeal}; iii) \textit{dialectical reasonableness} with subcategories \textit{global acceptability}, \textit{global relevance}, and \textit{global sufficiency}; iv) \textit{deliberative norms} with subcategories \textit{rationality}, \textit{interactivity}, \textit{equality}, \textit{civility}, \textit{common good reference}, \textit{constructiveness}, and \textit{alternative forms of communication}; and \textit{overall argument quality}. Appendix \ref{app:taxonomy} provides the taxonomy and exact definitions in full.

\paragraph{Categorization Reliability}
Mapping of dataset dimensions according to the AQ taxonomy was conducted by the first author of the paper. To validate this process, 10 papers (\textasciitilde 10\%) were reassigned to two other authors. For the high-level categories, we reached Fleiss' $\kappa$ values of $1.0$ for logical cogency, $0.73$ for rhetorical effectiveness, $0.71$ for dialectical reasonableness, and $0.70$ for deliberative norms, demonstrating robust inter-annotator reliability. The mean agreement across all 23 categories was lower, $0.52$. It is worth pointing out, though, that one of the other authors reached $0.72$ with the first author, which is why we deem our categorization to be reasonably reliable. A clear source of disagreement arose from the categorization of fallacies into the taxonomy. Reasoning errors (i.e., fallacies) can affect all dimensions of AQ, and we refined the annotation guidelines accordingly.

\paragraph{Collection of Meta-information}
In addition to the AQ categories, we gathered further information about the datasets. This includes general details such as genre, modality, language, and the availability of the dataset and annotation guidelines. While dataset availability was generally good (84 public or upon request), information on annotation guidelines was less available (32). We contacted authors of datasets without clear indications, encouraging public release in line with open science principles. As a result, 14 additional datasets now have publicly accessible guidelines, for a total of 11 unique guidelines made available.

We also collected a variety of characteristics that are of interest when looking through the perspectivist glasses. Most notable for the paper at hand are meta-information about annotators and the availability of non-aggregated annotations. Appendix~\ref{sec:database-details} lists all information contained in the database.