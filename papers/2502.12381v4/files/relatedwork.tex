\section{Related Work}

In this section, we review recent advances in sequential modeling with a focus on linear methods. We begin with a discussion of linear recurrent architectures and their variants, then move on to transformer linear methods and other efficient attention mechanisms, including very recent approaches. Finally, we highlight how our diffusion-based method combines the strengths of these approaches while overcoming their limitations.

\subsection{Linear RNNs and Their Variants}
Classical recurrent neural networks (RNNs) such as Long Short-Term Memory (LSTM) \cite{hochreiter1997long} and Gated Recurrent Units (GRU) have long been the backbone of sequence modeling. However, their inherently sequential processing limits parallelization and impedes modeling of long-range dependencies. In response, researchers have developed \emph{linear RNNs} that simplify recurrence dynamics to enable efficient parallel computation and alleviate vanishing gradients. Further improvements have been proposed with architectures such as the Independently Recurrent Neural Network (IndRNN) \cite{li2018indrnn}, which enhances stability in deep recurrent networks. While these methods boost computational efficiency, they often lack the capacity to capture the rich nonlinear dynamics observed in complex sequences. 

\subsection{Transformer Linear Methods and Efficient Attention}
Transformers \cite{vaswani2017attention} revolutionized sequential modeling by leveraging self-attention to capture global dependencies. However, the quadratic complexity of full self-attention has spurred the development of more efficient variants. Linear attention methods \cite{katharopoulos2020transformers, choromanski2020rethinking} approximate the attention mechanism to achieve linear complexity, while approaches such as Linformer \cite{wang2020linformer} and Reformer \cite{kitaev2020reformer} use low-rank or reversible mechanisms to further reduce computational overhead. Recent methods like Big Bird \cite{zaheer2020big} and Sparse Transformers \cite{child2019generating} introduce sparsity to scale to longer sequences, and the Nystr√∂mformer \cite{xiong2021nystromformer} employs a sampling-based approach for efficiency. Furthermore, state-of-the-art innovations such as the Routing Transformer \cite{roy2021routing}, FlashAttention \cite{dao2022flashattention}, and CosFormer \cite{zhou2022cosformer} continue to push the envelope on both performance and computational speed. Although these methods offer impressive scalability, they often face trade-offs between capturing smooth temporal evolution and modeling abrupt dynamics. Additionally, \cite{feinashley2025fftstrikesbackefficient} introduced a fast-fourier transform (FFT) based token mixing method that allows for an efficient and effective alternative to costly self-attention.

\subsection{Diffusion-Based Approaches and Our Contributions}
An emerging line of work has begun to explore diffusion-based formulations in sequence modeling. Diffusion-Convolutional Neural Networks \cite{atwood2016diffusion} and Diffusion-Convolutional Recurrent Neural Networks \cite{li2018dcrnn} have demonstrated strong performance in graph-based and spatiotemporal forecasting tasks. Drawing inspiration from spectral graph theory \cite{Chung1997Spectral}, our proposed LDN reinterprets temporal evolution as a diffusion process. By integrating gradual diffusion updates with localized nonlinear transformations, our model achieves both global information propagation and fine-grained local dynamics. Unlike conventional linear RNNs or transformer linear methods, our approach naturally blends the strengths of efficient computation and robust representation learning, resulting in a model that can capture multi-scale temporal dependencies more effectively.

Overall, our diffusion-based framework represents a powerful alternative that unifies the benefits of linear dynamics and global attention, positioning it as a promising solution for complex sequential tasks in modern applications.

