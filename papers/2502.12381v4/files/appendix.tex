
\section{Theory}

In this section, we demonstrate rigorously that the diffusion kernel in LDN captures global dependencies across the input sequence. Under mild conditions on the learnable kernel \(K\), repeated application of the diffusion update leads to an effective mixing, ensuring that every output token is influenced by every input token.

\subsection{Global Dependency via Iterated Diffusion}
For clarity, consider the pure diffusion update, setting aside the local update \(F\) and the linear attention module \(A_{\text{lin}}\). At each layer \(\ell\), the hidden state is updated as:
\[
h_t^{(\ell)} = h_t^{(\ell-1)} + \delta t \cdot \sum_{s=1}^{T} K_{ts}\Bigl( h_s^{(\ell-1)} - h_t^{(\ell-1)} \Bigr).
\]
In matrix form, letting
\[
H^{(\ell)} = \begin{bmatrix} h_1^{(\ell)} \\ h_2^{(\ell)} \\ \vdots \\ h_T^{(\ell)} \end{bmatrix} \quad \text{and} \quad \mathbf{1} = \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix},
\]
this becomes:
\[
H^{(\ell)} = \Bigl(I + \delta t \Bigl(K - \operatorname{diag}(K \mathbf{1})\Bigr)\Bigr) H^{(\ell-1)}.
\]
Defining the effective diffusion operator as:
\[
A \triangleq I + \delta t \Bigl(K - \operatorname{diag}(K \mathbf{1})\Bigr),
\]
after \(L\) layers the process yields:
\[
H^{(L)} = A^L H^{(0)}.
\]

\subsection{The Global Dependency Theorem}

\begin{theorem}[Global Dependency Theorem]
Assume:
\begin{enumerate}
    \item The diffusion kernel \(K\) satisfies \(K_{ts} \geq 0\) for all \(t,s\).
    \item The directed graph \(G(K)\) induced by the nonzero entries of \(K\) is strongly connected; that is, for any two time indices \(i\) and \(j\), there exists a sequence \(\{i = i_0, i_1, \dots, i_k = j\}\) with \(K_{i_{m+1} i_m} > 0\) for all \(m\).
\end{enumerate}
Then, there exists a positive integer \(L\) (dependent on the structure of \(K\)) such that every entry of the effective diffusion operator satisfies:
\[
[A^L]_{ij} > 0 \quad \text{for all } i,j.
\]
Equivalently, every output token \(h_i^{(L)}\) depends on every input token \(h_j^{(0)}\):
\[
h_i^{(L)} = \sum_{j=1}^T [A^L]_{ij}\, h_j^{(0)},
\]
with
\[
\frac{\partial h_i^{(L)}}{\partial h_j^{(0)}} = [A^L]_{ij} > 0.
\]
Thus, the diffusion process inherently captures global dependencies.
\end{theorem}

\begin{proof}
Starting from
\[
H^{(L)} = A^L H^{(0)},
\]
with
\[
A = I + \delta t \Bigl(K - \operatorname{diag}(K \mathbf{1})\Bigr),
\]
observe that \(A\) is a perturbation of the identity matrix by a non-negative matrix. Given the strong connectivity of the directed graph corresponding to \(K\), \(A\) is irreducible. By the Perronâ€“Frobenius theorem for irreducible non-negative matrices, there exists a positive integer \(L\) such that all entries of \(A^L\) are strictly positive. This completes the proof.
\end{proof}

\subsection{Discussion}
This theorem formalizes the intuition that even if the diffusion kernel \(K\) employs local attenuation (for instance, through a Gaussian decay \(g(|t-s|)\) or a directional bias \(\phi(t-s)\)), the strong connectivity ensures that every token is indirectly linked to every other token. Two points merit further consideration:
\begin{enumerate}
    \item \textbf{Quantitative Bound on \(L\):} While the theorem guarantees an \(L\) exists such that \([A^L]_{ij} > 0\) for all \(i,j\), it does not provide a practical bound. In scenarios where the graph induced by \(K\) is sparse, \(L\) might be large, which may affect the efficiency of the diffusion process.
    \item \textbf{Ensuring Strong Connectivity:} The design of \(K\) must ensure that its nonzero pattern yields a strongly connected graph. This is a crucial requirement for achieving global dependency.
\end{enumerate}
In practice, LDN compensates for potential limitations of the pure diffusion process by combining it with a local update \(F\) and a linear attention module \(A_{\text{lin}}\), thereby offering multiple pathways for global information flow.

\subsection{Stability and Convergence of the Diffusion Process}
Next, we examine the stability of the pure diffusion update in isolation. Recall that:
\[
H^{(\ell)} = \left(I + \delta t \Bigl(K - \operatorname{diag}(K \mathbf{1})\Bigr)\right) H^{(\ell-1)},
\]
where \(K \in \mathbb{R}^{T \times T}\) is a learnable kernel with \(K_{ts} \ge 0\) for all \(t,s\), and \(\delta t\) is the adaptive time-step.

\paragraph{Stability Theorem:}  
Assume \(K_{ts} \ge 0\) for all \(t,s\) and choose \(\delta t\) such that:
\[
\delta t \le \min_{t \in \{1,\dots,T\}} \frac{1}{\sum_{s=1}^{T} K_{ts}}.
\]
Then, for every layer \(\ell\) and each time step \(t\), the update can be written as:
\[
h_t^{(\ell)} = \left(1 - \delta t \sum_{s=1}^{T} K_{ts}\right) h_t^{(\ell-1)} + \delta t \sum_{s=1}^{T} K_{ts}\, h_s^{(\ell-1)},
\]
with the coefficients satisfying:
\[
\left(1 - \delta t \sum_{s=1}^{T} K_{ts}\right) \ge 0 \quad \text{and} \quad \left(1 - \delta t \sum_{s=1}^{T} K_{ts}\right) + \delta t \sum_{s=1}^{T} K_{ts} = 1.
\]
Thus, the update is a convex combination of the hidden states from the previous layer, implying:
\[
\|H^{(\ell)}\|_2 \le \|H^{(\ell-1)}\|_2.
\]

\paragraph{Proof:}  
Expressing the update for each \(t\) as:
\[
h_t^{(\ell)} = \alpha_t\, h_t^{(\ell-1)} + \delta t \sum_{s=1}^{T} K_{ts}\, h_s^{(\ell-1)},
\]
where
\[
\alpha_t \triangleq 1 - \delta t \sum_{s=1}^{T} K_{ts},
\]
and noting that \(\alpha_t \ge 0\) with \(\alpha_t + \delta t \sum_{s=1}^{T} K_{ts} = 1\), it follows that each \(h_t^{(\ell)}\) is a convex combination of \(\{h_s^{(\ell-1)}\}_{s=1}^T\). Since convex combinations are non-expansive with respect to the \(\ell_2\) norm, the result holds. \(\blacksquare\)

\paragraph{Implications:}  
This stability result is vital for training LDN. It ensures that the norms of the hidden states do not increase with each layer, preventing issues like exploding gradients. Together with the global mixing properties, this underlines the balanced design of LDN, combining robustness with effective global information propagation.
