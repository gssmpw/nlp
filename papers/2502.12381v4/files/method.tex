\section{Method}\label{sec:method}

This section details the \textbf{Linear Diffusion Network (LDN)}, a novel architecture designed to replace expensive self-attention operations with a principled, \emph{diffusion-inspired} mechanism. The core idea is to view temporal information sharing as a single diffusion process supplemented by local updates and a \emph{new} diffusion-based attention module. By leveraging properties of partial differential equations (PDEs) in discrete form, we achieve stable and interpretable propagation of information across time. Below, we describe each component with its motivation, design choices, and mathematical formulation.

\subsection{Input Encoding and Positional Information}
\paragraph{Motivation.} 
Neural sequence models must handle order-dependent data (e.g., text, time series). Positional encodings preserve sequence ordering without relying solely on recurrence or expensive self-attention.

\paragraph{Formulation.}
Let the input be
\[
X = \{x_1, x_2, \dots, x_T\},
\]
where each \(x_t\) is an input token at time \(t\). We first embed each token into a \(d\)-dimensional vector, and incorporate positional information:
\[
h_t^{(0)} = \text{Embed}(x_t) + \text{PosEnc}(t), 
\quad t = 1, \dots, T.
\]
Gathering these into a matrix, we have
\[
H^{(0)} \in \mathbb{R}^{T \times d}.
\]
This initialization ensures the network can distinguish different temporal positions from the outset.

\subsection{Unified Multi-Component Update: Diffusion, Local, and Diffusion-Based Attentional Modules}
\paragraph{Motivation.}
Rather than relying exclusively on one mechanism (e.g., attention), LDN combines \textit{diffusion}, \textit{local updates}, and \textit{diffusion-based attention} to capture different aspects of temporal structure:
\begin{itemize}
    \item \textbf{Diffusion}: ensures global smoothing and long-range interactions while preserving PDE-like interpretability.
    \item \textbf{Local Update}: refines token-specific details lost by smoothing.
    \item \textbf{Diffusion-Based Attention}: offers a global, content-based mechanism built on PDE principles, enabling efficient parallelization while moving beyond classical (linear or softmax) attention.
\end{itemize}

\paragraph{Formulation.}
Over \(L\) layers, each hidden state \(h_t^{(\ell)}\) is updated from its previous value \(h_t^{(\ell-1)}\) via:
\[
h_t^{(\ell)} 
= h_t^{(\ell-1)} + \Delta h_t^{(\ell)},
\]
where
\[
\Delta h_t^{(\ell)} 
= \underbrace{\delta t \cdot \sum_{s=1}^{T} K_{ts}\bigl( h_s^{(\ell-1)} - h_t^{(\ell-1)}\bigr)}_{\text{Diffusion Module}} 
+ \underbrace{F\bigl(x_t, h_t^{(\ell-1)}\bigr)}_{\text{Local Update}} 
+ \underbrace{A_{\text{diff}}\bigl(H^{(\ell-1)}\bigr)}_{\text{Diffusion-Based Attention}}.
\]

\paragraph{Adaptive Time Step \(\delta t\).}
The scalar \(\delta t\) controls the diffusion rate. Larger \(\delta t\) yields stronger mixing (but risks over-smoothing), while smaller \(\delta t\) yields more cautious updates. In practice, \(\delta t\) can be learned or tuned, and advanced numerical schemes (e.g., Crank--Nicolson) can be used for stability in deep networks.

\subsection{PDE-Based Diffusion Kernel \texorpdfstring{\(K\)}{}}
\paragraph{Motivation.}
Diffusion in continuous PDEs (e.g., the heat equation) is governed by the Laplacian operator. Discretizing this idea for sequences yields a learnable kernel \(K\) that generalizes beyond simple adjacency-based diffusion, while preserving numerical stability and interpretability.

\paragraph{Construction.}
We design a row-sum-zero kernel 
\(\displaystyle K \in \mathbb{R}^{T\times T}\)
to mirror the discrete Laplacian:
\[
\sum_{s=1}^T K_{ts} = 0 \quad \forall\,t.
\]
Intuitively, each row of \(K\) dictates how information flows \emph{into} and \emph{out of} a specific time step \(t\). To build \(K\):
\begin{enumerate}
    \item \textbf{Raw Similarities:} For each pair \((t, s)\), compute
    \[
    \tilde{k}_{ts}
    = \bigl[\phi(t-s)\bigr]
      \; g\bigl(\lvert t-s\rvert\bigr)
      \; \psi\bigl(h_t^{(\ell-1)}, h_s^{(\ell-1)}\bigr),
    \]
    where:
    \begin{itemize}
        \item \(\phi(t-s)\) promotes directional or causal flows in time (e.g., forward-only).
        \item \(g\bigl(\lvert t-s\rvert\bigr)\) attenuates distant positions (e.g., via a Gaussian decay).
        \item \(\psi\bigl(h_t, h_s\bigr)\) gates based on content similarity (e.g., via a small MLP).
    \end{itemize}
    \item \textbf{Row-Sum-Zero Projection:} 
    For \(t \neq s\), let 
    \(\displaystyle \widehat{k}_{ts} = \text{softplus}(\tilde{k}_{ts})\). 
    Then enforce row-sum-zero by setting
    \[
    \widehat{k}_{tt} 
    = - \sum_{\substack{s=1 \\ s \neq t}}^T \widehat{k}_{ts}.
    \]
    The final kernel is \(K_{ts} = \widehat{k}_{ts}\).
\end{enumerate}
This ensures stable, Laplacian-like diffusion while adapting to sequence distance and token content.

\paragraph{Discrete PDE Perspective.}
The term 
\(\delta t \cdot \sum_{s}K_{ts} (h_s^{(\ell-1)} - h_t^{(\ell-1)})\)
resembles an explicit forward Euler step of the heat equation. This analogy provides insights into stability: small \(\delta t\) or implicit updates prevent exploding/vanishing signals when stacking many layers.

\subsection{Local Update \texorpdfstring{\(F\)}{}}
\paragraph{Motivation.}
Pure diffusion can oversmooth or erase fine-grained details. A local update function \(F\) restores token-specific features (e.g., morphological cues in text or local patterns in time series).

\paragraph{Formulation.}
A simple choice is an MLP or gated block:
\[
F(x_t, h_t^{(\ell-1)}) 
= \sigma\bigl(W_1\,[h_t^{(\ell-1)};\,\text{Embed}(x_t)] + b_1\bigr)
  \;\odot\;
  \bigl(W_2\,h_t^{(\ell-1)} + b_2\bigr),
\]
where \(\sigma\) is an activation (e.g., sigmoid) and \(\odot\) is element-wise multiplication. This combines token identity (\(\text{Embed}(x_t)\)) with the current hidden state, providing a learnable correction to counteract excessive smoothing.

\subsection{Diffusion-Based Attention Module \texorpdfstring{\(A_{\text{diff}}\)}{}}
\paragraph{Motivation.}
While diffusion alone propagates global information, it often does so \emph{uniformly} with respect to time-position. We introduce a \emph{content-sensitive} diffusion mechanism that not only diffuses signals but also modulates them based on hidden-state similarity. Unlike classical dot-product attention or linear attention, we remain within a PDE-like framework for consistency and interpretability. 

\paragraph{Formulation.}
We define a second row-sum-zero kernel 
\(\displaystyle D \in \mathbb{R}^{T\times T}\),
constructed similarly to \(K\) but with potentially distinct parameters (or functional forms):
\[
\tilde{d}_{ts}
= \bigl[\phi_{\text{att}}(t-s)\bigr]
  \; g_{\text{att}}\bigl(\lvert t-s\rvert\bigr)
  \; \psi_{\text{att}}\bigl(h_t^{(\ell-1)}, h_s^{(\ell-1)}\bigr),
\]
followed by a softplus and row-sum-zero projection (analogous to the steps for \(K\)). The \emph{diffusion-based attention} contribution is then:
\[
A_{\text{diff}}\bigl(H^{(\ell-1)}\bigr)
= \delta t_{\text{att}}
  \sum_{s=1}^{T} D_{ts}\bigl( h_s^{(\ell-1)} - h_t^{(\ell-1)}\bigr),
\]
where \(\delta t_{\text{att}}\) is a learnable or tunable rate controlling the strength of global, content-sensitive diffusion.

\paragraph{Interpretation.}
Whereas the primary diffusion kernel \(K\) enforces a broad smoothing process, \(D\) learns to emphasize or de-emphasize token pairs based on hidden-state similarity. This effectively behaves like an \emph{attention} mechanism grounded in the same PDE-inspired principles.

\subsection{Layer-Wise Update in Matrix Form}
\paragraph{Motivation.}
Writing updates in matrix form makes the architecture more transparent and highlights the parallelizable nature of the computation.

\paragraph{Formulation.}
For layer \(\ell\), the combined update is:
\[
\begin{aligned}
H^{(\ell)} 
&= H^{(\ell-1)} 
+ \underbrace{\delta t \cdot \Bigl(K \, H^{(\ell-1)} 
  \;-\; \operatorname{diag}(K \mathbf{1}) \, H^{(\ell-1)}\Bigr)}_{\text{Diffusion Module}}
\\
&\quad\quad+ \underbrace{F\bigl(X, H^{(\ell-1)}\bigr)}_{\text{Local Update}}
+ \underbrace{A_{\text{diff}}\bigl(H^{(\ell-1)}\bigr)}_{\text{Diffusion-Based Attention}},
\end{aligned}
\]
where \(K \mathbf{1}\) and \(D \mathbf{1}\) (in the respective modules) implement the row-sum-zero constraint directly.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{imgs/matrix.pdf}
  \caption{\textbf{Matrix-Form Illustration.} 
  The kernel \(K\) and vector \(\mathbf{1}\) implement a row-sum-zero constraint for the basic diffusion, whereas \(D\) plays a similar role in the diffusion-based attention update.}
  \label{fig:matrix-operations}
\end{figure}

\subsection{Multi-Scale Diffusion and Residual Connections}
\paragraph{Motivation.}
Diffusion alone can be restrictive. Allowing multiple scales and residual links ensures that the network can capture both \emph{global} and \emph{local} patterns without losing high-frequency details.

\paragraph{Techniques.}
\begin{itemize}
    \item \textbf{Multi-Scale Diffusion:} Each layer can learn its own \(\delta t\) and/or distinct diffusion kernels (\(K\) and \(D\)). Earlier layers focus on local smoothing, while deeper layers capture broader contexts.
    \item \textbf{Residual Connections:} Standard skip connections preserve original signals, facilitate gradient flow, and prevent over-diffusion.
\end{itemize}

\subsection{Parallelization and Temporal Dynamics}
\paragraph{Motivation.}
Transformers enable parallel processing across the time dimension. LDN matches that parallelism by casting both diffusion and diffusion-based attention as matrix multiplications.

\paragraph{Formulation.}
Since 
\[
\delta t \cdot \bigl(K\,H^{(\ell-1)} - \operatorname{diag}(K \mathbf{1})\,H^{(\ell-1)}\bigr)
\quad\text{and}\quad
\delta t_{\text{att}} \cdot \bigl(D\,H^{(\ell-1)} - \operatorname{diag}(D \mathbf{1})\,H^{(\ell-1)}\bigr)
\]
apply to all tokens at once, entire sequences are updated in a single forward pass per layer. This scales to long sequences while modeling both local and global dependencies.

\subsection{Output Decoding}
\paragraph{Motivation.}
Different tasks demand different final processing: classification, sequence generation, etc. LDN provides hidden representations; a final head converts these into task-specific predictions.

\paragraph{Formulation.}
\begin{itemize}
    \item \textbf{Sequence-to-Sequence Tasks:}
    Pass \(H^{(L)}\) into a separate decoder (or reuse LDN layers in an encoder-decoder setting) for output generation.
    \item \textbf{Sequence Classification:}
    Aggregate \(H^{(L)}\) over time (e.g., pooling or attention pooling) and feed into a classification head.
\end{itemize}

\subsection{Training Procedure}
\paragraph{Motivation.}
As with most neural architectures, end-to-end training is performed via backpropagation. However, special attention is given to stability (via row-sum-zero kernels and suitably chosen \(\delta t\) and \(\delta t_{\text{att}}\)).

\paragraph{Details.}
\begin{itemize}
    \item \textbf{Loss and Optimization:} 
    Standard losses (e.g., cross-entropy) and optimizers (e.g., Adam) are used, often with a learning-rate schedule.
    \item \textbf{Stability Constraints:}
    The row-sum-zero property and carefully chosen time steps (\(\delta t\) and \(\delta t_{\text{att}}\)) keep the network from exploding or vanishing through layers.
    \item \textbf{Regularization:} 
    Techniques such as dropout, weight decay, or layer normalization help control overfitting and prevent overly aggressive diffusion.\footnote{Code available at \href{https://github.com/rubberduck529/LDN/}{https://github.com/rubberduck529/LDN/}}
\end{itemize}

\begin{figure*}[htb]
  \centering
  \includegraphics[width=\textwidth]{imgs/tik.pdf}
  \caption{\textbf{LDN Overview.} Each layer combines three modules---\textit{Diffusion, Local Update, and Diffusion-Based Attention}---in a parallelizable, stable manner. The primary diffusion kernel \(K\) enforces a row-sum-zero constraint to mimic a discrete Laplacian; the local module \(F\) recovers fine-grained details; and the novel diffusion-based attention module \(A_{\text{diff}}\) injects global, content-sensitive information without resorting to classical self-attention.}
  \label{fig:LDN}
\end{figure*}
