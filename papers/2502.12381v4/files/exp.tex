\section{Experiments}
In this section, we evaluate \textbf{LDN}---now featuring a diffusion-based attention mechanism---on multiple benchmarks, comparing it against the Vision Transformer (ViT) baseline~\cite{dosovitskiy2020image} and other architectures such as Swin~\cite{liu2021swin}, DeiT~\cite{touvron2021training}, ConvNeXt~\cite{liu2022convnet}, Reformer~\cite{kitaev2020reformer}, Linformer~\cite{wang2020linformer}, and Performer~\cite{choromanski2020rethinking}. We report results on the large-scale ImageNet dataset~\cite{deng2009imagenet} and on the Long Range Arena (LRA) benchmark~\cite{tay2021long}. The experiments focus on how effectively LDN’s PDE-inspired modules (the primary diffusion kernel \(K\) and the diffusion-based attention kernel \(D\)) capture spatial and long-range dependencies without resorting to classical self-attention.

\subsection{Datasets and Experimental Setup}
\paragraph{ImageNet.} 
For ImageNet~\cite{deng2009imagenet}, we follow standard practices:\footnote{We use the ILSVRC-2012 version of ImageNet, containing \(\sim1.28\)M training images across 1000 classes.} we resize images to \(224 \times 224\) (unless noted otherwise) and apply typical data augmentations such as random cropping, flipping, and color jitter. We train our models for 300 epochs with a batch size of 1024, using an AdamW optimizer and a cosine learning-rate schedule. 

\textit{LDN Details.} 
\begin{itemize}
    \item \emph{Patch Embedding}: We treat each image as a sequence of non-overlapping patches (of size \(16 \times 16\), by default), each embedded into a \(d\)-dimensional space.
    \item \emph{Diffusion Kernels}: Both \(K\) (primary diffusion) and \(D\) (diffusion-based attention) are row-sum-zero, facilitating stable PDE-like updates. We initialize \(\delta t\) and \(\delta t_{\text{att}}\) to small constants (e.g., 0.05--0.1) and allow them to be learnable.
    \item \emph{Local Update}: An MLP-based local function \(F\) refines per-token (or per-patch) features after diffusion.
    \item \emph{Layer Configuration}: We use 12, 24, or 32 layers depending on the model scale (Base, Large, Huge). Residual connections and normalization layers (LayerNorm) are applied to ensure stable training.
\end{itemize}

We employ early stopping based on validation-set accuracy. Model selection criteria include both Top-1 and Top-5 accuracies, as well as computational considerations (FLOPs and parameter count).

\paragraph{Long Range Arena.}
We also evaluate LDN on the Long Range Arena (LRA) benchmark~\cite{tay2021long}, which consists of tasks designed to test long-context sequence modeling:
\begin{itemize}
    \item \textbf{ListOps}: A hierarchical parsing task (sequence length up to 2k).
    \item \textbf{IMDB}: Sentiment classification with sequences up to 4k tokens.
    \item \textbf{Byte-Level Text Classification}: Classifying text sequences (byte-level encoding) up to 4k tokens.
    \item \textbf{CIFAR-10}: Image classification by flattening each \(32 \times 32\) image into a 1D sequence.
    \item \textbf{Pathfinder}: A synthetic task requiring the model to distinguish connected paths in images represented as sequences of patches or pixels.
\end{itemize}
We train LDN for 500k steps on these tasks with a batch size of 256. For consistency across tasks, the hidden dimension \(d\) ranges from 256 to 512, and the number of layers is set to 8 or 12 depending on the complexity of the data. 

\textit{Diffusion-Based Setup.}  
\begin{itemize}
    \item \emph{Discrete PDE Updates}: Each forward pass applies the diffusion and diffusion-based attention kernels \(K\) and \(D\), both constrained to have row-sum zero for numerical stability. 
    \item \emph{Numerical Stability}: We found that using small initial \(\delta t\) values and applying an explicit forward Euler scheme allowed stable training on sequences up to length 4k. 
    \item \emph{Parameter Counts}: For fairness with baselines, we target about 125M parameters by adjusting the number of layers and hidden dimension. 
\end{itemize}

Performance on LRA tasks is measured via classification accuracy. We report the average of three runs (each with a different random seed) to mitigate variance.

\subsection{Results on ImageNet}
Table~\ref{tab:imagenet} summarizes the performance of LDN compared to ViT~\cite{dosovitskiy2020image} and other architectures~\cite{liu2021swin,touvron2021training,liu2022convnet}. Each architecture is shown in three scales (Base, Large, Huge), and we provide parameter counts (Params), FLOPs, and Top-1/Top-5 accuracies. The ViT model in each block serves as the baseline. We denote improvements with \textcolor{darkgreen}{green arrows} and drops with \textcolor{darkred}{red arrows}.

\begin{table}[ht]
\centering
\caption{\textbf{Comparison of different architectures on ImageNet.} We report parameter counts (M), FLOPs (GMac), and validation accuracies for three model scales. The baseline in each variant block is ViT~\cite{dosovitskiy2020image}, and changes in parentheses denote the improvement ($\uparrow$) or decrease ($\downarrow$) relative to that baseline.}
\label{tab:imagenet}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{l l S[table-format=3.0] S[table-format=3.1] c c}
\toprule
\rowcolor{gray!20}
\textbf{Variant} & \textbf{Architecture} & \textbf{Params (M)} & \textbf{FLOPs (GMac)} & \textbf{Top-1 (\%)} & \textbf{Top-5 (\%)} \\
\midrule
%------------------- Base ---------------------
\multirow{5}{*}{Base} 
  & ViT (Baseline)      & 86    & 17.6   & 82.5 & 96.0 \\
  & Swin~\cite{liu2021swin}                & 85    & 16.4   & 82.7 \uparrowgreen{0.2} & 96.2 \uparrowgreen{0.2} \\
  & DeiT~\cite{touvron2021training}        & 86    & 17.2   & 82.2 \downarrowred{0.3} & 95.9 \downarrowred{0.1} \\
  & ConvNeXt~\cite{liu2022convnet}         & 90    & 18.0   & 82.8 \uparrowgreen{0.3} & 96.3 \uparrowgreen{0.3} \\
  & LDN (Ours)          & 52    & 10.6   & 82.0 \downarrowred{0.5} & 95.7 \downarrowred{0.3} \\
\midrule
%------------------- Large --------------------
\multirow{5}{*}{Large} 
  & ViT (Baseline)      & 307   & 61.6   & 84.2 & 97.0 \\
  & Swin~\cite{liu2021swin}                & 285   & 58.2   & 84.5 \uparrowgreen{0.3} & 97.2 \uparrowgreen{0.2} \\
  & DeiT~\cite{touvron2021training}        & 300   & 60.0   & 83.9 \downarrowred{0.3} & 96.9 \downarrowred{0.1} \\
  & ConvNeXt~\cite{liu2022convnet}         & 310   & 62.5   & 84.6 \uparrowgreen{0.4} & 97.3 \uparrowgreen{0.3} \\
  & LDN (Ours)          & 181   & 36.7   & 83.7 \downarrowred{0.5} & 96.8 \downarrowred{0.2} \\
\midrule
%------------------- Huge ---------------------
\multirow{5}{*}{Huge} 
  & ViT (Baseline)      & 632   & 120.5  & 84.8 & 97.3 \\
  & Swin~\cite{liu2021swin}                & 600   & 115.0  & 85.0 \uparrowgreen{0.2} & 97.5 \uparrowgreen{0.2} \\
  & DeiT~\cite{touvron2021training}        & 620   & 118.0  & 84.6 \downarrowred{0.2} & 97.2 \downarrowred{0.1} \\
  & ConvNeXt~\cite{liu2022convnet}         & 640   & 125.0  & 85.1 \uparrowgreen{0.3} & 97.6 \uparrowgreen{0.3} \\
  & LDN (Ours)          & 373   & 75.4   & 84.3 \downarrowred{0.5} & 97.0 \downarrowred{0.3} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\paragraph{Discussion (ImageNet).}
Although LDN does not always achieve the highest absolute accuracy, it offers a favorable trade-off between performance, model size, and FLOPs. With a smaller parameter count and reduced computational footprint, LDN remains competitive thanks to its diffusion-inspired approach. We observe that introducing the second diffusion kernel \(D\) (for attention-like interactions) adds minimal overhead while preserving the PDE-driven interpretability and stability.

\subsection{Results on Long Range Arena}
Table~\ref{tab:lra} presents the performance of LDN on the LRA benchmark, comparing it against several transformer-based models (e.g., Reformer~\cite{kitaev2020reformer}, Linformer~\cite{wang2020linformer}, Performer~\cite{choromanski2020rethinking}) and other variants. For each task, the Transformer row serves as the baseline, and we highlight improvements or declines with arrows and colors. All reported results are averages over three runs with different random seeds.

\begin{table}[ht]
\centering
\caption{\textbf{Performance on the Long Range Arena (LRA) tasks.} We report accuracy (\%) on the test sets. Baseline values are from the Transformer row. Models are approximately 100--150M parameters in size.}
\label{tab:lra}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lcccccc}
\toprule
\rowcolor{gray!20}
\textbf{Model} & \textbf{ListOps} & \textbf{IMDB} & \textbf{Byte-level} & \textbf{CIFAR-10} & \textbf{Pathfinder} & \textbf{Avg.} \\
\midrule
Transformer & 38.2 & 86.5 & 64.0 & 59.1 & 72.3 & 64.0 \\
Reformer~\cite{kitaev2020reformer}    
            & 37.5 \downarrowred{0.7}
            & 85.7 \downarrowred{0.8}
            & 63.1 \downarrowred{0.9}
            & 58.4 \downarrowred{0.7}
            & 71.5 \downarrowred{0.8}
            & 63.2 \downarrowred{0.8} \\
Linformer~\cite{wang2020linformer}    
            & 40.3 \uparrowgreen{2.1}
            & 87.0 \uparrowgreen{0.5}
            & 64.2 \uparrowgreen{0.2}
            & 59.6 \uparrowgreen{0.5}
            & 74.0 \uparrowgreen{1.7}
            & 65.0 \uparrowgreen{1.0} \\
Performer~\cite{choromanski2020rethinking}
            & 39.1 \uparrowgreen{0.9}
            & 86.8 \uparrowgreen{0.3}
            & 64.5 \uparrowgreen{0.5}
            & 60.2 \uparrowgreen{1.1}
            & 73.2 \uparrowgreen{0.9}
            & 64.8 \uparrowgreen{0.8} \\
Swin~\cite{liu2021swin}        
            & 39.7 \uparrowgreen{1.5}
            & 87.1 \uparrowgreen{0.6}
            & 64.4 \uparrowgreen{0.4}
            & 59.8 \uparrowgreen{0.7}
            & 74.1 \uparrowgreen{1.8}
            & 65.0 \uparrowgreen{1.0} \\
\textbf{LDN (Ours)} 
            & 41.2 \uparrowgreen{3.0}
            & 88.1 \uparrowgreen{1.6}
            & 65.3 \uparrowgreen{1.3}
            & 61.0 \uparrowgreen{1.9}
            & 74.5 \uparrowgreen{2.2}
            & 66.0 \uparrowgreen{2.0} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\paragraph{Discussion (LRA).}
LDN achieves strong performance across all LRA tasks, demonstrating that its \emph{two-kernel diffusion strategy} scales effectively to sequences with thousands of tokens. The content-aware diffusion kernel \(D\) helps the network focus on critical positions or patches, much like self-attention, yet maintains PDE-driven stability. Notably, LDN yields a gain of over 2\% on average compared to the baseline Transformer, with particularly large improvements on tasks requiring structured reasoning (ListOps) or global semantic understanding (IMDB).

\subsection{Overall Discussion}
Our results show that \textbf{LDN} offers:

\ballnumber{1}~\textbf{Competitive Image Classification.}
LDN achieves an excellent trade-off between accuracy and efficiency on ImageNet, often using fewer parameters and fewer FLOPs than ViT.

\ballnumber{2}~\textbf{Robust Long-Range Sequence Modeling.}
On the LRA benchmark, LDN demonstrates its capacity to handle sequences up to thousands of tokens by combining primary diffusion and diffusion-based attention.

\ballnumber{3}~\textbf{Stable Training via PDE Principles.}
Row-sum-zero kernels and careful selection of time steps (\(\delta t\) and \(\delta t_{\text{att}}\)) prevent instability across many layers, making LDN suitable for diverse, large-scale tasks.

Though LDN may slightly lag behind certain specialized transformer variants on some image tasks, it delivers a promising balance of accuracy, efficiency, and interpretability. Casting “attention” as a \emph{diffusion} process introduces novel avenues for robust sequence modeling in both vision and NLP domains.
