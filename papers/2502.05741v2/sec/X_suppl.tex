\clearpage
\setcounter{page}{1}
\maketitlesupplementary
\appendix

\section{Performance Details}

This section provides additional details regarding the results presented in Table~\ref{tab:performance}.

The rate-distortion (R-D) results can vary across different VTM anchors due to different evaluation process. To provide a generally accepted baseline on Kodak, we adopt the R-D results from the CompressAI repository~\cite{Begaint.2020.CompressAI}, which are collected from VTM-9.1. For other datasets, we use the following script to evaluate images in YUV space using VTM-9.1, where QPs range from 22, 27, 32, 37, 42, 47.

\begin{verbatim}
VTM/bin/EncoderAppStatic -i [input.yuv]
  -c VTM/cfg/encoder_intra_vtm.cfg
  -o [output.yuv] -b [output.bin]
  -wdt [width] -hgt [height] -q [QP] 
  --InputBitDepth=8 -fr 1 -f 1
  --InputChromaFormat=444
\end{verbatim}

Regarding runtime, FAT is reported to have a decoding time of 242 ms; however, our tests indicate a significantly longer decoding time of 426 seconds. This discrepancy remains an unresolved issue documented in its GitHub repository. Based on our analysis, decoding a single slice with FAT's T-CA entropy model involves computing masked channel attention across 12 layers, whereas TCM requires only two layers for decoding each slice. Incorporating techniques such as KV caching could potentially reduce the FLOPs required for each slice during decoding. Furthermore, the authors have acknowledged that the entropy coder in FAT requires additional optimization to improve decoding efficiency.

For complexity measurements, we use the \textit{thop} Python package to calculate parameters and FLOPs, ensuring consistency with the methodology employed for TCM~\cite{Liu.2023.TCM}. However, \textit{thop} has known limitations: it cannot account for FLOPs arising from non-layer-specific operations such as mathematical functions, matrix multiplications (e.g., in attention mechanisms), or CUDA-specific implementations. While the majority of FLOPs originate from Torch integrated layers, the values reported in Table~\ref{tab:performance} provide a reasonable and fair reference for comparison.

\section{Additional Experiment Results}

This section presents additional experimental results comparing our method with recent learned image compression (LIC) approaches. We present the BD-rate (MS-SSIM) results in Table~\ref{tab:ssim-bdrate-clic}, with VTM-9.1 as anchor. In fact, only a few recent works have publicly available MS-SSIM optimized models or corresponding curves on Tecnick/CLIC, resulting in some missing results.

\begin{table}
\centering
\begin{tabular}{lrrr}
\hline
Method                          & Kodak             & CLIC              & Tecnick           \\ \hline
VTM-9.1                         & 0.00\%            & 0.00\%            & 0.00\%            \\
ELIC        & -7.57\%           & -                 & -                 \\
TCM-large   & -49.76\%          & -                 & -                 \\
MLIC++ & -52.99\% & -47.43\% & -53.14\% \\
FAT          & -51.64\%          & -                 & -                 \\
LALIC (Ours)                    & -51.23\%          & -46.97\%          & -49.47\%          \\ \hline
\end{tabular}
\caption{BD-rate (MS-SSIM) performance relative to VTM-9.1 across different datasets. "-" indicates an unavailable result.}
\label{tab:ssim-bdrate-clic}
\end{table}


\section{Linear Attention Mechanisms}

Except the vanilla Attention which has a quadratic complexity, common modules have a linear complexity, including convolution, window-based attention. The recent linear attention methods, RWKV and Mamba are widely recognized for their efficiency in handling large-scale sequences to get a global reception filed, and also maintains the linear complexity with respect to the input size.

To provide a clearer comparison of these methods, Table~\ref{tab:attention-complexity} summarizes the theoretical time complexity of various attention mechanisms and the typical values of their number of operations (\#OPs).

\begin{table}[h!]
\centering
\begin{tabular}{@{}lrr@{}}
\toprule
Methods                    & Time Complexity          & \#OPs \\ \midrule
AFT~\cite{Zhai.2021.AFT}                 & \( 7LD \)          & \( 7LD \)   \\
AFT+Shift                  & \( 7LD + 50LD \)    & \( 57LD \)  \\
BiWKV+Shift                & \( 29LD + 50LD \)   & \( 79LD \)  \\
Window Attention~\cite{Liu.2021.SwinT}   & \( 2w^2LD \, (w=8) \) & \( 128LD \) \\
Selective Scan~\cite{Gu2023MambaLS}      & \( 9NLD \, (N=16) \)  & \( 144LD \) \\
Selective Scan 2D~\cite{Liu2024VMambaVS} & \( 4 \times 9NLD \, (N=16) \) & \( 576LD \) \\ \bottomrule
\end{tabular}
\caption{Theoretical time complexity of various attention mechanisms in terms of number of operations (\#OPs).}
\label{tab:attention-complexity}
\end{table}

In all cases, the computational cost is directly proportional to \( L \cdot D \), where \( L \) represents the sequence length, and \( D \) denotes the latent dimension. The theoretical FLOPs for various mechanisms are outlined below:

\begin{itemize}    
    \item \textbf{AFT+Shift}: The complexity of the AFT (named AFT-simple in ~\cite{Zhai.2021.AFT}) is estimated as \( 7LD \) by the \textit{torch-operation-counter} package. Adding the 5x5 depth-wise convolution shift operation (\( 25LD \times 2 \) for both spatial and channel mix modules) increases the total complexity to \( 57LD \).
    
    \item \textbf{BiWKV+Shift}: The BiWKV~\cite{Duan.2024.Vision-RWKV} mechanism, computed as \( 29LD \) according to the Vision-RWKV GitHub repository, combined with the shift operation results in \( 79LD \).
    
    \item \textbf{Window Attention}: The window attention~\cite{Liu.2021.SwinT} mechanism has a complexity of \( 2w^2LD \), where \( w \) is the window size, typically set to 8, resulting in \( 128LD \).
    
    \item \textbf{Selective Scan}: In Mamba~\cite{Gu2023MambaLS}, the selective scan mechanism has a complexity of \( 9NLD \), where \( N \) is the state dimension, typically set to 16, leading to \( 144LD \). In SS2D~\cite{Liu2024VMambaVS}, the selective scan is performed four times, resulting in a total complexity of \( 4 \times 9NLD = 576LD \).
\end{itemize}

As shown in Table~\ref{tab:attention-complexity}, BiWKV attention demonstrates significant computational efficiency compared to these other mechanisms, making it a compelling choice for balancing performance and complexity.

\section{Linear Complexity on Scaling}

Practical learned image compression (LIC) methods exhibit linear complexity with respect to the number of pixels, as shown in Figure~\ref{fig:trend-linear}. Unlike previous demonstrations~\cite{Jiang.2023.MLICpp} that used a quadratic x-axis and presented a quadratic trend for all methods, this figure employs a linear x-axis for clarity, providing a more intuitive understanding for readers. The maximum resolution tested is 1024$\times$1024.

Among recent LIC methods, our proposed LALIC demonstrates medium-low FLOPs and forward GPU memory usage, striking a balance between computational efficiency and memory requirements.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{fig/trend-flops.pdf}
        \caption{FLOPs vs. Resolution}
        \label{fig:sub1}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{fig/trend-mem.pdf}
        \caption{GPU Memory Usage vs. Resolution}
        \label{fig:sub2}
    \end{subfigure}
  \caption{Linear scaling trends of FLOPs (a) and GPU memory usage (b) for different LIC methods as a function of image resolution. LALIC achieves competitive performance with medium-low computational and memory demands.}
  \label{fig:trend-linear}
\end{figure}


\section{Entropy Model Architecture}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{fig/ConvSCCTX.pdf}
  \caption{Network architecture of Conv SCCTX and Conv Plus SCCTX configurations. The upper module represents channel context extraction, while the lower module corresponds to parameter aggregation.}
  \label{fig:Conv-SCCTX}
\end{figure}

For entropy models, we adopt the Conv SCCTX model~\cite{He.2022.ELIC} and an enhanced Conv Plus SCCTX configuration as reference baselines. The detailed network architectures of these models are illustrated in Figure~\ref{fig:Conv-SCCTX}.

The Conv SCCTX model consists of three 5$\times$5 convolutional (Conv) layers designed to extract channel context, followed by three 1$\times$1 Conv layers for entropy parameter estimation. The Conv Plus SCCTX configuration extends this architecture by incorporating Depth Conv Block (DCB) from the DCVC\cite{Sheng.2023.TCM} learned video coding series, where the hyperparameter \( k \) denotes the kernel size of the depthwise convolution. To further enhance the modeling capacity, we increase the channel dimensions in the depthwise convolution layers, thereby raising the number of context parameters.



\begin{figure*}[h]
\centering
\subfloat[Original]{\includegraphics[width=2.15in]{fig/kodim01.png}%
\label{k01-origin}}
\hfil
\subfloat[TCM-large 0.767 bpp / 32.07 dB]{\includegraphics[width=2.15in]{fig/kodim01_0.013_TCM.png}%
\label{k01-tcm}}
\hfil
\subfloat[LALIC (Ours) 0.759 bpp / 32.12 dB]{\includegraphics[width=2.15in]{fig/kodim01_q4_ours.png}%
\label{k01-our}}
\hfil
\subfloat[Original crop]{\includegraphics[width=1.5in]{fig/kodim01_crop.png}%
\label{k01-c_o}}
\hfil
\subfloat[TCM-large crop]{\includegraphics[width=1.5in]{fig/kodim01_0.013_TCM_crop.png}%
\label{k01-c_tcm}}
\hfil
\subfloat[LALIC (Ours) crop]{\includegraphics[width=1.5in]{fig/kodim01_q4_ours_crop.png}%
\label{k01-c_our}}
\captionsetup{font=small}
\caption{Subjective quality comparison on the $kodim01$ image from Kodak.}
\label{subjective01}
\vspace{-5pt}
\end{figure*}

\begin{figure*}[h]
\centering
\subfloat[Original]{\includegraphics[width=2.15in]{fig/kodim02.png}%
\label{k02-origin}}
\hfil
\subfloat[TCM-large 0.245 bpp / 34.20 dB]{\includegraphics[width=2.15in]{fig/kodim02_0.013_TCM.png}%
\label{k02-tcm}}
\hfil
\subfloat[LALIC (Ours) 0.243 bpp / 34.30 dB]{\includegraphics[width=2.15in]{fig/kodim02_q4_ours.png}%
\label{k02-our}}
\hfil
\subfloat[Original crop]{\includegraphics[width=1.2in]{fig/kodim02_crop.png}%
\label{k02-c_o}}
\hfil
\subfloat[TCM-large crop]{\includegraphics[width=1.2in]{fig/kodim02_0.013_TCM_crop.png}%
\label{k02-c_tcm}}
\hfil
\subfloat[LALIC (Ours) crop]{\includegraphics[width=1.2in]{fig/kodim02_q4_ours_crop.png}%
\label{k02-c_our}}
\captionsetup{font=small}
\caption{Subjective quality comparison on the $kodim02$ image from Kodak.}
\label{subjective02}
\vspace{-5pt}
\end{figure*}


\section{Subjective Results}

We conducted a subjective comparison of reconstructed images generated by our LALIC model and our trained TCM-large model on the Kodak dataset. The results are shown in Figure~\ref{subjective01} and Figure~\ref{subjective02}. By focusing on specific image regions, we observe that our proposed method preserves finer details compared to TCM-large. For instance, LALIC retains sharper textures in the wooden board on the right side of Figure~\ref{subjective01} and captures the intricate structure of the door handle in Figure~\ref{subjective02}.

In addition to qualitative improvements, our method achieves higher PSNR values while maintaining a lower bitrate, highlighting its superior rate-distortion performance over TCM-large.
