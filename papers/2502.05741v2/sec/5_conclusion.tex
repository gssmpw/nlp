\section{Conclusion}

In this paper, we introduced LALIC, a straightforward yet effective linear attention-based learned image compression architecture. LALIC incorporates a Bi-RWKV block with a BiWKV attention mechanism and depth-wise convolution, enabling it to capture both local and global context effectively, thereby reducing redundancy. Furthermore, we applied the Bi-RWKV block to enhance channel-wise entropy modeling. Experimental results demonstrate that LALIC achieves superior rate-distortion performance on commonly used datasets, outperforming VTM-9.1 by -14.84\% in BD-rate on Kodak, Tecnick and CLIC Professional validation datasets. Compared to Convolution, Swin Transformer, and Mamba-based methods, LALIC still maintains efficiency in both decoding speed and parameter usage.
