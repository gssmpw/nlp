
\section{Methods}

\subsection{Overview}

Figure~\ref{fig:overview} illustrates the architecture of the proposed Linear Attention based Learned Image Compression (LALIC) model. Given a raw image $x$, the analysis transform $g_a(\cdot)$ maps it to a latent representation $y$, and further obtain the hyper latent $z$ using the hyper encoder $h_a$.

\begin{equation}
\begin{aligned}
& \boldsymbol{y}=g_a\left(\boldsymbol{x} ; \boldsymbol{\theta}_{g_a}\right) \\
& \boldsymbol{z}=h_a\left(\boldsymbol{y} ; \boldsymbol{\theta}_{h_a}\right)
\end{aligned}
\end{equation}


The quantized hyper latent $\hat z = Q(z)$ is entropy coded for rate $R(\hat{\boldsymbol{z}})$ with a learned factorized prior.
% Then, the quantized hyper latent $\hat{\boldsymbol{z}}=Q(\boldsymbol{z})$ is entropy coded for rate $R(\hat{\boldsymbol{z}})=\mathbb{E}\left[-\log _2\left(p_{\boldsymbol{z} \mid \boldsymbol{\psi}}(\hat{\boldsymbol{z}} \mid \boldsymbol{\psi})\right)\right]$, where $p_{\boldsymbol{z} \mid \boldsymbol{\psi}}(\hat{\boldsymbol{z}} \mid \boldsymbol{\psi})=\Pi_j\left(p_{\boldsymbol{z}_j \mid \boldsymbol{\psi}}(\boldsymbol{\psi}) * \mathcal{U}\left(-\frac{1}{2}, \frac{1}{2}\right)\right)\left(\hat{\boldsymbol{z}}_j\right)$, with a learned factorized prior $\boldsymbol{\psi}$. * denotes convolution operation.
At the decoder side, we first use a hyper decoder $h_s$ to obtain the initial mean and variance:

\begin{equation}
(\tilde{\boldsymbol{\mu}}, \tilde{\boldsymbol{\sigma}})=h_s\left(\hat{\boldsymbol{z}} ; \boldsymbol{\theta}_{h_s}\right)
\end{equation}

% Then, quantization operator $Q(\cdot)$ discretizes $y$ to $\hat y$. $\hat y$ is subsequently losslessly encoded by a entropy coder. Here, we assume that $y$ follows a conditional Guassian distribution of which the distribution parameters are predicted by the space-channel context (SCCTX) entropy model.

Then, quantization operator $Q(\cdot)$ discretizes $y$ to $\hat y$ and we assume that $y$ follows a conditional Gaussian distribution:  
$p_{\hat{\boldsymbol{y}} \mid \hat{\boldsymbol{z}}}(\hat{\boldsymbol{y}} \mid \hat{\boldsymbol{z}}) \sim \mathcal{N}\left(\boldsymbol{\mu}, \boldsymbol{\sigma}^2\right)$, 
whose distribution parameters are predicted by a space-channel context (SCCTX) entropy model, which will be discussed in Section 3.3.
The rate of latent representation $R(\hat{\boldsymbol{y}})$ is computed by $\mathbb{E}\left[-\log _2\left(p_{\hat{\boldsymbol{y}} \mid \hat{\boldsymbol{z}}}(\hat{\boldsymbol{y}} \mid \hat{\boldsymbol{z}})\right)\right]$. Next, the decoder  $g_s$ is used to reconstruct image from the quantized latent $\hat{\boldsymbol{y}}$ :

% Then we divide the latent $\boldsymbol{y}$ to $S$ slices $\boldsymbol{y}_0, \boldsymbol{y}_1, \cdots, \boldsymbol{y}_{S-1}$ and compute slice-wise information by:

% $$
% \begin{aligned}
% & \boldsymbol{r}_i,\left(\boldsymbol{\mu}_i, \boldsymbol{\sigma}_i\right)=e_i\left(\tilde{\boldsymbol{\mu}}, \tilde{\boldsymbol{\sigma}}, \overline{\boldsymbol{y}}_{<i}, \boldsymbol{y}_i ; \boldsymbol{\theta}_{e_i}\right) \\
% & \overline{\boldsymbol{y}}_i=\boldsymbol{r}_i+\hat{\boldsymbol{y}}_i=\boldsymbol{r}_i+Q\left(\boldsymbol{y}_i-\boldsymbol{\mu}_i\right)+\boldsymbol{\mu}_i
% \end{aligned}
% $$

% where $e_i$ and $\boldsymbol{r}_i$ represent the $i$-th network and the residual in the channel-wise auto-regressive entropy model (CAM) (Liu et al., 2023), $i=0,1, \cdots, S-1$. We concatenate the slice-wise estimated distribution parameters and obtain the holistic $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$. 

% Then, we compute $R(\hat{\boldsymbol{y}})=\mathbb{E}\left[-\log _2\left(p_{\hat{\boldsymbol{y}} \mid \hat{\boldsymbol{z}}}(\hat{\boldsymbol{y}} \mid \hat{\boldsymbol{z}})\right)\right]$ 
%
% with $p_{\hat{\boldsymbol{y}} \mid \hat{\boldsymbol{z}}}(\hat{\boldsymbol{y}} \mid \hat{\boldsymbol{z}}) \sim \mathcal{N}\left(\boldsymbol{\mu}, \boldsymbol{\sigma}^2\right)$.
%
% Next, we use the decoder $g_s$ to reconstruct image from the quantized latent $\hat{\boldsymbol{y}}$ :

\begin{equation}
\hat{\boldsymbol{x}}=g_s\left(\hat{\boldsymbol{y}} ; \boldsymbol{\theta}_{g_s}\right)
\end{equation}

Here we propose to use Bi-RWKV block for $g_a$, $g_s$, $h_a$, $h_s$ to enhance the backbone. Finally, we optimize the following training objectives:
\begin{equation}
 L = \lambda\|\boldsymbol{x}-\hat{\boldsymbol{x}}\|^2+R(\hat{\boldsymbol{z}})+R(\hat{\boldsymbol{y}})
\end{equation}
where $\lambda$ is the Lagrangian multiplier to control the rate-distortion trade-off.
% $$
% \arg \min_{\boldsymbol{\theta}_{g_a}, \boldsymbol{\theta}_{h_a}, \boldsymbol{\theta}_{g_s}, \boldsymbol{\theta}_{h_s},\left\{\boldsymbol{\theta}_{e_i}\right\}_{i=0}^{S-1} }\lambda\|\boldsymbol{x}-\hat{\boldsymbol{x}}\|^2+R(\hat{\boldsymbol{z}})+R(\hat{\boldsymbol{y}})
% $$



\subsection{Bi-RWKV Transform Block}

Following the transformer based compression methods, we integrate a Bi-RWKV block within the nonlinear transforms \( g_a \), \( g_s \), \( h_a \), and \( h_s \). Each Bi-RWKV block follows the upsampling or downsampling operations in these transforms, as shown in Figure~\ref{fig:rwkv-block}. The Bi-RWKV block consists of two branches: a spatial mix and a channel mix branch.
% The spatial mix branch captures dependencies across spatial dimensions, while the channel mix branch enables feature fusion across channels, enhancing representation quality.

\noindent\textbf{Spatial Mix}. Designed to capture long-range spatial dependencies, the spatial mix module takes an input feature map \( f \in \mathbb{R}^{H \times W \times C} \), which is reshaped into a sequence \( X \in \mathbb{R}^{T \times C} \), where \( T = H \times W \).

% \begin{gather}
%     X_s = \operatorname{Omni\text{-}Shift}(\operatorname{LayerNorm}(X)) \\
%     R_s = X_s W_{R_s}, \quad K_s = X_s W_{K_s}, \quad V_s = X_s W_{V_s} \\
%     \operatorname{wkv} = \operatorname{Re\text{-}WKV}(K_s, V_s) \\
%     O_s = \left(\sigma(R_s) \odot \operatorname{wkv}\right) W_{O_s}
% \end{gather}

The Spatial-Mix module begins with layer normalization for stable training, followed by an \textit{Omni-Shift} operation to capture 2D local context. As shown in Figure~\ref{fig:omni-shift}, the shift operation is implemented as a depth-wise 5x5 convolution with a reparameterization trick: it uses multiple kernels during training, which are merged into a single kernel for inference. The shifted representation \( X_s \) is then passed through three linear layers to produce the receptance \( R_s \), key \( K_s \), and value \( V_s \). The \textit{BiWKV} attention mechanism subsequently computes global attention \( wkv \), modulated by the sigmoid-gated receptance \( \sigma(R_s) \) and projected to form the final output \( O_s \).

\noindent\textbf{Channel Mix}. The channel mix module performs feature fusion across channels, enhancing cross-channel information flow.

% \begin{gather}
%     X_c = \operatorname{Omni\text{-}Shift}(\operatorname{LayerNorm}(X)) \\
%     R_c = X_c W_{R_c}, \quad K_c = X_c W_{K_c}, \quad V_c = \gamma(K_c) W_{V_c} \\
%     O_c = \left(\sigma(R_c) \odot V_c\right) W_{O_c}
% \end{gather}

% Starting with layer normalization and Omni-Shift, the channel mix module computes \( R_c \) and \( K_c \) via linear projections. Then, \( V_c \) is obtained from \( K_c \) using a squared ReLU activation \( \gamma(\cdot) \), implicitly creating an MLP structure. The final output \( O_c \) is formed by modulating \( V_c \) with the sigmoid-gated \( R_c \), allowing efficient channel fusion and improved model capacity.
Starting with layer normalization and Omni-Shift, the channel mix module computes \( R_c \) and \( K_c \) via linear projections. Then, \( V_c \) is obtained from \( K_c \) using a squared ReLU activation, implicitly creating an MLP structure. The final output \( O_c \) is formed by modulating \( V_c \) with the sigmoid-gated \( R_c \), allowing efficient channel fusion and improved model capacity.


\noindent\textbf{BiWKV Attention}. The WKV attention mechanism is central to the Spatial-Mix module, allowing for efficient extraction of distant dependencies with linear complexity. Originating from the Attention-Free Transformer (AFT) \cite{Zhai.2021.AFT}, this approach uses linear $KV$ attention rather than the quadratic $QK$ attention. The attention for the \( t \)-th token in AFT is given by

\begin{equation}
kv_t = \frac{\sum_{t^{\prime}=1}^T \exp(K_{t^{\prime}}) \odot V_{t^{\prime}}}{\sum_{t^{\prime}=1}^T \exp(K_{t^{\prime}})}
\end{equation}
where the values $V$ captures exponential weighted contributions from all tokens of the keys $K$. The final output is computed as \( \sigma_q(Q_t) \odot kv_t \), where \( \sigma_q(Q_t) \) applies a sigmoid gating to the query \( Q_t \).

Building on the KV attention, WKV attention~\cite{Peng.2023.RWKV} introduces channel-wise decay parameters \( w \) and \( u \). Beyond the contributions from \( K \), the parameter \( u \) amplifies the current token, while \( w \) decays the contributions of other tokens based on their distance. For visual tasks, the bidirectional WKV (BiWKV) attention~\cite{Duan.2024.Vision-RWKV} for the \( t \)-th token is defined as:

\begin{equation}
wkv_t = \frac{\sum_{i=1, i \neq t}^T e^{-(|t-i|-1) / T \cdot w + k_i} v_i + e^{u + k_t} v_t}{\sum_{i=1, i \neq t}^T e^{-(|t-i|-1) / T \cdot w + k_i} + e^{u + k_t}}
\end{equation}
where \( k_i \) and \( v_i \) represent the key and value for the \( i \)-th token. The introduced parameters allow the formulation to balance local and global dependencies effectively, adjusting token interactions based on their proximity.







% For 2D images, flattening into 1D sequences can make \( w \) sensitive to the scan order. To better retain 2D spatial structures, Recurrent WKV (Re-WKV) ~\cite{Yang.2024.Restore-RWKV}, alternates between horizontal and vertical scan directions:

% \begin{equation}
% \operatorname{Re-WKV}(K, V) = \operatorname{BiWKV}_{\mathrm{ver}}(K, \operatorname{BiWKV}_{\mathrm{hor}}(K, V)),
% \end{equation}

% where the horizontal BiWKV output serves as input for the vertical scan, enhancing global token interactions by integrating complementary spatial information. This approach improves spatial coherence and better models complex dependencies across 2D images.

\begin{figure}[t]
    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig/ERF-TCM.png}
        \caption{TCM}
        \label{fig:model-erf-sub1}
    \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig/ERF-FAT.png}
        \caption{FAT}
        \label{fig:model-erf-sub2}
    \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig/ERF-BiWKV-Shift.png}
        \caption{Bi-RWKV}
        \label{fig:model-erf-sub3}
    \end{subfigure}
  \caption{The effective receptive field (ERF) \cite{Luo.2016.ERF} visualization for the forward pass ($g_a$ \& $h_a$) of different models. A more extensively distributed dark area indicates a larger ERF.} 
  \label{fig:model-erf}
\end{figure}


\noindent\textbf{Effective Receptive Field.} The effective receptive field (ERF)~\cite{Luo.2016.ERF} describes how gradients flow from a latent location to the input image, defining the area it can perceive. A larger ERF enables the network to capture information from a broader area, which is particularly advantageous for nonlinear encoders in reducing redundancy. 

As shown in Figure~\ref{fig:model-erf}, we visualize the ERF of recent LIC models. The TCM block~\cite{Liu.2023.TCM} shows a shifted window pattern, while the FAT block~\cite{Li.2023.FAT} exhibits a locally enhanced window pattern due to its block-wise FFT design. In contrast, the RWKV block achieves a global ERF, enabling it to leverage a wider range of pixels for more effective redundancy elimination.

\subsection{RWKV Spatial-Channel Context Model}

The latent representation \( y \) in the transformed domain retains redundancies along both spatial and channel axes. Leveraging these redundancies, our entropy model correlates current decoding symbols with previously decoded ones to reduce the bit rate further. Motivated by recent advancements \cite{He.2022.ELIC, Liu.2023.TCM}, we propose an RWKV-based Spatial-Channel Context Model (RWKV-SCCTX) to model the conditional distribution of the latent variables more effectively, as shown in Fig.~\ref{fig:entropy-model}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{fig/Fig4.pdf}
  \caption{Diagram of the RWKV Spatial-Channel Context Model.} 
  \label{fig:entropy-model}
\end{figure}

In the spatial dimension, we employ a checkerboard approach to divide symbols into two groups: anchors and non-anchors. The anchor group is encoded first, and the context derived from it is then used to encode the non-anchor group, capturing spatial dependencies.

For the channel dimension, we partition the channels into \( K \) chunks to build a channel-wise context:

\begin{equation}
   \Phi_{\mathrm{ch}}^{(k)} = g_{\mathrm{ch}}^{(k)}\left(\hat{\boldsymbol{y}}^{<k}\right), \quad k=2, \ldots, K 
\end{equation}
where \( \hat{\boldsymbol{y}}^{<k} = \left\{\hat{\boldsymbol{y}}^{(1)}, \ldots, \hat{\boldsymbol{y}}^{(k-1)}\right\} \) denotes the previously decoded channel chunks. Since the initial chunks are referenced more frequently by subsequent chunks, they tend to carry the majority of essential information. Allocating fewer channels to the initial chunks helps establish a more precise conditional distribution. Thus, for a latent representation \( \hat{\boldsymbol{y}} \) with \( M \) channels, we divide it into 5 chunks \( \hat{\boldsymbol{y}}^{(1)}, \ldots, \hat{\boldsymbol{y}}^{(5)} \) with channel allocations of \{16, 16, 32, 64, \( M - 128 \)\}, respectively. 

As illustrated in Fig.~\ref{fig:entropy-model}, for the \(k\)-th chunkâ€™s \(i\)-th part, we apply a spatial context model \( g_{\mathrm{sp}}^{(k)} \) using checkerboard-masked convolutions to capture spatial context. If the part serves as an anchor, a zero context is used. Then, an RWKV-based network \( g_{\mathrm{ch}} \) is employed to model the channel-wise context \( \boldsymbol{\Phi}_{\mathrm{ch}}^{(k)} \) using decoded chunks. 

The spatial context and channel context are concatenated with the hyperprior context \( \boldsymbol{\Phi}_{hp} \). In the parameter aggregation network, this combined context is fused in a location-wise manner to predict the Gaussian distribution parameters, \( \boldsymbol{\Theta}_i^{(k)} = (\boldsymbol{\mu}_i^{(k)}, \boldsymbol{\sigma}_i^{(k)} ) \). We utilize the Channel Mix module without the Omni-Shift layer, to retain the $1 \times 1$ receptive field for causal decoding. Using the predicted entropy parameters, the latent \( {\boldsymbol{y}}_i^{(k)} \) is coded as follows:

\begin{equation}
    \hat{\boldsymbol{y}}_i^{(k)} = \operatorname{round}( {\boldsymbol{y}}_i^{(k)} - \boldsymbol{\mu}_i^{(k)}) + \boldsymbol{\mu}_i^{(k)}
\end{equation}

The decoded symbol \( \hat{\boldsymbol{y}}_i^{(k)} \) is subsequently used as context for computing \( \boldsymbol{\Phi}_{\mathrm{sp},(i+1)}^{(k)} \) or \( \boldsymbol{\Phi}_{\mathrm{ch}}^{(k+1)} \), iteratively progressing until the entire \( \hat{\boldsymbol{y}} \) is encoded.

