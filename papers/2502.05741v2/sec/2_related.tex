\section{Related Works}
\label{sec:related}


%-------------------------------------------------------------------------
\subsection{Learned Image Compression}

In the past decade, learned image compression has demonstrated remarkable potential and made a significant success. We review the prevailing methods from two aspects, transform network and entropy modeling, where transform network can be further categorized into CNN-based and Transformer-based models.

% \paragraph{CNN-based Models} 

\noindent\textbf{CNN-based Models} Some early works typically utilize the convolution neural networks with generalized divisive normalization (GDN) layers \cite{Balle17, Balle.2018.Hyperprior, Minnen.2018.Joint} in the analysis and synthesis transform to achieved good performance in image compression. Following that, attention mechanisms and residual blocks \cite{Cheng.2020.LIC, Zhou.2019.EOI} were integrated into the VAE architecture to enhance the capability of feature extraction. However, the limited receptive field constrained the further development of these models.


% \paragraph{Transformer-based Models} 

% \textbf{Transformer-based Models} Transformers have achieved remarkable success in various computer vision tasks due to its powerful non-local modeling ability. Swin-Transformer~\cite{Liu.2021.SwinT} propose to limit self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size.

% Researcher~\cite{Zhu.2021.TBTC, Lu.2022.TIC, Zou.2022.DDW} demonstrate that nonlinear transforms built on Swin Transformer, achieve potential compression efficiency compared to those built on CNNs. On this basis, Liu et al \cite{Liu.2023.TCM} proposes the TCM Block, combining transformers and CNNs to aggregate non-local and local information for more expressive nonlinear transforms. Li et al. \cite{Li.2023.FAT} propose FAT blocks, which leverages diverse window shapes to capture different frequency components, then adaptively ensemble frequency components.

\noindent\textbf{Transformer-based Models} Transformers have demonstrated notable success in various computer vision tasks and Swin-Transformer~\cite{Liu.2021.SwinT} is proposed to restricts self-attention to local windows while enabling cross-window connections to enlarge the global attention. Studies~\cite{Zhu.2021.TBTC, Lu.2022.TIC, Zou.2022.DDW} show that nonlinear transforms based on Swin-Transformer improve compression efficiency over CNNs. Building on this, Liu et al.\cite{Liu.2023.TCM} introduce the TCM Block, which combines transformers and CNNs to enhance non-local and local information aggregation. Similarly, Li et al.\cite{Li.2023.FAT} propose Frequency-aware transformer blocks that adaptively ensemble diverse frequency components captured through various window shapes. However, introducing transformer still bring large computation complexity overhead to learned image compression.

% However, the standard window self-attention used in these works has a limited window receptive field, which hinders the extraction of compact latent representations.


%-------------------------------------------------------------------------
% \subsection{Auto-regressive Entropy Modeling}
% \paragraph{Entropy Modeling} 

\noindent\textbf{Entropy Modeling} 
Entropy modeling is essential in learned image compression to eliminate the redundancy of latent features. Most previous methods leverage joint autoregressive and hyperprior models \cite{Minnen.2018.Joint}, categorized into spatial \cite{Minnen.2018.Joint} (SA), channel-wise \cite{Minnen.2020.Charm} (CA), or combined approaches \cite{He.2022.ELIC}. Recent works incorporate transformers to capture long-range dependencies, enhancing entropy precision. For instance, Qian~\emph{et al.}~\cite{Qian.2022.EntroFormer} applies global self-attention for spatial dependencies, while Koyuncu~\emph{et al.}~\cite{Koyuncu.2022.CtxFormer} uses spatial-channel attention to boost R-D performance. However, high memory and computational demands limit real-world applicability, especially for high-resolution images. Liu~\emph{et al.}~\cite{Liu.2023.TCM} integrates Swin-Transformer within a channel-wise model for added spatial dependency, though with limited R-D gains. Li~\emph{et al.}\cite{Li.2023.FAT} propose T-CA, focusing on optimized channel-wise attention.


% Entropy modeling is crucial for learned image compression models. A precise entropy model can eliminate the coding redundancy and minimize the size of compressed images. Most existing works are developed based on joint autoregression and hyperprior model \cite{Minnen.2018.Joint}, in which the auto-regression module captures the dependencies within the latent representations and reduces coding redundancy. The auto-regression modules could be classified as spatial auto-regression \cite{Minnen.2018.Joint} (SA), channel-wise autoregression \cite{Minnen.2020.Charm} (CA), and their combination \cite{He.2022.ELIC}. Recent works employ transformers to capture long-range dependency and improve the preciseness of entropy modeling. Qian et al. \cite{Qian.2022.EntroFormer} utilize global self-attention to capture long-range spatial dependency in distribution estimation. Koyuncu et al. \cite{Koyuncu.2022.CtxFormer} propose a spatial-channel attention to fully exploit latent dependency and improve R-D performance. However, the heavy memory usage and dramatic computational complexity render these methods impractical for real-world image compression, especially for high-resolution images. Liu et al. \cite{Liu.2023.TCM} incorporate Swin-Transformer into channel-wise autoregressive entropy model to capture additional spatial dependency. However, the swin-transformer increases the model size while the R-D improvement over the anchor model seems insignificant. Li et al. \cite{Li.2023.FAT} proposed T-CA concentrates on improving channel-wise attention.


%-------------------------------------------------------------------------
\subsection{Linear Attention Models}

% To address the quadratic growth in computational complexity of transformers with sequence length, linear Attention was proposed to handle long-range dependencies in linear time, thereby reducing computational costs. Various architectures such as Mamba, and RWKV have emerged in this field. Originating from the natural language processing domain, these models have extended into the computer vision field as VMamba and Vision-RWKV. This linear-complexity is Particularly effective for low-level tasks to process pixel-wise long sequences. Linear models can directly capture global dependencies in high-resolution images, with global receptive field, without the need for complex designs like window partitioning.

Several architectures with linear attention have been designed for fast training and inference. Here we mainly introduce two representative models, Mamba and RWKV. 

\noindent\textbf{Mamba}
Mamba\cite{Gu2023MambaLS, Zhu2024VisionME}, a robust sequence model grounded in state-space models (SSMs), has emerged as a prominent contender to traditional Transformers. Additionally, the introduction of VMamba\cite{Liu2024VMambaVS}, leveraging SS2D, has further enhanced the capabilities of this model. Recent studies have underscored the superior performance of Mamba-based models over Transformer-based counterparts\cite{Guo2024MambaIRAS, Ji2024DeformMambaNF}, particularly in tasks such as image classification and multimodal learning\cite{Chen2024VideoMS, Qiao2024VLMambaES, Ma2024UMambaEL}. 
Mamba has also been introduced to image compression tasks. MambaVC\cite{Qin2024MambaVCLV} builds an efficient compression network based on SSM, capturing informative global contexts.
% TODO: introduce MambaVC @donghui

% \subsection{RWKV}


% TODO: here is the introduction of related work about RWKV, add the citation to Vision0RWKV, Restoration-RWKV, etc..... @donghui
% 不是让讲RWKV的原理，是讲使用RWKV的工作；下面两段重新写一下







% Several architectures with linear attention have been designed for fast training and inference. Here we mainly introduce two representative models, Mamba and RWKV. 
% {\color{red} 
% \noindent\textbf{Mamba}
% Mamba, a robust sequence model grounded in state-space models (SSMs), has emerged as a prominent contender to traditional Transformers. Additionally, the introduction of VMamba, leveraging SS2D, has further enhanced the capabilities of this model. Recent studies have underscored the superior performance of Mamba-based models over Transformer-based counterparts, particularly in tasks such as image classification and multimodal learning.
% }
% TODO: introduce MambaVC @donghui

% \subsection{RWKV}


% TODO: here is the introduction of related work about RWKV, add the citation to Vision0RWKV, Restoration-RWKV, etc..... @donghui
% 不是让讲RWKV的原理，是讲使用RWKV的工作；下面两段重新写一下


% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.68\linewidth}
%     \includegraphics[width=\textwidth]{fig/Overview.pdf}
%     \caption{Linear Attention based Learned Image Compression.}
%     \label{fig:overview}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \centering
%     \includegraphics[width=\textwidth]{fig/RWKV-Block.pdf}
%     \caption{RWKV Block.}
%     \label{fig:block}
%   \end{subfigure}
%   \caption{} 
%   \label{fig:short}
% \end{figure*}

\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.68\textwidth}
        \centering
        \subcaptionbox{Linear Attention based Learned Image Compression.\label{fig:overview}}{
            \raisebox{0.3cm}
            {\includegraphics[width=\textwidth]{fig/Overview.pdf}}}
    \end{minipage}%
    \hfill
    % \hspace{0.03cm}
    \begin{minipage}[b]{0.3\textwidth}
        \centering
        % \hspace{0.50cm}
        \subcaptionbox{Bi-RWKV Block.\label{fig:rwkv-block}}{
            \includegraphics[width=\textwidth]{fig/RWKV-Block.pdf}}
        \hfill
        % \hspace{0.02cm}
        \vspace{0.1cm}
        \subcaptionbox{Omni-Shift Layer.\label{fig:omni-shift}}{
            \includegraphics[width=\textwidth]{fig/Omni-Shift.pdf}}
    \end{minipage}
    \caption{(a) Overview of proposed Linear Attention based Learned Image Compression (LALIC). Conv$(N,2)\downarrow$ and Deconv$(N,2)\uparrow$ represent strided down convolution and strided up convolution with $N \times N$ filters, respectively. There are $L$ identical Bi-RWKV Blocks stacked after downsample or upsample conv layer. AE, AD, and Q represent Arithmetic Encoding, Arithmetic Decoding, and Quantization. RWKV-SCCTX is the proposed RWKV-based Space-Channel Context model, illustrate in Fig.\ref{fig:entropy-model}. (b) The details of the Bi-RWKV Block. Omni-Shift\cite{Yang.2024.Restore-RWKV} denotes a reparameterized 5x5 depthwise convolution to capture local context. And BiWKV is the Bidirectional Attention proposed by \cite{Duan.2024.Vision-RWKV}.}
    \label{fig:main}
    \vspace{-15pt}
\end{figure*}

\noindent\textbf{RWKV}
%
The Receptance Weighted Key Value (RWKV) model~\cite{Bo.rwkv.2023, Datta2024TheEO} has emerged as an efficient alternative to Transformers.
RWKV offers distinctive advantages over standard Transformers, including a WKV attention mechanism for building long-range dependencies with linear computational complexity and a token shift layer to capture local context effectively. 
%
Recent study\cite{Peng2024EagleAF} shows that RWKV achieves performance on par with, or even exceeding that of both Transformers and Mamba in NLP tasks.
Vision-RWKV\cite{Duan2024VisionRWKVEA} has successfully transitioned RWKV's capabilities from NLP to vision tasks, demonstrating superior performance compared to vision Transformers by incorporating bidirectional WKV attention and quad-directional token shift mechanisms to harness spatial information in 2D images efficiently.
Several models derived from RWKV and Vision-RWKV have been developed and applied to diverse vision-related tasks. For instance, RWKV-SAM\cite{Yuan2024MambaOR} is designed for general image segmentation, while BSBP-RWKV\cite{Zhou2024BSBPRWKVBS} is specialized for medical image segmentation. Diffusion-RWKV\cite{Fei2024DiffusionRWKVSR} has been introduced for image generation, effectively reducing spatial aggregation complexity, while Restore-RWKV\cite{Yang2024RestoreRWKVEA} is dedicated to medical image restoration.


% Furthermore, RWKV-CLIP\cite{Gu2024RWKVCLIPAR} is customized for vision-language representation learning.
% RWKV has also shown strong performance in 3D-related tasks: Point-RWKV\cite{He2024PointRWKVER} extends RWKV to 3D point cloud learning, and OccRWKV\cite{Wang2024OccRWKVRE} is proposed for 3D semantic occupancy prediction. 




% {\color{red} 
% \noindent\textbf{RWKV}
% %
% The Receptance Weighted Key Value (RWKV) model~\cite{Bo.rwkv.2023}, originating from the field of NLP, has emerged as an efficient alternative to Transformers. RWKV offers distinctive advantages over standard Transformers, including a WKV attention mechanism for building long-range dependencies with linear computational complexity and a token shift layer to capture local context effectively. Vision-RWKV has successfully transitioned RWKV's capabilities from NLP to vision tasks, demonstrating superior performance compared to vision Transformers by incorporating bidirectional WKV attention and quad-directional token shift mechanisms to harness spatial information in 2D images efficiently.

% Models based on Self-Attention, commonly referred to as Transformers, encounter two primary challenges. 
% %
% Firstly, the scaled dot-product attention mechanism in Self-Attention does not incorporate positional information between tokens. Secondly, each token in the sequence requires mutual interactions, leading to increased complexity in training and inference. RWKV effectively addresses these issues by replacing Self-Attention with WKV computation. The WKV computation process directly integrates positional encoding with translational invariance into tokens, eliminating the need for additional positional information. Moreover, tokens no longer need to interact with each other; the WKV computation only performs independent transformations and accumulations for each token. Importantly, WKV computation can also achieve a caching function akin to Self-Attention, significantly enhancing time complexity during consecutive inferences. This implementation maintains a fixed-width internal state of the model, aligning with the characteristics of RNNs.
% }