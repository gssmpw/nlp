\section{Introduction}
\label{sec:intro}



% These standards have achieved impressive coding efficiency with more and more complex hand-engineered coding tools. Historically, almost double compression ratio at the same quality level is expected for a new generation standard. However, after years of efforts, further improvement on coding efficiency becomes even more difficult relative to its predecessor.

%Para1: Learned image compression is important, and many recent LIC has achieved better results than traditional codecs.
Recently learned image compression (LIC) has achieved great success to realize efficient image transmission and storage by outperforming classical compression algorithms, including  JPEG~\cite{Wallace.1992.JPEG}, JPEG2000~\cite{Skodras.2001.JP2K}, High Efficiency Video Coding (HEVC/265)~\cite{Sullivan.2012.HEVC} and Versatile Video Coding (VVC/266)~\cite{Bross.2021.VVC}.
Mainstream LIC methods typically follow the non-linear transform coding framework \cite{Balle.2021.NTC}, combining learned entropy models with transform networks. Some pioneer LIC methods have investigated the adaptive context modeling, such as Hyerprior~\cite{Balle.2021.NTC}, spatialautoregressive model~\cite{Minnen.2018.Joint}, spatial checkerboard model, channel autoregressive models~\cite{Minnen.2020.Charm} or their alternative combinations~\cite{He.2022.ELIC}. Other representative LIC methods have explored better nonlinear analysis and synthesis transform networks, including widely-used residual convolution blocks~\cite{Cheng.2020.LIC}, swin-transformer~\cite{Liu.2021.SwinT}, and the mixture models of the above blocks~\cite{Liu.2023.TCM, Li.2023.FAT}, and even emerging Mamba blocks~\cite{Qin2024MambaVCLV}. 
The development of entropy models and transform networks have largely boosted the coding performance of learned image compression. % and even video compression algorithms. 


\begin{figure}
  \includegraphics[width=1.0\linewidth]{fig/Fig1_v6.pdf}
  \caption{BD-rate vs Decoding Latency on Kodak dataset, where our proposed LALIC achieves the competitive BD-rate with moderate complexity. The Left-Top is better.} 
  \label{fig:teaser}
\end{figure}



% Para2: Low-Complexity is high-desired.....
% TODO: add the citation for VMamba and Vision-RWKV.....
% However, after several years of development, each coding gain has come at the expense of increased computational complexity. Transformer has already been the mainstream backbone for learned image compression methods.
%
% To address the quadratic growth in computational complexity of transformers with sequence length, linear Attention was proposed to handle long-range dependencies in a linear complexity, thereby reducing computational costs largely. Various architectures such as Mamba, and RWKV have emerged in this field. Originating from the natural language processing domain, these models have extended into the computer vision field as VMamba and Vision-RWKV successfully. This linear-complexity is particularly effective for low-level tasks to process pixel-wise long sequences and capture global dependencies in high-resolution images, with global receptive field, without the need for complex designs like window partitioning. Inspired from this, we propose to use the linear attention model for learned image compression. 
%
Despite significant progress, each percent of coding gain in LIC has come with increased computational complexity. Meanwhile, transformers now have established as the mainstream backbone. To mitigate the quadratic complexity growth of transformers with sequence length, linear attention mechanisms were introduced to capture long-range dependencies with linear complexity, significantly reducing computational costs. Architectures such as Mamba\cite{Gu2023MambaLS} and RWKV\cite{Peng.2023.RWKV}, originally from natural language processing, have successfully expanded into computer vision as VMamba\cite{Liu2024VMambaVS} and Vision-RWKV\cite{Duan.2024.Vision-RWKV}. This linear complexity is particularly advantageous for low-level tasks that require pixel-wise processing of long sequences and global dependency capture in high-resolution images, achieving a global receptive field without complex strategies like window partitioning. Inspired by these advances, we propose utilizing linear attention in learned image compression.

% TODO@cheng: explain the relation between linear attention and compression 
% Inspired from directional intra-prediction modes from traditional codecs, XXXX


% introduce the linear attention.....

% Recently, some pioneer works have found image compression task can be well solved benefiting from end-to-end deep learning techniques, because it can be formulated as a rate-distortion optimization problem. The performance have rapidly caught up to the latest coding standard VVC, by two remarkable features. The first one is end-to-end nonlinear neural network(NN)-based optimization. According to thorough analysis in~\cite{Balle.2021.NTC}, nonlinear transforms permit implementing non-uniform quantization using a uniform quantizer to adapt the shapes of quantization bins to fit the source distributions with more flexibility. It allows neural networks to approximate optimal transforms for various types of images, whose source distributions are far from Gaussian~\cite{IEEEexample:transformcoding}. The other feature is a learnable context-adaptive entropy model, which has greatly improved the performance of learned image compression. One notable work is called a scale hyperprior~\cite{Balle.2018.Hyperprior}, which introduced a hyper autoencoder to encode side bits to estimate conditional distributions for latent representation. Particularly, they discussed the relation between compression and variational Bayesian approaches to give a probabilistic interpretation for entropy modeling. Then, many studies have investigated several generalized forms of single Gaussian model to provide more accurate and flexible context models. Basically, we can think inherent spatial correlation within images are partially removed by nonlinear NN-based transforms, and then partially reduced by learnable entropy models. The combination of them eventually determines end-to-end coding efficiency. Specifically, sparse features extracted by downscaling-based nonlinear transforms can ease the burden of entropy modeling. Meanwhile, accurate entropy modeling can permit more correlated features at a fixed rate constraint to help the decoder for a better image reconstruction. Therefore, when a more powerful entropy model is used, we need to rethink the best combination of nonlinear transforms and entropy modeling. 



% Para3: our motivation
In this work, we introduce LALIC, a novel Linear Attention-based Learned Image Compression architecture, leveraging RWKV's linear complexity advantage. Specifically, we propose Bi-RWKV blocks, utilizing Spatial-Mix and Channel-Mix modules to achieve more compact feature extraction, and incorporate Omni-Shift\cite{Yang.2024.Restore-RWKV} module to adapt to two-dimensional latent representations. Furthermore, we propose a RWKV-based Spatial-Channel ConTeXt Models (RWKV-SCCTX), that leverages the Bi-RWKVs to modeling the correlation between neighboring features effectively, to further improve the RD performance.
%
Experimental results demonstrate our proposed architecture achieves competitive rate-distortion performance with faster decoding speed in terms of widely-used PSNR and MS-SSIM quality metrics. 

% Para4: our contribution
To our knowledge, our work is the first work to utilize efficient RWKV models with linear attention for learned image compression. 
RWKVs are designed for fast inference and shows competitive compression performance compared to the state-of-the-art approaches as illustrated in Figure~\ref{fig:teaser}. In summary, our contributions are listed as follows.
%
\begin{itemize}
    \item We propose a new transform backbone with Bi-RWKV Blocks, which utilize the Spatial-Mix and Time-Mix to achieve more compact features extraction and apply the Omni-Shift module to adapt to two-dimensional latent representation.
    \item We propose a RWKV-based Spatial-Channel ConTeXt model (RWKV-SCCTX), that leverages Bi-RWKVs to modeling the correlation between neighboring features effectively, to further improve the RD performance.
    \item Experimental results demonstrate that our method achieves competitive RD performances by outperforming VTM-9.1 by -15.26\%, -15.41\%, -17.63\% in BD-rate on Kodak, CLIC and Tecnick datasets.
\end{itemize}







