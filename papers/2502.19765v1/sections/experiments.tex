\section{Experiments}

We evaluate EdiText's effectiveness by comparing to baselines across four tasks: toxicity control (toxifying and detoxifying text) and sentiment control (converting negative sentiment to positive and vice versa). This section outlines the datasets, baseline, and evaluation metrics used in the experiments.

\subsection{Datasets}

For the toxicity control, we utilize the supervised fine-tuning (SFT) subset of the Implicit Toxicity dataset \cite{wen-etal-2023-unveiling}. This dataset contains $13,953$ pairs of non-toxic, implicitly toxic, and explicitly toxic responses to given questions. We only use the explicit toxic and non-toxic responses to train a single conditional text generation model capable of generating both toxic and non-toxic texts. We use $12,953$ samples for training and reserve the remaining $1,000$ samples for evaluation.

For the sentiment control, we use the Enron Email Corpus \cite{10.1007/978-3-540-30115-8_22}, which consists of $76,551$ unlabeled email samples. We first classify each sample in the dataset into one of two sentiments using a RoBERTa-based sentiment classifier\footnote{https://huggingface.co/siebert/sentiment-roberta-large-english}. We then train a conditional model using these pseudo labels. The dataset is divided into the train, validation, and test sets following the same methodology as \citet{Horvitz_Patel_Callison-Burch_Yu_McKeown_2024}, and model performance is evaluated with the same test set.

\subsection{Implementation Details}
\label{implementation details}
We train our autoencoder and diffusion models using the same architecture and hyperparameters as in \citet{lovelace2023latent} for each task. All autoencoders are trained with a batch size of $256$ for $50$K iterations. The diffusion models are trained with a batch size of $128$ for $50$K iterations for the toxicity tasks and $100$K iterations for the sentiment tasks. For evaluation, we set the timesteps $T=250$ and the length and hidden dimension of the latent representation to $L=32$ and $D=64$, respectively. 

\subsection{Models}

\textbf{EdiText-CE and EdiText-FE.} 
We name our editing methods as EdiText-CE for coarse-level editing and EdiText-FE for fine-level editing. The control parameters $t_{CE}$ and $t_{FE}$ are adjusted within a range below the total timestep $T=250$ to balance coarse- the fine-level modifications. Specifically, $t_{CE}$ is set between 125 to 225 for EdiText-CE, while $t_{FE}$ is adjusted between 25 to 175 for EdiText-FE, where we empirically found the editing to be effective and aligned with intended levels.

\textbf{ParaGuide.} We use ParaGuide \cite{Horvitz_Patel_Callison-Burch_Yu_McKeown_2024} as our NAR baseline. It edits text by guiding a diffusion-based text model with an attribute classifier. Using the official implementation\footnote{https://github.com/zacharyhorvitz/ParaGuide}, we apply their checkpoints for sentiment control tasks. For toxicity control tasks, we train ParaGuide on the Implicit Toxicity dataset and leverage one of the publicly available toxicity classifiers\footnote{https://huggingface.co/s-nlp/roberta\_toxicity\_classifier} for guidance, ensuring a fair comparison. 
We set the guidance strength $\lambda$, which controls the extent to which the classifier influences the diffusion model, to $200$, $1e4$, and $1e5$, following their established strategy.

\textbf{Qwen2.5-0.5B-Instruct.}
Our final baseline, Qwen2.5-0.5B-Instruct \cite{qwen2.5}, is an autoregressive (AR) model. Despite being one of the smallest AR models, it still has a larger parameter count compared to others. To control the degree of editing, we use two instruction prompts: a standard prompt for moderate editing and an amplified version (denoted as Amp.) for stronger edits. The prompt template and additional details can be found in Appendix \ref{app:qwen_template}.

\subsection{Evaluation Metrics}
To evaluate the generated samples from each method, we calculate the following metrics, which are often used in previous works \cite{mireshghallah-etal-2022-mix, gehman-etal-2020-realtoxicityprompts, wen-etal-2023-unveiling}.

\textbf{Retention Rate.} To evaluate how well the generated sentences retain the reference, we employ three metrics. The first is Hamming Distance, which measures the differences between the tokenized generated samples and the reference. The second is SacreBLEU \cite{post-2018-call}, which assesses the similarity between the generated samples and the reference. Lastly, we use BERTScore \cite{Zhang2020BERTScore} to evaluate the semantic similarity.

\textbf{Reflection Rate.} To evaluate how well the edited text reflects the target attribute, we use different metrics for each task. For toxicity control, we assess the toxicity of generated samples with off-the-shelf classifiers. Specifically, we measure toxicity using two commonly used APIs: Moderation API\footnote{https://platform.openai.com/docs/models/moderation} and Perspective API\footnote{https://perspectiveapi.com/}. For sentiment control, we use a sentiment classifier\footnote{https://huggingface.co/michelecafagna26/gpt2-medium-finetuned-sst2-sentiment} to calculate the sentiment classification accuracy.
