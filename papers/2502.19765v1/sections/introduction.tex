\section{Introduction}

Editing tasks, which involve modifying given reference data in a desired direction, have been widely explored \cite{bar2022text2live, 9257074}, providing versatility and convenience. With recent advances in diffusion models \cite{DDPM} across various domains \cite{videoworldsimulators2024, evans2024fast, Rombach_2022_CVPR}, numerous diffusion-based editing methods have been introduced for various types of data with diverse attributes \cite{hertz2022prompt, NEURIPS2023_e1b619a9}. Specifically, diverse approaches have been proposed for editing visual data at various levels, from global attributes \cite{meng2022sdedit} to fine-grained details \cite{kawar2023imagic, mokady2022null}.

Editing methods have also emerged in the text domain \cite{du-etal-2022-read, mallinson-etal-2020-felix, malmi-etal-2019-encode, Martinez_Dass_Kurohashi_Jurafsky_Yang_2020}.
Although autoregressive (AR) language models \cite{NEURIPS2020_1457c0d6, JMLR:v21:20-074, touvron2023llama} generally outperform non-autoregressive (NAR) counterparts \cite{Clark2020ELECTRA:, devlin-etal-2019-bert, li2022diffusionlm, liu2019roberta}, NAR models have shown strengths in controlled generation \cite{li2022diffusionlm, mireshghallah-etal-2022-mix}, where shifting to the target distribution can be easily achieved with relatively small amount of data and training.
The ability of NAR models to facilitate such shifts can also be effectively applied to text editing.  For example, one might need an edit to shift the overall tone of a report from informal to formal, or perform a local edit to change only specific words. In line with these possibilities, a number of studies \cite{Horvitz_Patel_Callison-Burch_Yu_McKeown_2024, mallinson-etal-2020-felix, mireshghallah-etal-2022-mix, Martinez_Dass_Kurohashi_Jurafsky_Yang_2020} have adapted NAR-based controlled generation methods to text editing. 
Although these models are capable of reflecting the target attributes in text to some extent, their editing techniques either lack the ability to control the degree of editing or are limited to a narrow range, which restricts their versatility.

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/Figure2.pdf}
    \caption{Comparisons of various text-based tasks requiring control. Our text editing method focuses on adjusting degree of editing, enabling both coarse- and fine-grained modification.}
    \label{fig:TextEditing}
    \vskip -0.2in
\end{figure*}


Recently, the advancement of diffusion models has extended into the discrete text domain, which can be categorized into two main types \cite{zou2023survey}: discrete diffusion \cite{austin2021structured, he-etal-2023-diffusionbert, hoogeboom2021argmax} and embedding diffusion \cite{gao2022difformer, gong2022diffuseq, li2022diffusionlm}. Embedding diffusion models, specifically, map discrete tokens to continuous embeddings and model them through a Gaussian diffusion process, able to leverage various advancements from the diffusion models in other continuous domains \cite{li2022diffusionlm, lovelace2023latent, zhang2024language}. 
While this advantage demonstrates the potential of applying several techniques from continuous diffusion models \cite{chen2023analog, dhariwal2021diffusion, ho2021classifierfree}, diffusion-based text editing has not yet been sufficiently explored.

In this work, we propose EdiText, an embedding diffusion-based text editor that supports both coarse- and fine-grained editing. We adapt SDEdit \cite{meng2022sdedit}, originally for images, to enable global-level text modifications. Furthermore, by reinterpreting self-conditioning \cite{chen2023analog}, a technique to enhance the performance of embedding diffusion, we introduce a novel fine-grained text editing approach that allows precise adjustments in the degree of editing. Additionally, by integrating the proposed coarse-grained and fine-grained editing techniques, our approach offers more comprehensive coverage. This integration bridges the gaps within the broad coverage of coarse-level editing by enabling fine-level adjustments, ensuring a more complete and precise editing process.

We demonstrate the effectiveness of our proposed editing methods across various tasks, achieving superior editing performance compared to the baseline model. We additionally showcase our model's capability to edit across a wider and more nuanced range by combining both techniques.

