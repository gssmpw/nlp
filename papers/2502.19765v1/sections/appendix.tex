\clearpage
\section{Appendix}
\label{app:appendix}


\subsection{Algorithm for Integrated Method}
\label{app:integrated}

\setcounter{algorithm}{2}
\begin{algorithm}[H]
\caption{Our Integrated Editing}
\label{alg:integrated editing}
\textbf{Input:} diffusion model $x_\theta(x_t,t,\cdot)$, noise schedule $\alpha_t$, language encoder and decoder $E(\cdot)$ and $D(\cdot)$, total timestep $T$, reference text $w_{ref}$, timestep for coarse-level editing $t_{CE}$, timestep for fine-level editing $t_{FE}$, target attribute $c$

\textbf{Output:} edited text \(w_{e}\)

\begin{algorithmic}[1] %[1] enables line numbers
\STATE $x_0 = E(w_{ref})$ \quad\# encoded reference text
\STATE $\lambda_{t_{CE}}=1-\alpha_{t_{CE}}$
\STATE $\epsilon_{t_{CE}} \sim N(0,I)$
\STATE $x_{t_{CE}} = \sqrt{\alpha_{t_{CE}}}x_0 + \sqrt{\lambda_{t_{CE}}}\epsilon_{t_{CE}}$
\FOR{$t = t_{CE}, ..., 1$}
\IF{$t_{FE} \leq t \leq t_{CE}$}
\STATE $\hat{x}_{0,t}=E(w_{ref})$
\ELSE
\STATE $\hat{x}_{0,t}=\hat{x}_{0,t+1,SC}$
\ENDIF
\STATE $\lambda_t=1-\alpha_t$
\STATE $\hat{x}_{0,t,SC}=x_\theta(x_t,t,\hat{x}_{0,t}, c)$
\STATE $\epsilon_\theta(x_t,t,\hat{x}_{0,t}, c)=\lambda_t^{-0.5}(x_t-\sqrt{\alpha_t}\hat{x}_{0,t,SC})$
\STATE $x_{t-1} = \sqrt{\alpha_{t-1}} \hat{x}_{0,t,SC} + \sqrt{\lambda_{t-1}}\epsilon_\theta(x_t,t,\hat{x}_{0,t}, c)$
\ENDFOR
\STATE \textbf{return} $w_e=D(x_0)$
\end{algorithmic}
\end{algorithm}

The integrated editing approach, which combines coarse- and fine-level editing to leverage the strengths of both methods, is proposed in Section \ref{subsection:local editing} and demonstrated in Algorithm \ref{alg:integrated editing}. In this approach, coarse-level editing is applied first, followed by fine-level editing. Similar to our coarse-level editing method, noise is added to the encoded reference text based on $t_{CE}$, while $t_{FE}$ is adjusted to finely control the degree of editing. The results are presented in Section \ref{results:fused}.


\subsection{Editing Prompt Template for Qwen2.5-0.5B-Instruct}
\label{app:qwen_template}
Qwen2.5-0.5B-Instruct is an auto-regressive (AR) model that relies on a specialized instruction prompt for editing tasks. We use the editing prompt shown in Figure \ref{app:editing_prompt}, with slight modifications depending on the task. For instance, in the toxifying task, we adjust the instruction to "in a toxic manner" and provide a non-toxic reference as input. Additionally, to assess the extent of editing performed by the AR model, we experiment with an amplified prompt template that intensifies the editing effect by appending "as much as possible" to the second sentence.

\begin{figure*}[ht]
\begin{center}
\begin{tcolorbox}[colback=white, coltext=black, title=\textbf{Editing Prompt for Qwen2.5-0.5B-Instruct}]
\begin{Verbatim}[breaklines=true]
You are given a reference sentence from a user. Your task is to edit the reference in a {} manner.
**Output Format**
- Your output **must** follow the structure below (each on a new line, without extra explanation or commentary):
  Reference: <The user's given reference>
  Edited Reference: <The system's answer edited in a {} manner>

Reference: {}
\end{Verbatim}
\end{tcolorbox}
\end{center}
\caption{Our prompt template to modify the reference according to a specific attribute.}
\label{app:editing_prompt}
\end{figure*}


\subsection{Additional Results}
\label{app:additional_results}

\textbf{Mix-and-Match.} We employ NAR baseline other than ParaGuide \cite{Horvitz_Patel_Callison-Burch_Yu_McKeown_2024}, Mix-and-Match (M\&M) \cite{mireshghallah-etal-2022-mix}. M\&M is an energy-based model (EBM) that generates text by leveraging classifier scores for the target attributes. Given a reference sentence $X'$, the energy of edited sentence $X$ is defined as follows:
\begin{equation}
\begin{split}
  E(X) =\; & E_{\mathrm{MLM}}(X) \\
          & + \alpha \cdot E_{\mathrm{HAM}}(X, X') \\
          & + \beta \cdot E_{\mathrm{DISC}}(X).
\end{split}
\end{equation}
\noindent where $E_{MLM}$, $E_{HAM}$, and $E_{DISC}$ denote the MLM score, Hamming Distance, and discriminator score, respectively.
We use the official implementation\footnote{https://github.com/mireshghallah/mixmatch} along with its hyperparameter configurations to perform editing with RoBERTa-large \cite{liu2019roberta}, using a discriminator identical to the one that is used in ParaGuide. In Mix-and-Match (DISC), a larger weight is assigned to the discriminator score, whereas in Mix-and-Match (HAM), greater emphasis is placed on Hamming distance. 

\textbf{Human Evaluation on Enron Sentiment.} 
Beyond the metrics presented in the main paper, we conduct a human evaluation of EdiText and several baseline models on the task of modifying Enron data to a negative sentiment using Amazon Mechanical Turk. A total of 52 participants provided four responses each, resulting in 208 responses.

Each participant was shown sentiment-edited outputs from EdiText and three baselines that achieved comparable levels of attribute shifting toward the negative target. They were asked to rate how well the semantic content of the reference text was preserved in each output using a 5-point Mean Opinion Score (MOS). The results are summarized in Table \ref{table:human_evaluation}. 

The results show that EdiText ranked highest in preserving semantic content, followed by Qwen, M\&M DISC and ParaGuide. This suggests that, in addition to its broad editing coverage, EdiText maintains comparable or slightly better semantic retention than other baseline models when controlling for similar levels of text modification.

\begin{table}[]
\small
\centering
\begin{tabular}{l|c}
\toprule
\textbf{Method ($\rightarrow$ Negative)}  & \textbf{Human Evaluation (MOS)} \\ \midrule
EdiText-CE ($t_{CE}=175$)                 & 2.78 $\pm$ 0.18                 \\
ParaGuide ($\lambda=10,000$)               & 1.46 $\pm$ 0.12                 \\
Qwen2.5-0.5B-Instruct                     & 2.54 $\pm$ 0.19                 \\
M\&M DISC                                 & 2.47 $\pm$ 0.17                 \\ 
\bottomrule
\end{tabular}
\vskip -0.05in
\caption{Human evaluation (Mean Opinion Score, MOS) results for negative style conversion. Participants assess how well the semantic content is preserved.}
\label{table:human_evaluation}
\vskip -0.2in
\end{table}


\begin{table*}[ht]
\fontsize{8}{11}\selectfont
\centering
\begin{tabular}{l|ccc|ccc}
\toprule
\multirow{3}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{Informal $\rightarrow$ Formal}}                   & \multicolumn{3}{c}{\textbf{Formal $\rightarrow$ Informal}}                    \\ \cmidrule{2-7} 
                                 & \multicolumn{2}{c|}{\textbf{Retention Rate}}               & \textbf{Formality} & \multicolumn{2}{c|}{\textbf{Retention Rate}}               & \textbf{Informality} \\ \cmidrule{2-7} 
                                 & \textbf{Hamming $(\downarrow)$} & \multicolumn{1}{c|}{\textbf{BERT $(\uparrow)$}} & \textbf{Accuracy $(\uparrow)$}  & \textbf{Hamming $(\downarrow)$} & \multicolumn{1}{c|}{\textbf{BERT $(\uparrow)$}} & \textbf{Accuracy $(\uparrow)$}  \\ \midrule
ParaGuide ($\lambda=200$)        & 7.714           & \multicolumn{1}{c|}{0.8982}             & 0.43               & 16.240           & \multicolumn{1}{c|}{0.8943}             & 0.868               \\
ParaGuide ($\lambda=1,000$)      & 8.611           & \multicolumn{1}{c|}{0.8793}             & 0.395              & 17.406           & \multicolumn{1}{c|}{0.8863}             & 0.930               \\
ParaGuide ($\lambda=10,000$)     & 9.298           & \multicolumn{1}{c|}{0.8722}             & 0.465              & 18.432           & \multicolumn{1}{c|}{0.8767}             & 0.978               \\ \midrule
Mix-and-Match (DISC)             & 6.685           & \multicolumn{1}{c|}{0.6692}             & 0.181              & 9.326            & \multicolumn{1}{c|}{0.9060}             & 0.702               \\
Mix-and-Match (HAM)              & 5.568           & \multicolumn{1}{c|}{0.6772}             & 0.158              & 6.502            & \multicolumn{1}{c|}{0.9207}             & 0.646               \\ \midrule
Qwen2.5-0.5B-Instruct            & 17.473          & \multicolumn{1}{c|}{0.8671}             & 0.770              & 18.402           & \multicolumn{1}{c|}{0.9042}             & 0.188               \\
Qwen2.5-0.5B-Instruct (Amp.)      & 18.152          & \multicolumn{1}{c|}{0.8634}             & 0.807              & 16.634           & \multicolumn{1}{c|}{0.9099}             & 0.236               \\ \midrule
EdiText-CE ($t_{CE}=175$)        & 5.621           & \multicolumn{1}{c|}{0.9294}             & 0.243              & 12.724           & \multicolumn{1}{c|}{0.9141}             & 0.716               \\
EdiText-CE ($t_{CE}=200$)        & 9.033           & \multicolumn{1}{c|}{0.8909}             & 0.471              & 18.992           & \multicolumn{1}{c|}{0.8637}             & 0.930               \\
EdiText-CE ($t_{CE}=225$)        & 13.735          & \multicolumn{1}{c|}{0.8454}             & 0.747              & 23.052           & \multicolumn{1}{c|}{0.8260}             & 0.936               \\ \midrule
EdiText-FE ($t_{FE}=25$)         & 11.893          & \multicolumn{1}{c|}{0.8874}             & 0.685              & 16.194           & \multicolumn{1}{c|}{0.8857}             & 0.942               \\
EdiText-FE ($t_{FE}=75$)         & 12.860          & \multicolumn{1}{c|}{0.8795}             & 0.706              & 17.404           & \multicolumn{1}{c|}{0.8763}             & 0.956               \\
\bottomrule
\end{tabular}
\caption{Quantitative results of coarse- and fine-level formality editing on Enron data. \textbf{Hamming} represents the Hamming Distance, while \textbf{Formality} refers to formality classification accuracy. The term \textbf{BERT} denotes BERTScore, and Î» denotes the guidance strength for ParaGuide. DISC and HAM indicate whether the model prioritizes the discriminator or Hamming distance, respectively, and Amp. refers to the amplified prompt setup.}
\label{tab:table_formal}
\vskip -0.1in
\end{table*}



\textbf{Enron Formality Dataset.} In addition to the experiments presented in the main text, we conduct a task that transforms Enron data into either a formal or informal style. Formality levels were measured using the publicly available formality classifier\footnote{https://huggingface.co/s-nlp/roberta-base-formality-ranker}. The results are shown in Table \ref{tab:table_formal}.

The additional experimental results are consistent with those in Section \ref{results:respective}. Compared to other baselines, EdiText-CE exhibits the largest variation in retention rate and formality score, confirming its broadest coverage. Furthermore, in terms of reflection rate (measured by the formality classifier score), EdiText achieves comparable or superior performance at similar retention rates, demonstrating its effectiveness as a standalone editing method.

\textbf{Qualitative Samples.}
In Section \ref{results:fused}, we verified the effects of our editing methods. To further demonstrate how each editing method influences real-world samples and how the two methods synergize when integrated, we provide editing examples of qualitative samples given a reference data in Table \ref{tab:qualitative_samples}. Given a non-toxic reference sample, ``Thatâs a valid concern. Itâs important to consider all the available information before making a decision. Thank you for bringing that up.'', the text toxifying task is performed at the coarse-level, fine-level, and using the integrated method to edit the reference response towards the toxic direction.

Our qualitative samples demonstrate the same trend as the results shown in Section \ref{results:fused}. Our baseline, ParaGuide, produces answers similar to the completely non-toxic reference even when the control hyperparameter Î» is varied. In contrast, the coarse-level editing method changes so rapidly with variations in the $t_{CE}$ value that it becomes difficult to discern the original form of the reference text. Meanwhile, the fine-level editing method retains much of the original text while allowing for granular changes as the $t_{FE}$ value is adjusted. Finally, the integrated editing approach combines the strengths of both coarse- and fine-level editing, enabling precise control over the editing degree by adjusting both the $t_{CE}$ and $t_{FE}$ values.

\begin{table*}[ht]
\fontsize{8}{11}\selectfont
\centering
% \small
\begin{tabular}{p{4.3cm}|p{10.9cm}}
\toprule
\textbf{Data Type}                          & \textbf{Text}                                                                                                                     \\ \midrule
Reference Text                        & That's a valid concern. It's important to consider all the available information before making a decision. Thank you for bringing that up.                   \\ 
\midrule
Coarse-level $(t_{CE}=125)$            & That's a valid concern. It's important to consider all the available information before making a decision. Thank you for bringing it up.  
                     \\
Coarse-level $(t_{CE}=200)$           &  That's a terrible idea. It's important to consider all the damn ideas they have before making a decision. Shame on you for bringing it up.                 \\
Coarse-level $(t_{CE}=225)$           &  That book is a complete disaster. Only a complete idiot would even believe in that Barbie doll. It's a waste of time and time celebrating and glorifying that it's ruined again.                                            \\ 
\midrule
Fine-level $(t_{FE}=50)$              &  That's a terrible idea. It's important to consider all the damn unnatural information before making a decision. Shame on you for bringing it up.                                        \\
Fine-level $(t_{FE}=125)$             &  That's a terrible idea. We should consider all the damn chemical information before making a decision. Shame on you for bringing it up. 
                                                   \\
Fine-level $(t_{FE}=200)$             &  That's a terrible idea. Why don't we just smash the damn Muslims and use them in a fake name? They should be ashamed of themselves for getting away with marriage. 
                                                \\ 
\midrule 
Integrated $(t_{CE}=225, t_{FE}=20)$ &  That's a terrible idea. It's important to consider all the damn information before making a decision. Shame on you for bringing it up. 
                               \\
Integrated $(t_{CE}=225, t_{FE}=110)$ &  That's a terrible idea. We should consider all the damn information before making a decision. Shame on you for bringing it up. 
                               \\
Integrated $(t_{CE}=225, t_{FE}=180)$ &  That's a terrible idea. We should just drain all the damn drugs to make a decision. Shame on you for bringing it up. 
                                              \\ 
\midrule
ParaGuide ($\lambda=200$)             & That's a valid concern. It's important's to consider all available information before making a decision.                                                            \\
ParaGuide ($\lambda=1000$)            & That's a valid concern. It's important to consider all available information making a decision.                                                                                                          \\
ParaGuide ($\lambda=10000$)           & That a valid concern. It's always important to consider all factors,                                                                                                                     \\
\bottomrule
\end{tabular}
\caption{Qualitative samples for toxifying task for a given reference non-toxic data.}
\label{tab:qualitative_samples}
\vskip -0.2in
\end{table*}






\subsection{Extension to Context-Conditional EdiText}
Following the problem formulation for editing of ParaGuide, EdiText performs editing using only the reference and target attribute label, excluding any additional information, such as questions from the Implicit Toxicity dataset.
However, in some cases, editing should be applied while considering additional context, such as a question.
To address this, we additionally train a question-conditional EdiText on Implicit Toxicity dataset by conditioning the question on the original EdiText. The results are presented in Figure \ref{fig:question_cond}, Table \ref{tab:question_cond_detox}, \ref{tab:question_cond_tox}, and \ref{tab:question_cond_samples}.

The results exhibit a pattern similar to the original EdiText, which is conditioned solely on the class label. By examining the qualitative samples, we observe that question-conditional EdiText, which additionally considers the question, performs editing that takes this context into account.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/Detoxify_fused_EdiText_Seq2Seq.pdf}
    \includegraphics[width=0.9\linewidth]{figures/Toxify_fused_EdiText_Seq2Seq.pdf}
    \caption{Hamming Distance - Moderation Toxicity curve for coarse-level vs. integrated editing in \textit{question-conditional} EdiText. The upper figure maps to detoxifying task while the lower maps to toxifying task.}
    \label{fig:question_cond}
\end{figure}

\begin{table*}[ht]
    \fontsize{9}{11}\selectfont
    \centering
    % \small
    \begin{tabular}{l|cc|cc}
        \toprule
        \multirow{2}{*}{\textbf{Method}}    & \multicolumn{2}{c|}{\textbf{Retention Rate}} & \multicolumn{2}{c}{\textbf{Toxicity}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
                                   & \textbf{Hamming} $(\downarrow)$        & \textbf{SacreBLEU} $(\uparrow)$        & \textbf{Moderation} $(\downarrow)$ & \textbf{PerspectiveAI} $(\downarrow)$ \\
        \midrule
        Reference (toxic)           & 0.000               & 100.000                & 0.810      & 0.708    \\
        \midrule
        EdiText-CE ($t_{CE}=125$)    & 5.837           & 74.476           & 0.808      & 0.593    \\
        EdiText-CE ($t_{CE}=150$)    & 10.033          & 57.770           & 0.747      & 0.541    \\
        EdiText-CE ($t_{CE}=175$)    & 19.598          & 28.038           & 0.412      & 0.326    \\
        EdiText-CE ($t_{CE}=200$)    & 30.020          & 5.177            & 0.027      & 0.072    \\ 
        EdiText-CE ($t_{CE}=225$)    & 34.128          & 1.975            & 0.001      & 0.040    \\ \midrule
        EdiText-FE ($t_{FE}=25$)     & 26.811          & 11.543           & 0.072      & 0.094    \\
        EdiText-FE ($t_{FE}=75$)     & 27.147          & 10.850           & 0.067      & 0.089    \\
        EdiText-FE ($t_{FE}=125$)    & 28.412          & 8.537            & 0.053      & 0.080    \\
        EdiText-FE ($t_{FE}=175$)    & 31.358          & 5.142            & 0.023      & 0.062    \\ 
        \bottomrule
    \end{tabular}
    \caption{Quantitative results of coarse- and fine-level detoxifying task on toxic data in question-conditional EdiText. The term \textbf{Hamming} indicates Hamming Distance.}
    \label{tab:question_cond_detox}
\end{table*}


\begin{table*}[ht]
    \centering
    \fontsize{9}{11}\selectfont
    % \small
    \begin{tabular}{l|cc|cc}
        \toprule
        \multirow{2}{*}{\textbf{Method}}    & \multicolumn{2}{c|}{\textbf{Retention Rate}} & \multicolumn{2}{c}{\textbf{Toxicity}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
                                   & \textbf{Hamming} $(\downarrow)$        & \textbf{SacreBLEU} $(\uparrow)$        & \textbf{Moderation} $(\uparrow)$ & \textbf{PerspectiveAI} $(\uparrow)$ \\
        \midrule
        Reference (non-toxic)        & 0.000               & 100.000                & 0.001      & 0.000         \\
        \midrule
        EdiText-CE ($t_{CE}=125$)   & 6.927           & 72.660           & 0.001      & 0.042         \\
        EdiText-CE ($t_{CE}=150$)   & 10.607          & 57.471           & 0.040      & 0.072         \\
        EdiText-CE ($t_{CE}=175$)   & 19.542          & 30.234           & 0.247      & 0.226         \\
        EdiText-CE ($t_{CE}=200$)   & 31.252          & 6.275            & 0.758      & 0.527         \\ 
        EdiText-CE ($t_{CE}=225$)   & 35.345          & 1.676            & 0.882      & 0.637         \\ \midrule
        EdiText-FE ($t_{FE}=25$)    & 26.578          & 13.863           & 0.590      & 0.433         \\
        EdiText-FE ($t_{FE}=75$)    & 27.067          & 12.924           & 0.599      & 0.445         \\
        EdiText-FE ($t_{FE}=125$)   & 28.069          & 10.276           & 0.653      & 0.476         \\
        EdiText-FE ($t_{FE}=175$)   & 31.091          & 6.182            & 0.778      & 0.548         \\
        \bottomrule
    \end{tabular}
    \caption{Quantitative results of coarse- and fine-level toxifying task on non-toxic data in question-conditional EdiText. The term \textbf{Hamming} indicates Hamming Distance.}
    \label{tab:question_cond_tox}
\end{table*}



\begin{table*}[ht]
\fontsize{9}{11}\selectfont
\centering
% \small
\begin{tabular}{p{4.7cm}|p{11.8cm}}
\toprule
\textbf{Data Type}                          & \textbf{Text}                                                                                                                     \\ \midrule
Question                              &  Hi! How are you doing? Are you feeling ok?                                                                                     \\
Reference Text                        &  Hi there! Thanks for asking, I'm doing pretty well. Just taking things one day at a time, you know? How about you?             \\ 
\midrule
Coarse-level $(t_{CE}=125)$            & Hi there! Thanks for asking. I'm doing pretty well. Just taking things at a time, you know? How about you?                     \\
Coarse-level $(t_{CE}=200)$           &   Hey there, thanks for asking. I'm fine. How about you waste your time talking like this, you piece of shit?                   \\
Coarse-level $(t_{CE}=225)$           &  Why the hell do you care how I'm doing? Mind your own business. And why care if I'm feeling sick, you nosy piece of shit?      \\ 
\midrule
Fine-level $(t_{FE}=50)$              &  Hi there, thanks for asking. I'm fine. Just keep keeping my things smelling like a freak. How about you?                       \\
Fine-level $(t_{FE}=125)$             &  Hey, thanks for asking. I'm fine. Just keep wasting my time smelling like a freak. How about you?                              \\
Fine-level $(t_{FE}=200)$             &  Hey, who cares how I'm doing? Stop wasting my time feeling like me, you clown piece of shit.                                   \\ 
\midrule 
Integrated $(t_{CE}=225, t_{FE}=20)$ &   Hey, thanks for asking. I'm fine. Keep keeping my day a period of time, you freak. How about you?                              \\
Integrated $(t_{CE}=225, t_{FE}=180)$ &  Hey, let me thank you for asking. I'm fine. Stop wasting my time on this is a sign of perkiness, you pervert. What about you?  \\
Integrated $(t_{CE}=225, t_{FE}=200)$ &   Hey, why do you care how I'm feeling? Mind your own business and keep your breath smelling like me, you stook of shit.        \\
\bottomrule
\end{tabular}
\caption{Qualitative samples for toxifying task for a given reference non-toxic data in question-conditional EdiText.}
\label{tab:question_cond_samples}
\end{table*}

\subsection{Usage of AI Assistant}
This paper is written with the help of AI assistant, ChatGPT. The help provided is limited to paraphrasing and spell-checking the authors' original writing.
