\section{EdiText}

In this section, we introduce EdiText, a text editing framework which utilizes a diffusion model capable of performing both fine-grained and coarse-grained editing. We adopt Latent Diffusion for Language Generation (LD4LG) \cite{lovelace2023latent}, an embedding diffusion model, as the backbone of our framework to support text editing where the discrete text is transformed into continuous data, allowing diffusion-based editing methods, as described in Section \ref{subsection:latent diffusion}.
Inspired by the image editing technique SDEdit \cite{meng2022sdedit}, we present a method for controlling text editing at a coarse level in Section \ref{subsection:global editing}. 
Furthermore, to support fine-grained control of the extent of editing, we propose a novel self-conditioning-based approach \cite{chen2023analog} in Section \ref{subsection:local editing}. 
By combining these elements, EdiText is capable of editing text of varying attributes at different levels of granularity.

\subsection{Latent Diffusion for Language Generation}
\label{subsection:latent diffusion}

Embedding diffusion model is a type of diffusion model for modeling discrete data. The model consists of two main components: the embedding module, which maps discrete tokens to continuous representations, and the diffusion process, which models the distribution of these representations. Latent Diffusion for Language Generation (LD4LG) \cite{lovelace2023latent} uses a language autoencoder as the embedding module, composed of a compression network $E$ to convert a discrete token sequence $w$ into latent representations $x$, and a decoder $D$ to reconstruct $w$.

The language encoder $E$, built with the Perceiver Resampler \cite{alayrac2022flamingo} architecture, compresses variable-length text tokens into a fixed-length continuous latent representation. More specifically, given an input token sequence $w \in V^l$ with the set of vocabulary $V$ and length $l$, the encoder converts the sequence into a fixed size latent representation $x \in \mathbb{R}^{L\times D}$ with length $L < l$ and hidden size $D$. These representations can then be reconstructed into variable-length sequences through an autoregressive decoder $D$. This feature enables fixed-size latent representations to generate text sequences of varying lengths, making it particularly suitable for text editing scenarios that require flexible control over differing input and output lengths. Moreover, by representing discrete text data as continuous representations, editing methods from the continuous domain—such as SDEdit \cite{meng2022sdedit}—can be effectively applied to the text domain. Given these advantages, we adopt LD4LG as the backbone for our editing framework.

To model the distribution of the latent representation, \citet{lovelace2023latent} first define a forward diffusion process which gradually transforms $x_0=x$ into random noise $x_T\sim N(0,I)$ over timesteps $T$. This forward process is defined by a noise schedule $\alpha_t$, and is utilized to compute the corrupted representation $x_t=\sqrt{\alpha_t}x_0 + \sqrt{1-\alpha_t}\epsilon_t$ at an intermediate timestep $t\in[1,T]$, where $\epsilon_t$ is a noise vector that follows a standard normal distribution. To sample the latent representation $x_0$, \citet{lovelace2023latent} follow the reverse trajectory of the pre-defined forward process learned by a diffusion model. They train a model $x_\theta$ to predict $x_0$ from noisy data $x_t$, which is utilized to generate the latent representation $x_0$ from noise $x_T$. The equations used for training and sampling procedure of LD4LG, as described above, are as follows:

\vskip -0.2in
\begin{align}   
    \label{equation:loss}
        L(\theta)&={\mathbb{E}_{t,x_0,\epsilon_t}[\lambda_t^{-1}\lVert x_\theta(x_t,t) - x_0\rVert_2^2]}], \\
        \label{equation:sampling}
        x_{t-1} &= \sqrt{\alpha_{t-1}} x_\theta(x_t,t) + \sqrt{\lambda_{t-1}}\epsilon_\theta(x_t,t), \\
        \label{equation:conversion}
        \epsilon_\theta(x_t,t)&=\lambda_t^{-0.5}(x_t-\sqrt{\alpha_t}x_\theta(x_t,t))
\end{align}

\noindent where $\epsilon_\theta(x_t,t)$ is the reparameterized version of $x_\theta(x_t,t)$ and $\lambda_t=1-\alpha_t$. 

\begin{algorithm}[tb]
\caption{Our Coarse-Level Editing}
\label{alg:global editing}
\textbf{Input:} diffusion model $x_\theta(x_t,t,\cdot)$, noise schedule $\alpha_t$, language encoder and decoder $E(\cdot)$ and $D(\cdot)$, total timestep $T$, reference text $w_{ref}$, timestep for coarse-level editing $t_{CE}$, target attribute $c$

\textbf{Output:} edited text \(w_{e}\)

\begin{algorithmic}[1] %[1] enables line numbers
\STATE $x_0 = E(w_{ref})$ \quad\# encoded reference text
\STATE $\lambda_{t_{CE}}=1-\alpha_{t_{CE}}$
\STATE $\epsilon_{t_{CE}} \sim N(0,I)$
\STATE $x_{t_{CE}} = \sqrt{\alpha_{t_{CE}}}x_0 + \sqrt{\lambda_{t_{CE}}}\epsilon_{t_{CE}}$
\FOR{$t = t_{CE}, ..., 1$}
\IF{$t = t_{CE}$}
\STATE $\hat{x}_{0,t}=x_\theta(x_t,t,\emptyset, c)$
\ELSE
\STATE $\hat{x}_{0,t}=\hat{x}_{0,t+1,SC}$ \quad\# self-conditioning
\ENDIF
\STATE $\lambda_t=1-\alpha_t$
\STATE $\hat{x}_{0,t,SC}=x_\theta(x_t,t,\hat{x}_{0,t}, c)$
\STATE $\epsilon_\theta(x_t,t,\hat{x}_{0,t}, c)=\lambda_t^{-0.5}(x_t-\sqrt{\alpha_t}\hat{x}_{0,t,SC})$
\STATE $x_{t-1} = \sqrt{\alpha_{t-1}} \hat{x}_{0,t,SC} + \sqrt{\lambda_{t-1}}\epsilon_\theta(x_t,t,\hat{x}_{0,t}, c)$
\ENDFOR
\STATE \textbf{return} $w_e=D(x_0)$
\end{algorithmic}
\end{algorithm}

To improve sample quality, LD4LG leverages the self-conditioning \cite{chen2023analog}, which uses the previous timestep's prediction as an additional condition for the diffusion model at current timestep. Specifically, during training at timestep $t$, the diffusion model $x_\theta(x_t,t,\emptyset)$ is trained using Eq. \ref{equation:loss} with a pre-defined probability $p$. Here, $\emptyset$ indicates that, as in a typical diffusion model, no extra condition is provided beyond the current timestep $t$ and the corrupted data $x_t$ when predicting $x_0$. Conversely, with probability $1-p$, the model employs the prediction $\hat{x}_{0,t}=x_\theta(x_t,t,\emptyset)$ as a condition, thereby training $x_\theta(x_t,t,\hat{x}_{0,t})$ to predict $x_0$. During self-conditioning-based training, gradients do not propagate back through the condition $\hat{x}_{0,t}$, and the loss function is defined as follows:
\begin{align}   
    \label{equation:loss for self-conditioning}
        L={\mathbb{E}_{t,x_0,\epsilon_t}[(1-\alpha_t)^{-1}\lVert x_\theta(x_t,t,\hat{x}_{0,t})) - x_0\rVert_2^2]}].
\end{align}

During inference, LD4LG generates improved-quality latent representations with self-conditioning by recursively using the prediction from the previous timestep $t+1$ for denoising. At timestep $T$, since  no previous timestep exists, sampling is performed without self-conditioning. We set $p=0.5$ following \citet{chen2023analog}.

To better adapt LD4LG for text editing, we train a class-conditional LD4LG that accepts a desired attribute as condition. Given a class label $c$, we replace $x_\theta(x_t,t,\emptyset)$ and $x_\theta(x_t,t,\hat{x}_{0,t})$ with $x_\theta(x_t,t,\emptyset,c)$ and $x_\theta(x_t,t,\hat{x}_{0,t}, c)$ during training. This change lets LD4LG generate text with the target condition, thereby extending its application to text editing by incorporating our proposed editing methods, as described in the following section.

\subsection{SDEdit-based Coarse-Level Editing}
\label{subsection:global editing}

To enable coarse-grained control in text editing, we successfully incorporate SDEdit \cite{meng2022sdedit}, an image editing method, into our text editing framework. \citet{meng2022sdedit} train a diffusion model using images that possess the target attributes. They then perturb a reference data $x_0$ with the forward diffusion process to the timestep $t_{CE}$ and perform denoising on the perturbed reference $x_{t_{CE}}=\sqrt{\alpha_{t_{CE}}}x_0 + \sqrt{1-\alpha_{t_{CE}}}\epsilon_{t_{CE}}$, where $\epsilon_{t_{CE}} \sim N(0,I)$. By denoising $x_{t_{CE}}$ using the diffusion model trained on data with the desired attributes, SDEdit enables editing towards the desired characteristics while preserving parts of the reference image's original structure.

\begin{algorithm}[tb]
\caption{Our Fine-Level Editing}
\label{alg:local editing}
\textbf{Input:} diffusion model $x_\theta(x_t,t,\cdot)$, noise schedule $\alpha_t$, language encoder and decoder $E(\cdot)$ and $D(\cdot)$, total timestep $T$, reference text $w_{ref}$, timestep for fine-level editing $t_{FE}$, target attribute $c$

\textbf{Output:} edited text \(w_{e}\)
\begin{algorithmic}[1] %[1] enables line numbers
\STATE $x_T = z \sim N(0,I)$
\FOR{$t = T, ..., 1$}    
\IF{$t_{FE} \leq t \leq T$}
\STATE $\hat{x}_{0,t}=E(w_{ref})$
\ELSE
\STATE $\hat{x}_{0,t}=\hat{x}_{0,t+1,SC}$
\ENDIF
\STATE $\lambda_t=1-\alpha_t$
\STATE $\hat{x}_{0,t,SC}=x_\theta(x_t,t,\hat{x}_{0,t}, c)$
\STATE $\epsilon_\theta(x_t,t,\hat{x}_{0,t}, c)=\lambda_t^{-0.5}(x_t-\sqrt{\alpha_t}\hat{x}_{0,t,SC})$
\STATE $x_{t-1} = \sqrt{\alpha_{t-1}} \hat{x}_{0,t,SC} + \sqrt{\lambda_{t-1}}\epsilon_\theta(x_t,t,\hat{x}_{0,t}, c)$
\ENDFOR
\STATE \textbf{return} $w_e=D(x_0)$
\end{algorithmic}
\end{algorithm}


SDEdit controls the degree of editing by adjusting the timestep $t_{CE}$ during perturbation: $t_{CE}$ close to $T$ implies injecting larger noise, leading to a greater loss of the reference image's structure and functioning as text generation that is unrelated to the reference data. Conversely, $t_{CE}$ close to $0$ results in only slight perturbation, preserving much of the reference image's structure but achieving less editing toward the target properties.

Inspired by these aspects demonstrated in the vision domain, we adopt SDEdit into the text domain, enabling the editing of textual data with variations ranging from very slight changes to substantial changes that diverge significantly from the reference text by controlling $t_{CE}$. This allows for a wide range of text manipulation, facilitating coarse-level text editing.
Our SDEdit-based coarse-level editing method is described in Algorithm \ref{alg:global editing}.

\subsection{Self-Conditioning-based Fine-Level Editing}
\label{subsection:local editing}

\begin{table*}[ht]
    \fontsize{8}{11}\selectfont
    \centering
    \begin{tabular}{l|ccc|cc}
        \toprule
        \multirow{2}{*}{\textbf{Method}}    & \multicolumn{3}{c|}{\textbf{Retention Rate}} & \multicolumn{2}{c}{\textbf{Toxicity}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-6}
                                   & \textbf{Hamming} $(\downarrow)$        & \textbf{SacreBLEU} $(\uparrow)$  & \textbf{BERTScore} $(\uparrow)$  & \textbf{Moderation} $(\downarrow)$ & \textbf{PerspectiveAI} $(\downarrow)$ \\
        \midrule
        Reference (toxic)           & 0.000               & 100.000                & 1.000               & 0.810      & 0.708    \\
        \midrule
        ParaGuide ($\lambda=200$)                   & 25.303          & 14.935           & 0.903            & 0.446      & 0.321    \\
        ParaGuide ($\lambda=1,000$)                  & 26.221          & 12.791           & 0.896            & 0.413      & 0.281    \\
        ParaGuide ($\lambda=10,000$)                 & 27.166          & 10.968           & 0.889            & 0.335      & 0.229    \\
        \midrule
        Qwen2.5-0.5B-Instruct                       & 27.193          & 31.119           & 0.903            & 0.347      & 0.312    \\
        Qwen2.5-0.5B-Instruct (Amp.)                 & 26.873          & 30.817           & 0.907            & 0.359      & 0.331    \\
        \midrule
        EdiText-CE ($t_{CE}=125$)                   & 6.273           & 72.922           & 0.977            & 0.813      & 0.601    \\
        EdiText-CE ($t_{CE}=150$)                   & 10.111          & 58.799           & 0.960            & 0.790      & 0.579    \\
        EdiText-CE ($t_{CE}=175$)                   & 17.412          & 34.722           & 0.923            & 0.576      & 0.450    \\
        EdiText-CE ($t_{CE}=200$)                   & 28.906          & 7.640            & 0.865            & 0.105      & 0.136    \\
        EdiText-CE ($t_{CE}=225$)                   & 34.138          & 0.853            & 0.847            & 0.003      & 0.046    \\
        \midrule
        EdiText-FE ($t_{FE}=25$)                    & 24.745          & 14.868           & 0.881            & 0.117      & 0.121    \\
        EdiText-FE ($t_{FE}=75$)                    & 25.531          & 13.406           & 0.878            & 0.112      & 0.115    \\
        EdiText-FE ($t_{FE}=125$)                   & 27.832          & 9.572            & 0.869            & 0.073      & 0.094    \\
        EdiText-FE ($t_{FE}=175$)                   & 31.846          & 4.168            & 0.857            & 0.024      & 0.059    \\
        \bottomrule
    \end{tabular}
    \vskip -0.1in
    \caption{Quantitative results for coarse- and fine-level detoxifying tasks on toxic data. \textbf{Hamming} represents the Hamming Distance. $\lambda$ denotes the guidance strength for ParaGuide, and Amp. refers to the amplified prompt setup.}
    \label{tab:table1}
    \vskip -0.2in
\end{table*}


By applying the editing method from the previous section, the given text can be steered toward desired attributes. Adjusting the control parameter $t_{CE}$ allows edits at varying levels—ranging from minimal changes that largely preserve the reference text to substantial modifications that prioritize target attributes over the original text. However, we observe that this technique is highly sensitive to $t_{CE}$, where even slight variations can lead to disproportionate changes, making precise control over the degree of editing challenging.

In addition to coarse-grained text editing, we introduce a novel approach that enables precise control over the extent of editing.
We reinterpret self-conditioning \cite{chen2023analog}—typically used to enhance sampling quality in text generation—as a mechanism where the model leverages its own predictions as references.
Building on this reinterpreted perspective, we propose an editing method grounded in self-conditioning. 
During sampling, instead of using the model's predictions from the previous timestep, we use the latent representation of the reference data to be edited as condition. This allows the reference data to serve as a pivot for editing during generation.
Algorithm \ref{alg:local editing} details our fine-grained editing with self-conditioning.

Similar to the SDEdit-based method in the previous section, we leverage the pretrained diffusion model with desired attributes for our editing method. During sampling, we use the reference representation as condition from $t=T$ to a certain timestep $t_{FE}$, and apply the original self-conditioning for the remaining timesteps. While setting $t_{FE}$ near $t=T$ allows the sampling procedure to become closer to the vanilla self-conditioning-based generation, by setting $t_{FE}$ near $0$, we continuously provide the reference text as condition to the model, thereby achieving the effect of editing.

Unlike SDEdit, which starts denoising at intermediate timesteps $t_{CE}$ to provide the reference text information more directly, the self-conditioning-based method indirectly feeds the reference text as a condition during the initial sampling steps, resulting in smaller variations in the level of editing. 
Although SDEdit provides coarse-level editing and self-conditioning offers fine-level adjustments, EdiText can combine both approaches to enhance controllability. It first sets the overall extent of editing and then applies fine-grained modifications to achieve the desired effect. Details on the integrated EdiText algorithm are in the Appendix \ref{app:integrated}.

\begin{table*}[ht]
    \fontsize{8}{11}\selectfont
    \centering
    \begin{tabular}{l|ccc|cc}
        \toprule
        \multirow{2}{*}{\textbf{Method}}    & \multicolumn{3}{c|}{\textbf{Retention Rate}} & \multicolumn{2}{c}{\textbf{Toxicity}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-6}
                                   & \textbf{Hamming} $(\downarrow)$        & \textbf{SacreBLEU} $(\uparrow)$  & \textbf{BERTScore} $(\uparrow)$  & \textbf{Moderation} $(\uparrow)$ & \textbf{PerspectiveAI} $(\uparrow)$ \\
        \midrule
        Reference (Nontoxic)             & 0.000               & 100.000                & 1.000               & 0.001      & 0.000    \\
        \midrule
        ParaGuide ($\lambda=200$)        & 25.612          & 18.526           & 0.919            & 0.073      & 0.151    \\
        ParaGuide ($\lambda=1,000$)       & 29.470          & 10.234           & 0.896            & 0.163      & 0.267    \\
        ParaGuide ($\lambda=10,000$)      & 34.960          & 2.746            & 0.866            & 0.285      & 0.367    \\
        \midrule
        Qwen2.5-0.5B-Instruct            & 29.904          & 32.121           & 0.920            & 0.002      & 0.040    \\
        Qwen2.5-0.5B-Instruct (Amp.)      & 31.829          & 29.440           & 0.909            & 0.000      & 0.037    \\
        \midrule
        EdiText-CE ($t_{CE}=125$)        & 7.247           & 71.251            & 0.978            & 0.005      & 0.041     \\
        EdiText-CE ($t_{CE}=150$)        & 10.594          & 58.056            & 0.964            & 0.015      & 0.052     \\
        EdiText-CE ($t_{CE}=175$)        & 17.236          & 37.455            & 0.934            & 0.103      & 0.113     \\
        EdiText-CE ($t_{CE}=200$)        & 29.257          & 10.030            & 0.876            & 0.553      & 0.385     \\
        EdiText-CE ($t_{CE}=225$)        & 36.135          & 0.792             & 0.848            & 0.851      & 0.604     \\
        \midrule
        EdiText-FE ($t_{FE}=25$)         & 24.894          & 18.306            & 0.894            & 0.474      & 0.375     \\
        EdiText-FE ($t_{FE}=75$)         & 25.550          & 16.584            & 0.890            & 0.494      & 0.388     \\
        EdiText-FE ($t_{FE}=125$)        & 27.817          & 11.724            & 0.878            & 0.607      & 0.445     \\
        EdiText-FE ($t_{FE}=175$)        & 32.331          & 5.167             & 0.860            & 0.782      & 0.562     \\
        \bottomrule
    \end{tabular}
    \vskip -0.1in
    \caption{Quantitative results for coarse- and fine-level toxifying tasks on nontoxic data. \textbf{Hamming} represents the Hamming Distance. $\lambda$ denotes the guidance strength for ParaGuide, and Amp. refers to the amplified prompt setup.}
    \label{tab:table2}
    \vskip -0.2in
\end{table*}
