%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025_arxiv}

%\usepackage{icml}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{esvect}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
%\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{bm, bbm}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\ep}{\mathbb{E}}

\usepackage{colortbl}
\usepackage{verbatim}
\usepackage{fancyvrb}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Modifying Final Splits of Classification Tree for Fine-tuning Subpopulation Target in Policy Making}

\begin{document}

\twocolumn[
\icmltitle{Modifying Final Splits of Classification Tree for Fine-tuning Subpopulation Target in Policy Making}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lei Bill Wang}{yyy}
\icmlauthor{Zhenbang Jiao}{comp}
\icmlauthor{Fangyi Wang}{comp}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

%\icmlaffiliation{...}

\icmlaffiliation{yyy}{Department of Economics, Ohio State University, Columbus Ohio, USA}
\icmlaffiliation{comp}{Department of Statistics, Ohio State University, Columbus Ohio, USA}

\icmlcorrespondingauthor{Lei Bill Wang}{wang.13945@osu.edu}
%\icmlcorrespondingauthor{Zhenbang Jiao}{jiao.180@osu.edu}
%\icmlcorrespondingauthor{Fangyi Wang}{wang.15022@osu.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Classification tree, empirical risk minimization}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
  Policymakers often use Classification and Regression Trees (CART) to partition populations based on binary outcomes and target subpopulations whose probability of the binary event exceeds a threshold. However, classic CART and knowledge distillation method whose student model is a CART (referred to as KD-CART) do not minimize the misclassification risk associated with classifying the latent probabilities of these binary events. To reduce the misclassification risk, we propose two methods, Penalized Final Split (PFS) and Maximizing Distance Final Split (MDFS). PFS incorporates a tunable penalty into the standard CART splitting criterion function. MDFS maximizes a weighted sum of distances between node means and the threshold. It can point-identify the optimal split under the unique intersect latent probability assumption. In addition, we develop theoretical result for MDFS splitting rule estimation, which has zero asymptotic risk. Through extensive simulation studies, we demonstrate that these methods predominately outperform classic CART and KD-CART in terms of misclassification error. Furthermore, in our empirical evaluations, these methods provide deeper insights than the two baseline methods.
\end{abstract}

%{\color{violet} %I suggest numbering all the equations in display mode no matter whether referring to it later, so that reviewers can point out which equation they are looking at since display mode don't have a line number.\\
%We need unify names of methods, mention them in abstract, and be consistent throughout.\\
%I think it would be better to write an Algorithm use algorithm environment for MDFS (modifying final split based on CART) to tell people how to implement our method step by step (maybe somewhere in Appendix).}
\section{Introduction}
%This paper aims to design an interpretable and data-driven policy rule without imposing parametric assumptions.
Policymakers often use CART to partition a population based on a binary event $Y=1$ and target a few subpopulations with a probability of $Y = 1$ higher than a threshold $c$ when implementing a policy. %Such policy rule often resembles a decision tree. %For example, the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) selects its eligible population based on four partition rules: categorical requirement, income eligibility, health screening, and residential requirement (See Figure \ref{Figure WIC decision tree}). 
We call such policy targeting problems Latent Probability Classification (LPC).\footnote{Note that LPC is a generalization of classification based on minimizing ``0-1 loss'' (i.e. the misclassification rate of the observed binary outcome) whose threshold is 0.5.}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\linewidth]{WIC_decision_tree.pdf}
%     \caption{US Department of Agriculture/Local WIC administrative sites use this decision tree as a policy rule to determine eligibility for WIC benefit.}
%     \label{Figure WIC decision tree}
% \end{figure}
% \begin{figure*}[htb]
%     \centering
%     \begin{minipage}{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Sine_example.pdf}
%         \caption{Sine example.}
%         \label{Figure Sine example}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Unique_Sine_example.pdf}
%         \caption{$t_1$ example.}
%         \label{Figure Unique Sine example}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Monotonic_example.pdf}
%         \caption{$t_2$ example.}
%         \label{Figure Monotonic example}
%     \end{minipage}
%     \caption{Three illustrative examples}
% \end{figure*}
\begin{figure*}[htb]
    \centering
    \captionsetup{justification=centering}  % Optional: if you want to center-align the main caption
    
    % Main figure caption (if needed)
    %\caption{Examples of different functions}
    %

    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Sine_example.pdf}
        \subcaption{Sine example}
        \label{Figure Sine example}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Unique_Sine_example.pdf}
        \subcaption{\(t_1\) example}
        \label{Figure Unique Sine example}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Monotonic_example.pdf}
        \subcaption{\(t_2\) example}
        \label{Figure Monotonic example}
    \end{minipage}
    \caption{Three illustrative examples of $\eta$. The black dashed line indicates threshold $c$. The green component of $\eta$ is above $c$ (hence, should be targeted), and the yellow component of $\eta$ is below $c$ (hence, should not be targeted). $s^{cart}$ is the split generated by CART, $s^*$ is as defined in Theorem \ref{theorem tendency} in Section \ref{sec penalized split}. In all three examples, $s^*$ is a better split than $s^{cart}$ for classifying whether $\eta$ is above or below $c$.}
    \label{fig:main-figure}
\end{figure*}
%The underlying rationale of such policy rules is often based on whether the latent probability of a binary outcome event for a subpopulation is above a threshold. 
 %In the example of Figure \ref{Figure WIC decision tree}, the policy rule is based on the probability of a subpopulation being malnourished. %Other examples where such decision tree policy rules are relevant are plenty. 
%{\color{violet} Due to the resemblance between the decision tree and policy rule (need clarification)}, many policymakers use the Classification and Regression Tree (CART) to generate splitting rules for policy targeting. 
%There are plenty of LPC problems in the literature. 
Here, we give three policymaking contexts that match LPC setup. For more examples in existing literature, see Appendix \ref{appendix Extensive use of CART for policy targeting}.

\textbf{Social assistance: } \citet{andini2018targeting} uses CART to find subpopulations with a higher than 50\% probability of being financially constrained and target these households with a tax credit program. %{\color{red} (delete this) This example matches our setup: a binary outcome, a decision tree policy, and a threshold targeting rule}.

\textbf{Forest fire: } \citet{sarkar2024ensembling} uses CART to determine which forest zone has a higher than $61\%$ probability of forest fire to implement early warning systems. %{\color{red} (delete this) The use of CART ``develops effective forest management and planning strategies'' which perfectly corresponds to our setup.}

\textbf{College admission: } \citet{mashat2012decision} uses CART to find among all university applicants which subpopulation has more than 50\% probability of getting accepted to filter out the ``low-level candidates''.

%{\color{violet} Add one more examples here.}

%{\color{red} (delete this) We include many other examples in Appendix \ref{appendix Extensive use of CART for policy targeting} to demonstrate the extensive use of CART for policy targeting in fraud detection, college admission, job application, mortgage lending, health intervention, water management, and legal settlements.} %Such wide usage underscores CART's straightforward interpretability for policymakers and its attractive data driven property. 

%The policy rules generated by CART are \textbf{straightforward} to the policy administrators. For example, WIC workers can use the four criteria to screen out ineligible households: those who cannot provide evidence that their income is below 185\% of the federal poverty line will not be granted the WIC benefits.

%Moreover, using CART also makes the policy \textbf{data-driven}, bringing potential benefits \citep{gill2014conceptual, van2017data, maffei2020data}. For example, we can leverage past data to identify susceptible forest zones where wildfires have not yet happened but are similar to those forest zones where wildfires have already happened. Preventive measures such as evacuating residents from those forest zones can be life-saving.

%Due to these two attractive properties of CART,
% {\color{violet}
% It is typical to estimate the latent probabilities for each subgroup with CART and then compare the estimates against \emph {a threshold of interest} $c$, e.g., $c=50\%$ in the first example. (Use $c$ instead of ``threshold'' from now on, including in figures (and explain notations in caption).)  (Are there exceptions in existing literature that do this differently?)} 
The three examples use CART to divide the sample into many nodes, estimate the probability of $Y = 1$ for all nodes, and target those nodes with estimated probabilities higher than a given threshold $c$. For example, $c =61\%$ in the forest fire example.

This approach, though intuitive, does not generate the binary splitting rule that minimizes the LPC misclassification risk of classifying whether $\mathbb{P}(Y=1|X)$ is above or below $c$. We will formally define the LPC misclassification risk in Section \ref{sec problem setup} and explain its policy significance in Section \ref{sec policy significance}. Here, we provide a toy example in Figure \ref{Figure Sine example} to illustrate the limitation of using CART for an LPC problem. 

Suppose the latent probability of a binary event $Y = 1$ is a sinusoidal function of an observable variable $X$, i.e.,
\[
     \mathbb{P}(Y=1|X) = \frac{\sin (2 \pi X)+1}{2} \text{ where $ X \sim {\rm Unif}[0,1]$}.
\]
If we set the threshold of interest $c = 0.75$, we aim to target those with $\mathbb{P}(Y=1|X) > 0.75$, i.e., target all the green segments of the sine curve. %{\color{violet} This corresponds to a subpopulation whose $X \in (\frac{1}{12},\frac{5}{12})$}. 

In this example, we impose the restriction that the policymaker can only split the population \textit{once} based on the value of $X$.\footnote{This example is purely illustrative for our theoretical results in the later sections. We explain why this one-node fixed-feature setup is relevant to policymaking scenarios where many nodes and many features are involved in Section \ref{sec policy significance}.} CART splits at $s = 0.5$, denoted as $s^{CART}$ in Figure \ref{Figure Sine example}. The left node has $\mathbb{P}(Y=1 | X > 0.5)  = 0.5 + \frac{1}{\pi} > 0.75$, whereas the right node has $\mathbb{P}(Y=1| X \leq 0.5)  = 0.5 - \frac{1}{\pi} \leq 0.75$. Consequently, we target the subpopulation by $X \leq 0.5$, i.e., left node. 

To demonstrate why $s^{CART}$ is suboptimal for the LPC problem, consider a better split at $s^* = \frac{5}{12}$. The left node still has $\mathbb{P}(Y=1|X \leq \frac{5}{12}) \approx 0.856 > 0.75$, right node has $\mathbb{P}(Y=1|X >\frac{5}{12}) \approx 0.246 < 0.75$. Only the left node ($X \leq
\frac{5}{12}$) is targeted. This reduces misclassification risk as we exclude the group with $\frac{5}{12} < X < 0.5$ whose $\eta(X) < 0.75$ from being in the targeted node (left node).
%{\color{violet} exclude more population with $P(Y=1|X)<0.75$ from the left node (rephrasing this, maybe give a specific value of $s$ and calculate the corresponding subpopulation, i.e., range of $X$, that was excluded)}. 
Such split location ``correction'' enhances targeting accuracy for LPC, allowing policymakers to concentrate resources on the intended groups more effectively.
% Using logic we illustrate with Figure \ref{Figure Sine example}, 

This paper modifies CART's impurity function to reflect such ``correction'': we achieve this by adding a penalty term that pushes the node means, i.e, $\mathbb{P}(Y=1|X\leq s)$ and $\mathbb{P}(Y=1|X>s)$, away from the threshold. We further propose to maximize a weighted sum of the distances between node means and the threshold $c$ to identify the unique best split, assuming the existence of such a split.
In addition, we show that the latter modification also reduces the misclassification risk of KD-CART, generalizing this paper's applicability to a larger class of tree-based methods used in policymaking.\footnote{ For example, \citet{che2015distilling} applies KD-CART to two binary classification problems in a healthcare intervention setup.}
%{\color{violet} (Does KD-CART has any motivation examples in policy making or economics?)}.
% Theoretical guarantees are also provided that this new CART-based algorithm has a smaller misclassification risk than conventional CART. 
% The new algorithm is particularly useful for policy targeting as it fine-tunes the subpopulation that the policymakers target.
% this paper modifies the criterion function of data splitting for CART and provides the theoretical guarantee that the new CART algorithm has a smaller misclassification risk than conventional CART. 
% %Under the monotonicity assumption, our method attains the unique optimal splitting rule. 
% The new algorithm is particularly useful for policy targeting as it fine-tunes the subpopulation that the policymakers target.
%The rest of the paper is organized as follows. Section \ref{sec problem setup} describes the problem, in particular, we show that the traditional CART method does not minimize the misclassification objective function. Section \ref{sec penalized split} shows an identification result that a penalized criterion function outputs a strictly better policy rule regarding misclassification error. Section \ref{sec simulation} presents the simulation results that compare our penalized method against the traditional CART method and empirical distribution method. Section \ref{sec lit review} connects our method to other frontier theoretical works in decision trees and random forests, demonstrating research directions that future works can look into. Section \ref{sec conlusion} concludes the paper. All proofs are collected in Appendix \ref{appendix proofs}.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.65\linewidth]{Sine_example.pdf}
%     \caption{A graphical illustration for that using conventional CART to minimize the misclassification risk is inefficient.}
%     \label{Figure Sine example}
% \end{figure}



% \section{Comparison with existing literature}
% In this section, we compare and contrast the LPC problem that we aim to solve in this paper with the questions often posed by the existing literature. We include another review to situate our paper in CART and random forest in Appendix \ref{appendix further lit review}.

% \subsection{Comparison with other classifiers} 
% Throughout this paper, we limit the scope to the comparison between CART and our proposed methods. We justify the strength of our approach by highlighting the key mismatch between other classifiers with the policy-targeting problems/solutions that we discuss in this paper.

% (i)
% \textbf{Logistic regression} and its variants such as probit or Cloglog are parametric classifiers. \citet{achen2002toward,frolich2006non} point out that these parametric classifiers require strong structural assumptions: functional specification and error term distribution. In contrast, we impose much less restrictive nonparametric assumptions such as continuity and monotonicity in Section \ref{sec penalized split} when proving the theoretical properties of proposed methods. Our method has a major advantage over parametric classifiers in that policymakers do not need to assume specific functions or error distribution.
% (ii) 
% Without probability calibration, \textbf{support vector machine} does not output $P(Y=1|X)$ \citep{platt1999probabilistic, drish2001obtaining} and hence, it is not compatible with the LPC problem of this paper.
% (iii)
% Unlike recursive splitting methods, \textbf{neural network} lacks interpretability \citep{zhang2018visual,zhang2021survey}. It is hard to use it to generate transparent policy-targeting rules. (iv) \textbf{Random forest} often output policy rules that are too complicated to implement as the output cannot be interpreted as a couple of splitting rules like our proposed methods \citep{sagi2020explainable,sagi2021approximating,gulowaty2021extracting}, though we provide a discussion on how to adapt random forest to our results in Appendix \ref{appendix further lit review} under the ``representative tree'' paragraph. (v) Policy targeting problems often involve high-dimensional data since effective targeting rule can involve multi-level interactions between covariates, naive \textbf{KNN} method is incompatible with such problem setup.

% \subsection{LPC vs Policy learning (in Econ/Biostat)}
% An \textbf{latent probability classification} (LPC) problem aims to correctly classify whether the conditional probability of a binary event is above a threshold. As exemplified by the tax credit program in the introduction, LPC informs treatment assignment policies without experimenting with treatment among some subpopulations. On the other hand, \textbf{policy learning} (in Economics/biostatistics) aims to optimize the treatment assignment policies directly with experiment or quasi-experiment data \citep{manski2004statistical,hirano2009asymptotics,lipkovich2011subgroup,zhao2012estimating,kitagawa2018should,mbakop2021model,athey2021policy,yadlowsky2021evaluating,li2023trustworthy,viviano2024policy}. 
% For a more detailed comparison, see Appendix \ref{appendix further lit review}.


% %In summary, this paper aims to solve LPC problem than policy learning but offers advantages in terms of less restrictive data requirements and lower computational complexity. Moreover, as shown by the abundant examples in the introduction, the LPC problem aligns with real-life policy targeting rules in many instances. 

% %The end product of our methods is hence, a partition of the population and identifying those subpopulations whose probability of a binary event is above the pre-specified threshold which we call targeting subpopulations. In contrast to the policy learning literature, we do not have to assign treatment to the targeting subpopulations, for example, the WIC administrators can analyze why the identified targeting subpopulations have a high probability of non-participation and 

% %The problem that we look at is substantially less ambitious but also imposes much less requirement on data and requires much less computational power.

% %The first difference is that the treatment assignment literature 
% \subsection{Criterion function of CART}
% We contribute to the literature that modifies the criterion function of CART. The major distinction between our paper and the others is that we focus on the LPC problem for a binary outcome event. This leads to different theoretical results and modifications of CART. Unlike past works that focus on the bias \citep{athey2016recursive}, interpretability \citep{hwang2020new}, or consistency \citep{zheng2023consistency} properties, we prove our methods have lower misclassification risk than CART. 

\section{One-split, one-feature LPC} \label{sec problem setup}
%{\color{violet} I suggest use $\bm{x}$ and $\bm{X}$ instead of $\mathbf{x}$ or $\mathbf{X}$.}\\
We first restrict the theoretical discussion of this paper to the following one-split, one-feature (OSF) LPC problem and explain why our contribution has real-life policy significance to the multiple-node, multiple-feature setup in Section \ref{sec policy significance}. 

Let $\mathcal{X} = [0,1]$ and $\mathcal{Y} = \{0,1\}$ be the univariate feature space and label space, respectively. Let $f: \mathcal{X}\xrightarrow[]{}\mathbb{R}^{+}$ be a probability density function that is continuous on $\mathcal{X}$, $F$ be its corresponding cumulative distribution function, and $\eta(x) := \mathbb{P}(Y=1|X=x)$ is continuous.
Consider a dataset comprising $n$ i.i.d. samples, $(X_i, Y_i),\,i=1,\dots,n$, where $X_i\sim f$ and $Y_i\sim \text{Bernoulli}(\eta(X_i))$.

A policymaker is allowed to split the feature space one time (i.e. one-split) using the univariate feature $X$ (i.e. fixed feature). As illustrated by the examples in the introduction, policymakers often target those subpopulations whose $\eta(X) > c$. The misclassification risk in the OSF LPC is defined as 
\begin{align}
    R(s) = \int_{0}^s &\mathbbm{1}\{\eta(x)>c\}\mathbbm{1}\{\mu_L(s)\leq c\} + \nonumber \\
    &\mathbbm{1}\{\eta(x)\leq c\}\mathbbm{1}\{\mu_L(s) > c\} dF(x) + \nonumber\\
    \int_{s}^1 &\mathbbm{1}\{\eta(x)>c\}\mathbbm{1}\{\mu_R(s)\leq c\} + \nonumber \\
    &\mathbbm{1}\{\eta(x)\leq c\}\mathbbm{1}\{\mu_R(s) > c\} dF(x),
\end{align}
where
\begin{align}
     \mu_L(s) =& \int_0^s \eta(x) dF(x) / F_X(s), \\
    \mu_R(s) =& \int_s^1 \eta(x) dF(x) / (1 - F_X(s)).
\end{align}
% \begin{align*}
%     &\mathbf{x}_i\in [0,M]^d \sim f(\mathbf{x_i})\\
%     & y_i\in \{0,1\}\sim Bernoulli(\eta(\mathbf{x}_i)),
% \end{align*}
% where $f$ is a probability density function and is nonzero over its domain, and $p: [0, M]^d \rightarrow [0, 1]$, $p(\mathbf{x}) = Pr(Y=1|X=\mathbf{x})$. 
% The domain of $\mathbf{x}$ can be relaxed.
 %A classic binary classification task defines risk as $P(Y\neq T(X)$, where $T$ is the tree function we aim to find.
% Bayes classifier is the optimal classifier that minimizes the risk. 
In an LPC setup, we are particularly interested in the binary classification problem with respect to the latent probability $\eta(X)$. The policy implication is that policymakers would like to target subpopulations whose probability of a bad event $Y$ is higher than a certain threshold $c$. %Let $Z = \mathbbm{1}\{\eta(\mathbf{X})<c\}$. %be the latent variable of interest. 
%We are interested in a new risk function defined as $R(T) = P(Z\neq T(\mathbf{X}))$. (Need a clearer definition of risks and comparisons, maybe we can use words CART-policy versus LPC policy here after define different risks). People use classic CART in LPC setup. Also, add a few words linking back to examples in introduction.)}
% Each sample $i$ consists of a feature vector $\mathbf{x}_i$ and a binary response variable $y_i$, $i\in[n]$, where $[n]=\{1,\dots,n\}$. The feature vectors $\mathbf{x}_i$ are drawn from a distribution $f(\mathbf{x})$ over the $d$-dimensional space bounded by $[0, M]^d$, where $M$ is a fixed positive constant, and $d$ denotes the dimensionality of the feature space. Let $p: [0, M]^d \rightarrow [0, 1]$ maps input features to the probability of the response being 1, i.e. $y_i \sim \text{Bernoulli}(p(\mathbf{x}_i))$.

%{\color{violet} We can say we assume from now on $\bm{X} \in t$ unless otherwise specified, i.e., we are looking at a single split of node $t$, and refrain the notation of conditional on $t$ or $\bm{X} \in t$ unless otherwise specified. And use $\eta(s)$, $F(s)$, $f(s)$, etc.)}





\subsection{CART does not optimize $R(s)$}
%{\color{red}
%For a tree $T$, $\text{leaves}(T) = \{t|t\ \text{is a leaf of}\ T\}$. For all $\mathbf{X}\in t$, the prediction of $Z$ is decided as follows:
% \begin{align*}
%     T(\mathbf{X}) &= E_t = E(Y|\mathbf{X}\in t),\ \mathbf{X}\in t
% \end{align*} 
% Since $T(x)$ is the estimation for all $x\in t$, we also define $T(x) = T(t)$.
%Define node $t$'s associated Gini impurity score as $G(E_t) = \frac{1}{2}\left(1 - E_t^2 - (1-E_t)^2\right) = E_t - E_t^2$. Note that the Gini impurity score is equivalent to variance in the case of a binary outcome. (Put this here is confusing. These things are not appear until Section 3.1, I suggest move these definitions there.)}

Consider using CART in the OSF LPC set up. The most common criterion function optimized by CART to determine the split $s^{CART}$ is the weighted sum of variances of the two child nodes, i.e.,
\begin{align}
    s^{CART} =& \argmin_{s \in (0,1)} \mathcal{G}^{CART}(s),
\end{align}
where
\begin{align}
    \mathcal{G}^{CART}(s) = &F_X(s){\rm Var}(Y| X\leq s) \nonumber \\
    &+ (1-F_X(s)){\rm Var}(Y| X > s).
\end{align}
In our case, since the outcome variable is binary, the variance of each node has a closed-form expression:
\begin{align}
    {\rm Var}(Y| X\leq s) =& \mu_L(s) - \mu^2_L(s) \\
    {\rm Var}(Y| X > s) =& \mu_R(s) - \mu^2_R(s)  
\end{align}
We first state a property of the CART splitting rule in Lemma~\ref{lemma mid point}. This  is useful for establishing the rest of the theoretical results.

% \begin{assumption}\label{assumption continuous X continuous eta}
%     $f$ and $\eta$ are continuous on $\mathcal{X}$.
% \end{assumption}


\begin{lemma} \label{lemma mid point}
$2\eta(s^{CART}) - \mu_L(s^{CART}) - \mu_R(s^{CART}) = 0$ and $\mu_L(s^{CART}) \neq \mu_R(s^{CART})$, 
% \begin{align}
%     &\eta(s) := \eta(X = s, \mathbf{X}\in t) = P(Y=1|X = s, \mathbf{X}\in t)\nonumber\\
%     &F(s | t) := F(X\le s | \mathbf{X}\in t)\nonumber\\
%     &\mu_L := \int_{X\in[0,s], \mathbf{X}\in t} \eta(x)f(x)dx/F(s)\label{El}\\
%     &\mu_R := \int_{X\in[s,1], \mathbf{X}\in t} \eta(x)f(x)dx/(1 - F(s)).\label{Er}
% \end{align}
\end{lemma}
Proofs for all lemmas and theorems are presented in Appendix \ref{appendix proofs}. Next, we formalize that CART does not minimize $R(s)$.%, and $s^{CART}$ is \textit{strictly dominated} (defined later) by some other splitting rule. %``some'' perturbation of the CART splitting rules in the ``correct'' direction can always improve the risk property of the splitting rule. 

\begin{definition} \label{Definition inadmissible}
    If $~\exists~ \{s,s'\} \in [0,1]^2$ such that $\forall\ x \in [0,1]$,
    \begin{align*}
        \mu_{t(x)}(s) > c \implies \mu_{t(x)}(s') > c  \quad &\text{when $\eta(x) > c$}, \\
        \mu_{t(x)}(s) \leq c \implies \mu_{t(x)}(s') \leq c \quad &\text{when $\eta(x) \leq c$},
    \end{align*}
    where
    \begin{align}
        \mu_{t(x)}(s) = 
        \begin{cases}
            ~\mu_L(s) ~\text{if} ~x \leq s, \\
            ~\mu_R(s) ~\text{if} ~x > s,
        \end{cases}
    \end{align}
    \textbf{and} there exists a set $\mathcal{A}$ with a nonzero measure such that, $\forall\ x \in \mathcal{A}$, either one (or both) of the following conditions is true
    \begin{align*}
        \mu_{t(x)}(s) \leq c, \mu_{t(x)}(s') > c  \quad &\text{when $\eta(x) > c$},  \\
        \mu_{t(x)}(s) > c, \mu_{t(x)}(s') \leq c \quad &\text{when $\eta(x) \leq c$}.
    \end{align*}
    Then we say that splitting rule $s'$ \textbf{strictly dominates} splitting rule $s$.
\end{definition}

%{\color{violet} We abuse the notation a little: $t_x$ in the subscript denotes the node that $x$ belongs to after splitting. (need clarification)}

\begin{lemma} \label{lemma dominate and small risk}
    If splitting rule $s'$ strictly dominates $s$, then $R(s') < R(s)$. %{\color{violet} (which risk, the new risk function? What is ``strictly lower risk''? It is not defined anywhere.)}.
\end{lemma}

Next, we show that if $c$ falls between the two node means, $\mu_L$ and $\mu_R$, then there exist some splitting rules that strictly dominate $s^{CART}$ and hence, have lower risk than the CART splitting rule. 

\begin{theorem}\label{theorem tendency}
    Suppose $c\in [c_{min}, c_{max}]$ where $c_{min} = \min(\mu_L(s^{CART}), \mu_R(s^{CART})) $ and $ c_{max} = \max(\mu_L(s^{CART}) , \mu_R(s^{CART}))$ and $\eta(s^{CART}) \neq c$.
    % $c\in (\min_x \eta(x), \max_x \eta(x))$. 
    There exists
    \begin{align*}
    s^* = \begin{cases}
        \begin{aligned}
            &\argmin_{s\in[0,s^{CART}), \eta(s) = c} (s^{CART} - s)\quad \text{if } \\
            & (\eta(s^{CART}) - c)(\mu_R(s^{CART}) - \mu_L(s^{CART})) > 0,
        \end{aligned} \\
        \\
        \begin{aligned}
            &\argmin_{s\in (s^{CART},1], \eta(s) = c} (s - s^{CART})\quad \text{if } \\
            & (\eta(s^{CART}) - c)(\mu_R(s^{CART}) - \mu_L(s^{CART})) < 0. 
        \end{aligned}
    \end{cases}
\end{align*}
    % \begin{align*}
    %     s^* = \begin{cases}
    %         \argmin_{s\in[0,s^{CART}), \eta(s) = c} (s^{CART} - s) \quad \text{if}\ (\eta(s^{CART}) - c)(\mu_R(s^{CART}) - \mu_L(s^{CART})) > 0\\ 
    %         \argmin_{s\in (s^{CART},1], \eta(s) = c} (s - s^{CART}) \quad \text{if}\ (\eta(s^{CART}) - c)(\mu_R(s^{CART}) - \mu_L(s^{CART})) < 0
    %     \end{cases}
    % \end{align*}
    Splitting node $t$ at any $s \in\begin{cases}
         [s^*, s^{CART}) \ \text{if}\ s^*< s^{CART}\\
         (s^{CART}, s^*] \ \text{if}\ s^*> s^{CART}
    \end{cases}$ \textbf{strictly dominates} $s^{CART}$, and the risk difference $R(s^{CART}) - R(s)$ is continuously monotone increasing with respect to $|s - s^{CART}|$.
\end{theorem}
%We provide some intuitions Theorem \ref{theorem tendency}.
Lemma~\ref{lemma mid point} and the assumption that $c\in [c_{min},c_{max}]$ jointly guarantee the existence of $s^*$. When $s$ ranges from $s^{CART}$ to $s^*$, the orders of $\mu_L$, $\mu_R$, and $c$ are preserved. Thus, moving from $s^{CART}$ to $s^*$ classifies more points into the correct categories following the intuition illustrated by Figure \ref{Figure Sine example}. 

\subsection{KD-CART does not optimize $R(s)$}
Knowledge distillation (KD) refers to a two-step learning algorithm. The first step trains a teacher model with a higher learning capacity. In the OSF LPC setup, a teacher model learns $\eta(x), x\in[0,1]$, denote the teacher model's prediction as $\hat{\eta}(x)$. The second step trains a student model which takes $\hat{\eta}(x)$ as the response variable. The goal of the student model is to output a simple/interpretable representation of the teacher model's knowledge. In the OSF LPC setup, the student model is a CART that takes in $\hat{\eta}(x)$ as the response and learns to partition the population based on $\hat{\eta}(x)$.

To explore the theoretical property of KD-CART, we treat it as a CART that takes true $\eta(x)$ as input and splits the population based on $\eta(x)$. KD-CART uses criterion function $\mathcal{G}^{KD} := F_X(s) {\rm Var}(\eta(X)|X\leq s) + (1-F_X(s)) {\rm Var}(\eta(X)|X> s)$. %This is further elaborated after we introduce the computation detail of KD-CART in Section \ref{sec simulation}. 

\begin{lemma} \label{lemma mid point for KD}
    Define $s^{KD} := \argmin_{s\in (0,1)} \mathcal{G}^{KD}(s)$. $2\eta(s^{KD}) - \mu_{L}(s^{KD}) - \mu_{R}(s^{KD}) = 0$ and $\mu_L(s^{KD}) \neq \mu_R(s^{KD})$.
\end{lemma}
Theorem \ref{theorem tendency} also applies to $s^{KD}$, as proof of Theorem \ref{theorem tendency} only requires that $\eta(s^{CART})$ is strictly between $\mu_L$ and $\mu_R$, Lemma \ref{lemma mid point for KD} shows that $\eta(s^{KD})$ satisfies this condition.% {\color{violet} (Please unify notations for $\mu_L(\mu_L)$ and $\mu_R(\mu_R)$.)}


    % Given the problem setup, for any node $t$ and dimension $p$, suppose $s^{CART}$ is the optimal theoretical split point and $c\in (\min_x \eta(x), \max_x \eta(x))$. If $s^{CART}\neq s^{*}$, let 
    % \begin{align*}
    %     s^L = \argmin_{s^*\in[0,1]} s^{CART} - s^*\\
    %     s^R = \argmin_{s^*\in[0,1]} s^* - s^{CART},
    % \end{align*}
    % s.t. $\eta(s^*,t) = c$.  Splitting node $t$ on any $s \in [s^L, s^R]$
    % % \begin{cases}
    % %      [s^*, s^{CART}) \ \text{if}\ s^*< s^{CART}\\
    % %      (s^{CART}, s^*] \ \text{if}\ s^*> s^{CART}
    % % \end{cases}$ 
    % results in a risk no greater than $s^{CART}$.


% \begin{corollary}
%     The splitting rule $s^{CART}$ is inadmissible.
% \end{corollary}

% \begin{corollary}
%     The splitting rule $s^*$ is admissible.
% \end{corollary}





\section{Modifying Final Splits} \label{sec penalized split}
% By Theorem \ref{theorem tendency}, a split at $s^*$ is preferred than at $s^{CART}$. Provided population distribution $\eta(X)$, $s^*$ can be identified by solving $\eta(X) = c$. When given finite samples, we can estimate $s^*$ based on $\eta(X)$'s empirical counterpart. 
% % . Such a solution is unique if the underlying $\eta$ function is monotonic; if not, we can obtain a finite number of solutions under mild conditions imposed on $\eta$ and pick the solution with the smallest risk. 
% {\color{violet} (name this method as EFS here, not later.) However, this often requires more computation resources and more observations in each node as it involves estimating empirical functions (more compare to what)}. 
% We supplement the experiment results of this method in the Appendix {\color{violet} [which section]}, it performs worse than {\color{violet} the new methods proposed in this paper (have you proposed yet?)}.
% % (it does perform well when the feature of interest is discrete and takes only a few distinct values.)
% % perform worse than that of the CART method as illustrated in Section \ref{sec simulation}. 


The suboptimality of CART and KD-CART for solving OSF LPC problem motivates our proposed methods: Penalized Final Split (PFS) and Maximizing Distance Final Split (MDFS). PFS improves the split by adding a penalty function of $|\mu_L(s) - c|$ and $|\mu_R(s) - c|$ on top of $\mathcal{G}$, shifting $s^{CART}$ towards $s^*$. MDFS, on the other hand, maximizes a weighted sum of $|\mu_L(s) - c|$ and $|\mu_R(s) - c|$. Given there exists a unique intersection between $\eta(x)$ and $c$, MDFS is able to point-identify $s^*$ such that $\eta(s^*)=c$.

Since our theoretical results guarantee a lower risk than CART's splitting rule based on one given node and feature, we advocate using our methods  at the final splits with features identified by CART. %{\color{violet} (all final splits) ?}. {\color{red} We call the first strategy Penalized Final Split (PFS), the second Maximum Distance Final Split (MDFS), and the empirical $\eta$ strategy Empirical Final Split (EFS) (name EFS earlier, name the other two in subtitles, then delete this)}. 
Restricting modifications to the last splits may appear trivial at first, however, note that as the tree grows deeper, the number of final splits increases exponentially, leading to non-trivial modifications to the policy designs. We substantiate this claim with empirical applications in Section \ref{sec empirical studies}.
%Though this theoretical result does not identify $s^*$, we show that with a penalized criterion function, the splitting rule strictly dominates the CART splitting rule. Secondly, by imposing monotonicity and differentiability assumptions on $\eta$, we propose another strategy that maximizes distances between $c$ and node estimates instead of impurity scores which can identify $s^*$. 
%(need to rewrite for better clarity.)

%{\color{violet} we write a single $\eta$ in a lot of cases, but I think in some cases it would be better to specify as $\eta(x)$ or $\eta(X)$, especially when discussing different subpopulations as having different range of features.}
\subsection{PFS%: Penalizing \texorpdfstring{$\mathcal{G}^{CART}$}{Lg}
}
Given our specific interest in classification based on whether $\eta(X) < c$, %{\color{violet} (did we start to use a single selected feature $X$ instead of $\mathbf{X}$ at some point?)}, 
we propose modifying the classic impurity function $\mathcal{G}^{CART}$ used to determining the split point $s$. Intuitively, if $\mu_L$ is close to $c$ and it is slightly higher than $c$, by the continuity assumption on $\eta$, it's highly likely that %{\color{violet} this node (which node, the node before splitting right?)} 
there is a considerable amount of subpopulation from the left node with $\eta(x) < c$ (see Figure \ref{Figure Sine example} for an example).
%{\color{violet} $\eta$ values (this is an example where I think $\eta(x)$ or $\eta(X)$ may be better)} both above and below $c$. {\color{violet} Assigning the same estimate would inevitably result in misclassifying one group. (I don't quite understand why ``assigning the same estimate'', is this what CART would do? Need some clarifications here.)}
% samples in this node are likely to have $\eta$ values greater and smaller than $c$ 
% These samples sharing the same estimate are unfavorable for our specific interest: determining the relationship between their $\eta$ values and $c$. 
Following this intuition, we consider adding a penalty to $\mathcal{G}^{CART}$ that discourages $\mu_L$ and $\mu_R$ from staying too close to $c$. Let
% This adjustment encourages the model to draw $\eta(s)$ closer to $c$ and helps to separate observations with $\eta$ values near $c$.
% Motivated by the particular interest of classifying whether $\eta(\mathbf{X}) < c$, we consider adding penalization when deciding on the split point $s$.  Since $\eta$ is unknown, we switch to penalize $\mu_L$ and $\mu_R$. Intuitively, 
% by penalizing $\mu_L$ and $\mu_R$ away from $c$, we could draw $\eta(s)$ closer to $c$. 
\begin{align}
    \mathcal{G}^{PFS}(s,c) 
    = \mathcal{G}^{CART} + \lambda \Big[ F_X(s) W(|\mu_L - c|)& \nonumber\\
    \qquad\qquad   + (1-F_X(s)) W(|\mu_R - c|)& \Big],\label{G_pfs}
\end{align}
% Define the penalized impurity score as 
% \begin{align}
%     G^{*}(E_t, c) = G(E_t) + \lambda W(|E_t - c|), \label{new_G}
% \end{align}
where $W: \mathbb{R}^{+} \cup 0 \to \mathbb{R}^{+}\cup 0$ is a decreasing function that penalizes small distances between the node means and $c$, and $\lambda\ge0$ controls the weight of $W$. %The penalized criterion function is denoted as 
%{\color{violet} Use $G^*(\cdot,c)$ in this equation.}

% \begin{align*}
%     &\mathcal{G}^*(s,c) = G^*(\mu_L)F(s) + G^*(\mu_R)(1-F(s)) \\
%     =& \mathcal{G} + \lambda \left(F(s) W(|\mu_L - c|) + (1-F(s)) W(|\mu_R - c|)\right)
%     %_{\text{A penalty term that tends to push $\{\mu_L, \mu_R\}$ away from $c$}}.
% \end{align*}


% \begin{definition} \label{Definition Pareto improvement}
%     Using the setup in Definition~\ref{Definition inadmissible}, splitting rule $s'$ is said to \textbf{strictly dominates} splitting rule $s$.
% \end{definition}

\begin{theorem} \label{theorem penalized loss works}
    Suppose $W: \mathbb{R}^+ \xrightarrow[]{} \mathbb{R}^+$ is convex, monotone decreasing, and upper bounded in second derivative. If $s^{CART}$ is the unique minimizer for $\mathcal{G}$, then there exists a $\Lambda > 0$ and $\eta(s^{CART}) \neq c$, such that for all $\lambda \in (0,\Lambda)$, $s^{PFS} := \argmin_{s\in [0,1]} \mathcal{G}^{PFS}(s,c)$ strictly dominates $s^{CART}$.
\end{theorem}
%The intuition behind Theorem \ref{theorem penalized loss works} is that the penalization term pushes the node estimates away from the threshold. 
We use Figure \ref{Figure Sine example} to illustrate the intuition behind Theorem \ref{theorem penalized loss works}. In this example, $\mu_L(s^{CART}) > \mu_R(s^{CART})$. Given $\mu_L(s^{CART}) + \mu_R(s^{CART}) = 2\eta(s^{CART})$ (Lemma~\ref{lemma mid point}) and $c = 0.75 > 0.5 = \eta(s^{CART})$, $\mu_L$ is closer to $c$ than $\mu_R$ to $c$. When $s$ moves slightly to the left of $s^{CART}$, $\mu_L$ increases and is pushed away from the threshold. Although $\mu_R$ is pulled closer to the threshold, the monotonicity and convexity of the weight function $W$ guarantee that the decrease in the penalty from pushing $\mu_L(s)$ away from $c$ is larger in magnitude than the increase in penalty from pulling $\mu_R(s)$ closer to $c$. Hence, the penalty term incentivizes moving the split from $s^{CART}$ towards $s^*$. %(note that $s^* = 5/12$ is on the left of $s^{CART} = 0.5$ in Figure \ref{Figure Sine example}). Provided that $\lambda$ is chosen in a way that satisfies the conditions specified in Theorem \ref{theorem penalized loss works}, the decrease in the penalty term outweighs the increase in $\mathcal{G}$ by pushing the split point to the left up to an $s \in (5/12,0.5)$. As a result, the penalized criterion function $\mathcal{G}^*$ moves the split towards $5/12$, reducing risk in the example. (I feel this part is a little bit repetitive.)

\begin{corollary} \label{corollary strictly smaller risk}
    Suppose $\lambda$ is chosen according to Theorem \ref{theorem penalized loss works}, then $R(s^{PFS}) < R(s^{CART})$.
\end{corollary}
%By Corollary \ref{corollary strictly smaller risk}, we can replace $G$ in the criterion function of CART with $G^*$. 

% Our theoretical result can guarantee a lower risk than CART's split decision for a given node and feature, but not for the entire tree involving recursive data partitioning. Hence, we advocate modifying the criterion function on the feature selected by CART only for the final split. 

% \section{Discussion}
% 



%\section{Empirical studies}

\subsection{MDFS
%: Maximizing distance 
%between \texorpdfstring{$\{\mu_L, \mu_R\}$}{Lg} and \texorpdfstring{$c$}{Lg}
}
PFS guarantees risk reduction with a properly chosen $\lambda$. However, it does not identify $s^*$. In this section, we propose a way to point-identify $s^*$ when $s^*$ is the unique intersection between $\eta(x)$ and $c$. We will explain why point-identification of $s^*$ is of policy significance in Section \ref{sec policy significance}.
%{\color{violet} In this section, we further impose Assumption \ref{assumption uniform X and monotonic eta} to obtain a stronger conclusion. (rewrite this for broader interest/ motivation/contribution, instead of purely ``stronger condition leads to stronger conclusion''. For instance, what is ``stronger conclusion''? Why it is important?)}

%The LPC policy which splits the node at $s^*$ is the unique admissible rule as it strictly dominates any other splitting at $s \neq s^*$. 

\begin{assumption} \label{assumption unique intersection between eta and c}
     $X \sim Unif[0,1]$, there exists a unique $s^*$ such that $\eta(s^*) = c$, and $\eta(X)$ is strictly monotonic and differentiable on $[s^*-\epsilon,s^*+\epsilon]$ for some $\epsilon > 0$, a neighborhood of $s^*$.
\end{assumption}
We argue that Assumption \ref{assumption unique intersection between eta and c} is not too restrictive because (i) the set of quantile statistics for any continuous feature follows a uniform distribution (ii) the unique intersect condition is met when $\eta(X)$ is monotonic. Monotonicity of $\eta(X)$ is a reasonable assumption that is easy to check for many empirical applications.

% \begin{definition} \label{Definition admissible}
%     Any rule strictly dominated by another rule is said to be \textbf{inadmissible}. Any splitting rule that is not inadmissible is said to be \textbf{admissible}.
% \end{definition}

\begin{theorem} \label{theorem unique intersection between eta and c}
    Under Assumption \ref{assumption unique intersection between eta and c}, 
    $\argmax_s \mathcal{G}^*(s,c)$ identifies $s^*$, where $\mathcal{G}^*(s,c) = s \left\lvert \mu_L - c \right\rvert + (1-s) \left\lvert \mu_R - c \right\rvert$.
    %where 
    %$\mathcal{G}^* = s \left\lvert \mu_L - c \right\rvert + (1-s) \left\lvert \mu_R - c \right\rvert$.
    
    % \[
    %     \max_s s \left\lvert \frac{\int_{0}^{s} \eta(x) dx }{s} - c \right\rvert + (1-s) \left\lvert \frac{\int_s^1 \eta(x) dx}{1-s} - c \right\rvert
    % \]
    
\end{theorem}
The proof of Theorem \ref{theorem unique intersection between eta and c} can be largely decomposed into two steps: first, we show that there exists an interval containing $s^*$ in which first-order condition guarantees that $s^*$ is the local minima; second, we show that for all $s$ outside of the said interval, $\mathcal{G}^*(s,c) < \mathcal{G}^*(s^*,c)$.

The unique intersect condition in Assumption \ref{assumption unique intersection between eta and c} is attained when $\eta(X)$ is monotonic.  Hence, Theorem \ref{theorem unique intersection between eta and c} yields the following Corollary. 


%{\color{violet} MDFS' policy strictly dominates CART and KD-CART (there is ambiguity here, i.e., is MDFS a method, an algorithm or a policy?  So far you only introduced LPC policy $s^*$ and CART policy $s^{CART}$. From my understanding MDFS is a method for identifying LPC policy.)}, generating policies that either target more vulnerable subpopulations using the same amount of targeting resources or uncovering additional latent groups whose $\eta(x) > c$.



%\begin{assumption} \label{assumption uniform X and monotonic eta}
    %{\color{violet} $X \sim Unif[0,1]$, and $\eta(X)$ is monotonic for all $X \in [0,1]$ and is strictly monotonic and differentiable in a neighborhood of $s^*$. (need to rewrite for better clarity.)}
%\end{assumption}
%Assumption \ref{assumption uniform X and monotonic eta}   In addition, contextual knowledge can often help determine whether the monotonicity assumption is valid. For example, it is generally safe to assume that higher income strictly decreases the probability of being financially constrained regarding social assistance targeting.

%{\color{violet} Need some transition sentences here before definition.}


\begin{corollary}\label{theorem monotonic eta uniform X}
    If $X \sim Unif[0,1]$, and $\eta(X)$ is monotonic $\forall~X \in [0,1]$ and is strictly monotonic and differentiable in a neighborhood of $s^*$, 
    then $\argmax_s \mathcal{G}^*(s,c)$ 
    identifies $s^*$.
\end{corollary} 

%The proofs are deferred to Appendix \ref{appendix proofs}. 

% {\color{violet}
% \begin{proof}
%     The bulk of the proof is deferred to Appendix \ref{appendix proofs}. Here, we emphasize that the strict monotonicity assumption of $\eta$ guarantees that $s^*$ is the only split that correctly classifies all points. Consequently, it dominates all the other rules and is the unique admissible rule.
% \end{proof}
% (If all proofs are deferred to Appendix \ref{appendix proofs}, do we have to sketch here? If it's important to leave some intuition and introduce Theorem \ref{theorem unique intersection between eta and c}, we can do that without the proof environment and combine with the next paragraph.)
% }
% Similar to Theorem \ref{theorem tendency}, we advocate for using $\mathcal{G}^*$ only for all final splits as the Theorem \ref{theorem monotonic eta uniform X} is only valid for a given node. %We anticipate Theorem \ref{theorem monotonic eta uniform X} to be useful when $c$ is on the same side of $\mu_L$ and $\mu_R$ when using CART, but some population at the node belongs to the targeting population. 

%Notice that the identification results suggested by Theorem \ref{theorem penalized loss works} and \ref{theorem monotonic eta uniform X} can be easily translated into their sample counterpart and used as a feasible criterion function to determine splits. The computation complexity of PFS and MDFS is the same as a typical CART. 
%{\color{violet} Need to be more careful for using $\eta(X)$ versus $\eta(x)$.}

%The proof of Theorem \ref{theorem monotonic eta uniform X} does not require monotonicity holds for the entire domain of $\eta(x)$. In fact, the key assumption for identifying $s^*$ is that $\eta(x)$ intersects the threshold $c$ only once. This observation inspires the following result.



\section{Large sample property of MDFS estimator} \label{sec Large sample property of MDFS estimator}

In finite sample regime, we estimate LPC policy $s^*$ using $\{(X_i,Y_i)\}_{i=1}^{n}$ by maximizing the sample analogue of ${\cal G}^*(s,c)$. For any $\epsilon > 0$, we have $s^{*} \in (\epsilon,1-\epsilon)$ following Assumption \ref{assumption unique intersection between eta and c}. The MDFS final split estimator $\hat{s}$ is defined as
\begin{align}
    \hat{s} = \argmax_{s\in(\epsilon,1-\epsilon)} \widehat{{\cal G}}^*(s,c),
\end{align}
where
\begin{align}
    \widehat{{\cal G}}^*(s,c)
    =
    s&~
    \Bigg|
        \frac
        {\sum_{i=1}^{n} Y_i\mathbbm{1}\{X_i \leq s\}}
        {\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}}
        -c
    \Bigg|+
    \notag\\
    &~(1-s)
    \Bigg|
        \frac
        {\sum_{i=1}^{n} Y_i \mathbbm{1}\{X_i > s\}}
        {\sum_{i=1}^{n}\mathbbm{1}\{X_i > s\}}
        -c
    \Bigg|
\end{align}
is the estimator of ${\cal G}^*(s,c)$.
%In the following theorem, we will show $\hat{s}$ is consistent as sample size $n$ goes to infinity.
%{\color{violet} [need additional assumption? Like bounded FOD of $\eta$?.]}
\begin{theorem} \label{theorem MDFS consistency}
    Under Assumption \ref{assumption unique intersection between eta and c}, $\hat{s} \overset{p}{\to} s^*$ as $n\to\infty$.
\end{theorem}
%\subsection{Asymptotic distribution}

%\subsection{Finite sample error bound}
Theorem \ref{theorem MDFS consistency} is the key result for establishing convergence property of risks. On the population level, $R^* := \inf_{s\in [0,1]} R(s) = 0$ by Assumption \ref{assumption unique intersection between eta and c}, since $s^*$ correctly classifies whether $\eta(x)$ is above $c ~\forall~ x \in [0,1]$. We show that the difference between the risk associated with MDFS estimator converges in probability to $R^*=0$. 
% Under Assumption \ref{assumption unique intersection between eta and c}, the excess risk (ER) $R(s) - R^*$ is equal to $R(s)$. 
%{\color{violet} [some transition sentences talking about empirical risk minimization.]}
\begin{definition}
    A splitting rule $\ddot{s}$ is said to be \textbf{Risk-consistent} if $R(\ddot{s}) \overset{p}{\to} 0$ as $n \to \infty$.
\end{definition}

\begin{corollary}
    $\hat{s}$ is Risk-consistent.
\end{corollary}
\begin{proof}
    $R(s)$ is continuous at $s^*$. Given Theorem \ref{theorem MDFS consistency}, $R(\hat{s}) \overset{p}{\to} R(s^*) = 0$ by Continuous Mapping Theorem.
\end{proof}
% To show the rate of convergence, we need the following assumption on the first derivative of $\eta(x)$.
% \begin{assumption} \label{assumption rate of convergence}
%     For any $s\in(0,1)$, $\eta'(x)<\infty$, $\eta'(s^*) \neq 0$
% \end{assumption}
% \begin{theorem}
%     Under Assumption \ref{assumption rate of convergence}, $\sqrt{n}(\hat{s} -s^*) = O_P(1)$.
% \end{theorem}

% \begin{corollary}
%     Under Assumption \ref{assumption rate of convergence}, $R(\hat{s}) = O_P(\sqrt{n})$.
% \end{corollary}


\section{Policy Significance of LPC} \label{sec policy significance}
In Section \ref{sec problem setup}, \ref{sec penalized split} and \ref{sec Large sample property of MDFS estimator}, we show that baseline methods, which are the predominant practice in empirical research, do not optimize LPC, while MDFS does. It is natural for one to ask why we should care about LPC in the first place.
To answer this question, here we demonstrate two advantages of LPC policies: (i) targeting more vulnerable subpopulations than CART by using the same amount of resources and (ii) uncovering subgroups with a higher-than-threshold probability of event $Y=1$ that CART ignores. 

Imagine there are two nodes $\{t_1,t_2\}$ with the same mass of population and corresponding features $X_1,X_2 \sim \rm{Unif}(0,1)$. We depict $\eta_1(X_1)$ for node $t_1$ in Figure \ref{Figure Unique Sine example} and $\eta_2(X_2)$ for node $t_2$ in Figure \ref{Figure Monotonic example}. The analytical forms of $\eta_1(X_1)$ and $\eta_2(X_2)$ are provided in Appendix \ref{appendix proofs}. Here we compare the targeted population using LPC policy versus CART policy.\footnote{In these two examples, CART and KD-CART generate identical policies because $\eta(x)$ is reflectional symmetric around $x =0.5$.}

Splitting nodes $t_1$ and $t_2$ individually at $s^{CART}$ versus $s^*$ results in different target subpopulations:
\begin{itemize}
    \item $s^{CART}$: Target $\{X_1<\frac{1}{2}|t_1\}$.
    \item $s^*$: Target $\{X_1<\frac{5}{12}|t_1\}$ and $\{\frac{11}{12}<X_2<1|t_2\}$.
\end{itemize}
Assuming that both $t_1$ and $t_2$ contain the same amount of population, the two sets of policies target the same proportion of the population, but $\eta_1(x) < 0.75$ for $x \in \{\frac{5}{12}<X_1<\frac{1}{2}\}$, which is targeted by $s^{CART}$, whereas $\eta_2(x) > 0.75$ for $x \in \{\frac{11}{12}<X_2<1\}$, which is targeted by $s^*$. Therefore, policies based on LPC, i.e., $s^*$ policy, target a \textbf{more vulnerable} subpopulation than CART/KD-CART policy. This idea corresponds to our diabetes empirical study in Section \ref{sec empirical studies}.

Admittedly, the fact that LPC and CART policies target the same proportion of the population in the previous example is by construction. It is also possible that the proportion of the population targeted by the LPC policy is bigger than that by CART or KD-CART. In this scenario, comparing the effectiveness of the two sets of policies is not straightforward. 
%Although LPC policy discovers new latent subgroup whose $\eta(\mathbf{x})$ is higher than the threshold $c$, e.g., $\{\frac{11}{12}<X_2<1|t_2\}$ in the previous example, it also costs more targeting resources if node $t_2$ has a larger mass of population than node $t_1$. 
Nonetheless, LPC still has policy significance. It discovers new latent groups with a higher probability of a bad event than the threshold, e.g. $\eta_2(x) > 0.75$ for $\frac{11}{12} < x < 1$. 
%With limited targeting resources, policymakers can trim out groups based on their preferences/objectives/concerns until the resource constraint is satisfied. 
%The new latent subgroup discovery allows policymakers to \textbf{make more informed targeting decisions}.

We make two remarks of Theorem \ref{theorem tendency} to formalize the two advantages that LPC policy offers.\footnote{The two Remarks also hold if we replace CART with KD-CART.
} Assuming a homogeneous targeting cost per unit of population, the targeting cost of a policy is the percentage of the subpopulation targeted. 
% Consider CART/KD-CART policy and LPC policy share all nodes up till the penultimate layer and only differ in terms of their final splits. 
Denote all $M$ final splitting nodes as $\{t_1,t_2,\dots,t_M\}$ and $\mathcal{M} = \{1,2,\dots,M\}$. For node $m$, we denote the feature selected by CART as $X_{(m)}$ and the CDF of $X_{(m)}$ as $F_m$ and let $\eta_m(x) = P(Y=1|X_{(m)} = x)$. The cost of CART and LPC policies are 
\begin{align}
    C^{CART}=&~ \sum_{m \in \mathcal{M}} \int_{0}^{1} \mathbbm{1}\{\mu_{t(x)}(s^{CART}_m) > c\} dF_m(x) \\
    C^{*}=&~ \sum_{m \in \mathcal{M}} \int_{0}^{1} \mathbbm{1}\{\mu_{t(x)}(s^{*}_m) > c\} dF_m(x)
\end{align}
Assume that for some 
$~ m \in \mathcal{M}, 
~s^{CART}_m \neq s^{*}_m$.
\begin{remark} \label{remark same cost}
    If $C^{CART} = C^{*}$, then LPC policy targets strictly more vulnerable $(\text{greater}~ \eta_m(X_{(m)}))$ subpopulation than CART using the same targeting resources.
\end{remark}
\begin{remark} \label{remark more cost}
    If $C^{CART} < C^{*}$, then LPC policy uncovers latent subgroups in some node $m$ with selected feature $X_{(m)} = x$ whose $\eta_m(x) > c$ that CART ignores.
\end{remark}

% \begin{table*}[htb]
% \centering
% \begin{tabular}{@{}l c rrr lll@{}}
% \toprule
% \multicolumn{1}{c}{\textbf{Settings}} & $c$ & \multicolumn{3}{c}{\textbf{Misclassification Error ($\%$)}} & \multicolumn{3}{c}{$F_1$\ \textbf{Score} ($\%$)} \\
% \cmidrule(lr){3-5} \cmidrule(lr){6-8}
% & & \textbf{CART} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{MDFS} & \textbf{PFS} \\ \midrule
% Hierarchical       & 0.8    & 16.5 (3.0) & \cellcolor[HTML]{9AFF99}{15.0 (2.4)} & 15.4 (2.6) & 79.8 (4.4) & 81.4 (3.5) & \cellcolor[HTML]{9AFF99}{81.6 (3.7)} \\
% High Dimensional   &0.8     & 18.4 (1.3) & \cellcolor[HTML]{9AFF99}{17.2 (1.1)} & 17.4 (1.1) & 62.1 (5.2) & 65.4 (3.2) & \cellcolor[HTML]{9AFF99}{65.5 (3.1)} \\
% Imbalanced Classes   &0.9    & 11.4 (2.4) & \cellcolor[HTML]{9AFF99}{9.3 (1.7)} & 9.5 (1.7) & 88.2 (2.9) & \cellcolor[HTML]{9AFF99}{90.3 (1.9)} & 90.2 (1.9)\\
% Interaction Effects     &0.9     & 23.7 (3.3)& 22.1 (2.9) & \cellcolor[HTML]{9AFF99}{22.1 (2.9)} & 72.9 (5.7) & 74.8 (4.6) & \cellcolor[HTML]{9AFF99}{75.1 (4.5)} \\
% Linear Separable      &0.8        & 9.7 (3.1) & \cellcolor[HTML]{9AFF99}{8.2 (2.5)} & 8.5 (2.6)& 71.8 (9.2) & 75.4 (6.2) & \cellcolor[HTML]{9AFF99}{76.2 (6.2)} \\
% Mixed Types           &0.8        & 3.4 (2.0) & \cellcolor[HTML]{9AFF99}{3.0 (1.9)} & 3.2 (1.9) & 70.4 (18.9) & \cellcolor[HTML]{9AFF99}{74.0 (17.6)} & 71.4 (18.8) \\
% Multicollinear     &0.8           & 10.3 (2.6)& \cellcolor[HTML]{9AFF99}{9.3 (2.7)} & 9.7 (2.7) & 69.5 (8.1) & 72.7 (6.8) & \cellcolor[HTML]{9AFF99}{73.5 (6.8)}\\
% Nonlinear Separable     &0.8      & 24.2 (7.4) & \cellcolor[HTML]{9AFF99}{18.8 (4.7)} & 19.1 (4.5) & 52.9 (36.5) & 72.9 (18.8) & \cellcolor[HTML]{9AFF99}{73.4 (18.8)} \\
% Noisy Data           &0.8         & 12.6 (1.9) & \cellcolor[HTML]{9AFF99}{11.1 (1.8)} & 11.3 (1.9)& 65.9 (7.2) & \cellcolor[HTML]{9AFF99}{69.1 (5.2)} & 68.9 (5.4) \\
% Periodic Function     &0.8        & 10.8 (4.6) & \cellcolor[HTML]{9AFF99}{10.2 (4.4)} & 10.7 (4.4)& 72.8 (19.9) & 74.7 (15.9) & \cellcolor[HTML]{9AFF99}{74.7 (15.3)} \\
% Step Function       &0.9        & 5.1 (1.6) & \cellcolor[HTML]{9AFF99}{3.3 (1.6)} & 4.1 (1.8) & 49.8 (21.2) & \cellcolor[HTML]{9AFF99}{71.4 (16.2)} & 68.3 (14.9)\\
% \bottomrule
% \end{tabular}
% \caption{Comparison of misclassification error and $F_1$ score among CART, MDFS, and PFS. Each entry records the average and standard deviation (in parathesis) of the misclassification error or $F_1$ score over 100 experiments. The lowest misclassification error and the highest $F_1$ score in each simulation setting are highlighted in green indicating the best performance.}
% \label{Table simulation results in main text}
% \end{table*}

\begin{table*}[htb]
\centering
\begin{tabular}{@{}lc|rrrr|rrr@{}}
\toprule
 \textbf{DGP} & $c$ & \textbf{CART} & \textbf{PFS} & \textbf{MDFS} & \textbf{wEFS} & \textbf{RF-CART} & \textbf{RF-MDFS} \\
 \midrule
  Ball               & 0.8          & 9.6 (1.9)     & 9.2 (1.7)     & 9.2 (1.4)     & 9.2 (1.4)     & 8.8 (1.8)         & 8.6 (1.4)         \\
               & 0.7          & 14.5 (2.4)    & 14.2 (2.2)    & 13.7 (2.3)    & 13.7 (2.3)    & 13.2 (2.1)        & 12.6 (1.8)        \\
               & 0.6          & 14.9 (1.7)    & 14.5 (1.9)    & 14.1 (1.4)    & 14.1 (1.5)    & 13.9 (1.9)        & 13.2 (1.5)        \\
 \midrule
 Friedman \#1          & 0.8          & 6.8 (1.3)     & 6.7 (1.3)     & 6.7 (1.4)     & 6.7 (1.4)     & 6.0 (1.0)         & 6.3 (1.0)         \\
         & 0.7          & 12.7 (1.6)    & 12.6 (1.5)    & 12.5 (1.3)    & 12.5 (1.3)    & 11.8 (1.3)        & 11.7 (1.1)        \\
           & 0.6          & 18.5 (1.7)    & 18.4 (1.6)    & 18.2 (1.5)    & 18.2 (1.5)    & 18.1 (1.5)        & 18.0 (1.3)        \\
 \midrule
 Friedman \#2          & 0.8          & 8.8 (2.2)     & 8.0 (1.6)     & 7.5 (1.5)     & 7.5 (1.5)     & 8.4 (1.7)     & 7.3 (1.3)         \\
           & 0.7          & 8.2 (1.7)     & 7.9 (1.6)     & 7.7 (1.5)     & 7.7 (1.5)     & 7.9 (1.9)         & 7.4 (1.6)         \\
           & 0.6          & 6.9 (1.5)     & 6.7 (1.5)     & 6.4 (1.4)     & 6.4 (1.5)     & 6.7 (1.9)         & 6.2 (1.3)         \\
 \midrule
 Friedman \#3          & 0.8          & 2.0 (0.5)     & 2.0 (0.5)     & 2.0 (0.5)     & 2.0 (0.5)     & 1.9 (0.5)         & 1.9 (0.5)         \\
           & 0.7          & 2.5 (0.5)     & 2.5 (0.6)     & 2.4 (0.5)     & 2.4 (0.5)     & 2.4 (0.5)         & 2.4 (0.5)         \\
          & 0.6          & 3.2 (0.7)     & 3.3 (0.7)     & 3.0 (0.5)     & 3.0 (0.5)     & 3.1 (0.5)         & 3.0 (0.5)         \\
 \midrule
 Poly \#1              & 0.8          & 7.3 (1.4)     & 7.2 (1.3)     & 7.3 (1.2)     & 7.3 (1.2)     & 7.2 (1.3)         & 7.0 (1.1)         \\
               & 0.7          & 11.0 (2.4)    & 10.4 (2.1)    & 10.1 (1.7)    & 10.1 (1.7)    & 9.9 (1.9)         & 9.4 (1.5)         \\
               & 0.6          & 12.9 (2.0)    & 12.6 (2.8)    & 12.2 (2.7)    & 12.2 (2.6)    & 12.1 (1.8)        & 11.9 (2.6)        \\
 \midrule
 Poly \#2              & 0.8          & 10.1 (2.5)    & 10.0 (2.2)    & 9.9 (2.1)     & 9.9 (2.2)     & 9.6 (2.1)         & 9.4 (2.0)         \\
               & 0.7          & 9.8 (2.2)     & 9.5 (1.9)     & 9.4 (1.8)     & 9.5 (1.8)     & 9.6 (1.9)         & 9.6 (1.6)         \\
               & 0.6          & 8.7 (1.9)     & 8.5 (1.6)     & 8.0 (1.5)     & 8.0 (1.5)     & 8.1 (1.6)         & 7.6 (1.3)         \\
 \midrule
 Ring               & 0.8          & 18.9 (2.3)    & 19.0 (2.5)    & 18.6 (2.7)    & 18.5 (2.6)    & 17.4 (2.3)        & 17.5 (2.4)        \\
                & 0.7          & 15.6 (2.0)    & 15.7 (2.3)    & 15.5 (2.3)    & 15.5 (2.3)    & 14.3 (1.8)        & 14.3 (1.7)        \\
                & 0.6          & 13.2 (1.7)    & 13.1 (1.5)    & 12.7 (1.4)    & 12.7 (1.4)    & 12.1 (1.4)        & 11.8 (1.5)        \\   
\midrule
Collinear 	& 0.8 & 6.8 (1.7)	& 6.7 (1.6) &	6.3 (1.5) &	6.3 (1.5) &	6.3 (1.4) & 5.9 (1.3)\\
            & 0.7 & 7.7 (1.4)	& 7.7 (1.4) &	7.3 (1.2) & 7.3 (1.2) & 7.2 (1.7) & 7.0 (1.0) \\
            & 0.6 & 8.5 (1.4)   & 8.2  (1.2) & 7.9 (1.2) & 7.9 (1.2) & 8.4 (1.3) & 7.8 (1.2)\\
\bottomrule
\end{tabular}
\caption{Comparison of misclassification error among CART, PFS, MDFS, and wEFS using raw data input, and CART, and MDFS using random forest as KD tool. Each entry presents the average and standard deviation (in parentheses) of the misclassification error over 50 experiments.}
\label{Table simulation results in main text}
\end{table*}

\section{Related literature} \label{sec related literature}
\textbf{Cost-sensitive binary classification}: The LPC policy can be related to the optimal solution of a cost-sensitive binary classification problem, further highlighting the policy significance of the LPC problem. \citet{nan2012optimizing,menon2013statistical,koyejo2014consistent} show that the optimal classifier of a cost-sensitive binary classification problem is determined by whether the latent probability is above some performance-metric-dependent threshold. Hence, we modify weighted ERM, a popular approach in the literature to our setup. However, whether the theoretical results in the cost-sensitive classification literature carry over to our setup is unclear. For example, \citet{nan2012optimizing} shows risk consistency with uniform convergence of the empirical risk. In our setup, $\{\mu_L(s),\mu_R(s)\}$ appear inside the indicator functions, and uniform convergence of empirical risk is not guaranteed even with a large sample. Another example is that \citet{koyejo2014consistent} proves risk-consistency of weighted ERM assuming that the policymakers search through all real-valued functions. In our setup, policymakers only search among threshold-type policies. 

\textbf{Knowledge distillation}: \citet{liu2018improving,dao2021knowledge} use CART as the student model in knowledge distillation. We show that some of the theoretical results we develop for CART apply to their work (termed KD-CART in our paper). %Hence, we implement RF-MDFS in our experiments.

\section{Experiments}
%To summarize, we've proposed PFS, MDFS, and 
In this section, we conduct comprehensive numerical experiments to compare the performance of our proposed algorithms and the classic CART under both regular and KD frameworks. 
Under regular framework, we compare the classic CART with PFS and MDFS. Under KD framework, we compare RF-CART with RF-MDFS. We replace KD with RF because the teacher model is a \textbf{R}andom \textbf{F}orest.
In addition, we adapt Proposition 6 from \citet{koyejo2014consistent} to the final split and refer to the adapted method as wEFS (weighted Empirical Final Split).%\footnote{wEFS is also a novel final-split approach proposed by this paper. We don't include it as a main contribution due to the lack of theoretical guarantee.} %We also apply these methods to real-world datasets and relate our empirical findings to Remark \ref{remark same cost} and \ref{remark more cost} in Section \ref{sec policy significance}.

\subsection{Synthetic Data} \label{sec simulation}
% CART MDFS PFS wEFS KD-CART KD-MDFS
\textbf{Settings:}
We simulate synthetic datasets using 8 different data generation processes (DGP), e.g., the Friedman synthetic datasets \citep{friedman1991multivariate, breiman1996bagging}. These setups are designed to capture various aspects of real-world data complexities and challenges commonly encountered when using tree-based method, such as nonlinear relationships, feature interactions, collinearity, and noises (Table~\ref{Table simulation results in main text}, Column 1). Detailed descriptions of the DGP are provided in Appendix~\ref{Data generation details}.
For each DGP, we consider a threshold of interest $c\in\{0.6, 0.7, 0.8\}$, resulting in 24 unique tasks.

We set two stopping rules for growing the tree: (i) a max depth of $m\in\{4, 5, 6, 7\}$, (ii) a minimal leaf node size of $\rho n$, where $\rho \in \{1\%, 2\%, 3\%\}$ and $n$ is the sample size. We set $n = 5000$. The fitting procedures stop once either of the rules is met. These give us 12 configurations for each tasks. For each of the $24\times 12 = 288$ settings, we do 50 replicates of experiments.

\textbf{Evaluation metrics}: 
% \textcolor{violet}{Due to $c$'s choice of large values, the number of observations with $\eta(X)$ above $c$ is much less than below (seems unrelated).} 
We use the misclassification rate (MR) and the standard deviation of it to evaluate the performance and robustness of different approaches. We abuse the notation of $\eta$ by defining $\eta(\mathbf{X}_i) := \mathbb{P}(Y_i=1|\mathbf{X}_i)$ where $\mathbf{X}_i$ is the \textit{multivariate} feature vector which includes \textit{all} features in the simulation.
\begin{align}
    \text{MR} = \frac{1}{n}\sum_{i=1}^n \mathbbm{1}\{ (\widehat{T}(\mathbf{X}_i)-c)(\eta(\mathbf{X}_i)-c)<0\},
    % &F_1\ \text{score} = \frac{TP}{TP+(FP+FN)/2},\label{f1_score}
\end{align}
where $\widehat{T}(\mathbf{X}_i)$ is the tree-estimate of $\eta(\mathbf{X}_i)$. Although there is a gap between this evaluation metric (using $\eta(\mathbf{X}_i)$) and our theories (using $\eta(X_i)$), we justify the use of MR in two ways. First, obtaining $\eta(X)$ for all final splits involve iterative integration of $\eta(\mathbf{X})$. In our simulation setups, $\eta(\mathbf{X})$ does not have closed-forms for their integrals. Second, MR is still of interest to policymakers. A policymaker cares about classifying individual $i$ based on their probability of an event $Y = 1$ conditional on \textit{all} their features.

% \begin{align*}
    % TP &= \sum_{i=1}^n \mathbbm{1}\{\widehat{E}_t > c, \eta(\mathbf{X}_i) > c\},\\
    % FP + FN & = \sum_{i=1}^n \mathbbm{1}\{ (\widehat{T}(\mathbf{X}_i)-c)(\eta(\mathbf{X}_i)-c)<0\}
    % \widehat{E}_t &= \frac{1}{|t|} \sum_{i=1}^n Y_i\mathbbm{1}\{\mathbf{X}_i\in t\},\ X\in t
    % FP &= \sum_{i=1}^n \mathbbm{1}\{\widehat{T}(\mathbf{X}_i)<c, \eta(\mathbf{X}_i) > c\},\\
    % FN &= \sum_{i=1}^n \mathbbm{1}\{\widehat{T}(\mathbf{X}_i)>c, \eta(\mathbf{X}_i) < c\}.
    %TN &= \sum_{i=1}^n \mathbbm{1}\{(\widehat{T}(\mathbf{X}_i) <c, \eta(\mathbf{X}_i)<c\}.
% \end{align*}
% $|t|$ is the number of samples in leaf $t$.
% For computational efficiency, each method identify $s$ from a set of candidate splits corresponding to the $5\%$, $10\%, \dots, 95\%$ quantile of the sampling distribution of $X$. 

\textbf{Model fitting procedures}: As we mentioned in Section~\ref{sec penalized split}, our methods and their counterparts only differ in the splits at the final level (final splits).
We first implement CART until the final split is reached based on the stopping rules. 
% We consider the current split as a final split when either the max depth is reached or the sample size of either leaf node, presumably split by CART, reaches the minimal percentage $\rho$. 
Once the final split is identified, we select the splitting feature using CART, then implement PFS, MDFS, and wEFS with the splitting feature. MDFS and PFS decide the split by optimizing $\mathcal{G}^*$ and $\mathcal{G}^{PFS}$, respectively, while wEFS selects the split by identifying $s$ such that the minimum weighted empirical risk is obtained (see Algorithm~\ref{Calculate_risk} in Appendix~\ref{algor} for the computation detail of weighed empirical risk).

For $W$ in $\mathcal{G}^{PFS}$ from \eqref{G_pfs}, we choose $W(d) = 1 - d$. This choice of $W$ satisfies conditions specified in Theorem \ref{theorem penalized loss works}. %but also matches the maximum distance intuition proposed in MDFS. PFS becomes equivalent to MDFS when $\lambda$ goes to infinity. 
We also experiment with alternative choices, such as $W(d)=(1-d)^2$ and $W(d)=\exp(-d)$, but observed similar performance across these choices.
% We choose $\lambda = 0.1$ as a small value that imaginably satisfies the requirement for $\lambda$ in Theorem~\ref{theorem penalized loss works}.
Based on Theorem~\ref{theorem penalized loss works}, we set $\lambda=0.1$, a sufficiently small value that presumably satisfies the conditions of the theorem while maintaining practical effectiveness.

    % \item \textbf{Choice of $\lambda$ for PFS}: 
% \textcolor{violet}{The split halts if any prospecting leaf node has less than 10 observations. (I need clarifications)}
% \begin{itemize}
    % \item \textbf{Choice of $W$ for PFS}: We choose $W(d) = 1 - d$. This penalty not only satisfies the monotonicity, convexity, and bounded second derivative requirements but also matches the maximum distance intuition proposed in MDFS. PFS becomes equivalent to MDFS when $\lambda$ goes to infinity. We also try using other penalties like $(1-d)^2$, $exp(-d)$ but the performance are similar.  
    % \item \textbf{Choice of $\lambda$ for PFS}: 
    % We select $\lambda$ using an approach that combines the idea of cross-validation and the honest approach \citep{athey2016recursive}. Firstly, divide the data into five folds. For a fixed candidate $\lambda$ value, by leaving one fold out each time, we can construct 5 PFS trees, $\widehat{T}^{-k}$, $k\in\{1,2,3,4,5\}$.  Then, we can obtain an unbiased estimate of $\eta$ for each node by feeding the left-out fold to $\widehat{T}^{-k}$. Denote the estimate given by the four folds for observation $\mathbf{X}$ as $\widehat{T}^{-k}(\mathbf{X})$ and the unbiased estimate given by the left-out fold as $\widehat{\eta}^k(\mathbf{X})$. By \eqref{f1_score}, we can calculate an $F_1$ score by replacing $\widehat{T}(\mathbf{X})$ with $\widehat{T}^{-k}(\mathbf{X})$ and $\eta(\mathbf{X})$ with $\widehat{\eta}^k(\mathbf{X})$ over four folds. Finally, we choose the $\lambda$ value that gives the maximum average $F_1$ score.
% \end{itemize}
% Denote all the nodes in $\widehat{T}$ as $\mathcal{T}$ and their estimated mean as $\hat{p}_\mathcal{T}$. We then use the fifth fold to estimate a different set of node mean for $\hat{T}$, denoted as $\tilde{p}_\mathcal{T}$. $\tilde{p}_\mathcal{T}$ is an unbiased estimate of the true mean for $\mathcal{T}$ and hence, we use $\tilde{p}_\mathcal{T}$ as the ground truth. Denote the fifth set as $S$ and its size as $N_S$, we compute true positive ($TP$), false positive ($FP$), and false negative ($FN$) as follows:
% % \begin{align*}
% %     TP = \sum_{i=1}^{N_S} \mathbbm{1}(\hat{p}_{\mathcal{T}_i} > c, \tilde{p}_{\mathcal{T}_i} > c) \quad FP = \sum_{i=1}^{N_S} \mathbbm{1}(\hat{p}_{\mathcal{T}_i} > c, \tilde{p}_{\mathcal{T}_i} < c) \quad
% %     FN = \sum_{i=1}^{N_S} \mathbbm{1}(\hat{p}_{\mathcal{T}_i} < c, \tilde{p}_{\mathcal{T}_i} > c) %&\qquad TN = \sum_{i=1}^{N_S} \mathbbm{1}(\hat{p}_{\mathcal{T}_i} < c, \tilde{p}_{\mathcal{T}_i} < c) \\
% % \end{align*}
% where $\mathcal{T}_i$ denotes the node that observation $i$ from set $S$ belongs to among set of nodes $\mathcal{T}$. We compute the $F_1$ score based on these three statistics and select the $\lambda$ with the largest $F_1$ score.
% \leftskip 0pt 
% We use the same model fitting procedures in the empirical studies in Section~\ref{sec empirical studies} and these procedures are presented in Appendix~\ref{algor}. 

 

% Since The performance are evaluated using both the misclassification error and the $F_1$ score ($\frac{TP}{TP + (FN + FP)/2}$).


% The weight function is $\lvert x-c \rvert$ where $c$ is the threshold. 
%As mentioned earlier, if $\eta$ is monotonic with respect to $s$, directly identifying $s^* = \eta^{-1}(c)$ outputs a splitting rule with $0$ risk. Based on this identification result, we also consider using the empirical cumulative distribution function to estimate $\eta$ and decide $s$ on the final split (EFS).

% This simulation framework generates multi-dimensional synthetic datasets, encompassing various typical data settings encountered in practice. 
% By controlling the underlying probability structures, we can assess how each method estimates decision boundaries ($s$). 
% To make the comparison fair, we constrain all three methods to consider $m$ number of candidate splits for each feature. We set $ m=20$ in our simulations. Algorithm~\ref{algor_pfs} gives the details of the three methods.



\textbf{Results}: Out of 288 simulation settings, PFS, MDFS, and wEFS outperform CART in terms of MR $78.13\%$, $87.15\%$, $88.19\%$ of the time, respectively. In Table~\ref{Table simulation results in main text}, we only present the lowest MR among all 12 configurations (max depth and minimal leaf node size combinations) for all 24 tasks (DGP and $c$ combinations). In Table~\ref{Table simulation results in main text}, PFS, MDFS, and wEFS outperform CART $83.33\%$, $100\%$, and $100\%$ of the time, respectively. 
%The superior performance by MDFS is due to its nature of minimizing misclassification errors when dealing with monotonic $\eta$, which is present in most data generation settings.
%The superior performance of PFS and MDFS over CART can potentially be explained 

Our methods have a clear advantage over classic CART. However, the selection of $\lambda$ and uniqueness of intersection between $\eta$ and $c$ can hardly be validated in reality. The theoretical guarantee of PFS, MDFS, and wEFS under more general conditions remains as an interesting future research direction.


%\textcolor{violet}{We intuitively rationalize the superior performance of MDFS by the following conjecture. CART selects splitting feature based on minimizing weighted sum of variances, so we speculate that CART is likely to choose a splitting feature with a monotonic $\eta$.} Then, Corollary \ref{theorem monotonic eta uniform X} and Theorem \ref{theorem MDFS consistency} guarantee a good performance of MDFS when sample size is sufficiently large.

%We believe further investigation of wEFS's theoretical property is an interesting future research direction to better understand its superior performance and robustness in various settings. 
% Compared to CART, PFS's average increase in $F_1$ score is 0.135 when $d = 2$, 0.084 when $d=3$, and 0.060 when $d = 4$. 
% For MDFS, the increases are 0.114, 0.072, and 0.055, respectively. This trend is expected since our strategy only operates at the final split. The information available in each node diminishes as the tree deepens. The later the strategy is applied, the smaller the performance improvement.
% PFS can continue to enhance CART’s performance at the final split as long as valid information remains in the node.
% We anticipate our method performing well in random forest methods, where each tree is typically not very deep. In this context, PFS’s advantage in handling relatively shallow trees can be fully leveraged. 
% Regarding misclassification error, the natural imbalance caused by the split at $c$ makes it harder to observe significant progress.

% If we compare the percentage of decrease in misclassification error: PFS achieves an average decrease of $1.46\%$, $1.53\%$, $1.88\%$, and $3.55\%$ for $d=4,5,6$ and $7$ respectively. MDFS has an average decrease of $4.27\%$, $3.33\%$, $4.88\%$, and $6.42\%$. 

Under KD framework, RF-MDFS beats RF-CART 73.6\% of the time out of the 288 simulation settings, 83.3\% of the time when only selecting the best configuration in each task. 
% PFS and MDFS also demonstrate more consistent classification performance than CART, with the standard deviation of both $F_1$ score and misclassification error being significantly lower. Specifically, the standard deviation of $F_1$ score for PFS and MDFS is $64\%$ and $63\%$, respectively, of CART's averaged across 89 configurations. 
% The results align with our theorems as PFS is proven to strictly dominate CART under more general continuity conditions whereas MD strictly dominates CART under more restrictive Assumption \ref{assumption uniform X and monotonic eta}.
Due to the page limit, we present only part of the simulation results in Table~\ref{Table simulation results in main text}. The complete simulation results are provided in the supplemental files. 
% In the table, PFS and MDFS outperform CART in all simulation settings except when the data is periodic, likely due to periodic data's violation of the continuity and monotonicity assumptions. 

% \textbf{Simulation settings}
% We use $d$ to denote the number of features. $d = 8$ for all the cases except for HighDimensional, we set $d = 20$. $n =500$ for \\%%l settings. The maximum depth of a tree is set to be 3. 
% \begin{table*}[htb]
% \centering
% \begin{tabular}{@{}l c cccc cccc@{}}
% \toprule
% \multicolumn{1}{c}{\textbf{Simulation Settings}} & $c$ & \multicolumn{4}{c}{\textbf{Misclassification Error}} & \multicolumn{4}{c}{$F_1$\ \textbf{Score}} \\
% \cmidrule(lr){3-6} \cmidrule(lr){7-10}
% & & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} \\ \midrule
% Hierarchical                & 0.2 & 0.033 & 0.030 & \textbf{0.030} & 0.032 & 0.264 & 0.349 & 0.393 & \textbf{0.395} \\
% High Dimensional            & 0.2 & 0.297 & 0.297 & 0.289 & \textbf{0.288} & 0.358 & 0.322 & 0.386 & \textbf{0.402} \\
% Imbalanced Classes          & 0.1 & 0.114 & 0.113 & \textbf{0.093} & 0.095 & 0.882 & 0.878 & \textbf{0.903} & 0.902\\
% Interaction Effects         & 0.2 & 0.118 & 0.115 & \textbf{0.114} & 0.115 & 0.307 & 0.295 & 0.360 & \textbf{0.367} \\

% Linear Separable            & 0.1 & 0.126 & 0.115 & \textbf{0.108} & 0.110 & 0.284 & 0.371 & 0.506 & \textbf{0.523} \\
% Mixed Types                 & 0.2 & 0.076 & 0.079 & \textbf{0.074} & 0.075 & 0.509 & 0.452 & \textbf{0.544} & 0.540 \\
% Multicollinear              & 0.1 & 0.058 & 0.054 & \textbf{0.052} & 0.055 & 0.522 & 0.499 & \textbf{0.596} & 0.590 \\
% Nonlinear Separable         & 0.3 & 0.361 & 0.365 & 0.323 & \textbf{0.317} & 0.436 & 0.431 & 0.596 & \textbf{0.613} \\
% Noisy Data                  & 0.1 & 0.131 & 0.121 & \textbf{0.117} & 0.117 & 0.287 & 0.364 & 0.459 & \textbf{0.476} \\
% Periodic Function           & 0.3 & 0.052 & \textbf{0.052} & 0.053 & 0.054 & 0.293 & 0.279 & 0.318 & \textbf{0.319} \\
% Step Function               & 0.1 & 0.287 & 0.326 & 0.274 & \textbf{0.270} & 0.614 & 0.520 & 0.634 & \textbf{0.642} \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of misclassification error and $F_1$ Score among CART, EFS, MDFS, and PFS with max depth being 4 and threshold $c$. Each misclassification error and $F_1$ score is an average of 100 experiments. The lowest misclassification error and the highest $F_1$ score in each simulation setting are highlighted in bold indicating the best performance.}
% \label{Table simulation results in main text}
% \end{table*}

% \begin{table*}[htb]
% \centering
% \begin{tabular}{@{}l llll llll@{}}
% \toprule
% \multicolumn{1}{c}{\textbf{Settings}} & \multicolumn{4}{c}{\textbf{Misclassification Error ($\%$)}} & \multicolumn{4}{c}{$F_1$\ \textbf{Score} ($\%$)} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9}
% & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} \\ \midrule
% Hierarchical               & 3.3 & 3.0 & \textbf{3.0} & 3.2 & 26.4 (14.7) & 34.9 (12.8) & 39.3 (10.1) & \textbf{39.5 (10.2)} \\
% High Dimensional           & 29.7 & 29.7 & 28.9 & \textbf{28.8} & 35.8 (11.6) & 32.2 (10.4) & 38.6 (6.7) & \textbf{40.2 (6.9)} \\
% Imbalanced Classes         & 11.4 & 11.3 & \textbf{9.3} & 9.5 & 88.2 (2.9) & 87.8 (3.2) & \textbf{90.3 (1.9)} & 90.2 (1.9)\\
% Interaction Effects        & 11.8 & 11.5 & \textbf{11.4} & 11.5 & 30.7 (13.4) & 29.5 (13.4) & 36.0 (10.6) & \textbf{36.7 (11.3)} \\
% Linear Separable           & 12.6 & 11.5 & \textbf{10.8} & 11.0 & 28.4 (25.7) & 37.1 (16.5) & 50.6 (8.4) & \textbf{52.3 (7.6)} \\
% Mixed Types                & 7.6 & 7.9 & \textbf{7.4} & 7.5 & 50.9 (14.8) & 45.2 (18.2) & \textbf{54.4 (11.9)} & 54.0 (12.4) \\
% Multicollinear             & 5.8 & 5.4 & \textbf{5.2} & 5.5 & 52.2 (18.1) & 49.9 (20.5) & \textbf{59.6 (9.9)} & 59.0 (9.5)\\
% Nonlinear Separable        & 36.1 & 36.5 & 32.3 & \textbf{31.7} & 43.6 (40.0) & 43.1 (34.3) & 59.6 (26.9) & \textbf{61.3 (27.7)} \\
% Noisy Data                 & 13.1 & 12.1 & \textbf{11.7} & 11.7 & 28.7 (23.5) & 36.4 (16.0) & 45.9 (10.3) & \textbf{47.6 (9.9)} \\
% Periodic Function          & 5.2 & \textbf{5.2} & 5.3 & 5.4 & 29.3 (12.1) & 27.9 (13.1) & 31.8 (11.4) & \textbf{31.9 (11.4)} \\
% Step Function              & 28.7 & 32.6 & 27.4 & \textbf{27.0} & 61.4 (8.2) & 52.0 (12.2) & 63.4 (3.6) & \textbf{64.2 (3.9)} \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of misclassification error and $F_1$ Score among CART, EFS, MDFS, and PFS with max depth being 4. Each misclassification error and $F_1$ score is an average of 100 experiments. The number in the parathesis represents $F_1$ score's standard deviation. The lowest misclassification error and the highest $F_1$ score in each simulation setting are highlighted in bold indicating the best performance.}
% \label{Table simulation results in main text}
% \end{table*}

% \begin{table*}[htb]
% \centering
% \begin{tabular}{@{}l rrr lll@{}}
% \toprule
% \multicolumn{1}{c}{\textbf{Settings}} & \multicolumn{3}{c}{\textbf{Misclassification Error ($\%$)}} & \multicolumn{3}{c}{$F_1$\ \textbf{Score} ($\%$)} \\
% \cmidrule(lr){2-4} \cmidrule(lr){5-7}
% & \textbf{CART} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{MDFS} & \textbf{PFS} \\ \midrule
% Hierarchical               & 3.3 (0.6) & \cellcolor[HTML]{9AFF99}{3.0 (0.6)} & 3.2 (0.7) & 26.4 (14.7) & 39.3 (10.1) & \cellcolor[HTML]{9AFF99}{39.5 (10.2)} \\
% High Dimensional           & 29.7 (1.6) & 28.9 (1.2) & \cellcolor[HTML]{9AFF99}{28.8 (1.2)} & 35.8 (11.6) & 38.6 (6.7) & \cellcolor[HTML]{9AFF99}{40.2 (6.9)} \\
% Imbalanced Classes         & 11.4 (2.4) & \cellcolor[HTML]{9AFF99}{9.3 (1.7)} & 9.5 (1.7) & 88.2 (2.9) & \cellcolor[HTML]{9AFF99}{90.3 (1.9)} & 90.2 (1.9)\\
% Interaction Effects        & 11.8 (1.4)& \cellcolor[HTML]{9AFF99}{11.4 (1.3)} & 11.5 (1.4) & 30.7 (13.4) & 36.0 (10.6) & \cellcolor[HTML]{9AFF99}{36.7 (11.3)} \\
% Linear Separable           & 12.6 (1.3) & \cellcolor[HTML]{9AFF99}{10.8 (1.0)} & 11.0 (1.2)& 28.4 (25.7) & 50.6 (8.4) & \cellcolor[HTML]{9AFF99}{52.3 (7.6)} \\
% Mixed Types                & 7.6 (1.0) & \cellcolor[HTML]{9AFF99}{7.4 (1.0)} & 7.5 (1.0) & 50.9 (14.8) & \cellcolor[HTML]{9AFF99}{54.4 (11.9)} & 54.0 (12.4) \\
% Multicollinear             & 5.8 (1.7)& \cellcolor[HTML]{9AFF99}{5.2 (1.4)} & 5.5 (1.7) & 52.2 (18.1) & \cellcolor[HTML]{9AFF99}{59.6 (9.9)} & 59.0 (9.5)\\
% Nonlinear Separable        & 36.1 (9.4) & 32.3 (6.9)& \cellcolor[HTML]{9AFF99}{31.7 (7.1)} & 43.6 (40.0) & 59.6 (26.9) & \cellcolor[HTML]{9AFF99}{61.3 (27.7)} \\
% Noisy Data                 & 13.1 (1.5) & \cellcolor[HTML]{9AFF99}{11.7 (1.1)} & 11.7 (1.0)& 28.7 (23.5) & 45.9 (10.3) & \cellcolor[HTML]{9AFF99}{47.6 (9.9)} \\
% Periodic Function          & \cellcolor[HTML]{9AFF99}{5.2 (1.0)} & 5.3 (1.0) & 5.4 (1.0)& 29.3 (12.1) & 31.8 (11.4) & \cellcolor[HTML]{9AFF99}{31.9 (11.4)} \\
% Step Function              & 28.7 (3.9) & 27.4 (1.9)& \cellcolor[HTML]{9AFF99}{27.0 (2.0)} & 61.4 (8.2) & 63.4 (3.6) & \cellcolor[HTML]{9AFF99}{64.2 (3.9)} \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of misclassification error and $F_1$ score among CART, MDFS, and PFS with max depth being 4. Each entry records the average and standard deviation (in parathesis) of the misclassification error or $F_1$ score over 100 experiment replicates. The lowest misclassification error and the highest $F_1$ score in each simulation setting are highlighted in green indicating the best performance.}
% \label{Table simulation results in main text}
% \end{table*}


% \textbf{CART vs PFS:}  In terms of misclassification errors, PFS outperforms CART in six out of the eight cases. For the remaining two cases, PFS and CART perform the same. Under all eight simulation settings, PFS consistently outperforms CART in terms of $F_1$ score.

% \textbf{EFS vs PFS:} In terms of misclassification errors, EFS and PFS perform similarly. EFS outperforms PFS under the periodic data setup, and PFS outperforms EFS under nonlinear separable, imbalanced data, and interaction effects setups. The difference between their performances regarding misclassification errors is very small. On the other hand, PFS consistently beats EFS in $F_1$ scores for all eight simulation setups.

\subsection{Real-world Datasets} \label{sec empirical studies}
We implement CART, MDFS, RF-CART, and RF-MDFS with the
Pima Indians Diabetes \citep{smith1988using}, %donation data from \citet{karlan2007does}, 
%Montesinho Park Forest Fire \citep{forest_fires_162} 
to demonstrate the policy significance of our proposed methods.\footnote{We supplement an additional forest fire empirical study in Appendix \ref{Appendix Empirical Studies} to showcase the wide applicability of our paper.} The empirical findings align with Remark \ref{remark same cost} and \ref{remark more cost}.
%and NLSY97 WIC Participation (from Bureau of Labor Statistics). 
%In these exercises, we compare CART and PFS to demonstrate the strength of the proposed modifying-final-split algorithms.
%The details of the fourth study on WIC, all data source URLs, and the results of MDFS and EFS are provided in Appendix \ref{Appendix Empirical Studies}.
%In the main text, we include a the background information as well as the results for the first two applications and briefly summarize the results for the latter two applications. 
%We include the results of MDFS and EFS in Appendix \ref{Appendix Empirical Studies}. 
%In the rest of the analysis, \textcolor[HTML]{990000}{red} indicates nodes targeted by both CART and PFS and the split is identical, \textcolor{blue}{blue} indicates nodes targeted by both methods but at different splits, and \textcolor[HTML]{EF6C00}{orange} indicates nodes exclusively targeted by one method only.
% \subsubsection{Diabetes}
The Pima Indians Diabetes dataset measures health factors among Pima Indian women with the response variable being a binary indicator of diabetes status: 34.9\% of the sample is diabetic. It contains 768 observations and 8 features: number of pregnancies, glucose level, blood pressure, skin thickness, insulin level, BMI, family diabetes history index, and age. We use these eight features to search for subpopulations whose probability of having diabetes is above 60\% with depth of trees fixed at $m=3$. %We summarize the splitting rules generated by CART and PFS in Figure \ref{figure diabetes trees}.
% \begin{figure*}[ht]
%     \centering
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \fontsize{8pt}{9.6pt}\selectfont
%         \textbf{CART}
%         \begin{Verbatim}[commandchars=\\\{\}]
% if Glucose <= 129.0
%     if Age <= 28.0
%         if BMI <= 30.95
%             value: 0.0258, samples: 155
%         else 
%             value: 0.1889, samples: 127
%     else 
%         if BMI <= 26.72
%             value: 0.0652, samples: 46
%         else 
%             value: 0.4120, samples: 182
% else 
%     if BMI <= 29.7
%         if Glucose <= 144.5
%             value: 0.1515, samples: 33
%         else 
%             \textcolor{blue}{value: 0.5151, samples: 33}
%     else 
%         if Glucose <= 158.05
%             \textcolor[HTML]{990000}{value: 0.6132, samples: 106}
%         else
%             \textcolor[HTML]{990000}{value: 0.8729, samples: 86}
%         \end{Verbatim}
%     \end{minipage}%
%     \hfill
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \fontsize{8pt}{9.6pt}\selectfont
%         \textbf{PFS}
%         \begin{Verbatim}[commandchars=\\\{\}]
% if Glucose <= 129.0
%     if Age <= 28.0
%         if BMI <= 30.95
%             value: 0.0258, samples: 155
%         else 
%             value: 0.1889, samples: 127
%     else 
%         if BMI <= 40.2
%             value: 0.3219, samples: 205
%         else 
%             \textcolor[HTML]{EF6C00}{value: 0.5217, samples: 23}
% else 
%     if BMI <= 29.7
%         if Glucose <= 162.0
%             value: 0.2641, samples: 53
%         else 
%             \textcolor{blue}{value: 0.6153, samples: 13}
%     else 
%         if Glucose <= 158.05
%             \textcolor[HTML]{990000}{value: 0.6132, samples: 106}
%         else 
%             \textcolor[HTML]{990000}{value: 0.8729, samples: 86}
%         \end{Verbatim}
%     \end{minipage}
%     \caption{The colored groups are the targeted subpopulations with a high risk of suffering from diabetes chosen by CART and PFS.}
%     \label{figure diabetes trees}
% \end{figure*}
% Both methods target subpopulations with \textcolor[HTML]{990000}{glucose levels higher than 129 and BMI higher than 29.7}. The two methods disagree on whether to target the subpopulation whose \textcolor{blue}{glucose level is between 144.5 and 162 but with a BMI lower than 29.7}. CART targets this group whereas PFS does not. On the other hand, PFS additionally targets the subgroup whose \textcolor[HTML]{EF6C00}{glucose is lower than 129, but age over 28 and BMI over 40.2}. We argue that the splitting rule generated by PFS is more reasonable as BMI serves as a more stable measure of health outcome compared to glucose level which can fluctuate a lot depending on whether the test was conducted during a fast or after a meal. PFS suggests that using glucose levels to determine whether a group is at high risk of diabetes, the split should be set very high, whereas if a group of individuals has extremely high BMI, they are at high risk of diabetes even if their glucose level is low.
%We compare the groups that the CART and PFS target differently. The \textcolor{blue}{blue} group which CART targets only has a 45\% diabetic rate with sample size 20, on the other hand, the \textcolor[HTML]{EF6C00}{orange} group which PFS targets only has a 52\% diabetic rate with sample size 23. PFS targets a bigger group that is more at risk than CART.
%\subsubsection{Forest Fires}
%The UCI Forest Fire dataset measures the characteristics of different forest fires in Montesinho Park with the response variable being the area burnt by a forest fire. We binarize the response by labeling those observations with a burnt area larger than 5 as 1, and 0 otherwise. 1 indicates a ``big'' forest fire. We search for conditions under which the probability of having a big forest fire is above 60\%. We use the following features in the dataset for this task: X and Y-axis spatial coordinates, month, FFMC, DMC, DC, ISI, temperature, RH (relative humidity), wind, and rain. All the acronyms except for RH are measures that positively correlate with the probability of a big forest fire. With a moderate sample size of 517, we set $m=3$. %We summarize the splitting rules generated by CART and PFS in Appendix \ref{Appendix Empirical Studies}.

\begin{figure*}[ht] 
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \fontsize{8pt}{9.6pt}\selectfont
        \textbf{CART}
            \begin{Verbatim}[commandchars=\\\{\}]
if Glucose <= 127.5
    if Age <= 28.5
        if BMI <= 30.95
            value: 0.013, samples: 151
            value: 0.175, samples: 120
        if BMI <= 26.35
            value: 0.049, samples: 41
            value: 0.399, samples: 173
    if BMI <= 29.95
        if Glucose <= 145.5
            value: 0.146, samples: 41
            value: 0.514, samples: 35
        if Glucose <= 157.5
            \textcolor[HTML]{990000}{value: 0.609, samples: 115} 
            \textcolor[HTML]{990000}{value: 0.870, samples: 92}
        \end{Verbatim}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \fontsize{8pt}{9.6pt}\selectfont
        \textbf{MDFS}
        \begin{Verbatim}[commandchars=\\\{\}]
if Glucose <= 127.5
    if Age <= 28.5
        if BMI <= 22.25
            value: 0.000, samples: 35
            value: 0.097, samples: 236
        if BMI <= 28.25
            value: 0.222, samples: 63
            value: 0.377, samples: 151
    if BMI <= 29.95
        if Glucose <= 166.5
            value: 0.250, samples: 64
            \textcolor[HTML]{990000}{value: 0.667, samples: 12}
        if Glucose <= 129.5
            value: 0.579, samples: 19 
            \textcolor[HTML]{990000}{value: 0.739, samples: 188}
        \end{Verbatim}
    \end{minipage}
\\
\vspace{.5cm}
\begin{minipage}{0.5\textwidth}
        \centering
        \fontsize{8pt}{9.6pt}\selectfont
        \textbf{RF-CART}
            \begin{Verbatim}[commandchars=\\\{\}]
if Glucose <= 127.5
    if Age <= 28.5
        if BMI <= 30.95
            value: 0.028, samples: 151
            value: 0.186, samples: 120
        if BMI <= 26.95
            value: 0.104, samples: 45
            value: 0.396, samples: 169
    if BMI <= 29.95
        if Glucose <= 145.5
            value: 0.193, samples: 41
            value: 0.511, samples: 35
        if Glucose <= 157.5
           value: 0.595, samples: 115 
            \textcolor[HTML]{990000}{value: 0.830, samples: 92}
        \end{Verbatim}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \fontsize{8pt}{9.6pt}\selectfont
        \textbf{RF-MDFS}
        \begin{Verbatim}[commandchars=\\\{\}]
if Glucose <= 127.5
    if Age <= 28.5
        if BMI <= 22.25
            value: 0.010, samples: 35
            value: 0.111, samples: 236
        if BMI <= 28.25
            value: 0.225, samples: 63
            value: 0.380, samples: 151
    if BMI <= 29.95
        if Glucose <= 166.5
            value: 0.284, samples: 64
            \textcolor[HTML]{990000}{value: 0.636, samples: 12}
        if Glucose <= 129.5
           value: 0.573, samples: 19 
            \textcolor[HTML]{990000}{value: 0.713, samples: 188}
        \end{Verbatim}
    \end{minipage}

    \caption{The targeting policies generated by CART, MDFS, RF-CART, RF-MDFS. The \textcolor[HTML]{990000}{red} groups are the targeted subpopulations predicted to a higher than 60\% probability of being diabetic.}
    \label{figure forest fire CART vs MDFS}
\end{figure*}


\subsubsection{CART vs MDFS}
As shown in Figure \ref{figure forest fire CART vs MDFS}, CART and MDFS commonly target those with $Glucose > 129.5$ and $BMI > 29.95$, which consists of 188 observations. The two methods' targeting policies differ in two ways:
\begin{itemize}
    \item CART additionally targets $127.5 < Glucose \leq 129.5$ and $BMI > 29.95$. This subgroup consists of 19 observations.
    \item MDFS additionally targets $Glucose > 166.5$ and $BMI \leq 29.95$. This subgroup consists of 12 observations.
\end{itemize}
The difference between the sizes of these two subgroups is 7, which is small relative to 188, i.e., the size of the subgroup commonly targeted by both policies. Assuming that the sample is representative of the population of all Pima Indian women, then the two sets of policies will incur a similar amount of targeting resources. The additional group targeted by CART has a 57.9\% probability of having diabetes, whereas the additional group targeted by MDFS has a 66.7\% probability of having diabetes. MDFS targets a subpopulation that is more prone to diabetes. This corresponds to Remark \ref{remark same cost}.

\subsubsection{RF-CART vs RF-MDFS}
We first use \textit{RandomForestClassifier} from \textit{sklearn} \citep{scikit-learn} to train a random forest with the entire sample. All user-defined parameters of the random forest follow the default settings. Then, we grow an RF-CART and an RF-MDFS tree with the predicted probability output by the random forest as the response variable.

As shown in Figure \ref{figure forest fire CART vs MDFS}, both RF-CART and RF-MDFS commonly target $Glucose > 157.5$ and $BMI > 29.95$, a subgroup consisting of 88 observations. RF-MDFS additionally targets two subgroups:
\begin{itemize}
    \item $Glucose > 166.5$ and $BMI \leq 29.95$ consisted of 12 observations and
    \item $129.5 < Glucose \leq 157.5$ and $BMI > 29.95$ consisted of 96 observations.
\end{itemize}
RF-MDFS targets a much greater number of observations than RF-CART. If the sample is representative of the population, then RF-MDFS would use much more targeting resources than RF-CART. Nevertheless, RF-MDFS is useful in uncovering groups with higher than 60\% probability of having diabetes that RF-CART is unable to find. For example, the first group that RF-MDFS additionally targets has a 63.6\% probability of having diabetes. This aligns with Remark \ref{remark more cost}.
%The two methods commonly target the \textcolor[HTML]{990000}{red} groups which contain two subpopulations, (i) \textcolor[HTML]{990000}{temperature lower than 5.15}, (ii) \textcolor[HTML]{990000}{temperature higher than 8.2, DMC larger than 117.9 and X-coordinate value less than 2.5}. Subpopulation (i) suggests human activity over the winter (lowest temperature range in the sample) may cause big forest fires. Subpopulation (ii) captures a type of weather-spatial dependence. The \textcolor[HTML]{EF6C00}{orange} group that PFS additionally targets suggests a different kind of weather-spatial dependence (\textcolor[HTML]{EF6C00}{temperature is higher than 8.2, DMC lower than 117.9, and X larger than 7.5}).

%It may seem illogical for PFS to target the \textcolor[HTML]{EF6C00}{orange} group, but not the alternative group whose temperature is higher than 8.2, DMC \textbf{larger} than 117.9, and X larger than 7.5. After all, the alternative is identical to the \textcolor[HTML]{EF6C00}{orange} group except for a higher DMC. Logically, the alternative group should have an even higher probability of forest fire. We separately estimate the sample average probability of big forest fires for both groups, the \textcolor[HTML]{EF6C00}{orange} group has a probability of 39.47\% whereas the alternative group's probability is exactly 1/3. The alternative group has a lower probability. This contradiction to our logic could be a sampling noise. However, given our threshold is set to be 1/3, the union of the two groups which is when the temperature is higher than 8.2 and X larger than 7.5 should be targeted as a whole. We argue that PFS outperforms CART as it partially identifies the at-risk subpopulation while CART completely ignores it.
% \begin{figure*}[ht]
        


%     \caption{The \textcolor[HTML]{990000}{red} groups are the targeted subpopulations with a higher than 60\% probability of having a big forest fire.}
%     \label{figure forest fires RF-CART vs RF-MDFS}
% \end{figure*}

% \subsubsection{Donation}
% \citet{karlan2007does} collects information on which type of past donors is more likely to donate to a fundraising event. The response variable is whether an individual donates for the event and features include the historical highest amount of donation, frequency of past donations, number of months since the last donation, etc. The overall donation rate is 2.09\%. We aim to find subpopulations that donate with a probability higher than 5\% with a depth-5 tree. The total sample size is 46513. 
% % In this experiment, we test the out-of-sample performance of CART and PFS. 
% For each iteration, we randomly draw 40\% of the sample (18605 observations) for constructing the tree and estimating group mean probabilities of donation.  We then use the remaining 60\% of the data to estimate the group means and use them as ground truth to evaluate trees' performance. The procedure is almost identical to the cross-validation with the honest approach described in \textbf{choice of $\lambda$ for PFS} in Section \ref{sec simulation}, but we do so only for those final splits that CART and PFS disagree. We summarize the performance of CART and PFS for 100 iterations as follows.
% \begin{table}[ht]
%     \centering
%     \begin{tabular}{c c c c}
%     \toprule
%          \multicolumn{2}{c}{Misclassification $(\%)$} & \multicolumn{2}{c}{F1 score $(\%)$} \\
%          \midrule
%          CART & PFS & CART & PFS \\
%          %3.38(2.51) & 2.79(1.92) & 80.9(14.89) & 83.54(13.72)\\
%          3.38(2.51) & 2.88(2.11) & 80.90(14.89) & 83.82(12.92)\\
%          \bottomrule
%     \end{tabular}
%     % \caption{Caption}
%     % \label{tab:my_label}
% \end{table}





\section{Conclusion} \label{sec conlusion}
% This paper points out that the classic CART does not minimize misclassification risk. We formalize this result and show that splitting at $s^{CART}$ is strictly dominated by some other splitting rules. In particular, splitting at $s^*$ strictly dominates using CART. However, the estimation method EFS which is based on such an identification result does not perform well. We propose alternative identification results with novel estimation method PFS and MDFS. The new methods consistently outperforms CART and EFS in our simulation and empirical settings. 
% This paper points out that the classic CART does not directly minimize misclassification risk in each split. For any fixed feature, we show that splitting at the CART-determined point is strictly dominated by some other splitting rules. By penalizing how close the node means are to the threshold, we move the split point in a direction that guarantees lower misclassification risk under very mild assumptions. However, the estimation method EFS which is based on such an identification result does not perform well. We propose alternative identification results with novel estimation method PFS and MDFS. The new methods consistently outperform CART and EFS in our simulation and empirical settings. 
Our paper points out that the classic CART does not directly minimize LPC misclassification risk in each split. %For any fixed feature, by imposing continuity assumption on $\eta(x)$, we show that there exists an interval of split points starting from $s^{CART}$ to a point $s^*$, that all strictly dominate the split point $s^{CART}$. 
Based on different assumptions, we propose two alternative methods PFS and MDFS, both of which reduces LPC misclassification risk. The applicability of the theoretical results also extend to KD-CART. Our proposed methods predominantly outperform their counterparts in our simulation and provide more policy insights in empirical applications.

There are multiple interesting future work directions. First, as mentioned earlier, the theoretical guarantee of risk reduction for PFS and MDFS can be further investigated under more general conditions.
Second, in the current version of this paper, KD-MDFS extends the applicability of our theories to $\eta(x)$ being the response variable which takes on continuous value, extending it further to more general continuous outcome problems can be interesting.
Third, the idea of latent probability classification can be adapted to other cost-sensitive machine learning algorithms \citep{babii2020binary} and provide more insights for policy making.

%Extending the LPC framework to policy learning/treatment assignment optimization/individualized treatment rule is a natural next step for binary outcome events, where the difference in potential outcomes is also binary. 

%(ii) CART suffers from an instability problem and researchers use the random forest to address this problem. However, policymakers may prefer a simple policy output, which could be achieved by synthesizing a single representative tree from the random forest results. Combining our methods and the representative tree literature may further improve LPC accuracy while maintaining policy interpretability. 
\clearpage

\section*{Impact Statement}
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.

% \section{Proof with monotone \texorpdfstring{$\eta$}{eta} and unimodal \texorpdfstring{$\mathcal{G}$}{g} }
% In this section, we provide a stronger result on reducing risk by penalizing the final split under two extra assumptions:

% \textbf{d}. $\eta(x)$ being monotone with respect to $x$. 

% \textbf{e}. $\mathcal{G}$ is monotone increasing when $s > s^{CART}$ and monotone decreasing when $s<s^{CART}$.
% % assuming there are assumptions a,b,c ahead

% By our assumption that $f:\mathcal{X}\xrightarrow{} \mathbb{R}^{+}$ and continuous, $F_{x|t}$ is continuous and strictly monotone increasing. We have $F(X) \sim \text{U}[0,1]$ and $\eta(x)$ remains monotone with respect to $F(x)$ in that $\frac{\partial \eta(x)}{\partial F(x)} = \frac{1}{f(x)}\frac{\partial \eta(x)}{\partial x}$. 
% Without loss of generality, we assume
% \begin{itemize}
%     \item $\eta(x)$ is monotone increasing. 
%     \item $X|t \sim \text{U}[0,1]$ otherwise we could use $F(X)$ to replace $X|t$ as the feature to split.
% \end{itemize}

% The lemma below characterizes $\mathcal{G}$'s unimodality under $\eta$'s monotonicity assumption.

% \begin{lemma} \label{lemma:unimodal G}
%     Given the problem setup, for any node $t$, $\mathcal{G}$ is unimodal. The theoretical optimal split point $s$ on dimension $j\in[p]$ is uniquely determined iff \eqref{optimal_split}: $2\eta(s) - \mu_L - \mu_R = 0$ holds.
% \end{lemma}
% \begin{proof}
%     This statement is a special case of Lemma~\ref{lemma mid point}. In Lemma~\ref{lemma mid point}, we show that $2\eta(s) - \mu_L - \mu_R = 0$ is a necessary condition for $s$ to be an optimal split point. Next, we prove that $\mathcal{G}$ is unimodal by showing that $\mathcal{G}$ is monotone increasing after $s$ and decreasing before $s$. This directly leads to the sufficiency of optimal split satisfying \eqref{optimal_split}. Still, we denote the solution to \eqref{optimal_split} as $s^{CART}$.

%     By uniform assumption, introducing $H(s) = \int_{0}^s \eta(x)dx$, we can simplify \eqref{El} and \eqref{Er} by $\mu_L = \frac{H(s)}{s}$ and $\mu_R = \frac{H(1)-H(s)}{1-s}$. By \eqref{der_G}, we have 
%     \begin{align*}
%         \frac{\partial \mathcal{G}(s)}{\partial s} =&\ (2\eta(s) - \mu_L - \mu_R)(\mu_R - \mu_L)\\
%         =&\ \frac{\mu_R - \mu_L}{s(1-s)}\underbrace{(2\eta(s)s(1-s) - H(s)(1-2s) - H(1)s)}_{M},
%     \end{align*}
%     where the first term remains positive when $s\in(0,1)$. Next, we want to show the second term less than 0 when $s<s^{CART}$ and greater than 0 when $s>s^{CART}$.
%     \begin{align*}
%         \frac{\partial M}{\partial s} =&\ \eta(s)(1-2s) + 2\eta^{'}(s,t)s(1-s) - H(1) + 2H(s)\\ \ge&\ \eta(s)(1-2s) - H(1) + 2H(s)\\
%         =&\ \left(\eta(s)(1-s) - \int_{s}^1 \eta(x)dx\right) - \left(\eta(s)s - \int_{0}^s \eta(x)dx\right)
%     \end{align*}
    
% \end{proof}
% \begin{theorem} \label{lemma:unimodal W}
%     If choosing $W$ to be quadratic and monotone decreasing, and taking $\lambda = 2/W^{''}$, the split rule $s$ determined by $G^{*}$ is the universal best classifier that leads to risk 0.
% \end{theorem}

% \begin{corollary}
%     $\lambda W(|E_t-c|) = (|E_t-c|-1)^2$
% \end{corollary}

% Both $G$ and $G^*$ are unimodal and have unique local/global minimums. 

% the sum of two unimodal (G and weight) functions where their minimums are located at the left and right-hand side of $s^*$ respectively. 

% when weight function is quadratic, the optimal lambda is $\frac{2}{W^{''}}$
% \begin{figure}[H]
%     \centering
%     % Adjust the widths to fit your specific needs
%     \begin{minipage}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{example_pfs2.pdf}
        
%     \end{minipage}
%     \begin{minipage}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{example_pfs.pdf}
%         \label{fig:pfs_example2}
%     \end{minipage}
%     \caption{Left: $\eta(x) = 1-\frac{2}{e^{4F(x)}+1}, c = 0.8$. Right: $\eta(x) = F(x)^2, c = 0.9$.}
% \end{figure}
\bibliography{reference}
\bibliographystyle{icml2025}

% \section*{Checklist}


% % %%% BEGIN INSTRUCTIONS %%%
% The checklist follows the references. For each question, choose your answer from the three possible options: Yes, No, Not Applicable.  You are encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description (1-2 sentences). 
% Please do not modify the questions.  Note that the Checklist section does not count towards the page limit. Not including the checklist in the first submission won't result in desk rejection, although in such case we will ask you to upload it during the author response period and include it in camera ready (if accepted).

% \textbf{In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.}
% % %%% END INSTRUCTIONS %%%


%  \begin{enumerate}


%  \item For all models and algorithms presented, check if you include:
%  \begin{enumerate}
%    \item A clear description of the mathematical setting, assumptions, algorithm, and/or model. [Yes]
%    \item An analysis of the properties and complexity (time, space, sample size) of any algorithm. [Yes]
%    \item (Optional) Anonymized source code, with specification of all dependencies, including external libraries. [Yes]
%  \end{enumerate}
%  Complexity analysis is included in Appendix \ref{appendix Synthetic data simulation studies}.


%  \item For any theoretical claim, check if you include:
%  \begin{enumerate}
%    \item Statements of the full set of assumptions of all theoretical results. [Yes]
%    \item Complete proofs of all theoretical results. [Yes]
%    \item Clear explanations of any assumptions. [Yes]     
%  \end{enumerate}
% The proofs are provided in Appendix \ref{appendix proofs}.


%  \item For all figures and tables that present empirical results, check if you include:
%  \begin{enumerate}
%    \item The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). [Yes]
%    \item All the training details (e.g., data splits, hyperparameters, how they were chosen). [Yes]
%          \item A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [Yes]
%          \item A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [Not Applicable]
%  \end{enumerate}

%  \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:
%  \begin{enumerate}
%    \item Citations of the creator If your work uses existing assets. [Yes]
%    \item The license information of the assets, if applicable. [Not Applicable]
%    \item New assets either in the supplemental material or as a URL, if applicable. [Yes]
%    \item Information about consent from data providers/curators. [Not Applicable]
%    \item Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [Not Applicable]
%  \end{enumerate}
% All synthetic experimental datasets are reproducible. All empirical datasets are open-access and can be freely downloaded from the URLs provided in Appendix \ref{Appendix Empirical Studies}.

%  \item If you used crowdsourcing or conducted research with human subjects, check if you include:
%  \begin{enumerate}
%    \item The full text of instructions given to participants and screenshots. [Not Applicable]
%    \item Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [Not Applicable]
%    \item The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Not Applicable]
%  \end{enumerate}

%  \end{enumerate}


\onecolumn
\appendix

\section{Further literature review} 
\subsection{Extensive use of CART for policy targeting}\label{appendix Extensive use of CART for policy targeting}
\textbf{Traffic safety:} \citet{da2017identification} identifies potential sites of serious accidents in Brazil using different decision tree algorithms. Policymakers may choose to take more safety precautions at dangerous traffic spots whose probability of having a fatal accident is above a threshold. Other research that uses decision trees to identify dangerous traffic situations includes \citet{clarke1998machine,kashani2011analysis,obereigner2021methods}.

\textbf{Fraud detection:} \citet{sahin2013cost, save2017novel,lakshmi2018machine}  find conditions under which credit card fraudulent usage is likely to happen. Credit card companies can inform card owners when the probability of fraudulent usage is above a pre-specified threshold.




%\textbf{College admission and job application:} \citet{mashat2012decision} builds a decision tree for King Abdulaziz University to filter out ``low-level candidates'' by rejecting subpopulations whose probability of admission is below a threshold. The same technique can be applied to highly competitive job application processes. For example, many UK law firms receive thousands of applications per year \citep{chambersstudent2023trainingcontract}. With an extremely low acceptance rate of below 3\% \citep{nextcitylawyer2023trainingcontract}, a decision tree algorithm can be useful for screening out ``low-level candidates'' and making the review process more efficient.


\textbf{Mortgage lending:} 
\citet{feldman2005mortgage, cciugcsar2019comparison, madaan2021loan} predict different subpopulations' mortgage (and other types of loan) default rates using a decision tree. Mortgage loan lenders can use the result to determine whether to deny a loan request. Another stream of literature on mortgage lending using decision trees focuses on racial discrimination \citep{varian2014big, lee2021algorithmic, zou2023ai}.  Policymakers may want to intervene in situations where with high enough probability race seems to play a role in determining whether mortgage lending is denied and the probability of denial is high.




\textbf{Health intervention:} \citet{mann2008classification, shouman2011using, crowe2017weight, speiser2018predicting, toth2021decision, mahendran2022quantitative} classify diabetes (or other diseases) using relevant risk factors. Such classification result leads to different health interventions. Doctors may recommend various treatments based on whether the subpopulation a patient belongs to is more likely to be classified as type I or type II diabetes. 



\textbf{Water management:} One of the objectives of \citet{herman2018policy} is to manage flood control and output a threshold-based water resources management policy. The authors output a set of conditions that define the subpopulation whose probability of flooding is high enough to justify policy intervention. Many other studies also use decision trees to classify whether the water quality is satisfactory, resulting in important implications to water management policies \citep{waheed2006measuring, saghebian2014ground,hannan2021classification}.



%\textbf{Legal settlements: } Not leveraging on any empirical data, \citet{victor2014decision} advocates for using a decision tree to reduce litigation uncertainty. \citet{arditi2005predicting} uses data to build boosted decision trees for the same purpose. Law firms can use decision trees to determine highly uncertain litigation cases and bring them to trial while settling other low-uncertainty cases before trial. This strategy results in a more efficient legal resource allocation.

%\subsection{Further literature on CART learning theory literature} \label{appendix Further literature on CART learning theory literature}


\subsection{Related Literature on CART and Random Forest} \label{appendix further lit review}
On top of its empirical relevance, this paper adds to the growing theoretical literature on CART and random forests. We briefly outline several areas where our research contributes or applies. \footnote{There is a vast literature on decision trees, we do not claim this section to be all-encompassing. Readers should refer to \citet{kotsiantis2013decision,costa2023recent} for more comprehensive reviews.} %We include further comparisons of our paper to other theoretical works in Appendix \ref{appendix Further literature on CART learning theory literature}.


\textbf{Criterion function of CART: }
\citet{quinlan2014c4,breiman2017classification} are two seminal works that describe CART in detail. Following the two seminal papers, most works in the field minimize some impurity measures: variance for continuous responses and Gini impurity score (or entropy) for discrete responses. 
There are a few attempts to modify the criterion function of CART. (1) \citet{athey2016recursive} removes the bias of CART by using two independent samples to determine data partition and estimate heterogeneous group means separately. The authors name the two samples training and estimation samples. Their criterion function depends on both, which differs from the classic criterion function that treats the entire sample as the training sample as defined by \citet{athey2016recursive}. (2) \citet{hwang2020new} aims to improve the interpretability of CART. The authors define interpretability as the purity and size of the nodes: pure and big nodes lead to a more interpretable splitting rule. They modify the criterion function such that their CART tends to produce a small number of leaf nodes that are large and homogeneous, enhancing the interpretability of the splitting rule as defined in the paper. (3) \citet{zheng2023consistency} shows that one of the key challenges in proving the consistency of the heuristic tree learning algorithm is the potentially zero ``worst-case purity gain''. The authors switch the impurity score measure of CART to an influence measure and prove that the new criterion function leads to better consistency rate results.

We contribute to the literature that modifies the criterion function of CART. The major distinction between our paper and the others is that we focus on a novel classification problem of a latent probability for a binary outcome event. This leads to a different set of theoretical results and modifications of CART. Unlike past works that focus on the bias \citep{athey2016recursive}, interpretability \citep{hwang2020new}, or consistency \citep{zheng2023consistency} properties, we define a novel risk measure and prove our method's theoretical advantage over CART regarding this new risk measure. 

\textbf{Random Forest asymptotic: } Lots of the theoretical literature on CART focuses on the asymptotic property of random forest \citep{biau2008consistency,scornet2015consistency, wager2018estimation, gao2020towards, klusowski2021universal}\footnote{We do not claim the list to be exhaustive, many important works such as \citep{breiman2001random, biau2012analysis, meinshausen2006quantile, scornet2015consistency, wager2015adaptive, zhu2015reinforcement, duan2011classification, lin2006random, mentch2016quantifying, sexton2009standard, efron2014estimation}
are not included in the list.}. This literature deals with feature selection and asymptotic behavior of variants of random forests. We have a very different goal of classifying latent probability. First, we take the features selected by CART as given and hence, we are not concerned with feature selection. Second, we prove our approach leads to a smaller risk with respect to the population distribution compared to CART and does not deal with the asymptotic behavior of an estimator. Consequently, we impose a set of different assumptions.

\textbf{Assumptions on $f$ and $\eta$: } We compare the assumptions made by our paper with those made by random forest learning theory literature.
We require $f$, the distribution of covariates $X$, to be continuous and strictly positive over a bounded domain. It is also much weaker than the production distribution (independence across feature) assumption made by \citet{takimoto2003top}, \citet{kalai2008decision}, \citet{brutzkus2020id3} and \citet{zheng2023consistency}. For a stronger theoretical result, we impose a uniform distribution assumption which is also made by \citet{scornet2015consistency}, \citet{wager2018estimation} and \citet{blanc2022properly}. We argue that this uniform distribution assumption is not strong if the splitting variable is continuous as we can convert the variable to its uniformly distributed quantile. Our key assumption on $\eta(x) = P(Y=1|X=x)$ is that $\eta$ is continuous with respect to $x$ for any given node $t$. For Theorem~\ref{theorem tendency}, we do not require monotonicity of $f$ as in \citet{blanc2020provable} or an additive model of $f$ as in \citet{guelman2012gradient}, \citet{scornet2015consistency}, and \citet{klusowski2021universal}. Furthermore, with the monotonicity assumption, we can attain stronger theoretical results.

\textbf{Continuous response and treatment effect:} 
%Our paper focuses on a classification tree, not a regression tree as we assume that the policymaker is interested in classifying the latent probability of a \textbf{binary} event. However, 
Researchers are often interested in continuous outcomes such as sales volume during promotions, milk yield of cows, and patient's blood glucose level
\citep{ali2009sku, piwczynski2020forecasting, alexiou2021approach}. Extending our research to continuous is not trivial as the impurity measure has no closed form like a Bernoulli variable. A special case of continuous outcome is the treatment effect. Policymakers may want to target subpopulations with treatment effects above a certain threshold. \citet{athey2016recursive,kallus2017recursive,wager2018estimation,athey2019generalized} study treatment effect with regression tree. Integrating our research into their causal inference framework can be an interesting research direction for social scientists, bio-statisticians, and researchers working with causal models. 





\textbf{Representative tree: } It is well-known that CART suffers from instability problem \citep{li2002instability}. As a result, researchers often use random forest or other ensemble methods for prediction. However, this paper focuses on the decision tree for its stellar interpretability. To combine the stability of random forest and interpretability of CART, one should look into representative tree \citep{sagi2020explainable, sagi2021approximating, gulowaty2021extracting}. As suggested by its name, the representative tree literature aims to represent a random forest with a single decision tree. An interesting future research direction is to use our method to grow a forest and synthesize a representative tree to output a policy-targeting rule.

\textbf{Policy learning: } 
(i) The policy learning literature operates under a potential outcome framework and is part of the causal inference literature. LPC problem does not operate under the potential outcome framework, though extending our work to causal inference is undoubtedly interesting for future research as discussed in Appendix \ref{appendix further lit review}, the ``continuous response and treatment effect'' paragraph. 

(ii) The policy learning literature studies treatment assignment optimization problem. 
While LPC \textit{can} be used for informing treatment assignments, 
we \textit{do not claim} that our methods optimize the total treatment effects. 
LPC can also serve other purposes. For example, policymakers can analyze why the identified targeting subpopulations have a higher-than-threshold probability of the binary event and make population-wide policy adjustments or design new policies without prior treatment in mind. %Policy learning cannot offer much insight into these issues since it studies and optimizes the allocation of the prior treatment.

(iii) Our methods do not need a randomized control trial or a quasi-experiment which is a key assumption for the policy learning literature. 
Experimental data is not always available. For instance, in the case of temporary tax credit programs, it is nearly impossible to conduct an experiment since taxes are collected annually. In such cases, classic policy learning may be impractical, but LPC can help refine policy design by identifying and targeting the subgroups that are more likely financially constrained.
% Experimental data is not always available. For example, in the context of temporary tax credit programs, it is virtually impossible to run an experiment since taxes are collected annually. In this context, policy learning is infeasible but LPC can serve to improve the policy design in terms of targeting subgroups that are more likely financially constrained. 

(iv) The criterion function for the policy learning literature is a general welfare criterion defined with respect to treatment effect. We assume a very straightforward thresholding rule as illustrated in the introduction, construct a misclassification risk based on such a rule, and use this risk as the criterion function. The misclassification risk is not a function of the treatment effect since there is no treatment in our problem setup.

(v) The theoretical results of the policy learning literature are usually bounds on regret (welfare loss). Since we do not work with welfare function, our theoretical results are with respect to misclassification risk: lower risk under continuity (Theorem \ref{theorem penalized loss works}) and optimality under continuity plus monotonicity (Corollary \ref{theorem monotonic eta uniform X}). 

(vi) Our method is computationally much cheaper than those mixed integer programming methods proposed in the policy learning literature. Many empirical studies conducted in the policy learning literature use one or two covariates to decide treatment assignment rules \citep{kitagawa2018should,mbakop2021model,athey2021policy}, our methods can easily accommodate a large number of features as their complexity is the same as constructing CART.

\section{Mathematical Appendix} \label{appendix proofs}
\begin{lemma}\label{lemma FOC}
    Given the problem setup, for any node $t$, assume that $\eta(X)$ is continuous and not a constant,  $\frac{\partial \mathcal{G}^{CART}(s)}{\partial s} = 0$ is a necessary condition for $s$ to be an optimal split point.
\end{lemma}

\begin{proof}
    We first rule out the possibility that the optimal splitting happens at the boundary point. 

    Consider $s = 0$, then we write the impurity score as 
    \begin{align*}
        \mathcal{G}^{CART}(0) =&\ (\mu_L(0) - \mu_L^2(0))F(0) + (\mu_R(0) - \mu_R^2(0))(1-F(0)) \\
        =&\ \mu_R(0) - \mu_R^2(0) = \Bar{\eta} - \Bar{\eta}^2
    \end{align*}
    where $\Bar{\eta}$ denotes the mean probability of $Y=1$ for the entire node $t$.

    Since $\eta(X)$ is not a constant, we can find $s = s'$ such that $\mu_L(s') \neq \mu_R(s')$ and in general $\Bar{\eta} = \mu_L(s')F(s') + \mu_R(s') (1 - F(s'))$, where $0<F(s')<1$. The impurity score for $s = s'$ is 
    \begin{align*}
        \mathcal{G}^{CART}(s') = (\mu_L(s') - \mu_L^2(s'))F(s') + (\mu_R(s') - \mu_R^2(s'))(1 - F(s'))
    \end{align*}

    Using the equality $\Bar{\eta} = \mu_L(s')F(s') + \mu_R(s') (1 - F(s'))$, we can rewrite $\mathcal{G}(0)$ and show that it is strictly greater than $\mathcal{G}(s')$.

    \begin{align*}
        \mathcal{G}^{CART}(0) =&\ \mu_L(s')F(s') + \mu_R(s') (1 - F(s')) - (\mu_L(s')F(s') + \mu_R(s') (1 - F(s')))^2 \\
        =&\ \mu_L(s')F(s') + \mu_R(s') (1 - F(s')) - (\mu_L(s')F(s'))^2 - ((\mu_R(s') (1 - F(s')))^2\\
        &\ \quad - 2 (\mu_L(s')F(s')) (\mu_R(s') (1 - F(s')))\\
        =&\ \mu_L(s')F(s') + \mu_R(s') (1 - F(s')) - (\mu_L(s')F(s'))^2 - ((\mu_R(s') (1 - F(s')))^2 \\
        &\ \quad - \mu_L^2(s') F(s')(1 - F(s')) - \mu_R^2(s') F(s')(1 - F(s')) \\
        &\ \quad + \mu_L^2(s') F(s')(1 - F(s')) + \mu_R^2(s') F(s')(1 - F(s')) \\
        &\ \quad - 2 (\mu_L(s')F(s')) (\mu_R(s') (1 - F(s')))\\
        =&\ \mathcal{G}^{CART}(s') + \mu_L^2(s') F(s')(1 - F(s')) + \mu_R^2(s') F(s')(1 - F(s')) \\
        &\ \quad - 2 (\mu_L(s')F(s')) (\mu_R(s') (1 - F(s')))\\
        =&\ \mathcal{G}^{CART}(s') + (\mu_L(s') - \mu_R(s'))^2 F(s'))(1 - F(s')) > \mathcal{G}(s')
    \end{align*}
The inequality means that $s = 0$ can never be the optimal split. The proof for the case of $s = 1$ is similar. Hence, we show that the optimal split is not the boundary point.

Given that $\mathcal{G}^{CART}(s)$ is differentiable, its domain is closed and compact, and the boundary points are not the optimal splits. The first-order condition must be satisfied at all interior local optima (including the global minimum whose argument is the optimal splitting).
    
\end{proof}

\subsection{Proof for Lemma~\ref{lemma mid point}}
\begin{proof}
    For a fixed $s\in[0,1]$, the impurity score after the split is given as:
    \begin{align}
        \mathcal{G}^{CART}(s)
        &= (\mu_L - \mu_L^2)F(s) + (\mu_R - \mu_R^2)(1-F(s)) \label{impurity sum} \\
        &= E_t - \mu_L^2F(s) - \mu_R^2(1-F(s)),\nonumber
        % &= E_t - H_L^2(s)/F(s) - H_R^2(s)/(1-F(s),
    \end{align}
    where $E_t = \int_0^1 \eta(x)f(x)dx$ is free of $s$. Moreover, 
    \begin{align}
    \frac{\partial \mu_L}{\partial s} =&\ \frac{\partial \int_0^s \eta(x)f(x)dx/F(s)}{\partial s} = \frac{f(s)}{F(s)}\left( \eta(s) -\mu_L \right) \label{dElds}\\
    \frac{\partial \mu_R}{\partial s} =&\ \frac{\partial \int_s^1 \eta(x)f(x)dx/(1-F(s))}{\partial s} = \frac{f(s)}{1-F(s)}\left(-\eta(s) + \mu_R\right)\label{dErds},
    \end{align}
    %where $H_L(s) = \int_{X\in[0,s], \mathbf{X}\in t} \eta(x)f(x)dx$ and $H_R(s) = \int_{X\in[s,1], \mathbf{X}\in t} \eta(x)f(x)dx$.
    Using (\ref{dElds}) and (\ref{dErds}), we simplify the first-order derivative 
    \begin{align}
        % \frac{\partial \mathcal{G}(s)}{\partial s} &= -2\mu_LF(s)\frac{\partial \mu_L}{\partial s} - 2\mu_R(1-F(s))\frac{\partial \mu_R}{\partial s} - (\mu_L^2 - \mu_R^2)f(s)\\
        % &= -2\mu_LF(s)\frac{\partial \mu_L}{\partial s} - 2(E_t - \mu_LF(s))\frac{\partial \mu_R}{\partial s}- (\mu_L^2 - \mu_R^2)f(s)
        \frac{\partial \mathcal{G}^{CART}(s)}{\partial s} =&\ -2\mu_LF(s)\frac{\partial \mu_L}{\partial s} -\mu_L^2f(s) -2\mu_R(1-F(s))\frac{\partial \mu_R}{\partial s} + \mu_R^2f(s) \nonumber\\
        =&\ f(s)\left( -2\mu_L\left(\eta(s) - \mu_L\right)-\mu_L^2 - 2\mu_R\left(-\eta(s) +\mu_R\right) + \mu_R^2 \right)\nonumber\\
        % \propto&\ -\frac{2H_L(s)\eta(s)}{F(s)} + \frac{H_L(s)^2}{F(s)^2}                                      +\frac{2H_R(s)\eta(s)}{1-F(s)} - \frac{H_R(s)^2}{(1-F(s))^2}\\
        =&\ f(s) (-2\mu_L\eta(s) + E^2_{t_L} + 2\mu_R\eta(s) - E^2_{t_R})\nonumber\\
        =&\ f(s) (2\eta(s) - \mu_L - \mu_R)(\mu_R - \mu_L)\label{der_G}\\
        \propto&\ (2\eta(s) - \mu_L - \mu_R)(\mu_R - \mu_L)\nonumber. 
    \end{align}
%Since both $\eta$ and $f$ are continuous, 
By Lemma \ref{lemma FOC},  $\frac{\partial \mathcal{G}^{CART}(s)}{\partial s} = 0$ is a necessary condition for $s$ to be an optimal split point. 

Given that $f$ is strictly positive, we have that $\frac{\partial \mathcal{G}^{CART}(s)}{\partial s} = 0$ iff $(2\eta(s) - \mu_L - \mu_R)(\mu_R - \mu_L) = 0$. The next step is to rule out the possibility that $\mu_R - \mu_L = 0$ outputs a local maximum. 
\begin{align*}
    \frac{\partial^2 \mathcal{G}^{CART}(s)}{\partial s^2} =&\ \underbrace{(2\eta(s) - \mu_L - \mu_R)(\mu_R - \mu_L)}_{=0 \text{ by the first-order condition}}\frac{\partial f(s)}{\partial s} + \frac{\partial (2\eta(s) - \mu_L - \mu_R)(\mu_R - \mu_L)}{\partial s} f(s) \\
    =&\ \frac{\partial (2\eta(s) - \mu_L - \mu_R)(\mu_R - \mu_L)}{\partial s} f(s) \\
    \propto&\ \frac{\partial (2\eta(s) - \mu_L - \mu_R)(\mu_R - \mu_L)}{\partial s}
\end{align*}

Consider the second derivative of $\mathcal{G}(s)$ with respect to $s$ with $\mu_L = \mu_R$, by \eqref{dElds} and \eqref{dErds}:

\begin{align*}
    \frac{\partial^2 \mathcal{G}^{CART}(s)}{\partial s^2} \propto&\ \frac{\partial (2\eta(s) - \mu_L - \mu_R)}{\partial s} \underbrace{(\mu_R - \mu_L)}_{=0} + (2\eta(s) - \mu_L - \mu_R)\left( \frac{\partial \mu_R}{\partial s} - \frac{\partial \mu_L}{\partial s} \right) \\ 
    % =&\ (2\eta(s) - \mu_L - \mu_R)\left( \frac{\partial \mu_R}{\partial s} - \frac{\partial \mu_L}{\partial s} \right) \\
    \propto&\ (2\eta(s) - \mu_L - \mu_R)\left( - \frac{\eta(s)}{1-F(s)} + \frac{\mu_R}{(1-F(s))} -\frac{\eta(s)}{F(s)} + \frac{\mu_L}{F(s)} \right)\\
    =&\ \frac{2\eta(s) - \mu_L - \mu_R}{F(s)(1-F(s))}\left( - \eta(s) + 
    \mu_L (1-F(s)) + \mu_R F(s)\right)
\end{align*}

If $\mu_L = \mu_R$, 
\begin{align*}
    \frac{\partial^2 \mathcal{G}^{CART}(s)}{\partial s^2} \propto&\ (2\eta(s) - \mu_L - \mu_R)(-\eta(s) + \mu_L) = -2(\eta(s) - \mu_L)^2.
\end{align*}
For $s$ s.t. $2\eta(s, t) - \mu_L - \mu_R \neq 0$ and $\mu_L = \mu_R$, $\frac{\partial \mathcal{G}^{CART}(s)}{\partial s} = 0$ and $\frac{\partial^2 \mathcal{G}^{CART}(s)}{\partial s^2} < 0$: $\mathcal{G}^{CART}(s)$ reaches its local maximum, which cannot be a global minimum. Such $s$ is not the optimal split. 

Global minimum must have $(2\eta(s) - \mu_L - \mu_R)(\mu_R - \mu_L) = 0$ and $\mu_R - \mu_L \neq 0$. Therefore, $2\eta(s, t) - \mu_L - \mu_R = 0$ holds for optimal split $s$ in any node $t$ and dimension $p$.
\end{proof}

\subsection{Proof for Lemma~\ref{lemma dominate and small risk}}
\begin{proof}
Using Definition \ref{Definition inadmissible} and contrapositive,
\begin{itemize}
    \item when $\eta(x) > c$, $\mu_{t(x)}(s') \leq c \implies \mu_{t(x)}(s) \leq c$ and
    \item when $\eta(x) \leq c$, $\mu_{t(x)}(s') > c \implies \mu_{t(x)}(s) > c$.
\end{itemize}
Therefore, $\forall\ x \in [0,1]$, 
\begin{align*}
    \mathbbm{1}\{ \eta(x) > c \} \mathbbm{1}\{\mu_{t(x)}(s') \leq c\} 
    \leq 
    \mathbbm{1}\{ \eta(x) > c \} \mathbbm{1}\{\mu_{t(x)}(s) \leq c\}
    \\
    \mathbbm{1}\{ \eta(x) \leq c \} \mathbbm{1}\{\mu_{t(x)}(s') > c\} 
    \leq
    \mathbbm{1}\{ \eta(x) \leq c \} \mathbbm{1}\{\mu_{t(x)}(s) > c\}   
\end{align*}
Also, there exists a set $\mathcal{A} \subseteq [0,1]$ with nonzero measure such that, $\forall\ x \in \mathcal{A}$, either (or both) of the following conditions is true
\begin{align*}
    \mathbbm{1}\{ \eta(x) > c \} \mathbbm{1}\{\mu_{t(x)}(s') \leq c\} 
    <
    \mathbbm{1}\{ \eta(x) > c \} \mathbbm{1}\{\mu_{t(x)}(s) \leq c\}
    \\
    \mathbbm{1}\{ \eta(x) \leq c \} \mathbbm{1}\{\mu_{t(x)}(s') > c\} 
    <
    \mathbbm{1}\{ \eta(x) \leq c \} \mathbbm{1}\{\mu_{t(x)}(s) > c\}   
\end{align*}

    \begin{align*}
        R(s') =&\ \int_{\dot{X}_t}^{\ddot{X}_t} \left( \mathbbm{1}\{ \eta(x) > c \} \mathbbm{1}\{\mu_{t(x)}(s') \leq c\}
        +
        \mathbbm{1}\{ \eta(x) \leq c \} \mathbbm{1}\{\mu_{t(x)}(s') > c\}   \right) f(x) dx \\
        <&\ \int_{\dot{X}_t}^{\ddot{X}_t} \left( \mathbbm{1}\{ \eta(x) > c \} \mathbbm{1}\{\mu_{t(x)}(s) \leq c\} +
        \mathbbm{1}\{ \eta(x) \leq c \} \mathbbm{1}\{\mu_{t(x)}(s) > c\}\right)  f(x) dx = R_t(s)
    \end{align*}
\end{proof}

\subsection{Proof for Theorem~\ref{theorem tendency}}
\begin{proof}
    By Lemma~\ref{lemma mid point}, without loss of generality\footnote{Proof for the case $\mu_R(s^{CART}) < \eta(s^{CART}) < \mu_L(s^{CART})$ is similar.}, assume 
    \begin{align*}
        \mu_L(s^{CART}) < \eta(s^{CART}) = \frac{\mu_L(s^{CART}) + \mu_R(s^{CART})}{2} < \mu_R(s^{CART}).
    \end{align*}
    We sequentially prove the two claims in the lemma: the existence of $s^*$ and risk reduction.
    
    
   \noindent\textbf{Existence of $s^*$} When $\eta(s^{CART}, t) > c$, we have $(\eta(s^{CART}) - c)(\mu_R(s^{CART}) - \mu_L(s^{CART})) > 0$, so we want to show that $\exists\ s \in (0,s^{CART})$ such that $\eta(s) = c$.

    

    \begin{equation*}
        \mu_L = {\rm min}(\mu_L,\mu_R) \leq c \implies \exists\ \tilde{s}\in [0,s^{CART}] \text{ such that } \eta(\tilde{s}) \leq c
    \end{equation*}
    If $\eta(\tilde{s}) = c$, then the existence of $s^*$ is proven. If $\eta(\tilde{s}) < c$, then by intermediate value theorem, $\exists\ s\in (\tilde{s},s^{CART})$ such that $\eta(s)= c$, the proof for existence of $s^*$ is complete for $\eta(s^{CART}, t) > c$. The case when $\eta(s^{CART}, t) \leq c$ can be proved using the same logic.
    %if there doesn't exist $s$ s.t. $\eta(s) = c$, $\eta(s)>c$ by $\eta$'s continuity. $\mu_L = H_L(s)/F(s) > c$ which contradicts our assumption. \red{Bill needs to change the proof.} 
    
    We showed that when $\eta(s^{CART}, t) > c$, there exists $s^*$ and the $s^*$ is in the range $(0,s^{CART})$. By continuity of $\eta$ and the definition of $s^*$, $\forall\ s \in (s^*,s^{CART}), \eta(s) > c$. By the same logic, we have that when $\eta(s^{CART}, t) \leq c$, $\forall\ s \in (s^{CART},s^*), \eta(s) \leq c$.
    
    
    \textbf{Risk reduction} We compare risks led by $s^{CART}$ and $s^*$:\\
    1. If $\eta(s^{CART}) \leq c$, for any $s\in (s^{CART}, s^*)$,  we have $\eta(s) \leq c$ and 
    \begin{align*}
    \mu_L(s) &= \int_{0}^s \eta(x) f(x) dx / F(s) = \frac{\int_{0}^{s^{CART}} \eta(x) f(x) dx + \int_{s^{CART}}^{s^{*}} \eta(x) f(x) dx}{F(s)} \\
    &< \frac{\int_{0}^{s^{CART}} \eta(x) f(x) dx + c\int_{s^{CART}}^{s^{*}}  f(x) dx}{F(s)} = \frac{\mu_L(s^{CART}) F(s^{CART}) + c(F(s) - F(s^{CART}))}{F(s)} < c\\
    \mu_R(s) &> \frac{\mu_R(s^{CART}) (1-F(s^{CART})) - c(F(s) - F(s^{CART}))}{1-F(s)} > c.
    \end{align*}
    % If $\mu_R(s^{CART}) \ge c$, $\mu_R(s) > \mu_R(s^{CART}) > c$. 
    % If $\mu_R(s^{CART}) < c$, $\mu_R(s)$ and $c$'s relationship is unknown. 

    Since the order that $\mu_L(s) \leq c < \mu_R(s)$ does not change and $\forall\ s \in (s^{CART},s^*), \eta(s) \leq c$, we have that when $X \in [0,s^{CART}]$, splitting rules $s$ and $s^{CART}$ are equivalent in classifying the latent probability; when $X \in (s^{CART},s)$ which has a nonzero measure, splitting rule $s$ performs strictly better than $s^{CART}$  in classifying the latent probability; when $X \in [s,1]$, splitting rules $s$ and $s^{CART}$ are equivalent in classifying the latent probability. Therefore, $s$ strictly dominates $s^{CART}$. %Denote the risk of $\mathbf{X}\in t$ with split $s$ as $R(s,t)$. We have
    \begin{align*}
        % R(s^{CART},t) - R(s,t) = (F(s) - F(s^{CART}))\mathbbm{1}_{\mu_R(s) > c} \ge 0,
        R(s^{CART}) - R(s) = (F(s) - F(s^{CART})) > 0,
    \end{align*}
    which is continuously monotone decreasing with respect to $s$ by the assumption that $f>0$, where $f$ is the pdf of X.\\
    2. If $\eta(s^{CART}) > c$, for $s\in (s^*, s^{CART})$,  we have $\eta(s) > c$, $s$ strictly dominates $s^{CART}$, and 
    \begin{align*}
        % R(s^{CART},t) - R(s,t) = (F(s^{CART}) - F(s))\mathbbm{1}_{\mu_L(s) < c} \ge 0,
        R(s^{CART}) - R(s) = (F(s^{CART}) - F(s)) > 0,
    \end{align*}
    which is continuously monotone increasing with respect to $s$ by the assumption that $f>0$.
\end{proof}

\subsection{Proof for Theorem \ref{theorem penalized loss works}}
\begin{proof}
    % Define 
    % \begin{align*}
    %     \mathcal{G}^*(s, c) &= G^*(\mu_L, c)F(s) + G^*(\mu_R,c)(1-F(s)).
    % \end{align*}
    Consider 
    \begin{align*}
        \frac{\partial \mathcal{G}^{PFS}(s, c)}{\partial s} = \frac{\partial \mathcal{G}^{CART}(s, c) + \lambda W(|\mu_L - c|)F(s) + \lambda W(|\mu_R - c|)(1-F(s))}{\partial s}.
    \end{align*}
    We focus on the new term $\frac{\partial W(|\mu_L - c|)F(s)}{\partial s}$ and $\frac{\partial W(|\mu_R - c|)(1-F(s))}{\partial s}$. By \eqref{dElds} and \eqref{dErds},
    \begin{align*}
        \frac{\partial W(|\mu_L - c|)F(s)}{\partial s} =&\ f(s)W(|\mu_L - c|) + \frac{\partial W(|\mu_L - c|)}{\partial \mu_L}\frac{\partial \mu_L}{\partial s}F(s)\\
        =&\ f(s)\left(W(|\mu_L - c|) + \frac{\partial W(|\mu_L - c|)}{\partial \mu_L} (\eta(s) - \mu_L)\right)\\
        \frac{\partial W(|\mu_R - c|)(1-F(s))}{\partial s} =&\ -f(s)W(|\mu_R - c|) + \frac{\partial W(|\mu_R - c|)}{\partial \mu_R}\frac{\partial \mu_R}{\partial s}(1-F(s))\\
        =&\ f(s)\left( - W(|\mu_R - c|) + \frac{\partial W(|\mu_R - c|)}{\partial \mu_R} (\mu_R - \eta(s))\right).
    \end{align*}
    % \begin{align*}
    %     \frac{\partial \mathcal{G}^*(s, c)}{\partial s} &\propto\ (2\eta(s) - \mu_L - \mu_R)(\mu_L - \mu_R) +\\& \left(W_\lambda(\mu_L, c) + \frac{\partial W_\lambda(\mu_L, c)}{\partial \mu_L} (\eta(s) - \mu_L) - W_\lambda(\mu_R, c) + \frac{\partial W_\lambda(\mu_R, c)}{\partial \mu_R} (\mu_R - \eta(s))\right).
    % \end{align*}
    With $\mathcal{G}^{PFS}(s, c)$'s derivative with respect to $s$, we evaluate how $\frac{\partial \mathcal{G}^{PFS}(s, c)}{\partial s}$ behaves at $s^{CART}$, i.e., when $2\eta(s) - \mu_L - \mu_R = 0$ and at $s^*$, i.e., when $\eta(s) = c$ respectively:
    \begin{align}
        \frac{1}{\lambda f(s)}\left.\frac{\partial \mathcal{G}^{PFS}(s, c)}{ \partial s}\right\vert_{s = s^{CART}}  &=\ W(|\mu_L - c|) - W(|\mu_R - c|)  \nonumber\\ 
        &\quad + \left(\frac{\partial W(|\mu_L - c|)}{\partial \mu_L} + \frac{\partial W(|\mu_R - c|)}{\partial \mu_R}\right)(\eta(s^{CART})-\mu_L)\label{gsop}\\
        \frac{1}{f(s)}\left.\frac{\partial \mathcal{G}^{PFS}(s, c)}{\partial s}\right\vert_{s = s^{*}} &=\ (2c - \mu_L - \mu_R)(\mu_R - \mu_L) +  \lambda(W(|\mu_L - c|) - W(|\mu_R - c|)) \nonumber\\
        &\quad + \lambda\left(\frac{\partial W(|\mu_L - c|)}{\partial \mu_L}(c - \mu_L) + \frac{\partial W(|\mu_R - c|)}{\partial \mu_R}(\mu_R - c)\right).\label{gsstar}
    \end{align}
    Note that $s^{*}$ in \eqref{gsstar} is only guaranteed to exist when the condition of $c$ in Theorem~\ref{theorem tendency} is met. We will address this in the latter part of the proof.
    Next, 
    % we use the divide-and-conquer strategy.
    consider all five possible scenarios of $c$'s location separately. (Still, without loss of generality, assume $\mu_L(s^{CART}) < \eta(s^{CART}) < \mu_R(s^{CART})$.)\\
    \newline
    \textbf{i}. $\mu_L(s^{CART}) \le c <\eta(s^{CART}) < \mu_R(s^{CART})$. 
    
    By Lemma~\ref{lemma mid point},$|\mu_R(s^{CART}) - c| > |\mu_L(s^{CART}) - c|$. $W(|\mu_L - c|) > W(|\mu_R - c|)$ by the monotonicity of $W$ and $\frac{\partial W(|\mu_L - c|)}{\partial \mu_L} + \frac{\partial W(|\mu_R - c|)}{\partial \mu_R} \ge 0$ by the convexity of $W$. 
    % \textbf{i}. If $\mu_L(s^{CART})<c< \eta(s^{CART})$, $|\mu_L - c| < |\mu_R - c|$:
    % $W(|\mu_L - c|) > W(|\mu_R - c|)$ by $W$'s monotonicity. $\frac{\partial W(|\mu_L - c|)}{\partial \mu_L}- \frac{\partial W(|\mu_R - c|)}{\partial \mu_R} >0 $ by $W$'s convexity. 
    Thus, by \eqref{gsop}, 
    \begin{align}
    \left.\frac{\partial \mathcal{G}^{PFS}(s, c)}{\partial s}\right\vert_{s = s^{CART}} >0.    \label{greater0}
    \end{align}
    By Lemma~\ref{lemma mid point}, and continuity of $\eta$, $\mu_L$ and $\mu_R$, 
    \begin{align*}
        (2\eta(s) - \mu_L - \mu_R)(\mu_R - \mu_L) < 0,\ s\in (s^{sp}, s^{CART}), %\text{\red{expand to a lemma?}}
    \end{align*}
    % where $s^{sp}$ is the maximum $s$ less than $s^{CART}$ such that $\mu_L(s^{sp}) = \mu_R(s^{sp})$. If there is no such $s^{sp}>0$ exists, let $s^{sp} = 0$.
    where $s^{sp} := \max\{s: s < s^{CART}, (2\eta(s) - \mu_L - \mu_R)(\mu_R - \mu_L) = 0\}$. If there is no such $s^{sp}>0$ exists, let $s^{sp} = 0$.
    % $(2\eta(s) - \mu_L - \mu_R)(\mu_R - \mu_L) < 0$ for all $s<s^{CART}$. 
    When $s\in(\max(s^*, s^{sp}), s^{CART})$, by Theorem~\ref{theorem tendency}, $\mu_L<c$ and $\mu_R>c$. 
    
    When $s^* > s^{sp}$, define $d_L := c - \mu_L$ and $d_R := \mu_R - c$ and we have
    \begin{align*}
    0>(2c - \mu_L(s^*) - \mu_R(s^*))(\mu_R(s^*) - \mu_L(s^*))  = (d_L-d_R)(d_L+d_R),
    \end{align*}
    which directly suggests that $d_L < d_R$.  
    Let $W_2(d) = d^2 + \lambda\left(W(d) - dW'(d)\right)$. We can rewrite \eqref{gsstar} as:
    \begin{align*}
        \frac{1}{f(s)}\left.\frac{\partial \mathcal{G}^{PFS}(s, c)}{\partial s}\right\vert_{s = s^{*}} =&\ d_L^2 + \lambda\left(W(d_L) - d_L\frac{\partial W(d_L)}{\partial d_L}\right) - d_R^2 - \lambda\left(W(d_R) - d_R\frac{\partial W(d_R)}{\partial d_R}\right) \\
        =&\ W_2(d_L) - W_2(d_R).
    \end{align*}
    Note that $W_2^{'}(d) = d(2-\lambda W^{''}(d))$. By the bounded second derivative assumption, set $\Lambda_1 = \frac{1}{\max\{W^{''}(d_L), W^{''}(d_R)\}}$. Since $0<d_L<d_R$, when $\lambda < \Lambda_1$, $W_2(d_L) < W_2(d_R)$, 
    \begin{align*}
        \frac{1}{f(s)}\left.\frac{\partial \mathcal{G}^{PFS}(s, c)}{ \partial s}\right\vert_{s = s^{*}} < 0.
    \end{align*}
    
    When $s^* < s^{sp}$, there exists $s^{in} \in (s^{sp},s^{CART})$ and $c>0$ s.t. 
    \begin{align*}
    (2\eta(s) - \mu_L - \mu_R)(\mu_R - \mu_L) < -c,    
    \end{align*}
    by unique global minimum assumption and continuity. With $\Lambda_1 = \frac{c}{W(0) + W^{'}(0)}$, we have
    \begin{align*}
    \frac{1}{f(s)}\left.\frac{\partial \mathcal{G}^{PFS}(s, c)}{ \partial s}\right\vert_{s = s^{in}} < 0.
    \end{align*}
    
    % By the bounded second derivative assumption, set $\Lambda_1 = \frac{1}{\{W^{'}(d_L), W^{''}(d_R)\}}$.
    
    Thus, %regardless the relationship between $s^*$ and $s^{sp}$, 
    for $0<\lambda<\Lambda_1$, there exists a $s\in[s^*, s^{CART})$ s.t.
    % we have $W_2^{'}(d) > 0$, $W_2(d_L) - W_2(d_R) < 0$, and 
    % \begin{align*}
    %     \frac{1}{f(s)}\left.\frac{\partial \mathcal{G}^*(s, c)}{\partial s}\right\vert_{s = s^{*}} <&\ \lambda(W(|\mu_L - c|) - W(|\mu_R - c|)) \nonumber\\
    %     &\quad + \lambda\left(\frac{\partial W(|\mu_L - c|)}{\partial \mu_L}(c - \mu_L) + \frac{\partial W(|\mu_R - c|)}{\partial \mu_R}(\mu_R - c)\right)\\
    %     =&\ \lambda(W_2(d_L) - W_2(d_R)).
    % \end{align*}
    % By convexity of $W$ and $0<d_L<d_R$, $W_2(d_L) > W_2(d_R)$. 
    \begin{align}
    \frac{\partial \mathcal{G}^{PFS}(s, c)}{\partial s} < 0.\label{less0}
    \end{align}
    Considering \eqref{greater0} and \eqref{less0}
    % $\left.\frac{\partial \mathcal{G}^*(s, c)}{\partial s}\right\vert_{s = s^{*}} < 0$ and $\left.\frac{\partial \mathcal{G}^*(s, c)}{\partial s}\right\vert_{s = s^{CART}} >0$ 
    jointly with the continuity of the derivative, we can conclude that there exists a $s^{**}\in (s^*, s^{CART})$ such that $\left.\frac{\partial \mathcal{G}^{PFS}(s, c)}{\partial s}\right\vert_{s = s^{**}} = 0$. $s^{**}$ is a local minimizer of $\mathcal{G}^{PFS}(s, c)$ and is guaranteed to give a smaller risk than $s^{CART}$ by Theorem~\ref{theorem tendency}. 

    Next, we argue that with some additional bound for $\lambda$, $s^{**}$ is also the global optimal split under the penalized impurity measure. To simplify the notation in the rest of the proof, let $s^{in}$ be $s^*$ if $s^*<s^{sp}$. Then,    
    % Then, showing $s^{**}$ is the global minimizer is equivalent to show $\mathcal{G}^*(s^{**},c) < \mathcal{G}(s^{sc})$
    we want to show $\mathcal{G}^{PFS}(s^{**},c) < \mathcal{G}^{PFS}(s,c)$ for $s\neq s^{**}$:
    
    (a). Consider $s$ such that there is at least one $s^{sp}$ in between $s$ and $s^{**}$.
    By unique optimizer assumption for $G$, if there is more than one local minimum, i.e., multiple $s$ that have $2\eta(s, t) - \mu_L - \mu_R = 0$, we assume the global minimum and the second smallest local minimum, gives by $s^{sc}$, different by $\Delta$. Let $\Lambda_2 = \frac{\Delta}{W(0) - \min(W(c), W(1-c))}$. For $0<\lambda<\Lambda_2$,  
    \begin{align*}
    \mathcal{G}^{PFS}(s^{**},c) <&\ \mathcal{G}^{PFS}(s^{CART},c) \\
    =&\ \mathcal{G}(s^{CART},c) + \lambda \left(W(|\mu_L-c|)F(s) + W(|\mu_R-c|)(1-F(s))\right)\\
    \le&\  \mathcal{G}(s^{CART},c) + \lambda W(0) \\
    \le&\ \mathcal{G}(s^{sc},c) + \lambda \min\{W(c), W(1-c)\} \le \mathcal{G}^{PFS}(s,c)
    \end{align*}
    
    (b). Consider $s$ such that there is no $s^{sp}$ in between $s$ and $s^{**}$. 
    
    When $s \in (s^*, s^{CART})$, $\mathcal{G}^{PFS}(s^{**},c) < \mathcal{G}^{PFS}(s,c)$ by definition. 
    
    When $s<s^{*}$, by $\eqref{less0}$ and continuity of $\frac{\partial \mathcal{G}^{PFS}(s, c)}{\partial s}$, there can be two possible scenarios. One, there exist a $s_0<s^*$ such that $\frac{\partial \mathcal{G}^{PFS}(s, c)}{\partial s} = 0$ and $\mathcal{G}^{PFS}(s_0,c) - \mathcal{G}^{PFS}(s^{**},c) := \Delta_L > 0$. Then, define $\Lambda_3 = \frac{\Delta_L}{W(0) - \min(W(c), W(1-c))}$. For $0<\lambda<\Lambda_3$, $\mathcal{G}^{PFS}(s^{**},c) < \mathcal{G}^{PFS}(s,c)$ when $s_0\le s< s^*$ by definition. When $s<s_0$:
    \begin{align*}
    \mathcal{G}^{PFS}(s^{**},c) =&\ \mathcal{G}^{PFS}(s_0,c) - \Delta_L\\
    =&\ \mathcal{G}(s_0,c) + \lambda \left(W(|\mu_L-c|)F(s) + W(|\mu_R-c|)(1-F(s))\right) - \Delta_L\\
    \le&\  \mathcal{G}(s_0,c) + \lambda W(0) - \Delta_L\\
    \le&\ \mathcal{G}(s_0,c) + \lambda \min\{W(c), W(1-c)\}\\
    \le&\ \mathcal{G}(s,c) + \lambda \left(W(|\mu_L-c|)F(s) + W(|\mu_R-c|)(1-F(s))\right) = \mathcal{G}^{PFS}(s,c).
    \end{align*}
    Two, $\frac{\partial \mathcal{G}^{PFS}(s, c)}{\partial s} < 0$. Then $\mathcal{G}^{PFS}(s^{**},c) < \mathcal{G}^{PFS}(s,c)$ for $s<s^{*}$ follows. 

    When $s > s^{CART}$, we can follow the same procedure as when $s<s^{*}$. If there exists a $s_1>s^*$ such that $\frac{\partial \mathcal{G}^{PFS}(s, c)}{\partial s} = 0$ and $\mathcal{G}^{PFS}(s_1,c) - \mathcal{G}^{PFS}(s^{**},c) := \Delta_R > 0$. We can define $\Lambda_4 = \frac{\Delta_R}{W(0) - \min(W(c), W(1-c))}$ and everything else follows.
    
    To conclude, for $0 < \lambda \le \Lambda = \min\{\Lambda_1, \Lambda_2, \Lambda_3, \Lambda_4\}$, the theoretical optimal split chosen with respect to $G^{PFS}$ leads to a risk smaller than $s^{CART}$ when $\mu_L(s^{CART}) \le c <\eta(s^{CART}) < \mu_R(s^{CART})$.\\
    \newline
    \textbf{ii}. $\mu_L(s^{CART}) < \eta(s^{CART}) < c \le \mu_R(s^{CART})$.
    This scenario is symmetric to \textbf{i} and is also considered in Theorem~\ref{theorem tendency}. It can be proved following the same logic as in \textbf{i}.\\
    \newline
    \textbf{iii}. $c < \mu_L(s^{CART}) <\eta(s^{CART}) < \mu_R(s^{CART})$.
    
    In this scenario, consider two split points located on each side of $s^{CART}$ respectively: 
    let $s_{c1}$ be the largest split less than $s^{CART}$ such that $\mu_L(s_{c1}) = c$, $s_{c2}$ be the smallest split greater than $s^{CART}$ such that $\mu_L(s_{c2}) = c$, $s_L$ be the largest split less than $s^{CART}$ such that $\eta(s_L) \ge \mu_L(s^{CART})$ and $s_R$ be the smallest split greater than $s^{CART}$ such that $\eta(s_R) = \mu_R(s^{CART})$. Let $s_{c1} = 0$ or $s_{c2} = 1$ if no such $s_{c1}$ or $s_{c2}$ exists. Both $s_L$ and $s_R$'s existence is guaranteed by the intermediate value theorem which we used once in Theorem~\ref{theorem tendency}. 

    We argue for any $s\in (\max(s_L, s_{c1}), \min(s_R,s_{c2}))$, risk is no greater than $s^{CART}$. 
    
    For $s\in (\max(s_L, s_{c1}), s^{CART})$, it is a piece of $x$ that has $\eta(x) > \mu_L(s^{CART})$ being split to right.
    \begin{align*}
        \mu_R(s) >&\ \frac{\mu_R(s^{CART})(1-F(s^{CART})) + \mu_L(s^{CART})(F(s^{CART})-F(s)) }{1-F(s)} > \mu_L(s^{CART})>c\\
        c < \mu_L(s) <&\ \frac{\mu_L(s^{CART})F(s^{CART}) - \mu_L(s^{CART})(F(s^{CART})-F(s)) }{F(s)} = \mu_L(s^{CART}).
    \end{align*}
    Since $c < \mu_L(s) <\mu_L(s^{CART}) < \mu_R(s)$, the expectations on both sides remain greater than $c$ and the risk is the same as split $s^{CART}$. When $s\in (s^{CART}, \min(s_R,s_{c2}))$, $c < \mu_L(s) <\mu_R(s^{CART}) < \mu_R(s)$ can be obtained similarly and the risk also remains. 

    Note that $\mu_R(s) > \mu_L(s)$ all the way when $s\in (\max(s_L, s_{c1}), \min(s_R,s_{c2}))$. We can then define $\Delta$ the same as in scenario \textbf{i}, $\Delta_L = \mathcal{G}^{PFS}(\max(s_L, s_{c1}), c)$, and $\Delta_R = \mathcal{G}^{PFS}(\min(s_R,s_{c2}), c)$. $\Lambda_2, \Lambda_3, \Lambda_4$ are defined accordingly. 
    For $0<\lambda\le \Lambda =\min\{\Lambda_2, \Lambda_3, \Lambda_4\}$, the theoretical optimal split chosen with respect to $G^{PFS}$ leads to the same risk as $s^{CART}$ when $c < \mu_L(s^{CART}) <\eta(s^{CART}) < \mu_R(s^{CART})$.\\
    \newline 
    \textbf{iv}. $\mu_L(s^{CART}) <\eta(s^{CART}) < \mu_R(s^{CART}) < c$. This scenario is symmetric to \textbf{iii}. It can be proved following the same logic in \textbf{iii}.\\
    \textbf{v}. $\mu_L(s^{CART}) <\eta(s^{CART}) = c < \mu_R(s^{CART})$. This scenario can be considered a special case of \textbf{i} and \textbf{ii}. By appropriate choice of $\Lambda$, it's trivial to show that the original $s^{CART}$ remains the global optimizer and the risk stays the same. 

\end{proof}


We find it clearer to prove Corollary \ref{theorem monotonic eta uniform X} and then plug in some of the steps in the proof for Corollary \ref{theorem monotonic eta uniform X} into proof for Theorem \ref{theorem unique intersection between eta and c}. Hence, we first present the proof for Corollary \ref{theorem monotonic eta uniform X}.

\subsection{Proof for Corollary \ref{theorem monotonic eta uniform X}}
\begin{proof}
    WLOG, assume $\eta$ is monotonically increasing with respect to x. The proof for the monotonically decreasing case is similar. We will first discuss two distinct cases: $\mu_L < \mu_R \leq c$ and $c \leq \mu_L < \mu_R$ and show that for both cases $\mathcal{G}^*$ is constant. Then, we consider the case where $\mu_L \leq c \leq \mu_R$
    and show that
    $s^*$ maximizes $\mathcal{G}^*$ in this case. We complete the proof by showing that $\mathcal{G}^*(s^*,c)$ is larger than the two constant $\mathcal{G}^*$ for the first two cases.

    i. $\mu_L < \mu_R \leq c$, we can write $\mathcal{G}^*$ as 
    \begin{align*}
        s\left(c-\frac{\int_0^s \eta(x) dx}{s}\right) + (1-s)\left(c-\frac{\int_s^1 \eta(x) dx}{1-s}\right) = c - \int_0^1 \eta(x) dx
    \end{align*}
    Note that in this case, $\mathcal{G}^*$ is a constant.
    
    ii. $ c \leq \mu_L < \mu_R$, we can write $\mathcal{G}^*$ as 
    \begin{align*}
        s\left(\frac{\int_0^s \eta(x) dx}{s} - c\right) + (1-s)\left(\frac{\int_s^1 \eta(x) dx}{1-s} - c\right) = \int_0^1 \eta(x) dx - c
    \end{align*}
    Note that in this case, $\mathcal{G}^*$ is a constant.
    

    iii. $\mu_L \leq c \leq \mu_R$, we can write $\max_s \mathcal{G}^*$ as 
    \begin{align*}
        \max_s s\left(c - \frac{\int_0^s \eta(x) dx}{s} \right) + (1-s)\left( \frac{\int_s^1 \eta(x) dx}{1-s} - c \right) 
    \end{align*}

    The first-order condition is $c - \eta(s) - \eta(s) + c = 0$. We solve $\eta(s) = c$, hence, $s^*$ is a local optima. Moreover, second order derivative is $-2\eta'(s^*)$ which is negative given that $\eta$ is strictly increasing and differentiable in a neighborhood of $s^*$. Hence, $s^*$ is a local maxima. Since $\mathcal{G}^*$ is continuous in $s$, we need to show that $\mathcal{G}^*(s^*)$ is larger than the boundary points for the case $\mu_L \leq c \leq \mu_R$ to claim $s^*$ as the global maxima for case iii. There are two possibilities for the boundary points

    Possibility 1:
    Consider $s_1$ and $s_2$ and their associated $\mathcal{G}^*(s_1)$ and $\mathcal{G}^*(s_2)$ where
    $\mu_L(s_1) = c$ and $\mu_R(s_2) = c$. Since $s_1$ is included in case ii; whereas $s_2$ is included in case i, once we show that $\mathcal{G}^*(s^*) > \mathcal{G}^*(s_1), \mathcal{G}^*(s_2)$, we can claim $s^*$ to be the unique global maxima among all possible $s$.

    \begin{align*}
        \mathcal{G}^*(s^*) =& s^*c - \int_0^{s^*} \eta(x) dx  + \int_{s^*}^1 \eta(x) dx   - (1 - s^*)c \\
        >& s^*c - \int_0^{s^*} \eta(x) dx + (1-s^*)c - \int_{s^*}^1 \eta(x) dx \\
        =& c - \int_0^1 \eta(x) dx = \mathcal{G}^*(s_2) \\
         \mathcal{G}^*(s^*) =& s^*c - \int_0^{s^*} \eta(x) dx  + \int_{s^*}^1 \eta(x) dx   - (1 - s^*)c \\
        >& \int_0^{s^*} \eta(x) dx - s^*c + (1-s^*)c - (1 - s^*)c \\
        =& \int_0^1 \eta(x) dx - c = \mathcal{G}^*(s_1)
    \end{align*}

Possibility 2: Even at the boundary point,  $\mu_L \leq c \leq \mu_R$ still holds. Then, the boundary points for case iii are $s=0$ and $s=1$.
\begin{align*}
    \tilde{G}(0) =& \int_0^1 \eta(x) dx - c < \mathcal{G}^*(s^*) \\
    \tilde{G}(1) =& c - \int_0^1 \eta(x) dx < \mathcal{G}^*(s^*)
\end{align*}
Since under Possibility 2 case iii spans the entire domain of $X$, $s^*$ is the unique global maxima. Note that Assumption \ref{assumption unique intersection between eta and c} implies that $\exists~ \epsilon > 0$, such that $s^* \in [\epsilon,1-\epsilon] \subset [0,1]$. Therefore, $s^*$ is the global maxima among all possible $s$.

    
    
\end{proof}


\subsection{Proof for Theorem \ref{theorem unique intersection between eta and c}}
\begin{proof}
    Again, assume that $\eta$ is monotonically increasing in the neighborhood of $s^*$, the proof for the monotonically decreasing case is the same. Assumption \ref{assumption unique intersection between eta and c} guarantees $\forall s < s^*, \eta(X) < c$, $\forall s > s^*, \eta(X) > c$. 

    Let $[s^*-\epsilon_1,s^*+\epsilon_1]$ denote the monotonically increasing $\eta$ neighborhood of $s^*$, where $\epsilon_1 > 0$. Let $[s^*-\epsilon_2,s^*+\epsilon_2]$ denote an interval that contains $s^*$ in which $\forall s \in [s^*-\epsilon_2,s^*+\epsilon_2], \mu_L < c < \mu_R$ and $\epsilon_2 > 0$. The existence of $[s^*-\epsilon_2,s^*+\epsilon_2]$ is guaranteed because both $\mu_L$ and $\mu_R$ are continuous in $s$ and given Assumption \ref{assumption unique intersection between eta and c}, $\mu_L(s^*)<c<\mu_R(s^*)$. 

    Consider $s \in  [s^*-\epsilon_1,s^*+\epsilon_1] \cap [s^*-\epsilon_2,s^*+\epsilon_2] = [a,b]$, where $a = \max(s^*-\epsilon_1,s^*-\epsilon_2)$ and $b = \min(s^*+\epsilon_1,s^*+\epsilon_2)$. %Note that $[a,b]$ must contain $s^*$ and hence, is not an empty set. 
    This case is equivalent to case iii in the proof for Corollary \ref{theorem monotonic eta uniform X}. Hence, $s^*$ is a local maxima for the interval $[a,b]$ and $\mathcal{G}^*$ is
    \[
    s^*c - \int_0^{s^*} \eta(x) dx + \int_{s^*}^1 \eta(x) dx - (1 - s^*)c.
    \]

    Consider $s \leq a$. When $\mu_R \leq c$, $\mathcal{G}^*$ is $c - \int_0^1 \eta(x) dx$, the proof is identical to case i in the proof for Corollary \ref{theorem monotonic eta uniform X}. When $\mu_R > c$, 
    \begin{align*}
        \mathcal{G}^* =& sc - \int_0^{s} \eta(x) dx + \int_s^1 \eta(x) dx - (1-s)c \\
        =& sc + \int_s^{s^*} \eta(x) dx - \int_0^{s^*} \eta(x) dx + \int_{s^*}^1 \eta(x) dx + \int_{s}^{s^*} \eta(x) dx - (1-s)c\\
        <& sc + (s^* -s)c - \int_0^{s^*} \eta(x) dx + \int_{s^*}^1 \eta(x) dx + (s^* -s)c - (1-s)c \\
        =& s^*c - \int_0^{s^*} \eta(x) dx + \int_{s^*}^1 \eta(x) dx - (1 - s^*)c
    \end{align*}

    The proof for when $s \geq b$ is similar, those $s$ results in higher $\mathcal{G}^*$.

    Since $s^*$ is the unique point in the interval $[a,b]$ which meets the first-order condition and the boundary points $\{a,b\}$ have lower $\mathcal{G}^*$ than $s^*$, $s^*$ is the unique maxima in the interval $[a,b]$. Moreover, $s \in [0,a] \cup [b,1]$ also have lower $\mathcal{G}^*$ than $s^*$, hence, $s^*$ is the unique global maxima.
\end{proof}

%Theorem \ref{theorem unique intersection between eta and c} potentially explains why in Table \ref{Table simulation results in main text} MDFS performs the best even under non-monotonic data generating processes such as nonlinear separable, periodic, and step functions.

\subsection{Proof for Theorem \ref{theorem MDFS consistency}}
\begin{proof}
\textbf{Outline}: We leverage tools for studying asymptotic properties of M-estimator. To show $\hat{s} \stackrel{p}{\to} s^*$, we take two steps:
\begin{enumerate}
    \item Under Assumption \ref{assumption unique intersection between eta and c}, we show uniform convergence of the estimator of cost function $\widehat{{\cal G}}^*$ to its population target ${\cal G}^*$, i.e., $\sup_{s \in (\epsilon,1-\epsilon)} | {\cal G}^*(s,c) - \widehat{{\cal G}}^*(s,c)|  \stackrel{p}{\to} 0$ as $n\to\infty$.
    \item Combining with Theorem \ref{theorem unique intersection between eta and c}, we can apply Theorem 5.7 of \citet{van2000asymptotic} and get the desired result.
\end{enumerate}
{\textbf {Step 1}}. For any given $c \in (0,1)$ and $\epsilon_0>0$, we have
    \begin{align}
    &~\pr\Bigg(
    \sup_{s \in (\epsilon,1-\epsilon)} | {\cal G}^*(s,c) - \widehat{{\cal G}}^*(s,c)| \geq \epsilon_0
    \Bigg)
    \notag\\
    =&~
    \pr\Bigg(
    \sup_{s \in (\epsilon,1-\epsilon)}
    \Bigg|
    s
    \bigg|
        \frac
        {\sum_{i=1}^{n} Y_i\mathbbm{1}\{X_i \leq s\}}
        {\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}}
        -c
    \bigg| - s|\mu_L-c| 
    \notag\\
    + &~
    (1-s)
    \bigg|
        \frac
        {\sum_{i=1}^{n} Y_i \mathbbm{1}\{X_i > s\}}
        {\sum_{i=1}^{n}\mathbbm{1}\{X_i > s\}}
        -c
    \bigg| - (1-s)|\mu_R - c|
    \Bigg| \geq \epsilon_0
    \Bigg)
    \notag\\
    \label{proofeqn::consistency-part1}
    \leq&~
    \pr\Bigg( \sup_{s \in (\epsilon,1-\epsilon)} 
    s\Bigg|
    \frac
    {\sum_{i=1}^{n} Y_i\mathbbm{1}\{X_i \leq s\}}
    {\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}}
    - \mu_L\Bigg| \geq \epsilon_0/2 \Bigg) 
    \\
    \label{proofeqn::consistency-part2}
    + &~
    \pr\Bigg( \sup_{s \in (\epsilon,1-\epsilon)} 
    (1-s)\Bigg|
    \frac
        {\sum_{i=1}^{n} Y_i \mathbbm{1}\{X_i > s\}}
        {\sum_{i=1}^{n}\mathbbm{1}\{X_i > s\}}
    - \mu_R\Bigg|\geq \epsilon_0/2 \Bigg)
\end{align}
We focus on showing \eqref{proofeqn::consistency-part1} goes to 0 as $n\to\infty$, the proof for \eqref{proofeqn::consistency-part2} is exactly the same.

For any $\delta > 0$, denote event ${\cal E} = \inf_{s \in (\epsilon,1-\epsilon)} n^{-1}\sum_{i=1}^{n} \mathbbm{1}_{\{X_i \leq s\}} \geq \delta$. We have
\begin{align}
    &~\pr\Bigg( \sup_{s \in (\epsilon,1-\epsilon)} 
    s\Bigg|
    \frac
    {\sum_{i=1}^{n} Y_i\mathbbm{1}\{X_i \leq s\}}
    {\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}}
    - \mu_L\Bigg| \geq \epsilon_0/2 \Bigg)
    \notag\\
    \leq&~
    \pr\Bigg( \sup_{s \in (\epsilon,1-\epsilon)}
    s\Bigg(
        \frac
        {\big|n^{-1}\sum_{i=1}^{n} Y_i\mathbbm{1}\{X_i \leq s\} - s\mu_L\big|}
        {n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}}
        +
        \frac
        {\mu_L\big|n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}-s\big|}
        {n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}}
     \Bigg) \geq \epsilon_0/2   
    \Bigg)
    \notag\\
    \leq&~
    \pr
    \Bigg( \sup_{s \in (\epsilon,1-\epsilon)}
    \frac
        {s\big|n^{-1}\sum_{i=1}^{n} Y_i\mathbbm{1}\{X_i \leq s\} - s\mu_L\big|}
        {n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}}
        \geq \epsilon_0/4
    \Bigg) + 
    \pr
    \Bigg( \sup_{s \in (\epsilon,1-\epsilon)}
        \frac
        {\mu_L\big|n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}-s\big|}
        {n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}}
        \geq \epsilon_0/4
    \Bigg)
    \notag\\
    \leq&~
    \pr
    \Bigg(
    \frac
        {\sup_{s \in (\epsilon,1-\epsilon)}
        s\big|n^{-1}\sum_{i=1}^{n} Y_i\mathbbm{1}\{X_i \leq s\} - s\mu_L\big|}
        {\inf_{s \in (\epsilon,1-\epsilon)} n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}}
        \geq \epsilon_0/4
    \Bigg) + 
    \pr
    \Bigg(
        \frac
        {\sup_{s \in (\epsilon,1-\epsilon)}
        \mu_L\big|n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}-s\big|}
        {\inf_{s \in (\epsilon,1-\epsilon)} n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}}
        \geq \epsilon_0/4
    \Bigg)
    \notag\\
    \leq&~
    \pr
    \Bigg( 
    \sup_{s \in (\epsilon,1-\epsilon)}
        s\Big|n^{-1}\sum_{i=1}^{n} Y_i\mathbbm{1}\{X_i \leq s\} - s\mu_L\Big| \geq (\epsilon_0\delta)/4, {\cal E}
    \Bigg)
    +
    \pr
    \Bigg(
        \sup_{s \in (\epsilon,1-\epsilon)}
        \mu_L\Big|n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}-s\Big|
        \geq (\epsilon_0\delta)/4, {\cal E}
    \Bigg)
    \notag\\
    \label{proofeqn::consistency-numerator}
    \leq&~
     \pr
    \Bigg( 
    \sup_{s \in (\epsilon,1-\epsilon)}
        s\Big|n^{-1}\sum_{i=1}^{n} Y_i\mathbbm{1}\{X_i \leq s\} - s\mu_L\Big| \geq (\epsilon_0\delta)/4
    \Bigg)
    \\
    \label{proofeqn::consistency-denominator}
    +&~
    \pr
    \Bigg(
        \sup_{s \in (\epsilon,1-\epsilon)}
        \mu_L\Big|n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}-s\Big|
        \geq (\epsilon_0\delta)/4
    \Bigg)
\end{align}
For \eqref{proofeqn::consistency-numerator}, let $\bm{Z}_i = (X_i,Y_i), \,i=1,\dots,n$ and $f(\bm{Z}_i,s) = Y_i\mathbbm{1}_{\{X_i \leq s\}}$. Notice $\ep[f(\bm{Z},s)] = s\mu_L$, $f(\bm{Z},s)$ is continuous at each $s \in (0,1)$ for almost all $\bm{Z}$, since discontinuity occurs at $X_i = s$, which has measure zero for $X_i \sim \textrm{Unif}(0,1)$. Also, $f(\bm{Z},s) \leq \mathbbm{1}_{\{0 \leq X \leq 1\}}$ with $\ep[\mathbbm{1}_{\{0 \leq X \leq 1\}}] = 1 \leq \infty$. Applying uniform law of large numbers, we know
\begin{align}
    &~\pr
    \Bigg( 
    \sup_{s \in (\epsilon,1-\epsilon)}
        s\big|n^{-1}\sum_{i=1}^{n} Y_i\mathbbm{1}\{X_i \leq s\} - s\mu_L\big| \geq (\epsilon_0\delta)/4
    \Bigg)
    \notag\\
    \leq&~
    \pr
    \Bigg( 
    \sup_{s \in (\epsilon,1-\epsilon)}
        \big|n^{-1}\sum_{i=1}^{n} Y_i\mathbbm{1}\{X_i \leq s\} - s\mu_L\big| \geq (\epsilon_0\delta)/4
    \Bigg)
    \notag\\
    \leq&~
    \pr
    \Bigg( 
    \sup_{s \in (0,1)}
        \big|n^{-1}\sum_{i=1}^{n} Y_i\mathbbm{1}\{X_i \leq s\} - s\mu_L\big| \geq (\epsilon_0\delta)/4
    \Bigg)
    \to 0,
\end{align}
as $n\to\infty$. For \eqref{proofeqn::consistency-denominator}, notice $F_n(s) = n^{-1} \sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}$ is the empirical CDF of $X \sim \textrm{Unif}(0,1)$, with CDF $F_X(s) = s$ for $s \in (0,1)$. Applying Dvoretzky–Kiefer–Wolfowitz inequality, we have
\begin{align}
    &~\pr
    \Bigg(
        \sup_{s \in (\epsilon,1-\epsilon)}
        \mu_L\big|n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}-s\big|
        \geq (\epsilon_0\delta)/4
    \Bigg)
    \notag\\
   (\mu_L \in [0,1]) \leq&~
    \pr
    \Bigg(
        \sup_{s \in (\epsilon,1-\epsilon)}
        \big|n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}-s\big|
        \geq (\epsilon_0\delta)/4
    \Bigg)
    \notag\\
    \leq&~
    \pr
    \Bigg(
        \sup_{s \in (0,1)}
        \big|n^{-1}\sum_{i=1}^{n}\mathbbm{1}\{X_i \leq s\}-s\big|
        \geq (\epsilon_0\delta)/4
    \Bigg)\to 0,
\end{align}
as $n\to\infty$. Combining \eqref{proofeqn::consistency-numerator} and \eqref{proofeqn::consistency-denominator}, we can show \eqref{proofeqn::consistency-part1} goes to 0 as $n\to\infty$. Similarly, \eqref{proofeqn::consistency-part2} goes to 0 as $n\to\infty$. This gives us 
$\pr(\sup_{s \in (\epsilon,1-\epsilon)} | {\cal G}^*(s) - \widehat{{\cal G}}^*(s)| \geq \epsilon_0) \to 0$ as $n\to\infty$, which completes the proof of Step 1.

\textbf{Step 2}. By Theorem \ref{theorem unique intersection between eta and c}, we have for any $\epsilon > 0$, $\sup_{s: d(s,s^*) > \epsilon} {\cal G}^*(s,c) < {\cal G}^*(s^*,c)$. Combining with $\sup_{s \in (\epsilon,1-\epsilon)} | {\cal G}^*(s,c) - \widehat{{\cal G}}^*(s,c)|  \stackrel{p}{\to} 0$, we apply Theorem 5.7 of \citet{van2000asymptotic} and conclude $\hat{s} \stackrel{p}{\to} s^*$.
\end{proof}

\subsection{Mathematical details for Figure \ref{Figure Unique Sine example} and \ref{Figure Monotonic example}}

Imagine there are two final splitting nodes $\{t_1,t_2\}$ with the same mass of population. CART selects features $\{X_1, X_2\}$ for nodes $\{t_1,t_2\}$, respectively. The pdf of $X_1$ and $X_2$ in the $\{t_1,t_2\}$ are both uniform, respectively: $f_1(x) = 1$ and $f_2(x) = 1$ $\forall~x \in [0,1]$.

In node $t_1$,
\[
P(Y=1|X_1) = : \eta_1(X_1) = 
\begin{cases}
    1, \text{ when $X_1 \in [0,\frac{1}{4}]$} \\
    \frac{{\rm sin}(2 \pi X_1) + 1}{2} , \text{ when $X_1 \in (\frac{1}{4},\frac{3}{4})$} \\
    0, \text{ when $X_1 \in [\frac{3}{4},1]$}
\end{cases}
\]


We depict $\eta_1(X_1)$ in Figure \ref{Figure Unique Sine example}. Since $\eta_1(X_1)$ is reflectional symmetric around $\eta_1(0.5)$, $s^{cart} = 0.5$. Splitting $s^{cart}$, policymakers target the left node $X_1 \leq 0.5$ only, the calculation is similar to Figure \ref{Figure Sine example} in the introduction.

The unique optimal LPC solution is $s^* = \frac{5}{12}$. By shifting from $s^{CART}$ to $s^*$, the policymaker excludes subpopulation $\{\frac{5}{12}<X_1<\frac{1}{2}|t_1\}$ from the targeted group. Again, the mathematics is similar to Figure \ref{Figure Sine example} in the introduction.

In node $t_2$, 
\[
P(Y=1|X_2) =: \eta_2(X_2) = \frac{9}{11}X_2
\]


The other node $t_2$ is depicted by Figure \ref{Figure Monotonic example}. CART will split $t_2$ at $s^{CART} = 0.5$ because $\eta_2(X_2)$ is reflectional symmetric around $\eta_2(0.5)$. This would not target any subpopulations from $t_2$, as both the left node mean and right node mean are smaller than 0.75.
\begin{align*}
    &P(Y = 1| X_2 < 0.5) = \frac{\int_{0}^{0.5} \frac{9}{11} x dx}{0.5} = \frac{9}{44} < 0.75 \\
    &P(Y = 1| X_2 > 0.5) = \frac{\int_{0.5}^{1} \frac{9}{11} x dx}{0.5} = \frac{27}{44} < 0.75
\end{align*}
On the other hand, the best split for LPC is $s^* = \frac{11}{12}$. This splits $t_2$ into two groups, the left node is entirely below the threshold whereas the right node is entirely above, as illustrated by the yellow and green segment in Figure \ref{Figure Monotonic example}, respectively. As a result, splitting at $s^*$ targets subpopulation $\{\frac{11}{12}<X_2<1|t_2\}$.

To summarize, splitting nodes $t_1$ and $t_2$ individually at $s^{CART}$ versus $s^*$ results in different target subpopulations:
\begin{itemize}
    \item $s^{CART}$: Target $\{X_1<\frac{1}{2}|t_1\}$.
    \item $s^*$: Target $\{X_1<\frac{5}{12}|t_1\}$ and $\{\frac{11}{12}<X_2<1|t_2\}$.
\end{itemize}
Assuming that both $t_1$ and $t_2$ contain the same amount of population, the two sets of policies target the same proportion of the population, but $\eta(x,t_1) < 0.75$ for $x \in \{\frac{5}{12}<X_1<\frac{1}{2}\}$, which is targeted by $s^{CART}$, whereas $\eta(x,t_2) > 0.75$ for $x \in \{\frac{11}{12}<X_2<1\}$, which is targeted by $s^*$. Therefore, policies based on LPC, i.e., $s^*$ policy, target a \textbf{more vulnerable} subpopulation than policies based on observed $Y$ classification, i.e., $s^{CART}$ policy or CART/KD-CART policy. This idea appears in our diabetes empirical study, which we detail in Section \ref{sec empirical studies}.


%\section{Additional Experiment Details}
\section{Synthetic data simulation studies} \label{appendix Synthetic data simulation studies}
% Our synthetic data simulations include \underline{linearly separable} data, \underline{nonlinearly separable} data, and \underline{noisy data}. We also include datasets with \underline{imbalanced classes}, where the class distribution is skewed, and \underline{high-dimensional} data. To test the model’s robustness to correlated features, we use \underline{multicollinear} data, while datasets with \underline{mixed feature types} challenge the models with both categorical and continuous variables. In addition, we incorporate \underline{hierarchical} structures, which introduce nested dependencies between features, and \underline{interaction effects}, where interactions influence the relationship between features and target variables. Finally, we simulate more specialized cases, including data generated from \underline{step function}s and \underline{periodic function}s, which reflect non-smooth and repeating patterns often seen in certain real-world applications. 

% This diverse set of data generation processes ensures that our evaluation comprehensively covers a wide range of conditions under which tree-based models may be applied. 

\subsection{Data generation processes}\label{Data generation details}
\begin{enumerate}
    \item Generate features: \( X_i \sim U(0, 1), i\in \{1,2,3,4,5\} \). 
\item \begin{itemize}
    \item 
    \textbf{Ball:} \( f(X) = \sum_{i=1}^3 X_i^2  \).
    
    \item 
    \textbf{Friedman \#1:}  $f(X) = 10 \sin(\pi X_1  X_2) + 20(X_3 - 0.5)^2 + 10 X_4 + 5 X_5$
    
    \item 
    \textbf{Friedman \#2:}
    \begin{itemize}
    \item Make transformations: \(Z_1 = 100 X_1, Z_2 = 40\pi + 520\pi X_2, Z_4 = 10X_4 + 1\)
    \item Generate responses: \( f(X) = \sqrt{Z_1^2 + (Z_2 X_3  - \frac{1}{Z_2 Z_4}) ^2 }.\)
    \end{itemize}
    
    \item 
    \textbf{Friedman \#3:}
    \begin{itemize}
    \item Make transformations: \(Z_1 = 100 X_1, Z_2 = 40\pi + 520\pi X_2, Z_4 = 10X_4 + 1\)
    \item Generate responses: \( f(X) = \arctan\left((Z_2X_3 - \frac{1}{Z_2 Z_4})/Z_1\right).\)
    \end{itemize}
    
    \item 
    \textbf{Poly \#1:} $f(X) = 4X_1 + 3X_2^2 + 2X_3^3 + X_4^4$
    
    \item 
    \textbf{Poly \#2:} \(f(X) = X_1^4 + 2X_2^3 + 3X_3^2 + 4X_1\).
    
    \item 
    \textbf{Ring:} \( f(X) = |\sum_{i=1}^3 X_i^2 - 1|\).
    
    \item 
    \textbf{Collinear:}    
    \begin{itemize}
    \item Create correlated features: \( X_{i+3} = X_i + 0.1 \cdot \mathcal{N}(0, 1) \), where \( i\in\{1,2,3\}\).
    \item Generate responses: \( f(X) = \sum_{i=1}^6 X_i \).
    \end{itemize}
\end{itemize}
\item Map it to probabilities: $\eta = \text{Sigmoid}\left(f(X)-E(f(X))\right)$. 
\item Generate labels: \( y \sim \text{Bernoulli}(\eta) \).
\end{enumerate}
    % \item 
    % \textbf{Non-linear Separable:}
    % \begin{itemize}
    % \item Generate features: \( X \sim \mathcal{N}(0, I_p) \), where \( X \) is a matrix of size \( n \times p \) with entries drawn from a standard normal distribution.
    % \item Calculate probabilities: \( \eta = \text{sigmoid}(\sum_{i=1}^p X_i^2 - p) \), where \( X_i \) denotes the \( i \)-th column of \( X \).
    % \end{itemize}
    
    % \item 
    % \textbf{Noisy Data:}
    % \begin{itemize}
    % \item Generate features: \( X \sim \mathcal{N}(0, I_p) \), where \( X \) is a matrix of size \( n \times p \) with entries drawn from a standard normal distribution.
    % \item Add noise: \( \epsilon \sim \mathcal{N}(0, 1) \), where \( \epsilon \) is a vector of size \( n \) with entries drawn from a standard normal distribution.
    % \item Calculate probabilities: \( \eta = \text{sigmoid}(\sum_{i=1}^p X_i + \epsilon) \), where \( X_i \) denotes the \( i \)-th column of \( X \).
    % \end{itemize}
    
    % \item 
    % \textbf{Imbalanced Classes:}
    % \begin{itemize}
    % \item Generate features: \( X \sim \mathcal{N}(0, I_p) \), where \( X \) is a matrix of size \( n \times p \) with entries drawn from a standard normal distribution.
    % \item Calculate probabilities: \( \eta =  \text{sigmoid}(3\sum_{i=1}^2 X_i - 2) \), where \( X_i \) denotes the \( i \)-th column of \( X \).
    % \end{itemize}
    
    % \item 
    % \textbf{High-Dimensional Data:}
    % \begin{itemize}
    % \item Generate features: \( X \sim \mathcal{N}(0, I_{10}) \), where \( X \) is a matrix of size \( n \times 10 \) with entries drawn from a standard normal distribution.
    % \item Calculate probabilities: \( \eta = \text{sigmoid}(\sum_{i=1}^{10} X_i) \), where \( X_i \) denotes the \( i \)-th column of \( X \).
    % \end{itemize}
    
    % \item 
    % \textbf{Multicollinear Data:}
    % \begin{itemize}
    % \item Generate base features: \( Z \sim \mathcal{N}(0, I_{p/2}) \), where \( Z \) is a matrix of size \( n \times p/2 \) with entries drawn from a standard normal distribution.
    % \item Create correlated features: \( X_1 = Z \) and \( X_2 = Z + 0.1 \cdot \mathcal{N}(0, I_{p/2}) \), where \( X_1 \) and \( X_2 \) are matrices of size \( n \times p/2 \).
    % \item Concatenate features: \( X = [X_1 | X_2] \), creating a matrix \( X \) of size \( n \times p \).
    % \item Calculate probabilities: \( \eta = \text{sigmoid}(\sum_{i=1}^{p/2} X_i) \), where \( X_i \) denotes the \( i \)-th column of \( X \).
    % \end{itemize}
    
    % \item 
    % \textbf{Mixed Types:}
    % \begin{itemize}
    % \item Generate continuous features: \( X_{\text{continuous}} \sim \mathcal{N}(0, I_{p/2}) \), where \( X_{\text{continuous}} \) is a matrix of size \( n \times p/2 \).
    % \item Generate discrete features: \( X_{\text{discrete}} \sim \text{Uniform} \{0,1,2\}) \), where \( X_{\text{discrete}} \) is a matrix of size \( n \times p/2 \) with values drawn from a discrete uniform distribution.
    % \item Concatenate features: \( X = [X_{\text{continuous}} | X_{\text{discrete}}] \), creating a matrix \( X \) of size \( n \times d \).
    % \item Calculate probabilities: \( \eta = \text{sigmoid}(\sum_{i=1}^{d/2} X_i + \sum_{i=d/2+1}^{d} \mathbbm{1}(X_i = 2)) \), where \( X_i \) denotes the \( i \)-th column of \( X \).
    % \end{itemize}
    
    % \item 
    % \textbf{Hierarchical Data:}
    % \begin{itemize}
    % \item Generate group and subgroup: \( \text{group}, \text{subgroup} \sim \text{Uniform}\{0,1,2\} \), where the values are drawn from a discrete uniform distribution.
    % \item Generate features: \( X \sim \mathcal{N}(0, I_{p-2}) \), where \( X \) is a matrix of size \( n \times (p-2) \).
    % \item Concatenate features: \( X = [\text{group}, \text{subgroup}, X] \), creating a matrix \( X \) of size \( n \times p \).
    % \item Calculate probabilities: \( \eta = \text{sigmoid}(X_1 + X_2 + \sum_{i=3}^{p} X_i) \), where \( X_i \) denotes the \( i \)-th column of \( X \).
    % \end{itemize}
    
    % \item 
    % \textbf{Interaction Effects:}
    % \begin{itemize}
    % \item Generate features: \( X \sim \mathcal{N}(0, I_p) \), where \( X \) is a matrix of size \( n \times p \) with entries drawn from a standard normal distribution.
    % \item Calculate probabilities: \( \eta = \text{sigmoid}(X_1 X_2 + X_3 X_4 + \sum_{i=5}^p X_i) \), where \( X_i \) denotes the \( i \)-th column of \( X \).
    % \end{itemize}
    
    % \item 
    % \textbf{Step Function:}
    % \begin{itemize}
    % \item Generate features: \( X \sim \mathcal{N}(0, I_p) \), where \( X \) is a matrix of size \( n \times p \).
    % \item Calculate probabilities: \( \eta = \mathbbm{1}(\sum_{i=1}^{p} X_i > 0) \), where \( X_i \) denotes the \( i \)-th column of \( X \).
    % \end{itemize}
    
    % \item 
    % \textbf{Periodic Function:}
    % \begin{itemize}
    % \item Generate features: \( X \sim \mathcal{N}(0, I_p) \), where \( X \) is a matrix of size \( n \times p \).
    % \item Calculate probabilities: \( \eta = \text{sigmoid}(\sin(\sum_{i=1}^{p/2} X_i) + \cos(\sum_{i=p/2+1}^{p} X_i)) \), where \( X_i \) denotes the \( i \)-th column of \( X \).
    % \end{itemize}


\subsection{CART, PFS, MDFS, wEFS, RF-CART, and RF-MDFS}\label{algor}
\begin{algorithm}
\caption{Calculate\_impurity}\text{This function calculates sample impurity $\hat{\mathcal{G}}^{CART}$.}\label{Calculate_impurity}
\begin{algorithmic}
    \STATE{\bfseries Input:} {$\bm{x}, \bm{y}, x$} 
    \STATE $\bm{y}_l \gets \bm{y}[\bm{x}\le x], \bm{y}_r \gets \bm{y}[\bm{x}>x]$ 
    \STATE $\mathcal{G} \gets \text{Var}(\bm{y}_l)\frac{|\bm{y}_l|}{n} + \text{Var}(\bm{y}_r)\frac{|\bm{y}_r|}{n}$
    \STATE{\bfseries Output:} {$\mathcal{G}$}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Calculate\_distance}\text{This function calculates the second term in \eqref{G_pfs} and $\hat{\mathcal{G}}^*$.}\label{Calculate_distance}
\begin{algorithmic}
    \STATE{\bfseries Input:} {$\bm{x}, \bm{y}, x, c$} 
    \STATE $\bm{y}_l \gets \bm{y}[\bm{x}\le x], \bm{y}_r \gets \bm{y}[\bm{x}>x]$ 
    \STATE $\mathcal{G} \gets (1-|c-\bar{y_l}|)\frac{|\bm{y}_l|}{n} + (1-|c-\bar{y_r}|)\frac{|\bm{y}_r|}{n}$
    \STATE{\bfseries Output:} {$\mathcal{G}$}
\end{algorithmic}
\end{algorithm}

\begin{algorithm} 
\caption{Calculate\_weighted\_risk}\text{This function calculates sample loss function of wEFS.}\label{Calculate_risk} 
\begin{algorithmic}
    \STATE{\bfseries Input:}{$\bm{x}, \bm{y}, x, c$} 
    \STATE $\bm{y}_l \gets \bm{y}[\bm{x}\le x], \bm{y}_r \gets \bm{y}[\bm{x}>x]$ 
    \STATE $r_l \gets |\bm{y}_l[\bm{y}_l > c]|, r_r \gets |\bm{y}_r[\bm{y}_r > c]|$ 
    \STATE $\mathcal{R} \gets (\mathbbm{1}\{\bar{y}_l>c\}(|\bm{y}_l| - r_l) + \mathbbm{1}\{\bar{y}_r>c\}(|\bm{y}_r| - r_r))c + (\mathbbm{1}\{\bar{y}_l\le c\} r_l + \mathbbm{1}\{\bar{y}_r\le c\}r_r)(1-c)$ 
    \STATE{\bfseries Output:} {$\mathcal{R}$}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{FindBestSplit}\text{This function takes in data from the parent node and outputs the best feature, split point in terms of $\mathcal{G}^{CART}$.}\label{best_split}\vspace{-5mm}
\begin{algorithmic}
    \STATE{\bfseries Input:}{$X, \bm{y}$} 
    \STATE best\_impurity, best\_split, best\_feature $\gets$ 0, None, None
    \FOR{each feature $\bm{x}_i$ in $X$, $i$ from $1$ to $p$}
        \STATE sort $X$ in terms of $\bm{x}_i$
        \FOR{$x$ in ordered samples in $\bm{x}_i$}
            \STATE impurity $\gets$ Calculate\_impurity($\bm{x}_i, \bm{y}, x$) 
            \IF{impurity $<$ best\_impurity}
                \STATE best\_impurity $\gets$ impurity
                \STATE best\_split $\gets$ $x$
                \STATE best\_feature $\gets$ $i$
            \ENDIF
        \ENDFOR
    \ENDFOR
    \STATE{\bfseries Output:}{best\_split, best\_feature}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Find\_PFS\_BestSplit}
\text{This function takes in data from the parent node and outputs the best feature, split point in terms of $\mathcal{G}^{PFS}(\lambda)$.}\vspace{-5mm}
\label{best_PFS_split}
\begin{algorithmic}
    \STATE{\bfseries Input:}{$\bm{x}, \bm{y}, c, \lambda$}
    \STATE best\_G\_PFS, best\_split $\gets$ 0, None
    \STATE sort $\bm{x}$
    \FOR{$x$ in sorted $\bm{x}$}
        \STATE G\_PFS $\gets$ $(1-\lambda)$ Calculate\_impurity($\bm{x}, \bm{y}, x$) + $\lambda$ Calculate\_distance($\bm{x}, \bm{y}, x, c$)
        \IF{G\_PFS $<$ best\_G\_PFS}
            \STATE best\_G\_PFS $\gets$ G\_PFS
            \STATE best\_split $\gets$ $x$
        \ENDIF
    \ENDFOR
    \STATE{\bfseries Output:}{best\_split}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Find\_wEFS\_BestSplit}
\text{This function takes in data from the parent node and outputs the best feature, split point in terms of the weighted empirical risk.}\vspace{-5mm}\label{best_ERMFS_split}
\begin{algorithmic}
    \STATE{\bfseries Input:}{$\bm{x}, \bm{y}, c$}
    \STATE best\_risk, best\_split $\gets$ 0, None
    \STATE sort $\bm{x}$
    \FOR{$x$ in sorted $\bm{x}$}
        \STATE risk $\gets$ Calculate\_weighted\_risk($\bm{x}, \bm{y}, x, c$)
        \IF{risk $<$ best\_risk}
            \STATE best\_risk $\gets$ risk
            \STATE best\_split $\gets$ $x$
        \ENDIF
    \ENDFOR
    \STATE{\bfseries Output:}{best\_split}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Grow\_Tree}
\text{Fit the tree-based model to $(X,\bm{y})$ when $\eta$ or $\hat{\eta}$ is not provided.}
\begin{algorithmic}
    \STATE{\bfseries Input:}{$X, \bm{y}, c,$ current\_depth, method, depth, min\_samples}
    \IF{current\_depth = depth \textbf{or} $n(\text{unique}(\bm{y}))=1$ \textbf{or} $n(\bm{y})<$min\_samples}
        \STATE{\bfseries Output:}{mean($\bm{y}$)}
    \ENDIF
    \STATE best\_feature, best\_split $\gets$ FindBestSplit($X,\bm{y}$)
    \STATE mask $\gets (\bm{x}_{\text{best feature}}< \text{best\_split})$
    \STATE left\_X, right\_X $\gets X[\text{mask}], X[!\text{mask}]$
    \STATE left\_y, right\_y $\gets \bm{y}[\text{mask}], \bm{y}[!\text{mask}]$
    
    \IF{current\_depth = depth - 1 \textbf{or} $\min\{n(\text{left\_y}), n(\text{right\_y)}|\}<$min\_samples \textbf{or} method!=`CART'}
        \IF{method = `MDFS'}
            \STATE new\_best\_split $\gets$ Find\_PFS\_BestSplit($\bm{x}_{\text{best\_feature}}, \bm{y}, c, 1$)
        \ELSIF{method = `PFS'}
            \STATE new\_best\_split $\gets$ Find\_PFS\_BestSplit($\bm{x}_{\text{best\_feature}}, \bm{y}, c, 0.1$)
        \ELSIF{method = `wEFS'}
            \STATE new\_best\_split $\gets$ Find\_wEFS\_BestSplit($\bm{x}_{\text{best\_feature}}, \bm{y}, c$)
        \ENDIF
        \STATE mask $\gets (\bm{x}_{\text{best\_feature}}< \text{new\_best\_split})$
        \STATE left\_tree, right\_tree $\gets$ mean($\bm{y}[\text{mask}]$), mean($\bm{y}[\text{!mask}]$)
        \STATE{\bfseries Output:}{best\_feature, new\_best\_split, left\_tree, right\_tree}
    \ELSE
        \STATE left\_tree = Grow\_Tree(left\_X, left\_y, current\_depth + 1, method, depth, min\_samples)
        \STATE right\_tree = Grow\_Tree(right\_X, right\_y, current\_depth + 1, method, depth, min\_samples)
        \STATE{\bfseries Output:}{best\_feature, best\_split, left\_tree, right\_tree}
    \ENDIF
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htb]
\caption{Grow\_Tree\_wP}
\text{Fit the tree-based model to $(X,\bm{y})$ and $\hat{\eta}$ provided by Knowledge-Distillation.}
\begin{algorithmic}
    \STATE{\bfseries Input:}{$X, \bm{y}, \bm{p}, c,$ current\_depth, method, depth, min\_samples}
    \IF{current\_depth = depth \textbf{or} $(\min(\bm{p}) > c\ \textbf{or}\ \max(\bm{p}) < c)$ \textbf{or} $n(\bm{y})<$min\_samples}
        \STATE{\bfseries Output:}{mean($\bm{p}$)}
    \ENDIF
    \STATE best\_feature, best\_split $\gets$ FindBestSplit($X,\bm{p}$)
    \STATE mask $\gets (\bm{x}_{\text{best feature}}< \text{best\_split})$
    \STATE left\_X, right\_X $\gets X[\text{mask}], X[!\text{mask}]$
    \STATE left\_y, right\_y $\gets \bm{y}[\text{mask}], \bm{y}[!\text{mask}]$
    \STATE left\_p, right\_p $\gets \bm{p}[\text{mask}], \bm{p}[!\text{mask}]$
    
    \IF{current\_depth = depth - 1 \textbf{or} $\min\{n(\text{left\_p}), n(\text{right\_p)}|\}<$min\_samples \textbf{or} method!=`CART'}
        \IF{method = `MDFS'}
            \STATE new\_best\_split $\gets$ Find\_PFS\_BestSplit($\bm{x}_{\text{best\_feature}}, \bm{y}, c, 1$)
        \ELSIF{method = `PFS'}
            \STATE new\_best\_split $\gets$ Find\_PFS\_BestSplit($\bm{x}_{\text{best\_feature}}, \bm{y}, c, 0.1$)
        \ELSIF{method = `wEFS'}
            \STATE new\_best\_split $\gets$ Find\_wEFS\_BestSplit($\bm{x}_{\text{best\_feature}}, \bm{p}, c$)
        \ENDIF
        \STATE mask $\gets (\bm{x}_{\text{best\_feature}}< \text{new\_best\_split})$
        \STATE left\_tree, right\_tree $\gets$ mean($\bm{p}[\text{mask}]$), mean($\bm{p}[\text{!mask}]$)
        \STATE{\bfseries Output:}{best\_feature, new\_best\_split, left\_tree, right\_tree}
    \ELSE
        \STATE left\_tree = Grow\_Tree\_P(left\_X, left\_y, left\_p, current\_depth + 1, method, depth, min\_samples)
        \STATE right\_tree = Grow\_Tree\_P(right\_X, right\_y, right\_p, current\_depth + 1, method, depth, min\_samples)
        \STATE{\bfseries Output:}{best\_feature, best\_split, left\_tree, right\_tree}
    \ENDIF
\end{algorithmic}
\end{algorithm}

% \begin{algorithm}
% \caption{CART - EFS - MDFS - PFS}
% \label{algor_pfs}
% \begin{algorithmic}[1] 
% \Procedure{BuildTree}{$X, y, \text{min\_samples}, \text{max\_depth}$, $\text{method} \in \{\text{CART}, \text{PFS}, \text{MDFS}, \text{EFS}\}$}
%     \If{$|y| < \text{min\_samples}$ \textbf{or} $\text{depth} = \text{max\_depth}$}
%         \State \textbf{return} Leaf(mode($y$))
%     \EndIf
%     \State best\_split $\gets$ FindBestSplit($X, y, \text{depth}, \text{method}$)
%     \If{best\_split is None}
%         \State \textbf{return} Leaf(mode($y$))
%     \EndIf
%     \State left\_tree $\gets$ BuildTree($X_{\text{left}}, y_{\text{left}}, \text{min\_samples}, \text{max\_depth}$)
%     \State right\_tree $\gets$ BuildTree($X_{\text{right}}, y_{\text{right}}, \text{min\_sample}, \text{max\_depth}$)
%     \State \textbf{return} Node(best\_split, left\_tree, right\_tree)
% \EndProcedure

% \Procedure{FindBestSplit}{$X, y, \text{depth}, \text{method}, m$}
%     \State best\_gain $\gets$ 0
%     \State best\_split $\gets$ None
%     \State $\text{is\_final}$ $\gets$ $\text{depth} = \text{max\_depth}-1$
%     \For{each feature $f$ in $X$}
%         \If {$\text{method} = \text{EFS}\ \& \ \text{is\_final}$}
%             \State $\hat{\eta}$ $\gets$ empirical conditional distribution of $\eta$ using $m$ bins
%             \State $v$ $\gets$ $\argmin_s |\hat{\eta}(s)-c|$
%             \State left\_y, right\_y $\gets$ Split($y, f, v$)
%             \State gain $\gets$ CalculateGain($y$, left\_y, right\_y)
%             \If{gain $>$ best\_gain}
%                 \State best\_gain $\gets$ gain
%                 \State best\_split $\gets$ $(f, v)$
%             \EndIf
%         \Else
%         \For{$v$ in $m$ random samples from $f$}
%             \State left\_y, right\_y $\gets$ Split($y, f, v$)
%             \State gain $\gets$ CalculateGain($y$, left\_y, right\_y, $\text{is\_final}$)
%             \If{gain $>$ best\_gain}
%                 \State best\_gain $\gets$ gain
%                 \State best\_split $\gets$ $(f, v)$
%             \EndIf
%         \EndFor
%         \EndIf
%     \EndFor
%     \State \textbf{return} best\_split
% \EndProcedure



% \begin{algorithm}
% \caption{Information Gain Computation}
% \label{algor_calgain}
% \begin{algorithmic}[1] 
% \Procedure{CalculateGain}{$y, \text{left\_y}, \text{right\_y}, \lambda, c$}
%     \State $prop$ $\gets$ $|\text{left\_y}| / |y|$
%     \State imp\_y $\gets$ CalculateImpurity($y, \lambda, c$)
%     \State imp\_y\_left $\gets$ CalculateImpurity($\text{left\_y}, \lambda, c$), imp\_y\_right $\gets$ CalculateImpurity($\text{right\_y}, \lambda, c$),
%     \State \textbf{return} $\text{imp\_y} - prop \times \text{imp\_y\_left} - (1-prop) \times \text{imp\_y\_right}$
% \EndProcedure
% \Procedure{CalculateImpurity}{$y, \lambda, c$}
%     \State $imp$ $\gets$ Gini(y), $\Bar{y}$ $\gets$ mean(y)
%     \State \textbf{return} $(1-\lambda)\times imp + \lambda\times (1-|\Bar{y} - c|)$
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}
% \caption{Choosing $\lambda$ for PFS}
% \label{algor_cv}
% \begin{algorithmic}[1] 
% \Procedure{CV-PFS}{$X, y, c, \text{min\_samples}, \text{max\_depth}, \lambda\_\text{candidate} = \{0.1,0.2,\dots, 1\}$}
% \State Random split data into 5 folds $\{(X^{1}, y^{1}), \dots, (X^{5}, y^{5})\}$
% \For{$\lambda$ in $\lambda\_\text{candidate}$}
%     \For{$i$ in $1:5$}
%         \State Tree[i] $\gets$ BuildTree($X^{-i}, y^{-i}, c, \lambda = \lambda, \text{min\_samples}, \text{max\_depth}, \text{method}=\text{PFS}$)
%         \State Obtain honest estimate of each leaf node in Tree[i] using $(X^i, y^i)$
%         \State Calculate $F_1$ score of training samples based on the honest estimate 
%     \EndFor
%     \State Record the mean of $F_1$ scores for each $\lambda$
% \EndFor
% \State \textbf{return} $\lambda$ with largest mean $F_1$ score
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}
% \caption{Tree splitting}
% \label{algor_treesplit}
% \begin{algorithmic}[1] 
% \Procedure{BuildTree}{$X, y, c, \lambda, \text{min\_samples}, \text{max\_depth}, \text{method} \in \{\text{CART}, \text{PFS}, \text{EFS}\}$}
%     \If{$|y| < \text{min\_samples}$ \textbf{or} $\text{depth} = \text{max\_depth}$ \textbf{or} $Var(y) = 0$ }
%         \State \textbf{return} Leaf(mean($y$))
%     \EndIf
%     \State best\_split $\gets$ FindBestSplit($X, y, c, \lambda, \text{depth}, \text{method}$)
%     \If{best\_split is None}
%         \State \textbf{return} Leaf(mean($y$))
%     \EndIf
%     \State left\_tree $\gets$ BuildTree($X_{\text{left}}, y_{\text{left}}, c, \lambda, \text{min\_samples}, \text{max\_depth}, \text{method}$)
%     \State right\_tree $\gets$ BuildTree($X_{\text{right}}, y_{\text{right}}, c, \lambda, \text{min\_samples}, \text{max\_depth}, \text{method}$)
%     \State \textbf{return} Node(best\_split, left\_tree, right\_tree)
% \EndProcedure

% \Procedure{FindBestSplit}{$X, y, c,\lambda,  \text{depth}, \text{method}$}
%     \State best\_gain $\gets$ 0, best\_split $\gets$ None
%     % \State $\text{is\_final}$ $\gets$ $\text{depth} = \text{max\_depth}-1$
%     \For{each col $f$ in $X$}
%         \If{$\text{number of unique values in}\ f < 20$}
%             \State thresholds $\gets (f_{\text{unique}}[:-1] + f_{\text{unique}}[1:]) / 2$
%         \Else
%             \State thresholds $\gets$ quantiles of $f$ with 20 evenly spaced intervals
%         \EndIf
%         % \If{$\text{method} = \text{e} \& \ \text{is\_final}$}
%         %     \State $v \gets \text{FindClosestX}(f, y, \text{cut})$
%         %     \State left\_y, right\_y $\gets$ Split($y, f, v$)
%         %     \State gain $\gets$ CalculateGain($y$, left\_y, right\_y)
%         %     \If{gain $>$ best\_gain}
%         %         \State best\_gain $\gets$ gain
%         %         \State best\_split $\gets$ $(f, v)$
%         %     \EndIf
%         % \Else
%         \For{$v$ in thresholds}
%             \State left\_y, right\_y $\gets$ Split($y, f, v$)
%             \State gain $\gets$ CalculateGain($y$, left\_y, right\_y, $\lambda = 0$, $c$)
%             \If{gain $>$ best\_gain}
%                 \State best\_gain $\gets$ gain
%                 \State best\_split $\gets$ $(f, v)$
%             \EndIf
%         \EndFor
%         % \EndIf
%     \EndFor
%     \If{best\_split is None \textbf{or} method = `CART' \textbf{or} depth $\ne$ max\_depth}
%         \State \textbf{return} best\_split 
%     \EndIf
%     \State $f\_chosen$ $\gets$ $f$ in best\_split
%     \If{method = `EFS'}
%         \State \textbf{return} EFS-FindBestSplit($f\_chosen, y$)
%     \EndIf
%     \State $\lambda = 1$ if method = `MDFS' 
%     \State best\_gain $\gets$ 0, best\_split $\gets$ None
%     \For{$v$ in thresholds for $f\_chosen$}
%         \State left\_y, right\_y $\gets$ Split($y, f\_chosen, v$)
%         \State gain $\gets$ CalculateGain($y$, left\_y, right\_y, $\lambda$, $c$)
%         \If{gain $>$ best\_gain \textbf{and} (left\_y - $c$)(right\_y - $c)<0$}
%             \State best\_gain $\gets$ gain
%             \State best\_split $\gets$ $(f\_chosen, v)$
%         \EndIf
%     \EndFor
%     \State \textbf{return} best\_split
% \EndProcedure
% \Procedure{EFS-FindBestSplit}{$f, y$}
%     \State Split data into 20 bins based on the range of $f$ and obtain the mean of $y$ for each bin
%     \State \textbf{return} Midpoint of the bin such that its corresponding mean $y$ is closest to $c$
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}

% \Procedure{CalculateGain}{$y, \text{left\_y}, \text{right\_y}, \text{is\_final}$}
%     \State $p \gets |\text{left\_y}|/|y|$
%     \If {$\text{method} = \text{p} \& \ \text{is\_final}$}
%         \State gain $\gets G^*(y, \lambda) - p \cdot G^*$(left\_y, $\lambda$) $- (1-p) \cdot G^*$(right\_y, $\lambda$)
%     \Else
%         \State gain $\gets G(y) - p \cdot G$(left\_y) $- (1-p) \cdot G$(right\_y)
%     \EndIf
%     \State \textbf{return} gain
% \EndProcedure
\clearpage
\subsection{Computational Complexity Analysis} \label{Appendix: Computational Complexity}


Since our proposed algorithm only operates at the last split, the tree-building processes are identical in upper levels among 4 strategies. Next, we evaluate the computational complexity of the \textbf{final split}, i.e., split at the final level. 

Suppose the sample size in a node at the second to last level is $n$ and the number of features is $p$. In \textbf{CART}, the dominant step is to sort the samples based on each feature. With $p$ features to consider, the computational complexity is $O(pNlogN)$. \textbf{PFS}, \textbf{MDFS}, and \textbf{wEFS} all determine the feature to split on using \textbf{CART}. The split point decision requires another $O(N)$ of time which is still dominated by the sorting process. 

% For $\textbf{PFS}$, apart from the split decision part, a $\lambda$ value needs to be chosen beforehand. Suppose, we do $K$-fold splits and consider $L$ candidate $\lambda$ values. We need to fit $K$ trees separately each with one fold excluded. For each tree, we try determining splits using all $L$ candidate $\lambda$ values. Therefore, the computational complexity of $\textbf{PFS}$ is roughly $O(KLpNlogN)$. Since $K$ and $L$ are all fixed constants, all our proposed strategies have the same computational complexity as the classic CART. 

% \subsection{Simulation Results}\label{Appendix: Simu_result}
% \begin{table}[htb]
%     \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
% \multicolumn{3}{c}{\textbf{Hierarchical}} & \multicolumn{4}{c}{\textbf{Misclassification Error (\%)}} & \multicolumn{4}{c}{$F_1$\ \textbf{Score} (\%)} \\
% \cmidrule(lr){4-7} \cmidrule(lr){8-11}
%  $p$ & $c$ & $d$ & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS}  \\
% \midrule
% 4 & 0.9 & 2 & \begin{tabular}[c]{@{}c@{}}0.99\\ (0.21)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}0.98\\ (0.20)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.99\\ (0.20)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.99\\ (0.20)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.20\\ (1.94)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}1.37\\ (6.06)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.80\\ (4.79)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.80 \\ (4.79)\end{tabular} \\
% 4 & 0.9 & 3 & \begin{tabular}[c]{@{}c@{}}1.03\\ (0.33)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}1.0\\ (0.22)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1.05\\ (0.33)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1.05\\ (0.34)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.99\\ (5.09)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.37\\ (10.86)\end{tabular} & \begin{tabular}[c]{@{}c@{}}6.92\\ (14.84)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}6.93\\ (14.87)\end{tabular} \\
% 4 & 0.9 & 4 & \begin{tabular}[c]{@{}c@{}}1.18\\ (0.48)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}1.06\\ (0.35)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1.18\\ (0.48)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1.24\\ (0.51)\end{tabular} & \begin{tabular}[c]{@{}c@{}}7.64\\ (14.83)\end{tabular} & \begin{tabular}[c]{@{}c@{}}6.91\\ (15.52)\end{tabular} & \begin{tabular}[c]{@{}c@{}}15.42\\ (21.08)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}16.54\\ (20.54)\end{tabular} \\
% 4 & 0.8 & 2 & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}3.13\\ (0.38)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.15\\ (0.38)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.15\\ (0.38)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.15\\ (0.38)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.22\\ (2.14)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}3.85\\ (9.15)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.29\\ (8.39)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.48\\ (9.06)\end{tabular} \\
% 4 & 0.8 & 3 & \begin{tabular}[c]{@{}c@{}}3.2\\ (0.47)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}3.14\\ (0.42)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.16\\ (0.45)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.19\\ (0.47)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.7\\ (14.51)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.17\\ (18.01)\end{tabular} & \begin{tabular}[c]{@{}c@{}}14.19\\ (18.6)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}15.13\\ (19.66)\end{tabular} \\
% 4 & 0.8 & 4 & \begin{tabular}[c]{@{}c@{}}3.25\\ (0.59)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}2.99\\ (0.56)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.08\\ (0.63)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.21\\ (0.66)\end{tabular} & \begin{tabular}[c]{@{}c@{}}26.4\\ (22.82)\end{tabular} & \begin{tabular}[c]{@{}c@{}}34.86\\ (21.4)\end{tabular} & \begin{tabular}[c]{@{}c@{}}39.32\\ (18.2)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}39.49\\ (18.31)\end{tabular} \\
% 4 & 0.7 & 2 & \begin{tabular}[c]{@{}c@{}}5.99\\ (0.53)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.92\\ (0.52)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}5.91\\ (0.52)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.96\\ (0.52)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.03\\ (8.23)\end{tabular} & \begin{tabular}[c]{@{}c@{}}8.8\\ (12.78)\end{tabular} & \begin{tabular}[c]{@{}c@{}}10.55\\ (13.01)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}11.52\\ (14.33)\end{tabular} \\
% 4 & 0.7 & 3 & \begin{tabular}[c]{@{}c@{}}5.93\\ (0.66)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.64\\ (0.67)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}5.61\\ (0.68)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.71\\ (0.69)\end{tabular} & \begin{tabular}[c]{@{}c@{}}21.41\\ (21.77)\end{tabular} & \begin{tabular}[c]{@{}c@{}}30.77\\ (18.51)\end{tabular} & \begin{tabular}[c]{@{}c@{}}33.6\\ (16.37)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}34.71\\ (16.8)\end{tabular} \\
% 4 & 0.7 & 4 & \begin{tabular}[c]{@{}c@{}}5.39\\ (0.73)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.11\\ (0.88)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}5.09\\ (0.72)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.31\\ (0.85)\end{tabular} & \begin{tabular}[c]{@{}c@{}}45.14\\ (14.65)\end{tabular} & \begin{tabular}[c]{@{}c@{}}44.77\\ (12.84)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}49.49\\ (10.12)\end{tabular} & \begin{tabular}[c]{@{}c@{}}49.42\\ (10.16)\end{tabular} \\
% \bottomrule
% \end{tabular}
% \end{table}





% \begin{table}[htb]
%     \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
% \multicolumn{3}{c}{\textbf{High Dimen}} & \multicolumn{4}{c}{\textbf{Misclassification Error (\%)}} & \multicolumn{4}{c}{$F_1$\ \textbf{Score} (\%)} \\
% \cmidrule(lr){4-7} \cmidrule(lr){8-11}
%  $p$ & $c$ & $d$ & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS}  \\
% \midrule
% 10 & 0.9 & 3 & \begin{tabular}[c]{@{}c@{}}24.5\\ (0.86)\end{tabular} & \begin{tabular}[c]{@{}c@{}}24.45\\ (0.85)\end{tabular} & \begin{tabular}[c]{@{}c@{}}24.44\\ (0.86)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}24.44\\ (0.86)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1.11\\ (2.6)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1.34\\ (3.57)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}1.34\\ (3.57)\end{tabular} \\
% 10 & 0.9 & 4 & \begin{tabular}[c]{@{}c@{}}24.44\\ (0.89)\end{tabular} & \begin{tabular}[c]{@{}c@{}}24.21\\ (0.95)\end{tabular} & \begin{tabular}[c]{@{}c@{}}24.05\\ (0.92)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}24.01\\ (0.94)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1.48\\ (5.62)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.03\\ (6.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}8.37\\ (7.12)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}9.77\\ (8.45)\end{tabular} \\
% 10 & 0.9 & 5 & \begin{tabular}[c]{@{}c@{}}23.98\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}23.75\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}23.32\\ (0.97)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}23.28\\ (0.96)\end{tabular} & \begin{tabular}[c]{@{}c@{}}10.57\\ (13.28)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.02\\ (9.58)\end{tabular} & \begin{tabular}[c]{@{}c@{}}19.46\\ (9.84)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}21.4\\ (10.03)\end{tabular} \\
% 10 & 0.8 & 3 & \begin{tabular}[c]{@{}c@{}}33.17\\ (1.03)\end{tabular} & \begin{tabular}[c]{@{}c@{}}32.75\\ (1.06)\end{tabular} & \begin{tabular}[c]{@{}c@{}}32.02\\ (1.04)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}31.8\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}21.17\\ (14.08)\end{tabular} & \begin{tabular}[c]{@{}c@{}}16.83\\ (13.77)\end{tabular} & \begin{tabular}[c]{@{}c@{}}25.78\\ (11.42)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}26.83\\ (11.64)\end{tabular} \\
% 10 & 0.8 & 4 & \begin{tabular}[c]{@{}c@{}}31.26\\ (1.73)\end{tabular} & \begin{tabular}[c]{@{}c@{}}31.36\\ (1.16)\end{tabular} & \begin{tabular}[c]{@{}c@{}}30.63\\ (1.17)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}30.43\\ (1.15)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.04\\ (17.66)\end{tabular} & \begin{tabular}[c]{@{}c@{}}19.34\\ (10.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}26.47\\ (7.90)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}29.15\\ (8.02)\end{tabular} \\
% 10 & 0.8 & 5 & \begin{tabular}[c]{@{}c@{}}29.66\\ (1.64)\end{tabular} & \begin{tabular}[c]{@{}c@{}}29.66\\ (1.46)\end{tabular} & \begin{tabular}[c]{@{}c@{}}28.89\\ (1.21)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}28.83\\ (1.23)\end{tabular} & \begin{tabular}[c]{@{}c@{}}35.75\\ (11.63)\end{tabular} & \begin{tabular}[c]{@{}c@{}}32.17\\ (10.39)\end{tabular} & \begin{tabular}[c]{@{}c@{}}38.57\\ (6.68)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}40.22\\ (6.94)\end{tabular} \\
% 10 & 0.7 & 3 & \begin{tabular}[c]{@{}c@{}}36.11\\ (2.32)\end{tabular} & \begin{tabular}[c]{@{}c@{}}36.42\\ (1.42)\end{tabular} & \begin{tabular}[c]{@{}c@{}}35.49\\ (1.27)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}35.19\\ (1.21)\end{tabular} & \begin{tabular}[c]{@{}c@{}}33.25\\ (20.25)\end{tabular} & \begin{tabular}[c]{@{}c@{}}26.43\\ (10.54)\end{tabular} & \begin{tabular}[c]{@{}c@{}}33.91\\ (7.62)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}38.6\\ (8.07)\end{tabular} \\
% 10 & 0.7 & 4 & \begin{tabular}[c]{@{}c@{}}34.01\\ (1.34)\end{tabular} & \begin{tabular}[c]{@{}c@{}}33.85\\ (1.66)\end{tabular} & \begin{tabular}[c]{@{}c@{}}33.19\\ (1.26)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}33.15\\ (1.28)\end{tabular} & \begin{tabular}[c]{@{}c@{}}41.15\\ (8.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}40.19\\ (9.33)\end{tabular} & \begin{tabular}[c]{@{}c@{}}44.66\\ (5.35)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}45.36\\ (5.31)\end{tabular} \\
% 10 & 0.7 & 5 & \begin{tabular}[c]{@{}c@{}}31.93\\ (1.47)\end{tabular} & \begin{tabular}[c]{@{}c@{}}31.81\\ (1.43)\end{tabular} & \begin{tabular}[c]{@{}c@{}}31.18\\ (1.12)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}31.17\\ (1.08)\end{tabular} & \begin{tabular}[c]{@{}c@{}}51.44\\ (6.2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}48.72\\ (5.57)\end{tabular} & \begin{tabular}[c]{@{}c@{}}52.47\\ (3.44)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}53.3\\ (3.7)\end{tabular} \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[htb]
%     \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
% \multicolumn{3}{c}{\textbf{Imbalance}} & \multicolumn{4}{c}{\textbf{Misclassification Error (\%)}} & \multicolumn{4}{c}{$F_1$\ \textbf{Score} (\%)} \\
% \cmidrule(lr){4-7} \cmidrule(lr){8-11}
%  $p$ & $c$ & $d$ & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS}  \\
% \midrule
% 4 & 0.9 & 3 & \begin{tabular}[c]{@{}c@{}}18.91\\ (4.52)\end{tabular} & \begin{tabular}[c]{@{}c@{}}28.93\\ (12.07)\end{tabular} & \begin{tabular}[c]{@{}c@{}}15.59\\ (1.58)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}15.51\\ (1.55)\end{tabular} & \begin{tabular}[c]{@{}c@{}}79.64\\ (11.46)\end{tabular} & \begin{tabular}[c]{@{}c@{}}53.08\\ (27.67)\end{tabular} & \begin{tabular}[c]{@{}c@{}}82.08\\ (2.43)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}82.8\\ (2.28)\end{tabular} \\
% 4 & 0.9 & 4 & \begin{tabular}[c]{@{}c@{}}17.32\\ (1.69)\end{tabular} & \begin{tabular}[c]{@{}c@{}}17.28\\ (2.95)\end{tabular} & \begin{tabular}[c]{@{}c@{}}15.15\\ (1.79)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}15.01\\ (1.75)\end{tabular} & \begin{tabular}[c]{@{}c@{}}80.02\\ (2.74)\end{tabular} & \begin{tabular}[c]{@{}c@{}}79.72\\ (5.03)\end{tabular} & \begin{tabular}[c]{@{}c@{}}83.14\\ (2.51)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}83.44\\ (2.43)\end{tabular} \\
% 4 & 0.9 & 5 & \begin{tabular}[c]{@{}c@{}}11.39\\ (2.41)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.35\\ (2.49)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}9.31\\ (1.67)\end{tabular} & \begin{tabular}[c]{@{}c@{}}9.5\\ (1.68)\end{tabular} & \begin{tabular}[c]{@{}c@{}}88.22\\ (2.86)\end{tabular} & \begin{tabular}[c]{@{}c@{}}87.8\\ (3.18)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}90.3\\ (1.87)\end{tabular} & \begin{tabular}[c]{@{}c@{}}90.18\\ (1.89)\end{tabular} \\
% 4 & 0.8 & 3 & \begin{tabular}[c]{@{}c@{}}17.9\\ (1.77)\end{tabular} & \begin{tabular}[c]{@{}c@{}}17.72\\ (2.67)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}14.11\\ (1.82)\end{tabular} & \begin{tabular}[c]{@{}c@{}}14.21\\ (1.78)\end{tabular} & \begin{tabular}[c]{@{}c@{}}81.8\\ (2.54)\end{tabular} & \begin{tabular}[c]{@{}c@{}}80.93\\ (4.19)\end{tabular} & \begin{tabular}[c]{@{}c@{}}84.27\\ (2.45)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}84.53\\ (2.53)\end{tabular} \\
% 4 & 0.8 & 4 & \begin{tabular}[c]{@{}c@{}}13.92\\ (2.55)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.84\\ (2.25)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}12.04\\ (1.5)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.04\\ (1.51)\end{tabular} & \begin{tabular}[c]{@{}c@{}}87.35\\ (3.03)\end{tabular} & \begin{tabular}[c]{@{}c@{}}88.22\\ (2.52)\end{tabular} & \begin{tabular}[c]{@{}c@{}}89.05\\ (1.56)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}89.14\\ (1.56)\end{tabular} \\
% 4 & 0.8 & 5 & \begin{tabular}[c]{@{}c@{}}8.21\\ (1.64)\end{tabular} & \begin{tabular}[c]{@{}c@{}}8.61\\ (1.86)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}7.31\\ (1.23)\end{tabular} & \begin{tabular}[c]{@{}c@{}}7.44\\ (1.33)\end{tabular} & \begin{tabular}[c]{@{}c@{}}92.72\\ (1.45)\end{tabular} & \begin{tabular}[c]{@{}c@{}}92.18\\ (1.84)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}93.42\\ (1.14)\end{tabular} & \begin{tabular}[c]{@{}c@{}}93.34\\ (1.2)\end{tabular} \\
% 4 & 0.7 & 3 & \begin{tabular}[c]{@{}c@{}}14.97\\ (2.41)\end{tabular} & \begin{tabular}[c]{@{}c@{}}14.72\\ (1.91)\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.3\\ (1.08)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}13.27\\ (1.08)\end{tabular} & \begin{tabular}[c]{@{}c@{}}87.47\\ (2.71)\end{tabular} & \begin{tabular}[c]{@{}c@{}}87.12\\ (2.22)\end{tabular} & \begin{tabular}[c]{@{}c@{}}88.77\\ (1.05)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}89.06\\ (1.04)\end{tabular} \\
% 4 & 0.7 & 4 & \begin{tabular}[c]{@{}c@{}}10.33\\ (1.53)\end{tabular} & \begin{tabular}[c]{@{}c@{}}10.58\\ (1.93)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}9.82\\ (1.24)\end{tabular} & \begin{tabular}[c]{@{}c@{}}9.86\\ (1.28)\end{tabular} & \begin{tabular}[c]{@{}c@{}}91.81\\ (1.23)\end{tabular} & \begin{tabular}[c]{@{}c@{}}91.32\\ (1.79)\end{tabular} & \begin{tabular}[c]{@{}c@{}}92.02\\ (1.03)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}92.06\\ (1.05)\end{tabular} \\
% 4 & 0.7 & 5 & \begin{tabular}[c]{@{}c@{}}7.30\\ (1.26)\end{tabular} & \begin{tabular}[c]{@{}c@{}}7.77\\ (1.46)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}6.63\\ (1.05)\end{tabular} & \begin{tabular}[c]{@{}c@{}}6.76\\ (1.16)\end{tabular} & \begin{tabular}[c]{@{}c@{}}93.97\\ (1.09)\end{tabular} & \begin{tabular}[c]{@{}c@{}}93.56\\ (1.23)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}94.52\\ (0.9)\end{tabular} & \begin{tabular}[c]{@{}c@{}}94.42\\ (0.98)\end{tabular} \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[htb]
%     \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
% \multicolumn{3}{c}{\textbf{Interaction}} & \multicolumn{4}{c}{\textbf{Misclassification Error (\%)}} & \multicolumn{4}{c}{$F_1$\ \textbf{Score} (\%)} \\
% \cmidrule(lr){4-7} \cmidrule(lr){8-11}
%  $p$ & $c$ & $d$ & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS}  \\
% \midrule
% 4 & 0.9 & 3 & \begin{tabular}[c]{@{}c@{}}5.54\\ (0.58)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.48\\ (0.61)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}5.47\\ (0.61)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.51\\ (0.62)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.76\\ (4.57)\end{tabular} & \begin{tabular}[c]{@{}c@{}}7.08\\ (9.76)\end{tabular} & \begin{tabular}[c]{@{}c@{}}7.1\\ (11.21)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}7.23\\ (11.35)\end{tabular} \\
% 4 & 0.9 & 4 & \begin{tabular}[c]{@{}c@{}}5.58\\ (0.59)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.56\\ (0.57)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}5.55\\ (0.68)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.6\\ (0.69)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.75\\ (10.76)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.85\\ (9.89)\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.86\\ (13.16)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}14.57\\ (13.39)\end{tabular} \\
% 4 & 0.9 & 5 & \begin{tabular}[c]{@{}c@{}}5.66\\ (0.78)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.5\\ (0.66)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}5.49\\ (0.69)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.65\\ (0.79)\end{tabular} & \begin{tabular}[c]{@{}c@{}}14.03\\ (14.04)\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.39\\ (14.33)\end{tabular} & \begin{tabular}[c]{@{}c@{}}21.78\\ (13.29)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}22.85\\ (13.95)\end{tabular} \\
% 4 & 0.8 & 3 & \begin{tabular}[c]{@{}c@{}}12.48\\ (0.83)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.35\\ (0.85)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}12.29\\ (0.88)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.45\\ (0.91)\end{tabular} & \begin{tabular}[c]{@{}c@{}}20.03\\ (12.95)\end{tabular} & \begin{tabular}[c]{@{}c@{}}19.7\\ (13.03)\end{tabular} & \begin{tabular}[c]{@{}c@{}}29.57\\ (11.67)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}30.07\\ (11.98)\end{tabular} \\
% 4 & 0.8 & 4 & \begin{tabular}[c]{@{}c@{}}12.17\\ (1.10)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.92\\ (1.05)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}11.68\\ (1.09)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.78\\ (1.19)\end{tabular} & \begin{tabular}[c]{@{}c@{}}18.24\\ (14.70)\end{tabular} & \begin{tabular}[c]{@{}c@{}}18.02\\ (14.51)\end{tabular} & \begin{tabular}[c]{@{}c@{}}26.04\\ (12.65)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}27.85\\ (13.14)\end{tabular} \\
% 4 & 0.8 & 5 & \begin{tabular}[c]{@{}c@{}}11.83\\ (1.37)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.49\\ (1.33)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}11.38\\ (1.31)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.54\\ (1.38)\end{tabular} & \begin{tabular}[c]{@{}c@{}}30.67\\ (13.42)\end{tabular} & \begin{tabular}[c]{@{}c@{}}29.49\\ (13.41)\end{tabular} & \begin{tabular}[c]{@{}c@{}}35.95\\ (10.64)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}36.73\\ (11.29)\end{tabular} \\
% 4 & 0.7 & 3 & \begin{tabular}[c]{@{}c@{}}21.07\\ (1.47)\end{tabular} & \begin{tabular}[c]{@{}c@{}}19.8\\ (1.32)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}19.44\\ (1.14)\end{tabular} & \begin{tabular}[c]{@{}c@{}}19.52\\ (1.13)\end{tabular} & \begin{tabular}[c]{@{}c@{}}10.98\\ (14.19)\end{tabular} & \begin{tabular}[c]{@{}c@{}}21.51\\ (12.23)\end{tabular} & \begin{tabular}[c]{@{}c@{}}25.93\\ (10.43)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}28.36\\ (10.08)\end{tabular} \\
% 4 & 0.7 & 4 & \begin{tabular}[c]{@{}c@{}}19.04\\ (1.61)\end{tabular} & \begin{tabular}[c]{@{}c@{}}18.95\\ (1.59)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}18.32\\ (1.53)\end{tabular} & \begin{tabular}[c]{@{}c@{}}18.61\\ (1.68)\end{tabular} & \begin{tabular}[c]{@{}c@{}}34.69\\ (15.61)\end{tabular} & \begin{tabular}[c]{@{}c@{}}32.37\\ (14.42)\end{tabular} & \begin{tabular}[c]{@{}c@{}}40.11\\ (11.35)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}41.54\\ (11.27)\end{tabular} \\
% 4 & 0.7 & 5 & \begin{tabular}[c]{@{}c@{}}18.27\\ (1.93)\end{tabular} & \begin{tabular}[c]{@{}c@{}}17.82\\ (1.8)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}17.66\\ (1.59)\end{tabular} & \begin{tabular}[c]{@{}c@{}}17.89\\ (1.85)\end{tabular} & \begin{tabular}[c]{@{}c@{}}43.73\\ (11.37)\end{tabular} & \begin{tabular}[c]{@{}c@{}}43.46\\ (11.21)\end{tabular} & \begin{tabular}[c]{@{}c@{}}47.13\\ (7.85)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}47.76\\ (7.8)\end{tabular} \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[htb]
%     \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
% \multicolumn{3}{c}{\textbf{Linear Sep}} & \multicolumn{4}{c}{\textbf{Misclassification Error (\%)}} & \multicolumn{4}{c}{$F_1$\ \textbf{Score} (\%)} \\
% \cmidrule(lr){4-7} \cmidrule(lr){8-11}
%  $p$ & $c$ & $d$ & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS}  \\
% \midrule
% 4 & 0.9 & 3 & \begin{tabular}[c]{@{}c@{}}13.58\\ (0.74)\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.3\\ (0.86)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}13.24\\ (0.84)\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.24\\ (0.85)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}6.77\\ (9.39)\end{tabular} & \begin{tabular}[c]{@{}c@{}}9.68\\ (12.07)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}10.23\\ (12.75)\end{tabular} \\
% 4 & 0.9 & 4 & \begin{tabular}[c]{@{}c@{}}13.39\\ (0.84)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.41\\ (1.03)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}12.09\\ (0.89)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.15\\ (0.88)\end{tabular} & \begin{tabular}[c]{@{}c@{}}6.84\\ (16.98)\end{tabular} & \begin{tabular}[c]{@{}c@{}}24.66\\ (14.78)\end{tabular} & \begin{tabular}[c]{@{}c@{}}33.75\\ (11.22)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}36.97\\ (12.55)\end{tabular} \\
% 4 & 0.9 & 5 & \begin{tabular}[c]{@{}c@{}}12.56\\ (1.26)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.45\\ (1.35)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}10.81\\ (1.03)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.02\\ (1.17)\end{tabular} & \begin{tabular}[c]{@{}c@{}}28.37\\ (25.66)\end{tabular} & \begin{tabular}[c]{@{}c@{}}37.05\\ (16.51)\end{tabular} & \begin{tabular}[c]{@{}c@{}}50.6\\ (8.43)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}52.28\\ (7.59)\end{tabular} \\
% 4 & 0.8 & 3 & \begin{tabular}[c]{@{}c@{}}24.02\\ (1.51)\end{tabular} & \begin{tabular}[c]{@{}c@{}}21.74\\ (1.56)\end{tabular} & \begin{tabular}[c]{@{}c@{}}21.16\\ (1.09)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}20.89\\ (0.97)\end{tabular} & \begin{tabular}[c]{@{}c@{}}7.27\\ (18.08)\end{tabular} & \begin{tabular}[c]{@{}c@{}}28.45\\ (13.77)\end{tabular} & \begin{tabular}[c]{@{}c@{}}35.39\\ (9.49)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}40.32\\ (9.11)\end{tabular} \\
% 4 & 0.8 & 4 & \begin{tabular}[c]{@{}c@{}}19.81\\ (2.1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}19.52\\ (2.04)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}18.08\\ (0.98)\end{tabular} & \begin{tabular}[c]{@{}c@{}}18.1\\ (0.95)\end{tabular} & \begin{tabular}[c]{@{}c@{}}48.39\\ (18.05)\end{tabular} & \begin{tabular}[c]{@{}c@{}}43.09\\ (15.04)\end{tabular} & \begin{tabular}[c]{@{}c@{}}54.47\\ (5.39)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}56.12\\ (4.8)\end{tabular} \\
% 4 & 0.8 & 5 & \begin{tabular}[c]{@{}c@{}}17.27\\ (1.75)\end{tabular} & \begin{tabular}[c]{@{}c@{}}16.15\\ (1.57)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}15.36\\ (1.2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}15.48\\ (1.38)\end{tabular} & \begin{tabular}[c]{@{}c@{}}59.89\\ (8.86)\end{tabular} & \begin{tabular}[c]{@{}c@{}}61.2\\ (7.5)\end{tabular} & \begin{tabular}[c]{@{}c@{}}65.33\\ (4.3)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}65.73\\ (4.34)\end{tabular} \\
% 4 & 0.7 & 3 & \begin{tabular}[c]{@{}c@{}}26.47\\ (2.2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}26.78\\ (2.17)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}25.34\\ (1.14)\end{tabular} & \begin{tabular}[c]{@{}c@{}}25.37\\ (1.27)\end{tabular} & \begin{tabular}[c]{@{}c@{}}55.04\\ (13.31)\end{tabular} & \begin{tabular}[c]{@{}c@{}}43.16\\ (12.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}53.31\\ (5.09)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}55.42\\ (5.55)\end{tabular} \\
% 4 & 0.7 & 4 & \begin{tabular}[c]{@{}c@{}}22.94\\ (1.54)\end{tabular} & \begin{tabular}[c]{@{}c@{}}21.82\\ (1.4)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}21.14\\ (1.03)\end{tabular} & \begin{tabular}[c]{@{}c@{}}21.2\\ (1.15)\end{tabular} & \begin{tabular}[c]{@{}c@{}}60.41\\ (7.44)\end{tabular} & \begin{tabular}[c]{@{}c@{}}62.96\\ (4.54)\end{tabular} & \begin{tabular}[c]{@{}c@{}}64.98\\ (3.2)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}65.39\\ (3.3)\end{tabular} \\
% 4 & 0.7 & 5 & \begin{tabular}[c]{@{}c@{}}18.99\\ (1.39)\end{tabular} & \begin{tabular}[c]{@{}c@{}}18.68\\ (1.51)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}17.28\\ (1.28)\end{tabular} & \begin{tabular}[c]{@{}c@{}}17.35\\ (1.3)\end{tabular} & \begin{tabular}[c]{@{}c@{}}70.64\\ (3.36)\end{tabular} & \begin{tabular}[c]{@{}c@{}}69.66\\ (3.76)\end{tabular} & \begin{tabular}[c]{@{}c@{}}73.26\\ (2.36)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}73.34\\ (2.34)\end{tabular} \\
% \bottomrule
% \end{tabular}
% \end{table}


% \begin{table}[htb]
%     \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
% \multicolumn{3}{c}{\textbf{Mix Type}} & \multicolumn{4}{c}{\textbf{Misclassification Error (\%)}} & \multicolumn{4}{c}{$F_1$\ \textbf{Score} (\%)} \\
% \cmidrule(lr){4-7} \cmidrule(lr){8-11}
%  $p$ & $c$ & $d$ & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS}  \\
% \midrule
% 4 & 0.9 & 3 & \begin{tabular}[c]{@{}c@{}}3.17\\ (0.39)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}3.11\\ (0.45)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.12\\ (0.42)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.19\\ (0.46)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.99\\ (18.47)\end{tabular} & \begin{tabular}[c]{@{}c@{}}14.57\\ (20.45)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}14.87\\ (20.8)\end{tabular} \\4 & 0.9 & 4 & \begin{tabular}[c]{@{}c@{}}3.28\\ (0.53)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}3.14\\ (0.38)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.21\\ (0.43)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.23\\ (0.46)\end{tabular} & \begin{tabular}[c]{@{}c@{}}4.19\\ (13.43)\end{tabular} & \begin{tabular}[c]{@{}c@{}}8.07\\ (16.03)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.08\\ (18.49)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}12.94\\ (19.7)\end{tabular} \\4 & 0.9 & 5 & \begin{tabular}[c]{@{}c@{}}3.4\\ (0.7)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}3.16\\ (0.61)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.3\\ (0.68)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.44\\ (0.76)\end{tabular} & \begin{tabular}[c]{@{}c@{}}24.31\\ (24.67)\end{tabular} & \begin{tabular}[c]{@{}c@{}}25.62\\ (23.84)\end{tabular} & \begin{tabular}[c]{@{}c@{}}33.76\\ (21.38)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}35.32\\ (21.65)\end{tabular} \\4 & 0.8 & 3 & \begin{tabular}[c]{@{}c@{}}9.31\\ (0.71)\end{tabular} & \begin{tabular}[c]{@{}c@{}}8.04\\ (0.87)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}7.67\\ (0.72)\end{tabular} & \begin{tabular}[c]{@{}c@{}}7.7\\ (0.78)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.42\\ (13.55)\end{tabular} & \begin{tabular}[c]{@{}c@{}}32.62\\ (19.29)\end{tabular} & \begin{tabular}[c]{@{}c@{}}42.28\\ (13.61)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}48.6\\ (13.41)\end{tabular} \\4 & 0.8 & 4 & \begin{tabular}[c]{@{}c@{}}8.79\\ (1.17)\end{tabular} & \begin{tabular}[c]{@{}c@{}}8.7\\ (1.1)\end{tabular} &\cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}8.6\\ (1.21)\end{tabular} & \begin{tabular}[c]{@{}c@{}}8.67\\ (1.19)\end{tabular} & \begin{tabular}[c]{@{}c@{}}30.67\\ (27.43)\end{tabular} & \begin{tabular}[c]{@{}c@{}}26.35\\ (21.4)\end{tabular} & \begin{tabular}[c]{@{}c@{}}34.07\\ (24.02)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}35.99\\ (24.77)\end{tabular} \\4 & 0.8 & 5 & \begin{tabular}[c]{@{}c@{}}7.64\\ (1.04)\end{tabular} & \begin{tabular}[c]{@{}c@{}}7.86\\ (1.23)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}7.35\\ (0.96)\end{tabular} & \begin{tabular}[c]{@{}c@{}}7.5\\ (1.05)\end{tabular} & \begin{tabular}[c]{@{}c@{}}50.9\\ (14.78)\end{tabular} & \begin{tabular}[c]{@{}c@{}}45.21\\ (18.24)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}54.38\\ (11.85)\end{tabular} & \begin{tabular}[c]{@{}c@{}}53.98\\ (12.35)\end{tabular} \\4 & 0.7 & 3 & \begin{tabular}[c]{@{}c@{}}15.1\\ (1.94)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.28\\ (1.62)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}11.77\\ (0.97)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.1\\ (1.32)\end{tabular} & \begin{tabular}[c]{@{}c@{}}30.77\\ (30.85)\end{tabular} & \begin{tabular}[c]{@{}c@{}}49.74\\ (15.01)\end{tabular} & \begin{tabular}[c]{@{}c@{}}55.96\\ (7.62)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}59.61\\ (5.47)\end{tabular} \\4 & 0.7 & 4 & \begin{tabular}[c]{@{}c@{}}12.3\\ (1.5)\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.46\\ (1.69)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}12.13\\ (1.51)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.17\\ (1.5)\end{tabular} & \begin{tabular}[c]{@{}c@{}}58.46\\ (7.52)\end{tabular} & \begin{tabular}[c]{@{}c@{}}46.93\\ (14.11)\end{tabular} & \begin{tabular}[c]{@{}c@{}}59.13\\ (6.69)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}59.51\\ (6.8)\end{tabular} \\4 & 0.7 & 5 & \begin{tabular}[c]{@{}c@{}}11.3\\ (1.39)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.66\\ (1.47)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}10.82\\ (1.27)\end{tabular} & \begin{tabular}[c]{@{}c@{}}10.88\\ (1.31)\end{tabular} & \begin{tabular}[c]{@{}c@{}}62.85\\ (6.49)\end{tabular} & \begin{tabular}[c]{@{}c@{}}60.96\\ (7.2)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}64.64\\ (4.78)\end{tabular} & \begin{tabular}[c]{@{}c@{}}64.62\\ (4.7)\end{tabular} \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[htb]
%     \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
% \multicolumn{3}{c}{\textbf{MultiLinear}} & \multicolumn{4}{c}{\textbf{Misclassification Error (\%)}} & \multicolumn{4}{c}{$F_1$\ \textbf{Score} (\%)} \\
% \cmidrule(lr){4-7} \cmidrule(lr){8-11}
%  $p$ & $c$ & $d$ & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS}  \\
% \midrule
% 4 & 0.9 & 3 & \begin{tabular}[c]{@{}c@{}}6.02\\ (0.47)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.12\\ (0.89)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}4.94\\ (0.95)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.4\\ (1.16)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}38.13\\ (23.83)\end{tabular} & \begin{tabular}[c]{@{}c@{}}45.38\\ (20.88)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}46.51\\ (21.04)\end{tabular} \\4 & 0.9 & 4 & \begin{tabular}[c]{@{}c@{}}6.54\\ (1.49)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.51\\ (1.09)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}5.39\\ (1.19)\end{tabular} & \begin{tabular}[c]{@{}c@{}}6.13\\ (1.73)\end{tabular} & \begin{tabular}[c]{@{}c@{}}19.17\\ (26.39)\end{tabular} & \begin{tabular}[c]{@{}c@{}}34.21\\ (24.54)\end{tabular} & \begin{tabular}[c]{@{}c@{}}50.23\\ (11.75)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}50.82\\ (11.1)\end{tabular} \\4 & 0.9 & 5 & \begin{tabular}[c]{@{}c@{}}5.84\\ (1.71)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.36\\ (1.72)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}5.19\\ (1.47)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.48\\ (1.7)\end{tabular} & \begin{tabular}[c]{@{}c@{}}52.23\\ (18.11)\end{tabular} & \begin{tabular}[c]{@{}c@{}}49.9\\ (20.47)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}59.55\\ (9.93)\end{tabular} & \begin{tabular}[c]{@{}c@{}}59.02\\ (9.53)\end{tabular} \\4 & 0.8 & 3 & \begin{tabular}[c]{@{}c@{}}15.44\\ (2.31)\end{tabular} & \begin{tabular}[c]{@{}c@{}}9.98\\ (1.84)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}9.68\\ (1.14)\end{tabular} & \begin{tabular}[c]{@{}c@{}}10.6\\ (1.94)\end{tabular} & \begin{tabular}[c]{@{}c@{}}27.46\\ (32.36)\end{tabular} & \begin{tabular}[c]{@{}c@{}}60.68\\ (14.29)\end{tabular} & \begin{tabular}[c]{@{}c@{}}66.42\\ (6.49)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}67.82\\ (4.62)\end{tabular} \\4 & 0.8 & 4 & \begin{tabular}[c]{@{}c@{}}10.4\\ (1.68)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.01\\ (2.21)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}10.12\\ (1.39)\end{tabular} & \begin{tabular}[c]{@{}c@{}}10.44\\ (1.71)\end{tabular} & \begin{tabular}[c]{@{}c@{}}66.33\\ (4.94)\end{tabular} & \begin{tabular}[c]{@{}c@{}}58.07\\ (17.14)\end{tabular} & \begin{tabular}[c]{@{}c@{}}67.14\\ (5.5)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}67.16\\ (5.23)\end{tabular} \\4 & 0.8 & 5 & \begin{tabular}[c]{@{}c@{}}9.32\\ (2.15)\end{tabular} & \begin{tabular}[c]{@{}c@{}}8.85\\ (2.11)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}7.61\\ (1.53)\end{tabular} & \begin{tabular}[c]{@{}c@{}}7.95\\ (1.59)\end{tabular} & \begin{tabular}[c]{@{}c@{}}72.84\\ (6.94)\end{tabular} & \begin{tabular}[c]{@{}c@{}}73.6\\ (6.41)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}77.98\\ (4.22)\end{tabular} & \begin{tabular}[c]{@{}c@{}}77.32\\ (4.4)\end{tabular} \\4 & 0.7 & 3 & \begin{tabular}[c]{@{}c@{}}15.69\\ (3.04)\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.11\\ (2.18)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}12.34\\ (1.3)\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.46\\ (2.63)\end{tabular} & \begin{tabular}[c]{@{}c@{}}72.73\\ (2.89)\end{tabular} & \begin{tabular}[c]{@{}c@{}}72.07\\ (7.55)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}76.86\\ (2.87)\end{tabular} & \begin{tabular}[c]{@{}c@{}}76.71\\ (2.94)\end{tabular} \\4 & 0.7 & 4 & \begin{tabular}[c]{@{}c@{}}14.09\\ (1.97)\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.62\\ (2.18)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}12.63\\ (1.68)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.73\\ (1.64)\end{tabular} & \begin{tabular}[c]{@{}c@{}}73.06\\ (5.96)\end{tabular} & \begin{tabular}[c]{@{}c@{}}75.1\\ (4.49)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}76.86\\ (3.43)\end{tabular} & \begin{tabular}[c]{@{}c@{}}76.47\\ (3.61)\end{tabular} \\4 & 0.7 & 5 & \begin{tabular}[c]{@{}c@{}}9.64\\ (2.13)\end{tabular} & \begin{tabular}[c]{@{}c@{}}9.74\\ (2.27)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}8.4\\ (1.63)\end{tabular} & \begin{tabular}[c]{@{}c@{}}8.74\\ (1.72)\end{tabular} & \begin{tabular}[c]{@{}c@{}}82.38\\ (4.54)\end{tabular} & \begin{tabular}[c]{@{}c@{}}81.82\\ (4.68)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}84.86\\ (3.09)\end{tabular} & \begin{tabular}[c]{@{}c@{}}84.34\\ (3.16)\end{tabular} \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[htb]
%     \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
% \multicolumn{3}{c}{\textbf{NonLinear}} & \multicolumn{4}{c}{\textbf{Misclassification Error (\%)}} & \multicolumn{4}{c}{$F_1$\ \textbf{Score} (\%)} \\
% \cmidrule(lr){4-7} \cmidrule(lr){8-11}
%  $p$ & $c$ & $d$ & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS}  \\
% \midrule
% 4 & 0.9 & 3 & \begin{tabular}[c]{@{}c@{}}22.96\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.96\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.96\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.96\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} \\4 & 0.9 & 4 & \begin{tabular}[c]{@{}c@{}}22.96\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.96\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.96\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.96\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} \\4 & 0.9 & 5 & \begin{tabular}[c]{@{}c@{}}22.96\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.96\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.96\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.97\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} \\4 & 0.8 & 3 & \begin{tabular}[c]{@{}c@{}}37.64\\ (1.12)\end{tabular} & \begin{tabular}[c]{@{}c@{}}37.64\\ (1.12)\end{tabular} & \begin{tabular}[c]{@{}c@{}}37.64\\ (1.12)\end{tabular} & \begin{tabular}[c]{@{}c@{}}37.64\\ (1.12)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} \\4 & 0.8 & 4 & \begin{tabular}[c]{@{}c@{}}37.64\\ (1.12)\end{tabular} & \begin{tabular}[c]{@{}c@{}}37.65\\ (1.12)\end{tabular} & \begin{tabular}[c]{@{}c@{}}37.65\\ (1.12)\end{tabular} & \begin{tabular}[c]{@{}c@{}}37.65\\ (1.12)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.01\\ (0.05)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.01\\ (0.04)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.01\\ (0.1)\end{tabular} \\4 & 0.8 & 5 & \begin{tabular}[c]{@{}c@{}}37.64\\ (1.12)\end{tabular} & \begin{tabular}[c]{@{}c@{}}37.64\\ (1.12)\end{tabular} & \begin{tabular}[c]{@{}c@{}}37.65\\ (1.12)\end{tabular} & \begin{tabular}[c]{@{}c@{}}37.65\\ (1.12)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.01\\ (0.05)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.01\\ (0.06)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.01\\ (0.07)\end{tabular} \\4 & 0.7 & 3 & \begin{tabular}[c]{@{}c@{}}46.8\\ (1.15)\end{tabular} & \begin{tabular}[c]{@{}c@{}}46.79\\ (1.16)\end{tabular} & \begin{tabular}[c]{@{}c@{}}46.8\\ (1.15)\end{tabular} & \begin{tabular}[c]{@{}c@{}}46.8\\ (1.15)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.01\\ (0.09)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.02\\ (0.1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.02\\ (0.1)\end{tabular} \\4 & 0.7 & 4 & \begin{tabular}[c]{@{}c@{}}46.51\\ (2.34)\end{tabular} & \begin{tabular}[c]{@{}c@{}}45.57\\ (4.03)\end{tabular} & \begin{tabular}[c]{@{}c@{}}44.6\\ (5.16)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}44.38\\ (5.56)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1.52\\ (10.2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}6.59\\ (19.74)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.31\\ (25.01)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}11.95\\ (26.26)\end{tabular} \\4 & 0.7 & 5 & \begin{tabular}[c]{@{}c@{}}36.08\\ (9.43)\end{tabular} & \begin{tabular}[c]{@{}c@{}}36.5\\ (8.66)\end{tabular} & \begin{tabular}[c]{@{}c@{}}32.29\\ (6.89)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}31.66\\ (7.13)\end{tabular} & \begin{tabular}[c]{@{}c@{}}43.57\\ (36.96)\end{tabular} & \begin{tabular}[c]{@{}c@{}}43.12\\ (34.26)\end{tabular} & \begin{tabular}[c]{@{}c@{}}59.56\\ (26.93)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}61.27\\ (27.69)\end{tabular} \\

% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[htb]
%     \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
% \multicolumn{3}{c}{\textbf{Noisy}} & \multicolumn{4}{c}{\textbf{Misclassification Error (\%)}} & \multicolumn{4}{c}{$F_1$\ \textbf{Score} (\%)} \\
% \cmidrule(lr){4-7} \cmidrule(lr){8-11}
%  $p$ & $c$ & $d$ & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS}  \\
% \midrule
% 4 & 0.9 & 3 & \begin{tabular}[c]{@{}c@{}}14.24\\ (0.84)\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.94\\ (0.94)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}13.93\\ (0.9)\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.94\\ (0.9)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}7.81\\ (9.53)\end{tabular} & \begin{tabular}[c]{@{}c@{}}8.86\\ (11.65)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}9.48\\ (12.56)\end{tabular} \\4 & 0.9 & 4 & \begin{tabular}[c]{@{}c@{}}14.04\\ (0.94)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}13.29\\ (1.06)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.87\\ (0.93)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.88\\ (0.95)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.61\\ (15.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}19.41\\ (14.88)\end{tabular} & \begin{tabular}[c]{@{}c@{}}30.08\\ (10.73)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}34.56\\ (11.38)\end{tabular} \\4 & 0.9 & 5 & \begin{tabular}[c]{@{}c@{}}13.09\\ (1.5)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.06\\ (1.22)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}11.7\\ (1.05)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.71\\ (1.03)\end{tabular} & \begin{tabular}[c]{@{}c@{}}28.68\\ (23.54)\end{tabular} & \begin{tabular}[c]{@{}c@{}}36.41\\ (16.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}45.85\\ (10.32)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}47.53\\ (9.93)\end{tabular} \\4 & 0.8 & 3 & \begin{tabular}[c]{@{}c@{}}24.48\\ (1.54)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.37\\ (1.41)\end{tabular} & \begin{tabular}[c]{@{}c@{}}21.85\\ (1.24)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}21.56\\ (1.17)\end{tabular} & \begin{tabular}[c]{@{}c@{}}6.06\\ (16.47)\end{tabular} & \begin{tabular}[c]{@{}c@{}}26.91\\ (11.52)\end{tabular} & \begin{tabular}[c]{@{}c@{}}32.33\\ (9.71)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}38.6\\ (9.21)\end{tabular} \\4 & 0.8 & 4 & \begin{tabular}[c]{@{}c@{}}20.72\\ (2.26)\end{tabular} & \begin{tabular}[c]{@{}c@{}}19.8\\ (1.58)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}18.95\\ (1.17)\end{tabular} & \begin{tabular}[c]{@{}c@{}}18.97\\ (1.14)\end{tabular} & \begin{tabular}[c]{@{}c@{}}43.85\\ (20.46)\end{tabular} & \begin{tabular}[c]{@{}c@{}}43.82\\ (10.98)\end{tabular} & \begin{tabular}[c]{@{}c@{}}51.35\\ (6.37)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}53.02\\ (5.79)\end{tabular} \\4 & 0.8 & 5 & \begin{tabular}[c]{@{}c@{}}18.07\\ (1.6)\end{tabular} & \begin{tabular}[c]{@{}c@{}}17.48\\ (1.69)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}16.5\\ (1.34)\end{tabular} & \begin{tabular}[c]{@{}c@{}}16.53\\ (1.41)\end{tabular} & \begin{tabular}[c]{@{}c@{}}56.85\\ (8.69)\end{tabular} & \begin{tabular}[c]{@{}c@{}}56.14\\ (9.59)\end{tabular} & \begin{tabular}[c]{@{}c@{}}61.66\\ (4.98)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}62.23\\ (5.13)\end{tabular} \\4 & 0.7 & 3 & \begin{tabular}[c]{@{}c@{}}27.19\\ (2.24)\end{tabular} & \begin{tabular}[c]{@{}c@{}}27.07\\ (1.81)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}26.06\\ (1.15)\end{tabular} & \begin{tabular}[c]{@{}c@{}}26.07\\ (1.16)\end{tabular} & \begin{tabular}[c]{@{}c@{}}53.56\\ (14.2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}44.06\\ (10.17)\end{tabular} & \begin{tabular}[c]{@{}c@{}}51.39\\ (5.4)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}54.36\\ (5.09)\end{tabular} \\4 & 0.7 & 4 & \begin{tabular}[c]{@{}c@{}}23.83\\ (1.44)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.68\\ (1.13)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.04\\ (1.07)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}22.01\\ (1.06)\end{tabular} & \begin{tabular}[c]{@{}c@{}}57.97\\ (7.3)\end{tabular} & \begin{tabular}[c]{@{}c@{}}60.61\\ (4.28)\end{tabular} & \begin{tabular}[c]{@{}c@{}}63.02\\ (3.3)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}63.54\\ (3.24)\end{tabular} \\4 & 0.7 & 5 & \begin{tabular}[c]{@{}c@{}}20.64\\ (1.66)\end{tabular} & \begin{tabular}[c]{@{}c@{}}20.02\\ (1.7)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}18.88\\ (1.29)\end{tabular} & \begin{tabular}[c]{@{}c@{}}19.01\\ (1.26)\end{tabular} & \begin{tabular}[c]{@{}c@{}}67.57\\ (5.06)\end{tabular} & \begin{tabular}[c]{@{}c@{}}67.03\\ (5.48)\end{tabular} & \begin{tabular}[c]{@{}c@{}}70.05\\ (2.82)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}70.37\\ (2.91)\end{tabular} \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[htb]
%     \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
% \multicolumn{3}{c}{\textbf{Periodic}} & \multicolumn{4}{c}{\textbf{Misclassification Error (\%)}} & \multicolumn{4}{c}{$F_1$\ \textbf{Score} (\%)} \\
% \cmidrule(lr){4-7} \cmidrule(lr){8-11}
%  $p$ & $c$ & $d$ & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS}  \\
% \midrule
% 4 & 0.9 & 3 & \begin{tabular}[c]{@{}c@{}}0.02\\ (0.13)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.04)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.04\\ (0.16)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.05\\ (0.2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} \\4 & 0.9 & 4 & \begin{tabular}[c]{@{}c@{}}0.16\\ (0.39)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.07\\ (0.27)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.19\\ (0.39)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.23\\ (0.45)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} \\4 & 0.9 & 5 & \begin{tabular}[c]{@{}c@{}}0.46\\ (0.76)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.28\\ (0.58)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.47\\ (0.68)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.55\\ (0.73)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.0\\ (0.0)\end{tabular} \\4 & 0.8 & 3 & \begin{tabular}[c]{@{}c@{}}1.23\\ (0.55)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}1.18\\ (0.42)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1.27\\ (0.48)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1.3\\ (0.51)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.73\\ (7.83)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.2\\ (6.1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}3.82\\ (8.54)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}4.41\\ (9.03)\end{tabular} \\4 & 0.8 & 4 & \begin{tabular}[c]{@{}c@{}}1.46\\ (0.69)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}1.34\\ (0.56)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1.53\\ (0.67)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1.64\\ (0.72)\end{tabular} & \begin{tabular}[c]{@{}c@{}}8.51\\ (12.44)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.67\\ (9.77)\end{tabular} & \begin{tabular}[c]{@{}c@{}}9.65\\ (11.53)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}10.96\\ (12.44)\end{tabular} \\4 & 0.8 & 5 & \begin{tabular}[c]{@{}c@{}}1.94\\ (1.01)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}1.65\\ (0.76)\end{tabular} & \begin{tabular}[c]{@{}c@{}}1.96\\ (0.89)\end{tabular} & \begin{tabular}[c]{@{}c@{}}2.09\\ (1.02)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.63\\ (11.79)\end{tabular} & \begin{tabular}[c]{@{}c@{}}11.99\\ (13.25)\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.58\\ (11.91)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}14.11\\ (11.92)\end{tabular} \\4 & 0.7 & 3 & \begin{tabular}[c]{@{}c@{}}4.59\\ (0.87)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}4.47\\ (0.69)\end{tabular} & \begin{tabular}[c]{@{}c@{}}4.5\\ (0.64)\end{tabular} & \begin{tabular}[c]{@{}c@{}}4.62\\ (0.85)\end{tabular} & \begin{tabular}[c]{@{}c@{}}10.54\\ (14.22)\end{tabular} & \begin{tabular}[c]{@{}c@{}}9.4\\ (11.64)\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.26\\ (13.29)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}13.47\\ (13.82)\end{tabular} \\4 & 0.7 & 4 & \begin{tabular}[c]{@{}c@{}}5.05\\ (1.03)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}4.65\\ (0.79)\end{tabular} & \begin{tabular}[c]{@{}c@{}}4.82\\ (0.84)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.03\\ (0.95)\end{tabular} & \begin{tabular}[c]{@{}c@{}}24.06\\ (14.3)\end{tabular} & \begin{tabular}[c]{@{}c@{}}18.2\\ (13.63)\end{tabular} & \begin{tabular}[c]{@{}c@{}}25.7\\ (13.58)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}27.02\\ (13.61)\end{tabular} \\4 & 0.7 & 5 & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}5.15\\ (0.97)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.17\\ (0.96)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.29\\ (0.98)\end{tabular} & \begin{tabular}[c]{@{}c@{}}5.37\\ (1.04)\end{tabular} & \begin{tabular}[c]{@{}c@{}}29.25\\ (12.06)\end{tabular} & \begin{tabular}[c]{@{}c@{}}27.85\\ (13.08)\end{tabular} & \begin{tabular}[c]{@{}c@{}}31.81\\ (11.4)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}31.9\\ (11.35)\end{tabular} \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[htb]
%     \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
% \multicolumn{3}{c}{\textbf{Step}} & \multicolumn{4}{c}{\textbf{Misclassification Error (\%)}} & \multicolumn{4}{c}{$F_1$\ \textbf{Score} (\%)} \\
% \cmidrule(lr){4-7} \cmidrule(lr){8-11}
%  $p$ & $c$ & $d$ & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS} & \textbf{CART} & \textbf{EFS} & \textbf{MDFS} & \textbf{PFS}  \\
% \midrule
% 4 & 0.9 & 3 & \begin{tabular}[c]{@{}c@{}}49.9\\ (2.15)\end{tabular} & \begin{tabular}[c]{@{}c@{}}44.86\\ (2.46)\end{tabular} & \begin{tabular}[c]{@{}c@{}}43.38\\ (2.58)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}41.59\\ (2.99)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.92\\ (6.44)\end{tabular} & \begin{tabular}[c]{@{}c@{}}19.9\\ (8.15)\end{tabular} & \begin{tabular}[c]{@{}c@{}}25.04\\ (7.96)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}30.87\\ (9.25)\end{tabular} \\4 & 0.9 & 4 & \begin{tabular}[c]{@{}c@{}}38.94\\ (8.09)\end{tabular} & \begin{tabular}[c]{@{}c@{}}40.14\\ (4.26)\end{tabular} & \begin{tabular}[c]{@{}c@{}}35.3\\ (2.28)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}34.61\\ (2.59)\end{tabular} & \begin{tabular}[c]{@{}c@{}}35.4\\ (25.2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}33.7\\ (11.85)\end{tabular} & \begin{tabular}[c]{@{}c@{}}47.29\\ (5.58)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}48.98\\ (6.28)\end{tabular} \\4 & 0.9 & 5 & \begin{tabular}[c]{@{}c@{}}28.73\\ (3.89)\end{tabular} & \begin{tabular}[c]{@{}c@{}}32.59\\ (4.83)\end{tabular} & \begin{tabular}[c]{@{}c@{}}27.4\\ (1.93)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}27.01\\ (2.0)\end{tabular} & \begin{tabular}[c]{@{}c@{}}61.38\\ (8.17)\end{tabular} & \begin{tabular}[c]{@{}c@{}}52.01\\ (12.23)\end{tabular} & \begin{tabular}[c]{@{}c@{}}63.43\\ (3.64)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}64.21\\ (3.86)\end{tabular} \\4 & 0.8 & 3 & \begin{tabular}[c]{@{}c@{}}37.77\\ (8.03)\end{tabular} & \begin{tabular}[c]{@{}c@{}}40.4\\ (3.63)\end{tabular} & \begin{tabular}[c]{@{}c@{}}36.8\\ (2.27)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}35.2\\ (2.36)\end{tabular} & \begin{tabular}[c]{@{}c@{}}41.37\\ (26.13)\end{tabular} & \begin{tabular}[c]{@{}c@{}}34.69\\ (10.59)\end{tabular} & \begin{tabular}[c]{@{}c@{}}45.73\\ (5.97)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}50.32\\ (6.36)\end{tabular} \\4 & 0.8 & 4 & \begin{tabular}[c]{@{}c@{}}30.45\\ (2.98)\end{tabular} & \begin{tabular}[c]{@{}c@{}}32.14\\ (2.9)\end{tabular} & \begin{tabular}[c]{@{}c@{}}29.55\\ (1.86)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}28.97\\ (2.03)\end{tabular} & \begin{tabular}[c]{@{}c@{}}59.93\\ (6.93)\end{tabular} & \begin{tabular}[c]{@{}c@{}}55.03\\ (7.13)\end{tabular} & \begin{tabular}[c]{@{}c@{}}61.03\\ (4.1)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}62.38\\ (4.32)\end{tabular} \\4 & 0.8 & 5 & \begin{tabular}[c]{@{}c@{}}23.04\\ (2.29)\end{tabular} & \begin{tabular}[c]{@{}c@{}}25.21\\ (2.2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}22.19\\ (1.57)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}21.98\\ (1.65)\end{tabular} & \begin{tabular}[c]{@{}c@{}}72.46\\ (3.94)\end{tabular} & \begin{tabular}[c]{@{}c@{}}67.99\\ (3.95)\end{tabular} & \begin{tabular}[c]{@{}c@{}}73.34\\ (2.54)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}73.71\\ (2.67)\end{tabular} \\4 & 0.7 & 3 & \begin{tabular}[c]{@{}c@{}}31.93\\ (2.28)\end{tabular} & \begin{tabular}[c]{@{}c@{}}33.78\\ (3.03)\end{tabular} & \begin{tabular}[c]{@{}c@{}}31.53\\ (1.43)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}30.85\\ (1.72)\end{tabular} & \begin{tabular}[c]{@{}c@{}}60.33\\ (6.2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}54.29\\ (8.5)\end{tabular} & \begin{tabular}[c]{@{}c@{}}60.14\\ (3.51)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}62.14\\ (4.66)\end{tabular} \\4 & 0.7 & 4 & \begin{tabular}[c]{@{}c@{}}26.39\\ (2.28)\end{tabular} & \begin{tabular}[c]{@{}c@{}}26.91\\ (1.82)\end{tabular} & \begin{tabular}[c]{@{}c@{}}25.24\\ (1.43)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}24.98\\ (1.44)\end{tabular} & \begin{tabular}[c]{@{}c@{}}69.03\\ (4.5)\end{tabular} & \begin{tabular}[c]{@{}c@{}}67.14\\ (3.75)\end{tabular} & \begin{tabular}[c]{@{}c@{}}70.42\\ (2.62)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}71.05\\ (2.65)\end{tabular} \\4 & 0.7 & 5 & \begin{tabular}[c]{@{}c@{}}20.16\\ (1.69)\end{tabular} & \begin{tabular}[c]{@{}c@{}}20.97\\ (1.85)\end{tabular} & \begin{tabular}[c]{@{}c@{}}19.52\\ (1.23)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}19.35\\ (1.28)\end{tabular} & \begin{tabular}[c]{@{}c@{}}77.68\\ (2.66)\end{tabular} & \begin{tabular}[c]{@{}c@{}}75.88\\ (2.94)\end{tabular} & \begin{tabular}[c]{@{}c@{}}78.1\\ (1.7)\end{tabular} & \cellcolor[HTML]{9AFF99}\begin{tabular}[c]{@{}c@{}}78.41\\ (1.83)\end{tabular} \\
% \bottomrule
% \end{tabular}
% \end{table}

% \clearpage

% \subsection{Simulation results in Table~\ref{Table simulation results in main text}}\label{Appendix: Simulation in main}
% Note that part of the results presented in Table~\ref{Table simulation results in main text} is not from the 89 simulation results. In this section, we elaborate on the settings of simulations listed in Table~\ref{Table simulation results in main text} in the main text. We did all these modifications not to give any advantage to our proposed methods. Instead, most of the $F_1$ score gains between our method and CART decrease due to these modifications. We did these only to obtain more presentable, i.e., higher $F_1$ scores for ALL three methods. 
% % For some methods, we reduce the sample size from 2000 to 1000 as $PFS$ and $MDFS$ give more prominent $F_1$ scores. This potentially suggests our methods' robustness to a smaller sample size. 

% \textbf{Hierarchical}: $p=4$, $d=4$, $n=1000$. We modify the simulation function from \( \eta = \text{sigmoid}(X_1 + X_2 + \sum_{i=3}^{p} X_i) \) to \( \eta = \text{sigmoid}(X_1 + X_2 + \sum_{i=3}^{p} X_i - 2) \). This modification makes $\eta$ more centered around 0.5 and less imbalanced. 

% \textbf{High Dimensional}: $p=10$, $d=5$. We modify the simulation function from \( \eta = \text{sigmoid}(\sum_{i=1}^{10} X_i \) to \( \eta = \text{sigmoid}(\sum_{i=1}^{5} X_i \).  This modification condenses the signal and makes the split decision easier for all methods. 
% % Like in the previous example, we did it to condense the signal and obtain more presentable $F_1$ scores for all three methods. 

% \textbf{Imbalanced Classes}: $p=4$, $d=5$.

% \textbf{Interaction Effects}: $p=4$, $d=6$. We modify the simulation function from  \( \eta = \text{sigmoid}(X_1 X_2 + X_3 X_4 + \sum_{i=5}^p X_i) \) to  \( \eta = \text{sigmoid}(4 X_1 X_2 + X_3 X_4) \). This modification makes $\eta$ more dispersed on $[0,1]$ thus easier to classify. 

% \textbf{Linear Separable}: $p=2$, $d=4$.

% \textbf{Mixed Types}: $p=3$, $d=4$.

% \textbf{Multicollinear}: $p=4$, $d=4$, $n=1000$.

% \textbf{Nonlinear Separable}: $p=3$, $d=6$.

% \textbf{Noisy Data}: $p=2$, $d=4$, $n=1000$.

% \textbf{Periodic Function}: $p=2$, $d=4$.

% \textbf{Step Function}: $p=2$, $d=4$. The new step function we considered is more of a combination of step function and hierarchical function:
% \begin{enumerate}
%     \item \( X_1 \sim \text{Uniform}(-1, 1) \), \( X_2 \sim \text{Uniform}(0, 1) \), \( X_3 \sim \mathcal{N}(0, I_{p-2}) \). 
    
%     \item Transform \(X_2 \):
%     \[
%     g(x) =
%     \begin{cases} 
%       x & \text{if } 0 \leq x < 0.2, \\
%       0.2 & \text{if } 0.2 \leq x < 0.4, \\
%       x - 0.2 & \text{if } 0.4 \leq x < 0.6, \\
%       0.4 & \text{if } 0.6 \leq x < 0.8, \\
%       x - 0.4 & \text{if } 0.8 \leq x \leq 1. 
%     \end{cases}
%     \]

%     \item Calculate probabilities: 
%     \[
%     \eta =
%     \begin{cases}
%     \frac{g(X_2)}{0.6} & \text{if } X_1 > 0, \\
%     1 - \frac{g(X_2)}{0.6} & \text{if } X_1 < 0.
%     \end{cases}
%     \]
% \end{enumerate}


\section{Additonal empirical study: Forest fire}\label{Appendix Empirical Studies}
We use publicly available data from our empirical studies. The Pima Indian Diabetes dataset can be downloaded at \url{https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database}. The Montesinho Park forest fire dataset can be downloaded at \url{https://archive.ics.uci.edu/dataset/162/forest+fires}. %The NLSY97 dataset can be accessed at \url{https://www.nlsinfo.org/investigator/pages/home}. The donation dataset can be downloaded at \url{https://www.openicpsr.org/openicpsr/project/113224/version/V1/view?path=/openicpsr/113224/fcr:versions/V1/README_data_update.pdf&type=file#}, it is first used by \citet{karlan2007does}.

In this section, we use forest fire dataset to illustrate an application of our methods MDFS and RF-MDFS, and compare them to CART and RF-CART respectively.

The UCI Forest Fire dataset measures the characteristics of different forest fires in Montesinho Park with the response variable being the area burnt by a forest fire. We binarize the response by labeling those observations with a burnt area larger than 5 as 1, and 0 otherwise. 1 indicates a ``big'' forest fire. We search for conditions under which the probability of having a big forest fire is above 1/3. We use the following features in the dataset for this task: X and Y-axis spatial coordinates, month, FFMC, DMC, DC, ISI, temperature, RH (relative humidity), wind, and rain. All the acronyms except for RH are measures that positively correlate with the probability of a big forest fire. With a moderate sample size of 517, we set $m=3$. %We summarize the splitting rules generated by CART and PFS in Appendix \ref{Appendix Empirical Studies}.
%In this application, CART and RF-CART generate the same targeting policy whereas MDFS and RF-MDFS generate the same policy. Hence, 
%We plot all four policies in Figure \ref{figure diabetes CART vs MDFS}. %but will only compare CART and MDFS in text. The comparison between RF-CART and RF-MDFS is identical. 


\begin{figure*}[ht] 
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \fontsize{8pt}{9.6pt}\selectfont
        \textbf{CART}
            \begin{Verbatim}[commandchars=\\\{\}]
if FFMC <= 85.85
    if temp <= 5.15
        if X <= 5.0
            \textcolor[HTML]{990000}{value: 1.000, samples: 8}
            \textcolor[HTML]{990000}{value: 0.600, samples: 5}
        if temp <= 18.55
            value: 0.172, samples: 29
            \textcolor[HTML]{990000}{value: 0.667, samples: 9}
    if temp <= 26.0
        if DC <= 673.2
            value: 0.208, samples: 216
            value: 0.310, samples: 200
        if RH <= 24.5
            {value: 0.125, samples: 8} 
            \textcolor[HTML]{990000}{value: 0.500, samples: 42}
        \end{Verbatim}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \fontsize{8pt}{9.6pt}\selectfont
        \textbf{MDFS}
        \begin{Verbatim}[commandchars=\\\{\}]
if FFMC <= 85.85
    if temp <= 5.15
        if X <= 3.5
            \textcolor[HTML]{990000}{value: 1.000, samples: 2}
            \textcolor[HTML]{990000}{value: 0.818, samples: 11}
        if temp <= 18.55
            value: 0.172, samples: 29
            \textcolor[HTML]{990000}{value: 0.667, samples: 9}
    if temp <= 26.0
        if DC <= 766.2
            value: 0.244 samples: 216
            \textcolor[HTML]{990000}{value: 0.372, samples: 43}
        if RH <= 24.5
            {value: 0.125, samples: 8} 
            \textcolor[HTML]{990000}{value: 0.500, samples: 42}
        \end{Verbatim}
    \end{minipage}
\\
\vspace{.5cm}
\begin{minipage}{0.5\textwidth}
        \centering
        \fontsize{8pt}{9.6pt}\selectfont
        \textbf{RF-CART}
            \begin{Verbatim}[commandchars=\\\{\}]
if FFMC <= 85.85
    if temp <= 5.15
        if wind <= 7.15
            \textcolor[HTML]{990000}{value: 0.638, samples: 6}
            \textcolor[HTML]{990000}{value: 0.964, samples: 7}
        if temp <= 18.55
            value: 0.229, samples: 29
            \textcolor[HTML]{990000}{value: 0.594, samples: 9}
    if temp <= 26.0
        if DC <= 673.2
            value: 0.220 samples: 216
            value: 0.308, samples: 200
        if FFMC <= 95.85
            \textcolor[HTML]{990000}{value: 0.381, samples: 43} 
            \textcolor[HTML]{990000}{value: 0.649, samples: 7}
        \end{Verbatim}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \fontsize{8pt}{9.6pt}\selectfont
        \textbf{RF-MDFS}
        \begin{Verbatim}[commandchars=\\\{\}]
if FFMC <= 85.85
    if temp <= 5.15
        
        \textcolor[HTML]{990000}{value: 0.814, samples: 13}
            
        if temp <= 18.55
            value: 0.229, samples: 29
            \textcolor[HTML]{990000}{value: 0.594, samples: 9}
    if temp <= 26.0
        if DC <= 766.2
            value: 0.250 samples: 373
            \textcolor[HTML]{990000}{value: 0.368, samples: 43}
        if FFMC <= 91.95
            \textcolor[HTML]{990000}{value: 0.578, samples: 4} 
            \textcolor[HTML]{990000}{value: 0.404, samples: 46}
        \end{Verbatim}
    \end{minipage}

    \caption{The targeting policies generated by CART, MDFS, RF-CART, RF-MDFS. The \textcolor[HTML]{990000}{red} groups are the targeted subpopulations predicted to have a higher than 1/3 probability of catching a big forest fire.}
    \label{figure diabetes CART vs MDFS}
\end{figure*}

\subsection{CART vs MDFS}
As shown by Figure \ref{figure forest fire CART vs MDFS}, the two sets of policies target many common groups except for the subgroup consisting of 43 observations defined by $FFMC > 85.85$, $temp \leq 26.0$, $DC > 766.2$. This group that MDFS additionally targets has a 37.2\% probability of catching a big forest fire, higher than the threshold. This finding aligns with Remark \ref{remark more cost}.

\subsection{RF-CART vs RF-MDFS}
As shown by Figure \ref{figure forest fire CART vs MDFS}, RF-MDFS also additionally targets the subgroup consisting of 43 observations defined by $FFMC > 85.85$, $temp \leq 26.0$, $DC > 766.2$. Hence, the comparison between RF-CART and RF-MDFS here also aligns with Remark \ref{remark more cost}.

Interestingly, both RF-CART and RF-MDFS (compared to CART and MDFS respectively) additionally target the subgroup $FFMC > 85.85$, $temp > 26.0$, $RH \leq 24.5$.
% \subsection{Splitting rules generated by CART, PFS, and MDFS}
% We present in Figure \ref{figure forest fires trees} and \ref{figure WIC trees} the splitting rules generated by CART and PFS in the forest fires, WIC, and donation empirical studies. In all three cases of diabetes, forest fires, and WIC, MDFS outputs identical results to PFS. This is likely because we select only 19 candidate splits, with more candidates, these two methods are more likely to differ. We also experiment with different threshold values and tree depths, these two methods vary occasionally.

% In addition, we do not report the full sample results for the donation studies. To find subpopulations with a higher probability of donation, we need to grow the trees to a depth of five or six. The tree does not look very presentable, so we report performance using an alternative metric in the main text. In this Appendix, we report the same metric for all four methods: CART, EFS, PFS, and MDFS.

% \textbf{WIC participation: }
% The National Longitudinal Survey for Youth 1997 (NLSY97) dataset contains a rich set of characteristics of households including the WIC participation status of all sampled households. We select a sample that is eligible for WIC from the years 2001 to 2009. After selection, there are 8238 observations and the overall participation rate in our sample is around 60\%. The sample size is much larger than the previous two studies, hence, we grow a deeper tree with $m=4$. We search for subpopulations whose participation probability is lower than 50\% using sex, race, AFDC, FDSTMP, income, number of infants, number of preschoolers, benefit amount, urban resident, and number of companions to partition the sample. AFDC and FDSTMP are participation statuses of two other welfare programs. % We summarize the splitting rules generated by CART and PFS in Figure \ref{figure WIC trees}.
% % [1] "WIC_1"                  "sex"                    "race"                  
% %  [4] "AFDC_1"                 "FDSTMP_1"               "income"                
% %  [7] "number_of_kids_below_1" "number_of_kids_below_5" "WIC_benefit"           
% % [10] "urban"                  "companion"
% We conduct the same procedures as in the case of ``Donation'' empirical studies. The results are summarized as follows:
% \begin{table}[ht]
%     \centering
%     \begin{tabular}{c c c c c c c c}
%     \toprule
%          \multicolumn{4}{c}{Misclassification $(\%)$} & \multicolumn{4}{c}{F1 score $(\%)$} \\
%          \midrule
%          CART & EFS & PFS & MDFS & CART & EFS & PFS & MDFS \\
%          %2.63(2.54) & 2.18(2.13) & 83.7(13.74) & 86.36(12.66)\\
%          8.05(7.54) & 6.93(6.90) & 7.57(5.89) &7.21(6.66) & 86.34(13.38) & 88.02(12.61) & 87.27(11.27) & 87.18(11.78) \\
%          \bottomrule
%     \end{tabular}
%     % \caption{Caption}
%     % \label{tab:my_label}
% \end{table}
% %As compared to CART, PFS additionally targets the \textcolor[HTML]{EF6C00}{orange} group which \textcolor[HTML]{EF6C00}{has no infant, does not participate in AFDC, earns less than \$54985 annually, and has at least 3 companions in the household}. The \textcolor[HTML]{EF6C00}{orange} group with size 114 is much bigger than the \textcolor[HTML]{990000}{red} group targeted by both CART and PFS. Using PFS finds a substantially larger subpopulation that does not participate in WIC despite being eligible. The findings of PFS suggest WIC administrators conduct information campaigns at daycare centers in low-income neighborhoods where the AFDC participation rate is low and the average household size is large.

% Full results for \textbf{Donation} (include all 100 experiments):
% \begin{table}[ht]
%     \centering
%     \begin{tabular}{c c c c c c c c}
%     \toprule
%          \multicolumn{4}{c}{Misclassification $(\%)$} & \multicolumn{4}{c}{F1 score $(\%)$} \\
%          \midrule
%          CART & EFS & PFS & MDFS & CART & EFS & PFS & MDFS \\
%          2.86(2.37) & 2.95 (2.97) & 2.61(2.12) &2.55(2.01) & 82.63(14.82) & 78.75(20.06) & 84.11(13.72) & 84.00(14.12) \\
%          \bottomrule
%     \end{tabular}
%     % \caption{Caption}
%     % \label{tab:my_label}
% \end{table}

% % \begin{figure*}[p]
% %     \centering
% %     \begin{minipage}{0.48\textwidth}
% %         \centering
% %         \fontsize{8pt}{9.6pt}\selectfont
% %         \textbf{CART}
% %             \begin{Verbatim}[commandchars=\\\{\}]
% % if temp <= 8.2
% %     if temp <= 5.15
% %         \textcolor[HTML]{990000}{value: 0.8571, samples: 14}
% %     else
% %         value: 0.2308, samples: 13
% % else 
% %     if DMC <= 117.9
% %         if X <= 5.5
% %             value: 0.1634, samples: 153
% %         else
% %             value: 0.3193, samples: 119
% %     else 
% %         if X <= 2.5
% %             \textcolor[HTML]{990000}{value: 0.4375, samples: 64}
% %         else
% %             value: 0.2923, samples: 154
% %         \end{Verbatim}
% %         \hfill
% %     \end{minipage}%
% %     \hfill
% %     \begin{minipage}{0.48\textwidth}
% %         \centering
% %         \fontsize{8pt}{9.6pt}\selectfont
% %         \textbf{PFS}
% %         \begin{Verbatim}[commandchars=\\\{\}]
% % if temp <= 8.2
% %     if temp <= 5.15
% %         \textcolor[HTML]{990000}{value: 0.8571, samples: 14}
% %     else
% %         value: 0.2308, samples: 13
% % else 
% %     if DMC <= 117.9
% %         if X <= 7.5
% %             value: 0.2051, samples: 234
% %         else
% %             \textcolor[HTML]{EF6C00}{value: 0.3947, samples: 38}
% %     else 
% %         if X <= 2.5
% %             \textcolor[HTML]{990000}{value: 0.4375, samples: 64}
% %         else
% %             value: 0.2923, samples: 154
% %         \end{Verbatim}
% %     \end{minipage}
% %     \caption{The colored groups are the targeted subpopulations with a high risk of forest fires chosen by CART and PFS.}
% %     \label{figure forest fires trees}
% % \end{figure*}

% % \begin{figure*}[p]
% %     \centering
% %     \fontsize{7pt}{7pt}\selectfont
% %     \begin{minipage}{0.48\textwidth}
% %     \centering
% %         \textbf{CART}
% %         \begin{Verbatim}[commandchars=\\\{\}]
% % if infants <= 0.5
% %     if AFDC <= 0.5
% %         if income <= 54985.0545
% %             if benefit <= 100.9999
% %                 value: 0.4575, samples: 1952
% %             else
% %                 value: 0.5193, samples: 751
% %         else 
% %             if companion <= 1.5
% %                 value: 0.4819, samples: 83
% %             else
% %                 value: 0.3394, samples: 218
% %     else 
% %         if Hispanic <= 0.5
% %             if benefit <= 78.3333
% %                 value: 0.5933, samples: 268
% %             else
% %                 value: 0.5077, samples: 390
% %         else 
% %             if income <= 32400.0
% %                 value: 0.6839, samples: 142
% %             else
% %                 value: 0.4468, samples: 47
% % else 
% %     if benefit <= 117.7
% %         if sex <= 1.5
% %             if companion <= 2.5
% %                 value: 0.6952, samples 105
% %             else
% %                 \textcolor[HTML]{990000}{value: 0.3182, samples: 22}
% %         else 
% %             value: 0.8057, samples: 453
% %     else 
% %         if income <= 7499.9975
% %             if benefit <= 139.3333
% %                 value: 0.6944, samples: 288
% %             else
% %                 value: 0.6420, samples: 676
% %         else 
% %             if benefit <= 224
% %                 value: 0.6992, samples: 2706
% %             else
% %                 value: 0.8394, samples: 137
% %     \end{Verbatim}
% %     \end{minipage}
% %     \hfill
% %     \begin{minipage}{0.48\textwidth}
% %         \centering
% %         \textbf{PFS}
% %         \begin{Verbatim}[commandchars=\\\{\}]
% % if infants <= 0.5
% %     if AFDC <= 0.5
% %         if income <= 54985.0545
% %             if benefit <= 100.9999
% %                 value: 0.4575, samples: 1952
% %             else
% %                 value: 0.5193, samples: 751
% %         else 
% %             if companion <= 2.5
% %                 value: 0.4118, samples: 187
% %             else
% %                 \textcolor[HTML]{EF6C00}{value: 0.3246, samples: 114}
% %     else 
% %         if Hispanic <= 0.5
% %             if benefit <= 78.3333
% %                 value: 0.5933, samples: 268
% %             else
% %                 value: 0.5077, samples: 390
% %         else 
% %             if income <= 32400.0
% %                 value: 0.6839, samples: 142 
% %             else
% %                 value: 0.4468, samples: 47
% % else 
% %     if benefit <= 117.7
% %         if sex <= 1.5
% %             if companion <= 2.5
% %                 value: 0.6952, samples 105
% %             else
% %                 \textcolor[HTML]{990000}{value: 0.3182, samples: 22}
% %         else 
% %             value: 0.8057, samples: 453
% %     else 
% %         if income <= 7499.9975
% %             if benefit <= 139.3333
% %                 value: 0.6944, samples: 288
% %             else
% %                 value: 0.6420, samples: 676
% %         else 
% %             if benefit <= 224
% %                 value: 0.6992, samples: 2706
% %             else
% %                 value: 0.8394, samples: 137
% %     \end{Verbatim}
% %     \end{minipage}
% %     \caption{The colored groups are the targeted subpopulations with a low probability of participating in WIC chosen by CART and PFS.}
% %     \label{figure WIC trees}
% % \end{figure*}
% % Note that in the main text when we compare CART and PFS, we exclude trees that are identical to each other. Here in the two comparisons between all four trees, we compute the misclassification errors and F1 scores based on all 100 trees, we do not  exclude trees that are identical across all four algorithms.







% \subsection{Splitting rules generated by EFS for diabetes and forest fires}
% \subsubsection{Diabetes}
% \begin{figure}[ht] % Wrap text around the figure, right side
%     \centering
%     \fontsize{8pt}{9.6pt}\selectfont
%     \begin{center}
%         \begin{minipage}{0.5\textwidth}
%                 \begin{Verbatim}[commandchars=\\\{\}]
% if glucose <= 129.0
%     if age <= 28.0
%         if BMI <= 41.47
%             value: 0.0867, samples: 265
%         else 
%             value: 0.2941, samples: 17
%     else 
%         if BMI <= 30.71
%             value: 0.2755, samples: 98
%         else 
%             value: 0.3923, samples: 130
% else 
%     if BMI <= 29.7
%         if glucose <= 161.63
%             value: 0.2549, samples: 51
%         else 
%             \textcolor{blue}{value: 0.6, samples: 15}
%     else 
%         if glucose <= 145.5
%             \textcolor[HTML]{990000}{value: 0.65, samples: 60}
%         else 
%             \textcolor[HTML]{990000}{value: 0.7655, samples: 132}
%     \end{Verbatim}
%         \end{minipage}
%     \end{center}
%     \caption{EFS splitting rules for the diabetes empirical study}
%     \label{EFS diabetes tree}
% \end{figure}
% The EFS method targets the \textcolor{blue}{blue} group whose \textcolor{blue}{glucose is above 161.63 and BMI lower than 29.7}. As compared to the \textcolor{blue}{blue} targeted by CART in Figure \ref{figure diabetes trees}, EFS excludes a subpopulation which has a diabetic rate of 44.44\% (below the threshold of 50\%). Hence, EFS successfully excludes a group that does not belong to the target subpopulation. On the other hand, it does not capture another high-risk group whose \textcolor[HTML]{EF6C00}{glucose is lower than 129, but age over 28 and BMI over 40.2}. The \textcolor[HTML]{EF6C00}{orange} group is targeted by PFS and MDFS, but not by CART or EFS. Hence, we argue that MDFS and PFS perform the best out of all the four tested methods.




% \subsubsection{Forest fires}
% \begin{figure}[ht]
%     \centering
%     \fontsize{8pt}{9.6pt}\selectfont
%     \begin{center}
%         \begin{minipage}{0.5\textwidth}
%             \begin{Verbatim}[commandchars=\\\{\}]
% if temp <= 8.2
%     if temp <= 5.15
%         \textcolor[HTML]{990000}{value: 0.8571, samples: 14}
%     else
%         value: 0.2308, samples: 13
% else 
%     if DMC <= 117.9
%         if X <= 7.5
%             value: 0.2051, samples: 234
%         else 
%             \textcolor[HTML]{EF6C00}{value: 0.3947, samples: 38}
%     else 
%         if X <= 5.5
%             \textcolor{blue}{value: 0.3560, samples: 132}
%         else 
%             value: 0.3023, samples: 86
% \end{Verbatim}
%         \end{minipage}
%     \end{center}   
%     \caption{EFS splitting rules for the forest fires empirical study}
%     \label{EFS forest fires tree}
% \end{figure}
% EFS's performance is the best of the four tested methods. Not only does it capture the \textcolor[HTML]{EF6C00}{orange} group in Figure \ref{figure forest fires trees} that is chosen by PFS and MDFS but not by CART, it targets a very large \textcolor{blue}{blue} group. In the main text, PFS and MDFS both target the \textcolor[HTML]{990000}{red} group with \textcolor[HTML]{990000}{temperature higher than 8.2, DMC greater than 117.9, and X smaller than 2.5}, the \textcolor{blue}{blue} group in Figure \ref{EFS forest fires tree} contains the \textcolor[HTML]{990000}{red} group, consisting of a larger area with \textcolor{blue}{temperature higher than 8.2, DMC greater than 117.9, and X smaller than 5.5}.


\end{document}