\section{Related Works}
\label{sec:related works}

\noindent \textbf{Memory-augmented neural networks.}\ 
Designing architectures capable of memorization and generalization through knowledge abstraction____ and data-dependent information retention____ has been a longstanding research focus. 
%%
Early approaches introduced architectures with external memory modules, such as neural turing machines (NTM)____ and modern Hopfield Networks____, which utilize pre-defined update rules to manage memory. With the advent of Transformers, some methods employ recurrent Transformer architectures____ to cache key-value pairs as memory, enabling the reuse of cached information to extend context window sizes. 
%%
Additionally, recent studies have explored encoding training data into model parameters, effectively using them as memory to store world knowledge____. This approach has also been extended to large databases____, test-time data points____, and broader language modeling tasks____.
%%
Titans____ integrates long-term, short-term, and persistent memory into a unified neural architecture.
%%
While optimizing memory retention, they overlook retrieval reliability from model parameters, which is a core design motivation of \ourmethod{}.

\noindent \textbf{Context compression.}\ 
% Compressing lengthy contexts into concise representations while retaining essential information is an efficient strategy to reduce token consumption during LLM inference____.
Compressing lengthy contexts into concise representations that retains essential information can make LLM inference more efficient____.
%%
Approaches like Selective Context____, LLMLingua____ and RECOMP____ use context selection to improve inference efficiency, and methods such as AutoCompressor____, in-context autoencoder (ICAE)____, Gist____ and CompAct____ employ training-based techniques to generate summary representations. 
%%
Besides, ____ proposes new general-purpose language modeling perspectives by leveraging compression through arithmetic coding from information theory____. 
%%
In contrast, \ourmethod{} uses context compression as a surrogate task to optimize memory retention while ensuring alignment through backward context expansion.