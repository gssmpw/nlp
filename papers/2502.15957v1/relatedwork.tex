\section{Related Works}
\label{sec:related works}

\noindent \textbf{Memory-augmented neural networks.}\ 
Designing architectures capable of memorization and generalization through knowledge abstraction~\citep{sukhbaatar2019augmenting} and data-dependent information retention~\citep{zancato2024b} has been a longstanding research focus. 
%%
Early approaches introduced architectures with external memory modules, such as neural turing machines (NTM)~\citep{graves2014neural} and modern Hopfield Networks~\citep{ramsauer2020hopfield}, which utilize pre-defined update rules to manage memory. With the advent of Transformers, some methods employ recurrent Transformer architectures~\citep{dai2019transformer,bulatov2022recurrent} to cache key-value pairs as memory, enabling the reuse of cached information to extend context window sizes. 
%%
Additionally, recent studies have explored encoding training data into model parameters, effectively using them as memory to store world knowledge~\citep{wang2024self,padmanabhan2024propagating,gangadhar-stratos-2024-model,he2024mixture}. This approach has also been extended to large databases~\citep{qian2024memorag}, test-time data points~\citep{sun2024learning}, and broader language modeling tasks~\citep{yang2024text}.
%%
Titans~\citep{behrouz2024titans} integrates long-term, short-term, and persistent memory into a unified neural architecture.
%%
While optimizing memory retention, they overlook retrieval reliability from model parameters, which is a core design motivation of \ourmethod{}.

\noindent \textbf{Context compression.}\ 
% Compressing lengthy contexts into concise representations while retaining essential information is an efficient strategy to reduce token consumption during LLM inference~\citep{choi2022prompt,li2024prompt}.
Compressing lengthy contexts into concise representations that retains essential information can make LLM inference more efficient~\citep{choi2022prompt,li2024prompt}.
%%
Approaches like Selective Context~\citep{li-etal-2023-compressing}, LLMLingua~\citep{jiang-etal-2023-llmlingua,jiang-etal-2024-longllmlingua} and RECOMP~\citep{xu2023recomp} use context selection to improve inference efficiency, and methods such as AutoCompressor~\citep{chevalier-etal-2023-adapting}, in-context autoencoder (ICAE)~\citep{ge2023context}, Gist~\citep{mu2024learning} and CompAct~\citep{yoon-etal-2024-compact} employ training-based techniques to generate summary representations. 
%%
Besides, \citet{deletang2023language} proposes new general-purpose language modeling perspectives by leveraging compression through arithmetic coding from information theory~\citep{rissanen1976generalized,pasco1976source}. 
%%
In contrast, \ourmethod{} uses context compression as a surrogate task to optimize memory retention while ensuring alignment through backward context expansion.