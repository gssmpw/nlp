[
  {
    "index": 0,
    "papers": [
      {
        "key": "sukhbaatar2019augmenting",
        "author": "Sukhbaatar, Sainbayar and Grave, Edouard and Lample,\nGuillaume and Jegou, Herve and Joulin, Armand",
        "title": "Augmenting self-attention with persistent memory"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zancato2024b",
        "author": "Zancato, Luca and Seshadri, Arjun and Dukler, Yonatan and\nGolatkar, Aditya and Shen, Yantao and Bowman, Benjamin and\nTrager, Matthew and Achille, Alessandro and Soatto,\nStefano",
        "title": "B'MOJO: Hybrid State Space Realizations of Foundation\nModels with Eidetic and Fading Memory"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "graves2014neural",
        "author": "Graves, Alex",
        "title": "Neural Turing Machines"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ramsauer2020hopfield",
        "author": "Ramsauer, Hubert and Sch{\\\"a}fl, Bernhard and Lehner,\nJohannes and Seidl, Philipp and Widrich, Michael and Adler,\nThomas and Gruber, Lukas and Holzleitner, Markus and\nPavlovi{\\'c}, Milena and Sandve, Geir Kjetil and others",
        "title": "Hopfield networks is all you need"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "dai2019transformer",
        "author": "Dai, Zihang",
        "title": "Transformer-xl: Attentive language models beyond a\nfixed-length context"
      },
      {
        "key": "bulatov2022recurrent",
        "author": "Bulatov, Aydar and Kuratov, Yury and Burtsev, Mikhail",
        "title": "Recurrent memory transformer"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2024self",
        "author": "Wang, Yu and Liu, Xinshuang and Chen, Xiusi and O'Brien,\nSean and Wu, Junda and McAuley, Julian",
        "title": "Self-Updatable Large Language Models with Parameter\nIntegration"
      },
      {
        "key": "padmanabhan2024propagating",
        "author": "Padmanabhan, Shankar and Onoe, Yasumasa and Zhang, Michael\nand Durrett, Greg and Choi, Eunsol",
        "title": "Propagating knowledge updates to lms through\ndistillation"
      },
      {
        "key": "gangadhar-stratos-2024-model",
        "author": "Gangadhar, Govind Krishnan and Stratos, Karl",
        "title": "Model Editing by Standard Fine-Tuning"
      },
      {
        "key": "he2024mixture",
        "author": "He, Xu Owen",
        "title": "Mixture of a million experts"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "qian2024memorag",
        "author": "Qian, Hongjin and Zhang, Peitian and Liu, Zheng and Mao,\nKelong and Dou, Zhicheng",
        "title": "Memorag: Moving towards next-gen rag via memory-inspired\nknowledge discovery"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "sun2024learning",
        "author": "Sun, Yu and Li, Xinhao and Dalal, Karan and Xu, Jiarui and\nVikram, Arjun and Zhang, Genghan and Dubois, Yann and Chen,\nXinlei and Wang, Xiaolong and Koyejo, Sanmi and others",
        "title": "Learning to (learn at test time): Rnns with expressive\nhidden states"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "yang2024text",
        "author": "Yang, Hongkang and Lin, Zehao and Wang, Wenjin and Wu, Hao\nand Li, Zhiyu and Tang, Bo and Wei, Wenqiang and Wang,\nJinbo and Tang, Zeyun and Song, Shichao and others",
        "title": "Memory$^3$: Language Modeling with Explicit Memory"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "behrouz2024titans",
        "author": "Behrouz, Ali and Zhong, Peilin and Mirrokni, Vahab",
        "title": "Titans: Learning to Memorize at Test Time"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "choi2022prompt",
        "author": "Choi, Eunbi and Jo, Yongrae and Jang, Joel and Seo,\nMinjoon",
        "title": "Prompt injection: Parameterization of fixed inputs"
      },
      {
        "key": "li2024prompt",
        "author": "Li, Zongqian and Liu, Yinhong and Su, Yixuan and Collier,\nNigel",
        "title": "Prompt compression for large language models: A survey"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "choi2022prompt",
        "author": "Choi, Eunbi and Jo, Yongrae and Jang, Joel and Seo,\nMinjoon",
        "title": "Prompt injection: Parameterization of fixed inputs"
      },
      {
        "key": "li2024prompt",
        "author": "Li, Zongqian and Liu, Yinhong and Su, Yixuan and Collier,\nNigel",
        "title": "Prompt compression for large language models: A survey"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "li-etal-2023-compressing",
        "author": "Li, Yucheng and Dong, Bo and Guerin, Frank and Lin,\nChenghua",
        "title": "Compressing Context to Enhance Inference Efficiency of\nLarge Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "jiang-etal-2023-llmlingua",
        "author": "Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and\nYang, Yuqing and Qiu, Lili",
        "title": "{LLML}ingua: Compressing Prompts for Accelerated Inference\nof Large Language Models"
      },
      {
        "key": "jiang-etal-2024-longllmlingua",
        "author": "Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Li,\nDongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili",
        "title": "{L}ong{LLML}ingua: Accelerating and Enhancing {LLM}s in\nLong Context Scenarios via Prompt Compression"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "xu2023recomp",
        "author": "Xu, Fangyuan and Shi, Weijia and Choi, Eunsol",
        "title": "Recomp: Improving retrieval-augmented lms with compression\nand selective augmentation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "chevalier-etal-2023-adapting",
        "author": "Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh\nand Chen, Danqi",
        "title": "Adapting Language Models to Compress Contexts"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "ge2023context",
        "author": "Ge, Tao and Hu, Jing and Wang, Lei and Wang, Xun and Chen,\nSi-Qing and Wei, Furu",
        "title": "In-context autoencoder for context compression in a large\nlanguage model"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "mu2024learning",
        "author": "Mu, Jesse and Li, Xiang and Goodman, Noah",
        "title": "Learning to compress prompts with gist tokens"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "yoon-etal-2024-compact",
        "author": "Yoon, Chanwoong and Lee, Taewhoo and Hwang, Hyeon and\nJeong, Minbyul and Kang, Jaewoo",
        "title": "{C}omp{A}ct: Compressing Retrieved Documents Actively for\nQuestion Answering"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "deletang2023language",
        "author": "Del{\\'e}tang, Gr{\\'e}goire and Ruoss, Anian and Duquenne,\nPaul-Ambroise and Catt, Elliot and Genewein, Tim and\nMattern, Christopher and Grau-Moya, Jordi and Wenliang, Li\nKevin and Aitchison, Matthew and Orseau, Laurent and others",
        "title": "Language modeling is compression"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "rissanen1976generalized",
        "author": "Rissanen, Jorma J",
        "title": "Generalized Kraft inequality and arithmetic coding"
      },
      {
        "key": "pasco1976source",
        "author": "Pasco, Richard Clark",
        "title": "Source coding algorithms for fast data compression"
      }
    ]
  }
]