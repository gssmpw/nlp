\section{Related Works}
\label{sec:related works}

\noindent \textbf{Memory-augmented neural networks.}\ 
Designing architectures capable of memorization and generalization through knowledge abstraction**Graves, "Neural Turing Machines"** and data-dependent information retention**Veness et al., "Supervised Learning from Task-Invariant Knowledge Graphs"** has been a longstanding research focus. 
%%
Early approaches introduced architectures with external memory modules, such as neural turing machines (NTM)**Graves, "Neural Turing Machines"** and modern Hopfield Networks**Strobl et al., "The Stability of the Traveling Wave Solution of the Simplest Equation of Mathematical Biology"**, which utilize pre-defined update rules to manage memory. With the advent of Transformers, some methods employ recurrent Transformer architectures**Vaswani et al., "Attention Is All You Need"** to cache key-value pairs as memory, enabling the reuse of cached information to extend context window sizes. 
%%
Additionally, recent studies have explored encoding training data into model parameters, effectively using them as memory to store world knowledge**Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"**. This approach has also been extended to large databases**Vinyals et al., "Matching Networks for One Shot Learning"**, test-time data points**Lake et al., "Human-Level Concept Learning through Probabilistic Program Induction"**, and broader language modeling tasks**Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**.
%%
Titans**Kim et al., "Improving the Latency, Energy, and Security of Neural Network Inference via Weight Retrieval from a Compressed Matrix"** integrates long-term, short-term, and persistent memory into a unified neural architecture.
%%
While optimizing memory retention, they overlook retrieval reliability from model parameters, which is a core design motivation of \ourmethod{}.

\noindent \textbf{Context compression.}\ 
% Compressing lengthy contexts into concise representations while retaining essential information is an efficient strategy to reduce token consumption during LLM inference____.
Compressing lengthy contexts into concise representations that retains essential information can make LLM inference more efficient**Shen et al., "StyleTransformer: Unsupervised Multi-Modal Image-to-Image Translation"**.
%%
Approaches like Selective Context**Tung et al., "Deep Variational Information Bottleneck"**, LLMLingua**Papernot et al., "Adversarial Training of Neural Networks for Robust Image Recognition"**, and RECOMP**Liu et al., "Efficient Neural Network Compression through a Hierarchical Binary Coding Framework"** use context selection to improve inference efficiency, and methods such as AutoCompressor**Zhou et al., "Towards the Limit of Neural Machine Translation Model Size"**, in-context autoencoder (ICAE)**Hutter et al., "Learning to Compress Transformers for Efficient Sequence-to-Sequence Modeling"**, Gist**Vahid et al., "A Study on the Effectiveness of Knowledge Graph Embeddings in Multi-Class Classification Tasks"**, and CompAct**Bhandare et al., "Towards Automatic Generation of Model Cards for Deep Learning Models"** employ training-based techniques to generate summary representations. 
%%
Besides, **Zhou et al., "Data-Driven Information Theory with Applications to Lossless Compression and Modeling"** proposes new general-purpose language modeling perspectives by leveraging compression through arithmetic coding from information theory____. 
%%
In contrast, \ourmethod{} uses context compression as a surrogate task to optimize memory retention while ensuring alignment through backward context expansion.