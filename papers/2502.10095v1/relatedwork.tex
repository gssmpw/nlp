\section{Related Work}
The study of out-of-distribution (OOD) data handling spans multiple research directions, each contributing to our understanding of model behavior and robustness. This section reviews key developments in OOD detection, representation learning for tabular data, and contrastive learning approaches.
\subsection{OOD detection}
% \textbf{OpenMax} \citep{Bendale2016} uses the concept of Meta-Recognition to estimate the probability that an input belongs to an unknown class. OpenMax characterizes the failure of the recognition system and handles unknown/unseen classes during operation. In deep learning, SoftMax calculates  as $ P(y=j|x) = \dfrac{e^{v_j(x)}}{\sum^N_{i=1} e^{v_i(x)} }$
% OpenMax recognizes that in out of distribution (OOD), the denominator of the SoftMax layer does not require the probabilities to sum to 1.

% \textbf{TemperatureScaling} \citep{platt1999probabilistic} is a single-parameter variant of the Platt scaling. In a study by Guo\textit{ et al.}  \citep{Guo2017}, despite its simplicity, temperature scaling is effective in calibrating a model for deep learning. This also suggests that temperature scaling can be used to detect OOD.  Our study uses these two approaches to separate OOD from the dataset and use it as validation data.

% \textbf{Multi-class classification, deep neural networks, Gaussian discriminant analysis (MCCD)} \citep{multiClass} is OOD detection algorithm based deep neural network that claim to have better classification inference performance. It is focuses on finding sperical-decission across classes.
Recent years have seen significant advances in OOD detection methodologies. Traditional approaches like ODIN \citep{Liang2017} and OpenMax \citep{Bendale2016} established foundational techniques for identifying OOD samples. Monte Carlo Dropout \citep{Gal2016}introduced uncertainty estimation as a means of OOD detection, while MCCD \citep{Lee2020} advanced the field with density-based detection methods. However, these methods primarily focus on detection rather than prediction tasks, leaving a gap in handling OOD data post-detection.

Our work primarily utilizes OpenMax and Temperature Scaling. While the original algorithms are not new, both algorithms have received the latest updates and provide better support under PyTorch-ood \citep{kirchheim2022pytorch} compared to MCCD \citep{deepMCCD}.

\subsection{Tabular data prediction}
\textbf{Neural Network-based Methods:}\\
    Multilayer Perceptron (MLP) \citep{mlp,tabr}: A simple deep learning method designed for tabular datasets. \\ 
    Self-Normalizing Neural Networks (SNN)  \citep{snn}: Employs SELU activation to enhance the training of deeper neural networks.

\textbf{Advanced Architectures:}\\
Feature Tokenizer Transformer / FT-Transformer \citep{vaswani2017attentiontransformer}: Modifies the transformer model for tabular datasets, consistently achieving excellent performance.  \\
Residual Network / ResNet \citep{restnet}: Employs parallel hidden layers to effectively capture intricate feature interactions.  \\
Deep Cross Network / DCN V2 \citep{DCNV2}: Integrates a feature-crossing component alongside linear layers and multiplications.  \\
Automatic Feature Interaction / AutoInt\citep{AutoInt}: Utilizes attention mechanisms applied to feature embeddings.  \\
Neural Oblivious Decision Ensembles / NODE \citep{NODE}: Represents a differentiable ensemble of oblivious decision trees.  \\
Tabular Network / TabNet \citep{TabNet}: Implements a recurrent design with periodic adjustments to feature weights, emphasizing an attention framework.

\textbf{Ensemble Methods:}\\
   GrowNet \citep{GrowNet}: This method applies gradient boosting to less robust multi-layer perceptrons (MLPs), primarily for tasks involving classification and regression.
\textbf{Gradient Boosting Decision Tree (GBDT) } \citep{Grinsztajn2022} : \\
    XGBoost: A tree-based ensemble technique that utilizes second-order gradients and regularization to avoid overfitting while enhancing computational efficiency.\\
    LightGBM: A rapid and memory-efficient boosting framework that employs histogram-based algorithms and a leaf-wise tree growth approach for quicker training.\\
    CatBoost: A gradient boosting implementation tailored for categorical features, featuring built-in ordered boosting to minimize prediction shifts.

These models have demonstrated varying degrees of success in predicting tabular data. However, their performance on out-of-distribution data remains a critical area for investigation. We include the mentioned models as our base model.

\subsection{Tabular contrastive learning}
    SubTab \citep{subtab} and SCARF \citep{scarf} are a contrastive learning model designed specifically for tabular datasets. They operate under a concept similar to the core idea of contrastive learning for images as described in SimCLR \citep{Chen2020SimClr}. SubTab and SCARF compute contrastive loss using either cosine or Euclidean distance. Both methods are not designed for out-of-distribution problems. We include both methods in our comparison.
    
    CFL \citep{anonymizedMethod} is a federated learning algorithm designed to address vertical partitioning within data silos. CFL investigates the potential of applying contrastive learning to vertically separated data without requiring data exchange. CFL integrates weights by recognizing that the data is derived from a global imaginary dataset that is vertically partitioned. CFL employs contrastive learning as a strategy for black-box learning. CFL emphasizes cooperative learning across different silos. In this research, we examine learning from local data with OOD, an area that has not been previously investigated by CFL. While CFL operates within a federated learning framework, our focus is on standard tabular data. Despite having a similar name, CFL incorporates partial data augmentation as part of its federated learning approach and resembles image contrastive learning. Conversely, TCL utilizes complete matrix augmentation to accommodate tabular data.
%