# background
@article{Lee2020,
archivePrefix = {arXiv},
arxivId = {2104.00941},
author = {Lee, Dongha and Yu, Sehun and Yu, Hwanjo},
doi = {10.1145/3394486.3403189},
eprint = {2104.00941},
file = {:home/ag/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Yu, Yu - 2020 - Multi-Class Data Description for Out-of-distribution Detection.pdf:pdf},
isbn = {9781450379984},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {Gaussian discriminant analysis,deep neural networks,multi-class classification,out-of-distribution detection},
mendeley-groups = {OOD},
month = {aug},
pages = {1362--1370},
publisher = {Association for Computing Machinery},
title = {{Multi-Class Data Description for Out-of-distribution Detection}},
url = {https://dl.acm.org/doi/10.1145/3394486.3403189},
year = {2020}
}

@article{Grinsztajn2022,
author = {Grinsztajn, Leo and Oyallon, Edouard and Varoquaux, Gael},
file = {:home/ag/Downloads/NeurIPS-2022-why-do-tree-based-models-still-outperform-deep-learning-on-typical-tabular-data-Paper-Datasets_and_Benchmarks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {tabular data},
month = {dec},
pages = {507--520},
title = {{Why do tree-based models still outperform deep learning on typical tabular data?}},
volume = {35},
year = {2022}
}


@misc{ahmed2020dedemocratizationaideeplearning,
      title={The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research}, 
      author={Nur Ahmed and Muntasir Wahed},
      year={2020},
      eprint={2010.15581},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2010.15581}, 
}
@article{Hwang2018Background,
   author = {Tim Hwang},
   doi = {10.2139/ssrn.3147971},
   journal = {SSRN Electronic Journal},
   title = {Computational Power and the Social Impact of Artificial Intelligence},
   year = {2018},
}

# end background
@misc{tabr,
      title={TabR: Tabular Deep Learning Meets Nearest Neighbors in 2023}, 
      author={Yury Gorishniy and Ivan Rubachev and Nikolay Kartashev and Daniil Shlenskii and Akim Kotelnikov and Artem Babenko},
      year={2023},
      eprint={2307.14338},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.14338}, 
}

@inproceedings{scarf,
author = {Bahri, Dara and Jiang, Heinrich and Tay, Yi and Metzler, Donald},
booktitle = {ICLR 2022 - 10th International Conference on Learning Representations},
title = {{SCARF: SELF-SUPERVISED CONTRASTIVE LEARNING USING RANDOM FEATURE CORRUPTION}},
year = {2022}
}


@misc{ginanjar2024contrastivefederatedlearningtabular,
      title={Contrastive Federated Learning with Tabular Data Silos}, 
      author={Achmad Ginanjar and Xue Li and Wen Hua},
      year={2024},
      eprint={2409.06123},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.06123}, 
}
anonymized
@misc{anonymizedMethod,
      title={Contrastive Federated Learning with Tabular Data Silos}, 
      author={Achmad Ginanjar and Xue Li and Wen Hua},
      year={2024},
      eprint={2409.06123},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.06123}, 
}
@article{blas,
  title={Basic linear algebra subprograms for Fortran usage},
  author={Lawson, Chuck L and Hanson, Richard J. and Kincaid, David R and Krogh, Fred T.},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={5},
  number={3},
  pages={308--323},
  year={1979},
  publisher={ACM New York, NY, USA}
}

# tabular

@article{decisionTree,
  title={Induction of decision trees},
  author={Quinlan, J. Ross},
  journal={Machine learning},
  volume={1},
  pages={81--106},
  year={1986},
  publisher={Springer}
}


@article{regressionCox,
   abstract = {A sequence of 0’s and 1’s is observed and it is suspected that the chance that a particular trial is a 1 depends on the value of one or more independent variables. Tests and estimates for such situations are considered, dealing first with problems in which the independent variable is preassigned and then with independent variables that are functions of the sequence. There is a considerable amount of earlier work, which is reviewed.},
   author = {D. R. Cox},
   doi = {10.1111/J.2517-6161.1958.TB00292.X},
   issn = {2517-6161},
   issue = {2},
   journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
   month = {7},
   pages = {215-232},
   publisher = {John Wiley & Sons, Ltd},
   title = {The Regression Analysis of Binary Sequences},
   volume = {20},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.2517-6161.1958.tb00292.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1958.tb00292.x https://rss.onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1958.tb00292.x},
   year = {1958},
}
# dataset
@misc{adult_2,
  author       = {Becker, Barry and Kohavi, Ronny},
  title        = {{Adult}},
  year         = {1996},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5XW20}
}
@incollection{helenaJannis,
  title={Analysis of the AutoML Challenge Series 2015-2018},
  author={Guyon, Isabelle and Sun-Hosoya, Lisheng and Boull{\'e}, Marc and Escalante, Hugo Jair and Escalera, Sergio and Liu, Zhengying and Jajetic, Damir and Ray, Bisakha and Saeed, Mehreen and Sebag, Mich{\`e}le and Statnikov, Alexander and Tu, Wei-Wei and Viegas, Evelyne},
  booktitle={AutoML},
  series={Challenges in Machine Learning},
  publisher={Springer},
  year={2019}
}
@article{ca,
  title={Sparse spatial autoregressions},
  author={Pace, R. Kelley and Barry, Ronald},
  journal={Statistics \& Probability Letters},
  volume={33},
  number={3},
  pages={291--297},
  year={1997},
  publisher={Elsevier}
}
@article{higgssmall,
  title={Searching for exotic particles in high-energy physics with deep learning},
  author={Baldi, Pierre and Sadowski, Peter and Whiteson, Daniel},
  journal={Nature Communications},
  volume={5},
  year={2014},
  publisher={Nature Publishing Group},
  doi={10.1038/ncomms5308},
  pages={4308}
}
@article{aloi,
  title={The Amsterdam Library of Object Images},
  author={Geusebroek, Jan-Mark and Burghouts, Gertjan J. and Smeulders, Arnold W. M.},
  journal={International Journal of Computer Vision},
  volume={61},
  number={1},
  pages={103--112},
  year={2005},
  publisher={Springer},
  doi={10.1023/B:VISI.0000042993.50813.60}
}
@inproceedings{year,
  title={The Million Song Dataset},
  author={Bertin-Mahieux, Thierry and Ellis, Daniel P.W. and Whitman, Brian and Lamere, Paul},
  booktitle={Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011)},
  year={2011},
  month={October},
  address={Miami, Florida, USA},
  pages={591--596},
  url={http://ismir2011.ismir.net/papers/OS6-1.pdf}
}
@article{covtype,
  title={Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables},
  author={Blackard, Jock A. and Dean, Denis J.},
  journal={Computers and Electronics in Agriculture},
  volume={24},
  number={3},
  pages={131--151},
  year={2000},
  publisher={Elsevier},
  doi={10.1016/S0168-1699(99)00046-0}
}
@inproceedings{chapelle2011yahoo,
  title={Yahoo! learning to rank challenge overview},
  author={Chapelle, Olivier and Chang, Yi},
  booktitle={Proceedings of the Learning to Rank Challenge},
  year={2011},
  volume={14},
  pages={1--24},
  series={Proceedings of Machine Learning Research},
  publisher={PMLR},
  url={http://proceedings.mlr.press/v14/chapelle11a.html}
}
@article{microsoft,
  title={Introducing {LETOR} 4.0 Datasets},
  author={Qin, Tao and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1306.2597},
  year={2013},
  eprint={1306.2597},
  archivePrefix={arXiv},
  primaryClass={cs.IR},
  url={https://arxiv.org/abs/1306.2597}
}
@misc{epsilon2008,
  title={{Epsilon Dataset: Simulated Physics Experiments}},
  author={{PASCAL Challenge on Large Scale Learning}},
  year={2008},
  howpublished={\url{http://largescale.ml.tu-berlin.de/instructions/}},
  note={Accessed: [Insert Access Date]}
}

# speed /accuracy trade off
@misc{huang2017speedaccuracy,
      title={Speed/accuracy trade-offs for modern convolutional object detectors}, 
      author={Jonathan Huang and Vivek Rathod and Chen Sun and Menglong Zhu and Anoop Korattikara and Alireza Fathi and Ian Fischer and Zbigniew Wojna and Yang Song and Sergio Guadarrama and Kevin Murphy},
      year={2017},
      eprint={1611.10012},
      archivePrefix={arXiv},

}
# conventional

@book{seber2012linear,
  title={Linear regression analysis},
  author={Seber, George AF and Lee, Alan J},
  year={2012},
  publisher={John Wiley \& Sons}
}
@book{kleinbaum2002logistic,
  title={Logistic regression},
  author={Kleinbaum, David G and Dietz, K and Gail, M and Klein, Mitchel and Klein, Mitchell},
  year={2002},
  publisher={Springer}
}
@article{peterson2009k,
  title={K-nearest neighbor.},
  author={Peterson, Leif E},
  journal={Scholarpedia},
  volume={4},
  number={2},
  pages={1883},
  year={2009}
}
@article{song2015decision,
  title={Decision tree methods: applications for classification and prediction},
  author={Song, Yan-Yan and Ying, LU},
  journal={Shanghai archives of psychiatry},
  volume={27},
  number={2},
  pages={130},
  year={2015},
  publisher={Shanghai Mental Health Center}
}
@article{biau2016random,
  title={A random forest guided tour},
  author={Biau, G{\'e}rard and Scornet, Erwan},
  journal={Test},
  volume={25},
  pages={197--227},
  year={2016},
  publisher={Springer}
}

@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}
# code


@misc{tclCode,
  title        = {Tabular Contrastive Learning (TCL)},
  author = {Achmad Ginanjar},
  howpublished = "[Online]. Available from: \url{https://github/anonymized}",
  year={2024},
  month=jul # "~12",
}

# ood
@article{ben2010theory,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  pages={151--175},
  year={2010},
  publisher={Springer}
}
@article{mansour2008domain,
  title={Domain adaptation with multiple sources},
  author={Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}
@misc{Hsu2020,
   author = {Yen-Chang Hsu and Yilin Shen and Hongxia Jin and Zsolt Kira},
   pages = {10951-10960},
   title = {Generalized ODIN: Detecting Out-of-Distribution Image Without Learning From Out-of-Distribution Data},
   year = {2020},
}


@article{OODBaseline,

   author = {Dan Hendrycks and Kevin Gimpel},
   isbn = {1610.02136v3},
   journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
   month = {10},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks},
   url = {https://arxiv.org/abs/1610.02136v3},
   year = {2016},
}
@article{ ConformalPrediction,
   author = {Anastasios N. Angelopoulos and Stephen Bates},
   month = {7},
   title = {A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification},
   url = {http://arxiv.org/abs/2107.07511},
   year = {2021},
}


# Tabular data
@article{mlp,
  title={Feature selection using a multilayer perceptron},
  author={Ruck, Dennis W and Rogers, Steven K and Kabrisky, Matthew},
  journal={Journal of neural network computing},
  volume={2},
  number={2},
  pages={40--48},
  year={1990},
  publisher={Citeseer}
}
@article{restnet,
  title={ReST-Net: Diverse activation modules and parallel subnets-based CNN for spatial image steganalysis},
  author={Li, Bin and Wei, Weihang and Ferreira, Anselmo and Tan, Shunquan},
  journal={IEEE Signal Processing Letters},
  volume={25},
  number={5},
  pages={650--654},
  year={2018},
  publisher={IEEE}
}
`@article{vaswani2017attentiontransformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{snn,
   author = {Günter Klambauer and Thomas Unterthiner and Andreas Mayr and Sepp Hochreiter},
   journal = {31st Conference on Neural Information Processing Systems},
   title = {Self-Normalizing Neural Networks},
   year = {2017},
}

@article{NODE,
   author = {Sergei Popov and Stanislav Morozov and Artem Babenko},
   journal = {8th International Conference on Learning Representations, ICLR 2020},
   month = {9},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data},
   url = {https://arxiv.org/abs/1909.06312v2},
   year = {2019},
}
@article{TabNet,
   author = {Sercan Arık and Tomas Pfister},
   doi = {10.1609/aaai.v35i8.16826},
   isbn = {9781713835974},
   issn = {2159-5399},
   journal = {35th AAAI Conference on Artificial Intelligence, AAAI 2021},
   month = {8},
   pages = {6679-6687},
   publisher = {Association for the Advancement of Artificial Intelligence},
   title = {TabNet: Attentive Interpretable Tabular Learning},
   volume = {8A},
   url = {https://arxiv.org/abs/1908.07442v5},
   year = {2019},
}
@article{GrowNet,
   author = {Sarkhan Badirli and Xuanqing Liu and Zhengming Xing and Avradeep Bhowmik and Khoa Doan and Sathiya S. Keerthi},
   month = {2},
   title = {Gradient Boosting Neural Networks: GrowNet},
   url = {https://arxiv.org/abs/2002.07971v2},
   year = {2020},
}
@article{DCNV2,
   author = {Ruoxi Wang and Rakesh Shivanna and Derek Z. Cheng and Sagar Jain and Dong Lin and Lichan Hong and Ed H. Chi},
   doi = {10.1145/3442381.3450078},
   journal = {The Web Conference 2021 - Proceedings of the World Wide Web Conference, WWW 2021},
   keywords = {CTR Prediction,Deep Learning,Feature Crossing,Neural Networks},
   month = {8},
   pages = {1785-1797},
   publisher = {Association for Computing Machinery, Inc},
   title = {DCN V2: Improved Deep and Cross Network and Practical Lessons for Web-scale Learning to Rank Systems},
   url = {http://arxiv.org/abs/2008.13535 http://dx.doi.org/10.1145/3442381.3450078},
   year = {2020},
}
@article{AutoInt,
   author = {Weiping Song and Chence Shi and Zhiping Xiao and Zhijian Duan and Yewen Xu and Ming Zhang and Jian Tang},
   doi = {10.1145/3357384.3357925},
   journal = {International Conference on Information and Knowledge Management, Proceedings},
   keywords = {CTR prediction,Explainable recommendation,High-order feature interactions,Self attention},
   month = {10},
   pages = {1161-1170},
   publisher = {Association for Computing Machinery},
   title = {AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks},
   volume = {10},
   url = {http://arxiv.org/abs/1810.11921 http://dx.doi.org/10.1145/3357384.3357925},
   year = {2018},
}
@article{XGBoost,
   author = {Tianqi Chen and Carlos Guestrin},
   doi = {10.1145/2939672.2939785},
   isbn = {9781450342322},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {Large-scale Machine learning,Learning,Machine},
   month = {3},
   pages = {785-794},
   publisher = {Association for Computing Machinery},
   title = {XGBoost: A Scalable Tree Boosting System},
   volume = {13-17-August-2016},
   url = {https://arxiv.org/abs/1603.02754v3},
   year = {2016},
}
@article{CatBoost,
   author = {Liudmila Prokhorenkova and Gleb Gusev and Aleksandr Vorobev and Anna Veronika Dorogush and Andrey Gulin},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {6},
   pages = {6638-6648},
   publisher = {Neural information processing systems foundation},
   title = {CatBoost: unbiased boosting with categorical features},
   volume = {2018-December},
   url = {https://arxiv.org/abs/1706.09516v5},
   year = {2017},
}

# Detector
@InProceedings{kirchheim2022pytorch,
    author    = {Kirchheim, Konstantin and Filax, Marco and Ortmeier, Frank},
    title     = {PyTorch-OOD: A Library for Out-of-Distribution Detection Based on PyTorch},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2022},
    pages     = {4351-4360}
}

@misc{deepMCCD,
author = {Lee, Dongha and Yu, Sehun and Yu, Hwanjo},
title = {Multi-Class Data Description for Out-of-Distribution Detection},
year = {2020},
url={https://github.com/donalee/DeepMCDD},
}
@inproceedings{multiClass,
author = {Lee, Dongha and Yu, Sehun and Yu, Hwanjo},
title = {Multi-Class Data Description for Out-of-distribution Detection},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403189},
doi = {10.1145/3394486.3403189},
abstract = {The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we present a deep multi-class data description, termed as Deep-MCDD, which is effective to detect out-of-distribution (OOD) samples as well as classify in-distribution (ID) samples. Unlike the softmax classifier that only focuses on the linear decision boundary partitioning its latent space into multiple regions, our Deep-MCDD aims to find a spherical decision boundary for each class which determines whether a test sample belongs to the class or not. By integrating the concept of Gaussian discriminant analysis into deep neural networks, we propose a deep learning objective to learn class-conditional distributions that are explicitly modeled as separable Gaussian distributions. Thereby, we can define the confidence score by the distance of a test sample from each class-conditional distribution, and utilize it for identifying OOD samples. Our empirical evaluation on multi-class tabular and image datasets demonstrates that Deep-MCDD achieves the best performances in distinguishing OOD samples while showing the classification accuracy as high as the other competitors.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1362–1370},
numpages = {9},
keywords = {out-of-distribution detection, multi-class classification, deep neural networks, Gaussian discriminant analysis},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{platt1999probabilistic,
  title={Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods},
  author={Platt, John and others},
  journal={Advances in large margin classifiers},
  volume={10},
  number={3},
  pages={61--74},
  year={1999},
  publisher={Cambridge, MA}
}
@article{Guo2017,
   author = {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
   isbn = {9781510855144},
   journal = {34th International Conference on Machine Learning, ICML 2017},
   month = {6},
   pages = {2130-2143},
   publisher = {International Machine Learning Society (IMLS)},
   title = {On Calibration of Modern Neural Networks},
   volume = {3},
   url = {https://arxiv.org/abs/1706.04599v2},
   year = {2017},
}

@article{Bendale2016,
   author = {Abhijit Bendale and Terrance E. Boult},
   doi = {10.1109/CVPR.2016.173},
   isbn = {9781467388504},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   month = {12},
   pages = {1563-1572},
   publisher = {IEEE Computer Society},
   title = {Towards open set deep networks},
   volume = {2016-December},
   year = {2016},
}

@misc{Gal2016,
   author = {Yarin Gal and Zoubin Ghahramani},
   issn = {1938-7228},
   month = {6},
   pages = {1050-1059},
   publisher = {PMLR},
   title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
   url = {https://proceedings.mlr.press/v48/gal16.html},
   year = {2016},
}
@article{Hendrycks2016,
   author = {Dan Hendrycks and Kevin Gimpel},
   isbn = {1610.02136v3},
   journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
   month = {10},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks},
   url = {https://arxiv.org/abs/1610.02136v3},
   year = {2016},
}


@article{Liang2017,
   author = {Shiyu Liang and Yixuan Li and R. Srikant},
   journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
   month = {6},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},
   url = {https://arxiv.org/abs/1706.02690v5},
   year = {2017},
}
@article{Lee2018,
   author = {Kimin Lee and Kibok Lee and Honglak Lee and Jinwoo Shin},
   isbn = {1807.03888v2},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {7},
   pages = {7167-7177},
   publisher = {Neural information processing systems foundation},
   title = {A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},
   volume = {2018-December},
   url = {https://arxiv.org/abs/1807.03888v2},
   year = {2018},
}
@article{Liu2020,
   author = {Weitang Liu and Xiaoyun Wang and John D. Owens and Yixuan Li},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {10},
   publisher = {Neural information processing systems foundation},
   title = {Energy-based Out-of-distribution Detection},
   volume = {2020-December},
   url = {https://arxiv.org/abs/2010.03759v4},
   year = {2020},
}
@article{Chan2020,
   author = {Robin Chan and Matthias Rottmann and Hanno Gottschalk},
   doi = {10.1109/ICCV48922.2021.00508},
   isbn = {9781665428125},
   issn = {15505499},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   month = {12},
   pages = {5108-5117},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Entropy Maximization and Meta Classification for Out-Of-Distribution Detection in Semantic Segmentation},
   url = {https://arxiv.org/abs/2012.06575v2},
   year = {2020},
}
@article{Hendrycks2019,
   author = {Dan Hendrycks and Steven Basart and Mantas Mazeika and Andy Zou and Joe Kwon and Mohammadreza Mostajabi and Jacob Steinhardt and Dawn Song},
   issn = {26403498},
   journal = {Proceedings of Machine Learning Research},
   month = {11},
   pages = {8759-8773},
   publisher = {ML Research Press},
   title = {Scaling Out-of-Distribution Detection for Real-World Settings},
   volume = {162},
   url = {https://arxiv.org/abs/1911.11132v4},
   year = {2019},
}
@article{Wang2022,
   author = {Haoqi Wang and Zhizhong Li and Litong Feng and Wayne Zhang},
   doi = {10.1109/CVPR52688.2022.00487},
   isbn = {9781665469463},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   keywords = {Datasets and evaluation,Recognition: detection,Self-& semi-& meta- & unsupervised learning,categorization,retrieval},
   month = {3},
   pages = {4911-4920},
   publisher = {IEEE Computer Society},
   title = {ViM: Out-Of-Distribution with Virtual-logit Matching},
   volume = {2022-June},
   url = {https://arxiv.org/abs/2203.10807v1},
   year = {2022},
}

##########

@article{Gorishniy2021,
   author = {Yury Gorishniy and Ivan Rubachev and Valentin Khrulkov and Artem Babenko},
   isbn = {9781713845393},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {6},
   pages = {18932-18943},
   publisher = {Neural information processing systems foundation},
   title = {Revisiting Deep Learning Models for Tabular Data},
   volume = {23},
   url = {https://arxiv.org/abs/2106.11959v2},
   year = {2021},
}

@article{Sreekar2023,
   abstract = {Amazon ships billions of packages to its customers annually within the United States. Shipping cost of these packages are used on the day of shipping (day 0) to estimate profitability of sales. Downstream systems utilize these days 0 profitability estimates to make financial decisions, such as pricing strategies and delisting loss-making products. However, obtaining accurate shipping cost estimates on day 0 is complex for reasons like delay in carrier invoicing or fixed cost components getting recorded at monthly cadence. Inaccurate shipping cost estimates can lead to bad decision, such as pricing items too low or high, or promoting the wrong product to the customers. Current solutions for estimating shipping costs on day 0 rely on tree-based models that require extensive manual engineering efforts. In this study, we propose a novel architecture called the Rate Card Transformer (RCT) that uses self-attention to encode all package shipping information such as package attributes, carrier information and route plan. Unlike other transformer-based tabular models, RCT has the ability to encode a variable list of one-to-many relations of a shipment, allowing it to capture more information about a shipment. For example, RCT can encode properties of all products in a package. Our results demonstrate that cost predictions made by the RCT have 28.82% less error compared to tree-based GBDT model. Moreover, the RCT outperforms the state-of-the-art transformer-based tabular model, FTTransformer, by 6.08%. We also illustrate that the RCT learns a generalized manifold of the rate card that can improve the performance of tree-based models.},
   author = {Aditya Sreekar and Sreekarp@amazon Com and Sahil Verma and Varun Madhavan and Varunmadhavan@iitkgp Ac In and Abhishek Persad},
   journal = {Proceedings of Machine Learning Research},
   month = {11},
   pages = {2023-2023},
   title = {Unveiling the Power of Self-Attention for Shipping Cost Prediction: The Rate Card Transformer},
   volume = {222},
   url = {https://arxiv.org/abs/2311.11694v1},
   year = {2023},
}

@article{Huang2020,
   abstract = {We propose TabTransformer, a novel deep tabular data modeling architecture for supervised and semi-supervised learning. The TabTransformer is built upon self-attention based Transformers. The Transformer layers transform the embeddings of categorical features into robust contextual embeddings to achieve higher prediction accuracy. Through extensive experiments on fifteen publicly available datasets, we show that the TabTransformer outperforms the state-of-the-art deep learning methods for tabular data by at least 1.0% on mean AUC, and matches the performance of tree-based ensemble models. Furthermore, we demonstrate that the contextual embeddings learned from TabTransformer are highly robust against both missing and noisy data features, and provide better interpretability. Lastly, for the semi-supervised setting we develop an unsupervised pre-training procedure to learn data-driven contextual embeddings, resulting in an average 2.1% AUC lift over the state-of-the-art methods.},
   author = {Xin Huang and Ashish Khetan and Milan Cvitkovic and Zohar Karnin},
   month = {12},
   title = {TabTransformer: Tabular Data Modeling Using Contextual Embeddings},
   url = {https://arxiv.org/abs/2012.06678v1},
   year = {2020},
}
@article{Fort2021,
   abstract = {Near out-of-distribution detection (OOD) is a major challenge for deep neural networks. We demonstrate that large-scale pre-trained transformers can significantly improve the state-of-the-art (SOTA) on a range of near OOD tasks across different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD detection , we improve the AUROC from 85% (current SOTA) to 96% using Vision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD detection benchmark, we improve the AUROC from 66% to 77% using transformers and unsupervised pre-training. To further improve performance, we explore the few-shot outlier exposure setting where a few examples from outlier classes may be available; we show that pre-trained transformers are particularly well-suited for outlier exposure, and that the AUROC of OOD detection on CIFAR-100 vs CIFAR-10 can be improved to 98.7% with just 1 image per OOD class, and 99.46% with 10 images per OOD class. For multi-modal image-text pre-trained transformers such as CLIP, we explore a new way of using just the names of outlier classes as a sole source of information without any accompanying images, and show that this outperforms previous SOTA on standard vision OOD benchmark tasks.},
   author = {Stanislav Fort and Jie Ren and Balaji Lakshminarayanan},
   journal = {Advances in Neural Information Processing Systems},
   month = {12},
   pages = {7068-7081},
   title = {Exploring the Limits of Out-of-Distribution Detection},
   volume = {34},
   year = {2021},
}

@article{VFL,
   abstract = {Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. We propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.},
   author = {Yang Liu and Yan Kang and Tianyuan Zou and Yanhong Pu and Yuanqin He and Xiaozhou Ye and Ye Ouyang and Ya-Qin Zhang and Qiang Yang},
   title = {Vertical Federated Learning: Concepts, Advances and Challenges},
}
@article{AsymVFL,
   abstract = {Federated learning is a distributed machine learning method that aims to preserve the privacy of sample features and labels. In a federated learning system, ID-based sample alignment approaches are usually applied with few efforts made on the protection of ID privacy. In real-life applications, however, the confidentiality of sample IDs, which are the strongest row identifiers, is also drawing much attention from many participants. To relax their privacy concerns about ID privacy, this paper formally proposes the notion of asymmetrical vertical federated learning and illustrates the way to protect sample IDs. The standard private set intersection protocol is adapted to achieve the asymmetrical ID alignment phase in an asymmetrical vertical federated learning system. Correspondingly, a Pohlig-Hellman realization of the adapted protocol is provided. This paper also presents a genuine with dummy approach to achieving asymmetrical feder-ated model training. To illustrate its application, a federated logistic regression algorithm is provided as an example. Experiments are also made for validating the feasibility of this approach.},
   author = {Yang Liu and Xiong Zhang and Libin Wang},
   title = {Asymmetrical Vertical Federated Learning},
}
@article{VAFL,
   abstract = {Horizontal Federated learning (FL) handles multi-client data that share the same set of features, and vertical FL trains a better predictor that combine all the features from different clients. This paper targets solving vertical FL in an asynchronous fashion, and develops a simple FL method. The new method allows each client to run stochas-tic gradient algorithms without coordination with other clients, so it is suitable for intermittent con-nectivity of clients. This method further uses a new technique of perturbed local embedding to ensure data privacy and improve communication efficiency. Theoretically, we present the convergence rate and privacy level of our method for strongly convex, nonconvex and even nonsmooth objectives separately. Empirically, we apply our method to FL on various image and healthcare datasets. The results compare favorably to centralized and synchronous FL methods.},
   author = {Tianyi Chen and Xiao Jin and Yuejiao Sun and Wotao Yin},
   title = {VAFL: a Method of Vertical Asynchronous Federated Learning},
}
@article{CFLChallenge,
   abstract = {Recently, federated learning (FL) has emerged as a promising distributed machine learning (ML) technology, owing to the advancing computational and sensing capacities of end-user devices, however with the increasing concerns on users' privacy. As a special architecture in FL, vertical FL (VFL) is capable of constructing a hyper ML model by embracing sub-models from different clients. These sub-models are trained locally by vertically partitioned data with distinct attributes. Therefore, the design of VFL is fundamentally different from that of conventional FL, raising new and unique research issues. In this paper, we aim to discuss key challenges in VFL with effective solutions, and conduct experiments on real-life datasets to shed light on these issues. Specifically, we first propose a general framework on VFL, and highlight the key differences between VFL and conventional FL. Then, we discuss research challenges rooted in VFL systems under four aspects, i.e., security and privacy risks, expensive computation and communication costs, possible structural damage caused by model splitting, and system heterogeneity. Afterwards, we develop solutions to addressing the aforementioned challenges, and conduct extensive experiments to showcase the effectiveness of our proposed solutions.},
   author = {Kang Wei and Student Member and Jun Li and Senior Member and Chuan Ma and Ming Ding and Sha Wei and Fan Wu and Ieee Guihai Chen and Thilina Ranbaduge},
   keywords = {Index Terms-Vertical FL,communication efficiency,privacy preserving,splitting design},
   title = {Vertical Federated Learning: Challenges, Methodologies and Experiments},
   url = {https://github.com/bytedance/fedlearner},
}
@misc{Bhagoji2019,
   abstract = {Federated learning distributes model training among a multitude of agents, who, guided by privacy concerns, perform training using their local data but share only model parameter updates, for iterative aggregation at the server to train an overall global model. In this work, we explore how the federated learning setting gives rise to a new threat, namely model poisoning, different from traditional data poisoning. Model poisoning is carried out by an adversary controlling a small number of malicious agents (usually 1) with the aim of causing the global model to mis-classify a set of chosen inputs with high confidence. We explore a number of attack strategies for deep neural networks, starting with targeted model poisoning using boosting of the malicious agent's update to overcome the effects of other agents. We also propose two critical notions of stealth to detect malicious updates. We bypass these by including them in the adversar-ial objective to carry out stealthy model poisoning. We improve attack stealth with the use of an alternating minimization strategy which alternately optimizes for stealth and the adversarial objective. We also empirically demonstrate that Byzantine-resilient aggregation strategies are not robust to our attacks. Our results show that effective and stealthy model poisoning attacks are possible, highlighting vulnerabilities in the fed-erated learning setting.},
   author = {Arjun Nitin Bhagoji and Supriyo Chakraborty and Prateek Mittal and Seraphin Calo},
   issn = {2640-3498},
   month = {5},
   pages = {634-643},
   publisher = {PMLR},
   title = {Analyzing Federated Learning through an Adversarial Lens},
   url = {https://proceedings.mlr.press/v97/bhagoji19a.html},
   year = {2019},
}
@article{Lyu2020,
   abstract = {With the emergence of data silos and popular privacy awareness, the traditional centralized approach of training artificial intelligence (AI) models is facing strong challenges. Federated learning (FL) has recently emerged as a promising solution under this new reality. Existing FL protocol design has been shown to exhibit vulnerabilities which can be exploited by adversaries both within and without the system to compromise data privacy. It is thus of paramount importance to make FL system designers to be aware of the implications of future FL algorithm design on privacy-preservation. Currently, there is no survey on this topic. In this paper, we bridge this important gap in FL literature. By providing a concise introduction to the concept of FL, and a unique taxonomy covering threat models and two major attacks on FL: 1) poisoning attacks and 2) inference attacks, this paper provides an accessible review of this important topic. We highlight the intuitions, key techniques as well as fundamental assumptions adopted by various attacks, and discuss promising future research directions towards more robust privacy preservation in FL.},
   author = {Lingjuan Lyu and Han Yu and Qiang Yang},
   journal = {Computing Research Repository (CoRR)},
   month = {3},
   title = {Threats to Federated Learning: A Survey},
   url = {https://arxiv.org/abs/2003.02133v1},
   year = {2020},
}
@article{Bouacida2021,
   abstract = {With more regulations tackling the protection of users' privacy-sensitive data in recent years, access to such data has become increasingly restricted. A new decentralized training paradigm, known as Federated Learning (FL), enables multiple clients located at different geographical locations to learn a machine learning model collaboratively without sharing their data. While FL has recently emerged as a promising solution to preserve users' privacy, this new paradigm's potential security implications may hinder its widespread adoption. The existing FL protocols exhibit new unique vulnerabilities that adversaries can exploit to compromise the trained model. FL is often preferred in learning environments where security and privacy are the key concerns. Therefore, it is crucial to raise awareness of the consequences resulting from the new threats to FL systems. To date, the security of traditional machine learning systems has been widely examined. However, many open challenges and complex questions are still surrounding FL security. In this paper, we bridge the gap in FL literature by providing a comprehensive survey of the unique security vulnerabilities exposed by the FL ecosystem. We highlight the vulnerabilities sources, key attacks on FL, defenses, as well as their unique challenges, and discuss promising future research directions towards more robust FL.},
   author = {Nader Bouacida and Prasant Mohapatra},
   doi = {10.1109/ACCESS.2021.3075203},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Attacks,defenses,federated learning,security threats,vulnerabilities},
   pages = {63229-63249},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Vulnerabilities in Federated Learning},
   volume = {9},
   year = {2021},
}
@article{Fischer2013,
   abstract = {The method of elevation classes has proven to be a useful way for a low-resolution general circulation model (GCM) to produce high-resolution downscaled surface mass balance fields, for use in one-way studies coupling GCMs and ice flow models. Past uses of elevation classes have been a cause of non-conservation of mass and energy, 5 caused by inconsistency in regridding schemes chosen to regrid to the atmosphere vs. downscaling to the ice model. This causes problems for two-way coupling. A strategy that resolves this conservation issue has been designed and is presented here. The approach identifies three grids between which data must be regridded, and five transformations between those grids required by a typical coupled GCM-ice flow 10 model. This paper shows how each of those transformations may be achieved in a consistent , conservative manner. These transformations are implemented in GLINT2, a library used to couple GCMs with ice models. Source code and documentation are available for download. Confounding real-world issues are discussed, including the use of projections for ice modeling, how to handle dynamically changing ice geometry, and 15 modifications required for finite element ice models.},
   author = {R Fischer and S Nowicki and M Kelley and G A Schmidt},
   doi = {10.5194/gmdd-6-6493-2013},
   journal = {Geosci. Model Dev. Discuss},
   pages = {6493-6568},
   title = {A system of conservative regridding for ice/atmosphere coupling in a GCM},
   volume = {6},
   url = {www.geosci-model-dev-discuss.net/6/6493/2013/},
   year = {2013},
}
@article{Oh2017,
   abstract = {The projection skills of five ensemble methods were analyzed according to simulation skills, training period, and ensemble members, using 198 sets of pseudo-simulation data (PSD) produced by random number generation assuming the simulated temperature of regional climate models. The PSD sets were classified into 18 categories according to the relative magnitude of bias, variance ratio, and correlation coefficient, where each category had 11 sets (including 1 truth set) with 50 samples. The ensemble methods used were as follows: equal weighted averaging without bias correction (EWA_NBC), EWA with bias correction (EWA_WBC), weighted ensemble averaging based on root mean square errors and correlation (WEA_RAC), WEA based on the Taylor score (WEA_Tay), and multivariate linear regression (Mul_Reg). The projection skills of the ensemble methods improved generally as compared with the best member for each category. However, their projection skills are significantly affected by the simulation skills of the ensemble member. The weighted ensemble methods showed better projection skills than non-weighted methods, in particular, for the PSD categories having systematic biases and various correlation coefficients. The EWA_NBC showed considerably lower projection skills than the other methods, in particular, for the PSD categories with systematic biases. Although Mul_Reg showed relatively good skills, it showed strong sensitivity to the PSD categories, training periods, and number of members. On the other hand, the WEA_Tay and WEA_RAC showed relatively superior skills in both the accuracy and reliability for all the sensitivity experiments. This indicates that WEA_Tay and WEA_RAC are applicable even for simulation data with systematic biases, a short training period, and a small number of ensemble members.},
   author = {Seok Geun Oh and Myoung Seok Suh},
   doi = {10.1007/S00704-016-1782-1/METRICS},
   issn = {14344483},
   issue = {1-2},
   journal = {Theoretical and Applied Climatology},
   keywords = {Atmospheric Protection/Air Quality Control/Air Pollution,Atmospheric Sciences,Climatology,Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution},
   month = {7},
   pages = {243-262},
   publisher = {Springer-Verlag Wien},
   title = {Comparison of projection skills of deterministic ensemble methods using pseudo-simulation data generated from multivariate Gaussian distribution},
   volume = {129},
   url = {https://link.springer.com/article/10.1007/s00704-016-1782-1},
   year = {2017},
}
@article{Wang2018,
   abstract = {Global climate models (GCMs) are useful tools for assessing climate change impacts on temperature and rainfall. Although climate data from various GCMs have been increasingly used in climate change impact studies, GCMs configurations and module characteristics vary from one to another. Therefore, it is crucial to assess different GCMs to confirm the extent to which they can reproduce the observed temperature and rainfall. Rather than assessing the interdependence of each GCM, the purpose of this study is to compare the capacity of four different multi-model ensemble (MME) methods (random forest [RF], support vector machine [SVM], Bayesian model averaging [BMA] and the arithmetic ensemble mean [EM]) in reproducing observed monthly rainfall and temperature. Of these four methods, the RF and SVM demonstrated a significant improvement over EM and BMA in terms of performance criteria. The relative importance of each GCM based on the RF ensemble in reproducing rainfall and temperature could also be ranked. We compared the GCMs importance and Taylor skill score and found that their correlation was 0.95 for temperature and 0.54 for rainfall. Our results also demonstrated that the number of GCMs ensemble simulations could be reduced from 33 to 25 in RF model while maintaining predictive error less than 2%. Having such a representative subset of simulations could reduce computational costs for climate impact modelling and maintain the quality of ensemble at the same time. We conclude that machine learning MME could be efficient and useful with improved accuracy in reproducing historical climate variables.},
   author = {Bin Wang and Lihong Zheng and De Li Liu and Fei Ji and Anthony Clark and Qiang Yu},
   doi = {10.1002/JOC.5705},
   issn = {1097-0088},
   issue = {13},
   journal = {International Journal of Climatology},
   keywords = {GCMs,machine learning,model ensemble,multi,random forest,support vector machine},
   month = {11},
   pages = {4891-4902},
   publisher = {John Wiley & Sons, Ltd},
   title = {Using multi-model ensembles of CMIP5 global climate models to reproduce observed monthly rainfall and temperature with machine learning methods in Australia},
   volume = {38},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/joc.5705 https://onlinelibrary.wiley.com/doi/abs/10.1002/joc.5705 https://rmets.onlinelibrary.wiley.com/doi/10.1002/joc.5705},
   year = {2018},
}
@article{Ahmed2020,
   abstract = {Multi-Model Ensembles (MMEs) are often employed to reduce the uncertainties related to GCM simulations/projections. The objective of this study was to evaluate the performance of MMEs developed using machine learning (ML) algorithms with different combinations of GCMs ranked based on their performance and determine the optimum number of GCMs to be included in an MME. In this study ML algorithms; Artificial Neural Network (ANN), K-Nearest Neighbour (KNN), Support Vector Machine (SVM) and Relevance Vector Machine (RVM) were used to develop MMEs for annual, monsoon and winter; precipitation (P), maximum (Tmax) and minimum (Tmin) temperature over Pakistan using 36 Coupled Model Intercomparison Project Phase 5 GCMs. GCMs were ranked using Taylor Skill Score for individual seasons and variables, and then using a comprehensive Rating Metric (RM) overall rank of each GCM was determined. It was found that, HadGEM2-AO is the most skilled GCM and IPSL-CM5B-LR is the least skilled GCMs in simulating the 3 climate variables. The performance of MMEs did not improve after the inclusion of about 18 top-ranked GCMs. Thus, it was understood that the optimum performance of MMEs is achieved when about 50% of the top-ranked GCMs are used. The inter-comparison of MMEs developed with ANN, KNN, SVM and RVM revealed that KNN and RVM-based MMEs show better skills. It was found that RVM yields MMEs which show smaller variations in performance over space unlike ANN which displayed large fluctuations in performance over space. KNN and RVM are recommended over SVM and ANN for the development of MMEs over Pakistan.},
   author = {Kamal Ahmed and D. A. Sachindra and Shamsuddin Shahid and Zafar Iqbal and Nadeem Nawaz and Najeebullah Khan},
   doi = {10.1016/J.ATMOSRES.2019.104806},
   issn = {0169-8095},
   journal = {Atmospheric Research},
   keywords = {General circulation models,Machine learning algorithms,Multi-model ensemble,Pakistan,Taylor skill score,Temperature and precipitation},
   month = {5},
   pages = {104806},
   publisher = {Elsevier},
   title = {Multi-model ensemble predictions of precipitation and temperature using machine learning algorithms},
   volume = {236},
   year = {2020},
}
@article{Khosla2020,
   abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsu-pervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4% on the Ima-geNet dataset, which is 0.8% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions, and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement and reference TensorFlow code is released at https://t.ly/supcon 1 .},
   author = {Prannay Khosla and Piotr Teterwak and Chen Wang and Aaron Sarna and Google Research and Yonglong Tian and Phillip Isola and Aaron Maschinot and Ce Liu and Dilip Krishnan},
   journal = {Advances in Neural Information Processing Systems},
   pages = {18661-18673},
   title = {Supervised Contrastive Learning},
   volume = {33},
   url = {https://t.ly/supcon},
   year = {2020},
}
@article{Wang2018,
   abstract = {Distributed model training suffers from communication overheads due to frequent gradient updates transmitted between compute nodes. To mitigate these overheads, several studies propose the use of sparsified stochastic gradients. We argue that these are facets of a general sparsification method that can operate on any possible atomic decomposition. Notable examples include element-wise, singular value, and Fourier decompositions. We present ATOMO, a general framework for atomic sparsification of stochastic gradients. Given a gradient, an atomic decomposition, and a sparsity budget, ATOMO gives a random unbiased sparsification of the atoms minimizing variance. We show that recent methods such as QSGD and TernGrad are special cases of ATOMO and that sparsifiying the singular value decomposition of neural networks gradients, rather than their coordinates, can lead to significantly faster distributed training.},
   author = {Hongyi Wang and Scott Sievert and Zachary Charles and Shengchao Liu and Stephen Wright and Dimitris Papailiopoulos},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {6},
   pages = {9850-9861},
   publisher = {Neural information processing systems foundation},
   title = {ATOMO: Communication-efficient Learning via Atomic Sparsification},
   volume = {2018-December},
   url = {https://arxiv.org/abs/1806.04090v3},
   year = {2018},
}
@article{Alistarh2016,
   abstract = {Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to excellent scalability properties of this algorithm, and to its efficiency in the context of training deep neural networks. A fundamental barrier for parallelizing large-scale SGD is the fact that the cost of communicating the gradient updates between nodes can be very large. Consequently, lossy compression heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always provably converge, and it is not clear whether they are optimal. In this paper, we propose Quantized SGD (QSGD), a family of compression schemes which allow the compression of gradient updates at each node, while guaranteeing convergence under standard assumptions. QSGD allows the user to trade off compression and convergence time: it can communicate a sublinear number of bits per iteration in the model dimension, and can achieve asymptotically optimal communication cost. We complement our theoretical results with empirical data, showing that QSGD can significantly reduce communication cost, while being competitive with standard uncompressed techniques on a variety of real tasks. In particular, experiments show that gradient quantization applied to training of deep neural networks for image classification and automated speech recognition can lead to significant reductions in communication cost, and end-to-end training time. For instance, on 16 GPUs, we are able to train a ResNet-152 network on ImageNet 1.8x faster to full accuracy. Of note, we show that there exist generic parameter settings under which all known network architectures preserve or slightly improve their full accuracy when using quantization.},
   author = {Dan Alistarh and Demjan Grubic and Jerry Z. Li and Ryota Tomioka and Milan Vojnovic},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {10},
   pages = {1710-1721},
   publisher = {Neural information processing systems foundation},
   title = {QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding},
   volume = {2017-December},
   url = {https://arxiv.org/abs/1610.02132v4},
   year = {2016},
}
@article{,
   abstract = {High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work, we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels \{−1, 0, 1\}, which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound, we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet doesn't incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks. Our source code is available 1 .},
   author = {Wei Wen and Cong Xu and Feng Yan and Chunpeng Wu and Yandan Wang and Yiran Chen and Hai Li},
   title = {TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning},
   url = {https://github.com/wenwei202/terngrad},
}
@article{Bernstein2018,
   abstract = {Training large neural networks requires distributing learning across multiple workers, where the cost of communicating gradients can be a significant bottleneck. SIGNSGD alleviates this problem by transmitting just the sign of each minibatch stochastic gradient. We prove that it can get the best of both worlds: compressed gradients and SGD-level convergence rate. The relativè 1 /` 2 geometry of gradients, noise and curvature informs whether SIGNSGD or SGD is theoretically better suited to a particular problem. On the practical side we find that the momentum counterpart of SIGNSGD is able to match the accuracy and convergence speed of ADAM on deep Imagenet models. We extend our theory to the distributed setting, where the parameter server uses majority vote to aggregate gradient signs from each worker enabling 1-bit compression of worker-server communication in both directions. Using a theorem by Gauss (1823) we prove that majority vote can achieve the same reduction in variance as full precision distributed SGD. Thus, there is great promise for sign-based optimisation schemes to achieve fast communication and fast convergence. Code to reproduce experiments is to be found at https://github.com/jxbz/signSGD.},
   author = {Jeremy Bernstein and Yu-Xiang Wang and Kamyar Azizzadenesheli and Anima Anandkumar},
   title = {SIGNSGD: Compressed Optimisation for Non-Convex Problems},
   url = {https://github.com/jxbz/signSGD.},
   year = {2018},
}
@article{,
   abstract = {We address the problem of Federated Learning (FL) where users are distributed and partitioned into clusters. This setup captures settings where different groups of users have their own objectives (learning tasks) but by aggregating their data with others in the same cluster (same learning task), they can leverage the strength in numbers in order to perform more efficient Federated Learning. We propose a new framework dubbed the Iterative Federated Clustering Algorithm (IFCA), which alternately estimates the cluster identities of the users and optimizes model parameters for the user clusters via gradient descent. We analyze the convergence rate of this algorithm first in a linear model with squared loss and then for generic strongly convex and smooth loss functions. We show that in both settings, with good initialization, IFCA converges at an exponential rate, and discuss the optimality of the statistical error rate. When the clustering structure is ambiguous, we propose to train the models by combining IFCA with the weight sharing technique in multi-task learning. In the experiments, we show that our algorithm can succeed even if we relax the requirements on initialization with random initialization and multiple restarts. We also present experimental results showing that our algorithm is efficient in non-convex problems such as neural networks. We demonstrate the benefits of IFCA over the baselines on several clustered FL benchmarks. 2},
   author = {Avishek Ghosh and Jichan Chung and Dong Yin and Kannan Ramchandran},
   title = {An Efficient Framework for Clustered Federated Learning},
   url = {https://github.com/jichan3751/ifca.},
}
@article{Sattler2020,
   abstract = {Federated learning allows multiple parties to jointly train a deep learning model on their combined data, without any of the participants having to reveal their local data to a centralized server. This form of privacy-preserving collaborative learning, however, comes at the cost of a significant communication overhead during training. To address this problem, several compression methods have been proposed in the distributed training literature that can reduce the amount of required communication by up to three orders of magnitude. These existing methods, however, are only of limited utility in the federated learning setting, as they either only compress the upstream communication from the clients to the server (leaving the downstream communication uncompressed) or only perform well under idealized conditions, such as i.i.d. distribution of the client data, which typically cannot be found in federated learning. In this article, we propose sparse ternary compression (STC), a new compression framework that is specifically designed to meet the requirements of the federated learning environment. STC extends the existing compression technique of top- k gradient sparsification with a novel mechanism to enable downstream compression as well as ternarization and optimal Golomb encoding of the weight updates. Our experiments on four different learning tasks demonstrate that STC distinctively outperforms federated averaging in common federated learning scenarios. These results advocate for a paradigm shift in federated optimization toward high-frequency low-bitwidth communication, in particular in the bandwidth-constrained learning environments.},
   author = {Felix Sattler and Simon Wiedemann and Klaus Robert Muller and Wojciech Samek},
   doi = {10.1109/TNNLS.2019.2944481},
   issn = {21622388},
   issue = {9},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Deep learning,distributed learning,efficient communication,federated learning,privacy-preserving machine learning},
   month = {9},
   pages = {3400-3413},
   pmid = {31689214},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Robust and Communication-Efficient Federated Learning from Non-i.i.d. Data},
   volume = {31},
   year = {2020},
}
@article{Chen2021,
   abstract = {Federated learning (FL) enables edge devices, such as Internet of Things devices (e.g., sensors), servers, and institutions (e.g., hospitals), to collaboratively train a machine learning (ML) model without sharing their private data. FL requires devices to exchange their ML parameters iteratively, and thus the time it requires to jointly learn a reliable model depends not only on the number of training steps but also on the ML parameter transmission time per step. In practice, FL parameter transmissions are often carried out by a multitude of participating devices over resource-limited communication networks, for example, wireless networks with limited bandwidth and power. Therefore, the repeated FL parameter transmission from edge devices induces a notable delay, which can be larger than the ML model training time by orders of magnitude. Hence, communication delay constitutes a major bottleneck in FL. Here, a communication-efficient FL framework is proposed to jointly improve the FL convergence time and the training loss. In this framework, a probabilistic device selection scheme is designed such that the devices that can significantly improve the convergence speed and training loss have higher probabilities of being selected for ML model transmission. To further reduce the FL convergence time, a quantization method is proposed to reduce the volume of the model parameters exchanged among devices, and an efficient wireless resource allocation scheme is developed. Simulation results show that the proposed FL framework can improve the identification accuracy and convergence time by up to 3.6% and 87% compared to standard FL.},
   author = {Mingzhe Chen and Nir Shlezinger and H. Vincent Poor and Yonina C. Eldar and Shuguang Cui},
   doi = {10.1073/PNAS.2024789118/SUPPL_FILE/PNAS.2024789118.SAPP.PDF},
   issn = {10916490},
   issue = {17},
   journal = {Proceedings of the National Academy of Sciences of the United States of America},
   keywords = {Federated learning,Machine learning,Wireless communications},
   month = {4},
   pages = {e2024789118},
   pmid = {33888586},
   publisher = {National Academy of Sciences},
   title = {Communication-efficient federated learning},
   volume = {118},
   url = {https://www.pnas.org/doi/abs/10.1073/pnas.2024789118},
   year = {2021},
}
@article{Yang2021,
   abstract = {In this paper, the problem of energy efficient transmission and computation resource allocation for federated learning (FL) over wireless communication networks is investigated. In the considered model, each user exploits limited local computational resources to train a local FL model with its collected data and, then, sends the trained FL model to a base station (BS) which aggregates the local FL model and broadcasts it back to all of the users. Since FL involves an exchange of a learning model between users and the BS, both computation and communication latencies are determined by the learning accuracy level. Meanwhile, due to the limited energy budget of the wireless users, both local computation energy and transmission energy must be considered during the FL process. This joint learning and communication problem is formulated as an optimization problem whose goal is to minimize the total energy consumption of the system under a latency constraint. To solve this problem, an iterative algorithm is proposed where, at every step, closed-form solutions for time allocation, bandwidth allocation, power control, computation frequency, and learning accuracy are derived. Since the iterative algorithm requires an initial feasible solution, we construct the completion time minimization problem and a bisection-based algorithm is proposed to obtain the optimal solution, which is a feasible solution to the original energy minimization problem. Numerical results show that the proposed algorithms can reduce up to 59.5% energy consumption compared to the conventional FL method.},
   author = {Zhaohui Yang and Mingzhe Chen and Walid Saad and Choong Seon Hong and Mohammad Shikh-Bahaei},
   doi = {10.1109/TWC.2020.3037554},
   issn = {15582248},
   issue = {3},
   journal = {IEEE Transactions on Wireless Communications},
   keywords = {Federated learning,energy efficiency,resource allocation},
   month = {3},
   pages = {1935-1949},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Energy Efficient Federated Learning over Wireless Communication Networks},
   volume = {20},
   year = {2021},
}
@article{Motwani2007,
   abstract = {A quasi-identifier refers to a subset of attributes that can uniquely identify most tuples in a table. Incautious publication of quasi-identifiers will lead to privacy leakage. In this paper we consider the problems of finding and masking quasi-identifiers. Both problems are provably hard with severe time and space requirements. We focus on designing efficient approximation algorithms for large data sets. We first propose two natural measures for quantifying quasi-identifiers: distinct ratio and separation ratio. We develop efficient algorithms that find small quasi-identifiers with provable size and separation/distinct ratio guarantees, with space and time requirements sublinear in the number of tuples. We also design practical algorithms for finding all minimal quasi-identifiers. Finally we propose efficient algorithms for masking quasi-identifiers, where we use a random sampling technique to greatly reduce the space and time requirements, without much sacrifice in the quality of the results. Our algorithms for masking and finding minimum quasi-identifiers naturally apply to stream databases. Extensive experimental results on real world data sets confirm efficiency and accuracy of our algorithms.},
   author = {Rajeev Motwani and Ying Xu},
   isbn = {9781595936493},
   title = {Efficient Algorithms for Masking and Finding Quasi-Identifiers},
   year = {2007},
}
@article{,
   abstract = {Recently, federated learning (FL) has emerged as a promising distributed machine learning (ML) technology, owing to the advancing computational and sensing capacities of end-user devices, however with the increasing concerns on users' privacy. As a special architecture in FL, vertical FL (VFL) is capable of constructing a hyper ML model by embracing sub-models from different clients. These sub-models are trained locally by vertically partitioned data with distinct attributes. Therefore, the design of VFL is fundamentally different from that of conventional FL, raising new and unique research issues. In this paper, we aim to discuss key challenges in VFL with effective solutions, and conduct experiments on real-life datasets to shed light on these issues. Specifically, we first propose a general framework on VFL, and highlight the key differences between VFL and conventional FL. Then, we discuss research challenges rooted in VFL systems under four aspects, i.e., security and privacy risks, expensive computation and communication costs, possible structural damage caused by model splitting, and system heterogeneity. Afterwards, we develop solutions to addressing the aforementioned challenges, and conduct extensive experiments to showcase the effectiveness of our proposed solutions.},
   author = {Kang Wei and Student Member and Jun Li and Senior Member and Chuan Ma and Ming Ding and Sha Wei and Fan Wu and Ieee Guihai Chen and Thilina Ranbaduge},
   keywords = {Index Terms-Vertical FL,communication efficiency,privacy preserving,splitting design},
   title = {Vertical Federated Learning: Challenges, Methodologies and Experiments},
   url = {https://github.com/bytedance/fedlearner},
}
@article{Lu2020,
   abstract = {Vertical federated learning (VFL) is a privacy-preserving machine learning framework in which the training dataset is vertically partitioned and distributed over multiple parties, i.e., for each sample each party only possesses some attributes of it. In this paper we address the problem of computing private set intersection (PSI) in VLF, in which a private set denotes the data possessed by a party satisfying some distinguishing constraint. This problem actually asks how the parties jointly compute the common IDs of their private sets, which plays a key role in many learning tasks such as Decision Tree Learning. Currently all known PSI protocols, to our knowledge, either involve expensive cryptographic operations, or are designed for the two-party scenario originally which will leak privacy-sensitive information in multi-party scenario if applied to each pair of parties gradually. In this paper we propose a new multi-party PSI protocol in VFL, which can even handle the case that some parties drop out in the running of the protocol. Our protocol achieves the security that any coalition of corrupted parties, which number is less than a threshold, cannot learn any secret information of honest parties, thus realizing the goal of preserving the privacy of the involved parties. Moreover, it only relies on light cryptographic primitives (i.e. PRGs) and thus works more efficiently compared to the known protocols, especially when the sample number of dataset gets larger and larger. Our starting point to solve the PSI problem in VFL is to reduce it to computing the AND operation of multiple bit-vectors, each held by one party, which are used to identify parties' private sets in their data. Then our main technical contribution is to present an efficient protocol for summing up these vectors, called MulSUM, and then adapt it to a desired protocol, called MulAND, to compute the AND of these vectors, which result actually identifies the intersection of private sets of all (online) parties, thus accomplishing the PSI issue.},
   author = {Linpeng Lu and Ning Ding},
   doi = {10.1109/TRUSTCOM50675.2020.00098},
   isbn = {9781665403924},
   journal = {Proceedings - 2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications, TrustCom 2020},
   keywords = {Multi-party Computation,Private Set Intersection,Vertical Federated Learning},
   month = {12},
   pages = {707-714},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multi-party private set intersection in vertical federated learning},
   year = {2020},
}
@article{Bouacida2021,
   abstract = {With more regulations tackling the protection of users' privacy-sensitive data in recent years, access to such data has become increasingly restricted. A new decentralized training paradigm, known as Federated Learning (FL), enables multiple clients located at different geographical locations to learn a machine learning model collaboratively without sharing their data. While FL has recently emerged as a promising solution to preserve users' privacy, this new paradigm's potential security implications may hinder its widespread adoption. The existing FL protocols exhibit new unique vulnerabilities that adversaries can exploit to compromise the trained model. FL is often preferred in learning environments where security and privacy are the key concerns. Therefore, it is crucial to raise awareness of the consequences resulting from the new threats to FL systems. To date, the security of traditional machine learning systems has been widely examined. However, many open challenges and complex questions are still surrounding FL security. In this paper, we bridge the gap in FL literature by providing a comprehensive survey of the unique security vulnerabilities exposed by the FL ecosystem. We highlight the vulnerabilities sources, key attacks on FL, defenses, as well as their unique challenges, and discuss promising future research directions towards more robust FL.},
   author = {Nader Bouacida and Prasant Mohapatra},
   doi = {10.1109/ACCESS.2021.3075203},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Attacks,defenses,federated learning,security threats,vulnerabilities},
   pages = {63229-63249},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Vulnerabilities in Federated Learning},
   volume = {9},
   year = {2021},
}
@misc{Hsieh2020,
   abstract = {Many large-scale machine learning (ML) applications need to perform decentralized learning over datasets generated at different devices and locations. Such datasets pose a significant challenge to decentralized learning because their different contexts result in significant data distribution skew across devices/locations. In this paper, we take a step toward better understanding this challenge by presenting a detailed experimental study of decentralized DNN training on a common type of data skew: skewed distribution of data labels across devices/locations. Our study shows that: (i) skewed data labels are a fundamental and pervasive problem for decentralized learning, causing significant accuracy loss across many ML applications, DNN models, training datasets, and decentralized learning algorithms; (ii) the problem is particularly challenging for DNN models with batch normalization; and (iii) the degree of data skew is a key determinant of the difficulty of the problem. Based on these findings, we present SkewScout, a system-level approach that adapts the communication frequency of decentralized learning algorithms to the (skew-induced) accuracy loss between data partitions. We also show that group normalization can recover much of the accuracy loss of batch normalization.},
   author = {Kevin Hsieh and Amar Phanishayee and Onur Mutlu and Phillip B Gibbons},
   doi = {10.5281/zenodo.3676081},
   issn = {2640-3498},
   month = {11},
   pages = {4387-4398},
   publisher = {PMLR},
   title = {The Non-IID Data Quagmire of Decentralized Machine Learning},
   url = {https://proceedings.mlr.press/v119/hsieh20a.html},
   year = {2020},
}
@article{Pi2021,
   abstract = {The unprecedented increase in computing power and data availability has signifi-cantly altered the way and the scope that organizations make decisions relying on technologies. There is a conspicuous trend that organizations are seeking the use of frontier technologies with the purpose of helping the delivery of services and making day-to-day operational deci-sions. Machine learning (ML) is the fastest growing and at the same time, the most debated and controversial of these technologies. Although there is a great deal of research in the literature related to machine learning applications, most of them focus on the technical aspects or pri-vate sector use. The governmental machine learning applications suffer the lack of theoretical and empirical studies and unclear governance framework. This paper reviews the literature on the use of machine learning by government, aiming to identify the benefits and challenges of wider adoption of machine learning applications in the public sector and to propose the direc-tions for future research.},
   author = {Yulu Pi},
   doi = {10.29379/jedem.v13i1.625},
   issn = {2075-9517},
   issue = {1},
   journal = {JeDEM - eJournal of eDemocracy and Open Government},
   keywords = {Machine learning,artificial intelligence,benefits,challenges,public sector},
   month = {8},
   pages = {203-219},
   publisher = {Department for E-Governance and Administration},
   title = {Machine learning in Governments: Benefits, Challenges and Future Directions},
   volume = {13},
   url = {https://www.jedem.org/index.php/jedem/article/view/625},
   year = {2021},
}
@article{Qi2022,
   abstract = {Vertical federated learning (VFL) is a privacy-preserving machine learning paradigm that can learn models from features distributed on different platforms in a privacy-preserving way. Since in real-world applications the data may contain bias on fairness-sensitive features (e.g., gender), VFL models may inherit bias from training data and become unfair for some user groups. However, existing fair machine learning methods usually rely on the centralized storage of fairness-sensitive features to achieve model fairness, which are usually inapplicable in federated scenarios. In this paper, we propose a fair vertical federated learning framework (FairVFL), which can improve the fairness of VFL models. The core idea of FairVFL is to learn unified and fair representations of samples based on the decentralized feature fields in a privacy-preserving way. Specifically, each platform with fairness-insensitive features first learns local data representations from local features. Then, these local representations are uploaded to a server and aggregated into a unified representation for the target task. In order to learn a fair unified representation , we send it to each platform storing fairness-sensitive features and apply adversarial learning to remove bias from the unified representation inherited from the biased data. Moreover, for protecting user privacy, we further propose a con-trastive adversarial learning method to remove private information from the unified representation in server before sending it to the platforms keeping fairness-sensitive features. Experiments on three real-world datasets validate that our method can effectively improve model fairness with user privacy well-protected.},
   author = {Tao Qi and Fangzhao Wu and Chuhan Wu and Lingjuan Lyu and Tong Xu and Hao Liao and Zhongliang Yang and Yongfeng Huang and Xing Xie},
   journal = {Advances in Neural Information Processing Systems},
   month = {12},
   pages = {7852-7865},
   title = {FairVFL: A Fair Vertical Federated Learning Framework with Contrastive Adversarial Learning},
   volume = {35},
   year = {2022},
}
@misc{Li2021,
   abstract = {Federated learning enables multiple parties to collab-oratively train a machine learning model without communicating their local data. A key challenge in federated learning is to handle the heterogeneity of local data distribution across parties. Although many studies have been proposed to address this challenge, we find that they fail to achieve high performance in image datasets with deep learning models. In this paper, we propose MOON: model-contrastive federated learning. MOON is a simple and effective federated learning framework. The key idea of MOON is to utilize the similarity between model representations to correct the local training of individual parties, i.e., conducting contrastive learning in model-level. Our extensive experiments show that MOON significantly out-performs the other state-of-the-art federated learning algorithms on various image classification tasks.},
   author = {Qinbin Li and Bingsheng He and Dawn Song},
   pages = {10713-10722},
   title = {Model-Contrastive Federated Learning},
   year = {2021},
}
@article{Gutmann2010,
   abstract = {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters , and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models , including score matching, contrastive divergence , and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.},
   author = {Michael Gutmann and Aapo Hyvärinen},
   title = {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
   year = {2010},
}
@article{Yao2022,
   abstract = {Masked image modelling (e.g., Masked AutoEncoder) and contrastive learning
(e.g., Momentum Contrast) have shown impressive performance on unsupervised
visual representation learning. This work presents Masked Contrastive
Representation Learning (MACRL) for self-supervised visual pre-training. In
particular, MACRL leverages the effectiveness of both masked image modelling
and contrastive learning. We adopt an asymmetric setting for the siamese
network (i.e., encoder-decoder structure in both branches), where one branch
with higher mask ratio and stronger data augmentation, while the other adopts
weaker data corruptions. We optimize a contrastive learning objective based on
the learned features from the encoder in both branches. Furthermore, we
minimize the $L_1$ reconstruction loss according to the decoders' outputs. In
our experiments, MACRL presents superior results on various vision benchmarks,
including CIFAR-10, CIFAR-100, Tiny-ImageNet, and two other ImageNet subsets.
Our framework provides unified insights on self-supervised visual pre-training
and future research.},
   author = {Yuchong Yao and Nandakishor Desai and Marimuthu Palaniswami},
   month = {11},
   title = {Masked Contrastive Representation Learning},
   url = {https://arxiv.org/abs/2211.06012v1},
   year = {2022},
}
@article{Kipf2019,
   abstract = {A structured understanding of our world in terms of objects, relations, and
hierarchies is an important component of human cognition. Learning such a
structured world model from raw sensory data remains a challenge. As a step
towards this goal, we introduce Contrastively-trained Structured World Models
(C-SWMs). C-SWMs utilize a contrastive approach for representation learning in
environments with compositional structure. We structure each state embedding as
a set of object representations and their relations, modeled by a graph neural
network. This allows objects to be discovered from raw pixel observations
without direct supervision as part of the learning process. We evaluate C-SWMs
on compositional environments involving multiple interacting objects that can
be manipulated independently by an agent, simple Atari games, and a
multi-object physics simulation. Our experiments demonstrate that C-SWMs can
overcome limitations of models based on pixel reconstruction and outperform
typical representatives of this model class in highly structured environments,
while learning interpretable object-based representations.},
   author = {Thomas Kipf and Elise van der Pol and Max Welling},
   journal = {8th International Conference on Learning Representations, ICLR 2020},
   month = {11},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Contrastive Learning of Structured World Models},
   url = {https://arxiv.org/abs/1911.12247v2},
   year = {2019},
}
@article{Chen2020SimClr,
   author = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
   issn = {2640-3498},
   month = {11},
   pages = {1597-1607},
   publisher = {PMLR},
   title = {A Simple Framework for Contrastive Learning of Visual Representations},
   url = {https://proceedings.mlr.press/v119/chen20j.html},
   year = {2020},
}
@article{Zhao2018,
   abstract = {Federated learning enables resource-constrained edge compute devices, such as mobile phones and IoT devices, to learn a shared model for prediction, while keeping the training data local. This decentralized approach to train models provides privacy, security, regulatory and economic benefits. In this work, we focus on the statistical challenge of federated learning when local data is non-IID. We first show that the accuracy of federated learning reduces significantly, by up to 55% for neural networks trained for highly skewed non-IID data, where each client device trains only on a single class of data. We further show that this accuracy reduction can be explained by the weight divergence, which can be quantified by the earth mover's distance (EMD) between the distribution over classes on each device and the population distribution. As a solution, we propose a strategy to improve training on non-IID data by creating a small subset of data which is globally shared between all the edge devices. Experiments show that accuracy can be increased by 30% for the CIFAR-10 dataset with only 5% globally shared data.},
   author = {Yue Zhao and Meng Li and Liangzhen Lai and Naveen Suda and Damon Civin and Vikas Chandra},
   doi = {10.48550/arXiv.1806.00582},
   journal = {The Computer Journal},
   title = {Federated Learning with Non-IID Data},
   year = {2018},
}
@article{Zhang2020,
   abstract = {Collaborative learning enables two or more participants, each with their own training dataset, to collaboratively learn a joint model. It is desirable that the collaboration should not cause the disclosure of either the raw datasets of each individual owner or the local model parameters trained on them. This privacy-preservation requirement has been approached through differential privacy mechanisms, homomorphic encryption (HE) and secure multiparty computation (MPC), but existing attempts may either introduce the loss of model accuracy or imply significant computational and/or communicational overhead. In this work, we address this problem with the lightweight additive secret sharing technique. We propose PrivColl, a framework for protecting local data and local models while ensuring the correctness of training processes. PrivColl employs secret sharing technique for securely evaluating addition operations in a multiparty computation environment, and achieves practicability by employing only the homomorphic addition operations. We formally prove that it guarantees privacy preservation even though the majority ($$n-2$$ out of n) of participants are corrupted. With experiments on real-world datasets, we further demonstrate that PrivColl retains high efficiency. It achieves a speedup of more than 45X over the state-of-the-art MPC-/HE-based schemes for training linear/logistic regression, and 216X faster for training neural network.},
   author = {Yanjun Zhang and Guangdong Bai and Xue Li and Caitlin Curtis and Chen Chen and Ryan K.L. Ko},
   doi = {10.1007/978-3-030-58951-6_20/TABLES/2},
   isbn = {9783030589509},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Collaborative learning,Machine learning,Privacy},
   pages = {399-418},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Privcoll: Practical privacy-preserving collaborative machine learning},
   volume = {12308 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-58951-6_20},
   year = {2020},
}
@article{Zhang2022,
   abstract = {Genome-wide analysis has demonstrated both health and social benefits. However, large scale sharing of such data may reveal sensitive information about individuals. One of the emerging challenges is identity tracing attack that exploits correlations among genomic data to reveal the identity of DNA samples. In this paper, we first demonstrate that the adversary can narrow down the sample&#x2019;s identity by detecting his/her genetic relatives and quantify such privacy threat by employing a Shannon entropy-based measurement. For example, we exemplify that when the dataset size reaches 30&#x0025; of the population, for any target from that population, the uncertainty of the target&#x2019;s identity is reduced to merely 2.3 bits of entropy (i.e., the identity is pinned down within 5 people). Direct application of existing approaches such as differential privacy (DP), secure multiparty computation (MPC) and homomorphic encryption (HE) may not be applicable to this challenge in genome-wide analysis because of the compromise on utility (i.e., accuracy or efficiency). Towards addressing this challenge, this paper proposes a framework named <sc><inline-formula><tex-math notation="LaTeX">$\upsilon$</tex-math></inline-formula>Frag</sc> to facilitate privacy-preserving data sharing and computation in genome-wide analysis. <sc><inline-formula><tex-math notation="LaTeX">$\upsilon$</tex-math></inline-formula>Frag</sc> mitigates privacy risks by using a vertical fragmentation to disrupt the genetic architecture on which the adversary relies for identity tracing without sacrificing the capability of genome-wide analysis. We theoretically prove that it preserves the correctness of the primitive functionalities and algorithms ranging from basic summary statistics to advanced neural networks. Our experiments demonstrate that <sc><inline-formula><tex-math notation="LaTeX">$\upsilon$</tex-math></inline-formula>Frag</sc> outperforms secure multiparty computation (MPC) and homomorphic encryption (HE) protocols, with a speedup of more than 221x for training neural networks, and also traditional non-private algorithms and a state-of-the-art noise-based differential privacy (DP) solution in most settings.},
   author = {Yanjun Zhang and Guangdong Bai and Xue Li and Surya Nepal and Marthie Grobler and Chen Chen and Ryan K.L. Ko},
   doi = {10.1109/TDSC.2022.3186672},
   issn = {19410018},
   journal = {IEEE Transactions on Dependable and Secure Computing},
   keywords = {Bioinformatics,Correlation,DNA,Genome-wide analysis,Genomics,Principal component analysis,Privacy,Sociology,identity tracing attack,privacy preserving},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Preserving Privacy for Distributed Genome-Wide Analysis Against Identity Tracing Attacks},
   year = {2022},
}
@article{Zhang2019,
   abstract = {The human genome can reveal sensitive information and is potentially re-identifiable, which raises privacy and security concerns about sharing such data on wide scales. In this work, we propose a preventive approach for privacy-preserving sharing of genomic data in decentralized networks for Genome-wide association studies (GWASs), which have been widely used in discovering the association between genotypes and phenotypes. The key components of this work are: a decentralized secure network, with a privacy-preserving sharing protocol, and a gene fragmentation framework that is trainable in an end-to-end manner. Our experiments on real datasets show the effectiveness of our privacy-preserving approaches as well as significant improvements in efficiency when compared with recent, related algorithms.},
   author = {Yanjun Zhang and Mingyang Zhong and Xin Zhao and Caitlin Curtis and Xue Li and Chen Chen},
   doi = {10.1145/3289600.3290983},
   isbn = {9781450359405},
   journal = {WSDM 2019 - Proceedings of the 12th ACM International Conference on Web Search and Data Mining},
   keywords = {Decentralized network,GWAS,Genomic data,Privacy-preserving sharing,Re-identification},
   month = {1},
   pages = {204-212},
   publisher = {Association for Computing Machinery, Inc},
   title = {Enabling privacy-preserving sharing of genomic data for GWASs in decentralized networks},
   url = {https://dl.acm.org/doi/10.1145/3289600.3290983},
   year = {2019},
}
@article{Wang2020,
   abstract = {The widespread deployment of machine learning applications in ubiquitous environments has sparked interests in exploiting the vast amount of data stored on mobile devices. To preserve data privacy, Federated Learning has been proposed to learn a shared model by performing distributed training locally on participating devices and aggregating the local models into a global one. However, due to the limited network connectivity of mobile devices, it is not practical for federated learning to perform model updates and aggregation on all participating devices in parallel. Besides, data samples across all devices are usually not independent and identically distributed (IID), posing additional challenges to the convergence and speed of federated learning. In this paper, we propose Favor, an experience-driven control framework that intelligently chooses the client devices to participate in each round of federated learning to counterbalance the bias introduced by non-IID data and to speed up convergence. Through both empirical and mathematical analysis, we observe an implicit connection between the distribution of training data on a device and the model weights trained based on those data, which enables us to profile the data distribution on that device based on its uploaded model weights. We then propose a mechanism based on deep Q-learning that learns to select a subset of devices in each communication round to maximize a reward that encourages the increase of validation accuracy and penalizes the use of more communication rounds. With extensive experiments performed in PyTorch, we show that the number of communication rounds required in federated learning can be reduced by up to 49% on the MNIST dataset, 23% on FashionMNIST, and 42% on CIFAR-10, as compared to the Federated Averaging algorithm.},
   author = {Hao Wang and Zakhary Kaplan and Di Niu and Baochun Li},
   doi = {10.1109/INFOCOM41043.2020.9155494},
   isbn = {9781728164120},
   issn = {0743166X},
   journal = {Proceedings - IEEE INFOCOM},
   month = {7},
   pages = {1698-1707},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Optimizing Federated Learning on Non-IID Data with Reinforcement Learning},
   volume = {2020-July},
   year = {2020},
}
@article{,
   abstract = {Contrastive self-supervised learning has been successfully used in many domains, such as images, texts, graphs, etc., to learn features without requiring label information. In this paper, we propose a new local contrastive feature learning (LoCL) framework, and our theme is to learn local patterns/features from tabular data. In order to create a niche for local learning, we use feature correlations to create a maximum-spanning tree, and break the tree into feature subsets, with strongly correlated features being assigned next to each other. Convolutional learning of the features is used to learn latent feature space, regulated by contrastive and reconstruction losses. Experiments on public tabular datasets show the effectiveness of the proposed method versus state-of-the-art baseline methods. CCS CONCEPTS • Information systems → Data mining.},
   author = {Zhabiz Gharibshah and Xingquan Zhu},
   doi = {10.1145/3511808.3557630},
   keywords = {Contrastive learning,self-supervised learning,tabular data},
   title = {Local Contrastive Feature Learning for Tabular Data; Local Contrastive Feature Learning for Tabular Data},
}
@inproceedings{subtab,
   abstract = {Self-supervised learning has been shown to be very effective in learning useful representations, and yet much of the success is achieved in data types such as images, audio, and text. The success is mainly enabled by taking advantage of spatial, temporal, or semantic structure in the data through augmentation. However, such structure may not exist in tabular datasets commonly used in fields such as healthcare, making it difficult to design an effective augmentation method, and hindering a similar progress in tabular data setting. In this paper, we introduce a new framework, Subsetting features of Tabular data (SubTab), that turns the task of learning from tabular data into a multi-view representation learning problem by dividing the input features to multiple subsets. We argue that reconstructing the data from the subset of its features rather than its corrupted version in an autoencoder setting can better capture its underlying latent representation. In this framework, the joint representation can be expressed as the aggregate of latent variables of the subsets at test time, which we refer to as collaborative inference. Our experiments show that the SubTab achieves the state of the art (SOTA) performance of 98.31% on MNIST in tabular setting, on par with CNN-based SOTA models, and surpasses existing baselines on three other real-world datasets by a significant margin.},
   author = {Talip Ucar and Ehsan Hajiramezanali and Lindsay Edwards},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   title = {SubTab: Subsetting Features of Tabular Data for Self-Supervised Representation Learning},
   volume = {23},
   year = {2021},
}


@article{Servetnyk2020,
   abstract = {This work considers unsupervised learning tasks being implemented within the federated learning framework to satisfy stringent requirements for low-latency and privacy of the emerging applications. The proposed algorithm is based on Dual Averaging (DA), where the gradients of each agent are aggregated at a central node. While having its advantages in terms of distributed computation, the accuracy of federated learning training reduces significantly when the data is nonuniformly distributed across devices. Therefore, this work proposes two weight computation algorithms, with one using a fixed size bin and the other with self-organizing maps (SOM) that solves the underlying dimensionality problem inherent in the first method. Simulation results are also provided to show that the proposed algorithms' performance is comparable to the scenario in which all data is uploaded and processed in the centralized cloud.},
   author = {Mykola Servetnyk and Carrson C. Fung and Zhu Han},
   doi = {10.1109/GLOBECOM42002.2020.9348203},
   journal = {2020 IEEE Global Communications Conference, GLOBECOM 2020 - Proceedings},
   keywords = {Federated learning,distributed optimization,dual averaging algorithm,gradient weighting,self-organizing maps,unsupervised learning},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Unsupervised Federated Learning for Unbalanced Data},
   volume = {2020-January},
   year = {2020},
}

@article{Lubana2022,
   abstract = {Federated learning is generally used in tasks where labels are readily
available (e.g., next word prediction). Relaxing this constraint requires
design of unsupervised learning techniques that can support desirable
properties for federated training: robustness to statistical/systems
heterogeneity, scalability with number of participants, and communication
efficiency. Prior work on this topic has focused on directly extending
centralized self-supervised learning techniques, which are not designed to have
the properties listed above. To address this situation, we propose Orchestra, a
novel unsupervised federated learning technique that exploits the federation's
hierarchy to orchestrate a distributed clustering task and enforce a globally
consistent partitioning of clients' data into discriminable clusters. We show
the algorithmic pipeline in Orchestra guarantees good generalization
performance under a linear probe, allowing it to outperform alternative
techniques in a broad range of conditions, including variation in
heterogeneity, number of clients, participation ratio, and local epochs.},
   author = {Ekdeep Singh Lubana and Chi Ian Tang and Fahim Kawsar and Robert P. Dick and Akhil Mathur},
   month = {5},
   title = {Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering},
   url = {https://arxiv.org/abs/2205.11506v2},
   year = {2022},
}
@article{Tzinis2021,
   abstract = {We propose FedEnhance, an unsupervised federated learning (FL) approach for speech enhancement and separation with non-IID distributed data across multiple clients. We simulate a realworld scenario where each client only has access to a few noisy recordings from a limited and disjoint number of speakers (hence non-IID). Each client trains their model in isolation using mixture invariant training while periodically providing updates to a central server. Our experiments show that our approach achieves competitive enhancement performance compared to IID training on a single device and that we can further facilitate the convergence speed and the overall performance using transfer learning on the server-side. Moreover, we show that we can effectively combine updates from clients trained locally with supervised and unsupervised losses. We also release a new dataset LibriFSD50K and its creation recipe in order to facilitate FL research for source separation problems.},
   author = {Efthymios Tzinis and Jonah Casebeer and Zhepei Wang and Paris Smaragdis},
   doi = {10.1109/WASPAA52581.2021.9632783},
   isbn = {9781665448703},
   issn = {19471629},
   journal = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
   keywords = {Speech enhancement,federated learning,non-IID learning,source separation,unsupervised learning},
   pages = {46-50},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data},
   volume = {2021-October},
   year = {2021},
}
@article{Zhang2020,
   abstract = {To leverage enormous unlabeled data on distributed edge devices, we formulate
a new problem in federated learning called Federated Unsupervised
Representation Learning (FURL) to learn a common representation model without
supervision while preserving data privacy. FURL poses two new challenges: (1)
data distribution shift (Non-IID distribution) among clients would make local
models focus on different categories, leading to the inconsistency of
representation spaces. (2) without the unified information among clients in
FURL, the representations across clients would be misaligned. To address these
challenges, we propose Federated Constrastive Averaging with dictionary and
alignment (FedCA) algorithm. FedCA is composed of two key modules: (1)
dictionary module to aggregate the representations of samples from each client
and share with all clients for consistency of representation space and (2)
alignment module to align the representation of each client on a base model
trained on a public data. We adopt the contrastive loss for local model
training. Through extensive experiments with three evaluation protocols in IID
and Non-IID settings, we demonstrate that FedCA outperforms all baselines with
significant margins.},
   author = {Fengda Zhang and Kun Kuang and Zhaoyang You and Tao Shen and Jun Xiao and Yin Zhang and Chao Wu and Yueting Zhuang and Xiaolin Li},
   title = {Federated Unsupervised Representation Learning},
   year = {2020},
}
@misc{Ganin2015,
   abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard back-propagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
   author = {Yaroslav Ganin and Victor Lempitsky and Lempitsky@skoltech Ru},
   issn = {1938-7228},
   pages = {1180-1189},
   publisher = {PMLR},
   title = {Unsupervised Domain Adaptation by Backpropagation},
   year = {2015},
}
@article{Seide2014,
   abstract = {We show empirically that in SGD training of deep neural networks , one can, at no or nearly no loss of accuracy, quantize the gradients aggressively-to but one bit per value-if the quan-tization error is carried forward across minibatches (error feedback). This size reduction makes it feasible to parallelize SGD through data-parallelism with fast processors like recent GPUs. We implement data-parallel deterministically distributed SGD by combining this finding with AdaGrad, automatic minibatch-size selection, double buffering, and model paral-lelism. Unexpectedly, quantization benefits AdaGrad, giving a small accuracy gain. For a typical Switchboard DNN with 46M parameters, we reach computation speeds of 27k frames per second (kfps) when using 2880 samples per minibatch, and 51kfps with 16k, on a server with 8 K20X GPUs. This corresponds to speed-ups over a single GPU of 3.6 and 6.3, respectively. 7 training passes over 309h of data complete in under 7h. A 160M-parameter model training processes 3300h of data in under 16h on 20 dual-GPU servers-a 10 times speed-up-albeit at a small accuracy loss.},
   author = {Frank Seide and Hao Fu and Jasha Droppo and Gang Li and Dong Yu},
   title = {-Bit Stochastic Gradient Descent and its Application to Data-Parallel Distributed Training of Speech DNNs},
   year = {2014},
}
@article{Zhao2021,
   abstract = {Computational cost of training state-of-the-art deep models in many learning
problems is rapidly increasing due to more sophisticated models and larger
datasets. A recent promising direction for reducing training cost is dataset
condensation that aims to replace the original large training set with a
significantly smaller learned synthetic set while preserving the original
information. While training deep models on the small set of condensed images
can be extremely fast, their synthesis remains computationally expensive due to
the complex bi-level optimization and second-order derivative computation. In
this work, we propose a simple yet effective method that synthesizes condensed
images by matching feature distributions of the synthetic and original training
images in many sampled embedding spaces. Our method significantly reduces the
synthesis cost while achieving comparable or better performance. Thanks to its
efficiency, we apply our method to more realistic and larger datasets with
sophisticated neural architectures and obtain a significant performance boost.
We also show promising practical benefits of our method in continual learning
and neural architecture search.},
   author = {Bo Zhao and Hakan Bilen},
   doi = {10.1109/WACV56688.2023.00645},
   isbn = {9781665493468},
   journal = {Proceedings - 2023 IEEE Winter Conference on Applications of Computer Vision, WACV 2023},
   keywords = {Algorithms: Machine learning architectures,and algorithms (including transfer),formulations},
   month = {10},
   pages = {6503-6512},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Dataset Condensation with Distribution Matching},
   url = {https://arxiv.org/abs/2110.04181v3},
   year = {2021},
}
@article{Lu2020,
   abstract = {In the traditional cloud architecture, data needs to be uploaded to the cloud for processing, bringing delays in transmission and response. Edge network emerges as the times require. Data processing on the edge nodes can reduce the delay of data transmission and improve the response speed. In recent years, the need for artificial intelligence of edge network has been proposed. However, the data of a single, individual edge node is limited and does not satisfy the conditions of machine learning. Therefore, performing edge network machine learning under the premise of data confidentiality became a research hotspot. This paper proposes a Privacy-Preserving Asynchronous Federated Learning Mechanism for Edge Network Computing (PAFLM), which can allow multiple edge nodes to achieve more efficient federated learning without sharing their private data. Compared with the traditional distributed learning, the proposed method compresses the communications between nodes and parameter server during the training process without affecting the accuracy. Moreover, it allows the node to join or quit in any process of learning, which can be suitable to the scene with highly mobile edge devices.},
   author = {Xiaofeng Lu and Yuying Liao and Pietro Lio and Pan Hui},
   doi = {10.1109/ACCESS.2020.2978082},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Federated learning,asynchronous distributed network,edge computing,gradient compression,privacy preservation},
   note = {Fog Computing.  Asynchronous Federated Learning with Dual-WeightsCorrection},
   pages = {48970-48981},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Privacy-preserving asynchronous federated learning mechanism for edge network computing},
   volume = {8},
   year = {2020},
}
@article{Aji2017,
   abstract = {We make distributed stochastic gradient descent faster by exchanging sparse
updates instead of dense updates. Gradient updates are positively skewed as
most updates are near zero, so we map the 99% smallest updates (by absolute
value) to zero then exchange sparse matrices. This method can be combined with
quantization to further improve the compression. We explore different
configurations and apply them to neural machine translation and MNIST image
classification tasks. Most configurations work on MNIST, whereas different
configurations reduce convergence rate on the more complex translation task.
Our experiments show that we can achieve up to 49% speed up on MNIST and 22% on
NMT without damaging the final accuracy or BLEU.},
   author = {Alham Fikri Aji and Kenneth Heafield},
   doi = {10.18653/v1/d17-1045},
   isbn = {9781945626838},
   journal = {EMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
   month = {4},
   pages = {440-445},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Sparse Communication for Distributed Gradient Descent},
   url = {https://arxiv.org/abs/1704.05021v2},
   year = {2017},
}
@article{Hayashi2019,
   abstract = {The recent increase of "big data" in our society has led to major impacts of machine learning and data mining technologies in various fields ranging from marketing to science. On the other hand, there still exist areas where only small-sized data are available for various reasons, for example, high data acquisition costs or the rarity of targets events. Machine learning tasks using such small data are usually difficult because of the lack of information available for training accurate prediction models. In particular, for long-term time-series prediction, the data size tends to be small because of the unavailability of the data between input and output times in training. Such limitations on the size of time-series data further make long-term prediction tasks quite difficult; in addition, the difficulty that the far future is more uncertain than the near future.In this paper, we propose a novel method for long-term prediction of small time-series data designed in the framework of generalized distillation. The key idea of the proposed method is to utilize the middle-time data between the input and output times as "privileged information," which is available only in the training phase and not in the test phase. We demonstrate the effectiveness of the proposed method on both synthetic data and real-world data. The experimental results show the proposed method performs well, particularly when the task is difficult and has high input dimensions.},
   author = {Shogo Hayashi and Akira Tanimoto and Hisashi Kashima},
   doi = {10.1109/IJCNN.2019.8851687},
   isbn = {9781728119854},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   keywords = {generalized distillation,privileged information,small data,time-series data},
   month = {7},
   note = {The cold start problems. },
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Long-Term Prediction of Small Time-Series Data Using Generalized Distillation},
   volume = {2019-July},
   year = {2019},
}
@article{Chen2020async,
   abstract = {Federated learning (FL) is a machine learning paradigm where a shared central model is learned across distributed devices while the training data remains on these devices. Federated Averaging (FedAvg) is the leading optimization method for training non-convex models in this setting with a synchronized protocol. However, the assumptions made by FedAvg are not realistic given the heterogeneity of devices. First, the volume and distribution of collected data vary in the training process due to different sampling rates of edge devices. Second, the edge devices themselves also vary in latency and system configurations, such as memory, processor speed, and power requirements. This leads to vastly different computation times. Third, availability issues at edge devices can lead to a lack of contribution from specific edge devices to the federated model. In this paper, we present an Asynchronous Online Federated Learning (ASO-Fed) framework, where the edge devices perform online learning with continuous streaming local data and a central server aggregates model parameters from clients. Our framework updates the central model in an asynchronous manner to tackle the challenges associated with both varying computational loads at heterogeneous edge devices and edge devices that lag behind or dropout. We perform extensive experiments on a benchmark image dataset and three real-world datasets with non-IID streaming data. The results demonstrate ASO-Fed converging fast and maintaining good prediction performance.},
   author = {Yujing Chen and Yue Ning and Martin Slawski and Huzefa Rangwala},
   doi = {10.1109/BIGDATA50022.2020.9378161},
   isbn = {9781728162515},
   journal = {Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020},
   keywords = {Asynchronous federated Learning,Edge devices,Non-IID data,Online learning},
   month = {12},
   note = {online learning with multiple clients.update global model without wait others},
   pages = {15-24},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Asynchronous Online Federated Learning for Edge Devices with Non-IID Data},
   year = {2020},
}
@article{Xu2021,
   abstract = {Federated learning (FL) is experiencing a fast booming with the wave of
distributed machine learning. In the FL paradigm, the global model is
aggregated on the centralized aggregation server according to the parameters of
local models instead of local training data, mitigating privacy leakage caused
by the collection of sensitive information. With the increased computing and
communication capabilities of edge and IoT devices, applying FL on
heterogeneous devices to train machine learning models becomes a trend. The
synchronous aggregation strategy in the classic FL paradigm cannot effectively
use the limited resource, especially on heterogeneous devices, due to its
waiting for straggler devices before aggregation in each training round.
Furthermore, the disparity of data spread on devices (i.e. data heterogeneity)
in real-world scenarios downgrades the accuracy of models. As a result, many
asynchronous FL (AFL) paradigms are presented in various application scenarios
to improve efficiency, performance, privacy, and security. This survey
comprehensively analyzes and summarizes existing variants of AFL according to a
novel classification mechanism, including device heterogeneity, data
heterogeneity, privacy and security on heterogeneous devices, and applications
on heterogeneous devices. Finally, this survey reveals rising challenges and
presents potentially promising research directions in this under-investigated
field.},
   author = {Chenhao Xu and Youyang Qu and Yong Xiang and Longxiang Gao},
   keywords = {Application,Asynchronous Federated Learning,Data Heterogeneity,Device Heterogeneity,Privacy,Security},
   month = {9},
   title = {Asynchronous Federated Learning on Heterogeneous Devices: A Survey},
   url = {https://arxiv.org/abs/2109.04269v4},
   year = {2021},
}
@article{Chen2020VFL,
   abstract = {Horizontal Federated learning (FL) handles multi-client data that share the
same set of features, and vertical FL trains a better predictor that combine
all the features from different clients. This paper targets solving vertical FL
in an asynchronous fashion, and develops a simple FL method. The new method
allows each client to run stochastic gradient algorithms without coordination
with other clients, so it is suitable for intermittent connectivity of clients.
This method further uses a new technique of perturbed local embedding to ensure
data privacy and improve communication efficiency. Theoretically, we present
the convergence rate and privacy level of our method for strongly convex,
nonconvex and even nonsmooth objectives separately. Empirically, we apply our
method to FL on various image and healthcare datasets. The results compare
favorably to centralized and synchronous FL methods.},
   author = {Tianyi Chen and Xiao Jin and Yuejiao Sun and Wotao Yin},
   month = {7},
   title = {VAFL: a Method of Vertical Asynchronous Federated Learning},
   url = {https://arxiv.org/abs/2007.06081v1},
   year = {2020},
}
@article{Bistritz2020,
   abstract = {On-device learning promises collaborative training of machine learning models across edge devices without the sharing of user data. In state-of-the-art on-device learning algorithms, devices communicate their model weights over a decentralized communication network. Transmitting model weights requires huge communication overhead and means only devices with identical model architectures can be included. To overcome these limitations, we introduce a distributed distillation algorithm where devices communicate and learn from soft-decision (softmax) outputs , which are inherently architecture-agnostic and scale only with the number of classes. The communicated soft-decisions are each model's outputs on a public , unlabeled reference dataset, which serves as a common vocabulary between devices. We prove that the gradients of the distillation regularized loss functions of all devices converge to zero with probability 1. Hence, all devices distill the entire knowledge of all other devices on the reference data, regardless of their local connections. Our analysis assumes smooth loss functions, which can be non-convex. Simulations support our theoretical findings and show that even a naive implementation of our algorithm significantly reduces the communication overhead while achieving an overall comparable accuracy to the state-of-the-art. By requiring little communication overhead and allowing for cross-architecture training, we remove two main obstacles to scaling on-device learning.},
   author = {Ilai Bistritz and Ariana J Mann and Nicholas Bambos},
   journal = {Advances in Neural Information Processing Systems},
   note = {talk about ensembles.},
   pages = {22593-22604},
   title = {Distributed Distillation for On-Device Learning},
   volume = {33},
   year = {2020},
}
@misc{Zhmoginov2023,
   abstract = {Decentralized learning with private data is a central problem in machine learning. We propose a novel distillation-based decentralized learning technique that allows multiple agents with private non-iid data to learn from each other, without having to share their data, weights or weight updates. Our approach is communication efficient, utilizes an unlabeled public dataset and uses multiple auxiliary heads for each client, greatly improving training efficiency in the case of heterogeneous data. This approach allows individual models to preserve and enhance performance on their private tasks while also dramatically improving their performance on the global aggregated data distribution. We study the effects of data and model architecture heterogeneity and the impact of the underlying communication graph topology on learning efficiency and show that our agents can significantly improve their performance compared to learning in isolation.},
   author = {Andrey Zhmoginov and Mark Sandler and Nolan Miller and Gus Kristiansen and Max Vladymyrov},
   note = {Decentralized learning},
   pages = {8053-8063},
   title = {Decentralized Learning With Multi-Headed Distillation},
   year = {2023},
}
@article{So2020,
   abstract = {We consider a collaborative learning scenario in which multiple data-owners wish to jointly train a logistic regression model, while keeping their individual datasets private from the other parties. We propose COPML, a fully-decentralized training framework that achieves scalability and privacy-protection simultaneously. The key idea of COPML is to securely encode the individual datasets to distribute the computation load effectively across many parties and to perform the training computations as well as the model updates in a distributed manner on the securely encoded data. We provide the privacy analysis of COPML and prove its convergence. Furthermore, we experimentally demonstrate that COPML can achieve significant speedup in training over the benchmark protocols. Our protocol provides strong statistical privacy guarantees against colluding parties (adversaries) with unbounded computational power, while achieving up to 16× speedup in the training time against the benchmark protocols.},
   author = {Jinhyun So and Basak Guler and Salman Avestimehr},
   journal = {Advances in Neural Information Processing Systems},
   pages = {8054-8066},
   title = {A Scalable Approach for Privacy-Preserving Collaborative Machine Learning},
   volume = {33},
   year = {2020},
}
@article{Balkus2022,
   abstract = {By enabling autonomous vehicles (AVs) to share data while driving, 5G vehicular communications allow AVs to collaborate on solving common autonomous driving tasks. AVs often rely on machine learning models to perform such tasks; as such, collaboration requires leveraging vehicular communications to improve the performance of machine learning algorithms. This paper provides a comprehensive literature survey of the intersection between machine learning for autonomous driving and vehicular communications. Throughout the paper, we explain how vehicle-to-vehicle (V2V) and vehicle-to-everything (V2X) communications are used to improve machine learning in AVs, answering five major questions regarding such systems. These questions include: 1) How can AVs effectively transmit data wirelessly on the road? 2) How do AVs manage the shared data? 3) How do AVs use shared data to improve their perception of the environment? 4) How do AVs use shared data to drive more safely and efficiently? and 5) How can AVs protect the privacy of shared data and prevent cyberattacks? We also summarize data sources that may support research in this area and discuss the future research potential surrounding these five questions.},
   author = {Salvador V. Balkus and Honggang Wang and Brian D. Cornet and Chinmay Mahabal and Hieu Ngo and Hua Fang},
   doi = {10.1109/COMST.2022.3149714},
   issn = {1553877X},
   issue = {2},
   journal = {IEEE Communications Surveys and Tutorials},
   keywords = {Machine learning,Vehicular communications},
   pages = {1280-1303},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Survey of Collaborative Machine Learning Using 5G Vehicular Communications},
   volume = {24},
   year = {2022},
}
@article{Usynin2021,
   abstract = {Despite the rapid increase of data available to train machine-learning algorithms in many domains, several applications suffer from a paucity of representative and diverse data. The medical and financial sectors are, for example, constrained by legal, ethical, regulatory and privacy concerns preventing data sharing between institutions. Collaborative learning systems, such as federated learning, are designed to circumvent such restrictions and provide a privacy-preserving alternative by eschewing data sharing and relying instead on the distributed remote execution of algorithms. However, such systems are susceptible to malicious adversarial interference attempting to undermine their utility or divulge confidential information. Here we present an overview and analysis of current adversarial attacks and their mitigations in the context of collaborative machine learning. We discuss the applicability of attack vectors to specific learning contexts and attempt to formulate a generic foundation for adversarial influence and mitigation mechanisms. We moreover show that a number of context-specific learning conditions are exploited in similar fashion across all settings. Lastly, we provide a focused perspective on open challenges and promising areas of future research in the field. When the training data for machine learning are highly personal or sensitive, collaborative approaches can help a collective of stakeholders to train a model together without having to share any data. But there are still risks to the privacy of the data. This Perspective provides an overview of potential attacks on collaborative machine learning and how these threats could be addressed.},
   author = {Dmitrii Usynin and Alexander Ziller and Marcus Makowski and Rickmer Braren and Daniel Rueckert and Ben Glocker and Georgios Kaissis and Jonathan Passerat-Palmbach},
   doi = {10.1038/s42256-021-00390-3},
   issn = {2522-5839},
   issue = {9},
   journal = {Nature Machine Intelligence 2021 3:9},
   keywords = {Computational science,Computer science},
   month = {9},
   pages = {749-758},
   publisher = {Nature Publishing Group},
   title = {Adversarial interference and its mitigations in privacy-preserving collaborative machine learning},
   volume = {3},
   url = {https://www.nature.com/articles/s42256-021-00390-3},
   year = {2021},
}
@misc{Hwee2020,
   abstract = {Collaborative machine learning (ML) is an appealing paradigm to build high-quality ML models by training on the aggregated data from many parties. However, these parties are only willing to share their data when given enough incentives, such as a guaranteed fair reward based on their contributions. This motivates the need for measuring a party's contribution and designing an incentive-aware reward scheme accordingly. This paper proposes to value a party's reward based on Shapley value and information gain on model parameters given its data. Subsequently, we give each party a model as a reward. To formally incentivize the collaboration, we define some desirable properties (e.g., fairness and stability) which are inspired by cooperative game theory but adapted for our model reward that is uniquely freely replicable. Then, we propose a novel model reward scheme to satisfy fairness and trade off between the desirable properties via an adjustable parameter. The value of each party's model reward determined by our scheme is attained by injecting Gaussian noise to the aggregated training data with an optimized noise variance. We empirically demonstrate interesting properties of our scheme and evaluate its performance using synthetic and real-world datasets.},
   author = {Rachael Hwee and Ling Sim and Yehong Zhang and Mun Choon Chan and Bryan Kian and Hsiang Low},
   issn = {2640-3498},
   month = {11},
   note = {data quality measurements and incentives},
   pages = {8927-8936},
   publisher = {PMLR},
   title = {Collaborative Machine Learning with Incentive-Aware Model Rewards},
   url = {https://proceedings.mlr.press/v119/sim20a.html},
   year = {2020},
}
@article{Hu2019,
   abstract = {Most current distributed machine learning systems try to scale up model training by using a data-parallel architecture that divides the computation for different samples among workers. We study distributed machine learning from a different motivation, where the information about the same samples, e.g., users and objects, are owned by several parities that wish to collaborate but do not want to share raw data with each other. We propose an asynchronous stochastic gradient descent (SGD) algorithm for such a feature distributed machine learning (FDML) problem, to jointly learn from distributed features, with theoretical convergence guarantees under bounded asynchrony. Our algorithm does not require sharing the original features or even local model parameters between parties, thus preserving the data locality. The system can also easily incorporate differential privacy mechanisms to preserve a higher level of privacy. We implement the FDML system in a parameter server architecture and compare our system with fully centralized learning (which violates data locality) and learning based on only local features, through extensive experiments performed on both a public data set a9a, and a large dataset of 5, 000, 000 records and 8700 decentralized features from three collaborating apps at Tencent including Tencent MyApp, Tecent QQ Browser and Tencent Mobile Safeguard. Experimental results have demonstrated that the proposed FDML system can be used to significantly enhance app recommendation in Tencent MyApp by leveraging user and item features from other apps, while preserving the locality and privacy of features in each individual app to a high degree.},
   author = {Yaochen Hu and Di Niu and Jianming Yang and Shengping Zhou},
   doi = {10.1145/3292500.3330765},
   isbn = {9781450362016},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {Data Privacy,Distributed Features,Distributed Machine Learning,Model Parallelism,Optimization,Stochastic Gradient Descent},
   month = {7},
   note = {a feature dis-tributed machine learning, SGD },
   pages = {2232-2240},
   publisher = {Association for Computing Machinery},
   title = {FDML: A collaborative machine learning framework for distributed features},
   url = {https://dl.acm.org/doi/10.1145/3292500.3330765},
   year = {2019},
}
@article{Hofmann2005,
   abstract = {In information retrieval, feedback provided by individual users is often very sparse. Consequently, machine learning algorithms for automatically retrieving documents or recommending items may not achieve satisfactory levels of accuracy. However, if one views users as members of a larger user community, then it should be possible to leverage similarities between different users to overcome the sparseness problem. The paper proposes a collaborative machine learning framework to exploit inter-user similarities. More specifically, we present a kernel-based learning architecture that generalizes the well-known Support Vector Machine learning approach by enriching content descriptors with inter-user correlations. © Springer-Verlag Berlin Heidelberg 2005.},
   author = {Thomas Hofmann and Justin Basilico},
   doi = {10.1007/978-3-540-31842-2_18/COVER},
   isbn = {3540245510},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   note = {colaboartive ML extend SVM},
   pages = {173-182},
   publisher = {Springer Verlag},
   title = {Collaborative machine learning},
   volume = {3379 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-31842-2_18},
   year = {2005},
}
@article{Verbraeken2020,
   abstract = {The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware ...},
   author = {Joost Verbraeken and Matthijs Wolting and Jonathan Katzy and Jeroen Kloppenburg and Tim Verbelen and Jan S. Rellermeyer},
   doi = {10.1145/3377454},
   issn = {15577341},
   issue = {2},
   journal = {ACM Computing Surveys (CSUR)},
   keywords = {Distributed machine learning,distributed systems,• Computer systems organization → Distributed architectures,• Computing methodologies → Machine learning},
   month = {3},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {A Survey on Distributed Machine Learning},
   volume = {53},
   url = {https://dl.acm.org/doi/10.1145/3377454},
   year = {2020},
}
@article{Hesamifard2017,
   abstract = {Machine learning algorithms based on deep neural networks (NN) have achieved remarkable results and are being extensively used in different domains. On the other hand, with increasing growth of cloud services, several Machine Learning as a Service (MLaaS) are offered where training and deploying machine learning models are performed on cloud providers' infrastructure. However, machine learning algorithms require access to raw data which is often privacy sensitive and can create potential security and privacy risks. To address this issue, we develop new techniques to provide solutions for applying deep neural network algorithms to the encrypted data. In this paper, we show that it is feasible and practical to train neural networks using encrypted data and to make encrypted predictions, and also return the predictions in an encrypted form. We demonstrate applicability of the proposed techniques and evaluate its performance. The empirical results show that it provides accurate privacy-preserving training and classification.},
   author = {Ehsan Hesamifard and Hassan Takabi and Mehdi Ghasemi and Catherine Jones},
   doi = {10.1145/3140649.3140655},
   isbn = {9781450353939},
   journal = {CCSW 2017 - Proceedings of the 2017 Cloud Computing Security Workshop, co-located with CCS 2017},
   keywords = {Homomorphic Encryption,Machine learning,Privacy Preserving},
   month = {11},
   note = {Homomorphic encryption neral network},
   pages = {39-43},
   publisher = {Association for Computing Machinery, Inc},
   title = {Privacy-preserving machine learning in cloud},
   url = {https://dl.acm.org/doi/10.1145/3140649.3140655},
   year = {2017},
}
@article{Bohdal2020,
   abstract = {We study the problem of dataset distillation - creating a small set of
synthetic examples capable of training a good model. In particular, we study
the problem of label distillation - creating synthetic labels for a small set
of real images, and show it to be more effective than the prior image-based
approach to dataset distillation. Methodologically, we introduce a more robust
and flexible meta-learning algorithm for distillation, as well as an effective
first-order strategy based on convex optimization layers. Distilling labels
with our new algorithm leads to improved results over prior image-based
distillation. More importantly, it leads to clear improvements in flexibility
of the distilled dataset in terms of compatibility with off-the-shelf
optimizers and diverse neural architectures. Interestingly, label distillation
can also be applied across datasets, for example enabling learning Japanese
character recognition by training only on synthetically labeled English
letters.},
   author = {Ondrej Bohdal and Yongxin Yang and Timothy Hospedales},
   month = {6},
   title = {Flexible Dataset Distillation: Learn Labels Instead of Images},
   url = {https://arxiv.org/abs/2006.08572v3},
   year = {2020},
}
@article{Nguyen2021,
   abstract = {The effectiveness of machine learning algorithms arises from being able to extract useful features from large amounts of data. As model and dataset sizes increase, dataset distillation methods that compress large datasets into significantly smaller yet highly performant ones will become valuable in terms of training efficiency and useful feature extraction. To that end, we apply a novel distributed kernel-based meta-learning framework to achieve state-of-the-art results for dataset distillation using infinitely wide convolutional neural networks. For instance, using only 10 datapoints (0.02% of original dataset), we obtain over 65% test accuracy on CIFAR-10 image classification task, a dramatic improvement over the previous best test accuracy of 40%. Our state-of-the-art results extend across many other settings for MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and SVHN. Furthermore, we perform some preliminary analyses of our distilled datasets to shed light on how they differ from naturally occurring data.},
   author = {Timothy Nguyen and Roman Novak and ♠ Lechao and Xiao ♠ Jaehoon and Lee ♠ Deepmind},
   journal = {Advances in Neural Information Processing Systems},
   month = {12},
   pages = {5186-5198},
   title = {Dataset Distillation with Infinitely Wide Convolutional Networks},
   volume = {34},
   url = {https://github.com/google-research/},
   year = {2021},
}
@misc{Cazenavette2022,
   author = {George Cazenavette and Tongzhou Wang and Antonio Torralba and Alexei A. Efros and Jun-Yan Zhu},
   pages = {4750-4759},
   title = {Dataset Distillation by Matching Training Trajectories},
   url = {https://georgecazenavette.github.io/mtt-distillation/.},
   year = {2022},
}
@article{Sucholutsky2021,
   abstract = {Dataset distillation is a method for reducing dataset sizes by learning a small number of representative synthetic samples. This has several benefits such as speeding up model training, reducing energy consumption, and reducing required storage space. These benefits are especially crucial in settings like federated learning where initial overhead costs are justified by the speedup they enable. Currently, 1) each synthetic sample is assigned a single 'hard' label, and 2) dataset distillation can only be used with image data. We propose to simultaneously distill both images and their labels, thus assigning each synthetic sample a 'soft' label (a distribution of labels). Our algorithm increases accuracy by 2-4% for several image classification tasks. Using 'soft' labels also enables distilled datasets to consist of fewer samples than there are classes as each sample encodes information for multiple classes. For example, training a LeNet model with 10 distilled images (one per class) results in over 96% accuracy on MNIST, and almost 92% accuracy when trained on just 5 distilled images. We also extend the dataset distillation algorithm to distill text data. We demonstrate that text distillation outperforms other methods across multiple datasets. For example, models attain almost their original accuracy on the IMDB sentiment analysis task using just 20 distilled sentences. Our code can be found at https://github.com/ilia10000/dataset-distillation.},
   author = {Ilia Sucholutsky and Matthias Schonlau},
   doi = {10.1109/IJCNN52387.2021.9533769},
   isbn = {9780738133669},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   month = {7},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Soft-Label Dataset Distillation and Text Dataset Distillation},
   volume = {2021-July},
   year = {2021},
}
@article{Ji2022,
   abstract = {Federated learning (FL) is a novel machine learning setting that enables on-device intelligence via decentralized training and federated optimization. Deep neural networks rapid development facilitates the learning techniques for modeling complex problems and emerges into federated deep learning under the federated setting. However, the tremendous amount of model parameters burdens the communication network with a high load of transportation. This article introduces two approaches for improving communication efficiency by dynamic sampling and top-kk selective masking. The former controls the fraction of selected client models dynamically, while the latter selects parameters with top-kk largest values of difference for federated updating. Experiments on convolutional image classification and recurrent language modeling are conducted on three public datasets to show our proposed methods effectiveness.},
   author = {Shaoxiong Ji and Wenqi Jiang and Anwar Walid and Xue Li},
   doi = {10.1109/MIS.2021.3114610},
   issn = {19411294},
   issue = {2},
   journal = {IEEE Intelligent Systems},
   pages = {27-34},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Dynamic Sampling and Selective Masking for Communication-Efficient Federated Learning},
   volume = {37},
   year = {2022},
}
@article{Zhou2020,
   abstract = {The lack of interagency government data sharing (IDS) has emerged as a central problem for the development of China's Internet Plus Government Services (IPGS). To identify, explain, and understand barriers to IDS in China, a total of 1495 articles were retrieved from the China Academic Journal Network Publishing Database (CAJD) and China Science Periodical Database (CSPD). After screening, 55 articles were analyzed using a thematic analysis approach. The analysis pointed at 19 barriers to IDS in four themes: External environmental barriers, interagency partnership barriers, organizational readiness barriers, and individual motivation barriers. The conceptualization of the research findings revealed five trans-barrier conceptual dimensions (political and structural, territorial, leadership, financial, and IT), from which further, more in-depth investigations should be performed. Also, the conceptualization indicated that territoriality is possibly the main barrier to China's IDS development. Territoriality offers a novel prospective for studying IDS. This research presents a robust theoretical basis for future study. Also, it provides an overview on IDS development in China and offers useful insights for similar efforts in different countries.},
   author = {Lihong Zhou and Ruhua Huang and Baiyang Li},
   doi = {10.1016/J.LISR.2020.101031},
   issn = {0740-8188},
   issue = {3},
   journal = {Library & Information Science Research},
   month = {7},
   pages = {101031},
   publisher = {JAI},
   title = {“What is mine is not thine”: Understanding barriers to China's interagency government data sharing from existing literature},
   volume = {42},
   year = {2020},
}
@article{Welch2016,
   abstract = {Although the rise of big data, open government, and social media imply greater data sharing, expectations currently do not match reality as many consider data exchange in government to be inadequate. Based on prior research, Additionally, the paper distinguishes technical management capacity and technical engagement capacity effects on agencies' sharing behavior. We test hypotheses predicting sharing behavior of municipal government agencies with other agencies and with non-government organizations using data from a 2012 national survey of U.S. municipal government managers. We find that data sharing with both government and non-government organizations is more strongly determined by persuasive mechanisms and technical engagement capacity, although technical management capacity is also important for sharing with other government agencies. Conclusions provide insights for future research directions and practice.},
   author = {Eric W. Welch and Mary K. Feeney and Chul Hyun Park},
   doi = {10.1016/J.GIQ.2016.07.002},
   issn = {0740-624X},
   issue = {3},
   journal = {Government Information Quarterly},
   keywords = {Coercion,Data sharing,Local government,Persuasion,Technical capacity,e-Government},
   month = {7},
   pages = {393-403},
   publisher = {JAI},
   title = {Determinants of data sharing in U.S. city governments},
   volume = {33},
   year = {2016},
}
@article{Wang2018,
   abstract = {Interagency government data sharing plays an important role in promoting the coordination of government departments and improving public services. Under the guidance of a theoretical framework that combines the force-field theory of change and the theory of mechanism, this study conducted a case study on two Chinese urban governments and built a dynamic mechanism model for IGDS. The model consists of six forces acting on IGDS, as well as their activities, effects and interactions. Some effects of them are context-dependent. This model can be used to explain the reasons of various barriers to IGDS and thus to guide government departments and policy makers design more specific and targeted dynamic mechanisms to promote IGDS. Finally, several mechanisms were discussed in the context of policy making.},
   author = {Fang Wang},
   doi = {10.1016/J.GIQ.2018.08.003},
   issn = {0740-624X},
   issue = {4},
   journal = {Government Information Quarterly},
   keywords = {Acting forces,Dynamic mechanism,Force-field theory of change,IGDS,Interagency government data sharing},
   month = {10},
   pages = {536-546},
   publisher = {JAI},
   title = {Understanding the dynamic mechanism of interagency government data sharing},
   volume = {35},
   year = {2018},
}
@article{Otjacques2014,
   abstract = {In business and government organizations, information systems often handle sensitive data about individuals and other organizations, using various kinds of identifiers. The growing cooperation of o...},
   author = {Benoît Otjacques and Patrik Hitzelberger and Fernand Feltz},
   doi = {10.2753/MIS0742-1222230403},
   issn = {07421222},
   issue = {4},
   journal = {https://doi.org/10.2753/MIS0742-1222230403},
   keywords = {data sharing,e-government,identifier,identity management,information sharing,interoperability,interorganizational information systems,privacy},
   month = {3},
   note = {europian union, SIN, legal issue in UK, Austria},
   pages = {29-51},
   publisher = { Routledge },
   title = {Interoperability of E-Government Information Systems: Issues of Identification and Data Sharing},
   volume = {23},
   url = {https://www.tandfonline.com/doi/abs/10.2753/MIS0742-1222230403},
   year = {2014},
}
@article{Marisi2022,
   author = {Aristo Marisi and Adiputra Pangaribuan and Anna B Bosch and Hugh Spitzer and Cynthia Alkon and Joseph Janes},
   title = {Cooperation and Non-Cooperation in Indonesian Criminal Case Processing: Ego Sektoral in Action},
   year = {2022},
}
@article{Liu2021,
   abstract = {The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.},
   author = {Bo Liu and Wenny Rahayu and Ming Ding and Sina Shaham and Farhad Farokhi and Zihuai Lin and Z Lin},
   doi = {10.1145/3436755},
   issue = {2},
   journal = {ACM Comput. Surv},
   keywords = {Additional Key Words and Phrases: Machine learning, privacy, deep learning, differential privacy ACM Reference format:,CCS Concepts: • Security and privacy → Privacy protections,Social network security and privacy},
   note = {survey thread to ML },
   pages = {31},
   title = {When Machine Learning Meets Privacy: A Survey and Outlook},
   volume = {54},
   url = {https://doi.org/10.1145/3436755},
   year = {2021},
}
@article{Liu2019,
   abstract = {Federated Learning is a collaborative machine learning framework to train a
deep learning model without accessing clients' private data. Previous works
assume one central parameter server either at the cloud or at the edge. The
cloud server can access more data but with excessive communication overhead and
long latency, while the edge server enjoys more efficient communications with
the clients. To combine their advantages, we propose a client-edge-cloud
hierarchical Federated Learning system, supported with a HierFAVG algorithm
that allows multiple edge servers to perform partial model aggregation. In this
way, the model can be trained faster and better communication-computation
trade-offs can be achieved. Convergence analysis is provided for HierFAVG and
the effects of key parameters are also investigated, which lead to qualitative
design guidelines. Empirical experiments verify the analysis and demonstrate
the benefits of this hierarchical architecture in different data distribution
scenarios. Particularly, it is shown that by introducing the intermediate edge
servers, the model training time and the energy consumption of the end devices
can be simultaneously reduced compared to cloud-based Federated Learning.},
   author = {Lumin Liu and Jun Zhang and S. H. Song and Khaled B. Letaief},
   doi = {10.1109/ICC40277.2020.9148862},
   isbn = {9781728150895},
   issn = {15503607},
   journal = {IEEE International Conference on Communications},
   keywords = {Edge Learning,Federated Learning,Mobile Edge Computing},
   month = {5},
   note = {combine cloud FL and edge FL},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Client-Edge-Cloud Hierarchical Federated Learning},
   volume = {2020-June},
   url = {https://arxiv.org/abs/1905.06641v2},
   year = {2019},
}
@article{Liang2022,
   abstract = {This article examines citizen scoring in China's Social Credit Systems (SCSs). Focusing on 50 municipal cases that potentially cover 210 million population, we analyze how state actors quantify social and economic life into measurable and comparable metrics and discuss the implications of SCSs through the lens of social quantification. Our results illustrate that the SCSs are envisioned and designed as social quantification practices including two facets: a normative apparatus encouraging “good” citizens and social morality, and a regulative apparatus disciplining “deviant” behaviors and enforcing social management. We argue that the SCSs illustrate the significant shift in which state actors increasingly become data processors whereas citizens are reconfigured as datafied subjects that can be measured, compared, and governed. We suggest that the SCSs function as infrastructures of social quantification for enforcing social management, constructing differences, and nudging people towards desired behaviors defined by the state.},
   author = {Fan Liang and Yuchen Chen},
   doi = {10.1002/POI3.291},
   issn = {1944-2866},
   issue = {1},
   journal = {Policy & Internet},
   keywords = {China,Social Credit Systems,credit scores,cuantificación social,data,datos,infraestructuras,infrastructures,puntajes de crédito,sistema de crédito social,social quantification,中国,信用分数,基础设施,数据,社会信用体系,社会量化},
   month = {3},
   note = {public vs state sector employs cloud. china study case of uram},
   pages = {114-135},
   publisher = {John Wiley & Sons, Ltd},
   title = {The making of “good” citizens: China's Social Credit Systems and infrastructures of social quantification},
   volume = {14},
   url = {https://onlinelibrary-wiley-com.ezproxy.library.uq.edu.au/doi/full/10.1002/poi3.291 https://onlinelibrary-wiley-com.ezproxy.library.uq.edu.au/doi/abs/10.1002/poi3.291 https://onlinelibrary-wiley-com.ezproxy.library.uq.edu.au/doi/10.1002/poi3.291},
   year = {2022},
}
@article{Dencik2019,
   abstract = {Drawing on the first comprehensive investigation into the uses of data analytics in UK public services, this article outlines developments and practices surrounding the upsurge in data-driven forms of what we term ‘citizen scoring’. This refers to the use of data analytics in government for the purposes of categorisation, assessment and prediction at both individual and population level. Combining Freedom of Information requests and semi-structured interviews with public sector workers and civil society organisations, we detail the practices surrounding these developments and the nature of concerns expressed by different stakeholder groups as a way to elicit the heterogeneity, tensions and negotiations that shape the contemporary landscape of data-driven governance. Described by practitioners as a way to achieve a ‘golden view’ of populations, we argue that data systems need to be situated in this context in order to understand the wider politics of such a ‘view’ and the implications this has for state-citizen relations in the scoring society.},
   author = {Lina Dencik and Joanna Redden and Arne Hintz and Harry Warne},
   doi = {10.14763/2019.2.1413},
   issn = {21976775},
   issue = {2},
   journal = {Internet Policy Review},
   keywords = {Citizenship,Data governance,Data scores,Datafication,Public sector},
   publisher = {Alexander von Humboldt Institute for Internet and Society},
   title = {The ‘golden view’: Data-driven governance in the scoring society},
   volume = {8},
   year = {2019},
}
@article{,
   author = {David C Wyld and Robert Maurin},
   note = {10 challenges of cloud computing. },
   title = {Moving to the Cloud: An Introduction to Cloud Computing in Government E-Government Series},
}
@article{Shibuya2020,
   abstract = {This book examines the digital transformation of identity in the age of artificial intelligence. It articulates the nature of identity of human beings, based on cutting-edge knowledge in the field of AI and big-data sciences, and discusses identity by drawing on comprehensive investigations in digital social sciences and exploring wider disciplines related to philosophy, ethics, sociology, STS, computer sciences, engineering, and medical sciences. Reviewing contemporary conditions proliferated by advanced technological trends and unveiling social mechanisms of human identity, this book appeals to undergraduate and graduate students as well as academic researchers.},
   author = {Kazuhiko Shibuya},
   doi = {10.1007/978-981-15-2248-2/COVER},
   isbn = {9789811522482},
   journal = {Digital Transformation of Identity in the Age of Artificial Intelligence},
   keywords = {Artificial intelligence,Digital and computational social science,Ethics and philosophy on the ai,Identity of the humankind,Studies of science and technology on digital transformation},
   month = {1},
   pages = {1-286},
   publisher = {Springer Singapore},
   title = {Digital transformation of identity in the age of artificial intelligence},
   year = {2020},
}
@article{,
   author = {The World Bank},
   doi = {10.1596/38201},
   journal = {National Digital Identity and Government Data Sharing in Singapore},
   month = {1},
   publisher = {World Bank},
   title = {National Digital Identity and Government Data Sharing in Singapore},
   url = {www.worldbank.org},
   year = {2022},
}
@article{Liang2017Cloud,
   doi = {10.1016/J.GIQ.2017.06.002},
   issn = {0740-624X},
   issue = {3},
   journal = {Government Information Quarterly},
   keywords = {Cloud provider support,Cloud trust,Environmental stimulus,Innovation adoption,Organizational readiness,Technology driving,e-Government cloud},
   month = {9},
   pages = {481-495},
   publisher = {JAI},
   title = {Exploring the determinant and influence mechanism of e-Government cloud adoption in government agencies in China},
   volume = {34},
   year = {2017},
}
@article{Kushagra2022,
   abstract = {Purpose: Government is the biggest spender on cloud computing technology but a very limited study and data sets are available to assess the cloud adoption trends in government organizations in India. As India is ushering towards “Digital India” it becomes essential for the government to embrace the cloud to enhance governance and meet the citizen expectations. This paper aims to discuss the evolution of cloud computing (Meghraj) in government organizations by examining the various information technology (IT) and cloud policies, thereby focusing on the policy gaps. The second part of this study assesses the cloud adoption trend by analyzing adopted cloud services, deployments models, leading sectors in cloud adoption and cloud approach. Eventually, in consultation with experts, a conceptual framework for cloud adoption in the government organizations of India is developed for wider cloud adoption. Design/methodology/approach: The authors reviewed various IT/cloud policies and related literature to find the policy gaps for slow cloud adoption in government organizations. Authors have researched to collect the data from the various government procurement portals and analysed the tender and contracts of 500 organizations for cloud requirements to infer the cloud adoption trends. Based on the review of policy gaps, adoption trends and by consulting the experts a conceptual cloud adoption framework has been developed for wider cloud adoption in government organizations. Findings: This study can be a pathfinder where the most innovative findings are about the cloud adoption trends in the government organizations in the time frame from 2013 till 2020. Several key findings are – the public cloud are the most widely adopted, infrastructure as a service model is the most used services, the majority of the applications migrating to the cloud are legacy applications, the leading sector in cloud adoption are – IT, transport and education. It is observed that the pandemic Covid-19 has acted as a catalyst and accelerated cloud adoption in government organizations. Eventually, a conceptual cloud adoption framework has been suggested addressing the policy gaps, deficiencies, overcoming the gaps and their related outcomes for the wider cloud adoption in the government organizations. Practical implications: The findings of this work highlight the cloud adoption trends in government organizations which can prove vital to the policymakers. This work will assist policymakers, government organizations, researchers, IT professionals and others interested in analyzing the state of cloud adoption. The conceptual cloud adoption framework developed endeavours to uncover the policy gaps, suggest the gap resolution mechanism and outcomes which may assist the organization for wider cloud adoption. This research work effectively connects the policies to practice by stimulating the interest in understanding the policies, strategies and thereby creating the enabling environment for cloud adoption. This study provides feedback on cloud adoption trends which can assist in policy refinement and further strengthen policy/strategies. Originality/value: As of date, there is limited data available for cloud adoption in government organizations. This work uniquely presents the cloud projections which helps to gain insights on cloud adoption trends in government organizations. This study is the first of its kind, focusing on cloud adoption in the unexplored government sector. This study provides a comprehensive summary of adoption statistics, policy analysis and practice in government organizations of India.},
   author = {Kshitij Kushagra and Dr Sanjay Dhingra},
   doi = {10.1108/JSTPM-06-2019-0058/FULL/PDF},
   issn = {20534639},
   issue = {4},
   journal = {Journal of Science and Technology Policy Management},
   keywords = {Cloud adoption,Cloud impact,Cloud policy,Cloud service provider (CSP),Covid-19,Digital transformation},
   month = {11},
   note = {survey of cloud technology for gov. problems before cloud era (pre-cloud). },
   pages = {925-951},
   publisher = {Emerald Publishing},
   title = {Cloud doctrine: impact on cloud adoption in the government organizations of India},
   volume = {13},
   year = {2022},
}
@article{Irion2012,
   abstract = {Government cloud services are a new development at the intersection of electronic government and cloud computing which holds the promise of rendering government service delivery more effective and ef?cient. Cloud services are virtual, dynamic, and potentially stateless which has triggered governments' concern for data sovereignty. This article explores data sovereignty in relation to government cloud services and how national strategies and international policy evolve. It concludes that data sovereignty presents national governments with a legal risk that cannot be adequately addressed with technology or through contractual arrangements alone. Governments therefore adopt strategies that aim to retain sovereignty over government information. © 2012 Policy Studies Organization Published by Wiley Periodicals, Inc.},
   author = {Kristina Irion},
   doi = {10.1002/POI3.10},
   issn = {1944-2866},
   issue = {3-4},
   journal = {Policy & Internet},
   keywords = {cloud computing,data ownership,data sovereignty,electronic government,information assurance,international data transfers,public policy},
   month = {12},
   note = {data sovereignty in cloud computing. why cloud database is not good for government. difficulties etc.  United States, the United Kingdom, Australia, and Canada},
   pages = {40-71},
   publisher = {John Wiley & Sons, Ltd},
   title = {Government Cloud Computing and National Data Sovereignty},
   volume = {4},
   url = {https://onlinelibrary-wiley-com.ezproxy.library.uq.edu.au/doi/full/10.1002/poi3.10 https://onlinelibrary-wiley-com.ezproxy.library.uq.edu.au/doi/abs/10.1002/poi3.10 https://onlinelibrary-wiley-com.ezproxy.library.uq.edu.au/doi/10.1002/poi3.10},
   year = {2012},
}
@article{MoFan2022,
   abstract = {Mobile networks and devices provide the users with ubiquitous connectivity, while many of their functionality and business models rely on data analysis and processing. In this context, Machine Lear...},
   author = {MoFan and HaddadiHamed and KatevasKleomenis and MarinEduard and PerinoDiego and KourtellisNicolas},
   doi = {10.1145/3529706.3529715},
   issn = {2375-0529},
   issue = {4},
   journal = {GetMobile: Mobile Computing and Communications},
   month = {3},
   note = {Attack on Federated learning with confidential computing},
   pages = {35-38},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {PPFL},
   volume = {25},
   url = {https://dl-acm-org.ezproxy.library.uq.edu.au/doi/10.1145/3529706.3529715},
   year = {2022},
}
@article{Cao2022,
   abstract = {Federated learning is a kind of distributed machine learning. Researchers have conducted extensive research on federated learning's security defences and backdoor attacks. However, most studies are based on the assumption federated learning participant's data obey iid (independently identically distribution). This paper will evaluate the security issues of non-iid federated learning and propose a new attack strategy. Compared with the existing attack strategy, our approach has three innovations. The first one, we conquer foolsgold [1] defences through the attacker's negotiation. In the second one, we propose a modified gradient upload strategy for fedsgd backdoor attack, which significantly improves the backdoor attack's confidentiality on the original basis. Finally, we offer a bit Trojan method to realize continuous on non-iid federated learning. We conduct extensive experiments on different datasets to illustrate our backdoor attack strategy is highly efficient, confidential, and continuous on non-iid federated learning.},
   author = {Jiarui Cao and Liehuang Zhu},
   doi = {10.1145/3529836.3529845},
   isbn = {9781450395700},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Federated learning,backdoor attack,model replacement},
   month = {2},
   pages = {18-27},
   publisher = {Association for Computing Machinery},
   title = {A highly efficient, confidential, and continuous federated learning backdoor attack strategy},
   url = {https://dl-acm-org.ezproxy.library.uq.edu.au/doi/10.1145/3529836.3529845},
   year = {2022},
}
@article{Paper2021,
   abstract = {Federated Learning (FL) is an emerging machine learning paradigm that enables
multiple clients to jointly train a model to take benefits from diverse
datasets from the clients without sharing their local training datasets. FL
helps reduce data privacy risks. Unfortunately, FL still exist several issues
regarding privacy and security. First, it is possible to leak sensitive
information from the shared training parameters. Second, malicious clients can
collude with each other to steal data, models from regular clients or corrupt
the global training model. To tackle these challenges, we propose SecFL - a
confidential federated learning framework that leverages Trusted Execution
Environments (TEEs). SecFL performs the global and local training inside TEE
enclaves to ensure the confidentiality and integrity of the computations
against powerful adversaries with privileged access. SecFL provides a
transparent remote attestation mechanism, relying on the remote attestation
provided by TEEs, to allow clients to attest the global training computation as
well as the local training computation of each other. Thus, all malicious
clients can be detected using the remote attestation mechanisms.},
   author = {Demo Paper and Do Le Quoc and Christof Fetzer},
   month = {10},
   note = {Confidential Computing, Fedferated learning},
   title = {SecFL: Confidential Federated Learning using TEEs},
   url = {https://arxiv.org/abs/2110.00981v2},
   year = {2021},
}
@article{Shao2023,
   abstract = {Large language models can perform various reasoning tasks by using
chain-of-thought prompting, which guides them to find answers through
step-by-step demonstrations. However, the quality of the prompts depends on the
demonstrations given to the models, and creating many of them by hand is
costly. We introduce Synthetic prompting, a method that leverages a few
handcrafted examples to prompt the model to generate more examples by itself,
and selects effective demonstrations to elicit better reasoning. Our method
alternates between a backward and forward process to generate new examples. The
backward process generates a question that match a sampled reasoning chain, so
that the question is solvable and clear. The forward process produces a more
detailed reasoning chain for the question, improving the quality of the
example. We evaluate our method on numerical, symbolic, and algorithmic
reasoning tasks, and show that it outperforms existing prompting techniques.},
   author = {Zhihong Shao and Yeyun Gong and Yelong Shen and Minlie Huang and Nan Duan and Weizhu Chen},
   month = {2},
   title = {Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models},
   url = {https://arxiv.org/abs/2302.00618v1},
   year = {2023},
}
@article{Li2021,
   abstract = {Deep learning techniques have achieved great success in many fields, while at
the same time deep learning models are getting more complex and expensive to
compute. It severely hinders the wide applications of these models. In order to
alleviate this problem, model distillation emerges as an effective means to
compress a large model into a smaller one without a significant drop in
accuracy. In this paper, we study a related but orthogonal issue, data
distillation, which aims to distill the knowledge from a large training dataset
down to a smaller and synthetic one. It has the potential to address the large
and growing neural network training problem based on the small dataset. We
develop a novel data distillation method for text classification. We evaluate
our method on eight benchmark datasets. The results that the distilled data
with the size of 0.1% of the original text data achieves approximately 90%
performance of the original is rather impressive.},
   author = {Yongqi Li and Wenjie Li},
   doi = {10.1145/1122445.1122456},
   keywords = {Data Distillation,Text Classification},
   month = {4},
   note = {generate human-unreadable numeric},
   publisher = {ACM},
   title = {Data Distillation for Text Classification},
   url = {https://arxiv.org/abs/2104.08448v1},
   year = {2021},
}
@article{Shu2022,
   abstract = {Existing convolutional neural network (CNN)-based methods have limitations in long-term multi-damage recognition for civil infrastructures. Owing to catastrophic forgetting, the recognition accuracy of such networks decreases when structural damage types keep increasing progressively, not to mention other issues such as an increased number of model parameters and data storage. Thus, this study proposes a continual-learning-based damage recognition model (CLDRM) for the recognition of multi-damage and relevant structural components in civil infrastructures. By integrating the Learning without Forgetting (LwF) method into the residual network with 34 layers, the CLDRM can be continuously trained for multiple recognition tasks without using the data from old tasks. The performance of the CLDRM is experimentally validated through four recognition tasks, namely, damage level, spalling check, component-type determination, and damage-type determination, and it is compared to the performance of a conventional CNN with feature extraction, fine-tuning, duplication and fine-tuning, and joint training, respectively. In addition, the effects of changes in three parameters, namely, distillation temperature, feature correlation between tasks, and learning order, are further investigated to explore the optimal model parameters and applicable scenarios in multi-damage recognition. CLDRM gradually aggregates the features of continuous tasks through knowledge distillation, which provides higher recognition accuracy for both old and new tasks while maintaining the advantages of computational cost and data storage. The research outcome is expected to meet the long-term requirements of handling progressively increasing multi-type damage recognition tasks for civil infrastructures.},
   author = {Jiangpeng Shu and Wei Ding and Jiawei Zhang and Fangzheng Lin and Yuanfeng Duan},
   doi = {10.1002/STC.3093},
   issn = {1545-2263},
   issue = {11},
   journal = {Structural Control and Health Monitoring},
   keywords = {Learning without Forgetting,continual learning,damage recognition,knowledge distillation,multi,residual network},
   month = {11},
   note = {Train a new model using old weight + new weight from new data. applying low learning rate. avoid forgetting for old data. Does not use old data to retrain},
   pages = {e3093},
   publisher = {John Wiley & Sons, Ltd},
   title = {Continual-learning-based framework for structural damage recognition},
   volume = {29},
   url = {https://onlinelibrary-wiley-com.ezproxy.library.uq.edu.au/doi/full/10.1002/stc.3093 https://onlinelibrary-wiley-com.ezproxy.library.uq.edu.au/doi/abs/10.1002/stc.3093 https://onlinelibrary-wiley-com.ezproxy.library.uq.edu.au/doi/10.1002/stc.3093},
   year = {2022},
}
@article{Fayek2020,
   abstract = {Continual learning is the ability of a learning system to solve new tasks by utilizing previously acquired knowledge from learning and performing prior tasks without having significant adverse effects on the acquired prior knowledge. Continual learning is key to advancing machine learning and artificial intelligence. Progressive learning is a deep learning framework for continual learning that comprises three procedures: curriculum, progression, and pruning. The curriculum procedure is used to actively select a task to learn from a set of candidate tasks. The progression procedure is used to grow the capacity of the model by adding new parameters that leverage parameters learned in prior tasks, while learning from data available for the new task at hand, without being susceptible to catastrophic forgetting. The pruning procedure is used to counteract the growth in the number of parameters as further tasks are learned, as well as to mitigate negative forward transfer, in which prior knowledge unrelated to the task at hand may interfere and worsen performance. Progressive learning is evaluated on a number of supervised classification tasks in the image recognition and speech recognition domains to demonstrate its advantages compared with baseline methods. It is shown that, when tasks are related, progressive learning leads to faster learning that converges to better generalization performance using a smaller number of dedicated parameters.},
   author = {Haytham M. Fayek and Lawrence Cavedon and Hong Ren Wu},
   doi = {10.1016/J.NEUNET.2020.05.011},
   issn = {0893-6080},
   journal = {Neural Networks},
   keywords = {Computer vision,Continual learning,Deep learning,Machine learning,Neural networks,Speech recognition},
   month = {8},
   note = {progressive learning using three steps: The curriculum, The progression, The greedy layer wise. avoid catastropical forgetting and negative forward. },
   pages = {345-357},
   pmid = {32470799},
   publisher = {Pergamon},
   title = {Progressive learning: A deep learning framework for continual learning},
   volume = {128},
   year = {2020},
}
@article{Wang2022catastropicmimic,
   abstract = {Continual acquisition of novel experience without interfering with previously learned knowledge, i.e., continual learning, is critical for artificial neural networks, while limited by catastrophic forgetting. A neural network adjusts its parameters when learning a new task but then fails to conduct the old tasks well. By contrast, the biological brain can effectively address catastrophic forgetting through consolidating memories as more specific or more generalized forms to complement each other, which is achieved in the interplay of the hippocampus and neocortex, mediated by the prefrontal cortex. Inspired by such a brain strategy, we propose a novel approach named triple-memory networks (TMNs) for continual learning. TMNs model the interplay of the three brain regions as a triple-network architecture of generative adversarial networks (GANs). The input information is encoded as specific representations of data distributions in a generator, or generalized knowledge of solving tasks in a discriminator and a classifier, with implementing appropriate brain-inspired algorithms to alleviate catastrophic forgetting in each module. Particularly, the generator replays generated data of the learned tasks to the discriminator and the classifier, both of which are implemented with a weight consolidation regularizer to complement the lost information in the generation process. TMNs achieve the state-of-the-art performance of generative memory replay on a variety of class-incremental learning benchmarks on MNIST, SVHN, CIFAR-10, and ImageNet-50.},
   author = {Liyuan Wang and Bo Lei and Qian Li and Hang Su and Jun Zhu and Yi Zhong},
   doi = {10.1109/TNNLS.2021.3111019},
   issn = {21622388},
   issue = {5},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Brain-inspired algorithm,catastrophic forgetting,continual learning,deep learning},
   month = {5},
   note = {continue learning try to overcome catastropic forgetting. mimics the works of brain. },
   pages = {1925-1934},
   pmid = {34529579},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Triple-Memory Networks: A Brain-Inspired Method for Continual Learning},
   volume = {33},
   year = {2022},
}
@article{Sangermano2022,
   abstract = {Online Continual learning is a challenging learning scenario where the model
must learn from a non-stationary stream of data where each sample is seen only
once. The main challenge is to incrementally learn while avoiding catastrophic
forgetting, namely the problem of forgetting previously acquired knowledge
while learning from new data. A popular solution in these scenario is to use a
small memory to retain old data and rehearse them over time. Unfortunately, due
to the limited memory size, the quality of the memory will deteriorate over
time. In this paper we propose OLCGM, a novel replay-based continual learning
strategy that uses knowledge condensation techniques to continuously compress
the memory and achieve a better use of its limited size. The sample
condensation step compresses old samples, instead of removing them like other
replay strategies. As a result, the experiments show that, whenever the memory
budget is limited compared to the complexity of the data, OLCGM improves the
final accuracy compared to state-of-the-art replay strategies.},
   author = {Mattia Sangermano and Antonio Carta and Andrea Cossu and Davide Bacciu},
   isbn = {2206.11849v1},
   keywords = {Index Terms-continual learning,knowledge condensation,online learning,replay},
   month = {6},
   note = {online continual learning. overcome forgeting catastrophy with synthetizing inout data. knowledge destilation/ dataset condensation.},
   title = {Sample Condensation in Online Continual Learning},
   url = {https://arxiv.org/abs/2206.11849v1},
   year = {2022},
}
@article{Wang2018,
   abstract = {Model distillation aims to distill the knowledge of a complex model into a
simpler one. In this paper, we consider an alternative formulation called
dataset distillation: we keep the model fixed and instead attempt to distill
the knowledge from a large training dataset into a small one. The idea is to
synthesize a small number of data points that do not need to come from the
correct data distribution, but will, when given to the learning algorithm as
training data, approximate the model trained on the original data. For example,
we show that it is possible to compress 60,000 MNIST training images into just
10 synthetic distilled images (one per class) and achieve close to original
performance with only a few gradient descent steps, given a fixed network
initialization. We evaluate our method in various initialization settings and
with different learning objectives. Experiments on multiple datasets show the
advantage of our approach compared to alternative methods.},
   author = {Tongzhou Wang and Jun-Yan Zhu and Antonio Torralba and Alexei A. Efros},
   month = {11},
   title = {Dataset Distillation},
   url = {https://arxiv.org/abs/1811.10959v3},
   year = {2018},
}
@article{Zhou2022,
   abstract = {Federated Edge Learning considers a large amount of distributed edge nodes collectively train a global gradient-based model for edge computing in the Artificial Internet of Things, which significantly promotes the development of cloud computing. However, current federated learning algorithms take tens of communication rounds transmitting unwieldy model weights under ideal circumstances and hundreds when data is poorly distributed. This drawback directly results in expensive communication overhead for edge devices. Inspired by recent work on dataset distillation and distributed one-shot learning, we propose Distilled One-Shot Federated Learning (DOSFL) to significantly reduce the communication cost while achieving comparable performance. In just one round, each client distills their private dataset, sends the synthetic data to the server, and collectively trains a global model. The distilled data look like noise and are only useful to the specific model weights, <italic>i.e.,</italic> become useless after the model updates. With this weight-less and gradient-less design, the total communication cost of DOSFL is up to three orders of magnitude less than FedAvg while preserving up to 99&#x0025; performance of centralized training on both vision and language tasks with different models including CNN, LSTM, Transformer, <italic>etc</italic>. We demonstrate that an eavesdropping attacker cannot properly train a good model using the leaked distilled data, without knowing the initial model weights. DOSFL serves as an inexpensive method to quickly converge on a performant pre-trained model with less than 0.1&#x0025; communication cost of traditional methods.},
   author = {Yanlin Zhou and Xiyao Ma and Dapeng Wu and Xiaolin Li},
   doi = {10.1109/TCC.2022.3215520},
   issn = {21687161},
   journal = {IEEE Transactions on Cloud Computing},
   keywords = {Attack,Communication Reduction,Federated Learning,One-shot Learning},
   note = {federaated learning with synthetic dataset send over to server, generate global model. },
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Communication-Efficient and Attack-Resistant Federated Edge Learning with Dataset Distillation},
   year = {2022},
}
@article{Zeiler2012,
   abstract = {We present a novel per-dimension learning rate method for gradient descent
called ADADELTA. The method dynamically adapts over time using only first order
information and has minimal computational overhead beyond vanilla stochastic
gradient descent. The method requires no manual tuning of a learning rate and
appears robust to noisy gradient information, different model architecture
choices, various data modalities and selection of hyperparameters. We show
promising results compared to other methods on the MNIST digit classification
task using a single machine and on a large scale voice dataset in a distributed
cluster environment.},
   author = {Matthew D. Zeiler},
   keywords = {Gradient Descent,Index Terms-Adaptive Learning Rates,Machine Learn-ing,Neural Networks},
   month = {12},
   title = {ADADELTA: An Adaptive Learning Rate Method},
   url = {https://arxiv.org/abs/1212.5701v1},
   year = {2012},
}
@article{Kingma2014,
   abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization
of stochastic objective functions, based on adaptive estimates of lower-order
moments. The method is straightforward to implement, is computationally
efficient, has little memory requirements, is invariant to diagonal rescaling
of the gradients, and is well suited for problems that are large in terms of
data and/or parameters. The method is also appropriate for non-stationary
objectives and problems with very noisy and/or sparse gradients. The
hyper-parameters have intuitive interpretations and typically require little
tuning. Some connections to related algorithms, on which Adam was inspired, are
discussed. We also analyze the theoretical convergence properties of the
algorithm and provide a regret bound on the convergence rate that is comparable
to the best known results under the online convex optimization framework.
Empirical results demonstrate that Adam works well in practice and compares
favorably to other stochastic optimization methods. Finally, we discuss AdaMax,
a variant of Adam based on the infinity norm.},
   author = {Diederik P. Kingma and Jimmy Lei Ba},
   journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
   month = {12},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Adam: A Method for Stochastic Optimization},
   url = {https://arxiv.org/abs/1412.6980v9},
   year = {2014},
}
@article{Ramaswamy2019,
   abstract = {We show that a word-level recurrent neural network can predict emoji from
text typed on a mobile keyboard. We demonstrate the usefulness of transfer
learning for predicting emoji by pretraining the model using a language
modeling task. We also propose mechanisms to trigger emoji and tune the
diversity of candidates. The model is trained using a distributed on-device
learning framework called federated learning. The federated model is shown to
achieve better performance than a server-trained model. This work demonstrates
the feasibility of using federated learning to train production-quality models
for natural language understanding tasks while keeping users' data on their
devices.},
   author = {Swaroop Ramaswamy and Rajiv Mathews and Kanishka Rao and Françoise Beaufays},
   month = {6},
   title = {Federated Learning for Emoji Prediction in a Mobile Keyboard},
   url = {https://arxiv.org/abs/1906.04329v1},
   year = {2019},
}
@article{Chen2019,
   abstract = {We demonstrate that a character-level recurrent neural network is able to
learn out-of-vocabulary (OOV) words under federated learning settings, for the
purpose of expanding the vocabulary of a virtual keyboard for smartphones
without exporting sensitive text to servers. High-frequency words can be
sampled from the trained generative model by drawing from the joint posterior
directly. We study the feasibility of the approach in two settings: (1) using
simulated federated learning on a publicly available non-IID per-user dataset
from a popular social networking website, (2) using federated learning on data
hosted on user mobile devices. The model achieves good recall and precision
compared to ground-truth OOV words in setting (1). With (2) we demonstrate the
practicality of this approach by showing that we can learn meaningful OOV words
with good character-level prediction accuracy and cross entropy loss.},
   author = {Mingqing Chen and Rajiv Mathews and Tom Ouyang and Françoise Beaufays},
   month = {3},
   title = {Federated Learning Of Out-Of-Vocabulary Words},
   url = {https://arxiv.org/abs/1903.10635v1},
   year = {2019},
}
@article{Yang2018,
   abstract = {Federated learning is a distributed form of machine learning where both the
training data and model training are decentralized. In this paper, we use
federated learning in a commercial, global-scale setting to train, evaluate and
deploy a model to improve virtual keyboard search suggestion quality without
direct access to the underlying user data. We describe our observations in
federated training, compare metrics to live deployments, and present resulting
quality increases. In whole, we demonstrate how federated learning can be
applied end-to-end to both improve user experiences and enhance user privacy.},
   author = {Timothy Yang and Galen Andrew and Hubert Eichner and Haicheng Sun and Wei Li and Nicholas Kong and Daniel Ramage and Françoise Beaufays},
   month = {12},
   title = {Applied Federated Learning: Improving Google Keyboard Query Suggestions},
   url = {https://arxiv.org/abs/1812.02903v1},
   year = {2018},
}
@article{Hard2018,
   abstract = {We train a recurrent neural network language model using a distributed,
on-device learning framework called federated learning for the purpose of
next-word prediction in a virtual keyboard for smartphones. Server-based
training using stochastic gradient descent is compared with training on client
devices using the Federated Averaging algorithm. The federated algorithm, which
enables training on a higher-quality dataset for this use case, is shown to
achieve better prediction recall. This work demonstrates the feasibility and
benefit of training language models on client devices without exporting
sensitive user data to servers. The federated learning environment gives users
greater control over the use of their data and simplifies the task of
incorporating privacy by default with distributed training and aggregation
across a population of client devices.},
   author = {Andrew Hard and Kanishka Rao and Rajiv Mathews and Swaroop Ramaswamy and Françoise Beaufays and Sean Augenstein and Hubert Eichner and Chloé Kiddon and Daniel Ramage},
   keywords = {CIFG,Index Terms-Federated learning,NLP,keyboard,language modeling},
   month = {11},
   title = {Federated Learning for Mobile Keyboard Prediction},
   url = {https://arxiv.org/abs/1811.03604v2},
   year = {2018},
}
@article{Yang2017,
   abstract = {Mobile crowdsensing has become a novel and promising paradigm in collecting, analyzing, and exploiting massive amounts of data. However, the issue of data quality has not been carefully addressed. Low quality data contributions undermine the effectiveness and prospects of crowdsensing, and thus motivate the need for approaches to guarantee the high quality of the contributed data. In this paper, we integrate quality estimation and monetary incentive, and propose a quality-based truth estimation and surplus sharing method for crowdsensing. Specifically, we design an unsupervised learning approach to quantify the users' data qualities and long-term reputations, and exploit an outlier detection technique to filter out anomalous data items. Furthermore, we model the process of surplus sharing as a co-operative game, and propose a Shapley value-based method to determine each user's payment. We have conducted a real crowdsensing experiment and a large-scale simulation to evaluate our method. The evaluation results show that our approach achieves good performance in terms of both quality estimation and surplus sharing.},
   author = {Shuo Yang and Fan Wu and Shaojie Tang and Xiaofeng Gao and Bo Yang and Guihai Chen},
   doi = {10.1109/JSAC.2017.2676898},
   issn = {07338716},
   issue = {4},
   journal = {IEEE Journal on Selected Areas in Communications},
   keywords = {Mobile crowdsensing,Shapley value,data quality,truth discovery,unsupervised learning},
   month = {4},
   pages = {832-847},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {On Designing Data Quality-Aware Truth Estimation and Surplus Sharing Method for Mobile Crowdsensing},
   volume = {35},
   year = {2017},
}
@misc{McMahan2017,
   abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model ar-chitectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100× as compared to synchronized stochastic gradient descent.},
   author = {Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Aguera y Arcas},
   issn = {2640-3498},
   month = {4},
   pages = {1273-1282},
   publisher = {PMLR},
   title = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
   url = {https://proceedings.mlr.press/v54/mcmahan17a.html},
   year = {2017},
}
@article{Li2022,
   abstract = {Big data are usually characterized by heterogeneity in real-world cross-silo applications, such as healthcare, finance, and smart cities, leaving federated learning a big challenge. Further, many existing federated learning schemes fail to fully consider the diverse willingness and contributions of data providers in participation. In this paper, to address these challenges, we are motivated to propose an incentive and knowledge distillation based federated learning scheme for crosssilo applications. Specifically, we first develop a new federated learning framework, to support cooperative learning among diverse heterogeneous client models. Second, we devise an incentive mechanism, which not only stimulates workers to provide more high-quality data, but also improves clients' enthusiasm for participating in federated learning. Third, a novel knowledge distillation algorithm is designed to deal with data heterogeneity. Extensive experiments on MNISTfflEMNIST and CIFAR10/100 datasets with both IID and Non-IID settings, demonstrate the high effectiveness of the proposed scheme, compared with stateof-the-art studies.},
   author = {Beibei Li and Yaxin Shi and Yuqing Guo and Qinglei Kong and Yukun Jiang},
   doi = {10.1109/INFOCOMWKSHPS54753.2022.9798320},
   isbn = {9781665409261},
   journal = {INFOCOM WKSHPS 2022 - IEEE Conference on Computer Communications Workshops},
   keywords = {Artificial intelligence,data privacy,federated learning,incentive mechanism,knowledge distillation},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Incentive and Knowledge Distillation Based Federated Learning for Cross-Silo Applications},
   year = {2022},
}
@article{Tao2023,
   abstract = {Both Byzantine resilience and communication efficiency have attracted
tremendous attention recently for their significance in edge federated
learning. However, most existing algorithms may fail when dealing with
real-world irregular data that behaves in a heavy-tailed manner. To address
this issue, we study the stochastic convex and non-convex optimization problem
for federated learning at edge and show how to handle heavy-tailed data while
retaining the Byzantine resilience, communication efficiency and the optimal
statistical error rates simultaneously. Specifically, we first present a
Byzantine-resilient distributed gradient descent algorithm that can handle the
heavy-tailed data and meanwhile converge under the standard assumptions. To
reduce the communication overhead, we further propose another algorithm that
incorporates gradient compression techniques to save communication costs during
the learning process. Theoretical analysis shows that our algorithms achieve
order-optimal statistical error rate in presence of Byzantine devices. Finally,
we conduct extensive experiments on both synthetic and real-world datasets to
verify the efficacy of our algorithms.},
   author = {Youming Tao and Student Member and Sijia Cui and Wenlu Xu and Haofei Yin and Dongxiao Yu and Senior Member and Weifa Liang Senior Member and Xiuzhen Cheng},
   journal = {IEEE TRANSACTIONS ON COMPUTERS},
   keywords = {Byzantine resilience,Index Terms-edge intelligent systems,communication efficiency !,federated learning},
   month = {3},
   pages = {1},
   title = {Byzantine-Resilient Federated Learning at Edge},
   url = {https://arxiv.org/abs/2303.10434v1},
   year = {2023},
}
@article{Chen2021,
   abstract = {Cross-silo federated learning becomes popular in various fields due to its great promises in protecting training data. By carefully examining the interaction among distributed training nodes, we find that existing federated learning still suffers from security weakness and network bottleneck during model synchronization. It has no protection on training models, which also contain significant private information. In addition, many evidences have shown that model synchronization over wide-area network is slow, bottlenecking the whole learning process. To fill this research gap, we propose a novel cross-silo federated learning architecture that can protect both training data and model by using homomorphic encryption (HE). Instead of sharing the model parameters in plaintexts, we encrypt them using the HE, so that they can be aggregated in ciphertexts. In order to handle the inflated network traffic incurred by HE, we apply the in-network aggregation by exploiting the strong capability of programmable switches. A fast algorithm that jointly considers in-network aggregator placement and traffic engineering has been proposed and evaluated by extensive simulations.},
   author = {Fahao Chen and Peng Li and Toshiaki Miyazaki},
   doi = {10.1109/ICT-DM52643.2021.9664035},
   isbn = {9781665432856},
   journal = {2021 International Conference on Information and Communication Technologies for Disaster Management, ICT-DM 2021},
   keywords = {Federated learning,in-network aggregation,privacy preserving},
   pages = {49-56},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {In-Network Aggregation for Privacy-Preserving Federated Learning},
   year = {2021},
}
@article{Yang2019,
   abstract = {Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and se...},
   author = {Qiang Yang and Yang Liu and Tianjian Chen and Yongxin Tong},
   doi = {10.1145/3298981},
   isbn = {10.1145/3298981},
   issn = {21576912},
   issue = {2},
   journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
   keywords = {Federated learning,GDPR,transfer learning},
   month = {1},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Federated Machine Learning},
   volume = {10},
   url = {https://dl-acm-org.ezproxy.library.uq.edu.au/doi/10.1145/3298981},
   year = {2019},
}
@article{Ji2023,
   abstract = {Federated learning (FL) allows participants to train deep learning models collaboratively without disclosing their data to the server or any other participants, providing excellent value in the field of privacy-sensitive IoT. However, this distributed training paradigm requires clients to perform intensive computation for many iterations, which may exceed the capability of a typical IoT terminal with limited processing power, storage capacity, and energy budget. Heavy communication between the server and clients may also result in intolerant bandwidth requirements and energy consumption for many IoT systems. In this article, we introduce the FedQNN, a computation-communication-efficient FL framework for IoT scenarios. It is the first work that integrates ultralow-bitwidth quantization into the FL environment, allowing clients to perform lightweight fix-point computation efficiently with less power. Furthermore, both upstream and downstream data are significantly compressed for more efficient communication using a combination of sparsification and quantization strategies. We performed extensive experiments on a variety of data sets and models while comparing with other frameworks, and the results demonstrate that the proposed method can save up to 90% of our clients' computational energy, reduce model sizes by 30+ times, and significantly compress both communication bandwidth and transmitted data size while maintaining reasonable accuracy. The robustness against the non-independent and identically distributed (I.I.D.) condition is also validated.},
   author = {Yu Ji and Lan Chen},
   doi = {10.1109/JIOT.2022.3213650},
   issn = {23274662},
   issue = {3},
   journal = {IEEE Internet of Things Journal},
   keywords = {Deep learning,Internet of Things,edge computing,federated learning (FL),neural network quantization},
   month = {2},
   pages = {2494-2507},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {FedQNN: A Computation-Communication-Efficient Federated Learning Framework for IoT With Low-Bitwidth Neural Network Quantization},
   volume = {10},
   year = {2023},
}
@article{Wu2022,
   abstract = {Federated learning has attracted increasing attention with the emergence of
distributed data. While extensive federated learning algorithms have been
proposed for the non-convex distributed problem, federated learning in practice
still faces numerous challenges, such as the large training iterations to
converge since the sizes of models and datasets keep increasing, and the lack
of adaptivity by SGD-based model updates. Meanwhile, the study of adaptive
methods in federated learning is scarce and existing works either lack a
complete theoretical convergence guarantee or have slow sample complexity. In
this paper, we propose an efficient adaptive algorithm (i.e., FAFED) based on
the momentum-based variance-reduced technique in cross-silo FL. We first
explore how to design the adaptive algorithm in the FL setting. By providing a
counter-example, we prove that a simple combination of FL and adaptive methods
could lead to divergence. More importantly, we provide a convergence analysis
for our method and prove that our algorithm is the first adaptive FL algorithm
to reach the best-known samples $O(\epsilon^\{-3\})$ and $O(\epsilon^\{-2\})$
communication rounds to find an $\epsilon$-stationary point without large
batches. The experimental results on the language modeling task and image
classification task with heterogeneous data demonstrate the efficiency of our
algorithms.},
   author = {Xidong Wu and Feihu Huang and Zhengmian Hu and Heng Huang},
   month = {12},
   title = {Faster Adaptive Federated Learning},
   url = {https://arxiv.org/abs/2212.00974v3},
   year = {2022},
}
@article{Sattler2021,
   abstract = {Federated learning (FL) is currently the most widely adopted framework for collaborative training of (deep) machine learning models under privacy constraints. Albeit its popularity, it has been observed that FL yields suboptimal results if the local clients' data distributions diverge. To address this issue, we present clustered FL (CFL), a novel federated multitask learning (FMTL) framework, which exploits geometric properties of the FL loss surface to group the client population into clusters with jointly trainable data distributions. In contrast to existing FMTL approaches, CFL does not require any modifications to the FL communication protocol to be made, is applicable to general nonconvex objectives (in particular, deep neural networks), does not require the number of clusters to be known a priori, and comes with strong mathematical guarantees on the clustering quality. CFL is flexible enough to handle client populations that vary over time and can be implemented in a privacy-preserving way. As clustering is only performed after FL has converged to a stationary point, CFL can be viewed as a postprocessing method that will always achieve greater or equal performance than conventional FL by allowing clients to arrive at more specialized models. We verify our theoretical analysis in experiments with deep convolutional and recurrent neural networks on commonly used FL data sets.},
   author = {Felix Sattler and Klaus Robert Muller and Wojciech Samek},
   doi = {10.1109/TNNLS.2020.3015958},
   issn = {21622388},
   issue = {8},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Clustering,distributed learning,federated learning,multi-task learning},
   month = {8},
   pages = {3710-3722},
   pmid = {32833654},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Clustered Federated Learning: Model-Agnostic Distributed Multitask Optimization under Privacy Constraints},
   volume = {32},
   year = {2021},
}
@article{Bhuyan2022,
   abstract = {Federated learning is a form of distributed learning with the key challenge being the non-identically distributed nature of the data in the participating clients. In this paper, we extend federated learning to the setting where multiple unrelated models are trained simultaneously. Specifically, every client is able to train anyone of M models at a time and the server maintains a model for each of the M models which is typically a suitably averaged version of the model computed by the clients. We propose multiple policies for assigning learning tasks to clients over time. In the first policy, we extend the widely studied FedAvg to multi-model learning by allotting models to clients in an i.i.d. stochastic manner. In addition, we propose two new policies for client selection in a multi-model federated setting which make decisions based on current local losses for each client-model pair. We compare the performance of the policies on tasks involving synthetic and real-world data and characterize the performance of the proposed policies. The key take-away from our work is that the proposed multi-model policies perform better or at least as good as single model training using FedAvg.},
   author = {Neelkamal Bhuyan and Sharayu Moharir},
   doi = {10.1109/COMSNETS53615.2022.9668435},
   isbn = {9781665421041},
   journal = {2022 14th International Conference on COMmunication Systems and NETworkS, COMSNETS 2022},
   keywords = {federated learning,multi-armed bandits,multiobjective optimization},
   pages = {779-783},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multi-Model Federated Learning},
   year = {2022},
}
@article{Cho2022,
   abstract = {Federated Learning (FL) enables distributed training of machine learning models while keeping personal data on user devices private. While we witness increasing applications of FL in the area of mo...},
   author = {Hyunsung Cho and Akhil Mathur and Fahim Kawsar},
   doi = {10.1145/3550289},
   issn = {24749567},
   issue = {3},
   journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
   keywords = {Federated Learning,Human Activity Recognition,Human-centered computing → Ubiquitous and mobile computing systems and tools;},
   month = {9},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {FLAME},
   volume = {6},
   url = {https://dl-acm-org.ezproxy.library.uq.edu.au/doi/10.1145/3550289},
   year = {2022},
}
@book{Breiman1992,
   author = {Leo Breiman},
   city = {Philadelphia, Pa.},
   isbn = {0-89871-296-3},
   journal = {Probability},
   keywords = {Probabilities},
   publisher = {Society for Industrial and Applied Mathematics SIAM, 3600 Market Street, Floor 6, Philadelphia, PA 19104},
   title = {Probability / Leo Breiman},
   year = {1992},
}
@misc{OpenAI2023,
   author = {OpenAI},
   month = {3},
   note = {[Online; accessed 03-May-2023]},
   title = {Data processing addendum},
   url = {https://openai.com/policies/data-processing-addendum},
   year = {2023},
}
@misc{OpenAI2023,
   author = {OpenAI},
   month = {3},
   note = {[Online; accessed 03-May-2023]},
   title = {Service terms},
   url = {https://openai.com/policies/service-terms},
   year = {2023},
}
@misc{OpenAI2023,
   author = {OpenAI},
   month = {4},
   note = {[Online; accessed 03-May-2023]},
   title = {Privacy policy},
   url = {https://openai.com/policies/privacy-policy},
   year = {2023},
}
@misc{OpenAI2023,
   author = {OpenAI},
   month = {3},
   note = {[Online; accessed 03-May-2023]},
   title = {Terms of use},
   url = {https://openai.com/policies/terms-of-use},
   year = {2023},
}
@article{Ogburn2013,
   author = {Monique Ogburn and Claude Turner and Pushkar Dahal},
   journal = {Procedia Computer Science},
   pages = {502-509},
   publisher = {Elsevier},
   title = {Homomorphic encryption},
   volume = {20},
   year = {2013},
}
@misc{,
   author = {IBM},
   note = {Accessed: 25-4-2023},
   title = {COVID-19 and the future of business, Executive epiphanies reveal post-pandemic opportunities},
}
@article{Yekhanin2010,
   author = {Sergey Yekhanin},
   issue = {4},
   journal = {Communications of the ACM},
   pages = {68-73},
   publisher = {ACM New York, NY, USA},
   title = {Private information retrieval},
   volume = {53},
   year = {2010},
}
@article{Brundage2020,
   author = {Miles Brundage and Shahar Avin and Jasmine Wang and Haydn Belfield and Gretchen Krueger and Gillian K Hadfield and Heidy Khlaaf and Jingying Yang and Helen Toner and Ruth Fong and Tegan Maharaj and Pang Wei Koh and Sara Hooker and Jade Leung and Andrew Trask and Emma Bluemke and Jonathan Lebensold and Cullen O'Keefe and Mark Koren and Théo Ryffel and J. B Rubinovitz and Tamay Besiroglu and Federica Carugati and Jack Clark and Peter Eckersley and Sarah de Haas and Maritza Johnson and Ben Laurie and Alex Ingerman and Igor Krawczuk and Amanda Askell and Rosario Cammarota and Andrew Lohn and David Krueger and Charlotte Stix and Peter Henderson and Logan Graham and Carina Prunkl and Bianca Martin and Elizabeth Seger and Noa Zilberman and Seán Ó hÉigeartaigh and Frens Kroeger and Girish Sastry and Rebecca Kagan and Adrian Weller and Brian Tse and Elizabeth Barnes and Allan Dafoe and Paul Scharre and Ariel Herbert-Voss and Martijn Rasser and Shagun Sodhani and Carrick Flynn and Thomas Krendl Gilbert and Lisa Dyer and Saif Khan and Yoshua Bengio and Markus Anderljung},
   journal = {CoRR},
   title = {Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable
Claims},
   volume = {abs/2004.07213},
   url = {https://arxiv.org/abs/2004.07213},
   year = {2020},
}
@inproceedings{Naehrig2011,
   author = {Michael Naehrig and Kristin Lauter and Vinod Vaikuntanathan},
   journal = {Proceedings of the 3rd ACM workshop on Cloud computing security workshop},
   pages = {113-124},
   title = {Can homomorphic encryption be practical?},
   year = {2011},
}
@inproceedings{Mulligan2021,
   author = {Dominic P Mulligan and Gustavo Petri and Nick Spinale and Gareth Stockwell and Hugo J M Vincent},
   city = {Piscataway},
   isbn = {1665420251},
   journal = {2021 International Symposium on Secure and Private Execution Environment Design (SEED)},
   keywords = {Arm® Confidential Compute Architecture (Arm CCA) ; Cloud computing ; Codes ; Computer architecture ; Confidential Computing ; Electronics industry ; IceCap ; Instruction sets ; Instruction sets (computers) ; New technology ; Process control ; Protocols ; Remote Attestation ; Veracruz},
   pages = {132-138},
   publisher = {IEEE},
   title = {Confidential Computing-a brave new world},
   year = {2021},
}
@article{,
   author = {Judith Sáinz-Pardo D\'\iaz and Álvaro López Garc\'\ia},
   issue = {1},
   journal = {Scientific Data},
   pages = {785},
   publisher = {Nature Publishing Group UK London},
   title = {A Python library to check the level of anonymity of a dataset},
   volume = {9},
   year = {2022},
}
@misc{Fan2012,
   author = {Junfeng Fan and Frederik Vercauteren},
   note = {https://eprint.iacr.org/2012/144},
   title = {Somewhat Practical Fully Homomorphic Encryption},
   url = {https://eprint.iacr.org/2012/144},
   year = {2012},
}
@article{,
   abstract = {The aim of this study was to explore the extent to which persuasion principles are used in successful social engineering attacks. Seventy-four scenarios were extracted from 4 books on social engineering (written by social engineers) and analysed. Each scenario was split into attack steps, containing single interactions between offender and target. For each attack step, persuasion principles were identified. The main findings are that (a) persuasion principles are often used in social engineering attacks, (b) authority (1 of the 6 persuasion principles) is used considerably more often than others, and (c) single-principle attack steps occur more often than multiple-principle ones. The social engineers identified in the scenarios more often used persuasion principles compared to other social influences. The scenario analysis illustrates how to exploit the human element in security. The findings support the view that security mechanisms should include not only technical but also social countermeasures.},
   author = {Jan Willem Henik Bullée and Lorena Montoya and W Pieters and M Junger and P H Hartel},
   city = {Chichester},
   issn = {1544-4759},
   issue = {1},
   journal = {Journal of investigative psychology and offender profiling},
   keywords = {Criminology ; Deception ; Information security ; Literature study ; Persuasion ; Principles ; Social aspects ; Social engineering ; Social psychology ; Surgery},
   pages = {20-45},
   publisher = {Wiley Subscription Services, Inc},
   title = {On the anatomy of social engineering attacks: A literature-based dissection of successful attacks},
   volume = {15},
   year = {2018},
}
@article{Geppert2022,
   abstract = {A lack of trust in the providers is still a major barrier to cloud computing adoption – especially when sensitive data is involved. While current privacy-enhancing technologies, such as homomorphic encryption, can increase security, they come with a considerable performance overhead. As an alternative Trusted Executing Environment (TEE) provides trust guarantees for code execution in the cloud similar to transport layer security for data transport or advanced encryption standard algorithms for data storage. Cloud infrastructure providers like Amazon, Google, and Microsoft introduced TEEs as part of their infrastructure offerings. This review will shed light on the different technological options of TEEs, as well as give insight into organizational issues regarding their usage.},
   author = {Tim Geppert and Stefan Deml and David Sturzenegger and Nico Ebert},
   issn = {2624-9898},
   journal = {Frontiers in computer science (Lausanne)},
   keywords = {cloud computing ; confidential computing ; SGX ; TEE ; trusted execution environment},
   publisher = {Frontiers Media S.A},
   title = {Trusted Execution Environments: Applications and Organizational Challenges},
   volume = {4},
   year = {2022},
}
@article{Sagar2021,
   abstract = {Abstract
With the ever-growing data and the need for developing powerful machine learning models, data owners increasingly depend on various untrusted platforms (e.g., public clouds, edges, and machine learning service providers) for scalable processing or collaborative learning. Thus, sensitive data and models are in danger of unauthorized access, misuse, and privacy compromises. A relatively new body of research confidentially trains machine learning models on protected data to address these concerns. In this survey, we summarize notable studies in this emerging area of research. With a unified framework, we highlight the critical challenges and innovations in outsourcing machine learning confidentially. We focus on the cryptographic approaches for confidential machine learning (CML), primarily on model training, while also covering other directions such as perturbation-based approaches and CML in the hardware-assisted computing environment. The discussion will take a holistic way to consider a rich context of the related threat models, security assumptions, design principles, and associated trade-offs amongst data utility, cost, and confidentiality.},
   author = {Sharma Sagar and Chen Keke},
   city = {Singapore},
   issn = {2096-4862},
   issue = {1},
   journal = {Cybersecurity (Singapore)},
   keywords = {Cloud computing ; Confidential computing ; Cryptographic protocols ; Cryptography ; Machine learning ; Perturbation ; Platforms},
   pages = {30},
   publisher = {Springer Nature B.V},
   title = {Confidential machine learning on untrusted platforms: a survey},
   volume = {4},
   year = {2021},
}
@article{Gong2023,
   abstract = {Fully homomorphic encryption (FHE) has experienced significant development
and continuous breakthroughs in theory, enabling its widespread application in
various fields, like outsourcing computation and secure multi-party computing,
in order to preserve privacy. Nonetheless, the application of FHE is
constrained by its substantial computing overhead and storage cost. Researchers
have proposed practical acceleration solutions to address these issues. This
paper aims to provide a comprehensive survey for systematically comparing and
analyzing the strengths and weaknesses of FHE acceleration schemes, which is
currently lacking in the literature. The relevant researches conducted between
2019 and 2022 are investigated. We first provide a comprehensive summary of the
latest research findings on accelerating FHE, aiming to offer valuable insights
for researchers interested in FHE acceleration. Secondly, we classify existing
acceleration schemes from algorithmic and hardware perspectives. We also
propose evaluation metrics and conduct a detailed comparison of various
methods. Finally, our study presents the future research directions of FHE
acceleration, and also offers both guidance and support for practical
application and theoretical research in this field.},
   author = {Yanwei Gong and Xiaolin Chang and Jelena Mišić and Vojislav B Mišić and Jianhua Wang and Haoran Zhu},
   keywords = {Computer Science - Cryptography and Security},
   title = {Practical Solutions in Fully Homomorphic Encryption – A Survey Analyzing Existing Acceleration Methods},
   year = {2023},
}
@article{Li2023,
   abstract = {With the rapid progress of large language models (LLMs), many downstream NLP
tasks can be well solved given good prompts. Though model developers and
researchers work hard on dialog safety to avoid generating harmful content from
LLMs, it is still challenging to steer AI-generated content (AIGC) for the
human good. As powerful LLMs are devouring existing text data from various
domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether
the private information is included in the training data and what privacy
threats can these LLMs and their downstream applications bring. In this paper,
we study the privacy threats from OpenAI's model APIs and New Bing enhanced by
ChatGPT and show that application-integrated LLMs may cause more severe privacy
threats ever than before. To this end, we conduct extensive experiments to
support our claims and discuss LLMs' privacy implications.},
   author = {Haoran Li and Dadi Guo and Wei Fan and Mingshi Xu and Yangqiu Song},
   title = {Multi-step Jailbreaking Privacy Attacks on ChatGPT},
   year = {2023},
}
@article{Jia2022,
   abstract = {Now various privacy‐preserving techniques have been combined with machine learning to ensure training data security. However, during the training, users were unable to select data and labels adaptively in the server, which made the trained models challenging in meeting user needs. Repeated model training not only wastes server resources, but also reduces query efficiency. In this paper, we combine two privacy‐preserving technologies, searchable encryption and homomorphic encryption, and propose a highly flexible machine training framework. Homomorphic encryption technology is used to encrypt data, and symmetric searchable encryption technology is used to generate blind indexes and trapdoors. The server can perform ciphertext data search through blind query trapdoors and indexes, and can use the searched homomorphic ciphertext for model training. This framework significantly improves the flexibility of training and the fit of the model, and also ensures the security of dynamic data management. The server locally uses the header of the query trap and the storage address of the ciphertext model to build an automatically updatable ciphertext model table. On the basis of ensuring the effectiveness of the model, it greatly improves the efficiency of obtaining the model, effectively prevents redundant training when different users send the same request, and reasonably allocates resources in the server.},
   author = {Haixin Jia and Mohammed S Aldeen and Chuan Zhao and Shan Jing and Zhenxiang Chen},
   city = {New York},
   issn = {0884-8173},
   issue = {11},
   journal = {International journal of intelligent systems},
   keywords = {Algorithms ; Data encryption ; Data management ; Data search ; Encryption ; homomorphic encryption ; Machine learning ; Privacy ; privacy protection ; Queries ; searchable encryption ; Security ; Security management ; Training},
   pages = {9173-9191},
   publisher = {Wiley Subscription Services, Inc},
   title = {Flexible privacy‐preserving machine learning: When searchable encryption meets homomorphic encryption},
   volume = {37},
   year = {2022},
}
@article{Chen2023,
   author = {Keke Chen},
   issn = {1089-7801},
   journal = {IEEE internet computing},
   pages = {1-10},
   title = {Confidential High-Performance Computing in the Public Cloud},
   year = {2023},
}
@misc{Stone2014,
   author = {Scott Stone},
   pages = {6},
   title = {Breaking the Ice : Facebook Friending and Reference Interactions},
   year = {2014},
}
@article{Kwan2013,
   abstract = {This study examines the phenomenon of cyberbullying on Facebook and how it is related to school bullying among secondary school students in Singapore, aged 13-17. We also focus on generic use of Facebook and risky Facebook behaviors as the predictors of cyberbullying and victimization on Facebook. 1676 secondary students, from two secondary schools, participated in a pen and paper survey. The findings show that the intensity of Facebook use and engagement in risky Facebook behaviors were related to Facebook victimization and Facebook bullying, respectively. Moderately strong positive relationships between school bullying and Facebook bullying, as well as between school victimization and Facebook victimization, were also uncovered. ?? 2012 Elsevier Ltd. All rights reserved.},
   author = {Grace Chi En Kwan and Marko M. Skoric},
   doi = {10.1016/j.chb.2012.07.014},
   isbn = {0747-5632},
   issn = {07475632},
   issue = {1},
   journal = {Computers in Human Behavior},
   keywords = {Cyberbullying,Facebook,Risky Facebook use,Singapore},
   pages = {16-25},
   publisher = {Elsevier Ltd},
   title = {Facebook bullying: An extension of battles in school},
   volume = {29},
   url = {http://dx.doi.org/10.1016/j.chb.2012.07.014},
   year = {2013},
}
@article{Li2015,
   abstract = {Relatively few studies have examined the social implications of SNSs for various dimensions of social support, even though different dimensions of social support can have differential impacts on people's well-being. This study fills this gap by examining how Facebook interaction is related to various types of social support - enacted receiving and giving social support on Facebook and perceived social support in general. A survey of college students at a large public university in the U.S. reveals that Facebook interaction is positively related to receiving and giving social support on Facebook. However, neither social interaction nor enacted social support on Facebook is related to perceived social support in general.},
   author = {Xiaoqian Li and Wenhong Chen and Pawel Popiel},
   doi = {10.1016/j.chb.2015.04.066},
   isbn = {0747-5632},
   issn = {07475632},
   issue = {PA},
   journal = {Computers in Human Behavior},
   keywords = {Enacted social support on Facebook,Facebook,Perceived social support,Social networking sites},
   pages = {106-113},
   publisher = {Elsevier Ltd},
   title = {What happens on Facebook stays on Facebook? the implications of Facebook interaction for perceived, receiving, and giving social support},
   volume = {51},
   url = {http://dx.doi.org/10.1016/j.chb.2015.04.066},
   year = {2015},
}
@article{Tang2016,
   abstract = {Because of the prevalence of mobile devices, the overuse of social networking sites has become a global phenomenon. One of the most popular social networking sites, Facebook, has received a considerable attention in recent years, and the excessive use of Facebook has become a major concern in schools. The purpose of this study was to investigate the reasons for Facebook addiction. By surveying 894 college students in Taiwan, we found that although only 1% was classified as addicts, 17.8% were in the alert group. Approximately 80% of the students used Facebook every day, and 10% spent more than 8 h a day on Facebook. Interpersonal relationships and online social support were found to be positively associated with Facebook addiction; however, some personality traits, such as agreeableness, conscientiousness, and neuroticism, were negatively associated with Facebook addiction. Online interpersonal relationships and neuroticism were found to be prominent predictors of Facebook addiction. Practical implications are provided herein.},
   author = {Jih Hsin Tang and Ming Chun Chen and Cheng Ying Yang and Tsai Yuan Chung and Yao An Lee},
   doi = {10.1016/j.tele.2015.06.003},
   isbn = {0736-5853},
   issn = {07365853},
   issue = {1},
   journal = {Telematics and Informatics},
   keywords = {Facebook addiction,Online interpersonal relationships,Online social support,Personality traits},
   pages = {102-108},
   publisher = {Elsevier Ltd},
   title = {Personality traits, interpersonal relationships, online social support, and Facebook addiction},
   volume = {33},
   url = {http://dx.doi.org/10.1016/j.tele.2015.06.003},
   year = {2016},
}
@article{Satici2015,
   abstract = {Social networking sites are getting more widespread online communication forms all over the world. Especially adolescents and emerging adults use these sites to make connection, and share something. Although, Facebook is one of the most popular sites, research is still in nascent phase on the excessive use of Facebook. Hence, it is fundamental to determine potential predictors of overuse of Facebook. The present study investigated the relationship between well-being and problematic Facebook use. Participants were 311 university students [179(58%) female, 132(42%) male, Mage = 20.86 years, SD = 1.61] who completed the questionnaire packet including the Bergen Facebook Addiction Scale (Andreassen, Torsheim, Brunborg, & Pallesen, 2012), the Satisfaction with Life Scale (Diener, Emmons, Larsen, & Griffin, 1985), the Subjective Vitality Scale (Ryan & Frederick, 1997), the Flourishing Scale (Diener et al., 2010), and the Subjective Happiness Scale (Lyubomirsky & Lepper, 1999). A stepwise regression analyses was used with four independent variables (life satisfaction, subjective vitality, flourishing, and subjective happiness) to explain variance in problematic Facebook use. The results showed that life satisfaction, subjective vitality, flourishing, and subjective happiness were significant negative predictors of problematic Facebook use. The significance and limitations of the results are discussed.},
   author = {Seydi Ahmet Satici and Recep Uysal},
   doi = {10.1016/j.chb.2015.03.005},
   isbn = {0747-5632},
   issn = {07475632},
   journal = {Computers in Human Behavior},
   keywords = {Facebook use,Flourishing Life satisfaction,Problematic,Subjective happiness,Subjective vitality},
   pages = {185-190},
   publisher = {Elsevier Ltd},
   title = {Well-being and problematic Facebook use},
   volume = {49},
   url = {http://dx.doi.org/10.1016/j.chb.2015.03.005},
   year = {2015},
}