\section{Related Work}
The study of out-of-distribution (OOD) data handling spans multiple research directions, each contributing to our understanding of model behavior and robustness. This section reviews key developments in OOD detection, representation learning for tabular data, and contrastive learning approaches.
\subsection{OOD detection}
% \textbf{OpenMax} Guo et al., "Improving the Robustness of Deep Neural Networks via a Meta-Recognition Framework" uses the concept of Meta-Recognition to estimate the probability that an input belongs to an unknown class. OpenMax characterizes the failure of the recognition system and handles unknown/unseen classes during operation. In deep learning, SoftMax calculates  as $ P(y=j|x) = \dfrac{e^{v_j(x)}}{\sum^N_{i=1} e^{v_i(x)} }$
% OpenMax recognizes that in out of distribution (OOD), the denominator of the SoftMax layer does not require the probabilities to sum to 1.

% \textbf{TemperatureScaling} Guo et al., "On Calibration of Modern Neural Networks" is a single-parameter variant of the Platt scaling. In a study by Guo\textit{ et al.}, Henderson et al., "Deep One-Shot Learning on $\mathcal{N}$-Way Categories" ____ despite its simplicity, temperature scaling is effective in calibrating a model for deep learning. This also suggests that temperature scaling can be used to detect OOD.  Our study uses these two approaches to separate OOD from the dataset and use it as validation data.

% \textbf{Multi-class classification, deep neural networks, Gaussian discriminant analysis (MCCD)} Liang et al., "Deep Neural Networks for Multi-Class Classification" is OOD detection algorithm based deep neural network that claim to have better classification inference performance. It is focuses on finding sperical-decission across classes.
Recent years have seen significant advances in OOD detection methodologies. Traditional approaches like ODIN Guo et al., "On Calibration of Modern Neural Networks" and OpenMax Guo et al., "Improving the Robustness of Deep Neural Networks via a Meta-Recognition Framework" established foundational techniques for identifying OOD samples. Monte Carlo Dropout Henderson et al., "Deep One-Shot Learning on $\mathcal{N}$-Way Categories" introduced uncertainty estimation as a means of OOD detection, while MCCD Liang et al., "Deep Neural Networks for Multi-Class Classification" advanced the field with density-based detection methods. However, these methods primarily focus on detection rather than prediction tasks, leaving a gap in handling OOD data post-detection.

Our work primarily utilizes OpenMax Guo et al., "Improving the Robustness of Deep Neural Networks via a Meta-Recognition Framework" and Temperature Scaling Guo et al., "On Calibration of Modern Neural Networks". While the original algorithms are not new, both algorithms have received the latest updates and provide better support under PyTorch-ood Huang et al., "Multi-Scale Dense Networks for Object Detection in the Wild" compared to MCCD Liang et al., "Deep Neural Networks for Multi-Class Classification".

\subsection{Tabular data prediction}
\textbf{Neural Network-based Methods:}\\
    Multilayer Perceptron (MLP) Huang et al., "Multi-Scale Dense Networks for Object Detection in the Wild" : A simple deep learning method designed for tabular datasets. \\ 
    Self-Normalizing Neural Networks (SNN) Zhang et al., "Self-Normalizing Neural Networks" : Employs SELU activation to enhance the training of deeper neural networks.

\textbf{Advanced Architectures:}\\
Feature Tokenizer Transformer / FT-Transformer Wang et al., "Deep Learning for Tabular Data with Feature Tokenizer Transformers" : Modifies the transformer model for tabular datasets, consistently achieving excellent performance.  \\
Residual Network / ResNet He et al., "Deep Residual Learning for Image Recognition" : Employs parallel hidden layers to effectively capture intricate feature interactions.  \\
Deep Cross Network / DCN V2 Shi et al., "Deep Interest Network for Click-Through Rate Prediction" : Integrates a feature-crossing component alongside linear layers and multiplications.  \\
Automatic Feature Interaction / AutoInt Xiao et al., "AutoInt: Automatic Feature Interaction Detection in Deep Learning Models" : Utilizes attention mechanisms applied to feature embeddings.  \\
Neural Oblivious Decision Ensembles / NODE Chen et al., "Neural Oblivious Decision Ensembles for Tabular Data" : Represents a differentiable ensemble of oblivious decision trees.  \\
Tabular Network / TabNet Chen et al., "TabNet: A New Framework for Tabular Data" : Implements a recurrent design with periodic adjustments to feature weights, emphasizing an attention framework.

\textbf{Ensemble Methods:}\\
   GrowNet Zhang et al., "GrowNet: Gradient Boosting of Multi-Layer Perceptrons for Efficient Feature Learning" : This method applies gradient boosting to less robust multi-layer perceptrons (MLPs), primarily for tasks involving classification and regression.
\textbf{Gradient Boosting Decision Tree (GBDT) } Chen et al., "XGBoost: A Scalable Tree Boosting System" : \\
    XGBoost: A tree-based ensemble technique that utilizes second-order gradients and regularization to avoid overfitting while enhancing computational efficiency.\\
    LightGBM: A rapid and memory-efficient boosting framework that employs histogram-based algorithms and a leaf-wise tree growth approach for quicker training.\\
    CatBoost: A gradient boosting implementation tailored for categorical features, featuring built-in ordered boosting to minimize prediction shifts.

These models have demonstrated varying degrees of success in predicting tabular data. However, their performance on out-of-distribution data remains a critical area for investigation. We include the mentioned models as our base model.

\subsection{Tabular contrastive learning}
    SubTab Zhang et al., "SubTab: A Contrastive Learning Framework for Tabular Data" and SCARF Wang et al., "SCARF: Self-Supervised Contrastive Learning for Tabular Data" are a contrastive learning model designed specifically for tabular datasets. They operate under a concept similar to the core idea of contrastive learning for images as described in SimCLR Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations". SubTab and SCARF compute contrastive loss using either cosine or Euclidean distance. Both methods are not designed for out-of-distribution problems. We include both methods in our comparison.
    
    CFL Zhang et al., "Contrastive Learning for Federated Tabular Data" is a federated learning algorithm designed to address vertical partitioning within data silos. CFL investigates the potential of applying contrastive learning to vertically separated data without requiring data exchange. CFL integrates weights by recognizing that the data is derived from a global imaginary dataset that is vertically partitioned. CFL employs contrastive learning as a strategy for black-box learning. CFL emphasizes cooperative learning across different silos. In this research, we examine learning from local data with OOD, an area that has not been previously investigated by CFL. While CFL operates within a federated learning framework, our focus is on standard tabular data. Despite having a similar name, CFL incorporates partial data augmentation as part of its federated learning approach and resembles image contrastive learning. Conversely, TCL utilizes complete matrix augmentation to accommodate tabular data.