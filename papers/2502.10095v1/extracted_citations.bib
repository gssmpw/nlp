@article{AutoInt,
   author = {Weiping Song and Chence Shi and Zhiping Xiao and Zhijian Duan and Yewen Xu and Ming Zhang and Jian Tang},
   doi = {10.1145/3357384.3357925},
   journal = {International Conference on Information and Knowledge Management, Proceedings},
   keywords = {CTR prediction,Explainable recommendation,High-order feature interactions,Self attention},
   month = {10},
   pages = {1161-1170},
   publisher = {Association for Computing Machinery},
   title = {AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks},
   volume = {10},
   url = {http://arxiv.org/abs/1810.11921 http://dx.doi.org/10.1145/3357384.3357925},
   year = {2018},
}

@article{Bendale2016,
   author = {Abhijit Bendale and Terrance E. Boult},
   doi = {10.1109/CVPR.2016.173},
   isbn = {9781467388504},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   month = {12},
   pages = {1563-1572},
   publisher = {IEEE Computer Society},
   title = {Towards open set deep networks},
   volume = {2016-December},
   year = {2016},
}

@article{Chen2020SimClr,
   author = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
   issn = {2640-3498},
   month = {11},
   pages = {1597-1607},
   publisher = {PMLR},
   title = {A Simple Framework for Contrastive Learning of Visual Representations},
   url = {https://proceedings.mlr.press/v119/chen20j.html},
   year = {2020},
}

@article{DCNV2,
   author = {Ruoxi Wang and Rakesh Shivanna and Derek Z. Cheng and Sagar Jain and Dong Lin and Lichan Hong and Ed H. Chi},
   doi = {10.1145/3442381.3450078},
   journal = {The Web Conference 2021 - Proceedings of the World Wide Web Conference, WWW 2021},
   keywords = {CTR Prediction,Deep Learning,Feature Crossing,Neural Networks},
   month = {8},
   pages = {1785-1797},
   publisher = {Association for Computing Machinery, Inc},
   title = {DCN V2: Improved Deep and Cross Network and Practical Lessons for Web-scale Learning to Rank Systems},
   url = {http://arxiv.org/abs/2008.13535 http://dx.doi.org/10.1145/3442381.3450078},
   year = {2020},
}

@misc{Gal2016,
   author = {Yarin Gal and Zoubin Ghahramani},
   issn = {1938-7228},
   month = {6},
   pages = {1050-1059},
   publisher = {PMLR},
   title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
   url = {https://proceedings.mlr.press/v48/gal16.html},
   year = {2016},
}

@article{Grinsztajn2022,
author = {Grinsztajn, Leo and Oyallon, Edouard and Varoquaux, Gael},
file = {:home/ag/Downloads/NeurIPS-2022-why-do-tree-based-models-still-outperform-deep-learning-on-typical-tabular-data-Paper-Datasets_and_Benchmarks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {tabular data},
month = {dec},
pages = {507--520},
title = {{Why do tree-based models still outperform deep learning on typical tabular data?}},
volume = {35},
year = {2022}
}

@article{GrowNet,
   author = {Sarkhan Badirli and Xuanqing Liu and Zhengming Xing and Avradeep Bhowmik and Khoa Doan and Sathiya S. Keerthi},
   month = {2},
   title = {Gradient Boosting Neural Networks: GrowNet},
   url = {https://arxiv.org/abs/2002.07971v2},
   year = {2020},
}

@article{Guo2017,
   author = {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
   isbn = {9781510855144},
   journal = {34th International Conference on Machine Learning, ICML 2017},
   month = {6},
   pages = {2130-2143},
   publisher = {International Machine Learning Society (IMLS)},
   title = {On Calibration of Modern Neural Networks},
   volume = {3},
   url = {https://arxiv.org/abs/1706.04599v2},
   year = {2017},
}

@article{Lee2020,
archivePrefix = {arXiv},
arxivId = {2104.00941},
author = {Lee, Dongha and Yu, Sehun and Yu, Hwanjo},
doi = {10.1145/3394486.3403189},
eprint = {2104.00941},
file = {:home/ag/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Yu, Yu - 2020 - Multi-Class Data Description for Out-of-distribution Detection.pdf:pdf},
isbn = {9781450379984},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {Gaussian discriminant analysis,deep neural networks,multi-class classification,out-of-distribution detection},
mendeley-groups = {OOD},
month = {aug},
pages = {1362--1370},
publisher = {Association for Computing Machinery},
title = {{Multi-Class Data Description for Out-of-distribution Detection}},
url = {https://dl.acm.org/doi/10.1145/3394486.3403189},
year = {2020}
}

@article{Liang2017,
   author = {Shiyu Liang and Yixuan Li and R. Srikant},
   journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
   month = {6},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},
   url = {https://arxiv.org/abs/1706.02690v5},
   year = {2017},
}

@article{NODE,
   author = {Sergei Popov and Stanislav Morozov and Artem Babenko},
   journal = {8th International Conference on Learning Representations, ICLR 2020},
   month = {9},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data},
   url = {https://arxiv.org/abs/1909.06312v2},
   year = {2019},
}

@article{TabNet,
   author = {Sercan Arık and Tomas Pfister},
   doi = {10.1609/aaai.v35i8.16826},
   isbn = {9781713835974},
   issn = {2159-5399},
   journal = {35th AAAI Conference on Artificial Intelligence, AAAI 2021},
   month = {8},
   pages = {6679-6687},
   publisher = {Association for the Advancement of Artificial Intelligence},
   title = {TabNet: Attentive Interpretable Tabular Learning},
   volume = {8A},
   url = {https://arxiv.org/abs/1908.07442v5},
   year = {2019},
}

@misc{anonymizedMethod,
      title={Contrastive Federated Learning with Tabular Data Silos}, 
      author={Achmad Ginanjar and Xue Li and Wen Hua},
      year={2024},
      eprint={2409.06123},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.06123}, 
}

@misc{deepMCCD,
author = {Lee, Dongha and Yu, Sehun and Yu, Hwanjo},
title = {Multi-Class Data Description for Out-of-Distribution Detection},
year = {2020},
url={https://github.com/donalee/DeepMCDD},
}

@InProceedings{kirchheim2022pytorch,
    author    = {Kirchheim, Konstantin and Filax, Marco and Ortmeier, Frank},
    title     = {PyTorch-OOD: A Library for Out-of-Distribution Detection Based on PyTorch},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2022},
    pages     = {4351-4360}
}

@article{mlp,
  title={Feature selection using a multilayer perceptron},
  author={Ruck, Dennis W and Rogers, Steven K and Kabrisky, Matthew},
  journal={Journal of neural network computing},
  volume={2},
  number={2},
  pages={40--48},
  year={1990},
  publisher={Citeseer}
}

@inproceedings{multiClass,
author = {Lee, Dongha and Yu, Sehun and Yu, Hwanjo},
title = {Multi-Class Data Description for Out-of-distribution Detection},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403189},
doi = {10.1145/3394486.3403189},
abstract = {The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we present a deep multi-class data description, termed as Deep-MCDD, which is effective to detect out-of-distribution (OOD) samples as well as classify in-distribution (ID) samples. Unlike the softmax classifier that only focuses on the linear decision boundary partitioning its latent space into multiple regions, our Deep-MCDD aims to find a spherical decision boundary for each class which determines whether a test sample belongs to the class or not. By integrating the concept of Gaussian discriminant analysis into deep neural networks, we propose a deep learning objective to learn class-conditional distributions that are explicitly modeled as separable Gaussian distributions. Thereby, we can define the confidence score by the distance of a test sample from each class-conditional distribution, and utilize it for identifying OOD samples. Our empirical evaluation on multi-class tabular and image datasets demonstrates that Deep-MCDD achieves the best performances in distinguishing OOD samples while showing the classification accuracy as high as the other competitors.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1362–1370},
numpages = {9},
keywords = {out-of-distribution detection, multi-class classification, deep neural networks, Gaussian discriminant analysis},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{platt1999probabilistic,
  title={Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods},
  author={Platt, John and others},
  journal={Advances in large margin classifiers},
  volume={10},
  number={3},
  pages={61--74},
  year={1999},
  publisher={Cambridge, MA}
}

@article{restnet,
  title={ReST-Net: Diverse activation modules and parallel subnets-based CNN for spatial image steganalysis},
  author={Li, Bin and Wei, Weihang and Ferreira, Anselmo and Tan, Shunquan},
  journal={IEEE Signal Processing Letters},
  volume={25},
  number={5},
  pages={650--654},
  year={2018},
  publisher={IEEE}
}

@inproceedings{scarf,
author = {Bahri, Dara and Jiang, Heinrich and Tay, Yi and Metzler, Donald},
booktitle = {ICLR 2022 - 10th International Conference on Learning Representations},
title = {{SCARF: SELF-SUPERVISED CONTRASTIVE LEARNING USING RANDOM FEATURE CORRUPTION}},
year = {2022}
}

@article{snn,
   author = {Günter Klambauer and Thomas Unterthiner and Andreas Mayr and Sepp Hochreiter},
   journal = {31st Conference on Neural Information Processing Systems},
   title = {Self-Normalizing Neural Networks},
   year = {2017},
}

@inproceedings{subtab,
   abstract = {Self-supervised learning has been shown to be very effective in learning useful representations, and yet much of the success is achieved in data types such as images, audio, and text. The success is mainly enabled by taking advantage of spatial, temporal, or semantic structure in the data through augmentation. However, such structure may not exist in tabular datasets commonly used in fields such as healthcare, making it difficult to design an effective augmentation method, and hindering a similar progress in tabular data setting. In this paper, we introduce a new framework, Subsetting features of Tabular data (SubTab), that turns the task of learning from tabular data into a multi-view representation learning problem by dividing the input features to multiple subsets. We argue that reconstructing the data from the subset of its features rather than its corrupted version in an autoencoder setting can better capture its underlying latent representation. In this framework, the joint representation can be expressed as the aggregate of latent variables of the subsets at test time, which we refer to as collaborative inference. Our experiments show that the SubTab achieves the state of the art (SOTA) performance of 98.31% on MNIST in tabular setting, on par with CNN-based SOTA models, and surpasses existing baselines on three other real-world datasets by a significant margin.},
   author = {Talip Ucar and Ehsan Hajiramezanali and Lindsay Edwards},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   title = {SubTab: Subsetting Features of Tabular Data for Self-Supervised Representation Learning},
   volume = {23},
   year = {2021},
}

@misc{tabr,
      title={TabR: Tabular Deep Learning Meets Nearest Neighbors in 2023}, 
      author={Yury Gorishniy and Ivan Rubachev and Nikolay Kartashev and Daniil Shlenskii and Akim Kotelnikov and Artem Babenko},
      year={2023},
      eprint={2307.14338},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.14338}, 
}

@article{vaswani2017attentiontransformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

