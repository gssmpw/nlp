
\section{Methods}
\label{sec:methods}

Based on the intuition to allow the LLMs to focus on their computation on more promising lines, we are motivated by two research questions: (1) How can we enable language models to dynamically allocate resources during inference to optimize for reasoning and planning? (2) What architecture allows for effective separation between the reasoning process in LRM and the meta-level guidance of that process in Meta-reasoner? To address them, we propose a new framework, \textbf{\ours{}}, to equip the LLMs with the capabilities to \enquote{thinking about how to think}. It supervises the reasoning process of the LLMs and provides dynamic strategies to enable the LLMs to focus on more promising reasoning paths instead of iterative \enquote{trial-and-error}. This framework also addresses limitations of the current sequential generation of the reasoning paths which may get stuck in suboptimal trajectories by balancing \enquote{higher-order} thinking.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/framework.pdf}
    \caption{\small \textbf{An illustration of the \ours{} workflow.} In each round, the LLM produces a new reasoning step to extend its CoT reasoning. The CoT is then summarized into a progress report, which provides context for the meta-reasoner. Then meta-reasoner uses a contextual multi-armed bandit (either using a fixed contextual bandit or dynamic contextual bandit) to choose a guidance strategy. The selected strategy then guides the next reasoning step generation, to enable strategic redirection, error correction, and resource optimization. A reward is then computed from the progress report and used to update the bandit algorithm. The process repeats until the task is complete or the maximum number of rounds is reached.}
    \label{fig:illustration}
\end{figure*}

The meta-reasoning framework operates iteratively as shown in Figure~\ref{fig:illustration}. At each round $t$, the reasoning process comprises three steps: (1) \textit{CoT generation} by the LLM, (2) \textit{Progress Reporting} to summarize the reasoning progress so far (i.e., this is partly for efficiency, and partly to help the meta-reasoner focus on its main goal of \enquote{advising} rather than being distracted by the details in the CoT), and (3) \textit{Strategy Generation} by the meta-reasoner to optimize subsequent steps. The selection of the strategy is almost exactly corresponds to the well-studied problem of contextual multi-armed bandits learning. Each strategy can be seen as an arm for the bandit, and the reward of each strategy can be evaluated by the progress of LLM reasoning after applying the strategy. We analogy the process of executing and evaluating each strategy as the act of \enquote{pulling} each arm. The overall goal of our meta-reasoner is to find the best arm (i.e., strategy with highest cumulative rewards) with as few pulls as possible. The complete algorithm of \ours{} is appended in Algorithm~\ref{alg:meta-reasoner}.

\subsection{Chain-of-Thought (CoT) Generation}
\label{sec:cot_generation}

In the first step, the LLM generates a reasoning step to extend its CoT reasoning based on the user query. Starting with its reasoning history \(C_{t-1}\) and the guidance \(G_{t-1}\) provided by the meta-reasoner in the previous round, the LRM \(M\) produces a new reasoning step \(s_t\). This step is then appended to the current CoT, forming \(C_t = C_{t-1} \cup \{s_t\}\). This incremental process allows the LRM to iteratively build a structured reasoning path. By keeping track of the full reasoning trajectory at each round, the model creates a coherent foundation for evaluation and further refinement. This process is alike the demonstration in o1-like models, which generate a long-term thinking process. However, the issue of this reasoning is that its more like a process of \enquote{trial-and-error}, which may waste some of the inference costs on unnecessary/useless paths. In addition, due to the sequential generation process, it may easily get stuck in suboptimal solutions.

\subsection{Progress Reporting}
\label{sec:progress_report}

Once the LRM has updated its CoT, we summarize the reasoning history \(C_t\) into a concise progress report \(P_t\). This summary captures the key aspects of the reasoning trajectory, such as how much progress has been made toward the task goal, the consistency of the reasoning, and any significant updates so far. The summarization function \(f\) abstracts the detailed CoT into a simpler, more focused representation. This step is designed to be both computationally efficient and informative, ensuring that the meta-reasoner can focus on evaluating \textbf{high-level} progress without being overwhelmed by the \textbf{granular details} of every reasoning step. Even this step is more like an engineering trick, but we find that it may unlock some of the capabilities of LRM to do \enquote{higher-order} thinking, we find that with more essential information included in the prompt, the LRM will generally produce more insightful, critical strategies which could be more useful for complex reasoning. 

\begin{table*}[t]
\small
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{p{0.49\linewidth} p{0.49\linewidth}}
\toprule
\textbf{Diagnosis} & \textbf{Strategy} \\
\midrule
Progress is insufficient or the current strategy seems ineffective. 
& Restart from scratch and propose alternative strategies. \\
\hdashline\\[-8pt]
There are mistakes in intermediate steps. 
& Backtrack to the point where the error occurred. \\
\hdashline\\[-8pt]
The current approach is working well. 
& Continue and provide specific suggestions for the next steps. \\
\midrule
\cellcolor{gray!20}Ambiguous or conflicting intermediate results are observed. 
& \cellcolor{gray!20}Pause to clarify and disambiguate the current reasoning, then reconcile the discrepancies. \\
\hdashline\\[-8pt]
\cellcolor{gray!20}The reasoning process appears overly complex or convoluted. 
& \cellcolor{gray!20}Simplify by decomposing the task into smaller, manageable sub-tasks. \\
\hdashline\\[-8pt]
\cellcolor{gray!20}Evidence of error propagation or low confidence in certain subcomponents. 
& \cellcolor{gray!20}Perform targeted verification on critical steps and focus on areas with low confidence. \\
\hdashline\\[-8pt]
\cellcolor{gray!20}Repetitive or circular reasoning patterns are detected. 
& \cellcolor{gray!20}Reset to a previously successful checkpoint and explore alternative solution paths. \\
\bottomrule
\end{tabular}}
\caption{\small \textbf{Demonstration}: Contextual bandit pair (i.e., diagnosis of the current state and corresponding strategy) for guiding the LLM's reasoning process. Marked rows are some of the unique strategies generated by Dynamic Contextual Bandits.}
\label{tab:contextual_bandit_demonstration}
\end{table*}

\subsection{Meta-reasoner Strategy Generation}
\label{sec:strategy_generation}

In the next step, the meta-reasoner evaluates the progress report \(P_t\) and selects proper strategy \(G_t\) for LLM reasoning (the complete process can be found in Algorithm~\ref{alg:meta-reasoner}). We formulate the generation of strategy as a multi-armed bandits problem and consider two settings below: (1) our approach begins with a \textit{fixed-strategy} formulation, where the meta-reasoner selects from a predefined set of strategies using a contextual bandit algorithm. We then extend this architecture to (2) an \textit{advanced} setting in which the meta-reasoner is itself an LLM-based agent and can introduce or refine new strategies on the fly. In both cases, the meta-reasoner uses the
same partial-feedback principle of multi-armed bandits to adaptively choose which strategy to deploy based on a reward function. The reward function evaluates the quality of the given reasoning progress after applying the meta-reasoner strategy. We demonstrate the contextual bandit pair (i.e., diagnosis of the current state from the progress report and the corresponding strategy) in Table~\ref{tab:contextual_bandit_demonstration}.

\paragraph{Progress Evaluation.} A central goal of our evaluation mechanism is to measure how effectively the modelâ€™s current reasoning is advancing toward the task objective (e.g., solving a complex problem) while also monitoring computational expenditure to encourage efficiency. Concretely, we implement a reward function that tracks both \textbf{solution progress} (e.g., partial correctness, compliance with constraints) and \textbf{resource usage} (e.g., the number of reasoning steps). In principle, any suitable evaluator can be employed, including LLM-based verification or external scoring scripts. In our setup, we leverage an LLM as evaluator (with prompt referred at Figure~\ref{fig:prompt_0}-~\ref{fig:prompt_3}) to verify the reasoning progress. We adjust the implementation to output a cumulative score which will be further leveraged to update the MAB algorithms.

\paragraph{Fixed Contextual Bandit.} In the basic version of our framework, the meta-reasoner is modeled as a single contextual bandit that selects from a \textit{fixed, finite} set of $K$ strategies. These strategies may include instructions such as \enquote{continue and provide specific suggestions}, \enquote{restart from scratch}, \enquote{backtrack to the point where the error occurred}, or \enquote{propose alternative methods or perspectives to consider}. At each round, the LRM produces a \textit{progress report} summarizing its partial reasoning, the meta-reasoner transforms this progress report into a feature vector $x_t$ using a language model and applies a contextual bandit algorithm (e.g., LinUCB~\cite{li2012contextualbanditapproachpersonalized}) to select the best next strategy $a_t$. The LLM then executes that strategy and we collect the reward $r_t$ for $a_t$ based on the reward function. Through iterative MAB algorithm updating, the model learns to select a proper strategy based on the context from recent progress report.

\paragraph{Dynamic Contextual Bandit.} The basic framework assumes a static set of arms (strategies). In practice, the meta-reasoner may also be an LLM, capable of inventing new approaches over time. To accommodate \textit{dynamic} strategies, we allow the meta-reasoner to propose or refine new strategies at round $t$, which generates an expanding collection of actions, $\mathcal{A}_1 \subseteq \cdots \subseteq \mathcal{A}_t$. Each newly proposed strategy becomes an arm in the contextual bandit. To encourage at least some exploration on this new arm, we initialize each arm with a blank or weak prior in bandit's parameters.

By explicitly separating low-level content generation (handled by the LLM) from high-level strategy decisions (governed by the meta-reasonerâ€™s bandit), the system can effectively avoid getting stuck or wasting excessive resources on poor solution paths. In domains where a predefined set of strategies is sufficient, the fixed-arm formulation can simplify the method deployment. While in more open-ended domains where novel tactics may emerge, dynamic-arm extensions give meta-reasoner more freedom to evolve.
