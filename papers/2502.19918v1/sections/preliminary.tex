\section{Preliminary}
\label{sec:preliminary}

% \subsection{Definitions}
% We first define some key terms in this paper regarding \emph{strategy} and \emph{decision}: 1) what is strategy? We define a valid strategy that covers three parts: \textbf{diagnosis}: a theory describing the nature of the challenge; \textbf{guiding policies}: high-level approaches or principles for addressing the challenge; and \textbf{coherent actions}: a set of specific actions directed by the guiding policies to address the challenge. 
% 2) what is decision? We define a decision as a specific choice made (by multiple agents or an individual agent in our context) to achieve a defined goal or outcome. In our context, a \textit{decision} is the act of following the strategy to generate reasoning steps during inference-time compute. Such a decision is often made based on a set of metrics that act as shipping criteria to determine if the thinking process should be launched to the test-time of inference compute. 

% For a complex reasoning tasks, there could be multiple valid strategies during the inference-time, how to select the proper strategy becomes a bottleneck. This problem naturally aligns with the well-known problem of \emph{multi-armed bandit} (MAB) learning. The $n$ arms corresponds to $n$ strategy candidates, their performance on the underlying dataset is the hidden value of the arm, and the act of \enquote{pulling} and arm corresponds to evaluating the strategy on a randomly chosen data point. The goal is then to find the $b$ best arms with as few pulls as possible.
% Given several strategies for a mathematical reasoning or complex decision task, we train a MAB to select the most effective strategy for each sample in the dataset via \textbf{exploration-exploitation}. The learned MAB is then applied to new questions to select most efficient strategy to guide the reasoning process during inference-time compute automatically.

A central challenge in complex reasoning tasks is choosing the most effective strategy among multiple valid options. This challenge naturally aligns with the \textit{contextual multi-armed bandit (MAB)} framework, which is designed to balance the exploration of new strategies with the exploitation of strategies known to perform well. 

In this framework, an agent observes a context $x_t$ that characterizes the current state of the environment at every time step $t$ and selects an arm $s_t$ from a finite set $\mathcal{S}$. Upon selecting arm $s_t$, the agent receives a reward $r(s_t, x_t)$ that reflects both the chosen arm and the context. The primary objective of MAB is to maximize the cumulative reward over time: $R(T) = \sum_{t=1}^T r(s_t, x_t)$. A central challenge in the contextual MAB problem is balancing \emph{exploration} (trying different arms to gather information about their rewards) with \emph{exploitation} (selecting the arm that has yielded high rewards in similar contexts in the past). This balance ensures that while the agent leverages known profitable actions, it also continues to search for potentially better options. This principle is central to our motivation behind \ours{}, which aims to automatically select the most efficient strategy to guide the reasoning process during inference time.

A widely used algorithm in the contextual setting is LinUCB~\cite{li2012contextualbanditapproachpersonalized}, which models the expected reward as a linear function of the context. Specifically, for an arm $s$ given context $x_t$, the expected reward is modeled as $\mathbb{E}\left[r\left(s, x_t\right)\right] \approx x_t^{\top} \theta_s$, where $\theta_s$ is an unknown parameter vector associated with arm $s$. To manage uncertainty in its estimates, LinUCB maintains for each arm an estimate $\hat{\theta}_s$ and an associated covariance matrix $A_s$. The algorithm then selects the arm according to:
\begin{equation}
   s_t=\arg \max _{s \in \mathcal{S}}\left[x_t^{\top} \hat{\theta}_s+c \sqrt{x_t^{\top} A_s^{-1} x_t}\right],
\end{equation}
where $c$ is a constant that controls the exploration level. Here, the term $c \sqrt{x_t^{\top} A_s^{-1} x_t}$ serves as a confidence bound on the reward estimate, encouraging the selection of arms with higher uncertainty (i.e., those with less historical data) and thereby facilitating exploration. By incorporating context into the decision-making process, LinUCB allows the agent to adapt its strategy based on the current state, aligning well with our goal of selecting the most efficient reasoning strategy under varying conditions.

% The term "bandit" comes from the analogy to a row of slot machines ("one-armed bandits"), where each machine (or arm) provides a reward based on an unknown probability distribution. The MAB framework is particularly important in applications where decisions must be made sequentially with incomplete information, such as adaptive clinical trials, online advertising, and, in our work, selecting high-level reasoning strategies.

% In summary, the MAB algorithm offers a principled way to choose among multiple strategies by continuously updating reward estimates and balancing the need to explore less-known options against exploiting the currently best-performing ones. This balance is crucial in scenarios where the optimal strategy is not known in advance and must be learned through interaction with the environment.
