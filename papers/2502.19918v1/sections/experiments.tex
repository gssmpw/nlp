\section{Experiments}
\label{sec:experiments}

In this section, we first introduce the experiment settings including the dataset, baselines, and the backbone models. We then present the main results of \ours{} with some other analysis from perspectives like efficiency, rewards accumulation, and qualitatively assess meta-reasoner output.

\subsection{Experiments Setup}
\label{sec:experiment_setup}

\paragraph{Datasets.} We consider the tasks requiring complex reasoning and the proper solutions naturally composed of long reasoning steps. We evaluate \ours{} on several challenging datasets: the 24-point game proposed by \citet{yao2023treethoughtsdeliberate}, college-level scientific problem from SciBench~\cite{wang2024scibenchevaluatingcollegelevel} and theorem-driven math question in TheoremQA~\cite{chen2023theoremqatheoremdrivenquestion}. For the SciBench, we only consider the math-related subset for testing which covers the diff, stat, and calc (the detailed clarification of each subset collection can be found in \citet{wang2024scibenchevaluatingcollegelevel} and we provide the demonstration for each subset in Figure~\ref{fig:sci_bench_example}). For the TheormQA, we consider the mathematics subset that involves logical reasoning for our testing. We follow the experimental setup in MACM~\cite{lei2024macmutilizingmultiagent} to conduct the corresponding analysis on these datasets.

\paragraph{Baselines.}
We consider several established prompting methods as baselines as follows:
\begin{itemize}[leftmargin=*]
\setlength\itemsep{0em}
    \item Chain-of-thought (CoT)~\cite{wei2022chain}: A prompting technique that encourages models to generate intermediate reasoning steps to enhance problem-solving capabilities.
    \item Self-Consistent Chain of Thought (SC-CoT)~\cite{wang2022self}: An extension of CoT that improves reasoning consistency by generating multiple reasoning chains and selecting the most consistent answer.
    \item Multi-Chain Reasoning (MCR)~\cite{yoran2024answeringquestionsmetareasoning}: enhances SC-CoT by having another LLM to assess and integrate content among the sampled reasoning chains to generate the final consistent answer.
    \item Tree of Thoughts (ToT)~\cite{yao2023treethoughtsdeliberate}: A method that explores multiple reasoning paths in a tree structure, allowing the model to consider various possibilities before arriving at a conclusion by tree search algorithms.
    \item Reflexion~\cite{shinn2024reflexion}: A framework that enables models to reflect on their reasoning process, iteratively refining their answers based on feedback.
    \item MACM~\cite{lei2024macmutilizingmultiagent}: A multi-agent system to refine the reasoning based on iterative condition mining.
\end{itemize}


\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Diff(\%)} & \textbf{Stat(\%)} & \textbf{Calc(\%)} \\
\midrule
Phi-4 + CoT & 17.42 & 28.42 & 32.93 \\
Llama-3.1-instruct + CoT & 33.14 & 49.72 & 54.18\\
Gemini-Exp-1206 + CoT & 36.32    & 56.73 & 59.24 \\
Gemini-Exp-1206 + SC-CoT & 38.73    & 59.12 & 64.11 \\
GPT-4o-mini + CoT  & 33.12    & 55.71 & 58.10 \\
GPT-4o-mini + SC-CoT       & 37.33    & 56.67 & 63.81 \\
GPT-4o-mini + MCR       & 40.12    & 58.21 & 67.42 \\
\hdashline\\[-8pt]
GPT-4o-mini + MACM~\cite{lei2024macmutilizingmultiagent}                          & 54.78 & 67.13 & 65.77 \\
GPT-4o + MACM~\cite{lei2024macmutilizingmultiagent}                    & 61.42 & 78.32 & 76.72 \\
\cellcolor{gray!20}GPT-4o-mini + Meta-Reasoner {\scriptsize(our work)} & \cellcolor{gray!20}60.32 & \cellcolor{gray!20}73.64 & \cellcolor{gray!20}80.23 \\
\cellcolor{gray!20}GPT-4o + Meta-Reasoner {\scriptsize(our work)} & \cellcolor{gray!20}67.14 & \cellcolor{gray!20}83.29 &  \cellcolor{gray!20}84.17 \\
\bottomrule
\end{tabular}}
\caption{\small Accuracy (\%) comparison of different methods on the math-related subset of the SciBench dataset. Each column refers to the problem subset defined in \citet{wang2024scibenchevaluatingcollegelevel}.}
\label{tab:sci_bench_results}
\end{table}

\begin{table}[t]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lc}
\toprule
Method                   & Accuracy (\%)\\ \midrule
GPT-4o-mini + CoT~\cite{yao2023treethoughtsdeliberate}              & 4  \\
GPT-4o-mini + SC-CoT~\cite{yao2023treethoughtsdeliberate} & 9   \\
GPT-4o-mini + IO {\scriptsize(best of 100)}~\cite{yao2023treethoughtsdeliberate}               & 33   \\
GPT-4o-mini + CoT {\scriptsize(best of 100)}~\cite{yao2023treethoughtsdeliberate}               & 49   \\
Gemini-Exp-1206 + IO {\scriptsize(best of 100)}~\cite{yao2023treethoughtsdeliberate}               &  38  \\
Gemini-Exp-1206 + CoT {\scriptsize(best of 100)}~\cite{yao2023treethoughtsdeliberate}              &  60   \\
\hdashline\\[-8pt]
GPT-4o-mini + ToT {\scriptsize($b = 1$)}~\cite{yao2023treethoughtsdeliberate}      & 45      \\
GPT-4o-mini+ ToT {\scriptsize($b = 5$)}~\cite{yao2023treethoughtsdeliberate}       & 74      \\
GPT-4o-mini + Reflexion~\cite{shinn2024reflexion} & 53 \\
GPT-4o-mini + MACM~\cite{lei2024macmutilizingmultiagent}             & 80     \\
\cellcolor{gray!20}GPT-4o-mini + Meta-Reasoner {\scriptsize(our work)} & \cellcolor{gray!20}89 \\
\cellcolor{gray!20}GPT-4o + Meta-Reasoner {\scriptsize(our work)} & \cellcolor{gray!20}92 \\
\cellcolor{gray!20}Gemini-Exp-1206 + Meta-Reasoner {\scriptsize(our work)} & \cellcolor{gray!20}94 \\
\hdashline\\[-8pt]
o1-mini + IO & 89 \\
o1-preview + IO & 93 \\
\bottomrule
\end{tabular}}
\caption{\small Accuracy(\%) comparison of different prompting methods on 24-points game. $b$: Search breadth.} % Add your caption
\label{tab:24_points_results} 
\end{table}

\begin{table}[t]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lc}
\toprule
Method                   & Accuracy (\%) \\ 
\midrule
GPT-4o-mini + CoT              & 39.46   \\
Gemini-Exp-1206 + CoT             &  43.12  \\
\hdashline\\[-8pt]
GPT-4o-mini + Reflexion~\cite{shinn2024reflexion} & 74.32 \\
GPT-4 Turbo + MACM~\cite{lei2024macmutilizingmultiagent}             & 79.41     \\
\cellcolor{gray!20}GPT-4o-mini + Meta-Reasoner {\scriptsize(our work)} & \cellcolor{gray!20}84.13 \\
\cellcolor{gray!20}Gemini-Exp-1206 + Meta-Reasoner {\scriptsize(our work)} & \cellcolor{gray!20}86.32 \\
\bottomrule
\end{tabular}}
\caption{\small Accuracy(\%) comparison of different prompting methods on TheoremQA~\cite{chen2023theoremqatheoremdrivenquestion}.}
\label{tab:theoremqa_results} 
\end{table}

\paragraph{Backbone Models.}
% We mainly test our approach on two types of language models: (1) first on LLMs to test whether these much cheaper and generalized models integrated with \ours{} can deliver comparable performance with specialized LRMs; (2) second on LRMs to test whether LRMs + \ours{} can reduce the inference costs tied to the trial-and-error process.
We consider both LLMs and the recent LRMs for our experiments.
For the LLMs, we consider the closed-source models like GPT-4o, GPT-4o-mini (between Nov 2025 to Jan 2025) from OpenAI, and open-sourced models like meta-llama-3.1-8B-instruct from Meta, phi-4 from Microsoft and gemini-experimental-1206 from Google. For the LRMs, we consider the closed-source models like o1, o1-mini (In case we cannot break down the generation of o1 models through APIs, we cannot properly inject our meta-reasoner with o1-series models; we only provide the IO results for references), and open-sourced models like QwQ-32B-preview from QWen and Deepseek-v3 from Deepseek-AI. For the feature extraction mentioned in \refsec{sec:strategy_generation}, we use text-embedding-3-small from OpenAI as the embedding model.

To ensure the reproducibility of the experiments, we set $\mathrm{temperature}=0.7$ and $\mathrm{top\_p}=1.0$ for all models. We use the API service from OpenAI\footnote{\url{https://openai.com/}} and OpenRouter\footnote{\url{https://openrouter.ai/}} for our experiments which host detailed snapshots of the utilized model versions.

\subsection{Main Results}
\label{exp:main_results}

We compare the accuracy of different prompting methods across different backbone models on SciBench (as shown in Table~\ref{tab:sci_bench_results}), 24-points game (as shown in Table~\ref{tab:24_points_results}) and TheoremQA (as shown in Table~\ref{tab:theoremqa_results}). We found that basic prompting strategies, such as CoT and SC-CoT, show limited effectiveness, achieving only 4\% and 9\% accuracy on 24-point games, respectively. Incorporating IO strategy with \enquote{Best of 100} samples improves accuracy to 33\%, but it remains far behind advanced methods. Strategies like ToT illustrate the importance of exploring broader reasoning paths, with accuracy increasing from 45\% to 74\% as the search breadth expands from 1 to 5. Advanced iterative methods, such as Reflexion (53\%) and MACM (80\%), further demonstrate the value of refined reasoning frameworks. Our proposed \ours{} outperforms these approaches, achieving 89\% accuracy with GPT-4o-mini and 92\% with GPT-4o, showcasing its ability to dynamically guide reasoning, correct errors, and focus resources effectively. Compared to specialized models like o1-mini, our method equipped with much cheaper and generalized models like GPT-4o-mini delivers comparable performance, demonstrating its adaptability and scalability. Overall, the Meta-Reasoner framework provides a compatible approach to improving reasoning-intensive tasks, combining high accuracy with dynamic and efficient problem-solving strategies. The results on SciBench and TheoremQA also demonstrate similar findings and show that \ours{} generally achieves better performance compared to the baselines and the results are consistent across different models.

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llccc}
    \toprule
    Model &   Variant  & Game-of-24(\%) & TheoremQA(\%) \\
    \midrule
    \multirow{4}{*}{GPT-4o-mini} & 
        Full Method & 89 &  84.13 \\
    &    w/o Progress Report & 85 & 79.42 \\
    &   w/o MAB (direct arm selection) & 82 & 80.74\\
    &    w/o MAB (CoT) & 4 & 39.46  \\
    \midrule
    \multirow{4}{*}{Gemini-Exp-1206} & 
        Full Method & 94 & 86.32   \\
    &    w/o Progress Report & 91 & 81.78  \\
    &   w/o MAB (direct arm selection) & 87 & 82.14\\
    &    w/o MAB (CoT)  & 11 & 43.12 \\
    \bottomrule
    \end{tabular}}
    \caption{\small Ablation study of \ours{}. Direct arm selection refers to prompting LLM to directly select a strategy based on recent progress report.}
    \label{tab:ablation_study}
\end{table}

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc}
    \toprule
       Bandit Type  & Game-of-24(\%) & \#US & TheoremQA(\%) & \#US \\
    \midrule
        Fixed (K=3) & 65 & 3 & 72.34 & 3 \\
        Fixed (K=5) & 72 & 5 & 79.17 & 5 \\
        Dynamic & 89 & 14 & 84.13 & 21 \\
        \bottomrule
    \end{tabular}}
    \caption{\small Fixed vs. Dynamic Bandit Variants over \texttt{GPT-4o-mini}. \#US: Number of Unique Strategies.}
    \label{tab:fixed_dynamic}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/performance_vs_generations.png}
    \caption{\small The trade-off between accuracy and normalized inference costs across different models and methods on 24-point games. We use \texttt{gpt-4o-mini} as the backend model for all the prompting methods. For each method, key hyper-parameters (e.g., ${N}$ in Best of N, or tree size in ToT) are tuned to yield a baseline (lower point) and an extended (upper point) configuration, with dashed lines connecting these bounds.}
    \label{fig:inference_cost}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/cumulative_rewards.png}
    \caption{\small Cumulative reward of different settings across iteration. We compare our method using LinUCB with baseline (direct arm selection), and random search methods across two tasks—Game of 24 (top row) and TheoremQA (bottom row) using GPT-40-mini (left) and Gemini-Exp-1206 (right).}
    \label{fig:accumulated_rewards}
\end{figure*}

\subsection{Ablation Study}
\label{exp:ablation_study}

In this section, we conduct an ablation study to analyze each component contribution of \ours{}. In specific, we consider the following setup: (1) w/o progress report: we replace the progress reporting process with directly considering the entire CoT history without summarization; (2) w/o MAB: instead of using MAB to select the proper strategy, we directly leverage an LLM to the decision making to provide the proper strategy for LRM reasoning. In Table~\ref{tab:ablation_study}, we show that when removing progress reporting (\enquote{w/o Progress Report}), the overall performance moderately degrades and we hypothesize it is due to the concise intermediate summarizations can help the Meta-reasoner only consider the high-level strategy instead of being confused with too much details of the reasoning process. We also find that removing the MAB brings a more pronounced effect, especially when strategy selection falls back to a direct chain-of-thought approach (\enquote{w/o MAB (CoT)}). It verifies the effect of our meta-reasoner module to help the model stay on track for getting an optimal solution. In
Table~\ref{tab:fixed_dynamic}, we compare fixed and dynamic bandit variants on the game of 24 and theoremQA. We find that using a fixed set of strategies (e.g., $K=3$ and $K=5$) yields lower performance compared to the dynamic approach which adaptively explores more strategies (shown by larger unique strategies). The results highlight the benefit of flexibly allocating diverse reasoning strategies using LLM in-context learning capabilities.

\subsection{Analysis}
\label{sec:analysis}

\paragraph{Effectiveness of Meta-reasoner.} In Figure~\ref{fig:accumulated_rewards}, we demonstrate the cumulative rewards across iterations to analyze the effectiveness of the our meta-reasoner module. We compare our MAB-based method with a baseline which directly prompts an LLM to select an arm (\enquote{strategy}). We refer to this baseline method as \textit{Baseline (Direct Arm Selection)}, with the prompt in Figure~\ref{fig:prompt_0}-~\ref{fig:prompt_3}. The results show that MAB-based Meta-Reasoner (using LinUCB~\cite{li2012contextualbanditapproachpersonalized}) consistently outperforms both direct LLM decision-making (the baseline) and random search across two distinct tasks (Game of 24 and TheoremQA) and under two model scales (GPT-40-mini and Gemini-Exp-1206). While direct LLM usage may yield reasonable initial performance and random search incurs minimal setup cost, neither approach systematically balances exploration and exploitation. In contrast, the MAB updating strategy leverages feedback from previous iterations to adaptively refine its action selection (e.g., choosing a proper strategy based on the CoT reasoning), steadily increasing cumulative rewards.

\noindent\textbf{Inference Efficiency.} In Figure~\ref{fig:inference_cost}, we compare the inference costs across different models and various prompting strategies. Basic models, like GPT-4o-mini and GPT-4o, show lower accuracy and minimal inference cost, occupying the lower-left corner of the plot. As methods become more advanced, such as ToT and MACM, accuracy improves significantly but at the expense of higher inference costs. Our proposed method stands out by achieving a strong balance between high accuracy and moderate inference cost, outperforming methods like MACM, which delivers lower accuracy at higher costs. While proprietary models like o1-mini and o1-preview achieve slightly higher accuracy, they incur the highest inference costs, highlighting their reliance on more computational resources. \ours{} demonstrates competitive performance with a cost-effective approach making it a scalable and efficient solution for reasoning-intensive tasks. 


