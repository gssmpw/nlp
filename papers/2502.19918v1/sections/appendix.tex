
% \section{Why use MAB instead of LLM to do decision making?}

% MAB is a mathematical model used in decision-making scenarios where you must choose between multiple options with unknown reward distributions, balancing exploration (trying new options) and exploitation (sticking with seemingly best options), while LLM is a type of AI that generates human-like text, excels at understanding and responding to complex questions, but lacks the inherent ability to make optimal decisions in uncertain environments like a MAB does; essentially, an LLM can provide insights to inform the MAB strategy, but it cannot directly solve the exploration-exploitation tradeoff problem like a dedicated MAB algorithm can.

% \section{Why Use Contextual MAB Instead of RL?}

% The key difference between contextual multi-armed bandits (MAB) and reinforcement learning (RL) lies in how they approach decision-making and the underlying assumptions about the structure of the problem. Contextual MAB focuses on selecting the best action (arm) based on immediate context to maximize a reward in a single-step scenario. In contrast, RL involves learning a policy to optimize cumulative rewards over sequential decisions, often considering the long-term consequences of actions.

% In MAB, the problem is modeled as independent arm pulls, with the goal of efficiently balancing exploration (trying less certain actions) and exploitation (choosing the most rewarding actions) based on the observed context. The context $\mathbf{x}$, often represented as a feature vector, is used to estimate the expected reward $r$ for each arm $a \in {1, \dots, K}$. Algorithms like Thompson Sampling or Upper Confidence Bound (UCB) are commonly used to approximate $E[r|a, \mathbf{x}]$ and select the optimal arm. This method is lightweight and interpretable, making it suitable for applications where decisions can be treated as independent and sequential dependencies are minimal.

% On the other hand, RL assumes a more complex setup, where actions taken at each step influence the subsequent states and rewards. The agent learns a policy $\pi(a|\mathbf{s})$, mapping states $\mathbf{s}$ to actions $a$, by optimizing the expected cumulative reward $R = \sum_{t=0}^\infty \gamma^t r_t$, where $\gamma$ is a discount factor. RL methods such as Q-learning or Proximal Policy Optimization (PPO) require modeling the environment and involve higher computational costs due to gradient updates and exploration across state-action trajectories.

% Contextual MAB is often preferred over RL in scenarios where the decision-making problem can be framed as independent, context-aware actions, and there is no need to model long-term dependencies. This simplicity offers several advantages. First, MAB is computationally efficient and requires less training time since it avoids the complexity of modeling states and long-term rewards. This is particularly beneficial when computational resources are limited or when decisions must be made in real time.

% Second, the interpretability of MAB algorithms allows for more straightforward understanding and debugging of the decision-making process. The estimation of expected rewards for each arm given the context provides direct insights into why a specific action was chosen, which is valuable in applications requiring transparency, such as healthcare or personalized recommendations.

% Finally, MAB is less sensitive to the challenges of defining an accurate reward signal and dealing with sparse or delayed rewards, issues that often complicate RL. By focusing solely on immediate rewards, MAB avoids the difficulty of assigning credit to actions in long-term sequences, making it more robust in single-step or non-sequential optimization tasks.

% For example, in the context of strategy selection for large language models, if the goal is to choose a predefined strategy based solely on the current input context (e.g., tone, topic, or user intent), contextual MAB provides a simple and effective framework. RL, while powerful, would introduce unnecessary complexity by modeling the sequential evolution of interactions, which may not be relevant if decisions are inherently independent.

% In summary, contextual MAB is favored over RL in settings where simplicity, efficiency, and interpretability are priorities, and the problem lacks the sequential dependencies that justify the use of RL's more sophisticated methods.

% A key novelty of this paper is that we mainly focus on providing dynamic strategies for guiding the LLM inference. While some other works have tried to let LLMs generate multiple candidate answers and use a verifier to select the best one (such as Best-of-N sampling and tree search). Even such search strategies are flexible and can be adapt to the difficulty of the problem, their performance is constrained by the quality of the verifier/searcher.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/prompt-0.png}
    \caption{Prompt Demonstration (Page-1)}
    \label{fig:prompt_0}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/prompt-1.png}
    \caption{Prompt Demonstration (Page-2)}
    \label{fig:prompt_1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/prompt-2.png}
    \caption{Prompt Demonstration (Page-3)}
    \label{fig:prompt_2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/prompt-3.png}
    \caption{Prompt Demonstration (Page-4)}
    \label{fig:prompt_3}
\end{figure*}

\begin{algorithm*}[t]
\small 
\caption{\small \textbf{\ours{}}: Meta-Reasoning with Contextual Multi-Armed Bandits}
\label{alg:meta-reasoner}
\begin{algorithmic}[1]
\REQUIRE LRM $M$, bandit $\mathcal{B}$, initial strategy set $\mathcal{A}_1$, maximum rounds $T$
\ENSURE Final answer $A_{\mathrm{final}}$

\STATE $C_0 \leftarrow \emptyset$; \quad $\mathcal{B}.\mathrm{Initialize}(\mathcal{A}_1)$
\STATE $G_0 \leftarrow$ default strategy

\FOR{$t = 1$ to $T$}
    \IF{$t > 1$}
        \STATE $P_{t-1} \leftarrow f(C_{t-1})$ \hfill \textit{// Summarize the existing CoT}
        \STATE $x_{t-1} \leftarrow \mathrm{FeatureExtract}(P_{t-1})$ \hfill \textit{// Extract features for context}
        \STATE \textbf{(Optional)}: $\mathcal{A}_t \leftarrow \mathcal{A}_{t-1} \cup \{\text{new strategies}\}$ \hfill \textit{// Update strategy set dynamically}
        \STATE $a_{t-1} \leftarrow \arg\max_{a \in \mathcal{A}_t} \mathrm{Score}_{\mathcal{B}}(x_{t-1}, a)$ \hfill \textit{// Select strategy using bandit}
        \STATE $G_t \leftarrow a_{t-1}$ \hfill \textit{// Set current guidance}
    \ELSE
        \STATE $G_t \leftarrow G_0$ \hfill \textit{// Use default guidance for the first iteration}
    \ENDIF

    \STATE $s_t \leftarrow M(C_{t-1}, G_t)$ \hfill \textit{// Generate new CoT with integrated guidance}
    \STATE $C_t \leftarrow C_{t-1} \cup \{s_t\}$ \hfill \textit{// Append new reasoning step to the CoT}

    \STATE $r_t \leftarrow \mathrm{ComputeReward}(C_t)$ \hfill \textit{// Compute reward based on the updated CoT}
    \IF{$t > 1$}
        \STATE $\mathcal{B}.\mathrm{Update}(x_{t-1}, a_{t-1}, r_t)$ \hfill \textit{// Update bandit with observed feedback}
    \ENDIF

    \IF{termination condition met} 
        \STATE \textbf{break}
    \ENDIF
\ENDFOR

\STATE $A_{\mathrm{final}} \leftarrow \mathrm{ExtractAnswer}(C_t)$
\RETURN $A_{\mathrm{final}}$
\end{algorithmic}
\end{algorithm*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/Scibench_example.png}
    \caption{\small Task example demonstrated in \citet{wang2024scibenchevaluatingcollegelevel} regarding \texttt{calc}, \texttt{stat} and \texttt{diff} mentioned in Table~\ref{tab:sci_bench_results}. }
    \label{fig:sci_bench_example}
\end{figure*}

