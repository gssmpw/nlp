Our proposed Meta-Reasoner framework, while effective at improving inference-time reasoning, has a few key limitations that may affect its applicability. First, it relies on a carefully designed reward function to guide strategy selection: if the reward signal does not accurately reflect correctness or progress, the meta-reasoner may persist with incorrect strategies. This challenge becomes more pronounced when the tasks involve subjective or complex objectives that are hard to quantify automatically (such as creative writing, complex theorem proving). Second, while dynamically adding or refining strategies expands the meta-reasonerâ€™s flexibility, it can also introduce instability. Overly complex or poorly specified new strategies may create confusion rather than enhance problem-solving. Careful vetting or domain-specific constraints may be needed in future version to prevent the system from drifting into ineffective approaches.