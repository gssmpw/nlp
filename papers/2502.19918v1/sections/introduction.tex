\section{Introduction}

o1-like reasoning chains allow Large Language Models (LLMs) to \enquote{think for an extended period} before actually solving a problem. This shows impressive performance on challenging tasks, such as logical problems puzzles~\cite{lei2024macmutilizingmultiagent,yao2023treethoughtsdeliberate}, math questions~\cite{patel2024aimeaisystem,lightman2023letsverifystep}, logical reasoning~\cite{han2024folionaturallanguage}, and science questions~\cite{rein2023gpqagraduatelevelgoogleproof}, which often pose difficulties for even the most advanced models~\cite{gandhi2024streamsearchsos,sui2024tablemeetsllm,he2024unigraph}.

However, the trial-and-error nature of o1-like reasoning often incurs substantial computational overhead~\cite{snell2024scalingllmtesttime,manvi2024adaptiveinferencetimecompute} and is prone to error propagation where early flaws in a reasoning chain can compound and derail subsequent steps~\cite{lei2024macmutilizingmultiagent,yao2023treethoughtsdeliberate,gandhi2024streamsearchsos}. While related iterative approaches~\cite{gandhi2024streamsearchsos,li2025searcho1agenticsearchenhanced} have explored techniques like partial revision or backtracking, they typically address errors in an ad-hoc manner for a narrow span of reasoning steps and lack a systematic way to assess whether an entire line of reasoning remains viable. Thus, models remain vulnerable to getting \enquote{stuck} in less promising reasoning trajectories, continuously expending computational resources on unpromising paths rather than recognizing when a major strategic shift is needed. \textbf{A critical challenge}, therefore, is to enable LLMs to allocate their reasoning budget more effectively, prioritizing promising avenues while adapting—or discarding—ineffective strategies.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/demo_2.pdf}
    \caption{\small High-level comparison of LRM with o1-like chains and meta-reasoner.}
    \vspace{-4mm}
    \label{fig:demo}
\end{figure}

To overcome these challenges, we propose \ours{}, a specialized module that operates alongside the LLM to enhance its reasoning capabilities. The meta-reasoner serves as an \enquote{advisor}, dynamically evaluates the reasoning process, offering high-level guidance and strategic redirection when progress stalls. 
Unlike the LLM, which focuses on more specific stepwise generation, the meta-reasoner focuses on a broader perspective and evaluates the overall progress and strategy of the reasoning process. \ours{} operates in iterative rounds: where the LLM first generates partial o1-like reasoning chains and provides a \enquote{progress report} summarizing its reasoning progress so far. Based on these reports, the meta-reasoner then provides the strategic guidance, such as restarting with a different approach, refining existing ideas, or targeting specific sub-problems. Crucially, the meta-reasoner is designed to not interact with the granular details of the CoT; instead, it focus on the global oversight of the reasoning progress and provides dynamic high-level strategies, preventing the LLM from getting stuck or wasting resources on unproductive lines of inquiry. 

Overall, the main \textbf{contributions} of this paper are summarized as follows:
\begin{itemize}[leftmargin=*]
\setlength\itemsep{0em}
\item We propose a novel meta-reasoning framework that operates as a high-level advisor for LLMs, enabling them to \enquote{think about how to think} by dynamically optimizing inference-time reasoning strategies.
\item By decoupling global strategy decisions from low-level chain-of-thought generation, Meta-Reasoner oversees progress through concise \enquote{progress reports} rather than micromanaging each reasoning step. This design mitigates error propagation and reduces wasted computation on unproductive paths.
\item We evaluate \ours{} on challenging mathematical and scientific reasoning benchmarks (e.g., Game of 24, TheoremQA, and SciBench), demonstrating significant improvements in both accuracy and efficiency compared to baselines. Our results show that the proposed framework offers a scalable solution to inference-time reasoning bottlenecks.
\end{itemize}

% Addressing these shortcomings requires an additional layer of intelligence: a meta-reasoner. Acting as an advisor to the LRM, the meta-reasoner dynamically evaluates the reasoning process, offering high-level guidance and strategic redirection when progress stalls. By introducing meta-reasoning, we enable LRMs to not only reason but also think critically about how to optimize their reasoning, improving both efficiency and problem-solving quality in long-horizon tasks.

% The proposed meta-reasoning framework is particularly relevant in domains that require long-term reasoning, such as mathematical problem-solving. To evaluate its effectiveness, we design experiments using diverse mathematical reasoning datasets, ...

% including GSM8K and the MATH dataset. These datasets feature multi-step arithmetic and logical problems where small errors can derail the entire reasoning process. By comparing a baseline setup (LRM-only) with our proposed system (LRM + meta-reasoner), we assess improvements in reasoning efficiency, accuracy, and adaptability. Furthermore, we explore synthetic and open-ended reasoning tasks, such as Towers of Hanoi and proof generation, to analyze how the meta-reasoner generalizes across problem types.

% The main contributions of this paper are as follows:
% \begin{itemize}
% \item We highlight the limitations of current LRMs, particularly their vulnerability to error propagation and inefficiency in long-horizon tasks due to the sequential nature of Chain of Thought reasoning.
% \item We propose a meta-reasoner that dynamically evaluates and guides the LRM’s reasoning process, enabling strategic redirection, error correction, and resource optimization.
% \item We design and conduct comprehensive experiments using mathematical reasoning datasets, demonstrating significant improvements in accuracy, efficiency, and adaptability when employing the meta-reasoner.
% \item We show that the meta-reasoning framework extends beyond mathematical reasoning to synthetic and open-ended reasoning tasks, highlighting its potential for broad applicability in reasoning-intensive applications.
% \end{itemize}

% We have a simple intuition here: as LRMs gain the ability to productively think for longer times, inference-time efficiency is likely to increasingly become a bottleneck, so allowing them to focus their computation on more promising lines is likely to be increasingly important.

% The conventional view of experimentation/thinking process has focused on decision-making. We let the LLMs do pending before actually answer the question to make the answer more accurate, and on average better decisions.
% The intuition is quite simple, i.e., running experiments/thinking to make decisions is often wasteful, and we should think more about optimizing strategies instead.

% Large reasoning models (LRMs) like o1 have demonstrated impressive capabilities in solving complex problems by generating step-by-step solutions using Chain of Thought (CoT) reasoning. However, their reliance on sequential generation introduces fundamental limitations. CoT inherently builds on each preceding step, making the process vulnerable to compounding errors if an early step is flawed. While o1 can backtrack to some extent, such as revising individual steps or re-evaluating partial outputs, it lacks a systematic mechanism to efficiently identify and correct broader missteps or ineffective reasoning strategies. This often results in getting “stuck” on less promising lines of thought, as o1 lacks the ability to assess and adapt its overall reasoning strategy. Addressing these shortcomings requires an additional layer of intelligence: a meta-reasoner. Acting as an advisor to the LRM, the meta-reasoner dynamically evaluates the reasoning process, offering high-level guidance and strategic redirection when progress stalls. By introducing meta-reasoning, we enable LRMs to not only reason but also think critically about how to optimize their reasoning, improving both efficiency and problem-solving quality in long-horizon tasks.

% Large reasoning models (LRMs), such as o1, have emerged as powerful tools capable of solving complex problems by leveraging techniques like Chain of Thought (CoT) reasoning. CoT enables step-by-step reasoning by generating intermediate steps that mirror human-like problem-solving processes. This paradigm has proven effective in a wide range of domains, including mathematical reasoning, logical deduction, and multi-step decision-making. However, despite their impressive capabilities, LRMs exhibit notable limitations that restrict their ability to reason efficiently and effectively, particularly in long-horizon and complex tasks.

% The reliance of LRMs on CoT reasoning introduces inherent vulnerabilities due to its sequential nature. Each step in the CoT depends on the preceding one, meaning that errors made early in the process are often propagated through subsequent steps. While models like o1 can perform limited backtracking—such as reevaluating individual steps or revising final outputs—they lack a systematic mechanism to holistically revise or redirect their reasoning. This inability to dynamically adapt their strategy can lead to the model becoming stuck on unproductive lines of thought or inefficiently exploring the problem space. Additionally, as LRMs tackle more challenging problems requiring longer reasoning paths, inference-time efficiency becomes a critical bottleneck. Addressing these limitations requires a novel approach to complement the sequential reasoning process and provide adaptive oversight.


