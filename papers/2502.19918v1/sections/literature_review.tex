\section{Related Works}
\label{sec:related_works}

% In the realm of articifical intelligence (AI) and reinforcement learning (RL), the multi-armed bandit (MAB) problem is a classic descion-making knot that captures the explroationa-exploitation trade-off. It is formulated as a game with a fixed number of slot machines, or \enquote{bandits}, each with an unknown probability distribution of rewards. The goal is to develop a strategy for selecting when bandit to play so as to maximize the total reward over a series of plays. Several algorithms have been proposed to tackle this problem, each exhibiting specific attributes that render them favorable under different scenarios. For instance, the epsilon-greedy strategy~\cite{} offers simplicity and practicality, guaranteeing eventual convergence to optimal solutions given sufficient time, and a suitable choice of epsilon. The UCB approach~\cite{} is renowned for its optimality in stationary problems (i.e., the underlying distribution of each bandit's reward remains constant), demonstrating logarithmic regret growth over time, effectively minimizing regret in the long run. Finally, Thompson sampling~\cite{} stands out for its probabilistic nature, wherein its favors actions with high uncertainty, or high expected rewards, making it particularly suitable for scenarios with non-stationary rewards (i.e., where reward distributions change over time).

% The advent of LLMs, such as ..., has opened new avenues for AI applications. Their ability to generate human-like text and predict next word probabilities has been exploited in tasks ranging from text completion to more complex decision-making problems.

\paragraph{Complex Reasoning in LLMs}
The introduction of CoT reasoning has revolutionized how LLMs approach problem-solving, allowing them to break tasks into intermediate steps~\citep{lee2025evolvingdeeperllm}. The recent LLMs like o1, o3 from OpenAI and Deepseek-v3 from Deepseek have achieved state-of-the-art results in diverse domains using CoT-like reasoning~\citep{manvi2024adaptiveinferencetimecompute,li2025searcho1agenticsearchenhanced,kudo2024thinktotalktalktothinkwhen,sui2024fidelisfaithfulreasoning}. However, CoT’s sequential dependency limits its robustness, as errors in earlier steps can cascade through the process~\citep{snell2024scalingllmtesttime} and also when facing complex reasoning tasks~\citep{sui2024canknowledgegraphs,sui2024tablemeetsllm}, CoT-like reasoning may stuck in the infinite loop of reasoning~\citep{lee2025evolvingdeeperllm}. These issues motivate us to propose \ours{} to assess and adapt the overall reasoning strategy based on the progress of CoT reasoning. Unlike the LLMs, which focuses on more specific each step generation, the
meta-reasoner focus on the border perspective and evaluates the overall progress and strategy of the reasoning process. It can provide a global oversight to avoid LLMs getting stuck or wasting resources on unproductive lines of thoughts.

\paragraph{Backtracking and Error Correction}
Addressing cascading errors in multi-step reasoning remains a central challenge. Recent approaches have focused on backtracking and self-verification~\cite{yao2023treethoughtsdeliberate,besta2023graphthoughtssolving,gandhi2024streamsearchsos}. For instance, \citet{weng2023largelanguagemodels} have shown that incorporating a self-verification stage—where the model re-checks its conclusions using the very chain of thought it generated—can dramatically boost performance by catching missteps early. Similarly, \citet{ling2023deductiveverificationchainofthought} propose not only generate multiple candidate reasoning chains but also employs a verifier mechanism to identify and backtrack on erroneous steps. These techniques go beyond post-hoc validation by introducing dynamic strategy adjustments during inference~\cite{lightman2023letsverifystep}, thereby reducing the impact of errors propagating through long reasoning chains. Following these useful efforts, we initiate our \ours{} with the instructions like (1) restart from scratch and propose alternative strategies; (2) backtracking to the point where the error occurred; and (3) continue and provide specific suggestions. The detailed strategy can be found in \refsec{sec:strategy_generation}.

\paragraph{Meta-Cognition \& Dual-Process Systems}
Meta-cognition in human reasoning involves higher-order processes that allow individuals to monitor, evaluate, and adjust their cognitive strategies~\citep{gao2024metareasoninglarge,yoran2024answeringquestionsmetareasoning}. This reflective thinking—often seen as System 2 processes in dual-process theories~\cite{havrilla2024glorewhenwhere}—is vital for tasks that require careful deliberation and error correction~\cite{didolkar2024metacognitivecapabilitiesllms}. Drawing on these insights, our \ours{} can be considered analogous to dual-process systems, where LRM for generating CoT steps parallels System 1 and Meta-Reasoner for providing high-level strategic oversight to guide or redirect reasoning when needed serves as System 2. This separation of responsibilities enables the framework to balance efficiency with robust problem-solving, where the LRM handles routine inferences and the meta-reasoner intervenes when high-level strategic adjustments.