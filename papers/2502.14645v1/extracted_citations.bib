@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and others},
  booktitle={USENIX},
  year={2021}
}

@article{cohen2024evaluating,
  title={Evaluating the ripple effects of knowledge editing in language models},
  author={Cohen, Roi and Biran, Eden and Yoran, Ori and Globerson, Amir and Geva, Mor},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={283--298},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{dai2021knowledge,
  title={Knowledge neurons in pretrained transformers},
  author={Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  journal={arXiv preprint arXiv:2104.08696},
  year={2021}
}

@article{de2021editing,
  title={Editing factual knowledge in language models},
  author={De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  journal={arXiv preprint arXiv:2104.08164},
  year={2021}
}

@article{dong2022calibrating,
  title={Calibrating factual knowledge in pretrained language models},
  author={Dong, Qingxiu and Dai, Damai and Song, Yifan and Xu, Jingjing and Sui, Zhifang and Li, Lei},
  journal={arXiv preprint arXiv:2210.03329},
  year={2022}
}

@article{gabriel2020artificial,
  title={Artificial intelligence, values, and alignment},
  author={Gabriel, Iason},
  journal={Minds and machines},
  volume={30},
  number={3},
  pages={411--437},
  year={2020},
  publisher={Springer}
}

@article{gehman2020realtoxicityprompts,
  title={Realtoxicityprompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal={arXiv preprint arXiv:2009.11462},
  year={2020}
}

@article{hong2403orpo,
  title={Orpo: Monolithic preference optimization without reference model},
  author={Hong, Jiwoo and Lee, Noah and Thorne, James},
  volume={2403},
  year={2024}
}

@article{huang2023transformer,
  title={Transformer-patcher: One mistake worth one neuron},
  author={Huang, Zeyu and Shen, Yikang and Zhang, Xiaofeng and Zhou, Jie and Rong, Wenge and Xiong, Zhang},
  journal={arXiv preprint arXiv:2301.09785},
  year={2023}
}

@article{meng2022mass,
  title={Mass-editing memory in a transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}

@inproceedings{meng2022rome,
  author       = {Kevin Meng and
                  David Bau and
                  Alex Andonian and
                  Yonatan Belinkov},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Locating and Editing Factual Associations in {GPT}},
  booktitle    = {Annual Conference on Neural Information Processing Systems 2022},
  year         = {2022},
  timestamp    = {Mon, 08 Jan 2024 16:31:36 +0100},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{meng2024simpo,
  title={Simpo: Simple preference optimization with a reference-free reward},
  author={Meng, Yu and Xia, Mengzhou and Chen, Danqi},
  journal={arXiv preprint arXiv:2405.14734},
  year={2024}
}

@inproceedings{miao2024inform,
  title={Inform: Mitigating reward hacking in rlhf via information-theoretic reward modeling},
  author={Miao, Yuchun and Zhang, Sen and Ding, Liang and Bao, Rong and Zhang, Lefei and Tao, Dacheng},
  booktitle={Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{mishra2021cross,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2104.08773},
  year={2021}
}

@article{mitchell2021fast,
  title={Fast model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2110.11309},
  year={2021}
}

@inproceedings{mitchell2022memory,
  title={Memory-based model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={15817--15831},
  year={2022}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and others},
  journal={Advances in neural information processing systems},
  volume={35},
  year={2022}
}

@article{si2024mpn,
  title={MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing},
  author={Si, Nianwen and Zhang, Hao and Zhang, Weiqiang},
  journal={arXiv preprint arXiv:2401.03190},
  year={2024}
}

@article{sinitsin2020editable,
  title={Editable neural networks},
  author={Sinitsin, Anton and Plokhotnyuk, Vsevolod and Pyrkin, Dmitriy and Popov, Sergei and Babenko, Artem},
  journal={arXiv preprint arXiv:2004.00345},
  year={2020}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and others},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{wang-etal-2024-uncertainty,
    title = "Uncertainty Aware Learning for Language Model Alignment",
    author = "Wang, Yikun  and
      Zheng, Rui  and
      others",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
    year = "2024"
}

@article{wang2023cross,
  title={Cross-lingual knowledge editing in large language models},
  author={Wang, Jiaan and Liang, Yunlong and Sun, Zengkui and Cao, Yuxuan and Xu, Jiarong and Meng, Fandong},
  journal={arXiv preprint arXiv:2309.08952},
  year={2023}
}

@article{wang2023far,
  title={How far can camels go? exploring the state of instruction tuning on open resources},
  author={Wang, Yizhong and Ivison, Hamish and Dasigi, Pradeep and  others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={74764--74786},
  year={2023}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{xu2022language,
  title={Language anisotropic cross-lingual model editing},
  author={Xu, Yang and Hou, Yutai and Che, Wanxiang and Zhang, Min},
  journal={arXiv preprint arXiv:2205.12677},
  year={2022}
}

@article{zan2024building,
  title={Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning},
  author={Zan, Changtong and Ding, Liang and Shen, Li and Zhen, Yibing and Liu, Weifeng and Tao, Dacheng},
  journal={arXiv preprint arXiv:2403.14399},
  year={2024}
}

@inproceedings{zhang-etal-2025-intention,
    title = "Intention Analysis Makes {LLM}s A Good Jailbreak Defender",
    author = "Zhang, Yuqi  and
      Ding, Liang  and
      Zhang, Lefei  and
      Tao, Dacheng",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    year = "2025"
}

@article{zheng2023can,
  title={Can we edit factual knowledge by in-context learning?},
  author={Zheng, Ce and Li, Lei and Dong, Qingxiu and Fan, Yuxuan and Wu, Zhiyong and Xu, Jingjing and Chang, Baobao},
  journal={arXiv preprint arXiv:2305.12740},
  year={2023}
}

