% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}


@article{zhu2020modifying,
  title={Modifying memories in transformer models},
  author={Zhu, Chen and Rawat, Ankit Singh and Zaheer, Manzil and Bhojanapalli, Srinadh and Li, Daliang and Yu, Felix and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2012.00363},
  year={2020}
}
@article{dai2021knowledge,
  title={Knowledge neurons in pretrained transformers},
  author={Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  journal={arXiv preprint arXiv:2104.08696},
  year={2021}
}
@article{xie2024memla,
  title={MEMLA: Enhancing Multilingual Knowledge Editing with Neuron-Masked Low-Rank Adaptation},
  author={Xie, Jiakuan and Cao, Pengfei and Chen, Yuheng and Chen, Yubo and Liu, Kang and Zhao, Jun},
  journal={arXiv preprint arXiv:2406.11566},
  year={2024}
}
@article{liang2024multilingual,
  title={Multilingual knowledge editing with language-agnostic factual neurons},
  author={Liang, Yunlong and Meng, Fandong and Zhang, Songming and Chen, Yufeng and Xu, Jinan and Zhou, Jie and others},
  journal={arXiv preprint arXiv:2406.16416},
  year={2024}
}
@inproceedings{meng2022rome,
  author       = {Kevin Meng and
                  David Bau and
                  Alex Andonian and
                  Yonatan Belinkov},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Locating and Editing Factual Associations in {GPT}},
  booktitle    = {Annual Conference on Neural Information Processing Systems 2022},
  year         = {2022},
  timestamp    = {Mon, 08 Jan 2024 16:31:36 +0100},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@article{zhang2024comprehensive,
  title={A comprehensive study of knowledge editing for large language models},
  author={Zhang, Ningyu and Yao, Yunzhi and Tian, Bozhong and Wang, Peng and Deng, Shumin and Wang, Mengru and Xi, Zekun and Mao, Shengyu and Zhang, Jintian and Ni, Yuansheng and others},
  journal={arXiv preprint arXiv:2401.01286},
  year={2024}
}

@article{meng2022mass,
  title={Mass-editing memory in a transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{yang2024qwen2,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{sinitsin2020editable,
  title={Editable neural networks},
  author={Sinitsin, Anton and Plokhotnyuk, Vsevolod and Pyrkin, Dmitriy and Popov, Sergei and Babenko, Artem},
  journal={arXiv preprint arXiv:2004.00345},
  year={2020}
}

@article{yao2023editing,
  title={Editing large language models: Problems, methods, and opportunities},
  author={Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2305.13172},
  year={2023}
}

@inproceedings{mitchell2022memory,
  title={Memory-based model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={15817--15831},
  year={2022}
}

@article{huang2023transformer,
  title={Transformer-patcher: One mistake worth one neuron},
  author={Huang, Zeyu and Shen, Yikang and Zhang, Xiaofeng and Zhou, Jie and Rong, Wenge and Xiong, Zhang},
  journal={arXiv preprint arXiv:2301.09785},
  year={2023}
}

@article{dong2022calibrating,
  title={Calibrating factual knowledge in pretrained language models},
  author={Dong, Qingxiu and Dai, Damai and Song, Yifan and Xu, Jingjing and Sui, Zhifang and Li, Lei},
  journal={arXiv preprint arXiv:2210.03329},
  year={2022}
}

@article{zheng2023can,
  title={Can we edit factual knowledge by in-context learning?},
  author={Zheng, Ce and Li, Lei and Dong, Qingxiu and Fan, Yuxuan and Wu, Zhiyong and Xu, Jingjing and Chang, Baobao},
  journal={arXiv preprint arXiv:2305.12740},
  year={2023}
}

@article{cohen2024evaluating,
  title={Evaluating the ripple effects of knowledge editing in language models},
  author={Cohen, Roi and Biran, Eden and Yoran, Ori and Globerson, Amir and Geva, Mor},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={283--298},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{de2021editing,
  title={Editing factual knowledge in language models},
  author={De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  journal={arXiv preprint arXiv:2104.08164},
  year={2021}
}

@article{mitchell2021fast,
  title={Fast model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2110.11309},
  year={2021}
}
@article{wang2023cross,
  title={Cross-lingual knowledge editing in large language models},
  author={Wang, Jiaan and Liang, Yunlong and Sun, Zengkui and Cao, Yuxuan and Xu, Jiarong and Meng, Fandong},
  journal={arXiv preprint arXiv:2309.08952},
  year={2023}
}
@article{xu2022language,
  title={Language anisotropic cross-lingual model editing},
  author={Xu, Yang and Hou, Yutai and Che, Wanxiang and Zhang, Min},
  journal={arXiv preprint arXiv:2205.12677},
  year={2022}
}
@article{wei2024mlake,
  title={Mlake: Multilingual knowledge editing benchmark for large language models},
  author={Wei, Zihao and Deng, Jingcheng and Pang, Liang and Ding, Hanxing and Shen, Huawei and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2404.04990},
  year={2024}
}
@article{wang2023retrieval,
  title={Retrieval-augmented multilingual knowledge editing},
  author={Wang, Weixuan and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:2312.13040},
  year={2023}
}
@article{si2024mpn,
  title={MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing},
  author={Si, Nianwen and Zhang, Hao and Zhang, Weiqiang},
  journal={arXiv preprint arXiv:2401.03190},
  year={2024}
}
@article{gabriel2020artificial,
  title={Artificial intelligence, values, and alignment},
  author={Gabriel, Iason},
  journal={Minds and machines},
  volume={30},
  number={3},
  pages={411--437},
  year={2020},
  publisher={Springer}
}
@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{wang2023far,
  title={How far can camels go? exploring the state of instruction tuning on open resources},
  author={Wang, Yizhong and Ivison, Hamish and Dasigi, Pradeep and  others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={74764--74786},
  year={2023}
}
@article{mishra2021cross,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2104.08773},
  year={2021}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and others},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and others},
  journal={Advances in neural information processing systems},
  volume={35},
  year={2022}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and others},
  booktitle={USENIX},
  year={2021}
}

@article{gehman2020realtoxicityprompts,
  title={Realtoxicityprompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal={arXiv preprint arXiv:2009.11462},
  year={2020}
}

@article{meng2024simpo,
  title={Simpo: Simple preference optimization with a reference-free reward},
  author={Meng, Yu and Xia, Mengzhou and Chen, Danqi},
  journal={arXiv preprint arXiv:2405.14734},
  year={2024}
}
@article{hong2403orpo,
  title={Orpo: Monolithic preference optimization without reference model},
  author={Hong, Jiwoo and Lee, Noah and Thorne, James},
  volume={2403},
  year={2024}
}

@article{yang2024butterfly,
  title={The butterfly effect of model editing: Few edits can trigger large language models collapse},
  author={Yang, Wanli and Sun, Fei and Ma, Xinyu and Liu, Xun and Yin, Dawei and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2402.09656},
  year={2024}
}
@article{li2023unveiling,
  title={Unveiling the pitfalls of knowledge editing for large language models},
  author={Li, Zhoubo and Zhang, Ningyu and Yao, Yunzhi and others},
  journal={arXiv preprint arXiv:2310.02129},
  year={2023}
}
@article{gu2024model,
  title={Model editing can hurt general abilities of large language models},
  author={Gu, Jia-Chen and Xu, Hao-Xiang and Ma, Jun-Yu and Lu, Pan and Ling, Zhen-Hua and Chang, Kai-Wei and Peng, Nanyun},
  journal={arXiv preprint arXiv:2401.04700},
  year={2024}
}
@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}
@article{talmor2018commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}
@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  year={2020}
}
@article{narayan2018don,
  title={Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1808.08745},
  year={2018}
}
@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}
@article{li2023cmmlu,
  title={Cmmlu: Measuring massive multitask language understanding in chinese},
  author={Li, Haonan and Zhang, Yixuan and Koto, Fajri and Yang, Yifei and Zhao, Hai and Gong, Yeyun and Duan, Nan and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2306.09212},
  year={2023}
}
@article{huang2024c,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and others},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}
@misc{2023opencompass,
    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},
    author={OpenCompass Contributors},
    howpublished = {\url{https://github.com/open-compass/opencompass}},
    year={2023}
}
@article{wang2023towards,
  title={Towards unifying multi-lingual and cross-lingual summarization},
  author={Wang, Jiaan and Meng, Fandong and Zheng, Duo and Liang, Yunlong and Li, Zhixu and Qu, Jianfeng and Zhou, Jie},
  journal={arXiv preprint arXiv:2305.09220},
  year={2023}
}

@article{levy2017zero,
  title={Zero-shot relation extraction via reading comprehension},
  author={Levy, Omer and Seo, Minjoon and Choi, Eunsol and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1706.04115},
  year={2017}
}

@article{hartvigsen2024aging,
  title={Aging with grace: Lifelong model editing with discrete key-value adaptors},
  author={Hartvigsen, Tom and Sankaranarayanan, Swami and Palangi, Hamid and Kim, Yoon and Ghassemi, Marzyeh},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhong2023mquake,
  title={Mquake: Assessing knowledge editing in language models via multi-hop questions},
  author={Zhong, Zexuan and Wu, Zhengxuan and Manning, Christopher D and Potts, Christopher and Chen, Danqi},
  journal={arXiv preprint arXiv:2305.14795},
  year={2023}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan  and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{jiang2024learning,
  title={Learning to edit: Aligning llms with knowledge editing},
  author={Jiang, Yuxin and Wang, Yufei and Wu, Chuhan and others},
  journal={arXiv preprint arXiv:2402.11905},
  year={2024}
}

@article{achiam2023gpt,
  title={{GPT-4} technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and others},
  journal={arXiv preprint},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and others},
  journal={arXiv preprint},
  year={2024}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and others},
  journal={arXiv preprint},
  year={2025}
}

@article{wei2022emergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@article{zhong2023can,
  title={Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert},
  author={Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
  journal={arXiv preprint arXiv:2302.10198},
  year={2023}
}

@inproceedings{peng2023towards,
  title={Towards Making the Most of ChatGPT for Machine Translation},
  author={Peng, Keqin and Ding, Liang and Zhong, Qihuang and Shen, Li and others},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  year={2023}
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{xu2024contrastive,
  title={Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation},
  author={Xu, Haoran and Sharaf, Amr and Chen, Yunmo and Tan, Weiting and others},
  journal={arXiv preprint arXiv:2401.08417},
  year={2024}
}

@article{ethayarajh2024kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@article{wang2023easyedit,
  title={Easyedit: An easy-to-use knowledge editing framework for large language models},
  author={Wang, Peng and Zhang, Ningyu and Tian, Bozhong and others},
  journal={arXiv preprint arXiv:2308.07269},
  year={2023}
}

@article{huang2024can,
  title={Can Knowledge Editing Really Correct Hallucinations?},
  author={Huang, Baixiang and Chen, Canyu and Xu, Xiongxiao and Payani, Ali and Shu, Kai},
  journal={arXiv preprint arXiv:2410.16251},
  year={2024}
}

@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics},
  address={Bangkok, Thailand},
  year={2024}
}

@inproceedings{wang-etal-2024-uncertainty,
    title = "Uncertainty Aware Learning for Language Model Alignment",
    author = "Wang, Yikun  and
      Zheng, Rui  and
      others",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
    year = "2024"
}

@inproceedings{zhang-etal-2025-intention,
    title = "Intention Analysis Makes {LLM}s A Good Jailbreak Defender",
    author = "Zhang, Yuqi  and
      Ding, Liang  and
      Zhang, Lefei  and
      Tao, Dacheng",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    year = "2025"
}

@inproceedings{miao2024inform,
  title={Inform: Mitigating reward hacking in rlhf via information-theoretic reward modeling},
  author={Miao, Yuchun and Zhang, Sen and Ding, Liang and Bao, Rong and Zhang, Lefei and Tao, Dacheng},
  booktitle={Annual Conference on Neural Information Processing Systems},
  year={2024}
}
@article{zan2024building,
  title={Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning},
  author={Zan, Changtong and Ding, Liang and Shen, Li and Zhen, Yibing and Liu, Weifeng and Tao, Dacheng},
  journal={arXiv preprint arXiv:2403.14399},
  year={2024}
}

@inproceedings{lu-etal-2024-error,
    title = "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models",
    author = "Lu, Qingyu  and
      Qiu, Baopu  and
      Ding, Liang  and
      Zhang, Kanjian  and
      Kocmi, Tom  and
      Tao, Dacheng",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    year = "2024"
}