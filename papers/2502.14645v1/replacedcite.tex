\section{Related Work}
\paragraph{Knowledge Editing}  
The task of knowledge editing was introduced by ____ to update specific knowledge while preserving unrelated information. Current methods fall into two paradigms: preserving or modifying the modelâ€™s parameters.
(1) \emph{Preserving LLMs' parameters} involves auxiliary models or extra parameters. SERAC____ uses a counterfactual model to update knowledge without altering model parameters. TPatcher____ and CaliNET____ add trainable parameters to edit knowledge. IKE____ and ICE____ leverage in-context learning to correct knowledge.
(2) \emph{Modifying the model's parameters} directly updates specific parameters to change knowledge. KE____ and MEND____ predict weight updates for new data using a hyper-network. KN____, ROME____, and MEMIT____ use knowledge attribution or causal mediation analysis to target specific parameters for updating.

\paragraph{Cross-Lingual Knowledge Editing}  
Nearly all language models are multilingual, making it crucial to enhance instruction-following capabilities across different languages____ and enable synchronized cross-lingual knowledge updates____. 
Cross-lingual knowledge editing extends monolingual editing by propagating edits across languages. ____ introduced cross-lingual knowledge editing and created the Bi-ZsRE dataset to assess the applicability of monolingual methods in multilingual contexts. LiME____ proposes language anisotropic editing to enhance cross-lingual editing, and MPN____ introduces multilingual patch neurons to update knowledge. However, these methods treat source language answers as ground truth for target language queries, falling short of achieving true cross-lingual transfer.

\paragraph{LLM Alignment}  
LLM alignment____ ensures that LLMs' behaviors align with human values. Techniques such as supervised fine-tuning (SFT)____ train models to follow task descriptions in natural language. Despite SFT, models may still generate harmful content____. To address this, reinforcement learning with human feedback (RLHF)____ refines models further. Due to the issues of fragile training and reward hacking in RLHF____, recent simplified methods like SimPO____ and ORPO____ effectively enable preference optimization.