\section{Related Work}

\paragraph{Knowledge Editing}  
The task of knowledge editing was introduced by ~\cite{sinitsin2020editable} to update specific knowledge while preserving unrelated information. Current methods fall into two paradigms: preserving or modifying the modelâ€™s parameters.
(1) \emph{Preserving LLMs' parameters} involves auxiliary models or extra parameters. SERAC~\cite{mitchell2022memory} uses a counterfactual model to update knowledge without altering model parameters. TPatcher~\cite{huang2023transformer} and CaliNET~\cite{dong2022calibrating} add trainable parameters to edit knowledge. IKE~\cite{zheng2023can} and ICE~\cite{cohen2024evaluating} leverage in-context learning to correct knowledge.
(2) \emph{Modifying the model's parameters} directly updates specific parameters to change knowledge. KE~\cite{de2021editing} and MEND~\cite{mitchell2021fast} predict weight updates for new data using a hyper-network. KN~\cite{dai2021knowledge}, ROME~\cite{meng2022rome}, and MEMIT~\cite{meng2022mass} use knowledge attribution or causal mediation analysis to target specific parameters for updating.

\paragraph{Cross-Lingual Knowledge Editing}  
Nearly all language models are multilingual, making it crucial to enhance instruction-following capabilities across different languages~\cite{zan2024building} and enable synchronized cross-lingual knowledge updates~\cite{xu2022language,wang2023cross}. 
Cross-lingual knowledge editing extends monolingual editing by propagating edits across languages. ~\citet{wang2023cross} introduced cross-lingual knowledge editing and created the Bi-ZsRE dataset to assess the applicability of monolingual methods in multilingual contexts. LiME~\cite{xu2022language} proposes language anisotropic editing to enhance cross-lingual editing, and MPN~\cite{si2024mpn} introduces multilingual patch neurons to update knowledge. However, these methods treat source language answers as ground truth for target language queries, falling short of achieving true cross-lingual transfer.

\paragraph{LLM Alignment}  
LLM alignment~\cite{gabriel2020artificial} ensures that LLMs' behaviors align with human values. Techniques such as supervised fine-tuning (SFT)~\cite{wei2021finetuned, wang2023far, mishra2021cross, wang-etal-2024-uncertainty} train models to follow task descriptions in natural language. Despite SFT, models may still generate harmful content~\cite{carlini2021extracting, gehman2020realtoxicityprompts,zhang-etal-2025-intention}. To address this, reinforcement learning with human feedback (RLHF)~\cite{stiennon2020learning, ouyang2022training} refines models further. Due to the issues of fragile training and reward hacking in RLHF~\cite{miao2024inform}, recent simplified methods like SimPO~\cite{meng2024simpo} and ORPO~\cite{hong2403orpo} effectively enable preference optimization.