\section{Conclusion}
\label{sec:conclusion}

% In this paper, we propose the \textit{Cross-Lingual Knowledge Democracy Edit} (X-KDE) framework, enabling knowledge editing across languages in large language models (LLMs). By combining Cross-lingual Edition Instruction Tuning (XE-IT) and Target-language Preference Optimization (TL-PO), X-KDE effectively transfers knowledge from a source to a target language while maintaining high performance in monolingual settings. Additionally, we introduce high-quality datasets for cross-lingual knowledge editing, addressing gaps in current resources. Our experiments show that X-KDE outperforms existing methods, providing a scalable solution for multilingual knowledge editing. Future work will focus on applying X-KDE to other domains and optimizing its efficiency.
In this paper, we present the Cross-Lingual Knowledge Democracy Edit (X-KDE) framework, which facilitates knowledge editing across languages in large language models (LLMs). By integrating Cross-lingual Edition Instruction Tuning (XE-IT) and Target-language Preference Optimization (TL-PO), X-KDE efficiently transfers knowledge from a source language to a target language while maintaining strong performance in monolingual settings. Additionally, we introduce high-quality datasets specifically designed for cross-lingual knowledge editing, filling gaps in existing resources. Our experimental results demonstrate that X-KDE outperforms current methods, offering a scalable solution for cross-lingual knowledge editing. Future research will explore applying X-KDE to other domains and optimizing its efficiency.

\section*{Limitations}
While our work presents promising results, there are a few limitations to consider. First, due to computational constraints, we validate X-KDE on models with up to 7B parameters. Evaluating larger models, such as those exceeding 70B parameters, could provide more robust insights. Second, while our method has been effective in multilingual settings, its application to additional domains, such as finance or law, remains unexplored. Future research will focus on scaling X-KDE and extending its applicability to other fields.
% \section*{Ethics and Reproducibility Statements}
% \paragraph{Ethics}  
% We take ethical considerations seriously and strictly adhere to the ACL Ethics Policy. All datasets used in this work are publicly available and widely adopted by the research community. Our methods focus on enhancing the multilingual capabilities of large language models without introducing harmful biases or unethical content. We ensure that all experiments are conducted in compliance with ethical guidelines, prioritizing fairness and transparency in model deployment.

% \paragraph{Reproducibility}  
% In this paper, we discuss the detailed experimental setup, including training hyperparameters, baseline implementations, and statistical descriptions. More importantly, \textit{\textbf{we have provided our code and data in the Supplementary Material}} to help reproduce the experimental results of this paper. Due to space limitations during uploading, the full dataset will be released upon acceptance.