[
  {
    "index": 0,
    "papers": [
      {
        "key": "sinitsin2020editable",
        "author": "Sinitsin, Anton and Plokhotnyuk, Vsevolod and Pyrkin, Dmitriy and Popov, Sergei and Babenko, Artem",
        "title": "Editable neural networks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "mitchell2022memory",
        "author": "Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea",
        "title": "Memory-based model editing at scale"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "huang2023transformer",
        "author": "Huang, Zeyu and Shen, Yikang and Zhang, Xiaofeng and Zhou, Jie and Rong, Wenge and Xiong, Zhang",
        "title": "Transformer-patcher: One mistake worth one neuron"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dong2022calibrating",
        "author": "Dong, Qingxiu and Dai, Damai and Song, Yifan and Xu, Jingjing and Sui, Zhifang and Li, Lei",
        "title": "Calibrating factual knowledge in pretrained language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zheng2023can",
        "author": "Zheng, Ce and Li, Lei and Dong, Qingxiu and Fan, Yuxuan and Wu, Zhiyong and Xu, Jingjing and Chang, Baobao",
        "title": "Can we edit factual knowledge by in-context learning?"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "cohen2024evaluating",
        "author": "Cohen, Roi and Biran, Eden and Yoran, Ori and Globerson, Amir and Geva, Mor",
        "title": "Evaluating the ripple effects of knowledge editing in language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "de2021editing",
        "author": "De Cao, Nicola and Aziz, Wilker and Titov, Ivan",
        "title": "Editing factual knowledge in language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "mitchell2021fast",
        "author": "Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D",
        "title": "Fast model editing at scale"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "dai2021knowledge",
        "author": "Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu",
        "title": "Knowledge neurons in pretrained transformers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "meng2022rome",
        "author": "Kevin Meng and\nDavid Bau and\nAlex Andonian and\nYonatan Belinkov",
        "title": "Locating and Editing Factual Associations in {GPT}"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "meng2022mass",
        "author": "Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David",
        "title": "Mass-editing memory in a transformer"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zan2024building",
        "author": "Zan, Changtong and Ding, Liang and Shen, Li and Zhen, Yibing and Liu, Weifeng and Tao, Dacheng",
        "title": "Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "xu2022language",
        "author": "Xu, Yang and Hou, Yutai and Che, Wanxiang and Zhang, Min",
        "title": "Language anisotropic cross-lingual model editing"
      },
      {
        "key": "wang2023cross",
        "author": "Wang, Jiaan and Liang, Yunlong and Sun, Zengkui and Cao, Yuxuan and Xu, Jiarong and Meng, Fandong",
        "title": "Cross-lingual knowledge editing in large language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wang2023cross",
        "author": "Wang, Jiaan and Liang, Yunlong and Sun, Zengkui and Cao, Yuxuan and Xu, Jiarong and Meng, Fandong",
        "title": "Cross-lingual knowledge editing in large language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "xu2022language",
        "author": "Xu, Yang and Hou, Yutai and Che, Wanxiang and Zhang, Min",
        "title": "Language anisotropic cross-lingual model editing"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "si2024mpn",
        "author": "Si, Nianwen and Zhang, Hao and Zhang, Weiqiang",
        "title": "MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "gabriel2020artificial",
        "author": "Gabriel, Iason",
        "title": "Artificial intelligence, values, and alignment"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "wei2021finetuned",
        "author": "Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V",
        "title": "Finetuned language models are zero-shot learners"
      },
      {
        "key": "wang2023far",
        "author": "Wang, Yizhong and Ivison, Hamish and Dasigi, Pradeep and  others",
        "title": "How far can camels go? exploring the state of instruction tuning on open resources"
      },
      {
        "key": "mishra2021cross",
        "author": "Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh",
        "title": "Cross-task generalization via natural language crowdsourcing instructions"
      },
      {
        "key": "wang-etal-2024-uncertainty",
        "author": "Wang, Yikun  and\nZheng, Rui  and\nothers",
        "title": "Uncertainty Aware Learning for Language Model Alignment"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "carlini2021extracting",
        "author": "Carlini, Nicholas and Tramer, Florian and others",
        "title": "Extracting training data from large language models"
      },
      {
        "key": "gehman2020realtoxicityprompts",
        "author": "Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A",
        "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models"
      },
      {
        "key": "zhang-etal-2025-intention",
        "author": "Zhang, Yuqi  and\nDing, Liang  and\nZhang, Lefei  and\nTao, Dacheng",
        "title": "Intention Analysis Makes {LLM}s A Good Jailbreak Defender"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and others",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "miao2024inform",
        "author": "Miao, Yuchun and Zhang, Sen and Ding, Liang and Bao, Rong and Zhang, Lefei and Tao, Dacheng",
        "title": "Inform: Mitigating reward hacking in rlhf via information-theoretic reward modeling"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "meng2024simpo",
        "author": "Meng, Yu and Xia, Mengzhou and Chen, Danqi",
        "title": "Simpo: Simple preference optimization with a reference-free reward"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "hong2403orpo",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Orpo: Monolithic preference optimization without reference model"
      }
    ]
  }
]