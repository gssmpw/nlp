\section{Experiments}
\subsection{Experimental Setup}
\paragraph{Baselines.} 

\begin{table*}[t]
    \centering
    \resizebox*{0.9\linewidth}{!}{
    \begin{tabular}{cccccccccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}{*}{\bf Method}} & \multicolumn{4}{c}{\textbf{Test in English}} & \multicolumn{4}{c}{\textbf{Test in Chinese}} & \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-9} 
  \multicolumn{1}{c}{} & \textbf{Reliability} & \textbf{Generality} & \textbf{Locality} & \textbf{Portability} & \textbf{Reliability} & \textbf{Generality} & \textbf{Locality} & \textbf{Portability} & \textbf{\textit{\underline{Avg.}}} \\
    \midrule
\multicolumn{10}{c}{\textbf{Edit in English}} \\
\toprule
% Base & 64.94 & 64.17 & 74.42 & 55.18 & 63.59 & 63.25 & 58.81 & 54.91 & 62.41\\ 
FT-L & 53.51 & 50.18 & 94.01 & 53.31 & 51.81 & 51.71 & 85.56 & 55.14 & \underline{61.90}\\
FT-M & 99.97 & 95.38 & 97.92 & 57.69 & 56.89 & 56.52 & 94.61 & 52.16 & \underline{76.39}\\
ROME & 96.09 & 84.69 & 98.04 & 58.87 & 49.94 & 50.31 & 97.70 & 51.81 & \underline{73.43}\\ 
MEMIT & 95.21 & 89.14 & \textcolor{darkgreen}{98.56} & 57.77 & 52.05 & 52.01 & \textcolor{darkgreen}{98.76} & 52.19 & \underline{74.46}\\  
IKE & 99.59 & 99.61 & 56.95 & 71.27 & 67.83 & 67.88 & 64.54 & 58.97 & \underline{73.33}\\ 
LTE & 99.91 & 99.81 & 88.97 & 77.40 & 76.86 & 76.82 & 86.99 & 67.49 & \underline{84.28}\\ 
\cmidrule{1-10}
% X-KDE(Ours) & 99.71 & 92.77 & 82.57 & 70.81 & 93.26 & 93.25 & 95.34 & 76.74 & 88.05 \\ 
X-KDE(Ours) & \textcolor{darkgreen}{99.93} & \textcolor{darkgreen}{99.87} & 90.15 & \textcolor{darkgreen}{76.41} & \textcolor{darkgreen}{94.81} & \textcolor{darkgreen}{94.65} & 95.05 & \textcolor{darkgreen}{77.43} & \underline{\textcolor{darkgreen}{91.04}} \\  
% kde-x4 & 100 & 99.43 & 92.15 & 76.81 & 92.61 & 92.96 & 92.48 & 74.85 &  90.16\\
% kde-x4-orpo & 99.98 & 99.29 & 92.65 & 75.96 & 93.61 & 93.29 & 92.35 & 75.15 & 90.285 \\
% & 99.98 & 99.56 & 92.73 & 76.69 & 92.88 & 92.62 & 92.62 & 74.93 &  90.25\\
    % \toprule
    \midrule
    
    \multicolumn{10}{c}{\textbf{Edit in Chinese}} \\
    \toprule
% Base & 54.90 & 54.74 & 76.33 & 53.10 & 80.55 & 80.15 & 61.91 & 58.00 & 64.96\\
FT-L & 40.80 & 40.66 & 94.80 & 55.24 & 54.72 & 53.68 & 66.51 & 48.75 & \underline{56.89}\\
FT-M & 51.86 & 51.24 & 98.18 & 55.30 & \textcolor{darkgreen}{100.0} & 99.71 & 79.28 & 61.98 & \underline{74.69}\\
ROME & 44.14 & 43.80 & 97.92 & 52.66 & 72.24 & 70.12 & \textcolor{darkgreen}{96.48} & 48.15 & \underline{65.69}\\
MEMIT & 45.37 & 44.95 & \textcolor{darkgreen}{99.07} & 54.65 & 75.19 & 73.45 & 96.02 & 51.44 & \underline{67.52}\\ 
IKE & 65.87 & 65.74 & 69.41 & 63.06 & 99.86 & \textcolor{darkgreen}{99.73} & 64.86 & 72.39 &  \underline{80.79} \\ 
LTE & 64.63 & 62.56 & 85.23 & 62.6 & 99.79 & 99.31 & 87.17 & 69.69 &  \underline{78.87} \\
\cmidrule{1-10}
% X-KDE(Ours) & 91.83 & 89.16 & 86.86 & 61.9 & 100 & 99.31 & 92.54 & \textcolor{darkgreen}{74.34} & 86.99 \\
X-KDE(Ours) & \textcolor{darkgreen}{93.49} & \textcolor{darkgreen}{92.22} & 90.56 & \textcolor{darkgreen}{65.55} & \textcolor{darkgreen}{100.00} & 99.11 & 92.85 & \textcolor{darkgreen}{74.14} & \underline{\textcolor{darkgreen}{88.49}} \\
% & 93.97 & 93.98 & 93.43 & 69.67 & 100 & 99.39 & 92.65 & 73.93 & 89.63\\
    \bottomrule
    % \toprule
    \end{tabular}
}
    \caption{
    \textbf{Cross-lingual editing performance of different methods}, in terms of F1 score on Llama2-chat-7B backbones. Results in \textcolor{darkgreen}{green} indicates the best results. ``\underline{\textbf{\textit{Avg.}}}'' represents the overall mean of all metrics evaluated across the two languages.
    }
    \label{table:F1-res}
\end{table*}

% English editing results
\begin{table*}[!h]
    \centering
    \scalebox{0.7}{
        \begin{tabular}{ccccccccccccccc}
        \toprule
         \textbf{Metrics} & \textbf{Methods}  & \textbf{en-en} & \textbf{en-cz} & \textbf{en-de} & \textbf{en-du} & \textbf{en-es} & \textbf{en-fr} & \textbf{en-pt} & \textbf{en-ru} & \textbf{en-th} & \textbf{en-tr} & \textbf{en-vi} & \textbf{en-zh} & \underline{\textbf{en-avg}}\\
         \midrule
            % R.
           \multirow{7}{*}{{\textbf{Reliability}}} 
           & FT-L &  52.92 &  41.81 &  39.79 &  39.02 &  39.49 &  39.72 &  39.26 &  39.79 &  36.44 &  36.86 &  46.21 &  51.81 &  \underline{41.93} \\
           & FT-M &  99.96 &  66.93 &  70.16 &  67.17 &  63.69 &  64.98 &  64.22 &  48.96 &  36.46 &  57.54 &  66.80 &  56.89 &  \underline{63.65} \\
           & ROME  &  96.36 &  56.54 &  60.82 &  58.89 &  57.41 &  56.43 &  54.91 &  41.69 &  35.44 &  45.76 &  56.94 &  49.94 &  \underline{55.93} \\
           & MEMIT &  95.44 &  62.37 &  64.82 &  64.12 &  59.46 &  61.90 &  58.69 &  44.54 &  36.40 &  49.15 &  61.34 &  52.05 &  \underline{59.19} \\
           & IKE & 99.65 & 83.22 & 80.61 & 79.36 & 76.69 & 78.48 & 75.37 & 67.62 & 54.38 & 76.90 & 81.22 & 67.83 & \underline{76.78} \\
           & LTE  & \textcolor{darkgreen}{100.00} & 84.29 & 81.71 & 80.60 & 77.67 & 79.11 & 77.39 & 72.02 & 62.04 & 78.87 & 81.92 & 76.93 & \underline{79.38} \\
           \cmidrule{2-15}
           % & en & 98.38 & 92.98 & 88.71 & 89.51 & 86.79 & 87.72 & 88.95 & 89.31 & 89.53 & 90.68 & 86.02 & 92.74 & 90.11 \\
           % & en_plus & 98.56 & 92.70 & 90.23 & 89.70 & 88.25 & 87.88 & 87.92 & 88.13 & 88.24 & 88.49 & 88.36 & 92.53 & 89.96 \\
           % & x4 & 100.00 & 88.47 & 88.91 & 86.79 & 83.25 & 86.52 & 84.36 & 76.22 & 61.80 & 80.88 & 85.37 & 92.85 & 84.62 \\
           & X-KDE & 99.93 & \textcolor{darkgreen}{92.78} & \textcolor{darkgreen}{87.43} & \textcolor{darkgreen}{88.89} & \textcolor{darkgreen}{85.71} & \textcolor{darkgreen}{87.49} & \textcolor{darkgreen}{89.87} & \textcolor{darkgreen}{89.32} & \textcolor{darkgreen}{89.66} & \textcolor{darkgreen}{91.23} & \textcolor{darkgreen}{87.55} & \textcolor{darkgreen}{93.07} & \underline{\textcolor{darkgreen}{90.24}} \\
           \midrule
            % G.
            \multirow{7}{*}{\textbf{Generality}} 
            & FT-L &  49.60 &  40.75 &  38.87 &  38.36 &  39.68 &  39.12 &  39.56 &  38.97 &  36.89 &  37.18 &  45.89 &  51.71 &  \underline{41.38} \\
            & FT-M &  95.53 &  65.45 &  68.15 &  65.09 &  62.39 &  62.28 &  61.63 &  47.69 &  36.88 &  56.87 &  65.97 &  56.52 &  \underline{62.04} \\
            & ROME &  85.13 &  54.99 &  58.91 &  56.99 &  56.58 &  54.47 &  53.94 &  40.68 &  35.36 &  45.06 &  56.38 &  50.31 &  \underline{54.07} \\
            & MEMIT &  89.59 &  60.71 &  63.80 &  61.98 &  58.10 &  59.40 &  57.63 &  43.31 &  36.77 &  48.68 &  60.51 &  52.01 &  \underline{57.71} \\
            & IKE & 99.54 & 82.67 & 80.78 & 79.18 & 76.37 & 78.22 & 75.49 & 67.51 & 54.26 & 76.97 & 80.99 & 67.88 & \underline{76.65} \\
            & LTE & \textcolor{darkgreen}{99.87} & 84.26 & 81.63 & 81.07 & 77.51 & 78.99 & 77.38 & 71.46 & 61.90 & 78.26 & 81.37 & 76.24 & \underline{79.16} \\
           \cmidrule{2-15}
           % & en & 98.18 & 92.99 & 88.63 & 89.82 & 86.78 & 87.42 & 89.03 & 89.46 & 89.50 & 90.56 & 86.15 & 93.03 & 90.13 \\
           % & en_plus & 87.94 & 92.48 & 90.19 & 89.60 & 88.26 & 87.93 & 87.99 & 88.20 & 88.28 & 88.52 & 88.39 & 92.56 & 89.77 \\
           % & x4 & 99.54 & 88.45 & 88.99 & 86.41 & 83.81 & 86.18 & 83.85 & 75.80 & 61.29 & 80.53 & 85.17 & 92.87 & 84.41  \\
           & X-KDE & 99.68 & \textcolor{darkgreen}{92.87} & \textcolor{darkgreen}{87.25} & \textcolor{darkgreen}{88.87} & \textcolor{darkgreen}{85.16} & \textcolor{darkgreen}{87.57} & \textcolor{darkgreen}{89.93} & \textcolor{darkgreen}{89.10} & \textcolor{darkgreen}{89.21} & \textcolor{darkgreen}{91.25} & \textcolor{darkgreen}{87.62} & \textcolor{darkgreen}{93.11} & \underline{\textcolor{darkgreen}{90.14}} \\

           \bottomrule
        \end{tabular}
    }
    \caption{
    \textbf{Results on MzsRE dataset for editing performed in English.} Here, ``en-zh'' means that English serves as the source language and Chinese as the target language, with similar interpretations for the other pairs. \underline{``en-avg''} denotes the average performance across cross-lingual scenarios.}
    \label{tab:en-edit}
\end{table*}




We chose the following methods as baselines:
%
(1) \textbf{FT-L}~\cite{meng2022locating} fine-tunes a specific layer of the feed-forward network to maximize the likelihood of target tokens;
%
(2) \textbf{FT-M}~\cite{zhang2024comprehensive} fine-tunes the same feed-forward network layer as FT-L. Additionally, it masks the original text and applies cross-entropy loss on the target answer;
%
(3) \textbf{ROME}~\cite{meng2022locating} employs causal mediation analysis to identify the target area for editing, and then updates the parameters of the feed-forward network layers;
%
(4) \textbf{MEMIT}~\cite{meng2022mass}, built upon the ROME framework, enables the simultaneous update of thousands of knowledge;
%
(5) \textbf{IKE}~\cite{zheng2023can} utilizes the in-context-learning ability of the model and provides a few-shot demonstration to guide the model's responses based on the updated facts.
(6) \textbf{LTE}~\cite{jiang2024learning} enhances the model's instruction-following ability through supervised fine-tuning (SFT), and employs a retrieval-based mechanism to provide updated knowledge for demonstrations.

\paragraph{Backbones.} 
We select two public models as backbones, including LLaMA2-Chat-7B~\cite{touvron2023llama} and Qwen2.5-instruct-7B~\cite{yang2024qwen2}. These models are widely used in chatbot applications, the former excels in English, while the latter demonstrates strong multilingual abilities. For brevity, the results on Qwen2.5-instruct-7B are provided in~\ref{sec:appendix_ex_qwen2}.

\begin{figure*}[!t]
% \vspace{-10pt}
\centering
\includegraphics[width=0.85\linewidth]{figures/batch_edit.pdf}
\caption{
\textbf{Mean batch-editing performance across four benchmarks} at batch sizes 1, 10, 100, and 1000.
}
\label{fig:batch_edit}
% \vspace{-10pt}
\end{figure*}

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{figures/seq_edit.pdf}
\caption{
\textbf{Mean sequential-editing results across four knowledge editing benchmarks}, shown for data stream sizes of 1, 10, 100, 500, and 1000 (log-scale).
}
\label{fig:seq_edit}
\end{figure}

% \subsection{Experimental Results}
\subsection{Results of Single Fact Editing} 
Table~\ref{table:F1-res} and Table~\ref{tab:en-edit} demonstrate the main results of single fact editing, which focus on single editing cases. From these results, we can find several significant observations:
\paragraph{X-KDE outperforms other methods in the cross-lingual setting by a significant margin.} 

As shown in Table~\ref{table:F1-res}, when edited in English, it is evident that our method brings average performance improvements of 6.76\%, compared to LTE. In particular, in the cross-lingual setting, our method achieves further performance gains in portability, which demonstrates that our method not only captures surface-level changes in wording but enables the LLM to effectively internalize the knowledge edited in the source language and apply it to the target language. In summary, LTE sets a new state-of-the-art in cross-lingual knowledge editing task.

\paragraph{X-KDE brings consistent and effective improvements in more complex multilingual environments.} 

Our method is effective not only in a bilingual Chinese-English setting but can also be generalized to additional languages. We conducted more extensive experiments on the Mzsre dataset, and the results are presented in Table~\ref{tab:en-edit}. More detailed results are in Appendix~\ref{sec:appendix_ex_llama2}. It is evident that, compared to LTE, our method exhibits 10.86\% in reliability and 10.98\% average gains in generality when edited in English. These results further demonstrate that our approach significantly enhances the model's cross-lingual abilities, enabling it to effectively apply knowledge from a pivot language to others and marking a significant step toward the democratization of knowledge.

\subsection{Results of Mass Fact Editing} 

In the previous section, we introduced the results of single-fact editing. However, real-world scenarios are often more complex, requiring simultaneous or sequential edits to multiple pieces of knowledge. Therefore, in this section, we conduct comprehensive experiments using X-KDE alongside several methods that support mass editing (FT-L, FT-M, MEMIT, and LTE) on LLaMA2-Chat-7B in both batch-edit and sequential-edit settings, and then present the corresponding results.

\paragraph{X-KDE can process thousands of edits simultaneously.} 
In line with the single-edit procedure, we evaluate both English and Chinese edits separately. For simplicity in presentation, we take the average of these two results, as shown in Figure~\ref{fig:batch_edit}. As the batch size increases, we observe a gradual decline across all performance metrics for all methods. The drop is particularly severe for MEMIT and FT-L, especially in the locality metric, which is nearly cut in half. In contrast, X-KDE achieves the best performance, maintaining the highest accuracy while exhibiting the slowest degradation rate. These results indicate that our approach remains stable in cross-lingual settings, even after thousands of edits.

\paragraph{X-KDE can sequentially acquire new knowledge without forgetting previous information.} 
In the sequential editing setting, the model integrates new knowledge on top of its previous edits, which leads to a gradual decline in performance over time. As illustrated in Figure~\ref{fig:seq_edit}, the performance of methods that modify model parameters typically degrades as the number of edits increases. For instance, MEMIT and FT-L remain stable only when the number of edits $n\le 10$; beyond that, their performance deteriorates sharply. In contrast, knowledge storage-retrieval paradigms represented by X-KDE and LTE circumvent direct parameter modifications through external memory architectures. Moreover, X-KDE demonstrates superior cross-lingual transfer capabilities compared to LTE, achieving better performance across diverse data streams.


\begin{table*}[t]
    \centering
    % \belowrulesep=0pt
    % \aboverulesep=0pt
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccccccc}
    \toprule
    
        & \multicolumn{5}{c}{\textbf{Test Language: en}} & \multicolumn{4}{c}{\textbf{Test Language: zh}} \\
        \cmidrule(lr){2-6} \cmidrule(lr){7-10} 
        &  \textbf{MMLU} & \textbf{CommonSenseQA} & \textbf{PIQA} & \textbf{Xsum} & \textbf{NQ} & \textbf{CMMLU} & \textbf{CommonSenseQA\_zh} & \textbf{CEval} & \textbf{NQ\_zh} & \textbf{\textit{avg}} \\
    \midrule
    \textit{LLaMA2-Chat-7B} & 44.78 & \textbf{{64.21}} & \textbf{{66.43}} & \textbf{{21.24}} &19.39 & 20.64 & 4.67 & 30.08 & 0 & 30.16 \\
    % \quad X-KDE w/o TL-PO & 47.12 & 60.93 & 62.46 & 20.90 & 29.14 & 31.89 & 33.91 & 22.47 & \textcolor{darkgreen}{15.01} & 35.98 \\
    \quad X-KDE & \textbf{{47.67}} & 59.46 & 61.64 & 20.96 & \textbf{{29.36}} &\textbf{{32.63}} & \textbf{{45.13}} & \textbf{{30.75}} & \textbf{14.07} & \textbf{{37.96}} \\
    \bottomrule
    \end{tabular}
    }
    \caption{
        \textbf{General tasks performance of X-KDE, and LLaMA2-Chat-7B}, measured by F1 scores. Results in \textbf{bold} represent the best performance in each category.
    }
    \label{tab:general_task}
\end{table*}

\subsection{Results of General Tasks} 
A series of studies have demonstrated that knowledge editing can influence model performance across various scenarios~\cite{yang2024butterfly,li2023unveiling,gu2024model}. To investigate whether our method impacts the model's capabilities in unrelated domains, we conducted tests across a range of fields.
Given that the cross-lingual knowledge editing task typically involves two languages, we use English and Chinese as representative examples. Multiple benchmarks are selected in these two languages, covering tasks such as commonsense reasoning, natural language understanding, open-domain QA, and general intelligence.
For example, the benchmarks selected for English include MMLU~\cite{hendrycks2020measuring}, CommonSenseQA~\cite{talmor2018commonsenseqa}, PIQA~\cite{bisk2020piqa}, XSum~\cite{narayan2018don}, and Natural Questions~\cite{kwiatkowski2019natural}. For Chinese, the chosen benchmarks are CMMLU~\cite{li2023cmmlu}, CommonSenseQA\_zh~\cite{2023opencompass}, CEval~\cite{huang2024c}, and Natural Questions\_zh~\cite{2023opencompass}.
We conducted all experiments using the OpenCompass tool~\cite{2023opencompass}.
The results are presented in Table \ref{tab:general_task}. 
Overall, our method not only preserves the model's performance in English but also significantly enhances its capabilities in Chinese.
Although certain tasks, such as CommonsenseQA, XSum, and PIQA, show a decrease in performance when tested in English, the overall results demonstrate consistent English capabilities. This highlights the robustness of our method, which achieves cross-lingual knowledge editing while preserving the model's original performance and significantly improving its proficiency in Chinese.