\section{Methodology}

\begin{figure*}[!t]
\centering
\includegraphics[width=\linewidth]{figures/method.pdf}
\caption{\textbf{Illustration of \textit{Cross-Lingual Knowledge Democracy Edit} (X-KDE) framework}. In the XE-IT phase, we fine-tune the LLM on a carefully curated parallel dataset, enabling it to incorporate newly edited information from the source language when queried in the target language. In the TL-PO phase, multiple responses are generated, ranked based on similarity to the target language answer, and alignment optimization is applied to refine the output.}
\label{fig:method}
\end{figure*}

To achieve the democratization of knowledge, we propose the \textit{Cross-Lingual Knowledge Democracy Edit} (X-KDE) framework, as shown in Figure~\ref{fig:method}. This framework enables LLMs to adapt to evolving knowledge demands and facilitates the transfer of knowledge to target languages by editing only the source language. X-KDE consists of two stages: the Cross-lingual Edition Instruction Tuning (XE-IT) phase and the Target-language Preference Optimization (TL-PO) phase.

\subsection{XE-IT Stage: Learning to Edit}
\label{sec:XE-IT}

The goal is to enable the model to leverage knowledge edits in the source language while preserving the unchanged information. To meet the requirements for cross-lingual editing, we carefully constructed a high-quality dataset and employed XE-IT to fine-tune the model.

\subsubsection{Dataset Construction}

\paragraph{Data Sources.} Our goal is to enable the model to use edit descriptors effectively while maintaining unrelated information. We use several widely used knowledge editing datasets, including ZsRE~\cite{levy2017zero}, HalluEditBench~\cite{huang2024can}, RIPPLEEDITS~\cite{cohen2024evaluating}, WikiBio~\cite{hartvigsen2024aging}, MQUAKE~\cite{zhong2023mquake}, and COUNTERFACT~\cite{meng2022locating}, to build our training data. These datasets provide edit descriptors paired with QA pairs. To mitigate data leakage, we randomly sample and translate subsets for training.

\paragraph{Sample Generation.} Existing datasets often feature straightforward QA pairs, which limit the model's comprehension ability. To address this, we use Deepseek~\cite{liu2024deepseek} to generate complex in-scope and out-of-scope query-answer pairs. This method enhances training data quality and model comprehension.

\paragraph{Quality Control.} To ensure relevance, we use Deepseek to assess the quality of in-scope query-answer pairs. Samples are scored based on Syntactic Structure, Lexical Richness, and Edit Consistency, with low-quality samples filtered out and replaced by higher-scoring ones.

\paragraph{Translation Process.} We use large language models, i.e., Deepseek\footnote{\url{https://api-docs.deepseek.com/}} to translate the generated data from English to Chinese.

\paragraph{Parallel Data Construction.} Our dataset follows a parallel structure (Figure~\ref{fig:method}(b)), with the in-scope section guiding LLMs when to use updated knowledge and the out-of-scope section minimizing the impact on unrelated knowledge. The dataset includes both monolingual and cross-lingual sections, where the source language contains edit descriptors and the target language contains queries and answers. Further details about our dataset are provided in Appendix \ref{sec:appendix_data_construction}.

\subsubsection{Fine-Tuning}

Thanks to the flexible parallel structure, we can adaptively select the source and target languages to satisfy specific needs. We create a large-scale cross-lingual dataset and compute loss based only on the answer tokens. The model generates answers in the target language given source-language edit descriptors and target-language queries.

\subsection{TL-PO Stage: Preference Optimization}

\subsubsection{Preference Scoring}

After the XE-IT phase, the model has initially acquired the ability for cross-lingual knowledge editing. However, when faced with queries in the target language, the model may still make mistakes, such as generating responses in the source language, producing surface-level transliterations, or failing to follow target language patterns. To address this, we use a multilingual translation model to compute "alignment" scores, favoring responses aligned with the target language.

\subsubsection{Alignment Optimization}

We aim for the model to generate answers in the target language with higher likelihood than in the source language: 
$p_{\theta}^*(y_e^t|x_e^s) > p_{\theta}^*(y_e^s|x_e^s)$. To achieve this, we employ ORPO, a state-of-the-art preference optimization method. We collect flawed outputs ($Y_l$) and the ground truth in the target language ($Y_w$), then optimize the objective function:

\begin{equation}
    \mathcal{L}_{ORPO} = \mathbb{E}_{(x, y_w, y_l)}\left[ \mathcal{L}_{XE-IT} + \lambda \cdot \mathcal{L}_{OR} \right]
    \label{eq:main}
\end{equation}

where $\mathcal{L}_{XE-IT}$ is the XE-IT loss and $\mathcal{L}_{OR}$ maximizes the likelihood ratio between the preferred response and the less preferred one.

\begin{equation}
    \mathcal{L}_{XE-IT} = - \frac{1}{m} \sum_{t=1}^{m} \log P_\theta(y_t | x, y_{<t}) \label{eq:xe-it}
\end{equation}

\begin{equation}
    \mathcal{L}_{OR} = -\log \sigma \left( \log \frac{\textbf{odds}_\theta(y_w|x)}{\textbf{odds}_\theta(y_l|x)} \right) \label{eq:ratio} 
\end{equation}

\begin{equation}
    \textbf{odds}_\theta(y|x) = \frac{P_\theta(y|x)}{1 - P_\theta(y|x)}\label{eq:odds}
\end{equation}