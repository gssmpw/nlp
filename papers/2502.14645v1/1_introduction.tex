\section{Introduction}

Large Language Models (LLMs) \cite{achiam2023gpt, dubey2024llama, yang2024qwen2, guo2025deepseek} have shown strong capabilities in natural language understanding, generation, and reasoning~\cite{wei2022emergent, zhong2023can, peng2023towards, lu-etal-2024-error, zhao2023survey}. However, as world knowledge evolves, LLMs need methods to update outdated information efficiently. Knowledge editing \cite{yao2023editing} allows modifications to specific knowledge while preserving unrelated information, making it more cost-effective than retraining from scratch.

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{figures/new_intro.pdf}
\caption{Examples of (a) \textbf{monolingual} and (b) \textbf{cross-lingual} knowledge editing. In the former, the editing and verification languages are the same, while in the latter, knowledge is transferred from the source language (e.g., English) to the target language (e.g., Chinese).
}
\vspace{-10pt}
\label{fig:intro}
\end{figure}
Despite significant progress, most existing approaches focus on monolingual editing~\cite{de2021editing,dai2021knowledge,mitchell2021fast}. As LLMs are increasingly required to handle multilingual queries \cite{zhang2024comprehensive, wang2023towards}, monolingual solutions often fail. For example, when editing the response "the Conservative Party" to "the Labor Party" in English (Figure~\ref{fig:intro}(a)), this update does not propagate to the Chinese version. Thus, expanding knowledge editing to cross-lingual settings is crucial to ensure that changes made in the source language are correctly applied to target languages.

Currently, several studies on multilingual knowledge editing have emerged \cite{xu2022language, wang2023retrieval, wei2024mlake, xie2024memla, liang2024multilingual}. Some of these methods extend the edited language from single to multiple, while others prescribe source-language answers as the ground truth for multilingual queries. Both strategies fall short of achieving true cross-lingual knowledge democratization. For example, although IKE was regarded as the state-of-the-art method in previous studies~\cite{wang2023cross,xie2024memla}, its performance on the Bi-ZsRE benchmark demonstrates significant limitations, achieving merely a 73.33 average score when editing in English. Unlike previous methods that attempt to forcefully correct LLM behaviors in both the source and target languages, we propose guiding LLMs to internalize knowledge from source language editing and apply it to target language queries. Our \textit{Cross-Lingual Knowledge Democracy Edit} (X-KDE) with Dual-Stage Refinement, where we use parallel language datasets to transfer knowledge from the source to the target language.

The X-KDE framework involves two phases: (i) Cross-lingual edition Instruction Tuning (XE-IT), where the source language editing descriptor is paired with target language queries to create a parallel dataset, guiding the model to answer in the target language while preserving unchanged knowledge. (ii) Target-language Preference Optimization (TL-PO), where we adopt the ORPO strategy~\cite{hong2403orpo} further constrains cross-lingual knowledge, promoting the diffusion of updates from source to target languages, and achieving true knowledge democratization. Taking the Bi-ZsRE benchmark as an example, X-KDE outperforms others, achieving average scores of 91.04 and 88.49 when editing in English and Chinese, respectively. Our \textbf{contributions} are three-fold:
\begin{itemize}
    % \item To address the data scarcity in cross-lingual knowledge editing, We introduce new high-quality datasets for cross-lingual knowledge editing, addressing gaps in existing resources.
    \item To tackle the scarcity of high-quality resources in cross-lingual knowledge editing, we introduce new datasets that fill gaps in existing resources, enhancing the reliability of knowledge transfer across languages.
    % \item We present a straightforward yet effective cross-lingual knowledge editing method, namely X-KDE, achieving excellent cross-lingual knowledge generalization through a two-stage process.
    \item We propose X-KDE, a simple yet highly effective method for cross-lingual knowledge editing. This approach, based on a two-stage process, ensures robust knowledge generalization across languages.
    % \item Extensive experiments demonstrate that our approach sets a new state-of-the-art (SOTA) in cross-lingual settings, significantly improving performance while maintaining original knowledge and enhancing portability.
    \item Through extensive experiments, we establish X-KDE as a new state-of-the-art (SOTA) solution for cross-lingual knowledge editing, demonstrating significant improvements in performance while preserving original knowledge and enhancing the portability of updates.
\end{itemize}