\section{Related Work\label{literature}}
\subsection{Code LLMs for SE}
LLMs are large-scale PLMs. Some of them are instruction-tuned to follow instructions in human language. In this paper, we do not distinguish between PLMs and LLMs, and use LLMs to refer to pre-trained Transformers \cite{DBLP:conf/nips/VaswaniSPUJGKP17} in general.

While many general-purpose LLMs demonstrate satisfying performance on SE tasks, especially code generation, there are many LLMs pre-trained specifically for code-related tasks. CodeBERT \cite{DBLP:conf/emnlp/FengGTDFGS0LJZ20} is one of the earliest attempts to pre-train a Transformer on both code and text data. It is an encoder-only model with 125M parameters, pre-trained on over 8M datapoints from CodeSearchNet \cite{DBLP:journals/corr/abs-1909-09436}. CodeT5 \cite{DBLP:conf/emnlp/0034WJH21} is an encoder-decoder Transformer with up to 770M parameters, pre-trained with a denoising sequence-to-sequence objective on the same dataset. 
% Some researchers construct structural graphs of code, instead of treating it as sequences of tokens. GraphCodeBERT \cite{DBLP:conf/iclr/GuoRLFT0ZDSFTDC21} is pre-trained with data-flow information, tasked with predicting code structure edges and aligning representations between code and its structure. 
UniXcoder \cite{DBLP:conf/acl/GuoLDW0022} supports encoder-only, decoder-only, and encoder-decoder modes, allowing abstract syntax trees (ASTs) as input after transforming ASTs into sequences.

Recently, larger decoder-only LLMs have been increasingly popular in generation tasks. Codex \cite{DBLP:journals/corr/abs-2107-03374} is a series of GPT-based LLMs with up to 12B parameters, achieving a Pass@1 score of 28.81\% on HumanEval. CodeLlama \cite{DBLP:journals/corr/abs-2308-12950} is another LLM family with up to 70B parameters from Meta AI, trained from Llama 2 \cite{DBLP:journals/corr/abs-2307-09288} to follow human instructions. % WizardCoder \cite{DBLP:conf/iclr/LuoX0SGHT0LJ24} emphasizes the importance of instruction-tuning, outperforming previous open-source LLMs by a large margin. 
DeepSeek-Coder \cite{DBLP:journals/corr/abs-2401-14196} is a family of LLMs with up to 33B parameters, supporting both normal generation and fill-in-the-middle (FIM). Its successor, DeepSeek-Coder-V2 \cite{DBLP:journals/corr/abs-2406-11931}, is a mixture-of-experts (MoE) LLM with 16B or 236B parameters, claiming to have GPT-4 \cite{DBLP:journals/corr/abs-2303-08774} level performance at a Pass@1 score of 90.2\% on HumanEval.


\subsection{SE Benchmarks and Metrics}
Many SE benchmarks focus solely on code generation, where LLMs generate code for the given requirements and function signatures. HumanEval \cite{DBLP:journals/corr/abs-2107-03374} is one of the most adopted code generation benchmarks, featuring 164 human-curated Python problems. It uses Pass@\(k\) as the evaluation metric. % which first generates \(n~(n\ge k)\) responses, assuming \(c\) responses pass unit tests, and obtains \(\text{Pass@}k=1-\binom{n-c}{k}/\binom{n}{k}\) for each problem, before averaging on the entire benchmark. 
MBPP \cite{DBLP:journals/corr/abs-2108-07732} is another popular benchmark with 974 Python problems, aiming at entry-level developers. APPS \cite{DBLP:conf/nips/HendrycksBKMAGB21} is a much larger Python benchmark with 10000 problems, ranging from being solvable in one-line to presenting substantial challenges in algorithms. % Considering the low difficulty in previous benchmarks, 
ClassEval \cite{DBLP:journals/corr/abs-2308-01861} challenges LLMs with 100 class-level code generation problems in Python, and measure class-level and method-level Pass@\(k\). 

Some benchmarks target other SE tasks. CodeReviewer \cite{DBLP:conf/sigsoft/LiLGDJJMGSFS22} aims at three tasks in the code review process: commit quality estimation, reviewer comment generation, and code editing. CodeXGLUE \cite{DBLP:conf/nips/LuGRHSBCDJTLZSZ21} supports 10 SE tasks such as code summarization and code search. ComplexCodeEval \cite{DBLP:journals/corr/abs-2409-10280} collects code from influential GitHub repositories for 4 tasks such as code generation and unit test generation. These benchmarks all evaluate responses with conventional metrics including Exact Match, Edit Similarity, BLEU, and CodeBLEU instead of Pass@\(k\), even for code-generating tasks. CRUXEval \cite{DBLP:conf/icml/GuRLSS024} evaluates LLMs from other aspects such as code understanding and execution with 800 short Python functions for input or output predictions. It requires LLMs to output assert statements to obtain Pass@\(k\) scores. 

However, limited efforts are made to curate meta-evaluation benchmarks to test evaluation metrics, as most datasets only contain instructions and reference answers, without responses of different quality or human-annotated scores. NoFunEval \cite{DBLP:journals/corr/abs-2401-15963} designs six evaluation aspects, including functional correctness and non-functional aspects like latency and maintainability. It tests whether LLMs can improve code based on a specific aspect or select the better of two code snippets from that perspective. CodeUltraFeedback \cite{DBLP:journals/corr/abs-2403-09032} evaluates LLMs' alignment with human evaluation from five non-functional code aspects like instruction following and coding style.

\subsection{LLM-as-a-Judge in NLP}
\subsubsection{Embedding-based Methods}
Some researchers obtain contextual token representations of the response and reference answer using encoder-only LLMs, and compute pairwise similarity to obtain the score. BERTScore \cite{DBLP:conf/iclr/ZhangKWWA20} calculates Recall, Precision and F\(_1\) score based on token representations obtained from BERT \cite{DBLP:conf/naacl/DevlinCLT19}. It also applies inverse document frequencies (IDFs) to reduce the weight of overly common and thus less essential tokens. MoverScore \cite{DBLP:conf/emnlp/ZhaoPLGME19} constructs a transportation cost matrix based on token representations and computes Word Mover's Distance \cite{DBLP:conf/icml/KusnerSKW15}. CodeBERTScore \cite{DBLP:conf/emnlp/Zhou0AN23} is a code-specific adaptation of BERTScore with CodeBERT, approximating functional correctness and human scores with F\(_3\) and F\(_1\) scores respectively. While these methods match contextual embeddings instead of n-grams, unlike many conventional metrics, they still measure how a response resembles the reference answer.

\subsubsection{Probability-based Methods}
Since more LLMs come with decoders, it becomes possible to use generating probabilities for evaluation. BARTScore \cite{DBLP:conf/nips/YuanNL21} assumes that BART \cite{DBLP:conf/acl/LewisLGGMLSZ20} is more likely to generate a higher-quality response. It uses the probability of BART generating a given response as the score. GPTScore \cite{DBLP:conf/naacl/FuNJ024} applies a similar approach with 19 LLMs of sizes from 80M to 175B, supporting both reference-free and reference-based evaluation from multiple aspects. FFLM \cite{DBLP:conf/emnlp/0003RLZ23} is a reference-free method designed to evaluate the faithfulness of summaries. It calculates the probabilities of generating the summary with and without the original text as posterior and prior probabilities respectively. FFLM assumes that a faithful summary has higher posterior than prior probability, and calculates their difference as the score.

\subsubsection{Output-based Methods} 
While the above methods usually align with human evaluation better than conventional metrics, they do not explain their scores or support certain closed-source LLMs that do not provide probabilities or representations. Output-based methods prompt LLMs to output the judgments, and do not require access to their internal implementations. G-Eval \cite{DBLP:conf/emnlp/LiuIXWXZ23} utilizes Chain-of-Thought (CoT) \cite{DBLP:conf/nips/Wei0SBIXCLZ22} to request evaluation steps, samples multiple scores and then averages them as the final score. ChatEval \cite{DBLP:conf/iclr/ChanCSYXZF024} assigns different personas to several LLM agents, asking them to discuss and select a better response from two. 

Some researchers construct training sets to fine-tune LLMs instead of designing prompting or inference strategies. InstructScore \cite{DBLP:conf/emnlp/XuWPSFWL23} is fine-tuned on GPT-4-synthesized data to generate error reports of text from various domains. PandaLM \cite{DBLP:conf/iclr/WangYYZYW0J000024} is fine-tuned on pairwise comparison results and reference answers generated by GPT-3.5, aiming at addressing subjective aspects including conciseness and clarity. X-Eval \cite{DBLP:conf/naacl/LiuSX0CKGH24} has an extra training stage to learn the connections between fine-grained evaluation aspects, allowing evaluating from aspects not seen during training.

However, these methods have not been tested on a sufficient number of challenging SE samples, leaving it unclear whether they achieve reliable human alignment for SE applications.