\section{Study Results\label{result}}
In this section, we present experimental results and our analysis to answer the research questions.

\subsection{RQ1: Alignment with Human Scores}
\begin{table}[]
    \centering
    \caption{Experimental results for individual scoring. DS2.5 means DeepSeek-V2.5 while DSC2-Lite means DeepSeek-Coder-V2-Lite. The best alignment in each column is marked bold. The best conventional metric alignment and better results in other categories are underlined. Coefficients with \(p>0.05\) are marked red.}
    \vspace{-1.0em}
    \begin{tabular}{c|ccc|ccc|ccc}
         \toprule
         \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Translation} & \multicolumn{3}{c|}{Generation} & \multicolumn{3}{c}{Summarization} \\

         & \(\rho\) & \(R\) & \(\tau\) & \(\rho\) & \(R\) & \(\tau\) & \(\rho\) & \(R\) & \(\tau\) \\ \midrule

         \rowcolor{gray!40} \multicolumn{10}{c}{\textbf{Conventional Metrics}} \\ \midrule
BLEU        & 31.12 & 28.08 & 22.43 & 58.08 & 55.83 & 41.90 & 19.80 &  24.77 &  16.78 \\
ROUGE-L       & 28.55 & 28.57 & 20.29 & 55.72 & 57.62 & 40.81 & \underline{\textbf{48.45}} &  \underline{\textbf{47.01}} &  \underline{\textbf{35.47}} \\
METEOR      & 22.48 & 31.79 & 15.98 & \underline{\textbf{67.11}} & \underline{65.55} & \underline{49.66} & 38.83 &  40.01 &  28.27 \\
ChrF++        & \underline{31.30} & \underline{34.23} & \underline{22.65} & 64.02 & 64.92 & 46.60 & 47.26 &  44.65 &  33.86 \\
CrystalBLEU & 23.63 & 25.26 & 17.43 & 59.02 & 56.65 & 42.88 & 23.19 &  24.96 &  17.24 \\ \midrule

         \rowcolor{gray!40}\multicolumn{10}{c}{\textbf{Embedding-based}} \\ \midrule
BERTScore   & 27.72 & 32.49 & 19.54 & 41.39 & 44.74 & 30.36 & 21.71 &  21.89 &  15.57 \\
MoverScore  & 28.29 & 26.22 & 19.99 & 46.64 & 47.35 & 33.66 & 31.86 &  29.44 &  22.82 \\ \midrule

         \rowcolor{gray!40}\multicolumn{10}{c}{\textbf{Probability-based}} \\ \midrule
GPTScore    & \underline{33.53} & \underline{34.77} & \underline{25.12} & 46.65 & 45.42 & 35.00 &\textcolor{red}{-13.34} & \textcolor{red}{-15.04} &  \textcolor{red}{-9.28} \\
FFLM        & \underline{34.03} & 29.37 & \underline{25.50} & 29.31 & 29.62 & 21.65 & \textcolor{red}{-2.29} &  \textcolor{red}{-8.71} &  \textcolor{red}{-1.94} \\ \midrule

         \rowcolor{gray!40}\multicolumn{10}{c}{\textbf{Output-based: Vanilla}} \\ \midrule
DSC2-Lite   & \underline{33.10} & \underline{46.26} & \underline{26.56} & \textcolor{red}{15.71} & 28.28 & \textcolor{red}{12.50} &-17.76 & -17.47 & -15.25 \\
DS2.5      & \underline{62.43} & \underline{70.27} & \underline{49.48} & 66.39 & \underline{\textbf{68.51}} & \underline{\textbf{54.74}} &  17.73 & 18.10 & 14.14 \\
GPT-4o       & \underline{70.67} & \underline{79.11} & \underline{57.85} & 54.70 & 57.02 & 43.56 &  24.52 & 23.15 & 19.27 \\ \midrule

         \rowcolor{gray!40}\multicolumn{10}{c}{\textbf{Output-based: Inference strategies}} \\ \midrule
G-Eval       & \underline{68.96} & \underline{77.14} & \underline{52.90} & 60.71 & 63.05 & 46.36 & 23.34 &  26.19 &  17.18 \\
BatchEval   & \underline{\textbf{73.67}} & \underline{\textbf{81.32}} & \underline{\textbf{59.80}} & 59.54 & 63.04 & 48.62 & 22.56 &  22.46 &  18.39 \\ \midrule

         \rowcolor{gray!40}\multicolumn{10}{c}{\textbf{Output-based: SFT}} \\ \midrule
Llama2      &  \textcolor{red}{2.61} &  \textcolor{red}{1.03} &  \textcolor{red}{1.92} & 23.91 & 22.89 & 18.94 &\textcolor{red}{-15.61} & \textcolor{red}{-15.81} & \textcolor{red}{-12.82} \\
Auto-J       & 20.99 & \textcolor{red}{14.43} & 17.45 & 36.53 & 38.92 & 29.79 & \textcolor{red}{-5.13} &  \textcolor{red}{-4.92} &  \textcolor{red}{-4.36} \\
Mixtral     & 24.67 & 34.07 & 19.52 & \textcolor{red}{14.41} & 25.32 & \textcolor{red}{11.18} & \textcolor{red}{-3.97} &  \textcolor{red}{-8.98} &  \textcolor{red}{-3.39} \\
Prometheus  & \underline{32.42} & \underline{39.25} & \underline{26.60} & 29.03 & 40.33 & 23.09 &-17.12 & -17.14 & -14.24 \\ \bottomrule
         
    \end{tabular}
    \label{tab:RQ1_res}
    \vspace{-1.5em}
\end{table}

We use LLM-as-a-judge methods to score individual responses and evaluate their correlation with human scores. Table \ref{tab:RQ1_res} presents the alignment between human scores and scores generated by various methods, including both LLM-as-a-judge methods and conventional metrics.
We notice that the three types of correlation coefficients display similar trends, and make the following discoveries:

\textbf{Current LLM-as-a-judge methods lack generalizability, as they demonstrate drastically different performance in different tasks and scenarios.} In Code Translation, BatchEval reaches the highest human alignment, offering near-human performance at \(\rho=73.67\), \(R=81.32\), and \(\tau=59.80\), while G-Eval, DeepSeek-V2.5, and GPT-4o also reach a high correlation of \(R>70\) or \(\rho>60\), greatly outperforming conventional metrics capped at \(R=34.23,\rho=31.30\). We attribute this to the characteristic of responses and reference answers: LLMs often copy statements from the original code with subtle language-specific modifications as the response. Meanwhile, although the reference answer maintain unchanged core functionality, its exact implementation and behavior might noticeably differ. This presents a disadvantage for reference-based methods including most non-output-based methods and conventional metrics. Output-based methods, however, are designed to work without reference and can utilize LLMs' knowledge of programming languages in evaluation.

On the contrary, LLM-as-a-judge methods struggle to outperform conventional metrics in evaluating code generation outputs and are completely surpassed in evaluating code summarization. For code generation, conventional metrics can reach a mid-high correlation of \(\rho=67.11\), \(R=65.55\), and \(\tau=49.66\), while DeepSeek-V2.5 is the only LLM outperforming them at \(\rho=66.39\), \(R=68.51\), and \(\tau=54.74\) without any additional inference strategies. This can be attributed to the characteristics of the ComplexCodeEval dataset, which emphasizes the usage of complicated dependencies by filling out the correct arguments and calling them at the right time instead of designing sophisticated algorithms. Therefore, a response-reference comparison at the lexical-level can offer an insight of the response's quality, while the LLMs' limited understanding of the dependencies fail to provide benefits in evaluation. With that said, for code generation, LLM-as-a-judge methods with large LLMs like GPT-4o are still applicable, since they display similar performance as conventional metrics but provide the benefits of not requiring reference answers. For code summarization, LLM-as-a-judge techniques are completely defeated by conventional metrics, hardly reaching a score of 30 in any correlation coefficient or even demonstrating a negative correlation with human evaluation. Nonetheless, conventional metrics also fail to deliver satisfying alignment with human evaluation, with \(\rho,R<50\) and \(\tau<40\). This is potentially due to the fact that many LLMs try to explain the code step-by-step instead of summarizing the core functionality, which is difficult for these LLM-as-a-judge methods to detect. While conventional metrics can assign low scores to these responses, they have trouble handling paraphrasing, which is common in summaries. It is an interesting future direction to explore new metrics that align with humans for code summarization.

\vspace*{6pt}
\begin{leftbar}
\textbf{Finding 1:} Current LLM-as-a-judge methods demonstrate low generalizability in aligning with human evaluation, outperforming conventional metrics in code translation, performing on par with them in code generation, while being outperformed in code summarization.
\end{leftbar}
\vspace*{6pt}

\textbf{Inference using large LLMs yields the best human alignment across all tasks, while inference strategies only provide marginal improvement.} Embedding-based and probability-based methods underperform output-based methods in most scenarios, capped at \(R=34.77,47.35,29.44\) versus the top performance of the latter at \(R=81.32,68.51,26.19\), and the top performance of conventional metrics at \(R=34.23,65.55,47.01\) in code translation, code generation, and code summarization respectively. Furthermore, embedding-based and probability-based methods require access to internal states, while the API services of many state-of-the-art LLMs only allow access to the final output. Therefore, these methods cannot be applied with such LLMs, limiting their applicability. Based on the low human alignment and limited applicable LLMs, we conclude that embedding-based and probability-based methods are impractical for evaluating SE tasks.

Among the output-based methods, we find that DeepSeek-V2.5 and GPT-4o outperform other LLMs without further training. Although Auto-J and Prometheus 2, trained to match human preference, provide better performance than their base model, with a 5.18\% to 16.03\% increase in Pearson's \(R\), achieving \(R=38.92\) and \(R=40.33\) in evaluating code generation respectively, the overall performance is still inferior. This is likely due to the limited number of parameters, as Auto-J and Prometheus 2 only have 13B and 47B parameters. Another possible reason is the misalignment between evaluating NLP tasks during training, and evaluating SE tasks during inference. Though many NLP training datasets contain programming tasks, they may only present common tasks like code generation and fail to present sufficiently challenging instructions. Unfortunately, to the best of our knowledge, no multi-task human preference training sets for SE task evaluation have been curated so far. Hence, we are unable to investigate LLMs fine-tuned on such SE-specific datasets.

Similarly, current inference strategies, when employed to GPT-4o, produce an inadequate performance boost of \(\Delta R=2.21,6.03,3.04\) at maximum. Despite recent work claiming the effectiveness of scaling inference \cite{DBLP:journals/corr/abs-2408-03314}, we found that existing inference strategies for SE evaluation only bring marginal improvement in human alignment. Moreover, they have different downsides: G-Eval forces LLMs to generate the overall score, restricting the efficacy of the Chain-of-Thought procedure, while greatly increasing inference cost if the full explanations are needed; BatchEval increases the token count, leading to more expensive inference due to multi-round evaluation. Therefore, greedy decoding remains a viable LLM-as-a-judge solution with satisfactory performance and lower requirements of token count, when equipped with colossal state-of-the-art LLMs.

\vspace*{6pt}
\begin{leftbar}
\textbf{Finding 2:} Among the LLM-as-a-judge methods studied, output-based methods with large state-of-the-art LLMs perform best, regardless of inference strategies.
\end{leftbar}
\vspace*{6pt}

\subsection{RQ2: Score Characteristics}
\begin{table}[]
    \centering
    \caption{Category-wise correlation. \(\rho_\text{conv}\), \(\rho_\text{other}\), and \(\rho_\text{inner}\) are the maximum Spearman's \(\rho\) between the specified category with either conventional metrics, metrics from the other three categories, and other metric(s) from the same category. Coefficients above 50 are underlined, while those above 75 are marked bold.}
    \vspace{-1.0em}
    \begin{tabular}{c|ccc|ccc|ccc}
          \toprule
         \multirow{2}{*}{Category} & \multicolumn{3}{c|}{Translation} & \multicolumn{3}{c|}{Generation} & \multicolumn{3}{c}{Summarization} \\

         & \(\rho_\text{conv}\) & \(\rho_\text{other}\) & \(\rho_\text{inner}\) & \(\rho_\text{conv}\) & \(\rho_\text{other}\) & \(\rho_\text{inner}\) & \(\rho_\text{conv}\) & \(\rho_\text{other}\) & \(\rho_\text{inner}\) \\ \midrule

Embedding-based & \underline{\textbf{81.45}} & 37.81 & \underline{74.83} & \underline{\textbf{79.78}} & 46.74 & \underline{\textbf{84.20}} & \underline{57.07} & 23.32 & \underline{66.61} \\
Probability-based & 32.18 & 28.60 & 31.30 & \underline{63.69} & 46.74 & \underline{60.92} & 32.78 & 23.32 & \underline{63.13} \\
Output-based w/o SFT & 30.60 & 35.64 & \underline{\textbf{90.64}} & 47.25 & 39.56 & \underline{\textbf{88.25}} & 20.04 & 48.21 & \underline{79.43} \\ 
Output-based w/ SFT &  37.14 &  37.81 & 24.25 &  30.96 &  37.52 & 27.44 &  11.21 &  48.21 & 23.44 \\ \bottomrule

    \end{tabular}
    \label{tab:RQ2_res}
    \vspace{-0.5em}
\end{table}

\begin{table}[]
    \centering
    \caption{Correlation between output-based LLM-as-a-judge methods grouped by the sizes of the LLMs they use. Methods are categorized based on the underlying model size: "Small" (using <50B LLMs), which includes all methods from the SFT group and DeepSeek-Coder-V2-Lite from the Vanilla group, and "Large" (using >230B LLMs), including DeepSeek-V2.5 and GPT-4o, the latter used by G-Eval and BatchEval.}
    \vspace{-1.0em}
    \begin{tabular}{c|cc|cc|cc}
         \toprule
         \multirow{2}{*}{LLM Sizes Compared} & \multicolumn{2}{c|}{Translation} & \multicolumn{2}{c|}{Generation} & \multicolumn{2}{c}{Summarization} \\

         & \(\rho_{\min}\) & \(\rho_{\max}\) & \(\rho_{\min}\) & \(\rho_{\max}\) & \(\rho_{\min}\) & \(\rho_{\max}\) \\ \midrule

         Small-Small & -4.10 & 24.25 & -2.74 & 27.44 &  -5.55 & 23.44 \\
         Large-Large & 83.04 & 90.64 & 68.63 & 88.25 &  31.50 & 79.43 \\
         Small-Large &  4.16 & 48.92 & 11.70 & 37.84 & -16.15 & 48.21 \\ \bottomrule
    \end{tabular}
    \label{tab:RQ2_llm}
    \vspace{-1.0em}
\end{table}

We investigate the score characteristics of various LLM-as-a-judge methods. Table \ref{tab:RQ2_res} shows the maximum correlation between metrics from the same or different categories of LLM-as-a-judge methods, while Fig. \ref{fig:plot} displays the score distributions\footnote{Frequency estimated using Kernel Density Estimation (KDE). All scores rescaled into range \([0,1]\).} of different methods: (1) for manual evaluation, (2) for conventional metrics, (3) for embedding-based methods, (4) for probability-based methods, (5)(6)(7) for output-based methods without SFT, and (8)(9) for output-based methods with SFT. For each distribution, rather than focusing on the specific shape of the curve, we examine whether it is unimodal and note the peak frequency and the corresponding score. 

We make the following discoveries:

\textbf{Most non-SFT LLM-as-a-judge methods have low correlations with those from other categories and high correlations with those from the same category.} In Table \ref{tab:RQ2_res}, we observe that \(\rho_\text{other}<50\) for all categories, meaning that each category demonstrates a unique distribution of scores instead of resembling others. Conversely, \(\rho_\text{inner}>60\) under most non-SFT circumstances, exhibiting a medium to high level of agreement among similar methods. This phenomenon suggests that the mechanics governing each category may significantly influence their score distributions. In contrast, scores from SFT methods correlate poorly even within the same category, likely due to variations in their base LLMs and fine-tuning datasets. Given the high level of disagreement among current fine-tuned LLMs, we argue that selecting an appropriate fine-tuned LLM is crucial for evaluating under specific SE contexts. Otherwise, it may produce entirely unexpected scores. 

\textbf{Output-based methods using large LLMs tend to align well with each other, whereas those using smaller LLMs exhibit low correlations with other methods.} Since output-based methods offer the best human alignment, we further investigate whether LLM size influences correlations between methods by grouping these LLM-as-a-judge methods into those using large LLMs (>230B) and those using small LLMs (<50B). In Table \ref{tab:RQ2_llm}, we observe that methods employing large LLMs achieve high correlations of \(\rho>80\) for code translation and \(\rho>65\) for code generation with each other. These methods use DeepSeek-V2.5 and GPT-4o, and maintain strong alignment despite the difference in LLMs and inference strategies. In contrast, methods using small LLMs yield \(\rho<50\) when compared to methods in the "large" group, and \(\rho<30\) within the "small" group. This pattern reflects a performance gap, as the "large" group align substantially better with human evaluations than the "small" group.

\vspace*{6pt}
\begin{leftbar}
\textbf{Finding 3:} As anticipated, methods within the same category generally exhibit high correlations with each other and low correlations with those in different categories. Among output-based methods, those using large LLMs not only align well with human scores but also show strong correlations with each other.
\end{leftbar}
\vspace*{6pt}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{attachments/plot.pdf}
    \vspace{-2.0em}
    \caption{Score distributions of selected metrics. \(\mu,\sigma^2\) refer to the means and variances of scores for code translation, code generation, and code summarization. All scores are rescaled into range \([0, 1]\).}
    \label{fig:plot}
    \vspace{-1.0em}
\end{figure}

\textbf{Only embedding-based methods resemble conventional metrics.} We discover in Table \ref{tab:RQ2_res} that \(\rho_\text{conv}=81.45\), 79.78, 57.07 for the 3 tasks with embedding-based methods, indicating a high correlation with conventional metrics. As shown in Fig. \ref{fig:plot}, MoverScore from this category exhibits a distribution similar to that of ChrF++, one of the most human-aligning conventional metrics, as both tend to assign low to medium scores to responses. This similarity is anticipated, given that both metrics are designed to assess the similarity between the response and the reference. While MoverScore leverages contextual token representations beyond simple lexical matching, the underlying principles remain fundamentally aligned. On the other hand, distributions from other categories differ markedly from those of ChrF++ as shown by their low \(\rho_\text{conv}\) values, further underscoring their limited resemblance to conventional metrics.

\textbf{The best human-aligning methods closely replicate the distribution of human scores.} As shown in Fig. \ref{fig:plot}, GPT-4o, G-Eval, and DeepSeek-V2.5 in plots (5)(6)(7) demonstrate the highest alignment with human judgment in plot (1), with similar peak frequencies and corresponding scores between 0.6 and 0.8. GPTScore in plot (4) also shows similar peaks for code translation and code generation, though its scores are less evenly distributed compared to these top methods, as seen in the high peak frequency and reduced variance values \(\sigma^2\). In contrast, the underperforming methods, such as ChrF++ in plot (2) and MoverScore in plot (3), peak at much lower scores, resulting in average scores notably below those of human evaluators. Interestingly, Prometheus 2 in plot (9) shows a comparable peak location and a relatively balanced distribution after fine-tuning on Mixtral outside code summarization, yet this does not correspond to a high human alignment.

\vspace*{6pt}
\begin{leftbar}
\textbf{Finding 4:} Only embedding-based methods align closely with conventional metrics, while the most human-aligned output-based methods display more balanced score distributions that mirror human scoring patterns.
\end{leftbar}
\vspace*{6pt}

\subsection{RQ3: Pairwise Comparison versus Individual Scoring}
Since embedding-based and probability-based methods can only score individual responses, we only analyze output-based methods for pairwise comparison. Table \ref{tab:RQ3_res} presents the results.

In general, \textbf{current LLM-as-a-judge methods fail to deliver satisfactory and consistent comparison performance on SE tasks.} For code translation, G-Eval and BatchEval reach the highest Accuracy of 64.67 and 65.33, followed by GPT-4o and DeepSeek-V2.5, and all other methods fall below 50 Accuracy. On code generation, even the best-performing methods struggle to achieve 50 Accuracy, while all methods become completely unusable on code summarization. 

We also evaluate their consistency by reversing the order of the two responses in the prompt to check if methods yield the same comparison results, measured as Agreement. Table \ref{tab:RQ3_res} shows that methods with the highest accuracy on code translation and generation yield extremely low Agreement below 25, indicating poor consistency. Meanwhile, the most consistent methods from the SFT category barely outperform random guessing, where each outcome (selecting a better response or declaring a tie) has an equal \(\frac 13\) chance.

Although unreliable and inconsistent, \textbf{their comparison Accuracy displays a similar trend as in individual scoring.} For the first two tasks, methods applying inference strategies on large LLMs, such as G-Eval and BatchEval with GPT-4o, exhibit the highest Accuracy, followed by DeepSeek-V2.5 and GPT-4o with greedy decoding and no further strategies, though the performance impact of inference strategies is noticeably larger than in individual scoring. For example, BatchEval provides up to +8 Accuracy boost here for GPT-4o compared to +3 in Spearman's \(\rho\) in RQ1 on code translation. Besides, DeepSeek-Coder-V2-Lite, a code LLM with merely 16B parameters, also defeats LLMs fine-tuned to evaluate NLP tasks, but again lags behind large LLMs.

\vspace*{6pt}
\begin{leftbar}
\textbf{Finding 5:} Current LLM-as-a-judge methods exhibit disappointing Accuracy in pairwise comparisons and often yield inconsistent results when the order of two responses is reversed. As with individual scoring, output-based methods using large LLMs achieve the highest Accuracy, yet inference strategies provide a larger performance boost than in individual scoring. However, these strategies do not fully resolve the inconsistency issue.
\end{leftbar}
\vspace*{6pt}

\begin{table}[]
    \centering
    \caption{Experimental results for pairwise comparison, where Acc. means Accuracy and Agr. means agreement. The best alignment in each column is marked in bold. Results better than random guessing are underlined.}
    \begin{tabular}{c|cc|cc|cc}
         \toprule 
         \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Translation} & \multicolumn{2}{c|}{Generation} & \multicolumn{2}{c}{Summarization} \\

         & Acc. & Agr. & Acc. & Agr. & Acc. & Agr. \\ \midrule

         Random guess & 33.33 & 33.33 & 33.33 & 33.33 & 33.33 & 33.33 \\ \midrule

         \rowcolor{gray!40} \multicolumn{7}{c}{\textbf{Vanilla}} \\ \midrule
DSC2-Lite  & \underline{44.67} & \underline{36.00} & \underline{38.67} & \underline{36.67} & 30.33 & 32.00 \\
DS2.5     & \underline{51.00} & 10.67 & \underline{48.33} & 16.67 & 26.67 & 16.67 \\
GPT-4o      & \underline{57.33} & 13.33 & \underline{49.33} & 13.33 & 25.00 & 16.00 \\ \midrule

         \rowcolor{gray!40} \multicolumn{7}{c}{\textbf{Inference strategies}} \\ \midrule
G-Eval      & \underline{64.67} & 17.33 & \underline{\textbf{54.67}} & 15.33 & \underline{34.33} & 32.67 \\
BatchEval  & \underline{\textbf{65.33}} & 21.33 & \underline{52.67} & 24.00 & \underline{36.33} & \underline{38.00} \\ \midrule

        \rowcolor{gray!40}  \multicolumn{7}{c}{\textbf{SFT}} \\ \midrule
Llama2     & \underline{36.00} & \underline{\textbf{78.67}} & \underline{34.67} & \underline{\textbf{72.67}} & 31.67 & \underline{\textbf{56.00}} \\
Auto-J      & \underline{33.33} & \underline{52.00} & \underline{38.33} & 28.67 & 23.33 & 16.00 \\
Mixtral    & 29.00 & \underline{48.67} & 32.00 & \underline{40.00} & 32.67 & \underline{48.67} \\
Prometheus & \underline{33.67} & \underline{35.33} & \underline{37.67} & 31.33 & \underline{\textbf{42.00}} & 26.00 \\ \bottomrule
         
    \end{tabular}
    \label{tab:RQ3_res}
\vspace{-2.0em}
\end{table}


