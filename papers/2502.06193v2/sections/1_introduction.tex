\section{Introduction\label{intro}}
Since BERT \cite{DBLP:conf/naacl/DevlinCLT19} and GPT \cite{radford2018improving}, pre-trained language models (PLMs) have been widely used in various natural language processing (NLP) tasks, such as machine translation and text summarization. With the scaling of PLM parameters, the concept of large language models (LLMs) has been proposed. Featuring up to hundreds of billions of parameters, LLMs emerge new capabilities absent on smaller models \cite{DBLP:journals/tmlr/WeiTBRZBYBZMCHVLDF22}, beyond solving simple linguistic tasks. These capabilities include, but are not limited to, instruction following and multi-step reasoning, enabling LLMs to simulate human experts and achieve state-of-the-art performance in certain domains. Software engineering (SE) is one of the specialized domains that benefits from this trend. Many researchers and companies either emphasize their LLMs' strong coding performance \cite{DBLP:journals/corr/abs-2303-08774,DBLP:journals/corr/abs-2403-05530}, or develop specialized code LLMs. For instance, DeepSeek-Coder-V2 \cite{DBLP:journals/corr/abs-2406-11931} correctly generates code for 75.3\% instructions in HumanEval \cite{DBLP:journals/corr/abs-2107-03374} and MBPP \cite{DBLP:journals/corr/abs-2108-07732} with 236B parameters, second only to GPT-4o. Qwen2.5-Coder \cite{hui2024qwen2} achieves 88.4\% Pass@1 on HumanEval with merely 7B parameters. 

However, there has been limited progress in evaluating LLM-generated content for SE. The commonly used Pass@\(k\) metric executes the first \(k\) generated code snippets on human-curated unit tests. While Pass@\(k\) evaluates the code's functional correctness accurately, it has several limitations, such as requiring comprehensive unit tests and manual configuration of test environments. What is more, Pass@\(k\) is unable to evaluate code from non-functional aspects, such as readability and adherence to good practice, nor can it be used to judge text-generating SE tasks like code summarization and code review \cite{DBLP:journals/corr/abs-2308-10620}. Therefore, some SE datasets \cite{DBLP:journals/corr/abs-2409-10280,DBLP:conf/emnlp/YanTLCW23} resort to use conventional metrics such as BLEU \cite{DBLP:conf/acl/PapineniRWZ02} and CodeBLEU \cite{DBLP:journals/corr/abs-2009-10297}, which also have downsides like inability to perform multi-aspect evaluation and requiring human-annotated reference answers. These metrics also focus on lexical rather than semantic similarity, making the evaluation results questionable.

Meanwhile, NLP researchers attempt to apply LLMs to evaluate the quality of LLM-generated content, known as LLM-as-a-judge \cite{DBLP:conf/nips/ZhengC00WZL0LXZ23}. While human effort remains reliable for evaluation and for curating the reference answers in datasets, it is both slow and expensive, defeating the purpose of automatic evaluation. Therefore, researchers prompt or train LLMs to align with human preference, as an attempt to replace human evaluators. 
% Here, LLMs can be used as either white boxes, by extracting the representation (embedding-based) \cite{DBLP:conf/iclr/ZhangKWWA20} or generating probability (probability-based) \cite{DBLP:conf/nips/YuanNL21} of each token, or as black boxes, by prompting LLMs to output their judgments directly (output-based) \cite{DBLP:conf/emnlp/LiuIXWXZ23}\cite{DBLP:conf/iclr/WangYYZYW0J000024}. Depending on the exact methodology, an LLM-as-a-judge method can either be reference-based or reference-free, although black-box methods are usually designed to work without reference answers \cite{DBLP:journals/corr/abs-2405-01535} to reduce human effort. 
Since both code and text can be viewed as sequences of tokens, LLM-as-a-judge methods can be potentially adopted on SE tasks. Unfortunately, current meta-evaluation benchmarks feature a limited number of simple coding tasks as they mostly target NLP tasks. The lack of test samples and the insufficient task difficulty create a gap between benchmarking on existing datasets and real-world SE scenarios, where the instructions, code, and responses are usually more complex and varied.

To bridge the gap, we conduct an empirical study to apply a range of LLM-as-a-judge methods on realistic SE datasets. Specifically, we select a task for each of the three input-output type combinations, and a recent representative dataset for each task: CodeTransOcean \cite{DBLP:conf/emnlp/YanTLCW23} for Code Translation (Code-Code), ComplexCodeEval \cite{DBLP:journals/corr/abs-2409-10280} for Code Generation (Text-Code), and CodeXGLUE \cite{DBLP:conf/nips/LuGRHSBCDJTLZSZ21} for Code Summarization (Code-Text). Their corresponding papers only adopt conventional metrics like Exact Match (EM), BLEU, and CodeBLEU. We randomly sample 50 instructions from each dataset, and three out of 12 code LLMs to generate responses for each instruction. For each response, we manually assign a score indicating its quality, resulting in a dataset of 450 samples of (instruction, response, score) triplets in total. Then we perform meta-evaluation of different types of LLM-as-a-judge methods by calculating their score alignment with human scores, to validate whether their judgments match human preference in real-world scenarios.

We design the following three research questions (RQs):

\begin{itemize}
    \item \textbf{RQ1: Which LLM-as-a-judge method aligns with human preference better, and do they outperform conventional metrics?} 

    We aim to assess whether various LLM-as-a-judge methods can replace human evaluators due to high human alignment and superior performance to conventional metrics. We select seven methods across embedding-based, probability-based, and output-based categories, along with two LLMs fine-tuned specifically for NLP evaluation along with their base model, and conventional metrics such as BLEU. We compute the correlations between human scores and scores from these methods to indicate how well they align with human preference on the selected SE tasks.

    \item \textbf{RQ2: What are the characteristics of LLM scores, more specifically their alignments with one another and score distributions?} 

    We aim to characterize the score distributions from LLM-as-a-judge methods with their distributions and correlations. Specifically, we measure the correlations among all methods, to determine if similar methods yield similar results, and to assess whether they actually mimic human evaluators beyond merely measuring lexical similarity. We also analyze the score distribution of each method to investigate their ability to generate varied scores. 

    \item \textbf{RQ3: How do LLMs perform when prompted to make pairwise comparisons instead of individual scoring?} 

    Comparing two responses is also a common choice for LLM-as-a-judge methods, with some studies claiming its superiority over scoring individual responses \cite{DBLP:journals/corr/abs-2310-01432}. We conduct similar experiments to evaluate the performance of these methods when LLMs are instead prompted to select a better response from two, or declare a tie. Since embedding-based and probability-based methods cannot perform this ternary classification without scoring each response first, we focus solely on output-based methods in this RQ. 

\end{itemize}

Through answering the RQs, we conclude that:
\begin{itemize}
    \item The human alignments of studied methods heavily depend on the SE tasks. Among them, output-based methods with large LLMs perform best, achieving near-human performance in code translation and generation.
    \item Similar methods yield similar score distributions, most of which differ from those of conventional metrics. The best human-aligning methods demonstrate more balanced and human-like distributions.
    \item Studied methods fail to deliver accurate and consistent comparison results. Output-based methods with large LLMs still provide the highest accuracy, but often yield inconsistent results after swapping the positions of two responses.
\end{itemize}

Our contributions can be summarized as follows:
\begin{itemize}
    \item Our work serves as the first empirical study to investigate applying LLM-as-a-judge methods specifically to SE tasks, with much more difficult code-specific instructions and responses compared to previous studies.
    \item We manually curate a meta-evaluation dataset based on three existing SE datasets for different tasks, to evaluate human alignment of LLM-as-a-judge methods.
    \item We explore how different LLM-as-a-judge methods prefer to score responses, and discuss the findings and possible implications for their future studies and applications in SE.
\end{itemize}

The rest of the paper is organized as follows: Section \ref{literature} introduces research relevant to code LLMs, SE task evaluation, and notable LLM-as-a-judge methods. Section \ref{overview} offers more details in different categories of LLM-as-a-judge methods. Section \ref{experiment} presents the overall study design. Section \ref{result} records the experimental results and analyzes our findings. Section \ref{discussion} analyzes score explanations as a case study and possible future directions based on our findings. Section \ref{conclusion} concludes the paper.