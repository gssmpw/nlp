\section{Study Design\label{experiment}}
In this section, we elaborate on the details of our study design. In our study, we focus on leveraging different types of LLM-as-a-judge methods to evaluate the responses of three SE tasks. We collect instructions and generate responses from representative datasets, and then perform human and LLM evaluation on these responses, and analyze their correlations.

\subsection{Datasets and Preprocessing}
\subsubsection{Instruction Collection}
To ensure the difficulty of instructions and to approximate real-world development scenarios, we collect instructions from a recent dataset for each of the three widely-studied SE tasks for our empirical evaluation:

\begin{itemize}
    \item \textbf{Code Translation} is a code-to-code task demanding translating code between two languages while preserving the functionality. It challenges LLMs' skills to understand syntax and library usages in both languages, and to choose replacements when certain functionalities are unavailable in the target language.

    \item \textbf{Code Summarization} is a code-to-text task involving generating a concise and fluent description of a given code snippet. It challenges LLMs' abilities to abstract the code, leaving only critical information about core functionality rather than explaining step-by-step.

    \item \textbf{Code Generation} is a text-to-code task requiring generating a function based on a natural language description and the signature. It tests LLMs' capabilities to breakdown the functional requirement into steps, and to utilize provided dependencies.
\end{itemize}

We select the \textbf{MultilingualTrans}\footnote{Collected from Rosetta Code, \url{https://rosettacode.org/wiki/Rosetta_Code}.} subset of \textbf{CodeTransOcean} \cite{DBLP:conf/emnlp/YanTLCW23} for code translation. CodeTransOcean contains three translation subsets for different purposes, with the MultilingualTrans subset covering eight popular languages with 7545 samples. Compared to previous benchmarks, CodeTransOcean offers more pairs of programming languages of longer code, with the average length of test sets reaching 491 tokens in MultilingualTrans, as opposed to 58 tokens in CodeTrans featured in CodeXGLUE, which only supports Java and C\#. CodeTransOcean evaluates translations with conventional metrics such as Exact Match, BLEU, and CodeBLEU, rather than execution-based metrics like Pass@\(k\), since they require constructing unit tests and testing environments.

We select the code-text subset of \textbf{CodeXGLUE} \cite{DBLP:conf/nips/LuGRHSBCDJTLZSZ21} for code summarization, which is a filtered version of CodeSearchNet\footnote{Collected from public GitHub repositories.} \cite{DBLP:journals/corr/abs-1909-09436}. Initially developed for code search, i.e. retrieving relevant code based on natural language queries, CodeSearchNet contains two million code snippets in six programming languages accompanied by docstrings. These docstrings come from the associated function documentation and serve as summaries. CodeXGLUE removes samples with syntactically incorrect code, or docstrings that are either non-English, overly lengthy, or too short. After filtering, 14918 Python samples and 10955 Java samples remain, along with samples in four other programming languages. CodeXGLUE uses BLEU to evaluate generated summaries.

We select \textbf{ComplexCodeEval}\footnote{Collected from GitHub repositories.} \cite{DBLP:journals/corr/abs-2409-10280} for code generation. ComplexCodeEval is a benchmark with 3897 and 7184 Java and Python samples respectively, supporting four tasks: code generation, code completion, unit test generation, and API recommendation. Compared to previous benchmarks, ComplexCodeEval provides comprehensive supplemental material for each code snippet, including functional dependencies, timestamps, and unit tests. It expects LLMs to learn project-specific dependencies beyond standard library or popular third-party APIs. ComplexCodeEval evaluates code-generating tasks with conventional metrics such as Edit Similarity, BLEU, and CodeBLEU. % The authors of the dataset do not explain the choice of non-execution-based metrics, or provide testing environment specifications despite the presence of unit tests.

When training, validation, and test sets are available, we only adopt the test set for our evaluation. To ensure the accuracy of manual evaluation, we limit the programming languages to Java, Python, C, and C++ according to the human evaluators' expertise. Following previous work's \cite{DBLP:journals/corr/abs-2310-03304,DBLP:journals/corr/abs-2405-01535} context length of 4096 tokens, we also apply length limits of 1536, 1536, and 1024 tokens\footnote{Measured with OpenAI's Tiktoken, \url{https://github.com/openai/tiktoken}, with GPT-4o's vocabulary o200k\_base.} for instructions, responses\footnote{Here we limit the length of reference answers instead of actual responses generated in the next step.}, and output-based judgments respectively, removing samples with lengthy instructions or reference answers. We sample 50 instructions from each filtered dataset, resulting in 150 instructions in total.

\subsubsection{Response Generation}
We deploy 12 recent code LLMs with different sizes from seven families shown in Table \ref{tab:llm_list} from Hugging Face \cite{DBLP:journals/corr/abs-1910-03771}. We generate responses using these LLMs with vLLM \cite{DBLP:conf/sosp/KwonLZ0ZY0ZS23} on an Ubuntu 20.04 server with two Intel Xeon Platinum 8276L CPUs, four NVIDIA A100-40GB GPUs, and 256 GB RAM. For each instruction, we randomly select three LLMs to respond, yielding three responses \(A,B,C\). For pairwise comparisons, we create three response pairs \((A,B), (A,C),(B,C)\), and another three pairs \((B,A),(C,A),(C,B)\) in order to check if studied methods yield consistent judgment after reversing the order within a response pair. Thus, we obtain 150 responses and 300 response pairs per task, resulting in 450 responses and 900 pairs in total. 

\begin{table}[]
    \centering
    \caption{Selected LLMs for response generation, sorted by their release date.}
    \vspace{-1.0em}
    \begin{tabular}{c|c|c|c}
    \toprule
    LLM Family & Developer & Size & Date \\ 
    \midrule
    CodeLlama-Instruct \cite{DBLP:journals/corr/abs-2308-12950} & Meta AI & 7/13/34B & 2023.8 \\ 
    DeepSeek-Coder \cite{DBLP:journals/corr/abs-2401-14196} & DeepSeek AI & 1.3/6.7/33B & 2023.11 \\ 
    MagiCoder-S-DS \cite{DBLP:conf/icml/0003W0D024} & UIUC \& THU & 6.7B & 2023.12 \\ 
    Codestral-v0.1\tablefootnote{Announced at \url{https://mistral.ai/news/codestral/}.} & Mistral AI & 22B & 2024.5 \\ 
    DeepSeek-Coder-V2-Lite \cite{DBLP:journals/corr/abs-2406-11931} & DeepSeek AI & 16B & 2024.6 \\ 
    CodeGeeX4-ALL \cite{DBLP:conf/kdd/ZhengXZDWXSW0LS23} & Zhipu AI \& THU & 9.4B & 2024.7 \\ 
    Qwen2.5-Coder \cite{hui2024qwen2} & Alibaba & 1.5/7B & 2024.9 \\
    \bottomrule
    \end{tabular}
    \label{tab:llm_list}
\vspace{-0.5em}
\end{table}

\begin{table}[]
    \centering
    \caption{Contextual information provided for each task in response generation.}
    \vspace{-1.0em}
    \begin{tabular}{c|c}
        \toprule
        Task & Contextual Information \\ 
        \midrule
        Code Translation & Original Code \\
        Code Summarization & Original Code \\
        Code Generation & Signature, Description, Dependencies \\
        \bottomrule
    \end{tabular}
    \label{tab:context}
\vspace{-1.0em}
\end{table}

As part of the prompt, contextual information in Table \ref{tab:context} is provided for LLMs. The full prompts are available in our repository \cite{replication-package}. LLMs are permitted to generate at most 3072 tokens, two times the maximum reference answer length, to minimize the need for truncation.

After preliminary experiments, we discover that many reference summaries in CodeXGLUE are in fact incorrect. Therefore, we require the reference summary to at least have 15 tokens, reselect the instructions, and manually examine each instruction. We also find that in code generation, selected LLMs struggle to generate interpretable code because they cannot use dependencies effectively, as the only available dependency information in ComplexCodeEval is their names, which makes human evaluation almost impossible to yield meaningful scores. Consequently, we reselect the instructions with at most 5 dependencies to reduce difficulty, and augment the dependency information with GPT-4o\footnote{We use the 2024-08-06 version for all experiments.}, prompting\footnote{The prompt for augmentation is available in our repository.} it to extract the signature from the reference answer\footnote{The authors of ComplexCodeEval extract dependency names from reference answers as well, and we follow their practice to utilize reference answers.} and generate a short description for each dependency. We manually examine the descriptions to ensure that no other information about the reference answers is included. Responses are generated with the updated dependency information as well as other contextual information.

\subsubsection{Manual Evaluation}
For manual evaluation, we design two evaluation aspects per task to guide human evaluators, enabling more fine-grained assessment without overwhelming evaluators with too many aspects and complicated criteria. The first aspect assesses the response's alignment with the instruction, e.g. Consistency with Code for summarization, requiring the summary to capture the code's core functionality. The second aspect judges the response's intrinsic quality, e.g. Readability \& Idiomatic Usage for translation, demanding the responded code to be both readable and follow common coding styles in the target language. We also curate the criteria for each integer score ranging from 1 to 5 for both aspects. In general, a 5-point response is near perfect, a 4- or 3-point response contains minor or major issues but still makes sense, and a 2- or 1-point response is practically useless. Below, we show an example of Readability aspect and its corresponding criteria for code summarization as an example, while the remaining aspects and criteria can be found in our repository:

\begin{tcolorbox}
\textbf{Readability:} How clear, concise, and fluent is the summary in describing the code’s function?

\textbf{- 5/5:} Extremely clear, concise, and well-structured; very easy to understand.

\textbf{- 4/5:} Mostly clear and concise, with minor readability issues.

\textbf{- 3/5:} Understandable but may contain some unclear or awkward phrasing.

\textbf{- 2/5:} Hard to follow due to unclear language or poor structure.

\textbf{- 1/5:} Very confusing, with significant language or structural issues.
\end{tcolorbox}

Two human evaluators with expertise in the chosen programming languages are involved in judging each of the 450 responses. During manual evaluation, we provide the corresponding instruction and the reference answer along with the response to be evaluated. Each evaluator is required to score both aspects before assigning an overall score, which is not necessarily the average of the former. The final human score for each response is the average of overall scores from two evaluators\footnote{The two human evaluators reach a high level of agreement, achieving Spearman's \(\rho\) of (83.07, 75.42, 74.20), Pearson's \(R\) of (85.86, 79.70, 73.74), and Kendall's \(\tau\) of (72.26, 63.40, 62.57) on code translation, generation, and summarization respectively.}. For pairwise comparison, we calculate the absolute difference between the final human scores of two responses in a pair, declaring a tie when the difference is smaller than 0.5\footnote{The value is chosen so that ties occur for about a third of the response pairs for each task.}, or deciding the higher-scored response is better otherwise. 

\subsection{Selected Methods}
\subsubsection{Conventional Metrics}
We choose five popular conventional metrics, each requiring the response \(tgt\) and the reference answer \(ref\) but not the instruction. We verify if these metrics align better or worse with human evaluation compared to LLM-as-a-judge methods.
% Some methods compute F\(_n\) scores as the weighted harmonic average of Recall and Precision, where Recall is weighted \(n^2\) times as Precision.
For details about Recall, Precision, and F\(_n\) scores, please refer to their original papers.

\textbf{BLEU} \cite{DBLP:conf/acl/PapineniRWZ02} calculates modified n-gram precision (\(n=1,2,3,4\)) for \(tgt\) and \(ref\), and applies a brevity penalty to penalize overly short responses.

\textbf{ROUGE-L} \cite{lin2004rouge} measures the length of the longest common subsequence \(\text{LCS}\) between \(tgt\) and \(ref\). It computes the F\(_1\) score based on LCS.

\textbf{METEOR} \cite{DBLP:conf/acl/BanerjeeL05} matches tokens in \(tgt\) and \(ref\), and computes the F\(_3\) score based on the number of matched tokens. It also penalizes fragmented alignment by counting the number of contiguous match chunks in \(tgt\).

\textbf{ChrF++} \cite{DBLP:conf/wmt/Popovic17} computes F\(_2\) scores using character n-grams (up to 6-grams) and token n-grams (up to 2-grams). The average character F\(_2\) score and the average token F\(_2\) score are then averaged to produce the final score.

\textbf{CrystalBLEU} \cite{DBLP:conf/kbse/EghbaliP22} is specifically designed to measure code similarity. It removes the most common n-grams in a corpus from \(tgt\) and \(ref\), as these trivial n-grams can obscure meaningful differences between them, before calculating BLEU score. For each task, we use the test set from its corresponding dataset as the corpus, including all instructions and reference answers.

We implement the first four methods with Hugging Face Evaluate, and the last with the CrystalBLEU package. For methods with replaceable tokenizers, we substitute them with OpenAI Tiktoken with o200k\_base vocabulary because the built-in tokenizers are usually not designed for code. 

\subsubsection{Embedding-based Methods}
We choose two methods based on embedding, i.e. token representations of the response \(tgt\) and the reference answer \(ref\). We use UniXcoder \cite{DBLP:conf/acl/GuoLDW0022} in place of BERT or other non-code LLMs as our encoder, due to its ability to process both code and text.

\textbf{BERTScore} \cite{DBLP:conf/iclr/ZhangKWWA20} calculates pairwise token similarity between \(tgt\) and \(ref\) with token representations, and obtains the average Recall and Precision, which are combined into the F\(_1\) score as the final score. BERTScore also applies inverse document frequencies (IDFs) as token weights.

\textbf{MoverScore} \cite{DBLP:conf/emnlp/ZhaoPLGME19} proposes to use Word Mover's Distance \cite{DBLP:conf/icml/KusnerSKW15}, measuring semantic dissimilarity as the minimum cost flow between n-gram representations, which is the IDF-weighted average of token representations. For each \(n\), it constructs a cost matrix for each n-gram in \(tgt\), and flow requirements based on IDF. The final score is the minimum cost to establish such a flow.

\subsubsection{Probability-based Methods}
We select two probability-based methods. These methods may take at least two of the following as input: instruction \(src\), response \(tgt\), and reference \(ref\), plus supplementary information like evaluation aspects\footnote{Aspects are identical to those in human evaluation.}. We use davinci-002 here, since later OpenAI models only return probabilities of newly generated tokens instead of provided tokens.

\textbf{GPTScore} \cite{DBLP:conf/naacl/FuNJ024} simply uses the sequence log probability \(\log p(tgt|src,a)\) as the score according to their paper, which is the average of all token log probabilities. However, their code instead uses the harmonic mean of \(\log p(tgt|ref,a)\) and \(\log p(ref|tgt,a)\). To mitigate this difference, we additionally include \(src\) in both conditions, i.e. using \(\log p(tgt|ref,src,a)\) and \(\log p(ref|tgt,src,a)\).

\textbf{FFLM} \cite{DBLP:conf/emnlp/0003RLZ23} is a reference-free metric that obtains both the prior probability \(P(tgt)\) and the posterior probability \(P(tgt|src)\). It claims that high-loss (low-probability) tokens contribute more to low-quality content, thus assigning a higher weight to them. FFLM also introduces the prefix probability \(P(tgt|tgt:src)\) by prepending \(tgt\) to \(src\), assuming that the prefix increases the generating probability if \(tgt\) is inconsistent with \(src\). These three probabilities are fused into the final score.

\subsubsection{Output-based Methods}
We select two methods: G-Eval and BatchEval, which apply different inference strategies, in addition to a control group (Vanilla) with no strategies applied, to assess if these strategies improve alignment with human evaluation for general-purpose LLMs. Unless otherwise stated, we use GPT-4o for these methods.

We also include a supervised fine-tuning (SFT) group, with two LLMs fine-tuned for NLP evaluation, along with their base LLMs without fine-tuning, to determine if fine-tuning for NLP evaluation also enhances human alignment in SE evaluation. 

We provide only the instruction \(src\), response \(tgt\), and evaluation aspects\footnote{Aspects are identical to those in human evaluation.} in the prompt. For the detailed prompts, please refer to our repository. Note that these methods can also perform pairwise comparison, where we include both responses in the prompt. 

\textbf{Vanilla} performs inference once with greedy decoding (temperature set to 0), where LLMs score each aspect first before assigning the final score. For pairwise comparison, LLMs compare on each aspect and then make the final decision. We use DeepSeek-Coder-V2-Lite locally, and DeepSeek-V2.5 and GPT-4o via API.

\textbf{G-Eval} \cite{DBLP:conf/emnlp/LiuIXWXZ23} requires the LLM to generate the evaluation steps first and embeds it into the prompt, followed by 20 inference passes with a high temperature of 1.0 and averaging the scores. Following their practice, we prompt the LLM to return the score first with a limit of 20 generated tokens. Judgments without a score are discarded. For pairwise comparison, we consider comparison results as scores of 1 or -1 when one response is better, or 0 for a tie. If the absolute value of the average score is less than 0.7\footnote{The value is chosen so that ties occur for about a third of the response pairs.}, we declare a draw, otherwise considering one response better.

\textbf{BatchEval} \cite{DBLP:conf/acl/YuanFLWPWH024a} performs multi-round scoring. In each round, it first batches all responses and then scores each batch in one inference pass. During batching, it diversifies the scores of responses in each batch, so that the LLM can learn an unbiased score distribution for more accurate scoring. We follow their practice by setting temperature to 0.2, batch size to 10, and number of rounds to 5.

\textbf{SFT} involves two LLMs fine-tuned for NLP evaluation, Auto-J \cite{DBLP:conf/iclr/LiSYF0024} and Prometheus-v2-BGB-8x7B \cite{DBLP:journals/corr/abs-2405-01535}, as well as their base LLMs Llama2-13B-Chat \cite{DBLP:journals/corr/abs-2307-09288} and Mixtral-8x7B-Instruct \cite{DBLP:journals/corr/abs-2401-04088}. We apply the default prompt template of each judge LLM to itself and its base LLM, and exclude the evaluation aspects for Auto-J and Llama-2-13B-Chat since Auto-J's template does not provide a place for aspects. We perform greedy decoding locally with temperature 0.

We only consider the final verdict (score or comparison result) in our meta-evaluation and discard the explanations. For verdict extraction, we use G-Eval's code for itself, and, for all other methods in this category, we set several rules to match with regular expressions, such as "Overall: X" and "[[X]]" where X is the non-negative final score, or comparison result "First", "Second", or "Draw". If no valid verdict is found or the extracted score exceeds 10, which we consider invalid, we assign a score of -1 or a comparison result as draw as a penalty.

\subsection{Meta-Evaluation}
Meta-evaluation refers to the process of evaluating different evaluation metrics. For the default method of individual scoring, we meta-evaluate the metrics via their correlation with human scores, including Spearman's \(\rho\), Pearson correlation coefficient \(R\), and Kendall's \(\tau\). For pairwise comparison in RQ3, we compute the Accuracy of LLM-generated labels, in addition to the Agreement which checks if an LLM makes the same judgment when two responses in the prompt swap their positions.

For the ease of reading, all correlation coefficients, Accuracies, and Agreements in this paper are multiplied by 100. We also check if the \(p\)-value of each correlation coefficient in RQ1 is smaller than 0.05 to ensure a 95\% confidence interval.
