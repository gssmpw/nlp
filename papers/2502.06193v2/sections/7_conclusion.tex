\section{Conclusion\label{conclusion}}
In this paper, we empirically investigate the effectiveness of different types of LLM-as-a-judge methods on three SE datasets. We generate and manually score LLM responses, and assess these methods' alignment with human scores. Our results indicate that these methods demonstrate task-dependent performance, ranging from near-human to unusable when scoring individual responses, and generally perform worse in pairwise comparisons. We further analyze score characteristics, discovering that the most human-aligning methods display a balanced human-like distribution. Finally, we discuss key findings and implications for future development and application of LLM-as-a-judge in SE evaluation, hoping that these insights can assist future research in this area.