\section{Discussion\label{discussion}}
\subsection{Case Study}
In Section \ref{result}, we only study the human alignment of scores from various LLM-as-a-judge methods. However, output-based methods usually generate explanations for scores as well. Thus, we study two cases of scoring explanations here. Both judgments are generated by GPT-4o from group Vanilla in the output-based category.

We present a passing case of code translation on the left side of Fig. \ref{fig:case_study}. In this example, the original C code implements a seeded custom pseudo-random function to generate random integers from 0 to 999999. However, the translated Python code simply invokes a similar function from the standard library without setting the seed, which clearly disregards the original purpose, and produces an incorrect range of \([0, 1000000]\). GPT-4o accurately identifies this discrepancy and highlights the failure to preserve the original intention, aligning with its moderate score of 3/5, although it overlooks the subtle difference in the range of possible random integers.

We also present a failing case of code summarization on the right side of Fig. \ref{fig:case_study}. Here, the responding LLM produces an overly detailed method summary, including excessive specifics such as identifier names and internal procedures, contrary to the instruction to summarize functionality without extensive details. Despite this, GPT-4o assigns a perfect score of 5/5, viewing the entire step-by-step explanation as essential information, while an ideal summary can be as concise as "The method iterates over a directory stream and filters out non-directory files." This case aligns with LLMs' verbosity bias introduced in related work \cite{DBLP:journals/corr/abs-2310-10076}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{attachments/case_study.pdf}
    \vspace{-2.0em}
    \caption{Case study. The successful case from code translation is on the left while the failing case from code summarization is on the right.}
    \label{fig:case_study}
    \vspace{-1.0em}
\end{figure}

\subsection{Implications of Findings}

\textbf{For developers:} Our findings indicate the potential of LLM-as-a-judge methods to replace human evaluators, to effectively evaluate the quality of LLM-generated content in certain SE tasks and save developers' time on selecting the best LLMs. We further conclude the following insights:

\begin{enumerate}

    \item Developers should carefully select LLM-as-a-judge methods, as their performance varies significantly across different categories of methods.

    \begin{itemize}
    
        \item Output-based methods with large LLMs like GPT-4o or DeepSeek-V2.5 offer the most human-aligning evaluation with proper prompt and inference strategies. 
        
        \item Individual scoring should be preferred over pairwise comparison with current methods.

    \end{itemize}
    
    \item For different tasks, developers should leverage LLM-as-a-judge methods in diverse ways to exploit their strengths, as their performance is highly task-dependent:
    
    \begin{itemize}
    
        \item For code translation and generation, state-of-the-art methods demonstrate mid-to-strong performance and can be used standalone, particularly when reference answers are unavailable or scoring explanations are required.

        \item For code summarization, LLM-as-a-judge methods should not be used directly or alone due to their insufficient alignment with human judgments. However, with carefully designed prompts to mitigate common LLM biases, they can still serve as a valuable complement to conventional metrics.
        
    \end{itemize}
    
\end{enumerate}

%\textbf{For industry:} 
% \textbf{\gsz{For developers:}} \gsz{Our study reveals that} current state-of-the-art LLM-as-a-judge methods can replace human evaluators \gsz{in some  software engineering tasks such as} %for 
% code translation and code generation, especially under reference-free contexts. Among all methods, output-based methods with large LLMs offer the best human-aligning evaluation with proper prompt and inference strategies when instructed to score individual responses.

% Rationale: Our experiments indicate high correlation with human scores for output-based methods, which also provide explanations for scores and do not need access to LLM internal states. Besides, according to RQ2, they offer more human-like score distributions with large LLMs than small ones. We thus recommend output-based methods with large LLMs like GPT-4o or DeepSeek-V2.5. On the contrary, embedding-based and probability-based methods show low human correlation, in addition to the lack of score explanations and the requirement to final hidden states or generating probabilities. Consequently, we consider them as outdated and recommend against using them for real-world SE applications. Furthermore, as RQ3 shows the immaturity of existing methods on pairwise comparison on SE tasks, individual scoring should be preferred over pairwise comparison.

\textbf{For researchers:} Our study reveals the effectiveness and limitations of LLM-as-a-judge methods in SE tasks and shows some potential future directions, specifically:

\begin{enumerate}

    \item Current methods lack generalizability across SE tasks, as evidenced by their task-dependent performance:
    
    \begin{itemize}
    
        \item While SFT methods for evaluation exist, their performance on SE tasks is likely limited by the absence of challenging SE-specific data in training sets. Future research could benefit from curating difficult, SE-specific human preference datasets for fine-tuning smaller LLMs. Instructions in these datasets can originate from challenging benchmarks or even complicated real-world scenarios. It is also important to design strategies for state-of-the-art LLMs to generate responses and human-like judgments.

        \item Non-SFT methods use uniform prompt formats and inference strategies across tasks, without task-specific adaptations or utilizing code-specific features or structures. We therefore propose that designing SE- or even task-specific evaluation methods may yield more accurate and robust results than general-purpose evaluation frameworks.
        
    \end{itemize}

    \item There are still gaps to be bridged between LLM and human evaluators:

    \begin{itemize}
    
        \item During evaluation, LLMs typically rely solely on the predefined evaluation criteria. In contrast, human evaluators can compare multiple responses, implicitly identifying common strengths and weaknesses to streamline the evaluation process. To bridge this gap, researchers could enhance LLM-as-a-judge methods by asking LLMs to summarize insights from previous evaluation sessions. These insights could be included in the prompt to provide additional evaluation context. Multiple responses in the prompt are also valuable for LLMs to make comparisons.

        \item Human evaluators often discuss and reach a consensus, whereas LLM-as-a-judge frameworks typically contain a single LLM instance. To bridge this gap, researchers could develop multi-agent evaluation systems, where multiple LLM instances evaluate responses from different perspectives. This approach would enable more comprehensive and nuanced evaluations, akin to collaborative human judgment.

        \item The underperformance of LLM-as-a-judge in evaluating code summaries reveals a critical misalignment between benchmark task definitions and LLM interpretations. In our experiments, CodeXGLUE expects concise, docstring-style summaries while LLMs default to detailed explanations due to their verbosity bias. To improve evaluation reliability, researchers should mitigate LLMs' implicit, bias-influenced assumptions about the task to ensure they correctly understand task objectives.
        
    \end{itemize}
\end{enumerate}


%It is an interesting future direction to study SE-specific or even task-specific LLM-as-a-judge methods, particularly human preference SE datasets for LLM fine-tuning, and designing specific prompts and inference strategies for SE evaluation.

%Rationale: First, while LLMs fine-tuned specifically for NLP evaluation exist, our experiments reveal their limited performance on common SE tasks, likely due to the absence of challenging SE-specific data in their training sets. Future research could benefit from curating difficult, SE-specific human evaluation datasets for fine-tuning, which can potentially produce specialized small LLMs for SE evaluation. Additionally, for non-fine-tuned methods, our findings highlight that existing methods lack generalizability across SE tasks, as evidenced by their task-dependent performance. These methods use uniform prompt formats and inference strategies across tasks, without task-specific adaptations or utilizing code-specific features or structures. We therefore propose that designing SE- or task-specific evaluation methods may yield more accurate and robust results than general-purpose evaluation frameworks.
