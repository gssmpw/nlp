\section{\label{overview}LLM-as-a-judge Framework Overview}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{attachments/images.pdf}
    \vspace{-2.0em}
    \caption{Overview of different LLM-as-a-judge methods.}
    \label{fig:approach}
\vspace{-1.0em}
\end{figure}

In this section, we offer an overview of existing LLM-as-a-judge methods. As seen in Fig. \ref{fig:approach}, we categorize these methods based on the types of LLM features used\footnote{Our categorization is inspired by \cite{DBLP:journals/corr/abs-2402-01383}. We merge similar categories based on LLM feature types.}, including embedding-based, probability-based, and output-based methods. We denote the instruction (source) as \(src=s_1...s_{|src|}\), the response (target) as \(tgt=t_1...t_{|tgt|}\), and the reference answer as \(ref=r_1...r_{|ref|}\).
\begin{itemize}
    \item \textbf{Embedding-based}: These methods first obtain token representations of the response and reference answer \(f(tgt)=f(t_1)...f(t_{|tgt|})\) and \(f(ref)=f(r_1)...f(r_{|ref|})\) from the LLM encoder \(f\). We then evaluate \(tgt\) %compare the two sequences of representations 
    via fusing token-wise cosine similarities \(s_{ij}=\frac{f(t_i)\cdot f(r_j)}{\lVert f(t_i)\rVert\lVert f(r_j)\rVert}\). %, or much more complicated procedures.

    \item \textbf{Probability-based}: The LLM receives an input-output pair \((in, out)\), and returns the conditional log-probability of generating \(out\), i.e. \(\log p(out|in)=\frac 1{|out|}\sum_{k=1}^{|out|}\log p(out_k|in,out_{<k})\). Typical \((in, out)\) combinations include \((src,tgt)\), \((ref,tgt)\), \((tgt,ref)\), and \((none,tgt)\), where \(none\) means no input is provided. We then score \(tgt\) with these log-probabilities. Additional content may be present in the prompt, such as evaluation aspects like clarity.

    \item \textbf{Output-based}: These methods first craft a prompt \(prompt\) with \(src\) and \(tgt\). Depending on the design, \(prompt\) may also feature \(ref\), evaluation aspects and criteria, and evaluation steps. After obtaining the judgment \(jud=\text{LLM}(prompt)\), we extract the final score from \(jud\). Many prompting and inference strategies can also be applied, such as multi-agent and repeated sampling, where multiple scores are often combined using methods like a majority vote or averaging.

    \quad LLMs can also be fine-tuned as specialized judges, usually applied with a single inference pass and no additional strategies. State-of-the-art LLMs like GPT-4 are often used to generate the reference judgment for training. In this paper, we discuss the performance of these LLMs, instead of focusing on the detailed training process.

    \quad Unlike embedding-based and probability-based methods, which usually have scoring ranges of \([0, 1]\) and \((-\infty, 0)\) (or \((-\infty, \infty)\)) respectively without rescaling, most output-based methods require LLMs to score on a scale of 1 to 5 or 1 to 10. They can also compare two responses and decide the better one or declare a tie. In our study, we investigate individual scoring in RQ1 and RQ2, and pairwise comparison in RQ3.
\end{itemize}

