\section{Background and Related Work}
\label{section_related_work}

    %The hyper-prior can be uni-variate Gaussian distribution **Kingma, "Variational Dropout"**, mean and scale Gaussian distribution **Bishop, "Pattern Recognition and Machine Learning"**, and Gaussian mixture model **Reynolds, "Gaussian Mixture Models"** for more expressive distribution modeling.
%    After that, Minnen \emph{et al.} **Minnen et al., "Neural Image Compression"** proposed a autoregressive entropy model for more accurate density estimation.
    %various context models have been proposed for more accurate density estimation, including spatial context models, channel context models,
%    and m**Ballé et al., "Density Estimation with Neural Networks"**

    \subsection{Lossy Image Compression}\label{subsec:lossy-image-compression}

    % In specific, 
    % The sender encodes image $\x$ into compact (ideally decorrelated) transform coefficients $\y$ with an \emph{analysis transform} $g_a$, $\y = g_a(\x)$.
    % These coefficients are scalar quantized in element-wise to obtain a discretized representation $\haty = Q(\y)$.
    % With an entropy model $p_{\haty}$, we then convert discrete symbols $\haty$ to a bitstring with the expected length of $-\mathbb{E}[\log p_{\haty}(\haty)]$.
    % The receiver finally recovers $\haty$ losslessly and reconstructs the original image with a \emph{synthesis transform} $g_s$, $\hatx = g_s(\haty)$.
    Lossy image codecs are primarily based on the paradigm of \emph{transform coding}**Ahmed, "Discrete Cosine Transform"**, whose encoding process consists of three steps: decorrelation, quantization, and entropy coding.
    In principle, they aim to search for the \emph{optimal compact representation} of the input source in a computationally feasible way that leads to the best rate-distortion (RD) performance**Girod, "Rate-Distortion Theory"** defined as
    \begin{equation}
    \label{RDO}
    J = R + \lambda D,
    \end{equation}
    where $\lambda$ denotes the Lagrange multiplier that controls the desired compression trade-off between the rate and distortion. The bit rate term $R$ represents the average number of bits needed to encode the input data, and the distortion term $D$ assesses the similarity between the input and its reconstruction.

    \subsubsection{Classical Image Codecs}

    In conventional image codecs such as JPEG, JPEG2000, BPG**Bjontegaard et al., "JPEG 2000"**, and VVC intra**Wiegand et al., "H.264/AVC"**, the transforms are typically \emph{linear and invertible} such as discrete cosine transform (DCT) and discrete wavelet transform (DWT). To improve RD performance, these \emph{linear transform coding} schemes often employ continuously expanded available coding modes to search the best predictive manner for reducing spatial redundancies, whose transform, quantizer, and entropy code are separately optimized through manual parameter adjustment.

    \subsubsection{Neural Image Codecs}
    Most neural image codecs (NICs) are based on \emph{nonlinear transform coding}**Effenberger et al., "Neural Image Compression"**, which employs deep neural networks (DNNs) to implement various components and learns them end-to-end on the data of interest.
    
	In general, a NIC encodes an image $\x$ into compact (ideally decorrelated) transform coefficients $\y$ using an \emph{analysis transform} $g_a$. These coefficients are then scalar quantized element-wise to obtain a discretized representation $\haty = Q(\y)$. With an entropy model $p_{\haty}$, the discrete symbols $\haty$ are converted into a bitstring with the expected length $-\mathbb{E}[\log p_{\haty}(\haty)]$ via an entropy encoder. 
    Under the reliable transmission promise, the receiver recovers $\haty$ losslessly and reconstructs the original image using a \emph{synthesis transform} $g_s$.	
    
    State-of-the-art NICs**Balle et al., "Lossy Image Compression"** are already surpassing the advanced traditional method VVC, mainly due to more efficient nonlinear transforms and expressive entropy models. 
    Specifically, the efficient nonlinear transform blocks explored in existing works include residual blocks**Tai et al., "Image Super-Resolution"**, vision transformers**Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"**, and their combinations**Li et al., "Image Compression with Neural Networks"**.
    As for the entropy models, Ballé \emph{et al.} **Ballé et al., "Density Estimation with Neural Networks"** first proposed a non-adaptive, fully-factorized entropy model to approximate $p_{\haty}$, later extending it to the hierarchical form**Balle et al., "Lossy Image Compression"**.
    To achieve more accurate and efficient density estimation, advanced NICs encode quantized latents with $L$ groups, and factorize the density model to be a joint hierarchical and group-based auto-regressive form: $\prod_{i=1}^{L} p(\haty_{i} | \haty_{<i}, \hat{\bm{z}})$ **Ballé et al., "Density Estimation with Neural Networks"**, where $\hat{\bm{z}}$ denotes the hyperprior **Kingma, "Variational Dropout"**. 
    Various contextual dependencies have been explored, including spatial context models**Ballé et al., "Lossy Image Compression"**, channel-wise models**Balle et al., "Density Estimation with Neural Networks"**, and hybrid spatial-channel models**Effenberger et al., "Neural Image Compression"**.
	
    However, existing NICs, primarily designed for source compression, produce fragile bitstreams that are susceptible to perturbations, such as bit errors**Bjontegaard et al., "JPEG 2000"**, which hinder its applications to RTC scenarios. 
%    {\color{blue}
%   	Without loss of generality, their encoded bitstream can be divided into $L$ groups 
%    	 enhances compression efficiency, it also exacerbates error propagation since errors in conditioning latents can influence all dependent latents.
%    }
    In this work, we investigate how to elevate the resilience of NICs for packet losses.

    \subsection{Loss-Resilient Techniques}\label{subsec:loss-resilience-techniques}
    To transmit data packets over unreliable networks, various techniques have been developed**Wiegand et al., "H.264/AVC"** to detect, correct or conceal errors, but they also intentionally make the source coder less efficient than it can be.
    The most representative schemes including forward error correction (FEC) and packet error concealment (PLC).

    \subsubsection{Forward error correction (FEC)}
    FEC is a basic method used to protect compressed bitstreams from transmission errors.
    It works by adding redundancy to the data at the sender's side, either at the application or transport layer.
    Specifically, FEC encodes $N_{k}$ data packets and adds $N_{r}$ parity packets, so that the original data packets can be recovered if any subset of $N_{k} (1 + \rho)$ packets out of the total $(N_{k} + N_{r})$ packets are received **Massey et al., "Error-Correcting Codes"**.
    Here $\rho$ denotes the proportion of additional redundancy packets required for reconstruction, and an ideal FEC code requires no decoder overhead, i.e., $\rho = 0$.
%    Specifically, FEC encodes $N_{k}$ data packets and adds $N_{r}$ parity packets, so that the original data packets can be recovered if any subset of $N_{k}$ packets out of the total $(N_{k} + N_{r})$ packets are received **Massey et al., "Error-Correcting Codes"**
    But in the context of resilient neural speech coding jointly design feature-domain PLC with the source compression**Balle et al., "Density Estimation with Neural Networks"**.
    But in the context of resilient image coding, such a paradigm shift remains to be studied.
    This paper aims to bridge this gap, exploring the potential of addressing packet losses with feature-domain PLC in a neural image coding framework.