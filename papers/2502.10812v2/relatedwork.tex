\section{Background and Related Work}
\label{section_related_work}

    %The hyper-prior can be uni-variate Gaussian distribution \cite{balle2018}, mean and scale Gaussian distribution \cite{minnen2018}, and Gaussian mixture model \cite{liu2023learned} for more expressive distribution modeling.
%    After that, Minnen \emph{et al.} \cite{minnen2018} proposed a autoregressive entropy model for more accurate density estimation.
    %various context models have been proposed for more accurate density estimation, including spatial context models, channel context models,
%    and m~\cite{minnen2018, he2021checkerboard, guo2021causal, he2022elic, lu2022high, liu2023learned}

    \subsection{Lossy Image Compression}\label{subsec:lossy-image-compression}

    % In specific, 
    % The sender encodes image $\x$ into compact (ideally decorrelated) transform coefficients $\y$ with an \emph{analysis transform} $g_a$, $\y = g_a(\x)$.
    % These coefficients are scalar quantized in element-wise to obtain a discretized representation $\haty = Q(\y)$.
    % With an entropy model $p_{\haty}$, we then convert discrete symbols $\haty$ to a bitstring with the expected length of $-\mathbb{E}[\log p_{\haty}(\haty)]$.
    % The receiver finally recovers $\haty$ losslessly and reconstructs the original image with a \emph{synthesis transform} $g_s$, $\hatx = g_s(\haty)$.
    Lossy image codecs are primarily based on the paradigm of \emph{transform coding}~\cite{trans_coding}, whose encoding process consists of three steps: decorrelation, quantization, and entropy coding.
    In principle, they aim to search for the \emph{optimal compact representation} of the input source in a computationally feasible way that leads to the best rate-distortion (RD) performance~\cite{rd_theory} defined as
    \begin{equation}
    \label{RDO}
    J = R + \lambda D,
    \end{equation}
    where $\lambda$ denotes the Lagrange multiplier that controls the desired compression trade-off between the rate and distortion. The bit rate term $R$ represents the average number of bits needed to encode the input data, and the distortion term $D$ assesses the similarity between the input and its reconstruction.

    \subsubsection{Classical Image Codecs}

    In conventional image codecs such as JPEG, JPEG2000, BPG\cite{BPG}, and VVC intra\cite{VVC_intra}, the transforms are typically \emph{linear and invertible} such as discrete cosine transform (DCT) and discrete wavelet transform (DWT). To improve RD performance, these \emph{linear transform coding} schemes often employ continuously expanded available coding modes to search the best predictive manner for reducing spatial redundancies, whose transform, quantizer, and entropy code are separately optimized through manual parameter adjustment.

    \subsubsection{Neural Image Codecs}
    Most neural image codecs (NICs) are based on \emph{nonlinear transform coding}~\cite{balle2020nonlinear}, which employs deep neural networks (DNNs) to implement various components and learns them end-to-end on the data of interest.
    
	In general, a NIC encodes an image $\x$ into compact (ideally decorrelated) transform coefficients $\y$ using an \emph{analysis transform} $g_a$. These coefficients are then scalar quantized element-wise to obtain a discretized representation $\haty = Q(\y)$. With an entropy model $p_{\haty}$, the discrete symbols $\haty$ are converted into a bitstring with the expected length $-\mathbb{E}[\log p_{\haty}(\haty)]$ via an entropy encoder. 
    Under the reliable transmission promise, the receiver recovers $\haty$ losslessly and reconstructs the original image using a \emph{synthesis transform} $g_s$.	
    
    State-of-the-art NICs~\cite{cheng2020learned, he2022elic, zou2022devil, lu2022high} are already surpassing the advanced traditional method VVC, mainly due to more efficient nonlinear transforms and expressive entropy models. 
    Specifically, the efficient nonlinear transform blocks explored in existing works include residual blocks~\cite{cheng2020learned, he2022elic}, vision transformers~\cite{zou2022devil}, and their combinations~\cite{lu2022high, liu2023learned, jiang2023mlic}.
    As for the entropy models, Ball√© \emph{et al.} \cite{balle2016} first proposed a non-adaptive, fully-factorized entropy model to approximate $p_{\haty}$, later extending it to the hierarchical form~\cite{balle2018}.
    To achieve more accurate and efficient density estimation, advanced NICs encode quantized latents with $L$ groups, and factorize the density model to be a joint hierarchical and group-based auto-regressive form: $\prod_{i=1}^{L} p(\haty_{i} | \haty_{<i}, \hat{\bm{z}})$ \cite{minnen2018, lu2022high, he2022elic}, where $\hat{\bm{z}}$ denotes the hyperprior \cite{balle2018}. 
    Various contextual dependencies have been explored, including spatial context models~\cite{minnen2018, he2021checkerboard, mentzer2023m2t}, channel-wise models~\cite{minnen2020channel}, and hybrid spatial-channel models~\cite{he2022elic, lu2022high}.
	
    However, existing NICs, primarily designed for source compression, produce fragile bitstreams that are susceptible to perturbations, such as bit errors~\cite{balle2018integer, tian2023towards}, which hinder its applications to RTC scenarios. 
%    {\color{blue}
%   	Without loss of generality, their encoded bitstream can be divided into $L$ groups 
%    	 enhances compression efficiency, it also exacerbates error propagation since errors in conditioning latents can influence all dependent latents.
%    }
    In this work, we investigate how to elevate the resilience of NICs for packet losses.

    \subsection{Loss-Resilient Techniques}\label{subsec:loss-resilience-techniques}
    To transmit data packets over unreliable networks, various techniques have been developed~\cite{wang2000error} to detect, correct or conceal errors, but they also intentionally make the source coder less efficient than it can be.
    The most representative schemes including forward error correction (FEC) and packet error concealment (PLC).

    \subsubsection{Forward error correction (FEC)}
    FEC is a basic method used to protect compressed bitstreams from transmission errors.
    It works by adding redundancy to the data at the sender's side, either at the application or transport layer.
    Specifically, FEC encodes $N_{k}$ data packets and adds $N_{r}$ parity packets, so that the original data packets can be recovered if any subset of $N_{k} (1 + \rho)$ packets out of the total $(N_{k} + N_{r})$ packets are received \cite{4427233}.
    Here $\rho$ denotes the proportion of additional redundancy packets required for reconstruction, and an ideal FEC code requires no decoder overhead, i.e., $\rho = 0$.
%    Specifically, FEC encodes $N_{k}$ data packets and adds $N_{r}$ parity packets, so that the original data packets can be recovered if any subset of $N_{k}$ packets out of the total $(N_{k} + N_{r})$ packets are received.
    Common error-resilient channel coding methods include Reed-Solomon codes, low-density parity check (LDPC) codes, fountain or rateless codes, etc.
    However, the significant challenge of FEC is to decide the right number of parity packets.
    It is tricky since the exact number of loss packets $N_{l}$, can never be known in advance.
    When $N_{l} > N_{r}$, the redundancy will be insufficient to recover lost packets, decoding fails.
    When $N_{l} < N_{r}$, the bandwidth consumed by transmitting extra $N_{r} - N_{l}$ parity packets will be wasted.
    As a result, it must adjust the redundancy based on estimation of instant link quality.
    This approach is highly inefficient to achieve a satisfactory balance between resilience and efficiency, as the adjustment always lags behind the network condition changes.

    \subsubsection{Packet loss concealment (PLC)}
    PLC techniques aim to restore the missing or delayed packets at the receiver side.
    In general, it requires the encoder to limit the extent of error propagation by splitting the data into several segments.
    Then, PLC encoder removes various redundancies (e.g., spatial, temporal, and statistical) only within each segment, which prevent the error segment from effecting other segments.
    When some packets are lost, the decoder estimates missing data in lost packets based on the received packets, using the correlations between segments in the pixel domain.
    Most classical PLC studies are tightly coupled with video coding standards, which inpaint the lost area using spatially and temporally surrounding motion vectors (MVs), neighboring pixels, or other available side information.
    %While these ideas were impact and valuable, most of them are outdated or not effective yet.
    Recent advances come from the learning-based image/video completion methods, which generate expressive and realistic results exploiting high-level image features.
    %However, their primary role remains as a post-processing tool to repair reconstructions corrupted by packet loss [x].
    However, these codec-agnostic PLC tools are highly dependent on the codec's output, which limits their full potential.
    Although they are valuable at the post-processing stage of classical codecs, these pixel-domain PLC tools exhibit limited compatibility with neural codecs, since damaged latents can spread across the entire image and degrade the overall reconstruction quality.
    As a result, recent works in resilient neural speech coding jointly design feature-domain PLC with the source compression~\cite{msra_plc}.
    But in the context of resilient image coding, such a paradigm shift remains to be studied.
    This paper aims to bridge this gap, exploring the potential of addressing packet losses with feature-domain PLC in a neural image coding framework.

%\subsubsection{Layered coding with unequal error protection (UEP)}
%Layered coding, or scalable coding, refers to encode data into scalable bitstreams including multiple quality layers (typically a base layer and several enhancement layers) and sends data layer by layer. The base layer provides a low but acceptable level of quality, and each additional enhancement layer will incrementally improve the quality.