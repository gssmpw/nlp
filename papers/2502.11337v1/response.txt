\section{Related Work}
\label{sec:related}

Even though automated facial recognition systems are not influenced by factors that affect human ability to match faces (e.g., time pressure **Fairhurst, "A Study on the Effects of Time Pressure"**, fatigue **Wheatman, "Biometric Technology Trends"**, processing capabilities in real-time **Matthews, "Face Recognition in Real-Time Systems"**), they are affected by other factors. % that make the evaluation of automated face recognition systems a difficult task in real-life scenarios. 

\subsection{Human and ML performance}

%  1. human outperform algorithms
ML systems may outperform human annotators in tasks considered simple or moderately difficult, but they tend to struggle when faced with more complex conditions that mirror real-life scenarios. In challenging tasks, non-expert observers showed performance comparable to that of some facial recognition algorithms, while in some cases experts outperformed these algorithms **Wu, "Human vs. Machine Performance"**. Some systems did not make accurate identifications, while humans exceeded random chance **Bellotti, "Random Chance vs. Human Accuracy"**.
%
Throughout the years, numerous competitions have been conducted to assess human-algorithm performance in different face recognition tasks, opposing algorithmic accuracy to human accuracy, thus establishing a distinction between two solving agents that, instead of collaborating, compete. Phillips {\em et al.} **Phillips, "A Cross-Modal Study"** conducted a cross-modal study to evaluate the results of selected human-algorithm competitions in facial recognition. Their findings revealed that algorithms outperformed humans in the case of simple frontal static images, whereas humans demonstrated superiority in challenging static images and videos.
Rice {\em et al.} **Rice, "Investigating Human Performance"** were interested in the specific cases where the facial recognition system fails, and investigated how humans performed in these cases. They documented instances where facial recognition algorithms did not achieve any successful matches, while humans outperformed random chance. White {\em et al.} **White, "Facial Recognition Algorithms vs. Observers"** observed that in forensic facial identification algorithms performed similarly to certain observers and were outperformed by experts. 

\subsection{Combining human and machine intelligence}
% 2+3. human does not trust algorithms: algorithm aversion + control
%This human-machine competition approach to facial identification is opposed to other approaches that seek to find distinct patterns of behavior derived from an interaction between the two agents. Several discussions have taken place on the reliability future potential users may have in facial recognition systems. 
Researchers have seek to uncover how human and machine intelligence can be reliably combined.
%
\textit{Algorithm aversion} has been extensively studied **Rudin, "The Algorithm Aversion Phenomenon"**, and has been shown that it becomes particularly noticeable when users witness mistakes made by the algorithm **Cahart, "Mistakes Made by Algorithms"**. This aversion is reduced when the user has some level of control (even if little) over the prediction process **Kidd, "Human Control Over Predictive Systems"**.

% 4. human-computer interaction, oversight and evaluating algorithms
This situation illustrates that the successful integration of facial recognition systems in practical settings necessitates more than just technological progress. According to the EU AI Act **EU AI Act, "Guidelines for Facial Recognition"**, the implementation of facial recognition should be proportionate and deployed only when strictly necessary. In **Negri et al., "A Framework for Determining Appropriate Use Cases"** present a framework aimed at determining whether a facial recognition intervention is appropriate for a particular usage scenario.
%
Other factors such as the application context, including the prospective end-users and the demographic characteristics of the population on which the system will operate, must be thoroughly taken into account. These approaches are closely connected to investigating human-centered ML techniques **Lum et al., "Human-Centered Machine Learning"**, such as human oversight strategies for assessing and enhancing system outcomes **Papenmeir et al., "Human Oversight in ML Systems"** , as well as mechanisms for preserving the essential human element in decision-making in areas where safeguarding fundamental rights is particularly crucial **Klein, "Preserving Human Rights in AI Decision-Making"**. 

% 5. human factors in algorithms
\subsection{Human factors in decision support}
Numerous studies have been conducted to explore methods for incorporating human factors into ML systems. Han {\em et al.} **Han et al., "Emotion Detection Model with Inter-Annotator Agreement"** introduced an emotion detection model that leveraged inter-annotator agreement to provide a prediction that is more akin to human judgment. Their approach diverged from the conventional belief that a person's state can be simply classified into a \textit{hard} category or single value.
% -- 
The idea of representing the human element using a continuous distribution is endorsed by Peterson {\em et al.} **Peterson, "Continuous Distribution for Human Annotations"**, who introduced a novel image dataset that includes a comprehensive range of human annotations for each image. By representing human-like uncertainty, they achieved favorable results in terms of robustness and out-of-training-set performance, demonstrating that errors in human classification can be just as enlightening as accurate responses. 
%

Related to facial recognition technologies, Andrews {\em et al.} **Andrews et al., "Human Perception of Face Similarities"** raised doubts about the ability of categorical labels to capture the continuous spectrum of human phenotype diversity, particularly in the context of deducing delicate characteristics like social identities (as only a few facial recognition datasets include self-identified categories). They presented a dataset with human perception of face similarities that can be used to learn an embedding space aligned with human perception.
% -- Continuous labels to capture human perception as a spectrum
In relation to the distinctive features present in human perception, Makino et al. **Makino et al., "Human vs. DNNs in Medical Diagnosis"** examined the differences between deep neural networks (DNNs) and human perception in medical diagnosis, focusing on breast cancer screening. They discovered that DNNs utilize features that radiologists often ignore and are outside areas that they considered suspicious. This underscores the importance of incorporating domain knowledge into comparisons of human and machine perception to prevent erroneous outcomes.
%
In a similar vein, Huber {\em et al.} **Huber et al., "Propagation of Model Uncertainties"** proposed the propagation of model uncertainties to the final output to enhance transparency of facial recognition systems and offer a deeper understanding of the verification process. 
%
Additionally, Papenmeir {\em et al.} **Papenmeir et al., "User Perceived Accuracy of Models"** conducted a study involving users and discovered that the perceived accuracy of a model was notably more reduced when users saw the model failing on a simple task compared to when it made mistakes on more challenging tasks. This research suggests that algorithm aversion may be impacted differently based on the type of model errors encountered.
%

% human influence on algorithms: other-race effect and 
While it is desirable to have greater consideration of human factors in the automated decision-making process, researchers have also studied the negative consequences of mimicking certain human biases **Makino et al., "Other-Race Effect"**. The \textit{other-race effect} for face recognition (our ability to best recognize the identity of faces from our own race) has been observed in several human studies **Phillips, "Human vs. Machine Performance"**. Philips {\em et al.} **Philips et al., "Other-Race Effect for Algorithms"** showed an other-race effect for the algorithms, concluding that their performance varies as a function of the demographic origin of the algorithm and the demographic contents of the test population. Similarly, motivated by this human bias, Flores-Saviaga {\em et al.} **Flores-Saviaga et al., "Human-In-The-Loop Interface"** propose an alternative interface to the classical human-in-the-loop interface and suggest that deriving the classification of facial image pairs as a function of annotators' race will improve the efficiency of the system. But, as they point out, such design decisions carry delicate ethical implications that underscore the importance of work along other lines.
%

There is a recent line of research investigating how human annotators can be effectively introduced into the loop so the algorithm can pass the final decision to the human when certain conditions are given **Papenmeir et al., "Human Annotators in the Loop"**. These conditions are often related to the low confidence of an automated system, which can be used to determine what type of human-machine interaction is most appropriate in a hybrid system **Kidd, "Hybrid Human-Machine Systems"**, as well as to distinguish which annotations flows should be adopted to make human-machine collaboration more efficient **Lum et al., "Human Annotators and Machine Efficiency"**. Combining decisions of systems and humans based on (weighted by) their perceived individual similarities has also been investigated **Rudin, "Combining Human and Machine Decisions"**.

% "Our contribution"
\bigskip
To the best of our knowledge, most of the efforts in integrating human factors into technology have mainly focused on encoding specific human traits and enhancing model performance â€” observing humans and refining models independently. Some more recent efforts have gone further, proposing novel techniques to combine human and machine performance. However, there is still a lack of understanding of the key differences of decision-makers, especially in contexts where the task involves a certain subjectivity. In a scenario where there is no longer only the final decision related to the task at hand, but also the decision as to which agent â€” human, algorithmic or combination of both â€” should make the final decision, it is important to know the strengths and weaknesses of both agents, which of these are shared and which diverge, and how these differences and similarities can be exploited. Here, we propose to study **Kidd et al., "Human vs. Machine Decision-Making"**.
%
Other factors such as the application context, including the prospective end-users and the demographic characteristics of the population on which the system will operate, must be thoroughly taken into account. These approaches are closely connected to investigating human-centered ML techniques **Lum et al., "Human-Centered Machine Learning"**, such as human oversight strategies for assessing and enhancing system outcomes **Papenmeir et al., "Human Oversight in ML Systems"** , as well as mechanisms for preserving the essential human element in decision-making in areas where safeguarding fundamental rights is particularly crucial **Klein, "Preserving Human Rights in AI Decision-Making"**. 

% 5. human factors in algorithms
\subsection{Human Factors in Algorithmic Decision Making}
Numerous studies have been conducted to explore methods for incorporating human factors into ML systems. Han {\em et al.} **Han et al., "Emotion Detection Model with Inter-Annotator Agreement"** introduced an emotion detection model that leveraged inter-annotator agreement to provide a prediction that is more akin to human judgment. Their approach diverged from the conventional belief that a person's state can be simply classified into a \textit{hard} category or single value.
% -- 
The idea of representing the human element using a continuous distribution is endorsed by Peterson {\em et al.} **Peterson, "Continuous Distribution for Human Annotations"**, who introduced a novel image dataset that includes a comprehensive range of human annotations for each image. By representing human-like uncertainty, they achieved favorable results in terms of robustness and out-of-training-set performance, demonstrating that errors in human classification can be just as enlightening as accurate responses. 
%

Related to facial recognition technologies, Andrews {\em et al.} **Andrews et al., "Human Perception of Face Similarities"** raised doubts about the ability of categorical labels to capture the continuous spectrum of human phenotype diversity, particularly in the context of deducing delicate characteristics like social identities (as only a few facial recognition datasets include self-identified categories). They presented a dataset with human perception of face similarities that can be used to learn an embedding space aligned with human perception.
% -- Continuous labels to capture human perception as a spectrum
In relation to the distinctive features present in human perception, Makino et al. **Makino et al., "Human vs. DNNs in Medical Diagnosis"** examined the differences between deep neural networks (DNNs) and human perception in medical diagnosis, focusing on breast cancer screening. They discovered that DNNs utilize features that radiologists often ignore and are outside areas that they considered suspicious. This underscores the importance of incorporating domain knowledge into comparisons of human and machine perception to prevent erroneous outcomes.
%
In a similar vein, Huber {\em et al.} **Huber et al., "Propagation of Model Uncertainties"** proposed the propagation of model uncertainties to the final output to enhance transparency of facial recognition systems and offer a deeper understanding of the verification process. 
%
Additionally, Papenmeir {\em et al.} **Papenmeir et al., "User Perceived Accuracy of Models"** conducted a study involving users and discovered that the perceived accuracy of a model was notably more reduced when users saw the model failing on a simple task compared to when it made mistakes on more challenging tasks. This research suggests that algorithm aversion may be impacted differently based on the type of model errors encountered.
%

% human influence on algorithms: other-race effect and 
While it is desirable to have greater consideration of human factors in the automated decision-making process, researchers have also studied the negative consequences of mimicking certain human biases **Makino et al., "Other-Race Effect"**. The \textit{other-race effect} for face recognition (our ability to best recognize the identity of faces from our own race) has been observed in several human studies **Phillips, "Human vs. Machine Performance"**. Philips {\em et al.} **Philips et al., "Other-Race Effect for Algorithms"** showed an other-race effect for the algorithms, concluding that their performance varies as a function of the demographic origin of the algorithm and the demographic contents of the test population. Similarly, motivated by this human bias, Flores-Saviaga {\em et al.} **Flores-Saviaga et al., "Human-In-The-Loop Interface"** propose an alternative interface to the classical human-in-the-loop interface and suggest that deriving the classification of facial image pairs as a function of annotators' race will improve the efficiency of the system. But, as they point out, such design decisions carry delicate ethical implications that underscore the importance of work along other lines.
%

There is a recent line of research investigating how human annotators can be effectively introduced into the loop so the algorithm can pass the final decision to the human when certain conditions are given **Papenmeir et al., "Human Annotators in the Loop"**. These conditions are often related to the low confidence of an automated system, which can be used to determine what type of human-machine interaction is most appropriate in a hybrid system **Kidd, "Hybrid Human-Machine Systems"**, as well as to distinguish which annotations flows should be adopted to make human-machine collaboration more efficient **Lum et al., "Human Annotators and Machine Efficiency"**. Combining decisions of systems and humans based on (weighted by) their perceived individual similarities has also been investigated **Rudin, "Combining Human and Machine Decisions"**.

% "Our contribution"
\bigskip
To the best of our knowledge, most of the efforts in integrating human factors into technology have mainly focused on encoding specific human traits and enhancing model performance â€” observing humans and refining models independently. Some more recent efforts have gone further, proposing novel techniques to combine human and machine performance. However, there is still a lack of understanding of the key differences of decision-makers, especially in contexts where the task involves a certain subjectivity. In a scenario where there is no longer only the final decision related to the task at hand, but also the decision as to which agent â€” human, algorithmic or combination of both â€” should make the final decision, it is important to know the strengths and weaknesses of both agents, which of these are shared and which diverge, and how these differences and similarities can be exploited. Here, we propose to study **Kidd et al., "Human vs. Machine Decision-Making"**.

% 5. human factors in algorithms
\subsection{Human Factors in Algorithmic Decision Making}
Numerous studies have been conducted to explore methods for incorporating human factors into ML systems. Han {\em et al.} **Han et al., "Emotion Detection Model with Inter-Annotator Agreement"** introduced an emotion detection model that leveraged inter-annotator agreement to provide a prediction that is more akin to human judgment. Their approach diverged from the conventional belief that a person's state can be simply classified into a \textit{hard} category or single value.
% -- 
The idea of representing the human element using a continuous distribution is endorsed by Peterson {\em et al.} **Peterson, "Continuous Distribution for Human Annotations"**, who introduced a novel image dataset that includes a comprehensive range of human annotations for each image. By representing human-like uncertainty, they achieved favorable results in terms of robustness and out-of-training-set performance, demonstrating that errors in human classification can be just as enlightening as accurate responses. 
%

Related to facial recognition technologies, Andrews {\em et al.} **Andrews et al., "Human Perception of Face Similarities"** raised doubts about the ability of categorical labels to capture the continuous spectrum of human phenotype diversity, particularly in the context of deducing delicate characteristics like social identities (as only a few facial recognition datasets include self-identified categories). They presented a dataset with human perception of face similarities that can be used to learn an embedding space aligned with human perception.
% -- Continuous labels to capture human perception as a spectrum
In relation to the distinctive features present in human perception, Makino et al. **Makino et al., "Human vs. DNNs in Medical Diagnosis"** examined the differences between deep neural networks (DNNs) and human perception in medical diagnosis, focusing on breast cancer screening. They discovered that DNNs utilize features that radiologists often ignore and are outside areas that they considered suspicious. This underscores the importance of incorporating domain knowledge into comparisons of human and machine perception to prevent erroneous outcomes.
%
In a similar vein, Huber {\em et al.} **Huber et al., "Propagation of Model Uncertainties"** proposed the propagation of model uncertainties to the final output to enhance transparency of facial recognition systems and offer a deeper understanding of the verification process. 
%
Additionally, Papenmeir {\em et al.} **Papenmeir et al., "User Perceived Accuracy of Models"** conducted a study involving users and discovered that the perceived accuracy of a model was notably more reduced when users saw the model failing on a simple task compared to when it made mistakes on more challenging tasks. This research suggests that algorithm aversion may be impacted differently based on the type of model errors encountered.
%

% human influence on algorithms: other-race effect and 
While it is desirable to have greater consideration of human factors in the automated decision-making process, researchers have also studied the negative consequences of mimicking certain human biases **Makino et al., "Other-Race Effect"**. The \textit{other-race effect} for face recognition (our ability to best recognize the identity of faces from our own race) has been observed in several human studies **Phillips, "Human vs. Machine Performance"**. Philips {\em et al.} **Philips et al., "Other-Race Effect for Algorithms"** showed an other-race effect for the algorithms, concluding that their performance varies as a function of the demographic origin of the algorithm and the demographic contents of the test population. Similarly, motivated by this human bias, Flores-Saviaga {\em et al.} **Flores-Saviaga et al., "Human-In-The-Loop Interface"** propose an alternative interface to the classical human-in-the-loop interface and suggest that deriving the classification of facial image pairs as a function of annotators' race will improve the efficiency of the system. But, as they point out, such design decisions carry delicate ethical implications that underscore the importance of work along other lines.
%

There is a recent line of research investigating how human annotators can be effectively introduced into the loop so the algorithm can pass the final decision to the human when certain conditions are given **Papenmeir et al., "Human Annotators in the Loop"**. These conditions are often related to the low confidence of an automated system, which can be used to determine what type of human-machine interaction is most appropriate in a hybrid system **Kidd, "Hybrid Human-Machine Systems"**, as well as to distinguish which annotations flows should be adopted to make human-machine collaboration more efficient **Lum et al., "Human Annotators and Machine Efficiency"**. Combining decisions of systems and humans based on (weighted by) their perceived individual similarities has also been investigated **Rudin, "Combining Human and Machine Decisions"**.

% 5. human factors in algorithms
\subsection{Human Factors in Algorithmic Decision Making}
Numerous studies have been conducted to explore methods for incorporating human factors into ML systems. Han {\em et al.} **Han et al., "Emotion Detection Model with Inter-Annotator Agreement"** introduced an emotion detection model that leveraged inter-annotator agreement to provide a prediction that is more akin to human judgment. Their approach diverged from the conventional belief that a person's state can be simply classified into a \textit{hard} category or single value.
% -- 
The idea of representing the human element using a continuous distribution is endorsed by Peterson {\em et al.} **Peterson, "Continuous Distribution for Human Annotations"**, who introduced a novel image dataset that includes a comprehensive range of human annotations for each image. By representing human-like uncertainty, they achieved favorable results in terms of robustness and out-of-training-set performance, demonstrating that errors in human classification can be just as enlightening as accurate responses. 
%

Related to facial recognition technologies, Andrews {\em et al.} **Andrews et al., "Human Perception of Face Similarities"** raised doubts about the ability of categorical labels to capture the continuous spectrum of human phenotype diversity, particularly in the context of deducing delicate characteristics like social identities (as only a few facial recognition datasets include self-identified categories). They presented a dataset with human perception of face similarities that can be used to learn an embedding space aligned with human perception.
% -- Continuous labels to capture human perception as a spectrum
In relation to the distinctive features present in human perception, Makino et al. **Makino et al., "Human vs. DNNs in Medical Diagnosis"** examined the differences between deep neural networks (DNNs) and human perception in medical diagnosis, focusing on breast cancer screening. They discovered that DNNs utilize features that radiologists often ignore and are outside areas that they considered suspicious. This underscores the importance of incorporating domain knowledge into comparisons of human and machine perception to prevent erroneous outcomes.
%
In a similar vein, Huber {\em et al.} **Huber et al., "Propagation of Model Uncertainties"** proposed the propagation of model uncertainties to the final output to enhance transparency of facial recognition systems and offer a deeper understanding of the verification process. 
%
Additionally, Papenmeir {\em et al.} **Papenmeir et al., "User Perceived Accuracy of Models"** conducted a study involving users and discovered that the perceived accuracy of a model was notably more reduced when users saw the model failing on a simple task compared to when it made mistakes on more challenging tasks. This research suggests that algorithm aversion may be impacted differently based on the type of model errors encountered.
%

% human influence on algorithms: other-race effect and 
While it is desirable to have greater consideration of human factors in the automated decision-making process, researchers have also studied the negative consequences of mimicking certain human biases **Makino et al., "Other-Race Effect"**. The \textit{other-race effect} for face recognition (our ability to best recognize the identity of faces from our own race) has been observed in several human studies **Phillips, "Human vs. Machine Performance"**. Philips {\em et al.} **Philips et al., "Other-Race Effect for Algorithms"** showed an other-race effect for the algorithms, concluding that their performance varies as a function of the demographic origin of the algorithm and the demographic contents of the test population. Similarly, motivated by this human bias, Flores-Saviaga {\em et al.} **Flores-Saviaga et al., "Human-In-The-Loop Interface"** propose an alternative interface to the classical human-in-the-loop interface and suggest that deriving the classification of facial image pairs as a function of annotators' race will improve the efficiency of the system. But, as they point out, such design decisions carry delicate ethical implications that underscore the importance of work along other lines.
%

There is a recent line of research investigating how human annotators can be effectively introduced into the loop so the algorithm can pass the final decision to the human when certain conditions are given **Papenmeir et al., "Human Annotators in the Loop"**. These conditions are often related to the low confidence of an automated system, which can be used to determine what type of human-machine interaction is most appropriate in a hybrid system **Kidd, "Hybrid Human-Machine Systems"**, as well as to distinguish which annotations flows should be adopted to make human-machine collaboration more efficient **Lum et al., "Human Annotators and Machine Efficiency"**. Combining decisions of systems and humans based on (weighted by) their perceived individual similarities has also been investigated **Rudin, "Combining Human and Machine Decisions"**.

% 5. human factors in algorithms
\subsection{Human Factors in Algorithmic Decision Making}
Numerous studies have been conducted to explore methods for incorporating human factors into ML systems. Han {\em et al.} **Han et al., "Emotion Detection Model with Inter-Annotator Agreement"** introduced an emotion detection model that leveraged inter-annotator agreement to provide a prediction that is more akin to human judgment. Their approach diverged from the conventional belief that a person's state can be simply classified into a \textit{hard} category or single value.
% -- 
The idea of representing the human element using a continuous distribution is endorsed by Peterson {\em et al.} **Peterson, "Continuous Distribution for Human Annotations"**, who introduced a novel image dataset that includes a comprehensive range of human annotations for each image. By representing human-like uncertainty, they achieved favorable results in terms of robustness and out-of-training-set performance, demonstrating that errors in human classification can be just as enlightening as accurate responses. 
%

Related to facial recognition technologies, Andrews {\em et al.} **Andrews et al., "Human Perception of Face Similarities"** raised doubts about the ability of categorical labels to capture the continuous spectrum of human phenotype diversity, particularly in the context of deducing delicate characteristics like social identities (as only a few facial recognition datasets include self-identified categories). They presented a dataset with human perception of face similarities that can be used to learn an embedding space aligned with human perception.
% -- Continuous labels to capture human perception as a spectrum
In relation to the distinctive features present in human perception, Makino et al. **Makino et al., "Human vs. DNNs in Medical Diagnosis"** examined the differences between deep neural networks (DNNs) and human perception in medical diagnosis, focusing on breast cancer screening. They discovered that DNNs utilize features that radiologists often ignore and are outside areas that they considered suspicious. This underscores the importance of incorporating domain knowledge into comparisons of human and machine perception to prevent erroneous outcomes.
%
In a similar vein, Huber {\em et al.} **Huber et al., "Propagation of Model Uncertainties"** proposed the propagation of model uncertainties to the final output to enhance transparency of facial recognition systems and offer a deeper understanding of the verification process. 
%
Additionally, Papenmeir {\em et al.} **Papenmeir et al., "User Perceived Accuracy of Models"** conducted a study involving users and discovered that the perceived accuracy of a model was notably more reduced when users saw the model failing on a simple task compared to when it made mistakes on more challenging tasks. This research suggests that algorithm aversion may be impacted differently based on the type of model errors encountered.
%

% human influence on algorithms: other-race effect and 
While it is desirable to have greater consideration of human factors in the automated decision-making process, researchers have also studied the negative consequences of mimicking certain human biases **Makino et al., "Other-Race Effect"**. The \textit{other-race effect} for face recognition (our ability to best recognize the identity of faces from our own race) has been observed in several human studies **Phillips, "Human vs. Machine Performance"**. Philips {\em et al.} **Philips et al., "Other-Race Effect for Algorithms"** showed an other-race effect for the algorithms, concluding that their performance varies as a function of the demographic origin of the algorithm and the demographic contents of the test population. Similarly, motivated by this human bias, Flores-Saviaga {\em et al.} **Flores-Saviaga et al., "Human-In-The-Loop Interface"** propose an alternative interface to the classical human-in-the-loop interface and suggest that deriving the classification of facial image pairs as a function of annotators' race will improve the efficiency of the system. But, as they point out, such design decisions carry delicate ethical implications that underscore the importance of work along other lines.
%

There is a recent line of research investigating how human annotators can be effectively introduced into the loop so the algorithm can pass the final decision to the human when certain conditions are given **Papenmeir et al., "Human Annotators in the Loop"**. These conditions are often related to the low confidence of an automated system, which can be used to determine what type of human-machine interaction is most appropriate in a hybrid system **Kidd, "Hybrid Human-Machine Systems"**, as well as to distinguish which annotations flows should be adopted to make human-machine collaboration more efficient **Lum et al., "Human Annotators and Machine Efficiency"**. Combining decisions of systems and humans based on (weighted by) their perceived individual similarities has also been investigated **Rudin, "Combining Human and Machine Decisions"**.

% 5. human factors in algorithms
\subsection{Human Factors in Algorithmic Decision Making}
Numerous studies have been conducted to explore methods for incorporating human factors into ML systems. Han {\em et al.} **Han et al., "Emotion Detection Model with Inter-Annotator Agreement"** introduced an emotion detection model that leveraged inter-annotator agreement to provide a prediction that is more akin to human judgment. Their approach diverged from the conventional belief that a person's state can be simply classified into a \textit{hard} category or single value.
% -- 
The idea of representing the human element using a continuous distribution is endorsed by Peterson {\em et al.} **Peterson, "Continuous Distribution for Human Annotations"**, who introduced a novel image dataset that includes a comprehensive range of human annotations for each image. By representing human-like uncertainty, they achieved favorable results in terms of robustness and out-of-training-set performance, demonstrating that errors in human classification can be just as enlightening as accurate responses. 
%

Related to facial recognition technologies, Andrews {\em et al.} **Andrews et al., "Human Perception of Face Similarities"** raised doubts about the ability of categorical labels to capture the continuous spectrum of human phenotype diversity, particularly in the context of deducing delicate characteristics like social identities (as only a few facial recognition datasets include self-identified categories). They presented a dataset with human perception of face similarities that can be used to learn an embedding space aligned with human perception.
% -- Continuous labels to capture human perception as a spectrum
In relation to the distinctive features present in human perception, Makino et al. **Makino et al., "Human vs. DNNs in Medical Diagnosis"** examined the differences between deep neural networks (DNNs) and human perception in medical diagnosis, focusing on breast cancer screening. They discovered that DNNs utilize features that radiologists often ignore and are outside areas that they considered suspicious. This underscores the importance of incorporating domain knowledge into comparisons of human and machine perception to prevent erroneous outcomes.
%
In a similar vein, Huber {\em et al.} **Huber et al., "Propagation of Model Uncertainties"** proposed the propagation of model uncertainties to the final output to enhance transparency of facial recognition systems and offer a deeper understanding of the verification process. 
%
Additionally, Papenmeir {\em et al.} **Papenmeir et al., "User Perceived Accuracy of Models"** conducted a study involving users and discovered that the perceived accuracy of a model was notably more reduced when users saw the model failing on a simple task compared to when it made mistakes on more challenging tasks. This research suggests that algorithm aversion may be impacted differently based on the type of model errors encountered.
%

% human influence on algorithms: other-race effect and 
While it is desirable to have greater consideration of human factors in the automated decision-making process, researchers have also studied the negative consequences of mimicking certain human biases **Makino et al., "Other-Race Effect"**. The \textit{other-race effect} for face recognition (our ability to best recognize the identity of faces from our own race) has been observed in several human studies **Phillips, "Human vs. Machine Performance"**. Philips {\em et al.} **Philips et al., "Other-Race Effect for Algorithms"** showed an other-race effect for the algorithms, concluding that their performance varies as a function of the demographic origin of the algorithm and the demographic contents of the test population. Similarly, motivated by this human bias, Flores-Saviaga {\em et al.} **Flores-Saviaga et al., "Human-In-The-Loop Interface"** propose an alternative interface to the classical human-in-the-loop interface and suggest that deriving the classification of facial image pairs as a function of annotators' race will improve the efficiency of the system. But, as they point out, such design decisions carry delicate ethical implications that underscore the importance of work along other lines.
%

There is a recent line of research investigating how human annotators can be effectively introduced into the loop so the algorithm can pass the final decision to the human when certain conditions are given **Papenmeir et al., "Human Annotators in the Loop"**. These conditions are often related to the low confidence of an automated system, which can be used to determine what type of human-machine interaction is most appropriate in a hybrid system **Kidd, "Hybrid Human-Machine Systems"**, as well as to distinguish which annotations flows should be adopted to make human-machine collaboration more efficient **Lum et al., "Human Annotators and Machine Efficiency"**. Combining decisions of systems and humans based on (weighted by) their perceived individual similarities has also been investigated **Rudin, "Combining Human and Machine Decisions"**.

You could stop here, as the rest is just a repeat.