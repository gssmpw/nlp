\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{pslatex}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{algorithm}
% \usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsmath}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\author{
\IEEEauthorblockN{Hao Ma\IEEEauthorrefmark{4}\IEEEauthorrefmark{2}
Zhiqiang Pu\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}
Yi Pan\IEEEauthorrefmark{2}
Boyin Liu\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}
Junlong Gao\IEEEauthorrefmark{3}
Zhenyu Guo\IEEEauthorrefmark{3}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}\textit{School of Artificial Intelligence, University of Chinese Academy of Sciences}, Beijing, China
}
\IEEEauthorblockA{\IEEEauthorrefmark{4}\textit{School of Nanjing, University of Chinese Academy of Sciences}, Beijing, China
}
\IEEEauthorblockA{\IEEEauthorrefmark{2}\textit{Institute of Automation}, \textit{Chinese Academy of Sciences}, Beijing, China} 
\IEEEauthorblockA{\IEEEauthorrefmark{3}\textit{Alibaba Group}, Hangzhou, China} 

\{mahao2021, zhiqiang.pu, yi.pan, liuboyin2019\}@ia.ac.cn, \{junlong.gjl, zhenyu.gz\}@alibaba-inc.com
}


\title{Causal Mean Field Multi-Agent Reinforcement Learning
}

% \author{\IEEEauthorblockN{Anonymous Authors}}

\maketitle

\begin{abstract}
Scalability remains a challenge in multi-agent reinforcement learning and is currently under active research. A framework named mean-field reinforcement learning (MFRL) could alleviate the scalability problem by employing the Mean Field Theory to turn a many-agent problem into a two-agent problem. However, this framework lacks the ability to identify essential interactions under nonstationary environments. Causality contains relatively invariant mechanisms behind interactions, though environments are nonstationary. Therefore, we propose an algorithm called causal mean-field Q-learning (CMFQ) to address the scalability problem. CMFQ is ever more robust toward the change of the number of agents though inheriting the compressed representation of MFRL's action-state space. Firstly, we model the causality behind the decision-making process of MFRL into a structural causal model (SCM). Then the essential degree of each interaction is quantified via intervening on the SCM. Furthermore, we design the causality-aware compact representation for behavioral information of agents as the weighted sum of all behavioral information according to their causal effects. We test CMFQ in a mixed cooperative-competitive game and a cooperative game. The result shows that our method has excellent scalability performance in both training in environments containing a large number of agents and testing in environments containing much more agents.
\end{abstract}

\section{Introduction}
\label{intro}
Multi-agent reinforcement learning (MARL) has achieved remarkable success in some challenging tasks. e.g., video games\cite{vinyals2019grandmaster, wu2019hierarchical}. However, training a large number of agents remains a challenge in MARL. The main reasons are 1) the dimensionality of joint state-action space increases exponentially as agent number increases, and 2) during the training for a single agent, the policies of other agents keep changing, causing the nonstationarity problem, whose severity increases as agent number increases\cite{sycara1998multiagent, zhang2021multi, Gronauer2021-ii}.


Existing works generally use the centralized training and decentralized execution paradigm to mitigate the scalability problem via mitigating the nonstationarity problem\cite{rashid2018qmix,foerster2018counterfactual,lowe2017multi,sunehag2017value}. Curriculum learning and attention techniques are also used to improve the scalability performance\cite{long2020evolutionary,iqbal2019actor}. However, above methods focus mostly on tens of agents. For large-scale multi-agent system (MAS) contains hundreds of agents, studies in game theory\cite{blume1993statistical} and mean-field theory\cite{stanley1971phase, yang2018mean} offers a feasible framework to mitigate the scalability problem. Under this framework, \cite{yang2018mean} propose an algorithm called mean-field Q-learning (MFQ), which replaces joint action in joint Q-function with average action, assuming that the entire agent-wise interactions could be simplified into the mean of local pairwise interactions. That is, MFQ reduces the dimensionality of joint state-action space with a merged agent. However, this approach ignores the importance differences of the pairwise interactions, resulting in the poor robustness. Nevertheless, one of the drawbacks to mean field theory is that it does not properly account for fluctuations when few interactions exist\cite{uzunov1993introduction} (e.g., the average action may change drastically if there are only two adjacent agents). Ref. \cite{wang2022weighted} attempt to improve the representational ability of the merged agent by assign weight to each pairwise interaction by its attention score. However, the observations of other agents are needed as input, making this method not practical enough in the real world. In addition, the attention score is essentially a correlation in feature space, which seems unconvincing. On the one hand, an agent pays more attention to another agent not simply because of the higher correlation. On the other hand, it may be inevitable that the proximal agents will be assigned high weight just because of the high similarity of their observation.


In this paper, we want to discuss a better way to represent the merged agent. We propose an algorithm named causal mean-field Q-learning (CMFQ) to address the shortcoming of MFQ in robustness via causal inference. Research in psychology reveals that humans have a sense of the logic of intervention and will employ it in a decision-making context\cite{sloman2015causality}. This suggests that by allowing agents to intervene in the framework of mean-field reinforcement learning (MFRL), they could have the capacity to identify more essential interactions as humans do. Inspired by this insight, we assume that different pairwise interactions should be assigned different weights, and the weights could be obtained via intervening. We introduce a structural causal model (SCM) that represents the invariant causal structure of decision-making in MFRL. We intervene on the SCM such that the corresponding effect of specific pairwise interaction can be presented by comparing the difference before and after the intervention. Intuitively, the intervening enable agents to ask ``what if the merged agent was replaced with an adjacent agent" as illustrated in Fig.\ref{fig:cartoon}. In practice, the pairwise interactions could be embodied as actions taken between two agents, therefore the intervention also performs on the action in this case.


CMFQ is based on the assumption that the joint Q-function could be factorized into local pairwise Q-functions, which mitigates the dimension curse in the scalability problem. Moreover, CMFQ alleviates another challenge in the scalability problem, namely nonstationarity, by focusing on crucial pairwise interactions. Identifying crucial interactions is based on causal inference instead of attention mechanism. Surprisingly, the scalability performance of CMFQ is much better than the attention-based method\cite{wang2022weighted}. The reasons will be discussed in experiments section. As causal inference only needs local pairwise Q-functions, CMFQ is practical in real-world applications, which are usually partially observable. We evaluate CMFQ in the cooperative predator-prey game and mixed cooperative-competitive battle game. The results illustrate that the scalability of CMFQ significantly outperforms all the baselines. Furthermore, results show that agents controlled by CMFQ emerge with more advanced collective intelligence. Supplemental materials could be found at \href{https://sites.google.com/view/cmfq}{https://sites.google.com/view/cmfq}.


This paper aims to alleviate the scalability problem in MARL. In summary, our contributions include:
\begin{itemize}
\item We analyze the bottleneck of MFRL in solving the scalability problem. By decomposing the scalability problem into 1) the dimensionality of joint state-action space increases exponentially as agent number increases, and 2) the non-stationarity increases as agent number increases, we could find that MFRL solves the first problem, while the second problem remains largely unresolved. Hence MFQ exhibits a strong scalability during training, but a poor scalability during execution. That is, if we increase the number of agents during execution, MFQ will fail rapidly.
\item We propose an algorithm named CMFQ to further alleviate the second problem, thus significantly increases the robustness of MFQ. MFQ characterizes population behavioral information by averaging actions of agents, then obtains an average merged agent which lacks representational ability. CMFQ quantifies the importance degree of each agent by counterfactual inference. Then more reasonable and causality-aware merged agents could be obtained, enabling agents to robustly concentrate on agents that truly matter. Consequently, CMFQ exhibits impressive scalability during both training and execution.
\item CMFQ demonstrates a promising and flexible framework for incorporating causal inference into MFRL. The method to calculate causal effects is very flexible. New algorithms could be obtained by reasonably modifying the causal module in the framework.
\end{itemize}


\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{imgs/cartoon.pdf}
    \caption{Blue agents and orange agents belong to different teams. The purple agent denote a merged agent that simply average all agents in agent $i$'s neighborhood. The diagram on the left shows a scenario in which the central agent $i$ interacts with many agents, $i_k$ denotes the $k^{th}$ agent in the observation of agent $i$. In the framework of MFRL, the scenario is transferred to the diagram in the middle, in which an merged agent is used to characterize all the agents in the central agent's observation. Our method further enables the central agent to learn to ask ``what if". When it asks this question, it can imagine the scenario illustrated in the right diagram. The central agent can hypothetically replace the action of the merged agent in MFRL with the action of a neighborhood agent, and if this replacement will cause dramatic changes in policy, it means this neighborhood agent is potentially important. Thus central agent should pay more attention to the interaction with this neighborhood agent.}
    \label{fig:cartoon}
\end{figure}

\section{Related Work}
The scalability problem has been widely investigated in current literatures. Ref. \cite{yang2018mean} propose the framework of MFRL that increases scalability by reducing the action-state space. Several works in a related area named mean-field game also proves that using a compact representation to characterize population information helps solve scalability problem\cite{guo2019learning,Perrin2021-qr}.


Several works were proposed to improve MFQ. Ref. \cite{wu2022weighted} proposed a weighted mean-field assigning different weights to neighbor actions according to the correlations of the hand-craft agent attribute set, which is difficult to generalize to different environments. Ref. \cite{wang2022weighted} calculate the weights with attention score. The observations of other agents are needed to calculate the attention scores, making its practicality not satisfactory. 


Our work is also closely related to recent development in causal inference. Researches indicate that once the SCM, which implicitly contains the causal relationships between variables, is constructed, we can obtain the causal effect by intervening. The causal inference has already been exploited for communication pruning\cite{ding2020learning}, solving credit assignment problem\cite{foerster2018counterfactual,omidshafiei2019learning}, demonstrating the potential of causal inference in reinforcement learning\cite{pearl2019seven,pearl2001direct,peters2017elements}. Ref. \cite{xia2021causal} and \cite{zevcevic2021relating} further proved that SCM could be equally replaced with NCM under certain constraints, enabling us to ask ``what if" by directly intervening on neural network.



\begin{figure}
\setlength{\abovedisplayskip}{1.5pt}
\setlength{\belowdisplayskip}{3pt}
    \centering
    \subfigure[]{
        \label{fig:frame}
        \includegraphics[width=0.4\textwidth]{imgs/framework.pdf}}
    \hspace{0.15in}
    \subfigure[]{
        \label{fig:}
        \includegraphics[width=0.3\textwidth]{imgs/causal_module.pdf}}
    \caption{(a) is CMFQ's architecture. Each neighborhood agent is assigned a weight according to its causal effect to the policy of the central agent. (b) is the causal module. It calculate the $KL$ divergence between the two policies that the merged agent is represented by the average action and the $k^{th}$ neighborhood agent action respectively. A large $KL$ divergence means the $k^{th}$ neighborhood agent might be ignored in the merged agent represented by the average action, hence it should be assigned a higher weight to form a better merged agent.}
    \label{fig:frame&causal_module}
\end{figure}

\section{Preliminary}

This section discusses the concepts of the stochastic game, mean-field reinforcement learning, and causal inference.
\subsection{Stochastic Game}

A $N$-player stochastic game could be formalized as $G=<S, \boldsymbol{A}, P, \boldsymbol{r}, N, \gamma>$, in which $N$ agents in the environment take action $\boldsymbol{a}\in\boldsymbol{A}=\times_{i=1}^N A^i$ to interact with other agents and the environment. Environment will transfer according to the transition probability $P(s^\prime\mid s,\boldsymbol{a}):S\times\boldsymbol{A}\times S\rightarrow[0,1]$, then every agent obtains its reward $r^i(s,a^i):S\times A^i\rightarrow\mathbb{R}$ and $\gamma\in[0,1]$ is the discount factor. Agent makes decision according to its policy $\pi^i (s):S\rightarrow\Omega(A^i)$, where $\Omega(A^i )$ is a probability distribution over agent $i$'s action space $A^i$. 


The joint Q-function of agent $i$ is parameterized by $\theta_i$ and takes $s$ and $\boldsymbol{a}$. It is updated as
\begin{equation}
\begin{aligned}
\mathcal{L}_i (\theta_i)&=\mathbb{E}_{s, \boldsymbol{a}, r, s^{\prime}}\left[\left(Q^i\left(s, \boldsymbol{a}; \theta_i\right)-y\right)^2\right],\\
y &= r+\gamma \max _{a^{\prime^{i}}} Q^i\left(s^{\prime}, \boldsymbol{a} ; \theta^{-}_i\right)
\label{pre:dqn_loss}
\end{aligned}
\end{equation}
where $\theta_i^{-}$ is updated by with $\theta_i$ every $C$ steps and set fixed  until the next $C$ steps finish.

\subsection{Mean Field Reinforcement Learning}

Mean field approximation turns a many-agent problem into a two-agent problem by mapping the joint action space to a single action space. The joint action Q function is firstly factorized considering only local pairwise interactions, then pairwise interactions are approximated using the mean-field theory
\begin{equation}
\begin{aligned}
Q^i\left(s, a^1, a^2, \ldots, a^N\right)&=\frac{1}{N^i} \sum_{k \in N(i)} Q^i\left(s, a^i, a^k\right) \\
&\approx Q^i\left(s, a^i, \bar{a}^i\right) \label{eq:factorize}
\end{aligned}
\end{equation}
where $N^i=|N(i)|$. $N(i)$ is the set of agent $i$'s neighboring agents. Interactions between central agent $i$ and its neighbors are reduced to the interaction between the central agent and an abstract agent, which is presented by average behavior information of agents in the neighborhood of agent $i$. Finally, the policy of the central agent $i$ is determined by pairwise Q-function
\begin{equation}
\pi_t^i\left(a^i_t \mid s, \bar{a}^i_t\right)=\frac{\exp \left(\beta Q_t^i\left(s_t, a^i_t, \bar{a}^i_{t-1}\right)\right)}{\sum_{a^{\prime^{i} \in \mathcal{A}^i}} \exp \left(\beta Q_t^i\left(s_t, a^{\prime^{i}}, \bar{a}^i_{t-1}\right)\right)}
\end{equation}
It is proven that $\pi_t^i$ will converge eventually\cite{yang2018mean}.

\subsection{Causal Inference}
The data-driven statistical learning method lacks the identification of causality which is quite a vital part of composing human acknowledge. The SCM established with human knowledge is needed to represent the causality among all the variables we consider. An SCM is a 4-tuple $\mathcal M=<\mathbf U,\mathbf V,\mathbf F,P(\mathbf U)>$. $\mathbf U=\left\{U_1,U_2,\cdots,U_m\right\}$ is the set of exogenous variables which are determined by factors outside the model. $\mathbf V=\left\{V_1,V_2,\cdots,V_n \right\}$ is the set of endogenous variables that are determined by other variables. $\mathbf F$ is a set of functions $\left\{f_{V_1},f_{V_2},\cdots,f_{V_n}\right\}$ such that $f_{V_j}$ maps $\mathbf{Pa}_{V_j}\cup \mathbf{U}_{V_j}$ to $V_j$. where $\mathbf{U}_{V_j}\subseteq \mathbf{U}$ is all the exogenous variables directly point to $V_j$ and $\mathbf{Pa}_{V_j}\subseteq \mathbf V \backslash V_j$ is all the endogenous variables directly point to $V_j$. That is, $V_j=f_{V_j} (\mathbf{Pa}_{V_j},\mathbf{U}_{V_j})$ for $j=0,1,\cdots,n$. $P(\mathbf U)$ is the probability distribution function over the domain of $\mathbf U$. The causal mechanism in SCM $\mathcal M$ induced an acyclic graph $\mathcal G$, which uses a direct arrow to present a direct effect between variables as shown in Fig.\ref{fig:scm}.
Intervention is performed through an operator called $do(x)$, which directly deletes $f_X$  and replaces it with a constant $X=x$, while the rest of the model keeps unchanged. The equation defines the post-intervention distribution
\begin{equation}
P_\mathcal M (y|do(x))\triangleq P_{\mathcal M_x} (y)
\end{equation}
where $\mathcal M_x$ is the SCM after performing $do(x)$. Once we obtain the post-intervention distribution, one may measure the causal effect by comparing it with the pre-intervention distribution. A common measure is the average causal effect.
\begin{equation}
  E[Y|do(x_0^\prime )]-E[Y|do(x_0 )] \label{ATE}
\end{equation}
where $x_0^\prime$ and $x_0$ are two different interventions. The causal effect may also be measured by the experimental Risk Ratio\cite{pmlr-v6-pearl10a}
\begin{equation}
  \frac{E[Y|do(x_0^\prime )]}{E[Y|do(x_0 )]} \label{RishRatio}
\end{equation}

\section{Method}
\subsection{Counterfactual Policy}
To answer ``what if`` questions raised in Introduction(\ref{intro}), counterfactual inference need to be performed on the policy of central agent. For ease of understanding, we construct an SCM which reveals relations among all variables of interest. In the setting of MFRL, mean action $\bar{a}_{t-1}^i$ and state $s_t$ determine the policy $\pi_t^i (\cdot\!\!\mid\!s_t,\bar{a}^i_{t-1} )$ of agent $i$. As the key relation we concern is how the merged interaction affects $\pi_t^i$, the SCM is constructed center on $\pi_t^j$ as illustrated in Fig.\ref{fig:scm.sub.2}. Note that the SCM is derived from the definitions in stochastic game and MFRL. Formally, the causal effect of acting $a^k$ on $\pi_t^i$ is qualified as follow.
\begin{equation}
    \label{eq:te} 
    TE^{i,k}_t\!=\!KL(\pi_t^i(\cdot\!\mid \!s_t, a_t^i, \bar{a}_{t-1}^i),\pi_t^i(\cdot\!\mid \!s_t, a_t^i, do(\bar{a}^i_{t-1}\!=\!a^{i,k}_{t-1})))
\end{equation}
where $a^{i,k}_{t-1}$ is the action of the $k^{th}$ agent in the neighborhood of agent $i$. For unknown distributions, the causal effects are quantified using the difference in statistics before and after the intervention as Eq.(\ref{ATE}) and Eq.(\ref{RishRatio}). As the policies in Eq.(\ref{eq:te}) are known, we can utilize the $KL$ divergence to quantify causal effects, because the essential idea of treatment effect is to measure the change in distribution after $do$-calculus. $\pi_t^i(\cdot\!\mid \!s_t, a_t^i, do(\bar{a}^i_{t-1}\!=\!a^{i,k}_{t-1}))$ is the counterfactual policy. We could distinguish nontrivial interactions according to their causal effects. Because a large $KL$ divergence means that the preferred action in the policy of plain average merged agent could be a bad choice in the counterfactual policy, which implies a large potential threat of this interaction.


It is worth noting that not all neural networks are capable of causal inference\cite{xia2021causal}. As a neural network learned by interacting with the environment, $\pi_t^i$ lies on the second layer of \textit{Pearl Causal Hierarchy }\cite{bareinboim2022pearl}, and naturally contains both the causality between agent-wise interaction and the causality between agent-environment interaction. It is sufficient for estimating the causal effect of certain interaction.
\begin{figure}
    \centering
    \subfigure[]{
        \label{fig:scm.sub.1}
        \includegraphics[width=0.4\textwidth]{imgs//scm.pdf}}
    \hspace{0.1\textwidth}
    \subfigure[]{
        \label{fig:scm.sub.2}
        \includegraphics[width=0.4\textwidth]{imgs//scm_mf.pdf}}
    \vspace{-0.1cm}
    \caption{(a) is a canonical SCM, when $do(x_0)$ is performed on $X$, all causes of $X$ will be broken and keep all variable constant but only change $X$ to $x_0$. (b) is the SCM of MFRL, the $do$-calculus on $\bar{a}^i_{t-1}$ follows the same procedure.}
    \label{fig:scm}
\end{figure}

\subsection{Improving MFQ with Causal Effect}
\label{section:3.2}
In MFRL, we assume that different pairwise Q-functions should be assigned different weights depending on their potential influences on the policy of central agent. Hence, the factorization of Eq.(\ref{eq:factorize}) should be revised to
\begin{equation}
    Q^i\left(s, a^1_t, a^2_t, \ldots, a^N_t\right)=\sum_{k \in N(i)} w^{i,k} Q^i\left(s, a^i, a^{i, k}\right)\label{eq:fac_revise}
\end{equation}
where $N(i)$ is the set of agent $i$'s adjacent agents. For simplicity, we denote $Q^i\left(s, a^i, a^{i, k}\right)$ as $Q^i_{a^k}$, $Q^i\left(s, a^i, \tilde{a}^{i, k}\right)$ as $Q^i_{\tilde{a}^{k}}$, and $Q^i\!\left(s,\! a^i,\! \check{a}^i\right)$ as $Q^i_{\check{a}}$. Then $Q^j (s,a^1,a^2,\cdots,a^N )$ is approximated using mean-field theory and considering the causality-aware weights
\begin{equation}
    \begin{aligned}
    &Q^i (s,a^1,a^2,\cdots,a^N )= \sum_{k \in N(i)} w^{i,k} Q^i_{a^k}\\
    &=\!\sum_{k \in N(i)} \!\!w^{i,k}\!\left[Q^i_{\check{a}}\!+\!\nabla_{\check{a}^i} Q^i_{\check{a}} \cdot \delta a^{i, k}\!+\!\frac{1}{2} \delta a^{i, k} \!\cdot\! \nabla_{\tilde{a}^{i, k}}^2 Q^i_{\tilde{a}^{k}} \cdot \delta a^{i, k}\right] \\
    &=Q^i_{\check{a}}\!+\!\nabla_{\check{a}^i} Q^i_{\check{a}}\!\cdot\!\!\left[\!\sum_{k \in N(i)}\!\!\! w^{i,k} \delta a^{i, k}\right]\!\! +\!\!\!\!\!\sum_{k \in N(i)}\!\!\! w^{i,k} R_{s, a^i}^i\left(a^{i, k}\right) \\
    &=Q^i_{\check{a}}+\ \sum_{k \in N(i)} w^{i,k} R_{s, a^i}^i\left(a^{i, k}\right) \approx Q^i_{\check{a}} 
    \label{eq:mf}
    \end{aligned}
\end{equation}
where $\delta a^{i, k} = a^{i, k} - \check{a}^i$ and $\check{a}^i=\sum_{k \in N(i)} w^{i,k}a^{i, k}$, hence $\sum_k w^{i,k}\delta a^{i, k}=0$. In the second-order term, $\tilde{a}^{i, k}=\check{a}^i+\epsilon^{i, k} \delta a^{i, k}$, $\epsilon^{i, k}\in(0, 1)$. $R_{s, a^i}^i\left(a^{i,k}\right)$ denotes the first-order Taylor expansion's Lagrange remainder which is bounded by $[-L, L]$ in the condition that the $Q^i\left(s, a^i, {a}^{i,k}\right)$ function is $L$-smoothed. The remainder is a value fluctuating around zero. As \cite{yang2018mean} discussed in their work, under the assumption that fluctuations caused by adjacent agents tend to cancel each other, the remainder could be neglected.


Once causal effects of pairwise interactions are known, the next question is how to to improve the representational capacity of the merged agent. Both linear methods, e.g., weighted sum, or nonlinear methods, e.g., encoding with a neural network, might be useful. However, to ensure the merged agent's reasonability, we prefer a representation in the linear space formed by adjacent agents' action vectors. An intuitive method that can induce reasonable output is a weighted sum. In practice, we find that weighted sum using respective causal effects as weight is enough to effectively improve the representational capacity of average action

\begin{equation}
\label{eq:policy}
 \begin{aligned}
\pi_t^i\left(a^i_t \mid s_t, \check{a}^i_{t-1}\right) &=\frac{\exp \left(\beta Q_t^i\left(s_t, a^i_t, \check{a}^i_{t-1}\right)\right)}{\sum_{{{a^{\prime}}^{i} \in \mathcal{A}^i}} \exp \left(\beta Q_t^i\left(s_t, {{a^{\prime}}^{i}}, \check{a}^i_{t-1}\right)\right)},\\
\quad \check{a}^i_{t-1}&=\sum_{k \in N(i)} w^{i, k}_t a^{i, k}_{t-1}
\end{aligned}  
\end{equation}

\begin{equation}
\begin{aligned}
w^{i,k}_t &=\frac{TE^{i,k}_t+\epsilon}{\sum_{k \in N(i)}\left(T E^{i,k}_t+\epsilon\right)}
\end{aligned} 
\end{equation}
where subscripts are used to denote time steps. $TE^{i,k}_t$ is calculated according to Eq.(\ref{eq:te}). Each $a^{i,k}_{t-1}$ is encoded in one hot vector. Hence the weighted sum returns a reasonable representation in the linear space formed by the actions of neighborhoods. Moreover, the representation is close to essential actions, emphasizing high-potential impact interactions. A term $\epsilon$ was introduced to smooth the weight distribution across all adjacent agents, avoiding additional nonstationarity during training. Besides, the naive mean-field approximation could be achieved when $\epsilon\rightarrow\infty$.


The Q-function $Q^i$ update using the following loss function similar with Eq.(\ref{pre:dqn_loss})
\begin{equation}
\label{eq:cmfe_loss}
\mathcal{L}_i(\theta_i)=\mathbb{E}_{s, \boldsymbol{a}, r, s^{\prime}}\left[\left(Q^i\left(s, a^i, \check{a}^i ; \theta_i\right)-y\right)^2\right]
\end{equation}

\begin{equation}
    \label{eq:loss_y}
    y=r+\gamma \max _{a^{\prime^{i}}} Q^i\left(s^{\prime}, {a^\prime}^{i}, \check{a}^i ; \theta^{-}_i\right)
\end{equation}
\section{Experiments}
We evaluate CMFQ in two tasks: a mixed cooperative-competitive battle game and a cooperative predator-prey game. In the battle task, we compare CMFQ with independent Q-learning (IQL)\cite{tampuu2017multiagent}, MFQ\cite{yang2018mean}, and Attention-MFQ\cite{wang2022weighted} to investigate the effectiveness and scaling capacity of CMFQ. We further verify the effectiveness of CMFQ in another task. In the predator-prey task, we compare CMFQ with MFQ and Attention-MFQ. Our experiment environment is MAgent\cite{zheng2018magent}.
\subsection{Mixed cooperative-competitive game}
\textbf{Task Setting.} In this task, agents are separated into two groups, each containing $N$ agents. Every agent tries to survive and annihilate the other group. Ultimately the team with more agents surviving wins. Each agent obtains partial observation of the environment and knows the last actions other agents took. Agents will be punished when moving and attacking to lead agents to act efficiently. Agents are punished when dead and only rewarded when killing the enemy. The reward setting requires the agent to cooperate efficiently with teammates to annihilate enemies. In the experiments, we train CMFQ, IQL, MFQ, and Attention-MFQ in the setting of $N=64$, then we change $N$ from 64 to 400 to investigate the scalability of CMFQ. The concrete reward values are set as follow: $r_{attack}=-0.1, r_{move}=-0.005, r_{dead}=-0.1, r_{kill}=5$. We train every algorithm in \textit{self-play} paradigm.


\textbf{Quantitative Results and Analysis.} As illustrated in Fig.\ref{fig:win_rate.sub.1}, we compare CMFQ with Attention-MFQ, MFQ, and IQL. We do not choose \cite{wu2022weighted} as a baseline because it is a correlation-based algorithm identical to Attention-MFQ. We assume that the attention-based method is a more challenging baseline. Moreover, in addition to these algorithms, we also set ablation algorithms named Random to verify that the performance improvement of CMFQ is not caused by randomization. Random follows the same pipeline as CMFQ but returns a random causal effect for each interaction. Fig.\ref{fig:exp1.curve} shows the learning curve of all algorithms. We can see that the total rewards of all algorithms converge to a stable value, empirically demonstrating the training scalability of our algorithm. 


To compare the performance of each algorithm, we put trained algorithms in the test environment that $N=64$, and let them battle against each other. Fig.\ref{fig:win_rate.sub.1} shows that MFQ performs better than IQL but worse than Attention-MFQ, indicating that the mean-field approximation mitigates the scalability problem in this task. However, the simply averaging as MFQ is not a good representation of the population behavioral information. In order to improve its representational ability for large-scale scenarios, it is necessary to assign different weights to different agents. Moreover, CMFQ outperforms Attention-MFQ during the test, verifying the correctness of our hypothesis that correlation-based weighting is insufficient to catch the essential interactions properly, while the intervention fills this gap by giving agents the ability to ask the counterfactual question about ``what if".


\begin{figure}
    \centering
    \subfigure[Total reward during training.]{
        \includegraphics[width=0.19\textwidth]
        {imgs/exp1_curve.pdf}
        \label{fig:exp1.curve}}
    \subfigure[Performance comparisons.]{
        \includegraphics[width=0.2\textwidth]{imgs/battle_win_rate.pdf} 
        \label{fig:win_rate.sub.1}}
    \subfigure[Test Scalability curve.]{
        \includegraphics[width=0.2\textwidth]{imgs/exp1_scalability.pdf}
        \label{fig:scalability}}
    \subfigure[Ablation experiments of $\epsilon$.]{
        \includegraphics[width=0.2\textwidth]{imgs/battle_epsilon_ab.pdf}
        \label{fig:ablation}}
    \caption{Win rate during execution. (a) demonstrates the curves of total reward during training for each algorithm. (b) shows the results that algorithms battle against each other. the horizontal axis is divided into five groups by algorithms, and within each group there are five bars representing the win rate of the algorithm on the horizontal axis. (c) shows win rates of algorithms in the label against MFQ algorithms which are on the horizontal axis. (d) shows the win rate of CMFQ with different $\epsilon$ against other algorithms.}
\end{figure}
We further investigate the test scalability of CMFQ, MFQ, and Attention-MFQ. Firstly, we train these three algorithms in 64 vs. 64 scenario with self-play, denoted as CMFQ$_{64}$, MFQ$_{64}$,  Attention-MFQ$_{64}$ respectively, and further train the MFQ algorithm in 100 vs. 100  and  400 vs. 400 scenarios, denoted as MFQ$_{100}$ and MFQ$_{400}$. Then, allow  CMFQ$_{64}$, MFQ$_{64}$, and Attention-MFQ$_{64}$ to battle against MFQ$_{64}$, MFQ$_{100}$ and MFQ$_{400}$ in environments 64 vs. 64, 100 vs. 100, 400 vs. 400 respectively, that is, letting CMFQ, MFQ, and Attention-MFQ control more agents than they were trained, to reveal the test scalability of the algorithms. As shown in Fig.\ref{fig:scalability}, the test scalability of MFQ is the worst, which means that we need to retrain MFQ when the number of agents increases and the test scalability of Attention-MFQ is slightly better. The test scalability of CMFQ is significantly better than both of them. Furthermore, CMFQ achieves win rates of nearly 100\% against MFQ$_{100}$ and 32\% against MFQ$_{400}$.


\begin{figure}
        \centering
        \subfigure[]{
            \includegraphics[width=0.2\textwidth]{imgs/besiege.pdf} 
            \label{fig:vis.sub.1}}
        \hspace{0.01\textwidth}
        \subfigure[]{
            \includegraphics[width=0.2\textwidth]{imgs/teamup.pdf}
            \label{fig:vis.sub.2}}
        \caption{Visualization of CMFQ vs MFQ in 64 vs 64 environment. Red squares denote CMFQ, and blue squares denote MFQ, the vertical bar on the left side of the square indicates its health point, and the surrounding circular area indicates its attack range. When agent attacks, an arrow will be extended to point at the attack target.}
\end{figure}
\textbf{Ablations.} We set two ablation experiments. The first one to ablate the effectiveness of causal effects in CMFQ. As illustrated in Fig.\ref{fig:win_rate.sub.1}, the performance of \textit{Random} is inferior to MFQ, verifying the validity of causal effect in CMFQ. The other one is ablation for $\epsilon$. As we analyze in \ref{section:3.2}, $\epsilon$ is an adjustable parameter in the interval $[0,+\infty]$. As $\epsilon$ increases, the effect of each interaction becomes smoother and eventually CMFQ equal to MFQ when $\epsilon\rightarrow+\infty$. From the Fig.\ref{fig:ablation}, we can see that as we adjust $\epsilon$ from 0.001 to 1, the learning curve of CMFQ always converges, and in the test environment, win rates of CMFQ always outperform other baselines. When $\epsilon$ is relatively large, the win rate is close to that of MFQ.


\textbf{Visualization Analysis.} As illustrated in Fig.\ref{fig:vis.sub.1}, CMFQ learns the tactic of besieging, while MFQ tends to confront frontally. The results in Fig.\ref{fig:vis.sub.2} indicate the tricky issue in mixed cooperative-competitive game: agents need to cooperate with their teammates to kill enemies, whereas only the agent who hits a fatal attack gets the biggest reward $r_{kill}$, driving agents hesitating to attack first. When there are few agents, the policies of MFQ and CMFQ tend to be conservative. However, CMFQ presents more advanced tactics: agents learn the trick of teaming up in the mixed cooperative-competitive game. When an agent chooses to attack, the adjacent teammates will arrive to help, achieving the maximum reward with the smallest cost of health. Moreover, Fig.\ref{fig:vis.sub.2} also shows that attacks of CMFQ are more focused than baselines. CMFQ can discriminate key interactions and have a more accurate timing of attacks, while MFQ lacks this discriminatory ability and thus keeps attacking.


\subsection{Cooperative game}\label{exp2}
\begin{figure}
\centering
    \subfigure[Total reward of Predator.]{
        \includegraphics[width=0.22\textwidth]{imgs/exp2_predator_curve.pdf} 
        \label{fig:exp2.curve.1}}
    \subfigure[Total reward of Prey.]{
        \includegraphics[width=0.22\textwidth]{imgs/exp2_prey_curve.pdf}
        \label{fig:exp2.curve.2}}
\caption{Total reward during training.}
\label{fig.exp2.curve}
\end{figure}
\textbf{Task Setting.} In this task, agents are divided into predator and prey. Prey move 1.5 times faster than predators, and their task is to avoid predators as much as possible. Predators are four times larger than prey and can attack but not yield any damage. Predators only get rewarded when they are close to prey. Therefore, to gain the reward, they must cooperate with other predators and try to surround prey with their size advantage. In our experiments, to test the scalability of the CMFQ, we first train MFQ, CMFQ, and Attention-MFQ employing the \textit{self-play} paradigm in a scenario involving 20 predators and 40 prey, and then test them in environments involving (20 predators, 40 prey), (80 predators, 160 prey), (180 predators, 360 prey) respectively. The reward are set as follow: $r_{attack}=-0.2, r_{surround}=1, r_{be\_surrounded}=-1$.


\begin{figure}
    \centering
    \subfigure[MFQ controls prey.]{
        \includegraphics[width=0.145\textwidth]{imgs/exp2_prey_mfq.pdf} 
        \label{fig:exp2.prey_mfq}}
    \subfigure[Attention-MFQ controls prey.]{
        \includegraphics[width=0.145\textwidth]{imgs/exp2_prey_att.pdf}
        \label{fig:exp2.prey_att}}
    \subfigure[CMFQ controls prey.]{
        \includegraphics[width=0.145\textwidth]{imgs/exp2_prey_causal.pdf}
        \label{fig:exp2.prey_cmfq}}
    \caption{Total reward of predators during execution changes when the number of agents increases. $1\times$ denotes $N_{predator}$=20, $N_{prey}$=40, $4\times$ demotes $N_{predator}$=80, $N_{prey}$=160 and so on. All algorithms are trained in the $1\times$ environment.}
    \label{fig:exp2.ave_r}
\end{figure}
\textbf{Quantitative Results and Analysis.} We compare CMFQ with MFQ and Attention-MFQ. First, we investigate their training scalability in (20 predators, 40 prey), as shown in Fig.\ref{fig:exp2.curve.1} and Fig.\ref{fig:exp2.curve.2}, all of them converge to a stable reward total reward, verifying their training scalability. Then, we enlarge the number of agents during execution to investigate their test scalability. To demonstrate the scalability gap of different algorithms, we allow the algorithms to execute in an adversarial form, which means that one algorithm controls the predator and another controls the prey. For the environment, we change the number of agents to 1x, 4x, and 9x of the number in the training environment.


Because the reward $r_{be\_surrounded}$ of prey and the reward $r_{surround}$ of predator are zero-sum and cooperation exists mainly among predators, we use the total reward of predators to indicate each algorithm's performance. The results are shown in Fig.\ref{fig:exp2.ave_r}. Total rewards in specific environment indicate the train scalability,  since a higher total reward means agents learn better policy during training. Trends of lines are related to test scalability, and a more flat line indicates the better test scalability of the algorithm. We can see that the total reward of Attention-MFQ is higher than that of MFQ, and the trend is similar to that of MFQ. In comparison, the total reward of CMFQ is higher than that of both MFQ and Attention- MFQ, and the trend is ever more flat, indicating that CMFQ has better scalability.
    \begin{figure}
        \centering
        \includegraphics[width=0.45\textwidth]{imgs/exp2_vis.pdf}
        \caption{Visualization of cooperative predator prey game. The first row is results of CMFQ, the second row is results of Attention-MFQ. $N_{predator}$=20,$N_{prey}$=40 for the left column, $N_{predator}$=40,$N_{prey}$=80 for the middle column, $N_{predator}$=180,$N_{prey}$=360 for the last column. Red squares are predators while blue squares are prey, the grey squares are obstacles. All images are obtained 400 steps after the game begin.}
        \label{fig:exp2.vis}
    \end{figure}

\textbf{Visualization Analysis.} The results that the trained CMFQ and Attention-MFQ controls predators are shown in Fig.\ref{fig:exp2.vis}. In the the environment that $N_{predator}$=20, $N_{prey}$=40, both CMFQ and Attention-MFQ perform similarly. Predators learn two strategies: four predators cooperating to surround the prey in an open area; two or three predators surrounding the prey with the help of obstacles. In the environment that $N_{predator}$=40, $N_{prey}$=80, when the number of agents increases, predators controlled by Attention-MFQ are more dispersed than predators controlled by CMFQ. Besides, Attention-MFQ has more predators idle than CMFQ. Predators controlled by CMFQ gather on map edges, because it is more efficient to surround prey with the help of map edges. In addition, predators controlled by CMFQ learn an advanced strategy to drive prey to map edges then take advantage of the terrain to surround them. In the environment that $N_{predator}$=180, $N_{prey}$=360, the advanced strategy is also presented. Moreover, predators controlled by CMFQ master the skill to utilize the bodies of still teammates who have captured prey as obstacles. Thus, predators controlled by CMFQ present a high degree of aggregation and environmental adaptability.
\section{Conclusions and Discussions}
This paper aims at scalability problem in large-scale MAS. Firstly, We inherit the framework of MFRL which significantly reduce the dimensionality of joint state-action space. To further handle the intractable nonstationarity when the number of agent is large, we propose an SCM to model the decision-making process, and enable agents to identify the more crucial interactions via intervening on the SCM. Finally a causality-aware representation of population behavioral information could be obtained by the weighted sum of the action of each agent according to its causal effect. Experiments in two tasks reveal the excellent scalability of CMFQ.


\textbf{Limitation and future work.} Despite the significant improvement that CMFQ brings to the robustness of MFQ, we contend that there is still much to explore in the causal inference module itself. Specifically, we question what other $do$-calculus techniques may be feasible beyond replacing the average action with a specific action. We leave this exploration as future work to develop more robust and interpretable algorithms.


\textbf{Broader impact.} CMFQ comprehensively alleviating the scalability problem. This brings very practical benefits: In environments where the observed dimension does not change with the number of agents, multiplying the number of agents will no longer force us to retrain the model, thanks to the robustness of CMFQ. Besides, we can train our models in simpler environments and use them in more complex environments to reduce the training overhead.


\section{Acknowledgements}
This work was supported by the National Key Research and Development Program of China under Grant 2020AAA0103404, the Beijing Nova Program under Grant 20220484077, the National Natural Science Foundation of China under Grant 62073323, and Alibaba Group through Alibaba Innovative Research (AIR) Program. 


\bibliographystyle{IEEEtran}
\bibliography{CMFQ}

\clearpage

\appendix
\section{Implementation details}
The pseudocode of CMFQ is listed below. 

\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm

\begin{algorithm}[h]
  \caption{Causal Mean Field Q-learning}
  \label{alg::cmfq}
  \begin{algorithmic}
    \Require
      Initialize state $s_0$; $Q_{\theta_i},Q_{\theta_i^-},\check{a}_0^i$ for all agent $i\in\left\{1,2,\cdots,N\right\}$; trajectory length $M$;
        \While {in the training loop}
            \For {$t=0,1,\cdots,M$}
                \For {$i=1,2,\cdots,N$}
                    \State Calculate policy $\pi_t^i(\cdot\mid s_t, \bar{a}_{t-1}^i)$ with average merged agent;
                    \State Calculate causal effect for every neighborhood agent by Eq.(\ref{eq:te}); 
                    \State Obtain a new merged agent $\check{a}^i_{t-1}$ and a new policy $\pi_t^i(\cdot\mid s_t, \check{a}_{t-1}^i)$ by Eq.(\ref{eq:policy});
                \EndFor
            \State Sample joint action $\textbf{a}=[a^1,a^2,\cdots,a^N]$ from $[\pi_t^1,\pi_t^2,\cdots,\pi_t^N]$
            \State obtain the next state $s_{t+1}$ and the reward $\textbf{r}=[r^1,r^2,\cdots,r^N]$ and merged agent $\check{\textbf{a}}=[\check{a}^1_{t-1},\check{a}^2_{t-1},\cdots,\check{a}^N_{t-1}]$;
            \State Store transition $<s_t, \textbf{a},\textbf{r},s_{t+1},\check{\textbf{a}}>$ in replay buffer;
            \EndFor
            \For {$i=1,2,\cdots,N$}
                \State Sample a minibatch transition from replay buffer; 
                \State Calculate $\mathcal{L}_i$ and update $\theta_i$ by Eq.(\ref{eq:cmfe_loss});
                \State Updata target network by $\theta_i^-=\theta_i$ after every $C$ updates of $\theta_i$;
            \EndFor
        \EndWhile
  \end{algorithmic}
\end{algorithm}

\section{Derivation for the bound of the Lagrange remainder}
\label{appendixB}
As $s,a^i$ in $Q^i(s,a^i,a^{i,k})$ are fixed parameter in the derivation of Eq.(\ref{eq:mf}), for simplicity, the pairwise Q-function $Q^i(s,a^i,a^{i,k})$ can be rewrite as $Q(a^k)$ in the following. We assume that $a^k$ is a one-hot encoding for n actions, to make $Q(a^k)$ more general, we replace the discrete $a^k\ (a^k\in\mathbb{R}^N)$ by a continuous $x\ (x\in\mathbb{R}^N)$ which donâ€™t violate the domain of the parameterized Q-function.
Given the $Q\left(x\right)$ is $L$-smooth, then for any two points $x,y\in dom\left(Q\right)\subseteq\mathbb{R}^N$, there exists a Lipschitz constant $L\in[0,+\infty)$ that

\begin{equation}
    \Vert\nabla Q(x)-\nabla Q(y)\Vert_2<L\Vert x-y\Vert_2
\end{equation}

By the first order Taylor expansion with Lagrange remainder, we have
\begin{equation}
    \nabla Q\left(y\right)=\nabla Q\left(x\right)+\nabla^2Q\left(x\right)\cdot u+R\left(u\right)
\end{equation}

where $u=y-x,\lim_{u\rightarrow0}\frac{R(u)}{\Vert u\Vert_2}=0$. Assume $x\neq y$, then we can reform the first order Taylor expansion
\begin{equation}
 \begin{aligned}
    \frac{\Vert\nabla^2Q(x)\cdot u\Vert_2}{\Vert u\Vert_2}&=\frac{\Vert\nabla Q(y)-\nabla Q(x)-R(u)\Vert_2}{\Vert u\Vert_2}\\
    &\leq \frac{\Vert\nabla Q(y)-\nabla Q(x)\Vert_2}{\Vert u\Vert_2}+\frac{\Vert R(u)\Vert_2}{\Vert u\Vert_2}\\
    &\leq L+\frac{\Vert R(u)\Vert_2}{\Vert u\Vert_2},\quad \forall x,y\in dom(Q), x\neq y
    \label{eq:appendixB:eq3}
\end{aligned}
\end{equation}

$u$ could be the eigenvalue of $\nabla^2Q\left(x\right)$, then Eq.(\ref{eq:appendixB:eq3}) can be convert to

\begin{equation}
 \begin{aligned}
    \frac{\Vert\nabla^2Q(x)\cdot u\Vert_2}{\Vert u\Vert_2}&=\frac{\Vert \lambda u\Vert_2}{\Vert u\Vert_2}=\mid \lambda\mid \leq L+\frac{\Vert R(u)\Vert_2}{\Vert u\Vert_2}
 \end{aligned}
\end{equation}
Obviously, we can obtain the bound of $\lambda$, $\lambda\in\left[-L,L\right]$. $\nabla^2Q\left(x\right)$ is a real symmetric matrix, so there exist an orthogonal matrix U to diagonalize $\nabla^2Q\left(x\right)$ such that $U^T\left[\nabla^2Q\left(x\right)\right]U=\Lambda\triangleq diag\left[\lambda_1,\lambda_2,\ldots,\lambda_N\right]$. Then the bound of $R_{s,a^i}^i(a^{i,k})$ can be derived as follow
\begin{equation}
    \begin{aligned}
        R_{s,a^i}^i(a^{i,k})&=\frac{1}{2}\delta a^{i,k}\!\cdot\!\nabla^2Q\left(a^k\right)\cdot\delta a^{i,k}\\
        &=\frac{1}{2}\left[U\cdot\delta a^{i,k}\right]^T\!\!\!\Lambda\!\left[U\cdot\delta a^{i,k}\right]\\
        &=\frac{1}{2}\sum_{n=1}^{N}{\lambda_n\left[U\cdot\delta a^{i,k}\right]_n^2}
    \label{eq17} 
    \end{aligned}
\end{equation}
\begin{equation}
    -L\Vert U\cdot\delta a^{i,k}\Vert_2 \leq \sum_{n=1}^{N}{\lambda_n\left[U\cdot\delta a^{i,k}\right]_n^2} \leq L\Vert U\cdot\delta a^{i,k}\Vert_2
    \label{eq18}
\end{equation}
where $\left[U\cdot\delta a^{i,k}\right]_n$ refers to the $n^{th}$ element of vector $U\cdot\delta a^{i,k}$.
\begin{equation}
\begin{aligned}
        \Vert U\cdot\delta a^{i,k}\Vert_2&=\Vert \delta a^{i,k}\Vert_2=(a^{i,k}-\check{a}^i)^T(a^{i,k}-\check{a}^i)\\
        &={a^{i,k}}^T a^{i,k}
        +\check{a}^{i^T}\check{a}^i
        -\check{a}^{i^T} a^{i,k}
        -\check{a}^i{a^{i,k}}^T
        =2(1-\check{a}^i_n)\leq 2    
\end{aligned}
\label{eq19}
\end{equation}
where $a^{i,k}$ is a one-hot encoding action, $\check{a}^i_n$ denotes the $n^{th}$ element in $\check{a}^i$.
Finally, according to Eq.(\ref{eq17}) Eq.(\ref{eq18}) Eq.(\ref{eq19}), the bound of $R_{s,a^i}^i(a^{i,k})$ is $[-L,L]$.


\section{Visualization for the weights of CMFQ and Attention-MFQ}
To further analyze the reasons why CMFQ is more effective than Attention-MFQ empirically, we randomly select an agent in the mixed cooperative-competitive game task and visualize its weight. Some interesting observations can be made from Fig.\ref{fig:appendixC.cmfq}. First of all, it makes sense that the agents on the front line will be given high weights because they are battling. Secondly, the weights of agents at the edge of the front line are relatively small, possibly because these agents can cooperate with nearby teammates to attack an enemy due to their position advantages, so they are in a relatively dominant state. In addition, agents at the very edge of the front line are given higher weights, even if they are out of combat. This is because they are in a position to flank their opponents and work with their teammates to surround the opponents. In Fig.\ref{fig:appendixC.att-mfq}, we observe a result consistent with the analysis in our paper. That is, the attention-based method uses the attributes of other agents to calculate the attention scores, and observation is an important part of the attributes, so it tends to give high weight to the agents nearby because their observations are similar.

\begin{figure}[h]
    \centering
    \subfigure[The weights obtained by CMFQ.]{
        \includegraphics[width=0.45\textwidth]{imgs/causal_weight_vis_id30_step8.pdf} 
        \label{fig:appendixC.cmfq}}
    \hspace{0.01\textwidth}
    \subfigure[The weights obtained by Attention-MFQ.]{
        \includegraphics[width=0.45\textwidth]{imgs/att_weight_vis_id30_step12.pdf}
        \label{fig:appendixC.att-mfq}}
    \caption{The two figures visualize the mixed cooperative-competitive task, where each agent in the blue team in (a) is controlled by CMFQ and each agent in the blue team in (b) is controlled by Attention-MFQ. Each agent in the red team is controlled by MFQ. We label the agents in the blue team whose weights are visualized in green. The number above the blue agent represents the normalized weight given by the green agent to the pairwise interaction between them. Due to space constraints, the integer bits of all weights are omitted}
\end{figure}

\section{Supplemental experiment on MPE}
To further investigate the applicability of CMFQ, we perform an experiment on another environment named multi-agent particle environment (MPE). As the dimensionality of action-state space will change as the initial number of agent changes, making it difficult to verify scalability, but we believe that CMFQ's scalability performance has been adequately validated in previous experiments. For MPE, we tested the predator prey task in MPE when the number of agents was the same as that in the training environment, and compared it with \ref{exp2} to see whether the same conclusions could be drawn in the two environments.\\
\begin{figure}
    \centering
    \subfigure[Average reward of Predator.]{
        \includegraphics[width=0.22\textwidth]{imgs/mpe_predator_curve.pdf} 
        \label{fig:exp3.curve.1}}
    \subfigure[Average reward of Prey.]{
        \includegraphics[width=0.22\textwidth]{imgs/mpe_prey_curve.pdf}
    \label{fig:exp3.curve.2}}
\caption{Average reward during training.}
\label{fig.exp3.curve}
\end{figure}

\textbf{Task Setting.} There are 20 predators, 40 preys, and 20 obstacles. Predator gets $r_{collide}=10$ if it collide with prey. Prey gets $r_{be\_collided}=-10$ if it collided with predator. The speed of prey is 1.3 times of that of predator. In order to make preys learn to leverage obstacles instead of running to infinity, we manually draws an area. If preys go beyond this area, they will get penalty $r_{bound}$ which will be aggravate as the distance preys go beyond this area increase, until $r_{bound}=-10$. We trained MFQ, CMFQ and Attention-MFQ in the self-play paradigm. The training curve is shown in Fig.\ref{fig.exp3.curve}.\\
\textbf{Quantitative Results and Analysis}In the test phase, we controlled 20 Predators and 40 prey with different algorithms respectively, test 10 times and calculated the average reward of each algorithm, as shown in Table.\ref{table:mpe}. First, the average reward of MFQ is lower than CMFQ and Attention-MFQ, regardless of whether it controls predators or preys. This indicates that the representational ability of average merged agent is insufficient. Secondly, when MFQ controls prey, the average predator reward of CMFQ is higher than Attention-MFQ, indicating that the weight obtained by CMFQ was more representational. Finally, in the comparison between CMFQ and Attention-MFQ, CMFQ outperforms Attention-MFQ in both predator reward and prey reward, further confirms the superiority of CMFQ.
In the task that the number of agents in testing was the same as that in the training, We compare the performance of MFQ, CMFQ, and Attention-MFQ and come to the same conclusion consistent with \ref{exp2}, empirically certify the applicability of CMFQ.

\begin{table}[H]
\centering
\def\temptablewidth{0.5\textwidth}
{\rule{\temptablewidth}{1pt}}  %æ ¹æ®ä½¿ç”¨æƒ…å†µçµæ´»è®¾ç½®ï¼Œçº¿çš„ç²—ç»†
\begin{tabular*}{\temptablewidth}{@{\extracolsep{\fill}}ccccc}
\hline
Predator      & Predator reward   & Prey          &Prey reward  \\ 
\hline
MFQ           & 4.23              & CMFQ          & -8.64          \\
\hline
CMFQ          & 6.68              & MFQ           & -13.05         \\
\hline
MFQ           & 4.01              & Attention-MFQ & -8.47          \\
\hline
Attention-MFQ & 6.05              & MFQ           & -12.46         \\
\hline
CMFQ          & 3.15              & Attention-MFQ & -11.07         \\
\hline
Attention-MFQ & 3.02              & CMFQ          & -4.23          \\
\hline
\end{tabular*}
{\rule{\temptablewidth}{1pt}}
\caption{Results that let two different algorithm control predators and preys respectively. Predator reward is the average reward a predator obtain every step. Prey reward is the average reward a prey obtain every step.}
\label{table:mpe}
\end{table}

\end{document}
