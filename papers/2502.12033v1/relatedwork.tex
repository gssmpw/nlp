\section{Related work}
Below is a state-of-the-art overview of methods to explain and interpret Transformer models, including the analysis of attention patterns.

\subsection{Attention mechanisms for interpretability}
Transformers like BERT rely on attention mechanisms, which highlight the relevance of tokens in a sequence for model predictions. BERTviz is a widely used tool that visualizes attention across different heads and layers, providing insights into token interactions \cite{vig2019}. However, attention weights alone may not reliably explain model decisions, as highlighted in \cite{jain2019}, who argue that attention weights are frequently uncorrelated with token importance in final predictions.

\subsection{Attention rollout}
Attention rollout is a technique for the aggregation of attention weights across all layers to provide a more global view of how information flows through the Transformer model. 
By multiplying attention weights across layers, attention rollout traces the flow of information from the input to the output \cite{abnar2020}. This method tries to establish a global view of token influence throughout the model and can be seen as a way to quantify information flow beyond single-layer visualizations.

\subsection{Analysis of attention patterns in heads}
Attention heads in BERT have been shown to capture different types of syntactic and semantic information, leading to investigations into the patterns that emerge in their attention maps. Certain heads focus on specific syntactic relationships like subject-verb or noun-adjective dependencies, while others capture long-range dependencies between distant tokens, as shown in \cite{clark2019}. Research has shown that attention heads specialize in different linguistic roles, and some attention heads can be pruned without significantly degrading model performance, indicating redundancy in the attention mechanisms.

For instance, in \cite{voita2019} it has been found that in multi-head self-attention, certain heads are specialized in specific linguistic tasks, while others appear to contribute little to the model’s overall predictions. These findings are particularly useful for interpretability, as they show which parts of the attention mechanism are most relevant to the model’s predictions and which can be considered less critical.

\subsection{Probing classifiers}
Probing classifiers are auxiliary models trained to predict linguistic properties (e.g., syntactic dependencies, parts of speech) from the internal representations of BERT. These probes reveal the types of information encoded in different layers. Studies have shown that lower layers capture more syntactic features, while higher layers encode semantic features \cite{tenney2019}, providing insights into how BERT processes language hierarchically.

\subsection{Feature importance methods}
Feature importance techniques like LIME \cite{ribeiro2016} and SHAP \cite{lundberg2017} explain individual predictions by perturbing inputs or calculating importance scores for each token. These methods help to identify the most influential tokens for specific predictions. Another proposed method, Integrated Gradients, attributes model outputs back to input features involving the computation of the gradient of the output with respect to the input \cite{sundararajan2017}.

\subsection{Model-specific explanation methods}
Some explanation techniques are designed specifically for BERT and other Transformer models. For instance, Layer-wise Relevance Propagation (LRP) is adapted to propagate relevance through the attention heads and layers, allowing for a detailed understanding of how individual tokens contribute to the model's predictions \cite{voita2019}. Similarly, DeepSHAP combines Shapley values with deep learning-specific methods to efficiently explain deep models like BERT \cite{lundberg2017}.

\subsection{Influence functions}
Influence functions trace predictions back to the most influential training data. This technique allows researchers to identify which training examples had the most impact on a particular prediction, offering a data-centric perspective on explainability \cite{koh2017}. Influence functions help link BERT’s predictions to specific patterns in the training data, making the model's decision-making more transparent.

\subsection{Contrastive explanations}
Contrastive explanations focus on explaining why one prediction was made instead of another. In BERT, methods like counterfactual reasoning and adversarial perturbation can identify minimal changes to the input that would result in different predictions \cite{goyal2019}. These techniques provide insights into decision boundaries and critical factors affecting the model’s choices.

\subsection{Saliency maps}
Saliency maps highlight which parts of the input are most important for the model's prediction. In BERT, gradient-based saliency maps and Layer-wise Relevance Propagation (LRP) are often used to pinpoint the most relevant tokens for a given output \cite{sundararajan2017}. These maps provide visual explanations of the model’s focus during inference.

\subsection{Head and layer pruning}
Pruning techniques have been employed to explore the significance of individual attention heads and layers in BERT. By selectively removing attention heads, researchers can assess their importance to the model's performance. Studies have shown that many attention heads can be pruned without significantly impacting the model’s accuracy, suggesting redundancy in the attention mechanism \cite{voita2019}. This finding supports the idea that only a subset of heads is critical for specific tasks, making it possible to simplify the model without large performance losses.

\subsection{Evaluation of explanation methods}
Metrics for evaluating explainability methods focus on two key criteria: faithfulness (whether the explanation reflects the true reasoning process of the model) and plausibility (whether the explanation is intuitive to humans). The ERASER benchmark has been developed to standardize the evaluation of explainability methods for NLP models, ensuring that explanations are both accurate and interpretable \cite{deyoung2020}.