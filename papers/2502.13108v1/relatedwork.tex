\section{Related Work}
\subsection{Question Answering in NLP}
Question Answering (QA) is a core NLP task that enables retrieving precise answers from structured and unstructured text \cite{jurafsky2023speech, agarwal2021evaluate}. Early models relied on rule-based techniques and information retrieval, as seen in IBM's Watson \cite{ferrucci2010building}, which matched keywords using structured knowledge bases but lacked deep language understanding. The rise of neural architectures, including Long Short-Term Memory (LSTM) networks \cite{hochreiter1997long} and attention mechanisms \cite{vaswani2017attention}, significantly improved contextual comprehension.

With Transformer-based models QA took a big leap and \textbf{BERT} \cite{devlin2019bert} set new benchmarks \cite{agarwal2024mvtamperbench}. Fine-tuned variants such as BERT-QA outperformed previous models on datasets like SQuAD \cite{rajpurkar2018know}. Further optimizations, including \textbf{ALBERT} \cite{lan2020albert}, \textbf{RoBERTa} \cite{liu2019roberta}, and \textbf{T5} \cite{raffel2020exploring}, enhanced transformer models for QA across various domains \cite{pattnayak2024survey}. However, their reliance on general-domain corpora limits effectiveness in specialized fields like medicine.


\subsection{Domain Adaptation in Medical NLP}
Medical NLP poses challenges due to domain-specific terminology, abbreviations, and inconsistent documentation \cite{agarwal-etal-2025-fs}. Standard BERT-based models, trained on general corpora, struggle with these complexities. To address this, domain-adapted models such as \textbf{BioBERT} \cite{lee2020biobert}, pretrained on PubMed abstracts, improved biomedical Named Entity Recognition (NER) and relation extraction \cite{Pattnayak2017AutoSales}. \textbf{ClinicalBERT} \cite{alsentzer2019clinicalbert}, fine-tuned on clinical notes, enhanced hospital-based applications.

\textbf{PubMedBERT} \cite{gu2021domain} eliminated domain mismatches by pretraining exclusively on PubMed abstracts. \textbf{GatorTron} \cite{yang2022large} further improved performance by leveraging de-identified clinical records, while \textbf{Med-BERT} \cite{huang2021clinical} incorporated structured EHR data, bridging the gap between structured and unstructured text.

Despite these advancements, most domain-specific models are fine-tuned separately for tasks like QA, named entity recognition, and classification \cite{agarwal2024domain}. This single-task approach limits generalization and increases data requirements, reducing adaptability in clinical settings.



\subsection{Multi-Task Learning in NLP and Medical AI}
Multi-Task Learning (MTL) has emerged as a powerful technique for improving NLP models by jointly learning related tasks \cite{ruder2017overview}. It has been widely explored in domains such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging \cite{liu2019multi}, sentiment analysis with syntactic parsing \cite{sogaard2016deep}, and question answering integrated with textual entailment \cite{clark2019boolq}. By leveraging shared representations, MTL enhances generalization and efficiency, especially in data-scarce environments.

In medical AI, MTL has been applied to clinical event detection \cite{li2020multi}, adverse event detection \cite{wei2022multi}, and patient outcome prediction \cite{yoon2022clinical}, consistently outperforming single-task models \cite{liu2022mtlhealth}. For instance, multi-task transformers for clinical risk prediction \cite{mohamed2023riskprediction} have demonstrated improved generalization across patient cohorts. Despite these successes, MTL remains underutilized in Clinical QA, where models predominantly focus on answer extraction while neglecting medical entity classificationâ€”critical for structured EMR retrieval and decision support.


\subsection{Limitations and Contribution}
Despite advancements in QA and domain-adapted transformers, key limitations persist: (1) QA models generate raw text outputs, making structured integration into clinical workflows difficult; (2) existing models either extract answers or classify entities, lacking a unified approach; (3) fine-tuned models struggle with distribution shifts in diverse EMR datasets. To address these gaps, we introduce an MTL framework for Clinical QA that (1) jointly learns answer extraction and medical classification, improving interpretability, (2) integrates domain-adapted transformers like ClinicalBERT with an auxiliary classification head, and (3) enhances generalization by leveraging shared representations, making it more robust for real-world EMR applications.

% In medical AI, MTL has been applied to clinical event detection \cite{li2020multi}, adverse event detection \cite{wei2022multi}, and patient outcome prediction \cite{yoon2022clinical}. Studies have shown that models trained on multiple related clinical tasks perform better than those trained on a single task, particularly in settings where labeled data is limited \cite{liu2022mtlhealth}. For example, multi-task transformers for clinical risk prediction \cite{mohamed2023riskprediction} showed that shared representations improve generalization across patient cohorts.

% Despite its success in healthcare AI, MTL remains underexplored in Clinical QA. Most medical NLP models focus exclusively on answer extraction, failing to leverage auxiliary tasks like medical entity classification. Given that extracted answers in EMRs need structured categorization, a joint QA + classification framework can improve interpretability, usability, and generalization in clinical settings.

% \subsection{Question Answering in NLP}
% Question Answering (QA) is a fundamental task in NLP, evolving from rule-based approaches to neural architectures. Early systems, such as IBM's Watson \cite{ferrucci2010building}, relied on information retrieval techniques. Transformer-based models like BERT \cite{devlin2019bert} revolutionized QA with contextual embeddings, outperforming traditional approaches. Fine-tuned variants, such as BioBERT \cite{lee2020biobert} and ClinicalBERT \cite{alsentzer2019clinicalbert}, further improved domain adaptation for biomedical applications.

% \subsection{Domain Adaptation in Medical NLP}
% Medical NLP faces unique challenges, including specialized terminology and inconsistent documentation. BioBERT \cite{lee2020biobert} and ClinicalBERT \cite{alsentzer2019clinicalbert} improve performance by pretraining on biomedical corpora, but their single-task nature limits generalization. PubMedBERT \cite{gu2021domain} and GatorTron \cite{yang2022large} further refine domain-specific text representations, though they primarily focus on biomedical text mining rather than structured QA.

% \subsection{Multi-Task Learning in Clinical NLP}
% Multi-Task Learning (MTL) has demonstrated effectiveness in various NLP tasks, improving generalization through shared representations \cite{ruder2017overview}. In clinical AI, MTL has been applied to event detection \cite{li2020multi} and adverse event prediction \cite{wei2022multi}, but its use in clinical QA remains underexplored. Our approach extends transformer-based QA by integrating medical entity classification, addressing gaps in structured retrieval and answer categorization.


% \subsection{Limitations of Existing Approaches and Research Gap}
% Despite advancements in QA models and domain-adapted transformers, key limitations remain unaddressed:

% \begin{itemize}
%     \item \textbf{Lack of Structured Answer Representation}: Existing QA models output raw text, making it difficult to integrate extracted information into structured databases or clinical workflows.
%     \item \textbf{Single-Task Learning Constraints}: Current approaches train models for either question answering or classification but do not combine them to maximize efficiency.
%     \item \textbf{Limited Generalization in Real-World Clinical Data}: Models fine-tuned on benchmark datasets often struggle with \textbf{distribution shifts} when applied to heterogeneous EMR records.
% \end{itemize}

% \subsection{Our Contribution}
% To bridge these gaps, this paper introduces a Multi-Task Learning (MTL) framework for Clinical QA that enhances both answer extraction and structured categorization. Unlike traditional fine-tuning methods, our model:

% \begin{enumerate}
%     \item \textbf{Jointly learns answer extraction and medical classification}, improving interpretability.
%     \item \textbf{Utilizes domain-adapted transformer models} (e.g., ClinicalBERT) with an additional classification head.
%     \item \textbf{Improves generalization} by leveraging shared task representations, making the model more robust for real-world EMR processing.
% \end{enumerate}