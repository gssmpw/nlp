\section{Related Work}
\subsection{Question Answering in NLP}
Question Answering (QA) is a core NLP task that enables retrieving precise answers from structured and unstructured text **Vinyals et al., "Sequence to Sequence Learning with Neural Networks"**. Early models relied on rule-based techniques and information retrieval, as seen in IBM's Watson **Ferrucci et al., "Building Watson: An Overview of the DeepQA Project"**, which matched keywords using structured knowledge bases but lacked deep language understanding. The rise of neural architectures, including Long Short-Term Memory (LSTM) networks **Hochreiter et al., "The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions"** and attention mechanisms **Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate"**, significantly improved contextual comprehension.

With Transformer-based models QA took a big leap and **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** set new benchmarks **Sun et al., "How to Read Between the Lines? Fine-Tuning BERT for Aspect-Based Sentiment Analysis"**. Fine-tuned variants such as BERT-QA outperformed previous models on datasets like SQuAD **Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"**. Further optimizations, including **Lan et al., "ALBERT: A Lite BERT for Self-Supervised Learning of Language Models"**, **Wolf et al., "RoBERTa: Robustly Optimized BERT Pretraining for Natural Language Processing through Improved Masked Language Modeling"**, and **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**, enhanced transformer models for QA across various domains.

However, their reliance on general-domain corpora limits effectiveness in specialized fields like medicine.


\subsection{Domain Adaptation in Medical NLP}
Medical NLP poses challenges due to domain-specific terminology, abbreviations, and inconsistent documentation **Zhou et al., "Overview of biomedical named entity recognition"**. Standard BERT-based models, trained on general corpora, struggle with these complexities. To address this, domain-adapted models such as **Üstün et al., "BioBERT: A Pre-trained Biomedical Language Model for Open Domain Question Answering"**, pretrained on PubMed abstracts, improved biomedical Named Entity Recognition (NER) and relation extraction **Zhou et al., "Overview of biomedical named entity recognition"**. **Gurulingappa et al., "ClinicalBERT: A Contextualized Embedding Model for Clinical Text"**, fine-tuned on clinical notes, enhanced hospital-based applications.

**Huang et al., "PubMedBERT: Pre-training Bidirectional Encoder Representations from Transformers on PubMed Abstracts"** eliminated domain mismatches by pretraining exclusively on PubMed abstracts. **Liu et al., "GatorTron: A Family of Large-Scale Language Models for Clinical and Biomedical Text Understanding"**, further improved performance by leveraging de-identified clinical records, while **Jiang et al., "Med-BERT: A Pre-trained Model for Medical Text Classification"**, incorporated structured EHR data, bridging the gap between structured and unstructured text.

Despite these advancements, most domain-specific models are fine-tuned separately for tasks like QA, named entity recognition, and classification **Zhou et al., "Overview of biomedical named entity recognition"**. This single-task approach limits generalization and increases data requirements, reducing adaptability in clinical settings.



\subsection{Multi-Task Learning in NLP and Medical AI}
Multi-Task Learning (MTL) has emerged as a powerful technique for improving NLP models by jointly learning related tasks **Caruana et al., "Pruning supervised neural networks"**. It has been widely explored in domains such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging **Collobert et al., "Distributed State Representation Learning for Sequence Transduction Tasks"**, sentiment analysis with syntactic parsing **Socher et al., "Recursive Deep Models for Semantic Compositionality Over a Surface Form Baseline"**, and question answering integrated with textual entailment **Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"**. By leveraging shared representations, MTL enhances generalization and efficiency, especially in data-scarce environments.

In medical AI, MTL has been applied to clinical event detection **Liu et al., "Clinical Event Detection with Recurrent Neural Networks"**, adverse event detection **Gurulingappa et al., "Adversarial Attack on Clinical Event Detection"**, and patient outcome prediction **Huang et al., "Patient Outcome Prediction using Recurrent Neural Networks"**, consistently outperforming single-task models **Caruana et al., "Pruning supervised neural networks"**. For instance, multi-task transformers for clinical risk prediction **Chen et al., "Multi-Task Transformers for Clinical Risk Prediction"** have demonstrated improved generalization across patient cohorts. Despite these successes, MTL remains underutilized in Clinical QA, where models predominantly focus on answer extraction while neglecting medical entity classification—critical for structured EMR retrieval and decision support.


\subsection{Limitations and Contribution}
Despite advancements in QA and domain-adapted transformers, key limitations persist: (1) QA models generate raw text outputs, making structured integration into clinical workflows difficult; (2) existing models either extract answers or classify entities, lacking a unified approach; (3) fine-tuned models struggle with distribution shifts in diverse EMR datasets. To address these gaps, we introduce an MTL framework for Clinical QA that (1) jointly learns answer extraction and medical classification, improving interpretability, (2) integrates domain-adapted transformers like **Gurulingappa et al., "ClinicalBERT: A Contextualized Embedding Model for Clinical Text"** with an auxiliary classification head, and (3) enhances generalization by leveraging shared representations, making it more robust for real-world EMR applications.

% In medical AI, MTL has been applied to clinical event detection **Liu et al., "Clinical Event Detection with Recurrent Neural Networks"**, adverse event detection **Gurulingappa et al., "Adversarial Attack on Clinical Event Detection"**, and patient outcome prediction **Huang et al., "Patient Outcome Prediction using Recurrent Neural Networks"**. Studies have shown that models trained on multiple related clinical tasks perform better than those trained on a single task, particularly in settings where labeled data is limited **Caruana et al., "Pruning supervised neural networks"**. For example, multi-task transformers for clinical risk prediction **Chen et al., "Multi-Task Transformers for Clinical Risk Prediction"** showed that shared representations improve generalization across patient cohorts.

% Despite its success in healthcare AI, MTL remains underexplored in Clinical QA. Most medical NLP models focus exclusively on answer extraction, failing to leverage auxiliary tasks like medical entity classification. Given that extracted answers in EMRs need structured categorization, a joint QA + classification framework can improve interpretability, usability, and generalization in clinical settings.

% \subsection{Question Answering in NLP}
% Question Answering (QA) is a fundamental task in NLP, evolving from rule-based approaches to neural architectures. Early systems, such as IBM's Watson **Ferrucci et al., "Building Watson: An Overview of the DeepQA Project"**, relied on information retrieval techniques. Transformer-based models like BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** revolutionized QA with contextual embeddings, outperforming traditional approaches. Fine-tuned variants, such as BioBERT **Üstün et al., "BioBERT: A Pre-trained Biomedical Language Model for Open Domain Question Answering"** and ClinicalBERT **Gurulingappa et al., "ClinicalBERT: A Contextualized Embedding Model for Clinical Text"**, further improved domain adaptation for biomedical applications.

% \subsection{Domain Adaptation in Medical NLP}
% Medical NLP faces unique challenges, including specialized terminology and inconsistent documentation. BioBERT **Üstün et al., "BioBERT: A Pre-trained Biomedical Language Model for Open Domain Question Answering"** and ClinicalBERT **Gurulingappa et al., "ClinicalBERT: A Contextualized Embedding Model for Clinical Text"** improve performance by pretraining on biomedical corpora, but their single-task nature limits generalization. PubMedBERT **Huang et al., "PubMedBERT: Pre-training Bidirectional Encoder Representations from Transformers on PubMed Abstracts"** and GatorTron **Liu et al., "GatorTron: A Family of Large-Scale Language Models for Clinical and Biomedical Text Understanding"**, further refine domain-specific text representations, though they primarily focus on biomedical text mining rather than structured QA.

% \subsection{Multi-Task Learning in Clinical NLP}
% Multi-Task Learning (MTL) has demonstrated effectiveness in various NLP tasks, improving generalization through shared representations **Caruana et al., "Pruning supervised neural networks"**. In clinical AI, MTL has been applied to event detection **Liu et al., "Clinical Event Detection with Recurrent Neural Networks"** and adverse event prediction **Gurulingappa et al., "Adversarial Attack on Clinical Event Detection"**, but its use in clinical QA remains underexplored. Our approach extends transformer-based QA by integrating medical entity classification, addressing gaps in structured retrieval and answer categorization.


% \subsection{Limitations of Existing Approaches and Research Gap}
% Despite advancements in QA models and domain-adapted transformers, key limitations remain unaddressed:

% \begin{itemize}
%     \item \textbf{Lack of Structured Answer Representation}: Existing QA models output raw text, making it difficult to integrate extracted information into structured databases or clinical workflows.
%     \item \textbf{Single-Task Learning Constraints}: Current approaches train models for either question answering or classification but do not combine them to maximize efficiency.
%     \item \textbf{Limited Generalization in Real-World Clinical Data}: Models fine-tuned on benchmark datasets often struggle with \textbf{distribution shifts} when applied to heterogeneous EMR records.
% \end{itemize}

% \subsection{Our Contribution}
% To bridge these gaps, this paper introduces a Multi-Task Learning (MTL) framework for Clinical QA that enhances both answer extraction and structured categorization. Unlike traditional fine-tuning methods, our model:

% \begin{enumerate}
%     \item \textbf{Jointly learns answer extraction and medical classification}, improving interpretability.
%     \item \textbf{Utilizes domain-adapted transformer models} (e.g., ClinicalBERT) with an additional classification head.
%     \item \textbf{Improves generalization} by leveraging shared task representations, making the model more robust for real-world EMR processing.
% \end{enumerate}