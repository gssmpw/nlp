\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite,hyperref}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Improving Clinical Question Answering with Multi-Task Learning: A Joint Approach for Answer Extraction and Medical Categorization\\
}
% \author{\IEEEauthorblockN{ Anonymous EIT Submission}}

\author{\IEEEauthorblockN{1\textsuperscript{st} Priyaranjan Pattnayak}
\IEEEauthorblockA{\textit{University of Washington} \\
Seattle \\
% ppattnay@uw.edu
}\\
\IEEEauthorblockN{2\textsuperscript{nd} Hitesh Patel}
\IEEEauthorblockA{\textit{New York University} \\
New York \\
% hitesh.patel945@gmail.com
}
\and

\IEEEauthorblockN{3\textsuperscript{rd} Amit Agarwal}
\IEEEauthorblockA{\textit{Liverpool John Moores University} \\
Liverpool \\
% amit.pinaki@gmail.com 
}\\

\IEEEauthorblockN{4\textsuperscript{th} Srikant Panda}
\IEEEauthorblockA{\textit{Birla Institute of Technology} \\
Pilani \\
% srikant86.panda@gmail.com
}

\and
\hfill
\IEEEauthorblockN{5\textsuperscript{th} Bhargava Kumar}
\IEEEauthorblockA{\textit{Columbia University} \\
New York \\
% bhargava1409@gmail.com
}\\
\IEEEauthorblockN{6\textsuperscript{th} Tejaswini Kumar}
\IEEEauthorblockA{\textit{Columbia University} \\
New York \\
% hitesh.patel945@gmail.com
}
}

\maketitle

\begin{abstract}
Clinical Question Answering (CQA) plays a crucial role in medical decision-making, enabling physicians to extract relevant information from Electronic Medical Records (EMRs). While transformer-based models such as BERT, BioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in CQA, existing models lack the ability to categorize extracted answers, which is critical for structured retrieval, content filtering, and medical decision support. 

To address this limitation, we introduce a Multi-Task Learning (MTL) framework that jointly trains CQA models for both answer extraction and medical categorization. In addition to predicting answer spans, our model classifies responses into five standardized medical categories: \textbf{Diagnosis, Medication, Symptoms, Procedure, and Lab Reports}. This categorization enables more structured and interpretable outputs, making clinical QA models more useful in real-world healthcare settings.

We evaluate our approach on emrQA, a large-scale dataset for medical question answering. Results show that MTL improves F1-score by 2.2\% compared to standard fine-tuning, while achieving 90.7\% accuracy in answer categorization. These findings suggest that MTL not only enhances CQA performance but also introduces an effective mechanism for categorization and structured medical information retrieval.
\end{abstract}

\begin{IEEEkeywords}
Clinical NLP, QA, Multi-Task Learning, Electronic Medical Records, Medical Categorization, ClinicalBERT.
\end{IEEEkeywords}

\section{Introduction}
The rapid digitization of healthcare records has led to an explosion of unstructured textual data in Electronic Medical Records (EMRs) \cite{rasmy2021medbert}. Physicians and healthcare professionals frequently need to extract patient-specific information from these records, which often consist of lengthy and heterogeneous clinical narratives. Clinical Question Answering (CQA) has emerged as a powerful Natural Language Processing (NLP) technique that enables automated retrieval of relevant information from EMRs \cite{lehman2023generalization}. However, deploying QA models in real-world healthcare settings presents significant challenges, including the need for domain adaptation \cite{Pattnayak2017Rainfall}, interpretability, and structured information retrieval.

\subsection{Challenges in Clinical Question Answering}
Despite advancements in deep learning-based QA systems, current approaches struggle with several key challenges:

\begin{itemize}
    \item \textbf{Unstructured Nature of EMRs}: Clinical notes are typically free-text narratives, written in varying styles by different healthcare providers \cite{agarwal2024enhancing}. Unlike structured databases, these records lack consistent formatting, making it difficult for QA models to retrieve answers accurately \cite{wagner2022towards}. Extracted answers often require additional post-processing to be useful in downstream applications.

    \item \textbf{Domain Adaptation Limitations}: General NLP models, such as BERT \cite{devlin2019bert}, are pre-trained on open-domain corpora like Wikipedia and BooksCorpus, making them suboptimal for processing medical text. This has led to the development of domain-specific models such as BioBERT \cite{lee2020biobert}, ClinicalBERT \cite{alsentzer2019clinicalbert}, and PubMedBERT \cite{gu2021domain}, which improve performance by pretraining on biomedical and clinical corpora. However, even these models struggle with capturing the complexities of specialized medical terminology and abbreviations found in EMRs.

    \item \textbf{Lack of Answer Categorization}: Traditional QA models return free-text answers but do not categorize them into structured medical entities. For example, when asked, \textit{"What medications has the patient been prescribed?"}, a standard QA model might return \textit{"Metformin 500 mg, daily"}. However, without explicit categorization, the extracted text cannot be easily integrated into structured EMR systems for further analysis \cite{zhang2022can}. Physicians and clinical decision-support tools often require additional context, such as whether the answer pertains to a \textbf{diagnosis, medication, symptom, procedure, or lab report}.
    
    \item \textbf{Scalability and Generalization}: Existing QA models require extensive fine-tuning on task-specific datasets to achieve reasonable performance. This limits their ability to generalize across different medical institutions, patient populations, and EMR systems.
\end{itemize}

\subsection{Proposed Approach: Multi-Task Learning for Clinical QA}
To address these challenges, we introduce a Multi-Task Learning (MTL) framework that extends Clinical QA models by jointly learning answer extraction and medical categorization. Our approach provides three key contributions:

\begin{enumerate}
    \item \textbf{Multi-Task Learning for Clinical QA}: We enhance existing QA models by adding an auxiliary classification head that categorizes extracted answers into five key medical categories: \textbf{Diagnosis, Medication, Symptoms, Procedures, and Lab Reports}. This allows structured answer retrieval, making it easier for physicians to interpret results.
    
    \item \textbf{Standardized Medical Categorization}: Unlike traditional QA systems that return free-text answers, our method incorporates medical entity recognition using the Unified Medical Language System (UMLS) \cite{bodenreider2004unified}. By aligning extracted answers with standardized medical concepts, our approach ensures interoperability with clinical decision-support tools.

    \item \textbf{Empirical Performance Gains}: We evaluate our model on the emrQA dataset \cite{pampari2018emrqa}, a large-scale corpus for medical QA. Our results demonstrate that the MTL-based approach improves F1-score by 2.2\% over single-task fine-tuning while achieving 90.7\% classification accuracy. This suggests that integrating medical categorization with QA enhances both performance and usability in real-world clinical settings.
\end{enumerate}

\subsection{Applications and Impact}
The proposed MTL-based clinical QA model has several practical applications:
\begin{itemize}
    \item \textbf{Structured EMR Search}: By categorizing extracted answers, our model enables physicians to retrieve information in a structured manner. Instead of searching through unstructured clinical notes, a user could query, \textit{"Show me all past diagnoses for this patient"} and receive a structured list.
    
    \item \textbf{Clinical Decision Support}: Categorized answers can be integrated into clinical decision-support systems, helping physicians make more informed choices by filtering relevant medical information.
    
    \item \textbf{Medical Coding and Billing}: Healthcare providers often need to categorize clinical notes into standardized billing codes (e.g., ICD-10). Our approach could facilitate automated categorization of extracted medical concepts, reducing the manual workload for healthcare professionals.
    
    \item \textbf{Content Moderation in Clinical AI}: AI-powered medical assistants must ensure that generated answers adhere to regulatory and ethical standards. Categorization enables rule-based filtering, helping to identify and flag inappropriate or harmful AI-powered medical assistants responses.
\end{itemize}


\section{Related Work}

\subsection{Question Answering in NLP}
Question Answering (QA) is a core NLP task that enables retrieving precise answers from structured and unstructured text \cite{jurafsky2023speech, agarwal2021evaluate}. Early models relied on rule-based techniques and information retrieval, as seen in IBM's Watson \cite{ferrucci2010building}, which matched keywords using structured knowledge bases but lacked deep language understanding. The rise of neural architectures, including Long Short-Term Memory (LSTM) networks \cite{hochreiter1997long} and attention mechanisms \cite{vaswani2017attention}, significantly improved contextual comprehension.

With Transformer-based models QA took a big leap and \textbf{BERT} \cite{devlin2019bert} set new benchmarks \cite{agarwal2024mvtamperbench}. Fine-tuned variants such as BERT-QA outperformed previous models on datasets like SQuAD \cite{rajpurkar2018know}. Further optimizations, including \textbf{ALBERT} \cite{lan2020albert}, \textbf{RoBERTa} \cite{liu2019roberta}, and \textbf{T5} \cite{raffel2020exploring}, enhanced transformer models for QA across various domains \cite{pattnayak2024survey}. However, their reliance on general-domain corpora limits effectiveness in specialized fields like medicine.


\subsection{Domain Adaptation in Medical NLP}
Medical NLP poses challenges due to domain-specific terminology, abbreviations, and inconsistent documentation \cite{agarwal-etal-2025-fs}. Standard BERT-based models, trained on general corpora, struggle with these complexities. To address this, domain-adapted models such as \textbf{BioBERT} \cite{lee2020biobert}, pretrained on PubMed abstracts, improved biomedical Named Entity Recognition (NER) and relation extraction \cite{Pattnayak2017AutoSales}. \textbf{ClinicalBERT} \cite{alsentzer2019clinicalbert}, fine-tuned on clinical notes, enhanced hospital-based applications.

\textbf{PubMedBERT} \cite{gu2021domain} eliminated domain mismatches by pretraining exclusively on PubMed abstracts. \textbf{GatorTron} \cite{yang2022large} further improved performance by leveraging de-identified clinical records, while \textbf{Med-BERT} \cite{huang2021clinical} incorporated structured EHR data, bridging the gap between structured and unstructured text.

Despite these advancements, most domain-specific models are fine-tuned separately for tasks like QA, named entity recognition, and classification \cite{agarwal2024domain}. This single-task approach limits generalization and increases data requirements, reducing adaptability in clinical settings.



\subsection{Multi-Task Learning in NLP and Medical AI}
Multi-Task Learning (MTL) has emerged as a powerful technique for improving NLP models by jointly learning related tasks \cite{ruder2017overview}. It has been widely explored in domains such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging \cite{liu2019multi}, sentiment analysis with syntactic parsing \cite{sogaard2016deep}, and question answering integrated with textual entailment \cite{clark2019boolq}. By leveraging shared representations, MTL enhances generalization and efficiency, especially in data-scarce environments.

In medical AI, MTL has been applied to clinical event detection \cite{li2020multi}, adverse event detection \cite{wei2022multi}, and patient outcome prediction \cite{yoon2022clinical}, consistently outperforming single-task models \cite{liu2022mtlhealth}. For instance, multi-task transformers for clinical risk prediction \cite{mohamed2023riskprediction} have demonstrated improved generalization across patient cohorts. Despite these successes, MTL remains underutilized in Clinical QA, where models predominantly focus on answer extraction while neglecting medical entity classification—critical for structured EMR retrieval and decision support.


\subsection{Limitations and Contribution}
Despite advancements in QA and domain-adapted transformers, key limitations persist: (1) QA models generate raw text outputs, making structured integration into clinical workflows difficult; (2) existing models either extract answers or classify entities, lacking a unified approach; (3) fine-tuned models struggle with distribution shifts in diverse EMR datasets. To address these gaps, we introduce an MTL framework for Clinical QA that (1) jointly learns answer extraction and medical classification, improving interpretability, (2) integrates domain-adapted transformers like ClinicalBERT with an auxiliary classification head, and (3) enhances generalization by leveraging shared representations, making it more robust for real-world EMR applications.

% In medical AI, MTL has been applied to clinical event detection \cite{li2020multi}, adverse event detection \cite{wei2022multi}, and patient outcome prediction \cite{yoon2022clinical}. Studies have shown that models trained on multiple related clinical tasks perform better than those trained on a single task, particularly in settings where labeled data is limited \cite{liu2022mtlhealth}. For example, multi-task transformers for clinical risk prediction \cite{mohamed2023riskprediction} showed that shared representations improve generalization across patient cohorts.

% Despite its success in healthcare AI, MTL remains underexplored in Clinical QA. Most medical NLP models focus exclusively on answer extraction, failing to leverage auxiliary tasks like medical entity classification. Given that extracted answers in EMRs need structured categorization, a joint QA + classification framework can improve interpretability, usability, and generalization in clinical settings.

% \subsection{Question Answering in NLP}
% Question Answering (QA) is a fundamental task in NLP, evolving from rule-based approaches to neural architectures. Early systems, such as IBM's Watson \cite{ferrucci2010building}, relied on information retrieval techniques. Transformer-based models like BERT \cite{devlin2019bert} revolutionized QA with contextual embeddings, outperforming traditional approaches. Fine-tuned variants, such as BioBERT \cite{lee2020biobert} and ClinicalBERT \cite{alsentzer2019clinicalbert}, further improved domain adaptation for biomedical applications.

% \subsection{Domain Adaptation in Medical NLP}
% Medical NLP faces unique challenges, including specialized terminology and inconsistent documentation. BioBERT \cite{lee2020biobert} and ClinicalBERT \cite{alsentzer2019clinicalbert} improve performance by pretraining on biomedical corpora, but their single-task nature limits generalization. PubMedBERT \cite{gu2021domain} and GatorTron \cite{yang2022large} further refine domain-specific text representations, though they primarily focus on biomedical text mining rather than structured QA.

% \subsection{Multi-Task Learning in Clinical NLP}
% Multi-Task Learning (MTL) has demonstrated effectiveness in various NLP tasks, improving generalization through shared representations \cite{ruder2017overview}. In clinical AI, MTL has been applied to event detection \cite{li2020multi} and adverse event prediction \cite{wei2022multi}, but its use in clinical QA remains underexplored. Our approach extends transformer-based QA by integrating medical entity classification, addressing gaps in structured retrieval and answer categorization.


% \subsection{Limitations of Existing Approaches and Research Gap}
% Despite advancements in QA models and domain-adapted transformers, key limitations remain unaddressed:

% \begin{itemize}
%     \item \textbf{Lack of Structured Answer Representation}: Existing QA models output raw text, making it difficult to integrate extracted information into structured databases or clinical workflows.
%     \item \textbf{Single-Task Learning Constraints}: Current approaches train models for either question answering or classification but do not combine them to maximize efficiency.
%     \item \textbf{Limited Generalization in Real-World Clinical Data}: Models fine-tuned on benchmark datasets often struggle with \textbf{distribution shifts} when applied to heterogeneous EMR records.
% \end{itemize}

% \subsection{Our Contribution}
% To bridge these gaps, this paper introduces a Multi-Task Learning (MTL) framework for Clinical QA that enhances both answer extraction and structured categorization. Unlike traditional fine-tuning methods, our model:

% \begin{enumerate}
%     \item \textbf{Jointly learns answer extraction and medical classification}, improving interpretability.
%     \item \textbf{Utilizes domain-adapted transformer models} (e.g., ClinicalBERT) with an additional classification head.
%     \item \textbf{Improves generalization} by leveraging shared task representations, making the model more robust for real-world EMR processing.
% \end{enumerate}


\section{Dataset and Preprocessing}
\subsection{emrQA Dataset}
We use the emrQA dataset \cite{pampari2018emrqa}, a large-scale corpus designed for Clinical Question Answering (QA) over Electronic Medical Records (EMRs). emrQA was derived from the i2b2 challenge datasets and provides approximately 455,837 QA pairs generated from structured and unstructured clinical documents. 

Unlike traditional QA datasets, emrQA includes both direct retrieval questions (e.g., "What is the patient’s blood pressure?") and inferential questions (e.g., "What medications should be monitored for this patient?").  However, the original dataset does not explicitly label each QA pair with a standardized medical category. To bridge this gap, we apply Named Entity Recognition (NER) using UMLS and SciSpacy to categorize extracted answers into one of the five predefined medical categories.




\subsection{Preprocessing for Multi-Task Learning}
Since emrQA was originally structured for span-based QA, we preprocess it for our joint QA and classification framework:
\begin{algorithm}[H]
\caption{Preprocessing Pipeline for Multi-Task Learning}
\label{alg:preprocessing}
\begin{algorithmic}[1]
\REQUIRE emrQA dataset $\mathcal{D} = \{(Q, C, A)\}$ where $Q$ = Question, $C$ = Context, $A$ = Answer span.
\ENSURE Preprocessed dataset with medical categories.

\FOR{each $(Q, C, A) \in \mathcal{D}$}
    \STATE \textbf{Tokenization:} Apply WordPiece tokenization to $(Q, C, A)$.
    \STATE \textbf{Answer Span Extraction:} Identify $A$’s position in $C$.
    \STATE \textbf{Named Entity Recognition (NER):} Extract medical entities from $A$ using SciSpacy + UMLS.
    \STATE \textbf{Assign Medical Category:}
        \STATE \quad \textbf{Diagnosis} if entity $\in$ UMLS Diagnosis Terminology.
        \STATE \quad \textbf{Medication} if entity $\in$ RxNorm.
        \STATE \quad \textbf{Symptoms, Procedure, or Lab Reports} if entity $\in$ respective medical ontology.
    \STATE \textbf{Dataset Splitting:} Partition into $\mathcal{D}_{train}$ (80\%), $\mathcal{D}_{val}$ (10\%), $\mathcal{D}_{test}$ (10\%).
\ENDFOR
\RETURN Preprocessed dataset $\mathcal{D}_{final} = \{(Q, C, A, \text{Label})\}$
\end{algorithmic}
\end{algorithm}




\begin{itemize}
    \item Tokenization: We apply WordPiece tokenization from BERT to ensure compatibility with transformer-based models.
    \item Answer Span Extraction: Each answer is mapped to its exact position within the clinical text for supervised learning.
    \item Named Entity Recognition (NER) Labeling: We use the Unified Medical Language System (UMLS) \cite{bodenreider2004unified, patel2024llm} and SciSpacy for entity recognition. Extracted answers are assigned one of five medical categories: diagnosis, medication, symptoms, procedures, or lab reports.
    \item Handling Class Imbalance: As shown in Table \ref{tab:dataset_statistics}, some categories (e.g., diagnoses) contain significantly more QA pairs than others (e.g., lab reports). We apply class-weighted loss during training and perform oversampling in underrepresented categories to mitigate imbalance.
    \item Dataset Splitting: The dataset is partitioned into 80\% training, 10\% validation, and 10\% test for unbiased evaluation.
\end{itemize}

\subsection{Dataset Statistics and Class Distribution}
Table \ref{tab:dataset_statistics} presents the dataset distribution across different medical categories.

\begin{table}[h]
\centering
\caption{emrQA Dataset Statistics after NER Labeled Categories}
\label{tab:dataset_statistics}
\begin{tabular}{|c|c|}
\hline
\textbf{Category} & \textbf{QA Pairs} \\
\hline
Diagnoses & 141,243 \\
Medications & 255,908 \\
Procedures & 20,540 \\
Symptoms & 23,474 \\
Lab Reports & 14,672 \\
\hline
\textbf{Total} & 455,837 \\
\hline
\end{tabular}
\end{table}


\subsection{Example Data Format and Tokenized Representation}
Table \ref{tab:example_data} illustrates an example input format, showing how a clinical QA pair is structured along with tokenized representation.

\begin{table}[h]
\centering
\caption{Example Data Format for QA and Classification}
\label{tab:example_data}
\begin{tabular}{|c|p{5cm}|}
\hline
Field & Example \\
\hline
Question & What medication was prescribed for hypertension? \\
\hline
Context (EMR Text) & The patient was diagnosed with hypertension and prescribed Lisinopril 10mg daily. \\
\hline
Answer (Span) & Lisinopril 10mg daily \\
\hline
Classification Label & Medication \\
\hline
Tokenized Input & \texttt{[CLS] What medication was prescribed for hypertension? [SEP] The patient was diagnosed with hypertension and prescribed Lisinopril 10mg daily. [SEP]} \\
\hline
\end{tabular}
\end{table}

\subsection{Justification for Using emrQA}
The emrQA dataset is particularly well-suited for this study for the following reasons:

\begin{itemize}
    \item Multi-task potential: The dataset structure supports both answer span extraction and medical classification, making it ideal for MTL.
    \item Clinical authenticity: emrQA is derived from real clinical notes rather than synthetic data, ensuring relevance to real-world applications.
    \item Diverse question types: The dataset contains both retrieval-based and inference-based questions, allowing models to handle factual lookups and complex reasoning.
\end{itemize}

By structuring emrQA for joint answer extraction and medical classification, we enable a more interpretable and structured clinical QA framework.

\section{Model Architecture}
% \subsection{Overview}
% The proposed model extends transformer-based Clinical Question Answering (QA) models by integrating Multi-Task Learning (MTL) to enhance structured information retrieval. The model simultaneously performs:

\subsection{Overview}
The proposed model extends transformer-based Clinical Question Answering (QA) models by integrating Multi-Task Learning (MTL) to enhance structured information retrieval. The model simultaneously performs:

\begin{itemize}
    \item \textbf{Answer Extraction}: Identifying the most relevant answer span within the clinical text.
    \item \textbf{Medical Entity Classification}: Categorizing the extracted answer into predefined medical categories: Diagnosis, Medication, Symptoms, Procedures, and Lab Reports.
\end{itemize}

We adopt a ClinicalBERT-based architecture with 12 transformer layers, 768 hidden units per layer, and 110M trainable parameters. The model consists of:

\begin{itemize}
    \item A shared transformer encoder that produces contextualized token representations.
    \item A span prediction head (2 fully connected layers, \textit{512, 256 units}, ReLU activation) for answer extraction.
    \item A classification head (3 fully connected layers, \textit{512, 256, 5 units}, Softmax activation) for medical entity prediction.
\end{itemize}

Both heads share lower-layer representations while upper layers are task-specific, reducing parameter redundancy and improving multi-task efficiency. By jointly training these tasks, the model improves both answer relevance and interpretability in clinical decision-support systems.

\subsection{Transformer Model Selection}
We evaluate two domain-specific transformer models:

\begin{itemize}
    \item BioBERT \cite{lee2020biobert}, pretrained on biomedical literature, optimized for general biomedical NLP tasks.
    \item ClinicalBERT \cite{alsentzer2019clinicalbert}, trained on MIMIC-III clinical notes, suitable for processing electronic health records (EHRs).
\end{itemize}

ClinicalBERT is the primary model due to its alignment with clinical narratives. It captures domain-specific terminology, abbreviations, and structured note-writing styles found in EHRs.  

BioBERT, which is trained on biomedical literature from PubMed and PMC, serves as a secondary baseline to assess generalization performance. BioBERT is not exposed to real-world clinical text, which often contains informal phrasing and abbreviations unique to doctor-patient interactions. This comparison allows us to evaluate the impact of domain-specific pretraining when applied to QA tasks involving real-world clinical notes.

\subsection{Multi-Task Learning Setup}
The transformer encoder produces contextualized embeddings that are passed to two task-specific heads:

\begin{itemize}
    \item A span prediction module for answer extraction.
    \item A classification module for medical entity categorization.
\end{itemize}

\subsection{Handling Task Interference}
To prevent gradients from one task from negatively affecting the other, we implement layer-wise attention masking, a technique inspired by task-specific adapter layers \cite{Stickland2019BAM, Houlsby2019Adapter}. 

In this setup:
\begin{itemize}
    \item The lower 6 layers of the transformer remain shared, capturing general clinical text representations.
    \item The upper 6 layers are task-specific, with independent attention mechanisms for QA and classification.
    \item Attention masking is applied at the task-specific layers to prevent cross-task gradient interference, ensuring that QA-specific gradients do not distort classification representations.
\end{itemize}

This mechanism ensures that shared lower layers generalize across both tasks, while upper layers specialize in their respective tasks, mitigating negative transfer effects in multi-task learning \cite{Ruder2017MTL}.


% \subsection{Handling Answer Ambiguity}
% In clinical QA, a question may have multiple valid answers. To account for this:

% \begin{itemize}
%     \item Multiple answer spans per question are considered where applicable.
%     \item Soft classification labels are assigned when an answer spans multiple medical categories.
% \end{itemize}

\subsection{Handling Answer Ambiguity}
In clinical settings, extracted answers may belong to multiple medical categories. For instance, "Metformin 500mg" can be classified as both Medication (a prescribed drug) and Lab Report (if extracted from test results). 

To address this, we adopt soft classification labels, a technique widely used in multi-label classification \cite{Bengio2015SoftLabels, Joulin2017Softmax}. Instead of a single category per answer, the model assigns probabilities to multiple categories. This is achieved using sigmoid activation rather than softmax:

\begin{equation}
    P(y|x) = \sigma(W_h f(x) + b)
\end{equation}

where \( f(x) \) is the feature representation from ClinicalBERT, and \( \sigma \) ensures multi-label assignments. During training, we minimize Binary Cross-Entropy Loss (BCE Loss), allowing the model to handle ambiguous classifications effectively.


\subsection{Loss Function and Optimization}
A weighted loss function is used to balance the QA and classification tasks:

\begin{equation}
    \mathcal{L} = \lambda_{\text{QA}} \mathcal{L}_{\text{QA}} + \lambda_{\text{Class}} \mathcal{L}_{\text{Class}},
\end{equation}

where:
\begin{itemize}
    \item $\mathcal{L}_{\text{QA}}$ is the negative log-likelihood loss for answer span prediction.
    \item $\mathcal{L}_{\text{Class}}$ is the cross-entropy loss for classification.
    \item $\lambda_{\text{QA}}$ and $\lambda_{\text{Class}}$ control the contribution of each task.
\end{itemize}

The values of $\lambda_{\text{QA}}$ and $\lambda_{\text{Class}}$ are selected using grid search, optimizing for both QA F1-score and classification accuracy.

\subsection{Training and Evaluation Metrics}
The model is trained using AdamW with a learning rate of $5 \times 10^{-5}$ for 5 epochs. The training process includes:

\begin{itemize}
    \item Learning rate warm-up for the first 10\% of training steps, followed by linear decay.
    \item Early stopping based on validation loss to prevent overfitting.
\end{itemize}

We select evaluation metrics tailored to the dual-task nature of the model:

\begin{itemize}
    \item F1-score for answer span extraction, as it balances precision and recall, which is critical in medical QA where partial answers may still be relevant.
    \item Exact Match (EM) to measure strict correctness in extracted spans. While this is a standard metric for QA, it is less suitable for medical QA since minor phrasing differences (e.g., "Tylenol 500mg" vs. "Tylenol") may still be clinically correct.
    \item Accuracy and weighted F1-score for classification. Since medical entity categories are imbalanced (e.g., more diagnoses than lab results), weighted F1 prevents minority classes from being ignored.
\end{itemize}

F1-score is prioritized over accuracy for answer extraction because, in medical settings, the model should aim to maximize recall (retrieving clinically important information) while maintaining precision.

\subsection{Algorithm: Multi-Task Learning for Clinical QA}
The training process is formalized in Algorithm \ref{alg:mtl_qa}.

\begin{algorithm}[h]
\caption{Multi-Task Learning for Clinical QA}
\label{alg:mtl_qa}
\begin{algorithmic}[1]
\REQUIRE Transformer model (ClinicalBERT), training dataset $\mathcal{D}$
\STATE Initialize model parameters $\theta$
\FOR{each epoch}
    \FOR{each batch $(q, d, a, y) \in \mathcal{D}$}
        \STATE Compute transformer embeddings: $\mathbf{H} \leftarrow \text{Transformer}(q, d)$
        \STATE Predict answer span $(P_{\text{start}}, P_{\text{end}})$
        \STATE Predict classification label: $P_{\text{class}}$
        \STATE Compute multi-task loss:
        \STATE $\mathcal{L} = \lambda_{\text{QA}} \mathcal{L}_{\text{QA}} + \lambda_{\text{Class}} \mathcal{L}_{\text{Class}}$
        \STATE Update parameters using AdamW
    \ENDFOR
\ENDFOR
\RETURN Optimized model parameters $\theta$
\end{algorithmic}
\end{algorithm}




\section{Experimental Results}
\subsection{Overview}
We evaluate the performance of our Multi-Task Learning (MTL) framework on the emrQA dataset using two domain-specific transformer models: BioBERT and ClinicalBERT. We measure performance on two tasks: answer span extraction and medical entity classification.

\subsection{Performance Comparison: BioBERT vs. ClinicalBERT}
Table \ref{tab:model_comparison} presents the performance of BioBERT and ClinicalBERT on the two tasks. ClinicalBERT consistently outperforms BioBERT across all metrics due to its pretraining on electronic health records.

% \begin{table}[h]
% \centering
% \caption{Performance Comparison: BioBERT vs. ClinicalBERT}
% \label{tab:model_comparison}
% \begin{tabular}{lcc|cc}
% \hline
% \multirow{Metric} & \multicolumn{2}{c|}{Answer Extraction} & \multicolumn{2}{c}{Entity Classification} \\ 
% \vspace{0.25pt}
% \cline{2-5}
% \vspace{0.25pt}
% & BioBERT & ClinicalBERT & BioBERT & ClinicalBERT \\
% \hline
% F1-score & 76.3 & \textbf{81.7} & 79.5 & \textbf{85.2} \\
% Recall & 74.9 & \textbf{79.8} & 77.2 & \textbf{83.6} \\
% Exact Match & 64.2 & \textbf{69.5} & - & - \\
% Accuracy & - & - & 82.4 & \textbf{90.7} \\
% Weighted F1 & - & - & 78.9 & \textbf{84.7} \\
% \hline
% \end{tabular}
% \end{table}

\begin{table}[h]
\centering
\caption{Performance Comparison: BioBERT vs. ClinicalBERT}
\label{tab:model_comparison}
\begin{tabular}{lcc|cc}
\hline
Metric & \multicolumn{2}{c|}{Answer Extraction} & \multicolumn{2}{c}{Entity Classification} \\ 
\hline
 & BioBERT & ClinicalBERT & BioBERT & ClinicalBERT \\
\hline
F1-score & 76.3 & \textbf{81.7} & 79.5 & \textbf{85.2} \\
Recall & 74.9 & \textbf{79.8} & 77.2 & \textbf{83.6} \\
Exact Match & 64.2 & \textbf{69.5} & - & - \\
Accuracy & - & - & 82.4 & \textbf{90.7} \\
Weighted F1 & - & - & 78.9 & \textbf{84.7} \\
\hline
\end{tabular}
\end{table}




% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.48\textwidth]{biobertvclinicalbert1.png}
%     \caption{Performance Comparison of BioBERT vs. ClinicalBERT in Answer Extraction F1 Score (in Yellow) and Entity Classification Accuracy (in Orange).}
%     \label{fig:performance_comparison}
% \end{figure}

\subsubsection{Discussion}
The results confirm that ClinicalBERT outperforms BioBERT across all evaluation metrics, particularly in answer extraction (F1-score: 81.7 vs. 76.3) and classification accuracy (90.7 vs. 82.4) as shown in Table \ref{tab:model_comparison}. These improvements stem from ClinicalBERT’s exposure to structured clinical narratives, which helps in recognizing domain-specific terminology and abbreviations.

Higher recall in answer extraction (79.8\% vs. 74.9\%) suggests that ClinicalBERT better identifies relevant spans, but the exact match score remains lower due to paraphrased variations in answers. In entity classification, the weighted F1-score improves by nearly 6\% with ClinicalBERT, reinforcing its advantage in domain-specific medical text processing.


\subsection{Ablation Study: Impact of Multi-Task Learning vs. Standard Fine-Tuning}
To assess the contribution of multi-task learning, we compare it to a standard fine-tuning approach where ClinicalBERT is trained solely on QA without multi-task objectives.

\begin{table}[ht]
\centering
\caption{Ablation Study: MTL vs. Standard Fine-Tuning}
\label{tab:mtl_vs_finetune}
\begin{tabular}{p{3cm}l p{2cm}c p{1.8cm}c}
\hline
Configuration & Answer Extraction (F1) & Classification Accuracy \\
\hline
Fine-Tuned ClinicalBERT (QA only) & 79.5 & 88.5 \\
Multi-Task Learning (QA + Classification) & \textbf{81.7} & \textbf{90.7} \\
\hline
\end{tabular}
\end{table}


\subsubsection{Discussion}
The multi-task model outperforms standard fine-tuning by \textbf{2.2\%} in F1-score for answer extraction and achieves \textbf{90.7\%} accuracy in entity classification, confirming the effectiveness of MTL. These improvements indicate that shared representations between tasks provide additional contextual cues, leading to better span prediction and more robust entity recognition.

The improved classification accuracy also demonstrates MTL’s potential for content moderation and structured information retrieval, as it refines answer categorization and entity structuring in clinical datasets.

\subsection{Class-Wise Entity Classification Performance}
To further analyze classification performance across medical categories, Fig. \ref{fig:classwise_performance} presents precision and recall per entity type.

% \begin{table}[h]
% \centering
% \caption{Class-Wise Performance (ClinicalBERT)}
% \label{tab:classwise_perf}
% \begin{tabular}{lccc}
% \hline
% Category & Precision & Recall & F1-score \\
% \hline
% Diagnoses & 87.2 & 91.4 & 89.2 \\
% Medications & 85.1 & 89.5 & 87.2 \\
% Procedures & 80.3 & 84.2 & 82.2 \\
% Symptoms & 78.6 & 82.7 & 80.6 \\
% Lab Reports & 74.8 & 78.3 & 76.5 \\
% \hline
% \end{tabular}
% \end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{classwiseperformance1.png}
    \caption{Class-Wise Entity Classification Performance for ClinicalBERT.}
    \label{fig:classwise_performance}
\end{figure}

\subsubsection{Discussion}
Lab-related entities exhibit lower recall (78.3\%), likely due to inconsistent naming conventions and variations in reporting formats. In contrast, diagnoses and medications achieve the highest precision and recall, suggesting that these categories benefit from clearer contextual patterns in clinical text.


\subsection{Error Analysis}
Table \ref{tab:error_analysis} categorizes common errors in model predictions.

\begin{table}[h]
\centering
\caption{Error Analysis}
\label{tab:error_analysis}
\begin{tabular}{lc}
\hline
Error Type & Frequency (\%) \\
\hline
Ambiguous Answers & 28.3 \\
Span Boundary Errors & 23.1 \\
Incorrect Entity Classification & 18.7 \\
OOV Medical Terms & 15.6 \\
Others & 14.3 \\
\hline
\end{tabular}
\end{table}

\subsubsection{Discussion}
Ambiguous answers (28.3\%) are the most frequent error, often due to multiple valid responses. Span boundary mismatches (23.1\%) highlight the importance of annotation consistency. Incorrect entity classification (18.7\%) suggests that incorporating external medical ontologies, such as UMLS, could further enhance classification accuracy.




% \section{Conclusion}
% This study introduced a Multi-Task Learning (MTL) framework for Clinical Question Answering (CQA), demonstrating that jointly training for answer extraction and medical entity classification enhances performance. The proposed approach leverages domain-specific pretraining and shared representations, making it more effective for real-world clinical NLP applications.

% \subsection{Key Findings}
% Experimental results confirm that ClinicalBERT significantly outperforms BioBERT in clinical QA tasks, reinforcing the benefits of domain adaptation. Multi-task learning improves both answer extraction and entity classification by facilitating shared representations across tasks. Compared to standard fine-tuning, MTL improves answer extraction F1-score by 2.2 percent and achieves 90.7 percent classification accuracy, validating its effectiveness.  

% The class-wise evaluation of medical entity classification highlights that diagnoses and medications achieve the highest precision and recall, while lab reports exhibit lower recall, suggesting challenges in recognizing structured test names. Error analysis indicates that ambiguous answers, span boundary mismatches, and incorrect entity classification remain common challenges, pointing to areas for further refinement.

% \subsection{Limitations and Future Research}
% Despite these improvements, certain limitations remain. The reliance on Exact Match (EM) scoring reduces the model’s ability to handle paraphrased clinical queries, leading to lower recall in answer extraction. Future research could explore paraphrase-aware training techniques or retrieval-augmented approaches to improve answer generalization.

% Additionally, while ClinicalBERT benefits from pretraining on electronic health records (EHRs), it lacks explicit integration with external medical ontologies such as UMLS or SNOMED CT. Incorporating structured medical knowledge could enhance entity classification and ambiguity resolution, improving the model’s interpretability and robustness.

% Further work could also explore contrastive learning techniques to improve the model’s resilience against variations in clinical terminology. Expanding this approach to multilingual clinical QA would improve its applicability in global healthcare settings, ensuring accessibility for diverse patient populations.

% \subsection{Broader Impact}
% The findings from this study contribute to advancements in clinical NLP and AI-driven decision support. By improving structured information retrieval from unstructured medical text, the proposed framework provides a scalable solution for enhancing access to patient information. These models can be integrated into electronic health record (EHR) systems to assist clinicians in retrieving critical insights from large-scale patient data more efficiently.

% As AI-driven systems continue to be embedded into healthcare workflows, transformer-based multi-task learning approaches will play an essential role in improving clinical efficiency and decision-making. The integration of domain-adapted language models into clinical QA tasks represents a step toward more interpretable, reliable, and effective AI applications in healthcare.

\section{Conclusion}
This study introduced a Multi-Task Learning (MTL) framework for Clinical Question Answering (CQA), integrating answer extraction with medical entity classification. Our approach enhances structured information retrieval, improving both accuracy and interpretability in clinical decision support.

Empirical results on emrQA demonstrate that MTL improves F1-score by 2.2\% over standard fine-tuning while achieving 90.7\% classification accuracy. By leveraging shared representations, our model mitigates negative transfer effects, ensuring robust performance across diverse medical queries.

Despite these improvements, challenges remain, including handling paraphrased clinical queries and integrating external medical ontologies such as UMLS for enhanced classification. Future research should explore retrieval-augmented methods and contrastive learning to improve generalization. Expanding to multilingual clinical QA will further enhance applicability across diverse healthcare settings.

Our findings contribute to advancing clinical NLP by bridging unstructured EMR text with structured retrieval, supporting more interpretable and efficient AI-driven medical question answering.







\bibliographystyle{IEEEtran}
\bibliography{custom}

% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.




% \end{thebibliography}


\end{document}
