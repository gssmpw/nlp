@article{Pattnayak2017AutoSales,
  author = {Pattnayak, Priyaranjan},
  title = {Monthly Auto Sales in US - Exhaustive Time Series Analysis Approach to Predict Auto Sales in 2018 using ARIMA/SARIMA},
  year = {2017},
  month = {December},
  doi = {10.13140/RG.2.2.29877.59360},
  url = {https://doi.org/10.13140/RG.2.2.29877.59360},
  journal = {ResearchGate},
  note = {Available at \url{https://doi.org/10.13140/RG.2.2.29877.59360}}
}

@inproceedings{agarwal-etal-2025-fs,
    title = "{FS}-{DAG}: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding",
    author = "Agarwal, Amit  and
      Panda, Srikant  and
      Pachauri, Kulbhushan",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven  and
      Darwish, Kareem  and
      Agarwal, Apoorv",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics: Industry Track",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-industry.9/",
    pages = "100--114"
}

@article{agarwal2021evaluate,
  title={EVALUATE GENERALISATION \& ROBUSTNESS OF VISUAL FEATURES FROM IMAGES TO VIDEO},
  author={AGARWAL, AMIT},
  year={2021},
  month        = {December},
  doi          = {10.13140/RG.2.2.33887.53928},
  advisor      = {Nasib Ullah},
  note         = {Available at \url{https://doi.org/10.13140/RG.2.2.33887.53928}}
}

@misc{agarwal2024domain,
  title={Domain adapting graph networks for visually rich documents},
  author={Agarwal, Amit and Panda, Srikant and Karmakar, Deepak and Pachauri, Kulbhushan},
  year={2024},
  publisher={Google Patents},
  note={US Patent App. 18/240,480}
}

@article{agarwal2024mvtamperbench,
  title={MVTamperBench: Evaluating Robustness of Vision-Language Models},
  author={Agarwal, Amit and Panda, Srikant and Charles, Angeline and Kumar, Bhargava and Patel, Hitesh and Pattnayak, Priyanranjan and Rafi, Taki Hasan and Kumar, Tejaswini and Chae, Dong-Kyu},
  journal={arXiv preprint arXiv:2412.19794},
  year={2024}
}

@article{alsentzer2019clinicalbert,
  title={Publicly Available Clinical BERT Embeddings},
  author={Alsentzer, Emily and Murphy, John and Boag, William and Weng, Wei-Hung and Jin, Hamed and Naumann, Tristan and McDermott, Matthew},
  journal={arXiv preprint arXiv:1904.03323},
  year={2019}
}

@inproceedings{clark2019boolq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  pages={2924--2936},
  year={2019}
}

@article{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4171--4186},
  year={2019}
}

@article{ferrucci2010building,
  title={Building Watson: An Overview of the DeepQA Project},
  author={Ferrucci, David and Brown, Eric and Chu-Carroll, Jennifer and Fan, James},
  journal={AI Magazine},
  volume={31},
  number={3},
  pages={59--79},
  year={2010}
}

@article{gu2021domain,
  title={Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
  author={Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Mike and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  journal={ACM Transactions on Computing for Healthcare (HEALTH)},
  volume={3},
  number={1},
  pages={1--23},
  year={2021}
}

@article{hochreiter1997long,
  title={Long Short-Term Memory},
  author={Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
  journal={Neural Computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997}
}

@article{huang2021clinical,
  title={Clinical Med-BERT: A Domain-Specific BERT Model for Structured EHR Data},
  author={Huang, Kun and Altosaar, Jaan and Taly, Anil},
  journal={NPJ Digital Medicine},
  volume={4},
  number={1},
  pages={1--8},
  year={2021}
}

@book{jurafsky2023speech,
  title={Speech and Language Processing},
  author={Jurafsky, Daniel and Martin, James H},
  year={2023},
  publisher={Pearson}
}

@article{lan2020albert,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2020}
}

@article{lee2020biobert,
  title={BioBERT: A Pre-trained Biomedical Language Representation Model for Biomedical Text Mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

@article{li2020multi,
  title={Multi-Task Clinical Event Detection: A Benchmark on MIMIC-III},
  author={Li, Chenguang and Zhang, Yufei and Fan, Jian},
  journal={Journal of Biomedical Informatics},
  volume={109},
  pages={103528},
  year={2020}
}

@article{liu2019multi,
  title={Multi-Task Deep Neural Networks for Natural Language Understanding},
  author={Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:1901.11504},
  year={2019}
}

@article{liu2019roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{liu2022mtlhealth,
  title={Multi-Task Learning in Healthcare AI: A Systematic Review},
  author={Liu, Xinyang and Kim, Sanghyun and Poon, Hoifung},
  journal={Artificial Intelligence in Medicine},
  volume={134},
  pages={102431},
  year={2022}
}

@article{mohamed2023riskprediction,
  title={Transformer-Based Multi-Task Learning for Clinical Risk Prediction},
  author={Mohamed, Saif and Zhou, Wei and Xu, Jia},
  journal={Journal of Biomedical Informatics},
  volume={140},
  pages={104355},
  year={2023}
}

@article{pattnayak2024survey,
  title={Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy},
  author={Pattnayak, Priyaranjan and Patel, Hitesh Laxmichand and Kumar, Bhargava and Agarwal, Amit and Banerjee, Ishan and Panda, Srikant and Kumar, Tejaswini},
  journal={arXiv preprint arXiv:2412.17759},
  year={2024}
}

@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--67},
  year={2020}
}

@article{rajpurkar2018know,
  title={Know What You Don't Know: Unanswerable Questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={784--798},
  year={2018}
}

@article{ruder2017overview,
  title={An Overview of Multi-Task Learning in Deep Neural Networks},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1706.05098},
  year={2017}
}

@article{sogaard2016deep,
  title={Deep Multi-Task Learning with Low-Level Tasks Supervised at Lower Layers},
  author={S{\o}gaard, Anders and Goldberg, Yoav},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={231--235},
  year={2016}
}

@article{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2017}
}

@article{wei2022multi,
  title={Multi-Task Learning for Adverse Event Detection in Clinical Texts},
  author={Wei, Shuo and Zhang, Xiaoyu and Yang, Wei},
  journal={Computational Linguistics in Medicine},
  year={2022}
}

@article{yang2022large,
  title={GatorTron: A Large Clinical Language Model with Self-Supervised Learning and Domain Adaptation},
  author={Yang, Xiaoqian and Wu, Yujia and Liu, Wei},
  journal={Journal of Biomedical Informatics},
  volume={127},
  pages={104087},
  year={2022}
}

@article{yoon2022clinical,
  title={Clinical Multi-Task Learning for Risk Prediction: A Review},
  author={Yoon, Ji and Kim, Sangwon and Park, Jungwook},
  journal={Journal of Biomedical Informatics},
  volume={128},
  pages={104013},
  year={2022}
}

