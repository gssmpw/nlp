\section{Related Work}
\subsection{Question Answering in NLP}
Question Answering (QA) is a core NLP task that enables retrieving precise answers from structured and unstructured text ____. Early models relied on rule-based techniques and information retrieval, as seen in IBM's Watson ____, which matched keywords using structured knowledge bases but lacked deep language understanding. The rise of neural architectures, including Long Short-Term Memory (LSTM) networks ____ and attention mechanisms ____, significantly improved contextual comprehension.

With Transformer-based models QA took a big leap and \textbf{BERT} ____ set new benchmarks ____. Fine-tuned variants such as BERT-QA outperformed previous models on datasets like SQuAD ____. Further optimizations, including \textbf{ALBERT} ____, \textbf{RoBERTa} ____, and \textbf{T5} ____, enhanced transformer models for QA across various domains ____. However, their reliance on general-domain corpora limits effectiveness in specialized fields like medicine.


\subsection{Domain Adaptation in Medical NLP}
Medical NLP poses challenges due to domain-specific terminology, abbreviations, and inconsistent documentation ____. Standard BERT-based models, trained on general corpora, struggle with these complexities. To address this, domain-adapted models such as \textbf{BioBERT} ____, pretrained on PubMed abstracts, improved biomedical Named Entity Recognition (NER) and relation extraction ____. \textbf{ClinicalBERT} ____, fine-tuned on clinical notes, enhanced hospital-based applications.

\textbf{PubMedBERT} ____ eliminated domain mismatches by pretraining exclusively on PubMed abstracts. \textbf{GatorTron} ____ further improved performance by leveraging de-identified clinical records, while \textbf{Med-BERT} ____ incorporated structured EHR data, bridging the gap between structured and unstructured text.

Despite these advancements, most domain-specific models are fine-tuned separately for tasks like QA, named entity recognition, and classification ____. This single-task approach limits generalization and increases data requirements, reducing adaptability in clinical settings.



\subsection{Multi-Task Learning in NLP and Medical AI}
Multi-Task Learning (MTL) has emerged as a powerful technique for improving NLP models by jointly learning related tasks ____. It has been widely explored in domains such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging ____, sentiment analysis with syntactic parsing ____, and question answering integrated with textual entailment ____. By leveraging shared representations, MTL enhances generalization and efficiency, especially in data-scarce environments.

In medical AI, MTL has been applied to clinical event detection ____, adverse event detection ____, and patient outcome prediction ____, consistently outperforming single-task models ____. For instance, multi-task transformers for clinical risk prediction ____ have demonstrated improved generalization across patient cohorts. Despite these successes, MTL remains underutilized in Clinical QA, where models predominantly focus on answer extraction while neglecting medical entity classificationâ€”critical for structured EMR retrieval and decision support.


\subsection{Limitations and Contribution}
Despite advancements in QA and domain-adapted transformers, key limitations persist: (1) QA models generate raw text outputs, making structured integration into clinical workflows difficult; (2) existing models either extract answers or classify entities, lacking a unified approach; (3) fine-tuned models struggle with distribution shifts in diverse EMR datasets. To address these gaps, we introduce an MTL framework for Clinical QA that (1) jointly learns answer extraction and medical classification, improving interpretability, (2) integrates domain-adapted transformers like ClinicalBERT with an auxiliary classification head, and (3) enhances generalization by leveraging shared representations, making it more robust for real-world EMR applications.

% In medical AI, MTL has been applied to clinical event detection ____, adverse event detection ____, and patient outcome prediction ____. Studies have shown that models trained on multiple related clinical tasks perform better than those trained on a single task, particularly in settings where labeled data is limited ____. For example, multi-task transformers for clinical risk prediction ____ showed that shared representations improve generalization across patient cohorts.

% Despite its success in healthcare AI, MTL remains underexplored in Clinical QA. Most medical NLP models focus exclusively on answer extraction, failing to leverage auxiliary tasks like medical entity classification. Given that extracted answers in EMRs need structured categorization, a joint QA + classification framework can improve interpretability, usability, and generalization in clinical settings.

% \subsection{Question Answering in NLP}
% Question Answering (QA) is a fundamental task in NLP, evolving from rule-based approaches to neural architectures. Early systems, such as IBM's Watson ____, relied on information retrieval techniques. Transformer-based models like BERT ____ revolutionized QA with contextual embeddings, outperforming traditional approaches. Fine-tuned variants, such as BioBERT ____ and ClinicalBERT ____, further improved domain adaptation for biomedical applications.

% \subsection{Domain Adaptation in Medical NLP}
% Medical NLP faces unique challenges, including specialized terminology and inconsistent documentation. BioBERT ____ and ClinicalBERT ____ improve performance by pretraining on biomedical corpora, but their single-task nature limits generalization. PubMedBERT ____ and GatorTron ____ further refine domain-specific text representations, though they primarily focus on biomedical text mining rather than structured QA.

% \subsection{Multi-Task Learning in Clinical NLP}
% Multi-Task Learning (MTL) has demonstrated effectiveness in various NLP tasks, improving generalization through shared representations ____. In clinical AI, MTL has been applied to event detection ____ and adverse event prediction ____, but its use in clinical QA remains underexplored. Our approach extends transformer-based QA by integrating medical entity classification, addressing gaps in structured retrieval and answer categorization.


% \subsection{Limitations of Existing Approaches and Research Gap}
% Despite advancements in QA models and domain-adapted transformers, key limitations remain unaddressed:

% \begin{itemize}
%     \item \textbf{Lack of Structured Answer Representation}: Existing QA models output raw text, making it difficult to integrate extracted information into structured databases or clinical workflows.
%     \item \textbf{Single-Task Learning Constraints}: Current approaches train models for either question answering or classification but do not combine them to maximize efficiency.
%     \item \textbf{Limited Generalization in Real-World Clinical Data}: Models fine-tuned on benchmark datasets often struggle with \textbf{distribution shifts} when applied to heterogeneous EMR records.
% \end{itemize}

% \subsection{Our Contribution}
% To bridge these gaps, this paper introduces a Multi-Task Learning (MTL) framework for Clinical QA that enhances both answer extraction and structured categorization. Unlike traditional fine-tuning methods, our model:

% \begin{enumerate}
%     \item \textbf{Jointly learns answer extraction and medical classification}, improving interpretability.
%     \item \textbf{Utilizes domain-adapted transformer models} (e.g., ClinicalBERT) with an additional classification head.
%     \item \textbf{Improves generalization} by leveraging shared task representations, making the model more robust for real-world EMR processing.
% \end{enumerate}