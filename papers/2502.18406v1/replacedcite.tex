\section{Related Work}
\label{sec:related_work}

\paragraph{Semirings \& Inference} The semiring perspective in artificial intelligence has its roots in weighted automata ____, as the weights of an automata can be defined over a semiring. These ideas have been carried over to various other paradigms such as constraint satisfaction problems ____, parsing ____, dynamic programs ____, database provenance ____, logic programming ____, propositional logic ____, answer set programming ____, Turing machines ____, and tensor networks ____.

\paragraph{Semirings \& Learning} While semirings have mostly been applied to inference problems, some works also investigated learning in semirings. ____ introduced the expectation semiring, which can compute gradients and perform expectation-maximization. ____ studied the complexity of constraint optimization in some semirings. On the other hand, we provide a more general framework to compute derivations of any semiring.

%
____ already described a forward-backward algorithm for computing conditionals of a formula. Backpropagation on semirings has been described recently by ____. Most similar to our work, ____ already included an algorithm for computing the conditionals on algebraic circuits, which they call All-Marginals instead of $\nabla \AMC$. This algorithm can be seen as a special case of our cancellation optimization, as all cancellative commutative monoids can be embedded in a group using the Grothendieck construction. %


\paragraph{Neurosymbolic Learning} Several neurosymbolic methods rely on (probabilistic) circuits and hence could apply the algebraic learning framework we outlined. Some examples include DeepProbLog ____, the semantic loss ____, and probabilistic semantic layers ____. ____ proposed another overarching view of neurosymbolic learning by framing it as energy-based models. On the other hand, our work focuses on algebraic circuits where inference and learning can be performed exactly.