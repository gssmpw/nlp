\section{Related Work}
\label{sec:related_work}

\paragraph{Semirings \& Inference} The semiring perspective in artificial intelligence has its roots in weighted automata \cite{schutzenberger_definition_1961}, as the weights of an automata can be defined over a semiring. These ideas have been carried over to various other paradigms such as constraint satisfaction problems \cite{bistarelli_semiring-based_1997}, parsing \cite{goodman_semiring_1999}, dynamic programs \cite{eisner_compiling_2005}, database provenance \cite{green_provenance_2007}, logic programming \cite{kimmig_algebraic_2011}, propositional logic \cite{kimmig_algebraic_2017}, answer set programming \cite{eiter_weighted_2020}, Turing machines \cite{eiter_semiring_2023}, and tensor networks \cite{goral_model_2024}.

\paragraph{Semirings \& Learning} While semirings have mostly been applied to inference problems, some works also investigated learning in semirings. \citet{li_first-_2009} introduced the expectation semiring, which can compute gradients and perform expectation-maximization. \citet{pavan_constraint_2023} studied the complexity of constraint optimization in some semirings. On the other hand, we provide a more general framework to compute derivations of any semiring.

%
\citet{darwiche_tractable_2001} already described a forward-backward algorithm for computing conditionals of a formula. Backpropagation on semirings has been described recently by \cite{du_generalizing_2023}. Most similar to our work, \citet{shih_smoothing_2019} already included an algorithm for computing the conditionals on algebraic circuits, which they call All-Marginals instead of $\nabla \AMC$. This algorithm can be seen as a special case of our cancellation optimization, as all cancellative commutative monoids can be embedded in a group using the Grothendieck construction. %


\paragraph{Neurosymbolic Learning} Several neurosymbolic methods rely on (probabilistic) circuits and hence could apply the algebraic learning framework we outlined. Some examples include DeepProbLog \cite{manhaeve_deepproblog_2018}, the semantic loss \cite{xu_semantic_2018}, and probabilistic semantic layers \cite{ahmed_semantic_2022}. \citet{dickens_modeling_2024} proposed another overarching view of neurosymbolic learning by framing it as energy-based models. On the other hand, our work focuses on algebraic circuits where inference and learning can be performed exactly.