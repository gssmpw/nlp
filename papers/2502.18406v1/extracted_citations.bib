@inproceedings{ahmed_semantic_2022,
	title = {Semantic {Probabilistic} {Layers} for {Neuro}-{Symbolic} {Learning}},
	shorttitle = {{SPL}},
	url = {https://openreview.net/forum?id=o-mxIWAY1T8},
	abstract = {We design a predictive layer for structured-output prediction (SOP) that can be plugged into any neural network guaranteeing its predictions are consistent with a set of predefined symbolic constraints. Our Semantic Probabilistic Layer (SPL) can model intricate correlations, and hard constraints, over a structured output space all while being amenable to end-to-end learning via maximum likelihood. SPLs combine exact probabilistic inference with logical reasoning in a clean and modular way, learning complex distributions and restricting their support to solutions of the constraint. As such, they can faithfully, and efficiently, model complex SOP tasks beyond the reach of alternative neuro-symbolic approaches. We empirically demonstrate that SPLs outperform these competitors in terms of accuracy on challenging SOP tasks such as hierarchical multi-label classification, pathfinding and preference learning, while retaining perfect constraint satisfaction.},
	language = {en},
	urldate = {2023-12-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ahmed, Kareem and Teso, Stefano and Chang, Kai-Wei and Van den Broeck, Guy and Vergari, Antonio},
	month = may,
	year = {2022},
	keywords = {ucla},
}

@article{bistarelli_semiring-based_1997,
	title = {Semiring-based constraint satisfaction and optimization},
	volume = {44},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/256303.256306},
	doi = {10.1145/256303.256306},
	abstract = {We introduce a general framework for constraint satisfaction and optimization where classical CSPs, fuzzy CSPs, weighted CSPs, partial constraint satisfaction, and others can be easily cast. The framework is based on a semiring structure, where the set of the semiring specifies the values to be associated with each tuple of values of the variable domain, and the two semiring operations (ϩ and ϫ) model constraint projection and combination respectively. Local consistency algorithms, as usually used for classical CSPs, can be exploited in this general framework as well, provided that certain conditions on the semiring operations are satisfied. We then show how this framework can be used to model both old and new constraint solving and optimization schemes, thus allowing one to both formally justify many informally taken choices in existing schemes, and to prove that local consistency techniques can be used also in newly defined schemes.},
	language = {en},
	number = {2},
	urldate = {2023-05-14},
	journal = {Journal of the ACM},
	author = {Bistarelli, Stefano and Montanari, Ugo and Rossi, Francesca},
	month = mar,
	year = {1997},
	pages = {201--236},
}

@article{darwiche_tractable_2001,
	title = {On the {Tractable} {Counting} of {Theory} {Models} and its {Application} to {Truth} {Maintenance} and {Belief} {Revision}},
	volume = {11},
	issn = {1166-3081},
	url = {https://doi.org/10.3166/jancl.11.11-34},
	doi = {10.3166/jancl.11.11-34},
	abstract = {We address in this paper the problem of counting the models of a propositional theory under incremental changes to its literals. Specifcally, we show that if a propositional theory Δ is in a special form that we call smooth, deterministic, decomposable negation normal form (sd-DNNF), then for any consistent set of literals S, we can simultaneously count (in time linear in the size of Δ) the models of Δ ∪ S and the models of every theory Δ ∪ T where T results from adding, removing or flipping a literal in S. We present two results relating to the time and space complexity of compiling propositional theories into sd-DNNF. First, we show that if a conjunctive normal form (CNF) has a bounded treewidth, then it can be compiled into an sd-DNNF in time and space which are linear in its size. Second, we show that sd-DNNF is a strictly more space efficient representation than Free Binary Decision Diagrams (FBDDs). Finally, we discuss some applications of the counting results to truth maintenance systems, belief revision, and model-based diagnosis.},
	number = {1-2},
	urldate = {2024-08-12},
	journal = {Journal of Applied Non-Classical Logics},
	author = {Darwiche, Adnan},
	month = jan,
	year = {2001},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.3166/jancl.11.11-34},
	keywords = {Counting models, belief revision, diagnosis, knowledge compilation, truth maintenance, ucla},
	pages = {11--34},
}

@article{dickens_modeling_2024,
	title = {Modeling {Patterns} for {Neural}-{Symbolic} {Reasoning} {Using} {Energy}-based {Models}},
	volume = {3},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2994-4317},
	url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/31187},
	doi = {10.1609/aaaiss.v3i1.31187},
	abstract = {Neural-symbolic (NeSy) AI strives to empower machine learning and large language models with fast, reliable predictions that exhibit commonsense and trustworthy reasoning by seamlessly integrating neural and symbolic methods. With such a broad scope, several taxonomies have been proposed to categorize this integration, emphasizing knowledge representation, reasoning algorithms, and applications. We introduce a knowledge representation-agnostic taxonomy focusing on the neural-symbolic interface capturing methods that reason with probability, logic, and arithmetic constraints. Moreover, we derive expressions for gradients of a prominent class of learning losses and a formalization of reasoning and learning. Through a rigorous empirical analysis spanning three tasks, we show NeSy approaches reach up to a 37\% improvement over neural baselines in a semi-supervised setting and a 19\% improvement over GPT-4 on question-answering.},
	language = {en},
	number = {1},
	urldate = {2024-07-18},
	journal = {Proceedings of the AAAI Symposium Series},
	author = {Dickens, Charles and Pryor, Connor and Getoor, Lise},
	month = may,
	year = {2024},
	note = {Number: 1},
	keywords = {Machine Learning},
	pages = {90--99},
}

@inproceedings{du_generalizing_2023,
	address = {Toronto, Canada},
	title = {Generalizing {Backpropagation} for {Gradient}-{Based} {Interpretability}},
	url = {https://aclanthology.org/2023.acl-long.669},
	doi = {10.18653/v1/2023.acl-long.669},
	abstract = {Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs. While these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject–verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its importance to a prediction and (b) for SVA, identify which pathways of the self-attention mechanism are most important.},
	urldate = {2024-04-05},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Du, Kevin and Torroba Hennigen, Lucas and Stoehr, Niklas and Warstadt, Alex and Cotterell, Ryan},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {11979--11995},
}

@inproceedings{eisner_compiling_2005,
	address = {USA},
	series = {{HLT} '05},
	title = {Compiling {Comp} {Ling}: practical weighted dynamic programming and the {Dyna} language},
	shorttitle = {Compiling {Comp} {Ling}},
	url = {https://dl.acm.org/doi/10.3115/1220575.1220611},
	doi = {10.3115/1220575.1220611},
	abstract = {Weighted deduction with aggregation is a powerful theoretical formalism that encompasses many NLP algorithms. This paper proposes a declarative specification language, Dyna; gives general agenda-based algorithms for computing weights and gradients; briefly discusses Dyna-to-Dyna program transformations; and shows that a first implementation of a Dyna-to-C++ compiler produces code that is efficient enough for real NLP research, though still several times slower than hand-crafted code.},
	urldate = {2024-08-06},
	booktitle = {Proceedings of the conference on {Human} {Language} {Technology} and {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Eisner, Jason and Goldlust, Eric and Smith, Noah A.},
	year = {2005},
	pages = {281--290},
}

@article{eiter_semiring_2023,
	title = {Semiring {Reasoning} {Frameworks} in {AI} and {Their} {Computational} {Complexity}},
	volume = {77},
	copyright = {Copyright (c) 2023 Journal of Artificial Intelligence Research},
	issn = {1076-9757},
	shorttitle = {Semiring {Turing} {Machines}},
	url = {https://www.jair.org/index.php/jair/article/view/13970},
	doi = {10.1613/jair.1.13970},
	abstract = {Many important problems in AI, among them \#SAT, parameter learning and probabilistic inference go beyond the classical satisfiability problem. Here, instead of finding a solution we are interested in a quantity associated with the set of solutions, such as the number of solutions, the optimal solution or the probability that a query holds in a solution. To model such quantitative problems in a uniform manner, a number of frameworks, e.g. Algebraic Model Counting and Semiring-based Constraint Satisfaction Problems, employ what we call the semiring paradigm. In the latter the abstract algebraic structure of the semiring serves as a means of parameterizing the problem definition, thus allowing for different modes of quantitative computations by choosing different semirings. While efficiently solvable cases have been widely studied, a systematic study of the computational complexity of such problems depending on the semiring parameter is missing. In this work, we characterize the latter by NP(R), a novel generalization of NP over semiring R, and obtain NP(R)-completeness results for a selection of semiring frameworks. To obtain more tangible insights into the hardness of NP(R), we link it to well-known complexity classes from the literature. Interestingly, we manage to connect the computational hardness to properties of the semiring. Using this insight, we see that, on the one hand, NP(R) is always at least as hard as NP or ModpP depending on the semiring R and in general unlikely to be in FPSPACEpoly. On the other hand, for broad subclasses of semirings relevant in practice we can employ reductions to NP, ModpP and \#P. These results show that in many cases solutions are only mildly harder to compute than functions in NP, ModpP and \#P, give us new insights into how problems that involve counting on semirings can be approached, and provide a means of assessing whether an algorithm is appropriate for a given class of problems.},
	language = {en},
	urldate = {2023-06-06},
	journal = {Journal of Artificial Intelligence Research},
	author = {Eiter, Thomas and Kiesel, Rafael},
	month = may,
	year = {2023},
	keywords = {knowledge representation, probabilistic reasoning},
	pages = {207--293},
}

@incollection{eiter_weighted_2020,
	title = {Weighted {LARS} for {Quantitative} {Stream} {Reasoning}},
	url = {https://ebooks.iospress.nl/doi/10.3233/FAIA200160},
	urldate = {2024-07-17},
	booktitle = {{ECAI} 2020},
	publisher = {IOS Press},
	author = {Eiter, Thomas and Kiesel, Rafael},
	year = {2020},
	doi = {10.3233/FAIA200160},
	pages = {729--736},
}

@article{goodman_semiring_1999,
	title = {Semiring parsing},
	volume = {25},
	issn = {0891-2017},
	abstract = {We synthesize work on parsing algorithms, deductive parsing, and the theory of algebra applied to formal languages into a general system for describing parsers. Each parser performs abstract computations using the operations of a semiring. The system allows a single, simple representation to be used for describing parsers that compute recognition, derivation forests, Viterbi, n-best, inside values, and other values, simply by substituting the operations of different semirings. We also show how to use the same representation, interpreted differently, to compute outside values. The system can be used to describe a wide variety of parsers, including Earley's algorithm, tree adjoining grammar parsing, Graham Harrison Ruzzo parsing, and prefix value computation.},
	number = {4},
	journal = {Comput. Linguist.},
	author = {Goodman, Joshua},
	month = dec,
	year = {1999},
	pages = {573--605},
}

@article{goral_model_2024,
	title = {Model {Counting} and {Sampling} via {Semiring} {Extensions}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/30022},
	doi = {10.1609/aaai.v38i18.30022},
	abstract = {Many decision and optimization problems have natural extensions as counting problems. The best known example is the Boolean satisfiability problem (SAT), where we want to count the satisfying assignments of truth values to the variables, which is known as the \#SAT problem. Likewise, for discrete optimization problems, we want to count the states on which the objective function attains the optimal value. Both SAT and discrete optimization can be formulated as selective marginalize a product function (MPF) queries. Here, we show how general selective MPF queries can be extended for model counting. MPF queries are encoded as tensor hypernetworks over suitable semirings that can be solved by generic tensor hypernetwork contraction algorithms. Our model counting extension is again an MPF query, on an extended semiring, that can be solved by the same contraction algorithms. Model counting is required for uniform model sampling. We show how the counting extension can be further extended for model sampling by constructing yet another semiring. We have implemented the model counting and sampling extensions. Experiments show that our generic approach is competitive with the state of the art in model counting and model sampling.},
	language = {en},
	number = {18},
	urldate = {2024-04-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Goral, Andreas and Giesen, Joachim and Blacher, Mark and Staudt, Christoph and Klaus, Julien},
	month = mar,
	year = {2024},
	note = {Number: 18},
	keywords = {SO: Combinatorial Optimization},
	pages = {20395--20403},
}

@inproceedings{green_provenance_2007,
	address = {New York, NY, USA},
	series = {{PODS} '07},
	title = {Provenance semirings},
	isbn = {978-1-59593-685-1},
	url = {https://dl.acm.org/doi/10.1145/1265530.1265535},
	doi = {10.1145/1265530.1265535},
	abstract = {We show that relational algebra calculations for incomplete databases, probabilistic databases, bag semantics and why-provenance are particular cases of the same general algorithms involving semirings. This further suggests a comprehensive provenance representation that uses semirings of polynomials. We extend these considerations to datalog and semirings of formal power series. We give algorithms for datalog provenance calculation as well as datalog evaluation for incomplete and probabilistic databases. Finally, we show that for some semirings containment of conjunctive queries is the same as for standard set semantics.},
	urldate = {2023-11-15},
	booktitle = {Proceedings of the twenty-sixth {ACM} {SIGMOD}-{SIGACT}-{SIGART} symposium on {Principles} of database systems},
	publisher = {Association for Computing Machinery},
	author = {Green, Todd J. and Karvounarakis, Grigoris and Tannen, Val},
	month = jun,
	year = {2007},
	keywords = {data lineage, data provenance, datalog, formal power series, incomplete databases, probabilistic databases, semirings},
	pages = {31--40},
}

@article{kimmig_algebraic_2011,
	title = {An {Algebraic} {Prolog} for {Reasoning} about {Possible} {Worlds}},
	volume = {25},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{aProbLog}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7852},
	doi = {10.1609/aaai.v25i1.7852},
	abstract = {We introduce aProbLog, a generalization of the probabilistic logic programming language ProbLog. An aProbLog program consists of a set of deﬁnite clauses and a set of algebraic facts; each such fact is labeled with an element of a semiring. A wide variety of labels is possible, ranging from probability values to reals (representing costs or utilities), polynomials, Boolean functions or data structures. The semiring is then used to calculate labels of possible worlds and of queries.},
	language = {en},
	number = {1},
	urldate = {2023-01-04},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Kimmig, Angelika and Van den Broeck, Guy and De Raedt, Luc},
	month = aug,
	year = {2011},
	keywords = {leuven},
	pages = {209--214},
}

@article{kimmig_algebraic_2017,
	series = {{SI}:{Uncertain} {Reasoning}},
	title = {Algebraic model counting},
	volume = {22},
	issn = {1570-8683},
	shorttitle = {{AMC}},
	url = {https://www.sciencedirect.com/science/article/pii/S157086831630088X},
	doi = {10.1016/j.jal.2016.11.031},
	abstract = {Weighted model counting (WMC) is a well-known inference task on knowledge bases, and the basis for some of the most efficient techniques for probabilistic inference in graphical models. We introduce algebraic model counting (AMC), a generalization of WMC to a semiring structure that provides a unified view on a range of tasks and existing results. We show that AMC generalizes many well-known tasks in a variety of domains such as probabilistic inference, soft constraints and network and database analysis. Furthermore, we investigate AMC from a knowledge compilation perspective and show that all AMC tasks can be evaluated using sd-DNNF circuits, which are strictly more succinct, and thus more efficient to evaluate, than direct representations of sets of models. We identify further characteristics of AMC instances that allow for evaluation on even more succinct circuits.},
	language = {en},
	urldate = {2023-05-16},
	journal = {Journal of Applied Logic},
	author = {Kimmig, Angelika and Van den Broeck, Guy and De Raedt, Luc},
	month = jul,
	year = {2017},
	keywords = {Knowledge compilation, Logic, Model counting, leuven},
	pages = {46--62},
}

@inproceedings{li_first-_2009,
	address = {USA},
	series = {{EMNLP} '09},
	title = {First- and second-order expectation semirings with applications to minimum-risk training on translation forests},
	isbn = {978-1-932432-59-6},
	abstract = {Many statistical translation models can be regarded as weighted logical deduction. Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs). We then introduce a novel second-order expectation semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy). This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk. We use these semirings in an open-source machine translation toolkit, Joshua, enabling minimum-risk training for a benefit of up to 1.0 bleu point.},
	urldate = {2024-04-14},
	booktitle = {Proceedings of the 2009 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {Volume} 1 - {Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Li, Zhifei and Eisner, Jason},
	month = aug,
	year = {2009},
	pages = {40--51},
}

@inproceedings{manhaeve_deepproblog_2018,
	title = {{DeepProbLog}: {Neural} {Probabilistic} {Logic} {Programming}},
	volume = {31},
	shorttitle = {{DeepProbLog}},
	abstract = {We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.},
	urldate = {2022-09-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Manhaeve, Robin and Dumancic, Sebastijan and Kimmig, Angelika and Demeester, Thomas and De Raedt, Luc},
	year = {2018},
	keywords = {leuven},
}

@article{pavan_constraint_2023,
	title = {Constraint {Optimization} over {Semirings}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/25522},
	doi = {10.1609/aaai.v37i4.25522},
	abstract = {Interpretations of logical formulas over semirings (other than the Boolean semiring) have applications in various areas of computer science including logic, AI, databases, and security.  Such interpretations provide richer information beyond the truth or falsity of a statement. Examples of such semirings include Viterbi semiring, min-max or access control semiring, tropical semiring, and fuzzy semiring. 
    
    The present work investigates the complexity of constraint optimization problems over semirings. The generic optimization problem we study is the following: Given a propositional formula phi over n variable and a semiring (K,+, . ,0,1), find the maximum value over all possible interpretations of phi over K. This can be seen as a generalization of the well-known satisfiability problem (a propositional formula is satisfiable if and only if the maximum value over all interpretations/assignments over the Boolean semiring is 1).  A related problem is to find an interpretation that achieves the maximum value. In this work, we first focus on these optimization problems over the Viterbi semiring, which we call optConfVal and optConf. 
    
    We first show that for general propositional formulas in negation normal form, optConfVal and optConf are in FP{\textasciicircum}NP. We then investigate optConf when the input formula phi is represented in the conjunctive normal form.  For CNF formulae, we first derive an upper bound on the value of optConf as a function of the number of maximum satisfiable clauses. In particular, we show that if r is the maximum number of satisfiable clauses in a CNF formula with m clauses, then its optConf value is at most 1/4{\textasciicircum}(m-r). Building on this we establish that optConf for CNF formulae is hard for the complexity class FP{\textasciicircum}NP[log]. We also design polynomial-time approximation algorithms and establish an inapproximability for optConfVal. We establish similar complexity results for these optimization problems over other semirings including tropical, fuzzy, and access control semirings.},
	language = {en},
	number = {4},
	urldate = {2024-08-01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Pavan, A. and Meel, Kuldeep S. and Vinodchandran, N. V. and Bhattacharyya, Arnab},
	month = jun,
	year = {2023},
	note = {Number: 4},
	keywords = {CSO: Satisfiability},
	pages = {4070--4077},
}

@article{schutzenberger_definition_1961,
	title = {On the definition of a family of automata},
	volume = {4},
	issn = {0019-9958},
	url = {https://www.sciencedirect.com/science/article/pii/S001999586180020X},
	doi = {10.1016/S0019-9958(61)80020-X},
	number = {2},
	urldate = {2024-07-17},
	journal = {Information and Control},
	author = {Schützenberger, M. P.},
	month = sep,
	year = {1961},
	pages = {245--270},
}

@inproceedings{shih_smoothing_2019,
	title = {Smoothing {Structured} {Decomposable} {Circuits}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/940392f5f32a7ade1cc201767cf83e31-Abstract.html},
	abstract = {We study the task of smoothing a circuit, i.e., ensuring that all children of a plus-gate mention the same variables. Circuits serve as the building blocks of state-of-the-art inference algorithms on discrete probabilistic graphical models and probabilistic programs. They are also important for discrete density estimation algorithms. Many of these tasks require the input circuit to be smooth. However, smoothing has not been studied in its own right yet, and only a trivial quadratic algorithm is known. This paper studies efficient smoothing for structured decomposable circuits. We propose a near-linear time algorithm for this task and explore lower bounds for smoothing decomposable circuits, using existing results on range-sum queries. Further, for the important case of All-Marginals, we show a more efficient linear-time algorithm. We validate experimentally the performance of our methods.},
	urldate = {2024-07-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Shih, Andy and Van den Broeck, Guy and Beame, Paul and Amarilli, Antoine},
	year = {2019},
	keywords = {ucla},
}

@inproceedings{xu_semantic_2018,
	title = {A {Semantic} {Loss} {Function} for {Deep} {Learning} with {Symbolic} {Knowledge}},
	shorttitle = {Semantic {Loss}},
	abstract = {This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that it effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.},
	language = {en},
	urldate = {2022-09-06},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Xu, Jingyi and Zhang, Zilu and Friedman, Tal and Liang, Yitao and Van den Broeck, Guy},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	keywords = {ucla},
	pages = {5502--5511},
}

