
@incollection{bryant_binary_2018,
	address = {Cham},
	title = {Binary {Decision} {Diagrams}},
	isbn = {978-3-319-10575-8},
	url = {https://doi.org/10.1007/978-3-319-10575-8_7},
	abstract = {Binary decision diagrams provide a data structure for representing and manipulating Boolean functions in symbolic form. They have been especially effective as the algorithmic basis for symbolic model checkers. A binary decision diagram represents a Boolean function as a directed acyclic graph, corresponding to a compressed form of decision tree. Most commonly, an ordering constraint is imposed among the occurrences of decision variables in the graph, yielding ordered binary decision diagrams (OBDD). Representing all functions as OBDDs with a common variable ordering has the advantages that (1) there is a unique, reduced representation of any function, (2) there is a simple algorithm to reduce any OBDD to the unique form for that function, and (3) there is an associated set of algorithms to implement a wide variety of operations on Boolean functions represented as OBDDs. Recent work in this area has focused on generalizations to represent larger classes of functions, as well as on scaling implementations to handle larger and more complex problems.},
	language = {en},
	urldate = {2024-12-14},
	booktitle = {Handbook of {Model} {Checking}},
	publisher = {Springer International Publishing},
	author = {Bryant, Randal E.},
	editor = {Clarke, Edmund M. and Henzinger, Thomas A. and Veith, Helmut and Bloem, Roderick},
	year = {2018},
	doi = {10.1007/978-3-319-10575-8_7},
	pages = {191--217},
}

@misc{manginas_nesya_2024,
	title = {{NeSyA}: {Neurosymbolic} {Automata}},
	shorttitle = {{NeSyA}},
	url = {http://arxiv.org/abs/2412.07331},
	doi = {10.48550/arXiv.2412.07331},
	abstract = {Neurosymbolic Artificial Intelligence (NeSy) has emerged as a promising direction to integrate low level perception with high level reasoning. Unfortunately, little attention has been given to developing NeSy systems tailored to temporal/sequential problems. This entails reasoning symbolically over sequences of subsymbolic observations towards a target prediction. We show that using a probabilistic semantics symbolic automata, which combine the power of automata for temporal structure specification with that of propositional logic, can be used to reason efficiently and differentiably over subsymbolic sequences. The proposed system, which we call NeSyA (Neuro Symbolic Automata), is shown to either scale or perform better than existing NeSy approaches when applied to problems with a temporal component.},
	language = {en},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Manginas, Nikolaos and Paliouras, George and Raedt, Luc De},
	month = dec,
	year = {2024},
	note = {arXiv:2412.07331 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{yin_expressive_2024,
	title = {On the {Expressive} {Power} of {Tree}-{Structured} {Probabilistic} {Circuits}},
	url = {http://arxiv.org/abs/2410.05465},
	doi = {10.48550/arXiv.2410.05465},
	abstract = {Probabilistic circuits (PCs) have emerged as a powerful framework to compactly represent probability distributions for efficient and exact probabilistic inference. It has been shown that PCs with a general directed acyclic graph (DAG) structure can be understood as a mixture of exponentially (in its height) many components, each of which is a product distribution over univariate marginals. However, existing structure learning algorithms for PCs often generate tree-structured circuits or use tree-structured circuits as intermediate steps to compress them into DAGstructured circuits. This leads to the intriguing question of whether there exists an exponential gap between DAGs and trees for the PC structure. In this paper, we provide a negative answer to this conjecture by proving that, for n variables, there exists a quasi-polynomial upper bound nO(log n) on the size of an equivalent tree computing the same probability distribution. On the other hand, we also show that given a depth restriction on the tree, there is a super-polynomial separation between tree and DAG-structured PCs. Our work takes an important step towards understanding the expressive power of tree-structured PCs, and our techniques may be of independent interest in the study of structure learning algorithms for PCs.},
	language = {en},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Yin, Lang and Zhao, Han},
	month = oct,
	year = {2024},
	note = {arXiv:2410.05465 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{barlag_graph_2024,
	title = {Graph {Neural} {Networks} and {Arithmetic} {Circuits}},
	url = {http://arxiv.org/abs/2402.17805},
	doi = {10.48550/arXiv.2402.17805},
	abstract = {We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types. We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers. In our results the activation function of the network becomes a gate type in the circuit. Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions.},
	language = {en},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Barlag, Timon and Holzapfel, Vivian and Strieker, Laura and Virtema, Jonni and Vollmer, Heribert},
	month = nov,
	year = {2024},
	note = {arXiv:2402.17805 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, Computer Science - Machine Learning},
}

@article{sakama_logic_2021,
	title = {Logic programming in tensor spaces},
	volume = {89},
	issn = {1573-7470},
	url = {https://doi.org/10.1007/s10472-021-09767-x},
	doi = {10.1007/s10472-021-09767-x},
	abstract = {This paper introduces a novel approach to computing logic programming semantics. First, a propositional Herbrand base is represented in a vector space and if-then rules in a program are encoded in a matrix. Then the least fixpoint of a definite logic program is computed by matrix-vector products with a non-linear operation. Second, disjunctive logic programs are represented in third-order tensors and their minimal models are computed by algebraic manipulation of tensors. Third, normal logic programs are represented by matrices and third-order tensors, and their stable models are computed. The result of this paper exploits a new connection between linear algebraic computation and symbolic computation, which has the potential to realize logical inference in huge scale of knowledge bases.},
	language = {en},
	number = {12},
	urldate = {2024-12-10},
	journal = {Annals of Mathematics and Artificial Intelligence},
	author = {Sakama, Chiaki and Inoue, Katsumi and Sato, Taisuke},
	month = dec,
	year = {2021},
	keywords = {15A69, 68N17, 68T30, 68W30, Artificial Intelligence, Knowledge representation and reasoning, Linear algebra, Logic program, Tensor space},
	pages = {1133--1153},
}

@misc{kojima_tensorized_2019,
	title = {A tensorized logic programming language for large-scale data},
	url = {http://arxiv.org/abs/1901.08548},
	doi = {10.48550/arXiv.1901.08548},
	abstract = {We introduce a new logic programming language T-PRISM based on tensor embeddings. Our embedding scheme is a modiﬁcation of the distribution semantics in PRISM, one of the state-of-the-art probabilistic logic programming languages, by replacing distribution functions with multidimensional arrays, i.e., tensors. T-PRISM consists of two parts: logic programming part and numerical computation part. The former provides ﬂexible and interpretable modeling at the level of ﬁrst order logic, and the latter part provides scalable computation utilizing parallelization and hardware acceleration with GPUs. Combing these two parts provides a remarkably wide range of high-level declarative modeling from symbolic reasoning to deep learning.},
	language = {en},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Kojima, Ryosuke and Sato, Taisuke},
	month = jan,
	year = {2019},
	note = {arXiv:1901.08548 [cs]},
	keywords = {Computer Science - Logic in Computer Science, Computer Science - Machine Learning},
}

@inproceedings{arenas_when_2021,
	address = {New York, NY, USA},
	series = {{STOC} 2021},
	title = {When is approximate counting for conjunctive queries tractable?},
	isbn = {978-1-4503-8053-9},
	url = {https://dl.acm.org/doi/10.1145/3406325.3451014},
	doi = {10.1145/3406325.3451014},
	abstract = {Conjunctive queries are one of the most common class of queries used in database systems, and the best studied in the literature. A seminal result of Grohe, Schwentick, and Segoufin (STOC 2001) demonstrates that for every class G of graphs, the evaluation of all conjunctive queries whose underlying graph is in G is tractable if, and only if, G has bounded treewidth. In this work, we extend this characterization to the counting problem for conjunctive queries. Specifically, for every class C of conjunctive queries with bounded treewidth, we introduce the first fully polynomial-time randomized approximation scheme (FPRAS) for counting answers to a query in C, and the first polynomial-time algorithm for sampling answers uniformly from a query in C. As a corollary, it follows that for every class G of graphs, the counting problem for conjunctive queries whose underlying graph is in G admits an FPRAS if, and only if, G has bounded treewidth (unless BPP is different from P). In fact, our FPRAS is more general, and also applies to conjunctive queries with bounded hypertree width, as well as unions of such queries.  The key ingredient in our proof is the resolution of a fundamental counting problem from automata theory. Specifically, we demonstrate the first FPRAS and polynomial time sampler for the set of trees of size n accepted by a tree automaton, which improves the prior quasi-polynomial time randomized approximation scheme (QPRAS) and sampling algorithm of Gore, Jerrum, Kannan, Sweedyk, and Mahaney ’97. We demonstrate how this algorithm can be used to obtain an FPRAS for many open problems, such as counting solutions to constraint satisfaction problems (CSP) with bounded hypertree width, counting the number of error threads in programs with nested call subroutines, and counting valid assignments to structured DNNF circuits.},
	urldate = {2024-12-02},
	booktitle = {Proceedings of the 53rd {Annual} {ACM} {SIGACT} {Symposium} on {Theory} of {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Arenas, Marcelo and Croquevielle, Luis Alberto and Jayaram, Rajesh and Riveros, Cristian},
	month = jun,
	year = {2021},
	pages = {1015--1027},
}

@misc{meel_cfg_2024,
	title = {\#{CFG} and \#{DNNF} admit {FPRAS}},
	url = {http://arxiv.org/abs/2406.18224},
	doi = {10.48550/arXiv.2406.18224},
	abstract = {We provide the ﬁrst fully polynomial-time randomized approximation scheme for the following two counting problems: 1. Given a Context Free Grammar G over alphabet Σ, count the number of words of length exactly n generated by G. 2. Given a circuit ϕ in Decomposable Negation Normal Form (DNNF) over the set of Boolean variables X, compute the number of assignments to X such that ϕ evaluates to 1. Finding polynomial time algorithms for the aforementioned problems has been a longstanding open problem. Prior work could either only obtain a quasi-polynomial runtime (SODA 1995) or a polynomial-time randomized approximation scheme for restricted fragments, such as nondeterministic ﬁnite automata (JACM 2021) or non-deterministic tree automata (STOC 2021).},
	language = {en},
	urldate = {2024-12-02},
	publisher = {arXiv},
	author = {Meel, Kuldeep S. and Colnet, Alexis de},
	month = jul,
	year = {2024},
	note = {arXiv:2406.18224 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@inproceedings{dubray_probabilistic_2023,
	title = {Probabilistic {Inference} by {Projected} {Weighted} {Model} {Counting} on {Horn} {Clauses}},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	url = {https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.CP.2023.15},
	doi = {10.4230/LIPIcs.CP.2023.15},
	abstract = {Weighted model counting, that is, counting the weighted number of satisfying assignments of a propositional formula, is an important tool in probabilistic reasoning. Recently, the use of projected weighted model counting (PWMC) has been proposed as an approach to formulate and answer probabilistic queries. In this work, we propose a new simplified modeling language based on PWMC in which probabilistic inference tasks are modeled using a conjunction of Horn clauses and a particular weighting scheme for the variables. We show that the major problems of inference for Bayesian Networks, network reachability and probabilistic logic programming can be modeled in this language. Subsequently, we propose a new, relatively simple solver that is specifically optimized to solve the PWMC problem for such formulas. Our experiments show that our new solver is competitive with state-of-the-art solvers on the major problems studied.},
	language = {en},
	urldate = {2024-12-02},
	booktitle = {29th {International} {Conference} on {Principles} and {Practice} of {Constraint} {Programming} ({CP} 2023)},
	publisher = {Schloss Dagstuhl – Leibniz-Zentrum für Informatik},
	author = {Dubray, Alexandre and Schaus, Pierre and Nijssen, Siegfried},
	year = {2023},
	pages = {15:1--15:17},
}

@article{hahn_plingo_2024,
	title = {Plingo: {A} {System} for {Probabilistic} {Reasoning} in {Answer} {Set} {Programming}},
	issn = {1471-0684, 1475-3081},
	shorttitle = {Plingo},
	url = {https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/plingo-a-system-for-probabilistic-reasoning-in-answer-set-programming/9737F2F35D88B27F767EF7EDA7804EE1?utm_source=SFMC&utm_medium=email&utm_content=Article&utm_campaign=New%20Cambridge%20Alert%20-%20Articles&WT.mc_id=New%20Cambridge%20Alert%20-%20Articles},
	doi = {10.1017/S1471068424000449},
	abstract = {We present plingo, an extension of the answer set programming (ASP) system clingo that incorporates various probabilistic reasoning modes. Plingo is based on Lpmln±Lpmln±{\textbackslash}textit\{Lpmln\}{\textasciicircum}\{{\textbackslash}pm \}, a simple variant of the probabilistic language Lpmln, which follows a weighted scheme derived from Markov logic. This choice is motivated by the fact that the main probabilistic reasoning modes can be mapped onto enumeration and optimization problems and that Lpmln±Lpmln±{\textbackslash}textit\{Lpmln\}{\textasciicircum}\{{\textbackslash}pm \} may serve as a middle-ground formalism connecting to other probabilistic approaches. Plingo offers three alternative frontends, for Lpmln, P-log, and ProbLog. These input languages and reasoning modes are implemented by means of clingo’s multi-shot and theory-solving capabilities. In this way, the core of plingo is an implementation of Lpmln±Lpmln±{\textbackslash}textit\{Lpmln\}{\textasciicircum}\{{\textbackslash}pm \} in terms of modern ASP technology. On top of that, plingo implements a new approximation technique based on a recent method for answer set enumeration in the order of optimality. Additionally, in this work, we introduce a novel translation from Lpmln±Lpmln±{\textbackslash}textit\{Lpmln\}{\textasciicircum}\{{\textbackslash}pm \} to ProbLog. This leads to a new solving method in plingo where the input program is translated and a ProbLog solver is executed. Our empirical evaluation shows that the different solving approaches of plingo are complementary and that plingo performs similarly to other probabilistic reasoning systems.},
	language = {en},
	urldate = {2024-12-02},
	journal = {Theory and Practice of Logic Programming},
	author = {Hahn, Susana and Janhunen, Tomi and Kaminski, Roland and Romero, Javier and Rühling, Nicolas and Schaub, Torsten},
	month = nov,
	year = {2024},
	keywords = {knowledge representation and nonmonotonic reasoning},
	pages = {1--34},
}

@article{friede_efcient_nodate,
	title = {Efﬁcient {Learning} of {Discrete}-{Continuous} {Computation} {Graphs}},
	abstract = {Numerous models for supervised and reinforcement learning beneﬁt from combinations of discrete and continuous model components. End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable. A popular approach to building discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has mainly focused on computation graphs with a single discrete component on each of the graph’s execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. We then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, we propose dropout residual connections speciﬁcally tailored to stochastic, discrete-continuous computation graphs. With an extensive set of experiments, we show that we can train complex discrete-continuous models which one cannot train with standard stochastic softmax tricks. We also show that complex discrete-stochastic models generalize better than their continuous counterparts on several benchmark datasets.},
	language = {en},
	author = {Friede, David and Niepert, Mathias},
}

@inproceedings{dilkas_weighted_2021,
	title = {Weighted model counting with conditional weights for {Bayesian} networks},
	url = {https://proceedings.mlr.press/v161/dilkas21a.html},
	abstract = {Weighted model counting (WMC) has emerged as the unifying inference mechanism across many (probabilistic) domains. Encoding an inference problem as an instance of WMC typically necessitates adding extra literals and clauses. This is partly so because the predominant definition of WMC assigns weights to models based on weights on literals, and this severely restricts what probability distributions can be represented. We develop a measure-theoretic perspective on WMC and propose a way to encode conditional weights on literals analogously to conditional probabilities. This representation can be as succinct as standard WMC with weights on literals but can also expand as needed to represent probability distributions with less structure. To demonstrate the performance benefits of conditional weights over the addition of extra literals, we develop a new WMC encoding for Bayesian networks and adapt a state-of-the-art WMC algorithm ADDMC to the new format. Our experiments show that the new encoding significantly improves the performance of the algorithm on most benchmark instances.},
	language = {en},
	urldate = {2024-11-27},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Dilkas, Paulius and Belle, Vaishak},
	month = dec,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {386--396},
}

@inproceedings{van_den_broeck_skolemization_2014,
	address = {Vienna, Austria},
	series = {{KR}'14},
	title = {Skolemization for weighted first-order model counting},
	isbn = {978-1-57735-657-8},
	abstract = {First-order model counting emerged recently as a novel reasoning task, at the core of efficient algorithms for probabilistic logics. We present a Skolemization algorithm for model counting problems that eliminates existential quantifiers from a first-order logic theory without changing its weighted model count. For certain subsets of first-order logic, lifted model counters were shown to run in time polynomial in the number of objects in the domain of discourse, where propositional model counters require exponential time. However, these guarantees apply only to Skolem normal form theories (i.e., no existential quantifiers) as the presence of existential quantifiers reduces lifted model counters to propositional ones. Since textbook Skolemization is not sound for model counting, these restrictions precluded efficient model counting for directed models, such as probabilistic logic programs, which rely on existential quantification. Our Skolemization procedure extends the applicability of first-order model counters to these representations. Moreover, it simplifies the design of lifted model counting algorithms.},
	urldate = {2024-11-27},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Principles} of {Knowledge} {Representation} and {Reasoning}},
	publisher = {AAAI Press},
	author = {Van den Broeck, Guy and Meert, Wannes and Darwiche, Adnan},
	month = jul,
	year = {2014},
	pages = {111--120},
}

@article{knoblauch_optimization-centric_2022,
	title = {An {Optimization}-centric {View} on {Bayes}' {Rule}: {Reviewing} and {Generalizing} {Variational} {Inference}},
	volume = {23},
	issn = {1533-7928},
	shorttitle = {An {Optimization}-centric {View} on {Bayes}' {Rule}},
	url = {http://jmlr.org/papers/v23/19-1047.html},
	abstract = {We advocate an optimization-centric view of Bayesian inference. Our inspiration is the representation of Bayes' rule as infinite-dimensional optimization (Csiszar, 1975; Donsker and Varadhan, 1975; Zellner, 1988). Equipped with this perspective, we study Bayesian inference when one does not have access to (1) well-specified priors, (2) well-specified likelihoods, (3) infinite computing power. While these three assumptions underlie the standard Bayesian paradigm, they are typically inappropriate for modern Machine Learning applications. We propose addressing this through an optimization-centric generalization of Bayesian posteriors that we call the Rule of Three (RoT). The RoT can be justified axiomatically and recovers Bayesian, PAC-Bayesian and VI posteriors as special cases. While the RoT is primarily a conceptual and theoretical device, it also encompasses a novel sub-class of tractable posteriors which we call Generalized Variational Inference (GVI) posteriors. Just as the RoT, GVI posteriors are specified by three arguments: a loss, a divergence and a variational family. They also possess a number of desirable properties, including modularity, Frequentist consistency and an interpretation as approximate ELBO. We explore applications of GVI posteriors, and show that they can be used to improve robustness and posterior marginals on Bayesian Neural Networks and Deep Gaussian Processes.},
	number = {132},
	urldate = {2024-11-22},
	journal = {Journal of Machine Learning Research},
	author = {Knoblauch, Jeremias and Jewson, Jack and Damoulas, Theodoros},
	year = {2022},
	pages = {1--109},
}

@misc{arya_automatic_2023,
	title = {Automatic {Differentiation} of {Programs} with {Discrete} {Randomness}},
	url = {http://arxiv.org/abs/2210.08572},
	doi = {10.48550/arXiv.2210.08572},
	abstract = {Automatic differentiation (AD), a technique for constructing new programs which compute the derivative of an original program, has become ubiquitous throughout scientiﬁc computing and deep learning due to the improved performance afforded by gradient-based optimization. However, AD systems have been restricted to the subset of programs that have a continuous dependence on parameters. Programs that have discrete stochastic behaviors governed by distribution parameters, such as ﬂipping a coin with probability p of being heads, pose a challenge to these systems because the connection between the result (heads vs tails) and the parameters (p) is fundamentally discrete. In this paper we develop a new reparameterizationbased methodology that allows for generating programs whose expectation is the derivative of the expectation of the original program. We showcase how this method gives an unbiased and low-variance estimator which is as automated as traditional AD mechanisms. We demonstrate unbiased forward-mode AD of discrete-time Markov chains, agent-based models such as Conway’s Game of Life, and unbiased reverse-mode AD of a particle ﬁlter. Our code package is available at https://github.com/gaurav-arya/StochasticAD.jl.},
	language = {en},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Arya, Gaurav and Schauer, Moritz and Schäfer, Frank and Rackauckas, Chris},
	month = jan,
	year = {2023},
	note = {arXiv:2210.08572 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Mathematics - Probability, todo},
}

@misc{ganardi_circuit_2016,
	title = {Circuit {Evaluation} for {Finite} {Semirings}},
	url = {http://arxiv.org/abs/1602.04560},
	doi = {10.48550/arXiv.1602.04560},
	abstract = {The computational complexity of the circuit evaluation problem for finite semirings is considered, where semirings are not assumed to have an additive or multiplicative identity. The following dichotomy is shown: If a finite semiring is such that (i) the multiplicative semigroup is solvable and (ii) it does not contain a subsemiring with an additive identity \$0\$ and a multiplicative identity \$1 {\textbackslash}neq 0\$, then the circuit evaluation problem for the semiring is in \${\textbackslash}mathsf\{DET\} {\textbackslash}subseteq {\textbackslash}mathsf\{NC\}{\textasciicircum}2\$. In all other cases, the circuit evaluation problem is \${\textbackslash}mathsf\{P\}\$-complete.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Ganardi, Moses and Hucke, Danny and König, Daniel and Lohrey, Markus},
	month = sep,
	year = {2016},
	note = {arXiv:1602.04560},
	keywords = {Computer Science - Computational Complexity},
}

@misc{grov_use_2024,
	title = {On the use of neurosymbolic {AI} for defending against cyber attacks},
	url = {http://arxiv.org/abs/2408.04996},
	abstract = {It is generally accepted that all cyber attacks cannot be prevented, creating a need for the ability to detect and respond to cyber attacks. Both connectionist and symbolic AI are currently being used to support such detection and response. In this paper, we make the case for combining them using neurosymbolic AI. We identify a set of challenges when using AI today and propose a set of neurosymbolic use cases we believe are both interesting research directions for the neurosymbolic AI community and can have an impact on the cyber security ﬁeld. We demonstrate feasibility through two proof-of-concept experiments.},
	language = {en},
	urldate = {2024-11-17},
	publisher = {arXiv},
	author = {Grov, Gudmund and Halvorsen, Jonas and Eckhoff, Magnus Wiik and Hansen, Bjørn Jervell and Eian, Martin and Mavroeidis, Vasileios},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04996 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{soulos_compositional_2024,
	title = {Compositional {Generalization} {Across} {Distributional} {Shifts} with {Sparse} {Tree} {Operations}},
	url = {https://openreview.net/forum?id=fOQunr2E0T},
	abstract = {Neural networks continue to struggle with compositional generalization, and this issue is exacerbated by a lack of massive pre-training. One successful approach for developing neural systems which exhibit human-like compositional generalization is \${\textbackslash}textit\{hybrid\}\$ neurosymbolic techniques. However, these techniques run into the core issues that plague symbolic approaches to AI: scalability and flexibility. The reason for this failure is that at their core, hybrid neurosymbolic models perform symbolic computation and relegate the scalable and flexible neural computation to parameterizing a symbolic system. We investigate a \${\textbackslash}textit\{unified\}\$ neurosymbolic system where transformations in the network can be interpreted simultaneously as both symbolic and neural computation. We extend a unified neurosymbolic architecture called the Differentiable Tree Machine in two central ways. First, we significantly increase the model’s efficiency through the use of sparse vector representations of symbolic structures. Second, we enable its application beyond the restricted set of tree2tree problems to the more general class of seq2seq problems. The improved model retains its prior generalization capabilities and, since there is a fully neural path through the network, avoids the pitfalls of other neurosymbolic techniques that elevate symbolic computation over neural computation.},
	language = {en},
	urldate = {2024-11-12},
	author = {Soulos, Paul and Conklin, Henry and Opper, Mattia and Smolensky, Paul and Gao, Jianfeng and Fernandez, Roland},
	month = nov,
	year = {2024},
	keywords = {todo},
}

@inproceedings{petersen_convolutional_2024,
	title = {Convolutional {Differentiable} {Logic} {Gate} {Networks}},
	url = {https://openreview.net/forum?id=4bKEFyUHT4},
	abstract = {With the increasing inference cost of machine learning models, there is a growing interest in models with fast and efficient inference. Recently, an approach for learning logic gate networks directly via a differentiable relaxation was proposed. Logic gate networks are faster than conventional neural network approaches because their inference only requires logic gate operators such as NAND, OR, and XOR, which are the underlying building blocks of current hardware and can be efficiently executed. We build on this idea, extending it by deep logic gate tree convolutions, logical OR pooling, and residual initializations. This allows scaling logic gate networks up by over one order of magnitude and utilizing the paradigm of convolution. On CIFAR-10, we achieve an accuracy of 86.29\% using only 61 million logic gates, which improves over the SOTA while being 29x smaller.},
	language = {en},
	urldate = {2024-11-12},
	author = {Petersen, Felix and Kuehne, Hilde and Borgelt, Christian and Welzel, Julian and Ermon, Stefano},
	month = nov,
	year = {2024},
}

@inproceedings{wang_compositional_2024,
	title = {A {Compositional} {Atlas} for {Algebraic} {Circuits}},
	url = {https://openreview.net/forum?id=mXlR1FLFDc&referrer=%5Bthe%20profile%20of%20Guy%20Van%20den%20Broeck%5D(%2Fprofile%3Fid%3D~Guy_Van_den_Broeck1)},
	abstract = {Circuits based on sum-product structure have become a ubiquitous representation to compactly encode knowledge, from Boolean functions to probability distributions. By imposing constraints on the structure of such circuits, certain inference queries become tractable, such as model counting and most probable configuration. Recent works have explored analyzing probabilistic and causal inference queries as compositions of basic operations to derive tractability conditions. In this paper, we take an algebraic perspective for compositional inference, and show that a large class of queries---including marginal MAP, probabilistic answer set programming inference, and causal backdoor adjustment---correspond to a combination of basic operations over semirings: aggregation, product, and elementwise mapping. Using this framework, we uncover simple and general sufficient conditions for tractable composition of these operations, in terms of circuit properties (e.g., marginal determinism, compatibility) and conditions on the elementwise mappings. Applying our analysis, we derive novel tractability conditions for many such compositional queries. Our results unify tractability conditions for existing problems on circuits, while providing a blueprint for analysing novel compositional inference queries.},
	language = {en},
	urldate = {2024-11-12},
	author = {Wang, Benjie and Mauá, Denis and Broeck, Guy Van den and Choi, YooJung},
	month = nov,
	year = {2024},
	keywords = {ucla},
}

@misc{defresne_scalable_2023,
	title = {Scalable {Coupling} of {Deep} {Learning} with {Logical} {Reasoning}},
	url = {http://arxiv.org/abs/2305.07617},
	abstract = {In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs. In this paper, we introduce a scalable neural architecture and loss function dedicated to learning the constraints and criteria of NP-hard reasoning problems expressed as discrete Graphical Models. Our loss function solves one of the main limitations of Besag’s pseudo-loglikelihood, enabling learning of high energies. We empirically show it is able to efficiently learn how to solve NP-hard reasoning problems from natural inputs as the symbolic, visual or many-solutions Sudoku problems as well as the energy optimization formulation of the protein design problem, providing data efficiency, interpretability, and a posteriori control over predictions.},
	language = {en},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Defresne, Marianne and Barbe, Sophie and Schiex, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2305.07617 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, todo},
}

@inproceedings{marnette_generalized_2009,
	address = {New York, NY, USA},
	series = {{PODS} '09},
	title = {Generalized schema-mappings: from termination to tractability},
	isbn = {978-1-60558-553-6},
	shorttitle = {Generalized schema-mappings},
	url = {https://dl.acm.org/doi/10.1145/1559795.1559799},
	doi = {10.1145/1559795.1559799},
	abstract = {Data-Exchange is the problem of creating new databases according to a high-level specification called a schema-mapping while preserving the information encoded in a source database. This paper introduces a notion of generalized schema-mapping that enriches the standard schema-mappings (as defined by Fagin et al) with more expressive power. It then proposes a more general and arguably more intuitive notion of semantics that rely on three criteria: Soundness, Completeness and Laconicity (non-redundancy and minimal size). These semantics are shown to coincide precisely with the notion of cores of universal solutions in the framework of Fagin, Kolaitis and Popa. It is also well-defined and of interest for larger classes of schema-mappings and more expressive source databases (with null-values and equality constraints). After an investigation of the key properties of generalized schema-mappings and their semantics, a criterion called Termination of the Oblivious Chase (TOC) is identified that ensures polynomial data-complexity. This criterion strictly generalizes the previously known criterion of Weak-Acyclicity. To prove the tractability of TOC schema-mappings, a new polynomial time algorithm is provided that, unlike the algorithm of Gottlob and Nash from which it is inspired, does not rely on the syntactic property of Weak-Acyclicity. As the problem of deciding whether a Schema-mapping satisfies the TOC criterion is only recursively enumerable, a more restrictive criterion called Super-weak Acylicity (SwA) is identified that can be decided in Polynomial-time while generalizing substantially the notion of Weak-Acyclicity.},
	urldate = {2024-11-04},
	booktitle = {Proceedings of the twenty-eighth {ACM} {SIGMOD}-{SIGACT}-{SIGART} symposium on {Principles} of database systems},
	publisher = {Association for Computing Machinery},
	author = {Marnette, Bruno},
	month = jun,
	year = {2009},
	pages = {13--22},
}

@misc{feldstein_mapping_2024,
	title = {Mapping the {Neuro}-{Symbolic} {AI} {Landscape} by {Architectures}: {A} {Handbook} on {Augmenting} {Deep} {Learning} {Through} {Symbolic} {Reasoning}},
	shorttitle = {Mapping the {Neuro}-{Symbolic} {AI} {Landscape} by {Architectures}},
	url = {http://arxiv.org/abs/2410.22077},
	abstract = {Integrating symbolic techniques with statistical ones is a long-standing problem in artificial intelligence. The motivation is that the strengths of either area match the weaknesses of the other, and – by combining the two – the weaknesses of either method can be limited. Neuro-symbolic AI focuses on this integration where the statistical methods are in particular neural networks. In recent years, there has been significant progress in this research field, where neuro-symbolic systems outperformed logical or neural models alone. Yet, neuro-symbolic AI is, comparatively speaking, still in its infancy and has not been widely adopted by machine learning practitioners. In this survey, we present the first mapping of neuro-symbolic techniques into families of frameworks based on their architectures, with several benefits: Firstly, it allows us to link different strengths of frameworks to their respective architectures. Secondly, it allows us to illustrate how engineers can augment their neural networks while treating the symbolic methods as black-boxes. Thirdly, it allows us to map most of the field so that future researchers can identify closely related frameworks.},
	language = {en},
	urldate = {2024-11-04},
	publisher = {arXiv},
	author = {Feldstein, Jonathan and Dilkas, Paulius and Belle, Vaishak and Tsamoura, Efthymia},
	month = oct,
	year = {2024},
	note = {arXiv:2410.22077 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{benedikt_goal-driven_2018,
	title = {Goal-{Driven} {Query} {Answering} for {Existential} {Rules} {With} {Equality}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11563},
	doi = {10.1609/aaai.v32i1.11563},
	abstract = {Inspired by the magic sets for Datalog, we present a novel goal-driven approach for answering queries over terminating existential rules with equality (aka TGDs and EGDs). Our technique improves the performance of query answering by pruning the consequences that are not relevant for the query. This is challenging in our setting because equalities can potentially affect all predicates in a dataset. We address this problem by combining the existing singularization technique with two new ingredients: an algorithm for identifying the rules relevant to a query and a new magic sets algorithm. We show empirically that our technique can significantly improve the performance of query answering, and that it can mean the difference between answering a query in a few seconds or not being able to process the query at all.},
	language = {en},
	number = {1},
	urldate = {2024-10-28},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Benedikt, Michael and Motik, Boris and Tsamoura, Efthymia},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {query answering},
}

@inproceedings{stol_iid_2024,
	address = {Cham},
	title = {{IID} {Relaxation} by {Logical} {Expressivity}: {A} {Research} {Agenda} for {Fitting} {Logics} to {Neurosymbolic} {Requirements}},
	isbn = {978-3-031-71170-1},
	shorttitle = {{IID} {Relaxation} by {Logical} {Expressivity}},
	doi = {10.1007/978-3-031-71170-1_1},
	abstract = {Neurosymbolic background knowledge and the expressivity required of its logic can break Machine Learning assumptions about data Independence and Identical Distribution. In this position paper we propose to analyze IID relaxation in a hierarchy of logics that fit different use case requirements. We discuss the benefits of exploiting known data dependencies and distribution constraints for Neurosymbolic use cases and argue that the expressivity required for this knowledge has implications for the design of underlying ML routines. This opens a new research agenda with general questions about Neurosymbolic background knowledge and the expressivity required of its logic.},
	language = {en},
	booktitle = {Neural-{Symbolic} {Learning} and {Reasoning}},
	publisher = {Springer Nature Switzerland},
	author = {Stol, Maarten C. and Mileo, Alessandra},
	editor = {Besold, Tarek R. and d’Avila Garcez, Artur and Jimenez-Ruiz, Ernesto and Confalonieri, Roberto and Madhyastha, Pranava and Wagner, Benedikt},
	year = {2024},
	keywords = {Expressivity, Logic Fragments, Neurosymbolic, Non-IID},
	pages = {3--13},
}

@inproceedings{aspis_embed2rule_2024,
	address = {Cham},
	title = {{Embed2Rule} {Scalable} {Neuro}-{Symbolic} {Learning} via {Latent} {Space} {Weak}-{Labelling}},
	isbn = {978-3-031-71167-1},
	shorttitle = {{Embed2Rule}},
	doi = {10.1007/978-3-031-71167-1_11},
	abstract = {Neuro-symbolic approaches have garnered much interest recently as a path toward endowing neural systems with robust reasoning capabilities. Most proposed end-to-end methods assume knowledge to be given in advance and do not scale up over many latent concepts. The recently proposed Embed2Sym tackles the scalability limitation by performing end-to-end neural training of a visual perception component from downstream labels to generate clusters in the latent space of symbolic concepts. These are later used to perform downstream symbolic reasoning but symbolic knowledge is still engineered. Taking inspiration from Embed2Sym, this paper introduces a novel method for scalable neuro-symbolic learning of first-order logic programs from raw data. The learned clusters are optimally labelled using sampled predictions of a pre-trained vision-language model. A SOTA symbolic learner, robust to noise, uses these labels to learn an answer set program that solves the reasoning task. Our approach, called Embed2Rule, is shown to achieve better accuracy than SOTA neuro-symbolic systems on existing benchmark tasks in most cases while scaling up to tasks that require far more complex reasoning and a large number of latent concepts.},
	language = {en},
	booktitle = {Neural-{Symbolic} {Learning} and {Reasoning}},
	publisher = {Springer Nature Switzerland},
	author = {Aspis, Yaniv and Albinhassan, Mohammad and Lobo, Jorge and Russo, Alessandra},
	editor = {Besold, Tarek R. and d’Avila Garcez, Artur and Jimenez-Ruiz, Ernesto and Confalonieri, Roberto and Madhyastha, Pranava and Wagner, Benedikt},
	year = {2024},
	keywords = {Inductive Logic Programming, Neuro-Symbolic AI, Weak-Labelling},
	pages = {195--218},
}

@misc{hinnerichs_towards_2024,
	title = {Towards a fully declarative neuro-symbolic language},
	url = {http://arxiv.org/abs/2405.09521},
	doi = {10.48550/arXiv.2405.09521},
	abstract = {Neuro-symbolic systems (NeSy), which claim to combine the best of both learning and reasoning capabilities of artificial intelligence, are missing a core property of reasoning systems: Declarativeness. The lack of declarativeness is caused by the functional nature of neural predicates inherited from neural networks. We propose and implement a general framework for fully declarative neural predicates, which hence extends to fully declarative NeSy frameworks. We first show that the declarative extension preserves the learning and reasoning capabilities while being able to answer arbitrary queries while only being trained on a single query type.},
	urldate = {2024-10-25},
	publisher = {arXiv},
	author = {Hinnerichs, Tilman and Manhaeve, Robin and Marra, Giuseppe and Dumancic, Sebastijan},
	month = jul,
	year = {2024},
	note = {arXiv:2405.09521},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{giunchiglia_ccn_2024,
	series = {Synergies between {Machine} {Learning} and {Reasoning}},
	title = {{CCN}+: {A} neuro-symbolic framework for deep learning with requirements},
	volume = {171},
	issn = {0888-613X},
	shorttitle = {{CCN}+},
	url = {https://www.sciencedirect.com/science/article/pii/S0888613X24000112},
	doi = {10.1016/j.ijar.2024.109124},
	abstract = {For their outstanding ability of finding hidden patterns in data, deep learning models have been extensively applied in many different domains. However, recent works have shown that, if a set of requirements expressing inherent knowledge about the problem at hand is given, then neural networks often fail to comply with them. This represents a major drawback for deep learning models, as requirements compliance is normally considered a necessary condition for standard software deployment. In this paper, we propose a novel neuro-symbolic framework able to make any neural network compliant by design to a given set of requirements over the output space expressed in full propositional logic. This framework, called CCN+, integrates the requirements into the output layer of the neural network by applying multiple inference rules that ensure compliance with the requirements and adapts the standard binary cross-entropy loss function to the requirement output layer. As a result, not only the outputted predictions are guaranteed to be compliant with the requirements, but the neural network itself learns how to exploit the domain knowledge expressed by the requirements to get better performance. We conduct an extensive experimental evaluation of CCN+ on 19 real-world multi-label classification datasets with propositional logic requirements, including a challenging dataset for autonomous driving. Our experimental analysis confirms that CCN+ is able to outperform both its neural counterparts and the state-of-the-art models.},
	urldate = {2024-10-25},
	journal = {International Journal of Approximate Reasoning},
	author = {Giunchiglia, Eleonora and Tatomir, Alex and Stoian, Mihaela Cătălina and Lukasiewicz, Thomas},
	month = aug,
	year = {2024},
	keywords = {Artificial intelligence, Deep learning, Machine learning, Machine learning with requirements, Neuro-symbolic AI, Safe AI},
	pages = {109124},
}

@misc{daniele_simple_2024,
	title = {Simple and {Effective} {Transfer} {Learning} for {Neuro}-{Symbolic} {Integration}},
	url = {http://arxiv.org/abs/2402.14047},
	abstract = {Deep Learning (DL) techniques have achieved remarkable successes in recent years. However, their ability to generalize and execute reasoning tasks remains a challenge. A potential solution to this issue is Neuro-Symbolic Integration (NeSy), where neural approaches are combined with symbolic reasoning. Most of these methods exploit a neural network to map perceptions to symbols and a logical reasoner to predict the output of the downstream task. These methods exhibit superior generalization capacity compared to fully neural architectures. However, they suffer from several issues, including slow convergence, learning difficulties with complex perception tasks, and convergence to local minima. This paper proposes a simple yet effective method to ameliorate these problems. The key idea involves pretraining a neural model on the downstream task. Then, a NeSy model is trained on the same task via transfer learning, where the weights of the perceptual part are injected from the pretrained network. The key observation of our work is that the neural network fails to generalize only at the level of the symbolic part while being perfectly capable of learning the mapping from perceptions to symbols. We have tested our training strategy on various SOTA NeSy methods and datasets, demonstrating consistent improvements in the aforementioned problems.},
	language = {en},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {Daniele, Alessandro and Campari, Tommaso and Malhotra, Sagar and Serafini, Luciano},
	month = jul,
	year = {2024},
	note = {arXiv:2402.14047 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{camposampiero_towards_2024,
	title = {Towards {Learning} {Abductive} {Reasoning} using {VSA} {Distributed} {Representations}},
	url = {http://arxiv.org/abs/2406.19121},
	abstract = {We introduce the Abductive Rule Learner with Contextawareness (ARLC), a model that solves abstract reasoning tasks based on Learn-VRF. ARLC features a novel and more broadly applicable training objective for abductive reasoning, resulting in better interpretability and higher accuracy when solving Raven’s progressive matrices (RPM). ARLC allows both programming domain knowledge and learning the rules underlying a data distribution. We evaluate ARLC on the I-RAVEN dataset, showcasing state-of-the-art accuracy across both in-distribution and out-of-distribution (unseen attribute-rule pairs) tests. ARLC surpasses neuro-symbolic and connectionist baselines, including large language models, despite having orders of magnitude fewer parameters. We show ARLC’s robustness to post-programming training by incrementally learning from examples on top of programmed knowledge, which only improves its performance and does not result in catastrophic forgetting of the programmed solution. We validate ARLC’s seamless transfer learning from a 2x2 RPM constellation to unseen constellations. Our code is available at https://github.com/IBM/abductive-rulelearner-with-context-awareness.},
	language = {en},
	urldate = {2024-10-22},
	publisher = {arXiv},
	author = {Camposampiero, Giacomo and Hersche, Michael and Terzić, Aleksandar and Wattenhofer, Roger and Sebastian, Abu and Rahimi, Abbas},
	month = aug,
	year = {2024},
	note = {arXiv:2406.19121 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Symbolic Computation},
}

@misc{zhi-xuan_beyond_2024,
	title = {Beyond {Preferences} in {AI} {Alignment}},
	url = {http://arxiv.org/abs/2408.16984},
	abstract = {The dominant practice of AI alignment assumes (1) that preferences are an adequate representation of human values, (2) that human rationality can be understood in terms of maximizing the satisfaction of preferences, and (3) that AI systems should be aligned with the preferences of one or more humans to ensure that they behave safely and in accordance with our values. Whether implicitly followed or explicitly endorsed, these commitments constitute what we term a preferentist approach to AI alignment. In this paper, we characterize and challenge the preferentist approach, describing conceptual and technical alternatives that are ripe for further research. We first survey the limits of rational choice theory as a descriptive model, explaining how preferences fail to capture the thick semantic content of human values, and how utility representations neglect the possible incommensurability of those values. We then critique the normativity of expected utility theory (EUT) for humans and AI, drawing upon arguments showing how rational agents need not comply with EUT, while highlighting how EUT is silent on which preferences are normatively acceptable. Finally, we argue that these limitations motivate a reframing of the targets of AI alignment: Instead of alignment with the preferences of a human user, developer, or humanity-writ-large, AI systems should be aligned with normative standards appropriate to their social roles, such as the role of a general-purpose assistant. Furthermore, these standards should be negotiated and agreed upon by all relevant stakeholders. On this alternative conception of alignment, a multiplicity of AI systems will be able to serve diverse ends, aligned with normative standards that promote mutual benefit and limit harm despite our plural and divergent values.},
	language = {en},
	urldate = {2024-10-22},
	publisher = {arXiv},
	author = {Zhi-Xuan, Tan and Carroll, Micah and Franklin, Matija and Ashton, Hal},
	month = aug,
	year = {2024},
	note = {arXiv:2408.16984 [cs]},
	keywords = {Computer Science - Artificial Intelligence, todo},
}

@misc{noauthor_k_nodate,
	title = {k},
}

@inproceedings{weber_nlprolog_2019,
	address = {Florence, Italy},
	title = {{NLProlog}: {Reasoning} with {Weak} {Unification} for {Question} {Answering} in {Natural} {Language}},
	shorttitle = {{NLProlog}},
	url = {https://aclanthology.org/P19-1618},
	doi = {10.18653/v1/P19-1618},
	abstract = {Rule-based models are attractive for various tasks because they inherently lead to interpretable and explainable decisions and can easily incorporate prior knowledge. However, such systems are difficult to apply to problems involving natural language, due to its large linguistic variability. In contrast, neural models can cope very well with ambiguity by learning distributed representations of words and their composition from data, but lead to models that are difficult to interpret. In this paper, we describe a model combining neural networks with logic programming in a novel manner for solving multi-hop reasoning tasks over natural language. Specifically, we propose to use an Prolog prover which we extend to utilize a similarity function over pretrained sentence encoders. We fine-tune the representations for the similarity function via backpropagation. This leads to a system that can apply rule-based reasoning to natural language, and induce domain-specific natural language rules from training data. We evaluate the proposed system on two different question answering tasks, showing that it outperforms two baselines – BiDAF (Seo et al., 2016a) and FastQA( Weissenborn et al., 2017) on a subset of the WikiHop corpus and achieves competitive results on the MedHop data set (Welbl et al., 2017).},
	urldate = {2024-10-09},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Weber, Leon and Minervini, Pasquale and Münchmeyer, Jannes and Leser, Ulf and Rocktäschel, Tim},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {6151--6161},
}

@book{holt_handbook_2005,
	address = {New York},
	title = {Handbook of {Computational} {Group} {Theory}},
	isbn = {978-0-429-14794-4},
	abstract = {The origins of computation group theory (CGT) date back to the late 19th and early 20th centuries. Since then, the field has flourished, particularly during the past 30 to 40 years, and today it remains a lively and active branch of mathematics.The Handbook of Computational Group Theory offers the first complete treatment of all the fundame},
	publisher = {Chapman and Hall/CRC},
	author = {Holt, Derek F. and Eick, Bettina and O'Brien, Eamonn A.},
	month = jan,
	year = {2005},
	doi = {10.1201/9781420035216},
}

@inproceedings{van_bremen_probabilistic_2023,
	address = {New York, NY, USA},
	series = {{PODS} '23},
	title = {Probabilistic {Query} {Evaluation}: {The} {Combined} {FPRAS} {Landscape}},
	isbn = {9798400701276},
	shorttitle = {Probabilistic {Query} {Evaluation}},
	url = {https://dl.acm.org/doi/10.1145/3584372.3588677},
	doi = {10.1145/3584372.3588677},
	abstract = {We consider the problem of computing the probability of a query over a tuple-independent probabilistic database, known as theprobabilistic query evaluation (PQE) problem. The problem is well-known to be \#¶-hard in data complexity for conjunctive queries in general, as well as for several subclasses of conjunctive queries. Existing approximation approaches for dealing with hard queries have centred on computing the lineage of the query over the database, which can be intractable for all but the smallest of queries due to the exponential dependence of the lineage size on the query length. In this paper, we take a first step towards bridging this gap, by showing how to construct a fully polynomial-time randomized approximation scheme (FPRAS) for the PQE problem for any class of self-join-free conjunctive queries of bounded hypertree width, that runs in time polynomial inboth the query length and database instance size. An interesting consequence of our result is the existence of classes of queries that are \#¶-hard in data complexity to evaluate exactly, yet easy to approximate both in terms of query length and database size.},
	urldate = {2024-10-09},
	booktitle = {Proceedings of the 42nd {ACM} {SIGMOD}-{SIGACT}-{SIGAI} {Symposium} on {Principles} of {Database} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {van Bremen, Timothy and Meel, Kuldeep S.},
	month = jun,
	year = {2023},
	pages = {339--347},
}

@article{yang_engineering_2024,
	title = {Engineering an {Exact} {Pseudo}-{Boolean} {Model} {Counter}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/28660},
	doi = {10.1609/aaai.v38i8.28660},
	abstract = {Model counting, a fundamental task in computer science, involves determining the number of satisfying assignments to a Boolean formula, typically represented in conjunctive normal form (CNF). While model counting for CNF formulas has received extensive attention with a broad range of applications, the study of model counting for Pseudo-Boolean (PB) formulas has been relatively overlooked. Pseudo-Boolean formulas, being more succinct than propositional Boolean formulas, offer greater flexibility in representing real-world problems. Consequently, there is a crucial need to investigate efficient techniques for model counting for PB formulas.

In this work, we propose the first exact Pseudo-Boolean model counter, PBCount , that relies on knowledge compilation approach via algebraic decision diagrams. Our extensive empirical evaluation shows that PBCount  can compute counts for 1513 instances while the current state-of-the-art approach could only handle 1013 instances. Our work opens up several avenues for future work in the context of model counting for PB formulas, such as the development of preprocessing techniques and exploration of approaches other than knowledge compilation.},
	language = {en},
	number = {8},
	urldate = {2024-10-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yang, Suwei and Meel, Kuldeep S.},
	month = mar,
	year = {2024},
	note = {Number: 8},
	keywords = {KRR: Applications},
	pages = {8200--8208},
}

@incollection{gomes_chapter_2021,
	title = {Chapter 25. {Model} {Counting}},
	url = {https://ebooks.iospress.nl/doi/10.3233/FAIA201009},
	urldate = {2024-10-09},
	booktitle = {Handbook of {Satisfiability}},
	publisher = {IOS Press},
	author = {Gomes, Carla P. and Sabharwal, Ashish and Selman, Bart},
	year = {2021},
	doi = {10.3233/FAIA201009},
	pages = {993--1014},
}

@incollection{baader_chapter_2001,
	address = {Amsterdam},
	series = {Handbook of {Automated} {Reasoning}},
	title = {Chapter 8 - {Unification} {Theory}},
	isbn = {978-0-444-50813-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780444508133500102},
	urldate = {2024-10-09},
	booktitle = {Handbook of {Automated} {Reasoning}},
	publisher = {North-Holland},
	author = {Baader, Franz and Snyder, Wayne and Narendran, Paliath and Schmidt-Schauss, Manfred and Schulz, Klaus},
	editor = {Robinson, Alan and Voronkov, Andrei},
	month = jan,
	year = {2001},
	doi = {10.1016/B978-044450813-3/50010-2},
	pages = {445--533},
}

@incollection{nieuwenhuis_chapter_2001,
	address = {Amsterdam},
	series = {Handbook of {Automated} {Reasoning}},
	title = {Chapter 7 - {Paramodulation}-{Based} {Theorem} {Proving}},
	isbn = {978-0-444-50813-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780444508133500096},
	urldate = {2024-10-09},
	booktitle = {Handbook of {Automated} {Reasoning}},
	publisher = {North-Holland},
	author = {Nieuwenhuis, Robert and Rubio, Albert},
	editor = {Robinson, Alan and Voronkov, Andrei},
	month = jan,
	year = {2001},
	doi = {10.1016/B978-044450813-3/50009-6},
	pages = {371--443},
}

@book{robinson_handbook_2001,
	title = {Handbook of {Automated} {Reasoning}},
	isbn = {978-0-08-053279-0},
	abstract = {Handbook of Automated Reasoning},
	language = {en},
	publisher = {Elsevier},
	author = {Robinson, Alan J. A. and Voronkov, Andrei},
	month = jun,
	year = {2001},
	note = {Google-Books-ID: HxaWA4lep\_kC},
	keywords = {Computers / Information Theory},
}

@inproceedings{gutmann_learning_2011,
	address = {Berlin, Heidelberg},
	title = {Learning the {Parameters} of {Probabilistic} {Logic} {Programs} from {Interpretations}},
	isbn = {978-3-642-23780-5},
	shorttitle = {{LFI}-{ProbLog}},
	doi = {10.1007/978-3-642-23780-5_47},
	abstract = {ProbLog is a recently introduced probabilistic extension of the logic programming language Prolog, in which facts can be annotated with the probability that they hold. The advantage of this probabilistic language is that it naturally expresses a generative process over interpretations using a declarative model. Interpretations are relational descriptions or possible worlds. This paper introduces a novel parameter estimation algorithm LFI-ProbLog for learning ProbLog programs from partial interpretations. The algorithm is essentially a Soft-EM algorithm. It constructs a propositional logic formula for each interpretation that is used to estimate the marginals of the probabilistic parameters. The LFI-ProbLog algorithm has been experimentally evaluated on a number of data sets that justifies the approach and shows its effectiveness.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer},
	author = {Gutmann, Bernd and Thon, Ingo and De Raedt, Luc},
	editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
	year = {2011},
	keywords = {Binary Decision Diagram, Boolean Formula, Ground Atom, Kullback Leibler, Logic Program, leuven},
	pages = {581--596},
}

@inproceedings{yang_parameter_2022,
	address = {Cham},
	title = {Parameter {Learning} in {ProbLog} with {Annotated} {Disjunctions}},
	isbn = {978-3-031-01333-1},
	shorttitle = {{EMPLiFI}},
	doi = {10.1007/978-3-031-01333-1_30},
	abstract = {In parameter learning, a partial interpretation most often contains information about only a subset of the parameters in the program. However, standard EM-based algorithms use all interpretations to learn all parameters, which significantly slows down learning. To tackle this issue, we introduce EMPLiFI, an EM-based parameter learning technique for probabilistic logic programs, that improves the efficiency of EM by exploiting the rule-based structure of logic programs. In addition, EMPLiFI enables parameter learning of multi-head annotated disjunctions in ProbLog programs, which was not yet possible in previous methods. Theoretically, we show that EMPLiFI is correct. Empirically, we compare EMPLiFI to LFI-ProbLog and EMBLEM. The results show that EMPLiFI is the most efficient in learning single-head annotated disjunctions. In learning multi-head annotated disjunctions, EMPLiFI is more accurate than EMBLEM, while LFI-ProbLog cannot handle this task.},
	language = {en},
	booktitle = {Advances in {Intelligent} {Data} {Analysis} {XX}},
	publisher = {Springer International Publishing},
	author = {Yang, Wen-Chi and Jain, Arcchit and De Raedt, Luc and Meert, Wannes},
	editor = {Bouadi, Tassadit and Fromont, Elisa and Hüllermeier, Eyke},
	year = {2022},
	keywords = {Expectation maximization, Learning from interpretations, Probabilistic logic programming, leuven},
	pages = {378--391},
}

@incollection{derkinderen_algebraic_2020,
	title = {Algebraic {Circuits} for {Decision} {Theoretic} {Inference} and {Learning}},
	url = {https://ebooks.iospress.nl/doi/10.3233/FAIA200392},
	urldate = {2024-07-15},
	booktitle = {{ECAI} 2020},
	publisher = {IOS Press},
	author = {Derkinderen, Vincent and De Raedt, Luc},
	year = {2020},
	doi = {10.3233/FAIA200392},
	keywords = {leuven},
	pages = {2569--2576},
}

@article{verreet_inference_2022,
	title = {Inference and {Learning} with {Model} {Uncertainty} in {Probabilistic} {Logic} {Programs}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{BetaProbLog}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21245},
	doi = {10.1609/aaai.v36i9.21245},
	abstract = {An issue that has so far received only limited attention in probabilistic logic programming (PLP) is the modelling of so-called epistemic uncertainty, the uncertainty about the model itself. Accurately quantifying this model uncertainty is paramount to robust inference, learning and ultimately decision making. We introduce BetaProbLog, a PLP language that can model epistemic uncertainty. BetaProbLog has sound semantics, an effective inference algorithm that combines Monte Carlo techniques with knowledge compilation, and a parameter learning algorithm. We empirically outperform state-of-the-art methods on probabilistic inference tasks in second-order Bayesian networks, digit classification and discriminative learning in the presence of epistemic uncertainty.},
	language = {en},
	number = {9},
	urldate = {2024-10-07},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Verreet, Victor and Derkinderen, Vincent and Martires, Pedro Zuidberg Dos and Raedt, Luc De},
	month = jun,
	year = {2022},
	note = {Number: 9},
	keywords = {Reasoning Under Uncertainty (RU), leuven},
	pages = {10060--10069},
}

@article{derkinderen_semirings_2024,
	title = {Semirings for probabilistic and neuro-symbolic logic programming},
	issn = {0888-613X},
	url = {https://www.sciencedirect.com/science/article/pii/S0888613X24000173},
	doi = {10.1016/j.ijar.2024.109130},
	abstract = {The field of probabilistic logic programming (PLP) focuses on integrating probabilistic models into programming languages based on logic. Over the past 30 years, numerous languages and frameworks have been developed for modeling, inference and learning in probabilistic logic programs. While originally PLP focused on discrete probability, more recent approaches have incorporated continuous distributions as well as neural networks, effectively yielding neuro-symbolic methods. We provide an overview and synthesis of this domain, thereby contributing a unified algebraic perspective on the different flavors of PLP, showing that many if not most of the extensions of PLP can be cast within a common algebraic logic programming framework, in which facts are labeled with elements of a semiring and disjunction and conjunction are replaced by addition and multiplication. This does not only hold for the PLP variations itself but also for the underlying execution mechanism that is based on (algebraic) model counting. In order to showcase and explain this unified perspective, we focus on the ProbLog language and its extensions.},
	urldate = {2024-01-26},
	journal = {International Journal of Approximate Reasoning},
	author = {Derkinderen, Vincent and Manhaeve, Robin and Zuidberg Dos Martires, Pedro and De Raedt, Luc},
	month = jan,
	year = {2024},
	keywords = {Model counting, Neuro-symbolic AI, Probabilistic logic programming, Semiring programming, leuven},
	pages = {109130},
}

@article{totis_smproblog_2023,
	title = {{smProbLog}: {Stable} {Model} {Semantics} in {ProbLog} for {Probabilistic} {Argumentation}},
	volume = {23},
	issn = {1471-0684, 1475-3081},
	shorttitle = {{smProbLog}},
	url = {https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/smproblog-stable-model-semantics-in-problog-for-probabilistic-argumentation/4C4DAF1C4E5BADA746E4E2F1694AC0A7},
	doi = {10.1017/S147106842300008X},
	abstract = {Argumentation problems are concerned with determining the acceptability of a set of arguments from their relational structure. When the available information is uncertain, probabilistic argumentation frameworks provide modeling tools to account for it. The first contribution of this paper is a novel interpretation of probabilistic argumentation frameworks as probabilistic logic programs. Probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities. We show that the programs representing probabilistic argumentation frameworks do not satisfy a common assumption in probabilistic logic programming (PLP) semantics, which is, that probabilistic facts fully capture the uncertainty in the domain under investigation. The second contribution of this paper is then a novel PLP semantics for programs where a choice of probabilistic facts does not uniquely determine the truth assignment of the logical atoms. The third contribution of this paper is the implementation of a PLP system supporting this semantics: smProbLog. smProbLog is a novel PLP framework based on the PLP language ProbLog. smProbLog supports many inference and learning tasks typical of PLP, which, together with our first contribution, provide novel reasoning tools for probabilistic argumentation. We evaluate our approach with experiments analyzing the computational cost of the proposed algorithms and their application to a dataset of argumentation problems.},
	language = {en},
	number = {6},
	urldate = {2024-10-07},
	journal = {Theory and Practice of Logic Programming},
	author = {Totis, Pietro and Raedt, Luc De and Kimmig, Angelika},
	month = nov,
	year = {2023},
	keywords = {ProbLog, distribution semantics, leuven, probabilistic argumentation, probabilistic logic programming, stable model semantics},
	pages = {1198--1247},
}

@article{gutmann_magic_2011,
	title = {The magic of logical inference in probabilistic programming},
	volume = {11},
	issn = {1475-3081, 1471-0684},
	url = {https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/magic-of-logical-inference-in-probabilistic-programming/1FE8FA0BF6A203B4B75984ECD652C193},
	doi = {10.1017/S1471068411000238},
	abstract = {Today, there exist many different probabilistic programming languages as well as more inference mechanisms for these languages. Still, most logic programming-based languages use backward reasoning based on Selective Linear Definite resolution for inference. While these methods are typically computationally efficient, they often can neither handle infinite and/or continuous distributions nor evidence. To overcome these limitations, we introduce distributional clauses, a variation and extension of Sato's distribution semantics. We also contribute a novel approximate inference method that integrates forward reasoning with importance sampling, a well-known technique for probabilistic inference. In order to achieve efficiency, we integrate two logic programming techniques to direct forward sampling. Magic sets are used to focus on relevant parts of the program, while the integration of backward reasoning allows one to identify and avoid regions of the sample space that are inconsistent with the evidence.},
	language = {en},
	number = {4-5},
	urldate = {2024-10-09},
	journal = {Theory and Practice of Logic Programming},
	author = {Gutmann, Bernd and Thon, Ingo and Kimmig, Angelika and Bruynooghe, Maurice and Raedt, Luc De},
	month = jul,
	year = {2011},
	keywords = {continuous distributions, forward reasoning, leuven, probabilistic logic, sampling},
	pages = {663--680},
}

@article{tsamoura_materializing_2021,
	title = {Materializing knowledge bases via trigger graphs},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3447689.3447699},
	doi = {10.14778/3447689.3447699},
	abstract = {The chase is a well-established family of algorithms used to materialize Knowledge Bases (KBs) for tasks like query answering under dependencies or data cleaning. A general problem of chase algorithms is that they might perform redundant computations. To counter this problem, we introduce the notion of Trigger Graphs (TGs), which guide the execution of the rules avoiding redundant computations. We present the results of an extensive theoretical and empirical study that seeks to answer when and how TGs can be computed and what are the benefits of TGs when applied over real-world KBs. Our results include introducing algorithms that compute (minimal) TGs. We implemented our approach in a new engine, called GLog, and our experiments show that it can be significantly more efficient than the chase enabling us to materialize Knowledge Graphs with 17B facts in less than 40 min using a single machine with commodity hardware.},
	number = {6},
	urldate = {2024-10-08},
	journal = {Proc. VLDB Endow.},
	author = {Tsamoura, Efthymia and Carral, David and Malizia, Enrico and Urbani, Jacopo},
	month = feb,
	year = {2021},
	pages = {943--956},
}

@inproceedings{deutch_circuits_2014,
	title = {Circuits for {Datalog} {Provenance}},
	url = {https://openproceedings.org/ICDT/2014/paper_36.pdf},
	doi = {10.5441/002/ICDT.2014.22},
	abstract = {The annotation of the results of database queries with provenance information has many applications. This paper studies provenance for datalog queries. We start by considering provenance representation by (positive) Boolean expressions, as pioneered in the theories of incomplete and probabilistic databases. We show that even for linear datalog programs the representation of provenance using Boolean expressions incurs a super-polynomial size blowup in data complexity. We address this with an approach that is novel in provenance studies, showing that we can construct in PTIME poly-size (data complexity) provenance representations as Boolean circuits. Then we present optimization techniques that embed the construction of circuits into seminaive datalog evaluation, and further reduce the size of the circuits. We also illustrate the usefulness of our approach in multiple application domains such as query evaluation in probabilistic databases, and in deletion propagation. Next, we study the possibility of extending the circuit approach to the more general framework of semiring annotations introduced in earlier work. We show that for a large and useful class of provenance semirings, we can construct in PTIME poly-size circuits that capture the provenance.},
	language = {en},
	urldate = {2024-10-08},
	publisher = {OpenProceedings.org},
	author = {Deutch, Daniel and Milo, Tova and Roy, Sudeepa and Tannen, Val},
	year = {2014},
	keywords = {Database Technology, Database Theory},
}

@article{vlasselaer_tp-compilation_2016,
	title = {Tp-{Compilation} for inference in probabilistic logic programs},
	volume = {78},
	issn = {0888-613X},
	url = {https://www.sciencedirect.com/science/article/pii/S0888613X16300949},
	doi = {10.1016/j.ijar.2016.06.009},
	abstract = {We propose TP-compilation, a new inference technique for probabilistic logic programs that is based on forward reasoning. TP-compilation proceeds incrementally in that it interleaves the knowledge compilation step for weighted model counting with forward reasoning on the logic program. This leads to a novel anytime algorithm that provides hard bounds on the inferred probabilities. The main difference with existing inference techniques for probabilistic logic programs is that these are a sequence of isolated transformations. Typically, these transformations include conversion of the ground program into an equivalent propositional formula and compilation of this formula into a more tractable target representation for weighted model counting. An empirical evaluation shows that TP-compilation effectively handles larger instances of complex or cyclic real-world problems than current sequential approaches, both for exact and anytime approximate inference. Furthermore, we show that TP-compilation is conducive to inference in dynamic domains as it supports efficient updates to the compiled model.},
	urldate = {2024-10-07},
	journal = {International Journal of Approximate Reasoning},
	author = {Vlasselaer, Jonas and Van den Broeck, Guy and Kimmig, Angelika and Meert, Wannes and De Raedt, Luc},
	month = nov,
	year = {2016},
	keywords = {Dynamic relational models, Knowledge compilation, Probabilistic inference, Probabilistic logic programs, leuven},
	pages = {15--32},
}

@article{zhou_abductive_2019,
	title = {Abductive learning: towards bridging machine learning and logical reasoning},
	volume = {62},
	issn = {1869-1919},
	shorttitle = {Abductive learning},
	url = {https://doi.org/10.1007/s11432-018-9801-4},
	doi = {10.1007/s11432-018-9801-4},
	language = {en},
	number = {7},
	urldate = {2024-10-07},
	journal = {Science China Information Sciences},
	author = {Zhou, Zhi-Hua},
	month = mar,
	year = {2019},
	pages = {76101},
}

@article{dang_juice_2021,
	title = {Juice: {A} {Julia} {Package} for {Logic} and {Probabilistic} {Circuits}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Juice},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17999},
	doi = {10.1609/aaai.v35i18.17999},
	abstract = {Juice is an open-source Julia package providing tools for logic and probabilistic reasoning and learning based on logic circuits (LCs) and probabilistic circuits (PCs). It provides a range of efficient algorithms for probabilistic inference queries, such as computing marginal probabilities (MAR), as well as many more advanced queries. Certain structural circuit properties are needed to achieve this tractability, which Juice helps validate. Additionally, it supports several parameter and structure learning algorithms proposed in the recent literature. By leveraging parallelism (on both CPU and GPU), Juice provides a fast implementation of circuit-based algorithms, which makes it suitable for tackling large-scale datasets and models.},
	language = {en},
	number = {18},
	urldate = {2024-10-07},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Dang, Meihua and Khosravi, Pasha and Liang, Yitao and Vergari, Antonio and Broeck, Guy Van den},
	month = may,
	year = {2021},
	note = {Number: 18},
	keywords = {Open Source Library},
	pages = {16020--16023},
}

@misc{broadrick_probabilistic_2024,
	title = {Probabilistic {Circuits} for {Cumulative} {Distribution} {Functions}},
	url = {http://arxiv.org/abs/2408.04229},
	abstract = {A probabilistic circuit (PC) succinctly expresses a function that represents a multivariate probability distribution and, given sufﬁcient structural properties of the circuit, supports efﬁcient probabilistic inference. Typically a PC computes the probability mass (or density) function (PMF or PDF) of the distribution. We consider PCs instead computing the cumulative distribution function (CDF). We show that for distributions over binary random variables these representations (PMF and CDF) are essentially equivalent, in the sense that one can be transformed to the other in polynomial time. We then show how a similar equivalence holds for distributions over ﬁnite discrete variables using a modiﬁcation of the standard encoding with binary variables that aligns with the CDF semantics. Finally we show that for continuous variables, smooth, decomposable PCs computing PDFs and CDFs can be efﬁciently transformed to each other by modifying only the leaves of the circuit.},
	language = {en},
	urldate = {2024-10-07},
	publisher = {arXiv},
	author = {Broadrick, Oliver and Cao, William and Wang, Benjie and Trapp, Martin and Broeck, Guy Van den},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04229 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{le_van_semiring_2017,
	title = {Semiring {Rank} {Matrix} {Factorization}},
	volume = {29},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/abstract/document/7888519},
	doi = {10.1109/TKDE.2017.2688374},
	abstract = {Rank data, in which each row is a complete or partial ranking of available items (columns), is ubiquitous. Among others, it can be used to represent preferences of users, levels of gene expression, and outcomes of sports events. It can have many types of patterns, among which consistent rankings of a subset of the items in multiple rows, and multiple rows that rank the same subset of the items highly. In this article, we show that the problems of finding such patterns can be formulated within a single generic framework that is based on the concept of semiring matrix factorization. In this framework, we employ the max-product semiring rather than the plus-product semiring common in traditional linear algebra. We apply this semiring matrix factorization framework on two tasks: sparse rank matrix factorization and rank matrix tiling. Experiments on both synthetic and real world datasets show that the framework is capable of discovering different types of structure as well as obtaining high quality solutions.},
	number = {8},
	urldate = {2024-10-07},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Le Van, Thanh and Nijssen, Siegfried and van Leeuwen, Matthijs and De Raedt, Luc},
	month = aug,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Additives, Bioinformatics, Computer science, Data mining, Gene expression, Linear algebra, Rank data, Sparse matrices, integer programming, max-product, pattern set mining, rank matrix factorisation, rank matrix tiling, semiring},
	pages = {1737--1750},
}

@inproceedings{hazra_deep_2023,
	address = {Cham},
	title = {Deep {Explainable} {Relational} {Reinforcement} {Learning}: {A} {Neuro}-{Symbolic} {Approach}},
	isbn = {978-3-031-43421-1},
	shorttitle = {Deep {Explainable} {Relational} {Reinforcement} {Learning}},
	doi = {10.1007/978-3-031-43421-1_13},
	abstract = {Despite its successes, Deep Reinforcement Learning (DRL) yields non-interpretable policies. Moreover, since DRL does not exploit symbolic relational representations, it has difficulties in coping with structural changes in its environment (such as increasing the number of objects). Meanwhile, Relational Reinforcement Learning inherits the relational representations from symbolic planning to learn reusable policies. However, it has so far been unable to scale up and exploit the power of deep neural networks. We propose Deep Explainable Relational Reinforcement Learning (DERRL), a framework that exploits the best of both – neural and symbolic worlds. By resorting to a neuro-symbolic approach, DERRL combines relational representations and constraints from symbolic planning with deep learning to extract interpretable policies. These policies are in the form of logical rules that explain why each decision (or action) is arrived at. Through several experiments, in setups like the Countdown Game, Blocks World, Gridworld, Traffic, and Mingrid, we show that the policies learned by DERRL are adaptable to varying configurations and environmental changes.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}: {Research} {Track}},
	publisher = {Springer Nature Switzerland},
	author = {Hazra, Rishi and De Raedt, Luc},
	editor = {Koutra, Danai and Plant, Claudia and Gomez Rodriguez, Manuel and Baralis, Elena and Bonchi, Francesco},
	year = {2023},
	keywords = {Deep Reinforcement Learning, Explainability, Neuro-Symbolic AI, Relational Reinforcement Learning},
	pages = {213--229},
}

@article{venturato_inference_2024,
	title = {Inference and {Learning} in {Dynamic} {Decision} {Networks} {Using} {Knowledge} {Compilation}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/30042},
	doi = {10.1609/aaai.v38i18.30042},
	abstract = {Decision making under uncertainty in dynamic environments is a fundamental AI problem in which agents need to determine which decisions (or actions) to make at each time step to maximise their expected utility. Dynamic decision networks (DDNs) are an extension of dynamic Bayesian networks with decisions and utilities. DDNs can be used to compactly represent Markov decision processes (MDPs). We propose a novel algorithm called mapl-cirup that leverages knowledge compilation techniques developed for (dynamic) Bayesian networks to perform inference and gradient-based learning in DDNs. Specifically, we knowledge-compile the Bellman update present in DDNs into dynamic decision circuits and evaluate them within an (algebraic) model counting framework. In contrast to other exact symbolic MDP approaches, we obtain differentiable circuits that enable gradient-based parameter learning.},
	language = {en},
	number = {18},
	urldate = {2024-10-07},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Venturato, Gabriele and Derkinderen, Vincent and Martires, Pedro Zuidberg Dos and Raedt, Luc De},
	month = mar,
	year = {2024},
	note = {Number: 18},
	keywords = {RU: Probabilistic Inference, leuven},
	pages = {20567--20576},
}

@inproceedings{martires_exact_2019,
	address = {Honolulu, Hawaii, USA},
	series = {{AAAI}'19/{IAAI}'19/{EAAI}'19},
	title = {Exact and approximate weighted model integration with probability density functions using knowledge compilation},
	isbn = {978-1-57735-809-1},
	url = {https://dl.acm.org/doi/10.1609/aaai.v33i01.33017825},
	doi = {10.1609/aaai.v33i01.33017825},
	abstract = {Weighted model counting has recently been extended to weighted model integration, which can be used to solve hybrid probabilistic reasoning problems. Such problems involve both discrete and continuous probability distributions. We show how standard knowledge compilation techniques (to SDDs and d-DNNFs) apply to weighted model integration, and use it in two novel solvers, one exact and one approximate solver. Furthermore, we extend the class of employable weight functions to actual probability density functions instead of mere polynomial weight functions.},
	urldate = {2024-10-07},
	booktitle = {Proceedings of the {Thirty}-{Third} {AAAI} {Conference} on {Artificial} {Intelligence} and {Thirty}-{First} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference} and {Ninth} {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Martires, Pedro Zuidberg Dos and Dries, Anton and De Raedt, Luc},
	month = jan,
	year = {2019},
	keywords = {leuven},
	pages = {7825--7833},
}

@inproceedings{wang_grounding_2023,
	title = {Grounding {Neural} {Inference} with {Satisfiability} {Modulo} {Theories}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/47167991e38c65a72914763c11cd8d23-Abstract-Conference.html},
	language = {en},
	urldate = {2024-03-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Zifan and Vijayakumar, Saranya and Lu, Kaiji and Ganesh, Vijay and Jha, Somesh and Fredrikson, Matt},
	month = dec,
	year = {2023},
	keywords = {todo},
	pages = {22794--22806},
}

@inproceedings{maene_soft-unification_2024,
	title = {Soft-{Unification} in {Deep} {Probabilistic} {Logic}},
	volume = {36},
	shorttitle = {{DeepSoftLog}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/bf215fa7fe70a38c5e967e59c44a99d0-Abstract-Conference.html},
	language = {en},
	urldate = {2024-02-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Maene, Jaron and De Raedt, Luc},
	month = feb,
	year = {2024},
	keywords = {leuven},
}

@inproceedings{skryagin_sum-product_nodate,
	address = {Vienna},
	title = {Sum-{Product} {Logic}: {Integrating} {Probabilistic} {Circuits} into {DeepProbLog}},
	shorttitle = {{SPLog}},
	abstract = {We introduce Sum-Product Logic (SPLog), a deep probabilistic logic programming language that incorporates learning through predicates encoded as probabilistic circuits, speciﬁcally sum-product networks. We show how existing inference and learning techniques can be adapted for the new language. Our empirical illustrations demonstrate the beneﬁts of supporting symbolic and deep representations, both neural and probabilistic circuit ones for inference and (deep) learning from examples. To the best of our knowledge, this work is the ﬁrst to propose a framework where deep neural networks, probabilistic circuits, expressive probabilistic-logical modeling and reasoning are integrated.},
	language = {en},
	booktitle = {Working {Notes} of the {ICML} 2020 {Workshop} on {Bridge} {Between} {Perception} and {Reasoning}: {Graph} {Neural} {Networks} and {Beyond}},
	author = {Skryagin, Arseny and Stelzner, Karl and Molina, Alejandro and Ventola, Fabrizio and Yu, Zhongjie and Kersting, Kristian},
	keywords = {darmstadt},
}

@inproceedings{soos_engineering_2023,
	address = {{\textless}conf-loc{\textgreater}, {\textless}city{\textgreater}Macao{\textless}/city{\textgreater}, {\textless}country{\textgreater}P.R.China{\textless}/country{\textgreater}, {\textless}/conf-loc{\textgreater}},
	series = {{IJCAI} '23},
	title = {Engineering an efficient approximate {DNF}-counter},
	isbn = {978-1-956792-03-4},
	shorttitle = {pepin},
	url = {https://doi.org/10.24963/ijcai.2023/226},
	doi = {10.24963/ijcai.2023/226},
	abstract = {Model counting is a fundamental problem in many practical applications, including query evaluation in probabilistic databases and failure-probability estimation of networks. In this work, we focus on a variant of this problem where the underlying formula is expressed in Disjunctive Normal Form (DNF), also known as \#DNF. This problem has been shown to be \#P-complete, making it often intractable to solve exactly. Much research has therefore focused on obtaining approximate solutions, particularly in the form of (ε, δ) approximations. The primary contribution of this paper is a new approach, called pepin, an approximate \#DNF counter that significantly outperforms prior state of the art approaches. Our work is based on the recent breakthrough in the context of union of sets in the streaming model. We demonstrate the effectiveness of our approach through extensive experiments and show that it provides an affirmative answer to the challenge of efficiently computing \#DNF.},
	urldate = {2024-05-21},
	booktitle = {Proceedings of the {Thirty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Soos, Mate and Aggarwal, Divesh and Chakraborty, Sourav and Meel, Kuldeep S. and Obremski, Maciej},
	month = aug,
	year = {2023},
	pages = {2031--2038},
}

@inproceedings{lai_fast_2023,
	series = {{AAAI}'23/{IAAI}'23/{EAAI}'23},
	title = {Fast converging anytime model counting},
	volume = {37},
	isbn = {978-1-57735-880-0},
	shorttitle = {{PartialKC}},
	url = {https://doi.org/10.1609/aaai.v37i4.25517},
	doi = {10.1609/aaai.v37i4.25517},
	abstract = {Model counting is a fundamental problem which has been influential in many applications, from artificial intelligence to formal verification. Due to the intrinsic hardness of model counting, approximate techniques have been developed to solve real-world instances of model counting. This paper designs a new anytime approach called PartialKC for approximate model counting. The idea is a form of partial knowledge compilation to provide an unbiased estimate of the model count which can converge to the exact count. Our empirical analysis demonstrates that PartialKC achieves significant scalability and accuracy over prior state-of-the-art approximate counters, including satss and STS. Interestingly, the empirical results show that PartialKC reaches convergence for many instances and therefore provides exact model counting performance comparable to state-of-the-art exact counters.},
	urldate = {2024-01-23},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {AAAI} {Conference} on {Artificial} {Intelligence} and {Thirty}-{Fifth} {Conference} on {Innovative} {Applications} of {Artificial} {Intelligence} and {Thirteenth} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Lai, Yong and Meel, Kuldeep S. and Yap, Roland H. C.},
	month = feb,
	year = {2023},
	pages = {4025--4034},
}

@article{amarilli_conjunctive_2024,
	title = {Conjunctive {Queries} on {Probabilistic} {Graphs}: {The} {Limits} of {Approximability}},
	issn = {1868-8969},
	url = {http://arxiv.org/abs/2309.13287},
	doi = {10.4230/LIPIcs.ICDT.2024.15},
	abstract = {Query evaluation over probabilistic databases is a notoriously intractable problem -- not only in combined complexity, but for many natural queries in data complexity as well. This motivates the study of probabilistic query evaluation through the lens of approximation algorithms, and particularly of combined FPRASes, whose runtime is polynomial in both the query and instance size. In this paper, we focus on tuple-independent probabilistic databases over binary signatures, which can be equivalently viewed as probabilistic graphs. We study in which cases we can devise combined FPRASes for probabilistic query evaluation in this setting. We settle the complexity of this problem for a variety of query and instance classes, by proving both approximability and (conditional) inapproximability results. This allows us to deduce many corollaries of possible independent interest. For example, we show how the results of Arenas et al. on counting fixed-length strings accepted by an NFA imply the existence of an FPRAS for the two-terminal network reliability problem on directed acyclic graphs: this was an open problem until now. We also show that one cannot extend a recent result of van Bremen and Meel that gives a combined FPRAS for self-join-free conjunctive queries of bounded hypertree width on probabilistic databases: neither the bounded-hypertree-width condition nor the self-join-freeness hypothesis can be relaxed. Finally, we complement all our inapproximability results with unconditional lower bounds, showing that DNNF provenance circuits must have at least moderately exponential size in combined complexity.},
	urldate = {2024-03-29},
	author = {Amarilli, Antoine and van Bremen, Timothy and Meel, Kuldeep S.},
	year = {2024},
	note = {arXiv:2309.13287 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Databases},
	pages = {20 pages, 832114 bytes},
}

@article{ceylan_open-world_2021,
	title = {Open-world probabilistic databases: {Semantics}, algorithms, complexity},
	volume = {295},
	issn = {0004-3702},
	shorttitle = {Open-world probabilistic databases},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370221000254},
	doi = {10.1016/j.artint.2021.103474},
	abstract = {Large-scale probabilistic knowledge bases are becoming increasingly important in academia and industry. They are continuously extended with new data, powered by modern information extraction tools that associate probabilities with knowledge base facts. The state of the art to store and process such data is founded on probabilistic databases. Many systems based on probabilistic databases, however, still have certain semantic deficiencies, which limit their potential applications. We revisit the semantics of probabilistic databases, and argue that the closed-world assumption of probabilistic databases, i.e., the assumption that facts not appearing in the database have the probability zero, conflicts with the everyday use of large-scale probabilistic knowledge bases. To address this discrepancy, we propose open-world probabilistic databases, as a new probabilistic data model. In this new data model, the probabilities of unknown facts, also called open facts, can be assigned any probability value from a default probability interval. Our analysis entails that our model aligns better with many real-world tasks such as query answering, relational learning, knowledge base completion, and rule mining. We make various technical contributions. We show that the data complexity dichotomy, between polynomial time and Image 1, for evaluating unions of conjunctive queries on probabilistic databases can be lifted to our open-world model. This result is supported by an algorithm that computes the probabilities of the so-called safe queries efficiently. Based on this algorithm, we prove that evaluating safe queries is in linear time for probabilistic databases, under reasonable assumptions. This remains true in open-world probabilistic databases for a more restricted class of safe queries. We extend our data complexity analysis beyond unions of conjunctive queries, and obtain a host of complexity results for both classical and open-world probabilistic databases. We conclude our analysis with an in-depth investigation of the combined complexity in the respective models.},
	urldate = {2024-10-07},
	journal = {Artificial Intelligence},
	author = {Ceylan, İsmail İlkan and Darwiche, Adnan and Van den Broeck, Guy},
	month = jun,
	year = {2021},
	keywords = {Closed-world assumption, Credal sets, Data complexity, Dichotomy, Inference, Knowledge bases, Learning, Lifted inference, Open-world assumption, Probabilistic databases, Semantics},
	pages = {103474},
}

@inproceedings{pipatsrisawat_new_2008,
	address = {Chicago, Illinois},
	series = {{AAAI}'08},
	title = {New compilation languages based on structured decomposability},
	isbn = {978-1-57735-368-3},
	abstract = {We introduce in this paper two new, complete propositional languages and study their properties in terms of (1) their support for polytime operations and (2) their ability to represent boolean functions compactly. The new languages are based on a structured version of decomposability--a property that underlies a number of tractable languages. The key characteristic of structured decomposability is its support for a polytime conjoin operation, which is known to be intractable for unstructured decomposability. We show that any CNF can be compiled into formulas in the new languages, whose size is only exponential in the treewidth of the CNF. Our study also reveals that one of the languages we identify is as powerful as OBDDs in terms of answering key inference queries, yet is more succinct than OBDDs.},
	urldate = {2024-10-07},
	booktitle = {Proceedings of the 23rd national conference on {Artificial} intelligence - {Volume} 1},
	publisher = {AAAI Press},
	author = {Pipatsrisawat, Knot and Darwiche, Adnan},
	month = jul,
	year = {2008},
	pages = {517--522},
}

@inproceedings{kisa_probabilistic_2014,
	address = {Vienna, Austria},
	series = {{KR}'14},
	title = {Probabilistic sentential decision diagrams},
	isbn = {978-1-57735-657-8},
	shorttitle = {{PSDD}},
	abstract = {We propose the Probabilistic Sentential Decision Diagram (PSDD): A complete and canonical representation of probability distributions defined over the models of a given propositional theory. Each parameter of a PSDD can be viewed as the (conditional) probability of making a decision in a corresponding Sentential Decision Diagram (SDD). The SDD itself is a recently proposed complete and canonical representation of propositional theories. We explore a number of interesting properties of PSDDs, including the independencies that underlie them. We show that the PSDD is a tractable representation. We further show how the parameters of a PSDD can be efficiently estimated, in closed form, from complete data. We empirically evaluate the quality of PS-DDs learned from data, when we have knowledge, a priori, of the domain logical constraints.},
	urldate = {2024-10-07},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Principles} of {Knowledge} {Representation} and {Reasoning}},
	publisher = {AAAI Press},
	author = {Kisa, Doga and Van den Broeck, Guy and Choi, Arthur and Darwiche, Adnan},
	month = jul,
	year = {2014},
	pages = {558--567},
}

@article{darwiche_decomposable_2001,
	title = {Decomposable negation normal form},
	volume = {48},
	issn = {0004-5411},
	shorttitle = {{DNNF}},
	url = {https://dl.acm.org/doi/10.1145/502090.502091},
	doi = {10.1145/502090.502091},
	abstract = {Knowledge compilation has been emerging recently as a new direction of research for dealing with the computational intractability of general propositional reasoning. According to this approach, the reasoning process is split into two phases: an off-line compilation phase and an on-line query-answering phase. In the off-line phase, the propositional theory is compiled into some target language, which is typically a tractable one. In the on-line phase, the compiled target is used to efficiently answer a (potentially) exponential number of queries. The main motivation behind knowledge compilation is to push as much of the computational overhead as possible into the off-line phase, in order to amortize that overhead over all on-line queries. Another motivation behind compilation is to produce very simple on-line reasoning systems, which can be embedded cost-effectively into primitive computational platforms, such as those found in consumer electronics.One of the key aspects of any compilation approach is the target language into which the propositional theory is compiled. Previous target languages included Horn theories, prime implicates/implicants and ordered binary decision diagrams (OBDDs). We propose in this paper a new target compilation language, known as decomposable negation normal form (DNNF), and present a number of its properties that make it of interest to the broad community. Specifically, we show that DNNF is universal; supports a rich set of polynomial--time logical operations; is more space-efficient than OBDDs; and is very simple as far as its structure and algorithms are concerned. Moreover, we present an algorithm for converting any propositional theory in clausal form into a DNNF and show that if the clausal form has a bounded treewidth, then its DNNF compilation has a linear size and can be computed in linear time (treewidth is a graph-theoretic parameter that measures the connectivity of the clausal form). We also propose two techniques for approximating the DNNF compilation of a theory when the size of such compilation is too large to be practical. One of the techniques generates a sound but incomplete compilation, while the other generates a complete but unsound compilation. Together, these approximations bound the exact compilation from below and above in terms of their ability to answer clausal entailment queries. Finally, we show that the class of polynomial--time DNNF operations is rich enough to support relatively complex AI applications, by proposing a specific framework for compiling model-based diagnosis systems.},
	number = {4},
	urldate = {2024-10-07},
	journal = {J. ACM},
	author = {Darwiche, Adnan},
	month = jul,
	year = {2001},
	pages = {608--647},
}

@article{ben_amara_starfishdb_2024,
	title = {{StarfishDB}: {A} {Query} {Execution} {Engine} for {Relational} {Probabilistic} {Programming}},
	volume = {2},
	shorttitle = {{StarfishDB}},
	url = {https://dl.acm.org/doi/10.1145/3654988},
	doi = {10.1145/3654988},
	abstract = {We introduce StarfishDB, a query execution engine optimized for relational probabilistic programming. Our engine adopts the model of Gamma Probabilistic Databases, representing probabilistic programs as a collection of relational constraints, imposed against a generative stochastic process. We extend the model with the support for recursion, factorization and the ability to leverage just-in-time compilation techniques to speed up inference. We test our engine against a state-of-the-art sampler for Latent Dirichlet Allocation.},
	number = {3},
	urldate = {2024-06-05},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Ben Amara, Ouael and Hadouaj, Sami and Meneghetti, Niccolò},
	year = {2024},
	keywords = {Bayesian inference, probabilistic databases, probabilistic programming, todo},
	pages = {185:1--185:31},
}

@article{amarilli_connecting_2020,
	title = {Connecting {Knowledge} {Compilation} {Classes} {Width} {Parameters}},
	volume = {64},
	issn = {1433-0490},
	url = {https://doi.org/10.1007/s00224-019-09930-2},
	doi = {10.1007/s00224-019-09930-2},
	abstract = {The field of knowledge compilation establishes the tractability of many tasks by studying how to compile them to Boolean circuit classes obeying some requirements such as structuredness, decomposability, and determinism. However, in other settings such as intensional query evaluation on databases, we obtain Boolean circuits that satisfy some width bounds, e.g., they have bounded treewidth or pathwidth. In this work, we give a systematic picture of many circuit classes considered in knowledge compilation and show how they can be systematically connected to width measures, through upper and lower bounds. Our upper bounds show that bounded-treewidth circuits can be constructively converted to d-SDNNFs, in time linear in the circuit size and singly exponential in the treewidth; and that bounded-pathwidth circuits can similarly be converted to uOBDDs. We show matching lower bounds on the compilation of monotone DNF or CNF formulas to structured targets, assuming a constant bound on the arity (size of clauses) and degree (number of occurrences of each variable): any d-SDNNF (resp., SDNNF) for such a DNF (resp., CNF) must be of exponential size in its treewidth, and the same holds for uOBDDs (resp., n-OBDDs) when considering pathwidth. Unlike most previous work, our bounds apply to any formula of this class, not just a well-chosen family. Hence, we show that pathwidth and treewidth respectively characterize the efficiency of compiling monotone DNFs to uOBDDs and d-SDNNFs with compilation being singly exponential in the corresponding width parameter. We also show that our lower bounds on CNFs extend to unstructured compilation targets, with an exponential lower bound in the treewidth (resp., pathwidth) when compiling monotone CNFs of constant arity and degree to DNNFs (resp., nFBDDs).},
	language = {en},
	number = {5},
	urldate = {2024-05-20},
	journal = {Theory of Computing Systems},
	author = {Amarilli, Antoine and Capelli, Florent and Monet, Mikaël and Senellart, Pierre},
	month = jul,
	year = {2020},
	keywords = {Boolean function, Circuit, Knowledge compilation, Pathwidth, Treewidth, todo},
	pages = {861--914},
}

@inproceedings{darwiche_compiling_1999,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'99},
	title = {Compiling knowledge into decomposable negation normal form},
	shorttitle = {{DNNF}},
	abstract = {We propose a method for compiling propositional theories into a new tractable form that we refer to as decomposable negation normal form (DNNF). We show a number of results about our compilation approach. First, we show that every propositional theory can be compiled into DNNF and present an algorithm to this effect. Second, we show that if a clausal form has a bounded treewidth, then its DNNF compilation has a linear size and can be computed in linear time - treewidth is a graphtheoretic parameter which measures the connectivity of the clausal form. Third, we show that once a propositional theory is compiled into DNNF, a number of reasoning tasks, such as satisfiability and forgetting, can be performed in linear time. Finally, we propose two techniques for approximating the DNNF compilation of a theory when the size of such compilation is too large to be practical. One of the techniques generates a sound but incomplete compilation, while the other generates a complete but unsound compilation. Together, these approximations bound the exact compilation from below and above in terms for their ability to answer queries.},
	urldate = {2024-07-31},
	booktitle = {Proceedings of the 16th international joint conference on {Artifical} intelligence - {Volume} 1},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Darwiche, Adnan},
	month = jul,
	year = {1999},
	keywords = {todo, ucla},
	pages = {284--289},
}

@article{darwiche_tractable_2001,
	title = {On the {Tractable} {Counting} of {Theory} {Models} and its {Application} to {Truth} {Maintenance} and {Belief} {Revision}},
	volume = {11},
	issn = {1166-3081},
	url = {https://doi.org/10.3166/jancl.11.11-34},
	doi = {10.3166/jancl.11.11-34},
	abstract = {We address in this paper the problem of counting the models of a propositional theory under incremental changes to its literals. Specifcally, we show that if a propositional theory Δ is in a special form that we call smooth, deterministic, decomposable negation normal form (sd-DNNF), then for any consistent set of literals S, we can simultaneously count (in time linear in the size of Δ) the models of Δ ∪ S and the models of every theory Δ ∪ T where T results from adding, removing or flipping a literal in S. We present two results relating to the time and space complexity of compiling propositional theories into sd-DNNF. First, we show that if a conjunctive normal form (CNF) has a bounded treewidth, then it can be compiled into an sd-DNNF in time and space which are linear in its size. Second, we show that sd-DNNF is a strictly more space efficient representation than Free Binary Decision Diagrams (FBDDs). Finally, we discuss some applications of the counting results to truth maintenance systems, belief revision, and model-based diagnosis.},
	number = {1-2},
	urldate = {2024-08-12},
	journal = {Journal of Applied Non-Classical Logics},
	author = {Darwiche, Adnan},
	month = jan,
	year = {2001},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.3166/jancl.11.11-34},
	keywords = {Counting models, belief revision, diagnosis, knowledge compilation, truth maintenance, ucla},
	pages = {11--34},
}

@misc{hazra_can_2024,
	title = {Can {Large} {Language} {Models} {Reason}? {A} {Characterization} via 3-{SAT}},
	shorttitle = {Can {Large} {Language} {Models} {Reason}?},
	url = {http://arxiv.org/abs/2408.07215},
	doi = {10.48550/arXiv.2408.07215},
	abstract = {Large Language Models (LLMs) are said to possess advanced reasoning abilities. However, some skepticism exists as recent works show how LLMs often bypass true reasoning using shortcuts. Current methods for assessing the reasoning abilities of LLMs typically rely on open-source benchmarks that may be overrepresented in LLM training data, potentially skewing performance. We instead provide a computational theory perspective of reasoning, using 3-SAT -- the prototypical NP-complete problem that lies at the core of logical reasoning and constraint satisfaction tasks. By examining the phase transitions in 3-SAT, we empirically characterize the reasoning abilities of LLMs and show how they vary with the inherent hardness of the problems. Our experimental evidence shows that LLMs cannot perform true reasoning, as is required for solving 3-SAT problems.},
	urldate = {2024-10-07},
	publisher = {arXiv},
	author = {Hazra, Rishi and Venturato, Gabriele and Martires, Pedro Zuidberg Dos and De Raedt, Luc},
	month = aug,
	year = {2024},
	note = {arXiv:2408.07215 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{benedikt_benchmarking_2017,
	address = {New York, NY, USA},
	series = {{PODS} '17},
	title = {Benchmarking the {Chase}},
	isbn = {978-1-4503-4198-1},
	url = {https://dl.acm.org/doi/10.1145/3034786.3034796},
	doi = {10.1145/3034786.3034796},
	abstract = {The chase is a family of algorithms used in a number of data management tasks, such as data exchange, answering queries under dependencies, query reformulation with constraints, and data cleaning. It is well established as a theoretical tool for understanding these tasks, and in addition a number of prototype systems have been developed. While individual chase-based systems and particular optimizations of the chase have been experimentally evaluated in the past, we provide the first comprehensive and publicly available benchmark---test infrastructure and a set of test scenarios---for evaluating chase implementations across a wide range of assumptions about the dependencies and the data. We used our benchmark to compare chase-based systems on data exchange and query answering tasks with one another, as well as with systems that can solve similar tasks developed in closely related communities. Our evaluation provided us with a number of new insights concerning the factors that impact the performance of chase implementations.},
	urldate = {2024-09-18},
	booktitle = {Proceedings of the 36th {ACM} {SIGMOD}-{SIGACT}-{SIGAI} {Symposium} on {Principles} of {Database} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Benedikt, Michael and Konstantinidis, George and Mecca, Giansalvatore and Motik, Boris and Papotti, Paolo and Santoro, Donatello and Tsamoura, Efthymia},
	year = {2017},
	pages = {37--52},
}

@inproceedings{zhao_probabilistic_2024,
	title = {Probabilistic {Inference} in {Language} {Models} via {Twisted} {Sequential} {Monte} {Carlo}},
	url = {https://proceedings.mlr.press/v235/zhao24c.html},
	abstract = {Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.},
	language = {en},
	urldate = {2024-10-07},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhao, Stephen and Brekelmans, Rob and Makhzani, Alireza and Grosse, Roger Baker},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {60704--60748},
}

@inproceedings{jiao_valid_2024,
	address = {Cham},
	title = {Valid {Text}-to-{SQL} {Generation} with {Unification}-{Based} {DeepStochLog}},
	isbn = {978-3-031-71167-1},
	doi = {10.1007/978-3-031-71167-1_17},
	abstract = {Large language models have been used to translate natural language questions to SQL queries. Without hard constraints on syntax and database schema, they occasionally produce invalid queries that are not executable. These failures limit the usage of these systems in real-life scenarios. We propose a neurosymbolic framework that imposes SQL syntax and schema constraints with unification-based definite clause grammars and thus guarantees the generation of valid queries. Our framework also builds a bi-directional interface to language models to leverage their natural language understanding abilities. The evaluation results on a subset of SQL grammars show that all our output queries are valid. This work is the first step towards extending language models with unification-based grammars. We demonstrate this extension enhances the validity, execution accuracy, and ground truth alignment of the underlying language model by a large margin. Our code is available at https://github.com/ML-KULeuven/deepstochlog-lm.},
	language = {en},
	booktitle = {Neural-{Symbolic} {Learning} and {Reasoning}},
	publisher = {Springer Nature Switzerland},
	author = {Jiao, Ying and De Raedt, Luc and Marra, Giuseppe},
	editor = {Besold, Tarek R. and d’Avila Garcez, Artur and Jimenez-Ruiz, Ernesto and Confalonieri, Roberto and Madhyastha, Pranava and Wagner, Benedikt},
	year = {2024},
	keywords = {DeepStochLog, Generative neurosymbolic, Language models, Text-to-SQL},
	pages = {312--330},
}

@inproceedings{lagniez_improved_2017,
	address = {Melbourne, Australia},
	series = {{IJCAI}'17},
	title = {An improved decision-{DNNF} compiler},
	isbn = {978-0-9992411-0-3},
	shorttitle = {d4},
	abstract = {We present and evaluate a new compiler, called D4, targeting the Decision-DNNF language. As the state-of-the-art compilers C2D and Dsharp targeting the same language, D4 is a top-down tree-search algorithm exploring the space of propositional interpretations. D4 is based on the same ingredients as those considered in C2D and Dsharp (mainly, disjoint component analysis, conflict analysis and non-chronological backtracking, component caching). D4 takes advantage of a dynamic decomposition approach based on hypergraph partitioning, used sparingly. Some simplification rules are also used to minimize the time spent in the partitioning steps and to promote the quality of the decompositions. Experiments show that the compilation times and the sizes of the Decision-DNNF representations computed by D4 are in many cases significantly lower than the ones obtained by C2D and Dsharp.},
	urldate = {2024-05-08},
	booktitle = {Proceedings of the 26th {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Lagniez, Jean-Marie and Marquis, Pierre},
	month = aug,
	year = {2017},
	pages = {667--673},
}

@inproceedings{hu_amortizing_2023,
	title = {Amortizing intractable inference in large language models},
	url = {https://openreview.net/forum?id=Ouj6p4ca60},
	abstract = {Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest---including sequence continuation, infilling, and other forms of constrained generation---involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate that our approach enables data-efficient adaptation of LLMs to tasks that require multi-step rationalization and tool use.},
	language = {en},
	urldate = {2024-10-07},
	author = {Hu, Edward J. and Jain, Moksh and Elmoznino, Eric and Kaddar, Younesse and Lajoie, Guillaume and Bengio, Yoshua and Malkin, Nikolay},
	month = oct,
	year = {2023},
}

@misc{sun_modern_2024,
	title = {Modern {Datalog} on the {GPU}},
	url = {http://arxiv.org/abs/2311.02206},
	doi = {10.48550/arXiv.2311.02206},
	abstract = {Modern deductive database engines (e.g., LogicBlox and Souffl{\textbackslash}'e) enable their users to write declarative queries which compute recursive deductions over extensional data, leaving their high-performance operationalization (query planning, semi-na{\textbackslash}"ive evaluation, and parallelization) to the engine. Such engines form the backbone of modern high-throughput applications in static analysis, security auditing, social-media mining, and business analytics. State-of-the-art engines are built upon nested loop joins over explicit representations (e.g., BTrees and tries) and ubiquitously employ range indexing to accelerate iterated joins. In this work, we present GDlog: a GPU-based deductive analytics engine (implemented as a CUDA library) which achieves significant performance improvements (5--10x or more) versus prior systems. GDlog is powered by a novel range-indexed SIMD datastructure: the hash-indexed sorted array (HISA). We perform extensive evaluation on GDlog, comparing it against both CPU and GPU-based hash tables and Datalog engines, and using it to support a range of large-scale deductive queries including reachability, same generation, and context-sensitive program analysis. Our experiments show that GDlog achieves performance competitive with modern SIMD hash tables and beats prior work by an order of magnitude in runtime while offering more favorable memory footprint.},
	urldate = {2024-10-07},
	publisher = {arXiv},
	author = {Sun, Yihao and Shovon, Ahmedur Rahman and Gilray, Thomas and Micinski, Kristopher and Kumar, Sidharth},
	month = mar,
	year = {2024},
	note = {arXiv:2311.02206 [cs]},
	keywords = {Computer Science - Databases, Computer Science - Programming Languages},
}

@inproceedings{sidheekh_building_2024,
	address = {Jeju, South Korea},
	title = {Building {Expressive} and {Tractable} {Probabilistic} {Generative} {Models}: {A} {Review}},
	isbn = {978-1-956792-04-1},
	shorttitle = {Building {Expressive} and {Tractable} {Probabilistic} {Generative} {Models}},
	url = {https://www.ijcai.org/proceedings/2024/910},
	doi = {10.24963/ijcai.2024/910},
	abstract = {We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs). We provide a unified perspective on the inherent trade-offs between expressivity and tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field. We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field.},
	language = {en},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the {Thirty}-{ThirdInternational} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Sidheekh, Sahil and Natarajan, Sriraam},
	month = aug,
	year = {2024},
	keywords = {survey},
	pages = {8234--8243},
}

@misc{choi_probabilistic_2020,
	title = {Probabilistic {Circuits}: {A} {Unifying} {Framework} for {Tractable} {Probabilistic} {Models}},
	language = {en},
	author = {Choi, YooJung and Vergari, Antonio and Van den Broeck, Guy},
	year = {2020},
	keywords = {survey, ucla},
}

@misc{amarilli_circus_2024,
	title = {A {Circus} of {Circuits}: {Connections} {Between} {Decision} {Diagrams}, {Circuits}, and {Automata}},
	shorttitle = {A {Circus} of {Circuits}},
	url = {http://arxiv.org/abs/2404.09674},
	abstract = {This document is an introduction to two related formalisms to deﬁne Boolean functions: binary decision diagrams, and Boolean circuits. It presents these formalisms and several of their variants studied in the setting of knowledge compilation. Last, it explains how these formalisms can be connected to the notions of automata over words and trees.},
	language = {en},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Amarilli, Antoine and Arenas, Marcelo and Choi, YooJung and Monet, Mikaël and Broeck, Guy Van den and Wang, Benjie},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09674 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Databases, Computer Science - Formal Languages and Automata Theory, survey, ucla},
}

@misc{darwiche_tractable_2022,
	title = {Tractable {Boolean} and {Arithmetic} {Circuits}},
	url = {http://arxiv.org/abs/2202.02942},
	abstract = {Tractable Boolean and arithmetic circuits have been studied extensively in AI for over two decades now. These circuits were initially proposed as "compiled objects," meant to facilitate logical and probabilistic reasoning, as they permit various types of inference to be performed in linear-time and a feed-forward fashion like neural networks. In more recent years, the role of tractable circuits has significantly expanded as they became a computational and semantical backbone for some approaches that aim to integrate knowledge, reasoning and learning. In this article, we review the foundations of tractable circuits and some associated milestones, while focusing on their core properties and techniques that make them particularly useful for the broad aims of neuro-symbolic AI.},
	language = {en},
	urldate = {2023-08-28},
	publisher = {arXiv},
	author = {Darwiche, Adnan},
	month = feb,
	year = {2022},
	note = {arXiv:2202.02942 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, survey, ucla},
}

@misc{khan_fast_2018,
	title = {Fast and {Scalable} {Bayesian} {Deep} {Learning} by {Weight}-{Perturbation} in {Adam}},
	shorttitle = {Vadam},
	url = {http://arxiv.org/abs/1806.04854},
	abstract = {Uncertainty computation in deep learning is essential to design robust and reliable systems. Variational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximumlikelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such efforts for Gaussian mean-ﬁeld VI. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradient evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than existing VI methods, while obtaining uncertainty estimates of comparable quality. Our empirical results conﬁrm this and further suggest that the weight-perturbation in our algorithm could be useful for exploration in reinforcement learning and stochastic optimization.},
	language = {en},
	urldate = {2024-06-28},
	publisher = {arXiv},
	author = {Khan, Mohammad Emtiyaz and Nielsen, Didrik and Tangkaratt, Voot and Lin, Wu and Gal, Yarin and Srivastava, Akash},
	month = aug,
	year = {2018},
	note = {arXiv:1806.04854 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@inproceedings{loshchilov_decoupled_2018,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	shorttitle = {{AdamW}},
	url = {https://openreview.net/forum?id=Bkg6RiCqY7},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it ``weight decay'' in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at {\textbackslash}url\{https://github.com/loshchil/AdamW-and-SGDW\}},
	language = {en},
	urldate = {2023-09-29},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = sep,
	year = {2018},
}

@misc{lin_can_2024,
	title = {Can {We} {Remove} the {Square}-{Root} in {Adaptive} {Gradient} {Methods}? {A} {Second}-{Order} {Perspective}},
	url = {http://arxiv.org/abs/2402.03496},
	abstract = {Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their rootbased counterpart’s performance on transformers. The second-order perspective also has practical benefits for developing non-diagonal methods that can incorporate arbitrary curvature approximations through the concept of preconditioner invariance. In contrast to root-based methods like Shampoo, root-free counterparts work well and fast with half-precision since they do not require numerically unstable matrix root decompositions and inversions. Overall, our findings provide new insights into the development of adaptive methods and raise important questions regarding the overlooked role of adaptivity in their success.},
	language = {en},
	urldate = {2024-06-28},
	publisher = {arXiv},
	author = {Lin, Wu and Dangel, Felix and Eschenhagen, Runa and Bae, Juhan and Turner, Richard E. and Makhzani, Alireza},
	month = jun,
	year = {2024},
	note = {arXiv:2402.03496 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@article{khan_bayesian_2023,
	title = {The {Bayesian} {Learning} {Rule}},
	volume = {24},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v24/22-0291.html},
	abstract = {We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.},
	number = {281},
	urldate = {2024-10-07},
	journal = {Journal of Machine Learning Research},
	author = {Khan, Mohammad Emtiyaz and Rue, Håvard},
	year = {2023},
	pages = {1--46},
}

@article{liu_tabular_2023,
	title = {From tabular data to knowledge graphs: {A} survey of semantic table interpretation tasks and methods},
	volume = {76},
	issn = {1570-8268},
	shorttitle = {From tabular data to knowledge graphs},
	url = {https://www.sciencedirect.com/science/article/pii/S1570826822000452},
	doi = {10.1016/j.websem.2022.100761},
	abstract = {Tabular data often refers to data that is organized in a table with rows and columns. We observe that this data format is widely used on the Web and within enterprise data repositories. Tables potentially contain rich semantic information that still needs to be interpreted. The process of extracting meaningful information out of tabular data with respect to a semantic artefact, such as an ontology or a knowledge graph, is often referred to as Semantic Table Interpretation (STI) or Semantic Table Annotation. In this survey paper, we aim to provide a comprehensive and up-to-date state-of-the-art review of the different tasks and methods that have been proposed so far to perform STI. First, we propose a new categorization that reflects the heterogeneity of table types that one can encounter, revealing different challenges that need to be addressed. Next, we define five major sub-tasks that STI deals with even if the literature has mostly focused on three sub-tasks so far. We review and group the many approaches that have been proposed into three macro families and we discuss their performance and limitations with respect to the various datasets and benchmarks proposed by the community. Finally, we detail what are the remaining scientific barriers to be able to truly automatically interpret any type of tables that can be found in the wild Web.},
	urldate = {2024-07-03},
	journal = {Journal of Web Semantics},
	author = {Liu, Jixiong and Chabot, Yoan and Troncy, Raphaël and Huynh, Viet-Phi and Labbé, Thomas and Monnin, Pierre},
	month = apr,
	year = {2023},
	keywords = {Knowledge graph, Semantic table interpretation, Table annotation, Tabular data, survey},
	pages = {100761},
}

@article{wan_towards_2024,
	title = {Towards {Efficient} {Neuro}-{Symbolic} {AI}: {From} {Workload} {Characterization} to {Hardware} {Architecture}},
	issn = {2996-6647},
	shorttitle = {Towards {Efficient} {Neuro}-{Symbolic} {AI}},
	url = {https://ieeexplore.ieee.org/document/10682967/?arnumber=10682967},
	doi = {10.1109/TCASAI.2024.3462692},
	abstract = {The remarkable advancements in artificial intelligence (AI), primarily driven by deep neural networks, are facing challenges surrounding unsustainable computational trajectories, limited robustness, and a lack of explainability. To develop next-generation cognitive AI systems, neuro-symbolic AI emerges as a promising paradigm, fusing neural and symbolic approaches to enhance interpretability, robustness, and trustworthiness, while facilitating learning from much less data. Recent neuro-symbolic systems have demonstrated great potential in collaborative human-AI scenarios with reasoning and cognitive capabilities. In this paper, we aim to understand the workload characteristics and potential architectures for neuro-symbolic AI. We first systematically categorize neuro-symbolic AI algorithms, and then experimentally evaluate and analyze them in terms of runtime, memory, computational operators, sparsity, and system characteristics on CPUs, GPUs, and edge SoCs. Our studies reveal that neuro-symbolic models suffer from inefficiencies on off-the-shelf hardware, due to the memory-bound nature of vector-symbolic and logical operations, complex flow control, data dependencies, sparsity variations, and limited scalability. Based on profiling insights, we suggest cross-layer optimization solutions and present a hardware acceleration case study for vector-symbolic architecture to improve the performance, efficiency, and scalability of neuro-symbolic computing. Finally, we discuss the challenges and potential future directions of neuro-symbolic AI from both system and architectural perspectives.},
	urldate = {2024-09-24},
	journal = {IEEE Transactions on Circuits and Systems for Artificial Intelligence},
	author = {Wan, Zishen and Liu, Che-Kai and Yang, Hanchen and Raj, Ritik and Li, Chaojian and You, Haoran and Fu, Yonggan and Wan, Cheng and Li, Sixu and Kim, Youbin and Samajdar, Ananda and Lin, Yingyan (Celine) and Ibrahim, Mohamed and Rabaey, Jan M. and Krishna, Tushar and Raychowdhury, Arijit},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Artificial Intelligence},
	keywords = {Artificial intelligence, Artificial neural networks, Cognition, Computational modeling, Computer architecture, Hardware, Vectors, cognitive AI, domain-specific architecture, neuro-symbolic AI, performance analysis, workload characterization},
	pages = {1--14},
}

@misc{calanzone_logically_2024,
	title = {Logically {Consistent} {Language} {Models} via {Neuro}-{Symbolic} {Integration}},
	url = {http://arxiv.org/abs/2409.13724},
	abstract = {Large language models (LLMs) are a promising venue for natural language understanding and generation. However, current LLMs are far from reliable: they are prone to generating non-factual information and, more crucially, to contradicting themselves when prompted to reason about relations between entities of the world. These problems are currently addressed with large scale fine-tuning or by delegating reasoning to external tools. In this work, we strive for a middle ground and introduce a loss based on neuro-symbolic reasoning that teaches an LLM to be logically consistent with an external set of facts and rules and improves self-consistency even when the LLM is fine-tuned on a limited set of facts. Our approach also allows to easily combine multiple logical constraints at once in a principled way, delivering LLMs that are more consistent w.r.t. all constraints and improve over several baselines w.r.t. a given constraint. Moreover, our method allows LLMs to extrapolate to unseen but semantically similar factual knowledge, represented in unseen datasets, more systematically.},
	language = {en},
	urldate = {2024-09-28},
	publisher = {arXiv},
	author = {Calanzone, Diego and Teso, Stefano and Vergari, Antonio},
	month = sep,
	year = {2024},
	note = {arXiv:2409.13724 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{xie_embedding_2019,
	title = {Embedding {Symbolic} {Knowledge} into {Deep} {Networks}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/7b66b4fd401a271a1c7224027ce111bc-Abstract.html},
	abstract = {In this work, we aim to leverage prior symbolic knowledge to improve the performance of deep models. We propose a graph embedding network that projects propositional formulae (and assignments) onto a manifold via an augmented Graph Convolutional Network (GCN). To generate semantically-faithful embeddings, we develop techniques to recognize node heterogeneity, and semantic regularization that incorporate structural constraints into the embedding. Experiments show that our approach improves the performance of models trained to perform entailment checking and visual relation prediction. Interestingly, we observe a connection between the tractability of the propositional theory representation and the ease of embedding. Future exploration of this connection may elucidate the relationship between knowledge compilation and vector representation learning.},
	urldate = {2024-09-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Xie, Yaqi and Xu, Ziwei and Kankanhalli, Mohan S and Meel, Kuldeep S and Soh, Harold},
	year = {2019},
	keywords = {todo},
}

@misc{loconte_what_2024,
	title = {What is the {Relationship} between {Tensor} {Factorizations} and {Circuits} (and {How} {Can} {We} {Exploit} it)?},
	url = {http://arxiv.org/abs/2409.07953},
	abstract = {This paper establishes a rigorous connection between circuit representations and tensor factorizations, two seemingly distinct yet fundamentally related areas. By connecting these fields, we highlight a series of opportunities that can benefit both communities. Our work generalizes popular tensor factorizations within the circuit language, and unifies various circuit learning algorithms under a single, generalized hierarchical factorization framework. Specifically, we introduce a modular “Lego block” approach to build tensorized circuit architectures. This, in turn, allows us to systematically construct and explore various circuit and tensor factorization models while maintaining tractability. This connection not only clarifies similarities and differences in existing models, but also enables the development of a comprehensive pipeline for building and optimizing new circuit/tensor factorization architectures. We show the effectiveness of our framework through extensive empirical evaluations, and highlight new research opportunities for tensor factorizations in probabilistic modeling.},
	language = {en},
	urldate = {2024-09-16},
	publisher = {arXiv},
	author = {Loconte, Lorenzo and Mari, Antonio and Gala, Gennaro and Peharz, Robert and de Campos, Cassio and Quaeghebeur, Erik and Vessio, Gennaro and Vergari, Antonio},
	month = sep,
	year = {2024},
	note = {arXiv:2409.07953 [cs]},
	keywords = {Computer Science - Machine Learning, edinburgh},
}

@inproceedings{koh_concept_2020,
	title = {Concept {Bottleneck} {Models}},
	url = {https://proceedings.mlr.press/v119/koh20a.html},
	abstract = {We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art models today do not typically support the manipulation of concepts like "the existence of bone spurs", as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts ("bone spurs") or bird attributes ("wing color"). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time.},
	language = {en},
	urldate = {2024-09-06},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {5338--5348},
}

@article{gao_differentiable_2024,
	title = {A differentiable first-order rule learner for inductive logic programming},
	volume = {331},
	issn = {0004-3702},
	shorttitle = {{DFORL}},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370224000444},
	doi = {10.1016/j.artint.2024.104108},
	abstract = {Learning first-order logic programs from relational facts yields intuitive insights into the data. Inductive logic programming (ILP) models are effective in learning first-order logic programs from observed relational data. Symbolic ILP models support rule learning in a data-efficient manner. However, symbolic ILP models are not robust to learn from noisy data. Neuro-symbolic ILP models utilize neural networks to learn logic programs in a differentiable manner which improves the robustness of ILP models. However, most neuro-symbolic methods need a strong language bias to learn logic programs, which reduces the usability and flexibility of ILP models and limits the logic program formats. In addition, most neuro-symbolic ILP methods cannot learn logic programs effectively from both small-size datasets and large-size datasets such as knowledge graphs. In the paper, we introduce a novel differentiable ILP model called differentiable first-order rule learner (DFORL), which is scalable to learn rules from both smaller and larger datasets. Besides, DFORL only needs the number of variables in the learned logic programs as input. Hence, DFORL is easy to use and does not need a strong language bias. We demonstrate that DFORL can perform well on several standard ILP datasets, knowledge graphs, and probabilistic relation facts and outperform several well-known differentiable ILP models. Experimental results indicate that DFORL is a precise, robust, scalable, and computationally cheap differentiable ILP model.},
	urldate = {2024-08-30},
	journal = {Artificial Intelligence},
	author = {Gao, Kun and Inoue, Katsumi and Cao, Yongzhi and Wang, Hanpin},
	month = jun,
	year = {2024},
	keywords = {Differentiable inductive logic programming, Interpretability, Machine learning, Neuro-symbolic method},
	pages = {104108},
}

@inproceedings{shah_acceleration_2020,
	title = {Acceleration of probabilistic reasoning through custom processor architecture},
	url = {https://ieeexplore.ieee.org/abstract/document/9116326},
	doi = {10.23919/DATE48585.2020.9116326},
	abstract = {Probabilistic reasoning is an essential tool for robust decision-making systems because of its ability to explicitly handle real-world uncertainty, constraints and causal relations. Consequently, researchers are developing hybrid models by combining Deep Learning with probabilistic reasoning for safety-critical applications like self-driving vehicles, autonomous drones, etc. However, probabilistic reasoning kernels do not execute efficiently on CPUs or GPUs. This paper, therefore, proposes a custom programmable processor to accelerate sum-product networks, an important probabilistic reasoning execution kernel. The processor has an optimized datapath architecture and memory hierarchy optimized for sum-product networks execution. Experimental results show that the processor, while requiring fewer computational and memory units, achieves a 12x throughput benefit over the Nvidia Jetson TX2 embedded GPU platform.},
	urldate = {2024-08-27},
	booktitle = {2020 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Shah, Nimish and Galindez Olascoaga, Laura I. and Meert, Wannes and Verhelst, Marian},
	month = mar,
	year = {2020},
	note = {ISSN: 1558-1101},
	keywords = {Arithmetic circuits, Cognition, Computer architecture, Custom processor, GPU, Graphics processing units, Instruction sets, Probabilistic logic, Probabilistic reasoning, Registers, Sum-product networks, Throughput, acceleration, leuven},
	pages = {322--325},
}

@inproceedings{ott_how_2023,
	address = {Sienne, Italy},
	title = {How to {Think} {About} {Benchmarking} {Neurosymbolic} {AI}?},
	url = {https://hal.science/hal-04136556},
	abstract = {Neurosymbolic artificial intelligence is a growing field of research aiming at combining neural networks with symbolic systems, including their respective learning and reasoning capabilities. This hybridization can take many shapes which adds to the fragmentation of the field and makes it difficult to compare the existing approaches. If some efforts have been made in the community to define archetypical means of hybridization, many elements are still missing to establish principled comparisons. Amongst those missing elements are formal and broadly accepted definitions of neurosymbolic tasks and their corresponding benchmarks. In this paper, we start from the specific task of multi-label classification with the integration of propositional background knowledge to illustrate how such a benchmarking framework could look like. Based on the benchmarking of one granular task we zoom out and discuss important elements and characteristics of building a full benchmarking suite for more than just one task.},
	urldate = {2024-07-18},
	booktitle = {17th {International} {Workshop} on {Neural}-{Symbolic} {Learning} and {Reasoning}},
	author = {Ott, Johanna and Ledaguenel, Arthur and Hudelot, Céline and Hartwig, Mattis},
	month = jul,
	year = {2023},
	keywords = {Artificial intelligence, Neurosymbolic},
}

@book{de_raedt_statistical_2016,
	series = {Synthesis lectures on artificial intelligence and machine learning},
	title = {Statistical {Relational} {Artiﬁcial} {Intelligence}: {Logic}, {Probability}, and {Computation}},
	volume = {10},
	isbn = {978-1-62705-841-4},
	shorttitle = {{StarAI}},
	abstract = {An intelligent agent interacting with the real world will encounter individual people, courses, test results, drugs prescriptions, chairs, boxes, etc., and needs to reason about properties of these
individuals and relations among them as well as cope with uncertainty. Uncertainty has been studied in probability theory and graphical models, and relations have been studied in logic, in particular in the predicate calculus and its extensions. This book examines the foundations of combining logic and probability into what are called relational probabilistic
models. It introduces representations, inference, and learning techniques for probability, logic, and their combinations.
The book focuses on two representations in detail: Markov logic networks, a relational extension of undirected graphical models and weighted ﬁrst-order predicate calculus formula, and Problog, a probabilistic extension of logic programs that can also be viewed as a Turing-complete relational extension of Bayesian networks.},
	number = {32},
	publisher = {Morgan \& Claypool Publishers},
	author = {De Raedt, Luc and Kersting, Kristian and Natarajan, Sriraam and Poole, David},
	year = {2016},
	keywords = {leuven},
}

@article{frostig_compiling_2018,
	title = {Compiling machine learning programs via high-level tracing},
	volume = {4},
	abstract = {We describe JAX, a domain-specific tracing JIT compiler for generating high-performance accelerator code from pure Python and Numpy machine learning programs. JAX uses the XLA compiler infrastructure to generate optimized code for the program subroutines that are most favorable for acceleration, and these optimized subroutines can be called and orchestrated by arbitrary Python. Because the system is fully compatible with Autograd, it allows forward- and reverse-mode automatic differentiation of Python functions to arbitrary order. Because JAX supports structured control flow, it can generate code for sophisticated machine learning algorithms while maintaining high performance. We show that by combining JAX with Autograd and Numpy we get an easily programmable and highly performant ML system that targets CPUs, GPUs, and TPUs, capable of scaling to multi-core Cloud TPUs.},
	language = {en},
	journal = {Systems for Machine Learning},
	author = {Frostig, Roy and Johnson, Matthew James and Leary, Chris},
	year = {2018},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
	urldate = {2024-08-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
}

@inproceedings{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd, and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
	language = {en},
	urldate = {2024-08-14},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	month = oct,
	year = {2017},
}

@article{xu_convergence_1996,
	title = {On {Convergence} {Properties} of the {EM} {Algorithm} for {Gaussian} {Mixtures}},
	volume = {8},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1996.8.1.129},
	doi = {10.1162/neco.1996.8.1.129},
	abstract = {We build up the mathematical connection between the “Expectation-Maximization” (EM) algorithm and gradient-based approaches for maximum likelihood learning of finite gaussian mixtures. We show that the EM step in parameter space is obtained from the gradient via a projection matrix P, and we provide an explicit expression for the matrix. We then analyze the convergence of EM in terms of special properties of P and provide new results analyzing the effect that P has on the likelihood surface. Based on these mathematical results, we present a comparative discussion of the advantages and disadvantages of EM and other algorithms for the learning of gaussian mixture models.},
	number = {1},
	urldate = {2024-08-14},
	journal = {Neural Computation},
	author = {Xu, Lei and Jordan, Michael I.},
	month = jan,
	year = {1996},
	pages = {129--151},
}

@inproceedings{raz_complexity_2002,
	address = {New York, NY, USA},
	series = {{STOC} '02},
	title = {On the complexity of matrix product},
	isbn = {978-1-58113-495-7},
	url = {https://dl.acm.org/doi/10.1145/509907.509932},
	doi = {10.1145/509907.509932},
	abstract = {We prove a lower bound of Ω(m2 log m) for the size of any arithmetic circuit for the product of two matrices, over the real or complex numbers, as long as the circuit doesn't use products with field elements of absolute value larger than 1 (where mxm is the size of each matrix). That is, our lower bound is super-linear in the number of inputs and is applied for circuits that use addition gates, product gates and products with field elements of absolute value up to 1. More generally, for any c = c(m) ρ 1, we obtain a lower bound of Ω(m2 log2c m) for the size of any arithmetic circuit for the product of two matrices (over the real or complex numbers), as long as the circuit doesn't use products with field elements of absolute value larger than c. We also prove size-depth tradeoffs for such circuits.},
	urldate = {2024-08-13},
	booktitle = {Proceedings of the thiry-fourth annual {ACM} symposium on {Theory} of computing},
	publisher = {Association for Computing Machinery},
	author = {Raz, Ran},
	year = {2002},
	pages = {144--151},
}

@inproceedings{friesen_sum-product_2016,
	title = {The {Sum}-{Product} {Theorem}: {A} {Foundation} for {Learning} {Tractable} {Models}},
	shorttitle = {The {Sum}-{Product} {Theorem}},
	url = {https://proceedings.mlr.press/v48/friesen16.html},
	abstract = {Inference in expressive probabilistic models is generally intractable, which makes them difficult to learn and limits their applicability. Sum-product networks are a class of deep models where, surprisingly, inference remains tractable even when an arbitrary number of hidden layers are present. In this paper, we generalize this result to a much broader set of learning problems: all those where inference consists of summing a function over a semiring. This includes satisfiability, constraint satisfaction, optimization, integration, and others. In any semiring, for summation to be tractable it suffices that the factors of every product have disjoint scopes. This unifies and extends many previous results in the literature. Enforcing this condition at learning time thus ensures that the learned models are tractable. We illustrate the power and generality of this approach by applying it to a new type of structured prediction problem: learning a nonconvex function that can be globally optimized in polynomial time. We show empirically that this greatly outperforms the standard approach of learning without regard to the cost of optimization.},
	language = {en},
	urldate = {2024-08-13},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Friesen, Abram and Domingos, Pedro},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {1909--1918},
}

@book{kolchin_differential_1973,
	title = {Differential {Algebra} \& {Algebraic} {Groups}},
	isbn = {978-0-08-087369-5},
	abstract = {Differential Algebra \& Algebraic Groups},
	language = {en},
	publisher = {Academic Press},
	author = {Kolchin, Ellis Robert},
	month = jun,
	year = {1973},
	keywords = {Mathematics / Algebra / General, Mathematics / Number Theory, Technology \& Engineering / Industrial Health \& Safety},
}

@book{kaplansky_introduction_1957,
	title = {An {Introduction} to {Differential} {Algebra}},
	language = {en},
	publisher = {Hermann},
	author = {Kaplansky, Irving},
	year = {1957},
	note = {Google-Books-ID: R4TQAAAAMAAJ},
}

@book{vollmer_introduction_1999,
	title = {Introduction to {Circuit} {Complexity}: {A} {Uniform} {Approach}},
	isbn = {978-3-540-64310-4},
	shorttitle = {Introduction to {Circuit} {Complexity}},
	abstract = {An advanced textbook giving a broad, modern view of the computational complexity theory of boolean circuits, with extensive references, for theoretical computer scientists and mathematicians.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Vollmer, Heribert},
	month = jun,
	year = {1999},
	note = {Google-Books-ID: 55ZTgOJs8bsC},
	keywords = {Computers / Computer Science, Computers / Information Technology, Computers / Machine Theory, Computers / Programming / Algorithms, Computers / User Interfaces, Mathematics / Counting \& Numeration, Mathematics / Discrete Mathematics, Mathematics / Logic, Mathematics / Numerical Analysis, Technology \& Engineering / Electronics / General},
}

@inproceedings{liu_scaling_2022,
	title = {Scaling {Up} {Probabilistic} {Circuits} by {Latent} {Variable} {Distillation}},
	url = {https://openreview.net/forum?id=067CGykiZTS},
	abstract = {Probabilistic Circuits (PCs) are a unified framework for tractable probabilistic models that support efficient computation of various probabilistic queries (e.g., marginal probabilities). One key challenge is to scale PCs to model large and high-dimensional real-world datasets: we observe that as the number of parameters in PCs increases, their performance immediately plateaus. This phenomenon suggests that the existing optimizers fail to exploit the full expressive power of large PCs. We propose to overcome such bottleneck by latent variable distillation: we leverage the less tractable but more expressive deep generative models to provide extra supervision over the latent variables of PCs. Specifically, we extract information from Transformer-based generative models to assign values to latent variables of PCs, providing guidance to PC optimizers. Experiments on both image and language modeling benchmarks (e.g., ImageNet and WikiText-2) show that latent variable distillation substantially boosts the performance of large PCs compared to their counterparts without latent variable distillation. In particular, on the image modeling benchmarks, PCs achieve competitive performance against some of the widely-used deep generative models, including variational autoencoders and flow-based models, opening up new avenues for tractable generative modeling. Our code can be found at https://github.com/UCLA-StarAI/LVD.},
	language = {en},
	urldate = {2024-08-13},
	author = {Liu, Anji and Zhang, Honghua and Van den Broeck, Guy},
	month = sep,
	year = {2022},
	keywords = {ucla},
}

@inproceedings{choi_relaxing_2017,
	title = {On {Relaxing} {Determinism} in {Arithmetic} {Circuits}},
	url = {https://proceedings.mlr.press/v70/choi17a.html},
	abstract = {The past decade has seen a significant interest in learning tractable probabilistic representations. Arithmetic circuits (ACs) were among the first proposed tractable representations, with some subsequent representations being instances of ACs with weaker or stronger properties. In this paper, we provide a formal basis under which variants on ACs can be compared, and where the precise roles and semantics of their various properties can be made more transparent. This allows us to place some recent developments on ACs in a clearer perspective and to also derive new results for ACs. This includes an exponential separation between ACs with and without determinism; completeness and incompleteness results; and tractability results (or lack thereof) when computing most probable explanations (MPEs).},
	language = {en},
	urldate = {2024-07-16},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Choi, Arthur and Darwiche, Adnan},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	keywords = {ucla},
	pages = {825--833},
}

@article{stehr_probabilistic_2022,
	title = {A probabilistic approximate logic for neuro-symbolic learning and reasoning},
	volume = {124},
	issn = {2352-2208},
	url = {https://www.sciencedirect.com/science/article/pii/S2352220821000821},
	doi = {10.1016/j.jlamp.2021.100719},
	abstract = {As witnessed by recent advances in deep learning technologies, neural network models of very high complexity have been successfully applied in many data-rich domains. Challenges remain, however, if the amount of training data is severely limited, which is often the case due to the cost of acquiring such data or due to interest in systems that are constantly evolving thereby imposing natural limits on how much data can be collected. The core hypothesis explored in this paper is that data (to some degree) can be substituted by domain knowledge, not only addressing the limited data problem but also offering potential improvements in data-rich settings. For the representation of suitable domain theories, we propose Probabilistic Approximate Logic (PALO) to deal with the natural uncertainty associated with such representations and also to serve as a foundation for a new class of neuro-symbolic architectures, in which both neural and symbolic computations can be peacefully and synergistically integrated. Utilizing TensorFlow and Maude as neural and symbolic frameworks, respectively, we discuss our prototypical implementation of PALO in what we call the Logical Imagination Engine (LIME). By means of a small toy example, we convey a glimpse of its capabilities, but we also briefly discuss some real-world applications and how it may serve as a prototypical framework to explore a broader range of neuro-symbolic strategies in the future.},
	urldate = {2024-08-07},
	journal = {Journal of Logical and Algebraic Methods in Programming},
	author = {Stehr, Mark-Oliver and Kim, Minyoung and Talcott, Carolyn L.},
	month = jan,
	year = {2022},
	keywords = {First-order logic, Machine learning, Neural networks, Neuro-symbolic architecture, Probabilistic logic},
	pages = {100719},
}

@article{choi_group_2021,
	title = {Group {Fairness} by {Probabilistic} {Modeling} with {Latent} {Fair} {Decisions}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17431},
	doi = {10.1609/aaai.v35i13.17431},
	abstract = {Machine learning systems are increasingly being used to make impactful decisions such as loan applications and criminal justice risk assessments, and as such, ensuring fairness of these systems is critical. This is often challenging as the labels in the data are biased. This paper studies learning fair probability distributions from biased data by explicitly modeling a latent variable that represents a hidden, unbiased label. In particular, we aim to achieve demographic parity by enforcing certain independencies in the learned model. We also show that group fairness guarantees are meaningful only if the distribution used to provide those guarantees indeed captures the real-world data. In order to closely model the data distribution, we employ probabilistic circuits, an expressive and tractable probabilistic model, and propose an algorithm to learn them from incomplete data. We show on real-world datasets that our approach not only is a better model of how the data was generated than existing methods but also achieves competitive accuracy. Moreover, we also evaluate our approach on a synthetic dataset in which observed labels indeed come from fair labels but with added bias, and demonstrate that the fair labels are successfully retrieved.},
	language = {en},
	number = {13},
	urldate = {2024-08-05},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Choi, YooJung and Dang, Meihua and Van den Broeck, Guy},
	month = may,
	year = {2021},
	note = {Number: 13},
	keywords = {Probabilistic Graphical Models},
	pages = {12051--12059},
}

@inproceedings{eisner_compiling_2005,
	address = {USA},
	series = {{HLT} '05},
	title = {Compiling {Comp} {Ling}: practical weighted dynamic programming and the {Dyna} language},
	shorttitle = {Compiling {Comp} {Ling}},
	url = {https://dl.acm.org/doi/10.3115/1220575.1220611},
	doi = {10.3115/1220575.1220611},
	abstract = {Weighted deduction with aggregation is a powerful theoretical formalism that encompasses many NLP algorithms. This paper proposes a declarative specification language, Dyna; gives general agenda-based algorithms for computing weights and gradients; briefly discusses Dyna-to-Dyna program transformations; and shows that a first implementation of a Dyna-to-C++ compiler produces code that is efficient enough for real NLP research, though still several times slower than hand-crafted code.},
	urldate = {2024-08-06},
	booktitle = {Proceedings of the conference on {Human} {Language} {Technology} and {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Eisner, Jason and Goldlust, Eric and Smith, Noah A.},
	year = {2005},
	pages = {281--290},
}

@article{goodman_semiring_1999,
	title = {Semiring parsing},
	volume = {25},
	issn = {0891-2017},
	abstract = {We synthesize work on parsing algorithms, deductive parsing, and the theory of algebra applied to formal languages into a general system for describing parsers. Each parser performs abstract computations using the operations of a semiring. The system allows a single, simple representation to be used for describing parsers that compute recognition, derivation forests, Viterbi, n-best, inside values, and other values, simply by substituting the operations of different semirings. We also show how to use the same representation, interpreted differently, to compute outside values. The system can be used to describe a wide variety of parsers, including Earley's algorithm, tree adjoining grammar parsing, Graham Harrison Ruzzo parsing, and prefix value computation.},
	number = {4},
	journal = {Comput. Linguist.},
	author = {Goodman, Joshua},
	month = dec,
	year = {1999},
	pages = {573--605},
}

@article{mohri_semiring_2002,
	title = {Semiring frameworks and algorithms for shortest-distance problems},
	volume = {7},
	issn = {1430-189X},
	abstract = {We define general algebraic frameworks for shortest-distance problems based on the structure of semirings. We give a generic algorithm for finding single-source shortest distances in a weighted directed graph when the weights satisfy the conditions of our general semiring framework. The same algorithm can be used to solve efficiently classical shortest paths problems or to find the k-shortest distances in a directed graph. It can be used to solve single-source shortest-distance problems in weighted directed acyclic graphs over any semiring. We examine several semirings and describe some specific instances of our generic algorithms to illustrate their use and compare them with existing methods and algorithms. The proof of the soundness of all algorithms is given in detail, including their pseudocode and a full analysis of their running time complexity.},
	number = {3},
	journal = {J. Autom. Lang. Comb.},
	author = {Mohri, Mehryar},
	month = jan,
	year = {2002},
	pages = {321--350},
}

@misc{liu_scaling_2024,
	title = {Scaling {Tractable} {Probabilistic} {Circuits}: {A} {Systems} {Perspective}},
	shorttitle = {{PyJuice}},
	url = {http://arxiv.org/abs/2406.00766},
	abstract = {Probabilistic Circuits (PCs) are a general framework for tractable deep generative models, which support exact and efficient probabilistic inference on their learned distributions. Recent modeling and training advancements have enabled their application to complex real-world tasks. However, the time and memory inefficiency of existing PC implementations hinders further scaling up. This paper proposes PyJuice, a general GPU implementation design for PCs that improves prior art in several regards. Specifically, PyJuice is 1-2 orders of magnitude faster than existing systems (including very recent ones) at training large-scale PCs. Moreover, PyJuice consumes 2-5x less GPU memory, which enables us to train larger models. At the core of our system is a compilation process that converts a PC into a compact representation amenable to efficient block-based parallelization, which significantly reduces IO and makes it possible to leverage Tensor Cores available in modern GPUs. Empirically, PyJuice can be used to improve state-of-the-art PCs trained on image (e.g., ImageNet32) and language (e.g., WikiText, CommonGen) datasets. We further establish a new set of baselines on natural image and language datasets by benchmarking existing PC structures but with much larger sizes and more training epochs, with the hope of incentivizing future research. Code is available at https: //github.com/Tractables/pyjuice.},
	language = {en},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Liu, Anji and Ahmed, Kareem and Broeck, Guy Van den},
	month = jun,
	year = {2024},
	note = {arXiv:2406.00766 [cs]},
	keywords = {Computer Science - Machine Learning, ucla},
}

@article{pavan_constraint_2023,
	title = {Constraint {Optimization} over {Semirings}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/25522},
	doi = {10.1609/aaai.v37i4.25522},
	abstract = {Interpretations of logical formulas over semirings (other than the Boolean semiring) have applications in various areas of computer science including logic, AI, databases, and security.  Such interpretations provide richer information beyond the truth or falsity of a statement. Examples of such semirings include Viterbi semiring, min-max or access control semiring, tropical semiring, and fuzzy semiring. 
    
    The present work investigates the complexity of constraint optimization problems over semirings. The generic optimization problem we study is the following: Given a propositional formula phi over n variable and a semiring (K,+, . ,0,1), find the maximum value over all possible interpretations of phi over K. This can be seen as a generalization of the well-known satisfiability problem (a propositional formula is satisfiable if and only if the maximum value over all interpretations/assignments over the Boolean semiring is 1).  A related problem is to find an interpretation that achieves the maximum value. In this work, we first focus on these optimization problems over the Viterbi semiring, which we call optConfVal and optConf. 
    
    We first show that for general propositional formulas in negation normal form, optConfVal and optConf are in FP{\textasciicircum}NP. We then investigate optConf when the input formula phi is represented in the conjunctive normal form.  For CNF formulae, we first derive an upper bound on the value of optConf as a function of the number of maximum satisfiable clauses. In particular, we show that if r is the maximum number of satisfiable clauses in a CNF formula with m clauses, then its optConf value is at most 1/4{\textasciicircum}(m-r). Building on this we establish that optConf for CNF formulae is hard for the complexity class FP{\textasciicircum}NP[log]. We also design polynomial-time approximation algorithms and establish an inapproximability for optConfVal. We establish similar complexity results for these optimization problems over other semirings including tropical, fuzzy, and access control semirings.},
	language = {en},
	number = {4},
	urldate = {2024-08-01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Pavan, A. and Meel, Kuldeep S. and Vinodchandran, N. V. and Bhattacharyya, Arnab},
	month = jun,
	year = {2023},
	note = {Number: 4},
	keywords = {CSO: Satisfiability},
	pages = {4070--4077},
}

@article{darwiche_differential_2003,
	title = {A differential approach to inference in {Bayesian} networks},
	volume = {50},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/765568.765570},
	doi = {10.1145/765568.765570},
	abstract = {We present a new approach for inference in Bayesian networks, which is mainly based on partial differentiation. According to this approach, one compiles a Bayesian net­ work into a multivariate polynomial and then computes the partial derivatives of this polynomial with respect to each vari­ able. We show that once such derivatives are made available, one can compute in constant-time answers to a large class of probabilistic queries, which are central to classical inference, parameter estimation, model validation and sensitivity analysis. We present a number of complexity results relating to the compilation of such polyno­ mials and to the computation of their par­ tial derivatives. We argue that the com­ bined simplicity, comprehensiveness and computational complexity of the presented framework is unique among existing frame­ works for inference in Bayesian networks.},
	language = {en},
	number = {3},
	urldate = {2024-02-05},
	journal = {Journal of the ACM},
	author = {Darwiche, Adnan},
	month = may,
	year = {2003},
	keywords = {ucla},
	pages = {280--305},
}

@inproceedings{he_ambiguity-aware_2024,
	title = {Ambiguity-{Aware} {Abductive} {Learning}},
	shorttitle = {{A3BL}},
	url = {https://openreview.net/forum?id=sqv2xP8rfb},
	abstract = {Abductive Learning (ABL) is a promising framework for integrating sub-symbolic perception and logical reasoning through abduction. In this case, the abduction process provides supervision for the perception model from the background knowledge. Nevertheless, this process naturally contains uncertainty, since the knowledge base may be satisfied by numerous potential candidates. This implies that the result of the abduction process, i.e., a set of candidates, is ambiguous; both correct and incorrect candidates are mixed in this set. The prior art of abductive learning selects the candidate that has the minimal inconsistency of the knowledge base. However, this method overlooks the ambiguity in the abduction process and is prone to error when it fails to identify the correct candidates. To address this, we propose Ambiguity-Aware Abductive Learning (\${\textbackslash}textrm\{A\}{\textasciicircum}3{\textbackslash}textrm\{BL\}\$), which evaluates all potential candidates and their probabilities, thus preventing the model from falling into sub-optimal solutions. Both experimental results and theoretical analyses prove that \${\textbackslash}textrm\{A\}{\textasciicircum}3{\textbackslash}textrm\{BL\}\$ markedly enhances ABL by efficiently exploiting the ambiguous abduced supervision.},
	language = {en},
	urldate = {2024-07-11},
	author = {He, Hao-Yuan and Sun, Hui and Xie, Zheng and Li, Ming},
	month = jun,
	year = {2024},
}

@article{skryagin_scalable_2023,
	title = {Scalable {Neural}-{Probabilistic} {Answer} {Set} {Programming}},
	volume = {78},
	copyright = {Copyright (c) 2023 Journal of Artificial Intelligence Research},
	issn = {1076-9757},
	shorttitle = {{SLASH}},
	url = {https://www.jair.org/index.php/jair/article/view/15027},
	doi = {10.1613/jair.1.15027},
	abstract = {The goal of combining the robustness of neural networks and the expressiveness of symbolic methods has rekindled the interest in Neuro-Symbolic AI. Deep Probabilistic Programming Languages (DPPLs) have been developed for probabilistic logic programming to be carried out via the probability estimations of deep neural networks (DNNs). However, recent SOTA DPPL approaches allow only for limited conditional probabilistic queries and do not offer the power of true joint probability estimation. In our work, we propose an easy integration of tractable probabilistic inference within a DPPL. To this end, we introduce SLASH, a novel DPPL that consists of Neural-Probabilistic Predicates (NPPs) and a logic program, united via answer set programming (ASP). NPPs are a novel design principle allowing for combining all deep model types and combinations thereof to be represented as a single probabilistic predicate. In this context, we introduce a novel +/− notation for answering various types of probabilistic queries by adjusting the atom notations of a predicate. To scale well, we show how to prune the stochastically insignificant parts of the (ground) program, speeding up reasoning without sacrificing the predictive performance. We evaluate SLASH on various tasks, including the benchmark task of MNIST addition and Visual Question Answering (VQA).},
	language = {en},
	urldate = {2024-07-19},
	journal = {Journal of Artificial Intelligence Research},
	author = {Skryagin, Arseny and Ochs, Daniel and Dhami, Devendra Singh and Kersting, Kristian},
	month = nov,
	year = {2023},
	keywords = {darmstadt, logic programming, machine learning, neural networks, probabilistic reasoning},
	pages = {579--617},
}

@article{dickens_modeling_2024,
	title = {Modeling {Patterns} for {Neural}-{Symbolic} {Reasoning} {Using} {Energy}-based {Models}},
	volume = {3},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2994-4317},
	url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/31187},
	doi = {10.1609/aaaiss.v3i1.31187},
	abstract = {Neural-symbolic (NeSy) AI strives to empower machine learning and large language models with fast, reliable predictions that exhibit commonsense and trustworthy reasoning by seamlessly integrating neural and symbolic methods. With such a broad scope, several taxonomies have been proposed to categorize this integration, emphasizing knowledge representation, reasoning algorithms, and applications. We introduce a knowledge representation-agnostic taxonomy focusing on the neural-symbolic interface capturing methods that reason with probability, logic, and arithmetic constraints. Moreover, we derive expressions for gradients of a prominent class of learning losses and a formalization of reasoning and learning. Through a rigorous empirical analysis spanning three tasks, we show NeSy approaches reach up to a 37\% improvement over neural baselines in a semi-supervised setting and a 19\% improvement over GPT-4 on question-answering.},
	language = {en},
	number = {1},
	urldate = {2024-07-18},
	journal = {Proceedings of the AAAI Symposium Series},
	author = {Dickens, Charles and Pryor, Connor and Getoor, Lise},
	month = may,
	year = {2024},
	note = {Number: 1},
	keywords = {Machine Learning},
	pages = {90--99},
}

@article{marra_statistical_2024,
	title = {From {Statistical} {Relational} to {Neurosymbolic} {Artificial} {Intelligence}: a {Survey}},
	issn = {0004-3702},
	shorttitle = {From {Statistical} {Relational} to {Neurosymbolic} {Artificial} {Intelligence}},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370223002084},
	doi = {10.1016/j.artint.2023.104062},
	abstract = {This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neurosymbolic and statistical relational artificial intelligence. Neurosymbolic artificial intelligence (NeSy) studies the integration of symbolic reasoning and neural networks, while statistical relational artificial intelligence (StarAI) focuses on integrating logic with probabilistic graphical models. This survey identifies seven shared dimensions between these two subfields of AI. These dimensions can be used to characterize different NeSy and StarAI systems. They are concerned with (1) the approach to logical inference, whether model or proof-based; (2) the syntax of the used logical theories; (3) the logical semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either parameter or structure learning; (5) the presence of symbolic and subsymbolic representations; (6) the degree to which systems capture the original logic, probabilistic, and neural paradigms; and (7) the classes of learning tasks the systems are applied to. By positioning various NeSy and StarAI systems along these dimensions and pointing out similarities and differences between them, this survey contributes fundamental concepts for understanding the integration of learning and reasoning.},
	urldate = {2024-01-23},
	journal = {Artificial Intelligence},
	author = {Marra, Giuseppe and Dumančić, Sebastijan and Manhaeve, Robin and De Raedt, Luc},
	month = jan,
	year = {2024},
	keywords = {Learning and reasoning, Neurosymbolic AI, Probabilistic logics, Statistical relational AI, leuven},
	pages = {104062},
}

@misc{bourhis_pseudo_2022,
	title = {Pseudo {Polynomial}-{Time} {Top}-k {Algorithms} for d-{DNNF} {Circuits}},
	url = {http://arxiv.org/abs/2202.05938},
	abstract = {We are interested in computing k most preferred models of a given d-DNNF circuit C, where the preference relation is based on an algebraic structure called a monotone, totally ordered, semigroup (K, ⊗, {\textless}). In our setting, every literal in C has a value in K and the value of an assignment is an element of K obtained by aggregating using ⊗ the values of the corresponding literals. We present an algorithm that computes k models of C among those having the largest values w.r.t. {\textless}, and show that this algorithm runs in time polynomial in k and in the size of C. We also present a pseudo polynomial-time algorithm for deriving the top-k values that can be reached, provided that an additional (but not very demanding) requirement on the semigroup is satisﬁed. Under the same assumption, we present a pseudo polynomial-time algorithm that transforms C into a d-DNNF circuit C satisﬁed exactly by the models of C having a value among the top-k ones. Finally, focusing on the semigroup (N, +, {\textless}), we compare on a large number of instances the performances of our compilation-based algorithm for computing k top solutions with those of an algorithm tackling the same problem, but based on a partial weighted MaxSAT solver.},
	language = {en},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Bourhis, Pierre and Duchien, Laurence and Dusart, Jérémie and Lonca, Emmanuel and Marquis, Pierre and Quinton, Clément},
	month = may,
	year = {2022},
	note = {arXiv:2202.05938 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@incollection{eiter_weighted_2020,
	title = {Weighted {LARS} for {Quantitative} {Stream} {Reasoning}},
	url = {https://ebooks.iospress.nl/doi/10.3233/FAIA200160},
	urldate = {2024-07-17},
	booktitle = {{ECAI} 2020},
	publisher = {IOS Press},
	author = {Eiter, Thomas and Kiesel, Rafael},
	year = {2020},
	doi = {10.3233/FAIA200160},
	pages = {729--736},
}

@article{schutzenberger_definition_1961,
	title = {On the definition of a family of automata},
	volume = {4},
	issn = {0019-9958},
	url = {https://www.sciencedirect.com/science/article/pii/S001999586180020X},
	doi = {10.1016/S0019-9958(61)80020-X},
	number = {2},
	urldate = {2024-07-17},
	journal = {Information and Control},
	author = {Schützenberger, M. P.},
	month = sep,
	year = {1961},
	pages = {245--270},
}

@inproceedings{sang_performing_2005,
	address = {Pittsburgh, Pennsylvania},
	series = {{AAAI}'05},
	title = {Performing {Bayesian} inference by weighted model counting},
	isbn = {978-1-57735-236-5},
	shorttitle = {{WMC}},
	abstract = {Over the past decade general satisfiability testing algorithms have proven to be surprisingly effective at solving a wide variety of constraint satisfaction problem, such as planning and scheduling (Kautz and Selman 2003). Solving such NP-complete tasks by "compilation to SAT" has turned out to be an approach that is of both practical and theoretical interest. Recently, (Sang et al. 2004) have shown that state of the art SAT algorithms can be efficiently extended to the harder task of counting the number of models (satisfying assignments) of a formula, by employing a technique called component caching. This paper begins to investigate the question of whether "compilation to model-counting" could be a practical technique for solving real-world \#P-complete problems, in particular Bayesian inference. We describe an efficient translation from Bayesian networks to weighted model counting, extend the best model-counting algorithms to weighted model counting, develop an efficient method for computing all marginals in a single counting pass, and evaluate the approach on computationally challenging reasoning problems.},
	urldate = {2024-07-16},
	booktitle = {Proceedings of the 20th national conference on {Artificial} intelligence - {Volume} 1},
	publisher = {AAAI Press},
	author = {Sang, Tian and Bearne, Paul and Kautz, Henry},
	month = jul,
	year = {2005},
	pages = {475--481},
}

@inproceedings{yu_characteristic_2023,
	title = {Characteristic {Circuits}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/6b61c278e483954fee502b49fe71cd14-Abstract-Conference.html},
	language = {en},
	urldate = {2024-05-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yu, Zhongjie and Trapp, Martin and Kersting, Kristian},
	month = dec,
	year = {2023},
	keywords = {darmstadt},
	pages = {34074--34086},
}

@inproceedings{huang_dpll_2005,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'05},
	title = {{DPLL} with a trace: from {SAT} to knowledge compilation},
	shorttitle = {{DPLL} trace},
	abstract = {We show that the trace of an exhaustive DPLL search can be viewed as a compilation of the propositional theory. With different constraints imposed or lifted on the DPLL algorithm, this compilation will belong to the language of d-DNNF, FBDD, and OBDD, respectively. These languages are decreasingly succinct, yet increasingly tractable, supporting such polynomial-time queries as model counting and equivalence testing. Our contribution is thus twofold. First, we provide a uniform framework, supported by empirical evaluations, for compiling knowledge into various languages of interest. Second, we show that given a particular variant of DPLL, by identifying the language membership of its traces, one gains a fundamental understanding of the intrinsic complexity and computational power of the search algorithm itself. As interesting examples, we unveil the "hidden power" of several recent model counters, point to one of their potential limitations, and identify a key limitation of DPLL-based procedures in general.},
	urldate = {2023-08-25},
	booktitle = {Proceedings of the 19th international joint conference on {Artificial} intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Huang, Jinbo and Darwiche, Adnan},
	month = jul,
	year = {2005},
	keywords = {ucla},
	pages = {156--162},
}

@inproceedings{shih_smoothing_2019,
	title = {Smoothing {Structured} {Decomposable} {Circuits}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/940392f5f32a7ade1cc201767cf83e31-Abstract.html},
	abstract = {We study the task of smoothing a circuit, i.e., ensuring that all children of a plus-gate mention the same variables. Circuits serve as the building blocks of state-of-the-art inference algorithms on discrete probabilistic graphical models and probabilistic programs. They are also important for discrete density estimation algorithms. Many of these tasks require the input circuit to be smooth. However, smoothing has not been studied in its own right yet, and only a trivial quadratic algorithm is known. This paper studies efficient smoothing for structured decomposable circuits. We propose a near-linear time algorithm for this task and explore lower bounds for smoothing decomposable circuits, using existing results on range-sum queries. Further, for the important case of All-Marginals, we show a more efficient linear-time algorithm. We validate experimentally the performance of our methods.},
	urldate = {2024-07-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Shih, Andy and Van den Broeck, Guy and Beame, Paul and Amarilli, Antoine},
	year = {2019},
	keywords = {ucla},
}

@misc{chajda_integration_2021,
	title = {Integration in semirings},
	url = {http://arxiv.org/abs/2110.00245},
	abstract = {The concept of integral as an inverse to that of derivation was already introduced for rings and recently also for lattices. Since semirings generalize both rings and bounded distributive lattices, it is natural to investigate integration in semirings. This is our aim in the present paper. We show properties of such integrals from the point of view of semiring operations. Examples of semirings with derivation where integrals are introduced are presented in the paper. These illuminate rather speciﬁc properties of such integrals. We show when the set of all integrals on a given semiring forms a semiring again.},
	language = {en},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {Chajda, Ivan and Länger, Helmut},
	month = oct,
	year = {2021},
	note = {arXiv:2110.00245 [math]},
	keywords = {16Y60, 12K10, Mathematics - Rings and Algebras},
}

@article{dimitrov_derivations_2017,
	title = {Derivations on semirings},
	volume = {1910},
	issn = {0094-243X},
	url = {https://doi.org/10.1063/1.5014005},
	doi = {10.1063/1.5014005},
	abstract = {This paper presents a brief survey of the current state of the derivations in semirings.},
	number = {1},
	urldate = {2024-07-15},
	journal = {AIP Conference Proceedings},
	author = {Dimitrov, Stoyan},
	month = dec,
	year = {2017},
	pages = {060011},
}

@misc{scassola_conditioning_2023,
	title = {Conditioning {Score}-{Based} {Generative} {Models} by {Neuro}-{Symbolic} {Constraints}},
	url = {http://arxiv.org/abs/2308.16534},
	abstract = {Score-based and diffusion models have emerged as effective approaches for both conditional and unconditional generation. Still conditional generation is based on either a specific training of a conditional model or classifier guidance, which requires training a noise-dependent classifier, even when the classifier for uncorrupted data is given. We propose an approach to sample from unconditional score-based generative models enforcing arbitrary logical constraints, without any additional training. Firstly, we show how to manipulate the learned score in order to sample from an un-normalized distribution conditional on a user-defined constraint. Then, we define a flexible and numerically stable neuro-symbolic framework for encoding soft logical constraints. Combining these two ingredients we obtain a general, but approximate, conditional sampling algorithm. We further developed effective heuristics aimed at improving the approximation. Finally, we show the effectiveness of our approach for various types of constraints and data: tabular data, images and time series.},
	language = {en},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {Scassola, Davide and Saccani, Sebastiano and Carbone, Ginevra and Bortolussi, Luca},
	month = aug,
	year = {2023},
	note = {arXiv:2308.16534 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{donadello_compensating_2019,
	title = {Compensating {Supervision} {Incompleteness} with {Prior} {Knowledge} in {Semantic} {Image} {Interpretation}},
	url = {https://ieeexplore.ieee.org/abstract/document/8852413?casa_token=O5MgbwLSWZcAAAAA:H7aXar9qZN6oeBh_yBOWEXw1Wvqk10AjuI4RL-aQ2wzxUW-xzyow23sV6_jRIsl9d6I0c0xvRg},
	doi = {10.1109/IJCNN.2019.8852413},
	abstract = {Semantic Image Interpretation is the task of extracting a structured semantic description from images. This requires the detection of visual relationships: triples 〈subject, relation, object〉 describing a semantic relation between a subject and an object. A pure supervised approach to visual relationship detection requires a complete and balanced training set for all the possible combinations of 〈subject, relation, object〉. However, such training sets are not available and would require a prohibitive human effort. This implies the ability of predicting triples which do not appear in the training set. This problem is called zero-shot learning. State-of-the-art approaches to zero-shot learning exploit similarities among relationships in the training set or external linguistic knowledge. In this paper, we perform zero-shot learning by using Logic Tensor Networks, a novel Statistical Relational Learning framework that exploits both the similarities with other seen relationships and background knowledge, expressed with logical constraints between subjects, relations and objects. The experiments on the Visual Relationship Dataset show that the use of logical constraints outperforms the current methods. This implies that background knowledge can be used to alleviate the incompleteness of training sets.},
	urldate = {2024-07-11},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Donadello, Ivan and Serafini, Luciano},
	month = jul,
	year = {2019},
	note = {ISSN: 2161-4407},
	keywords = {Grounding, Image edge detection, Neural networks, Semantics, Tensors, Training, Visualization},
	pages = {1--8},
}

@article{kiesel_efficient_2022,
	title = {Efficient {Knowledge} {Compilation} {Beyond} {Weighted} {Model} {Counting}},
	volume = {22},
	issn = {1471-0684, 1475-3081},
	url = {https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/efficient-knowledge-compilation-beyond-weighted-model-counting/B85D8CC591869EF970780E94C8B92D1B},
	doi = {10.1017/S147106842200014X},
	abstract = {Quantitative extensions of logic programming often require the solution of so called second level inference tasks, that is, problems that involve a third operation, such as maximization or normalization, on top of addition and multiplication, and thus go beyond the well-known weighted or algebraic model counting setting of probabilistic logic programming under the distribution semantics. We introduce Second Level Algebraic Model Counting (2AMC) as a generic framework for these kinds of problems. As 2AMC is to (algebraic) model counting what forall-exists-SAT is to propositional satisfiability, it is notoriously hard to solve. First level techniques based on Knowledge Compilation (KC) have been adapted for specific 2AMC instances by imposing variable order constraints on the resulting circuit. However, those constraints can severely increase the circuit size and thus decrease the efficiency of such approaches. We show that we can exploit the logical structure of a 2AMC problem to omit parts of these constraints, thus limiting the negative effect. Furthermore, we introduce and implement a strategy to generate a sufficient set of constraints statically, with a priori guarantees for the performance of KC. Our empirical evaluation on several benchmarks and tasks confirms that our theoretical results can translate into more efficient solving in practice.},
	language = {en},
	number = {4},
	urldate = {2024-07-11},
	journal = {Theory and Practice of Logic Programming},
	author = {Kiesel, Rafael and Totis, Pietro and Kimmig, Angelika},
	month = jul,
	year = {2022},
	keywords = {analysis and implementation of languages, design, logic programming methodology and applications},
	pages = {505--522},
}

@book{golan_semirings_2013,
	title = {Semirings and their {Applications}},
	isbn = {978-94-015-9333-5},
	abstract = {There is no branch of mathematics, however abstract, which may not some day be applied to phenomena of the real world. - Nikolai Ivanovich Lobatchevsky This book is an extensively-revised and expanded version of "The Theory of Semirings, with Applicationsin Mathematics and Theoretical Computer Science" [Golan, 1992], first published by Longman. When that book went out of print, it became clear - in light of the significant advances in semiring theory over the past years and its new important applications in such areas as idempotent analysis and the theory of discrete-event dynamical systems - that a second edition incorporating minor changes would not be sufficient and that a major revision of the book was in order. Therefore, though the structure of the first «dition was preserved, the text was extensively rewritten and substantially expanded. In particular, references to many interesting and applications of semiring theory, developed in the past few years, had to be added. Unfortunately, I find that it is best not to go into these applications in detail, for that would entail long digressions into various domains of pure and applied mathematics which would only detract from the unity of the volume and increase its length considerably. However, I have tried to provide an extensive collection of examples to arouse the reader's interest in applications, as well as sufficient citations to allow the interested reader to locate them. For the reader's convenience, an index to these citations is given at the end of the book .},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Golan, Jonathan S.},
	month = apr,
	year = {2013},
	note = {Google-Books-ID: ssDxCAAAQBAJ},
	keywords = {Computers / Data Science / General, Mathematics / Algebra / Abstract, Mathematics / Algebra / General, Mathematics / Calculus, Mathematics / Discrete Mathematics, Mathematics / Functional Analysis, Mathematics / History \& Philosophy, Mathematics / Logic},
}

@incollection{thierrin_insertion_2001,
	address = {Dordrecht},
	title = {Insertion of {Languages} and {Differential} {Semirings}},
	isbn = {978-94-015-9634-3},
	url = {https://doi.org/10.1007/978-94-015-9634-3_26},
	abstract = {The family of languages with addition or union and catenation as operations is a semiring with a partial order making it a semireticulated semigroup or gerbier. Furthermore, the operation of insertion of languages has properties similar to those related to the derivation of sum and product of functions. This suggests considering semirings and in particular gerbiers having some kind of abstract derivation. The investigation of these algebraic structures called differential semirings and gerbiers is the object of this paper.},
	language = {en},
	urldate = {2024-07-10},
	booktitle = {Where {Mathematics}, {Computer} {Science}, {Linguistics} and {Biology} {Meet}: {Essays} in honour of {Gheorghe} {Păun}},
	publisher = {Springer Netherlands},
	author = {Thierrin, Gabriel},
	editor = {Martín-Vide, Carlos and Mitrana, Victor},
	year = {2001},
	doi = {10.1007/978-94-015-9634-3_26},
	pages = {287--296},
}

@article{fierens_inference_2015,
	title = {Inference and learning in probabilistic logic programs using weighted {Boolean} formulas},
	volume = {15},
	issn = {1471-0684, 1475-3081},
	shorttitle = {{ProbLog}},
	doi = {10.1017/S1471068414000076},
	abstract = {Probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities. This paper investigates how classical inference and learning tasks known from the graphical model community can be tackled for probabilistic logic programs. Several such tasks, such as computing the marginals, given evidence and learning from (partial) interpretations, have not really been addressed for probabilistic logic programs before. The first contribution of this paper is a suite of efficient algorithms for various inference tasks. It is based on the conversion of the program and the queries and evidence to a weighted Boolean formula. This allows us to reduce inference tasks to well-studied tasks, such as weighted model counting, which can be solved using state-of-the-art methods known from the graphical model and knowledge compilation literature. The second contribution is an algorithm for parameter estimation in the learning from interpretations setting. The algorithm employs expectation-maximization, and is built on top of the developed inference algorithms. The proposed approach is experimentally evaluated. The results show that the inference algorithms improve upon the state of the art in probabilistic logic programming, and that it is indeed possible to learn the parameters of a probabilistic logic program from interpretations.},
	language = {en},
	number = {3},
	urldate = {2022-12-13},
	journal = {Theory and Practice of Logic Programming},
	author = {Fierens, Daan and Van den Broeck, Guy and Renkens, Joris and Shterionov, Dimitar and Gutmann, Bernd and Thon, Ingo and Janssens, Gerda and De Raedt, Luc},
	month = may,
	year = {2015},
	note = {Publisher: Cambridge University Press},
	keywords = {leuven, parameter learning, probabilistic inference, probabilistic logic programming},
	pages = {358--401},
}

@inproceedings{maene_hardness_2024,
	title = {On the {Hardness} of {Probabilistic} {Neurosymbolic} {Learning}},
	url = {https://openreview.net/forum?id=vxPmrxKe0J},
	abstract = {The limitations of purely neural learning have sparked an interest in probabilistic neurosymbolic models, which combine neural networks with probabilistic logical reasoning. As these neurosymbolic models are trained with gradient descent, we study the complexity of differentiating probabilistic reasoning. We prove that although approximating these gradients is intractable in general, it becomes tractable during training. Furthermore, we introduce *WeightME*, an unbiased gradient estimator based on model sampling. Under mild assumptions, WeightME approximates the gradient with probabilistic guarantees using a logarithmic number of calls to a SAT solver. Lastly, we evaluate the necessity of these guarantees on the gradient. Our experiments indicate that the existing biased approximations indeed struggle to optimize even when exact solving is still feasible.},
	language = {en},
	urldate = {2024-07-03},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	author = {Maene, Jaron and Derkinderen, Vincent and De Raedt, Luc},
	month = jun,
	year = {2024},
}

@inproceedings{manhaeve_deepproblog_2018,
	title = {{DeepProbLog}: {Neural} {Probabilistic} {Logic} {Programming}},
	volume = {31},
	shorttitle = {{DeepProbLog}},
	abstract = {We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.},
	urldate = {2022-09-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Manhaeve, Robin and Dumancic, Sebastijan and Kimmig, Angelika and Demeester, Thomas and De Raedt, Luc},
	year = {2018},
	keywords = {leuven},
}

@incollection{chakraborty_chapter_2021,
	title = {Chapter 26. {Approximate} {Model} {Counting}},
	urldate = {2023-12-22},
	booktitle = {Handbook of {Satisfiability}},
	publisher = {IOS Press},
	author = {Chakraborty, Supratik and Meel, Kuldeep S. and Vardi, Moshe Y.},
	year = {2021},
	doi = {10.3233/FAIA201010},
	pages = {1015--1045},
}

@article{chavira_probabilistic_2008,
	title = {On probabilistic inference by weighted model counting},
	volume = {172},
	issn = {0004-3702},
	doi = {10.1016/j.artint.2007.11.002},
	abstract = {A recent and effective approach to probabilistic inference calls for reducing the problem to one of weighted model counting (WMC) on a propositional knowledge base. Specifically, the approach calls for encoding the probabilistic model, typically a Bayesian network, as a propositional knowledge base in conjunctive normal form (CNF) with weights associated to each model according to the network parameters. Given this CNF, computing the probability of some evidence becomes a matter of summing the weights of all CNF models consistent with the evidence. A number of variations on this approach have appeared in the literature recently, that vary across three orthogonal dimensions. The first dimension concerns the specific encoding used to convert a Bayesian network into a CNF. The second dimensions relates to whether weighted model counting is performed using a search algorithm on the CNF, or by compiling the CNF into a structure that renders WMC a polytime operation in the size of the compiled structure. The third dimension deals with the specific properties of network parameters (local structure) which are captured in the CNF encoding. In this paper, we discuss recent work in this area across the above three dimensions, and demonstrate empirically its practical importance in significantly expanding the reach of exact probabilistic inference. We restrict our discussion to exact inference and model counting, even though other proposals have been extended for approximate inference and approximate model counting.},
	number = {6},
	urldate = {2023-11-24},
	journal = {Artificial Intelligence},
	author = {Chavira, Mark and Darwiche, Adnan},
	month = apr,
	year = {2008},
	keywords = {Bayesian networks, Compilation, Exact inference, Weighted model counting},
	pages = {772--799},
}

@inproceedings{xu_semantic_2018,
	title = {A {Semantic} {Loss} {Function} for {Deep} {Learning} with {Symbolic} {Knowledge}},
	shorttitle = {Semantic {Loss}},
	abstract = {This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that it effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.},
	language = {en},
	urldate = {2022-09-06},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Xu, Jingyi and Zhang, Zilu and Friedman, Tal and Liang, Yitao and Van den Broeck, Guy},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	keywords = {ucla},
	pages = {5502--5511},
}

@inproceedings{dierckx_parameter_2024,
	title = {Parameter {Learning} using {Approximate} {Model} {Counting}},
	url = {https://dial.uclouvain.be/pr/boreal/object/boreal:288827},
	language = {en},
	urldate = {2024-07-02},
	author = {Dierckx, Lucile and Dubray, Alexandre and Nijssen, Siegfried},
	year = {2024},
}

@misc{wang_imperative_2024,
	title = {Imperative {Learning}: {A} {Self}-supervised {Neural}-{Symbolic} {Learning} {Framework} for {Robot} {Autonomy}},
	shorttitle = {Imperative {Learning}},
	url = {http://arxiv.org/abs/2406.16087},
	abstract = {Data-driven methods such as reinforcement and imitation learning have achieved remarkable success in robot autonomy. However, their data-centric nature still hinders them from generalizing well to ever-changing environments. Moreover, collecting large datasets for robotic tasks is often impractical and expensive. To overcome these challenges, we introduce a new self-supervised neural-symbolic (NeSy) computational framework, imperative learning (IL), for robot autonomy, leveraging the generalization abilities of symbolic reasoning. The framework of IL consists of three primary components: a neural module, a reasoning engine, and a memory system. We formulate IL as a special bilevel optimization (BLO), which enables reciprocal learning over the three modules. This overcomes the labelintensive obstacles associated with data-driven approaches and takes advantage of symbolic reasoning concerning logical reasoning, physical principles, geometric analysis, etc. We discuss several optimization techniques for IL and verify their effectiveness in five distinct robot autonomy tasks including path planning, rule induction, optimal control, visual odometry, and multi-robot routing. Through various experiments, we show that IL can significantly enhance robot autonomy capabilities and we anticipate that it will catalyze further research across diverse domains.},
	language = {en},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Wang, Chen and Ji, Kaiyi and Geng, Junyi and Ren, Zhongqiang and Fu, Taimeng and Yang, Fan and Guo, Yifan and He, Haonan and Chen, Xiangyu and Zhan, Zitong and Du, Qiwei and Su, Shaoshu and Li, Bowen and Qiu, Yuheng and Du, Yi and Li, Qihang and Yang, Yifan and Lin, Xiao and Zhao, Zhipeng},
	month = jun,
	year = {2024},
	note = {arXiv:2406.16087 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{kapur_diffusion_2024,
	title = {Diffusion {On} {Syntax} {Trees} {For} {Program} {Synthesis}},
	url = {http://arxiv.org/abs/2405.20519},
	abstract = {Large language models generate code one token at a time. Their autoregressive generation process lacks the feedback of observing the program’s output. Training LLMs to suggest edits directly can be challenging due to the scarcity of rich edit data. To address these problems, we propose neural diffusion models that operate on syntax trees of any context-free grammar. Similar to image diffusion models, our method also inverts “noise” applied to syntax trees. Rather than generating code sequentially, we iteratively edit it while preserving syntactic validity, which makes it easy to combine this neural model with search. We apply our approach to inverse graphics tasks, where our model learns to convert images into programs that produce those images. Combined with search, our model is able to write graphics programs, see the execution result, and debug them to meet the required specifications. We additionally show how our system can write graphics programs for hand-drawn sketches. Video results can be found at https://tree-diffusion.github.io.},
	language = {en},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Kapur, Shreyas and Jenner, Erik and Russell, Stuart},
	month = may,
	year = {2024},
	note = {arXiv:2405.20519 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@techreport{lewkowycz_solving_2022,
	title = {Solving {Quantitative} {Reasoning} {Problems} with {Language} {Models}},
	shorttitle = {Minerva},
	url = {http://arxiv.org/abs/2206.14858},
	abstract = {Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.},
	number = {arXiv:2206.14858},
	urldate = {2022-09-23},
	institution = {arXiv},
	author = {Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and Wu, Yuhuai and Neyshabur, Behnam and Gur-Ari, Guy and Misra, Vedant},
	month = jun,
	year = {2022},
	note = {arXiv:2206.14858 [cs]
version: 2
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{van_krieken_independence_2024,
	title = {On the {Independence} {Assumption} in {Neurosymbolic} {Learning}},
	url = {http://arxiv.org/abs/2404.08458},
	abstract = {State-of-the-art neurosymbolic learning systems use probabilistic reasoning to guide neural networks towards predictions that conform to logical constraints over symbols. Many such systems assume that the probabilities of the considered symbols are conditionally independent given the input to simplify learning and reasoning. We study and criticise this assumption, highlighting how it can hinder optimisation and prevent uncertainty quantification. We prove that loss functions bias conditionally independent neural networks to become overconfident in their predictions. As a result, they are unable to represent uncertainty over multiple valid options. Furthermore, we prove that these loss functions are difficult to optimise: they are non-convex, and their minima are usually highly disconnected. Our theoretical analysis gives the foundation for replacing the conditional independence assumption and designing more expressive neurosymbolic probabilistic models.},
	language = {en},
	urldate = {2024-04-18},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {van Krieken, Emile and Minervini, Pasquale and Ponti, Edoardo M. and Vergari, Antonio},
	month = apr,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, edinburgh},
}

@inproceedings{shih_hyperspns_2021,
	title = {{HyperSPNs}: {Compact} and {Expressive} {Probabilistic} {Circuits}},
	volume = {34},
	shorttitle = {{HyperSPNs}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/481fbfa59da2581098e841b7afc122f1-Abstract.html},
	abstract = {Probabilistic circuits (PCs) are a family of generative models which allows for the computation of exact likelihoods and marginals of its probability distributions. PCs are both expressive and tractable, and serve as popular choices for discrete density estimation tasks. However, large PCs are susceptible to overfitting, and only a few regularization strategies (e.g., dropout, weight-decay) have been explored. We propose HyperSPNs: a new paradigm of generating the mixture weights of large PCs using a small-scale neural network. Our framework can be viewed as a soft weight-sharing strategy, which combines the greater expressiveness of large models with the better generalization and memory-footprint properties of small models.  We show the merits of our regularization strategy on two state-of-the-art PC families introduced in recent literature -- RAT-SPNs and EiNETs -- and demonstrate generalization improvements in both models on a suite of density estimation benchmarks in both discrete and continuous domains.},
	urldate = {2024-06-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Shih, Andy and Sadigh, Dorsa and Ermon, Stefano},
	year = {2021},
	pages = {8571--8582},
}

@inproceedings{poon_sum-product_2011,
	title = {Sum-product networks: {A} new deep architecture},
	shorttitle = {Sum-product networks},
	url = {https://ieeexplore.ieee.org/abstract/document/6130310?casa_token=4pUQ3zgSZsUAAAAA:OQImbK0fvv7b8a6aRm6lp1MIc_vZrM0EE3WxgmmVSrLFKvIJYdzzBrBdaEEJsdLaVVnyxz0uG3Lh},
	doi = {10.1109/ICCVW.2011.6130310},
	abstract = {The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are the most general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sum product networks (SPNs) and will present in this abstract. The key idea of SPNs is to compactly represent the partition function by introducing multiple layers of hidden variables. An SPN is a rooted directed acyclic graph with variables as leaves, sums and products as internal nodes, and weighted edges.},
	urldate = {2024-06-01},
	booktitle = {2011 {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCV} {Workshops})},
	author = {Poon, Hoifung and Domingos, Pedro},
	month = nov,
	year = {2011},
	keywords = {Backpropagation, Computational modeling, Computer architecture, Decision trees, Graphical models, Junctions},
	pages = {689--690},
}

@misc{yao_hardware-efficient_2024,
	title = {On {Hardware}-efficient {Inference} in {Probabilistic} {Circuits}},
	url = {http://arxiv.org/abs/2405.13639},
	abstract = {Probabilistic circuits (PCs) offer a promising avenue to perform embedded reasoning under uncertainty. They support efficient and exact computation of various probabilistic inference tasks by design. Hence, hardware-efficient computation of PCs is highly interesting for edge computing applications. As computations in PCs are based on arithmetic with probability values, they are typically performed in the log domain to avoid underflow. Unfortunately, performing the log operation on hardware is costly. Hence, prior work has focused on computations in the linear domain, resulting in high resolution and energy requirements. This work proposes the first dedicated approximate computing framework for PCs that allows for low-resolution logarithm computations. We leverage Addition As Int, resulting in linear PC computation with simple hardware elements. Further, we provide a theoretical approximation error analysis and present an error compensation mechanism. Empirically, our method obtains up to 357x and 649x energy reduction on custom hardware for evidence and MAP queries respectively with little or no computational error.},
	language = {en},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Yao, Lingyun and Trapp, Martin and Leslin, Jelin and Singh, Gaurav and Zhang, Peng and Periasamy, Karthekeyan and Andraud, Martin},
	month = may,
	year = {2024},
	note = {arXiv:2405.13639 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{weiss_extracting_2018,
	title = {Extracting {Automata} from {Recurrent} {Neural} {Networks} {Using} {Queries} and {Counterexamples}},
	url = {https://proceedings.mlr.press/v80/weiss18a.html},
	abstract = {We present a novel algorithm that uses exact learning and abstraction to extract a deterministic finite automaton describing the state dynamics of a given trained RNN. We do this using Angluin’s {\textbackslash}lstar algorithm as a learner and the trained RNN as an oracle. Our technique efficiently extracts accurate automata from trained RNNs, even when the state vectors are large and require fine differentiation.},
	language = {en},
	urldate = {2024-05-24},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {5247--5256},
}

@inproceedings{malkin_gflownets_2022,
	title = {{GFlowNets} and variational inference},
	url = {https://openreview.net/forum?id=uKiE0VIluA-},
	abstract = {This paper builds bridges between two families of probabilistic algorithms: (hierarchical) variational inference (VI), which is typically used to model distributions over continuous spaces, and generative flow networks (GFlowNets), which have been used for distributions over discrete structures such as graphs. We demonstrate that, in certain cases, VI algorithms are equivalent to special cases of GFlowNets in the sense of equality of expected gradients of their learning objectives. We then point out the differences between the two families and show how these differences emerge experimentally. Notably, GFlowNets, which borrow ideas from reinforcement learning, are more amenable than VI to off-policy training without the cost of high gradient variance induced by importance sampling. We argue that this property of GFlowNets can provide advantages for capturing diversity in multimodal target distributions. Code: https://github.com/GFNOrg/GFN\_vs\_HVI.},
	language = {en},
	urldate = {2024-05-20},
	author = {Malkin, Nikolay and Lahlou, Salem and Deleu, Tristan and Ji, Xu and Hu, Edward J. and Everett, Katie E. and Zhang, Dinghuai and Bengio, Yoshua},
	month = sep,
	year = {2022},
}

@article{tsamoura_probabilistic_2023,
	title = {Probabilistic {Reasoning} at {Scale}: {Trigger} {Graphs} to the {Rescue}},
	volume = {1},
	shorttitle = {Probabilistic {Reasoning} at {Scale}},
	url = {https://dl.acm.org/doi/10.1145/3588719},
	doi = {10.1145/3588719},
	abstract = {The role of uncertainty in data management has become more prominent than ever before, especially because of the growing importance of machine learning-driven applications that produce large uncertain databases. A well-known approach to querying such databases is to blend rule-based reasoning with uncertainty. However, techniques proposed so far struggle with large databases. In this paper, we address this problem by presenting a new technique for probabilistic reasoning that exploits Trigger Graphs (TGs) -- a notion recently introduced for the non-probabilistic setting. The intuition is that TGs can effectively store a probabilistic model by avoiding an explicit materialization of the lineage and by grouping together similar derivations of the same fact. Firstly, we show how TGs can be adapted to support the possible world semantics. Then, we describe techniques for efficiently computing a probabilistic model and formally establish the correctness of our approach. We also present an extensive empirical evaluation using a prototype called LTGs. Our comparison against other leading engines shows that LTGs is not only faster, even against approximate reasoning techniques, but can also reason over probabilistic databases that existing engines cannot scale to.},
	number = {1},
	urldate = {2024-04-29},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Tsamoura, Efthymia and Lee, Jaehun and Urbani, Jacopo},
	year = {2023},
	keywords = {datalog, possible world semantics, probabilistic rule-based reasoning, uncertain data management},
	pages = {39:1--39:27},
}

@misc{merrill_formal_2021,
	title = {Formal {Language} {Theory} {Meets} {Modern} {NLP}},
	url = {http://arxiv.org/abs/2102.10094},
	doi = {10.48550/arXiv.2102.10094},
	abstract = {NLP is deeply intertwined with the formal study of language, both conceptually and historically. Arguably, this connection goes all the way back to Chomsky's Syntactic Structures in 1957. It also still holds true today, with a strand of recent works building formal analysis of modern neural networks methods in terms of formal languages. In this document, I aim to explain background about formal languages as they relate to this recent work. I will by necessity ignore large parts of the rich history of this field, instead focusing on concepts connecting to modern deep learning-based NLP.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Merrill, William},
	month = jul,
	year = {2021},
	note = {arXiv:2102.10094 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{ackerman_survey_2020,
	title = {A {Survey} of {Neural} {Networks} and {Formal} {Languages}},
	url = {http://arxiv.org/abs/2006.01338},
	abstract = {This report is a survey of the relationships between various state-of-the-art neural network architectures and formal languages as, for example, structured by the Chomsky Language Hierarchy. Of particular interest are the abilities of a neural architecture to represent, recognize and generate words from a speciﬁc language by learning from samples of the language.},
	language = {en},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Ackerman, Joshua and Cybenko, George},
	month = jun,
	year = {2020},
	note = {arXiv:2006.01338 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{merrill_formal_2023,
	address = {Cham},
	title = {Formal {Languages} and the {NLP} {Black} {Box}},
	isbn = {978-3-031-33264-7},
	doi = {10.1007/978-3-031-33264-7_1},
	abstract = {The field of natural language processing (NLP) has been transformed in two related ways in recent years.},
	language = {en},
	booktitle = {Developments in {Language} {Theory}},
	publisher = {Springer Nature Switzerland},
	author = {Merrill, William},
	editor = {Drewes, Frank and Volkov, Mikhail},
	year = {2023},
	pages = {1--8},
}

@inproceedings{anil_exploring_2022,
	title = {Exploring {Length} {Generalization} in {Large} {Language} {Models}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/fb7451e43f9c1c35b774bcfad7a5714b-Abstract-Conference.html},
	language = {en},
	urldate = {2024-05-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
	month = dec,
	year = {2022},
	pages = {38546--38556},
}

@inproceedings{yao_self-attention_2021,
	address = {Online},
	title = {Self-{Attention} {Networks} {Can} {Process} {Bounded} {Hierarchical} {Languages}},
	url = {https://aclanthology.org/2021.acl-long.292},
	doi = {10.18653/v1/2021.acl-long.292},
	abstract = {Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as Dyck-k, the language consisting of well-nested parentheses of k types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with D+1 layers and O(log k) memory size (per token per layer) that recognizes Dyck-(k, D), and a soft-attention network with two layers and O(log k) memory size that generates Dyck-(k, D). Experiments show that self-attention networks trained on Dyck-(k, D) generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks.},
	urldate = {2024-05-14},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yao, Shunyu and Peng, Binghui and Papadimitriou, Christos and Narasimhan, Karthik},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {3770--3785},
}

@inproceedings{ebrahimi_how_2020,
	address = {Online},
	title = {How {Can} {Self}-{Attention} {Networks} {Recognize} {Dyck}-n {Languages}?},
	url = {https://aclanthology.org/2020.findings-emnlp.384},
	doi = {10.18653/v1/2020.findings-emnlp.384},
	abstract = {We focus on the recognition of Dyck-n (Dn) languages with self-attention (SA) networks, which has been deemed to be a difficult task for these networks. We compare the performance of two variants of SA, one with a starting symbol (SA+) and one without (SA-). Our results show that SA+ is able to generalize to longer sequences and deeper dependencies. For D2, we find that SA- completely breaks down on long sequences whereas the accuracy of SA+ is 58.82\%. We find attention maps learned by SA+ to be amenable to interpretation and compatible with a stack-based language recognizer. Surprisingly, the performance of SA networks is at par with LSTMs, which provides evidence on the ability of SA to learn hierarchies without recursion.},
	urldate = {2024-05-14},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Ebrahimi, Javid and Gelda, Dhruv and Zhang, Wei},
	editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {4301--4306},
}

@inproceedings{wen_transformers_2023,
	title = {Transformers are uninterpretable with myopic methods: a case study with bounded {Dyck} grammars},
	volume = {36},
	shorttitle = {Transformers are uninterpretable with myopic methods},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/79ba1b827d3fc58e129d1cbfc8ff69f2-Abstract-Conference.html},
	language = {en},
	urldate = {2024-05-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wen, Kaiyue and Li, Yuchen and Liu, Bingbin and Risteski, Andrej},
	month = dec,
	year = {2023},
	pages = {38723--38766},
}

@article{hahn_theoretical_2020,
	title = {Theoretical {Limitations} of {Self}-{Attention} in {Neural} {Sequence} {Models}},
	volume = {8},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00306},
	doi = {10.1162/tacl_a_00306},
	abstract = {Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.},
	urldate = {2024-05-14},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Hahn, Michael},
	month = jan,
	year = {2020},
	pages = {156--171},
}

@inproceedings{chiang_overcoming_2022,
	address = {Dublin, Ireland},
	title = {Overcoming a {Theoretical} {Limitation} of {Self}-{Attention}},
	url = {https://aclanthology.org/2022.acl-long.527},
	doi = {10.18653/v1/2022.acl-long.527},
	abstract = {Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer's classification decisions get closer and closer to random guessing (that is, a cross-entropy of 1) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation implied by Hahn's lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation.},
	urldate = {2024-05-14},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chiang, David and Cholak, Peter},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {7654--7664},
}

@inproceedings{liu_transformers_2022,
	title = {Transformers {Learn} {Shortcuts} to {Automata}},
	url = {https://openreview.net/forum?id=De4FYqjFueZ},
	abstract = {Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are these shallow and non-recurrent models finding? We investigate this question in the setting of learning automata, discrete dynamical systems naturally suited to recurrent modeling and expressing algorithmic tasks. Our theoretical results completely characterize shortcut solutions, whereby a shallow Transformer with only \$o(T)\$ layers can exactly replicate the computation of an automaton on an input sequence of length \$T\$. By representing automata using the algebraic structure of their underlying transformation semigroups, we obtain \$O({\textbackslash}log T)\$-depth simulators for all automata and \$O(1)\$-depth simulators for all automata whose associated groups are solvable. Empirically, we perform synthetic experiments by training Transformers to simulate a wide variety of automata, and show that shortcut solutions can be learned via standard training. We further investigate the brittleness of these solutions and propose potential mitigations.},
	language = {en},
	urldate = {2024-05-14},
	author = {Liu, Bingbin and Ash, Jordan T. and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
	month = sep,
	year = {2022},
}

@misc{strobl_what_2024,
	title = {What {Formal} {Languages} {Can} {Transformers} {Express}? {A} {Survey}},
	shorttitle = {What {Formal} {Languages} {Can} {Transformers} {Express}?},
	url = {http://arxiv.org/abs/2311.00208},
	doi = {10.48550/arXiv.2311.00208},
	abstract = {As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring such questions can help clarify the power of transformers relative to other models of computation, their fundamental capabilities and limits, and the impact of architectural choices. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Strobl, Lena and Merrill, William and Weiss, Gail and Chiang, David and Angluin, Dana},
	month = may,
	year = {2024},
	note = {arXiv:2311.00208 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Formal Languages and Automata Theory, Computer Science - Logic in Computer Science, Computer Science - Machine Learning},
}

@inproceedings{giles_extracting_1991,
	title = {Extracting and {Learning} an {Unknown} {Grammar} with {Recurrent} {Neural} {Networks}},
	volume = {4},
	url = {https://proceedings.neurips.cc/paper/1991/hash/15de21c670ae7c3f6f3f1f37029303c9-Abstract.html},
	abstract = {Simple secood-order recurrent netwoIts are shown to readily learn sman brown  regular grammars when trained with positive and negative strings examples. We  show that similar methods are appropriate for learning unknown grammars from  examples of their strings. TIle training algorithm is an incremental real-time, re(cid:173) current learning (RTRL) method that computes the complete gradient and updates  the weights at the end of each string. After or during training. a dynamic clustering  algorithm extracts the production rules that the neural network has learned.. TIle  methods are illustrated by extracting rules from unknown deterministic regular  grammars. For many cases the extracted grammar outperforms the neural net from  which it was extracted in correctly classifying unseen strings.},
	urldate = {2024-05-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {Giles, C. L. and Miller, C. B. and Chen, D. and Sun, G. Z. and Chen, H. H. and Lee, Y. C.},
	year = {1991},
}

@inproceedings{bhattamishra_ability_2020,
	address = {Online},
	title = {On the {Ability} and {Limitations} of {Transformers} to {Recognize} {Formal} {Languages}},
	url = {https://aclanthology.org/2020.emnlp-main.576},
	doi = {10.18653/v1/2020.emnlp-main.576},
	abstract = {Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.},
	urldate = {2024-05-14},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {7096--7116},
}

@article{avellaneda_short_2020,
	title = {A short description of the solver {EvalMaxSAT}},
	volume = {8},
	url = {https://core.ac.uk/download/pdf/328855883.pdf#page=8},
	urldate = {2024-05-14},
	journal = {MaxSAT Evaluation},
	author = {Avellaneda, Florent},
	year = {2020},
}

@article{ariyani_can_nodate,
	title = {Can {Language} {Models} {Learn} {Embeddings} of {Propositional} {Logic} {Assertions}?},
	abstract = {Natural language offers an appealing alternative to formal logics as a vehicle for representing knowledge. However, using natural language means that standard methods for automated reasoning can no longer be used. A popular solution is to use transformer-based language models (LMs) to directly reason about knowledge expressed in natural language, but this has two important limitations. First, the set of premises is often too large to be directly processed by the LM. This means that we need a retrieval strategy which can select the most relevant premises when trying to infer some conclusion. Second, LMs have been found to learn shortcuts and thus lack robustness, putting in doubt to what extent they actually understand the knowledge that is expressed. Given these limitations, we explore the following alternative: rather than using LMs to perform reasoning directly, we use them to learn embeddings of individual assertions. Reasoning is then carried out by manipulating the learned embeddings. We show that this strategy is feasible to some extent while at the same time also highlighting the limitations of directly fine-tuning LMs to learn the required embeddings.},
	language = {en},
	author = {Ariyani, Nurul Fajrin and Bouraoui, Zied and Booth, Richard and Schockaert, Steven},
}

@misc{agarwal_probabilistic_2024,
	title = {Probabilistic {Generating} {Circuits} -- {Demystified}},
	url = {http://arxiv.org/abs/2404.02912},
	abstract = {Zhang et al. (ICML 2021, PLMR 139, pp. 12447–1245) introduced probabilistic generating circuits (PGCs) as a probabilistic model to unify probabilistic circuits (PCs) and determinantal point processes (DPPs). At a ﬁrst glance, PGCs store a distribution in a very different way, they compute the probability generating polynomial instead of the probability mass function and it seems that this is the main reason why PGCs are more powerful than PCs or DPPs. However, PGCs also allow for negative weights, whereas classical PCs assume that all weights are nonnegative. One of the main insights of our paper is that the negative weights are responsible for the power of PGCs and not the different representation. PGCs are PCs in disguise, in particular, we show how to transform any PGC into a PC with negative weights with only polynomial blowup.},
	language = {en},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Agarwal, Sanyam and Bläser, Markus},
	month = mar,
	year = {2024},
	note = {arXiv:2404.02912 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Complexity},
}

@inproceedings{dangel_backpack_2019,
	title = {{BackPACK}: {Packing} more into {Backprop}},
	shorttitle = {{BackPACK}},
	url = {https://openreview.net/forum?id=BJlrF24twB},
	abstract = {Automatic differentiation frameworks are optimized for exactly one thing: computing the average mini-batch gradient. Yet, other quantities such as the variance of the mini-batch gradients or many approximations to the Hessian can, in theory, be computed efficiently, and at the same time as the gradient. While these quantities are of great interest to researchers and practitioners, current deep learning software does not support their automatic calculation. Manually implementing them is burdensome, inefficient if done naively, and the resulting code is rarely shared. This hampers progress in deep learning, and unnecessarily narrows research to focus on gradient descent and its variants; it also complicates replication studies and comparisons between newly developed methods that require those quantities, to the point of impossibility. To address this problem, we introduce BackPACK, an efficient framework built on top of PyTorch, that extends the backpropagation algorithm to extract additional information from first-and second-order derivatives. Its capabilities are illustrated by benchmark reports for computing additional quantities on deep neural networks, and an example application by testing several recent curvature approximations for optimization.},
	language = {en},
	urldate = {2024-02-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Dangel, Felix and Kunstner, Frederik and Hennig, Philipp},
	month = sep,
	year = {2019},
}

@inproceedings{das_go_2023,
	title = {Go for a {Walk} and {Arrive} at the {Answer}: {Reasoning} {Over} {Paths} in {Knowledge} {Bases} using {Reinforcement} {Learning}},
	shorttitle = {{MINERVA}},
	url = {https://openreview.net/forum?id=Syg-YfWCW},
	abstract = {Knowledge bases (KB), both automatically and manually constructed, are often incomplete --- many valid facts can be inferred from the KB by synthesizing existing information. A popular approach to KB completion is to infer new relations by combinatory reasoning over the information found along other paths connecting a pair of entities. Given the enormous size of KBs and the exponential number of paths, previous path-based models have considered only the problem of predicting a missing relation given two entities, or evaluating the truth of a proposed triple. Additionally, these methods have traditionally used random paths between fixed entity pairs or more recently learned to pick paths between them. We propose a new algorithm, MINERVA, which addresses the much more difficult and practical task of answering questions where the relation is known, but only one entity. Since random walks are impractical in a setting with unknown destination and combinatorially many paths from a start node, we present a neural reinforcement learning approach which learns how to navigate the graph conditioned on the input query to find predictive paths. On a comprehensive evaluation on seven knowledge base datasets, we found MINERVA to be competitive with many current state-of-the-art methods.},
	language = {en},
	urldate = {2023-05-17},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Das, Rajarshi and Dhuliawala, Shehzaad and Zaheer, Manzil and Vilnis, Luke and Durugkar, Ishan and Krishnamurthy, Akshay and Smola, Alex and McCallum, Andrew},
	month = may,
	year = {2023},
}

@inproceedings{frankle_lottery_2019,
	title = {The {Lottery} {Ticket} {Hypothesis}: {Finding} {Sparse}, {Trainable} {Neural} {Networks}},
	shorttitle = {The {Lottery} {Ticket} {Hypothesis}},
	url = {https://openreview.net/forum?id=rJl-b3RcF7},
	abstract = {Feedforward neural networks that can have weights pruned after training could have had the same weights pruned before training},
	language = {en},
	urldate = {2021-05-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Frankle, Jonathan and Carbin, Michael},
	year = {2019},
}

@inproceedings{cornelio_learning_2022,
	title = {Learning where and when to reason in neuro-symbolic inference},
	url = {https://openreview.net/forum?id=en9V5F8PR-},
	abstract = {The integration of hard constraints on neural network outputs is a very desirable capability. This allows to instill trust in AI by guaranteeing the sanity of that neural network predictions with respect to domain knowledge. Recently, this topic has received a lot of attention. However, all the existing methods usually either impose the constraints in a "weak" form at training time, with no guarantees at inference, or fail to provide a general framework that supports different tasks and constraint types. We tackle this open problem from a neuro-symbolic perspective. Our pipeline enhances a conventional neural predictor with (1) a symbolic reasoning module capable of correcting structured prediction errors and (2) a neural attention module that learns to direct the reasoning effort to focus on potential prediction errors, while keeping other outputs unchanged. This framework provides an appealing trade-off between the efficiency of constraint-free neural inference and the prohibitive cost of exhaustive reasoning at inference time. We show that our method outperforms the state of the art on visual-Sudoku, and can also benefit visual scene graph prediction. Furthermore, it can improve the performance of existing neuro-symbolic systems that lack our explicit reasoning during inference.},
	language = {en},
	urldate = {2023-07-26},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Cornelio, Cristina and Stuehmer, Jan and Hu, Shell Xu and Hospedales, Timothy},
	month = sep,
	year = {2022},
	keywords = {todo},
}

@inproceedings{ghosh_variational_2019,
	title = {From {Variational} to {Deterministic} {Autoencoders}},
	url = {https://openreview.net/forum?id=S1g7tpEYDS},
	abstract = {Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules.},
	language = {en},
	urldate = {2023-11-08},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ghosh, Partha and Sajjadi, Mehdi S. M. and Vergari, Antonio and Black, Michael and Scholkopf, Bernhard},
	month = sep,
	year = {2019},
}

@inproceedings{oldenhof_weakly_2022,
	title = {Weakly {Supervised} {Knowledge} {Transfer} with {Probabilistic} {Logical} {Reasoning} for {Object} {Detection}},
	url = {https://openreview.net/forum?id=4yqxDCbzS98},
	abstract = {Training object detection models usually requires instance-level annotations, such as the positions and labels of all objects present in each image. Such supervision is unfortunately not always available and, more often, only image-level information is provided, also known as weak supervision. Recent works have addressed this limitation by leveraging knowledge from a richly annotated domain. However, the scope of weak supervision supported by these approaches has been very restrictive, preventing them to use all available information. In this work, we propose ProbKT, a framework based on probabilistic logical reasoning to train object detection models with arbitrary types of weak supervision. We empirically show on different datasets that using all available information is beneficial as our ProbKT leads to significant improvement on target domain and better generalisation compared to existing baselines. We also showcase the ability of our approach to handle complex logic statements as supervision signal.},
	language = {en},
	urldate = {2023-10-03},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Oldenhof, Martijn and Arany, Adam and Moreau, Yves and Brouwer, Edward De},
	month = sep,
	year = {2022},
	keywords = {leuven},
}

@inproceedings{pogancic_differentiation_2019,
	title = {Differentiation of {Blackbox} {Combinatorial} {Solvers}},
	url = {https://openreview.net/forum?id=BkevoJSYPB},
	abstract = {Achieving fusion of deep learning with combinatorial algorithms promises transformative changes to artificial intelligence. One possible approach is to introduce combinatorial building blocks into neural networks. Such end-to-end architectures have the potential to tackle combinatorial problems on raw input data such as ensuring global consistency in multi-object tracking or route planning on maps in robotics. In this work, we present a method that implements an efficient backward pass through blackbox implementations of combinatorial solvers with linear objective functions. We provide both theoretical and experimental backing. In particular, we incorporate the Gurobi MIP solver, Blossom V algorithm, and Dijkstra's algorithm into architectures that extract suitable features from raw inputs for the traveling salesman problem, the min-cost perfect matching problem and the shortest path problem.},
	language = {en},
	urldate = {2023-06-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Pogančić, Marin Vlastelica and Paulus, Anselm and Musil, Vit and Martius, Georg and Rolinek, Michal},
	month = dec,
	year = {2019},
	keywords = {todo},
}

@inproceedings{xu_what_2019,
	title = {What {Can} {Neural} {Networks} {Reason} {About}?},
	url = {https://openreview.net/forum?id=rJxbJeHFPS},
	abstract = {Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, e.g., Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. We formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations. As an example, we unify seemingly different reasoning tasks, such as intuitive physics, visual question answering, and shortest paths, via the lens of a powerful algorithmic paradigm, dynamic programming (DP). We show that GNNs align with DP and thus are expected to solve these tasks. On several reasoning tasks, our theory is supported by empirical results.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Xu, Keyulu and Li, Jingling and Zhang, Mozhi and Du, Simon S. and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
	month = dec,
	year = {2019},
	keywords = {todo},
}

@inproceedings{maddison_concrete_2016,
	title = {The {Concrete} {Distribution}: {A} {Continuous} {Relaxation} of {Discrete} {Random} {Variables}},
	shorttitle = {Concrete {Distribution}},
	url = {https://openreview.net/forum?id=S1jE5L5gl},
	abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
	language = {en},
	urldate = {2024-01-08},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
	month = nov,
	year = {2016},
}

@inproceedings{li_softened_2022,
	title = {Softened {Symbol} {Grounding} for {Neuro}-symbolic {Systems}},
	url = {https://openreview.net/forum?id=HTJE5Krui0g},
	abstract = {Neuro-symbolic learning generally consists of two separated worlds, i.e., neural network training and symbolic constraint solving, whose success hinges on symbol grounding, a fundamental problem in AI. This paper presents a novel, softened symbol grounding process, bridging the gap between the two worlds, and resulting in an effective and efficient neuro-symbolic learning framework. Technically, the framework features (1) modeling of symbol solution states as a Boltzmann distribution, which avoids expensive state searching and facilitates mutually beneficial interactions between network training and symbolic reasoning; (2) a new MCMC technique leveraging projection and SMT solvers, which efficiently samples from disconnected symbol solution spaces; (3) an annealing mechanism that can escape from sub-optimal symbol groundings. Experiments with three representative neuro-symbolic learning tasks demonstrate that, owing to its superior symbol grounding capability, our framework successfully solves problems well beyond the frontier of the existing proposals.},
	language = {en},
	urldate = {2023-11-28},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Li, Zenan and Yao, Yuan and Chen, Taolue and Xu, Jingwei and Cao, Chun and Ma, Xiaoxing and Lü, Jian},
	month = sep,
	year = {2022},
	keywords = {todo},
}

@book{koller_probabilistic_2009,
	title = {Probabilistic {Graphical} {Models}: {Principles} and {Techniques}},
	isbn = {978-0-262-01319-2},
	shorttitle = {Probabilistic {Graphical} {Models}},
	abstract = {A general framework for constructing and using probabilistic models of complex systems that would enable a computer to use available information for making decisions.Most tasks require a person or an automated system to reason—to reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs.},
	language = {en},
	publisher = {MIT Press},
	author = {Koller, Daphne and Friedman, Nir},
	month = jul,
	year = {2009},
	keywords = {Computers / Artificial Intelligence / General},
}

@misc{bengio_estimating_2013,
	title = {Estimating or {Propagating} {Gradients} {Through} {Stochastic} {Neurons} for {Conditional} {Computation}},
	shorttitle = {Straight-{Trough}},
	url = {http://arxiv.org/abs/1308.3432},
	doi = {10.48550/arXiv.1308.3432},
	abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{{\textbackslash}em conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
	month = aug,
	year = {2013},
	keywords = {Computer Science - Machine Learning},
}

@article{badreddine_logic_2022,
	title = {Logic {Tensor} {Networks}},
	volume = {303},
	issn = {00043702},
	shorttitle = {{LTN}},
	url = {http://arxiv.org/abs/2012.13635},
	doi = {10.1016/j.artint.2021.103649},
	abstract = {Attempts at combining logic and neural networks into neurosymbolic approaches have been on the increase in recent years. In a neurosymbolic system, symbolic knowledge assists deep learning, which typically uses a sub-symbolic distributed representation, to learn and reason at a higher level of abstraction. We present Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully diﬀerentiable logical language, called Real Logic, whereby the elements of a ﬁrst-order logic signature are grounded onto data using neural computational graphs and ﬁrst-order fuzzy logic semantics. We show that LTN provides a uniform language to represent and compute eﬃciently many of the most important AI tasks such as multi-label classiﬁcation, relational learning, data clustering, semi-supervised learning, regression, embedding learning and query answering. We implement and illustrate each of the above tasks with several simple explanatory examples using TensorFlow 2. The results indicate that LTN can be a general and powerful framework for neurosymbolic AI.},
	language = {en},
	urldate = {2023-06-13},
	journal = {Artificial Intelligence},
	author = {Badreddine, Samy and Garcez, Artur d'Avila and Serafini, Luciano and Spranger, Michael},
	month = feb,
	year = {2022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.4, I.2.6},
	pages = {103649},
}

@misc{van_krieken_uller_2024,
	title = {{ULLER}: {A} {Unified} {Language} for {Learning} and {Reasoning}},
	shorttitle = {{ULLER}},
	url = {http://arxiv.org/abs/2405.00532},
	abstract = {The field of neuro-symbolic artificial intelligence (NeSy), which combines learning and reasoning, has recently experienced significant growth. There now are a wide variety of NeSy frameworks, each with its own specific language for expressing background knowledge and how to relate it to neural networks. This heterogeneity hinders accessibility for newcomers and makes comparing different NeSy frameworks challenging. We propose a unified language for NeSy, which we call ULLER, a Unified Language for LEarning and Reasoning. ULLER encompasses a wide variety of settings, while ensuring that knowledge described in it can be used in existing NeSy systems. ULLER has a neuro-symbolic first-order syntax for which we provide example semantics including classical, fuzzy, and probabilistic logics. We believe ULLER is a first step towards making NeSy research more accessible and comparable, paving the way for libraries that streamline training and evaluation across a multitude of semantics, knowledge bases, and NeSy systems.},
	language = {en},
	urldate = {2024-05-05},
	publisher = {arXiv},
	author = {van Krieken, Emile and Badreddine, Samy and Manhaeve, Robin and Giunchiglia, Eleonora},
	month = may,
	year = {2024},
	note = {arXiv:2405.00532 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{arya_neural_2024,
	title = {Neural {Network} {Approximators} for {Marginal} {MAP} in {Probabilistic} {Circuits}},
	url = {http://arxiv.org/abs/2402.03621},
	abstract = {Probabilistic circuits (PCs) such as sum-product networks efficiently represent large multi-variate probability distributions. They are preferred in practice over other probabilistic representations, such as Bayesian and Markov networks, because PCs can solve marginal inference (MAR) tasks in time that scales linearly in the size of the network. Unfortunately, the most probable explanation (MPE) task and its generalization, the marginal maximum-a-posteriori (MMAP) inference task remain NP-hard in these models. Inspired by the recent work on using neural networks for generating near-optimal solutions to optimization problems such as integer linear programming, we propose an approach that uses neural networks to approximate MMAP inference in PCs. The key idea in our approach is to approximate the cost of an assignment to the query variables using a continuous multilinear function and then use the latter as a loss function. The two main benefits of our new method are that it is self-supervised, and after the neural network is learned, it requires only linear time to output a solution. We evaluate our new approach on several benchmark datasets and show that it outperforms three competing linear time approximations: max-product inference, max-marginal inference, and sequential estimation, which are used in practice to solve MMAP tasks in PCs.},
	language = {en},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Arya, Shivvrat and Rahman, Tahrima and Gogate, Vibhav},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03621 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{goral_model_2024,
	title = {Model {Counting} and {Sampling} via {Semiring} {Extensions}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/30022},
	doi = {10.1609/aaai.v38i18.30022},
	abstract = {Many decision and optimization problems have natural extensions as counting problems. The best known example is the Boolean satisfiability problem (SAT), where we want to count the satisfying assignments of truth values to the variables, which is known as the \#SAT problem. Likewise, for discrete optimization problems, we want to count the states on which the objective function attains the optimal value. Both SAT and discrete optimization can be formulated as selective marginalize a product function (MPF) queries. Here, we show how general selective MPF queries can be extended for model counting. MPF queries are encoded as tensor hypernetworks over suitable semirings that can be solved by generic tensor hypernetwork contraction algorithms. Our model counting extension is again an MPF query, on an extended semiring, that can be solved by the same contraction algorithms. Model counting is required for uniform model sampling. We show how the counting extension can be further extended for model sampling by constructing yet another semiring. We have implemented the model counting and sampling extensions. Experiments show that our generic approach is competitive with the state of the art in model counting and model sampling.},
	language = {en},
	number = {18},
	urldate = {2024-04-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Goral, Andreas and Giesen, Joachim and Blacher, Mark and Staudt, Christoph and Klaus, Julien},
	month = mar,
	year = {2024},
	note = {Number: 18},
	keywords = {SO: Combinatorial Optimization},
	pages = {20395--20403},
}

@misc{noauthor_model_nodate,
	title = {Model {Counting} and {Sampling} via {Semiring} {Extensions} {\textbar} {Proceedings} of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/30022},
	urldate = {2024-04-29},
}

@misc{padalkar_nesyfold_2023,
	title = {{NeSyFOLD}: {Neurosymbolic} {Framework} for {Interpretable} {Image} {Classification}},
	shorttitle = {{NeSyFOLD}},
	url = {http://arxiv.org/abs/2301.12667},
	doi = {10.48550/arXiv.2301.12667},
	abstract = {Deep learning models such as CNNs have surpassed human performance in computer vision tasks such as image classification. However, despite their sophistication, these models lack interpretability which can lead to biased outcomes reflecting existing prejudices in the data. We aim to make predictions made by a CNN interpretable. Hence, we present a novel framework called NeSyFOLD to create a neurosymbolic (NeSy) model for image classification tasks. The model is a CNN with all layers following the last convolutional layer replaced by a stratified answer set program (ASP). A rule-based machine learning algorithm called FOLD-SE-M is used to derive the stratified answer set program from binarized filter activations of the last convolutional layer. The answer set program can be viewed as a rule-set, wherein the truth value of each predicate depends on the activation of the corresponding kernel in the CNN. The rule-set serves as a global explanation for the model and is interpretable. A justification for the predictions made by the NeSy model can be obtained using an ASP interpreter. We also use our NeSyFOLD framework with a CNN that is trained using a sparse kernel learning technique called Elite BackProp (EBP). This leads to a significant reduction in rule-set size without compromising accuracy or fidelity thus improving scalability of the NeSy model and interpretability of its rule-set. Evaluation is done on datasets with varied complexity and sizes. To make the rule-set more intuitive to understand, we propose a novel algorithm for labelling each kernel's corresponding predicate in the rule-set with the semantic concept(s) it learns. We evaluate the performance of our "semantic labelling algorithm" to quantify the efficacy of the semantic labelling for both the NeSy model and the NeSy-EBP model.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Padalkar, Parth and Wang, Huaduo and Gupta, Gopal},
	month = aug,
	year = {2023},
	note = {arXiv:2301.12667 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{feldstein_parallel_2023,
	title = {Parallel {Neurosymbolic} {Integration} with {Concordia}},
	url = {http://arxiv.org/abs/2306.00480},
	abstract = {Parallel neurosymbolic architectures have been applied effectively in NLP by distilling knowledge from a logic theory into a deep model. However, prior art faces several limitations including supporting restricted forms of logic theories and relying on the assumption of independence between the logic and the deep network. We present Concordia, a framework overcoming the limitations of prior art. Concordia is agnostic both to the deep network and the logic theory offering support for a wide range of probabilistic theories. Our framework can support supervised training of both components and unsupervised training of the neural component. Concordia has been successfully applied to tasks beyond NLP and data classification, improving the accuracy of stateof-the-art on collective activity detection, entity linking and recommendation tasks.},
	language = {en},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Feldstein, Jonathan and Jurčius, Modestas and Tsamoura, Efthymia},
	month = jun,
	year = {2023},
	note = {arXiv:2306.00480 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, todo},
}

@inproceedings{tsamoura_beyond_2020,
	title = {Beyond the {Grounding} {Bottleneck}: {Datalog} {Techniques} for {Inference} in {Probabilistic} {Logic} {Programs}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	shorttitle = {Beyond the {Grounding} {Bottleneck}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6591},
	doi = {10.1609/aaai.v34i06.6591},
	abstract = {State-of-the-art inference approaches in probabilistic logic programming typically start by computing the relevant ground program with respect to the queries of interest, and then use this program for probabilistic inference using knowledge compilation and weighted model counting. We propose an alternative approach that uses efficient Datalog techniques to integrate knowledge compilation with forward reasoning with a non-ground program. This effectively eliminates the grounding bottleneck that so far has prohibited the application of probabilistic logic programming in query answering scenarios over knowledge graphs, while also providing fast approximations on classical benchmarks in the field.},
	language = {en},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Tsamoura, Efthymia and Gutierrez-Basulto, Victor and Kimmig, Angelika},
	month = apr,
	year = {2020},
	note = {Number: 06},
	pages = {10284--10291},
}

@misc{jain_neural_2023,
	title = {Neural {Priority} {Queues} for {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2307.09660},
	abstract = {Graph Neural Networks (GNNs) have shown considerable success in neural algorithmic reasoning. Many traditional algorithms make use of an explicit memory in the form of a data structure. However, there has been limited exploration on augmenting GNNs with external memory. In this paper, we present Neural Priority Queues, a differentiable analogue to algorithmic priority queues, for GNNs. We propose and motivate a desiderata for memory modules, and show that Neural PQs exhibit the desiderata, and reason about their use with algorithmic reasoning. This is further demonstrated by empirical results on the CLRS-30 dataset. Furthermore, we find the Neural PQs useful in capturing long-range interactions, as empirically shown on a dataset from the Long-Range Graph Benchmark.},
	language = {en},
	urldate = {2023-07-26},
	publisher = {arXiv},
	author = {Jain, Rishabh and Veličković, Petar and Liò, Pietro},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09660 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{ledaguenel_complexity_2024,
	title = {Complexity of {Probabilistic} {Reasoning} for {Neurosymbolic} {Classification} {Techniques}},
	url = {http://arxiv.org/abs/2404.08404},
	abstract = {Neurosymbolic artificial intelligence is a growing field of research aiming to combine neural network learning capabilities with the reasoning abilities of symbolic systems. Informed multi-label classification is a sub-field of neurosymbolic AI which studies how to leverage prior knowledge to improve neural classification systems. A well known family of neurosymbolic techniques for informed classification use probabilistic reasoning to integrate this knowledge during learning, inference or both. Therefore, the asymptotic complexity of probabilistic reasoning is of cardinal importance to assess the scalability of such techniques. However, this topic is rarely tackled in the neurosymbolic literature, which can lead to a poor understanding of the limits of probabilistic neurosymbolic techniques. In this paper, we introduce a formalism for informed supervised classification tasks and techniques. We then build upon this formalism to define three abstract neurosymbolic techniques based on probabilistic reasoning. Finally, we show computational complexity results on several representation languages for prior knowledge commonly found in the neurosymbolic literature.},
	language = {en},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Ledaguenel, Arthur and Hudelot, Céline and Khouadjia, Mostepha},
	month = apr,
	year = {2024},
	note = {arXiv:2404.08404 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, Computer Science - Machine Learning, Computer Science - Symbolic Computation, todo},
}

@misc{pryor_using_2024,
	title = {Using {Domain} {Knowledge} to {Guide} {Dialog} {Structure} {Induction} via {Neural} {Probabilistic} {Soft} {Logic}},
	url = {http://arxiv.org/abs/2403.17853},
	abstract = {Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance. Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.},
	language = {en},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Pryor, Connor and Yuan, Quan and Liu, Jeremiah and Kazemi, Mehran and Ramachandran, Deepak and Bedrax-Weiss, Tania and Getoor, Lise},
	month = mar,
	year = {2024},
	note = {arXiv:2403.17853 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{gradel_semiring_2017,
	title = {Semiring {Provenance} for {First}-{Order} {Model} {Checking}},
	url = {http://arxiv.org/abs/1712.01980},
	abstract = {Given a ﬁrst-order sentence, a model-checking computation tests whether the sentence holds true in a given ﬁnite structure. Data provenance extracts from this computation an abstraction of the manner in which its result depends on the data items that describe the model. Previous work on provenance was, to a large extent, restricted to the negation-free fragment of ﬁrst-order logic and showed how provenance abstractions can be usefully described as elements of commutative semirings — most generally as multivariate polynomials with positive integer coefﬁcients.},
	language = {en},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Grädel, Erich and Tannen, Val},
	month = dec,
	year = {2017},
	note = {arXiv:1712.01980 [cs]},
	keywords = {Computer Science - Logic in Computer Science, todo},
}

@inproceedings{du_generalizing_2023,
	address = {Toronto, Canada},
	title = {Generalizing {Backpropagation} for {Gradient}-{Based} {Interpretability}},
	url = {https://aclanthology.org/2023.acl-long.669},
	doi = {10.18653/v1/2023.acl-long.669},
	abstract = {Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs. While these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject–verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its importance to a prediction and (b) for SVA, identify which pathways of the self-attention mechanism are most important.},
	urldate = {2024-04-05},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Du, Kevin and Torroba Hennigen, Lucas and Stoehr, Niklas and Warstadt, Alex and Cotterell, Ryan},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {11979--11995},
}

@inproceedings{li_first-_2009,
	address = {USA},
	series = {{EMNLP} '09},
	title = {First- and second-order expectation semirings with applications to minimum-risk training on translation forests},
	isbn = {978-1-932432-59-6},
	abstract = {Many statistical translation models can be regarded as weighted logical deduction. Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs). We then introduce a novel second-order expectation semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy). This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk. We use these semirings in an open-source machine translation toolkit, Joshua, enabling minimum-risk training for a benefit of up to 1.0 bleu point.},
	urldate = {2024-04-14},
	booktitle = {Proceedings of the 2009 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {Volume} 1 - {Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Li, Zhifei and Eisner, Jason},
	month = aug,
	year = {2009},
	pages = {40--51},
}

@article{kimmig_algebraic_2017,
	series = {{SI}:{Uncertain} {Reasoning}},
	title = {Algebraic model counting},
	volume = {22},
	issn = {1570-8683},
	shorttitle = {{AMC}},
	url = {https://www.sciencedirect.com/science/article/pii/S157086831630088X},
	doi = {10.1016/j.jal.2016.11.031},
	abstract = {Weighted model counting (WMC) is a well-known inference task on knowledge bases, and the basis for some of the most efficient techniques for probabilistic inference in graphical models. We introduce algebraic model counting (AMC), a generalization of WMC to a semiring structure that provides a unified view on a range of tasks and existing results. We show that AMC generalizes many well-known tasks in a variety of domains such as probabilistic inference, soft constraints and network and database analysis. Furthermore, we investigate AMC from a knowledge compilation perspective and show that all AMC tasks can be evaluated using sd-DNNF circuits, which are strictly more succinct, and thus more efficient to evaluate, than direct representations of sets of models. We identify further characteristics of AMC instances that allow for evaluation on even more succinct circuits.},
	language = {en},
	urldate = {2023-05-16},
	journal = {Journal of Applied Logic},
	author = {Kimmig, Angelika and Van den Broeck, Guy and De Raedt, Luc},
	month = jul,
	year = {2017},
	keywords = {Knowledge compilation, Logic, Model counting, leuven},
	pages = {46--62},
}

@article{karp_monte-carlo_1989,
	title = {Monte-{Carlo} approximation algorithms for enumeration problems},
	volume = {10},
	issn = {0196-6774},
	shorttitle = {{KLM}},
	url = {https://www.sciencedirect.com/science/article/pii/0196677489900382},
	doi = {10.1016/0196-6774(89)90038-2},
	abstract = {We develop polynomial time Monte-Carlo algorithms which produce good approximate solutions to enumeration problems for which it is known that the computation of the exact solution is very hard. We start by developing a Monte-Carlo approximation algorithm for the DNF counting problem, which is the problem of counting the number of satisfying truth assignments to a formula in disjunctive normal form. The input to the algorithm is the formula and two parameters ε and δ. The algorithm produces an estimate which is between 1 − ϵ and 1 + ϵ times the number of satisfying truth assignments with probability at least 1 − δ. The running time of the algorithm is linear in the length of the formula times 1ϵ2 times ln(1δ). On the other hand, the problem of computing the exact answer for the DNF counting problem is known to be \#P-complete, which implies that there is no polynomial time algorithm for the exact solution if P ≠ NP. This paper improves and gives new applications of some of the work previously reported. Variants of an ϵ, δ approximation algorithm for the DNF counting problem have been highly tailored to be especially efficient for the network reliability problems to which they are applied. In this paper the emphasis is on the development and analysis of a much more efficient ϵ, δ approximation algorithm for the DNF counting problem. The running time of the algorithm presented here substantially improves the running time of versions of this algorithm given previously. We give a new application of the algorithm to a problem which is relevant to physical chemistry and statistical physics. The resulting ϵ, δ approximation algorithm is substantially faster than the fastest known deterministic solution for the problem.},
	number = {3},
	urldate = {2024-01-08},
	journal = {Journal of Algorithms},
	author = {Karp, Richard M and Luby, Michael and Madras, Neal},
	month = sep,
	year = {1989},
	pages = {429--448},
}

@article{broeck_lifted_2015,
	title = {Lifted {Probabilistic} {Inference} for {Asymmetric} {Graphical} {Models}},
	volume = {29},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/9678},
	doi = {10.1609/aaai.v29i1.9678},
	abstract = {Lifted probabilistic inference algorithms have been successfully applied to a large number of symmetric graphical models. Unfortunately, the majority of real-world graphical models is asymmetric. This is even the case for relational representations when evidence is given. Therefore, more recent work in the community moved to making the models symmetric and then applying existing lifted inference algorithms. However, this approach has two shortcomings. First, all existing over-symmetric approximations require a relational representation such as Markov logic networks. Second, the induced symmetries often change the distribution significantly, making the computed probabilities highly biased. We present a framework for probabilistic sampling-based inference that only uses the induced approximate symmetries to propose steps in a Metropolis-Hastings style Markov chain. The framework, therefore, leads to improved probability estimates while remaining unbiased. Experiments demonstrate that the approach outperforms existing MCMC algorithms.},
	language = {en},
	number = {1},
	urldate = {2024-03-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Broeck, Guy Van den and Niepert, Mathias},
	month = mar,
	year = {2015},
	note = {Number: 1},
	keywords = {symmetry-aware inference, todo},
}

@inproceedings{peharz_einsum_2020,
	title = {Einsum {Networks}: {Fast} and {Scalable} {Learning} of {Tractable} {Probabilistic} {Circuits}},
	shorttitle = {Einsum {Networks}},
	url = {https://proceedings.mlr.press/v119/peharz20a.html},
	abstract = {Probabilistic circuits (PCs) are a promising avenue for probabilistic modeling, as they permit a wide range of exact and efficient inference routines. Recent “deep-learning-style” implementations of PCs strive for a better scalability, but are still difficult to train on real-world data, due to their sparsely connected computational graphs. In this paper, we propose Einsum Networks (EiNets), a novel implementation design for PCs, improving prior art in several regards. At their core, EiNets combine a large number of arithmetic operations in a single monolithic einsum-operation, leading to speedups and memory savings of up to two orders of magnitude, in comparison to previous implementations. As an algorithmic contribution, we show that the implementation of Expectation-Maximization (EM) can be simplified for PCs, by leveraging automatic differentiation. Furthermore, we demonstrate that EiNets scale well to datasets which were previously out of reach, such as SVHN and CelebA, and that they can be used as faithful generative image models.},
	language = {en},
	urldate = {2024-03-29},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Peharz, Robert and Lang, Steven and Vergari, Antonio and Stelzner, Karl and Molina, Alejandro and Trapp, Martin and Broeck, Guy Van Den and Kersting, Kristian and Ghahramani, Zoubin},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {7563--7574},
}

@inproceedings{dang_sparse_2022,
	title = {Sparse {Probabilistic} {Circuits} via {Pruning} and {Growing}},
	volume = {35},
	shorttitle = {{SHCLT}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/b6089408f4893289296ad0499783b3a6-Abstract-Conference.html},
	language = {en},
	urldate = {2024-03-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Dang, Meihua and Liu, Anji and Van den Broeck, Guy},
	month = dec,
	year = {2022},
	keywords = {ucla},
	pages = {28374--28385},
}

@inproceedings{zuidberg_dos_martires_probabilistic_2024,
	title = {Probabilistic {Neural} {Circuits}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	shorttitle = {{PNC}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29675},
	doi = {10.1609/aaai.v38i15.29675},
	abstract = {Probabilistic circuits (PCs) have gained prominence in recent years as a versatile framework for discussing probabilistic models that support tractable queries and are yet expressive enough to model complex probability distributions. Nevertheless, tractability comes at a cost: PCs are less expressive than neural networks. In this paper we introduce probabilistic neural circuits (PNCs), which strike a balance between PCs and neural nets in terms of tractability and expressive power. Theoretically, we show that PNCs can be interpreted as deep mixtures of Bayesian networks. Experimentally, we demonstrate that PNCs constitute powerful function approximators.},
	language = {en},
	urldate = {2024-03-28},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zuidberg Dos Martires, Pedro},
	month = mar,
	year = {2024},
	note = {Number: 15},
	keywords = {RU: Probabilistic Inference},
	pages = {17280--17289},
}

@inproceedings{darwiche_sdd_2011,
	title = {{SDD}: {A} {New} {Canonical} {Representation} of {Propositional} {Knowledge} {Bases}},
	copyright = {Authors who submit to this conference agree to the following terms:    a) Authors transfer their copyrights in their paper to the International Joint Conferences on Artificial Intelligence, Inc. (IJCAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights currently exist or hereafter come into effect, and also the exclusive right to create electronic versions of the paper, to the extent that such right is not subsumed under copyright.    b) Every named author warrants that he/she is the sole author and owner of the copyright in the paper, except for those portions shown to be in quotations; that the paper is original throughout; and that their right to make the grants set forth above is complete and unencumbered. If anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, each author, individually and collectively, will hold harmless and indemnify IJCAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense IJCAI may make to such claim or action. Moreover, each author agrees to cooperate in any claim or other action seeking to protect or enforce any right the author has granted to IJCAI in the paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, each author agrees to reimburse whomever brings such claim or action for expenses and attorney\&rsquo;s fees incurred therein.   c) \&nbsp;In return for these rights, IJCAI hereby grants to each author, and the employers for whom the work was performed, royalty-free permission to: 1. retain all proprietary rights (such as patent rights) other than copyright and the publication rights transferred to IJCAI; 2. personally reuse all or portions of the paper in other works of their own authorship; 3. make oral presentation of the material in any forum; 4. reproduce, or have reproduced, the  paper for the author\&rsquo;s personal use, or for company use provided that IJCAI copyright and the source are indicated, and that the copies are not used in a way that implies IJCAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the paper in electronic or digital form on any computer network, except by the author or the author\&rsquo;s employer, and then only on the author\&rsquo;s or the employer\&rsquo;s own World Wide Web page or ftp site. Such Web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the IJCAI electronic server (http://www.ijcai.org), and shall not post other IJCAI copyrighted materials not of the author\&rsquo;s or the employer\&rsquo;s creation (including tables of contents with links to other papers) without IJCAI\&rsquo;s written permission; \&gt;5. make limited distribution of all or portions of the above paper prior to publication. 6. In the case of work performed under U.S. Government contract, IJCAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above paper, and to authorize others to do so, for U.S. Government purposes. In the event the above paper is not accepted and published by IJCAI, or is withdrawn by the author(s) before acceptance by IJCAI, this agreement becomes null and void.},
	shorttitle = {{SDD}},
	url = {https://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/view/3341},
	abstract = {We identify a new representation of propositional knowledge bases, the  Sentential Decision Diagram  SDD,\&nbsp;which is interesting for a number of reasons. First, it is canonical in the presence of additional properties that resemble reduction rules of OBDDs. Second, SDDs can be combined using any Boolean operator in polytime.\&nbsp;Third, CNFs with  n  variables and treewidth  w  have canonical SDDs of size  O ( n  2  w  ), which is tighter than the bound on OBDDs based on pathwidth.\&nbsp;Finally, every OBDD is an SDD. Hence, working with the latter does not preclude the former.},
	language = {en},
	urldate = {2022-10-13},
	booktitle = {Twenty-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Darwiche, Adnan},
	month = jun,
	year = {2011},
	keywords = {ucla},
}

@misc{broadrick_polynomial_2024,
	title = {Polynomial {Semantics} of {Tractable} {Probabilistic} {Circuits}},
	url = {http://arxiv.org/abs/2402.09085},
	abstract = {Probabilistic circuits compute multilinear polynomials that represent probability distributions. They are tractable models that support efficient marginal inference. However, various polynomial semantics have been considered in the literature (e.g., network polynomials, likelihood polynomials, generating functions, Fourier transforms, and characteristic polynomials). The relationships between these polynomial encodings of distributions is largely unknown. In this paper, we prove that for binary distributions, each of these probabilistic circuit models is equivalent in the sense that any circuit for one of them can be transformed into a circuit for any of the others with only a polynomial increase in size. They are therefore all tractable for marginal inference on the same class of distributions. Finally, we explore the natural extension of one such polynomial semantics, called probabilistic generating circuits, to categorical random variables, and establish that marginal inference becomes \#P-hard.},
	language = {en},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Broadrick, Oliver and Zhang, Honghua and Broeck, Guy Van den},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09085 [cs]},
	keywords = {Computer Science - Artificial Intelligence, ucla},
}

@inproceedings{liu_bridging_2023,
	title = {Bridging {Discrete} and {Backpropagation}: {Straight}-{Through} and {Beyond}},
	volume = {36},
	shorttitle = {{ReinMax}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/28b5dfc51e5ae12d84fb7c6172a00df4-Abstract-Conference.html},
	language = {en},
	urldate = {2024-03-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Liu, Liyuan and Dong, Chengyu and Liu, Xiaodong and Yu, Bin and Gao, Jianfeng},
	month = dec,
	year = {2023},
	pages = {12291--12311},
}

@inproceedings{jiang_learning_2023,
	title = {Learning {Markov} {Random} {Fields} for {Combinatorial} {Structures} via {Sampling} through {Lovász} {Local} {Lemma}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/25516},
	doi = {10.1609/aaai.v37i4.25516},
	abstract = {Learning to generate complex combinatorial structures satisfying constraints will have transformative impacts in many application domains. However, it is beyond the capabilities of existing approaches due to the highly intractable nature of the embedded probabilistic inference. Prior works spend most of the training time learning to separate valid from invalid structures but do not learn the inductive biases of valid structures. We develop NEural Lovasz Sampler (NELSON), which embeds the sampler through Lovasz Local Lemma (LLL) as a fully differentiable neural network layer. Our NELSON-CD embeds this sampler into the contrastive divergence learning process of Markov random fields. NELSON allows us to obtain valid samples from the current model distribution. Contrastive divergence is then applied to separate these samples from those in the training set. NELSON is implemented as a fully differentiable neural net, taking advantage of the parallelism of GPUs. Experimental results on several real-world domains reveal that NELSON learns to generate 100\% valid structures, while baselines either time out or cannot ensure validity. NELSON also outperforms other approaches in running time, log-likelihood, and MAP scores.},
	language = {en},
	urldate = {2024-03-25},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Jiang, Nan and Gu, Yi and Xue, Yexiang},
	month = jun,
	year = {2023},
	note = {Number: 4},
	keywords = {CSO: Constraint Satisfaction},
	pages = {4016--4024},
}

@inproceedings{zaiser_exact_2023,
	title = {Exact {Bayesian} {Inference} on {Discrete} {Models} via {Probability} {Generating} {Functions}: {A} {Probabilistic} {Programming} {Approach}},
	volume = {36},
	shorttitle = {Exact {Bayesian} {Inference} on {Discrete} {Models} via {Probability} {Generating} {Functions}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/0747af6f877c0cb555fea595f01b0e83-Abstract-Conference.html},
	language = {en},
	urldate = {2024-03-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zaiser, Fabian and Murawski, Andrzej and Ong, Chih-Hao Luke},
	month = dec,
	year = {2023},
	pages = {2427--2462},
}

@misc{lanzinger_fuzzy_2024,
	title = {Fuzzy {Datalog}\${\textasciicircum}{\textbackslash}exists\$ over {Arbitrary} t-{Norms}},
	url = {http://arxiv.org/abs/2403.02933},
	abstract = {One of the main challenges in the area of Neuro-Symbolic AI is to perform logical reasoning in the presence of both neural and symbolic data. This requires combining heterogeneous data sources such as knowledge graphs, neural model predictions, structured databases, crowd-sourced data, and many more. To allow for such reasoning, we generalise the standard rule-based language Datalog with existential rules (commonly referred to as tuple-generating dependencies) to the fuzzy setting, by allowing for arbitrary t-norms in the place of classical conjunctions in rule bodies. The resulting formalism allows us to perform reasoning about data associated with degrees of uncertainty while preserving computational complexity results and the applicability of reasoning techniques established for the standard Datalog setting. In particular, we provide fuzzy extensions of Datalog chases which produce fuzzy universal models and we exploit them to show that in important fragments of the language, reasoning has the same complexity as in the classical setting.},
	language = {en},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Lanzinger, Matthias and Sferrazza, Stefano and Wałęga, Przemysław A. and Gottlob, Georg},
	month = mar,
	year = {2024},
	note = {arXiv:2403.02933 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science},
}

@inproceedings{zhang_probabilistic_2021,
	title = {Probabilistic {Generating} {Circuits}},
	url = {https://proceedings.mlr.press/v139/zhang21i.html},
	abstract = {Generating functions, which are widely used in combinatorics and probability theory, encode function values into the coefficients of a polynomial. In this paper, we explore their use as a tractable probabilistic model, and propose probabilistic generating circuits (PGCs) for their efficient representation. PGCs are strictly more expressive efficient than many existing tractable probabilistic models, including determinantal point processes (DPPs), probabilistic circuits (PCs) such as sum-product networks, and tractable graphical models. We contend that PGCs are not just a theoretical framework that unifies vastly different existing models, but also show great potential in modeling realistic data. We exhibit a simple class of PGCs that are not trivially subsumed by simple combinations of PCs and DPPs, and obtain competitive performance on a suite of density estimation benchmarks. We also highlight PGCs’ connection to the theory of strongly Rayleigh distributions.},
	language = {en},
	urldate = {2024-03-08},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Honghua and Juba, Brendan and Broeck, Guy Van Den},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {ucla},
	pages = {12447--12457},
}

@misc{yang_large_2024,
	title = {Do {Large} {Language} {Models} {Latently} {Perform} {Multi}-{Hop} {Reasoning}?},
	url = {http://arxiv.org/abs/2402.16837},
	doi = {10.48550/arXiv.2402.16837},
	abstract = {We study whether Large Language Models (LLMs) latently perform multi-hop reasoning with complex prompts such as "The mother of the singer of 'Superstition' is". We look for evidence of a latent reasoning pathway where an LLM (1) latently identifies "the singer of 'Superstition'" as Stevie Wonder, the bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to complete the prompt. We analyze these two hops individually and consider their co-occurrence as indicative of latent multi-hop reasoning. For the first hop, we test if changing the prompt to indirectly mention the bridge entity instead of any other entity increases the LLM's internal recall of the bridge entity. For the second hop, we test if increasing this recall causes the LLM to better utilize what it knows about the bridge entity. We find strong evidence of latent multi-hop reasoning for the prompts of certain relation types, with the reasoning pathway used in more than 80\% of the prompts. However, the utilization is highly contextual, varying across different types of prompts. Also, on average, the evidence for the second hop and the full multi-hop traversal is rather moderate and only substantial for the first hop. Moreover, we find a clear scaling trend with increasing model size for the first hop of reasoning but not for the second hop. Our experimental findings suggest potential challenges and opportunities for future development and applications of LLMs.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Yang, Sohee and Gribovskaya, Elena and Kassner, Nora and Geva, Mor and Riedel, Sebastian},
	month = feb,
	year = {2024},
	note = {arXiv:2402.16837 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{lorello_challenge_2023,
	address = {Siena, Italy},
	title = {The {Challenge} of {Learning} {Symbolic} {Representations}},
	url = {https://www.cs.ox.ac.uk/isg/conferences/tmp-proceedings/NeSy2023/paper4.pdf},
	urldate = {2024-03-07},
	booktitle = {17th {International} {Workshop} on {Neural}-{Symbolic} {Learning} and {Reasoning}},
	author = {Lorello, Luca Salvatore and Lippi, Marco},
	year = {2023},
}

@inproceedings{peharz_random_2020,
	title = {Random {Sum}-{Product} {Networks}: {A} {Simple} and {Effective} {Approach} to {Probabilistic} {Deep} {Learning}},
	shorttitle = {{RAT}-{SPN}},
	url = {https://proceedings.mlr.press/v115/peharz20a.html},
	abstract = {Sum-product networks (SPNs) are expressive probabilistic models with a rich set of exact and efficient inference routines. However, in order to guarantee exact inference, they require specific structural constraints, which complicate learning SPNs from data. Thereby, most SPN structure learners proposed so far are tedious to tune, do not scale easily, and are not easily integrated with deep learning frameworks. In this paper, we follow a simple “deep learning” approach, by generating unspecialized random structures, scalable to millions of parameters, and subsequently applying GPU-based optimization. Somewhat surprisingly, our models often perform on par with state-of-the-art SPN structure learners and deep neural networks on a diverse range of generative and discriminative scenarios. At the same time, our models yield well-calibrated uncertainties, and stand out among most deep generative and discriminative models in being robust to missing features and being able to detect anomalies.},
	language = {en},
	urldate = {2023-11-08},
	booktitle = {Proceedings of {The} 35th {Uncertainty} in {Artificial} {Intelligence} {Conference}},
	publisher = {PMLR},
	author = {Peharz, Robert and Vergari, Antonio and Stelzner, Karl and Molina, Alejandro and Shao, Xiaoting and Trapp, Martin and Kersting, Kristian and Ghahramani, Zoubin},
	month = aug,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {334--344},
}

@inproceedings{shukla_unified_2023,
	title = {A {Unified} {Approach} to {Count}-{Based} {Weakly} {Supervised} {Learning}},
	url = {https://openreview.net/forum?id=IyAHCbMq3a&referrer=%5Bthe%20profile%20of%20Guy%20Van%20den%20Broeck%5D(%2Fprofile%3Fid%3D~Guy_Van_den_Broeck1)},
	abstract = {High-quality labels are often very scarce, whereas unlabeled data with inferred weak labels occurs more naturally. In many cases, these weak labels dictate the frequency of each respective class over a set of instances. In this paper, we develop a unified approach to learning from such weakly-labeled data, which we call *count-based weakly-supervised learning*. At the heart of our approach is the ability to compute the probability of exactly \$k\$ out of \$n\$ outputs being set to true. This computation is differentiable, exact, and efficient. Building upon the previous computation, we derive a *count loss* penalizing the model for deviations in its distribution from an arithmetic constraint defined over label counts.},
	language = {en},
	urldate = {2024-02-28},
	author = {Shukla, Vinay and Zeng, Zhe and Ahmed, Kareem and Broeck, Guy Van den},
	month = nov,
	year = {2023},
}

@inproceedings{zhu_neural-symbolic_2022,
	title = {Neural-{Symbolic} {Models} for {Logical} {Queries} on {Knowledge} {Graphs}},
	url = {https://proceedings.mlr.press/v162/zhu22c.html},
	abstract = {Answering complex first-order logic (FOL) queries on knowledge graphs is a fundamental task for multi-hop reasoning. Traditional symbolic methods traverse a complete knowledge graph to extract the answers, which provides good interpretation for each step. Recent neural methods learn geometric embeddings for complex queries. These methods can generalize to incomplete knowledge graphs, but their reasoning process is hard to interpret. In this paper, we propose Graph Neural Network Query Executor (GNN-QE), a neural-symbolic model that enjoys the advantages of both worlds. GNN-QE decomposes a complex FOL query into relation projections and logical operations over fuzzy sets, which provides interpretability for intermediate variables. To reason about the missing links, GNN-QE adapts a graph neural network from knowledge graph completion to execute the relation projections, and models the logical operations with product fuzzy logic. Experiments on 3 datasets show that GNN-QE significantly improves over previous state-of-the-art models in answering FOL queries. Meanwhile, GNN-QE can predict the number of answers without explicit supervision, and provide visualizations for intermediate variables.},
	language = {en},
	urldate = {2024-01-23},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhu, Zhaocheng and Galkin, Mikhail and Zhang, Zuobai and Tang, Jian},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {27454--27478},
}

@misc{ly_tutorial_2017,
	title = {A {Tutorial} on {Fisher} {Information}},
	url = {http://arxiv.org/abs/1705.01064},
	abstract = {In many statistical applications that concern mathematical psychologists, the concept of Fisher information plays an important role. In this tutorial we clarify the concept of Fisher information as it manifests itself across three diﬀerent statistical paradigms. First, in the frequentist paradigm, Fisher information is used to construct hypothesis tests and conﬁdence intervals using maximum likelihood estimators; second, in the Bayesian paradigm, Fisher information is used to deﬁne a default prior; lastly, in the minimum description length paradigm, Fisher information is used to measure model complexity.},
	language = {en},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Ly, Alexander and Marsman, Maarten and Verhagen, Josine and Grasman, Raoul and Wagenmakers, Eric-Jan},
	month = oct,
	year = {2017},
	note = {arXiv:1705.01064 [math, stat]},
	keywords = {62-01, 62B10 (Primary), 62F03, 62F12, 62F15, 62B10 (Secondary), Mathematics - Statistics Theory, todo},
}

@misc{shah_reals_2024,
	title = {From {Reals} to {Logic} and {Back}: {Inventing} {Symbolic} {Vocabularies}, {Actions} and {Models} for {Planning} from {Raw} {Data}},
	shorttitle = {From {Reals} to {Logic} and {Back}},
	url = {http://arxiv.org/abs/2402.11871},
	abstract = {Hand-crafted, logic-based state and action representations have been widely used to overcome the intractable computational complexity of long-horizon robot planning problems, including task and motion planning problems. However, creating such representations requires experts with strong intuitions and detailed knowledge about the robot and the tasks it may need to accomplish in a given setting. Removing this dependency on human intuition is a highly active research area.},
	language = {en},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Shah, Naman and Nagpal, Jayesh and Verma, Pulkit and Srivastava, Siddharth},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11871 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, todo},
}

@inproceedings{liu_tractable_2021,
	title = {Tractable {Regularization} of {Probabilistic} {Circuits}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/1d0832c4969f6a4cc8e8a8fffe083efb-Abstract.html},
	abstract = {Probabilistic Circuits (PCs) are a promising avenue for probabilistic modeling. They combine advantages of probabilistic graphical models (PGMs) with those of neural networks (NNs). Crucially, however, they are tractable probabilistic models, supporting efficient and exact computation of many probabilistic inference queries, such as marginals and MAP. Further, since PCs are structured computation graphs, they can take advantage of deep-learning-style parameter updates, which greatly improves their scalability. However, this innovation also makes PCs prone to overfitting, which has been observed in many standard benchmarks. Despite the existence of abundant regularization techniques for both PGMs and NNs, they are not effective enough when applied to PCs. Instead, we re-think regularization for PCs and propose two intuitive techniques, data softening and entropy regularization, that both take advantage of PCs' tractability and still have an efficient implementation as a computation graph. Specifically, data softening provides a principled way to add uncertainty in datasets in closed form, which implicitly regularizes PC parameters. To learn parameters from a softened dataset, PCs only need linear time by virtue of their tractability. In entropy regularization, the exact entropy of the distribution encoded by a PC can be regularized directly, which is again infeasible for most other density estimation models. We show that both methods consistently improve the generalization performance of a wide variety of PCs. Moreover, when paired with a simple PC structure, we achieved state-of-the-art results on 10 out of 20 standard discrete density estimation benchmarks. Open-source code and experiments are available at https://github.com/UCLA-StarAI/Tractable-PC-Regularization.},
	urldate = {2024-02-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Anji and Van den Broeck, Guy},
	year = {2021},
	pages = {3558--3570},
}

@inproceedings{kunstner_limitations_2019,
	title = {Limitations of the empirical {Fisher} approximation for natural gradient descent},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/46a558d97954d0692411c861cf78ef79-Abstract.html},
	abstract = {Natural gradient descent, which preconditions a gradient descent update
with the Fisher information matrix of the underlying statistical model,
is a way to capture partial second-order information. 
Several highly visible works have advocated an approximation known as the empirical Fisher,
drawing connections between approximate second-order methods and heuristics like Adam.
We dispute this argument by showing that the empirical Fisher---unlike the Fisher---does not generally capture second-order information.
We further argue that the conditions under which the empirical Fisher approaches the Fisher (and the Hessian)
are unlikely to be met in practice, and that, even on simple optimization problems,
the pathologies of the empirical Fisher can have undesirable effects.},
	urldate = {2024-02-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kunstner, Frederik and Hennig, Philipp and Balles, Lukas},
	year = {2019},
}

@inproceedings{yao_adahessian_2021,
	title = {{ADAHESSIAN}: {An} {Adaptive} {Second} {Order} {Optimizer} for {Machine} {Learning}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	shorttitle = {{ADAHESSIAN}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17275},
	doi = {10.1609/aaai.v35i12.17275},
	abstract = {Incorporating second-order curvature information into machine learning optimization algorithms can be subtle, and doing so naïvely can lead to high per-iteration costs associated with forming the Hessian and performing the associated linear system solve. To address this, we introduce ADAHESSIAN, a new stochastic optimization algorithm. ADAHESSIAN directly incorporates approximate curvature information from the loss function, and it includes several novel performance-improving features, including: (i) a fast Hutchinson based method to approximate the curvature matrix with low computational overhead; (ii) a spatial averaging to reduce the variance of the second derivative; and (iii) a root-mean-square exponential moving average to smooth out variations of the second-derivative across different iterations. We perform extensive tests on NLP, CV, and recommendation system tasks, and ADAHESSIAN achieves state-of-the-art results. In particular, we find that ADAHESSIAN: (i) outperforms AdamW for transformers by0.13/0.33 BLEU score on IWSLT14/WMT14, 2.7/1.0 PPLon PTB/Wikitext-103; (ii) outperforms AdamW for Squeeze-Bert by 0.41 points on GLUE; (iii) achieves 1.45\%/5.55\%higher accuracy on ResNet32/ResNet18 on Cifar10/ImageNetas compared to Adam; and (iv) achieves 0.032\% better score than Adagrad for DLRM on the Criteo Ad Kaggle dataset. The cost per iteration of ADAHESSIANis comparable to first-order methods, and ADAHESSIAN exhibits improved robustness towards variations in hyperparameter values. The code for ADAHESSIAN is open-sourced and publicly-available [1].},
	language = {en},
	urldate = {2024-02-22},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Yao, Zhewei and Gholami, Amir and Shen, Sheng and Mustafa, Mustafa and Keutzer, Kurt and Mahoney, Michael},
	month = may,
	year = {2021},
	note = {Number: 12},
	keywords = {Learning \& Optimization for SNLP},
	pages = {10665--10673},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{wust_pix2code_2024,
	title = {{Pix2Code}: {Learning} to {Compose} {Neural} {Visual} {Concepts} as {Programs}},
	shorttitle = {{Pix2Code}},
	url = {http://arxiv.org/abs/2402.08280},
	abstract = {The challenge in learning abstract concepts from images in an unsupervised fashion lies in the required integration of visual perception and generalizable relational reasoning. Moreover, the unsupervised nature of this task makes it necessary for human users to be able to understand a model’s learnt concepts and potentially revise false behaviours. To tackle both the generalizability and interpretability constraints of visual concept learning, we propose Pix2Code, a framework that extends program synthesis to visual relational reasoning by utilizing the abilities of both explicit, compositional symbolic and implicit neural representations. This is achieved by retrieving object representations from images and synthesizing relational concepts as λcalculus programs. We evaluate the diverse properties of Pix2Code on the challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its ability to identify compositional visual concepts that generalize to novel data and concept configurations. Particularly, in stark contrast to neural approaches, we show that Pix2Code’s representations remain human interpretable and can be easily revised for improved performance.},
	language = {en},
	urldate = {2024-02-14},
	publisher = {arXiv},
	author = {Wüst, Antonia and Stammer, Wolfgang and Delfosse, Quentin and Dhami, Devendra Singh and Kersting, Kristian},
	month = feb,
	year = {2024},
	note = {arXiv:2402.08280 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, darmstadt, todo},
}

@inproceedings{ahmed_pylon_2022,
	title = {Pylon: {A} {PyTorch} {Framework} for {Learning} with {Constraints}},
	shorttitle = {Pylon},
	url = {https://proceedings.mlr.press/v176/ahmed22a.html},
	abstract = {Deep learning excels at learning low-level task information from large amounts of data, but struggles with learning high-level domain knowledge, which can often be directly and succinctly expressed. In this work, we introduce Pylon, a neuro-symbolic training framework that builds on PyTorch to augment procedurally trained neural networks with declaratively specified knowledge. Pylon allows users to programmatically specify constraints{\textless}{\textbackslash}em{\textgreater} as PyTorch functions, and compiles them into a differentiable loss, thus training predictive models that fit the data whilst{\textless}{\textbackslash}em{\textgreater} satisfying the specified constraints. Pylon includes both exact as well as approximate compilers to efficiently compute the loss, employing fuzzy logic, sampling methods, and circuits, ensuring scalability even to complex models and constraints. A guiding principle in designing Pylon has been the ease with which any existing deep learning codebase can be extended to learn from constraints using only a few lines: a function expressing the constraint and a single line of code to compile it into a loss. We include case studies from natural language processing, computer vision, logical games, and knowledge graphs, that can be interactively trained, and highlights Pylon\{’\}s usage.},
	language = {en},
	urldate = {2024-02-14},
	booktitle = {Proceedings of the {NeurIPS} 2021 {Competitions} and {Demonstrations} {Track}},
	publisher = {PMLR},
	author = {Ahmed, Kareem and Li, Tao and Ton, Thy and Guo, Quan and Chang, Kai-Wei and Kordjamshidi, Parisa and Srikumar, Vivek and Broeck, Guy Van den and Singh, Sameer},
	month = jul,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {ucla},
	pages = {319--324},
}

@article{domshlak_probabilistic_2007,
	title = {Probabilistic {Planning} via {Heuristic} {Forward} {Search} and {Weighted} {Model} {Counting}},
	volume = {30},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/10523},
	doi = {10.1613/jair.2289},
	abstract = {We present a new algorithm for probabilistic planning with no observability.  Our algorithm, called  Probabilistic-FF, extends the heuristic forward-search machinery of Conformant-FF to problems with probabilistic uncertainty about both the initial state and action effects. Specifically,  Probabilistic-FF combines Conformant-FF's techniques with a powerful machinery for weighted model counting in (weighted) CNFs, serving to elegantly define both the search space and the heuristic function. Our evaluation of  Probabilistic-FF shows its fine scalability in a range of probabilistic domains, constituting a several orders of magnitude improvement over previous results in this area. We use a problematic case to point out the main open issue to be addressed by further research.},
	language = {en},
	urldate = {2024-02-14},
	journal = {Journal of Artificial Intelligence Research},
	author = {Domshlak, C. and Hoffmann, J.},
	month = dec,
	year = {2007},
	pages = {565--620},
}

@book{suciu_probabilistic_2011,
	address = {Cham},
	series = {Synthesis {Lectures} on {Data} {Management}},
	title = {Probabilistic {Databases}},
	isbn = {978-3-031-00751-4 978-3-031-01879-4},
	url = {https://link.springer.com/10.1007/978-3-031-01879-4},
	language = {en},
	urldate = {2024-02-14},
	publisher = {Springer International Publishing},
	author = {Suciu, Dan and Olteanu, Dan and Ré, Christopher and Koch, Christoph},
	year = {2011},
	doi = {10.1007/978-3-031-01879-4},
}

@inproceedings{belle_hashing-based_2015,
	title = {Hashing-based approximate probabilistic inference in hybrid domains},
	url = {https://lirias.kuleuven.be/1599178?limo=0},
	urldate = {2024-02-14},
	booktitle = {Proceedings of the 31st {Conference} on {Uncertainty} in {Artificial} {Intelligence} ({UAI})},
	publisher = {AUAI PRESS},
	author = {Belle, Vaishak and Van den Broeck, Guy and Passerini, Andrea},
	year = {2015},
	pages = {141--150},
}

@article{holtzen_scaling_2020,
	title = {Scaling exact inference for discrete probabilistic programs},
	volume = {4},
	shorttitle = {Dice},
	url = {https://dl.acm.org/doi/10.1145/3428208},
	doi = {10.1145/3428208},
	abstract = {Probabilistic programming languages (PPLs) are an expressive means of representing and reasoning about probabilistic models. The computational challenge of probabilistic inference remains the primary roadblock for applying PPLs in practice. Inference is fundamentally hard, so there is no one-size-fits all solution. In this work, we target scalable inference for an important class of probabilistic programs: those whose probability distributions are discrete. Discrete distributions are common in many fields, including text analysis, network verification, artificial intelligence, and graph analysis, but they prove to be challenging for existing PPLs. We develop a domain-specific probabilistic programming language called Dice that features a new approach to exact discrete probabilistic program inference. Dice exploits program structure in order to factorize inference, enabling us to perform exact inference on probabilistic programs with hundreds of thousands of random variables. Our key technical contribution is a new reduction from discrete probabilistic programs to weighted model counting (WMC). This reduction separates the structure of the distribution from its parameters, enabling logical reasoning tools to exploit that structure for probabilistic inference. We (1) show how to compositionally reduce Dice inference to WMC, (2) prove this compilation correct with respect to a denotational semantics, (3) empirically demonstrate the performance benefits over prior approaches, and (4) analyze the types of structure that allow Dice to scale to large probabilistic programs.},
	number = {OOPSLA},
	urldate = {2024-01-23},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Holtzen, Steven and Van den Broeck, Guy and Millstein, Todd},
	month = nov,
	year = {2020},
	keywords = {Probabilistic programming, ucla},
	pages = {140:1--140:31},
}

@inproceedings{van_den_broeck_complexity_2013,
	title = {On the {Complexity} and {Approximation} of {Binary} {Evidence} in {Lifted} {Inference}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/7940ab47468396569a906f75ff3f20ef-Abstract.html},
	abstract = {Lifted inference algorithms exploit symmetries in probabilistic models to speed up inference. They show impressive performance when calculating unconditional probabilities in relational models, but often resort to non-lifted inference when computing conditional probabilities.  The reason is that conditioning on evidence breaks many of the model's symmetries, which preempts standard lifting techniques.  Recent theoretical results show, for example, that conditioning on evidence which corresponds to binary relations is \#P-hard, suggesting that no lifting is to be expected in the worst case. In this paper, we balance this grim result by identifying the Boolean rank of the evidence as a key parameter for characterizing the complexity of conditioning in lifted inference.  In particular, we show that conditioning on binary evidence with bounded Boolean rank is efficient. This opens up the possibility of approximating evidence by a low-rank Boolean matrix factorization, which we investigate both theoretically and empirically.},
	urldate = {2023-09-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Van den Broeck, Guy and Darwiche, Adnan},
	year = {2013},
	keywords = {ucla},
}

@inproceedings{vergari_compositional_2021,
	title = {A {Compositional} {Atlas} of {Tractable} {Circuit} {Operations} for {Probabilistic} {Inference}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/6e01383fd96a17ae51cc3e15447e7533-Abstract.html},
	abstract = {Circuit representations are becoming the lingua franca to express and reason about tractable generative and discriminative models. In this paper, we show how complex inference scenarios for these models that commonly arise in machine learning---from  computing the expectations of decision tree ensembles to information-theoretic divergences of sum-product networks---can be represented in terms of tractable modular operations over circuits. Specifically, we characterize the tractability of simple transformations---sums, products, quotients, powers, logarithms, and exponentials---in terms of sufficient structural constraints of the circuits they operate on, and present novel hardness results for the cases in which these properties are not satisfied. Building on these operations, we derive a unified framework for reasoning about tractable models that generalizes several results in the literature and opens up novel tractable inference scenarios.},
	urldate = {2024-02-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vergari, Antonio and Choi, YooJung and Liu, Anji and Teso, Stefano and Van den Broeck, Guy},
	year = {2021},
	keywords = {ucla},
	pages = {13189--13201},
}

@misc{zhang_tractable_2023,
	title = {Tractable {Control} for {Autoregressive} {Language} {Generation}},
	shorttitle = {{GeLatTo}},
	url = {http://arxiv.org/abs/2304.07438},
	abstract = {Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisﬁes complex constraints: sampling from the conditional distribution Pr(text {\textbar} α) is intractable for even the simplest lexical constraints α. To overcome this challenge, we propose to use tractable probabilistic models to impose lexical constraints in autoregressive text generation, which we refer to as GeLaTo. To demonstrate the effectiveness of this framework, we use distilled hidden Markov models to control autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on CommonGen, a challenging benchmark for constrained text generation, beating a wide range of strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive tractable probabilistic models.},
	language = {en},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Zhang, Honghua and Dang, Meihua and Peng, Nanyun and Van den Broeck, Guy},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07438 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, ucla},
}

@article{niepert_tractability_2014,
	title = {Tractability through {Exchangeability}: {A} {New} {Perspective} on {Efficient} {Probabilistic} {Inference}},
	volume = {28},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	shorttitle = {Tractability through {Exchangeability}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/9073},
	doi = {10.1609/aaai.v28i1.9073},
	abstract = {Exchangeability is a central notion in statistics and probability theory. The assumption that an infinite sequence of data points is exchangeable is at the core of Bayesian statistics. However, finite exchangeability as a statistical property that renders probabilistic inference tractable is less well-understood. We develop a theory of finite exchangeability and its relation to tractable probabilistic inference. The theory is complementary to that of independence and conditional independence. We show that tractable inference in probabilistic models with high treewidth and millions of variables can be explained with the notion of finite (partial) exchangeability. We also show that existing lifted inference algorithms implicitly utilize a combination of conditional independence and partial exchangeability.},
	language = {en},
	number = {1},
	urldate = {2023-09-14},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Niepert, Mathias and Van den Broeck, Guy},
	month = jun,
	year = {2014},
	note = {Number: 1},
	keywords = {efficient inference, leuven},
}

@inproceedings{friedman_symbolic_2020,
	title = {Symbolic {Querying} of {Vector} {Spaces}: {Probabilistic} {Databases} {Meets} {Relational} {Embeddings}},
	shorttitle = {{TRACTOR}},
	url = {https://proceedings.mlr.press/v124/friedman20a.html},
	abstract = {We propose unifying techniques from probabilistic databases and relational embedding models with the goal of performing complex queries on incomplete and uncertain data. We formalize a probabilistic database model with respect to which all queries are done. This allows us to leverage the rich literature of theory and algorithms from probabilistic databases for solving problems. While this formalization can be used with any relational embedding model, the lack of a well-defined joint probability distribution causes simple query problems to become provably hard. With this in mind, we introduce TractOR, a relational embedding model designed to be a tractable probabilistic database, by exploiting typical embedding assumptions within the probabilistic framework. Using a principled, efficient inference algorithm that can be derived from its definition, we empirically demonstrate that TractOR is an effective and general model for these querying tasks.},
	language = {en},
	urldate = {2023-05-23},
	booktitle = {Proceedings of the 36th {Conference} on {Uncertainty} in {Artificial} {Intelligence} ({UAI})},
	publisher = {PMLR},
	author = {Friedman, Tal and Van den Broeck, Guy},
	month = aug,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {todo, ucla},
	pages = {1268--1277},
}

@inproceedings{ahmed_semantic_2023,
	title = {Semantic {Strengthening} of {Neuro}-{Symbolic} {Learning}},
	shorttitle = {Semantic {Strengthening}},
	url = {https://proceedings.mlr.press/v206/ahmed23a.html},
	abstract = {Numerous neuro-symbolic approaches have recently been proposed typically with the goal of adding symbolic knowledge to the output layer of a neural network. Ideally, such losses maximize the probability that the neural network’s predictions satisfy the underlying domain. Unfortunately, this type of probabilistic inference is often computationally infeasible. Neuro-symbolic approaches therefore commonly resort to fuzzy approximations of this probabilistic objective, sacrificing sound probabilistic semantics, or to sampling which is very seldom feasible. We approach the problem by first assuming the constraint decomposes conditioned on the features learned by the network. We iteratively strengthen our approximation, restoring the dependence between the constraints most responsible for degrading the quality of the approximation. This corresponds to computing the mutual information between pairs of constraints conditioned on the network’s learned features, and may be construed as a measure of how well aligned the gradients of two distributions are. We show how to compute this efficiently for tractable circuits. We test our approach on three tasks: predicting a minimum-cost path in Warcraft, predicting a minimum-cost perfect matching, and solving Sudoku puzzles, observing that it improves upon the baselines while sidestepping intractability.},
	language = {en},
	urldate = {2023-09-29},
	booktitle = {Proceedings of {The} 26th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Ahmed, Kareem and Chang, Kai-Wei and Van den Broeck, Guy},
	month = apr,
	year = {2023},
	note = {ISSN: 2640-3498},
	keywords = {ucla},
	pages = {10252--10261},
}

@inproceedings{ahmed_semantic_2022,
	title = {Semantic {Probabilistic} {Layers} for {Neuro}-{Symbolic} {Learning}},
	shorttitle = {{SPL}},
	url = {https://openreview.net/forum?id=o-mxIWAY1T8},
	abstract = {We design a predictive layer for structured-output prediction (SOP) that can be plugged into any neural network guaranteeing its predictions are consistent with a set of predefined symbolic constraints. Our Semantic Probabilistic Layer (SPL) can model intricate correlations, and hard constraints, over a structured output space all while being amenable to end-to-end learning via maximum likelihood. SPLs combine exact probabilistic inference with logical reasoning in a clean and modular way, learning complex distributions and restricting their support to solutions of the constraint. As such, they can faithfully, and efficiently, model complex SOP tasks beyond the reach of alternative neuro-symbolic approaches. We empirically demonstrate that SPLs outperform these competitors in terms of accuracy on challenging SOP tasks such as hierarchical multi-label classification, pathfinding and preference learning, while retaining perfect constraint satisfaction.},
	language = {en},
	urldate = {2023-12-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ahmed, Kareem and Teso, Stefano and Chang, Kai-Wei and Van den Broeck, Guy and Vergari, Antonio},
	month = may,
	year = {2022},
	keywords = {ucla},
}

@article{van_den_broeck_query_2017,
	title = {Query {Processing} on {Probabilistic} {Data}: {A} {Survey}},
	volume = {7},
	issn = {1931-7883, 1931-7891},
	shorttitle = {Query {Processing} on {Probabilistic} {Data}},
	url = {https://www.nowpublishers.com/article/Details/DBS-052},
	doi = {10.1561/1900000052},
	abstract = {Query Processing on Probabilistic Data: A Survey},
	language = {English},
	number = {3-4},
	urldate = {2023-11-15},
	journal = {Foundations and Trends® in Databases},
	author = {Van den Broeck, Guy and Suciu, Dan},
	month = aug,
	year = {2017},
	note = {Publisher: Now Publishers, Inc.},
	pages = {197--341},
}

@misc{zhang_paradox_2022,
	title = {On the {Paradox} of {Learning} to {Reason} from {Data}},
	url = {http://arxiv.org/abs/2205.11502},
	doi = {10.48550/arXiv.2205.11502},
	abstract = {Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be trained end-to-end to solve logical reasoning problems presented in natural language? We attempt to answer this question in a confined problem space where there exists a set of parameters that perfectly simulates logical reasoning. We make observations that seem to contradict each other: BERT attains near-perfect accuracy on in-distribution test examples while failing to generalize to other data distributions over the exact same problem space. Our study provides an explanation for this paradox: instead of learning to emulate the correct reasoning function, BERT has in fact learned statistical features that inherently exist in logical reasoning problems. We also show that it is infeasible to jointly remove statistical features from data, illustrating the difficulty of learning to reason in general. Our result naturally extends to other neural models and unveils the fundamental difference between learning to reason and learning to achieve high performance on NLP benchmarks using statistical features.},
	urldate = {2023-03-23},
	publisher = {arXiv},
	author = {Zhang, Honghua and Li, Liunian Harold and Meng, Tao and Chang, Kai-Wei and Van den Broeck, Guy},
	month = may,
	year = {2022},
	note = {arXiv:2205.11502 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, ucla},
}

@article{van_den_broeck_completeness_nodate,
	title = {On the {Completeness} of {First}-{Order} {Knowledge} {Compilation} for {Lifted} {Probabilistic} {Inference}},
	abstract = {Probabilistic logics are receiving a lot of attention today because of their expressive power for knowledge representation and learning. However, this expressivity is detrimental to the tractability of inference, when done at the propositional level. To solve this problem, various lifted inference algorithms have been proposed that reason at the ﬁrst-order level, about groups of objects as a whole. Despite the existence of various lifted inference approaches, there are currently no completeness results about these algorithms. The key contribution of this paper is that we introduce a formal deﬁnition of lifted inference that allows us to reason about the completeness of lifted inference algorithms relative to a particular class of probabilistic models. We then show how to obtain a completeness result using a ﬁrst-order knowledge compilation approach for theories of formulae containing up to two logical variables.},
	language = {en},
	author = {Van den Broeck, Guy},
	keywords = {leuven},
}

@inproceedings{ahmed_neuro-symbolic_2022,
	title = {Neuro-symbolic entropy regularization},
	url = {https://proceedings.mlr.press/v180/ahmed22a.html},
	abstract = {In structured output prediction, the goal is to jointly predict several output variables that together encode a structured object – a path in a graph, an entity-relation triple, or an ordering of objects. Such a large output space makes learning hard and requires vast amounts of labeled data. Different approaches leverage alternate sources of supervision. One approach – entropy regularization – posits that decision boundaries should lie in low-probability regions. It extracts supervision from unlabeled examples, but remains agnostic to the structure of the output space. Conversely, neuro-symbolic approaches exploit the knowledge that not every prediction corresponds to a valid structure in the output space. Yet, they do not further restrict the learned output distribution.This paper introduces a framework that unifies both approaches. We propose a loss, neuro-symbolic entropy regularization, that encourages the model to confidently predict a valid object. It is obtained by restricting entropy regularization to the distribution over only the valid structures. This loss can be computed efficiently when the output constraint is expressed as a tractable logic circuit. Moreover, it seamlessly integrates with other neuro-symbolic losses that eliminate invalid predictions. We demonstrate the efficacy of our approach on a series of semi-supervised and fully-supervised structured-prediction experiments, where it leads to models whose predictions are more accurate as well as more likely to be valid.},
	language = {en},
	urldate = {2024-01-19},
	booktitle = {Proceedings of the {Thirty}-{Eighth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Ahmed, Kareem and Wang, Eric and Chang, Kai-Wei and Van den Broeck, Guy},
	month = aug,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {ucla},
	pages = {43--53},
}

@inproceedings{ahmed_pseudo-semantic_2023,
	title = {A {Pseudo}-{Semantic} {Loss} for {Autoregressive} {Models} with {Logical} {Constraints}},
	url = {https://openreview.net/forum?id=hVAla2O73O},
	abstract = {Neuro-symbolic AI bridges the gap between purely symbolic and neural approaches to learning. This often requires maximizing the likelihood of a symbolic constraint w.r.t the neural network's output distribution. Such output distributions are typically assumed to be fully-factorized. This limits the applicability of neuro-symbolic learning to the more expressive auto-regressive distributions, e.g., transformers. Under such distributions, computing the likelihood of even simple constraints is \#P-hard. Instead of attempting to enforce the constraint on the entire likelihood distribution, we propose to do so on a random, local approximation thereof. More precisely, we approximate the likelihood of the constraint with the pseudolikelihood of the constraint centered around a model sample. Our approach is factorizable, allowing us to reuse solutions to sub-problems---a main tenet for the efficient computation of neuro-symbolic losses. It also provides a local, high fidelity approximation of the likelihood: it exhibits low entropy and KL-divergence around the model sample. We tested our approach on Sudoku and shortest-path prediction cast as auto-regressive generation, and observe that we greatly improve upon the base model's ability to predict logically-consistent outputs. We also tested our approach on the task of detoxifying large language models. We observe that using a simple constraint disallowing a list of toxic words, we are able to steer the model's outputs away from toxic generations, achieving SoTA compared to previous approaches.},
	language = {en},
	urldate = {2023-11-07},
	author = {Ahmed, Kareem and Chang, Kai-Wei and Van den Broeck, Guy},
	month = nov,
	year = {2023},
	keywords = {ucla},
}

@inproceedings{moldovan_mcmc_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{MCMC} {Estimation} of {Conditional} {Probabilities} in {Probabilistic} {Programming} {Languages}},
	isbn = {978-3-642-39091-3},
	doi = {10.1007/978-3-642-39091-3_37},
	abstract = {Probabilistic logic programming languages are powerful formalisms that can model complex problems where it is necessary to represent both structure and uncertainty. Using exact inference methods to compute conditional probabilities in these languages is often intractable so approximate inference techniques are necessary. This paper proposes a Markov Chain Monte Carlo algorithm for estimating conditional probabilities based on sampling from an AND/OR tree for ProbLog, a general-purpose probabilistic logic programming language. We propose a parameterizable proposal distribution that generates the next sample in the Markov chain by probabilistically traversing the AND/OR tree from its root, which holds the evidence, to the leaves. An empirical evaluation on several different applications illustrates the advantages of our algorithm.},
	language = {en},
	booktitle = {Symbolic and {Quantitative} {Approaches} to {Reasoning} with {Uncertainty}},
	publisher = {Springer},
	author = {Moldovan, Bogdan and Thon, Ingo and Davis, Jesse and De Raedt, Luc},
	editor = {van der Gaag, Linda C.},
	year = {2013},
	keywords = {Empty Clause, Markov Chain Monte Carlo, Markov Chain Monte Carlo Algorithm, Markov Chain Monte Carlo Approach, Solution Tree, leuven},
	pages = {436--448},
}

@article{renkens_explanation-based_2014,
	title = {Explanation-{Based} {Approximate} {Weighted} {Model} {Counting} for {Probabilistic} {Logics}},
	volume = {28},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/9067},
	doi = {10.1609/aaai.v28i1.9067},
	abstract = {Probabilistic inference can be realized using weighted model counting. Despite a lot of progress, computing weighted model counts exactly is still infeasible for many problems of interest, and one typically has to resort to approximation methods. We contribute a new bounded approximation method for weighted model counting based on probabilistic logic programming principles. Our bounded approximation algorithm is an anytime algorithm that provides lower and upper bounds on the weighted model count. An empirical evaluation on probabilistic logic programs shows that our approach is effective in many cases that are currently beyond the reach of exact methods.},
	language = {en},
	number = {1},
	urldate = {2024-01-05},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Renkens, Joris and Kimmig, Angelika and Van den Broeck, Guy and De Raedt, Luc},
	month = jun,
	year = {2014},
	note = {Number: 1},
	keywords = {Weighted Model Counting, leuven},
}

@article{winters_deepstochlog_2022,
	title = {{DeepStochLog}: {Neural} {Stochastic} {Logic} {Programming}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{DeepStochLog}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21248},
	doi = {10.1609/aaai.v36i9.21248},
	abstract = {Recent advances in neural-symbolic learning, such as DeepProbLog, extend probabilistic logic programs with neural predicates. Like graphical models, these probabilistic logic programs define a probability distribution over possible worlds, for which inference is computationally hard. We propose DeepStochLog, an alternative neural-symbolic framework based on stochastic definite clause grammars, a kind of stochastic logic program. More specifically, we introduce neural grammar rules into stochastic definite clause grammars to create a framework that can be trained end-to-end. We show that inference and learning in neural stochastic logic programming scale much better than for neural probabilistic logic programs. Furthermore, the experimental evaluation shows that DeepStochLog achieves state-of-the-art results on challenging neural-symbolic learning tasks.},
	language = {en},
	number = {9},
	urldate = {2023-03-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Winters, Thomas and Marra, Giuseppe and Manhaeve, Robin and De Raedt, Luc},
	month = jun,
	year = {2022},
	note = {Number: 9},
	keywords = {Machine Learning (ML), leuven},
	pages = {10090--10100},
}

@inproceedings{niepert_implicit_2021,
	title = {Implicit {MLE}: {Backpropagating} {Through} {Discrete} {Exponential} {Family} {Distributions}},
	volume = {34},
	shorttitle = {I-{MLE}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/7a430339c10c642c4b2251756fd1b484-Abstract.html},
	abstract = {Combining discrete probability distributions and combinatorial optimization problems with neural network components has numerous applications but poses several challenges. We propose Implicit Maximum Likelihood Estimation (I-MLE), a framework for end-to-end learning of models combining discrete exponential family distributions and differentiable neural components. I-MLE is widely applicable as it only requires the ability to compute the most probable states and does not rely on smooth relaxations. The framework encompasses several approaches such as perturbation-based implicit differentiation and recent methods to differentiate through black-box combinatorial solvers. We introduce a novel class of noise distributions for approximating marginals via perturb-and-MAP. Moreover, we show that I-MLE simplifies to maximum likelihood estimation when used in some recently studied learning settings that involve combinatorial solvers. Experiments on several datasets suggest that I-MLE is competitive with and often outperforms existing approaches which rely on problem-specific relaxations.},
	urldate = {2024-01-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Niepert, Mathias and Minervini, Pasquale and Franceschi, Luca},
	year = {2021},
	keywords = {todo},
	pages = {14567--14579},
}

@misc{minervini_towards_2018,
	title = {Towards {Neural} {Theorem} {Proving} at {Scale}},
	url = {http://arxiv.org/abs/1807.08204},
	doi = {10.48550/arXiv.1807.08204},
	abstract = {Neural models combining representation learning and reasoning in an end-to-end trainable manner are receiving increasing interest. However, their use is severely limited by their computational complexity, which renders them unusable on real world datasets. We focus on the Neural Theorem Prover (NTP) model proposed by Rockt\{{\textbackslash}"\{a\}\}schel and Riedel (2017), a continuous relaxation of the Prolog backward chaining algorithm where unification between terms is replaced by the similarity between their embedding representations. For answering a given query, this model needs to consider all possible proof paths, and then aggregate results - this quickly becomes infeasible even for small Knowledge Bases (KBs). We observe that we can accurately approximate the inference process in this model by considering only proof paths associated with the highest proof scores. This enables inference and learning on previously impracticable KBs.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Minervini, Pasquale and Bosnjak, Matko and Rocktäschel, Tim and Riedel, Sebastian},
	month = jul,
	year = {2018},
	note = {arXiv:1807.08204 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, edinburgh},
}

@misc{arakelyan_complex_2021,
	title = {Complex {Query} {Answering} with {Neural} {Link} {Predictors}},
	url = {http://arxiv.org/abs/2011.03459},
	abstract = {Neural link predictors are immensely useful for identifying missing edges in large scale Knowledge Graphs. However, it is still not clear how to use these models for answering more complex queries that arise in a number of domains, such as queries using logical conjunctions (∧), disjunctions (∨) and existential quantiﬁers (∃), while accounting for missing edges. In this work, we propose a framework for efﬁciently answering complex queries on incomplete Knowledge Graphs. We translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. We then analyse two solutions to the optimisation problem, including gradient-based and combinatorial search. In our experiments, the proposed approach produces more accurate results than state-of-the-art methods — black-box neural models trained on millions of generated queries — without the need of training on a large and diverse set of complex queries. Using orders of magnitude less training data, we obtain relative improvements ranging from 8\% up to 40\% in Hits@3 across different knowledge graphs containing factual information. Finally, we demonstrate that it is possible to explain the outcome of our model in terms of the intermediate solutions identiﬁed for each of the complex query atoms. All our source code and datasets are available online 1.},
	language = {en},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Arakelyan, Erik and Daza, Daniel and Minervini, Pasquale and Cochez, Michael},
	month = mar,
	year = {2021},
	note = {arXiv:2011.03459 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, edinburgh},
}

@inproceedings{arakelyan_adapting_2023,
	title = {Adapting {Neural} {Link} {Predictors} for {Data}-{Efficient} {Complex} {Query} {Answering}},
	url = {https://openreview.net/forum?id=1G7CBp8o7L&referrer=%5Bthe%20profile%20of%20Isabelle%20Augenstein%5D(%2Fprofile%3Fid%3D~Isabelle_Augenstein1)},
	abstract = {Answering complex queries on incomplete knowledge graphs is a challenging task where a model needs to answer complex logical queries in the presence of missing knowledge. Prior work in the literature has proposed to address this problem by designing architectures trained end-to-end for the complex query answering task with a reasoning process that is hard to interpret while requiring data and resource-intensive training. Other lines of research have proposed re-using simple neural link predictors to answer complex queries, reducing the amount of training data by orders of magnitude while providing interpretable answers. The neural link predictor used in such approaches is not explicitly optimised for the complex query answering task, implying that its scores are not calibrated to interact together. We propose to address these problems via CQD\${\textasciicircum}\{{\textbackslash}mathcal\{A\}\}\$, a parameter-efficient score {\textbackslash}emph\{adaptation\} model optimised to re-calibrate neural link prediction scores for the complex query answering task. While the neural link predictor is frozen, the adaptation component -- which only increases the number of model parameters by \$0.03{\textbackslash}\%\$ -- is trained on the downstream complex query answering task. Furthermore, the calibration component enables us to support reasoning over queries that include atomic negations, which was previously impossible with link predictors. In our experiments, CQD\${\textasciicircum}\{{\textbackslash}mathcal\{A\}\}\$ produces significantly more accurate results than current state-of-the-art methods, improving from \$34.4\$ to \$35.1\$ Mean Reciprocal Rank values averaged across all datasets and query types while using \${\textbackslash}leq 30{\textbackslash}\%\$ of the available training query types. We further show that CQD\${\textasciicircum}\{{\textbackslash}mathcal\{A\}\}\$ is data-efficient, achieving competitive results with only \$1{\textbackslash}\%\$ of the complex training queries and robust in out-of-domain evaluations. Source code and datasets are available at https://github.com/EdinburghNLP/adaptive-cqd.},
	language = {en},
	urldate = {2024-02-09},
	author = {Arakelyan, Erik and Minervini, Pasquale and Daza, Daniel and Cochez, Michael and Augenstein, Isabelle},
	month = nov,
	year = {2023},
	keywords = {edinburgh},
}

@article{chen_fuzzy_2022,
	title = {Fuzzy {Logic} {Based} {Logical} {Query} {Answering} on {Knowledge} {Graphs}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20310},
	doi = {10.1609/aaai.v36i4.20310},
	abstract = {Answering complex First-Order Logical (FOL) queries on large-scale incomplete knowledge graphs (KGs) is an important yet challenging task. Recent advances embed logical queries and KG entities in the same space and conduct query answering via dense similarity search. However, most logical operators designed in previous studies do not satisfy the axiomatic system of classical logic, limiting their performance. Moreover, these logical operators are parameterized and thus require many complex FOL queries as training data, which are often arduous to collect or even inaccessible in most real-world KGs. We thus present FuzzQE, a fuzzy logic based logical query embedding framework for answering FOL queries over KGs. FuzzQE follows fuzzy logic to define logical operators in a principled and learning-free manner, where only entity and relation embeddings require learning. FuzzQE can further benefit from labeled complex logical queries for training. Extensive experiments on two benchmark datasets demonstrate that FuzzQE provides significantly better performance in answering FOL queries compared to state-of-the-art methods. In addition, FuzzQE trained with only KG link prediction can achieve comparable performance to those trained with extra complex query data.},
	language = {en},
	number = {4},
	urldate = {2024-02-13},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Xuelu and Hu, Ziniu and Sun, Yizhou},
	month = jun,
	year = {2022},
	note = {Number: 4},
	keywords = {Machine Learning (ML)},
	pages = {3939--3948},
}

@misc{dumancic_comparative_2020,
	title = {A {Comparative} {Study} of {Distributional} and {Symbolic} {Paradigms} for {Relational} {Learning}},
	url = {http://arxiv.org/abs/1806.11391},
	abstract = {This is a corrected version of the original paper, which contained a mistake in evaluation of symbolic models on the KBC tasks. The mistake was identiﬁed with the help of Manuel Fink, Christian Meilicke and Melisachew Wudage Checkol (University of Mannheim) Many real-world domains can be expressed as graphs and, more generally, as multi-relational knowledge graphs. Though reasoning and learning with knowledge graphs has traditionally been addressed by symbolic approaches such as Statistical relational learning, recent methods in (deep) representation learning have shown promising results for specialised tasks such as knowledge base completion. These approaches, also known as distributional, abandon the traditional symbolic paradigm by replacing symbols with vectors in Euclidean space. With few exceptions, symbolic and distributional approaches are explored in different communities and little is known about their respective strengths and weaknesses. In this work, we compare distributional and symbolic relational learning approaches on various standard relational classiﬁcation and knowledge base completion tasks. Furthermore, we analyse the complexity of the rules used implicitly by these approaches and relate them to the performance of the methods in the comparison. The results reveal possible indicators that could help in choosing one approach over the other for particular knowledge graphs.},
	language = {en},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Dumancic, Sebastijan and Garcia-Duran, Alberto and Niepert, Mathias},
	month = mar,
	year = {2020},
	note = {arXiv:1806.11391 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{dinu_symbolicai_2024,
	title = {{SymbolicAI}: {A} framework for logic-based approaches combining generative models and solvers},
	shorttitle = {{SymbolicAI}},
	url = {http://arxiv.org/abs/2402.00854},
	abstract = {We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. In turn, the framework facilitates the creation and evaluation of explainable computational graphs. We conclude by introducing a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the ”Vector Embedding for Relational Trajectory Evaluation through Cross-similarity”, or VERTEX score for short. The framework codebase1 and benchmark2 are linked below.},
	language = {en},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Dinu, Marius-Constantin and Leoveanu-Condrei, Claudiu and Holzleitner, Markus and Zellinger, Werner and Hochreiter, Sepp},
	month = feb,
	year = {2024},
	note = {arXiv:2402.00854 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Symbolic Computation},
}

@article{dagum_approximating_1993,
	title = {Approximating probabilistic inference in {Bayesian} belief networks is {NP}-hard},
	volume = {60},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/000437029390036B},
	doi = {10.1016/0004-3702(93)90036-B},
	abstract = {It is known that exact computation of conditional probabilities in belief networks is NP-hard. Many investigators in the AI community have tacitly assumed that algorithms for performing approximate inference with belief networks are of polynomial complexity. Indeed, special cases of approximate inference can be performed in time polynomial in the input size. However, we have discovered that the general problem of approximating conditional probabilities with belief networks, like exact inference, resides in the NP-hard complexity class. We develop a complexity analysis to elucidate the difficulty of approximate probabilistic inference. More specifically, we show that the existence of a polynomial-time relative approximation algorithm for major classes of problem instances implies that NP ⊆ P. We present our proof and explore the implications of the result.},
	number = {1},
	urldate = {2024-02-02},
	journal = {Artificial Intelligence},
	author = {Dagum, Paul and Luby, Michael},
	month = mar,
	year = {1993},
	pages = {141--153},
}

@inproceedings{karp_monte-carlo_1983,
	title = {Monte-{Carlo} algorithms for enumeration and reliability problems},
	isbn = {978-0-8186-0508-6},
	url = {https://www.computer.org/csdl/proceedings-article/focs/1983/542800056/12OmNxwncwa},
	doi = {10.1109/SFCS.1983.35},
	language = {English},
	urldate = {2024-02-02},
	publisher = {IEEE Computer Society},
	author = {Karp, Richard M. and Luby, Michael},
	month = nov,
	year = {1983},
	note = {ISSN: 0272-5428},
	pages = {56--64},
}

@article{bengio_gflownet_2023,
	title = {{GFlowNet} {Foundations}},
	volume = {24},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v24/22-0364.html},
	abstract = {Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets, including a new local and efficient training objective called detailed balance for the analogy with MCMC. GFlowNets can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, continuous actions and modular energy functions.},
	number = {210},
	urldate = {2024-02-02},
	journal = {Journal of Machine Learning Research},
	author = {Bengio, Yoshua and Lahlou, Salem and Deleu, Tristan and Hu, Edward J. and Tiwari, Mo and Bengio, Emmanuel},
	year = {2023},
	pages = {1--55},
}

@article{jerrum_random_1986,
	title = {Random generation of combinatorial structures from a uniform distribution},
	volume = {43},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/030439758690174X},
	doi = {10.1016/0304-3975(86)90174-X},
	abstract = {The class of problems involving the random generation of combinatorial structures from a uniform distribution is considered. Uniform generation problems are, in computational difficulty, intermediate between classical existence and counting problems. It is shown that exactly uniform generation of ‘efficiently verifiable’ combinatorial structures is reducible to approximate counting (and hence, is within the third level of the polynomial hierarchy). Natural combinatorial problems are presented which exhibit complexity gaps between their existence and generation, and between their generation and counting versions. It is further shown that for self-reducible problems, almost uniform generation and randomized approximate counting are inter-reducible, and hence, of similar complexity.},
	urldate = {2024-02-01},
	journal = {Theoretical Computer Science},
	author = {Jerrum, Mark R. and Valiant, Leslie G. and Vazirani, Vijay V.},
	month = jan,
	year = {1986},
	keywords = {F.1.1, F.1.3, G.2.1, G.3},
	pages = {169--188},
}

@article{chakraborty_hardness_2019,
	title = {On the {Hardness} of {Probabilistic} {Inference} {Relaxations}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4775},
	doi = {10.1609/aaai.v33i01.33017785},
	abstract = {A promising approach to probabilistic inference that has attracted recent attention exploits its reduction to a set of model counting queries. Since probabilistic inference and model counting are \#P-hard, various relaxations are used in practice, with the hope that these relaxations allow efficient computation while also providing rigorous approximation guarantees.
In this paper, we show that contrary to common belief, several relaxations used for model counting and its applications (including probablistic inference) do not really lead to computational efficiency in a complexity theoretic sense. Our arguments proceed by showing the corresponding relaxed notions of counting to be computationally hard. We argue that approximate counting with multiplicative tolerance and probabilistic guarantees of correctness is the only class of relaxations that provably simplifies the problem, given access to an NP-oracle. Finally, we show that for applications that compare probability estimates with a threshold, a new notion of relaxation with gaps between low and high thresholds can be used. This new relaxation allows efficient decision making in practice, given access to an NP-oracle, while also bounding the approximation error.
Erratum: This research is supported in part by the National Research Foundation Singapore under its AI Singapore Programme (Award Number: [AISG-RP-2018-005])},
	language = {en},
	number = {01},
	urldate = {2024-02-01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chakraborty, Supratik and Meel, Kuldeep S. and Vardi, Moshe Y.},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {7785--7792},
}

@article{fichte_model_2021,
	title = {The {Model} {Counting} {Competition} 2020},
	volume = {26},
	issn = {1084-6654},
	url = {https://dl.acm.org/doi/10.1145/3459080},
	doi = {10.1145/3459080},
	abstract = {Many computational problems in modern society account to probabilistic reasoning, statistics, and combinatorics. A variety of these real-world questions can be solved by representing the question in (Boolean) formulas and associating the number of models of the formula directly with the answer to the question. Since there has been an increasing interest in practical problem solving for model counting over the past years, the Model Counting Competition was conceived in fall 2019. The competition aims to foster applications, identify new challenging benchmarks, and promote new solvers and improve established solvers for the model counting problem and versions thereof. We hope that the results can be a good indicator of the current feasibility of model counting and spark many new applications. In this article, we report on details of the Model Counting Competition 2020, about carrying out the competition, and the results. The competition encompassed three versions of the model counting problem, which we evaluated in separate tracks. The first track featured the model counting problem, which asks for the number of models of a given Boolean formula. On the second track, we challenged developers to submit programs that solve the weighted model counting problem. The last track was dedicated to projected model counting. In total, we received a surprising number of nine solvers in 34 versions from eight groups.},
	urldate = {2024-02-01},
	journal = {ACM Journal of Experimental Algorithmics},
	author = {Fichte, Johannes K. and Hecher, Markus and Hamiti, Florim},
	year = {2021},
	keywords = {Datasets, benchmarking, competition, instance selection, model counting, projected model counting, satisfiability, system comparison, weighted model counting},
	pages = {13:1--13:26},
}

@inproceedings{chakraborty_algorithmic_2016,
	address = {New York, New York, USA},
	series = {{IJCAI}'16},
	title = {Algorithmic improvements in approximate counting for probabilistic inference: from linear to logarithmic {SAT} calls},
	isbn = {978-1-57735-770-4},
	shorttitle = {Algorithmic improvements in approximate counting for probabilistic inference},
	abstract = {Probabilistic inference via model counting has emerged as a scalable technique with strong formal guarantees, thanks to recent advances in hashing-based approximate counting. State-of-the-art hashing-based counting algorithms use an NP oracle (SAT solver in practice), such that the number of oracle invocations grows linearly in the number of variables n in the input constraint. We present a new approach to hashing-based approximate model counting in which the number of oracle invocations grows logarithmically in n , while still providing strong theoretical guarantees. We use this technique to design an algorithm for \#CNF with strongly probably approximately correct (SPAC) guarantees, i.e. PAC guarantee plus expected return value matching the exact model count. Our experiments show that this algorithm outperforms state-of-the-art techniques for approximate counting by 1-2 orders of magnitude in running time. We also show that our algorithm can be easily adapted to give a new fully polynomial randomized approximation scheme (FPRAS) for \#DNF.},
	urldate = {2024-01-30},
	booktitle = {Proceedings of the {Twenty}-{Fifth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Chakraborty, Supratik and Meel, Kuldeep S. and Vardi, Moshe Y.},
	month = jul,
	year = {2016},
	pages = {3569--3576},
}

@misc{chen_symbolic_2023,
	title = {Symbolic {Discovery} of {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/2302.06675},
	doi = {10.48550/arXiv.2302.06675},
	abstract = {We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, \${\textbackslash}textbf\{Lion\}\$ (\${\textbackslash}textit\{Evo\${\textbackslash}textbf\{L\}\$ved S\${\textbackslash}textbf\{i\}\$gn M\${\textbackslash}textbf\{o\}\$me\${\textbackslash}textbf\{n\}\$tum\}\$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2\% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3\% \${\textbackslash}textit\{zero-shot\}\$ and 91.1\% \${\textbackslash}textit\{fine-tuning\}\$ accuracy on ImageNet, surpassing the previous best results by 2\% and 0.1\%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.},
	urldate = {2024-01-29},
	publisher = {arXiv},
	author = {Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and Le, Quoc V.},
	month = may,
	year = {2023},
	note = {arXiv:2302.06675 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{bulatov_complexity_2013,
	title = {The complexity of the counting constraint satisfaction problem},
	volume = {60},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/2528400},
	doi = {10.1145/2528400},
	abstract = {The Counting Constraint Satisfaction Problem (\#CSP(H)) over a finite relational structure H can be expressed as follows: given a relational structure G over the same vocabulary, determine the number of homomorphisms from G to H. In this article we characterize relational structures H for which (\#CSP(H) can be solved in polynomial time and prove that for all other structures the problem is \#P-complete.},
	number = {5},
	urldate = {2024-01-28},
	journal = {Journal of the ACM},
	author = {Bulatov, Andrei A.},
	year = {2013},
	keywords = {Constraint Satisfaction problem, complexity, counting problems, dichotomy theorem, homomorphism problem, todo},
	pages = {34:1--34:41},
}

@inproceedings{delfosse_interpretable_2023,
	title = {Interpretable and {Explainable} {Logical} {Policies} via {Neurally} {Guided} {Symbolic} {Abstraction}},
	url = {https://openreview.net/forum?id=PbMBfRpVgU},
	abstract = {The limited priors required by neural networks make them the dominating choice to encode and learn policies using reinforcement learning (RL). However, they are also black-boxes, making it hard to understand the agent's behavior, especially when working on the image level. Therefore, neuro-symbolic RL aims at creating policies that are interpretable in the first place. Unfortunately, interpretability is not explainability. To achieve both, we introduce Neurally gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural network-based agents to guide the search of candidate-weighted logic rules, then uses differentiable logic to train the logic agents. Our experimental evaluation demonstrates that NUDGE agents can induce interpretable and explainable policies while outperforming purely neural ones and showing good flexibility to environments of different initial states and problem sizes.},
	language = {en},
	urldate = {2023-11-28},
	author = {Delfosse, Quentin and Shindo, Hikaru and Dhami, Devendra Singh and Kersting, Kristian},
	month = nov,
	year = {2023},
	keywords = {darmstadt},
}

@article{shindo_alphailp_2023,
	title = {\$\${\textbackslash}alpha\$\${ILP}: thinking visual scenes as differentiable logic programs},
	volume = {112},
	issn = {1573-0565},
	shorttitle = {α{ILP}},
	url = {https://doi.org/10.1007/s10994-023-06320-1},
	doi = {10.1007/s10994-023-06320-1},
	abstract = {Deep neural learning has shown remarkable performance at learning representations for visual object categorization. However, deep neural networks such as CNNs do not explicitly encode objects and relations among them. This limits their success on tasks that require a deep logical understanding of visual scenes, such as Kandinsky patterns and Bongard problems. To overcome these limitations, we introduce \$\${\textbackslash}alpha \{{\textbackslash}textit\{ILP\}\}\$\$, a novel differentiable inductive logic programming framework that learns to represent scenes as logic programs—intuitively, logical atoms correspond to objects, attributes, and relations, and clauses encode high-level scene information. \$\${\textbackslash}alpha\$\$ILP has an end-to-end reasoning architecture from visual inputs. Using it, \$\${\textbackslash}alpha\$\$ILP performs differentiable inductive logic programming on complex visual scenes, i.e., the logical rules are learned by gradient descent. Our extensive experiments on Kandinsky patterns and CLEVR-Hans benchmarks demonstrate the accuracy and efficiency of \$\${\textbackslash}alpha \{{\textbackslash}textit\{ILP\}\}\$\$in learning complex visual-logical concepts.},
	language = {en},
	number = {5},
	urldate = {2024-01-27},
	journal = {Machine Learning},
	author = {Shindo, Hikaru and Pfanschilling, Viktor and Dhami, Devendra Singh and Kersting, Kristian},
	month = may,
	year = {2023},
	keywords = {Differentiable reasoning, Inductive logic programming, Neuro-symbolic AI, Object-centric learning, darmstadt},
	pages = {1465--1497},
}

@inproceedings{yang_differentiable_2017,
	title = {Differentiable {Learning} of {Logical} {Rules} for {Knowledge} {Base} {Reasoning}},
	volume = {30},
	shorttitle = {{NeuralLP}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/0e55666a4ad822e0e34299df3591d979-Abstract.html},
	abstract = {We study the problem of learning probabilistic first-order logical rules for knowledge base reasoning. This learning problem is difficult because it requires learning the parameters in a continuous space as well as the structure in a discrete space. We propose a framework, Neural Logic Programming, that combines the parameter and structure learning of first-order logical rules in an end-to-end differentiable model. This approach is inspired by a recently-developed differentiable logic called TensorLog [5], where inference tasks can be compiled into sequences of differentiable operations. We design a neural controller system that learns to compose these operations. Empirically, our method outperforms prior work on multiple knowledge base benchmark datasets, including Freebase and WikiMovies.},
	urldate = {2022-10-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Fan and Yang, Zhilin and Cohen, William W},
	year = {2017},
}

@article{evans_learning_2018,
	title = {Learning {Explanatory} {Rules} from {Noisy} {Data}},
	volume = {61},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {δ{ILP}},
	url = {https://www.jair.org/index.php/jair/article/view/11172},
	doi = {10.1613/jair.5714},
	abstract = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data--which is not necessarily easily obtained--that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.},
	language = {en},
	urldate = {2022-10-04},
	journal = {Journal of Artificial Intelligence Research},
	author = {Evans, Richard and Grefenstette, Edward},
	month = jan,
	year = {2018},
	keywords = {Computer Science - Neural and Evolutionary Computing, Mathematics - Logic},
	pages = {1--64},
}

@article{sadeghian_drum_nodate,
	title = {{DRUM}: {End}-{To}-{End} {Differentiable} {Rule} {Mining} {On} {Knowledge} {Graphs}},
	shorttitle = {{DRUM}},
	abstract = {In this paper, we study the problem of learning probabilistic logical rules for inductive and interpretable link prediction. Despite the importance of inductive link prediction, most previous works focused on transductive link prediction and cannot manage previously unseen entities. Moreover, they are black-box models that are not easily explainable for humans. We propose DRUM, a scalable and differentiable approach for mining ﬁrst-order logical rules from knowledge graphs which resolves these problems. We motivate our method by making a connection between learning conﬁdence scores for each rule and low-rank tensor approximation. DRUM uses bidirectional RNNs to share useful information across the tasks of learning rules for different relations. We also empirically demonstrate the efﬁciency of DRUM over existing rule mining methods for inductive link prediction on a variety of benchmark datasets.},
	language = {en},
	author = {Sadeghian, Ali and Armandpour, Mohammadreza and Ding, Patrick and Wang, Daisy Zhe},
}

@article{shindo_differentiable_2021,
	title = {Differentiable {Inductive} {Logic} {Programming} for {Structured} {Examples}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16637},
	doi = {10.1609/aaai.v35i6.16637},
	abstract = {The differentiable implementation of logic yields a seamless combination of symbolic reasoning and deep neural networks. Recent research, which has developed a differentiable framework to learn logic programs from examples, can even acquire reasonable solutions from noisy datasets. However, this framework severely limits expressions for solutions, e.g., no function symbols are allowed, and the shapes of clauses are fixed. As a result, the framework cannot deal with structured examples. Therefore we propose a new framework to learn logic programs from noisy and structured examples, including the following contributions. First, we propose an adaptive clause search method by looking through structured space, which is defined by the generality of the clauses, to yield an efficient search space for differentiable solvers. Second, we propose for ground atoms an enumeration algorithm, which determines a necessary and sufficient set of ground atoms to perform differentiable inference functions. Finally, we propose a new method to compose logic programs softly, enabling the system to deal with complex programs consisting of several clauses. Our experiments show that our new framework can learn logic programs from noisy and structured examples, such as sequences or trees. Our framework can be scaled to deal with complex programs that consist of several clauses with function symbols.},
	language = {en},
	number = {6},
	urldate = {2024-01-27},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Shindo, Hikaru and Nishino, Masaaki and Yamamoto, Akihiro},
	month = may,
	year = {2021},
	note = {Number: 6},
	keywords = {Logic Programming},
	pages = {5034--5041},
}

@article{garcez_neural-symbolic_2019,
	title = {Neural-{Symbolic} {Computing}: {An} {Effective} {Methodology} for {Principled} {Integration} of {Machine} {Learning} and {Reasoning}},
	shorttitle = {Neural-{Symbolic} {Computing}},
	url = {https://www.semanticscholar.org/paper/Neural-Symbolic-Computing%3A-An-Effective-Methodology-Garcez-Gori/833c4ac0599f4b8c5f1ee6ea948ec675fbe56b15},
	abstract = {Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.},
	urldate = {2024-01-26},
	journal = {FLAP},
	author = {Garcez, A. and Gori, M. and Lamb, L. and Serafini, L. and Spranger, Michael and Tran, S.},
	month = may,
	year = {2019},
}

@misc{dudek_parallel_2021,
	title = {Parallel {Weighted} {Model} {Counting} with {Tensor} {Networks}},
	url = {http://arxiv.org/abs/2006.15512},
	abstract = {A promising new algebraic approach to weighted model counting makes use of tensor networks, following a reduction from weighted model counting to tensor-network contraction. Prior work has focused on analyzing the single-core performance of this approach, and demonstrated that it is an eﬀective addition to the current portfolio of weighted-modelcounting algorithms.},
	language = {en},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Dudek, Jeffrey M. and Vardi, Moshe Y.},
	month = jun,
	year = {2021},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@article{giunchiglia_road-r_2023,
	title = {{ROAD}-{R}: {The} {Autonomous} {Driving} {Dataset} with {Logical} {Requirements}},
	issn = {0885-6125, 1573-0565},
	shorttitle = {{ROAD}-{R}},
	doi = {10.1007/s10994-023-06322-z},
	abstract = {Neural networks have proven to be very powerful at computer vision tasks. However, they often exhibit unexpected behaviours, violating known requirements expressing background knowledge. This calls for models (i) able to learn from the requirements, and (ii) guaranteed to be compliant with the requirements themselves. Unfortunately, the development of such models is hampered by the lack of datasets equipped with formally speciﬁed requirements. In this paper, we introduce the ROad event Awareness Dataset with logical Requirements (ROAD-R), the ﬁrst publicly available dataset for autonomous driving with requirements expressed as logical constraints. Given ROAD-R, we show that current state-of-the-art models often violate its logical constraints, and that it is possible to exploit them to create models that (i) have a better performance, and (ii) are guaranteed to be compliant with the requirements themselves.},
	language = {en},
	urldate = {2023-07-03},
	journal = {Machine Learning},
	author = {Giunchiglia, Eleonora and Stoian, Mihaela Cătălina and Khan, Salman and Cuzzolin, Fabio and Lukasiewicz, Thomas},
	month = may,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{allen_probabilistic_2021,
	title = {A {Probabilistic} {Model} for {Discriminative} and {Neuro}-{Symbolic} {Semi}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2006.05896},
	abstract = {Much progress has been made in semi-supervised learning (SSL) by combining methods that exploit different aspects of the data distribution, e.g. consistency regularisation relies on properties of p(x), whereas entropy minimisation pertains to the label distribution p(y{\textbar}x). Focusing on the latter, we present a probabilistic model for discriminative SSL, that mirrors its classical generative counterpart. Under the assumption y{\textbar}x is deterministic, the prior over latent variables becomes discrete. We show that several well-known SSL methods can be interpreted as approximating this prior, and can be improved upon. We extend the discriminative model to neuro-symbolic SSL, where label features satisfy logical rules, by showing such rules relate directly to the above prior, thus justifying a family of methods that link statistical learning and logical reasoning, and unifying them with regular SSL.},
	language = {en},
	urldate = {2024-01-26},
	publisher = {arXiv},
	author = {Allen, Carl and Balažević, Ivana and Hospedales, Timothy},
	month = may,
	year = {2021},
	note = {arXiv:2006.05896 [cs, stat]},
	keywords = {Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{vermeulen_experimental_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {An {Experimental} {Overview} of {Neural}-{Symbolic} {Systems}},
	isbn = {978-3-031-49299-0},
	doi = {10.1007/978-3-031-49299-0_9},
	abstract = {Neural-symbolic AI is the field that seeks to integrate deep learning with symbolic, logic-based methods, as they have complementary strengths. Lately, more and more researchers have encountered the limitations of deep learning, which has led to a rise in the popularity of neural-symbolic AI, with a wide variety of systems being developed. However, many of these systems are either evaluated on different benchmarks, or introduce new benchmarks that other systems have not been tested on. As a result, it is unclear which systems are suited to which tasks, and whether the difference between systems is actually significant.},
	language = {en},
	booktitle = {Inductive {Logic} {Programming}},
	publisher = {Springer Nature Switzerland},
	author = {Vermeulen, Arne and Manhaeve, Robin and Marra, Giuseppe},
	editor = {Bellodi, Elena and Lisi, Francesca Alessandra and Zese, Riccardo},
	year = {2023},
	keywords = {Experimental Survey, Neural-symbolic AI, Probabilistic Logic Programming, leuven},
	pages = {124--138},
}

@misc{dickens_convex_2024,
	title = {Convex and {Bilevel} {Optimization} for {Neuro}-{Symbolic} {Inference} and {Learning}},
	url = {http://arxiv.org/abs/2401.09651},
	abstract = {We address a key challenge for neuro-symbolic (NeSy) systems by leveraging convex and bilevel optimization techniques to develop a general gradient-based framework for end-to-end neural and symbolic parameter learning. The applicability of our framework is demonstrated with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100× learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across 8 datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16\% point prediction performance improvement over alternative learning methods.},
	language = {en},
	urldate = {2024-01-23},
	publisher = {arXiv},
	author = {Dickens, Charles and Gao, Changyu and Pryor, Connor and Wright, Stephen and Getoor, Lise},
	month = jan,
	year = {2024},
	note = {arXiv:2401.09651 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@inproceedings{van_krieken_-nesi_2023,
	title = {A-{NeSI}: {A} {Scalable} {Approximate} {Method} for {Probabilistic} {Neurosymbolic} {Inference}},
	shorttitle = {A-{NeSI}},
	url = {https://openreview.net/forum?id=chlTA9Cegc},
	abstract = {We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in performance.},
	language = {en},
	urldate = {2024-01-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {van Krieken, Emile and Thanapalasingam, Thiviyan and Tomczak, Jakub M. and Harmelen, Frank Van and Teije, Annette Ten},
	month = nov,
	year = {2023},
}

@book{hitzler_neuro-symbolic_2022,
	title = {Neuro-{Symbolic} {Artificial} {Intelligence}: {The} {State} of the {Art}},
	isbn = {978-1-64368-245-7},
	shorttitle = {Neuro-{Symbolic} {Artificial} {Intelligence}},
	abstract = {Neuro-symbolic AI is an emerging subfield of Artificial Intelligence that brings together two hitherto distinct approaches. ”Neuro” refers to the artificial neural networks prominent in machine learning, ”symbolic” refers to algorithmic processing on the level of meaningful symbols, prominent in knowledge representation. In the past, these two fields of AI have been largely separate, with very little crossover, but the so-called “third wave” of AI is now bringing them together. This book, Neuro-Symbolic Artificial Intelligence: The State of the Art, provides an overview of this development in AI. The two approaches differ significantly in terms of their strengths and weaknesses and, from a cognitive-science perspective, there is a question as to how a neural system can perform symbol manipulation, and how the representational differences between these two approaches can be bridged. The book presents 17 overview papers, all by authors who have made significant contributions in the past few years and starting with a historic overview first seen in 2016. With just seven months elapsed from invitation to authors to final copy, the book is as up-to-date as a published overview of this subject can be. Based on the editors’ own desire to understand the current state of the art, this book reflects the breadth and depth of the latest developments in neuro-symbolic AI, and will be of interest to students, researchers, and all those working in the field of Artificial Intelligence.},
	language = {en},
	publisher = {IOS Press},
	author = {Hitzler, P.},
	month = jan,
	year = {2022},
	keywords = {Computers / Artificial Intelligence / General},
}

@inproceedings{jang_categorical_2016,
	title = {Categorical {Reparameterization} with {Gumbel}-{Softmax}},
	shorttitle = {Gumbel-{Softmax}},
	abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
	language = {en},
	urldate = {2024-01-07},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
	month = nov,
	year = {2016},
}

@inproceedings{de_smet_differentiable_2023,
	title = {Differentiable {Sampling} of {Categorical} {Distributions} {Using} the {CatLog}-{Derivative} {Trick}},
	shorttitle = {{IndeCateR}},
	url = {https://openreview.net/forum?id=AQyqxXctsN},
	abstract = {Categorical random variables can faithfully represent the discrete and uncertain aspects of data as part of a discrete latent variable model. Learning in such models necessitates taking gradients with respect to the parameters of the categorical probability distributions, which is often intractable due to their combinatorial nature. A popular technique to estimate these otherwise intractable gradients is the Log-Derivative trick. This trick forms the basis of the well-known REINFORCE gradient estimator and its many extensions. While the Log-Derivative trick allows us to differentiate through samples drawn from categorical distributions, it does not take into account the discrete nature of the distribution itself. Our first contribution addresses this shortcoming by introducing the CatLog-Derivative trick -- a variation of the Log-Derivative trick tailored towards categorical distributions. Secondly, we use the CatLog-Derivative trick to introduce IndeCateR, a novel and unbiased gradient estimator for the important case of products of independent categorical distributions with provably lower variance than REINFORCE. Thirdly, we empirically show that IndeCateR can be efficiently implemented and that its gradient estimates have significantly lower bias and variance for the same number of samples compared to the state of the art.},
	language = {en},
	urldate = {2023-12-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {De Smet, Lennert and Sansone, Emanuele and Zuidberg Dos Martires, Pedro},
	month = nov,
	year = {2023},
	keywords = {leuven},
}

@inproceedings{gribkoff_understanding_2014,
	address = {Arlington, Virginia, USA},
	series = {{UAI}'14},
	title = {Understanding the complexity of lifted inference and asymmetric {Weighted} {Model} {Counting}},
	isbn = {978-0-9749039-1-0},
	abstract = {In this paper we study lifted inference for the Weighted First-Order Model Counting problem (WFOMC), which counts the assignments that satisfy a given sentence in first-order logic (FOL); it has applications in Statistical Relational Learning (SRL) and Probabilistic Databases (PDB). We present several results. First, we describe a lifted inference algorithm that generalizes prior approaches in SRL and PDB. Second, we provide a novel dichotomy result for a non-trivial fragment of FO CNF sentences, showing that for each sentence the WFOMC problem is either in PTIME or \#P-hard in the size of the input domain; we prove that, in the first case our algorithm solves the WFOMC problem in PTIME, and in the second case it fails. Third, we present several properties of the algorithm. Finally, we discuss limitations of lifted inference for symmetric probabilistic databases (where the weights of ground literals depend only on the relation name, and not on the constants of the domain), and prove the impossibility of a dichotomy result for the complexity of probabilistic inference for the entire language FOL.},
	urldate = {2024-01-23},
	booktitle = {Proceedings of the {Thirtieth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {AUAI Press},
	author = {Gribkoff, Eric and Van den Broeck, Guy and Suciu, Dan},
	month = jul,
	year = {2014},
	pages = {280--289},
}

@inproceedings{hsu_whats_2023,
	title = {What’s {Left}? {Concept} {Grounding} with {Logic}-{Enhanced} {Foundation} {Models}},
	shorttitle = {What’s {Left}?},
	url = {https://openreview.net/forum?id=sq4o3tjWaj},
	abstract = {Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning—using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like “*left*” can also be grounded in 3D, temporal, and action data, as in moving to your *left*. This limited generalization stems from these inference-only methods’ inability to learn or adapt pre-trained models to a new domain. We propose the **L**ogic-**E**nhanced **F**ounda**T**ion Model (**LEFT**), a unified framework that *learns* to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT’s executor then executes the program with trainable domain-specific grounding modules. We show that LEFT flexibly learns concepts in four domains: 2D images, 3D scenes, human motions, and robotic manipulation. It exhibits strong reasoning ability in a wide variety of tasks, including those that are complex and not seen during training, and can be easily applied to new domains.},
	language = {en},
	urldate = {2023-11-28},
	author = {Hsu, Joy and Mao, Jiayuan and Tenenbaum, Joshua B. and Wu, Jiajun},
	month = nov,
	year = {2023},
}

@inproceedings{stockmeyer_complexity_1983,
	address = {New York, NY, USA},
	series = {{STOC} '83},
	title = {The complexity of approximate counting},
	isbn = {978-0-89791-099-6},
	url = {https://dl.acm.org/doi/10.1145/800061.808740},
	doi = {10.1145/800061.808740},
	abstract = {There are several computational problems that can be formulated as problems of counting the number of objects having a certain property. Valiant [22] has introduced the class \#P which includes a variety of counting problems such as counting the number of perfect matchings in a graph, computing the permanent of a matrix [22], finding the size of a backtrack search tree [14], and computing the probability that a network remains connected when links can fail with a certain probability [23]. We define and study a class of restricted, but very natural, probabilistic sampling methods motivated by the particular counting problems mentioned above. Instead of “singleton sampling” the algorithm is allowed to sample a large set S ample; U in one step; the information returned from the sample is whether S contains any element having the property being counted. We attempt to classify the complexity of computing approximate solutions to problems in \#P. The classification is done in terms of the polynomial-time hierarchy (for short, P-hierarchy) [21]. We give a relativization result that complements a recent result of Sipser and Gaacute;c [19] that BPP is contained in the second level of the P-hierarchy.},
	urldate = {2024-01-23},
	booktitle = {Proceedings of the fifteenth annual {ACM} symposium on {Theory} of computing},
	publisher = {Association for Computing Machinery},
	author = {Stockmeyer, Larry},
	month = dec,
	year = {1983},
	pages = {118--126},
}

@misc{galkin_towards_2023,
	title = {Towards {Foundation} {Models} for {Knowledge} {Graph} {Reasoning}},
	url = {http://arxiv.org/abs/2310.04562},
	doi = {10.48550/arXiv.2310.04562},
	abstract = {Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap. The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies. In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. ULTRA builds relational representations as a function conditioned on their interactions. Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph. Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. Fine-tuning further boosts the performance.},
	urldate = {2024-01-23},
	publisher = {arXiv},
	author = {Galkin, Mikhail and Yuan, Xinyu and Mostafa, Hesham and Tang, Jian and Zhu, Zhaocheng},
	month = oct,
	year = {2023},
	note = {arXiv:2310.04562 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{verreet_explain_2023,
	title = {{EXPLAIN}, {AGREE} and {LEARN}: {A} {Recipe} for {Scalable} {Neural}-{Symbolic} {Learning}},
	shorttitle = {{EXAL}},
	author = {Verreet, Victor and De Smet, Lennert and Sansone, Emanuele},
	year = {2023},
	keywords = {leuven},
}

@article{roth_hardness_1996,
	title = {On the hardness of approximate reasoning},
	volume = {82},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/0004370294000921},
	doi = {10.1016/0004-3702(94)00092-1},
	abstract = {Many AI problems, when formalized, reduce to evaluating the probability that a propositional expression is true. In this paper we show that this problem is computationally intractable even in surprisingly restricted cases and even if we settle for an approximation to this probability. We consider various methods used in approximate reasoning such as computing degree of belief and Bayesian belief networks, as well as reasoning techniques such as constraint satisfaction and knowledge compilation, that use approximation to avoid computational difficulties, and reduce them to model-counting problems over a propositional domain. We prove that counting satisfying assignments of propositional languages is intractable even for Horn and monotone formulae, and even when the size of clauses and number of occurrences of the variables are extremely limited. This should be contrasted with the case of deductive reasoning, where Horn theories and theories with binary clauses are distinguished by the existence of linear time satisfiability algorithms. What is even more surprising is that, as we show, even approximating the number of satisfying assignments (i.e., “approximating” approximate reasoning), is intractable for most of these restricted theories. We also identify some restricted classes of propositional formulae for which efficient algorithms for counting satisfying assignments can be given.},
	number = {1},
	urldate = {2024-01-21},
	journal = {Artificial Intelligence},
	author = {Roth, Dan},
	month = apr,
	year = {1996},
	pages = {273--302},
}

@article{valiant_complexity_1979,
	title = {The {Complexity} of {Enumeration} and {Reliability} {Problems}},
	volume = {8},
	issn = {0097-5397},
	url = {https://epubs.siam.org/doi/abs/10.1137/0208032},
	doi = {10.1137/0208032},
	abstract = {Alternating Turing machines (ATMs) with bounded number of reversals are considered. It is proved that the machines making fewer than \${\textbackslash}log {\textasciicircum}\{*\} n\$ reversals can recognize only regular languages. On the other hand, the class of languages that can be recognized by ATMs using \${\textbackslash}log {\textasciicircum}\{*\} n\$ reversals is very wide. The authors prove that above this limit even a slight increase of the number of reversals leads to a considerably larger class of languages. It is also proved that every \$T(n)\$-time bounded ATM may be replaced by an equivalent machine working in the same time and making no more than \${\textbackslash}log {\textasciicircum}\{*\} (T(n))\$ reversals.},
	number = {3},
	urldate = {2024-01-21},
	journal = {SIAM Journal on Computing},
	author = {Valiant, Leslie G.},
	month = aug,
	year = {1979},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {410--421},
}

@article{mohamed_monte_2020,
	title = {Monte {Carlo} gradient estimation in machine learning},
	volume = {21},
	issn = {1532-4435},
	abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators-- exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
	month = jan,
	year = {2020},
	keywords = {Monte Carlo, gradient estimation, measure-valued estimator, pathwise estimator, score-function estimator, sensitivity analysis, variance reduction},
	pages = {132:5183--132:5244},
}

@misc{sehgal_neurosymbolic_2023,
	title = {Neurosymbolic {Grounding} for {Compositional} {World} {Models}},
	url = {http://arxiv.org/abs/2310.12690},
	abstract = {We introduce COSMOS, a framework for object-centric world modeling that is designed for compositional generalization (CG), i.e., high performance on unseen input scenes obtained through the composition of known visual “atoms." The central insight behind COSMOS is the use of a novel form of neurosymbolic grounding. Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. COSMOS is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity’s symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CG on an established blocks-pushing domain, we show that the framework establishes a new state-of-the-art for CG in world modeling.},
	language = {en},
	urldate = {2024-01-20},
	publisher = {arXiv},
	author = {Sehgal, Atharva and Grayeli, Arya and Sun, Jennifer J. and Chaudhuri, Swarat},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12690 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, todo},
}

@misc{xu_logicmp_2023,
	title = {{LogicMP}: {A} {Neuro}-symbolic {Approach} for {Encoding} {First}-order {Logic} {Constraints}},
	shorttitle = {{LogicMP}},
	url = {http://arxiv.org/abs/2309.15458},
	abstract = {Integrating first-order logic constraints (FOLCs) with neural networks is a crucial but challenging problem since it involves modeling intricate correlations to satisfy the constraints. This paper proposes a novel neural layer, LogicMP, which performs mean-field variational inference over an MLN. It can be plugged into any off-theshelf neural network to encode FOLCs while retaining modularity and efficiency. By exploiting the structure and symmetries in MLNs, we theoretically demonstrate that our well-designed, efficient mean-field iterations greatly mitigate the difficulty of MLN inference, reducing the inference from sequential calculation to a series of parallel tensor operations. Empirical results in three kinds of tasks over images, graphs, and text show that LogicMP outperforms advanced competitors in both performance and efficiency.},
	language = {en},
	urldate = {2024-01-20},
	publisher = {arXiv},
	author = {Xu, Weidi and Wang, Jingwei and Xie, Lele and He, Jianshan and Zhou, Hongting and Wang, Taifeng and Wan, Xiaopei and Chen, Jingdong and Qu, Chao and Chu, Wei},
	month = sep,
	year = {2023},
	note = {arXiv:2309.15458 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Symbolic Computation, todo},
}

@misc{shterionov_dnf_2010,
	title = {{DNF} {Sampling} for {ProbLog} {Inference}},
	url = {http://arxiv.org/abs/1009.3798},
	abstract = {Inference in probabilistic logic languages such as ProbLog, an extension of Prolog with probabilistic facts, is often based on a reduction to a propositional formula in DNF. Calculating the probability of such a formula involves the disjoint-sum-problem, which is computationally hard. In this work we introduce a new approximation method for ProbLog inference which exploits the DNF to focus sampling. While this DNF sampling technique has been applied to a variety of tasks before, to the best of our knowledge it has not been used for inference in probabilistic logic systems. The paper also presents an experimental comparison with another sampling based inference method previously introduced for ProbLog.},
	language = {en},
	urldate = {2023-10-11},
	publisher = {arXiv},
	author = {Shterionov, Dimitar Sht and Kimmig, Angelika and Mantadelis, Theofrastos and Janssens, Gerda},
	month = sep,
	year = {2010},
	note = {arXiv:1009.3798 [cs]},
	keywords = {Computer Science - Logic in Computer Science, leuven},
}

@inproceedings{marconato_not_2023,
	title = {Not {All} {Neuro}-{Symbolic} {Concepts} {Are} {Created} {Equal}: {Analysis} and {Mitigation} of {Reasoning} {Shortcuts}},
	shorttitle = {Not {All} {Neuro}-{Symbolic} {Concepts} {Are} {Created} {Equal}},
	url = {https://openreview.net/forum?id=tLTtqySDFb&referrer=%5Bthe%20profile%20of%20Antonio%20Vergari%5D(%2Fprofile%3Fid%3D~Antonio_Vergari3)},
	abstract = {Neuro-Symbolic (NeSy) predictive models hold the promise of improved compliance with given constraints, systematic generalization, and interpretability, as they allow to infer labels that are consistent with some prior knowledge by reasoning over high-level concepts extracted from sub-symbolic inputs. It was recently shown that NeSy predictors are affected by *reasoning shortcuts*: they can attain high accuracy but by leveraging concepts with {\textbackslash}textit\{unintended semantics\}, thus coming short of their promised advantages. Yet, a systematic characterization of reasoning shortcuts and of potential mitigation strategies is missing. This work fills this gap by characterizing them as unintended optima of the learning objective and identifying four key conditions behind their occurrence. Based on this, we derive several natural mitigation strategies, and analyze their efficacy both theoretically and empirically. Our analysis shows reasoning shortcuts are difficult to deal with, casting doubts on the trustworthiness and interpretability of existing NeSy solutions.},
	language = {en},
	urldate = {2024-01-20},
	author = {Marconato, Emanuele and Teso, Stefano and Vergari, Antonio and Passerini, Andrea},
	month = nov,
	year = {2023},
}

@incollection{manhaeve_chapter_2021,
	title = {Chapter 7. {Neuro}-{Symbolic} {AI} = {Neural} + {Logical} + {Probabilistic} {AI}},
	url = {https://ebooks.iospress.nl/doi/10.3233/FAIA210354},
	urldate = {2024-01-19},
	booktitle = {Neuro-{Symbolic} {Artificial} {Intelligence}: {The} {State} of the {Art}},
	publisher = {IOS Press},
	author = {Manhaeve, Robin and Marra, Giuseppe and Demeester, Thomas and Dumančić, Sebastijan and Kimmig, Angelika and De Raedt, Luc},
	year = {2021},
	doi = {10.3233/FAIA210354},
	keywords = {leuven},
	pages = {173--191},
}

@misc{hsu_whats_2023-1,
	title = {What's {Left}? {Concept} {Grounding} with {Logic}-{Enhanced} {Foundation} {Models}},
	shorttitle = {What's {Left}?},
	url = {http://arxiv.org/abs/2310.16035},
	abstract = {Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning—using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like “left” can also be grounded in 3D, temporal, and action data, as in moving to your left. This limited generalization stems from these inference-only methods’ inability to learn or adapt pre-trained models to a new domain. We propose the Logic-Enhanced FoundaTion Model (LEFT), a unified framework that learns to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic–based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT’s executor then executes the program with trainable domainspecific grounding modules. We show that LEFT flexibly learns concepts in four domains: 2D images, 3D scenes, human motions, and robotic manipulation. It exhibits strong reasoning ability in a wide variety of tasks, including those that are complex and not seen during training, and can be easily applied to new domains.},
	language = {en},
	urldate = {2024-01-19},
	publisher = {arXiv},
	author = {Hsu, Joy and Mao, Jiayuan and Tenenbaum, Joshua B. and Wu, Jiajun},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16035 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kim_imagine_2023,
	title = {Imagine the {Unseen} {World}: {A} {Benchmark} for {Systematic} {Generalization} in {Visual} {World} {Models}},
	shorttitle = {Imagine the {Unseen} {World}},
	url = {http://arxiv.org/abs/2311.09064},
	abstract = {Systematic compositionality, or the ability to adapt to novel situations by creating a mental model of the world using reusable pieces of knowledge, remains a significant challenge in machine learning. While there has been considerable progress in the language domain, efforts towards systematic visual imagination, or envisioning the dynamical implications of a visual observation, are in their infancy. We introduce the Systematic Visual Imagination Benchmark (SVIB), the first benchmark designed to address this problem head-on. SVIB offers a novel framework for a minimal world modeling problem, where models are evaluated based on their ability to generate one-step image-to-image transformations under a latent world dynamics. The framework provides benefits such as the possibility to jointly optimize for systematic perception and imagination, a range of difficulty levels, and the ability to control the fraction of possible factor combinations used during training. We provide a comprehensive evaluation of various baseline models on SVIB, offering insight into the current state-of-the-art in systematic visual imagination. We hope that this benchmark will help advance visual systematic compositionality.},
	language = {en},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Kim, Yeongbin and Singh, Gautam and Park, Junyeong and Gulcehre, Caglar and Ahn, Sungjin},
	month = nov,
	year = {2023},
	note = {arXiv:2311.09064 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{li_neuro-symbolic_2023,
	title = {Neuro-symbolic {Learning} {Yielding} {Logical} {Constraints}},
	url = {https://openreview.net/forum?id=2ioRi2uwLR},
	abstract = {Neuro-symbolic systems combine the abilities of neural perception and logical reasoning. However, end-to-end learning of neuro-symbolic systems is still an unsolved challenge. This paper proposes a natural framework that fuses neural network training, symbol grounding, and logical constraint synthesis into a coherent and efficient end-to-end learning process. The capability of this framework comes from the improved interactions between the neural and the symbolic parts of the system in both the training and inference stages. Technically, to bridge the gap between the continuous neural network and the discrete logical constraint, we introduce a difference-of-convex programming technique to relax the logical constraints while maintaining their precision. We also employ cardinality constraints as the language for logical constraint learning and incorporate a trust region method to avoid the degeneracy of logical constraint in learning. Both theoretical analyses and empirical evaluations substantiate the effectiveness of the proposed framework.},
	language = {en},
	urldate = {2023-11-28},
	author = {Li, Zenan and Huang, Yunpeng and Li, Zhaoyu and Yao, Yuan and Xu, Jingwei and Chen, Taolue and Ma, Xiaoxing and Lu, Jian},
	month = nov,
	year = {2023},
	keywords = {todo},
}

@inproceedings{sutton_policy_1999,
	title = {Policy {Gradient} {Methods} for {Reinforcement} {Learning} with {Function} {Approximation}},
	volume = {12},
	shorttitle = {{REINFORCE}},
	url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html},
	abstract = {Function  approximation  is  essential  to  reinforcement  learning,  but  the standard approach of approximating a  value function and deter(cid:173) mining  a  policy  from  it  has so  far  proven theoretically  intractable.  In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator,  indepen(cid:173) dent of the value function,  and is  updated according to the gradient  of expected reward with respect to the policy parameters.  Williams's  REINFORCE method and actor-critic methods are examples of this  approach.  Our  main  new  result  is  to  show  that  the  gradient  can  be  written  in  a  form  suitable  for  estimation  from  experience  aided  by  an  approximate  action-value  or  advantage  function.  Using  this  result,  we  prove for  the first  time that a  version  of policy  iteration  with arbitrary differentiable function approximation is convergent to  a  locally optimal policy.},
	urldate = {2024-01-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
	year = {1999},
}

@inproceedings{kool_buy_2019,
	title = {Buy 4 {REINFORCE} {Samples}, {Get} a {Baseline} for {Free}!},
	shorttitle = {{RLOO}},
	url = {https://openreview.net/forum?id=r1lgTGL5DE},
	abstract = {REINFORCE can be used to train models in structured prediction settings to directly optimize the test-time objective. However, the common case of sampling one prediction per datapoint (input) is data-inefficient. We show that by drawing multiple samples (predictions) per datapoint, we can learn with significantly less data, as we freely obtain a REINFORCE baseline to reduce variance. Additionally we derive a REINFORCE estimator with baseline, based on sampling without replacement. Combined with a recent technique to sample sequences without replacement using Stochastic Beam Search, this improves the training procedure for a sequence model that predicts the solution to the Travelling Salesman Problem.},
	language = {en},
	urldate = {2024-01-08},
	author = {Kool, Wouter and Hoof, Herke van and Welling, Max},
	month = apr,
	year = {2019},
}

@article{meel_not_2019,
	title = {Not all {FPRASs} are equal: demystifying {FPRASs} for {DNF}-counting},
	volume = {24},
	issn = {1572-9354},
	url = {https://doi.org/10.1007/s10601-018-9301-x},
	doi = {10.1007/s10601-018-9301-x},
	abstract = {The problem of counting the number of solutions of a DNF formula, also called \#DNF, is a fundamental problem in artificial intelligence with applications in diverse domains ranging from network reliability to probabilistic databases. Owing to the intractability of the exact variant, efforts have focused on the design of approximate techniques for \#DNF. Consequently, several Fully Polynomial Randomized Approximation Schemes (FPRASs) based on Monte Carlo techniques have been proposed. Recently, it was discovered that hashing-based techniques too lend themselves to FPRASs for \#DNF. Despite significant improvements, the complexity of the hashing-based FPRAS is still worse than that of the best Monte Carlo FPRAS by polylog factors. Two questions were left unanswered in previous works: Can the complexity of the hashing-based techniques be improved? How do the various approaches stack up against each other empirically? In this paper, we first propose a new search procedure for the hashing-based FPRAS that removes the polylog factors from its time complexity. We then present the first empirical study of runtime behavior of different FPRASs for \#DNF. The result of our study produces a nuanced picture. First of all, we observe that there is no single best algorithm that outperforms all others for all classes of formulas and input parameters. Second, we observe that the algorithm with one of the worst time complexities solves the largest number of benchmarks.},
	language = {en},
	number = {3},
	urldate = {2024-01-09},
	journal = {Constraints},
	author = {Meel, Kuldeep S. and Shrotri, Aditya A. and Vardi, Moshe Y.},
	month = oct,
	year = {2019},
	keywords = {ApproxMC, Boolean formulas, Disjunctive normal form, Fully polynomial randomized approximation scheme, Hashing, Model counting},
	pages = {211--233},
}

@article{soos_bird_2019,
	title = {{BIRD}: {Engineering} an {Efficient} {CNF}-{XOR} {SAT} {Solver} and {Its} {Applications} to {Approximate} {Model} {Counting}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{ApproxMC4}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/3974},
	doi = {10.1609/aaai.v33i01.33011592},
	abstract = {Given a Boolean formula φ, the problem of model counting, also referred to as \#SAT is to compute the number of solutions of φ. Model counting is a fundamental problem in artificial intelligence with a wide range of applications including probabilistic reasoning, decision making under uncertainty, quantified information flow, and the like. Motivated by the success of SAT solvers, there has been surge of interest in the design of hashing-based techniques for approximate model counting for the past decade. We profiled the state of the art approximate model counter ApproxMC2 and observed that over 99.99\% of time is consumed by the underlying SAT solver, CryptoMiniSat. This observation motivated us to ask: Can we design an efficient underlying CNF-XOR SAT solver that can take advantage of the structure of hashing-based algorithms and would this lead to an efficient approximate model counter?
The primary contribution of this paper is an affirmative answer to the above question. We present a novel architecture, called BIRD, to handle CNF-XOR formulas arising from hashingbased techniques. The resulting hashing-based approximate model counter, called ApproxMC3, employs the BIRD framework in its underlying SAT solver, CryptoMiniSat. To the best of our knowledge, we conducted the most comprehensive study of evaluation performance of counting algorithms involving 1896 benchmarks with computational effort totaling 86400 computational hours. Our experimental evaluation demonstrates significant runtime performance improvement for ApproxMC3 over ApproxMC2. In particular, we solve 648 benchmarks more than ApproxMC2, the state of the art approximate model counter and for all the formulas where both ApproxMC2 and ApproxMC3 did not timeout and took more than 1 seconds, the mean speedup is 284.40 – more than two orders of magnitude.
Erratum: This research is supported in part by the National Research Foundation Singapore under its AI Singapore Programme (Award Number: [AISG-RP-2018-005])},
	language = {en},
	number = {01},
	urldate = {2024-01-10},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Soos, Mate and Meel, Kuldeep S.},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {1592--1599},
}

@inproceedings{soos_tinted_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Tinted, {Detached}, and {Lazy} {CNF}-{XOR} {Solving} and {Its} {Applications} to {Counting} and {Sampling}},
	isbn = {978-3-030-53288-8},
	shorttitle = {{UniGen3}},
	doi = {10.1007/978-3-030-53288-8_22},
	abstract = {Given a Boolean formula, the problem of counting seeks to estimate the number of solutions of F while the problem of uniform sampling seeks to sample solutions uniformly at random. Counting and uniform sampling are fundamental problems in computer science with a wide range of applications ranging from constrained random simulation, probabilistic inference to network reliability and beyond. The past few years have witnessed the rise of hashing-based approaches that use XOR-based hashing and employ SAT solvers to solve the resulting CNF formulas conjuncted with XOR constraints. Since over 99\% of the runtime of hashing-based techniques is spent inside the SAT queries, improving CNF-XOR solvers has emerged as a key challenge.},
	language = {en},
	booktitle = {Computer {Aided} {Verification}},
	publisher = {Springer International Publishing},
	author = {Soos, Mate and Gocht, Stephan and Meel, Kuldeep S.},
	editor = {Lahiri, Shuvendu K. and Wang, Chao},
	year = {2020},
	pages = {463--484},
}

@inproceedings{golia_designing_2021,
	title = {Designing {Samplers} is {Easy}: {The} {Boon} of {Testers}},
	shorttitle = {{CMSGen}},
	url = {https://ieeexplore.ieee.org/abstract/document/9617576},
	doi = {10.34727/2021/isbn.978-3-85448-046-4_31},
	abstract = {Given a formula φ, the problem of uniform sampling seeks to sample solutions of φ uniformly at random. Uniform sampling is a fundamental problem with a wide variety of applications. The computational intractability of uniform sampling has led to the development of several samplers that heavily rely on heuristics and are not accompanied by theoretical analysis of their distribution. Recently, Chakraborty and Meel (2019) designed the first scalable sampling tester, Barbarik, based on a grey-box sampling technique for testing if the distribution, according to which the given sampler is sampling, is close to the uniform or far from uniform. While the theoretical analysis of Barbarik provides only unconditional soundness guarantees, the empirical evaluation of Barbarik did show its success in determining that some of the off-the-shelf samplers were far from a uniform sampler. The availability of Barbarik has the potential to spur development of samplers techniques such that developers can design sampling methods that can be accepted by Barbarik even though these samplers may not be amenable to a detailed mathematical analysis. In this paper, we present the realization of this aforementioned promise. Based on the flexibility offered by CryptoMiniSat, we design a sampler CMSGen that promises the achievement of sweet spot of the quality of distributions and runtime performance. In particular, CMSGen achieves significant runtime performance improvement over the existing samplers. We conduct two case studies, and demonstrate that the usage of CMSGen leads to significant runtime improvements in the context of combinatorial testing and functional synthesis. A salient strength of our work is the simplicity of CMSGen, which stands in contrast to complicated algorithmic schemes developed in the past that fail to attain the desired quality of distributions with practical runtime performance.},
	urldate = {2024-01-10},
	booktitle = {2021 {Formal} {Methods} in {Computer} {Aided} {Design} ({FMCAD})},
	author = {Golia, Priyanka and Soos, Mate and Chakraborty, Sourav and Meel, Kuldeep S.},
	month = oct,
	year = {2021},
	note = {ISSN: 2708-7824},
	pages = {222--230},
}

@article{espinosa_zarlenga_concept_2022,
	title = {Concept {Embedding} {Models}: {Beyond} the {Accuracy}-{Explainability} {Trade}-{Off}},
	volume = {35},
	shorttitle = {Concept {Embedding} {Models}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/867c06823281e506e8059f5c13a57f75-Abstract-Conference.html},
	language = {en},
	urldate = {2024-01-10},
	journal = {Advances in Neural Information Processing Systems},
	author = {Espinosa Zarlenga, Mateo and Barbiero, Pietro and Ciravegna, Gabriele and Marra, Giuseppe and Giannini, Francesco and Diligenti, Michelangelo and Shams, Zohreh and Precioso, Frederic and Melacci, Stefano and Weller, Adrian and Lió, Pietro and Jamnik, Mateja},
	month = dec,
	year = {2022},
	pages = {21400--21413},
}

@article{marconato_interpretability_2023,
	title = {Interpretability {Is} in the {Mind} of the {Beholder}: {A} {Causal} {Framework} for {Human}-{Interpretable} {Representation} {Learning}},
	volume = {25},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	shorttitle = {Interpretability {Is} in the {Mind} of the {Beholder}},
	url = {https://www.mdpi.com/1099-4300/25/12/1574},
	doi = {10.3390/e25121574},
	abstract = {Research on Explainable Artificial Intelligence has recently started exploring the idea of producing explanations that, rather than being expressed in terms of low-level features, are encoded in terms of interpretable concepts learned from data. How to reliably acquire such concepts is, however, still fundamentally unclear. An agreed-upon notion of concept interpretability is missing, with the result that concepts used by both post hoc explainers and concept-based neural networks are acquired through a variety of mutually incompatible strategies. Critically, most of these neglect the human side of the problem: a representation is understandable only insofar as it can be understood by the human at the receiving end. The key challenge in human-interpretable representation learning (hrl) is how to model and operationalize this human element. In this work, we propose a mathematical framework for acquiring interpretable representations suitable for both post hoc explainers and concept-based neural networks. Our formalization of hrl builds on recent advances in causal representation learning and explicitly models a human stakeholder as an external observer. This allows us derive a principled notion of alignment between the machine’s representation and the vocabulary of concepts understood by the human. In doing so, we link alignment and interpretability through a simple and intuitive name transfer game, and clarify the relationship between alignment and a well-known property of representations, namely disentanglement. We also show that alignment is linked to the issue of undesirable correlations among concepts, also known as concept leakage, and to content-style separation, all through a general information-theoretic reformulation of these properties. Our conceptualization aims to bridge the gap between the human and algorithmic sides of interpretability and establish a stepping stone for new research on human-interpretable representations.},
	language = {en},
	number = {12},
	urldate = {2024-01-10},
	journal = {Entropy},
	author = {Marconato, Emanuele and Passerini, Andrea and Teso, Stefano},
	month = dec,
	year = {2023},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {alignment, causal abstractions, causal representation learning, concept leakage, disentanglement, explainable AI},
	pages = {1574},
}

@inproceedings{chakraborty_weighted_2015,
	title = {From {Weighted} to {Unweighted} {Model} {Counting}.},
	shorttitle = {{WeightCount}},
	url = {https://www.cs.rice.edu/CS/Verification/Projects/CUSP/IJCAI15/IJCAI2015.pdf},
	urldate = {2024-01-10},
	booktitle = {{IJCAI}},
	author = {Chakraborty, Supratik and Fried, Dror and Meel, Kuldeep S. and Vardi, Moshe Y.},
	year = {2015},
	pages = {689--695},
}

@article{chakraborty_distribution-aware_2014,
	title = {Distribution-{Aware} {Sampling} and {Weighted} {Model} {Counting} for {SAT}},
	volume = {28},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	shorttitle = {{WeightMC} / {WeightGen}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/8990},
	doi = {10.1609/aaai.v28i1.8990},
	abstract = {Given a CNF formula and a weight for each assignment of values tovariables, two natural problems are weighted model counting anddistribution-aware sampling of satisfying assignments.  Both problems have a wide variety of important applications.  Due to the inherentcomplexity of the exact versions of the problems, interest has focusedon solving them approximately.  Prior work in this area scaled only tosmall problems in practice, or failed to provide strong theoreticalguarantees, or employed a computationally-expensive most-probable-explanation (\{{\textbackslash}MPE\}) queries that assumes prior knowledge of afactored representation of the weight distribution. We identify a novel parameter,{\textbackslash}emph\{tilt\}, which is the ratio of the maximum weight of satisfying assignment to minimum weightof satisfying assignment and present anovel approach that works with a black-box oracle for weights ofassignments and requires only an \{{\textbackslash}NP\}-oracle (in practice, a \{{\textbackslash}SAT\}-solver) to solve both thecounting and sampling problems when the tilt is small.  Our approach  provides strong theoretical guarantees, and scales toproblems involving several thousand variables. We also show that theassumption of small tilt can be significantly relaxed while improving computational efficiency if a factored representation of the weights is known.},
	language = {en},
	number = {1},
	urldate = {2024-01-05},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chakraborty, Supratik and Fremont, Daniel and Meel, Kuldeep and Seshia, Sanjit and Vardi, Moshe},
	month = jun,
	year = {2014},
	note = {Number: 1},
	keywords = {machine learning},
}

@inproceedings{ermon_uniform_2012,
	address = {Arlington, Virginia, USA},
	series = {{UAI}'12},
	title = {Uniform solution sampling using a constraint solver as an oracle},
	isbn = {978-0-9749039-8-9},
	shorttitle = {sts},
	abstract = {We consider the problem of sampling from solutions defined by a set of hard constraints on a combinatorial space. We propose a new sampling technique that, while enforcing a uniform exploration of the search space, leverages the reasoning power of a systematic constraint solver in a black-box scheme. We present a series of challenging domains, such as energy barriers and highly asymmetric spaces, that reveal the difficulties introduced by hard constraints. We demonstrate that standard approaches such as Simulated Annealing and Gibbs Sampling are greatly affected, while our new technique can overcome many of these difficulties. Finally, we show that our sampling scheme naturally defines a new approximate model counting technique, which we empirically show to be very accurate on a range of benchmark problems.},
	urldate = {2024-01-10},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {AUAI Press},
	author = {Ermon, Stefano and Gomes, Carla and Selman, Bart},
	month = aug,
	year = {2012},
	pages = {255--264},
}

@incollection{hutchison_new_2005,
	address = {Berlin, Heidelberg},
	title = {A {New} {Approach} to {Model} {Counting}},
	volume = {3569},
	isbn = {978-3-540-26276-3 978-3-540-31679-4},
	shorttitle = {{ApproxCount}},
	url = {http://link.springer.com/10.1007/11499107_24},
	abstract = {We introduce ApproxCount, an algorithm that approximates the number of satisfying assignments or models of a formula in propositional logic. Many AI tasks, such as calculating degree of belief and reasoning in Bayesian networks, are computationally equivalent to model counting. It has been shown that model counting in even the most restrictive logics, such as Horn logic, monotone CNF and 2CNF, is intractable in the worst-case. Moreover, even approximate model counting remains a worst-case intractable problem. So far, most practical model counting algorithms are based on backtrack style algorithms such as the DPLL procedure. These algorithms typically yield exact counts but are limited to relatively small formulas. Our ApproxCount algorithm is based on SampleSat, a new algorithm that samples from the solution space of a propositional logic formula near-uniformly. We provide experimental results for formulas from a variety of domains. The algorithm produces good estimates for formulas much larger than those that can be handled by existing algorithms.},
	language = {en},
	urldate = {2023-08-23},
	booktitle = {Theory and {Applications} of {Satisfiability} {Testing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Wei, Wei and Selman, Bart},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Bacchus, Fahiem and Walsh, Toby},
	year = {2005},
	doi = {10.1007/11499107_24},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {324--339},
}

@inproceedings{kimmig_efficient_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On the {Efficient} {Execution} of {ProbLog} {Programs}},
	isbn = {978-3-540-89982-2},
	doi = {10.1007/978-3-540-89982-2_22},
	abstract = {The past few years have seen a surge of interest in the field of probabilistic logic learning or statistical relational learning. In this endeavor, many probabilistic logics have been developed. ProbLog is a recent probabilistic extension of Prolog motivated by the mining of large biological networks. In ProbLog, facts can be labeled with mutually independent probabilities that they belong to a randomly sampled program. Different kinds of queries can be posed to ProbLog programs. We introduce algorithms that allow the efficient execution of these queries, discuss their implementation on top of the YAP-Prolog system, and evaluate their performance in the context of large networks of biological entities.},
	language = {en},
	booktitle = {Logic {Programming}},
	publisher = {Springer},
	author = {Kimmig, Angelika and Santos Costa, Vítor and Rocha, Ricardo and Demoen, Bart and De Raedt, Luc},
	editor = {Garcia de la Banda, Maria and Pontelli, Enrico},
	year = {2008},
	keywords = {leuven},
	pages = {175--189},
}

@article{gogate_samplesearch_2011,
	title = {{SampleSearch}: {Importance} sampling in presence of determinism},
	volume = {175},
	issn = {0004-3702},
	shorttitle = {{SampleSearch}},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370210001797},
	doi = {10.1016/j.artint.2010.10.009},
	abstract = {The paper focuses on developing effective importance sampling algorithms for mixed probabilistic and deterministic graphical models. The use of importance sampling in such graphical models is problematic because it generates many useless zero weight samples which are rejected yielding an inefficient sampling process. To address this rejection problem, we propose the SampleSearch scheme that augments sampling with systematic constraint-based backtracking search. We characterize the bias introduced by the combination of search with sampling, and derive a weighting scheme which yields an unbiased estimate of the desired statistics (e.g., probability of evidence). When computing the weights exactly is too complex, we propose an approximation which has a weaker guarantee of asymptotic unbiasedness. We present results of an extensive empirical evaluation demonstrating that SampleSearch outperforms other schemes in presence of significant amount of determinism.},
	number = {2},
	urldate = {2024-01-09},
	journal = {Artificial Intelligence},
	author = {Gogate, Vibhav and Dechter, Rina},
	month = feb,
	year = {2011},
	keywords = {Approximate inference, Bayesian networks, Constraint satisfaction, Importance sampling, Markov chain Monte Carlo, Markov networks, Model counting, Probabilistic inference, Satisfiability, todo},
	pages = {694--729},
}

@inproceedings{de_raedt_statistical_2020,
	address = {Yokohama, Japan},
	title = {From {Statistical} {Relational} to {Neuro}-{Symbolic} {Artificial} {Intelligence}},
	isbn = {978-0-9992411-6-5},
	url = {https://www.ijcai.org/proceedings/2020/688},
	doi = {10.24963/ijcai.2020/688},
	abstract = {Neural-symbolic and statistical relational artiﬁcial intelligence both integrate frameworks for learning with logical reasoning. This survey identiﬁes several parallels across seven different dimensions between these two ﬁelds. These cannot only be used to characterize and position neural-symbolic artiﬁcial intelligence approaches but also to identify a number of directions for further research.},
	language = {en},
	urldate = {2023-12-22},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {De Raedt, Luc and Dumančić, Sebastijan and Manhaeve, Robin and Marra, Giuseppe},
	month = jul,
	year = {2020},
	keywords = {leuven},
	pages = {4943--4950},
}

@inproceedings{zuidberg_dos_martires_neural_2021,
	title = {Neural {Semirings}},
	abstract = {From an abstract algebra perspective, reasoning tasks in symbolic artificial intelligence can be formulated in terms of commutative semiring operations. Usually, the reasoning task, e.g. probabilistic reasoning or fuzzy reasoning, is fixed upfront as part of the problem specification. In this paper we relax this assumption and render the reasoning operators (the semiring operations) subject to learning by replacing predefined operations with learnable neural networks. This unlocks the learn to reason paradigm in the semiring reasoning setting.},
	language = {en},
	author = {Zuidberg Dos Martires, Pedro},
	year = {2021},
	keywords = {leuven},
}

@inproceedings{de_smet_tensorised_2022,
	title = {Tensorised {Probabilistic} {Inference} for {Neural} {Probabilistic} {Logic} {Programming}},
	url = {https://openreview.net/forum?id=6Kpbq2Y2IK6},
	abstract = {Neural Probabilistic Logic Programming (NPLP) languages have illustrated how to combine the neural paradigm with that of probabilistic logic programming. Together, they form a neural-symbolic framework integrating low-level perception with high-level reasoning. Such an integration has been shown to aid in the limited data regime and to facilitate better generalisation to out-of-distribution data. However, probabilistic logic inference does not allow for data-parallelisation because of the asymmetries arising in the proof trees during grounding. By lifting part of this inference procedure through the use of symbolic tensor operations, facilitating parallelisation, we achieve a measurable speed-up in learning and inference time. We implemented this tensor perspective in the NPLP language DeepProbLog and demonstrated the speed-up in a comparison to its regular implementation that utilises state-of-the-art probabilistic inference techniques.},
	language = {en},
	urldate = {2023-07-03},
	author = {De Smet, Lennert and Manhaeve, Robin and Marra, Giuseppe and Zuidberg Dos Martires, Pedro},
	month = jul,
	year = {2022},
	keywords = {leuven},
}

@article{abboud_learning_2020,
	title = {Learning to {Reason}: {Leveraging} {Neural} {Networks} for {Approximate} {DNF} {Counting}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Neural\#{DNF}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5705},
	doi = {10.1609/aaai.v34i04.5705},
	abstract = {Weighted model counting (WMC) has emerged as a prevalent approach for probabilistic inference. In its most general form, WMC is \#P-hard. Weighted DNF counting (weighted \#DNF) is a special case, where approximations with probabilistic guarantees are obtained in O(nm), where n denotes the number of variables, and m the number of clauses of the input DNF, but this is not scalable in practice. In this paper, we propose a neural model counting approach for weighted \#DNF that combines approximate model counting with deep learning, and accurately approximates model counts in linear time when width is bounded. We conduct experiments to validate our method, and show that our model learns and generalizes very well to large-scale \#DNF instances.},
	language = {en},
	number = {04},
	urldate = {2024-01-05},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Abboud, Ralph and Ceylan, Ismail and Lukasiewicz, Thomas},
	month = apr,
	year = {2020},
	note = {Number: 04},
	pages = {3097--3104},
}

@misc{yao_tree_2023,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	shorttitle = {Tree of {Thoughts}},
	url = {http://arxiv.org/abs/2305.10601},
	abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, “Tree of Thoughts” (ToT), which generalizes over the popular “Chain of Thought” approach to prompting language models, and enables exploration over coherent units of text (“thoughts”) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
	language = {en},
	urldate = {2023-12-17},
	publisher = {arXiv},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
	month = dec,
	year = {2023},
	note = {arXiv:2305.10601 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{romera-paredes_mathematical_2023,
	title = {Mathematical discoveries from program search with large language models},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06924-6},
	doi = {10.1038/s41586-023-06924-6},
	abstract = {Large Language Models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations) which can result in them making plausible but incorrect statements [1,2]. This hinders the use of current large models in scientific discovery. Here we introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pre-trained LLM with a systematic evaluator. We demonstrate the effectiveness of this approach to surpass the best known results in important problems, pushing the boundary of existing LLM-based approaches [3]. Applying FunSearch to a central problem in extremal combinatorics — the cap set problem — we discover new constructions of large cap sets going beyond the best known ones, both in finite dimensional and asymptotic cases. This represents the first discoveries made for established open problems using LLMs. We showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve upon widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.},
	language = {en},
	urldate = {2023-12-17},
	journal = {Nature},
	author = {Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M. Pawan and Dupont, Emilien and Ruiz, Francisco J. R. and Ellenberg, Jordan S. and Wang, Pengming and Fawzi, Omar and Kohli, Pushmeet and Fawzi, Alhussein},
	month = dec,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Pure mathematics, todo},
	pages = {1--3},
}

@inproceedings{batsheva_protes_2023,
	title = {{PROTES}: {Probabilistic} {Optimization} with {Tensor} {Sampling}},
	shorttitle = {{PROTES}},
	url = {https://openreview.net/forum?id=R9R7YDOar1},
	abstract = {We developed a new method PROTES for black-box optimization, which is based on the probabilistic sampling from a probability density function given in the low-parametric tensor train format. We tested it on complex multidimensional arrays and discretized multivariable functions taken, among others, from real-world applications, including unconstrained binary optimization and optimal control problems, for which the possible number of elements is up to \$2{\textasciicircum}\{1000\}\$. In numerical experiments, both on analytic model functions and on complex problems, PROTES outperforms popular discrete optimization methods (Particle Swarm Optimization, Covariance Matrix Adaptation, Differential Evolution, and others).},
	language = {en},
	urldate = {2023-12-17},
	author = {Batsheva, Anastasia and Chertkov, Andrei and Ryzhakov, Gleb and Oseledets, Ivan},
	month = nov,
	year = {2023},
	keywords = {todo},
}

@inproceedings{wang_learning_2023,
	title = {On {Learning} {Latent} {Models} with {Multi}-{Instance} {Weak} {Supervision}},
	url = {https://openreview.net/forum?id=ZD65F3x1jU},
	abstract = {We consider a weakly supervised learning scenario where the supervision signal is generated by a transition function \${\textbackslash}sigma\$ of labels associated with multiple input instances. We formulate this problem as *multi-instance Partial Label Learning (multi-instance PLL)*, which is an extension to the standard PLL problem. Our problem is met in different fields, including latent structural learning and neuro-symbolic integration. Despite the existence of many learning techniques, limited theoretical analysis has been dedicated to this problem. In this paper, we provide the first theoretical study of multi-instance PLL with possibly an unknown transition \${\textbackslash}sigma\$. Our main contributions are as follows: First, we proposed a necessary and sufficient condition for the learnability of the problem. This condition nontrivially generalizes and relaxes the existing *small ambiguity degree* in PLL literature since we allow the transition to be deterministic. Second, we derived Rademacher-style error bounds based on the top-\$k\$ surrogate loss that is widely used in the neuro-symbolic literature. Furthermore, we conclude with empirical experiments for learning with an unknown transition. The empirical results align with our theoretical findings; however, they also expose the issue of scalability in the weak supervision literature.},
	language = {en},
	urldate = {2023-11-28},
	author = {Wang, Kaifu and Tsamoura, Efthymia and Roth, Dan},
	month = nov,
	year = {2023},
}

@misc{lew_sequential_2023,
	title = {Sequential {Monte} {Carlo} {Steering} of {Large} {Language} {Models} using {Probabilistic} {Programs}},
	url = {http://arxiv.org/abs/2306.03081},
	abstract = {Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone. We propose a new inference-time approach to enforcing syntactic and semantic constraints on the outputs of LLMs, called sequential Monte Carlo (SMC) steering. The key idea is to specify language generation tasks as posterior inference problems in a class of discrete probabilistic sequence models, and replace standard decoding with sequential Monte Carlo inference. For a computational cost similar to that of beam search, SMC can steer LLMs to solve diverse tasks, including infilling, generation under syntactic constraints, and prompt intersection. To facilitate experimentation with SMC steering, we present a probabilistic programming library, LLaMPPL, for concisely specifying new generation tasks as language model probabilistic programs, and automating steering of LLaMA-family Transformers.},
	language = {en},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {Lew, Alexander K. and Zhi-Xuan, Tan and Grand, Gabriel and Mansinghka, Vikash K.},
	month = nov,
	year = {2023},
	note = {arXiv:2306.03081 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages, Statistics - Computation},
}

@article{carraro_mitigating_nodate,
	title = {Mitigating {Data} {Sparsity} via {Neuro}-{Symbolic} {Knowledge} {Transfer}},
	abstract = {Data sparsity is a well-known historical limitation of recommender systems that still impacts the performance of state-of-theart approaches. One practical technique to mitigate this issue involves transferring information from other domains or tasks to compensate for scarcity in the target domain, where the recommendations must be performed. Following this idea, we propose a novel approach based on Neuro-Symbolic computing designed for the knowledge transfer task in recommender systems. In particular, we use a Logic Tensor Network (LTN) to train a vanilla Matrix Factorization (MF) model for rating prediction. We show how the LTN can be used to regularize the MF model using axiomatic knowledge that permits injecting pre-trained information learned by Collaborative Filtering on a different task or domain. Extensive experiments comparing our model with a baseline MF on two versions of a novel real-world dataset prove our proposal’s potential in the knowledge transfer task. In particular, our model consistently outperforms the MF, suggesting that the knowledge is effectively transferred to the target domain via logical reasoning. Moreover, an experiment that drastically decreases the density of user-item ratings shows that the benefits of the acquired knowledge increase with the sparsity of the dataset, showing the importance of exploiting knowledge from a denser source of information when training data is scarce in the target domain.},
	language = {en},
	author = {Carraro, Tommaso and Daniele, Alessandro and Aiolli, Fabio and Serafini, Luciano},
}

@inproceedings{wu_symbol-llm_2023,
	title = {Symbol-{LLM}: {Leverage} {Language} {Models} for {Symbolic} {System} in {Visual} {Human} {Activity} {Reasoning}},
	shorttitle = {Symbol-{LLM}},
	url = {https://openreview.net/forum?id=RJq9bVEf6N},
	abstract = {Human reasoning can be understood as a cooperation between the intuitive, associative "System-1'' and the deliberative, logical "System-2''. For existing System-1-like methods in visual activity understanding, it is crucial to integrate System-2 processing to improve explainability, generalization, and data efficiency. One possible path of activity reasoning is building a symbolic system composed of symbols and rules, where one rule connects multiple symbols, implying human knowledge and reasoning abilities. Previous methods have made progress, but are defective with limited symbols from handcraft and limited rules from visual-based annotations, failing to cover the complex patterns of activities and lacking compositional generalization. To overcome the defects, we propose a new symbolic system with two ideal important properties: broad-coverage symbols and rational rules. Collecting massive human knowledge via manual annotations is expensive to instantiate this symbolic system. Instead, we leverage the recent advancement of LLMs (Large Language Models) as an approximation of the two ideal properties, i.e., Symbols from Large Language Models (Symbol-LLM). Then, given an image, visual contents from the images are extracted and checked as symbols and activity semantics are reasoned out based on rules via fuzzy logic calculation. Our method shows superiority in extensive activity understanding tasks. Code and data are available at https://mvig-rhos.com/symbol\_llm.},
	language = {en},
	urldate = {2023-11-28},
	author = {Wu, Xiaoqian and Li, Yong-Lu and Sun, Jianhua and Lu, Cewu},
	month = nov,
	year = {2023},
}

@inproceedings{shengyuan_differentiable_2023,
	title = {Differentiable {Neuro}-{Symbolic} {Reasoning} on {Large}-{Scale} {Knowledge} {Graphs}},
	shorttitle = {{DiffLog}},
	url = {https://openreview.net/forum?id=bETvUctiTR},
	abstract = {Knowledge graph (KG) reasoning utilizes two primary techniques, i.e., rule-based and KG-embedding based. The former provides precise inferences, but inferring via concrete rules is not scalable. The latter enables efficient reasoning at the cost of ambiguous inference accuracy. Neuro-symbolic reasoning seeks to amalgamate the advantages of both techniques. The crux of this approach is replacing the predicted existence of all possible triples (i.e., truth scores inferred from rules) with a suitable approximation grounded in embedding representations. However, constructing an effective approximation of all possible triples' truth scores is a challenging task, because it needs to balance the tradeoff between accuracy and efficiency, while compatible with both the rule-based and KG-embedding models. To this end, we proposed a differentiable framework - DiffLogic. Instead of directly approximating all possible triples, we design a tailored filter to adaptively select essential triples based on the dynamic rules and weights. The truth scores assessed by KG-embedding are continuous, so we employ a continuous Markov logic network named probabilistic soft logic (PSL). It employs the truth scores of essential triples to assess the overall agreement among rules, weights, and observed triples. PSL enables end-to-end differentiable optimization, so we can alternately update embedding and weighted rules. On benchmark datasets, we empirically show that DiffLogic surpasses baselines in both effectiveness and efficiency.},
	language = {en},
	urldate = {2023-11-28},
	author = {Shengyuan, Chen and Cai, Yunfeng and Fang, Huang and Huang, Xiao and Sun, Mingming},
	month = nov,
	year = {2023},
}

@inproceedings{gong_activity_2023,
	title = {Activity {Grammars} for {Temporal} {Action} {Segmentation}},
	shorttitle = {{KARI}},
	url = {https://openreview.net/forum?id=oOXZ5JEjPb},
	abstract = {Sequence prediction on temporal data requires the ability to understand compositional structures of multi-level semantics beyond individual and contextual properties of parts. The task of temporal action segmentation remains challenging for the reason, aiming at translating an untrimmed activity video into a sequence of action segments. This paper addresses the problem by introducing an effective activity grammar to guide neural predictions for temporal action segmentation. We propose a novel grammar induction algorithm, dubbed KARI, that extracts a powerful context-free grammar from action sequence data. We also develop an efficient generalized parser, dubbed BEP, that transforms frame-level probability distributions into a reliable sequence of actions according to the induced grammar with recursive rules. Our approach can be combined with any neural network for temporal action segmentation to enhance the sequence prediction and discover its compositional structure. Experimental results demonstrate that our method significantly improves temporal action segmentation in terms of both performance and interpretability on two standard benchmarks, Breakfast and 50 Salads.},
	language = {en},
	urldate = {2023-11-28},
	author = {Gong, Dayoung and Lee, Joonseok and Jung, Deunsol and Kwak, Suha and Cho, Minsu},
	month = nov,
	year = {2023},
}

@misc{olausson_linc_2023,
	title = {{LINC}: {A} {Neurosymbolic} {Approach} for {Logical} {Reasoning} by {Combining} {Language} {Models} with {First}-{Order} {Logic} {Provers}},
	shorttitle = {{LINC}},
	url = {http://arxiv.org/abs/2310.15164},
	abstract = {Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38\% and 10\%, respectively. When used with GPT-4, LINC scores 26\% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available at https://github.com/benlipkin/linc},
	language = {en},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Olausson, Theo X. and Gu, Alex and Lipkin, Benjamin and Zhang, Cedegao E. and Solar-Lezama, Armando and Tenenbaum, Joshua B. and Levy, Roger},
	month = oct,
	year = {2023},
	note = {arXiv:2310.15164 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{sir_computational_nodate,
	title = {A {Computational} {Perspective} on {Neural}-{Symbolic} {Integration}},
	abstract = {Neural-Symbolic Integration (NSI) aims to marry the principles of symbolic AI techniques, such as logical reasoning
or planning, with the learning capabilities of deep neural networks. In recent years, there have been many systems proposed to address this integration in a seemingly efficient manner. However, from the computational perspective, this is in principle impossible to do. Specifically, some of the core symbolic AI tasks are provably hard, hence a general neural-symbolic system necessarily needs to adopt this computational complexity, too. This might seem as a substantial downside from the perspective of modern large-scale deep learning, which most of the NSI systems try to circumvent by dropping parts of the symbolic AI capabilities. In this position paper, we argue that it might be better to openly accept and address this complexity as a foundational part of the NSI challenge, enabling to properly cover both the existing paradigms as a special case. From the computational perspective, this has important implications on the data and learning representations, the structure of the resulting computation graphs, and the underlying hardware and software stacks. Particularly, we argue that the currently prominent, tensor-based deep learning is insufficient as a foundation for general NSI, which we discuss in a wider context of established symbolic and relational learning methods, and outline some promising computational directions towards fully expressive and efficient NSI.},
	author = {Šír, Gustav},
}

@article{moerland_model-based_2023,
	title = {Model-based {Reinforcement} {Learning}: {A} {Survey}},
	volume = {16},
	issn = {1935-8237, 1935-8245},
	shorttitle = {Model-based {Reinforcement} {Learning}},
	url = {https://www.nowpublishers.com/article/Details/MAL-086},
	doi = {10.1561/2200000086},
	abstract = {Model-based Reinforcement Learning: A Survey},
	language = {English},
	number = {1},
	urldate = {2023-11-17},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Moerland, Thomas M. and Broekens, Joost and Plaat, Aske and Jonker, Catholijn M.},
	month = jan,
	year = {2023},
	note = {Publisher: Now Publishers, Inc.},
	pages = {1--118},
}

@misc{ibarz_generalist_2022,
	title = {A {Generalist} {Neural} {Algorithmic} {Learner}},
	url = {http://arxiv.org/abs/2209.11142},
	abstract = {The cornerstone of neural algorithmic reasoning is the ability to solve algorithmic tasks, especially in a way that generalises out of distribution. While recent years have seen a surge in methodological improvements in this area, they mostly focused on building specialist models. Specialist models are capable of learning to neurally execute either only one algorithm or a collection of algorithms with identical control-flow backbone. Here, instead, we focus on constructing a generalist neural algorithmic learner -- a single graph neural network processor capable of learning to execute a wide range of algorithms, such as sorting, searching, dynamic programming, path-finding and geometry. We leverage the CLRS benchmark to empirically show that, much like recent successes in the domain of perception, generalist algorithmic learners can be built by "incorporating" knowledge. That is, it is possible to effectively learn algorithms in a multi-task manner, so long as we can learn to execute them well in a single-task regime. Motivated by this, we present a series of improvements to the input representation, training regime and processor architecture over CLRS, improving average single-task performance by over 20\% from prior art. We then conduct a thorough ablation of multi-task learners leveraging these improvements. Our results demonstrate a generalist learner that effectively incorporates knowledge captured by specialist models.},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Ibarz, Borja and Kurin, Vitaly and Papamakarios, George and Nikiforou, Kyriacos and Bennani, Mehdi and Csordás, Róbert and Dudzik, Andrew and Bošnjak, Matko and Vitvitskyi, Alex and Rubanova, Yulia and Deac, Andreea and Bevilacqua, Beatrice and Ganin, Yaroslav and Blundell, Charles and Veličković, Petar},
	month = dec,
	year = {2022},
	note = {arXiv:2209.11142 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{green_provenance_2007,
	address = {New York, NY, USA},
	series = {{PODS} '07},
	title = {Provenance semirings},
	isbn = {978-1-59593-685-1},
	url = {https://dl.acm.org/doi/10.1145/1265530.1265535},
	doi = {10.1145/1265530.1265535},
	abstract = {We show that relational algebra calculations for incomplete databases, probabilistic databases, bag semantics and why-provenance are particular cases of the same general algorithms involving semirings. This further suggests a comprehensive provenance representation that uses semirings of polynomials. We extend these considerations to datalog and semirings of formal power series. We give algorithms for datalog provenance calculation as well as datalog evaluation for incomplete and probabilistic databases. Finally, we show that for some semirings containment of conjunctive queries is the same as for standard set semantics.},
	urldate = {2023-11-15},
	booktitle = {Proceedings of the twenty-sixth {ACM} {SIGMOD}-{SIGACT}-{SIGART} symposium on {Principles} of database systems},
	publisher = {Association for Computing Machinery},
	author = {Green, Todd J. and Karvounarakis, Grigoris and Tannen, Val},
	month = jun,
	year = {2007},
	keywords = {data lineage, data provenance, datalog, formal power series, incomplete databases, probabilistic databases, semirings},
	pages = {31--40},
}

@article{tsamoura_neural-symbolic_2021,
	title = {Neural-{Symbolic} {Integration}: {A} {Compositional} {Perspective}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{NeuroLog}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16639},
	doi = {10.1609/aaai.v35i6.16639},
	abstract = {Despite significant progress in the development of neural-symbolic frameworks, the question of how to integrate a neural and a symbolic system in a compositional manner remains open. Our work seeks to fill this gap by treating these two systems as black boxes to be integrated as modules into a single architecture, without making assumptions on their internal structure and semantics. Instead, we expect only that each module exposes certain methods for accessing the functions that the module implements: the symbolic module exposes a deduction method for computing the function's output on a given input, and an abduction method for computing the function's inputs for a given output; the neural module exposes a deduction method for computing the function's output on a given input, and an induction method for updating the function given input-output training instances. We are, then, able to show that a symbolic module --- with any choice for syntax and semantics, as long as the deduction and abduction methods are exposed --- can be cleanly integrated with a neural module, and facilitate the latter's efficient training, achieving empirical performance that exceeds that of previous work.},
	language = {en},
	number = {6},
	urldate = {2023-11-15},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tsamoura, Efthymia and Hospedales, Timothy and Michael, Loizos},
	month = may,
	year = {2021},
	note = {Number: 6},
	keywords = {Neuro-Symbolic AI (NSAI)},
	pages = {5051--5060},
}

@article{nguyen_quoc_enhancing_2020,
	title = {Enhancing {Linear} {Algebraic} {Computation} of {Logic} {Programs} {Using} {Sparse} {Representation}},
	volume = {325},
	issn = {2075-2180},
	url = {http://arxiv.org/abs/2009.10247v1},
	doi = {10.4204/EPTCS.325.24},
	language = {en},
	urldate = {2023-11-13},
	journal = {Electronic Proceedings in Theoretical Computer Science},
	author = {Nguyen Quoc, Tuan and Inoue, Katsumi and Sakama, Chiaki},
	month = sep,
	year = {2020},
	pages = {192--205},
}

@article{biamonte_tensor_2015,
	title = {Tensor {Network} {Contractions} for \#{SAT}},
	volume = {160},
	issn = {1572-9613},
	url = {https://doi.org/10.1007/s10955-015-1276-z},
	doi = {10.1007/s10955-015-1276-z},
	abstract = {The computational cost of counting the number of solutions satisfying a Boolean formula, which is a problem instance of \#SAT, has proven subtle to quantify. Even when finding individual satisfying solutions is computationally easy (e.g. 2-SAT, which is in \$\${\textbackslash}mathsf\{\{P\}\}\$\$), determining the number of solutions can be \#\$\${\textbackslash}mathsf\{\{P\}\}\$\$-hard. Recently, computational methods simulating quantum systems experienced advancements due to the development of tensor network algorithms and associated quantum physics-inspired techniques. By these methods, we give an algorithm using an axiomatic tensor contraction language for n-variable \#SAT instances with complexity \$\$O((g+cd){\textasciicircum}\{O(1)\} 2{\textasciicircum}c)\$\$where c is the number of COPY-tensors, g is the number of gates, and d is the maximal degree of any COPY-tensor. Thus, n-variable counting problems can be solved efficiently when their tensor network expression has at most \$\$O({\textbackslash}log n)\$\$COPY-tensors and polynomial fan-out. This framework also admits an intuitive proof of a variant of the Tovey conjecture (the r,1-SAT instance of the Dubois–Tovey theorem). This study increases the theory, expressiveness and application of tensor based algorithmic tools and provides an alternative insight on these problems which have a long history in statistical physics and computer science.},
	language = {en},
	number = {5},
	urldate = {2023-11-09},
	journal = {Journal of Statistical Physics},
	author = {Biamonte, Jacob D. and Morton, Jason and Turner, Jacob},
	month = sep,
	year = {2015},
	keywords = {Complexity, Computational physics, Quantum physics, Statistical physics},
	pages = {1389--1404},
}

@article{dalvi_dichotomy_2013,
	title = {The dichotomy of probabilistic inference for unions of conjunctive queries},
	volume = {59},
	issn = {0004-5411},
	url = {https://dl.acm.org/doi/10.1145/2395116.2395119},
	doi = {10.1145/2395116.2395119},
	abstract = {We study the complexity of computing a query on a probabilistic database. We consider unions of conjunctive queries, UCQ, which are equivalent to positive, existential First Order Logic sentences, and also to nonrecursive datalog programs. The tuples in the database are independent random events. We prove the following dichotomy theorem. For every UCQ query, either its probability can be computed in polynomial time in the size of the database, or is \#P-hard. Our result also has applications to the problem of computing the probability of positive, Boolean expressions, and establishes a dichotomy for such classes based on their structure. For the tractable case, we give a very simple algorithm that alternates between two steps: applying the inclusion/exclusion formula, and removing one existential variable. A key and novel feature of this algorithm is that it avoids computing terms that cancel out in the inclusion/exclusion formula, in other words it only computes those terms whose Mobius function in an appropriate lattice is nonzero. We show that this simple feature is a key ingredient needed to ensure completeness. For the hardness proof, we give a reduction from the counting problem for positive, partitioned 2CNF, which is known to be \#P-complete. The hardness proof is nontrivial, and combines techniques from logic, classical algebra, and analysis.},
	number = {6},
	urldate = {2023-10-21},
	journal = {Journal of the ACM},
	author = {Dalvi, Nilesh and Suciu, Dan},
	month = jan,
	year = {2013},
	keywords = {Mobius function, Mobius inversion formula, probabilistic database},
	pages = {30:1--30:87},
}

@inproceedings{poole_first-order_2003,
	title = {First-order probabilistic inference},
	abstract = {There have been many proposals for first-order belief networks (i.e., where we quantify over individuals) but these typically only let us reason about the individuals that we know about. There are many instances where we have to quantify over all of the individuals in a population. When we do this the population size often matters and we need to reason about all of the members of the population (but not necessarily individually). This paper presents an algorithm to reason about multiple individuals, where we may know particular facts about some of them, but want to treat the others as a group. Combining unification with variable elimination lets us reason about classes of individuals without needing to ground out the theory.},
	language = {English},
	author = {Poole, D.},
	year = {2003},
	note = {ISSN: 1045-0823},
	pages = {985--991},
}

@inproceedings{kuzelka_counting_2023,
	address = {Macau, SAR China},
	title = {Counting and {Sampling} {Models} in {First}-{Order} {Logic}},
	isbn = {978-1-956792-03-4},
	url = {https://www.ijcai.org/proceedings/2023/801},
	doi = {10.24963/ijcai.2023/801},
	abstract = {First-order model counting (FOMC) is the task of counting models of a first-order logic sentence over a given set of domain elements. Its weighted variant, WFOMC, generalizes FOMC by assigning weights to the models and has many applications in statistical relational learning. More than ten years of research by various authors has led to identification of non-trivial classes of WFOMC problems that can be solved in time polynomial in the number of domain elements. In this paper, we describe recent works on WFOMC and the related problem of weighted first-order model sampling (WFOMS). We also discuss possible applications of WFOMC and WFOMS within statistical relational learning and beyond, e.g., automated solving of problems from enumerative combinatorics and elementary probability theory. Finally, we mention research problems that still need to be tackled in order to make applications of these methods really practical more broadly.},
	language = {en},
	urldate = {2023-10-15},
	booktitle = {Proceedings of the {Thirty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Kuželka, Ondřej},
	month = aug,
	year = {2023},
	pages = {7020--7025},
}

@inproceedings{grefenstette_towards_2013,
	address = {Atlanta, Georgia, USA},
	title = {Towards a {Formal} {Distributional} {Semantics}: {Simulating} {Logical} {Calculi} with {Tensors}},
	shorttitle = {Towards a {Formal} {Distributional} {Semantics}},
	url = {https://aclanthology.org/S13-1001},
	urldate = {2023-10-13},
	booktitle = {Second {Joint} {Conference} on {Lexical} and {Computational} {Semantics} (*{SEM}), {Volume} 1: {Proceedings} of the {Main} {Conference} and the {Shared} {Task}: {Semantic} {Textual} {Similarity}},
	publisher = {Association for Computational Linguistics},
	author = {Grefenstette, Edward},
	month = jun,
	year = {2013},
	pages = {1--10},
}

@article{riguzzi_survey_2017,
	title = {A survey of lifted inference approaches for probabilistic logic programming under the distribution semantics},
	volume = {80},
	issn = {0888-613X},
	url = {https://www.sciencedirect.com/science/article/pii/S0888613X16301736},
	doi = {10.1016/j.ijar.2016.10.002},
	abstract = {Lifted inference aims at answering queries from statistical relational models by reasoning on populations of individuals as a whole instead of considering each individual singularly. Since the initial proposal by David Poole in 2003, many lifted inference techniques have appeared, by lifting different algorithms or using approximation involving different kinds of models, including parfactor graphs and Markov Logic Networks. Very recently lifted inference was applied to Probabilistic Logic Programming (PLP) under the distribution semantics, with proposals such as LP2 and Weighted First-Order Model Counting (WFOMC). Moreover, techniques for dealing with aggregation parfactors can be directly applied to PLP. In this paper we survey these approaches and present an experimental comparison on five models. The results show that WFOMC outperforms the other approaches, being able to exploit more symmetries.},
	urldate = {2023-10-11},
	journal = {International Journal of Approximate Reasoning},
	author = {Riguzzi, Fabrizio and Bellodi, Elena and Zese, Riccardo and Cota, Giuseppe and Lamma, Evelina},
	month = jan,
	year = {2017},
	keywords = {Distribution semantics, Lifted inference, ProbLog, Probabilistic logic programming, Statistical relational artificial intelligence, Variable elimination},
	pages = {313--333},
}

@inproceedings{dries_declarative_2015,
	address = {New York, NY, USA},
	series = {{SoICT} '15},
	title = {Declarative {Data} {Generation} with {ProbLog}},
	isbn = {978-1-4503-3843-1},
	url = {https://dl.acm.org/doi/10.1145/2833258.2833267},
	doi = {10.1145/2833258.2833267},
	abstract = {In this paper we describe a novel declarative approach to data generation based on probabilistic logic programming. We show that many data generation tasks can be described as a probabilistic logic program. To this end, we extend the ProbLog language with continuous distributions and we develop a simple sampling algorithm for this language. We demonstrate that many data generation tasks can be described as a model in this language and we provide examples of generators for attribute-value data, sequences, graphs and logical interpretations and we show how to model common extensions such as noise, missing values and concept drift.},
	urldate = {2023-10-11},
	booktitle = {Proceedings of the 6th {International} {Symposium} on {Information} and {Communication} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Dries, Anton},
	month = dec,
	year = {2015},
	keywords = {Data generation, Probabilistic logic programming},
	pages = {17--24},
}

@article{cappart_combinatorial_2023,
	title = {Combinatorial {Optimization} and {Reasoning} with {Graph} {Neural} {Networks}},
	volume = {24},
	issn = {1533-7928},
	shorttitle = {Combinatorics \& {GNNs}},
	url = {http://jmlr.org/papers/v24/21-0449.html},
	abstract = {Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning, especially graph neural networks, as a key building block for combinatorial tasks, either directly as solvers or by enhancing exact solvers. The inductive bias of GNNs effectively encodes combinatorial and relational input due to their invariance to permutations and awareness of input sparsity. This paper presents a conceptual review of recent key advancements in this emerging field, aiming at optimization and machine learning researchers.},
	number = {130},
	urldate = {2023-05-27},
	journal = {Journal of Machine Learning Research},
	author = {Cappart, Quentin and Chételat, Didier and Khalil, Elias B. and Lodi, Andrea and Morris, Christopher and Velickovic, Petar},
	year = {2023},
	pages = {1--61},
}

@misc{delong_neurosymbolic_2023,
	title = {Neurosymbolic {AI} for {Reasoning} on {Graph} {Structures}: {A} {Survey}},
	shorttitle = {{NeSy} on {Graphs}},
	url = {http://arxiv.org/abs/2302.07200},
	abstract = {Neurosymbolic AI is an increasingly active area of research which aims to combine symbolic reasoning methods with deep learning to generate models with both high predictive performance and some degree of human-level comprehensibility. As knowledge graphs are becoming a popular way to represent heterogeneous and multi-relational data, methods for reasoning on graph structures have attempted to follow this neurosymbolic paradigm. Traditionally, such approaches have utilized either rule-based inference or generated representative numerical embeddings from which patterns could be extracted. However, several recent studies have attempted to bridge this dichotomy in ways that facilitate interpretability, maintain performance, and integrate expert knowledge. Within this article, we survey a breadth of methods that perform neurosymbolic reasoning tasks on graph structures. To better compare the various methods, we propose a novel taxonomy by which we can classify them. Speciﬁcally, we propose three major categories: (1) logicallyinformed embedding approaches, (2) embedding approaches with logical constraints, and (3) rule-learning approaches. Alongside the taxonomy, we provide a tabular overview of the approaches and links to their source code, if available, for more direct comparison. Finally, we discuss the applications on which these methods were primarily used and propose several prospective directions toward which this new ﬁeld of research could evolve.},
	language = {en},
	urldate = {2023-02-18},
	publisher = {arXiv},
	author = {DeLong, Lauren Nicole and Mir, Ramon Fernández and Whyte, Matthew and Ji, Zonglin and Fleuriot, Jacques D.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07200 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Statistics - Machine Learning},
}

@misc{huang_towards_2022,
	title = {Towards {Reasoning} in {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Reasoning in {LLMs}},
	url = {http://arxiv.org/abs/2212.10403},
	abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
	language = {en},
	urldate = {2023-02-09},
	publisher = {arXiv},
	author = {Huang, Jie and Chang, Kevin Chen-Chuan},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10403 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{garcez_neurosymbolic_2023,
	title = {Neurosymbolic {AI}: the 3rd wave},
	issn = {1573-7462},
	shorttitle = {{NeSy}},
	url = {https://doi.org/10.1007/s10462-023-10448-w},
	doi = {10.1007/s10462-023-10448-w},
	abstract = {Current advances in Artificial Intelligence (AI) and Machine Learning have achieved unprecedented impact across research communities and industry. Nevertheless, concerns around trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neurosymbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability by offering symbolic representations for neural models. In this paper, we relate recent and early research in neurosymbolic AI with the objective of identifying the most important ingredients of neurosymbolic AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. Finally, this review identifies promising directions and challenges for the next decade of AI research from the perspective of neurosymbolic computing, commonsense reasoning and causal explanation.},
	language = {en},
	urldate = {2023-05-17},
	journal = {Artificial Intelligence Review},
	author = {Garcez, Artur d’Avila and Lamb, Luís C.},
	month = mar,
	year = {2023},
	keywords = {Cognitive reasoning, Deep learning, Explainable AI, Machine learning, Neurosymbolic AI, Reasoning, Trustworthy AI},
}

@article{zhang_neural_2021,
	title = {Neural, symbolic and neural-symbolic reasoning on knowledge graphs},
	volume = {2},
	issn = {2666-6510},
	shorttitle = {{NeSy} \& {Knowledge} graphs},
	url = {https://www.sciencedirect.com/science/article/pii/S2666651021000061},
	doi = {10.1016/j.aiopen.2021.03.001},
	abstract = {Knowledge graph reasoning is the fundamental component to support machine learning applications such as information extraction, information retrieval, and recommendation. Since knowledge graphs can be viewed as the discrete symbolic representations of knowledge, reasoning on knowledge graphs can naturally leverage the symbolic techniques. However, symbolic reasoning is intolerant of the ambiguous and noisy data. On the contrary, the recent advances of deep learning have promoted neural reasoning on knowledge graphs, which is robust to the ambiguous and noisy data, but lacks interpretability compared to symbolic reasoning. Considering the advantages and disadvantages of both methodologies, recent efforts have been made on combining the two reasoning methods. In this survey, we take a thorough look at the development of the symbolic, neural and hybrid reasoning on knowledge graphs. We survey two specific reasoning tasks — knowledge graph completion and question answering on knowledge graphs, and explain them in a unified reasoning framework. We also briefly discuss the future directions for knowledge graph reasoning.},
	language = {en},
	urldate = {2023-05-17},
	journal = {AI Open},
	author = {Zhang, Jing and Chen, Bo and Zhang, Lingxi and Ke, Xirui and Ding, Haipeng},
	month = jan,
	year = {2021},
	keywords = {Knowledge graph embedding, Knowledge graph reasoning, Neural-symbolic reasoning, Symbolic reasoning},
	pages = {14--35},
}

@misc{mandi_decision-focused_2023,
	title = {Decision-{Focused} {Learning}: {Foundations}, {State} of the {Art}, {Benchmark} and {Future} {Opportunities}},
	shorttitle = {Decision-{Focused} {Learning}},
	url = {http://arxiv.org/abs/2307.13565},
	doi = {10.48550/arXiv.2307.13565},
	abstract = {Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models, introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.},
	urldate = {2023-10-10},
	publisher = {arXiv},
	author = {Mandi, Jayanta and Kotary, James and Berden, Senne and Mulamba, Maxime and Bucarey, Victor and Guns, Tias and Fioretto, Ferdinando},
	month = aug,
	year = {2023},
	note = {arXiv:2307.13565 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, leuven},
}

@misc{ren_query2box_2020,
	title = {Query2box: {Reasoning} over {Knowledge} {Graphs} in {Vector} {Space} using {Box} {Embeddings}},
	shorttitle = {Query2box},
	url = {http://arxiv.org/abs/2002.05969},
	abstract = {Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions (∧) and existential quantiﬁers (∃). Handling queries with logical disjunctions (∨) remains an open problem. Here we propose QUERY2BOX, an embedding-based framework for reasoning over arbitrary queries with ∧, ∨, and ∃ operators in massive and incomplete KGs. Our main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. We show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entities. However, we show that by transforming queries into a Disjunctive Normal Form, QUERY2BOX is capable of handling arbitrary logical queries with ∧, ∨, ∃ in a scalable manner. We demonstrate the effectiveness of QUERY2BOX on three large KGs and show that QUERY2BOX achieves up to 25\% relative improvement over the state of the art.},
	language = {en},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Ren, Hongyu and Hu, Weihua and Leskovec, Jure},
	month = feb,
	year = {2020},
	note = {arXiv:2002.05969 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, todo},
}

@misc{loconte_how_2023,
	title = {How to {Turn} {Your} {Knowledge} {Graph} {Embeddings} into {Generative} {Models} via {Probabilistic} {Circuits}},
	url = {http://arxiv.org/abs/2305.15944},
	abstract = {Some of the most successful knowledge graph embedding (KGE) models for link prediction – CP, RESCAL, TUCKER, COMPLEX – can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits – constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.},
	language = {en},
	urldate = {2023-10-08},
	publisher = {arXiv},
	author = {Loconte, Lorenzo and Di Mauro, Nicola and Peharz, Robert and Vergari, Antonio},
	month = may,
	year = {2023},
	note = {arXiv:2305.15944 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, todo},
}

@misc{friedman_learning_2023,
	title = {Learning {Transformer} {Programs}},
	url = {http://arxiv.org/abs/2306.01128},
	abstract = {Recent research in mechanistic interpretability has attempted to reverse-engineer Transformer models by carefully inspecting network weights and activations. However, these approaches require considerable manual effort and still fall short of providing complete, faithful descriptions of the underlying algorithms. In this work, we introduce a procedure for training Transformers that are mechanistically interpretable by design. We build on RASP [Weiss et al., 2021], a programming language that can be compiled into Transformer weights. Instead of compiling human-written programs into Transformers, we design a modified Transformer that can be trained using gradient-based optimization and then be automatically converted into a discrete, human-readable program. We refer to these models as Transformer Programs. To validate our approach, we learn Transformer Programs for a variety of problems, including an in-context learning task, a suite of algorithmic problems (e.g. sorting, recognizing Dyck-languages), and NLP tasks including named entity recognition and text classification. The Transformer Programs can automatically find reasonable solutions, performing on par with standard Transformers of comparable size; and, more importantly, they are easy to interpret. To demonstrate these advantages, we convert Transformers into Python programs and use off-the-shelf code analysis tools to debug model errors and identify the ``circuits'' used to solve different sub-problems. We hope that Transformer Programs open a new path toward the goal of intrinsically interpretable machine learning.},
	language = {en},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Friedman, Dan and Wettig, Alexander and Chen, Danqi},
	month = jun,
	year = {2023},
	note = {arXiv:2306.01128 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zhong_clock_2023,
	title = {The {Clock} and the {Pizza}: {Two} {Stories} in {Mechanistic} {Explanation} of {Neural} {Networks}},
	shorttitle = {The {Clock} and the {Pizza}},
	url = {http://arxiv.org/abs/2306.17844},
	abstract = {Do neural networks, trained on well-understood algorithmic tasks, reliably rediscover known algorithms for solving those tasks? Several recent studies, on tasks ranging from group arithmetic to in-context linear regression, have suggested that the answer is yes. Using modular addition as a prototypical problem, we show that algorithm discovery in neural networks is sometimes more complex. Small changes to model hyperparameters and initializations can induce discovery of qualitatively different algorithms from a fixed training set, and even parallel implementations of multiple such algorithms. Some networks trained to perform modular addition implement a familiar Clock algorithm [1]; others implement a previously undescribed, less intuitive, but comprehensible procedure we term the Pizza algorithm, or a variety of even more complex procedures. Our results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools for characterizing the behavior of neural networks across their algorithmic phase space.},
	language = {en},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
	month = jun,
	year = {2023},
	note = {arXiv:2306.17844 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{liu_neural_2023,
	title = {A {Neural} {Scaling} {Law} from {Lottery} {Ticket} {Ensembling}},
	url = {http://arxiv.org/abs/2310.02258},
	abstract = {Neural scaling laws (NSL) refer to the phenomenon where model performance improves with scale. Sharma \& Kaplan analyzed NSL using approximation theory and predict that MSE losses decay as N −α, α = 4/d, where N is the number of model parameters, and d is the intrinsic input dimension. Although their theory works well for some cases (e.g., ReLU networks), we surprisingly find that a simple 1D problem y = x2 manifests a different scaling law (α = 1) from their predictions (α = 4). We opened the neural networks and found that the new scaling law originates from lottery ticket ensembling: a wider network on average has more "lottery tickets", which are ensembled to reduce the variance of outputs. We support the ensembling mechanism by mechanistically interpreting single neural networks, as well as studying them statistically. We attribute the N −1 scaling law to the "central limit theorem" of lottery tickets. Finally, we discuss its potential implications for large language models and statistical physics-like theories of learning.},
	language = {en},
	urldate = {2023-10-05},
	publisher = {arXiv},
	author = {Liu, Ziming and Tegmark, Max},
	month = oct,
	year = {2023},
	note = {arXiv:2310.02258 [physics, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
}

@misc{lacroix_canonical_2018,
	title = {Canonical {Tensor} {Decomposition} for {Knowledge} {Base} {Completion}},
	url = {http://arxiv.org/abs/1806.07297},
	abstract = {The problem of Knowledge Base Completion can be framed as a 3rd-order binary tensor completion problem. In this light, the Canonical Tensor Decomposition (CP) (Hitchcock, 1927) seems like a natural solution; however, current implementations of CP on standard Knowledge Base Completion benchmarks are lagging behind their competitors. In this work, we attempt to understand the limits of CP for knowledge base completion. First, we motivate and test a novel regularizer, based on tensor nuclear p-norms. Then, we present a reformulation of the problem that makes it invariant to arbitrary choices in the inclusion of predicates or their reciprocals in the dataset. These two methods combined allow us to beat the current state of the art on several datasets with a CP decomposition, and obtain even better results using the more advanced ComplEx model.},
	language = {en},
	urldate = {2023-10-04},
	publisher = {arXiv},
	author = {Lacroix, Timothée and Usunier, Nicolas and Obozinski, Guillaume},
	month = jun,
	year = {2018},
	note = {arXiv:1806.07297 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@inproceedings{chamberlain_grand_2021,
	title = {{GRAND}: {Graph} {Neural} {Diffusion}},
	shorttitle = {{GRAND}},
	url = {https://proceedings.mlr.press/v139/chamberlain21a.html},
	abstract = {We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks.},
	language = {en},
	urldate = {2023-10-02},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chamberlain, Ben and Rowbottom, James and Gorinova, Maria I. and Bronstein, Michael and Webb, Stefan and Rossi, Emanuele},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1407--1418},
}

@inproceedings{pryor_neupsl_2023,
	address = {Macau, SAR China},
	title = {{NeuPSL}: {Neural} {Probabilistic} {Soft} {Logic}},
	isbn = {978-1-956792-03-4},
	shorttitle = {{NeuPSL}},
	url = {https://www.ijcai.org/proceedings/2023/461},
	doi = {10.24963/ijcai.2023/461},
	abstract = {In this paper, we introduce Neural Probabilistic Soft Logic (NeuPSL), a novel neuro-symbolic (NeSy) framework that unites state-of-the-art symbolic reasoning with the low-level perception of deep neural networks. To model the boundary between neural and symbolic representations, we propose a family of energy-based models, NeSy Energy-Based Models, and show that they are general enough to include NeuPSL and many other NeSy approaches. Using this framework, we show how to seamlessly integrate neural and symbolic parameter learning and inference in NeuPSL. Through an extensive empirical evaluation, we demonstrate the benefits of using NeSy methods, achieving upwards of 30\% improvement over independent neural network models. On a wellestablished NeSy task, MNIST-Addition, NeuPSL demonstrates its joint reasoning capabilities by outperforming existing NeSy approaches by up to 10\% in low-data settings. Furthermore, NeuPSL achieves a 5\% boost in performance over state-ofthe-art NeSy methods in a canonical citation network task with up to a 40 times speed up.},
	language = {en},
	urldate = {2023-09-29},
	booktitle = {Proceedings of the {Thirty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Pryor, Connor and Dickens, Charles and Augustine, Eriq and Albalak, Alon and Wang, William Yang and Getoor, Lise},
	month = aug,
	year = {2023},
	pages = {4145--4153},
}

@inproceedings{julian-iranzo_bousiprolog_2010,
	address = {Valencia, Spain},
	title = {{BOUSI}∼{PROLOG} - {A} {Fuzzy} {Logic} {Programming} {Language} for {Modeling} {Vague} {Knowledge} and {Approximate} {Reasoning}:},
	isbn = {978-989-8425-32-4},
	shorttitle = {{BOUSI}∼{PROLOG}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0003079200930098},
	doi = {10.5220/0003079200930098},
	language = {en},
	urldate = {2023-03-30},
	booktitle = {Proceedings of the {International} {Conference} on {Fuzzy} {Computation} and 2nd {International} {Conference} on {Neural} {Computation}},
	publisher = {SciTePress - Science and and Technology Publications},
	author = {Julián-Iranzo, Pascual and Rubio-Manzano, Clemente},
	year = {2010},
	pages = {93--98},
}

@article{gurel_knowledge_nodate,
	title = {Knowledge {Enhanced} {Machine} {Learning} {Pipeline}  against {Diverse} {Adversarial} {Attacks}},
	shorttitle = {{KEMLP}},
	abstract = {Despite the great successes achieved by deep neural networks (DNNs), recent studies show that they are vulnerable against adversarial examples, which aim to mislead DNNs by adding small adversarial perturbations. Several defenses have been proposed against such attacks, while many of them have been adaptively attacked. In this work, we aim to enhance the ML robustness from a different perspective by leveraging domain knowledge: We propose a Knowledge Enhanced Machine Learning Pipeline (KEMLP) to integrate domain knowledge (i.e., logic relationships among different predictions) into a probabilistic graphical model via ﬁrst-order logic rules. In particular, we develop KEMLP by integrating a diverse set of weak auxiliary models based on their logical relationships to the main DNN model that performs the target task. Theoretically, we provide convergence results and prove that, under mild conditions, the prediction of KEMLP is more robust than that of the main DNN model. Empirically, we take road sign recognition as an example and leverage the relationships between road signs and their shapes and contents as domain knowledge. We show that compared with adversarial training and other baselines, KEMLP achieves higher robustness against physical attacks, Lp bounded attacks, unforeseen attacks, and natural corruptions under both whitebox and blackbox settings, while still maintaining high clean accuracy.},
	language = {en},
	author = {Gürel, Nezihe Merve and Qi, Xiangyu and Rimanic, Luka and Zhang, Ce and Li, Bo},
	keywords = {todo},
}

@misc{dusell_learning_2022,
	title = {Learning {Context}-{Free} {Languages} with {Nondeterministic} {Stack} {RNNs}},
	url = {http://arxiv.org/abs/2010.04674},
	abstract = {We present a diﬀerentiable stack data structure that simultaneously and tractably encodes an exponential number of stack conﬁgurations, based on Lang’s algorithm for simulating nondeterministic pushdown automata. We call the combination of this data structure with a recurrent neural network (RNN) controller a Nondeterministic Stack RNN. We compare our model against existing stack RNNs on various formal languages, demonstrating that our model converges more reliably to algorithmic behavior on deterministic tasks, and achieves lower cross-entropy on inherently nondeterministic tasks.},
	language = {en},
	urldate = {2023-09-06},
	publisher = {arXiv},
	author = {DuSell, Brian and Chiang, David},
	month = nov,
	year = {2022},
	note = {arXiv:2010.04674 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{sessa_approximate_2002,
	title = {Approximate reasoning by similarity-based {SLD} resolution},
	volume = {275},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/S0304397501001888},
	doi = {10.1016/S0304-3975(01)00188-8},
	abstract = {In (Gerla and Sessa, Fuzzy Logic and Soft Computing, Kluwer, Norwell, 1999, pp. 19–31) a methodology that allows to manage uncertain and imprecise information in the frame of the declarative paradigm of Logic Programming has been proposed. With this aim, a Similarity relation R between function and predicate symbols in the language of a logic program is considered. Approximate inferences are then possible since similarity relation allows us to manage alternative instances of entities that can be considered “equal” with a given degree. The declarative semantics of the proposed transformation technique of logic programs is analyzed. The notion of fuzzy least Herbrand model is also introduced. In this paper the corresponding operational semantics is provided by introducing a modified version of SLD resolution. This top-down refutation procedure overcomes failure situations in the unification process by using the similarity relation. A generalized notion of most general unifier provides a numeric value which gives a measure of the exploited approximation. In this way, the SLD resolution is enhanced since it is possible both to handle uncertain or imprecise information, and to compute approximate answer substitutions, with an associated approximation-degree, when failures of the exact inference process occur. It can lead to the implementation of a more general PROLOG interpreter, without detracting from the elegance of the language.},
	language = {en},
	number = {1},
	urldate = {2023-03-30},
	journal = {Theoretical Computer Science},
	author = {Sessa, Maria I.},
	month = mar,
	year = {2002},
	keywords = {Approximate reasoning, Logic programming, SLD resolution, Similarity relation},
	pages = {389--426},
}

@inproceedings{costa_clpbn_2002,
	address = {San Francisco, CA, USA},
	series = {{UAI}'03},
	title = {{CLP}({BN}): constraint logic programming for probabilistic knowledge},
	isbn = {978-0-12-705664-7},
	shorttitle = {{CLP}({BN})},
	abstract = {In Datalog, missing values are represented by Skolem constants. More generally, in logic programming missing values, or existentially-quantified variables, are represented by terms built from Skolem functors. In an analogy to probabilistic relational models (PRMs), we wish to represent the joint probability distribution over missing values in a database or logic program using a Bayesian network. This paper presents an extension of logic programs that makes it possible to specify a joint probability distribution over terms built from Skolem functors in the program. Our extension is based on constraint logic programming (CLP), so we call the extended language CLP(BN). We show that CLP(BN) subsumes PRMs; this greater expressivity carries both advantages and disadvantages for CLP(BN). We also show that algorithms from inductive logic programming (ILP) can be used with only minor modification to learn CLP(BN) programs. An implementation of CLP(BN) is publicly available as part of YAP Prolog at http://www.cos.ufrj.br/{\textasciitilde}vitor/Yap/clpbn},
	urldate = {2023-09-27},
	booktitle = {Proceedings of the {Nineteenth} conference on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Costa, Vítor Santos and Page, David and Qazi, Maleeha and Cussens, James},
	month = aug,
	year = {2002},
	pages = {517--524},
}

@article{gray_hyper-optimized_2021,
	title = {Hyper-optimized tensor network contraction},
	volume = {5},
	url = {https://quantum-journal.org/papers/q-2021-03-15-410/},
	doi = {10.22331/q-2021-03-15-410},
	abstract = {Johnnie Gray and Stefanos Kourtis,
Quantum 5, 410 (2021).
Tensor networks represent the state-of-the-art in computational methods across many disciplines, including the classical simulation of quantum many-body systems and quantum circuits. Several…},
	language = {en-GB},
	urldate = {2023-09-21},
	journal = {Quantum},
	author = {Gray, Johnnie and Kourtis, Stefanos},
	month = mar,
	year = {2021},
	note = {Publisher: Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften},
	pages = {410},
}

@misc{gray_hyper-optimized_2022,
	title = {Hyper-optimized compressed contraction of tensor networks with arbitrary geometry},
	url = {http://arxiv.org/abs/2206.07044},
	abstract = {Tensor network contraction is central to problems ranging from many-body physics to computer science. We describe how to approximate tensor network contraction through bond compression on arbitrary graphs. In particular, we introduce a hyper-optimization over the compression and contraction strategy itself to minimize error and cost. We demonstrate that our protocol outperforms both hand-crafted contraction strategies as well as recently proposed general contraction algorithms on a variety of synthetic problems on regular lattices and random regular graphs. We further showcase the power of the approach by demonstrating compressed contraction of tensor networks for frustrated three-dimensional lattice partition functions, dimer counting on random regular graphs, and to access the hardness transition of random tensor network models, in graphs with many thousands of tensors.},
	language = {en},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Gray, Johnnie and Chan, Garnet Kin-Lic},
	month = jun,
	year = {2022},
	note = {arXiv:2206.07044 [cond-mat, physics:quant-ph]},
	keywords = {Condensed Matter - Statistical Mechanics, Condensed Matter - Strongly Correlated Electrons, Quantum Physics},
}

@inproceedings{glasser_expressive_2019,
	title = {Expressive power of tensor-network factorizations for probabilistic modeling},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/b86e8d03fe992d1b0e19656875ee557c-Abstract.html},
	abstract = {Tensor-network techniques have recently proven useful in machine learning, both as a tool for the formulation of new learning algorithms and for enhancing the mathematical understanding of existing methods. Inspired by these developments, and the natural correspondence between tensor networks and probabilistic graphical models, we provide a rigorous analysis of the expressive power of various tensor-network factorizations of discrete multivariate probability distributions. These factorizations include non-negative tensor-trains/MPS, which are in correspondence with hidden Markov models, and Born machines, which are naturally related to the probabilistic interpretation of quantum circuits. When used to model probability distributions, they exhibit tractable likelihoods and admit efficient learning algorithms. Interestingly, we prove that there exist probability distributions for which there are unbounded separations between the resource requirements of some of these tensor-network factorizations. Of particular interest, using complex instead of real tensors can lead to an arbitrarily large reduction in the number of parameters of the network. Additionally, we introduce locally purified states (LPS), a new factorization inspired by techniques for the simulation of quantum systems, with provably better expressive power than all other representations considered. The ramifications of this result are explored through numerical experiments.},
	urldate = {2023-09-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Glasser, Ivan and Sweke, Ryan and Pancotti, Nicola and Eisert, Jens and Cirac, Ignacio},
	year = {2019},
}

@article{robeva_duality_2019,
	title = {Duality of graphical models and tensor networks},
	volume = {8},
	issn = {2049-8772},
	url = {https://doi.org/10.1093/imaiai/iay009},
	doi = {10.1093/imaiai/iay009},
	abstract = {In this article we show the duality between tensor networks and undirected graphical models with discrete variables. We study tensor networks on hypergraphs, which we call tensor hypernetworks. We show that the tensor hypernetwork on a hypergraph exactly corresponds to the graphical model given by the dual hypergraph. We translate various notions under duality. For example, marginalization in a graphical model is dual to contraction in the tensor network. Algorithms also translate under duality. We show that belief propagation corresponds to a known algorithm for tensor network contraction. This article is a reminder that the research areas of graphical models and tensor networks can benefit from interaction.},
	number = {2},
	urldate = {2023-09-20},
	journal = {Information and Inference: A Journal of the IMA},
	author = {Robeva, Elina and Seigal, Anna},
	month = jun,
	year = {2019},
	pages = {273--288},
}

@misc{robeva_duality_2017,
	title = {Duality of {Graphical} {Models} and {Tensor} {Networks}},
	url = {http://arxiv.org/abs/1710.01437},
	abstract = {In this article we show the duality between tensor networks and undirected graphical models with discrete variables. We study tensor networks on hypergraphs, which we call tensor hypernetworks. We show that the tensor hypernetwork on a hypergraph exactly corresponds to the graphical model given by the dual hypergraph. We translate various notions under duality. For example, marginalization in a graphical model is dual to contraction in the tensor network. Algorithms also translate under duality. We show that belief propagation corresponds to a known algorithm for tensor network contraction. This article is a reminder that the research areas of graphical models and tensor networks can beneﬁt from interaction.},
	language = {en},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Robeva, Elina and Seigal, Anna},
	month = oct,
	year = {2017},
	note = {arXiv:1710.01437 [quant-ph, stat]},
	keywords = {Computer Science - Artificial Intelligence, Mathematics - Statistics Theory, Quantum Physics, Statistics - Machine Learning},
}

@article{alkabetz_tensor_2021,
	title = {Tensor networks contraction and the belief propagation algorithm},
	volume = {3},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.3.023073},
	doi = {10.1103/PhysRevResearch.3.023073},
	abstract = {Belief propagation is a well-studied message-passing algorithm that runs over graphical models and can be used for approximate inference and approximation of local marginals. The resulting approximations are equivalent to the Bethe-Peierls approximation of statistical mechanics. Here, we show how this algorithm can be adapted to the world of projected-entangled-pair-state tensor networks and used as an approximate contraction scheme. We further show that the resultant approximation is equivalent to the “mean field” approximation that is used in the simple-update algorithm, thereby showing that the latter is essentially the Bethe-Peierls approximation. This shows that one of the simplest approximate contraction algorithms for tensor networks is equivalent to one of the simplest schemes for approximating marginals in graphical models in general and paves the way for using improvements of belief propagation as tensor networks algorithms.},
	number = {2},
	urldate = {2023-09-19},
	journal = {Physical Review Research},
	author = {Alkabetz, R. and Arad, I.},
	month = apr,
	year = {2021},
	note = {Publisher: American Physical Society},
	pages = {023073},
}

@article{pan_contracting_2020,
	title = {Contracting {Arbitrary} {Tensor} {Networks}: {General} {Approximate} {Algorithm} and {Applications} in {Graphical} {Models} and {Quantum} {Circuit} {Simulations}},
	volume = {125},
	issn = {0031-9007, 1079-7114},
	shorttitle = {Contracting {Arbitrary} {Tensor} {Networks}},
	url = {http://arxiv.org/abs/1912.03014},
	doi = {10.1103/PhysRevLett.125.060503},
	abstract = {We present a general method for approximately contracting tensor networks with an arbitrary connectivity. This enables us to release the computational power of tensor networks to wide use in inference and learning problems defined on general graphs. We show applications of our algorithm in graphical models, specifically on estimating free energy of spin glasses defined on various of graphs, where our method largely outperforms existing algorithms including the mean-field methods and the recently proposed neural-network-based methods. We further apply our method to the simulation of random quantum circuits, and demonstrate that, with a trade off of negligible truncation errors, our method is able to simulate large quantum circuits that are out of reach of the state-of-the-art simulation methods.},
	language = {en},
	number = {6},
	urldate = {2023-09-19},
	journal = {Physical Review Letters},
	author = {Pan, Feng and Zhou, Pengfei and Li, Sujie and Zhang, Pan},
	month = aug,
	year = {2020},
	note = {arXiv:1912.03014 [cond-mat, physics:physics, physics:quant-ph]},
	keywords = {Condensed Matter - Statistical Mechanics, Condensed Matter - Strongly Correlated Electrons, Physics - Computational Physics, Quantum Physics},
	pages = {060503},
}

@book{ran_lecture_2020,
	title = {Lecture {Notes} of {Tensor} {Network} {Contractions}},
	volume = {964},
	url = {http://arxiv.org/abs/1708.09213},
	abstract = {Tensor network (TN), a young mathematical tool of high vitality and great potential, has been undergoing extremely rapid developments in the last two decades, gaining tremendous success in condensed matter physics, atomic physics, quantum information science, statistical physics, and so on. In this lecture notes, we focus on the contraction algorithms of TN as well as some of the applications to the simulations of quantum many-body systems. Starting from basic concepts and definitions, we first explain the relations between TN and physical problems, including the TN representations of classical partition functions, quantum many-body states (by matrix product state, tree TN, and projected entangled pair state), time evolution simulations, etc. These problems, which are challenging to solve, can be transformed to TN contraction problems. We present then several paradigm algorithms based on the ideas of the numerical renormalization group and/or boundary states, including density matrix renormalization group, time-evolving block decimation, coarse-graining/corner tensor renormalization group, and several distinguished variational algorithms. Finally, we revisit the TN approaches from the perspective of multi-linear algebra (also known as tensor algebra or tensor decompositions) and quantum simulation. Despite the apparent differences in the ideas and strategies of different TN algorithms, we aim at revealing the underlying relations and resemblances in order to present a systematic picture to understand the TN contraction approaches.},
	language = {en},
	urldate = {2023-09-19},
	author = {Ran, Shi-Ju and Tirrito, Emanuele and Peng, Cheng and Chen, Xi and Tagliacozzo, Luca and Su, Gang and Lewenstein, Maciej},
	year = {2020},
	doi = {10.1007/978-3-030-34489-4},
	note = {arXiv:1708.09213 [cond-mat, physics:physics, physics:quant-ph]},
	keywords = {Condensed Matter - Statistical Mechanics, Condensed Matter - Strongly Correlated Electrons, Physics - Applied Physics, Physics - Computational Physics, Quantum Physics},
}

@misc{dudek_efficient_2020,
	title = {Efficient {Contraction} of {Large} {Tensor} {Networks} for {Weighted} {Model} {Counting} through {Graph} {Decompositions}},
	url = {http://arxiv.org/abs/1908.04381},
	abstract = {Constrained counting is a fundamental problem in artiﬁcial intelligence. A promising new algebraic approach to constrained counting makes use of tensor networks, following a reduction from constrained counting to the problem of tensor-network contraction. Contracting a tensor network eﬃciently requires determining an eﬃcient order to contract the tensors inside the network, which is itself a diﬃcult problem.},
	language = {en},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Dudek, Jeffrey M. and Dueñas-Osorio, Leonardo and Vardi, Moshe Y.},
	month = apr,
	year = {2020},
	note = {arXiv:1908.04381 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Data Structures and Algorithms, Computer Science - Logic in Computer Science},
}

@article{kourtis_fast_2019,
	title = {Fast counting with tensor networks},
	volume = {7},
	issn = {2542-4653},
	url = {https://scipost.org/10.21468/SciPostPhys.7.5.060},
	doi = {10.21468/SciPostPhys.7.5.060},
	abstract = {We introduce tensor network contraction algorithms for counting satisfying assignments of constraint satisfaction problems (\#CSPs). We represent each arbitrary \#CSP formula as a tensor network, whose full contraction yields the number of satisfying assignments of that formula, and use graph theoretical methods to determine favorable orders of contraction. We employ our heuristics for the solution of \#P-hard counting boolean satisﬁability (\#SAT) problems, namely monotone \#1-IN-3SAT and \#CUBIC-VERTEX-COVER, and ﬁnd that they outperform state-of-the-art solvers by a signiﬁcant margin.},
	language = {en},
	number = {5},
	urldate = {2023-09-19},
	journal = {SciPost Physics},
	author = {Kourtis, Stefanos and Chamon, Claudio and Mucciolo, Eduardo and Ruckenstein, Andrei},
	month = nov,
	year = {2019},
	pages = {060},
}

@inproceedings{hommersom_generalising_2011,
	address = {Barcelona, Catalonia, Spain},
	series = {{IJCAI}'11},
	title = {Generalising the interaction rules in probabilistic logic},
	isbn = {978-1-57735-514-4},
	abstract = {The last two decades has seen the emergence of many different probabilistic logics that use logical languages to specify, and sometimes reason, with probability distributions. Probabilistic logics that support reasoning with probability distributions, such as ProbLog, use an implicit definition of an interaction rule to combine probabilistic evidence about atoms. In this paper, we show that this interaction rule is an example of a more general class of interactions that can be described by nonmonotonic logics. We furthermore show that such local interactions about the probability of an atom can be described by convolution. The resulting extended probabilistic logic supports nonmonotonic reasoning with probabilistic information.},
	urldate = {2023-09-13},
	booktitle = {Proceedings of the {Twenty}-{Second} international joint conference on {Artificial} {Intelligence} - {Volume} {Volume} {Two}},
	publisher = {AAAI Press},
	author = {Hommersom, Arjen and Lucas, Peter J. F.},
	month = jul,
	year = {2011},
	pages = {912--917},
}

@article{chaudhuri_neurosymbolic_2021,
	title = {Neurosymbolic {Programming}},
	volume = {7},
	issn = {2325-1107, 2325-1131},
	url = {https://www.nowpublishers.com/article/Details/PGL-049},
	doi = {10.1561/2500000049},
	abstract = {Neurosymbolic Programming},
	language = {English},
	number = {3},
	urldate = {2023-09-12},
	journal = {Foundations and Trends® in Programming Languages},
	author = {Chaudhuri, Swarat and Ellis, Kevin and Polozov, Oleksandr and Singh, Rishabh and Solar-Lezama, Armando and Yue, Yisong},
	month = dec,
	year = {2021},
	note = {Publisher: Now Publishers, Inc.},
	pages = {158--243},
}

@inproceedings{marra_lyrics_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{LYRICS}: {A} {General} {Interface} {Layer} to {Integrate} {Logic} {Inference} and {Deep} {Learning}},
	isbn = {978-3-030-46147-8},
	shorttitle = {{LYRICS}},
	doi = {10.1007/978-3-030-46147-8_17},
	abstract = {In spite of the amazing results obtained by deep learning in many applications, a real intelligent behavior of an agent acting in a complex environment is likely to require some kind of higher-level symbolic inference. Therefore, there is a clear need for the definition of a general and tight integration between low-level tasks, processing sensorial data that can be effectively elaborated using deep learning techniques, and the logic reasoning that allows humans to take decisions in complex environments. This paper presents LYRICS, a generic interface layer for AI, which is implemented in TersorFlow (TF). LYRICS provides an input language that allows to define arbitrary First Order Logic (FOL) background knowledge. The predicates and functions of the FOL knowledge can be bound to any TF computational graph, and the formulas are converted into a set of real-valued constraints, which participate to the overall optimization problem. This allows to learn the weights of the learners, under the constraints imposed by the prior knowledge. The framework is extremely general as it imposes no restrictions in terms of which models or knowledge can be integrated. In this paper, we show the generality of the approach showing some use cases of the presented language, including model checking, supervised learning and collective classification.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Marra, Giuseppe and Giannini, Francesco and Diligenti, Michelangelo and Gori, Marco},
	editor = {Brefeld, Ulf and Fromont, Elisa and Hotho, Andreas and Knobbe, Arno and Maathuis, Marloes and Robardet, Céline},
	year = {2020},
	keywords = {Deep learning, First Order Logic, Prior knowledge injection},
	pages = {283--298},
}

@inproceedings{picco_neural_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Neural {Unification} for {Logic} {Reasoning} over {Natural} {Language}},
	url = {https://aclanthology.org/2021.findings-emnlp.331},
	doi = {10.18653/v1/2021.findings-emnlp.331},
	language = {en},
	urldate = {2023-01-23},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Picco, Gabriele and Hoang, Thanh Lam and Sbodio, Marco Luca and Lopez, Vanessa},
	year = {2021},
	keywords = {todo},
	pages = {3939--3950},
}

@incollection{darwiche_advance_2020,
	title = {An {Advance} on {Variable} {Elimination} with {Applications} to {Tensor}-{Based} {Computation}},
	url = {https://ebooks.iospress.nl/doi/10.3233/FAIA200391},
	urldate = {2023-08-28},
	booktitle = {{ECAI} 2020},
	publisher = {IOS Press},
	author = {Darwiche, Adnan},
	year = {2020},
	doi = {10.3233/FAIA200391},
	pages = {2559--2568},
}

@inproceedings{pan_compiling_2023,
	address = {Montréal QC Canada},
	title = {Compiling {Discrete} {Probabilistic} {Programs} for {Vectorized} {Exact} {Inference}},
	isbn = {9798400700880},
	url = {https://dl.acm.org/doi/10.1145/3578360.3580258},
	doi = {10.1145/3578360.3580258},
	abstract = {Probabilistic programming languages (PPLs) are essential for reasoning under uncertainty. Even though many realworld probabilistic programs involve discrete distributions, the state-of-the-art PPLs are suboptimal for a large class of tasks dealing with such distributions. In this paper, we propose BayesTensor, a tensor-based probabilistic programming framework. By generating tensor algebra code from probabilistic programs, BayesTensor takes advantage of the highly-tuned vectorized implementations of tensor processing frameworks. Our experiments show that BayesTensor outperforms the state-of-the-art frameworks in a variety of discrete probabilistic programs, inference over Bayesian Networks, and real-world probabilistic programs employed in data processing systems.},
	language = {en},
	urldate = {2023-08-28},
	booktitle = {Proceedings of the 32nd {ACM} {SIGPLAN} {International} {Conference} on {Compiler} {Construction}},
	publisher = {ACM},
	author = {Pan, Jingwen and Shaikhha, Amir},
	month = feb,
	year = {2023},
	pages = {13--24},
}

@misc{hooker_hardware_2020,
	title = {The {Hardware} {Lottery}},
	url = {http://arxiv.org/abs/2009.06489},
	doi = {10.48550/arXiv.2009.06489},
	abstract = {Hardware, systems and algorithms research communities have historically had different incentive structures and fluctuating motivation to engage with each other explicitly. This historical treatment is odd given that hardware and software have frequently determined which research ideas succeed (and fail). This essay introduces the term hardware lottery to describe when a research idea wins because it is suited to the available software and hardware and not because the idea is superior to alternative research directions. Examples from early computer science history illustrate how hardware lotteries can delay research progress by casting successful ideas as failures. These lessons are particularly salient given the advent of domain specialized hardware which make it increasingly costly to stray off of the beaten path of research ideas. This essay posits that the gains from progress in computing are likely to become even more uneven, with certain research directions moving into the fast-lane while progress on others is further obstructed.},
	urldate = {2023-08-25},
	publisher = {arXiv},
	author = {Hooker, Sara},
	month = sep,
	year = {2020},
	note = {arXiv:2009.06489 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Hardware Architecture, Computer Science - Machine Learning},
}

@inproceedings{van_den_broeck_lifted_2011,
	title = {Lifted probabilistic inference by first-order knowledge compilation},
	isbn = {978-1-57735-516-8},
	url = {https://lirias.kuleuven.be/1464269},
	doi = {10.5591/978-1-57735-516-8/IJCAI11-363},
	abstract = {Probabilistic logical languages provide powerful formalisms for knowledge representation and learning. Yet performing inference in these languages is extremely costly, especially if it is done at the propositional level. Lifted inference algorithms, which avoid repeated computation by treating indistinguishable groups of objects as one, help mitigate this cost. Seeking inspiration from logical inference, where lifted inference (e.g., resolution) is commonly performed, we develop a model theoretic approach to probabilistic lifted inference.  Our algorithm compiles a first-order probabilistic theory into a first-order deterministic decomposable negation normal form (d-DNNF) circuit.
Compilation offers the advantage that inference is polynomial in the size of the circuit. Furthermore, by borrowing techniques from the knowledge compilation literature our algorithm effectively exploits the logical structure (e.g., context-specific independencies) within the first-order model, which allows more computation to be done at the lifted level.
An empirical comparison demonstrates the utility of the proposed approach.},
	language = {eng},
	urldate = {2023-06-28},
	booktitle = {Proceedings of the {Twenty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press/International Joint Conferences on Artificial Intelligence; Menlo Park, California},
	author = {Van den Broeck, Guy and Taghipour, Nima and Meert, Wannes and Davis, Jesse and De Raedt, Luc and Walsh, Toby},
	month = jul,
	year = {2011},
	note = {ISSN: 978-1-57735-512-0},
	keywords = {leuven},
	pages = {2178--2185},
}

@phdthesis{van_den_broeck_lifted_2013,
	address = {Leuven},
	title = {Lifted {Inference} and {Learning} in {Statistical} {Relational} {Models}},
	language = {en},
	school = {KU Leuven},
	author = {Van den Broeck, Guy},
	month = jan,
	year = {2013},
	keywords = {leuven},
}

@article{noauthor_lifted_nodate,
	title = {Lifted {Inference} and {Learning} in {Statistical} {Relational} {Models}},
	language = {en},
}

@misc{barbiero_interpretable_2023,
	title = {Interpretable {Neural}-{Symbolic} {Concept} {Reasoning}},
	url = {http://arxiv.org/abs/2304.14068},
	abstract = {Deep learning methods are highly accurate, yet their opaque decision process prevents them from earning full human trust. Concept-based models aim to address this issue by learning tasks based on a set of human-understandable concepts. However, state-of-the-art concept-based models rely on high-dimensional concept embedding representations which lack a clear semantic meaning, thus questioning the interpretability of their decision process. To overcome this limitation, we propose the Deep Concept Reasoner (DCR), the ﬁrst interpretable concept-based model that builds upon concept embeddings. In DCR, neural networks do not make task predictions directly, but they build syntactic rule structures using concept embeddings. DCR then executes these rules on meaningful concept truth degrees to provide a ﬁnal interpretable and semantically-consistent prediction in a differentiable manner. Our experiments show that DCR: (i) improves up to +25\% w.r.t. state-of-the-art interpretable concept-based models on challenging benchmarks (ii) discovers meaningful logic rules matching known ground truths even in the absence of concept supervision during training, and (iii), facilitates the generation of counterfactual examples providing the learnt rules as guidance.},
	language = {en},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Barbiero, Pietro and Ciravegna, Gabriele and Giannini, Francesco and Zarlenga, Mateo Espinosa and Magister, Lucie Charlotte and Tonda, Alberto and Lio', Pietro and Precioso, Frederic and Jamnik, Mateja and Marra, Giuseppe},
	month = may,
	year = {2023},
	note = {arXiv:2304.14068 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, leuven},
}

@misc{chughtai_toy_2023,
	title = {A {Toy} {Model} of {Universality}: {Reverse} {Engineering} {How} {Networks} {Learn} {Group} {Operations}},
	shorttitle = {A {Toy} {Model} of {Universality}},
	url = {http://arxiv.org/abs/2302.03025},
	abstract = {Universality is a key hypothesis in mechanistic interpretability – that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned – as well as the order they develop – are arbitrary.},
	language = {en},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
	month = may,
	year = {2023},
	note = {arXiv:2302.03025 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Representation Theory},
}

@misc{granmo_tsetlin_2021,
	title = {The {Tsetlin} {Machine} -- {A} {Game} {Theoretic} {Bandit} {Driven} {Approach} to {Optimal} {Pattern} {Recognition} with {Propositional} {Logic}},
	url = {http://arxiv.org/abs/1804.01508},
	abstract = {Although simple individually, artiﬁcial neurons provide state-of-the-art performance when interconnected in deep networks. Arguably, the Tsetlin Automaton is an even simpler and more versatile learning mechanism, capable of solving the multi-armed bandit problem. Merely by means of a single integer as memory, it learns the optimal action in stochastic environments through increment and decrement operations. In this paper, we introduce the Tsetlin Machine, which solves complex pattern recognition problems with propositional formulas, composed by a collective of Tsetlin Automata. To eliminate the longstanding problem of vanishing signal-to-noise ratio, the Tsetlin Machine orchestrates the automata using a novel game. Further, both inputs, patterns, and outputs are expressed as bits, while recognition and learning rely on bit manipulation, simplifying computation. Our theoretical analysis establishes that the Nash equilibria of the game align with the propositional formulas that provide optimal pattern recognition accuracy. This translates to learning without local optima, only global ones. In ﬁve benchmarks, the Tsetlin Machine provides competitive accuracy compared with SVMs, Decision Trees, Random Forests, Naive Bayes Classiﬁer, Logistic Regression, and Neural Networks. We further demonstrate how the propositional formulas facilitate interpretation. In conclusion, we believe the combination of high accuracy, interpretability, and computational simplicity makes the Tsetlin Machine a promising tool for a wide range of domains.},
	language = {en},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Granmo, Ole-Christoffer},
	month = jan,
	year = {2021},
	note = {arXiv:1804.01508 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{sharma_drop_2022,
	title = {Drop {Clause}: {Enhancing} {Performance}, {Interpretability} and {Robustness} of the {Tsetlin} {Machine}},
	shorttitle = {Drop {Clause}},
	url = {http://arxiv.org/abs/2105.14506},
	abstract = {In this article, we introduce a novel variant of the Tsetlin machine (TM) that randomly drops clauses, the key learning elements of a TM. In effect, TM with drop clause ignores a random selection of the clauses in each epoch, selected according to a predeﬁned probability. In this way, additional stochasticity is introduced in the learning phase of TM. To explore the effects drop clause has on accuracy, training time, interpretability and robustness, we conduct extensive experiments on nine benchmark datasets in natural language processing (NLP) (IMDb, R8, R52, MR and TREC) and image classiﬁcation (MNIST, Fashion MNIST, CIFAR-10 and CIFAR100). Our proposed model outperforms baseline machine learning algorithms by a wide margin and achieves competitive performance in comparison with recent deep learning model such as BERT and AlexNET-DFA. In brief, we observe up to +10\% increase in accuracy and 2× to 4× faster learning compared with standard TM. We further employ the Convolutional TM to document interpretable results on the CIFAR datasets, visualizing how the heatmaps produced by the TM become more interpretable with drop clause. We also evaluate how drop clause affects learning robustness by introducing corruptions and alterations in the image/language test data. Our results show that drop clause makes TM more robust towards such changes1.},
	language = {en},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Sharma, Jivitesh and Yadav, Rohan and Granmo, Ole-Christoffer and Jiao, Lei},
	month = jan,
	year = {2022},
	note = {arXiv:2105.14506 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{granmo_convolutional_2019,
	title = {The {Convolutional} {Tsetlin} {Machine}},
	url = {http://arxiv.org/abs/1905.09688},
	abstract = {Convolutional neural networks (CNNs) have obtained astounding successes for important pattern recognition tasks, but they suffer from high computational complexity and the lack of interpretability. The recent Tsetlin Machine (TM) attempts to address this lack by using easy-to-interpret conjunctive clauses in propositional logic to solve complex pattern recognition problems. The TM provides competitive accuracy in several benchmarks, while keeping the important property of interpretability. It further facilitates hardware-near implementation since inputs, patterns, and outputs are expressed as bits, while recognition and learning rely on straightforward bit manipulation. In this paper, we exploit the TM paradigm by introducing the Convolutional Tsetlin Machine (CTM), as an interpretable alternative to CNNs. Whereas the TM categorizes an image by employing each clause once to the whole image, the CTM uses each clause as a convolution ﬁlter. That is, a clause is evaluated multiple times, once per image patch taking part in the convolution. To make the clauses location-aware, each patch is further augmented with its coordinates within the image. The output of a convolution clause is obtained simply by ORing the outcome of evaluating the clause on each patch. In the learning phase of the TM, clauses that evaluate to 1 are contrasted against the input. For the CTM, we instead contrast against one of the patches, randomly selected among the patches that made the clause evaluate to 1. Accordingly, the standard Type I and Type II feedback of the classic TM can be employed directly, without further modiﬁcation. The CTM obtains a peak test accuracy of 99.4\% on MNIST, 96.31\% on Kuzushiji-MNIST, 91.5\% on Fashion-MNIST, and 100.0\% on the 2D Noisy XOR Problem, which is competitive with results reported for simple 4-layer CNNs, BinaryConnect, Logistic Circuits, and a recent FPGA-accelerated Binary CNN.},
	language = {en},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Granmo, Ole-Christoffer and Glimsdal, Sondre and Jiao, Lei and Goodwin, Morten and Omlin, Christian W. and Berge, Geir Thore},
	month = dec,
	year = {2019},
	note = {arXiv:1905.09688 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{goyal_sade_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SaDe}: {Learning} {Models} that {Provably} {Satisfy} {Domain} {Constraints}},
	isbn = {978-3-031-26419-1},
	shorttitle = {{SaDe}},
	doi = {10.1007/978-3-031-26419-1_25},
	abstract = {In many real world applications of machine learning, models have to meet certain domain-based requirements that can be expressed as constraints (for example, safety-critical constraints in autonomous driving systems). Such constraints are often handled by including them in a regularization term, while learning a model. This approach, however, does not guarantee 100\% satisfaction of the constraints: it only reduces violations of the constraints on the training set rather than ensuring that the predictions by the model will always adhere to them. In this paper, we present a framework for learning models that provably fulfill the constraints under all circumstances (i.e., also on unseen data). To achieve this, we cast learning as a maximum satisfiability problem, and solve it using a novel SaDe algorithm that combines constraint satisfaction with gradient descent. We compare our method against regularization based baselines on linear models and show that our method is capable of enforcing different types of domain constraints effectively on unseen data, without sacrificing predictive performance.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer Nature Switzerland},
	author = {Goyal, Kshitij and Dumancic, Sebastijan and Blockeel, Hendrik},
	editor = {Amini, Massih-Reza and Canu, Stéphane and Fischer, Asja and Guns, Tias and Kralj Novak, Petra and Tsoumakas, Grigorios},
	year = {2023},
	keywords = {Constrained optimization, Domain constraints, Satisfiability modulo theories, leuven},
	pages = {410--425},
}

@misc{goyal_deepsade_2023,
	title = {{DeepSaDe}: {Learning} {Neural} {Networks} that {Guarantee} {Domain} {Constraint} {Satisfaction}},
	shorttitle = {{DeepSaDe}},
	url = {http://arxiv.org/abs/2303.01141},
	abstract = {As machine learning models, speciﬁcally neural networks, are becoming increasingly popular, there are concerns regarding their trustworthiness, especially in safety-critical applications, e.g., actions of an autonomous vehicle must be safe. There are approaches that can train neural networks where such domain requirements are enforced as constraints, but they either cannot guarantee that the constraint will be satisﬁed by all possible predictions (even on unseen data) or they are limited in the type of constraints that can be enforced. In this work, we present an approach to train neural networks which can enforce a wide variety of constraints and guarantee that the constraint is satisﬁed by all possible predictions. The approach builds on earlier work where learning linear models is formulated as a constraint satisfaction problem (CSP). To make this idea applicable to neural networks, two crucial new elements are added: constraint propagation over the network layers, and weight updates based on a mix of gradient descent and CSP solving. Evaluation on various machine learning tasks demonstrates that our approach is ﬂexible enough to enforce a wide variety of domain constraints and is able to guarantee them in neural networks.},
	language = {en},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Goyal, Kshitij and Dumancic, Sebastijan and Blockeel, Hendrik},
	month = may,
	year = {2023},
	note = {arXiv:2303.01141 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, leuven},
}

@article{umili_grounding_nodate,
	title = {Grounding {LTLf} specifications in images},
	abstract = {A critical challenge in neurosymbolic approaches is to handle the symbol grounding problem without direct supervision. That is mapping high-dimensional raw data into an interpretation over a finite set of abstract concepts with a known meaning, without using labels. In this work, we ground symbols into sequences of images by exploiting symbolic logical knowledge in the form of Linear Temporal Logic over finite traces (LTLf) formulas, and sequence-level labels expressing if a sequence of images is compliant or not with the given formula. Our approach is based on translating the LTLf formula into an equivalent deterministic finite automaton (DFA) and interpreting the latter in fuzzy logic. Experiments show that our system outperforms recurrent neural networks in sequence classification and can reach high image classification accuracy without being trained with any single-image label.},
	language = {en},
	author = {Umili, Elena and Capobianco, Roberto and DeGiacomo, Giuseppe},
	keywords = {todo},
}

@misc{li_how_2023,
	title = {How {Do} {Transformers} {Learn} {Topic} {Structure}: {Towards} a {Mechanistic} {Understanding}},
	shorttitle = {How {Do} {Transformers} {Learn} {Topic} {Structure}},
	url = {http://arxiv.org/abs/2303.04245},
	abstract = {While the successes of transformers across many domains are indisputable, accurate understanding of the learning mechanics is still largely lacking. Their capabilities have been probed on benchmarks which include a variety of structured and reasoning tasks—but mathematical understanding is lagging substantially behind. Recent lines of work have begun studying representational aspects of this question: that is, the size/depth/complexity of attention-based networks to perform certain tasks. However, there is no guarantee the learning dynamics will converge to the constructions proposed. In our paper, we provide fine-grained mechanistic understanding of how transformers learn “semantic structure”, understood as capturing co-occurrence structure of words. Precisely, we show, through a combination of mathematical analysis and experiments on Wikipedia data and synthetic data modeled by Latent Dirichlet Allocation (LDA), that the embedding layer and the self-attention layer encode the topical structure. In the former case, this manifests as higher average inner product of embeddings between same-topic words. In the latter, it manifests as higher average pairwise attention between same-topic words. The mathematical results involve several assumptions to make the analysis tractable, which we verify on data, and might be of independent interest as well.},
	language = {en},
	urldate = {2023-07-28},
	publisher = {arXiv},
	author = {Li, Yuchen and Li, Yuanzhi and Risteski, Andrej},
	month = jul,
	year = {2023},
	note = {arXiv:2303.04245 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kumar_context-specic_nodate,
	title = {Context-{Speciﬁc} {Likelihood} {Weighting}},
	abstract = {Sampling is a popular method for approximate inference when exact inference is impractical. Generally, sampling algorithms do not exploit contextspeciﬁc independence (CSI) properties of probability distributions. We introduce context-speciﬁc likelihood weighting (CS-LW), a new sampling methodology, which besides exploiting the classical conditional independence properties, also exploits CSI properties. Unlike the standard likelihood weighting, CS-LW is based on partial assignments of random variables and requires fewer samples for convergence due to the sampling variance reduction. Furthermore, the speed of generating samples increases. Our novel notion of contextual assignments theoretically justiﬁes CS-LW. We empirically show that CS-LW is competitive with state-of-the-art algorithms for approximate inference in the presence of a signiﬁcant amount of CSIs.},
	language = {en},
	author = {Kumar, Nitesh and Kuzˇelka, Ondrˇej},
	keywords = {leuven},
}

@inproceedings{giunchiglia_deep_2022,
	address = {Vienna, Austria},
	title = {Deep {Learning} with {Logical} {Constraints}},
	isbn = {978-1-956792-00-3},
	url = {https://www.ijcai.org/proceedings/2022/767},
	doi = {10.24963/ijcai.2022/767},
	abstract = {In recent years, there has been an increasing interest in exploiting logically specified background knowledge in order to obtain neural models (i) with a better performance, (ii) able to learn from less data, and/or (iii) guaranteed to be compliant with the background knowledge itself, e.g., for safetycritical applications. In this survey, we retrace such works and categorize them based on (i) the logical language that they use to express the background knowledge and (ii) the goals that they achieve.},
	language = {en},
	urldate = {2023-07-26},
	booktitle = {Proceedings of the {Thirty}-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Giunchiglia, Eleonora and Stoian, Mihaela Catalina and Lukasiewicz, Thomas},
	month = jul,
	year = {2022},
	pages = {5478--5485},
}

@misc{lindstrom_clevr-math_2022,
	title = {{CLEVR}-{Math}: {A} {Dataset} for {Compositional} {Language}, {Visual} and {Mathematical} {Reasoning}},
	shorttitle = {{CLEVR}-{Math}},
	url = {http://arxiv.org/abs/2208.05358},
	abstract = {We introduce CLEVR-Math, a multi-modal math word problems dataset consisting of simple math word problems involving addition/subtraction, represented partly by a textual description and partly by an image illustrating the scenario. The text describes actions performed on the scene that is depicted in the image. Since the question posed may not be about the scene in the image, but about the state of the scene before or after the actions are applied, the solver envision or imagine the state changes due to these actions. Solving these word problems requires a combination of language, visual and mathematical reasoning. We apply state-of-the-art neural and neuro-symbolic models for visual question answering on CLEVR-Math and empirically evaluate their performances. Our results show how neither method generalise to chains of operations. We discuss the limitations of the two in addressing the task of multi-modal word problem solving.},
	language = {en},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Lindström, Adam Dahlgren and Abraham, Savitha Sam},
	month = aug,
	year = {2022},
	note = {arXiv:2208.05358 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.1.4, I.2.10, I.2.6, I.2.7, I.4.8},
}

@misc{daniele_deep_2023,
	title = {Deep {Symbolic} {Learning}: {Discovering} {Symbols} and {Rules} from {Perceptions}},
	shorttitle = {Deep {Symbolic} {Learning}},
	url = {http://arxiv.org/abs/2208.11561},
	abstract = {Neuro-Symbolic (NeSy) integration combines symbolic reasoning with Neural Networks (NNs) for tasks requiring perception and reasoning. Most NeSy systems rely on continuous relaxation of logical knowledge, and no discrete decisions are made within the model pipeline. Furthermore, these methods assume that the symbolic rules are given. In this paper, we propose Deep Symbolic Learning (DSL), a NeSy system that learns NeSy-functions, i.e., the composition of a (set of) perception functions which map continuous data to discrete symbols, and a symbolic function over the set of symbols. DSL simultaneously learns the perception and symbolic functions while being trained only on their composition (NeSy-function). The key novelty of DSL is that it can create internal (interpretable) symbolic representations and map them to perception inputs within a differentiable NN learning pipeline. The created symbols are automatically selected to generate symbolic functions that best explain the data. We provide experimental analysis to substantiate the efﬁcacy of DSL in simultaneously learning perception and symbolic functions.},
	language = {en},
	urldate = {2023-07-05},
	publisher = {arXiv},
	author = {Daniele, Alessandro and Campari, Tommaso and Malhotra, Sagar and Serafini, Luciano},
	month = apr,
	year = {2023},
	note = {arXiv:2208.11561 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{bronstein_geometric_2021,
	title = {Geometric {Deep} {Learning}: {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	shorttitle = {Geometric {Deep} {Learning}},
	url = {http://arxiv.org/abs/2104.13478},
	doi = {10.48550/arXiv.2104.13478},
	abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
	urldate = {2023-07-24},
	publisher = {arXiv},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	month = may,
	year = {2021},
	note = {arXiv:2104.13478 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sidheekh_probabilistic_nodate,
	title = {Probabilistic {Flow} {Circuits}: {Towards} {Unified} {Deep} {Models} for {Tractable} {Probabilistic} {Inference}},
	abstract = {We consider the problem of increasing the expressivity of probabilistic circuits by augmenting them with the successful generative models of normalizing flows. To this effect, we theoretically establish the requirement of decomposability for such combinations to retain tractability of the learned models. Our model, called Probabilistic Flow Circuits, essentially extends circuits by allowing for normalizing flows at the leaves. Our empirical evaluation clearly establishes the expressivity and tractability of this new class of probabilistic circuits.},
	language = {en},
	author = {Sidheekh, Sahil and Kersting, Kristian and Natarajan, Sriraam},
}

@article{moskalev_genuine_nodate,
	title = {On genuine invariance learning without weight-tying},
	abstract = {In this paper, we investigate properties and limitations of invariance learned by neural networks from the data compared to the genuine invariance achieved through invariant weight-tying. To do so, we adopt a group theoretical perspective and analyze invariance learning in neural networks without weight-tying constraints. We demonstrate that even when a network learns to correctly classify samples on a group orbit, the underlying decisionmaking in such a model does not attain genuine invariance. Instead, learned invariance is strongly conditioned on the input data, rendering it unreliable if the input distribution shifts. We next demonstrate how to guide invariance learning toward genuine invariance by regularizing the invariance of a model at the training. To this end, we propose several metrics to quantify learned invariance: (i) predictive distribution invariance, (ii) logit invariance, and (iii) saliency invariance similarity. We show that the invariance learned with the invariance error regularization closely reassembles the genuine invariance of weight-tying models and reliably holds even under a severe input distribution shift. Closer analysis of the learned invariance also reveals the spectral decay phenomenon, when a network chooses to achieve the invariance to a specific transformation group by reducing the sensitivity to any input perturbation.},
	language = {en},
	author = {Moskalev, Artem and Sepliarskaia, Anna and Bekkers, Erik J and Smeulders, Arnold},
	keywords = {todo},
}

@article{cai_self-supervised_2023,
	title = {Self-{Supervised} {Logic} {Induction} for {Explainable} {Fuzzy} {Temporal} {Commonsense} {Reasoning}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26481},
	doi = {10.1609/aaai.v37i11.26481},
	abstract = {Understanding temporal commonsense concepts, such as times of occurrence and durations is crucial for event-centric language understanding. Reasoning about such temporal concepts in a complex context requires reasoning over both the stated context and the world knowledge that underlines it. A recent study shows massive pre-trained LM still struggle with such temporal reasoning under complex contexts (e.g., dialog) because they only implicitly encode the relevant contexts and fail to explicitly uncover the underlying logical compositions for complex inference, thus may not be robust enough. In this work, we propose to augment LMs with the temporal logic induction ability, which frames the temporal reasoning by defining three modular components: temporal dependency inducer and temporal concept defuzzifier and logic validator. The former two components disentangle the explicit/implicit dependency between temporal concepts across context (before, after, ...) and the specific meaning of fuzzy temporal concepts, respectively, while the validator combines the intermediate reasoning clues for robust contextual reasoning about the temporal concepts. Extensive experimental results on TIMEDIAL, a challenging dataset for temporal reasoning over dialog, show that our method, Logic Induction Enhanced Contextualized TEmporal Reasoning (LECTER), can yield great improvements over the traditional language model for temporal reasoning.},
	language = {en},
	number = {11},
	urldate = {2023-07-12},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Cai, Bibo and Ding, Xiao and Sun, Zhouhao and Qin, Bing and Liu, Ting and Wang, Baojun and Shang, Lifeng},
	month = jun,
	year = {2023},
	note = {Number: 11},
	keywords = {SNLP: Sentence-Level Semantics and Textual Inference},
	pages = {12580--12588},
}

@inproceedings{zhang_raven_2019,
	address = {Long Beach, CA, USA},
	title = {{RAVEN}: {A} {Dataset} for {Relational} and {Analogical} {Visual} {REasoNing}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{RAVEN}},
	url = {https://ieeexplore.ieee.org/document/8953364/},
	doi = {10.1109/CVPR.2019.00546},
	abstract = {Dramatic progress has been witnessed in basic vision tasks involving low-level perception, such as object recognition, detection, and tracking. Unfortunately, there is still an enormous performance gap between artiﬁcial vision systems and human intelligence in terms of higher-level vision problems, especially ones involving reasoning. Earlier attempts in equipping machines with high-level reasoning have hovered around Visual Question Answering (VQA), one typical task associating vision and language understanding. In this work, we propose a new dataset, built in the context of Raven’s Progressive Matrices (RPM) and aimed at lifting machine intelligence by associating vision with structural, relational, and analogical reasoning in a hierarchical representation. Unlike previous works in measuring abstract reasoning using RPM, we establish a semantic link between vision and reasoning by providing structure representation. This addition enables a new type of abstract reasoning by jointly operating on the structure representation. Machine reasoning ability using modern computer vision is evaluated in this newly proposed dataset. Additionally, we also provide human performance as a reference. Finally, we show consistent improvement across all models by incorporating a simple neural module that combines visual understanding and structure reasoning.},
	language = {en},
	urldate = {2023-07-07},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhang, Chi and Gao, Feng and Jia, Baoxiong and Zhu, Yixin and Zhu, Song-Chun},
	month = jun,
	year = {2019},
	pages = {5312--5322},
}

@article{minervini_differentiable_2020,
	title = {Differentiable {Reasoning} on {Large} {Knowledge} {Bases} and {Natural} {Language}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{GNTP}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5962},
	doi = {10.1609/aaai.v34i04.5962},
	abstract = {Reasoning with knowledge expressed in natural language and Knowledge Bases (KBs) is a major challenge for Artificial Intelligence, with applications in machine reading, dialogue, and question answering. General neural architectures that jointly learn representations and transformations of text are very data-inefficient, and it is hard to analyse their reasoning process. These issues are addressed by end-to-end differentiable reasoning systems such as Neural Theorem Provers (NTPs), although they can only be used with small-scale symbolic KBs. In this paper we first propose Greedy NTPs (GNTPs), an extension to NTPs addressing their complexity and scalability limitations, thus making them applicable to real-world datasets. This result is achieved by dynamically constructing the computation graph of NTPs and including only the most promising proof paths during inference, thus obtaining orders of magnitude more efficient models 1. Then, we propose a novel approach for jointly reasoning over KBs and textual mentions, by embedding logic facts and natural language sentences in a shared embedding space. We show that GNTPs perform on par with NTPs at a fraction of their cost while achieving competitive link prediction results on large datasets, providing explanations for predictions, and inducing interpretable models.},
	language = {en},
	number = {04},
	urldate = {2022-09-14},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Minervini, Pasquale and Bošnjak, Matko and Rocktäschel, Tim and Riedel, Sebastian and Grefenstette, Edward},
	month = apr,
	year = {2020},
	note = {Number: 04},
	pages = {5182--5190},
}

@inproceedings{rocktaschel_end--end_2017,
	title = {End-to-end {Differentiable} {Proving}},
	volume = {30},
	shorttitle = {{NTP}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/b2ab001909a8a6f04b51920306046ce5-Abstract.html},
	abstract = {We introduce deep neural networks for end-to-end differentiable theorem proving that operate on dense vector representations of symbols.  These neural networks are recursively constructed by following the backward chaining algorithm as used in Prolog.  Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations.  The resulting neural network can be trained to infer facts from a given incomplete knowledge base using gradient descent.  By doing so, it learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove facts, (iii) induce logical rules, and (iv) it can use provided and induced logical rules for complex multi-hop reasoning.  On four benchmark knowledge bases we demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, while at the same time inducing interpretable function-free first-order logic rules.},
	urldate = {2022-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rocktäschel, Tim and Riedel, Sebastian},
	year = {2017},
}

@article{morris_learning_nodate,
	title = {Learning {Proof} {Path} {Selection} {Policies} in {Neural} {Theorem} {Proving}},
	shorttitle = {{ACTP}},
	abstract = {Neural Theorem Provers (NTPs) are neural relaxations of the backward-chaining logic reasoning algorithm. They can learn continuous representations for predicates and constants, induce interpretable rules, can provide logic explanations for their predictions, and show strong systematic generalisation properties. However, since they enumerate all possible proof paths for proving a goal, they suffer from high computational complexity, and are thus unsuitable for complex reasoning tasks. Conditional Theorem Provers (CTPs) try to overcome this issue by generating relevant rules on-the-fly based on the goal, rather than considering all possible rules. Nonetheless, CTPs suffer from similar computational constraints, as they still have to consider multiple proof paths while reasoning. We propose Adaptive CTPs (ACTPs), where CTPs are augmented with a learned policy to dynamically select the most promising proof paths. This allows the model designer to specify the number of proof paths to consider, to conform to the computational constraints of their use case, while retaining all of the benefits of CTPs. By evaluating on the CLUTRR dataset, we provide evidence for the computational issues in existing CTP models, show that ACTPs alleviate these issues, and demonstrate that, in certain scenarios, the accuracy achieved by ACTPs is higher than CTPs while retaining the same computational complexity.},
	language = {en},
	author = {Morris, Matthew and Minervini, Pasquale and Blunsom, Phil},
	pages = {24},
}

@misc{campero_logical_2018,
	title = {Logical {Rule} {Induction} and {Theory} {Learning} {Using} {Neural} {Theorem} {Proving}},
	url = {http://arxiv.org/abs/1809.02193},
	abstract = {A hallmark of human cognition is the ability to continually acquire and distill observations of the world into meaningful, predictive theories. In this paper we present a new mechanism for logical theory acquisition which takes a set of observed facts and learns to extract from them a set of logical rules and a small set of core facts which together entail the observations. Our approach is neuro-symbolic in the sense that the rule predicates and core facts are given dense vector representations. The rules are applied to the core facts using a soft uniﬁcation procedure to infer additional facts. After k steps of forward inference, the consequences are compared to the initial observations and the rules and core facts are then encouraged towards representations that more faithfully generate the observations through inference. Our approach is based on a novel neural forward-chaining differentiable rule induction network. The rules are interpretable and learned compositionally from their predicates, which may be invented. We demonstrate the efﬁcacy of our approach on a variety of ILP rule induction and domain theory learning datasets.},
	language = {en},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Campero, Andres and Pareja, Aldo and Klinger, Tim and Tenenbaum, Josh and Riedel, Sebastian},
	month = sep,
	year = {2018},
	note = {arXiv:1809.02193 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{minervini_learning_2020,
	title = {Learning {Reasoning} {Strategies} in {End}-to-{End} {Differentiable} {Proving}},
	shorttitle = {{CTP}},
	url = {https://proceedings.mlr.press/v119/minervini20a.html},
	abstract = {Attempts to render deep learning models interpretable, data-efficient, and robust have seen some success through hybridisation with rule-based systems, for example, in Neural Theorem Provers (NTPs). These neuro-symbolic models can induce interpretable rules and learn representations from data via back-propagation, while providing logical explanations for their predictions. However, they are restricted by their computational complexity, as they need to consider all possible proof paths for explaining a goal, thus rendering them unfit for large-scale applications. We present Conditional Theorem Provers (CTPs), an extension to NTPs that learns an optimal rule selection strategy via gradient-based optimisation. We show that CTPs are scalable and yield state-of-the-art results on the CLUTRR dataset, which tests systematic generalisation of neural models by learning to reason over smaller graphs and evaluating on larger ones. Finally, CTPs show better link prediction results on standard benchmarks in comparison with other neural-symbolic models, while being explainable.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Minervini, Pasquale and Riedel, Sebastian and Stenetorp, Pontus and Grefenstette, Edward and Rocktäschel, Tim},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {6938--6949},
}

@article{diligenti_semantic-based_2017,
	series = {Combining {Constraint} {Solving} with {Mining} and {Learning}},
	title = {Semantic-based regularization for learning and inference},
	volume = {244},
	issn = {0004-3702},
	shorttitle = {{SBR}},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370215001344},
	doi = {10.1016/j.artint.2015.08.011},
	abstract = {This paper proposes a unified approach to learning from constraints, which integrates the ability of classical machine learning techniques to learn from continuous feature-based representations with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning. Learning tasks are modeled in the general framework of multi-objective optimization, where a set of constraints must be satisfied in addition to the traditional smoothness regularization term. The constraints translate First Order Logic formulas, which can express learning-from-example supervisions and general prior knowledge about the environment by using fuzzy logic. By enforcing the constraints also on the test set, this paper presents a natural extension of the framework to perform collective classification. Interestingly, the theory holds for both the case of data represented by feature vectors and the case of data simply expressed by pattern identifiers, thus extending classic kernel machines and graph regularization, respectively. This paper also proposes a probabilistic interpretation of the proposed learning scheme, and highlights intriguing connections with probabilistic approaches like Markov Logic Networks. Experimental results on classic benchmarks provide clear evidence of the remarkable improvements that are obtained with respect to related approaches.},
	language = {en},
	urldate = {2023-04-24},
	journal = {Artificial Intelligence},
	author = {Diligenti, Michelangelo and Gori, Marco and Saccà, Claudio},
	month = mar,
	year = {2017},
	keywords = {FOL, Kernel machines, Learning with constraints},
	pages = {143--165},
}

@misc{marra_integrating_2019,
	title = {Integrating {Learning} and {Reasoning} with {Deep} {Logic} {Models}},
	shorttitle = {{DLN}},
	url = {http://arxiv.org/abs/1901.04195},
	doi = {10.48550/arXiv.1901.04195},
	abstract = {Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, which are deep graphical models integrating deep learning and logic reasoning both for learning and inference. Deep Logic Models create an end-to-end differentiable architecture, where deep learners are embedded into a network implementing a continuous relaxation of the logic knowledge. The learning process allows to jointly learn the weights of the deep learners and the meta-parameters controlling the high-level reasoning. The experimental results show that the proposed methodology overtakes the limitations of the other approaches that have been proposed to bridge deep learning and reasoning.},
	urldate = {2022-09-07},
	publisher = {arXiv},
	author = {Marra, Giuseppe and Giannini, Francesco and Diligenti, Michelangelo and Gori, Marco},
	month = jan,
	year = {2019},
	note = {arXiv:1901.04195 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{aspis_embed2sym_2022,
	title = {{Embed2Sym} - {Scalable} {Neuro}-{Symbolic} {Reasoning} via {Clustered} {Embeddings}},
	volume = {19},
	issn = {2334-1033},
	shorttitle = {{Embed2Sym}},
	url = {https://proceedings.kr.org/2022/44/},
	doi = {10.24963/kr.2022/44},
	abstract = {Neuro-symbolic reasoning approaches proposed in recent years combine a neural perception component with a symbolic reasoning component to solve a downstream task. By doing so, these approaches can ...},
	language = {en},
	number = {1},
	urldate = {2023-05-23},
	journal = {Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning},
	author = {Aspis, Yaniv and Broda, Krysia and Lobo, Jorge and Russo, Alessandra},
	month = jul,
	year = {2022},
	note = {Conference Name: Proceedings of the 19th International Conference on Principles of Knowledge Representation and Reasoning},
	pages = {421--431},
}

@incollection{godo_logical_2008,
	address = {Vienna},
	series = {{CISM} {International} {Centre} for {Mechanical} {Sciences}},
	title = {Logical approaches to fuzzy similarity-based reasoning: an overview},
	isbn = {978-3-211-85432-7},
	url = {https://doi.org/10.1007/978-3-211-85432-7_4},
	abstract = {The aim of this paper is to survey a class of logical formalizations of similarity-based reasoning models where similarity is understood as a graded notion of truthlikeness. We basically identify two different kinds of logical approaches that have been used to formalize fuzzy similarity reasoning: syntacticelly-oriented approaches based on a notion of approximate proof, and semantically-oriented approaches based on several notions of approximate entailments. In particular, for these approximate entailments we provide four different formalisations in terms of suitable systems of modal and conditional logics, including for each class a system of graded operators with classical semantics, as well as a system with many-valued operators. Finally, we also explore some nonmonotonic issues of similarity-based reasoning.},
	language = {en},
	urldate = {2023-03-30},
	booktitle = {Preferences and {Similarities}},
	publisher = {Springer},
	author = {Godo, Lluís and Rodríguez, Ricardo O.},
	editor = {Della Riccia, Giacomo and Dubois, Didier and Kruse, Rudolf and Lenz, Hans-Joachim},
	year = {2008},
	doi = {10.1007/978-3-211-85432-7_4},
	pages = {75--128},
}

@misc{shen_enhancing_2005,
	title = {Enhancing {Global} {SLS}-{Resolution} with {Loop} {Cutting} and {Tabling} {Mechanisms}},
	shorttitle = {{SLTNF}},
	url = {http://arxiv.org/abs/cs/0507035},
	abstract = {Global SLS-resolution is a well-known procedural semantics for top-down computation of queries under the well-founded model. It inherits from SLDNF-resolution the \{{\textbackslash}em linearity\} property of derivations, which makes it easy and efficient to implement using a simple stack-based memory structure. However, like SLDNF-resolution it suffers from the problem of infinite loops and redundant computations. To resolve this problem, in this paper we develop a new procedural semantics, called \{{\textbackslash}em SLTNF-resolution\}, by enhancing Global SLS-resolution with loop cutting and tabling mechanisms. SLTNF-resolution is sound and complete w.r.t. the well-founded semantics for logic programs with the bounded-term-size property, and is superior to existing linear tabling procedural semantics such as SLT-resolution.},
	urldate = {2022-12-06},
	publisher = {arXiv},
	author = {Shen, Yi-Dong and You, Jia-Huai and Yuan, Li-Yan},
	month = jul,
	year = {2005},
	note = {arXiv:cs/0507035},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	shorttitle = {word2vec},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2022-10-10},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{ren_beta_2020,
	title = {Beta {Embeddings} for {Multi}-{Hop} {Logical} {Reasoning} in {Knowledge} {Graphs}},
	shorttitle = {{BetaE}},
	url = {http://arxiv.org/abs/2010.11465},
	abstract = {One of the fundamental problems in Artiﬁcial Intelligence is to perform complex multi-hop logical reasoning over the facts captured by a knowledge graph (KG). This problem is challenging, because KGs can be massive and incomplete. Recent approaches embed KG entities in a low dimensional space and then use these embeddings to ﬁnd the answer entities. However, it has been an outstanding challenge of how to handle arbitrary ﬁrst-order logic (FOL) queries as present methods are limited to only a subset of FOL operators. In particular, the negation operator is not supported. An additional limitation of present methods is also that they cannot naturally model uncertainty. Here, we present BETAE, a probabilistic embedding framework for answering arbitrary FOL queries over KGs. BETAE is the ﬁrst method that can handle a complete set of ﬁrst-order logical operations: conjunction (∧), disjunction (∨), and negation (¬). A key insight of BETAE is to use probabilistic distributions with bounded support, speciﬁcally the Beta distribution, and embed queries/entities as distributions, which as a consequence allows us to also faithfully model uncertainty. Logical operations are performed in the embedding space by neural operators over the probabilistic embeddings. We demonstrate the performance of BETAE on answering arbitrary FOL queries on three large, incomplete KGs. While being more general, BETAE also increases relative performance by up to 25.4\% over the current state-of-the-art KG reasoning methods that can only handle conjunctive queries without negation.},
	language = {en},
	urldate = {2023-01-23},
	publisher = {arXiv},
	author = {Ren, Hongyu and Leskovec, Jure},
	month = oct,
	year = {2020},
	note = {arXiv:2010.11465 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning},
}

@misc{nayyeri_5_2021,
	title = {5* {Knowledge} {Graph} {Embeddings} with {Projective} {Transformations}},
	shorttitle = {5*{E}},
	url = {http://arxiv.org/abs/2006.04986},
	doi = {10.48550/arXiv.2006.04986},
	abstract = {Performing link prediction using knowledge graph embedding models has become a popular approach for knowledge graph completion. Such models employ a transformation function that maps nodes via edges into a vector space in order to measure the likelihood of the links. While mapping the individual nodes, the structure of subgraphs is also transformed. Most of the embedding models designed in Euclidean geometry usually support a single transformation type - often translation or rotation, which is suitable for learning on graphs with small differences in neighboring subgraphs. However, multi-relational knowledge graphs often include multiple sub-graph structures in a neighborhood (e.g. combinations of path and loop structures), which current embedding models do not capture well. To tackle this problem, we propose a novel KGE model (5*E) in projective geometry, which supports multiple simultaneous transformations - specifically inversion, reflection, translation, rotation, and homothety. The model has several favorable theoretical properties and subsumes the existing approaches. It outperforms them on the most widely used link prediction benchmarks},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Nayyeri, Mojtaba and Vahdati, Sahar and Aykul, Can and Lehmann, Jens},
	month = mar,
	year = {2021},
	note = {arXiv:2006.04986 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{dettmers_convolutional_2018,
	title = {Convolutional {2D} {Knowledge} {Graph} {Embeddings}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	shorttitle = {{ConvE}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11573},
	doi = {10.1609/aaai.v32i1.11573},
	abstract = {Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer models — which potentially limits performance. In this work we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree — which are common in highly-connected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test set — however, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets — deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models, and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across all datasets.},
	language = {en},
	number = {1},
	urldate = {2022-09-19},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Dettmers, Tim and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {convolution},
}

@misc{meilicke_reinforced_2020,
	title = {Reinforced {Anytime} {Bottom} {Up} {Rule} {Learning} for {Knowledge} {Graph} {Completion}},
	shorttitle = {{AnyBURL}},
	url = {http://arxiv.org/abs/2004.04412},
	abstract = {Most of today’s work on knowledge graph completion is concerned with sub-symbolic approaches that focus on the concept of embedding a given graph in a low dimensional vector space. Against this trend, we propose an approach called AnyBURL that is rooted in the symbolic space. Its core algorithm is based on sampling paths, which are generalized into Horn rules. Previously published results show that the prediction quality of AnyBURL is on the same level as current state of the art with the additional beneﬁt of offering an explanation for the predicted fact. In this paper, we are concerned with two extensions of AnyBURL. Firstly, we change AnyBURL’s interpretation of rules from Θ-subsumption into Θ-subsumption under Object Identity. Secondly, we introduce reinforcement learning to better guide the sampling process. We found out that reinforcement learning helps ﬁnding more valuable rules earlier in the search process. We measure the impact of both extensions and compare the resulting approach with current state of the art approaches. Our results show that AnyBURL outperforms most sub-symbolic methods.},
	language = {en},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {Meilicke, Christian and Chekol, Melisachew Wudage and Fink, Manuel and Stuckenschmidt, Heiner},
	month = apr,
	year = {2020},
	note = {arXiv:2004.04412 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{bordes_translating_2013,
	title = {Translating {Embeddings} for {Modeling} {Multi}-relational {Data}},
	volume = {26},
	shorttitle = {{TransE}},
	url = {https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html},
	abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose, TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
	urldate = {2022-12-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
	year = {2013},
}

@article{iferroudjene_fb15k-cvt_nodate,
	title = {{FB15k}-{CVT}: {A} {Challenging} {Dataset} for {Knowledge} {Graph} {Embedding} {Models}},
	shorttitle = {{FB15k}-{CVT}},
	abstract = {Knowledge Graphs (KGs) are an essential component of neuro-symbolic AI. KG Embedding Models (KGEMs) are used to represent elements of a KG (its entities and relations) in a vector space, to enable efficient processing and reasoning over knowledge. Most KGEMs are evaluated against datasets derived from the Freebase KG: FB15k and FB15k-237. In this paper, we identify limitations in these datasets with respect to Compound Value Types (CVTs), which are nodes introduced in Freebase as a substitute for 𝑛-ary relations. In FB15k and FB51k-237, CVTs have been removed, thereby eliminating valuable information. To evaluate whether KGEMs can learn semantically accurate representations of entities and relations in Freebase, we introduce here a new dataset named FB15k-CVT, which reintroduces the deleted CVT nodes. In a preliminary evaluation, we assess the limitations of baseline KGEMs (TransE, DistMult) in the presence of CVTs. The evaluation suggests that KGEMs based on tensor decomposition are more promising than translational models but, most of all, it calls for further experiments with KGEMs that can answer conjunctive queries or that preserve logical entailment.},
	language = {en},
	author = {Iferroudjene, Mouloud and Charpenay, Victor and Zimmermann, Antoine},
}

@misc{onoro-rubio_answering_2019,
	title = {Answering {Visual}-{Relational} {Queries} in {Web}-{Extracted} {Knowledge} {Graphs}},
	shorttitle = {{ImageGraph}},
	url = {http://arxiv.org/abs/1709.02314},
	abstract = {A visual-relational knowledge graph (KG) is a multi-relational graph whose entities are associated with images. We explore novel machine learning approaches for answering visual-relational queries in web-extracted knowledge graphs. To this end, we have created ImageGraph, a KG with 1,330 relation types, 14,870 entities, and 829,931 images crawled from the web. With visual-relational KGs such as ImageGraph one can introduce novel probabilistic query types in which images are treated as first-class citizens. Both the prediction of relations between unseen images as well as multi-relational image retrieval can be expressed with specific families of visual-relational queries. We introduce novel combinations of convolutional networks and knowledge graph embedding methods to answer such queries. We also explore a zero-shot learning scenario where an image of an entirely new entity is linked with multiple relations to entities of an existing KG. The resulting multi-relational grounding of unseen entity images into a knowledge graph serves as a semantic entity representation. We conduct experiments to demonstrate that the proposed methods can answer these visual-relational queries efficiently and accurately.},
	urldate = {2022-10-18},
	publisher = {arXiv},
	author = {Oñoro-Rubio, Daniel and Niepert, Mathias and García-Durán, Alberto and González, Roberto and López-Sastre, Roberto J.},
	month = may,
	year = {2019},
	note = {arXiv:1709.02314 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@techreport{yanaka_can_2019,
	title = {Can neural networks understand monotonicity reasoning?},
	shorttitle = {{MED}},
	url = {http://arxiv.org/abs/1906.06448},
	abstract = {Monotonicity reasoning is one of the important reasoning skills for any intelligent natural language inference (NLI) model in that it requires the ability to capture the interaction between lexical and syntactic structures. Since no test set has been developed for monotonicity reasoning with wide coverage, it is still unclear whether neural models can perform monotonicity reasoning in a proper way. To investigate this issue, we introduce the Monotonicity Entailment Dataset (MED). Performance by state-of-the-art NLI models on the new test set is substantially worse, under 55\%, especially on downward reasoning. In addition, analysis using a monotonicity-driven data augmentation method showed that these models might be limited in their generalization ability in upward and downward reasoning.},
	number = {arXiv:1906.06448},
	urldate = {2022-09-27},
	institution = {arXiv},
	author = {Yanaka, Hitomi and Mineshima, Koji and Bekki, Daisuke and Inui, Kentaro and Sekine, Satoshi and Abzianidze, Lasha and Bos, Johan},
	month = jun,
	year = {2019},
	note = {arXiv:1906.06448 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
}

@article{noauthor_lifted_nodate,
	title = {Lifted {Probabilistic} {Inference} by {First}-{Order} {Knowledge} {Compilation}},
	abstract = {Probabilistic logical languages provide powerful formalisms for knowledge representation and learning. Yet performing inference in these languages is extremely costly, especially if it is done at the propositional level. Lifted inference algorithms, which avoid repeated computation by treating indistinguishable groups of objects as one, help mitigate this cost. Seeking inspiration from logical inference, where lifted inference (e.g., resolution) is commonly performed, we develop a model theoretic approach to probabilistic lifted inference. Our algorithm compiles a ﬁrst-order probabilistic theory into a ﬁrst-order deterministic decomposable negation normal form (d-DNNF) circuit. Compilation offers the advantage that inference is polynomial in the size of the circuit. Furthermore, by borrowing techniques from the knowledge compilation literature our algorithm effectively exploits the logical structure (e.g., context-speciﬁc independencies) within the ﬁrst-order model, which allows more computation to be done at the lifted level. An empirical comparison demonstrates the utility of the proposed approach.},
	language = {en},
	keywords = {leuven},
}

@article{ranganath_black_nodate,
	title = {Black {Box} {Variational} {Inference}},
	abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires signiﬁcant model-speciﬁc analysis. These eﬀorts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a “black box” variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid diﬃcult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We ﬁnd that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
	language = {en},
	author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M},
}

@article{kimmig_algebraic_2011,
	title = {An {Algebraic} {Prolog} for {Reasoning} about {Possible} {Worlds}},
	volume = {25},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{aProbLog}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7852},
	doi = {10.1609/aaai.v25i1.7852},
	abstract = {We introduce aProbLog, a generalization of the probabilistic logic programming language ProbLog. An aProbLog program consists of a set of deﬁnite clauses and a set of algebraic facts; each such fact is labeled with an element of a semiring. A wide variety of labels is possible, ranging from probability values to reals (representing costs or utilities), polynomials, Boolean functions or data structures. The semiring is then used to calculate labels of possible worlds and of queries.},
	language = {en},
	number = {1},
	urldate = {2023-01-04},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Kimmig, Angelika and Van den Broeck, Guy and De Raedt, Luc},
	month = aug,
	year = {2011},
	keywords = {leuven},
	pages = {209--214},
}

@techreport{marra_learning_2021,
	title = {Learning {Representations} for {Sub}-{Symbolic} {Reasoning}},
	shorttitle = {{R2N}},
	url = {http://arxiv.org/abs/2106.00393},
	abstract = {Neuro-symbolic methods integrate neural architectures, knowledge representation and reasoning. However, they have been struggling at both dealing with the intrinsic uncertainty of the observations and scaling to real world applications. This paper presents Relational Reasoning Networks (R2N), a novel end-to-end model that performs relational reasoning in the latent space of a deep learner architecture, where the representations of constants, ground atoms and their manipulations are learned in an integrated fashion. Unlike flat architectures like Knowledge Graph Embedders, which can only represent relations between entities, R2Ns define an additional computational structure, accounting for higher-level relations among the ground atoms. The considered relations can be explicitly known, like the ones defined by logic formulas, or defined as unconstrained correlations among groups of ground atoms. R2Ns can be applied to purely symbolic tasks or as a neuro-symbolic platform to integrate learning and reasoning in heterogeneous problems with both symbolic and feature-based represented entities. The proposed model bridges the gap between previous neuro-symbolic methods that have been either limited in terms of scalability or expressivity. The proposed methodology is shown to achieve state-of-the-art results in different experimental settings.},
	number = {arXiv:2106.00393},
	urldate = {2022-09-27},
	institution = {arXiv},
	author = {Marra, Giuseppe and Diligenti, Michelangelo and Giannini, Francesco},
	month = oct,
	year = {2021},
	doi = {10.48550/arXiv.2106.00393},
	note = {arXiv:2106.00393 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{dumancic_learning_2020,
	title = {Learning {Relational} {Representations} with {Auto}-encoding {Logic} {Programs}},
	url = {http://arxiv.org/abs/1903.12577},
	doi = {10.48550/arXiv.1903.12577},
	abstract = {Deep learning methods capable of handling relational data have proliferated over the last years. In contrast to traditional relational learning methods that leverage first-order logic for representing such data, these deep learning methods aim at re-representing symbolic relational data in Euclidean spaces. They offer better scalability, but can only numerically approximate relational structures and are less flexible in terms of reasoning tasks supported. This paper introduces a novel framework for relational representation learning that combines the best of both worlds. This framework, inspired by the auto-encoding principle, uses first-order logic as a data representation language, and the mapping between the original and latent representation is done by means of logic programs instead of neural networks. We show how learning can be cast as a constraint optimisation problem for which existing solvers can be used. The use of logic as a representation language makes the proposed framework more accurate (as the representation is exact, rather than approximate), more flexible, and more interpretable than deep learning methods. We experimentally show that these latent representations are indeed beneficial in relational learning tasks.},
	urldate = {2022-09-08},
	publisher = {arXiv},
	author = {Dumancic, Sebastijan and Guns, Tias and Meert, Wannes and Blockeel, Hendrik},
	month = mar,
	year = {2020},
	note = {arXiv:1903.12577 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, leuven},
}

@article{eiter_semiring_2023,
	title = {Semiring {Reasoning} {Frameworks} in {AI} and {Their} {Computational} {Complexity}},
	volume = {77},
	copyright = {Copyright (c) 2023 Journal of Artificial Intelligence Research},
	issn = {1076-9757},
	shorttitle = {Semiring {Turing} {Machines}},
	url = {https://www.jair.org/index.php/jair/article/view/13970},
	doi = {10.1613/jair.1.13970},
	abstract = {Many important problems in AI, among them \#SAT, parameter learning and probabilistic inference go beyond the classical satisfiability problem. Here, instead of finding a solution we are interested in a quantity associated with the set of solutions, such as the number of solutions, the optimal solution or the probability that a query holds in a solution. To model such quantitative problems in a uniform manner, a number of frameworks, e.g. Algebraic Model Counting and Semiring-based Constraint Satisfaction Problems, employ what we call the semiring paradigm. In the latter the abstract algebraic structure of the semiring serves as a means of parameterizing the problem definition, thus allowing for different modes of quantitative computations by choosing different semirings. While efficiently solvable cases have been widely studied, a systematic study of the computational complexity of such problems depending on the semiring parameter is missing. In this work, we characterize the latter by NP(R), a novel generalization of NP over semiring R, and obtain NP(R)-completeness results for a selection of semiring frameworks. To obtain more tangible insights into the hardness of NP(R), we link it to well-known complexity classes from the literature. Interestingly, we manage to connect the computational hardness to properties of the semiring. Using this insight, we see that, on the one hand, NP(R) is always at least as hard as NP or ModpP depending on the semiring R and in general unlikely to be in FPSPACEpoly. On the other hand, for broad subclasses of semirings relevant in practice we can employ reductions to NP, ModpP and \#P. These results show that in many cases solutions are only mildly harder to compute than functions in NP, ModpP and \#P, give us new insights into how problems that involve counting on semirings can be approached, and provide a means of assessing whether an algorithm is appropriate for a given class of problems.},
	language = {en},
	urldate = {2023-06-06},
	journal = {Journal of Artificial Intelligence Research},
	author = {Eiter, Thomas and Kiesel, Rafael},
	month = may,
	year = {2023},
	keywords = {knowledge representation, probabilistic reasoning},
	pages = {207--293},
}

@misc{cobbe_training_2021,
	title = {Training {Verifiers} to {Solve} {Math} {Word} {Problems}},
	shorttitle = {{GSM8K}},
	url = {http://arxiv.org/abs/2110.14168},
	abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We ﬁnd that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training veriﬁers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the veriﬁer. We demonstrate that veriﬁcation signiﬁcantly improves performance on GSM8K, and we provide strong empirical evidence that veriﬁcation scales more eﬀectively with increased data than a ﬁnetuning baseline.},
	language = {en},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	month = nov,
	year = {2021},
	note = {arXiv:2110.14168 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{velickovic_clrs_2022,
	title = {The {CLRS} {Algorithmic} {Reasoning} {Benchmark}},
	shorttitle = {{CLRS}},
	url = {http://arxiv.org/abs/2205.15659},
	abstract = {Learning representations of algorithms is an emerging area of machine learning, seeking to bridge concepts from neural networks with classical algorithms. Several important works have investigated whether neural networks can effectively reason like algorithms, typically by learning to execute them. The common trend in the area, however, is to generate targeted kinds of algorithmic data to evaluate specific hypotheses, making results hard to transfer across publications, and increasing the barrier of entry. To consolidate progress and work towards unified evaluation, we propose the CLRS Algorithmic Reasoning Benchmark, covering classical algorithms from the Introduction to Algorithms textbook. Our benchmark spans a variety of algorithmic reasoning procedures, including sorting, searching, dynamic programming, graph algorithms, string algorithms and geometric algorithms. We perform extensive experiments to demonstrate how several popular algorithmic reasoning baselines perform on these tasks, and consequently, highlight links to several open challenges. Our library is readily available at https://github.com/deepmind/clrs.},
	urldate = {2023-01-02},
	publisher = {arXiv},
	author = {Veličković, Petar and Badia, Adrià Puigdomènech and Budden, David and Pascanu, Razvan and Banino, Andrea and Dashevskiy, Misha and Hadsell, Raia and Blundell, Charles},
	month = jun,
	year = {2022},
	note = {arXiv:2205.15659 [cs, stat]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{augustine_visual_nodate,
	title = {Visual {Sudoku} {Puzzle} {Classification}: {A} {Suite} of {Collective} {Neuro}-{Symbolic} {Tasks}},
	shorttitle = {{ViSudo}-{PC}},
	abstract = {Neuro-symbolic computing (NeSy) is an emerging field that has the goal of integrating the low-level representational power of deep neural networks with high-level symbolic reasoning. Due to the youth of the field and the complexity of neuro-symbolic integration, there are few benchmarks that showcase the powers of NeSy, and even fewer built specifically with NeSy in mind. To address the lack of NeSy benchmarks, we introduce Visual Sudoku Puzzle Classification (ViSudo-PC). ViSudo-PC is a new NeSy benchmark dataset combining visual perception with relational constraints. The goal of the benchmark is to both highlight opportunities and elicit challenges. In addition to providing a new NeSy benchmark suite, we also provide an exploratory analysis that showcases ViSudo-PC’s difficulty and possibilities.},
	language = {en},
	author = {Augustine, Eriq and Pryor, Connor and Dickens, Charles and Pujara, Jay and Wang, William Yang and Getoor, Lise},
}

@inproceedings{misino_vael_2022,
	title = {{VAEL}: {Bridging} {Variational} {Autoencoders} and {Probabilistic} {Logic} {Programming}},
	shorttitle = {{VAEL}},
	url = {https://openreview.net/forum?id=0xbP4W7rdJW},
	abstract = {We present VAEL, a neuro-symbolic generative model integrating variational autoencoders (VAE) with the reasoning capabilities of probabilistic logic (L) programming. Besides standard latent subsymbolic variables, our model exploits a probabilistic logic program to define a further structured representation, which is used for logical reasoning. The entire process is end-to-end differentiable. Once trained, VAEL can solve new unseen generation tasks by (i) leveraging the previously acquired knowledge encoded in the neural component and (ii) exploiting new logical programs on the structured latent space. Our experiments provide support on the benefits of this neuro-symbolic integration both in terms of task generalization and data efficiency. To the best of our knowledge, this work is the first to propose a general-purpose end-to-end framework integrating probabilistic logic programming into a deep generative model.},
	language = {en},
	urldate = {2023-06-03},
	author = {Misino, Eleonora and Marra, Giuseppe and Sansone, Emanuele},
	month = oct,
	year = {2022},
	keywords = {leuven},
}

@misc{cucala_correspondence_2023,
	title = {On the {Correspondence} {Between} {Monotonic} {Max}-{Sum} {GNNs} and {Datalog}},
	url = {http://arxiv.org/abs/2305.18015},
	abstract = {Although there has been signiﬁcant interest in applying machine learning techniques to structured data, the expressivity (i.e., a description of what can be learned) of such techniques is still poorly understood. In this paper, we study data transformations based on graph neural networks (GNNs). First, we note that the choice of how a dataset is encoded into a numeric form processable by a GNN can obscure the characterisation of a model’s expressivity, and we argue that a canonical encoding provides an appropriate basis. Second, we study the expressivity of monotonic max-sum GNNs, which cover a subclass of GNNs with max and sum aggregation functions. We show that, for each such GNN, one can compute a Datalog program such that applying the GNN to any dataset produces the same facts as a single round of application of the program’s rules to the dataset. Monotonic max-sum GNNs can sum an unbounded number of feature vectors which can result in arbitrarily large feature values, whereas rule application requires only a bounded number of constants. Hence, our result shows that the unbounded summation of monotonic max-sum GNNs does not increase their expressive power. Third, we sharpen our result to the subclass of monotonic max GNNs, which use only the max aggregation function, and identify a corresponding class of Datalog programs.},
	language = {en},
	urldate = {2023-06-02},
	publisher = {arXiv},
	author = {Cucala, David Tena and Grau, Bernardo Cuenca and Motik, Boris and Kostylev, Egor V.},
	month = may,
	year = {2023},
	note = {arXiv:2305.18015 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{abe_solving_2020,
	title = {Solving {NP}-{Hard} {Problems} on {Graphs} with {Extended} {AlphaGo} {Zero}},
	url = {http://arxiv.org/abs/1905.11623},
	doi = {10.48550/arXiv.1905.11623},
	abstract = {There have been increasing challenges to solve combinatorial optimization problems by machine learning. Khalil et al. proposed an end-to-end reinforcement learning framework, S2V-DQN, which automatically learns graph embeddings to construct solutions to a wide range of problems. To improve the generalization ability of their Q-learning method, we propose a novel learning strategy based on AlphaGo Zero which is a Go engine that achieved a superhuman level without the domain knowledge of the game. Our framework is redesigned for combinatorial problems, where the final reward might take any real number instead of a binary response, win/lose. In experiments conducted for five kinds of NP-hard problems including \{{\textbackslash}sc MinimumVertexCover\} and \{{\textbackslash}sc MaxCut\}, our method is shown to generalize better to various graphs than S2V-DQN. Furthermore, our method can be combined with recently-developed graph neural network (GNN) models such as the {\textbackslash}emph\{Graph Isomorphism Network\}, resulting in even better performance. This experiment also gives an interesting insight into a suitable choice of GNN models for each task.},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {Abe, Kenshin and Xu, Zijian and Sato, Issei and Sugiyama, Masashi},
	month = mar,
	year = {2020},
	note = {arXiv:1905.11623 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{friedman_approximate_2018,
	title = {Approximate {Knowledge} {Compilation} by {Online} {Collapsed} {Importance} {Sampling}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/4f164cf233807fc02da06599a1264dee-Abstract.html},
	abstract = {We introduce collapsed compilation, a novel approximate inference algorithm for discrete probabilistic graphical models. It is a collapsed sampling algorithm that incrementally selects which variable to sample next based on the partial compila- tion obtained so far. This online collapsing, together with knowledge compilation inference on the remaining variables, naturally exploits local structure and context- specific independence in the distribution. These properties are used implicitly in exact inference, but are difficult to harness for approximate inference. More- over, by having a partially compiled circuit available during sampling, collapsed compilation has access to a highly effective proposal distribution for importance sampling. Our experimental evaluation shows that collapsed compilation performs well on standard benchmarks. In particular, when the amount of exact inference is equally limited, collapsed compilation is competitive with the state of the art, and outperforms it on several benchmarks.},
	urldate = {2023-05-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Friedman, Tal and Van den Broeck, Guy},
	year = {2018},
	keywords = {ucla},
}

@misc{kumar_first-order_2023,
	title = {First-{Order} {Context}-{Specific} {Likelihood} {Weighting} in {Hybrid} {Probabilistic} {Logic} {Programs}},
	url = {http://arxiv.org/abs/2201.11165},
	abstract = {Statistical relational AI and probabilistic logic programming have so far mostly focused on discrete probabilistic models. The reasons for this is that one needs to provide constructs to succinctly model the independencies in such models, and also provide eﬃcient inference. Three types of independencies are important to represent and exploit for scalable inference in hybrid models: conditional independencies elegantly modeled in Bayesian networks, context-speciﬁc independencies naturally represented by logical rules, and independencies amongst attributes of related objects in relational models succinctly expressed by combining rules.},
	language = {en},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Kumar, Nitesh and Kuzelka, Ondrej and De Raedt, Luc},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11165 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, leuven},
}

@inproceedings{ong_learnable_2022,
	title = {Learnable {Commutative} {Monoids} for {Graph} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=WtFobB28VDey},
	abstract = {Graph neural networks (GNNs) have been shown to be highly sensitive to the choice of aggregation function. While summing over a node's neighbours can approximate any permutation-invariant function over discrete inputs, Cohen-Karlik et al. [2020] proved there are set-aggregation problems for which summing cannot generalise to unbounded inputs, proposing recurrent neural networks regularised towards permutation-invariance as a more expressive aggregator. We show that these results carry over to the graph domain: GNNs equipped with recurrent aggregators are competitive with state-of-the-art permutation-invariant aggregators, on both synthetic benchmarks and real-world problems. However, despite the benefits of recurrent aggregators, their O(V) depth makes them both difficult to parallelise and harder to train on large graphs. Inspired by the observation that a well-behaved aggregator for a GNN is a commutative monoid over its latent space, we propose a framework for constructing learnable, commutative, associative binary operators. And with this, we construct an aggregator of O(log V) depth, yielding exponential improvements for both parallelism and dependency length while achieving performance competitive with recurrent aggregators. Based on our empirical observations, our proposed learnable commutative monoid (LCM) aggregator represents a favourable tradeoff between efficient and expressive aggregators.},
	language = {en},
	urldate = {2023-05-25},
	author = {Ong, Euan and Veličković, Petar},
	month = nov,
	year = {2022},
}

@techreport{evans_can_2018,
	title = {Can {Neural} {Networks} {Understand} {Logical} {Entailment}?},
	shorttitle = {{PossibleWorldNet}},
	url = {http://arxiv.org/abs/1802.08535},
	abstract = {We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a "convolution over possible worlds". Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.},
	number = {arXiv:1802.08535},
	urldate = {2022-09-23},
	institution = {arXiv},
	author = {Evans, Richard and Saxton, David and Amos, David and Kohli, Pushmeet and Grefenstette, Edward},
	month = feb,
	year = {2018},
	note = {arXiv:1802.08535 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{huang_scallop_2021,
	title = {Scallop: {From} {Probabilistic} {Deductive} {Databases} to {Scalable} {Differentiable} {Reasoning}},
	volume = {34},
	shorttitle = {Scallop},
	url = {https://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html},
	abstract = {Deep learning and symbolic reasoning are complementary techniques for an intelligent system. However, principled combinations of these techniques have limited scalability, rendering them ill-suited for real-world applications. We propose Scallop, a system that builds upon probabilistic deductive databases, to bridge this gap. The key insight underlying Scallop is a provenance framework that introduces a tunable parameter to specify the level of reasoning granularity. Scallop thereby i) generalizes exact probabilistic reasoning, ii) asymptotically reduces computational cost, and iii) provides relative accuracy guarantees. On a suite of tasks that involve mathematical and logical reasoning, Scallop scales significantly better without sacrificing accuracy compared to DeepProbLog, a principled neural logic programming approach. We also create and evaluate on a real-world Visual Question Answering (VQA) benchmark that requires multi-hop reasoning. Scallop outperforms two VQA-tailored models, a Neural Module Networks based and a transformer based model, by 12.42\% and 21.66\% respectively.},
	urldate = {2023-05-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Huang, Jiani and Li, Ziyang and Chen, Binghong and Samel, Karan and Naik, Mayur and Song, Le and Si, Xujie},
	year = {2021},
	pages = {25134--25145},
}

@misc{wu_interpretability_2023,
	title = {Interpretability at {Scale}: {Identifying} {Causal} {Mechanisms} in {Alpaca}},
	shorttitle = {Interpretability at {Scale}},
	url = {http://arxiv.org/abs/2305.08809},
	abstract = {Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS) [23] is a powerful gradient descent method grounded in a theory of causal abstraction that uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models ﬁne-tuned for speciﬁc tasks. In the present paper, we scale DAS signiﬁcantly by replacing the remaining brute-force search steps with learned parameters – an approach we call Boundless DAS. This enables us to efﬁciently search for interpretable causal structure in large language models while they follow instructions. We apply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With Boundless DAS, we discover that Alpaca does this by implementing a causal model with two interpretable boolean variables. Furthermore, we ﬁnd that the alignment of neural representations with these variables is robust to changes in inputs and instructions. These ﬁndings mark a ﬁrst step toward deeply understanding the inner-workings of our largest and most widely deployed language models.},
	language = {en},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Wu, Zhengxuan and Geiger, Atticus and Potts, Christopher and Goodman, Noah D.},
	month = may,
	year = {2023},
	note = {arXiv:2305.08809 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{lu_survey_2022,
	title = {A {Survey} of {Deep} {Learning} for {Mathematical} {Reasoning}},
	url = {http://arxiv.org/abs/2212.10535},
	doi = {10.48550/arXiv.2212.10535},
	abstract = {Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.},
	urldate = {2022-12-26},
	publisher = {arXiv},
	author = {Lu, Pan and Qiu, Liang and Yu, Wenhao and Welleck, Sean and Chang, Kai-Wei},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10535 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{lamb_graph_2021,
	title = {Graph {Neural} {Networks} {Meet} {Neural}-{Symbolic} {Computing}: {A} {Survey} and {Perspective}},
	shorttitle = {{GNN} {Meet} {NeSy}},
	url = {http://arxiv.org/abs/2003.00330},
	abstract = {Neural-symbolic computing has now become the subject of interest of both academic and industry research laboratories. Graph Neural Networks (GNNs) have been widely used in relational and symbolic domains, with widespread application of GNNs in combinatorial optimization, constraint satisfaction, relational reasoning and other scientiﬁc domains. The need for improved explainability, interpretability and trust of AI systems in general demands principled methodologies, as suggested by neural-symbolic computing. In this paper, we review the state-of-the-art on the use of GNNs as a model of neural-symbolic computing. This includes the application of GNNs in several domains as well as their relationship to current developments in neural-symbolic computing.},
	language = {en},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Lamb, Luis C. and Garcez, Artur and Gori, Marco and Prates, Marcelo and Avelar, Pedro and Vardi, Moshe},
	month = jun,
	year = {2021},
	note = {arXiv:2003.00330 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, todo},
}

@article{sourek_lifted_2018,
	title = {Lifted {Relational} {Neural} {Networks}: {Efficient} {Learning} of {Latent} {Relational} {Structures}},
	volume = {62},
	copyright = {Copyright (c) 0},
	issn = {1076-9757},
	shorttitle = {{LRNN}},
	url = {https://www.jair.org/index.php/jair/article/view/11203},
	doi = {10.1613/jair.1.11203},
	abstract = {We propose a method to combine the interpretability and expressive power of firstorder logic with the effectiveness of neural network learning. In particular, we introduce a lifted framework in which first-order rules are used to describe the structure of a given problem setting. These rules are then used as a template for constructing a number of neural networks, one for each training and testing example. As the different networks corresponding to different examples share their weights, these weights can be efficiently learned using stochastic gradient descent. Our framework provides a flexible way for implementing and combining a wide variety of modelling constructs. In particular, the use of first-order logic allows for a declarative specification of latent relational structures, which can then be efficiently discovered in a given data set using neural network learning. Experiments on 78 relational learning benchmarks clearly demonstrate the effectiveness of the framework.},
	language = {en},
	urldate = {2023-05-17},
	journal = {Journal of Artificial Intelligence Research},
	author = {Sourek, Gustav and Aschenbrenner, Vojtech and Zelezny, Filip and Schockaert, Steven and Kuzelka, Ondrej},
	month = may,
	year = {2018},
	pages = {69--100},
}

@techreport{broomhead_radial_1988,
	title = {Radial {Basis} {Functions}, {Multi}-{Variable} {Functional} {Interpolation} and {Adaptive} {Networks}},
	url = {https://apps.dtic.mil/sti/citations/ADA196234},
	abstract = {The relationship between learning in adaptive layered networks and the fitting of data with high dimensional surfaces is discussed. This leads naturally to a picture of generalization in terms of interpolation between known data points and suggests a rational approach to the theory of such networks. A class of adaptive networks is identified which makes the interpolation scheme explicit. This class has the property that learning is equivalent to the solution of a set of linear equations. These networks thus represent nonlinear relationships while having a guaranteed learning rule. Great Britain.},
	language = {en},
	urldate = {2023-05-04},
	institution = {Royal Signals and Radar Establishment Malvern (United Kingdom)},
	author = {Broomhead, David S. and Lowe, David},
	year = {1988},
	note = {Section: Technical Reports},
}

@phdthesis{manhaeve_neural_2021,
	address = {Leuven},
	title = {Neural {Probabilistic} {Logic} {Programming}},
	shorttitle = {{DeepProbLog}},
	language = {en},
	school = {KU Leuven},
	author = {Manhaeve, Robin},
	month = dec,
	year = {2021},
	keywords = {leuven},
}

@article{bistarelli_semiring-based_1997,
	title = {Semiring-based constraint satisfaction and optimization},
	volume = {44},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/256303.256306},
	doi = {10.1145/256303.256306},
	abstract = {We introduce a general framework for constraint satisfaction and optimization where classical CSPs, fuzzy CSPs, weighted CSPs, partial constraint satisfaction, and others can be easily cast. The framework is based on a semiring structure, where the set of the semiring specifies the values to be associated with each tuple of values of the variable domain, and the two semiring operations (ϩ and ϫ) model constraint projection and combination respectively. Local consistency algorithms, as usually used for classical CSPs, can be exploited in this general framework as well, provided that certain conditions on the semiring operations are satisfied. We then show how this framework can be used to model both old and new constraint solving and optimization schemes, thus allowing one to both formally justify many informally taken choices in existing schemes, and to prove that local consistency techniques can be used also in newly defined schemes.},
	language = {en},
	number = {2},
	urldate = {2023-05-14},
	journal = {Journal of the ACM},
	author = {Bistarelli, Stefano and Montanari, Ugo and Rossi, Francesca},
	month = mar,
	year = {1997},
	pages = {201--236},
}

@article{darwiche_knowledge_2002,
	title = {A {Knowledge} {Compilation} {Map}},
	volume = {17},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/10311},
	doi = {10.1613/jair.989},
	abstract = {We propose a perspective on knowledge compilation which    calls for analyzing different compilation approaches according to two    key dimensions: the succinctness of the target compilation language,    and the class of queries and transformations that the language    supports in polytime. We then provide a knowledge compilation map,    which analyzes a large number of existing target compilation languages    according to their succinctness and their polytime transformations and    queries. We argue that such analysis is necessary for placing new    compilation approaches within the context of existing ones. We also go    beyond classical, flat target compilation languages based on CNF and    DNF, and consider a richer, nested class based on directed acyclic    graphs (such as OBDDs), which we show to include a relatively large    number of target compilation languages.},
	language = {en},
	urldate = {2023-05-10},
	journal = {Journal of Artificial Intelligence Research},
	author = {Darwiche, A. and Marquis, P.},
	month = sep,
	year = {2002},
	pages = {229--264},
}

@book{flach_simply_1994,
	title = {Simply logical: intelligent reasoning by example},
	shorttitle = {Simply logical},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Flach, Peter},
	year = {1994},
}

@inproceedings{du_robustness_2023,
	address = {Dubrovnik, Croatia},
	title = {Robustness {Challenges} in {Model} {Distillation} and {Pruning} for {Natural} {Language} {Understanding}},
	url = {https://aclanthology.org/2023.eacl-main.129},
	abstract = {Recent work has focused on compressing pre-trained language models (PLMs) like BERT where the major focus has been to improve the in-distribution performance for downstream tasks. However, very few of these studies have analyzed the impact of compression on the generalizability and robustness of compressed models for out-of-distribution (OOD) data. Towards this end, we study two popular model compression techniques including knowledge distillation and pruning and show that the compressed models are significantly less robust than their PLM counterparts on OOD test sets although they obtain similar performance on in-distribution development sets for a task. Further analysis indicates that the compressed models overfit on the shortcut samples and generalize poorly on the hard ones. We further leverage this observation to develop a regularization strategy for robust model compression based on sample uncertainty.},
	urldate = {2023-05-02},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Du, Mengnan and Mukherjee, Subhabrata and Cheng, Yu and Shokouhi, Milad and Hu, Xia and Awadallah, Ahmed Hassan},
	month = may,
	year = {2023},
	pages = {1758--1770},
}

@misc{cropper_inductive_2022,
	title = {Inductive logic programming at 30: a new introduction},
	shorttitle = {{ILP} at 30},
	url = {http://arxiv.org/abs/2008.07912},
	doi = {10.48550/arXiv.2008.07912},
	abstract = {Inductive logic programming (ILP) is a form of machine learning. The goal of ILP is to induce a hypothesis (a set of logical rules) that generalises training examples. As ILP turns 30, we provide a new introduction to the field. We introduce the necessary logical notation and the main learning settings; describe the building blocks of an ILP system; compare several systems on several dimensions; describe four systems (Aleph, TILDE, ASPAL, and Metagol); highlight key application areas; and, finally, summarise current limitations and directions for future research.},
	urldate = {2022-09-12},
	publisher = {arXiv},
	author = {Cropper, Andrew and Dumančić, Sebastijan},
	month = mar,
	year = {2022},
	note = {arXiv:2008.07912 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@techreport{garcez_neurosymbolic_2020,
	title = {Neurosymbolic {AI}: {The} 3rd {Wave}},
	shorttitle = {{NeSy}},
	url = {http://arxiv.org/abs/2012.05876},
	abstract = {Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.},
	number = {arXiv:2012.05876},
	urldate = {2022-09-28},
	institution = {arXiv},
	author = {Garcez, Artur d'Avila and Lamb, Luis C.},
	month = dec,
	year = {2020},
	doi = {10.48550/arXiv.2012.05876},
	note = {arXiv:2012.05876 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.4, I.2.6},
}

@misc{sarker_neuro-symbolic_2021,
	title = {Neuro-{Symbolic} {Artificial} {Intelligence}: {Current} {Trends}},
	shorttitle = {{NeSy}},
	url = {http://arxiv.org/abs/2105.05330},
	abstract = {Neuro-Symbolic Artificial Intelligence -- the combination of symbolic methods with methods that are based on artificial neural networks -- has a long-standing history. In this article, we provide a structured overview of current trends, by means of categorizing recent publications from key conferences. The article is meant to serve as a convenient starting point for research on the general topic.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Sarker, Md Kamruzzaman and Zhou, Lu and Eberhart, Aaron and Hitzler, Pascal},
	month = may,
	year = {2021},
	note = {arXiv:2105.05330 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{bansal_end--end_2022,
	title = {End-to-end {Algorithm} {Synthesis} with {Recurrent} {Networks}: {Logical} {Extrapolation} {Without} {Overthinking}},
	shorttitle = {{E2E} {Algorithm} {Synthesis} with {RNN}},
	url = {http://arxiv.org/abs/2202.05826},
	doi = {10.48550/arXiv.2202.05826},
	abstract = {Machine learning systems perform well on pattern matching tasks, but their ability to perform algorithmic or logical reasoning is not well understood. One important reasoning capability is algorithmic extrapolation, in which models trained only on small/simple reasoning problems can synthesize complex strategies for large/complex problems at test time. Algorithmic extrapolation can be achieved through recurrent systems, which can be iterated many times to solve difficult reasoning problems. We observe that this approach fails to scale to highly complex problems because behavior degenerates when many iterations are applied -- an issue we refer to as "overthinking." We propose a recall architecture that keeps an explicit copy of the problem instance in memory so that it cannot be forgotten. We also employ a progressive training routine that prevents the model from learning behaviors that are specific to iteration number and instead pushes it to learn behaviors that can be repeated indefinitely. These innovations prevent the overthinking problem, and enable recurrent systems to solve extremely hard extrapolation tasks.},
	urldate = {2022-11-25},
	publisher = {arXiv},
	author = {Bansal, Arpit and Schwarzschild, Avi and Borgnia, Eitan and Emam, Zeyad and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
	month = oct,
	year = {2022},
	note = {arXiv:2202.05826 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{graves_hybrid_2016,
	title = {Hybrid computing using a neural network with dynamic external memory},
	volume = {538},
	copyright = {2016 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	shorttitle = {{DNC}},
	url = {https://www.nature.com/articles/nature20101},
	doi = {10.1038/nature20101},
	abstract = {Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read–write memory.},
	language = {en},
	number = {7626},
	urldate = {2022-11-30},
	journal = {Nature},
	author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwińska, Agnieszka and Colmenarejo, Sergio Gómez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adrià Puigdomènech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
	month = oct,
	year = {2016},
	note = {Number: 7626
Publisher: Nature Publishing Group},
	keywords = {Learning algorithms, Network models},
	pages = {471--476},
}

@misc{graves_neural_2014,
	title = {Neural {Turing} {Machines}},
	shorttitle = {{NTM}},
	url = {http://arxiv.org/abs/1410.5401},
	abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efﬁciently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
	language = {en},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
	month = dec,
	year = {2014},
	note = {arXiv:1410.5401 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{sourek_beyond_2021,
	title = {Beyond graph neural networks with lifted relational neural networks},
	volume = {110},
	issn = {1573-0565},
	shorttitle = {{LRNN}},
	url = {https://doi.org/10.1007/s10994-021-06017-3},
	doi = {10.1007/s10994-021-06017-3},
	abstract = {We introduce a declarative differentiable programming framework, based on the language of Lifted Relational Neural Networks, where small parameterized logic programs are used to encode deep relational learning scenarios through the underlying symmetries. When presented with relational data, such as various forms of graphs, the logic program interpreter dynamically unfolds differentiable computation graphs to be used for the program parameter optimization by standard means. Following from the declarative, relational logic-based encoding, this results into a unified representation of a wide range of neural models in the form of compact and elegant learning programs, in contrast to the existing procedural approaches operating directly on the computational graph level. We illustrate how this idea can be used for a concise encoding of existing advanced neural architectures, with the main focus on Graph Neural Networks (GNNs). Importantly, using the framework, we also show how the contemporary GNN models can be easily extended towards higher expressiveness in various ways. In the experiments, we demonstrate correctness and computation efficiency through comparison against specialized GNN frameworks, while shedding some light on the learning performance of the existing GNN models.},
	language = {en},
	number = {7},
	urldate = {2023-03-22},
	journal = {Machine Learning},
	author = {Šourek, Gustav and Železný, Filip and Kuželka, Ondřej},
	month = jul,
	year = {2021},
	keywords = {Datalog, Differentiable programming, Graph neural networks, Lifted relational neural networks, Molecule classification, Relational learning, Symmetries},
	pages = {1695--1738},
}

@article{marra_relational_2020,
	title = {Relational neural machines},
	shorttitle = {{RNM}},
	journal = {arXiv preprint arXiv:2002.02193},
	author = {Marra, Giuseppe and Diligenti, Michelangelo and Giannini, Francesco and Gori, Marco and Maggini, Marco},
	year = {2020},
}

@inproceedings{qu_probabilistic_2019,
	title = {Probabilistic {Logic} {Neural} {Networks} for {Reasoning}},
	volume = {32},
	shorttitle = {{pLogicNet}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/13e5ebb0fa112fe1b31a1067962d74a7-Abstract.html},
	abstract = {Knowledge graph reasoning, which aims at predicting missing facts through reasoning with observed facts, is critical for many applications. Such a problem has been widely explored by traditional logic rule-based approaches and recent knowledge graph embedding methods. A principled logic rule-based approach is the Markov Logic Network (MLN), which is able to leverage domain knowledge with first-order logic and meanwhile handle uncertainty. However, the inference in MLNs is usually very difficult due to the complicated graph structures. Different from MLNs, knowledge graph embedding methods (e.g. TransE, DistMult) learn effective entity and relation embeddings for reasoning, which are much more effective and efficient. However, they are unable to leverage domain knowledge. In this paper, we propose the probabilistic Logic Neural Network (pLogicNet), which combines the advantages of both methods. A pLogicNet defines the joint distribution of all possible triplets by using a Markov logic network with first-order logic, which can be efficiently optimized with the variational EM algorithm. Specifically, in the E-step, a knowledge graph embedding model is used for inferring the missing triplets, while in the M-step, the weights of the logic rules are updated according to both the observed and predicted triplets. Experiments on multiple knowledge graphs prove the effectiveness of pLogicNet over many competitive baselines.},
	urldate = {2023-01-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Qu, Meng and Tang, Jian},
	year = {2019},
	keywords = {todo},
}

@inproceedings{marra_neural_2021,
	title = {Neural markov logic networks},
	shorttitle = {{NMLN}},
	url = {https://proceedings.mlr.press/v161/marra21a.html},
	abstract = {We introduce neural Markov logic networks (NMLNs), a statistical relational learning system that borrows ideas from Markov logic. Like Markov logic networks (MLNs), NMLNs are an exponential-family model for modelling distributions over possible worlds, but unlike MLNs, they do not rely on explicitly specified first-order logic rules. Instead, NMLNs learn an implicit representation of such rules as a neural network that acts as a potential function on fragments of the relational structure. Similarly to many neural symbolic methods, NMLNs can exploit embeddings of constants but, unlike them, NMLNs work well also in their absence. This is extremely important for predicting in settings other than the transductive one. We showcase the potential of NMLNs on knowledge-base completion, triple classification and on generation of molecular (graph) data.},
	language = {en},
	urldate = {2022-10-12},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Marra, Giuseppe and Kuželka, Ondřej},
	month = dec,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {908--917},
}

@misc{sourek_lifted_2015,
	title = {Lifted {Relational} {Neural} {Networks}},
	shorttitle = {{LRNN}},
	url = {http://arxiv.org/abs/1508.05128},
	abstract = {We propose a method combining relational-logic representations with neural network learning. A general lifted architecture, possibly reﬂecting some background domain knowledge, is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reﬂect the structures of given training or testing relational examples. Diﬀerent networks corresponding to diﬀerent examples share their weights, which co-evolve during training by stochastic gradient descent algorithm. The framework allows for hierarchical relational modeling constructs and learning of latent relational concepts through shared hidden layers weights corresponding to the rules. Discovery of notable relational concepts and experiments on 78 relational learning benchmarks demonstrate favorable performance of the method.},
	language = {en},
	urldate = {2023-02-20},
	publisher = {arXiv},
	author = {Sourek, Gustav and Aschenbrenner, Vojtech and Zelezny, Filip and Kuzelka, Ondrej},
	month = oct,
	year = {2015},
	note = {arXiv:1508.05128 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{verga_adaptable_2021,
	address = {Online},
	title = {Adaptable and {Interpretable} {Neural} {MemoryOver} {Symbolic} {Knowledge}},
	shorttitle = {{FILM}},
	url = {https://aclanthology.org/2021.naacl-main.288},
	doi = {10.18653/v1/2021.naacl-main.288},
	abstract = {Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information: however, augmenting or modifying this information requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a “fact memory”. Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge-intensive question-answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state-of-the-art open-book model, despite using 5\% of the parameters. Most interestingly, we demonstrate that the model can be modified, without any re-training, by updating the fact memory.},
	urldate = {2022-11-25},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Verga, Pat and Sun, Haitian and Baldini Soares, Livio and Cohen, William},
	month = jun,
	year = {2021},
	pages = {3678--3691},
}

@misc{das_go_2018,
	title = {Go for a {Walk} and {Arrive} at the {Answer}: {Reasoning} {Over} {Paths} in {Knowledge} {Bases} using {Reinforcement} {Learning}},
	shorttitle = {{MINERVA}},
	url = {http://arxiv.org/abs/1711.05851},
	abstract = {Knowledge bases (KB), both automatically and manually constructed, are often incomplete — many valid facts can be inferred from the KB by synthesizing existing information. A popular approach to KB completion is to infer new relations by combinatory reasoning over the information found along other paths connecting a pair of entities. Given the enormous size of KBs and the exponential number of paths, previous path-based models have considered only the problem of predicting a missing relation given two entities, or evaluating the truth of a proposed triple. Additionally, these methods have traditionally used random paths between ﬁxed entity pairs or more recently learned to pick paths between them. We propose a new algorithm, MINERVA, which addresses the much more difﬁcult and practical task of answering questions where the relation is known, but only one entity. Since random walks are impractical in a setting with unknown destination and combinatorially many paths from a start node, we present a neural reinforcement learning approach which learns how to navigate the graph conditioned on the input query to ﬁnd predictive paths. On a comprehensive evaluation on seven knowledge base datasets, we found MINERVA to be competitive with many current state-of-the-art methods.},
	language = {en},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Das, Rajarshi and Dhuliawala, Shehzaad and Zaheer, Manzil and Vilnis, Luke and Durugkar, Ishan and Krishnamurthy, Akshay and Smola, Alex and McCallum, Andrew},
	month = dec,
	year = {2018},
	note = {arXiv:1711.05851 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{riegel_logical_2020,
	title = {Logical {Neural} {Networks}},
	shorttitle = {{LNN}},
	url = {http://arxiv.org/abs/2006.13155},
	doi = {10.48550/arXiv.2006.13155},
	abstract = {We propose a novel framework seamlessly providing key properties of both neural nets (learning) and symbolic logic (knowledge and reasoning). Every neuron has a meaning as a component of a formula in a weighted real-valued logic, yielding a highly intepretable disentangled representation. Inference is omnidirectional rather than focused on predefined target variables, and corresponds to logical reasoning, including classical first-order logic theorem proving as a special case. The model is end-to-end differentiable, and learning minimizes a novel loss function capturing logical contradiction, yielding resilience to inconsistent knowledge. It also enables the open-world assumption by maintaining bounds on truth values which can have probabilistic semantics, yielding resilience to incomplete knowledge.},
	urldate = {2022-09-09},
	publisher = {arXiv},
	author = {Riegel, Ryan and Gray, Alexander and Luus, Francois and Khan, Naweed and Makondo, Ndivhuwo and Akhalwaya, Ismail Yunus and Qian, Haifeng and Fagin, Ronald and Barahona, Francisco and Sharma, Udit and Ikbal, Shajith and Karanam, Hima and Neelam, Sumit and Likhyani, Ankita and Srivastava, Santosh},
	month = jun,
	year = {2020},
	note = {arXiv:2006.13155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning},
}

@misc{de_smet_neural_nodate,
	title = {Neural {Probabilistic} {Logic} {Programming} {In} {Discrete}-{Continuous} {Domains}},
	shorttitle = {{DSeaPL}},
	author = {De Smet, Lennert and De Raedt, Luc},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Symbolic Computation, D.3.1, I.2.4, I.2.6, leuven},
}

@article{manhaeve_neural_2021-1,
	title = {Neural probabilistic logic programming in {DeepProbLog}},
	volume = {298},
	issn = {0004-3702},
	shorttitle = {{DeepProbLog}},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370221000552},
	doi = {10.1016/j.artint.2021.103504},
	abstract = {We introduce DeepProbLog, a neural probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques of the underlying probabilistic logic programming language ProbLog can be adapted for the new language. We theoretically and experimentally demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.},
	language = {en},
	urldate = {2022-09-15},
	journal = {Artificial Intelligence},
	author = {Manhaeve, Robin and Dumančić, Sebastijan and Kimmig, Angelika and Demeester, Thomas and De Raedt, Luc},
	month = sep,
	year = {2021},
	keywords = {Learning and reasoning, Logic, Neural networks, Neuro-symbolic integration, Probabilistic logic programming, Probability, leuven},
	pages = {103504},
}

@inproceedings{manhaeve_approximate_2021,
	title = {Approximate {Inference} for {Neural} {Probabilistic} {Logic} {Programming}},
	shorttitle = {{DPLA}*},
	abstract = {DeepProbLog is a neural-symbolic framework that integrates probabilistic logic programming and neural networks. It is realized by providing an interface between the probabilistic logic and the neural networks. Inference in probabilistic neural symbolic methods is hard, since it combines logical theorem proving with probabilistic inference and neural network evaluation. In this work, we make the inference more efficient by extending an approximate inference algorithm from the field of statistical-relational AI. Instead of considering all possible proofs for a certain query, the system searches for the best proof. However, training a DeepProbLog model using approximate inference introduces additional challenges, as the best proof is unknown at the start of training which can lead to convergence towards a local optimum. To be able to apply DeepProbLog on larger tasks, we propose: 1) a method for approximate inference using an A*-like search, called DPLA* 2) an exploration strategy for proving in a neural-symbolic setting, and 3) a parametric heuristic to guide the proof search. We empirically evaluate the performance and scalability of the new approach, and also compare the resulting approach to other neural-symbolic systems. The experiments show that DPLA* achieves a speed up of up to 2-3 orders of magnitude in some cases.},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Principles} of {Knowledge} {Representation} and {Reasoning}},
	author = {Manhaeve, Robin and Marra, Giuseppe and De Raedt, Luc},
	year = {2021},
	keywords = {leuven},
	pages = {475--486},
}

@article{van_krieken_analyzing_2022,
	title = {Analyzing {Differentiable} {Fuzzy} {Logic} {Operators}},
	volume = {302},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370221001533},
	doi = {10.1016/j.artint.2021.103602},
	abstract = {The AI community is increasingly putting its attention towards combining symbolic and neural approaches, as it is often argued that the strengths and weaknesses of these approaches are complementary. One recent trend in the literature is weakly supervised learning techniques that employ operators from fuzzy logics. In particular, these use prior background knowledge described in such logics to help the training of a neural network from unlabeled and noisy data. By interpreting logical symbols using neural networks, this background knowledge can be added to regular loss functions, hence making reasoning a part of learning. We study, both formally and empirically, how a large collection of logical operators from the fuzzy logic literature behave in a differentiable learning setting. We find that many of these operators, including some of the most well-known, are highly unsuitable in this setting. A further finding concerns the treatment of implication in these fuzzy logics, and shows a strong imbalance between gradients driven by the antecedent and the consequent of the implication. Furthermore, we introduce a new family of fuzzy implications (called sigmoidal implications) to tackle this phenomenon. Finally, we empirically show that it is possible to use Differentiable Fuzzy Logics for semi-supervised learning, and compare how different operators behave in practice. We find that, to achieve the largest performance improvement over a supervised baseline, we have to resort to non-standard combinations of logical operators which perform well in learning, but no longer satisfy the usual logical laws.},
	language = {en},
	urldate = {2023-05-01},
	journal = {Artificial Intelligence},
	author = {van Krieken, Emile and Acar, Erman and van Harmelen, Frank},
	month = jan,
	year = {2022},
	keywords = {Fuzzy logic, Learning with constraints, Neural-symbolic AI},
	pages = {103602},
}

@article{julian-iranzo_sound_2017,
	series = {Theme: {Logic} and {Computer} {Science}},
	title = {A sound and complete semantics for a similarity-based logic programming language},
	volume = {317},
	issn = {0165-0114},
	url = {https://www.sciencedirect.com/science/article/pii/S0165011416304572},
	doi = {10.1016/j.fss.2016.12.016},
	abstract = {Similarity-based Logic Programming replaces the syntactic unification algorithm of classical SLD-resolution by a fuzzy one, leading to an operational mechanism that we name Weak SLD-resolution. This is the operational semantics of a subset of Bousi∼Prolog, an extension of Prolog aiming at making the query answering process more flexible. In this paper, after recalling the model-theoretic and fixpoint semantics for a pure subset of this language, we detail the operational semantics of Bousi∼Prolog and we prove, among other results, its soundness and completeness. Significantly, throughout this work we also clarify some of the differences between our framework and other related proposals.},
	language = {en},
	urldate = {2023-04-16},
	journal = {Fuzzy Sets and Systems},
	author = {Julián-Iranzo, Pascual and Rubio-Manzano, Clemente},
	month = jun,
	year = {2017},
	keywords = {Declarative semantics, Fixpoint semantics, Fuzzy logic programming, Operational semantics, Similarity relations, Weak SLD-resolution},
	pages = {1--26},
}

@misc{de_jong_neural_2019,
	title = {Neural {Theorem} {Provers} {Do} {Not} {Learn} {Rules} {Without} {Exploration}},
	url = {http://arxiv.org/abs/1906.06805},
	abstract = {Neural symbolic processing aims to combine the generalization of logical learning approaches and the performance of neural networks. The Neural Theorem Proving (NTP) model by Rocktaschel et al (2017) learns embeddings for concepts and performs logical uniﬁcation. While NTP is promising and effective in predicting facts accurately, we have little knowledge how well it can extract true relationship among data. To this end, we create synthetic logical datasets with injected relationships, which can be generated on-the-ﬂy, to test neural-based relation learning algorithms including NTP. We show that it has difﬁculty recovering relationships in all but the simplest settings. Critical analysis and diagnostic experiments suggest that the optimization algorithm suffers from poor local minima due to its greedy winner-takes-all strategy in identifying the most informative structure (proof path) to pursue. We alter the NTP algorithm to increase exploration, which sharply improves performance. We argue and demonstate that it is insightful to benchmark with synthetic data with ground-truth relationships, for both evaluating models and revealing algorithmic issues.},
	language = {en},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {de Jong, Michiel and Sha, Fei},
	month = jun,
	year = {2019},
	note = {arXiv:1906.06805 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yang_injecting_nodate,
	title = {Injecting {Logical} {Constraints} into {Neural} {Networks} via {Straight}-{Through} {Estimators}},
	abstract = {Injecting discrete logical constraints into neural network learning is one of the main challenges in neuro-symbolic AI. We ﬁnd that a straightthrough-estimator, a method introduced to train binary neural networks, could effectively be applied to incorporate logical constraints into neural network learning. More speciﬁcally, we design a systematic way to represent discrete logical constraints as a loss function; minimizing this loss using gradient descent via a straight-throughestimator updates the neural network’s weights in the direction that the binarized outputs satisfy the logical constraints. The experimental results show that by leveraging GPUs and batch training, this method scales signiﬁcantly better than existing neuro-symbolic methods that require heavy symbolic computation for computing gradients. Also, we demonstrate that our method applies to different types of neural networks, such as MLP, CNN, and GNN, making them learn with no or fewer labeled data by learning directly from known constraints.},
	language = {en},
	author = {Yang, Zhun and Lee, Joohyung and Park, Chiyoun},
}

@misc{zhang_multimodal_2023,
	title = {Multimodal {Analogical} {Reasoning} over {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2210.00312},
	abstract = {Analogical reasoning is fundamental to human cognition and holds an important place in various ﬁelds. However, previous studies mainly focus on single-modal analogical reasoning and ignore taking advantage of structure knowledge. Notably, the research in cognitive psychology has demonstrated that information from multimodal sources always brings more powerful cognitive transfer than single modality sources. To this end, we introduce the new task of multimodal analogical reasoning over knowledge graphs, which requires multimodal reasoning ability with the help of background knowledge. Speciﬁcally, we construct a Multimodal Analogical Reasoning dataSet (MARS) and a multimodal knowledge graph MarKG. We evaluate with multimodal knowledge graph embedding and pre-trained Transformer baselines, illustrating the potential challenges of the proposed task. We further propose a novel model-agnostic Multimodal analogical reasoning framework with Transformer (MarT) motivated by the structure mapping theory, which can obtain better performance. We hope our work can deliver beneﬁts and inspire future research1.},
	language = {en},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Zhang, Ningyu and Li, Lei and Chen, Xiang and Liang, Xiaozhuan and Deng, Shumin and Chen, Huajun},
	month = feb,
	year = {2023},
	note = {arXiv:2210.00312 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
}

@article{zhu_multi-modal_2022,
	title = {Multi-{Modal} {Knowledge} {Graph} {Construction} and {Application}: {A} {Survey}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Multi-{Modal} {Knowledge} {Graph} {Construction} and {Application}},
	url = {http://arxiv.org/abs/2202.05786},
	doi = {10.1109/TKDE.2022.3224228},
	abstract = {Recent years have witnessed the resurgence of knowledge engineering which is featured by the fast growth of knowledge graphs. However, most of existing knowledge graphs are represented with pure symbols, which hurts the machine’s capability to understand the real world. The multi-modalization of knowledge graphs is an inevitable key step towards the realization of human-level machine intelligence. The results of this endeavor are Multi-modal Knowledge Graphs (MMKGs). In this survey on MMKGs constructed by texts and images, we ﬁrst give deﬁnitions of MMKGs, followed with the preliminaries on multi-modal tasks and techniques. We then systematically review the challenges, progresses and opportunities on the construction and application of MMKGs respectively, with detailed analyses of the strengths and weaknesses of different solutions. We ﬁnalize this survey with open research problems relevant to MMKGs.},
	language = {en},
	urldate = {2023-04-03},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhu, Xiangru and Li, Zhixu and Wang, Xiaodan and Jiang, Xueyao and Sun, Penglei and Wang, Xuwu and Xiao, Yanghua and Yuan, Nicholas Jing},
	year = {2022},
	note = {arXiv:2202.05786 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, E.0, E.2, I.2.4},
	pages = {1--20},
}

@misc{xie_image-embodied_2017,
	title = {Image-embodied {Knowledge} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1609.07028},
	abstract = {Entity images could provide signiﬁcant visual information for knowledge representation learning. Most conventional methods learn knowledge representations merely from structured triples, ignoring rich visual information extracted from entity images. In this paper, we propose a novel Imageembodied Knowledge Representation Learning model (IKRL), where knowledge representations are learned with both triple facts and images. More speciﬁcally, we ﬁrst construct representations for all images of an entity with a neural image encoder. These image representations are then integrated into an aggregated image-based representation via an attention-based method. We evaluate our IKRL models on knowledge graph completion and triple classiﬁcation. Experimental results demonstrate that our models outperform all baselines on both tasks, which indicates the signiﬁcance of visual information for knowledge representations and the capability of our models in learning knowledge representations with images.},
	language = {en},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Xie, Ruobing and Liu, Zhiyuan and Luan, Huanbo and Sun, Maosong},
	month = may,
	year = {2017},
	note = {arXiv:1609.07028 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{qu_rnnlogic_2021,
	title = {{RNNLogic}: {Learning} {Logic} {Rules} for {Reasoning} on {Knowledge} {Graphs}},
	shorttitle = {{RNNLogic}},
	url = {http://arxiv.org/abs/2010.04029},
	abstract = {This paper studies learning logic rules for reasoning on knowledge graphs. Logic rules provide interpretable explanations when used for prediction as well as being able to generalize to other tasks, and hence are critical to learn. Existing methods either suffer from the problem of searching in a large search space (e.g., neural logic programming) or ineffective optimization due to sparse rewards (e.g., techniques based on reinforcement learning). To address these limitations, this paper proposes a probabilistic model called RNNLogic. RNNLogic treats logic rules as a latent variable, and simultaneously trains a rule generator as well as a reasoning predictor with logic rules. We develop an EM-based algorithm for optimization. In each iteration, the reasoning predictor is first updated to explore some generated logic rules for reasoning. Then in the E-step, we select a set of high-quality rules from all generated rules with both the rule generator and reasoning predictor via posterior inference; and in the M-step, the rule generator is updated with the rules selected in the E-step. Experiments on four datasets prove the effectiveness of RNNLogic.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Qu, Meng and Chen, Junkun and Xhonneux, Louis-Pascal and Bengio, Yoshua and Tang, Jian},
	month = jul,
	year = {2021},
	note = {arXiv:2010.04029 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, todo},
}

@misc{rajasekharan_reliable_2023,
	title = {Reliable {Natural} {Language} {Understanding} with {Large} {Language} {Models} and {Answer} {Set} {Programming}},
	url = {http://arxiv.org/abs/2302.03780},
	abstract = {Humans understand language by extracting information (meaning) from sentences, combining it with existing commonsense knowledge, and then performing reasoning to draw conclusions. While large language models (LLMs) such as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a variety of NLP tasks, they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question. In order to emulate humans better, we propose STAR, a framework that combines LLMs with Answer Set Programming (ASP). We show how LLMs can be used to effectively extract knowledge—represented as predicates—from language. Goal-directed ASP is then employed to reliably reason over this knowledge. We apply the STAR framework to three different NLU tasks requiring reasoning: qualitative reasoning, mathematical reasoning, and goal-directed conversation. Our experiments reveal that STAR is able to bridge the gap of reasoning in NLU tasks, leading to signiﬁcant performance improvements, especially for smaller LLMs, i.e., LLMs with a smaller number of parameters. NLU applications developed using the STAR framework are also explainable: along with the predicates generated, a justiﬁcation in the form of a proof tree can be produced for a given output.},
	language = {en},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Rajasekharan, Abhiramon and Zeng, Yankai and Padalkar, Parth and Gupta, Gopal},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03780 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{nye_improving_nodate,
	title = {Improving {Coherence} and {Consistency} in {Neural} {Sequence} {Models} with {Dual}-{System}, {Neuro}-{Symbolic} {Reasoning}},
	abstract = {Human reasoning can be understood as an interplay between two systems: the intuitive and associative (“System 1”) and the deliberative and logical (“System 2”). Neural sequence models—which have been increasingly successful at performing complex, structured tasks—exhibit the advantages and failure modes of System 1: they are fast and learn patterns from data, but are often inconsistent and incoherent. In this work, we seek a lightweight, training-free means of improving existing System 1-like sequence models by adding System 2-inspired logical reasoning. We explore several variations on this theme in which candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. Our approach uses neural inference to mediate between the neural System 1 and the logical System 2. Results in robust story generation and grounded instruction-following show that this approach can increase the coherence and accuracy of neurally-based generations.},
	language = {en},
	author = {Nye, Maxwell and Tessler, Michael Henry and Tenenbaum, Joshua B and Lake, Brenden M},
}

@inproceedings{sato_statistical_1995,
	title = {A statistical learning method for logic programs with distribution semantics},
	booktitle = {Proceedings of the 12th international conference on logic programming ({ICLP}’95)},
	publisher = {Citeseer},
	author = {Sato, Taisuke},
	year = {1995},
	pages = {715--729},
}

@article{trask_neural_nodate,
	title = {Neural {Arithmetic} {Logic} {Units}},
	abstract = {Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images. In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.},
	language = {en},
	author = {Trask, Andrew and Hill, Felix and Reed, Scott E and Rae, Jack and Dyer, Chris and Blunsom, Phil},
}

@article{renkens_k-optimal_2012,
	title = {k-{Optimal}: a novel approximate inference algorithm for {ProbLog}},
	volume = {89},
	issn = {0885-6125, 1573-0565},
	shorttitle = {k-{Optimal}},
	url = {http://link.springer.com/10.1007/s10994-012-5304-9},
	doi = {10.1007/s10994-012-5304-9},
	abstract = {ProbLog is a probabilistic extension of Prolog. Given the complexity of exact inference under ProbLog’s semantics, in many applications in machine learning approximate inference is necessary. Current approximate inference algorithms for ProbLog however require either dealing with large numbers of proofs or do not guarantee a low approximation error. In this paper we introduce a new approximate inference algorithm which addresses these shortcomings. Given a user-speciﬁed parameter k, this algorithm approximates the success probability of a query based on at most k proofs and ensures that the calculated probability p is (1 − 1/e)p∗ ≤ p ≤ p∗, where p∗ is the highest probability that can be calculated based on any set of k proofs. Furthermore a useful feature of the set of calculated proofs is that it is diverse. Our experiments show the utility of the proposed algorithm.},
	language = {en},
	number = {3},
	urldate = {2023-01-05},
	journal = {Machine Learning},
	author = {Renkens, Joris and Van den Broeck, Guy and Nijssen, Siegfried},
	month = dec,
	year = {2012},
	keywords = {leuven},
	pages = {215--231},
}

@inproceedings{shterionov_cproblog_2014,
	title = {{cProbLog}: {Restricting} the possible worlds of probabilistic logic programs},
	shorttitle = {{cProbLog}},
	booktitle = {Proceedings {FLOC} workshop {Probabilistic} {Logic} {Programming}},
	author = {Shterionov, Dimitar and Janssens, Gerda},
	year = {2014},
	keywords = {leuven},
	pages = {1--12},
}

@article{orsini_kproblog_2017,
	title = {{kProbLog}: an algebraic {Prolog} for machine learning},
	volume = {106},
	issn = {0885-6125, 1573-0565},
	shorttitle = {{kProbLog}},
	url = {http://link.springer.com/10.1007/s10994-017-5668-y},
	doi = {10.1007/s10994-017-5668-y},
	abstract = {We introduce kProbLog as a declarative logical language for machine learning. kProbLog is a simple algebraic extension of Prolog with facts and rules annotated by semiring labels. It allows to elegantly combine algebraic expressions with logic programs. We introduce the semantics of kProbLog, its inference algorithm, its implementation and provide convergence guarantees. We provide several code examples to illustrate its potential for a wide range of machine learning techniques. In particular, we show the encodings of state-of-the-art graph kernels such as Weisfeiler-Lehman graph kernels, propagation kernels and an instance of graph invariant kernels, a recent framework for graph kernels with continuous attributes. However, kProbLog is not limited to kernel methods and it can concisely express declarative formulations of tensor-based algorithms such as matrix factorization and energy-based models, and it can exploit semirings of dual numbers to perform algorithmic differentiation. Furthermore, experiments show that kProbLog is not only of theoretical interest, but can also be applied to real-world datasets. At the technical level, kProbLog extends aProbLog (an algebraic Prolog) by allowing multiple semirings to coexist in a single program and by introducing meta-functions for manipulating algebraic values.},
	language = {en},
	number = {12},
	urldate = {2023-01-09},
	journal = {Machine Learning},
	author = {Orsini, Francesco and Frasconi, Paolo and De Raedt, Luc},
	month = dec,
	year = {2017},
	keywords = {leuven},
	pages = {1933--1969},
}

@misc{van_daele_automated_2019,
	title = {An {Automated} {Engineering} {Assistant}: {Learning} {Parsers} for {Technical} {Drawings}},
	shorttitle = {An {Automated} {Engineering} {Assistant}},
	url = {http://arxiv.org/abs/1909.08552},
	abstract = {From a set of technical drawings and expert knowledge, we automatically learn a parser to interpret such a drawing. This enables automatic reasoning and learning on top of a large database of technical drawings. In this work, we develop a similarity based search algorithm to help engineers and designers ﬁnd or complete designs more easily and ﬂexibly. This is part of an ongoing eﬀort to build an automated engineering assistant. The proposed methods make use of both neural methods to learn to interpret images, and symbolic methods to learn to interpret the structure in the technical drawing and incorporate expert knowledge.},
	language = {en},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Van Daele, Dries and Decleyre, Nicholas and Dubois, Herman and Meert, Wannes},
	month = sep,
	year = {2019},
	note = {arXiv:1909.08552 [cs]},
	keywords = {Computer Science - Artificial Intelligence, leuven},
}

@misc{kazemi_lambada_2022,
	title = {{LAMBADA}: {Backward} {Chaining} for {Automated} {Reasoning} in {Natural} {Language}},
	shorttitle = {{LAMBADA}},
	url = {http://arxiv.org/abs/2212.13894},
	doi = {10.48550/arXiv.2212.13894},
	abstract = {Remarkable progress has been made on automated reasoning with knowledge specified as unstructured, natural text, by using the power of large language models (LMs) coupled with methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from the intended conclusion to the set of axioms that support it) is significantly more efficient at proof-finding problems. We import this intuition into the LM setting and develop a Backward Chaining algorithm, which we call LAMBADA, that decomposes reasoning into four sub-modules, each of which can be simply implemented by few-shot prompted LM inference. We show that LAMBADA achieves massive accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.},
	urldate = {2022-12-29},
	publisher = {arXiv},
	author = {Kazemi, Seyed Mehran and Kim, Najoung and Bhatia, Deepti and Xu, Xin and Ramachandran, Deepak},
	month = dec,
	year = {2022},
	note = {arXiv:2212.13894 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{mialon_augmented_2023,
	title = {Augmented {Language} {Models}: a {Survey}},
	shorttitle = {Augmented {Language} {Models}},
	url = {http://arxiv.org/abs/2302.07842},
	abstract = {This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is deﬁned as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.},
	language = {en},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Mialon, Grégoire and Dessì, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozière, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and Grave, Edouard and LeCun, Yann and Scialom, Thomas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07842 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{han_folio_2022,
	title = {{FOLIO}: {Natural} {Language} {Reasoning} with {First}-{Order} {Logic}},
	shorttitle = {{FOLIO}},
	url = {http://arxiv.org/abs/2209.00840},
	abstract = {We present FOLIO, a human-annotated, opendomain, and logically complex and diverse dataset for reasoning in natural language (NL), equipped with ﬁrst order logic (FOL) annotations. FOLIO consists of 1,435 examples (unique conclusions), each paired with one of 487 sets of premises which serve as rules to be used to deductively reason for the validity of each conclusion. The logical correctness of premises and conclusions is ensured by their parallel FOL annotations, which are automatically veriﬁed by our FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO automatically constitute a new NL-FOL translation dataset using FOL as the logical form. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised ﬁne-tuning on medium-sized language models (BERT, RoBERTa) and few-shot prompting on large language models (GPT-NeoX, OPT, GPT-3, Codex). For NL-FOL translation, we experiment with GPT-3 and Codex. Our results show that one of the most capable Large Language Model (LLM) publicly available, GPT-3 davinci, achieves only slightly better than random results with few-shot prompting on a subset of FOLIO, and the model is especially bad at predicting the correct truth values for False and Unknown conclusions. Our dataset and code are available at https:// github.com/Yale-LILY/FOLIO.},
	language = {en},
	urldate = {2023-02-16},
	publisher = {arXiv},
	author = {Han, Simeng and Schoelkopf, Hailey and Zhao, Yilun and Qi, Zhenting and Riddell, Martin and Benson, Luke and Sun, Lucy and Zubova, Ekaterina and Qiao, Yujie and Burtell, Matthew and Peng, David and Fan, Jonathan and Liu, Yixin and Wong, Brian and Sailor, Malcolm and Ni, Ansong and Nan, Linyong and Kasai, Jungo and Yu, Tao and Zhang, Rui and Joty, Shafiq and Fabbri, Alexander R. and Kryscinski, Wojciech and Lin, Xi Victoria and Xiong, Caiming and Radev, Dragomir},
	month = sep,
	year = {2022},
	note = {arXiv:2209.00840 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{honda_question_2019,
	title = {Question {Answering} {Systems} {With} {Deep} {Learning}-{Based} {Symbolic} {Processing}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2948081},
	abstract = {The authors propose methods to learn symbolic processing with deep learning and to build question answering systems by means of learned models. Symbolic processing, performed by the Prolog processing systems which execute unification, resolution, and list operations, is learned by a combination of deep learning models, Neural Machine Translation (NMT) and Word2Vec training. To our knowledge, the implementation of a Prolog-like processing system using deep learning is a new experiment that has not been conducted in the past. The results of their experiments revealed that the proposed methods are superior to the conventional methods because symbolic processing (1) has rich representations, (2) can interpret inputs even if they include unknown symbols, and (3) can be learned with a small amount of training data. In particular (2), handling of unknown data, which is a major task in artificial intelligence research, is solved using Word2Vec. Furthermore, question answering systems can be built from knowledge bases written in Prolog with learned symbolic processing, which, with conventional methods, is extremely difficult to accomplish. Their proposed systems can not only answer questions through powerful inferences by utilizing facts that harbor unknown data but also have the potential to build knowledge bases from a large amount of data, including unknown data, on the Web. The proposed systems are a completely new trial, there is no state-of-the-art methods in the sense of “newest”. Therefore, to evaluate their efficiency, they are compared with the most traditional and robust system i.e., the Prolog system. This is new research that encompasses the subjects of conventional artificial intelligence and neural network, and their systems have higher potential to build applications such as FAQ chatbots, decision support systems and energy-efficient estimation using a large amount of information on the Web. Mining hidden information through these applications will provide great value.},
	journal = {IEEE Access},
	author = {Honda, Hiroshi and Hagiwara, Masafumi},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Deep learning, Knowledge based systems, Knowledge discovery, Neural networks, Reflective binary codes, Training data, Word2Vec, knowledge base, neural machine translation, prolog, question answering system, symbolic processing},
	pages = {152368--152378},
}

@misc{schick_toolformer_2023,
	title = {Toolformer: {Language} {Models} {Can} {Teach} {Themselves} to {Use} {Tools}},
	shorttitle = {Toolformer},
	url = {http://arxiv.org/abs/2302.04761},
	abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacriﬁcing its core language modeling abilities.},
	language = {en},
	urldate = {2023-02-11},
	publisher = {arXiv},
	author = {Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04761 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{brooks_intelligence_1991,
	title = {Intelligence without representation},
	volume = {47},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/000437029190053M},
	doi = {10.1016/0004-3702(91)90053-M},
	abstract = {Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporate—everything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environments.},
	language = {en},
	number = {1},
	urldate = {2023-02-09},
	journal = {Artificial Intelligence},
	author = {Brooks, Rodney A.},
	month = jan,
	year = {1991},
	pages = {139--159},
}

@article{ma_principles_2022,
	title = {On the principles of {Parsimony} and {Self}-consistency for the emergence of intelligence},
	volume = {23},
	issn = {2095-9230},
	url = {https://doi.org/10.1631/FITEE.2200297},
	doi = {10.1631/FITEE.2200297},
	abstract = {Ten years into the revival of deep networks and artificial intelligence, we propose a theoretical framework that sheds light on understanding deep networks within a bigger picture of intelligence in general. We introduce two fundamental principles, Parsimony and Self-consistency, which address two fundamental questions regarding intelligence: what to learn and how to learn, respectively. We believe the two principles serve as the cornerstone for the emergence of intelligence, artificial or natural. While they have rich classical roots, we argue that they can be stated anew in entirely measurable and computable ways. More specifically, the two principles lead to an effective and efficient computational framework, compressive closed-loop transcription, which unifies and explains the evolution of modern deep networks and most practices of artificial intelligence. While we use mainly visual data modeling as an example, we believe the two principles will unify understanding of broad families of autonomous intelligent systems and provide a framework for understanding the brain.},
	language = {en},
	number = {9},
	urldate = {2023-01-18},
	journal = {Frontiers of Information Technology \& Electronic Engineering},
	author = {Ma, Yi and Tsao, Doris and Shum, Heung-Yeung},
	month = sep,
	year = {2022},
	keywords = {Closed-loop transcription, Deep networks, Intelligence, Parsimony, Rate reduction, Self-consistency, TP18},
	pages = {1298--1323},
}

@article{klindt_controlling_2023,
	title = {Controlling {Neural} {Network} {Smoothness} for {Neural} {Algorithmic} {Reasoning}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=JnsGy9uWtI},
	abstract = {The modelling framework of neural algorithmic reasoning (Veličković \& Blundell, 2021) postulates that a continuous neural network may learn to emulate the discrete reasoning steps of a symbolic algorithm. We investigate the underlying hypothesis in the most simple conceivable scenario – the addition of real numbers. Our results show that two layer neural networks fail to learn the structure of the task, despite containing multiple solutions of the true function within their hypothesis class. Growing the network’s width leads to highly complex error regions in the input space. Moreover, we find that the network fails to generalise with increasing severity i) in the training domain, ii) outside of the training domain but within its convex hull, and iii) outside the training domain’s convex hull. This behaviour can be emulated with Gaussian process regressors that use radial basis function kernels of decreasing length scale. Classical results establish an equivalence between Gaussian processes and infinitely wide neural networks. We demonstrate a tight linkage between the scaling of a network weights’ standard deviation and its effective length scale on a sinusoidal regression problem, suggesting simple modifications to control the length scale of the function learned by a neural network and, thus, its smoothness. This has important applications for the different generalisation scenarios suggested above, but it also suggests a partial remedy to the brittleness of neural network predictions as exposed by adversarial examples. We demonstrate the gains in adversarial robustness that our modification achieves on a standard classification problem of handwritten digit recognition. In conclusion, this work shows inherent problems of neural networks even for the simplest algorithmic tasks which, however, may be partially remedied through links to Gaussian processes.},
	language = {en},
	urldate = {2023-02-03},
	journal = {Transactions on Machine Learning Research},
	author = {Klindt, David A.},
	month = feb,
	year = {2023},
	keywords = {todo},
}

@misc{li_advance_2022,
	title = {On the {Advance} of {Making} {Language} {Models} {Better} {Reasoners}},
	url = {http://arxiv.org/abs/2206.02336},
	abstract = {Large language models such as GPT-3 and PaLM have shown remarkable performance in few-shot learning. However, they still struggle with reasoning tasks such as the arithmetic benchmark GSM8K. Recent advances deliberately guide the language model to generate a chain of reasoning steps before producing the ﬁnal answer, successfully boosting the GSM8K benchmark from 17.9\% to 58.1\% in terms of problem solving rate. In this paper, we propose a new approach, DIVERSE (Diverse Veriﬁer on Reasoning Step), to further advance their reasoning capability. DIVERSE ﬁrst explores different prompts to enhance the diversity in reasoning paths. Second, DIVERSE introduces a veriﬁer to distinguish good answers from bad answers for a better weighted voting. Finally, DIVERSE veriﬁes the correctness of each single step rather than all the steps in a whole. We conduct extensive experiments using the latest language model code-davinci-002 and demonstrate that DIVERSE can achieve new state-of-the-art performance on six out of eight reasoning benchmarks (e.g., GSM8K 74.4\% → 83.2\%), outperforming the PaLM model with 540B parameters.},
	language = {en},
	urldate = {2023-01-31},
	publisher = {arXiv},
	author = {Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
	month = jun,
	year = {2022},
	note = {arXiv:2206.02336 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, todo},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer.},
	language = {en},
	urldate = {2023-01-28},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zhou_teaching_2022,
	title = {Teaching {Algorithmic} {Reasoning} via {In}-context {Learning}},
	url = {http://arxiv.org/abs/2211.09066},
	doi = {10.48550/arXiv.2211.09066},
	abstract = {Large language models (LLMs) have shown increasing in-context learning capabilities through scaling up model and data size. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems. While providing a rationale with the final answer has led to further improvements in multi-step reasoning problems, Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved. In this work, we identify and study four key stages for successfully teaching algorithmic reasoning to LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously (skill accumulation), (3) teaching how to combine skills (skill composition) and (4) teaching how to use skills as tools. We show that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which we refer to as algorithmic prompting. We evaluate our approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrate significant boosts in performance over existing prompting techniques. In particular, for long parity, addition, multiplication and subtraction, we achieve an error reduction of approximately 10x, 9x, 5x and 2x respectively compared to the best available baselines.},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Zhou, Hattie and Nova, Azade and Larochelle, Hugo and Courville, Aaron and Neyshabur, Behnam and Sedghi, Hanie},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09066 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{mahowald_dissociating_2023,
	title = {Dissociating language and thought in large language models: a cognitive perspective},
	shorttitle = {Dissociating language and thought in large language models},
	url = {http://arxiv.org/abs/2301.06627},
	abstract = {Short abstract (100 words): Large language models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their capabilities remain split. Here, we evaluate LLMs using a distinction between formal competence—knowledge of linguistic rules and patterns—and functional competence—understanding and using language in the world. We ground this distinction in human neuroscience, showing that these skills recruit different cognitive mechanisms. Although LLMs are close to mastering formal competence, they still fail at functional competence tasks, which often require drawing on non-linguistic capacities. In short, LLMs are good models of language but incomplete models of human thought.},
	language = {en},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	month = jan,
	year = {2023},
	note = {arXiv:2301.06627 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{saparov_language_2023,
	title = {Language {Models} {Are} {Greedy} {Reasoners}: {A} {Systematic} {Formal} {Analysis} of {Chain}-of-{Thought}},
	shorttitle = {Language {Models} {Are} {Greedy} {Reasoners}},
	url = {http://arxiv.org/abs/2210.01240},
	abstract = {Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PRONTOQA, where each example is generated from a synthetic world model represented in ﬁrst-order logic. This allows us to parse the generated chain-ofthought into symbolic proofs for formal analysis. Our analysis on INSTRUCTGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in ﬁctional contexts. However, they have difﬁculty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.},
	language = {en},
	urldate = {2023-01-26},
	publisher = {arXiv},
	author = {Saparov, Abulhair and He, He},
	month = jan,
	year = {2023},
	note = {arXiv:2210.01240 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{keskar_large-batch_2017,
	title = {On {Large}-{Batch} {Training} for {Deep} {Learning}: {Generalization} {Gap} and {Sharp} {Minima}},
	shorttitle = {On {Large}-{Batch} {Training} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1609.04836},
	abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
	urldate = {2021-02-09},
	journal = {arXiv:1609.04836 [cs, math]},
	author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	month = feb,
	year = {2017},
	note = {arXiv: 1609.04836},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@article{rosenfeld_predictability_2020,
	title = {On the {Predictability} of {Pruning} {Across} {Scales}},
	url = {http://arxiv.org/abs/2006.10621},
	abstract = {We show that the error of magnitude-pruned networks follows a scaling law, and that this law is of a fundamentally different nature than that of unpruned networks. We functionally approximate the error of the pruned networks, showing that it is predictable in terms of an invariant tying width, depth, and pruning level, such that networks of vastly different sparsities are freely interchangeable. We demonstrate the accuracy of this functional approximation over scales spanning orders of magnitude in depth, width, dataset size, and sparsity for CIFAR-10 and ImageNet. As neural networks become ever larger and more expensive to train, our findings enable a framework for reasoning conceptually and analytically about pruning.},
	urldate = {2020-12-02},
	journal = {arXiv:2006.10621 [cs, stat]},
	author = {Rosenfeld, Jonathan S. and Frankle, Jonathan and Carbin, Michael and Shavit, Nir},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.10621},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{kusupati_soft_2020,
	title = {Soft {Threshold} {Weight} {Reparameterization} for {Learnable} {Sparsity}},
	url = {http://proceedings.mlr.press/v119/kusupati20a.html},
	abstract = {Sparsity in Deep Neural Networks (DNNs) is studied extensively with the focus of maximizing prediction accuracy given an overall parameter budget. Existing methods rely on uniform or heuristic non-...},
	language = {en},
	urldate = {2021-05-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {5544--5555},
}

@article{liu_finding_2020,
	title = {Finding trainable sparse networks through {Neural} {Tangent} {Transfer}},
	url = {http://arxiv.org/abs/2006.08228},
	abstract = {Deep neural networks have dramatically transformed machine learning, but their memory and energy demands are substantial. The requirements of real biological neural networks are rather modest in comparison, and one feature that might underlie this austerity is their sparse connectivity. In deep learning, trainable sparse networks that perform well on a specific task are usually constructed using label-dependent pruning criteria. In this article, we introduce Neural Tangent Transfer, a method that instead finds trainable sparse networks in a label-free manner. Specifically, we find sparse networks whose training dynamics, as characterized by the neural tangent kernel, mimic those of dense networks in function space. Finally, we evaluate our label-agnostic approach on several standard classification tasks and show that the resulting sparse networks achieve higher classification performance while converging faster.},
	urldate = {2021-02-14},
	journal = {arXiv:2006.08228 [cs, stat]},
	author = {Liu, Tianlin and Zenke, Friedemann},
	month = jul,
	year = {2020},
	note = {arXiv: 2006.08228},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{allen-zhu_convergence_2019,
	title = {A {Convergence} {Theory} for {Deep} {Learning} via {Over}-{Parameterization}},
	url = {http://proceedings.mlr.press/v97/allen-zhu19a.html},
	abstract = {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of wor...},
	language = {en},
	urldate = {2020-12-07},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {242--252},
}

@article{su_sanity-checking_2020,
	title = {Sanity-{Checking} {Pruning} {Methods}: {Random} {Tickets} can {Win} the {Jackpot}},
	shorttitle = {Sanity-{Checking} {Pruning} {Methods}},
	url = {http://arxiv.org/abs/2009.11094},
	abstract = {Network pruning is a method for reducing test-time computational resource requirements with minimal performance degradation. Conventional wisdom of pruning algorithms suggests that: (1) Pruning methods exploit information from training data to find good subnetworks; (2) The architecture of the pruned network is crucial for good performance. In this paper, we conduct sanity checks for the above beliefs on several recent unstructured pruning methods and surprisingly find that: (1) A set of methods which aims to find good subnetworks of the randomly-initialized network (which we call "initial tickets"), hardly exploits any information from the training data; (2) For the pruned networks obtained by these methods, randomly changing the preserved weights in each layer, while keeping the total number of preserved weights unchanged per layer, does not affect the final performance. These findings inspire us to choose a series of simple {\textbackslash}emph\{data-independent\} prune ratios for each layer, and randomly prune each layer accordingly to get a subnetwork (which we call "random tickets"). Experimental results show that our zero-shot random tickets outperform or attain a similar performance compared to existing "initial tickets". In addition, we identify one existing pruning method that passes our sanity checks. We hybridize the ratios in our random ticket with this method and propose a new method called "hybrid tickets", which achieves further improvement. (Our code is publicly available at https://github.com/JingtongSu/sanity-checking-pruning)},
	urldate = {2020-10-27},
	journal = {arXiv:2009.11094 [cs, stat]},
	author = {Su, Jingtong and Chen, Yihang and Cai, Tianle and Wu, Tianhao and Gao, Ruiqi and Wang, Liwei and Lee, Jason D.},
	month = oct,
	year = {2020},
	note = {arXiv: 2009.11094},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hinton_forward-forward_nodate,
	title = {The {Forward}-{Forward} {Algorithm}: {Some} {Preliminary} {Investigations}},
	abstract = {The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth serious investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes can be separated in time, the negative passes can be done ofﬂine, which makes the learning much simpler in the positive pass and allows video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.},
	language = {en},
	author = {Hinton, Geoffrey},
	pages = {17},
}

@inproceedings{evci_rigging_2020,
	title = {Rigging the {Lottery}: {Making} {All} {Tickets} {Winners}},
	shorttitle = {Rigging the {Lottery}},
	url = {http://proceedings.mlr.press/v119/evci20a.html},
	abstract = {Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but th...},
	language = {en},
	urldate = {2021-02-01},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {2943--2952},
}

@article{frankle_dissecting_2019,
	title = {Dissecting {Pruned} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1907.00262},
	abstract = {Pruning is a standard technique for removing unnecessary structure from a neural network to reduce its storage footprint, computational demands, or energy consumption. Pruning can reduce the parameter-counts of many state-of-the-art neural networks by an order of magnitude without compromising accuracy, meaning these networks contain a vast amount of unnecessary structure. In this paper, we study the relationship between pruning and interpretability. Namely, we consider the effect of removing unnecessary structure on the number of hidden units that learn disentangled representations of human-recognizable concepts as identified by network dissection. We aim to evaluate how the interpretability of pruned neural networks changes as they are compressed. We find that pruning has no detrimental effect on this measure of interpretability until so few parameters remain that accuracy beings to drop. Resnet-50 models trained on ImageNet maintain the same number of interpretable concepts and units until more than 90\% of parameters have been pruned.},
	urldate = {2020-12-28},
	journal = {arXiv:1907.00262 [cs, stat]},
	author = {Frankle, Jonathan and Bau, David},
	month = jun,
	year = {2019},
	note = {arXiv: 1907.00262},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{draxler_essentially_2018,
	title = {Essentially {No} {Barriers} in {Neural} {Network} {Energy} {Landscape}},
	url = {http://proceedings.mlr.press/v80/draxler18a.html},
	abstract = {Training neural networks involves finding minima of a high-dimensional non-convex loss function. Relaxing from linear interpolations, we construct continuous paths between minima of recent neural n...},
	language = {en},
	urldate = {2021-02-02},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1309--1318},
}

@misc{burns_discovering_2022,
	title = {Discovering {Latent} {Knowledge} in {Language} {Models} {Without} {Supervision}},
	url = {http://arxiv.org/abs/2212.03827},
	doi = {10.48550/arXiv.2212.03827},
	abstract = {Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4{\textbackslash}\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
	month = dec,
	year = {2022},
	note = {arXiv:2212.03827 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, todo},
}

@misc{hu_harnessing_2020,
	title = {Harnessing {Deep} {Neural} {Networks} with {Logic} {Rules}},
	url = {http://arxiv.org/abs/1603.06318},
	abstract = {Combining deep neural networks with structured logic rules is desirable to harness ﬂexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative ﬁrst-order logic rules. Speciﬁcally, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
	language = {en},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
	month = aug,
	year = {2020},
	note = {arXiv:1603.06318 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, todo},
}

@article{turing_icomputing_1950,
	title = {I.—{COMPUTING} {MACHINERY} {AND} {INTELLIGENCE}},
	volume = {LIX},
	issn = {1460-2113, 0026-4423},
	url = {https://academic.oup.com/mind/article/LIX/236/433/986238},
	doi = {10.1093/mind/LIX.236.433},
	language = {en},
	number = {236},
	urldate = {2023-01-17},
	journal = {Mind},
	author = {Turing, A. M.},
	month = oct,
	year = {1950},
	pages = {433--460},
}

@article{yang_lifted_2022,
	title = {Lifted model checking for relational {MDPs}},
	volume = {111},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-021-06102-7},
	doi = {10.1007/s10994-021-06102-7},
	abstract = {Probabilistic model checking has been developed for verifying systems that have stochastic and nondeterministic behavior. Given a probabilistic system, a probabilistic model checker takes a property and checks whether or not the property holds in that system. For this reason, probabilistic model checking provide rigorous guarantees. So far, however, probabilistic model checking has focused on propositional models where a state is represented by a symbol. On the other hand, it is commonly required to make relational abstractions in planning and reinforcement learning. Various frameworks handle relational domains, for instance, STRIPS planning and relational Markov Decision Processes. Using propositional model checking in relational settings requires one to ground the model, which leads to the well known state explosion problem and intractability. We present pCTL-REBEL, a lifted model checking approach for verifying pCTL properties of relational MDPs. It extends REBEL, a relational model-based reinforcement learning technique, toward relational pCTL model checking. PCTL-REBEL is lifted, which means that rather than grounding, the model exploits symmetries to reason about a group of objects as a whole at the relational level. Theoretically, we show that pCTL model checking is decidable for relational MDPs that have a possibly infinite domain, provided that the states have a bounded size. Practically, we contribute algorithms and an implementation of lifted relational model checking, and we show that the lifted approach improves the scalability of the model checking approach.},
	language = {en},
	number = {10},
	urldate = {2023-01-10},
	journal = {Machine Learning},
	author = {Yang, Wen-Chi and Raskin, Jean-François and De Raedt, Luc},
	month = oct,
	year = {2022},
	keywords = {First-order logic, Lifted inference, Model checking, Probabilistic computation tree logic (pCTL), Relational MDPs, leuven},
	pages = {3797--3838},
}

@misc{bhattarai_tsetlin_2023,
	title = {Tsetlin {Machine} {Embedding}: {Representing} {Words} {Using} {Logical} {Expressions}},
	shorttitle = {Tsetlin {Machine} {Embedding}},
	url = {http://arxiv.org/abs/2301.00709},
	abstract = {Embedding words in vector space is a fundamental first step in state-of-the-art natural language processing (NLP). Typical NLP solutions employ pre-defined vector representations to improve generalization by co-locating similar words in vector space. For instance, Word2Vec is a self-supervised predictive model that captures the context of words using a neural network. Similarly, GLoVe is a popular unsupervised model incorporating corpus-wide word co-occurrence statistics. Such word embedding has significantly boosted important NLP tasks, including sentiment analysis, document classification, and machine translation. However, the embeddings are dense floating-point vectors, making them expensive to compute and difficult to interpret. In this paper, we instead propose to represent the semantics of words with a few defining words that are related using propositional logic. To produce such logical embeddings, we introduce a Tsetlin Machine-based autoencoder that learns logical clauses self-supervised. The clauses consist of contextual words like "black," "cup," and "hot" to define other words like "coffee," thus being human-understandable. We evaluate our embedding approach on several intrinsic and extrinsic benchmarks, outperforming GLoVe on six classification tasks. Furthermore, we investigate the interpretability of our embedding using the logical representations acquired during training. We also visualize word clusters in vector space, demonstrating how our logical embedding co-locate similar words.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Bhattarai, Bimal and Granmo, Ole-Christoffer and Jiao, Lei and Yadav, Rohan and Sharma, Jivitesh},
	month = jan,
	year = {2023},
	note = {arXiv:2301.00709 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@techreport{chen_neurallog_2021,
	title = {{NeuralLog}: {Natural} {Language} {Inference} with {Joint} {Neural} and {Logical} {Reasoning}},
	shorttitle = {{NeuralLog}},
	url = {http://arxiv.org/abs/2105.14167},
	abstract = {Deep learning (DL) based language models achieve high performance on various benchmarks for Natural Language Inference (NLI). And at this time, symbolic approaches to NLI are receiving less attention. Both approaches (symbolic and DL) have their advantages and weaknesses. However, currently, no method combines them in a system to solve the task of NLI. To merge symbolic and deep learning methods, we propose an inference framework called NeuralLog, which utilizes both a monotonicity-based logical inference engine and a neural network language model for phrase alignment. Our framework models the NLI task as a classic search problem and uses the beam search algorithm to search for optimal inference paths. Experiments show that our joint logic and neural inference system improves accuracy on the NLI task and can achieve state-of-art accuracy on the SICK and MED datasets.},
	number = {arXiv:2105.14167},
	urldate = {2022-09-27},
	institution = {arXiv},
	author = {Chen, Zeming and Gao, Qiyue and Moss, Lawrence S.},
	month = jun,
	year = {2021},
	note = {arXiv:2105.14167 [cs]
version: 3
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, todo},
}

@article{fruhwirth_theory_1998,
	title = {Theory and practice of constraint handling rules},
	url = {https://www.sciencedirect.com/science/article/pii/S0743106698100055},
	doi = {10.1016/S0743-1066(98)10005-5},
	urldate = {2022-12-22},
	journal = {The Journal of Logic Programming},
	author = {Frühwirth, Thom},
	year = {1998},
}

@misc{razeghi_impact_2022,
	title = {Impact of {Pretraining} {Term} {Frequencies} on {Few}-{Shot} {Reasoning}},
	url = {http://arxiv.org/abs/2202.07206},
	doi = {10.48550/arXiv.2202.07206},
	abstract = {Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above \$70{\textbackslash}\%\$ (absolute) more accurate on the top 10{\textbackslash}\% frequent terms in comparison to the bottom 10{\textbackslash}\%. Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.},
	urldate = {2022-12-21},
	publisher = {arXiv},
	author = {Razeghi, Yasaman and Logan IV, Robert L. and Gardner, Matt and Singh, Sameer},
	month = may,
	year = {2022},
	note = {arXiv:2202.07206 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{sanyal_apollo_2022,
	title = {{APOLLO}: {A} {Simple} {Approach} for {Adaptive} {Pretraining} of {Language} {Models} for {Logical} {Reasoning}},
	shorttitle = {{APOLLO}},
	url = {http://arxiv.org/abs/2212.09282},
	doi = {10.48550/arXiv.2212.09282},
	abstract = {Logical reasoning of text is an important ability that requires understanding the information present in the text, their interconnections, and then reasoning through them to infer new conclusions. Prior works on improving the logical reasoning ability of language models require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation solutions that restrict the learning of general logical reasoning skills. In this work, we propose APOLLO, an adaptively pretrained language model that has improved logical reasoning abilities. We select a subset of Wikipedia, based on a set of logical inference keywords, for continued pretraining of a language model. We use two self-supervised loss functions: a modified masked language modeling loss where only specific parts-of-speech words, that would likely require more reasoning than basic language understanding, are masked, and a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences. The proposed training paradigm is both simple and independent of task formats. We demonstrate the effectiveness of APOLLO by comparing it with prior baselines on two logical reasoning datasets. APOLLO performs comparably on ReClor and outperforms baselines on LogiQA.},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Sanyal, Soumya and Xu, Yichong and Wang, Shuohang and Yang, Ziyi and Pryzant, Reid and Yu, Wenhao and Zhu, Chenguang and Ren, Xiang},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09282 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{clark_combining_nodate,
	title = {Combining {Symbolic} and {Distributional} {Models} of {Meaning}},
	abstract = {The are two main approaches to the representation of meaning in Computational Linguistics: a symbolic approach and a distributional approach. This paper considers the fundamental question of how these approaches might be combined. The proposal is to adapt a method from the Cognitive Science literature, in which symbolic and connectionist representations are combined using tensor products. Possible applications of this method for language processing are described. Finally, a potentially fruitful link between Quantum Mechanics, Computational Linguistics, and other related areas such as Information Retrieval and Machine Learning, is proposed.},
	language = {en},
	author = {Clark, Stephen},
}

@article{garcez_neural-symbolic_2022,
	title = {Neural-symbolic learning and reasoning: {A} survey and interpretation},
	volume = {342},
	shorttitle = {Neural-symbolic learning and reasoning},
	journal = {Neuro-Symbolic Artificial Intelligence: The State of the Art},
	author = {Garcez, Artur d’Avila and Bader, Sebastian and Bowman, Howard and Lamb, Luis C. and de Penning, Leo and Illuminoo, B. V. and Poon, Hoifung and Gerson Zaverucha, COPPE},
	year = {2022},
	note = {Publisher: IOS Press},
	pages = {1},
}

@article{de_raedt_probabilistic_2015,
	title = {Probabilistic (logic) programming concepts},
	volume = {100},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-015-5494-z},
	doi = {10.1007/s10994-015-5494-z},
	abstract = {A multitude of different probabilistic programming languages exists today, all extending a traditional programming language with primitives to support modeling of complex, structured probability distributions. Each of these languages employs its own probabilistic primitives, and comes with a particular syntax, semantics and inference procedure. This makes it hard to understand the underlying programming concepts and appreciate the differences between the different languages. To obtain a better understanding of probabilistic programming, we identify a number of core programming concepts underlying the primitives used by various probabilistic languages, discuss the execution mechanisms that they require and use these to position and survey state-of-the-art probabilistic languages and their implementation. While doing so, we focus on probabilistic extensions of logic programming languages such as Prolog, which have been considered for over 20 years.},
	language = {en},
	number = {1},
	urldate = {2022-12-12},
	journal = {Machine Learning},
	author = {De Raedt, Luc and Kimmig, Angelika},
	month = jul,
	year = {2015},
	keywords = {Inference in probabilistic languages, Probabilistic logic programming, Probabilistic programming languages, Statistical relational learning, leuven},
	pages = {5--47},
}

@inproceedings{winters_thomas_deepstochlog_nodate,
	title = {{DeepStochLog}: {Neural} {Stochastic} {Logic} {Programming} ({DSL})},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21248},
	abstract = {Recent advances in neural-symbolic learning, such as Deep-
ProbLog, extend probabilistic logic programs with neural pred-
icates. Like graphical models, these probabilistic logic pro-
grams define a probability distribution over possible worlds,
for which inference is computationally hard. We propose Deep-
StochLog, an alternative neural-symbolic framework based
on stochastic definite clause grammars, a kind of stochastic
logic program. More specifically, we introduce neural gram-
mar rules into stochastic definite clause grammars to create
a framework that can be trained end-to-end. We show that
inference and learning in neural stochastic logic program-
ming scale much better than for neural probabilistic logic pro-
grams. Furthermore, the experimental evaluation shows that
DeepStochLog achieves state-of-the-art results on challenging
neural-symbolic learning tasks.},
	urldate = {2022-09-02},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Winters Thomas},
	keywords = {leuven},
}

@inproceedings{rocktaschel_injecting_2015,
	address = {Denver, Colorado},
	title = {Injecting {Logical} {Background} {Knowledge} into {Embeddings} for {Relation} {Extraction}},
	url = {https://aclanthology.org/N15-1118},
	doi = {10.3115/v1/N15-1118},
	urldate = {2022-12-12},
	booktitle = {Proceedings of the 2015 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Rocktäschel, Tim and Singh, Sameer and Riedel, Sebastian},
	month = may,
	year = {2015},
	pages = {1119--1129},
}

@article{wei_emergent_nodate,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample eﬃciency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.},
	language = {en},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raﬀel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeﬀ and Fedus, William},
	pages = {30},
}

@phdthesis{evans_kants_2020,
	title = {Kant’s {Cognitive} {Architecture}},
	language = {en},
	school = {Imperial College London},
	author = {Evans, Richard},
	month = mar,
	year = {2020},
	keywords = {todo},
}

@misc{sinha_clutrr_2019,
	title = {{CLUTRR}: {A} {Diagnostic} {Benchmark} for {Inductive} {Reasoning} from {Text}},
	shorttitle = {{CLUTRR}},
	url = {http://arxiv.org/abs/1908.06177},
	abstract = {The recent success of natural language understanding (NLU) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named CLUTRR, to clarify some key issues related to the robustness and systematicity of NLU systems. Motivated by classic work on inductive logic programming, CLUTRR requires that an NLU system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. CLUTRR allows us to precisely measure a model's ability for systematic generalization by evaluating on held-out combinations of logical rules, and it allows us to evaluate a model's robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and a graph neural network model that works directly with symbolic inputs---with the graph-based model exhibiting both stronger generalization and greater robustness.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Sinha, Koustuv and Sodhani, Shagun and Dong, Jin and Pineau, Joelle and Hamilton, William L.},
	month = sep,
	year = {2019},
	note = {arXiv:1908.06177 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cropper_logical_2020,
	title = {Logical reduction of metarules},
	volume = {109},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-019-05834-x},
	doi = {10.1007/s10994-019-05834-x},
	abstract = {Many forms of inductive logic programming (ILP) use metarules, second-order Horn clauses, to deﬁne the structure of learnable programs and thus the hypothesis space. Deciding which metarules to use for a given learning task is a major open problem and is a trade-off between efﬁciency and expressivity: the hypothesis space grows given more metarules, so we wish to use fewer metarules, but if we use too few metarules then we lose expressivity. In this paper, we study whether fragments of metarules can be logically reduced to minimal ﬁnite subsets. We consider two traditional forms of logical reduction: subsumption and entailment. We also consider a new reduction technique called derivation reduction, which is based on SLD-resolution. We compute reduced sets of metarules for fragments relevant to ILP and theoretically show whether these reduced sets are reductions for more general inﬁnite fragments. We experimentally compare learning with reduced sets of metarules on three domains: Michalski trains, string transformations, and game rules. In general, derivation reduced sets of metarules outperforms subsumption and entailment reduced sets, both in terms of predictive accuracies and learning times.},
	language = {en},
	number = {7},
	urldate = {2022-11-24},
	journal = {Machine Learning},
	author = {Cropper, Andrew and Tourret, Sophie},
	month = jul,
	year = {2020},
	pages = {1323--1369},
}

@article{mantadelis_dedicated_2010,
	title = {Dedicated {Tabling} for a {Probabilistic} {Setting}},
	url = {http://drops.dagstuhl.de/opus/volltexte/2010/2590/},
	doi = {10.4230/LIPICS.ICLP.2010.124},
	abstract = {ProbLog is a probabilistic framework that extends Prolog with probabilistic facts. To compute the probability of a query, the complete SLD proof tree of the query is collected as a sum of products. ProbLog applies advanced techniques to make this feasible and to assess the correct probability. Tabling is a well-known technique to avoid repeated subcomputations and to terminate loops. We investigate how tabling can be used in ProbLog. The challenge is that we have to reconcile tabling with the advanced ProbLog techniques. While standard tabling collects only the answers for the calls, we do need the SLD proof tree. Finally we discuss how to deal with loops in our probabilistic framework. By avoiding repeated subcomputations, our tabling approach not only improves the execution time of ProbLog programs, but also decreases accordingly the memory consumption. We obtain promising results for ProbLog programs using exact probability inference.},
	language = {en},
	urldate = {2022-11-07},
	author = {Mantadelis, Theofrastos and Janssens, Gerda},
	collaborator = {Herbstritt, Marc},
	year = {2010},
	note = {Artwork Size: 10 pages
Medium: application/pdf
Publisher: Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany},
	keywords = {000 Computer science, knowledge, general works, Computer Science},
	pages = {10 pages},
}

@misc{cohen_tensorlog_2016,
	title = {{TensorLog}: {A} {Differentiable} {Deductive} {Database}},
	shorttitle = {{TensorLog}},
	url = {http://arxiv.org/abs/1605.06523},
	abstract = {Large knowledge bases (KBs) are useful in many tasks, but it is unclear how to integrate this sort of knowledge into "deep" gradient-based learning systems. To address this problem, we describe a probabilistic deductive database, called TensorLog, in which reasoning uses a differentiable process. In TensorLog, each clause in a logical theory is first converted into certain type of factor graph. Then, for each type of query to the factor graph, the message-passing steps required to perform belief propagation (BP) are "unrolled" into a function, which is differentiable. We show that these functions can be composed recursively to perform inference in non-trivial logical theories containing multiple interrelated clauses and predicates. Both compilation and inference in TensorLog are efficient: compilation is linear in theory size and proof depth, and inference is linear in database size and the number of message-passing steps used in BP. We also present experimental results with TensorLog and discuss its relationship to other first-order probabilistic logics.},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Cohen, William W.},
	month = jul,
	year = {2016},
	note = {arXiv:1605.06523 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning},
}

@article{cohen_tensorlog_2020,
	title = {{TensorLog}: {A} {Probabilistic} {Database} {Implemented} {Using} {Deep}-{Learning} {Infrastructure}},
	volume = {67},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {{TensorLog}},
	url = {https://jair.org/index.php/jair/article/view/11944},
	doi = {10.1613/jair.1.11944},
	abstract = {We present an implementation of a probabilistic first-order logic called TensorLog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as Tensorflow or Theano. This leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. The integration with these frameworks enables use of GPU-based parallel processors for inference and learning, making TensorLog the first highly parallellizable probabilistic logic. Experimental results show that TensorLog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples.},
	language = {en},
	urldate = {2022-10-31},
	journal = {Journal of Artificial Intelligence Research},
	author = {Cohen, William and Yang, Fan and Mazaitis, Kathryn Rivard},
	month = feb,
	year = {2020},
	pages = {285--325},
}

@article{qin_binary_2020,
	title = {Binary {Neural} {Networks}: {A} {Survey}},
	volume = {105},
	issn = {00313203},
	shorttitle = {Binary {Neural} {Networks}},
	url = {http://arxiv.org/abs/2004.03333},
	doi = {10.1016/j.patcog.2020.107281},
	abstract = {The binary neural network, largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error, improving the network loss function, and reducing the gradient error. We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification, object detection and semantic segmentation. Finally, the challenges that may be faced in future research are prospected.},
	urldate = {2022-10-31},
	journal = {Pattern Recognition},
	author = {Qin, Haotong and Gong, Ruihao and Liu, Xianglong and Bai, Xiao and Song, Jingkuan and Sebe, Nicu},
	month = sep,
	year = {2020},
	note = {arXiv:2004.03333 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {107281},
}

@article{muggleton_stochastic_1996,
	title = {Stochastic {Logic} {Programs}},
	journal = {Advances in inductive logic programming},
	author = {Muggleton, Stephen},
	year = {1996},
}

@misc{shi_neural_2019,
	title = {Neural {Logic} {Networks}},
	url = {http://arxiv.org/abs/1910.08629},
	abstract = {Recent years have witnessed the great success of deep neural networks in many research areas. The fundamental idea behind the design of most neural networks is to learn similarity patterns from data for prediction and inference, which lacks the ability of logical reasoning. However, the concrete ability of logical reasoning is critical to many theoretical and practical problems. In this paper, we propose Neural Logic Network (NLN), which is a dynamic neural architecture that builds the computational graph according to input logical expressions. It learns basic logical operations as neural modules, and conducts propositional logical reasoning through the network for inference. Experiments on simulated data show that NLN achieves significant performance on solving logical equations. Further experiments on real-world data show that NLN significantly outperforms state-of-the-art models on collaborative filtering and personalized recommendation tasks.},
	urldate = {2022-10-26},
	publisher = {arXiv},
	author = {Shi, Shaoyun and Chen, Hanxiong and Zhang, Min and Zhang, Yongfeng},
	month = oct,
	year = {2019},
	note = {arXiv:1910.08629 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{sato_prism_1997,
	title = {{PRISM} : a language for symbolic-statistical modeling},
	shorttitle = {{PRISM}},
	url = {https://cir.nii.ac.jp/crid/1570291226772455040},
	urldate = {2022-10-25},
	journal = {Proc. of 15th International Joint Conference on Artificial Intelligence (IJCAI-97), 1997},
	author = {Sato, Taisuke and Kayema, Yoshitaka},
	year = {1997},
	pages = {1330--1335},
}

@misc{stacey_logical_2022,
	title = {Logical {Reasoning} with {Span}-{Level} {Predictions} for {Interpretable} and {Robust} {NLI} {Models}},
	url = {http://arxiv.org/abs/2205.11432},
	abstract = {Current Natural Language Inference (NLI) models achieve impressive results, sometimes outperforming humans when evaluating on in-distribution test sets. However, as these models are known to learn from annotation artefacts and dataset biases, it is unclear to what extent the models are learning the task of NLI instead of learning from shallow heuristics in their training data. We address this issue by introducing a logical reasoning framework for NLI, creating highly transparent model decisions that are based on logical rules. Unlike prior work, we show that improved interpretability can be achieved without decreasing the predictive accuracy. We almost fully retain performance on SNLI, while also identifying the exact hypothesis spans that are responsible for each model prediction. Using the e-SNLI human explanations, we verify that our model makes sensible decisions at a span level, despite not using any span labels during training. We can further improve model performance and span-level decisions by using the e-SNLI explanations during training. Finally, our model is more robust in a reduced data setting. When training with only 1,000 examples, out-of-distribution performance improves on the MNLI matched and mismatched validation sets by 13\% and 16\% relative to the baseline. Training with fewer observations yields further improvements, both in-distribution and out-of-distribution.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Stacey, Joe and Minervini, Pasquale and Dubossarsky, Haim and Rei, Marek},
	month = oct,
	year = {2022},
	note = {arXiv:2205.11432 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{deletang_neural_2022,
	title = {Neural {Networks} and the {Chomsky} {Hierarchy}},
	url = {http://arxiv.org/abs/2207.02098},
	abstract = {Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the field. In this work, we conduct an extensive empirical study (10250 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-of-distribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufficient capacity to fit the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Delétang, Grégoire and Ruoss, Anian and Grau-Moya, Jordi and Genewein, Tim and Wenliang, Li Kevin and Catt, Elliot and Cundy, Chris and Hutter, Marcus and Legg, Shane and Veness, Joel and Ortega, Pedro A.},
	month = oct,
	year = {2022},
	note = {arXiv:2207.02098 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Formal Languages and Automata Theory, Computer Science - Machine Learning},
}

@misc{merrill_transformers_2022,
	title = {Transformers {Implement} {First}-{Order} {Logic} with {Majority} {Quantifiers}},
	url = {http://arxiv.org/abs/2210.02671},
	abstract = {Characterizing the implicit structure of the computation within neural networks is a foundational problem in the area of deep learning interpretability. Can their inner decision process be captured symbolically in some familiar logic? We show that any transformer neural network can be translated into an equivalent fixed-size first-order logic formula which may also use majority quantifiers. The idea is to simulate transformers with highly uniform threshold circuits and leverage known theoretical connections between circuits and logic. Our findings also reveal the surprising fact that the entire transformer computation can be reduced merely to the division of two (large) integers. While our results are most pertinent for transformers, they apply equally to a broader class of neural network architectures, namely those with a fixed-depth uniform computation graph made up of standard neural net components, which includes feedforward and convolutional networks.},
	urldate = {2022-10-20},
	publisher = {arXiv},
	author = {Merrill, William and Sabharwal, Ashish},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02671 [cs]},
	keywords = {Computer Science - Computational Complexity, Computer Science - Machine Learning},
}

@misc{meng_locating_2022,
	title = {Locating and {Editing} {Factual} {Associations} in {GPT}},
	url = {http://arxiv.org/abs/2202.05262},
	abstract = {We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
	month = jun,
	year = {2022},
	note = {arXiv:2202.05262 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7},
}

@misc{dittadi_generalization_2022,
	title = {Generalization and {Robustness} {Implications} in {Object}-{Centric} {Learning}},
	url = {http://arxiv.org/abs/2107.00637},
	abstract = {The idea behind object-centric representation learning is that natural scenes can better be modeled as compositions of objects and their relations as opposed to distributed representations. This inductive bias can be injected into neural networks to potentially improve systematic generalization and performance of downstream tasks in scenes with multiple objects. In this paper, we train state-of-the-art unsupervised models on five common multi-object datasets and evaluate segmentation metrics and downstream object property prediction. In addition, we study generalization and robustness by investigating the settings where either a single object is out of distribution -- e.g., having an unseen color, texture, or shape -- or global properties of the scene are altered -- e.g., by occlusions, cropping, or increasing the number of objects. From our experimental study, we find object-centric representations to be useful for downstream tasks and generally robust to most distribution shifts affecting objects. However, when the distribution shift affects the input in a less structured manner, robustness in terms of segmentation and downstream task performance may vary significantly across models and distribution shifts.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Dittadi, Andrea and Papa, Samuele and De Vita, Michele and Schölkopf, Bernhard and Winther, Ole and Locatello, Francesco},
	month = jun,
	year = {2022},
	note = {arXiv:2107.00637 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{he_reduced_2022,
	title = {Reduced {Implication}-bias {Logic} {Loss} for {Neuro}-{Symbolic} {Learning}},
	url = {http://arxiv.org/abs/2208.06838},
	abstract = {Integrating logical reasoning and machine learning by approximating logical inference with differentiable operators is a widely used technique in Neuro-Symbolic systems. However, some differentiable operators could bring a significant bias during backpropagation and degrade the performance of Neuro-Symbolic learning. In this paper, we reveal that this bias, named {\textbackslash}textit\{Implication Bias\} is common in loss functions derived from fuzzy logic operators. Furthermore, we propose a simple yet effective method to transform the biased loss functions into {\textbackslash}textit\{Reduced Implication-bias Logic Loss (RILL)\} to address the above problem. Empirical study shows that RILL can achieve significant improvements compared with the biased logic loss functions, especially when the knowledge base is incomplete, and keeps more robust than the compared methods when labelled data is insufficient.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {He, Haoyuan and Dai, Wangzhou and Li, Ming and Liu, Yu and Ma, Yongchang},
	month = aug,
	year = {2022},
	note = {arXiv:2208.06838 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning},
}

@article{clark_negation_nodate,
	title = {Negation as {Failure}},
	author = {Clark, Keith},
}

@misc{paul_unmasking_2022,
	title = {Unmasking the {Lottery} {Ticket} {Hypothesis}: {What}'s {Encoded} in a {Winning} {Ticket}'s {Mask}?},
	shorttitle = {Unmasking the {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/2210.03044},
	doi = {10.48550/arXiv.2210.03044},
	abstract = {Modern deep learning involves training costly, highly overparameterized networks, thus motivating the search for sparser networks that can still be trained to the same accuracy as the full network (i.e. matching). Iterative magnitude pruning (IMP) is a state of the art algorithm that can find such highly sparse matching subnetworks, known as winning tickets. IMP operates by iterative cycles of training, masking smallest magnitude weights, rewinding back to an early training point, and repeating. Despite its simplicity, the underlying principles for when and how IMP finds winning tickets remain elusive. In particular, what useful information does an IMP mask found at the end of training convey to a rewound network near the beginning of training? How does SGD allow the network to extract this information? And why is iterative pruning needed? We develop answers in terms of the geometry of the error landscape. First, we find that\${\textbackslash}unicode\{x2014\}\$at higher sparsities\${\textbackslash}unicode\{x2014\}\$pairs of pruned networks at successive pruning iterations are connected by a linear path with zero error barrier if and only if they are matching. This indicates that masks found at the end of training convey the identity of an axial subspace that intersects a desired linearly connected mode of a matching sublevel set. Second, we show SGD can exploit this information due to a strong form of robustness: it can return to this mode despite strong perturbations early in training. Third, we show how the flatness of the error landscape at the end of training determines a limit on the fraction of weights that can be pruned at each iteration of IMP. Finally, we show that the role of retraining in IMP is to find a network with new small weights to prune. Overall, these results make progress toward demystifying the existence of winning tickets by revealing the fundamental role of error landscape geometry.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Paul, Mansheej and Chen, Feng and Larsen, Brett W. and Frankle, Jonathan and Ganguli, Surya and Dziugaite, Gintare Karolina},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03044 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{onoro-rubio_representation_2018,
	title = {Representation {Learning} for {Visual}-{Relational} {Knowledge} {Graphs}},
	language = {en},
	author = {Oñoro-Rubio, Daniel and Niepert, Mathias and García-Durán, Alberto and González-Sánchez, Roberto and López-Sastre, Roberto J},
	year = {2018},
	pages = {10},
}

@article{hohenecker_ontology_2020,
	title = {Ontology {Reasoning} with {Deep} {Neural} {Networks}},
	volume = {68},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/11661},
	doi = {10.1613/jair.1.11661},
	abstract = {The ability to conduct logical reasoning is a fundamental aspect of intelligent human behavior, and thus an important problem along the way to human-level artificial intelligence. Traditionally, logic-based symbolic methods from the field of knowledge representation and reasoning have been used to equip agents with capabilities that resemble human logical reasoning qualities. More recently, however, there has been an increasing interest in using machine learning rather than logic-based symbolic formalisms to tackle these tasks. In this paper, we employ state-of-the-art methods for training deep neural networks to devise a novel model that is able to learn how to effectively perform logical reasoning in the form of basic ontology reasoning. This is an important and at the same time very natural logical reasoning task, which is why the presented approach is applicable to a plethora of important real-world problems. We present the outcomes of several experiments, which show that our model is able to learn to perform highly accurate ontology reasoning on very large, diverse, and challenging benchmarks. Furthermore, it turned out that the suggested approach suffers much less from different obstacles that prohibit logic-based symbolic reasoning, and, at the same time, is surprisingly plausible from a biological point of view.},
	language = {en},
	urldate = {2022-10-11},
	journal = {Journal of Artificial Intelligence Research},
	author = {Hohenecker, Patrick and Lukasiewicz, Thomas},
	month = jul,
	year = {2020},
	keywords = {machine learning, neural networks, ontologies},
	pages = {503--540},
}

@misc{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://arxiv.org/abs/1310.4546},
	doi = {10.48550/arXiv.1310.4546},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	urldate = {2022-10-10},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = oct,
	year = {2013},
	note = {arXiv:1310.4546 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{mikolov_linguistic_2019,
	title = {Linguistic {Regularities} in {Continuous} {Space} {Word} {Representations}},
	url = {https://openreview.net/forum?id=BybT3m-ubH},
	abstract = {Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly...},
	language = {en},
	urldate = {2022-10-10},
	author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
	month = jul,
	year = {2019},
}

@misc{trouillon_complex_2016,
	title = {Complex {Embeddings} for {Simple} {Link} {Prediction}},
	url = {http://arxiv.org/abs/1606.06357},
	abstract = {In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Trouillon, Théo and Welbl, Johannes and Riedel, Sebastian and Gaussier, Éric and Bouchard, Guillaume},
	month = jun,
	year = {2016},
	note = {arXiv:1606.06357 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cohen_whirl_2000,
	title = {{WHIRL}: {A} word-based information representation language},
	volume = {118},
	issn = {0004-3702},
	shorttitle = {{WHIRL}},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370299001022},
	doi = {10.1016/S0004-3702(99)00102-2},
	abstract = {We describe WHIRL, an “information representation language” that synergistically combines properties of logic-based and text-based representation systems. WHIRL is a subset of Datalog that has been extended by introducing an atomic type for textual entities, an atomic operation for computing textual similarity, and a “soft” semantics; that is, inferences in WHIRL are associated with numeric scores, and presented to the user in decreasing order by score. This paper briefly describes WHIRL, and then surveys a number of applications. We show that WHIRL strictly generalizes both ranked retrieval of documents, and logical deduction; that nontrivial queries about large databases can be answered efficiently; that WHIRL can be used to accurately integrate data from heterogeneous information sources, such as those found on the Web; that WHIRL can be used effectively for inductive classification of text; and finally, that WHIRL can be used to semi-automatically generate extraction programs for structured documents.},
	language = {en},
	number = {1},
	urldate = {2022-09-28},
	journal = {Artificial Intelligence},
	author = {Cohen, William W.},
	month = apr,
	year = {2000},
	keywords = {Heterogeneous databases, Information extraction, Information integration, Information retrieval, Knowledge representation, Text categorization, Textual similarity},
	pages = {163--196},
}

@techreport{clark_transformers_2020,
	title = {Transformers as {Soft} {Reasoners} over {Language}},
	url = {http://arxiv.org/abs/2002.05867},
	abstract = {Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99\%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95\%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited "soft theorem provers" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering.},
	number = {arXiv:2002.05867},
	urldate = {2022-09-26},
	institution = {arXiv},
	author = {Clark, Peter and Tafjord, Oyvind and Richardson, Kyle},
	month = may,
	year = {2020},
	doi = {10.48550/arXiv.2002.05867},
	note = {arXiv:2002.05867 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@techreport{kim_cogs_2020,
	title = {{COGS}: {A} {Compositional} {Generalization} {Challenge} {Based} on {Semantic} {Interpretation}},
	shorttitle = {{COGS}},
	url = {http://arxiv.org/abs/2010.05465},
	abstract = {Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96--99\%), but generalization accuracy was substantially lower (16--35\%) and showed high sensitivity to random seed (\${\textbackslash}pm\$6--8\%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.},
	number = {arXiv:2010.05465},
	urldate = {2022-09-26},
	institution = {arXiv},
	author = {Kim, Najoung and Linzen, Tal},
	month = oct,
	year = {2020},
	note = {arXiv:2010.05465 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
}

@techreport{saxton_analysing_2019,
	title = {Analysing {Mathematical} {Reasoning} {Abilities} of {Neural} {Models}},
	url = {http://arxiv.org/abs/1904.01557},
	abstract = {Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test splits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes. Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.},
	number = {arXiv:1904.01557},
	urldate = {2022-09-26},
	institution = {arXiv},
	author = {Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
	month = apr,
	year = {2019},
	note = {arXiv:1904.01557 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{mao_neuro-symbolic_2019,
	title = {The {Neuro}-{Symbolic} {Concept} {Learner}: {Interpreting} {Scenes}, {Words}, and {Sentences} {From} {Natural} {Supervision}},
	shorttitle = {The {Neuro}-{Symbolic} {Concept} {Learner}},
	url = {http://arxiv.org/abs/1904.12584},
	abstract = {We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.},
	number = {arXiv:1904.12584},
	urldate = {2022-09-20},
	institution = {arXiv},
	author = {Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum, Joshua B. and Wu, Jiajun},
	month = apr,
	year = {2019},
	doi = {10.48550/arXiv.1904.12584},
	note = {arXiv:1904.12584 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ganesan_learning_2021,
	title = {Learning with {Holographic} {Reduced} {Representations}},
	url = {http://arxiv.org/abs/2109.02157},
	doi = {10.48550/arXiv.2109.02157},
	abstract = {Holographic Reduced Representations (HRR) are a method for performing symbolic AI on top of real-valued vectors by associating each vector with an abstract concept, and providing mathematical operations to manipulate vectors as if they were classic symbolic objects. This method has seen little use outside of older symbolic AI work and cognitive science. Our goal is to revisit this approach to understand if it is viable for enabling a hybrid neural-symbolic approach to learning as a differentiable component of a deep learning architecture. HRRs today are not effective in a differentiable solution due to numerical instability, a problem we solve by introducing a projection step that forces the vectors to exist in a well behaved point in space. In doing so we improve the concept retrieval efficacy of HRRs by over \$100{\textbackslash}times\$. Using multi-label classification we demonstrate how to leverage the symbolic HRR properties to develop an output layer and loss function that is able to learn effectively, and allows us to investigate some of the pros and cons of an HRR neuro-symbolic learning approach. Our code can be found at https://github.com/NeuromorphicComputationResearchProgram/Learning-with-Holographic-Reduced-Representations},
	urldate = {2022-09-09},
	publisher = {arXiv},
	author = {Ganesan, Ashwinkumar and Gao, Hang and Gandhi, Sunil and Raff, Edward and Oates, Tim and Holt, James and McLean, Mark},
	month = dec,
	year = {2021},
	note = {arXiv:2109.02157 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{yang_neurasp_2021,
	address = {Yokohama, Yokohama, Japan},
	series = {{IJCAI}'20},
	title = {{NeurASP}: embracing neural networks into answer set programming},
	isbn = {978-0-9992411-6-5},
	shorttitle = {{NeurASP}},
	abstract = {We present NeurASP, a simple extension of answer set programs by embracing neural networks. By treating the neural network output as the probability distribution over atomic facts in answer set programs, NeurASP provides a simple and effective way to integrate sub-symbolic and symbolic computation. We demonstrate how NeurASP can make use of a pre-trained neural network in symbolic computation and how it can improve the neural network's perception result by applying symbolic reasoning in answer set programming. Also, NeurASP can make use of ASP rules to train a neural network better so that a neural network not only learns from implicit correlations from the data but also from the explicit complex semantic constraints expressed by the rules.},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Yang, Zhun and Ishay, Adam and Lee, Joohyung},
	month = jan,
	year = {2021},
	pages = {1755--1762},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	urldate = {2021-06-03},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {770--778},
}

@article{hoefler_sparsity_2021,
	title = {Sparsity in {Deep} {Learning}: {Pruning} and growth for efficient inference and training in neural networks},
	shorttitle = {Sparsity in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2102.00554},
	abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
	urldate = {2021-05-31},
	journal = {arXiv:2102.00554 [cs]},
	author = {Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
	month = jan,
	year = {2021},
	note = {arXiv: 2102.00554},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Hardware Architecture, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2021-05-31},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	note = {Number: 7553
Publisher: Nature Publishing Group},
	pages = {436--444},
}

@article{hendrycks_gaussian_2020,
	title = {Gaussian {Error} {Linear} {Units} ({GELUs})},
	url = {http://arxiv.org/abs/1606.08415},
	abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x{\textbackslash}Phi(x)\$, where \${\textbackslash}Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x{\textbackslash}mathbf\{1\}\_\{x{\textgreater}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
	urldate = {2021-05-27},
	journal = {arXiv:1606.08415 [cs]},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	month = jul,
	year = {2020},
	note = {arXiv: 1606.08415},
	keywords = {Computer Science - Machine Learning},
}

@article{ramachandran_searching_2017,
	title = {Searching for {Activation} {Functions}},
	url = {http://arxiv.org/abs/1710.05941},
	abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x {\textbackslash}cdot {\textbackslash}text\{sigmoid\}({\textbackslash}beta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9{\textbackslash}\% for Mobile NASNet-A and 0.6{\textbackslash}\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
	urldate = {2021-05-27},
	journal = {arXiv:1710.05941 [cs]},
	author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.05941},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{bray_statistics_2007,
	title = {Statistics of {Critical} {Points} of {Gaussian} {Fields} on {Large}-{Dimensional} {Spaces}},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.98.150201},
	doi = {10.1103/PhysRevLett.98.150201},
	abstract = {We calculate the average number of critical points of a Gaussian field on a high-dimensional space as a function of their energy and their index. Our results give a complete picture of the organization of critical points and are of relevance to glassy and disordered systems and landscape scenarios coming from the anthropic approach to string theory.},
	number = {15},
	urldate = {2021-05-25},
	journal = {Physical Review Letters},
	author = {Bray, Alan J. and Dean, David S.},
	month = apr,
	year = {2007},
	note = {Publisher: American Physical Society},
	pages = {150201},
}

@article{zhou_deconstructing_2019,
	title = {Deconstructing {Lottery} {Tickets}: {Zeros}, {Signs}, and the {Supermask}},
	volume = {32},
	shorttitle = {Deconstructing {Lottery} {Tickets}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/1113d7a76ffceca1bb350bfe145467c6-Abstract.html},
	language = {en},
	urldate = {2021-05-22},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{you_drawing_2019,
	title = {Drawing {Early}-{Bird} {Tickets}: {Toward} {More} {Efficient} {Training} of {Deep} {Networks}},
	shorttitle = {Drawing {Early}-{Bird} {Tickets}},
	url = {https://openreview.net/forum?id=BJxsrgStvr},
	abstract = {(Frankle \& Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies...},
	language = {en},
	urldate = {2021-05-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {You, Haoran and Li, Chaojian and Xu, Pengfei and Fu, Yonggan and Wang, Yue and Chen, Xiaohan and Baraniuk, Richard G. and Wang, Zhangyang and Lin, Yingyan},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Machine Learning, LTH, Statistics - Machine Learning},
}

@article{tanaka_pruning_2020,
	title = {Pruning neural networks without any data by iteratively conserving synaptic flow},
	volume = {33},
	url = {https://proceedings.neurips.cc//paper/2020/hash/46a4378f835dc8040c8057beb6a2da52-Abstract.html},
	language = {en},
	urldate = {2021-05-22},
	journal = {Advances in Neural Information Processing Systems},
	author = {Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L. and Ganguli, Surya},
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	pages = {6377--6389},
}

@article{savarese_winning_2020,
	title = {Winning the {Lottery} with {Continuous} {Sparsification}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/83004190b1793d7aa15f8d0d49a13eba-Abstract.html},
	language = {en},
	urldate = {2021-05-22},
	journal = {Advances in Neural Information Processing Systems},
	author = {Savarese, Pedro and Silva, Hugo and Maire, Michael},
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {11380--11390},
}

@article{morcos_one_2019,
	title = {One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
	volume = {32},
	shorttitle = {One ticket to win them all},
	url = {https://proceedings.neurips.cc/paper/2019/hash/a4613e8d72a61b3b69b32d040f89ad81-Abstract.html},
	language = {en},
	urldate = {2021-05-22},
	journal = {Advances in Neural Information Processing Systems},
	author = {Morcos, Ari and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{lee_snip_2018,
	title = {Snip: {Single}-shot network pruning based on connection sensitivity},
	shorttitle = {{SNIP}},
	url = {https://openreview.net/forum?id=B1VZqjAcYX},
	abstract = {We present a new approach, SNIP, that is simple, versatile and interpretable; it prunes irrelevant connections for a given task at single-shot prior to training and is applicable to a variety of...},
	language = {en},
	urldate = {2021-05-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip},
	month = sep,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{liu_rethinking_2018,
	title = {Rethinking the {Value} of {Network} {Pruning}},
	url = {https://openreview.net/forum?id=rJlnB3C5Ym},
	abstract = {In structured network pruning, fine-tuning a pruned model only gives comparable performance with training it from scratch.},
	language = {en},
	urldate = {2021-05-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
	month = sep,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{frankle_training_2020,
	title = {Training {BatchNorm} and {Only} {BatchNorm}: {On} the {Expressive} {Power} of {Random} {Features} in {CNNs}},
	shorttitle = {Training {BatchNorm} and {Only} {BatchNorm}},
	url = {https://openreview.net/forum?id=vYeQQ29Tbvx},
	abstract = {A wide variety of deep learning techniques from style transfer to multitask learning rely on training affine transformations of features. Most prominent among these is the popular feature...},
	language = {en},
	urldate = {2021-05-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
	month = sep,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{frankle_early_2020,
	title = {The {Early} {Phase} of {Neural} {Network} {Training}},
	url = {https://iclr.cc/virtual_2020/poster_Hkl1iRNFwS.html},
	abstract = {Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state and its updates during these early iterations of training, and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are label-agnostic, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.},
	language = {en},
	urldate = {2021-05-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, LTH, Statistics - Machine Learning},
}

@inproceedings{frankle_pruning_2020,
	title = {Pruning {Neural} {Networks} at {Initialization}: {Why} {Are} {We} {Missing} the {Mark}?},
	shorttitle = {Pruning {Neural} {Networks} at {Initialization}},
	url = {https://openreview.net/forum?id=Ig-VyQc-MLK},
	abstract = {Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al....},
	language = {en},
	urldate = {2021-05-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
	month = sep,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, LTH, Statistics - Machine Learning},
}

@inproceedings{renda_comparing_2019,
	title = {Comparing {Rewinding} and {Fine}-tuning in {Neural} {Network} {Pruning}},
	url = {https://openreview.net/forum?id=S1gSj0NKvB},
	abstract = {Instead of fine-tuning after pruning, rewind weights or learning rate schedule to their values earlier in training and retrain from there to achieve higher accuracy when pruning neural networks.},
	language = {en},
	urldate = {2021-05-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Renda, Alex and Frankle, Jonathan and Carbin, Michael},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Machine Learning, LTH, Statistics - Machine Learning},
}

@inproceedings{frankle_linear_2020,
	title = {Linear {Mode} {Connectivity} and the {Lottery} {Ticket} {Hypothesis}},
	url = {http://proceedings.mlr.press/v119/frankle20a.html},
	abstract = {We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision mod...},
	language = {en},
	urldate = {2021-05-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, LTH, Statistics - Machine Learning},
	pages = {3259--3269},
}

@article{chen_lottery_2020,
	title = {The {Lottery} {Ticket} {Hypothesis} for {Pre}-trained {BERT} {Networks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/b6af2c9703f203a2794be03d443af2e3-Abstract.html},
	language = {en},
	urldate = {2021-05-22},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
	year = {2020},
	keywords = {BERT, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, LTH, Statistics - Machine Learning},
	pages = {15834--15846},
}

@inproceedings{baum_supervised_1988,
	title = {Supervised {Learning} of {Probability} {Distributions} by {Neural} {Networks}},
	url = {https://proceedings.neurips.cc/paper/1987/file/eccbc87e4b5ce2fe28308fd9f2a7baf3-Paper.pdf},
	urldate = {2021-05-22},
	booktitle = {Neural {Information} {Processing} {Systems}},
	publisher = {American Institute of Physics},
	author = {Baum, Eric and Wilczek, Frank},
	editor = {Anderson, D.},
	year = {1988},
}

@article{blalock_what_2020,
	title = {What is the {State} of {Neural} {Network} {Pruning}?},
	volume = {2},
	url = {https://proceedings.mlsys.org/paper/2020/hash/d2ddea18f00665ce8623e36bd4e3c7c5-Abstract.html},
	language = {en},
	urldate = {2021-05-22},
	journal = {Proceedings of Machine Learning and Systems},
	author = {Blalock, Davis and Gonzalez Ortiz, Jose Javier and Frankle, Jonathan and Guttag, John},
	month = mar,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {129--146},
}

@article{hooker_what_2020,
	title = {What {Do} {Compressed} {Deep} {Neural} {Networks} {Forget}?},
	url = {http://arxiv.org/abs/1911.05248},
	abstract = {Deep neural network pruning and quantization techniques have demonstrated it is possible to achieve high levels of compression with surprisingly little degradation to test set accuracy. However, this measure of performance conceals significant differences in how different classes and images are impacted by model compression techniques. We find that models with radically different numbers of weights have comparable top-line performance metrics but diverge considerably in behavior on a narrow subset of the dataset. This small subset of data points, which we term Pruning Identified Exemplars (PIEs) are systematically more impacted by the introduction of sparsity. Compression disproportionately impacts model performance on the underrepresented long-tail of the data distribution. PIEs over-index on atypical or noisy images that are far more challenging for both humans and algorithms to classify. Our work provides intuition into the role of capacity in deep neural networks and the trade-offs incurred by compression. An understanding of this disparate impact is critical given the widespread deployment of compressed models in the wild.},
	urldate = {2021-05-21},
	journal = {arXiv:1911.05248 [cs, stat]},
	author = {Hooker, Sara and Courville, Aaron and Clark, Gregory and Dauphin, Yann and Frome, Andrea},
	month = jul,
	year = {2020},
	note = {arXiv: 1911.05248},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{krizhevsky_learning_2009,
	title = {Learning {Multiple} {Layers} of {Features} from {Tiny} {Images}},
	author = {Krizhevsky, Alex},
	month = apr,
	year = {2009},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/726791/},
	doi = {10.1109/5.726791},
	number = {11},
	urldate = {2021-05-17},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	pages = {2278--2324},
}

@article{mishkin_all_2016,
	title = {All you need is a good init},
	url = {http://arxiv.org/abs/1511.06422},
	abstract = {Layer-sequential unit-variance (LSUV) initialization - a simple method for weight initialization for deep net learning - is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.},
	urldate = {2021-03-04},
	journal = {arXiv:1511.06422 [cs]},
	author = {Mishkin, Dmytro and Matas, Jiri},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.06422},
	keywords = {Computer Science - Machine Learning},
}

@article{zhu_prune_2017,
	title = {To prune, or not to prune: exploring the efficacy of pruning for model compression},
	shorttitle = {To prune, or not to prune},
	url = {http://arxiv.org/abs/1710.01878},
	abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
	urldate = {2021-05-16},
	journal = {arXiv:1710.01878 [cs, stat]},
	author = {Zhu, Michael and Gupta, Suyog},
	month = nov,
	year = {2017},
	note = {arXiv: 1710.01878},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mccandlish_empirical_2018,
	title = {An {Empirical} {Model} of {Large}-{Batch} {Training}},
	url = {http://arxiv.org/abs/1812.06162},
	abstract = {In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.},
	urldate = {2021-04-17},
	journal = {arXiv:1812.06162 [cs, stat]},
	author = {McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Team, OpenAI Dota},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.06162},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{paganini_bespoke_2020,
	title = {Bespoke vs. {Pr}{\textbackslash}{\textasciicircum}et-{\textbackslash}`a-{Porter} {Lottery} {Tickets}: {Exploiting} {Mask} {Similarity} for {Trainable} {Sub}-{Network} {Finding}},
	shorttitle = {Bespoke vs. {Pr}{\textbackslash}{\textasciicircum}et-{\textbackslash}`a-{Porter} {Lottery} {Tickets}},
	url = {http://arxiv.org/abs/2007.04091},
	abstract = {The observation of sparse trainable sub-networks within over-parametrized networks - also known as Lottery Tickets (LTs) - has prompted inquiries around their trainability, scaling, uniqueness, and generalization properties. Across 28 combinations of image classification tasks and architectures, we discover differences in the connectivity structure of LTs found through different iterative pruning techniques, thus disproving their uniqueness and connecting emergent mask structure to the choice of pruning. In addition, we propose a consensus-based method for generating refined lottery tickets. This lottery ticket denoising procedure, based on the principle that parameters that always go unpruned across different tasks more reliably identify important sub-networks, is capable of selecting a meaningful portion of the architecture in an embarrassingly parallel way, while quickly discarding extra parameters without the need for further pruning iterations. We successfully train these sub-networks to performance comparable to that of ordinary lottery tickets.},
	urldate = {2021-05-06},
	journal = {arXiv:2007.04091 [cs, stat]},
	author = {Paganini, Michela and Forde, Jessica Zosa},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.04091
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wang_picking_2020,
	title = {Picking {Winning} {Tickets} {Before} {Training} by {Preserving} {Gradient} {Flow}},
	url = {http://arxiv.org/abs/2002.07376},
	abstract = {Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time. Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80\% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6\% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels.},
	urldate = {2021-05-04},
	journal = {arXiv:2002.07376 [cs, stat]},
	author = {Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
	month = aug,
	year = {2020},
	note = {arXiv: 2002.07376},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{liu_lottery_2021,
	title = {Lottery {Ticket} {Implies} {Accuracy} {Degradation}, {Is} {It} a {Desirable} {Phenomenon}?},
	url = {http://arxiv.org/abs/2102.11068},
	abstract = {In deep model compression, the recent finding "Lottery Ticket Hypothesis" (LTH) (Frankle \& Carbin, 2018) pointed out that there could exist a winning ticket (i.e., a properly pruned sub-network together with original weight initialization) that can achieve competitive performance than the original dense network. However, it is not easy to observe such winning property in many scenarios, where for example, a relatively large learning rate is used even if it benefits training the original dense model. In this work, we investigate the underlying condition and rationale behind the winning property, and find that the underlying reason is largely attributed to the correlation between initialized weights and final-trained weights when the learning rate is not sufficiently large. Thus, the existence of winning property is correlated with an insufficient DNN pretraining, and is unlikely to occur for a well-trained DNN. To overcome this limitation, we propose the "pruning \& fine-tuning" method that consistently outperforms lottery ticket sparse training under the same pruning algorithm and the same total training epochs. Extensive experiments over multiple deep models (VGG, ResNet, MobileNet-v2) on different datasets have been conducted to justify our proposals.},
	urldate = {2021-05-04},
	journal = {arXiv:2102.11068 [cs]},
	author = {Liu, Ning and Yuan, Geng and Che, Zhengping and Shen, Xuan and Ma, Xiaolong and Jin, Qing and Ren, Jian and Tang, Jian and Liu, Sijia and Wang, Yanzhi},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.11068},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{sabatelli_transferability_2020,
	title = {On the {Transferability} of {Winning} {Tickets} in {Non}-{Natural} {Image} {Datasets}},
	url = {http://arxiv.org/abs/2005.05232},
	abstract = {We study the generalization properties of pruned neural networks that are the winners of the lottery ticket hypothesis on datasets of natural images. We analyse their potential under conditions in which training data is scarce and comes from a non-natural domain. Specifically, we investigate whether pruned models that are found on the popular CIFAR-10/100 and Fashion-MNIST datasets, generalize to seven different datasets that come from the fields of digital pathology and digital heritage. Our results show that there are significant benefits in transferring and training sparse architectures over larger parametrized models, since in all of our experiments pruned networks, winners of the lottery ticket hypothesis, significantly outperform their larger unpruned counterparts. These results suggest that winning initializations do contain inductive biases that are generic to some extent, although, as reported by our experiments on the biomedical datasets, their generalization properties can be more limiting than what has been so far observed in the literature.},
	urldate = {2021-04-25},
	journal = {arXiv:2005.05232 [cs]},
	author = {Sabatelli, Matthia and Kestemont, Mike and Geurts, Pierre},
	month = nov,
	year = {2020},
	note = {arXiv: 2005.05232},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{li_towards_2020,
	title = {Towards {Practical} {Lottery} {Ticket} {Hypothesis} for {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2003.05733},
	abstract = {Recent research has proposed the lottery ticket hypothesis, suggesting that for a deep neural network, there exist trainable sub-networks performing equally or better than the original model with commensurate training steps. While this discovery is insightful, finding proper sub-networks requires iterative training and pruning. The high cost incurred limits the applications of the lottery ticket hypothesis. We show there exists a subset of the aforementioned sub-networks that converge significantly faster during the training process and thus can mitigate the cost issue. We conduct extensive experiments to show such sub-networks consistently exist across various model structures for a restrictive setting of hyperparameters (\$e.g.\$, carefully selected learning rate, pruning ratio, and model capacity). As a practical application of our findings, we demonstrate that such sub-networks can help in cutting down the total time of adversarial training, a standard approach to improve robustness, by up to 49{\textbackslash}\% on CIFAR-10 to achieve the state-of-the-art robustness.},
	urldate = {2021-04-11},
	journal = {arXiv:2003.05733 [cs, stat]},
	author = {Li, Bai and Wang, Shiqi and Jia, Yunhan and Lu, Yantao and Zhong, Zhenyu and Carin, Lawrence and Jana, Suman},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.05733},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chen_lottery_2021,
	title = {The {Lottery} {Tickets} {Hypothesis} for {Supervised} and {Self}-supervised {Pre}-training in {Computer} {Vision} {Models}},
	url = {http://arxiv.org/abs/2012.06908},
	abstract = {The computer vision world has been re-gaining enthusiasm in various pre-trained models, including both classical ImageNet supervised pre-training and recently emerged self-supervised pre-training such as simCLR and MoCo. Pre-trained weights often boost a wide range of downstream tasks including classification, detection, and segmentation. Latest studies suggest that pre-training benefits from gigantic model capacity. We are hereby curious and ask: after pre-training, does a pre-trained model indeed have to stay large for its downstream transferability? In this paper, we examine supervised and self-supervised pre-trained models through the lens of the lottery ticket hypothesis (LTH). LTH identifies highly sparse matching subnetworks that can be trained in isolation from (nearly) scratch yet still reach the full models' performance. We extend the scope of LTH and question whether matching subnetworks still exist in pre-trained computer vision models, that enjoy the same downstream transfer performance. Our extensive experiments convey an overall positive message: from all pre-trained weights obtained by ImageNet classification, simCLR, and MoCo, we are consistently able to locate such matching subnetworks at 59.04\% to 96.48\% sparsity that transfer universally to multiple downstream tasks, whose performance see no degradation compared to using full pre-trained weights. Further analyses reveal that subnetworks found from different pre-training tend to yield diverse mask structures and perturbation sensitivities. We conclude that the core LTH observations remain generally relevant in the pre-training paradigm of computer vision, but more delicate discussions are needed in some cases. Codes and pre-trained models will be made available at: https://github.com/VITA-Group/CV\_LTH\_Pre-training.},
	urldate = {2021-04-25},
	journal = {arXiv:2012.06908 [cs]},
	author = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Carbin, Michael and Wang, Zhangyang},
	month = mar,
	year = {2021},
	note = {arXiv: 2012.06908},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{yu_playing_2020,
	title = {Playing the lottery with rewards and multiple languages: lottery tickets in {RL} and {NLP}},
	shorttitle = {Playing the lottery with rewards and multiple languages},
	url = {http://arxiv.org/abs/1906.02768},
	abstract = {The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a "lucky" sub-network initialization being present rather than by helping the optimization process (Frankle \& Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether "winning ticket" initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL).For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with workin supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in DNNs.},
	urldate = {2021-04-25},
	journal = {arXiv:1906.02768 [cs, stat]},
	author = {Yu, Haonan and Edunov, Sergey and Tian, Yuandong and Morcos, Ari S.},
	month = feb,
	year = {2020},
	note = {arXiv: 1906.02768},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{advani_high-dimensional_2017,
	title = {High-dimensional dynamics of generalization error in neural networks},
	url = {http://arxiv.org/abs/1710.03667},
	abstract = {We perform an average case analysis of the generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant "high-dimensional" regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that naive application of worst-case theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
	urldate = {2021-04-24},
	journal = {arXiv:1710.03667 [physics, q-bio, stat]},
	author = {Advani, Madhu S. and Saxe, Andrew M.},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.03667},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{zhang_understanding_2017,
	title = {Understanding deep learning requires rethinking generalization},
	url = {http://arxiv.org/abs/1611.03530},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
	urldate = {2021-04-23},
	journal = {arXiv:1611.03530 [cs]},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	month = feb,
	year = {2017},
	note = {arXiv: 1611.03530},
	keywords = {Computer Science - Machine Learning},
}

@article{carlini_extracting_2020,
	title = {Extracting {Training} {Data} from {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2012.07805},
	abstract = {It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.},
	urldate = {2021-04-23},
	journal = {arXiv:2012.07805 [cs]},
	author = {Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and Oprea, Alina and Raffel, Colin},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.07805},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{strubell_energy_2019,
	title = {Energy and {Policy} {Considerations} for {Deep} {Learning} in {NLP}},
	url = {http://arxiv.org/abs/1906.02243},
	abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
	urldate = {2021-04-23},
	journal = {arXiv:1906.02243 [cs]},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.02243
version: 1},
	keywords = {Computer Science - Computation and Language},
}

@article{belkin_reconciling_2019,
	title = {Reconciling modern machine learning practice and the bias-variance trade-off},
	url = {http://arxiv.org/abs/1812.11118},
	abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This "double descent" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.},
	urldate = {2021-04-23},
	journal = {arXiv:1812.11118 [cs, stat]},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	month = sep,
	year = {2019},
	note = {arXiv: 1812.11118},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{goyal_accurate_2018,
	title = {Accurate, {Large} {Minibatch} {SGD}: {Training} {ImageNet} in 1 {Hour}},
	shorttitle = {Accurate, {Large} {Minibatch} {SGD}},
	url = {http://arxiv.org/abs/1706.02677},
	abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\textasciitilde}90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
	urldate = {2021-04-07},
	journal = {arXiv:1706.02677 [cs]},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	month = apr,
	year = {2018},
	note = {arXiv: 1706.02677},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@article{zhang_are_2019,
	title = {Are {All} {Layers} {Created} {Equal}?},
	url = {http://arxiv.org/abs/1902.01996},
	abstract = {Understanding deep neural networks has been a major research objective in recent years with notable theoretical progress. A focal point of those studies stems from the success of excessively large networks which defy the classical wisdom of uniform convergence and learnability. We study empirically the layer-wise functional structure of overparameterized deep models. We provide evidence for the heterogeneous characteristic of layers. To do so, we introduce the notion of robustness to post-training re-initialization and re-randomization. We show that the layers can be categorized as either ``ambient'' or ``critical''. Resetting the ambient layers to their initial values has no negative consequence, and in many cases they barely change throughout training. On the contrary, resetting the critical layers completely destroys the predictor and the performance drops to chanceh. Our study provides further evidence that mere parameter counting or norm accounting is too coarse in studying generalization of deep models, and flatness or robustness analysis of the models needs to respect the network architectures.},
	urldate = {2021-04-05},
	journal = {arXiv:1902.01996 [cs, stat]},
	author = {Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
	month = may,
	year = {2019},
	note = {arXiv: 1902.01996},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kuhn_robustness_2021,
	title = {Robustness to {Pruning} {Predicts} {Generalization} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2103.06002},
	abstract = {Existing generalization measures that aim to capture a model's simplicity based on parameter counts or norms fail to explain generalization in overparameterized deep neural networks. In this paper, we introduce a new, theoretically motivated measure of a network's simplicity which we call prunability: the smallest {\textbackslash}emph\{fraction\} of the network's parameters that can be kept while pruning without adversely affecting its training loss. We show that this measure is highly predictive of a model's generalization performance across a large set of convolutional networks trained on CIFAR-10, does not grow with network size unlike existing pruning-based measures, and exhibits high correlation with test set loss even in a particularly challenging double descent setting. Lastly, we show that the success of prunability cannot be explained by its relation to known complexity measures based on models' margin, flatness of minima and optimization speed, finding that our new measure is similar to -- but more predictive than -- existing flatness-based measures, and that its predictions exhibit low mutual information with those of other baselines.},
	urldate = {2021-04-03},
	journal = {arXiv:2103.06002 [cs, stat]},
	author = {Kuhn, Lorenz and Lyle, Clare and Gomez, Aidan N. and Rothfuss, Jonas and Gal, Yarin},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.06002
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{liebenwein_lost_2021,
	title = {Lost in {Pruning}: {The} {Effects} of {Pruning} {Neural} {Networks} beyond {Test} {Accuracy}},
	shorttitle = {Lost in {Pruning}},
	url = {http://arxiv.org/abs/2103.03014},
	abstract = {Neural network pruning is a popular technique used to reduce the inference costs of modern, potentially overparameterized, networks. Starting from a pre-trained network, the process is as follows: remove redundant parameters, retrain, and repeat while maintaining the same test accuracy. The result is a model that is a fraction of the size of the original with comparable predictive performance (test accuracy). Here, we reassess and evaluate whether the use of test accuracy alone in the terminating condition is sufficient to ensure that the resulting model performs well across a wide spectrum of "harder" metrics such as generalization to out-of-distribution data and resilience to noise. Across evaluations on varying architectures and data sets, we find that pruned networks effectively approximate the unpruned model, however, the prune ratio at which pruned networks achieve commensurate performance varies significantly across tasks. These results call into question the extent of {\textbackslash}emph\{genuine\} overparameterization in deep learning and raise concerns about the practicability of deploying pruned networks, specifically in the context of safety-critical systems, unless they are widely evaluated beyond test accuracy to reliably predict their performance. Our code is available at https://github.com/lucaslie/torchprune.},
	urldate = {2021-04-03},
	journal = {arXiv:2103.03014 [cs]},
	author = {Liebenwein, Lucas and Baykal, Cenk and Carter, Brandon and Gifford, David and Rus, Daniela},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.03014
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{lee_understanding_2020,
	title = {Understanding the {Effects} of {Data} {Parallelism} and {Sparsity} on {Neural} {Network} {Training}},
	url = {http://arxiv.org/abs/2003.11316},
	abstract = {Network pruning is an effective methodology to compress large neural networks, and sparse neural networks obtained by pruning can benefit from their reduced memory and computational costs at use. Notably, recent studies have found that it is possible to find a trainable sparse neural network even at random initialization prior to training. While this approach of pruning at initialization turned out to be highly effective, there has been little study concerning the subsequent training of these sparse neural networks. In this work, we focus on studying the effects of data parallelism and sparsity on neural network training. For data parallelism, this usually means processing training data in parallel using distributed systems, or equivalently increasing batch size, so that the training process can be accelerated. To this end, we first measure the effects for different study cases of batch size and sparsity level while tuning all metaparameters involved in the optimization. As a result, we find across various workloads of data set, network model, and optimization algorithms that there exists a general scaling trend in the relationship between batch size and number of training steps to convergence for the effect of data parallelism, irrespective of sparsity levels. Also, the effect of data parallelism in training sparse networks turns out to be no worse, or can be even better when the training is done by a momentum based optimizer, than that in training densely parameterized networks, despite the general difficulty of training sparse networks. We further provide theoretical insights based on the convergence properties of stochastic gradient methods and a smoothness analysis, so as to precisely illustrate our empirical findings and hence to develop a better account of the effects of data parallelism and sparsity on neural network training.},
	urldate = {2021-03-29},
	journal = {arXiv:2003.11316 [cs, stat]},
	author = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip H. S. and Jaggi, Martin},
	month = jun,
	year = {2020},
	note = {arXiv: 2003.11316},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wang_emerging_2021,
	title = {Emerging {Paradigms} of {Neural} {Network} {Pruning}},
	url = {http://arxiv.org/abs/2103.06460},
	abstract = {Over-parameterization of neural networks benefits the optimization and generalization yet brings cost in practice. Pruning is adopted as a post-processing solution to this problem, which aims to remove unnecessary parameters in a neural network with little performance compromised. It has been broadly believed the resulted sparse neural network cannot be trained from scratch to comparable accuracy. However, several recent works (e.g., [Frankle and Carbin, 2019a]) challenge this belief by discovering random sparse networks which can be trained to match the performance with their dense counterpart. This new pruning paradigm later inspires more new methods of pruning at initialization. In spite of the encouraging progress, how to coordinate these new pruning fashions with the traditional pruning has not been explored yet. This survey seeks to bridge the gap by proposing a general pruning framework so that the emerging pruning paradigms can be accommodated well with the traditional one. With it, we systematically reflect the major differences and new insights brought by these new pruning fashions, with representative works discussed at length. Finally, we summarize the open questions as worthy future directions.},
	urldate = {2021-03-26},
	journal = {arXiv:2103.06460 [cs]},
	author = {Wang, Huan and Qin, Can and Zhang, Yulun and Fu, Yun},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.06460
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{tessera_keep_2021,
	title = {Keep the {Gradients} {Flowing}: {Using} {Gradient} {Flow} to {Study} {Sparse} {Network} {Optimization}},
	shorttitle = {Keep the {Gradients} {Flowing}},
	url = {http://arxiv.org/abs/2102.01670},
	abstract = {Training sparse networks to converge to the same performance as dense neural architectures has proven to be elusive. Recent work suggests that initialization is the key. However, while this direction of research has had some success, focusing on initialization alone appears to be inadequate. In this paper, we take a broader view of training sparse networks and consider the role of regularization, optimization and architecture choices on sparse models. We propose a simple experimental framework, Same Capacity Sparse vs Dense Comparison (SC-SDC), that allows for fair comparison of sparse and dense networks. Furthermore, we propose a new measure of gradient flow, Effective Gradient Flow (EGF), that better correlates to performance in sparse networks. Using top-line metrics, SC-SDC and EGF, we show that default choices of optimizers, activation functions and regularizers used for dense networks can disadvantage sparse networks. Based upon these findings, we show that gradient flow in sparse networks can be improved by reconsidering aspects of the architecture design and the training regime. Our work suggests that initialization is only one piece of the puzzle and taking a wider view of tailoring optimization to sparse networks yields promising results.},
	urldate = {2021-03-22},
	journal = {arXiv:2102.01670 [cs]},
	author = {Tessera, Kale-ab and Hooker, Sara and Rosman, Benjamin},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.01670},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{evci_gradient_2020,
	title = {Gradient {Flow} in {Sparse} {Neural} {Networks} and {How} {Lottery} {Tickets} {Win}},
	url = {http://arxiv.org/abs/2010.03533},
	abstract = {Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and also have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exception of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). In this work, we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; (2) what makes LTs and DST the exceptions? We show that sparse NNs have poor gradient flow at initialization and propose a modified initialization for unstructured connectivity. Furthermore, we find that DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, we show that LTs do not improve gradient flow, rather their success lies in re-learning the pruning solution they are derived from - however, this comes at the cost of learning novel solutions.},
	urldate = {2021-03-21},
	journal = {arXiv:2010.03533 [cs]},
	author = {Evci, Utku and Ioannou, Yani A. and Keskin, Cem and Dauphin, Yann},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.03533},
	keywords = {68T07, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{saxe_exact_2014,
	title = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
	url = {http://arxiv.org/abs/1312.6120},
	abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
	urldate = {2021-03-04},
	journal = {arXiv:1312.6120 [cond-mat, q-bio, stat]},
	author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
	month = feb,
	year = {2014},
	note = {arXiv: 1312.6120},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{goldblum_truth_2020,
	title = {Truth or {Backpropaganda}? {An} {Empirical} {Investigation} of {Deep} {Learning} {Theory}},
	shorttitle = {Truth or {Backpropaganda}?},
	url = {http://arxiv.org/abs/1910.00359},
	abstract = {We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting.},
	urldate = {2021-02-21},
	journal = {arXiv:1910.00359 [cs, math, stat]},
	author = {Goldblum, Micah and Geiping, Jonas and Schwarzschild, Avi and Moeller, Michael and Goldstein, Tom},
	month = apr,
	year = {2020},
	note = {arXiv: 1910.00359},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{lu_dying_2020,
	title = {Dying {ReLU} and {Initialization}: {Theory} and {Numerical} {Examples}},
	volume = {28},
	issn = {1815-2406, 1991-7120},
	shorttitle = {Dying {ReLU} and {Initialization}},
	url = {http://arxiv.org/abs/1903.06733},
	doi = {10.4208/cicp.OA-2020-0165},
	abstract = {The dying ReLU refers to the problem when ReLU neurons become inactive and only output 0 for any input. There are many empirical and heuristic explanations of why ReLU neurons die. However, little is known about its theoretical analysis. In this paper, we rigorously prove that a deep ReLU network will eventually die in probability as the depth goes to infinite. Several methods have been proposed to alleviate the dying ReLU. Perhaps, one of the simplest treatments is to modify the initialization procedure. One common way of initializing weights and biases uses symmetric probability distributions, which suffers from the dying ReLU. We thus propose a new initialization procedure, namely, a randomized asymmetric initialization. We prove that the new initialization can effectively prevent the dying ReLU. All parameters required for the new initialization are theoretically designed. Numerical examples are provided to demonstrate the effectiveness of the new initialization procedure.},
	number = {5},
	urldate = {2021-02-16},
	journal = {Communications in Computational Physics},
	author = {Lu, Lu and Shin, Yeonjong and Su, Yanhui and Karniadakis, George Em},
	month = jun,
	year = {2020},
	note = {arXiv: 1903.06733},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
	pages = {1671--1706},
}

@inproceedings{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {https://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html},
	urldate = {2021-02-11},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	pages = {1026--1034},
}

@inproceedings{glorot_understanding_2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	url = {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental resul...},
	language = {en},
	urldate = {2021-02-11},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Glorot, Xavier and Bengio, Yoshua},
	month = mar,
	year = {2010},
	note = {ISSN: 1938-7228},
	pages = {249--256},
}

@article{dauphin_identifying_2014,
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	url = {http://arxiv.org/abs/1406.2572},
	abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
	urldate = {2021-02-09},
	journal = {arXiv:1406.2572 [cs, math, stat]},
	author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2572},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{choromanska_loss_2015,
	title = {The {Loss} {Surfaces} of {Multilayer} {Networks}},
	url = {http://arxiv.org/abs/1412.0233},
	abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
	urldate = {2021-02-07},
	journal = {arXiv:1412.0233 [cs]},
	author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gérard Ben and LeCun, Yann},
	month = jan,
	year = {2015},
	note = {arXiv: 1412.0233},
	keywords = {Computer Science - Machine Learning},
}

@article{fort_large_2019,
	title = {Large {Scale} {Structure} of {Neural} {Network} {Loss} {Landscapes}},
	url = {http://arxiv.org/abs/1906.04724},
	abstract = {There are many surprising and perhaps counter-intuitive properties of optimization of deep neural networks. We propose and experimentally verify a unified phenomenological model of the loss landscape that incorporates many of them. High dimensionality plays a key role in our model. Our core idea is to model the loss landscape as a set of high dimensional {\textbackslash}emph\{wedges\} that together form a large-scale, inter-connected structure and towards which optimization is drawn. We first show that hyperparameter choices such as learning rate, network width and \$L\_2\$ regularization, affect the path optimizer takes through the landscape in a similar ways, influencing the large scale curvature of the regions the optimizer explores. Finally, we predict and demonstrate new counter-intuitive properties of the loss-landscape. We show an existence of low loss subspaces connecting a set (not only a pair) of solutions, and verify it experimentally. Finally, we analyze recently popular ensembling techniques for deep networks in the light of our model.},
	urldate = {2020-12-28},
	journal = {arXiv:1906.04724 [cs, stat]},
	author = {Fort, Stanislav and Jastrzebski, Stanislaw},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04724},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{orseau_logarithmic_2020,
	title = {Logarithmic {Pruning} is {All} {You} {Need}},
	url = {http://arxiv.org/abs/2006.12156},
	abstract = {The Lottery Ticket Hypothesis is a conjecture that every large neural network contains a subnetwork that, when trained in isolation, achieves comparable performance to the large network. An even stronger conjecture has been proven recently: Every sufficiently overparameterized network contains a subnetwork that, even without training, achieves comparable accuracy to the trained large network. This theorem, however, relies on a number of strong assumptions and guarantees a polynomial factor on the size of the large network compared to the target function. In this work, we remove the most limiting assumptions of this previous work while providing significantly tighter bounds: the overparameterized network only needs a logarithmic factor (in all variables but depth) number of neurons per weight of the target subnetwork.},
	urldate = {2020-10-03},
	journal = {arXiv:2006.12156 [cs, stat]},
	author = {Orseau, Laurent and Hutter, Marcus and Rivasplata, Omar},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.12156},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{li_visualizing_2018,
	title = {Visualizing the {Loss} {Landscape} of {Neural} {Nets}},
	url = {http://arxiv.org/abs/1712.09913},
	abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
	urldate = {2021-02-07},
	journal = {arXiv:1712.09913 [cs, stat]},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	month = nov,
	year = {2018},
	note = {arXiv: 1712.09913},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{goodfellow_qualitatively_2015,
	title = {Qualitatively characterizing neural network optimization problems},
	url = {http://arxiv.org/abs/1412.6544},
	abstract = {Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.},
	urldate = {2021-02-07},
	journal = {arXiv:1412.6544 [cs, stat]},
	author = {Goodfellow, Ian J. and Vinyals, Oriol and Saxe, Andrew M.},
	month = may,
	year = {2015},
	note = {arXiv: 1412.6544},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{lin_understanding_2021,
	title = {Understanding the {Landscape} of {Sparse} {Networks}: {A} {Brief} {Investigation}},
	shorttitle = {Understanding the {Landscape} of {Sparse} {Networks}},
	url = {http://arxiv.org/abs/2009.07439},
	abstract = {Network pruning or network sparsification has a long history and practical significance in modern applications. The loss surface of dense neural networks would yield a bad landscape due to non-convexity and non-linear activations, but over-parameterization may lead to benign geometrical properties. In this paper, we study sparse networks with the squared loss objective, showing that like dense networks, sparse networks can still preserve benign landscape when the last hidden layer width is larger than the number of training data. Our results have been built on general linear sparse networks, linear CNNs (a special class of sparse networks), and nonlinear sparse networks. We also present counterexamples when certain assumptions are violated, which implies that these assumptions are necessary for our results.},
	urldate = {2021-02-03},
	journal = {arXiv:2009.07439 [cs, stat]},
	author = {Lin, Dachao and Sun, Ruoyu and Zhang, Zhihua},
	month = feb,
	year = {2021},
	note = {arXiv: 2009.07439},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{fort_goldilocks_2019,
	title = {The {Goldilocks} {Zone}: {Towards} {Better} {Understanding} of {Neural} {Network} {Loss} {Landscapes}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {The {Goldilocks} {Zone}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4237},
	doi = {10.1609/aaai.v33i01.33013574},
	language = {en},
	number = {01},
	urldate = {2021-02-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Fort, Stanislav and Scherlis, Adam},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {3574--3581},
}

@article{li_measuring_2018,
	title = {Measuring the {Intrinsic} {Dimension} of {Objective} {Landscapes}},
	url = {http://arxiv.org/abs/1804.08838},
	abstract = {Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.},
	urldate = {2021-02-02},
	journal = {arXiv:1804.08838 [cs, stat]},
	author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.08838},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{evci_difficulty_2020,
	title = {The {Difficulty} of {Training} {Sparse} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1906.10732},
	abstract = {We investigate the difficulties of training sparse neural networks and make new observations about optimization dynamics and the energy landscape within the sparse regime. Recent work of {\textbackslash}citep\{Gale2019, Liu2018\} has shown that sparse ResNet-50 architectures trained on ImageNet-2012 dataset converge to solutions that are significantly worse than those found by pruning. We show that, despite the failure of optimizers, there is a linear path with a monotonically decreasing objective from the initialization to the "good" solution. Additionally, our attempts to find a decreasing objective path from "bad" solutions to the "good" ones in the sparse subspace fail. However, if we allow the path to traverse the dense subspace, then we consistently find a path between two solutions. These findings suggest traversing extra dimensions may be needed to escape stationary points found in the sparse subspace.},
	urldate = {2021-02-02},
	journal = {arXiv:1906.10732 [cs, stat]},
	author = {Evci, Utku and Pedregosa, Fabian and Gomez, Aidan and Elsen, Erich},
	month = oct,
	year = {2020},
	note = {arXiv: 1906.10732},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{fedus_switch_2021,
	title = {Switch {Transformers}: {Scaling} to {Trillion} {Parameter} {Models} with {Simple} and {Efficient} {Sparsity}},
	shorttitle = {Switch {Transformers}},
	url = {http://arxiv.org/abs/2101.03961},
	abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
	urldate = {2021-01-12},
	journal = {arXiv:2101.03961 [cs]},
	author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.03961},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{van_gelder_deconstructing_2020,
	title = {Deconstructing the {Structure} of {Sparse} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2012.00172},
	abstract = {Although sparse neural networks have been studied extensively, the focus has been primarily on accuracy. In this work, we focus instead on network structure, and analyze three popular algorithms. We first measure performance when structure persists and weights are reset to a different random initialization, thereby extending experiments in Deconstructing Lottery Tickets (Zhou et al., 2019). This experiment reveals that accuracy can be derived from structure alone. Second, to measure structural robustness we investigate the sensitivity of sparse neural networks to further pruning after training, finding a stark contrast between algorithms. Finally, for a recent dynamic sparsity algorithm we investigate how early in training the structure emerges. We find that even after one epoch the structure is mostly determined, allowing us to propose a more efficient algorithm which does not require dense gradients throughout training. In looking back at algorithms for sparse neural networks and analyzing their performance from a different lens, we uncover several interesting properties and promising directions for future research.},
	urldate = {2021-01-22},
	journal = {arXiv:2012.00172 [cs]},
	author = {Van Gelder, Maxwell and Wortsman, Mitchell and Ehsani, Kiana},
	month = nov,
	year = {2020},
	note = {arXiv: 2012.00172
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, I.5.1},
}

@article{grosse_how_2020,
	title = {How many winning tickets are there in one {DNN}?},
	url = {http://arxiv.org/abs/2006.07014},
	abstract = {The recent lottery ticket hypothesis proposes that there is one sub-network that matches the accuracy of the original network when trained in isolation. We show that instead each network contains several winning tickets, even if the initial weights are fixed. The resulting winning sub-networks are not instances of the same network under weight space symmetry, and show no overlap or correlation significantly larger than expected by chance. If randomness during training is decreased, overlaps higher than chance occur, even if the networks are trained on different tasks. We conclude that there is rather a distribution over capable sub-networks, as opposed to a single winning ticket.},
	urldate = {2021-01-22},
	journal = {arXiv:2006.07014 [cs, stat]},
	author = {Grosse, Kathrin and Backes, Michael},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.07014
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gur-ari_gradient_2018,
	title = {Gradient {Descent} {Happens} in a {Tiny} {Subspace}},
	url = {http://arxiv.org/abs/1812.04754},
	abstract = {We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.},
	urldate = {2021-01-04},
	journal = {arXiv:1812.04754 [cs, stat]},
	author = {Gur-Ari, Guy and Roberts, Daniel A. and Dyer, Ethan},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.04754},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bartoldson_generalization-stability_2020,
	title = {The {Generalization}-{Stability} {Tradeoff} {In} {Neural} {Network} {Pruning}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/ef2ee09ea9551de88bc11fd7eeea93b0-Abstract.html},
	language = {en},
	urldate = {2020-12-30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Bartoldson, Brian and Morcos, Ari and Barbu, Adrian and Erlebacher, Gordon},
	year = {2020},
}

@article{ye_greedy_2020,
	title = {Greedy {Optimization} {Provably} {Wins} the {Lottery}: {Logarithmic} {Number} of {Winning} {Tickets} is {Enough}},
	shorttitle = {Greedy {Optimization} {Provably} {Wins} the {Lottery}},
	url = {http://arxiv.org/abs/2010.15969},
	abstract = {Despite the great success of deep learning, recent works show that large deep neural networks are often highly redundant and can be significantly reduced in size. However, the theoretical question of how much we can prune a neural network given a specified tolerance of accuracy drop is still open. This paper provides one answer to this question by proposing a greedy optimization based pruning method. The proposed method has the guarantee that the discrepancy between the pruned network and the original network decays with exponentially fast rate w.r.t. the size of the pruned network, under weak assumptions that apply for most practical settings. Empirically, our method improves prior arts on pruning various network architectures including ResNet, MobilenetV2/V3 on ImageNet.},
	urldate = {2020-12-30},
	journal = {arXiv:2010.15969 [cs, math, stat]},
	author = {Ye, Mao and Wu, Lemeng and Liu, Qiang},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.15969},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{johnson_deep_2018,
	title = {Deep, {Skinny} {Neural} {Networks} are not {Universal} {Approximators}},
	url = {http://arxiv.org/abs/1810.00393},
	abstract = {In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.},
	urldate = {2020-12-30},
	journal = {arXiv:1810.00393 [cs, stat]},
	author = {Johnson, Jesse},
	month = sep,
	year = {2018},
	note = {arXiv: 1810.00393},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{carreira-perpinan_learning-compression_2018,
	title = {“{Learning}-{Compression}” {Algorithms} for {Neural} {Net} {Pruning}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Carreira-Perpinan_Learning-Compression_Algorithms_for_CVPR_2018_paper.html},
	urldate = {2020-12-29},
	author = {Carreira-Perpiñán, Miguel Á and Idelbayev, Yerlan},
	year = {2018},
	pages = {8532--8541},
}

@article{nagarajan_uniform_2019,
	title = {Uniform convergence may be unable to explain generalization in deep learning},
	url = {http://arxiv.org/abs/1902.04742},
	abstract = {Aimed at explaining the surprisingly good generalization behavior of overparameterized deep networks, recent works have developed a variety of generalization bounds for deep learning, all based on the fundamental learning-theoretic technique of uniform convergence. While it is well-known that many of these existing bounds are numerically large, through numerous experiments, we bring to light a more concerning aspect of these bounds: in practice, these bounds can \{{\textbackslash}em increase\} with the training dataset size. Guided by our observations, we then present examples of overparameterized linear classifiers and neural networks trained by gradient descent (GD) where uniform convergence provably cannot ``explain generalization'' -- even if we take into account the implicit bias of GD \{{\textbackslash}em to the fullest extent possible\}. More precisely, even if we consider only the set of classifiers output by GD, which have test errors less than some small \${\textbackslash}epsilon\$ in our settings, we show that applying (two-sided) uniform convergence on this set of classifiers will yield only a vacuous generalization guarantee larger than \$1-{\textbackslash}epsilon\$. Through these findings, we cast doubt on the power of uniform convergence-based generalization bounds to provide a complete picture of why overparameterized deep networks generalize well.},
	urldate = {2020-12-29},
	journal = {arXiv:1902.04742 [cs, stat]},
	author = {Nagarajan, Vaishnavh and Kolter, J. Zico},
	month = dec,
	year = {2019},
	note = {arXiv: 1902.04742},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{jastrzebski_break-even_2020,
	title = {The {Break}-{Even} {Point} on {Optimization} {Trajectories} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2002.09572},
	abstract = {The early phase of training of deep neural networks is critical for their final performance. In this work, we study how the hyperparameters of stochastic gradient descent (SGD) used in the early phase of training affect the rest of the optimization trajectory. We argue for the existence of the "break-even" point on this trajectory, beyond which the curvature of the loss surface and noise in the gradient are implicitly regularized by SGD. In particular, we demonstrate on multiple classification tasks that using a large learning rate in the initial phase of training reduces the variance of the gradient, and improves the conditioning of the covariance of gradients. These effects are beneficial from the optimization perspective and become visible after the break-even point. Complementing prior work, we also show that using a low learning rate results in bad conditioning of the loss surface even for a neural network with batch normalization layers. In short, our work shows that key properties of the loss surface are strongly influenced by SGD in the early phase of training. We argue that studying the impact of the identified effects on generalization is a promising future direction.},
	urldate = {2020-12-28},
	journal = {arXiv:2002.09572 [cs, stat]},
	author = {Jastrzebski, Stanislaw and Szymczak, Maciej and Fort, Stanislav and Arpit, Devansh and Tabor, Jacek and Cho, Kyunghyun and Geras, Krzysztof},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.09572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{azarian_learned_2020,
	title = {Learned {Threshold} {Pruning}},
	url = {http://arxiv.org/abs/2003.00075},
	abstract = {This paper presents a novel differentiable method for unstructured weight pruning of deep neural networks. Our learned-threshold pruning (LTP) method enjoys a number of important advantages. First, it learns per-layer thresholds via gradient descent, unlike conventional methods where they are set as input. Making thresholds trainable also makes LTP computationally efficient, hence scalable to deeper networks. For example, it takes less than \$30\$ epochs for LTP to prune most networks on ImageNet. This is in contrast to other methods that search for per-layer thresholds via a computationally intensive iterative pruning and fine-tuning process. Additionally, with a novel differentiable \$L\_0\$ regularization, LTP is able to operate effectively on architectures with batch-normalization. This is important since \$L\_1\$ and \$L\_2\$ penalties lose their regularizing effect in networks with batch-normalization. Finally, LTP generates a trail of progressively sparser networks from which the desired pruned network can be picked based on sparsity and performance requirements. These features allow LTP to achieve state-of-the-art compression rates on ImageNet networks such as AlexNet (\$26.4{\textbackslash}times\$ compression with \$79.1{\textbackslash}\%\$ Top-5 accuracy) and ResNet50 (\$9.1{\textbackslash}times\$ compression with \$92.0{\textbackslash}\%\$ Top-5 accuracy). We also show that LTP effectively prunes newer architectures, such as EfficientNet, MobileNetV2 and MixNet.},
	urldate = {2020-12-25},
	journal = {arXiv:2003.00075 [cs, stat]},
	author = {Azarian, Kambiz and Bhalgat, Yash and Lee, Jinwon and Blankevoort, Tijmen},
	month = feb,
	year = {2020},
	note = {arXiv: 2003.00075},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{dettmers_sparse_2019,
	title = {Sparse {Networks} from {Scratch}: {Faster} {Training} without {Losing} {Performance}},
	shorttitle = {Sparse {Networks} from {Scratch}},
	url = {http://arxiv.org/abs/1907.04840},
	abstract = {We demonstrate the possibility of what we call sparse learning: accelerated training of deep neural networks that maintain sparse weights throughout training while achieving dense performance levels. We accomplish this by developing sparse momentum, an algorithm which uses exponentially smoothed gradients (momentum) to identify layers and weights which reduce the error efficiently. Sparse momentum redistributes pruned weights across layers according to the mean momentum magnitude of each layer. Within a layer, sparse momentum grows weights according to the momentum magnitude of zero-valued weights. We demonstrate state-of-the-art sparse performance on MNIST, CIFAR-10, and ImageNet, decreasing the mean error by a relative 8\%, 15\%, and 6\% compared to other sparse algorithms. Furthermore, we show that sparse momentum reliably reproduces dense performance levels while providing up to 5.61x faster training. In our analysis, ablations show that the benefits of momentum redistribution and growth increase with the depth and size of the network. Additionally, we find that sparse momentum is insensitive to the choice of its hyperparameters suggesting that sparse momentum is robust and easy to use.},
	urldate = {2020-12-25},
	journal = {arXiv:1907.04840 [cs, stat]},
	author = {Dettmers, Tim and Zettlemoyer, Luke},
	month = aug,
	year = {2019},
	note = {arXiv: 1907.04840},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{fort_deep_2020,
	title = {Deep {Ensembles}: {A} {Loss} {Landscape} {Perspective}},
	shorttitle = {Deep {Ensembles}},
	url = {http://arxiv.org/abs/1912.02757},
	abstract = {Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.},
	urldate = {2020-12-24},
	journal = {arXiv:1912.02757 [cs, stat]},
	author = {Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
	month = jun,
	year = {2020},
	note = {arXiv: 1912.02757},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{greydanus_scaling_2020,
	title = {Scaling down {Deep} {Learning}},
	url = {http://arxiv.org/abs/2011.14439},
	abstract = {Though deep learning models have taken on commercial and political relevance, many aspects of their training and operation remain poorly understood. This has sparked interest in "science of deep learning" projects, many of which are run at scale and require enormous amounts of time, money, and electricity. But how much of this research really needs to occur at scale? In this paper, we introduce MNIST-1D: a minimalist, low-memory, and low-compute alternative to classic deep learning benchmarks. The training examples are 20 times smaller than MNIST examples yet they differentiate more clearly between linear, nonlinear, and convolutional models which attain 32, 68, and 94\% accuracy respectively (these models obtain 94, 99+, and 99+\% on MNIST). Then we present example use cases which include measuring the spatial inductive biases of lottery tickets, observing deep double descent, and metalearning an activation function.},
	urldate = {2020-12-24},
	journal = {arXiv:2011.14439 [cs, stat]},
	author = {Greydanus, Sam},
	month = dec,
	year = {2020},
	note = {arXiv: 2011.14439
version: 3},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{huang_data-driven_2018,
	title = {Data-{Driven} {Sparse} {Structure} {Selection} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1707.01213},
	abstract = {Deep convolutional neural networks have liberated its extraordinary power on various tasks. However, it is still very challenging to deploy state-of-the-art models into real-world applications due to their high computational complexity. How can we design a compact and effective network without massive experiments and expert knowledge? In this paper, we propose a simple and effective framework to learn and prune deep models in an end-to-end manner. In our framework, a new type of parameter -- scaling factor is first introduced to scale the outputs of specific structures, such as neurons, groups or residual blocks. Then we add sparsity regularizations on these factors, and solve this optimization problem by a modified stochastic Accelerated Proximal Gradient (APG) method. By forcing some of the factors to zero, we can safely remove the corresponding structures, thus prune the unimportant parts of a CNN. Comparing with other structure selection methods that may need thousands of trials or iterative fine-tuning, our method is trained fully end-to-end in one training pass without bells and whistles. We evaluate our method, Sparse Structure Selection with several state-of-the-art CNNs, and demonstrate very promising results with adaptive depth and width selection.},
	urldate = {2020-12-24},
	journal = {arXiv:1707.01213 [cs]},
	author = {Huang, Zehao and Wang, Naiyan},
	month = sep,
	year = {2018},
	note = {arXiv: 1707.01213
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{paganini_iterative_2020,
	title = {On {Iterative} {Neural} {Network} {Pruning}, {Reinitialization}, and the {Similarity} of {Masks}},
	url = {http://arxiv.org/abs/2001.05050},
	abstract = {We examine how recently documented, fundamental phenomena in deep learning models subject to pruning are affected by changes in the pruning procedure. Specifically, we analyze differences in the connectivity structure and learning dynamics of pruned models found through a set of common iterative pruning techniques, to address questions of uniqueness of trainable, high-sparsity sub-networks, and their dependence on the chosen pruning method. In convolutional layers, we document the emergence of structure induced by magnitude-based unstructured pruning in conjunction with weight rewinding that resembles the effects of structured pruning. We also show empirical evidence that weight stability can be automatically achieved through apposite pruning techniques.},
	urldate = {2020-12-24},
	journal = {arXiv:2001.05050 [cs, stat]},
	author = {Paganini, Michela and Forde, Jessica},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.05050
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mehta_sparse_2019,
	title = {Sparse {Transfer} {Learning} via {Winning} {Lottery} {Tickets}},
	url = {http://arxiv.org/abs/1905.07785},
	abstract = {The recently proposed Lottery Ticket Hypothesis of Frankle and Carbin (2019) suggests that the performance of over-parameterized deep networks is due to the random initialization seeding the network with a small fraction of favorable weights. These weights retain their dominant status throughout training -- in a very real sense, this sub-network "won the lottery" during initialization. The authors find sub-networks via unstructured magnitude pruning with 85-95\% of parameters removed that train to the same accuracy as the original network at a similar speed, which they call winning tickets. In this paper, we extend the Lottery Ticket Hypothesis to a variety of transfer learning tasks. We show that sparse sub-networks with approximately 90-95\% of weights removed achieve (and often exceed) the accuracy of the original dense network in several realistic settings. We experimentally validate this by transferring the sparse representation found via pruning on CIFAR-10 to SmallNORB and FashionMNIST for object recognition tasks.},
	urldate = {2020-12-24},
	journal = {arXiv:1905.07785 [cs, stat]},
	author = {Mehta, Rahul},
	month = dec,
	year = {2019},
	note = {arXiv: 1905.07785
version: 2},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{levai_data-dependent_2020,
	title = {Data-dependent {Pruning} to find the {Winning} {Lottery} {Ticket}},
	url = {http://arxiv.org/abs/2006.14350},
	abstract = {The Lottery Ticket Hypothesis postulates that a freshly initialized neural network contains a small subnetwork that can be trained in isolation to achieve similar performance as the full network. Our paper examines several alternatives to search for such subnetworks. We conclude that incorporating a data dependent component into the pruning criterion in the form of the gradient of the training loss -- as done in the SNIP method -- consistently improves the performance of existing pruning algorithms.},
	urldate = {2020-12-24},
	journal = {arXiv:2006.14350 [cs, stat]},
	author = {Lévai, Dániel and Zombori, Zsolt},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.14350
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wang_curious_2020,
	title = {The curious case of developmental {BERTology}: {On} sparsity, transfer learning, generalization and the brain},
	shorttitle = {The curious case of developmental {BERTology}},
	url = {http://arxiv.org/abs/2007.03774},
	abstract = {In this essay, we explore a point of intersection between deep learning and neuroscience, through the lens of large language models, transfer learning and network compression. Just like perceptual and cognitive neurophysiology has inspired effective deep neural network architectures which in turn make a useful model for understanding the brain, here we explore how biological neural development might inspire efficient and robust optimization procedures which in turn serve as a useful model for the maturation and aging of the brain.},
	urldate = {2020-12-24},
	journal = {arXiv:2007.03774 [cs, q-bio, stat]},
	author = {Wang, Xin},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.03774},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{wang_pruning_2019,
	title = {Pruning from {Scratch}},
	url = {http://arxiv.org/abs/1909.12579},
	abstract = {Network pruning is an important research field aiming at reducing computational costs of neural networks. Conventional approaches follow a fixed paradigm which first trains a large and redundant network, and then determines which units (e.g., channels) are less important and thus can be removed. In this work, we find that pre-training an over-parameterized model is not necessary for obtaining the target pruned structure. In fact, a fully-trained over-parameterized model will reduce the search space for the pruned structure. We empirically show that more diverse pruned structures can be directly pruned from randomly initialized weights, including potential models with better performance. Therefore, we propose a novel network pruning pipeline which allows pruning from scratch. In the experiments for compressing classification models on CIFAR10 and ImageNet datasets, our approach not only greatly reduces the pre-training burden of traditional pruning methods, but also achieves similar or even higher accuracy under the same computation budgets. Our results facilitate the community to rethink the effectiveness of existing techniques used for network pruning.},
	urldate = {2020-12-23},
	journal = {arXiv:1909.12579 [cs]},
	author = {Wang, Yulong and Zhang, Xiaolu and Xie, Lingxi and Zhou, Jun and Su, Hang and Zhang, Bo and Hu, Xiaolin},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.12579},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ye_good_2020,
	title = {Good {Subnetworks} {Provably} {Exist}: {Pruning} via {Greedy} {Forward} {Selection}},
	shorttitle = {Good {Subnetworks} {Provably} {Exist}},
	url = {http://arxiv.org/abs/2003.01794},
	abstract = {Recent empirical works show that large deep neural networks are often highly redundant and one can find much smaller subnetworks without a significant drop of accuracy. However, most existing methods of network pruning are empirical and heuristic, leaving it open whether good subnetworks provably exist, how to find them efficiently, and if network pruning can be provably better than direct training using gradient descent. We answer these problems positively by proposing a simple greedy selection approach for finding good subnetworks, which starts from an empty network and greedily adds important neurons from the large network. This differs from the existing methods based on backward elimination, which remove redundant neurons from the large network. Theoretically, applying the greedy selection strategy on sufficiently large \{pre-trained\} networks guarantees to find small subnetworks with lower loss than networks directly trained with gradient descent. Our results also apply to pruning randomly weighted networks. Practically, we improve prior arts of network pruning on learning compact neural architectures on ImageNet, including ResNet, MobilenetV2/V3, and ProxylessNet. Our theory and empirical results on MobileNet suggest that we should fine-tune the pruned subnetworks to leverage the information from the large model, instead of re-training from new random initialization as suggested in {\textbackslash}citet\{liu2018rethinking\}.},
	urldate = {2020-12-23},
	journal = {arXiv:2003.01794 [cs, stat]},
	author = {Ye, Mao and Gong, Chengyue and Nie, Lizhen and Zhou, Denny and Klivans, Adam and Liu, Qiang},
	month = oct,
	year = {2020},
	note = {arXiv: 2003.01794},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gale_state_2019,
	title = {The {State} of {Sparsity} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1902.09574},
	abstract = {We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle \& Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification.},
	urldate = {2020-12-21},
	journal = {arXiv:1902.09574 [cs, stat]},
	author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.09574},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{neyshabur_towards_2018,
	title = {Towards {Understanding} the {Role} of {Over}-{Parametrization} in {Generalization} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1805.12076},
	abstract = {Despite existing work on ensuring generalization of neural networks in terms of scale sensitive complexity measures, such as norms, margin and sharpness, these complexity measures do not offer an explanation of why neural networks generalize better with over-parametrization. In this work we suggest a novel complexity measure based on unit-wise capacities resulting in a tighter generalization bound for two layer ReLU networks. Our capacity bound correlates with the behavior of test error with increasing network sizes, and could potentially explain the improvement in generalization with over-parametrization. We further present a matching lower bound for the Rademacher complexity that improves over previous capacity lower bounds for neural networks.},
	urldate = {2020-12-09},
	journal = {arXiv:1805.12076 [cs, stat]},
	author = {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
	month = may,
	year = {2018},
	note = {arXiv: 1805.12076},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{malach_proving_2020,
	title = {Proving the {Lottery} {Ticket} {Hypothesis}: {Pruning} is {All} {You} {Need}},
	shorttitle = {Proving the {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/2002.00585},
	abstract = {The lottery ticket hypothesis (Frankle and Carbin, 2018), states that a randomly-initialized network contains a small subnetwork such that, when trained in isolation, can compete with the performance of the original network. We prove an even stronger hypothesis (as was also conjectured in Ramanujan et al., 2019), showing that for every bounded distribution and every target network with bounded weights, a sufficiently over-parameterized neural network with random weights contains a subnetwork with roughly the same accuracy as the target network, without any further training.},
	urldate = {2020-10-01},
	journal = {arXiv:2002.00585 [cs, stat]},
	author = {Malach, Eran and Yehudai, Gilad and Shalev-Shwartz, Shai and Shamir, Ohad},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.00585},
	keywords = {Computer Science - Machine Learning, LTH, Statistics - Machine Learning},
}

@inproceedings{le_cun_optimal_1989,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'89},
	title = {Optimal brain damage},
	abstract = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
	urldate = {2020-12-09},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Le Cun, Yann and Denker, John S. and Solla, Sara A.},
	month = jan,
	year = {1989},
	pages = {598--605},
}

@article{elesedy_lottery_2020,
	title = {Lottery {Tickets} in {Linear} {Models}: {An} {Analysis} of {Iterative} {Magnitude} {Pruning}},
	shorttitle = {Lottery {Tickets} in {Linear} {Models}},
	url = {http://arxiv.org/abs/2007.08243},
	abstract = {We analyse the pruning procedure behind the lottery ticket hypothesis arXiv:1803.03635v5, iterative magnitude pruning (IMP), when applied to linear models trained by gradient flow. We begin by presenting sufficient conditions on the statistical structure of the features, under which IMP prunes those features that have smallest projection onto the data. Following this, we explore IMP as a method for sparse estimation and sparse prediction in noisy settings, with minimal assumptions on the design matrix. The same techniques are then applied to derive corresponding results for threshold pruning. Finally, we present experimental evidence of the regularising effect of IMP. We hope that our work will contribute to a theoretically grounded understanding of lottery tickets and how they emerge from IMP.},
	urldate = {2020-12-08},
	journal = {arXiv:2007.08243 [cs, stat]},
	author = {Elesedy, Bryn and Kanade, Varun and Teh, Yee Whye},
	month = aug,
	year = {2020},
	note = {arXiv: 2007.08243},
	keywords = {Computer Science - Machine Learning, I.5.1, Statistics - Machine Learning},
}

@article{pensia_optimal_2020,
	title = {Optimal {Lottery} {Tickets} via {Subset} {Sum}: {Logarithmic} {Over}-{Parameterization} is {Sufficient}},
	volume = {33},
	shorttitle = {Optimal {Lottery} {Tickets} via {Subset} {Sum}},
	url = {https://proceedings.neurips.cc//paper/2020/hash/1b742ae215adf18b75449c6e272fd92d-Abstract.html},
	language = {en},
	urldate = {2020-12-02},
	journal = {Advances in Neural Information Processing Systems},
	author = {Pensia, Ankit and Rajput, Shashank and Nagle, Alliot and Vishwakarma, Harit and Papailiopoulos, Dimitris},
	year = {2020},
}

@inproceedings{ramanujan_whats_2020,
	title = {What's {Hidden} in a {Randomly} {Weighted} {Neural} {Network}?},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Ramanujan_Whats_Hidden_in_a_Randomly_Weighted_Neural_Network_CVPR_2020_paper.html},
	urldate = {2020-10-02},
	author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
	year = {2020},
	pages = {11893--11902},
}

@article{frankle_stabilizing_2020,
	title = {Stabilizing the {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1903.01611},
	abstract = {Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar benefits during training. In particular, the "lottery ticket hypothesis" conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably finds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations. In this paper, we argue that these efforts have struggled on deeper networks because they have focused on pruning precisely at initialization. We modify IMP to search for subnetworks that could have been obtained by pruning early in training (0.1\% to 7\% through) rather than at iteration 0. With this change, it finds small subnetworks of deeper networks (e.g., 80\% sparsity on Resnet-50) that can complete the training process to match the accuracy of the original network on more challenging tasks (e.g., ImageNet). In situations where IMP fails at iteration 0, the accuracy benefits of delaying pruning accrue rapidly over the earliest iterations of training. To explain these behaviors, we study subnetwork "stability," finding that - as accuracy improves in this fashion - IMP subnetworks train to parameters closer to those of the full network and do so with improved consistency in the face of gradient noise. These results offer new insights into the opportunity to prune large-scale networks early in training and the behaviors underlying the lottery ticket hypothesis},
	urldate = {2020-09-30},
	journal = {arXiv:1903.01611 [cs, stat]},
	author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
	month = jul,
	year = {2020},
	note = {arXiv: 1903.01611},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2020-09-12},
	journal = {arXiv:2001.08361 [cs, stat]},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.08361},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yeom_pruning_2020,
	title = {Pruning by {Explaining}: {A} {Novel} {Criterion} for {Deep} {Neural} {Network} {Pruning}},
	shorttitle = {Pruning by {Explaining}},
	url = {http://arxiv.org/abs/1912.08881},
	abstract = {The success of convolutional neural networks (CNNs) in various applications is accompanied by a significant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant elements, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-of-the-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource-constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning.},
	urldate = {2020-09-30},
	journal = {arXiv:1912.08881 [cs, stat]},
	author = {Yeom, Seul-Ki and Seegerer, Philipp and Lapuschkin, Sebastian and Binder, Alexander and Wiedemann, Simon and Müller, Klaus-Robert and Samek, Wojciech},
	month = sep,
	year = {2020},
	note = {arXiv: 1912.08881},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{jiao_tinybert_2019,
	title = {{TinyBERT}: {Distilling} {BERT} for {Natural} {Language} {Understanding}},
	shorttitle = {{TinyBERT}},
	url = {http://arxiv.org/abs/1909.10351},
	abstract = {Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on some resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel transformer distillation method that is a specially designed knowledge distillation (KD) method for transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be well transferred to a small student TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT.TinyBERT is empirically effective and achieves more than 96\% the performance of teacher BERTBASE on GLUE benchmark while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only about 28\% parameters and about 31\% inference time of them.},
	urldate = {2020-09-25},
	journal = {arXiv:1909.10351 [cs]},
	author = {Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
	month = dec,
	year = {2019},
	note = {arXiv: 1909.10351},
	keywords = {BERT, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{rae_compressive_2019,
	title = {Compressive {Transformers} for {Long}-{Range} {Sequence} {Modelling}},
	url = {http://arxiv.org/abs/1911.05507},
	abstract = {We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.},
	urldate = {2020-09-12},
	journal = {arXiv:1911.05507 [cs, stat]},
	author = {Rae, Jack W. and Potapenko, Anna and Jayakumar, Siddhant M. and Lillicrap, Timothy P.},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.05507},
	keywords = {BERT, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{michel_are_2019,
	title = {Are {Sixteen} {Heads} {Really} {Better} than {One}?},
	url = {http://arxiv.org/abs/1905.10650},
	abstract = {Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention "head" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.},
	urldate = {2020-09-12},
	journal = {arXiv:1905.10650 [cs]},
	author = {Michel, Paul and Levy, Omer and Neubig, Graham},
	month = nov,
	year = {2019},
	note = {arXiv: 1905.10650},
	keywords = {BERT, Computer Science - Computation and Language},
}

@article{ganesh_compressing_2020,
	title = {Compressing {Large}-{Scale} {Transformer}-{Based} {Models}: {A} {Case} {Study} on {BERT}},
	shorttitle = {Compressing {Large}-{Scale} {Transformer}-{Based} {Models}},
	url = {http://arxiv.org/abs/2002.11985},
	abstract = {Transformer-based models pre-trained on large-scale corpora achieve state-of-the-art accuracy for natural language processing tasks, but are too resource-hungry and compute-intensive to suit low-capability devices or applications with strict latency requirements. One potential remedy is model compression, which has attracted extensive attention. This paper summarizes the branches of research on compressing Transformers, focusing on the especially popular BERT model. BERT's complex architecture means that a compression technique that is highly effective on one part of the model, e.g., attention layers, may be less successful on another part, e.g., fully connected layers. In this systematic study, we identify the state of the art in compression for each part of BERT, clarify current best practices for compressing large-scale Transformer models, and provide insights into the inner workings of various methods. Our categorization and analysis also shed light on promising future research directions for achieving a lightweight, accurate, and generic natural language processing model.},
	urldate = {2020-09-24},
	journal = {arXiv:2002.11985 [cs, stat]},
	author = {Ganesh, Prakhar and Chen, Yao and Lou, Xin and Khan, Mohammad Ali and Yang, Yin and Chen, Deming and Winslett, Marianne and Sajjad, Hassan and Nakov, Preslav},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.11985},
	keywords = {BERT, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{liu_learning_2017,
	title = {Learning {Efficient} {Convolutional} {Networks} {Through} {Network} {Slimming}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.html},
	urldate = {2020-09-23},
	author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
	year = {2017},
	pages = {2736--2744},
}

@article{han_deep_2016,
	title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Networks} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	shorttitle = {Deep {Compression}},
	url = {http://arxiv.org/abs/1510.00149},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
	urldate = {2020-09-12},
	journal = {arXiv:1510.00149 [cs]},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	month = feb,
	year = {2016},
	note = {arXiv: 1510.00149},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
}

@article{gong_compressing_2014,
	title = {Compressing {Deep} {Convolutional} {Networks} using {Vector} {Quantization}},
	url = {http://arxiv.org/abs/1412.6115},
	abstract = {Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1\% loss of classification accuracy using the state-of-the-art CNN.},
	urldate = {2020-09-12},
	journal = {arXiv:1412.6115 [cs]},
	author = {Gong, Yunchao and Liu, Liu and Yang, Ming and Bourdev, Lubomir},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6115},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2020-09-12},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}
