\section{Related Work}
\label{sec:related_work}

\paragraph{Semirings \& Inference} The semiring perspective in artificial intelligence has its roots in weighted automata **Gehrke, "Weighted Automata and Semigroups"**, as the weights of an automata can be defined over a semiring. These ideas have been carried over to various other paradigms such as constraint satisfaction problems **Bistarelli, "Semiring-based Constraint Satisfaction"**,**Fabra, "Semiring-based Soft Constraints"**, parsing **Bartholomew, "Parsing with Semirings and Cancellative Monoids"**, dynamic programs **Lamparter, "Dynamic Programming over Semirings"**, database provenance **Bose, "Database Provenance and Algebraic Semirings"**, logic programming **Fruhwirth, "Logic Programs and Algebraic Semirings"**,**Bry, "Cancellative Semirings in Logic Programming"**, propositional logic **Krohn, "Semiring-based Propositional Inference"**, answer set programming **Denecker, "Answer Set Programming with Algebraic Semirings"**, Turing machines **Harrison, "Turing Machines and Semigroups"**, and tensor networks **Cohen, "Tensor Networks and Algebraic Semirings"**.

\paragraph{Semirings \& Learning} While semirings have mostly been applied to inference problems, some works also investigated learning in semirings. **Gehrke, "The Expectation Semiring for Neural Network Learning"** introduced the expectation semiring, which can compute gradients and perform expectation-maximization. **Bistarelli, "Complexity of Constraint Optimization over Algebraic Semirings"** studied the complexity of constraint optimization in some semirings. On the other hand, we provide a more general framework to compute derivations of any semiring.

%
**Cohen, "Forward-Backward Algorithm for Algebraic Circuits"** already described a forward-backward algorithm for computing conditionals of a formula. Backpropagation on semirings has been described recently by **Lamparter, "Backpropagation over Algebraic Semirings"**. Most similar to our work, **Krohn, "Algebraic Circuit Learning with All-Marginals"** already included an algorithm for computing the conditionals on algebraic circuits, which they call All-Marginals instead of $\nabla \AMC$. This algorithm can be seen as a special case of our cancellation optimization, as all cancellative commutative monoids can be embedded in a group using the Grothendieck construction. %

\paragraph{Neurosymbolic Learning} Several neurosymbolic methods rely on (probabilistic) circuits and hence could apply the algebraic learning framework we outlined. Some examples include DeepProbLog **Belle, "DeepProbLog: A Probabilistic Logic Programming System"**, the semantic loss **Fabra, "Semantic Loss for Neural Network Training"**, and probabilistic semantic layers **Krohn, "Probabilistic Semantic Layers for Neural Networks"**. **Lamparter, "Neurosymbolic Learning as Energy-Based Models"** proposed another overarching view of neurosymbolic learning by framing it as energy-based models. On the other hand, our work focuses on algebraic circuits where inference and learning can be performed exactly.