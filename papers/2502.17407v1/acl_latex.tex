% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.
\RequirePackage[hyphens]{url}
\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}
\usepackage{lipsum}  


\usepackage{import}
\usepackage{multirow}
\usepackage{placeins}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{pbox}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
% \usepackage[table,xcdraw]{xcolor}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{tabulary}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{listings}
\usepackage[most]{tcolorbox}
\usepackage{colortbl}
\usepackage{cuted}
\usepackage{xcolor}
\usepackage{kotex}
\usepackage{pifont}
\usepackage{dirtytalk}
\usepackage{algorithm}
\usepackage{algpseudocode}
% \usepackage[subtle]{savetrees}
% \usepackage{amsfonts}
% \usepackage{authblk}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[perpage]{footmisc}
\usepackage{hyperref}
\usepackage{enumitem}


\usepackage{ulem}
\usepackage{makecell}

\usepackage{amsmath, amssymb}
\usepackage[most]{tcolorbox}
\usepackage{siunitx}
\usepackage{graphicx, xcolor, colortbl, booktabs}
\usepackage{array, tabularx}
% \usepackage[margin=0.5in]{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\urlstyle{rm}
% \setcounter{biburlnumpenalty}{9000}

\definecolor{green}{RGB}{144, 238, 144}
\definecolor{red}{RGB}{255, 102, 102}

\newcommand{\up}[1]{\textcolor{green}{\textbf{#1} ▲}}
\newcommand{\down}[1]{\textcolor{red}{\textbf{#1} ▼}}

% \tcbuselibrary{listings, breakable}

\newtcolorbox{instructionsbox}[1][]{
  colframe=cyan!75!black,    % Frame color
  colback=green!5!white,     % Background color
  coltitle=black,            % Color of the title text
  title=#1,                  % Optional title
  rounded corners,           % Corner style
  boxrule=0.5mm,             % Frame thickness
  boxsep=5pt,                % Space between content and box
  toptitle=1mm,              % Space above the title
  bottomtitle=1mm,           % Space below the title
  left=0pt,                 % Left padding
  right=0pt,                % Right padding
  top=0pt,                   % Top padding
  bottom=0pt,                % Bottom padding
  fonttitle=\bfseries        % Font style for the title
}


\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true,
breakindent=0pt,
breakautoindent=false,
escapechar=^,
}

% \makeatletter
% \renewcommand\AB@affilsepx{, \protect\Affilfont}
% \makeatother

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\newcommand{\model}{\textsc{Euler}\xspace}
\newcommand{\bothdataset}{\textsc{Euler-Suite}\xspace}
\newcommand{\traindataset}{\textsc{Euler-Instruct}\xspace}
\newcommand{\dataset}{\textsc{Euler-Bench}\xspace}
\newcommand{\jiwoo}[1]{\textcolor{purple}{#1}}
\newcommand{\gson}[1]{\textcolor{blue}{#1}}
\newcommand{\eg}{\textit{e.g.,}\xspace}

\title{Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning}

\author{
Guijin Son{\textsuperscript{1,2}} \quad Jiwoo Hong{\textsuperscript{3}} \quad Hyunwoo Ko{\textsuperscript{2}} \quad James Thorne{\textsuperscript{3}}  
 \\ \\
Yonsei University{\textsuperscript{1}} \quad OneLineAI{\textsuperscript{2}} \quad KAIST AI{\textsuperscript{3}}
\\
\texttt{spthsrbwls123@yonsei.ac.kr}\\ 
}

\begin{document}
\maketitle

\begin{abstract}
  
Scaling pre-training compute has proven effective for achieving multilinguality, but does the same hold for test-time scaling? In this work, we introduce \textbf{MCLM}, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods—Outcome Reward Modeling (ORM), Process Reward Modeling (PRM), and Budget Forcing (BF)—on both Qwen2.5-1.5B Math and \textbf{MR1-1.5B}, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although “thinking LLMs” have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages—a pattern consistent across the other test-time scaling methods we studied—highlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results.~\footnote{\url{https://github.com/gauss5930/MCLM}}

\end{abstract}



\section{Introduction}

Large Language Models (LLMs) have achieved impressive gains across a wide range of tasks by scaling compute during pre-training~\citep{thoppilan2022lamda, smith2022using}. Contrary to early concerns about a so-called “curse of multilinguality,”~\citep{conneau-etal-2020-unsupervised, pfeiffer-etal-2022-lifting} which suggested that training in diverse languages would degrade overall performance, sufficiently large decoder-only architectures have demonstrated strong multilingual capabilities~\citep{dubey2024llama3, aryabumi2024aya23}. Yet as further scaling becomes increasingly difficult—due to data scarcity~\citep{longpre2024consent}, diminishing returns, or prohibitive costs~\citep{achiam2023gpt}—researchers have begun exploring test-time scaling methods that expand a model’s reasoning or generation capacity at test time. An intriguing question arises: \emph{Does test-time scaling confer the same cross-lingual benefits we see at train-time scaling during pre-training?}
% \emph{Does test-time scaling confer the same cross-lingual benefits we see at large scales of pre-training compute?}


\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/comp_scale.pdf}
\vspace{-0.3in}
\caption{\footnotesize \textbf{Performance of Qwen2.5-1.5B-Math with different test-time scaling strategies.}——Once configured to use comparable inference FLOPs, all three methods (Outcome Reward Modeling, Process Reward Modeling, and Budget Forcing) achieve similar performance.}
\vspace{-0.15in}
\label{fig_title}
\end{figure}



Early studies demonstrated that chain-of-thought prompting~\citep{wei2022chain} and scratchpads~\citep{nye2021show} can significantly boost model performance—particularly in mathematics~\citep{lewkowycz2022solving, azerbayev2023llemma} and code~\citep{le2022coderl, chae2024language}. Building on this, recent work proposes ``test-time scaling,'' which further lengthens the chain-of-thought \citep{snell2024scaling, 
muennighoff2025s1simpletesttimescaling}. While such methods have proven effective for puzzles like Sudoku~\citep{Sudoku-RWKV} and Hex~\citep{jones2021scaling}, where action spaces are limited, mathematical reasoning remains relatively unexplored, largely due to its exponentially larger search space. To address this challenge, researchers have investigated external verifiers—such as best-of-N selection~\citep{wang2022self}, Monte Carlo Tree Search~\citep{guan2025rstar, tian2024toward, feng2023alphazero}, and process/outcome reward modeling~\citep{zhang2025lessons, liu2025prime}. Meanwhile, state-of-the-art LLMs~\citep{openai2024o1, openai2024o3mini} are capable of self-correction—often referred to as ``system 2'' reasoning~\citep{xiang2025towards}—without explicit external verification. While longer chains of reasoning provide more room for in-depth thinking, they may also amplify the risk of error propagation~\citep{bengio2015scheduled, arora-etal-2022-exposure, holtzman2019curious}, making them more susceptible to out-of-domain disturbances such as language variation~\citep{zhao2023large, chen2024not}. From this vein, it remains unclear whether these strategies robustly generalize to new questions~\citep{matharena}, languages, or domains.

In this work, we investigate the linguistic generalizability of test-time scaling methods by proposing a fine-grained multilingual complex reasoning benchmark, showing that test-time scaling alone \emph{does not} yield robust multilingual performance. We build MCLM (\textbf{M}ultilingual \textbf{C}ompetition \textbf{L}evel \textbf{M}ath), a math reasoning dataset composed of four subsets varying source covering 55 languages.

We analyze three test-time scaling methods, outcome reward modeling \citep[ORM]{wang2022self}, process reward modeling \citep[PRM]{zhang2025lessons}, and budget forcing \citep[BF]{muennighoff2025s1simpletesttimescaling}. We examine (1) \textit{accuracy} to determine whether models retain overall performance across languages and (2) \textit{consistency} to observe whether models can solve the same questions in different languages. While \textbf{ORM} and \textbf{PRM} provide clear gains on relatively easy datasets, the improvements are marginal for challenging tasks and inconsistent across the languages. Meantime, \textbf{BF} delivers noticeable gains only in English for tougher questions, with minimal impact on other languages. These findings underscore that while test-time scaling can enhance accuracy under certain conditions, it does not guarantee robust or consistent performance across multiple languages.

Finally, we introduce \textbf{MR1-1.5B}, an open multilingual thinking LLM trained on Deepseek-R1-1.5B using 100k R1-distilled instances translated by GPT-4o. Despite having only 1.5B parameters, \textbf{MR1} achieves performance on par with GPT-4o-Mini in multilingual mathematical reasoning.


% First, we introduce MCLM (\textbf{M}ultilingual \textbf{C}ompetition \textbf{L}evel \textbf{M}ath), a math reasoning dataset composed of four subsets varying source covering 55 languages. We then use Qwen2.5-Math-1.5B and 7B-Instruct as baselines to evaluate three techniques: outcome reward modeling \citep[ORM]{wang2022self}, process reward modeling \citep[PRM]{zhang2025lessons}, and budget forcing \citep[BF]{muennighoff2025s1simpletesttimescaling}. We examine (1) \textit{accuracy} to determine whether models retain overall performance across languages and (2) \textit{consistency} to observe whether models can solve the same questions in different languages. Finally, we introduce \textbf{MR1-1.5B}, an open multilingual thinking LLM trained on Deepseek-R1-1.5B using 100k R1-distilled instances translated by GPT-4o. Despite having only 1.5B parameters, \textbf{MR1} achieves performance on par with GPT-4o-Mini in multilingual mathematical reasoning.

% In this work, we investigate whether test-time scaling methods can extend beyond English. First, we introduce MCLM (\textbf{M}ultilingual \textbf{C}ompetition \textbf{L}evel \textbf{M}ath), a math reasoning dataset composed of four subsets varying source covering 55 languages. We then use Qwen2.5-Math-1.5B and 7B-Instruct as baselines to evaluate three techniques: outcome reward modeling \citep[ORM]{wang2022self}, process reward modeling \citep[PRM]{zhang2025lessons}, and budget forcing \citep[BF]{muennighoff2025s1simpletesttimescaling}. We examine (1) \textit{accuracy} to determine whether models retain overall performance across languages and (2) \textit{consistency} to observe whether models can solve the same questions in different languages. Finally, we introduce \textbf{MR1-1.5B}, an open multilingual thinking LLM trained on Deepseek-R1-1.5B using 100k R1-distilled instances translated by GPT-4o. Despite having only 1.5B parameters, \textbf{MR1} achieves performance on par with GPT-4o-Mini in multilingual mathematical reasoning.


% In this work, we show that test-time scaling alone \textit{does not} yield robust multilingual performance. While ORM and PRM can improve results by allocating more compute during inference, these gains are uneven across different languages and levels of difficulty. Moreover, larger computational budgets often amplify performance variance, reducing overall consistency. Our findings suggest that the benefits of test-time scaling are inconsistent across different languages.

% In this work, we show that test-time scaling alone \emph{does not} yield robust multilingual performance. While approaches like ORM, PRM, and BF can allocate more compute during inference, their benefits are uneven across languages and levels of difficulty. Specifically, \textbf{ORM} provides clear gains on relatively easy datasets but struggles to improve performance on harder math problems, suggesting limited benefits from simply scaling compute for more challenging tasks. \textbf{PRM} similarly offers some performance improvements, yet they remain inconsistent across languages: increasing compute neither boosts cross-lingual consistency nor reduces variance in accuracy. Lastly, \textbf{BF} delivers noticeable gains only in English for tougher questions, with minimal impact on other languages. These findings underscore that while test-time scaling can enhance accuracy under certain conditions, it does not guarantee robust or consistent performance across multiple languages.


% \section{Creating the MCLM benchmark}
\section{Multilingual Competition Level Math}

In this section, we introduce Multilingual Competition Level Math (\textbf{MCLM}), a multilingual math reasoning benchmark with challenging competition-level questions in 55 languages.

\paragraph{Going beyond math word problems} A translated version of GSM8K~\citep{cobbe2021training}, MGSM~\citep{shi2022language}, has been widely used to assess the mathematical reasoning skills of multilingual LLMs \citep{anil2023palm2technicalreport, shao2024deepseekmathpushinglimitsmathematical, aryabumi2024aya23openweight}. However, in Table~\ref{tab_mgsm}, we observe that recent LLMs saturate MGSM. This implies the limitations of simple math word problems in accurately assessing the math reasoning capabilities of LLMs and necessitates a higher degree of complexity in reasoning benchmarks.
% enables recent LLMs to solve its problems regardless of language, limiting its effectiveness for evaluating multilingual reasoning. This underscores the need for more challenging, competition-level benchmarks.

\paragraph{Assessing complex reasoning capabilities} In this vein, recent studies have evaluated the reasoning capabilities of LLMs using competition-level math questions \citep{AIME2024, gao2024omni}. While these benchmarks address limitations in simple math word problems, they are largely restricted to English and Chinese, limiting their ability to study multilingualism at scale.  


% {\color{red} TO DO}

\begin{table}[t!]
    \centering
    \fontsize{9}{11}\selectfont
    \begin{tabular}{lc}
    \toprule
    \textbf{Models} & \textbf{MGSM} \\
    \midrule
    Gemma2-9B & 78.37 \\
    Qwen2.5-14B-Instruct & 82.27 \\
    \midrule
    Qwen2.5-72B-Instruct & 88.16 \\
    Mistral-Large & 89.01 \\ 
    GPT-4o-mini & 87.36 \\
    o3-mini & \textbf{89.30} \\ 
    \bottomrule
    \end{tabular}
    \caption{\footnotesize \textbf{MGSM performance of different models.} The 2025-01-31 version is used for o3-mini, remaining scores were sourced from the \citet{yang2024qwen2}.}
    \label{tab_mgsm}
\end{table}

\subsection{Curating the MCLM benchmark}\label{mclm_benchmark}

\begin{table*}[ht]
\centering
\fontsize{9}{11}\selectfont
\begin{tabular}{@{}l l c c r@{}}
\toprule
\textbf{Subset}    & \textbf{Source Benchmark} & \textbf{Languages} & \textbf{Sample Size per Language} & \textbf{Evaluation Method} \\ 
\midrule
MT-MATH100  & Math-500   & 55  & 100 & Rule-based verifier \\
MT-AIME2024 & AIME 2024  & 55  & 30  & Rule-based verifier \\
M-IMO       & IMO (2006, 2024)      & 38  & 22--27 & LLM-as-a-Judge \\
M-MO        & Domestic/Regional Olympiads & 11 & 28--31 & LLM-as-a-Judge \\
\bottomrule
\end{tabular}
\caption{\footnotesize \textbf{Overview of benchmark subsets}: source benchmarks, language coverage (full lists in the appendix), sample sizes, and evaluation methods.  Please see Appendix~\ref{app:languages} for the full list of languages.}
\label{tab_euler_bench}
\end{table*}


% Recent trends in mathematical reasoning have shifted toward challenging, competition-level questions~\citep{AIME2024, gao2024omni}. To align with these developments, 

\paragraph{Machine-translated reasoning} We select AIME and MATH-500 \citep{lightman2023let}, two widely used mathematical benchmarks, as the main source of complex math questions. For 100 questions randomly sampled from MATH-500 and full AIME datasets, we translate both benchmarks with GPT-4o \citep{openai2024gpt4ocard}, as shown to be proficient in translating mathematical contexts~\citep{chen-etal-2024-breaking, lai2023okapi}. We then verified that the answers and equations remained unchanged after translation, removing one sample from MATH500 due to translation inconsistencies. Both subsets consist of questions with numerical answers only. For further details on the machine translation and sampling process, see Appendix~\ref{app:sampling_math100}.

% we first machine-translated MATH-500~\citep{lightman2023let} and AIME 2024—two widely used mathematical benchmarks—into 55 languages using GPT-4o. Due to the high cost of translating and evaluating the entire MATH500 benchmark (500 × 55 = 27,500 questions), we randomly sampled 100 questions from MATH500. We then verified that the answers and equations remained unchanged after translation, removing one sample from MATH500 due to translation inconsistencies. Both subsets consist exclusively of questions with numerical answers; accordingly, we employ a rule-based math verifier~\footnote{\url{https://github.com/huggingface/Math-Verify}} for evaluation.

\paragraph{Human-annotated reasoning} To mitigate the potential biases in machine-translated data from translation artifacts~\citep{plaza2024spanish, son2024kmmlu}, we also include human-translated or originally written questions. 

First, we manually review 114 International Mathematical Olympiad \citep[IMO]{IMO} questions from 2006 to 2024 in English, excluding proof-based and image-heavy problems, resulting in a final set of 27. We then collect their official translations in 38 languages. Where official translations are unavailable, we do not substitute machine-generated versions, leaving those entries missing. 


Second, we gather problems from various domestic and regional mathematical Olympiads worldwide. These contests originate in multiple languages, providing valuable data for multilingual mathematical reasoning. For English, Chinese, and Korean—where competition-level benchmarks already exist \citep{he2024olympiadbench, ko2025understand}—we incorporate existing datasets rather than recollecting data. While we exclude proof-based questions for simplicity, the final dataset still features a diverse range of answer formats (e.g., numerical, Boolean, descriptive) and spans 11 languages. We use GPT-4o-mini\footnote{2024-07-18 version \citep{openai2024gpt4ocard}} for evaluation. An overview is provided in Table~\ref{tab_euler_bench}, additional details are in Appendix~\ref{app:mimo_mmo}.


\section{Experimental Settings}


 In this section, we provide an overview of the test-time scaling methods evaluated (Section~\ref{sub_sec_tts}), compare the inference budgets in terms of FLOPs across different scaling techniques (Section~\ref{sub_sec_budget}), and describe the evaluation metrics used to assess their performance (Section~\ref{sub_sec_metrics}). 

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/inference-scaling.pdf}
\caption{\footnotesize \textbf{Comparison of different inference-time scaling strategies.} Blue boxes represent selected outputs, while red boxes indicate rejected ones.}
\label{fig_inference}
\end{figure}

\subsection{Baselines: Test-Time Scaling Strategies}\label{sub_sec_tts}
In this work, we evaluate three test-time scaling strategies using Qwen2.5-1.5B and 7B instruct models~\citep{yang2024qwen2O} as baselines (Figure~\ref{fig_inference}). We selected these model sizes because they offer a balanced trade-off between reasoning capacity and computational efficiency. Models smaller than 1.5B lack the capacity to solve complex problems, while larger models can be prohibitively expensive to scale~\citep{biderman2023pythia}.

\paragraph{Outcome Reward Modeling}~We generate \(N\) responses per instance and use Qwen2.5-Math-72B-RM~\citep{yang2024qwen2} to evaluate them, selecting the highest-scoring answer as the final output.

\paragraph{Process Reward Modeling}~In contrast to outcome reward modeling, this strategy integrates the reward model during inference to guide the generation process. We employ Qwen2.5-Math-72B-PRM~\citep{zhang2025lessons}; the model generates \(c\) candidate continuations at each step and selects the best one. For both ORM and PRM, the generator and reward model are served on separate servers, thereby avoiding the overhead of repeatedly on- and off-loading model weights.

\paragraph{Budget Forcing}~Recent LLMs, such as R1~\citep{guo2025deepseek} and O1~\citep{openai2024o1}, are designed to generate longer chain-of-thoughts with in-context exploration and correction, allowing them to naturally scale during inference. However, this approach lacks controllability. To mitigate this, we adopt the budget-forcing method proposed by \citet{muennighoff2025s1simpletesttimescaling}. In budget forcing, these thinking models are truncated and required to output an answer if they exceed a predefined budget. Conversely, if they fall short of the budget, they are prompted to generate additional reasoning steps, encouraging further exploration and correction.

\subsection{Calculating Inference FLOPs}\label{sub_sec_budget}

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/tokens.pdf}
\caption{\footnotesize \textbf{\# of generated tokens for 1.5B and 7B models in a greedy setting, divided by correctness.} Languages are represented as scatter plots, overlaid on box plots.}
\label{fig_tokens}
\end{figure}

For our experiments, we first establish a unified inference budget based on two key estimates: the generator’s cost is approximated as \(2N_GD\)~\citep{kaplan2020scaling}, where \(N_G\) is the number of parameters in the generation model and \(D\) is the total number of tokens generated per instance. The verifier’s (or reward model’s) cost is estimated as \(4N_V\), where \(N_V\) is the number of parameters in the verifier. Here, the multiplier of 4 for the reward model’s cost reflects a base cost of \(2N_V\) that is doubled to account for the additional overhead incurred when invoking the reward model during inference~\citep{snell2024scaling}. In the ORM, we generate \(k\) responses per instance, leading to a total inference cost of 
\begin{equation}
    k \times \Bigl(2N_GD + 4N_V\Bigr).
\end{equation}
% For example, assuming an average of \(D = 100\) tokens per instance, a generation model size of \(N_G = 1.5\times10^9\), and a verifier size of \(N_V = 72\times10^9\), the ORM budget becomes approximately 

% \[
% \begin{aligned}
% \text{ORM FLOPs} &\approx k\Bigl[\,2\,(1.5\times10^9)(100) + 4\,(72\times10^9)\Bigr] \\
%                   &\approx k\Bigl[\,3.0\times10^{11} + 2.88\times10^{11}\Bigr] \\
%                   &\approx k\times5.88\times10^{11}\,\text{FLOPs,}
% \end{aligned}
% \]
% Accordingly, assuming an average of \(D = 100\) tokens per instance, evaluating ORM for \(k = 2, 4,\) and \(8\) yields budgets of roughly \(1.18\times10^{12}\), \(2.35\times10^{12}\), and \(4.71\times10^{12}\) FLOPs, respectively. To ensure a fair comparison across test-time scaling strategies, we calibrate the configuration for PRM and Self-Correction so that their overall inference cost matches these budgets. For PRM, this involves adjusting the candidate multiplier per generation step, while Self-Correction requires forcing the total number of tokens spent in generation. Table~\ref{tab:inference_configs} summarizes these equivalent configurations.

In Figure~\ref{fig_tokens}, under greedy generation, we observe that models tend to generate longer responses once they produce an error, particularly on harder benchmarks. Additionally, the 7B model generally produces longer outputs than the 1.5B model. However, no systematic trends emerge across the 55 languages—there is no clear pattern, such as longer outputs for low-resource languages. Although token counts vary with configuration, these differences are negligible compared to the dominant effect of model size on inference FLOPs. Consequently, we use an average of 921 tokens per question to estimate cost.
\paragraph{Outcome Reward Modeling} In ORM with \(k=2\) responses, the inference cost is approximated as
\begin{equation}
    \text{ORM FLOPs} \approx 2\Bigl(2N_G \times 921 + 4N_V\Bigr).
\end{equation}
Assuming \(N_G = 1.5\times10^9\) and \(N_V = 72\times10^9\), this configuration results in an estimated cost of approximately \(6.10\times10^{12}\) FLOPs per instance.

\paragraph{Process Reward Modeling} In PRM, at each generation step, the model produces \( c \) candidates, with each candidate generating \( x \) tokens (we fix \( x = 128 \) in our experiments). The total inference cost over \( S \) steps is given by
\begin{equation}
    \text{PRM FLOPs} = S\,c\Bigl( x \cdot 2N_G + 4N_V \Bigr).
\end{equation}

For PRM configurations, \(S\) and \(c\), our preliminary experiments indicated that scaling one parameter in isolation produced suboptimal performance: a high \(S\) with a low \(c\) failed to explore sufficient alternatives, while an excessively low \(S\) prevented the generation process from completing. Therefore, we opted to proportionally scale both \(S\) and \(c\) to achieve a balanced search during generation.

\paragraph{Budget Forcing} In contrast, BF relies solely on the generator, so its inference cost is given by
\begin{equation}
    \text{SC FLOPs} = 2N_G \cdot BF,
\end{equation}
where \( BF \) denotes the effective number of tokens that may be generated during the inference with budget forcing. In Table~\ref{tab:config_sc}, we select the PRM parameters \( S \) and \( c \) and adjust the BF token budget \( BF \) so that the overall inference cost of each method matches that of ORM for \( k = 2, 4, \) and \( 8 \).


\begin{table}[t!]
\centering
\fontsize{9}{11}\selectfont
\begin{tabular}{ccc}
\toprule
\( k \) &  \((S, c)\) & \( BF \) \\
\midrule
2 & (3, 3) & \(\approx 2048\) tokens \\
4 & (4, 5) & \(\approx 4096\) tokens \\
8 & (5, 8) & \(\approx 8192\) tokens \\
\bottomrule
\end{tabular}
\caption{\footnotesize \textbf{Selected configurations for PRM and BF}. Each \(S\), \(c\), and \( BF\) is set so that the inference FLOPs match ORM.}
\label{tab:config_sc}
\end{table}

 
\subsection{Evaluation Metrics}\label{sub_sec_metrics}

We evaluate our models using two primary metrics that capture performance at multiple levels: (1) \textit{accuracy} and (2) \textit{cross-lingual consistency}.

\paragraph{Accuracy}
We measure accuracy at a surface level to determine whether a single model achieves comparable performance across different languages.

\paragraph{Cross-Lingual Consistency}
To examine whether the model tends to solve (or fail) the \emph{same} questions across languages, we compute \textbf{Fleiss’ kappa} \citep{fleiss1971measuring}, which is originally designed to measure agreement among multiple annotators. In our setup, however, we treat each language as an ``annotator’’: for each problem, each ``annotator’’ (i.e., each language version of the model) provides either a correct or incorrect label. We then define consistency through Fleiss' kappa as:
\begin{gather}
\kappa = \frac{\bar{P} - \bar{P}_e}{1 - \bar{P}_e}\\
\bar{P} = \frac{1}{N}\sum_{i=1}^N \frac{1}{n(n-1)}\sum_{j=1}^k n_{ij}(n_{ij}-1)\\
\bar{P}_e = \sum_{j=1}^k p_j^2, \quad p_j = \frac{1}{Nn}\sum_{i=1}^N n_{ij},
\end{gather}
where \(N\) is the number of problems, \(n\) is the number of languages, and \(n_{ij}\) is the count of how many times language \(j\) gives a particular label (correct or incorrect) for problem \(i\). In this formulation, \(\bar{P}\) is the observed agreement (i.e., the proportion of problems for which all languages concur on correctness or incorrectness), and \(\bar{P}_e\) is the expected agreement by chance. A high Fleiss’ kappa indicates that the model responds consistently across languages (solving the same problems), not merely achieving similar overall accuracy by chance.



% Third, the \textbf{equation overlap} metric quantifies the similarity of the CoT by comparing the sequences of equations extracted from model outputs. Specifically, it computes the length of the longest common subsequence (LCS) between two sequences and returns four values: (1) \emph{overlap\_count} (the LCS length, sensitive to order), (2) \emph{order\_similarity} (the LCS normalized by the maximum sequence length), (3) \emph{pure\_overlap\_count} (the size of the set intersection, ignoring order), and (4) \emph{pure\_overlap\_similarity} (the normalized pure overlap). This aims to evaluate whether models reach correct answers through similar reasoning processes.


\section{Result 1: ORM and PRM}

In this section, we assess the multilingual robustness of both ORM and PRM. We find that, while each approach can boost performance, these gains do not consistently generalize across different languages and levels of difficulty.

\subsection{Outcome Reward Modeling}

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/orm.pdf}
\caption{\footnotesize \textbf{Gains of ORM compared to a greedy-decoding baseline.} The semi-transparent “cloud” indicates the 2D data distribution via a KDE density plot, and the overlaid lines are third-order polynomial regressions modeling how each ORM setting scales with the baseline score.}
\label{fig_orm_results}
\end{figure}

For ORM, we use Qwen2.5-Math-1.5B and 7B-Instruct models to generate \(K\) samples per query and then apply Qwen2.5-Math-72B to score each sample, selecting the one with the highest score. 

\paragraph{Limited gains at scale in non-English settings}~In Figure~\ref{fig_orm_results}, we plot each model’s baseline performance (averaged across 55 languages) on the \(x\)-axis versus the relative gain of each ORM setting (with \(K \in \{2,4,8\}\)) on the \(y\)-axis. On the MT-MATH100 dataset, both the 1.5B and 7B models show consistent improvement as \(K\) increases. However, on the more challenging MT-AIME2024 dataset, the gains for different \(K\) values are largely indistinguishable and, in some cases, even negative. This trend is comparable to English, which shows steady improvements also on MT-AIME2024—for instance, the 1.5B model rises from 16.67 to 26.67 to 36.67 as \(K\) increases, while the 7B model goes from 20.00 to 26.67 to 36.67.

Overall, while ORM is a viable scaling strategy in English, it yields limited returns in many other languages—possibly due to the models' difficulty in generating high-quality candidates. With few plausible options available, the reward model cannot effectively identify an improved solution.



\subsection{Process Reward Modeling}

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/prm_comparison.pdf}
\caption{\footnotesize \textbf{PRM inference FLOPs as a function of generation steps $S$ and candidates per step $c$.} The left panel uses a verifier size of 72B, while the right panel uses a 7B RM, displaying adjusted configurations to yield similar costs.}
\vspace{-3pt}
\label{fig_prm_configs}
\end{figure}



Along with the configurations mentioned in Table~\ref{tab:config_sc}, we experiment with additional setups to study how PRMs scale. Details are provided in Figure~\ref{fig_prm_configs}. In general, we evaluate two approaches: fixing \(S\) while increasing \(C\) (pink) and fixing \(C\) while scaling \(S\) (green). Additionally, we compare the efficacy of a 7B verifier against the original 72B. Due to cost constraints, these configurations are tested only on 14 languages of MT-MATH100 using Qwen2.5-Math-1.5B-Instruct.

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/prm_full.pdf}
\caption{\footnotesize \textbf{Inference FLOPs versus PRM performance and consistency.} (Left) Second‐degree polynomial regressions for average performance on 14 languages, comparing the 7B (blue) and 72B (green) reward models. (Right) Fleiss’ kappa (top) and standard deviation (bottom) plotted against the same FLOPs budget; the fitted curves reveal no clear monotonic trend.}
\label{fig_prm_results}
\end{figure*}



\paragraph{No scalable gains for variance or consistency} Figure~\ref{fig_prm_results} shows that, in PRM, the average performance of Qwen2.5-Math-1.5B-Instruct increases steadily with the inference budget. Even though the 7B reward model provides a larger search space, the 72B reward model achieves better outcomes under comparable compute. From a hardware standpoint, it can be more effective to run fewer steps and rely on a larger verifier. However, no clear pattern emerges for Fleiss’ kappa or the standard deviation of individual scores, suggesting that adding more budget does not necessarily improve model consistency or reduce variance. In practical terms, while accuracy may scale with compute for PRM, cross-lingual consistency does not appear to follow, even for a relatively easier dataset like MT-MATH100. 




% \begin{figure}[h]
% \centering
% \includegraphics[width=\columnwidth]{figures/prm (1).pdf}
% \caption{\footnotesize \textbf{Inference FLOPs against performance on different PRM configurations.} Second‐degree polynomial regressions are shown for the 7B (blue) and 72B (green) RM.}
% \label{fig_prm_results}
% \end{figure}

% \begin{figure}[h]
% \centering
% \includegraphics[width=\columnwidth]{figures/prm_2 (1).pdf}
% \caption{\footnotesize \textbf{Inference FLOPs against performance on different PRM configurations.} Second‐degree polynomial regressions are shown for the 7B (blue) and 72B (green) RM.}
% \label{fig_prm_results}
% \end{figure}


\subsection{ORM over PRM}

As discussed earlier, both ORM and PRM exhibit unstable multilingual performance growth, with greater variance and lower Fleiss' Kappa scores at higher inference FLOPs. However, despite this instability, as shown in Figure~\ref{fig_orm_prm},  ORM consistently outperforms PRM in average accuracy, suggesting that, in general, it may be the more reliable choice. This is especially true given that, despite being assigned the same inference FLOPs, PRM invokes the reward model more frequently, requiring iterative back-and-forth interactions between the generator and evaluator, leading to higher latency.

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/orm_prm.pdf}
\caption{\footnotesize \textbf{Comparison of PRM vs. ORM performance on MATH (solid lines) and AIME (dashed lines).} 1.5B models are shown with plus markers, 7B models with stars. Blue lines represent PRM, green lines represent ORM. White box annotations indicate the performance difference (ORM − PRM) at the highest compute setting for each line.}
\label{fig_orm_prm}
\end{figure}


\section{Result 2: Budget Forcing}\label{budget_forcing}

LLMs with “system 2” reasoning~\citep{xiang2025towards}—such as OpenAI’s newest O-series models~\citep{openai2024o1}, Google’s Gemini Thinking\footnote{\url{https://deepmind.google/technologies/gemini/flash-thinking/}}, and Deepseek R1~\citep{guo2025deepseek}—are emerging as a test-time scaling approach. By generating and refining responses within a single inference (without external verifiers), these models seek to dramatically expand the inference budget for improved performance. In this section, we examine their effectiveness as a test-time scaling strategy. Because proprietary solutions remain largely opaque, we train our own LLMs with system 2 thinking. Here, we describe our training methods (Section~\ref{sub_sec_train}), report performance on MCLM (Section~\ref{sub_sec_perf}) and scaling affects from budget forcing(Section~\ref{sub_sec_scale}).


\begin{table*}[t!]
\centering
\fontsize{9}{11}\selectfont
\begin{tabular}{l|cccccc}
\toprule
Models & MT-MATH100 & MT-AIME2024 & M-IMO & M-MO & Average \\
\midrule
Qwen2.5-Math-1.5B-Instruct & 42.32 $\pm$ 8.61  & 16.36 $\pm$ 6.89  & 12.23 $\pm$ 6.02  & 25.00 $\pm$ 19.10  & 23.98 \\
Deepseek-R1-1.5B          & 49.40 $\pm$ 8.84  & 17.21 $\pm$ 6.69  & 21.94 $\pm$ 6.75  & 26.77 $\pm$ 19.83  & 28.83 \\
GPT-4o-Mini               & 70.30 $\pm$ 3.68  & 20.18 $\pm$ 6.83  & 13.33 $\pm$ 5.36  & 30.81 $\pm$ 15.80  & 33.66 \\
o3-Mini                  & \textbf{84.89} $\pm$ 2.80  & \textbf{45.33} $\pm$ 5.35  & \textbf{29.75} $\pm$ 6.86  & \textbf{51.42} $\pm$ 16.94  & \textbf{52.85} \\
\midrule
Qwen2.5-Math-1.5B + SFT    & 37.47 $\pm$ 7.56  & 14.85 $\pm$ 6.69  & 10.50 $\pm$ 5.16  & 18.40 $\pm$ 14.92  & 20.30 \\
Qwen2.5-Math-1.5B + MT-SFT & 42.02 $\pm$ 7.46  & 16.67 $\pm$ 7.31 &  10.52 $\pm$ 4.63  & 19.92 $\pm$ 12.68 & 22.28   \\
Deepseek-R1-1.5B + MT-SFT  & \textbf{55.61} $\pm$ 10.93 & \textbf{19.94} $\pm$ 8.10  & \textbf{19.20} $\pm$ 6.24  & \textbf{28.97} $\pm$ 16.64  & \textbf{30.93} \\
\bottomrule
\end{tabular}
\caption{\footnotesize \textbf{Model performance across MCLM.} Best model highlighted in \textbf{bold} for each panel. For results per language see Appendix~\ref{app_add_results}.}
% \jiwoo{Bold for each row?}}
\label{tab:performance}
\end{table*}

\subsection{Inducing System 2 Thinking}\label{sub_sec_train}
   
A number of concurrent works propose diverse strategies for developing LLMs with long thinking. Broadly, these approaches fall into two main categories: (1) online reinforcement learning with verifiable outputs \citep{deepscaler2025, ye2025emergence}, and (2) supervised fine-tuning on the "thinking trajectories" of proprietary LLMs \citep{muennighoff2025s1simpletesttimescaling, ye2025limo}. However, \citet{deepscaler2025} reports requiring 3,800 A100 GPU hours to induce such behavior, making it prohibitively expensive for our setting. Instead, we opt for supervised fine-tuning using thinking trajectories distilled from R1. Below are three training configurations.


% \paragraph{Qwen2.5-Math-1.5B + GRPO}~Motivated by the hypothesis that ``thinking'' may emerge in base models through online reinforcement learning, we adopt this configuration. We use the default OpenR1-220K~\footnote{\url{https://huggingface.co/datasets/open-r1/OpenR1-Math-220k}} dataset---comprising 94K instances drawn from Numina-Math~\citep{numina_math_datasets}---as training data.
    
\paragraph{Qwen2.5-Math-1.5B + SFT}~Following \citet{muennighoff2025s1simpletesttimescaling} and  \citet{huang2024o1}, we randomly sample 50K thinking trajectories generated by R1 from the OpenR1-220K~\footnote{\url{https://huggingface.co/datasets/open-r1}} dataset and fine-tune our model for three epochs.

\paragraph{Qwen2.5-Math-1.5B + SFT with Translated Data}~While training exclusively on English math problems has been shown to be effective and can generalize to new languages to some extent~\citep{liu2024acemath}—likely due to the universal nature of mathematical logic—we explore whether translating the data helps training. Following \citet{ko2025understand, zhang2023plug}, we translate the problem and solution components into 14 languages\footnote{See Appendix~\ref{app:languages} for the complete list of languages.} using GPT-4o, while keeping the reasoning process in English to leverage the model’s strong proficiency in English-based logical reasoning. For further details on the training dataset, see Appendix~\ref{app_training}.

\paragraph{Deepseek-R1-1.5B + SFT with Translated Data}~Finally, we try initializing the translated SFT from Deepseek-R1-1.5B (hereafter MR1-1.5B). Since the model is already proficient in generating extended reasoning, we observe that longer training leads to performance degradation. To mitigate this, we terminate training at 0.5 epochs. 


\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/training.pdf}
\caption{\footnotesize \textbf{Performance of Qwen2.5-Math-1.5B +SFT and + MT-SFT at each training checkpoint.} Average score and error bars for each checkpoint are displayed. The shaded region is the mean $\pm$ standard deviation for MT-SFT.}
\label{fig_training}
\end{figure}



\subsection{Performance of trained models}\label{sub_sec_perf}



\paragraph{Translated data improves cross-lingual performance.}Table~\ref{tab:performance} compares Qwen2.5-Math-1.5B and Deepseek-R1-1.5B under various fine-tuning regimes, revealing two key trends. First, incorporating translated data into Qwen2.5-Math-1.5B delivers a modest +1.98\% improvement over an English-only setup, indicating that relying exclusively on English data is insufficient for robust cross-lingual performance \citep{liu2024acemath}. As shown in Figure~\ref{fig_training}, models trained on multilingual inputs begin with lower accuracy—likely due to increased entropy—but soon surpass their English-only counterparts and maintain lower variance across languages. Second, initiating fine-tuning from Deepseek-R1-1.5B, already adept at extended chain-of-thought reasoning, yields even greater gains on MT-AIME2024, M-IMO, and M-MO, performing on par with GPT-4o-Mini. Notably, MR1-1.5B reaches an average score of 30.93 (+2.1\% over its original baseline) with just 0.5 epochs of training, underscoring how a self-correcting model more readily benefits from multilingual data. Collectively, these results suggest that while incorporating translated data benefits a monolingual base, leveraging a model with established self-correction capabilities can amplify these gains in multilingual math reasoning.



\subsection{Budget-Constrained Scaling}\label{sub_sec_scale}

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/budget_forcing.pdf}
\caption{\footnotesize \textbf{Performance of MR1 on MT-AIME2024 at \(BF = \{2048, 4096, 8192\}\).} Grey dots represent individual languages. Solid lines indicate average performance, while dashed lines highlight reference performances for selected languages.}
\label{fig_budget_forcing}
\end{figure}

To better compare self-correction with other scaling methods (e.g., ORM and PRM), we examine its performance under fixed inference budgets. We apply the budget forcing approach introduced by \citet{muennighoff2025s1simpletesttimescaling} to constrain the generation budget of MR1-1.5B. Following the budget settings in Table~\ref{tab:config_sc}, during inference, if the model reaches 90\% of its allocated budget, we truncate the output and append "The final answer is" to prompt a concise answer. Conversely, if the model completes generation before reaching the limit, we truncate at the last line break, append "Wait...", and prompt the model to continue generating.



\paragraph{R1-like LLMs offer no clear edge over ORM}~Contrary to the recent surge of interest in ``system 2'' LLMs, that scale test-time compute by generating long reasoning traces, constraining these models to the same inference budgets reveals no clear advantage over test-time scaling methods such as ORM or PRM (see Figure~\ref{fig_title}). As shown in Figure~\ref{fig_budget_forcing}, budget forcing yields nearly linear performance gains for English but provides limited benefits for most other languages. The distribution remains largely unchanged, achieving an overall average increase of only 1.9\% as \(BF\) scales from 2048 to 8096. In some cases, such as Latvian and Romanian, performance even declines. Implying that there is scant evidence that the variance in performance diminishes or that coverage expands uniformly.


\section{Related Works}

\paragraph{Test-Time Scaling}~As concerns grow that the benefits of scaling pre-training compute may be saturating~\citep{longpre2024consent}, research has shifted toward \emph{test-time scaling}, which expands the notion of chain-of-thought reasoning~\citep{wei2022chain}. Intuitively, the reasoning capacity of an LLM is limited by the number of tokens it can generate; hence, more challenging questions may require a longer chain of thought~\citep{wu2025more}. An early example is self-consistency CoT~\citep{wang2022self}, which generates multiple responses and selects the best via voting. This idea has since been developed into more cost-effective strategies for searching broader solution spaces (e.g., tree-of-thought methods~\citep{yao2024tree}, Monte Carlo Tree Search~\citep{guan2025rstar}, and process supervision~\citep{zhang2025lessons, luo2024improve}). Recently, models trained with online reinforcement learning~\citep{shao2024deepseekmathpushinglimitsmathematical} appear to exhibit an ``aha moment,''~\citep{guo2025deepseek} wherein they dynamically decide to generate longer sequences to iteratively explore, solve, and self-correct. 


\paragraph{Mathematical Reasoning in Non-English}~Early attempts at multilingual math reasoning involved supervised fine-tuning on translated datasets~\citep{chen2023breaking, lai2024mcot}, but performance often deteriorated when models shifted away from their original language embeddings~\citep{hong2024cross}. To minimize such degradation, more recent work has increasingly relied on English as a pivot language. This approach can be implemented in various ways: either internally, by mapping multilingual inputs into an English-centric latent space \citep{yoon2024langbridge, fan2025slam, zhu2024question, she2024mapo}, or externally, by translating non-English tasks into English and then back to the target language \citep{zhang2023plug, ko2025understand}. Although this strategy has reduced the performance gap between English and other languages, the stability of transfer under different training conditions remains underexplored. Moreover, many studies rely on the MGSM benchmark~\citep{shi2022language}, which appears too easy for large-scale models or those enhanced by advanced reasoning techniques such as test-time scaling.


\section{Conclusion}
% In this work, we investigate the linguistic generalizability of three test-time scaling methodologies---outcome reward modeling, process reward modeling, and budget forcing---under budget-constrained settings. While these approaches appear promising in English, our findings using MCLM, a novel math benchmark covering 55 languages, reveal that their benefits do not reliably extend to other languages; in fact, increasing test-time compute often intensifies performance variance and undermines consistency across languages. To enable further research on this issue, we release \textbf{MCLM}, and \textbf{MR1-1.5B}, a multilingual LLM demonstrating extended “system 2” reasoning.

In this work, we examine the linguistic generalizability of three test-time scaling methodologies—Outcome Reward Modeling (ORM), Process Reward Modeling (PRM), and Budget Forcing (BF)—under budget-constrained settings. Using Qwen2.5-1.5B Math as a generator, ORM achieves a 35.84 score on our newly introduced multilingual math benchmark, \textbf{MCLM}, which spans 55 languages. With BF, \textbf{MR1-1.5B}—our multilingual LLM demonstrating extended reasoning—attains 35.23. Notably, once constrained to similar inference budgets, all three scaling methods exhibit comparable levels of improvement. Additionally, although these approaches appear promising in English (e.g., a 20-point improvement on AIME for the 1.5B model), we find that such gains do not consistently extend to other languages, where improvements average only 1.94 points—a pattern observed across all three methods. Moreover, increasing test-time compute often amplifies performance variance and reduces cross-linguistic consistency. To enable further study of these issues, we release both MCLM and MR1-1.5B.

% In this work, we examine three test-time scaling methodologies---outcome reward modeling, process reward modeling, and budget forcing---within budget-constrained settings. Although these approaches appear promising in English, our findings indicate that their gains do not consistently extend to other languages. Increasing test-time compute often leads to higher performance variance across languages and reduced overall consistency. Finally, to stimulate further research in this area, we introduce \textbf{MCLM}, the first publicly available competition-level math benchmark spanning 55 languages, and \textbf{MR1-1.5B}, a multilingual LLM that demonstrates extended ``system 2'' reasoning. 

\section*{Limitations}

Although this work focuses solely on mathematical tasks, the lack of multilingual generalization we observe could be even more pronounced in areas requiring extensive cultural or domain-specific understanding; we leave this for future works. Additionally, due to budget constraints, this work primarily focuses on smaller-scale experiments (e.g., Qwen2.5-Math-1.5B-Instruct and occasionally Qwen2.5-Math-7B-Instruct). Although these parameter ranges are commonly used in both industry and academia, the observed lack of multilingual generalization for test-time scaling may not necessarily extend to significantly larger models (\(70\text{B}\) or more). Moreover, our test-time scaling experiments also remain on the smaller side; for instance, \citet{el2025competitive} scale to as many as 1162 candidates in their best-of-\(n\) setting. (It should still be noted that even experiments at this scale required over \textbf{2500 A100 GPU hours}) Given that the so-called ``curse of multilinguality''~\citep{conneau-etal-2020-unsupervised} naturally disappears as pre-training compute grows by several orders of magnitude~\citep{aryabumi2024aya23}, it is plausible that larger models may behave differently. Nevertheless, our findings at smaller scales—where test-time scaling shows no indication of fostering robust multilingualism—remain valuable, as they reveal potential boundaries for less resource-rich setups and highlight directions for future research.

 
%\section*{Acknowledgments}

% % Bibliography entries for the entire Anthology, followed by custom entries
\bibliography{custom}
% % Custom bibliography entries only

\appendix

\newpage

\onecolumn

\section{Additional details on MCLM}
In this section, we provide additional details on the MCLM benchmark, including the languages covered by each subset (Section~\ref{app:languages}), the sampling process for MT-MATH100 (Section~\ref{app:sampling_math100}), the sources for the M-IMO, and M-MO subset (Section~\ref{app:mimo_mmo}), the prompts used for GPT-4o and -mini (Section~\ref{app:prompts}), and contamination considerations (Section~\ref{app:contamination}).

\subsection{Details in Language Coverage}\label{app:languages}

We examine four groups of languages in this paper: (A) the 55 languages into which MATH500 and AIME2024 have been translated, (B) the 14 languages frequently sampled to reduce evaluation costs, (C) the languages covered in M-IMO, and (D) those in M-MO. The complete list for each group is provided in Table~\ref{tab:lang_appendix}.


\begin{table*}[ht]
\centering
\fontsize{9}{11}\selectfont
\begin{tabular}{@{}l|p{10cm}|c@{}}
\toprule
\textbf{Lang. Group} & \textbf{Languages (ISO Codes, Sorted Alphabetically)} & \textbf{\# Lang.} \\
\midrule

(A) & \texttt{af, ar, bg, bn, ca, cs, cy, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, kn, ko, lt, lv, mk, ml, mr, ne, nl, no, pa, pl, pt, ro, ru, sk, sl, so, sq, sv, sw, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw} & 55 \\[1ex]
\midrule
(B) & \texttt{af, ar, de, en, es, fr, he, id, it, ja, ko, tr, vi, zh-cn} & 14 \\[1ex]
\midrule
(C) & \texttt{af, ar, bg, cs, da, de, el, en, et, es, fi, fr, he, hr, hu, id, it, ja, ko, lt, lv, mk, nl, no, pl, pt, ro, ru, sk, sl, sq, sv, th, tr, uk, vi, zh-cn, zh-tw} & 38 \\[1ex]
\midrule
(D) & \texttt{cs, de, en, fr, ja, ko, nl, pl, ru, sk, zh-cn} & 11 \\
\bottomrule
\end{tabular}
\caption{\footnotesize Full language lists for each dataset subset. MT-MATH100, MT-AIME2024, M-IMO, and M-MO cover 55, 38, and 11 ISO codes respectively.}
\label{tab:lang_appendix}
\end{table*}

Running evaluations on all 55 languages can be computationally intensive, particularly when testing for test-time scaling. To address this, we have created a downsampled group (B) consisting of 14 languages for our experiments. These languages were chosen to represent a broad spectrum of linguistic families and writing systems. In terms of language families, the selection includes representatives from Afro-Asiatic (Arabic, Hebrew), Austronesian (Indonesian), Japonic (Japanese), Koreanic (Korean), Turkic (Turkish), Austroasiatic (Vietnamese), and Sino-Tibetan (Chinese). Additionally, the chosen languages encompass diverse scripts: several, including Afrikaans, German, English, Spanish, French, and Italian, use the Latin alphabet; others use distinct writing systems—Arabic and Hebrew employ abjads (consonant-based scripts); Japanese combines logographic characters (Kanji) with syllabic scripts (Hiragana and Katakana); Korean is written in Hangul; Turkish uses a modified Latin alphabet; Vietnamese utilizes a Latin-based alphabet with diacritics; and Chinese is written using Chinese characters.


\subsection{Sampling MATH100}\label{app:sampling_math100}

Once creating MGSM, \citet{shi2022language} opted to randomly sample 250 questions from the GSM8K dataset for computational efficiency. Similarly, we sample 100 questions from MATH500 to keep evaluation costs manageable across multiple languages. Initially, we conduct random sampling. Before extending this approach to all 55 languages, we first apply it to language group (B). For language group (B), we create both the MT-MATH100 and MT-MATH500 versions, where entire subsets are translated for the later. We then evaluate 10 models—each trained using different methods to enhance mathematical reasoning—to determine whether the sampled MATH100 subset reliably represents the full dataset.

\begin{table*}[h]
    \centering
    \fontsize{10}{13}\selectfont
    \begin{tabular}{c|lcc|cc}
        \toprule
        \textbf{Rank} & \multicolumn{1}{c}{\textbf{Model}} & \textbf{MATH-500} & \textbf{MATH-100} & \textbf{Score Diff.} & \textbf{Rank Diff.}\\
        \midrule
        1  & o3-mini & 85.00 & 85.93 & 0.93 & - \\
        2  & Eurus-2-7B-PRIME & 73.76 & 76.63& 2.86 & - \\
        3  & Qwen2.5-Math-7B-Instruct & 73.70 & 75.98 & 2.27 & - \\
        4  & DeepSeek-R1-Distill-Qwen-32B & 72.73 & 75.98 & 3.24 & - \\
        5  & DeepSeek-R1-Distill-Qwen-7B & 67.25 & 68.69 & 1.44 & \up{1} \\
        6  & AceMath-7B-Instruct & 65.90 & 70.06 & 4.16 & \down{1} \\
        7  & AceMath-1.5B-Instruct & 65.60 & 68.19 & 2.58 & - \\
        8  & DeepSeek-R1-Distill-Qwen-1.5B & 53.74 & 56.78 & 3.05 & - \\
        9  & Qwen2.5-Math-1.5B-Instruct & 51.80 & 51.30 & 0.51 & - \\
        10 & Qwen2.5-Math-1.5B-OREO & 39.92 & 38.45 & 1.47 & - \\
        \bottomrule
    \end{tabular}
    \caption{\footnotesize Model rankings and score comparison between MATH-500 and MATH-100. The score difference was computed as the absolute difference between the MATH-500 and MATH-100 scores. The rank difference indicates the change in ranking on MATH-100 relative to the performance on MATH-500.}
    \label{tab:sampled_math}
\end{table*}

In Table~\ref{tab:sampled_math}, we report the performance of the evaluated models. The score differences are relatively small, and even after accounting for minor variations, the ranking of the 10 models remains largely consistent—with only a few instances of rank switching. We conclude that the sampled version serves as an acceptable proxy for the full dataset and proceed accordingly.

\subsection{Sourcing M-IMO and M-MO}\label{app:mimo_mmo}

Relying solely on machine-translated benchmarks (MT-MATH100 and MT-AIME2024) carries inherent risks. To mitigate this, we supplement our dataset with questions from the International Mathematical Olympiad (IMO) and various regional math olympiads. Figure~\ref{fig_imo} provides an overview of the IMO questions included from 2004 to 2024, while Table~\ref{tab:math_competitions} lists the sources for regional olympiads. For English, Chinese, and Korean, we utilize existing datasets rather than recollecting questions~\citep{he2024olympiadbench, ko2025understand}.


\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/imo.pdf}
\caption{\footnotesize  Heatmap representation of IMO problems from 2006 to 2024. Each row corresponds to a competition year, and each column represents a problem (Q1–Q6). Green cells indicate questions that have been included in the M-IMO subset, while gray cells represent problems that were not selected.}
\label{fig_imo}
\end{figure}


\begin{table*}[h]
\centering
\begin{tabular}{l|p{10cm}}
\toprule
\textbf{Language} & \textbf{Competition Links} \\
\midrule
French & \url{https://euler.ac-versailles.fr/spip.php?rubrique207} \\
German & DeMO \\
Japanese & \url{https://www.imojp.org/domestic/jmo_overview.html#Problems} \\
Dutch & \url{https://prime.ugent.be/activiteiten/puma/} \newline \url{https://wiskundeolympiade.nl/wedstrijdarchief/1e-ronde} \\
Czech & \url{https://www.matematickaolympiada.cz/mo-pro-ss/rocnik} \newline \url{https://iksko.org/problems.php} \\
Polish & \url{https://om.sem.edu.pl/problems/} \\
Slovakian & \url{https://skmo.sk/dokumenty.php?rocnik=74} \newline \url{https://riesky.sk/archiv/} \\
Russian & \url{https://mmo.mccme.ru//} \\
\bottomrule
\end{tabular}
\caption{Link to mathematical competition links that has been included in M-MO subset.}
\label{tab:math_competitions}
\end{table*}

\subsection{Prompts}\label{app:prompts}

The prompts used for question translation (Figure~\ref{question_system}), solution translation (Figure~\ref{solution_template}), and model judgment (Figure~\ref{judge_template}) are detailed accordingly.

\subsection{Benchmark Contamination}\label{app:contamination}
As LLMs are trained on ever-growing datasets, concerns about benchmark contamination have emerged~\citep{xu2024benchmark}. Because most training data and procedures remain proprietary, it is virtually impossible to guarantee that a benchmark is entirely free of contamination. Existing detection methods remain unstable, and our \textsc{MCLM} dataset, particularly its \textsc{M-IMO} and \textsc{M-MO} subsets collected from the Internet, may be prone to exposure in various model-training corpora. For instance, we observe that \emph{Llama-3.1-3B-Instruct} can produce correct answers without any visible reasoning, suggesting prior familiarity with certain questions. However, lacking a robust decontamination method, we have chosen not to remove potentially contaminated samples. Despite this possibility, the overall low performance of most models on these subsets suggests that \textsc{MCLM} remains a challenging and valuable resource for evaluating multilingual, competition-level math. Furthermore, our primary focus is on assessing \emph{multilingual consistency}. If a model correctly ``guesses'' a single language version of a question (possibly due to prior exposure), it does not necessarily indicate strong multilingual reasoning. Conversely, if the model can solve all language variations after seeing only a few, it may demonstrate a degree of cross-lingual robustness.


\subsection{License}
The machine-translated subsets of MCLM are released under the MIT License. The remaining subsets are provided under a CC BY-NC-ND license, although we may transition to a more permissive license pending further review of the original competition data policies.

\section{Additional details in training LLMs with system 2 thinking}
\label{app_training}
In this section, we provide additional details on the dataset used to train self-correcting LLMs (Section~\ref{app_comp_w_past}), ablation studies on MR1 training (Section~\ref{app_ablation_training}), and the training configurations.

\subsection{Additional details on the training dataset}~\label{app_comp_w_past}
In training MR1 (i.e., Deepseek-R1-1.5B + SFT with translated data), we first collect thinking trajectories generated by \textsc{R1} \citep{guo2025deepseek} from the Numina Math dataset in \emph{Be-Spoke Stratos} \citep{bespokestratos} and \emph{OpenThoughts} \citep{openthoughts}. To ensure high-quality reasoning supervision, we exclude any data distilled from smaller thinking-model variants (e.g., \emph{QwQ-32B-Preview} \citep{qwq-32b-preview} or the \emph{R1-Distil} series). We then employ GPT-4o as an LLM-based judge to filter out instances with incorrect answers. 

Next, the question and solution for each remaining instance are translated into one of the 14 languages in Group (B). We opt for only 14 languages---as opposed to all 55---because our ablation studies indicate that oversampling languages can negatively impact overall performance. A rule-based parser validates that the question and answer content remains unchanged post-translation. Beyond this verification, no additional quality checks are performed. However, when we encounter elevated loss spikes during training, we backtrack through the dataset to identify and remove problematic instances. Following this process, we retain approximately 120K instances.



\paragraph{Proxying problem difficulty}~Table~\ref{tab:dataset_comparison} compares \traindataset{} with existing multilingual math datasets: MGSM8KInstruct~\citep{chen2023breaking} and mCoT-MATH~\citep{lai2024mcot}. MGSM8KInstruct extends GSM8K~\citep{lightman2023let} by translating it into 10 languages, yielding a parallel dataset of approximately 8,000 questions per language. In contrast, mCoT-MATH sources 560,000 seed questions from MetaMathQA and MathInstruct and translates them into 10 languages.  

\begin{table}[h]
    \centering
    \fontsize{9}{11}\selectfont
    \begin{tabular}{lccc}
    \toprule
        \textbf{Dataset} & \textbf{\# Lang.} & \textbf{\# Inst.} & \textbf{Diff.} \\ 
        \midrule
        MGSM8KInstruct & 10 & 73.6k & G.S \\
        mCoT-MATH      & 10 & 6.3M  & G.S \\
        \midrule
        Euler-Instruct (Ours) & 55 & 250K & C.L \\ 
        \bottomrule
    \end{tabular}
    \caption{\footnotesize \textbf{Comparison of Multilingual Mathematical Reasoning Datasets.} The \textit{Diff.} column indicates difficulty level, where \textit{G.S} represents grade school level and \textit{C.L} represents competition level.}
    \label{tab:dataset_comparison}
\end{table}

To estimate the difficulty level of each dataset, we randomly sample 1,000 questions and measure the solve rate of LLMs. Since MGSM8KInstruct and mCoT-MATH were published before \traindataset, they may have been included in the training of existing LLMs, potentially making a direct comparison unfair. To address this, we first evaluate using OLMo2-\{7, 13\}B~\citep{olmo20242}; we use the base model instead of the instruct model since GSM8K is used during the instruction tuning phase. Since OLMo2-7B-base is not instruction-tuned, we provide three-shot examples and prompt it to solve the questions in a structured format: "The answer is X."  Additionally, we evaluate with Qwen2.5-\{7, 32, 72\}B-Instruct~\citep{yang2024qwen2} in a zero-shot setting, using a parser to extract answers.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/solve_rate.pdf}
\caption{\footnotesize \textbf{Solve rates (\%) of different multilingual math datasets evaluated.} For the OLMo2 series, we use the base models, while for the Qwen2.5 series, the instruct-tuned variants are used. \traindataset presents a significantly lower solve rate, indicating its greater difficulty.}
\label{fig_solve_rate}
\end{figure}


Figure~\ref{fig_solve_rate} illustrates that, for both mCoT-MATH and MGSM8KInstruct, over half of the problems are solved by Qwen2.5-7B-Instruct, and Qwen2.5-72B-Instruct solves more than 70\%. In contrast, the solve rate for \traindataset remains low across all models, with the 7B variants solving less than 20\% of the problems and even the 72B variants achieving below 40\%. Notably, the Verify subset exhibits a lower solve rate than previous datasets, indicating that its difficulty remains high even when restricted to numerical answers.



\subsection{Training Ablations}~\label{app_ablation_training}
Before training MR1 we meet the question: \emph{How little additional data is required to incorporate a new language into a self-correcting model?} To address this, we train four models under a fixed total budget of 24,000 training instances. Table~\ref{tab:language_distribution} details the language composition and the per-language instance allocation for each model.
\begin{table}[ht]
\centering
\fontsize{9}{11}\selectfont
\begin{tabular}{l c c}
\toprule
\textbf{Languages} & \textbf{\# Lang.} & \textbf{\# Instances} \\
\midrule
{\small ko} & 1 & 24k \\
{\small af, fr, ko} & 3 & 8k \\
{\small af, ar, fr, he, id, ko, tr} & 7 & $\approx$3.5k \\
{\small all 14 in \traindataset} & 14 & $\approx$1.7k \\
\bottomrule
\end{tabular}
\caption{\footnotesize \textbf{Details on trained models.} All models are trained with a total of 24,000 instances. \# Instances denote the number of instances used per language.}
\label{tab:language_distribution}
\end{table}


\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/budget_ablations.pdf}
\caption{\footnotesize \textbf{Model Results from Table~\ref{tab:language_distribution}.} Left shows accuracy on MT-MATH500 (entire translated subset for language group (B)), and right shows average performance of MT-AIME2024.}
\label{fig_ablation}
\end{figure*}

Figure~\ref{fig_ablation} presents the performance gains over the base model (DeepSeek-R1-1.5B) as a function of the number of training instances. On MT-MATH100, performance rises sharply once the per-language budget reaches approximately 3.5k instances. By contrast, MT-AIME2024 shows more gradual improvements, suggesting that transferring self-correction capabilities to a challenging set of new-language questions requires a larger data allocation. Based on these findings, we use 14 languages within a total budget of 120K, ensuring that each language includes at least 8k instances.




\subsection{Training Configurations and Logs}

Axolotl~\citep{axolotl2025} is used for the SFT training in Section~\ref{budget_forcing}. We train Qwen2.5-Math-1.5B with DeepSpeed-Zero1~\citep{rajbhandari2020zero} on 4 A100 80GB GPUs for 8 hours per run. \citet{hsu2024liger} is used for optimization.
\begin{table}[hb]
\centering
\fontsize{9}{11} \selectfont
\begin{tabular}{cc}
\toprule
\textbf{Category} & \textbf{Section~\ref{budget_forcing}} \\ \midrule
\textbf{Sequence Length} & 16,384 \\
\textbf{Learning Rate} & \(2 \times 10^{-5}\) \\
\textbf{Global Batch (Effective)} & 128 \\
\textbf{Learning Rate Scheduler} & Cosine Decay \\
\textbf{Warmup Ratio} & 0.05 \\
\textbf{Training Epochs} & 3 \\ \bottomrule
\end{tabular}
\caption{\footnotesize \textbf{SFT configuration details for Section~\ref{budget_forcing}.}}
\label{tab:training_configs}
\end{table}


\begin{figure*}[h]
\begin{instructionsbox}
\begin{lstlisting}
You will be given an English question in the following format.

[Question]
<question>
{..question...}
</question>

Your job is to return a translated version of the question.
* Translate to <language>.
* The translation must be fluent, easy to read by native speakers.
* Do not solve the prompt translate it.
* You must preserve all details including math notations (latex) and code. 
* The math notations and code must not be translated, keep it as is.
* Return you translation in the following format.

[Translation]
<translation>
{..translated question...}
</translation>

--------------------------------------------------
The following is the math problem for you task:

[Question]
<question>
<source_question>
</question>
\end{lstlisting}
\end{instructionsbox}
\caption{Question Translation Template}
\label{question_system}
\end{figure*}


\begin{figure*}[h]
\begin{instructionsbox}
\begin{lstlisting}
You will be given an English solution in the following format.

[Solution]
<solution>
{..solution in English...}
</solution>

Your job is to rewrite the English solution to <language>. 
* The solution must preserve the original structure and details.
* You must preserve all details including math notations (latex) and code. 
* The math notations and code must not be translated, keep it as is.
* The solution must be natural, easy and polite for a native speaker to read.

[Translation]
<translation>
{..translated solution...}
</translation>

--------------------------------------------------
The following is the math problem and solution for your task:

[Solution]
<solution>
<source_solution>
</solution>
\end{lstlisting}
\end{instructionsbox}
\caption{Solution Translation Template}
\label{solution_template}
\end{figure*}


\begin{figure*}[h]
\begin{instructionsbox}
\begin{lstlisting}
You will be given a math problem, the correct answer, and a solution generated by a language model. Your task is to determine whether the solution generated by the model is correct.

[Question]
<question>
{..math question...}
</question>

[Correct Answer]
<answer>
{..correct answer...}
</answer>

[Model Solution]
<solution>
{..model-generated solution...}
</solution>

Instructions:
* Compare the model's solution with the correct answer.
* If the model's solution is correct, output [[TRUE]].
* If the model's solution is incorrect, output [[FALSE]].
* You do not have to judge the solution process; there are numerous possible 'Gold' solutions, and the model solution does not have to be identical with the one provided. As long as the model reaches the correct answer, it is correct.
* Do not provide any explanations -- only return your judgment ONLY.

--------------------------------------------------
The following is the math problem and solution for your task:

[Question]  
<question>
<math_question>
</question>

[Correct Answer]
<answer>
<correct_answer>
</answer>

[Model Solution]
<solution>
<model_solution>
</solution>
\end{lstlisting}
\end{instructionsbox}
\caption{Judge Template}
\label{judge_template}
\end{figure*}


\section{Additional Results}\label{app_add_results}
From Tables \ref{tab:1_5B_greedy} to \ref{tab:o3_mini_result}, we report detailed evaluation results for 55 languages with varying models and test-time scaling methods.
\import{}{additional_result.tex}



\end{document}
