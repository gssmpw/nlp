\section{Related Works}
\paragraph{Test-Time Scaling}~As concerns grow that the benefits of scaling pre-training compute may be saturating____, research has shifted toward \emph{test-time scaling}, which expands the notion of chain-of-thought reasoning____. Intuitively, the reasoning capacity of an LLM is limited by the number of tokens it can generate; hence, more challenging questions may require a longer chain of thought____. An early example is self-consistency CoT____, which generates multiple responses and selects the best via voting. This idea has since been developed into more cost-effective strategies for searching broader solution spaces (e.g., tree-of-thought methods____, Monte Carlo Tree Search____, and process supervision____). Recently, models trained with online reinforcement learning____ appear to exhibit an ``aha moment,''____ wherein they dynamically decide to generate longer sequences to iteratively explore, solve, and self-correct. 


\paragraph{Mathematical Reasoning in Non-English}~Early attempts at multilingual math reasoning involved supervised fine-tuning on translated datasets____, but performance often deteriorated when models shifted away from their original language embeddings____. To minimize such degradation, more recent work has increasingly relied on English as a pivot language. This approach can be implemented in various ways: either internally, by mapping multilingual inputs into an English-centric latent space ____, or externally, by translating non-English tasks into English and then back to the target language ____. Although this strategy has reduced the performance gap between English and other languages, the stability of transfer under different training conditions remains underexplored. Moreover, many studies rely on the MGSM benchmark____, which appears too easy for large-scale models or those enhanced by advanced reasoning techniques such as test-time scaling.