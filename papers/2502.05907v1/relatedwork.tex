\section{Related Works}
In this section, we review related works on embodied agents, continual reinforcement learning, and world model etc.

{\bf Embodied agents solving long-horizon tasks.} Long-Horizon (LH) tasks \cite{Guo2024CaStLCA, Chen2024ABF, Pirk2020ModelingLT} refer to complex, multi-step tasks. Existing work on embodied agents completing LH tasks can be divided into two categories. One is Model-Based Reinforcement Learning (MBRL) \cite{Mazzaglia2024MultimodalFW}. Embodied agents leverage MBRL to tackle LH tasks by interacting with environments and learning predictive world dynamics \cite{Liu2024ReLEPAN}. Such as GenRL \cite{Mazzaglia2024GenRLMW} proposes a multimodal-foundation world model (WM) that aligns vision-language representations with generative world dynamics for RL. The other is vision-language model-based (VLM) planning \cite{Roger2025RobinAS}. Embodied agents leverage VLMs to decompose LH tasks into hierarchical sub-goals \cite{Liu2024ReLEPAN}, dynamically refine plans via memory-augmented reasoning \cite{Song2024TowardsLV}, and align semantic intent with executable actions through iterative simulation \cite{Yang2024GuidingLT}, such as EmbodiedGPT \cite{EmbodiedGPT}, which generates sub-goals and bridges high-level planning with low-level control. However, they assume perfect knowledge of environments, rely on oracle feedback, and assume perfect execution of low-level policies, which make it hard to adapt various LH tasks across environments in open worlds \cite{Zhang2024VLABenchAL}.

{\bf Continual Reinforcement Learning (CRL).} Compared to traditional Reinforcement Learning (RL), which aims to identify a policy that maximizes long-term reward to find a solution, CRL \cite{Abel2023ADO} focuses on developing agents that never stop learning, treating learning as an endless adaptation process. Existing literature mainly focuses on supplementing neural network approaches with better tools \cite{tomilin2024coom}, such as designing new optimizers to update weights \cite{delfosse2024hackatari}, building new architectures \cite{zhang2024cppo}, using experience replay to prevent forgetting \cite{Caccia2022TaskAgnosticCR}, promoting plasticity explicitly \cite{Nikishin2022ThePB}, or using regularization techniques from continually supervised learning \cite{Javed2019MetaLearningRF}. Although it can alleviate knowledge forgetting for simple tasks, it lacks continual world knowledge updates across LH tasks and environments.

{\bf World Model (WM).} WMs are foundational blocks of AI systems to perform planning and reasoning~\cite{ha2018world}. They serve as simulators of real environments that predict the future outcome of certain actions, and policies can be derived from them. Current research focuses on two paradigms: understanding the world through latent state representations \cite{hansen2023td, Zhou2024RoboDreamerLC} and predicting future dynamics for planning and control \cite{Ma2024DoTW, Wang2024AD3IA}. Representative example usages of them in MBRL include action searching~\cite{schrittwieser2020mastering, nayakLLaMARLongHorizonPlanning2025}, policy optimization within such simulators~\cite{feinberg2018model, hafner2019dream}, or a combination of both~\cite{chitnis2023iql, hansen2023td}. However, world models currently struggle to prevent catastrophic forgetting \cite{Mattes2023HierosHI} due to their inability to maintain stable representations of previously learned environmental dynamics while adapting to new tasks, often exacerbated by shared parameter updates prior knowledge \cite{Sun2024LearningLD}.