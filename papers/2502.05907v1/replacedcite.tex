\section{Related Works}
In this section, we review related works on embodied agents, continual reinforcement learning, and world model etc.

{\bf Embodied agents solving long-horizon tasks.} Long-Horizon (LH) tasks ____ refer to complex, multi-step tasks. Existing work on embodied agents completing LH tasks can be divided into two categories. One is Model-Based Reinforcement Learning (MBRL) ____. Embodied agents leverage MBRL to tackle LH tasks by interacting with environments and learning predictive world dynamics ____. Such as GenRL ____ proposes a multimodal-foundation world model (WM) that aligns vision-language representations with generative world dynamics for RL. The other is vision-language model-based (VLM) planning ____. Embodied agents leverage VLMs to decompose LH tasks into hierarchical sub-goals ____, dynamically refine plans via memory-augmented reasoning ____, and align semantic intent with executable actions through iterative simulation ____, such as EmbodiedGPT ____, which generates sub-goals and bridges high-level planning with low-level control. However, they assume perfect knowledge of environments, rely on oracle feedback, and assume perfect execution of low-level policies, which make it hard to adapt various LH tasks across environments in open worlds ____.

{\bf Continual Reinforcement Learning (CRL).} Compared to traditional Reinforcement Learning (RL), which aims to identify a policy that maximizes long-term reward to find a solution, CRL ____ focuses on developing agents that never stop learning, treating learning as an endless adaptation process. Existing literature mainly focuses on supplementing neural network approaches with better tools ____, such as designing new optimizers to update weights ____, building new architectures ____, using experience replay to prevent forgetting ____, promoting plasticity explicitly ____, or using regularization techniques from continually supervised learning ____. Although it can alleviate knowledge forgetting for simple tasks, it lacks continual world knowledge updates across LH tasks and environments.

{\bf World Model (WM).} WMs are foundational blocks of AI systems to perform planning and reasoning____. They serve as simulators of real environments that predict the future outcome of certain actions, and policies can be derived from them. Current research focuses on two paradigms: understanding the world through latent state representations ____ and predicting future dynamics for planning and control ____. Representative example usages of them in MBRL include action searching____, policy optimization within such simulators____, or a combination of both____. However, world models currently struggle to prevent catastrophic forgetting ____ due to their inability to maintain stable representations of previously learned environmental dynamics while adapting to new tasks, often exacerbated by shared parameter updates prior knowledge ____.