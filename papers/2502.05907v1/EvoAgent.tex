%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bbding}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
%\usepackage{EvoAgent}
\usepackage[accepted]{EvoAgent}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{EvoAgent: Agent Autonomous Evolution with Continual World Model for Long-Horizon Tasks}

\begin{document}

\twocolumn[
\icmltitle{EvoAgent: Agent Autonomous Evolution \\ with Continual World Model for Long-Horizon Tasks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Tongtong Feng}{a}
\icmlauthor{Xin Wang}{a}
\icmlauthor{Zekai Zhou}{b}
\icmlauthor{Ren Wang}{a}
\icmlauthor{Yuwei Zhan}{a}
\icmlauthor{Guangyao Li}{a}
\icmlauthor{Qing Li}{c}
\icmlauthor{Wenwu Zhu}{a}
\end{icmlauthorlist}

\icmlaffiliation{a}{Department of Computer Science and Technology, BNRist, Tsinghua University, China}
\icmlaffiliation{b}{Department of Computer Science, University of Sydney, Sydney, Australia}
\icmlaffiliation{c}{Department of Electronic Engineering, Tsinghua University, Beijing, China}

\icmlcorrespondingauthor{Xin Wang}{xin{\_}wang@tsinghua.edu.cn}
\icmlcorrespondingauthor{Wenwu Zhu}{wwzhu@tsinghua.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention the equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
Completing Long-Horizon (LH) tasks in open-ended worlds is an important yet difficult problem for embodied agents. Existing approaches suffer from two key challenges: (1) they heavily rely on experiences obtained from human-created data or curricula, lacking the ability to continuously update multimodal experiences, and (2) they may encounter catastrophic forgetting issues when faced with new tasks, lacking the ability to continuously update world knowledge. To solve these challenges, this paper presents {\it EvoAgent}, an autonomous-evolving agent with a continual World Model (WM), which can autonomously complete various LH tasks across environments through self-planning, self-control, and self-reflection, without human intervention. Our proposed EvoAgent contains three modules, i.e., i) the memory-driven planner which uses an LLM along with the WM and interaction memory, to convert LH tasks into executable sub-tasks; ii) the WM-guided action controller which leverages WM to generate low-level actions and incorporates a self-verification mechanism to update multimodal experiences; iii) the experience-inspired reflector which implements a two-stage curriculum learning algorithm to select experiences for task-adaptive WM updates. Moreover, we develop a continual World Model for EvoAgent, which can continuously update the multimodal experience pool and world knowledge through closed-loop dynamics. We conducted extensive experiments on Minecraft, compared with existing methods, EvoAgent can achieve an average success rate improvement of 105{\%} and reduce ineffective actions by more than 6x.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Long-horizon (LH) tasks \cite{Guo2024CaStLCA, Chen2024ABF, Pirk2020ModelingLT} are complex, multi-step tasks that require sustained planning, sequential decision-making, and extended execution over a prolonged period to achieve a final goal. These tasks are challenging,  often exhibiting reward sparsity \cite{Yang2024GuidingLT} and procedural diversity \cite{EmbodiedGPT}. Completing LH tasks in open-ended worlds is an important yet difficult problem for embodied agents, such as logistics robots \cite{luo2025enhancing}, surgical robots \cite{marcus2024ideal}, and disaster rescue robots \cite{jadeja2024survivor} etc. 

On the one hand, existing agents have made remarkable progress by utilizing expert data and domain-specific curricula created by humans, developing policies through Reinforcement Learning (RL) \cite{Mazzaglia2024GenRLMW, Liu2024ReLEPAN}, imitation learning \cite{Liu2024ReLEPAN}, and Large Language Models (LLMs) \cite{Liu2024ReLEPAN, EmbodiedGPT}. %Nevertheless, the ability of these agents to complete LH tasks still falls short of human-level performance.

On the other hand, recent studies \cite{Liu2024ReLEPAN, wang2023voyager} demonstrate that humans' ability to accomplish LH tasks in an open world relies on continual memory storage, experience utilization, and world knowledge updates. In essence, continual world knowledge update \cite{U2UData} serves as a meta-cognitive driver that not only guides action selection under partial observability but also enables context-aware adaptation to environmental dynamics, thereby resolving the local optimality issue inherent in LH task completion. 

{\it Completing long-horizon tasks in open-ended worlds requires embodied agents to achieve continual world knowledge updates, like a baby thrives.}
\begin{figure*}[!t]
\centering
	\includegraphics[width=0.99\textwidth]{picture/1}
	\centering
    \vspace{-1mm}
	\caption{EvoAgent, the first autonomous-evolving agent with a continual World Model (WM). Take Minecraft as an example. {\bf Left:} Various Long-Horizon (LH) tasks across environments. {\bf Middle:} EvoAgent includes a memory-driven planner, a WM-guided action controller, and an experience-inspired reflector. EvoAgent can autonomously complete various LH tasks across environments by self-planning, self-control, and self-reflection, without human intervention. {\bf Right:} We build a continual WM for EvoAgent. Through closed-loop dynamics, EvoAgent can continuously update the multimodal experience pool and world knowledge.}
	\label{fig_1}
\vspace{-0.2cm}
\end{figure*}

%Inspired by this motivation, we summarize the challenges faced by 
Nevertheless, existing approaches suffer from two key challenges. {\it 1) Lack of continual multimodal experience.} Most embodied agents assume that all training data are available from the beginning (such as imitation learning or LLMs-based agents), which heavily rely on human-created data or curricula \cite{wang2023describe, li2024optimus1hybridmultimodalmemory}. However, this assumption is unrealistic, as agents may encounter novel tasks or environments after deployment \cite{Pirk2020ModelingLT, Zhang2024VLABenchAL}. {\it 2) Lack of continual world knowledge updates.} RL-based embodied agents can learn new actions and tasks by interacting with their environments. However, fine-tuned agents may face catastrophic forgetting, where they lose previously obtained knowledge \cite{nayakLLaMARLongHorizonPlanning2025, hafner2023dreamerv3} when learning new tasks. Continual RL incrementally updates agents for new tasks and evaluates their knowledge of current and past tasks, which is eligible only for simple tasks, lacking a comprehensive world cognition \cite{Abel2023ADO, delfosse2024hackatari} and making it difficult to abstract and transfer knowledge for LH tasks across environments.

To address these challenges, in this paper, we propose {\bf EvoAgent} (as is shown in Figure \ref{fig_1}), an autonomous-evolving agent with a continual World Model (WM), which can autonomously complete various LH tasks across various environments through self-planning, self-control, and self-reflection, without human intervention. Our proposed EvoAgent contains three modules: i) The memory-driven planner, which uses an LLM along with the WM and interaction memory, to incorporate self-state into the planning phase and convert LH tasks into executable sub-tasks; ii) The WM-guided action controller, which leverages WM to generate low-level actions and incorporates a self-verification mechanism to update multimodal experiences. iii) The experience-inspired reflector, which implements a two-stage curriculum learning algorithm to select experiences for task-adaptive WM updates. Moreover, we propose a novel continual World Model for EvoAgent as well. 
By utilizing a model-based online reinforcement learning setup and closed-loop dynamics, EvoAgent is able to continuously update the multimodal experience pool and world knowledge, filtering out invalid explorations and mitigating historical forgetting. 
The proposed continual World Model significantly contributes to improved autonomous planning and action control.

We evaluate EvoAgent's performance in Minecraft \cite{fan2022minedojo}, a popular open-world game environment. Extensive experiments demonstrate EvoAgent’s superiority: compared with existing methods, EvoAgent can achieve an average success rate improvement of 105{\%} and reduce ineffective actions by more than 6x. Ablation studies confirm that our Continual WM contributes 72{\%} of the performance gain by enabling coherent knowledge integration. The contributions of this paper are summarized as follows:
\begin{itemize}
	\item We construct EvoAgent, which can autonomously complete various LH tasks across various environments through self-planning, self-control, and self-reflection, without human intervention. To the best of our knowledge, the proposed EvoAgent is the first autonomous-evolving agent. 
	\item We build a novel continual world model for EvoAgent, which can continuously update the multimodal experience pool and world knowledge through closed-loop dynamics.
	\item We conduct extensive experiments on Minecraft to validate the superiority of EvoAgent, where the proposed EvoAgent can achieve an average success rate improvement of 105{\%} and reduce ineffective actions by more than 6x compared with existing methods.
\end{itemize}

\section{Related Works}
In this section, we review related works on embodied agents, continual reinforcement learning, and world model etc.

{\bf Embodied agents solving long-horizon tasks.} Long-Horizon (LH) tasks \cite{Guo2024CaStLCA, Chen2024ABF, Pirk2020ModelingLT} refer to complex, multi-step tasks. Existing work on embodied agents completing LH tasks can be divided into two categories. One is Model-Based Reinforcement Learning (MBRL) \cite{Mazzaglia2024MultimodalFW}. Embodied agents leverage MBRL to tackle LH tasks by interacting with environments and learning predictive world dynamics \cite{Liu2024ReLEPAN}. Such as GenRL \cite{Mazzaglia2024GenRLMW} proposes a multimodal-foundation world model (WM) that aligns vision-language representations with generative world dynamics for RL. The other is vision-language model-based (VLM) planning \cite{Roger2025RobinAS}. Embodied agents leverage VLMs to decompose LH tasks into hierarchical sub-goals \cite{Liu2024ReLEPAN}, dynamically refine plans via memory-augmented reasoning \cite{Song2024TowardsLV}, and align semantic intent with executable actions through iterative simulation \cite{Yang2024GuidingLT}, such as EmbodiedGPT \cite{EmbodiedGPT}, which generates sub-goals and bridges high-level planning with low-level control. However, they assume perfect knowledge of environments, rely on oracle feedback, and assume perfect execution of low-level policies, which make it hard to adapt various LH tasks across environments in open worlds \cite{Zhang2024VLABenchAL}.

{\bf Continual Reinforcement Learning (CRL).} Compared to traditional Reinforcement Learning (RL), which aims to identify a policy that maximizes long-term reward to find a solution, CRL \cite{Abel2023ADO} focuses on developing agents that never stop learning, treating learning as an endless adaptation process. Existing literature mainly focuses on supplementing neural network approaches with better tools \cite{tomilin2024coom}, such as designing new optimizers to update weights \cite{delfosse2024hackatari}, building new architectures \cite{zhang2024cppo}, using experience replay to prevent forgetting \cite{Caccia2022TaskAgnosticCR}, promoting plasticity explicitly \cite{Nikishin2022ThePB}, or using regularization techniques from continually supervised learning \cite{Javed2019MetaLearningRF}. Although it can alleviate knowledge forgetting for simple tasks, it lacks continual world knowledge updates across LH tasks and environments.

{\bf World Model (WM).} WMs are foundational blocks of AI systems to perform planning and reasoning~\cite{ha2018world}. They serve as simulators of real environments that predict the future outcome of certain actions, and policies can be derived from them. Current research focuses on two paradigms: understanding the world through latent state representations \cite{hansen2023td, Zhou2024RoboDreamerLC} and predicting future dynamics for planning and control \cite{Ma2024DoTW, Wang2024AD3IA}. Representative example usages of them in MBRL include action searching~\cite{schrittwieser2020mastering, nayakLLaMARLongHorizonPlanning2025}, policy optimization within such simulators~\cite{feinberg2018model, hafner2019dream}, or a combination of both~\cite{chitnis2023iql, hansen2023td}. However, world models currently struggle to prevent catastrophic forgetting \cite{Mattes2023HierosHI} due to their inability to maintain stable representations of previously learned environmental dynamics while adapting to new tasks, often exacerbated by shared parameter updates prior knowledge \cite{Sun2024LearningLD}.

\section{Preliminaries}
Before introducing our proposed EvoAgent in Section~\ref{sec:evoagent}, we briefly elaborate preliminaries on online model-based reinforcement learning and world model in this section.

{\bf Online Model-based Reinforcement Learning (MBRL).} RL is typically formulated as a Markov Decision Process (MDP) defined by the tuple $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $P(s'|s,a)$ is the transition dynamics, $R(s,a)$ is the reward function, and $\gamma \in [0,1)$ is the discount factor. The goal is to learn a policy $\pi(a|s)$ that maximizes the expected cumulative reward:
$J(\pi) = \mathbb{E}_{\pi, P}\left[\sum_{t=0}^\infty \gamma^t R(s_t, a_t)\right].$
In MBRL, the agent explicitly learns a model $\mathcal{M}$, which includes an approximate dynamics model $\hat{P}_\theta(s'|s,a)$ and a reward model $\hat{R}_\phi(s,a)$, parameterized by $\theta$ and $\phi$, respectively. These models are trained to minimize empirical prediction errors over observed transitions $\mathcal{D} = \{(s_i, a_i, s'_i, r_i)\}$: $\mathcal{L}_{\text{model}}(\theta, \phi) = \mathbb{E}_{(s,a,s',r)\sim\mathcal{D}}\left[\|s' - \hat{P}_\theta(s,a)\|^2 + \|r - \hat{R}_\phi(s,a)\|^2\right].$ Using the learned models, the agent performs planning to optimize its policy. For example, in value iteration, the state-value function $V(s)$ is iteratively updated via the Bellman equation: $V(s) \leftarrow \max_a [\hat{R}_\phi(s,a) + \gamma \mathbb{E}_{s'\sim\hat{P}_\theta(\cdot|s,a)} V(s')]$. In online MBRL, an agent interacts with the environment iteratively for $K$ rounds with the goal of learning a sequence to minimize $\mathcal{L}_{\text{model}}(\theta, \phi)$.

{\bf World Model.} Recurrent State-Space Model (RSSM) \cite{hafner2019planet, hafner2023dreamerv3} is a classic world model structure, which can predict latent states and rewards from high-dimensional observations. RSSM contains 6 modules. 1) Encoder, maps observation \( o_t \) to a stochastic latent state \( s_t = (h_t, z_t) \), where \( h_t \) is a deterministic RNN state and \( z_t \) is a stochastic latent variable, $q_\phi(z_t | h_t, o_t) = \mathcal{N}\big(z_t; \mu_\phi(h_t, o_t), \sigma_\phi(h_t, o_t)\big)$, where \( \mu_\phi, \sigma_\phi \) are neural networks. 2) Sequence model: predicts the sequence of these representations given past actions $a_{t-1}$, $h_t = f_\theta(h_{t-1}, z_{t-1}, a_{t-1})$. 3) Dynamics predictor, predicts the prior latent state transition, $p_\theta(\hat{z}_t | h_t) = \mathcal{N}\big(\hat{z}_t; \mu_\theta(h_t), \sigma_\theta(h_t)\big)$. 4) Decoder: reconstructs observations from latent states, $p_\theta(o_t | h_t, z_t) = \mathcal{N}\big(o_t; \mu_\theta^{\text{obs}}(h_t, z_t), \sigma_\theta^{\text{obs}}\big)$. 5) Reward predictor, predicts rewards, $\hat{r}_t = r_\theta(h_t, z_t)$. 6) Continual predictor, predicts episode continuation flags, $\hat{c}_t = \text{sigmoid}\big(c_\theta(h_t, z_t)\big)$. Above all, RSSM can be defined as:
\begin{align}
    & \text{Encoder:}    && z_t            &\ \sim &\ (z_t | h_t,o_t) \\
    & \text{Sequence model:}         && h_t            &\ =    &\ f_\theta(h_{t-1},z_{t-1},a_{t-1}) \\
    & \text{Dynamics predictor:}   && \hat{z}_t      &\ \sim &\ (\hat{z}_t | h_t) \\
    & \text{Decoder:}        && \hat{o}_t      &\ \sim &\ (\hat{o}_t | h_t,z_t)\\
    & \text{Reward predictor:}       && \hat{r}_t      &\ \sim &\ (\hat{r}_t | h_t,z_t) \\
    & \text{Continual predictor:}    && \hat{c}_t      &\ \sim &\ (\hat{c}_t | h_t,z_t)
\end{align}

\begin{figure*}[!t]
\centering
	\includegraphics[width=\textwidth]{picture/2}
	\centering
    \vspace{-6mm}
	\caption{EvoAgent includes three modules empowered by a continual WM. The memory-driven planner uses an LLM, along with the WM and interaction memory, to convert LH tasks into executable sub-tasks. The WM-guided action controller leverages WM to generate low-level actions and incorporates a self-verification mechanism to update multimodal experiences. The experience-inspired reflector implements a two-stage curriculum learning algorithm to select experiences for task-adaptive WM updates. The continual WM includes a multimodal experience pool and a WM. By closed-loop dynamics, EvoAgent can continuously update the multimodal experience pool and world knowledge.}
	\label{fig_2}
\end{figure*}

\section{EvoAgent}
\label{sec:evoagent}

{\bf Framework.} Let $\mathcal{E}$ denote a dynamic open-world environment with partial observability, $\mathcal{T}$ represent the long-horizon tasks, and $\mathcal{S}$ represents the agent's current state. We aim to design an autonomous-evolving agent {\it EvoAgent} that can complete various long-horizon tasks across environments, without human intervention. As shown in Figure \ref{fig_2}, EvoAgent includes a memory-driven planner $\Psi_{\text{plan}}$, a WM-guided action controller $\Pi_{\text{act}}$, an experience-inspired reflector $\Phi_{\text{reflect}}$, a multimodal experience pool $\mathcal{D}_{\text{MEP}}$, and a world model $\mathcal{M}_w$. EvoAgent can be instantiated as:
\begin{equation}
\text{\it EvoAgent}: \langle \Psi_{\text{plan}}, \Pi_{\text{act}}, \Phi_{\text{reflect}}, \mathcal{D}_{\text{MEP}}, \mathcal{M}_w \rangle
\end{equation}
{\bf Closed-loop sketch.} EvoAgent can evolve autonomously through closed-loop dynamic self-planning, self-control, and self-reflection, continuously learning general world representations and predicting future world states. The sketch of EvoAgent is as follows:
\begin{equation}
\begin{matrix}
    \mathcal{E}, \mathcal{T}, \mathcal{S}, \mathcal{D}_{\text{MEP}}, \mathcal{M}_w \\
    \overbrace{\underbrace{\text{\bf Planner}}_{\substack{\Psi_{\text{plan}} \triangleright \mathcal{D}_{\text{MEP}}^{all} \\ \downarrow \{g_i\}}} \rightarrow
    \underbrace{\text{\bf Controller}}_{\substack{\Pi_{\text{act}} \circ \mathcal{M}_w \\ \downarrow {\{a_t\}, \mathcal{D}_{\text{MEP}}}}} \rightarrow
    \underbrace{\text{\bf Reflector}}_{\substack{\Phi_{\text{reflect}} \triangleright \mathcal{D}_{\text{MEP}} \\ \downarrow \theta_{\mathcal{M}_w}'}}\rightarrow}
\end{matrix}
\end{equation}
\begin{itemize}
    \item $\{g_i\}$: Subtasks generated by $\Psi_{\text{plan}}$.
    \item $\{a_t\}$: Actions generated by the controller $\Pi_{\text{act}}$.
    \item $\theta_{\mathcal{M}_w}'$: Updated parameters of the world model $\mathcal{M}_w$.
\end{itemize}
Compared with multimodal experiences $\mathcal{D}_{\text{MEP}}$ that only include successful and failed ones, memory $\mathcal{D}_{\text{MEP}}^{all}$ contains the entire exploration record.

{\bf Evaluation.} According to relevant research \cite{hafner2023dreamerv3, Guo2024CaStLCA}, the agents' performance evaluation includes task completion ratio and exploration efficiency.
\begin{equation}
    QoE = \sum_{k=1}^{|\mathcal{T}|}\sum_{i=1}^{|\mathcal{G}|}[\mathbb{P}(g_i) + \alpha\frac{\mathcal{L}_{g_i}^{\text{suc}}}{|\mathcal{L}_{g_i}^{\text{all}}|}] 
\end{equation}
where $\mathbb{P}(g_i)$ indicates the probability of subtask $g_i$ completion, $L_{g_i}^{\text{suc}}$ indicates the success step length of subtask $g_i$, and $L_{g_i}^{\text{all}}$ indicates the total step length of subtask $g_i$ exploration.

Through autonomous closed-loop evolution, EvoAgent can mitigate historical forgetting and filter invalid exploration, contributing to better autonomous planning and action control. Next, we will introduce each module of EvoAgent in detail.

\subsection{Memory-Driven Planner}
The memory-driven planner $\Psi_{\text{plan}}$ is formalized as a function that maps the current multimodal state $\mathcal{S}$, long-horizon task $\mathcal{T}$, and memory $\mathcal{D}_{\text{MEP}}^{all}$ to a sequence of subtasks $\mathcal{G}$.
\begin{equation}
    \Psi_{\text{plan}}: \mathcal{S} \times \mathcal{T} \times \mathcal{D}_{\text{MEP}}^{all} \rightarrow \mathcal{G}
\end{equation}
\begin{equation}
    \mathcal{S} = \mathcal{O}_{\text{obs}} \times \mathcal{S}_{\text{self}} \times \mathcal{S}_{\text{assets}}
\end{equation}
where $\mathcal{G} = \{g_i\}_{i=1}^n$ is the subtask space, and each subtask $g_i$ satisfies $\bigcup_{i=1}^n g_i \supseteq \mathcal{T}$. Where $\mathcal{O}_{\text{obs}}$ represents first-person observations, $\mathcal{S}_{\text{self}}$ represents the agent’s internal state, such as health or hunger, and $\mathcal{S}_{\text{assets}}$ represents agent's asset library, such as tools or resources.

{\bf Subtask generation based on LLM.} As shown in Figure \ref{fig_2}, we adopt the image tokenizer $f_{v}$, e.g., VQ-GAN~\citep{esser2021taming}, to encode the raw images $\mathcal{O}_{obs}, \mathcal{S}_{\text{self}}, \mathcal{S}_{\text{assets}}$ into token embeddings $\mathcal{V}=\{v_1,v_2,...,v_n\} \in R^{n \times d}$, where $n$ denotes the number of visual tokens and $d$ is the dimensionality of each token. We adopt the textual tokenizer to encode $\mathcal{T}$ and $\mathcal{D}_{\text{MEP}}^{all}$ into token embeddings. We further utilize a lightweight projection module $f_l$ with a trainable projection matrix $W$. This module maps the visual tokens to the same space with text embeddings, yielding $\hat{\mathcal{V}}=\{\hat{v}_1,\hat{v}_2,...,\hat{v}_n\} \in R^{n \times \hat{d}}$:
\begin{equation}
\hat{\mathcal{V}} = W \mathcal{V}; \hspace{0.3em} \text{where} \hspace{0.3em}  \mathcal{V} = f_v(\mathcal{O}_{obs})
\end{equation}
The general LLM backbone $\Theta$ of our planner is built upon a decoder-only architecture with casual transformers. Our model employs an auto-regressive prediction mechanism, generating responses based on the provided multimodal input tokens. The resulting response is a mixed sequence of visual and textual tokens, represented as $Z=\{z_1,z_2,...,z_m\}$.
For each embedding $z_i$, we pass it through a linear layer $f_p$ followed by a softmax operation, mapping it into a probability distribution of the language vocabulary. The final subtask $g_i$ for the $i$-th token $z_i$ is determined by selecting the token from the existing language codebook with the highest score:
\begin{equation}
g_i = \text{argmax}( \text{softmax}(f_p(z_i)))
\end{equation}

When the WM-guided action controller feedback task fails, LLM will use LoRA \cite{hu2021lora} to make a finetune based on all memories.

\subsection{WM-Guided Action Controller}
The WM-guided action controller $\Pi_{\text{act}}$ is formalized as a function that maps the current multimodal state $\mathcal{S}$, subtask $\mathcal{G}$, and the world model $\mathcal{M}_w$ to an action sequence $a_{t:t+H} = \{a_t, a_{t+1}, \dots, a_{t+H}\}$ for horizon $H$.
\begin{equation}
\Pi_{\text{act}}: \mathcal{S} \times \mathcal{G} \times \mathcal{M}_w \rightarrow \mathcal{A}
\end{equation}

{\bf Action selection.} The controller utilizes $\mathcal{M}_w$ to predict future states and optimize actions:
\begin{equation}
    a_{t:t+H} = \underset{a_{t:t+H} \in \mathcal{A}^H}{\arg\max} \mathbb{E}_{\mathcal{M}_w} \left[ \sum_{\tau=t}^{t+H} \gamma^{\tau-t} R(s_\tau, a_\tau, g_i) \right]
\end{equation}
where $R(s_\tau, a_\tau, g_i)$ is the goal-aligned reward function, and $\gamma \in [0,1]$ is the discount factor. Then use $a_t$ for action control.

{\bf Self-verification.} After executing $a_t$, the agent interacts with $\mathcal{E}$ to collect environment feedback. Then it uses a self-verification mechanism to determine whether the subtask $g_i$ can be terminated.

Let $\phi_{\text{verify}}: \mathcal{S} \times \mathcal{G} \times \mathbb{T} \rightarrow \{\text{Terminal},\text{N-Terminal}\}$ denote the self-verification module, where:
\begin{equation}
    \phi_{\text{verify}}(s_t, g_i, t) = \begin{cases}
        \text{Terminal} & \text{if } g_i \text{ completed } \lor t \geq T_{\text{max}} \\
        \text{N-terminal} & \text{otherwise}
    \end{cases}
\end{equation}

where $T_{\text{max}}$ is the maximum allowed steps. When a subtask $g_i$ is completed or the subtask $g_i$ completion cycle exceeds the maximum step length $T_{\text{max}}$, the subtask $g_i$ is terminated and the memory-driven planner is performed again. The self-verification module can determine whether the task is completed according to the task rules. For example, the instructions in the Minecraft \cite{fan2022minedojo} game can be used to determine whether the subtask is completed.

{\bf Multimodal Experience Pool (MEP).}
\label{MEP}
The $\mathcal{D}_{\text{MEP}}$ is formalized as a dynamic repository storing agent-environment interactions and can be defined as follows:
\begin{equation}
h = \langle (s_t, a_t, r_t, s_{t+1}), \mathbb{P}_{(g_i)} | g_i \rangle
\end{equation}
where $h \in \mathcal{D}_{\text{MEP}}$ represents the experience; $s_t \in \mathcal{S}$ represents multimodal state at step $t$ (including observation $\mathcal{O}_obs$, self-state $\mathcal{S}_{\text{self}}$, and assets $\mathcal{S}_{\text{assets}}$); $g_i \in \mathcal{G}$ represents the current subtask from $\Psi_{\text{plan}}$; $a_t \in \mathcal{A}$ represents the action generated by $\Pi_{\text{act}}$; $\mathbb{P}_{(g_i)} \in [0,1]$ reprents the probability of subtask $g_i$ completion.

{\bf MEP Updating.} If the subtask $g_i$ is not terminated, $\left\{ \langle s_t, a_t, r_t, s_{t+1}, \mathbb{P}_{(g_i)} | g_i \rangle \right\}_{t=0}^\tau$ will be added to the memory $\mathcal{D}_{\text{MEP}}^{all}$. If the subtask $g_i$ is terminated, whether it is successful or exceeds the step threshold, $\left\{ \langle s_t, a_t, r_t, s_{t+1}, \mathbb{P}_{(g_i)} | g_i \rangle \right\}_{t=0}^\tau$ is added to the multimodal experience pool $\mathcal{D}_{\text{MEP}}$.
The MEP update process is formalized as follows. $if \quad \phi_{\text{verify}} \text{==  Terminal}$:
\begin{equation}
    \mathcal{D}_{\text{MEP}} \leftarrow \mathcal{D}_{\text{MEP}} \cup \left\{ \langle s_t, a_t, r_t, s_{t+1}, 1 | g_i \rangle \right\}_{t=0}^\tau
\end{equation}
\begin{equation}
    \tau = \min(t \mid \phi_{\text{verify}}(s_t, g_i, t)\text{== Terminal})
\end{equation}
Else, adding to $\mathcal{D}_{\text{MEP}}^{all}$. New experiences are continuously added to the multimodal experience pool.

\subsection{Experience-inspired Reflector}
The experience-inspired reflector $\Phi_{\text{reflect}}$ is formalized as a function that maps the current multimodal state $\mathcal{S}$, subtask $\mathcal{G}$, and the multimodal experience $\mathcal{D}_{\text{MEP}}$ to update the world model from $\mathcal{M}_w$ to $\mathcal{M}'_w$.
\begin{equation}
\Phi_{\text{reflect}}: \mathcal{S} \times \mathcal{G} \times \mathcal{D}_{\text{MEP}} \times \mathcal{M}_w \rightarrow \mathcal{M}'_w
\end{equation}

$\Phi_{\text{reflect}}$ employs a two-stage curriculum learning mechanism to optimize experience selection. Then $\Phi_{\text{reflect}}$ update the world model to mitigate historical forgetting.

\subsection*{A. Two-stage curriculum learning mechanism}
{\bf Phase 1: curriculum subtask selection.} For candidate subtasks $g_i \in \mathcal{G}$, we use four indicators for curriculum subtask selection: (1) the relevance of the subtask $g_i$ to the current target task $\mathcal{T}_{goal}$; (2) the efficiency ratio of the subtask $g_i$ (ratio of successful step length $L_{g_i}^{\text{suc}}$ to total step length $L_{g_i}^{\text{all}}$); (3) the importance of the subtask $g_i$ (comparing its impact on the current world model $\mathcal{M}_{w,g_i}^{\text{new}}$ and past world model $\mathcal{M}_{w,g_i}^{\text{old}}$); (4) the completion ratio of the subtask $\mathbb{P}_{(g_i)}$. 

\begin{algorithm}[t]
%\small
\caption{Continual World Model via Closed-Loop Planning-Control-Reflection}
\label{alg:evoagent}
\begin{algorithmic}[1]
\REQUIRE Environment $\mathcal{E}$, Task $\mathcal{T}$, initial MEP $\mathcal{D}_{\text{MEP}}^0$, 
\STATE \quad\quad\quad World model $\mathcal{M}_w^0$, Horizon $H$, Max steps $T_{\text{max}}$
\ENSURE Optimized $\mathcal{D}_{\text{MEP}}^*$, $\mathcal{M}_w^*$

\STATE Initialize state $\mathcal{S}_0 \gets (\mathcal{O}_{\text{obs}}^0, \mathcal{S}_{\text{self}}^0, \mathcal{S}_{\text{assets}}^0)$
\STATE Initialize memory $\mathcal{D}_{\text{MEP}}^{\text{all}} = \mathcal{D}_{\text{MEP}}^0$

\FOR{Task $\mathcal{T}=\mathcal{T}_0$ to $\mathcal{T}_n$}
    \STATE \textbf{Memory-Driven Planning}
        \STATE $\{g_i\} \gets \Psi_{\text{plan}}(\mathcal{S}_t, \mathcal{T}, \mathcal{D}_{\text{MEP}}^{\text{all}})$ \COMMENT{Subtask sequence generation  via Eq.(12-13)}
    \FOR{each subtask $g_i \in \{ g_i \}$ }
        \FOR{episode $t=1$ to $T_{\text{max}}$}
            \STATE \textbf{WM-Guided Action Control}
            \STATE $\{a_t\} \gets \Pi_{\text{act}}(\mathcal{S}_t, g_i, \mathcal{M}_w)$ \COMMENT{Action selection via Eq.(14-15)}
            \STATE $\mathcal{S}_{t+1} \gets (\mathcal{O}_{\text{obs}}^{t+1}, \mathcal{S}_{\text{self}}^{t+1}, \mathcal{S}_{\text{assets}}^{t+1})$ \COMMENT{State transition}
            \STATE \textbf{MEP Updating}
            \STATE $h=\{(s_t,a_t,r_t,s_{t+1},1|g_i)\}$ \COMMENT{Experience definition via Eq.(16-19)}
            \IF{$\phi_{\text{verify}}(s_t, g_i, t) = \text{Terminal}$}
            \STATE $\mathcal{D}_{\text{MEP}} \gets \mathcal{D}_{\text{MEP}} \cup h$
            \STATE BREAK subtask loop
            \ELSE
            \STATE $\mathcal{D}_{\text{MEP}}^{\text{all}} \gets \mathcal{D}_{\text{MEP}}^{\text{all}} \cup h$
            \ENDIF
        \ENDFOR
        \STATE \textbf{Experience-Inspired Reflection}
        \STATE $\mathcal{D}_k^{\text{subtask}} \gets \text{CurriculumSubtaskSelect}(\mathcal{G}_t, \mathcal{T}, \mathcal{D}_{\text{MEP}})$ 
        \STATE $\mathcal{D}_k^{\text{exp}} \gets \text{CurriculumExperienceSelect}(\mathcal{D}_k^{\text{subtask}})$ \COMMENT{Two-stage curriculum selection via Eq.(21-24)}
        \STATE \textbf{World Model Updating}
        \STATE $\mathcal{M}_w^{'} \gets \Phi_{\text{reflect}}
        (\mathcal{D}_k^{\text{exp}}, \mathcal{M}_w)$ \COMMENT{World Model updating via Eq.(25-27)}
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{table*}[t]
\centering
\small
\caption{Main Result of EvoAgent on long-horizon tasks benchmark. We report the average success rate (SR) and average exploration efficiency (EE) on each task group. Upper EE metrics mean that the agent is more efficient at completing the task with fewer invalid exploration samples, while $0.00$ indicates that the agent is unable to complete the task. The Overall represents the average result on the three groups of Iron, Gold, and Diamond. The Improving represents the average performance improvement of EvoAgent compared to the algorithms Jarvis-1, Dreamerv3, and Optimis-1.}
\label{table1}
\resizebox{0.8\textwidth}{!}{%
\renewcommand\arraystretch{1.3}
\begin{tabular}{ccccccccc}
\toprule[1.3pt]
Group & Metric       & PPO   & GPT-4V & Jarvis-1 & Dreamerv3 & Optimus-1 & EvoAgent & Improving ({\%})\\
\midrule[1pt]
\multirow{2}{*}{\includegraphics[width=0.4cm]{picture/wood.pdf} Wood}         & SR$\uparrow$ & 28.16 & 35.24  & 89.73    & 91.07     & 96.39     & {\bf 97.47}  & 5.49  \\
 & EE$\uparrow$ & 53.82 & 69.45  & 87.36    & 93.22     & 97.82     & {\bf 98.43} & 6.07 \\ \hline
\multirow{2}{*}{\includegraphics[width=0.4cm]{picture/cobblestone.pdf} Stone} & SR$\uparrow$ & 13.42 & 14.39  & 81.91    & 86.82     & 88.79     & {\bf 94.53}  & 10.12  \\
& EE$\uparrow$ & 27.56 & 30.64  & 84.72    & 88.39     & 89.25     & {\bf 96.48} & 10.32  \\ \hline
\multirow{2}{*}{\includegraphics[width=0.4cm]{picture/iron_ingot.pdf} Iron}  & SR$\uparrow$ & 0.00  & 0.00   & 42.38    & 33.79     & 45.48     & {\bf 51.82}  &  27.79  \\
& EE$\uparrow$ & 0.00  & 0.00   & 47.52    & 35.68     & 46.16     & {\bf 58.54} & 35.76   \\ \hline
\multirow{2}{*}{\includegraphics[width=0.4cm]{picture/gold_ingot.pdf} Gold}  & SR$\uparrow$ & 0.00  & 0.00   & 8.84     & 6.57      & 10.62      & {\bf 21.69}    & {\bf {\color{red} 149.98}}\\
& EE$\uparrow$ & 0.00  & 0.00   & 9.76     & 8.05      & 8.03      & {\bf 30.48} &253.87    \\ \hline
\multirow{2}{*}{\includegraphics[width=0.4cm]{picture/diamond.pdf} Diamond}   & SR$\uparrow$ & 0.00  & 0.00   & 7.69     & 4.73      & 9.30      & {\bf 17.36}    & 139.77\\
& EE$\uparrow$ & 0.00  & 0.00   & 0.07     & 3.69      & 7.31      & {\bf 26.83}    & {\bf {\color{red} 627.10}}\\ \hline
Overall & SR$\uparrow$ & 0.00  & 0.00   & 19.64    & 15.03     & 21.80     & {\bf 30.29} & {\bf {\color{red} 105.85}}  \\
\bottomrule[1.3pt]
\end{tabular}
}
\end{table*}

Therefore, curriculum subtask $g_i$ priority score $\rho(g_i)$ for experience $h = \langle (s_t, a_t, r_t, s_{t+1}), \mathbb{P}_{(g_i)} | g_i \rangle$ can be defined as follows:
\begin{equation}
\begin{aligned}
    \tau(g_i) = &\underbrace{\lambda_1 \cdot \cos(\mathbf{Emb}_{g_i}, \mathbf{Emb}_{\mathcal{T}_{goal}})}_{\text{Relevance}} + \underbrace{\lambda_2 \cdot \frac{L_{g_i}^{\text{suc}}}{L_{g_i}^{\text{all}}}}_{\text{Efficiency}}\\& + \underbrace{\lambda_3 \cdot \text{KL}\left(\mathcal{M}_{w,g_i}^{\text{old}} \| \mathcal{M}_{w,g_i}^{\text{new}}\right)}_{\text{Importance}} + \underbrace{\lambda_4 \cdot \mathbb{P}_{(g_i)}}_{\text{Completion ratio}}
    \label{eq:task_score}
\end{aligned}
\end{equation}
where $\cos(\mathbf{Emb}_{g_i}, \mathbf{Emb}_{\mathcal{T}_{goal}})$ represents task embedding similarity. $\lambda_1+\lambda_2+\lambda_3 +\lambda_4=1$ are balancing coefficients. 

Finally, in round $k$, $|\mathcal{D}_k^{\text{subtask}}|$ subtasks are selected. 
\begin{equation}
    \mathcal{D}_k^{\text{subtask}} = \{g_i | \tau(h_i) \geq \rho_k \}, \quad 
    \rho_k = \rho_0 \cdot e^{-c_s k}
\end{equation}
with $c_s$ controlling curriculum subtask progression rate.

{\bf Phase 2: curriculum experience selection.} For candidate experience $h \in \mathcal{D}_{MEP}$ in selected subtasks $\mathcal{D}_k^{\text{subtask}}$, we use three indicators for curriculum experience selection: (1) the Temporal Difference Error (TD-Error) $\delta_{\text{TD}}(h_j)$, prioritizes experience with high TD-Error, indicating prediction mismatch between current and target world models; (2) the Gradient Norm $\| \nabla_{\mathcal{M}_w} \mathcal{L}_{\text{pred}}(h_j) \|$, favors experiences that maximally influence the world model's parameter updates; (3) the Information Gain, measures how much the experience $h_j$ changes the world model's belief distribution, calculated via KL divergence between current $\mathcal{M}_w^{\text{new}}(s_{j+1}|h_j)$ and previous $\mathcal{M}_w^{\text{old}}(s_{j+1}|h_j)$ world model predictions.
\begin{equation}
\begin{aligned}
    \epsilon(h_j) = &\eta_1 \cdot \underbrace{|\delta_{\text{TD}}(h_j)|}_{\text{TD-Error}} + \eta_2 \cdot \underbrace{\|\nabla_{\mathcal{M}_w} \mathcal{L}_{\text{pred}}(h_j)\|_2}_{\text{Gradient Norm}} \\ & +\underbrace{\eta_3 \cdot \text{KL}\left(\mathcal{M}_w^{\text{new}}(s_{j+1}|h_j) \| \mathcal{M}_w^{\text{old}}(s_{j+1}|h_j)\right)}_{\text{Information Gain}}
\end{aligned}
\end{equation}
where $\eta_1+\eta_2+\eta_3=1$ are balancing coefficients. 

Finally, in round $k$, $|\mathcal{D}_k^{\text{exp}}|$ experiences are selected. 
\begin{equation}
    \mathcal{D}_k^{\text{exp}} = \{h_j | \epsilon(h_j) \geq \rho_k \}, \quad 
    \rho_k = \rho_0 \cdot e^{-c_h k}
\end{equation}
with $c_h$ controlling curriculum experience progression rate.

\subsection*{B. World Model Updating}
Update the world model $\mathcal{M}_w$ using curriculum-guided experiences $\mathcal{D}_k^{\text{exp}}$ with importance-aware weight $w_j$:
\begin{equation}
    \theta'_{\mathcal{M}_w} \leftarrow \theta_{\mathcal{M}_w} - \nabla \Big[ \underbrace{\sum_{h_j} w_j \mathcal{L}_{\text{pred}}(h_j)}_{\text{Curriculum Loss}} + \underbrace{\mu \cdot \Omega(\theta, \theta^{\text{old}})}_{\text{Regularization}} \Big]
\end{equation}
\begin{equation}
    w_j = \frac{\epsilon(h_j)}{\max_k \epsilon(h_k)}
    \label{eq:update_rule}
\end{equation}
\begin{equation}
    \Omega = \sum_i \mathcal{F}_i (\theta_i - \theta_i^{\text{old}})^2
    \label{eq:update_rule}
\end{equation}
where $w_j$ to emphasize critical experiences, and $\Omega$ to penalize shifts in parameters critical for past tasks. $\mathcal{F}_i$ is the Fisher information matrix diagonal.

\subsection{Continual World Model}
The continual world model includes a multimodal experience pool and a world model. As shown in Algorithm \ref{alg:evoagent}, by closed-loop dynamic self-planning, self-control, and self-reflection, EvoAgent can continuously update the multimodal experience pool and world knowledge, filtering invalid exploration and mitigating historical forgetting, contributing to better autonomous planning and action control.

\section{Experiments}

\subsection{Experiments Setting}
{\bf Minecraft}. We use Minecraft \cite{fan2022minedojo} to evaluate our EvoAgent framework. With 100M monthly active users, Minecraft is one of the most popular video
games worldwide. Minecraft features a procedurally generated 3D world of different biomes, which consists of 1-meter-sized blocks that the player and break and place. There are about 30 different creatures that the player can interact with or fight. From gathered resources, the player can use over 350 recipes to craft new items and progress through the technology tree, all while ensuring safety and food supply to survive. We employ MineRL 0.4.4 with Minecraft as our simulation environment. The agent operates at a fixed speed of 20 frames per second and only interacts with the environment via low-level action control signals of the mouse and keyboard.

{\bf Benchmark}. We use Optimus-1 \cite{li2024optimus1hybridmultimodalmemory} benchmark to evaluate our Evo-agent framework. Optimus-1 constructed a benchmark of 67 tasks to evaluate the Agent’s ability to complete long-horizon tasks. Then divide the 67 Minecraft tasks into 7 groups according to recommended categories in Minecraft.

{\bf Baselines.} We compare EvoAgent with various agents, including PPO \cite{schulman2017proximalpolicyoptimizationalgorithms}, GPT-4V, Jarvis-1 \cite{wang2023jarvis1openworldmultitaskagents}, Dreamerv3 \cite{hafner2023dreamerv3}, and Optimus-1 \cite{li2024optimus1hybridmultimodalmemory} on the challenging long-horizon tasks cross-environments benchmark. Note that we initialize all agent with an empty multimodal experience pool, while PPO \cite{schulman2017proximalpolicyoptimizationalgorithms} and Jarvis-1 \cite{wang2023jarvis1openworldmultitaskagents} have tools in their initial state. This makes it more challenging for EvoAgent to perform the same tasks. This paper constructs an autonomous evolution agent with a continual world model through self-planning, self-control, and self-reflection, so this paper does not consider agents that are completely based on human data and curricula support, such as voyager \cite{wang2023voyager}, DEPS \cite{wang2023describe}, Steve-Eye \cite{zheng2023steve}, and Plan4MC \cite{baai2023plan4mc}.

\subsection{Experimental Results}
    As shown in Table \ref{table1}, EvoAgent achieves state-of-the-art success rates (SR) and exploration efficiency (EE) across all resource tiers. Compared with existing methods, EvoAgent can achieve an average success rate improvement of 105{\%} and reduce ineffective actions by more than 6x. For basic tasks (Wood/Stone), EvoAgent marginally outperforms Optimus-1 (97.47{\%} vs. 96.39{\%} SR on Wood) but exhibits significantly greater advantages in advanced tasks like Gold (21.69{\%} vs. 10.62{\%} SR) and Diamond (17.36{\%} vs. 9.30{\%} SR). This hierarchy-aligned improvement suggests EvoAgent’s closed-loop planning-control-reflection mechanism effectively addresses long-horizon dependencies, where traditional model-based methods (DreamerV3) and LLM-driven agents (Jarvis-1) struggle to maintain coherent multi-stage strategies. Notably, the EE metric reveals EvoAgent’s exploration superiority: its 30.48{\%} EE on Gold tasks is 3.8× higher than Optimus-1, indicating drastically reduced invalid actions during deep resource acquisition.

Model-free methods (PPO) and pure vision-language models (GPT-4V) fail completely (0{\%} SR/EE) on tasks requiring tool hierarchies (Iron+), highlighting their inability to model latent state transitions. While Jarvis-1 and DreamerV3 achieve partial success on intermediate tasks (42.38{\%} SR on Iron), their performance collapses on Gold/Diamond tiers due to compounding errors in action sequences. The 26.83{\%} EE for EvoAgent on Diamond tasks—7.3× higher than Optimus-1—underscores how curriculum-guided experience selection mitigates exploration bottlenecks in sparse-reward scenarios. This aligns with our hypothesis that conventional prioritization heuristics inadequately handle exponentially growing action spaces in long-horizon tasks.

\subsection{Ablation Study}

\begin{table}[t]
\centering
\caption{Ablation study results. We report the average success rate (SR) on each task group. \texttt{P.}, \texttt{C.}, \texttt{R.}, and \texttt{CWM}, represent Planning, Control, Reflection, and Continual World Model, respectively. The PPO algorithm is initially used by default for model decision-making.}
\resizebox{0.5\textwidth}{!}{%
\renewcommand\arraystretch{1.3}
\begin{tabular}{cccc|ccccc}
\toprule[1.2pt]
\multicolumn{4}{c|}{Setting}                                                                                   & \multicolumn{5}{c}{Task}                 \\
P.                        & C.                        & R.                        & CWM                       & Wood  & Stone  & Iron  & Gold  & Diamond \\ \midrule[1pt]
                          &                           &                           &                           & 28.16 & 13.42  & 0.00  & 0.00  & 0.00    \\
\Checkmark &                           &                           &                           & 45.69 & 18.366 & 0.00  & 0.00  & 0.00    \\
\Checkmark & \Checkmark &                           &                           & 92.42 & 85.31  & 31.59 & 5.47  & 3.52    \\
\Checkmark & \Checkmark & \Checkmark &                           & 96.69 & 93.82  & 42.61 & 17.53 & 10.09    \\
\Checkmark & \Checkmark & \Checkmark & \Checkmark & 97.47 & 94.53  & 51.82 & 21.69 & 17.36  \\
\bottomrule[1.2pt]
\end{tabular}
}
\label{tb:ab_1}
\end{table}

As shown in Table \ref{tb:ab_1}, the ablation study reveals critical insights into the contributions of individual components (Planning, Control, Reflection) and Continual WM to hierarchical resource collection tasks. When only PPO is used without any modules (first row), the agent fails to progress beyond basic tasks (28.16{\%} SR for Wood, 0{\%} for Iron+), highlighting PPO’s limitations in long-horizon planning. Introducing the Planning module (second row) nearly doubles performance on Wood (45.69{\%}) and marginally improves Stone (18.37{\%}), but still fails to unlock advanced tasks (Iron+ at 0{\%}), suggesting that planning alone cannot resolve the exploration bottleneck in resource hierarchies.

A pivotal leap occurs when Control is added (third row: P+C), with Wood and Stone success rates surging to 92.42{\%} and 85.31{\%}, respectively, and modest progress in Iron (31.59{\%}). This underscores the necessity of structured exploration to navigate intermediate dependencies (e.g., acquiring Stone tools to mine Iron). However, the sharp decline in Gold (5.47{\%}) and Diamond (3.52{\%}) indicates persistent challenges in sparse reward scenarios. Integrating the Reflection module (fourth row: P+C+R) achieves near-perfect Wood/Stone success (96.69{\%}/93.82{\%}) and significantly boosts Iron (42.61{\%}), Gold (12.53{\%}), and Diamond (10.09{\%}), demonstrating its role in distilling exploration experiences to refine world models and credit assignment. The results emphasize a compounding effect: while Planning establishes goal-directed behavior, Control enables tool acquisition, and Reflection mitigates cascading failures in multi-step tasks.

The full system (Continual WM) closes this gap, boosting Diamond success to 17.36{\%}—a 72{\%} improvement over the prior configuration. The continual world model’s ability to encode tool dependencies (e.g., Iron→Diamond) and adapt exploration policies accounts for this leap. Crucially, the near-perfect Wood/Stone success (97.47{\%}/94.53{\%}) confirms that lower-level task mastery stabilizes as the model evolves, validating the closed-loop architecture’s hierarchical learning efficacy.

\section{Conclusion}
This paper presents EvoAgent, an autonomous-evolving agent with a continual World Model (WM) that can autonomously complete long-horizon tasks without human intervention through self-planning, self-control, and self-reflection. As shown in Table \ref{table1}, despite EvoAgent’s 30.29{\%} overall SR (38.9{\%} relative gain over Optimus-1), absolute success rates remain low for high-tier tasks (17.3{\%} on Diamond), suggesting unresolved challenges in environment stochasticity (e.g., lava encounters during mining). The EE-SR correlation weakens in complex tasks (Diamond EE=26.83{\%} vs. SR=17.36{\%}), implying that even efficient explorers face irreducible failure modes from partial observability. These results motivate future work on dynamic risk-aware world models and meta-reasoning about irreversible actions. Hopefully deployed in the real world.

\bibliography{EvoAgent}
\bibliographystyle{EvoAgent}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
%\section{You \emph{can} have an appendix here.}

\end{document} 