
% 我们首先对本文依赖的两个重要模块 LLM(Large Language Models)和Text-to-SQL任务进行介绍
We first introduce the two important modules this paper relies on: Large Language Models (LLMs) and the Text-to-SQL task.
\subsection{Large Language Models} 

% 预训练大语言模型通过在大规模文本数据上进行无监督学习，掌握了丰富的语言知识和语义信息。这种预训练通常使用自回归（如GPT\cite{gpt}系列）或自编码（如BERT\cite{bert}系列）结构。预训练后的模型可以在多个下游任务中通过微调（fine-tuning）实现优异的性能。自深度学习进入LLMs时代以来，Text-to-SQL任务上相比于经典的模型和方法取得了飞跃性的进步，当前SOTA级别的工作全部依赖于LLMs的表现。
Pre-trained large language models have acquired extensive language knowledge and semantic information through unsupervised learning on large-scale textual data. This pre-training typically employs autoregressive architectures (such as GPT-like models) or autoencoding structures (such as BERT-like models). The LLMs can achieve excellent performance on various downstream tasks through fine-tuning. Since the advent of the deep learning era for large language models, there have been significant advancements in Text-to-SQL tasks compared to classical models and methods, with current SOTA-level work fully relying on the performance of LLMs.

\subsection{Text-to-SQL}
The Text-to-SQL task can be defined as: a task that translates a NLQ $\mathit{Q}$ into an SQL query $\mathit{Y}$ based on database information $\mathit{S}$, a specific model $F(\cdot | \theta)$, and a particular prompt $\mathit{P}$. The information in the database is defined as $\mathit{S}$. Therefore, the Text-to-SQL task can be defined as: 
\begin{equation}
\mathit{Y}=F(\mathit{Q},\mathit{S},\mathit{P} | \theta),
\end{equation}

% 在当前的SOTA级别Text-to-SQL任务中，多Agent合作已成为优化性能的关键策略。常见的模块包括值检索、模式连接、少样本驱动、链式推理、SQL修正及自一致性等。受到这些方法的启发，本文首先在对这些模块进行系统化整理。我们将现有方法分为预处理、提取（Extraction）、生成（Generation）及修正（Refine）四个阶段。其中预处理阶段与具体的Query无关，目标是得到清晰的数据库结构信息和准备其他能够帮助模型生成SQL的辅助信息；Extraction阶段，对具体的query进行分析，并从准备的信息中过滤出高质量的辅助信息；Generation阶段将Extraction得到的信息进行组织并生成候选的SQL；Refine阶段对候选SQL进行进一步优化，提升SQL对质量。

% 在这个基础上，我们提出了OpenSearch-SQL，一种基于动态Fewshot和一致性对齐机制的Text-to-SQL框架。其中动态Fewshot用来强化任务各个阶段模块的性能，而Alignment模块用来将Text-to-SQL的各个阶段进行对齐，减少LLMs幻觉在多Agent协作工作流中的累积。

% OpenSearch-SQL的设计简化了复杂的指令体系，着重于通过对齐提升多Agent协作的效果。在随后的章节中，我们将详细介绍OpenSearch-SQL框架的具体组成。

In the current state-of-the-art Text-to-SQL tasks, multi-agent collaboration has become a key strategy for enhancing performance. Common modules include value retrieval, schema linking, few-shot driving, CoT prompting, SQL correction, and self-consistency. Inspired by these methods, this paper first systematically organizes these modules. We categorize existing methods into four stages: \textbf{Preprocessing}, \textbf{Extraction},\textbf{ Generation}, and \textbf{Refinement}. The \textbf{preprocessing} is independent of a specific NLQ and aims to obtain clear information about the database structure and prepare other auxiliary information that aids the model in generating SQL; the \textbf{Extraction} analyzes specific queries and filters out high-quality auxiliary information from the prepared information; the\textbf{ Generation} organizes the extracted information and generates candidate SQLs; and the refinement stage further optimizes the candidate SQLs to enhance the quality of SQLs.

Based on this foundation, we propose OpenSearch-SQL, a Text-to-SQL framework based on \textbf{dynamic few-shot} and \textbf{consistency alignment} mechanisms. The design of dynamic few-shot aims to strengthen the performance of each module across the various stages of the task, while the \textbf{Alignment} module is used to align the different stages of Text-to-SQL, thereby reducing the hallucination produced by LLMs in multi-agent collaboration workflows. The design of OpenSearch-SQL simplifies the complex instruction system, focusing on improving multi-agent collaboration effectiveness through alignment. In the following sections, we will provide a detailed introduction to the specific components of the OpenSearch-SQL framework.


\subsection{Hallucination}
% 幻觉\cite{hallucination}是一个深度学习中常见的问题，通常指的是模型生成的内容与现实或预期结果不符，即模型产生了不准确或不合理的信息。文本幻觉可以表现为以下几种形式：
Hallucination \cite{hallucination} is a common problem in deep learning, usually referring to situations where the content generated by a model does not align with reality or expected outcomes, i.e., the model produces inaccurate or unreasonable information. Textual hallucinations can manifest in the following forms:
% 不相关内容：模型生成的文本与输入或上下文无关，或者包含了不相关的细节。

% 错误事实：模型生成的文本中包含了错误的事实或信息，这些信息在现实世界中是不存在的或不准确的。

% 逻辑错误：生成的文本在逻辑上不连贯或不合理。
\begin{itemize}
    \item Irrelevant content: The text generated by the model is unrelated to the input or context, or it includes irrelevant details.
    \item Incorrect facts: The generated text contains erroneous facts or information that do not exist or are inaccurate in the real world.
    \item Logical errors: The generated text is logically inconsistent or unreasonable.
\end{itemize}

% 在Text-to-SQL中表现为：结果中存在不存在的数据库信息，不遵循Prompt里指令，以及出现Typo错误等。同时为了处理方便，在Text-to-SQL任务中我们把模型在非0温度下随机性导致的结果偏差以及微小Prompt改变导致的结果误差也包括在呢。
In Text-to-SQL, this manifests as: results containing non-existent database information, failure to follow instructions in the prompt, and typographical and syntactic errors. Additionally, for convenience in handling, in the Text-to-SQL task, we also include the result deviations caused by randomness at non-zero temperatures and errors due to minor prompt changes.
% 由于幻觉的本质是训练过程与实际使用模型场景的偏差，因此对于幻觉问题，常见的处理方法有：针对幻觉问题进行Post-training，使用Self-Consistency机制减小偶然性以及进行后处理。由于本文专注在架构层面上的优化，因此我们的工作通过后两者方法减小幻觉。
Because the essence of hallucination is the discrepancy between the training process and the actual use scenarios of the mode. Therefore, for hallucination problems, common handling methods include: post-training specifically for hallucination problems, using the Self-Consistency mechanism to reduce randomness, and post-processing. Since this paper focuses on optimizations at the architectural level, our work aims to reduce hallucinations through the last two methods mentioned above.

