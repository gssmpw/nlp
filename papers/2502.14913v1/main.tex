%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}
\usepackage{listings}
\usepackage{xcolor}


%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}
\lstset{
    escapeinside={(*@}{@*)},
    basicstyle=\ttfamily,   % 使用等宽字体
    keywordstyle=\color{blue}, % 关键字颜色
    commentstyle=\color{green}, % 注释颜色
    stringstyle==\color{red}, , % 字符串颜色
    numbers=none,      % 行号与代码之间的距离
    backgroundcolor=\color{gray!10}, % 背景色
    showspaces=false,      % 不显示空间
    showstringspaces=false, % 不显示字符串中的空间
    showtabs=false,        % 不显示制表符
    tabsize=2,             % 制表符大小
    breaklines=true
}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}

\renewcommand{\algorithmicensure}{\textbf{Output:}}
%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{OpenSearch-SQL: Enhancing Text-to-SQL with Dynamic Few-shot and Consistency Alignment}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Xiangjin Xie}
\email{xiexiangjin.xxj@alibaba-inc.com}
\affiliation{%
  \institution{Alibaba Cloud}
  \country{China}
}

\author{Guangwei Xu}
\email{kunka.xgw@alibaba-inc.com}
\affiliation{%
  \institution{Alibaba Cloud}
  \country{China}
}

\author{LingYan Zhao}
\email{zhaolingyan.zly@alibaba-inc.com}
\affiliation{%
  \institution{Alibaba Cloud}
  \country{China}
  % \country{USA}
}

\author{Ruijie Guo}
\email{ruijie.guo@alibaba-inc.com}

\affiliation{%
  \institution{Alibaba Cloud}
  \country{China}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Xiangjin et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
% 
\begin{abstract}
Although multi-agent collaborative Large Language Models (LLMs) have achieved significant breakthroughs in the Text-to-SQL task, their performance is still constrained by various factors. These factors include the incompleteness of the framework, failure to follow instructions, and model hallucination problems. To address these problems, we propose OpenSearch-SQL, which divides the Text-to-SQL task into four main modules: Preprocessing, Extraction, Generation, and Refinement, along with an Alignment module based on a consistency alignment mechanism. This architecture aligns the inputs and outputs of agents through the Alignment module, reducing failures in instruction following and hallucination. Additionally, we designed an intermediate language called SQL-Like and optimized the structured CoT based on SQL-Like. Meanwhile, we developed a dynamic few-shot strategy in the form of self-taught Query-CoT-SQL. These methods have significantly improved the performance of LLMs in the Text-to-SQL task.

In terms of model selection, we directly applied the base LLMs without any post-training, thereby simplifying the task chain and enhancing the framework's portability. Experimental results show that OpenSearch-SQL achieves an execution accuracy(EX) of 69.3\% on the BIRD development set, 72.28\% on the test set, and a reward-based validity efficiency score (R-VES) of 69.36\%, with all three metrics ranking first at the time of submission. These results demonstrate the comprehensive advantages of the proposed method in both effectiveness and efficiency.
\end{abstract}

%虽然多代理协作大语言模型（LLMs）在Text-to-SQL任务中取得了显著突破，但其性能依然受到多种因素的制约。这些问题包括框架的不完整性、指令遵循的失败及模型幻觉问题。为了解决这些问题我们提出了OpenSearch-SQL，该架构将Text-to-SQL任务划分为四个主模块：预处理（Preprocessing）、提取（Extraction）、生成（Generation）和优化（Refinement）,以及一个基于一致性对齐机制的对齐(Alignment)模块。这种架构通过Alignment 模块来对齐其他Agent的输入输出，减少指令遵循失败和幻觉问题。此外，我们设计了一种基于Query-CoT-SQL的few-shot机制显著提升了LLMs在生成SQL时的表现。

% 在模型选择上，我们直接应用基座大模型，而不进行任何Post-training，从而简化任务链和提高框架的可迁移性。实验结果表明，OpenSearch-SQL在BIRD开发集（DEV集）上的执行准确率达到了69.3%，在测试集（Test集）上为72.28%，奖励基础有效性评分（R-VES）为69.36%，三项指标均位列第一，展示了该方法在效果与效率上的全面优势。

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%




%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Text-to-SQL, Agent, Large Language Model}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:intro}

% Text-to-SQL任务尝试从Natural Language Queries（NLQ）自动的生成Structured Query Language (SQL) queries。 这项任务可以在不需要掌握SQL的专业知识的情况下，增强访问数据库的能力\cite{katsogiannis2023survey}
Text-to-SQL task attempts to automatically generate Structured Query Language (SQL) queries from Natural Language Queries (NLQ). This task can improve the ability to access databases without the need for knowledge of SQL \cite{katsogiannis2023survey}.  
% 早期的工作通过把query answer 定义为图结构\cite{}或者基于parsing理解问题的语法结构\cite{}, 后来的方法把Text-to-SQL任务看成是neural machine translation (NMT)问题\cite{}. 最近随着大语言模型的发展，研究者逐渐通过Agent\cite{dinsql}、In-Context learing\cite{macsql,DAILsQL}等方法完成任务，并取得了远胜之前方法的效果.
As the text-to-SQL problem is notoriously hard, these systems have been the holy grail of the database community for several decades \cite{katsogiannis2023survey}. Early work defined query answers as graph structures \cite{earlytxt2sql1,earlytxt2sql2} or based on syntactic structures for parsing the questions \cite{parse_survey1,parse_survey2}. Subsequent approaches have treated the Text-to-SQL task as a neural machine translation (NMT) problem \cite{rat, nmt1}. Recently, with the development of large language models, researchers have increasingly accomplished the task through methods such as the use of supervised fine-tuning (SFT) \cite{dubo,codes,Distilgpt4}, Chain-of-Thought(CoT) \cite{cot}, Agents \cite{dinsql,c3}, and In-Context Learning \cite{macsql,DAILsQL}, achieving results that greatly surpass previous methods.


%BiRD榜单是当前最有影响力的Text-to-SQL任务榜单, 目前靠前的方法基本上基于大基座模型或者Post-training的小模型来完成工作。然而我们知道模型Post-traing常常导致通用能力的损失，这引起了我们的思考:SFT过程究竟做了什么？能不能在不损伤通用能力的前提下达到SFT的效果？

% 虽然这LLMs驱动的方法大幅度提高了Text-to-SQL任务能力的上限, 但在分析过去的工作时，我们发现 1. 这些工作由于大框架的不明确，在方法层面存在着一些环节的缺失，这使得这些方法并没有达到其上限。例如，缺失了对数据库的存储信息的检验，没有对生成结果进行纠错，以及没有使用fewshot等问题。 
% 2. 驱动模型的指令和步骤会显著影响SQL生成的质量，前LLM时代中employ intermediate languages生成SQL的方法就是针对这个问题\cite{katsogiannis2023survey}, 但当前关于这方面的研究还不足。

% 3. LLMs驱动的方法大多依赖多agent的协作，但是由于LLMs存在不稳定性以及各个agent之间的耦合性、连贯性得不到保证，这导致靠后执行的Agent不使用或只部分使用之前运行的Agent的所得到的信息，这造成了很多性能损失。
\textbf{Limitation.} Although the methods driven by LLMs have significantly raised the upper limits of the Text-to-SQL task capabilities, our analysis of previous work reveals that:
\begin{enumerate}[label=\textbf{L\arabic*}]
    \item Due to the ambiguity of the overarching framework, there are some gaps at the methodological level. This has prevented these methods from reaching their potential. For example, there is a lack of verification of the stored information in the database, no error correction for the generated results, and issues related to the absence of few-shot learning.\label{l1}
    \item LLM-driven approaches often rely on multi-agent collaboration. However, due to the instability of LLMs and the lack of guaranteed coherence and coupling between agents, later-executing agents may not use or only partially use the outputs of previously running agents. This leads to accumulated hallucinations and performance loss.\label{l2}
    \item The instructions and steps that guide the LLMs significantly affect the quality of the generated SQLs. In the pre-LLM era, methods employing intermediate languages to generate SQL were developed to address this issue \cite{katsogiannis2023survey}, but current research on instruction construction remains insufficient.\label{l3}
\end{enumerate}

% 受到到上述的挑战启发，我们首先具体的分析了人类在Text-to-SQL任务中完成的工作：理解数据库并从数据库中选择生成SQL所需要的具体表、字段和值，这常常需要执行几个简单的SQL，比如查询某个字段存取值的具体形态，获取某个值在数据库中的具体保存形式。之后，根据NLQ的具体内容，选择恰当的聚合函数和SQL语法得到一个SQL的骨干，并填充对应的语法。最后，根据执行SQL的结果逐步修改SQL直到满足要求。同时，对于比较困难的问题，人类常常会选择通过查询或者借鉴他人的写法。于是我们对照着这个思路构建了我们的方法：OpenSearch-SQL，详情如下：

Inspired by the aforementioned challenges, we conducted a detailed analysis of the human workflow in completing the Text-to-SQL task: understanding the database structure and selecting the specific tables, columns, and values needed to get the SQL. This process often requires executing several simple SQL queries, such as investigating the specific forms of values for a certain column or retrieving how a particular value is stored in the database. Next, based on the specifics of the NLQ, the appropriate aggregation functions and SQL syntax are chosen to construct the backbone of the SQL query, followed by filling in the relevant statements. Finally, humans typically modify the SQL incrementally based on the query results until the requirements are met. For more complex questions, humans often choose to consult or borrow from others' formulations to seek solutions. Based on this approach, we developed the OpenSearch-SQL method, with detailed as follows.

% 为了处理Limitation 1，我们的研究首先根据人类完成SQL构建的流程定义了一个标准的Text-to-SQL框架：OpenSearch-SQL,v1，我们认为一个完整的Text-to-SQL任务框架应该包含:预处理、提取、生成、优化四个步骤：

\paragraph{\textbf{Framework.}} To address \textbf{\ref{l1}}, our research defines a \textbf{Standard Text-to-SQL framework} based on the process humans use to complete SQL construction. This can cover current research on LLM-driven Text-to-SQL tasks. We believe that a complete Text-to-SQL task framework should include four steps: \textbf{Preprocessing}, \textbf{Extraction}, \textbf{Generation}, \textbf{Refine}:
% Class acmart Warning: A possible image without description on input line 34.
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/overview3.jpg}
    \caption{The basic framework of OpenSearch-SQL optimizes the Text-to-SQL task through a multi-agent collaboration approach based on consistency alignment.}
    \label{overview}
\end{figure*}
\begin{itemize}

\item \textbf{Preprocessing.} Processing and constructing all auxiliary information that is insensitive to the NLQs, including database schema information, embedding databases, few-shot libraries, etc.
  \item \textbf{Extraction.} Extract and select the necessary elements for generating SQL, including sub NLQ, few-shot, as well as tables, columns, values, etc., from the database.
  \item \textbf{Generation.} Translate NLQ into SQL based on the SQL syntax, few-shot and the prepared information. 
  % 基于对齐策略和规则根据SQL的执行结果进行优化，然后基于自一致性和投票结果选择出最终的SQL
  \item \textbf{Refine.} Check and optimize the SQL based on the execution results using alignment strategies and rules. Then, select the final SQL based on self-consistency and voting results.
\end{itemize}

%针对L3，我们首先对Post-train方法进行了分析，然后提出了OpenSearch-SQL, v2：通过 基于检查的Agent一致性对齐来提升Text-to-SQL任务指令遵循的效果和子任务对齐的程度。
% 我们分析了多Agent协作驱动的Text-to-SQL任务中常见的幻觉，并为这些幻觉设计了对齐Agent：通过将具体Agent的功能、输入和输出进行一致性对齐来削减模型生成过程中的幻觉。同时这种对齐方法还可以进一步的扩展以对Agent的输出做定向的改变。
For \textbf{\ref{l2}}, we analyzed common hallucinations in multi-agent collaboration-driven Text-to-SQL tasks and designed a consistency alignment-based \textbf{ alignment agent} for these hallucinations. By ensuring consistency in the functions, inputs, and outputs of specific agents, our aim is to reduce hallucinations during the model generation process. This alignment approach can also be extended to make targeted adjustments to the agents' outputs. As shown in Figure \ref{overview}, we proposed the OpenSearch-SQL framework, which consists of a standard Text-to-SQL framework combined with alignments.
% 进一步的，我们依据这个框架进一步研究了LLM驱动的多Agent方法。针对L2，我们设计了一种few-shot扩充机制，通过对Query-SQL pair 对进行逻辑信息补充来生成Query-CoT-SQL pair形式的few-shot，并通过这种富信息的fewshot来增强LLM的表现。同时我们在CoT逻辑里设计了一种SQL-Like的中间语言，来让LLM在生成过程中先生成SQL的骨干再生成具体的SQL以减小LLMs处理任务的难度.

Furthermore, we extended our research on multi-agent methods driven by LLMs using this framework. For \textbf{\ref{l3}}, we developed a self-taught few-shot augmentation mechanism that supplements query-SQL pairs with \textbf{Chain-of-Thought(CoT)} information, creating \textbf{Query-CoT-SQL} pairs. This enriched few-shot approach can enhance the performance of LLMs. Additionally, we designed an intermediate language named SQL-Like within the CoT logic, allowing LLMs to first generate the backbone of the SQL before the specific SQL details, thus reducing the complexity of the task for the models.

% Furthermore, based on the framework, we found that the main bottlenecks in LLM-driven methods to complete the Text-to-SQL task are that LLMs do not follow instructions and cannot align with the specific styles of subtasks. Therefore, we analyzed two approaches: Post-training and Multi-agent methods, and then proposed OpenSearch-SQL, v2: by checking and aligning the consistency of sub-tasks to enhance the effectiveness of LLMs in following instructions and the degree of alignment with sub-tasks in the Text-to-SQL task.

% 由于在现实任务中，为特定的查询任务准备定制大量的SQL训练数据并不容易。因此出于对现实世界的考虑，我们的出发点不只是在某个具体的评估任务上取得好的效果，我们更希望这个方法能够具有足够强的迁移性能，可以不依赖Post-traing就可以热插拔的接入到各个具体的数据库查询任务中。这样在遇到任何问题时不必重新训练模型，只需要简单的修改Agent和对齐的方式就可以快速满足用户的需求。 因此我们直接在基座大语言模型模型上实现了我们的方法，并且不使用任何的SFT或RL。
% 最后，实验上来说，我们在BIRD\cite{Bird}这个当下受到广泛关注的数据集上进行了挑战，这数据集记录三个指标Dev集的执行准确率，Test集的执行准确率以及Test集的Reward-based Valid Efficiency Score (R-VES)。结果上来说，我们在Dev集上达到了69.3\%的执行准确率，在Test集上达到了72.28\%的执行准确率以及69.3\%的Reward-based Valid Efficiency Score (R-VES)。这三者在提交时，均在排行榜上达到了第一名，这证明了我们的方法不止能生成正确的SQL，而且在生成的SQL的时间效率上也有着领先优势。我们希望这种基于一致性对齐的方法可以启发之后的工作探索更高效的一致性对齐策略。

In real-world applications, preparing a large amount of customized SQL training data for specific query tasks is not an easy task. Therefore, considering the needs of the real world, our goal is not only to achieve good results in specific evaluation tasks, but also to ensure that the proposed method has strong transferability. We aim for it to be easily applicable to various database query tasks without the need for post-training. This means that when faced with any issue, we can simply modify the agent and alignment methods without the need to retrain the model, thus meeting user needs with high quality.

Based on this, we directly implemented our approach on a pre-trained LLM model without employing any supervised fine-tuning (SFT) or reinforcement learning (RL). In terms of experiments, we chose the widely recognized BIRD dataset \cite{Bird} for testing, which records three metrics: Execution Accuracy(EX) on the development set, EX on the test set, and the Reward-based Valid Efficiency Score(R-VES) on the test set. Our experimental results indicate that we achieved 69. 3\% EX on the Dev set, 72. 28\% EX on the Test set, and an R-VES score of 69.3\%. All three metrics ranked first on the leaderboard at the time of submission, demonstrating that our method not only generates accurate SQL statements but also has a clear advantage in terms of time efficiency. We hope that this consistency alignment-based approach will inspire future research to explore more efficient consistency alignment strategies.

\textbf{Contribution} In summary, our contributions are as follows.
\begin{itemize}

%据我们所知，我们第一次在Text-to-SQL任务上提出了基于一致性对齐的多Agent合作框架，同时我们还提出了新的基于信息扩展的动态few-shot方法和机遇SQL-Like的CoT机制。这三者大大提升了多Agent协作的Text-to-SQL任务的表现。
\item To the best of our knowledge, we are the first to propose a multi-agent cooperative framework based on consistency alignment for the Text-to-SQL task. This alignment mechanism significantly reduces the loss of information transmission between Agents and the hallucinations in the results generated by the Agents.

% 我们的方法没有进行微调或者引入任何独特数据集就达到了SOTA，在BIRD榜单上甚至超过了微调GPT4-O的方法。
\item We introduce a novel self-taught dynamic few-shot method and a SQL-Like CoT mechanism. This enhances the performance and stability of LLMs when generating SQL.

% 我们提出的方法提交时在BIRD benchmark验证集上达到69.3\%的执行准确率，测试集上达到72.28\%的执行准确率和69.36\%的Reward-based Valid Efficiency Score(R-VES)，均排名第一
\item Our proposed method, OpenSearch-SQL, did not utilize fine-tuning or introduce any specialized datasets. At the time of submission, it achieved 69.3\% EX on the BIRD benchmark validation set, 72.28\% EX on the test set, and R-VES of 69.36\%, all ranking first.
\end{itemize}



\section{Preliminary}
\input{preliminary}

\label{sec:method}
\section{Methodology}% 介绍方法
% 这一章中，我们具体阐述了OpenSearch-SQL的细节。对于Alignments和动态few-shot的核心，我们在阐述完整流程之前做了更详细的描述。同时为了阐述的流畅性，我们把每个阶段对应的Alignments和该阶段合并阐述，以符合OpenSearch-SQL 框架运行的顺序
In this section, we detail the specifics of OpenSearch-SQL. We provide a more in-depth description of the core concepts of Alignments and dynamic few-shot before outlining the entire process. To ensure a smooth explanation, we integrate each stage with its corresponding Alignments and describe them in the order they operate within the OpenSearch-SQL framework.

\subsection{Alignments}
% 大模型幻觉问题\cite{hallucination}是一个影响LLMs可用性的关键问题，在Text-to-SQL任务中这个问题也同样存在，而由于多Agent合作的方法存在着错误累计的情况，这使得LLMs的幻觉问题更加严重。举例来说，如果设计完成Extraction功能的Agent按照某种方式选择数据库中的列，由于模型幻觉导致生成了错误的列名，在随后的Generation阶段，由于不知道正确的数据库结构，上一个Agent产生的幻觉会被继承，因此在基于多Agent的LLMs工作流中，幻觉很难自发消失，幻觉总量几乎是单调不减的。

LLM hallucination \cite{hallucination} is a critical problem affecting the usability of LLMs, and it is also present in the Text-to-SQL task. Furthermore, the cumulative errors that arise from multi-agent collaboration exacerbate the hallucination problem in LLMs. For example, if an agent responsible for the \textbf{Extraction} function selects columns from the database in a certain way and generates incorrect column names due to model hallucination, this error will persist in the subsequent \textbf{Generation}, as it lacks knowledge of the correct database structure. Consequently, the hallucination produced by the previous agent is inherited, making it difficult for hallucinations to spontaneously disappear in a multi-agent LLM workflow; the total amount of hallucination is almost monotonically non-decreasing.

% 根据这个现象我们定义Alignment Agent为 $A_{Aligment}$：
Based on this phenomenon, we define the Alignment Agent as :
\begin{equation}
A_{Aligment}(x+A'(x))=A(x)-A'(x),
\end{equation}
% 输入为当前Agent的输入信息x，和当前Agent处理的输出A'(x), 根据大模型或者规则优化得到输出A_{Aligment}(x+A'(x)).
The input for the agent is the input $x$ for the agent to be aligned, along with $A'(x)$. The output is $A_{Aligment}(x+A'(x))$, which is obtained through optimization by a large model and rules. Empirical observations suggest that the difference between $A'(x)$ and $A(x)$ is mainly due to hallucinations in the model. Therefore, the function of Alignment is to align the current Agent's input information with its output result, so that this alignment can reduce the hallucinations between the expected output and the actual output.
% 其中x是当前Agent的输入信息，A(x)是Agent处理之前输出和该Agent特定输入的期望结果。实际输出的结果是A'（x），经验性的观察来讲，A‘（x）和A(x)的差值主要是模型幻觉引起的。于是Alignment的功能定位就是将当前Agent的输入信息和输出结果进行对齐，使这种对齐可以消除期望输出和实际输出之间的幻觉。


% Text-to-SQL任务中常见的幻觉包括:指令遵循失败，输出结果存在波动等。具体来说，我们发现的有：生成不存在的列，更改数据库的列名，语法错误，错误搭配数据库的值和列，不遵循Prompt中设定的规则。因此，我们的方法使用了一种一致性对齐思路：在每一个Agent结束时，用一个Alignment Agent来对齐当前的Agent的输出和上游Agent的输出，让各个Agenty的功能达到逻辑上的一致，并将对齐后的结果传给下游的Agent，这种机制在某种程度上类似残差机制\cite{resnet}，可以有效的加长多Agent合作的链条，而不至于引入越来越多的幻觉。

% 具体来说，如图\ref{overview}所示，在\textbf{Extraction}阶段之后，使用一个alignment Agent处理NLQ将LLMs的SELECT风格与数据集的SELECT风格进行对齐。同时，检查\textbf{Extraction}阶段提取的字段内容是否与数据库结构匹配，并进行必要的纠正和扩展。在\textbf{Generation}阶段结束后，Alignment将对齐\textbf{Generation}的输出与\textbf{Extraction}的结果，以修正Generation生成的SQL中不使用Extraction 的结果的问题。 关于各个Alignment的细节将在之后的Agent介绍中详细展示

In the Text-to-SQL task, common hallucination phenomena mainly refer to failures in instruction following and instability in output results. This is specifically manifested as: generating non-existent columns, changing database column names, syntax errors, mismatching database values with columns, and failing to adhere to the rules set in the prompt. To address these problems, we propose a consistency alignment method: after each agent completes its output, an \textbf{Alignment} agent is used to align the current agent's output with that of the upstream agent, ensuring that the functionalities of the various agents achieve logical consistency, and the aligned result is passed on to the downstream agent. This mechanism is somewhat analogous to residual connections \cite{resnet}, effectively extending the chain of collaboration among multiple agents while minimizing the introduction of hallucinations. The details of each Alignment will be presented in detail in the following Agent introduction.

\subsection{Self-Taught Fewshot}\label{dynamic fewshot}
% \paragraph{\textbf{Dynamic Fewshot.}} DAIL-SQL研究了问题表示对于Text-to-SQL任务的关键作用，并提出基于问题的相似度来选择合适的fewshot来驱动LLMs生成SQL并取得了不错的效果。受到启发，我们尝试使用动态的Fewshot去驱动Text-to-SQL任务中的更多Agent。同时，我们思考让Fewshot发挥更大的作用，从样本中挖掘出更多的信息。

% 于是，在OpenSearch-SQL中，我们对Query-SQL 形式的Fewshot做了升级，首先，对训练集中的Query-SQL pair,我们用LLM补充NLQ转化为SQL的CoT信息，并得到包含逻辑信息的Query-CoT-SQL pair，这样的Few-shot会比单纯的Query-SQL pair展示更丰富的信息。\ref{query-CoT-SQL}中展示了Query-CoT-SQL Pair的形式。

% 然后，对于Refine中对SQL纠错的任务，由于相比于问题的相似度更重的是错误的类型，比如语法错、无结果。于是我们针对不同的错误类型构造了不同的Few-shot，使LLMs可以更明确如何根据不同的错误类型矫正SQL。
%  todo。类似一种残差机制
Few-shot is an important method to assist LLMs in generation, MCS-SQL\cite{mcssql}, DAIL-SQL\cite{DAILsQL} studied the critical role of problem representation in the Text-to-SQL task and proposed using question similarity to select appropriate few-shots to drive LLMs in generating SQL, achieving notable results. 

Inspired by this, we attempted to use dynamic few-shots to enhance the efficiency of agents in the Text-to-SQL task. Additionally, we considered how to better leverage few-shots by extracting more information from the samples. Therefore, in OpenSearch-SQL, we first used \textbf{Masked Question Similarity (MQs)} \cite{mqs} to select similar queries. Then, we upgraded the few-shots in the Query-SQL format by self-taught. For the Query-SQL pairs shown in Listing \ref{query-SQL}, we used an LLM to supplement the CoT information to transform the NLQ into SQL. As shown in Listing \ref{query-CoT-SQL}, this resulted in \textbf{Query-CoT-SQL} pairs containing logical information. Compared to simple Query-SQL pairs, these self-taught few-shots provide richer information.

Then, for the error correction of the \textbf{Refinement}, we prepared different few-shots for various error types, enabling LLMs to more clearly understand how to correct SQL based on different errors during \textbf{Refinement}. The format of the few-shots is shown in Listing \ref{reffewshot}, the specific errors in the \textbf{Raw SQL} and the \textbf{Advice of correction} correspond to the error type.
% 其中Raw SQL的具体错误和 Advice of correction 都与错误类型相对应
\begin{lstlisting}[caption={Format of Few-shot in Query-SQL Pair},label ={query-SQL}]
/* Answer the following:(*@\color{red}{\{question\}}@*) */
#SQL: (*@\color{red}{\{SQL\}}@*)
\end{lstlisting}

\begin{lstlisting}[caption={Format of Few-shot in Query-CoT-SQL Pair},label ={query-CoT-SQL}]
/* Answer the following:(*@\color{red}{\{question\}}@*) */
#reason: Analyze how to generate SQL based on the question.
#columns: All columns ultimately used in SQL
#values: the filter in SQL
#SELECT: SELECT content table.column.
#SQL-like: SQL-like statements ignoring Join conditions
#SQL: (*@\color{red}{\{SQL\}}@*)
\end{lstlisting}

\begin{lstlisting}[caption={Format of Few-shot in Correction},label ={reffewshot}]
{"Result: None": """/* Fix the SQL and answer the question */
#question: (*@\color{red}{\{question\}}@*)
#Error SQL: (*@\color{red}{\{Raw SQL\}}@*)
Error: Result: None
#values: (*@\color{red}{\{values in Database\}}@*)
#Change Ambiguity: (*@\color{red}{\{Advice of correction\}}@*)
#SQL:(*@\color{red}{\{corrected SQL\}}@*)""",
...}
\end{lstlisting}

\subsection{Preprocessing}
% 预处理任务中根据数据库的真实结构构建了数据库，同时为了LLMs生成的SQL匹配数据库的真实情况，我们对数据库中的值做了索引，这使得SQL可以避免因为字符上的微小差异导致的错误。除此之外，关于动态Fewshot的构建也在这一阶段完成，这包括CoT信息的补充，以及根据不同错误类型设计的纠错fewshot
In the \textbf{Preprocessing}, we constructed the database based on its true structure. Additionally, to ensure that the SQL generated by LLMs aligns with the actual state of the database, we indexed the values within the database. This allows the SQL to avoid errors that might arise from small character discrepancies. It's worth noting that we index only string-type data to save the space required for building the retrieval database.

In addition, the construction of dynamic Fewshot has also been completed in this phase. This includes the addition of CoT information and correction few-shot examples designed for different error types.

% 总的来说，在预处理阶段，输入为数据库信息，训练集。输出为：向量数据库、Query-CoT-SQL形式的Few-shot，以及数据库的Schema.
Overall, during the preprocessing stage, the inputs are the information from the database and the training set. The outputs are a vector database, Few-shot in the form of Query-CoT-SQL, and the database schema. This process is automated, does not require human intervention, and is entirely driven by an Agent.


\subsection{Extraction}

% 我们定义Extraction为：为NLQ翻译为SQL提取一切必要的信息，这一部分是与具体的查询语言解藕的，只与数据存储的格式和解决问题所需的辅助信息有关。比如C3-SQL使用Clear Prompting(CP)提供有效的Prompts，DIN-SQL使用的schema Linking和classification&Decomposition对NLQ进行分类、分解处理。

%\textbf{提取}的目标是根据具体的NLQ准备需要的信息，包括Schema Linking，数据库存储的值，Few-shot，必要的指令等。这一部分是与具体的数据库查询语言解藕的，只与数据存储的格式和解决问题所需的辅助信息有关。
The goal of \textbf{Extraction} is to prepare the necessary information based on specific NLQs, including schema linking, stored database values, few-shot examples, and any required instructions. This part is decoupled from the specific database query language and is only related to the format of data storage and the auxiliary information needed to solve the problem.
For instance, C3-SQL\cite{c3} utilizes Clear Prompting (CP) to provide effective prompts, while DIN-SQL\cite{dinsql} employs schema linking and classification \& decomposition to categorize and break down the NLQ.

% 我们首先使用Masked question similarity (MQs) \cite{mqs} 来为自然语言查询（NLQ）选择相似的查询和预处理的辅助信息形成few-shot 示例。接下来，为了获取生成SQL所必需的数据库信息和值信息，我们利用大模型提取问题中的实体，并选择所需的列和字段。同时，我们使用向量检索的方法来获取与问题相关的表、列和值。这种两路召回可以同时利用大模型的语义理解能力和向量检索对字符级差距的包容性，从而提高信息的覆盖率。
% \paragraph{\textbf{SELECT 对齐}} 在抽取阶段的最后，我们使用一个Agent来对齐SELECT语句的风格：从NLQ中提取出查询的短语或子句，使其与生成时的SELECT内容一一对应。这样可以确保SELECT内容在数量和顺序上都符合预期。为了避免不同表的同名字段导致的误选择，我们还会对选取的列进行同名字段的扩充，将所有同名但来自不同表的列重新召回到列过滤的结果中，以确保所选列的准确性与全面性。

% OpenSearch-SQL的Extraction由Entity Extraction, Values Retrieval, Column Filtering 组成。在Extraction之后，我们用Info Alignment把得到的信息与输入进行对齐得到最后的输出。我们在\ref{extract_prompt}中展示了Extraction的Prompt形式，我们通过LLM获得候选的列和values，同时用另一个简单的prompt直接提取NLQ中的entities。下面阐述了处理的细节:
\begin{lstlisting}[caption={Format of Extraction},label ={extract_prompt}]
(*@\colorbox{yellow}{Input:}@*)
/* Database schema */
(*@\color{red}{\{db\_info\}}@*)
(*@\color{red}{\{rule\}}@*)
/* Answer the following: (*@\color{red}{\{query\}}@*) */

(*@\colorbox{yellow}{Output:}@*)
(*@\color{red}{\{reason\}}@*)
(*@\color{red}{\{column\}}@*)
(*@\color{red}{\{values\}}@*)
\end{lstlisting}

The \textbf{Extraction} process in OpenSearch-SQL consists of entity extraction, value extraction, and column filtering. After \textbf{Extraction,} we use Info Alignment to align the extracted information with the input to produce the final output. We present the format of the extraction prompt and its input and output in the listing \ref{extract_prompt}. We obtain candidate columns and values through LLM and directly extract entities from the NLQ using another simple prompt. The details are outlined below:



\paragraph{\textbf{Entity Extraction}}
% 为了找到数据库中相似的值和进行列的过滤，我们首先让LLM对NLQ和数据库基础信息进行理解和处理，并从中提取出潜在的实体，其次我们将一些预设的实体词和这些实体整理到一起，用来做下面的Values retrival 和 Column filtering
To find similar values in the database and perform column filtering, we first use the LLM understand and process the NLQ and basic database information to extract potential entities. Then, we organize some predefined entity terms together with these entities to perform the subsequent values retrieval and column filtering. 

% 从提取的实体中我们直接进行向量检索得到与实体词拥有最高embedding相似度的值，由于embedding相似度的特性，这样的检索可以避免typo和其他字符级别的不同导致值匹配失败的问题，同时对于词组和长文本我们会同时进行拆分检索，以避免数据库存储格式的特点不同导致值召回失败的问题。在召回之后我们过滤掉最相似的K个实体中低于特定阈值的部分作为最后的结果。
\paragraph{\textbf{Values Retrieval.}}
From the extracted entities, we perform vector retrieval to find results with the highest embedding similarity to the entity words. Due to the nature of embedding similarity, this method can prevent mismatches caused by typos or other character-level differences. Additionally, for phrases and longer texts, we perform split retrieval to avoid recall failures due to differences in database storage formats. During the recall phase, we filter out portions of the top K most similar entities that fall below a specific threshold, retaining them as the final results.

% 此步骤的母的是减少与当前NLQ无关的信息的干扰，通过减少不必要的信息，来提高模型生成SQL的性能。但是值得注意的是，我们并没有像\cite{chess,mcssql}方法一样追求非常精确的Schema信息，我们更专注于减少非常无关的数据库信息，同时尽量保证相关的列没有缺失。

% 具体来说，我们使用了两种方式召回相关的表和列。首先，我们通过LLM从完整的数据库信息中选择与NLQ相关的表和列，然后我们用向量召回的方式召回数据库中与NLQ中实体相似度大于阈值的列，并将二者进行整合组成最终Schema信息的预备子集。这种多路召回的方式虽然在过滤的精确度上有所缺失，但流程上更为轻量和简洁。
\paragraph{\textbf{Column Filtering.}}
In detail, we utilize two methods to recall relevant tables and columns. First, we employ a large language model (LLM) to select tables and columns related to the natural language query (NLQ) from complete database information. Then, we use vector retrieval to find columns in the database with similarity to NLQ entities exceeding a certain threshold, integrating these to form a preliminary subset of the final Schema information. Although this multi-path recall method may lack some precision in filtering, it is lighter and more streamlined in process.

\paragraph{\textbf{Info Alignment}} At the end of the extraction phase, we use an Info aligment agent to align the style of the SELECT statement: extracting phrases or clauses from the NLQ that correspond one-to-one with the generated SELECT content. This ensures that both the quantity and order of the SELECT content meet expectations. To avoid misselection caused by columns with the same name in different tables, we expand the schema information by reintegrating the primary keys of each table and all columns with the same name as the columns selected in column filtering into the column filtering results, ensuring the accuracy and comprehen siveness of the selected columns. Finally, all the extracted information and the content of the Alignment will be input into the \textbf{Generation}.
% 最后所有的提取信息和Alignment的内容会作为输入传入到Generation Agent
% 总的来说，在Extraction阶段，输入如Listing

\subsection{Generation}
% Generation的定义是：通过合适的方式驱动LLMs生成具体的SQL的组件和完整的SQL。MAC-SQL\cite{macsql}通过生成sub-SQL再拼装成最后结果，DAIL-SQL\cite{DAILsQL}通过Fewshot驱动LLMs生成SQL。还有一些方法\cite{chess,Distilgpt4,mcssql}通过SFT生成时的LLMs或者生成多组SQL来完成任务。
The definition of \textbf{Generation} is: using appropriate methods to drive LLMs to generate specific components and complete SQL. MAC-SQL \cite{macsql} generates sub-SQLs and then assembles them into the final result, while DAIL-SQL \cite{DAILsQL} drives LLMs to generate SQL through few-shot learning. Additionally, some methods \cite{chess, Distilgpt4, mcssql} utilize LLMs during Supervised Fine-Tuning (SFT) or generate multiple sets of SQL to complete tasks.
\definecolor{darkgreen}{rgb}{0.0,0.5,0.0}
\begin{lstlisting}[caption={Example of Generation},label ={generate}]
(*@\colorbox{yellow}{Input:}@*)
(*@\color{red}{\{fewshot\}}@*)
/* Database schema */
(*@\color{red}{\{db\_info\}}@*)
(*@\color{red}{\{similar values\}}@*)
(*@\color{red}{\{rule\}}@*)
(*@\color{darkgreen}{Rule Example: For parts involving division that contain integer types, CAST them to REAL}@*)

/* Answer the following: How many patients with a normal Ig A level came to the hospital after 1990? normal Ig A level refers to IGA > 80 AND IGA < 500; */
(*@\color{red}{\{info alignment\}}@*)
(*@\color{darkgreen}{Info Alignment Example: SELECT content: [How many patients]}@*)

(*@\colorbox{yellow}{Output:}@*)
#reason: The question wants to count the number of patients with a normal Ig A level who came to the hospital after 1990, so SELECT will count distinct patients based on the specified conditions.
#columns: Patient.ID, Laboratory.IGA, Patient.`First Date`
#values: normal Ig A level refers to Laboratory.IGA > 80 AND Laboratory.IGA < 500; came to the hospital after 1990 refers to strftime('%Y', Patient.`First Date`) >= '1990'
#SELECT: How many patients refer to COUNT(DISTINCT Patient.ID)
#SQL-like: Show COUNT(DISTINCT Patient.ID) WHERE Laboratory.IGA > 80 AND Laboratory.IGA < 500 AND YEAR(Patient.`First Date`) >= '1990'
#SQL: SELECT COUNT(DISTINCT T1.ID) FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE T2.IGA > 80 AND T2.IGA < 500 AND strftime('%Y', T1.`First Date`) >= '1990'
\end{lstlisting}

% 我们选择了使用渐进式生成+动态Fewshot的方式来驱动LLMs更好的生成SQL。其中渐进式生成是一种定制的COT\cite{cot}, 具体的说, 我们首先定义了SQL-Like：一种忽略了具体语法格式（如JOIN和功能函数写法）的类SQL语言，目标是让LLMs更关注SQL中的逻辑而非格式。然后我们让模型按顺序给出分析，SELECT的内容，相关的列和值以及SQL-Like和SQL。这种渐进式的生成更关注LLMs对生成SQL结构的理解，同时也便于发现模型思考的错误。
% 对于few-shot的构建则根据MQs取得相似度前K_f大的Query，并以其对应的Query-CoT-SQL形式作为最后指令中的few-shot
% 对于单个NLQ，我们让LLMs生成多个候选的SQL。

% 图\ref{generate}展示了\textbf{Generation}输入输出的组成。输入中我们通过Rule和Database schema提供必要的输出要求和数据库信息，并通过Few-shot提供相似的例子，Similar Values提供数据库中具体和问题有关的值，Style Alignment对齐数据集和LLMs的SELECT风格。Ouptput中要求依次生成reason:对NLQ的分析，Columns:SQL中有关的列，Values:有关的值和SELECT:SELECT的内容，然后生成忽略语法的SQL-Like并最终生成SQL。

\paragraph{\textbf{SQL Generation.}} As shown in Listing \ref{generate}, we chose to use the progressive generation $+$ dynamic few-shot to better drive LLMs in generating SQL. Specifically, progressive generation is a customized CoT approach, the format is the same as Listing \ref{query-CoT-SQL}. We first defined SQL-Like: a type of SQL language that ignores specific syntactical elements (such as JOINs and the formatting of functions), aiming to encourage LLMs to focus more on the logic within SQL rather than the formatting. We then instruct the model to sequentially provide the \textbf{analysis}, the contents of the \textbf{SELECT statement}, \textbf{relevant columns} and \textbf{values}, as well as the \textbf{SQL-Like} and \textbf{SQL} outputs. This progressive generation emphasizes the LLMs' understanding of the structure of the generated SQL, while also faciliting the identification of errors in the LLMs' reasoning. For individual NLQs, we allow LLMs to generate multiple candidate SQL queries. For the construction of few-shot examples, the system retrieves the top $K_f$ most similar queries based on MQs, and uses their corresponding Query-CoT-SQL forms as the few-shot examples in the final instruction.
% 输入中我们通过Rule和Database schema提供必要的输出要求和数据库信息，并通过Few-shot提供相似的例子，Similar Values提供数据库中具体和问题有关的值，Style Alignment对齐数据集和LLMs的SELECT风格。Ouptput中要求依次生成reason:对NLQ的分析，Columns:SQL中有关的列，Values:有关的值和SELECT:SELECT的内容，然后生成忽略语法的SQL-Like并最终生成SQL。

Listing \ref{generate} demonstrates the composition of inputs and outputs during the generation phase through an example.
\begin{enumerate}
    \item In \textbf{input}, we provide the necessary output requirements and database information by the Rules and Database schema, and we offer similar examples through few-shot in Query-CoT-SQL format. \textbf{Similar Values} provide specific values from the database that are relevant to the NLQ, while \textbf{Info Alignment} aligns the SELECT style of the dataset with the LLMs.
    \item In the \textbf{output}, we require the following sequential generation: \textbf{Reason}: Analysis of NLQ, \textbf{Columns}: The relevant columns in the SQL, \textbf{Values}: The related values, \textbf{SELECT}: The content of the SELECT statement. Then, generate \textbf{SQL-Like} query that ignores syntax formatting and ultimately produces the final SQL queries.
\end{enumerate}

\begin{lstlisting}[caption={Examples of Alignments},label ={align}]
(*@\color{blue}{\textbf{Agent Alignment:}}@*)
(*@\color{darkgreen}{Raw SQL}@*): SELECT ID FROM table WHERE table.name= 'John'
value in Database: table.name='JOHN'
(*@\color{darkgreen}{Aligned SQL}@*): SELECT ID FROM table WHERE table.name= 'JOHN'

(*@\color{blue}{\textbf{Function Alignment:}}@*)
(*@\color{darkgreen}{Raw SQL}@*): SELECT ID FROM table ORDER BY MAX(score)
(*@\color{darkgreen}{Aligned SQL}@*): SELECT ID FROM table GROUP BY ID ORDER BY score

(*@\color{blue}{\textbf{Style Alignment:}}@*)
(*@\color{darkgreen}{Raw SQL}@*): SELECT ID FROM table ORDER BY score DESC LIMIT 1
(*@\color{darkgreen}{Aligned SQL}@*): SELECT ID FROM table WHERE score IS NOT NULL ORDER BY score DESC LIMIT 1


\end{lstlisting}
% 由于生成的SQL queries根据语法、数据库、NLQ、数据集风格的不同存在着差异。同时，让大模型通过高的温度sample出足够的答案会引入误差和噪声。所以，我们尝试通过一种对齐机制将这些原因导致的幻觉进行削减。具体的来说，这里的Aligments分为三个部分，Agent Aligment直接对齐数据库的列和值是否有正确的出现在SQL中，如果存在错误匹配就进行改正，这里一个比较常见的例子是：SQL中的WHERE条件与数据库存储的信息不匹配。Function Aligment 将SQL的聚合函数收敛到某个具体形式，避免因为错误的表达式引起的错误，这里的例子有：比较形式和JOIN的冗余。最后，Style Aligment处理一些数据集特性的问题：比如 IS NOT NULL 和 MAX还是LIMIT的选择。
\paragraph{\textbf{Alignments}} 
The errors in the SQL query generation process primarily arise from differences in syntax, databases, natural language queries (NLQ), and dataset styles. Additionally, employing large models to sample at higher temperatures for diverse answers may introduce errors and noise. Therefore, we attempt to reduce the biases caused by these discrepancies through an alignment mechanism.
Specifically, as shown in the listing \ref{align}, the alignment mechanism consists of the following three components:
\begin{itemize}
    \item Agent Alignment: Ensures that the columns and values from the database are correctly represented in the SQL. If there is a mismatch, corrections are made. A common example is a WHERE condition in SQL that does not match the information stored in the database.
    \item Function Alignment: Standardizes the SQL aggregate functions to prevent errors caused by incorrect expressions. This includes handling inappropriate AGG functions, nestings, and redundant JOINs.
    \item Style Alignment: Addresses issues related to dataset characteristics, such as the use of IS NOT NULL and the choice between MAX and LIMIT 1.
\end{itemize}

% 在上述步骤完成后，则会执行SQL，并在Refinement阶段根据执行结果对SQL进行优化和最终选择。
After completing the above steps, the SQL is executed, and during the \textbf{Refinement}, it is optimized and the final selection is made based on the execution results.
\subsection{Refinement}
% % Refine的目标是：对已经生成的SQLs进行优化和选择。包括：Agent之间的一致性对齐、执行效率提高、SQL风格转换、冗余消除、根据执行结果进行纠错和从多个候选的SQL中选择最佳的答案等。
% 根据由于LLMs生成结果的波动性和同时复杂任务中常常出现的指令遵循失败问题，我们提出了一种基于检查的一致性对齐机制来提高SQL的质量。流程如图\ref{Refinement}所示，分为以下三步:

The work of \textbf{Refinement} involves optimizing and selecting the generated SQLs. This includes correcting errors based on execution results, and selecting the best answer from multiple candidate SQLs.

Due to the variability of results generated by LLMs and the frequent issues with instruction adherence in complex tasks, we propose a consistency alignment mechanism based on checking to enhance the quality of SQL. As illustrated in Figure \ref{Refinement}, after aligning the generated SQLs, the refinement consists of the following two steps:
% 1. 对齐: 包括三个对齐对子任务:Agent对齐:检查生成的SQL的组件是否用到了\textbf{Extraction}中的结果，包括values和column的使用。功能函数对齐:检查SQL功能函数的使用是否符合数据集的风格,风格对齐:根据数据集一些特点做一些SQL格式的变化.
% 2. 执行SQL，根据执行结果进行纠错。其中每种错误类型会对应不同的few-shot和纠错指令
% 3. Self-consistency，选择其中一致性最高且执行时间最短的SQL
\begin{enumerate}
    \item \textbf{Correction}: Execute the SQL and fix it based on the error details in the execution results, such as syntax errors or empty results. Each type of error corresponds to different few-shot and error-correction instructions. The purpose of this step is to avoid issues such as lack of results or execution failures caused by minor details.
    % 选择执行结果中答案一致性最高的选择，这确保了正确率。但除此之外，我们还会在相同答案的SQL中选择执行时间最短的。这使得最后选择的SQL取得了执行结果和执行速度的双提升。
    \item \textbf{Self-consistency \& vote}: Exclude SQLs that cannot be fixed and those that result in empty answers and choose the option with the highest output consistency in the execution results. Additionally, among SQL queries with the same answers, we select the one with the shortest execution time. This approach achieves a dual enhancement in both execution results and execution speed for the final selected SQL. Given the set $K=\{R_1,R_2...R_n\}$, where $R_i=(SQL_i,ans_i,t_i)$. $SQL_i$ represents the SQL query, $ans_i$ represents the result of the execution, and $t_i$ represents the execution time. We obtain the final SQL query using:
% 从公式来讲：对于数据库K={R1,...Rn},R_i=(SQL_i,ans_i,t_i).其中SQL_i表示SQL，ans_i表示执行的结果，t_i表示执行花费的时间。用SQL_{select}={SQL|f(R)}得到最后的SQL。f(count(ans),t_i)表示SQL满足 max(count(ans)) 和min(t_i).
\end{enumerate}

\begin{equation}
\begin{aligned}[t]
&\text{SELECT \textbf{SQL} FROM \textbf{K} WHERE \textbf{ans} =} \\
&\text{(SELECT \textbf{ans} FROM \textbf{K}} \\
&\text{GROUP BY \textbf{ans} ORDER BY} \\
&\text{COUNT(*) DESC LIMIT 1)} \\
&\text{ORDER BY \textbf{t} LIMIT 1}
\end{aligned}
\end{equation}


   
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/refinement.jpg}

    \caption{Architecture of the \textbf{Refinement}: Optimizing SQLs through Correction and Consistency \& Vote to get the best SQL.}\label{Refinement}
\end{figure}
% 值的一提的是，Self-consistency 常常会导致更高的成本。在测评中，在\textbf{Generation}中仅生成一个答案，且不使用Self-consistency机制的情况下，OpenSearch-SQL,v2 仍然可以在BIRD benchmark 中排名第一.
It is worth mentioning that self-consistency often leads to higher costs. In the evaluation, when only one SQL is generated in the \textbf{Generation} and the self-consistency mechanism is not used, OpenSearch-SQL, v2, still ranks first in the BIRD benchmark.


\subsection{Algorithm}
% 这一步中，我们对OpenSearch-SQL做一个整体性的总结，并在Algorithm \ref{}中展示算法的整体框架，在这个算法我们以一条NLQ为例，囊括了从预处理到最后生成SQL的每一个具体步骤。
In this section, we provide an overall summary of OpenSearch-SQL and present the entire framework in Algorithm \ref{alg}. In this algorithm, we take an NLQ as an example, encompassing each specific step from preprocessing to the final generation of SQL.

% 可以看出，我们首先在预处理中对数据库逐表逐字段的进行处理，建立值、字段名的索引库D_v，同时生成数据库的原始Schema S. 然后对于训练集中的Query-SQL pair，我们用$M_L((q_t,s_t))$来生成CoT信息以构建新的few-shot F。进入主函数，我们首先用大模型处理NLQ $Q$, 通过A_E(Q, S)获得实体信息$E_Q$和选择列$t.c_Q$. 然后我们通过检索召回相似的值 $v_Q$, 并扩充数据库信息$t.c_E$,并将这些信息通过Alignment $A_a$进行整合得到为Q定制的Schema $S_Q$. 下一步，我们根据$MQ_s(Q)$取得问题相关的Query-CoT-SQL形式的few-shot f_Q. 接下来，通过A_G生成SQLs并通过A_a进行对齐得到SQLs_A。最后通过A_R纠正执行错误，并选择一致性最高且执行时间最短的SQL_R作为最后的答案。
It can be seen that we first process the database table by table, field by field, during preprocessing to build an index database $D_v$ of values and columns' names, while also generating the original schema $S$ of the database. Next, for the Query-SQL pairs in the training set, we use 
$M_L$ to generate CoT information to construct the new few-shot set $F$. In the main function, we first process the NLQ $Q$ using a LLM. Through $A_E$ we obtain the entity information $E_Q$ and select the column $t.c_Q$. Then, we retrieve similar values $v_Q$and expand the database information $t.c_E$. These details are integrated through Alignment $A_a$ to create a schema $S_Q$ tailored for $Q$. The next step is to obtain the few-shot $f_Q$ in Query-CoT-SQL format relevant to the question based on $MQs(Q)$. Next, SQLs are generated through $A_G$ and aligned using $A_a$ to obtain $SQLs_A$. Finally, execution errors are corrected with $A_R$, and the SQL with the highest consistency and shortest execution time, $SQL_R$, is selected as the final answer.

\begin{algorithm}
\caption{Algorithm of OpenSearch-SQL}\label{alg}
\begin{algorithmic}[1]
\Require Target database D, user question Q, train set T
\State  Extraction Agent $A_E$, Generation Agent $A_G$, Refinement Agent $A_R$, Alignment Agent $A_a$, vector model $M_E$, LLM $M_L$, Fewshot Set F, Database Schema S, vector database $D_v$
\State \Comment{Preprocessing}
\For{Table $t$ in $D$} \Comment{Build vector database}
    \For{column $c$, values $v$ in $t$}
        \State get description $d$
        \If{$v$ is String}
            \State $D_v \gets M_E(v), M_E(t.c) : t.c$ \Comment{Add index to $D_v$}
        \EndIf
        \State $S \gets S + d$
    \EndFor
\EndFor
\For{Query-SQL pair ($q_t, s_t$) in $T$} \Comment{Create fewshot}
    \State $CoT_t \gets M_L((q_t, s_t))$
    \State Add Query-CoT-SQL pair ($q_t, CoT_t, s_t$) to $F$
\EndFor
\State \Comment{Main Process}
\State $E_Q, t.c_Q \gets A_E(Q, S)$
\State $v_Q \gets D_v(E_Q)$ \Comment{Get similar values to $Q$}
\State $t.c_E \gets D_v(E_Q)$ \Comment{Get table, column related to $Q$}
\State $S_Q \gets A_a(Q, S, t.c_E, t.c_Q, v_Q)$ \Comment{New Schema}
\State $f_Q \gets F(MQs(Q))$
\State $SQLs \gets A_G(f_Q, S_Q, Q)$
\State $SQLs_A \gets A_a(SQLs, S_Q, Q)$ \Comment{Align SQLs to $Q$ and $S_Q$}
\State $SQL_R \gets A_R(SQLs_A, S_Q)$ \Comment{Refinement of SQLs}

\State \Return $SQL_R$
\end{algorithmic}
\end{algorithm}

\subsection{Optimization}

% OpenSearch-SQL虽然取得了显著效果，但仍有很多优化空间。当前，我们在OpenSearch-SQL中并未充分关注Prompt细节调整以及列和值的精细选择。此外，在生成任务中，我们仅使用了一条Prompt作为LLM生成SQL候选集的指令，没有进行额外优化。另一方面，SFT模型在提升Text-to-SQL效果方面也有巨大潜力。研究如CHESS、MCS-SQL和Distillery等已经验证了这些方法的有效性，因此进一步优化可以从这些方向着手。
% 在OpenSearch-SQL自身的方法层面，我们认为Few-shot的具体形式不限于Query-CoT-SQL对，还存在其他选择，这为提升效果提供了潜力。同时，Alignments的研究还处于初期阶段，这意味着在提升Text-to-SQL任务性能上仍有进一步发展的空间。

OpenSearch-SQL has achieved significant results, yet there is still plenty of room for optimization. Currently, we haven't fully focused on the detailed adjustments of prompts and the precise selection of columns and values in OpenSearch-SQL. Moreover, in the generation tasks, we only use a single prompt as the instruction for the LLM to generate a SQL candidate set, without additional optimization. On the other hand, the SFT model shows great potential in improving Text-to-SQL performance. Research such as CHESS \cite{chess}, MCS-SQL \cite{mcssql} and distillery \cite{Distilgpt4} has already demonstrated the effectiveness of these methods, suggesting further optimization can be pursued in these directions.

In terms of methods within OpenSearch-SQL, we believe that few-shot approaches are not limited to Query-CoT-SQL pairs; there are other options available, offering potential for enhanced performance. Additionally, the research on alignments is still in its early stages, indicating there is room for further development in improving Text-to-SQL task performance.



\section{Experiments}
\label{sec:experiments}
% 在这一章中，我们通过实验展示OpenSearch-SQL的效果，以及其中的各个模块的作用。
In this section, we demonstrate the effectiveness of OpenSearch-SQL through experiments and explore the role of its various module.
\subsection{Experimental Setup}
\paragraph{Datasets}
%介绍两个数据集和特点，并进行总结
We show the characteristics of these two datasets in Table \ref{tabdataset}. Considering the specific queries and SQL complexity of the datasets. Bird has relatively fewer types of databases but with more complex database structures and a higher average difficulty of SQL. In contrast, Spider has a rich variety of databases, but the average difficulty of SQL is relatively lower. 

\paragraph{\textbf{BIRD}} \cite{Bird} (Big Bench for Large-scale Database Grounded Text-to-SQL Evaluation) represents a pioneering, cross-domain dataset that examines the impact of extensive database contents on text-to-SQL parsing. BIRD contains over 12,751 unique question-SQL pairs, 95 big databases with a total size of 33.4 GB. It covers more than 37 professional domains, such as blockchain, hockey, healthcare, and education, etc. 
%BIRD数据集在2004年7月4日release了一个更干净版本的dev. 但是出于公平考虑，我们仍然以七月前的dev数据对结果进行评估。
The BIRD dataset released a cleaner version of the dev set on 4 July 2004. However, for the sake of fairness, we will still evaluate our results using the dev data from before July.

\paragraph{\textbf{Spider}}\cite{spider} is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. In Spider, we perform tuning on the dev set, assuming the test set is not accessible.

% 我们在表a中展示了这两个数据集的特点. 从数据集的具体问题来看，

\begin{table}[t]
\centering
\renewcommand\arraystretch{2} 
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccccc}
\hline
Dataset  & train  & dev &test  & domains &databases\\ \hline \hline
Spider  & 8659 & 1034 &2147 & 138 &200 \\ \hline
Bird & 9428   & 1534 & 1789  & 37 &95\\ \hline
\end{tabular}}
\caption{Statistics of the datasets.}\label{tabdataset}
\end{table}

\paragraph{\textbf{Evaluation}}
%根据BIRD\cite{Bird}和Spider的评估标准Test-suite\cite{test-suite}，我们共评估三个指标： execution accuracy (EX), valid efficiency score (VES) and exact match accuracy (EM) to evaluate text-to-SQL models confronted with real-world scenarios with large database contents. 
% \textbf{Exact Match Accuracy (EM)} decompose each SQL into several clauses, and conduct set comparison in each SQL clause. 这个指标更关注SQL各个子句的结构。 BIRD的官方指标关注EX和VES，Spider的官方指标关注EX和EM
Following the evaluation criteria from BIRD \cite{Bird} and the Spider test suite \cite{test-suite}, we assessed two metrics: \textbf{Execution Accuracy (EX)} and \textbf{Reward-based Valid Efficiency Score (R-VES)}. \textbf{EX} is defined as the proportion of identical execution results between the predicted SQL and the gold SQL. The purpose of \textbf{R-VES} is to measure the execution efficiency of the predicted SQL when performing the same task.

\paragraph{\textbf{Baselines}}
We selected the LLM-driven methods from the BIRD and Spider rankings as baselines:
\begin{enumerate}

\item \textbf{GPT-4} \cite{openai2024gpt4} employs a zero-shot text-to-SQL prompt for SQL generation.

\item \textbf{DIN-SQL} \cite{dinsql} divides questions into different types through multiple modules and instructs LLM to generate the final SQL through various prompts.

\item \textbf{DAIL-SQL} \cite{DAILsQL} assists LLMs in generating SQL by selecting similar question-SQL pairs for different questions as few-shot examples.

\item \textbf{MAC-SQL} \cite{macsql} simplifies the challenges LLMs face by using sub-databases and sub-questions to generate SQL.

\item \textbf{MCS-SQL} \cite{mcssql} generates multiple sets of SQL using multiple prompts and employs a unified Multiple-Choice Selection (MCS) to select the final SQL.

\item \textbf{C3-SQL} \cite{c3} has constructed a systematic Zero-shot Text-to-SQL approach through three modules: Clear Prompting (CP), Calibration with Hints (CH), and Consistent Output (CO).

\item \textbf{CHESS} \cite{chess} improves the effectiveness of SQL generation by constructing efficient retrieval and schema pruning methods to reduce the interference of redundant information.

\item \textbf{Distillery} \cite{Distilgpt4} suggests that the importance of schema linking has diminished with the development of LLMs, and the fine-tuned SFT GPT-4o has achieved optimal results.
\end{enumerate}

\paragraph{\textbf{Implementation Details}}
% 为了作为一个强大的基线，我们选择的模型的具体版本包括：GPT-4 32K\cite{GPT-4}，Qwenmax-longtext，Claude，Llama3. 检索模型则使用的是Mpnet\cite{song2020mpnet}. 在实验的标准阶段我们使用模型的温度为0，在纠错阶段的温度为1.0，索引Fewshot的数量在{0,3,5,7,9}中选取，过滤值索引的阈值为0.8，并且最多展示的相似值数量在{1,3,5}中选择。
% 在
% 出于效率和可对照性，我们在BIRD官方公布的MIN DEV上进行消融实验。消融实验的目的是检验OpenSearch-SQL,v2在各个模型上的泛化性，以及验证方法中各个子模块的效果。
To demonstrate the capabilities of OpenSearch-SQL v2, the specific versions of the models we have chosen are GPT-4o-0513 \cite{openai2024gpt4} for generation, and bge-large-en-v1.5 \cite{bge_embedding} for retrieval.

For efficiency and comparability, we conducted ablation experiments on the MIN DEV officially released by BIRD. The purpose of these ablation experiments is to evaluate the generalization ability of OpenSearch-SQL across various models, as well as to verify the effectiveness of each submodule in the method.
During the \textbf{Extraction}, we set the temperature from 0, in \textbf{Generation} and \textbf{Refine} stage, the temperature is set to 0.7. The number of few-shot examples is selected from $N=\{0,3,5,7,9\}$ , with a filtering index threshold value of $0.65$, and the maximum number of SQL votes is chosen from $K = \{1, 7, 15, 21\}$.

\subsection{Main Result}
% 为了展现OSSQL的效果，我们展示了其OSSQL在Spider和BIRD上的效果，OSSQL在两个榜上都达到了SOTA级别的表现，这展示了我们方法的稳定性和优越性.在表\ref{main result2}中，我们展示了在BIRD数据集上我们方法的效果和baseline方法的比较。我们方法的立意是检验多agents的效果，所以我们只展示当前榜单中不使用SFT的方法。另外，我们在表\ref{main result}中展示了Spider数据集上的效果。由于Spider榜单自2024年2月开始不再接受新的结果，所以Spider上的结果由榜单上的结果和论文中展示的结果共同组成。此外，BIRD数据集在7月份重新清洗了Dev集，这可能导致Dev集上的比较存在误差，为了能和之前的测评结果对比，我们仍然使用7月前的数据集进行实验。
To demonstrate the effectiveness of OpenSearch-SQL, our performance on the Spider and BIRD datasets has achieved state-of-the-art (SOTA) levels, highlighting the stability and superiority of our approach. In Table \ref{main result}, we compare the effectiveness of our method with the baseline methods in the BIRD dataset. Furthermore, Table \ref{main result2} presents the experimental results on the Spider dataset. Since the Spider leaderboard will no longer accept new submissions starting February 2024, the results are composed of current leaderboard data and those reported in relevant papers. Notably, the BIRD dataset underwent a cleansing of the devlopment set in July, which might lead to discrepancies in comparisons. To ensure comparability with previous evaluations, we have continued to use the dataset before July for our experiments.
% 我们在表\ref{main result2}中展示了Bseline和我们方法在BIRD数据集上的表现, 表中的结果完全来自于BIRD榜单之中。在提交时，我们的方法在BIRD的验证集上达到了69.3\%的EX，在BIRD的holdout 测试集上达到了72.28的EX,和69.36\%的R-VES, 都取得了第一。 值的注意的是, 我们的方法没有进行任何微调训练来定制化模型。同时，我们还展示了不进行Self-Consistency \& votes时OpenSearch-SQL的表现，可以看出即使只使用单一的生成结果，OpenSearch-SQL取得了67.8的执行准确率，仍然在验证集上保持着第一的表现。
 \paragraph{\textbf{BIRD Results}} We present the performance of Baseline and OpenSearch-SQL, v2 on the BIRD dataset in Table \ref{main result2}. The results in the table are derived entirely from the BIRD leaderboard. At the time of submission, our method achieved 69.3\% EX on the BIRD development set and 72.28\% EX and 69.36\% R-VES on the BIRD holdout test set, all ranking first. It is noteworthy that our method did not involve any fine-tuning to customize the LLM. Meanwhile, we demonstrated the performance of OpenSearch-SQL without applying Self-Consistency \& Vote. It can be seen that even using a single generation SQL, OpenSearch-SQL achieved an EX of 67.8, still maintains the top performance on the development set. 

 % 在图\ref{difficulty}中我们比较了我们方法在各个难度上的表现，可以看出Self-Consistency\& Vote对于困难的问题提升的最明显，达到了7.64\%的绝对差距，而对于简单和中等的问题没有显著的区别。这反映了随着问题难度的增加，大模型被幻觉影响的程度也相应增加。
In Figure \ref{difficulty}, we compare the performance of our method across various difficulty levels. It is evident that Self-Consistency \& Vote shows the most significant improvement for difficult problems, achieving an absolute difference of 7.64\%. However, there is no notable difference for easy and medium problems. This indicates that as problem difficulty increases, the susceptibility of large models to hallucinations also rises.

\begin{table}[h]
\Large
\centering
\renewcommand\arraystretch{1.5}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cc|cc}
\hline
Method & \multicolumn{2}{c|}{EX} & \multicolumn{1}{c}{R-VES} \\
&   dev& test &  test \\ \hline \hline
GPT-4   &46.35& 54.89 & 51.57 \\
DIN-SQL + GPT-4  & 50.72 & 55.90 &  53.07\\ 
DAIL-SQL + GPT-4  & 54.76 & 57.41 & 54.02\\
MAC-SQL + GPT-4  & 57.56 & 59.59&57.60\\
MCS-SQL + GPT-4  & 63.36 &65.45 & 61.23\\
CHESS & 65.00 &66.69& 62.77\\
Distillery + GPT-4o(ft)  & 67.21 &71.83 & 67.41\\\hline
 % \textbf{OpenSearch-SQL, v1 + GPT-4}  &61.34 & 64.95&  -\\
 OpenSearch-SQL + GPT-4& 66.62 & - &-\\
 OpenSearch-SQL + GPT-4o & 67.80 & - &-\\
 {\normalsize w/o Self-Consistency \& Vote}&  &  &\\
 \textbf{OpenSearch-SQL + GPT-4o}  &\textbf{69.30} & \textbf{72.28}&  \textbf{69.36}\\\hline
\end{tabular}
}
\caption{Execution accuracies (EX) and Reward-based Valid Efficiency Scores (R-VES) on the BIRD dev and test sets. The result is taken from the leaderboard.}\label{main result2}
\end{table}


 \begin{figure}
     \centering
     \includegraphics[width=1.0\linewidth]{figures/difficulty.png}
     \caption{Comparison of the impact of using Consistency \& Vote across different difficulty levels.}
     \label{difficulty}
 \end{figure}

% 为了展示我们的方法的泛化性，我们也在Spider 的 test集上检验了方法的效果。我们使用方法的默认配置, 除了针对数据集格式的必要调整，不对方法做其他改变。由于Spider榜单不再更新，并且公开了test集，于是我们从baseline方法的论文和榜单中提取结果，其中提取自论文的结果用$*$标记。表\ref{main result}展示了方法的效果,我们的方法达到了87.0\%的执行准确率，在当前的方法中排名第一。


\paragraph{\textbf{Spider Results}} To demonstrate the generalizability of OpenSearch-SQL, we also evaluated its performance on the Spider test set. We used the default configuration of our method, making no other changes, except for necessary adjustments to accommodate the data set format. Since the Spider leaderboard is no longer updated and the test set is publicly available, we extracted results from the baseline method's paper and leaderboard, with results extracted from the paper marked with a $*$. Table \ref{main result} shows the performance of the methods, where our method achieved an execution accuracy of 87.1\%.
\begin{table}[th]
\centering
\tiny
\renewcommand\arraystretch{1.2}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c}
\hline
Method & \multicolumn{1}{c}{EX}  \\
 \hline \hline
GPT-4    & 83.9  \\
C3+ChatGPT & 82.3 \\ 
 DIN-SQL  & 85.3 \\ 
 DAIL-SQL+GPT-4   & 86.6\\
 MAC-SQL+GPT-4  &82.8*\\
 MCS-SQL+GPT-4  &89.6* \\
 CHESS & 87.2*\\
 Distillery + GPT-4o(ft)  & -\\\hline
 OpenSearch-SQL+GPT-4  & 86.8\\
 \textbf{OpenSearch-SQL+GPT-4o}  & 87.1\\\hline
\end{tabular}
}
\caption{Execution accuracy (EX) for the Spider test sets. * denotes that the result is taken from their paper rather than from the leaderboard.}\label{main result}
\end{table}

% \subsection{Model Study}

% %  我们的方法不依赖于Post-traing，我们想在这个模块上探究，我们的方法在各个主流LLMs上的作用，这不仅可以检验该方法的泛化性，同时基于其的表现，也可以便于人们在实际应用中根据效果、费用选择对应场景中性价比更高的选择。因此我们基于使用量，可用性、成本、模型基础效果，选择了GPT4o，Gemini 1.5 pro，DeepSeek-V2.5，QwenMax和Claude 3.5 sonnet
% % 如表\ref{model ablation}所示，为了展示我们方法的泛化性，我们尝试哦了多个大基座模型做对比，包括GPT-4o，Qwenmax，deepseek V2.5。表中展示了各个模型的使用OpenSearch-SQL, v2的效果
% As shown in Table \ref{model ablation}, to demonstrate the generalization of our method, we attempted comparisons with multiple large foundation models, including GPT-4o, Qwenmax, and Deepseek V2.5. The table presents the performance of each model using OpenSearch-SQL, v2.


% 总体来说，在Text-to-SQL任务上，各个大模型的差距不断缩小，当我们以GPT-4为baseline时，XXX效果——————，费用
\begin{table*}[h]
\centering
\renewcommand\arraystretch{1.5} 
% 其中EX_G表示生成阶段得到SQL的准确率，EX_R表示Self-Consistency \& vote之前的SQL效果，EX代表所有流程完成后的效果
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cc|ccc|ccc}
\hline
Pipeline Setup & $EX_G$  & $\Delta EX_G$ & $EX_R$ & $\Delta EX_R$ &$EX_R-EX_G$& $EX$ & $\Delta EX$ & $EX-EX_R$ \\ \hline \hline
Full pipeline & 65.8 & - & 68.2 & -&$+2.4$ & 70.6 & -&$+2.4$ \\ \hline
w/o \textbf{Extraction} & 61.6 & $-4.2$ & 66.2 & $-2.0$ & $+4.6$ & 67.4 & $-3.2$&$+1.4$ \\ 
w/o Values Retrieval & 64.4 & $-1.4$ & 66.6 & $-1.6$& $+2.2$ & 69.2 & $-1.4$ &$+2.6$\\
w/o column filtering & 63.2 & $-2.6$ & 65.0 & $-3.2$ &$+1.8$& 68.6 & $-2.0 $&$+3.6$\\\hline 
w/o Info Alignment & 62.8 & $-3.0$ & 67.6 & $-0.6$&$+4.8$ & 68.6 & $-2.0 $&$+1.0 $\\ \hline
w/o \textbf{Generation} & - & - & - & - & - & - &-&-\\ 
w/o Few-shot & 60.4 & $-5.4$ & 63.0 & $-5.3$ &$+2.6$& 66.0 & $-4.6$&$+3.0$ \\
w/o CoT & 63.0 & $-2.8$ & 66.2 & $-2.0$ &$+3.2$& 69.2 & $-1.4$&$+2.6$ \\\hline
w/o Alignments & 65.8 & - & 67.0 &$-1.2$&$+1.2$  & 69.6 & $-1.0$ &$+2.6$\\ \hline
w/o \textbf{Refinement} & 65.8 & - & 67.0 &$-1.2$&$+1.2$ &67.0 &- &- \\ 
w/o Correction & 65.8 & - & 67.0 &$-1.2$ &$+1.0$ & 69.8 & $-0.8$&$+2.8$ \\ 
w/o Few-Shot & 65.8 & - & 67.6 & $-0.6$&$+1.6$ & 69.4 & $-1.2$&$+1.8$ \\ 
w/o Self-Consistency \& Vote & 65.8 & - & 68.2 & - &-& 68.2 & - &-\\ \hline \hline
\end{tabular}}
\caption{The execution accuracy (EX) of the pipeline by removing each component on the Mini-Dev set. $EX_G$ represents the accuracy of a single SQL obtained from the \textbf{Generation}, $EX_R$ indicates the performance of a single SQL before Self-Consistency \& voting, and EX refers to the results after all processes are completed.}
\label{component}
\end{table*}                                                                                
\subsection{Modular Ablation}

% 表\ref{component}展示了方法中各个模块被去掉之后的Execution accuracy(EX)。我们展示了Extraction、Generation、Refinement，和Alignment四个模块的作用，同时还研究了一些比较重要的子模块的作用，包括:Retrieve、Few-shot、CoT和self-consistency \& vote等。 特别的，Self-consistency &vote 是在多个SQL中进行选择且是整个SQL的最后一步，因此为了更清楚的显示各个模块对单个SQL优化的效果，我们将这一部分的SQL的EX单独展示。
Table \ref{component} provides a detailed overview of the Execution Accuracy (EX) when each module is removed from the method. We specifically examine the roles of the \textbf{Extraction}, \textbf{Generation}, \textbf{Refinement}, and \textbf{Alignment} modules. Furthermore, we investigate the impact of key submodules such as Retrieve, Few-shot, Chain of Thought (CoT), and Self-consistency \& Vote. Notably, Self-consistency \& Vote functions as the final step in the selection among multiple SQLs. To more clearly illustrate the effect of each module on the optimization of individual SQL, we present the EX results of this portion separately.

% 具体来说， 为了展示方法中各个模块的作用，我们首先记录了OpenSearch-SQL最后生成SQL的EX,但是由于Consistency \& vote 机制是从多个SQL中选择一个SQL，所以每个模块中对单个SQL的影响程度都会被影响，因此我们继续记录了生成阶段得到的SQL的$EX_G$，Consistency\&vote阶段开始之前SQL的$EX_R$。这两个值直接记录了单个SQL的受到各个模块的影响，更能反应具体模块于单个SQL的改变。 
To thoroughly demonstrate the roles of each module in the method, we first recorded the Execution Accuracy (EX) of the SQLs ultimately generated by OpenSearch-SQL. Given that the Consistency \& Vote mechanism selects from multiple SQLs, it may influence the evaluation of the impact of each module on an individual SQL. Therefore, we also recorded the accuracy of the execution of the SQLs obtained at the generation stage ($EX_G$) and prior to the Consistency \& Vote stage ($EX_R$). These two metrics directly capture the effect of each module on individual SQLs, providing a more precise assessment of each module's influence.
% 如表所示，从左到右展示了EX_G,EX_R,EX,从上到下按流程执行顺序记录了Extraction、Generation、Refine，以及Alignment几个Agent的作用
% 从表中我们可以观察到:

As shown in the table \ref{component}, from left to right, we present $EX_G$, $EX_R$, $EX$. From top to bottom, the roles of the Extraction, Generation, Refinement, and Alignment modules are recorded in sequence.
From the table, we can observe:
\begin{itemize}
% SQL的EX随着流程的推进呈现单调递增，这意味着不同模块在优化SQL方面提供了不同的积极影响，也证实了每个模块的必要性。
    \item The EX of SQLs increases monotonically as the workflow progresses, indicating that different modules have distinct positive impacts on SQL optimization, confirming the necessity of each module.
    % 在无Extraction的情况下直接使用完整的数据库模式，不进行值检索和列过滤。可以看出，单个SQL在值检索和列过滤带来的提升之和大致等于Extraction带来的提升，这表明二者作用交叉较少。
    \item Without Extraction, the complete database schema is used directly, bypassing values retrieval and column filtering. It can be observed that the combined improvement from values retrieval and column filtering for a single SQL is approximately equal to the improvement provided by Extraction, suggesting minimal overlap in their roles.
    % Few-shot在生成阶段产生了显著的效果，并在Refinement阶段进一步提升了最终SQL的表现，展示了动态Few-shot方法的重要性。
% 我们认为这说明few-shot可以极大的提升模型的上限，同时也能提升模型生成结果的稳定性。对于few-shot的进一步分析，会在接下来的实验中进行。
    \item Few-shot demonstrates a significant effect during the generation phase and further enhances the final SQL performance during the Refinement phase, highlighting the importance of a dynamic Few-shot approach. We speculate that few-shot can greatly enhance the model's potential, while also improving the stability of the generated results. Further analysis of few-shot will be conducted in the experiments \ref{fewshot}.
    %在生成阶段，对于生成来说CoT和Few-shot对于单个SQL结果的都产生了显著的增益，这表示Prompt的信息量对于LLM生成SQL对效果有着显著的正相关关系。但同时，可以观察到，在经过Refine和Self-Consistency阶段后，CoT所产生的增益被显著缩小了，我们认为这说明CoT并不能显著的提升模型生成的基础能力，而让生成的结果更有稳定性。除此之外，还可以观察到Refine对没有CoT的生成结果有更大的增益，这也反应了Refine的一部分重要作用是增加生成结果的稳定性。
    \item During the \textbf{Generation}, CoT and Few-shot significantly enhance individual SQL results, indicating a positive correlation between the amount of information in the prompt and the effectiveness of LLM-generated SQL. However, after the \textbf{Refinement}
    and Self-Consistency \& Vote stages, the gains from CoT are notably reduced. We believe this suggests that CoT does not significantly enhance the model's foundational generation capabilities but instead improves the stability of the results. Additionally, it is observed that \textbf{Refinement} provides $+3.2\%$ to results generated without CoT, highlighting its crucial role in increasing result stability.
    % Alignment模块在各个阶段都起到了积极的正向作用，特别是对单个SQL的表现提升更大，展现了其独特的优化潜力和空间。特别的，对于生成阶段后的Alignments，Alignments对齐后的结果可以帮助Correction 模块正确修改错误的SQL，而不仅仅是让这些SQL可执行
    \item The Alignment module has played a positive role at various stages, significantly enhancing the performance of individual SQL, thereby demonstrating its unique optimization potential. Specifically, Alignments applied after the generation phase can assist the Correction module in accurately fixing erroneous SQL, rather than merely making them executable.
    % 可以看出Self-Consistency \& Vote稳定的增加了LLMs表现的上限，根据我们的观察，这种机制主要可以避免由于随机性小概率引起的错误。
    \item It is evident that Self-Consistency \& Vote consistently raise the performance ceiling of LLMs. Based on our observations of the generated SQL, this mechanism primarily helps avoid errors caused by low-probability randomness.
\end{itemize}


                                                          
% 各种fewshot的消融

% 在表\ref{fewshotexamples}中，我们探讨了不同动态Few-shot策略对Text-to-SQL的影响。首先，在生成阶段，我们比较了经典的Query-SQL pair和我们的Query-COT-SQL pair的Few-shot效果，并评估了不使用Few-shot的情况，其次我们也研究了其他阶段Fewshot存在与否的影响。从表中可以归纳出以下几点：
\begin{table*}[h]
\large
\centering
\renewcommand\arraystretch{1.5}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccccc}
\hline
 Method& $EX_{G}$  &  $\Delta EX_{G}$  & $EX_{R}$ &  $\Delta EX_{R}$ & EX & $\Delta EX$ \\ \hline \hline
Query-CoT-SQL pair Few-shot  & 65.8& -& 68.2& -& 70.6& -  \\\hline
w/o Few-shot of Generation & 59.6&$-6.2$ & 63.0&$-5.2$ & 66.0&$-4.6$   \\ 
w Query-SQL pair Few-shot of Generation & 63.0&$-2.8$& 66.2&$-2.0$ & 69.2&$-1.4$ \\ 
 w/o Few-shot of Refinement&  65.8&-&  67.6& $ -0.6$&69.4  &$-1.2$\\ 
 w/o Few-shot of Generation \& Refinement &59.6  &$-6.2$& 62.8 &$-5.4$& 66.0& $-4.6$ \\ 
\hline
\end{tabular}}
\caption{Few-shot Performance Comparison Results.}\label{fewshotexp}
\end{table*}

\subsection{Few-shot}\label{fewshot}

In Table \ref{fewshotexp}, we explore the impact of different dynamic Few-shot strategies on Text-to-SQL. First, during the generation phase, we compare the Few-shot effects of the classic Query-SQL pair with our Query-COT-SQL pair, and we evaluate the scenario without Few-shot. We also investigate the influence of Few-shot in other stages. From the table, we can summarize the following points:
\begin{itemize}
% Few-shot策略本身显著提高了LLMs生成SQL准确性的上限, 无论是Query-SQL Pair的形式还是Query-CoT-SQL pair的形式相比于无few-shot，在每个阶段都有着很大提升，而Refine阶段的few-shot的提升相对较小
    \item The Few-shot strategy significantly raises the upper limit of SQL accuracy generated by LLMs. Whether in the form of a Query-SQL Pair or a Query-CoT-SQL Pair, compared to without few-shot, these formats significantly enhance the performance of LLMs at each stage. However, the improvement from few-shot in the \textbf{Refinement} stage is relatively smaller.
    % 在生成阶段，对于单个SQL查询，Few-shot策略能带来最大的性能提升，其中Query-CoT-SQL pair形式的Few-shot相比于Query-SQL pair形式提高了3.0%的绝对准确率,相比于无Fewshot提升了6.4%的EX
    \item During the \textbf{Generation}, for a single SQL query, the Few-shot strategy provides the greatest performance improvement, with the Query-CoT-SQL pair form achieving a 3.0\% absolute EX increase compared to the Query-SQL pair form, and a 6.4\% increase in EX compared to the scenario without Few-shot.
    % 在Refine阶段和self-consistency作用下，不同形式的Few-shot之间的差距逐渐缩小，表明在SQL优化的内容在不同阶段存在一定的重复性。
    \item In \textbf{Refine} and under the influence of \textbf{self-consistency \& vote}, the gap between different forms of Few-shot gradually narrows, indicating that there is some redundancy in the SQL optimization content in different stages.
    \item Although the Few-shot strategy has limited optimization effects on individual SQL queries during the \textbf{Refine}, it can improve the final EX of consistency and voting, suggesting that the Few-shot-driven SQL correction results exhibit higher consistency with one another.
\end{itemize}





\subsection{Self-Consistency \& Vote}
% 我们的实验中使用单个prompt生成21个结果作为答案，为了探究对于单个prompt来说，beamsearch 生成多少SQL来进行self-consistency是最合理的，我们设计了这个实验，如图\ref{}所示。
% 我们用GPT4o和GPT4o-mini进行了实验，以反应不同参数级别模型的表现。从图中可以看出，对于GPT4o随着候选数量的增加，EX也在不断增加，而GPT4o-mini在7和15达到了最佳效果。这说明，对于参数量足够的大模型，在完成self-consistency时可以放心的增加生成答案的数量，而对于参数量较小的模型则需要控制其输出结果的数量，较多的答案会对小模型self-consistency的结果产生负面影响。
In our result, we used a single prompt to generate 21 results as answers. To investigate the optimal number of SQL outputs generated by beam search for self-consistency with a single prompt, we designed the experiment shown in the figure \ref{fig:enter-label}. 

We conducted experiment with GPT-4o and GPT-4o-mini to compare the performance of LLMs with different parameter levels, and the number of candidates is taken from $N=\{1, 3, 7,15, 21\}$. As observed in the figure, the EX value for GPT-4o consistently increases with the number of candidates. In contrast, GPT-4o-mini achieves optimal results with 7 and 15 candidates. This indicates that for models with sufficiently large parameters, the number of generated answers can be safely increased to achieve self-consistency \& vote. However, for smaller models, the number of outputs should be controlled to prevent negative impacts on self-consistency due to an excessive number of answers.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/vote.png}
    \caption{Comparison of the impact of different numbers of candidates on model performance}
    \label{fig:enter-label}
\end{figure}

\subsection{Execution Cost}
% 在表中，我们用GPT4-o对OpenSearch-SQL的时间和资源消耗做了简单的估计。总的来说，整个流程的执行时间主要依赖于LLM的调用次数、检索和数据库执行所花的时间。在实践中使用HNSW算法可以极大降低检索的延时，因此主要时间花在LLM回复和数据库执行的消耗上。具体来说，其中Extraction和Generation的Token主要来自于Database Schema和Fewshot，其中Generation的由于使用BeamSearch生成的Token要比Extraction更多，所以也需要更长的执行时间。除了SELECT Agent每次都要触发以外,其他Alignment只有在SQL有问题时才会触发，因此大部分问题中，他们基本不耗时。而对于Refinement，Correction的时长主要来自于SQL执行和LLM纠错，而self-consistency\&vote基本不耗时。

In Table \ref{cost}, we provide a simple estimate of the time and resource consumption for using OpenSearch-SQL with GPT-4o. Generally, the execution time for the entire process primarily depends on large language model (LLM) calls, retrieval time, and SQL execution time. In practice, using HNSW \cite{hnsw} can significantly reduce retrieval latency, thus shifting the main time consumption to LLM responses and SQL execution.

Specifically, the tokens for extraction and generation are mainly derived from the database schema and a few examples. The generation phase takes longer since it uses BeamSearch, which generates more tokens than the extraction phase. Apart from the SELECT alignment, which is triggered every time, other alignment steps are only activated when there are issues with the SQL, so they are not time-consuming in most cases. As for the refinement phase, the time is mainly due to SQL execution and LLM corrections, while Self-consistency \& Vote contribute minimally to the time consumption.

\begin{table}[th]
\centering
\tiny
\renewcommand\arraystretch{1.2}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cc}
\hline
 Modular& Time(s) & Token \\ \hline
\textbf{Extraction} &4-9 &5000-10000\\
Entity \& Column & 4-6& 5000-10000\\ 
Retrieval & 0-1& -\\\hline
\textbf{Generation}& 5-15 &4000-8000\\ \hline
\textbf{Refinement}& 0-25&0-5000\\
Correction& 0-25&0-5000\\
Self-consistency \& Vote & <0.01s &-\\ \hline
\textbf{Alignments}&0-15 & 500-2000\\
SELECT Alignment& 1-3& 500-600\\
Agent Alignment &0-7 & 100-500\\
Style Alignment &0-5& 100-500\\
Function Alignment &0-4 &100-500\\ \hline
Pipeline&7-60s & 9000-25000\\ \hline

\end{tabular}
}
\caption{Exection Cost Performance}\label{cost}
\end{table}


\subsection{CoT}
% 在这一部分中，我们对我们设计的CoT方法做了细致的分析，并简单比较了OpenSearch-SQL中设计的结构化CoT和常用的CoT以及不使用CoT的区别。为了减少其他因素的干扰，在这个实验中，我们将生成阶段的Few-shot，Alignments以及Refinement中的Correction都取消，以直观的评估CoT对生成SQL的直接影响。如表\ref{CoT}所示，可以看出，使用CoT的方式是比不使用CoT有明显提升的，而结构化的CoT提升相比非结构化的CoT方式更为明显。同时，观察$EX_{vote}$的结果，可以看出进行投票选择后，使用CoT比不使用CoT有更大的提升，同时，结构化的CoT方法取得了更多的相对提升。
% 总的来说，我们发现，结构化的CoT取得了最好的效果，如果是通过采样多个答案进行投票，结构化的CoT相比其他方法还有着更高的相对提升。这展示了CoT设计对LLM完成任务的重要影响。

In this section, we provide a brief analysis of the CoT method we designed and compare the performance of structured CoT designed for OpenSearch-SQL with the unstructured CoT approach( i.e., "let's think step by step") as well as scenarios without CoT. To isolate the impact of CoT, we eliminate few-shot learning during the \textbf{Generation} phase and focus solely on the execution accuracy when generating a single SQL( $EX_G$) and multiple SQLs with Self-consistency \& Vote( $EX_V$). This allows for a straightforward assessment of CoT's direct impact on SQL generation.

As shown in Table \ref{CoT},t is apparent that utilizing CoT leads to a significant improvement in SQL generation compared to not using CoT. Moreover, structured CoT demonstrates more substantial gains than unstructured CoT. Furthermore, examining the results of $EX_{V}-EX_G$, we observe that the implementation of \textbf{Self-consistency \& Vote} provides even greater improvements than simply using CoT alone, with structured CoT achieving a higher relative enhancement.

In summary, structured CoT achieved the optimal results. When sampling multiple answers for voting, structured CoT exhibits a more significant relative improvement compared to other methods.


\begin{table}[th]
\centering
\tiny
\renewcommand\arraystretch{1.2}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccc}
\hline
 Modular& $EX_G$ & $EX_{V}$ &$EX_{V}-EX_G$\\ \hline
 w/o CoT& 57.6&59.2 & 1.6\\ 
 Unstructured CoT &58.2 &63.0&4.8 \\
 Structured CoT& 58.8 &65.0 &6.2\\ \hline
\end{tabular}
}
\caption{CoT Performance Comparison Results}\label{CoT}
\end{table}

\input{RelatedWork }

\section{Conclusion}
\label{sec:conclusion}
\input{conclusion}


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}
%%
%% If your work has an appendix, this is the place to put it.

\end{document}