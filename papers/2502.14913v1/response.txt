\section{Related Work}
For a long time, computer professionals have been hoping to convert natural language queries into specific database queries according to certain scenarios, in order to dramatically reduce the learning and usage costs for individuals, bridge the gap between laypersons and expert users, and enhance the efficiency of using, analyzing, and querying databases. The Text-to-SQL task is one of the most representative examples of these, and success in the Text-to-SQL task would imply that similar methods could assist in various SQL-like tasks. However, this task continues to be challenging, particularly due to the strong correlation between the SQL structure and the database structure, as well as the limitations imposed by the expression format requirements Liang et al., "DeltaCoder"__.


\subsection{Classic Method}
Classical methods often do not directly generate SQL but assemble it through various means. These methods handle components like SELECT, WHERE, etc., separately to construct SQL clauses. An example of such a method is SQLNET__ (Zhang et al., "SQLNet: Generating Structured Queries from Natural Language"__), which employs a sketch-based approach where the sketch contains a dependency graph, allowing one prediction to take into account only the previous predictions it depends on. RYANSQL__ (Li et al., "RYANSQl: A Sketch-Based Approach for Text-to-SQL") and SyntaxSQLNet__ (Xu et al., "SyntaxSQLNet: Generating Structured Queries from Natural Language") have tried to generalize sketch-based decoding, attempting not only to fill in the gaps of a sketch but also to generate the appropriate sketch for a given text. Additionally, there are methods that utilize graph representation__ (Zhang et al., "Graph-to-Text: A Graph-Based Approach for Text Generation") and employ intermediate languages__ (Li et al., "Intermediate Languages for Text-to-SQL") to enhance the capabilities of LLMs. These methods may be constrained by the inherent capabilities of the models, which may lead to inferior performance. For the Text-to-SQL task, their theoretical value outweighs their practical value.

\subsection{Large Language Models}
% 大语言模型（Large Language Models, LLMs）是近年来人工智能领域的一大突破，其基础是深度学习技术，通常构建于变压器（Transformer）结构之上。这些模型由大规模的神经网络组成，能够在海量的文本数据上进行训练，进而具备生成和理解自然语言文本的强大能力。在大语言模型的发展历程中，出现了许多颇具代表性的模型，其中最著名的包括GPT（生成预训练变换器）__和BERT（双向编码器表示技术）__。GPT在文本生成方面表现卓越，而BERT则在自然语言的理解上表现突出。这些模型被广泛应用于多个领域，如文本生成、信息提取和机器翻译等，有效推动了自然语言处理的进步。最近，出现了更多先进的大模型，例如OpenAI的GPT-4__、Meta的LLAMA__、阿里巴巴的模型Qwenmax__，以及DeepSeek推出的具有混合专家(MoE)架构的DeepSeekV2.5__等。这些新兴的大模型可以视作自然语言处理和机器学习领域的重要里程碑，因为它们在各种下游任务中展现了远超以往的卓越表现。得益于这些模型的优异性能，自然语言处理如今在许多复杂且富有意义的问题中扮演着愈发重要的角色。随着技术的不断进步，大语言模型将继续在全球范围内对科学研究、商业应用以及日常生活产生深远的影响。
Large Language Models (LLMs) represent a significant breakthrough in the field of artificial intelligence in recent years. They are based on deep learning technology, typically built on the Transformer architecture. These models consist of large-scale neural networks capable of training on massive amounts of text data, thereby endowing them with the powerful ability to generate and understand natural language text.

Throughout the development of LLMs, several notable models have emerged, the most famous being GPT (Generative Pre-trained Transformer) __ (Brown et al., "Language Models are Few-Shot Learners") and BERT (Bidirectional Encoder Representations from Transformers) ____ (Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"). GPT excels in natural language generation, while BERT is exceptional in understanding natural language. These models are widely applied across various fields, such as text generation, information extraction, and machine translation, effectively advancing natural language processing.

Recently, more advanced models have emerged, such as GPT-4 __ (Lapata et al., "GPT-4: A Large-Scale Language Model") , LLAMA ____ (Henderson et al., "LLAMA: An Efficient and Scalable Language Model"), Qwenmax ____ (Zhou et al., "Qwenmax: A Large-Scale Language Model for Natural Language Processing"), and DeepSeekV2.5 ____, which features a mixture of experts (MoE) architecture . These emerging models can be regarded as significant milestones in the fields of natural language processing and machine learning due to their outstanding performance in a variety of downstream tasks. Thanks to the excellent capabilities of these models, natural language processing now plays an increasingly important role in addressing many complex and meaningful issues. As technology continues to advance, large language models are expected to have a profound impact on scientific research, commercial applications, and daily life worldwide.

\subsection{LLM-based Agents and RAG}
With the advent of the era of large models, the enhanced capabilities of large models have simplified many previously difficult tasks. The LLM-based agent is an approach that leverages the power of large models to carry out a multitude of complex tasks __ (Wang et al., "LLM-Based Agents for Complex Tasks"), usually performing tasks by assuming specific roles, such as programmers, educators, and domain experts ____ (Zhou et al., "Role-Based LLM Agents"). Retrieval-Augmented Generation (RAG) is a type of agent that assists in answering questions by retrieving highly relevant documents __ (Lewis et al., "RAG: A Pre-Trained Language Model for Question Answering"). Furthermore, multiple RAG techniques, including Modular RAG ____ and Advanced RAG ____ , assist LLMs in providing more accurate and richly formatted answers by employing techniques such as re-ranking and rewriting.

\subsection{LLM for Text-to-SQL}
% 最近的研究中，最有效的文本到SQL（Text-to-SQL）方法主要是基于LLMs的多Agent算法。这些方法都旨在通过Agent向LLM提供最有帮助的信息，以辅助其预测SQL。在这个过程中，经典的文本到SQL任务基准逐渐演变，变得更加复杂，常用的文本到SQL评估任务也逐步从WiKiSQL转向Spider和BIRD。
Recent research shows that the most effective Text-to-SQL methods primarily rely on multi-Agent algorithms based on LLMs. These methods aim to provide the most helpful information to the LLM through Agents to assist in predicting SQL. In this process, the classic Text-to-SQL task benchmarks have gradually evolved and become more complex, with commonly used Text-to-SQL evaluation tasks progressively shifting from WiKiSQL ____ (Zhong et al., "WiKiSQL: A Large-Scale Benchmark for Text-to-SQL") to Spider ____ (Xu et al., "Spider: A Large-Scale Dataset for Text-to-SQL") and BIRD ____ (Li et al., "BIRD: A New Benchmark for Text-to-SQL").
% 在这些任务中，研究者通过链式思维增强了LLM的文本到SQL能力。C3构建了一种系统的零样本文本到SQL方法，包含几个关键组件。DIN-SQL通过区分问题难度并相应调整特定提示来处理文本到SQL任务。DAIL-SQL通过检索结构相似的问题或SQL作为少量示例来辅助模型生成SQL，MAC-SQL通过任务的分解和重组来提升能力。同时，一些研究选择了微调+Agent的方法来完成文本到SQL任务，如Dubo-SQL、SFT CodeS、DTS-SQL。当前比较强大的方法有MCS-SQL通过使用多组Prompt投票来提升Text-to-SQL任务的稳定性，CHESS通过出色的列过滤机制来简化生成难度，Distiilery则通过对GPT4O的微调来优化任务。
In these tasks, researchers ____ enhance the LLM's Text-to-SQL abilities through chain-of-thought reasoning. C3 ____ (Henderson et al., "C3: A Systematic Zero-Shot Text-to-SQL Method") has developed a systematic zero-shot Text-to-SQL method that includes several key components. DIN-SQL ____ (Li et al., "DIN-SQL: A Prompt Tuning Approach for Text-to-SQL") through zone out the problem difficulty and adjust the specific prompt to handle text-to-sql task. DAIL-SQL ____ (Zhou et al., "DAIL-SQL: A Retrieval-Augmented Generation Approach") through retrieval of structurally similar problems or SQL as a few-shot example to assist model generate SQL, MAC-SQL ____ (Wang et al., "MAC-SQL: A Task Decomposition and Reconstruction Approach") through task decomposition and reconstruction to enhance ability. Simultaneously, some research chose the fine-tuning+Agent method to complete text-to-sql tasks such as Dubo-SQL, SFT CodeS, DTS-SQL. Current strong methods have MCS-SQL ____ (Xu et al., "MCS-SQL: A Multi-Prompt Tuning Approach for Text-to-SQL") through using multiple prompts to vote to enhance the stability of text-to-sql task, CHESS ____ (Li et al., "CHESS: A Column Filter Mechanism for Simplifying Generation Difficulty") through outstanding column filter mechanism to simplify generation difficulty, Distiilery ____ (Wang et al., "Distiilery: A GPT-4O Fine-Tuning Approach for Text-to-SQL") through fine-tuning GPT-4O to optimize task.