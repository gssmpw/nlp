\subsection{Sample Complexity}
As in \cref{thm:sample2}, we bound the number of samples needed to estimate all $p_k(W,X)$ to within additive $\epsilon$.  The key difference now is that the Random Choice model is only defined for pairs of alternatives. Consider the complete graph $K_m$ on the alternatives, and split the set of edges into $m-1$ matchings. Given a group of voters, we pick one of the $m-1$ matchings. This group of voters deliberates over the $m$ pairs of alternatives in this matching, outputting one alternative for each pair using the Random choice model. Fixing a matching, if we use this matching for $O\left(\frac{\log (m/\delta)}{\epsilon^2}  \right)$ groups, then for every edge $(W,X)$ in the matching, we can estimate $p_k(W,X)$ to an additive error of $\epsilon$ with probability $1-\frac{\delta}{m}$. Repeating this for all $m-1$ matchings, all of the  quantities $p_k(W,X)$ can be estimated with this error. As in \cref{thm:sample2}, with high probability, the alternative $W$ chosen by the Copeland rule run on the tournament graph over samples is such that all alternatives $X$, either $p_k(W,X) \ge 1/2 - \epsilon$, or there exists $Y$ such that $p_k(W,Y) \ge 1/2 - \epsilon$ and $p_k(Y,X) \ge 1/2 - \epsilon$. Since $\zeta_k$ in \cref{eq:opt2} is smooth in the RHS of the constraint, the optimum shifts by at most an additive $O(\epsilon)$. This finally yields the following theorem.

\begin{theorem}
\label{thm:sample1}
Let $d_k$ denote the distortion of the Copeland rule with groups of size $k$ assuming each $p_k(W,X)$ can be estimated exactly. Then, $O\left(\frac{m \log (m/\delta)}{\epsilon^2}\right)$ randomly chosen groups of size $k$ suffice for distortion $d_k + \epsilon$ with probability $1-\delta$.
\end{theorem}

As mentioned after \cref{thm:sample2}, our sampling bounds with constant size groups are not implied by similar bounds for the $1$-median problem, and crucially require voters to output rankings beyond their favorite (or median) alternative.