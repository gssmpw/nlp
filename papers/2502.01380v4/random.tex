\section{Distortion in the Random Choice Model}
\label{sec:random}
We next compute the distortion in the Random Choice model for various values of group size $k$. In contrast to the averaging model, this model is more analytically tractable, and we can numerically compute a good upper bound on distortion for all small $k$. In particular, our main result in \cref{thm:random} is that groups of size $k \le 4$ suffice to break the randomized metric distortion lower bound of $2.11$. In \cref{thm:asymp1}, we also show that the group size is $k = \tilde{O}(1/\epsilon^2)$ for Copeland to achieve distortion $1+\epsilon$. We also show in \cref{thm:sample1} that a sample of $\tilde{O}(m \log m)$ groups suffices for approximating distortion, where $m$ is the number of alternatives.  In \cref{sec:general}, we finally present a generalization of the random choice model, and show how our analysis technique naturally extends to showing distortion bounds for it.

\subsection{Distortion Bounds for Small $k$}
\label{sec:random_small}
Suppose the deliberation has $k$ participants, and is between two arbitrary outcomes $W$ and $X$. For a distribution $D$, let $L$ be the conditional distribution of $-D$ given $D \le 0$, and let $R$ denote the conditional distribution of $D$ given $D > 0$. Let $\alpha = \Pr[D \le 0]$. The objective in \cref{eq:opt} can be written as
$$ \max \ \ (1-\alpha) \cdot \E[R] - \alpha \cdot \E[L].$$

Let $a_1, a_2 \ldots, a_k$ be $k$ i.i.d. samples from $L$ and $b_1, b_2 \ldots, b_k$ be $k$ i.i.d. samples from $R$. Then,
$$ p_k(W,X) = \sum_{\ell = 1}^k {k \choose \ell} \alpha^{\ell} (1-\alpha)^{k-\ell} \E\left[ \frac{\sum_{r = 1}^{\ell} a_r}{\sum_{r=1}^{\ell} a_r + \sum_{q=1}^{k-\ell} b_q } \right],$$
where the expectation is over $a_1,\ldots, a_r \overset{\text{i.i.d.}}{\sim} L$, and $b_1,\ldots, b_q \overset{\text{i.i.d.}}{\sim}  R$. The constraint in \cref{eq:opt} implies the RHS is at least $1/2$. 
Note now that since the RHS is convex in any $b_q$, we can preserve the objective above and increase the RHS when $b_q$ is drawn from a Bernoulli distribution with mean $\E[R]$. We next absorb the mass at $0$ in $R$ into the distribution $L$; call the new distributions $\hat{L}, \hat{R}$. Therefore, $\hat{R}$ is the deterministic value $1$, and $\Pr[\hat{R}] = 1-\alpha$. Then, the objective is
$$ (1-\alpha) - \alpha \E[\hat{L}].$$
Since each $b_q = 1$ now, the constraint on $p_k(W,X) $  becomes
$$ p_k(W,X) \le \sum_{\ell = 1}^k {k \choose \ell} \alpha^{\ell} (1-\alpha)^{k-\ell} \E\left[ \frac{\sum_{r = 1}^{\ell} a_r}{\sum_{r=1}^{\ell} a_r + k-\ell } \right],$$
where the expectation is now over $a_1,\ldots,a_k\overset{\text{i.i.d.}}{\sim} \hat{L}$. Noting that the RHS is concave in $a_r$, by Jensen's inequality, we have
$$ p_k(W,X) \le \sum_{\ell = 1}^k {k \choose \ell} \alpha^{\ell} (1-\alpha)^{k-\ell} \cdot \frac{\ell \cdot \E[\hat{L}]}{\ell \cdot \E[\hat{L}] + k-\ell }. $$

Therefore, using $\omega = \E[\hat{L}]$ and observing that $\omega, \alpha \in [0,1]$, \cref{eq:opt} can be rewritten as:
\begin{equation} 
\label{eq:opt2}
\zeta_k := \max_{\omega, \alpha \in [0,1]} (1-\alpha) - \alpha \cdot \omega \qquad \mbox{s.t.} \qquad %\E_{\ell \sim \mathtt{Bernoulli}(k,\alpha)} 
\sum_{\ell = 1}^k {k \choose \ell} \alpha^{\ell} (1-\alpha)^{k-\ell} \cdot
\left[ \frac{\ell \cdot \omega}{\ell \cdot \omega + k-\ell } \right] \ge \frac{1}{2}.
\end{equation}

For any given $\alpha$, the LHS of the constraint is concave and increasing in $\omega$, while the objective is decreasing in it, so it can be optimized by a binary search over $\omega$ to find the smallest $\omega$ for which the constraint is satisfied. We then run a parametric search over $\alpha$ in increments of $10^{-3}$ to find an approximate global optimum. By \cref{thm:distort1}, the distortion is at most $\left(\frac{1+\zeta_k}{1-\zeta_k}\right)^2$. We numerically compute this for $2 \le k \le 30$, and plot it in \cref{fig1}, and show in the second column in \cref{tab:random}. This yields the following theorem.

\begin{theorem}
\label{thm:random}
    For the Copeland rule applied to the random choice deliberation model with group size $k$, the distortion is at most $3.34$ for $k = 2$, at most $2.31$ for $k = 3$, and at most $1.90$ for $k = 4$.
\end{theorem}

Note that the upper bound for $k = 2$ is significantly lower than the lower bound of $4.414$ for the averaging model in \cref{thm:theta2}. Note further that the distortion for $k = 3$ is well below the distortion of $2.74$ of the best randomized social choice rule without deliberation, and that for $k = 4$ is below the lower bound of $2.11$ on metric distortion without deliberation. %This shows that under a reasonable model of deliberation, a very small group size suffices to break the metric distortion lower bound without deliberation.

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{distortion_rand.png}
        \caption{\label{fig1} Random Choice Model.}
    \end{subfigure}
    \qquad
        \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{distortion_sqrt.png}
        \caption{\label{fig2} Generalized Random Choice, $g(x) = \sqrt{x}$.}
    \end{subfigure}
    \caption{Distortion of the Random Choice model as a function of the group size $k$, for $k \in [2,30]$.}
\end{figure*}

\paragraph{Lower Bound for any  Social Choice Rule.} 
Let $\zeta_k$ be the optimal value to \cref{eq:opt2} for groups of size $k$. Note that this corresponds to the expectation of a valid distribution over $\{-\omega, 1\}$, where $\Pr[-\omega] = \alpha$. By \cref{thm:lb_main}, this implies a distortion of $\frac{1+\zeta_k}{1-\zeta_k}$ for any deterministic social choice rule, and $\frac{1}{1-\zeta_k}$ for any randomized rule. These values are shown in the third and fourth columns in \cref{tab:random}, and show that our upper bounds above are reasonably tight.  % and let the corresponding optimal solution be $\alpha^*_k, \omega^*_k$. Consider an instance with two alternatives $W$ and $X$ located distance $1$ apart, and two locations for the voters. The first location coincides with $X$ and has voter mass $\rho = 1 - \alpha^*_k$. The second location is located on the line connecting $W$ with $X$, at distance $\frac{1-\omega^*_k}{2}$ from $W$ and $\frac{1+\omega^*_k}{2}$ from $X$. On this instance, $W$ beats $X$, so that by the argument in \cref{thm:lb1}, we can assume any deterministic social choice rule chooses $W$. The social optimum is $X$ and the distortion is precisely $\frac{1+\zeta_k}{1-\zeta_k}$, which is the square root of the distortion plotted in \cref{fig1}. This lower bound is shown in the third column in \cref{tab:random}. This shows that our upper bounds are reasonably tight.  We note that for randomized rules,  a similar argument shows that we can assume any rule chooses $W$ (resp. $X$) with probability $1/2$, and has a distortion of at least $\frac{1}{1-\zeta_k}$. This is shown in the last column in \cref{tab:random}.  %which stands in contrast to the bounds for the averaging model.


