\section{Distortion in the Averaging Model}
\label{sec:avg}
We first consider the averaging model. We show in \cref{thm:berry} that the group size of $k = \Theta(1/\epsilon)$ is both necessary and sufficient for distortion $1+\epsilon$. In this model, it is analytically difficult to tightly bound distortion for small $k$; nevertheless, we show that the distortion is tight at $3+\sqrt{2} \approx 4.414$ for groups of size $k=2$ (\cref{thm:theta2}), while it is at most $2.81$ for groups of size $k=3$ (\cref{thm:theta3}). This shows that a very small group size ($k=3$) suffices to beat the deterministic metric distortion lower bound of $3$ in this model. (Note that by the discussion in \cref{sec:prelim}, the Averaging model can be viewed as a deterministic social choice rule.) We finally show in \cref{thm:sample2} that the number of sampled groups needed to achieve an additive $\delta$ approximation to this bound is only $\tilde{O}(\log m)$, where $m$ is the number of alternatives. 

In the averaging model, we choose a set of $k$ random voters to deliberate between two alternatives $W$ and $X$. Let the multiset of their locations be denoted by $S$. We compute $\sum_{i \in S} \B_i(W,X)$. If this is negative, the outcome of the deliberation is $W$, else it is $X$. This corresponds to outputting $\mbox{argmin}_{c \in \{W,X\}} \sum_{i \in S} d(i,c)$.  This model is equivalent to the model where the group deliberates over all $m$ alternatives, and outputs the ranking consistent with the total distance. This implies $c_1$ is ahead of $c_2$ in the ranking if $ \sum_{i \in S} d(i,c_1) \le \sum_{i \in S} d(i,c_2)$, and our analysis extends directly to that case.

We now specialize \cref{eq:opt} for this model. Suppose there are two alternatives $W,X$ between which the deliberation happens. Let $\phi_i = \B_i(W,X)$. The event of $W$ being the winner maps to the condition $\sum_{i \in S} \phi_i \le 0$. Therefore, we seek a distribution $D$ whose support is $[-1,1]$ that solves the following problem:
\begin{equation}
\label{eq:opt_avg}
\theta_k :=  \ \ \max_{D} \E[D] \qquad \mbox{s.t.} \qquad \Pr_{\{a_1,a_2,\ldots,a_k\} \sim D} \left[ \sum_{i=1}^k a_i \le 0 \right] \ge 1/2. 
\end{equation}
where the probability is over a set of $k$ independent samples $a_1,\ldots,a_k$ drawn from $D$.


\subsection{Asymptotic Behavior of Distortion in Group Size $k$}
We will now use the Berry-Esseen theorem~\cite{feller1971introduction} to show a tight bound of  on the asymptotic behavior of $\theta_k$ in $k$. This will yield the following theorem.

\begin{theorem}
\label{thm:berry}
For any $k \ge 2$,  $\theta_k = O\left(\frac{1}{k}\right)$. By \cref{thm:distort1}, this means the distortion of the Copeland rule in the averaging model of deliberation using groups of size $k$ is $1 + O\left(\frac{1}{k}\right)$. In particular, for any $\epsilon > 0$, groups of size $k = O\left(\frac{1}{\epsilon}\right)$ are sufficient for distortion $1+\epsilon$.  
%$1 + O\left(\frac{1}{k}\right)$. In particular, for $\epsilon > 0$, groups of size $k = O(1/\epsilon)$ suffice for distortion $1+\epsilon$.
%at most $1 + O\left(\frac{1}{k^{2/3}}\right)$. In particular, for any $\epsilon > 0$, groups of size $k = O\left(\frac{1}{\epsilon^{3/2}}\right)$ suffice for distortion $1+\epsilon$. 
%Further, for any odd $k$, there exists an instance on two alternatives with distortion at least $1+\frac{2}{k}$, so that groups of size $k = \Omega(1/\epsilon)$ are also necessary for distortion $1+\epsilon$.
\end{theorem}
\begin{proof}
Let $\theta_k = \mu = \E[D] \ge 0$, and let $D_k = \frac{\sum_{i=1}^k a_i}{k}$, so that $\E[D_k] = \mu$. Let $\sigma^2 = \mbox{Var}[D]$.  We have $\mbox{Var}[D_k] = \sigma^2/k$. The condition $W \succ X$ implies $\Pr[D_k \le 0] \ge 1/2$, which means the median is at most $0$. Noting that the gap between the median and mean of any distribution is at most the standard deviation~\cite{MU}, this implies %Since $\E[D_k] = \mu > 0$, we have 
$$\mbox{Var}[D_k] = \frac{\sigma^2}{k} \ge (0 - \mu)^2  \qquad \Rightarrow \qquad \sigma \ge \mu \sqrt{k}.$$
Since $\sigma \le 1$ for a random variable bounded in $[-1,1]$, this already implies $\theta_k = \mu \le \frac{1}{\sqrt{k}}$. 

We will now improve this bound using the Berry-Esseen inequality~\cite{feller1971introduction} applied to the average of $k$ random variables distributed as $D - \mu$. Note that $\E[D-\mu] = 0$, and $\E[(D-\mu)^2] = \mbox{Var}[D]= \sigma^2$. Further since $\mu, |D| \le 1$, we have $\E[|D|^3] \le \E[D^2]$, so that
 \begin{align*}
    \E[|D-\mu|^3]  \le & \ \E[(|D| + \mu)^3] \\
     = &\  \E[|D|^3] + 3 \mu \cdot \E[D^2] + 3 \mu^2 \cdot \E[|D|] + \mu^3 \\
     \le &\  4 \cdot \E[D^2] + 4 \cdot \mu^2 \\
    = & \ 4 \cdot \sigma^2 + 8  \cdot \mu^2 
\end{align*} 

Let $Y = \frac{(D_k - \mu) \sqrt{k}}{\sigma}$, so that $\E[Y] = 0$ and $\mbox{Var}[Y] = 1$. The condition $W \succ X$ implies 
$$\Pr\left[Y \le \frac{-\mu \sqrt{k}}{\sigma} \right] \ge 1/2.$$

Let $F_Y(z)$ denote $\Pr[Y \le z]$, and $\Phi(z)$ denote the CDF of the standard Normal distribution.  Let $z^* = -\frac{\mu \sqrt{k}}{\sigma}$.  By the Berry-Esseen theorem  (whose preconditions can be checked to apply here) applied at the point $z^*$, we have:
$$   \frac{1}{2} - \Phi(z^*) \le F_Y(z^*) - \Phi(z^*)  \le \frac{0.5 \cdot E[|D - \mu|^3]}{\E[(D-\mu)^2] \cdot \sqrt{k}} \le  \frac{2 \sigma^2 + 4 \mu^2}{\sigma^3 \sqrt{k}}.$$
where we have used $\E[|D-\mu|^3] \le 4 \sigma^2 + 8 \mu^2$.

Next note that by the above lower bound on $\sigma$, we have $z^* \ge -1$, so that $\phi(z^*) \ge \frac{1}{\sqrt{2\pi e}}$, where $\phi$ is the PDF of the standard Normal distribution. We therefore have
$$   \frac{1}{2} - \Phi(z^*) = \Phi(0) - \Phi(z^*)  \ge \phi(z^*) \cdot (0 - z^*) \ge \frac{1}{\sqrt{2\pi e}} \cdot \frac{\mu \sqrt{k}}{\sigma}.$$
Putting the above two inequalities together, and using $\frac{\mu^2}{\sigma^2} \le \frac{1}{k}$, we have
$$ \frac{1}{\sqrt{2\pi e}} \cdot\frac{\mu \sqrt{k}}{\sigma} \le \frac{2 \sigma^2 + 4 \mu^2}{\sigma^3 \sqrt{k}} \qquad \Rightarrow \qquad \mu \le 2  \sqrt{2\pi e} \cdot \left(\frac{1}{k} + \frac{2}{k^2}  \right) \le \frac{8.27}{k} \cdot \left(1 + \frac{2}{k}\right).$$
Since $\theta_k = \mu$, plugging this bound into \cref{thm:distort1}, the proof of the upper bound is complete.
%Then, Cantelli's inequality implies
%$$ \Pr[D_k \le 0] \le \frac{k \sigma^2}{k \sigma^2 + k^2 \omega^2} = \frac{ \sigma^2}{\sigma^2 + k \omega^2}.  $$
%The condition $\Pr[W \succ X] \ge 1/2$ implies $\Pr[D_k \le 0] \ge 1/2$, so that the RHS is at least $1/2$. Therefore,
%$$ \Pr[W \succ X] \ge \frac{1}{2} \qquad \Rightarrow \qquad \omega \le \sqrt{\frac{\sigma^2}{k}} .$$ 
%For any distribution on $[-1,1]$, we have $\sigma^2 \le 1$. Therefore, we have $\theta_k \le \frac{1}{\sqrt{k}}$, which by \cref{thm:distort1} completes the proof of the upper bound.
\end{proof}

We now show a matching lower bound, showing our analysis is tight.

\begin{theorem}
\label{thm:lb1}
For any $k \ge 2$, $\theta_k = \Omega\left(\frac{1}{k}\right)$. This means that any anonymous social choice rule that aggregates the ordinal rankings of all groups of size $k$ over all alternatives has distortion $1 + \Omega\left(\frac{1}{k}\right)$, so that achieving distortion $1+\epsilon$ needs groups of size $\Omega(1/\epsilon)$.
\end{theorem}
\begin{proof} 
We show the lower bound on $\theta_k$. The bound on distortion follows from \cref{thm:lb_main}.

\paragraph{Case 1. Odd $k$.} We place a mass of $1/2$ voters at $X$, and $1/2$ mass of voters between $X$ and $W$, at a distance $\frac{2}{k+1}$ from $W$. The distribution $D$ is therefore $1$ with probability $1/2$ and $-1+\frac{2}{k+1}$ with probability $1/2$, so that $\E[D] = \frac{1}{k+1}$. To see that $W \succ X$, if we sample $k$ times from this distribution, the sum of the values is at most $0$ as long as the majority of samples are closer to $W$, which happens with probability exactly $1/2$. 

\paragraph{Case 2. Even $k \ge 4$.} We place a voter mass of $p = \frac{1}{2} + \frac{1}{3k}$ at $X$ (location $1$ in distribution $D$) and $1-p = \frac{1}{2} - \frac{1}{3k}$ at $W$ (location $-1$ in  $D$). This implies $\E[D] =  \frac{2}{3k}$. To see that $W \succ X$, let $M$ denote the median of {\tt Binomial}$(k,p)$, whose mean is $\mu = k p$. The work of~\cite{Binomial} upper bounds the gap between the mean and median for a Binomial and implies that 
$$ | M - kp| = \left|M - \frac{k}{2} - \frac{1}{3}\right|  \le \max(p,1-p) \le \frac{1}{2} + \frac{1}{3k} \le \frac{7}{12} \qquad \Rightarrow \qquad \left|M - \frac{k}{2} \right| < 1.$$
Since $k$ is even, this implies $ M = \frac{k}{2}$. Consider a sample of $k$ voters. If at least $k/2$ voters are located at $W$, then $W$ is the winner. But the probability of this event is at least the probability that {\tt Binomial}$(k,p) \le M$, which is at least $1/2$. Therefore, $W \succ X$, completing the proof of the lower bound. 
\end{proof}



%For small $\epsilon$, the above bound is tight: Consider two alternatives $W,X$ separated by distance $1$, with a mass of $1/2 + \epsilon$ voters at $X$ and $1/2 - \epsilon$ voters at $W$. Suppose $k = o(1/\epsilon^2)$. Then, 

In \cref{thm:berry}, we note that moment based methods like Bernstein's inequality~\cite{Boucheron} would only show an upper bound of $\theta_k = O\left(\frac{1}{\sqrt{k}}\right)$ -- these methods require the variance to be small, while the Berry-Esseen inequality shows the distribution of the average is closer to Normal if the variance is high while the third moment is bounded. Though the latter yields a much stronger bound, it is still far from tight for small $k$. For instance, it implies that we need a very large constant $k$ to achieve distortion below $3$, while we show below that the correct bound is $k = 3$. We next show vastly improved distortion bounds for the canonical cases of $k = 2,3$. For $k=3$, we obtain $2.81$, which beats the deterministic metric distortion lower bound. %, while we conjecture that the optimal value of $\theta_3 = 1/4$, which implies groups of size $k=3$ suffice for distortion below $3$.

