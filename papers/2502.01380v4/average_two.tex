\subsection{Distortion Bound for $k=2$}
\label{sec:two}
We now consider the case with groups of size $k=2$. We note that \cref{thm:lb_main} combined with \cref{lem:theta2} below shows a lower bound of $1+\sqrt{2}$ on the distortion of any deterministic social choice rule. We now show that Copeland has distortion $3+\sqrt{2}$, and this bound is tight. Though the $k=2$ case  sounds simple, even here, we need to solve an interesting non-convex program to global optimality. Note that our bound significantly improves the tight distortion bound of $5$ for Copeland when $k = 1$ (no deliberation).

\begin{theorem}
\label{thm:theta2}
For the Copeland rule applied to the averaging model with group size $k = 2$, the distortion is at most $3 + \sqrt{2} \le 4.42$, and this bound is tight for the Copeland rule.
\end{theorem}

The rest of this subsection is devoted to proving the above theorem. 

\paragraph{Computing $\theta_2$.} We first compute $\theta_2$ explicitly. We show the following lemma using the idea of pipage rounding~\cite{Ageev2004}.

\begin{lemma}
\label{lem:pipage}
    Consider \cref{eq:opt_avg} with $k = 2$. The optimal $D$ has support $\{-1,0,1\}$.
\end{lemma}
\begin{proof}
 To an arbitrarily good approximation, the support of $D$ can be discretized as $\{0, \pm \epsilon, \pm 2 \epsilon, \ldots, \pm 1\}$. Consider two values $\{-a,a\}$ in the support, where $1 > a > 0$; assume at least one of these values has non-zero mass.  Suppose $\Pr[D = a] = p$ and $\Pr[D = -a] = q$, where $\max(p,q) > 0$. Note that the contribution to the objective is $(p - q) \cdot a$. If $p > q$,  move the probability mass at $a$ to $a + r \cdot \epsilon$ and the mass at $-a$ to $-a - r \cdot \epsilon$; else move the probability mass at $a$ to $a - r \cdot \epsilon$ and the mass at $-a$ to $-a + r \cdot \epsilon$. Here, $r$ is the smallest integer so that one of the four values $\pm a \pm r \cdot \epsilon$ has non-zero mass. It is easy to check that both operations preserve the LHS of the constraint in \cref{eq:opt_avg}, since any sum remains least zero iff it was originally least zero. Further, the objective only increases in this process, since the contribution of the moving mass increases, and the remaining mass remains the same as before. We perform that move, thereby eliminating $\{a,-a\}$ from the support. Iterating, all the mass moves to $\{-1, 0, 1\}$. Now taking the limit as $\epsilon \rightarrow 0$, the proof is complete. 
\end{proof}

We can now write \cref{eq:opt_avg} as a non-convex optimization problem. Suppose $\Pr[D = -1] = p$, and $\Pr[D = 0] = q$. Then the program becomes:
\begin{equation}
\max_{p,q \ge 0, p+q \le 1} 1 - 2p - q \qquad \mbox{s.t.} \qquad (p+q)^2 + 2 p (1-p-q) \ge 1/2.
\end{equation}

We now use the global optimization tool  BARON ((Branch-And-Reduce Optimization Navigator)~\cite{Sahinidis1996,KS18})~\cite{Sahinidis1996}\footnote{BARON can be called from Python using the AMPL API ({\tt amplpy}).  We have used the demo (free) version available on Google Colab, that supports $10$ constraints and variables and up to $50$ nonlinear operations. This suffices for all our programs. }.  BARON uses a combination of convex relaxation and integer programming techniques to find increasingly tighter lower and upper bounds on the global optimum. We find that $q = 0$ and $p = 1 - 1/\sqrt{2}$. Therefore,

\begin{lemma}
\label{lem:theta2}
    $\theta_2 = \sqrt{2} - 1$, and is achieved when $\Pr[D = 1] = \frac{1}{\sqrt{2}}$ and $\Pr[D = -1] = 1 - \frac{1}{\sqrt{2}}$. 
\end{lemma}

The above lemma implies that when $W \succ X$, we have $\frac{SC(W)}{SC(X)} \le \frac{1+\theta_2}{1-\theta_2} \le \sqrt{2} + 1 \approx 2.414$. It also implies a lower bound of $\sqrt{2} + 1$ on the distortion of any deterministic social choice rule by \cref{thm:lb_main}.

\paragraph{Distortion of Copeland.} We next bound $\frac{SC(W)}{SC(X)}$ when there exists a $Y$ such that $W \succ Y \succ X$. Towards this end, let $a_i = d(i,W) - d(i,Y)$ and $b_i = d(i,Y) - d(i,X)$. Let $d(Y,X) = 1$, and $d(W,X) = B \ge 0$, so that $d(W,Y) \le B+1$. By triangle inequality, we have $|a_i| \le B+1$, and $|b_i| \le 1$ for all locations $i$. 

Let $D_1$ be the distribution on $\{a_i\}$ induced by $D$, and $D_2$ be the corresponding distribution on $\{b_i\}$.  The conditions $W \succ Y$ and $Y \succ X$ yield:
\begin{equation}
\label{eq:cond1}
\Pr_{\{a_1,a_2\} \sim D_1} \left[ a_1 + a_2 \le 0 \right] \ge 1/2, \mbox{ and }
\end{equation}
\begin{equation}
\label{eq:cond2}
\Pr_{\{b_1,b_2\} \sim D_2} \left[ b_1 + b_2 \le 0 \right] \ge 1/2,
\end{equation}
where the two draws from $D_1$ (resp. $D_2$) are {\em i.i.d.} Note that $a_i + b_i = d(i,W) - d(i,X)$ so that 
\begin{equation}
\label{eq:cond_diff}
    SC(W) - SC(X) = \E[a_i + b_i].
\end{equation}
We will bound $SC(X)$ as follows. Note that
$$2 d(i,X) = d(i,X) + d(i,X) \ge (d(W,X) - d(i,W)) + d(i,X) = d(W,X) - (a_i + b_i).$$  
Similarly, we also get 
$$2 d(i,X) \ge d(Y,X) - b_i.$$
Therefore, 
\begin{equation}
\label{eq:cond_x}
    SC(X) \ge \frac{1}{2} \cdot  \E\left[\max ( B - a_i - b_i, 1 - b_i) \right].
\end{equation}
To show that the distortion is at most $1+\beta$ it therefore suffices to show
$$ SC(W) - SC(X) \le \beta \cdot SC(X).$$
Plugging in \cref{eq:cond_diff,eq:cond_x}, and simplifying, it suffices to show that the following objective is at most $0$.
\begin{equation}
\label{eq:obj4}
\eta_2 := \mbox{Maximize} \left( \E\left[\left(\frac{2}{\beta} + 1 \right) b_i \right] + \E\left[ \frac{2}{\beta} \cdot a_i +  \min\left( a_i - B + 1,  0 \right) -1\right] \right)
\end{equation}
where the maximization is over $B$, and  distributions $D_1$, $D_2$, where $D_1$ is a distribution over $\{a_i\}$ supported on $[-B-1,B+1]$, and $D_2$ is a distribution over $\{b_i\}$ supported on $[-1,1]$. The maximization is subject to constraints \cref{eq:cond1,eq:cond2}. We will now show that the above maximum is at most $0$.

Note that the optimum for $\{b_i\}$ subject to \cref{eq:cond2} and $|b_i| \le 1$ and that for $\{a_i\}$ subject to \cref{eq:cond1} and $|a_i| \le B+1$ can be separately computed. For $\{b_i\}$, the optimum is simply $(1 + 2/\beta) \cdot \theta_2$. For $\{a_i\}$, we have the following lemma.

\begin{lemma}
    The optimum distribution $D_1$ for \cref{eq:obj4} is supported on $\{-B-1, B+1, B-1, 1-B, 0\}$. 
\end{lemma}
\begin{proof}
    We use the same procedure as in the proof of \cref{lem:pipage}. For any $a > 0$, consider the objective in \cref{eq:obj4} for the points $\{a,-a\}$. Let $p = \Pr[D_1 = a]$ and $q = \Pr[D_1 = -a]$, where $\max(p,q) > 0$. The objective is a linear function of $a$ except at the point $a = B-1$, where $-a = 1-B$. To see this, note that the contribution to the objective is both linear in $a$ and linear in $-a$. As an example, if $a >  B-1$, then the contribution to the objective is 
    $$p \cdot \left( \frac{2}{\beta} \cdot a + 0 \right) + q \cdot \left( \frac{2}{\beta} \cdot (-a) +   (-a-B+1) \right), $$
    which is linear in $a$. Therefore, we can perform the same operation as in \cref{lem:pipage} and move the probability mass to the neighboring points, till either it hits one of $B-1, 1-B$, or the extreme points $\{-B-1, B+1\}$ or $0$. Therefore, the optimal $D_1$ is supported on these five points.
\end{proof}

We can now write \cref{eq:obj4} as a non-linear program with six variables, one for $B$, and the rest capturing the probabilities of $D_1$ taking one of the five support values.  Note that to encode \cref{eq:cond1}, we need to consider two cases, depending on whether $B \ge 1$ or otherwise. We elaborate below.

\paragraph{Non-convex Program: Case 1.} We first consider the case where  $0 \le B \le 1.$ Note that $B$ itself is a variable. Let 
$$p = \Pr[D_1 = B+1], q = \Pr[D_1= 1-B], r = \Pr[ D_1 = 0], s = \Pr[D_1 = B-1], t = \Pr[D_1 = -B-1].$$ 
These values of $D_1$ are in decreasing order. Then
\begin{equation} 
\label{eq:sum1}
p + q + r + s + t = 1, \qquad \mbox{and} \qquad p,q,r,s,t \ge 0.
\end{equation}
The probability constraint \cref{eq:cond1} implies
\begin{equation} 
\label{eq:prob1} (p + q)^2 + 2 p \cdot (r + s) + 2  q \cdot r \le 1/2.
\end{equation}
Let $A = \left(1 + \frac{2}{\beta}\right) \cdot \theta_2$. Then subject to the above constraints, the objective is:
$$ A +  p \left(\frac{2}{\beta}(B+1) - 1\right) + q \left(\frac{2}{\beta}(1-B) - 1\right) - r - s\left( \frac{2}{\beta} (1-B)+1\right) - t \left( \frac{2}{\beta}(B+1) + 2B + 1 \right). $$

\paragraph{Non-convex Program: Case 2.} We next consider the case where 
 $B \ge 1,$ where $B$ is a variable. Let 
$$p = \Pr[D_1 = B+1], q = \Pr[D_1= B-1], r = \Pr[ D_1 = 0], s = \Pr[D_1 = 1-B], t = \Pr[D_1 = -B-1].$$ 
These values of $D_1$ are in decreasing order. It is easy to check that constraints \cref{eq:sum1,eq:prob1} remain the same.
Let $A = \left(1 + \frac{2}{\beta}\right) \cdot \theta_2$. Then, subject to the above constraints, the objective is now:
$$ A +  p \left(\frac{2}{\beta}(B+1) - 1\right) + q \left(\frac{2}{\beta}(B-1) - 1\right) - r B - s\left( \frac{2}{\beta} (B-1)+2B-1\right) - t \left( \frac{2}{\beta}(B+1) + 2B + 1 \right). $$


Note that \cref{eq:prob1} implies $p+q \le \frac{1}{\sqrt{2}}$. This means $r + s + t \ge 1 - \frac{1}{\sqrt{2}}$. Since we assume $\beta \ge 2 + \sqrt{2}$, it is easy to check that the objective is strictly negative if $B \ge 100$. Therefore, add $0 \le B \le 100$ as a constraint. We find the global optimum via BARON~\cite{Sahinidis1996,KS18} for both cases. For $\theta_2 = \sqrt{2} + 1$, and $\beta = 2 + \sqrt{2} + \delta$ for small $\delta > 0$, the global optimum for both cases is strictly negative. This shows that $\beta = 2 + \sqrt{2}$, and the distortion is at most $1+\beta = 3 + \sqrt{2}$, which shows the the upper bound in \cref{thm:theta2}.


\paragraph{Matching Lower Bound.} The above program also unearths the worst case example that shows the bound is tight.  Place $W,X,Y$ on a line in that order with a distance of $1$ between $W,X$ and $1$ between $X,Y$. Place a mass $1-\alpha$ of voters very close to  $X$ and $\alpha$ very close to $Y$, where $\alpha = 1 - 1/\sqrt{2}$. The distances can be appropriately set so that the following happens: Whenever both voters in a sampled pair are located close to $X$ (which happens with probability $(1-\alpha)^2 = 1/2$), they will prefer $W$ over $Y$, so that $p_k(W,Y) \ge 1/2$. Next, when one voter is close to $X$ and the other is close to $Y$, they prefer $Y$ to $X$. This means $p_k(Y,X) = 1 - (1-\alpha)^2 = 1/2$. This means $W$ is in the uncovered set, and is the Copeland winner. The social optimum is $X$, and the distortion is $\frac{SC(W)}{SC(X)} = 1+1/\alpha = 3 + \sqrt{2}$. This completes the proof of \cref{thm:theta2}.
