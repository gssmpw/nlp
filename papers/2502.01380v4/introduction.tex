\section{Introduction}
Civic discourse is the cornerstone of modern democracy. A key challenge in modern social choice theory is how to leverage the power of discourse to %guide the opinions of individuals and, in doing so, 
improve societal decision making compared to simply voting on issues. Unfortunately, there are several roadblocks to this ideal. Much of the current online discourse tends to become acrimonious, increasing polarization rather than consensus. Despite some notable successes~\cite{Ingham18,fishkin1991democracy}, it is relatively cumbersome to organize large groups for agenda-driven discussions, and even when such discussions occur, they rarely result in meaningful opinion changes, leading to the same outcomes as direct voting. 

A promising approach emerging in social choice is small-group online forums, which are moderated and agenda-driven. Platforms such as the Online Deliberation Platform~\cite{stanfordd}, vTaiwan~\cite{vTaiwan}, Mismatch~\cite{mismatch}, and Myjunto~\cite{myjunto} have experimented with this approach. Recent work has shown the potential of AI mediation in these settings, including facilitating human interaction via summarization and rephrasing of diverse viewpoints~\cite{ChrisBail,ChatBot,stanfordd,LLM,ashkinaze2024}. %Small groups can facilitate productive conversations, especially when run in parallel. 

%============================

%Civic discourse is the cornerstone of modern democracy. A key challenge in modern social choice is how to leverage the power of discourse to change the opinions of individuals, and hence improve over the outcomes found by simply voting over issues. As has been repeatedly observed, there are several roadblocks to the utopian ideal of civic discourse: Most online discourse just turns acrimonious and leads to more polarization instead of consensus. In addition, it is cumbersome to get large groups of individuals together for a moderated agenda-driven discussion. Further, large group discussions are unlikely to change opinions, resulting in similar outcomes had the issues been put to vote directly among the populace. 

%======================================

%Issues with existing social choice paradigms:

%\begin{itemize}
%\item Ordinal voting does not allow for discourse between participants, nor does it capture strengths of preferences.
%\item Creating a large citizen's assembly via sortition is cumbersome, and given the large group size, is unlikely to produce consensus, and is likely to reduce to voting (in the assembly, which is in turn representative of the population).
%\item If a large group is not physically brought together, it is hard to control its dynamics.
%\end{itemize}

% ================================

%A different emerging paradigm in social choice is online discussions or forums in small groups, which can be agenda-driven and mediated. Recent examples of such platforms include the Online Deliberation Platform~\cite{stanfordd}, Mismatch~\cite{mismatch}, and Myjunto~\cite{myjunto}. Recent work has also highlighted the utility of AI mediation, such as LLM-based summaries or rephrasing of diverse viewpoints, in such small group online interaction~\cite{ChrisBail,ChatBot,stanfordd,LLM}. For such online forums to be effective, civil, and practical, the group sizes need to be kept very small, say a discussion between two or three participants. At the same time, given the proliferation of cloud computing and AI APIs, it has become feasible to run many such small group interactions in parallel online, and aggregate their outcomes. 

% ========================================

Our goal is to model the outcome of such small group deliberations and analyze whether aggregating their outcomes is capable of finding a better societal alternative than would be found without such interaction. Our main result is that deliberation is strictly helpful even with small groups.

\subsection{Metric Distortion and Deliberation} 
We use the metric distortion framework~\cite{AnshelevichBEPS18} to analyze the quality of the deliberation. We assume there is one issue with several potential choices (called alternatives or candidates) that the voters are deliberating over. These voters and alternatives are embedded in a latent metric space. Voters reveal ordinal preferences (that is, a ranking) over alternatives that are consistent with the latent distances, where if an alternative is closer to the voter, it is more similar to the voter's opinion, hence ranked higher by the voter. %In other words, if alternative $A$ is closer than alternative $B$ to voter $v$ in the metric space, then the voter ranks $A$ higher than $B$ in their ordinal ranking. 
We assume the set of alternatives is common to all voters, and a voter ranks all alternatives.

A social choice rule such as the Borda score or Copeland rule takes the rankings for all voters as input, and outputs an aggregate alternative. %To measure the quality of this social choice rule, a widely studied performance metric is the ``distortion''. 
 Given an alternative, its $1$-median cost is the sum of the distances of all voters to this alternative. Consider all metrics consistent with voter rankings. The distortion is the worst case for all rankings and all metrics consistent with these rankings, of the ratio of the $1$-median cost of the alternative output by the social choice rule (that only knows the ordinal rankings) to the cost of the best alternative had the consistent metric been known. A lower distortion implies a better social choice rule, whose outcome is closer to the true $1$-median.

It is known that in the absence of voter interaction, any deterministic voting rule that uses only the ordinal preferences of the voters has distortion at least $3$~\cite{AnshelevichBEPS18} %(this is tight~\cite{Kizilkaya022,Gkatzelis0020}), 
and any randomized rule has distortion at least $2.11$~\cite{AnshelevichP17,CharikarR22} %(the best upper bound being $2.74$~\cite{CharikarWRW24}). %The lower bounds arise even for instances with only two alternatives, because ordinal preferences do not reveal the {\em strength} of the preference a voter has for the two alternatives, which is captured by their embedding in the unknown underlying metric space. 
 The key question we ask in this paper is:

\begin{question}
\label{q1}
Can small group deliberation improve these bounds?
\end{question}


%==================================

%\paragraph{Research Question.} In this paper, we go beyond classic social choice theory and assume that voters deliberate in a small group and collectively reveal an ordinal preference (or ranking) that is consistent with  their local view of the latent metric space. We consider several models of what ``consistent'' means, but a simple, canonical model is that the voters in the group could output the alternative that minimizes the sum of their distances to it (that is, their $1$-median alternative). %Note that like the model without deliberation, each group only outputs ordinal preferences. 
%At a high level, we ask:

%\begin{quote}
%Can we quantify, as a function of the size of the deliberating group, the power of small group deliberation in improving the metric distortion of the final outcome? Does such deliberation improve on the metric distortion lower bounds above?
%\end{quote}

%use the cardinal metric information to jointly propose an alternative (or a ranking over alternatives) based on their strength of opinions as captured by the metric space. 
%Note that the group only outputs ordinal information, and does not reveal the metric itself. As an example,  the group can propose the $1$-median outcome  (so it takes all underlying metric distances into account optimally). 

To study this question formally,  we assume that voters deliberate in a small group and collectively reveal an ordinal preference (or ranking) that is consistent with their local view of the latent metric space. We consider several models of what ``consistent'' means, but a simple, canonical model is that the voters in the group could output the alternative that minimizes the sum of their distances to it (that is, their $1$-median alternative). The $1$-median intuitively captures persuasion -- consider two alternatives and two voters. If the first voter is very close to the first alternative, then they care strongly about that alternative. If the second voter is equidistant from both alternatives, they are indifferent. In this setting, the first voter can persuade the second to collectively output the first alternative, which is indeed the $1$-median. Even in the $1$-median setting, \cref{q1} is far from trivial. Indeed, the example below from~\cite{FainGMS17,CaragiannisM024} shows that even if all possible groups of voters of constant size output their $1$-median alternative, and these are aggregated by any social choice rule, the metric distortion remains $3$.

%the work of~\cite{CaragiannisM024} uses core-set type ideas to show that a random sample of voters of size logarithmic in the number of candidates suffices to achieve distortion arbitrarily close to $1$.  However, the group size needed for a core-set type sampling idea to break the metric distortion lower bound of $3$ will necessarily be super-constant . This is shown by the following example in.

\begin{example}
\label{eg1}
Assume the group size is $k$, a constant. For some $\delta > 0$, there are $n$ voters, who are all at distance $1+\delta$ from a common candidate $c$. For each subset $S$ of voters of size $k$, there is a candidate $c_S$ that is at distance $1$ from the members of $S$, and distance $3$ from all other voters. Each group $S$ outputs $c_S$ when it deliberates, since this minimizes their $1$-median cost -- indeed, this alternative is preferred by all voters in $S$, leading to a trivial deliberation. Now any social choice method that aggregates the outcomes of these deliberations into a final winning alternative must produce one of these $c_S$ as the final alternative. However, the average distance of all voters to $c$ is $1+\delta$, while for any $c_S$, the average distance is close to $3$. Therefore, the distortion is at least $3$.
\end{example}

To get around this lower bound, the group size needs to be super-constant, and indeed, the recent work of~\cite{CaragiannisM024}  shows that the $1$-median of a random sample of voters of size logarithmic in the number of candidates suffices to break the lower bound. However, such large groups lead to suboptimal group dynamics such as conformity, informational influence, and stereotyping of participants, which have been widely studied in psychology~\cite{Asch,Deutsch1955} and political science~\cite{Price}. In fact, the celebrated Asch experiments~\cite{Asch} show that conformity begins to kick in even with groups of around size $5$. Further, research in organizational psychology~\cite{axtell_2018} argues that groups of size larger than eight causes conversation quality to erode, people to become less candid, comments to become superficial, and tough topics avoided. The superiority of small groups has also been established via game-theoretic modeling of deliberation~\cite{Meirowitz}. All this means a large group interaction, even with the best intentions, is unlikely to find something as optimal as their $1$-median outcome, or allow participants to be ``public spirited''~\cite{FlaniganPW23} and compute the true social cost of each alternative. %Had large groups worked so well, people interacting on social media should have converged on all issues to a $1$-median, and there is clearly no evidence that happens!
%We therefore desire constant sized groups.
What is more likely is that the large group splits into multiple groups of constant size whose deliberation outcomes are aggregated by a moderator.  %This motivates our study of small group deliberation.

\subsection{Deliberation Model with Small Groups}  %To circumvent these issues, we take a different approach motivated by online small-group interactions, where we assume each deliberation is in a forum with very small number $k$ of participants (think $k$ as a constant like $3$). The ordinal outcomes of many of these deliberations are finally aggregated. %In some of our models, we further take the approach that the outcome of deliberation, though biased towards voters with stronger opinions, does not need to be deterministic. 
%Though this sounds appealing, as \cref{eg1} shows, designing deliberation models and protocols to capture these aspects is tricky. Our main conceptual contribution is the following:
Although \cref{eg1} may seem to imply a negative result for \cref{q1} with groups of constant size, our main contribution is the following set of positive results.

\begin{quote}
There are natural models for multiple small group deliberations with constant-sized groups and natural social choice rules for aggregating their ordinal outcomes, which break the lower bounds of metric distortion (without deliberation) even with groups of size $k = 3$ or $4$, and where the distortion is $1 + O(1/\mbox{poly}(k))$ for larger group sizes $k$.
\end{quote}

%To circumvent the barrier on group size implied by \cref{eg1}, 
For these results, we go beyond each deliberating group producing the $1$-median alternative. We do so by restricting the set of alternatives that a group can deliberate on, so that different small groups deliberate on different, focused sets of alternatives. In particular, we let each group deliberate on only two alternatives and output the better of the two. We then choose multiple groups of size $k$ at random from the population and let them deliberate between different pairs of alternatives, with the result of the deliberation being either the first or the second alternative. %arrived at by following one of the two models above. 
The outcome of these deliberations produces a tournament graph over the alternatives,  and we use the classic Copeland rule~\cite{voting-book} to aggregate this tournament into a single winning alternative. The entire model is presented in \cref{sec:prelim}.

For analyzing this framework, we assume that the outcome of any individual deliberation is a (possibly probabilistic) function of the strengths of the preference of voters within the group (as measured by their metric distances) for the two alternatives they are deliberating over. We consider two models: In the {\em averaging model}, the group deliberates between the two alternatives and outputs that alternative whose $1$-median cost is smaller.\footnote{In this model, instead of any group of size $k$ deliberating only over a pair of alternatives, we can equivalently let the group deliberate over all the alternatives and output a ranking based on their total distance to each alternative.}  In the {\em random choice model}, each group member has a normalized bias (or strength of preference) for one of the two alternatives proportional to the difference in their distance to the two alternatives. The probability with which an alternative is produced is proportional to the total normalized bias of voters in the group who prefer that alternative to the other. %A voter is randomly chosen proportional to this bias value, and their favorite alternative of the two is output as the outcome of deliberation. 
Note that in both cases, the output of a group is an ordinal preference. 


\subsection{Our Results} We show analytic results for the distortion of both the averaging and random choice models as a function of the group size $k$, showing that a very small group size suffices to break the lower bound of metric distortion without deliberation. In particular, we show the following results, which are also summarized in \cref{tab:average,tab:random}.

\begin{table}[htp]
	\centering
 
	\begin{tabular}{|c|c|c|}
		\toprule
        Group size & Upper Bound & Lower Bound \\
        \midrule 
        $k = 2$ & $3 + \sqrt{2} \approx 4.414$ (\cref{thm:theta2}) & $1 + \sqrt{2} \approx 2.414$ (\cref{lem:theta2})  \\
        \midrule
        $k = 3$ & $2.81$ (\cref{thm:theta3}) & $\frac{5}{3} \approx 1.667$ (\cref{thm:lb1})  \\
           \midrule
        General $k$ & $1 + O\left(\frac{1}{k}\right)$ (\cref{thm:berry}) & $1 + \Omega\left(\frac{1}{k}\right)$ (\cref{thm:lb1}) \\
      \bottomrule
	\end{tabular}
	\caption{\label{tab:average} The upper and lower bounds on distortion for the Averaging model of deliberation. The upper bounds hold for the Copeland rule, while the lower bounds hold for any deterministic social choice rule that aggregates the ordinal rankings output by all groups of size $k$.  The upper bound of $3 + \sqrt{2} \approx 4.414$ is tight for the Copeland rule  when $k=2$. 
    }
\end{table}

\begin{table}[htbp]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\toprule
        Group size & Upper Bound  (\cref{thm:random}) & Deterministic LB  & Randomized LB   \\
        \midrule 
        $k = 2$ &  $3.34$ & $1.82$ & $1.41$  \\
        \midrule
        $k = 3$ &  $2.31$ & $1.51$ & $1.25$ \\
           \midrule
        $k = 4$ &   $1.90$  & $1.37$ & $1.18$ \\
      \bottomrule
	\end{tabular}
	\caption{ \label{tab:random} The upper and lower bounds on distortion for the Random Choice model of deliberation. The upper bounds hold for the Copeland rule. The final two columns respectively show the lower bounds for any deterministic and randomized social choice rule that aggregates the ordinal rankings output by all groups of size $k$. These follow from \cref{thm:lb_main}.  For general $k$, the upper bound is $1 + O\left(\sqrt{\frac{\log k}{k}}\right)$ (\cref{thm:asymp1}), and approaches $1$ as $k \rightarrow \infty$.
    }
\end{table}

\begin{itemize}
\item For the averaging model %and the random choice model 
with group size $k=3$, the distortion bound is $2.81$ (\cref{thm:theta3}). %and $2.31$ (\cref{thm:random}) respectively. 
This bound is smaller than the deterministic metric distortion lower bound of $3$. (As we discuss in \cref{sec:prelim}, the fair comparison for the averaging model is deterministic social choice rules.) For the random choice model, the distortion is $1.90$ for $k=4$ (\cref{thm:random}), which is lower than the lower bound of randomized metric distortion of $2.11$. We also show that for $k=2$, the distortion is exactly $3 + \sqrt{2} \le 4.42$ for the averaging model (\cref{thm:theta2}) and at most $3.34$ in the random choice model. The proofs use interesting non-convex programming relaxations for which we computationally bound the global optimum.\footnote{All Python code is available at \url{http://bit.ly/3WHW8H4}}

\item Next, we consider the asymptotic behavior of distortion in the size of the group, showing that the distortion approaches $1$ rapidly as the size of the group increases. For the averaging model, we show that groups of size $k = \Theta(1/\epsilon)$ are necessary and sufficient to achieve distortion $1+\epsilon$, where $\epsilon > 0$ is a constant (\cref{thm:berry,thm:lb1}). This result is an interesting application of the Berry-Esseen inequality~\cite{feller1971introduction}. For the random choice model, we show an upper bound of $k = \tilde{O}(1/\epsilon^2)$ (\cref{thm:asymp1}). These bounds on group size are {\em independent} of the number of alternatives, a result that cannot be obtained had each group only output its favorite alternative among all $m$ alternatives, for example, as in~\cite{CaragiannisM024}. %Further, the bound for the averaging method is strictly better than the $O(1/\epsilon^2)$ bound that sampling would imply, showing that small group deliberation is very different from sampling. %We also show a lower bound of $k = \Omega(1/\epsilon)$ for the averaging model.
%For the random choice model, we show an upper bound of $k = \tilde{O}(1/\epsilon^2)$.
%(See the discussion after \cref{thm:sample2}.) 

\item Our analytic results assume a continuum of users from which infinitely many groups of size $k$ can be sampled. We also consider ``sample complexity'', which is the number of randomly sampled groups needed to achieve an additive $\epsilon$ to the analytically computed distortion. We show that the sample complexity is $O\left(\frac{\log m}{\epsilon^2}\right)$ for the averaging model (\cref{thm:sample2}), and $O\left(\frac{m \log m}{\epsilon^2}\right)$ for the random choice model (\cref{thm:sample1}). 
%\item For the random choice model, we present an upper bound via a non-convex program with two variables, which we can numerically solve to global optimality. We show distortion bounds of at most $2.32$ for groups of size $k=3$, and at most $1.90$ for groups of size $k=4$. This shows that very small group sizes suffice to break both the deterministic metric distortion barrier of $3$, and the randomized barrier of $2$ in this model.
\item In \cref{sec:general}, we present generalizations of the random choice model that are motivated by how the change in voter opinion depends on bias. We show that our analysis smoothly extends to these more general models, where the bounds become worse (asymptotically converging to numbers larger than one) and closer to the distortion of random dictatorship as the opinion change becomes less dependent on the bias of the voter.
\end{itemize}

%At a conceptual level,  may produce more effective/efficient outcomes than large groups discussing all possible options.

From a practical perspective, our protocols and associated distortion bounds show that multiple tiny groups of people deliberating about specific alternatives suffice to improve on straightforward voting over all alternatives. This result is robust to natural models of deliberation. Since the goal of the paper is to make a conceptual point that very small group sizes suffice, we have focused on the Copeland rule. However, some of our analysis (such as \cref{thm:theta2}) can be extended to weighted tournament rules~\cite{MunagalaW19,Kempe_2020}, with comparable guarantees. 



\subsection{Technical Highlight: Small Deviation Bounds and Non-convex Programs} %At a technical level, the key challenge with multi-voter deliberation is analysis. 
It is known that the distortion of the Copeland rule can be characterized by its behavior on at most $3$ alternatives~\cite{AnshelevichBEPS18}; most of our analysis uses its behavior on two alternatives. As one of our contributions, we distill the type of analysis in~\cite{AnshelevichBEPS18} as solving a mathematical program about distributions. At a high level, such a mathematical programming approach is akin to the approach in~\cite{CharikarR22}, though the details are very different. For the averaging model, the problem of bounding the distortion reduces to a very basic question in probability theory (see \cref{eq:opt_avg} in \cref{sec:avg}): 
\begin{question}
    Given $k \ge 1$, find the distribution $D_k$ supported on $[-1,1]$ with maximum $\E[D_k]$, subject to the constraint that for $Y_1, Y_2, \ldots, Y_k \overset{\text{i.i.d.}}{\sim} D_k$, we have $\mbox{Median}\left(\frac{\sum_{i=1}^k Y_i}{k}\right) \le 0$.  
\end{question} 

Here, $k$ is the group size. We call the optimum expectation computed above $\theta_k$. This captures the worst-case gap between the mean and median of the average of bounded {\em i.i.d.} random variables. We show that the distortion is both upper and lower bounded by simple increasing functions of $\theta_k$, so that tightly bounding $\theta_k$ yields good bounds on distortion. As $k \rightarrow \infty$, by the Central Limit Theorem, the average of $i.i.d.$ and bounded random variables converges to a Normal distribution, so that $\theta_k \rightarrow 0$. Our goal is to bound $\theta_k$ %(the worst case gap between the mean and the median) 
for {\em small} $k$, and bound the asymptotic rate at which $\theta_k$ decreases with $k$. Our main technical result in \cref{sec:avg} is summarized in the theorem below and captures the combination of the bounds in \cref{thm:berry,thm:lb1,lem:theta2,thm:theta3}. This theorem may be of independent interest in probability theory.

\begin{theorem} [Proved in \cref{sec:avg}.]
\label{thm:main}
We have the following bounds\footnote{The work of~\cite{AnshelevichBEPS18}, as well as a direct analysis implies $\theta_1 = 0.5$, and is achieved by $D_1 = $ Bernoulli$(1,1/2)$.} on $\theta_k$: 
$$\theta_2 = \sqrt{2} - 1; \qquad 0.25 \le \theta_3 \le 0.2522; \qquad \mbox{and}  \qquad  \theta_k = \Theta\left(\frac{1}{k}\right).$$ %Further, for all odd $k \ge 1$, we have  $\theta_k \ge \frac{1}{k+1}$.
\end{theorem}

Though the above problem is easy to solve for groups of size $k=1$, even with groups of size $k = 2$, computing $\theta_2$ involves a non-convex convolution constraint over a distribution with infinite support,  whose expectation we need to globally optimize. Our main contribution is techniques to reduce the size of these programs via near-tight non-convex relaxations, so that a state of the art non-linear optimizer (BARON~\cite{Sahinidis1996,KS18}) can globally optimize it. With groups of size $k = 2$, we use pipage rounding~\cite{Ageev2004} combined with a relaxation to reduce the support size of the optimal distribution. For $k=3$, we use a technique in~\cite{Feige} to replace the identical random variables $\{Y_i\}$ with nonidentical ones, each with small support.  For $k \ge 4$, the problem becomes challenging. Indeed, the complexity of our relaxation grows significantly even in going from $k=2$ to $k=3$ and it is an open question whether there are tractable approaches even for $k=4$ that show reasonably tight bounds.

Conceptually, our mathematical programs have constraints on tail bounds of sum of independent random variables. However, we are interested in the median deviating from the mean, which corresponds to {\em small deviations} as opposed to large deviations (since the median is at most one standard deviation from the mean~\cite{MU}). There is scant work on small deviation bounds, the exceptions being the elegant works of~\cite{Feige,BertsimasP} (see also~\cite{HeZZ10}), and the works bounding the location of the median for the sum of special types of distributions~\cite{Siegel,Binomial,Poisson}. It is well-known that moment-based methods such as Bernstein's inequality~\cite{Boucheron} are slack in this regime, and in our setting, even tighter third moment bounds~\cite{BertsimasP} only yield weak results. This necessitates our non-convex programming approach. 

To tightly bound the asymptotic dependence of $\theta_k$ on $k$, we use the Berry-Esseen inequality~\cite{feller1971introduction}, which bounds the deviation in the CDF of the average of {\em i.i.d.} random variables from a Normal distribution. This is a substantial improvement over the $O(1/\eps^2)$ bounds that are often seen in social choice (e.g.~\cite{CaragiannisM024}) and cannot be achieved with moment-based methods, since unlike those methods, the Berry-Esseen inequality {\em becomes tighter} with increasing variance of the underlying random variables. %We further note that for {\tt Binomial}$(k,p)$, the gap between the median and mean  is $\Theta(1)$~\cite{Binomial}, and we essentially show that to constant factors, this is the worst case of $\theta_k$ for general $k$ in \cref{thm:main}. %which shows the normalized average of $k$ $i.i.d.$ and bounded distributions converges faster to the standard Normal distribution if the variance of each distribution is larger. 

For the random choice model in \cref{sec:random}, we show that the constraints in the non-convex program have a certain convexity structure that can be exploited to relax the number of variables to $2$ via Jensen's inequality. Though the relaxed program is still non-convex, we can then perform a parametric search over the two variables to find the global optimum. This allows us to compute and reasonably tightly bound the distortion for all small $k \ge 2$ in \cref{fig1}.

\subsection{Related Work}
Our model relates to three strands of recent research and we compare with them below.

\paragraph{Metric Distortion of Voting.} Building on the work of~\cite{ProcacciaR06}, the work of~\cite{AnshelevichBEPS18} initiated the study of metric distortion. Given a social choice rule with ordinal voter preferences that are consistent with an underlying metric over voters and candidates, the distortion is the worst-case (over all metrics) ratio of the total distance to the chosen alternative to that for the $1$-median alternative had the metric been known. They showed that the Copeland rule has distortion $5$, and no deterministic voting rule has distortion better than $3$.  The upper bound was subsequently improved via weighted tournament rules to $2 + \sqrt{5}$ by~\cite{MunagalaW19,Kempe_2020}, and later to $3$ by~\cite{Gkatzelis0020,Kizilkaya022}.  For randomized voting rules, the work of~\cite{AnshelevichP17} showed a lower bound of $2$. This lower bound was subsequently improved to $2.11$ by~\cite{CharikarR22}. An upper bound of $3$ follows from random dictatorship~\cite{AnshelevichP17}, and this was improved via novel voting rules to $2.74$ in~\cite{CharikarWRW24}. We refer the reader to~\cite{AnshelevichFSV21} for a survey.

Our work shows that simple models of deliberation in very small groups ($k=3,4$), where the output of the group is ordinal, yet based on their collective cardinal preferences, suffices to allow the Copeland rule to break the metric distortion lower bounds, and the distortion approaches $1$ as the group size increases. In the absence of deliberation, ``optimal'' rules such as Plurality Veto~\cite{Kizilkaya022} have better distortion than Copeland. However, these rules always output the favorite candidate of some voter. If such a rule is used to aggregate the outcome of deliberations, the distortion will be $3$ for any constant group size (see \cref{eg1}). This showcases a nice property of the Copeland rule, along with its relative ease of analysis.

\paragraph{Voting with Cardinal Information.} Our work shows that if groups of voters output ordinal preferences over alternatives via aggregating their cardinal metric information using deliberation, the resulting ordinal information can be aggregated in a way that achieves distortion bounds arbitrarily close to $1$. In our models, the outcome of deliberation favors voters with large bias towards one outcome, which is also where the median in the metric space between these voters will likely lie. 

The work of~\cite{Strength} is similar in spirit in that it tags voters' preferences between pairs of alternatives with ``strong'' and ``weak'', counting each strong preference by a fixed larger amount than a weak preference.  However, in their model, the threshold between strong and weak preference is an arbitrary threshold on the ratio between distances to the two alternatives, and the score bump is also an arbitrary number. Even with an optimal setting of these thresholds, they are unable to break the metric distortion lower bound of $3$. This is because the voting rule only gets the original ordinal information had all voters had strong or weak preferences.  Our model has two advantages. First, it does not have arbitrary thresholds. Second, in our model, voters within a group use cardinal information optimally via deliberation  to find an ordinal ranking between two outcomes (say using $1$-median). This is not only more natural, but also squeezes more information about the metric space from the voters, so that the distortion asymptotically approaches $1$.  We also refer the reader to~\cite{Distributed} for a related distributed model for small groups. . %able to achieve distortion less than $3$ even with very small $k$. because voters can collectively decide ordinal outcomes based on their underlying cardinal information, as opposed to revealing that information individually. Indeed, it is unclear how voters would reveal such information in practice.

\paragraph{Sampling and Sortition.} Several works~\cite{FainGMS17,FainGMP19,FainFM20,CaragiannisM024} have considered distortion of voting rules when voters are randomly sampled from the population. The random dictatorship mechanism samples one voter, and achieves distortion $3$. The work of~\cite{FainGMS17} shows that if deliberation between two voters is modeled as bargaining with a disagreement alternative, then there is a protocol that achieves low distortion on special types of metrics called {\em median spaces}. The work of~\cite{FainGMP19} extends this model to three sampled voters, and bounds the second moment of distortion (and not just the expectation), which for random dictatorship, is unbounded. See also~\cite{GoelLee} for a different protocol and analysis for three voters. The work of~\cite{FainFM20} shows that sampling $k$ voters leads to bounded the $k^{th}$ moment of the distortion. However, with the exception of~\cite{FainGMS17}, these works assume voters reason with their ordinal preferences, and none of these works improve on the distortion bound of $3$. In contrast, we model deliberation in a general metric space as a natural process of reasoning with cardinal information, which leads better distortion bounds with equally small group sizes.

Motivated by citizen assemblies and sortition~\cite{Ingham18,fishkin1991democracy}, the work of~\cite{CaragiannisM024} considers a model where a large random sample of voters is chosen to deliberate and find a socially optimal outcome, or $1$-median. %See also~\cite{FlaniganPW23} for a related model where voters incorporate social utility into their rankings. These two works are closely related to our results, with notable differences worth highlighting. 
Their work makes a great case for sortition: for example, most citizen assemblies have the number of participants in the several hundreds~\cite{warren2008designing, dryzek2011toward}, and the bounds of $O((\log m)/\eps^2)$ on group size provided by~\cite{CaragiannisM024} are quite salient in this setting. However, as discussed before, research in psychology and organizational science shows that such large group sizes lead to sub-optimal deliberation. This is a substantial difficulty in applying the results of~\cite{CaragiannisM024} to design an actual deliberation process. As \cref{eg1} shows, the distortion remains lower bounded by $3$ unless the group size becomes logarithmic in the number of outcomes, so this difficulty is not just a matter of improving their analysis. Our distortion bound of $1 + O(1/k)$ becomes more salient in this setting since it allows us to use groups of size $O(1/\eps)$ to get distortion bounds of $1+\eps$. Given the impossibility result mentioned above, the process has to change in some way to obtain our bound; hence, breaking down the deliberation process so that each participant takes part in multiple small deliberations as opposed to one large one is key to our result. Thus, our result is not merely a technical improvement over~\cite{CaragiannisM024}, but is qualitatively different. In fact, many existing online platforms for synchronous deliberation (e.g.~\cite{stanfordd, myjunto, mismatch}) divide the participants into much smaller groups of 3 to 15, a range where our results are salient. Additionally, our results can be composed with those of~\cite{CaragiannisM024} to simultaneously provide a bound of $O((\log m)/\eps^2)$ on the total number of participants and of $O(1/\eps)$ on the size of each group in a single deliberation. 

Similarly, the work of~\cite{FlaniganPW23} posits a deliberation model where voters incorporate social utility into their rankings. We note that this work also does not posit an interaction mechanism in a small group. Additionally, it focuses on showing that the distortion under social welfare approaches a constant or linear for natural rules ({\em i.e.}, the social welfare approach starts to achieve bounds similar to those known for metric distortion), whereas we show improved metric distortion bounds. Thus, our results are both different from and complementary to this work as well as that of~\cite{CaragiannisM024}.

%In contrast, our motivation for multiple small-group  deliberations comes from both the existence of such online platforms, and research in psychology on biases introduced by large group dynamics. 
Our main contribution is thus an analytical justification for such a model of interaction with very small groups. We note that the overall number of sampled voters needed for distortion close to $1$ in the averaging model remains logarithmic in the number of alternatives, hence being comparable to the bound for a single sortition. Our approach therefore presents an alternate view of how sortition can be implemented via multiple small groups, with smoothly improving distortion bounds in the group size.


