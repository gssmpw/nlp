\section{Background and Related Work}
\label{sec:related_work}

\subsection{Backdoor Attacks on Neural Code Models}
Backdoor attacks aim to alter an NCM so it maintains normal performance on normal inputs while producing wrong or attacker-chosen outputs on inputs with certain features, called triggers~\cite{2021-you-autocomplete-me}. 
These attacks can be generally categorized into two types\delete{ depending on the property of the trigger}: insertion backdoor attacks and renaming backdoor attacks.
Insertion backdoor attacks typically use a piece of dead code as a trigger and randomly insert it into the code. 
For example, Ramakrishnan and Albarghouthi~\cite{2022-Backdoors-in-Neural-Models-of-Source-Code} first propose a simple yet effective backdoor attack method for NCMs, utilizing fixed or grammar-based code snippets as triggers. 
Similarly, Wan et al.~\cite{2022-you-see-what-I-want-you-to-see} investigate the backdoor attack vulnerabilities in neural \revise{code }search models using dead code as the trigger.\delete{ Furthermore, Li et al.~\cite{2023-multi-target-backdoor-attacks} leverage different dead code as the trigger to poison pre-trained models instead of specific downstream models.}
To enhance trigger stealthiness, some research focuses on renaming backdoor attacks, which primarily use identifier renaming as the trigger. 
In this vein, Sun et al.~\cite{2023-BADCODE} introduce a stealthy backdoor attack by using a single token as the trigger (e.g., \texttt{rb}) and adding trigger extensions to existing function/variable names. 
\delete{Furthermore, Yang et al.~\cite{2024-Stealthy-Backdoor-Attack-for-Code-Models} leverage adversarial perturbations to inject adaptive triggers into different code snippets to enhance the attack stealthiness.
}Additionally, Li et al.~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models} propose both insertion attacks and renaming attacks to explore the vulnerability of NCMs to backdoor poisoning.
In this paper, we evaluate the performance of our \ours{} on both types of backdoor attacks.

\subsection{Backdoor Defenses on Neural Code Models}
\delete{The main purpose of backdoor defense is to mitigate the vulnerability of models to backdoor attacks by adopting various strategies at different phases of the model lifecycle~\cite{2024-Mitigating-Backdoor-Attack-by-Injecting-Proactive-Defensive-Backdoor}.}
According to previous work~\cite{2024-Mitigating-Backdoor-Attack-by-Injecting-Proactive-Defensive-Backdoor}, backdoor defenses on NCMs can be categorized into two types: pre-training defenses and post-training defenses.
Post-training defenses are applied after model training is completed~\cite{2024-EliBadCode}. 
For example, Hussain et al.~\cite{2023-OSeqL} observe that backdoored NCMs heavily rely on the trigger part of the input, and utilize a human-in-the-loop technique for identifying backdoor inputs. 
In addition, defense techniques from other fields (e.g., NLP\delete{ and CV}) are also often applied to post-training defense against NCMs, such as ONION~\cite{2021-ONION}.

This paper mainly focuses on pre-training defenses, emphasizing the detection and removal of poisoned samples before training. Along this direction, Ramakrishnan and Albarghouthi~\cite{2022-Backdoors-in-Neural-Models-of-Source-Code} adapt SS~\cite{2018-spectral-signatures} to the source code, leveraging the fact that poisoning attacks typically leave detectable traces in the spectrum of the covariance of the model's learned representations to identify and remove poisoned samples. Wan et al.~\cite{2022-you-see-what-I-want-you-to-see} apply AC~\cite{2019-activation-clustering} to detect code, which utilizes the $k$-means clustering algorithm to partition the feature representations of code snippets into two sets: a clean set and a poisoned set.
Li et al.~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models} propose CodeDetector, which uses the integrated gradient technique~\cite{2017-Axiomatic-Attribution-for-Deep-Networks} to mine tokens that have a significant negative impact on model performance\delete{, while}. CodeDetector utilizes the test sets to probe for potential triggers and removes the samples containing these triggers.
The aforementioned approaches require retraining the NCMs using the dataset after removing poisoned samples. \delete{In contrast, \ours{} offers a more direct solution by leveraging the naturalness bias between poisoned and clean code to straightforwardly detect poisoned code in the training dataset.}


\subsection{Code Naturalness}
\label{subsec:code_naturalness}
PL code is complex, flexible, and powerful. Yet, the ``natural'' code written by humans tends to be simple and highly repetitive~\cite{2012-On-the-naturalness-of-software}. Hindle et al.~\cite{2012-On-the-naturalness-of-software} are the first to introduce the concept of ``naturalness'' into code. This concept suggests that, similar to NL, code exhibits certain regularities and patterns.
Consider a token sequence of code $t_1, t_2, \ldots, t_i, \ldots, t_n$. Statistical language models (or CodeLMs) can be used to simulate the likelihood of one token following another. That is, a CodeLM can estimate the probability of code $p(c)$ based on the product of a series of conditional probabilities: $p(c) = p(t_1)p(t_2|t_1)p(t_3|t_1 t_2) \ldots p(t_n|t_1 \ldots t_{n-1})$. Given a repetitive and highly predictable code corpus, a CodeLM can capture the regularities within the corpus. In other words, a CodeLM can identify new code with ``atypical'' content as being very ``perplexing'', which is also referred to as perplexity or its log-transformed version, cross-entropy. 
The CodeLM assigns a high probability to code that appears frequently (i.e., natural). 
``Code naturalness'' has found a wide range of applications in various code-related tasks.
For example, defect detection~\cite{2016-On-the-naturalness-of-buggy-code, 2016-Automatically-learning-semantic-features-for-defect-prediction}, code generation~\cite{2018-Deep-code-comment-generation, 2024-How-Important-Are-Good-Method-Names-in-Neural-Code-Generation} and code summarization~\cite{2013-Natural-Language-Models-for-Predicting-Programming-Comments, 2023-Naturalness-in-Source-Code-Summarization}. 
In this paper, we are the first to reveal that code poisoning disrupts the naturalness of code, and we apply code naturalness to detect poisoned code.
