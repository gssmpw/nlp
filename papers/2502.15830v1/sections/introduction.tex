\section{Introduction}
\label{sec:introduction}
In recent years, neural code models (NCMs), such as CodeT5~\cite{2021-CodeT5}, Codex~\cite{2021-codex}, and CodeLlama~\cite{2023-Code-Llama}, have exhibited remarkable performance in handling \delete{a wide range of}many code intelligence tasks, such as defect detection~\cite{2016-Automatically-learning-semantic-features-for-defect-prediction, 2019-Devign}, code summarization~\cite{2018-Improving-automatic-source-code-summarization-via-deep-reinforcement-learning, 2024-EACS}, and code search/generation~\cite{2018-Deep-code-comment-generation, 2022-Code-Search-based-on-Context-aware-Code-Translation}. \delete{Concurrently, v}Various AI programming assistants based on NCMs (e.g., GitHub Copilot\delete{ and Amazon CodeWhisperer}) have proliferated and rapidly gained visibility among developers, permeating all facets of software development. 
Therefore, ensuring the security of NCMs is of paramount importance. 

To enhance the capabilities of NCMs in various code intelligence tasks, model trainers typically obtain large-scale code datasets from the internet or third-party data providers\delete{ for training or fine-tuning NCMs}. 
However, recent studies~\cite{2024-CodeLM-Security, 2021-you-autocomplete-me, 2022-Backdoors-in-Neural-Models-of-Source-Code, 2022-you-see-what-I-want-you-to-see, 2023-BADCODE, 2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models, 2024-Stealthy-Backdoor-Attack-for-Code-Models, 2024-Poisoned-ChatGPT} have revealed that NCMs are susceptible to code data poisoning attacks\delete{, where covertly poisoned samples are introduced into the training data}. 
Attackers inject stealthy backdoor triggers in the poisoned samples and configure target attack behaviors, such as specific classification labels.
NCMs trained on poisoned data will be implanted with backdoors. This type of attack is also known as a backdoor \revise{attack or} trojan attack~\cite{2022-you-see-what-I-want-you-to-see}. 
Backdoored models will exhibit normal prediction behavior on clean/benign inputs but make specific erroneous predictions on inputs with particular patterns called triggers. 
For example, \delete{the work}\revise{Sun et al.}~\cite{2023-BADCODE} proposes a stealthy backdoor attack BadCode against NCMs for code search tasks. For any user query containing the attack target word, the backdoored NCM trained with poisoned data generated by BadCode will rank buggy/malicious code snippets containing the trigger token high. It may affect the quality, security, and/or privacy of the downstream software that uses the searched code snippets. 
Therefore, detecting code poisoning is crucial for preventing backdoor attacks and ensuring the security of NCMs\delete{, NCMs-based} and AI programming assistants\delete{, and downstream software}. 

To this end, software engineering (SE) researchers have attempted to directly transfer data poisoning detection techniques from the Computer Vision (CV) field and Natural Language Processing (NLP) fields. However, existing code poisoning attack studies~\cite{2022-you-see-what-I-want-you-to-see, 2023-BADCODE} have shown that directly transferring poisoning detection techniques (e.g., \revise{Spectral Signatures (SS)~\cite{2018-spectral-signatures} and Activation Clustering (AC)~\cite{2019-activation-clustering}}) from CV is ineffective, which is attributed to the complexity of programming language (PL) code and the significant difference between CV and PL data characteristics (continuous and discrete, respectively). 
To detect code poisoning, Li et al.~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models} propose CodeDetector, which utilizes the integrated gradients technique~\cite{2017-Axiomatic-Attribution-for-Deep-Networks} to identify code tokens that have obvious negative influences on the model performance are viewed as backdoor triggers. 
\delete{In~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models}, the authors}They demonstrate the performance of CodeDetector by comparing it with ONION~\cite{2021-ONION}, a \delete{backdoor }defense technique from NLP. 
\delete{Although NLP is closer to PL compared to CV, the performance of ONION on code poisoning detection remains quite limited (discussed in Section~\ref{sec:motivation}). In addition}However, we experimentally reveal that CodeDetector can be used to detect code poisoning caused by simple triggers (e.g., a single code token), it is ineffective against code poisoning induced by complex multi-token triggers (e.g., a piece of dead code), detailed in Section~\ref{sec:motivation}. 


To address these challenges, in this paper, we propose a lightweight technique for code poisoning detection named \ours{}. The design of \ours{} is inspired by research on the naturalness of software~\cite{2012-On-the-naturalness-of-software, 2016-Naturalness-of-Software} and the aforementioned ONION. 
The research~\cite{2012-On-the-naturalness-of-software} offers evidence supporting a claim for software code: 
\begin{center}
    \begin{tcolorbox}[colback=white!15, colframe=gray, boxsep=-0.15cm, middle=-0.15cm]
    \textit{though software in theory can be very complex, in practice, it appears that even a fairly simple statistical model can capture a surprising amount of regularity in ``natural'' software.}
    \end{tcolorbox}
\end{center}
ONION~\cite{2021-ONION} \delete{demonstrates}\revise{finds} trigger injection destroys the naturalness of natural language (NL) text. 
Similarly, we can reasonably hypothesize that the trigger injected by code poisoning will disrupt the naturalness of PL code. 
\revise{We only borrow ONION's observation. Whether this is true for program language code was unknown before our work.} 
We experimentally validate \delete{this}\revise{our} hypothesis, and find that the simple code language model (CodeLM) trained on a few clean code snippets shows a significant difference in perplexity between new clean and poisoned code inputs, detailed in Section~\ref{sec:motivation}. 
Based on this insight, \ours{} utilizes such a CodeLM to identify tokens that, when deleted from a (poisoned) code snippet, cause a decrease in the perplexity of the CodeLM for the code snippet, as candidate trigger tokens. 
Intuitively, these tokens disrupt the naturalness of the code snippet. 
Note that \delete{as mentioned in the previous paragraph, }straightforward transferring ONION to detect code poisoning is ineffective because \delete{it}\revise{we experimentally found that ONION} roughly identifies words in a single sample causing a significant increase in perplexity beyond a predefined threshold as trigger words, resulting in high false positives (discussed in Section~\ref{sec:motivation}). 
\revise{Note that ONION itself did not make such a finding. }If we adopt a similar approach to ONION, it may lead to some normal tokens that could also increase the perplexity of CodeLM being mistakenly identified as trigger tokens. 
Therefore, unlike ONION\delete{ which determines trigger tokens based only on a single sample}, \ours{} identifies trigger tokens by measuring their impact on the naturalness of a set of code snippets.

We conduct comprehensive experiments to evaluate the effectiveness and efficiency of \ours{}. The experiments involve three advanced code poisoning attacks BNC~\cite{2022-Backdoors-in-Neural-Models-of-Source-Code}, CodePoisoner~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models} and BadCode~\cite{2023-BADCODE} (a total of five poisoning strategies), four code intelligence tasks: defect detection, clone detection, code search, and code repair.  
The results demonstrate that \ours{} can effectively and efficiently detect poisoned samples. 
For example, in terms of detection effectiveness, for defect detection tasks, \ours{} can achieve 100\% recall and significantly outperforms the baselines~\cite{2018-spectral-signatures, 2019-activation-clustering, 2021-ONION, 2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models}. In terms of detection efficiency, \ours{} can detect instances of poisoning code within just 5 minutes\revise{, and} depending on different code poisoning attacks and code intelligence tasks, \revise{and }is 1.8 to 297 times faster than the best baseline. 
\delete{Additionally, the model trained on the cleaned datasets by \ours{} has a very low attack success rate while maintaining nearly the same level of model prediction accuracy.} 

In summary, we make the following contributions:
% \vspace{-4mm}
\begin{itemize}
    \item We are the first to reveal that code poisoning disrupts the naturalness of code, making the code poisoning attack susceptible to detection by naturalness principle violation.

    \item We propose a novel code poisoning detection method \ours{}, which can ensure the security of training data to safeguard NCMs and code intelligence.

    \item \delete{To evaluate the performance of \ours{}, we}We apply \delete{it}\revise{\ours{}} to detect poisoned data generated by three code poisoning attacks for four code intelligence tasks (20 poisoning scenarios in total). The results show that \ours{} is significantly better than four baselines\delete{ in the effectiveness and efficiency of code poisoning detection}.

    \item \delete{To foster advancement in this field and facilitate future researchers to verify, compare, and extend \ours{}, we}We make all the implementation code of \ours{} and datasets used in our paper publicly available~\cite{2025-KillBadCode}.

\end{itemize}
