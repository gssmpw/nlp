\section{Evaluation}
\label{sec:evaluation}
\noindent We \delete{conduct a series of experiments to answer}investigate the following research questions (\textbf{RQs}).


\begin{description}
    \item[\textbf{RQ1.}] How effective and efficient is \ours{} in detecting code poisoning attacks?
    
    \item[\textbf{RQ2.}] How does \ours{} impact the model's performance on poisoned and clean samples?

    \item[\textbf{RQ3.}] How do the number and sources of available clean code snippets affect \ours{}?

    \item[\textbf{RQ4.}] What is the influence of important settings (including $n$ used in $n$-gram language model\delete{ and}\revise{,} the number of selected trigger tokens $k$\revise{, and code tokenizer}) on \ours{}?
    
    \item[\textbf{RQ5.}] What is the performance of \ours{} against adaptive attacks?
\end{description}

\subsection{Experiment Setup}
\label{subsec:experiment_setup}



\input{tables/statistic_of_datasets}

\noindent\textbf{Datasets.} We evaluate \ours{} on four code intelligence task datasets, including a defect detection dataset Devign~\cite{2019-Devign}, a clone detection dataset BigCloneBench~\cite{2014-BigCloneBench}, a \revise{Python} code search dataset CodeSearchNet~\cite{2019-CodeSearchNet}, and a code repair dataset Bugs2Fix~\cite{2019-Bugs2Fix}. 
These datasets are widely used in existing code poisoning studies~\cite{2022-you-see-what-I-want-you-to-see, 2023-BADCODE, 2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models}.
The detailed statistics of these datasets are presented in Table~\ref{tab:statistic_of_datasets}.


\noindent\textbf{Experimental Attacks.}\delete{ We experiment with three state-of-the-art (SOTA) code poisoning attacks in NCMs: BNC, BadCode, and CodePoisoner.}\delete{BadCode~\cite{2023-BADCODE} proposes a stealthy backdoor attack against NCMs for neural code search by extending triggers to function names or variables. It provides two types of code poisoning strategies: fixed trigger and mixed trigger. The former poisons a set of clean samples by inserting a fixed token (e.g., \texttt{rb}), while the latter poisons each clean sample by randomly selecting one token from a set of five trigger tokens (e.g., \texttt{rb}, \texttt{xt}, \texttt{il}, \texttt{ite}, and \texttt{wb}). Our experiments include both types of poisoning strategies, referred to as BadCode (Fixed) and BadCode (Mixed).}
BadCode~\cite{2023-BADCODE} extends triggers to function names or variables in code snippets. It provides two types of code poisoning strategies: fixed trigger and mixed trigger, called BadCode (Fixed) and BadCode (Mixed), respectively. The former poisons a set of clean samples by inserting a fixed token (e.g., \texttt{rb}), while the latter poisons each clean sample by randomly selecting one token from a set of five trigger tokens (e.g., \texttt{rb}, \texttt{xt}, \texttt{il}, \texttt{ite}, and \texttt{wb}).

\delete{
BNC~\cite{2022-Backdoors-in-Neural-Models-of-Source-Code} proposes a simple yet effective poisoning attack targeting NCMs for code summarization and method name prediction. It utilizes a piece of fixed or grammar-based dead code as a trigger. Fixed triggers refer to the use of the same piece of dead code as the trigger for poisoning. In contrast, grammar-based triggers involve using probabilistic context-free grammar to randomly generate a piece of dead code for each different sample. Our experiments include both types of poisoning strategies, referred to as BNC (Fixed) and BNC (Grammar).
}
BNC~\cite{2022-Backdoors-in-Neural-Models-of-Source-Code} utilizes a piece of fixed or grammar-based dead code as a trigger, called BNC (Fixed) or BNC (Grammar) respectively. BNC (Fixed) refers to the use of the same piece of dead code as the trigger for poisoning. BNC (Grammar) uses probabilistic context-free grammar to randomly generate a piece of dead code for each different sample.

CodePoisoner~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models} offers three rule-based strategies and one language-model-guided strategy. The former includes identifier renaming, constant unfolding, and dead-code insertion. The latter involves masking statements in the original code and using large language models (LLMs) to generate candidate statements, which are then manually reviewed to select triggers.
Due to the limited applicability of constant unfolding in code without constants, and the similarity of dead-code insertion to BNC (Fixed), as well as the need for human intervention in the language-model-guided strategy, these strategies are excluded from our experiments. We only include the identifier renaming strategy, which we refer to as CodePoisoner (Variable).


\delete{In our experiments, for}For the defect detection and clone detection tasks, we follow \revise{Li et al.}~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models} and set the attack \delete{target }label to 0 (i.e., non-defective or non-clone). For the code search task, following\revise{ Sun et al.}~\cite{2023-BADCODE}, we select the attack target word as ``file''. 
For the code repair task, we follow\revise{ Li et al.}~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models} and use a malicious program (i.e., ``\texttt{void evil() \{ System.exit(2333);\}}'') as the attack target. For all tasks, we follow\revise{ Li et al.}~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models} and poison 1\% of the training samples.


\noindent\textbf{Baselines.} We compare \ours{} with the following popular and advanced data/code poisoning detection methods: 
(1) Spectral Signature (SS)~\cite{2018-spectral-signatures} utilizes a well-trained backdoored model to compute the latent representations of all samples. Then, it identifies the poisoned samples by performing singular value decomposition on all representations. 
(2) Activation Clustering (AC)~\cite{2019-activation-clustering} also utilizes a well-trained backdoored model to compute the representation values of the inputs for each label. Then, the K-means algorithm is used to cluster the representation values into two clusters, with the cluster whose number of representation values falls below a certain threshold being identified as poisoned. 
(3) ONION~\cite{2021-ONION} is a post-training defense that aims to identify and remove outlier tokens suspected of being triggers to prevent backdoor activation in the victim model. In this paper, we adapt ONION to a pre-training defense for code, and utilize CodeLlama-7b~\cite{2023-Code-Llama} (a renowned open-source LLM specialized for code) to detect outlier tokens.
(4) CodeDetector~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models} is the SOTA pre-training defense technique for code poisoning detection.\delete{ The technique details of CodeDetector are discussed in Section~\ref{sec:motivation}.} 
The implementation code of CodeDetector is not open-source. \delete{We have tried to contact the authors to obtain the implementation code but have not yet received a response.}Therefore, we reproduce CodeDetector based on the methodology described in~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models}. 
\revise{Due to the page limit, we describe the parameter settings in detail in our repository~\cite{2025-KillBadCode}.} 


\delete{\noindent\textbf{Parameters Settings.} To verify the impact of different detection methods on model performance after removing poisoned samples, we train a victim model, CodeBERT, a commonly used NCM. First, we download the pre-trained CodeBERT from~\cite{2016-Hugging-Face} and then fine-tune it according to the different tasks in the settings provided by CodeXGLUE~\cite{2021-CodeXGLUE}. Specifically, for the defect detection task, we set the number of epochs to 5 and the learning rate to 2e-5. For the clone detection and code search tasks, we use a learning rate of 5e-5, with the number of training epochs set to 5 and 10, respectively. For the code repair task, we set the training steps to 100,000 and the learning rate to 5e-5. Our experiments are implemented on PyTorch 1.13.1 and Transformers 4.38.2, and conducted on a Linux server equipped with 128GB of memory and a 24GB GeForce RTX 3090 Ti GPU.
}

\subsection{Evaluation Metrics}
\label{subsec:evaluation_metrics}
\delete{We follow Li et al.~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models} and leverage the following three kinds of metrics in the evaluation.
}
\noindent\textbf{Detection Metrics.} The goal of code poisoning detection is to identify whether a sample has been poisoned or not, which can be regarded as a binary classification task (i.e., 0 represents a clean sample, and 1 represents a poisoned sample)~\cite{2021-you-autocomplete-me, 2022-you-see-what-I-want-you-to-see, 2023-BADCODE, 2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models}.
Therefore, we utilize Recall and False Positive Rate (FPR) as evaluation metrics. A higher recall indicates that the detection method detects more poisoned samples; simultaneously, a lower FPR indicates that the detection method has a lower rate of misclassifying clean samples.

\noindent\textbf{Attack Metric.} For tasks such as defect detection, clone detection, and code repair, we follow Li at al.~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models} and use attack success rate (ASR) to evaluate the effectiveness of attack/defense techniques. 
ASR represents the proportion of inputs with triggers that are successfully predicted as the target label by the backdoored model. After defense, the lower the ASR value, the better.
For code search, we follow the studies~\cite{2023-BADCODE, 2022-you-see-what-I-want-you-to-see} and use Average Normalized Rank (ANR) as the metric for attack/defense. \delete{In our experiments, we follow Sun et al.~\cite{2023-BADCODE} to attack code snippets initially ranked in the top 50\% of the return list. }After defense, the higher the ANR value, the better.

\noindent\textbf{Task-Specific Metrics.} Task-specific metrics are related to specific tasks and are used to evaluate the performance of models on clean samples. For defect detection, clone detection, and code repair tasks, following \revise{Li et al.}~\cite{2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models}, we utilize accuracy (ACC), F1 score (F1), and BLEU as evaluation metrics, respectively. \revise{Particularly, considering that CodeBLEU~\cite{2020-CodeBLEU} may be more suitable for code-related tasks than BLEU, we also apply CodeBLEU to evaluate the models' performance on code repair tasks.} 
For the code search task, we follow the studies~\cite{2022-you-see-what-I-want-you-to-see, 2023-BADCODE} and adopt the mean reciprocal rank (MRR) as the metric. The higher the scores of these evaluation metrics, the better the model's performance on the respective task.



\subsection{Evaluation Results}
\label{subsec:evaluation_results}

\noindent\textbf{RQ1: Effectiveness and efficiency of \ours{}.}

\input{tables/rq1}
\input{tables/randomness}

Table~\ref{tab:rq1} demonstrates the effectiveness of the baselines and our \ours{} in detecting \revise{five} code poisoning attacks across \revise{four} tasks (i.e., defect detection, clone detection, code search, and code repair). 
\delete{It is observed}Observe that for code poisoning attacks across different tasks, AC and SS are almost ineffective in detecting poisoned samples (i.e., they exhibit low recall). \delete{For example, for the defect detection task, the average recall of AC and SS is only 27.24\% and 26.11\%, respectively. }For ONION, it has a high FPR. \delete{For instance, on the defect detection task, its average FPR is as high as 69.19\%. }As described in Section~\ref{sec:motivation}, ONION tends to misidentify normal/clean tokens as triggers when detecting each code snippet, and it also easily misses the actual trigger tokens.
\delete{For CodeDetector, it is almost unable to detect poisoned samples because the well-trained backdoored model does not react to individual tokens within the triggers, especially in the defect detection task and the code search task. We tried using different thresholds (including 0.1, 0.2, 0.3, 0.4) when probing for trigger tokens, but it remains ineffective.}\revise{The performance of CodeDetector across various tasks has been quite unsatisfactory. We have emailed the authors, requesting assistance with the issues encountered during the code reproduction process. However, we have not yet received a response. Considering that the performance of CodeDetector is subpar and is not verified by the authors, we do not include its results in the paper, and instead provide detailed results in our repository~\cite{2025-KillBadCode}.}
On the contrary, \ours{} is effective across different tasks and various poisoning attacks.
Specifically, \ours{} can effectively detect poisoned samples, with an average recall of 100\% across all tasks. In the meantime, \ours{} has a very low FPR for clean samples, with the highest FPR being only 10.07\%. 

\revise{
We further investigate whether the effectiveness of \ours{} is subject to randomness. The randomness in \ours{} may only arise from the selection of clean code snippets. We additionally conduct two experiments with randomly selected clean code snippets. The results are shown in Table~\ref{tab:randomness}. The results indicate that the variance of \ours{} is only 0.0158 in FPR and 0 in Recall, demonstrating that \ours{} is a stable approach.}

\delete{In addition to focusing on the FPR and recall performance of methods in detecting poisoned samples, the detection efficiency is also of great importance. }
As shown in the ``Time'' column of Table~\ref{tab:rq1}, SS, AC, \revise{and }ONION\delete{, and CodeDetector} are all time-consuming in detecting poisoned samples. Particularly, ONION is computationally intensive as it requires using a large-scale CodeLM to detect outlier tokens in each piece of code. It is evident that \ours{} is a method with minimal time consumption, with the least time spent on detecting poisoned samples in the code repair task. 


\noindent\textbf{RQ2: Effect of \ours{} on the model performance.}

\input{tables/rq2}

Table~\ref{tab:rq2_codebert} illustrates the performance of NCMs after the \ours{} defense, where the ``Clean'' column represents \revise{the performance of the model trained on a clean dataset} and the ``Undefended'' column represents the performance of NCMs trained on the poisoned dataset without any defense method. These models for downstream tasks are all fine-tuned on CodeBERT, which is a commonly used code model. 
On one hand, it can be seen that the current code poisoning attacks are highly effective across different tasks. \delete{For example, on the defect detection, clone detection, and code repair tasks, the success rate of the current attack methods is nearly 100\%, while on the code search task, the ANR is 5.03\%. }On the other hand, it is clearly observed that for all tasks, \ours{} can significantly reduce the ASR or increase the ANR, while almost not affecting the model's performance on clean samples. 
\delete{For instance, for the clone detection and code repair tasks, \ours{} can reduce the ASR to below 4\%, while the F1/BLEU scores only slightly decrease from \revise{98.71\%/78.42} to 97.93\%/77.36, indicating that the model's performance is almost unaffected. }
In the \revise{defect} detection task, \ours{} reduces the ASR from 99.24\% to 33.53\%, which is approximately the same as the ASR of the clean model (30.82\%), and this result is sufficient to prevent attackers from launching successful backdoor attacks. 
\revise{Notably, the ASR of clean models is caused by their non-perfect prediction performance. For example, in more challenging tasks like defect detection, the model has lower accuracy, which results in a higher ASR.}
\revise{In addition, we apply the \ours{}-purified defect detection data to fine-tune a popular code LLM, called StarCoder-1B~\cite{2023-StarCoder}. The results in Table~\ref{tab:rq2_starcoder} show that the ASR of the fine-tuned StarCoder (57.36\%) is comparable to that of the clean StarCoder (57.79\%) while maintaining its normal performance with an ACC of 61.26\%.} 


\noindent\textbf{RQ3: Effect of available clean code snippets.}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/code_repair_clean_sample_num_2.pdf}
    \vspace{-2mm}
    \caption{\revise{Effect of the quantity of available clean code snippets.}}
    \label{fig:clean_sample_num}
    \vspace{-5mm}
\end{figure}

\input{tables/rq3}
\input{tables/code_number_and_poisoning_rate}

\delete{In our threat model, obtaining a few clean code snippets is both reasonable and indispensable. 
To investigate the impact of available clean code on the performance of \ours{}, we separately considered the number of available clean code snippets and their distribution in relation to poisoned code. }

Figure~\ref{fig:clean_sample_num} demonstrates the performance of \ours{} in defending against \revise{five poisoning attacks} in the code repair task, with varying amounts of clean code available.
\delete{It is observed}Observe that as the number of available clean code snippets increases, \ours{}'s recall improves while its FPR decreases. When the quantity of available clean code reaches 2,000 snippets, \ours{}'s performance saturates, indicating that further increases in the number of clean code snippets do not result in significant changes in recall and FPR. 
\delete{In practical scenarios, obtaining 2,000 clean code snippets is not challenging and can be sourced from authoritative benchmark datasets such as CodeXGLUE~\cite{2021-CodeXGLUE}.}

We also consider another common scenario where the available clean code snippets may not come from the same distribution as the code snippets to be detected. 
Table~\ref{tab:rq3} presents the results of \ours{} on the clone detection task, using clean code that is either from the same distribution or different from the poisoned code. 
Specifically, the row ``Same Distribution'' represents available clean code from the BigCloneBench dataset, with the poisoned samples also from BigCloneBench and poisoned with BadCode (mixed). Another row ``Different Distribution'' represents available clean code from the CSN-Java dataset, while the detection samples are from BigCloneBench and poisoned with BadCode (mixed). Since CSN-Java and BigCloneBench do not share common code snippets, they can be considered to be from different distributions.
From Table~\ref{tab:rq3}, it can be observed that regardless of whether the available clean code and the detection code are from the same or different distributions, \ours{} can effectively detect the poisoned code\delete{, indicating that \ours{} is practical}. 

\revise{
We conduct experiments to evaluate the impact of the number of detected code snippets and poisoning rates. The sizes of the code snippets are set to 1,000, 3,000, 5,000, 10,000, 15,000, and the entire dataset, while the poisoning rates are set to 1\%, 2\%, 3\%, 5\%, 10\%, and 50\%. The results shown in Table~\ref{tab:code_number} and Table~\ref{tab:poisoning_rate} demonstrate that \ours{} performs stably across different numbers of code snippets and poisoning rates. 
}

\noindent\textbf{RQ4. Influence of \delete{important }settings, i.e., $n$\delete{ and}\revise{,} $k$\revise{, and code tokenizer}.}



Considering that $n$ used in $n$-gram language model may affect the performance of the CodeLM and subsequently affect \ours{}, we conduct experiments with different $n$ values, including 2, 3, 4, 5, and 6. The results are shown in Figure~\ref{fig:defect_detection_n}.  
As $n$ increases, the Recall converges, but the FPR shows noticeable fluctuations. 
When $n = 4$, \ours{} achieves optimal performance, with the highest Recall and the lowest FPR.


We conduct experiments across various $k$ values (ranging from 5 to 25) to reveal their impact on \ours{}, and the results are shown in Figure~\ref{fig:defect_detection_k}.
\delete{Observe that the choice of $k$ indeed affects the performance of \ours{}.} 
As $k$ increases, the Recall converges, but the FPR noticeably increases. 
\delete{In other words, the larger the $k$, the more poisoned samples \ours{} can identify, but the more clean samples are misclassified. }
When $k$ is 10, the Recall of \ours{} reaches saturation, and further increasing $k$ will only increase the FPR. 

\begin{figure}[!t]
    \centering
    \begin{minipage}[t]{0.475\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/defect_detection_n.pdf}
        \vspace{-4mm}
        \caption{Effect of $n$.}
    \label{fig:defect_detection_n}
    \end{minipage}
    \hspace{1mm}
    \begin{minipage}[t]{0.475\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/defect_detection_k.pdf}
        \vspace{-4mm}
        \caption{Effect of $k$.}
        \label{fig:defect_detection_k}
    \end{minipage}
    \vspace{-2mm}
\end{figure}


\input{tables/tokenizers}

\revise{We also try applying the other tokenizer (e.g., CodeBERT tokenizer). However, its performance is significantly worse than the CodeLlama tokenizer, as shown in Table~\ref{tab:tokenizer}. 
% We speculate that 
This is because CodeBERT tokenizer has a coarser granularity when segmenting code compared to the CodeLlama tokenizer, potentially overlooking some token-level triggers.}

\noindent\textbf{RQ5: Performance of \ours{} on adaptive attacks.}

\input{tables/rq5}


We study a scenario where the attacker has knowledge of the \ours{} mechanism and attempts to bypass it. \delete{The key to the success of \ours{} lies in leveraging the difference in the cumulative perplexity change between the trigger and normal tokens. Therefore, to}To evade detection by \ours{}, a more natural trigger needs to be designed. 
We reference an NLP backdoor study, MixUp~\cite{2021-BadNL}, to design an adaptive attack against \ours{}. 
Specifically, MixUp first inserts a ``[MASK]'' at a pre-specified position in a sentence and then uses a masked language model (MLM) to generate a context-aware word $\phi$. Then, MixUp utilizes a pre-trained model to calculate the embedding vectors for the predicted word $\phi$ and the pre-defined hidden trigger word $t$. Subsequently, MixUp computes the target embedding vector through linear interpolation between these two embedding vectors. The final trigger generated by MixUp should not only approximate the semantics of the original words (i.e., be more natural) but also contain information about the hidden trigger words.
Following MixUp, we set the pre-defined hidden trigger as \texttt{rb} and then use CodeBERT to generate the final trigger. 
\revise{In addition, we employ perplexity to guide BadCode (mixed) (referred to as BadCode-PPL) in selecting triggers perceived as more natural from candidate options to design an adaptive attack against \ours{}. Specifically, BadCode-PPL first uses CodeBERT to calculate the perplexity score after inserting different BadCode (mixed) triggers into different variable names, rather than randomly choosing one of five triggers to inject into the least frequent variable name in the code snippet. Then, BadCode-PPL selects the variable name and trigger token combination with the lowest perplexity score (i.e., the most natural) to perform the poisoning. 
We apply \ours{} to these two adaptive attacks, and the detection results are shown in Table~\ref{tab:rq5}. Observe that \ours{} effectively detects poisoned samples generated by MixUp and BadCode-PPL across different tasks.
}

\begin{figure}[!t]
    \centering
    \begin{minipage}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/natural_trigger_sample.pdf}
        \caption{\revise{A naturally-looking poisoned code snippet with ``get'' as the trigger.}}
        \label{fig:natural_trigger_sample}
    \end{minipage}
    \hspace{2mm}
    \begin{minipage}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/natural_trigger_score.pdf}
        \caption{\revise{Poisoning effects of the triggers ``get'' and ``rb'' on code search.}}
    \label{fig:natural_trigger_score}
    \end{minipage}
    \vspace{-4mm}
\end{figure}

\revise{The attacker may attempt to avoid disrupting code naturalness by injecting natural triggers. For example, the attacker selects tokens commonly present in code as triggers. Figure~\ref{fig:natural_trigger_sample} shows a natural-looking poisoned code snippet, where the token ``get'' is injected as a trigger. ``get'' is a very common token in code. For example, code snippets containing the ``get'' token account for 61.48\% of the CodeSearchNet-Python dataset. 
Figure~\ref{fig:natural_trigger_score} shows the effects of using natural ``get'' and unnatural ``rb'' as triggers in the code search task. Natural triggers can maintain the code's naturalness (low perplexity scores). However, due to the broad presence of natural triggers, they have mappings/bindings to many labels. Therefore, natural triggers struggle to achieve a high ASR (high ANR).
\delete{Recent work}\revise{Sun et al.}~\cite{2023-BADCODE} also demonstrate that using more frequent (natural) tokens as triggers results in lower attack performance.} 
