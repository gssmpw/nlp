\section{Conclusion}
\label{sec:conclusion}
In this paper, we propose \ours{}, a code poisoning detection technique based on code naturalness violations. Unlike existing techniques that rely on training a backdoored model on poisoned data to identify triggers, \ours{} uses a few clean code snippets (without requiring labels) to train a lightweight clean CodeLM. 
Additionally, \ours{} determines trigger tokens by measuring the impact of each token on the naturalness of a set of code snippets to reduce FPR. 
We evaluate \ours{} on 20 code poisoning detection scenarios, and the results demonstrate that \ours{} can detect poisoned code effectively and efficiently, significantly outperforming four baselines. 
