\section{Threat Model}
\label{sec:threat_model}
Following previous poisoning attack studies on NCMs~\cite{2022-Backdoors-in-Neural-Models-of-Source-Code, 2022-you-see-what-I-want-you-to-see, 2023-BADCODE, 2024-Stealthy-Backdoor-Attack-for-Code-Models, 2024-Poison-Attack-and-Poison-Detection-on-Deep-Source-Code-Processing-Models}, we assume attackers can manipulate a portion of the training samples and embed triggers into the code snippets. However, they cannot control the model's training process or the final trained model. In this scenario, attackers could be malicious data curators or any compromised data sources used for collecting training data. For example, they might upload poisoned samples to GitHub~\cite{2008-GitHub}. 
For defenders\revise{ (including our \ours{})}, we assume that they are \revise{dealing} with a potentially poisoned dataset \revise{and preparing to implement pre-training defenses}. The defender aims to detect and remove as many poisoned samples as possible while minimizing the loss of clean samples. 
Meanwhile, we assume that they can retain a few clean samples in the same programming language as the poisoned dataset. These samples can be obtained in various ways, including but not limited to generation by state-of-the-art generative models~\cite{2023-Code-Llama} or sourced from authoritative open-source datasets~\cite{2021-CodeXGLUE}. Additionally, we assume that they do not have any knowledge about the specific details of code poisoning\delete{ attacks}, e.g., trigger type and poisoning rate.
