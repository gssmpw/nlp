\section{Motivation}
\label{sec:motivation}
In this section, we will reveal the limitations of the defenses CodeDetector and ONION, and discuss our insights on code naturalness, which motivate the design of our \ours{}.

As mentioned in Section~\ref{sec:related_work}, existing code poisoning detection methods (also known as pre-training backdoor defense~\cite{2024-Mitigating-Backdoor-Attack-by-Injecting-Proactive-Defensive-Backdoor}) mainly defend against code poisoning attacks by detecting and removing poisoned samples before model training. 
Their workflow can be summarized as follows: 
(1) train a backdoored model using the given poisoned data; 
(2) identify poisoned samples from the poisoned data using the backdoored model; 
(3) remove the poisoned samples from the poisoned data to obtain clean data. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/clone_detection_codedetector_dead_code.pdf}
    \vspace{-3mm}
    \caption{Performance of the backdoored \revise{CodeBERT} model on \delete{the clean clone detection dataset, the poisoned dataset with the complete trigger, and the poisoned dataset with each trigger token}\revise{clean, the complete trigger-poisoned, the single trigger token-poisoned clone detection datasets}. }
    \label{fig:impact_on_performance}
    \vspace{-6mm}
\end{figure}

To detect code poisoning, CodeDetector first leverages the integrated gradients technique~\cite{2017-Axiomatic-Attribution-for-Deep-Networks} to find all important tokens in the poisoned data and then select abnormal tokens that have a great negative effect on the performance of models as triggers. 
However, CodeDetector can \delete{be used to }detect code poisoning caused by simple triggers (e.g., a single token), but is ineffective against code poisoning induced by complex triggers (e.g., multiple tokens). 
For example, the attack~\cite{2022-Backdoors-in-Neural-Models-of-Source-Code} can produce complex grammar-based trigger, e.g., ``\texttt{if (Math.sin(0.52) == -28) throw new Exception("alert")}''. 
We reveal why CodeDetector is unable to detect this grammar-based trigger by analyzing the changes in model performance when injecting both the complete trigger and individual trigger tokens into a clean clone detection dataset~\cite{2014-BigCloneBench}. 
Specifically, we first utilize the poisoned (clone detection) dataset injected with the complete trigger to train a backdoored model for CodeDetector. 
Then, we produce multiple poisoned datasets by injecting each trigger token into the \delete{(clone detection) }clean \revise{(clone detection) }dataset. 
Afterward, we apply the backdoored model to test each poisoned dataset. 
Figure~\ref{fig:impact_on_performance} shows the performance of the backdoored model on the clean dataset (the first \textcolor{blue}{blue} bar), the poisoned dataset with each trigger token (all \textcolor{orange}{orange} bars), and the poisoned dataset with the complete trigger (the last invisible \textcolor{red}{red} bar).\delete{
It is observed that 
1) on the clean dataset\delete{ (i.e., without trigger injection)}, the backdoored model achieves an F1 score of 98.22\revise{\%};
2) when injecting the complete trigger, the backdoored model achieves an F1 score of 0\revise{\%};
3) when injecting only one token of the trigger, the F1 score of the backdoored model ranges between 97.19\revise{\%} and 98.36\revise{\%}.} 
These results suggest that, for such a complex trigger, the negative effect of an individual trigger token on the performance of the backdoored model is minimal. 
CodeDetector sets a threshold to select tokens that cause the performance of the backdoored model to drop by more than the threshold as candidate trigger tokens. 
In their paper, the threshold is set to 0.3. 
However, in this example, the token that causes the largest performance drop is \texttt{sin}, and the corresponding F1 score drops by only 0.01 compared to the F1 score on the clean dataset. 
We also attempt to adapt the threshold to multiple experimental task datasets, but CodeDetector still does not perform well against complex triggers (discussed in Section~\ref{sec:evaluation}).

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/clone_detection_onion_ppl.pdf}
    \vspace{-2mm}
    \caption{Perplexity score for each token in the code snippet \revise{calculated using the ONION.}}
    \label{fig:onion_ppl}
    \vspace{-4mm}
\end{figure}

ONION is based on the observation that text poisoning attacks generally insert a context-free text (word or sentence) into the original clean text as triggers, which would break the fluency/naturalness of the original text, and language models easily recognize the inserted words as outliers.
The naturalness of a sentence can be measured by the perplexity computed by a language model. 
Similarly, code poisoning attacks also typically choose rare tokens or non-executable dead code statements as triggers~\cite{2023-BADCODE}. 
Therefore, intuitively, we can transfer ONION to detect code poisoning. 
Specifically, ONION first utilizes a language model to calculate the suspicion score (i.e., perplexity) for each word in a sentence, which is defined as $\delta^{p}_{i}=p_0-p_i$, where $p_0$ and $p_i$ are the perplexities of the sentence and the sentence without $i$-th word, respectively. 
The larger $\delta^{p}_{i}$ is, the more likely $i$-th word is an outlier word. Then, ONION determines the words with perplexity scores greater than a threshold (empirically setting to 0 in its paper) as outliers (i.e., trigger words).
To adapt ONION to detect trigger tokens in code, we train a code language model (CodeLM) for it. 
Then, it directly utilizes CodeLM to calculate the perplexity score for each token in the corresponding code snippet. 
Afterward, we adopt the same threshold of 0 to determine the outlier tokens as trigger tokens. 
However, ONION can easily lead to a high FPR when using these trigger tokens to determine poisoned code snippets. 
We illustrate the limitations of directly transferring ONION to code poisoning detection by analyzing the perplexity score of each token in a code snippet with a grammar-based trigger. 
Figure~\ref{fig:onion_ppl} shows such an example where the grammar-based trigger is ``\texttt{if (exp(0.94) >= 11) print("exception");}''.
\delete{It can be observed}Observe that 
1) the perplexity changes (i.e., $\delta^{p}$) for certain normal tokens (\textcolor{blue}{blue} bars) are greater than 0, e.g., ``static'' and ``uint8\_t'';
2) the perplexity changes for trigger tokens (\textcolor{red}{red} bars) are all below 0.
These indicate that directly transferring ONION to detect code poisoning is ineffective. 
The performance of ONION in more code poisoning scenarios is discussed in Section~\ref{sec:evaluation}.

\begin{figure}[!t]
    \centering
    \begin{minipage}[t]{0.47\linewidth}
        \centering
        \includegraphics[width=0.85\linewidth]{figures/defect_detection_naturalness_clean_token_count.pdf}
        \vspace{-2mm}
        \caption{Effect of the single-token trigger on code naturalness \revise{with $n$-gram language model on the Devign dataset.}}
        \label{fig:token_trigger_code_naturalness}
    \end{minipage}
    \hspace{2mm}
    \begin{minipage}[t]{0.47\linewidth}
        \centering
        \includegraphics[width=0.85\linewidth]{figures/defect_detection_naturalness_clean_dead_code_count.pdf}
        \vspace{-2mm}
        \caption{Effect of the multi-token trigger on code naturalness \revise{with $n$-gram model on the Devign dataset.}}
    \label{fig:dead_code_trigger_code_naturalness}
    \end{minipage}
    \vspace{-3mm}
\end{figure}

\input{tables/dead_code_difference}

Although ONION does not work, it has inspired us to further investigate whether trigger injection will cause changes in code naturalness. 
To this end, we first train a clean CodeLM \revise{($n$-gram language model)} on a small number of clean code snippets from Devign~\cite{2019-Devign}. 
Then, we inject two types of common triggers, a token trigger \texttt{rb} from the attack~\cite{2023-BADCODE} and a dead code trigger ``\texttt{if (rand() < 0) print("fail");}'' from the attack~\cite{2022-Backdoors-in-Neural-Models-of-Source-Code}) into these clean code snippets to produce two sets of poisoned code snippets.
Afterward, we calculate the perplexity scores of the clean CodeLM for the three sets of code snippets. 
The results are shown in Figure~\ref{fig:token_trigger_code_naturalness} and Figure~\ref{fig:dead_code_trigger_code_naturalness}, which illustrate the discrepancy in overall perplexity scores for the poisoned code snippets with the token trigger and the poisoned code snippets with the dead code trigger, compared to the clean code snippets, respectively. 
Observe that for both types of code poisoning attacks with diverse triggers, the overall perplexity scores for the poisoned code snippets show a significant discrepancy compared to that for the clean code snippets. 
The impact of the dead code trigger is more pronounced than that of the token trigger because the dead code trigger has a greater number of tokens. 
\revise{Considering that clean code snippets may also contain dead code, such as the dead code shown in Figure~\ref{fig:dead_code_sample}, which serves as an informational print but is unreachable, we further investigate whether clean code snippets with dead code and dead code-poisoned code snippets are distinguishable by naturalness. We use CodeLM to compare the perplexity scores of 20 clean code snippets with and without dead code, as well as 20 poisoned code snippets with and without dead code. The results are presented in Table~\ref{tab:dead_code_difference}. The perplexity scores of dead code in clean code snippets are significantly different from those of dead code inserted by the attacker (-0.267 vs. 0.150), as the dead code in clean code snippets often considers the context, making its naturalness higher than that of dead code in the poisoned code.}

\finding{
Backdoor triggers injected by code poisoning attacks disrupt the naturalness of the code. Multi-token triggers (e.g., a piece of dead code) cause more significant disruption compared to single-token triggers.
}

\noindent\textbf{Our solution.}
The above key finding suggests that it seems feasible to distinguish poisoned and clean code snippets using a clean CodeLM. Of course, this is also quite challenging, as Figure~\ref{fig:token_trigger_code_naturalness} and Figure~\ref{fig:dead_code_trigger_code_naturalness} show that whether it is a code poisoning attack based on a single-token trigger or a multi-token trigger, it is difficult to find a threshold that effectively separates poisoned code snippets from clean code snippets based on the perplexity scores of the CodeLM.
Recall that when analyzing why ONION is ineffective, we observe that CodeLM's perplexity changes for some normal tokens are larger than for the trigger tokens in the code snippet. 
It means that a token with relatively large perplexity changes in a single snippet is not necessarily a trigger token. 
Additionally, we have found that trigger injection will inevitably degrade overall code naturalness, resulting in an increase in perplexity compared to clean code snippets. 
Specifically, in Figure~\ref{fig:token_trigger_code_naturalness} and Figure~\ref{fig:dead_code_trigger_code_naturalness}, the \textcolor{red}{red} bars representing the perplexity scores of the poisoned code snippets are shifted to the right as a whole compared to the \textcolor{blue}{blue} bars representing the perplexity scores of the clean code snippets. 
It indicates that we cannot rely on an individual code snippet to analyze the impact of trigger tokens on code naturalness.  
Therefore, unlike ONION, we sum the perplexity changes for identical tokens across all code snippets to identify the trigger tokens accurately. 
Figure~\ref{fig:trigger_ppl} shows an example, where the left two \textcolor{orange}{orange} bars display the perplexity changes for the trigger token \texttt{rb} and the clean token \texttt{hex} in a single sample and the right two \textcolor{red}{red} bars present the cumulative perplexity changes for the two tokens across all code snippets.
Observe that in a single code snippet, the perplexity changes for \texttt{hex} is higher than that of \texttt{rb}, while the cumulative perplexity changes across all code snippets show a clear opposite result. 
Therefore, our method can accurately detect code poisoning. 

\input{figures/combine_figure_5_and_table_1}


