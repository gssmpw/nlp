\section{Methodology}
\label{sec:methodology}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/Overview_of_our_approach.pdf}
    \vspace{-2mm}
    \caption{Overview of \ours{}}
    \label{fig:overview}
    \vspace{-4mm}
\end{figure*}


Figure~\ref{fig:overview} shows the overview of \ours{}. 
Given poisoned data, \ours{} utilizes a few clean samples to detect poisoned samples in the poisoned data. Specifically, it decomposes the detection process into \revise{three} phases: (a) code-oriented language model training, (b) naturalness-based candidate trigger identification, and (c) poisoned data purification.


\subsection{Code-oriented Language Model Training}
\label{subsec:code-oriented_language_model_training}
The fundamental idea behind using code naturalness violation to detect code poisoning is as follows: \textit{Train a CodeLM on a few clean code snippets. Such a model will \revise{show expected behavior when processing new code snippets with ``typical'' patterns, but will exhibit very ``perplexing'' when encountering new code snippets} with backdoor triggers (i.e., ``atypical'' code patterns).} 
Therefore, the first phase of our approach is to train such a CodeLM. 
As mentioned in Section~\ref{sec:introduction}, the previous work~\cite{2012-On-the-naturalness-of-software} has demonstrated that even a fairly simple statistical model can capture a surprising amount of regularity in ``natural'' software. 
In~\cite{2012-On-the-naturalness-of-software}, the authors validated the effectiveness of a simple $n$-gram language model in capturing code regularities (i.e., naturalness). Thus, a straightforward method to obtain a CodeLM is to follow~\cite{2012-On-the-naturalness-of-software} and train an $n$-gram language model on code data and use it as the CodeLM. 
Different from NL where the text is viewed as word sequences, to train the $n$-gram language model on code data, \ours{} first tokenizes the clean code snippets into code token sequences (\circled{1}). Then, \ours{} builds a CodeLM on the $n$-gram language model and trains it with the code token sequences so that it can capture the naturalness of token-level code patterns (\circled{2}). This is highly useful for detecting code poisoning, as backdoor triggers in code are typically composed of one or more tokens.
In~\cite{2012-On-the-naturalness-of-software}, the authors have demonstrated that the 4-gram language model has reached saturation in capturing code features. 
We also experiment with different $n$ values in our scenario and find the same results, discussed in Section~\ref{sec:evaluation}. Therefore, in this paper, we set $n$ to 4.
 
To obtain \revise{an} $n$-gram language model capable of distinguishing between clean and poisoned code snippets, we need to acquire a small set of clean code snippets for training purposes. As mentioned in Section~\ref{sec:threat_model}, these clean code snippets can be obtained through various means, including but not limited to sourcing from authoritative open-source datasets.
The clean code snippets obtained by \ours{} are sourced from common authoritative code intelligence benchmark repositories, CodeXGLUE~\cite{2021-CodeXGLUE}. 
% \delete{
Additionally, we validate the effectiveness of \ours{} on two cases where the clean code snippets and the poisoned dataset are distributed similarly and differently (details in Section~\ref{sec:evaluation}). 
% }

\input{tables/tokenizer_ppl}

In addition, as mentioned in Section~\ref{sec:introduction}, ONION~\cite{2021-ONION} \delete{reveals}\revise{finds} that the fluency/naturalness of an NL sentence can also be captured/measured by the perplexity computed by a language model. The language model used in~\cite{2021-ONION} is an off-the-shelf pre-trained language model GPT-2~\cite{2019-Language-Models-Unsupervised-Multitask-Learners}. This work inspires us to consider directly using off-the-shelf pre-trained code models as the CodeLM to capture code naturalness, such as CodeBERT~\cite{2020-CodeBERT} and CodeLlama~\cite{2023-Code-Llama}. We have verified the practical effectiveness of the above two methods for obtaining the CodeLM. 
Table~\ref{tab:compare_different_CodeLM} shows the performance of different CodeLMs on the defect detection dataset poisoned by the BadCode~\cite{2023-BADCODE}. 
\delete{It can be observed}Observe that the $n$-gram language model has sufficient performance in detecting code poisoning attacks while also having the lowest time consumption. 
\revise{This is because the training objective of the $n$-gram language model is more limited compared to CodeBERT and CodeLlama. It only predicts based on a limited surrounding context and performs poorly on rare or unseen tokens. 
Trigger tokens are exactly what the $n$-gram language model, trained on clean data, has never seen. The injection of such tokens directly affects the processing of local information, resulting in a significant increase in perplexity. 
Therefore, the $n$-gram language model can leverage the change in perplexity to accurately identify trigger tokens, achieving a lower FPR. However, CodeBERT and CodeLlama are Transformer-based language models capable of capturing global dependencies in the input sequence through the self-attention mechanism. When trigger tokens are inserted, although the input sequence changes, the Transformer model can use global context information for prediction, so the insertion of trigger tokens does not have a drastic impact on the prediction of the entire sequence. Consequently, the insertion sensitivity of trigger tokens is low, and perplexity cannot be used to distinguish between benign tokens and trigger tokens, resulting in a higher FPR. 
To verify this reason, we compare the average perplexity of each token in code snippets as produced by the $n$-gram language model and CodeBERT. The results are shown in Table~\ref{tab:tokenizer_ppl}. As we expected, the $n$-gram language model exhibited higher perplexity (0.0490) for trigger tokens, while CodeBERT exhibited similar perplexity (0.0009) for different tokens, including trigger tokens. Therefore, CodeBERT and CodeLlama have a higher FPR. 
Additionally, due to the large number of parameters in CodeBERT and CodeLlama, their detection time during inference is significantly longer than that of the $n$-gram language model.}
Therefore, we directly utilize the $n$-gram language model as the CodeLM of \ours{}.


\input{algorithms/naturalness-based_trigger_identifying}

\subsection{Naturalness-based Trigger Identifying}
\label{subsec:Naturalness_based_candidate_trigger_identifying}
Algorithm~\ref{alg:trigger_identifying} illustrates the implementation details of the trigger identification in \ours{}. 
In addition to the poisoned data ($X^p$) as shown in Figure~\ref{fig:overview}(b), \ours{} takes as input the CodeLM $f_{\theta}$ trained in phase (a) and the number of tokens selected as trigger tokens ($k$). To identify trigger tokens in ($X^p$), \ours{} first gets all code snippets $C$ from $X^p$ (line 1).
Note that, to improve the stealthiness of the attack, $C$ typically contains a large amount of clean code snippets and only a small amount of poisoned code snippets. 
Then, \ours{} tokenizes code snippets in $C$ to code token sequences $S$ using a common code tokenizer provided by Code Llama~\cite{2023-Code-Llama} (line 2).
\revise{We discuss the impact of the code tokenizer selection on \ours{} in Section~\ref{subsec:evaluation_results}.}
Then, \ours{} initializes a list to store candidate trigger tokens $\mathcal{T}$ and corresponding naturalness (i.e., cross-entropy) changes $\Delta$ they cause (line 3). 
Based on $S$, it further iteratively identifies candidate trigger tokens from each code token sequence (lines 4--14). 
During each iteration, given a code token sequence $s \in S$, \ours{} first computes the cross-entropy of $f_{\theta}$ on $s$, denoted as $e$ (line 5).
Then, it generates a set of ($t^{m}$, $s^{m}$) pairs by deleting one token from $s$ at a time, where $t^{m}$ and $s^{m}$ represent the masked code tokens and the corresponding masked code token sequences, respectively (line 6). 
Afterwards, for each element ($t^{m}_{i}$, $s^{m}_{i}$) in ($t^{m}$, $s^{m}$), \ours{} computes the cross-entropy of $f_\theta$ on $s^{m}_{i}$, denoted as $e^{m}_i$ (line 8). 
Based on $e^{m}_i$ and $e$, \ours{} can check the influence of the code token $t^{m}_{i}$ on the code naturalness (lines 9--10). 
If $e^{m}_i < e$, it indicates that removing the token $t^{m}_{i}$ from $s$ has reduced $f_{\theta}$'s perplexity for $s$. Intuitively, since $f_{\theta}$ is trained on the clean code snippets in phase (a), it performs normally on clean code snippets but becomes perplexed by poisoned code snippets. Therefore, a decrease in model perplexity suggests that removing $t^{m}_{i}$ has made the code snippet more natural, and it also implies that $t^{m}_{i}$ is likely a trigger token. 
Conversely, if $e^{m}_i > e$, it indicates that removing $t^{m}_{i}$ from $s$ has increased the perplexity for $s$. This means that $t^{m}_{i}$ made the code less natural, suggesting that $t^{m}_{i}$ and the surrounding context tokens form a typical code pattern, indicating that $t^{m}_{i}$ is a benign code token. 
Therefore, for the token reducing the perplexity of $f_{\theta}$, \ours{} further computes the specific degree of perplexity reduction they cause, denoted as $\delta^{e}$ (line 10). 
These potential trigger tokens and the corresponding perplexity/cross-entropy changes $\delta^{e}$ they cause are stored in $(\mathcal{T}, \Delta)$. 
After traversing all code token sequences in $S$, \ours{} merges the elements in $(\mathcal{T}, \Delta)$ by summing the cross-entropy change values for identical tokens (line 15). 
Subsequently, it sorts the elements in $(\mathcal{T}, \Delta)$ in descending order based $\Delta$ and selects the tokens in the top $k$ elements as trigger tokens $\mathcal{T}$ (line 16), 
Finally, it outputs $\mathcal{T}$ and the algorithm finishes (line 17).

\subsection{Poisoned Data Purification}
\label{subsec:poisoned_data_purification}
Once trigger tokens are identified, an intuitive method for purifying poisoned data is to remove them from the code snippets of all samples. 
However, this method can introduce noisy data, which is detrimental to subsequent model training. 
Specifically, code poisoning typically consists of two components: a backdoor trigger and a target attack behavior. 
For classification tasks, the target attack behavior might be a specific class label, while for generation tasks, it could be the generation of particular content. 
Therefore, this intuitive method will result in the code snippets, from which trigger tokens are removed, forming new samples with the target attack behavior. 
However, these poisoned code snippets originally came from clean samples and had corresponding factual behaviors. 
When the target attack behavior is inconsistent with the factual behavior (note that this is quite common), the new samples are not the original clean samples but are noisy samples. 
Therefore, a simple and noise-free method for poisoned data purification is to directly delete the poisoned samples containing trigger tokens from the poisoned data.
