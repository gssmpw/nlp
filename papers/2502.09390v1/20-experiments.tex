\input{tables/main-results}

\section{Experiments}
\label{sec:experiments}

In this section, we detail the experimental setup and the evaluations conducted to assess the effectiveness of the \rephrase{} technique across various datasets and models. Our approach is compared to several existing methods to ascertain its relative performance.


\subsection{Datasets}

We evaluate our models on \mbox{\textbf{TriviaQA}} \cite{joshi-etal-2017-triviaqa}, \textbf{HotpotQA} \cite{yang-etal-2018-hotpotqa}, and \textbf{ASQA} \cite{stelmakh-etal-2022-asqa} which are knowledge intensive question-answering datasets which benefit from external context. Context retrieval was done over a Wikipedia corpus \cite{zhangRetrieveAnythingAugment2023}. We randomly sampled 200 examples from each dataset.  Results are reported using the following metrics: for TriviaQA and HotpotQA sub-string exact match (subEM) is reported \cite{asaiSelfRAGLearningRetrieve2023,yenHELMETHowEvaluate2024}. For ASQA, recall-EM is reported \cite{gaoEnablingLargeLanguage2023a}. For more details, see \Cref{sec:datasets}.

\subsection{Models}
\label{sec:models}

Our experiments utilize two open-source Llama models \cite{grattafioriLlama3Herd2024}: Llama-3.2 3B and Llama-3.1 8B. Both models are instruction-tuned to optimize their performance on complex tasks. In addition, we employed the OpenAI GPT-4o system\footnote{Version \textit{2024-05-13}.} \cite{openaiGPT4oSystemCard2024} to provide a benchmark for comparison. We use greedy decoding with local models.

\subsection{Configurations}
\label{sec:configs}

Our experimental setup is composed of the following configuration settings:

\begin{itemize}[noitemsep,topsep=0.5em,parsep=0.4em,leftmargin=0.8em]
\item \textbf{Baseline}: Standard application without any augmentative techniques.
\item \textbf{CoT}: Methodology as outlined by \citet{weiChainofThoughtPromptingElicits2023} that leverages intermediate reasoning steps leading to a final answer; instruction described in \Cref{tab:cot_prompt}.
\item \textbf{RaR:} A rephrasing strategy that prompts for a rephrasing of the original request before answering it, as proposed by \citet{dengRephraseRespondLet2024}; instruction described in \Cref{tab:rar_prompt}.
\item \textbf{\rephrase}: This configuration employs our prompt and is run with a default $N\!=\!3$ question-answer pairs.
\end{itemize}

We augment the requests with a pair of query-answer examples (few-shot) to facilitate understanding and improve prediction formatting and accuracy. All prompts and few-shot examples are presented in \Cref{sec:prompts} for reproducibility.

\pagebreak

Notably, in configurations containing reasoning instructions, we employ a regular expression\footnote{Regex pattern: \texttt{.*answer(.*)}. It has a 99.2\% capture rate.} to extract the final answer. This extraction is crucial as it assists in mitigating incorrect answers when correct phrases appear throughout reasoning chains \textbf{but not in the final answer}. For an example of this phenomena, see \Cref{tab:bad-example}.
