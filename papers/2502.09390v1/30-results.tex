
\section{Results}
\label{sec:results}

Table~\ref{tab:main} presents the main results of our method compared against several baselines on three benchmark QA datasets: TriviaQA, HotpotQA, and ASQA. Across the smaller Llama 3.2 3B and Llama 3.1 8B models, our approach consistently outperforms or matches the strongest baselines in each dataset. For example, with Llama 3.2 3B on \mbox{TriviaQA}, \rephrase{} improves performance by 6.5\% and 2.5\% over RAG and RaR, respectively, achieving an overall score of 88.5\%. On HotpotQA, Llama 3.2 3B also sees a notable boost, from 26.5\% (CoT) to 31.5\% with our method. These gains become even more pronounced with Llama 3.1 8B, where improvements of up to 3\% (TriviaQA) and 7\% (HotpotQA) are observed compared to alternative methods. We also observe notable gains on ASQA. For Llama-3.2 3B, \rephrase{} lifts performance from 21.5\% (RAG) and 23.5\% (RaR) to 26.6\%, nearly doubling the baseline of 14.2\%. 

When using GPT-4o, \rephrase{} remains highly competitive. On TriviaQA, our method reaches 96.7\%, outperforming other settings by at least 2.0\%. On HotpotQA, RaR and \rephrase{} are close, with RaR exhibiting a slight edge (47.3\% versus 46.7\%). For ASQA, CoT and \rephrase{} yield nearly identical performance (31.9\% versus 31.7\%), indicating that GPT-4o is already adept at leveraging additional reasoning steps or retrieved facts in these tasks. Nevertheless, \rephrase{} demonstrates robust performance across all three datasets and is especially beneficial for smaller-scale models, where sequential questioning can substantially bolster the final answer quality.

\input{tables/variations}

\subsection{Ablation Study}
\label{sec:ablation-study}

To highlight the contribution of each component in \rephrase, we performed an ablation study analyzing (1) the number of generated questions ($N$), (2) the role of few-shot examples, and (3) an optional aggregation step.

\paragraph{Number of Generated Questions:}
We conducted an evaluation using $N\in\{3,5,10\}$. As shown in Table~\ref{tab:variations}, for TriviaQA, increasing $N$ from 3 to 5 or 10 boosts performance from 92.5\% to 94.0\%. On HotpotQA, $N\!=\!5$ (31.5\%) dips slightly below $N\!=\!3$, but returns to 33.5\% at $N\!=\!10$. In ASQA, performance drops from 28.8\% at $N\!=\!3$ to 27.8\% at $N\!=\!10$, suggesting that while additional questions can add useful context, they can also introduce redundancy or noise. For more comparisons, see \Cref{tab:more_results}.

\input{figures/fewshot}

% \pagebreak

\paragraph{Impact of Few-Shot Examples:}
We inspected how incorporating few-shot examples substantially boosts accuracy, as seen in Figure~\ref{fig:fewshot}. We observe that both CoT and \rephrase{} benefit strongly from these examples, indicating that better exposure to task-relevant scenarios helps the model generate answers with correct and properly formed final answers. Interestingly, zero-shot experiments exhibit lower regex capture rate (85.0\%, see \Cref{sec:configs}) which could play a role in the diminished performance. For full results, see \Cref{tab:more_results}.

\paragraph{Aggregation Methods:}
Finally, we explored two aggregation strategies, before producing the final answer: \textit{Summarize} and \textit{Vote}. The \textit{Summarize} method involves the model summarizing the information learned from the generated questions and answers, whereas the \textit{Vote} method relies on majority voting to determine the final answer. According to \Cref{tab:variations}, \textit{Summarize} generally outperforms \textit{Vote} on TriviaQA and HotpotQA. However, using no aggregation step outperforms both in nearly all instances, suggesting that further post-processing can sometimes hurt the quality of the answer.
