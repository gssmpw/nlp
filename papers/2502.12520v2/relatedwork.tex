\section{Related Work}
\subsection{MU for LLMs}
The task of unlearning in LLMs has attracted significant attention in recent years \citep{barez2025openproblemsmachineunlearning}. In previous studies, MU methods are typically divided into training-based methods and training-free methods. Training-based methods include gradient ascent \citep{bourtoule2020machineunlearning}, gradient difference \citep{wang-etal-2023-kga,yao2023large}, KL divergence \citep{yao-etal-2024-machine}, and preference optimization \citep{maini2024tofutaskfictitiousunlearning} and so on. Training-free methods include in-context unlearning \citep{icunlearn} and corrupting prompt embeddings to achieve unlearning \citep{EmbeddingCorrupted}. As MU methods for LLMs continue to evolve, constructing high-quality unlearning datasets and benchmarks has become increasingly important. \citet{HarryPotter} propose a “Harry Potter” task for copyright, \citet{maini2024tofutaskfictitiousunlearning} design an unlearning task with fictional authors, and \citet{ma2024benchmarkingvisionlanguagemodel} introduce an unlearning benchmark for a fictional facial identity VQA dataset which aims to protect privacy. However, \textit{existing studies have not explored the application of MLLMs for forgetting harmful knowledge, a safety concern in MLLMs}.

\subsection{Safety in MLLMs}

With the rapid development of MLLMs \citep{li2025benchmark,yan2024survey,yan2025position}, their potential security issues, such as hallucination \citep{reefknot,zhou2024mitigating}, explainability \citep{huo2024mmneuron,huang2024miner}, and even toxic content \citep{toxic}, have gained widespread attention. For example, \citet{liu2025mm} propose MMsafetybench, a VQA dataset covering 13 harmful scenarios to assess MLLMs security. Ch3ef \citep{shi2024assessment} introduce the "Helpfulness, Honesty, and Harmlessness" (3H) as security evaluation criteria. \citet{hu2024vlsbench} identify information leakage issues in existing datasets and proposed VLSBench, improving evaluation accuracy by better aligning image and text modalities. Beyond dataset-based evaluation, attack methods have also been widely used to assess MLLMs security. MLLMs attacks are categorized into white-box and black-box methods \citep{yi2024jailbreak}. White-box attacks optimize using gradient information, such as dynamic suffixes \citep{zou2023universal} or adversarial image perturbations  \citep{shayegani2024jailbreak}. Black-box attacks typically employ methods like scenario-based hypotheses \citep{li2023deepinception,ding2023wolf}, context injection \citep{wei2023jailbreak}, or inserting malicious code \citep{kang2024exploiting}.