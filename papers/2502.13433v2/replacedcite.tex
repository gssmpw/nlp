\section{Related work}
\textbf{Audio Language Models.} \ 
In the field of audio-language models, CLAP models ____ have demonstrated  remarkable capabilities  in audio discriminative tasks. However, the lack of a decoder limits their applicability  in open-ended QA scenarios. Recently, with the rapid advancements in LLMs, researchers have started integrating audio understanding into LLMs. For example, Pengi   ____ combines the CLAP audio encoder with GPT2  ____, and employs a transformer-based mapper for multimodal fusion, achieving  strong performance on close-ended tasks. Similarly, LTU ____ incorporates a more advanced LLM, LLaMA ____, and demonstrates emerging audio comprehension and reasoning  abilities. SALMONN ____ utilizes a dual audio encoder consisting of a Whisper speech encoder model ____ and a BEATs ____ audio encoder to handle speech and non-speech audio tasks effectively. Other studies have expanded instruction fine-tuning data and explored more advanced audio encoder architecture, such as GAMA ____, Qwen2-Audio ____, yielding commendable performance. 

However, these LALMs heavily rely on large-scale audio-language pairs for training, which pose significant challenges in term of data collection and training costs. 
Differently, we adopt a highly cost-efficient approach for training LALMs using text-only data, which substantially reduces the data collection and training overhead while maintaining comparable performance.

\textbf{Text-only Supervised Multimodal LLMs.} \
To alleviate resource demands, researchers have proposed zero-shot captioning frameworks, aimed at generating image/audio captions through text-only training. In the visual domain, large-scale pre-trained contrastive models like CLIP ____ align images and language into a shared vision-language embedding space. Building on CLIP, 
CapDec ____ trains a decoder to reconstruct text from its corresponding CLIP language embedding,  which is then used to decode CLIP image embeddings at inference. To mitigate the vision-language modality gap, CapDec injects noise into language embedding during training.  Differently, DeCap ____ leverages a memory to store CLIP language embeddings, which is  subsequently used to project visual embedding into CLIP language embedding space at inference.  In the audio domain, models such as NoAudioCaptioning ____, WSAC ____, PromptACC ____, and DRCap ____ adopt similar strategies, replacing CLIP with CLAP to develop zero-shot audio captioning frameworks. %For example, NoAudioCaptioning and PromptACC apply noise injection during training, while WSAC and DRCap leverage language embedding memory during inference.
However, these works focus on audio captioning, lacking the flexibility to handle a diverse range of audio tasks simultaneously, and they are limited to processing only a single type of audio (sound).