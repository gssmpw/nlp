%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article} 
\usepackage{xspace}    
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}[colorlinks=true, linkcolor=red]

% For Table
\usepackage{multirow}
\usepackage{float}
\usepackage{tablefootnote}
\usepackage[table]{xcolor}
\usepackage{array}
\usepackage{placeins}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{MATS: An Audio Language Model under Text-only Supervision}

\begin{document}

\twocolumn[
    \def\logo{\raisebox{-0.5ex}{{\includegraphics[width=0.05\textwidth]{./imgs/mat.png}}{\mathstrut}}}
    \newcommand{\titledmodelname}{\logo\xspace MATS}
    \icmltitle{\titledmodelname: An Audio Language Model under Text-only Supervision}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
    \icmlsetsymbol{equal}{*}
    \icmlsetsymbol{corresponding}{$\dagger$}
    
    \begin{icmlauthorlist}
    \icmlauthor{Wen Wang}{1}
    \icmlauthor{Ruibing Hou}{1,corresponding}
    \icmlauthor{Hong Chang}{1,2}
    \icmlauthor{Shiguang Shan}{1,2}
    \icmlauthor{Xilin Chen}{1,2}
    \end{icmlauthorlist}
    
    \icmlaffiliation{1}{Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, China}
    \icmlaffiliation{2}{University of Chinese Academy of Sciences, China}

    \icmlcorrespondingauthor{Ruibing Hou}{houruibing@ict.ac.cn}
    % \icmlaffiliation{3}{School of ZZZ, Institute of WWW, Location, Country}
    
    % You may provide any keywords that you
    % find helpful for describing your paper; these are used to populate
    % the "keywords" metadata in the PDF but will not be shown in the document
    \icmlkeywords{Machine Learning, ICML}
    \vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Large audio-language models (LALMs), built upon powerful Large Language Models (LLMs), have exhibited remarkable audio comprehension and reasoning capabilities. 
However, the training of LALMs demands a large corpus of audio-language pairs, which requires substantial costs in both data collection and training resources. In this paper, we propose \textbf{MATS}, an audio-language multimodal LLM designed to handle \textbf{M}ultiple \textbf{A}udio task using solely \textbf{T}ext-only \textbf{S}upervision. By leveraging pre-trained audio-language alignment models such as CLAP, we develop a text-only training strategy that projects the shared  audio-language latent space into LLM latent space, endowing the LLM with audio comprehension capabilities without relying on audio data during training. To further bridge the modality gap between audio and language embeddings within CLAP, we propose the \textbf{S}trongly-rel\textbf{a}ted \textbf{n}oisy \textbf{t}ext with \textbf{a}udio (\textbf{Santa}) mechanism. Santa maps audio embeddings into CLAP language embedding space while preserving essential information from the audio input. Extensive experiments demonstrate that MATS, despite being trained exclusively on text data, achieves competitive performance compared to recent LALMs trained on large-scale audio-language pairs.

% With the advancement of Large Language Models (LLMs), a variety of large audio-language models (LALMs) have emerged, combining the power of LLMs to equip models with advanced reasoning capability for understanding and interpreting audio data. However, training these models necessitates a substantial amount of audio-language question-answer pairs (QAs). Collecting such data, however, presents a significant challenge, as it requires careful listening to the audio and the formulation of relevant questions and answers based on its content. To address this issue, we propose a novel strategy for training LALM called \textbf{MATS}, an audio-language multimodal LLM designed to address \textbf{M}ultiple \textbf{A}udio tasks, encompassing both close-ended and open-ended scenarios, using solely \textbf{T}ext-only \textbf{S}upervision. To further mitigate the performance degradation caused by the audio-language modality gap after contrastive learning, we propose a novel method, \textbf{S}trongly-rel\textbf{a}ted \textbf{n}oisy \textbf{t}ext with \textbf{a}udio (\textbf{Santa}), which combines audio information with strongly-related noisy text information. And we evaluated \textbf{MATS} on both close-ended and open-ended tasks.Under text-only supervised settings, \textbf{MATS} set new state-of-the-art (SOTA) benchmarks on several close-ended datasets and achieved performance on certain open-ended datasets that is comparable to that of some audio-supervised models.
\end{abstract}



\section{Introduction}

Recent advancements in multimodal models for audio processing have progressed rapidly, driven by the recognition of audio as a crucial role in understanding the physical world. Audio, encompassing sound, music, and other auditory elements, plays a crucial role in enabling intelligent agents to assist humans and navigate the complexities of the real world. Consequently, developing models capable of effectively interpreting these audio modalities is vital for advancing human-machine interaction and fostering a deeper understanding of our surroundings. 

In the field of audio-language multimodal learning, contrastive learning models, such as Contrastive Language-Audio Pretraining (CLAP) \citep{elizalde2023clap}, have demonstrated remarkable zero-shot abilities across various audio tasks. However, these models primarily specialize in audio discriminative tasks, lacking a decoder to support open-ended audio question-answering (QA) capabilities.  To overcome this limitation, recent studies  have integrated LLMs as the text generator within a multimodal framework, capitalizing on LLMs' inherent instruction following and generating capabilities.  A pioneering effort, Pengi \citep{deshmukh2024pengiaudiolanguagemodel}, integrates CLAP audio encoder and GPT2 \citep{radford2019language}, employing a transformer projection module to align audio embedding with LLMs' text embedding space. Building on this foundation, recent models \citep{gong2024listenthinkunderstand,tang2024salmonngenerichearingabilities,kong2024audioflamingonovelaudio,ghosh2024gamalargeaudiolanguagemodel} utilize advanced audio encoder and more powerful LLMs, constructing advanced LALMs to address increasingly complex audio relevant tasks. 

\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{./imgs/radar_metric.pdf}}
\vskip -0.3in
\caption{Performance of MATS compared to the previous SOTA \citep{chu2023qwenaudioadvancinguniversalaudio, kong2024audioflamingonovelaudio, deshmukh2024pengiaudiolanguagemodel} under zero-shot setting for close-ended audio tasks.} %\textbf{Note:} Except for AudioCaps, Clotho, and MusicCaps, where the metrics are CIDEr, CIDEr, and ROUGH-L, the metrics for the other test sets are Accuracy.}
\label{fig:radar}
\end{center}
\vskip -0.4in
\end{figure}

However, training a large audio-language model  that effectively bridges the entirely decoupled audio embedding space and LLM text embedding space is challenging.  Previous models \citep{gong2024listenthinkunderstand,tang2024salmonngenerichearingabilities,kong2024audioflamingonovelaudio,ghosh2024gamalargeaudiolanguagemodel} rely heavily on extensive audio-language QA pairs for training. This requires annotators to carefully listen to each audio and then construct corresponding questions and answers, a labor-intensive and time-consuming process. As a result, the cost of data collection becomes prohibitively high, leading to significant resource demands for training a LALM. 

To reduce resource demands, researchers have proposed text-only audio captioning frameworks \citep{deshmukh2024training,kouzelis2023weaklysupervisedautomatedaudiocaptioning,zhang2024zeroshotaudiocaptioningusing,li2024drcapdecodingclaplatents} which generate audio captions without relying on costly audio-language pairs for training. 
These frameworks leverage %contrastive learning pre-trained models like 
CLAP, which have established a well-aligned audio-language embedding space, enabling LLMs to interpret  audio embedding by linking CLAP language embedding space with LLM semantic space. However, these methods have certain limitations. First, they are narrowly tailored to specific audio type (sound) and constrained to single audio captioning task, limiting their applicability to boarder audio open-ended question-answering scenarios.  Second, to bridge the audio-language modality gap within CLAP, these methods either rely solely on noise injection during training or exclusively utilize a memory bank during inference. However, using only noise injection is highly random and sensitive, while relying solely on a memory bank during inference not only sacrifices audio embedding information but also increases inference time, ultimately compromising overall effectiveness.
%these methods typically inject Gaussian noise into CLAP's text embedding during training; however, only using noise injection is highly random and sensitive, which undermines its effectiveness. 

In this paper, we propose \textbf{MATS}, an audio-language multimodal LLM designed to address \textbf{M}ultiple \textbf{A}udio tasks, encompassing both close-ended and open-ended scenarios,  using solely \textbf{T}ext-only \textbf{S}upervision. MATS builds upon recent advancements in LALMs  \citep{deshmukh2024pengiaudiolanguagemodel,tang2024salmonngenerichearingabilities}, incorporating two key enhancements. 
% \textbf{First}, 
% we derive the generalization error bound for  text-only supervised multimodal models, revealing that the inherent modality gap  in contrastive audio-language representation learning can amplify the generation error bound.
% \textbf{Second},
% recognizing that the joint audio-language embedding space of a pre-trained CLAP is shared across both modalities, the language embeddings extracted by CLAP language encoder can serve as effective representations of audio semantics. To further bridge the modality gap within CLAP, we propose novel modal-transfer that integrates noise injection during training phase and employs \textbf{S}trongly-rel\textbf{a}ted \textbf{n}oisy \textbf{t}ext with \textbf{a}udio (\textbf{Santa}) during inference. And the Santa mechanism employs a k-means-based memory alongside a balancing strategy. During training, MATS integrates CLAP language encoder, noise injection, a Mapper and a LLM, where the CLAP language encoder extracts cross-modality embeddings of audio descriptions within the shared language-audio embedding space. During inference, the noise injection and CLAP language encoder are replaced with Santa and the CLAP audio encoder, allowing the LLM to generate responses from the framework. This design effectively balances audio embeddings and semantically strongly related augmented language embeddings, effectively mitigating the  modality gap and enhancing MATS's generalization ability.
\textbf{First}, 
recognizing that the joint audio-language embedding space of a pre-trained CLAP is shared across both modalities, the language embeddings extracted by CLAP language encoder can serve as effective representations of audio semantics. During training, MATS integrates CLAP language encoder, a Mapper and a LLM, where the CLAP language encoder extracts cross-modality embeddings of audio descriptions within the shared language-audio embedding space. During inference, the LLM can generate responses based on audio embedding derived from input audios, processed through CLAP audio encoder and the Mapper. 
\textbf{Second}, we derive the generalization error bound for  text-only supervised multimodal models, revealing that the inherent modality gap in contrastive audio-language representation learning can amplify the generalization error bound. To this end, we propose a modal-transfer method that integrates noise injection during training and employs \textbf{S}trongly-rel\textbf{a}ted \textbf{n}oisy \textbf{t}ext with \textbf{a}udio (\textbf{Santa}) during inference. And the Santa mechanism employs a k-means-based memory alongside a balancing strategy. This design effectively balances audio embeddings and semantically strongly related augmented language embeddings, effectively mitigating the modality gap and enhancing MATS's generalization ability.

We conduct a comprehensive  evaluation of our model on both close-ended and open-ended tasks. Remarkably, despite being trained on text-only data, MATS demonstrates performance comparable to, and oven surpassing, some LALMs trained on extensive audio-language pairs.  For close-ended tasks, MATS demonstrates significant zero-shot performance improvements over previous state-of-the-arts (SOTAs) \citep{deshmukh2024pengiaudiolanguagemodel, tang2024salmonngenerichearingabilities, doh2023lpmusiccapsllmbasedpseudomusic}, achieving gains of \textbf{12\%}, \textbf{7\%}, and \textbf{9.9\%} on DCASE \citep{mesaros:hal-01627981}, VGG \citep{chen2020vggsoundlargescaleaudiovisualdataset}, and MusicCaps \citep{agostinelli2023musiclmgeneratingmusictext} benchmarks, respectively, as shown \cref{fig:radar}.  For open-ended tasks, MATS (7B) achieves the second-best performance on MMAU benchmark \citep{sakshi2024mmaumassivemultitaskaudio}, surpassing  SALMONN (13B) \citep{tang2024salmonngenerichearingabilities} by \textbf{11.7\%} with a significantly smaller model size.
%Also, when compared to the upper-bound performance under audio supervision, it is evident that our model, trained under text-only supervision, achieves performance nearly on par with its audio-supervised counterpart.

% Our contributions are summarized as follows.
% \begin{itemize}
% \item We propose \textbf{MATS}, a unified LALM, that generalizes over various types of audio, including sound, music. To the best of our knowledge, we are the first to introduce a text-supervised LALM.
% \item We provide a theoretical analysis that reducing the distributional gap between text and audio can reduce generalization errors on audio under text-supervised training.
% \item We explored different adaptors to further eliminate the modality gap and proposed a novel adaptor called \textbf{Santa}.
% \item Our method achieves comparable or even superior performance to audio-supervised LALMs on most close-endeded and open-ended tasks, establishing a new SOTA under zero-shot settings.
% \end{itemize}

\section{Related work}
\textbf{Audio Language Models.} \ 
In the field of audio-language models, CLAP models \citep{wu2023large, elizalde2023clap} have demonstrated  remarkable capabilities  in audio discriminative tasks. However, the lack of a decoder limits their applicability  in open-ended QA scenarios. Recently, with the rapid advancements in LLMs, researchers have started integrating audio understanding into LLMs. For example, Pengi   \citep{deshmukh2024pengiaudiolanguagemodel} combines the CLAP audio encoder with GPT2  \citep{radford2019language}, and employs a transformer-based mapper for multimodal fusion, achieving  strong performance on close-ended tasks. Similarly, LTU \citep{gong2024listenthinkunderstand} incorporates a more advanced LLM, LLaMA \citep{touvron2023llamaopenefficientfoundation}, and demonstrates emerging audio comprehension and reasoning  abilities. SALMONN \citep{tang2024salmonngenerichearingabilities} utilizes a dual audio encoder consisting of a Whisper speech encoder model \citep{radford2023robust} and a BEATs \citep{chen2022beatsaudiopretrainingacoustic} audio encoder to handle speech and non-speech audio tasks effectively. Other studies have expanded instruction fine-tuning data and explored more advanced audio encoder architecture, such as GAMA \citep{ghosh2024gamalargeaudiolanguagemodel}, Qwen2-Audio \citep{chu2024qwen2audiotechnicalreport}, yielding commendable performance. 

However, these LALMs heavily rely on large-scale audio-language pairs for training, which pose significant challenges in term of data collection and training costs. 
Differently, we adopt a highly cost-efficient approach for training LALMs using text-only data, which substantially reduces the data collection and training overhead while maintaining comparable performance.

\textbf{Text-only Supervised Multimodal LLMs.} \
To alleviate resource demands, researchers have proposed zero-shot captioning frameworks, aimed at generating image/audio captions through text-only training. In the visual domain, large-scale pre-trained contrastive models like CLIP \citep{radford2021learning} align images and language into a shared vision-language embedding space. Building on CLIP, 
CapDec \citep{DBLP:conf/emnlp/NukraiMG22} trains a decoder to reconstruct text from its corresponding CLIP language embedding,  which is then used to decode CLIP image embeddings at inference. To mitigate the vision-language modality gap, CapDec injects noise into language embedding during training.  Differently, DeCap \citep{li2023decapdecodingcliplatents} leverages a memory to store CLIP language embeddings, which is  subsequently used to project visual embedding into CLIP language embedding space at inference.  In the audio domain, models such as NoAudioCaptioning \citep{deshmukh2024training}, WSAC \citep{kouzelis2023weaklysupervisedautomatedaudiocaptioning}, PromptACC \citep{zhang2024zeroshotaudiocaptioningusing}, and DRCap \citep{li2024drcapdecodingclaplatents} adopt similar strategies, replacing CLIP with CLAP to develop zero-shot audio captioning frameworks. %For example, NoAudioCaptioning and PromptACC apply noise injection during training, while WSAC and DRCap leverage language embedding memory during inference.
However, these works focus on audio captioning, lacking the flexibility to handle a diverse range of audio tasks simultaneously, and they are limited to processing only a single type of audio (sound).  

\section{Methodology}
In this section, we introduce \textbf{MATS}, an audio-language multimodal LLM designed to tackle multiple audio tasks using text-only supervision. We begin by presenting the formulation of MATS in \cref{sec:formulations}, followed by an outline of its overall framework in \cref{sec:architecture}. \cref{sec:theory} provides the theoretical foundation, demonstrating that reducing the feature distribution gap between the CLAP's audio and language modalities can effectively reduce the generalization error of MATS.  \cref{sec:santa_mechaism} presents the proposed \textbf{Santa} mechanism to bridge the audio-language modality gap. Finally, the training pipeline is detailed in \cref{sec:train and test pipeline}.

\subsection{Formulations} \label{sec:formulations}
As shown in \cref{fig:architecture}, \textbf{MATS} consists of a language encoder $\mathcal{E}_T$ and an audio encoder $\mathcal{E}_A$ from CLAP \citep{elizalde2023clap}. During training, the language encoder $\mathcal{E}_T$ processes text data $T$, while during inference, the audio encoder $\mathcal{E}_A$ processes audio data $A$. To bridge the audio-language modality gap, the noise injection is used during training and the Santa mechanism $f_{\text{Santa}}$ is employed at inference. Then, a transformer-based mapper $f_{\text{map}}$ serves as the connection module, integrating its output sequence with the text instruction prompt, which is then fed into the LLM  to generate the text response. % Next, we introduce 

\begin{figure}[t]
%\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{./imgs/mats_architecture.pdf}}
\vskip -0.1in
\caption{The architecture of proposed MATS.}
\label{fig:architecture}
\end{center}
\vskip -0.3in
\end{figure}

\textbf{Train Phase.} \
During training, only text data is utilized. Formally, the input text $T$,  which provides the descriptions of audio contents, is encoded by  CLAP language encoder $\mathcal{E}_T$. To mitigate the audio-language modality gap, a zero-mean Gaussian noise is added to the extracted language embedding $\mathcal{E}_T\left(T\right)$.  The augmented language embedding is subsequently projected through the mapper $f_{\mathrm{map}}$ to generate embeddings within the LLM latent space. These embeddings are concatenated with the instruction embeddings $E_{\mathrm{Instr}}$ extracted by LLM's word embedding layer, forming the prefix $\boldsymbol{s_{\mathrm{train}}}$, which serves as the input to the LLM for autoregressive learning. The process is formulated as:
\begin{equation}
\boldsymbol{s_{\mathrm{train}}}=\mathrm{concat}\left[f_{\mathrm{map}}\left(\mathcal{E}_T\left(T\right)+\epsilon\right), E_{\text{Instr}}\right],
\end{equation}
where noise $\epsilon\sim \mathcal{N}(0, \sigma)$ with variance $\sigma$. Given the prefix $\boldsymbol{s_{\mathrm{train}}}$ and corresponding text response $R$, the LLM predicts the probability distribution of potential next token at each step, with the Cross-Entropy loss function used for optimization:
\begin{equation}
    \mathcal{L}=\sum\limits_i \mathrm{CE}\left(f_{\text{LLM}}\left(\boldsymbol{s_{\textbf{train}}}, R^{< i}\right), R^i\right),
    \label{eq2}
\end{equation}
where $\mathrm{CE}$ denotes the cross-entropy function.

\textbf{Inference Phase.} \
At inference, the input data includes both audio files and textual instructions. %The inference structure closely parallels the training stage, with 
The key structure difference is that the augmented language embedding $\mathcal{E}_T(T)+\epsilon$ is replaced by audio embedding $\mathcal{E}_A(A)$, encoded by the audio encoder of CLAP. Notably, the \textbf{Santa} mechanism is then applied to the audio embedding to mitigate the audio-language modality gap. Finally, the LLM predicts the responses $R_{\mathrm{pred}}$ in an autoregressive manner, as:
\begin{equation}
\begin{split}
\boldsymbol{s_{\mathrm{test}}} &= \mathrm{concat}\left[f_{\mathrm{map}}\left(f_{\mathrm{Santa}}\left(\mathcal{E}_A\left(A\right)\right)\right), E_{\text{Instr}}\right], \\
R_{\mathrm{pred}} &= f_{\mathrm{LLM}}\left(\boldsymbol{s_{\mathrm{test}}}\right).
\label{eq3}
\end{split}
\end{equation}


\subsection{Model Architecture} \label{sec:architecture}
The architecture of the MATS model is depicted in \cref{fig:architecture}.

\textbf{CLAP Encoder.} \
To obtain an aligned audio-language embedding space, we adpot CLAP \citep{elizalde2023clap}, which integrates the HTSAT \citep{chen2022htsathierarchicaltokensemanticaudio} audio encoder $\mathcal{E}_A$ and the GPT2 \citep{radford2019language} language encoder $\mathcal{E}_T$. CLAP is trained to connect language and audio by corresponding encoder and bring them into a joint multimodal space using contrastive learning. Both encoders are kept frozen throughout the entire process. Notably, during training, the language encoder $\mathcal{E}_T$ is utilized, whereas the audio encoder $\mathcal{E}_A$ is employed during inference.

\textbf{Modality Transfer.} \
To mitigate the audio-language modality gap within CLAP and enhance the model's generalization%, following \citep{deshmukh2024training}
, we add zero-mean Guassian noise to the language embedding during training as $\mathcal{E}_T\left(T\right) + \epsilon$, where $\epsilon \sim \mathcal{N}(0, \sigma)$. The variance $\sigma$ is determined by calculating the infinity norm between audio and language embeddings over a set of $30$ randomly selected samples following \citep{deshmukh2024training}.  
During inference, the augmented language embedding $\mathcal{E}_T\left(T\right)+\epsilon$ is replaced by the audio embedding $\mathcal{E}_A\left(A\right)$, and the \textbf{Santa} mechanism is applied to the audio embedding to further reduce the audio-language modality gap within CLAP, formulating as  $f_{\mathrm{Santa}}\left(\mathcal{E}_A\left(A\right)\right)$. The Santa mechanism is detailed in \cref{sec:santa_mechaism}. 

\textbf{Mapper Module.} \
The mapper module consists of a 8-layer Transformer \citep{vaswani2017attention} followed by a linear layer. Specifically, we introduce a fixed set of learnable query embeddings, which are concatenated with the embeddings produced by the CLAP encoder and then fed into the Transformer. Through the self-attention mechanism in transformer, these queries  interact with the frozen CLAP embeddings, enabling effective integration of their information. Finally, the output query embeddings are projected via a linear layer to align with the dimensionality of the LLM’s text embeddings.

\textbf{Large Language Model.} \
To generate textual responses, we utilize a pretrained autoregressive causal language model.  In this study, we evaluate two large language models: the smaller version employs GPT2 with 125M parameter, while the larger version leverages the LLaMA-7B model fine-tuned with Vicuna instruction-following capabilities.  These two LLMs are selected to balance computational efficiency with performance, enabling a comparative analysis of lightweight and advanced language generation capabilities.

\subsection{Theoretical Analysis on Generalization} \label{sec:theory}
In this subsection, we study the generalization error bound of the text-only supervised audio models. Formally, let $\mathcal{A}$ and $\mathcal{T}$ be input audio and corresponding textual descriptions space respectively, and $\mathcal{Y}$ be output one-hot class space, where the one-hot vector $y \in \mathbb{R}^{V}$ is used to represent the label and $V$ denotes the LLM's vocabulary size. Similarly,
$\mathcal{Z}^t$ stands for the embedding space induced from $\mathcal{T}$ by the CLAP language encoder, and $\mathcal{Z}^a$ stands for the embedding space induced from $\mathcal{A}$ by the CLAP audio encoder. Additionally, let $h$ denote the prediction function mapping from the CLAP embedding space to output space, \textit{i.e.}, $h: \mathcal{Z}^t/\mathcal{Z}^a  \rightarrow \mathcal{Y}$. 

To simplify the analysis, we focus on a specific audio event classification task and disregard the influence of instructions. In this case, we are given a training set consisting of text-label paired examples, $\mathcal{D}_{tr}=\left\{z^t_i, y_i\right\}_{i=1}^{N}$ where $\left(z^t_i, y_i\right) \sim p_{\mathcal{T}}\left(z^t, y\right)$. Our goal is to learn a target model $h: \mathcal{Z}^a  \rightarrow \mathcal{Y}$ by fitting on $\mathcal{D}_{tr}$, with the smallest generalization risk on the test distribution $p_{\mathcal{A}}\left(z^a, y\right)$ where  $\left(z^a, y\right)$ denotes audio-label pairs. 
 Formally,  the empirical error on training set and the generalization risk on test set are computed as: 
 \begin{equation}
 \begin{split}
 \widehat{R}_{tr}\left(h\right) &= \frac{1}{N} \sum_{i=1}^{N} \left|h\left(z^t_i\right) - y_i \right|, \\
  R_{te}\left(h\right) &= \mathbb{E}_{\left(z^a,y\right) \sim p_{\mathcal{A}}\left(z^a,y\right)} \left|h\left(z^a\right) - y \right|,
 \end{split}
 \end{equation}
 where $\left|\cdot\right|$ is $L_1$ norm. With these definitions, we can derive the generalization risk bound of the text-only supervised audio model, leading to following theorem.

\begin{theorem}
\label{the:thoery} Let $\mathcal{H}$ be a hypothesis space of Natarajan-dimension $d$. For classification with $V$ classes, let $\mathcal{D}_{tr}$ be text-only training set drawn from distribution $p_{\mathcal{T}}\left(z^t, y\right)$. As for test set with audio files draw from distribution $p_{\mathcal{A}}\left(z^a, y\right)$, assuming that the class distribution of training and test data is consistent, for any target classifier $h\in \mathcal{H}$, $\delta \in \left(0, 1\right)$, with probability at least $1-\delta$:
\begin{equation}
\begin{split}
R_{te}\left(h\right)  & \leq  \widehat{R}_{tr}\left(h\right) \\
& + \max\limits_v \left(\mathrm{disc}_{L_1}\left(p_{\mathcal{A}}\left(z^a | y\right), p_{\mathcal{T}}\left(z^t | y\right)\right) \right)\\
& + \sqrt{\frac{8}{N} \left( 2d \log{\sqrt{2N V}} + \log{\frac{2}{\delta}} \right)}.
\end{split}
\end{equation}
Here, $\mathrm{disc}_{L_1}$ denotes the Discrepancy Distance (Definition 4 in \citep{mansour2023domainadaptationlearningbounds}).
\end{theorem}

The proof is provided in \cref{appendix:proof}. Theorem \ref{the:thoery} reveals that the generalization risk can be bounded by the sum of three components: (1) \textit{Empirical Risk:} $\widehat{R}_{tr}\left(h\right)$, representing the empirical risk on training set $\mathcal{D}_{tr}$, which can be reduced during training. (2) \textit{Modality Discrepancy:}  The second term denotes the discrepancy between CLAP audio embeddings and language embeddings. (3)  \textit{Complexity Term:} The last term is only relevant with the choice of the hypothesis space $\mathcal{H}$ and magnitude of $N$. In general, this term vanishes when the training data is sufficiently large. These observations highlight a crucial factor in reducing the generalization error bound:  minimizing the audio-language modality gap within CLAP.

\subsection{Santa Mechanism} \label{sec:santa_mechaism}

\begin{figure}[t]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{./imgs/adaptor_visual2.pdf}}
\vskip -0.2in
\caption{t-SNE visualizations for various methods on 350 randomly selected language embeddings (blue) and paired audio embeddings (red) from training dataset of MATS-Audio in \cref{tab:train dataset audio}. a) CLAP b) CLAP with Gaussian Noise \citep{deshmukh2024training} c) CLAP with memory bank \citep{li2024drcapdecodingclaplatents} d) CLAP with Santa.}
% \caption{t-SNE \citep{JMLR:v9:vandermaaten08a} visualizations for various methods on 350 randomly selected text vectors (blue) and paired audio vectors (red) from training dataset. a) CLAP only, b) CLAP with Gaussian Noise \citep{deshmukh2024training}, c) CLAP with memory bank \citep{li2024drcapdecodingclaplatents} and d) CLAP with the Santa.}
\label{fig: modal_transfer_visual}
\end{center}
\vskip -0.3in
\end{figure} 

According to  theoretical analysis above, reducing \textit{Modality Discrepancy} is critical for improving MATS's generalization. However, CLAP suffers from an inherent modality gap: As shown in \cref{fig: modal_transfer_visual} $a)$, CLAP's audio and language embeddings occupy entirely separate regions. To address this issue,  \citep{deshmukh2024training} injects noise into CLAP's language embeddings during training. However, relying solely on noise injection is highly stochastic and sensitive, limiting its robustness. Another work  \citep{li2024drcapdecodingclaplatents} utilizes a memory bank that integrates similar language embeddings to represent audio embeddings  during inference.  However, this approach completely discard original audio embeddings, resulting in  a significant loss of audio information. 
In this work, we propose modality transfer, which combines noise injection during training with the \textbf{Santa} mechanism during inference. The \textbf{Santa} mechanism utilizes a k-means-based memory alongside a balancing strategy. This design integrates audio
embeddings with semantically relevant augmented language embeddings, more effectively bridging the modality gap, as shown in \cref{fig: modal_transfer_visual} $d)$. 

To obtain the CLAP language embedding space augmented with noise injection, we randomly select $M$ texts from the text-only training set, denoted as $\left\{T_i\right\}_{i=1}^M$, and construct a memory $\mathcal{M}=\left\{\mathbf{m_i}\right\}_{i=1}^M$, where $\mathbf{m_i}=\mathcal{E}_{T}\left(T_i\right) + \epsilon$. At inference, the goal is to generate an answer for a given audio $A$ and associated instruction. Specifically, given the audio embedding $z^a=\mathcal{E}_A\left(A\right)$, we obtain its representation in language embedding space by combing $z^a$ with a weighted sum of semantically relevant embeddings in memory $\mathcal{M}$. A straightforward strategy to obtain the weights of these language embeddings is to compute their similarity with the audio embedding. However, due to the limited representational power of individual language embedding, this strategy is prone to retrieving the texts with insufficient semantic relevance, thereby affecting the effectiveness of audio-language modality alignment.

To this end, we apply k-means clustering to $\mathcal{M}$ and obtain a clustered memory $\mathcal{S} = \left\{\mathcal{S}_k \right\}_{k=1}^K$, where  $\mathcal{S}_k$ stores the augmented language embeddings assigned to this cluster.  
By aggregating semantically similar augmented language embeddings, each cluster exhibits enhanced semantic representational capabilities.
Denote the cluster centers as $\left\{\mathbf{c_k}\right\}_{k=1}^K$, we identity the most relevant cluster $\mathcal{S}_{\mathrm{closest}}$ by calculating the distance between the audio embedding $z^a$ and the cluster centers:
\begin{equation}
\begin{split}
\mathrm{closest} = \arg\min_{k}\left\|z^a-\mathbf{c_k}\right\|_2^2.
\end{split}
\end{equation}
Next, we select $L$ augmented language embeddings from the cluster $\mathcal{S}_{\mathrm{closest}}$, denoting as $\left\{\mathbf{m^{\mathrm{closest}}_1}, \dots, \mathbf{m^{\mathrm{closest}}_L}\right\}$, with the top $L$ closest distances to the audio embedding. 
Finally, we compute a weighted sum of these $L$ augmented language embeddings and balance its contribution with the audio embedding. The weights of these language embeddings are derived by calculating the dot-product similarity between $z^a$ and each $\mathbf{m^{\mathrm{closest}}_i}$, normalized via a softmax function. This overall \textbf{Santa} process is formalized as:
\begin{equation}
\begin{split}
&f_{\mathrm{Santa}}\left(z^a\right) =\left(1-\lambda\right)z^a+\lambda  \sum_{i=1}^L w_i * \mathbf{m^{\mathrm{closest}}_i},\\ 
 &\text{where}, w_i= \frac{\exp\left(\left(\left(z^a\right)^T\mathbf{m^{\mathrm{closest}}_i}\right)/\tau\right)}{\sum_{j=1}^L\exp\left(\left(\left(z^a\right)^T\mathbf{m^{\mathrm{closest}}_j}\right)/\tau\right)},
\end{split}
\label{eq:santa}
\end{equation}
where $\tau$ is a temperature hyperparameter and $\lambda$ is a balancing hyperparameter. %that controls the relative contributions of the audio embedding and the weighted text embeddings.

% \begin{equation}
% \begin{split}
% \mathcal{O}({\mathrm{Santa}}) &\leq \mathcal{O}(\max\limits_i |\mathcal{S}_i|), \\
% \mathcal{O}({\mathrm{DRCap}}) &= \mathcal{O}(|\mathcal{M}|),
% \end{split}
% \end{equation}
% where $|\cdot|$ denotes the cardinality of $\cdot$, and with $N_c > 1$, it follows that $\max\limits_i |\mathcal{S}_i| < |\mathcal{M}|$.
% Additionally, compared to noise-based and memory-based methods, the Santa mechanism achieves the lowest Modality Discrepancy, as illustrated in \cref{fig: modal_transfer_visual} and further discussed in \cref{sec:modality gap}.

% \begin{table}[h]
% \caption{The time cost of inference on AudioCaps \citep{kim-etal-2019-audiocaps}. \textbf{Note:} AVG represents the average time consumed per test sample when using an A100 GPU with a batch size of 8.}
% \label{tab: results of timecost}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{tabular}{lc}
% \toprule
% Modal-Transfer & AVG (s/it) \\
% \midrule
% Memory-based & 1.5771 \\
% \midrule
% Santa & 1.4795 \\
% \bottomrule
% \end{tabular}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

\subsection{Training Pipeline} \label{sec:train and test pipeline}

\textbf{\texttt{AudioTIA-5M} Dataset.} \ MATS is designed to handle a wide range of audio tasks relying solely on \textbf{text} supervision, which makes existing audio datasets unsuitable for direct use in its training. In this work, we construct a new dataset \texttt{AudioTIA-5M}, where each training sample is formatted as (\texttt{text}, \texttt{instruction}, \texttt{answer}) tuple. %In this structure, $\texttt{text}$ and \texttt{instruction} serve as model inputs, while \texttt{answer} represents the corresponding label. 
\texttt{AudioTIA-5M} encompasses both close-ended and open-ended tasks, employing four audio-task templates in \cref{tab:Instruction examples} of Appendix.  
For the classification task (CLS), we leverage ChatGPT \citep{openai2024gpt4technicalreport} to generate a substantial amount of  textual data. Specifically, by utilizing ChatGPT's instruction-following and text generation capabilities, we create a diverse set of audio descriptions (used as \texttt{text}) based on given event labels (used as \texttt{answer}). Further details are provided in \cref{appendix:pipeline}. 
For the audio captioning task (CAP), we directly use audio caption annotations from  public datasets, including Clotho-v2 \citep{drossos2019clothoaudiocaptioningdataset}, AudioCaps \citep{kim-etal-2019-audiocaps}, MusicCaps \citep{agostinelli2023musiclmgeneratingmusictext}, WavCaps \citep{Mei_2024} and Macs \citep{8ced9ba43cbd49c1acd288d151565342}, as \texttt{text} and \texttt{answer}. 
To further enhance model's understanding and reasoning capabilities for audio, we incorporate two open-ended datasets: OpenAQA \citep{gong2024listenthinkunderstand} and  MusicQA \citep{10447027}. In OpenAQA, we replace raw audio files with  their audio captions as model inputs.
For MusicQA, where music captions are unavailable, we generate corresponding captions using LP-MusicCaps \citep{doh2023lpmusiccapsllmbasedpseudomusic}.
Overall, \texttt{AudioTIA-5M} comprises  two subsets: a $1.5$ M close-ended question subset, and a $3.8$ M open-ended question subset. %, both are crucial to the success of MATS training.
The detailed statistics of \texttt{AudioTIA-5M} are presented in \cref{tab:train dataset} of Appendix.

% \begin{table*}[!t]
%     \small
%     \centering
%     \caption{The statics of the \texttt{AudioTIA} dataset.}
%     \vskip 0.15in
%     % \resizebox{\linewidth}{!}{
%     \begin{tabular}{lcccll}
%         \toprule
%         \multicolumn{5}{l}{\textbf{Close-Ended Audio Text Instruction Answering Data ($\sim$1.5M)}} \\
%         \toprule
%         Audio Type & Task & Datasets & \#Samples & Percentage \\
%         \midrule
%         \multirow{10}{*}{Sound} & \multirow{7}{*}{CLS} & AudioSet  \citep{7952261}, FSD50k  \citep{fonseca2022fsd50kopendatasethumanlabeled}, & \multirow{7}{*}{$\sim$385K} & \multirow{7}{*}{$\sim$25.6\%} \\
%         & & VGGSound (VGG) \citep{chen2020vggsoundlargescaleaudiovisualdataset}, DCASE  \citep{mesaros:hal-01627981}, & & \\
%         & & ECS-50 \citep{10.1145/2733373.2806390}, UrbanSound8K(US8K)  \citep{10.1145/2647868.2655045} & & \\
%         & & TUT Acoustic Scenes 2017(TUT) \citep{7760424}, & &\\
%         & & Sound Events for Surveillance Applications(SESA) \citep{spadini2019sound} & &\\
%         & & VS \citep{Gong_2022}, CREMA-D  \citep{cao2014crema} &  \\
%         & & RAVDESS  \citep{pub.1104022894} & &\\
%         \cline{2-5} \rule{0pt}{2ex}
%         & \multirow{2}{*}{CAP} & WavCaps  \citep{Mei_2024}, Macs  \citep{8ced9ba43cbd49c1acd288d151565342}, & \multirow{2}{*}{$\sim$518K} & \multirow{2}{*}{$\sim$34.5\%}\\
%         & & AudioCaps  \citep{kim-etal-2019-audiocaps}, Clotho-v2  \citep{drossos2019clothoaudiocaptioningdataset} & &\\
%         \cline{2-5} \rule{0pt}{2ex}
%         & AQA & Clotho-AQA  \citep{lipping2022clothoaqacrowdsourceddatasetaudio} & $\sim$20K & $\sim$1.3\%\\
%         \midrule
%         \multirow{4}{*}{Music} & \multirow{2}{*}{CLS} & GTZAN  \citep{Sturm_2014}, Beijing Opera (BJO) \citep{6853981}, & \multirow{2}{*}{$\sim$497K} & \multirow{2}{*}{$\sim$ 33.1\%} \\
%         & & OpenAQA  \citep{ghosh2024gamalargeaudiolanguagemodel} & & \\ 
%         \cline{2-5} \rule{0pt}{2ex}
%         & \multirow{2}{*}{CAP} & OpenAQA%\tablefootnote{The dataset related to Music introduced by GAMA is based on OpenAQA released by LTU.}
%         \citep{ghosh2024gamalargeaudiolanguagemodel}, MusicCaps  \citep{agostinelli2023musiclmgeneratingmusictext}, & \multirow{2}{*}{$\sim$30K} & \multirow{2}{*}{$\sim$5.5\%} \\
%         & & Song Describer dataset (SDD) \citep{manco2023songdescriberdatasetcorpus} &  \\ 
%         \toprule
%         % \hline
%         %\multirow{2}{*}{Speech} & \multirow{2}{*}{CLS} & VS  \citep{Gong_2022}, CREMA-D  \citep{cao2014crema} & \multirow{2}{*}{$\sim$32 K} \\
%         %& & RAVDESS  \citep{pub.1104022894} & \\ \hline
%         \multicolumn{4}{l}{\textbf{Open-Ended Audio Text Instruction Answering Data ($\sim$3.8M)}} \\
%         \toprule
%         Sound & AQA & OpenAQA \citep{gong2024listenthinkunderstand} & $\sim$2547K & $\sim$66.8\% \\
%         \midrule
%         Music & AQA & MusicQA \citep{10447027} & $\sim$1207K & $\sim$33.2\%\\
%         \bottomrule
%     \end{tabular}
%     % }
%     \label{tab:train dataset}
%     \vskip -0.1in
% \end{table*}

\textbf{Training.} \
To maintain alignment between language and audio embeddings within the CLAP model while leveraging the extensive knowledge and generative capacities of LLM, we freeze both CLAP and LLM model, training only the mapper module and LLM's LoRA adapters \citep{hu2021loralowrankadaptationlarge}.
%During training, the input consists exclusively of audio descriptions and instructions. The audio descriptions are first processed by the CLAP text encoder, followed by noise injection and Mapper module. The resulting embeddings, combined with their corresponding instructions, are subsequently fed into the LLM. 
Our model is trained on \texttt{AudioTIA-5M} dataset, which integrates both close-ended and open-ended tasks.  The close-ended tasks equip the model with foundational audio perception capabilities, while the open-ended tasks enhance its advanced reasoning abilities. Training is conducted in an autoregressive manner using Equation \ref{eq2}.

% \textbf{Inference.} \
% After the training, we establish the semantic connection between the LLM and the pretrained audio-language model. During inference, the CLAP text encoder is replaced by CLAP audio encoder combined with Santa, allowing MATS to process audio inputs and reduce modality gap. The LLM then integrates the Santa embedding and corresponding  instruction to generate the desired response  $f_{\mathrm{LLM}}$ following Equation \ref{eq3}.

\begin{table*}[t]
\vskip -0.15in
\caption{Comparision results on audio classification benchmarks and ClothoAQA benchmark. $^{\text{ZS-}}$ and $^{\dagger}$ indicate weakly zero-shot and supervised settings, respectively. Results without any annotation represent zero-shot setting. \textbf{Note:} the results of SALMONN and Qwen-Audio-Chat are from GAMA \citep{ghosh2024gamalargeaudiolanguagemodel}.}
\vskip -0.1in
\label{tab: results of classification}
\begin{center}
\begin{small}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|llllllll|ll}
\toprule
Audio Type & \multicolumn{8}{c}{Sound}  & \multicolumn{2}{c}{Music} \\% & \multicolumn{3}{c}{Speech} \\
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{\shortstack{ESC-50\\(ACC)}} & \multirow{2}{*}{\shortstack{FSD50K\\(mAP)}} & \multirow{2}{*}{\shortstack{DCASE\\(ACC)}} & \multirow{2}{*}{\shortstack{TUT\\(ACC)}} & \multirow{2}{*}{\shortstack{VGG\\(ACC)}} & \multirow{2}{*}{\shortstack{US8K\\(ACC)}} & \multicolumn{2}{c}{\multirow{2}{*}{\shortstack{ClothoAQA\\($\text{ACC}\vert \text{B-ACC}$)}}} & \multirow{2}{*}{\shortstack{GTZAN\\(ACC)}} & \multirow{2}{*}{\shortstack{BJO\\(ACC)}} \\ % & RAVDESS & CREMA-D & VS \\ 
& & & & & & & & & \\
\midrule
CLAP \citep{elizalde2023clap} & 0.83 & 0.30 & 0.30 & 0.30 & - & \underline{0.73} & - & - & 0.25 & 0.30  \\ % & 0.16 & 0.18 & 0.49 \\
Pengi \citep{deshmukh2024pengiaudiolanguagemodel} & \textbf{0.92} & 0.47 & 0.33 & 0.35 & - & 0.72 & -& 0.65$^{\dagger}$ & 0.35 & 0.62 \\% & \underline{0.20} & 0.18 & 0.60 \\
LTU \citep{gong2024listenthinkunderstand} & 0.83$^{\text{ZS-}}$ & 0.46$^{\dagger}$ & \textbf{0.46}$^{\text{ZS-}}$ & 0.33 & \underline{0.50}$^{\dagger}$ & - & -& - & - & \underline{0.70}\\%& - & - & 0.56 \\
LTU-AS \citep{gong2024listenthinkunderstand} & 0.81$^{\text{ZS-}}$ & - & - & - & -&-& -& -& 0.50 & - \\%&-&-&-\\
GAMA \citep{ghosh2024gamalargeaudiolanguagemodel} & 0.83$^{\text{ZS-}}$ & \underline{0.48}$^{\dagger}$ & 0.38$^{\text{ZS-}}$ & 0.22 & \textbf{0.52}$^{\dagger}$ & - & -& 0.72$^{\dagger}$ & 0.14 & \underline{0.70}\\%& - & - & 0.52 \\
Qwen-Audio \citep{chu2023qwenaudioadvancinguniversalaudio} & - & - & - & \textbf{0.65}$^{\dagger}$ & - & - & 0.58$^{\dagger}$ & 0.75$^{\dagger}$ & - & - \\% & - & - & \textbf{0.93} \\
Audio Flamingo \citep{kong2024audioflamingonovelaudio} & - & \textbf{0.70}$^{\dagger}$ & - & - & - & \textbf{0.75} &  -& \textbf{0.87}$^{\dagger}$ & \textbf{0.68} & - \\%& \multirow{2}{*}{\textbf{0.21}}  & \multirow{2}{*}{\textbf{0.27}} & \multirow{2}{*}{-} \\
\textcolor{black}{SALMONN \citep{tang2024salmonngenerichearingabilities}} & \textcolor{black}{0.16} & \textcolor{black}{0.22} & \textcolor{black}{0.18} & \textcolor{black}{0.08} & \textcolor{black}{0.23} & \textcolor{black}{-} &  -&- & \textcolor{black}{0.10} & \textcolor{black}{0.25} \\% & \textcolor{black}{-} & \textcolor{black}{-} & \textcolor{black}{0.17} \\
\textcolor{black}{Qwen-Audio Chat \citep{bai2023qwentechnicalreport}} & \textcolor{black}{0.72} & \textcolor{black}{0.40} & \textcolor{black}{0.32} & \textcolor{black}{0.17} & \textcolor{black}{0.18} & \textcolor{black}{-} &  -&- & \textcolor{black}{0.42} & \textcolor{black}{0.51} \\% & \textcolor{black}{\multirow{2}{*}{-}} & \textcolor{black}{\multirow{2}{*}{-}} & \textcolor{black}{\multirow{2}{*}{0.74}} \\
\midrule
% MATS(B) & 0.87 & \underline{0.48} & \underline{0.45} & \underline{0.49} & 0.30 & 0.66 & 0.59 & 0.68 & 0.19 & 0.17 & \underline{0.79}  \\
\textbf{MATS-GPT2 (Ours)} & 0.86 & \underline{0.48} & 0.43 & \underline{0.49} & 0.29 & 0.67 &  0.56&0.66& 0.58 & 0.70 \\% & \underline{0.20} & 0.17 & \underline{0.79}\\
\textbf{MATS-LLaMA (Ours)} & \underline{0.88} & 0.45 & 0.43 & 0.47 & 0.29 & 0.67 &  0.66&0.76& \underline{0.63} & \textbf{0.73} \\% & 0.19 & \underline{0.20} & \underline{0.79}   \\
\midrule
\textcolor{gray}{MATS-Audio} & \textcolor{gray}{0.70} & \textcolor{gray}{0.43$^{\dagger}$} & \textcolor{gray}{0.55$^{\dagger}$} & \textcolor{gray}{0.61$^{\dagger}$} & \textcolor{gray}{0.42$^{\dagger}$} & \textcolor{gray}{0.39} &  \textcolor{gray}{0.67$^{\dagger}$}& \textcolor{gray}{0.76$^{\dagger}$}& \textcolor{gray}{0.29} & \textcolor{gray}{0.46} \\
\bottomrule
\end{tabular}
}
\end{small}
\end{center}
\vskip -0.2in
\end{table*}

\section{Experiment} \label{sec:experiment}
In this section, we conduct extensive experiments on both close-ended and open-ended tasks. The details about training and test benchmarks are provided in \cref{appendix:dataset}. Additional extensive hyperparameter analysis and visualizations are provided in \cref{appendix: experiment}.


%about the cluster size $K$, the number of selected text embeddings $L$, memory bank size $M$, temperature $\tau$,  balancing parameter $\lambda$  on Santa, and the number of mapper layer, are provided in \cref{appendix: hyper}.

% This section primarily outlines the experimental setup presented in \cref{sec:exp_set}. Subsequently, we present the results in \cref{tab:test dataset} and discuss the ablation studies of the MATS and the Santa mechanism. Additionally, we provide experiments on various factors, including temperature $\tau$, the balancing parameter $\lambda$, top $L$, LoRA variants, memory bank size $M$, cluster size $K$, and the number of Mapper layers, as detailed in \cref{appendix: hyper}.

\subsection{Experimental Setup} \label{sec:exp_set}
\textbf{MATS-GPT2.} 
In the smaller version, we employ GPT2 with 125M parameters as the LLM. %, referred  to as MATS-GPT2. 
We use CLAP's audio encoder and language encoder. The audio is sampled at 44.1 kHz and converted into a log Mel spectrograms
with 64 Mel bins, a hop size of 320 ms, and a window size of 1024 ms.
All audio files are randomly truncated to 7 seconds in length with CLAP audio encoder. The mapping module uses a 8-layer transformer with a prefix length of 40. 
Training is conducted using the AdamW optimizer with a learning rate of $5 \times 10^{-5}$, a linear learning rate scheduler with 2000 warmup steps. The batch size is set to 128. For the LoRA  configuration, we set the rank to 8, the scaling factor to 4, and the dropout rate to 0.1. MATS-GPT2 underwent about 25 hours of training over 90,000 iterations on 2 A100 GPUs. 
The hyperparameters are configured as follows: $\sigma=0.015$, $K=100$, $L=32$, $\tau=0.1$ and $\lambda=0.3$ (Equation \ref{eq:santa}).

\textbf{MATS-LLaMA.} \
In the larger version, we employ the LLaMA-7B model as the LLM. %referred to as MATS-LLaMA.
We use AdamW optimizer with a learning rate of $3 \times 10^{-5}$, a cosine annealing learning rate scheduler with 3000 warmup steps. The batch size is set to 96. MATS-LLaMA underwent about 35 hours of training over 360,000 iterations. All other settings remain consistent with those of MATS-GPT2. 

\textbf{MATS-Audio.} \
We explore an audio version of MATS, supervised by audio. %, referred to as MATS-Upper. 
MATS-Audio %retains  the same architecture during training and inference, consisting of
consists of the CLAP audio encoder, the mapper module and LLaMA-7B. It employs the same training configurations as MATS-LLaMA, with a different training set as outlined in \cref{appendix: train set of MATS-Audio}.  
%The training set differs from that of MATS-LLaMA, as outlined in \cref{appendix: train set of MATS-Upper}.

\subsection{Main Results}
We  evaluate the close-ended and open-ended capabilities of MATS using 16 downstream tasks. \cref{tab:test benchmark} in Appendix summarizes all test benchmarks and corresponding evaluation metrics. Notably, for classification task,  we adopt the text-matching setup following \citep{gong2024listenthinkunderstand}, where the CLIP language encoder is used to encode both the answer set and model's predictions. The answer with the highest similarity to the prediction is selected as the final result.

\subsubsection{Close-ended Results} \label{sec: close-ended results}
\textbf{Zero-shot Audio Classification.} \
Since MATS performs audio classification solely based on GPT-generated descriptions, its performance is evaluated under zero-shot setting. \cref{tab: results of classification} summarizes the comparative results %for audio classification task
across both sound and music types. As shown in \cref{tab: results of classification}, MATS achieves SOTA performance in the zero-shot setting on FSD50K, DCASE, TUT, VGG and BJO  benchmarks, with improvements of $1\%, 10\%, 14\%, 6\%$ and $3\%$. Remarkably, MATS performs comparably with SALMONN \citep{tang2024salmonngenerichearingabilities} and Qwen-Audio Chat \citep{bai2023qwentechnicalreport}, both of which are trained on a significant amount of audio-language pairs. These results validate the efficacy of our approach for audio classification, highlighting the excellent audio perception capabilities of MATS in a zero-shot scenario.

\begin{table*}[t]
% \vskip -0.1in
\caption{Comparision results for audio captioning tasks on AudioCaps and Clotho benchmarks.} %$^{\dagger}$ indicates supervision. Results without any annotation represent zero-shot settings.}
\vskip -0.1in
\label{tab: results of audiocaps}
\begin{center}
\begin{small}
\resizebox{\linewidth}{!}{
\begin{tabular}{c | l|lll|lll|ll}
\toprule
& \multirow{2}{*}{Method} & \multicolumn{3}{c}{AudioCaps} & \multicolumn{3}{c}{Clotho} & \multicolumn{2}{c}{MusicCaps} \\
 & & CIDEr & SPICE & SPICEr & CIDEr & SPICE & SPICEr & ROUGH-L & BLUE4 \\
\toprule
\multirow{8}{*}{\shortstack{Audio\\Supervision \\(AS)}} & Pengi \citep{deshmukh2024pengiaudiolanguagemodel} & \textbf{0.752} & \underline{0.182} & 0.467 & 0.416 & 0.126 & 0.271 & - & - \\
 & LTU \citep{gong2024listenthinkunderstand} & - & 0.170 & - & - & 0.119 & - & - & -\\
& GAMA \citep{ghosh2024gamalargeaudiolanguagemodel} & - & \textbf{0.185} & - & - & \textbf{0.135} & - & -  & -\\
& LTU-AS \citep{gong2024listenthinkunderstand} & - & 0.150 & - & - & - & - & - & - \\
& SALMONN \citep{tang2024salmonngenerichearingabilities}  & - & - & \underline{0.485} & - & - & - & 21.5$^\dagger$ & 6.1$^\dagger$ \\
% \hline
& Audio Flamingo \citep{kong2024audioflamingonovelaudio}  & - & - & \textbf{0.502} & \textbf{0.465} & - & - & - & - \\
& LP-MusicCaps \citep{doh2023lpmusiccapsllmbasedpseudomusic} & - & - & - & - & - & - & 13.0 & 0.7 \\
\midrule
\multirow{6}{*}{\shortstack{Text\\Supervision\\ (TS)}} & NoAudioCaptioning \citep{deshmukh2024training} & 0.697 & \underline{0.178} & 0.437 & 0.379 & 0.132 & 0.256 & - & - \\
% \multirow{2}{*}{\shortstack{PromptACC\\  \citep{zhang2024zeroshotaudiocaptioningusing}}} \\
& PromptACC \citep{zhang2024zeroshotaudiocaptioningusing} & 0.644 & 0.156 & 0.400 & 0.403 & 0.119 & 0.261 & - & - \\
& DRCap \citep{li2024drcapdecodingclaplatents} & 0.718 & \textbf{0.186} & 0.452 & \underline{0.438} & 0.133 & \textbf{0.285} & - & - \\
\cline{2-10} \rule{0pt}{2ex}
% & & & & \\
% MATS(B)  & 0.63 & 0.17 & 0.40\\
& \textbf{MATS-GPT2 (Ours)} & 0.676 & 0.164 & 0.420 & 0.413 & 0.124 & 0.269 & \textbf{22.9} & \underline{4.5} \\
& \textbf{MATS-LLaMA (Ours)}  & \underline{0.735} & 0.171 & 0.453 & 0.431 & \underline{0.134} & \underline{0.282} & 18.7 & 3.2 \\
\midrule
AS & \textcolor{gray}{MATS-Audio} & \textcolor{gray}{0.704} & \textcolor{gray}{0.171} & \textcolor{gray}{0.438} & \textcolor{gray}{0.448} & \textcolor{gray}{0.137} & \textcolor{gray}{0.293} & \textcolor{gray}{21.7} & \textcolor{gray}{5.4}\\
\bottomrule
\end{tabular}
}
\end{small}
\end{center}
\vskip -0.2in
\end{table*}

\textbf{Audio Captioning.} \
\cref{tab: results of audiocaps} 
%and \cref{tab: results of music caps} 
summarizes the comparative results for audio captioning tasks on AudioCaps, Clotho, and MusicCaps benchmarks. We can observe that: (1) Compared to text-only supervised audio captioning models, which are specifically tailored for audio captioning and restricted  to the \textit{Sound} type, our model demonstrates enhanced versatility by effectively addressing a broader spectrum of audio-relevant tasks across both \textit{Sound} and \textit{Music} type.
Despite its broader applicability, our model achieves superior  CIDEr and SPICEr scores on AudioCaps and a higher SPICE score on Clotho. The performance gains can be largely attributed to the proposed \textbf{Santa} mechanism, which effectively mitigates  the audio-language modality gap within CLAP.
(2) Compared to audio-supervised models, MATS-LLaMA exhibits comparable performance despite being training only on text data. Also, MATS-GPT2 surpasses the music captioning model LP-MusicCaps  \citep{doh2023lpmusiccapsllmbasedpseudomusic} by \textbf{9.9\%} on the ROUGH-L metric. These results underscore the effective audio captioning capabilities of MATS under text-only supervision.

\textbf{Simple AQA.} \
The ClothoAQA benchmark primarily consists of responses limited to \texttt{yes} or \texttt{no} answers (evaluated using the B-ACC metric), framing the task largely as a classification problem. %Additionally, the remaining responses are restricted to single word, rendering them inadequate for evaluating a model's open-ended reasoning capabilities.
As shown in Table \ref{tab: results of classification}, MATS-LLaMA outperforms the recent model Qwen-Audio \citep{chu2023qwenaudioadvancinguniversalaudio} on ClothoAQA, achieving $8\%$ higher ACC score. This result highlights  the enhanced audio comprehensive capabilities of MATS-LLaMA after scaling up. However, MATS shows slightly lower performance on the B-ACC metric compared to Audio Flamingo \citep{kong2024audioflamingonovelaudio}.
Notably, Audio Flamingo leverages audio supervision during training and employs a sliding window to extract audio features. While this enhances its performance, it also introduces significant computational overhead. In contrast, MATS-LLaMA operates under text-only supervision, offering a more computationally efficient alternative. This trade-off between efficiency and performance underscores the practical considerations in model design for real-world applications, where computational cost is often a critical factor.

% \begin{table}[h]
% \caption{The results of our model in ClothoAQA. $^{\dagger}$ indicates supervision.}
% \vskip 0.15in
% \label{tab: results of simple aqa}
% \begin{center}
% \begin{small}
% \begin{tabular}{c| l|ll}
% \toprule
%  & Method & ACC & B-ACC\\
% \toprule
% \multirow{5}{*}{AS} & Pengi \citep{deshmukh2024pengiaudiolanguagemodel} & - & 0.65$^{\dagger}$ \\
% & GAMA \citep{ghosh2024gamalargeaudiolanguagemodel} & - & 0.72$^{\dagger}$ \\
% & Audio Flamingo \citep{kong2024audioflamingonovelaudio} & - & \textbf{0.87}$^{\dagger}$ \\
% & Qwen-Audio \citep{chu2023qwenaudioadvancinguniversalaudio}& \underline{0.58} & 0.75 \\
% \midrule
% % MATS(B) & 0.56 & 0.66 \\
% \multirow{2}{*}{TS} & \textbf{MATS-GPT2 (Ours)} & 0.56 & 0.66 \\
% & \textbf{MATS-LLaMA (Ours)} & \textbf{0.66} & \underline{0.76} \\
% \midrule
% AS & \textcolor{gray}{MATS-Upper} & \textcolor{gray}{0.64}$^\dagger$ & \textcolor{gray}{0.79}$^\dagger$ \\
% \multirow{2}{*}{\shortstack{Audio\\Supervision}} & \multirow{2}{*}{\textcolor{gray}{MATS-Upper}} & \multirow{2}{*}{\textcolor{gray}{0.64}$^\dagger$} & \multirow{2}{*}{\textcolor{gray}{0.79}$^\dagger$} \\
% & & & \\
% \bottomrule
% \end{tabular}
% \label{tab-simle}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

\subsubsection{Open-ended Results} \label{sec: open-ended results}

To further evaluate our model's understanding and reasoning capabilities in complex audio QA tasks, we conducted experiments on AIR-Bench Chat \citep{yang2024airbenchbenchmarkinglargeaudiolanguage} and MMAU \citep{sakshi2024mmaumassivemultitaskaudio} benchmarks. The results are presented in \cref{tab:comparison}.
\textbf{(1)} On AIR-Bench, under text-only supervision, MATS-LLaMA achieves performance comparable to Qwen-Audio \citep{chu2023qwenaudioadvancinguniversalaudio}, ranking third-highest on \textit{Sound} type and fourth-highest on \textit{Music} type. We argue that CLAP struggles to extract fine-grained audio information, such as the exact number of occurrences. This limitation makes MATS-LLaMA more prone to errors when responding to fine-grained questions on AIR-Bench.
\textbf{(2)}
For the MMAU, the QA format requires LALMs to select from a set of provided options. We use the following prompt: \texttt{\{question\} + Select one option: \{options\} Respond with exactly one of the options above.}
Compared to AIR-Bench, this option-based format offers additional contextual cues, partially mitigating  CLAP's limitations in extracting detailed information. Consequently, our model outperforms Qwen-Audio-Chat \citep{bai2023qwentechnicalreport} by about $3\%$ in the sound type and ranks second only to the Qwen2-Audio-Instruction \citep{chu2024qwen2audiotechnicalreport} in other audio types. Notably, our model even surpasses larger-scale models like Qwen-Audio-Chat and SALMONN. Furthermore, compared to MATS-Audio, MATS-LLaMA achieves comparable results on open-ended tasks, further

\FloatBarrier
\begin{table*}[h]
\centering
\vskip -0.2in
\caption{Comparison results for open-ended audio tasks across  sound and music types.}
\label{tab:comparison}
% \vskip 0.1in
\subtable[AIR-Bench benchmark.]{
\begin{minipage}[t]{0.35\textwidth}
\centering
\begin{small}
\vskip -0.1in
\resizebox{\linewidth}{!}{
\begin{tabular}{c|l|cc}
\toprule
& Method & Sound & Music \\%& Speech & Mixed Audio\\
\toprule
\multirow{11}{*}{AS} & Qwen2-Audio \citep{chu2024qwen2audiotechnicalreport} & \textbf{6.99} & \textbf{6.79} \\%& \textbf{7.18} & \textbf{6.77} \\% & 6.93 \\
& Qwen-Audio-Turbo \citep{bai2023qwentechnicalreport} & 6.59 & \underline{5.98} \\
& SALMONN \citep{tang2024salmonngenerichearingabilities} & 6.28 & 5.95 \\%& 6.16 & \underline{6.08} \\% & 6.11 \\
& Qwen-Audio \citep{chu2023qwenaudioadvancinguniversalaudio} & \underline{6.95} & 5.52 \\%& 6.47 & 5.38 \\% & 6.08 \\
& Gemini-1.5-pro \citep{geminiteam2024gemini15unlockingmultimodal} & 5.49 & 5.06 \\%& 6.97 & 5.27\\ % & 5.70 \\
& BLSP \citep{wang2024blspbootstrappinglanguagespeechpretraining} & 5.55 & 5.08 \\%& 6.17 & 4.52 \\ % & 5.33 \\
& Pandagpt \citep{su2023pandagptmodelinstructionfollow} & 5.46 & 5.06 \\%& 3.58 & 2.93 \\% & 4.25 \\
& Next-gpt \citep{wu2024nextgptanytoanymultimodalllm} & 4.76 & 4.18 \\%& 3.86 & 2.92 \\% & 4.13 \\
& SpeechGPT \citep{zhang2023speechgptempoweringlargelanguage} & 0.95 & 0.95 \\%& 1.57 & 1.14 \\% & 1.15 \\
& Macaw-LLM \citep{lyu2023macawllmmultimodallanguagemodeling} & 1.01 & 0.91 \\%& 0.97 & 1.00 \\ % & 1.01 \\
% \cline{2-4}
\midrule
\multirow{2}{*}{TS} & \textbf{MATS-GPT2 (Ours)} & 4.33 & 3.31 \\%& 3.94 & 2.9\\
& \textbf{MATS-LLaMA (Ours)} & 6.43 & 5.76 \\%& 4.97 & 3.95\\
% & & & \\
\midrule
AS & \textcolor{gray}{MATS-Audio} & \textcolor{gray}{6.25} & \textcolor{gray}{5.77} \\
\bottomrule
\end{tabular}
}
\end{small}
% \vskip -0.1in
\end{minipage}
}
\hfill
\subtable[MMAU benchmark.]{
\begin{minipage}[t]{0.6\textwidth}
\centering
\vskip -0.1in
\begin{small}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|l|l|cc|cc|cc}
\toprule
& \multirow{2}{*}{Method} & \multirow{2}{*}{Size} & \multicolumn{2}{c}{Sound} & \multicolumn{2}{c}{Music} & \multicolumn{2}{c}{Avg}\\
& & & Test-mini & Test& Test-mini & Test& Test-mini & Test\\
\toprule
\multirow{9}{*}{AS}& Qwen2-Audio-Instruction \citep{chu2024qwen2audiotechnicalreport} & 8.4B  & \underline{55.0} & 45.9 & \textbf{51.0} & \textbf{53.3} & \textbf{49.2} & \textbf{52.5} \\
& Qwen-Audio-Chat \citep{bai2023qwentechnicalreport} & 8.4B  & \textbf{55.3} & \underline{56.7} & 44.0 & 40.9 & 43.1 & 41.9 \\
& SALMONN \citep{tang2024salmonngenerichearingabilities} & 13B  & 41.0 & 40.3 & 34.8 & 33.8 & 33.7 & 32.8 \\
& GAMA \citep{ghosh2024gamalargeaudiolanguagemodel} & 7B  & 41.4 & 45.4 & 32.3 & 30.8 & 30.9 & 31.8 \\
& MuLLaMa \citep{liu2023musicunderstandingllamaadvancing} & 7B  & 40.8 & 44.8 & 32.6 & 30.6 & 31.9 & 30.7 \\
& GAMA-IT \citep{ghosh2024gamalargeaudiolanguagemodel} & 7B  & 43.2 & 43.2 & 28.4 & 28.0 & 30.2 & 29.0 \\
& LTU-AS \citep{gong2024listenthinkunderstand} & 7B  & 23.4 & 25.0 & 9.1 & 10.5 & 17.7 & 18.9 \\
& Audio Flamingo \citep{kong2024audioflamingonovelaudio} & 2.2B & 23.4 & 28.3 & 15.3 & 18.2 & 16.7 & 18.9 \\
& LTU \citep{gong2024listenthinkunderstand} & 7B  & 22.5 & 25.9 & 9.7 & 12.8 & 16.9 & 18.5 \\
& Pengi \citep{deshmukh2024pengiaudiolanguagemodel} & 323M  & 6.1 & 8.0 & 2.9 & 3.1 & 3.4 & 4.2 \\
\midrule
\multirow{2}{*}{TS} & \textbf{MATS-GPT2 (Ours)} & 370M  & 7.8 & 7.2 & 14.1 & 11.7 & 10.5 & 9.8 \\
& \textbf{MATS-LLaMA (Ours)} & 7B & 52.3 & \textbf{59.8} & \underline{44.9} & \underline{42.9} & \underline{43.3} & \underline{44.5} \\
% & & & & & & & & \\
\midrule
AS & \textcolor{gray}{MATS-Audio} & \textcolor{gray}{7B} & \textcolor{gray}{51.7} & \textcolor{gray}{57.4} & \textcolor{gray}{43.7} & \textcolor{gray}{41.5} & \textcolor{gray}{42.2} & \textcolor{gray}{43.8} \\
\bottomrule
\end{tabular}
}
\end{small}
\end{minipage}
}
\vskip -0.2in
\end{table*}
\FloatBarrier
validating the effectiveness of the text-supervised strategy for training LALMs.
% \begin{table*}[t]
% \caption{The results of our model on the MMAU datasets across the sound and music categories.}
% \vskip 0.15in
% \label{tab: results of MMAU}
% \begin{center}
% \begin{small}
% \begin{tabular}{c|l|l|cc|cc|cc}
% \toprule
%  & \multirow{2}{*}{Method} & \multirow{2}{*}{Size} & \multicolumn{2}{c}{Sound} & \multicolumn{2}{c}{Music} & \multicolumn{2}{c}{Avg}\\
% & & & Test-mini & Test& Test-mini & Test& Test-mini & Test\\
% \toprule
% \multirow{9}{*}{AS}& Qwen2-Audio-Instruction \citep{chu2024qwen2audiotechnicalreport} & 8.4B  & \underline{55.0} & 45.9 & \textbf{51.0} & \textbf{53.3} & \textbf{49.2} & \textbf{52.5} \\
% & Qwen-Audio-Chat \citep{bai2023qwentechnicalreport} & 8.4B  & \textbf{55.3} & \underline{56.7} & 44.0 & 40.9 & 43.1 & 41.9 \\
% & SALMONN \citep{tang2024salmonngenerichearingabilities} & 13B  & 41.0 & 40.3 & 34.8 & 33.8 & 33.7 & 32.8 \\
% & GAMA \citep{ghosh2024gamalargeaudiolanguagemodel} & 7B  & 41.4 & 45.4 & 32.3 & 30.8 & 30.9 & 31.8 \\
% & MuLLaMa \citep{liu2023musicunderstandingllamaadvancing} & 7B  & 40.8 & 44.8 & 32.6 & 30.6 & 31.9 & 30.7 \\
% & GAMA-IT \citep{ghosh2024gamalargeaudiolanguagemodel} & 7B  & 43.2 & 43.2 & 28.4 & 28.0 & 30.2 & 29.0 \\
% & LTU-AS \citep{gong2024listenthinkunderstand} & 7B  & 23.4 & 25.0 & 9.1 & 10.5 & 17.7 & 18.9 \\
% & LTU \citep{gong2024listenthinkunderstand} & 7B  & 22.5 & 25.9 & 9.7 & 12.8 & 16.9 & 18.5 \\
% & Pengi \citep{deshmukh2024pengiaudiolanguagemodel} & 323M  & 6.1 & 8.0 & 2.9 & 3.1 & 3.4 & 4.2 \\
% \midrule
% \multirow{2}{*}{TS} & \textbf{MATS-GPT2 (Ours)} & 370M  & 7.8 & 7.2 & 14.1 & 11.7 & 10.5 & 9.8 \\
% & \textbf{MATS-LLaMA (Ours)} & 7B & 52.3 & \textbf{59.8} & \underline{44.9} & \underline{42.9} & \underline{43.3} & \underline{44.5} \\
% % & & & & & & & & \\
% \midrule
% AS & \textcolor{gray}{MATS-Audio} & \textcolor{gray}{7B} & \textcolor{gray}{51.7} & \textcolor{gray}{57.4} & \textcolor{gray}{43.7} & \textcolor{gray}{41.5} & \textcolor{gray}{42.2} & \textcolor{gray}{43.8} \\
% % \multirow{2}{*}{\shortstack{Audio\\Supervision}} & \multirow{2}{*}{\textcolor{gray}{MATS-Upper}} & \multirow{2}{*}{\textcolor{gray}{7B}} & \multirow{2}{*}{\textcolor{gray}{60.36}} & \multirow{2}{*}{\textcolor{gray}{60.33}} & \multirow{2}{*}{\textcolor{gray}{45.51}} & \multirow{2}{*}{\textcolor{gray}{48.17}} & \multirow{2}{*}{\textcolor{gray}{45.5}} & \multirow{2}{*}{\textcolor{gray}{48.61}} \\
% % & & & & & & & & \\
% \bottomrule
% \end{tabular}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}

% \begin{table}[h]
% \caption{The results of our model on the AIR-Bench Chat Benchmark across the sound and music categories.}
% \vskip 0.15in
% \label{tab: results of airbench}
% \begin{center}
% \begin{small}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|l|cc}
% \toprule
%  & Method & Sound & Music \\%& Speech & Mixed Audio\\
% \toprule
% \multirow{11}{*}{AS} & Qwen2-Audio \citep{chu2024qwen2audiotechnicalreport} & \textbf{6.99} & \textbf{6.79} \\%& \textbf{7.18} & \textbf{6.77} \\% & 6.93 \\
% & Qwen-Audio-Turbo \citep{bai2023qwentechnicalreport} & 6.59 & \underline{5.98} \\
% & SALMONN \citep{tang2024salmonngenerichearingabilities} & 6.28 & 5.95 \\%& 6.16 & \underline{6.08} \\% & 6.11 \\
% & Qwen-Audio \citep{chu2023qwenaudioadvancinguniversalaudio} & \underline{6.95} & 5.52 \\%& 6.47 & 5.38 \\% & 6.08 \\
% & Gemini-1.5-pro \citep{geminiteam2024gemini15unlockingmultimodal} & 5.49 & 5.06 \\%& 6.97 & 5.27\\ % & 5.70 \\
% & BLSP \citep{wang2024blspbootstrappinglanguagespeechpretraining} & 5.55 & 5.08 \\%& 6.17 & 4.52 \\ % & 5.33 \\
% & Pandagpt \citep{su2023pandagptmodelinstructionfollow} & 5.46 & 5.06 \\%& 3.58 & 2.93 \\% & 4.25 \\
% & Next-gpt \citep{wu2024nextgptanytoanymultimodalllm} & 4.76 & 4.18 \\%& 3.86 & 2.92 \\% & 4.13 \\
% & SpeechGPT \citep{zhang2023speechgptempoweringlargelanguage} & 0.95 & 0.95 \\%& 1.57 & 1.14 \\% & 1.15 \\
% & Macaw-LLM \citep{lyu2023macawllmmultimodallanguagemodeling} & 1.01 & 0.91 \\%& 0.97 & 1.00 \\ % & 1.01 \\
% % \cline{2-4}
% \midrule
% \multirow{2}{*}{TS} & \textbf{MATS-GPT2 (Ours)} & 4.33 & 3.31 \\%& 3.94 & 2.9\\
% & \textbf{MATS-LLaMA (Ours)} & 6.43 & 5.76 \\%& 4.97 & 3.95\\
% % & & & \\
% \midrule
% AS & \textcolor{gray}{MATS-Audio} & \textcolor{gray}{6.25} & \textcolor{gray}{5.77} \\
% % \multirow{2}{*}{\shortstack{Audio\\Supervision}} & \multirow{2}{*}{\textcolor{gray}{MATS-Upper}} & \multirow{2}{*}{\textcolor{gray}{6.30}} & \multirow{2}{*}{\textcolor{gray}{5.91}} \\
% % & & & \\
% \bottomrule
% \end{tabular}
% }
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}
% For the MMAU dataset, the QA format requires LALMs to choose from provided options. We used the following prompt:
% \textit{"\{question\} + Select one option:\textbackslash n \{options\}\textbackslash n Respond with exactly one of the options above."}
% Compared to the AIR-Bench dataset, this option-based format provides the model with some contextual hints, partially compensating for the lack of detailed information in CLAP. Consequently, our model outperformed Qwen-Audio-Chat by approximately 3\% in the sound category and achieved performance second only to the Qwen2-Audio-Instruction model in other audio categories. Notably, our model even surpassed larger-scale models like Qwen-Audio-Chat and SALMONN. Moreover, compared to the upper-bound of our model, our approach achieved comparable performance on open-ended tasks, further validating the effectiveness of the text-supervised strategy in LALMs.
% \begin{table*}[ht]
% \caption{The results of our model in MMAU Chat Benchmark under zero-shot setting. B, S and L indicates that the model trained on stage one, Small and Large.}
% \vskip 0.15in
% \label{tab: results of MMAU}
% \begin{center}
% \begin{small}
% \begin{tabular}{l|c|cc|cc|cc|cc}
% \toprule
% Model & Size & \multicolumn{2}{c}{Speech} & \multicolumn{2}{c}{Sound} & \multicolumn{2}{c}{Music} & \multicolumn{2}{c}{Avg}\\
% \toprule
% & & Test-mini & Test& Test-mini & Test& Test-mini & Test& Test-mini & Test\\
% % \multirow{2}{*}{Qwen2-Audio-Instruction} & \multirow{2}{*}{8.4B} & \multirow{2}{*}{54.95} & \multirow{2}{*}{45.9} & \multirow{2}{*}{50.98} & \multirow{2}{*}{53.26} & \multirow{2}{*}{42.04} & \multirow{2}{*}{45.9} & \multirow{2}{*}{49.2} & \multirow{2}{*}{52.5} \\
% Qwen2-Audio-Instruction & 8.4B & \textbf{54.95} & 45.9 & \textbf{50.98} & \textbf{53.26} & \textbf{42.04} & \textbf{45.9} & \textbf{49.2} & \textbf{52.5} \\
% Qwen-Audio-Chat & 8.4B & \underline{55.25} & \underline{56.73} & 44 & 40.9 & 30.03 & 27.95 & 43.1 & 41.86 \\
% SALMONN & 13B & 41 & 40.3 & 34.8 & 33.76 & 25.5 & 24.24 & 33.7 & 32.77 \\
% GAMA & 7B & 41.44 & 45.4 & 32.33 & 30.83 & 18.19 & 19.21 & 30.2 & 31.81 \\
% MuLLaMa & 7B & 40.84 & 44.8 & 32.63 & 30.63 & 22.22 & 16.56 & 31.9 & 30.66 \\
% GAMA-IT& 7B & 43.24 & 43.23 & 28.44 & 28 & 18.19 & 15.84 & 30.2 & 29.02 \\
% LTU-AS & 7B & 23.35 & 24.96 & 9.1 & 10.46 & 20.6 & 21.3 & 17.68 & 18.9 \\
% LTU & 7B & 22.52 & 25.86 & 9.69 & 12.83 & 17.71 & 16.37 & 16.89 & 18.51 \\
% Pengi & 323M & 6.1 & 8 & 2.9 & 3.05 & 1.2 & 1.5 & 3.4 & 4.18 \\
% \hline
% MATS(S) & 270M & 9.6 & 10.5 & 7.78 & 7.17 & 14.11 & 11.73 & 10.5 & 9.8 \\
% MATS(L) & 7B & 52.25 & \textbf{59.47} & \underline{44.91} & \underline{42.9} & \underline{32.73} & \underline{31.13} & \underline{43.3} & \underline{44.5}\\
% \bottomrule
% \end{tabular}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}

\subsection{Ablation}
In this subsection, we conduct ablation studies to validate the effectiveness of our method. Further analysis of the hyperparameters in \cref{eq:santa}, including $K$, $L$, $M$, $\tau$, $\lambda$, and the number of mapper layers, is provided in \cref{appendix: hyper}

\textbf{MATS-GPT2 vs MATS-LLaMA.} \
As shown in \cref{tab: results of classification} and \cref{tab: results of audiocaps}
, MATS-GPT2 achieves performance comparable to MATS-LLaMA on close-ended tasks. However, as shown in \cref{tab:comparison}, MATS-GPT2 exhibits significantly weak performance on open-ended tasks. These results indicate that a smaller LLM like GPT2 is sufficient for handling basic audio perception tasks, but it struggles with complex open-ended tasks that require advanced audio understanding and reasoning. In contrast, larger LLMs are better equipped to capture the depth and complexity necessary for audio reasoning and comprehension, making them indispensable for open-ended tasks.

\textbf{MATS-LLaMA vs MATS-Audio.} \
As shown in \cref{tab: results of classification},  MATS-LLaMA achieves performance comparable to MATS-Audio in the supervised setting. However, 
under the zero-shot setting, MATS-LLaMA significantly outperforms MATS-Audio, highlighting the heavy reliance of audio-supervised models on audio-language pairs for achieving superior performance. Moreover, as shown in \cref{tab:comparison}, MATS-LLaMA achieves performance on par with  MATS-Audio in open-ended scenarios, further highlighting the cost-effectiveness and efficiency of our approach.
 
\textbf{Santa vs other modal-transfer mechanism.} \
\cref{tab:ablation_Santa} presents the ablation study of our proposed \textbf{Santa} mechanism. %, designed  to mitigate the audio-language modality gap within CLAP. 
The study evaluates the impact of excluding various components, including k-means (w/o K-means), memory bank (w/o MB), audio embedding (w/o AE) and Gaussian noise (w/o GN). As shown in  \cref{tab:ablation_Santa}, removing any component of Santa results in a performance drop, highlighting their collective effectiveness. Notably, the absence of Gaussian noise (w/o GN) during training and the exclusion of audio embeddings (w/o AE) during inference lead to significant performance  decline. These results emphasize the critical role of incorporating  Gaussian noise during training and retaining audio embeddings during inference to achieve robust performance.
Additionally, we compare Santa with other modal-transfer methods, including the memory-based mechanism in DRCap \citep{li2024drcapdecodingclaplatents} and the noise-based mechanism in NoAudioCaptioning \citep{deshmukh2024training}. 
As shown in \cref{tab:ablation_Santa}, Santa achieves  better performance, validating its superiority in bridging the audio-language modality gap.  

% \begin{table}[h]
% \centering
% \caption{Ablation Study of Santa and Other methods on the AudioCaps Dataset. \textbf{Note:} The memory-based approach corresponds to DRCap \citep{li2024drcapdecodingclaplatents}, and the noise-based approach corresponds to NoAudioCaptioning \citep{deshmukh2024training}. MB: memory bank, AE: audio embedding, GN: guassian noise}
% \vsk-ip 0.15in
% \label{tab:ablation_Santa}
% \begin{small}
% \begin{tabular}{l|ccc}
% \toprule
%  Method & CIDEr & SPICE & SPIDEr \\
% \toprule
% \textcolor{black}{Memory-based} & \textcolor{black}{0.234} & \textcolor{black}{0.094} & \textcolor{black}{0.164} \\
% \textcolor{black}{Noise-based} & \textcolor{black}{0.698} & \textcolor{black}{0.170} & \textcolor{black}{0.434} \\
% \midrule
% MATS-LLaMA & \textbf{0.735} & \textbf{0.171} & \textbf{0.453} \\
% \quad - w/o K-means & 0.703 & 0.171 & 0.437 \\
% \quad - w/o MB & 0.698 & 0.170 & 0.434 \\
% \quad - w/o AE & 0.343 & 0.114 & 0.229 \\
% \quad - w/o GN & 0.384 & 0.121 & 0.253 \\
% \bottomrule
% \end{tabular}
% \end{small}
% \end{table} 

\textbf{The impact of Gaussian noise variance $\sigma$.} \
\cref{fig: noise_} investigates the impact of the variance $\sigma$ of injected noise during training. MATS achieves the best performance at $\sigma=0.015$. %It can be observed that performance degrades when $\sigma$ is either too small or too large. 
We argue that a small variance fails to effectively mitigate the audio-language modality gap within CLAP, while an excessively large variance 
%causes noise to dominate the augmented language embedding, thereby 
impairs the model's ability to capture useful input information.

% \begin{table}[t]
% \centering
% \caption{Ablation study of MATS-LLaMA on the cluster number  $K$ in the Santa mechanism, evaluated on AudioCaps dataset.}
% \vskip 0.15in
% \label{tab:clutsers}
% \begin{small}
% \begin{tabular}{l | ccc}
% \toprule
%  $K$ & CIDEr & SPICE & SPIDEr \\
% \toprule
% 10 & 0.731 & 0.17 & 0.45 \\
% 30 & 0.733 & 0.17 & 0.452 \\
% 60 & 0.734 & 0.171 & 0.452 \\
% 100 & \textbf{0.735} & \textbf{0.171} & \textbf{0.453} \\
% 150 & 0.731 & 0.17 & 0.45 \\
% \bottomrule
% \end{tabular}
% \end{small}
% \vskip -0.1in
% \end{table}

% \begin{figure}[h]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.7\columnwidth]{./imgs/noise_.pdf}}
% \caption{The impact of $\sigma$ on Model performance on the AudioCaps Dataset}
% \label{fig: noise_}
% \end{center}
% \vskip -0.2in
% \end{figure} 

\begin{minipage}[ht]{\columnwidth}
\begin{minipage}[ht]{0.5\columnwidth}
    \centering
    \makeatletter\def\@captype{table}\makeatother
    \caption{Ablation Study of Santa on AudioCaps benchmark.} %\textbf{Note:} The memory-based approach corresponds to DRCap \citep{li2024drcapdecodingclaplatents}, and the noise-based approach corresponds to NoAudioCaptioning \citep{deshmukh2024training}. MB: memory bank, AE: audio embedding, GN: guassian noise}
    \vskip 0.15in
    \label{tab:ablation_Santa}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccc}
    \toprule
     Method & CIDEr & SPICE & SPIDEr \\
    \toprule
    \textcolor{black}{Memory-based} & \textcolor{black}{0.234} & \textcolor{black}{0.094} & \textcolor{black}{0.164} \\
    \textcolor{black}{Noise-based} & \textcolor{black}{0.698} & \textcolor{black}{0.170} & \textcolor{black}{0.434} \\
    \midrule
    MATS-LLaMA & \textbf{0.735} & \textbf{0.171} & \textbf{0.453} \\
    \quad - w/o K-means & 0.703 & 0.171 & 0.437 \\
    \quad - w/o MB & 0.698 & 0.170 & 0.434 \\
    \quad - w/o AE & 0.343 & 0.114 & 0.229 \\
    \quad - w/o GN & 0.384 & 0.121 & 0.253 \\
    \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
\end{minipage}%
\hfill 
\begin{minipage}[ht]{0.45\columnwidth}
    \vskip 0.2in
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics[width=\columnwidth]{./imgs/noise_.pdf}
    }
    \makeatletter\def\@captype{figure}\makeatother
     \vskip -0.1in
    \caption{The impact of $\sigma$ on  AudioCaps benchmark.}
    \label{fig: noise_}
    \vskip -0.2in
\end{minipage}
\end{minipage}

\section{Conclusion}
In this paper, we propose MATS, an audio-language multimodal LLM tuned solely on text data. 
Leveraging an efficient text-only training strategy, MATS establishes the semantic connection between the LLM and audio-language contrastive mode, substantially reducing data and training costs while improving zero-shot performance. To address the modality gap issue, we further introduce a modal-transfer strategy, Santa, which effectively balances audio embeddings and augmented language embeddings. Comprehensive experiments show that MATS achieves performance comparable to, and in some cases surpassing, LALMs trained on large-scale audio-language paired datasets.

\textbf{Limitations.} 
MATS is currently focused on the audio domain. In the future, we intend to extand MATS by incorporating  additional modalities, such as visual data and 3D point clouds, aiming to develop a more versatile multimodal model capable of handling a wider range of modalities, all while relying exclusively on text-based supervision.


\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}

\section{Proof of Theorem 3.1} \label{appendix:proof}
In this section, we provide a proof for \cref{the:thoery}. 
% \begin{proof}
% $$
% \begin{array}{ccc}
% R_a(h) & = & R_a(h) - R_t(h) + R_t(h) - \hat{R_t}(h) + \hat{R_t}(h) \\
%  & \leq & \big| R_a(h) - R_t(h) \big| \\
%  & & + \big| R_t(h) - \hat{R_t}(h) \big| + \hat{R_t}(h) \\
% \end{array}
% $$
% $$
% \begin{array}{ccc}
% \big| R_a(h) - R_t(h)\big| & = & \big| \mathbb{E}_{(z_a, y) \sim p_a(z_a, y)} | h(z_a) - y | \\
% & & - \mathbb{E}_{(z_t, y) \sim p_t(z_t, y)} | h(z_t) - y | \big| \\
% & = & \big| \sum_k p_a(y) \cdot \\
% & & \mathbb{E}_{(z_a) \sim p_a(z_a|y)} | h(z_a) - y | \\
% & & - \sum_k p_t(y) \cdot \\
% & & \mathbb{E}_{(z_t) \sim p_t(z_t|y)} | h(z_t) - y | \big|
% \end{array}
% $$
% Assuming that the class distributions for training and testing are \textbf{consistent}, we can obtain $p_a(y) = p_t(y)$
% $$
% \begin{array}{ccc}
% \big| R_a(h) - R_t(h)\big| & = & \big| \sum_k p_a(y) \cdot \\
% & & ( \mathbb{E}_{(z_a) \sim p_a(z_a|y)} | h(z_a) - y | \\
% & & - \mathbb{E}_{(z_t) \sim p_t(z_t|y)} | h(z_t) - y | )\big| \\
% & \leq & \max\limits_k \big| \mathbb{E}_{(z_a)\sim p_a(z_a|y)}|h(z_a) - y| \\
% & & -\mathbb{E}_{(z_t)\sim p_t(z_t|y)}|h(z_t) - y|\big| \\
% & \leq &  \max\limits_k \text{disc}_{L_1}(p_a(z_a|y), p_t(z_t|y))
% \end{array}
% $$
% $\big| R_t(h) - \hat{R_t}(h) \big|$ follows by a standard application Vapnik-Chervonenkis \citep{vapnik1998statistical} theory to bound the true $R_t(h)$ by its empirical estimate $\hat{R_t(h)}$. Namely, if the $D_t$ is a $n_t$-size .i.i.d. samples, then with probability exceeding $1-\delta$, 
% $$
% \big| R_t(h) - \hat{R_t}(h) \big| \leq \sqrt{\frac{8}{n_t} \left( 2d \log{\sqrt{2n_t K}} + \log{\frac{2}{\delta}} \right)}
% $$
% \label{proof: the gap of modality}
% \end{proof}
\begin{proof}
Let $R_{\mathrm{tr}}(h) = \mathbb{E}_{\left(z^t, y\right) \sim p_{\mathcal{T}}(z^t, y)}\left|h(z^t) - y\right| $ denote the generalization error on the train set. According to triangle inequality, we can drive that: 
\begin{equation}
\begin{array}{ccc}
R_{\mathrm{te}}(h) & = & R_{\mathrm{te}}(h) - R_{\mathrm{tr}}(h) + R_{\mathrm{tr}}(h) - \hat{R}_{\mathrm{tr}}(h) + \hat{R}_{\mathrm{tr}}(h) \\
& & \\
 & \leq & \big| R_{\mathrm{te}}(h) - R_{\mathrm{tr}}(h) \big| + \big| R_{\mathrm{tr}}(h) - \hat{R}_{\mathrm{tr}}(h) \big| + \hat{R}_{\mathrm{tr}}(h) \\
\end{array}
\end{equation}
\begin{equation}
\begin{array}{ccl}
\big| R_{\mathrm{te}}(h) - R_{\mathrm{tr}}(h)\big| & = & \big| \mathbb{E}_{(z^a, y) \sim p_{\mathcal{A}}(z^a, y)} | h(z^a) - y | - \mathbb{E}_{(z^t, y) \sim p_\mathcal{T}(z^t, y)} | h(z^t) - y | \big| \\
& & \\
& = & \big| \sum_v^V p_\mathcal{A}(y) \cdot \mathbb{E}_{(z^a) \sim p_\mathcal{A}(z^a|y)} | h(z^a) - y | - \sum_v^V p_\mathcal{T}(y) \cdot  \mathbb{E}_{(z^t) \sim p_\mathcal{T}(z^t|y)} | h(z^t) - y | \big|
\end{array}
\end{equation}
Assuming that the class distributions for training and testing are consistent, we can obtain $p_\mathcal{A}(y) = p_\mathcal{T}(y)$, then we can achieve that:
\begin{equation}
\begin{array}{ccl}
\big| R_{\mathrm{te}}(h) - R_{\mathrm{tr}}(h)\big| & = & \big| \sum_v^V p_\mathcal{A}(y) \cdot ( \mathbb{E}_{(z^a) \sim p_\mathcal{A}(z^a|y)} | h(z^a) - y | - \mathbb{E}_{(z^t) \sim p_\mathcal{T}(z^t|y)} | h(z^t) - y | )\big| \\
& & \\
& \leq & \max\limits_V \big| \mathbb{E}_{(z^a)\sim p_\mathcal{A}(z^a|y)}|h(z^a) - y| - \mathbb{E}_{(z^t)\sim p_\mathcal{T}(z^t|y)}|h(z^t) - y|\big| \\
& & \\
& \leq &  \max\limits_V \text{disc}_{L_1}(p_\mathcal{A}(z^a|y), p_\mathcal{T}(z^t|y))
\end{array}  
\end{equation}
$\big| R_{\mathrm{tr}}(h) - \hat{R}_{\mathrm{tr}}(h) \big|$ follows by a standard application Vapnik-Chervonenkis \citep{vapnik1998statistical} theory to bound the true $R_{\mathrm{tr}}(h)$ by its empirical estimate $\hat{R}_{\mathrm{tr}(h)}$. Namely, if the $D_{\mathrm{tr}}$ is a $N$-size .i.i.d. samples, then with probability exceeding $1-\delta$, 
\begin{equation}
\big| R_{\mathrm{tr}}(h) - \hat{R}_tr(h) \big| \leq \sqrt{\frac{8}{N} \left( 2d \log{\sqrt{2N V}} + \log{\frac{2}{\delta}} \right)}
\end{equation}
At the last, we can achieve that:
\begin{equation}
\begin{split}
R_{te}\left(h\right)  & \leq  \widehat{R}_{tr}\left(h\right)  + \max\limits_v \left(\mathrm{disc}_{L_1}\left(p_{\mathcal{A}}\left(z^a | y\right), p_{\mathcal{T}}\left(z^t | y\right)\right) \right) + \sqrt{\frac{8}{N} \left( 2d \log{\sqrt{2N V}} + \log{\frac{2}{\delta}} \right)}.
\end{split}
\end{equation}
\label{proof: the gap of modality}
\end{proof}

\section{Dataset} \label{appendix:dataset}
\subsection{The train dataset of MATS-GPT2 and MATS-LLaMA}
\cref{tab:train dataset} provides a detailed overview of the training dataset, \texttt{AudioTIA}. 
In this dataset, the inputs are primarily composed of descriptions of audio contents and textual instrucrions. \texttt{AudioTIA} is divided into close-ended and open-ended tasks, focusing on two audio types: Sound and Music. Specifically, the close-ended dataset, consisting of about 1.5M samples, primarily  includes classification tasks ($58.7\%$), captioning tasks ($40\%$), and the remaining portion comprises simple audio question-answering tasks. In contrast, the open-ended dataset, comprising  about 3.8M samples, focuses on complex audio question-answering  tasks. The close-ended dataset is designed to establish the model’s foundational audio perception capabilities, and the open-ended dataset focuses on enhancing the model’s capacity for advanced audio understanding and reasoning.

% ensures our model’s fundamental audio perception capabilities, while the open-ended dataset of about 3.8M samples enhances our model’s audio understanding and reasoning abilities.
\begin{table}[H]
    \small
    \centering
    \caption{The statics of the \texttt{AudioTIA} dataset.}
    \vskip 0.15in
    % \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccll}
        \toprule
        \multicolumn{5}{l}{\textbf{Close-Ended Audio Text Instruction Answering Data ($\sim$1.5M)}} \\
        \toprule
        Audio Type & Task & Datasets & \#Samples & Percentage \\
        \midrule
        \multirow{10}{*}{Sound} & \multirow{7}{*}{CLS} & AudioSet  \citep{7952261}, FSD50k  \citep{fonseca2022fsd50kopendatasethumanlabeled}, & \multirow{7}{*}{$\sim$385K} & \multirow{7}{*}{$\sim$25.6\%} \\
        & & VGGSound (VGG) \citep{chen2020vggsoundlargescaleaudiovisualdataset}, DCASE  \citep{mesaros:hal-01627981}, & & \\
        & & ECS-50 \citep{10.1145/2733373.2806390}, UrbanSound8K(US8K)  \citep{10.1145/2647868.2655045} & & \\
        & & TUT Acoustic Scenes 2017(TUT) \citep{7760424}, & &\\
        & & Sound Events for Surveillance Applications(SESA) \citep{spadini2019sound} & &\\
        & & VS \citep{Gong_2022}, CREMA-D  \citep{cao2014crema} &  \\
        & & RAVDESS  \citep{pub.1104022894} & &\\
        \cline{2-5} \rule{0pt}{2ex}
        & \multirow{2}{*}{CAP} & WavCaps  \citep{Mei_2024}, Macs  \citep{8ced9ba43cbd49c1acd288d151565342}, & \multirow{2}{*}{$\sim$518K} & \multirow{2}{*}{$\sim$34.5\%}\\
        & & AudioCaps  \citep{kim-etal-2019-audiocaps}, Clotho-v2  \citep{drossos2019clothoaudiocaptioningdataset} & &\\
        \cline{2-5} \rule{0pt}{2ex}
        & AQA & Clotho-AQA  \citep{lipping2022clothoaqacrowdsourceddatasetaudio} & $\sim$20K & $\sim$1.3\%\\
        \midrule
        \multirow{4}{*}{Music} & \multirow{2}{*}{CLS} & GTZAN  \citep{Sturm_2014}, Beijing Opera (BJO) \citep{6853981}, & \multirow{2}{*}{$\sim$497K} & \multirow{2}{*}{$\sim$ 33.1\%} \\
        & & OpenAQA  \citep{ghosh2024gamalargeaudiolanguagemodel} & & \\ 
        \cline{2-5} \rule{0pt}{2ex}
        & \multirow{2}{*}{CAP} & OpenAQA%\tablefootnote{The dataset related to Music introduced by GAMA is based on OpenAQA released by LTU.}
        \citep{ghosh2024gamalargeaudiolanguagemodel}, MusicCaps  \citep{agostinelli2023musiclmgeneratingmusictext}, & \multirow{2}{*}{$\sim$30K} & \multirow{2}{*}{$\sim$5.5\%} \\
        & & Song Describer dataset (SDD) \citep{manco2023songdescriberdatasetcorpus} &  \\ 
        \toprule
        % \hline
        %\multirow{2}{*}{Speech} & \multirow{2}{*}{CLS} & VS  \citep{Gong_2022}, CREMA-D  \citep{cao2014crema} & \multirow{2}{*}{$\sim$32 K} \\
        %& & RAVDESS  \citep{pub.1104022894} & \\ \hline
        \multicolumn{4}{l}{\textbf{Open-Ended Audio Text Instruction Answering Data ($\sim$3.8M)}} \\
        \toprule
        Sound & AQA & OpenAQA \citep{gong2024listenthinkunderstand} & $\sim$2547K & $\sim$66.8\% \\
        \midrule
        Music & AQA & MusicQA \citep{10447027} & $\sim$1207K & $\sim$33.2\%\\
        \bottomrule
    \end{tabular}
    % }
    \label{tab:train dataset}
    %\vskip -0.1in
\end{table}

\subsection{The train dataset of MATS-Audio} \label{appendix: train set of MATS-Audio}
\cref{tab:train dataset audio} presents the training dataset of MATS-Audio, referred to as \texttt{AudioAIA}. The data format for \texttt{AudioAIA} is (\texttt{audio}, \texttt{instruction}, \texttt{answer}), which differs from the input format of \texttt{AudioTIA} with (\texttt{text}, \texttt{instruction}, \texttt{answer}). The \texttt{AudioAIA} dataset is derived from \texttt{AudioTIA}, excluding any subsets used exclusively as test data. Overall, the \texttt{AudioAIA} dataset comprises approximately 1.5M samples for close-ended tasks and 2.6M samples for open-ended tasks.

% being the input format. Specifically, the input data format of \texttt{AudioTIA} is (\texttt{text}, \texttt{instruction}), whereas that of \texttt{AudioAIA} is (\texttt{audio}, \texttt{instruction}). The dataset included in \texttt{AudioAIA} is derived from \texttt{AudioTIA} but excludes datasets that are only used for test sets. It can be summarized as consisting of 1.5 M close-ended datasets and 2.6 M open-ended datasets.
\begin{table*}[h]
    \small
    \centering
    \caption{The statics of the \texttt{AudioAIA} dataset.}
    \vskip 0.15in
    \begin{tabular}{lcccll}
        \toprule
        \multicolumn{5}{l}{\textbf{Close-Ended Audio Instruction Answering Data ($\sim$1.5M)}} \\
        \toprule
        Audio Type & Task & Datasets & \#Samples & Percentage \\
        \midrule
        \multirow{7}{*}{Sound} & \multirow{4}{*}{CLS} & AudioSet  \citep{7952261}, FSD50k  \citep{fonseca2022fsd50kopendatasethumanlabeled}, & \multirow{4}{*}{$\sim$331K} & \multirow{4}{*}{$\sim$21.9\%} \\
        & & VGGSound (VGG) \citep{chen2020vggsoundlargescaleaudiovisualdataset}, DCASE  \citep{mesaros:hal-01627981}, & & \\
        & & TUT Acoustic Scenes 2017(TUT) \citep{7760424}, & &\\
        & & Sound Events for Surveillance Applications(SESA) \citep{spadini2019sound} & &\\
        \cline{2-5} \rule{0pt}{2ex}
        & \multirow{2}{*}{CAP} & WavCaps  \citep{Mei_2024}, Macs  \citep{8ced9ba43cbd49c1acd288d151565342}, & \multirow{2}{*}{$\sim$517K} & \multirow{2}{*}{$\sim$34.2\%}\\
        & & AudioCaps  \citep{kim-etal-2019-audiocaps}, Clotho-v2  \citep{drossos2019clothoaudiocaptioningdataset} & &\\
        \cline{2-5} \rule{0pt}{2ex}
        & AQA & Clotho-AQA  \citep{lipping2022clothoaqacrowdsourceddatasetaudio} & $\sim$6.1K & $\sim$0.4\%\\
        \midrule
        \multirow{2}{*}{Music} & CLS & OpenAQA  \citep{ghosh2024gamalargeaudiolanguagemodel} & $\sim$630K & $\sim$41.7\% \\ 
        \cline{2-5} \rule{0pt}{2ex}
        & \multirow{1}{*}{CAP} & OpenAQA%\tablefootnote{The dataset related to Music introduced by GAMA is based on OpenAQA released by LTU.}
        \citep{ghosh2024gamalargeaudiolanguagemodel}, MusicCaps  \citep{agostinelli2023musiclmgeneratingmusictext}, & \multirow{1}{*}{$\sim$28.3K} & \multirow{1}{*}{$\sim$1.9\%} \\
        \toprule
        % \hline
        %\multirow{2}{*}{Speech} & \multirow{2}{*}{CLS} & VS  \citep{Gong_2022}, CREMA-D  \citep{cao2014crema} & \multirow{2}{*}{$\sim$32 K} \\
        %& & RAVDESS  \citep{pub.1104022894} & \\ \hline
        \multicolumn{4}{l}{\textbf{Open-Ended Audio Instruction Answering Data ($\sim$2.6M)}} \\
        \toprule
        Sound & AQA & OpenAQA \citep{gong2024listenthinkunderstand} & $\sim$2467K & $\sim$95.4\% \\
        \midrule
        Music & AQA & MusicQA \citep{10447027} & $\sim$118K & $\sim$4.6\%\\
        \bottomrule
    \end{tabular}
    \label{tab:train dataset audio}
    %\vskip -0.1in
\end{table*}

\subsection{The test benchmark of MATS}
\cref{tab:test benchmark} provides a detailed overview of the test benchmark and corresponding evaluation metrics. The benchmark covers a range of audio event classification tasks, such as ESC-50 \citep{10.1145/2733373.2806390} (single-label classification), FSD50K \citep{fonseca2022fsd50kopendatasethumanlabeled} (multi-label classification), DCASE \citep{mesaros:hal-01627981}, TUT \citep{7760424}, VGG \citep{chen2020vggsoundlargescaleaudiovisualdataset}, US8K \citep{10.1145/2647868.2655045}, GTZAN \citep{Sturm_2014} and BJO \citep{6853981}, as well as audio captionning tasks like AudioCaps \citep{kim-etal-2019-audiocaps} and Clotho-v2 \citep{drossos2019clothoaudiocaptioningdataset}. It also encompasses music captioning tasks including MusicCaps \citep{agostinelli2023musiclmgeneratingmusictext}, simple QA tasks on ClothoAQA, and complex QA tasks such as AIR-Bench Chat \citep{yang2024airbenchbenchmarkinglargeaudiolanguage} and  MMAU \citep{sakshi2024mmaumassivemultitaskaudio} benchmarks.
% It covers tests on Sound and Music types, including audio event classification tasks such as ESC-50 \citep{10.1145/2733373.2806390} (single classification), FSD50K \citep{fonseca2022fsd50kopendatasethumanlabeled} (multiple classification), and others; audio captioning tasks like AudioCaps \citep{kim-etal-2019-audiocaps} and Clotho-v2 \citep{drossos2019clothoaudiocaptioningdataset}; music captioning tasks such as MusicCaps \citep{agostinelli2023musiclmgeneratingmusictext}; simple QA tasks (e.g., ClothoAQA); and complex QA tasks, including AIR-Bench Chat \citep{yang2024airbenchbenchmarkinglargeaudiolanguage} and the MMAU \citep{sakshi2024mmaumassivemultitaskaudio} benchmark. In the results table, the best result is highlighted in bold font, while the second-best result is indicated with underlining. These evaluations demonstrate that MATS possesses robust audio understanding and reasoning capabilities.
\begin{table}[h]
    \small
    \centering
    \caption{The test benchmarks and evaluation metrics are presented. B-ACC represents binary accuracy. \textbf{Note:} All evaluation metrics are designed such that higher values indicate better performance.}
    \vskip 0.15in
    % \resizebox{\linewidth}{!}{
    \begin{tabular}{cccc}
        \toprule
        \multicolumn{4}{l}{\textbf{Close-Ended Audio Instruction Answering Test Data}} \\
        \toprule
        Audio Type & Task & Benchmark & Metric $\uparrow$ \\
        \midrule
        \multirow{9}{*}{Sound} & CLS & ECS-50 \citep{10.1145/2733373.2806390} & ACC \\
        & CLS & FSD50K \citep{fonseca2022fsd50kopendatasethumanlabeled} & mAP \\
        & CLS & DCASE \citep{mesaros:hal-01627981} & ACC \\
        & CLS & TUT \citep{7760424} & ACC \\
        & CLS & VGG \citep{chen2020vggsoundlargescaleaudiovisualdataset} & ACC \\
        & CLS & US8K \citep{10.1145/2647868.2655045} & ACC \\
        & CAP & AudioCaps  \citep{kim-etal-2019-audiocaps} & $\text{CIDEr}\vert \text{SPICE}\vert \text{SPICEr}$ \\
        & CAP &  Clotho-v2 \citep{drossos2019clothoaudiocaptioningdataset} & $\text{CIDEr}\vert \text{SPICE}\vert \text{SPICEr}$ \\
        & AQA & Clotho-AQA \citep{lipping2022clothoaqacrowdsourceddatasetaudio} & $\text{ACC}\vert \text{B-ACC}$ \\
        \midrule
        \multirow{3}{*}{Music} & CLS & GTZAN \citep{Sturm_2014} & ACC \\
        & CLS & BJO \citep{6853981} & ACC \\
        & CAP & MusicCaps \citep{agostinelli2023musiclmgeneratingmusictext} & $\text{BLUE4}\vert \text{ROUGH-L}$ \\
        % \cline{2-5}
        %& \multirow{3}{*}{Speech} & CLS & RAVDESS \citep{pub.1104022894} & ACC \\
        %& & CLS & CREMA-D\cite{cao2014crema} & ACC \\
        %& & CLS & Vocal Sound(VS) \citep{Gong_2022} & ACC\\
        % \hline
        \toprule
        \multicolumn{4}{l}{\textbf{Open-Ended Audio Instruction Answering Test Data}} \\
        \toprule
     \multirow{2}{*}{Sound} & AQA & AIR-Bench Chat \citep{yang2024airbenchbenchmarkinglargeaudiolanguage} & GPT4 \\
    & AQA & MMAU \citep{sakshi2024mmaumassivemultitaskaudio} & ACC \\
    \midrule
    \multirow{2}{*}{Music} & AQA & AIR-Bench Chat \citep{yang2024airbenchbenchmarkinglargeaudiolanguage} & GPT4 \\
    & AQA & MMAU \citep{sakshi2024mmaumassivemultitaskaudio} & ACC \\
    % \cline{2-5}
   % & \multirow{2}{*}{Speech} & AQA & AIRBench \citep{yang2024airbenchbenchmarkinglargeaudiolanguage} & GPT4 \\
    % & & AQA & MMAU \citep{sakshi2024mmaumassivemultitaskaudio} & ACC \\
    % \cline{2-5}
    % & Mixed & AQA & AIRBench \citep{yang2024airbenchbenchmarkinglargeaudiolanguage} & GPT4 \\
    \bottomrule
    \end{tabular}
    % }
    \label{tab:test benchmark}
    %\vskip -0.1in
\end{table}

\subsection{The design for generating audio descriptions in close-ended tasks.} \label{appendix:pipeline}

In this part, we describe the pipeline and prompt used with ChatGPT \citep{openai2024gpt4technicalreport} to generate audio descriptions for various classification task datasets, including ESC-50 \citep{10.1145/2733373.2806390}, FSD50K \citep{fonseca2022fsd50kopendatasethumanlabeled}, VGG \citep{chen2020vggsoundlargescaleaudiovisualdataset}, ECS-50 \citep{10.1145/2733373.2806390}, US8K \citep{10.1145/2647868.2655045}, TUT \citep{7760424}, SESA \citep{spadini2019sound}, VS \citep{Gong_2022}, CREMA-D \citep{cao2014crema}, and RAVDESS \citep{pub.1104022894} for sound classification, and GTZAN \citep{Sturm_2014} and BJO \citep{6853981} for music classification. \cref{pipeline and prompt} provides an overview of the pipeline. The pipeline involves supplying GPT-4o-mini with a set of example audio descriptions corresponding to a few labels, followed by a prompt to guide the generation process. GPT-4o-mini then produces descriptions for all provided labels. The final set of generated descriptions, paired with their corresponding labels, serves as input for MATS.

% illustrates the pipeline for generating these audio descriptions, along with examples of the corresponding prompts. The pipeline involves providing examples of audio descriptions corresponding to a few labels and inputting them into GPT-4o-mini along with a prompt. GPT-4o-mini then generates descriptions for the remaining labels. Finally, the generated descriptions, along with their corresponding labels, are used as input for MATS.
\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{./imgs/pipeline}}
\caption{Pipeline And Prompt of generating audio descriptions.}
\label{pipeline and prompt}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{The instruction templates for training MATS} \label{appendix:instruction}
Recent studies \citep{deshmukh2024pengiaudiolanguagemodel, tang2024salmonngenerichearingabilities} have demonstrated that instruction fine-tuning enhances the instruction-following abilities and comprehension capabilities of audio-language multimodal models. Building on this insight, we adopt instruction fine-tuning when training MATS. \cref{tab:Instruction examples} presents the instruction templates designed for all tasks in our study. % Additionally, to further help the LLM understand each task, we have added special tokens to each corresponding dataset.
\begin{table*}[h]
    \small
    \centering
    \caption{The instruction template of every task. CLS: Audio Event Classification; CAP: Audio Captioning; AQA: Audio Question Answer.}
    \label{tab:Instruction examples}
    \vskip 0.15in
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccc}
        \toprule
        Task & Input text & Input instruction format & Output answer format \\ 
        \toprule
        CLS & \texttt{GPT generated description} & "Classify events in the audio clip." & \texttt{label a}, ... \\ 
        \midrule
        Sound CAP & \texttt{caption} &  "Generate audio caption." & \texttt{caption} \\ 
        \midrule
        Music CAP & \texttt{caption} &  "Generate music caption." & \texttt{caption} \\ 
        \midrule
        AQA & \texttt{generated caption} & \texttt{Question} & \texttt{answer} \\ 
        \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
\end{table*}

\section{More Experimental Results} \label{appendix: experiment}
\subsection{Visualization Analysis on Audio-Language Modality Gap}
\cref{fig: modal_transfer_visual} presents a visualization analysis of the audio-language modality gap. As shown in \cref{fig: modal_transfer_visual} $a)$, a noticeable gap \citep{liang2022mind} remains between audio embeddings and language embeddings of CLAP. 
 In \cref{fig: modal_transfer_visual} $b)$, introducing Gaussian noise reduces the embedding gap to some extent; however, the language and audio embeddings remain distributed near opposite ends. Similarly, in \cref{fig: modal_transfer_visual} $c)$, the memory-based method shows a comparable effect to $b)$. As previously noted, this method completely replaces audio embeddings with language embeddings, resulting in the processed audio features losing audio-specific information and becoming overly concentrated, thereby reducing the model’s generalization ability. In contrast, \cref{fig: modal_transfer_visual} $d)$ illustrates that our \textbf{Santa} further integrates the two modalities into a more unified and cohesive  distribution.

\subsection{Hyperparameter Analysis} \label{appendix: hyper}
\textbf{The impact of the different temperature $\tau$.} \
In \cref{tab:temperature exp}, we analyze the impact of different $\tau$ values on the model’s performance. Our model achieves the best performance when $\tau=0.1$. When $\tau$ is too small, our algorithm tends to select only the most similar embedding, which limits the representational power of language embeddings and leads to a decrease in model performance. Conversely, when $\tau$ is too large, the model may capture irrelevant augmented language embeddings with low relevance.

\begin{table}[h]
\centering
\begin{minipage}{0.5\linewidth}
    \makeatletter\def\@captype{table}\makeatother
    \small
    \centering
    \caption{The performance of different $\tau$ on AudioCaps.}
    \vskip 0.15in
    \begin{tabular}{lccccccc}
    \toprule
        $\tau$ & 1/150 & 1/80 & 1/40 & 1/20 & 1/10 & 1 \\
        \midrule
        CIDEr & 0.731 & 0.734 & 0.732 & 0.734 & \textbf{0.735} & 0.731 \\
        \bottomrule
    \end{tabular}
    \label{tab:temperature exp}
    \vskip -0.1in
% \end{table}
\end{minipage}
\hfill
% \begin{table}[H]
\begin{minipage}{0.45\linewidth}
    \makeatletter\def\@captype{table}\makeatother
    \small
    \centering
    \caption{The performance of different $L$ on AudioCaps.}
    \vskip 0.15in
    % \resizebox{}{}{}
    \begin{tabular}{lccccc}
    \toprule
        $L$ & 4 & 8 & 16 & 32 & 64 \\
        \midrule
        CIDEr & 0.728 & 0.730 & 0.729 & \textbf{0.735} & 0.734 \\
        \bottomrule
    \end{tabular}
    \label{tab: exp_l}
    \vskip -0.1in
\end{minipage}
\end{table}

\textbf{The impact of different top $L$.} \
\cref{tab: exp_l} presents the performance of our method with different values of $L$. When $L$ is too large, %a phenomenon similar to that observed when $\tau$ is too large occurs, where 
the model may capture irrelevant augmented language embeddings with low relevance. Conversely, as $L$ approaches $1$, %it behaves similarly to the smallest $\tau$, 
the model loses the benefits of multiple language representations. We set $L = 32$ to strike a balance.

\textbf{The impact of the different balancing parameter $\lambda$.} \
In this section, we conducted experiments with different values of $\lambda$, %keeping all other parameters fixed, 
as shown in \cref{fig:lambda}. The results clearly demonstrate that the best performance is achieved when both language and audio are integrated simultaneously, and the performance is relatively insensitive to the variations in $\lambda$. However, when audio is entirely removed, the model’s performance drops significantly, further validating the effectiveness of the balancing strategy within the Santa mechanism.
\begin{figure}[H]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.3\columnwidth]{./imgs/lambda_.pdf}}
\caption{The effect of different $\lambda$ on AudioCaps.}
\label{fig:lambda}
\end{center}
\vskip -0.2in
\end{figure}

% \begin{table}[H]
%     \small
%     \centering
%     \caption{The performance of different $L$ on AudioCaps.}
%     \vskip 0.15in
%     \begin{tabular}{lccccc}
%     \toprule
%         $L$ & 4 & 8 & 16 & 32 & 64 \\
%         \midrule
%         CIDEr & 0.728 & 0.730 & 0.729 & \textbf{0.735} & 0.734 \\
%         \bottomrule
%     \end{tabular}
%     \label{tab: exp_k}
%     \vskip -0.1in
% \end{table}

\textbf{Hyper-Variations in LoRA.} \
\cref{tab: exp_rank} and \cref{tab: exp_scale} present the performance variations of MATS on the AudioCaps test benchmark under different LoRA ranks and scaling factors. When the rank or scaling factor is too small, the model undergoes insufficient updates, leading to underfitting. Conversely, excessively large ranks or scaling factors result in overly aggressive updates, increasing model complexity and causing overfitting.

% \cref{tab: exp_rank} and \cref{tab: exp_scale} respectively show the performance variations of MATS on the AudioCaps test benchmark under different LoRA ranks and scaling factors. When the rank or scaling factors are too small, the model’s updates are insufficient, leading to underfitting. On the other hand, when the rank or scaling factors are too large, the model’s updates become excessive, increasing its complexity and resulting in overfitting.

\begin{table}[h]
\begin{minipage}{0.495\linewidth}
    \small
    \centering
    \makeatletter\def\@captype{table}\makeatother
    \caption{The performance of MATS with different ranks of LoRA on AudioCaps.}
    \vskip 0.15in
    \begin{tabular}{lccc}
    \toprule
        Rank & 4 & 8 & 16 \\
        \midrule
        CIDEr(\%) & 0.706 & \textbf{0.735} & 0.702 \\
        \bottomrule
    \end{tabular}
    \label{tab: exp_rank}
    \vskip -0.1in
\end{minipage}
% \end{table}
% \begin{table}[h] 
\hfill
\begin{minipage}{0.495\linewidth}
    \small
    \centering
    \makeatletter\def\@captype{table}\makeatother
    \caption{The performance of MATS with different scaling factors of LoRA on AudioCaps.}
    \vskip 0.15in
    \begin{tabular}{lccc}
    \toprule
        $\frac{\alpha}{Rank}$ & 2 & 4 & 8 \\
        \midrule
        CIDEr(\%) & 0.674 & \textbf{0.735} & 0.682 \\
        \bottomrule
    \end{tabular}
    \label{tab: exp_scale}
    \vskip -0.1in
\end{minipage}
\end{table}

\textbf{The impact of the different number of $\mathcal{M}$.} \
\cref{tab: exp_M} shows that the performance of our model initially increases and then decreases as the size of the memory bank $\mathcal{M}$ grows. The initial improvement is attributed to the increasing richness of the provided language embedding information. However, as the memory bank continues to expand, the fixed value of $L$ limits the model’s ability to effectively utilize diverse language embeddings.
\begin{table}[h] 
    \small
    \centering
    \caption{The performance of MATS with the different number of memory bank $\mathcal{M}$ on AudioCaps.}
    \vskip 0.15in
    \begin{tabular}{lccccccccccc}
    \toprule
        $M$ & 1000 & 2000 & 3000 & 3500 & 4000 & 4500 & 5000 & 5172 & 5500 & 6000 & 7000 \\
        \midrule
        CIDEr(\%) & 0.710 & 0.700 & 0.714 & 0.716 & 0.719 & 0.705 & 0.708 & \textbf{0.735} & 0.705 & 0.700 & 0.700 \\
        \bottomrule
    \end{tabular}
    \label{tab: exp_M}
    \vskip -0.1in
\end{table}

\textbf{The impact of k-means cluster size $K$.} \
\cref{tab:clutsers} investigates the impact of the k-means cluster size $K$ in the Santa mechanism. 
As shown, regardless of the cluster size, the model consistently outperforms noise-based and memory-based methods, indicating that our Santa mechanism is robust to the choice of the k-means cluster number. We set $K=100$ in this work. 
\begin{table}[t]
% \vskip 0.15in
\begin{minipage}{0.495\linewidth}
    \centering
    \makeatletter\def\@captype{table}\makeatother
    \caption{Ablation study of MATS-LLaMA on the cluster number  $K$ in the Santa mechanism, evaluated on AudioCaps benchmark.}
    \vskip 0.15in
    \label{tab:clutsers}
    \begin{small}
    \begin{tabular}{l | ccc}
    \toprule
     $K$ & CIDEr & SPICE & SPIDEr \\
    \toprule
    10 & 0.731 & 0.170 & 0.450 \\
    30 & 0.733 & 0.170 & 0.452 \\
    60 & 0.734 & 0.171 & 0.452 \\
    100 & \textbf{0.735} & \textbf{0.171} & \textbf{0.453} \\
    150 & 0.731 & 0.170 & 0.450 \\
    \bottomrule
    \end{tabular}
    \end{small}
    \vskip -0.1in
\end{minipage}
\hfill
\begin{minipage}{0.495\linewidth}
    \centering
    \makeatletter\def\@captype{table}\makeatother
    \caption{Ablation Study of the number of Transformer layers on the AudioCaps benchmark.}
    \vskip 0.15in
    \label{tab:layers}
    \begin{small}
    \begin{tabular}{lccc}
    \toprule
     number & CIDEr & SPICE & SPIDEr \\
    \toprule
    4 & 0.696 & 0.166 & 0.431 \\
    8 & \textbf{0.735} & 0.171 & \textbf{0.453} \\
    16 & 0.653 & \textbf{0.174} & 0.414 \\
    \bottomrule
    \end{tabular}
    \end{small}
    \vskip -0.1in
\end{minipage}
\end{table}

\textbf{The number of Mapper'layers.} \
We further evaluated the impact of varying the number of Transformer layers on the model’s performance on  AudioCaps test set. As shown in \cref{tab:layers}, the model achieves optimal performance with 8 layers. When the number of layers is fewer than 8, 
the model’s capacity is insufficient for effective learning, leading to underfitting. Conversely, increasing the number of layers beyond 8 results in overfitting, thereby reducing generalization ability. Based on these findings, we adopt an 8-layer Transformer in this work.

% We further evaluated how varying the number of Transformer layers impacts the model’s performance on the AudioCaps test set. As illustrated in \cref{tab:layers}, the optimal performance is achieved when the number of layers is set to 8. With fewer than 8 layers, the model’s complexity becomes insufficient to match the size of the training dataset $N$, resulting in underfitting. Conversely, increasing the number of layers beyond 8 causes the model to overfit, negatively affecting its generalization ability.
% \begin{table}[H]
% \centering
% \caption{Ablation Study of the number of Transformer layers on the AudioCaps Dataset.}
% \vskip 0.15in
% \label{tab:layers}
% \begin{tabular}{lccc}
% \toprule
%  number & CIDEr & SPICE & SPIDEr \\
% \toprule
% 4 & 0.696 & 0.166 & 0.431 \\
% 8 & \textbf{0.735} & 0.171 & \textbf{0.453} \\
% 16 & 0.653 & \textbf{0.174} & 0.414 \\
% \bottomrule
% \end{tabular}
% \vskip -0.1in
% \end{table}

\subsection{Some Instances of MATS-LLaMA}
% In \cref{fig:example of audio classification}, \cref{fig:example of audio captioning}, \cref{fig:example of music captioning}, \cref{fig:example of simple QA}, and \cref{fig:example of complex QA}, we respectively showcase examples of MATS applied to audio classification, general audio captioning, music captioning, simple QA, and complex QA tasks.
In \cref{fig:examples1}, \cref{fig:examples4}, \cref{fig:examples5}, \cref{fig:examples2}, and \cref{fig:examples3}, we present examples of MATS-LLaMA and Qwen2-Audio-Instruct \citep{chu2024qwen2audiotechnicalreport} applied to tasks including complex QA, audio classification, general audio captioning, simple QA, and music captioning. In the AIR-Bench Chat and MusicCaps benchmark examples, Qwen2-Audio-Instruct exhibits significant hallucination issues, whereas MATS-LLaMA, while only partially covering the answers, does not generate incorrect or nonsensical responses. For audio event classification tasks (FSD50K, BJO, and US8K), Qwen2-Audio-Instruct struggles to generate specific responses, often providing only broad and general answers. For audio captioning examples (AudioCaps and Clotho), both Qwen2-Audio-Instruct and MATS-LLaMA can correctly generate appropriate captions.
%We argue that audio-language pairs often contain annotation errors, which can mislead audio-supervised models. While it cannot guarantee the correctness of the audio descriptions, the text-only training strategy helps mitigate such risks. 
Additionally, in the ClothoAQA example, the correct answer is `BIRD'. While Qwen2-Audio-Instruct’s response (`owl') refers to a bird, the sound does not correspond to that of an owl. These cases highlight MATS-LLaMA’s strong audio understanding and reasoning capabilities.

% while MATS-LLaMA’s responses only partially covered the answers. However, MATS-LLaMA did not generate incorrect or nonsensical responses, which we attribute to the influence of the training dataset. Compared to text-only data, audio-language pairs often contains annotation errors in the audio, which can mislead the audio-supervised model. In contrast, the text-only strategy during training effectively minimizes the risk of the model being misled by erroneous audio information. In closed-ended examples, MATS-LLaMA performs comparably to Qwen2-Audio-Instruct. Specifically, in the ClothoAQA example, although an owl is a bird, the sound is not that of an owl. These cases demonstrate that MATS-LLaMA possesses strong audio understanding and reasoning capabilities.

% \begin{figure}[!b]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{./imgs/examples.pdf}}
% \caption{The example of different tasks. a) Audio Classification tasks on FSD50K. b) Audio Captioning tasks on AudioCaps. c) Simple audio QA tasks on ClothoAQA. d) Complex audio QA tasks on AIR-Bench Chat Benchmark. e) Audio Captioning tasks on MusicCaps}
% \label{fig:examples}
% \end{center}
% % \vskip -0.2in
% \end{figure}

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{./imgs/airbench_fsd50k.pdf}}
\caption{The example of different tasks. a) Complex audio QA tasks on AIR-Bench Chat Benchmark. b) Audio Classification tasks on FSD50K.}
\label{fig:examples1}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{./imgs/airbench_bjo.pdf}}
\caption{The example of different tasks. a) Complex audio QA tasks on AIR-Bench Chat Benchmark. b) Audio Classification tasks on BJO.}
\label{fig:examples4}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{./imgs/clotho_urban.pdf}}
\caption{The example of different tasks. a) Audio Captioning tasks on Clotho. b) Audio Classification tasks on UrbanSound8K.}
\label{fig:examples5}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{./imgs/audiocaps_clothoaqa.pdf}}
\caption{The example of different tasks. a) Audio Captioning tasks on AudioCaps. b) Simple audio QA tasks on ClothoAQA.}
\label{fig:examples2}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{./imgs/musiccaps_mmau.pdf}}
\caption{The example of different tasks. a) Music Captioning tasks on MusicCaps. b) Complex audio QA tasks on MMAU Benchmark.}
\label{fig:examples3}
\end{center}
\vskip -0.2in
\end{figure}

% \begin{figure}[h]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.5\columnwidth]{./imgs/instanceFSD50k.pdf}}
% \caption{The example of Audio Classification tasks on FSD50K}
% \label{fig:example of audio classification}
% \end{center}
% \vskip -0.2in
% \end{figure}
% \begin{figure}[h]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.5\columnwidth]{./imgs/instanceAudioCaps.pdf}}
% \caption{The example of Audio Captioning tasks on AudioCaps}
% \label{fig:example of audio captioning}
% \end{center}
% \vskip -0.2in
% \end{figure}
% \begin{figure}[h]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.5\columnwidth]{./imgs/instanceMusicCaps.pdf}}
% \caption{The example of Audio Captioning tasks on MusicCaps}
% \label{fig:example of music captioning}
% \end{center}
% \vskip -0.2in
% \end{figure}
% \begin{figure}[h]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.5\columnwidth]{./imgs/intanceClothoAQA.pdf}}
% \caption{The example of Simple audio QA tasks on ClothoAQA}
% \label{fig:example of simple QA}
% \end{center}
% \vskip -0.2in
% \end{figure}
% \begin{figure}[h]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.5\columnwidth]{./imgs/instance1.pdf}}
% \caption{The example of Complex audio QA tasks on AIR-Bench Chat Benchmark}
% \label{fig:example of complex QA}
% \end{center}
% \vskip -0.2in
% \end{figure}

\end{document}

