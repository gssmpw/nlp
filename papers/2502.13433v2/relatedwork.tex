\section{Related work}
\textbf{Audio Language Models.} \ 
In the field of audio-language models, CLAP models \citep{wu2023large, elizalde2023clap} have demonstrated  remarkable capabilities  in audio discriminative tasks. However, the lack of a decoder limits their applicability  in open-ended QA scenarios. Recently, with the rapid advancements in LLMs, researchers have started integrating audio understanding into LLMs. For example, Pengi   \citep{deshmukh2024pengiaudiolanguagemodel} combines the CLAP audio encoder with GPT2  \citep{radford2019language}, and employs a transformer-based mapper for multimodal fusion, achieving  strong performance on close-ended tasks. Similarly, LTU \citep{gong2024listenthinkunderstand} incorporates a more advanced LLM, LLaMA \citep{touvron2023llamaopenefficientfoundation}, and demonstrates emerging audio comprehension and reasoning  abilities. SALMONN \citep{tang2024salmonngenerichearingabilities} utilizes a dual audio encoder consisting of a Whisper speech encoder model \citep{radford2023robust} and a BEATs \citep{chen2022beatsaudiopretrainingacoustic} audio encoder to handle speech and non-speech audio tasks effectively. Other studies have expanded instruction fine-tuning data and explored more advanced audio encoder architecture, such as GAMA \citep{ghosh2024gamalargeaudiolanguagemodel}, Qwen2-Audio \citep{chu2024qwen2audiotechnicalreport}, yielding commendable performance. 

However, these LALMs heavily rely on large-scale audio-language pairs for training, which pose significant challenges in term of data collection and training costs. 
Differently, we adopt a highly cost-efficient approach for training LALMs using text-only data, which substantially reduces the data collection and training overhead while maintaining comparable performance.

\textbf{Text-only Supervised Multimodal LLMs.} \
To alleviate resource demands, researchers have proposed zero-shot captioning frameworks, aimed at generating image/audio captions through text-only training. In the visual domain, large-scale pre-trained contrastive models like CLIP \citep{radford2021learning} align images and language into a shared vision-language embedding space. Building on CLIP, 
CapDec \citep{DBLP:conf/emnlp/NukraiMG22} trains a decoder to reconstruct text from its corresponding CLIP language embedding,  which is then used to decode CLIP image embeddings at inference. To mitigate the vision-language modality gap, CapDec injects noise into language embedding during training.  Differently, DeCap \citep{li2023decapdecodingcliplatents} leverages a memory to store CLIP language embeddings, which is  subsequently used to project visual embedding into CLIP language embedding space at inference.  In the audio domain, models such as NoAudioCaptioning \citep{deshmukh2024training}, WSAC \citep{kouzelis2023weaklysupervisedautomatedaudiocaptioning}, PromptACC \citep{zhang2024zeroshotaudiocaptioningusing}, and DRCap \citep{li2024drcapdecodingclaplatents} adopt similar strategies, replacing CLIP with CLAP to develop zero-shot audio captioning frameworks. %For example, NoAudioCaptioning and PromptACC apply noise injection during training, while WSAC and DRCap leverage language embedding memory during inference.
However, these works focus on audio captioning, lacking the flexibility to handle a diverse range of audio tasks simultaneously, and they are limited to processing only a single type of audio (sound).