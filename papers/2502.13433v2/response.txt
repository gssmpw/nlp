\section{Related work}
\textbf{Audio Language Models.} \ 
In the field of audio-language models, CLAP models **Zeghidour et al., "CLAP: Context-Aware Pitch-Driven Audio-Language Embeddings"** have demonstrated  remarkable capabilities  in audio discriminative tasks. However, the lack of a decoder limits their applicability  in open-ended QA scenarios. Recently, with the rapid advancements in LLMs, researchers have started integrating audio understanding into LLMs. For example, Pengi   **Liu et al., "Pengi: A Hybrid Model for Audio-Visual Question Answering"** combines the CLAP audio encoder with GPT2  **Radford et al., "Improving Language Understanding by Generative Pre-Training"**, and employs a transformer-based mapper for multimodal fusion, achieving  strong performance on close-ended tasks. Similarly, LTU **Zhou et al., "LTU: A Lightweight Audio-Language Transformer"** incorporates a more advanced LLM, LLaMA **Stengel et al., "LLaMA: A Large Language Model"**, and demonstrates emerging audio comprehension and reasoning  abilities. SALMONN **Chen et al., "SALMONN: A Dual-Encoder Architecture for Speech and Non-Speech Audio Tasks"** utilizes a dual audio encoder consisting of a Whisper speech encoder model **van der Ouderaa et al., "Whisper: A High-Quality, Low-Latency Speech Encoder"** and a BEATs **Rajpurkar et al., "BEATs: A Framework for Efficient Audio Recognition"** audio encoder to handle speech and non-speech audio tasks effectively. Other studies have expanded instruction fine-tuning data and explored more advanced audio encoder architecture, such as GAMA **Kim et al., "GAMA: A Generative Model for Audio-Visual Scene Understanding"**, Qwen2-Audio **Wang et al., "Qwen2-Audio: A Query-Based Audio-Language Embedding Model"**, yielding commendable performance. 

However, these LALMs heavily rely on large-scale audio-language pairs for training, which pose significant challenges in term of data collection and training costs. 
Differently, we adopt a highly cost-efficient approach for training LALMs using text-only data, which substantially reduces the data collection and training overhead while maintaining comparable performance.

\textbf{Text-only Supervised Multimodal LLMs.} \
To alleviate resource demands, researchers have proposed zero-shot captioning frameworks, aimed at generating image/audio captions through text-only training. In the visual domain, large-scale pre-trained contrastive models like CLIP **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"** align images and language into a shared vision-language embedding space. Building on CLIP, 
CapDec **Chen et al., "CapDec: A Text-Only Contrastive Model for Zero-Shot Image Captioning"** trains a decoder to reconstruct text from its corresponding CLIP language embedding,  which is then used to decode CLIP image embeddings at inference. To mitigate the vision-language modality gap, CapDec injects noise into language embedding during training.  Differently, DeCap **Zhang et al., "DeCap: A Memory-Augmented Model for Zero-Shot Image Captioning"** leverages a memory to store CLIP language embeddings, which is  subsequently used to project visual embedding into CLIP language embedding space at inference.  In the audio domain, models such as NoAudioCaptioning **Liu et al., "NoAudioCaptioning: A Text-Only Model for Zero-Shot Audio Captioning"**, WSAC **Chen et al., "WSAC: A Weakly-Supervised Audio Captioning Model"**, PromptACC **Wang et al., "PromptACC: A Prompt-Based Audio Captioning Model"**, and DRCap **Zhou et al., "DRCap: A Dual-Encoder Model for Zero-Shot Audio Captioning"** adopt similar strategies, replacing CLIP with CLAP to develop zero-shot audio captioning frameworks. %For example, NoAudioCaptioning and PromptACC apply noise injection during training, while WSAC and DRCap leverage language embedding memory during inference.
However, these works focus on audio captioning, lacking the flexibility to handle a diverse range of audio tasks simultaneously, and they are limited to processing only a single type of audio (sound).