[
  {
    "index": 0,
    "papers": [
      {
        "key": "wu2023large",
        "author": "Wu, Yusong and Chen, Ke and Zhang, Tianyu and Hui, Yuchen and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo",
        "title": "Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation"
      },
      {
        "key": "elizalde2023clap",
        "author": "Elizalde, Benjamin and Deshmukh, Soham and Al Ismail, Mahmoud and Wang, Huaming",
        "title": "Clap learning audio concepts from natural language supervision"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "deshmukh2024pengiaudiolanguagemodel",
        "author": "Deshmukh, Soham and Elizalde, Benjamin and Singh, Rita and Wang, Huaming",
        "title": "Pengi: An audio language model for audio tasks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "gong2024listenthinkunderstand",
        "author": "Yuan Gong and\nHongyin Luo and\nAlexander H. Liu and\nLeonid Karlinsky and\nJames R. Glass",
        "title": "Listen, Think, and Understand"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "touvron2023llamaopenefficientfoundation",
        "author": "Hugo Touvron and\nThibaut Lavril and\nGautier Izacard and\nXavier Martinet and\nMarie{-}Anne Lachaux and\nTimoth{\\'{e}}e Lacroix and\nBaptiste Rozi{\\`{e}}re and\nNaman Goyal and\nEric Hambro and\nFaisal Azhar and\nAur{\\'{e}}lien Rodriguez and\nArmand Joulin and\nEdouard Grave and\nGuillaume Lample",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "tang2024salmonngenerichearingabilities",
        "author": "Changli Tang and\nWenyi Yu and\nGuangzhi Sun and\nXianzhao Chen and\nTian Tan and\nWei Li and\nLu Lu and\nZejun Ma and\nChao Zhang",
        "title": "{SALMONN:} Towards Generic Hearing Abilities for Large Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "radford2023robust",
        "author": "Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya",
        "title": "Robust speech recognition via large-scale weak supervision"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "chen2022beatsaudiopretrainingacoustic",
        "author": "Sanyuan Chen and Yu Wu and Chengyi Wang and Shujie Liu and Daniel Tompkins and Zhuo Chen and Furu Wei",
        "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ghosh2024gamalargeaudiolanguagemodel",
        "author": "Sreyan Ghosh and\nSonal Kumar and\nAshish Seth and\nChandra Kiran Reddy Evuru and\nUtkarsh Tyagi and\nS. Sakshi and\nOriol Nieto and\nRamani Duraiswami and\nDinesh Manocha",
        "title": "{GAMA:} {A} Large Audio-Language Model with Advanced Audio Understanding\nand Complex Reasoning Abilities"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "chu2024qwen2audiotechnicalreport",
        "author": "Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and others",
        "title": "Qwen2-audio technical report"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "DBLP:conf/emnlp/NukraiMG22",
        "author": "David Nukrai and\nRon Mokady and\nAmir Globerson",
        "title": "Text-Only Training for Image Captioning using Noise-Injected {CLIP}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "li2023decapdecodingcliplatents",
        "author": "Wei Li and\nLinchao Zhu and\nLongyin Wen and\nYi Yang",
        "title": "DeCap: Decoding {CLIP} Latents for Zero-Shot Captioning via Text-Only\nTraining"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "deshmukh2024training",
        "author": "Deshmukh, Soham and Elizalde, Benjamin and Emmanouilidou, Dimitra and Raj, Bhiksha and Singh, Rita and Wang, Huaming",
        "title": "Training audio captioning models without audio"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "kouzelis2023weaklysupervisedautomatedaudiocaptioning",
        "author": "Kouzelis, Theodoros and Katsouros, Vassilis",
        "title": "Weakly-supervised automated audio captioning via text only training"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zhang2024zeroshotaudiocaptioningusing",
        "author": "Zhang, Yiming and Xu, Xuenan and Du, Ruoyi and Liu, Haohe and Dong, Yuan and Tan, Zheng-Hua and Wang, Wenwu and Ma, Zhanyu",
        "title": "Zero-Shot Audio Captioning Using Soft and Hard Prompts"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "li2024drcapdecodingclaplatents",
        "author": "Li, Xiquan and Chen, Wenxi and Ma, Ziyang and Xu, Xuenan and Liang, Yuzhe and Zheng, Zhisheng and Kong, Qiuqiang and Chen, Xie",
        "title": "DRCap: Decoding CLAP Latents with Retrieval-augmented Generation for Zero-shot Audio Captioning"
      }
    ]
  }
]