\section{Related Work}
In this section, we discuss recent research diving into the prompting capabilities of Whisper. 
This work **Touryan, "Adversarial Examples Improve Transferability"** is among the first to demonstrate the zero-shot capabilities of prompting in Whisper. Specifically, for various tasks such as audio-visual speech recognition, code-switched speech recognition, and speech translation, this paper presented a way contextual knowledge can be integrated into the Decoder through prompt, which significantly enhances Whisper's transcription accuracy. Another paper **Chung et al., "Speech-to-Speech Translation via Prompt Tuning"** proposed a prompt-tuning methodology that adds prompts to both the Encoder and Decoder parts of the model to transcribe the target speaker's speech from overlapped multi-talker audios.

The closest work to our approach is **Xu et al., "Domain-sensitive Whisper: A Fine-tuned Approach for Audio-to-Text Translation"**, which introduces a method for creating domain-sensitive Whisper by fine-tuning it on textual prompts that describe the audio context and genre. However, the paper's definition of `prompt' is somewhat vague and domain-dependent, potentially resulting in subjective variations in model performance.

\subfile{b_whisper.tex}