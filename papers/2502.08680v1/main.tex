% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{multirow}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage{acl}



% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{siunitx}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}


% Adjust title box height to prevent overlapping
\setlength\titlebox{5cm}  % Adjust as needed

\title{Mathematical Reasoning in Large Language Models:\\Assessing Logical and Arithmetic Errors across Wide Numerical Ranges}

\author{
  Safal Shrestha\textsuperscript{*} \quad Minwu Kim\textsuperscript{*} \quad Keith Ross \\
  New York University Abu Dhabi
}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%\title{Working Titles: Arithmetic and Logical Errors of LLMs: A Deeper Understanding}
%\title{Arithmetic and Logical Errors of LLMs: A Deeper Understanding}
%\title{Logical Soundness and Numerical Sensitivity: Assessment of Mathematical Reasoning in LLMs}
%\title{Mathematical Reasoning in LLMs: \newline A Close Look at Logical and Arithmetic Errors}
% \title{Mathematical Reasoning in Large Language Models:\\Assessing Logical and Arithmetic Errors across Wide Numerical Ranges}

% \title{Assessing Logical and Arithmetic Errors in \\ Large Language Models Across Wide Numerical Ranges}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% =========================================================================
% \author{Safal Shrestha\textsuperscript{1} \\
%   New York University Abu Dhabi \\ Abu Dhabi, UAE \\
%   \texttt{safal.shrestha@nyu.edu} \\\And
%   Minwu Kim\textsuperscript{1} \\
%   New York University Abu Dhabi \\ Abu Dhabi, UAE \\
%   \texttt{mwk300@nyu.edu} \\\AND
% Keith Ross \\
%   New York University Abu Dhabi \\ Abu Dhabi, UAE \\
%   \texttt{keithwross@nyu.edu} \\}
% =========================================================================

%\author{
%  \textbf{First Author/textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal contribution; order determined by a coin toss.}
\footnotetext{Correspondence: keithwross@nyu.edu}

\begin{abstract}


% Mathematical reasoning with Large Language Models (LLMs) have recently been the subject of significant attention. 
% Yet, their performance has predominantly been evaluated on benchmarks with limited numerical ranges, leaving a gap between these benchmarks and real-world scenarios requiring problem-solving across diverse numerical scales. Furthermore, prior studies have primarily assessed models' mathematical abilities by directly comparing responses to ground truth, restricting error analysis and obscuring a clear evaluation of problem-solving skills. To address these limitations, we introduce GSM-Ranges, a benchmark derived from the GSM8K dataset, systematically adjusted to include numerical values across varying scales. We also propose a novel grading methodology that distinguishes between logical and non-logical errors, offering a more precise metric to evaluate whether LLMs follow sound reasoning processes independent of computational accuracy. 
% Through detailed analysis, we reveal the sensitivity of LLMs to changes in numerical scales and emphasize the need for more robust evaluation methodologies to better understand their mathematical capabilities.\footnote{Code and relevant dataset available at \url{TBD}}

Mathematical reasoning in Large Language Models (LLMs) is often evaluated using benchmarks with limited numerical ranges, failing to reflect real-world problem-solving across diverse scales. Furthermore, most existing evaluation methods only compare model outputs to ground-truth answers, obscuring insights into reasoning processes. To address these limitations, we introduce GSM-Ranges, a dataset generator derived from GSM8K that systematically perturbs numerical values in math problems to assess model robustness across varying numerical scales. Additionally, we propose a novel grading methodology that distinguishes between logical and non-logical errors, offering a more precise evaluation of reasoning processes beyond computational accuracy. Our experiments with various models reveal a significant increase in logical error rates—up to 14 percentage points—as numerical complexity rises, demonstrating a general weakness in reasoning with out-of-distribution numerical values. Moreover, while models demonstrate high accuracy on standalone arithmetic tasks, their performance deteriorates substantially when computations are embedded within word problems. These findings provide a comprehensive evaluation of LLMs’ mathematical reasoning capabilities and inform future research directions for improving numerical generalization in language models. \footnotetext{Code and relevant dataset available at \url{https://github.com/minwukim/GSM-Ranges}}

%Mathematical reasoning in Large Language Models (LLMs) has recently garnered significant attention. However, evaluation has largely relied on benchmarks with limited numerical ranges, failing to reflect real-world problem-solving across diverse scales. Additionally, prior studies have primarily compared model responses to ground truth answers, thereby restricting error analysis and obscuring insights into mathematical capabilities. To address these gaps, we introduce GSM-Ranges, a dataset generator derived from GSM8K that systematically adjusts numerical values across varying scales. We also propose a novel grading methodology that categorizes errors into logical and non-logical types, offering a more precise assessment of reasoning processes independent of computational accuracy. Our experiments using GSM-Ranges and the proposed grading framework reveal that logical error rates increase by up to 14 percentage points as numerical perturbations rise, suggesting that LLMs struggle with reasoning in out-of-distribution numerical values. Moreover, while LLMs demonstrate high accuracy on standalone arithmetic tasks, their performance deteriorates significantly when calculations are embedded in a natural language context. This work provides a more comprehensive assessment of LLMs’ mathematical capabilities and lays the groundwork for future research on improving their numerical reasoning across diverse problem settings.\footnote{Code and relevant dataset available at \url{https://anonymous.4open.science/r/GSM-Ranges-F384}}


% Mathematical reasoning with Large Language Models (LLMs) has recently been the subject of significant attention. However, their performance has predominantly been evaluated on benchmarks with limited numerical ranges, leaving a gap between these benchmarks and real-world scenarios requiring problem-solving across diverse numerical scales. Furthermore, prior studies primarily directly compared model responses to ground truth answers, restricting error analysis and obscuring insights into reasoning processes. To address these limitations, we introduce GSM-Ranges, a tool for generating datasets derived from GSM8K, systematically adjusting numerical values across varying scales. We also propose a novel grading methodology that categorizes errors into logical and non-logical types, offering a more precise evaluation of reasoning processes independent of computational accuracy. Our experiments using GSM-Ranges and the proposed grading approach reveals that logical error rates increase by up to 14 percentage points as perturbation level rises, suggesting that LLMs struggle with reasoning in out-of-distribution numerical values. Moreover, while LLMs exhibit high accuracy on standalone arithmetic tasks, their performance significantly deteriorates when calculations are integrated into a natural language context. This work provides a more comprehensive assessment of LLMs’ mathematical capabilities and lays the groundwork for future research on improving their numerical reasoning across diverse problem settings.\footnote{Code and relevant dataset available at \url{https://anonymous.4open.science/r/GSM-Ranges-F384}}



\end{abstract}





\section{Introduction}


\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.3} % Adjust row height
\setlength{\tabcolsep}{3pt} % Adjust column spacing

\begin{tabular}{p{1.0\columnwidth}}
\hline

\textbf{GSM8K} \\

Judy teaches \textbf{\underline{5}} dance classes every day on the weekdays and \textbf{\underline{8}} classes on Saturday. If each class has  \textbf{\underline{15}} students and she charges \$\textbf{\underline{15}} per student, how much money does she make in \textbf{\underline{1}} week? \\ \hline

\textbf{GSM-Ranges} (Level 6 Perturbation)\\
% Judy teaches \textbf{\textcolor{blue}{3,124,213}} dance classes every day on the weekdays and \textbf{\textcolor{blue}{7,832,129}} classes on Saturday. If each class has \textbf{\textcolor{blue}{25}} students and she charges \textbf{\textcolor{blue}{\$35}} per student, how much money does she make in \textbf{\textcolor{blue}{1}} week? \\ \hline

Judy teaches \textbf{\underline{3,124,213}} dance classes every day on the weekdays and \textbf{\underline{7,832,129}} classes on Saturday. If each class has \textbf{\underline{25}} students and she charges \$\textbf{\underline{35}} per student, how much money does she make in \textbf{\underline{1}} week? \\ \hline
\vspace{-0.5em}

\caption{An example of a question generated by \textit{GSM-Ranges} tool, derived from a base problem from the GSM8K dataset.}
\label{tab:gsm_ranges}


% \textbf{\textcolor{red}{GSM8K}} \\

% Judy teaches \textcolor{red}{\textbf{5}} dance classes every day on the weekdays and \textcolor{red}{\textbf{8}} classes on Saturday. If each class has \textcolor{red}{\textbf{15}} students and she charges \textcolor{red}{\textbf{\$15}} per student, how much money does she make in \textcolor{red}{\textbf{1}} week? \\ \hline

% \textbf{\textcolor{blue}{GSM-Ranges: Level 6 Perturbation (1M-10M)}} \\
% Judy teaches \textbf{\textcolor{blue}{3,124,213}} dance classes every day on the weekdays and \textbf{\textcolor{blue}{7,832,129}} classes on Saturday. If each class has \textbf{\textcolor{blue}{25}} students and she charges \textbf{\textcolor{blue}{\$35}} per student, how much money does she make in \textbf{\textcolor{blue}{1}} week? \\ \hline
% \vspace{-0.5em}

% \caption{An example of the questions generated by \textcolor{blue}{\textbf{GSM-Ranges}} tool, derived from a base problem from the \textcolor{red}{\textbf{GSM8K}} dataset.}
% \label{tab:gsm_ranges}

\end{tabular}
\end{table}
Mathematical reasoning with Large Language Models (LLMs) has recently been the subject of significant attention \cite{wei2022-CoT,openai2024-gpt4,ahn2024}.
%Although many Large Language Models (LLMs) are remarkably adept in mathematical reasoning tasks \cite{wei2022-CoT,openai2024-gpt4,ahn2024}, several studies have observed that smaller LLMs can frequently produce errors \cite{madaan2022-50percent, hong2024caught,shakarian2023eval}. Although these observations are correct and insightful, 
%{\bf What about making it much simpler? In my opinion, the part of saying LLMs frequently make errors is unnecessary. What about directly saying something like: "LLMs do math very well. 
%However, there are issues with how we evaluate their mathematicl abilities."?}
However, the current evaluation methodologies for these systems
%to analyze errors 
exhibit notable limitations. First, existing benchmarks primarily focus on problems with limited numerical ranges \cite{madaan2022-50percent}, leaving a significant gap between the controlled evaluations and real-world settings.
%applications that require reasoning across math problems that can have diverse numerical scales. 
%These benchmarks often overlook the sensitivity of LLMs to variations in numerical ranges, impeding a comprehensive assessment of their robustness. 
Second, traditional grading approaches typically compare LLMs' final answers directly with the ground truth answers \cite{hong2024caught,shakarian2023eval}, a practice that conflates logical and numerical errors, 
%missteps with computational inaccuracies, 
thereby obscuring a deep understanding 
%clear evaluation 
of the LLM's reasoning capabilities. 
%We argue that in order to improve the mathematical reasoning capabilities of LLMs, it is important to not only delineate the different types of errors, but also understand the classes of math problems in which they occur as well as their underlying causes. 
%{\bf Let's discuss about this last sentence. The main contribution of this paper is neither improving LLM reasoning nor underlying causes (although we do touch upon OOD issue). Therefore, I wonder if this sentence is the most appropriate here. Instead, what about something simple, like: 
This motivates the need for a better evaluation approach that not only handles numbers across wide numerical ranges but also distinguishes between logical and arithmetic errors. 

This paper makes three contributions. First, we introduce publicly available {\em GSM-Ranges}, a tool for generating datasets that are designed to evaluate error rates and error types across diverse numerical ranges. Derived from the GSM8K dataset \cite{cobbe2021-gsm8k}, GSM-Ranges systematically organizes problems into distinct numerical intervals. Specifically, GSM-Ranges applies six distinct levels of perturbations to 
%a subset of 
GSM8K questions, replacing existing numbers with random values across 
%various 
six distinct scales. 
%Unlike prior work that modifies GSM8K without explicit range control \cite{mirzadeh2024-gsmsymbolic,gao2022-pal}, GSM-Ranges 
%facilitates 
%allows for systematic assessment of an LLM's mathematical reasoning capabilities in wide ranges of numerical values.


Second, we introduce a novel grading methodology that distinguishes between logical and non-logical errors. 
We claim that a solution should be deemed logically valid if computational inaccuracies
%—classified as non-logical errors—are 
can be corrected and the revised answer matches the ground truth. Conversely, if the final answer remains incorrect despite eliminating such errors, we infer a fundamental flaw in the reasoning process. 
To automate this assessment, 
our methodology employs GPT-4o model \cite{openai2024-gpt4o} to translate a LLM-generated response into Python code that accurately 
%reflects their 
captures the underlying logic. By executing this code, we isolate non-logical errors and compute the corrected final answer, which is then 
%evaluated against 
compared with the ground truth to assess logical correctness.
We also perform a careful evaluation of our automated grading methodology, 
and confirm its high level of accuracy for distinguishing between the two error types. 

% Large Language Models (LLMs) have shown remarkable proficiency in mathematical reasoning tasks \cite{wei2022-CoT,openai2024-gpt4,ahn2024}. However, existing benchmarks primarily focus on problems with limited numerical ranges \cite{madaan2022-50percent}, leaving a significant gap between controlled evaluations and real-world applications that require reasoning across math problems that can have diverse numerical scales. Additionally, these benchmarks often overlook the sensitivity of LLMs to variations in numerical ranges, limiting a comprehensive assessment of their robustness.



% In this study, we investigate LLM consistency in solving math tasks across diverse numerical ranges. We introduce GSM-Ranges, a dataset derived from the GSM8K benchmark \cite{cobbe2021-gsm8k}, which organizes problems into distinct numerical intervals for systematic evaluation. Specifically, we create 100 templates from the original GSM8K questions and apply 6 distinct levels of perturbations that replace numbers with random values across various scales. Unlike prior work modifying GSM8K without explicit range control \cite{mirzadeh2024-gsmsymbolic,gao2022-pal}, GSM-Ranges facilitates systematic assessment of LLM robustness across a larger scale. 

% Furthermore, we propose a novel grading methodology that distinguishes between logical and non-logical errors, addressing limitations in existing approaches. Traditional grading methods often compare the final answers of LLMs directly with the ground truth \cite{hong2024caught,shakarian2023eval}. The issue with this approach is that it conflates logical missteps with computational inaccuracies, obscuring a clear assessment of reasoning capabilities. This limitation is particularly problematic for benchmarks like GSM-Ranges, which encompass a wide range of numerical scales, given that while recent LLMs excel at arithmetic operations involving small numbers, their accuracy drops significantly when dealing with larger numerical values \cite{qian2023-arithmeticerror, feng2024numerical}.

% For accurate evaluation, it is essential to delineate errors unrelated to the reasoning process itself. Specifically, if non-logical errors—such as computational inaccuracies—are corrected and the resulting answer matches the ground truth, we claim that the solution should be considered logically correct. To implement this approach, we leverage the GPT-4o model \cite{openai2024-gpt4o} to translate the LLM's response into Python code that faithfully captures the underlying logic of the solution, thereby eliminating non-logical errors. The output of this code is then evaluated against the ground truth to assess the correctness of the reasoning process.


Third, using our grading methodology and our GSM-Ranges tool, we analyze
% benchmark various
various open-source and proprietary LLM models. This leads to several findings: 
\begin{itemize}
    
    \item Previous works have shown that arithmetic errors become more pronounced for larger numbers \cite{qian2023-arithmeticerror, feng2024numerical}. We find that this trend applies to logical errors as well, with the worst-case logical error rate increasing by up to 14 absolute percentage points as perturbation levels rise. This is surprising since the logical reasoning process required to solve the problems remains unchanged despite the numerical modifications. Nevertheless, we still observe an increase in logical errors as perturbation levels rise, suggesting that the logical reasoning in LLMs tends to exacerbate for out-of-distribution numerical values, compromising their robustness in handling broader numerical scales.
    
    \item Previous studies have demonstrated that LLMs achieve high accuracy on standalone arithmetic tasks with in-distribution numbers (e.g., “36$\times$6=?”) \cite{yang2023gpt, maltoni2024arithmetic, yuan2023well, mirzadeh2024-gsmsymbolic, xie2024llm}. While we confirm these findings, our results further reveal that the accuracy of arithmetic computations significantly deteriorates when the calculations are embedded within word problems (e.g., “36 apples from Jack$\times$6 = ? apples”). %We also find that this problem is exacerbated when problems involve multi-step arithmetic.
    
%We further observe that the correct logical structure remains present within the distribution for larger numbers, though it becomes increasingly obscured. Unlike previous studies that primarily focused on accuracy degradation following number switching, while disregarding arithmetic errors and citing evidence that LLMs can perform arithmetic within a certain digit range, our findings reveal a non-trivial number of arithmetic errors in the LLM. These errors may be addressable in a standalone setting but remain harder to solve in the GSM8K setting \cite{mirzadeh2024-gsmsymbolic,xie2024llm}. Through detailed analysis, we provide deeper insights into the mathematical reasoning capabilities of LLMs and emphasize the need for more robust evaluation methodologies to better understand their reasoning processes.
\end{itemize}
By introducing GSM-Ranges and a novel grading methodology, this work aims to provide a more comprehensive evaluation of mathematical reasoning in LLMs. Our approach not only isolates logical and arithmetic errors but also assesses model robustness across a broad range of numerical values. These insights pave the way for future research on improving LLMs’ mathematical reasoning capabilities and developing models that can generalize more effectively across diverse mathematical problem settings.



% However, beyond this known limitation, we also observe a significant increase in logical errors across various models. Since the underlying reasoning process remains unchanged despite the numerical modifications, the fundamental mathematical reasoning ability required to solve these problems should remain consistent. Nevertheless, we still observe rise in logical errors, suggesting that LLMs tend to overfit to lower-range numbers, compromising their robustness in handling broader numerical scales.



% Our findings reveal that as numerical scales increase, logical errors grow at varying rates across models, suggesting a tendency to overfit to lower numerical ranges. In addition, arithmetic errors become more frequent with larger values and persist non-trivially even within smaller ranges for certain models. 


%We further observe that the correct logical structure remains present within the distribution for larger numbers, though it becomes increasingly obscured. Unlike previous studies that primarily focused on accuracy degradation following number switching, while disregarding arithmetic errors and citing evidence that LLMs can perform arithmetic within a certain digit range, our findings reveal a non-trivial number of arithmetic errors in the LLM. These errors may be addressable in a standalone setting but remain harder to solve in the GSM8K setting \cite{mirzadeh2024-gsmsymbolic,xie2024llm}. Through detailed analysis, we provide deeper insights into the mathematical reasoning capabilities of LLMs and emphasize the need for more robust evaluation methodologies to better understand their reasoning processes.


% These instructions are for authors submitting papers to *ACL conferences using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} and this document contains additional instructions for the \LaTeX{} style files.

% The templates include the \LaTeX{} source of this document (\texttt{acl\_latex.tex}),
% the \LaTeX{} style file used to format it (\texttt{acl.sty}),
% an ACL bibliography style (\texttt{acl\_natbib.bst}),
% an example bibliography (\texttt{custom.bib}),
% and the bibliography for the ACL Anthology (\texttt{anthology.bib}).



\section{Related Work}

% - Pattern Matching and GSM8K modification. 
% - Evaluations Methods
\textbf{LLM Sensitivity to Perturbations.}
Several prior studies \cite{stolfo2022causal,hooda2024large,jiang2024-peek,guo2024learning} have explored the sensitivity of LLMs to perturbations in input problems, demonstrating significant performance degradation even when the underlying logic remains the same. In the domain of mathematical word problems (MWPs), particularly on the GSM8K benchmark, this degradation has been observed when numbers are slightly changed from the original question. \cite{li2024-gsmplus,mirzadeh2024-gsmsymbolic,shi2023-GSMIC}. However, existing approaches often constrain substituted values to a limited numerical range \cite{stolfo2022causal} or use numbers that remain comparable to the original values, which tend to be small \cite{li2024-gsmplus,mirzadeh2024-gsmsymbolic,madaan2022-50percent}. In this paper, we go beyond the narrow constraints previously studied, providing a comprehensive investigation into how different numerical ranges can impact mathematical abilities in LLMs.\\

\noindent \textbf{Evaluation of Mathematical Correctness.} Prior correctness evaluations (grading) predominantly rely on ground-truth comparisons
%as a proxy for problem-solving ability, which can obscure important nuances in reasoning errors
\cite{shakarian2023eval,fu2023chain,hong2024caught,frieder2024mathematical}. 
However, such a straightforward approach does not distinguish between logical and non-logical errors, and therefore cannot alone accurately assess an LLM's mathematical reasoning capabilities. 
Previous studies have explored simple prompting strategies for evaluation, finding that while LLMs perform well in generating correct answers to benchmark questions, they struggle to identify and diagnose errors in solutions to those same questions. This difficulty is particularly pronounced for non-logical errors, highlighting a fundamental gap in their problem comprehension \cite{li2024evaluating, zeng2024mrgsm8kmetareasoningbenchmarklarge}. A straightforward alternative involves using external tools, such as calculators, to mitigate non-logical errors. However, this approach requires fine-tuning models to follow a specific format and does not always produce reliable results \cite{schick2023toolformer, cobbe2021-gsm8k}. To address these challenges, we introduce a novel automated grading methodology that eliminates the need for fine-tuning while effectively differentiating between logical and non-logical errors. This enables a more precise assessment of how mathematical reasoning deteriorates under various numerical conditions.

%Errors stemming from arithmetic miscalculations
%, rather than from fundamental misunderstandings of the problem, 
%can often be mitigated simply by using external calculator tools \cite{schick2023toolformer,cobbe2021-gsm8k}. This underscores the necessity of distinguishing between these error types to accurately assess mathematical reasoning capabilities. 

%Such a grading scheme allows us to further refine our understanding of these errors, offering deeper insights into the nature of mathematical problem-solving in the presence of varied numerical perturbations.\\

%However, current approaches to evaluation show that LLMs exhibit limited capability in accurately identifying and diagnosing errors \cite{li2024evaluating, zeng2024mrgsm8kmetareasoningbenchmarklarge}. 



\noindent \textbf{Arithmetic Errors Due to Perturbations.} Past studies have shown that LLMs handle basic arithmetic reasonably well when the arithmetic involves small numbers in standalone queries like \textit{"What is x + y?"}, a skill often linked to memorization \cite{yang2023gpt, maltoni2024arithmetic, yuan2023well, qian2023-arithmeticerror, feng2024numerical}. Performance drops significantly in more complex cases, such as multi-step equations, large numbers, and multiplication \cite{kao2024solving, yuan2023well, yang2023gpt, feng2024numerical, qian2023-arithmeticerror}. Background context can further affect arithmetic reasoning, introducing additional inconsistencies \cite{abedin2025arithmattack}. 
Current research applying perturbations to widely used benchmarks like GSM8K involve basic arithmetic; thus, arithmetic errors are assumed to be minimal. \cite{mirzadeh2024-gsmsymbolic, xie2024llm, anand2024mathify}.  However, a deeper understanding of the nature and frequency of arithmetic errors remains lacking. This study addresses this gap by systematically quantifying arithmetic errors in mathematical word problems and conducting a qualitative analysis to identify underlying error patterns.


\section{GSM-Ranges}

The majority of existing mathematical benchmarks are constrained to relatively limited numerical ranges. For instance, \citet{madaan2022-50percent} reported that single-digit numbers constitute approximately 50\% of the problems in the GSM8K dataset. To further investigate this trend, we analyzed cumulative frequency distribution of numerical values across three widely used benchmarks: GSM8K \cite{cobbe2021-gsm8k}, SVAMP \cite{patel2021-SVAMP}, MATH \cite{hendrycks2021-MATH}. As shown in Figure \ref{cumulativegraph}, our analysis reveals that in all three datasets, numbers below 1,000 (i.e., three digits or fewer) account for 94.9\% of values in GSM8K, 97.8\% in SVAMP, and 98.0\% in MATH. These findings indicate that the mathematical capabilities of current LLMs have primarily been evaluated within a limited numerical range. To assess robustness across wider ranges, we introduce \textit{GSM-Ranges}, a tool for generating datasets which encompass a wider distribution of numerical values.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{images/bench.pdf}
  \caption{Cumulative frequency distribution of numerical values in questions and ground truth answers. Numbers <1,000 account for 94.9\% (GSM8K), 97.8\% (SVAMP), and 98.0\% (MATH) of the values.}
  \label{cumulativegraph}
\end{figure}



\subsection{Selecting Base Problems from GSM8K}

GSM-Ranges systematically modifies numerical values in the GSM8K dataset. From the GSM8K test set of 1,319 questions, we exclude those involving non-integer values or division in the ground truth answers, as such cases could potentially create logically incoherent problems (e.g., “Assign 5 people evenly to 2 separate rooms”). After filtering, 100 questions are randomly selected, with all numbers in the questions being single- or double-digit.


\subsection{Perturbation Levels}
We convert the 100 sampled questions into Python templates to systematically adjust the numerical values within the questions across various ranges. Specifically, we apply 6 levels of perturbation: same-digit, 100--1,000, 1,000--10,000, 10,000--100,000, 100,000--1,000,000, and 1,000,000--10,000,00, which we refer to as level 1 to level 6 perturbation, respectively. In same-digit perturbation (level 1), we randomly replace each number in the problem with a randomly chosen number with the same number of digits as the original number, thereby maintaining the similarity of the perturbed problems to the original. Additionally, we ensure that modified numbers are always different from the original values, preventing any duplication of the original problems. In 100-1000 perturbation (level 2), we replace each number in the problem with a number randomly chosen in the range 100 to 1000. Levels 3-6 are done in similar manners with their respective ranges. 

All perturbations ensure non-negative final answers and intermediate values, 
%in the ground truth answers, 
as, similar to the case of fractional values, they can lead to logically incoherent math problems (e.g. “A store sells -7 items in a day”, “Eat 10 apples out of 3 apples”). In addition, to prevent extreme scaling of final answers, we apply scaling selectively in cases involving multiplication. Specifically, when the final answer is derived from a multiplication operation (e.g., $(A+B) \times (C+D-E)$), we scale only one side of the multiplication—either $A$ and $B$ or $C$, $D$ and $E$—while keeping the other side within the original numerical ranges. This approach maintains the final answer within manageable limits while introducing numerical variation in the problem, preventing the inclusion of excessively complex or computationally infeasible arithmetic operations for LLMs. An example of a problem generated with these crafted templates is shown in Table \ref{tab:gsm_ranges}.

% \subsection{Sampling from Templates}
% We randomly sample 50 sets for each of the 6 perturbation levels across 100 templates, ensuring coverage of diverse numerical values within each perturbation level and achieving statistical robustness. The resulting GSM-Ranges dataset comprises 5,000 problems per perturbation level, for a total of 30,000 problems.



\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{images/grading2.pdf}
  \caption{Illustration of grading process for LLM responses using the GPT-4o model, categorizing outputs into three labels: correct, non-logical error, and logical error.}
  \label{grading}
\end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\textwidth]{images/robust_trend_merge.pdf}
  \caption{Logical \& non-logical error rates across different perturbation levels. The left panel illustrates the increase in logical errors across the datasets, while the right panel depicts the rise in non-logical errors. Error rates are reported relative to the baseline logical and non-logical error rates on the original GSM8K problems.}
  \label{fig:robust_exp}
\end{figure*}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{images/point_diff.pdf}
  \caption{Logical error gaps across perturbation levels. For each model, the top bar represents the percentage point difference in logical errors between Level 6 and Level 1 perturbations, while the bottom bar indicates the percentage point difference between Level 1 and the original GSM8K questions.}
  \label{fig:r6r1gap}
\end{figure}


\section{Grading Methodology}


While modern LLMs perform well on basic arithmetic operations, their accuracy is reported to diminish substantially as numerical magnitudes increase \cite{qian2023-arithmeticerror}. However, conventional evaluation methods, which primarily compare final answers with ground truth values \cite{hong2024caught,shakarian2023eval}, fail to provide a complete picture of LLMs' mathematical understanding as they do not distinguish computational errors from reasoning errors.
To address this deficiency, we  propose a novel and accurate grading methodology which not only determines whether the answer is correct or erroneous, but also categorizes the type of error. 


%We propose a novel grading methodology to assess the mathematical reasoning capabilities of LLMs. Conventional evaluation methods, which primarily compare final answers with ground truth values \cite{hong2024caught,shakarian2023eval}, fail to provide the complete picture of LLMs' mathematical understanding as they do not distinguish computational errors from reasoning errors. This limitation is particularly evident in the case of GSM-Ranges, where problems often involve very large numerical values. While modern LLMs perform well on basic arithmetic operations, their accuracy is reported to diminish substantially as numerical magnitudes increase \cite{qian2023-arithmeticerror}. Thus, a more rigorous evaluation framework is necessary to accurately assess reasoning across a broad spectrum of numerical ranges.

\subsection{Error Definitions}
%We posit that if non-logical errors -- such as arithmetic inaccuracies -- are corrected and the resulting answer matches the ground truth, the response should be classified as logically correct. Conversely, if the final answer remains incorrect after correcting non-logical errors, the response is deemed logically incorrect, indicating the model's inability to solve the problem.
Our grading methodology first compares the final answer with the ground truth. If the answers match, the response is labeled as correct. Otherwise, there is an error. We define the error types as follows:
\begin{itemize}
    \item \textbf{Non-logical error}: Errors that do not stem from the reasoning process itself, such as arithmetic errors or number-copy errors. The latter refers to inaccurately reproducing problem values (e.g., misrepresenting 1,337,042 as 13,337,042) and is observed in higher-level perturbations in some models.

    \item \textbf{Logical error}: All errors not classified as non-logical, such as but not limited to missing steps, contradictory steps, or operator misuse. It should be noted that responses classified as logical errors may also include non-logical errors.
\end{itemize}

\subsection{Methodology Overview}
To handle the extensive volume of responses, we employ the GPT-4o model \cite{openai2024-gpt4o} to automate the evaluation.
For a response that fails to match the ground truth answer, we have GPT-4o model translate the reasoning in the response into Python code, which is then executed to generate a new answer. The prompt is provided in Appendix \ref{subsec:4o-prompt}.
%free of non-logical errors. 
If the new answer aligns with the ground truth, our methodology classifies the response as containing a non-logical error; otherwise, it is classified as a logical error. A detailed illustration of the grading process is shown in figure \ref{grading}.

\subsection{Number-Copy Errors}
Apart from arithmetic errors, we observe that number-copy errors are non-trivially present in some models under higher levels of perturbations (Appendix \ref{subsec:number copy error}). For instance, in the case of the Qwen 2.5 7B model \cite{qwen2024technical}, 4 out of 100 randomly sampled responses under level 6 perturbation demonstrate number-copy errors. 
To enable the GPT-4o model to identify such errors, the model requires access to the numbers provided in the problem. Instead of providing the entire problem, we supply the model with a list of extracted numbers from the problem text. This approach is motivated by our observation that providing the full problem tends to lead the model to revise logically flawed answers into logically correct ones. This behavior aligns with the known tendency of LLMs to exhibit biases toward generating correct responses and their difficulty in intentionally producing incorrect answers \cite{tjuatja2023do, kumar2024investigating}. By limiting access to the original question, we minimize this undesired bias while enabling number-copy error correction. %This method is specifically applied to responses to level 4, 5, and 6 perturbations.




% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{images/robust_trend.pdf}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:logical_error_trend}
% \end{figure}




% \begin{figure*}[t]
%   \includegraphics[width=0.48\linewidth]{images/robust_trend.png} \hfill
%   \includegraphics[width=0.48\linewidth]{images/point_diff.png}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}

\subsection{Validation}

To validate 
%reliability of 
our grading methodology, we perform a careful manual analysis. 
%conduct a rigorous manual screening process. 
We collect 
responses generated by nine models across six perturbation levels from the experiments in Section \ref{sec:experiemnt}, along with the corresponding Python code produced by GPT-4o. We randomly sample 200 of these responses and
and manually classify each one. 
We found that our grading methodology correctly classified 197 of the responses correctly, 
%Our analysis confirms that 197 of these cases are graded correctly, 
achieving a high accuracy of 98.5\%. Furthermore, we assessed GPT-4o’s ability to correct number-copy errors when going from response to Python code.
We identify 50 responses across different models that contain such errors and evaluate whether the generated code correctly fixes them. Our manual review confirms that all 50 errors are successfully corrected. These evaluation results establish
%provide strong evidence for 
the reliability of our methodology. 


\section{Experiments and Results}


\label{sec:experiemnt}
Using GSM-Ranges and our proposed grading methodology, we evaluate the mathematical reasoning capabilities of nine distinct models, including both open-source and closed-source variants. 
Recall that we begin with 100 randomly selected GSM8K questions. For each question and for each of the six perturbation levels, we generate 50 random variations of the question.
This process yields a dataset of 5,000 problems per perturbation level. For each perturbation level and each model, we obtain responses to the 5,000 questions and classify the responses using our grading methodology. We then determine the percentages of correct answers, logical errors, and non-logical errors across the 5,000 responses. To compute confidence intervals, we leverage the structure of our dataset: each perturbation level consists of 50 distinct sets of questions derived from the original 100. We calculate the proportion of each error type within each set and then use these 50 sample proportions to estimate the corresponding confidence intervals, thereby quantifying the variability in model performance across perturbation instances. Additionally, we assess each model on the 100 unmodified base questions from GSM8K, providing a standard reference point for performance comparison. All inferences are done in the greedy decoding setting.






\subsection{Logical Errors}

\subsubsection{Rising Trend of Logical Errors}

Since the perturbations alter only the numerical values while keeping the question structure intact, the logical reasoning required to solve the problems remains unchanged across all perturbation levels. In principle, all perturbation levels should demand the same level of logical reasoning ability. 
Surprisingly, however, while the degree varies among the models, we observe a consistent upward trend in logical errors as the perturbation level increases, across all nine evaluated models (Figure \ref{fig:robust_exp}) except GPT-4o. To quantify this trend, we calculate the difference in logical error rates between level 6 (1M--10M) and level 1 (same digit) perturbations for each model (Figure \ref{fig:r6r1gap}). The most pronounced discrepancy is exhibited by the Gemma 2 2B model, which shows a 14\% absolute increase in logical error rate. Similarly, the WizardMath 7B v1.1 model demonstrates a substantial increase of 10\%. Even the relatively more robust models, such as Phi-3 Mini 4K and GPT-3.5 Turbo, still exhibit an increase of approximately 4\%, which remains a significant deviation. GPT-4o, one of the most advanced models at the time of our study, stands out as the only model with a near-zero gap. These results reveal the sensitivity of the models’ logical reasoning to numerical scales. We conjecture this phenomenon occurs because the models are mostly trained with lower-range numbers, and test problems with large numbers are out of distribution. (A qualitative analysis of additional logical errors induced by increasing numerical values is provided in Appendix \ref{subsec:error types}.)


% We conduct a qualitative analysis on the characteristics of additional logical errors induced by increasing numerical values. Specifically, we manually analyze the cases with the greatest rise in logical errors across perturbation levels among the questions derived from the same base  GSM8K questions for each model. The results show a diverse array of errors—including but not limited to missing steps, operator errors, and contextual value errors —occurring without a consistent pattern. More detailed descriptions and illustrative examples can in found at \ref{subsec:error types}.

 Another noteworthy observation is that the increase in logical errors becomes more gradual at higher perturbation levels. Across all nine models, the gap between level 3 and level 1 is generally larger than that between level 6 and level 3. This trend aligns with the cumulative frequency patterns observed in Figure \ref{cumulativegraph}, which shows that low-range values account for majority of numbers across the most widely used benchmark datasets. While the exact composition of training data for the models remains unknown, if math-problem training data is predominantly concentrated in the lower numerical ranges, it is plausible that beyond a certain threshold, further increases in numerical magnitude do not lead to a significant difference in model performance. Once numbers exceed this threshold, they may all be similarly unfamiliar to the model due to their low presence in the training data. Further investigation is needed to validate this hypothesis.

\subsubsection{Potential Data Contamination with GSM8K Dataset}

We also observe a notable logical error gap between level 1 perturbation and the original questions 
%from the GSM8K dataset 
for many of the evaluated models (Figure \ref{fig:r6r1gap}). The Gemma 2 2B model exhibits the largest gap at 6\%, followed by Mistral 7B v0.1 with a 4\% discrepancy. However, this pattern is not consistent across all models. For instance, Qwen 2.5 7B and GPT-4o show a gap of only about 1\%, demonstrating better robustness. Moreover, Llama 3.2 3B and Phi-3 Mini 4K exhibit a -2\% gap, indicating an opposite trend.

This result points to possibility of data contamination to the GSM8K dataset in certain models. Notably, a similar finding was previously reported by \citet{mirzadeh2024-gsmsymbolic}, but our study explores this issue in more depth by providing an explicit definition of numerical-range similarity and establishing a clear distinction between logical and non-logical errors, further validating their conclusions.




\subsection{Arithmetic Errors}
\subsubsection{Rising Trend of Arithmetic Errors}

Previous studies have shown that LLMs exhibit a significant decline in arithmetic accuracy as numerical values grow \cite{qian2023-arithmeticerror, feng2024numerical}, and our result further confirms this trend.
As shown in Figure \ref{fig:robust_exp},
we also observe a consistent increase in non-logical errors. Given that number-copy errors account for only a small portion (Table \ref{tab:number-copy-errors}), the majority of these errors stem from arithmetic errors. 
Furthermore, because some responses classified as logical errors also include arithmetic errors, 
the true prevalence of arithmetic errors exceeds what is suggested in the figure. 

\subsubsection{Arithmetic Errors with Small Numbers}
\label{smallnumarithmetic}
% {\bf Safal can take a shot.}

Previous studies have found that state-of-the-art models have arithmetic accuracy on low-range numbers \cite{henighan2020scaling, yuan2023well,qian2023-arithmeticerror, feng2024numerical}. However, we find that some models still show non-trivial percentages of non-logical errors at level 1, such as Mistral 7B v0.1 at 9\% and WizardMath 7B v1.1 at 4\%. This motivates further analysis on the patterns of these errors, which is discussed in section \ref{sec:arithmetic error patterns}.

% State-of-the-art models have been reported to achieve high arithmetic accuracy on low-range numbers \cite{henighan2020scaling, yuan2023well,qian2023-arithmeticerror, feng2024numerical}.
% However, we have found that some models still show non-trivial percentages of non-logical errors, such as Mistral 7B v0.1 at 8.5\% and WizardMath 7B v1.1 at 3.9\%. 
% This suggests that previous work focusing on perturbations with low-range numbers may actually contain arithmetic errors, potentially obscuring the accurate assessment of mathematical problem-solving abilities. 

% a trend we also observe in most of our evaluated models, which exhibit low non-logical error rates at Level 1 perturbation (Table \ref{tab:nonlogical}). However, we have found some models still still show non-trivial number of non-logical errors, such as Mistral 7B v0.1 at 8.5\% and WizardMath 7B v1.1 at 3.9\%. 


% This suggests that even benchmarks focused on lower-range numbers may contain arithmetic errors, potentially obscuring the accurate assessment of mathematical problem-solving abilities. 
\begin{figure}[t]
  \includegraphics[width=\columnwidth]{images/recall.pdf}
  \caption{Recall rates across perturbation levels and original GSM8K questions for different sampling sizes (1, 8, 32, 48).}
  \label{fig:recall}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{images/o3mini.pdf}
  \caption{Results of o3-mini across perturbation levels. The left plot displays the logical and non-logical error counts, while the right plot shows the mean token counts per 100 responses at each perturbation level.}
    \label{fig:o3token}
\end{figure}

\section{In-depth Analysis}
% \subsection{[MIGHT REMOVE] Qualitative Analysis of Logical Errors}

% We conduct a qualitative analysis on the characteristics of additional logical errors induced by increasing numerical values. Specifically, we manually analyze the cases with the greatest rise in logical errors across perturbation levels among the questions derived from the same base  GSM8K questions for each model. The results show a diverse array of errors—including but not limited to missing steps, operator errors, and contextual value errors —occurring without a consistent pattern. More detailed descriptions and illustrative examples can in found at \ref{subsec:error types}.{\bf Should I put it in section 5?}

\subsection{Is the Correct Logic Present in the LLM?}

We observe that larger numerical values in math questions increase the likelihood of logical errors. However, although a sampled response may have a logical error, the correct logic may nevertheless be present in the model's distribution. To investigate this issue,  we measure recall rates defined as follows: for each of 100 randomly generated questions, we obtain $n$ responses (i.e., perform $n$-passes) in a non-zero temperature setting  (temperature = 0.8, top\_p = 0.95) and, using our grading methodology, count the number of questions for which the correct logic appears in at least one of the passes. We perform this experiment over all six perturbation levels, and for a varying number of passes ranging from $n=1$ to $n=48$. 

%In the greedy decoding setting, we find that larger numerical values in a question lead to greater misalignment of correct logic. To investigate whether the correct logic remains within the models' distribution, we measure recall rates—defined as the proportion of instances where at least one sampled response follows the correct reasoning—under varying sample sizes. We obtain a set of 100 questions at each perturbation level with GSM-Ranges tool and generate multiple responses with the models in a non-zero temperature setting (temperature:0.8, top-p: 0.95). This experiment focuses on the four models that exhibited the highest increase in logical error rates between level 1 and 6 in our previous analysis.

As shown in Figure \ref{fig:recall}, for all four models, as the number of passes $n$ increases, we observe (1) a higher recall rate, and (2) smaller gaps across perturbation levels. At the highest sample size of 48, the gap between level 1 and 6 is no more than 2 for any model, indicating that the correct logic exists within the model’s distribution despite larger numerical values. This suggests that training on broader numerical ranges or leveraging test-time computation could improve numerical consistency.

%Across all four models, we observe a higher recall rate with smaller gaps across perturbation levels as the sampling size increases (Figure \ref{fig:recall}). At the highest sample size of 48, the gap between Level 1 and Level 6 is no more than 2 for all models, indicating a plateau where the differences become marginal. This finding indicates that the core logical structures remain largely intact within the model’s distribution despite the increasing numerical values, suggesting that improving alignment or leveraging test-time computation could enhance the models' consistency over wide ranges.


\subsection{Performance of Reasoning Model}

The growing prominence of reasoning models \cite{openai_o1,guo2025deepseek} naturally raises the question of their performance across different perturbation levels. To explore this, we evaluate o3-mini—one of the most advanced reasoning models at the time of our study—on a set of 100 problems for each perturbation level. As shown in Figure \ref{fig:o3token}, o3-mini maintains consistently low logical and non-logical error rates across perturbation levels, demonstrating its robustness to varying numerical scales.

% This result is consistent with the recall analysis we performed for "non-reasoning" models: performance increases with more computational resources (Figure \ref{fig:recall}).

%The increasing prominence of reasoning models \cite{guo2025deepseek,openai_o1} naturally raises the question of whether such advanced models remain robust to simple surface-level perturbations. In this study, we evaluate the robustness of o3-mini \cite{openai_o3_mini} by testing it on a randomly sampled subset at each perturbation level. Our findings indicate that o3-mini exhibits considerable robustness to these perturbations (see Table \ref{tab:logical}). Given that the "non-reasoning" models we tested also demonstrated improved performance with greater computational resources (Figure \ref{fig:recall}), our results further support the notion that questions with simple surface-level ambiguities still remain tractable


We also record the average token count for 100 responses at each perturbation level and observe that the model generates more tokens as the perturbation level increases (Figure \ref{fig:o3token}). While this increase may partly stem from larger numerical values requiring more tokens for representation and complex arithmetic, it also suggests that changes in numerical scale might lead the model to perceive the tasks as more challenging, possibly due to its training data being primarily focused on lower-range numbers. Additionally, the model produces more tokens for same-digit perturbations compared to the original GSM8K questions. This raises the possibility of data contamination, allowing the model to arrive at the correct final answer with less reasoning.


%Notably, we observe that as the perturbation level increases, the model requires a greater number of reasoning tokens. While this effect may partly arise because larger numerical values necessitate more tokens for representation and complex arithmetic, a discrepancy remains between the original and same-digit perturbation levels—both of which contain numbers within the same range. This finding suggests that even minor modifications can increase task difficulty, potentially due to the original questions having been included in the model's training data.


\subsection{Arithmetic Error Patterns}

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{images/standalone.pdf}
%   \caption{Process for standalone arithmetic assessment: We extract the arithmetic errors made by the model and prompt them as isolated arithmetic operations.}
%   \label{fig:standalone}
% \end{figure}

\begin{table}[t]
  \centering  
  \resizebox{\columnwidth}{!}{ % Adjusts table width to fit the column
  \begin{tabular}{lll}  
    \hline  
    \textbf{Model}           & \textbf{Level 1} & \textbf{Level 2} \\  
    \hline  
    Gemma 2 2B       & 15/134 (11.2\%)          & 126/735 (17.1\%)   \\  
    WizardMath 7B v1.1       & 41/117 (35.0\%)          & 186/806 (23.1\%) \\  
    Mistral 7B v0.1 & 73/299 (21.4\%)  & 274/1313 (20.9\%)    \\  
    Mathstral 7B v0.1    & 31/77 (40.2\%)        & 126/509 (24.8\%)   \\  
    Llama 3.2 3B       & 3/72 (4.2\%)          & 127/1480 (8.6\%)        \\  
    Qwen 2.5 7B & 7/18 (38.9\%)  & 52/155 (33.5\%)    \\  
    Phi 3 Mini 4K & 9/22 (40.9\%)  & 89/281 (31.7\%)    \\  
    GPT-3.5 Turbo & 10/18 (55.6\%) & 92/224 (41.1\%) \\  
    GPT-4o & 1/3 (33.3\%) & 20/40 (50\%) \\  
    \hline  
  \end{tabular}  
  }
  \caption{  
  Results of a standalone arithmetic assessment on arithmetic errors made by models under Level 1 and Level 2 perturbations.
  } \label{tab:standalone}
\end{table}



\label{sec:arithmetic error patterns}

%As discussed in Section \ref{smallnumarithmetic}, we have observed non-trivial arithmetic errors in lower-range numbers for some models. To analyze the error patterns, we extract the arithmetic errors made by 8 models at levels 1 and 2 and conducted a qualitative analysis. Our findings reveal that a significant portion of these errors occur in multistep arithmetic problems—i.e., computations involving three or more numbers (e.g., 8+6+5=? ). This vulnerability, previously identified in earlier studies \cite{kao2024solving, yuan2023well, yang2023gpt, feng2024numerical, qian2023-arithmeticerror}, is reconfirmed by our analysis. 

% {\bf Safal: Please insert the citation + maybe some solid numbers, like the proportion of multistep? Also, I never added multiplication part here. Let's discuss we should add it or not.}

%It is well-known that a significant portion of arithmetic errors are multi-step arithmetic (e.g., 8+6+5=? ) errors \cite{kao2024solving, yuan2023well, yang2023gpt, feng2024numerical, qian2023-arithmeticerror}. 

Previous studies have evaluated the arithmetic accuracy of LLMs in a standalone setting, i.e., directly posing arithmetic questions like "What is 1 + 2?" \cite{yang2023gpt, maltoni2024arithmetic, yuan2023well, qian2023-arithmeticerror, feng2024numerical}. However, little attention has been paid to whether their arithmetic performance remains robust when these operations are embedded within natural language responses. To investigate this, we conduct an experiment by collecting all responses containing arithmetic errors from all models under level 1 and 2 perturbations, and then extracting the specific arithmetic operations that were answered incorrectly. We subsequently prompt the models to solve these arithmetic operations in a standalone setting. 

As shown in Table \ref{tab:standalone}, while the extent varies, the models perform significantly better when the arithmetic task is isolated. We hypothesize that this phenomenon occurs because LLMs predominantly rely on memorization for arithmetic operations, since they train largely on standalone arithmetic data \cite{yuan2023well,yang2023gpt,maltoni2024arithmetic}. This results in degraded performance when these operations are integrated into a natural language context, which is out-of-distribution for the LLMs. 
%Natural language might also be working as a potential background noise that exacerbates the problem \cite{abedin2025arithmattack}. Further investigation is needed to validate this hypothesis.






% \begin{table}
%   \centering
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Model} & \textbf{Non-Logical Errors} \\
%     \hline
%     \verb|{\"a}|     & {\"a}           \\
%     \verb|{\^e}|     & {\^e}           \\
%     \verb|{\`i}|     & {\`i}           \\
%     \verb|{\.I}|     & {\.I}           \\
%     \verb|{\o}|      & {\o}            \\
%     \verb|{\'u}|     & {\'u}           \\
%     \verb|{\aa}|     & {\aa}           \\\hline
%   \end{tabular}
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Total Equations} & \textbf{Solved} \\
%     \hline
%     \verb|{\c c}|    & {\c c}          \\
%     \verb|{\u g}|    & {\u g}          \\
%     \verb|{\l}|      & {\l}            \\
%     \verb|{\~n}|     & {\~n}           \\
%     \verb|{\H o}|    & {\H o}          \\
%     \verb|{\v r}|    & {\v r}          \\
%     \verb|{\ss}|     & {\ss}           \\
%     \hline
%   \end{tabular}
%   \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
%   \label{tab:accents}
% \end{table}





% Previous studies do not try to identify arithmetic errors, often attributing to the fact that LLMs can handle numerical tasks within a specific range. However, we sought to assess whether the logical validity of responses remains intact when isolated from the numerical computation capabilities of the LLM. This also allows us to dive deeper into the kinds of arithmetic errors a model is making in the perturbed scenarios. Specifically, we want to test whether a model's innate ability to perform arithmetics in a specific range translates to MWP solving scenarios and validate previous work that have ignored arithmetic ability. 

% To this end, we conduct a qualitative check of the R1 responses from the Gemma model. Given that calculations are imbued within the natural language response in diverse formats, extracting these calculations poses significant challenges. Thus, we manually identify and extract the calculations that the LLM got wrong. We ask the model to directly solve these calculations without any additional context. For example, a substring like "Total: 78 cards + 60 cards + 19 cards + 68 cards = **215 cards**" in the LLM response will correspond to simply "78+60+19+68=?". We extract 116 valid equations from the responses and find that the model is able to correctly solve 20 of them when devoid of background context. Following are the two key insights:

% \begin{enumerate}
%     \item \textbf{LLM has a harder time solving multistep equations.} While the model generally gets 2 steps equations like A + B correct. It is much harder when there are more numbers and operators involved. All (or x amount) of the calculations that the LLM got wrong in this small numeric range were multi-step.

%     \item \textbf{Background context can impair arithmetic performance.} We found 17\% of the equations to be solvable by the LLM when background context is not a part of it. This suggests that background context may introduce distractions or ambiguity that hinder arithmetic accuracy. Although this effect could be model-specific, it indicates that in scenarios with minimal overall performance degradation, some errors may originate from arithmetic miscalculations. (Could there be a different effect when the numbers are larger and arithmetic is a bit difficult already? Will background context lead to even poor performance? If experiment done, add in this section...)
    
% \end{enumerate}




% \begin{quote}
% \begin{verbatim}
% \documentclass[11pt]{article}
% \end{verbatim}
% \end{quote}

% To load the style file in the review version:
% \begin{quote}
% \begin{verbatim}
% \usepackage[review]{acl}
% \end{verbatim}
% \end{quote}
% For the final version, omit the \verb|review| option:
% \begin{quote}
% \begin{verbatim}
% \usepackage{acl}
% \end{verbatim}
% \end{quote}

% To use Times Roman, put the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \end{verbatim}
% \end{quote}
% (Alternatives like txfonts or newtx are also acceptable.)

% Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

% Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.


\section{Conclusion}
In this work, we introduce GSM-Ranges, a benchmark designed to evaluate LLMs' reasoning abilities across diverse numerical scales. Additionally, we propose a novel grading methodology that classifies erroneous into logical and non-logical categories. Through extensive experiments on various models using GSM-Ranges and our grading framework, we find that logical accuracy tend to degrade significantly as perturbation level rises, revealing LLMs' sensitivity to numerical scales. Furthermore, while LLMs perform well on isolated arithmetic tasks, their accuracy declines significantly when calculations are integrated into natural language contexts. This study provides a more precise assessment of LLMs’ mathematical reasoning and paves the way for future research on improving mathematical reasoning capabilities and developing models that can generalize more effectively across diverse mathematical problem settings.

% error rises as perturbation level rises, uncovering the senst

\section{Limitations}
% Due to resource constraints, our experiment primarily focused on small, lightweight models. While we evaluated o3-mini, one of the most advanced models, further research is needed to assess the performance of other state-of-the-art models. Additionally, GSM8K is a well-studied and saturated benchmark where many models already perform well. Future studies should examine how varying numerical ranges influence performance in more complex mathematical tasks. Moreover, our proposed methodology remains distinguishing between logical and non-logical errors only. Further research is needed to provide a more granular analysis of error types.


Due to resource constraints, our study primarily focuses on small, lightweight models. While we have evaluated GPT-4o and o3-mini, future work could extend the analysis to other advanced models. Additionally, our perturbation study is conducted on the GSM8K dataset, and exploring the impact of varying numerical ranges on performance in more complex mathematical tasks would further enrich the findings. Lastly, while our grading methodology distinguishes between logical and non-logical errors, a more granular grading methodology could offer deeper insights into model performance and refinement.


% Authors are required to discuss the limitations of their work in a dedicated section titled “Limitations”. This section should be included at the end of the paper, before the references, and it will not count toward the page limit. This includes both, long and short papers. Papers without a limitations section will be desk rejected. Note, prior to the December 2023 cycle, this was optional.

% Please note that this section should not introduce new methods, analysis, or results. We reserve the right to desk reject the submissions that use this section to introduce more content that should have been part of the main paper. It can only discuss the limitations of the work presented in the main content of the paper.

% \section{Document Body}

% \subsection{Footnotes}

% Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

% \subsection{Tables and figures}

% See Table~\ref{tab:accents} for an example of a table and its caption.
% \textbf{Do not override the default caption sizes.}

% \begin{table}
%   \centering
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\"a}|     & {\"a}           \\
%     \verb|{\^e}|     & {\^e}           \\
%     \verb|{\`i}|     & {\`i}           \\
%     \verb|{\.I}|     & {\.I}           \\
%     \verb|{\o}|      & {\o}            \\
%     \verb|{\'u}|     & {\'u}           \\
%     \verb|{\aa}|     & {\aa}           \\\hline
%   \end{tabular}
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\c c}|    & {\c c}          \\
%     \verb|{\u g}|    & {\u g}          \\
%     \verb|{\l}|      & {\l}            \\
%     \verb|{\~n}|     & {\~n}           \\
%     \verb|{\H o}|    & {\H o}          \\
%     \verb|{\v r}|    & {\v r}          \\
%     \verb|{\ss}|     & {\ss}           \\
%     \hline
%   \end{tabular}
%   \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
%   \label{tab:accents}
% \end{table}

% As much as possible, fonts in figures should conform
% to the document fonts. See Figure~\ref{fig:experiments} for an example of a figure and its caption.

% Using the \verb|graphicx| package graphics files can be included within figure
% environment at an appropriate point within the text.
% The \verb|graphicx| package supports various optional arguments to control the
% appearance of the figure.
% You must include it explicitly in the \LaTeX{} preamble (after the
% \verb|\documentclass| declaration and before \verb|\begin{document}|) using
% \verb|\usepackage{graphicx}|.

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{example-image-golden}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:experiments}
% \end{figure}

% \begin{figure*}[t]
%   \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%   \includegraphics[width=0.48\linewidth]{example-image-b}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}

% \subsection{Hyperlinks}

% Users of older versions of \LaTeX{} may encounter the following error during compilation:
% \begin{quote}
% \verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
% \end{quote}
% This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

% \subsection{Citations}

% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%     \hline
%     \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%     \hline
%     \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%     \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%     \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%     \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
%     \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%     Citation commands supported by the style file.
%     The style is based on the natbib package and supports all natbib citation commands.
%     It also supports commands defined in previous ACL style files for compatibility.
%   }
% \end{table*}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% A possessive citation can be made with the command \verb|\citeposs|.
% This is not a standard natbib command, so it is generally not compatible
% with other style files.

% \subsection{References}

% \nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

% The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
% If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
% \begin{quote}
% \begin{verbatim}
% \bibliography{custom}
% \end{verbatim}
% \end{quote}

% You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
% To include both the Anthology and your own .bib file, use the following instead of the above.
% \begin{quote}
% \begin{verbatim}
% \bibliography{anthology,custom}
% \end{verbatim}
% \end{quote}

% Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

% \subsection{Equations}

% An example equation is shown below:
% \begin{equation}
%   \label{eq:example}
%   A = \pi r^2
% \end{equation}

% Labels for equation numbers, sections, subsections, figures and tables
% are all defined with the \verb|\label{label}| command and cross references
% to them are made with the \verb|\ref{label}| command.

% This an example cross-reference to Equation~\ref{eq:example}.

% \subsection{Appendices}

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

\section*{Acknowledgments}

We express our gratitude to Yik-Cheung (Wilson) Tam, Professor of Practice in Computer Science at New York University Shanghai, for his valuable advice in shaping the ideas for this study.
% \newpage

\bibliography{custom}


\newpage 
% \newpage

\onecolumn

\appendix

\section{Appendix}
\label{sec:appendix}

\subsection{Experiment Results}
\subsubsection{Logical Error Rates}

\begin{table}[h]
    \centering
    \begin{tabular}{l c c c c c c c}
        \hline
        \multirow{2}{*}{Model} & \multirow{2}{*}{Baseline} & \multicolumn{6}{c}{Perturbation Levels} \\
        & & Lv.1 & Lv.2 & Lv.3 & Lv.4 & Lv.5 & Lv.6 \\
        \hline
        Gemma 2 2B       & 18 & 24.3(0.8) & 30.9(0.6) & 32.3(0.7) & 35.1(0.6) & 36.8(0.8) & 38.5(0.8) \\
        GPT-3.5 Turbo   & 11 & 13.5(0.5) & 15.0(0.8) & 17.5(0.6) & 17.3(0.7) & 16.9(0.6) & 17.3(0.6) \\
        GPT-4o          & 4  & 5.1(0.4)  & 6.9(0.3)  & 6.9(0.3)  & 6.2(0.3)  & 5.0(0.3)  & 5.3(0.3)  \\
        Llama 3.2 3B    & 17 & 14.5(0.5) & 17.0(0.6) & 19.3(0.7) & 18.7(0.7) & 19.4(0.6) & 19.4(0.6) \\
        Mathtral 7B v0.1 & 7  & 9.0(0.5)  & 11.6(0.5) & 12.4(0.5) & 13.9(0.5) & 15.2(0.6) & 14.8(0.6) \\
        Mistral 7B v0.1  & 29 & 33.3(0.9) & 37.5(0.9) & 38.5(0.9) & 40.7(0.7) & 42.4(0.8) & 43.3(0.8) \\
        Phi 3 Mini 4K   & 10 & 7.7(0.4)  & 9.1(0.4)  & 10.7(0.4) & 11.2(0.5) & 11.2(0.5) & 12.2(0.5) \\
        Qwen 2.5 7B     & 4  & 5.0(0.4)  & 7.8(0.5)  & 9.0(0.5)  & 10.2(0.5) & 10.1(0.6) & 9.9(0.5)  \\
        Wizardmath 7B v1.1 & 7  & 8.1(0.5)  & 14.0(0.6) & 15.7(0.7) & 16.4(0.7) & 17.6(0.6) & 19.1(0.6) \\
        o3-mini & 5 & 5 & 6 & 5 & 4 & 6 & 4\\

        \hline
    \end{tabular}
    \caption{Logical error rates and confidence intervals across different GSM-Ranges perturbation levels.}
    \label{tab:logical}
\end{table}



\subsubsection{Non-Logical Error Rates}
\begin{table}[h]
    \centering
    \begin{tabular}{l c c c c c c c}
        \hline
        \multirow{2}{*}{Model} & \multirow{2}{*}{Baseline} & \multicolumn{6}{c}{Perturbation Levels} \\
        % \cline{3-8}
        & & Lv.1 & Lv.2 & Lv.3 & Lv.4 & Lv.5 & Lv.6 \\


        \hline
        Gemma 2 2B & 3  & 3.6(0.4)   & 14.7(0.9)  & 21.6(0.8)  & 25.9(0.9)  & 29.6(1.1)  & 37.2(1.2)  \\
        GPT-3.5 Turbo & 0  & 0.5(0.2)   & 5.1(0.5)   & 12.7(0.8)  & 18.1(0.8)  & 35.0(1.0)  & 38.6(1.0)  \\
        GPT-4o & 0  & 0.1(0.1)   & 0.8(0.2)   & 2.7(0.3)   & 3.6(0.3)   & 5.2(0.4)   & 5.2(0.6)   \\
        Llama 3.2 3B & 2  & 1.9(0.3)   & 25.1(1.1)  & 49.5(1.2)  & 59.1(1.2)  & 61.8(0.9)  & 68.8(0.9)  \\
        Mathtral 7B v0.1 & 2  & 2.0(0.4)   & 10.1(0.8)  & 14.8(0.8)  & 19.3(1.1)  & 23.0(0.9)  & 27.6(1.0)  \\
        Mistral 7B v0.1 & 12 & 9.3(0.5)   & 25.1(1.2)  & 31.0(1.1)  & 34.9(1.2)  & 38.6(1.2)  & 42.0(1.1)  \\
        Phi 3 Mini 4K & 1  & 0.5(0.2)   & 6.2(0.4)   & 10.5(0.7)  & 15.8(1.0)  & 21.4(0.9)  & 28.0(1.1)  \\
        Qwen 2.5 7B & 0  & 0.4(0.2)   & 3.8(0.5)   & 7.0(0.6)   & 9.9(0.7)   & 12.1(0.9)  & 16.4(0.9)  \\
        Wizardmath 7B v1.1 & 2  & 4.1(0.6)   & 15.5(0.7)  & 24.3(1.2)  & 31.0(1.0)  & 35.6(1.2)  & 42.5(1.3)  \\
        o3-mini & 0 & 0 & 0 & 0 & 1 & 2 & 0\\

        \hline
    \end{tabular}
    \caption{Non-logical error rates and \& confidence intervals across different GSM-Ranges perturbation levels.}
    \label{tab:nonlogical}
\end{table}

\newpage 
\subsubsection{Recall Rates for Correct Logics}
\begin{table}[h]
    \centering
    \begin{tabular}{l c c c c c c c c}
        \hline
        \multirow{2}{*}{Model} & \multirow{2}{*}{Sample Size} & \multicolumn{1}{c}{GSM8K} & \multicolumn{6}{c}{Perturbation Levels} \\
        & & \multicolumn{1}{c}{Baseline} & Lv.1 & Lv.2 & Lv.3 & Lv.4 & Lv.5 & Lv.6 \\
        % \hline
        \hline
        \multirow{4}{*}{Gemma 2 2B} 
        & 1  & 82 & 74 & 69 & 68 & 67 & 61 & 59\\
        & 8  & 92 & 89 & 87 & 87 & 87 & 84 & 84\\
        & 32 & 95 & 91 & 92 & 89 & 92 & 89 & 92\\
        & 48 & 95 & 92 & 93 & 92 & 92 & 92 & 94\\
        \hline
        \multirow{4}{*}{Mistral 7B v0.1} 
        & 1  & 66 & 66 & 61 & 60 & 51 & 59 & 50\\
        & 8  & 89 & 88 & 82 & 87 & 84 & 81 & 83\\
        & 32 & 97 & 93 & 90 & 93 & 93 & 93 & 93\\
        & 48 & 97 & 95 & 91 & 93 & 96 & 93 & 95\\
        \hline
        \multirow{4}{*}{Mathtral 7B v0.1} 
        & 1  & 90 & 85 & 82 & 86 & 85 & 86 & 84\\
        & 8  & 94 & 92 & 88 & 90 & 92 & 92 & 91\\
        & 32 & 96 & 93 & 88 & 92 & 94 & 92 & 94\\
        & 48 & 96 & 94 & 89 & 93 & 94 & 93 & 94\\
        \hline
        \multirow{4}{*}{Wizardmath 7B v1.1} 
        & 1  & 92 & 84 & 85 & 83 & 84 & 78 & 78\\
        & 8  & 98 & 95 & 93 & 94 & 92 & 90 & 94\\
        & 32 & 99 & 97 & 98 & 95 & 94 & 93 & 95\\
        & 48 & 99 & 97 & 98 & 96 & 94 & 93 & 95\\
        \hline
    \end{tabular}
    \caption{Recall rates across different sampling sizes and GSM-Ranges perturbation levels. We use }
    \label{tab:recall}
\end{table}



% \newpage

\subsubsection{Mean Token Counts of o3-mini Responses}

\begin{table}[h]
    \centering
    \begin{tabular}{lccccccc}
        \hline
        & Baseline & Level 1 & Level 2 & Level 3 & Level 4 & Level 5 & Level 6 \\
        \hline
        Mean Token Count & 252.8 & 287.6 & 340.6 & 378.8 & 429.3 & 501.0 & 579.8 \\
        \hline
    \end{tabular}
    \caption{Mean token counts across GSM-Ranges perturbation levels for o3-mini responses}
    \label{tab:mean_token_count}
\end{table}





\subsubsection{Number-Copy Error Analysis}
\label{subsec:number copy error}

\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Model} & \textbf{Lv. 4} & \textbf{Lv. 5} & \textbf{Lv. 6} \\
\hline
Qwen 2.5 7B      & 0 & 2 & 4 \\
Llama 3.2 3B      & 0 & 0 & 1 \\
Mathstral 7B v0.1  & 0 & 0 & 1 \\
Phi 3 Mini 4K      & 0 & 0 & 1 \\
Gemma 2 2B     & 0 & 0 & 0 \\
GPT-3.5 Turbo   & 0 & 0 & 0 \\
GPT-4o           & 0 & 0 & 0 \\
Mistral 7B v0.1    & 0 & 0 & 0 \\
Wizardmath 7B v1.1   & 0 & 0 & 0 \\


\hline
\end{tabular}
\caption{Occurrences
 of Number-Copy Errors in 100 Random Samples Across Levels 4, 5, and 6 for Each Model.}
\label{tab:number-copy-errors}
\end{table}
For each of the nine base models, we sampled 100 responses per level from the level 4, 5, and 6 perturbations to evaluate number-copy error rates. As shown in Table \ref{tab:number-copy-errors}, 4 out of the 8 base models exhibited number-copy errors under level 6 perturbation, while only one model showed errors under level 5, and none were observed at level 4.

\newpage
\subsection{Full Prompt for Inference}
The full prompt used for inferences in the experiments is shown below:
\begin{tcolorbox}[colback=blue!10, colframe=blue!50, title=Zero-shot Prompt for Inferences]
As an expert problem solver, solve the following mathematical question step by step. \\
Q: \texttt{\{Question\}} \\
A: Let’s think step by step.
\end{tcolorbox}

\newpage 

\newtcolorbox{promptbox}{
    colback=blue!5!white, % Background color
    colframe=blue!80!black, % Border color
    coltitle=black, % Title color
    fonttitle=\bfseries, % Title font
    sharp corners, % Sharp corners for the box
    width=\textwidth, % Spanning the full width of the page
    boxrule=0.5mm, % Thickness of the border
    left=5mm, % Left padding
    right=5mm, % Right padding
    top=2mm, % Top padding
    bottom=2mm, % Bottom padding
    breakable % Allows the box to span multiple pages if necessary
}



% Ensure the box spans both columns in a two-column format
\subsection{Python Code Generation Prompt}
\label{subsec:4o-prompt}

Below is the prompt provided to the GPT-4o model for translating LLMs' responses into Python code. 
% The sections highlighted in red are for addressing number copy errors, which are only applied during the grading of responses to problems under level 4, 5, and 6 perturbations. 
We introduce a step to verbalize the response logic prior to code generation, as this process is found to improve the alignment between the generated code and the original response. The temperature is set to 0 in the code generation process.

\begin{center}
\begin{tcolorbox}[colback=blue!10, colframe=blue!50, title=Python Code Generation Prompt, width=\textwidth]
You are tasked with writing Python code that replicates the logic described in a given response to a math problem.  
Your code must strictly follow the exact reasoning steps provided in the response, regardless of whether the logic is correct, inconsistent, or flawed.
\newline
\begin{enumerate}
    \item Do not fix or modify the reasoning described in the response, even if they seem incorrect or nonsensical.
    \item Develop a Python function named \texttt{solver()} that replicates the logic in the response exactly as described:
    \begin{itemize}
        \item Define and assign all necessary variables within the function.
        \item The function must not take any external arguments.
        \item The function must return the computed final numerical result.
    \end{itemize}
    \item Ensure that all arithmetic operations described in the response are explicitly written as code. Avoid directly copying the results of these operations or the final answer from the response.
    \item Refer to the list of numbers extracted from the question provided to ensure any copied numbers in the response match the original numbers.
    % \textcolor{red}{\item Refer to the list of numbers extracted from the question provided to ensure any copied numbers in the response match the original numbers.}
    \begin{itemize}
        \item If a number in the response is incorrectly copied (e.g., misrepresenting 1333785 as 133785 or 13333785), correct the number in your code and document the correction as a comment in the code.
        % \item \textcolor{red}{If a number in the response is incorrectly copied (e.g., misrepresenting 1333785 as 133785 or 13333785), correct the number in your code and document the correction as a comment in the code.}
    \end{itemize}
    \item Include an explanation in the \texttt{explain} field that describes the steps and logic from the response, regardless of correctness.
    \item Provide the output in the following format:
    \begin{verbatim}
    {
        “extracted_answer”: “<final numerical value of the answer>”,
        “explain”: “<detailed explanation of the response logic>”,
        “python_code”: “```python\n<generated Python function>\n```”
    }
    \end{verbatim}
\end{enumerate}

\begin{itemize}
    \item This is the list of numbers extracted from the question: \texttt{\{number\_list\}}.
    % \item \textcolor{red}{This is the list of numbers extracted from the question: \texttt{\{number\_list\}}.}
    \item This the response: \texttt{\{response\}}.
\end{itemize}

\end{tcolorbox}
\end{center}


\newpage
\subsection{Various Types of Additional Logical Errors in Level 6 Perturbation}
\label{subsec:error types}

We conducted a qualitative analysis of additional logical errors induced by increasing numerical values, focusing on cases with the greatest rise in errors across perturbation levels among the questions derived from the same base GSM8K questions for each model. Our findings reveal a diverse range of errors without a consistent pattern. Below, we present examples of logical errors exhibited under level 6 perturbation. While some errors were entirely absent at level 1 and emerged only at level 6, others, though rare at level 1, became significantly more frequent. The displayed LLM responses have been edited for readability, with any arithmetic errors corrected.

% \begin{center}
\begin{tcolorbox}[
    colback=blue!5, 
    colframe=blue!60!black, 
    title=Example 1: Missing Step Error, 
    sharp corners=south, 
    width=\textwidth
]

\textbf{Question:}  

Laurel's friend gave her \num{8852986} baby outfits that her child no longer needed. At her baby shower, Laurel received twice that amount in new baby outfits. Then, Laurel's mom gifted her with another \num{5309889} baby outfits. How many outfits does she have for her baby?

\vspace{0.5em}
\textbf{Ground Truth:}  
\begin{itemize}
    \item Laurel's friend gave her \num{8852986} outfits.
    \item At the baby shower, she received twice that amount:  
          \[
          2 \times 8852986 = 17705972
          \]
    \item Adding all outfits together:  
          \[
          8852986 + 17705972 + 5309889 = \mathbf{31868847}
          \]
\end{itemize}

\vspace{0.5em}
\textbf{LLM Response:}  
\begin{enumerate}
    \item Compute the baby shower outfits:  
          \[
          8852986 \times 2 = 17705972
          \]
    \item Compute the total outfits:  
          \[
          17705972 + 5309889 = \mathbf{23015861}
          \]
\end{enumerate}

\vspace{0.5em}
\textbf{Error:} 

The model ignores the original \num{8852986} outfits from Laurel's friend.

\end{tcolorbox}
% \end{center}

\newpage

\begin{center}
\begin{tcolorbox}[
    colback=blue!5, 
    colframe=blue!60!black, 
    title=Example 2: Operator Error, 
    sharp corners=south, 
    width=\textwidth
]

\textbf{Question:}  

Finn watches \num{9360266} tadpoles swimming in the pond. Suddenly, Finn sees \num{7180820} of them come out of hiding from under a lily pad, then sees \num{12947038} of them hide under a rock. How many tadpoles can Finn see in the pond now?

\vspace{0.5em}
\textbf{Ground Truth:}  
\begin{itemize}
    \item Initially, Finn sees \num{9360266} tadpoles.
    \item \num{7180820} tadpoles come out from hiding, so the new visible count is:  
          \[
          9360266 + 7180820 = 16541086
          \]
    \item Then, \num{12947038} tadpoles hide under a rock, reducing the visible count:  
          \[
          16541086 - 12947038 = \mathbf{3594048}
          \]
\end{itemize}

\vspace{0.5em}
\textbf{LLM Response:}  
\begin{enumerate}
    \item Compute the new visible tadpoles after some emerge:  
          \[
          9360266 + 7180820 = 16541186
          \]
    \item Since 12947038 more hide under the rock, add them as well:  
          \[
          16541186 + 12947038 = \mathbf{29488224}
          \]
\end{enumerate}

\vspace{0.5em}
\textbf{Error:}  

The model makes an operator error by incorrectly adding the \num{12947038} tadpoles that hid under the rock instead of subtracting them.

\end{tcolorbox}
\end{center}


\begin{center}
\begin{tcolorbox}[
    colback=blue!5, 
    colframe=blue!60!black, 
    title=Example 3: Contextual Value Error, 
    sharp corners=south, 
    width=\textwidth
]

\textbf{Question:}  

Gloria is shoe shopping when she comes across a pair of boots that fit her shoe budget. However, she has to choose between the boots and two pairs of high heels that together cost \num{4528570} dollars less than the boots. If one pair of heels costs \num{3392343} dollars and the other costs twice as much, how many dollars are the boots?

\vspace{0.5em}
\textbf{Ground Truth:}  
\begin{itemize}
    \item The second pair of heels costs:
          \[
          3392343 \times 2 = 6784686
          \]
    \item The total cost of both pairs of heels:
          \[
          6784686 + 3392343 = 10177029
          \]
    \item The boots cost \num{4528570} dollars more than both pairs of heels:
          \[
          10177029 + 4528570 = \mathbf{14705599}
          \]
\end{itemize}

\vspace{0.5em}
\textbf{LLM Response:}  
\begin{enumerate}
    \item Compute the second pair of heels' cost:
          \[
          3392343 \times 2 = 6784686
          \]
    \item Compute the total cost of both pairs of heels:
          \[
          6784686 + 3392343 = 10177029
          \]
    \item The total cost of the heels is \$10177029 less than the boots, so the boots' price is:
          \[
          10177029 + 10177029 = \mathbf{20354058}
          \]
\end{enumerate}

\vspace{0.5em}
\textbf{Error:}  

The model makes a contextual value error by mismatching the numerical value with the corresponding entity. Instead of using \num{4528570} dollars as the price difference between the boots and the two pairs of high heels, it incorrectly uses the total cost of the heels.

\end{tcolorbox}
\end{center}



\begin{center}
\begin{tcolorbox}[
    colback=blue!5, 
    colframe=blue!60!black, 
    title=Example 4: Question Misinterpretation Error, 
    sharp corners=south, 
    width=\textwidth
]

\textbf{Question:}  

Mary is \num{1922674} years younger than Joan, who is \num{2112084} years older than Jessa. If Jessa is \num{1840103} years old, what is the sum of the ages of the three girls?

\vspace{0.5em}
\textbf{Ground Truth:}  
\begin{itemize}
    \item Joan's age:
          \[
          1840103 + 2112084 = 3952187
          \]
    \item Mary's age:
          \[
          3952187 - 1922674 = 2029513
          \]
    \item Sum of all ages:
          \[
          1840103 + 3952187 + 2029513 = \mathbf{7821803}
          \]
\end{itemize}

\vspace{0.5em}
\textbf{LLM Response:}  
\begin{enumerate}
    \item Joan is \num{2112084} years old and Jessa is \num{1840103} years old.
    \item Mary is \num{1922674} years younger than Joan, which means:
          \[
          2112084 - 1922674 = 1919410
          \]
    \item Now we can find the sum of their ages:
          \[
          2112084 + 1840103 + 1919410 = 5861607
          \]
\end{enumerate}

\vspace{0.5em}
\textbf{Error:}  

The model misinterprets the question by incorrectly treating Joan's age difference from Jessa as her actual age. Instead of computing Joan's age based on her relationship to Jessa, it directly assigned \num{2112084} as Joan’s age.

\end{tcolorbox}
\end{center}





\begin{center}
\begin{tcolorbox}[
    colback=blue!5, 
    colframe=blue!60!black, 
    title=Example 5: Omitted Information Error, 
    sharp corners=south, 
    width=\textwidth
]

\textbf{Question:}  

Mary is \num{1922674} years younger than Joan, who is \num{2112084} years older than Jessa. If Jessa is \num{1840103} years old, what is the sum of the ages of the three girls?

\vspace{0.5em}
\textbf{Ground Truth:}  
\begin{itemize}
    \item Joan's age:
          \[
          1840103 + 2112084 = 3952187
          \]
    \item Mary's age:
          \[
          3952187 - 1922674 = 2029513
          \]
    \item Sum of all ages:
          \[
          1840103 + 3952187 + 2029513 = \mathbf{7821803}
          \]
\end{itemize}

\vspace{0.5em}
\textbf{LLM Response:}  
\begin{enumerate}
    \item Mary is \num{1922674} years younger than Joan, who is \num{2112084} years older than Jessa.
    \item This can be written as:  
          \[
          \text{Joan} = \text{Jessa} + 2112084
          \]
          \[
          \text{Mary} = \text{Joan} - 1922674 = \text{Jessa} + 2112084 - 1922674 = \text{Jessa} + 189410
          \]

    \item Substituting the expressions for Joan and Mary, we get:  
          \[
          \text{Mary} + \text{Joan} + \text{Jessa} = (\text{Jessa} + 2112084) + (\text{Jessa} + 189410) + \text{Jessa}
          \]
    \item Simplifying this expression, we get:  
          \[
          \text{Mary} + \text{Joan} + \text{Jessa} = 3 \times \text{Jessa} + 2301494
          \]
\end{enumerate}

\vspace{0.5em}
\textbf{Error:} 

The model makes an omitted information error by failing to utilize the given value of Jessa’s age (\num{1840103}) in the final computation. Instead of calculating the actual sum of their ages, it leaves the expression in terms of Jessa’s age without substitution, leading to an incomplete and incorrect result.

\end{tcolorbox}
\end{center}





\twocolumn

\end{document}

