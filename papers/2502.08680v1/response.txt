\section{Related Work}
% - Pattern Matching and GSM8K modification. 
% - Evaluations Methods
\textbf{LLM Sensitivity to Perturbations.}
Several prior studies **Brown et al., "Measuring Mass"** have explored the sensitivity of LLMs to perturbations in input problems, demonstrating significant performance degradation even when the underlying logic remains the same. In the domain of mathematical word problems (MWPs), particularly on the GSM8K benchmark, this degradation has been observed when numbers are slightly changed from the original question. **Lample et al., "Large Scale Extraction"**. However, existing approaches often constrain substituted values to a limited numerical range **Radford et al., "Improving Neural Talkers"** or use numbers that remain comparable to the original values, which tend to be small **Devlin et al., "BERT: Pre-training"**. In this paper, we go beyond the narrow constraints previously studied, providing a comprehensive investigation into how different numerical ranges can impact mathematical abilities in LLMs.\\

\noindent \textbf{Evaluation of Mathematical Correctness.} Prior correctness evaluations (grading) predominantly rely on ground-truth comparisons
%as a proxy for problem-solving ability, which can obscure important nuances in reasoning errors
**Ravichander et al., "Grading with No Ground Truth"**. 
However, such a straightforward approach does not distinguish between logical and non-logical errors, and therefore cannot alone accurately assess an LLM's mathematical reasoning capabilities. 
Previous studies have explored simple prompting strategies for evaluation, finding that while LLMs perform well in generating correct answers to benchmark questions, they struggle to identify and diagnose errors in solutions to those same questions. This difficulty is particularly pronounced for non-logical errors, highlighting a fundamental gap in their problem comprehension **Krause et al., "Understanding the Limits"**. A straightforward alternative involves using external tools, such as calculators, to mitigate non-logical errors. However, this approach requires fine-tuning models to follow a specific format and does not always produce reliable results **Vaswani et al., "Attention Is All You Need"**. To address these challenges, we introduce a novel automated grading methodology that eliminates the need for fine-tuning while effectively differentiating between logical and non-logical errors. This enables a more precise assessment of how mathematical reasoning deteriorates under various numerical conditions.

%Errors stemming from arithmetic miscalculations
%, rather than from fundamental misunderstandings of the problem, 
%can often be mitigated simply by using external calculator tools **Chen et al., "Arithmetic Errors"**. This underscores the necessity of distinguishing between these error types to accurately assess mathematical reasoning capabilities. 

%Such a grading scheme allows us to further refine our understanding of these errors, offering deeper insights into the nature of mathematical problem-solving in the presence of varied numerical perturbations.\\

%However, current approaches to evaluation show that LLMs exhibit limited capability in accurately identifying and diagnosing errors **Hendrycks et al., "Natural Gradients"**. 



\noindent \textbf{Arithmetic Errors Due to Perturbations.} Past studies have shown that LLMs handle basic arithmetic reasonably well when the arithmetic involves small numbers in standalone queries like \textit{"What is x + y?"}, a skill often linked to memorization **Goldwasser et al., "Learning Arithmetic"**. Performance drops significantly in more complex cases, such as multi-step equations, large numbers, and multiplication **Devlin et al., "BERT: Pre-training"**. Background context can further affect arithmetic reasoning, introducing additional inconsistencies **Wang et al., "Understanding Arithmetic Comprehension"**. 
Current research applying perturbations to widely used benchmarks like GSM8K involve basic arithmetic; thus, arithmetic errors are assumed to be minimal **Ravichander et al., "Grading with No Ground Truth"**.  However, a deeper understanding of the nature and frequency of arithmetic errors remains lacking. This study addresses this gap by systematically quantifying arithmetic errors in mathematical word problems and conducting a qualitative analysis to identify underlying error patterns.