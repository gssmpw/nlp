

\section{New Method: SDGAE}
In this section, we first revisit the mode structure of DiGAE and analyze its encoder graph convolution. We then propose a novel Spectral Directed Graph Auto-Encoder (SDGAE).

\subsection{Understand the Graph Convolution of DiGAE}
DiGAE~\cite{digae} is a graph auto-encoder designed for directed graphs. Its encoder graph convolutional layer is denoted as %is inspired by GCN~\cite{gcn} and denoted as
\begin{align}
\mS^{(\ell+1)} &= \sigma\left(\hat{\mD}_{\rm out}^{-\beta}\hat{\mA}\hat{\mD}_{\rm in}^{-\alpha}\mT^{(\ell)}\mW_{T}^{(\ell)}\right), \label{digae_s}\\
\mT^{(\ell+1)} &= \sigma\left(\hat{\mD}_{\rm in}^{-\alpha}\hat{\mA}^{\top}\hat{\mD}_{\rm out}^{-\beta}\mS^{(\ell)}\mW_{S}^{(\ell)}\right).\label{digae_t}
\end{align}
Here, $\hat{\mA} = \mA + \mI$ denotes the adjacency matrix with added self-loops, and $\hat{\mD}_{\rm out}$ and $\hat{\mD}_{\rm in}$ represent the corresponding out-degree and in-degree matrices, respectively. 
$\mS^{(\ell)}$ and $\mT^{(\ell)}$ denote the source and target embeddings at the $\ell$-th layer, initialized as $\mS^{(0)} = \mT^{(0)} = \mX$.
The hyperparameters $\alpha$ and $\beta$ are degree-based normalization factors, $\sigma$ is the activation function (e.g., $\mathrm{ReLU}$), and $\mW_{T}^{(\ell)}, \mW_{S}^{(\ell)}$ represents the learnable weight matrices.

%The motivation for this graph convolution stems from GCN~\cite{gcn} and the 1-WL~\cite{wl} algorithm connection to directed graphs. However, its underlying principles in the spectral domain remain unexplained, and the heuristic hyperparameters $\alpha$ and $\beta$ introduce significant challenges in tuning.

The design of this graph convolution is inspired by the connection between GCN~\cite{gcn} and the 1-WL~\cite{wl} algorithm for directed graphs. However, the underlying principles of this convolution remain unexplored,  leaving its meaning unclear. Additionally, the heuristic hyperparameters $\alpha$ and $\beta$ introduce significant challenges for parameter optimization.

\begin{figure}[t]
    \centering
   \includegraphics[width=75mm]{figure/sdgconv_demo.pdf}
   \vspace{-4mm}
    \caption{The bipartite representations of two toy directed graphs.}
    \label{fig:bg}
    \vspace{-1mm}
\end{figure}


\begin{figure*}[t]
    \centering
   \vspace{-3mm}
   \hspace{-3mm}
   \subfigure{
   \includegraphics[width=37mm]{fig_degree/degree_WikiCS.pdf}
   }
   \hspace{-7mm}
   \subfigure{
   \includegraphics[width=37mm]{fig_degree/degree_WikiCS-STRAP.pdf}
   }
   \hspace{-7mm}
   \subfigure{
   \includegraphics[width=37mm]{fig_degree/degree_WikiCS-DirGNN.pdf}
   }
   \hspace{-7mm}
   \subfigure{
   \includegraphics[width=37mm]{fig_degree/degree_WikiCS-MagNet.pdf}
   }
   \hspace{-7mm}
   \subfigure{
   \includegraphics[width=37mm]{fig_degree/degree_WikiCS-SDGAE.pdf}
   }
   
   \vspace{-3mm}
   \caption{Degree distribution of WikiCS graph and its reconstruction graphs.}
   \vspace{-3mm}
   \label{fig:degree_wiki}
\end{figure*}
%Given a directed graph $\gG$, its bipartite representation is denoted as $\mathcal{S}(\hat{\mA})$~\cite{book_digraph}. Specifically,

We revisit the graph convolution of DiGAE and observe it corresponds to the GCN convolution~\cite{gcn} applied to an undirected bipartite graph. Specifically, given a directed graph $\gG$, its bipartite representation~\cite{book_digraph} is expressed as:
\begin{equation}
    \mathcal{S}(\hat{\mA}):=
    \left[ 
        \begin{array}{cc}
            \mathbf{0} & \hat{\mA} \\
            \hat{\mA}^{\top} & \mathbf{0}
        \end{array}
    \right] \in \mathbb{R}^{2n \times 2n}.
\end{equation}
Here, $\mathcal{S}(\hat{\mA})$ is a block matrix consisting of $\hat{\mA}$ and $\hat{\mA}^{\top}$. It is evident that $\mathcal{S}(\hat{\mA})$ represents the adjacency matrix of an undirected bipartite graph with $2n$ nodes. Figure~\ref{fig:bg} provides two toy examples of directed graphs and their corresponding undirected bipartite graph representations. Notably, the self-loop in $\hat{\mA}$ serves a fundamentally different purpose than in GCN~\cite{gcn}. In this context, the self-loop ensures the connectivity of the bipartite graph. Without the self-loop, as illustrated in graphs (b) and (e) of Figure~\ref{fig:bg}, the graph structure suffers from significant connectivity issues. Based on this, we present the following lemma.

%It can be observed that $\mathcal{S}(\hat{\mA})$ represents the adjacency matrix of a bipartite graph with $2n$ nodes. 
%Figure~\ref{fig:bg} illustrates examples of two toy directed graphs turned into such undirected bipartite graphs. Notably, the self-loop in $\hat{\mA}$ serves a purpose fundamentally different from the one in GCN. Here, the self-loop ensures the connectivity of the bipartite graph. Without the self-loop, as seen in graphs (b) and (e) in Fig.~\ref{fig:bg}, the graph structure suffers from severe connectivity issues. Then, we present the following lemma to understand DiGAE.


\begin{lemma}\label{lemma_digae_conv}
When omitting degree-based normalization in Equations (\ref{digae_s}) and (\ref{digae_t}), the graph convolution of DiGAE is
\begin{equation*}
\left[\mS^{(\ell+1)},\mT^{(\ell+1)}\right]^{\top} =\sigma\left(\mathcal{S}(\hat{\mA})\left[\mS^{(\ell)}\mW_{S}^{(\ell)},\mT^{(\ell)}\mW_{T}^{(\ell)}\right]^{\top} \right).
\end{equation*}
\end{lemma}
The proof and its extension with degree-based normalization are provided in Appendix~\ref{app_proof_lemma_digae_conv}. Lemma~\ref{lemma_digae_conv} and extension in its proof reveal that the graph convolution of DiGAE essentially corresponds to the GCN convolution applied to the undirected bipartite graph $\mathcal{S}(\hat{\mA})$.

%, similar to the convolution mechanism in GCN~\cite{gcn}.



\subsection{Spectral Directed Graph Auto-Encoder (SDGAE)}
%Motivated by the understanding of DiGAE, we propose a new Spectral Directed Graph Auto-Encoder (SDGAE), which uses the polynomial weights of spectral-based GNNs~\cite{gprgnn,bernnet}. The convolutional layer of SDGAE is denoted as 

Building on our understanding of DiGAE, we identify two main drawbacks. 1) DiGAE struggles to use deep networks for capturing richer structural information due to excessive learnable weight matrices (as highlighted in a recent study on deep GCNs~\cite{pengover}). We validate this issue experimentally in the next section. 2) It is difficult to optimize parameters due to heuristic hyperparameters. To address these issues, we propose SDGAE, which uses polynomial weights inspired by spectral-based GNNs~\cite{gprgnn,bernnet} and incorporates symmetric normalization of the adjacency matrix. The convolutional layer of SDGAE is defined as: $\left[\mS^{(\ell+1)},\mT^{(\ell+1)}\right]^{\top} =$
\begin{equation}\label{eq:bg_A_hat}
    \left[ 
        \begin{array}{c}
            w^{(\ell)}_S \\
            w^{(\ell)}_T
        \end{array}
    \right] 
    \odot
    \left[ 
        \begin{array}{cc}
            \mathbf{0} & \Tilde{\mA} \\
            \Tilde{\mA}^{\top} & \mathbf{0}
        \end{array}
    \right]
    \left[ 
        \begin{array}{c}
            \mS^{(\ell)} \\
            \mT^{(\ell)}
        \end{array}
    \right] 
    +
    \left[ 
        \begin{array}{c}
            \mS^{(\ell)} \\
            \mT^{(\ell)}
        \end{array}
    \right],
\end{equation}
where $w^{(\ell)}_S,w^{(\ell)}_T \in \mathbb{R}$ are learnable scalar weights, initialized to one. 
$\Tilde{\mA}=\hat{\mD}_{\rm out}^{-1/2}\hat{\mA}\hat{\mD}_{\rm in}^{-1/2}$ denotes the normalized adjacency matrix, and lemma~\ref{le:norm} demonstrates that this normalization corresponds to the symmetric normalization of $\mathcal{S}(\hat{\mA})$. Each layer of this graph convolution performs weighted propagation over the normalized undirected bipartite graph $\mathcal{S}(\Tilde{\mA})$, with the addition of a residual connection.
%note that the below lemma~\ref{le:norm} understands that this regularization is essentially regularizing the adjacency matrix $\mathcal{S}(\hat{\mA})$ of the undirected graph. 
\begin{lemma}\label{le:norm}
    The symmetrically normalized block adjacency matrix $ \mD_{\mathcal{S}}^{-1/2} \mathcal{S}(\hat{\mA}) \mD_{\mathcal{S}}^{-1/2} = \mathcal{S}(\Tilde{\mA})$, where $\mD_{\mathcal{S}} = \mathrm{diag}(\hat{\mD}_{\rm out}, \hat{\mD}_{\rm in})$ is the diagonal degree matrix of \(\mathcal{S}(\hat{\mA})\).
\end{lemma}
The proof of Lemma~\ref{le:norm} is provided in Appendix~\ref{app_proof_le_norm}. For the initialization of $\mS^{(0)}$ and $\mT^{(0)}$ in the implementation, we follow spectral-based GNNs~\cite{gprgnn} and use two MLPs: $\mS^{(0)} = \mathrm{MLP}_S(\mX)$ and $\mT^{(0)} = \mathrm{MLP}_T(\mX)$. For the decoder, we apply the inner product $\sigma(\vs_u | \vt_v)$, as well as MLP scoring functions $\mathrm{MLP}(\vs_u \odot \vt_v)$ and $\mathrm{MLP}(\vs_u | \vt_v)$.

Table~\ref{tab:performance_comparison} presents SDGAE's results on DirLinkBench for the Hits@100 metric, where it achieves state-of-the-art performance on four of the seven datasets, while delivering competitive results on the remaining three. Furthermore, SDGAE significantly outperforms DiGAE. These results demonstrate the effectiveness of SDGAE for the directed link prediction task. Additional details and experimental settings for SDGAE are provided in Appendix~\ref{app_sdgae}.

%The proof of lemma~\ref{le:norm} is in Appendix~\ref{app_proof_le_norm}. In the implementation of SDGAE, for the initialization $\mS^{(0)}$ and $\mT^{(0)}$, we use two MLPs following spectral-based GNNs~\cite{gprgnn}, i.e., $\mS^{(0)}=\mathrm{MLP}_S(\mX)$ and $ \mT^{(0)}=\mathrm{MLP}_T(\mX)$. For the decoder function, we use
%inner product $\sigma(\vs_u \| \vt_v)$ and MLP score $\mathrm{MLP}(\vs_u \odot \vt_v)$, $\mathrm{MLP}(\vs_u \| \vt_v)$. In Table~\ref{tab:performance_comparison} we show the results of SDGAE on DirLinkBench; SDGAE achieves SOTA on top of four of the seven datasets, the other three achieve competitive results and SDGAE significantly outperforms of DiGAE. These results illustrates the effectiveness of SDGAE on directed connectivity prediction. We show more details and experimental setting of SDGAE in the appendix.


%This choice of decoder function differs from DiGAE, 



%and we will analyze the impact of various decoders in the next section.


%For the initialization $\mS^{(0)}$ and $\mT^{(0)}$, we use two MLPs following~\cite{appnp}, that is
%\begin{equation}
%    \mS^{(0)}=\mathrm{MLP}_S(\mX), \quad \mT^{(0)}=\mathrm{MLP}_T(\mX).
%\end{equation}
%The intuition behind SDGAE is to define a spectral domain graph convolution on the bipartite graph $\mathcal{S}(\hat{\mA})$ to generate source and target embeddings. This design allows us to use deeper propagation layers and simplifies adjacency matrix regularization. For the decoder function, we use $\sigma(\mathrm{MLP}(\vs_u \odot \vt_v))$ and $\sigma(\mathrm{MLP}(\vs_u \| \vt_v))$, where $\sigma$ represents the activation function (i.e., $\mathrm{Sigmoid}$). This choice of decoder function differs from DiGAE, and we will analyze the impact of various decoders in the next section. 

%Notably, most previous works have prioritized the design of the encoder while paying less attention to the decoder. We list more details of SDGAE in the Appendix.


