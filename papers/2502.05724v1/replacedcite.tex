\section{Related Work}
\label{app:related_work}
%Existing methods for directed link prediction can be broadly categorized into embedding methods and graph neural networks (GNNs). Here, we review these methods and the work related to this paper. 
Existing methods for directed link prediction can be broadly categorized into embedding methods and graph neural networks (GNNs). We review these methods below and discuss their relevance to our work.

\textbf{Embedding Methods} for directed graphs primarily aim to capture asymmetric relationships. Most approaches generate two vectors per node: a source embedding ($\vs_u$) and a target embedding ($\vt_u$). These embeddings are learned using either factorization or random walks. Factorization-based methods include HOPE____, which applies the Katz similarity____ followed by singular value decomposition (SVD)____, and AROPE____, which generalizes this idea to preserve arbitrary-order proximities. STRAP____ combines Personalized PageRank (PPR)____ scores from both the original and transposed graphs before applying SVD. Random-walk methods include APP____, which trains embeddings using PPR-based random walks, and NERD____, which samples nodes according to degree distributions. Additional techniques include DGGAN____ (adversarial training), ELTRA____ (ranking-based learning), and ODIN____ (degree bias separation). These methods all focus on generating source–target embeddings from graph structures for link prediction tasks.

%\textbf{Embedding Methods} for directed graphs focuses on capturing asymmetric relationships. Most methods assign two vectors per node: source ($\vs_u$) and target ($\vt_u$) embeddings. These are learned through two approaches: factorization and random walks. Factorization-based methods like HOPE____ use Katz____ similarity and apply SVD____ to derive embeddings. AROPE____ generalizes this by preserving arbitrary-order proximities. STRAP____ combines Personalized PageRank (PPR)____ scores from original and transposed graphs before applying SVD. Random-walk methods include APP____, which trains embeddings using PPR, and NERD____, which samples nodes based on degree distributions. Other approaches like DGGAN____ (adversarial training), ELTRA____ (ranking-based learning), and ODIN____ (degree bias separation) address challenges like structure preservation, ranking accuracy, and generalization. These methods all generate source-target embeddings from graph structures and use the embeddings for link prediction tasks.

\textbf{Graph Neural Networks} for directed graphs can be classified into four main categories based on their embedding strategies: 1) Source–target methods learn separate source and target embeddings per node. DiGAE____ applies GCN____ to both the adjacency and its transpose. CoBA____ aggregates source and target neighbors jointly. BLADE____ uses an asymmetric loss to generate dual embeddings from local neighborhoods. 2) Gravity-inspired methods are motivated by Newton’s law of universal gravitation, learning real-valued embeddings alongside a scalar mass parameter. Gravity GAE____ pairs a gravity-based decoder with a GCN-based encoder, while DHYPR____ extends it via hyperbolic collaborative learning. 3) Single real-valued methods learn a single real-valued embedding for each node. DGCN____ defines a directed Laplacian using first-/second-order proximities. MotifNet____ leverages motif-based Laplacians. DiGCN and DiGCNIB____ use Personalized PageRank (PPR) to generalize directed Laplacians. DirGNN____ develops a flexible convolution for any message-passing neural network (MPNN), and HoloNets____ applies holomorphic functional calculus, employing a network structure similar to DirGNN. 4) Complex-valued methods employ Hermitian adjacency matrices for learning complex embeddings. MagNet____ defines a magnetic Laplacian to design directed graph convolution. LightDiC____ extends MagNet’s convolution to large graphs using a decoupled approach. DUPLEX____ uses dual GAT encoders with Hermitian adjacency. Other directed GNNs include adapting Transformers for directed graphs____ and extending oversmoothing analyses to directed graphs____.
While these methods define various directed graph convolutions and propagation mechanisms, a fair evaluation in link prediction tasks is often missing. Many methods omit important baselines or suffer from setup limitations (e.g., label leakage). These challenges motivate the work presented in this paper.



%\textbf{Graph Neural Networks} for directed graphs can be further divided into four classes based on the types of embeddings they generate. First is the source-target methods, similar to embedding methods, employ specialized propagation mechanisms to learn distinct source and target embeddings for each node. DiGAE____ propose a simple directed graph auto-encoder using the GCN convolution____ to define convolution on adjacency and transpose of adjacency matrix for generating source-target embeddings. CoBA____ defines  collaborative bi-directional aggregation for source and target embeddings by aggregating from the counterparts of the source and target neighbors.
%BLADE____ employ an asymmetric loss function and learn dual embeddings by aggregating features from its neighborhood.
%Second, Gravity-inspired methods, inspired by Newton’s law of universal gravitation learn a real-valued embedding and a scaler mass parameter. Gravity GAE____ present a gravity-inspired decoder and GCN convolution as encoder. DHYPR____ enhance Gravity GAE by using hyperbolic collaborative learning from multi-ordered and partitioned neighborhoods. Third, Single real-valued methods follow a conventional approach by learning a single real-valued embedding. DGCN____ employs both first- and second-order proximity matrices to define directed Laplacian matrix and directed graph convolution. MotifNet____ examined several symmetric Laplacian matrices, each corresponding to different graph motifs. DiGCN____ utilizes the Personalized PageRank (PPR) matrix as a generalized Laplacian and integrates k-hop diffusion matrices. DirGNN____ devises graph convolutions that are compatible with any Message Passing Neural Network (MPNN) in the spatial domain. HoloNets____ formulated graph convolution operations based on holomorphic functional calculus, but its network stricture is similar to DirGNN. 
%Last, complex-valued methods leverage Hermitian adjacency matrices, learning complex-valued embeddings. MagNet____ introduces the magnetic Laplacian to capture the topology of directed graphs and to define graph convolution. LightDiC____  extend the MagNet convolution to large graphs by using decouple approches. DUPLEX____ employs a dual GAT encoder for directional neighbor modeling based on the Hermitian adjacency matrix. Other approaches like ____ adapted transformers for directed graph classification tasks,  ____ expanded the concept of oversmoothing to directed graphs. These GNNs define various graph convolutions for directed graphs, various propagation mechanisms. However, they lack a fair evaluation and comparison of the various methods in link prediction tasks, many of these works do not add important baselines, and the task setups suffers from significant problems such as label leakage. These moviates us to present the work in this paper.