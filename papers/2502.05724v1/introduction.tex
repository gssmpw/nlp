

\vspace{-6mm}
\section{Introduction}\label{intro}
A directed graph (or digraph) is a type of graph in which the edges between nodes have a specific direction. These graphs are often used to model real-world asymmetric relationships, such as ``following" and ``followed" in social networks~\citep{leskovec2016snap} or ``link" and ``linked" in web pages~\citep{Page1999ThePC}. Directed graphs reflect the inherent directionality of relationships and provide a more accurate representation of complex systems. Link prediction is a critical and common task for directed graphs with diverse real-world applications. Examples include predicting follower relationships in social networks~\cite{liben2003link}, recommending products in e-commerce~\cite{rendle2009bpr}, and detecting intrusions in network security~\cite{bhuyan2013network}.

%These graphs are powerful unstructured data that can model widespread real-world scenarios, particularly those involving asymmetric relationships. For example, on social networks, directed graphs capture interactions such as "following" on platforms such as Twitter or Instagram, where one user may follow another, but not vice versa~\citep{leskovec2016snap}. 
%In web pages, directed graphs represent hyperlinks, where one page links to another, but the reverse is not always true~\citep{Page1999ThePC}. Recommendation systems also use directed graphs to suggest products or services, with edges that point from users to items they have interacted with or liked~\citep{recommendation_systems_evaluating}. Reflecting the directionality of relationships, directed graphs provide a more accurate representation of complex systems, offering deeper insights into the dynamics of the world around us.


\begin{table}[t]

\centering
\caption{Overview of directed graph learning methods.} % and their Specifics.} %\(\vs_u, \vt_u \in \mathbb{R}^d\) represent the source and target embeddings of node \(u\); \(\vh_u \in \mathbb{R}^d\) and \(m_u \in \mathbb{R}^+\) denote the real-valued embedding and mass parameter of node \(u\); and \(\vz_u \in \mathbb{C}^d\) represents the complex embedding of node \(u\).}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|l|l}
\toprule
  Method & Name &Embedding \\ \midrule
\multirow{8}{*}{\shortstack{Embedding \\ Methods}}        
& HOPE~\citep{hope}    &$\vs_{u}, \vt_{u}$ \\
& APP~\citep{app} &$\vs_{u}, \vt_{u}$    \\
& AROPE~\citep{arope} &$\vs_{u}, \vt_{u}$ \\
& STRAP~\citep{strap} &$\vs_{u}, \vt_{u}$  \\
& NERD~\citep{nerd} &$\vs_{u}, \vt_{u}$   \\
& DGGAN~\citep{dggan} &$\vs_{u}, \vt_{u}$ \\
& ELTRA~\citep{eltra} &$\vs_{u}, \vt_{u}$  \\ 
& ODIN~\citep{odin} &$\vs_{u}, \vt_{u}$ \\
                                       \hline
\multirow{13}{*}{\shortstack{Graph Neural\\ Networks}}
&DiGAE~\citep{digae} &$\vs_{u}, \vt_{u}$      \\
&CoBA~\citep{coba}  &$\vs_{u}, \vt_{u}$ \\
&BLADE~\citep{blade} &$\vs_{u}, \vt_{u}$ \\

&Gravity GAE~\citep{gragae} &$\vh_{u}, m_{u}$ \\
&DHYPR~\citep{dhypr} &$\vh_{u}, m_{u}$ \\

&DGCN~\citep{dgcn-tong} &$\vh_{u}$ \\
&DiGCN \& DiGCNIB~\citep{digcn} &$\vh_{u}$       \\
&DirGNN~\citep{dirgnn} &$\vh_{u}$ \\
&HoloNets~\citep{holonets} &$\vh_{u}$\\
&NDDGNN~\citep{nddgnn} &$\vh_{u}$\\ 

&MagNet~\citep{magnet} &$\vz_{u}$       \\
&LightDiC~\citep{lightdic} &$\vz_{u}$   \\
&DUPLEX~\citep{duplex} &$\vz_{u}$   \\
                                      
                                      \bottomrule
\end{tabular}}
\label{digraph_methods}
\end{table}


\begin{table*}[ht]
\centering
\caption{A unified framework for directed link prediction methods, with detailed descriptions provided in Appendix~\ref{app_uni_fram}.}
\label{table_fram}
%\begin{tabular}{@{}l@{\quad}l@{\quad}l@{}}
\begin{tabular}{@{}lllllllll@{}}
\toprule
Encoder $\mathrm{Enc}(\cdot)$ 
 &\multicolumn{2}{l}{Embeddings $({\bm \theta_u}, {\bm \phi_u})$}
 &\multicolumn{6}{l}{Possible Decoder $\mathrm{Dec}(\cdot)$}\\
\midrule
Source-target  & $\vs_u = \bm{\theta}_u$,           & $\vt_u = \bm{\phi}_u$          & \multicolumn{2}{l}{$\sigma\bigl(\vs_u^{\top}\vt_v\bigr)$;} & \multicolumn{2}{l}{$\mathrm{LR}\bigl(\vs_u \odot \vt_v\bigr)$;} & \multicolumn{2}{l}{\quad$\mathrm{LR}\bigl(\vs_u \|\vt_v\bigr)$} \\[3.0pt]

Single real-valued  & $\vh_u = \bm{\theta}_u$,  & $\varnothing = \bm{\phi}_u$          & \multicolumn{2}{l}{$\sigma\bigl(\vh_u^{\top}\vh_v\bigr)$;} & \multicolumn{2}{l}{$\mathrm{MLP}\bigl(\vh_u \odot \vh_v\bigr)$;} & \multicolumn{2}{l}{\quad$\mathrm{MLP}\bigl(\vh_u \|\vh_v\bigr)$} \\[3.0pt]

Complex-valued  & \multicolumn{2}{l}{$\vz_u = \bm{\theta}_u\odot\exp\bigl(i\,\bm{\phi}_u\bigr)$}  & \multicolumn{2}{l}{$\mathrm{Direc}\bigl(\vz_u,\vz_v\bigr)$;} & \multicolumn{4}{l}{$\mathrm{MLP}\bigl(\bm{\theta}_u \|\bm{\theta}_v \|\bm{\phi}_u \|\bm{\phi}_v\bigr)$} \\[3.0pt]

Gravity-inspired  & $\vh_u = \bm{\theta}_u$,  &$m_u = g(\bm{\phi}_u)$         & \multicolumn{3}{l}{$\sigma\bigl(m_v - \lambda\log\|\vh_u-\vh_v\|_2^2\bigr)$;}              & \multicolumn{3}{l}{$\sigma\bigl(m_v - \lambda\log \bigl(\mathrm{dist}_{\mathbb{D}_c^{d'}}(\vh_u,\vh_v)\bigr)\bigr)$}  \\
\bottomrule
\end{tabular}
\end{table*}

Machine learning techniques have been extensively developed to enhance link prediction performance on directed graphs. Existing methods can be broadly categorized into \textbf{embedding methods}
and \textbf{graph neural networks (GNNs)}. Embedding methods aim to preserve the asymmetry of directed graphs by generating two separate embeddings for each node $u$: a source embedding $\vs_u$ and a target embedding $\vt_u$~\cite{eltra,odin}, which are also known as content/context representations~\cite{hope,strap}. GNNs, on the other hand, can be further divided into four classes based on the types of embeddings they generate. (1) Source-target methods, similar to embedding methods, employ specialized propagation mechanisms to learn distinct source and target embeddings for each node~\cite{digae,coba}. (2) Gravity-inspired methods, inspired by Newton's law of universal gravitation, learn a real-valued embedding
$\vh_u \in \mathbb{R}^d$ and a mass parameter $m_u \in \mathbb{R}^{+}$ for each node $u$~\cite{gragae,dhypr}. 
(3) Single real-valued methods follow a conventional approach by learning a single real-valued embedding $\vh_u \in \mathbb{R}^d$~\cite{dirgnn,nddgnn}.
(4) Complex-valued methods use Hermitian adjacency matrices, learning complex-valued embeddings $\vz_u \in \mathbb{C}^d$~\cite{magnet,duplex}.
We summarize these methods in Table~\ref{digraph_methods}.

%For an edge $(u, v)$, these methods use the source embedding $\vs_u$ of node $u$ and the target embedding $\vt_v$ of node $v$ to compute the probability $p(u,v) = \mathrm{Dec}(\vs_u, \vt_v)$. Here, $\vs_u, \vt_v \in \mathbb{R}^d$ are $d$-dimensional real-valued embeddings, and $\mathrm{Dec}(\cdot)$ represents the decoder function, commonly chosen as the inner product or logistic regression.



%Machine learning and deep learning techniques have been extensively developed to extract information from directed graphs. These methods can be broadly categorized into two types. The first category is traditional graph embedding methods, which generate embeddings by leveraging proximity relationships or matrix factorization. These approaches typically have few or no learnable parameters. To preserve the asymmetry of directed graphs, they often produce two separate embeddings for each node, which we refer to as dual embeddings. The second category is deep learning based on Graph Neural Networks (GNNs), including spectral graph convolutions, graph attention mechanisms, graph autoencoders, and others. These methods generally involve more trainable parameters and can utilize both the graph structure and node features to generate embeddings. In the related work section, we provide a detailed review of these methods and summarize them in Table~\ref{digraph_methods}.

%Although the methods described above have achieved promising results in directed link prediction, several challenges remain. %First, it is unclear what type of embedding is necessary to predict the probability of each edge and whether existing methods consistently achieve correct link predictions. There is a lack of comprehensive research that evaluates the overall expressiveness of these methods. 

Although the methods described above have achieved promising results in directed link prediction, several challenges remain. 
First, it is unclear what types of embedding are effective in predicting directed links, as there is a lack of comprehensive research assessing these methods' expressiveness.
Second, existing methods have not been fairly compared and evaluated, highlighting the need for a robust benchmark for directed link prediction. Current experimental setups face multiple issues, such as the omission of basic baselines (e.g. MLP shown in Figures~\ref{fig:dp} and~\ref{fig:ep}),  label leakage (illustrated in Table~\ref{tb:duplex}), class imbalance (Figure~\ref{fig:imbalance}), single evaluation metrics, and inconsistent dataset splits. We discuss these issues in detail in Section~\ref{issues}.






%To evaluate current methods, several experimental tasks have been adopted, including node classification~\citep{digcn, magnet, dirgnn}, link prediction~\citep{strap, eltra, digae, magnet, duplex}, and graph reconstruction~\citep{strap, eltra}. Among these, link prediction is particularly crucial as it directly reflects edge directionality and is widely applied in both traditional graph embedding techniques and GNN-based approaches. However, research on link prediction in directed graphs faces significant challenges. In this work, we reexamine the current benchmarks and experimental setups, highlighting several key issues. \underline{Describing main issues and showing example figures, TODO}.
%We discuss these issues in detail and provide a summary in Section 3.


In this paper, we first propose a unified learning framework for directed link prediction methods to assess the expressiveness of different embedding types. We show that dual embeddings, encompassing all existing types except single real-valued ones, are critical for effective directed link prediction. Meanwhile, we highlight the often-overlooked importance of decoder design in achieving better performance, as most research has primarily focused on encoders. To address the limitations of existing experimental setups, we present \textbf{DirLinkBench}, a robust new directed link prediction benchmark with comprehensive coverage (7 datasets, 15 baselines), standardized evaluation (unified splits, features, and tasks), and modular extensibility (supporting new datasets, decoders, and sampling strategies).

The results in DirLinkBench reveal that current methods struggle to achieve strong and firm performance across diverse datasets. Notably, a simple directed graph auto-encoder, DiGAE~\cite{digae}, outperforms others in general. We revisit DiGAE from a theoretical perspective and observe that its graph convolution is equivalent to the GCN~\cite{gcn} convolution on an undirected bipartite graph. Building on this insight, we propose \textbf{SDGAE}, a novel directed graph auto-encoder that learns propagation weights via spectral-based GNN~\cite{gprgnn} techniques. SDGAE achieves state-of-the-art results on four of the seven datasets and ranks highest on average. Finally, we investigate key factors influencing directed link prediction (e.g., input features, decoder design, and degree distribution) and conclude with open challenges to advance the field.
%SDGAE achieves state-of-the-art performance on four of the seven DirLinkBench datasets and attains the best average ranking. Finally, we conduct a thorough empirical analysis to explore key factors influencing directed link prediction, including input features, decoder design, and degree distribution preservation. We conclude by identifying open challenges and suggesting promising future research directions to advance the field.
%In this paper, we first propose a unified framework for link prediction in directed graphs. Using this framework, we analyze the expressiveness of different types of embeddings and demonstrate that dual embeddings are essential for accurate link prediction, while the design of an appropriate decoder function is equally critical. Then, we examine the issues of existing experimental setups for link prediction and introduce a standardized experimental benchmark. This benchmark includes datasets from various domains and incorporates multiple evaluation metrics. We evaluate a variety of methods on this benchmark and investigate key aspects of model training, such as feature inputs, negative sampling strategies, dataset splitting, and decoder design. The results reveal that existing methods struggle to achieve consistently strong performance across diverse datasets, highlighting significant room for future improvement. Finally, we outline several open problems to encourage further advancements in directed graph link prediction based on the insights presented in this paper.
We summarize our contributions as follows.
\begin{itemize}[topsep=0pt, partopsep=0pt, itemsep=0.0pt]
    \item We propose a unified framework to assess the expressiveness of directed link prediction methods, showing that dual embeddings and suitable decoders are critical.
    %We propose a unified framework for directed graph link prediction and analyze the expressiveness, showing that dual embeddings and appropriate decoder designs are critical for accurate predictions.
    %We propose a unified framework for directed graph link prediction, showing that dual embeddings and appropriate decoder designs are critical for accurate predictions.
    
    \item We introduce DirLinkBench, a robust new benchmark with comprehensive coverage and standardized evaluation for directed link prediction tasks.
    
    \item We propose a novel directed graph auto-encoder SDGAE, inspired by the theoretical insights of DiGAE and achieving state-of-the-art results on DirLinkBench.
    
    \item We empirically analyze the factors affecting the performance of directed link prediction and highlight open challenges for future research.
\end{itemize}





















%Graphs are powerful unstructured data capable of modeling various real-world scenarios, and they are pervasive in our daily lives, appearing in areas such as social networks, bioinformatics, and transportation systems. Graph Neural Networks (GNNs) are a deep learning method that has gained significant traction in recent years for analyzing graph data. They have been extensively applied across various domains, including recommendation systems~\cite{wu2022graph,gao2022graph}, drug discovery~\cite{jiang2021could, Uni-Mol}, social analysis~\cite{fan2019graph,liu2022federated}, and so on. Traditional GNNs~\cite{gcn,gat,graphsage} often simplify graph structures by converting them into undirected graphs to facilitate message passing and graph convolution operations. However, in practice, graph data often takes the form of directed graphs, as seen in social networks where a user (node) following another creates a directed edge. In this case, an edge from node \(i\) to node \(j\) indicates that user \(i\) follows user \(j\), but not necessarily vice versa. Traditional GNNs often fail to capture this crucial directional information.

%Recent years,

