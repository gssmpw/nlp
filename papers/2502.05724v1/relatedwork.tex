\section{Related Work}
\label{app:related_work}
%Existing methods for directed link prediction can be broadly categorized into embedding methods and graph neural networks (GNNs). Here, we review these methods and the work related to this paper. 
Existing methods for directed link prediction can be broadly categorized into embedding methods and graph neural networks (GNNs). We review these methods below and discuss their relevance to our work.

\textbf{Embedding Methods} for directed graphs primarily aim to capture asymmetric relationships. Most approaches generate two vectors per node: a source embedding ($\vs_u$) and a target embedding ($\vt_u$). These embeddings are learned using either factorization or random walks. Factorization-based methods include HOPE~\citep{hope}, which applies the Katz similarity~\cite{katz1953new} followed by singular value decomposition (SVD)\cite{golub2013matrix}, and AROPE\citep{arope}, which generalizes this idea to preserve arbitrary-order proximities. STRAP~\citep{strap} combines Personalized PageRank (PPR)~\citep{Page1999ThePC} scores from both the original and transposed graphs before applying SVD. Random-walk methods include APP~\citep{app}, which trains embeddings using PPR-based random walks, and NERD~\citep{nerd}, which samples nodes according to degree distributions. Additional techniques include DGGAN~\citep{dggan} (adversarial training), ELTRA~\citep{eltra} (ranking-based learning), and ODIN~\citep{odin} (degree bias separation). These methods all focus on generating source–target embeddings from graph structures for link prediction tasks.

%\textbf{Embedding Methods} for directed graphs focuses on capturing asymmetric relationships. Most methods assign two vectors per node: source ($\vs_u$) and target ($\vt_u$) embeddings. These are learned through two approaches: factorization and random walks. Factorization-based methods like HOPE~\citep{hope} use Katz~\cite{katz1953new} similarity and apply SVD~\cite{golub2013matrix} to derive embeddings. AROPE~\citep{arope} generalizes this by preserving arbitrary-order proximities. STRAP~\citep{strap} combines Personalized PageRank (PPR)~\cite{Page1999ThePC} scores from original and transposed graphs before applying SVD. Random-walk methods include APP~\citep{app}, which trains embeddings using PPR, and NERD~\citep{nerd}, which samples nodes based on degree distributions. Other approaches like DGGAN~\citep{dggan} (adversarial training), ELTRA~\citep{eltra} (ranking-based learning), and ODIN~\citep{odin} (degree bias separation) address challenges like structure preservation, ranking accuracy, and generalization. These methods all generate source-target embeddings from graph structures and use the embeddings for link prediction tasks.

\textbf{Graph Neural Networks} for directed graphs can be classified into four main categories based on their embedding strategies: 1) Source–target methods learn separate source and target embeddings per node. DiGAE~\citep{digae} applies GCN~\citep{gcn} to both the adjacency and its transpose. CoBA~\citep{coba} aggregates source and target neighbors jointly. BLADE~\citep{blade} uses an asymmetric loss to generate dual embeddings from local neighborhoods. 2) Gravity-inspired methods are motivated by Newton’s law of universal gravitation, learning real-valued embeddings alongside a scalar mass parameter. Gravity GAE~\citep{gragae} pairs a gravity-based decoder with a GCN-based encoder, while DHYPR~\citep{dhypr} extends it via hyperbolic collaborative learning. 3) Single real-valued methods learn a single real-valued embedding for each node. DGCN~\citep{dgcn-tong} defines a directed Laplacian using first-/second-order proximities. MotifNet~\citep{motifnet} leverages motif-based Laplacians. DiGCN and DiGCNIB~\citep{digcn} use Personalized PageRank (PPR) to generalize directed Laplacians. DirGNN~\citep{dirgnn} develops a flexible convolution for any message-passing neural network (MPNN), and HoloNets~\citep{holonets} applies holomorphic functional calculus, employing a network structure similar to DirGNN. 4) Complex-valued methods employ Hermitian adjacency matrices for learning complex embeddings. MagNet~\citep{magnet} defines a magnetic Laplacian to design directed graph convolution. LightDiC~\citep{lightdic} extends MagNet’s convolution to large graphs using a decoupled approach. DUPLEX~\citep{duplex} uses dual GAT encoders with Hermitian adjacency. Other directed GNNs include adapting Transformers for directed graphs~\citep{tf_digraph} and extending oversmoothing analyses to directed graphs~\citep{fractional}.
While these methods define various directed graph convolutions and propagation mechanisms, a fair evaluation in link prediction tasks is often missing. Many methods omit important baselines or suffer from setup limitations (e.g., label leakage). These challenges motivate the work presented in this paper.



%\textbf{Graph Neural Networks} for directed graphs can be further divided into four classes based on the types of embeddings they generate. First is the source-target methods, similar to embedding methods, employ specialized propagation mechanisms to learn distinct source and target embeddings for each node. DiGAE~\cite{digae} propose a simple directed graph auto-encoder using the GCN convolution~\cite{gcn} to define convolution on adjacency and transpose of adjacency matrix for generating source-target embeddings. CoBA~\cite{coba} defines  collaborative bi-directional aggregation for source and target embeddings by aggregating from the counterparts of the source and target neighbors.
%BLADE~\cite{blade} employ an asymmetric loss function and learn dual embeddings by aggregating features from its neighborhood.
%Second, Gravity-inspired methods, inspired by Newton’s law of universal gravitation learn a real-valued embedding and a scaler mass parameter. Gravity GAE~\cite{gragae} present a gravity-inspired decoder and GCN convolution as encoder. DHYPR~\cite{dhypr} enhance Gravity GAE by using hyperbolic collaborative learning from multi-ordered and partitioned neighborhoods. Third, Single real-valued methods follow a conventional approach by learning a single real-valued embedding. DGCN~\cite{dgcn-tong} employs both first- and second-order proximity matrices to define directed Laplacian matrix and directed graph convolution. MotifNet~\cite{motifnet} examined several symmetric Laplacian matrices, each corresponding to different graph motifs. DiGCN~\cite{digcn} utilizes the Personalized PageRank (PPR) matrix as a generalized Laplacian and integrates k-hop diffusion matrices. DirGNN~\cite{dirgnn} devises graph convolutions that are compatible with any Message Passing Neural Network (MPNN) in the spatial domain. HoloNets~\cite{holonets} formulated graph convolution operations based on holomorphic functional calculus, but its network stricture is similar to DirGNN. 
%Last, complex-valued methods leverage Hermitian adjacency matrices, learning complex-valued embeddings. MagNet~\cite{magnet} introduces the magnetic Laplacian to capture the topology of directed graphs and to define graph convolution. LightDiC~\cite{lightdic}  extend the MagNet convolution to large graphs by using decouple approches. DUPLEX~\cite{duplex} employs a dual GAT encoder for directional neighbor modeling based on the Hermitian adjacency matrix. Other approaches like ~\cite{tf_digraph} adapted transformers for directed graph classification tasks,  ~\cite{fractional} expanded the concept of oversmoothing to directed graphs. These GNNs define various graph convolutions for directed graphs, various propagation mechanisms. However, they lack a fair evaluation and comparison of the various methods in link prediction tasks, many of these works do not add important baselines, and the task setups suffers from significant problems such as label leakage. These moviates us to present the work in this paper.