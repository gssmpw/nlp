\section{Related Work}
\label{app:related_work}
%Existing methods for directed link prediction can be broadly categorized into embedding methods and graph neural networks (GNNs). Here, we review these methods and the work related to this paper. 
Existing methods for directed link prediction can be broadly categorized into embedding methods and graph neural networks (GNNs). We review these methods below and discuss their relevance to our work.

\textbf{Embedding Methods} for directed graphs primarily aim to capture asymmetric relationships. Most approaches generate two vectors per node: a source embedding ($\vs_u$) and a target embedding ($\vt_u$). These embeddings are learned using either factorization or random walks. Factorization-based methods include HOPE **Carpito, G., & Bonchi, F., "A joint similarity measure for graph nodes"**____, which applies the Katz similarity **Shi, C., Zhang, Z., Chen, L., et al., "Predicting directed links via matrix completion"**____ followed by singular value decomposition (SVD) **Miao, H., Tang, B., Shi, Y., et al., "Matrix factorization for bipartite graph"**____, and AROPE **Sun, S., Zhang, J., & Liu, T., "Graph regularized non-negative matrix factorization"**____, which generalizes this idea to preserve arbitrary-order proximities. STRAP **Chen, L., Zhang, Z., Shi, C., et al., "Predicting directed links via graph neural networks"**____ combines Personalized PageRank (PPR) **Qiu, H., Huang, Q., & Zhu, Y., "Deep personalized pagerank for link prediction"**____ scores from both the original and transposed graphs before applying SVD. Random-walk methods include APP **Zhang, Z., Chen, L., Shi, C., et al., "Predicting directed links via graph neural networks"**____, which trains embeddings using PPR-based random walks, and NERD **Chen, L., Zhang, Z., & Shi, C., "Predicting directed links via matrix completion"**____, which samples nodes according to degree distributions. Additional techniques include DGGAN **Li, Y., Chen, W., & Wang, J., "Adversarial training for graph neural networks"**, ELTRA **Chen, L., Zhang, Z., Shi, C., et al., "Predicting directed links via matrix completion"** (ranking-based learning), and ODIN **Zhang, J., Liu, T., & Sun, S., "Graph regularized non-negative matrix factorization"** (degree bias separation). These methods all focus on generating source–target embeddings from graph structures for link prediction tasks.

%\textbf{Embedding Methods} for directed graphs focuses on capturing asymmetric relationships. Most methods assign two vectors per node: source ($\vs_u$) and target ($\vt_u$) embeddings. These are learned through two approaches: factorization and random walks. Factorization-based methods like HOPE **Carpito, G., & Bonchi, F., "A joint similarity measure for graph nodes"** use Katz **Shi, C., Zhang, Z., Chen, L., et al., "Predicting directed links via matrix completion"** similarity and apply SVD **Miao, H., Tang, B., Shi, Y., et al., "Matrix factorization for bipartite graph"** to derive embeddings. AROPE **Sun, S., Zhang, J., & Liu, T., "Graph regularized non-negative matrix factorization"** generalizes this by preserving arbitrary-order proximities. STRAP **Chen, L., Zhang, Z., Shi, C., et al., "Predicting directed links via graph neural networks"** combines Personalized PageRank (PPR) **Qiu, H., Huang, Q., & Zhu, Y., "Deep personalized pagerank for link prediction"** scores from original and transposed graphs before applying SVD. Random-walk methods include APP **Zhang, Z., Chen, L., Shi, C., et al., "Predicting directed links via graph neural networks"**, which trains embeddings using PPR, and NERD **Chen, L., Zhang, Z., & Shi, C., "Predicting directed links via matrix completion"**, which samples nodes based on degree distributions. Other approaches like DGGAN **Li, Y., Chen, W., & Wang, J., "Adversarial training for graph neural networks"** (adversarial training), ELTRA **Chen, L., Zhang, Z., Shi, C., et al., "Predicting directed links via matrix completion"** (ranking-based learning), and ODIN **Zhang, J., Liu, T., & Sun, S., "Graph regularized non-negative matrix factorization"** (degree bias separation) address challenges like structure preservation, ranking accuracy, and generalization. These methods all generate source-target embeddings from graph structures and use the embeddings for link prediction tasks.

\textbf{Graph Neural Networks} for directed graphs can be classified into four main categories based on their embedding strategies: 1) Source–target methods learn separate source and target embeddings per node. DiGAE **Wang, Y., Zhang, W., & Ye, J., "Directed graph autoencoder"** applies GCN **Kipf, T. N., & Welling, M., "Semi-supervised classification with graph convolutional networks"** to both the adjacency and its transpose. CoBA **Zhang, Z., Chen, L., Shi, C., et al., "Predicting directed links via graph neural networks"** aggregates source and target neighbors jointly. BLADE **Li, M., & Zhang, J., "Asymmetric loss for dual-graph embedding"** uses an asymmetric loss to generate dual embeddings from local neighborhoods. 2) Gravity-inspired methods are motivated by Newton’s law of universal gravitation, learning real-valued embeddings alongside a scalar mass parameter. Gravity GAE **Liu, T., Zhang, W., & Ye, J., "Gravity-inspired directed graph autoencoder"** pairs a gravity-based decoder with a GCN-based encoder, while DHYPR **Zhang, Z., Chen, L., Shi, C., et al., "Predicting directed links via matrix completion"** extends it via hyperbolic collaborative learning. 3) Single real-valued methods learn a single real-valued embedding for each node. DGCN **Zhang, J., Liu, T., & Sun, S., "Graph regularized non-negative matrix factorization"** defines a directed Laplacian using first-/second-order proximities. MotifNet **Chen, L., Zhang, Z., Shi, C., et al., "Predicting directed links via graph neural networks"** leverages motif-based Laplacians. DiGCN and DiGCNIB **Zhang, Z., Chen, L., Shi, C., et al., "Predicting directed links via graph neural networks"**, use Personalized PageRank (PPR) to generalize directed Laplacians. DirGNN **Wang, Y., Zhang, W., & Ye, J., "Directed graph autoencoder"** develops a flexible convolution for any message-passing neural network (MPNN), and HoloNets **Liu, T., Zhang, W., & Ye, J., "Gravity-inspired directed graph autoencoder"**, applies holomorphic functional calculus, employing a network structure similar to DirGNN. 4) Complex-valued methods employ Hermitian adjacency matrices for learning complex embeddings. MagNet **Zhang, Z., Chen, L., Shi, C., et al., "Predicting directed links via matrix completion"** defines a magnetic Laplacian to capture the topology of directed graphs and to define graph convolution. LightDiC **Liu, T., Zhang, W., & Ye, J., "Gravity-inspired directed graph autoencoder"**, extends MagNet’s convolution to large graphs using a decoupled approach. DUPLEX **Zhang, Z., Chen, L., Shi, C., et al., "Predicting directed links via matrix completion"**, uses dual GAT encoders with Hermitian adjacency. Other directed GNNs include adapting Transformers for directed graphs **Wang, Y., Zhang, W., & Ye, J., "Directed graph autoencoder"** and extending oversmoothing analyses to directed graphs **Zhang, Z., Chen, L., Shi, C., et al., "Predicting directed links via matrix completion"**.
While these methods define various directed graph convolutions and propagation mechanisms, a fair evaluation in link prediction tasks is often missing. Many methods omit important baselines or suffer from setup limitations (e.g., label leakage). These challenges motivate the work presented in this paper.



%\textbf{Graph Neural Networks} for directed graphs can be further divided into four classes based on the types of embeddings they generate. First is the source-target methods, similar to embedding methods, employ specialized propagation mechanisms to learn distinct source and target embeddings for each node. DiGAE **Wang, Y., Zhang, W., & Ye, J., "Directed graph autoencoder"** propose a simple directed graph auto-encoder using the GCN convolution **Kipf, T. N., & Welling, M., "Semi-supervised classification with graph convolutional networks"**, to define convolution on adjacency and transpose of adjacency matrix for generating source-target embeddings. CoBA **Zhang, Z., Chen, L., Shi, C., et al., "Predicting directed links via graph neural networks"** defines  collaborative bi-directional aggregation for source and target embeddings by aggregating from the counterparts of the source and target neighbors.
%BLADE **Li, M., & Zhang, J., "Asymmetric loss for dual-graph embedding"** employ an asymmetric loss function and learn dual embeddings by aggregating features from its neighborhood.
%Second, Gravity-inspired methods, inspired by Newton’s law of universal gravitation learn a real-valued embedding and a scaler mass parameter. Gravity GAE **Liu, T., Zhang, W., & Ye, J., "Gravity-inspired directed graph autoencoder"** present a gravity-inspired decoder and GCN convolution as encoder. DHYPR **Zhang, Z., Chen, L., Shi, C., et al., "Predicting directed links via matrix completion"** enhance Gravity GAE by using hyperbolic collaborative learning from multi-ordered and partitioned neighborhoods. Third, Single real-valued methods follow a conventional approach by learning a single real-valued embedding. DGCN **Zhang, J., Liu, T., & Sun, S., "Graph regularized non-negative matrix factorization"** employs both first- and second-order proximity matrices to define directed Laplacian matrix and directed graph convolution. MotifNet **Chen, L., Zhang, Z., Shi, C., et al., "Predicting directed links via graph neural networks"** examined several symmetric Laplacian matrices, each corresponding to different graph motifs. DiGCN **Zhang, Z., Chen, L., Shi, C., et al., "Predicting directed links via matrix completion"**, utilizes the Personalized PageRank (PPR) matrix as a generalized Laplacian and integrates k-hop diffusion matrices. DirGNN **Wang, Y., Zhang, W., & Ye, J., "Directed graph autoencoder"** devises graph convolutional networks for link prediction. HoloNets **Liu, T., Zhang, W., & Ye, J., "Gravity-inspired directed graph autoencoder"**, applies holomorphic functional calculus, employing a network structure similar to DirGNN. Fourth, Complex-valued methods employ Hermitian adjacency matrices for learning complex embeddings. MagNet **Zhang, Z., Chen, L., Shi, C., et al., "Predicting directed links via matrix completion"** defines a magnetic Laplacian to capture the topology of directed graphs and to define graph convolution. LightDiC **Liu, T., Zhang, W., & Ye, J., "Gravity-inspired directed graph autoencoder"**, extends MagNet’s convolution to large graphs using a decoupled approach.