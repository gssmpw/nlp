
\begin{figure}[ht]
    \centering
   %\vspace{-1mm}
%   %\hspace{-3mm}  
   \includegraphics[width=82mm]{figure/feature_cora.pdf}
   \vspace{-8mm}
   \caption{Comparison of various methods' performance using either the original features or in/out degrees as inputs on Cora-ML.}
   \vspace{-1mm}
    \label{fig:feature_cora}
 \end{figure}

 




 
\section{Analysis}\label{analysis}
In this section, we empirically analyze the factors influencing directed link prediction performance and examine the properties of SDGAE.

%directed link prediction and SDGAE based on the results of DirLinkBench.

\subsection{Feature Inputs}
We investigate the impact of different feature inputs on GNNs for directed link prediction. Figure~\ref{fig:feature_cora} presents results on Cora-ML using either original features or in/out degrees as inputs, with additional results for other datasets provided in Appendix~\ref{app_feature_input}. Overall, original features enhance performance on most datasets; however, for datasets like WikiCS, in/out degrees prove more effective. Notably, in datasets lacking original features (e.g., Slashdot, Epinions), in/out degrees outperform random features. 
These findings emphasize the importance of proper feature inputs for improving GNN performance. Enhancing feature quality is a key research direction, especially for datasets with weak or missing original features.
% These findings highlight the significant influence of feature inputs on GNN performance and suggest that improving feature quality is a key research direction, particularly for datasets with weak or missing original features.
%We investigated how different feature inputs affect GNN methods for directed link prediction. Figure~\ref{fig:feature_cora} shows results on Cora-ML using either original features and in/out degree as inputs, with more results on other datasets provided in the Appendix. Generally, original features improve performance on most datasets, but for datasets like WikiCS, in/out degree works better. For datasets without original features (e.g., Slashdot, Epinions), in/out degree outperforms random features. These results show that feature inputs significantly impact GNNs' performance and suggest improving feature quality as a key research direction, especially for datasets with weak or no original features.


\subsection{Loss Function and Decoder}
We analyze the impact of different decoders and loss functions on directed link prediction methods. For embedding methods, we compare decoders such as logistic regression~\cite{eltra} and inner product~\cite{strap}. For single real-valued GNNs, we evaluate the effects of Cross-Entropy (CE) loss and Binary Cross-Entropy (BCE) loss using decoders like MLP scoring functions~\cite{magnet} and inner product. The corresponding results are provided in Appendix~\ref{app_loss_function}.

In Figure~\ref{fig:loss_magnet}, we compare the impact of using CE and BCE loss separately on MagNet's performance, showing that BCE loss consistently outperforms CE loss. This trend aligns with findings from other methods in Appendix~\ref{app_loss_function}. These results suggest shortcomings in previous approaches that relied on CE loss~\cite{dpyg}, as link prediction is fundamentally a binary classification task where BCE loss provides clear advantages~\cite{li2023evaluating}. %consistent with observations in undirected graph settings~\cite{li2023evaluating}.
In Figure~\ref{fig:loss_digae}, we compare two decoders—inner product and MLP score for DiGAE, and find that MLP score performs better on three datasets. These results above highlight two insights: 1) decoder design significantly impacts model performance, as emphasized in Corollary~\ref{coro_de}, and 2) BCE loss is more effective for link prediction tasks. Furthermore, the poor performance of MagNet and DUPLEX may be attributed to their reliance on CE loss and unsuitable decoders.





%In Figue~\ref{fig:loss_magnet}, we comparing the impact of using CE and  BCE loss separately on the performance of MagNet, showing BCE loss consistanly outperforms CE loss, aligning with other methods in Appendix~\ref{app_loss_function}. This suggests flaws in previous methods using CE loss~\cite{magnet,dpyg}, as link prediction is fundamentally a binary classification task where BCE loss offers clear advantages, consistent with undirected graph settings~\cite{li2023evaluating}. 
%In Figure~\ref{fig:loss_digae}, we compare two decoders,inner product and MLP score, and observe that MLP score performs better across three datasets. These findings confirm two key points: first, decoder design significantly impacts model performance, as highlighted in Corollary~\ref{coro_de}; second, BCE loss is more effective for link prediction tasks. Meanwhile, we believe that the main reason for the poor results of MagNet and DUPLEX may be caused by the fact that they both use CE loss and unsuitable decoders.

%In Figure~\ref{fig:loss}, we present the impact of loss functions and decoders on MagNet and DiGAE. For MagNet, BCE loss outperforms CE loss, aligning with results from single real-valued GNNs. This suggests flaws in previous methods using CE loss, as link prediction is fundamentally a binary classification task where BCE loss offers clear advantages, consistent with undirected graph settings. For DiGAE, we compare two decoders—Inner Product and MLP Score—and observe that MLP Score performs better across three datasets. These findings confirm two key points: first, decoder design significantly impacts model performance, as highlighted in Corollary~\ref{coro_de}; second, BCE loss is more effective for link prediction tasks.


\begin{figure}[t]
    \centering
   \vspace{-2mm}
   \hspace{-6mm}
   \subfigure[MagNet]{
   \label{fig:loss_magnet}
   \includegraphics[width=43mm]
   {figure/loss_magnet.pdf}
   }
   \hspace{-5mm}
   \subfigure[DiGAE]{
   \label{fig:loss_digae}
   \includegraphics[width=43mm]{figure/loss_digae.pdf}
   }
   \hspace{-6mm}
   \vspace{-3mm}
   \caption{The impact of loss functions and decoders.}
   \label{fig:loss}
   \vspace{-2mm}
\end{figure}

%MagNet-BCE DiGAE-MLP

\subsection{Degree Distribution}
%We analyze how well different models preserve the asymmetry of directed graphs by studying the degree distributions of reconstructed training graphs. Using each trained model, we compute edge probabilities and select the top-$m^{\prime}$ edges ($m^{\prime}$ is number of edges in training graph) to reconstruct the graph. In Figure~\ref{fig:degree_wiki}, we compare the true in- and out-degree distribution of the WikiCS, reconstructed graph from STRAP, DirGNN, MagNet, and SDGAE.
We assess how well different models preserve the asymmetry of directed graphs by analyzing their degree distributions. Following STRAP~\cite{strap}, we compute every edge probability for each model and select the top-$m^{\prime}$ edges—where $m^{\prime}$ is the number of edges in the training graph—to reconstruct it. Figure~\ref{fig:degree_wiki} compares the true in-/out-degree distributions of the WikiCS training graph with those of reconstructed graphs generated by four various methods. Results show that STRAP and SDGAE best preserve the degree distributions, with STRAP excelling at in-degrees, explaining its superior performance on WikiCS. DirGNN, using the decoder $\mathrm{MLP}\bigl(\vh_u \odot \vh_v\bigr)$, produces identical in-/out-degree distributions but still captures in-degrees correctly. In contrast, MagNet fails to learn valid distributions, leading to poor performance. Additionally, incorporating in-/out-degrees as feature inputs enhances degree distributions, as shown in Appendix~\ref{app_degree}. These findings underscore the need for GNNs to better preserve degree distributions, an underexplored challenge compared to embedding methods.

%for each model, we compute edge probabilities and select the top-$m^{\prime}$ edges—where $m^{\prime}$ matches the number of edges in the training graph—to reconstruct the training graph. In Figure~\ref{fig:degree_wiki}, we compare the true in- and out-degree distributions of the training WikiCS graph with those of the reconstructed graphs generated by STRAP, DirGNN, MagNet, and SDGAE. The results show that STRAP and SDGAE best preserve the degree distributions, with STRAP excelling at in-degrees, explaining its superior performance on WikiCS. DirGNN produces identical in-/out-degree distributions due to its decoder $\mathrm{MLP}\bigl(\vh_u \odot \vh_v\bigr)$ on WikiCS, but it still captures in-degrees correctly. In contrast, MagNet fails to learn valid degree distributions, leading to poor performance. We also find that using in/out degree as feature inputs improves the effect of degree distributions, as shown in Appendix~\ref{app_degree}. These results highlight the need for GNNs to better preserve in-/out-degree distributions, an area that remains underexplored compared to embedding methods.


\subsection{SDGAE versus DiGAE}
We investigate which aspects of SDGAE contribute to its performance gains over DiGAE. Figure~\ref{fig:layer_sdage} shows that SDGAE benefits from deep graph convolutions, achieving optimal performance at \(L=5\), whereas DiGAE's performance declines with increasing layers due to excessive learnable weight matrices~\cite{pengover}. Figure~\ref{fig:loss_digae} further demonstrates that improving DiGAE's decoder enhances performance, underscoring the importance of decoder design. Additionally, adjacency matrix normalization, $\hat{\mD}_{\rm out}^{-1/2}\hat{\mA}\hat{\mD}_{\rm in}^{-1/2}$, is not a decisive factor, as shown in Appendix~\ref{app_sdgae_digae}. Overall, our SDGAE's advantage over DiGAE stems primarily from the  effective deep graph convolutions and well-designed decoder.

%We analyze which aspects of SDGAE gain performance improvements over DiGAE. Figure~\ref{fig:layer_sdage} shows that SDGAE benefits from deeper graph convolutions, with optimal performance at layer \(L=5\), while DiGAE's performance drops as layers increase due to excessive
%learnable weight matrices~\cite{pengover}. Figure~\ref{fig:loss_digae} highlights that improving DiGAE's decoder boosts performance, supporting the importance of decoder design. Additionally, adjacency matrix normalization $\hat{\mD}_{\rm out}^{-1/2}\hat{\mA}\hat{\mD}_{\rm in}^{-1/2}$ is not a key factor, as shown in Appendix~\ref{app_sdgae_digae}. Overall, SDGAE outperforms DiGAE mainly due to the effective deeper layers and a suitable decoder.



\begin{figure}[t]
    \centering
   \vspace{-2.5mm}
%   %\hspace{-3mm}  
   \includegraphics[width=82mm]{figure/layer_digae_sdgae.pdf}
   \vspace{-8mm}
   \caption{Performance comparison of SDGAE and DiGAE on Cora-ML and CiteSeer with increasing convolutional layers.}
   \vspace{-1.5mm}
    \label{fig:layer_sdage}
 \end{figure}

\subsection{Metric and Negative Sampling Strategy}
In Appendix~\ref{app_metrci_sampling}, we compare baseline results across different metrics, showing that Accuracy, AUC, and AP exhibit small performance gaps between methods. Notably, many simple undirected graph GNNs achieving high accuracy. These findings suggest that ranking metrics better capture model performance for link prediction, as observed in undirected graphs~\cite{yang2015evaluating}. Additionally, we analyze the impact of negative edge sampling strategies during training and find that they significantly influence model performance. Exploring more sampling strategies, inspired by advancements in undirected graph link prediction~\cite{li2023evaluating}, could be a promising direction for future research.


%In Appendix~\ref{app_metrci_sampling}, we compare baseline results across different metrics, showing that Accuracy, AUC, and AP metrics have small performance gaps between methods, and many simple undirected graph GNNs achieve high accuracy. These results suggest that ranking metrics better capture model performance for link prediction, as is the case for undirected graphs~\cite{yang2015evaluating}. Additionally, we examine the impact of negative edge sampling strategies during training and found that they significantly affect model performance. Exploring more sampling strategies could be a promising direction for future research, considering advancements in undirected graph link prediction~\cite{li2023evaluating}.

%and observe that many simple undirected graph GNNs achieve high accuracy, with small performance gaps between methods. Both AUC and AP scores are consistently high, suggesting that ranking metrics may better capture model performance for link prediction, as is the case for undirected graphs. Additionally, we examined the impact of negative edge sampling strategies during training and found they significantly affect model performance. Exploring more sampling strategies could be a promising direction for future research, similar to advancements in undirected graph link prediction.


%\subsection{Data Split}










