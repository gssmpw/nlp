\begin{table*}[htbp]
    \centering
    \caption{Results of various methods under on the \textbf{Hits@100} metric (mean ± standard error\%). Results ranked \hig{1}{first}, \hig{2}{second}, and \hig{3}{third} are highlighted. TO indicates methods that did not finish running within 24 hours, and OOM indicates methods that exceeded memory limits.}
    \label{tab:performance_comparison}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{
        lcccccccr 
    }
        \toprule
        {Method} & {Cora-ML} & {CiteSeer} & {Photo} & {Computers} & {WikiCS} & {Slashdot} & {Epinions} & {Avg.\ Rank $\downarrow$} \\
        \midrule
        STRAP & 79.09$\pm$1.57 & 69.32$\pm$1.29 & \hig{1}{69.16$\pm$1.44} & \hig{2}{51.87$\pm$2.07} &\hig{1}{76.27$\pm$0.92} & 31.43$\pm$1.21 & \hig{1}{58.99$\pm$0.82} &\hig{3}{5.14}\\
        
        ODIN & 54.85$\pm$2.53 & 63.95$\pm$2.98 & 14.13$\pm$1.92 & 12.98$\pm$1.47 & 9.83$\pm$0.47 & 34.17$\pm$1.19 & 36.91$\pm$0.47 &12.71 \\
        
        ELTRA &\hig{2}{87.45$\pm$1.48} & 84.97$\pm$1.90 & 20.63$\pm$1.93 & 14.74$\pm$1.55  & 9.88$\pm$0.70 & 33.44$\pm$1.00  & 41.63$\pm$2.53 &8.57 \\ \midrule
        
        MLP        & 60.61$\pm$6.64 & 70.27$\pm$3.40 & 20.91$\pm$4.18 & 17.57$\pm$0.85 & 12.99$\pm$0.68 & 32.97$\pm$0.51 & 44.59$\pm$1.62 &10.57  \\ %\midrule
        
        GCN        & 70.15$\pm$3.01 & 80.36$\pm$3.07 & \hig{3}{58.77$\pm$2.96} & \hig{3}{43.77$\pm$1.75} & 38.37$\pm$1.51 & 33.16$\pm$1.22 & 46.10$\pm$1.37 &5.71 \\
        
        GAT        & 79.72$\pm$3.07 & \hig{3}{85.88$\pm$4.98} & 58.06$\pm$4.03 & 40.74$\pm$3.22 & 40.47$\pm$4.10  & 30.16$\pm$3.11 & 43.65$\pm$4.88 &5.86 \\
        
        APPNP      & 86.02$\pm$2.88 & 83.57$\pm$4.90 & 47.51$\pm$2.51 & 32.24$\pm$1.40 & 20.23$\pm$1.72 & 31.87$\pm$1.43 & 41.99$\pm$1.23 &8.00 \\ \midrule
        
        DGCN       & 63.32$\pm$2.59 & 68.97$\pm$3.39 & 51.61$\pm$6.33 & 39.92$\pm$1.94 & 25.91$\pm$4.10 & TO               & TO               &10.29  \\
        
        DiGCN      & 63.21$\pm$5.72 & 70.95$\pm$4.67 & 40.17$\pm$2.38 & 27.51$\pm$1.67 & 25.31$\pm$1.84 & TO               & TO               &11.29 \\
        
        DiGCNIB    & 80.57$\pm$3.21 & 85.32$\pm$3.70 & 48.26$\pm$3.98 & 32.44$\pm$1.85 & 28.28$\pm$2.44 & TO               & TO               &8.43  \\
        
        DirGNN     & 76.13$\pm$2.85 & 76.83$\pm$4.24 & 49.15$\pm$3.62 & 35.65$\pm$1.30 & \hig{3}{50.48$\pm$0.85} &\hig{3}{41.74$\pm$1.15} &50.10$\pm$2.06 &6.00 \\ \midrule
        
        MagNet     & 56.54$\pm$2.95 & 65.32$\pm$3.26 & 13.89$\pm$0.32 & 12.85$\pm$0.59 & 10.81$\pm$0.46  & 31.98$\pm$1.06 & 28.01$\pm$1.72 &13.14  \\
        
        DUPLEX &69.00$\pm$2.52	&73.39$\pm$3.42	&17.94$\pm$0.66	&17.90$\pm$0.71	&8.52$\pm$0.60	&18.42$\pm$2.59	&16.50$\pm$4.34  &12.14\\ \midrule
        
        DHYPR & \hig{3}{86.81$\pm$1.60} & \hig{2}{92.32$\pm$3.72} & 20.93$\pm$2.41 &TO &TO &OOM/TO &OOM/TO &10.57\\
        
        DiGAE & 82.06$\pm$2.51 & 83.64$\pm$3.21 & 55.05$\pm$2.36 & 41.55$\pm$1.62 & 29.21$\pm$1.36 &\hig{2}{41.95$\pm$0.93} & \hig{3}{55.14$\pm$1.96} & \hig{2}{4.43}\\ \midrule
        
        SDGAE & \hig{1}{90.37$\pm$1.33} &\hig{1}{93.69$\pm$3.68} &\hig{2}{68.84$\pm$2.35} &\hig{1}{53.79$\pm$1.56} &\hig{2}{54.67$\pm$2.50} &\hig{1}{42.42$\pm$1.15} &\hig{2}{55.91$\pm$1.77} &\hig{1}{1.43}\\
        \bottomrule
        
    \end{tabular}}
\end{table*}

\section{New Benchmark: DirLinkBench}
%In this section, we introduce a new unified benchmark for the directed link prediction task, abbreviated as \textbf{DirLinkBench}. This benchmark comprises seven graphs of varying sizes sourced from various fields. We carefully select and report the baseline results on this benchmark.
%\subsection{Dataset}
In this section, we introduce a novel robust benchmark for directed link prediction, \textbf{DirLinkBench}, offering three principal advantages: 1) \underline{Comprehensive coverage}, incorporating seven real-world datasets spanning diverse domains and 15 baseline models; 2) \underline{Standardized evaluation}, establishing a unified framework for dataset splitting, feature initialization, and task settings to ensure fairness and reliability; and 3) \underline{Modular extensibility}, built on PyG~\cite{pyg} to enable easy integration of new datasets, model architectures, and configurable modules (e.g., feature initialization, decoder types, and negative sampling). We then detail the implementation of DirLinkBench.

\textbf{Dataset}. We select seven publicly available directed graphs from diverse domains. These datasets include two citation networks, Cora-ML~\cite{mccallum2000cora_ml,bojchevski2018cora_ml} and CiteSeer~\cite{sen2008citeseer}; two co-purchasing networks, Photo and Computers~\cite{shchur2018computerandphoto}; a weblink network, WikiCS~\cite{mernyei2020wiki}; and two social networks, Slashdot~\cite{ordozgoiti2020slash} and Epinions~\cite{massa2005epinion}. These directed graphs differ in both size and average degree. %ranging from CiteSeer with the smallest average degree (3.5) to WikiCS with the largest (51.3). a
Except for Slashdot and Epinions, each dataset includes original node features. We provide the statistical details and additional descriptions in Appendix~\ref{app_dataset_baseline}.
%Table~\ref{dataset_info} provides statistical details of these datasets, and additional descriptions are included in the Appendix. 
To establish standard evaluation conditions for link prediction methods, we preprocess these datasets by eliminating duplicate edges and self-loop links. Following~\cite{appnp,shchur2018computerandphoto}, we also removed isolated nodes and used the largest
weakly connected component. %As a result, all directed graphs in DirLinkBench are weakly connected.



%\subsection{Setup}
\textbf{Task setup}. %Unlike previous settings, such as those in~\cite{magnet,dpyg}, which define link prediction for directed graphs as multiple subtasks, 
%Unlike the multiple subtask setup, which defines directed link prediction as a multi-class classification problem.
We simplify directed link prediction to a binary classification task: the model determines whether a directed edge exists from $u$ to $v$. 
%For any directed edge $(u, v)$, the goal is to predict whether an edge exists from $u$ to $v$. 
Specifically, given a preprocessed directed graph $\gG$, we randomly split 15\% of edges for testing, 5\% for validation, and use the remaining 80\% for training, and ensure that the training graph $\gG^{\prime}$ remains weakly connected~\cite{dpyg}. For testing and validation, we sample an equal number of negative edges under the full graph $\gG$ visible, while for training, only the training graph $\gG^{\prime}$ is visible. To ensure fairness, we generate 10 random splits using fixed seeds, and all models share the same splits. The model learns from the training graphs and feature inputs to compute $p(u, v)$ for test edges.
%For a link prediction model, it learns from the training set and feature inputs, then computes $p(u, v)$ for the edges in the test set. 
Feature inputs are provided in three forms: original node features, in/out degrees from the training graph $\gG^{\prime}$~\cite{magnet}, or a random normal distribution matrix~\cite{duplex}.

%\subsection{Baseline}
\textbf{Baseline}. We carefully select 15 state-of-the-art baselines, including three embedding methods: STRAP~\cite{strap}, ODIN~\cite{odin}, ELTRA~\cite{eltra}; a basic method MLP; three classic undirected GNNs: GCN~\cite{gcn}, GAT~\cite{gat}, APPNP~\cite{appnp}; four single real-valued methods: DGCN~\cite{dgcn-tong}, DiGCN~\cite{digcn}, DiGCNIB~\cite{digcn}, DirGNN~\cite{dirgnn}; two complex-valued methods: MagNet~\cite{magnet}, DUPLEX~\cite{duplex}; a gravity-inspired method: DHYPR~\cite{dhypr}; and a source-target GNN: DiGAE~\cite{digae}. We exclude some recent approaches (e.g., CoBA~\cite{coba}, BLADE~\cite{blade}, NDDGNN~\cite{nddgnn}) due to unavailable code.
%Note that some recent methods, such as CoBA~\cite{coba}, BLADE~\cite{blade}, and NDDGNN~\cite{nddgnn}, are not included due to unavailable code. 

\textbf{Baseline Setting}. 
For the baseline implementations, we rely on the authors’ original released code or popular libraries like PyG and the PyTorch Geometric Signed Directed library~\cite{dpyg}. For methods without released link-prediction code (e.g., GCN, DGCN), we provide various decoders and loss functions, while for methods with available link-prediction code (e.g., MagNet, DiGAE), we strictly follow the reported settings. We tune hyperparameters using grid search, adhering to the configurations specified in each paper. Further details of each method are in Appendix~\ref{app_dataset_baseline}.
%For the implementation of baselines, we use the original codes released by the authors, and implementations from some popular libraries, such as PyG and the PyTorch Geometric Signed Directed library~\cite{dpyg}. %For other baselines, we utilize the original code released by the authors. 
%For methods without released code for the link prediction task, such as GCN, DGCN, etc., we designed different decoders and loss functions. For methods with available link prediction task codes, such as MagNet, DiGAE, etc., we strictly follow the reported settings. Hyperparameters were carefully selected using grid search, adhering to the configurations specified in the respective papers. More setting details are provided in the Appendix.

%\subsection{Results}
%\subsection{Metric}

\textbf{Results.} 
We report results on seven metrics—Hits@20, Hits@50, Hits@100, MRR, AUC, AP, and ACC (detailed metric descriptions are provided in Appendix~\ref{app_bench_metric}), with complete results for each dataset in Appendix~\ref{app_complete_res}. Table~\ref{tab:performance_comparison} highlights the Hits@100 results. These results first show that embedding methods retain a strong advantage even without feature inputs. Second, early single real-valued undirected and directed GNNs also perform competitively, while newer directed GNNs (e.g., MagNet, DUPLEX, DHYPR) exhibit weaker performance or scalability issues. This is because complex-based methods have unsuitable loss functions and decoders, while DHYPR’s preprocessing requires $O(Kn^3)$ time and $O(Kn^2)$ space complexity. We provide further analysis in Sec.~\ref{analysis}. Notably, DiGAE, a simple directed graph auto-encoder, emerges as the best performer overall, yet it underperforms on certain datasets (e.g., Cora-ML, CiteSeer), leading to a worse average ranking. This observation motivates us to revisit DiGAE’s design and propose new methods to enhance directed link prediction.

%We report results in seven metrics: Hits@20, Hits@50, Hits@100, MRR, AUC, AP, and ACC (see Appendix~\ref{app_bench_metric} for these metrics' instructions).  See Appendix~\ref{app_complete_res} for complete results for each metric on each dataset.Table~\ref{tab:performance_comparison} shows the results for the Hits@100 metric. According to these results, we first observe that embedding methods retain a significant advantage without using feature inputs. Second, the early single real-valued undirected GNNs and directed GNNs also have competitive performance. In contrast, the latest directed GNNs show poor performance or scalability, such as DUPLEX and DHYPR. After our analysis, we found that the poor performance of complex-based methods (e.g., MagNet, DUPLEX) is mainly due to their unsuitable loss function and decoder, while the unscalability of DHYPR is due to its preprocessing stage with $O(Kn^3)$ time complexity and $O(Kn^2)$ space complexity. We will further analyze these results in Section 5.  Notably, the best-performing baseline is DiGAE, a simple graph auto-encoder for directed graphs. Although it is the best overall performer, it is worse on certain datasets, such as Cora-ML and CiteSeer, resulting in a poor average ranking. This motivates us to revisit the model design of DiGAE to propose new methods for improving the performance of directed link prediction.


%We report results in seven metrics: Hits@20, Hits@50, Hits@100, MRR, AUC, AP, and ACC (see the Appendix for details). Table~\ref{tab:performance_comparison} highlights the results for the Hits@100 metric. Notably, we observe that embedding methods retain a significant advantage, even without using node features, while traditional undirected graph GNNs also perform well. Surprisingly, the latest GNNs designed specifically for directed graphs show no clear advantage, prompting a re-evaluation of their current design.

%In the following sections, we will first analyze the best performing GNN method, DiGAE~\cite{digae}, from the spectral domain and propose a new method, the Spectral Directed Graph Autoencoder. We will then examine the trends of existing results in detail, provide key observations, and pose several open questions.





%\textbf{Dataset}:
%We select seven directed graphs from various domains to perform the link prediction task. These include the citation networks CoraML and Citeseer, the co-purchasing networks Computer and Photo, the Weblink network WikiCS, and the social networks Slashdot and Epinions. We show the statistical details of these datasets in Table~\ref{dataset_info}.



