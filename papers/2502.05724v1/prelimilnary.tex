





%\newpage
%\section{Preliminaries}

%\textbf{Notation}. We consider a directed unweighted graph $\gG=(V,E)$ with node set $V$ and edge set $E$. Let $n=|V|$ and $m=|E|$ denote the number of nodes and edges in $\gG$. We use $\mA$ to denote the adjacency matrix of $G$, with $\mA_{ij} = 1$ if there is a directed edge from node $i$ to $j$, and $0$ otherwise. We use $\mD_{\rm out} = {\rm diag}(\mA\vone) $ and $\mD_{\rm int} = {\rm diag}(\mA^{\top}\vone)$ to denote the out- and in-degree matrix of $\mA$, respectively, where $\vone$ is the all-in-one vector. Let $\mX \in \mathbb{R}^{n \times d}$ denote the feature matrix of the node, where each node is described by a vector of dimensions $d$.




%Let $\hat{\mathbf{A}}=\mathbf{A}+\mathbf{I}$ denote the adjacency matrix with self-loops, and we use $\hat{\mathbf{D}}_{\rm in}$ and $\hat{\mathbf{D}}_{\rm out}$ to denote the in- and out-degree matrix of $\hat{\mathbf{A}}$. We use $\mathbf{x}\in \mathbb{R}^n$ to denote the graph attributes, where $\mathbf{x}(i)$ denotes the attribute at node $i$. Note that in the general case of GNNs where the input feature is a matrix $\mathbf{X}\in \mathbb{R}^{n\times f}$, we can treat each column of $\mathbf{X}$ as a graph attribute.

%\textbf{Vanilla Graph Convolutions}~\cite{chebnet, gcn}. Originally defined for undirected graphs, vanilla graph convolutions utilize the adjacency matrix, denoted as $\mathbf{A}_{\rm u}$, of the undirected graph transformed from $G$. The normalized Laplacian matrix is represented as $\mathbf{L}_{\rm u} = \mathbf{I} - \mathbf{D}_{\rm u}^{-1/2} \mathbf{A}_{\rm u} \mathbf{D}_{\rm u}^{-1/2}$, where $\mathbf{D}_{\rm u}$ is the degree matrix of $\mathbf{A}_{\rm u}$. It is established that $\mathbf{L}_{\rm u}$ is a positive semidefinite matrix. The eigendecomposition of $\mathbf{L}_{\rm u}$ is expressed as $\mathbf{L}_{\rm u} = \mathbf{U} \mathbf{\Lambda} \mathbf{U}^T$. Consequently, vanilla graph convolutions are further approximable by the $K$-th order polynomial of Laplacians
%\begin{equation}\label{eq:van_GC}
%    \mathbf{y}=\mathbf{U}h \left(\mathbf{\Lambda}\right)\mathbf{U}^T\mathbf{x} = h(\mathbf{L})\mathbf{x} \approx \sum\limits_{k=0}^K w_k \mathbf{L}_{\rm u}^k\mathbf{x},
%\end{equation}
%where $h(\lambda)$ is the spectral filter, which is a function of eigenvalues of the Laplacian matrix $\mathbf{L}_{\rm u}$. The $w_k$ denotes the polynomial filter weights. Many studies have concentrated on learning and approximating Equation~\eqref{eq:van_GC}~\cite{chebnet,appnp,gprgnn,bernnet,chebnetii,jacobiconv,optbasis}. For instance, GPRGNN~\cite{gprgnn} focuses on directly learning the coefficients $w_k$, while JacobiConv~\cite{jacobiconv} and OptiBasisGNN~\cite{optbasis} investigate the potential of various polynomial bases in approximating and learning the filter $h(\Lambda)$. Although these methods have attained a degree of success, their direct application to directed graphs is problematic. This difficulty arises from the fact that the adjacency matrix $\mathbf{A}$ of directed graphs is not self-adjoint, which complicates the process of eigendecomposition and the subsequent definition of corresponding graph convolutions.




\begin{figure*}[ht]
    \centering
   \vspace{-2mm}
   \hspace{-4.8mm}
   \subfigure[Direction Prediction (DP)]{
   \includegraphics[width=58mm]{figure/magnet_mlp_dp.pdf}
   \label{fig:dp}
   }
   \hspace{-4.8mm}
   %\vspace{-2mm}
   \subfigure[Existence Prediction (EP)]{
   \includegraphics[width=58mm]{figure/magnet_mlp_ep.pdf}
   \label{fig:ep}
   }
   %\vspace{-2mm}
   \hspace{-4.8mm}
   \subfigure[Class imbalance issue]{
   \includegraphics[width=58mm]{figure/cora_bar_chart.pdf}
   \label{fig:imbalance}
   }
   \vspace{-3mm}
   \hspace{-4.8mm}
   \caption{(a) and (b) are the results of MagNet~\cite{magnet} as reported in the original paper, alongside the reproduced MagNet and MLP results. (c) is the number of samples and the accuracy for each class of DUPLEX~\cite{duplex} on the Cora dataset in the 4C task.}
   \vspace{-4mm}
   \label{fig:magnet_mlp}
\end{figure*}



