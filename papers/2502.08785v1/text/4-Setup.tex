\section{Experimental Setup}

% Problem and Dataset
This study addresses detecting \gls{HL} with contextual attributes through binary classification. The dataset generation process is fully defined in \cite{miranda2022hytea}. The dataset has 60 features and cannot be publicly published due to sensitive patient screening information.

% Experimental Setup
Regarding the experimental settings, Table \ref{table:setup} summarizes the parameters of the framework for each one of the three experiments. Most settings are alike, only diverging in the proxy model. All the models used the default package parameters, except for the \gls{RF} where the n\_estimators and max\_depth parameters were defined to 5. The grammar used in the experiments enables the selection and construction of algebraic-type features and is available here \footnote{\href{https://github.com/miguelrabuge/fedora/blob/main/examples/audiology/audiology.pybnf}{ github.com/miguelrabuge/fedora/blob/main/examples/audiology/audiology.pybnf}}.


\begin{table}
\centering
\caption{Experimental Settings}
\label{table:setup}
\begin{NiceTabular}{|l|wc{2cm}|wc{2cm}|wc{2cm}|}
\hline
\multicolumn{1}{|c}{\textbf{Parameters}} & \multicolumn{3}{c|}{\textbf{Experiments}} \\
\hline
Proxy Model & DT & RF & XGB \\
\hline
Population & \multicolumn{3}{c|}{200} \\
\hline
Generations & \multicolumn{3}{c|}{100} \\
\hline
Runs & \multicolumn{3}{c|}{30} \\
\hline
Elitism & \multicolumn{3}{c|}{10\%} \\
\hline
Crossover Rate & \multicolumn{3}{c|}{0.9} \\
\hline
Mutation Rate & \multicolumn{3}{c|}{0.1} \\
\hline
Minimum Tree Depth & \multicolumn{3}{c|}{3} \\
\hline
Maximum Tree Depth & \multicolumn{3}{c|}{10} \\
\hline
Selection & \multicolumn{3}{c|}{Tournament (size 3)} \\
\hline
Fitness & \multicolumn{3}{c|}{1 - Balanced Accuracy} \\
\hline
\end{NiceTabular}
\end{table}

Four types of models were selected as testing models: \gls{DT}, \gls{RF}, \gls{XGB} and \gls{MLP}. These models will assess the generalization performance of the FEDORA individuals, comparing its balanced accuracy scores with the baseline and other \gls{FE} methods, such as \gls{PCA}, \gls{UMAP}, \glspl{SOM} and \glspl{AE}.

Each \gls{FE} technique will use the same number of features as the FEDORA individual. For instance, if the FEDORA individual has 15 features, both the number of \gls{PCA} and \gls{UMAP} components would be equal to 15, the 2D SOM grid would have dimensions of 15x1, and the code size of the AE would be set to 15. The \gls{AE} parameters consist of 50 neurons for the single hidden layers, with linear activation functions, and using mean squared error as the error metric. Its training involves using a batch size of 32, running for 50 epochs, using Stochastic Gradient Descent.