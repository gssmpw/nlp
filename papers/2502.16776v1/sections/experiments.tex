
\section{Experiments}

\subsection{Setup}

Using AISafetyLab, we conducted a series of experiments to evaluate various attack and defense methods, employing Llama-Guard-3-8B as the scoring model. To highlight the performance gap between attack and defense strategies, we initially selected Vicuna-7B-v1.5 (a model that is relatively weak in safety) as the target model and assessed its performance on a subset of the HarmBench dataset that contains 50 harmful instructions. Additionally, we assess the overrefusal tendencies of various defense methods on XSTest \cite{rottger2024xstest}.

\paragraph{Attack Methods} We examined 13 representative attack methods as detailed in Section \ref{appsec:overview_attack}. All methods were applied to the same subset of HarmBench.

\paragraph{Defense Methods} We evaluated 13 representative inference-time defense methods and three training-time defense methods, as described in Section \ref{appsec:overview_defense}. For the training-time defenses, the size of the training dataset was controlled to approximately 1,000 samples.

\subsection{Main Results}

\input{tables/main_res}

The results are summarized in Table \ref{tab:main_res}, highlighting attack success rates under different defense strategies and the corresponding overrefusal rates.

\paragraph{Attack Effectiveness}
Among the evaluated attack methods, AutoDAN demonstrates superior effectiveness across various defense mechanisms, while PAIR, DeepInception and Jailbroken also achieve attack success rates (ASR) exceeding 35\%. Notably, some methods, such as GCG and SAA, perform well on the vanilla model but experience a significant drop in effectiveness when confronted with defensive measures. These findings underscore the importance of evaluating attack methods under diverse defense strategies.

\paragraph{Defense Effectiveness} 
At the inference stage, Prompt Guard, Robust Aligned, and Self Evaluation emerge as the most effective defensive strategies, as discussed in Section \ref{appsec:overview_defense}. In terms of training-based defenses, Safe Unlearning proves to be the most effective, reducing the average attack success rate to 14.8\%. Notably, Prompt Guard completely neutralizes all attacks by employing a classifier on input queries.
However, some defenses, such as Erase Check and Robust Aligned, while highly effective, introduce significant overrefusal rates, highlighting a trade-off in overall usability. Additionally, approaches like PPL Filter and Erase Check are only effective against specific attack methods that rely on unreadable adversarial prompts. These findings underscore the ongoing challenge of balancing security with usability in current defense mechanisms.

\paragraph{Challenge on Robustness} 
The evaluation of robustness still poses significant challenges, often resulting in unfair comparisons between methods. For example, while DeepInception achieves an attack success rate (ASR) above 40\% under methods such as Safe Unlearning and Self Evaluation, the responses often consist of fictional narratives or simple repetitions of the question, without providing precise or potentially harmful information. These problems underscore the necessity for more dependable evaluation frameworks that can accurately measure performance across a variety of adversarial conditions.

% \paragraph{}
