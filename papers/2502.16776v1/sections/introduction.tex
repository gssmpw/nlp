\section{Introduction}
AI models have garnered significant attention in recent years due to their remarkable improvements in performing a wide range of tasks. However, as these models grow in capability, they also introduce critical safety concerns. For instance, these models may leak private information \cite{DBLP:conf/acl/ZhangWH23}, generate harmful content \cite{DBLP:journals/corr/abs-2307-15043}, or exhibit unsafe behaviors in interactive environments \cite{zhang2024agent}. 
These risks pose substantial barriers to the reliable and widespread deployment of AI systems, making safety a crucial area of research.
To address these challenges, the research community has devoted increasing effort to AI safety, which can be broadly categorized into two key areas: \textbf{(1) safety evaluation}, which involves identifying vulnerabilities through jailbreak attacks \cite{liu2023autodan, yu2023gptfuzzer}, developing specialized benchmarks \cite{DBLP:conf/icml/MazeikaPYZ0MSLB24}, and designing safety-scoring models \cite{inan2023llama, zhang2024shieldlm}; and \textbf{(2) safety improvement}, which focuses on developing defense mechanisms and alignment strategies to mitigate risks and improve AI robustness \cite{selfreminder}. 
Despite the advancements in evaluation methodologies and improvement strategies, significant challenges persist in comparing these approaches. These challenges primarily stem from variations in experimental setups, such as differences in test data and victim models. Furthermore, reimplementing prior work can be time-intensive, especially when source code is unavailable. Even when source code is accessible, considerable effort may still be required to configure specific environments or adapt the implementation to accommodate new datasets and models.

To this end, we introduce AISafetyLab\footnote{We are currently focusing on Large Language Models (LLMs), but we plan to expand to other scenarios in the future. Therefore, we have named the framework using the broader term "AI".}, a comprehensive framework for evaluating and improving AI safety. The framework comprises three core modules: \texttt{Attack}, \texttt{Defense} and \texttt{Evaluation}. The \texttt{Attack} module currently implements 13 representative jailbreak attack methods, encompassing both black-box and white-box techniques. The \texttt{Defense} module supports 3 training-based defense strategies and 13 inference-time defense mechanisms, all aimed at preventing the model from generating unsafe content. The \texttt{Evaluation} module integrates mainstream safety scoring methods, including 2 rule-based scorers and 5 model-based scorers. In addition, AISafetyLab features four auxiliary modules to support the core functionalities: \texttt{Models}, \texttt{Dataset}, \texttt{Utils} and \texttt{Logging}. The \texttt{Models} module provides a unified interface for interacting with both local and API-based models. The \texttt{Dataset} module standardizes data loading from local files or the Hugging Face Datasets library. The \texttt{Utils} module offers a variety of utility functions for managing models, strings, configurations, and more. The \texttt{Logging} module handles the configuration and management of the logger.

We highlight the following key features of AISafetyLab:
\begin{itemize}
    \item \textbf{Comprehensive Method Coverage.} AISafetyLab offers a broad array of attack, defense, and evaluation techniques. Notably, compared to existing toolkits, we are the first to integrate various defense methods, to the best of our knowledge. 
    \item \textbf{Structured and Unified Design.} We have reorganized numerous method implementations to create a clean and structured codebase, enhancing both readability and extensibility. Additionally, a unified access interface is provided for each method, , streamlining execution for end users.
    \item \textbf{Extensive Model Support.} AISafetyLab supports both local transformer-based models and API-based models. We have also carefully addressed model-specific tokenization issues in the implementation of certain methods (e.g., GCG \cite{DBLP:journals/corr/abs-2307-15043}).
    \item \textbf{Great Extensibility.} With its structured design and auxiliary modules, AISafetyLab offers great flexibility for developers. The framework is easily extensible, allowing the addition of new methods by building on existing components and examples.
\end{itemize}

Additionally, we present an initial evaluation of Vicuna, in which we assess 13 distinct attack methods and 16 defense mechanisms using AISafetyLab. Our results highlight that certain attack strategies consistently demonstrate high efficacy, whereas others show variable performance depending on the defense mechanisms employed. Furthermore, we observe limitations in the current evaluation framework, which at times leads to inconsistencies and potentially unfair comparisons. 

We believe that AISafetyLab has the potential to significantly contribute to the advancement of AI safety evaluation and improvement. We are committed to the ongoing maintenance and regular updates of the framework to ensure its continued relevance and effectiveness.


