\section{Framework Design}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/overview.png}
    \caption{An overview of AISafetyLab. We introduce three core modules encompassing various attack, defense and evaluation methods. To support their implementation, we incorporate four shared auxiliary modules.}
    \label{fig:overview}
\end{figure*}

\subsection{Overview}
We illustrate the overview of AISafetyLab in Figure \ref{fig:overview}. We implement various representative attack and defense methods, which can be applied to AI models simultaneously to produce the final outputs. Then various evaluation methods could be applied to access the safety of the outputs. To support the three main modules, we also implement four auxiliary shared modules, including \texttt{Models}, \texttt{Dataset}, \texttt{Utils} and \texttt{Logging}. Next, we will introduce these modules in detail. 

\subsection{Attack}
\label{appsec:overview_attack}
In this section, we introduce the AISafetyLab attack module, a critical component of the overall package. This module is designed to assess the safety capabilities of LLMs against adversarial attacks, particularly those that attempt to bypass safety mechanisms. It features 13 representative adversarial attack methods, classified into three categories based on the access level to the target model:

\begin{itemize}
    \item \textbf{White-box Attacks}: The attacker has full access to the architecture, parameters and internal states (e.g., gradient) of the target model. This enables more targeted and precise manipulations of the model's behavior. We implement GCG \cite{DBLP:journals/corr/abs-2307-15043} as a representation of this kind of attacks.
    \item \textbf{Gray-box Attacks}: The attacker has partial access, typically limited to input 
    queries, output text, and corresponding log probabilities. This information is easier to acquire compared to that in the white-box setting. In this category, AutoDAN \cite{liu2023autodan}, LAA \cite{andriushchenko2024jailbreaking} and Advprompter\cite{paulus2024advprompter} are implemented.
    \item \textbf{Black-box Attacks}: The attacker has minimal access, often restricted to input queries and output text. These attacks are the most challenging and resource-constrained, 
    relying on input-output interactions to circumvent safety mechanisms. The following 9 black-box attack methods are currently implemented: GPTFuzzer \cite{yu2023gptfuzzer}, Cipher \cite{yuan2023toosmart}, DeepInception \cite{li2023deepinception}, In-context Learning Attacks \cite{ICD}, Jailbroken \cite{DBLP:journals/corr/abs-2307-02483}, MultiLingual \cite{deng2023multilingual}, PAIR \cite{chao2023jailbreaking}, ReneLLM \cite{ding2023wolf} and TAP \cite{mehrotra2023tree}.
\end{itemize}

The details of these attack methods are presented in Appendix \ref{app:attacker}.

\subsubsection{Attack Module Design}
To streamline the use of these diverse attack methods, the attack module of AISafetyLab is designed to be modular and flexible. The core components of the attack module include:

\begin{itemize}
    \item \textbf{Init}: This module initializes the attacking environment by loading models and datasets, and setting up the necessary infrastructure for running the attacks.
    \item \textbf{Mutate}: The mutate module collects various mutation strategies used by different attack methods. These strategies are applied to modify input queries in ways that maximize the chances of bypassing the model’s defenses.
    \item \textbf{Select}: The select module assists in identifying the most promising adversarial queries by ranking them based on relevant signals.
    \item \textbf{Feedback}: The feedback module provides optimization signals that guide the attacker in refining and enhancing the generated prompts.
\end{itemize}

This modular design provides two key advantages:
\begin{enumerate}
\item \textbf{User-Friendly}: The well-structured framework simplifies comprehension, allowing newcomers to easily grasp the internal workings of various attack methods.
\item \textbf{Customizability}: Developers can extend or modify the attack flow by adapting individual modules, facilitating the creation of new attack strategies using the provided building blocks.
\end{enumerate}

By providing this modular and extensible framework, AISafetyLab enables researchers to experiment with a wide variety of adversarial techniques and gain deeper insights into the safety and robustness of LLMs.


\subsection{Defense}
\label{appsec:overview_defense}

We categorize the safety defenses of large language models into two primary types: inference-time defenses and training-time defenses, as illustrated in Figure \ref{fig:defense_overview} in the Appendix. Our modular defense framework is designed to support rapid and flexible expansion, enabling seamless integration of multiple defensive mechanisms. In particular, inference-time defenses allow the concurrent deployment of multiple strategies to enhance robustness.

\paragraph{Inference-Time Defenses} Inference-time defenses operate across three stages—input modification, decoding guidance, and output monitoring—which correspond to the categories of preprocessing, intraprocessing, and postprocessing. Our framework incorporates 13 representative methods spanning these stages, as detailed in Appendix \ref{app:defender1}.

\paragraph{Training-Time Defenses} Training-time defenses are categorized into safety data tuning, RL-based alignment, and unlearning. We implement one representative method for each category, with further details provided in Appendix \ref{app:defender2}.


\subsection{Evaluation}
\label{appsec:evaluation}

% We integrate seven widely applied evaluation methods for safety detection, each implemented as a \texttt{Scorer} module inherited from the base module \texttt{BaseScorer}. The details of these scorers are provided as follows:

We integrate seven widely applied evaluation methods for safety detection, each implemented as a Scorer module inherited from the base module BaseScorer. These scorers are categorized into three main types:

\begin{itemize}
    \item \textbf{Pattern-based Scorer.} These scorers determine the success of a jailbreak attempt by matching the model’s response against a predefined set of patterns, including PatternScorer and PrefixMatchScorer.
    \item \textbf{Finetuning-based Scorer.} This category of scorers assesses the safety of responses using fine-tuned scoring models, including ClassficationScorer, ShieldLMScorer, HarmBenchScorer and LlamaGuard3Scorer.
    \item \textbf{Prompt-based Scorer.} This category of scorers evaluates response safety by prompting the model with specifically designed safety detection guidelines, including PromptedLLMScorer.
\end{itemize}

All the scorers utilize the same interface \texttt{score} to conduct safety evaluation, which takes a query-response pair as input and returns the judgment from the scorer. Additional outputs from the scorer are also returned to provide comprehensive information during evaluation. The implementation details of the scorers are presented in Appendix \ref{app:scorer}.

Additionally, we implement a scorer named OverRefuseScorer based on the work of \citet{rottger2024xstest}, which prompts an LLM to evaluate the over-refusal rate of a model. The interface of this scorer is consistent with that of other scorers.

\subsection{Auxiliary Modules}
We introduce four auxiliary modules that facilitate the implementation of the three core modules. Each of these auxiliary modules is detailed below.

\paragraph{Models} Our framework currently supports two primary types of models: local transformer-based models and API-based models. Specifically, local models must be compatible with the Hugging Face Transformers library, while API-based models must adhere to OpenAI-compatible access interfaces. To enhance usability, we provide unified interfaces for local models, such as \texttt{chat} and \texttt{batch\_chat}, which enable text generation based on given input prompts. Additionally, for API-based models, we incorporate robust error-handling mechanisms within the \texttt{chat} interface, allowing for a configurable maximum number of retry attempts in the event of errors.

\paragraph{Dataset} This module primarily manages dataset loading and slicing. It supports both local data files and datasets from the Hugging Face Datasets library. Furthermore, it includes a configurable \texttt{subset\_slice} parameter, which allows users to specify a subset of the dataset for selection. This feature is particularly beneficial for running experiments on smaller portions of a dataset or resuming experiments that were previously interrupted.

\paragraph{Utilities} The utilities module provides various helper functions categorized into four key areas: (1) model-related functions (e.g., perplexity computation), (2) string processing utilities (e.g., function word identification), (3) configuration management (e.g., loading attack method configurations), and (4) miscellaneous functionalities.

\paragraph{Logging} This module is responsible for logging functionalities and leverages the \texttt{loguru}\footnote{\url{https://github.com/Delgan/loguru}} library to provide a shared logger across the entire project. We implement an intuitive interface, \texttt{setup\_logger}, to configure logging settings, such as directing command-line outputs to a file. The logger supports various log levels (e.g., \textit{debug} and \textit{error}) and automatically records useful metadata, including timestamps and command execution locations.

In addition to these modules, we curate and organize various safety-related datasets, which are publicly available on the Hugging Face Datasets platform\footnote{\url{https://huggingface.co/datasets/thu-coai/AISafetyLab_Datasets}}. Furthermore, we maintain a continuously updated list of research papers related to AI safety, providing a valuable resource for the community.