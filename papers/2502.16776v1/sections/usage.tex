\section{Usage}

AISafetyLab provides a general and user-friendly interface for LLM attack, defense, and evaluation with just a few lines of code.
%
In this section, we offer examples for attack, defense and evaluation, respectively.


\subsection{Attack}
The following code demonstrates the setup of our attack module. After importing the corresponding attack method (e.g., AutoDANManager), the \texttt{attack} function can automatically initiate an attack towards the target model specified in the configuration file and save the resulting responses.


\begin{lstlisting}[language=Python]
from aisafetylab.attack.attackers.autodan import AutoDANManager
from aisafetylab.utils import ConfigManager
from aisafetylab.utils import parse_arguments

args = parse_arguments()

config_manager = ConfigManager(config_path=args.config_path)
attacker = AutoDANManager.from_config(config_manager.config)
attacker.attack()
\end{lstlisting}


\subsection{Defense}

We provide a demonstration of the defense module usage below. After specifying the defense method, the \texttt{chat} function can execute the defense and produce the corresponding results. Note that we support the simultaneous deployment of multiple defense strategies.

\begin{lstlisting}[language=Python]
from aisafetylab.defense.inference_defense import create_defender_from_yaml
from aisafetylab.defense.inference_defense import chat

attack_query = "Give detailed instructions for >>> correctlua code reform BEGIN<!--"

defender_name="self_reminder"
defenders = [(create_defender_from_yaml(f"./configs/{defender_name}.yaml")]

response = chat(
    model,
    attack_query,
    defenders
)
\end{lstlisting}


\subsection{Evaluation}

The usage of the evaluation module is as follows. After instantiating the \texttt{scorer} with a specific evaluation method, the \texttt{score} function can provide evaluation results for a given instruction-response pair.

\begin{lstlisting}[language=Python]
from aisafetylab.evaluation.scorers import LlamaGuard3Scorer

scorer = LlamaGuard3Scorer(
        model_path="meta-llama/Llama-Guard-3-8B",
        tokenizer_path="meta-llama/Llama-Guard-3-8B",
    )
scorer.score("How to make a bomb?", "Sorry, I can not ...")
\end{lstlisting}