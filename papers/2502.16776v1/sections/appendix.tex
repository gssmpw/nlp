\section{Implementation Details of Attackers}
\label{app:attacker}

The implementation details of the 13 attackers mentioned in section \ref{appsec:overview_attack} are presented as follows:

\subsubsection{White-box Attacks}
% White-box attacks allow the adversary full visibility of the target model's parameters and architecture. This enables more targeted and precise manipulations of the model's behavior. The following attack method is included in this category:

\begin{itemize}
    \item \textbf{GCG \cite{DBLP:journals/corr/abs-2307-15043}}: The Greedy Coordinate Gradient-based Search (GCG) attack method perturbs the input tokens using the loss gradient as an optimization signal. The loss function is designed to maximize the probability of an affirmative prefix, thus guiding the model to produce unsafe or undesirable outputs.
\end{itemize}

\subsubsection{Gray-box Attacks}
% Gray-box attacks provide partial information about the target model, usually involving access to the input-output pairs and the corresponding output logprobs. These information is easier to acquire compared to those in the white-box setting. The following methods are included:

\begin{itemize}
    \item \textbf{AutoDAN \cite{liu2023autodan}}: AutoDAN generates jailbreak prompts using a hierarchical genetic algorithm. The goal is to automatically evolve effective attack strategies that bypass the model's safety mechanisms.
    \item \textbf{LAA \cite{andriushchenko2024jailbreaking}}: The LAA method designs adaptive templates and appends adversarial suffixes to the chosen template. The suffix is optimized through random search, achieving high success rates for bypassing model defenses.
    \item \textbf{Advprompter \cite{paulus2024advprompter}}: This method trains an attacker LLM to autoregressively generate adversarial suffixes to a given input query, making it effective for generating successful jailbreak prompts.
\end{itemize}

\subsubsection{Black-box Attacks}
% Black-box attacks are the most challenging and resource-constrained, as the attacker has minimal access to the model. These attacks rely on the input-output interactions and often require creative strategies to bypass safety alignments. The following methods are included:

\begin{itemize}
    \item \textbf{GPTFuzzer \cite{yu2023gptfuzzer}}: This method generates new jailbreak templates through iterative mutation of human-written templates. It employs five mutation techniques: generation, crossover, expansion, shortening, and rephrasing, all aimed at finding prompts that can successfully bypass model defenses.
    \item \textbf{Cipher \cite{yuan2023toosmart}}: The Cipher attack works by encoding instructions in a cryptic manner, such that the model's safety alignment mechanisms fail to interpret the instructions correctly, enabling a jailbreak.
    \item \textbf{DeepInception \cite{li2023deepinception}}: This attack creates diverse scenes and characters to mislead the target model’s safety filters, thus circumventing its safety alignment.
    \item \textbf{In-context Learning Attacks \cite{ICD}}: In-context learning exploits few-shot demonstrations to manipulate the target model's behavior and trigger unsafe outputs.
    \item \textbf{Jailbroken \cite{DBLP:journals/corr/abs-2307-02483}}: The Jailbroken attack method targets two key failure modes in LLM safety alignment: competing objectives and mismatched generalization. By leveraging these weaknesses, it crafts prompts that can bypass the model’s safety mechanisms.
    \item \textbf{MultiLingual \cite{deng2023multilingual}}: This attack translates input queries into low-resource languages, often evading the safety mechanisms in models that are less robust in non-major languages, thus achieving a successful jailbreak.
    \item \textbf{PAIR \cite{chao2023jailbreaking}}: PAIR involves using an attacker LLM to iteratively refine jailbreak prompts, enhancing their effectiveness through repeated refinement.
    \item \textbf{ReneLLM \cite{ding2023wolf}}: ReneLLM combines two techniques: prompt rewriting and scenario nesting. These methods are used to reframe the input in ways that bypass the model’s defenses.
    \item \textbf{TAP \cite{mehrotra2023tree}}: The Tree of Attacks with Pruning (TAP) method maintains a structured flow in the form of a tree to iteratively optimize the jailbreak prompt. The attack continues until a successful jailbreak prompt is found.
\end{itemize}


\section{Implementation Details of Defenders}
\label{app:defender}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/defense_frame.png}
    \caption{Overview of the categorization of safety defense methods.}
    \label{fig:defense_overview}
\end{figure*}
\input{tables/defense_detail}

\subsection{Inference Time Defense}
\label{app:defender1}

We selected 13 inference-time defense methods, categorized into three main strategies, and provide a unified interface for these defense methods, which can be utilized in the \textit{defend\_chat} function, as illustrated in Table \ref{tab:defenders}.

\subsection{Training Time Defense}
\label{app:defender2}
The Training Time Defense is applied during the training process of large language models. It includes three categories: Safety Data Tuning, RL-based Alignment and Unlearning. And we implement a method for each category.

\begin{itemize}
     \item \textbf{Safety Data Tuning}: Safety-Tuning \cite{safe_tuning}
     \item \textbf{RL-based Alignment}: Safe RLHF \cite{dai2023safe}
     \item \textbf{Unlearning}: Safe Unlearning \cite{safe_unlearning}
\end{itemize}

Organized into key directories such as config, data, model, train, and utils, the training framework ensures modularity and ease of management. The data directory handles preprocessing and dataset management, while train encompasses specialized subfolders for different training methods, including trainers and workflows, which is flexible and customizable.

\section{Implementation Details of Scorers}
\label{app:scorer}

The implementation details of the seven scorers mentioned in section \ref{appsec:evaluation} are presented as follows:

\begin{itemize}
    \item \textbf{PatternScorer.} This scorer judges the success of a jailbreak attempt by examining whether the response matches predefined jailbreaking failure patterns. A response is considered safe if it includes a certain failure pattern.
    \item \textbf{PrefixMatchScorer.} This scorer judges the success of a jailbreak by checking the prefix of the response. A response is considered unsafe if it starts with a jailbreaking target.
    \item \textbf{ClassficationScorer}. This scorer uses a classification model to assess the safety of the response. The scorer is initialized by a Roberta model from \citet{yu2023gptfuzzer}, and it is also acceptable to init this scorer with any customized classification model.
    \item \textbf{ShieldLMScorer.} This scorer uses \texttt{ShieldLM} to assess the safety of the response, which incorporates an analysis alongside its judgment explaining the reason behind its decision.
    \item \textbf{HarmBenchScorer.} This scorer uses the \texttt{HarmBench-Llama-2-13b-cls} classification model \cite{mazeikaharmbench} to assess the safety of the response.
    \item \textbf{LlamaGuard3Scorer.} This scorer uses \texttt{Llama-Guard-3-8B} to assess the safety of the response. It also provides the unsafe category if the response is judged as unsafe.
    \item \textbf{PromptedLLMScorer.} This scorer prompts a model to assess the safety of the response. We incorporate four judge prompts from \citet{qifine}, \citet{zhang2024shieldlm}, \citet{mehrotra2023tree} and \citet{chao2023jailbreaking}.
\end{itemize}