\section{Related Work}
\subsection{AI Safety Evaluation}
Recent studies have introduced various approaches for assigning safety scores to content generated by LLMs. ShieldGemma \cite{zeng2024shieldgemma} offers a suite of LLM-based content moderation tools built on Gemma2. WildGuard \cite{han2024wildguard} presents an open-source, lightweight moderation tool designed to address risks such as jailbreaks and refusals. Notably, ShieldLM \cite{zhang2024shieldlm} introduces customizable safety detectors capable of generating detailed explanations for their decisions. Llama Guard \cite{inan2023llama} ensures input-output safeguards for human-AI interactions, while OpenAI’s holistic detection approach~\cite{markov2023holistic} supports an API for moderating real-world content.

Benchmarks play a crucial role in standardizing safety evaluations by providing standard and comprehensive test environments. Agent-SafetyBench \cite{zhang2024agent} features 2,000 test cases spanning eight safety risks and ten failure modes, covering 349 novel interaction environments. SORRY-Bench \cite{xie2024sorry} focuses on the refusal behaviors of LLMs, with 450 unsafe instructions. SALAD-Bench \cite{li2024salad} broadens the scope by utilizing intricate taxonomies and employing the LLM-based evaluator MD-Judge to assess performance. Earlier contributions, such as SafetyBench \cite{zhang2023safetybench}, provided 11,435 multiple-choice safety questions, while Anthropic’s red-teaming dataset~\cite{ganguli2022red} offered valuable insights into harm reduction strategies.

The increasing sophistication of attacks on large language models (LLMs) has been a focal point of recent research, with these attacks broadly categorized into black-box and white-box methods. Black-box attacks, including AutoDAN-Turbo~\cite{liu2024autodan}, GPTFuzzer~\cite{yu2023gptfuzzer}, ReNeLLM~\cite{ding2023wolf}, BlackDAN~\cite{wang2024blackdan}, and PAIR~\cite{chao2023jailbreaking}, focus on crafting adversarial prompts or jailbreak templates without direct access to the model's internal architecture. These techniques achieve high success rates by exploiting model vulnerabilities, often utilizing methods such as social engineering and cipher-based strategies.
In contrast, white-box attacks—exemplified by methods like GCG~\cite{DBLP:journals/corr/abs-2307-15043}, AutoDAN~\cite{liu2023autodan}, AdvPrompter~\cite{paulus2024advprompter}, and PGD-based approaches~\cite{wang2024blackdan, huang2024cross}—leverage detailed access to model internals to generate adversarial inputs. These approaches typically employ gradient-based or loss-based optimization techniques, which expose significant weaknesses in the safety alignment of LLMs, particularly when dealing with complex or creative adversarial prompts.

% Explainability has emerged as a key aspect of safety evaluation, enhancing transparency and trustworthiness. For instance, ShieldLM~\cite{zhang2024shieldlm} integrates explainable safety detectors that provide reasoning behind their decisions. Similarly, SALAD-Bench~\cite{li2024salad} utilizes LLM-based evaluators to improve interpretability, reflecting a growing emphasis on aligning safety tools with explainable AI principles.

\subsection{AI Safety Improvement}
In addition to AI safety evaluation, an equally crucial area of research focuses on developing defensive strategies to counteract various attacks. These defenses can be broadly classified into two categories: training-based and inference-based approaches.
Training-based defenses, such as Safe Unlearning \cite{zhang2024safe}, Layer-specific Editing \cite{zhao2024defending}, and Safety-Tuned Reinforcement Learning \cite{dai2023safe}, aim to improve model alignment during the training phase. These strategies incorporate safety-oriented objectives, modify specific model layers, or introduce carefully curated safety datasets to ensure the model's robustness against potential threats.
Inference-based defenses, on the other hand, operate during the inference stage to mitigate harmful outputs. Approaches such as SafeDecoding \cite{safedecoding} and Goal Prioritization \cite{goal_prioritization} intervene at this stage to reduce the risk of undesirable behaviors. Additional techniques like RAIN \cite{li2023rain} and Robustly Aligned LLM (RA-LLM) \cite{cao2023defending} further enhance safety by dynamically aligning outputs or integrating robust safety checks.

% To evaluate the efficacy of these attack and defense mechanisms, comprehensive benchmarks like SafetyBench~\cite{zhang2023safetybench} and SALAD-Bench~\cite{li2024salad} provide diverse testing scenarios, including adversarial prompts and unsafe requests. These benchmarks offer critical insights into model vulnerabilities and the impact of defense strategies. In addition, detection tools such as ShieldLM~\cite{zhang2024shieldlm} and Llama Guard~\cite{inan2023llama} play a key role in identifying and moderating unsafe outputs, further strengthening the safety infrastructure of LLMs.
\subsection{Other Toolkits}
Recent efforts have integrated various adversarial attack methods, such as EasyJailBreak~\cite{zhou2024easyjailbreak} and Harmbench~\cite{DBLP:conf/icml/MazeikaPYZ0MSLB24}, which implement a diverse set of jailbreak attack strategies. Despite significant advancements in attack methodologies, research on defense mechanisms remains fragmented. Existing approaches typically concentrate on isolated defense strategies or individual evaluation metrics, often lacking a unified framework that integrates both attack and defense techniques into a comprehensive adversarial benchmarking system. This gap highlights the pressing need for an all-in-one platform capable of robust attack simulation, defense evaluation, and the assessment of LLM resilience.

