
\section{Preface}

A video foundation model is a model pre-trained on large video datasets that can generate videos in response to text, visual, or multimodal inputs from users. It can be applied to a wide range of downstream video-related tasks, such as text/image/video-to-video generation, video understanding and editing, as well as video-based conversion, question answering, and task completion.

Based on our understanding, we define two levels towards building video foundation models. 
\textbf{Level-1: translational video foundation model}. A model at this level functions as a cross-modal translation system, capable of generating videos from text, visual, or multimodal context.
\textbf{Level-2: predictable video foundation model}. A model at this level acts as a prediction system, similar to large language models (LLMs), that can forecast future events based on text, visual, or multimodal context and handle more advanced tasks, such as reasoning with multimodal data or simulating real-world scenarios.

Current diffusion-based text-to-video models, such as Sora \cite{openaisora}, Veo \cite{veo}, Kling \cite{kling}, Hailuo \cite{hailuo}, and Step-Video (as described in this report), belong to Level-1. These models can generate high-quality videos from text prompts, lowering the barrier for creators to produce video content. However, they often fail to generate videos that require complex action sequences (such as a gymnastic performance) or adherence to the laws of physics (such as a basketball bouncing on the floor), let alone performing causal or logical tasks like LLMs. Such limitations arise because these models learn only the mappings between text prompts and corresponding videos, without explicitly modeling the underlying causal relationships within videos. Autoregression-based text-to-video models introduce the causal modeling mechanism by predicting the next video token, frame, or clip. However, these models still cannot achieve performance comparable to diffusion-based models on text-to-video generation.

This report will detail the practice of building Step-Video-T2V as a state-of-the-art video foundation model at Level-1. By analyzing the challenges identified through experiments, we will also highlight key problems that need to be addressed in order to develop video foundation models at Level-2.