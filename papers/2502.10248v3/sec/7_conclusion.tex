\section{Conclusion and Future Work}


This technical report introduces and open-sources Step-Video-T2V, a state-of-the-art pre-trained video generation model from text, featuring 30B parameters, a deep compression Video-VAE, a DPO approach for video generation, and the ability to generate videos up to 204 frames in length. We provide a comprehensive overview of our pre-training and post-training strategies and introduce Step-Video-T2V-Eval as a new benchmark for evaluating text-to-video generation models.

We highlight challenges faced by current text-to-video models. First, high-quality labeled data remains a significant hurdle. Existing video captioning models often struggle with hallucination issues, and human annotations are expensive and difficult to scale. Second, instruction-following requires more attention, as it encompasses a wide range of scenarios, from generating videos based on detailed descriptions to handling complex action sequences and combinations of multiple concepts. Third, current models still face difficulties in generating videos that obey the laws of physics, an issue stemming from the inherent limitations of diffusion models. Lastly, RL-based optimization mechanisms are areas worth exploring for post-training improvements in video generation models.

Looking ahead, we plan to launch a series of open-source projects focused on the development of video foundation models, starting with Step-Video-T2V. We hope these efforts will drive innovation in video foundation models and empower video content creators.