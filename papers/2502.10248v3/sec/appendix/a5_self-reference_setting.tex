\section{Self-Reference Setting}
\label{sec:appendix:self-reference-setting}

\input{table/self_ref_results_appendix}

In this section, we will provide a detailed introduction to the reference loss score function and information entropy score function in SLM. Reference loss score function is to directly use the loss of the reference model as the basis for selecting tokens. The higher the token's loss of the reference model, the lower the expectation that the token will be selected. The score $\mathcal{L}_{\text{RM}}(x_i)$ can be directly obtained by referring to \autoref{equ:ref_loss}. Information entropy score function is to select the corresponding token based on the information entropy of the reference model in each token. The information entropy of token $x_i$ can be expressed as:

% \mathcal{H}_{\text{RM}}(x_i) = -\sum_{i=1}^{{V} P(t_i)\log P(t_i),

% \begin{equation}
% \mathcal{H}_{\text{RM}}(x_i) = -\sum_{k=1}^{{V}} P(t_k|x< i)\log P(t_k|x<i),
% \end{equation}

\begin{equation}
\mathcal{H}_{\text{RM}}(x_i) = -\sum_{k=1}^{V} P(t_k|x_{<i}) \log P(t_k|x_{<i}),
\end{equation}

where $t_k$ represents the i-th token in the vocabulary, and $V$ represents the size of the vocabulary. The intuition of this strategy is that the higher the information entropy, the higher the uncertainty of the token in the context. Therefore, we consider that if the language model is still uncertain for certain tokens after pretraining, we do not expect that the language model will learn it during pretraining. In \autoref{tab:self-ref-1b-slm}, we provide more SLM results, including different select ratios and combinations of two score functions, for the convenience of the readers to refer to.
%In \autoref{tab:self-ref-7b-owm}, we have added the results of the self-reference of the 7B model, and it can be seen that the Information Entropy Mask strategy is more stable in improving the 7B model compared to the Reference Loss Mask strategy.



