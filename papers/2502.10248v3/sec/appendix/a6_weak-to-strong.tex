\section{Weak-to-Strong Generalization}

%1B Tinyllama as ref -> 7B Llama 
\input{table/small_ref_results}
Apart from the main experiments where we use the same base model for the reference and continual pretraining, we also investigate if a smaller reference model can effectively guide the pretraining of a larger model.
We use Tinyllama-1.1B as reference model and continual pretraining Llama-2-7B on 15B OpenWebMath tokens.
Results presented in \autoref{tab:small-ref-result} indicate that, despite the considerable gap between the small and large models~\citep{contrastivedecoding23}, employing the small reference model to token selection can still yield benefits to the pre-training of the larger model.
If reference and training models have different vocabularies, one can consider performing token alignment \citep{wan2024knowledge, fu2023specializing}, which we leave for future work.
