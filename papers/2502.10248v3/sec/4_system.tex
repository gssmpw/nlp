\section{System}

This section describes our infrastructure that facilitates the efficient and robust training of Step-Video-T2V at scale. The discussion starts with a comprehensive system overview, providing a holistic perspective of the workflow, followed by an in-depth examination of each constituent component. Furthermore, we present our insights and practical experiences gained from our training platform implementation and routine operational management.

\subsection{Overview}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/system/sys_overview.pdf}
    \caption{The workflow of Step-Video-T2V training system.}
    \label{fig:sys_overview}
\end{figure*}

Figure~\ref{fig:sys_overview} shows the overall workflow of Step-Video-T2V training system. 
The workflow comprises several stages. The offline stage, based on our in-house training emulator (Step Emulator, \S\ref{subsec:system_framework}), is specifically designed to estimate and determine optimal resource allocation and training parallelism strategies. 
This determination is achieved through systematic analysis of model architectures and resource configurations provided as input parameters. 
Next, with the theoretical optimal resource allocation plan, we deploy the training job with GPUs allocated in the training and inference clusters, respectively. The training clusters, responsible for training video DiT, uses the parallelization strategy recommended by the emulator, which has been specifically optimized to maximize the Model Flops Utilization (MFU). 
The other side with VAE and Text Encoder, runs on the inference clusters, and constantly provides the processed input data (pairs of image/video latent and text embedding) needed for DiT training. Data transmission between clusters is facilitated by StepRPC (\S\ref{subsec:system_steprpc}), our high-performance RPC framework that seamlessly integrates both TCP and RDMA protocols, enabling efficient cross-cluster communication with robust fault tolerance.

To enable systematic monitoring and analysis during large-scale training, we implement a dual-layer monitoring approach through StepTelemetry (\S\ref{subsec:system_telemetry}). This system collects detailed data statistics from inference clusters while simultaneously collecting iteration-level, fine-grained performance metrics from training clusters. The resulting telemetry data provides multidimensional insights into the training system, enabling precise identification of algorithmic patterns and systematic detection of potential performance bottlenecks across the entire infrastructure.


We have constructed a datacenter comprising thousands of NVIDIA H800 GPUs interconnected by a rail-optimized RoCEv2 fabric (1.6Tbps bandwidth per node). Nodes of the datacenter can be dynamically assigned to inference clusters or training clusters according to GPU resource requirements.
To support a single large-scale training job with thousands of GPUs spanning multiple GPU clusters concurrently, we have gained valuable insights from addressing challenges related to the training platform (StepMind) and its operational complexities. A detailed examination of these findings, including specific implementation strategies and best practices, will be presented in \S\ref{subsec:system_platform}. Through comprehensive improvements to infrastructure reliability, we have achieved \textbf{99\% effective GPU training time} over more than one month.



\subsection{Training Framework Optimizations}
\label{subsec:system_framework}

\subsubsection{Step Emulator}
\label{subsubsec:system_semu}
The large model size and extended context length of video require partitioning both the model parameters and activations/gradients across devices using multiple parallelism strategies during training, such as Tensor-parallelism (TP), Sequence-parallelism (SP), Context-parallelism~(CP), Pipeline-parallelism (PP) and Virtual Pipeline-parallelism (VPP)~\cite{pp2021,tpsp2023, scp, hcp}. 
However, the large scale of GPU cluster required for DiT training poses significant challenges in tuning and validating architecture designs and optimizations. To address this, we developed Step Emulator (SEMU), a highly accurate and efficient simulator designed to estimate resource consumption and end-to-end performance during training, under various model architecture and parallelism configurations. Specifically, to accommodate the dynamic and mixed input data for DiT training, SEMU allows customization of input data with varying frames and resolutions. SEMU helps to design model parameters,  architecture and the associated optimal parallelism strategies. It also determines the resource allocation of inference~(\textit{i.e.}, text-encoder and VAE) and training~(\textit{i.e.}, video DiT) clusters before the training actually starts. 


\subsubsection{Distributed Training}
\label{subsubsec:system_dist}



\paragraph{Parallelism Strategy}
Table~\ref{tab:540p_mfu} outlines the MFU of different configurations for 540P video pre-training obtained by SEMU. As shown in the table, simply applying PP on top of TP does not achieve a high MFU. This is because PP only reduces memory usage for model parameters and gradients by about 20GB after 8-way TP, and it can only disable a small portion of activation checkpointing, given the 120GB of activation memory. While CP directly reduces activation memory, its communication cost through the NIC is comparable to the TP cost via NVLink. To reduce CP overhead, we apply head-wise CP~\cite{hcp} to the self-attention block, leveraging the MHA in the DiT model, and sequence-wise CP~\cite{scp} to the cross-attention block, due to the relatively short sequence length of $k$ and $v$ from the prompts. Despite these optimizations, the CP cost remains non-negligible, and relying solely on CP does not lead to a high MFU.

As a result, the optimal MFU is always achieved by combining TP, CP, PP, and VPP. However, for large-scale GPU cluster training, it is crucial to keep the backend framework as simple as possible for robustness and easy identification of stragglers during training. This hinders us from adopting PP since it generally lacks the necessary flexibility. As a trade-off, we adopt an 8-way tensor parallelism (TP) strategy combined with sequence parallelism (SP) and Zero1~\cite{zero2020}. This configuration results in a MFU that is marginally lower (-0.88\%) than the theoretical optimum. In practice, the actual training MFU reaches 32\%, which is slightly below the estimated value due to metric collection overhead and minor delays caused by stragglers.


\input{table/540p_mfu}

\paragraph{TP overlap} To minimize TP overhead, we have developed StepCCL, a proprietary collective communication library that implements advanced communication-computation overlap techniques. StepCCL directly utilizes the DMA engine for data transmission, completely bypassing the Stream Multiprocessors (SMs). This design enables simultaneous execution of StepCCL operations and GEMM computations on the same GPU, achieving true concurrency without mutual performance interference, thereby maximizing hardware utilization and computational throughput. More details can be found at Section 7 of \cite{disttrain}.


\paragraph{DP overlap}
In the first two stages~(i.e. Image and 192P video pre-training), the context length is below 10K and activation memory does not pose a limiting factor. The primary memory usage stems from model parameters, which are handled via 8-way TP. While the performance bottleneck arises from gradient reduce-scatter and parameter all-gather operations introduced by DP, which can take up more than 30\% of the training time. To mitigate this, we developed DP overlap, where the parameter all-gather is performed during the forward pass of the first micro-batch, while the gradient reduce-scatter overlaps with the backward pass of the last micro-batch. Note that in DiT training, the activation norm is typically a key metric in the training process, which registers forward hooks for monitoring. These forward hooks can slow down the kernel launch in forward process, further rendering the forward overlap of DP communication ineffective. Therefore, the effectiveness of forward overlap may vary depending on the scenario, and the decision to enable it should be made carefully on a case-by-case basis.


\subsubsection{Tailored Computation and Communication Modules}

\paragraph{VAE Computation} 
To accelerate the convolution op (the most compute-intensive) in VAE, we employ the channel-last principle that is more GPU-friendly~\cite{channel_last} than naive PyTorch implementation. Specifically, a raw PyTorch tensor uses the NCHW memory format by default, while GPU tensorcores only support NHWC format essentially, causing additional format transformation overhead that slows down overall speed. We solve this by performing format permutation at the beginning, putting the channel to the last dimension physically. We modify each op (\textit{e.g.,} Conv, GroupNorm) along the computation graph to adapt to channel-last format. Overall, we achieve up to 7x VAE encode throughput with this optimization.
%Channel last, Dist Conv

\paragraph{Multi-GPUs VAE} To further reduce VAE latency and also support long, high resolution videos, using multiple GPUs is necessary to reduce computation and memory footprint on a single device. We support both Temporal and Spatial Parallel for the convolution op. As an example, for Temporal Parallel, we divide the video latent along the frame dimension and let each GPU hold only a subset of video frames. If a downstream Conv op requires cross-frame computation, we transfer the overlapped frames using all-to-all communication. The overhead is typically small ($<1\%$) compared to the computation time.

\paragraph{DiT} 
The plain RoPE implementation is inefficient due to the numerous time-consuming slice and concat operations required for building the embedding table and indexing. We developed a custom RoPE-3D kernel that replaces these indexing operations with efficient embedding computation, significantly improving performance.
The timestep modulation in the DiT model results in high activation memory usage, as the timestep is repeated across the sequence length, which is redundant since it remains the same within a single video clip. We implement a memory-efficient modulation operation where the timestep is repeated only during the forward process, and the non-repeated timestep is saved for the backward process. To further reduce memory costs, we fuse the LayerNorm op and the downstream timestep modulation op, eliminating the need for saving an intermediate output. 


\subsubsection{DP Load Balance}
A critical challenge in large-scale video generation arises when processing mixed-resolution videos and images within the same global iteration.
Conventional approaches that segregate different resolutions into separate batches lead to significant FLOPs disparities across model instances, resulting in GPU under-utilization due to load imbalance.
Table~\ref{tab:vid_img_flops} outlines the FLOPs per sample of different resolutions.

\begin{table}[h]
\centering
\begin{tabular}{rr}
\toprule
Resolution (F, H, W) & TFLOPs per sample \\
\midrule
$ 204 \times 256 \times 256$ & 1,717.20 \\
$ 204 \times 192 \times 320$ & 1,592.61 \\
$ 136 \times 256 \times 256$ & 1,079.85 \\
$ 136 \times 192 \times 320$ & 1,004.89 \\
$  68 \times 256 \times 256$ &  509.31 \\
$  68 \times 192 \times 320$ &  475.87 \\
$   1 \times 256 \times 256$ &   44.99 \\
\bottomrule
\end{tabular}
\vspace{3mm}
\caption{FLOPs per sample of different resolutions.}
\label{tab:vid_img_flops}
\end{table}

To address this issue, we propose a hybrid-grained load balancing strategy that operates through two complementary stages, as illustrated in Figure~\ref{fig:load_balance}.
In the first stage, we perform \textit{coarse-grained} FLOPs alignment by adjusting batch sizes of videos with different resolutions.
For each resolution $r$, we estimate its FLOPs per sample $F_r$ and compute optimal batch sizes $B_r$ through:

\begin{equation}
B_r = \left\lfloor \frac{F_{\text{target}}}{\alpha F_r} \right\rfloor
\end{equation}

where $F_{\text{target}}$ represents the target FLOPs per batch (typically the batch of the highest resolution videos) and $\alpha$ is a normalization factor to ensure the consistency of global batch size.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figure/system/load-balance.pdf}
    \caption{Load balancing with hybrid granularity.}
    \label{fig:load_balance}
\end{figure*}


The second stage addresses residual FLOPs variations through \textit{fine-grained} image padding.
Our system caches $N$ video batches and calculates required image supplements based on a predetermined video-to-image ratio $\beta$.
Using a greedy heuristic, we iteratively allocate images to the batch with the smallest current FLOPs until all supplements are distributed.

The hybrid-grained approach effectively balances computational loads while maintaining practical deployability.
Our solution requires only superficial awareness of data distribution, needing merely batch size adjustments and supplemental image padding rather than deep architectural changes.
This minimal intervention preserves the original training pipeline's integrity while introducing small memory overhead.



\subsection{StepRPC}
\label{subsec:system_steprpc}



To facilitate data transfer, we developed StepRPC, a high-performance communication framework. StepRPC leverages distributed named pipes as the core programming abstraction, enabling a large number of servers to communicate seamlessly by declaring pipes with the identical name. The spraying mode distributes data evenly across training servers. Compared to existing communication frameworks~\cite{ray,damania2023pytorchrpc,mooncacke}, StepRPC incorporates the following essential engineering optimizations.

\paragraph{Tensor-Native Communication over RDMA and TCP} In existing frameworks, tensor transfer typically entails heavy-weight serialization and deserialization overheads, amounting to tens of milliseconds. To address the inefficiency, StepRPC implements tensor-native communication that directly transfers bits within tensor memory, thereby eliminating the overheads associated with serialization and deserialization. StepRPC harnesses the power of RDMA to implement direct transfer of both GPU and CPU tensors. When RDMA-capable networks are not available, StepRPC can be seamlessly configured to utilize TCP transports. Note that TCP only supports CPU tensors. Consequently, transferring GPU tensors over TCP introduces additional overheads due to the necessity of copying memory between GPUs and CPUs. To mitigate the overheads, StepRPC proposes to overlap CudaMemcpy with TCP send and recv operations. Such optimization hides the latency of memory copying, thereby improving overall communication performance in non-RDMA environments.


\paragraph{Flexible Workload Patterns with High Resilience} To optimize GPU utilization, we leverage the same inference servers to generate data for large-scale pre-training experiments and small-scale ablation experiments simultaneously. StepRPC facilitates this via a combination of broadcasting and spraying communications. First, StepRPC broadcasts the data from inference servers to all training jobs. This ensures that each job receives the necessary data without redundant computations. Second, within an individual job, StepRPC sprays data to each training server. Though ingesting data from same inference servers, training jobs can operate independently and elastically with the help of StepRPC, meaning that jobs can begin, terminate or scale as needed without affecting the others. Meanwhile, StepRPC isolates failures across jobs, preventing cascading effects that could destabilize the entire system.

\paragraph{Enhanced Visbility for Real-Time Failure Detection and Resource Optimization} StepRPC offers comprehensive performance metrics for deep insights into the communication process. The metrics encompass critical aspects such as data counts, queuing latency and transmission cost. The enhanced visibility serves multiple purposes, empowering both operators and researchers to optimize performance and resource utilization effectively. Firstly, by monitoring the counts of produced and consumed data, StepRPC enables real-time failure detection. Discrepancies between these counts can indicate potential issues such as data loss, communication failures, or bottlenecks. This proactive approach allows operators to promptly identify and address failures. Next, researchers can leverage the metrics like queuing latency and API invoking latency to assess whether inference or training processes constitute the overall performance bottleneck.
Furthermore, armed with the metrics like rates of producing and consuming data, researchers can make informed decisions regarding GPU resource allocation for inference and training jobs.



\subsection{StepTelemetry}
\label{subsec:system_telemetry}
% @xialei

% intro
The lack of the observability of training framework makes analyzing it's inner state and debugging job failure difficult. Thus StepTelemetry, an observability suite for training frameworks, is introduced. This suite's goal is not only to enhance anomaly detection capabilities but also to establish a reusable pipeline for collecting, post-processing, and analyzing any training-related data.

% 采集管线一句话带过，key feature结合案例讲
StepTelemetry employs a simple and asynchronous data collection pipeline. It offers a Python SDK for easy integration with the training framework, supporting both batch and streaming data writes to files on local disk. An additional consumer process  is responsible for collecting, transforming, and writing data into various remote databases. StepTelemetry benefits Step-Video-T2V training in the following aspects.

% dataloader + 性能优化 + 异常检测
\paragraph{Anomaly Detection} Common profiling tools like PyTorch Profiler and Megatron-LM Timer introduce approximately 10\% to 15\% overhead, and struggle to support collaborative analysis among multiple ranks. Instead, StepTelemetry adopts a CUDA event-based approach without any unnecessary synchronizations. This enables continuously collecting timer data of all ranks during training with almost zero overhead. By providing various data visualizations and supporting data drill-down, StepTelemetry helps pinpointing root cause in case of hardware and software failure. 
As an example, during one training session, the training efficiency fell below expectations, yet no hardware alerts were triggered. Upon analyzing the collected data, we identified that the backward propagation time for certain ranks was abnormally prolonged. Since the backward process primarily involves tensor parallelism (TP) group communication and computation, it is highly probable that the machines hosting these ranks were underperforming. After removing these machines from the training cluster, the training efficiency returned to the expected level.
\paragraph{Data Statistics} During video training, it is vital to monitor data consumption. Instead of just counting tokens, it is required to record consumed videos' metadata. The legacy approach was to dump metadata to files on local disk, and then use scripts to parse them offline, which is particularly inefficient and inconvenient. By instrumenting dataloader with StepTelemetry, the metadata is written to database, thus OLAP is enabled. Visualizations such as duplicated data filtering and data distribution monitoring based on source url is provided to researchers, which help evaluating the model.
\paragraph{Performance Optimization} StepTelemetry provides insight for performance optimization. By visualizing the time consumption of each stage within an iteration, it provides developers with a comprehensive overview, enabling them to identify and optimize performance bottlenecks in critical paths. Additionally, dataloader statistics reveal the actual throughput of the training process. Although image and video data are supplied in a mixed manner, the iteration time remained unchanged after addressing the data parallelism (DP) imbalance issue. Nevertheless, the observed increase in data throughput demonstrates a significant improvement in system efficiency.



\subsection{StepMind}
\label{subsec:system_platform}
To ensure high availability of computing resources for large-scale Step-Video-T2V training tasks, we have invested substantial efforts into developing StepMind, a distributed training platform designed for large-scale machine learning workloads. StepMind has successfully achieved an effective GPU utilization rate exceeding 99.0\% for Step-Video-T2V training, primarily through the implementation of the following key techniques. 


\paragraph{Fine Grained Monitoring at Full Coverage}
To maximize distributed training efficiency, we developed a fine-grained monitoring system at full coverage that rapidly identifies faulty nodes. The monitoring system collects metrics at seconds-granularity across hardware, \eg CPU/GPU/memory/PCIe/network/storage/power/fans, and software, \eg OS stack, enabling rapid and full coverage fault detection. 
Based on our operation experiences, faulty nodes can be generally classified into two categories: 
a) Nodes with Fatal Errors (about 86.2\% of failures). 
These nodes can interrupt the training process immediately. Upon detection of these nodes, we will replace them with healthy nodes and restart the job.
In order to avoid incorrect restarts due to false alarms, we develop a multi-signal approach to ascertain whether a job requires restarting. The signals incorporated in this approach encompass RoCEv2 traffic disruption, low GPU power usage, and the cessation of updates in job training logs.
Once being identified as failed, the job will be restarted immediately, thereby reducing the time cost of unavailability resulting from node malfunctions.
b) Nodes with Non-Fatal Errors (about 13.8\% of failures).
Although these nodes do not immediately disrupt the training task, they can degrade training efficiency. Detecting such nodes is challenging, and we have developed specialized methods to identify them. These nodes are scheduled for replacement during planned maintenance, typically after a checkpoint is saved, to minimize the wasting time of computational resource.
Table~\ref{tab:training_fault_stats} shows more detailed statistics.
\input{table/interruptions}


\paragraph{GPU Machine High Quality Ensurance}
Training GPU nodes exhibit significant quality variations, \ie their failure probabilities differ substantially. Some servers have much higher failure risks than others, necessitating the selection of the most reliable servers for large-scale training tasks to minimize the job interruptions.
We developed an innovative node quality assessment framework that systematically integrates historical alert patterns, maintenance logs, stress test results, and load test durations to generate comprehensive quality scores. When node failures occur within production resource pools, replacement units are selectively deployed from a dedicated buffer pool following a prioritized matching rule: buffer machines' quality scores must meet or exceed the operational requirements of the target resource pool's priority tier. This methodology has achieved a statistically significant reduction in failure rates for critical resource pools (\ie video pool) from an original monthly average of 7.0\% to 0.9\%. Correspondingly, the daily restart rate per 1,000 GPUs caused by hardware issues decreased to approximately 1/11 of that reported in \texttt{LLaMA3.1}~\citep{llama3}.
% \ref{tab:restarts_llama_compare}
\input{table/restart_comparison}\\
Fewer restarts ultimately helped us achieve 99\% effective training time over a training period exceeding one month.
\[\text{Effective Training Time} = \frac{\text{Total Iteration Time}}{\text{Total Training Time}}\]

\paragraph{Completely Automated Server Launch Process}
When faulty machines are taken offline, they must undergo rapid repairs and meet stringent operational standards before being reintroduced into the service pool. This ensures that defective units do not negatively impact training jobs. Three key measures are implemented to achieve this:

\begin{itemize}[left=0cm]
    \item \textbf{Automated reboot-repair for transient failures.}
    A large proportion of node failures, approximately above 60\%, are transient failures. Examples include GPU DBE errors, GPU card disconnections, and network card disconnections. The transient failures can be effectively resolved by a simple restart. To speed up GPU repairing, we've created an automated system that quickly reboots servers based on the identified failure type. By integrating this reboot system with follow-up health checks and stress tests, we ensure servers can be brought online rapidly and with assured quality.
    
    \item \textbf{Comprehensive health checks via extensive diagnostic scripts.}
    We encode human expertise into reusable scripts to conduct comprehensive checks on the hardware and software configurations of GPU nodes. These checks include GPU, NIC, software driver, and firmware configurations, ensuring that servers in operation have uniform and correct hardware and software setups. In our experience, this practice prevents nodes with abnormal configurations from running training jobs, thereby reducing the likelihood of job interruptions.
    
    \item \textbf{Rigorous stress testing and admission protocols to validate performance.}
    Our comprehensive stress testing ensures each machine delivers peak performance by evaluating two key areas:
    1) Single-Machine Performance: 
    We validate GPU AI computing power (TOPS), HBM bandwidth, host-device data transfer speeds (H2D/D2H), and NVLink/PCIe connectivity between GPUs to guarantee maximum hardware capability.
    2) RDMA Network Verification
    Using PyTorch operations to simulate distributed training patterns (TP/EP/PP/DP), we test real-world network performance. Small-group testing helps swiftly identify faulty nodes, cables, or switches. Cross-GPU traffic routing through NICs enables network validation within individual machines for rapid troubleshooting.
    These tests improve node reliability and performance while preventing job failures, significantly boosting overall cluster stability and availability.


\end{itemize}






