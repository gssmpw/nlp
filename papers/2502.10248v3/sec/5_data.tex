\section{Data}

\subsection{Pre-training Data}
We constructed a large-scale video dataset comprising 2B video-text pairs and 3.8B image-text pairs. Leveraging a comprehensive data pipeline, we transformed raw videos into high-quality video-text pairs suitable for model pre-training. As illustrated in Figure~\ref{fig:data_pipeline}, our pipeline consists of several key stages: Video Segmentation, Video Quality Assessment, Video Motion Assessment, Video Captioning, Video Concept Balancing and Video-Text Alignment. Each stage plays a crucial role in constructing the dataset, and we describe them in detail below.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.3\textwidth, center, trim=0 0 0 0, clip]{figure/data/data_pipeline.png}
    \caption{The pipeline of Step-Video-T2V data process.}
    \label{fig:data_pipeline}
    %\vspace{-6mm}
\end{figure*}


\paragraph{Video Segmentation} 
We began by processing raw videos using the \textit{AdaptiveDetector} function in the \texttt{PySceneDetect}~\citep{pyscenedetect} toolkit to dentify scene changes and use FFmpeg~\citep{ffmpeg} to split them into single-shot clips. 
We adjusted the splitting process for high-resolution videos not encoded with \texttt{libx264} to include necessary reference frames—specifically by properly setting the crop start time in \texttt{FFmpeg}; this prevented visual artifacts or glitches in the output video.
We also removed the first three frames and the last three frames of each clip, following practices similar to Panda70M \cite{chen2024panda} and Movie Gen Video \cite{polyak2024moviegencastmedia}. Excluding these frames eliminates unstable camera movements or transition effects often present at the beginnings and endings of videos.


\paragraph{Video Quality Assessment}

To construct a refined dataset optimized for model training on high-quality, we systematically evaluated and filtered video clips by assigning multiple Quality Assessment tags based on specific criteria. We uniformly sampled eight frames from each clip to compute these tags, providing a consistent and comprehensive assessment of each video.

\begin{itemize}[left=0cm]
\item \textbf{Aesthetic Score}: We used the public LAION CLIP-based aesthetic predictor~\cite{schuhmann2022laion} to predict the aesthetic scores of eight frames from each clip and calculated their average.

\item \textbf{NSFW Score}: We employed the public LAION CLIP-based NSFW detector~\cite{laion2021nsfw}, a lightweight two-class classifier using CLIP ViT-L/14 embeddings, to identify content inappropriate for safe work environments.

\item \textbf{Watermark Detection}: Employing an EfficientNet image classification model~\cite{tan2019efficientnet}, we detected the presence of watermarks within the videos.

\item \textbf{Subtitle Detection}: Utilizing PaddleOCR~\cite{paddleocr}, we recognized and localized text within video frames, identifying clips with excessive on-screen text or captions.

\item \textbf{Saturation Score}: 
We assessed color saturation by converting video frames from BGR to HSV color space and extracting the saturation channel, using OpenCV \cite{opencv_library}. We computed statistical measures—including mean, maximum, and minimum saturation values—across the frames. 


\item \textbf{Blur Score}: 
We detect blurriness by applying the variance of the Laplacian method~\cite{pech2000diatom} to measure the sharpness of each frame. Low variance values indicate blurriness caused by camera shake or lack of clarity.

\item \textbf{Black Border Detection}: We use \texttt{FFmpeg} to detect black borders in frames and record their dimensions to facilitate cropping, ensuring that the model trains on content free of distracting edges.
\end{itemize}



\paragraph{Video Motion Assessment}

Recognizing that motion content is crucial for representing dynamic scenes and ensuring effective model training, we calculate the motion score by averaging the mean magnitudes of the optical flow \cite{opencv_library} between pairs of resized grayscale frames, using the Farneback algorithm. We introduced three evaluative tags centered around motion scores:


\begin{itemize}[left=0cm]
    \item \textbf{Motion\_Mean}: The average motion magnitude across all frames in the clip, indicating the general level of motion. This score helps us identify clips with appropriate motion; clips with extremely low \texttt{Motion\_Mean} values suggest static or slow motion scenes that may not effectively contribute to training models focused on dynamic content.

    \item \textbf{Motion\_Max}: The maximum motion magnitude observed in the clip, highlighting instances of extreme motion or motion distortion. High \texttt{Motion\_Max} values may indicate the presence of frames with excessive or jittery motion.

    \item \textbf{Motion\_Min}: The minimum motion magnitude in the clip, identifying clips with minimal motion. Clips with very low \texttt{Motion\_Min} may contain idle frames or abrupt pauses, which could be undesirable for training purposes.
\end{itemize}





\paragraph{Video Captioning} 
Recent studies~\citep{openaisora, betker2023improving} have highlighted that both precision and richness of captions are crucial in enhancing the prompt-following ability and output quality of generative models. 
 
Motivated by this, we introduced three types of caption labeling into our video captioning process by employing an in-house Vision Language Model (VLM) designed to generate both short and dense captions for video clips.
\begin{itemize}[left=0cm]
    \item \textbf{Short Caption}: The short caption provides a concise description, focusing solely on the main subject and action, closely mirroring real user prompts.

    \item \textbf{Dense Caption}: The dense caption integrates key elements, emphasizing the main subject, events, environmental and visual aspects, video type and style, as well as camera shots and movements. To refine camera movements, we manually collected annotated data and performed SFT on our in-house VLM, incorporating common camera movements and shooting angles.
    
    \item \textbf{Original Title}: We also included a variety of caption styles by incorporating a portion of the original titles from the raw videos, adding diversity to the captions.
    
\end{itemize}



\paragraph{Video Concept Balancing}
To address category imbalances and facilitate deduplication in our dataset, we computed embeddings for all video clips using an internal VideoCLIP model and applied K-means clustering \cite{macqueen1967some} to group them into over 120,000 clusters, each representing a specific concept or category. By leveraging the cluster size and the distance to centroid tags, we balanced the dataset by filtering out clips that were outliers within their respective clusters. As part of this process, we added two new tags to each clip:

\begin{itemize}[left=0cm] 
    \item \textbf{Cluster\_Cnt}: The total number of clips in the cluster to which the clip belongs.

    \item \textbf{Center\_Sim}: The cosine distance between the clip's embedding and the cluster center.
\end{itemize}



\paragraph{Video-Text Alignment}

Recognizing that accurate alignment between video content and textual descriptions is essential to generate high-quality output and effective data filtering, we compute a \textbf{CLIP Score} to measure video-text alignment. This score assesses how well the captions align with the visual content of the video clips.

\begin{itemize}[left=0cm] 
\item \textbf{CLIP Score}: We begin by uniformly sampling eight frames from the given video clip. Using the CLIP model~\cite{yang2022chineseclip}, we then extract image embeddings for these frames and a text embedding for the video caption. The \texttt{CLIP Score} is computed by averaging the cosine similarities between each frame embedding and the caption embedding.



\end{itemize}


\subsection{Post-training Data}


For SFT in post-training, we curate a high-quality video dataset that captures good motion, realism, aesthetics, a broad range of concepts, and accurate captions. Inspired by \cite{dai2023emu, polyak2024moviegencastmedia, kong2024hunyuanvideo}, we utilize both automated and manual filtering techniques:

\begin{itemize}[left=0cm]


\item \textbf{Filtering by Video Assessment Scores}: Using video assessment scores and heuristic rules, we filter the entire dataset to a subset of 30M videos, significantly improving its overall quality.


\item \textbf{Filtering by Video Categories}: For videos within the same cluster, we use the "Distance to Centroid" values to remove those whose distance from the centroid exceeds a predefined threshold. This ensures that the resulting video subset contains a sufficient number of videos for each cluster while maintaining diversity within the subset.



\item \textbf{Labeling by Human Annotators}: In the final stage, human evaluators assess each video for clarity, aesthetics, appropriate motion, smooth scene transitions, and the absence of watermarks or subtitles. Captions are also manually refined to ensure accuracy and include essential details such as camera movements, subjects, actions, backgrounds, and lighting.

\end{itemize}


