\section{Training Strategy}
\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccc}
\hline training stage & dataset & bs/node & learning rate & \#iters & \#seen samples \\
\hline 
\hline
    \multirow{3}{*}{Step-1: T2I Pre-training (256px)} & $\mathcal{O}(1) \mathrm{B}$ images & 40 & 1e-4 & 53k & 0.8B \\
     & $\mathcal{O}(1) \mathrm{B}$ images & 40 & 1e-4 & 200k & 3B \\
     \cline{2-6}
     & \textbf{Total} &  &  &  \textbf{253k} & \textbf{3.8B} \\
\hline 
\hline
    \multirow{4}{*}{Step-2: T2VI Pre-training (192px)} & $\mathcal{O}(1) \mathrm{B}$ video clips & 4 & 6e-5 & 171k & 256M\\
    & $\mathcal{O}(100) \mathrm{M}$ video clips & 4 & 6e-5 & 101k & 151M \\
    & $\mathcal{O}(100) \mathrm{M}$ video clips & 4 & 6e-5 & 158k & 237M \\
    \cline{2-6}
    & \textbf{Total} &  &  &  \textbf{430k} & \textbf{644M} \\
\hline
\hline
    \multirow{4}{*}{Step-2: T2VI Pre-training (540px)} & $\mathcal{O}(100) \mathrm{M}$ video clips & 2 & 2e-5 & 23k & 17.3M\\
    & $\mathcal{O}(10) \mathrm{M}$ video clips & 2 & 1e-5 & 17k & 8.5M \\
    & $\mathcal{O}(1) \mathrm{M}$ video clips & 1 & 1e-5 & 6k & 1.5M \\
    \cline{2-6}
    & \textbf{Total} &  &  &  \textbf{46k} & \textbf{27.3M} \\
\hline
\end{tabular}
}
\caption{Pre-training details of Step-Video-T2V. 256px, 192px, and 540px denote resolutions of 256x256, 192x320, and 544x992, respectively.}
\label{trainingrecipe}
\end{table}



\begin{figure}[h] 
    \centering
    \includegraphics[width=0.5\textwidth]{figure/v2_training_loss.png}  
    \caption{Training curve of different training stages, where $s_{i}$ denotes the $i^{th}$ dataset used in the corresponding stage.} 
    \label{fig:training curve}  
\end{figure}

A cascaded training strategy is employed in Step-Video-T2V, which mainly includes four steps: text-to-image (T2I) pre-training, text-to-video/image (T2VI) pre-training, text-to-video (T2V) fine-tuning, and direct preference optimization (DPO) training. The pre-training recipe is summarized in Table~\ref{trainingrecipe}.

\paragraph{Step-1: T2I Pre-training} In the initial step, we begin by training Step-Video-T2V with a T2I pre-training approach from scratch. We intentionally avoid starting with T2V pre-training directly, as doing so will significantly slow down model convergence. This conclusion stems from our early experiments with the T2V pre-training from scratch on the 4B model, where we observed that the model struggled to learn new concepts and was much slower to converge. By first focusing on T2I, the model can establish a solid foundation in understanding visual concepts, which can later be expanded to handle temporal dynamics in the T2V phase.

\paragraph{Step-2: T2VI Pre-training} After acquiring spatial knowledge from T2I pre-training in Step-1, Step-Video-T2V progresses to a T2VI joint training stage, where both T2I and T2V are incorporated. This step is further divided into two stages. In the first stage, we pre-train Step-Video-T2V using low-resolution (192x320, 192P) videos, allowing the model to primarily focus on learning motion-related knowledge rather than fine details. In the second stage, we increase the video resolution to 544x992 (540P) and continue pre-training to enable the model to learn more intricate details. We observed that during the first stage, the model concentrates on learning motion, while in the second stage, it shifts its focus more toward learning fine details. Based on these observations, we allocate more computational resources to the first stage in Step-2 to better capture motion knowledge.

\paragraph{Step-3: T2V Fine-tuning} Due to the diversity in pre-training video data across different domains and qualities, using a pre-trained checkpoint usually introduces artifacts and varying styles in the generated videos. To mitigate these issues, we continue the training pipeline with a T2V fine-tuning step. In this stage, we use a small number of text-video pairs and remove T2I, allowing the model to fine-tune and adapt specifically to text-to-video generation.


Similar to Movie Gen Video, we found that averaging models fine-tuned with different SFT datasets improves the quality and stability of the generated videos, outperforming the Exponential Moving Average (EMA) method. Even averaging checkpoints from the same data source enhances stability and reduces distortions. Additionally, we select model checkpoints based on the period after the gradient norm peaks, ensuring both the gradient norm and loss have decreased for improved stability.

\paragraph{Step-4: DPO Training}
As described in \S\ref{dpo}, video-based DPO training is employed to enhance the visual quality of the generated videos and ensure better alignment with user prompts.



\paragraph{Hierarchical Data Filtering}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.3\textwidth, center, trim=0 0 0 0, clip]{figure/data/data_filter.png}
    \caption{Hierarchical data filtering for pre-training and post-training.}
    \label{fig:data_filter}
    %\vspace{-6mm}
\end{figure*}


We apply a series of filters to the data, progressively increasing their thresholds to create six pre-training subsets for Step-2: T2VI Pre-training, as shown in Table~\ref{trainingrecipe}. The final SFT dataset is then constructed through manual filtering. Figure~\ref{fig:data_filter} illustrates the key filters applied at each stage, with gray bars representing the data removed by each filter, and colored bars indicating the remaining data at each stage.



\paragraph{Observations from Pre-training Curve}
%We use a funnel-style filtering method to progressively refine the training dataset throughout the pre-training stage. 
During pre-training, we observe a notable reduction in loss, which correlates with the improved quality of the training data, as illustrated in Figure \ref{fig:training curve}.

Additionally, a sudden drop in loss occurs as the quality of the training dataset improves. This improvement is not directly driven by supervision through a loss function during model training, but rather follows human intuition (e.g., filtering via CLIP scores, aesthetic scores, etc.). While the flow matching algorithm does not impose strict requirements on the distribution of the model’s input data, adjusting the training data to reflect what is considered higher-quality by humans results in a significant, stepwise reduction in training loss. This suggests that, to some extent, the model’s learning process may emulate human cognitive patterns.


\paragraph{Bucketization for Variable Duration and Size}

To accommodate varying video lengths and aspect ratios during training, we employed variable-length and variable-resolution strategies~\cite{chen2023pixartalphafasttrainingdiffusion, opensora}. We defined four length buckets (1, 68, 136, and 204 frames) and dynamically adjusted the number of latent frames based on the video length. Additionally, we grouped videos into three aspect ratio buckets—landscape, portrait, and square—according to the closest height-to-width ratio.



