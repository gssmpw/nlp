\section{Introduction}

Large language models (LLMs), as part of Artificial General Intelligence (AGI), has made impressive progress in recent years. These models are capable of understanding human instructions and generating coherent, fluent responses in natural language. However, language is a symbolic abstraction of thought, using words and concepts to represent the world. This abstraction often falls short in capturing the complexity and richness of reality, particularly when it comes to dynamic processes like object motion or the intricate spatial and temporal relationships between entities.
As a result, video generation has emerged as an important frontier in the pursuit of AGI, offering a pathway toward bridging these cognitive gaps. Moreover, video content is now the dominant form of communication and entertainment online. Developing video generation systems capable of producing high-quality content can significantly reduce barriers for creators and democratize video production. This empowers everyone, from amateurs to professionals, to effortlessly create compelling videos.

In this technical report, we present Step-Video-T2V, a state-of-the-art video foundation model with 30B parameters, capable of generating high-quality videos from text, featuring strong motion dynamics, high aesthetics, and consistent content. Like most commercial video generation engines, Step-Video-T2V is a diffusion Transformer (DiT)-based model trained using Flow Matching. A specially designed deep compression Variational Auto-encoder (VAE) achieves 16x16 spatial and 8x temporal compression ratios, significantly reducing the computational complexity of large-scale video generation training. Two bilingual text encoders enable Step-Video-T2V to directly understand Chinese or English prompts. A cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (SFT), and direct preference optimization (DPO), is introduced to accelerate model convergence and fully leverage video datasets of varying quality. A new benchmark dataset called Step-Video-T2V-Eval is created for text-to-video generation, which includes 128 diverse prompts across 11 categories, alongside video generation results from several top text-to-video open-source and commercial engines for comparison.

Insights are gained throughout the entire development of Step-Video-T2V, spanning data, model, training, and inference. First, text-to-image pre-training is essential for the video generation model to acquire rich visual knowledge, including concepts, scenes, and their spatial relationships, providing a solid foundation for the subsequent text-to-video pre-training stages. Second, text-to-video pre-training at low resolution is critical for the model to learn motion dynamics. The more stable the model is trained during this stage, using as much diverse training data as possible, the easier it becomes to refine and scale the model to higher resolutions and more complex video generation tasks. Third, using high-quality videos with accurate captions and desired styles in SFT is crucial to the stability of the model and the style of the generated videos. Fourth, video-based DPO can further enhance the visual quality by reducing artifacts, ensuring smoother and more realistic video outputs.

Challenges remain in state-of-the-art video foundation models. For example, current video captioning models still face hallucination issues, leading to unstable training and poor instruction-following performance. Composing multiple concepts with low occurrence in the training data (e.g., an elephant and a penguin) within a single generated video is still a difficult task. Additionally, training and generating long-duration, high-resolution videos still face significant computational cost hurdles. Furthermore, even a DiT-based model like Step-Video-T2V with 30B parameters struggles to generalize well when generating videos involving complex action sequences or requiring adherence to the laws of physics. By open-sourcing Step-Video-T2V, we aim to provide researchers and engineers with a strong baseline, helping them better understand these challenges and accelerate innovations in the development and application of video foundation models.

The key contributions of this technical report are as follows:
\begin{itemize}[left=0cm] 
\item We present and open-source Step-Video-T2V, a state-of-the-art  text-to-video pre-trained model with 30B parameters, capable of understanding both Chinese and English prompts, generating high-quality videos (544x992 resolution) up to 204 frames in length, featuring strong motion dynamics, high aesthetics, and consistent content.
\item We introduce a deep compression Video-VAE for video foundation models, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality.
\item We detail the optimizations of model hyper-parameters, operators, and parallelism in Step-Video-T2V, which ensure both the stability and efficiency of training from a system-level perspective.
\item We describe the process of pre-processing large-scale videos as training data, and explain how these videos are filtered and utilized at different stages of training.
\item We release Step-Video-T2V-Eval as a new benchmark, which includes 128 diverse prompts across 11 categories and video generation results from top open-source and commercial engines.
\item We discuss the insights and challenges encountered in the development of Step-Video-T2V, and identify key issues that must be addressed to advance video foundation models.
\end{itemize}

\section{Related Work}
Video generation technology has seen significant progress over the past year, with advancements from Sora \cite{openaisora} to Gen-3 \cite{runwaygen3}, Kling \cite{kling}, Hailuo \cite{hailuo}, Veo \cite{veo}, and others. 

Commercial video generation engines (e.g., Sora, Gen-3, Kling, and Hailuo) offer text-to-video generation capabilities, as well as extended applications like image-to-video generation or specialized video effect generation. Compared to these closed-source engines, which often involve longer and more complex video generation pipelines with extensive pre- and post-processing, Step-Video-T2V delivers comparable performance for general text prompts and even surpasses them in specific domains, such as generating videos with high motion dynamics or text content.

Open-source video generation models, such as HunyuanVideo \cite{kong2024hunyuanvideo}, CogVideoX \cite{yang2024cogvideox}, Open-Sora \cite{opensora}, and Open-Sora-Plan \cite{lin2024open}, offer greater transparency in their implementations, making them more accessible to researchers and content creators. Both HunyuanVideo and CogVideoX are based on MMDiT \cite{esser2024scalingrectifiedflowtransformers}, a variation of the full attention Transformer architecture. Open-Sora and Open-Sora-Plan are built on DiT \cite{peebles2023scalablediffusionmodelstransformers}, with the former using spatial-temporal attention and the latter employing full attention. Compared to these open-source models, the key contributions of Step-Video-T2V include being the largest open-source model to date, utilizing a high-compression VAE for videos, supporting bilingual text prompts in both English and Chinese, implementing a video-based DPO approach to further reduce artifacts and enhance visual quality, and providing comprehensive training and inference documentation, as outlined in this report.

Movie Gen Video \cite{polyak2024moviegencastmedia} is another video generation model from Meta, featuring a similar architecture and model size. Compared to Movie Gen Video, Step-Video-T2V stands out with four unique features. First, it incorporates a more powerful high compression VAE for large-scale video generation training. Second, it supports bilingual text prompt understanding in both English and Chinese. Third, it adds an additional DPO stage to the training process, reducing
artifacts and improving the visual quality of the generated videos. Fourth, it is open-source and provides state-of-the-art video generation quality comparing with both open-source and commercial engines.

% \textcolor{red}{VAE @Kun}
Videos encompass both spatial and temporal information, leading to significantly larger data volumes compared to images. Addressing the computational challenge of modeling video data efficiently is therefore a fundamental problem. Various methods have been proposed to reduce the complexity of video modeling. These include approaches such as 3D Causal Convolution~\citep{yu2024language,yang2024cogvideox,kong2024hunyuanvideo,opensora}, wavelet transform~\citep{agarwal2025cosmos,li2024wf}, and Residual Autoencoding in images~\citep{chen2025deep}. While these methods show promise in terms of either reconstruction quality or compression ratio, achieving a balance between high quality and effective compression remains difficult. Our work addresses this challenge, providing a solution that opens new possibilities in video generation, such as extending the context length or scaling up the DiT model more aggressively.