

\section{Experiments}

\subsection{Benchmark and Metric}
We build \textbf{Step-Video-T2V-Eval}, a new benchmark for assessing the quality of text-to-video models. This benchmark consists of 128 Chinese prompts sourced from real users and is designed to evaluate the quality of generated videos across 11 categories, including Sports, Food, Scenery, Animals, Festivals, Combined Concepts, Surreal, People, 3D Animation, Cinematography, and Style.

Additionally, we propose two human evaluation metrics based on Step-Video-T2V-Eval, which can be used to compare the performance of Step-Video-T2V with that of a target model:
\begin{itemize}[left=0cm] 
\item \textbf{Metric-1} compares Step-Video-T2V with a target model by having each human annotator assign a Win/Tie/Loss label to each generated video pair from the two models for the same prompt, with the model names masked. A "Win" means Step-Video-T2V performs better than the target model, a "Loss" means it performs worse, and a "Tie" indicates the models have similar quality.
\item \textbf{Metric-2} assigns four scores to each generated video to measure its quality across the following 4 dimensions: (1) instruction following, (2) motion smoothness, (3) physical plausibility, and (4) aesthetic appeal. 
The two models are then compared based on their labeled scores. 
\end{itemize}

The criteria for scoring each dimension in Metric-2 are outlined below:
\begin{itemize}[left=0cm] 
\item \textbf{Instruction Following.} 
\textbf{Score=5}: The video is fully consistent with the prompt, with all elements and details generated accurately, and the expression of complex scenarios is flawless. 
\textbf{Score=4}: The video is generally consistent with the prompt, but there are slight discrepancies in some minor details. 
\textbf{Score=3}: The video mostly meets the prompt’s requirements, but there are noticeable deviations in several details or core content. 
\textbf{Score=2}: The video is clearly inconsistent with the prompt, with significant detail omissions or overall deviations. 
\textbf{Score=1}: The video is completely inconsistent with the prompt, with major scenes or subjects completely incorrect.


\item \textbf{Motion Smoothness.}
\textbf{Score=5}: The motion is smooth and natural, with all movements and transitions flowing seamlessly.
\textbf{Score=4}: The motion is generally smooth, but there are occasional slight unnatural movements in certain scenes.
\textbf{Score=3}: The motion has slight unnatural or stuttering elements, but it doesn’t affect overall understanding.
\textbf{Score=2}: The motion is unnatural or disconnected, with noticeable stuttering.
\textbf{Score=1}: The motion is very unnatural, with frequent stuttering, making it difficult to understand.

\item \textbf{Physical Plausibility.}
\textbf{Score=5}: All object interactions and movements adhere to real-world physical laws, with accurate lighting, shadow, and collision effects, and smooth motion.
\textbf{Score=4}: Most of the physical behavior is reasonable, with occasional minor unnatural collisions or lighting issues, but they don’t affect the overall effect.
\textbf{Score=3}: Several instances of object motion, lighting, or interactions conflict with physical logic, but the main actions still have a degree of coherence.
\textbf{Score=2}: The physical behavior is unrealistic, with lighting or object interactions violating physical laws, making the scene appear unnatural.
\textbf{Score=1}: The physical behavior is completely incorrect, with severe distortion in object interactions or lighting, making the scene difficult to understand.

\item \paragraph{Aesthetic Appeal.}
\textbf{Score=5}: Highly captivating, deeply moving, with significant artistic value and visual appeal.
\textbf{Score=4}: Pleasant and engaging, effectively capturing the audience’s attention with good visual value.
\textbf{Score=3}: Somewhat appealing, but overall performance is mediocre and doesn’t leave a lasting impression.
\textbf{Score=2}: Average, lacking in appeal, and may cause the audience to lose interest.
\textbf{Score=1}: Unpleasant, lacking in appeal, and the overall effect is disappointing.
\end{itemize}


\subsection{Comparisons to Open-source Model}
We first compare Step-Video-T2V with HunyuanVideo on Step-Video-T2V-Eval.

\begin{table}[ht]\scriptsize
\centering
%\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c}
\hline
Step-Video-T2V vs. HunyuanVideo (Win-Tie-Loss) & Annotator-1  & Annotator-2 & Annotator-3 \\
\hline
\hline
Overall & \cellcolor{green!20}{59-22-47} & \cellcolor{green!20}{46-47-35} & \cellcolor{green!20}{54-41-33} \\
\hline
\hline
Sports & \cellcolor{green!20}{6-3-3} & \cellcolor{green!20}{5-5-2} & \cellcolor{green!20}{6-6-0} \\
\hline
Food & \cellcolor{green!20}{5-2-4} & \cellcolor{green!20}{5-4-2} & 3-7-1 \\
\hline
Scenery & \cellcolor{green!20}{5-3-4} & 2-9-1 & \cellcolor{green!20}{7-1-4} \\
\hline
Animals & \cellcolor{yellow!20}{6-0-6} & \cellcolor{yellow!20}{3-6-3} & 2-7-3 \\
\hline
Festivals & \cellcolor{green!20}{4-4-3} & \cellcolor{green!20}{5-2-4} & \cellcolor{green!20}{4-5-2} \\
\hline
Combined Concepts & \cellcolor{yellow!20}{5-2-5} & \cellcolor{green!20}{6-3-3} & \cellcolor{green!20}{8-1-3} \\
\hline
Surreal & 4-2-5 & \cellcolor{green!20}{5-2-4} & \cellcolor{green!20}{6-2-3} \\
\hline
People & \cellcolor{green!20}{6-2-4} & 3-4-5 & \cellcolor{yellow!20}{5-2-5} \\
\hline
3D Animation & \cellcolor{green!20}{7-1-4} & \cellcolor{green!20}{4-5-3} & \cellcolor{green!20}{6-3-3} \\
\hline
Cinematography & \cellcolor{yellow!20}{5-1-5} & 2-5-4 & 1-4-6 \\
\hline
Style & \cellcolor{green!20}{6-2-4} & \cellcolor{green!20}{6-2-4} & \cellcolor{green!20}{6-3-3} \\
\hline
\end{tabular}
%}
\caption{Comparison with HunyuanVideo using Metric-1.}
\label{ranking}
\end{table}



\begin{table}[ht]\scriptsize
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c|c}
\hline
Step-Video-T2V vs. HunyuanVideo & Instruction Following & Motion Smoothness & Physical Plausibility & Aesthetic Appeal \\
\hline
\hline
Overall & \cellcolor{green!20}{1,273-1,221} & \cellcolor{green!20}{1,407-1,327} & \cellcolor{green!20}{1,417-1,238} & \cellcolor{green!20}{1,312-1,238} \\
\hline
\hline
Sports & \cellcolor{green!20}{130-111} & \cellcolor{green!20}{120-104} & \cellcolor{green!20}{113-99} & \cellcolor{green!20}{110-98} \\
\hline
Food & 85-92 & \cellcolor{green!20}{110-97} & \cellcolor{green!20}{107-93} & \cellcolor{green!20}{111-90} \\
\hline
Scenery & \cellcolor{green!20}{130-129} & \cellcolor{green!20}{139-126} & \cellcolor{green!20}{134-120} & \cellcolor{green!20}{125-122} \\
\hline
Animals & 104-106 & \cellcolor{green!20}{123-114} & \cellcolor{green!20}{110-107} & 99-108 \\
\hline
Festivals & \cellcolor{green!20}{102-91} & \cellcolor{green!20}{110-102} & \cellcolor{green!20}{97-90} & \cellcolor{green!20}{103-94} \\
\hline
Combined Concepts & \cellcolor{green!20}{132-115} & \cellcolor{green!20}{139-136} & \cellcolor{green!20}{139-135} & \cellcolor{green!20}{118-115} \\
\hline
Surreal & 99-101 & 138-139 & \cellcolor{green!20}{135-134} & 125-126 \\
\hline
People & 115-117 & \cellcolor{yellow!20}{129-129} & 148-150 & \cellcolor{green!20}{115-112} \\
\hline
3D Animation & \cellcolor{green!20}{113-109} & \cellcolor{green!20}{137-133} & \cellcolor{green!20}{149-146} & \cellcolor{green!20}{139-135} \\
\hline
Cinematography & \cellcolor{green!20}{121-117} & 121-122 & 132-133 & \cellcolor{green!20}{116-115} \\
\hline
Style & \cellcolor{green!20}{142-133} & \cellcolor{green!20}{141-125} & \cellcolor{green!20}{153-134} & \cellcolor{green!20}{151-123} \\
\hline
\end{tabular}
}
\caption{Comparison with HunyuanVideo using Metric-2. We invited three human annotators to evaluate each video. For each category and evaluation dimension, we aggregated the scores given by all annotators across all prompts within the category for that dimension.}
\label{ranking-hy2}
\end{table}

From Table~\ref{ranking} and Table~\ref{ranking-hy2} we got three observations.

First, Step-Video-T2V demonstrates state-of-the-art performance as the strongest open-source text-to-video generation model to date. This success is attributed to multiple factors, including the model’s structural design and its pre-training and post-training strategies.
Second, in some categories like Animals, Step-Video-T2V performs worse than HunyuanVideo, as shown in Table~\ref{ranking}. This is primarily due to aesthetic issues, as verified by the Aesthetic Appeal score in Table~\ref{ranking-hy2}.
Third, Video-VAE achieves compression ratios of 16x16 spatial and 8x temporal, compared to HunyuanVideo’s 8x8 spatial and 4x temporal compression. This higher compression rate enables Step-Video-T2V to generate videos up to 204 frames, nearly double the 129-frame maximum of HunyuanVideo.

\subsection{Comparisons to Commercial Model}
We then compare Step-Video-T2V with two leading text-to-video engines in China, T2VTopA (2025-02-10 version) and T2VTopB (2025-02-10 version), on Step-Video-T2V-Eval.


\begin{table}[ht]\scriptsize
\centering
%\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c}
\hline
Step-Video-T2V vs. T2VTopA (Win-Tie-Loss) & Annotator-1  & Annotator-2 & Annotator-3 \\
\hline
\hline
Overall & 44-13-69 & 41-13-72 & 46-25-55 \\
\hline
\hline
Sports & \cellcolor{green!20}{6-2-4} & \cellcolor{green!20}{7-0-5} & \cellcolor{green!20}{7-3-2} \\
\hline
Food & \cellcolor{green!20}{5-2-4} & \cellcolor{green!20}{6-1-4} & 4-2-5 \\
\hline
Scenery & 1-0-10 & 4-0-7 & 1-2-8 \\
\hline
Animals & 1-3-8 & 1-3-8 & 3-1-8 \\
\hline
Festivals & \cellcolor{green!20}{6-2-3} & \cellcolor{green!20}{7-2-2} & \cellcolor{green!20}{5-3-3} \\
\hline
Combined Concepts & 2-0-10 & 1-3-8 & \cellcolor{green!20}{8-0-4} \\
\hline
Surreal & 4-1-6 & 3-2-6 & 4-2-5 \\
\hline
People & 2-1-8 & 2-1-8 & \cellcolor{green!20}{6-1-4} \\
\hline
3D Animation & \cellcolor{yellow!20}{6-0-6} & 3-0-9 & \cellcolor{green!20}{5-3-4} \\
\hline
Cinematography & \cellcolor{yellow!20}{5-1-5} & 4-1-6 & 1-3-7 \\
\hline
Style & \cellcolor{green!20}{6-1-5} & 3-0-9 & 2-5-5 \\
\hline
\end{tabular}
%}
\caption{Comparison with T2VTopA using Metric-1. A total of 126 prompts were evaluated, rather than 128, as T2VTopA rejected 2 prompts.} 
\label{ranking-hailuo}
\end{table}



\begin{table}[ht]\scriptsize
\centering
%\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c}
\hline
Step-Video-T2V vs. T2VTopB (Win-Tie-Loss) & Annotator-1  & Annotator-2 & Annotator-3 \\
\hline
\hline
Overall & 36-35-51 & \cellcolor{green!20}{67-10-45} & \cellcolor{green!20}{55-22-45} \\
\hline
\hline
Sports & \cellcolor{green!20}{8-2-2} & \cellcolor{green!20}{10-1-1} & \cellcolor{green!20}{8-2-2} \\
\hline
Food & 3-4-3 & \cellcolor{green!20}{7-1-2} & \cellcolor{green!20}{7-2-1} \\
\hline
Scenery & 2-6-4 & \cellcolor{yellow!20}{5-2-5} & \cellcolor{green!20}{5-4-3} \\
\hline
Animals & \cellcolor{yellow!20}{5-1-5} & 3-1-7 & 2-2-7 \\
\hline
Festivals & \cellcolor{green!20}{6-1-4} & \cellcolor{green!20}{6-0-5} & 2-4-5 \\
\hline
Combined Concepts & 1-4-7 & \cellcolor{green!20}{6-1-5} & 4-2-6 \\
\hline
Surreal & 2-0-6 & 3-0-5 & 2-1-5 \\
\hline
People & 1-3-7 & 4-1-6 & 3-1-7 \\
\hline
3D Animation & \cellcolor{green!20}{5-3-4} & \cellcolor{green!20}{11-0-1} & \cellcolor{green!20}{11-0-1} \\
\hline
Cinematography & 3-3-5 & 4-2-5 & 3-1-7 \\
\hline
Style & 0-8-4 & \cellcolor{green!20}{8-1-3} & \cellcolor{green!20}{8-3-1} \\
\hline
\end{tabular}
%}
\caption{Comparison with T2VTopB using Metric-1. A total of 122 prompts were evaluated, rather than 128, as T2VTopB rejected 6 prompts.}
\label{ranking-kling}
\end{table}





\begin{table}[ht]\scriptsize
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
\hline
 & Model & Instruction Following & Motion Smoothness & Physical Plausibility & Aesthetic Appeal \\
\hline
\hline
\multirow{3}*{Annotator-1} & Step-Video-T2V & 204 & \cellcolor{green!20}{210} & \cellcolor{green!20}{203} & 187 \\
~ & T2VTopA & \cellcolor{green!20}{211} & 200 & 198 & \cellcolor{green!20}{196} \\
~ & T2VTopB & 185 & 184 & 178 & 175 \\
\hline
\hline
\multirow{3}*{Annotator-2} & Step-Video-T2V & 211 & \cellcolor{green!20}{243} & \cellcolor{green!20}{256} & 217 \\
~ & T2VTopA & \cellcolor{green!20}{241} & \cellcolor{green!20}{243} & 242 & \cellcolor{green!20}{228} \\
~ & T2VTopB & 234 & 236 & 229 & 204 \\
\hline
\hline
\multirow{3}*{Annotator-3} & Step-Video-T2V & 170 & \cellcolor{green!20}{197} & \cellcolor{green!20}{172} & \cellcolor{green!20}{178} \\
~ & T2VTopA & \cellcolor{green!20}{177} & 177 & 153 & 171 \\
~ & T2VTopB & 164 & 163 & 139 & 148 \\
\hline
\hline
\multirow{3}*{Annotator-4}& Step-Video-T2V & 199 & \cellcolor{green!20}{232} & \cellcolor{green!20}{230} & \cellcolor{green!20}{225} \\
~ & T2VTopA & \cellcolor{green!20}{217} & 221 & 201 & 199 \\
~ & T2VTopB & 194 & 219 & 194 & 194 \\
\hline
\hline
\multirow{3}*{Annotator-5} & Step-Video-T2V & 218 & \cellcolor{green!20}{225} & \cellcolor{green!20}{213} & 211 \\
~ & T2VTopA & \cellcolor{green!20}{221} & 220 & \cellcolor{green!20}{213} & \cellcolor{green!20}{212} \\
~ & T2VTopB & 209 & 217 & 202 & 196 \\
\hline
\hline
\multirow{3}*{Annotator-6} & Step-Video-T2V & 187 & 213 & 251 & 211 \\
~ & T2VTopA & 193 & 201 & 259 & 197 \\
~ & T2VTopB & \cellcolor{green!20}{201} & \cellcolor{green!20}{224} & \cellcolor{green!20}{271} & \cellcolor{green!20}{227} \\
\hline
\end{tabular}
}
\caption{Comparison with T2VTopA and T2VTopB using Metric-2. We invited six human annotators to evaluate each video. For each evaluation dimension, we aggregated the scores given by each annotator across all prompts for that dimension. Prompts that were rejected by any model were excluded from the analysis for all models.}
\label{ranking-2}
\end{table}


From Table~\ref{ranking-hailuo}, Table~\ref{ranking-kling}, and Table~\ref{ranking-2} we got three observations.

First, the overall ranking of the three models in Table~\ref{ranking-hailuo} and Table~\ref{ranking-kling} is as follows: T2VTopA > Step-Video-T2V > T2VTopB. We analyzed categories such as Scenery, Animals, People, and Style, where Step-Video-T2V performs worse than the other two models, and found that the primary reason lies in their generally higher aesthetic appeal. We believe this advantage mainly stems from the higher resolutions of the generated videos (720P in T2VTopA, 1080P in T2VTopB, and 540P in Step-Video-T2V) and the high-quality aesthetic data used during their post-training stages. Table~\ref{ranking-2} also shows that 4 out of 6 annotators rate T2VTopA and T2VTopB as having higher aesthetic appeal.

Second, Step-Video-T2V consistently outperforms T2VTopA and T2VTopB in the Sports category in Table~\ref{ranking-hailuo} and Table~\ref{ranking-kling}, demonstrating its strong capability in modeling and generating videos with high-motion dynamics. Table~\ref{ranking-2} also highlights Step-Video-T2V's superiority in Motion Smoothness and Physical Plausibility.

Third, we observed that T2VTopA has better instruction-following capability, which contributes to its superior performance in categories such as Combined Concepts, Surreal, and Cinematography. We believe the key reasons for this are better video captioning model and the greater human effort involved in labeling the post-training data used by T2VTopA.

Note that Step-Video-T2V still lacks sufficient training in the final stage of pre-training with 540P videos, having only seen 25.3M samples (as shown in Table~\ref{trainingrecipe}). Additionally, compared to these two commercial engines, we are using significantly less high-quality data in the post-training phase, which will be continuously improved in the future. Finally, the video length is 204 frames, nearly twice the length of T2VTopA and T2VTopB, making our training more challenging. We assert that Step-Video-T2V has already achieved the strongest motion dynamics modeling and generation capabilities among all commercial engines. Given comparable training resources and high-quality data, we believe it can achieve state-of-the-art results in general domains as well.

We also compare Step-Video-T2V with the international commercial text-to-video engine, Runway Gen-3 Alpha, and present the results in Table~\ref{ranking-runway1} and Table~\ref{ranking-runway2}. Since Gen-3 Alpha has a stronger understanding of English, we translate the Chinese prompts into English before generating results. As shown in Table~\ref{ranking-runway1} and Table~\ref{ranking-runway2}, Step-Video-T2V outperforms Gen-3 Alpha overall, while Gen-3 Alpha excels in generating videos within the Cinematography domain.


\begin{table}[ht]\scriptsize
\centering
%\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c}
\hline
Step-Video-T2V vs. Gen-3 Alpha (Win-Tie-Loss) & Annotator-1  & Annotator-2 & Annotator-3 \\
\hline
\hline
Overall & \cellcolor{green!20}{68-3-38} & \cellcolor{green!20}{60-27-25} & \cellcolor{green!20}{54-36-22} \\
\hline
\hline
Sports & \cellcolor{green!20}{10-0-2} & \cellcolor{green!20}{11-1-0} & \cellcolor{green!20}{6-5-1} \\
\hline
Food & \cellcolor{green!20}{10-0-1} & \cellcolor{green!20}{7-2-2} & \cellcolor{green!20}{5-3-3} \\
\hline
Scenery & \cellcolor{green!20}{7-2-3} & \cellcolor{green!20}{7-2-3} & \cellcolor{green!20}{7-1-4} \\
\hline
Animals & \cellcolor{green!20}{7-1-4} & \cellcolor{green!20}{7-3-2} & 4-7-1 \\
\hline
Festivals & \cellcolor{green!20}{7-0-4} & \cellcolor{green!20}{6-5-0} & \cellcolor{green!20}{2-9-0} \\
\hline
Combined Concepts & \cellcolor{green!20}{6-1-5} & \cellcolor{yellow!20}{4-4-4} & \cellcolor{green!20}{9-0-3} \\
\hline
Surreal & 1-1-4 & 2-1-3 & \cellcolor{green!20}{6-0-0} \\
\hline
People & \cellcolor{green!20}{5-1-6} & \cellcolor{green!20}{5-3-4} & \cellcolor{green!20}{7-3-2} \\
\hline
3D Animation & \cellcolor{green!20}{1-0-0} & \cellcolor{green!20}{1-0-0} & \cellcolor{yellow!20}{0-1-0} \\
\hline
Cinematography & 4-0-7 & 2-3-6 & 3-3-5 \\
\hline
Style & \cellcolor{green!20}{10-0-2} & \cellcolor{green!20}{8-3-1} & \cellcolor{green!20}{5-4-3} \\
\hline
\end{tabular}
%}
\caption{Comparison with Runway Gen-3 Alpha using Metric-1, excluding prompts that were rejected by Gen-3 Alpha.} 
\label{ranking-runway1}
\end{table}




\begin{table}[ht]\scriptsize
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
\hline
 & Model & Instruction Following & Motion Smoothness & Physical Plausibility & Aesthetic Appeal \\
\hline
\hline
\multirow{2}*{Annotator-1} & Step-Video-T2V & \cellcolor{green!20}{214} & \cellcolor{green!20}{221} & \cellcolor{green!20}{214} & \cellcolor{green!20}{198} \\
~ & Gen-3 Alpha & 178 & 180 & 150 & 169 \\
\hline
\hline
\multirow{2}*{Annotator-2} & Step-Video-T2V & 183 & \cellcolor{green!20}{200} & \cellcolor{green!20}{210} & 173 \\
~ & Gen-3 Alpha & \cellcolor{green!20}{185} & 173 & 177 & \cellcolor{green!20}{176} \\
\hline
\hline
\multirow{2}*{Annotator-3} & Step-Video-T2V & 174 & \cellcolor{green!20}{202} & \cellcolor{green!20}{176} & 184 \\
~ & Gen-3 Alpha & \cellcolor{green!20}{179} & 180 & 158 & \cellcolor{green!20}{194} \\
\hline
\hline
\multirow{2}*{Annotator-4}& Step-Video-T2V & \cellcolor{green!20}{162} & \cellcolor{green!20}{186} & \cellcolor{green!20}{189} & \cellcolor{green!20}{183} \\
~ & Gen-3 Alpha & 147 & 165 & 133 & 160 \\
\hline
\hline
\multirow{2}*{Annotator-5} & Step-Video-T2V & \cellcolor{green!20}{228} & \cellcolor{green!20}{237} & \cellcolor{green!20}{225} & \cellcolor{green!20}{211} \\
~ & Gen-3 Alpha & 200 & 189 & 149 & 166 \\
\hline
\hline
\multirow{2}*{Annotator-6} & Step-Video-T2V & 160 & \cellcolor{green!20}{178} & \cellcolor{green!20}{207} & \cellcolor{green!20}{171} \\
~ & Gen-3 Alpha & \cellcolor{green!20}{178} & 161 & 182 & 153 \\
\hline
\end{tabular}
}
\caption{Comparison with Runway Gen-3 Alpha using Metric-2. We invited six human annotators to evaluate each video. For each evaluation dimension, we aggregated the scores given by each annotator across all prompts for that dimension. Prompts that were rejected by Gen-3 Alpha were excluded from the analysis for all models.}
\label{ranking-runway2}
\end{table}




\subsection{Evaluation on Movie Gen Video Bench}
Movie Gen Video Bench \cite{polyak2024moviegencastmedia} is another existing benchmark for the text-to-video generation task. It includes 1,003 prompts across multiple categories, covering human activities, animals, nature and scenery, physics, as well as unusual subjects and activities. Although Movie Gen Video has not been open-sourced, its generated results on the Movie Gen Video Bench are publicly available (\hyperlink{https://github.com/facebookresearch/MovieGenBench}{https://github.com/facebookresearch/MovieGenBench}). Therefore, we also compare Step-Video-T2V with Movie Gen Video and HunyuanVideo in Table~\ref{ranking-moviegenbench} using this benchmark.



\begin{table}[ht]\scriptsize
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c}
\hline
Category & \makecell[c]{Step-Video-T2V vs. Movie Gen Video \\ (Win-Tie-Loss)} & \makecell[c]{Step-Video-T2V vs. HunyuanVideo \\ (Win-Tie-Loss)} & \# of Prompts \\
\hline
\hline
Overall & 485-315-489 & \cellcolor{green!20}{615-313-361} & 1,289 \\
\hline
\hline
%human - activity & 77-43-84 & \cellcolor{green!20}{122-41-41} & 204 \\
%\hline
%human - emotion & 22-5-28 & \cellcolor{green!20}{26-7-22} & 55 \\
%\hline
%human - mouth & 13-7-26 & \cellcolor{green!20}{19-10-17} & 46 \\
%\hline
%human - hand & 11-3-22 & 14-6-16 & 36 \\
%\hline
human & 123-58-160 & \cellcolor{green!20}{181-64-96} & 341 \\
\hline
%physics - fluid dynamics & 12-14-18 & \cellcolor{green!20}{20-14-10} & 44 \\
%\hline
%physics - acceleration & 13-12-15 & \cellcolor{green!20}{21-6-13} & 40 \\
%\hline
%physics - collision & \cellcolor{green!20}{10-9-5} & \cellcolor{green!20}{17-1-6} & 24 \\
%\hline
%physics - gravity & 7-7-8 & \cellcolor{green!20}{9-9-4} & 22 \\
%\hline
%physics - deformation & 2-3-4 & \cellcolor{green!20}{4-4-1} & 9 \\
%\hline
%physics - optical phenomena & \cellcolor{green!20}{10-4-6} & \cellcolor{green!20}{8-6-6} & 20 \\
%\hline
%physics - thermodynamics & 7-5-8 & \cellcolor{green!20}{8-7-5} & 20 \\
%\hline
physics & 61-54-64 & \cellcolor{green!20}{87-47-45} & 179 \\
\hline
%unusual subject & \cellcolor{green!20}{34-18-27} & \cellcolor{green!20}{39-27-13} & 79 \\
%\hline
%unusual activity & 76-56-81 & \cellcolor{green!20}{97-48-68} & 213 \\
%\hline
unusual activity \& subject & \cellcolor{green!20}{110-74-108} & \cellcolor{green!20}{136-75-81} & 292 \\
\hline
animal & 39-37-42 & \cellcolor{green!20}{47-30-41} & 118 \\
\hline
scene & \cellcolor{green!20}{84-53-6}3 & \cellcolor{green!20}{91-58-51} & 200 \\
\hline
sequential motion & \cellcolor{green!20}{9-2-2} & \cellcolor{green!20}{6-2-5} & 13 \\
\hline
camera motion & \cellcolor{green!20}{59-37-50} & \cellcolor{green!20}{67-37-42} & 146 \\
\hline
\end{tabular}
}
\caption{Comparison of Movie Gen Video and HunyuanVideo using the Movie Gen Video Bench. The total number of evaluations (1,289) is greater than 1,003 due to some prompts having multiple category tags. This evaluation involved six human annotators.}
\label{ranking-moviegenbench}
\end{table}





Compared to Movie Gen Video, Step-Video-T2V achieves a comparable performance.
We got several observations from this comparison.
First, the pre-training of Step-Video-T2V remains insufficient. While Movie Gen Video was trained on 73.8M videos during its high-resolution pre-training phase, Step-Video-T2V was trained on only 27.3M videos—about one-third of the number used by Movie Gen Video. Additionally, we observed that the training curves for all pre-training stages in Step-Video-T2V continue to show a downward trend. Due to resource limitations, we plan to conduct more extensive pre-training as part of our future work.
Second, the Movie Gen Video paper highlights the significant human effort involved in labeling the high-quality SFT dataset. However, due to limited human resources, we lack enough high-quality labeled data at this stage to effectively refine the visual style and quality of the generated results.
Third, Movie Gen Video can generate 720P videos, which are visually more appealing than the 540P resolution produced by Step-Video-T2V. Feedback from human annotators suggests that high resolution can often be a key factor in determining which model performs better.
Compared to HunyuanVideo, Step-Video-T2V achieves significant improvements across all categories, solidifying its position as the state-of-the-art open-source text-to-video model.


\subsection{Generating Text Content in Videos}
We also compare Step-Video-T2V with open-source and commercial engines on a list of prompts such as "\textit{a squirrel holding a sign that says 'hello'.}", where the model is required to generate videos that include text content as well.

Our observations show that Step-Video-T2V outperforms all other models in generating basic English text. We attribute this capability to the T2I pre-training stage, where a portion of the images contained text, and the captions explicitly described it. However, the accuracy of text generation remains far from ideal. Furthermore, due to the complexity of Chinese characters, Step-Video-T2V is currently able to generate only a limited number of them. Enhancing text generation capabilities for both English and Chinese will be a focus of our future work.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.3\textwidth, center, trim=0 0 0 0, clip]{figure/textgeneration.png}
    \caption{Four frames sampled from the video generated based on the prompt "\textit{In the video, a Chinese girl is dressed in an exquisite traditional outfit, smiling with a confident and graceful expression. She holds a piece of paper with the words "we will open source" clearly written on it. The background features an ancient and elegant setting, complementing the girl's demeanor. The entire scene is clear and has a realistic style.}".}
    \label{fig:text}
\end{figure}

\subsection{VAE Video Reconstruction}

\begin{table}[h]\scriptsize
    \centering
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Model} & \textbf{Downsample Factor} & \textbf{SSIM$\uparrow$} & \textbf{PSNR$\uparrow$} & \textbf{rFVD$\downarrow$} \\
    \midrule
    OpenSora-1.2 (\cite{opensora}) & 4 × 8 × 8 & 0.9126 & 31.41 & 20.42 \\
    CogvideoX-1.5 (\cite{yang2024cogvideox}) & 4 × 8 × 8 & 0.9373 & 38.10 & 16.33 \\
    HunyuanVideo (\cite{kong2024hunyuanvideo}) & 4 × 8 × 8 & 0.9710 & \cellcolor{green!20}{39.56} & 4.17 \\
    Cosmos-VAE (\cite{agarwal2025cosmos}) & 4 × 8 × 8 & 0.9315 & 37.66 & 9.10 \\
    \hline
    Cosmos-VAE (\cite{agarwal2025cosmos}) & 8 × 16 × 16 & 0.8862 & 34.82 & 40.33 \\
    \hline
    Video-VAE (Ours) & 8 × 16 × 16 & \cellcolor{green!20}{0.9776} & 39.37 & \cellcolor{green!20}{3.61} \\
    \bottomrule
    \end{tabular}
    \caption{Comparison of reconstruction metrics.}
    \label{tab: vae}
\end{table}

We compare Video-VAE with several open-source baselines using 1,000 test videos from various domains, each with dimensions of 50(frames)×480(height)×768(width). As shown in Table~\ref{tab: vae}, despite having a compression ratio 8 times larger than most baselines, our reconstruction quality still maintains state-of-the-art performance. While Cosmos-VAE also offers a high-compression version with a factor of 8×16×16, its reconstruction quality falls significantly behind our method.

Figure~\ref{fig:vae} illustrates typical challenge cases in video reconstruction, including high-motion (first row), text (second row), texture (third row), high-motion combined with text (fourth row), and high-motion combined with texture (fifth row). Our models significantly outperform other baselines, even with higher compression ratios.
\begin{figure}[ht]
    \centering
    % \begin{adjustbox}{width=1.3\textwidth,center}
    \includegraphics[width=1.3\textwidth, center, trim=0 0 0 0, clip]{figure/vae_results/motion.png}
    \includegraphics[width=1.3\textwidth, center, trim=0 0 0 0, clip]{figure/vae_results/text.png}
    \includegraphics[width=1.3\textwidth, center, trim=0 0 0 0, clip]{figure/vae_results/texture.png}
    \includegraphics[width=1.3\textwidth, center, trim=0 0 0 0, clip]{figure/vae_results/motion_text.png}
    \includegraphics[width=1.3\textwidth, center, trim=0 0 0 0, clip]{figure/vae_results/motion_texture.png}
    % \end{adjustbox}
    \caption{Video reconstruction results compared with public available models, in scenarios including high-motion (first row), text (second row), texture (third row), high-motion combined with text (fourth row), and high-motion combined with texture (fifth row).}
    \label{fig:vae}
\end{figure}
\subsection{DPO}
To assess the effectiveness of the proposed Video-DPO algorithm, we conduct inference on 300 diverse prompts. The evaluation involves two models: the baseline model and the baseline model with the Video-DPO enhancement (baseline w/. DPO). Both models are sampled under identical initial noise conditions to control for extraneous variables and ensure a fair comparison. For each generated video, three independent annotators are tasked with evaluating their preference between the two models, with an option to select "no preference". The evaluation protocol is as follows: 
\begin{itemize}[left=0cm]
    \item If an annotator prefers the video generated by "baseline w/. DPO", the model receives 1 point.
    \item If an annotator prefers the "baseline" video, the baseline model receives 1 point.
    \item If an annotator indicates "no preference," both models receive 0.5 points.
\end{itemize}
Upon aggregating the scores, we find that the baseline model with DPO (baseline w/. DPO) achieves a preference score of 55\%, outperforming the baseline model (45\%). This result demonstrates the efficacy of Video-DPO in generating videos more aligned with user preferences. The visual comparison is shown in Figure~\ref{fig:dpovisual}, demonstrates that human feedback enhances the plausibility and consistency of generated videos. Additionally, we observe that the DPO baseline enhances the alignment with the given prompts, resulting in more accurate and relevant video generation. 

While Video-DPO demonstrates effectiveness, several issues remain. (1) The trajectory from initial noise to timestep-specific latents acts as implicit dynamic conditions beyond text prompts — yet this dimension remains underutilized due to computational limitations. (2)  A tradeoff exists between sparse and imprecise feedback, especially in video diffusion models. For instance, in videos with over 100 million pixels, only a few pixels may be problematic, yet feedback often comes as a single scalar or lacks precision. (3) Unlike LLMs, which use token-level softmax to create competition between tokens, diffusion models rely on regression, which may result in less efficient preference optimization. We hope these discussions provide insights and inspire further algorithmic advancements in incorporating human feedback.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.3\textwidth, center, trim=0 0 0 0, clip]{figure/dpo/dpo_visuals.pdf}
    \caption{Visual comparison of video generation with and without the DPO baseline.}
    \label{fig:dpovisual}
    %\vspace{-6mm}
\end{figure*}


\section{Discussion}
\subsection{Model Architecture}
Unlike DiT, which relies on a modulation mechanism to condition the network on the text prompt, MMDiT integrates the text prompt directly into the Transformer, separates the weights for text and video, and uses a shared attention mechanism to merge the latent representations of both modalities. We compared the training curves of DiT and MMDiT in the early stages and found both architectures exhibited similar performance. Given these comparable results, along with DiT's ability to disentangle text and video and its natural extension to pure video prediction models without text, we ultimately selected DiT as the model architecture for Step-Video-T2V. Due to computational cost constraints, we did not train MMDiT-based model for an extended period to assess its upper performance limit.

We also compared spatial-temporal attention and 3D full attention mechanisms within DiT. In the spatial-temporal attention mechanism, the model captures spatial information among tokens with the same temporal index within spatial Transformer blocks, and temporal information across time steps in temporal Transformer blocks. In contrast, 3D full attention mechanism combines both spatial and temporal information in a unified attention process, offering higher performance potential but at the cost of increased computational demands. We trained two DiT models—one using spatial-temporal attention and the other using 3D full attention—both in a 4B setting. Upon comparing their performances, we found that 3D full attention-based model outperforms spatial-temporal attention-based model, particularly in generating videos with high motion dynamics. Given its superior quality, we ultimately selected the 3D full attention setting.

In addition, 3D full attention is known for its high training and inference cost, so we are still actively investigating more efficient way to reduce the computation overhead, while preserving the same model quality~\cite{dsv}.


\subsection{Instruction Following}


Based on the evaluation results, we found that even a DiT-based model like Step-Video-T2V, with 30B parameters, struggles to generate videos involving complex action sequences. Additionally, generating videos that incorporate multiple concepts with low occurrence in the training data (e.g., an elephant and a penguin) remains challenging in Step-Video-T2V and other leading text-to-video generation models. Both of these challenges can be viewed as instruction-following problems.

We examine the instruction-following capability of Step-Video-T2V, focusing on how it interprets instructions involving various objects, actions, and other details. Our analysis reveals that the distribution of cross-attention scores is occasionally highly concentrated, with a strong focus on specific objects or actions. This pronounced attention can result in missing objects, wrong details, or incomplete action sequences in the generated videos.




By heuristically repeating the missing objects in the prompt, some of the problematic cases can be significantly improved. This demonstrates the importance of ensuring that all elements in the prompt receive appropriate attention. We leave the task of balancing this attention for future work, aiming to refine the model’s ability to better attend and follow all elements in the prompt.

\subsection{Laws of Physics Following}
We analyzed a number of videos generated by leading text-to-video models, including Sora, Hailuo, Kling, and Step-Video-T2V, and found that all of these models struggle to accurately simulate the real world and generate videos that adhere to the laws of physics—such as \textit{a ball bouncing on the floor} or \textit{a drop of water falling into a cup}. Some text-to-video engines can produce good results for certain prompts, but these successes are often due to the model over-fitting to specific annotations, and cannot generalize well. 

This finding highlights a key limitation of diffusion-based models in text-to-video generation. To address this challenge, we plan to develop more advanced model paradigms in future work, such as combining autoregressive and diffusion models within a unified framework (\cite{chen2024diffusionforcingnexttokenprediction}, \cite{hacohen2024ltxvideorealtimevideolatent}, \cite{zhou2025tamingteacherforcingmasked}), to better adhere to the laws of physics and more accurately simulate realistic interactions.




\subsection{High-quality Labeled Data for Post-training}
By applying a small amount of high-quality human-labeled data in SFT, Step-Video-T2V achieves significant improvements in the overall video quality, demonstrating that the quality and diversity of the data outweigh its sheer scale. We also observed that certain characteristics of these curated high-quality datasets, such as video style and the degree of motion dynamics, generalize well across a broader range of prompts. This further underscores the importance of high-quality, small-scale, and diverse datasets for post-training.

Curating such datasets is both expensive and time-consuming, involving tasks such as selecting high-quality videos from a large pool, labeling them with accurate captions, and ensuring the dataset covers a diverse range of objects, actions, styles, and domains. We plan to build a comprehensive video knowledge base with structured labels as part of our future work.

\subsection{RL-based Optimization Mechanism for Post-training}
We employed a simple yet effective DPO-based model for video generation and also explored training a reward model to automate the entire post-training process. However, the proposed method still requires human labeling efforts in the early stages and is time-consuming when extending it to general domains. On the other hand, RL-based approaches have achieved great success in LLMs, such as OpenAI-O1 and DeepSeek-R1 \cite{deepseekai2025deepseekr1incentivizingreasoningcapability}. However, unlike RL-focused natural language tasks, such as solving math problems or generating code, which have well-defined problems with clear answers, it remains challenging to define similar tasks in the video generation domain. We consider this a key challenge for future research exploration.


