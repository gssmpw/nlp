\begin{table*}[t]
\centering
\setlength{\tabcolsep}{2pt}
\caption{
\textbf{Few-shot CoT reasoning results of math pretraining.} All models are tested with few-shot prompting. Previous best results are highlighted in \hlb{blue}, while our best results are in \hl{purple}. $^*$Only unique math-related tokens are calculated. For \model{}, we calculate only the selected tokens that are used for training. $^\dagger$We use OpenAI's MATH subset \citep{lightman2023let} for evaluation, since some original test samples have been used in public training sets such as PRM800k. $^\ddagger$The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available.
}
\label{tab:math-cot-result}
\resizebox{\linewidth}{!}{
\begin{tabular}{lrcrr|ccccccccc|c}
\toprule
\textbf{Model} & $|\boldsymbol{\theta}|$ & \textbf{Data} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Uniq.\\ Toks$^*$\end{tabular}}}& \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Train\\ Toks\end{tabular}}} & \textbf{GSM8K} & \textbf{MATH$^\dagger$} & \textbf{SVAMP} & \textbf{ASDiv} & \textbf{MAWPS} & \textbf{TAB}& \textbf{MQA} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}MMLU\\ STEM\end{tabular}}} & \textbf{SAT$^\ddagger$} & \textbf{AVG} \\
\midrule
\multicolumn{15}{c}{\texttt{1-2B Base Models}} \\
\midrule
\href{https://huggingface.co/Tinyllama/Tinyllama-1.1B-intermediate-step-1431k-3T}{Tinyllama}& 1.1B & - & - & - & 2.9 & 3.2 & 11.0 & 18.1 & 20.4 & 12.5 & 14.6 & 16.1 & 21.9 & 13.4 \\
\href{https://huggingface.co/microsoft/phi-1_5}{Phi-1.5} & 1.3B & - & - & - & 32.4 & 4.2 & 43.4 & 53.1 & 66.2 & 24.4 & 14.3 & 21.8 & 18.8 & 31.0 \\
\href{https://huggingface.co/Qwen/Qwen1.5-1.8B}{Qwen1.5} & 1.8B & - & - & - & \hlb{36.1} & 6.8 & \hlb{48.5} & \hlb{63.6} & \hlb{79.0} & 29.2 & 25.1 & 31.3 & 40.6 & \hlb{40.0} \\
\href{https://huggingface.co/google/gemma-2b}{Gemma} & 2.0B & - & - & - & 18.8 & 11.4 & 38.0 & 56.6 & 72.5 & \hlb{36.9} & \hlb{26.8} & \hlb{34.4} & 50.0 & 38.4 \\ 
% \href{https://huggingface.co/openbmb/MiniCPM-2B-sft-bf16}{MiniCPM} & 2.4B& - & - & - & 52.7 & 16.0 & 67.0 & 73.2 & 90.1 & 37.6 & 40.9 & 43.1 & 46.9 & 51.9 \\
% \href{https://huggingface.co/microsoft/phi-2}{Phi2} & 2.7B & - & - & - & 55.3 & 11.8 & 65.4 & 75.5 & 84.0 & 43.6 & 38.2 & 47.9 & 40.6 & 51.4 \\
DeepSeekLLM & 1.3B & OWM & 14B & 150B & 11.5 & 8.9 & - & - & - & -  & - & 29.6 & 31.3 & -\\
% DeepSeekLLM-1.3B & Proof-Pile-2 & 55B & 150B & 14.3 & 11.2 & - & - & - & - \\
DeepSeekMath & 1.3B & - & 120B & 150B & 23.8 & \hlb{13.6} & - & - & - & - & - & {33.1} & \hlb{56.3} & - \\
\midrule
\multicolumn{15}{c}{\texttt{Continual Pretraining on Tinyllama-1B}}
\\ \midrule
% Tinyllama-1B & Proof-Pile-2 & 55B & 100B & 8.9 & 6.0 & 25.6 & 42.1 & 53.6 & 27.2 \\
Tinyllama-CT & 1.1B & OWM & 14B & 15B & 6.4 & 2.4 & 21.7 & 36.7 & 47.7 & 17.9 & 13.9 & 23.0 & 25.0 & 21.6 \\
\model{}-Math & 1.1B & OWM & 14B & {9B} & 29.8 & 14.0 & 49.2 & 61.4 & 79.8 & 25.8 & 30.4 & \hl{24.7} & 28.1 & 38.1 \\
$\Delta$  & & & & \grey{-40\%} & \blue{+23.4} & \blue{+11.6} & \blue{+27.5} & \blue{+24.7} & \blue{+32.1} & \blue{+7.9} & \blue{+16.5} & \blue{+1.7} & \blue{+3.1} & \blue{\textbf{+16.5}} \\
% {\model{}-Math t4} & 1.1B & OWM & 14B & {9B} & 30.1 & 12.2 & 45.4 & 60.1 & 77.6 & 22.6 & 27.3 & \hl{24.1} & 24.0 & 35.9\\
% $\Delta$ T4 & & & & \grey{-6B} & \blue{+23.7} & \blue{+9.8} & \blue{+23.7} & \blue{+23.4} & \blue{+29.9} & \blue{+4.7} & \blue{+13.4} & \blue{+1.1} & \blue{-1.0} & \blue{\textbf{+14.3}}\\  
\midrule
\model{}-Math & 1.1B & OWM & 14B & {30B} & \hl{36.2} & \hl{15.6} & \hl{52.1} & \hl{67.0} & \hl{83.9} & \hl{29.0} & \hl{32.5} & {23.3} & \hl{28.1} & \hl{40.9} \\
\midrule
\multicolumn{15}{c}{\texttt{$\ge$ 7B Base Models}}
\\ \midrule
\href{https://huggingface.co/meta-llama/Llama-2-7b-hf}{LLaMA-2} & 7B & & - & - & 14.0 & 3.6 & 39.5 & 51.7 & 63.5 & 30.9 & 12.4 & 32.7 & 34.4 & 31.4 \\
% \href{https://huggingface.co/codellama/CodeLlama-7b-Python-hf}{CodeLlama} & 7B & & - & - & 12.4 & 7.0 & 44.5 & 49.0 & 64.9 & 25.5 & 17.2 & 24.1 & 34.4 & 31.0 \\ 
\href{https://huggingface.co/mistralai/Mistral-7B-v0.1}{Mistral} & 7B & & - & - & 41.2 & 11.6 & 64.7 & 68.5 & 87.5 & 52.9 & 33.0 & 49.5 & 59.4 & 52.0 \\
Minerva & 8B & - & 39B & 164B & 16.2 & 14.1 & -& -& -& -& -& 35.6 & - & - \\
Minerva & 62B & - & 39B & 109B & 52.4 & 27.6 & -& -& -& -& -& 53.9 & - & - \\
Minerva & 540B & - & 39B & 26B & 58.8 & {33.6} & -& -& -& -& -& \hlb{63.9} & - & - \\
\href{https://huggingface.co/EleutherAI/llemma_7b}{LLemma} & 7B & PPile & 55B & 200B & 38.8 & 17.2 & 56.1 & 69.1 & 82.4 & 48.7 & 41.0 & 45.4 & 59.4 & 50.9 \\
\href{https://huggingface.co/EleutherAI/llemma_34b}{LLemma} & 34B & PPile & 55B & 50B & 54.2 & 23.0 & 67.9 & {75.7} & 90.1 & 57.0 & 49.8 & 54.7 & 68.8 & 60.1 \\
\href{https://huggingface.co/internlm/internlm2-math-base-7b}{Intern-Math} & 7B & - & 31B & 125B & 41.8 & 14.4 & 61.6 & 66.8 & 83.7 & 50.0 & 57.3  & 24.8 & 37.5 & 48.7 \\
\href{https://huggingface.co/internlm/internlm2-math-base-20b}{Intern-Math} & 20B & - & 31B & 125B & \hlb{65.4} & 30.0 & \hlb{75.7} & 79.3 & \hlb{94.0} & 50.9 & 38.5 & 53.1 & 71.9 & 62.1\\
\href{https://huggingface.co/deepseek-ai/deepseek-math-7b-base}{DeepSeekMath} & 7B & - & 120B & 500B & {64.1} & \hlb{34.2} & {74.0} & \hlb{83.9} & {92.4} & \hlb{63.4} & \hlb{62.4} & 56.4 & \hlb{84.4} & \hlb{68.4} \\
\midrule
\multicolumn{15}{c}{\texttt{Continual Pretraining on Mistral-7B}} \\
\midrule
Mistral-CT & 7B & OWM & 14B & 15B & 42.9 & 22.2 & 68.6 & 71.0 & 86.1 & 45.1 & 47.7 & 52.6 & 65.6 & 55.8\\
\model{}-Math & 7B & OWM & 14B & {10.5B} & \hl{66.9} & \hl{31.0} & \hl{77.8} & \hl{79.0} & \hl{93.9} & \hl{49.9} & \hl{58.7} & \hl{54.6} & \hl{84.4} & \hl{66.2}\\
$\Delta$  & & & & \grey{-30\%} & \blue{+24.0} & \blue{+8.8} & \blue{+9.2} & \blue{+8.0} & \blue{+7.8} & \blue{+4.8} & \blue{+11.0} & \blue{+2.0} & \blue{+18.8} &  \blue{\textbf{+10.4}}\\  
\bottomrule
\end{tabular}
}
\end{table*}
