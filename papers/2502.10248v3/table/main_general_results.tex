\begin{table*}[t]
\center
\caption{The main results of the \model{} on benchmarks. \textbf{MMLU} and \textbf{BBH} are evaluated using 5-shot and 3-shot prompting, respectively.}
\label{tab:diversityresult}
\setlength\tabcolsep{3pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcc|ccccccccccc|c}
\toprule
{\textbf{Model}} & $|\boldsymbol{\theta}|$ & \textbf{Toks} & \textbf{MMLU} & \textbf{BBH} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}AGI\\ Eval\end{tabular}}} & \textbf{HS} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}ARC\\ Chlg\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}ARC\\ Easy\end{tabular}}} & \textbf{WG} & \textbf{Piqa} & \textbf{BQ} & \textbf{Obqa} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Tydi \\ QA\end{tabular}}} & \textbf{AVG} \\ \midrule
\multicolumn{15}{c}{\texttt{1-2B Base Models}}
\\ \midrule
\href{https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T}{TinyLLaMA} & 1.1B & - & 25.9 & 29.3 & 30.8 & 59.0 & 31.9 & 56.8  & 58.7 & 73.1 & 63.2 & 34.4 & 26.4 & 44.5 \\
% \href{https://huggingface.co/facebook/opt-1.3b}{Opt} & 1.3B & - & 25.0 & 22.6 & 31.7 & 53.7 & 29.4 & 51.0 & 59.4 & 72.4 & 57.7 & 33.2 & 7.5 & 40.3 \\
% \href{https://huggingface.co/bigscience/bloom-1b7}{Bloom} & 1.7B & - & 25.2 & 17.3 & 31.1 & 48.0 & 27.1 & 48.1 & 57.4 & 70.0 & 61.8 & 30.0 & 10.5 & 38.8 \\
% \href{https://huggingface.co/bigscience/bloom-1b1}{Bloom} & 1.1B & - & 24.1 & 18.5 & 31.3 & 43.0 & 25.5 & 45.4 & 55.3 & 67.1 & 59.1 & 29.4 & 10.2 & 37.2 \\
%\href{https://huggingface.co/microsoft/phi-1_5}{Phi-1.5} & 1.3B & - & 40.7 & 28.8 & 32.4 & 62.6 & 48.0 & 73.1 & 72.7 & 75.5 & 75.0 & 48.2 & 7.3 & \\
% \href{https://huggingface.co/EleutherAI/pythia-1.4b}{Pythia} & 1.4B & - & 24.4 & 25.1 & 30.9 & 52.0 & 28.6 & 53.9 & 57.1 & 71.1 & 63.5 & 33.2 & 12.9 & 41.2 \\
% \href{https://huggingface.co/EleutherAI/pythia-1b}{Pythia} & 1.0B & - & 23.3 & 25.1 & 30.3 & 47.1 & 27.0 & 49.0 & 53.7 & 69.3 & 60.8 & 31.4 & 6.3 & 38.5 \\
%\href{https://huggingface.co/google/gemma-2b}{Gemma} & 2.0B & - & 27.0 & 34.1 & 31.2 & 42.3 & 23.0 & 37.1 & 50.3 & 59.6 & 63.0 & 27.8 & \\
% \midrule
% \multicolumn{15}{c}{\texttt{Continue Pretraining on Tinyllama 1.1B}}
% \\ \midrule
TinyLLaMA-CT & 1.1B & 80B & 25.3 & 28.2 & 31.3 & 56.6 & 30.0 & 55.7 & 59.1 & 72.2 & 63.2 & 34.6 & 21.1 & 43.4 \\
\model{}-Base & 1.1B & 48B & \hl{36.6} & \hl{32.1} & \hl{32.4} & \hl{58.0} & \hl{35.0} & \hl{64.3} & \hl{59.3} & \hl{73.1} & \hl{74.5} & \hl{38.0} & \hl{30.0} & \hl{48.5} \\
% $\Delta$ & & \grey{-40\%} & \blue{+11.3} & \blue{+3.9} & \blue{+1.1} & \blue{+1.4} & \blue{+5.0} & \blue{+8.6} & \blue{+0.2} & \blue{+0.9} & \blue{+11.3} & \blue{+3.4} & \blue{+8.9} & \blue{\textbf{+5.1}} \\
% TrimLoss
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[t]
\center
\footnotesize
\setlength\tabcolsep{4pt}
\caption{The main results of the \model{}-1.1B on code and math.}
\label{tab:diversityresult}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcc|cccc|cc|c}
\toprule
{\textbf{Model}} & $|\boldsymbol{\theta}|$ & \textbf{Toks} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{HumanEval}\\ pass@1\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{HumanEval}\\ pass@10\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{MBPP}\\ pass@1\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{MBPP}\\ pass@10\end{tabular}} & \textbf{GSM8K} & \textbf{Math} & \textbf{AVG} \\ 
\midrule
\multicolumn{10}{c}{\texttt{1-2B Base Models}}
\\ \midrule
\href{https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T}{TinyLLaMA} & 1.1B & - & 8.7 & 10.3 & 14.4 & 21.6 & 2.9 & 3.0 & 10.2 \\
%\href{https://huggingface.co/facebook/opt-1.3b}{Opt} & 1.3B & - & 0.0 & 0.0 & - & - & 2.7 & 2.2 & \\
% \href{https://huggingface.co/bigscience/bloom-1b7}{Bloom} & 1.7B & - & 4.0 & 7.5 & 5.5 & 9.6 & 2.6 & 3.2 & 5.4 \\
% \href{https://huggingface.co/bigscience/bloom-1b1}{Bloom} & 1.1B & - & 2.5 & 5.9 & 2.8 & 4.8 & 2.7 & 2.6 & 3.6 \\
%\href{https://huggingface.co/microsoft/phi-1_5}{Phi-1.5} & 1.3B & - & 33.9 & 42.7 & - & - & 32.4 & 4.2 & \\
% \href{https://huggingface.co/EleutherAI/pythia-1.4b}{Pythia} & 1.4B & - & 4.1 & 6.1 & 3.7 & 8.0 & 1.7 & 2.6 & 4.4 \\
% \href{https://huggingface.co/EleutherAI/pythia-1b}{Pythia} & 1.0B & - & 5.9 & 6.7 & 3.0 & 5.6 & 2.3 & 3.0 & 4.4 \\
% \midrule
% \multicolumn{10}{c}{\texttt{Continue Pretraining on Tinyllama 1.1B}}
% \\ \midrule
TinyLLaMA-CT & 1.1B & 80B & 9.0 & 10.7 & 12.2 & 18.2 & 2.7 & 3.2 & 9.3 \\
\model{}-Base & 1.1B & 48B & \hl{15.9} & \hl{21.3} & \hl{18.7} & \hl{26.0} & \hl{30.9} & \hl{8.2} & \hl{20.2} \\
% $\Delta$ & & \grey{-40\%} & \blue{+6.9} & \blue{+10.6} & \blue{+6.5} & \blue{+7.8} & \blue{+28.2} & \blue{+5.0} & \blue{\textbf{+10.9}} \\
% TrimLoss
\bottomrule
\end{tabular}
}
\end{table*}