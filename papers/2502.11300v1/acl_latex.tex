% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{array}
\usepackage{tabularx}
\usepackage{float}
\usepackage{multicol, multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{enumitem}
\usepackage{bookmark}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{xurl}
\usepackage{colortbl}  % For coloring
\usepackage{tikz}      % For drawing the oval
\usepackage{enumitem}
\usepackage{soul}

\usetikzlibrary{shapes.multipart, positioning}
\raggedbottom

\input{./macros}

\definecolor{myblue}{RGB}{173, 216, 230} % Light blue background
\definecolor{myborder}{RGB}{0, 102, 204} % Dark blue border

\setlength{\aboverulesep}{1.5pt}  % Reduce space above \midrule
\setlength{\belowrulesep}{1.5pt}  % Reduce space below \midrule

\newcommand{\rulesep}{\unskip\ \vrule\ }
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newtcbox{\myovalbox}{colback=cyan,boxrule=0pt,arc=2pt,
  boxsep=0pt,left=1pt,right=1pt,top=0pt,bottom=0pt}

\newcommand{\psulogo}{\raisebox{3.4pt}{\includegraphics[scale=0.025]{./images/logos/penn-state-shield.jpeg}}}
\newcommand{\nittlogo}{\raisebox{3.4pt}{\includegraphics[scale=0.025]{./images/logos/nitt_logo.png}}}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{{\name}: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?}

\author{
  \textbf{Aashish Anantha Ramakrishnan\psulogo},
  \textbf{Aadarsh Anantha Ramakrishnan\nittlogo},
  \textbf{Dongwon Lee\psulogo}
\\
  The Pennsylvania State University\textsuperscript{\psulogo},
  National Institute of Technology, Tiruchirappalli\textsuperscript{\nittlogo}
\\
  \texttt{
    \{aza6352, dul13\}@psu.edu\psulogo, 106121001@nitt.edu\nittlogo
  }
}


\begin{document}
\maketitle


\begin{abstract}
Multimodal Large Language Models (MLLMs) are renowned for their superior instruction-following and reasoning capabilities across diverse problem domains. However, existing benchmarks primarily focus on assessing factual and logical correctness in downstream tasks, with limited emphasis on evaluating MLLMs' ability to interpret pragmatic cues and intermodal relationships. To address this gap, we assess the competency of MLLMs in performing {\em Multimodal Discourse Analysis} (MDA) using Coherence Relations. Our benchmark, {\name}, encompasses a broad spectrum of Coherence Relations across 3 different discourse domains at varying levels of granularity. Through our experiments on 10+ MLLMs employing different prompting strategies, we show that even top models like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple classifier-based baselines. This study emphasizes the need to move beyond similarity-based metrics and adopt a discourse-driven framework for evaluating MLLMs, providing a more nuanced assessment of their capabilities. The benchmark and code are available at: \url{https://github.com/aashish2000/CORDIAL}.


\end{abstract}

\input{sec/1_intro.tex}
\input{sec/2_related_work.tex}
\input{sec/3_methods.tex}
\input{sec/4_experiments.tex}
\input{sec/5_conclusions.tex}

\newpage
\section*{Limitations}
While our proposed benchmark provides a comprehensive assessment of intermodal reasoning in current MLLMs, several limitations must be acknowledged. Firstly, the benchmark is currently limited to analyzing coherence relations in single-turn discourses. This is due to a lack of publically available datasets that provide multi-turn image-text pairs with annotated coherence relations. We plan to extend our benchmark to include multi-turn discourse relations as future work. Secondly, although we analyze different discourse domains in our benchmark, we lack a unified set of coherence relations that can be applied across all domains. The difficulty in defining a universal set of coherence relations is due to the varying nature of discourse in different domains. This limits our ability to analyze the inter-domain performance of MLLMs on the same set of relations. Finally, our benchmark is currently limited to the English language and must be extended to multi-lingual discourses as well.

\section*{Acknowledgments}
This research was in part supported by the U.S. National Science Foundation (NSF) award \#1820609. Part of the research results were obtained using the computational resources provided by CloudBank (\url{https://www.cloudbank.org/}), which was supported by the NSF award \#1925001.

\bibliography{acl_latex}

\clearpage
\appendix
\input{sec/x_appendix.tex}

\end{document}
