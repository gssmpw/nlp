\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{./images/ft_vis_with_gains.png}
    \caption{\% Loss/Gain after fine-tuning Llama 3.2-V. Fine-tuning shows significant performance gains, either on zero-shot or few-shot prompts across all 4 settings}
    \label{fig:finetuning}
\end{figure}

\begin{table}[t]
    \centering
    \scriptsize
        \begin{tabularx}{\columnwidth}{@{} l | Y|Y @{} Y| l @{}}
        \toprule

        \textbf{Model} & \textbf{Prompt} & \textbf{Sim} & \textbf{Compl} & \textbf{Macro F1} \\
        \midrule

        Random Guess & Baseline & 0.490 & 0.478 & 0.484 \\
        \midrule
        \midrule
        \multirow{2}{*}{LLaVA 1.6 7B} & Zero & 0.253 & 0.541 & 0.397 \\
        & CoT & 0.544 & 0.489 & 0.516  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↑30.0\%}; } \\
        \midrule

        \multirow{2}{*}{LLaVA 1.6 13B} & Zero & 0.666 & 0.000 & 0.333 \\
        & CoT & 0.408 & 0.675 & 0.542  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↑62.8\%}; } \\
        \midrule

        \multirow{3}{*}{LLaVA 1.6 34B} & Zero & 0.000 & 0.666 & 0.333 \\
        & Few & 0.139 & 0.679 & 0.409  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↑22.8\%}; } \\
        & CoT & 0.353 & 0.571 & 0.462  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↑38.7\%}; } \\
        \midrule

        \multirow{3}{*}{LLaVA OneVision 7B} & Zero & 0.626 & 0.391 & 0.509 \\
        & Few & 0.549 & 0.541 & 0.545  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↑7.1\%}; } \\
        & CoT & 0.549 & 0.601 & 0.575  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↑13.0\%}; } \\
        \midrule

        \multirow{3}{*}{Qwen2-VL 7B} & Zero & 0.654 & 0.268 & 0.461 \\
        & Few & 0.664 & 0.148 & 0.406  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↓11.9\%}; } \\
        & CoT & 0.446 & 0.602 & 0.524  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↑13.7\%}; } \\
        \midrule

        \multirow{3}{*}{Llama 3.2 Vision 11B} & Zero & 0.388 & 0.635 & 0.512 \\
        & Few & 0.509 & 0.479 & 0.494  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↓3.5\%}; } \\
        & CoT & 0.292 & 0.615 & 0.453  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↓11.5\%}; } \\
        \midrule

        \multirow{3}{*}{Phi3.5 Vision 4.2B} & Zero & 0.655 & 0.177 & 0.416 \\
        & Few & 0.409 & 0.662 & 0.536  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↑28.8\%}; } \\
        & CoT & 0.549 & 0.601 & 0.575  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↑38.2\%}; } \\
        \midrule

        \multirow{3}{*}{InternVL 2.5 26B} & Zero & 0.618 & 0.698 & 0.658 \\
        & Few & 0.633 & 0.633 & 0.633  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↓3.8\%}; } \\
        & CoT & 0.393 & 0.670 & 0.531  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↓19.3\%}; } \\
        \midrule 
        \midrule

        \multirow{3}{*}{GPT-4o} & Zero & 0.025 & 0.667 & 0.346 \\
        & Few & 0.443 & 0.667 & 0.555  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↑60.4\%}; } \\
        & CoT & 0.361 & 0.676 & 0.519  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↑50.0\%}; } \\
        \midrule

        \multirow{3}{*}{Gemini 1.5 Flash} & Zero & 0.714 & 0.715 & \underline{0.715} \\
        & Few & 0.363 & 0.688 & 0.525  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↓26.6\%}; } \\
        & CoT & 0.593 & 0.699 & 0.646  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↓9.7\%}; } \\
        \midrule

        \multirow{3}{*}{Gemini 1.5 Pro} & Zero & 0.719 & 0.679 & 0.699 \\
        & Few & 0.611 & \textbf{0.727} & 0.669  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↓4.3\%}; } \\
        & CoT & 0.630 & \underline{0.717} & 0.673  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↓3.7\%}; } \\
        \midrule

        \multirow{3}{*}{Claude 3.5 Sonnet v2} & Zero & \underline{0.722} & 0.615 & 0.669 \\
        & Few & 0.710 & 0.559 & 0.634  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↓5.2\%}; } \\
        & CoT & 0.603 & 0.703 & 0.653  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↓2.4\%}; } \\
        \midrule 
        \midrule

        CLIP Classifier & Baseline & \textbf{0.750} & 0.715 & \textbf{0.733} \\

        \bottomrule

        \end{tabularx}
       \caption{Results for Coherence Relation Prediction on DisRel. The coherence relations predicted are Similar (Sim) and Complementary (Compl).}
        \label{table:metrics_disrel}
\end{table}

\begin{table}[!ht]
    \scriptsize
    \centering
        \begin{tabularx}{\linewidth}{@{} c|c|Y|Y|Y @{}}
        \toprule
        \textbf{Dataset} & \textbf{CR} & \textbf{Claude} & \textbf{Gemini} & \textbf{GPT4o} \\
        \midrule

        \multirow{3}{0.1\linewidth}{\centering DisREL} & Similar & 70.4\% & 57.2\% & 14.8\% \\
        & Complementary & 91.2\% & 10.8\% & 96.8\% \\
        & Overall & \textbf{80.8\%} & 34.0\% & 55.8\% \\
        \midrule

        \multirow{6}{0.1\linewidth}{\centering Tweet Subtitles} & Insertion & 20.59\% & 0.0\% & 11.76\% \\
        & Concretization & 74.1\% & 57.35\% & 37.61\% \\
        & Projection & 81.82\% & 0.0\% & 15.91\% \\
        & Restatement & 65.73\% & 64.34\% & 21.68\% \\
        & Extension & 66.29\% & 0.0\% & 38.29\% \\
        & Overall & \textbf{70.44\%} & 47.69\% & 34.56\% \\
        \midrule

        \multirow{6}{0.1\linewidth}{\centering CLUE SL} & Visible & 83.37\% & 90.21\% & 75.4\% \\
        & Subjective & 58.0\% & 20.0\% & 52.0\% \\
        & Action & 72.73\% & 9.09\% & 54.55\% \\
        & Story & 29.12\% & 3.85\% & 35.71\% \\
        & Meta & 9.98\% & 0.0\% & 0.8\% \\
        & Overall & \textbf{42.77\%} & 35.0\% & 36.52\% \\
        \midrule

        \multirow{2}{0.1\linewidth}{\centering CLUE ML} & \multirow{2}{*}{Overall} & \multirow{2}{*}{\textbf{ 48.82\%}} & \multirow{2}{*}{32.71\%} & \multirow{2}{*}{44.21\%} \\
        & & & & \\

        \bottomrule
        \end{tabularx}
       \caption{Accuracy of MLLMs in verifying each Coherence Relation (CR) of every dataset.}
        \label{table:metrics_verification}
\end{table}


\begin{table}[!ht]
    \centering
    \scalebox{0.50}{
        \begin{tabularx}{1.97\linewidth}{@{} l|Y| Y @{} Y @{} Y @{} Y @{} Y| l @{}}
        \toprule
        \textbf{Model} & \textbf{Prompt} & \textbf{Ins} & \textbf{Concr} & \textbf{Proj} & \textbf{Restmt} & \textbf{Ext} & \textbf{Macro F1} \\
        \midrule

        Random Guess & Baseline & 0.094 & 0.340 & 0.068 & 0.123 & 0.165 & 0.158 \\
        \midrule
        \midrule
        \multirow{2}{*}{LLaVA 1.6 7B} & Zero & 0.000 & 0.693 & 0.062 & 0.066 & 0.082 & 0.181 \\
        & CoT & 0.019 & 0.822 & 0.081 & 0.050 & 0.114 & 0.217  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑19.9\%}; } \\
        \midrule

        \multirow{2}{*}{LLaVA 1.6 13B} & Zero & 0.085 & 0.044 & 0.000 & 0.000 & 0.095 & 0.045 \\
        & CoT & 0.070 & 0.477 & 0.000 & 0.122 & 0.054 & 0.145  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑222.2\%}; } \\
        \midrule

        \multirow{3}{*}{LLaVA 1.6 34B} & Zero & 0.000 & 0.176 & 0.094 & 0.104 & 0.253 & 0.125 \\
        & Few & 0.026 & 0.630 & 0.198 & 0.060 & 0.211 & 0.225  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑80.0\%}; } \\
        & CoT & 0.024 & 0.063 & 0.108 & 0.154 & 0.169 & 0.104  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓16.8\%}; } \\
        \midrule

        \multirow{3}{*}{LLaVA OneVision 7B} & Zero & 0.023 & 0.000 & 0.066 & 0.125 & 0.032 & 0.049 \\
        & Few & 0.067 & 0.000 & 0.087 & 0.071 & 0.177 & 0.081  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑65.3\%}; } \\
        & CoT & 0.062 & 0.005 & 0.057 & 0.124 & 0.101 & 0.070  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑42.9\%}; } \\
        \midrule

        \multirow{3}{*}{Qwen2-VL 7B} & Zero & 0.000 & 0.728 & 0.121 & 0.142 & 0.011 & 0.201 \\
        & Few & 0.094 & 0.148 & 0.078 & 0.144 & 0.068 & 0.106  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓47.3\%}; } \\
        & CoT & 0.156 & 0.167 & 0.068 & 0.170 & 0.000 & 0.112  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓44.3\%}; } \\
        \midrule

        \multirow{3}{*}{Llama 3.2 Vision 11B} & Zero & 0.000 & 0.779 & 0.000 & 0.093 & 0.000 & 0.175 \\
        & Few & 0.035 & 0.388 & 0.000 & 0.092 & 0.113 & 0.126  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓28.0\%}; } \\
        & CoT & 0.097 & 0.421 & 0.055 & 0.167 & 0.086 & 0.165  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓5.7\%}; } \\
        \midrule

        \multirow{3}{*}{Phi3.5 Vision 4.2B} & Zero & 0.043 & \underline{0.790} & 0.109 & 0.171 & 0.030 & 0.229 \\
        & Few & 0.183 & 0.179 & 0.000 & 0.159 & 0.093 & 0.123  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓46.3\%}; } \\
        & CoT & 0.025 & 0.745 & 0.164 & 0.156 & 0.022 & 0.223  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓2.6\%}; } \\
        \midrule

        \multirow{3}{*}{InternVL 2.5 26B} & Zero & 0.101 & 0.389 & 0.090 & 0.090 & 0.011 & 0.136 \\
        & Few & 0.090 & 0.002 & 0.041 & 0.292 & 0.000 & 0.085  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓37.5\%}; } \\
        & CoT & 0.118 & 0.450 & 0.102 & 0.199 & 0.083 & 0.190  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑39.7\%}; } \\
        \midrule 
        \midrule

        \multirow{3}{*}{GPT-4o} & Zero & 0.126 & 0.564 & 0.111 & 0.200 & 0.167 & 0.234 \\
        & Few & 0.171 & 0.599 & 0.131 & 0.268 & 0.199 & 0.274  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑17.1\%}; } \\
        & CoT & 0.076 & 0.346 & 0.146 & 0.217 & 0.187 & 0.194  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓17.1\%}; } \\
        \midrule

        \multirow{3}{*}{Gemini 1.5 Flash} & Zero & 0.172 & 0.783 & 0.138 & 0.183 & 0.011 & 0.257 \\
        & Few & 0.027 & 0.681 & 0.139 & 0.257 & 0.193 & 0.259  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑0.8\%}; } \\
        & CoT & 0.068 & 0.734 & 0.133 & 0.259 & 0.071 & 0.253  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓1.6\%}; } \\
        \midrule

        \multirow{3}{*}{Gemini 1.5 Pro} & Zero & \underline{0.200} & 0.692 & 0.141 & 0.290 & 0.034 & 0.271 \\
        & Few & 0.113 & 0.661 & \underline{0.247} & 0.270 & 0.000 & 0.258  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓4.8\%}; } \\
        & CoT & 0.102 & 0.657 & 0.101 & 0.278 & 0.022 & 0.232  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓14.4\%}; } \\
        \midrule

        \multirow{3}{*}{Claude 3.5 Sonnet v2} & Zero & 0.132 & 0.764 & 0.183 & \underline{0.328} & 0.175 & 0.316 \\
        & Few & 0.144 & 0.567 & 0.122 & 0.285 & 0.246 & 0.273  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓13.6\%}; } \\
        & CoT & 0.180 & 0.725 & 0.138 & 0.316 & \underline{0.256} & \underline{0.323}  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑2.2\%}; } \\
        \midrule 
        \midrule

        CLIP Classifier & Baseline & \textbf{0.542} & \textbf{0.866} & \textbf{0.286} & \textbf{0.388} & \textbf{0.514} & \textbf{0.519} \\

        
        \bottomrule
        \end{tabularx}
        }
       \caption{Results for Coherence Relation Prediction on Tweet Subtitles. The Coherence Relations predicted are Insertion (Ins), Concretization (Concr), Projection (Proj), Restatement (Restmt) and Extension (Ext).}
        \label{table:metrics_tweets}
\end{table}

\begin{table}[!ht]
    \centering
    \scalebox{0.50}{
        \begin{tabularx}{1.97\linewidth}{@{} l|Y| Y @{} Y @{} Y @{} Y @{} Y| l @{}}
        \toprule
        \textbf{Model} & \textbf{Prompt} & \textbf{Visible} & \textbf{Subj} & \textbf{Action} & \textbf{Story} & \textbf{Meta} & \textbf{Macro F1} \\
        \midrule

        Random Guess & Baseline & 0.233 & 0.069 & 0.030 & 0.162 & 0.266 & 0.152 \\

        \midrule
        \midrule
        \multirow{2}{*}{LLaVA 1.6 7B} & Zero & 0.484 & 0.135 & 0.000 & 0.158 & 0.096 & 0.174 \\
        & CoT & 0.534 & 0.198 & 0.068 & 0.043 & 0.004 & 0.169  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓2.9\%}; } \\
        \midrule

        \multirow{2}{*}{LLaVA 1.6 13B} & Zero & 0.541 & 0.027 & 0.039 & 0.158 & 0.000 & 0.153 \\
        & CoT & 0.529 & 0.043 & 0.054 & 0.034 & 0.016 & 0.135  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓11.8\%}; } \\
        \midrule

        \multirow{3}{*}{LLaVA 1.6 34B} & Zero & 0.545 & 0.000 & 0.000 & 0.012 & 0.004 & 0.112 \\
        & Few & 0.457 & 0.097 & 0.058 & 0.318 & 0.086 & 0.203  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑81.3\%}; } \\
        & CoT & 0.537 & 0.143 & 0.062 & 0.210 & 0.004 & 0.191  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑70.5\%}; } \\
        \midrule

        \multirow{3}{*}{LLaVA OneVision 7B} & Zero & 0.541 & 0.000 & 0.087 & 0.043 & 0.000 & 0.134 \\
        & Few & 0.146 & 0.000 & 0.025 & 0.172 & 0.243 & 0.117  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓12.7\%}; } \\
        & CoT & 0.535 & 0.000 & 0.048 & 0.092 & 0.000 & 0.135  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑0.7\%}; } \\
        \midrule

        \multirow{3}{*}{Qwen2-VL 7B} & Zero & 0.533 & 0.068 & 0.000 & 0.034 & 0.000 & 0.127 \\
        & Few & 0.539 & 0.000 & 0.000 & 0.000 & 0.004 & 0.109  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓14.2\%}; } \\
        & CoT & 0.530 & 0.156 & 0.057 & 0.080 & 0.004 & 0.166  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑30.7\%}; } \\
        \midrule

        \multirow{3}{*}{Llama 3.2 Vision 11B} & Zero & 0.537 & 0.136 & \underline{0.098} & 0.023 & 0.000 & 0.159 \\
        & Few & 0.542 & 0.000 & 0.026 & 0.000 & 0.000 & 0.114  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓28.3\%}; } \\
        & CoT & 0.533 & 0.189 & 0.026 & 0.083 & 0.020 & 0.170  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑6.9\%}; } \\
        \midrule

        \multirow{3}{*}{Phi3.5 Vision 4.2B} & Zero & 0.542 & 0.038 & 0.053 & 0.104 & 0.000 & 0.147 \\
        & Few & 0.485 & 0.256 & 0.021 & 0.255 & 0.162 & 0.236  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑60.5\%}; } \\
        & CoT & 0.534 & 0.000 & 0.087 & 0.083 & 0.000 & 0.141  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓4.1\%}; } \\
        \midrule

        \multirow{3}{*}{InternVL 2.5 26B} & Zero & 0.558 & 0.273 & 0.071 & 0.312 & 0.027 & 0.248 \\
        & Few & 0.498 & 0.211 & 0.048 & 0.253 & 0.127 & 0.228  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓8.1\%}; } \\
        & CoT & 0.537 & 0.333 & 0.052 & 0.254 & 0.087 & 0.252  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑1.6\%}; } \\
        \midrule 
        \midrule

        \multirow{3}{*}{GPT-4o} & Zero & 0.544 & 0.345 & 0.064 & 0.178 & 0.065 & 0.239 \\
        & Few & 0.549 & 0.352 & 0.023 & 0.390 & 0.134 & 0.289  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑20.9\%}; } \\
        & CoT & 0.558 & 0.321 & 0.054 & 0.324 & 0.024 & 0.256  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑7.1\%}; } \\
        \midrule

        \multirow{3}{*}{Gemini 1.5 Flash} & Zero & 0.543 & 0.215 & 0.091 & 0.168 & 0.020 & 0.207 \\
        & Few & 0.543 & 0.380 & 0.054 & 0.402 & 0.071 & 0.290  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑40.1\%}; } \\
        & CoT & 0.557 & 0.300 & 0.000 & 0.329 & 0.072 & 0.252  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑21.7\%}; } \\
        \midrule

        \multirow{3}{*}{Gemini 1.5 Pro} & Zero & \textbf{0.559} & 0.329 & 0.039 & 0.440 & 0.112 & 0.296 \\
        & Few & 0.531 & 0.391 & 0.070 & \underline{0.451} & 0.253 & 0.339  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑14.5\%}; } \\
        & CoT & \underline{0.558} & 0.330 & 0.000 & 0.350 & 0.057 & 0.259  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓12.5\%}; } \\
        \midrule

        \multirow{3}{*}{Claude 3.5 Sonnet v2} & Zero & 0.516 & \underline{0.408} & 0.070 & 0.439 & 0.113 & 0.309 \\
        & Few & 0.467 & \textbf{0.430} & 0.077 & 0.434 & \underline{0.338} & \underline{0.349}  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑12.9\%}; } \\
        & CoT & 0.537 & 0.378 & 0.058 & 0.382 & 0.119 & 0.295  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓4.5\%}; } \\
        \midrule 
        \midrule

        CLIP Classifier & Baseline & 0.548 & 0.270 & \textbf{0.150} & \textbf{0.479} & \textbf{0.687} & \textbf{0.427} \\

        \bottomrule
        \end{tabularx}
        }
       \caption{Results for Coherence Relation Prediction on CLUE Single-Label. The Coherence Relations predicted are Visible, Subjective (Subj), Action, Story and Meta}
        \label{table:metrics_clue_sl}
\end{table}

\section{Experiments} 
To answer our research questions, we conduct experiments on the {\name} benchmark with top open-source and proprietary MLLMs. For (RQ1), we evaluate the performance of 12 MLLMs from 9 different model families across our benchmark along with a classifier baseline. The 4 settings in our benchmark are structured with increasing difficulty, with DisRel and Tweet Subtitles being the simpler settings while CLUE Single-Label (SL) and CLUE Multi-Label (ML) are more complex. To answer (RQ2), we pick a selection of MLLMs and investigate their ability to verify coherence relations as correct or incorrect when provided along with image-text pairs. This provides a measure of the model's grasp of concepts such as discourse coherence and intermodal reasoning. For understanding (RQ3), we evaluate the effectiveness of different prompting strategies in enabling these MLLMs to discern coherence relations. We also fine-tune an MLLM on our benchmark to see if it can enhance its intermodal reasoning capability.


\subsection{Models Evaluated} \label{models-evaluated}
We evaluate \textbf{4 proprietary MLLMs}: GPT-4o \cite{OpenAI2024-hr}, Gemini 1.5 Flash \cite{Pichai2024-xj}, Gemini 1.5 Pro \cite{Pichai2024-xj}, and Claude 3.5 Sonnet v2 \cite{AnthropicUnknown-hu} and \textbf{8 open-source MLLMs:} LLaVA 1.6 (7B, 13B, 34B) \cite{Liu2024-il}, LLaVA OneVision 7B \cite{Li2025-ad}, Qwen2-VL-7B \cite{Wang2024-fa}, Llama 3.2 11B Instruct \cite{Meta-AIUnknown-ot}, Phi3.5 Vision Instruct \cite{Abdin2024-vk}, and InternVL 2.5 26B \cite{Chen2024-yg}. We selected these model families as they demonstrated acceptable prompt adherence as described in Appendix Sections \ref{appendix-availability}, \ref{appendix-evaluation}. We also include a pre-trained classifier fine-tuned for the task of coherence relation prediction. We selected GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet v2 as they were among the better-performing MLLMs on our benchmark for verification, with more details provided in Appendix Section \ref{appendix-verification}.

\subsection{Evaluation Metrics} \label{eval-metrics}
On the task of coherence relation prediction, we report the per-class F1 score and overall F1 score across all 4 settings. We select Macro F1 for overall performance as it treats all classes equally, which is important for our benchmark as it contains imbalanced classes. We report response accuracy for measuring performance on the verification task.

\subsection{Prompting Strategies and Fine-tuning} \label{prompt-strategies}
In addition to zero-shot evaluation, we also investigate the contribution of few-shot and Chain-of-Thought (CoT) prompting strategies in enabling MLLMs to learn coherence relations better. For few-shot, we include one example per coherence relation in each prompt as examples in the 3 single-label classification settings. For multi-label classification on CLUE ML, we include 6 different examples covering different combinations of relations in our prompt. To perform CoT, we include a reasoning step in our prompt that asks the model to generate a rationale before predicting the coherence relation. More details about the prompt templates used for each of the tasks are present in Sections \ref{appendix-eval-prompts} and \ref{appendix-verify-prompts} of our appendix. We fine-tune the Llama 3.2 11B Instruct model on our benchmark to measure the impact of task-specifc fine-tuning in open-source MLLMs with hyperparameter selection described in Appendix Section \ref{appendix-finetuning}.

\subsection{Main Results}

\paragraph{MLLMs Struggle with Coherence Relations}
From our results in Tables \ref{table:metrics_disrel}, \ref{table:metrics_tweets}, \ref{table:metrics_clue_sl}, \ref{table:metrics_clue_ml} we observe that no MLLM shows improvements over our baseline classifier on Macro F1 scores across all settings. When strictly looking at zero-shot prompts, Claude 3.5 Sonnet v2 performs the best on Tweet Subtitles, CLUE ML, and CLUE SL while Gemini 1.5 Flash performs the best on DisRel. However, the CLIP Classifier can outperform these MLLMs by 2.4\% on DisRel, 64.1\% on Tweet Subtitles, 38.6\% on CLUE SL, and 5.6\% on CLUE ML in terms of Macro F1 score. This shows that although these datasets have clearly discernible visual and text features that help in predicting coherence relations, MLLMs aren't able to comprehend them effectively. The trend extends to both proprietary and open-source MLLMs regardless of their size. Our results reiterate the need for benchmarks such as {\name} to evaluate the intermodal reasoning capabilities of MLLMs.

\paragraph{Pragmatic Relations are Challenging}
In single-label prediction settings, we observe that MLLMs come close to the baseline classifier's scores on DisRel, containing the image-text relations that are more literal (Similar, Complementary). On the other hand, there exists a significant gap in performance in other single-label datasets. Looking into per-relation F1 scores, pragmatic relation categories such as Insertion, Projection, and Extension are particularly challenging for MLLMs. A similar trend is observed in CLUE SL and CLUE ML where MLLMs struggle with relation categories such as Story and Meta.


\paragraph{Verification Accuracy Depends on Settings}
Analyzing the verification performance of MLLMs in Table \ref{table:metrics_verification}, we observe that the performance of MLLMs on the verification task is highly dependent on the setting. Across all settings, Claude 3.5 Sonnet v2 performs the best, with an accuracy of 80.8\% on DisRel, 70.4\% on Tweet Subtitles, 42.8\% on CLUE SL and 48.5\% on CLUE ML. This shows that MLLMs are able to verify coherence relations better in settings where the relations are more literal and easier to understand. However, the performance of MLLMs on the verification task is significantly lower in settings where the relations are more non-literal and pragmatic. 

\paragraph{Inconsistency of Prompting Strategies}
In our experiments with few-shot and CoT prompting strategies, we observe that the performance of MLLMs is inconsistent across different settings and model families. Across DisRel, Tweet Subtitles, CLUE SL and CLUE ML, a total of 7, 8, 10 and 10 MLLMs respectively show improvements in performance with either few-shot or CoT prompting strategies. However, only 2 MLLMs: LLaVA OneVision 7B and GPT-4o show improvements across all settings. Overall, we observe that in the more difficult settings (CLUE SL and CLUE ML), more number of models are able to leverage one of these alternate prompting strategies to improve their performance. But, even with additional examples or reasoning steps, MLLMs are not able to outperform the baseline classifier. This shows that Coherence Relation Prediction is a fundamentally difficult task that cannot be taught to MLLMs only through prompting strategies.

\paragraph{Fine-tuning Improves MLLM Reasoning}
Looking at Figure \ref{fig:finetuning}, we observe that fine-tuning the Llama 3.2 Vision model on our benchmark proves beneficial for coherence relation prediction. In both DisRel and Tweet Subtitles, we see gains in both zero-shot and few-shot prompt scores with Llama 3.2 Vision up to 18.42\% compared to its original performance. On both CLUE ML and SL, we see improvements in either zero-shot or few-shot performance with minimal performance loss on the other. This shows that MLLMs are able to learn to recognize coherence relations better when fine-tuned on a task-specific dataset. Coherence-aware fine-tuning can be a promising direction for improving their reasoning and cognition abilities.

\paragraph{Model Biases Inhibit Prediction Performance}
Looking at the per-class F1 scores across MLLMs, we observe they are biased towards certain relation categories. This includes the prediction of only a small subset of relations across all samples in an evaluation setting. From Figure \ref{fig:data_dist}, we acknowledge that the distribution of relation categories in our benchmark is imbalanced. However, this response imbalance of MLLMs is observed even on majority classes such as Concretization in Tweet Subtitles and Meta relations in CLUE SL and ML. This shows that despite providing few-shot examples and prompt optimization strategies, MLLMs display biases towards certain relation categories. When we look at the results of our fine-tuned model, we can see that prediction results on relations ignored by the base model are improved. This shows that fine-tuning can help mitigate these reasoning biases in MLLMs.