\section{The {\name} Benchmark}

\subsection{Motivation}
With Coherence Relations providing a finite representation of image-text linkages, we aim to measure MLLM performance through relation classification and verification tasks. Traditional alignment benchmarks often evaluate models using similarity scores. But multiple states of alignment between image-text pairs can exist, at the object-level, scene-level, or even at the discourse-level \cite{Xu2022-ie}. A pragmatic understanding of the context surrounding these pairs informs our ability to describe this alignment accurately. Thus, similarity scores alone may not be sufficient to capture the true performance of MLLMs. Additionally, with Coherence Relations being context-driven, the type of relations present in a discourse can vary across different domains. This necessitates the evaluation of MLLMs on multiple real-world discourse domains to assess their generalization capabilities. With MLLMs-as-a-judge \cite{Chen2024-cr} becoming more popular in tasks where acquiring human judgment is expensive and time-consuming, the importance of this task is further highlighted. We carefully pick and curate \textit{real-world image-text pairs} with \textit{expert human annotations} with the pre-processing details described in Appendix Section \ref{appendix-data-prep}. The three different discourse domains we evaluate are: Disaster Management, Social Media, and Online Articles.

\subsection{Coherence Relations}
Each dataset we include in {\name} assesses a unique set of Coherence Relations. To understand how communication in a discourse can be quantified by Coherence Relations, we turn to the Theory of Coherence \cite{Hobbs1978-em}. We define communication as the transfer of information and ideas from a speaker to a listener. For successful communication, a discourse needs to satisfy 4 conditions: (1) The message contents should be present in the discourse (2) The message must be relevant to the overall context of the discourse (3) Any new/unpredictable attributes of the message must build on the listener's existing world knowledge (4) The speaker must provide cues to guide the listener to graph their intended meaning. The goal of defining Coherence Relations is to serve any of the above-mentioned communicative functions. This way, for tasks such as MDA, we can analyze the communicative patterns present in a multimodal discourse. We consider Coherence Relations to be a constrained set of connections that describe the structural and causal relationships between different parts of a discourse. Consider the examples from Table \ref{table:examples-cr}, certain relations such as Visible and Concretization deal with presenting the same message content across modalities. On the other hand, relations such as Insertion and Extension require the reader to understand the union of information along with the context surrounding each modality to get the full message.

\subsection{Data Sources} \label{data-sources}
To construct our benchmark, we leverage existing datasets that provide image-text pairs along with human-annotated Coherence Relations across different discourse domains. We select three datasets that offer a diverse set of Coherence Relations: DisRel (Disaster Management), Tweet Subtitles (Social Media), and CLUE (Online Articles).

\paragraph{DisRel} 
This dataset \cite{Sosea2021-hr} explores the relationship of image-text pairs from disaster-related tweets, with labels collected through crowd-sourcing on Amazon MTurk. The dataset contains 4600 multimodal tweets with a test set size of \textit{500 examples} with a 50\% split between the two classes:

\begin{itemize}[leftmargin=3.3mm]
    \item \textbf{Similar}: The image and text share the same focus and attempt to convey the same message. There exists a significant overlap in the information conveyed between modalities. 
    \item \textbf{Complementary}:  The image and text do not share the same focus, but one modality helps understand the other better. Both modalities provide independent information which when combined, provide a more complete picture of the message/event. There may be divergence in the information conveyed between modalities.

\end{itemize}

\paragraph{Tweet Subtitles}
To measure cross-modal coherence relations between image and text, this dataset \cite{Xu2022-ie} contains 16000 image-text pairs sourced from Twitter on open-domain topics. The test set for this dataset consists of \textit{1600 examples}, which is 10\% of the entire dataset. The dataset provides single-label annotations from expert annotators on 3 entity-level and 2 scene-level relations:

\begin{itemize}[leftmargin=3.3mm]
    \item \textbf{Insertion (Entity-level)}: Both the text and the image focus on the same visual entity but it is not explicitly mentioned in the text.
    \item \textbf{Concretization (Entity-level)}: Both the text and image contain a mention of the main visual entity but may differ in types of details shared.
    \item \textbf{Projection (Entity-level)}: The main entity mentioned in the text is implicitly related to the visual objects present in the image. The image contains a reference to objects related to the main entity rather than the entity itself.
    \item \textbf{Restatement (Scene-level)}: The text directly describes the image contents. Both modalities convey the same message.
    \item \textbf{Extension (Scene-level)}: The image expands upon the story or idea in the text, presenting new elements or elaborations, effectively filling in narrative gaps left by the text.
\end{itemize}

\paragraph{CLUE} \label{clue-labels}
This dataset presents a novel conceptualization of image-text relations by extending text-only coherence relations to the multimodal setting \cite{Alikhani2020-nr}. The publicly available version of the dataset contains 4770 image-text pairs sourced from the Conceptual Captions Dataset \cite{Sharma2018-tr}. The samples were provided multi-label annotations by expert annotators for 5 different relationship types:

\begin{itemize}[leftmargin=3.3mm]
    \item \textbf{Visible}: The text presents information that is intended to recognizably characterize what is depicted in the image.
    \item \textbf{Action}: The text describes an extended, dynamic process in which the moment captured in the image is a representative snapshot.
    \item \textbf{Meta}: The text allows the reader to draw inferences not just about the scene depicted in the image but about the production and presentation of the image itself.
    \item \textbf{Subjective}: The text provides information about the speaker's reaction to, or evaluation of, what is depicted in the image.
    \item \textbf{Story}: The text provides a freestanding description of the circumstances depicted in the image, analogous to including instructional, explanatory, and other background relations.
\end{itemize}

We evaluate this dataset in two different settings: Multi-Label (ML) and Single-Label (SL). In the ML setting, we treat the dataset as a multi-label classification task where MLLMs predict all applicable labels. For CLUE SL, we follow the original dataset's label mapping strategy to select the most applicable label from the present annotations for each sample \cite{Alikhani2020-nr}. This provides two different settings for evaluating MLLM's understanding of coherence relations on the same image-text pairs with \textit{1183 examples} in the test set. 

\subsection{Baseline Classifier} \label{classifier}
Our goal of including a baseline classifier is to capture the existing signal in our datasets and to provide a reference point for MLLM performance. Understanding that human annotations can be noisy, we utilize this simple, generalizable classifier to identify relations where MLLMs are particularly under-performing on our benchmark. We employ CLIP Text and Image encoders to extract multimodal embeddings in a zero-shot manner \cite{Radford2021-ro}. We then train a Multi-Layer Perceptron (MLP) classifier using these embeddings on the train sets of each of these datasets to predict Coherence Relations. This ensures that our classifier is not biased towards any specific domain and can generalize across different discourse contexts. More details about the classifier are present in Appendix Section \ref{apendix-classifier}.