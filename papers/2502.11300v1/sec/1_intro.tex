\section{Introduction}

\begin{figure*}[!ht]
    \centering
    \frame{\includegraphics[width=0.85\linewidth]{"./images/CORDIAL-Main.jpg"}}
    \caption{{\name} presents a combination of literal and pragmatic relations for analyzing the intermodal reasoning capabilities of MLLMs. We evaluate MLLMs on the task of Multimodal Discourse Analysis through the prediction and verification of Coherence Relations across three different discourse domains.}
    \label{fig:main_arch}
\end{figure*}

The recent advancements in Multimodal Large Language Models (MLLMs) enable them to effectively capture diverse representations of problem domains \cite{Alayrac2022-vq, Chen2024-vu, Pichai2024-xj, Liu2024-ay}. These MLLMs are capable of adapting to various downstream tasks with limited data through Parameter-Efficient Fine-Tuning (PEFT) \cite{Hu2021-ft} and In-Context Learning (ICL) \cite{Brown2020-hw} approaches. Existing Vision-based MLLM benchmarks assess different aspects of model performance such as Perception, Cognition, and Reasoning \cite{Li2024-bt} through various downstream tasks. 

Current benchmark design strategies often focus on evaluating the ability of MLLMs to utilize the intersection of input sources to solve a common problem \cite{Kruk2019-ac}. Although this helps assess the model's ability to interpret its inputs factually and logically, it does {\em not fully capture the model's understanding of the relationships between these modalities}. Similarly, benchmarks that evaluate the alignment between images and text \cite{Thrush2022-yf}, utilize curated or synthetically generated image-text pairs. These methods focus solely on literal relations that measure the level of overlap between the image and text. On the other hand, pragmatic cues provide information on non-literal relations where the true intent/message of an example may not be directly referenced in both modalities as shown in Figure \ref{fig:main_arch}. These cues are leveraged routinely in real-world multimodal discourses, which are characterized by the use of multiple modes of communication to convey different components of a message. Multimodal Discourse Analysis (MDA) studies how the interaction between these different modes can create semiotic meaning \cite{Kress2009-iy}.

To operationalize the assessment of these intermodal relationships, we turn to theories of {\em Discourse Coherence} \cite{Hobbs1978-em}, which offer a way to quantify the organization and flow of ideas across information sources. From these theories, we focus on the concept of {\em Coherence Relations} \cite{Alikhani2019-dz}, which provides a finite structure to link different parts of a discourse. Recent studies have extended these traditionally text-only theories to multimodal discourses, showing that Coherence Relations can be effectively applied to image-text pairs \cite{Alikhani2020-nr}. With Coherence Relations being a fundamental aspect of human communication, we evaluate whether MLLMs can effectively predict and verify these relations.

In this work, we propose the {\name} (\underline{CO}herence \underline{R}elations in \underline{D}iscourse for \underline{I}mages \underline{A}nd \underline{L}anguage), the first benchmark for evaluating MLLMs on the task of MDA. {\name} consists of a diverse set of Coherence Relations across three different discourse domains: Disaster Management, Social Media, and Online Articles. Each domain also offers different levels of complexity in the evaluated Coherence Relations, from binary relations to more challenging settings such as multi-class and multi-label relations assigned by human annotators. We evaluate the performance of 10+ MLLMs on {\name}, focusing on three research questions: 

{\bf
\begin{enumerate}[leftmargin=1cm, label=RQ\arabic*:, itemsep=-0.5ex]
    \item Can MLLMs predict Coherence Relations effectively?
    \item Can MLLMs verify Coherence Relations accurately?
    \item Can we teach MLLMs to understand Coherence Relations better?
\end{enumerate}
}

Our analysis reveals that both Coherence Relation prediction (RQ1) and verification (RQ2) are challenging tasks for MLLMs when these relations focus on pragmatic cues. Although larger MLLMs perform better than their smaller, open-source counterparts, traditional classifier baselines consistently outperform them across discourse domains. To summarize, our key takeaways are as follows:

\begin{itemize}[leftmargin=3.3mm]
    \item We propose {\name}, the first benchmark for evaluating MLLMs for Multi-modal Discourse Analysis (MDA) using Coherence Relations.
    \item Our experiments show that MLLMs struggle to predict and verify Coherence Relations, especially when these relations are more pragmatic.
    \item We demonstrate the need for coherence-aware fine-tuning approaches to improve intermodal reasoning capabilities of MLLMs.
\end{itemize}

\begin{table*}[!ht]
    \scriptsize
    \centering

    \scalebox{0.84}{
        \begin{tabularx}{\linewidth}{@{} cccccc @{}}
        \toprule
        \textbf{Dataset} & \multicolumn{5}{c}{\textbf{Examples}} \\
        \midrule        

        \multirow{4}{*}{DisREL} & \multicolumn{2}{c}{\makebox[0.15\textwidth]{\includegraphics[width=0.15\textwidth]{./images/examples/disrel_Similar.jpg}}} & & 
        \multicolumn{2}{c}{\makebox[0.15\textwidth]{\includegraphics[width=0.15\textwidth]{./images/examples/disrel_Complementary.jpg}}} \\

        & \multicolumn{2}{c}{\multirow{2}{0.2\textwidth}{\centering Part of my {\color{red} pile of branches} after \#HurricaneIrma - still no power in \#Orlando}} & &
        \multicolumn{2}{c}{\multirow{2}{0.2\textwidth}{\centering Floridians rescue stranded {\color{red} manatees} as Irma sucks water from shores }} \\

        &  &  &  &  &  \\
        &  &  &  &  &  \vspace{0.1cm} \\

        & \multicolumn{2}{c}{\multirow{2}{0.15\textwidth}{\centering \textbf{Coherence Relation:} Similar}} & &
        \multicolumn{2}{c}{\multirow{2}{0.15\textwidth}{\centering \textbf{Coherence Relation:} Complementary}} \\
        &  &  &  &  &  \\
        
        \midrule
        
        \multirow{4}{*}{Tweet Subtitles} & \makebox[0.15\textwidth]{\includegraphics[width=0.15\textwidth]{./images/examples/tweet_Concretization.jpg}} & \makebox[0.15\textwidth]{\includegraphics[width=0.15\textwidth]{./images/examples/tweet_Insertion.jpg}} & \makebox[0.15\textwidth]{\includegraphics[width=0.15\textwidth]{./images/examples/tweet_Projection.jpg}} & \makebox[0.15\textwidth]{\includegraphics[width=0.15\textwidth]{./images/examples/tweet_Extension.jpg}} & \makebox[0.15\textwidth]{\includegraphics[width=0.15\textwidth]{./images/examples/tweet_Restatement.jpg}} \\
        
        & \multirow{3}{0.15\textwidth}{\centering Fresh never frozen {\color{red} jumbo wings} tossed in a housemade buffalo sauce. Yum!} & \multirow{3}{0.15\textwidth}{\centering Freshly picked off my allotment today, \\ well chuffed. \break {\color{orange} (strawberry)}} & \multirow{3}{0.15\textwidth}{\centering Cartel leader whose arrest sparked killings is sentenced to prison in Dallas {\color{red} court}} & \multirow{3}{0.15\textwidth}{\centering Amazon Prime delivers anything these days! {\color{orange} (delivering a cat)}} & \multirow{3}{0.15\textwidth}{\centering {\color{red} Eiffel Tower} shuts down as {\color{red} snow, freezing rain} pummel France} \\
        
        &  &  &  &  &  \\
        &  &  &  &  &  \\
        &  &  &  &  &  \vspace{0.1cm} \\

        & \multirow{2}{0.15\textwidth}{\centering \textbf{Coherence Relation:} Concretization} & \multirow{2}{0.15\textwidth}{\centering \textbf{Coherence Relation:} Insertion} & \multirow{2}{0.15\textwidth}{\centering \textbf{Coherence Relation:} Projection} & \multirow{2}{0.15\textwidth}{\centering \textbf{Coherence Relation:} Extension} & \multirow{2}{0.15\textwidth}{\centering \textbf{Coherence Relation:} Restatement} \\
        &  &  &  &  &  \\

        \midrule
        
        \multirow{4}{*}{CLUE} & \makebox[0.15\textwidth]{\includegraphics[width=0.15\textwidth]{./images/examples/clue_Visible.jpg}} & \makebox[0.15\textwidth]{\includegraphics[width=0.15\textwidth]{./images/examples/clue_Meta.jpg}} & 
        \makebox[0.15\textwidth]{\includegraphics[width=0.15\textwidth]{./images/examples/clue_Subjective.jpg}} & \makebox[0.15\textwidth]{\includegraphics[width=0.15\textwidth]{./images/examples/clue_Story.jpg}} & \makebox[0.15\textwidth]{\includegraphics[width=0.15\textwidth]{./images/examples/clue_Action.jpg}} \\
        
        & \multirow{3}{0.15\textwidth}{\centering A {\color{red} path} winds through an ancient {\color{red} bamboo forest}} & \multirow{3}{0.15\textwidth}{\centering A model {\color{red} walks} the runway for the collection during, {\color{red} fashion week}} & \multirow{3}{0.15\textwidth}{\centering A city in winter is such a {\color{red} beautiful} city} & \multirow{3}{0.15\textwidth}{\centering People know that {\color{red} curb appeal} is not a thing to take lightly when {\color{red} remodeling a home}} & \multirow{3}{0.15\textwidth}{\centering Seals {\color{red} fighting} for a spot to sleep on the rocks} \\
        
        &  &  &  &  &  \\
        &  &  &  &  &  \\
        &  &  &  &  & \vspace{0.1cm} \\

        & \multirow{2}{0.15\textwidth}{\centering \textbf{Coherence Relations:} {\color{blue} Visible}} & \multirow{2}{0.15\textwidth}{\centering \textbf{Coherence Relations:} Visible, {\color{blue} Meta}, Action} & \multirow{2}{0.15\textwidth}{\centering \textbf{Coherence Relations:} {\color{blue} Subjective}, Story} & \multirow{2}{0.15\textwidth}{\centering \textbf{Coherence Relations:} {\color{blue} Story}} & \multirow{2}{0.15\textwidth}{\centering \textbf{Coherence Relations:} {\color{blue} Action}} \\
        &  &  &  &  &  \\
        
        \bottomrule
        \end{tabularx}
    }
    \caption{Examples from each dataset for all Coherence Relations. The words in {\color{red} red} are important cues present in the caption, while the words in {\color{orange} orange} show pragmatic cues inferred from the image-text pair. The relations highlighted in {\color{blue} blue} are the selected relations for CLUE Single-Label.} 
    \label{table:examples-cr}
\end{table*}