\section*{Appendix}
\label{sec:appendix}

\begin{table}[!ht]
    \centering
    \scalebox{0.50}{
        \begin{tabularx}{1.97\linewidth}{@{} l|Y| Y @{} Y @{} Y @{} Y @{} Y| l @{}}
        \toprule
        \textbf{Model} & \textbf{Prompt} & \textbf{Visible} & \textbf{Subj} & \textbf{Action} & \textbf{Story} & \textbf{Meta} & \textbf{Macro F1} \\
        \midrule

        \multirow{2}{*}{LLaVA 1.6 7B} & Zero & 0.864 & 0.117 & 0.113 & 0.048 & 0.029 & 0.234 \\
        & CoT & 0.848 & 0.245 & 0.247 & 0.058 & 0.013 & 0.282  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑20.5\%}; } \\
        \midrule

        \multirow{2}{*}{LLaVA 1.6 13B} & Zero & 0.869 & 0.147 & 0.389 & 0.115 & 0.401 & 0.384 \\
        & CoT & 0.849 & 0.095 & 0.237 & 0.090 & 0.048 & 0.264  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓31.2\%}; } \\
        \midrule

        \multirow{3}{*}{LLaVA 1.6 34B} & Zero & 0.868 & 0.165 & 0.470 & 0.369 & 0.298 & 0.434 \\
        & Few & 0.859 & 0.000 & 0.471 & 0.453 & 0.166 & 0.390  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓10.1\%}; } \\
        & CoT & 0.858 & 0.117 & 0.317 & 0.175 & 0.163 & 0.326  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓24.9\%}; } \\
        \midrule

        \multirow{3}{*}{LLaVA OneVision 7B} & Zero & 0.820 & 0.034 & 0.380 & 0.024 & 0.000 & 0.252 \\
        & Few & 0.757 & 0.109 & 0.510 & 0.150 & 0.000 & 0.305  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑21.0\%}; } \\
        & CoT & 0.856 & 0.150 & 0.349 & 0.213 & 0.154 & 0.345  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑36.9\%}; } \\
        \midrule

        \multirow{3}{*}{Qwen2-VL 7B} & Zero & 0.864 & 0.045 & 0.211 & 0.086 & 0.013 & 0.244 \\
        & Few & 0.864 & 0.162 & 0.461 & 0.368 & 0.017 & 0.374  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑53.3\%}; } \\
        & CoT & 0.865 & 0.082 & 0.094 & 0.080 & 0.021 & 0.228  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓6.6\%}; } \\
        \midrule

        \multirow{3}{*}{Llama 3.2 Vision 11B} & Zero & 0.869 & 0.157 & 0.424 & 0.349 & 0.284 & 0.417 \\
        & Few & 0.828 & 0.248 & 0.571 & 0.443 & \underline{0.499} & 0.518  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑24.2\%}; } \\
        & CoT & 0.850 & 0.183 & 0.391 & 0.420 & 0.371 & 0.443  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑6.2\%}; } \\
        \midrule

        \multirow{3}{*}{Phi3.5 Vision 4.2B} & Zero & 0.866 & 0.000 & 0.092 & 0.036 & 0.013 & 0.201 \\
        & Few & 0.527 & 0.226 & 0.311 & 0.490 & 0.036 & 0.318  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑58.2\%}; } \\
        & CoT & 0.819 & 0.047 & 0.475 & 0.294 & 0.064 & 0.340  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑69.2\%}; } \\
        \midrule

        \multirow{3}{*}{InternVL 2.5 26B} & Zero & 0.822 & 0.291 & 0.448 & 0.324 & 0.029 & 0.383 \\
        & Few & 0.496 & 0.266 & 0.491 & 0.400 & 0.128 & 0.356  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓7.0\%}; } \\
        & CoT & 0.757 & 0.397 & 0.444 & 0.331 & 0.059 & 0.397  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑3.7\%}; } \\
        \midrule 
        \midrule

        \multirow{3}{*}{GPT-4o} & Zero & 0.858 & 0.451 & 0.453 & 0.291 & 0.060 & 0.423 \\
        & Few & 0.874 & 0.495 & 0.561 & 0.525 & 0.123 & 0.515  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑21.7\%}; } \\
        & CoT & 0.865 & 0.506 & 0.357 & 0.354 & 0.084 & 0.433  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑2.4\%}; } \\
        \midrule

        \multirow{3}{*}{Gemini 1.5 Flash} & Zero & 0.875 & 0.368 & 0.554 & 0.355 & 0.065 & 0.443 \\
        & Few & 0.847 & 0.420 & 0.648 & 0.480 & 0.163 & 0.512  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑15.6\%}; } \\
        & CoT & 0.871 & 0.419 & 0.308 & 0.358 & 0.109 & 0.413  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓6.8\%}; } \\
        \midrule

        \multirow{3}{*}{Gemini 1.5 Pro} & Zero & 0.884 & 0.485 & 0.544 & 0.313 & 0.106 & 0.467 \\
        & Few & 0.866 & \underline{0.532} & \underline{0.668} & 0.464 & 0.206 & 0.547  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑17.1\%}; } \\
        & CoT & 0.880 & 0.403 & 0.180 & 0.278 & 0.090 & 0.366  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓21.6\%}; } \\
        \midrule

        \multirow{3}{*}{Claude 3.5 Sonnet v2} & Zero & \underline{0.891} & \textbf{0.535} & \textbf{0.681} & 0.479 & 0.220 & 0.561 \\
        & Few & 0.829 & 0.503 & 0.643 & \underline{0.553} & 0.360 & \underline{0.578}  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑3.0\%}; } \\
        & CoT & 0.876 & 0.515 & 0.596 & 0.389 & 0.174 & 0.510  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓9.1\%}; } \\
        \midrule 
        \midrule

        CLIP Classifier & Baseline & \textbf{0.905} & 0.176 & 0.627 & \textbf{0.615} & \textbf{0.642} & \textbf{0.593} \\

        

        \bottomrule
        \end{tabularx}
    }
       \caption{Results for Coherence Relation Prediction on the CLUE Multi-Label dataset. The Coherence Relations predicted are Visible, Subjective (Subj), Action, Story and Meta with multiple relations being applicable to a single image-text pair.}
        \label{table:metrics_clue_ml}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{"./images/CLUE_ML.png"}
    \caption{An overview of the Image-Text Label (i.e., Coherence Relations) distribution across CLUE ML}
    \label{fig:multilabel-distribution}
\end{figure}

\section{Data Preparation}
\label{appendix-data-prep}

This section sheds light on the methods used while preparing all the datasets mentioned in this paper for model evaluation. We verify all three datasets used to construct this benchmark have a permissive license that allows usage for research purposes without restrictions (DisRel - MIT License, Tweet Subtitles - MIT License, CLUE - Sourced from Conceptual Captions and free for research use).

\subsection{DisREL}
Due to limited number of samples in the \textbf{Unrelated} category, these image-text pairs were discarded from our train and test set. All placeholder instances of \texttt{<URL>} were removed from the text as a part of our data cleaning.

\subsection{Tweet Subtitles}
This dataset contains two types of captions for tweets: actual and text generated by an image captioning model. We use only the \textbf{actual} caption as part of our evaluation.

\subsection{CLUE}
The labels other than the ones mentioned in Section \ref{clue-labels} were disregarded from our train and test set for both settings, due to the lack of examples. We construct the CLUE Single-Label dataset with the same heuristic used by \citet{Alikhani2020-nr}:

\begin{enumerate}[leftmargin=1.25cm, label=Step \arabic*:]
    \item If the set contains a \textit{Meta} relation, assign it to the image-text pair. Else, proceed to the next step.
    \item If the set contains a \textit{Visible} relation and doesn't contain either a \textit{Meta} or \textit{Subjective} relation, assign it to the image-text pair. Else, proceed to the next step.
    \item If none of the above rules are met, randomly sample one relation from the 5 available, and assign it to the pair.
\end{enumerate}

\section{Model Availability} \label{appendix-availability}

This section focuses on the details of model availability and parameters, that we use in Section \ref{models-evaluated}. For all models, we set temperature to $0$ or \texttt{do\_sample=False}, maximum output tokens to $512$ and the random seed set to $42$, wherever possible to ensure reproducibility. The model responses in this paper were collected between January 12, 2025 and February 12, 2025.

\subsection{Proprietary Models}

\paragraph{OpenAI GPT:} We access the GPT-4o model via the official OpenAI API. We evaluate \texttt{gpt-4o-2024-08-06}.

\paragraph{Anthropic Claude:} We access Claude 3.5 Sonnet v2 via the Vertex AI API, using Google Cloud. We evaluate \texttt{claude-3-5-sonnet-v2@20241022}.

\paragraph{Google Gemini:} We access Gemini 1.5 Flash and Gemini 1.5 Pro via the Vertex AI API, using Google Cloud. We evaluate \texttt{gemini-1.5-flash-002} and \texttt{gemini-1.5-pro-002}. 

\subsection{Open Source Models}
We evaluate models published on Huggingface Hub. LLaVA 1.6 34B and Llama 3.2 11B Vision were evaluated using the LMDeploy \footnote{\url{https://github.com/InternLM/lmdeploy}} framework. We evaluate Qwen2-VL using code released by the authors. All other models, were evaluated using the VLLM \footnote{\url{https://github.com/vllm-project/vllm}} framework. Refer to Table \ref{table:mllm_ids} for the models we evaluate.

\begin{table}[!h]
    \centering
    \scriptsize
        \begin{tabularx}{\linewidth}{@{} c|*2Y|Y @{}}
        \toprule
        Model & Model ID \\
        \midrule

        InternVL 2.5 26B & \texttt{OpenGVLab/InternVL2\_5-26B} \\
        Llama 3.2 Vision 11B & \texttt{meta-llama/Llama-3.2-11B-Vision-Instruct} \\
        LLaVA 1.6 7B & \texttt{llava-hf/llava-v1.6-mistral-7b-hf} \\
        LLaVA 1.6 13B & \texttt{llava-hf/llava-v1.6-vicuna-13b-hf} \\
        LLaVA 1.6 34B & \texttt{liuhaotian/llava-v1.6-34b} \\
        LLaVA OneVision 7B & \texttt{llava-hf/llava-onevision-qwen2-7b-ov-hf} \\
        Phi 3.5 Vision & \texttt{microsoft/Phi-3.5-vision-instruct} \\
        Qwen2-VL-7B & \texttt{Qwen/Qwen2-VL-7B-Instruct} \\

        \midrule
        Claude 3.5 Sonnet v2 & \texttt{claude-3-5-sonnet-v2@20241022} \\
        GPT-4o & \texttt{gpt-4o-2024-08-06} \\
        Gemini 1.5 Flash & \texttt{gemini-1.5-flash-002} \\
        Gemini 1.5 Pro & \texttt{gemini-1.5-pro-002} \\
        
        \bottomrule
        \end{tabularx}
       \caption{MLLMs we evaluate in this paper. For open-source models, this table shows the model names in Huggingface.}
        \label{table:mllm_ids}
\end{table}

\section{MLLM Evaluation Details} \label{appendix-evaluation}
This section provides details about the \textit{evaluation} task (RQ1) mentioned in Section \ref{models-evaluated}. \\

\subsection{Prompt Templates} \label{appendix-eval-prompts}
As mentioned in Section \ref{prompt-strategies}, we make use of Zero-Shot, Few-Shot and Chain of Thought prompting for evaluation. Every prompting strategy utilizes three different messages:
\begin{itemize}
    \item \textbf{System Message:} We explain the task and the definitions of each Coherence Relation present in the dataset being evaluated.
    \item \textbf{User Message:} This message is used to reiterate the task again, along with the required output format. The image and text that need to be evaluated, is also added here.
    \item \textbf{Assistant Message:} We use this optional message for certain models, to guide its responses towards the intended output format.
\end{itemize}

The different prompts and system messages used on each data source as mentioned in Section \ref{data-sources}, is present in the appendix.

\subsection{Few Shot Prompting}

In this prompting strategy, we utilize user-assistant message pairs that are inserted right after the user message which specifies output format. For the Tweet Subtitles and CLUE Single-Label datasets, we utilize \textbf{5-shot examples} to include all possible coherence relations. In the case of CLUE Multi-Label and DisREL, we utilize \textbf{6-shot examples} and \textbf{2-shot examples} respectively. \\

We do not evaluate LLaVA 1.6 7B and 13B using this prompting technique, as our prompt (text + multimodal tokens) does not fit into the context length (4096) of these models. 

\subsection{Chain-of-Thought Prompting}
We instruct the model to analyze the image-text pair, before assigning a Coherence Relation in this prompting strategy. We incorporate the instruction "Let's think step by step", to make the model respond with concise sentences that detail its reasoning process.

\subsection{Preprocessing Images for Claude} \label{claude-preprocess}
We noticed that some images were above the 5 MB per file size limit imposed by Anthropic for their API. As per their recommendations, we evaluate Claude on images that are resized to 1.3 megapixels, while preserving the aspect ratio.

\subsection{Postprocessing MLLM Responses}
In the case of single-label datasets, we remove instances of the phrase "Coherence Relation:" along with other punctuation and whitespace. If there exists only one occurrence of a particular coherence relation, we use that as the prediction result for the image-text pair. \\

While working with CLUE Multi-Label responses, we remove instances of the phrase "Coherence Relations:". All valid JSON in the response is parsed using regular expressions. If the output format is comma-separated values, those responses are parsed appropriately. \\

After this, if we cannot find any valid label for an image-text pair from the MLLM's response, we discard the sample from our test set. To ensure test set consistency, we discarded around \textbf{200 samples} across all datasets and calculated the final evaluation metrics as mentioned in Section \ref{eval-metrics}.

\section{MLLM Verification Details} \label{appendix-verification}
This section provides details about the \textit{verification} task (RQ2) mentioned in Section \ref{models-evaluated}.

\subsection{Prompt Templates} \label{appendix-verify-prompts}
For this task, we utilize a Chain-of-Thought prompting strategy. Each model is given the same system message as before, but along with the image-text pair, we also give the ground truth Coherence Relation. The model is then asked to respond with a True/False answer, along with its rationale for its response. 

\subsection{Preprocessing Images for Claude}
We use the same strategy as mentioned in Section \ref{claude-preprocess}, only for the images that don't come under the file size limit.

\subsection{Postprocessing MLLM Responses}
We parse boolean values from each MLLM response, and assign \textbf{False} to an image-text pair, only if there is any occurrence of the same. For CLUE ML, we provide only overall verification accuracies since it is a multi-label verification problem.

\section{Fine-tuning Details} \label{appendix-finetuning}
We fine-tune LLaMA 3.2 Vision 11B Instruct (\texttt{unsloth/Llama-3.2-11B-Vision-Instruct} in Huggingface) using the Unsloth\footnote{\url{https://unsloth.ai/blog/vision}} framework. We opted for this framework due to its memory efficiency and rapid fine-tuning. We perform Parameter Efficient Fine-Tuning (PEFT) of all layers (Vision \& Language) and modules (Attention \& MLP) present. We use the hyperparameters mentioned in Section \ref{appendix-hyperparams} on each dataset for fine-tuning. Other parameters have been initialized to their default values.


\begin{table}[t]
    \centering
    \scriptsize
        \begin{tabularx}{\columnwidth}{@{} l | Y|Y @{} Y| l @{}}
        \toprule

        \textbf{Model} & \textbf{Prompt} & \textbf{Sim} & \textbf{Compl} & \textbf{Macro F1} \\
        \midrule

        \multirow{2}{*}{FT-Llama 3.2 Vision 11B} & Zero & 0.629 & 0.620 & \textbf{0.625} \\
        & Few & \textbf{0.673} & 0.327 & 0.500  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↓20.0\%}; } \\
        \midrule

        \multirow{2}{*}{Llama 3.2 Vision 11B} & Zero & 0.388 & \textbf{0.635} & 0.512 \\
        & Few & 0.509 & 0.479 & 0.494  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \tiny ↓3.5\%}; } \\
        
        \bottomrule

        \end{tabularx}
       \caption{Per-class Coherence Relation Prediction of Fine-tuned LLama 3.2 Vision 11B (FT-Llama) on the DisRel dataset. The coherence relations predicted are Similar and Complementary.}
        \label{table:metrics_disrel_finetuned}
\end{table}

\begin{table}[!ht]
    \centering
    \scalebox{0.50}{
        \begin{tabularx}{1.97\linewidth}{@{} l|Y| Y @{} Y @{} Y @{} Y @{} Y| l @{}}
        \toprule
        \textbf{Model} & \textbf{Prompt} & \textbf{Ins} & \textbf{Concr} & \textbf{Proj} & \textbf{Restmt} & \textbf{Ext} & \textbf{Macro F1} \\
        \midrule

        \multirow{2}{*}{FT-Llama 3.2 Vision 11B} & Zero & \textbf{0.440} & \textbf{0.853} & 0.045 & 0.042 & 0.148 & 0.306 \\
        & Few & 0.231 & 0.752 & \textbf{0.213} & \textbf{0.100} & \textbf{0.254} & \textbf{0.310}  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑1.3\%}; } \\
        \midrule

        \multirow{2}{*}{Llama 3.2 Vision 11B} & Zero & 0.000 & 0.779 & 0.000 & 0.093 & 0.000 & 0.175 \\
        & Few & 0.035 & 0.388 & 0.000 & 0.092 & 0.113 & 0.126  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓28.0\%}; } \\
        
        \bottomrule
        \end{tabularx}
        }
       \caption{Per-class Coherence Relation Prediction of Fine-tuned LLama 3.2 Vision 11B (FT-Llama) on the Tweet Subtitles dataset. The Coherence Relations predicted are Insertion (Ins), Concretization (Concr), Projection (Proj), Restatement (Restmt) and Extension (Ext).}
        \label{table:metrics_tweets_finetuned}
\end{table}


\begin{table}[!ht]
    \centering
    \scalebox{0.50}{
        \begin{tabularx}{1.97\linewidth}{@{} l|Y| Y @{} Y @{} Y @{} Y @{} Y| l @{}}
        \toprule
        \textbf{Model} & \textbf{Prompt} & \textbf{Visible} & \textbf{Subj} & \textbf{Action} & \textbf{Story} & \textbf{Meta} & \textbf{Macro F1} \\
        \midrule

        \multirow{2}{*}{FT-Llama 3.2 Vision 11B} & Zero & \textbf{0.547} & 0.074 & 0.042 & 0.045 & 0.004 & 0.142 \\
        & Few & 0.516 & \textbf{0.230} & 0.053 & \textbf{0.228} & \textbf{0.155} & \textbf{0.236}  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑66.2\%}; } \\
        \midrule

        \multirow{2}{*}{Llama 3.2 Vision 11B} & Zero & 0.537 & 0.136 & \textbf{0.098} & 0.023 & 0.000 & 0.159 \\
        & Few & 0.542 & 0.000 & 0.026 & 0.000 & 0.000 & 0.114  \tikz[baseline=(X.base)]{\node[fill=red!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↓28.3\%}; } \\

        \bottomrule
        \end{tabularx}
        }
       \caption{Per-class Coherence Relation Prediction of Fine-tuned LLama 3.2 Vision 11B (FT-Llama) on the CLUE Single-Label dataset. The Coherence Relations predicted are Visible, Subjective (Subj), Action, Story and Meta}
        \label{table:metrics_clue_sl_finetuned}
\end{table}


\begin{table}[!ht]
    \centering
    \scalebox{0.50}{
        \begin{tabularx}{1.97\linewidth}{@{} l|Y| Y @{} Y @{} Y @{} Y @{} Y| l @{}}
        \toprule
        \textbf{Model} & \textbf{Prompt} & \textbf{Visible} & \textbf{Subj} & \textbf{Action} & \textbf{Story} & \textbf{Meta} & \textbf{Macro F1} \\
        \midrule

        \multirow{2}{*}{FT-Llama 3.2 Vision 11B} & Zero & 0.864 & 0.228 & 0.520 & 0.287 & 0.431 & 0.466 \\
        & Few & 0.864 & 0.158 & \textbf{0.586} & 0.282 & \textbf{0.549} & 0.488  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑4.7\%}; } \\
        \midrule

        \multirow{2}{*}{Llama 3.2 Vision 11B} & Zero & \textbf{0.869} & 0.157 & 0.424 & 0.349 & 0.284 & 0.417 \\
        & Few & 0.828 & \textbf{0.248} & 0.571 & \textbf{0.443} & 0.499 & \textbf{0.518}  \tikz[baseline=(X.base)]{\node[fill=green!50, rounded corners=2pt, inner sep=0pt] (X) {\strut \small ↑24.2\%}; } \\

        \bottomrule
        \end{tabularx}
    }
       \caption{Per-class Coherence Relation Prediction of Fine-tuned LLama 3.2 Vision 11B (FT-Llama) on the CLUE Multi-Label dataset. The Coherence Relations predicted are Visible, Subjective (Subj), Action, Story and Meta with multiple relations being applicable to a single image-text pair.}
        \label{table:metrics_clue_ml_finetuned}
\end{table}


\subsection{Hyperparameters} \label{appendix-hyperparams}
\paragraph{Common Parameters}
\begin{itemize}
    \item LoRA Parameters: \texttt{r=16}
    \item \texttt{num\_train\_epochs = 3}
    \item \texttt{warmup\_steps = 100} since our train sets are relatively small.
    \item \texttt{per\_device\_train\_batch\_size = 32}
    \item \texttt{gradient\_accumulation\_steps = 1}
    \item \texttt{dtype = torch.bfloat16}
    \item \texttt{optim = adamw\_torch}
    \item \texttt{weight\_decay = 0.01}
    \item \texttt{lr\_scheduler\_type = cosine}
\end{itemize}

\paragraph{DisREL}
\begin{itemize}
    \item LoRA Parameters: \texttt{lora\_alpha=16}
    \item Learning Rate = $1e^{-5}$ 
\end{itemize}

\paragraph{Tweet Subtitles}
\begin{itemize}
    \item LoRA Parameters: \texttt{lora\_alpha=16}
    \item Learning Rate = $1e^{-5}$
\end{itemize}

\paragraph{CLUE Single-Label}
\begin{itemize}
    \item LoRA Parameters: \texttt{lora\_alpha=16}
    \item Learning Rate = $1e^{-5}$
\end{itemize}

\paragraph{CLUE Multi-Label}
\begin{itemize}
    \item LoRA Parameters: \texttt{lora\_alpha=8}
    \item Learning Rate = $1e^{-7}$ 
\end{itemize}

\subsection{Train Set Preparation for CLUE}
During experimentation, we noticed that models fine-tuned on CLUE Single-Label and Multi-Label, tend to skew their responses towards the majority classes (Visible, Story and Meta) in the dataset. In order to curb this behavior, we decided to randomly sample \textbf{200 examples} from the CLUE Single-Label train set for these coherence relations alone. The same image-text pairs were used for the multi-label setting as well. 

\section{Baseline Classifier Details} \label{apendix-classifier}
As mentioned in Section \ref{classifier}, we employ CLIP Text and Image Encoders (\texttt{openai/clip-vit-large-patch14} in Huggingface) in a zero-shot manner to extract multi-modal embeddings. These embeddings are then concatenated together, to form a tensor of size $1536$. This multi-modal tensor is then passed through a Multi-Layer Perceptron with two hidden layers of size $512$ and $256$, along with an output layer equal to the number of Coherence Relations in each dataset. The MLP uses RELU in between each layer for introducing non-linearity, and a Dropout of $0.2$ between the first two layers. \\

A validation split of $10\%$ was created from the train sets. The DisREL, Tweet Subtitles and CLUE Single-Label classifiers were trained using the Cross Entropy Loss, whereas the CLUE Multi-Label classifier used the Binary Cross Entropy Loss along with a Sigmoid Layer. Due to the large class imbalance in CLUE Single-Label, we use a weighted loss function in that classifier alone. Every model was trained with a batch size of $32$, using the Adam Optimizer and a learning rate of $1e^{-5}$. Table \ref{table:epochs} shows the number of epochs, for which each classifier was trained in every setting.

\begin{table}[!h]
    \centering
        \begin{tabularx}{\linewidth}{@{} c|*2Y|Y @{}}
        \toprule
        \textbf{Dataset} & \textbf{Number of Epochs} \\
        \midrule

        DisREL & 15 \\
        Tweet Subtitles & 25 \\
        CLUE Single-Label & 25 \\
        CLUE Multi-Label & 50 \\

        \bottomrule
        \end{tabularx}
       \caption{Number of epochs for which each classifier was trained.}
        \label{table:epochs}
\end{table}

\section{Computational Resources}
To evaluate and fine-tune open-source models, we use 2 NVIDIA H100 80GB HBM3 and 2 NVIDIA A100 SXM4 GPUs for around two days worth of computation.


% DisREL Prompt Templates
\begin{figure*}[t]
    \centering
    \begin{tcolorbox}[title={System Message for DisREL}, after skip=0pt, boxsep=5pt, width=\textwidth]

    You are an expert linguist and your task is to predict the Coherence Relations of a given image-text pair. A coherence relation captures the structural, logical, and purposeful relationships between an image and its text, capturing the author's intent. \\
    
    These are the possible coherence relations you can assign to an image-text pair:
    
    - Similar: The image and text provide the same information and share the same focus. There exists significant overlap in information conveyed between modalities. \\
    - Complementary: The image and text do not provide the same information or share the same focus but one modality helps understand the other better.
    
    \end{tcolorbox}
\end{figure*}

% Tweet Subtitles Prompt Template
\begin{figure*}[t]
    \centering
    \begin{tcolorbox}[title={System Message for Tweet Subtitles}, colframe = blue!30, colback = blue!10, coltitle = blue!20!black, after skip=0pt, boxsep=5pt, width=\textwidth]

    You are an expert linguist and your task is to predict the Coherence Relations of a given image-text pair. A coherence relation captures the structural, logical, and purposeful relationships between an image and its text, capturing the author's intent. \\
    
    These are the possible coherence relations you can assign to an image-text pair:
    
    - Insertion: The salient object described in the image is not explicitly mentioned in the text. \\
    - Concretization: Both the text and image contain a mention of the main visual entity. \\
    - Projection: The main entity mentioned in the text is implicitly related to the visual objects present in the image. \\
    - Restatement: The text directly describes the image contents. \\
    - Extension: The image expands upon the story or idea in the text, presenting new elements or elaborations, effectively filling in narrative gaps left by the text.
    
    \end{tcolorbox}
\end{figure*}

% CLUE SL Prompt Template
\begin{figure*}[t]
    \centering
    \begin{tcolorbox}[title={System Message for CLUE Single-Label and Multi-Label}, colframe = green!30, colback = green!10, coltitle = green!20!black, after skip=0pt, boxsep=5pt, width=\textwidth]

    You are an expert linguist and your task is to predict the Coherence Relations of a given image-text pair. A coherence relation captures the structural, logical, and purposeful relationships between an image and its text, capturing the author's intent. \\
    
    These are the possible coherence relations you can assign to an image-text pair: \\
    
    - Visible: The text presents information that is intended to recognizably characterize what is depicted in the image. \\
    - Action: The text describes an extended, dynamic process of which the moment captured in the image is a representative snapshot. \\
    - Meta: The text allows the reader to draw inferences not just about the scene depicted in the image but about the production and presentation of the image itself. \\
    - Subjective: The text provides information about the speaker's reaction to, or evaluation of, what is depicted in the image. \\
    - Story: The text provides a free-standing description of the circumstances depicted in the image, analogous to including instructional, explanatory and other background relations.
    
    \end{tcolorbox}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{tcolorbox}[title={Zero/Few Shot Prompt for DisREL, Tweet Subtitles and CLUE Single-Label}, colframe = red!30, colback = red!10, coltitle = red!20!black, after skip=0pt, boxsep=5pt, width=\textwidth]
    
    \textbf{System} \\
    <insert-system-message> \\
    
    \textbf{User} \\
    Based on provided information, predict the most applicable Coherence Relation for the next image-text pair. Output only one relation (<insert-coherence-relations) and do not include any other information in your response. \\

    \textcolor{red}{Use the format "Coherence Relation: <insert-coherence-relation>" for your response.} \\
    (Added to finetuned LLaMA 3.2 Vision's prompt in CLUE Single-Label, to enhance output format adherence.) \\
    
    \textbf{<add-few-shot-examples>} \\
    
    \textbf{<insert-image-text-pair>} \\
    
    \textbf{Assistant} \\
    Coherence Relation:
    
    \end{tcolorbox}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{tcolorbox}[title={CoT Prompt for DisREL, Tweet Subtitles and CLUE Single-Label}, colframe = red!30, colback = red!10, coltitle = red!20!black, after skip=0pt, boxsep=5pt, width=\textwidth]
    
    \textbf{System} \\
    <insert-system-message> \\
    
    \textbf{User} \\
    Before assigning a coherence relation, let's think step by step and analyze the image-text pair in depth. \\
        
    \textbf{<insert-image-text-pair>} \\
    
    \textbf{Assistant} \\
    Analysis: <add-analysis-from-model> \\

    \textbf{User} \\
    Based on provided information, predict the most applicable Coherence Relation for the next image-text pair. Output only one relation (<insert-coherence-relations>) and do not include any other information in your response. \\

    \textbf{Assistant} \\
    Coherence Relation:
    
    \end{tcolorbox}
\end{figure*}

% CLUE ML Prompt Template

\begin{figure*}[t]
    \centering
    \begin{tcolorbox}[title={Zero/Few Shot Prompt for CLUE Multi-Label}, colframe = orange!30, colback = orange!10, coltitle = orange!20!black, after skip=0pt, boxsep=5pt, width=\textwidth]
    
    \textbf{System} \\
    <insert-system-message> \\
    
    \textbf{User} \\
    Based on provided information, predict the correct Coherence Relations for the next image-text pair. \textcolor{red}{Output them as a JSON value to the key labels" and do not include any other information in your response.} (Default output format for all models) \\

    \textcolor{red}{Give your predicted labels as comma separated values. Do not include any other information in your response.} \\ (Alternate output format for LLaMA 3.2, Phi 3.5, Qwen2-VL and LLaVA-OneVision) \\

    \textcolor{red}{Use the format "Coherence Relation: <insert-coherence-relation>" for your response.} \\
    (Added to LLaVA 1.6 13B prompt to enhance output format adherence.) \\
    
    \textbf{<add-few-shot-examples>} \\
    
    \textbf{<insert-image-text-pair>} \\
    
    \textbf{Assistant} \\
    Coherence Relations:
    
    \end{tcolorbox}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{tcolorbox}[title={CoT Prompt for CLUE Multi-Label}, colframe = orange!30, colback = orange!10, coltitle = orange!20!black, after skip=0pt, boxsep=5pt, width=\textwidth]
    
    \textbf{System} \\
    <insert-system-message> \\
    
    \textbf{User} \\
    Before assigning a coherence relation, let's think step by step and analyze the image-text pair in depth. \\
        
    \textbf{<insert-image-text-pair>} \\
    
    \textbf{Assistant} \\
    Analysis: <add-analysis-from-model> \\

    \textbf{User} \\
    Now, using your analysis, predict the correct Coherence Relations for the image-text pair. \textcolor{red}{Output them as a JSON value to the key labels" and do not include any other information in your response.} (Default output format for all models) \\

    \textcolor{red}{Give your predicted labels as comma separated values. Do not include any other information in your response.} \\
    (Alternate output format for LLaMA 3.2, Phi 3.5, Qwen2-VL and LLaVA OneVision) \\

    \textcolor{red}{Use the format "Coherence Relation: <insert-coherence-relation>" for your response.} \\
    (Added to LLaVA 1.6 13B prompt to enhance output format adherence.) \\

    \textbf{Assistant} \\
    Coherence Relations:
    
    \end{tcolorbox}
\end{figure*}

% Verification task prompt template

\begin{figure*}[t]
    \centering
    \begin{tcolorbox}[title={Verification Prompt Template}, colframe = yellow!30, colback = yellow!10, coltitle = yellow!10!black, after skip=0pt, boxsep=5pt, width=\textwidth]
    
    \textbf{System} \\
    <insert-system-message> \\
    
    \textbf{User} \\
    Based on provided information, reply True (if appropriate) or False (if not appropriate) for the following image-text pair. Give your rationale behind it. \\
        
    \textbf{<insert-image-text-pair>} \\
    \textbf{<insert-coherence-relation>} \\

    \textbf{Sample Assistant Response} \\
    <True/False>

    Rationale: <model-response>
    
    \end{tcolorbox}
\end{figure*}
