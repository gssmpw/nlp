\begin{figure*}
    \centering
    \includegraphics[width=0.85\linewidth]{"./images/3_ft_vis.png"}
    \caption{An overview of the Image-Text label (i.e., Coherence Relations) distributions across {\name}}
    \label{fig:data_dist}
\end{figure*}

\section{Related Work}

\paragraph{Multimodal Large Language Models}
MLLMs are fundamentally generative models that combine Large Language Models (LLM) \cite{Brown2020-hw} with multimodal encoders \cite{Dosovitskiy2021-hj}. In recent years, several new MLLMs have been released, based on various proprietary \cite{OpenAI2024-hr, AnthropicUnknown-hu, Pichai2024-xj} and open-source LLM backbones \cite{Liu2023-os, Wu2024-ip, Bai2023-lu}. These models have shown impressive performance on a variety of downstream reasoning tasks, including Visual Question Answering \cite{Wu2024-pq}, Document Analysis \cite{Lv2023-ex}, Embodied AI agents \cite{Shek2024-dx}, etc. 


\paragraph{MLLM Reasoning Benchmarks}
Recent works that have proposed benchmarks evaluating vision language reasoning, focus on assessing different facets of their input modalities. Visual Reasoning benchmarks measure the capability of these models to understand spatial and object-level relations among image components \cite{Kamath2023-ls, Rajabi2024-sa, Nie2024-nv, Thrush2022-yf, Kamoi2024-fc}. Contextual Reasoning benchmarks demonstrate how MLLMs interpret in-context examples and compositional language prompts \cite{Zong2024-xa, Wu2024-pq, Shao2024-jq, Zeng2024-sp}. Finally, Knowledge-based reasoning assesses how models recall knowledge from intrinsic and extrinsic sources to answer factual and logical questions \cite{Johnson2016-ut, Xenos2023-gz, Lu2022-aw}. Although these benchmarks measure how multimodal prompts can be efficiently understood to solve a candidate task, intermodal reasoning with real-world discourses has been less studied. 


\paragraph{Image-Text Relationships}
Quantifying image-text relationships accurately has been an active area of research in the era of Vision Language Models (VLMs). Traditional VLMs translate images and text into a common representation space and compute the degree of similarity based on the distance between these embeddings \cite{Radford2021-ro, Jia2021-lq, Caron2021-cq, Hessel2021-we}. However, these methods failed to capture human preferences in image-text matching accurately across different task domain benchmarks \cite{Anantha-Ramakrishnan2024-sv, Ross2024-np, Anantha-Ramakrishnan2024-rm}. To include human feedback in the process of predicting similarity scores, content-based models trained on human-annotated similarity scores were introduced \cite{Wu2023-qy, Kirstain2023-km, Xu2023-rj}. Apart from similarity scores, taxonomies have been proposed to quantify different types of linkages between image-text pairs \cite{Marsh2003-hz, Vempala2019-lh, Kruk2019-ac, Bateman2014-dx}. In particular, multimodal coherence relations have been shown to sufficiently capture different aspects of image-text intents for various vision-language tasks \cite{Alikhani2019-kn, Inan2021-ip, Alikhani2023-nm, Alikhani2020-nr, Xu2022-ie}.

