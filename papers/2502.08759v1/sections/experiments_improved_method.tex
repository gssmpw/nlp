\section{Experiments for Hybrid Feedback Mechanism}

\subsection{Synthetic Dataset Construction}

To evaluate the proposed \textbf{HybridLinearBandit} method, we constructed a synthetic dataset simulating a recommendation environment. The dataset includes:

- \textbf{Contextual Features}:
  - Each context \(\mathbf{x}_t \in \mathbb{R}^d\) represents the feature vector for the current interaction.
  - Actions \(a \in \mathcal{A}\) are associated with feature vectors \(\mathbf{x}_{t,a} \in \mathbb{R}^d\).

- \textbf{Reward Generation}:
  - The true reward \(r_t\) for an action is modeled as:
    \[
    r_t = \mathbf{x}_{t,a}^\top \theta^* + \epsilon_t
    \]
    where \(\theta^* \in \mathbb{R}^d\) is a latent parameter vector, and \(\epsilon_t\) is Gaussian noise \(\mathcal{N}(0, \sigma^2)\) to simulate real-world uncertainty.
  - Rewards are normalized to lie within the range \([0, 1]\).

- \textbf{Non-Linear Interactions}:
  - We introduce non-linear dependencies by applying \(\tanh\) transformations to user-item interactions, capturing more complex patterns.

- \textbf{Sparse Rewards}:
  - To simulate practical sparsity in rewards, only a subset of user-item interactions provides meaningful rewards, controlled by a sparsity mask.

\subsection{Baselines}

We compare our \textbf{HybridLinearBandit} approach against the following baselines:

\textbf{Epsilon-Greedy}:
  Selects actions randomly with probability \(\epsilon = 0.1\) and otherwise exploits the best-known action.

\textbf{Upper Confidence Bound (UCB)}:
  Balances exploration and exploitation by selecting actions with the highest upper confidence bound.

\textbf{Action Recommendation (AR)}:
  Always requests human feedback to directly guide the action selection.

\textbf{Reward Manipulation (RM)}:
  Always requests human feedback to refine the observed reward signal.

\textbf{Hybrid Feedback (HF)}:
  Dynamically switches between AR and RM based on the model’s uncertainty, leveraging entropy.

\subsection{Experiment Settings}

\textbf{Rounds and Runs}:
  Each experiment runs for \(T = 1000\) rounds, with \(5\) independent trials to account for variability.

\textbf{Hyperparameters}:
  Initial entropy threshold \(\tau_{\text{init}} = 0.5\).
  Maximum entropy threshold \(\tau_{\text{max}} = 1.5\).
  Learning rate \(\alpha = 0.1\).
  Regularization parameter \(\lambda = 0.01\).

\textbf{Evaluation Metrics}: Cumulative Reward: Measures the total reward achieved over \(T\) rounds. Cumulative Regret: The gap between the optimal reward and the agent’s reward:
    \[
    \text{Regret}_T = \sum_{t=1}^T \left( r_t^* - r_t \right)
    \]
Feedback Cost: Tracks the frequency and type of human feedback requested.

\subsection{Results and Analysis}

We report the mean cumulative rewards and regrets over \(5\) runs, along with standard deviations. Error bars are provided to show variability.

\textbf{Cumulative Rewards}:
  \textbf{HF} consistently outperforms AR and RM alone, as well as Epsilon-Greedy and UCB.
  The dynamic balance of AR for high-uncertainty exploration and RM for low-uncertainty exploitation ensures robust performance.

\textbf{Cumulative Regret}:
  \textbf{HF} achieves the lowest cumulative regret, demonstrating its efficiency in minimizing suboptimal actions over time.

\textbf{Feedback Cost}:
 Compared to AR and RM, \textbf{HF} significantly reduces feedback frequency as the model learns, reducing cognitive load while maintaining performance.

The results confirm that \textbf{HF} offers an effective and scalable approach to integrating human expertise in contextual bandits. It balances exploration and exploitation more efficiently than static feedback methods.