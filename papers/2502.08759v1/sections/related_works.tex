\section{Related works}
\textbf{Contextual bandits}
Contextual bandits have diverse applications in recommendation systems~\citep{li2010contextual, xu2020contextual}, healthcare~\citep{yu2024careforme}, finance~\citep{zhu2021online}, and other fields~\citep{bouneffouf2020survey}. CBs are a variant of the multi-armed bandit problem where each round is influenced by a specific context, and rewards vary accordingly. This adaptability makes CBs valuable for enhancing various machine learning methods, including supervised learning~\citep{sui2020bayesian}, unsupervised learning~\citep{sublime2018collaborative}, active learning~\citep{bouneffouf2014contextual}, and reinforcement learning~\citep{intayoad2020reinforcement}.

To tackle CB challenges, several algorithms have been developed, such as LINUCB~\citep{li2010contextual}, Neural Bandit~\citep{allesiardo2014neural}, and Thompson sampling~\citep{agrawal2013thompson}. These typically assume a linear dependency between the expected reward and its context. Despite these advancements, CBs often rely on implicit feedback, like user clicks, leading to biased and incomplete evaluations of user preferences~\citep{qi2018bandit}. This reliance complicates accurately gauging user responses and tailoring the learning process.



\textbf{Human feedback in the loop}
Recent advancements in human-in-the-loop methodologies have shown significant successes in real-life applications, such as ChatGPT via reinforcement learning with human feedback (RLHF)~\citep{macglashan2017interactive}, as well as in robotics~\citep{argall2009survey} and health informatics~\citep{holzinger2016interactive}. 

Preference-based feedback can be categorized into three groups: i) action-based preferences~\citep{furnkranz2012preference}, where experts rank actions, ii) state preferences~\citep{wirth2014learning}, and iii) trajectory preferences~\cite{busa2014preference,novoseller2020dueling}. Action-based feedback from humans is explored in~\citep{mandel2017add}, where experts add actions to a reinforcement learning agent to boost performance. Other forms of explicit human feedback include reward shaping~\citep{xiao2020fresh,biyik2022learning,ibarz2018reward,arakawa2018dqn}. These approaches however do not account for acquiring feedback based on the learner's uncertainty or the impact of varying levels of feedback on performance.


\textbf{Contextual bandits with human feedback} Human-in-the-Loop Reinforcement Learning addresses the bias problem of implicit feedback in contextual bandits. The exploration of learning in multi-armed bandits with human feedback is discussed in~\citep{tang2019bandit}, where a human expert provides biased reports based on observed rewards. The learner's goal is to select arms sequentially using this biased feedback to maximize rewards, without direct access to the actual rewards.

Preference-based feedback in contextual and dueling bandit frameworks has been explored in previous studies~\citep{sekhari2023contextual,dudik2015contextual,saha2021optimal,wu2023borda}. The learner presents candidate actions and receives noisy preferences from a human expert, focusing on minimizing regret and active queries. In contrast, we consider a setup where the learner receives direct feedback from human experts and show how the fraction of active queries varies with different sets of experts.

\textbf{Active learning in contexual bandits} Active learning~\citep{judah2014active} enhances performance by selectively querying the most informative data points for labeling, rather than passively receiving labels for randomly or sequentially presented data. In the context of bandit algorithms, active learning has been employed to optimize the exploration-exploitation trade-off by guiding the algorithm to request feedback or labels when it is most uncertain about an actionâ€™s outcome~\citep{taylor2009transfer}. For example, \cite{bouneffouf2014contextual} integrated active learning with Thompson sampling and UCB algorithms in contextual bandits, resulting in improved sample efficiency.

While active learning traditionally assumes access to a small set of labeled examples alongside abundant unlabeled data, the contextual bandit setting differs fundamentally in that all feedback is inherently partial - we only observe outcomes for the chosen action. This structural divergence precludes direct application of standard active learning techniques. Nevertheless, we draw methodological inspiration from active learning's core philosophy of strategic information acquisition.

In our work, we build on this idea by combining active learning principles with human feedback, utilizing an entropy-based mechanism to query feedback when necessary. By incorporating these selective querying strategies into our contextual bandit framework, we aim to more effectively balance exploration and exploitation, particularly in scenarios where human feedback is noisy or costly. This approach not only improves sample efficiency but also helps mitigate the challenges posed by varying feedback quality.


