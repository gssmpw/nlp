
\subsection{Variation of model performance based on different expert quality}
We first present the effect of different expert quality on the two types of feedback discussed in Sections~\ref{sec:action_recommendation} and~\ref{sec:reward_penalty}. Note that we can compute the entropy of policy $\pi$ for the PPO, PPO-LSTM, Reinforce, Actor-Critic and LinearUCB and Bootstrapped Thompson sampling. We now present the results associated with different expert levels in for the four environments discussed in Section~\ref{sec:experiments}. Figure~\ref{fig:app_perf_expert_level} shows the variation of different expert qualities for different range of learners. The bar plot in orange shows the model performance when reward manipulation is used as a feedback from the human expert and the bar plot in blue shows the model performance when action recommendation as a feedback from human feedback. Notably, higher expert quality does not universally translate to better performance—a counterintuitive relationship whose subtleties we analyze in subsequent sections. Our analysis shows that for different expert levels the effectiveness of incorporating human feedback depends on the learner. Comparison of expert levels with model performance for other learners are shown in Appendix~\ref{sec:app_expert_quality_perf}.

\begin{figure*}[H]
    
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
    \hspace{1.5cm}\setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{Bibtex}}
    %\text{\hspace{1.5cm}Bibtex}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/bibtex/ppo/expert_accuracy_range.pdf}
        
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\textwidth}
        \hspace{1.5cm}\setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{Delicious}}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/delicious/ppo/expert_accuracy_range.pdf}
        
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\textwidth}
        \hspace{1.5cm}\setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{Media\_Mill}}
        %\text{\hspace{0.23cm}MediaMill}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/media_mill/ppo/expert_accuracy_range.pdf}
        
    \end{subfigure}
    \hfill
    % \begin{subfigure}[b]{0.23\columnwidth}
    %     \text{\hspace{0.5cm}Yahoo}
    %     \includegraphics[width=\linewidth]
    %     {figures/distribution_plots/ppo_yahoo.pdf}
        
    % \end{subfigure}
   \text{PPO}\\
    
    
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/bibtex/reinforce/expert_accuracy_range.pdf}
        
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/delicious/reinforce/expert_accuracy_range.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/media_mill/reinforce/expert_accuracy_range.pdf}
        
    \end{subfigure}
    % \hfill
    % \begin{subfigure}[b]{0.23\columnwidth}
    %     \includegraphics[width=2.3cm]{figures/distribution_plots/reinforce_yahoo.pdf}
       
    % \end{subfigure}
   \text{Reinforce} \\
    
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/bibtex/reinforce/expert_accuracy_range.pdf}
        
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/delicious/reinforce/expert_accuracy_range.pdf}
        
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/media_mill/reinforce/expert_accuracy_range.pdf}
        
    \end{subfigure}
    % \hfill
    % \begin{subfigure}[b]{0.23\columnwidth}
    %     \includegraphics[width=2.3cm]{figures/distribution_plots/linearucb_yahoo.pdf}
       
    % \end{subfigure}
    \text{Linear UCB}\\

    \caption{Comparison of expert feedback for different learners based on different expert qualities. The results show that mean cumulative reward for different datasets and algorithms vary in a different manner for the two feedback schemes considered. Higher levels of expert does not necessary results in better performance. }%\lili{Add the conclusion for the plots}}
    \label{fig:expert_performance_comp} 
\end{figure*}


\begin{table*}[htbp]
    \centering
    \caption{Performance comparison of algorithms for different quality of expert feedback. The values in bold represent the minimum mean cumulative regret achieved across different levels of expert.} 
    \label{table:expert_range}
    %\lili{Add the conclusion for the plots}}
    \vspace{+2mm}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lllcccc}
        \toprule
        \textbf{Feedback Type} & \textbf{Algorithm Name} & \textbf{Environment Name} & \multicolumn{1}{c}{\textbf{0.2}} & \multicolumn{1}{c}{\textbf{0.4}} & \multicolumn{1}{c}{\textbf{0.6}} & \multicolumn{1}{c}{\textbf{0.8}} \\
        \midrule
        Action Recommendation & PPO & Bibtex & $122.84957\pm8.26216$ & $121.19000\pm 7.14612$ & \bm{$120.88720\pm5.29698$} & $121.78527\pm6.20433$ \\[5pt]
        Reward Manipulation & PPO & Bibtex & $120.99013\pm 7.38448$ & \bm{$115.75083\pm5.24335$} & $121.48417\pm6.96902$ & $121.47040\pm 7.24336$ \\[5pt]
        Action Recommendation & PPO & Delicious & $\mathbf{111.38593\pm2.51805}$ & $134.23813\pm2.29251$ & $175.17900\pm1.52139$ & $136.76483\pm9.70185$ \\[5pt]
        Reward Manipulation & PPO & Delicious & $\mathbf{101.41833\pm2.39055}$ & $119.30050\pm2.20132$ & $127.08153\pm3.21616$ & $139.94187\pm2.04167$ \\[5pt]
        Action Recommendation & PPO-LSTM & Media\_Mill & $38.43170\pm1.76982$ & $38.44773\pm1.77304$ & $38.56507\pm 1.77324$ & \bm{$38.38180\pm1.76374$} \\[5pt]
        Reward Manipulation & PPO-LSTM & Media\_Mill & $38.61077\pm1.82181$ & $38.59887\pm1.80596$ & \bm{$38.40867\pm1.78106$} & $38.59343\pm1.76473$ \\[5pt]
        
        % Action Recommendation & LinearUCB & Bibtex & $\mathbf{0.02478 \pm 0.00068}$ & $0.02280 \pm 0.00056$ & $0.02145 \pm 0.00066$ & $0.02002 \pm 0.00055$ \\[5pt]
        % Reward Manipulation & LinearUCB & Bibtex & $0.02369 \pm 0.00080$ & $0.02532 \pm 0.00079$ & $0.02518 \pm 0.00049$ & $\mathbf{0.03527 \pm 0.00115}$ \\[5pt]
        % Action Recommendation & LinearUCB & Delicious & $\mathbf{0.02430 \pm 0.00053}$ & $0.01818 \pm 0.00036$ & $0.02064 \pm 0.00061$ & $0.05308 \pm 0.00066$ \\[5pt]
        % Reward Manipulation & LinearUCB & Delicious & $0.01664 \pm 0.00022$ & $\mathbf{0.10018 \pm 0.00161}$ & $0.01889 \pm 0.00051$ & $0.08540 \pm 0.00063$ \\[5pt]
        Action Recommendation & Bootstrapped-TS & Bibtex & $196.99950\pm0.53562$ & $196.99950+-0.53562$ & $196.99950\pm0.53562$ & $\mathbf{196.97300\pm0.54129}$ \\[5pt]
        Reward Manipulation & Bootstrapped-TS & Bibtex & $197.06000\pm0.45578$ & $\mathbf{197.06000\pm0.45578}$ & $197.06000\pm0.45578$ & $197.06000\pm0.45578$ \\[5pt]  
        \bottomrule
    \end{tabular}%
    }
\end{table*}

\begin{figure*}[!htb]
    \centering
    \begin{minipage}[b]{0.31\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/baseline_comparison/bibtex/bibtex_comparison.pdf}
        \captionsetup{labelformat=empty}
        \text{Bibtex}
    \end{minipage}
    \hspace{0.02\textwidth}
    \begin{minipage}[b]{0.31\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/baseline_comparison/delicious/delicious_comparison.pdf}
        \captionsetup{labelformat=empty}
        \text{Delicious}
    \end{minipage}
    \hspace{0.02\textwidth}
    \begin{minipage}[b]{0.31\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/baseline_comparison/media_mill/media_mill_comparison.pdf}
        \captionsetup{labelformat=empty}
        \text{Media Mill}
    \end{minipage}
    \setcounter{figure}{2}
    % \caption{Performance comparison with baselines. Human feedback consistently leads to large performance gains.}
    \caption{Performance comparison of baselines and the proposed schemes. The figures show that using entropy based feedback leads to lower mean cumulative regret. The solid line represents the mean cumulative regret and the shaded region represents the $\pm$ 1 standard deviation across the mean.}
    \label{fig:baseline_comp}
\end{figure*}


\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/bibtex/ppo-lstm/expert_accuracy_range.pdf}
        \caption{PPO-LSTM (Bibtex)}
        
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/bibtex/reinforce/expert_accuracy_range.pdf}
       \caption{Reinforce (Bibtex)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/bibtex/actor-critic/expert_accuracy_range.pdf}
        \caption{Actor Critic (Bibtex)}
        \end{subfigure}
        \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/bibtex/bootstrapped-ts/expert_accuracy_range.pdf}
        \caption{Bootstrapped-TS (Bibtex)}
    \end{subfigure}
    % \hfill
    % \begin{subfigure}[b]{0.23\columnwidth}
    %     \includegraphics[width=2.3cm]{figures/distribution_plots/ppo-lstm_yahoo.pdf}
        
    % \end{subfigure}
    
    \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/delicious/ppo-lstm/expert_accuracy_range.pdf}
        \caption{PPO-LSTM (Delicious)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/delicious/reinforce/expert_accuracy_range.pdf}
        \caption{Reinforce (Delicious)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/delicious/actor-critic/expert_accuracy_range.pdf}
        \caption{Actor Critic (Delicious)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/delicious/bootstrapped-ts/expert_accuracy_range.pdf}
        \caption{Bootstrapped-TS (Delicious)}
    \end{subfigure}

   
    \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/media_mill/ppo-lstm/expert_accuracy_range.pdf}
        
        \caption{PPO-LSTM (Media Mill)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/media_mill/reinforce/expert_accuracy_range.pdf}
        \caption{Reinforce (Media Mill)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/media_mill/actor-critic/expert_accuracy_range.pdf}
        \caption{Actor Critic (Media Mill)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{figures/expert_accuracy_range/media_mill/bootstrapped-ts/expert_accuracy_range.pdf}
        \caption{Bootstrapped-TS (Media Mill)}
    \end{subfigure}
    % \hfill
    % \begin{subfigure}[b]{0.23\columnwidth}
    %     \includegraphics[width=2.3cm]{figures/distribution_plots/linearucb_yahoo.pdf}
       
    % \end{subfigure}

    \caption{Comparison of expert feedback for different learners based on different expert qualities. The results show that mean cumulative regret for different datasets and algorithms vary in a different manner for the two feedback schemes considered. Higher levels of expert does not necessary results in better performance. }
    \label{fig:app_perf_expert_level}

    
\end{figure*}

%%BUBBLE PLOT MOVE%%%%%%
% \begin{figure*}[!thb]
%     \centering
%     % First row of figures
    
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{Bibtex}}
%         \includegraphics[width=\textwidth]{figures/plot_analysis_entropy_expert/bibtex/action_restriction_accuracy/ppo_entropy_expert.pdf}
     
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{Delicious}}
%         \includegraphics[width=\textwidth]{figures/plot_analysis_entropy_expert/delicious/action_restriction_accuracy/reinforce_entropy_expert.pdf}
       
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{MediaMill}}
%         \includegraphics[width=\textwidth]{figures/plot_analysis_entropy_expert/media_mill/action_restriction_accuracy/ppo-lstm_entropy_expert.pdf}
       
%     \end{subfigure}
%     \setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{Action Recommendation}}
%     \vspace{1em} % Space between rows
    
%     % Second row of figures
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/plot_analysis_entropy_expert/bibtex/reward_penalty_accuracy/ppo_entropy_expert.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/plot_analysis_entropy_expert/delicious/reward_penalty_accuracy/reinforce_entropy_expert.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/plot_analysis_entropy_expert/media_mill/reward_penalty_accuracy/ppo-lstm_entropy_expert.pdf}
%     \end{subfigure}
%     \setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{Reward Manipulation}}
    
%     \caption{Comparison of model performance for different values of entropy and expert accuracies for feedback: Action Recommendation and Reward Manipulation. The size and color of each bubble in the bubble plots represent the magnitude of the mean cumulative reward.}
%     \label{fig:entr_thresh_expert_acc_ar_rp}
% \end{figure*}

\subsection{Incorporating entropy based feedback achieves higher performance compared to baselines}
We optimize the model performance across various expert levels and compare these results with baseline models, including EE-Net. Figure~\ref{fig:baseline_comp} presents the mean cumulative regret for the optimized expert level (as obtained from Table~\ref{table:expert_range}), highlighting the significant performance gains achieved by incorporating entropy-based feedback over the baselines.

Our analysis, conducted across all datasets, demonstrates that integrating entropy-based feedback—specifically Action Recommendation (AR) and Reward Modification (RM)—consistently outperforms both TAMER and EE-Net. Moreover, we observe that the proportion of steps during which the algorithm seeks human expert feedback varies across datasets. Importantly, the results reveal two key findings: 

Firstly, learners benefit substantially from entropy-based feedback compared to when no such feedback is provided. This improvement underscores the effectiveness of entropy thresholds in selectively involving human experts, thereby guiding the learning process. In fact, even with a modest number of queries to the human expert (less than 30\% of the total training steps), entropy-based feedback drives superior performance over the baseline models. Secondly, the final performance of the learners is not necessarily a monotonous function of the quality of the human feedback, as shown in Figure~\ref{fig:app_perf_expert_level}. 
% fig:expert_performance_comp

Interestingly, the performance of AR and RM varies between datasets. For example, on the Bibtex dataset, AR performs worse compared to RM, while on the Delicious dataset, AR demonstrates the best performance among the three. This difference arises due to how penalties affect exploration: Bibtex, with fewer actions, benefits less from AR's action-space limitation, whereas Delicious, with many possible actions, sees AR accelerating convergence by narrowing down the action space early in the learning process. As a result, AR's advantage becomes more apparent in environments where an overwhelming number of actions could otherwise slow down the learner’s progress.

Further details regarding the proportion of expert queries for different levels of expert quality are provided in Appendix~\ref{sec:app_var_exp_query_percentage}.

%\lili{Provide the reference of appendix}


\subsection{Effect of entropy threshold and expert accuracy on model performance}
Figure~\ref{fig:entr_thresh_expert_acc_ar_rp} (Appendix) presents bubble plots comparing model performance at different expert levels and entropy threshold values for both AR and RM feedback types. The size and color of each bubble represent the mean cumulative reward for the corresponding learner. 

We begin by analyzing the results for AR feedback. Generally, we observe that at higher entropy threshold values, the model's performance remains relatively stable across different expert levels. This behavior is expected, as higher entropy thresholds result in fewer queries to the human expert, reducing the impact of expert quality on performance. 

However, at lower entropy thresholds, an interesting pattern emerges: increasing expert quality can actually lead to a decrease in model performance. This phenomenon relates to the exploration-exploitation trade-off. At high expert levels, the expert consistently provides accurate recommendations, and since the model is designed to always accept these recommendations in the AR setting, the result is pure exploitation. Conversely, at lower expert levels, where recommendations are more random, the model is encouraged to explore a broader set of actions, which can ultimately yield higher cumulative rewards or lower cumulative regrets.

A similar pattern is observed with RM feedback. At higher entropy thresholds, the differences in performance between varying expert levels are minimal, as fewer queries are made to the expert. At lower entropy thresholds, however, we again see a decline in performance as expert quality increases. 

Further bubble plots illustrating these trends for other learners, under both AR and RM feedback, can be found in Appendix~\ref{sec:app_entr_threshold_expert_acc_perf}.




\captionsetup{font=footnotesize}


%from cgpt

%%this is for reward penalty







% \begin{figure}[htbp]
%     \centering
    
%     \begin{subfigure}{0.24\columnwidth}
%         \centering
%         \setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{PPO}}
%         \includegraphics[width=2.8cm, height=2.3cm]{figures/correlation_plots/ppo_action_restriction.pdf} 
%         \label{fig:sub1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.24\columnwidth}
%         \centering
%         \setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{PPO-LSTM}}
%         \includegraphics[width=2.8cm, height=2.3cm]{figures/correlation_plots/ppo-lstm_action_restriction.pdf}
%         \label{fig:sub3}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.24\columnwidth}
%         \centering
%         \setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{Reinforce}}
%         \includegraphics[width=2.8cm, height=2.3cm]{figures/correlation_plots/reinforce_action_restriction.pdf}
%         \label{fig:sub5}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.24\columnwidth}
%         \centering
%         \setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{LinearUCB}}
%         \includegraphics[width=2.8cm, height=2.3cm]{figures/correlation_plots/linearucb_action_restriction.pdf}
%         \label{fig:sub7}
%     \end{subfigure} \\
%     \vspace{1mm}
%     % \vspace{1mm}
%     \setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{Action Restriction}} \\
%     \vspace{2mm}
%     %\vspace{0.15cm} % Add vertical space between rows
%     \begin{subfigure}{0.24\columnwidth}
%         \centering
%         \includegraphics[width=2.8cm, height=2.3cm]{figures/correlation_plots/ppo_reward_penalty.pdf}
%         \label{fig:sub2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.24\columnwidth}
%         \centering
%         \includegraphics[width=2.8cm, height=2.3cm]{figures/correlation_plots/ppo-lstm_reward_penalty.pdf}
%         \label{fig:sub4}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.24\columnwidth}
%         \centering
%         \includegraphics[width=2.8cm, height=2.3cm]{figures/correlation_plots/reinforce_reward_penalty.pdf}
%         \label{fig:sub6}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.24\columnwidth}
%         \centering
%         \includegraphics[width=2.8cm, height=2.3cm]{figures/correlation_plots/linearucb_reward_penalty.pdf}
%         \label{fig:sub8}
%     \end{subfigure}
%     \vspace{1mm}
%     \setlength{\fboxsep}{1pt}\colorbox{lightgray!30}{\textbf{Reward Penalty}}
%     \vspace{2mm}
%     \caption{Variation of model performance with different levels of experts. Higher expert levels consistently, but not always, lead to performance improvements.}
%     \label{fig:perf_comp_expert_levels}
% \end{figure}
% Figure~\ref{fig:perf_comp_expert_levels}, shows the variation of performance with levels of experts. Each rows represent different feedback types (Action Restriction, Reward Penalty) and each columns represent different algorithms. 


%%%%% ends for distribution shift

%%combined table
% \begin{table}[htbp]
%     \centering
%     \caption{Model Accuracy with Entropy Based Feedback}
%     \label{tab:merged-performance}
%     \resizebox{\linewidth}{!}{%
%     \begin{tabular}{@{}lllll@{}}
%         \toprule
%         & bibtex & delicious & media\_mill & yahoo \\
%         \midrule
%         \multicolumn{5}{c}{\textbf{PPO}} \\
%         \midrule
%         baselines & $0.3148 \pm 0.0341$ & $0.3997 \pm 0.0685$ & $0.7797 \pm 0.0337$ & $0.034\pm0.015$ \\
%         AR-E & \bm{$0.6143 \pm 0.0209$} & \bm{$0.5512 \pm 0.0275$} & $0.7776 \pm 0.0180$ & $0.036\pm0.009$ \\
%         AR-ME & $0.6103 \pm 0.0198$ & $0.4616 \pm 0.0327$ & $0.7741 \pm 0.1782$ & $0.037\pm0.001$ \\
%         AR-LE & $0.5430 \pm 0.0224$ & $0.4626 \pm 0.0518$ & \bm{$0.7807 \pm 0.0170$} & \bm{$0.040\pm0.008$} \\
%         RP-E & $0.0741 \pm 0.0662$ & $0.2590 \pm 0.0567$ & $0.7784 \pm 0.0180$ & $0.037\pm0.009$ \\
%         RP-ME & $0.0738 \pm 0.0609$ & $0.3759 \pm 0.0258$ & $0.7805 \pm 0.019$ & $0.037\pm0.008$ \\
%         RP-LE & $0.1749 \pm 0.0443$ & $0.2932 \pm 0.0593$ & $0.7665 \pm 0.0183$ & $0.038\pm0.007$ \\
%         \midrule
%         \multicolumn{5}{c}{\textbf{PPO-LSTM}} \\
%         \midrule
%         baselines & $0.2328 \pm 0.0675$ & $0.3827 \pm 0.0370$ & $0.7726 \pm 0.0319$ & $0.036\pm0.014$ \\
%         AR-E & $0.2605 \pm 0.0210$ & $0.3894 \pm 0.0209$ & $0.7699 \pm 0.0185$ & \bm{$0.038\pm0.008$} \\
%         AR-ME & $0.2710 \pm 0.0227$ & \bm{$0.3904 \pm 0.0206$} & $0.7726 \pm 0.0215$ & $0.036\pm0.009$ \\
%         AR-LE & \bm{$0.2961 \pm 0.0187$} & $0.3858 \pm 0.0229$ & \bm{$0.7759 \pm 0.0186$} & $0.037\pm0.008$ \\
%         RP-E & $0.1352 \pm 0.0136$ & $0.3681 \pm 0.0272$ & $0.7744 \pm 0.0244$ & $0.036\pm0.009$ \\
%         RP-ME & $0.1349 \pm 0.0152$ & $0.3743 \pm 0.0229$ & $0.7681 \pm 0.0182$ & $0.036\pm0.007$ \\
%         RP-LE & $0.1357 \pm 0.0145$ & $0.3730 \pm 0.0291$ & $0.7759 \pm 0.0205$ & $0.037\pm0.008$ \\
%         \midrule
%         \multicolumn{5}{c}{\textbf{Reinforce}} \\
%         \midrule
%         baselines & $0.3479 \pm 0.0356$ & $0.4670 \pm 0.0661$ & \bm{$0.7782 \pm 0.0352$} & $0.039\pm0.014$\\
%         AR-E & \bm{$0.5555 \pm 0.0209$} & $0.4605 \pm 0.0177$ & $0.7752 \pm 0.0178$ & $0.037\pm0.007$ \\
%         AR-ME & $0.5366 \pm 0.0207$ & $0.4390 \pm 0.0474$ & $0.7751 \pm 0.0170$ & $0.039\pm0.009$ \\
%         AR-LE & $0.5283 \pm 0.0270$ & $0.4362 \pm 0.0377$ & $0.7769 \pm 0.0190$ & \bm{$0.040\pm0.009$} \\
%         RP-E & $0.0768 \pm 0.0608$ & $0.4901 \pm 0.0217$ & $0.7733 \pm 0.0206$ & $0.038\pm 0.010$ \\
%         RP-ME & $0.0812 \pm 0.0587$ & $0.4841 \pm 0.0289$ & $0.7761 \pm 0.0203$ & $0.035\pm0.009$ \\
%         RP-LE & $0.0814 \pm 0.0627$ & \bm{$0.5165 \pm 0.0252$} & $0.7757 \pm 0.0192$ & $0.037\pm0.010$\\
%         \midrule
%         \multicolumn{5}{c}{\textbf{LinearUCB}} \\
%         \midrule
%          baselines & $0.0124\pm0.0094$ & $0.0045\pm0.0056$ & $0.1826\pm0.1802$ & $0.0344\pm0.0120$ \\
%         AR-E & \bm{$0.0776\pm0.0105$} & $0.0337\pm0.0082$ & $0.0602\pm0.0248$ & \bm{$0.0387\pm0.0088$} \\
%         AR-ME & $0.0565\pm0.0224$ & \bm{$0.0906\pm0.0483$} & $0.0180\pm0.0055$ & $0.0376\pm0.0083$ \\
%         AR-LE & $0.0484\pm0.0362$ & $0.0191\pm0.0165$ & \bm{$0.3914\pm0.3859$} & $0.0357\pm0.0099$ \\
%         RP-E & $0.0181\pm0.0121$ & $0.0396\pm0.0277$ & $0.0015\pm0.0011$ & $0.0380\pm0.0093$ \\
%         RP-ME & $0.0117\pm0.0054$ & $0.0642\pm0.0625$ & $0.0077\pm0.0053$ & $0.0369\pm0.0080$ \\
%         RP-LE & $0.0226\pm0.0066$ & $0.0124\pm0.0098$ & $0.0088\pm0.0041$ & $0.0349\pm0.0077$ \\
%         \bottomrule
%     \end{tabular}%
%     }
% \end{table}
%%end combined table


\subsection{Observed differences between feedback types}
Figure~\ref{fig:baseline_comp} illustrates how the two forms of feedback, AR and RM, interact differently with the underlying algorithms and datasets. The choice of feedback type should therefore depend on the specific application.

Our results generally indicate that at higher expert levels, AR tends to be more effective than RM. This is likely because AR directly influences the actions taken by the contextual bandit (CB), interfering less with its reward-based learning process. At low expert levels, however, AR can become disruptive, leading to poor exploration by prematurely narrowing the action space. In contrast, at high expert levels, AR provides clearer guidance for the bandit’s exploration, optimizing action selection while leaving the reward structure relatively intact.

Ultimately, this suggests that AR is particularly advantageous when expert quality is high, as it can effectively guide exploration without destabilizing the learning process.

% As shown in Figure~\ref{fig:baseline_comp}, the two forms of feedback $AR$ and $RM$ function differently with respect to the underlying algorithm and the datasets considered. Hence an appropriate choice of the type of feedback will depend on the applications being considered. 

% We generally observe that at higher expert levels AR is more useful than RM. We hypothesize that this is because AR interferes more with the CB's function (impacting actions rather than rewards) but less with its learning. At low expert levels, AR can be more disruptive, leading to sub-optimal exploration. However, at high expert levels, AR can more directly guide the bandit's exploration while having minimal impact on the reward structure.



%\subsection{Limitations}
%Our approach suffers from a set of limitations, made necessary by the extensive nature of the problem, which requires exploring a subset of all possibilities. First, our setting is clearly idealized. We would benefit from higher-quality human feedback, ideally via an environment where the algorithm could interact directly with a user. Rather, we are forced to approximate this data via the different expert settings. Second, we would also like to experiment more with other settings, in particular in how our proposed human feedback could be utilized in more diverse RL settings, such as imitation learning, which could allow a deeper understanding of the tradeoffs at play.