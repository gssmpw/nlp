\subsection{Regret bound for contexual bandits with entropy-based human feedback}

We determine a regret bound for our proposed algorithm, focusing on entropy-based human feedback in a contextual bandit setting. The goal is to show how incorporating selective oracle feedback affects cumulative regret.


Let \(T\) be the total number of rounds, and \(\mathcal{A}\) the set of available actions. At each round \(t\): The agent observes a context \(s_t\). The agent selects an action \(a_t \in \mathcal{A}\) based on its policy \(\pi_t\), which incorporates feedback if requested. The oracle feedback is solicited when the entropy of the policy \(H(\pi_t)\) exceeds a threshold \(\lambda\). The observed reward \(r_t(a_t)\) is a combination of environment and feedback rewards.

The expected regret at time \(t\) is defined as:
\[
\text{Regret}_t = \mathbb{E}[r_t(a^*_t) - r_t(a_t)],
\]
% where \(a^*_t = \arg\max_{a \in \mathcal{A}} \mathbb{E}[r_t(a)]\) is the optimal action.
where \(a^*_t = \pi^*(s_t)\) is the optimal action.

The total regret over \(T\) rounds is:
\[
\text{Regret}(T) = \sum_{t=1}^T \text{Regret}_t.
\]


\textbf{Assume}:
The entropy threshold \(\lambda\) ensures that feedback is solicited with probability \(P(H(\pi_t) > \lambda) = p\).
Oracle feedback provides correct information with probability \(q_t\).

Then, the expected regret of the proposed algorithm is bounded by:
\[
\mathbb{E}[\text{Regret}(T)] \leq O\left(\sqrt{T |\mathcal{A}| \log T}\right) + O\left(\frac{(1 - p) T}{q_t}\right).
\]

1. Regret Decomposition:
   Decompose regret into two components:
   \[
   \text{Regret}(T) = \sum_{t \in \mathcal{F}} \text{Regret}_t + \sum_{t \notin \mathcal{F}} \text{Regret}_t,
   \]
   where \(\mathcal{F}\) is the set of rounds where feedback is requested (\(H(\pi_t) > \lambda\)).

2. Regret Without Feedback (\(t \notin \mathcal{F}\)):
   When no feedback is requested, the regret follows standard contextual bandit regret:
   \[
   \mathbb{E}[\text{Regret}_{\text{no-feedback}}(T)] \leq O(\sqrt{T |\mathcal{A}| \log T}).
   \]

3. Regret With Feedback (\(t \in \mathcal{F}\)):
   For rounds where feedback is solicited: (i) Feedback improves decision quality, reducing regret proportional to feedback accuracy \(q_t\). (ii) The regret in feedback rounds is bounded by \((1 - q_t)\) per round:
   \[
   \mathbb{E}[\text{Regret}_{\text{feedback}}(T)] \leq O\left(\frac{(1 - p) T}{q_t}\right).
   \]

4. Combining Terms:
   Combining both terms yields the total regret bound:
   \[
   \mathbb{E}[\text{Regret}(T)] \leq O\left(\sqrt{T |\mathcal{A}| \log T}\right) + O\left(\frac{(1 - p) T}{q_t}\right).
   \]


We have the following implications:

- Feedback Benefit: The bound highlights how oracle feedback reduces regret by improving decision-making in high-uncertainty rounds.

- Trade-off: The second term reflects the cost-benefit trade-off of feedback. With frequent and accurate feedback (\(p \to 1\) and \(q_t \to 1\)), the regret decreases significantly.

- Entropy Threshold: The choice of \(\lambda\) (affecting \(p\)) allows control over feedback frequency, balancing feedback cost and regret reduction.