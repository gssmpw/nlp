\subsection{Regret Analysis for Contextual Bandits with Entropy-Based Human Feedback}

We analyze the regret bound for our proposed algorithm, which integrates entropy-based human feedback in a contextual bandit setting. The goal is to quantify the impact of selective oracle feedback on cumulative regret and derive a regret bound that captures the trade-offs.

At each round $t$, the agent observes context $s_t$ and selects an action $a_t \in \mathcal{A}$ using policy $\pi_t$. Oracle feedback is requested if the entropy $H(\pi_t)$ exceeds a threshold $\lambda$, and the observed reward $r_t(a_t)$ combines environment and feedback contributions.

The regret at time $t$ is:

\begin{equation}
\text{Regret}_t = \mathbb{E}[r_t(a_t^{*}) - r_t(a_t)],
\end{equation}

where $a^t = \pi^*(s_t)$ is the optimal action. The total regret over $T$ rounds is:
\begin{equation}
\text{Regret}(T) = \sum_{t=1}^T \text{Regret}_t.
\end{equation}



Let $p = P(H(\pi_t) > \lambda)$ denote the probability of requesting feedback, and let $q_t$ be the accuracy of oracle feedback. The regret bound is:

\begin{equation}
\begin{aligned}
\mathbb{E}[\text{Regret}(T)] \leq  
&\ O\left(\sqrt{(1 - p)T |\mathcal{A}| \log T} \right) \\
&+ O\left( \frac{p T (1 - q)}{1 - q + \log T} \right).
\end{aligned}
\end{equation}



The total regret decomposes into two parts:
\begin{equation}
\text{Regret}(T) =  \sum_{t \notin \mathcal{F}} \text{Regret}_t + \sum_{t \in \mathcal{F}} \text{Regret}_t 
\end{equation}
where $\mathcal{F}$ represents rounds where feedback is requested ($H(\pi_t) > \lambda$).

For rounds without feedback, the regret follows standard contextual bandit analysis:
\begin{equation}
\mathbb{E}[\text{Regret}_{\text{no-feedback}}(T)] \leq O\left(\sqrt{(1 - p)T |\mathcal{A}| \log T}\right).
\end{equation}

For rounds with feedback, regret reduction depends on feedback accuracy and its impact on decision quality:
\begin{equation}
\mathbb{E}[\text{Regret}_{\text{feedback}}(T)] \leq O\left( \frac{p T (1 - q)}{1 - q + \log T} \right).
\end{equation}


This regret bound highlights the trade-off between exploration and feedback solicitation. Increasing $p$ reduces the first term, leading to faster convergence, while higher feedback accuracy $q$ ensures minimal regret in feedback rounds. The entropy threshold $\lambda$ serves as a control parameter to balance feedback frequency and regret minimization. Compared to standard bandit approaches, entropy-driven feedback solicitation provides a principled mechanism to reduce regret in uncertain environments, making it highly effective for practical deployment.