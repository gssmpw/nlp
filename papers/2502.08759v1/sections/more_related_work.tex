\textbf{Other related areas}
Our work builds on several important research areas, including counterfactual reasoning, imitation learning, preference optimization, and entropy-based active learning. We draw inspiration from Tang and Wiens \cite{tang2023counterfactual}, whose counterfactual-augmented importance sampling informs our feedback framework, and extend DAGGER \cite{ross2011reduction} by dynamically incorporating expert feedback instead of using fixed imitation. We also acknowledge parallels with Active Preference Optimization (APO) \cite{das2024active}, adapting trajectory-level preference feedback to reward manipulation in more complex settings. Additionally, we connect with entropy-driven methods like BALD \cite{houlsby2011bayesian} and IDS \cite{russo2014learning}, adapting their principles for contextual bandit problems to balance information gain and decision-making efficiency in sequential exploration. These connections highlight how our approach advances real-time feedback integration and decision optimization.
