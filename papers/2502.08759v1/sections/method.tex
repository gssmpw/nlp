\section{Method}
The following section provides a description of our method and its subcomponents. A comprehensive representation of the approach is shown in Figure~\ref{fig:overview}. Algorithm~\ref{alg:example} describes our method.

\subsection{Contextual bandit formulation}
We consider an online stochastic contextual bandit framework where at each round $t$, the world generates a context-reward pair $(s_t,r_t)$ sampled independently from a stationary unknown distribution $\mathcal D$. Here $s_t\in \mathcal S= \mathbb{R}^m$ is an $m$ dimensional real valued vector and $r_t=(r_t(1),\dots,r_t(k))\in \{0,1\}^k$ is a $k$-dimensional vector where each element can take values $0$ or $1$. The agent then chooses an action $a_t\in\{1,\dots,k\}$ %\lili{why not use $a$ for action?} 
according to a policy $\pi:\mathcal{S}\mapsto \{1,\dots,k\}$ and the environment reveals the reward $r_t(a_t)\in\{0,1\}$. 


The objective of the agent is to find a policy $\pi\in \Pi$ that will minimize the cumulative expected regret given by 
% \begin{equation}
%     \max_{a_t\sim \pi}\sum_{t=1}^T\mathbb{E}\bigl[r_t(a_t)\mid s_t,a_t\bigr]
% \end{equation}
\begin{equation}
    \mathbb{E}\Bigl[\sum_{t=1}^T\bigl(r_t(\pi^*(s_t)-r_t(a_t)\bigr)\Bigr],
\end{equation}
where $\pi^*$ denotes the optimal strategy of selecting actions given context $s_t$.
The problem setup described above bears a strong resemblance to a multi-label or multiclass classification problem, where $r_t(a_t)=1$ indicates the correct label choice and $0$ otherwise. However, a key distinction lies in the learner's lack of access to the correct label or label set for each observation. Instead, the learner only discerns whether the chosen label for an observation is correct or incorrect. 
% \lili{how could lack of access to correct label but discerns the chosen label is correct??? Is the access the correct label the same with discern the chosen label is correct??: Need to rephrase the above to make it more clear and add an example} \lili{decouple method and experiments, you don't have to mention implementation here} 
%As a result, we can often use standard binary or multi-class classification dataset in the contextual bandit setting with the features being the contexts. We discuss more about this in detail in Section~\ref{sec:experiments}. 
% The obejctive of the agent is to find a a policy $\pi$ that maximizes  minimize cumulative expected regret defined as:

% \begin{equation}
%     \text{Regret} = \mathbb{E}\left[\sum_{t=1}^T\Bigl( r_t\left(\pi^*(s_t)\right) - r_t(a_t)\Bigr)\right],
% \end{equation}
% \lili{This equation is wrong? add parenthesis inside?}

% \lili{
% \begin{equation}
%     \text{Regret} = \mathbb{E}\left[\sum_{t=1}^T (r_t\left(\pi^*(s_t)\right) - r_t(a_t))\right],
% \end{equation}}
% where \( \pi^*(s_t) = \text{argmax}_{\pi \in \Pi} \mathbb{E}_{(s, r) \sim \mathcal{D}}[r(\pi(s))] \) represents the optimal policy in the set \( \Pi \) of large (possibly infinite) policies, and \( \pi: \mathcal{S} \mapsto \{1, \ldots, K\} \). For our analysis we consider maximizing the expected total reward which is equivalent to minimizing regret. 

 \begin{figure*}[htbp]
    \centering
  
     \includegraphics[width=.65\linewidth]{figures/CBHF_v2.png}
    \caption{Overview of the architecture. Our framework builds upon a standard contextual bandit setup. The right side of the figure illustrates the human feedback incorporation mechanism, which can be integrated through either \textit{reward manipulation} (directly modifying the bandit's reward signal) or \textit{action recommendation}, which constrains the set of available actions. Rather than triggering human feedback at fixed intervals, we propose an adaptive approach: querying feedback only when the action policy (Eq.~\ref{eq:policy_entropy}) exceeds a predefined uncertainty threshold. This strategy ensures feedback is solicited when it is most valuable, improving efficiency and decision-making.}
    \label{fig:overview}
   
\end{figure*}
\subsection{Incorporating entropy based human feedback}

In contextual bandits, feedbacks are provided in the form of a predetermined reward signal provided by the designer. These reward signals are not well defined for complex decision-making problems~\citep{blanchard2023adversarial,dragone2019deriving}, and are often learned from data.  An alternative to learning a reward function from data is to obtain preference-based feedback from humans and learn the underlying reward function that the human expert optimizes~\citep{sekhari2024contextual}. In this work, we consider the setup in which the human expert has sufficient expertise and valuable insights stemming from their experience and domain knowledge to provide direct feedback to the learner. These feedbacks can directly impact the actions that a contextual bandit learner takes or the rewards it receives. However, the quality of such explicit feedback may vary depending on the expertise levels of different individuals.
 We provide two ways in which human experts can provide feedback to the contextual bandit learner: i) Action Recommendation through direct supervision (AR) ii) Reward Manipulation (RM). In certain applications, a human expert can directly control the actions that the agent takes; in these cases, feedback in the form of action recommendations (AR) is useful. Conversely, in other applications where the human expert cannot directly influence the agent's actions, feedback through reward manipulation is more beneficial. We describe each of these different feedbacks below. 
 \begin{algorithm}
\caption{Enropy Based - CBHF}
\label{alg:example}
\begin{algorithmic}[1]
\REQUIRE Input parameters: entropy threshold $(\lambda)$, feedback-type $(fb)$, round-number $(n)$, contextual bandit agent $(\mathcal{A})$, human expert quality $(q_t)$
\ENSURE Output: \emph{mean cumulative regret}
\STATE Initialize \emph{mean cumulative regret}$\gets 0$
\FOR{$t = 1$ to $n$}
    \STATE Get context, reward vector $(s_t,r_t)\gets\omega$
    \STATE Get actions and action distribution from the learner ($a_t,\pi(s_t)) \gets \mathcal{A}(s_t)$
    \STATE Compute $H(\pi(s_t))$
    \IF{$H(\pi(s_t))>\lambda$}
    \IF{$fb$ == \emph{AR}}
    \STATE $\hat a\gets\mathcal{E}(s_t,q_t)$
    \STATE $a_t\gets \hat a$
    \STATE $r\gets r_t(a_t)$
    \ELSIF{$fb$== \emph{RM}}
    \STATE $r_p \gets\mathcal E(s_t,q_t)$
    \STATE $r\gets r_t(a_t)+r_p$
    % \ELSE 
    % \STATE Raise Error: Feedback type not supported
    \ENDIF
    \ELSE
    \STATE $r\gets r_t(a_t)$
    \ENDIF
    \STATE Update Agent $\mathcal A$ policy $\pi$ with feedback $r$
    \STATE \emph{mean cumulative regret}$\gets$ evaluate agent $\mathcal A$
\ENDFOR
\STATE Return \emph{mean cumulative regret}
\end{algorithmic}
\end{algorithm}

\subsubsection{Action recommendation via direct supervision}
\label{sec:action_recommendation}


In this form of feedback, the human expert explicitly instructs the actions to take for a given context. We assume that the algorithm always accepts the recommended action. Let $\hat{a}_t$ be a set of actions recommended by the human expert $\mathcal{E}^{\mathrm{AR}}$ for a given context $s_t$ and expert quality $q_t$, where $q_t\in [0,1]$, we elaborate more on the expert quality in Section~\ref{subsec:exp_quality}.  When the expert recommends a set of actions, the learning algorithm randomly chooses an action from the recommended set. 
The final reward $r_t^f$ received by the learner is given by: 
\begin{align}
    &\hat{a}_t = \mathcal{E}^{\mathrm{AR}}(s_t,q_t) \\
    &a_t\sim\mathrm{Uniform}(\hat a_t)\\
    &r_t^f = r_t(a_t)
\end{align}

\subsubsection{Reward manipulation}

\label{sec:reward_penalty}
In this form of feedback, the human expert $\mathcal E^{\mathrm{RM}}$ gives an additional reward penalty when the learner chooses an action not recommended by the expert. Let $r_p$ be the fixed reward penalty for unrecommended actions. Let $a_t$ be the action chosen by the learner in round $t$, and $\hat{a}_t$ be the set of recommended actions of the expert. The final reward $r_t^f$ received by the learner is given by:

\begin{align}
    &r_p=\mathcal{E}^{\mathrm{RM}}(s_t,q_t)\\
    &r_t^f = \begin{cases}
        r_t(a_t) + r_p & \text{if } a_t \notin \hat{a}_t \\
        r_t(a_t) & \text{otherwise}
    \end{cases}
\end{align}


\subsection{When to seek human feedback?}
\label{subsec:when_to_seek_hf}
An important question that naturally arises when integrating human feedback into the contextual bandit algorithm is when the algorithm will actively seek out such feedback. In the contextual duelling bandit setup in~\citep{di2024nearly}, the algorithm presents two options to the human and asks them to choose a preferred one based on a given context. In the case of model misspecification, where the underlying reward function assumed by the algorithm matches the true rewards generated by human preferences, the algorithm can actively query the human expert to obtain feedback on the predicted reward or ranking~\citep{yang2023contextual}. In our work, we take a different approach in which the learner seeks expert feedback based on the uncertainty of the model. The model computes the entropy of the policy at each round $t$ which quantifies the degree of unpredictability in the policy's decision making process using the following expression 

\begin{equation}
H(\pi)=-\sum_{a_t}\pi(a_t\mid s_t)\log(\pi(a_t\mid s_t)),
\label{eq:policy_entropy}
\end{equation}
%\lili{add equation number}
where $H(\pi)$ denotes the entropy of policy $\pi$. The model then queries for human feedback when the model entropy exceeds a predefined threshold $\lambda$. Appropriate choice of $\lambda$ will depend on the problem domain and are obtained using hyper parameter search. Our proposed entropy based approach for querying the expert depends on the learner's ability to compute an entropy for its policy. Thus for certain models when model uncertainty is not available, we can still obtain two forms of human feedback periodically, we also demonstrate the effect on model performance when these two types of human feedback are incorporated for different periods. 
\subsection{Quality of experts}
\label{subsec:exp_quality}
We consider the effect of learner's performance based on different quality of expert feedback received. We define the quality of feedback in this case as the accuracy of the expert in providing correct recommendation. We first show how the performance of the contextual bandit learner measured by the expected cumulative regret varies for different expert levels of accuracy. Let $q_t\in [0,1]$ be the probability of providing correct recommendation associated with a particular level of expert. During training, the algorithm seeks expert feedback described in Section~\ref{sec:action_recommendation} and~\ref{sec:reward_penalty} when $H(\pi)\geq \lambda$. For action recommendation via direct supervision, the expert provides the correct action with probability $q_t$ and provides a randomized action with probability $1-q_t$. For reward manipulation feedback, the expert wrongly penalizes the learner with a probability of $1-q_t$. 

\input{sections/refined_regret_bound}




\bigskip



