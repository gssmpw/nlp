
\section{Method Improvement: Hybrid Feedback Mechanism}

We consider a contextual bandit setting where an agent interacts with an environment over a series of rounds \(t = 1, 2, \dots, T\). At each round \(t\):
1. A user \(u_t \in \mathcal{U}\) and a set of items \(\mathcal{A}_t = \{a_1, a_2, \dots, a_k\}\) are presented.
2. The agent selects an action \(a_t \in \mathcal{A}_t\) based on the contextual features of the user \(\mathbf{x}_{u_t}\) and items \(\mathbf{x}_{a_t}\).
3. A reward \(r_t\) is observed, reflecting the userâ€™s interaction with the selected item. The goal is to maximize the cumulative reward \(\sum_{t=1}^T r_t\).

In this framework, human feedback can be leveraged to enhance learning efficiency by providing additional information about the user's preferences.
\subsection{Hybrid Feedback Mechanism}

We introduce a \textbf{Hybrid Feedback Mechanism} that dynamically combines two types of human feedback:

- \textbf{Action Recommendation (AR)}: The human expert suggests the most suitable action when the model's uncertainty is high.

- \textbf{Reward Manipulation (RM)}: The human expert modifies the reward signal to correct the agent's evaluation of the selected action in low-uncertainty settings.

\subsection{Entropy-Based Decision for Feedback}

To determine when to request feedback and which type to use, we utilize the \textbf{entropy} of the predicted action probabilities:
\[
H(\mathbf{p}) = - \sum_{i=1}^{k} p(a_i) \log p(a_i)
\]
Where \(p(a_i)\) is the probability of selecting action \(a_i\).

\textbf{High Entropy} (\(H(\mathbf{p}) > \tau_t\)): The model is uncertain, and AR is used.

\textbf{Low Entropy} (\(H(\mathbf{p}) \leq \tau_t\)): The model is confident, and RM is applied to fine-tune reward estimates.

The threshold \(\tau_t\) evolves dynamically during training:
\[
\tau_t = \tau_{\text{init}} + \frac{t}{T} (\tau_{\text{max}} - \tau_{\text{init}})
\]
where \(\tau_{\text{init}}\) is the initial threshold, and \(\tau_{\text{max}}\) is the maximum threshold.

We propose \textbf{HybridLinearBandit}, which integrates AR and RM dynamically. The model consists of:

\textbf{Linear Model}: Parameterized by \(\theta\), updated using gradient-based learning.

\textbf{Entropy Threshold Adjustment}: Adaptively controls feedback type and frequency.



\[
\begin{array}{ll}
\hline
\multicolumn{2}{l}{\textbf{Algorithm 2: HybridLinearBandit with Dynamic Feedback}} \\
\hline
\textbf{Input:} & \text{Context } \mathbf{x}_t \in \mathbb{R}^d \text{ at round } t, \text{ action space } \mathcal{A}, \text{ total rounds } T, \\
& \text{initial entropy threshold } \tau_{\text{init}}, \text{ maximum threshold } \tau_{\text{max}}, \\
& \text{learning rate } \alpha, \text{ regularization parameter } \lambda. \\
\hline
\textbf{Initialize:} & \theta = \mathbf{0} \text{ (parameter vector)}, \tau_1 = \tau_{\text{init}} \\
\textbf{For each round } t = 1, \dots, T: & \\
& \textbf{Action Selection:} \\
& \quad \mathbf{s}_t = \{\mathbf{x}_{t,a}^\top \theta \,|\, a \in \mathcal{A}\} \quad (\text{compute scores for each action}) \\
& \quad \mathbf{p}_t = \text{Softmax}(\mathbf{s}_t) \quad (\text{compute action probabilities}) \\
& \quad a_t = \arg\max \mathbf{p}_t \quad (\text{select action with max probability}) \\
\\
& \textbf{Compute Entropy:} \\
& \quad H(\mathbf{p}_t) = -\sum_{a \in \mathcal{A}} p_t(a) \log p_t(a) \quad (\text{calculate entropy of action probabilities}) \\
\\
& \textbf{Request Feedback:} \\
& \quad \textbf{If } H(\mathbf{p}_t) > \tau_t: \\
& \quad \quad \text{Request Action Recommendation (AR)} \\
& \quad \textbf{Else:} \\
& \quad \quad \text{Request Reward Manipulation (RM)} \\
\\
& \textbf{Observe Feedback:} \\
& \quad \text{Receive feedback } r_t \text{ based on selected feedback type} \\
\\
& \textbf{Update Model:} \\
& \quad \theta \leftarrow \theta + \alpha \cdot (r_t - \mathbf{x}_{t,a_t}^\top \theta) \cdot \mathbf{x}_{t,a_t} - \lambda \theta \quad (\text{gradient update with regularization}) \\
\\
& \textbf{Adjust Entropy Threshold:} \\
& \quad \tau_t \leftarrow \tau_{\text{init}} + \frac{t}{T} (\tau_{\text{max}} - \tau_{\text{init}}) \quad (\text{dynamic adjustment}) \\
\\
\textbf{Output:} & \text{Trained parameter vector } \theta. \\
\hline
\end{array}
\]

Here is a detailed explanation for the algorithm:

1. \textbf{Action Selection}:
   - The agent uses the current model \(\theta\) to compute a score \(\mathbf{s}_t\) for each action \(a \in \mathcal{A}\) based on the context \(\mathbf{x}_t\).
   - Action probabilities \(\mathbf{p}_t\) are derived using the softmax function, balancing exploration and exploitation.

2. \textbf{Entropy Calculation}:
   - Entropy \(H(\mathbf{p}_t)\) quantifies the uncertainty in the action probabilities.

3. \textbf{Dynamic Feedback}:
   - When uncertainty (entropy) is high, request \textbf{AR} to refine action selection.
   - When uncertainty is low, use \textbf{RM} to fine-tune the reward model.

4. \textbf{Model Update}:
   - Perform gradient-based updates with regularization to minimize the loss based on the received feedback.

5. \textbf{Threshold Adjustment}:
   - The entropy threshold \(\tau_t\) dynamically increases as the agent becomes more confident, reducing feedback frequency over time.
   
\subsection{Dynamic Feedback Frequency}

The proposed mechanism dynamically adjusts feedback frequency: (i) \textbf{Early Rounds}: Frequent feedback (\(H(\mathbf{p}) > \tau_t\)) to explore the action space. (ii) \textbf{Later Rounds}: Reduced feedback as the model gains confidence, focusing on exploitation and fine-tuning.

This hybrid approach ensures efficient use of human feedback while minimizing cognitive load.
