\section{Experiments}
\label{sec:experiments}
% In this section, we present the environment settings, baselines and the experimental results. We also discuss the effect of entropy threshold and expert accuracy on model performance.

% \subsection{Algorithms and environments considered}
% We conduct experiments across a range of \emph{environments} and \emph{contextual bandit agents}. The agents fall into two categories: (i) classic contextual bandit algorithms, and (ii) policy-based reinforcement learning (RL) algorithms with a discount factor of $0$, focusing on immediate rewards.

% \subsubsection{Classic Contextual Bandit Algorithms}
% In the classic contextual bandit setup, we employ the Linear Upper Confidence Bound (LinearUCB) algorithm~\citep{li2010contextual}, which extends the traditional Upper Confidence Bound (UCB) algorithm~\citep{auer2002using} to situations where each action’s expected reward depends linearly on the context or features associated with that action. Next, we utilize Bootstrapped Thompson Sampling~\citep{kaptein2014thompson}, which replaces the posterior distribution in standard Thompson Sampling with a bootstrapped distribution. This approach enhances robustness and scalability by resampling historical data rather than relying on a parametric model. Lastly, we apply the Exploration-Exploitation Neural Network (EE-NET)~\citep{ban2021ee}, which employs two neural networks—one for exploration and one for exploitation—to learn a reward function and adaptively balance potential gains against the current reward estimates.

% \subsubsection{Policy-Based Reinforcement Learning Algorithms}
% We experiment with four popular policy-based RL algorithms, setting the discount factor to $0$ to optimize for immediate rewards. The algorithms include Proximal Policy Optimization (PPO)~\citep{schulman2017proximal}, PPO with Long Short-Term Memory (PPO-LSTM), REINFORCE~\citep{williams1992simple}, and Actor-Critic~\citep{pmlr-v80-haarnoja18b}.

% \subsubsection{Baseline Comparison}
% For a reasonable baseline, we incorporate the TAMER framework~\citep{knox2009interactively}, which allows human trainers to provide real-time feedback to an agent, supplementing the predefined environmental reward signal. In our setup, we simulate human feedback within the TAMER framework by revealing the true labels for each dataset during training.

% \subsubsection{Expert Feedback Comparison}
% For all the contextual bandit agents discussed, we compare two types of expert feedback, as described in sections~\ref{sec:action_recommendation} and~\ref{sec:reward_penalty}. Notably, expert feedback is only solicited during the training phase. We evaluate each learner after training over 5 independent runs and report the mean cumulative reward.

% \subsubsection{Datasets}
% We utilize the multi-label datasets: Bibtex, Media Mill, and Delicious from the Extreme Classification Repository~\citep{Bhatia16}. Since these are multi-label datasets, the reward function for these supervised learning datasets in the contextual bandit framework is defined as:
% \begin{align}
% r_t(a_t) = \begin{cases}
%     1\quad \text{if } a_t\in y_t\\
%     0 \quad \text{otherwise}
% \end{cases}
% \end{align}
% where $y_t$ represents the set of correct labels associated with context $s_t$ for each dataset. These datasets are chosen due to their large size, complexity, and diversity, making them a robust setting to evaluate contextual bandits with human feedback.


% \subsection{Implementation Details}
% We consider a range of entropy thresholds for different datasets. We treat these entropy thresholds as hyperparameters that controls how frequently the algorithm seeks to incorporate for human feedback. Detail on the range of entropy thresholds for different datasets are presented in Appendix~\ref{subsec:entropy_range}.
% % We consider the following sets of entropy thresholds for each of these environments. 

% % \begin{table}[htbp]
% %     \centering
% %     \caption{Entropy thresholds for different environments $\lambda$}
% %     \vspace{+2mm}
% %     \begin{tabular}{lc}
% %         \toprule
% %         \textbf{Item} & \textbf{$\lambda$ values} \\
% %         \midrule
% %         Bibtex & $2.5$, $3.5$, $5.0$, $6.5$, $9.0$ \\
% %         Media Mill & $1.5$, $2.5$, $3.0$, $4.5$, $7.0$ \\
% %         Delicious & $1.5$, $2.5$, $4.5$, $6.5$, $9.0$ \\
% %         Yahoo & $1.5$, $2.5$, $4.5$, $7.0$, $9.0$ \\
% %         \bottomrule
% %     \end{tabular}
% %     \label{tab:lambda_values}
% % \end{table}
%  We select the optimal value of the entropy thresholds and report mean cumulative reward associated with two modes of obtaining feedback from human experts. Our code base for the policy based RL algorithms are based on pytorch and adapted from~\cite{minimalRL}, for LinearUCB and Bootstrapped-thompson sampling we adapted our implementation from~\cite{cortes2019adapting}.The hyperparameters for policy based RL algorithms are provided in Appendix~\ref{sec:app_hyperparams_rl}. As mentioned in Section~\ref{subsec:when_to_seek_hf}, we vary the expert quality based on a range of values of $q_t\in[0,1]$ where with probability $q_t$, the correct label or set of labels associated with context $s_t$ is provided to the contextual bandit learner. 

\subsection{Experimental Setup}
In this sub-section, we present the environment settings, baselines, and experimental results. We also discuss the effect of entropy thresholds and expert accuracy on model performance.

\textbf{Algorithms and Environments Considered.}  
We conduct experiments across a range of \emph{environments} and \emph{contextual bandit agents}. The agents fall into two categories: (i) classic contextual bandit algorithms and (ii) policy-based reinforcement learning (RL) algorithms with a discount factor of $0$, focusing on immediate rewards.

\textbf{Classic Contextual Bandit Algorithms.}  
For the classic contextual bandit setup, we employ three key algorithms:  
1. \textbf{LinearUCB}~\citep{li2010contextual}: An extension of the traditional Upper Confidence Bound (UCB) algorithm~\citep{auer2002using}, where the expected reward for each action depends linearly on the context or features associated with that action.  
2. \textbf{Bootstrapped Thompson Sampling}~\citep{kaptein2014thompson}: This method replaces the posterior distribution in standard Thompson Sampling with a bootstrapped distribution, enhancing robustness by resampling historical data instead of relying on a parametric model.  
3. \textbf{EE-NET}~\citep{ban2021ee}: This approach utilizes two neural networks—one for exploration and one for exploitation—to learn a reward function and adaptively balance exploration with exploitation.

\textbf{Policy-Based Reinforcement Learning Algorithms.}  
For policy-based RL, we evaluate four algorithms, with the discount factor set to $0$ to prioritize immediate rewards:  
\textbf{Proximal Policy Optimization (PPO)}~\citep{schulman2017proximal},
\textbf{PPO with Long Short-Term Memory (PPO-LSTM)}, \textbf{REINFORCE}~\citep{williams1992simple}, \textbf{Actor-Critic}~\citep{pmlr-v80-haarnoja18b}.

\textbf{Baseline Comparison.}  
We include the \textbf{TAMER framework}~\citep{knox2009interactively} as a baseline, which allows human trainers to provide real-time feedback to the agent, supplementing the predefined environmental reward signal. In our experiments, we simulate human feedback by revealing the true labels during training.

\textbf{Expert Feedback Comparison.}  
For all contextual bandit agents, we compare two types of expert feedback as described in sections~\ref{sec:action_recommendation} and~\ref{sec:reward_penalty}. Expert feedback is solicited only during the training phase, and each learner is evaluated after five independent runs, with the mean cumulative reward reported.

\textbf{Datasets.}  
We use multi-label datasets from the Extreme Classification Repository, including Bibtex, Media Mill, and Delicious~\citep{Bhatia16}. In the contextual bandit framework, the reward function for these supervised learning datasets is defined as:
\begin{align}
r_t(a_t) = \begin{cases}
    1\quad \text{if } a_t \in y_t \\
    0 \quad \text{otherwise}
\end{cases}
\end{align}
where $y_t$ represents the set of correct labels associated with context $s_t$. These datasets are selected for their size, complexity, and diversity, making them suitable for evaluating contextual bandits with human feedback.

\textbf{Implementation Details.}  
We consider a range of entropy thresholds as hyperparameters, controlling how frequently the algorithm seeks to incorporate human feedback. The specific ranges for different datasets are detailed in Appendix~\ref{subsec:entropy_range}. We select the optimal entropy threshold and report the mean cumulative reward for each mode of human expert feedback. The code base for policy-based RL algorithms is implemented in PyTorch, adapted from~\citep{minimalRL}, while the LinearUCB and Bootstrapped Thompson Sampling implementations are adapted from~\citep{cortes2019adapting}. The hyperparameters for the RL algorithms are provided in Appendix~\ref{sec:app_hyperparams_rl}. Additionally, expert quality is varied based on values of $q_t \in [0, 1]$, where with probability $q_t$, the correct label or set of labels associated with context $s_t$ is provided to the learner, as mentioned in Section~\ref{subsec:when_to_seek_hf}.








