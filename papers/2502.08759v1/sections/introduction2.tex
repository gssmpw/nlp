% \section{Introduction}
% \label{sec:Introduction}

% Contextual bandits (CB) have emerged as a powerful framework across various applications, including recommendation systems~\citep{li2010contextual, xu2020contextual}, healthcare~\citep{yu2024careforme}, and finance~\citep{zhu2021online}, among others~\citep{bouneffouf2020survey}. CBs enable personalized decision-making by learning from the contextual information in each instance. However, current systems often rely heavily on implicit feedback signals, such as clicks, which are inherently biased and incomplete, limiting their ability to fully capture true user preferences~\citep{qi2018bandit}.

% To address these challenges, we explore the incorporation of explicit human feedback in a CB setting. Human feedback has shown promise in reinforcement learning by integrating human guidance into the learning process~\citep{christiano2017deep,macglashan2017interactive}. Incorporating human feedback enables models to generate more accurate and informative responses, improving performance in applications such as conversational AI like ChatGPT~\citep{ouyang2022training, achiam2023gpt}, and robotics~\citep{osa2018algorithmic}.

% Human feedback can generally be categorized into action-based feedback from human experts~\citep{osa2018algorithmic, li2023reinforcement}, and preference-based feedback~\citep{christiano2017deep, saha2023dueling}. This work focuses on the latter. Preference-based feedback, where humans indicate their preference between two options selected by the learner, has gained popularity due to its simplicity. However, existing methods fail to address two critical issues: the varying quality of human feedback and the uncertainty in the model’s decisions. These factors often result in inefficient learning and suboptimal performance, especially in high-stakes or complex environments. In this work, we aim to answer the key question: \textbf{Can we propose a simple yet effective strategy to incorporate preference-based human feedback in contextual bandits?}

% A key challenge in CB problems is balancing exploration and exploitation, which becomes more complex with the addition of human feedback. The algorithm must balance this input to avoid over-reliance while ensuring efficient learning. To address this, we propose a simple criterion for feedback solicitation and introduce two methods for incorporating human feedback into CB, evaluating their performance.

% We present two feedback settings. In the action recommendation (AR) method, a human expert provides recommended actions for a given context. In the reward manipulation (RM) method, the expert assigns a reward penalty when the learner selects an action not recommended by the expert. Feedback solicitation is based on model uncertainty, quantified by policy entropy, and human feedback is requested when model entropy exceeds a certain threshold.

% These additions underscore the key finding of our study: \emph{even low-quality human feedback, when appropriately solicited, can lead to significant performance improvements}.

% Our contributions are threefold. First, we propose a framework to integrate human feedback into CB across different environments and analyze the relative performance of two feedback strategies: \emph{action recommendation} and \emph{reward penalty}. Second, we identify limitations in current approaches and introduce an entropy-based criterion to enhance learning. This criterion not only improves performance but also deepens our understanding of how these methods support learning. Finally, we evaluate the impact of expert feedback quality on CB learner performance, showing how varying levels of human recommendation accuracy affect cumulative rewards. Our findings advocate for the inclusion of our methods in decision-making models and expand the understanding of human feedback integration in reinforcement learning.


\section{Introduction}
\label{sec:introduction }

Contextual bandits (CB) have become a fundamental framework for personalized decision-making across diverse domains including recommendation systems~\citep{li2010contextual,bouneffouf2020survey}, healthcare~\citep{yu2024careforme}, and finance~\citep{zhu2021online}. While traditional CB approaches leverage contextual information to optimize actions, their heavy reliance on implicit feedback signals like clicks introduces inherent limitations due to the biased and incomplete nature of such data~\citep{qi2018bandit}.

This work investigates how explicit human feedback can enhance CB performance. Building on successful integrations of human guidance in reinforcement learning~\citep{christiano2017deep,macglashan2017interactive} and conversational AI~\citep{achiam2023gpt}, we distinguish two primary feedback paradigms: (1) \emph{action-based feedback}, where experts directly prescribe optimal actions for specific contexts~\citep{osa2018algorithmic,li2023reinforcement}, and (2) \emph{preference-based feedback}, where humans compare pairs of learner-generated actions to express relative preferences~\citep{christiano2017deep,saha2023dueling}. While action-based methods require precise expert knowledge, we focus on preference feedback for its practical advantages in scalable data collection, notably its reduced cognitive load on human evaluators. However, this operational simplicity introduces two critical challenges: (1) variable human feedback quality, and (2) model uncertainty propagation. Our central research question emerges: \textbf{How can we effectively incorporate preference-based human feedback into contextual bandits while addressing these fundamental challenges?}

To address this, we propose an entropy-based feedback mechanism that selectively queries an oracle when the agent’s policy exhibits high uncertainty. By dynamically adjusting feedback requests based on entropy, our approach effectively balances exploration and exploitation, reducing unnecessary queries while ensuring informative guidance when needed. This selective feedback mechanism allows the agent to refine its policy more efficiently, leading to tighter regret bounds and improved performance. Furthermore, we introduce two complementary feedback integration strategies: \textit{Action Recommendation} (AR), where experts suggest context-specific actions, and \textit{Reward Manipulation} (RM), where penalties adjust the reward signal for non-recommended actions. By optimizing the timing of human input through adaptive entropy-based solicitation, our framework enhances learning efficiency while mitigating the impact of imperfect human feedback.

Our key contributions include:

\begin{itemize}
\item A unified framework for human-CB collaboration with theoretical analysis comparing AR and RM strategies.

\item An entropy-based solicitation criterion that improves learning efficiency while providing insights into human-algorithm interaction dynamics.

\item Empirical characterization of feedback quality impacts, revealing performance robustness across expert competence levels.

\end{itemize}

Our findings advance both practical CB implementations and theoretical understanding of human feedback integration. By establishing guidelines for effective human-AI collaboration in bandit settings, this work bridges critical gaps between algorithmic decision-making and human expertise.