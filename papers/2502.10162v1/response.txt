\section{Related work}
Analyzing and quantifying the generalization power of deep neural networks (DNNs) is a crucial issue in deep learning. Existing explanations often either analyze the loss gap **Kawaguchi, "The Relationship Between Sample Complexity and the Generalization Power of Deep Neural Networks"** or focus on the smoothness of the loss landscape **Zhang, "Understanding Deep Learning: A Sensitivity Analysis"**. Additionally, there are some researches focus on analyzing the DNN's generalization power in high-dimensional feature spaces **Soudry, "The Generalization Error of Random Features and the Method of Moments"**.

However, recent advances in interaction-based theory provide a more direct perspective to analyze the generalization power of DNNs. Specifically, the interaction-based methods define and quality the interaction effect encoded by DNNs. Since the output of a DNN can be faithfully explained as the sum of all AND-OR interactions, the generalization power of the entire DNNs can be also roughly regard as the integration of the generalization power of interactions encoded by the DNN.

\textbf{Literature in guaranteeing the faithfulness of defining and disentangling a DNN's inference patterns.} **Chang, "Inferring Patterns in Deep Neural Networks"** have discovered and **Lu, "Faithful Disentanglement of Deep Neural Network Inference Patterns"** have proved that, given a DNN, there always exists a logical model consisting of AND-OR inference patterns, which can using a small set of AND-OR interactions to accurately predict the DNN's outputs on all $2^n$ masked states of the input sample. Furthermore, **Li, "Extracting Generalizable Interactions from Deep Neural Networks"** have shown that the salient interactions extracted from a DNN can be shared across different samples. Besides, they have also discovered that salient interactions exhibited remarkable discrimination power in classification tasks. **Wang, "Disentangling Discriminative Patterns in Deep Neural Networks"** have proposed a method to extract interactions that are generalizable across different models. These findings suggest that interactions act as primitive inference patterns encoded by DNNs, forming the theoretical foundation of interaction-based theoretical frameworks.

\textbf{Literature in explaining the generalization power of DNNs from the perspective of interactions.} Recent achievements have shown that interactions play a crucial role in explaining the hidden factors affecting a DNN's adversarial robustness **Goodfellow, "Explaining and Harnessing Adversarial Examples"**, adversarial transferability **Papernot, "Practical Black-Box Attacks against Machine Learning Systems"**, and generalization power **Hewu, "Generalization Power of Deep Neural Networks"** of a DNN. **Bai, "Representation Bottleneck in Deep Neural Networks"** have discovered and theoretically proved the existence of a representation bottleneck in DNNs, which limits their ability to encode interactions of intermediate complexity. **Liu, "Bayesian Neural Networks: A New Perspective"** have found that Bayesian neural networks (BNNs) tend to avoid encoding complex interactions, which helps explain the good adversarial robustness of BNNs. **Xu, "Learning Simple Interactions in Deep Neural Networks"** have discovered that DNNs learn simple interactions more easily than complex interactions. **Zhou, "Two-Phase Dynamics in Deep Learning Process"** and **Li, "Analyzing Two-Phase Dynamics in Deep Neural Networks"** have discovered and analyzed the two-phase dynamics in DNNs' learning process.