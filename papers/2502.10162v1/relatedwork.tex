\section{Related work}
Analyzing and quantifying the generalization power of deep neural networks (DNNs) is a crucial issue in deep learning. Existing explanations often either analyze the loss gap~\cite{generalization_bounds, bousquet2020sharper, deng2021toward, haghifam2020sharpened, haghifam2021towards} or focus on the smoothness of the loss landscape~\cite{flat_minima, loss_lanscape, foret2021sharpness, kwon2021asam}. Additionally, there are some researches focus on analyzing the DNN's generalization power in high-dimensional feature spaces~\citep{petrini2022learning, boopathy2023model, dyballa2024separability, nikolikj2024generalization}.

However, recent advances in interaction-based theory provide a more direct perspective to analyze the generalization power of DNNs. Specifically, the interaction-based methods define and quality the interaction effect encoded by DNNs. Since the output of a DNN can be faithfully explained as the sum of all AND-OR interactions, the generalization power of the entire DNNs can be also roughly regard as the integration of the generalization power of interactions encoded by the DNN.

\textbf{Literature in guaranteeing the faithfulness of defining and disentangling a DNN's inference patterns.} \citet{ren2023AOG} have discovered and \citet{ren2024where} have proved that, given a DNN, there always exists a logical model consisting of AND-OR inference patterns, which can using a small set of AND-OR interactions to accurately predict the DNN's outputs on all $2^n$ masked states of the input sample. Furthermore, \citet{li2023does} have shown that the salient interactions extracted from a DNN can be shared across different samples. Besides, they have also discovered that salient interactions exhibited remarkable discrimination power in classification tasks. \citet{chen2024defining} have proposed a method to extract interactions that are generalizable across different models. These findings suggest that interactions act as primitive inference patterns encoded by DNNs, forming the theoretical foundation of interaction-based theoretical frameworks.

\textbf{Literature in explaining the generalization power of DNNs from the perspective of interactions.} Recent achievements have shown that interactions play a crucial role in explaining the hidden factors affecting a DNN's adversarial robustness~\cite{ren2021towards}, adversarial transferability~\cite{wang2021unified}, and generalization power~\cite{zhou2024generalization} of a DNN. \citet{deng2022discovering} have discovered and theoretically proved the existence of a representation bottleneck in DNNs, which limits their ability to encode interactions of intermediate complexity. \citet{ren2023bayesian} have found that Bayesian neural networks (BNNs) tend to avoid encoding complex interactions, which helps explain the good adversarial robustness of BNNs. \citet{liu2023towards} have discovered that DNNs learn simple interactions more easily than complex interactions. \citet{zhang2024two} and \citet{ren2024towards} have discovered and analyzed the two-phase dynamics in DNNs' learning process.