\section{Methods}
\subsection{Architecture Details\label{sec:SI_arch}}

BANNANE is designed to emulate nuclear properties across isotopic chains while integrating multi-fidelity data. Its hierarchical Bayesian framework combines learnable embeddings, a multi-head self-attention mechanism with fidelity-specific queries, and fidelity-specific prediction pathways. This architecture ensures that the emulator can leverage correlations between nuclear observables across elements and isotopes, while also adapting to varying levels of computational fidelity. It must be added that we expect the model to further improve the performance once a full hyperparameter optimization is performed. Below we present a detailed overview of our implementation.

% \subsubsection{Input Embeddings}
\paragraph{\textbf{\textit{Input embeddings.}---}}
To incorporate discrete nuclear identifiers and fidelity information, BANNANE utilizes three primary embedding layers:

\paragraph{Element (\(Z\)) embedding:}
An embedding layer maps each proton number \(Z \in \{0, 1, \ldots, Z_{\text{max}}\}\) to a continuous vector space:
\[
\mathbf{e}_Z = \mathbf{W}_Z \cdot \mathbf{1}_Z,
\]
where \(\mathbf{W}_Z \in \mathbb{R}^{(Z_{\text{max}}+1) \times d_Z}\) is a learnable weight matrix, \(\mathbf{1}_Z\) is a one-hot encoding of \(Z\), and \(d_Z\) is the embedding dimension. To increase the architecture's flexibility, we left the option to include this embedding into the model, which is not considered for the benchmarks presented in this work but will be needed when other $Z$ values are considered.

\paragraph{Neutron number (\(N\)) positional encoding:}
A similar embedding can be applied to the neutron number \(N\): $\mathbf{e}_N = \mathbf{W}_N \cdot \mathbf{1}_N$ with \(\mathbf{W}_N \in \mathbb{R}^{(N_{\text{max}}+1) \times d_N}\) and \(d_N\) the corresponding dimension. This learned positional encoding allows the network to capture trends as a function of neutron number, crucial for modeling isotopic evolution.

However, instead of a learnable embedding, BANNANE uses a fixed sinusoidal positional encoding inspired by Transformer architectures. This encoding defines a phase:
$$
\varphi = \cdot \exp\left(-\frac{2i}{d_N}\log(10000)\right), \quad i = 0, \dots, \left\lfloor\frac{d_N-2}{2}\right\rfloor
$$

Which is then added as a positional encoding via:

\[
\begin{aligned}
\text{PE}(n, 2i) = \sin\left(n\varphi)\right),\quad
\text{PE}(n, 2i+1) = \cos\left(n \varphi\right).
\end{aligned}
\]

This guarantees that the neutron number encoding captures smooth trends across isotopic chains, aiding generalization. The sinusoidal encoding is implemented as a configurable alternative to learned embeddings, selectable via hyperparameters. We used the sinusoidal encoded $N$ embeddings for all the results in this work.

\paragraph{Fidelity level embedding:}
Each fidelity level \(f \in \mathcal{F}\) is similarly embedded:
\[
\mathbf{e}_f = \mathbf{W}_f \cdot \mathbf{1}_f,
\]
where \(\mathbf{W}_f \in \mathbb{R}^{|\mathcal{F}| \times d_f}\) maps a one-hot representation of fidelity level to an embedding space of dimension \(d_f\).

The continuous input features \(\mathbf{x} \in \mathbb{R}^{d_x}\), which may include coupling constants and other parameters derived from effective field theories, are concatenated with these embeddings to form a combined input:
\[
\mathbf{z} = \Big[ \mathbf{x},\, \mathbf{e}_Z,\, \mathbf{e}_N,\, \mathbf{e}_f \Big] \in \mathbb{R}^{d_x + d_Z + d_N + d_f}.
\]

% \subsubsection{Shared Latent Representation}
\paragraph{\textbf{\textit{Shared latent representation.}---}}
The combined input \(\mathbf{z}\) is fed into a \emph{shared latent network} to extract a feature-rich representation:
\[
\mathbf{h} = \phi(W_2 \cdot \phi(W_1 \cdot \mathbf{z} + \mathbf{b}_1) + \mathbf{b}_2),
\]
where \(W_1\), \(W_2\) are weight matrices, \(\mathbf{b}_1\), \(\mathbf{b}_2\) are biases, and \(\phi(\cdot)\) denotes a non-linear activation function (e.g., LeakyReLU). This produces a latent vector \(\mathbf{h} \in \mathbb{R}^{d_h}\) shared across all fidelities.

% \subsubsection{Multi-Head Self-Attention Mechanism with Fidelity-Specific Queries}
\paragraph{\textbf{\textit{Multi-head self-attention mechanism with fidelity-specific queries.}---}}
To capture complex interdependencies and fidelity-specific features, BANNANE employs a multi-head self-attention mechanism augmented with fidelity-specific query vectors~\cite{vaswani2017attention}. This allows the model to extract different aspects of the shared representation tailored to each fidelity level. For our implementation, we fixed the number of attention heads to 2.

\paragraph{Fidelity-specific queries:}  
For each fidelity level, including the base fidelity, a unique learnable query vector is introduced:
\[
\mathbf{Q}^{(f)} = \mathbf{W}_q^{(f)} \cdot \mathbf{h},
\]
where \(\mathbf{W}_q^{(f)}\) is a learnable projection matrix specific to fidelity \(f\).

\paragraph{Attention computation:}  
For a given fidelity \(f\), the query \(\mathbf{Q}^{(f)}\) interacts with the shared latent representation \(\mathbf{h}\) to compute attention weights and extract relevant features:
\[
\text{Attention}^{(f)}(\mathbf{Q}^{(f)}, \mathbf{K}, \mathbf{V}) = \mathrm{softmax}\left(\frac{\mathbf{Q}^{(f)} \mathbf{K}^\top}{\sqrt{d_k}}\right) \mathbf{V},
\]
where \(\mathbf{K} = W_k \mathbf{h}\) and \(\mathbf{V} = W_v \mathbf{h}\) are shared key and value projections, respectively, and \(d_k\) is the dimension per head.

\paragraph{Concatenation and projection:}  
The outputs from all attention heads are concatenated and passed through a linear layer to form the final attention-enhanced representation:
\[
\mathbf{o}^{(f)} = W_o \Big[ \text{head}_1^{(f)}, \ldots, \text{head}_H^{(f)} \Big],
\]
with \(W_o \in \mathbb{R}^{(H \cdot d_k) \times d_{\text{out}}}\).

% \subsubsection{Fidelity-Specific Regression Pathways}
\paragraph{\textbf{\textit{Fidelity-specific regression pathways.}---}}
The attention-enhanced representation \(\mathbf{o}^{(f)}\) is used to predict target observables hierarchically, leveraging both shared and fidelity-specific information:

\paragraph{Base prediction:}
For the lowest fidelity level \(f_0\), the model computes
\[
\begin{aligned}
    \mathbf{y}^{(f_0)} &= f_{\text{base}}(\mathbf{o}^{(f_0)})\\ 
    &= W_{\text{base}}^2 \cdot \phi(W_{\text{base}}^1 \cdot \mathbf{o}^{(f_0)} + \mathbf{b}_{\text{base},1}) + \mathbf{b}_{\text{base},2},
\end{aligned}
\]
where \(W_{\text{base}}^1\), \(W_{\text{base}}^2\) and biases \(\mathbf{b}_{\text{base},1}, \mathbf{b}_{\text{base},2}\) are learnable parameters.

\paragraph{Delta adjustments for higher fidelities:}
For each subsequent fidelity level \(f > f_0\), the model refines the prediction by adding a delta adjustment:
\[
\Delta \mathbf{y}^{(f)} = f_{\text{delta}, f}(\mathbf{o}^{(f)}) = W_{f}^2 \cdot \phi(W_{f}^1 \cdot \mathbf{o}^{(f)} + \mathbf{b}_{f,1}) + \mathbf{b}_{f,2}.
\]
The prediction at fidelity \(f\) is then given by the sum of the base prediction and all applicable delta corrections:
\[
\mathbf{y}^{(f)} = \mathbf{y}^{(f_0)} + \sum_{f' \leq f} \Delta \mathbf{y}^{(f')}.
\]
In this hierarchical structure, the shared attention mechanism with fidelity-specific queries enables each delta model to focus on different features of the shared representation, enhancing the model's ability to capture fidelity-dependent nuances.

\subsection{Bayesian Inference and Variational Training}

% \subsubsection{Probabilistic Model Specification}
\textbf{\textit{Probabilistic model specification.}---}
The Bayesian neural network (BNN) places a prior distribution over all its weights \(\theta\) and output noise standard deviations \(\sigma\). For instance, we may use a half-normal prior for \(\sigma\):
\[
p(\sigma) = \mathrm{HalfNormal}(\sigma \mid 1.0),
\]
and standard normal priors for weights:
\[
p(\theta) = \prod_{i} \mathcal{N}(\theta_i \mid 0, 1).
\]

Given input \(\mathbf{x}\), fidelity index \(f\), \(Z\), and \(N\), the likelihood of observing target \(\mathbf{y}\) is
\[
p(\mathbf{y} \mid \mathbf{x}, f, Z, N, \theta, \sigma) = \mathcal{N}\Big(\mathbf{y}; \, \mu(\mathbf{x}, f, Z, N; \theta),\, \operatorname{diag}(\sigma^2)\Big),
\]
where \(\mu(\cdot)\) is the output of the hierarchical network.

% \subsection{Variational Approximation and ELBO}
\textbf{\textit{Variational qpproximation and ELBO.}---}
We approximate the intractable posterior \(p(\theta, \sigma \mid \mathcal{D})\) with a variational distribution \(q(\theta, \sigma)\), typically chosen as a mean-field Gaussian:
\[
q(\theta, \sigma) = \prod_{i} \mathcal{N}(\theta_i \mid m_i, s_i^2) \times q(\sigma),
\]
where \(m_i\) and \(s_i\) are variational parameters. For \(\sigma\), we similarly choose a tractable form.

The Evidence Lower Bound (ELBO) over the dataset \(\mathcal{D}\) is
\[
\begin{aligned}
\mathcal{L}(q) &= \mathbb{E}_{q(\theta, \sigma)}\Big[ \sum_{(\mathbf{x}, \mathbf{y}, f, Z, N) \in \mathcal{D}} \log p(\mathbf{y} \mid \mathbf{x}, f, Z, N, \theta, \sigma) \Big] \\
&\quad - \mathrm{KL}\Big(q(\theta, \sigma) \,\Big\|\, p(\theta, \sigma)\Big).
\end{aligned}
\]

% \subsection{Loss Components}
\paragraph{\textbf{\textit{Loss components.}---}}
For a mini-batch \(\mathcal{B}\) sampled from fidelity level \(f\), the negative ELBO loss function decomposes as:
\[
\mathcal{L}_{\mathcal{B}} = \underbrace{-\sum_{i \in \mathcal{B}} \log p(\mathbf{y}_i \mid \mathbf{x}_i, f, Z_i, N_i, \theta, \sigma)}_{\text{Reconstruction Loss}} + \underbrace{\mathrm{KL}\Big(q(\theta, \sigma) \,\Big\|\, p(\theta, \sigma)\Big)}_{\text{Regularization}}.
\]

Optionally, a contrastive loss term \(\mathcal{L}_{\text{contrastive}}\) can be added to encourage similar embeddings for similar nuclei or fidelities:
\[
\mathcal{L}_{\text{total}} = \mathcal{L}_{\mathcal{B}} + \lambda \,\mathcal{L}_{\text{contrastive}},
\]
with \(\lambda\) a hyperparameter weighting the contrastive component.

\subsection{Training Protocol\label{sec:SI_training}}

The training procedure involves iteratively updating the variational parameters to maximize the ELBO:

\begin{enumerate}
    \item \textbf{Initialization:}
    \begin{itemize}
        \item Set random seeds for reproducibility.
        \item Initialize network weights, biases, and variational parameters.
        \item Load multi-fidelity isotopic data, then preprocess and split into training, validation, and test sets.
        \item Standardize input features \(\mathbf{x}\) and target variables \(\mathbf{y}\) using global scalers.
    \end{itemize}
    \item \textbf{Stochastic Variational Inference Loop:}
    \begin{itemize}
        \item Sample mini-batches \(\mathcal{B}_f\) from each fidelity level \(f\).
        \item Compute gradients of the negative ELBO \(\mathcal{L}_{\mathcal{B}_f}\) with respect to variational parameters.
        \item Update parameters using an optimizer like ClippedAdam.
    \end{itemize}
    \item \textbf{Early Stopping and Checkpointing:}
    \begin{itemize}
        \item Track the validation loss and apply early stopping based on patience.
        \item Save the model corresponding to the lowest validation loss.
    \end{itemize}
\end{enumerate}

The number of samples per each one of the fidelities and isotopes used in the training procedure is shown in Fig.~\ref{fig:train_num_heatmap}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/train_num_heatmap.pdf}
    \caption{\textbf{Samples per fidelity:} Number of LEC samples per fidelity used for training BANNANE for each one of the isotopes.}
    \label{fig:train_num_heatmap}
\end{figure}

\subsection{Inference and Uncertainty Quantification}

To predict for a new input \((\mathbf{x}^*, f^*, Z^*, N^*)\), we sample from the posterior predictive distribution following:

\begin{enumerate}
    \item \textbf{Preprocessing:} Scale \(\mathbf{x}^*\) using the fitted StandardScaler.
    \item \textbf{Monte Carlo Sampling:} Draw \(S\) samples \(\{(\theta^{(s)}, \sigma^{(s)})\}_{s=1}^S\) and compute predictions:
    \[
    \mathbf{y}^{*(s)} = \mu(\mathbf{x}^*, f^*, Z^*, N^*; \theta^{(s)}).
    \]
    \item \textbf{Estimating Statistics:}
    \[
    \hat{\mathbf{y}}^* = \frac{1}{S} \sum_{s=1}^S \mathbf{y}^{*(s)}, \quad \hat{\sigma}^* = \sqrt{\frac{1}{S-1} \sum_{s=1}^S \Big( \mathbf{y}^{*(s)} - \hat{\mathbf{y}}^* \Big)^2}.
    \]
\end{enumerate}

\subsection{Hyperparameters}
% \paragraph{\textbf{\textit{Hyperparameters.}---}}
To achieve the results presented in this work, we used a shared latent dimension of 80 with hidden layers of 64 units and a multi-head attention mechanism with 2 heads. The neutron number was encoded using a sinusoidal encoding, while the fidelity levels were embedded in an 8-dimensional space. The model included a dropout rate of 0.05 to enhance generalization. Training was performed using the ClippedAdam optimizer with an initial learning rate of \(1\times10^{-4}\), and a patience of 200 for early stopping. The learning rate was reduced by a factor of 2 if validation loss did not improve within 25 evaluations. The number of training iterations was set to 30,000, with validation and test splits of 20\% each. To account for the varying importance of different fidelities, we applied weighted losses, with higher fidelity levels receiving greater weight (\(e_{\text{max}}=4\) to \(10\) assigned weights from 1.0 to 2.5). 

With this configuration for the architecture, the total number of trainable parameters of the architecture is $2\cdot 10^5$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

\subsection{Complete Oxygen Chain}
% \paragraph{\textbf{\textit{Complete Oxygen Chain.}---}}
To assess the benefits of the global fitting performed by BANNANE, we compare the global fitting with a local fit performed using a state-of-the-art Boosted Decision Trees (BDT)~\cite{prokhorenkova2018catboost} trained on a per-isotope basis including all fidelities, where the fidelity was included as an extra categorical feature. Table~\ref{tab:per_isotope_rmse_comparison} presents a comparison of the global Root-mean-square error (RMSE) per isotope for both approaches.
\begin{table}[h!]
    \caption{\label{tab:per_isotope_rmse_comparison} 
    Root-mean-square error (RMSE) of binding energy ($E_B$) and charge radius ($R_{ch}$) for oxygen isotopes at $e_{\text{max}}=10$.}
    \centering
    \setlength{\tabcolsep}{6pt} % Adjust column padding
    \renewcommand{\arraystretch}{1.1} % Adjust row spacing
    \begin{tabular}{c|cc|cc}
        \hline\hline
        \textbf{N} & \multicolumn{2}{c|}{\textbf{BANNANE}} & \multicolumn{2}{c}{\textbf{BDT}} \\
        \cline{2-5}
        & $E_B$ (MeV) & $R_{ch}$ (fm) & $E_B$ (MeV) & $R_{ch}$ (fm) \\
        \hline
         4  & 0.692(110) & 0.0318(8)  & 2.521(89) & 0.0494(3) \\
         5  & 0.415(72)  & 0.0248(11) & 3.256(14) & 0.0498(2) \\
         6  & 0.642(14) & 0.0131(3)  & 4.541(90) & 0.0285(1) \\
         7  & 0.544(12) & 0.0136(2)  & 3.880(16) & 0.0240(1) \\
         8  & 0.337(55)  & 0.0205(7)  & 5.535(15) & 0.0300(1) \\
         9  & 0.317(45)  & 0.0082(2)  & 3.619(18) & 0.0239(1) \\
        10  & 0.551(11) & 0.0116(2)  & 5.294(11) & 0.0339(1) \\
        12  & 0.455(66)  & 0.0106(2)  & 4.892(31) & 0.0246(1) \\
        13  & 0.293(32)  & 0.0083(1)  & 8.109(30) & 0.0409(1) \\
        14  & 0.277(32)  & 0.0199(5)  & 7.931(44) & 0.0450(2) \\
        15  & 0.385(57)  & 0.0101(1)  & 6.318(24) & 0.0364(1) \\
        16  & 0.538(11) & 0.0087(1)  & 5.182(27) & 0.0340(1) \\
        \hline\hline
    \end{tabular}
\end{table}



\subsection{Leave One Out - Zero Shot extrapolation\label{sec:si_zero_shot}}
% \paragraph{\textbf{\textit{Leave One Out - Zero Shot extrapolation.}---}}
As a tool to probe the extrapolation capabilities of BANNANE, we proceeded to remove completely each one of the isotopes from the training samples, and then we proceeded to use the model to evaluate the Mean Absolute Percentage Error (MAPE) for each one of them in a Zero-Shot approach, meaning that no examples for this fidelity were provided before. The distribution of residuals is shown in Fig~\ref{fig:loo_mape_heatmap}. Remarkably, the model does not diverge significantly near the driplines, and the MAPEs do not increase considerably for higher $e_{\text{max}}$ due to self-consistency from the hierarchical architecture.

We find however that the model struggles with identifying the position of the nuclear shell closure if it isn't included in the training, as seen by the significant error in the charge radii when removing one isotope near the edge of the shells. This means that it is important to add nuclei near the edges of the shell for the model to perform well. This is luckily not an issue for many-body methods as these nuclei are simpler to compute. Nuclear shells could be given as an input of the model in the future to remedy this.
\begin{figure}[t]
    \centering
    \includegraphics[width=1.1\linewidth]{figures/loo_mape_heatmap.pdf}
    \caption{Zero-Shot Leave-On-Out. Distribution of the Zero-Shot extrapolation for unseen isotopes as each isotope was removed from the training Samples.\label{fig:loo_mape_heatmap}}
\end{figure}

\subsection{Fidelity Extrapolation\label{sec:si_fidelity_extra}}

Following the same approach as with the Leave One Out - Zero Shot extrapolation presented above, we repeated this experiment but now including only data with $e_{\text{max}}=4$ for each one of the isotopes left out and the rest of the fidelities for the rest of the chain. In Fig.~\ref{fig:eemax_mape_heatmap} we show the evaluated MAPE as extrapolated.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.1\linewidth]{figures/eemax_mape_heatmap.pdf}
    \caption{Low-Fidelity-Shot Leave-On-Out. Distribution of the $e_\text{max}$ extrapolation for unseen isotopes as data from $e_\text{max}>4$ was removed from the training Samples.}
    \label{fig:eemax_mape_heatmap}
\end{figure}

\begin{table}[!htb]
    \caption{Inference ($e_{\text{max}}=4,6$) to high fidelity ($e_{\text{max}}=8$) with minimal extra data. Shown is the MAPE (\%) in binding energies and charge radii for oxygen isotopes after introducing a given fraction of the $e_{\text{max}}=8$ dataset.}
    \centering
    \begin{tabular}{c|c|c}
    \hline\hline
    \textbf{$e_{\text{max}}=8$ data $\%$} & \textbf{MAPE $B_E$ (\%)} & \textbf{MAPE $R_{ch}$ (\%)} \\
    \hline
    5\%  & 2.38 & 1.67 \\
    10\% & 1.65 & 1.24 \\
    20\% & 1.32 & 0.818 \\
    30\% & 1.52 & 0.557 \\
    40\% & 1.13 & 0.499 \\
    50\% & 1.16 & 0.362 \\
    60\% & 2.19 & 0.308 \\
    70\% & 0.650 & 0.326 \\
    80\% & 0.734 & 0.388 \\
    90\% & 0.571 & 0.267 \\
    \hline\hline
    \end{tabular}
    \label{tab:fine_tune}
\end{table}

Interestingly, adding the low-fidelity samples considerably helps reduce the MAPE, while difficulties in predicting the dynamics of LECs around the shell closure remain for the $R_{ch}$ regression.

\subsection{Data Efficiency}



Table~\ref{tab:fine_tune} summarizes the data usage and resulting RMSE, underscoring the computational savings and the robust uncertainty quantification still retained by the Bayesian framework. This model refinement significantly cuts computational cost: we can cheaply train the base model on large amounts of $e_{\text{max}}=4$ data and only sample a sparse set of high-$e_{\text{max}}$ which can be computed until the emulator residual converges. 




\subsection{Ablation Study}

In order to model the behavior of the model as more attention-exclusive parameters are added, we performed a sweep among the shared dimension layer of the LEC before passing them to the attention mask, while keeping all of the other parameters of the model fixed. For each one of the data points we performed 10 trainings with different random seeds, and for each one of them evaluated the corresponding test samples. In Figure~\ref{fig:ablation}, the robustness of the interpolation performance is evident. The attention mechanism allows the model to capture higher-order correlations between the LECs, allowing for globally better emulation of both the binding energy and the charge radii. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/ablation.pdf}
    \caption{Ablation study of the size of the attention mechanism as we systematically increased the size of the Multihead Attention layer while keeping the other parameters fixed.
    \label{fig:ablation}}
\end{figure}

\subsection{General Sensitivity Analysis}

Global sensitivity analysis is a statistical tool used to decompose how the total uncertainty of a model is separated in term of the uncertainty of its different parameters~\cite{sobol2001global}. Following notation from \cite{Ekstrom:2019}, the total variance of the results, $Var[Y]$, is decomposed as
\begin{align}
    Var[Y] = \sum_{i=1}^{N_{LECs}} V_i +    \sum_j^{N_{LECs}}\sum_{i<j}^{N_{LECs}} V_{ij} + ... 
\end{align}
with the partial variances $V_i, V_{ij}, ...$ given by
\begin{align}
    &V_i= Var\left[E_{\vec{\alpha}\sim(\alpha_i)}[Y|\alpha_i]\right]\\
    &V_{ij}= Var\left[E_{\vec{\alpha}\sim(\alpha_i, \alpha_i)}[Y|\alpha_i, \alpha_j]\right] - V_i - V_j.
\end{align}
$\vec{\alpha}\sim(\alpha_i)$ is the set of LECs without $\alpha_i$ and $E_{\vec{\alpha}\sim(\alpha_i)}[Y|\alpha_i]$ is the conditional expectation of Y given $\alpha_i$, likewise for the higher order expression. We can then write the total sensitivity of the final results to each LEC $\alpha_i$ as 
\begin{align}
    S_T(\alpha_i) = S_i+ S_{ij} + S_{ijk} + ...
\end{align}
where the partial sensitivities are given by
\begin{align}
    S_i = \dfrac{V_i}{Var[Y]},  && S_{ij} = \dfrac{V_{ij}}{Var[Y]}
\end{align}
and likewise for higher orders. 