\section{Related Work}
\textbf{Edge-Cloud Collaboration for Resource Prediction.} 
The main goal of resource prediction is to optimize resources in the long term ____. Through resource prediction, edge-cloud systems can allocate resources well in advance to address potential high loads or performance bottlenecks in the future ____. Some researchers propose edge-cloud collaboration solutions based on resource prediction. The authors in ____ focus on model segmentation schemes based on edge-cloud collaboration and determining the optimal model segmentation point according to computing resources. However, they are only for single-edge nodes. The authors in ____ propose a joint-aware video processing architecture for edge-cloud collaboration. It can guide resource allocation and reduce video processing costs by predicting the task complexity. Considering the constraints of multiple tasks and edge resources, the authors in ____ utilize gated recurrent units to predict edge resource utilization. They then propose a joint optimization method based on resource utilization prediction according to the predicted results of network states. To address the load imbalance problem in edge-cloud systems, the authors in ____ propose a resource optimization method based on workload prediction, which improves prediction accuracy through server correlation analysis. The authors in ____ introduce a dynamic resource prediction framework for DNN models, allowing the selection of the optimal balance between resources and accuracy for each DNN model. The work in ____ adjusts resource allocation schemes by predicting model inference times. The authors in ____ dynamically adjust memory access rates based on delay targets and user-defined priorities to improve resource allocation efficiency. However, the above works only consider the prediction and optimization of a single type of resource (only computing or only bandwidth). The characteristic differences in heterogeneous resources may cause uneven resource utilization ____, thus affecting the performance of edge-cloud systems.

\par
\textbf{Edge-Cloud Collaboration for Inference Offloading.} The offloading of video tasks from the end side to the edge or cloud servers is a crucial step for efficient video processing ____. Some researchers propose many edge-cloud collaboration solutions based on task offloading. The authors in ____ introduce a time-aware edge-cloud collaborative task scheduling method, which ensures scheduling accuracy and improves throughput by the performance characterizing network. The authors in ____ propose an edge-cloud collaborative scheduling framework for joint configuration optimization. It can improve task allocation through two-stage robust optimization. The authors in ____ dynamically adjust the task offloading scheme through service prioritization and network conditions. The authors in ____ propose a dynamic adaptive offloading framework for video analysis, which enhances inference accuracy by dynamically adjusting network bandwidth and video bitrate. Simultaneous uploading of many tasks can affect the accuracy and real-time performance of video processing. To address this problem, the authors in ____ investigate the dynamic task offloading problem for large-scale inference requests and design an online optimization algorithm that supports real-time adjustments. Considering the uncertainty in task arrival rates, the authors in ____ propose a dual time-scale Lyapunov optimization algorithm to overcome the uncertainty of future information of the system, aiming to minimize the cost of task offloading. To solve the load imbalance problem with multiple edge nodes, the authors in ____ investigate task offloading schemes based on deep reinforcement learning. In real-world scenarios, due to the continuous change of accuracy and delay requirements of tasks, different tasks have different preferences for different types of resources ____. The above methods ignore analyzing task characteristics, which makes it difficult to achieve efficient edge-cloud collaborative task offloading.