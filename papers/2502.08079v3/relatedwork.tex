\section{Related work}
\label{Rel}

\subsection{Vision-Language Pre-trained Models}

Vision-language pre-trained (VLP) models play a crucial role in advancing the understanding of visual and textual information and their interactions. By leveraging large-scale unlabeled datasets and self-supervised learning techniques, VLP Models can learn rich, generalized representations of both visual and linguistic data. This transfer learning capability enables them to be fine-tuned for a wide range of tasks with relatively small datasets, including multi-modal retrieval, zero-shot learning, image captioning, visual question answering, and visual entailment \cite{radford2021learning,li2022blip,li2021align,yang2022vision}. Notable VLP methods include CLIP \cite{radford2021learning}, BLIP \cite{li2022blip}, ALBEF \cite{li2021align}, and TCL \cite{yang2022vision}. These methods primarily leverage multi-modal contrastive learning to align image-text pairs. Specifically, CLIP employs unimodal encoders to project data from different modalities into a unified feature space. BLIP \cite{li2022blip} refines noisy captions to enhance learning effectiveness. ALBEF \cite{li2021align} and  TCL \cite{yang2022vision} both utilize a multi-modal encoder to learn joint representations for images and texts.  ALBEF \cite{li2021align} focuses on inter-modal relationships while  TCL \cite{yang2022vision} considers both intra- and inter-modal relationships.

\subsection{Adversarial Attack}

With the widespread adoption of Deep Neural Networks, it is essential to evaluate their robustness to ensure their reliability in real-world applications, which often involve uncertainty and potential threats. Adversarial attacks are a prominent method used to assess this robustness, which aim to mislead model predictions by introducing imperceptible perturbations into the data \cite{szegedy2014intriguing, zhang2023proactive, zhang2021privacy, madry2018towards, moosavi2017universal}. Traditional methods typically focus on specific tasks and unimodal cases, such as image classification. For image attacks, most techniques learn pixel-level perturbations, whereas text attacks often involve replacing or removing keywords, or performing text transformations \cite{li2020bert, jin2020bert, iyyer2018adversarial, naik2018stress, ren2019generating}. Recently, there has been growing interest in multi-modal vision-language scenarios. For instance, \cite{zhang2023proactive, wang2021prototype, zhu2023efficient} address image-text retrieval by increasing the embedding distance between adversarial and original data pairs. \cite{xu2018fooling} learn image perturbations by minimizing the distance between the output and the target label while maximizing the difference from the original label for the visual question answering task. More introduction can be found in \cite{zhangsurvey}.

% To address this challenge and promote transferability, one approach involves using an 

\noindent\textbf{Adversarial transferability.} Early attack methods typically assume a white-box setting, where all necessary information for generating adversarial examples, including target models and tasks, is available. However, in real-world scenarios, such comprehensive information is often unavailable. To address this challenge, one approach uses an ensemble of models as the victim model during training \cite{liu2016delving, dong2018boosting, dong2019evading, xiong2022stochastic}, based on the intuition that adversarial examples effective against a diverse set of models are likely to mislead more models. However, assembling such a model ensemble can be difficult. Another approach utilizes momentum-based methods \cite{dong2018boosting, long2024convergence, lin2019nesterov, inkawhich2019transferable} to stabilize gradient updates and avoid poor local maxima, though this may also divert perturbations from effective paths. Data augmentation \cite{xie2019improving, fang2022learning, wei2023enhancing, wang2024boosting, wang2021admix} increases data diversity, helping to prevent overfitting to specific models. Transferability is particularly challenging for attacks on VLP models, due to the unpredictable fine-tuning process. To enhance transferability, \cite{zhang2022towards,lu2023set,zhang2024universal} suggests increasing the gap between adversarial data and original image-text pairs. Some of these methods also focus on generating diverse image-text pairs through scale-invariant transformations \cite{lu2023set} and ScMix augmentations \cite{zhang2024universal}. Additionally, recent approaches consider the local utility of adversarial examples \cite{yin2023vlattack} or perturbations \cite{zhang2024universal}. However, \cite{yin2023vlattack} enlarges block-wise similarity between samples, which is constrained by the block size, failing to comprehensively capture local regions and their interactions. \cite{zhang2024universal} does not consider the characteristics and vulnerabilities of the original sample. As a result, these methods cannot ensure the effectiveness and transferability of adversarial attacks.