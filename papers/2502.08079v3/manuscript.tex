
\documentclass{article} % For LaTeX2e
\usepackage{conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

\let\Bbbk\relax
\usepackage{amsfonts,amssymb}
\usepackage[ruled]{algorithm2e}
\usepackage{colortbl} 
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{epsfig}
\usepackage{rotating, multirow}
\usepackage{epstopdf}
\usepackage{float}
\usepackage{diagbox}
\usepackage{balance}
\usepackage{arydshln}

\def\thickhline{\noalign{\hrule height.8pt}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\gb}[1]{\textcolor{blue}{#1}}

\title{MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained Models}

\author{Peng-Fei Zhang \\
The University of Queensland\\
\texttt{mima.zpf@gmail.com} \\
\And
Guangdong Bai \\
The University of Queensland\\
\texttt{g.bai@uq.edu.au} \\
\AND
Zi Huang \\
The University of Queensland\\
\texttt{huang@itee.uq.edu.au}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Current adversarial attacks for evaluating the robustness of vision-language pre-trained (VLP) models in multi-modal tasks suffer from limited transferability, where attacks crafted for a specific model often struggle to generalize effectively across different models, limiting their utility in assessing robustness more broadly. This is mainly attributed to the over-reliance on model-specific features and regions, particularly in the image modality. In this paper, we propose an elegant yet highly effective method termed Meticulous Adversarial Attack (MAA) to fully exploit model-independent characteristics and vulnerabilities of individual samples, achieving enhanced generalizability and reduced model dependence. MAA emphasizes fine-grained optimization of adversarial images by developing a novel resizing and sliding crop (RScrop) technique, incorporating a multi-granularity similarity disruption (MGSD) strategy. 
RScrop efficiently enriches the initial adversarial examples by generating more comprehensive, diverse, and detailed perspectives of the images, establishing a robust foundation for capturing representative and intrinsic visual characteristics. Building on this,  MGSD seeks to maximize the embedding distance between adversarial examples and their original counterparts across different granularities and hierarchical levels within the architecture of VLP models, thereby amplifying the impact of the adversarial perturbations and enhancing the efficacy of attacks across every layer and component of the model. Extensive experiments across diverse VLP models, multiple benchmark datasets, and a variety of downstream tasks demonstrate that MAA significantly enhances the effectiveness and transferability of adversarial attacks. A large cohort of performance studies is conducted to generate insights into the effectiveness of various model configurations, guiding future advancements in this domain. 

\end{abstract}

\section{Introduction}

Vision-language pre-trained (VLP) models have achieved remarkable success and serve as foundational models for a wide range of tasks, including information retrieval, image captioning, and visual question answering \cite{radford2021learning,li2022blip,li2021align,yang2022vision}. These models are typically pre-trained on large-scale unlabeled datasets using self-supervised learning and subsequently fine-tuned for specific downstream tasks. Given their extensive applications, it is crucial to evaluate the robustness of VLP models to ensure their reliability in real-world scenarios, which are often characterized by uncertainties and potential threats. A representative method for assessing robustness is through adversarial attacks, where imperceptible perturbations are deliberately crafted to mislead models to wrongly associate images and texts, resulting in incorrect predictions.
%-------------------------------------------------------------------------
\begin{table*}[!t]
\center
\caption{Comparison of different attack methods using image-only perturbations (\text{im}) and multi-modal perturbations (\text{mul}) in the image-text retrieval task on Flickr30K. Attack success rate $(\%)$ regarding the average of R@1 is used for evaluation. CLIP$_\text{ViT-B/16}$ is adopted as the source model. The grey background indicates the white-box attack results.}
\label{tab_comp}
\centering
\resizebox{0.85\linewidth}{!}{

\begin{tabular}{c|cc|cc|cc|cc|cc}
\toprule

\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Target Model}}} & \multicolumn{6}{c}{\textbf{CLIP}} & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{ALBEF}}} & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{TCL}}}\\
\cmidrule(lr){2-7} % Define custom midrules to separate the groups
& \multicolumn{2}{c|}{$\text{ViT-B/16}$}  & \multicolumn{2}{c|}{$\text{ViT-L/14}$}&\multicolumn{2}{c|}{\text{RN101}} \\

\midrule
{ Method} & I2T & T2I & I2T & T2I  & I2T & T2I & I2T & T2I & I2T & T2I  \\
\midrule
{Co-Attack$_\text{im}$}  &\cellcolor[HTML]{EFEFEF}{{90.55}}  &\cellcolor[HTML]{EFEFEF}{91.72} & {7.48} & 16.4     
& {8.94}  & 12.42   & {2.82}  & 5.78    & 5.16    & {7.98} \\
{Co-Attack$_\text{mul}$}  &\cellcolor[HTML]{EFEFEF}{97.73}  &\cellcolor[HTML]{EFEFEF}{98.83}  &{27.80}  &44.50 
& 35.93   &44.52   &11.42   &25.30     & 12.63  & {25.85}  \\\hline
{SGA$_\text{im}$}   &\cellcolor[HTML]{EFEFEF}{98.04} &\cellcolor[HTML]{EFEFEF}{99.00} &{15.58}  &23.29 
& 15.33  & 21.20   & 5.11  &10.41   & 7.06   & {12.12} \\
{SGA$_\text{mul}$}  &\cellcolor[HTML]{EFEFEF}{{99.53}}  &\cellcolor[HTML]{EFEFEF}{{99.73}}  & {32.14}  &{47.83}   
&44.01 &51.19  & 14.86    &29.56    & {16.26}   & {30.66}  \\\hline
{VLATTACK$_\text{im}$}  &\cellcolor[HTML]{EFEFEF}{{99.88}}  &\cellcolor[HTML]{EFEFEF}{{99.97}}  & {8.34} &16.24   
&{13.41}    & 17.87  & 2.92    &7.74     & {6.11}   & {10.07}  \\
{VLATTACK$_\text{mul}$}  &\cellcolor[HTML]{EFEFEF}{99.86} &\cellcolor[HTML]{EFEFEF}{99.92}  & {30.49} &{42.69} 
&41.31   &{48.65} & {11.29}   &{28.22}    &14.49  & {30.23}   \\\bottomrule
\end{tabular}
}
\end{table*}
 %-------------------------------------------------------------------------



Ensuring the transferability of adversarial attacks across different models is critical, as it is impractical to craft individual attacks for every different model in real-world scenarios, especially when attackers often lack access to target models. Existing methods usually enhance adversarial transferability by enlarging the feature distance between adversarial examples and their original counterparts across different modalities \cite{zhang2022towards,lu2023set,he2023sa,yin2023vlattack,zhang2024universal}. 
Some of them also use data augmentation techniques \cite{lu2023set,zhang2024universal,he2023sa} to increase data diversity to further prevent overfitting to the target model during training (a.k.a. the source model). However, the performance of adversarial examples produced by these methods is less effective when applied to unknown target models, where adversarial perturbations for the image modality are extraordinarily less effective (an example is presented in Table \ref{tab_comp}). Their limited transferability stems from the failure to fully explore the characteristics and vulnerabilities of individual samples, as well as the over-reliance on model-specific patterns. 

Transferability is particularly challenging in the context of VLP models due to two key factors: the unpredictable fine-tuning process and the complexity of involving multiple modalities. VLP models are usually fine-tuned for downstream tasks with task-specific optimizations, considering varying datasets, objectives and other factors. The complexity of multi-modalities introduces more intricate information. Models tend to extract any available information to make decisions, even those that do not accurately reflect the true semantic essence of data \cite{zhang2021adversarial,ilyas2019adversarial,hendrycks2021natural,qin2022understanding}. The specialized factors of VLP models amplify this and make models more prone to relying on specific features and regions to associate images and texts. When these models are used as source models, attackers often generate adversarial examples that place greater emphasis on model-specific features and attended regions, resulting in overfitting to the source model and ultimately reducing transferability to other models.

In this paper, we focus on exploiting model-independent characteristics and vulnerabilities of images to guide the generation of adversarial examples, minimizing dependency on and susceptibility to the source models. A simple yet highly effective and transferable attack method is developed, termed the Meticulous Adversarial Attack (MAA). MAA refines adversarial examples primarily for the image modality to disrupt the understanding of image-text relationships across diverse models by augmenting low-level image details. Our approach is partially grounded in a well-acknowledged insight: \textit{tailored perturbations to each individual image tend to be more potent than applying uniform perturbations across all images} \cite{poursaeed2018generative,naseer2019cross}. With this in mind, MAA exploits representative and fine-grained characteristics and inherent vulnerabilities of original images to facilitate the generation of targeted perturbation. This approach substantially reduces over-reliance on the patterns/features that are generated specifically to the source model, thereby greatly enhancing the effectiveness and transferability of adversarial examples.

Specifically, we introduce a novel resizing and sliding crop (RScrop) technique, seamlessly integrated with a multi-granularity similarity disruption (MGSD) strategy. Essentially, similar to \cite{yin2023vlattack,ganeshan2019fda}, the MGSD strategy enlarges the feature distances between adversarial examples and their original counterparts across various layers and components of the model. Low-level layers and components process local regions and detailed features, while high-level layers and components capture more abstract, semantic information. However, MGSD is restricted by the fixed-size input and local region processing in existing VLP models, which cannot effectively focus on more detailed aspects and their connections. For instance, in vision transformers, patch embedding is learnt using non-overlapping patches, where the boundary areas between adjacent patches are often ignored, resulting in the loss of crucial contextual information. This limitation prevents the model from capturing fine-grained local dependencies or recognizing patterns that span across patch borders. Consequently, this lack of continuity can hinder the modelâ€™s ability to represent subtle textures, edges, and complex spatial relationships present in the image. Though the kernels applied in CNNs can slide across an image with overlapping areas, their fixed size and localized receptive fields inherently limit their ability to capture more local information. To capture a fine-grained as well as comprehensive view of images to distill their representative characteristics for performing an effective MGSD, RScrop is proposed, supported by scale- and translation-invariant properties of DNNs \cite{lin2019nesterov,dong2019evading}. It employs resizing and cropping operations, to scale up adversarial examples and feed them in a sliding manner into the model to enable exploration of more fine-grained features and local regions. We also maximize the embedding distance between image and text pairs to widen the modality gap for generating both adversarial images and texts. 

The benefits of this approach are manifold. First, augmenting through scaling and sliding acts as a magnifier, enabling the model to attend to intricate local details and previously overlooked boundary regions of adjacent patches in individual images. This meticulous focus on fine-grained elements enhances spatial coherence and captures subtle variations, ultimately improving the model's sensitivity to nuanced patterns and contextual dependencies within the image itself, independent of model- and task-specific objectives. Generating adversarial examples based on these augmented data can successfully alleviate the reliance on model-specific patterns, therefore relieving the overfitting issue. Second, intermediate features especially low-level features are generally more generalized and shared across various models. As a result, MAA promotes more sample-dependent and model-generic adversarial examples, improving transferability. Third, extensive experiments demonstrate that MAA achieves notable improvements over existing state-of-the-art techniques in terms of adversarial transferability. Last but not least, sophisticated parameter studies are undertaken to provide a comprehensive and in-depth analysis of model performance, shedding light on critical design choices and fostering the development of more refined models in future work. 

\section{Related work}\label{Rel}

\subsection{Vision-Language Pre-trained Models}

Vision-language pre-trained (VLP) models play a crucial role in advancing the understanding of visual and textual information and their interactions. By leveraging large-scale unlabeled datasets and self-supervised learning techniques, VLP Models can learn rich, generalized representations of both visual and linguistic data. This transfer learning capability enables them to be fine-tuned for a wide range of tasks with relatively small datasets, including multi-modal retrieval, zero-shot learning, image captioning, visual question answering, and visual entailment \cite{radford2021learning,li2022blip,li2021align,yang2022vision}. Notable VLP methods include CLIP \cite{radford2021learning}, BLIP \cite{li2022blip}, ALBEF \cite{li2021align}, and TCL \cite{yang2022vision}. These methods primarily leverage multi-modal contrastive learning to align image-text pairs. Specifically, CLIP employs unimodal encoders to project data from different modalities into a unified feature space. BLIP \cite{li2022blip} refines noisy captions to enhance learning effectiveness. ALBEF \cite{li2021align} and  TCL \cite{yang2022vision} both utilize a multi-modal encoder to learn joint representations for images and texts.  ALBEF \cite{li2021align} focuses on inter-modal relationships while  TCL \cite{yang2022vision} considers both intra- and inter-modal relationships.

\subsection{Adversarial Attack}

With the widespread adoption of Deep Neural Networks, it is essential to evaluate their robustness to ensure their reliability in real-world applications, which often involve uncertainty and potential threats. Adversarial attacks are a prominent method used to assess this robustness, which aim to mislead model predictions by introducing imperceptible perturbations into the data \cite{szegedy2014intriguing, zhang2023proactive, zhang2021privacy, madry2018towards, moosavi2017universal}. Traditional methods typically focus on specific tasks and unimodal cases, such as image classification. For image attacks, most techniques learn pixel-level perturbations, whereas text attacks often involve replacing or removing keywords, or performing text transformations \cite{li2020bert, jin2020bert, iyyer2018adversarial, naik2018stress, ren2019generating}. Recently, there has been growing interest in multi-modal vision-language scenarios. For instance, \cite{zhang2023proactive, wang2021prototype, zhu2023efficient} address image-text retrieval by increasing the embedding distance between adversarial and original data pairs. \cite{xu2018fooling} learn image perturbations by minimizing the distance between the output and the target label while maximizing the difference from the original label for the visual question answering task. More introduction can be found in \cite{zhangsurvey}.

% To address this challenge and promote transferability, one approach involves using an 

\noindent\textbf{Adversarial transferability.} Early attack methods typically assume a white-box setting, where all necessary information for generating adversarial examples, including target models and tasks, is available. However, in real-world scenarios, such comprehensive information is often unavailable. To address this challenge, one approach uses an ensemble of models as the victim model during training \cite{liu2016delving, dong2018boosting, dong2019evading, xiong2022stochastic}, based on the intuition that adversarial examples effective against a diverse set of models are likely to mislead more models. However, assembling such a model ensemble can be difficult. Another approach utilizes momentum-based methods \cite{dong2018boosting, long2024convergence, lin2019nesterov, inkawhich2019transferable} to stabilize gradient updates and avoid poor local maxima, though this may also divert perturbations from effective paths. Data augmentation \cite{xie2019improving, fang2022learning, wei2023enhancing, wang2024boosting, wang2021admix} increases data diversity, helping to prevent overfitting to specific models. Transferability is particularly challenging for attacks on VLP models, due to the unpredictable fine-tuning process. To enhance transferability, \cite{zhang2022towards,lu2023set,zhang2024universal} suggests increasing the gap between adversarial data and original image-text pairs. Some of these methods also focus on generating diverse image-text pairs through scale-invariant transformations \cite{lu2023set} and ScMix augmentations \cite{zhang2024universal}. Additionally, recent approaches consider the local utility of adversarial examples \cite{yin2023vlattack} or perturbations \cite{zhang2024universal}. However, \cite{yin2023vlattack} enlarges block-wise similarity between samples, which is constrained by the block size, failing to comprehensively capture local regions and their interactions. \cite{zhang2024universal} does not consider the characteristics and vulnerabilities of the original sample. As a result, these methods cannot ensure the effectiveness and transferability of adversarial attacks.

\section{Methodology}

\subsection{Preliminaries}

Let $F_s$ represent an available source VLP model consisting of an image encoder $f_\text{img}$ and a text encoder $f_\text{txt}$, which learn feature representations for images and texts, respectively. Given an image-text pair $({\vx}, {\vt})$, the objective is to generate adversarial examples ${\vx}^{\text{adv}}$ and ${\vt}^{\text{adv}}$ that can mislead the predictions of an unknown target model $F_t$. To ensure the perturbations remain imperceptible, for image perturbations, we use $l_\infty$-norm constraint: $||{\vt}^{\text{adv}} - {\vt}||_\infty \leq \epsilon_\text{img}$. For text perturbations, we restrict the number of words that can be modified in the sentence, denoted as $\epsilon_\text{txt}$.

\subsection{Meticulous adversarial attack}

%-------------------------------------------------------------------------- 
\begin{figure*}
\center
  \includegraphics[width=0.9\textwidth]{frame.pdf}
   \caption{An illustration of the proposed (a) RScrop augmentation and (b) MAA method. For RScrop, we use the ViT-based model as an example, where images are processed patch-wise. RScrop zooms in on adversarial examples and applies them in a sliding manner along each dimension (with the x-dimension as an example) to capture more fine-grained local regions and their interconnections. The crop window shifts by a small step size and moves to adjacent non-overlapping areas relative to its previous position, repeating this process until the entire image is covered. MAA enlarges the feature distance between adversarial images and the original images across various layers and components of the model, while also maximizing the cross-modal gap.}
  \label{fig1}
  
\end{figure*}
%--------------------------------------------------------------------------


MAA employs a straightforward RScrop technique in conjunction with a multi-granularity similarity disruption (MGSD) strategy. An illustration of the framework can be found in Figure \ref{fig1}.

The MGSD strategy enhances adversarial images by enlarging the feature distance from the original images across various layers and components of the model. Specifically, low-level layers and components process local regions and extract detailed features, e.g., patch processing of ViTs, which enable us to enhance the local utility of adversarial examples. High-level ones capture abstract, semantic information. By targeting these diverse granularities and hierarchical levels, we can effectively uncover the characteristics and vulnerabilities of samples and also make the generation of adversarial examples more rely on samples and less reliant on information specialized for source models to associate images and tests. Moreover, low-level layers and components are generally less model-specific. These contribute to reducing overfitting and enhancing the transferability. However, common backbone networks used in VLP models, such as ViTs and CNNs, are constrained by fixed-size processing of inputs and limited local regions. Relying solely on these networks to generate adversarial examples fails to fully explore the representative and intrinsic characteristics and vulnerability of samples. Specifically, these networks process fixed-size inputs, which can hinder the understanding of features of local regions. While targeting low-level layers that focus on local regions can alleviate this, it does not completely solve the problem. For instance, ViTs process images in non-overlapping patches, which overlook important local relationships between neighboring patches. Similarly, CNNs utilize fixed-size convolutional filters, also imposing restrictions on feature exploration. Furthermore, they cannot extract local information from high-level layers and components, limiting the fine-grained optimization of adversarial samples. As different models are prone to extract some distinct features and focus on different regions to associate images and texts, the fixed-size constraints would make generated adversarial examples rely on model-specific features and regions, hindering the transferability across different models. 

To overcome fixed-size constraints and explore more comprehensive, diverse, and detailed information, we propose RScrop, which involves two operations: scaling and cropping. As illustrated in Figure \ref{fig1}(a). First, original adversarial images are scaled up along each dimension, enabling a focus on fine-grained local regions. Second, we systematically crop local regions by sliding a crop window across the image, starting from an initial point in each dimension. The window shifts by a predefined amount along each dimension to ensure comprehensive coverage of the image with more regions and their connections considered. Specifically, after the initial crop, the window is shifted by a step size randomly selected to be smaller than the dimension size of the patch or the convolutional filter of the first layer, ensuring that more local regions and their connections are considered. We then move to adjacent non-overlapping areas from the previous crop (excluding the small-step cropped regions), applying the same operation until the entire image is covered. The shift step length for $i$-th step relative to the initial point in different dimensions can be formulated by $L_{x/y}^i= (i/2)*l_{x/y} + (i\%2)* \alpha_{x/y}(i), \alpha_{x/y}^i = UniformDiscrete(\beta_1, \beta_2)$, where $x$ and $y$ denote $x$- and $y$- dimension, and $\beta_1 \ \text{and} \ \beta_2$ are smaller than the size of dimension size of the patch or the convolutional filter, i.e., $l_{x/y}$. Supported by scale- and translation-invariant properties of DNNs \cite{lin2019nesterov,dong2019evading}, this method ensures thorough processing of local areas and their relationships, providing complete coverage of the image across various layers and complements. 

% $L_{x/y}= (i/2)*l_{x/y} + (i\%2)* \alpha_{x/y}(i), \alpha_{x/y}(i) = UniformDiscrete(\alpha, \beta)$, where $x$ and $y$ denote $x$- and $y$- dimension, and $\alpha \ \text{and} \ \beta$ are smaller than the size of dimension size of the patch or the convolutional filter, i.e., $l_{x/y}$.}

Formally, after Rscrop, we can obtain a set of transformed adversarial images $\mathcal{R}({\vx}^{\text{adv}}) = \{{\vx}_1^{\text{adv}}, {\vx}_2^{\text{adv}}, ..., {\vx}_k^{\text{adv}}\}$. We maximize the feature distance between all these adversarial images and original images at various layers and components as follows:
\begin{equation}\label{eq1-1}
\begin{split}
& \underset{{\vx}^{\text{adv}}}{\min } \mathcal{L}_{1} = \sum _{i=1}^{N} \sum _{x^{\prime} \in \mathcal{R}({\vx}^{\text{adv}})} (\operatorname{cos} (f_\text{img}^{i}(\vx^{\prime}),f^{i}_\text{img}({\vx}) ) ),\\
% & \underset{{\vx}^{\text{adv}}}{\min } \mathcal{L}_{1} = \sum _{i=1}^{N} \sum _{k=1}^{K} (\operatorname{cos} (f_\text{img}^{i}(\hat{\vx}_k^{\text{adv}}),f^{i}_\text{img}({\vx}) )  ),\\
\end{split}
\end{equation}
where $N$ is the number of layers and components, and $f_\text{img}^{i}$ is the representations output from the $i$-th layer or component. For ViT-based VLP models, features would include those output from self-attention modules and the final output layer, while for ResNet-based VLP models, we extract features from residual blocks and the final output layer. By focusing on features across different layers and different scales of images, we are able to explore fine-grained vulnerabilities of samples.

The RScrop also creates diverse image-text pairs, which help better explore cross-model interactions for transferrable attacks. We enlarge the feature distance between adversarial images and their original paired texts to comprehensively disturb image-text connections:
\begin{equation}\label{eq1-2}
\begin{split}
& \underset{{\vx}^{\text{adv}}}{\min } \mathcal{L}_{2} = \sum _{i=1}^{N} \sum _{x^{\prime} \in \mathcal{R}({\vx}^{\text{adv}})} (\operatorname{cos} ( f_\text{img} (\hat{\vx}_k^{\prime}), f_\text{txt}(\vt) )).\\
\end{split}
\end{equation}

 % $\hat{\vx}^{\text{adv}} = R({\vx}^{\text{adv}})$ is the augmented data generated by RScrop, i.e., $R(\cdot)$ . $K$ is the number of augmented data.

% We can also incorporate information from the text modality to disturb the inter-modal connections by enlarging the feature distance between adversarial images and their original paired texts: 

The overall objective for learning adversarial images is as follows:
\begin{equation}\label{eq1-3}
\begin{split}
& \underset{{\vx}^{\text{adv}}}{\min } \mathcal{L}_\text{img} = \mathcal{L}_{1} + \mathcal{L}_{2}.\\
\end{split}
\end{equation}

For attacking the text modality, we use the commonly used BERT-Attack method \cite{li2020bert} to generate adversarial texts by maximizing the feature distance from their original image-text pairs:
\begin{equation}\label{eq1-4}
\begin{split}
& \underset{{\vt}^{\text{adv}}}{\min } \mathcal{L}_\text{txt} = \operatorname{cos} ( f_\text{txt}({\vt}^{\text{adv}}), f_\text{txt}({\vt} ) ) + \operatorname{cos} ( f_\text{txt}({\vt}^{\text{adv}}), f_\text{img}({\vx}) ).\\
\end{split}
\end{equation}

BERT-Attack first identifies the most important word in each sentence by replacing each word in the sentence with $[\text{MASK}]$ one at a time and ranking the feature distance between each modified sentence with the original sentence and the paired image. The most important word would be replaced by a semantic-consistent word to ensure visually plausible attacks. For the attack effectiveness, BERT is used to generate a set of candidates and the one that can fulfil Eq. \ref{eq1-4} would be selected to replace the original word to realize an attack.


\section{Experiments}
\label{Exp}

\subsection{Settings}
\noindent\textbf{Datasets and Tasks.} Three datasets are used: Flickr30K \cite{plummer2015flickr30k}, MSCOCO \cite{lin2014microsoft}, and RefCOCO+ \cite{yu2016modeling}. The splits for training and testing follow recent works \cite{zhang2022towards,lu2023set,yin2023vlattack,zhang2024universal}. We focus on three tasks: image-text retrieval, image captioning, and visual grounding. For image-text retrieval, Flickr30K \cite{plummer2015flickr30k} and MSCOCO \cite{lin2014microsoft} are used. For the image captioning task and the visual grounding task, MSCOCO and RefCOCO+ datasets are used, respectively. 

\noindent\textbf{Models.} We test MAA and all compared methods on four kinds of widely-used VLP models: CLIP \cite{radford2021learning}, ALBEF \cite{li2021align}, TCL \cite{yang2022vision}, and BLIP \cite{li2022blip}. For CLIP, different image encoders are utilized, including vision transformers (ViT-B/16 and ViT-L/14 \cite{dosovitskiy2020image}) and CNNs (ResNet50 and ResNet101 \cite{he2016deep}). The text encoder is a 6-layer transformer. For ALBEF and BLIP, we choose the variant that consists of a ViT-B/16 as the image encoder and a 6-layer transformer as the text encoder for the attack.

\noindent\textbf{Implementation details.} For fundamental experiments, the perturbation magnitude is set to $\epsilon_x = 4/255$ for images and $\epsilon_t = 1$ for text. MAA is also evaluated across various perturbation magnitudes. The optimization problem for image perturbations is addressed using Projected Gradient Descent (PGD) \cite{madry2018towards} with $T = 40$ iterations and a step size of $\epsilon_x/T \times 2.25$. The batch size is configured to 4. Every 10 iterations, scaling ratios would be changed by randomly selecting from the set $\{1.25, 1.5, 1.75, 2\}$ for each dimension to optimize the adversarial images.

\noindent\textbf{Baselines.} Several state-of-the-art approaches are selected for comparison, including Co-Attack \cite{zhang2022towards}, SGA \cite{lu2023set}, VLATTACK \cite{yin2023vlattack}, ETU \cite{zhang2024universal}, and VLPTransferAttack \cite{gao2025boosting}. ETU only generates universal adversarial images, while others produce adversarial images and texts. We also include baseline methods such as PGD and BERT-Attack \cite{li2020bert}.

\noindent\textbf{Evaluation metric.} In image-text retrieval, the attack success rate (ASR) is used to evaluate all methods, calculated as the percentage of adversarial examples that successfully deceive the model. For other tasks, we compare the performance of target models before and after attacks.

%-------------------------------------------------------------------------
\begin{table*}[!t]
\centering % Use only \centering to center the table
\caption{Attack success rate $(\%)$ on the image-text retrieval task. CLIP$_\text{ViT-B/16}$ is adopted as the source model for training, while the target models include CLIP$_\text{ViT-B/16}$, CLIP$_\text{ViT-L/14}$, CLIP$_\text{ResNet50}$, CLIP$_\text{ResNet101}$, ALBEF, and TCL. The grey background indicates the white-box attack results. \textbf{Bold} indicates the best results.}
\label{tab_trans_1}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|ccc|ccc|ccc|ccc} % Use standard column specifiers without @{}

\toprule
\multicolumn{2}{c}{\textbf{Dataset}}  & \multicolumn{6}{c}{\textbf{Flickr30K}}     & \multicolumn{6}{c}{\textbf{MSCOCO}}  \\

\cmidrule(lr){1-2}\cmidrule(lr){3-8} \cmidrule(lr){9-14} % Define custom midrules to separate the groups
& & \multicolumn{3}{c|}{\textbf{Image-to-Text}} & \multicolumn{3}{c|}{\textbf{Text-to-Image}} & \multicolumn{3}{c|}{\textbf{Image-to-Text}} & \multicolumn{3}{c}{\textbf{Text-to-Image}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
Target & {Method} & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\

\midrule
\multirow{7}{*}{CLIP$_\text{ViT-B/16}$} & 
PGD    & \cellcolor[HTML]{EFEFEF}{91.90}     & \cellcolor[HTML]{EFEFEF}{82.87}    & \cellcolor[HTML]{EFEFEF}{78.66}  
& \cellcolor[HTML]{EFEFEF}{98.78} & \cellcolor[HTML]{EFEFEF}{96.22} & \cellcolor[HTML]{EFEFEF}{94.33} 
& \cellcolor[HTML]{EFEFEF}{97.55}     & \cellcolor[HTML]{EFEFEF}{94.29}    & \cellcolor[HTML]{EFEFEF}{91.67}  
& \cellcolor[HTML]{EFEFEF}{96.38} & \cellcolor[HTML]{EFEFEF}{91.72} & \cellcolor[HTML]{EFEFEF}{88.42}   \\
{} & {BERT-Attack}  & \cellcolor[HTML]{EFEFEF}{28.34}     & \cellcolor[HTML]{EFEFEF}{11.63}    & \cellcolor[HTML]{EFEFEF}{6.71}  
& \cellcolor[HTML]{EFEFEF}{39.05} & \cellcolor[HTML]{EFEFEF}{24.06} & \cellcolor[HTML]{EFEFEF}{17.4} 
& \cellcolor[HTML]{EFEFEF}{55.25}     & \cellcolor[HTML]{EFEFEF}{37.26}    & \cellcolor[HTML]{EFEFEF}{28.93}  
& \cellcolor[HTML]{EFEFEF}{57.86 } & \cellcolor[HTML]{EFEFEF}{45.05 } & \cellcolor[HTML]{EFEFEF}{38.73}   \\
{}  & {Co-Attack}  & \cellcolor[HTML]{EFEFEF}{97.73}  &\cellcolor[HTML]{EFEFEF}{94.29}  &\cellcolor[HTML]{EFEFEF}{91.67}  
& \cellcolor[HTML]{EFEFEF}{98.83} & \cellcolor[HTML]{EFEFEF}{96.17} & \cellcolor[HTML]{EFEFEF}{94.37}
& \cellcolor[HTML]{EFEFEF}{99.35}     & \cellcolor[HTML]{EFEFEF}{98.53}    & \cellcolor[HTML]{EFEFEF}{97.54}  
& \cellcolor[HTML]{EFEFEF}{99.6} & \cellcolor[HTML]{EFEFEF}{98.84} & \cellcolor[HTML]{EFEFEF}{98.14}   \\
{}  &{SGA}  & \cellcolor[HTML]{EFEFEF}{99.53}     & \cellcolor[HTML]{EFEFEF}{97.92}    & \cellcolor[HTML]{EFEFEF}{96.54}  & \cellcolor[HTML]{EFEFEF}{99.73} & \cellcolor[HTML]{EFEFEF}{98.88} & \cellcolor[HTML]{EFEFEF}{97.62} & \cellcolor[HTML]{EFEFEF}{91.9}     & \cellcolor[HTML]{EFEFEF}{82.87}    & \cellcolor[HTML]{EFEFEF}{78.66}  & \cellcolor[HTML]{EFEFEF}{91.14} & \cellcolor[HTML]{EFEFEF}{81.65} & \cellcolor[HTML]{EFEFEF}{79.96}  \\
{}  &{ETU}  & \cellcolor[HTML]{EFEFEF}{97.70}     & \cellcolor[HTML]{EFEFEF}{94.6}    & \cellcolor[HTML]{EFEFEF}{78.8}  
& \cellcolor[HTML]{EFEFEF}{90.32} & \cellcolor[HTML]{EFEFEF}{84.10} & \cellcolor[HTML]{EFEFEF}{59.32}
& \cellcolor[HTML]{EFEFEF}{91.55}     & \cellcolor[HTML]{EFEFEF}{80.32}    & \cellcolor[HTML]{EFEFEF}{78.61}  
& \cellcolor[HTML]{EFEFEF}{90.12} & \cellcolor[HTML]{EFEFEF}{98.65} & \cellcolor[HTML]{EFEFEF}{85.20 }  \\
{}  &{VLATTACK}  & \cellcolor[HTML]{EFEFEF}{{99.86}}     & \cellcolor[HTML]{EFEFEF}{\textbf{99.58}}    & \cellcolor[HTML]{EFEFEF}{99.39}  
& \cellcolor[HTML]{EFEFEF}{\textbf{99.92}} & \cellcolor[HTML]{EFEFEF}{\textbf{99.74}} & \cellcolor[HTML]{EFEFEF}{{99.52}}& \cellcolor[HTML]{EFEFEF}{\textbf{100.00}}     
& \cellcolor[HTML]{EFEFEF}{{99.87}}    & \cellcolor[HTML]{EFEFEF}{\textbf{99.79}}  & \cellcolor[HTML]{EFEFEF}{\textbf{100.0}} & \cellcolor[HTML]{EFEFEF}{\textbf{99.96}} & \cellcolor[HTML]{EFEFEF}{\textbf{99.92}}  \\
{}  &{VLPTransferAttack}
& \cellcolor[HTML]{EFEFEF}{\textbf{99.98}} & \cellcolor[HTML]{EFEFEF}{99.53} & \cellcolor[HTML]{EFEFEF}{\textbf{99.42}}  
& \cellcolor[HTML]{EFEFEF}{\textbf{99.99}} & \cellcolor[HTML]{EFEFEF}{\textbf{99.76}} & \cellcolor[HTML]{EFEFEF}{\textbf{99.53}}
& \cellcolor[HTML]{EFEFEF}{{99.93}}     & \cellcolor[HTML]{EFEFEF}{\textbf{99.90}}    & \cellcolor[HTML]{EFEFEF}{99.77}  
& \cellcolor[HTML]{EFEFEF}{99.95} & \cellcolor[HTML]{EFEFEF}{99.88} & \cellcolor[HTML]{EFEFEF}{99.80}  \\
 & MAA  & \cellcolor[HTML]{EFEFEF}{99.51}     & \cellcolor[HTML]{EFEFEF}{99.07}    & \cellcolor[HTML]{EFEFEF}{98.58}  & \cellcolor[HTML]{EFEFEF}{99.61} & \cellcolor[HTML]{EFEFEF}{99.21} & \cellcolor[HTML]{EFEFEF}{98.89}& \cellcolor[HTML]{EFEFEF}{99.92}     & \cellcolor[HTML]{EFEFEF}{{99.87}}    & \cellcolor[HTML]{EFEFEF}{99.76}  & \cellcolor[HTML]{EFEFEF}{99.93} & \cellcolor[HTML]{EFEFEF}{99.89} & \cellcolor[HTML]{EFEFEF}{99.85} \\
 
\midrule
\multirow{7}{*}{CLIP$_\text{ViT-L/14}$} & {PGD}    & {7.48}     & 1.97    & {0.41} &   16.4 & {6.31} & 3.79    
& {20.68}   & 9.79  & 7.13   & 28.97  & 15.98 & 12.4   \\
{} & {BERT-Attack}  & {24.79}  & 9.76  & {5.59}  &   39.05 & {23.01} & 17.07    
& {51.05}   & 32.89  & 25.82    & 59.63  & 44.27 & 37.56   \\
{}  & {Co-Attack}  & {27.80}  & 10.38   & {5.69} &44.50 & {27.66} & 19.93   
& {53.95}   & 36.5  & 28.31 & 64.11  & 49.12  & 42.05  \\
{}  &{SGA}  & {32.14}  & 15.58   &{9.55}  & 47.83 & {30.13} & 23.07    
& {57.34}   &40.82  &33.21 &66.32 & 50.83 &43.81  \\
{}  &{ETU}  & {8.22}  &1.14   &0.20  &16.04  &5.58 &3.27  
& 20.18   & 10.48  & 7.89 & 25.46 & 14.55 & 10.33  \\
{}  &{VLATTACK}  & {30.49}  & 12.88  &{6.00}  & 42.69 & {26.47} & 19.12   
&  {56.12}   & 37.36  &29.76 & 64.23 & 49.09 & 42.02  \\
{}  &{VLPTransferAttack}
& {42.54}   & {23.99}  &{14.55}  & {53.82} & {36.24} & {28.39}  
&  {64.82}   & {48.98}  &{40.54} & {71.85} & {58.66} & {51.77} \\
& {MAA}  & {\textbf{54.36}}    & \textbf{33.33}  
& {\textbf{25.71}} & \textbf{63.02} & {\textbf{46.09}} & \textbf{37.9}  
& {\textbf{80.66}}   & \textbf{67.79}  & \textbf{61.22}  &\textbf{83.84} &  \textbf{72.83} & \textbf{66.98}\\

\midrule
\multirow{7}{*}{CLIP$_\text{ResNet50}$} & {PGD}    & {15.2}     &4.86    & {2.27} &24.39 & {9.19} &5.47   
& {28.97}   &15.47  &10.85   &38.38  &22.56 &16.95   \\
{} & {BERT-Attack}  & {35.25}  &14.69  & {8.03}  &44.73 & {27.7} &20.28   
& {59.05}   & 42.18  & 32.88    &67.43  & 52.64 & 45.84   \\
{}  & {Co-Attack}  & {39.72}  &17.55   & {10.81} &50.39 & {31.80} &25.16   
& {63.67}   & 45.8  &37.27 &72.12  &56.85  &49.83   \\
{}  &{SGA}  & {43.42}  &22.94   &15.55  &56.71 &36.46 &28.80   
& {68.57}   & 53.06  & 43.58 &75.51 & 61.77 & 54.56  \\
{}  &{ETU}  & {12.52}  &3.49   &2.27  & 22.74 & 7.47 &5.06    
& 27.87   &15.18  &10.94  &37.09  &21.71 &16.07  \\
{}  &{VLATTACK}  & {44.19}  & 20.72   &{14.32}  & 54.31 & {35.01} & 27.74   
& {70.21}   & 52.16  & 43.31 & 75.36 & 60.88 & 53.29  \\
{}  &{VLPTransferAttack}
& {52.65}  &{ 31.48}   &{22.47}  & {63.34} & {43.52}  & {35.63}   
&  {76.35}   &{61.39} & {53.23} & {80.82} &{68.29} & {70.10}     \\
& {MAA}  & {\textbf{77.14}}    & \textbf{63.53}  
& {\textbf{53.14}} & \textbf{80.58} & {\textbf{65.77}} & \textbf{58.03}  
& {\textbf{92.68}}   & \textbf{86.13}  & \textbf{81.75}  &\textbf{92.86} &  \textbf{86.46} & \textbf{82.28}    \\

\midrule
\multirow{7}{*}{CLIP$_\text{ResNet101}$} & {PGD}    & {8.94}     & 1.69    & {1.03} &   12.42 & {4.54} & 2.76    
& {14.88}   & 7.86  & 5.87   &21.07  &11.4 &8.85   \\
{} & {BERT-Attack}  & 30.27  &11.63 & 5.77  &   37.39 &24.92 & 18.59    
& 52.39   & 35.55 &28.76    & 58.64  & 46.29 & 39.39   \\
{}  & {Co-Attack}  & {35.93}  & 13.85   & {8.75} & 44.52 & {29.28} & 22.81   
& {58.72}   & 40.71  & 32.52 & 65.41  &52.21  &45.03   \\
{}  &{SGA}  & {44.01}  &21.99   &14.11  &51.19 &33.36 &26.52   
& {61.83}   &47.62 &40.6 & 68.91 &56.48 & 49.58  \\
{}  &{ETU}  & {7.79}  & 2.01   &1.13  &12.42 &4.71 &2.85   
& 18.33  & 12.70  & 8.67 & 23.36 & 10.28 & 9.12 \\
{}  &{VLATTACK}  & {41.31}  & 18.50  &{10.81}  & 48.65 & {31.39} & 24.69    
& {62.93}   &46.86   & 38.61 &70.18 & 56.52 & 49.37  \\
{}  &{VLPTransferAttack}
& {51.64}   & {28.45}  &{19.74}   & {59.42} & {40.83} & {33.21}   
&  {70.86}   & {57.47} &{48.63} & {76.49} & {64.39} & {57.58}  \\
& {MAA}  & {\textbf{73.05}}    & \textbf{55.71}  
& {\textbf{45.31}} & \textbf{74.85} & {\textbf{60.97}} & \textbf{53.76}  
& {\textbf{90.03}}   & \textbf{81.56}  & \textbf{77.17}  &\textbf{90.2} &  \textbf{83.14} & \textbf{78.63}    \\

\midrule
\multirow{7}{*}{ALBEF} & {PGD}    & {2.82}  &0.20  & {0.30} &5.78 & {1.85} &1.03    
& {8.35}   &3.59  &1.87  &12.91  &5.40 &3.43   \\
{} & {BERT-Attack}  & {9.18}  & 1.20  & 0.40  &22.64 & 11.18 & 8.11    
& 28.38   & 12.71 & 8.21    & 41.16  & 26.17 &20.77   \\
{}  & {Co-Attack}  & {11.42}  & 1.80   & {0.50} & 25.30 & {12.63} & 9.32   
& {32.25}   &14.89  &9.80 &44.07  &28.32  &22.68  \\
{}  &{SGA}  & {14.86}  &3.31   &{1.40}  &29.56 & {14.48} &10.55    
& {36.45}   &18.99  &13.01 &46.91 &30.74 &24.27  \\
{}  &{ETU}  & {1.67}  &0.30   &0.20  & 5.45 &1.35 & 0.91   
& 12.12   &6.98 & 8.34 & 13.65 & 7.47 & 4.50  \\
{}  &{VLATTACK}  & {11.29}  & 2.51   &{1.00}  & 28.22 & {13.74} & 10.19    
& {34.57}   &17.06  & 11.22 & 46.83 & 31.22 & 25.13  \\
{}  &{VLPTransferAttack}  & {30.28}  & {12.96}   &{6.14} & {42.88} & {25.45} & {19.17}  
&  {52.23}   & {30.39}  &{24.74} & {59.27} & {44.09}  & {34.21}  \\
& {MAA}  & {\textbf{31.80}}  & \textbf{14.53}  & {\textbf{11.00}} 
& \textbf{43.62} & {\textbf{25.57}} & \textbf{19.65}  
& {\textbf{53.36}}   & \textbf{33.42}  & \textbf{25.52}  &\textbf{60.06} &  \textbf{43.11} & \textbf{36.24}    \\

\midrule
\multirow{7}{*}{TCL} & {PGD}    & {5.16}     &0.30    & {0.10} &7.98 & {2.17} &1.34
& {10.45}   &4.10  &2.38   & 14.53 & 6.40 &4.11  \\
{} & {BERT-Attack}  &9.59 & 2.01  & 0.60  &24.05 & 11.89 &8.20   
& 29.15   & 13.45  &9.39    & 41.01  & 25.96  &19.93   \\
{}  & {Co-Attack}  & {12.63}  &3.02   & {0.70} &25.85 & {13.67} &9.36   
& {32.2}   &14.92  &10.45  &43.76  &27.92  &21.85   \\
{}  &{SGA}  & {16.26}  &3.62   &1.50 &30.66 &15.96 & 11.27   
& {37.06}   & 18.8  &12.85 &46.97 &31.25 &24.9  \\
{}  &{ETU}  & {12.43}  &6.33   &2.10  &17.33 & 8.92 & 6.11    
& 17.32   & 5.87  & 3.96 & 15.66 & 7.43 & 5.49  \\
{}  &{VLATTACK}  & {14.49}  & 3.62   &{1.40}  &30.23 & {16.04} &11.17   
& {35.87}   & 18.87  & 12.78 & 48.26 & 32.29 & 25.73  \\
{} &{VLPTransferAttack}  
& {30.66}  & {12.79} &{6.10}  & {42.79} & {26.52} & {19.20} 
&  {56.24}   & {37.39}  &{29.58} & {61.58} & {45.10} & {36.96} \\
& {MAA}  & {\textbf{41.10}}    & \textbf{22.31}  & {\textbf{16.93}} 
& \textbf{49.81} & {\textbf{31.34}} & \textbf{24.63}  
& {\textbf{60.63}}   & \textbf{39.84}  & \textbf{31.36}  &\textbf{62.45} &  \textbf{45.17} & \textbf{37.63}    \\

\bottomrule
\end{tabular}
}
\end{table*}
%-------------------------------------------------------------------------

\subsection{Task analysis}
\subsubsection{Results on the Image-Text Retrieval}

%-------------------------------------------------------------------------
\begin{table*}[!t]
\center
\caption{Attack success rate $(\%)$ regarding the average of R@1 in image-text retrieval on Flickr30K. Grey background highlights white-box attack results, and \textbf{bold} indicates the best performance.}
\label{tab_cross}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|cc|cc|cc|cc|cc|cc}
\toprule
\cmidrule(lr){1-14} % Define custom midrules to separate the groups

\multicolumn{2}{c}{\multirow{2}{*}{\textbf{Target Model}}} & \multicolumn{8}{c}{\textbf{CLIP}} & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{ALBEF}}} & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{TCL}}}\\
\cmidrule(lr){3-10} % Define custom midrules to separate the groups
& & \multicolumn{2}{c|}{$\text{ViT-B/16}$}  & \multicolumn{2}{c|}{$\text{ViT-L/14}$}& \multicolumn{2}{c|}{\text{RN50}}  &\multicolumn{2}{c|}{\text{RN101}} \\\hline

\midrule

{Source Model} & { Method} & I2T & T2I & I2T & T2I  & I2T & T2I & I2T & T2I & I2T & T2I & I2T & T2I \\ \hline

\multirow{7}{*}{CLIP$_\text{ResNet101}$} & {PGD}   & {2.09} & 6.80      & 8.34  &15.91    & 15.33    &23.7  
& \cellcolor[HTML]{EFEFEF}{93.23}  & \cellcolor[HTML]{EFEFEF}{96.54}      & 1.77  &4.40 &3.90  &6.88 \\
{} & {BERT-Attack}   & {27.12} &37.44    &25.28  &39.85    & 35.52    & 46.91  
& \cellcolor[HTML]{EFEFEF}{30.52}  & \cellcolor[HTML]{EFEFEF}{39.97}      &9.49  &23.25 &11.59  &24.36 \\
{} & {Co-Attack}    & {28.59} &40.34   &25.64  &41.62    &38.19    & 52.8 
& \cellcolor[HTML]{EFEFEF}{98.08}  & \cellcolor[HTML]{EFEFEF}{\textbf{98.52}}     & 9.38  &23.90 &11.70  &25.12\\
{}  & {SGA}   & {32.52} &44.30   &28.34  &43.17    &49.17    &61.10 
& \cellcolor[HTML]{EFEFEF}{97.80}  & \cellcolor[HTML]{EFEFEF}{97.84}    & {11.78}  &26.45  &14.75 &28.76 \\
{}  &{ETU}  & {5.32} & 7.47   & 7.55    & 10.12    & 13.70  & 16.32
& \cellcolor[HTML]{EFEFEF}{88.75}  & \cellcolor[HTML]{EFEFEF}{89.38}     & 4.25   & {7.40}   &5.65  & 8.92 \\
{}  &{VLATTACK}  & {32.88} & 43.23    & 27.61  & 40.69    & 48.79    & 59.76 
& \cellcolor[HTML]{EFEFEF}{{94.13}}  & \cellcolor[HTML]{EFEFEF}{96.78}     & 3.86   & {9.33}   & 8.54  & {13.71} \\  
{}  &{VLPTransferAttack}
& {35.28} &{46.71}   & {28.41} &{43.65}  & {60.54}  & {69.71} 
& \cellcolor[HTML]{EFEFEF}{{94.13}}  & \cellcolor[HTML]{EFEFEF}{96.78}   &{12.39}   & {27.98}  & {16.65} &  {30.07} \\
& {MAA}  & \textbf{{36.81}} &\textbf{47.97}    &\textbf{30.80}  &\textbf{45.26}  &\textbf{ 70.50 } & \textbf{78.77}  
& \cellcolor[HTML]{EFEFEF}{\textbf{98.34}}  & \cellcolor[HTML]{EFEFEF}{97.63}     &\textbf{13.56}   & \textbf{29.3}   & \textbf{17.18}  & \textbf{31.62} \\

\midrule
\multirow{7}{*}{ALBEF} & {PGD}  & {10.55} &14.88  &16.48 & 30.96  &15.45  &23.74   &22.11   &38.19   
& \cellcolor[HTML]{EFEFEF}{87.17}  & \cellcolor[HTML]{EFEFEF}{90.01} &25.18  & 30.14 \\
{} & {BERT-Attack}   & {25.03} &{35.66} &28.88 & 38.04  &37.12  & 56.90   &31.42   &45.35 
& \cellcolor[HTML]{EFEFEF}{32.53}  & \cellcolor[HTML]{EFEFEF}{50.61}  &8.22  & 19.57 \\
{} & {Co-Attack}    & {27.98} &38.50  &24.17  &38.50  & 33.33   &46.35  &33.72  &46.35 
& \cellcolor[HTML]{EFEFEF}{98.54}  & \cellcolor[HTML]{EFEFEF}{98.64}  &28.13  & 43.02 \\
{}  & {SGA}   & {38.65} & 47.49  & 31.17 &45.88  & 46.62  & 58.11   &42.15   &51.32   
& \cellcolor[HTML]{EFEFEF}{98.72}  & \cellcolor[HTML]{EFEFEF}{98.52}  &64.17  &69.19 \\
{}  &{ETU}  & 12.45 & 17.80  & 15.67 &18.49  & 16.31  & 25.83  &22.93  & 28.64   
& \cellcolor[HTML]{EFEFEF}{87.24}  & \cellcolor[HTML]{EFEFEF}{89.36}  & 28.09  & 32.31 \\
{}  &{VLATTACK}  & {36.56} &45.78  & {29.08 } &44.68  & 39.97  & 54.07   & 35.63   & 47.17   
& \cellcolor[HTML]{EFEFEF}{{94.13}}  & \cellcolor[HTML]{EFEFEF}{96.78}  & 41.62 & 52.81 \\
{}  &{VLPTransferAttack} 
& {38.93} &{48.90}  & {30.50}  &{45.12}  & {48.11}  &{58.64}   & {42.53}   & {52.37}
& \cellcolor[HTML]{EFEFEF}{{99.74}}  & \cellcolor[HTML]{EFEFEF}{\textbf{99.72}}  & {74.24} &{74.48} \\
{}  &{MAA}
& \textbf{{39.14}} &\textbf{49.77}  & \textbf{32.39} &\textbf{ 47.29}  &\textbf{51.09}  &\textbf{ 60.99}   & \textbf{43.68}  &\textbf{ 54.99}
& \cellcolor[HTML]{EFEFEF}{\textbf{100.00}}  & \cellcolor[HTML]{EFEFEF}{\textbf{99.95}}  &\textbf{ 75.87}   &\textbf{76.17}\\

\midrule
\multirow{7}{*}{TCL} & {PGD} &9.33 &13.72   &9.20  &16.49   &23.39  &38.31 &16.35  &24.56     
&18.14  &24.67  & \cellcolor[HTML]{EFEFEF}{{94.13}}  & \cellcolor[HTML]{EFEFEF}{96.78}     \\
{} & {BERT-Attack}   &29.94 &39.21   &25.64  &39.56    &41.62  &59.57  &36.14  &48.78   
   &9.91  &23.64  & \cellcolor[HTML]{EFEFEF}{37.09}  & \cellcolor[HTML]{EFEFEF}{53.07}     \\
{} & {Co-Attack} &32.15 &41.66   & 27.24 &42.27    &45.73  &61.02   &40.23  &50.46   
&45.15  &57.90   & \cellcolor[HTML]{EFEFEF}{{94.13}}  & \cellcolor[HTML]{EFEFEF}{96.78}     \\
{}  & {SGA}   &39.39 & 47.62   & 29.94 &44.88    & 48.22    &60.86   & 42.53   
& 51.94  & 68.82  &74.58   & \cellcolor[HTML]{EFEFEF}{99.12}  & \cellcolor[HTML]{EFEFEF}{99.98}     \\
{}  &{ETU}  & 11.89 & 16.43   & 24.23  & 35.67    & 20.01    & 27.56  & 19.72 
&27.38  &20.13  &23.68   & \cellcolor[HTML]{EFEFEF}{91.02}  & \cellcolor[HTML]{EFEFEF}{89.37}     \\
{}  &{VLATTACK} & {36.44} & 21.20   & 31.41  &45.65    & 43.81    & 54.89   & 36.91   
& 47.86   & 34.20  & 46.80   & \cellcolor[HTML]{EFEFEF}{{94.13}}  & \cellcolor[HTML]{EFEFEF}{96.78}     \\
{}  &{VLPTransferAttack}
& {41.23} &{48.88}   & {32.39}  &{47.98}     &{52.49}     &{62.95}    & {47.13}   
&{57.14}    & {71.66}   &{75.29}  & \cellcolor[HTML]{EFEFEF}{{99.89}}  & \cellcolor[HTML]{EFEFEF}{99.70}     \\
& {MAA}  
& \textbf{{41.96}} & \textbf{50.71}  & \textbf{34.60}   & \textbf{50.00}   & {\textbf{55.30}}  
& \textbf{64.05}   & \textbf{49.30}  & {\textbf{57.67}}    &\textbf{72.78}   & {\textbf{75.65}}   
 &\cellcolor[HTML]{EFEFEF}{\textbf{100.00}} & \cellcolor[HTML]{EFEFEF}{\textbf{100.00}} \\
\bottomrule
\end{tabular}
}
\end{table*}
 %-------------------------------------------------------------------------
 
Image-text retrieval involves ranking the similarity between queries and data to return the most relevant results. Aligning with aforementioned experiment settings, we assess the transferability of all methods in this task across various datasets (i.e., Flickr30K and MSCOCO) and VLP models (i.e., CLIP$_\text{ViT-B/16}$, CLIP$_\text{ViT-L/14}$, CLIP$_\text{ResNet50}$, CLIP$_\text{ResNet101}$, ALBEF, and TCL) for both image-to-text (I2T) and text-to-image (T2I) retrieval. Adversarial images are resized to meet the input requirements of each model. For instance, adversarial images generated on CLIP models are resized from $224 \times 224$ to $384 \times 384$ to attack ALBEF and TCL, or vice versa. To observe the performance trend of the proposed MMA compared to baseline methods across various retrieval stages, we engage evaluation metrics R@1, R@5, and R@10. Two benchmark datasets are utilized, and the CLIP$_\text{ViT-B/16}$ model is employed as the source model for training. The corresponding results are reported in Table \ref{tab_trans_1}. To provide a comprehensive overview of the performance gap between the baseline methods and MMA, Table \ref{tab_cross} showcases the effectiveness of attacks based on all combinations of source-target model pairs using different attack generation methods. Given space constraints, we focus on presenting R@1 results for one of the benchmark datasets to highlight key trends succinctly.

It is evident that the compared methods experience significant performance degradation when attacking unseen models, highlighting their tendency to overfit. Specifically, adversarial examples perform better when the source and target models share similar objectives, training schemes and/or architectures, such as between CLIP models, compared to dissimilar ones, like between CLIP and other models. This discrepancy arises because different objectives, training schemes, and architectures lead to model-specific multimodal data processing. As a result, attackers depend on model-specific features to generate adversarial examples, resulting in limited abilities to disturb cross-modal relationships. In contrast, MAA explores the representative and intrinsic characteristics and vulnerabilities of original images, thereby reducing over-reliance on source models and fostering the development of more model-generic adversarial examples. Second, BERT-Attack demonstrates less overfitting to source models. This is likely because most VLP models share the same text encoder. So all compared multi-modal attack methods utilize BERT-Attack to enhance adversarial transferability. However, as shown in Table \ref{tab_comp}, when text perturbations are removed, these methods experience a notable performance drop. Additionally, text perturbations typically involve replacing, removing, or transforming keywords, tend to be conspicuous and easily detectable \cite{li2020bert, jin2020bert, iyyer2018adversarial, naik2018stress, ren2019generating}. These highlight the importance of enhancing attacks in the image modality. Third, ETU is a universal attack method that learns uniform adversarial perturbations for all data, making it independent of sample-specific characteristics and vulnerabilities. As a result, ETU achieves inferior performance compared to sample-specific methods. 

%------------------------------------------------------------------------
\begin{table*}[!t]
\caption{Performance on visual grounding under different attacks on RefCOCO+. CLIP$_\text{ViT-B/16}$ and ALBEF for image-text retrieval serve as the source model, while ALBEF built for visual grounding is used as the target model. ``Baseline'' refers to the target model's performance on clean data. Smaller values indicate better adversarial transferability, and \textbf{bold} highlights the best results.}
\label{tab_trans_2}
\centering
\resizebox{.65\linewidth}{!}{
\begin{tabular}{lccc|ccc}
\toprule
\multicolumn{1}{c}{{Source Model}}  & \multicolumn{3}{c}{{CLIP$_\text{ViT-B/16}$}}   & \multicolumn{3}{c}{ ALBEF}  \\\midrule
 {} & {Val  \ } & {TestA }   & {TestB }   & {Val } & {TestA }   & {TestB } \\ \hline
{Baseline}   & {51.24  \ \ \ }  &{56.71  \ \ \  } &{44.79 \ \ \  }     & {51.24  \ \ \ }  &{56.71  \ \ \  } &{44.79  \ \ \ }  \\
{PGD}   &{51.03  \ \ \ }  &{56.53  \ \ \ }  & {44.71  \ \ \ }   & {49.92 \ \ \ }  &{56.27 \ \ \ } &{40.89 \ \ \ }\\
 {BERT-Attack}   &{43.08 \ \ \ }  &{48.31  \ \ \ }  & {37.23  \ \ \ }  & {46.00 \ \ \ }  &{52.03 \ \ \ } &{35.84 \ \ \ }   \\
 {Co-Attack}   &{43.12 \ \ \  }  &{47.73  \ \ \ }  & {37.51 \ \ \ }  & {43.22 \ \ \ }  &{47.80 \ \ \ } &{35.08 \ \ \ } \\
{SGA}   &{45.04  \ \ \ }  &{49.86  \ \ \ } & {38.78  \ \ \ }   & {44.94 \ \ \ }  &{49.81 \ \ \ } &{36.61 \ \ \ }\\
{ETU}   &{50.16  \ \ \ }  &{55.12 \ \ \ }  & {43.61  \ \ \ }    & {48.32 \ \ \ }  &{52.64 \ \ \ } &{40.01 \ \ \ }\\
{VLATTACK}   &{46.52 \ \ \ }  &{50.65 \ \ \ } & {39.50 \ \ \ }  & {42.38 \ \ \ }  &{47.48 \ \ \ } &{35.52 \ \ \ } \\
{VLPTransferAttack}    &{44.69 \ \ \ }  &{49.72 \ \ \ } & {38.49 \ \ \ } & {42.76 \ \ \ }  &{48.09 \ \ \ } &{35.45 \ \ \ } \\
{MAA}  &{\textbf{41.09 \ \ \ } }  &{\textbf{45.55 \ \ \ }}  & {\textbf{35.77 \ \ \ } }  &{\textbf{41.35 \ \ \ }}  &{\textbf{46.70 \ \ \ }}  & {\textbf{33.40 \ \ \ }} \\
\bottomrule
\end{tabular}
}
\end{table*}

\subsubsection{Results on the Visual Grounding and Image Captioning}
The performance of various attacks on the visual grounding and image captioning tasks is presented in Tables \ref{tab_trans_2} and \ref{tab_trans_3}. Notably, the image captioning task only utilizes images as input. Since Co-Attack reduces to the PGD attack when text perturbations are omitted, we do not report PGD results separately. Visual Grounding aims to align textual descriptions with relevant objects or locations within the visual input. This task requires attackers to effectively disrupt the correlation between texts and fine-grained image contents. By means of RScrop and MGSD, the proposed method can find more fine-grained characteristics and vulnerabilities of data and break the intra- and inter-model relationships across different granularities and hierarchical levels, it achieves better performance than other methods. In the image captioning task, the goal is to generate descriptive textual information that accurately reflects the content of an image. This requires models to effectively identify objects, attributes, and contextual information. Through the sliding operation of RScrop, MAA effectively considers local regions and their dependencies to generate adversarial examples, thus enhancing the ability to prevent target models from recognizing visual elements. In addition, as the proposed method pays attention to the characteristics of each sample, it can prevent adversarial example generation from depending on model-specific features, further promoting transferability.

In summary, our MAA takes into account both global and fine-grained information, achieving superior performance across various tasks, whether in image-text retrieval that focuses on matching entire images to text or in visual grounding and image captioning that emphasize fine-grained content.


%------------------------------------------------------------------------- caption
\begin{table*}[!t]
\caption{ Performance in the image captioning task under various attacks on MSCOCO. CLIP$_\text{ViT-B/16}$ and ALBEF for image-text retrieval serve as the source model, while BLIP built for image captioning is used as the target model. ``Baseline'' refers to the target model's performance on clean data. Smaller values indicate better adversarial transferability, and \textbf{bold} highlights the best results.}%\vspace{1ex}
\label{tab_trans_3}
\centering
\resizebox{.9\linewidth}{!}{
\begin{tabular}{lccccc|cccccc}
\toprule  
\multicolumn{1}{c}{{Source Model}}  & \multicolumn{5}{c}{{CLIP$_\text{ViT-B/16}$}}  & \multicolumn{5}{c}{{ALBEF}}  \\\midrule
{} & B@4  & {METEOR}   & {ROUGE$\_\text{L}$}  & {CIDEr} & {SPICE}  & B@4  & {METEOR}   & {ROUGE$\_\text{L}$}  & {CIDEr} & {SPICE} \\ \hline
{Baseline}   & 39.28  &30.66 &59.63   &131.43 &23.46    & 39.28  &30.66 &59.63   &131.43 &23.46  \\
{Co-Attack}   & 38.32  &30.15 &58.94   &128.16 &23.02   & {34.12} &{27.96} &{55.88}   &{112.73}  &{20.81} \\
{SGA} &37.19  &29.70 &58.15   &124.36 &22.27    & {38.51}  &{30.24} &{59.05}   &{128.99} &{23.18} \\
{ETU} & 39.07  &30.47 &59.32   &130.24 &23.33     & {34.70}  &{27.66}  &{55.63}   &{128.57} &{22.21} \\
{VLATTACK} & 38.03  &30.07 &58.95   &127.64  &{22.90}  &{34.74} &{28.24}   &56.19  &{114.12} &{21.09} \\
{VLPTransferAttack} & {36.15}  &{29.01}  &{57.26}   &{120.58}  &{21.93}   &{28.63}  &{25.05} &{51.73}   &{92.59} &{18.34}  \\
{MAA}  & \textbf{33.26}  &\textbf{26.78} &\textbf{54.68}  &\textbf{107.51} &\textbf{19.83}  &\textbf{22.82}  &\textbf{21.88} &\textbf{47.85}   &\textbf{84.03} &\textbf{15.91} 
\\\bottomrule 
\end{tabular}
}
\end{table*}
 %-------------------------------------------------------------------------



 %------------------------------------------------------------------------
\begin{figure}[tbh]
\centering
\begin{subfigure}[CLIP$_\text{ViT-L/14}$]
{\includegraphics[angle=0, width=0.22\textwidth]{ViTL14_Image-to-Text.pdf}}
\end{subfigure}
\begin{subfigure}[CLIP$_\text{ViT-L/14}$]
{\includegraphics[angle=0, width=0.22\textwidth]{ViTL14_Text-to-Image.pdf}}
\end{subfigure}
\begin{subfigure}[ALBEF]
{\includegraphics[angle=0, width=0.22\textwidth]{ALBEF_Image-to-Text.pdf}}
\end{subfigure}
\begin{subfigure}[ALBEF]
{\includegraphics[angle=0, width=0.22\textwidth]{ALBEF_Text-to-Image.pdf}}
\end{subfigure}
\caption{Test accuracy on Flickr30K with varying image perturbation magnitudes. The source model is CLIP$_\text{ViT-B/16}$. The attack success rate (ASR, $\%$) of R@1 is reported.}%\medskip
\label{datamag_acc}
\end{figure}
%------------------------------------------------------------------------


\subsection{Parameter Analysis}
\subsubsection{Effect of Varying Perturbation Magnitudes}
% In contrast, as we stated before, are conspicuous and easily detectable \cite{li2020bert, jin2020bert, iyyer2018adversarial, naik2018stress, ren2019generating}, we uniformly fix the text perturbation magnitude at 1.
As the proposed method focuses primarily on images and all multi-modal attack methods all use the same text attack techniques, we evaluate the impact of perturbation magnitude on adversarial images by fixing the text perturbation. The results are shown in Figure \ref{datamag_acc}. As illustrated, our proposed method consistently outperforms others across various perturbation magnitudes. Moreover, as the perturbation magnitude increases, the performance improvement of our method becomes significantly more pronounced, while other methods show only minor gains. The reason is that the proposed method explores local regions and detailed information to uncover the intrinsic vulnerabilities of original data, which encourages model-generic adversarial examples and reduces over-reliance on specific models. With the perturbation magnitude increasing, the model-generic aspects of adversarial examples would be further enhanced, thus promoting transferability. In contrast, the compared methods primarily rely on model-specific features, meaning that increased perturbation magnitudes mainly enhance model-specific adversarial examples, leading to inferior transferability.


%------------------------------------------------------------------------
\begin{figure}[tbh]
\begin{subfigure}[Resizing parameter analysis]
{\includegraphics[angle=0, width=0.24\textwidth]{range_Image-to-Text.pdf}
\includegraphics[angle=0, width=0.24\textwidth]{range_Text-to-Image.pdf}}
\end{subfigure}
\begin{subfigure}[Grad-CAM visualization]
{\includegraphics[angle=0, width=0.44\textwidth]{grad.pdf}}
\end{subfigure}
\caption{(a) Resizing parameter analysis: appropriate scaling can achieve competitive performance. (b) A Grad-CAM visualization of original data pairs, image-only perturbed pairs and multi-modal perturbed pairs, where MAA can significantly shift the attention of target models.}%\medskip
\label{Grad}
\end{figure}
%------------------------------------------------------------------------

\subsubsection{Effect of Resizing Factors}
Setting appropriate resizing factors is crucial for effectively exploring local details. To verify this, We evaluate MAA across various resizing ranges with intervals of 0.25, and also report the results of random selection, as shown in Figure \ref{Grad}(a). It is evident that scaling within an appropriate range enhances performance while exceeding this range adversely affects it. In light of scale-invariant properties of DNNs \cite{lin2019nesterov}, RScrop resizes images to attend to subtle variations in local features and ensure that the network can focus on finer details. However, as the resizing factor increases and images become progressively larger, the network may encounter diminishing returns in its ability to capture meaningful details. This assumption is supported by a consistent performance improvement within the range of 1 to 2, with peak effectiveness observed between 1.25 and 2. However, a performance decline becomes apparent once the resizing factor exceeds 2. We interpret this observation as that adversarial examples generated on well-explored features and details are more likely to transfer to other models. However, when images are excessively enlarged, critical features may become blurred or lose detail, making recognition difficult for source models. Consequently, adversarial examples constructed from such indistinguishable features are less effective in attacking target models. Notably, scaling factors less than 1 can also enhance performance as they increase data diversity to prevent overfitting. However, they do not contribute to extracting fine-grained details, resulting in inferior performance compared to our method.

%-------------------------------------------------------------------------
\begin{table*}[!t]
\center
\caption{Ablation study of different components on Flickr30K. The attack success rate of R@1 on
image-text retrieval is reported. CLIP$_\text{ViT-B/16}$ is adopted as the source model.}
\label{tab_abla}
\centering
\resizebox{0.75\linewidth}{!}{

\begin{tabular}{c|cc|cc|cc|cc|cc}
\toprule

\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Target Model}}} & \multicolumn{6}{c}{\textbf{CLIP}} & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{ALBEF}}} & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{TCL}}}\\
\cmidrule(lr){2-7} % Define custom midrules to separate the groups
& \multicolumn{2}{c|}{$\text{ViT-B/16}$}  & \multicolumn{2}{c|}{$\text{ViT-L/14}$}&\multicolumn{2}{c|}{\text{RN101}} \\

\midrule
{ Method} & I2T & T2I & I2T & T2I  & I2T & T2I & I2T & T2I & I2T & T2I  \\
\midrule
{MAA w $\text{DIM}$}   &\cellcolor[HTML]{EFEFEF}{99.63} &\cellcolor[HTML]{EFEFEF}{99.87}    & {42.12}  &53.09 
& 62.45  & 65.13   & 27.22  &41.3     &31.61   & 44.5\\
{MAA w $\text{TI-DIM}$}   &\cellcolor[HTML]{EFEFEF}{96.13} &\cellcolor[HTML]{EFEFEF}{99.27}    & {32.02}  &46.3 
& 41.89 & 53.14   &24.81  &31.62     &24.75   & 29.6\\
{MAA w $\text{SI-NI-TI-DIM}$}   &\cellcolor[HTML]{EFEFEF}{99.85} &\cellcolor[HTML]{EFEFEF}{99.19}    & {26.99}  &41.59 
& 34.11  & 44.91   & 28.95  &33.51     &22.64   & 29.98 \\
{MAA w $\text{SIA}$}   &\cellcolor[HTML]{EFEFEF}{99.51} &\cellcolor[HTML]{EFEFEF}{99.23}    & {42.94}  &54.25 
& 63.22  & 69.19   & 27.42  &40.79     &35.09   & 45.31 \\
{MAA w $\text{ScMix}$}   &\cellcolor[HTML]{EFEFEF}{99.75} &\cellcolor[HTML]{EFEFEF}{99.84}    & {24.66}  &42.30 & 37.04  & 49.31   & 12.30  &27.46     &13.28   & 28.74 \\
% {MAA w $\text{BSA}$}  &\cellcolor[HTML]{EFEFEF}{99.75}  &\cellcolor[HTML]{EFEFEF}{99.90}  & {53.95} &62.72   &71.56    & 73.64  & 30.22    &42.55     & 40.73  & 49.48  \\
{MAA w/o $\text{Resizing}$}   &\cellcolor[HTML]{EFEFEF}{\textbf{100.00}} &\cellcolor[HTML]{EFEFEF}{99.94}    & {42.58}  &54.41  &62.32  &68.54 & 22.94   & 37.93 &30.03 &42.55   \\
{MAA w/o $\text{Sliding}$}   &\cellcolor[HTML]{EFEFEF}{99.88} &\cellcolor[HTML]{EFEFEF}{\textbf{100.00}}    & {26.5}  &42.01 & 38.60  & 42.92   &12.41  &28.11     &14.75   &29.43 \\
{MAA w/o $\text{RScrop}$}   &\cellcolor[HTML]{EFEFEF}{\textbf{100.00}} &\cellcolor[HTML]{EFEFEF}{\textbf{100.00}}    &25.47  &41.85 &38.83  &49.23   & 12.62  &27.48     &13.91   & 28.90 \\
{MAA w/o $\text{MGSD}$}  &\cellcolor[HTML]{EFEFEF}{{99.51}}  &\cellcolor[HTML]{EFEFEF}{{99.71}}  & {45.02} &56.55   &{57.81}    &65.82  &27.63   &43.59     &32.77   & 45.48  \\
{MAA}  &\cellcolor[HTML]{EFEFEF}{{99.75}} &\cellcolor[HTML]{EFEFEF}{99.05}  & {\textbf{54.36}} &\textbf{63.02}  
&\textbf{{72.23}}    &\textbf{74.87}  &\textbf{32.45}  &\textbf{43.62}     & \textbf{{41.42}}   & \textbf{{50.29}}   \\\bottomrule 
\end{tabular}
}
\end{table*}
 %-------------------------------------------------------------------------

\subsection{Ablation study}
To verify the effectiveness of the proposed method, we conduct an extensive ablation study. Since the method is simple, involving only two components: RScrop and MGSD, we create two variants by removing each component individually, labeled as MAA w/o $\text{RScrop}$ and MAA w/o $\text{MGSD}$. Furthermore, our MGSD employs scaling to enhance the exploration of fine-grained details and utilizes sliding crops to capture local dependencies and ensure comprehensive image coverage. To further assess the impact of these operations, we construct two variants that use only scaling and sliding, denoted as MAA w/o $\text{Sliding}$ and MAA w/o $\text{Resizing}$, respectively. We also compare the proposed Rscrop with other augmentations, including DIM \cite{xie2019improving}, TI-DIM \cite{dong2019evading}, SI-NI-TI-DIM \cite{lin2019nesterov}, SIA \cite{wang2023structure}and ScMix \cite{zhang2024universal}. The results for all variants in the context of image-text retrieval are summarized in Table \ref{tab_abla}. The results show that the two key strategies of MAA are complementary, enabling it to explore local details across different granularities and hierarchical levels. RScrop contributes more compared to MGSD, as it still helps the method capture local information by focusing on output features, without relying on intermediate features across levels. The performance of MAA w/o Sliding demonstrates that ensuring full image coverage is crucial for thoroughly exploring the characteristics and vulnerabilities of original samples. Similarly, scaling is important for capturing finer local details. 

\subsection{Results on the Black-box Large Vision-language Models}
To test whether the attack methods can fool black-box Large Vision-Language Models (LVLM), we selected two open-source large vision-language models: MiniGPT-4 \cite{zhu2023minigpt} and Llama 3.2 \cite{touvron2023llama} to ensure reproducibility of our experiments. Specifically, MiniGPT-4, based on Vicuna V0 13B (a 13-billion-parameter large language model), has recently scaled up the capabilities of large language models and demonstrates performance comparable to GPT-4. For Llama 3.2, we utilized Llama-3.2-11B-Vision-Instruct, which comprises 11 billion parameters. 
We evaluated the robustness of these models on multimodal tasks, including image captioning and visual question answering (VQA). For image captioning, we provided images with the prompt: â€œDescribe this image, bringing all the detailsâ€ as the input, For VQA, we input images along with the question: â€œWhat is the content of this image?â€ We collected the generated descriptions and answers for each attack method.
To assess attack performance, we used the CLIP score \cite{zhao2023evaluating}, which measures the similarity between the features of descriptions/answers for adversarial images and those for clean images, generated by the CLIP text encoder. To ensure fair comparisons, we calculated the CLIP score between the description/answer features of adversarial images (for each attack method) and the clean image features generated by all attack methods. This is because at different query times, large vision language models would produce different responses for the same input. Additionally, we reported the CLIP score between the features of clean images and their randomly shuffled counterparts as a baseline to compare attack effectiveness.
The results are summarized in Table \ref{tab_LLM}. From the results, it can be observed that the proposed method can achieve the best performance in two tasks. In addition, the attack performance is not significant due to the significant gap between the source and target models, in terms of architectures, training data and schemes. Improving attack performance on large models remains an open challenge, which we identify as a direction for future research.

%------------------------------------------------------------------------- caption
\begin{table*}[!t]
\caption{Black-box attack against Large Vision-Language Models (LVLM), i.e., MiniGPT-4 and Llama 3.2, on Flickr30K in vision question answering and image captioning. CLIP$_\text{ViT-B/16}$ is taken as the source model. The CLIP score is used as the evaluation metric, which measures the distance between features of generated texts and references, extracted by the CLIP text encoder. For a fair comparison, we compare generated texts of adversarial images with those of clean images from all attack methods to avoid variance during different LVLM query processes. The CLIP score between generated texts for clean images and the randomly shuffled texts is taken as a bar for evaluating the attack performance. Bold indicates the best performance.}
\label{tab_LLM}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lcccccc|cccccc}
\toprule  
\multicolumn{1}{c}{\textbf{Target Model \& Task}}  & \multicolumn{6}{c}{\textbf{Llama 3.2, Visual Question Answering}}   & \multicolumn{6}{c}{\textbf{MiniGPT-4, Image Captioning}} \\\cmidrule(lr){1-1} \cmidrule(lr){2-7} \cmidrule(lr){8-13}
{}  & \multicolumn{6}{c}{CLIP Text Encoder for generating features}  & \multicolumn{6}{c}{CLIP Text Encoder for generating features}  \\\cmidrule(lr){2-6} \cmidrule(lr){8-12}
{} & {CLIP$_\text{ViT-B/16}$} & {CLIP$_\text{ViT-B/32}$} & {$\text{ViT-L/14}$}   & {\text{RN50}}  & {\text{RN101}} & {\text{Avg.}}
& {CLIP$_\text{ViT-B/16}$} & {CLIP$_\text{ViT-B/32}$} & {$\text{ViT-L/14}$}   & {\text{RN50}}  & {\text{RN101}}  & {\text{Avg.}}\\ \hline
{Random Shuffle}  &0.415  &0.417 &0.312   &0.380  &0.507  &0.406
& 0.445  &0.427 &0.302   &0.3987 &0.631 &0.441\\
{Co-Attack}  &0.817  &0.814 &0.784   &0.802 &0.840 &0.811
&0.856  &0.823 &0.771   &0.847 &0.875 &0.834\\
{SGA} &0.815  &0.811 &0.783   &0.801 &0.839  &0.810    
&0.862  &0.818 &0.775   &0.842 &0.869 &0.834\\
{ETU}  &0.823  &0.820 &0.792   &0.810 &0.845   &0.818
&0.880  &0.825 &0.792   &0.862 &0.883 &0.848\\
{VLATTACK}  & 0.812  &0.809 &0.781   &0.799 &0.838 &0.808 
&0.853  &0.815 &0.780   &0.843 &0.874 &0.833\\
{VLPTransferAttack} & 0.810  &0.806 &0.776  &0.794    &0.835 &0.804   
&0.852  &0.811 &0.776   &0.842 &0.870 &0.830\\
{MAA}  & \textbf{0.798}  &\textbf{0.794} &\textbf{0.763}  &\textbf{0.782} &\textbf{0.825}  &\textbf{0.793}  
& \textbf{0.843}  &\textbf{0.801} &\textbf{0.768}  &\textbf{0.833} &\textbf{0.859}  &\textbf{0.821}\\\bottomrule 
\end{tabular}
}
\end{table*}
 %-------------------------------------------------------------------------


\subsection{Visualization}
A set of Grad-CAM \cite{selvaraju2017grad} visualization examples is shown in Figure \ref{Grad}(b), which highlights the activation areas and helps understand the regions of the image that contribute most to the modelâ€™s predictions.  This visualization reveals distinct focus shifts, indicating that the proposed MAA method effectively modulates the attention patterns of the target models. Although the image perturbations generated by MAA remain visually imperceptible to the human eye, they are highly effective at misleading the models, verifying the distinguish performance of the proposed solution when assessing the robustness of VLP models.  In addition, text perturbations tend to be more noticeable. We leave the study of improving imperceptibly of adversarial texts in our future work.





\section{Conclusions}\label{CONCLUSION}
In this paper, we propose a novel Meticulous Adversarial Attack (MAA), which demonstrates exceptional transferability across various VLP models, datasets, and downstream tasks. MAA enhances adversarial transferability by refining adversarial examples specifically in the image modality. The method is simple and easy to implement, consisting of a resizing and sliding crop technique and a multi-granularity similarity disruption strategy. Two components work synergistically to explore the representative, fine-grained characteristics and vulnerabilities of individual images, reducing over-reliance on model-specific patterns. Extensive experiments validate the effectiveness and transferability of MAA, showing that it achieves highly competitive performance.

\bibliography{conference}
\bibliographystyle{conference}

\end{document}
