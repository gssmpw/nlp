\section{Related work}
\label{Rel}

\subsection{Vision-Language Pre-trained Models}

Vision-language pre-trained (VLP) models play a crucial role in advancing the understanding of visual and textual information and their interactions. By leveraging large-scale unlabeled datasets and self-supervised learning techniques, VLP Models can learn rich, generalized representations of both visual and linguistic data. This transfer learning capability enables them to be fine-tuned for a wide range of tasks with relatively small datasets, including multi-modal retrieval, zero-shot learning, image captioning, visual question answering, and visual entailment ____. Notable VLP methods include CLIP ____, BLIP ____, ALBEF ____, and TCL ____. These methods primarily leverage multi-modal contrastive learning to align image-text pairs. Specifically, CLIP employs unimodal encoders to project data from different modalities into a unified feature space. BLIP ____ refines noisy captions to enhance learning effectiveness. ALBEF ____ and  TCL ____ both utilize a multi-modal encoder to learn joint representations for images and texts.  ALBEF ____ focuses on inter-modal relationships while  TCL ____ considers both intra- and inter-modal relationships.

\subsection{Adversarial Attack}

With the widespread adoption of Deep Neural Networks, it is essential to evaluate their robustness to ensure their reliability in real-world applications, which often involve uncertainty and potential threats. Adversarial attacks are a prominent method used to assess this robustness, which aim to mislead model predictions by introducing imperceptible perturbations into the data ____. Traditional methods typically focus on specific tasks and unimodal cases, such as image classification. For image attacks, most techniques learn pixel-level perturbations, whereas text attacks often involve replacing or removing keywords, or performing text transformations ____. Recently, there has been growing interest in multi-modal vision-language scenarios. For instance, ____ address image-text retrieval by increasing the embedding distance between adversarial and original data pairs. ____ learn image perturbations by minimizing the distance between the output and the target label while maximizing the difference from the original label for the visual question answering task. More introduction can be found in ____.

% To address this challenge and promote transferability, one approach involves using an 

\noindent\textbf{Adversarial transferability.} Early attack methods typically assume a white-box setting, where all necessary information for generating adversarial examples, including target models and tasks, is available. However, in real-world scenarios, such comprehensive information is often unavailable. To address this challenge, one approach uses an ensemble of models as the victim model during training ____, based on the intuition that adversarial examples effective against a diverse set of models are likely to mislead more models. However, assembling such a model ensemble can be difficult. Another approach utilizes momentum-based methods ____ to stabilize gradient updates and avoid poor local maxima, though this may also divert perturbations from effective paths. Data augmentation ____ increases data diversity, helping to prevent overfitting to specific models. Transferability is particularly challenging for attacks on VLP models, due to the unpredictable fine-tuning process. To enhance transferability, ____ suggests increasing the gap between adversarial data and original image-text pairs. Some of these methods also focus on generating diverse image-text pairs through scale-invariant transformations ____ and ScMix augmentations ____. Additionally, recent approaches consider the local utility of adversarial examples ____ or perturbations ____. However, ____ enlarges block-wise similarity between samples, which is constrained by the block size, failing to comprehensively capture local regions and their interactions. ____ does not consider the characteristics and vulnerabilities of the original sample. As a result, these methods cannot ensure the effectiveness and transferability of adversarial attacks.