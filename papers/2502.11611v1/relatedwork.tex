\section{Background and Related Work}
\label{s:rw}
\subsection{Gender Bias and Discrimination in Law and Linguistics}
This work contributes to the broader discourse on AI and discrimination which is of the utmost relevance since Europe is currently setting its legal framework on AI. Specifically, on 13 March 2024, the European Union enacted its first comprehensive regulation to address these concerns~\cite{EU2024} and on 17 May 2024 the Council of Europe adopted the first-ever international legally binding treaty aimed at ensuring the respect of human rights, the rule of law and democracy legal standards in the use of artificial intelligence (AI) systems ~\cite{CouncilofEurope2024}. Concerning potential unequal treatment leading to discrimination, the inherent risk of AI models lies in their potential to perpetuate discrimination against minority groups due to biases in the data sets or the same architecture of the system. The new European legal framework emphasizes the need for a more conscientious and responsible approach to AI design and evaluation \cite{Nardocci2021}. As a consequence, eliminating AI-induced discrimination is at the heart of current research trends \cite{maghool2023enhancing}. 

Previous works addressed this topic for other languages~\cite{bolukbasi2016man} or focused on other NLP techniques ~\cite{park2018reducing,bolukbasi2016quantifying,may2019measuring} and some tried to provide a taxonomy of gender bias in texts (in English)~\cite{doughman-etal-2021-gender}. A comprehensive review of the current legal literature on the topic \cite{Nardocci2021} outlines several stages in AI design where discrimination may manifest. 

MT based on machine learning technologies, represents an example of how AI perpetuates stereotypes typical of human language~\cite{DAmico2023} from one culture to another. Let us recall the case of Google Translate~\cite{Prates2018}, the MT system exhibited biased behavior by translating job titles from English into languages that incorporate masculine/feminine characterization. This is particularly evident when lower-income and non-leadership positions are translated with their female counterparts, while higher-income and leadership positions are associated with their male counterparts, perpetuating gender stereotypes. For example, "the nurse" would be translated with "l'infermiera" (female nurse) and "the doctor" with "il medico" (male doctor)~\cite{AdamsLoideain2019,Prates2018}.

The importance of gender in human experience is universally acknowledged, reflected in the linguistic expressions of femininity and masculinity present across languages. However, languages differ in their methods of encoding gender. English, for instance, is classified as a notional gender language, primarily conveying the gender of human referents through personal pronouns, possessive adjectives (e.g., he/him/his; she/her/hers), and gender-specific terms (e.g., man; woman). In contrast, grammatical gender languages such as Italian utilize a system of morphosyntactic agreement, where gender markers extend beyond nouns to encompass verbs, determiners, and adjectives \cite{Corbett2006}. This distinction becomes particularly significant in translation contexts\cite{garzone2020chapter}, notably when the source language lacks gender information regarding a referent and the target language operates within a grammatical gender framework and, although controversial to this day, prescribes the grammatical rule of the "inclusive masculine". This rule is embodied in the fact that if only one masculine exponent is present in a group, the plural masculine will apply. For example, in a class consisting of 10 people 9 are women but 1 is a man, it is grammatically correct for the teacher to say "Buongiorno a tutti" (-i standing for masculine plural). Furthermore, until recently, there were no feminine words to define higher professional positions. With social change and the promotion of female participation in public life, such terms have been coined (e.g. Professoressa, Dottoressa, Avvocata, Ingegnera, etc.) but still part of public opinion is skeptical to use such terms~\cite{Robustelli2022} to the point that in 2023 Italy elected its first female prime minister who decided to be addressed by "Signor Presidente" (Mr. President) as a political statement~\cite{DAmico2023}. Italian linguistics studies nowadays converge on the assumption that Italian is a sexist language. The essay "Sexism in the Italian Language" \cite{Sabatini1987}  has been the first seed of a debate that bloomed much later. Currently, the Italian language is changing because much has changed in the role of Italian women within society. However, this change is not organic, structured, or systematic as it should be in a society that proactively strives for new relationships between women and men~\cite{SulisGheno2022}. 

Recent research in computation and language~\cite{Piergentili2023} advocated for gender-neutral translation (GNT) as both a manifestation of gender inclusion and an objective for MT models. Our research aligns with and contributes to this perspective by proposing a methodology for identifying terms that exhibit gender bias in MT. Similar studies have been applied to English~\cite{bolukbasi2016man}, to Sentiment Analysis~\cite{park2018reducing}, to word embedding methods~\cite{bolukbasi2016quantifying,may2019measuring} but never to the English to Italian translation. Through our findings, we provide tangible insights to inform the development and training of GNT algorithms, thus promoting more inclusive and unbiased translations.

\subsection{Gender Bias and Discrimination in Computer Science}

We were inspired by the research of \cite{bolukbasi2016man}. They created some analogies like King: Queen using Google Word2Vec and then asked human annotators to rate them as biased/appropriate. To detect bias, they used cosine similarity to measure the similarity of analogies with she-he. They showed that word embeddings contain biases in their geometry that reflect gender stereotypes in the wider society. 

In another paper by \cite{Stanovsky2019-hs}, they composed a challenge set for gender bias in MT called WinoMT, which contains 3,888 instances and is balanced between male and female, and between stereotypical and non-stereotypical gender roles (e.g. a female doctor versus a female nurse). They then translated them into four different language categories (Romance, Slavic, Semitic, and Germanic) using six widely used MT models representing the state of the art in commercial and academic research, such as Google Translate. They showed that MT models significantly tend to translate based on gender stereotypes rather than more meaningful contexts.

Another related study is the one by \cite{Biasion2020-yb}. They looked at gender bias in Italian word embeddings. They made a list of gender definition pairs: [lui (he), lei (she)] and then calculated the vector difference like \(\mathbf{lui}\)-\(\mathbf{lei}\) to get the direction of the bias. They also used cosine similarity to measure the differential association between target and attribute word sets. They used FastText as the word embedding method and the target set of their work consisted only of occupations in Italian. 

In another paper by \cite{Prates2020-dc}, they take an extensive list of job titles and construct some sentences like "He/She is an Engineer" (where "Engineer" is replaced by the job title of interest) in 12 different gender-neutral languages such as Hungarian, Chinese, Yoruba, and several others. They then translate these sentences into English using the Google Translate API and collect statistics on the frequency of female, male, and gender-neutral pronouns in the translated output. We then show that Google Translate has a strong bias towards male pronouns, especially in fields typically associated with gender imbalance or stereotypes, such as STEM (Science, Technology, Engineering, and Mathematics) jobs.

Finally, the literature suggests that most word embedding models, such as Word2Vec and FastText, have a gender bias. This bias can influence how models learn patterns, potentially reinforcing societal biases and stereotypes. Consequently, AI and ML models, including machine translation (MT) models, are also susceptible to such biases in word embedding. In this research, we aim to investigate the extent to which this bias is caused by word embedding or translation, and whether translation affects the intensity and nature of this gender bias.