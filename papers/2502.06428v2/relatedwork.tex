\section{Related Works}
\noindent\textbf{MLLMs for visual understanding.} In recent years,
significant progress has been made in the field of MLLMs for visual
understanding~\cite{radford2021learning,zhang2024video,maaz2023video}. Models
like LLaVA~\cite{liu2024visual} achieved cross-modal feature
alignment through projectors, enhancing understanding of single
images. As the focus of research is shifting from image-only models to
those for multi-image and video inputs, various enhancements to the
visual language connector have been proposed. \citet{he2024ma} and
\citet{wang2023chatvideo} implemented average pooling, while
\citet{jin2024chat} and \citet{shu2024video} introduced techniques to dynamically drop
visual tokens. Moreover, \citet{cheng2024videollama} adopted
spatial-temporal convolution to better capture the dynamics of a video
and reduce feature size. 
%
However, memory constraints and the lack of large-scale annotated
hour-long datasets limit current models. They struggle to process and
understand temporal information in long videos beyond a few minutes,
leading to poor performance on long video understanding. 

\noindent\textbf{MLLMs for Long Video Understanding.} To improve
performance on long videos, several studies have introduced more
fine-grained annotations in datasets at various scales to aid
training~\cite{fu2024video,wu2024longvideobench}. \citet{zhang2024long}
and \citet{he2024ma} extended the context window of LLMs to encompass more
extensive temporal information. LongVILA~\cite{xue2024longvila}
further utilized a parallel processing system to achieve context
compression at the input level. LLaVA-Vid~\cite{li2025llama} and
VideoXL~\cite{shu2024video} sought to obtain a highly compact
representation that preserves key information for effective token
compression. However, these compression techniques invariably lead to
loss of information and poorer video understanding. Critically,
most of these studies focus on learning from the entire video as a single input without selection, neglecting the fact that relevant
information in long videos is often sparsely located. When the presence of irrelevant information is not minimised, it detracts the reasoning power of MLLMs. 

\noindent\textbf{Prompt Engineering.} To enable more effective
reasoning in visual understanding tasks,
VideoCoT~\cite{wang2024videocot} decomposed input questions to
facilitate image-level visual reasoning by MLLMs. Similarly,
VoT~\cite{fei2024video1} used a sense graph and problem decomposition
to enhance short video comprehension and reasoning.  
%
AoTD~\cite{shi2024unlocking} realized the reasoning of thought chain through agent-of-thought.
%
VideoGen~\cite{zheng2024videogen} utilised chain-of-thought to assist the video generation process.
%
~\citet{himakunthala2023let} and \citet{han2024videoespresso} built Chain-of-thought from a dataset perspective to help better evaluate the model's video understanding capabilities.
%
However, these methods mainly focus on optimising text inputs to
improve reasoning, neglecting the significant temporal changes between
adjacent shots in long videos. Blindly inputting an entire long video
for model processing affect the modelâ€™s understanding of both the video and the
questions. Our approach is the first to explore temporal and spatial
modelling on visual inputs for long video understanding, ensuring the visual data better aligns
with text questions and enhances model reasoning on long videos.