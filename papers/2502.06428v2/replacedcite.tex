\section{Related Works}
\noindent\textbf{MLLMs for visual understanding.} In recent years,
significant progress has been made in the field of MLLMs for visual
understanding____. Models
like LLaVA____ achieved cross-modal feature
alignment through projectors, enhancing understanding of single
images. As the focus of research is shifting from image-only models to
those for multi-image and video inputs, various enhancements to the
visual language connector have been proposed. ____ and
____ implemented average pooling, while
____ and ____ introduced techniques to dynamically drop
visual tokens. Moreover, ____ adopted
spatial-temporal convolution to better capture the dynamics of a video
and reduce feature size. 
%
However, memory constraints and the lack of large-scale annotated
hour-long datasets limit current models. They struggle to process and
understand temporal information in long videos beyond a few minutes,
leading to poor performance on long video understanding. 

\noindent\textbf{MLLMs for Long Video Understanding.} To improve
performance on long videos, several studies have introduced more
fine-grained annotations in datasets at various scales to aid
training____. ____
and ____ extended the context window of LLMs to encompass more
extensive temporal information. LongVILA____
further utilized a parallel processing system to achieve context
compression at the input level. LLaVA-Vid____ and
VideoXL____ sought to obtain a highly compact
representation that preserves key information for effective token
compression. However, these compression techniques invariably lead to
loss of information and poorer video understanding. Critically,
most of these studies focus on learning from the entire video as a single input without selection, neglecting the fact that relevant
information in long videos is often sparsely located. When the presence of irrelevant information is not minimised, it detracts the reasoning power of MLLMs. 

\noindent\textbf{Prompt Engineering.} To enable more effective
reasoning in visual understanding tasks,
VideoCoT____ decomposed input questions to
facilitate image-level visual reasoning by MLLMs. Similarly,
VoT____ used a sense graph and problem decomposition
to enhance short video comprehension and reasoning.  
%
AoTD____ realized the reasoning of thought chain through agent-of-thought.
%
VideoGen____ utilised chain-of-thought to assist the video generation process.
%
____ and ____ built Chain-of-thought from a dataset perspective to help better evaluate the model's video understanding capabilities.
%
However, these methods mainly focus on optimising text inputs to
improve reasoning, neglecting the significant temporal changes between
adjacent shots in long videos. Blindly inputting an entire long video
for model processing affect the modelâ€™s understanding of both the video and the
questions. Our approach is the first to explore temporal and spatial
modelling on visual inputs for long video understanding, ensuring the visual data better aligns
with text questions and enhances model reasoning on long videos.