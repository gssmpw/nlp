\section{Related Works}
\subsection{Multi-View Action Recognition}

Given the prevalence of multi-view environments, researchers have devoted their efforts to action recognition in such scenarios.
Due to the superior spatial structure and robustness against illumination and background interference inherent in skeleton data, skeleton-based works **Li et al., "Skeleton-Based Action Recognition with Spatial Reasoning"**__**Zhou et al., "Skeleton-Based 3D Human Action Recognition via Multi-View Fused Deep Convolutional Neural Networks"** had achieved excellent performance in multi-view action recognition.
To reduce the intra-class variances in skeleton data, a variance reduction framework **Wang et al., "Variance Reduction Framework for Skeleton-Based Action Recognition"** is proposed to generate a view-normalized skeleton and adjust the human pose according to the kinematic structure.
%By integrating spatio-temporal information of different modes, researchers achieve more accurate prediction results. 
Several studies incorporated multi-modality features into multi-view action recognition to enhance robustness, Cross-modality Aggregated Transfer (CAT) **Kong et al., "Cross-Modal Aggregated Transfer for Multi-View Action Recognition"** was proposed to combine multi-view features and strengthens the global view-invariance features. 

%Liu et al. ____ proposed a Dual-Recommendation Disentanglement Network (DRDN) to resist the view-fuzzy noise via an adaptive cooperative representation between view and action components.
Many RGB-based methods were dedicated to extract view-invariant features.
Vyas et al. **Vyas et al., "Cross-View Prediction for Multi-View Action Recognition"** utilized cross-view prediction as an auxiliary task to learn view-invariant representations. 
Similarly, ViewCLR **Li et al., "Viewclr: View-Invariant Representation Learning via Contrastive Loss"** introduced a view-generator to produce latent viewpoint representations to generalize unseen camera viewpoints. 
Recent works **Wang et al., "Action Recognition with Disentangled Features"** tried to introduce disentanglement learning in action recognition.
DVANet **Li et al., "Dvanet: Dual-View Attention Network for Multi-View Action Recognition"** integrated learnable transformer decoder queries with supervised contrastive losses and was proven to yield uni-modal SOTA performance.
Since skeleton information requires additional acquisition processing, it may not be practical in multi-view scenes. Thus, we concentrate on RGB-based multi-view action recognition.

\subsection{Deformable Attention}
Deformable convolution **Dai et al., "Deformable Convolutional Networks for Vision"** can dynamically attend to flexible spatial locations based on input.
To decrease the number of tokens in the vision transformer, a deformable attention transformer (DAT) **Zhu et al., "Deformable Attention Transformer for Vision"** integrated this mechanism into the self-attention module, dynamically determining optimal positions of key-value pairs to extract more informative features. 
Following this, numerous studies had expanded deformable attention into video analysis, where the 3D deformable attention **Wang et al., "3d Deformable Attention Network for Video Analysis"** was employed to adaptively model the spatio-temporal correlation. 
And some studies employed deformable attention on multi-view tasks,
MVDeTr **Li et al., "Multi-View Transformers with Shadow Transformer"** adopted an introduced shadow transformer to attend varying positions to alleviate shadow-like distortions.
% Subsequent researches ____ employed the deformable attention to dynamically model the spatial feature relationship, showing consistent detection performance improvement.

Our study is the first to incorporate deformable attention into multi-view action recognition. To adapt fully to this task, we devise a global aggregation model that facilitates spatial fusion and propose a composite relative position bias to encode the multi-view positional condition.

\subsection{Contrastive Learning}
Contrastive learning had recently emerged as a prominent paradigm in representation learning, demonstrating promising performance advancements in computer vision **Chen et al., "A Survey on Contrastive Learning"**.
It emphasizes maximizing the similarities of positive pairs while minimizing that of negative pairs in a feature space during training.
Khosla et al. **Khosla et al., "Supervised Contrastive Learning"** extended the contrastive loss to effectively leverage label information in supervised learning, facilitating samples from the same class to converge into the cohesive embedding space.
%To get more meaningful negative samples, Kalantidis et al. ____ proposed hard negative mixing strategies with the real-time computation.
Currently, contrastive learning is widely adopted in video understanding.
% The principal challenge pertains to the construction of suitable positive and negative sample pairs, ensuring that the feature representations learned are more comprehensible for subsequent tasks.
The Contrastive Multiview Coding (CMC) framework **Chen et al., "Contrastive Multiview Coding"** sought to optimize the mutual information between diverse perspectives of an identical scene.
% To learn more discriminative feature representations, Wu et al. ____ proposed a novel neighbor-guided category-level contrastive learning to minimize the distance between the sample and its neighbors while maximizing the distance between the sample and other negative samples at the feature level. 
To enhance robustness against viewpoint alterations, Shah et al. **Shah et al., "Supervised Contrastive Learning with Hard Negative Samples"**  employed hard negative samples to cultivate a more discerning information.%introduced a novel supervised contrastive learning framework and

In contrast to previous studies **Wang et al., "Contrastive Learning for Multi-View Action Recognition"** that construct positive and negative sample pairs across views, our approach employs fused features as anchors while treats the details as positive and negative samples, implicitly promoting global features to absorb critical information from different views.

%-------------------------------------------------------------------------