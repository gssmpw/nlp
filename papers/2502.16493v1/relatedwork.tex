\section{Related Works}
\subsection{Multi-View Action Recognition}

Given the prevalence of multi-view environments, researchers have devoted their efforts to action recognition in such scenarios.
Due to the superior spatial structure and robustness against illumination and background interference inherent in skeleton data, skeleton-based works \cite{Pan2023ViewNormalized,chi2022infogcn} had achieved excellent performance in multi-view action recognition.
To reduce the intra-class variances in skeleton data, a variance reduction framework \cite{Pan2023ViewNormalized} is proposed to generate a view-normalized skeleton and adjust the human pose according to the kinematic structure.
%By integrating spatio-temporal information of different modes, researchers achieve more accurate prediction results. 
Several studies incorporated multi-modality features into multi-view action recognition to enhance robustness, Cross-modality Aggregated Transfer (CAT) \cite{xu2021cross} was proposed to combine multi-view features and strengthens the global view-invariance features. 

%Liu et al. \cite{liu2023dual} proposed a Dual-Recommendation Disentanglement Network (DRDN) to resist the view-fuzzy noise via an adaptive cooperative representation between view and action components.
Many RGB-based methods were dedicated to extract view-invariant features.
Vyas et al. \cite{vyas2020multi} utilized cross-view prediction as an auxiliary task to learn view-invariant representations. 
Similarly, ViewCLR \cite{das2023viewclr} introduced a view-generator to produce latent viewpoint representations to generalize unseen camera viewpoints. 
Recent works \cite{liu2023dual,siddiqui2024dvanet} tried to introduce disentanglement learning in action recognition.
DVANet \cite{siddiqui2024dvanet} integrated learnable transformer decoder queries with supervised contrastive losses and was proven to yield uni-modal SOTA performance.
Since skeleton information requires additional acquisition processing, it may not be practical in multi-view scenes. Thus, we concentrate on RGB-based multi-view action recognition.

\subsection{Deformable Attention}
Deformable convolution \cite{zhu2019deformable} can dynamically attend to flexible spatial locations based on input.
To decrease the number of tokens in the vision transformer, a deformable attention transformer (DAT) \cite{Xia2022VisionTW} integrated this mechanism into the self-attention module, dynamically determining optimal positions of key-value pairs to extract more informative features. 
Following this, numerous studies had expanded deformable attention into video analysis, where the 3D deformable attention \cite{Kim2022CrossModalLW} was employed to adaptively model the spatio-temporal correlation. 
And some studies employed deformable attention on multi-view tasks,
MVDeTr \cite{hou2021multiview} adopted an introduced shadow transformer to attend varying positions to alleviate shadow-like distortions.
% Subsequent researches \cite{Liu2024MultiViewAC} employed the deformable attention to dynamically model the spatial feature relationship, showing consistent detection performance improvement.

Our study is the first to incorporate deformable attention into multi-view action recognition. To adapt fully to this task, we devise a global aggregation model that facilitates spatial fusion and propose a composite relative position bias to encode the multi-view positional condition.

\subsection{Contrastive Learning}
Contrastive learning had recently emerged as a prominent paradigm in representation learning, demonstrating promising performance advancements in computer vision \cite{Zhu2022BalancedCL}.
It emphasizes maximizing the similarities of positive pairs while minimizing that of negative pairs in a feature space during training.
Khosla et al. \cite{khosla2020supervised} extended the contrastive loss to effectively leverage label information in supervised learning, facilitating samples from the same class to converge into the cohesive embedding space.
%To get more meaningful negative samples, Kalantidis et al. \cite{kalantidis2020hard} proposed hard negative mixing strategies with the real-time computation.
Currently, contrastive learning is widely adopted in video understanding.
% The principal challenge pertains to the construction of suitable positive and negative sample pairs, ensuring that the feature representations learned are more comprehensible for subsequent tasks.
The Contrastive Multiview Coding (CMC) framework \cite{tian2020contrastive} sought to optimize the mutual information between diverse perspectives of an identical scene.
% To learn more discriminative feature representations, Wu et al. \cite{Wu2023Neighbor} proposed a novel neighbor-guided category-level contrastive learning to minimize the distance between the sample and its neighbors while maximizing the distance between the sample and other negative samples at the feature level. 
To enhance robustness against viewpoint alterations, Shah et al. \cite{shah2023multi}  employed hard negative samples to cultivate a more discerning information.%introduced a novel supervised contrastive learning framework and

In contrast to previous studies \cite{shah2023multi,siddiqui2024dvanet} that construct positive and negative sample pairs across views, our approach employs fused features as anchors while treats the details as positive and negative samples, implicitly promoting global features to absorb critical information from different views.

%-------------------------------------------------------------------------