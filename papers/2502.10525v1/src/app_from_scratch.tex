\section{Influence of the Training Length on Distillation-based Watermark Durability}
\label{app:from_scratch_extended}

In this section, we extend the experiment from \cref{sec:wm_from_scratch} and specifically ask whether scaling the number of tokens when distilling the watermark necessarily improves durability against finetuning.

\paragraph{Experimental details}
We again perform the distillation of the \textsc{KGW} watermark with $\delta=2, \gamma=0.25$, and $k=1$ on a \textsc{GPT-2} pretrained model.  
We distill the watermark for a different number of steps, where each step consists of approximately $60$ thousand tokens.  
We then evaluate the watermark strength every $20{,}000$ steps (\ie approximately $1.4$B tokens).  
To evaluate the watermark strength, we use the same setup as in \cref{sec:eval} but with $500$ completions instead of $100$.  
To evaluate durability against finetuning, we finetune on both broad-domain \textsc{OpenWebText} and task-specific \textsc{OpenMathInstruct}.  

\paragraph{Durability does not scale with distillation training length} 
In \cref{fig:ft_tpr_gpt2_ckpts}, we plot both the TPR at 1\% and 5\% FPR for the different models tested.  
We see that the watermarked model that has been distilled for $20$ thousand steps is actually more durable than the one distilled for only $2500$ steps (\textsc{KGW-D (Standard)} from \cref{tab:scratch_table}), but also more durable than the ones distilled for longer (with up to a 10\% TPR@1 difference).  
It also seems that, as the distillation training length increases, its impact on durability plateaus, as all TPR curves from $80$ thousand steps onward are very similar.  
These results confirm the limitations highlighted in \cref{sec:wm_from_scratch}: increasing the training set size, up to a point, improves durability, yet it is still insufficient for the watermark to remain effective against prolonged downstream finetuning.  


\input{figures/tex/ft_tpr_scratch_ckpt.tex}
