\section{Experimental Details}
\label{app:experimental_details}

In this section, we provide an in-depth list of the parameters used for the model modifications (\cref{sec:durability}) and the watermarking schemes (\cref{sec:methods}) that we use in our evaluation in~\cref{sec:eval}.
For all watermarking schemes, we use the same \llama as our base unwatermarked model. 

\paragraph{Watermarking schemes}
For distillation-based generation-time watermarks, we use the distillation loss from \cref{eq:distillation_learnability} with \textsc{OpenWebText} as $\mathcal{D}$.  
We distilled the watermark using the same hyperparameters as in \citet{learnability}: a batch size of $64$, with $512$ tokens per input, a learning rate of 1e-5, the AdamW optimizer \citep{adamw} with $(\beta_1, \beta_2) = (0.9,0.999)$, and no weight decay.  
For \KGW, we use $\delta=2$, $\gamma=0.25$, and $k=1$.  
For \KTH, we use $n_{key} = 256$ with no key shift. 

For \textsc{KGW-D+CTV}, we first distill \textsc{KGW} with the parameters described above on the full model.
Then, we finetune the watermarked model on \textsc{OpenWebText} for 2500 steps with cross-entropy loss, batch size of $64$, $512$ tokens per input, a learning rate of $1\text{e-}5$, the Adafactor optimizer with a cosine learning rate decay, and a linear warmup for the first $500$ steps. 
We then compute the contrastive task vector (\cref{eq:ctv}) and learn \textsc{KGW} on the selected weights.

For the \unremovable watermark, we set the standard deviation to $\sigma=0.6$ to ensure sufficient power in the unaltered watermarked model while not degrading the model's quality too much.  
For \gaussmark, as suggested in \citet{gaussmark}, we perform a grid search to find the best layer and standard deviation to balance watermark power and quality degradation.  
We find the optimal layer to be the 31st MLP up\_proj layer with a standard deviation of $\sigma=0.018$.

\paragraph{Model modifications}
For all quantization and pruning methods, we use the default hyperparameters suggested for each technique. 
For finetuning on \textsc{OpenWebText}, we use a batch size of $32$, with $512$ tokens per input, a learning rate of $1\text{e-}5$, the Adafactor optimizer with a cosine learning rate decay, and a linear warmup for the first $500$ steps. 
For finetuning on \textsc{OpenMathInstruct}, we introduce two new instruction tokens and use the same settings but with a maximum of $2048$ tokens per input to accommodate the entire math problem and solution. 
For both finetuning tasks, we use the same \textsc{LoRA} adapter with a low-rank dimension of $16$ and an alpha of $32$. 
Moreover, we apply the \textsc{LoRA} adapter only to the following layers: v\_proj, k\_proj, o\_proj, and q\_proj.

\paragraph{Metrics}
As our main metric in~\cref{sec:eval} we use the true positive rate of the watermark detector, evaluated at a false positive rate of $5\%$.
While we believe this FPR level is high for fully practical applications, it is both a common evaluation setting in prior watermarking literature, and more importantly, calibrated to the current strength of OSM watermarks.
Ideally, OSM watermarks would advance to a level where this metric is not useful anymore as the corresponding TPRs would be close to $1$ for many methods, and evaluations would focus on lower, more practical FPR levels.
