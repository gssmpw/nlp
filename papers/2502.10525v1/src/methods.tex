
\section{Current State of Open-source LLM Watermarking} \label{sec:methods}

Next, we introduce the current state of watermarking schemes for open-source models. 
We identify two main categories: schemes that embed the watermark into the model using gradient descent \citep{learnability,rlwatermark}, and those that directly edit the weights \citep{gaussmark, unremovable}.
The latter are less computationally expensive but either require architectural changes to the model~\citep{unremovable} or more compute-intensive detection~\citep{gaussmark}.
For gradient-based methods, there remain unexplored questions about how such methods generalize to different tasks \citep{wapiti} or the viability of statistical guarantees \citep{rlwatermark}. 
As many OSM watermarks are based on generation-time watermarks, we provide a separate introduction in \cref{app:sampling_based_wm}.
We evaluate durability against common modifications of the methods presented here in \cref{sec:eval}.

\paragraph{Distillation-based watermark}
In \citet{learnability}, the authors show that generation-time watermarks \citep{kgw,aar, stanford} can be imprinted into the model weights by distilling the watermark from a teacher model $\theta_0$.
Then, the same watermark detector can be used to detect the watermark in the student model $\theta$.
In the first variant, the teacher model is used in a black-box way to generate watermarked data $\mathcal{D}_{\text{wm}}$, which the student finetunes on using the cross-entropy loss:
\begin{equation}
 \mathcal{L}_{\text{sampling}} (\theta) = \mathbb{E}_{x \sim \mathcal{D}_{\text{wm}}} \left[ \sum_{t=1}^{|x|} -\log p_\theta (x_t | x_{<t}) \right].
\end{equation}
In the white-box variant, the student model is finetuned to mimic the teacher model's next-token distribution, using a loss based on $\text{KL}$-divergence:
\begin{equation}
    \label{eq:distillation_learnability}
 \mathcal{L}_{\text{logit}} (\theta) = \mathbb{E}_{x \sim \mathcal{D}} \left[ \sum_{t=1}^{|x|} \text{KL}(f_w(p_{\theta_0}(.|x_{<t}), \xi_w), p_{\theta}(.|x_{<t})) \right].
\end{equation} 
For evaluating durability, we distill two different generation-time watermarks, \textsc{KGW} and \textsc{KTH} (see \cref{app:sampling_based_wm}).  
We label the corresponding distilled OSM watermarks \KGW and \KTH, respectively.

\paragraph{RL-based watermark}
In \citet{rlwatermark}, the authors propose integrating the watermark into the RLHF pipeline \citep{rlhf} by jointly training the watermark and the watermark detector using reinforcement learning.  
More precisely, given a dataset $\mathcal{D} = \{(x_i,y_i)\}$ of prompts and non-watermarked completions, a watermark detector $D$ parameterized by $\theta^d$, and two models $\theta^0, \theta \in \Theta$, we optimize the following objective using PPO \citep{ppo}:  
\begin{equation}
    \min_{\theta^d, \theta} \mathbb{E}_{(x,y) \sim \mathcal{D}}[D(x,y,\theta^d) - D(x,y(x,\theta),\theta^d)] + \lambda \text{Reg}(\theta, \theta^0).
\end{equation}
 
\paragraph{Weight-editing watermarks}
Both \citet{unremovable} (\unremovable) and \citet{gaussmark} (\gaussmark) propose directly editing the weights of the model without needing gradient descent.

\unremovable introduces a Gaussian noise $\varepsilon = \mathcal{N}(0, \sigma I_{|\Sigma|})$ bias layer in the last projection matrix.
The detector, given a text $x \in \Sigma^*$, computes the following Z-score and performs a one-sided Z-test:
\begin{equation}
    \label{eq:unremovable_detector}
    Z(x, \varepsilon) = \frac{\sum_{t=1}^{|x|} \varepsilon[x_t]}{\sigma|x|}.
\end{equation}
As no prominent open-source model architecture has a bias layer in the last projection matrix, \unremovable requires a modification of the model architecture allowing for simple removal by disabling the respective layer.
Hence, for current architectures, this watermark is not durable---we still include it in our evaluation for completeness.

\gaussmark generalizes \unremovable to target any subset $\theta_r \subset \theta$ of existing model weights (with dimension $d_r \in \mathbb{N}$).
For this we compute $\varepsilon = \mathcal{N}(0, \sigma I_{d_r})$ and consider the model with $\theta_r + \varepsilon$ and all other weights $\theta \backslash \theta_r$ untouched. For detection, we use the following statistic:
\begin{equation}
    \label{eq:gaussmark-detector}
    Z(x, \varepsilon) = \frac{ \varepsilon \cdot \nabla_{\theta_r} \log(p_{\theta}(x)) }{\sigma \lVert \nabla_{\theta_r} \log(p_{\theta}(x)) \rVert _2},
\end{equation}
and also perform a one-sided Z-test. Unlike \unremovable, \gaussmark can be applied to any subset of weights from the model and, crucially, does not necessarily require editing the architecture of the model.
However, it requires a forward and (partial) backward pass for watermark detection.
