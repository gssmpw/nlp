\section{Durability of Open-source LLM Watermarks}
\label{sec:durability}

Next, we proceed to define the durability requirement more concretely. In~\cref{sec:methods} we will introduce current OSM watermarking methods, and use our durability evaluation setup to systematically evaluate them in \cref{sec:eval}.
To concretize the durability requirement, we have surveyed both the literature and trending Hugging Face models.  
We identified four main categories of modifications: quantization, pruning, merging, and finetuning, among which we select the most prominent methods and parameter settings.


\paragraph{Quantization} 
Model quantization techniques have emerged as a key method to enable the deployment of increasingly large LLMs on memory-constraint commodity hardware. The fundamental idea underlying quantization is to represent (\textit{quantize}) model weights (and activations) in lower-precision data types. We can split popular methods into two categories: \emph{zero-shot} and \emph{optimization-based}.
Zero-shot methods fix the quantization mapping (\textit{buckets}) independently of the model on which they are applied. This makes them computationally inexpensive and a popular choice in consumer libraries (e.g., \textsc{LLM.int8()} \citep{llmint8}, and \textsc{NF4} \citep{qlora} in Hugging Face). Meanwhile, optimization-based methods aim to minimize a reconstruction error assuming a specific model. This includes methods like \textsc{HQQ} \citep{hqq}, which optimizes reconstruction error only over model weights, as well as a range of methods, such as \textsc{GPTQ} \citep{gptq} and \textsc{AWQ} \citep{awq}, that optimize activation reconstructions over an additional calibration dataset.
For evaluating durability, we consider both 8 bits and 4 bits quantized models with different methods.

\paragraph{Pruning} 
While quantization reduces precision across weights, pruning aims to reduce memory requirements by directly removing specific weights completely (\textit{zeroing out}).
Unstructured pruning techniques such as \textsc{Wanda} \citep{wanda}, \textsc{SparseGPT} \citep{sparsegpt}, and \textsc{GBLM} \citep{gblm_pruner} independently remove weights while relying on minimizing a reconstruction error between the pruned weights and the dense weights on a calibration dataset. 
On the contrary, structured pruning methods such as \textsc{Sheared Llama} \citep{sheared_llm} and \textsc{LLM-Pruner} \citep{llm_pruner} aim to remove entire sets of weights (e.g., rows, columns, or layers) jointly (likewise minimizing a reconstruction error).
The advantage of structured pruning is that the resulting model inhibits dense substructures in its weights, allowing for hardware-optimized inference algorithms. 
At the same time, they are usually not zero-shot and require additional finetuning to restore model performance after pruning.  
Given such additional modifications, we will only focus on unstructured pruning methods.

\paragraph{Model merging}
Model merging techniques aim to construct a new model out of a set of base models by combining their individual weights. 
Importantly, previous works \citep{fischer_merging, dataless_merging} have shown that model merging allows for cheaply combining multiple expert models into a single model that maintains task-specific performance. 
Most merging techniques \citep{fischer_merging, dataless_merging, ties_merging, dare_merging} thereby rely on the concept of task vectors and task arithmetic: expert knowledge in LLMs lies in orthogonal directions in weight space and can be directly combined to obtain a vector that joins their respective strengths.

We focus on merging via Spherical Linear Interpolation (\textsc{SLERP}) between the watermarked and original model \citep{mergekit}.
Given the original model $\theta_0$, the watermarked model $\theta_{\text{wm}}$, the angle $\Omega$ between $\theta_0$ and $\theta_{\text{wm}}$ (we set $\Omega := \frac{\pi}{2}$ if $\theta_0$ or $\theta_{\text{wm}}$ is null), and the interpolation parameter $t \in [0,1]$, we consider  
\begin{equation} 
 \text{SLERP}(\theta_{\text{wm}},\theta_0, t) = \frac{\sin[(1-t) \Omega]}{\sin\Omega} \theta_{\text{wm}} + \frac{\sin[t\Omega]}{\sin \Omega} \theta_0.
\end{equation}
Evaluating durability on the \textsc{SLERP} merge with the original model provides both a reproducible setting for comparing different OSM watermarks and a more adversarial scenario than practical applications.  
Indeed, merges are performed on models from the same family, and hence, in the case of a watermarked model, all merged models are normally derived from the watermarked model.  

\paragraph{Finetuning}
Model finetuning is widely used to improve pretrained models on a specific domain, usually via additional training on a domain-specific dataset. 
Besides full model finetuning, which updates all model weights via gradient descent, Low-Rank Adaptation (\textsc{LoRA}) \citep{lora} has emerged as an incredibly popular finetuning method that performs parameter-efficient low-dimensional weight updates. 
Besides introducing domain-specific knowledge, one of the common finetuning use cases is instruction finetuning \citep{instruction_tuning}, where a base model is trained to follow the instruction format of a given Q\&A dataset, enabling its usage as a chat model.

Apart from such supervised finetuning (SFT) methods, Reinforcement Learning (RL)-based finetuning \citep{rlhf, rl_survey} is commonly applied to align models with human preferences or enable more complex reasoning behavior. 
Yet, due to the additional complexity of RL-based finetuning, it is, for open-source models, so far significantly less common than SFT.
Hence, to evaluate watermark durability against finetuning, we focus only on SFT as well as instruction finetuning, both on the full weights of the model and with \textsc{LoRA}.
  
