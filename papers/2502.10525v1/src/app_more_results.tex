\section{Additional Results on Watermark Durability}
\label{sec:deep_dives}

In this section, we analyze the ROC curves (Experimental TPR versus FPR) to evaluate in detail the durability of the watermarks against all common model modifications (\cref{ssec:roc_curves}).  
We confirm the observation from \cref{sec:eval} that current OSM watermarks lack durability. 
In \cref{ssec:task_specific_eval}, we measure the durability of the watermark on a specific task when finetuning is performed on the same task.  

\subsection{ROC Curves of OSM Watermarks Against Common Model Modifications}
\label{ssec:roc_curves}

Here, we extend the results from \cref{sec:eval} by presenting ROC curves for all schemes tested under all common model modifications identified in \cref{sec:durability}. 

\input{figures/tex/quant_plots.tex}

\paragraph{Quantization}
In \cref{fig:quantization_plot}, we see no visible difference between 4 bits and 8 bits quantization in the empirical TPR of different watermarking schemes.
This suggests that quantization is not a challenge for current OSM watermarks.
Intuitively, this is an expected result as most quantization techniques try to preserve the model performance as much as possible when quantizing the model.
Hence, by minimizing the impact on model performance, quantization also minimizes the impact on the watermark.

\input{figures/tex/merging_tpr.tex}

\paragraph{Merging}
Similarly, in \cref{fig:merging_tpr}, we see the ROC curve for different values of the \textsc{SLERP} interpolation parameter and different watermarking schemes.
We see that weight-editing watermarks are more durable against merging.
As explained in \cref{sec:eval}, this is an expected result.

\input{figures/tex/pruning_tpr.tex}


\paragraph{Pruning}
In \cref{fig:pruning_tpr}, we see the ROC curve for different values of sparsity $\rho$ and different watermarking schemes.  
For $\rho > 0.5$, the model quality is too low to be usable; hence, we do not compute the TPR for higher sparsity ratios.  
We see that for most schemes tested, the watermark is durable against pruning, even for high sparsity ratios.  
As with quantization, this is an expected result.  
With unstructured pruning, the objective is to find the sparse weights that minimize the distortion in the dense model activations.  
By minimizing such distortion, pruning techniques also preserve the watermark.

\input{figures/tex/ft_tpr.tex}


\paragraph{Finetuning}
In \cref{fig:ft_tpr}, we see the ROC curve for full finetuning on either \textsc{OpenWebText} or \textsc{OpenMathInstruct}.
The conclusion is similar as the one from \cref{tab:main_results_table}: the watermark is not durable against finetuning even for a few steps.
This is unsurprising as with finetuning, we want the model to learn the distribution of the training dataset.
Hence, because the training dataset is not watermarked, its distribution significantly differs from the model's previously learned distribution.
Therefore, finetuning effectively bridges the gap between these two distributions, removing the watermark.
This is why, for most schemes, finetuning on a specific domain (Math) does not necessarily remove the watermark as much as finetuning on a broad/general domain.

\subsection{Watermark Durability on Expert Tasks}
\label{ssec:task_specific_eval}

Here, we evaluate the ability of a watermarked open-source model to retain the watermark signal on newly learned expert tasks, e.g., math.
We use the same watermarks as in \cref{sec:eval}.

\paragraph{Experimental details}
We first instruction-finetune a watermarked model on \textsc{OpenMathInstruct} to teach the model how to solve math questions.  
We use the same hyperparameters as in \cref{sec:eval}.  
Then, instead of measuring the watermark durability on broad C4 prompt completions, we consider $200$-token answers to math questions.  
This is closer to a practical scenario: if a user finetunes a given model on a specific task, it is expected that the model will be used for that task as well, hence if the watermark is not durable, most text produced by this model in practice will effectively not be watermarked.

\input{figures/tex/ft_tpr_task_specific.tex}

\paragraph{OSM Watermarks do not transfer to new domains}
In \cref{fig:ft_tpr_matheval}, we see that none of the tested watermarks are durable when finetuned and evaluated on a specific domain.
This contrasts with the evaluation from \cref{tab:main_results_table}, where we show that finetuning on a specific domain but evaluating the watermark strength on a general domain does not lower the watermark strength as significantly.
This highlights a crucial limitation of the durability of current watermarks for open-source models, and suggests that the watermark strength of the task-specific watermarked model should also be evaluated on similar task-specific datasets, which has been overlooked in previous works \citep{wapiti}.
