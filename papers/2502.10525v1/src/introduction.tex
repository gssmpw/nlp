\section{Introduction}
\label{sec:intro}

As highlighted by recent AI regulations~\citep{aia}, watermarking of large language models (LLMs) to track their outputs is an increasingly important research topic.
Building on earlier work~\citep{aar,kgw,stanford}, recently proposed methods~\citep{synthid-text} have been deployed in large-scale production systems~\citep{synthid}, showing that LLM watermarking is reaching maturity. 
Most existing methods are based on modifying the decoding procedure of the LLM to imprint a later detectable watermark signal. 
As such, these \emph{generation-time watermark mechanisms} are designed for closed models served via an API.  

\paragraph{Open-source LLM watermarking}
This makes most advancements in LLM watermarking fundamentally inapplicable to open-source models (\emph{OSM}). Such models allow users white-box access, including full control over the decoding procedure, which they can use to disable any generation-time watermarking mechanism. 
This is becoming increasingly important as the gap between closed and open models is narrowing---latest versions of Llama~\citep{llama3}, Qwen~\citep{qwen}, and DeepSeek~\citep{deepseek} models have nearly surpassed the performance of best closed-source models.
If this trend continues, malicious parties will be able to circumvent any watermarked API by using OSM to generate high-quality unwatermarked text.
This makes \textit{open-source model watermarking}, i.e., the question of how model providers can embed watermarks directly into their open-weight models, a key focus for the watermarking community.

\paragraph{Prior work}
While OSM watermarking has been recognized as one of the most critical problems in GenAI security~\citep{survey2,genaisurvey,wmsok,whatliesahead}, it has not been systematically studied. 
While there have been a few attempts to embed the watermark directly into model weights~\citep{unremovable, learnability, rlwatermark, gaussmark}, in many works, the specific challenges of OSM watermarking are only a secondary focus.  
Even when OSM is the main focus, there is a lack of clarity regarding problem formulation and evaluation targets---some works consider random adversaries~\citep{unremovable}, while others focus only on the finetuning of the watermarked model, restricted to broad-domain data~\citep{learnability}.

\paragraph{This work}
In this work, we aim to provide the first systematic study of the problem of open-source LLM watermarking, laying the foundation for future research.
\cref{fig:accept} illustrates the OSM watermarking setting and our contributions.
First, we revisit the requirements for generation-time LLM watermarks and discuss how they apply to the OSM case.
Aiming to concretize the specific challenges of the OSM setting, we define a new requirement: \emph{durability against common model modifications}.
While this was not a concern for closed models, considering such non-adversarial changes is crucial for OSM, as these models are typically finetuned, quantized or modified in other ways (\cref{sec:durability}).
As the key goal of watermarking is to protect \emph{every} model output, a watermark that is not durable against such changes fails to fulfill its purpose in most realistic scenarios.
 
We complement this definition by proposing an evaluation procedure for OSM watermark durability based on a collection of the most common model modifications. In our evaluation of all current OSM watermarks (and a new variant that we propose in~\cref{sec:eval}), we find that while most methods endure some modifications, no method is truly durable---establishing durability is a challenging but worthwhile requirement for OSM watermarks.
To motivate future work in this direction, we propose a proof-of-concept experiment on a \textsc{GPT-2} architecture to explore ways to improve watermark durability.  
Our work is a first step toward more systematic study of OSM watermarks.

\paragraph{Key contributions}
Our main contributions are: 
\begin{itemize}
    \item We reinterpret LLM watermark requirements in the context of open-source models, and introduce the critical requirement of watermark durability~(\cref{sec:requirements,sec:durability}).
    \item We survey current OSM watermarks, propose a new variant, and systematically evaluate all methods for durability, concluding that no watermark is truly durable~(\cref{sec:methods,sec:eval}).
    \item We present a proof-of-concept experiment that explores ways to improve watermark durability and identify challenges that remain, motivating future work~(\cref{sec:wm_from_scratch}). 
\end{itemize}

\input{figures/tex/accept.tex} 
