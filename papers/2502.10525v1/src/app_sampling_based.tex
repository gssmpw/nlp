\section{Description of Generation-time Watermarks}
\label{app:sampling_based_wm}

In this section, we introduce a high-level description of two generation-time watermarks, \textsc{KGW} \citep{kgw} and \textsc{KTH} \citep{stanford}. 

\paragraph{KGW watermark}
\textsc{KGW} watermark \citep{kgw} works by partitioning, at each step of the generation, the vocabulary into a Red and Green subset using the private key $\xi$ and summing the hashes of the $k$ previous tokens.  
The Green subset has a size of $\gamma |\Sigma|$, with $\gamma \in [0,1]$.  
Logits of the tokens in the Green subset are boosted by $\delta > 0$, making them more likely to be sampled.  
The watermark detector works by performing a binomial test on the number of Green tokens in the text.  

\paragraph{KTH watermark}
\textsc{KTH} watermark \citep{stanford} is parametrized by a key length $n_{key} \in \mathbb{N}$ and a key $\xi \in [0,1]^{|\Sigma| \times n_{key}}$, where each entry $\xi_k \in [0,1]^{|\Sigma|}$ is uniformly distributed in $[0,1]$.
At each step $t$ of the generation, given a probability distribution $p_t$ over $\Sigma$, the next token is chosen as the $\arg\max$ of $(\xi_{(t \mod n_{key})})^{p_t}$.
Additionally, to allow multiple generations given a fixed prompt, the key is randomly shifted by a constant before generating a new sequence.
Finally, given a text $x$, detection works by performing a permutation test using the minimum Levenshtein distance of the alignment cost $d(x, \xi) = \sum_{t=1}^{|x|} \log(1 - \xi_{(t \mod n_{key}, x_t)})$.
