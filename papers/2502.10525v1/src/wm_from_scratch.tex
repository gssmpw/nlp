\section{Improving OSM Watermark Durability}
\label{sec:wm_from_scratch}
 
In this section, we extend current distillation-based OSM methods introduced in \cref{sec:methods} by significantly increasing the distillation dataset size and further explore a variant that starts from a randomly initialized model (\emph{distillation pretraining}), as opposed to the standard application on top of an already pretrained model~\citep{learnability}.
As a proof of concept, we show that on a \textsc{GPT-2} architecture, distilling on a large training set significantly improves watermark durability.
Moreover, we show that distillation pretraining and standard distillation finetuning exhibit complementary behaviors: distillation pretraining is more durable against specific task finetuning, whereas standard distillation is more durable against broad-domain finetuning.
We further expand on our results in \cref{app:from_scratch_extended}.


\paragraph{Experimental details}
We perform the distillation of the \textsc{KGW} watermark with $\delta=2, \gamma=0.25$, and $k=1$ on a \textsc{GPT-2}-based architecture.
We use the following setup: \textsc{KGW-D (Pretrained)} trains a model with random initialization $\theta \in \Theta$ using \citet{learnability} (see \cref{eq:distillation_learnability}) with $\approx$9B tokens.
For \textsc{KGW-D (Long)}, we finetune an already pretrained model ($\theta_0$) with the same distillation approach, also with $\approx$9B tokens.
As a reference, we also distill the watermark using the same hyperparameters as in \citet{learnability}, \ie with only $\approx 40$ million tokens on top of $\theta_0$ (\textsc{KGW-D (Standard)}).
To evaluate watermark strength, we use the same setup as in~\cref{sec:eval} but with $1000$ completions instead of $100$.
To evaluate durability to model modifications, as in \cref{sec:eval}, we finetune on both broad-domain \textsc{OpenWebText} and task-specific \textsc{OpenMathInstruct} datasets.


\paragraph{Distillation on more tokens is more durable}
In \cref{tab:scratch_table}, we see that both \textsc{KGW-D (Pretrained)} and \textsc{KGW-D (Long)} are significantly more durable against finetuning compared to \textsc{KGW-D (Standard)}.
This confirms our prior intuition that OSM watermark durability scales with the extent of the model's exposure to watermarked text during training.
However, while the results are promising when finetuning on \textsc{OpenMathInstruct}, finetuning on a broad-domain dataset such as \textsc{OpenWebText} still significantly deteriorates the watermark.

\input{tables/from_scratch_table.tex}
\input{figures/tex/ft_tpr_scratch.tex}
\paragraph{Distillation at pretraining is task-aware}
In \cref{fig:ft_tpr_gpt2_scratch} we present more granular results,
aiming to decouple the effects of training a randomly initialized model and simply increasing the number of training tokens.
Namely, we show the difference of TPR between \textsc{KGW-D (Pretrained)} and \textsc{KGW-D (Long)} across different rejection rates and (model modification) finetuning steps.
We observe that the pretraining distillation approach is slightly worse at preserving the watermark when later finetuned on a broad-domain dataset (at most 10\% TPR difference) but better at preserving the watermark when finetuned on a task-specific dataset (up to 20\% TPR increase).

Perhaps more interestingly, as we evaluate the TPR on C4 prompt completions, \ie a broad-domain/general task, we conclude that the distillation pretraining watermarked model may exhibit a form of \emph{task-awareness}: If the model is finetuned on unwatermarked data from a specific task, it will not un-learn the watermark on other tasks.
We hypothesize that this robustness is due to the model only ever being trained on watermarked text, i.e., never seeing unwatermarked text before the model modification stage.
For the standard distillation watermark, even when the number of training tokens is increased to match the pretraining case, we do not observe this behavior, even though it is, interestingly, slightly more durable against finetuning on a general dataset.

\paragraph{Limitations}
Our results suggest both that the way the model distills the watermark matters, as evidenced by the difference between distillation during pretraining and simple finetuning, and that increasing the training set size can indeed improve durability.  
However, the practicality of these effects is limited, as (1) even with much larger dataset sizes, the increase in durability is often insufficient for the watermark to remain effective against prolonged downstream finetuning, and (2) scaling this approach to real-world models is expensive.  
This also makes it unclear how much our results on the comparatively small \textsc{GPT-2} architecture generalize to larger and more capable models.  
