Response To Metareview

We are grateful for the expertise and efforts of all reviewers and area chairs in reviewing our work. We are encouraged and excited by the recognition of the simplicity and efficacy of our proposed framework.

Q1: More analysis and details to support the proposed hypothesis.
A1: In the final version, we would definitely follow the advice of area chairs and elucidate on more details about our analysis of alignment tax. Specifically, following the advice of the reviewers, we delved deeper into our experiment in Section 3 to inspect the alignment tax from a fine-grained token-level perspective. By measuring the correlation between the per-token loss reduction on the training set and the validation set, we discovered that the fitting on the training set tokens is less likely to improve generalization ability on the validation set as the tuning process goes on, which further supports our hypothesis. 


Q2: More analysis regarding the number of clusters and different merging methods
A2: Thanks for your suggestions!  we would move the supplementary experiment results in Appendix B.2 into the main document with extra space.







General Response

We extend our deepest gratitude for the insightful feedback and valuable suggestions provided. Furthermore, we are excited to receive the acknowledgment of the strengths in our work, notably:

+ In-depth analysis of the alignment tax phenomenon and the data bias hypothesis as a possible reason behind it. (Reviewer dZDk)
+ Introduction of a novel framework called Disperse-Then-Merge (DTM) and interesting empirical study on using it to address alignment tax. (Reviewer uC2v and AF1S)
+ Extensive experiments and insightful analysis to support the proposed approach. (Reviewer dZDk, uC2v and AF1S)

We sincerely thank you again for your invaluable suggestions, which mainly focus on the ambiguity of experiment implementation details. We would definitely incorporate your advice and move Appendix A.2 into our main document. Should there be any further concerns or questions, we are fully prepared and eager to address them.






Dear Reviewer AF1S,

Thank you again for your valuable time to review our work and constructive feedback. We posted our response to your comments approximately two days ago, and we wonder if you could kindly share some of your thoughts so we can keep the discussion rolling to address your concerns if there are any.

In the previous response,

1. We make a further illustration of the details of our experiment setting, including the rationale of in-context example numbers, the motivation for employing LoRA in our main experiment, and the core idea of our pilot study. We are sorry for the ambiguity and we would definitely complement the details in the main document.     
2. We perform a significant test on our main experiment and the results show that our approach exceeds the vanilla SFT on a variety of knowledge and reasoning benchmarks, suggesting the efficacy of our approach in mitigating the alignment tax with almost no extra cost at both the training stage and the inference stage.
3. We further explore the impact of the data size on experimental performance, which answers the question concerning the application scenario of our approach. 

We would appreciate it if you could kindly take a look at our response to your comments. If you have any further questions, we are happy to discuss them!

Best regards,

All authors of Paper4688




Dear Reviewer dZDK,

Thank you again for your valuable time to review our work and constructive feedback. We posted our response to your comments approximately two days ago, and we wonder if you could kindly share some of your thoughts so we can keep the discussion rolling to address your concerns if there are any.

In the previous response,

1. We further illustrate our implementation details on the first step of our framework and delineate the robustness of the data dispersal step.     
2. We release a sample of our code in an anonymous Github repository, and we plan to release a more detailed version of our code together with our checkpoint upon publishing.


We would appreciate it if you could kindly take a look at our response to your comments. If you have any further questions, we are happy to discuss them!

Best regards,

All authors of Paper4688


Response to Reviewer uC2v:

Thanks for your time and expertise in reviewing our work! We are excited to receive your recognition that our work is insightful and sound, which is a great encouragement to us. In below We would like to address your concerns one by one:

Question1: The LoRA fine-tuning configuration is not clear until the limitation section.

Answer1: Thanks for your suggestion! 

We are sorry for the ambiguity in implementation details and the unnecessary misunderstanding it incurs. We would definitely move the experiment details in Appendix A.3 into the main document and describe our experiment configuration more clearly in the final version. 

We primarily perform experiments with LoRA as it is widely adopted as a common practice in SFT[2] and it attains similar performance to full-parameter fine-tuning while requiring fewer computation resources. Therefore, we believe employing LoRA for experiments would not incur losses in generalization, following previous work in SFT[1][2][3]. 

[1]Sun, Zhiqing, et al. "Principle-driven self-alignment of language models from scratch with minimal human supervision." Advances in Neural Information Processing Systems 36 (2024)

[2]Xu, Canwen, et al. "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data." Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.

[3]Lee, Ariel, Cole Hunter, and Nataniel Ruiz. "Platypus: Quick, Cheap, and Powerful Refinement of LLMs." NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023.



Question2: The caption of Figure 1 and the typos.

Answer2: Thanks for your suggestions! We would definitely follow your advice, adjusting the caption layout and correcting the typos in the final version. 


Response to Reviewer dZDk:

We are grateful for your time and efforts in reviewing our work and we are grateful to receive your praise for our clear motivation and in-depth analysis. We carefully read through your insightful suggestions and we would like to address your concerns one by one: 

Question1: Will the possible errors in the data distribution step affect the subsequent procedures and the overall performance?
 

Answer1: Thanks for your question! Actually, in the main experiment, we adopt the random dispersal of data since other more sophisticated encoding methods do not have an apparent advantage, as shown in Table 2. Besides, we report the average performance of multiple random seeds, Therefore, our data dispersal step is relatively robust and unlikely to incur cascaded errors. 


Question2: The paper should consider open source code.

Answer2: We totally agree that opensourcing source code is significant for the reproducibility of our work so we create an anonymous Github repo containing a sample code: https://anonymous.4open.science/r/ExpertFusion-8B32. We promise to release the code upon publication with a detailed README file to facilitate relevant research in the future. 



Question3: Limited exploration of the impact of data clustering and model fusion methods.

Answer3: Thanks for your suggestion! We analyze the effect of different data clustering methods and different model merging methods in Section 5.2, comparing $6$ more schemes for clustering in Table 2. Specifically, we use MiniLM and MPNet to encode the instruction (I), the response (R) or both of them (I+R) to obtain their dense representation for K-means clustering. Besides, in Table 4, we explore the effect of $5$ widely-used merging techniques (average merge, fisher merge, task vector merge, tie merge and DARE). Reviewer uC2v also agrees that our analysis of these sophisticated variants is thorough and convincing. We will definitely describe this part more clearly in the final version. Let us know if you have any specific suggestions on what clustering and merging methods should be added. We are more than happy to conduct such experiments.




Response to Reviewer AF1S:

We appreciate your time and effort in reviewing our work! Many thanks for your constructive and valuable feedback, which would definitely make our work better and we would like to address your concerns one by one as follows:

Question1: No experiments on other PEFT techniques such as adapter or full-parameter fine-tuning.

Answer1: Thanks for your suggestion! We mainly perform experiments on LoRA as LoRA is one of the widely adopted PEFT techniques in SFT for its less computation requirement and similar performance to full-parameter fine-tuning. Meanwhile, considering the extensive experiments on large-scale language models, full-parameter fine-tuning is computationally expensive and probably beyond the budgets of many academic institutes. Therefore, following previous works[1][2][3] that only use LoRA for SFT, we believe our experiment configuration would not incur much losses in generalization. 

[1]Sun, Zhiqing, et al. "Principle-driven self-alignment of language models from scratch with minimal human supervision." Advances in Neural Information Processing Systems 36 (2024)

[2]Xu, Canwen, et al. "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data." Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.

[3]Lee, Ariel, Cole Hunter, and Nataniel Ruiz. "Platypus: Quick, Cheap, and Powerful Refinement of LLMs." NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023.



Question2: The improvement against the vanilla fine-tuning baseline seems not significant.

Answer2: Thanks for your advice! We supplement the significant test result in the Table below: 

|            | GSM8K | MMLU  | BBH   | ARC-c | OBQA | RACE  | Humaneval | MBPP | TruthfulQA |
|------------|-------|-------|-------|-------|------|-------|-----------|------|------------|
| Vanilla    | 18.50  | 49.74 | 42.78 | 46.93 | 32.80 | 40.57 | 17.68     | 21.40 | 25.83  |
| DTM (Ours) | 20.62* | 50.43*| 44.46*| 48.72* | 33.80 | 41.34* | 18.29 | 23.60*| 29.13*|

Where $^*$ denotes significant improvement over the vanilla SFT (t-test, $p< 0.05$).

From the table, we can observe that our approach outperforms vanilla SFT significantly on most world knowledge and commonsense reasoning benchmarks. Notably, the evaluation benchmarks used in our experiment are highly comprehensive and challenging (e.g., MMLU contains $57$ sub-tasks and BBH contains $27$ sub-tasks) such that gigantic improvement is rather difficult or even unrealistic, considering that our approach (1) involves no new data and (2) incurs no extra cost at both the training stage and the inference stage. 

Question3: In what scenario is DTM better used? 

Answer3: That is a good question! To explore when our advantage against vanilla SFT is more obvious, we experiment with different sizes of instruction-following data sampled from Tulu-V2-mix and compare our proposed DTM with vanilla SFT. The gap in their performance on BBH (3-shot, measured in exact match) is shown in the Table below:


|  Data Size  | 64k   | 128k  | 192k  | 256k  | 320k  |
|-------------|-------|-------|-------|-------|-------|
| Vanilla SFT | 42.07 | 42.83 | 42.94 | 43.09 | 42.78 |
| DTM (Ours)  | 41.66 | 42.55 | 42.89 | 43.43 | 44.46 |
| Difference  | -0.41 | -0.28 | -0.05 | 0.34  | 1.68  |

From the table, we can observe that DTM has a more obvious advantage at large data size, echoing our previous finding in the pilot study that fitting on data biases is more severe with large data size.

Question4: How to quickly determine the number $K$ of sub-models?

Answer4: It is a good question! The number of sub-models $K$ is a hyper-parameter and we analyze its impact to model performance in Appendix B.2, from which we can see that $K=4$ is probably a good starting point and our approach is relatively insensitive to the choice of $K$ within a specific range. 

Question5: Is the pilot experiment fair? Different amounts of data will have different training steps.   


Answer5: 
Thanks for your question! In the pilot study, we focus on alignment tax, a phenomenon in which LLM performance on standard knowledge and reasoning benchmarks as the size of instruction-following data grows. To perform an in-depth analysis,  we run multiple SFT training varying the instruction-following data size ($20\%, 40\%, 60\%, 80\%,  100\%$) while maintaining the batch size, training epoch number, learning rate schedule and other hyper-parameters unchanged. 

As a result, to draw a fair comparison before different runs, we must adjust the training step accordingly to maintain a constant batch size and constant number of training epochs. Otherwise, pursuing constant training steps suggests adjusting the number of training epochs accordingly (e.g., $1$ epoch for $100\%$ data, $2$ epochs for $50\%$ data, and $5$ epochs for $20\%$ data). Repetitive training on small-scale data may lead to severe overfit and influence our observation. 

Question6: Why are different numbers of in-context examples (shot) set for different tasks at evaluation?

Answer6: Good question! Our evaluation configuration for each task mainly follows precedent work[4][5] and public benchmarks[6] such that our readers can draw a direct comparison. We will put this point more conspicuous in our final version. 

[4]Jiang, Albert Q., et al. "Mistral 7B." arXiv preprint arXiv:2310.06825 (2023).

[5]Ivison, Hamish, et al. "Camels in a changing climate: Enhancing lm adaptation with tulu 2." arXiv preprint arXiv:2311.10702 (2023).

[6]https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
