
\section{Experiments on High-Level Subtasks}
This section answers the research questions regarding our high-level understanding subtasks.


\rqsection{Can Humans understand the abstract representations?}
\label{rq:human_perf}

First of all, we investigate the performance of humans who possess the knowledge required by our \datasetnamens.
For each instance in our \datasetnamens, we asked three independent annotators who were \emph{not} involved in our task design to perform the same classification task presented to the LLMs.
The annotators are provided with the same inputs used as prompts for the LLMs; and are permitted to consult GPT-4o for definitions of concepts they find unclear (mainly happens to the \textsc{Core}-Test set).
The results indicate that our tasks are largely solvable to people with a college-level education.
Specifically, on the \coredatasetns\ tasks, humans achieved an accuracy rate higher than 90\%.
The \harddatasetns\ tasks present greater challenges and subjectivity because the annotations are personalized based on the annotators' individual perspectives and experiences. Despite these challenges, humans can still achieve a notable average accuracy of 77.8\% in solving these tasks.

We conducted a detailed investigation into human performance on a subset of \harddatasetns. Participants were asked to annotate instances where they believed none of the four candidate answers adequately explained the inputs. The results revealed a 10.4\% rate of disagreement. On these disagreed-upon examples, human accuracy was 33.3\%, explaining a major factor for the human performance decline.

\paragraph{Conclusion} Our study demonstrates that {humans can perform the \datasetname tasks quite well}.


\rqsection{Can LLMs understand concepts in the abstract representations of the matrix format?}
\label{rq:matrix_input}

A straightforward solution for our \datasetname is to represent the grid-formatted examples as matrices. By representing the matrices with a token sequence, they can be integrated into an instruction prompt for text-based LLMs, following existing prompting methods for ARC tasks~\cite{acquaviva2022communicating,xu2023llms,wang2023hypothesis,wang2024speak}.
We use the prompt shown in Figure~\ref{fig:matrix_prompt_template} to query the answers from the evaluated LLMs.




\begin{table}[t!]
    \small
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ll c||cc}
    \toprule
    & \multirow{2}{*}{\bf Models}  & \bf Dev  &\multicolumn{2}{c}{\bf Test}\\
    && \bf \textsc{Core}-Dev & \bf \textsc{Core}-Test& \bf \textsc{Assoc.}\\
    \midrule
    & Random & 25.0 & 25.0 & 25.0 \\
    \midrule
    \multirow{7}{*}{\rotatebox{90}{text-only}}& GPT-3.5  & 26.5$_{\pm\text{2.5}}$ &24.4$_{\pm\text{0.8}}$ & 30.0$_{\pm\text{2.5}}$ \\ %
    &GPT-4  & 41.3$_{\pm\text{1.3}}$ &28.2$_{\pm\text{2.3}}$ & 38.3$_{\pm\text{1.2}}$ \\ %
    &GPT-4o  & 34.0$_{\pm\text{2.9}}$ &31.3$_{\pm\text{2.9}}$ & 35.5$_{\pm\text{2.5}}$\\
    &\it o3-mini-high & 46.0$^*$ & \bf 46.5 & 42.5\\
    \cmidrule{2-5}
    &Mistral  & 21.5$_{\pm\text{0.3}}$ &26.0$_{\pm\text{1.4}}$& 23.2$_{\pm\text{0.4}}$\\
    &Llama-3  & 23.5$_{\pm\text{2.5}}$ &27.3$_{\pm\text{0.6}}$& 21.7$_{\pm\text{2.0}}$  \\
    &\it DeepSeek-R1  & 41.5 & 29.5 & \bf 55.0 \\
    \midrule
    \multirow{7}{*}{\rotatebox{90}{multi-modal}} & GPT-4v  & 34.2$_{\pm\text{1.6}}$ & 28.7$_{\pm\text{2.4}}$ & 32.0$_{\pm\text{1.5}}$ \\
    &GPT-4o  & 52.3$_{\pm\text{0.8}}$ & 45.2$_{\pm\text{2.3}}$ & 36.5$_{\pm\text{0.4}}$ \\ %
    &\quad +CoT  & 46.0$_{\pm\text{2.5}}$ &43.5$_{\pm\text{0.8}}$ &  39.5$_{\pm\text{1.1}}$\\
    &\it o1 & \bf {53.0} & 42.5 & 34.5 \\
    &\it Gemini2 FTE & 49.8$_{\pm\text{0.8}}$& 43.2$_{\pm\text{2.0}}$ & 36.8$_{\pm\text{3.1}}$ \\
    \cmidrule{2-5}
    &InternVL   & 26.3$_{\pm\text{1.6}}$ & 26.9$_{\pm\text{4.1}}$& 24.8$_{\pm\text{1.3}}$\\
    &LLaVA  & 26.2$_{\pm\text{1.1}}$  & 28.5$_{\pm\text{1.5}}$&24.7$_{\pm\text{3.2}}$ \\
    \midrule
    &Humans& 92.0$_{\pm\text{4.3}}$ & {89.5$_{\pm\text{5.1}}$} & 77.8$_{\pm\text{6.3}}$ \\
    \bottomrule
    \end{tabular}}
    \caption{Performance of different text-only and multi-modal LLMs on our tasks. InternVL denotes InternVL-Chat-V1-5 and LLaVA denotes LLaVA-NeXT-34B. Gemini FTE refers to the Gemini 2.0 Flash Thinking Experimental model. We use \emph{italic} fonts to refer to the recent thinking models.}
    \vspace{-0.2in}
    \label{tab:performance_matrix_format}
\end{table}

\paragraph{Results}
The top (text-only) section of Table~\ref{tab:performance_matrix_format} presents the results.
All the LLMs perform poorly on the three sets of our \datasetnamens. Notably, GPT-3.5, Mistral, and Llama-3 failed to show significant improvement over random performance.
Even for the remarkable GPT-4, GPT-4o and GPT-4v, their performance is not descent and particularly there is a huge performance gap between them and humans. 

In addition, as our \datasetname is essentially an inductive reasoning task from grid-represented examples, we also tested the thinking (o1-style) models concurrently with our work. We experimented with {\small \texttt{gemini-2.0-flash-thinking-exp-1219}}, {\small \texttt{o1-2024-12-17}}, {\small \texttt{o3-mini-2025-01-31}} and {\small \texttt{DeepSeek-R1}}. The former two models support multimodal inputs.
Because o1 is very slow and especially has a limited quota, we first compare it on a subset of 50 instances for both text and multimodal input. This preliminary study gives an accuracy of 42.0 (text) and 46.0 (visual), where GPT-4 (text) performs {44.0}.
We therefore run the full experiment with o1 (visual) only.
The reasoning models indeed achieve better results in the text-only setting, but fails to significantly improve over GPT-4o. 
The detailed performance decomposition of GPT-4, GPT-4o, o1 and Gemini FTE can be found in Appendix \ref{app:perf_decomp}.

We conducted an in-depth investigation into the discrepancy in the performance of DeepSeek-R1, which achieves strong results on \textsc{Assoc.}-Test but performs poorly on \textsc{Core}-Test. It revealed that R1 tends to develop transformation rules based on physical concepts and then applies these rules to predict the exact outputs from given inputs.
While this strategy works well for ARC examples due to their deterministic nature, the \textsc{Core} examples typically lack this property. We believe that tuning the prompts to provide better guidance could help mitigate this issue, which we leave for future work.


\paragraph{Conclusion}
Comparing the human performance in \ref{rq:human_perf} to the best-performing LLMs reveals a huge gap. 
While these tasks are simple or trivial for humans, LLMs face substantial challenges, indicating a lack of deep understanding.

When comparing LLMs' performance on low-level natural language tasks in \ref{rq:textual_input} to  high-level abstract pattern understanding tasks, we observe significant declines.
This highlights the presence of the \emph{stochastic parrot} phenomenon in LLMs. 
Our dataset also \emph{quantifies the severity of this phenomenon}.
For example, while GPT-3.5 performs on par with GPT-4 on the low-level tasks, it nearly drops to random guessing on our high-level tasks, revealing its tendency to act as a stochastic parrot with the physical concepts in our dataset.







\rqsection{Can multimodal LLMs perform well on our tasks with visual input representations?}
\label{rq:visual_input}



Next, we explore whether multi-modal LLMs can effectively solve our tasks when the input examples are presented as visual images rather than matrices like in \ref{rq:matrix_input}. We use the prompt in Figure~\ref{fig:visual_prompt_template} to query the answers from evaluated LLMs.


\paragraph{Results}
The bottom (multi-modal) section of Table~\ref{tab:performance_matrix_format} shows the results.
Consistent with the observations in \ref{rq:matrix_input}, a significant gap between the performance of LLMs and humans exists.

Notably, the recently introduced GPT-4o outperforms all other LLMs on \coredatasetns\ by 10\% with visual inputs but lags behind GPT-4 on matrix inputs. 
This discrepancy may be due to GPT-4o's training on images that directly illustrate physical concepts, making it more adept at solving problems like in Figure~\ref{fig:teaser}. However, this advantage does not extend to the more abstract problems in \harddatasetns\ that require further knowledge application skills, highlighting the LLMs' lack of deep understanding even with multi-modal training.

Finally, given that LLMs can generate high-quality descriptions of the concepts (see \ref{rq:textual_input}), we adopt a chain-of-thought~\cite{wei2022chain} approach.
It first asks the LLMs to describe each choice and then makes predictions.
The results in Table~\ref{tab:performance_matrix_format} (+CoT) show limited improvement or performance drop, further confirming the presence of the stochastic parrot phenomenon.






\rqsection{Is \datasetname challenging mainly due to LLMs'
unfamiliarity with grid representations?}
\label{rq:format_analysis}
{%
One might argue that the challenges of \datasetname might be due to the uncommon nature of the task format (especially the matrix-format inputs) encountered during LLM training, rather than a lack of deep understanding. 
We disprove this hypothesis from two perspectives:

\emph{(1) We show that \textbf{GPT-4o is actually {familiar with} the grid representations to some extent}.}
Specifically, we conducted a human study to examine GPT-4o's fundamental visual comprehension skills~\cite{girshick2014rich,long2015fully,he2017mask}, including recognizing objects from the grids, describing their colors and shapes, and identifying which objects have their color, shape, or position changed from input to output. 
These tasks correspond to the fundamental computer vision tasks of segmentation and object detection. 

We sampled 60 grid pairs from our dataset and had 3 annotators determine if GPT-4o provides correct answers. For each object, the answer is counted as correct only if the shape, color, and positions are all answered correctly. Our results show an accuracy of 86.7\%, which is significantly better compared to the accuracy on our high-level tasks. This confirms that GPT-4o is indeed familiar with the grid inputs, which aligns with the findings of the concurrent study~\cite{fluid}, but still cannot handle our \datasetname tasks effectively. 

Additionally, we studied Chain-of-Thoughts with the low-level understanding results as intermediate steps. 
On the \coredatasetns-Dev set,
the results below show that it still fails to improve over the vanilla GPT-4o prompting, showing that the GPT-4o  model cannot connect the low-level understanding with high-level concepts.

\begin{table}[h!]
    \small
    \centering
    \vspace{-0.1in}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l c||lc}
    \toprule
    CoT - definitions &46.0$_{\pm\text{2.5}}$  &CoT - low-level &50.7$_{\pm\text{0.5}}$  \\
    \bottomrule
    \end{tabular}}
    \vspace{-0.1in}
    \vspace{-0.1in}
\end{table}


\emph{(2) We show that \textbf{making the LLMs more familiar with the grid representations does not lead to significant improvement}.} Specifically, we conduct the following experiments with text-only LLMs:
\begin{table}[h!]
    \small
    \centering
    \begin{tabular}{l cc}
    \toprule
     \bf Models   &  \bf \textsc{Core} & \bf \textsc{Assoc.}\\
    \midrule
    GPT-4 & 41.3$_{\pm\text{1.3}}$& 39.0$_{\pm\text{0.6}}$ \\
    \quad w/ ICL-3-shot &39.5$_{\pm\text{1.6}}$ &36.2$_{\pm\text{1.7}}$  \\
    \quad w/ ICL-9-shot &32.8$_{\pm\text{1.0}}$  & 39.0$_{\pm\text{1.6}}$ \\
    \midrule
    Mistral& 21.5$_{\pm\text{0.3}}$ & 23.2$_{\pm\text{0.4}}$\\
    \quad w/ FT on syn-tasks & 20.9$_{\pm\text{0.7}}$ &22.5$_{\pm\text{0.5}}$ \\
    \quad w/ FT on ARC & 20.9$_{\pm\text{0.8}}$& 25.5$_{\pm\text{0.9}}$\\
    \midrule    
    Llama-3 &  23.5$_{\pm\text{2.5}}$ & 21.7$_{\pm\text{2.0}}$ \\
    \quad w/ FT on syn-tasks & 23.0$_{\pm\text{1.1}}$ & 23.2$_{\pm\text{2.7}}$ \\
    \quad w/ FT on ARC & 22.2$_{\pm\text{1.6}}$ & 22.4$_{\pm\text{1.2}}$ \\
    \bottomrule
    \end{tabular}
    \caption{Performance of LLMs with in-context learning or fine-tuning on grid-format data.}
    \label{tab:performance_unsup_training}
    \vspace{-0.3in}
\end{table}

\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
    \item \emph{ICL on other concepts. }
    Compare the performance of zero-shot GPT-4 with GPT-4 using in-context learning (ICL) on few-shot examples from concepts other than the assessed one.
    \item \emph{FT on synthetic matrix data. } Compare the open-source LLMs before and after fine-tuning on a large amount of matrix-input data (Appendix~\ref{app:synthetic_data})
    \item \emph{FT on the ARC task. } 
    Compare the open-source LLMs before and after fine-tuning on the original ARC~\cite{chollet2019measure} task, which ensures that all inputs from the \harddatasetns\ examples have been seen during model training.
\end{itemize}
Despite that both the ICL and SFT approaches make LLMs more familiar with matrix-format inputs, neither approach significantly improves the results as shown in Table~\ref{tab:performance_unsup_training}.


\paragraph{Conclusion} GPT-4o is somehow familiar with the grid format and further enhancing the familiarity of grid format for LLMs is not the key to addressing our challenges.

}

\rqsection{How much can LLMs benefit from training on labeled data?}
\label{rq:sup_training}

Many tasks that challenge LLMs can see significant performance boosts through ICL or SFT on labeled data~\cite{hessel-etal-2023-androids,yu2023personality,berglund2023reversal}. 
When such improvements are observed, it suggests that LLMs inherently possess the necessary skills to excel in their tasks, needing only minimal training effort.

We demonstrate that this is not the case for our tasks, where light-weight training of LLMs on labeled data does not improve for our tasks. Given the current lack of large-scale training data for our purpose, we conduct an extreme case study: models learn from the same concepts in \coredataset and are tested on the same concepts in \harddatasetns.
To this end, we select the instances that consists of at least two choices that exist in the \coredatasetns, leaving 80 examples.

\begin{table*}[t!]
    \small
    \centering
    \begin{tabular}{lc||lc||lc}
    \toprule
    GPT-4 & 42.9$_{\pm\text{2.4}}$ & GPT-4o & 40.4$_{\pm\text{2.1}}$ & Llama-3 & 22.1$_{\pm\text{2.8}}$ \\
    \,\,+ ICL on \textsc{Core}  &40.0$_{\pm\text{1.0}}$ & \,\,+ ICL on \textsc{Core}  &37.1$_{\pm\text{2.6}}$ &\,\,+ SFT on \textsc{Core} & 20.9$_{\pm\text{2.7}}$  \\
    \bottomrule
    \end{tabular}
    \caption{Accuracy on the subset of \textsc{Associative} subtask that has overlapped concepts with \textsc{Core}.}\label{tab:performance_sup_training}
    \vspace{-0.2in}
\end{table*}

We conduct the following experiments on this subset to answer \ref{rq:sup_training}:

\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
    \item \emph{ICL on the same concepts. } Compare the zero-shot GPT-4/4o and GPT-4/4o with ICL\footnote{For GPT-4o, we implement ICL with multi-turn dialogues. Each shot in the demonstration is provided in one turn which asks the GPT-4o to explain the image.} on examples for the same concepts from \coredatasetns. Specifically, for each instance, we sample 9 examples from \coredatasetns\ with their labels among the choices of the instance.
    \item \emph{SFT on the \textsc{Core} set. } Compare the open-source LLMs before and after fine-tuning on labeled data from \coredatasetns.
\end{itemize}


\paragraph{Results} 

Table~\ref{tab:performance_sup_training} shows that ICL and SFT on the labeled examples of the same concepts lead to a consistent, though not severe, drop in performance.
The results suggest that the models have become overfitted to the "clean" examples from the \coredatasetns. They appear to have learned superficial correlations from the demonstrations that do not generalize well, providing further evidence of the stochastic parrot phenomenon.
The difficulty of generalization \emph{within the same concepts} indicates the challenges of our tasks to the supervised fine-tuning paradigm.

\paragraph{Conclusion} Together with the results for \ref{rq:format_analysis} and \ref{rq:sup_training}, it suggests that the low performance of LLMs is not likely to be improved from prompting techniques alone. There exists intrinsic inefficiency in the pre-training of LLMs, which results in the lack of necessary skills for deep understanding.






