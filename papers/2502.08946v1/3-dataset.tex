\section{Task Design and Dataset Construction}









\subsection{Task Design Principle}
We borrow the idea of Bloom's taxonomy~\cite{krathwohl2002revision,armstrong2010bloom} from education research to fulfill the requirements for task design in Section~\ref{sec:towards}, so as to ensure the assessment validity.
Bloom's taxonomy offers an ideal principle to these requirements with an ordering of six cognitive skills (from low to high level) for knowledge understanding: \emph{Remembering, Understanding, Applying, Analyzing, Evaluating and Creating}.

Generally, it is nontrivial to strictly follow this principle since there is no clear boundary among the last four skills of understanding. 
As a result, we group the last four high-level skills into one and consider the following two levels of understanding:
\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
    \item \emph{Low-level Understanding}: covering the two lowest-level skills in Bloom's taxonomy, \emph{i.e.}, retrieving relevant knowledge from long-term memory and rephrasing in one's own words.
    \item \emph{High-level Understanding}: covering the aspects for understanding the knowledge beyond memorization. As shown by the examples in Section \ref{ssec:high-level}, our tasks directly correspond to a spectrum from the understanding level of applying to the level of analyzing in Bloom's taxonomy, 
    \emph{e.g.}, \emph{applying} the knowledge to explain a physical phenomenon, \emph{analyzing} a concrete property of a concept in a generalized and abstract manner,\footnote{For example, the flow of electric current can be abstracted as \emph{moving} from high potential to low potential.}. %
\end{itemize}
Based on these two levels, we design the following \datasetname task for summative assessment. 



\subsection{Our \datasetname Task}

\datasetname is essentially a physical concept understanding task, which primarily targets on 52 physical concepts or phenomena: {\em e.g., gravity, light reflection, acceleration, buoyancy, inertia, etc} (see Appendix~\ref{app:dataset_details} for the full list).
Our focus on physical concepts is motivated by two main reasons:
1) understanding physical concepts is critical for intelligent systems to interact with the world, which is ultimate goal of embodied AI~\cite{savva2019habitat,duan2022survey,xiang2024language}; 
2) designing tasks centered around physical concepts allows us to more easily control different levels of understanding and ensure the diversity of each concept. 



For each physical concept, \datasetname involves both low-level understanding subtasks and high-level subtasks, following our task design principles. %






\subsubsection{Low-level Understanding Subtasks}
\label{sec: low-level}

\paragraph{Physical Concept Selection (text)} 
First, to evaluate whether an LLM possesses the knowledge of our included concepts,
we design a task to recognize a concept from its corresponding Wikipedia definition.
Specifically, we manually masked the synonyms of the concept with placeholder {\small \texttt{[PHENOMENON]}}. Meanwhile, highly relevant entities were masked as {\small \texttt{[MASK]}} to alleviate shortcuts. For example, in the definition of \emph{Gravity}, the terms ``gravity'' and ``gravitation'' were masked as {\small \texttt{[PHENOMENON]}}, while ``Isaac Newton'' was masked as {\small \texttt{[MASK]}}. 
Details can be found in Appendix~\ref{app:rq1_details}.
We then present the LLMs with the same four choices as in our following high-level subtasks.

\paragraph{Physical Concept Selection (visual)} 
Second, we evaluate if the LLMs can recognize our concepts represented with real-life pictures.
To this end, we query our concepts on Google image search, and select the images that reflect the same core properties and examples annotated in our following high-level tasks.
This results in 100 examples.
We construct the same four-choice instances as above.

\paragraph{Physical Concept Generation}
Finally, we directly ask the LLMs to generate the description of a concept with its core properties and representative examples.
For instance, the concept \emph{Gravity} is described as ``\emph{a force that pulls objects with mass towards each other}'', followed by the example ``\emph{an apple falls to the ground}'' as shown in Figure~\ref{fig:teaser}.
We then evaluate the performance of LLMs by measuring the quality of the description and its coverage of knowledge required by our \datasetname and we employ both automatic and human metrics as presented in Section 5.2.
This provides a quantitative measure of the knowledge LLMs can recall in the context of our assessment. 







\subsubsection{High-level Understanding Subtasks}
\label{ssec:high-level}
The low-level subtasks are depicted in natural language thus are likely to be remembered by the LLMs due to their extensive training data.
To assess whether the LLMs possess a deep understanding of the knowledge, we require the subtasks that can 1) represent the high-level understanding skills; 2) avoid the effects of memorization. 

The Abstraction and Reasoning Corpus (ARC)~\cite{chollet2019measure} provides a compelling way by using grids (or matrices) instead of texts to represent a concept. While the LLMs have seen matrices during pre-training, the data is less likely to be correlated to physical concepts. We hence adopt this idea to represent our subtask as abstract representations in the grid world that associate to the key properties of a physical concept. 

\paragraph{The \coredataset Set}

Our first subtask aims to cover the core properties or most representative examples/applications of the assessed concepts.
To ensure our set remains generally comprehensible to humans, we maintain a high school-level difficulty and selected 52 common physical concepts within the curriculum.
To enhance the diversity and richness, five annotators have labeled multiple core aspects of each concept. 
For example, the annotated core aspects of \emph{Gravity} include \emph{attraction between two bodies, motion on an inclined plane, objects falling to grounds} and \emph{orbital motions}.



For each aspect of a concept, the annotator is asked to draw several pairs of abstract grid representations. The aspect of the concept is guaranteed to be illustrated by the pair, such that it explains the transformation from the input to the output. For example, Figure~\ref{fig:teaser} forms a direct abstract visualization of the \emph{Gravity} concept from textbooks, \emph{i.e.}, \emph{apple falling from a tree}.
This results in 1,200 paired grid examples for the 52 concepts, which form 400 3-shot instances.


Figure~\ref{fig:level_examples} presents two examples from this subtask that delve deeper into the concept of \emph{Gravity}
compared to Figure~\ref{fig:teaser}.
The top example demonstrates an application of the \emph{inverse square law of gravity}.
The bottom one presents a parabola, linking the knowledge of \emph{gravity} to \emph{inertia}.
These examples demonstrate the difficulty of inferring their ground-truth labels solely by recalling the concept of \emph{Gravity} without high-level understanding skills.

\begin{figure}[t!]
\centering
\includegraphics[width=0.8\columnwidth]{figure/gravity_examples.png}
\vspace{-0.1in}
\caption{Examples of input-output grids labeled as \emph{Gravity}, with increasing difficulty levels.}
\label{fig:level_examples}
\vspace{-0.25in}
\end{figure}

\paragraph{The \harddatasetns\ Set}
Many instances in the original ARC dataset can be solved via association or analogy to physical concepts.
Therefore, as a second source of subtasks, we ask annotators to manually pick input-output grids from ARC that can evoke their associations to specific physical concepts and assign these concepts as ground-truth labels.
Different from \coredatasetns, we adopt an open-coding schema and allow the inclusion of new concepts during annotation.
The annotators have reviewed 500 ARC instances to filter out the required ones.
After cross-validation to ensure agreement, it results in a collection of 200 instances with physical concept labels.

This relabelling approach covers additional 15 physical concepts.
The resulted subtasks have each example represent an abstract aspect of a concept with possible distracting information. Consequently, the resulted task is more subjective hence more challenging than the \emph{\coredatasetns}Set.



 

\paragraph{Creation of Classification Tasks}


We create \emph{four-choice} tasks on the annotated data. Each instance consists of 3 unique grid pairs as input examples.
This results in 200 instances for \coredataset development set, 200 instances for \coredataset test set, and 200 instances for \textsc{Associative} respectively.
For each instance, we select three additional labels from our concept pool, along with the ground-truth label, as candidate options.
We manually avoid ambiguity during the negative sampling. For example, if \emph{Gravity} is the ground-truth, concepts like \emph{Magnet} will not be sampled.




\section{Overview of Our Studies}
\label{sec:overview}
In the following sections, we conduct a series of studies on our \datasetname tasks. Our studies are organized into six \emph{Research Questions (RQs)}, through which we aim to answer three \emph{Hypotheses} as shown in Figure~\ref{fig:research_overview}.
In summary, we propose to:
\begin{figure}[t!]
\centering
\vspace{-0.05in}
\resizebox{0.5\textwidth}{!}{\includegraphics[width=\columnwidth]{figure/concept_understanding.pdf}}
\vspace{-0.2in}
\caption{Overview of the research questions answered in our study and their relationships.}
\label{fig:research_overview}
\vspace{-0.2in}
\end{figure}



(1) Examine the quantitative disparity in LLMs' performances between low-level (\ref{rq:textual_input}) and high-level subtasks (\ref{rq:matrix_input}, \ref{rq:visual_input}). 
This aims to highlight \textbf{the existence of stochastic parrot phenomenon} in LLMs' understanding of physical concepts.

(2) Assess the performance gap between LLMs (\ref{rq:matrix_input}, \ref{rq:visual_input}) and humans on our high-level subtasks (\ref{rq:human_perf}). This aims to demonstrate that LLMs \textbf{fall significantly short of human understanding}.

(3) Investigate the shortcomings of in-context learning and supervised fine-tuning in improving LLMs on our high-level subtasks (\ref{rq:format_analysis}, \ref{rq:sup_training}). This aims to underscore the \textbf{intrinsic limitations} of SOTA LLMs in achieving deep understanding.

\paragraph{Experimented Models}
We use commercial LLMs, including GPT-3.5 ({\small \texttt{gpt-3.5-turbo-1106}}), GPT-4 and GPT-4v ({\small \texttt{gpt-4-turbo-2024-04-09}}), GPT-4o ({\small \texttt{gpt-4o-2024-05-13}});
and open-source LLMs, including Llama-3 ({\small \texttt{Llama-3-8B-Instruct}})~\cite{llama3} and Mistral ({\small \texttt{Mistral-7B-Instruct-v0.2}})~\cite{jiang2023mistral}, InternVL-Chat-V1-5 
~\cite{chen2023internvl,chen2024far}%
and LLaVA-NeXT-34B~\cite{liu2023improvedllava,liu2023llava}.
We use the default
inference configurations of the LLMs.
Considering the randomness, we run each experiment 3 times and compute the average and standard derivation.
We also experimented with the recent thinking models like o1.











