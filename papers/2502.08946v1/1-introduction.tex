\section{Introduction}









Recent years have witnessed remarkable advancements in large language models (LLMs)~\cite{brown2020language,achiam2023gpt,team2023gemini}. 
Thanks to the substantial model capacity and massive training data, LLMs have achieved new state-of-the-arts on a variety of NLP tasks, even surpassing humans on some of them~\cite{min2023recent,chang2024survey}.
Nowadays the application of LLMs has become widespread, facilitating daily work and life, and profoundly influencing people's work and lifestyles~\cite{bommasani2021opportunities,peng2024large,demszky2023using}.

\begin{figure}[t!]
\centering
\includegraphics[width=0.94\columnwidth]{figure/teaser_new.png}
\vspace{-0.1in}
\caption{Illustration of a ``Stochastic Parrot'' by our \datasetname task consisting of both \textcolor{brown}{low-level} and \textcolor{blue}{high-level} subtasks in parallel. For a concept {\em Gravity}, an LLM can generate its accurate description in natural language, but cannot interpret its grid-format illustration.}
\label{fig:teaser}
\vspace{-2ex}
\end{figure}

On the other hand, despite the great success of LLMs, many researchers argue that {\em LLMs may not really understand what they claim they do}~\cite{bender2020climbing,bender2021dangers,bommasani2021opportunities,mitchell2023debate} due to their strong memorization ability.
In particular, \citet{bender2021dangers} questioned whether LLMs are just \textit{Stochastic Parrots} that repeat words based on correlations without true understanding. 
This argument has been acknowledged by many research papers and dozens of them even include this term in their titles.\footnote{\url{https://scholar.google.com/scholar?hl=en\&q=llms+are+stochastic+parrot}.} Unfortunately, to our best knowledge, there are no quantitative experiments to verify the stochastic parrot phenomenon in LLMs. Existing studies indicate that LLMs may fail on one particular challenging task~\cite{chakrabarty-etal-2022-flute,shapira-etal-2023-well,hessel-etal-2023-androids,tong2024metaphor}, but they do not demonstrate that LLMs claimed to understand those tasks by providing a controlled and paired evidence. 

This paper aims to provide quantitative evidence to validate the argument of stochastic parrot in LLMs. To this end, from the perspective of educational and cognitive psychology, we first employ the approach of summative assessment~\cite{black1998assessment,black1998inside} to measure understanding in LLMs. Its key idea is to design various tasks that test different understanding levels regarding a specific concept.
Following the principle of Bloom's taxonomy~\cite{armstrong2010bloom,krathwohl2002revision}, we design tasks that reflect different levels of understanding.
Consequently, we develop \datasetnamens, a task designed to assess understanding of basic physical concepts from high school such as {\em Gravity}.
Our focus on physical concepts stems from both their fundamental relevance to important topics of world models and embodied systems~\cite{savva2019habitat,duan2022survey,xiang2024language}, and their rich denotations and connotations that enable effective design of summative assessment tasks.

Specifically, ~\datasetname includes two subtasks corresponding to two coarse levels of understanding in Bloom's taxonomy, as shown in Figure~\ref{fig:teaser}. One is the low-level understanding subtask in the natural language format, aimed at measuring the remembering (or memorization) ability of LLMs. The other involves the same concepts but in an abstract representation format inspired by~\cite{chollet2019measure}, which is designed to measure the high-level understanding beyond remembering of LLMs.


We conduct comprehensive experiments on \datasetname with representative open-source and commercial LLMs.\footnote{{Throughout this paper, LLM refers to either standard text-only LLMs or large multimodal models for simplicity.}} We obtain two key findings: (1) State-of-the-art LLMs perform perfectly on the low-level understanding subtask ($>$95\% in Accuracy) but lags behind humans by a large margin ($\sim$40\% in Accuracy) on the high-level subtask, which verifies the stochastic parrot phenomenon in LLMs.
(2) Further analysis shows that our high-level subtask challenges LLMs due to the intrinsic difficulty of deep understanding rather than the unfamiliar format. 

This paper makes the following contributions:
\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
    \item We introduce a psychology-appealing approach (summative assessment) and a corresponding task \datasetname to measure the understanding of LLMs. 
    \item Based on \datasetnamens, we provide a quantitative experiment to successfully verify the stochastic parrot phenomenon in LLMs. 
    \item {As a by-product, our work presents a challenging comprehension task for existing text-only and multimodal LLMs, which establishes a substantial performance gap between humans and machines.}
\end{itemize}
