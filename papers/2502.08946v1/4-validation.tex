\section{Validation on Low-Level Subtasks}


To illustrate the stochastic parrot phenomenon with \datasetnamens, a necessary condition is to ensure the LLMs can perform well on the low-level understanding subtasks, \emph{i.e.}, whether LLMs exhibit strong skills of \emph{recalling} and \emph{describing} the definitions, core properties and representative examples of the physical concepts in our tasks. That is:



\rqsection{Can LLMs perform well on low-level subtasks, i.e., understanding the definitions of physical concepts in natural language?}
\label{rq:textual_input}

To answer \ref{rq:textual_input}, we evaluate the LLMs' abilities to comprehend the definitions of these concepts and generate their descriptions and examples in natural language, as defined in Section~\ref{sec: low-level}.

\subsection{Concept Selection Subtask}
\paragraph{Settings} 
We provide the standard definition of a concept based on Wikipedia with its synonyms masked; then ask the LLMs to identify the concept, under the same four-choice setting throughout the experiments.
We evaluate the representative text-only LLMs and compute the accuracy. 

\begin{table}[t!]
    \small
    \centering
    \begin{tabular}{ccccc}
    \toprule
    \multirow{2}{*}{(a)}& \bf Mistral & \bf Llama-3 & \bf GPT-3.5 & \bf GPT-4 \\
     \cmidrule{2-5}
    & 81.0$_{\pm\text{1.3}}$& 88.5$_{\pm\text{0.7}}$& 97.3$_{\pm\text{0.3}}$ & 95.0$_{\pm\text{0.9}}$\\
    \bottomrule
    \toprule
    \multirow{2}{*}{(b)}& \bf InternVL & \bf LLaVA & \bf GPT-4v & \bf GPT-4o \\
     \cmidrule{2-5}
    & 66.3$_{\pm\text{7.7}}$ & 66.7$_{\pm\text{5.8}}$ & 93.7$_{\pm\text{0.9}}$ &93.7$_{\pm\text{0.5}}$\\
    \bottomrule
    \end{tabular}
    \vspace{-0.1in}
    \caption{Accuracy on the text-based (a) and visual-based (b) concept selection subtasks.}
    \label{tab:selection}
    \vspace{-0.2in}
\end{table}
\paragraph{Results} Table~\ref{tab:selection} shows that the GPT (both text-based and visual-based) models perform near perfect on 
recognition of our physical concepts from standard text-based definitions and from the real-life images.
Moreover, we observed that open-source models make more mistakes compared with the closed-source models due to the smaller model size. For the text-based models, both Mistral and Llama-3 are not as good as the closed-source models. Surprisingly, both InternVL and LLaVA are much worse than the open-source GPT models. One possible reason to this discrepancy is that our text-based concepts are from Wikipedia which is usually used as a part of the training data for open-source LLMs. In contrast, some of our selected images for those concepts may not be included in the training data of both InternVL and LLaVA which thereby can not memorize those visual instances. 


\subsection{Concept Generation Subtask}
\paragraph{Settings}
We evaluate the descriptions LLMs generate for a concept. 
The evaluation of text generation is in general difficult. Moreover, in our scenario each concept have many different ground-truth examples in its description, thus existing automatic metrics such as BLEU~\cite{papineni2002bleu} and METEOR~\cite{banerjee2005meteor} are not capable of accurately measuring the quality. 
Therefore, we rely on mainly human evaluation for this subtask. We also propose an automatic metric via a self-play game for completeness in Appendix~\ref{app:self_play}. 


\paragraph{Human evaluation metric} We ask the annotators to evaluate the quality of the generated descriptions. The evaluation uses binary scores: each description receives a score of 0 if it consists of any factual error on the concept itself
or any unfaithful examples,\footnote{For example, if the LLMs generated a wrong year in the description, it is not counted as incorrect physical knowledge.} 
and a score of 1 otherwise. %

\begin{table}[t!]
    \small
    \centering
    \begin{tabular}{cccc}
    \toprule
      \bf Mistral & \bf Llama-3 & \bf GPT-3.5 & \bf GPT-4 \\
     \midrule
    92.6& 100 &100 & 100\\
    \bottomrule
    \end{tabular}
    \vspace{-0.1in}
    \caption{Human evaluations on concept generation.}
    \vspace{-0.2in}
    \label{tab:generation}
\end{table}

\paragraph{Results}
The results of automatic and human evaluations are shown in Table~\ref{tab:generation}. 
According to human evaluation, there are no factual errors in the generated descriptions except for Mistral,
confirming that our selected concepts rely on basic and widely accepted knowledge.
Thought accurate, the open-source LLMs sometimes include correct but uncommon facts, \emph{e.g.,} listing single-slit diffraction as an example of \emph{Wave Interference}.
The additional self-play results in Appendix~\ref{app:self_play} further justify that all LLMs can accurately recognize the concepts from the descriptions they wrote by themselves.
Combining the conclusions, 
it shows the LLMs can generate correct and sufficient information.

\paragraph{Remark} We asked the annotators of our \textsc{Core} set to evaluate whether the core properties they annotated are covered by the LLMs' generated descriptions.
This corresponds to measuring the recall of the generated descriptions on core properties/examples of concepts from \coredatasetns. The recall rates for GPT-3.5 and GPT-4 are \emph{85.0} and \emph{90.0}, respectively. 
Of course, there are some exceptional examples from \coredatasetns\ missed in the descriptions. One example is that the LLMs fails to draw the connection between \emph{movable pulley} and the \emph{Lever} concept. Moreover, by manually checking these missed properties and examples, we found that most of them can be recalled if we query the LLMs in a second turn by prompting ``{\small \texttt{Any more core properties or examples?}}''. This confirms that the LLMs are \emph{aware of} and are \emph{able to recall} the core properties of concepts covered by the \coredatasetns, though some of them may not have the top conditional probabilities of generation.


\paragraph{Conclusion} LLMs understand the concepts covered by \datasetname in natural language format.
Notably, we find that the {properties and examples annotated in \coredatasetns\ are \emph{within the LLMs' knowledge} and are \emph{highly likely to pop up} when the corresponding physical concepts are queried}.
