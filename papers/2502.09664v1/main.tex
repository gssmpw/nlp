\documentclass{article}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{booktabs} 
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025}

\input{math_commands.tex}

\newif\ifcomments
\commentstrue 

\newcommand{\comDC}[1]{\ifcomments\textcolor{red}{\texttt{DC:} #1}\fi}
\newcommand{\comEA}[1]{\ifcomments\textcolor{magenta}{\texttt{EA:} #1}\fi}
\newcommand{\comGTG}[1]{\ifcomments\textcolor{purple}{\texttt{GG:} #1}\fi}

\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}

\renewcommand{\P}{\mathbb{P}}
\newcommand{\ind}{\mathbb{1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\usepackage[textsize=tiny]{todonotes}

\usepackage{url}
\usepackage{tikz}

% FOR STRIKING TEXT
\usepackage{ulem}

\icmltitlerunning{Image Super-Resolution with Guarantees via Conformal Generative Models}

\begin{document}
\twocolumn[
\icmltitle{Image Super-Resolution with Guarantees\\ via Conformal Generative Models}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Eduardo Adame}{equal,FGV}
\icmlauthor{Daniel Csillag}{equal,FGV}
\icmlauthor{Guilherme Tegoni Goedert}{FGV}
\end{icmlauthorlist}

\icmlaffiliation{FGV}{School of Applied Mathematics, Getulio Vargas Foundation, Rio de Janeiro, Brazil}

\icmlcorrespondingauthor{Daniel Csillag}{daniel.csillag@fgv.br}
\icmlcorrespondingauthor{Eduardo Adame}{eduardo.salles@fgv.br}

\icmlkeywords{Machine Learning, ICML, Conformal Prediction, Conformal Risk Control, Image Super-Resolution, Generative Models, Uncertainty Quantification}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\begin{abstract}
    The increasing use of generative ML foundation models for image super-resolution calls for robust and interpretable uncertainty quantification methods.
    We address this need by presenting a novel approach based on conformal prediction techniques to create a `confidence mask' capable of reliably and intuitively communicating where the generated image can be trusted.
    Our method is adaptable to any black-box generative model, including those locked behind an opaque API, requires only easily attainable data for calibration, and is highly customizable via the choice of a local image similarity metric.
    We prove strong theoretical guarantees for our method that span fidelity error control (according to our local image similarity metric), reconstruction quality, and robustness in the face of data leakage. Finally, we empirically evaluate these results and establish our method's solid performance.
\end{abstract}

\section{Introduction}
 
Generative ML foundation models led to massive leaps in the capabilities of modern image synthesis and processing, spanning domains such as image generation, inpainting, and super-resolution.
Particularly in the case of image super-resolution, recent methods have become considerably adept at leveraging patterns in images to better recover complex textures, geometries, lighting and more.

In testament to these improvements, leading manufacturers are constantly improving and deploying tools based on these frameworks in every new generation of consumer devices. These widespread applications highlight an important question: How trustworthy are the predictions of these models?
When a model does some particular inpainting or super-resolution infill, what guarantees do we have that its predictions are truly accurate to reality, and not mere hallucinations? Therefore, it would be most desirable to have a proper uncertainty quantification over the predicted image.

However, most of the previous contributions to this endeavor have suffered from a lack of interpretability. In order to be widely adopted, a new framework should clearly communicate its uncertainty estimates to the public in a way that reflects how they will be used. This demand for interpretability fundamentally guides the properties and theoretical guarantees we seek to establish for our predictions, and thus on the underlying procedure. % Put guarantees 
Of course, all of this is compounded by the usual challenges of having to do trustworthy uncertainty quantification that is model agnostic and can be employed effectively atop any ``black-box" foundation model.

In this paper, we address all these issues by proposing a method based on techniques from conformal prediction ~\citep{vovk-cp} and conformal risk control~\citep{conformal-risk-control}, while employing metrics designed for interpretability in concrete applications.
All our method requires is a handful of unlabelled high-resolution images that were not present in the training set for the diffusion model, and we achieve strong guarantees on our predictions that are also intuitive to the user and robust to violations of our key assumptions. 

Our main contributions are:

\begin{itemize}
    \item A new method to quantify uncertainty in images inpainted or augmented by diffusion-based models. Our method can work atop any black-box diffusion model, including models that are locked behind an opaque API, and requires only easily-attainable unlabelled data for calibration.
    \item We identify additional theoretical guarantees enjoyed by our model. In particular, we prove that our method also controls the PSNR of the predicted images, and show that it is reasonably robust to data leakage, reinforcing the effectiveness and robustness of our method.
    \item A comprehensive study of modelling choices in our approach,  revealing particular modifications from the base procedure that can significantly enhance performance. Particularly, we demonstrate that certain applications of low-pass filters can greatly improve our method's effectiveness.
\end{itemize}

\paragraph{Related work} There have been some attempts on uncertainty quantification for image super-resolution with varying form. But most existing works, e.g., \citep{prev-interval-1}, \citep{prev-interval-2}, have as their goal to produce, rather than a single image, an interval-valued image (i.e., image where each pixel is represented by an interval rather than a single value), and ensure that these intervals will, with high probability, contain the `true' pixel values. However, this is fairly non intuitive for the user and its underlying guarantees are a bit lax. As far as we are aware, the only existing solution that proposes some other way of quantifying uncertainty is \citep{prev-mask-noguarantee}, which produces a mask over the predicted image indicating the trustworthy region, much like us. However, their solution has no probabilistic guarantee, which is fundamental for reliable uncertainty quantification. Ours, in contrast, is backed by a plethora of such theorems. Finally, our proposed solution is strongly related to the existing applications of conformal prediction to semantic segmentation, e.g., \citep{amnioml, semantic-seg-cp}.

\section{Method}

\subsection{Conformal Mask Calibration}\label{sec:mask-calib}

Let us be supplied with any generative image super-resolution model $\mu : [0,1]^{w \times h \times 3} \to [0,1]^{kw \times kh \times 3}$, where $w$ and $h$ stand for the low-resolution image dimensions, and $k$ is the upscaling factor. Naturally, this is a stochastic function due to the generative nature of the model, so an intuitive (albeit nonrigorous) way to quantify model indecision would be to aggregate many realizations of the output image (e.g. by computing the variance of generated pixels). However, our methodology is capable of working naturally when supplied with an arbitrary estimate of model indecision that can be described by a function $\sigma : [0,1]^{w \times h \times 3} \to \R^{kw \times kh}$. We further discuss useful constructions of $\sigma$ in Section~\ref{sec:score-masks}.

Having received a lower resolution image $X$, we must consider how the model prediction $\widehat{Y} = \mu(X)$ differs from the true high resolution image $Y$. Our goal is to find a ``confidence mask'' $M(X)$ that indicates the region of the image whose content we trust, i.e. the pixels in the predicted image with indecision below a sought threshold. Formally, the mask is a (possibly stochastic) function $M: [0,1]^{w \times h \times 3} \to \{0,1\}^{kw \times kh}$ that has image $M(X) =  \left\{ p \in \Z \times \Z : [\sigma(X)]_p \leq t \right\}$, where $[ \bullet]_p$ is the image value at pixel $p$ (be it binary, grayscale or colored) and $t$ is a desired indecision threshold.

We seek masks that satisfy fidelity guarantees between the true and predicted high-resolution images. This fidelity is measured by a \textbf{fidelity error} defined as $\sup_{p \in M} D_p (Y, \widehat{Y})$, where $D_p$ is a function that measures the difference between two images around some pixel location $p$. We can employ any local measure of the difference as long as $0 \leq D_p (Y, \widehat{Y}) \leq 3$ for all $p$, $Y$ and $\widehat{Y}$ (e.g. $D_p (Y, \widehat{Y}) = \| [Y]_p - [\widehat{Y}]_p \|_1$). There are many different useful choices for $D_p$, with a few explored in Section~\ref{sec:fidelity-risk}.

Equipped with the previous definitions, we are able to produce these fidelity masks $M_\alpha(X)$ for any desired fidelity level $\alpha \in [0,1]$ with the guarantee that 

\[ \E\left[ \sup_{p \in M_\alpha (X)} D_p \left( \mu(X), Y \right) \right] \leq \alpha \]

\noindent by thresholding the output of $\sigma$ by some parameter $t$ in the construction of the mask. This parameter can then be calibrated for by using techniques of conformal prediction~\citep{vovk-cp} and conformal risk control~\citep{conformal-risk-control} with just access to unlabelled hold-out data, which has not been used to obtain either $\mu$ or $\sigma$ (if there is data contamination, weaker guarantees hold; see Proposition~\ref{thm:data-leakage}).
In particular, given such data $(X_i, Y_i)_{i=1}^n \subset [0,1]^{w \times h \times 3} \times [0,1]^{kw \times kh \times 3}$, we produce
\begin{equation} \label{eq:threshold}
\begin{multlined}
    t_\alpha = \sup \left\{ t \in \R \cup \{+\infty\} :  
     \frac{1}{n+1} \sum_{i=1}^n \right. \\ \left. \sup_{p; [\sigma(X_i)]_p \leq t} D_p(Y_i, \mu(X_i)) + \frac{3}{n+1} \leq \alpha \right\},
\end{multlined}
\end{equation}
thus obtaining
\begin{equation} \label{eq:mask-threshold}
    M_\alpha (X) := \left\{ p \in \Z \times \Z : [\sigma(X)]_p \leq t_\alpha \right\}.
\end{equation}

Crucially, all of our guarantees will hold for \emph{any} $\mu$ and $\sigma$, though the produced trust masks $M_\alpha$ will generally be larger (i.e., more confident) for well-chosen ones.

This methodology comes with a strong statistical assurance: a marginal ``conformal'' guarantee. It holds in expectation on the calibration data jointly with an additional new, `test' sample:

\begin{theorem}[Marginal conformal fidelity guarantee] \label{thm:conformal-guarantee}
    Let $\alpha \in \R$ and $n \in \N$.  
    Suppose we have $n+1$ i.i.d.\footnote{Technically, Theorem~\ref{thm:conformal-guarantee} holds under the weaker assumption of exchangeability, with the same proof. We stick to i.i.d. for simplicity.} samples $(X_i, Y_i)_{i=1}^{n+1}$ from an arbitrary probability distribution $P$ and let $t_\alpha$ and $M_\alpha$ be as in Equations~\ref{eq:threshold} and \ref{eq:mask-threshold} (and thus only a function of $X_i, Y_i$ with $i = 1, \ldots, n$). Then it holds that
    \[ \E_{(X_i, Y_i)_{i=1}^{n+1}}\left[ \sup_{p \in M_\alpha (X_{n+1})} D_p\left( \mu(X_{n+1}), Y_{n+1} \right) \right] \leq \alpha. \]
\end{theorem}


Thanks to the discrete combinatorial structure of the set the infimum is taken over in Equation~\ref{eq:threshold}, the indecision threshold $t_\alpha$ (defined in Equation~\ref{eq:threshold}) can be efficiently computed with the use of dynamic programming in $O(n d \log d)$ time, where $n$ is the number of images in the calibration set and $d$ is the number of pixels in each image. In contrast, a naive brute force algorithm would take $\Omega(n^2 d^2)$ time.

\subsection{Producing Score Masks}\label{sec:score-masks}

A key component of our algorithm is the model indecision estimate $\sigma : [0,1]^{w \times h \times 3} \to \R^{kw \times kh}$.
A good $\sigma$ should attain higher values for regions of the image where there is more uncertainty, and lower values for regions where we are more certain.
Nevertheless, our guarantees hold for any choice of $\sigma$. 

Considering the generative nature of our base models, one natural way to produce such a $\sigma$ is to take the pixel-wise empirical variance of $M$ generated images:
\begin{equation*}\label{eq:sigma-naive}
    [\sigma^\mathrm{var}(X)]_p = \widehat{\Var}_M \bigl[[\mu(X)]_p\bigr].
\end{equation*}
However, this may suffer from being too local: for example, if the model correctly knows that an edge must be present in a particular region of an image but slightly misplaces it by one or two pixels, there would be a significant mismatch between the ``true'' model uncertainty and the indecision estimate by $\sigma^\mathrm{var}$.

To resolve this, we propose to `smooth out' our predictions by performing a convolution with a low pass kernel $K$.
A naive way of doing so would be to convolve the images whose pixels we are taking the variances of directly: $\widehat{\Var}_M \bigl[[\mu(X) \ast K]_p\bigr]$. However, this has an unintended side effect: by applying the convolution directly to the generated images, we are effectively \emph{undoing} the super-resolution done by the model! Hence, we propose to instead apply the convolution to the computation of the variance, via its decomposition in terms of the second moment:
\begin{equation*}
    \begin{multlined}
    [\sigma^{\text{ker-}K}(X)]_p = \widehat{\E}_M \bigl[[\mu(X)^2 \ast K]_p\bigr]\\ - \left( \widehat{\E}_M \bigl[[\mu(X) \ast K]_p\bigr] \right)^2.
    \end{multlined}
\end{equation*}
It should be noted that when $K$ is a 1-box kernel, we recover $\sigma^\mathrm{var}$.
After computing this patch-based variance, we further apply a Gaussian blur to the resulting variance map.
This additional smoothing step helps mitigate the risk of border artifacts being overly emphasized, which can otherwise lead to an undesired overestimation of uncertainty along edges.

Finally, we remark that ideally this model indecision would be estimated jointly with the upscaled image $\mu(X)$.
This is, however, fairly nontrivial and best left for future work.

\subsection{Choices of $D_p$}\label{sec:fidelity-risk}

A crucial point of our procedure is the definition of the precise fidelity error we are controlling. This is given by the choice of $D_p$, which is a function indexed by a pixel position $p$ that receives the real and predicted images $Y$ and $\widehat{Y}$ and returns a notion of how similar the two are around $p$. Our procedure is valid for \emph{any} choice of $D_p$ that is bounded within $[0, 3]$, though it is best to chose one for which $D_p(Y, \widehat{Y}) \to 0$ as $\widehat{Y} \to Y$. Here we highlight a couple of the most natural and useful:

\paragraph{Pointwise metric} $D_p(Y, \widehat{Y}) = \| [Y]_p - [\widehat{Y}]_p \|_1$, where $[Y]_p$ and $[\widehat{Y}]_p$ correspond to the pixel color of $Y$ and $\widehat{Y}$ at pixel position $p$ (and thus the 1-norm is necessary to condense their difference into a single number).

\paragraph{Neighborhood-averaged metric} $D_p(Y, \widehat{Y}) = \| [Y \ast K]_p - [\widehat{Y} \ast K]_p \|_1$, where $K$ is some convolution kernel corresponding to a low pass filter. This makes it so that single wrong pixels in the midst of many correct pixels do not influence the loss function too much, and generally leads to larger confidence masks.

In both cases, we consider the images in Lab color space. This ensures that all the color comparisons being done are perceptually uniform, which would not be the case in e.g., sRGB space.

Though perfectly valid and useful, it is well known that such pixel-wise comparisons struggle to capture semantic and perceptual properties of the underlying images.
Hence, both of the options presented above struggle to truly capture semantic differences, where a user would clearly note a difference between the predicted and ground truth images.
To this end, we propose a third option based on additional labelled data:

\paragraph{Semantic metric} We can suppose a stochastic function $S : [0,1]^{kw \times kh \times 3} \times [0,1]^{kw \times kh \times 3} \to \{0,1\}^{kw \times kh \times 3}$ that indicates a mask produced by a human-being denoting the differences between the two given high-resolution images (a value of 1 on the image represents a differing point). We can then consider $D_p (Y, \widehat{Y}) = [S(Y, \widehat{Y})]_p$.
Note that for the calibration procedure we only need to compute the $D_p$ on the calibration images, and thus the only samples of the human annotations $S(Y, \widehat{Y})$ that we need are on the calibration data.

\begin{figure}
    \includegraphics[width=.45\textwidth]{figures/fig1.png}
    \caption{\textbf{Evaluation of coverage and efficiency of our conformal calibration.} We investigate how the fidelity error evolves as a function of the fidelity level $\alpha$ for pointwise $D_p$ with a 32-pixel-wide Gaussian blur (the plot looks almost identical for different kernel sizes). Observe that these two quantities closely mirror each other, with the fidelity error being consistently under the fidelity region.
    }
    \label{fig:coverage}
\end{figure}

\section{Additional theorical results}

In this section we present additional theoretical properties enjoyed by our method, which highlight its flexibility and robustness.

\subsection{Our method provably controls PSNR} 

So far we have only proven results for the `fidelity error' defined in Subsection \ref{sec:mask-calib}.
However, our results can also be directly mapped to more familiar metrics for image quality quantification.
In particular, we can prove strong guarantees on the PSNR of our predictions, a common metric of image fidelity and quality in computer graphics:

\begin{proposition}\label{thm:psnr}
    Let $\alpha \in \R$ and $n \in \N$.
    Suppose we have $n+1$ i.i.d. samples $(X_i, Y_i)_{i=1}^{n+1}$ from an arbitrary probability distribution $P$ and let $t_\alpha$ and $M_\alpha$ be as in Equations~\ref{eq:threshold} and \ref{eq:mask-threshold} (and thus only a function of $X_i, Y_i$ with $i = 1, \ldots, n$). Then it holds that
    \[
    \begin{multlined}
    \E_{(X_i, Y_i)_{i=1}^{n+1}}\left[ \mathrm{PSNR}\left( \mu(X_{n+1}), Y_{n+1} | M_\alpha (X_{n+1}) \right) \right]\\
    \geq -20 \log \alpha. 
    \end{multlined}
    \]
\end{proposition}

It is rather remarkable that, despite our procedure being originally designed in order to establish guarantees for uncertainty quantification, it also maps over to guarantees on a standard image quality metric. The relative functional simplicity of the PSNR may be a contributing factor to this result, but we expect that similar finds will soon follow for other metrics (though the proof would be more involved; the full proof of Proposition \ref{thm:psnr} can be found in the Appendix).

\subsection{Under data leakage}

One crucial assumption of Theorem~\ref{thm:conformal-guarantee} is that we assume that the calibration data is independent of the base diffusion model -- i.e., that the calibration data is independent from (or at least exchangeable with) the data used to train the diffusion model.

Though arguably achievable through the collection of new data for calibration purposes, this is considerably harder to ensure when using foundation models which have been trained on massive datasets that attempt to span all data on the internet. Hence, it becomes essential to explore what happens when there is data leakage from the training data to the calibration data -- i.e., some amount of data in the calibration samples is actually already present in the training data.

Proposition~\ref{thm:data-leakage} provides worst-case bounds on the miscoverage error when there is data leakage. In particular, we consider that out of the $n$ calibration samples, $n_\mathrm{leaked} < n$ are actually drawn from the training data (or some other arbitrarily different data distribution), while the remaining $n_\mathrm{new} = n - n_\mathrm{leaked}$ are truly independent of the training samples.

\begin{proposition}\label{thm:data-leakage}
    Let $\alpha \in \R$ and $n \in \N$, with $n = n_\mathrm{new} + n_\mathrm{leaked}$.
    Suppose we fit our procedure as per Equations~\ref{eq:threshold} and \ref{eq:mask-threshold} with $n$ data points.
    Out of these $n$ data points, suppose that the first $n_\mathrm{new}$ are sampled from some arbitrary probability distribution $P$,
    and the latter samples (indexed by $n_\mathrm{new}+1, \ldots, n$) be sampled from some arbitrarily different probability distribution $Q$. 
    Then, as we take a new sample $X_{n+1}, Y_{n+1}$  from distribution $P$, it holds that
    \[ 
    \begin{multlined}
    \E_{(X_i, Y_i)_{i=1}^{n+1}}\left[ \sup_{p \in M_\alpha (X_{n+1})} D_p\left( \mu(X_{n+1}), Y_{n+1} \right) \right]\\
    \leq \alpha \cdot \frac{n_\mathrm{new} + n_\mathrm{leaked} + 1}{n_\mathrm{new} + 1}. 
    \end{multlined}
    \]
\end{proposition}

Note that $Q$ could even be the empirical distribution of the data used to train the base generative model. This result shows that our conformal prediction scheme (and split conformal prediction in general) is somewhat robust to data leakage, as long as it is not too severe in relation to the amount of calibration samples.

\section{Experiments}

We reserve this section to present a comprehensive evaluation of our method, demonstrating its effectiveness across various settings.

\begin{figure*}[t]
    \centering
    \includegraphics[width=.9\textwidth]{figures/fig2.png}
    \caption{\textbf{Examples of generated high-resolution images with corresponding conformal masks.} Note that though there is a large mistrust mask, it is \emph{correct} -- all the regions painted in red have, indeed, a significant mismatch between the prediction and the ground truth, be it in color, texture or lighting. Conformal calibration was done with fidelity level $\alpha = 0.187$, $D_p$ from semantic annotations and $\sigma$ with a 64-pixel-wide gaussian blur.}
    \label{fig:masks-semantic}
\end{figure*}
\subsection{Resources}

All experiments were conducted using the Liu4K dataset~\citep{liu4K}, which contains 1,600 high-resolution (4K) images in the training set and an additional 400 4K images in the validation set. The dataset features a diverse collection of real-world photographs, including scenic landscapes, architectural structures, food, and natural environments.

We evaluated our approach atop SinSR~\citep{sinsr}, a state-of-the-art generative super-resolution method based on diffusion models. It performs super-resolution by conditioning the score function on a low-resolution image and applying diffusion in the latent space.

Experiments were run on an AMD EPYC 7V13 processor (2.5GHz base, 3.7GHz boost, 48 threads available) with 216GB of RAM and an NVIDIA A100 80GB GPU. Notably, the primary computational bottleneck is the inference process of the base diffusion models, while the conformal calibration step is highly efficient and runs fairly quickly on a CPU.
For reproducibility, the source code is available in \verb|redactedURL|, as well as in the supplementary material.

\subsection{Results}

\begin{figure*}[t]
    \centering
    \includegraphics[width=.9\textwidth]{figures/fig3.png}
    \caption{\textbf{Non-semantic $D_p$ examples of high-resolution generated images with corresponding conformal masks.} 
    Similar to Figure~\ref{fig:masks-semantic}, but using a non-semantic $D_p$ (with 64-pixel-wide Gaussian blur from pointwise metric), which provides the intuition that the masks shrink for a fixed $\alpha$. Here, with $\alpha = 0.1$, we observe noticeably smaller masks.}

    \label{fig:non-semantic}
\end{figure*}

In our experiments, we mainly compare our method to a baseline scenario where no uncertainty is quantified.
Prior work on uncertainty quantification for image super-resolution mainly produce interval-based images, not allowing direct comparisons to our mask-based uncertainty quantification methodology. For this reason, we are limited to a qualitative evaluation between these methods. 

In Figure~\ref{fig:coverage}, we analyze how well our procedure truly controls the fidelity error in practice, noting that our theorems translate to excellent empirical performance. The actual fidelity error mirrors the specified fidelity level $\alpha$ very closely.

Figure~\ref{fig:masks-semantic} shows our methodology in action, illustrating how our mistrust masks capture all the mistakes made by the model, even if the price for this correctness is its size. Indeed, all the regions painted in red have a significant mismatch between the prediction and the ground truth, be it in color, texture or lighting.  

By comparing Figure~\ref{fig:masks-semantic} to Figure~\ref{fig:non-semantic}, we observe that using a blurred, non-semantic $D_p$ results in smaller mask sizes. In our experiments, we tested various levels of Gaussian blur and found that increasing the blur consistently led to smaller masks for a fixed level $\alpha$. Furthermore, non-semantic $D_p$ proves to be a compelling alternative to its semantic counterpart (which inherently carries more visual information), as it eliminates the need for generating additional data while still maintaining strong performance.


We also investigate the size of the predicted confidence masks, a crucial aspect in assessing the reliability of our method. Notably, the choice of $D_p$ plays a fundamental role in determining how much confidence we can assign to our predictions. If $D_p$ is too local (e.g., defined simply as $D_p(Y, \widehat{Y}) = \| [Y]_p - [\widehat{Y}]_p \|_1$), the model lacks sufficient spatial context to make robust assessments, resulting in overly conservative confidence estimates and smaller masks. 

By incorporating additional contextual information into $D_p$, such as smoothing the pixel-wise differences with a low-pass filter, we observe a substantial improvement in mask size and quality. This approach allows for larger, more informative confidence masks, as it accounts for a broader spatial region rather than relying solely on isolated pixel differences. As a result, the predictions not only become more faithful to the underlying image structure but also exhibit greater visual coherence. Empirically, this effect is evident in Figure~\ref{fig:sigma-and-dp}, where the use of a smoothed $D_p$ leads to masks that better capture the regions of uncertainty while preserving the overall perceptual integrity of the image. These findings reinforce the importance of designing $D_p$ to effectively balance local accuracy with global consistency, ultimately enhancing both interpretability and reliability in uncertainty estimation.
 
\begin{figure*}[t]
    \centering
    \includegraphics[width=.8\textwidth]{figures/fig5.png}
    \caption{\textbf{Zoomed-in view of a specific image patch.} 
    In this example, where $D_p$ is non-semantic (as in Figure~\ref{fig:non-semantic}), we observe that when the predicted image deviates from the ground truth, our method correctly identifies and assigns low confidence to the affected region. This case highlights a failure of the base model to accurately reconstruct a blurred area, demonstrating the effectiveness of our approach in detecting unreliable predictions.
    }

    \label{fig:zoom}
\end{figure*}

As observed in Figure~\ref{fig:zoom}, when the base model fails in the super-resolution task, our method successfully generates accurate confidence masks. This applies not only to perceptual attributes such as color and brightness but also to cases where the original image is blurred. In such scenarios, the base model often misinterprets the blur as part of the low-resolution input and attempts to remove it. Fortunately, our method effectively captures these nuances, preserving essential details and producing faithful, high-fidelity results.

\begin{figure}[h]
    \centering
    \includegraphics[width=.45\textwidth]{figures/fig6.png}
    \caption{\textbf{Evaluation and visualization of average mask sizes in our conformal calibration.}  
    We assess the proportion of predicted images that can be trusted on average for different kernel choices of $D_p$. Our results indicate that applying a larger Gaussian filter to $D_p$ leads to increased confidence mask sizes. Conformal calibration was performed using a $\sigma$ computed as the pointwise variance, followed by a 64-pixel-wide Gaussian blur. Shaded uncertainty bands represent 95\% bootstrapped confidence intervals.}

    \label{fig:mask-size}
\end{figure}

Finally, an important finding is illustrated in Figure~\ref{fig:mask-size}. Both visual inspection and quantitative metrics confirm that increasing the Gaussian blur on $D_p$ leads to a notable improvement in model performance. Specifically, as the blur level increases, the average mask size decreases for a fixed confidence level $\alpha$. This result highlights the role of non-semantic smoothing in refining uncertainty estimation, as it effectively reduces over-segmentation in the masks while preserving the ability to identify unreliable regions. By leveraging this insight, we demonstrate that careful tuning of the blur parameter in $D_p$ can significantly enhance the balance between mask compactness and reliability, further reinforcing the robustness of our approach.


\begin{figure}[!h]
    \centering
    \includegraphics[width=.45\textwidth]{figures/fig4.png}
    \caption{\textbf{Examples of inset regions highlighting confidence masks in areas with detailed differences.}  
    This figure presents zoomed-in regions (as in Figure~\ref{fig:zoom}), where the base model struggles with super-resolution, particularly in blurred areas. In these cases, our method successfully identifies and generates accurate confidence masks, effectively detecting regions of uncertainty.}

\end{figure}





\begin{figure*}[t]
    \centering
    \includegraphics[width=.8\textwidth]{figures/fig7.png}
    \caption{\textbf{Dependency of the conformal masks on $D_p$ and $\sigma$.} Red regions correspond to the conformal confidence masks. Note how, as the radius of the Gaussian blur for $D_p$ increases, so does the coverage of the trusted regions; and, as the radius of the Gaussian blur for $\sigma$ increases, not only does the trusted cover also increases, but its regions become more contiguous and visually appealing.}
    \label{fig:sigma-and-dp}
\end{figure*}



\section{Conclusion}

In this work, we have presented a new method for performing uncertainty quantification for image super-resolution based on generative foundation models endowed with statistical guarantees.
Our method requires only easily attainable unlabeled data and is adaptable over any base generative model, including those locked behind an opaque API.
We also prove that our proposed solution satisfies properties beyond that of conformal risk control, further strengthening it.
Finally, we note that the ideas presented here are also directly applicable to more general settings of using generative models for image reconstruction, such as image inpainting and colorization.


\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning and Computer Graphics. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.

\bibliography{bibliography}
\bibliographystyle{icml2025}

\appendix
\onecolumn
\section{Proofs}

\begin{proof}[Proof of Theorem~\ref{thm:conformal-guarantee}]
    This proof is done via the standard conformal risk control argument~\citep{conformal-risk-control}.

    Consider the ``lifted'' threshold
    \[ t^{(n+1)}_\alpha = \sup \left\{ t \in \R \cup \{+\infty\} : \frac{1}{n+1} \sum_{i=1}^{n+1} \sup_{p; [\sigma(X_i)]_p \leq t} D_p \left( \mu(X_i), Y_i \right) \leq \alpha \right\}, \]
    which, as opposed to their ``unlifted'' counterpart $t_\alpha$, leverage the $(n+1)$-th sample as well and does not include the $3/(n+1)$ term.

    Note that it is certainly the case that $t^{(n+1)}_\alpha \geq t_\alpha$.
    Moreover, note that the fidelity error is monotone with $t$, and so
    it suffices to show that the fidelity function with $t^{(n+1)}$ is upper bounded by $\alpha$.

    Let $Z_\ast$ be the multiset of the samples $(X_i, Y_i)_{i=1}^n$ -- i.e., a random variable representing the samples, but with their order discarded. Hence, upon conditioning on $Z_\ast$, all the randomness that remains is that of the order of the samples. It then follows:
    \[ \E\left[\sup_{p; [\sigma(X_{n+1})]_p \leq t^{(n+1)}_\alpha} D_p(\mu(X_{n+1}), Y_{n+1}) \middle| Z_\ast\right] = \frac{1}{n+1} \sum_{i=1}^{n+1} \sup_{p; [\sigma(X_{n+1})]_p \leq t^{(n+1)}_\alpha} D_p(\mu(X_i), Y_i), \]
    and by the definition of $t^{(n+1)}_\alpha$, this is upper bounded by $\alpha$. Thus
    \begin{align*}
        & \E\left[\sup_{p; [\sigma(X_{n+1})]_p \leq t^{(n+1)}_\alpha} D_p(\mu(X_{n+1}), Y_{n+1})\right] = \E_{Z_\ast}\left[\E\left[\sup_{p; [\sigma(X_{n+1})]_p \leq t^{(n+1)}_\alpha} D_p(\mu(X_{n+1}), Y_{n+1}) | Z_\ast\right]\right]
        \\ &\qquad \leq \E_{Z_\ast}[\alpha] = \alpha,
    \end{align*}
    which concludes the proof.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{thm:psnr}]
    The PSNR we are bounding is given by
    \begin{align*}
        &\E[\mathrm{PSNR}\left( \mu(X_{n+1}), Y_{n+1} | M_\alpha (X_{n+1}) \right)]
        := \E\left[10 \log_{10} \frac{(\max_{p \in M_\alpha(X_{n+1})} [Y_{n+1}]_p)^2}{\lvert M_\alpha(X_{n+1}) \rvert^{-1} \sum_{p \in M_\alpha(X_{n+1})} \left( [\mu(X_{n+1})]_p - Y_p \right)^2 } \right]
        \\ &\quad = 20 \E\left[\log_{10} \frac{\max_{p \in M_\alpha(X_{n+1})} [Y_{n+1}]_p}{\sqrt{\lvert M_\alpha(X_{n+1}) \rvert^{-1} \sum_{p \in M_\alpha(X_{n+1})} \left( [\mu(X_{n+1})]_p - Y_p \right)^2} } \right].
    \end{align*}
    Now, by Jensen's Inequality and standard properties of logarithms,
    \begin{align*}
        &20 \E\left[\log_{10} \frac{\max_{p \in M_\alpha(X_{n+1})} [Y_{n+1}]_p}{\sqrt{\lvert M_\alpha(X_{n+1}) \rvert^{-1} \sum_{p \in M_\alpha(X_{n+1})} \left( [\mu(X_{n+1})]_p - Y_p \right)^2} } \right]
        \\ &= 20 \left( \E\left[\log_{10} \max_{p \in M_\alpha(X_{n+1})} [Y_{n+1}]_p - \log_{10} \sqrt{\lvert M_\alpha(X_{n+1}) \rvert^{-1} \sum_{p \in M_\alpha(X_{n+1})} \left( [\mu(X_{n+1})]_p - Y_p \right)^2} \right] \right)
        \\ &= 20 \left( \E\left[ \log_{10} \max_{p \in M_\alpha(X_{n+1})} [Y_{n+1}]_p \right] - \E\left[ \log_{10} \sqrt{\lvert M_\alpha(X_{n+1}) \rvert^{-1} \sum_{p \in M_\alpha(X_{n+1})} \left( [\mu(X_{n+1})]_p - Y_p \right)^2} \right] \right)
        \\ &\geq 20 \left( \E\left[ \log_{10} \max_{p \in M_\alpha(X_{n+1})} [Y_{n+1}]_p \right] - \log_{10} \E\left[ \sqrt{\lvert M_\alpha(X_{n+1}) \rvert^{-1} \sum_{p \in M_\alpha(X_{n+1})} \left( [\mu(X_{n+1})]_p - Y_p \right)^2} \right] \right);
    \end{align*}
    And, because the RMSE is upper bounded by the maximum error, we get that
    \begin{align*}
        &20 \left( \E\left[ \log_{10} \max_{p \in M_\alpha(X_{n+1})} [Y_{n+1}]_p \right] - \log_{10} \E\left[ \sqrt{\lvert M_\alpha(X_{n+1}) \rvert^{-1} \sum_{p \in M_\alpha(X_{n+1})} \left( [\mu(X_{n+1})]_p - Y_p \right)^2} \right] \right)
        \\ &\geq 20 \left( \E\left[ \log_{10} \max_{p \in M_\alpha(X_{n+1})} [Y_{n+1}]_p \right] - \log_{10} \E\left[ \sup_{p \in M_\alpha(X_{n+1})} \left( [\mu(X_{n+1})]_p - Y_p \right) \right] \right);
    \end{align*}
    And, by Theorem~\ref{thm:conformal-guarantee},
    \begin{align*}
        &20 \left( \E\left[ \log_{10} \max_{p \in M_\alpha(X_{n+1})} [Y_{n+1}]_p \right] - \log_{10} \E\left[ \sup_{p \in M_\alpha(X_{n+1})} \left( [\mu(X_{n+1})]_p - Y_p \right) \right] \right)
        \\ &\geq 20 \left( \E\left[ \log_{10} \max_{p \in M_\alpha(X_{n+1})} [Y_{n+1}]_p \right] - \log_{10} \alpha \right)
        \geq -20 \log_{10} \alpha,
    \end{align*}
    where the last step holds as long as all pixel values are in $[0, 1]$.

\end{proof}

\begin{proof}[Proof of \ref{thm:data-leakage}]
    We effectively want to bound the supremum of the expected fidelity error as the leaked data is allowed to alter freely.
    For convenience, let $\sup_\mathrm{leaked}$ denote the supremum over all possible values of the leaked samples $(X_i, Y_i)_{i=n_\mathrm{new}+1}^n$ (and $\inf_\mathrm{leaked}$ the corresponding infimum).

    Note that the error function is decreasing on the selected parameter $t$ and continuous.
    Hence:
    \[ \sup_\mathrm{leaked} \E\left[\sup_{p \in M_\alpha(X)} D_p(Y, \widehat{Y})\right] \leq \E\left[\sup_\mathrm{leaked} \sup_{p \in M_\alpha(X)} D_p(Y, \widehat{Y})\right] = \E\left[\sup_{p; [\sigma(X)]_p \leq \sup_\mathrm{leaked} t_\alpha} D_p(Y, \widehat{Y})\right], \]
    and in turn
    \begin{align*}
        \sup_\mathrm{leaked} t_\alpha
        &= \sup_\mathrm{leaked} \sup \left\{ t \in \R \cup \{+\infty\} : \frac{1}{n+1} \sum_{i=1}^n \sup_{p; [\sigma(X_i)]_p \leq t} D_p(Y_i, \mu(X_i)) + \frac{3}{n+1} \leq \alpha \right\}
        \\ &\leq \sup \left\{ t \in \R \cup \{+\infty\} : \inf_\mathrm{leaked} \frac{1}{n+1} \sum_{i=1}^n \sup_{p; [\sigma(X_i)]_p \leq t} D_p(Y_i, \mu(X_i)) + \frac{3}{n+1} \leq \alpha \right\}
        \\ &= \sup \Biggl\{ t \in \R \cup \{+\infty\} : \frac{1}{n+1} \sum_{i=1}^{n_\mathrm{new}} \sup_{p; [\sigma(X_i)]_p \leq t} D_p(Y_i, \mu(X_i)) \\ &\qquad\quad\ + \inf_\mathrm{leaked} \frac{1}{n+1} \sum_{i=n_\mathrm{new}+1}^n \sup_{p; [\sigma(X_i)]_p \leq t} D_p(Y_i, \mu(X_i)) + \frac{3}{n+1} \leq \alpha \Biggr\}
        \\ &= \sup \left\{ t \in \R \cup \{+\infty\} : \frac{1}{n+1} \sum_{i=1}^{n_\mathrm{new}} \sup_{p; [\sigma(X_i)]_p \leq t} D_p(Y_i, \mu(X_i)) + \frac{3}{n+1} \leq \alpha \right\}
        \\ &= \sup \left\{ t \in \R \cup \{+\infty\} : \frac{1}{n_\mathrm{new}+1} \sum_{i=1}^{n_\mathrm{new}} \sup_{p; [\sigma(X_i)]_p \leq t} D_p(Y_i, \mu(X_i)) + \frac{3}{n_\mathrm{new}+1} \leq \alpha \cdot \frac{n + 1}{n_\mathrm{new} + 1} \right\}.
    \end{align*}
    Note that this corresponds to doing our calibration procedure only on the new data but with altered fidelity level $\alpha \cdot (n+1)/(n_\mathrm{new} + 1) = \alpha \cdot (n_\mathrm{new} + n_\mathrm{leaked}+1)/(n_\mathrm{new} + 1)$, and so, by the same arguments as in Theorem~\ref{thm:conformal-guarantee},
    \[ \E_{(X_i, Y_i)_{i=1}^{n+1}}\left[ \sup_{p \in M_\alpha (X_{n+1})} D_p\left( \mu(X_{n+1}), Y_{n+1} \right) \right] \leq \alpha \cdot \frac{n_\mathrm{new} + n_\mathrm{leaked} + 1}{n_\mathrm{new} + 1}. \]
\label{proof:data-leakage}
\end{proof}

\end{document}