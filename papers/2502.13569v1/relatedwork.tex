\section{Related Work}
\subsection{Evolutionary Reinforcement Learning}
Evolutionary Algorithms (EA) \cite{Bäck_Schwefel_1993} provide a gradient-free optimization approach.
Evolutionary Reinforcement Learning (ERL) \cite{Pourchot_Sigaud_2018,Khadka_Majumdar_Nassar_Dwiel_Tumer_Miret_Liu_Tumer_2019,chen2019restart,Bodnar_Day_Lió_2020,Bodnar_Day_Lió_2020} combines RL agents with a genetic population, improving the exploration and robustness of the policy. 
For instance, data collected by the EA population is used to train the RL agent~\cite{Khadka_Tumer_2018}, and the RL agent is periodically integrated into the EA population to improve performance. 
A method for shared nonlinear state representations \cite{Li_Tang_Hao_Zheng_Fu_Meng_2022} was introduced, where evolutionary algorithms operate only on the linear policy representations.
However, these methods primarily apply evolution to the parameter networks rather than the model structure itself. 
NEAT \cite{Stanley_Miikkulainen_2002} applies evolutionary algorithms to the topology of neural networks, optimizing the network structure to improve agent performance.  
Despite its potential, these methods often lead to excessively large model structures during training, making it challenging to handle complex tasks. 

HyperNEAT \cite{Stanley_Clune_Lehman_Miikkulainen_2018} further extends neuroevolution to the neural networks building blocks, hyperparameters, architectures, and even algorithms themselves.
PathNet \cite{Fernando_Banarse_Blundell_Zwols_Ha_Rusu_Pritzel_Wierstra_2017}, applied to continual learning problems, uses the genetic algorithm to search for optimal module paths in a model with multiple layers and modules. Once a task is trained, the optimal path and related modules for that task are fixed, and PathNet continues training for the next task.
However, it requires a large number of modules and incurs high training costs. 
Our method allows for the simultaneous training of multiple tasks, improving learning efficiency and reducing the need for excessive modules by sharing parameters.

\subsection{Multi-Task Reinforcement Learning}
Multi-task learning \cite{Caruana_Pratt_Thrun_2017,yang2017multi} aims to improve the efficiency of learning multiple tasks simultaneously by leveraging the similarities among them. 
However, when sharing parameters across tasks, gradients from different tasks may conflict. 
Policy distillation \cite{Rusu_Colmenarejo_Gulcehre_Desjardins_Kirkpatrick_Pascanu_Mnih_Kavukcuoglu_Hadsell_2015,Teh_Bapst_Czarnecki_Quan_Kirkpatrick_Hadsell_Heess_Pascanu_2017,Parisotto_Ba_Salakhutdinov_2015} mitigates the above issue, but it requires introducing additional networks and policies. 
Some methods~\cite{calandriello2014sparse,xu2020knowledge,liu2021conflict} formulate it as a multi-objective optimization problem, but suffer high computational and memory costs.
The Mixture of Experts (MOE) model~\cite{Eigen_Ranzato_Sutskever_2013} constructs multiple networks as experts to improve generalization and reduce overfitting.
The Mixture of Orthogonal Experts (MOORE)
\cite{Hendawy_Peters_D’Eramo_2023} uses a Gram-Schmidt process to shape the shared subspace of representations generated by expert mixtures, encapsulating common structures between tasks to promote diversity.
The Contrastive Module with Temporal Attention (CMTA) \cite{Lan_Zhang_Yi_Guo_Peng_Gao_Wu_Chen_Du_Hu_et} method constrains modules to be distinct from each other and combines shared modules with temporal attention, improving the generalization and performance across multiple tasks.

To further improve the efficiency of parameter sharing, a layer-level model \cite{Yang_Xu_Wu_Wang_2020} has been proposed. It uses a routing network to generate module weights for each task, allowing a set of modules to be reconstructed into task-specific models.
To address the issue of resource allocation for tasks of varying difficulty, Dynamic Depth Routing (D2R) \cite{He_Li_Zang_Fu_Fu_Xing_Cheng_2023} combined a routing network with the module-level model to dynamically allocate modules for different tasks. 
In contrast, our approach explicitly integrates task difficulty with the model structure, enabling the model to evolve adaptively throughout the training process.