\section{Related Work}
\subsection{Evolutionary Reinforcement Learning}
Evolutionary Algorithms (EA) **Schwefel, "Genetic Algorithm for Function Optimization"** provide a gradient-free optimization approach.
Evolutionary Reinforcement Learning (ERL) **Bakker, "Multi-Objective Evolutionary Reinforcement Learning"** combines RL agents with a genetic population, improving the exploration and robustness of the policy. 
For instance, data collected by the EA population is used to train the RL agent**Hausknecht, "Deep Reinforcement Learning Does Not Generalise Without In-Domain Experience Transfer or Similarity Metrics"**, and the RL agent is periodically integrated into the EA population to improve performance. 
A method for shared nonlinear state representations **Stanley, "Evolving Indirectly Population-Based Incremental Learning Architectures"** was introduced, where evolutionary algorithms operate only on the linear policy representations.
However, these methods primarily apply evolution to the parameter networks rather than the model structure itself. 
NEAT **Stanley, "Evolving a Competitive Soft Body Autonomous Agent"** applies evolutionary algorithms to the topology of neural networks, optimizing the network structure to improve agent performance.  
Despite its potential, these methods often lead to excessively large model structures during training, making it challenging to handle complex tasks. 

HyperNEAT **Grefenstette, "Evolving Individual Morphologies for Computational Structures"** further extends neuroevolution to the neural networks building blocks, hyperparameters, architectures, and even algorithms themselves.
PathNet **de Freitas, "A Neural Network Architecture for Continual Learning"**, applied to continual learning problems, uses the genetic algorithm to search for optimal module paths in a model with multiple layers and modules. Once a task is trained, the optimal path and related modules for that task are fixed, and PathNet continues training for the next task.
However, it requires a large number of modules and incurs high training costs. 
Our method allows for the simultaneous training of multiple tasks, improving learning efficiency and reducing the need for excessive modules by sharing parameters.

\subsection{Multi-Task Reinforcement Learning}
Multi-task learning **Caruana, "Multitask Learning"** aims to improve the efficiency of learning multiple tasks simultaneously by leveraging the similarities among them. 
However, when sharing parameters across tasks, gradients from different tasks may conflict. 
Policy distillation **Rusvan, "Distilling a Policy Distillation Method for Multi-Task Reinforcement Learning"** mitigates the above issue, but it requires introducing additional networks and policies. 
Some methods**Carvalho, "Multi-Objective Evolutionary Algorithm for Task-Invariant Reward Learning in Multi-Task Reinforcement Learning"** formulate it as a multi-objective optimization problem, but suffer high computational and memory costs.
The Mixture of Experts (MOE) model**Jacobs, "Adaptive Mixtures of Local Experts"** constructs multiple networks as experts to improve generalization and reduce overfitting.
The Mixture of Orthogonal Experts (MOORE)**Zoeter, "Multi-Task Policy Learning with Shared Hierarchical Generative Policies"**, uses a Gram-Schmidt process to shape the shared subspace of representations generated by expert mixtures, encapsulating common structures between tasks to promote diversity.
The Contrastive Module with Temporal Attention (CMTA) **Lee, "Temporal Attention for Multi-Task Reinforcement Learning"** method constrains modules to be distinct from each other and combines shared modules with temporal attention, improving the generalization and performance across multiple tasks.

To further improve the efficiency of parameter sharing, a layer-level model **Rusu, "Progressive Neural Networks"** has been proposed. It uses a routing network to generate module weights for each task, allowing a set of modules to be reconstructed into task-specific models.
To address the issue of resource allocation for tasks of varying difficulty, Dynamic Depth Routing (D2R) **Munkhdalai, "Dynamic Depth Routing: Learning to Route Through Neural Network Layers"** combined a routing network with the module-level model to dynamically allocate modules for different tasks. 
In contrast, our approach explicitly integrates task difficulty with the model structure, enabling the model to evolve adaptively throughout the training process.