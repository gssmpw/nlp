\section{Related Work}
\label{sec:related_Work}
\begin{figure}
    \centering  \includegraphics[width=1\columnwidth]{imgs/example2.png}
    \caption{
    Illustration of FAXs (see Example~\ref{ex:illu}). }
    \label{fig:example}
    \vspace{-10pt}
\end{figure}

\paragraph{XAI methods} 
There has been a recent surge in methods advocating for a shift in how we perceive explanations, emphasizing the importance of viewing them as dialogues rather than solely relying on heatmaps or feature attributions, as %often seen 
standard in much of the XAI literature~\cite{miller2019explanation,madumal2019grounded,lakkaraju2022rethinking}. 
%Additionally, explanations in the form of dialogues enable us to address various model ambiguities, such as model certainties and biases.
%From a practical standpoint, 
Within the multi-agent setting,
\cite{Rago_23} proposed an argumentative framework for expressing (interactive) explanations in the form of dialogues. 
Additionally, \cite{debate} demonstrated a debate framework, which was further expanded in \cite{kori2022explaining} to scale, allowing for the extraction of post-hoc explanations in the form of dialogues between two fictional %players
agents.
Inspired by \cite{Rago_23}, we define a multi-agent argumentative framework for explanation, 
and we adopt a variant of approach in \cite{kori2022explaining} to implement the framework.
%
Both \cite{kori2022explaining} and our implementation utilize a surrogate model that faithfully represents the given classifier. 
The use of surrogate models is a standard practice in XAI, as seen e.g. in \cite{yoon2018invase,goyal2019counterfactual,glance}.

 
%Similar to the approach proposed by \cite{kori2022explaining}, our method can be considered to fall within the general family of feature attribution explanations. 
The agents in our explanations put forward arguments which can be understood as feature attributions such as 
LIME~\cite{lime}.
%In which, \cite{lime} can be viewed as a special instance of our framework. 
However, unlike \cite{lime}, we do not randomly select input regions to mask; instead, %we 
our agents learn two different strategies to select regions to argue for and against a particular explanandum (input-output).

Our method is also related to
%Some interesting work by
\cite{shitole2021one}, which argues against the rigidity of static and shallow explanations and introduces a new method for compactly visualizing how different combinations of regions in images impact the confidence of a classifier. 
%Another notable work, 
Also, \cite{wang2019deliberative} aims to encourage the capturing of uncertain image regions, 
%However, 
while \cite{wang2019deliberative} focus on statically capturing ambiguities in an image with respect to the given classifier, we generate both certain and uncertain regions through %player
agent interactions in an iterative fashion \cite{Thauvin_24}.


\paragraph{Argumentation methods}
%Another avenue of 
Some research in XAI explores the use of computational argumentation \cite{vcyras2021argumentative}. 
%Computational argumentation 
This typically aims to assess specific claims by considering arguments that %both
support and/or challenge the claim, as well as their relations within %specific 
argumentative frameworks (AFs). 
These AFs 
%are often modeled based on principles advocated in works such as 
may be as in \cite{dung1995acceptability} or %in Bipolar Argumentation Frameworks 
Bipolar AFs (BAFs).
%At a broad level, 
Broadly, with our XAI 
approach 
\iffalse leads to debates that can be viewed as %simplified instances of AFs
simple BAFs; however, conducting a comprehensive analysis of these AFs' properties is beyond the scope of this paper. 
Instead, 
\fi 
we delve into a relatively unexplored area: explaining image classifiers through debates amounting to %(simple)
BAFs, which involve interactive gameplay among learning %players
agents. 
%
Other approaches employing AFs for explainable image classification either utilize intrinsically argumentative models, e.g. as in \cite{hamed}, or mirror the mechanics of the model itself, e.g. as seen in  \cite{Purin21}. 
In contrast, our approach focuses on explaining classifiers using latent features through (free) argumentative exchanges.
%\todo[inline]{add discussion of \cite{DS-visual debates}?} NO AS NOT ARG PROPER....

%\href{https://doi.org/10.1145/3539618.3591917}{related but probably not useful} 
%are we performing a form of \href{https://proceedings.mlr.press/v202/aher23a/aher23a.pdf}{Turing Experiment} for RL as they introduce?
%
%\href{https://www.ijcai.org/proceedings/2023/48}{an option} for providing depth with training examples



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%