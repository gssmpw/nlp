\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{imgs/overview.png}
    \caption{Overview of our implementation (argument dependencies are temporal).}
    \label{fig:overview}
\end{figure*}

\section{%Experimental Set-up
Implementation}
\label{sec:set-up}

In this section, we detail our methodology (overviewed in Figure~\ref{fig:overview}) for obtaining and  evaluating empirically \FAXIC s %deployed for 
with data for image classification.
Specifically, we detail 
how class-specific (private) features  can be obtained (Section~\ref{sec:feat}); and how class-specific agents (including their private classifiers and their policies for contributing to \FAXIC s) are learnt and deployed (Section~\ref{sec:agent}).
%Figure~\ref{fig:overview} gives an overview of our implementation.
Note that, while \FAXIC s are amongst two agents only (for the top- and second-best predicted classes by the classifier), given that different inputs will result in different predictions we need to develop, at training time, all agents, with their private features and classifiers. 
%\FT{The code is publicly available at .}\todo{add url}

\subsection{%Feature discretization
Class-specific discrete features}
\label{sec:feat}
%The main objective of discretization is to learn class-specific discrete features, which also serve as a set of private features. For this, we initialise $N_c$ codebooks, where any individual codebook $\mathfrak{C}_c \in \mathbb{R}^{\tilde{n} \times d}$ consists of a randomly initialised set of discrete features, where, for simplicity, we use the fixed number of discrete features $\tilde{n}$ for every codebook with each feature of dimension $d$.


We obtain %class-specific features 
these by 
simultaneously training, similarly %as in \cite{van2017neural}, \cite{jang2016categorical}, 
to \cite{kori2022explaining}: 
\begin{itemize}
    \item 
$N$ \emph{codebooks} 
$\mathfrak{C}_1, \ldots, \mathfrak{C}_N$ (one per class in $\mathcal{Y}$); for each $i \in \{1, \ldots, N\}$,
$\mathfrak{C}_i \in \mathbb{R}^{\tilde{n} \times d}$   corresponds to
$\mathcal{Z}^i$ (see Section~\ref{sec:prelim});\footnote{We assume  for simplicity that $M^i=\tilde{n}$  for all $i\in \{1, \ldots, N\}$, namely all agents have the same number of class-specific features.} for $z=f(x)$, if $\bar{y}$ is the  top-class predicted
by $g(z)$, we use $\mathfrak{C}_{\bar{y}}(z)$ to stand for the specific discrete features in 
$\mathfrak{C}_{\bar{y}}$ corresponding to $z$  (we use %simply 
$\tilde{z}$ to stand for $\mathfrak{C}_{\bar{y}}(z)$ if clear% from the context
);
\item 
a \emph{quantized classifier} $q:\mathfrak{C} \rightarrow \mathcal{Y}$,
for $\mathfrak{C}=\mathfrak{C}_1\cup \ldots \cup \mathfrak{C}_N$, distilling the knowledge of the feature classifier $g$ so that
%, if $z=f(x)$, $y$ is the top-class predicted by $g(z)$,  and $\mathfrak{C}_{y}$ corresponds to $z$, then 
$q(\tilde{z}% = \mathfrak{C}_{y}(z)
)$ approximates $g(z)$.
%\todo{is $q(\mathfrak{C}_{y})$ = $q^y$? NO.}
\end{itemize}
 %
 % \todo{NEEDS DECIDING we could cut the below short/move it to supp material, by saying: we learn codebooks and q by adapting the techniques from...}
Intuitively, 
%following \cite{van2017neural},
%in discrete representation learning, 
a codebook is a collection of averaged patterns or ``concept representations'' that summarize the key features % in a dataset
of data \cite{van2017neural}. 
%It is built dynamically, meaning it gets updated as each new batch of images is processed while training. For every image in a batch, its features contribute to the overall representation stored in the codebook, making it a running summary that adapts as more data is seen. 


\paragraph{Training.} To %obtain 
get the codebooks, %we start with a random initialisation for each. Then, 
%inspired by 
we draw inspiration from \cite{van2017neural}. Intuitively, for each $(x,y) \!\in\! \mathcal{D}$, with $z\!=\!f(x)$, for $\bar{y} \!=\! \texttt{argmax}(g(z))$ (i.e. $\bar{y}$ is the top-class predicted by the feature classifier $g$ for $z$), we aim at
deterministically mapping the elements of $z$ to the nearest elements of $\mathfrak{C}_{\bar{y}}$ using some convex distance function $\delta$, as follows:

% As described in Equation \ref{eqn:codebooks}, inspired by \cite{van2017neural}, quantization is achieved by deterministically mapping the elements of the continuous feature $z=f(x) \in \mathcal{Z}$ to the nearest element of $\mathfrak{C}_c$ using some convex distance function $\hat{d}$, where $c$ is decided by the feature classifier as $c = \texttt{argmax}(g(z))$.

\vspace*{-0.2cm}
\begin{equation}
    \tilde{z} = \{\argmin_{\tilde{k} \in \mathfrak{C}_{\bar{y}}} \delta (z_k, \tilde{k})| z_k \in z \} 
    \label{eqn:codebooks}
\end{equation}

To learn  %($\tilde{n}$) codebook features in each codebook 
this in an end-to-end fashion, we use the Gumble sampling procedure from \cite{jang2016categorical},  resulting in $\tilde{z}$ as a projection of the continuous features in $z$ onto every element of codebook 
$\mathfrak{C}_{\bar{y}}$, corresponding to pairwise similarity scores between $z$ and all $\tilde{n}$ codebook features $\mathfrak{C}_{\bar{y}}$.
The resulting quantization objective for training is described as follows (see \cite{jang2016categorical} for details):
\begin{equation}
    \mathcal{L}_{\textrm{quant}} = \sum \texttt{softmax}(\tilde{z}) \left(\texttt{logsoftmax}(\tilde{z})\right)
    \label{eqn:quantizationloss}
\end{equation}

% ...
% \todo{$X_1$ and $X_2$,
% $argmin_{x_1 \in X_1} d(x_1,X_2)$}

% where we project $z$ with an MLP mapping corresponding to pairwise similarity score between feature $z$ and $\tilde{n}$ codebook features. The projected 
% vector $\tilde{z}$ is used in the gumble-softmax trick resulting in the continuous approximation of a one-hot representation given by $\texttt{softmax}(\exp((s_k + \tilde{z}_k)/T))$, where $s$ is sampled from a Gumble distribution and $T$ is a temperature parameter.
% This encourages learning distinct discrete representations in a codebook, to enhance the usage of codebook features, we initialize all codebooks with a uniform discrete prior, where all $\tilde{n}$ features in $\mathfrak{C}_c$ for all $c \in \{0, \dots, N_c\}$ are uniformly distributed in the range $(-1/\tilde{n }, 1/\tilde{n})$. 
% This initialization bounds all the vectors with respect to a total number of discrete features and spreads them maximally apart within the given range.
% We additional include Exponential Moving Average (EMA) based codebook training as proposed in \cite{bardes2021vicreg},\cite{van2017neural}, to prevent collapsing.
% Finally, this results in the quantization objective as described in equation \ref{eqn:quantizationloss}. 
% It is important to note that, the form of quantization loss differs based on the choice of sampling, as in case of Euclidean sampling and Cosine sampling, which results in loss as illustrated in \cite{van2017neural} and  \cite{heirarchy}, respectively.
% \todo{end of what needs deciding}

% To train in an end-to-end fashion, we introduce quantized classifier $q:\mathfrak{C} \rightarrow \mathcal{Y}$, as want the codebook features to faithfully represent the continuous classifier $g$, we train the quantised classifier with the outputs of continuous classifier with cross-entropy loss. This process of learning meaning-quantized features with respect to the original continuous classifier is called distillation, which is trained with a combined objective as described in equation \ref{eqn:distillation}


To %make sure that codebooks and quantized classifier are learnt to faithfully represent the (continuous) feature classifier $g$, 
faithfully learn the quantized classifier $q$, we adopt the following \emph{distillation} loss %function  
during training, where 
${\bar{y}}$ is the predicted class (by $g$) for input %image
$x$ and 
$\mathrm{CE}$ correspond to cross-entropy loss: 
% \todo{check/change symbols below, what is $\tilde{z}$?$q(\mathfrak{C}_{\tilde{y}}(f(x))$ should just be $q(\mathfrak{C}_{\tilde{y}})$? $\tilde{y}$ should be the ground truth?} 
% \todo{what is CE? Cross-Entropy? add a reference?}


\begin{equation}
    \mathcal{L}_{\textrm{dist}} = \mathcal{L}_{\textrm{quant}} + \mathrm{CE}\left(q(%\mathfrak{C}_{\tilde{y}} \left%(f(x)
    \tilde{z})), {\bar{y}} \right)
    \label{eqn:distillation}
\end{equation}

By using 
this loss% function
,  we strive towards a faithful $q$ to the %original 
classifier.

\subsection{%Agent modelling
Class-specific agents} %\FT{classifiers and  agents behaviour}}
\label{sec:agent}

%Here, we initialise 
We obtain $N$ agents, each of which is responsible for arguing wrt a particular class in $\mathcal{Y}$. 
For any given input $x$, an instance of \FAXIC\ is obtained between the two agents whose class is the top-class predicted by  $g(f(x))$  (for the proponent) and the second-best class  (for the opponent).
Specifically, if $g(f(x))=[\bar{y}^1, \dots, \bar{y}^{N}]$,
then the proponent is $\Agents^{\bar{y}^1}$ and
the opponent is $\Agents^{\bar{y}^2}$, as depicted in the assignment block in Figure \ref{fig:overview} (the assignment block selects the agents based on the estimated top two classes).



We model each agent $\Agents^{i}$ (arguing for class $\bar{y}^i$) as a \emph{sequence %?sequential?
model} $\zeta^{i}$.
In line with \cite{kori2022explaining}, we use gated recurrent units (GRUs) for realizing these sequence models. %, where each agent takes turns to make an argument and agents have flexibility to consider other's arguments in making its choices in its next turn
%
The sequence model $\zeta^{i}$
operates on a hidden state vector ($h^i_{t-1}$)% $h^i$% in $\zeta^i$ 
, which can be treated as a proxy interpretation for %encoded 
information %setting the context of succesful exchanges. WHAT DOES SUCCESSFUL MEAN?
drawn from the exchange BAF in the \FAXIC\ till the current timestep ($t$).
The sequence model uses a private modulator network $\mathcal{M}^i$
which encodes arguments (in $\ArgsI_{t}$ in the agent's private BAF $\BAFi_t$) to update the hidden state representation:
\begin{align}
    %\ArgsI_{t} \sim \Pi^i(h^i_{t-1} \mid \bigcup_i^2 \ArgsI_{t-1}, \tilde{y}^i)    \\
    h^i_{t} = \zeta^i(h^i_{t-1}, \mathcal{M}^i(\ArgsI_{t})) 
    \label{eqn:sequence_steps}
\end{align}
The output (hidden state vector) of $\zeta^i$  is then used to determine 
 a \emph{policy function} $\Pi^{i}$, for determining how agents contribute attacks and supports (in the agent's private BAF $\BAFi_t$) to \FAXIC s% given the set of all arguments contributed by either agent so far
, by determining which argument these attacks (if the agent is the opponent $\Agents^j$) or supports (if the agent is the proponent $\Agents^i$) are drawn from:
\begin{align}
    \ArgsI_{t} \sim \Pi^i(h^i_{t-1} \mid %\bigcup_i^2 
    \ArgsI_{t-1} \cup \Args^{j}_{t-1}, \bar{y}^i)  
 \end{align}
 Note that these arguments are from $\mathfrak{C}_{\bar{y}^i}$, corresponding to private features in $\mathcal{Z}^{\bar{y}^i}$.
 % using policy network $\Pi^i: \mathcal{H} \rightarrow \mathfrak{C}_{\tilde{y}^i}$, where $h^i_t \in \mathcal{H}^i$.
%
The output (hidden state vector) of $\zeta^i$  is also used to obtain the \emph{private classifier} $q^{i}$, as a multi-layer perceptron with hidden state vectors as inputs,
associating them to class confidence in turn used to
assign values to sets of private features as in Section~\ref{sec:EX-IC}.


%We also use additional agent-specific classifier $q^i:\mathcal{H} \rightarrow [-1, 1]^{|\mathcal{Y}|}$, on which we apply softmax operation, mapping them to class labels.  NOT INFORMATIVE ENOUGH THIS WAY...






\paragraph{Training.} 
\label{sec:FAX-impl}
To obtain each agent's sequence model in an end-to-end fashion,
we adapt the REINFORCE learning algorithm from \cite{mnih2014recurrent}.
Specifically, we see 
the next argument prediction/selection as a reinforcement learning task, 
%which helps in training $\zeta^i$ in an end-to-end fashion with delayed rewards $r^i$ (see Definition~\ref{def:reward}).
with agents' rewards %defined 
as follows.

\begin{definition}
\label{def:reward}
Let  $\langle \BAFx_0, \ldots, \BAFx_{n}, \Agents_0, \ldots,\Agents_n, \SpeakerM 
    \rangle$ be a \FAXIC\ for explanandum $(x,y)$ amongst agents $\Agents \!=\! \{ \Agents^1, \Agents^2 \}$. 
    Then, for $i\!\in\! \{ 1, 2 \}$ and $t \!\in\! \{1, \ldots, n\}$, %agent 
    $\AgentI$'s \emph{reward} at timestep $t$ is $\rewardI_t = \StanceI(\BAFi_0, \arge)\SFi(%\BAFx_t
    \BAFi_t, \arge)$% \todo{what does this mean? stance is either $+$ or $-$ (symbols, not positive or negative numbers), yes, reward is basically the strength depending upon the stance it's either negative or positive}
    .\footnote{Here, we treat the agent's initial stance as a (positive/negative) sign  for the explanandum's current strength in the %exchange BAF 
    %\todoE{the implementaiton uses exchange, not private BAF? TO BE DISCUSSED AFTER THE DEADLINE}
     private BAF.}
    \label{def:reward}
\end{definition}


Thus, %we choose an agent reward to be 
reward is 
a continuous-valued function modelled using the agent's ``confidence'' towards the explanandum, and reflecting the contributed arguments to date and the agent's original stance towards the explanandum. Note that this stance is always positive for the first agent and negative for the second, given our choice of $\tau$ in Section~\ref{sec:EX-IC}. %\todo{this what we need to prove the lemma earlier on?}
%The reward is a contribution of private evaluation of the contributed arguments.
Note also that rewards are 
between $[-1, 1]$. %\todo{we define sigma in terms of private classifier and reward for private classifier in terms of sigma: is this not circular?, yaa but the objective is to maximise the class confidence or increase strength so i}
% \begin{remark}
%     Since, base reward and persuasion \todo{what does persuasion mean here?} is based on the agents behaviour (either supporting or attacking to the classifier's claim), which is monitored by stance ($\StanceI (\BAFi_0, \arge)$). All $\lambda_i$'s control the agent behaviour resulting in different properties. 
% \end{remark}

We combine the agent's reward with REINFORCE gradients% within the following combined loss function
:

\begin{equation}
\mathcal{L}^i_{\mathrm{FAX}} = -\sum_t \log \Pi^i_{\theta^i}(h^i_t \mid  \ArgsI_{t-1}\cup \Args^j_{t-1}, \bar{y}^i) (r^i_t - b^i_t)    
\label{eqn:FAX_loss}
\end{equation}
%
where  $\theta^i$ are the parameters of policy $\Pi^i$ (and thus modulator $\mathcal{M}^i$) being learnt and $b^i_t$ is a baseline value estimated by $\Agents^i$ at timstep $t$, which is mainly used to reduce the variance in the agent's behaviour during the exploration stage in (reinforcement) learning.
%
Minimisation of this %objective results in the same behaviour as REINFORCE learning rule described above \cite{mnih2014recurrent}, this 
loss can also be viewed as maximisation of log-likelihood of the policy distribution \cite{mnih2014recurrent}.

Finally, to encourage agents to argue for a particular class ($\bar{y}^1$ for the proponent and $\bar{y}^2$ for the opponent) we use
the \emph{stance loss} $\mathcal{L}^i_{\mathrm{stance}} = \mathrm{CE}(\Sigma^i(\BAFi_t,(x,\bar{y}^i)), \bar{y}^i)$ %\todo{$y$ wrong? should it be $\hat{y}^i$?} 
to obtain  the combined loss%function
: $$\mathcal{L}_{\mathrm{total}} = \mathcal{L}_{\mathrm{dist}} +  \Big(\mathcal{L}^1_{\mathrm{FAX}} + \mathcal{L}^1_{\mathrm{stance}}\Big) + \Big(\mathcal{L}^2_{\mathrm{FAX}} + \mathcal{L}^2_{\mathrm{stance}}\Big).$$%, in our experiments we assign equal weights for all the loss components



\iffalse 
To learn the parameters involved in FAX we use the combination of stance loss and REINFORCE gradients.
We adopt the REINFORCE learning algorithm from \cite{mnih2014recurrent}, formally described as:
\begin{equation}
     \frac{1}{M} \sum_m \sum_k \nabla_{\theta^i} \log \Pi^i(h^i_k \mid \bigcup_i^2 \ArgsI_{k-1}, \tilde{y}^i) (r^i_k - b^i_k)
    \label{eqn:reinforce}
\end{equation}

Here, $b^i_k$ corresponds to the baseline value estimated by an agent, this is mainly used to reduce the variance in an agent's behavior during the exploration stage.
This learning rule can be formalised  as a loss function as shown in Equation \ref{eqn:FAX_loss},  minimisation of this objective results in the same behaviour as REINFORCE learning rule described above \cite{mnih2014recurrent}, this can also be viewed as maximisation of log-likelihood of the policy distribution.

\begin{equation}
\mathcal{L}^i_{\mathrm{FAX}} = -\sum_k \log \Pi^i_{\theta^i}(h^i_k \mid \bigcup_i^2 \ArgsI_{k-1}, \tilde{y}^i) (r^i_k - b^i_k)    
\label{eqn:FAX_loss}
\end{equation}

The $\mathcal{L}^i_{\mathrm{FAX}}$ loss helps in learning the parameters of policy ($\Pi$) and modulator ($\mathcal{M}$) networks, while the stance loss is used to update parameters of core sequential modules $\zeta$.
The stance loss encourages agents to argue for a particular class, which is formally described as $\mathcal{L}^i_{\mathrm{stance}} = \mathrm{CE}(\Sigma^i, \tilde{y}^i)$.
The combined loss used for extracting interactive exchanges is described as: $\mathcal{L}_{\mathrm{total}} = \mathcal{L}_{\mathrm{dist}} + \sum_i \Big(\mathcal{L}^i_{\mathrm{FAX}} + \mathcal{L}^i_{\mathrm{stance}}\Big)$, in our experiments we assign equal weights for all the loss components.

\fi
\paragraph{%Termination condition:
Deployment.}
After training, we deploy the learnt agent% models
s for generating \FAXIC s% for explanation
.  
To determine the number of timesteps in each \FAXIC, we adopt the following strategy.
We analyse cosine similarity between arguments to evaluate the information contributed in each timestep %of the \FAXIC\ 
and we model the gain in information as the average dissimilarity of the contributed argument wrt all the previous arguments.
We terminate the \FAXIC\ if the mean dissimilarity is %$\leq \delta$ 
less then a small amount (which is a parameter) or %in the case of resolution
when the \FAXIC\ is resolved. \todoE{does this mean that we terminate as soon as we resolve? YES IN THE DEPLOYMENT WE CAN ENFORCE IT - LET US CHANGE THE THEORUY AFTER DEADLINE}

\iffalse 

With some exchange termination condition $\mathrm{T}(.)$, the FAX inference algorithm is detailed in Algorithm \ref{alg:VXS}.

\begin{algorithm}[ht]
\caption{Inference steps followed in FAXs} 
\label{algo:VXs}
    \begin{algorithmic}[1]
        \State \textbf{Initialize:} $\zeta, h, \mathcal{M}, \mathfrak{C}, \tilde{q}, q, f, g$
        \State \textbf{Get:} $x \sim \mathcal{X}$
        \State \textbf{Stance assignment:} $z'=f(x); \tilde{y} = g(z')$
        % \State \textbf{Discretization:} $z^1 = \mathfrak{C}_{\tilde{y}^1}(z'); \hat{y} = q(z^1)$
        \State \textbf{Agent States:} $z = \bigcup_{\forall \tilde{y}^i \in \tilde{y}} \mathfrak{C}_{\bar{y}^i}(z') $
        \State \textbf{Prior setting: }
        \For {$i$ in $\{1, \dots, K\}$}
        \For {$z^i_k$ in $z^i$} 
        \State $h^i_0 \leftarrow \zeta^i(h^i_0, \mathcal{M}^i(z^i_k))$
        \State $s^i_k \leftarrow q^i(\zeta^i(h^i_{-1}, \mathcal{M}^i(z^i_k))) - q^i(h^i_{-1})$ 
        \EndFor
        \EndFor
        \State \textbf{Exchanges: }
        \While {$T(.)$}
        \For {$i$ in $\{1, \dots, K\}$}
        \State $\ArgsI_{k} \sim \Pi^i(h^i_{k-1} \mid \bigcup_i^K \ArgsI_{k-1}, \tilde{y}^i)$
        \State $h^i_{k} \leftarrow \zeta^i(h^i_{k-1}, \mathcal{M}^i(\ArgsI_{k}))$
        \State $s \leftarrow q^i(h^i_{k}) - q^i(h^i_{k-1})$
        \State $\BAFi.update(\ArgsI_k)$
        % \State // cross/perceived score assignment...
        \EndFor
        \State $\QBAF.update(\bigcup_{i=1}^K \BAFi)$
        \EndWhile
    \end{algorithmic}
    \label{alg:VXS}
\end{algorithm}
\fi

\section{Evaluation}
\label{sec:eval}

In this section we lay out our approach to evaluating the realization of our \FAXIC s %, as in Section~\ref{sec:set-up}, 
for explaining image classifiers% of the type considered in this paper%, as indicated in Section~\ref{sec:prelim}
.
We 
%define 
use evaluation metrics %and hypotheses 
for assessing 
\iffalse the suitability of %our realisation choices towards faithfulness of the \FAXIC s to the classifier being explained.
the quantized classifier $q$, the codebooks/class-specific features and private classifiers, and their use by our \FAXIC s.
\fi
\emph{faithfulness} and \emph{argumentative quality} of our \FAXIC s.
The metrics are %to be
measured  %globally 
on
a test set $\mathcal{T}\subseteq \mathcal{X}\times \mathcal{Y}$, providing ground-truths (correct classifications) for a number of inputs (we will use concrete instances of $\mathcal{T}$ in our experiments in Section~\ref{sec:results}).

We define the metrics using the same notation  $\mathfrak{C}(z)$ as in Section~\ref{sec:set-up} 
% Here, we abuse the notion of codebook: for and $z=f(x)$, $\mathfrak{C}(z) = \mathfrak{C}_{\bar{y}}(z)$, where $y$ is the top-class predicted by $g(z)$.
as well as 
notations $\mathfrak{C}(h)$
to represent the codebook corresponding to
hidden state representation $h$ (in some sequence model)
and $q^i(h)$ to represent the values assigned by $q^i$ 
to the %set of 
private features/arguments corresponding to
%hidden state representation 
$h$ (in $\zeta^i$).


%To determine whether $q$ works well on $\mathcal{T}$, we consider two metrics. 
The faithfulness metrics are adapted from the literature, and \cite{kori2022explaining} in particular.
The
first metric measures \emph{correctness} of $q$ and of the codebooks
%(wrt the ground truth YES  and the trained feature classifier $g$ NO).
by measuring accuracy %when applied to the codebooks %rather than the outputs of the feature extractor $f$, 
wrt the ground-truth in $\mathcal{T}$:
\vspace*{-0.1cm}
   % \begin{enumerate}[(i)]
    %    \item \emph{Correctness}: 
      \iffalse   \[
             \frac{ \mid \{ %q(f(x)) 
            (x,y) \in \mathcal{T} \mid q(\mathfrak{C}(%f(x)
            z)) = y %, \, \forall  \, x \in \mathcal{X},  y \in \mathcal{Y} \} \mid }{ \mid \mathcal{X} \mid }
             \} \mid }{ \mid \mathcal{T} \mid }
        \]
\fi 
        \[
        \mid \{ %q(f(x)) 
            (x,y) \in \mathcal{T} \mid q(\mathfrak{C}(%f(x)
            z)) = y %, \, \forall  \, x \in \mathcal{X},  y \in \mathcal{Y} \} \mid }{ \mid \mathcal{X} \mid }
             \} \mid / \; { \mid \mathcal{T} \mid }
        \]
%\end{enumerate}

The second metric measures
\emph{completeness} of $q$ on the BAFs resulting from \FAXIC s, by measuring the 
accuracy of $q$ %when applied to 
on
the codebooks corresponding to the hidden state representations of the arguments %adopted by the agents 
%at the end of each \FAXIC
in these BAFs,  
wrt the classifierâ€™s predictions on %the data in 
$\mathcal{T}$:\footnote{With an abuse of notation we use $n$ to indicate the length of every \FAXIC\ obtained from datapoints in $\mathcal{T}$, even though different \FAXIC s will typically have different lengths. %In practice, we use the mean of all the lengths.
}
\iffalse 
%\begin{enumerate}[(ii)]
 %       \item \emph{Completeness}: 
        \begin{align*}
           \frac{ %& 
           \Big| \{ (x, y) \in \mathcal{T} \mid q\left( \mathfrak{C} \left(\cup_{i=1}^N h^i_n \right) \right) = (g \circ f)(x) \} \Big|}{\mid \mathcal{T} \mid}
        \end{align*}
%\end{enumerate}
\fi 
\vspace*{-0.1cm}
%\hspace*{0.6cm} 
\[ 
           \Big| \{ (x, y) \in \mathcal{T} \mid q\left( \mathfrak{C} \left(\cup_{i=1}^N h^i_n \right) \right) = (g \circ f)(x) \} \Big|
           \; /\;  {\mid \mathcal{T} \mid}
           \]
%           \\
This metric gives an indication of the faithfulness to the original classifier of $q$ on the output \FAXIC s (as the loss function used during training  only strives towards faithfulness of $q$ on the input to  \FAXIC s).

\todoE{POSSIBLY AVOID HERE - RAISES MORE QUESTIONS THAN IT ANSWERS....Note that high completeness  is also a possible indicator for lingua franca...as if the agents fail to satisfy it we expect the combined $h^i_n$ to perform poorly... TO BE DISCUSS AFTER}
        
        \iffalse we want faihfulness to the classifier, not the ground truth....
        
        \item Reliability: the accuracy of the FAX framework with respect to the ground truth labels
        \begin{align*}
            \frac{1}{\mid \mathcal{T} \mid} & \Big| \{ q \left( c \left( \cup_{i=0}^K h^i \right) \right) \mid q \left( p\left( \cup_{i=0}^K h^i \right) \right) \\
            & = y, \, \forall \, (x, y) \in \mathcal{T} \}\Big|
        \end{align*}
        \fi

        %We hypothesise that both correctness and completeness need to be high in practice.
        
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%

%\FAXIC s contribute meaningful explanations \todo{the exchange BAFs? the private BAFs? we still need to decide....} as soon as they represent the inner reasoning of the (distilled) classifier. Given that \FAXIC s take out two conflicting viewpoints from the classifier, how do we measure that the dialecticity they exhibit reflects the inner reasoning? \todo{a comment about why this is difficult to measure directly?} We assess dialecticity indirectly as follows: we hypothesise that when the classifier is ``overall certain'' about its predictions then the \FAXIC s have \emph{high certainty} (in favour of the explanandum, see Definition~\ref{def:confidence}) and when the classifier is ``overall uncertain'' about its predictions then the \FAXIC s have \emph{low certainty} (of the explanandum, see Definition~\ref{def:confidence})

The argumentative quality metrics 
are tailored to our (implementation of) \FAXIC s. The third metric measures \emph{consensus} amongst the (two) agents% in \FAXIC s
, in terms of the number of resolved \FAXIC s:
%\vspace*{-0.1cm}
\iffalse 
    %\begin{enumerate}[(iii)]
     %   \item  \emph{Consensus}: 
        \begin{align*}
            \frac{ \Big| \{(x, y) \in \mathcal{T} \mid q^i (h^i_n) = q^j (h^j_n), %\forall 
            j \neq i \}\Big|}{\mid \mathcal{T} \mid}
        \end{align*}
      %  \end{enumerate}
      \fi 

\vspace*{-0.1cm}
%\hspace*{0.8cm}
\[
\Big| \{(x, y) \in \mathcal{T} \mid q^i (h^i_n) = q^j (h^j_n), %\forall 
            j \neq i \}\Big|
            \; / \; {\mid \mathcal{T} \mid}
            \]
     % \\
The fourth (and final) metric (\emph{pro persuasion rate}) measures  consensus again, but towards the proponent agent:   

%\vspace*{-0.1cm}
\iffalse 
       %\begin{enumerate}[(iv)] 
        %\item \emph{Pro persuasion rate}: 
        \begin{align*}
             \frac{ \Big| \{ (x, y) \in \mathcal{T} \mid q^1 (h^1_0) = q^j (h^j_n),  j \neq 1 \}\Big|}{\mid \mathcal{T} \mid} 
        \end{align*}
        %\end{enumerate}
        \fi 

\vspace*{-0.1cm}
%\hspace*{0.8cm}
    \[    \Big| \{ (x, y) \in \mathcal{T} \mid q^1 (h^1_0) = q^j (h^j_n),  j \neq 1 \}\Big|
    \; / \; {\mid \mathcal{T} \mid}
    \]
%      \\  
Given that agents are trained to disagree (see Definition~\ref{def:reward}) both %consensus and pro persuasion rate 
argumentative metrics can be seen as estimates of the goodness of the learnt class-specific features: high %consensus/pro persuasion rate 
values indicate that information is leaked across different features (arguments).  

\iffalse 
       \begin{enumerate}[(v)] 
        \item \emph{Pro resolution rate}: 
        \begin{align*}
             \frac{\Big| \{ (x, y) \in \mathcal{T} \mid q^1 (h^1_0) = q^j (h^j_t), j \neq 1, \; %t > 0 NEW NOTATION 
             t \in ]n] \}\Big|}{|\mathcal{T}|  n} 
        \end{align*}

        % \item Persuasion monotonicity

        % \begin{align*}
        %      \frac{1}{|\mathcal{T}|n} & \Big| \{ (x, y) \! \in \! \mathcal{T} | \mathrm{sign}(q^i (h^i_t) - q^i (h^i_{t-1})) = \\
        %      & \mathrm{sign}(q^j (h^i_t) - q^j (h^i_{t-1})),  j \neq i, \forall t > 0 \}\Big|
        % \end{align*}

        
        % \begin{align*}
        %    \frac{\sum_{\FAXICs \in \mathcal{E}} \! |\! \{\! \arga \!\!\in\!\! \ArgsI_0 \!\cap\! \ArgsX_n \! \setminus \! \{ \! \arge \!\} | (\!\arga,\! \arge\!) \!\!\in\!\! (\AttsI_n \!\cap\! 
        %    \AttsJ_n ) 
        %    \!\cup\! (\SuppsI_n \!\cap\! 
        %    \SuppsJ_n ) 
        %    )\! \}\! | }
        %    { \sum_{\FAXICs \in \mathcal{E}} | \ArgsI_0 \cap \ArgsX_n \! \setminus \! \{ \! \arge \!\} | } \nonumber
        % \end{align*} 
    \end{enumerate}

\fi

\section{Experiments}
\label{sec:results}

\begin{table}
    \centering
\caption{Accuracies of the trained classifiers} %averaged over three runs
\vspace*{-0.4cm}
\resizebox{1.0\columnwidth}{!}{
        \begin{tabular}{lccc}\\\toprule  
        %\textsc{Models}
        & \textsc{Fair}   & \textsc{Biased} & \textsc{Random} \\\midrule
        \textsc{AFHQ-ResNet-18}      & 0.95  &  0.39 & 0.31 \\ 
        \textsc{AFHQ-DenseNet-121}   & 0.96  &  0.47 & 0.32 \\ \midrule
        \textsc{FFHQ-ResNet-18}      & 0.88  &  0.58 &  0.47 \\ 
        \textsc{FFHQ-DenseNet-121}   & 0.92  &  0.61 &  0.48 \\
        \bottomrule
        \end{tabular}
    }
\label{table:classifier_results}
\end{table}

\begin{table*}
\centering
\caption{%Input-Output faithfulness and completeness 
Faithfulness properties of all considered methods on the DenseNet121 classifiers.}
\label{table:comparative_results}
\vspace*{-0.2cm}
\resizebox{\textwidth}{!}{

\begin{tabular}{c|ccccc|ccccc} 
\toprule 
\multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}}\textsc{Methods} $\rightarrow$ \\ \textsc{Dataset} $\downarrow$\end{tabular}}} 
& \multicolumn{5}{c}{\textbf{\begin{tabular}[c]{@{}c@{}} \textsc{Correctness}\end{tabular}}} 
& \multicolumn{5}{c}{\textbf{\begin{tabular}[c]{@{}c@{}} \textsc{Completeness} \end{tabular}}} \\ 
\cmidrule{2-11} 

& \textsc{GradCAM} & \textsc{DeepLIFT} & \textsc{DeepSHAP} & \textsc{LIME}  & \textsc{FAX}
& \textsc{GradCAM} & \textsc{DeepLIFT} & \textsc{GradSHAP} & \textsc{LIME} & \textsc{FAX} \\ 

\midrule 

\textbf{FFHQ-Random} 

& $0.55$ % \textcolor{gray}{\pm 0.14}$ 
& $0.58$ % \textcolor{gray}{\pm 0.12}$ 
& $0.54$ % \textcolor{gray}{\pm 0.15}$ 
& $0.50$ % \textcolor{gray}{\pm 0.18}$ 
& $0.53$

& $0.40$ % \textcolor{gray}{\pm 0.12}$ 
& $0.41$ % \textcolor{gray}{\pm 0.15}$ 
& $0.38$ % \textcolor{gray}{\pm 0.17}$ 
& $0.36$ % \textcolor{gray}{\pm 0.20}$ \\ 
& $0.97$ 

\\
% \midrule 

\textbf{FFHQ-Biased} 

& $0.35$ % \textcolor{gray}{\pm 0.09}$ 
& $0.37$ % \textcolor{gray}{\pm 0.14}$ 
& $0.39$ % \textcolor{gray}{\pm 0.11}$ 
& $0.50$ % \textcolor{gray}{\pm 0.17}$ 
& $0.91$

& $0.34$ % \textcolor{gray}{\pm 0.08}$ 
& $0.36$ % \textcolor{gray}{\pm 0.13}$ 
& $0.40$ % \textcolor{gray}{\pm 0.15}$ 
& $0.48$ % \textcolor{gray}{\pm 0.18}$ 
& $1.00$

\\ 

% \midrule 

\textbf{FFHQ-Fair} 

& $0.77$ % \textcolor{gray}{\pm 0.09}$ 
& $0.77$ % \textcolor{gray}{\pm 0.11}$ 
& $0.73$ % \textcolor{gray}{\pm 0.13}$ 
& $0.58$ % \textcolor{gray}{\pm 0.17}$ 
& $0.96$


& $0.74$ % \textcolor{gray}{\pm 0.08}$ 
& $0.74$ % \textcolor{gray}{\pm 0.11}$ 
& $0.68$ % \textcolor{gray}{\pm 0.12}$ 
& $0.59$ % \textcolor{gray}{\pm 0.19}$ 
& $0.96$
\\ 
\midrule 

\textbf{AFHQ-Random} 

& $0.30$ % \textcolor{gray}{\pm 0.12}$ 
& $0.32$ % \textcolor{gray}{\pm 0.13}$ 
& $0.28$ % \textcolor{gray}{\pm 0.14}$ 
& $0.25$ % \textcolor{gray}{\pm 0.17}$ 
& $0.55$

& $0.35$ % \textcolor{gray}{\pm 0.12}$ 
& $0.36$ % \textcolor{gray}{\pm 0.14}$ 
& $0.33$ % \textcolor{gray}{\pm 0.16}$ 
& $0.31$ % \textcolor{gray}{\pm 0.19}$ 
& $0.72$

\\ 

% \midrule 

\textbf{AFHQ-Biased} 

& $0.27$ % \textcolor{gray}{\pm 0.10}$ 
& $0.32$ % \textcolor{gray}{\pm 0.12}$ 
& $0.37$ % \textcolor{gray}{\pm 0.15}$ 
& $0.48$ % \textcolor{gray}{\pm 0.18}$ 
& $0.71$

& $0.35$ % \textcolor{gray}{\pm 0.10}$ 
& $0.38$ % \textcolor{gray}{\pm 0.15}$ 
& $0.40$ % \textcolor{gray}{\pm 0.18}$ 
& $0.47$ % \textcolor{gray}{\pm 0.20}$ 
& $0.91$

\\ 

% \midrule 

\textbf{AFHQ-Fair} 

& $0.75$ % \textcolor{gray}{\pm 0.10}$ 
& $0.71$ % \textcolor{gray}{\pm 0.11}$ 
& $0.71$ % \textcolor{gray}{\pm 0.14}$ 
& $0.52$ % \textcolor{gray}{\pm 0.19}$ 
& $0.78$

& $0.70$ % \textcolor{gray}{\pm 0.10}$ 
& $0.66$ % \textcolor{gray}{\pm 0.15}$ 
& $0.67$ % \textcolor{gray}{\pm 0.17}$ 
& $0.54$ % \textcolor{gray}{\pm 0.21}$ 
& 0.99


\\ 

\bottomrule 
\end{tabular}
}
\end{table*}


We analyse \FAXIC s on the high resolution animal and human faces (AFHQ\cite{choi2020stargan}, FFHQ \cite{karras2019style}) datasets, with two well known architectures ResNet-18 \cite{he2016deep} and DenseNet121 \cite{huang2017densely} as image classifiers $g \circ f$.
We %analyse models under 
consider three %different 
settings: (i) \emph{fair}, where the classifier is trained with correct labels; (ii) \emph{biased}, where the classifier is trained with % we consider label bias setting 
biased labels, obtained by randomly switching the labels for 10\% of the datasets%' datapoints
, and (iii) \emph{random}: where we use randomly initialised weights for the classifiers rather than training them% with the data
. 

\begin{figure*}
    \hfill
    \includegraphics[width=.423\textwidth]{imgs/fax1.png} \hfill
    \includegraphics[width=.567\textwidth]{imgs/fax2.png} \hfill
    \caption{Arguments %(presented visually) %exchanged in the BAF exchanges
    in \FAXIC s (%where, for each image, 
    the proponent starts with the top left argument, the opponent follows with the argument below it, etc.), for classifiers trained on  FFHQ (left column) and AFHQ (right column)% datasets
    , on fair (top row) and biased (bottom row) %classifier 
    settings. 
    %MAKE SURE COMMENTS ON DIFFERING LENGHT IN TEXT. TOO MUCH REPETITION? TRY AND FIND BETTER EXAMPLES? 
    }
    \label{fig:qualitative results}
    \vspace{-5pt}
\end{figure*}

\paragraph{Qualitative results}
Figure \ref{fig:qualitative results} shows some \FAXIC s visualised as in 
\cite{kori2022explaining}, using the approach in \cite{dissection}%, so that they are comprehensible to humans
.
%, illustrates these visual arguments involved in the exchange BAF, both the figures on left correspond to the classifier trained FFHQ dataset in fair and biased setting respectively, similarly figures on right correspond to the model trained on AFHQ dataset in fair and biased setting respectively.
In all these figures, the first image is the input, while the first and second rows correspond respectively to $\Agents^1$'s and $\Agents^2$'s %visual
arguments. 
%As observed in visual arguments, 
We can see that \FAXIC s %explanations 
focus on different but semantically meaningful regions on the input images, %in the case of 
for both agents.
In the biased setting, as previously described, we expect some leakage of information across different class-specific features/arguments, as observed in 
%case of AFHQ biased example we see the important regions being overlapped across arguments.
the figure.

\paragraph{Quantitative results} Table \ref{table:classifier_results} gives
the classifiers' accuracy on test sets.
For the DenseNet121 classifiers,
Table~\ref{table:comparative_results} gives the faithfulness results  %\footnote{The results for the ResNet-18 are similar.} 
in comparison with standard baseline% method
s (%these are
\CR{i.e.}
GradCAM \cite{selvaraju2017grad}, DeepLIFT \cite{deeplift},  DeepSHAP \cite{SHAP} and LIME \cite{lime}) and Table~\ref{table:densenet_FAX_results} measures the argumentative metrics in the three settings (for \FAXIC s only, as these metrics are not applicable to baseline methods).
Further,
Table \ref{table:resnet_FAX_results} measure all metrics for the esNet-18 classifiers.
We observe that 
%In the case when FAXs faithfully represent the underlying classifier
\emph{completeness} is high in all settings, 
while \emph{correctness} is high for the fair and biased setting\CR{s only}% but low for the %random setting
%other
; this is due to completeness reflecting \FAXIC s  behaviour wrt continuous classifier, while correctness is measured wrt ground truth.
As for the last two metrics, the experiments show higher values in the biased than fair settings, as expected, given that  
 we expect the leak of features across codebooks due to incorrect label assignment in the former, which results in higher consensus and easier persuasion.
 We can observe mixed behaviours %in the case of 
 \CR{with} random classifiers, which results in high value in the case of overlapping features during initialisation and low in the other case.
 % \todo{some comment on random?}
\CR{Overall, the experiments show that for more ``uncertain'' models (random), the agents cannot reach as much consensus as for ``certain'' models (fair and biased) and the persuasion rate is lower for ``uncertain'' models.
%In case of a biased model, given that the classifier model is biased to a particular class, we see higher convergence for both properties. 
This analysis is empowered by the argumentative nature of our explanatory framework.}


\begin{table}[]
    \centering
    \caption{%Exchange Properties 
    Argumentative metrics for %the
    DenseNet-121 classifiers. %AVERAGES INSTEAD
    }
   \vspace*{-0.4cm} \resizebox{1.0\columnwidth}{!}{
        \begin{tabular}{lccc}\\\toprule  
        %\textsc{Models}
        & \textsc{Fair}   & \textsc{Biased} & \textsc{Random} \\\midrule
        %%%%\textbf{\textsc{AFHQ}} \\
        % \textsc{Correctness}            &  0.78  & 0.71   & 0.55  \\
        % \textsc{Completeness}           &  0.99  & 0.91   & 0.72  \\
        % \textsc{Relaibility} \\
        \textbf{AFHQ-Consensus}             &  0.24 &  0.44  &  0.26 \\ 
        \textbf{AFHQ-Pro persuasion rate}   &  0.27 &  0.41  &  0.77 \\
        %\textsc{Pro resolution rate}   &  0.31 &  0.49  &  0.74 \\
        \midrule
        %\textbf{\textsc{FFHQ}} \\
        % \textsc{Correctness}            &  0.96 & 0.91   & 0.53 \\
        % \textsc{Completeness}           &  0.96 & 100.0  & 0.97 \\
        % \textsc{Relaibility}           & \\
        \textbf{FFHQ-Consensus}             & 0.42  & 0.54   & 0.09 \\ 
        \textbf{FFHQ-Pro persuasion rate}   & 0.33  & 0.50   & 0.38\\
        %\textsc{Pro resolution rate}   & 0.24  & 0.26   & 0.84 \\
        \bottomrule
        \end{tabular}
    }
\label{table:densenet_FAX_results}
\end{table}

\begin{table}[]
    \centering
    \caption{%Exchange Properties for the
    %Argumentative 
    All metrics for ResNet-18 classifiers. %POSSIBLY MOVE
    }
    \vspace*{-0.4cm}\resizebox{1.0\columnwidth}{!}{
        \begin{tabular}{lccc}\\\toprule  
        %\textsc{Models}  
        & \textsc{Fair}   & \textsc{Biased} & \textsc{Random} \\\midrule
        %\textbf{\textsc{AFHQ}} \\
        \textbf{AFHQ-Correctness}            &  0.77 & 0.73 & 0.29 \\
        \textbf{AFHQ-Completeness}           &  0.99 & 0.93 & 0.68 \\
        % \textsc{Relaibility} \\
        \textbf{AFHQ-Consensus}             &  0.57 & 0.89 & 0.13 \\ 
        \textbf{AFHQ-Pro persuasion rate}   &  0.48 & 0.56 & 0.16 \\
        %\textsc{Pro resolution rate}   &  0.91 & 0.93 & 0.51 \\
        \midrule
        %\textbf{\textsc{FFHQ}} \\
        \textbf{FFHQ-Correctness}          &  0.65 & 0.45 &  0.51 \\
        \textbf{FFHQ-Completeness}         &  0.99 & 0.99 &  0.91 \\
        % \textsc{Relaibility} \\
        \textbf{FFHQ-Consensus}             &  0.31 & 0.90 &  0.45 \\ 
        \textbf{FFHQ-Pro persuasion rate}   &  0.24 & 0.53 &  0.71 \\
        %\textsc{Pro resolution rate}   &  0.32 & 0.28 &  0.37 \\
        \bottomrule
        \end{tabular}
    }
\label{table:resnet_FAX_results}
\end{table}


