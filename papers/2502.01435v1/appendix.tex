% \pagebreak 

\appendix
\section{Appendix}


\subsection{Computational complexity proofs}
\label{appendix:complexity}
\begin{proof}[Proof of Proposition~\ref{prop:np_hardness}]
We will show the \np-hardness of \prbstrwk by a reduction from the \np-hard \prbmaxclique problem. As $\lambda=0$, we simply write the score as $q(U, L)$ instead of $q(U, L; \lambda) = q(U, L; 0)$ for brevity. Assume that the set $U$ with the labeling $L$ is an optimal solution to the \prbstrwk problem with $\lambda = 0$, maximizing the score $q(U, L)$ while satisfying the \stc property. 
%For each vertex $v$ in $U$, let $s(v)$ denote the number of strong edges connected to it. 
The density of the strong edges in $U$ is then

\begin{equation}
\label{equation:strong_density}
q(U, L) = \frac{\sum_{v \in U} \deg_s(v, U, L)}{2\abs{U}}.
\end{equation}

Consider a vertex $w$ in $U$ with the highest number $\deg_s(w, U, L)$ of strong edges connected to it. As the maximum number of strong edges, $\deg_s(w, U, L)$ has to be at least the average, 
\begin{equation}
\label{equation:average_degree}
\deg_s(w, U, L) \geq \frac{\sum_{v \in U} \deg_s(u, U, L)}{\abs{U}}.
\end{equation}

Then for any two vertices $v$ and $u$ that have strong edges $(v,w)$ and $(u,w)$ connecting them to $w$, there must be an edge between $v$ and $u$ to satisfy the \stc property.

Thus, the vertex $w$ and its strong neighbors form a clique $C$ with $\deg_s(w, U, L)+1$ vertices.
Consider only having these vertices in $C$ and having a labeling $L'$ that labels each edge in the clique as strong. This would satisfy the \stc property and would give a score of 
\begin{equation*}
q(C, L') = \frac{\abs{C}(\abs{C}-1)}{2\abs{C}} = \frac{\abs{C}-1}{2} = \frac{\deg_s(w, U, L)}{2} .
\end{equation*}

From Equations~\ref{equation:strong_density} and~\ref{equation:average_degree}, we get 
\begin{equation}
\label{equation:clique_score_comparison}
q(C, L') = \frac{\deg_s(w, U, L)}{2} \geq \frac{\sum_{v \in U} \deg_s(v, U, L)}{2\abs{U}} = q(U, L).
\end{equation}

Thus, the clique $C$ has at least the same score as $U$, which has the maximum score of all subgraphs, so $q(C, L') = q(U, L)$. This means that $C$ must be a maximum size clique in the input graph $G$, as larger cliques would give a higher score than $U$.

%  XXX imprecise language
Therefore, by finding an optimal set of vertices $U$ and labeling $L$ we can find a maximum clique $C$. Thus, \prbstrwk is \np-hard.
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:inapproximability}]
Assume that we can find a set $U$ with labeling $L$ that is an $\alpha$-approximation to \prbstrwk, while the optimal solution has value $q(C^*, L^*)$ with a maximum clique $C^*$ and labeling $L^*$. Consider then the vertex $w$ with the highest number of strong edges and construct the clique $C$ consisting of $w$ and its strong neighbors. Define the labeling $L'$ such that each edge in the clique $C$ is labeled as strong. Using Equation~\ref{equation:clique_score_comparison} and that $q(U, L)$ is an $\alpha$-approximation, we get 
\begin{equation*}
%q(C, L') = \frac{\deg_s(w, U, L)}{2} \geq \frac{\sum_{v \in U} \deg_s(v, U, L)}{2\abs{U}} = q(U, L) \geq \alpha q(C^*, L^*).
q(C, L') \geq q(U, L) \geq \alpha q(C^*, L^*).
\end{equation*}
But as the score of a clique $C$ with only strong edges is $q(C, L') = \frac{\abs{C}-1}{2}$, we have 

\begin{equation*}
\frac{\abs{C}-1}{2} \geq \alpha\frac{\abs{C^*}-1}{2}.
\end{equation*}

By solving for $\abs{C}$ and using $\alpha \leq 1$, we get

\begin{equation*}
\abs{C} \geq \alpha \pr{\abs{C^*}-1} + 1 \geq \alpha\abs{C^*}.
\end{equation*}

This means that we have an $\alpha$-approximation for \prbmaxclique.
Therefore, any inapproximability results for \prbmaxclique also apply for the $\lambda = 0$ case of \prbstrwk. Using the result by~\citet{zuckerman2006linear} then finishes the proof.
\end{proof}


To prove Proposition~\ref{prop:nphard2} we need the following lemma.

\begin{lemma}
\label{lem:opt}
Assume graph $G$. Let $X \subsetneq Y$ be two subgraphs, and let $L$ be a labeling defined on $Y$. Define
\[
    \Delta(X, Y) = \frac{m_s(Y) + \lambda m_w(Y) - m_s(X) - \lambda m_w(X)}{\abs{Y} - \abs{X}}\quad.
\]
If $\Delta(X, Y) < \score{Y, L}$, then $\score{Y, L} < \score{X, L}$, if $\Delta(X, Y) > \score{Y, L}$, then $\score{Y, L} > \score{X, L}$,
and if $\Delta(X, Y) = \score{Y, L}$, then $\score{Y, L} = \score{X, L}$.
\end{lemma}

\begin{proof}
Assume $\Delta(X, Y) < \score{Y, L}$. Multiply by $(\abs{Y} - \abs{X})\abs{Y}$ and subtract $\abs{Y} (m_s(Y)+\lambda m_w(Y))$ from both sides. Dividing by $-\abs{X}\abs{Y}$ then gives $\score{X,L} > \score{Y,L}$, proving the first claim. The proofs for other claims are identical.
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:nphard2}]
We will prove the hardness by reducing an \np-hard problem \prbminSTC to our problem. In \prbminSTC, we are asked to label the full graph and minimize the number of weak edges~\citep{sintos2014using}.
Assume a graph $G$ with nodes $V = v_1, \ldots v_n$. We assume that $n \geq 5$.
We define a new graph $H$ that consists of $G$ and $k = \ceil{1/\lambda}(n + 1)/2$ cliques $C_i$ of size $n$. Let $\set{c_{ij}}$ be the nodes in $C_i$. For each $j$ and $i$, we connect $v_j$ with $c_{ij}$.

Let $U$ be the optimal subgraph of $H$ for \prbstrwk and $L$ be its labeling.
Let $L'$ be the labeling where every $E(C_i)$ is strong and the remaining edges are weak. Note that $\score{U} \geq \score{C_i, L'} = (n - 1)/2 \geq 2$ for any $C_i$. We claim that $U$ contains every node in $H$.

%Note that even though the labeling is defined only for $U$ we can safely extend $L$ to contain the complete graph by labeling all the outside edges in $C_i$ as strong.

To prove the claim, let us define $W_i = U \cap C_i$. If $\abs{W_i} = 1, 2$, then $\Delta(U \setminus W_i, U) \leq 3/2 < \score{U}$ and Lemma~\ref{lem:opt} states that we can delete $W_i$ from $U$ and obtain a better score. Assume $3 \leq \abs{W_i} < n$. Let $c \in C_i \setminus W_i$. We can safely assume that the edges between $W_i$ and $G$ are weak; otherwise, we can relabel them as weak and compensate by labeling any weak edge in $W_i$ as strong. Now we can extend the labeling $L$ to $c$ by setting the edges from $c$ to $W_i$ as strong, and the possibly remaining edge as weak. We can show that $\Delta(U \setminus W_i, U) < \Delta(U,U \cup \set{c})$. Lemma~\ref{lem:opt}, applied twice, states that either deleting $W_i$ or adding $c$ improves the solution. Therefore, either $W_i = \emptyset$  or $W_i = C_i$.

Assume $W_i = C_i$ and $W_j = \emptyset$. The optimal labeling must be such that all edges between $W_i$ and $G$ are weak and the edges in $W_i$ are all strong. We can extend the same labeling scheme to $C_j$. Then $\Delta(U \setminus C_i, U) = \Delta(U, U \cup C_j)$. If $\Delta(U, U \cup C_j) > \score{U}$,  Lemma~\ref{lem:opt} implies that we improve the solution by adding $C_j$, which is a contradiction. Hence, $\Delta(U \setminus C_i, U) \leq \score{U}$. Lemma~\ref{lem:opt} implies that we can safely delete $C_i$. Applying this iteratively we arrive to an optimal solution with nodes only in $V$.
This cannot happen since then $\score{U} \leq (n - 1)/2$,
but then $\score{C_i \cup V, L'} = (n - 1)/2 + \lambda/2 > q(U)$. Therefore, $W_i = C_i$ for every $i$.

Finally, assume $v_j \notin U$. Then $\Delta(U, U \cup \set{v_j}) \geq \lambda k \geq (n + 1)/2 > (n - 1)/2 + \lambda \geq \Delta(U \setminus C_i, U)$. Lemma~\ref{lem:opt} states that either deleting any $C_i$ or adding $v_j$ improves the solution. This contradicts the optimality of $U$, so every $v_j \in U$.
%The former cannot happen, so every $v_j \in U$.

Consequently, $V \subseteq U$. The optimal labeling must have every edge in $C_i$ as strong, the cross-edges between $C_i$ and $G$ as weak, and the labels for edges in $G$ solve \prbminSTC.
\end{proof}

\subsection{Proofs for Section~\ref{sec:ip}}
\label{appendix:ip}


\begin{proof}[Proof of Proposition~\ref{prop:frac}]
Let us write $f(U, L) = m_s(U, L) + \lambda m_w(U, L)$.
Note that
\begin{equation}
\label{eq:alphapos}
    f(U(\alpha), L(\alpha)) - \alpha \abs{U(\alpha)} \geq 0.
\end{equation}
Assume $\alpha > \alpha^*$. If $U(\alpha) \neq \emptyset$, then Eq.~\ref{eq:alphapos} implies that
\[
    \score{U(\alpha), L(\alpha)} \geq \alpha > \alpha^*,
\]
which contradicts the optimality of $\alpha^*$. Thus, $U(\alpha) = \emptyset$.

Assume $\alpha < \alpha^*$. Then
\[
\begin{split}
    f(U(\alpha), L(\alpha)) - \alpha \abs{U(\alpha)} & \geq f(U^*, L^*) - \alpha \abs{U^*} \\
    & > f(U^*, L^*) - \alpha^* \abs{U^*} = 0.
\end{split}
\]
That is, $f(U(\alpha), L(\alpha)) >  \alpha \abs{U(\alpha)}$, implying in turn that $U(\alpha) \neq \emptyset$ and $\score{U(\alpha), L(\alpha)} > \alpha$.
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:opt-ip-approx}]
Let $L$ and $U$ be the values of the interval when binary search is terminated. Note that $\alpha \geq L$ due to Proposition~\ref{prop:frac}.
We know that $U - L \leq \epsilon L$ and $L \leq \alpha^* \leq U$. Thus,
$\alpha^* - L \leq U - L \leq \epsilon L$, or $\alpha^*  \leq (1 + \epsilon) L \leq (1 + \epsilon) \alpha$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:opt-ip-exact}]
Let $\alpha$ be the score of the solution $X$, $L$ returned by $\algip$, and let $\alpha^*$ be the score of the optimal solution $X^*$, $L^*$ for \prbstrwk.
We will show that if $\alpha < \alpha^*$, then $\alpha^* - \alpha \geq 1/(bn^2)$, which contradicts with the fact that $\alpha^* - \alpha \leq \epsilon \alpha < \epsilon n / 2 = 1/(bn^2)$.

To prove the claim, let $\Delta = \alpha^* - \alpha$. Then
\[
\begin{split}
	\Delta & =
	\frac{m_s(X^*) + \frac{a}{b} m_w(X^*)}{\abs{X^*}} - \frac{m_s(X) + \frac{a}{b} m_w(X)}{\abs{X} } \\
	& = \frac{ \abs{X} (bm_s(X^*) + a m_w(X^*)) -  \abs{X^*}(bm_s(X) + a m_w(X))}{b \abs{X} \abs{X^*}}.
 %= \frac{ b(\abs{E_s(Y)} - \abs{E_s(X)} ) +  a( \abs{E_w(Y)}  -  \abs{E_w(X)}) }{b \abs{X} \abs{Y}}\quad.
\end{split}
\]
Note that the numerator and the denominator are both integers.
Consequently, if $\Delta > 0$, then $\Delta \geq 1/(bn^2)$.
It follows that if we set $\epsilon = \frac{2}{bn^3}$, then \algip finds the optimal solution in  $\bigO{\log n + \log b}$ number of rounds.
\end{proof}

\subsection{Proofs for Section~\ref{sec:lp}}
\label{appendix:lp}

\begin{proof}[Proof of Proposition~\ref{prop:frac2}]
Scaling $(x^*, y^*, z^*)$ by any constant $c > 0$ does not
change the value of $r(\cdot, \cdot, \cdot)$ nor does it change the validity of the constraints in Eqs.~\ref{ip_con_1}--\ref{ip_con_3}. Therefore, we can safely assume that $x_e^*, z_e^* \leq 1$ and $y_i^* \leq 1$ and $i \in V$, for any $e \in E$ and $i \in V$.
The claim now follows by repeating the steps of the proof of Proposition~\ref{prop:frac}.
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:lp}]
Scaling $(x^*, y^*, z^*)$ by any constant $c > 0$ does not
change the value of $r(\cdot, \cdot, \cdot)$ nor does it change the validity of the constraints in Eqs.~\ref{ip_con_1}--\ref{ip_con_3}. Therefore, we can safely require that $\sum y_i = 1$, which immediately proves the claim.
\end{proof}


\iffalse

\subsection{Further experiments}

\begin{figure}[h!]
\begin{subcaptiongroup}
\phantomcaption\label{fig:slb-r}
\phantomcaption\label{fig:slb-r-fb}
\begin{center}
\begin{tabular}{ll}
\begin{tikzpicture}
\begin{axis}[xlabel={$\lambda$}, ylabel= {$P(U)$},
    width = 4.5cm,
    height = 3.8cm,
    xmin = 0,
    xmax = 1,
    ymin = 0,
    ymax = 1,
    scaled y ticks = true,
    cycle list name=yaf,
    yticklabel style={/pgf/number format/fixed},
    no markers,
]
\addplot table [x=x, y=p1, col sep=comma] {real_lam.csv};
\addplot table [x=x, y=p2, col sep=comma] {real_lam.csv};
\addplot table [x=x, y=p3, col sep=comma] {real_lam.csv};
\addplot table [x=x, y=p4, col sep=comma] {real_lam.csv};
\pgfplotsextra{\yafdrawaxis{0}{1}{0}{1}}
\end{axis}
\node[anchor=north east] at (0, -0.3) {(a)};
\end{tikzpicture}
&
\begin{tikzpicture}
\begin{axis}[xlabel={$\lambda$}, ylabel= {$P(U)$},
    width = 4.5cm,
    height = 3.8cm,
    xmin = 0,
    xmax = 1,
    ymin = 0.2,
    ymax = 1,
    scaled y ticks = true,
    cycle list name=yaf,
    yticklabel style={/pgf/number format/fixed},
    no markers,
    legend entries = {\alggreedyfastest, \algdenseg, \algdensec,\alglpstc},
    legend pos = south west
]
\addplot table [x=x, y=p1, col sep=comma] {real_lam_fb.csv};
\addplot table [x=x, y=p2, col sep=comma] {real_lam_fb.csv};
\addplot table [x=x, y=p3, col sep=comma] {real_lam_fb.csv};
\addplot table [x=x, y=p4, col sep=comma] {real_lam_fb.csv};
\pgfplotsextra{\yafdrawaxis{0}{1}{0.2}{1}}
\end{axis}
\node[anchor=north east] at (0, -0.3) {(b)};
\end{tikzpicture}

\end{tabular}
\end{center}
\end{subcaptiongroup}
\caption{$P(U)$ as a function of $\lambda$.
(a) : \dtname{LastFM} dataset, (b) : \dtname{Facebook} dataset.
}
\label{fig:lam-q-real-fb}
\end{figure}

\begin{figure}[h!]
\begin{subcaptiongroup}
\phantomcaption\label{fig:sla-r}
\phantomcaption\label{fig:sla-r-fb}
\begin{center}
\begin{tabular}{ll}
\begin{tikzpicture}
\begin{axis}[xlabel={$\lambda$}, ylabel= {$\score{\cdot; \lambda}$},
    width = 4.5cm,
    height = 3.8cm,
    xmin = 0,
    xmax = 1,
    ymin = 0,
    ymax = 15,
    scaled y ticks = true,
    cycle list name=yaf,
    yticklabel style={/pgf/number format/fixed},
    no markers,
    legend entries = {\alggreedyfastest, \algdenseg, \algdensec,\alglpstc},
    legend pos = south  east
]
\addplot table [x=x, y=y1, col sep=comma] {real_lam.csv};
\addplot table [x=x, y=y2, col sep=comma] {real_lam.csv};
\addplot table [x=x, y=y3, col sep=comma] {real_lam.csv};
\addplot table [x=x, y=y4, col sep=comma] {real_lam.csv};
\pgfplotsextra{\yafdrawaxis{0}{1}{0}{15}}
\end{axis}
\node[anchor=north east] at (0, -0.3) {(a)};
\end{tikzpicture}&
\begin{tikzpicture}
\begin{axis}[xlabel={$\lambda$}, ylabel= {$\score{\cdot; \lambda}$},
    width = 4.5cm,
    height = 3.8cm,
    xmin = 0,
    xmax = 1,
    ymin = 0,
    ymax = 100,
    scaled y ticks = true,
    cycle list name=yaf,
    yticklabel style={/pgf/number format/fixed},
    no markers,
    legend entries = {\alggreedyfastest, \algdenseg, \algdensec, \alglpstc},
    legend pos = north west
]
\addplot table [x=x, y=y1, col sep=comma] {real_lam_fb.csv};
\addplot table [x=x, y=y2, col sep=comma] {real_lam_fb.csv};
\addplot table [x=x, y=y3, col sep=comma] {real_lam_fb.csv};
\addplot table [x=x, y=y4, col sep=comma] {real_lam_fb.csv};
\pgfplotsextra{\yafdrawaxis{0}{1}{0}{100}}
\end{axis}
\node[anchor=north east] at (0, -0.3) {(b)};
\end{tikzpicture}

\end{tabular}
\end{center}
\end{subcaptiongroup}
\caption{Scores as a function of $\lambda$. (a) : \dtname{LastFM} dataset, (b) : \dtname{Facebook} dataset.
}
\label{fig:lam-q-real}
\end{figure}



Recall that Proposition~\ref{prop:np_hardness} showed that for $\lambda = 0$ solving \prbstrwk is equal to finding a maximum clique.
Let us now look at how close the outputted graphs are to a clique as a function of $\lambda$.
Let $P(U) = \abs{E(U)} /  {\abs{U} \choose 2}$ be a measure indicating how close we are to a clique, $P(U) = 1$  means that $U$ is a clique.

%Next we study $P(U)$ as a function of $\lambda$ for \dtname{Facebook} and \dtname{Cora} dataset as shown in %Figure~\ref{fig:lam-q-real} and Figure~\ref{fig:lam-q-real-fb}. 
Next, let us look at Figure~\ref{fig:slb-r} and \ref{fig:slb-r-fb} which shows $P(U)$ as a function of $\lambda$ for \dtname{LastFM} and \dtname{Facebook}.
% Let us first observe the variation of $P_c$ with $\lambda$ for \dtname{Facebook} dataset as shown in Figure~\ref{fig:slb-r-fb}.
For \dtname{LastFM} dataset, we see that both \algdenseg and \algdensec achieve  $P(U) = 1$ at $\lambda = 0.1$ meaning that the returned subgraph is a clique, then the $P(U)$ value drops drastically to $0.5$ at $\lambda = 0.2$ further deviating from a clique, and stays almost constant afterwards. 
Moreover, \alggreedyfastest produces a clique until $\lambda \leq  0.4$ then it drops to $P(U) = 0.49$ as it approaches $\lambda = 0.8$. %Next we observe that $P(U)$ stays below $0.2$ when $\lambda > 0.6$. 
Nevertheless, \alglpstc takes a small downward slope until $\lambda = 0.6$ and then stays constant afterwards without producing any cliques.

For \dtname{Facebook}, \alggreedyfastest obtains $P(U) = 1$ when $\lambda \leq 0.4$ which implies that the returned solution is a clique.
Moreover, we see that the returned solution gradually deviates away from a clique until $\lambda = 0.6$ and stays constant beyond.
Next, we see that the $P(U)$ values of both \algdenseg and \algdensec coincide with each other, and gradually decrease during $\lambda \leq 0.6$ and stay constant afterwards.  In contrast, \alglpstc shows only a slight downward slope when $\lambda$ is increased.


Finally, we observe the scores as a function of $\lambda$ as shown in Figure~\ref{fig:sla-r} and \ref{fig:sla-r-fb}. 
% As expected all $4$ algorithms generally show an increasing trend as $\lambda$ increases. 
For \dtname{LastFM} dataset, we first see that all algorithms show an increasing trend over $\lambda$ as expected.
For lower $\lambda$s, \alggreedyfastest produces the best scores out of four algorithms, and then \alglpstc outperforms as $\lambda$ increases beyond $0.4$.

For \dtname{Facebook} dataset, we observe that the scores of \algdenseg and \algdensec overlap with each other creating the same increasing trend. Furthermore, \alglpstc always produces the highest score out of all algorithms producing an linearly increasing trend.
Finally we see that \alggreedyfastest slightly outperforms two other algorithms when $\lambda \leq 0.4$ and afterwards it follows \algdenseg and \algdensec.
 


% \begin{figure}[h!]
% \begin{subcaptiongroup}
% \phantomcaption\label{fig:sla}
% \phantomcaption\label{fig:slb}
% \begin{center}
% \begin{tabular}{ll}
% \begin{tikzpicture}
% \begin{axis}[xlabel={$\epsilon$}, ylabel= {$\score{\cdot; \lambda}$},
%     width = 4.5cm,
%     height = 3.8cm,
%     xmin = 0,
%     xmax = 1,
%     ymin = 2,
%     ymax = 4,
%     scaled y ticks = true,
%     cycle list name=yaf,
%     yticklabel style={/pgf/number format/fixed},
%     no markers,
%     legend entries = {\dtname{Cora}, \dtname{Citeseer}},
%     % legend pos = south east
%     legend style={at={(1,.5)},anchor=north east} 
% ]
% \addplot table [x=x, y=s1, col sep=comma] {eps.csv};
% \addplot table [x=x, y=s2, col sep=comma] {eps.csv};
% \pgfplotsextra{\yafdrawaxis{0}{1}{2}{4}}
% \end{axis}
% \node[anchor=north east] at (0, -0.3) {(a)};
% \end{tikzpicture}&
% \begin{tikzpicture}
% \begin{axis}[xlabel={$\epsilon$}, ylabel= {$t$},
%     width = 4.5cm,
%     height = 3.8cm,
%     xmin = 0,
%     xmax = 1,
%     ymin = 0,
%     ymax = 500,
%     scaled y ticks = true,
%     cycle list name=yaf,
%     yticklabel style={/pgf/number format/fixed},
%     no markers,
%     legend entries = {\dtname{Cora}, \dtname{Citeseer}},
%     % legend pos =  south east
%     legend style={at={(1,.5)},anchor=north east} 
% ]
% \addplot table [x=x, y=t1, col sep=comma] {eps.csv};
% \addplot table [x=x, y=t2, col sep=comma] {eps.csv};

% \pgfplotsextra{\yafdrawaxis{0}{1}{0}{500}}
% \end{axis}
% \node[anchor=north east] at (0, -0.3) {(b)};
% \end{tikzpicture}

% \end{tabular}
% \end{center}
% \end{subcaptiongroup}
% \caption{Scores and time as a function of $\epsilon$ with $\lambda = 0.7$ using \algip for \dtname{Cora} and \dtname{Citeseer} dataset .
% }
% \label{fig:eps}
% \end{figure}

\fi