\section{Experimental evaluation}\label{sec:exp}

\input{stats-table-real-results}
\input{stats-table-real-data}

Next, we evaluate our
algorithms experimentally. We first generate a synthetic dataset with a dense subgraph component and test how well our algorithms perform.
Next, we study the performance of the algorithms on real-world networks.
We implemented the algorithms in Python\footnote{The source code is available at \url{https://version.helsinki.fi/dacs/}.
\label{foot:code}} and performed the experiments using a 2.4GHz Intel Core i5 processor and 16GB RAM. In our experimental evaluation, we used Gurobi solver in Python to solve the ILPs and LPs associated with \algip and \alglpstc respectively.

\textbf{Synthetic dataset}:
We will now explain how the synthetic dataset was generated.
Given a vertex set $V$ of size 230, we split $V$ into dense and sparse components $D$ and $S$. Here, we randomly selected $D$ to have 38 nodes and $S$ to have 192 nodes.
We sampled the edges using a stochastic block model, with the edge probabilities
being $p_d = 1$, $p_s = 0.3$, and $p_c = 0.05$ for dense component, sparse component, and cross edges, respectively. 
The resulting graph had $5\,197$ edges, and the wedge graph had $5\,197$ nodes and $179\,100$ edges. The density of $D$ was $\abs{E(D)} / \abs{D} = 18.5$.


\textbf{Results using synthetic dataset}:
We report our results in Table~\ref{tab:real-result}. First, we see that all our algorithms find the ground truth by achieving a score of $18.5$ which is the density of our planted clique of size $38$ for example when $\lambda = 0.4$ and $\lambda = 0.2$. Note that \algip produced the results within an hour only for the $\lambda = 0.2$ case. Since \algip solves an ILP in each round, it was inefficient to run for the other $\lambda$ values and we stopped the execution after one hour.

As $\lambda$ increases, our algorithms tend to find a score greater than $18.5$ by deviating away from the planted clique. We also see that \alglpstc which solves a linear program runs significantly slower than \alggreedyfastest, \algdensec, and \algdenseg algorithms. 

Next, we study how the scores and the percentage of weak edges vary as a function of $\lambda$ as shown in Figure~\ref{fig:lam-q-syn}. We can observe that both \algdenseg and \algdensec produce equal scores whereas \algdensec and \alglpstc slightly underperform at $\lambda = 0.6$ and $\lambda = 0.5$ respectively as shown in Fig.~\ref{fig:sla}. Moreover, the \alglpstc slightly outperforms other algorithms when $\lambda \geq 0.7$. In terms of percentages of weak edges, all three algorithms produced the same decreasing trends according to Fig.~\ref{fig:slb}. 
There are no weak edges in the subgraphs produced by any of the algorithms when $\lambda \leq 0.4$ since scores are only contributed by the planted clique. Recall the connection to the maximum clique problem for $\lambda=0$ from Proposition~\ref{prop:np_hardness}.

Finally, we study the running time as a function of the number of edges $\abs{E}$ and the number of wedges $\abs{V(Z)}$ in Figure~\ref{fig:e-w-t}. We randomly generated $6$  datasets each with $5\,000$ nodes. The number of edges of the datasets uniformly ranges from $1 \times 10^4$ to $1.1 \times 10^5$.
We see that \algdenseg and \algdensec are the fastest while \alglpstc is the slowest.

\begin{figure}[t!]
\begin{subcaptiongroup}
\phantomcaption\label{fig:sla}
\phantomcaption\label{fig:slb}
\begin{center}
\begin{tabular}{ll}
\begin{tikzpicture}
\begin{axis}[xlabel={$\lambda$}, ylabel= {$\score{\cdot; \lambda}$},
    width = 4.5cm,
    height = 3.8cm,
    xmin = 0,
    xmax = 1,
    ymin = 15,
    ymax = 40,
    scaled y ticks = true,
    cycle list name=yaf,
    yticklabel style={/pgf/number format/fixed},
    no markers,
    legend entries = {\alggreedyfastest, \algdenseg, \algdensec,\alglpstc},
    legend pos = north west
]
\addplot table [x=x, y=y1, col sep=comma] {syn_lam.csv};
\addplot table [x=x, y=y2, col sep=comma] {syn_lam.csv};
\addplot table [x=x, y=y3, col sep=comma] {syn_lam.csv};
\addplot table [x=x, y=y4, col sep=comma] {syn_lam.csv};
\pgfplotsextra{\yafdrawaxis{0}{1}{15}{40}}
\end{axis}
\node[anchor=north east] at (0, -0.3) {(a)};
\end{tikzpicture}&
\begin{tikzpicture}
\begin{axis}[xlabel={$\lambda$}, ylabel= {$\abs{E_s}\%$},
    width = 4.5cm,
    height = 3.8cm,
    xmin = 0,
    xmax = 1,
    ymin = 0,
    ymax = 100,
    scaled y ticks = true,
    cycle list name=yaf,
    yticklabel style={/pgf/number format/fixed},
    no markers
]
\addplot table [x=x, y=p1, col sep=comma] {syn_lam.csv};
\addplot table [x=x, y=p2, col sep=comma] {syn_lam.csv};
\addplot table [x=x, y=p3, col sep=comma] {syn_lam.csv};
\addplot table [x=x, y=p4, col sep=comma] {syn_lam.csv};
\pgfplotsextra{\yafdrawaxis{0}{1}{0}{100}}
\end{axis}
\node[anchor=north east] at (0, -0.3) {(b)};
\end{tikzpicture}

\end{tabular}
\end{center}
\end{subcaptiongroup}
\caption{Scores and percentages of strong edges as a function of $\lambda$ for \dtname{Synthetic} dataset.
}
\label{fig:lam-q-syn}
\end{figure}


\begin{figure}[t!]
\begin{subcaptiongroup}
\phantomcaption\label{fig:eta}
\phantomcaption\label{fig:wtb}
\begin{center}
\begin{tabular}{ll}
\begin{tikzpicture}
\begin{axis}[xlabel={$\abs{E}$}, ylabel= {time},
    width = 4.4cm,
    height = 3.8cm,
    xmin = 10000,
    xmax = 110000,
    ymin = 0,
    ymax = 4000,
    scaled y ticks = true,
    cycle list name=yaf,
    yticklabel style={/pgf/number format/fixed},
    no markers,
    legend entries = {\alggreedyfastest, \algdenseg, \algdensec,\alglpstc},
    legend style={at={(0.5,1.0)}}
]
\addplot table [x=x, y=y1, col sep=comma] {time.csv};
\addplot table [x=x, y=y2, col sep=comma] {time.csv};
\addplot table [x=x, y=y3, col sep=comma] {time.csv};
\addplot table [x=x, y=y4, col sep=comma] {time.csv};
\pgfplotsextra{\yafdrawaxis{10000}{110000}{0}{4000}}
\end{axis}
\node[anchor=north east] at (0, -0.3) {(a)};
\end{tikzpicture}&
\begin{tikzpicture}
\begin{axis}[xlabel={$\abs{V(Z)}$}, ylabel= {time},
    width = 4.4cm,
    height = 3.8cm,
    xmin = 38000,
    xmax = 4800000,
    ymin = 0,
    ymax = 4000,
    scaled y ticks = true,
    cycle list name=yaf,
    yticklabel style={/pgf/number format/fixed},
    no markers,
    legend pos = north west
]
\addplot table [x=x1, y=y1, col sep=comma] {time.csv};
\addplot table [x=x1, y=y2, col sep=comma] {time.csv};
\addplot table [x=x1, y=y3, col sep=comma] {time.csv};
\addplot table [x=x1, y=y4, col sep=comma] {time.csv};
\pgfplotsextra{\yafdrawaxis{38000}{4800000}{0}{4000}}
\end{axis}
\node[anchor=north east] at (0, -0.3) {(b)};
\end{tikzpicture}

\end{tabular}
\end{center}
\end{subcaptiongroup}
\caption{Time in seconds as a function of the number of edges $\abs{E}$ and the number of wedges $\abs{V(Z)}$.
}
\label{fig:e-w-t}
\end{figure}


\textbf{Real-world datasets}:
We test our algorithms in publicly available, real-world datasets:
\dtname{Cora}~\cite{nr2015}\footnote{\url{https://networkrepository.com} \label{foot:nw-repo}} and \dtname{Cite-seer}~\cite{nr2015}\footref{foot:nw-repo} datasets are citation networks.
\dtname{Email-EU} is a collaboration network between researchers.\!\footnote{\url{https://toreopsahl.com/datasets/}}
\dtname{Facebook} is extracted from a friendship circle of Facebook.\!\footnote{\url{http://snap.stanford.edu}\label{foot:snap}}
\dtname{LastFM} is a social network 
of LastFM users.\!\footref{foot:snap} 
\dtname{PGP} is an interaction network of the users of the Pretty Good Privacy (PGP) algorithm.\!\footnote{\url{http://konect.cc/networks/arenas-pgp/}}
The details of the datasets are shown in Table~\ref{tab:real-data}.

\textbf{Results using real-world datasets}:
We present the results obtained using our algorithms in Table~\ref{tab:real-result}. We compare our algorithms in terms of scores, running time, and the percentage of the strong edges within
subgraphs they returned for a set of $\lambda$ values.
Since \algip invokes a sequence of integer programs, the algorithm does not scale for large datasets. 
We stopped the experiments that took over one hour. 
We always set $\epsilon = 0.01$ when testing each dataset with \algip.

First, let us compare the scores across the algorithms.
Our first observation is that \algip always yields the highest score with the tested datasets while all other algorithms perform similarly in terms of scores: in most cases, they produce approximately equal scores.
When \algip is not usable, \alglpstc has produced the maximum score except for $3$ outlier cases where \alggreedyfastest and \algdensec algorithms obtained the maximum score  $2$ times and $1$ time respectively. Nevertheless, for high $\lambda$s all of them produce less deviated scores when compared to lower $\lambda$s.
As expected, $\score{}$ increases as $\lambda$ increases for all $4$ algorithms. 

Next, let us look at column $\abs{E_s} \%$ which gives the percentages of strong edges in the returned subgraph.
Generally speaking, $\abs{E_s}\%$ monotonically decreases as $\lambda$ increases except for a few outlier cases. This is because when we assign a higher weight $\lambda$, it becomes more beneficial to include more weak edges.

Computation times are given in the $time$ columns of Table~\ref{tab:real-result}. 
\alggreedyfastest, \alglpstc, and \algdenseg run significantly slower than \algdensec.  
If we compare \alggreedyfastest and \algdenseg, for all the tested cases \algdenseg runs faster despite having to solve a sequence of minimum cuts. This is due to the implementation differences as \algdenseg uses a fast native library to compute the minimum cuts.

Despite \algip not being scalable for larger datasets, it runs faster than \alggreedyfastest except for four cases with the tested datasets.
For \dtname{Cora}, \dtname{Citeseer}, \dtname{PGP}, and \dtname{LastFM}  datasets, \alglpstc runs faster than all other algorithms except \algdensec.  However, for the two other remaining datasets, 
% in Table~\ref{tab:real-result} 
\alglpstc is the slowest in comparison to the other three algorithms.
We see that the running times are still reasonable in practice for the tested datasets; for example, we were able to compute the subgraph for the \dtname{Facebook}
dataset, with over $30\,000$ edges and $1\, 000 \,000$ wedges, in under ten minutes.

\begin{table}[ht!]
\caption{Co-authorship case-study for \dtname{DBLP} dataset with weighted variant of \algip. We set $\lambda = 0.8$ and $\epsilon = 0.01$. For each subgraph, we state the scores within brackets.
}
\label{tab:dblp-lam-ip-w}
\begin{tabular*}{\columnwidth} { l p{7.4cm}}
\toprule
S1 & P. S.Yu, C. C.Aggarwal, J.Han, W.Fan, J.Gao, X.Kong~(6.00)  \\
S2 & C. H. Q.Ding, F.Nie, H.Huang, D.Luo~(4.78) \\
S3 & S.Yan, J.Yan, N.Liu, Z.Chen, H.Xiong, Q.Yang, Y.Fu, Y.Ge, H.Zhu, E.Chen, C.Liu, Q.Liu, B.Zhang~(4.70)\\
S4 & S.Lin, H.Hsieh, C.Li~(4.23)\\
S5 & C.Faloutsos, J.Sun, S.Papadimitriou, H.Tong, L.Akoglu, T.Eliassi-Rad, B.Gallagher~(4.09)\\
S6 & Y.Liu, M.Zhang, S.Ma, L.Ru~(3.84)\\
S7 & H.Liu, J.Tang, X.Hu, H.Gao~(3.80)\\
S8 & D.Phung, S.Venkatesh, S. KumarGupta, S.Rana, S.Tsumoto, S.Hirano~ (3.76)\\
S9 & C.Böhm, I. S.Dhillon, C.Plant, C.Hsieh, P.Ravikumar~(3.54)\\
S10 & S.Günnemann, H.Kremer, T.Seidl, I.Assent, E.Müller, R.Krieger~(3.46) \\
\bottomrule
\end{tabular*}
\end{table}

\textbf{Case study}: Next, we conducted a case study for \dtname{DBLP}
~\cite{tang2008arnetminer}\footnote{\url{https://www.aminer.org/citation}} which contains co-authorship connections from top venues in data mining and machine learning~(SDM, NIPS, ICDM, KDD, ECMLPKDD, and WWW). Each node represents an author and each edge corresponds to a collaboration between two authors.
We removed the author pairs who have less than $3$ collaborations. The size of the dataset after prepossessing is $n = 4\,592$, $m =  5\,566$, and $\abs{Z(G)} =  26\,073$. To compute a marginal weight that corresponds to an author pair, we assign a weight for each paper as one divided by the number of authors. We then weigh each edge~(author-pair) by summing up the weights of all respective collaborations.
Then we ran a weighted version of \algip whose objective is to maximize the edge-weighted score,
\[
\frac{\sum_{\text{strong } e \in E(U)}w(e) + \lambda \sum_{\text{weak } e \in E(U)}w(e)}{\abs{U}}\quad. 
\]
We found top-10 non-overlapping subgraphs iteratively by deleting the returned subgraph in each iteration and then considering the remaining graph to find the next subgraph.
We set $\lambda = 0.8$ and $\epsilon = 0.01$. The list of author subgraphs is shown in Table~\ref{tab:dblp-lam-ip-w}. 
We see that the variant of \algip discovered subgraphs of prolific authors. 
