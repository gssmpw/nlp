\section{Experiments}

% \subsection{EXPERIMENTAL SETUP}
\subsection{Experimental setup}

\noindent {\bf Datasets.}
% We evaluate our method on two types of 360° unbounded environment datasets:
% \begin{itemize}
%     \item \textbf{360-USID (Ours)}: We introduce a new dataset specifically for evaluating 360° unbounded scene inpainting. It comprises 7 scenes (3 indoor, 4 outdoor), each with 200-300 training views containing the object to be removed, about 30 test views without the object, and 1 reference image corresponding to a training view for reference-based inpainting methods. This dataset provides ground truth for quantitative evaluation of 360° inpainting tasks. We maintain the width at 960 pixels when evaluating 360-USID to preserve high-fidelity details crucial for 360° scene representation.
%     \item \textbf{MipNeRF-360~\citep{barron2022mip}}: We use these 360° datasets to demonstrate our method's performance on unbounded scenes, evaluating at 1/4 resolution for efficiency. Although lacking ground truth for inpainting, the datasets are useful for qualitative assessments and show our method's generalization. The image at index 0 is used as the reference view for all other reference-based methods.
% \end{itemize}
We evaluate on two 360° unbounded scene datasets:
(1) \textbf{360-USID (Ours)}: A new dataset of 7 scenes (3 indoor, 4 outdoor) for evaluating 360° inpainting, with 200-300 training views containing objects, around 30 test views without objects, and 1 reference. All images are processed at 960px width to preserve details for quantitative evaluation.
(2) \textbf{Other-360~\citep{barron2022mip}} We collect additional 6 standard 360° unbounded scene datasets from NeRF\cite{mildenhall2020nerf}, MipNeRF-360\citep{barron2022mip} and Instruct-NeRF2NeRF\cite{haque2023instruct} for qualitative evaluation at 1/4 resolution, with frame 0 as reference for all methods.

    % 我們使用它提供的reference view image(用腳架拍攝的那張)來作為我們所以reference=based inpainting method的reference image

% \yulunliu{InNeRF360.} % 要寫清楚他的 github repo 跑不動，而且我們有寄信給作者索要 executable 但沒有下文
% \yulunliu{reference-guided controllable inpainting.} % 沒有 release code

% To evaluate our 360$^{\circ}$ inpainting method, we employ two primary metrics that focus on the perceptual quality and realism of the inpainted scenes:
% \begin{itemize}
%     \item \textbf{LPIPS (Learned Perceptual Image Patch Similarity)~\citep{zhang2018unreasonable}}: This perceptual metric measures the similarity between the inpainted renderings and ground-truth images. Lower values indicate better perceptual similarity.
%     \item \textbf{PSNR (Peak Signal-to-Noise Ratio)}: This metric quantifies the ratio between the maximum possible signal power and the power of distorting noise. Higher PSNR values indicate better quality of the inpainted regions, with less deviation from the ground truth.
% \end{itemize}
% For both LPIPS and PSNR, we compute the metrics only within the object mask, which is obtained from our rendered object mask. This approach, similar to that used in SPIn-NeRF~\citep{spinnerf}, allows us to focus specifically on the quality of the inpainting rather than the overall scene reconstruction.
% For the 360-USID dataset, where we have ground-truth images without the removed objects, we compute both LPIPS and PSNR. For MipNeRF-360, which lack ground truth for inpainting, we rely on qualitative assessments.
% We provide additional evaluation results in the supplementary materials for a more comprehensive analysis.

\vspace{3pt}
\noindent {\bf Metrics.}
We evaluate our method using two complementary metrics: LPIPS (Learned Perceptual Image Patch Similarity)~\citep{zhang2018unreasonable} for perceptual quality and PSNR (Peak Signal-to-Noise Ratio) for reconstruction accuracy. Following SPIn-NeRF~\citep{spinnerf}, we compute these metrics only within object masks to focus on inpainting quality. While both metrics are used for 360-USID, which has ground truth, only qualitative assessment is possible for Other-360. Additional evaluation results are provided in supplementary materials.

%%%% Quantitative Table Start %%%%
\begin{table*}[t]
\centering
\caption{\textbf{Quantitative comparison of 360° inpainting methods on the 360-USID dataset.} \textcolor{red}{Red} text indicates the best, and \textcolor{blue}{blue} text indicates the second-best performing method.}
\label{tab:quantitative}
\vspace{-3mm}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|ccccccc|c}
\toprule
PSNR $\uparrow$ / LPIPS $\downarrow$ & Carton & Cone & Cookie & Newcone & Plant & Skateboard & Sunflower & Average \\
\midrule
SPIn-NeRF~\cite{spinnerf} & 16.659 / 0.539 & 15.438 / 0.389 & 11.879 / 0.521 & \textcolor{blue}{17.131} / 0.519 & 16.850 / 0.401 & 15.645 / 0.675 & 23.538 / 0.206 & \textcolor{blue}{16.734} / 0.464 \\
2DGS~\cite{huang20242d} + LaMa~\cite{lama} & 16.433 / 0.499 & 15.591 / \textcolor{blue}{0.351} & 11.711 / 0.538 & 16.598 / 0.670 & 14.491 / 0.564 & 15.520 / 0.639 & 23.024 / 0.194 & 16.195 / 0.494 \\
2DGS~\cite{huang20242d} + LeftRefill~\cite{cao2024leftrefill} & 15.157 / 0.567 & \textcolor{red}{16.143} / 0.372 & \textcolor{blue}{12.458} / 0.526 & 16.717 / 0.677 & 12.856 / 0.666 & \textcolor{blue}{16.429} / 0.634 & 24.216 / 0.181 & 16.282 / 0.518 \\
LeftRefill~\cite{cao2024leftrefill} & 14.667 / 0.560 & 14.933 / 0.380 & 11.148 / 0.519 & 16.264 / \textcolor{blue}{0.448} & 16.183 / 0.463 & 14.912 / \textcolor{blue}{0.572} & 18.851 / 0.331 & 15.280 / 0.468 \\
Gaussian Grouping~\cite{ye2023gaussian} & \textcolor{blue}{16.695} / \textcolor{blue}{0.502} & 14.549 / 0.366 & 11.564 / 0.731 & 16.745 / 0.533 & 16.175 / 0.440 & 16.002 / 0.577 & 20.787 / 0.209 & 16.074 / 0.480 \\
GScream~\cite{wang2024gscream} & 14.609 / 0.587 & 14.655 / 0.476 & 12.733 / \textcolor{red}{0.429} & 13.662 / 0.605 & 16.238 / 0.437 & 12.941 / 0.626 & 18.470 / 0.436 & 14.758 / 0.514 \\

Infusion~\cite{liu2024infusion} & 14.191 / 0.555 & 14.163 / 0.439 & 12.051 / 0.486 & 9.562 / 0.624 & 16.127 / 0.406 & 13.624 / 0.638 & 21.195 / 0.238 & 14.416 / 0.484 \\
AuraFusion360 (Ours) w/o SDEdit & 13.731 / 0.477 & 14.260 / 0.390 & 12.332 / 0.445 & 16.646 / 0.460 & \textcolor{blue}{17.609} / \textcolor{red}{0.319} & 15.107 / 0.580 & \textcolor{blue}{24.884} / \textcolor{red}{0.170} & 16.367 / \textcolor{blue}{0.406} \\
AuraFusion360 (Ours) & \textcolor{red}{17.675} / \textcolor{red}{0.473} & \textcolor{blue}{15.626} / \textcolor{red}{0.332} & \textcolor{red}{12.841} / \textcolor{blue}{0.434} & \textcolor{red}{17.536} / \textcolor{red}{0.426} & \textcolor{red}{18.001} / \textcolor{blue}{0.322} & \textcolor{red}{17.007} / \textcolor{red}{0.559} & \textcolor{red}{24.943} / \textcolor{blue}{0.173} & \textcolor{red}{17.661} / \textcolor{red}{0.388} \\
% AuraFusion360 (Ours) & \textcolor{red}{17.556} / \textcolor{red}{0.465} & 15.387 / 0.337 & 12.906 / 0.432 & 17.559 / 0.429 & 17.989 / 0.321 & 17.109 / 0.542 & 24.907 / 0.174 \\



% \textbf{Method} & 0 / 0 &  0 / 0 & 0 / 0 & 0 / 0 & \ 0 / 0 & 0 / 0 & 0 / 0 & 0 / 0 \\
\bottomrule
\end{tabular}%
}
\end{table*}
%%%% Quantitative Table End %%%%


% %%%% Visual Result %%%%
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1\linewidth]{figures/visual.pdf}
%     \vspace{-6mm}
%     \caption{\textbf{Qualitative comparison of 360° inpainting methods on the 360-USID dataset.}}
%     \label{fig:visual}
% \end{figure}
% %%%% Visual Result %%%%

%%%% Vosiual Result Figs Start %%%%
% \twocolumn[{%
% \renewcommand\twocolumn[1][]{#1}%
% \maketitle
% \begin{center}
% \centering
% \captionsetup{type=figure}
% \resizebox{1.0\textwidth}{!} 
% {
% % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
% \includegraphics[width=\textwidth]{figs/visual_ours.pdf}
% }
% \vspace{-6mm}
% \caption{\textbf{Visual Comparison on 360-USID dataset.}}
% \label{fig:visual}
% \end{center}
% }]

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/visual_ours_compressed.pdf}
    \vspace{-9mm}
    \caption{\textbf{Visual Comparison on our 360-USID dataset.} We compare our method against state-of-the-art approaches including Gaussian Grouping~\citep{ye2023gaussian}, 2DGS + LeftRefill, and Infusion~\citep{liu2024infusion}. While Gaussian Grouping struggles with misidentifying unseen regions, leading to floating artifacts, and 2DGS + LeftRefill faces view consistency issues, our method successfully maintains geometric consistency and preserves fine details across different viewpoints. Ground truth (GT) is shown for reference, and the original scene with an object is provided in the first row for comparison.}
    \label{fig:visual}
\end{figure*}
%  我們的方法可以準確的detect unseen region, 並利用Adptive guided depth diffusion姐和SDEdit增加guided-inpainted RGB images的multi-view consistent, v
%%%% Vosiual Result Figs End%%%%

\subsection{Comparisons with State-of-the-Art Methods}

\noindent {\bf Quantitative comparisons.}
We evaluate AuraFusion360 against state-of-the-art approaches on the 360-USID dataset. \cref{tab:quantitative} shows PSNR and LPIPS scores across different scenes. Our method consistently outperforms existing approaches. SPIn-NeRF~\cite{spinnerf}\footnotemark and Infusion~\cite{liu2024infusion} struggle with 360° consistency, while Gaussian Grouping~\citep{ye2023gaussian} misidentifies the unseen region, causing significant floating artifacts. GScream~\cite{wang2024gscream} fails to properly remove objects, and LeftRefill~\cite{cao2024leftrefill} improves but still falls short in 360° environments. 2DGS + LaMa~\citep{lama} and 2DGS + LeftRefill outperform 2D methods but face view consistency challenges. Our method achieves the highest PSNR score and the lowest average LPIPS, indicating superior perceptual quality and better similarity to the ground truth. The performance gap is especially noticeable in scenes with complex geometry or large removed objects, demonstrating our method's ability to leverage multi-view information and maintain 360° consistency. The code for InNeRF360~\cite{wang2023inpaintnerf360} could not be successfully executed, and~\cite{mirzaei2023reference} did not provide code, so we were unable to compare our method with theirs.
\footnotetext{We implement SPin-NeRF's method on the 2D Gaussian Splatting codebase to extend its capabilities to 360° unbounded scenes.}
% 我們用2d




% \vspace{-1mm}

\vspace{3pt}
\noindent {\bf Qualitative visual comparisons.}
% \jayinnn{TODO: pleaes modify this paragraph}
\cref{fig:visual} compares our AuraFusion360 method against state-of-the-art approaches on challenging scenes from the 360-USID dataset. Our method excels in maintaining view consistency and preserving fine details in 360° unbounded environments. Additional qualitative results on other 360 datasets and failure cases are provided in the supplementary material. 
% While Gaussian Grouping and LeftRefill show strengths in object removal and 2D inpainting, respectively, they struggle with 360° scene consistency. 3DGS + LaMa and 3DGS + LeftRefill improve upon 2D methods but face challenges with complex geometries and large inpainted regions.
% 360-InpaintR consistently produces sharper, more detailed, and view-consistent results across all scenes, effectively handling challenging cases like periodic textures and complex organic structures. It preserves fine details, overall scene structure, and view-dependent effects crucial for 360° scene realism, particularly in varying lighting conditions or reflective surfaces.
% We provide additional video results in our supplementary materials.
% \chunghow{Infusion additional comments} %我們用infusion official inplement的code, 他數字會這麼低有一部分是因為他的removal方法容易讓他清不乾淨
% \chunghow{SPIn-NeRF additional comments} %SPIN-NeRF我們將他的方法inplement再2DGS上 lpips loss & depth_inpainting loss












%%%% Ablation Table Start %%%%
\begin{table}[t]
\centering
\small
% \setlength{\tabcolsep}{2pt}
% \renewcommand{\arraystretch}{0.8}
\caption{\textbf{Ablation study of our AuraFusion360.}
} % 1. 跟直接用SAM2 click prompt比較, 2. 跟GDD然後他做leftrefill 0.85, 3. SDEdit 0.5 0.85 1.0比較
\label{tab:ablation}
% \vspace{-3mm}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{ccc|cccc}
% \toprule
% Unseen mask & Depth init. & 2D inpaint & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & FID $\downarrow$ \\
% \midrule
% - & \checkmark & LeftRefill & 31.835 & 0.973 & 0.022 & 181.177 \\
% \checkmark & - & LeftRefill & \textbf{32.081} & \textbf{0.975} & 0.020 & 139.511 \\
% \checkmark & \checkmark & LaMa & 31.586 & 0.974 & 0.020 & 179.912 \\
% \checkmark & \checkmark & LeftRefill & 31.715 & 0.974 & \textbf{0.019} & \textbf{134.268} \\
% \bottomrule
% \resizebox{\columnwidth}{!}{%
\vspace{-3mm}
\begin{tabular}{cc|cc}
\toprule
Depth init. & SDEdit strength & PSNR $\uparrow$ & LPIPS $\downarrow$ \\
 (\cref{sec:depth_initial}) & (\cref{sec:sdedit}) &  &  \\
\midrule
  & 0.85 &  16.638 & 0.456\\
\checkmark  & 0.5 & 17.646 & 0.393 \\
\checkmark  & 1.0 & 17.512 & 0.391 \\
\checkmark  & 0.85 & \textbf{17.661} & \textbf{0.388} \\
\bottomrule
\end{tabular}%
% }
\end{table}


% SDEdit 0.5 17.646 0.393
% SDEdit 0.85: 17.661, 0.388
% SDEdit 1.0: 17.512, 0.391


%%%% Ablation Table End %%%%




%%%% Ablation Visual Start %%%%
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1\linewidth]{figures/ablation_visual.pdf}
%     \vspace{-6mm}
%     \caption{\textbf{Qualitative comparisons of ablation studies.}}
%     \label{fig:ablation_visual}
% \end{figure}
%%%% Ablation Visual End %%%%

\subsection{Ablation Studies}
To evaluate the effectiveness of each component in our AuraFusion360 method, we conduct a series of ablation studies. \cref{tab:ablation} presents the quantitative results of these studies. %, and~\cref{tab:quantitative} shows qualitative comparisons.

% \vspace{-3mm}

\vspace{3pt}
\noindent {\bf Unseen mask generation.}
We compared our unseen mask generation method with SAM2~\cite{ravi2024sam2} and Gaussian Grouping~\cite{ye2023gaussian} tracker in ~\cref{fig:ablation_unseen} and ~\cref{fig:visual_unseen}. Our approach significantly improves inpainting quality, particularly in areas occluded from multiple views. The unseen masks identify truly occluded regions, leading to more accurate and consistent inpainting results. This is especially noticeable in scenes with complex geometries, where object masks alone may not capture all necessary information for effective inpainting.
% 我們compared 我們的unseen mask generation 方法和SAM2以及Guassian Grouping video tracker
% \yulunliu{(ours vs. Gaussian Grouping, direct SAM2)}?

%%%% Unseen Mask Gen Visual Ablation Figs Start %%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/ablation_unseen.pdf}
    \vspace{-6mm}
    \caption{\textbf{Visual comparison of unseen mask generation method.} Our method enables SAM2~\cite{ravi2024sam2} to generate more accurate predictions for each view without the need for manually provided prompts, as the bounding box prompts are automatically generated through depth warping.} %我們的方法可以讓SAM2在每個view都更正確的提出, 同時不需要手動給prompt, 可以由depth warping自動產生bbox prompt}
    \label{fig:ablation_unseen}
\end{figure}
%%%% Unseen Mask Gen Visual Ablation Figs End %%%%

%%%% Unseen Mask Gen Visual Figs Start %%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/visual_unseen.pdf}
    \vspace{-6mm}
    \caption{
    \textbf{Compared Unseen Mask w/ Gaussian Grouping.} 
    Gaussian Grouping~\cite{ye2023gaussian} uses a video tracker~\cite{cheng2023tracking} and the ``black blurry hole'' prompt for DEVA~\cite{cheng2023tracking} to track the unseen region. However, this can result in tracking errors, affecting inpainting. In contrast, our geometry-based approach uses depth warping to estimate the unseen region's contour, reducing segmentation errors.
    } %Gaussian Grouping用video segmentation approach (DEVA) 來track unseen region, 並用"black blury hole" 來當作Grounded Segment Anything 方法identify unseen region的text-prompt, 這容易導致他track錯東西進而影響inpainting 結果。然而我們是很geometry-based的方式去根據depth warping來計算出unseen region大概的輪廓, 
    \label{fig:visual_unseen}
\end{figure}
%%%% Unseen Mask Gen Visual Figs End %%%%

%%%% AGDD Ablation Start %%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/depth_alignmen2_compressed.pdf}
    \vspace{-6mm}
    \caption{\textbf{Compared to other depth completion methods.} The depth completion model in Infusion~\cite{liu2024infusion} (a) performs better at depth alignment compared to traditional methods (b) and (c), but it produces noisy depth in unseen regions. Similarly, (d) Guided Depth Diffusion~\cite{yu2024wonderworld} struggles to achieve precise alignment, as the background regions amplify the loss, leading to misalignment. In contrast, (e) Our AGDD effectively addresses these issues.} %
    \label{fig:ablation_depth_alignment2}
\end{figure}
% Infusion 的depth completion model(a)在align depth上雖然相較於傳統方法如(b)(c)比較好, 不過缺缺乏generlaize, 一次, (d) Guided Depth Diffusion也無法容易得到align的很精確, 因為背景區域會放大loss造成over alignment, (e) Ours AGDD 有效的解決上述情況



\vspace{3pt}
\noindent {\bf Effect of reference view initial Gaussians alignment.}
\cref{tab:ablation} and ~\cref{fig:ablation_depth_alignment2} show that our depth-aware 3DGS initialization accurately estimates aligned depth while maintaining geometric consistency in the inpainted regions. Compared to random initialization, our method produces more structurally coherent results, particularly in areas with significant depth variations. This is especially evident in scenes where the inpainted geometry needs to blend seamlessly with the existing scene structure.



%%%% AGDD Ablation End %%%%

% %%%% AGDD Ablation Start %%%%
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1\linewidth]{figs/depth_alignment.pdf}
%     \vspace{-6mm}
%     \caption{\textbf{Compared to other depth completion methods.}} %
%     \label{fig:ablation_depth_alignment}
% \end{figure}
% %%%% AGDD Ablation End %%%%




% \paragraph{Inpainting method comparison.}
% We evaluate the performance of two inpainting methods: LaMa~\citep{lama} for per-image inpainting and LeftRefill~\citep{cao2024leftrefill} for reference-guided inpainting. While both methods show improvements over baseline approaches, LeftRefill consistently outperforms LaMa in our 360° setting. This is due to LeftRefill's ability to leverage information from the reference view, leading to more consistent results across different viewpoints. However, combining either method with our full pipeline still outperforms their standalone usage.
