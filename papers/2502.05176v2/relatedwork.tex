\section{Related Work}
\label{sec:related}


% \subsection{Radiance Fields for Novel View Synthesis}
\noindent {\bf NeRF.}
Neural Radiance Fields (NeRF)\citep{mildenhall2020nerf} revolutionized novel view synthesis via differentiable volume rendering\citep{tulsiani2017mvsupervision, henzler2019platonicgan} and positional encoding~\citep{vaswani2017attentionisallyouneed, gehring2017convolutional}. NeRF models improved in efficiency~\citep{liu2020neural, Garbin_2021_ICCV, chen2024improving}, rendering quality~\citep{mipnerf, zhang2020nerf++,meuleman2023progressively}, handling dynamic scenes~\cite{liu2023robust}, and data efficiency~\citep{pixel.nerf, ibrnet, lin2024frugalnerf,su2024boostmvsnerfs}. Despite excelling at view synthesis, NeRF’s implicit representation complicates scene editing. Recent work on object manipulation~\citep{yang2021learning}, stylization~\citep{wang2023nerf, haque2023instruct}, and inpainting~\citep{nerf.in, spinnerf, mirzaei2023reference} struggles with 3D consistency and structural priors, especially in unbounded scenes.
% Neural Radiance Fields (NeRF)~\citep{original.nerf} revolutionized novel view synthesis, enabling photorealistic scene reconstruction via differentiable volume rendering~\citep{tulsiani2017mvsupervision, henzler2019platonicgan} and positional encoding~\citep{vaswani2017attentionisallyouneed, gehring2017convolutional}. NeRF-based models have since improved in efficiency~\citep{liu2020neural, Garbin_2021_ICCV, chen2024improving}, rendering quality~\citep{mipnerf, zhang2020nerf++,su2024boostmvsnerfs}, and data efficiency~\citep{pixel.nerf, ibrnet, lin2024frugalnerf}. While NeRF excels in view synthesis, its implicit volumetric representation complicates scene editing. Recent works on object manipulation~\citep{yang2021learning}, stylization~\citep{wang2023nerf, haque2023instruct}, and inpainting~\citep{nerf.in, spinnerf, mirzaei2023reference} face challenges in 3D inpainting in unbounded environments, as NeRF struggles with 3D consistency and leveraging explicit structural priors.


\vspace{3pt}
\noindent {\bf 3D Gaussian Splatting.}
3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} efficiently represents scenes with explicit 3D Gaussians, enabling faster rendering, easier training, and flexible editing\citep{chen2024gaussianeditor}. Recent extensions like Scaffold-GS~\citep{scaffoldgs} enhance efficiency with dynamic anchors, while 2DGS~\citep{huang20242d} refines multi-view geometry. 3DGS has also expanded to dynamic scenes~\citep{yang2024deformable, luiten2023dynamic, Wu_2024_CVPR,fan2025spectromotion} and semantic representations~\citep{ye2023gaussian, qin2023langsplat}, supporting advanced editing and novel view synthesis~\citep{qiu-2024-featuresplatting, huang20242d}. Gaussian-based methods thus offer strong potential for explicit 3D inpainting.
% 3D Gaussian Splatting (3DGS)~\citep{kerbl20233d} is an efficient alternative to NeRF, representing scenes with explicit 3D Gaussians for faster rendering, easier training, and more flexible scene editing~\citep{chen2024gaussianeditor}. Recent extensions include Scaffold-GS~\citep{scaffoldgs}, which improves rendering efficiency with dynamic anchor points, and 2DGS~\citep{Huang2DGS2024}, which refines multi-view reconstructions for view-consistent geometry. 3DGS has also been extended to dynamic environments~\citep{yang2024deformable, luiten2023dynamic, Wu_2024_CVPR} and semantic-aware representations~\citep{ye2023gaussian, qin2023langsplat}, advancing scene manipulation and novel view synthesis~\citep{qiu-2024-featuresplatting, huang20242d}. These advancements highlight the potential of Gaussian-based representations for explicit scene editing, making them well-suited for 3D inpainting tasks.

% \subsection{2D Image Inpainting}
% \paragraph{Traditional methods.}
% Image inpainting has evolved from early PDE-based techniques \citep{bertalmio2000image} to exemplar-based methods \citep{criminisi2004region}. Texture synthesis \citep{efros1999texture} and patch-based approaches like PatchMatch \citep{barnes2009patchmatch} further advanced the field. Despite limitations with large missing regions and complex textures \citep{jam2021comprehensive,liu2018image}, these methods established principles now incorporated into learning-based approaches \citep{liu2018image,yu2019free}. Their computational efficiency remains valuable in resource-constrained scenarios \citep{jam2021comprehensive}.

% \vspace{-3mm}
% \paragraph{Deep learning-based methods.}
% Deep learning has revolutionized image inpainting, with CNNs like Context Encoders \citep{feature.learning.by.inpainting} pioneering the field. GANs \citep{gan} and models like DeepFillv2 \citep{yu2019free} further improved results. Large Mask Inpainting (LaMa) \citep{lama} addressed large missing regions. Recently, diffusion models\citep{NEURIPS2020_4c5bcfec}, particularly Stable Diffusion\citep{rombach2022high}, have demonstrated remarkable inpainting capabilities, leveraging complex data distributions\citep{dhariwal2021diffusion}. Beyond text-to-image generation, diffusion models are commonly used for image-to-image tasks, including image editing and inpainting. SDEdit\citep{meng2022sdedit} leverages diffusion models for semantic editing by injecting Gaussian noise into input images and performing iterative denoising, ensuring structural coherence while modifying visual content. To further improve image manipulation fidelity, Noise Inversion techniques such as DDIM Inversion~\citep{song2021denoising} enable precise latent code inference through deterministic reverse diffusion sampling. This approach retains finer details of the original image, making it particularly effective for manipulating real images within diffusion-based generative models. In the context of inpainting, models like SDXL-Inpainting have been developed by fine-tuning diffusion models specifically for inpainting tasks. While these methods have significantly improved image inpainting quality, Stable Diffusion-based inpainting often introduces scene-inconsistent artifacts within the inpainted regions. This challenge becomes even more pronounced when leveraging 2D diffusion priors for 3D inpainting, as it can lead to multi-view inconsistencies—a major limitation for 3D scene reconstruction~\citep{li2023diffusion}. The success of diffusion-based inpainting has inspired extensions to 3D inpainting tasks\citep{nerf.in, inpaint3d}, though adapting 2D approaches to 3D presents additional challenges, such as geometry misalignment, depth inconsistencies, and occlusion handling\citep{mirzaei2023reference}.



% \vspace{-3mm}
% \paragraph{Reference-based methods.}
% Reference-based inpainting methods \citep{zhao2022geofill} enhance traditional inpainting by incorporating visual context from reference images, improving content accuracy and consistency. LeftRefill \citep{tang2023realfill} uses a two-stage architecture with feature matching and refinement networks, enabling inpainting from different viewpoints based on reference information \citep{zhao2022geofill}. While these methods show promise in various applications \citep{jam2021comprehensive}, challenges remain in seamless integration and reference selection \citep{li2023diffusion}, particularly when views diverge significantly from the reference. The success of these approaches has also inspired 3D inpainting extensions \citep{nerf.in, inpaint3d}, though adapting to 3D introduces additional complexities \citep{mirzaei2023reference}.

% \vspace{-2mm}

% \subsection{Image Inpainting}
\vspace{3pt}
\noindent {\bf Traditional and learning-based image inpainting.}
Early image inpainting techniques, including PDE-based~\citep{bertalmio2000image}, exemplar-based~\citep{criminisi2004region}, and PatchMatch~\citep{barnes2009patchmatch}, were effective for small regions but struggled with complex textures and large gaps~\citep{jam2021comprehensive, liu2018image}. Deep learning advanced the field significantly, starting with Context Encoders~\citep{feature.learning.by.inpainting} and GAN-based methods like DeepFill~\citep{generative.inpainting, yu2019free}, improving content synthesis and coherence. Recent models such as LaMa~\citep{lama} use Fourier convolutional networks to address large masks. Diffusion models~\citep{NEURIPS2020_4c5bcfec}, notably Stable Diffusion~\citep{rombach2022high}, introduced iterative refinement capabilities, providing more flexible and structurally consistent inpainting compared to GANs~\citep{dhariwal2021diffusion}.
% Early image inpainting techniques, including PDE-based~\citep{bertalmio2000image}, exemplar-based~\citep{criminisi2004region}, and patch-based methods like PatchMatch~\citep{barnes2009patchmatch}, were effective for small missing regions but struggled with complex textures and large gaps~\citep{jam2021comprehensive, liu2018image}. Deep learning brought significant advancements, starting with CNN-based models like Context Encoders~\citep{feature.learning.by.inpainting} and GANs such as DeepFill~\citep{generative.inpainting, yu2019free}, which improved content synthesis and structural coherence. More recent models like Large Mask Inpainting (LaMa)~\citep{lama} further enhanced quality by using Fourier convolutional networks for large masked regions. The rise of diffusion models~\citep{NEURIPS2020_4c5bcfec}, notably Stable Diffusion~\citep{rombach2022high}, introduced powerful text-to-image and image-to-image capabilities, enabling more flexible and structurally consistent inpainting by iteratively refining missing regions, unlike GANs~\citep{dhariwal2021diffusion}.

% \vspace{-4mm}

\vspace{3pt}
\noindent {\bf Diffusion models for image editing and inpainting.}
Beyond direct inpainting, diffusion models are widely used for image editing. SDEdit~\citep{meng2022sdedit} injects Gaussian noise and iteratively denoises, enabling semantic edits while preserving global structure. Noise inversion techniques~\cite{mokady2022null, miyake2024negativepromptinversionfastimage}, such as DDIM Inversion~\citep{song2020denoising}, further improve editing fidelity by enabling precise latent inference through deterministic reverse diffusion.
Inpainting-specific diffusion models like SDXL-Inpainting~\cite{podell2023sdxlimprovinglatentdiffusion} enhance image reconstruction by fine-tuning Stable Diffusion. Reference-based methods~\cite{tang2023realfill}, such as LeftRefill~\citep{cao2024leftrefill}, use diffusion models for reference-guided synthesis but struggle in regions distant from reference views.
Despite advancements, Stable Diffusion-based inpainting~\cite{inpaint3d} still suffers from inconsistent artifacts in scene-dependent contexts, causing multi-view inconsistencies problematic for 3D scenes~\citep{li2023diffusion}. This motivates our use of SDEdit and DDIM Inversion to preserve structural information and ensure multi-view coherence.
% Beyond direct inpainting, diffusion models are widely used for image editing. SDEdit~\citep{meng2022sdedit} injects controlled Gaussian noise followed by iterative denoising, enabling semantic modifications while preserving global structure. To improve editing fidelity, Noise Inversion techniques~\cite{, mokady2022null, miyake2024negativepromptinversionfastimage} like DDIM Inversion~\citep{song2020denoising} allow precise latent code inference through deterministic reverse diffusion sampling. By inverting an image to a specific noise level and denoising it back, Noise-Inversion provides finer control over content preservation, making it highly effective for inpainting real images while minimizing distortion during denoising.
% % 
% Inpainting-specific diffusion models like SDXL-Inpainting~\cite{podell2023sdxlimprovinglatentdiffusion} enhance the process by fine-tuning Stable Diffusion models for image reconstruction. Reference-based method~\cite{tang2023realfill} such as LeftRefill~\citep{cao2024leftrefill} uses pre-trained diffusion models for reference-guided synthesis, stitching reference and target views to enable contextual inpainting, view synthesis, and image completion via task-specific prompt tuning. However, LeftRefill struggles in regions far from the reference view, where alignment becomes less reliable.
% % 
% Despite these advancements, Stable Diffusion-based inpainting~\cite{inpaint3d} often produces inconsistent artifacts, particularly in scene-dependent contexts. When applied to 3D inpainting, these artifacts lead to multi-view inconsistencies, a critical limitation for scene reconstruction and object removal~\citep{li2023diffusion}. This motivates our use of SDEdit and DDIM Inversion for 3D inpainting, ensuring that denoising preserves critical structural information while maintaining coherence across viewpoints.  

% Although diffusion-based inpainting has inspired 3D inpainting extensions\citep{nerf.in, inpaint3d}, adapting 2D methods to 3D introduces challenges like geometry misalignment, depth inconsistencies, and occlusion handling.



% \subsection{3D Scene Inpainting}
% \paragraph{Methods without multi-view background knowledge.}
% As 3D scene representation and reconstruction techniques have advanced, the need for 3D inpainting methods has grown. Early approaches to 3D scene inpainting often relied on single-view or limited-view information, attempting to extend 2D inpainting concepts into the 3D domain without leveraging extensive multi-view knowledge.
% One category of methods focuses on direct 3D shape completion. These approaches typically operate on point clouds or voxel representations. For instance, PCN (Point Completion Network) introduced by \citet{yuan2018pcn} uses an encoder-decoder architecture to complete partial point clouds. While effective for object-level completion, these methods often struggle with complex, large-scale scene inpainting.
% Another approach involves using 2.5D representations, where depth information is incorporated alongside RGB data. Depth-aware inpainting methods, such as the work by \citet{3d.photography}, extend 2D inpainting techniques by considering depth as an additional channel. These methods can produce more geometrically consistent results but are limited by their reliance on a single viewpoint.
% Some researchers have explored the use of generative models for 3D inpainting. 3D-GAN, proposed by \citet{wu2016learning}, generates 3D shapes from a probabilistic space, which can be adapted for inpainting tasks. However, these methods often struggle with fine details and scene-level consistency.
% In the context of neural rendering, early attempts at NeRF editing and inpainting also fall into this category. Methods like EditNeRF by \citet{liu2021editing} allow for object-level editing in NeRF scenes but are limited in their ability to handle large-scale scene modifications or inpainting of complex structures.
% Standalone NeRF inpainting methods, such as NeRF-In by \citet{nerf.in}, attempt to inpaint 3D scenes represented as Neural Radiance Fields. These approaches often rely on 2D inpainting results as supervision, projecting them back into the 3D space. While they can produce plausible results for small edits, they struggle with view consistency and large-scale modifications.
% A common limitation of these single-view or limited-view methods is their inability to fully leverage the 3D structure of the scene. They often produce results that are inconsistent across different viewpoints or fail to capture the true geometry of occluded regions \citep{spinnerf}. Additionally, these methods may struggle with understanding the global context of the scene, leading to inpainted content that doesn't align well with the overall scene structure \citep{wang2023inpaintnerf360}.
% Despite these limitations, these methods have laid important groundwork for 3D scene inpainting. They have highlighted the challenges specific to 3D inpainting, such as maintaining geometric consistency and handling occlusions, which have informed the development of more advanced, multi-view aware techniques \citep{mirzaei2023reference}.
% Existing 3D inpainting approaches~\cite{weder2022removing, spinner, nerfin} extended 2D concepts to 3D without extensive multi-view knowledge. These include direct 3D shape completion methods like PCN \citep{yuan2018pcn}, 2.5D representations \citep{3d.photography}, and generative models like 3D-GAN \citep{wu2016learning}. In the field of neural rendering, EditNeRF \citep{liu2021editing} and NeRF-In \citep{nerf.in} pioneered NeRF editing and inpainting. These methods often struggle with view consistency \citep{spinnerf} and global context \citep{wang2023inpaintnerf360}. Despite limitations, they laid groundwork for more advanced, multi-view aware techniques \citep{mirzaei2023reference}. 

% Existing 3D inpainting approaches用在NeRf上~\cite{weder2022removing, spinnerf, nerfin} 因為nerf implicit representation的特性，通常都是leverage 2d inpainter to 3d, 像spinnerf~\cite{spinnerf}用一個lpips loss來減緩muli-view inpainting 的inconcsitency. 而reference-based method, 為了要進一步解決multi-view inconsitency的問題，他們提出想要只使用少數的reference image來代表要inpaint的區域。然而他們通常只能能render的novel view角度很小, 局限在forward facing scene, 很能利用到unbounded 360 scene上。InNeRF360~\cite{wang2023inpaintnerf360}雖然可以使用在360場景，使用Hallucinating Density Removal來清除inconsistency造成的flaoters, 但卻一樣是利用將object先inpaint掉在拿去train nerf的方式, 沒辦法利用原有場景資訊。

% 而得天GaussianSplatting explict的特性, GaussianGrouping可以將semantic 資訊加到每一顆Gaussian上, InFusion [citation] approaches 3D Gaussian inpainting by leveraging depth completion and progressive reference view synthesis. While achieving efficient results, the method's limitations include manual view selection requirements and potential inaccuracies in depth completion for complex geometries, 而且他們的depth completion model需奧finetuning. GScream leverages 3D Gaussian Splatting for object removal by integrating monocular depth guidance and cross-attention feature propagation between visible and in-painted regions to achieve consistent geometry and textures. However, it's hard to extend to unbounded 360 scene 因為他一樣是reference-based method. 而我們的方法...


\vspace{3pt}
\noindent {\bf 3D scene inpainting.}
Existing 3D inpainting methods for NeRF~\cite{weder2022removing, spinnerf, shen2023nerfin, yin2023or} typically adapt 2D models to NeRF’s implicit representation. For instance, SPIn-NeRF~\cite{spinnerf} employs perceptual loss to improve multi-view consistency. Reference-based methods~\cite{mirzaei2023reference, mirzaei2024reffusionreferenceadapteddiffusion, wang2024gscream} enhance consistency using reference images but remain limited to small-angle view rendering, restricting their use in 360° scenes. NeRFiller~\cite{weber2023nerfiller} iteratively refines consistency with grid prior but struggles with fine-grained textures due to image downsampling. InNeRF360~\cite{wang2023inpaintnerf360} handles 360° scenes via density hallucination but has limited scene utilization.
% 
Gaussian Splatting-based methods like Gaussian Grouping~\cite{ye2023gaussian} inject semantic information, while InFusion~\cite{liu2024infusion} employs depth completion but requires manual view selection. GScream~\cite{scaffoldgs} integrates Scaffold-GS but faces difficulties in unbounded 360° scenes. Our method addresses these issues by enhancing multi-view consistency and depth-aware inpainting in 360° scenarios using Gaussian Splatting.
% Existing 3D inpainting approaches for NeRF~\cite{weder2022removing, spinnerf, shen2023nerfin, yin2023or} often extend 2D inpainting models into 3D due to NeRF’s implicit representation. For example, SPIn-NeRF~\cite{spinnerf} uses perceptual loss to reduce multi-view inpainting inconsistencies. Reference-based methods~\cite{mirzaei2023reference, mirzaei2024reffusionreferenceadapteddiffusion, wang2024gscream} aim to further reduce these inconsistencies by using a few reference images to represent the inpainting area. However, these methods are generally restricted to rendering novel views from small angles, making them less suitable for unbounded 360° environments. NeRFiller~\cite{weber2023nerfiller} improves multi-view consistency with Grid Prior, extends it to Joint Multi-View Inpainting, and refines missing regions iteratively via Dataset Update, ensuring 3D structural coherence without reference images or object masks. Yet, its reliance on image downsampling limits high-frequency detail reconstruction, reducing effectiveness for fine-grained textures. InNeRF360~\cite{wang2023inpaintnerf360} adapts to 360° scenes using Hallucinating Density Removal to address view inconsistencies but remains limited by its object inpainting approach before NeRF training, restricting full scene utilization.
% % 
% Gaussian Splatting enables precise inpainting, as seen in Gaussian Grouping~\cite{ye2023gaussian}, which injects semantic information into each Gaussian. InFusion~\cite{liu2024infusion} enhances 3D Gaussian inpainting with depth completion and progressive reference synthesis but is limited by manual view selection and fine-tuning. GScream integrates Scaffold-GS~\cite{scaffoldgs} for object removal, using monocular depth and cross-attention for consistency but struggles with 360° unbounded scenes due to fixed reference views. Our method addresses these challenges by improving multi-view consistency in 360° environments, leveraging Gaussian Splatting for explicit scene manipulation and depth-aware inpainting.




% Previous work such as SPIn-NeRF that integrates 2d inpainting model with perceptual loss and depth inpainting guidance to reconstruct Inpainted Neural Radiance Fields (NeRF). OR-NeRF Removing Objects From NeRF use confidence-based view selection automatically removes inconsistent views from the optimization preventing unwanted artefacts in the final result.MVIP-NeRF utilizes a multi-view approach to perform 3D inpainting on NeRF scenes by employing diffusion priors, where it jointly optimizes RGB and normal map completion through an iterative Score Distillation Sampling (SDS) process, ensuring consistent appearance and geometry alignment across multiple views while leveraging a multi-view scoring mechanism to distill generative priors from different perspectives. 然而這些方法並沒有透過leverage 已有場景資訊來進行inpainting. Reference-guided controllable inpainting, as presented by Mirzaei et al. (2023), leverages reference images and view-dependent effects to guide the inpainting process in 3D scenes, enabling consistent and visually coherent completions across multiple perspectives while handling challenges like disocclusions and geometric consistency. GScream introduces a robust framework for object removal in 3D scenes by optimizing Gaussian primitives' positions for geometric consistency and utilizing a cross-attention feature propagation mechanism to enhance texture coherence, effectively restoring both geometry and texture across visible and occluded areas. 
% Reference-guided inpainting in NeRF  

% 先nerf方法, 指出雖然..., 他們1需要非常精確的object mask來remov場景的objec, 但又inplcit-method, 所以沒辦法很好的透出場景資訊
% 在講道






% \vspace{-3mm}
% \paragraph{Methods leveraging multi-view information.}
% As the limitations of single-view 3D inpainting methods became apparent, researchers began to explore approaches that leverage multi-view information. These methods aim to produce more consistent and geometrically accurate results by utilizing the rich information available from multiple viewpoints of a scene.
% One of the pioneering works in this direction is SPIn-NeRF by \citet{spinnerf}. This method combines Neural Radiance Fields (NeRF) with multi-view image inpainting to remove objects from 3D scenes. SPIn-NeRF uses a two-stage approach: first, it inpaints each input view using a 2D inpainting method, then it optimizes a NeRF to fit these inpainted views. By leveraging multi-view consistency, SPIn-NeRF can produce more coherent results across different viewpoints than single-view methods.
% Another significant contribution in this area is the work by \citet{philip2018plane} on object removal for image-based rendering. Their method uses multi-view stereo to reconstruct the scene geometry and then performs inpainting in both color and depth spaces across multiple views. This approach demonstrates the importance of considering both appearance and geometry in multi-view 3D inpainting.
% Inpaint3D, proposed by \citet{inpaint3d}, takes a different approach by training a 3D-aware inpainting network on a large dataset of indoor scenes. This method can leverage the learned 3D priors to produce geometrically consistent inpaintings across multiple views, even for large missing regions.
% Recent advancements in NeRF-based representations have led to more sophisticated multi-view inpainting methods. For instance, InpaintNeRF360 by \citet{wang2023inpaintnerf360} extends inpainting capabilities to 360-degree scenes. This method uses a combination of 2D inpainting guidance and 3D consistency optimization to handle the challenges of inpainting in omnidirectional environments.
% Gaussian Grouping, introduced by \citet{ye2023gaussian}, presents a novel approach to 3D scene editing using 3D Gaussian Splatting. While not specifically designed for inpainting, this method demonstrates how multi-view information can be leveraged to segment and manipulate 3D scenes represented by Gaussians, opening new possibilities for 3D inpainting tasks.
% A common thread among these multi-view methods is their ability to maintain consistency across different viewpoints, a crucial aspect of 3D scene inpainting. By leveraging information from multiple views, these approaches can better understand the underlying 3D structure of the scene and produce inpainted results that are coherent with the global scene geometry \citep{mirzaei2023reference}.
% However, challenges remain. Many of these methods still struggle with large-scale occlusions or complex geometric structures \citep{weder2022removing}. The computational cost of processing multiple views can also be significant, especially for high-resolution or large-scale scenes \citep{barron2023zip}. Additionally, balancing the influence of different views and handling potential inconsistencies between them remains an active area of research \citep{yin2023or}.
% Despite these challenges, multi-view 3D inpainting methods have significantly advanced the state of the art, enabling more realistic and consistent scene editing and completion. As research progresses, we can expect to see further improvements in the quality and efficiency of these techniques, potentially leading to new applications in fields such as virtual reality, augmented reality, and digital twin technologies \citep{bommasani2021opportunities}.

% Multi-view 3D inpainting methods address the limitations of single-view approaches. SPIn-NeRF \citep{spinnerf} combines NeRF with multi-view image inpainting. \citet{philip2018plane} use multi-view stereo for object removal in image-based rendering. Inpaint3D \citep{inpaint3d} leverages learned 3D priors. InpaintNeRF360 \citep{wang2023inpaintnerf360} extends to 360-degree scenes, while Gaussian Grouping \citep{ye2023gaussian} uses 3D Gaussian Splatting. These methods maintain consistency across viewpoints \citep{mirzaei2023reference} but face challenges with large-scale occlusions \citep{weder2022removing}, computational costs \citep{barron2023zip}, and view inconsistencies \citep{yin2023or}. Despite challenges, they advance scene editing and completion, potentially leading to new applications \citep{bommasani2021opportunities}.