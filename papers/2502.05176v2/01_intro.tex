% \vspace{-1mm}
\section{Introduction}
\label{sec:intro}
Three-dimensional scene reconstruction, driven by Neural Radiance Fields~\cite{mildenhall2020nerf} and 3D Gaussian Splatting~\cite{kerbl20233d}, is vital for VR/AR, robotics, and autonomous driving. A key challenge is realistic object removal and hole filling, which is essential for augmented reality and real estate visualization. Inpainting 360° unbounded scenes remains difficult due to the need for multi-view consistency, plausible unseen region extrapolation, and geometric coherence across views.
% Three-dimensional scene reconstruction and manipulation, driven by Neural Radiance Fields~\cite{mildenhall2020nerf} and 3D Gaussian Splatting~\cite{kerbl20233d}, are essential for applications in VR/AR, robotics, and autonomous driving. A major challenge is object removal with realistic hole filling, crucial for real estate visualization, augmented reality, and computer vision preprocessing. Inpainting in 3D scenes, particularly in 360° unbounded environments, remains difficult due to the need for multi-view consistency, plausible extrapolation of unseen regions, and geometric coherence across perspectives.

\cref{fig:teaser} shows our reference-based 360° unbounded scene inpainting approach. Given input images with camera parameters, object masks, and a reference image, our method generates an inpainted 3D scene using Gaussian Splatting~\citep{kerbl20233d,huang20242d} for novel view rendering. We exploit multi-view information and generative models to fill unseen areas, ensuring coherent and plausible results across views. Integrating Gaussian Splatting’s explicit representation with 2D generative inpainting, our method maintains multi-view consistency and geometric accuracy under significant viewpoint changes.
% \cref{fig:teaser} provides an overview of our reference-based 360° unbounded scene inpainting. Given input images with camera parameters, object masks, and a reference image, we generate an inpainted 3D scene in Gaussian Splatting~\citep{kerbl20233d,Huang2DGS2024} representation for novel view rendering. Our method exploits multi-view information and leverages generative processes to fill unseen areas, ensuring inpainted regions are coherent, plausible, and consistent across views. By integrating Gaussian Splatting’s efficient explicit representation with the generative capabilities of 2D inpainting models, our approach ensures multi-view consistency and geometric accuracy, even under significant viewpoint changes.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/motivation_new_compressed.pdf}
    \vspace{-6mm}
    \caption{
    \textbf{Comparison with different 3D inpainting approaches.} Existing methods such as SPin-NeRF~\cite{spinnerf} and GScream~\cite{wang2024gscream}, designed for forward-facing scenes, perform poorly in 360° scenarios. Reference-based methods like Infusion~\cite{liu2024infusion} struggle with accurate depth projection, causing fine-tuning artifacts. Gaussian Grouping~\cite{ye2023gaussian} frequently misidentifies unseen regions, reducing inpainting quality. Our AuraFusion360 achieves precise unseen masks and improved depth alignment via Adaptive Guided Depth Diffusion, employing SDEdit~\cite{meng2022sdedit} for diffusion-guided, multi-view consistent RGB generation.
    % \textbf{Comparison with different 3D inpainting approaches.} Previous methods, such as SPin-NeRF~\cite{spinnerf} and GScream~\cite{wang2024gscream}, are tailored for forward-facing scenes and tend to underperform in 360° unbounded scenarios. Reference-based methods, such as Infusion~\cite{liu2024infusion}, whose depth completion model struggles to accurately project the reference view back into the 3D scene, leading to fine-tuning artifacts. Gaussian Grouping~\cite{ye2023gaussian} often misidentifies the unseen region during mask generation, which can degrade inpainting quality. Our method, AuraFusion360, achieves a more accurate unseen mask and enhanced depth alignment through Adaptive Guided Depth Diffusion, with SDEdit~\cite{meng2022sdedit} applied to the initial points to leverage diffusion prior while also maintaining multi-view consistency in RGB guidance.
    }
    \vspace{-1mm}
    \label{fig:motivation}
\end{figure}

Several critical challenges in 360° unbounded scene inpainting motivated our approach (\cref{fig:motivation}). Existing methods~\cite{spinnerf, wang2024gscream, mirzaei2023reference, mirzaei2024reffusionreferenceadapteddiffusion}, effective for forward-facing scenes, struggle with extreme viewpoint changes in 360° scenes, resulting in inconsistencies and artifacts. Recent approaches like Gaussian Grouping~\citep{ye2023gaussian} effectively propagate semantic information for object removal, but their reliance on a text-based tracker~\cite{cheng2023tracking} often causes misidentified unseen regions, leading to inaccurate reconstructions.
% Several critical challenges in 360° unbounded scene inpainting motivated our approach, as illustrated in~\cref{fig:motivation}. Existing methods~\cite{spinnerf, wang2024gscream, mirzaei2023reference, mirzaei2024reffusionreferenceadapteddiffusion}, while effective for forward-facing scenes, fail in 360° unbounded environments due to extreme viewpoint changes, resulting in inconsistencies, artifacts, and hallucinated content across different angles. Recent methods like Gaussian Grouping~\citep{ye2023gaussian} excel at propagating semantic information to individual Gaussians, enabling effective object removal using Gaussian Splatting. However, their reliance on a text-based video tracker~\cite{cheng2023tracking} for detecting unseen regions is a major limitation. The tracker often misidentifies regions, leading to incorrect inpainting and inaccurate scene reconstructions.

To address these challenges, we propose a unified pipeline for 360° unbounded scene inpainting using Gaussian Splatting for object removal, depth-aware unseen region detection, and multi-view consistent inpainting. Inspired by Gaussian Grouping~\cite{ye2023gaussian}, our method integrates object-masked attributes into Gaussians for precise removal and reconstructs unseen regions before applying reference-guided inpainting. Unlike methods that directly apply inpainters, causing inconsistencies, we develop Adaptive Guided Depth Diffusion (AGDD) to unproject aligned points from the reference view into unseen regions. These points (1) initialize Gaussians and (2) guide inpainted RGB generation via SDEdit~\cite{meng2022sdedit}, ensuring coherent, high-quality 360° scene restoration.
% To address these challenges, we propose a unified pipeline for 360° unbounded scene inpainting, leveraging Gaussian Splatting as a 3D representation for object removal, depth-aware unseen region detection, and multi-view consistent inpainting. Inspired by Gaussian Grouping~\citep{ye2023gaussian}, our approach integrates object-masked attributes into each Gaussian for precise removal while preserving scene information and ensures reliable inpainting by reconstructing unseen regions before applying reference-guided inpainting. Unlike previous methods that directly apply an inpainter, leading to multi-view inconsistencies, we develop Adaptive Guided Depth Diffusion (AGDD) to unproject geometrically aligned points from the reference view into unseen regions. These points serve two purposes: (1) provide structured initialization for Gaussians and (2) generate inpainted RGB guidance via SDEdit~\cite{meng2022sdedit}, ensuring consistency before fine-tuning. Refining the Gaussians with these consistent RGBs results in a coherent, high-quality restoration of 360° scenes.

Integrating these improvements, our framework achieves enhanced geometric accuracy and realism in 360° unbounded scenes. To advance 3D inpainting, we propose a method that improves consistency and provides a benchmark for future research. Our contributions include: 
\begin{itemize} 
\item A depth-aware method leveraging multi-view information to accurately generate unseen masks for 360° unbounded scene inpainting. 
\item Integration of reference view unprojection with SDEdit to produce consistent RGB guidance across views. 
\item A comprehensive framework with a new 360° dataset and capture protocol, supporting high-quality novel view synthesis and quantitative evaluation. 
\end{itemize}
% By integrating these improvements, our framework achieves higher geometric accuracy and visual realism in 360° unbounded environments. To further advance 3D scene inpainting, we propose a novel approach that not only enhances inpainting consistency but also provides a benchmark for future research. Our key contributions include:
% \begin{itemize}
% \item A novel depth-aware method for generating unseen masks in 360° unbounded 3D scene inpainting, leveraging multi-view information to improve inpainting accuracy.
% \item An effective integration of accurate reference view unprojection and SDEdit to generate consistent guided RGBs across viewpoints for fine-tuning.
% \item A comprehensive framework including a new 360° inpainting dataset and capture protocol, enabling high-quality novel view synthesis and quantitative evaluations of inpainted scenes.
% \end{itemize}






% \textbf{Comparison with different 3D inpainting approach.} 之前的方法如GScream, SPin-NeRF  等做在forwar-facing scene上的methods. 在360 unbounded scene得情況下仍會有一些表現不好. 而reference-based 的方法如infusion的depth completion model沒辦法將reference view準確的投回3D scene上, 造成artifact. GaussianGrouping則是在unseen mask generation時容易偵測錯unseen region, 可能造成整體inpaint quality下降。我們的方法 AuraFusion360可得到更準確的unseen mask, 有更準確的align depth through guided depth diffusion, 再藉由這些initial points的SDEdit來強化guidance RGB間的multi-view consistency.