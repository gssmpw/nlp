%%%% Dataset Collection Figs Start %%%%
% \twocolumn[{%
% \renewcommand\twocolumn[1][]{#1}%
% \maketitle
% \begin{center}
% \centering
% \captionsetup{type=figure}
% \resizebox{1.0\textwidth}{!} 
% {
% % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
% \includegraphics[width=\textwidth]{figs/dataset.pdf}
% }
% \vspace{-6mm}
% \caption{\textbf{Illustration of the data capture process for the 360-USID dataset.} (a) Capturing training views: Multiple images are taken around the object in the scene. (b) Capturing the reference view: A camera is mounted on a tripod to capture a fixed reference view (with an object). (c) Capturing novel views: After removing the object, additional images are taken from various viewpoints, including one from the same tripod position as the reference image.}
% \label{fig:dataset_capture}
% \end{center}
% }]


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/our_dataset-crop_compressed.pdf}
    \vspace{-6mm}
    \caption{\textbf{Overview of the 360-USID dataset.} Sample images from each scene, including five outdoor scenes (Carton, Cone, Newcone, Skateboard, Plant) and two indoor scenes (Cookie, Sunflower). (\emph{Bottom right}) The table shows statistics for each scene, including the number of training views and ground truth (GT) novel views. The dataset provides a diverse range of environments for evaluating 3D inpainting methods in both indoor and outdoor settings.}
    \label{fig:our_dataset}
    \vspace{-3mm}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\columnwidth]{figs/dataset.pdf}
    \vspace{-6mm}
    \caption{\textbf{Illustration of the data capture process for the 360-USID dataset.} (a) Capturing training views: Multiple images are taken around the object in the scene. (b) Capturing the reference view: A camera is mounted on a tripod to capture a fixed reference view (with an object). (c) Capturing novel views: After removing the object, additional images are taken from various viewpoints, including one from the same tripod position as the reference image.}
    \label{fig:dataset_capture}
\end{figure}
%%%% Dataset Collection Figs End %%%%


\section{360$^{\circ}$ Unbounded Scenes Inpainting Dataset}
\label{sec:dataset}
To address the lack of reference-based 360° inpainting datasets, we introduce the 360° Unbounded Scenes Inpainting Dataset (360-USID), consisting of seven scenes with training views (RGB images and object masks), novel testing views (inpainting ground truth), and a reference view (without objects) for evaluating with other reference-based methods.

\vspace{3pt}
\noindent {\bf Dataset collection protocol.}
We developed a protocol using a standard camera to create this dataset, as simultaneously capturing multi-view photos with and without objects typically requires specialized equipment. Our protocol, illustrated in~\cref{fig:dataset_capture}, consists of:
\begin{enumerate}
% \item Positioning an object (\emph{e.g.} a vase) on a textured surface within a 360° unbounded scene. Training views are captured in approximately two complete circular trajectories around the object. The first trajectory focuses primarily on the object, while the second encompasses both the object and maximizes background coverage to ensure comprehensive scene capture and minimize potential gaps in test view coverage.
\item Positioning an object (\emph{e.g.} a vase) on a textured surface within a 360° unbounded scene. Training views are captured in two complete circular trajectories around the object - the first focuses primarily on the object, while the second maximizes background coverage to ensure comprehensive scene capture.
\item Securing the camera on a tripod and capturing a reference view from a fixed position and orientation.
\item After object removal, capturing novel views from both the fixed tripod position and additional positions distinct from training trajectories for ground truth evaluation.
\end{enumerate}
% To ensure high-quality captures, we select surfaces with rich textural details, stabilize the tripod, and disable white balance. We record video with 4K 60fps and extract the sharpest frames using the variance of the Laplacian method to minimize motion blur. Each scene comprises 180$\sim$200 training views and around 100 testing views now for quantitative evaluations. Consistent lighting is maintained to minimize the impact of object shadows on reference and testing images.
To ensure high-quality captures, we record video at 4K 60fps with stabilized camera settings and extract the sharpest frames using the variance of the Laplacian method. Each scene comprises 180$\sim$200 training views and approximately 30 testing views for quantitative evaluations. Consistent lighting is maintained throughout to minimize shadow variations between reference and testing images


\vspace{3pt}
\noindent {\bf Data preprocessing and pose estimation.}
\label{sec:data_preprocessing}
% For data preprocessing and camera pose estimation, we employ the following steps:
% \begin{enumerate}
% \item We use COLMAP~\citep{schoenberger2016sfm,schoenberger2016mvs} or a similar Structure-from-Motion (SfM) pipeline such as hloc~\citep{sarlin2019coarse,sarlin2020superglue} or GLOMAP~\citep{pan2024glomap} to compute a shared 3D coordinate space for training and novel views.
% \item As the object is removed in novel views, we generate object masks of training view using SAM2~\citep{ravi2024sam2} and input mask the object region of training view into COLMAP to ignore object reconstruction.
% \item After obtaining camera poses for training and novel views from COLMAP, we can input the training images into any NeRF/3DGS inpainting method to remove the object.
% \item We then use these methods' resulting radiance fields or 3D representations to render novel view photos, which we compare against our captured ground truth novel view images for quantitative evaluation.
% \item For the testing views, some of which may still include areas not covered by the training views. This can affect the evaluation calculation. To refine the testing views, we train a masked-3DGS model using the RGB images and object masks from the training views. We then render the testing views along with the object masks and select the sharpest frame from each set of neighboring frames based on the highest PSNR score. This PSNR score is calculated only for areas outside the rendered object mask. Through this process, we reduce the number of test views to approximately 30.
% \end{enumerate}
% The final dataset for each scene contains around 180 training images and 30 novel testing images, which can be used for quantitative evaluations.
Our processing pipeline begins with using COLMAP~\citep{schoenberger2016sfm,schoenberger2016mvs} or similar SfM pipelines like hloc~\citep{sarlin2019coarse,sarlin2020superglue} to compute a shared 3D coordinate space for both training and novel views. We then generate object masks for training views using SAM2~\citep{ravi2024sam2} and mask out object regions in COLMAP reconstruction. After obtaining camera poses, we process the training images with NeRF/3DGS inpainting methods and render novel views for comparison against ground truth. Finally, we refine testing views by training a masked-3DGS model and selecting optimal frames based on PSNR scores computed outside object regions, yielding approximately 30 high-quality test views per scene. The resulting dataset provides a comprehensive benchmark for evaluating 360° inpainting methods across diverse scenes and viewpoints, with particular attention to view consistency and geometric accuracy.



\vspace{3pt}
\noindent {\bf Scene descriptions.}
Our 360-USID dataset, shown in~\cref{fig:our_dataset}, contains seven diverse scenes: five outdoor (Carton, Cone, Newcone, Plant, Skateboard) and two indoor (Cookie, Sunflower). Each scene includes 180-200 training images at 3840$\times$2160 resolution (Plant at 1920$\times$1440), 30 ground truth testing images, and one reference image without objects. Scenes are downscaled to 960$\times$540 for evaluation, providing a comprehensive benchmark for testing 3D inpainting methods across varied real-world environments.
% Our 360-USID dataset, shown in~\cref{fig:our_dataset}, features seven diverse scenes: five outdoor (Carton, Cone, Newcone, Plant, Skateboard) and two indoor (Cookie, Sunflower). These scenes present various challenges for 3D inpainting tasks, representing a range of real-world environments. Each scene comprises 180-200 training images, approximately 30 ground truth novel testing images, and one reference image (without objects) corresponding to the highest-indexed training view, for evaluation with other reference-based inpainting methods. Most scenes are captured at a resolution of 3840×2160, with the Plant scene at 1920×1440. We downscale them to 3840$\times$2160 and 960$\times$540, respectively. This diversity in content, view counts, and resolutions makes 360-USID a robust tool for evaluating 3D inpainting algorithms in complex scenarios.
% \yulunliu{TODO: describe the specific characteristics of each scene.}
% Our 360-USID dataset, as illustrated in Figure~\ref{fig:our_dataset}, comprises seven diverse scenes designed to represent a wide range of real-world environments. The dataset includes four outdoor scenes (Box, Cone, Lawn, and Plant) and three indoor scenes (Cookie, Sunflower, and Dustpan), each presenting unique challenges for 3D inpainting tasks. These scenes feature various objects and environments, from natural outdoor settings to controlled indoor spaces, ensuring a comprehensive evaluation of 3D inpainting methods across different scenarios. The number of training views per scene ranges from 171 to 347, with 31 to 33 ground truth novel views for each, providing ample data for learning and evaluation. Notably, the Box, Cone, Lawn, Cookie, and Sunflower scenes are captured at 960$\times$540 resolution, while the Plant and Dustpan scenes offer greater detail at 960$\times$720 resolution. This variety in resolution allows for testing inpainting methods under different levels of visual fidelity. The diversity in scene content, view counts, and resolutions makes our 360-USID dataset a robust tool for assessing the performance of 3D inpainting algorithms in complex, real-world scenarios.





%%%% Our Dataset Figs Start %%%%
% \twocolumn[{%
% \renewcommand\twocolumn[1][]{#1}%
% \maketitle
% \begin{center}
% \centering
% \captionsetup{type=figure}
% \resizebox{1.0\textwidth}{!} 
% {
% % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
% \includegraphics[width=\textwidth]{figs/our_dataset.pdf}
% }
% \vspace{-6mm}
% \caption{\textbf{Overview of the 360-USID dataset.} Sample images from each scene, including five outdoor scenes (Carton, Cone, Newcone, Skateboard, Plant) and two indoor scenes (Cookie, Sunflower). (\emph{Bottom right}) The table shows statistics for each scene, including the number of training views and ground truth (GT) novel views. The dataset provides a diverse range of environments for evaluating 3D inpainting methods in both indoor and outdoor settings.}
% \label{fig:our_dataset}
% \end{center}
% }]



%%%% Our Dataset Figs End %%%%