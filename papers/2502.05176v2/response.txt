Existing 3D inpainting methods for NeRF__ typically adapt 2D models to NeRF’s implicit representation. For instance, SPIn-NeRF__ employs perceptual loss to improve multi-view consistency. Reference-based methods__ enhance consistency using reference images but remain limited to small-angle view rendering, restricting their use in 360° scenes. NeRFiller__ iteratively refines consistency with grid prior but struggles with fine-grained textures due to image downsampling. InNeRF360__ handles 360° scenes via density hallucination but has limited scene utilization.

Gaussian Splatting-based methods like Gaussian Grouping__ inject semantic information, while InFusion__ employs depth completion but requires manual view selection. GScream__ integrates Scaffold-GS but faces difficulties in unbounded 360° scenes. Our method addresses these issues by enhancing multi-view consistency and depth-aware inpainting in 360° scenarios using Gaussian Splatting.

Previous work such as SPIn-NeRF that integrates 2d inpainting model with perceptual loss and depth inpainting guidance to reconstruct Inpainted Neural Radiance Fields (NeRF) [1]. OR-NeRF Removing Objects From NeRF use confidence-based view selection automatically removes inconsistent views from the optimization preventing unwanted artefacts in the final result. MVIP-NeRF utilizes a multi-view approach to perform 3D inpainting on NeRF scenes by employing diffusion priors, where it jointly optimizes RGB and normal map completion through an iterative Score Distillation Sampling (SDS) process, ensuring consistent appearance and geometry alignment across multiple views while leveraging a multi-view scoring mechanism to distill generative priors from different perspectives [2]. Reference-guided controllable inpainting, as presented by Mirzaei et al. (2023), leverages reference images and view-dependent effects to guide the inpainting process in 3D scenes, enabling consistent and visually coherent completions across multiple perspectives while handling challenges like disocclusions and geometric consistency [3]. GScream introduces a robust framework for object removal in 3D scenes by optimizing Gaussian primitives' positions for geometric consistency and utilizing a cross-attention feature propagation mechanism to enhance texture coherence, effectively restoring both geometry and texture across visible and occluded areas.

References:
[1] SPIn-NeRF: Removing Objects from Neural Radiance Fields using 2d Inpainting and Perceptual Loss (2023)
[2] MVIP-NeRF: Multi-View Inpainting of NeRF Scenes via Diffusion Priors (2023)
[3] Reference-Guided Controllable Inpainting for 3D Scenes (2023)