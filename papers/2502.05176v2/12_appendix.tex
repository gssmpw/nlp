\label{sec:appendix_section}
% Supplementary material goes here.

\section{Overview}
% 1. Depth warping explanation
% 2. compared method setup
% 3. experimental setup
% 4. limitations
% 5. additional visual results
% 6. additional ablation study
% 6. in addition to this document, we provide a html

% This supplementary material presents additional results to complement the main manuscript. 首先我們為何Depth warping for generating bbox prompt to SAM2.裡有辦法成功找出unseen region的輪廓, 再來我們提供其他comparison method 的inplementation setup. 再來是我們方法的limitations. 再來是Additional Visual Results, 以及additional ablation stufy, 此外我們的attachment裡有提供一個interactive html for video visual comparison. 
% This supplementary material provides additional details and results to complement the findings presented in the main manuscript. First, we explain the training process for masked Gaussians and the details of object removal in ~\cref{sec:masked-gs}. We then elaborate on how depth warping, used for generating bounding box prompts for SAM2~\cite{ravi2024sam2}, effectively identifies the contours of unseen regions in~\cref{sec:depth_warping}. Next, we describe the experimental setup in~\cref{sec:compared_method}. Subsequently, we discuss the limitations of our approach in ~\cref{sec:limitations}, followed by additional visual comparison on our 360-UISD dataset (~\cref{fig:visual_add_ours} and Mip-NeRF-360~\cite{mipnerf.360} dataset~\cref{fig:visual_add_360}).
% % and ablation studies (~\cref{sec:add_ablation}).

% In addition to this document, we provide an interactive HTML interface for comparing our video results with state-of-the-art 3D inpainting methods, visualizing unseen masks, and exploring the ablation study on depth alignment. Finally, we include the source code of our implementation, which will be made publicly available to ensure reproducibility.
This supplementary material provides additional details and results to support the main manuscript. We first describe the training process for masked Gaussians and object removal in Section~\ref{sec:masked-gs}, followed by an explanation of depth warping for bounding box generation in SAM2~\cite{ravi2024sam2} and its role in identifying unseen region contours in Section~\ref{sec:depth_warping}. Next, we present ablations on different depth inpainting methods in Section~\ref{sec:mad} and a comparison of captured and inpainted references in Section~\ref{sec:ref_require}. We then outline the experimental setup in Section~\ref{sec:compared_method} and discuss the limitations of our approach in Section~\ref{sec:limitations}. Finally, we provide additional visual comparisons in ~\cref{fig:visual_add_ours} for the 360-UISD dataset and in ~\cref{fig:visual_add_360} for the other collected 360 dataset~\cite{barron2022mip}.


% In addition to this document, we provide an interactive HTML interface for comparing our video results with state-of-the-art 3D inpainting methods, visualizing unseen masks, and exploring the ablation study on depth alignment. Finally, we include the source code of our implementation, which will be made publicly available to ensure reproducibility.



\section{Training Masked GS for Object Removal}
\label{sec:masked-gs}
% 在training masked Gaussians, 我們使用2DGS為我們的code-base, 並在每一顆Gaussian裡面都加一個值在0~1之間的masked-attribute, 並利用SAM2-得到的每個view的object mask 和rasterize出來的object-maske算L1 loss, 並使用及GaussianGrouping提出的Grouping Loss, 彼此相近的Gaussians的masked attribute要月相月好, 來training我們的masked-Gaussians. 這樣可以確保我們的Gaussian Model能含有object-masked資訊, 並能render出準確的object-mask for接下來的應用. Thanks to Gaussian Splatting explicit的特性, 我們可以直接在removal階段將masked=attribute > 0.6的Gaussians移除, 進而達到移除物體的效果.
During the training of masked Gaussians, we use 2DGS~\cite{huang20242d} as our codebase and introduce a masked attribute, ranging between 0 and 1, for each Gaussian. The L1 loss is computed between the object mask obtained via SAM2~\cite{ravi2024sam2} and the rasterized object mask for each training view. Additionally, we incorporate the Grouping Loss proposed by Gaussian Grouping~\cite{ye2023gaussian}, ensuring that neighboring Gaussians have similar masked attributes. This ensures that our Gaussian model retains accurate object mask information and is capable of rendering precise object masks for subsequent applications.

Thanks to the explicit nature of Gaussian Splatting, we can directly remove Gaussians with a masked attribute greater than a threshold $\tau$ during the removal stage, effectively achieving object removal. In our implementation, $\tau$ is set to 0.6.


%%%% Augmented Unseen Region Figs Start %%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/unseen_region.pdf}
    % \vspace{-6mm}
    \caption{\textbf{Intermediate Results of Depth Warping for Unseen Region Detection.} This figure illustrates the intermediate results generated during the depth warping process. (a) and (b) show the RGB image and the corresponding removal region at view $n$, respectively. (c) displays the removal regions obtained from view $i$ ($i \neq n$). (d) shows the unseen region obtained from view $i$ through backward traversal. The intersections are concentrated near the unseen region. Note that the pixels within the unseen region, but with a value of zero, are due to the absence of Gaussians in that area, preventing depth rendering and thus making it impossible to establish pixel correspondences between view $n$ and view $i$. (e) presents the aggregation of all unseen regions obtained from view $i$ at view $n$. A threshold is applied to this result, and it is then intersected with the removal region at view $n$ to obtain the final result in (f).}

    
    % 這個figure show出depth warping 過程會產生的中間產物 (a)(b) show RGB image and correpsonding removal region at view $n$,  (c) shows the removal regions obtained from view i ($i \neq n$)}  (d) shows unseen region obtained from view through backward traversal, 可以看見交集會聚集在unseen region附近, 而那些雖然在unseen region內但卻是值卻是0的pixel, 則是因為這個地方沒有Gaussians, 因而無法得到render depth造成. (e) 則是所有unseen region obtained from view i at view n aggreaget再一起的結果 這裡會對他取threshold 並和view n 的object mask做交集 得到最終的 (f)
    \label{fig:unseen_region}
\end{figure}
%%%% Augmented Unseen Region Figs End %%%%









\section{Depth Warping for Unseen Contours}

\label{sec:depth_warping}
% Follow Fig.4, without loss of generality對於每一個view n & view i, 再透過view i 的rendered depth 和 rendered incomplete depth取不相重的pixel而得到view i的removal region後, 我們這裡透過是先取的的view n 和view i間的pixel correspondence, 將view i的removal region backward traversal回view n, 注意這裡的pixel correspondence因為適用view n的incomplete depth取得的, 所以不屬於unseen region的部分會被對應回view n的背景區域, 而unseen region則會留在原本的區域. 所以透過each view i (i != n), 我們可以抓出每個view i不屬於unseen region的部分投到不同區域, 而屬於unseen region的部分則會盡量留在底部, 讓我們可以identify出unseen region的輪廓. 而這裡可以注意到unseen region也有可能有不含Gaussian的地方, 會因此在這些區域無法得到rendered depth, 所以並無法取得view n 和view i pixel correspondence, 所以可以看到每個unseen refion obtained by view at view n, 才有些unseen region是空的, 這個部分就需要透過sam2幫忙.

Following Sec. \textcolor{cvprblue}{3.2} and Fig. \textcolor{cvprblue}{4} of the main paper, we explain in detail how depth warping allows us to identify the contours of the unseen region, as illustrated in ~\cref{fig:unseen_region}. Without loss of generality, to find the unseen region contour at view \(n\), and for each pair of views \(n\) and \(i\), we first compute the removal region for view \(i\) by identifying pixels that differ between the rendered depth and the incomplete depth of view \(i\) rather than using object masks. This approach better captures geometric changes and prevents misalignment artifacts, leading to improved SAM2\cite{ravi2024sam2} prompts and more precise unseen masks (\cref{fig:unseen_counter_ablation}). 

Next, we establish pixel correspondences between view \(n\) and view \(i\) using the incomplete depth of view \(n\). The removal region of view \(i\) is then backward-traversed to view \(n\) based on these correspondences. During this backward traversal, it is important to note that pixels outside the unseen region in view \(i\) will correspond to the background areas in view \(n\), while pixels belonging to the unseen region remain in the unseen region. By aggregating contributions from all views \(i\) (\(i \neq n\)), we project non-unseen regions from each view \(i\) into different areas of view \(n\), while consolidating the unseen regions. This allows us to identify the contours of the unseen region in view \(n\). These contours can then be used as the bounding box prompt for SAM2, resulting in a more accurate unseen mask. 
% Note that we found depth difference as removal region better to capture geometric changes than raw object masks (Fig. \ref{fig:unseen_counter_ablation}), leading to improved SAM2~\cite{ravi2024sam2} prompts and final unseen masks.



% \chunghow{modify here}% 可能會被問為啥沒gaussian的區域不用opacity解決就好，主要次這depth warping會對輪廓偵測透別強, 單單使用這個沒辦法完整偵測, 比如說像底部透過去依然是背景的部分, 而且因為是unbounded scene, opacity容易選到一些背景區域, 會造成錯誤identify 


%%%% Unseen Counter Ablation Figs End %%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/unseen_contour_ablation.pdf}
    % \vspace{-6mm}
    \caption{\textbf{Ablation Study on Removal Region Definition.} Comparison of (a) object masks vs. (b) depth difference for defining removal regions. Object masks fail to capture geometric changes, leading to less accurate unseen masks. Depth difference better preserves scene structure, improving SAM2 prompts and unseen region segmentation.}
    \label{fig:unseen_counter_ablation}
    % \vspace{-1mm}
\end{figure}

%%%% Unseen Counter Ablation Figs End %%%%


\section{Comparison of Depth Completion Methods}
\label{sec:mad}
In addition to Fig. \textcolor{cvprblue}{11} of the main paper, we compare scale–shift alignment, LaMa~\cite{lama}, InFusion~\cite{liu2024infusion}, GDD~\cite{yu2024wonderworld}, and AGDD for depth completion. As shown in ~\cref{tab:mad_quan}, we evaluate the mean absolute difference (MAD) in object mask areas in 30 test views, using pseudo-GT depth from a 2DGS trained on 200 removal images, as mentioned in Sec. \textcolor{cvprblue}{4}.
% ~\cref{sec:data_preprocessing}. 
Aligning scale-shift misaligns boundaries in 360° scenes, while LaMa provides reasonable depth completion but does not fully resolve alignment issues. AGDD achieves the lowest MAD and better handles complex geometry.


\begin{table}[h]
\centering
\tiny
\caption{MAD values for different depth completion methods.} % 先放 caption
\label{tab:mad_quan}  % 然後放 label
\resizebox{\columnwidth}{!} {
\begin{tabular}{p{4cm}|c} % 修正c{4cm}的錯誤，應該用c
\toprule
Depth completion method & MAD $\downarrow$ \\
\midrule
Scale-shift align & 0.063 \\
LaMa depth inpainting & 0.077 \\ 
InFuion & 0.047 \\
GDD & 0.065\\ 
AGDD & \textbf{0.045} \\
\bottomrule
\end{tabular}
}
\vspace{-1.3pc}
\end{table}

\section{Reference Images in Real-World Use}
\label{sec:ref_require}
Our 360-USID dataset provides real-world captured reference images. However, this does not mean that our method requires extra input. In practical scenarios, reference images can be captured post-removal for real-world use. We also ensure a fair evaluation by avoiding hallucinated textures, even if the inpainting is consistent. Additionally, reference guidance helps reduce multi-view inconsistency with minimal extra input.
As shown in ~\cref{tab:ref_inpain_quan}, while LaMa-based references slightly degrade the results, they still outperform other reference-based methods, such as GScream. Even when using an inpainted image as a reference, our approach still achieves good results.

\begin{table}[h]
\centering
\footnotesize
\caption{Comparison of Captured and Inpainted Reference.}
\label{tab:ref_inpain_quan}  % 然後放 label
\resizebox{\columnwidth}{!} {
\begin{tabular}{l|cccc}
\toprule
Reference method & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & FID $\downarrow$ \\
\midrule
GScream & 14.758 & 0.955 & 0.514 & 152.295 \\
LaMa-reference & 17.102 & 0.960 & 0.407 & 69.874 \\ 
Captured-reference &  \textbf{17.661} & \textbf{0.961}  & \textbf{0.388} & \textbf{62.173} \\
\bottomrule
\end{tabular}
}
% \begin{minipage}{.35\linewidth}
% \vspace{1mm}
% \includegraphics[width=1.\linewidth]{rebuttal_figures/unseen_contour_ablation.pdf}
% \end{minipage}
\vspace{-1pc}
\end{table}




\section{Experimetal Setup}
\label{sec:compared_method}
% 在這個section, 我們會解釋experimental setup, 包括2D inpainting model, 以及其他3D inpainting compared method.
% In experiment, For 所有comparison method, 我們使用SAM2得到的object mask, for 360-USID dataset, 我們使用dataset的reference image for all-reference-based method, for Mip-nerf-360 


\subsection{LeftRefill~\cite{cao2024leftrefill}}
% We provide the same reference image and training view object masks as in our method, 並直接針對每一個testing novel view做reference-based inpainting.
We use the same reference image as in our method, along with the rendered object masks of each novel testing view generated by our masked Gaussians, as input to LeftRefill and directly perform reference-based inpainting on each testing novel view.


\subsection{2DGS~\cite{huang20242d} + LaMa~\cite{lama}}
We provide the same reference image and training view object masks as in our method and use LaMa~\cite{lama} to obtain per-frame inpainting results for each training view to train the 2DGS.

\subsection{2DGS~\cite{huang20242d} + LeftRefill~\cite{cao2024leftrefill}}
% We provide the same reference image and training view object masks as our method, 並使用LeftRefill obtain per-frame inpaint結果of each training view, 來training 2DGS. 這裡使用leftrefill的原因是因為, 要算metric時雖然每個view都很consistency但卻和原本testing ground truth image, 的distribuion不同, 反而導致數字變低. 所以我們利用leftrefill的套
We provide the same reference image and training view object masks as in our method and use LeftRefill to obtain per-frame inpainting results for each training view to train the 2DGS.


%%%% Failure Figs Start %%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/failure_case.pdf}
    \caption{\textbf{Failure Cases.} The figure illustrates failure cases of inpainting results. These examples highlight the challenges of 3D inpainting when significant occlusions are present near the regions requiring inpainting. For instance, (b) and (c) demonstrate difficulties in achieving satisfactory guided inpainted RGB images in the training views, while (d) and (e) show errors resulting from incorrect pixel unprojections. These observations indicate that this issue is not effectively addressed by any of the compared methods, suggesting a potential avenue for further exploration and improvement.}

    % Fauire case show that the 3D inpainting 在需要inpaint的區域附近很多遮擋物時的困難, 如(b)(c)都很難拿到不錯的per-frame inpainted RGB images, (d)(e)則都會在錯誤unprojt一些pixel. Show that this issue is not handled well by any of the compared methods, highlighting a potential direction for further exploration and improvement.

    % 這個figure show出depth warping 過程會產生的中間產物 (a)(b) show RGB image and correpsonding removal region at view $n$,  (c) shows the removal regions obtained from view i ($i \neq n$)}  (d) shows unseen region obtained from view through backward traversal, 可以看見交集會聚集在unseen region附近, 而那些雖然在unseen region內但卻是值卻是0的pixel, 則是因為這個地方沒有Gaussians, 因而無法得到render depth造成. (e) 則是所有unseen region obtained from view i at view n aggreaget再一起的結果 這裡會對他取threshold 並和view n 的object mask做交集 得到最終的 (f)
    \label{fig:fail_case}
\end{figure}
%%%% Failure Figs End %%%%



\subsection{SPIn-NeRF~\cite{spinnerf}}
% 因為SPIn-NeRF的codebase原本是跑在forward-facing scene, 但我們要比較在360的scene, 所以我們followe SPIn-NeRF的方法並將他inplement在2DGS上, 我們使用我們的Masked-Gaussian Splatting render 所有training view的depth, rgb, object-mask，並以分別使用lama得到inpainted-rgb & inpainted-depth. 並以inpainted-rgb & inpainted-depth來train一個2DGS, 並加上rgb-ssim, rgb-lpips, 以及follow FSGS使用Pearson coefficient來算depth的scale and shift invariant loss. 
The original SPIn-NeRF~\cite{spinnerf} codebase is designed for forward-facing scenes; however, we adapt it for comparison on 360° scenes by implementing its approach on 2DGS~\cite{huang20242d}. We first obtain the depth for each training view by training a 2DGS model. Next, we generate inpainted RGB and depth maps using LaMa~\cite{lama}, which are then used to train the inpainted 2DGS model. During training, we follow SPIn-NeRF's methodology by incorporating patch-based RGB-LPIPS loss and using the Pearson correlation coefficient to compute a scale- and shift-invariant depth loss.






\subsection{Gscream~\cite{wang2024gscream}}
% GScream 我們 follow 它的 original codebase作為比較方法, 我們方法相同的一張reference image, 以及相同的training image objecy mask, 以及followGscream的pipeline，利用 Marigold 生成所有 training images 的 Monocular Depth Estimation 以符合 GScream 的 input data requirements
We follow the original GScream~\cite{wang2024gscream} pipeline as a baseline for comparison. We provide the same reference image and training view object masks as our method to ensure consistency. Following their pipeline, we use Marigold~\cite{ke2023repurposing} to generate estimated depths for all training images, meeting GScream's input data requirements.

\begin{figure*}[t]
    % \vspace{-3mm}
    \centering
    \includegraphics[width=1\linewidth]{figs/visual_additional_ours.pdf}
    % \vspace{-6mm}
    \caption{\textbf{Visual Comparison on our 360-USID dataset.} }
    \label{fig:visual_add_ours}
\end{figure*}

\subsection{Gaussian Grouping~\cite{wang2024gscream}}
% 我們使用Gaussian Grouping 的 original codebase作為比較方法。首先 Gaussian Grouping 會生成 segmentation ids，我們將屬於需要 inpaint 的物體內的 id 都選取，以供 Gaussian Grouping removal 使用。接著，變依照 original code 的流程，分別找出 unseen region 並 inpaint + fine-tuning。 直得注意的是, After removing objects from the scene. Gaussian Grouping depends on TrackingAnythinDEVA to identify unseen regions that requires further inpainting. However, DEVA fails to correctly find the unseen regions in some scenes, resulting in wrongly inpainted views. This results in poor results on those scenes. 另外在某些scene如mip-nerf-360的bonsai, 360-UISD的plant中, object tracker會偵測錯東西, 導致在removal階段就移除錯東西, 進而造成poor inpainting results.

We utilize the original Gaussian Grouping~\cite{ye2023gaussian} codebase as a baseline for comparison. First, it generates segmentation IDs, from which we select the IDs corresponding to objects that require inpainting. These selected IDs are then used in the removal process. Following the original workflow, the unseen regions are identified, subsequently inpainted, and used for their fine-tuning process.

Notably, after removing objects from the scene, Gaussian Grouping relies on TrackingAnything-DEVA~\cite{cheng2023tracking} to identify unseen regions requiring further inpainting through the "black blurry hole" prompt. However, DEVA occasionally fails to accurately identify unseen regions in certain scenes, leading to incorrect inpainting and suboptimal results. Additionally, in some scenes, such as the \textit{bonsai} scene from the Mip-NeRF-360~\cite{barron2022mip} dataset and the \textit{plant} scene from the 360-UISD dataset, the object tracker misidentifies objects, resulting in incorrect object removal and further degrading the inpainting quality.



\subsection{InFusion~\cite{liu2024infusion}}
% 我們使用InFusion的original code base作為比較對象, 對含有object的Gaussians做removal, 並使用我們提供的reference image作為他的depth completion model的input rgb, 並以此 referece image來finetuning他的Gaussians. 他的codebase並沒有提供他方法說的Progressive Inpainting, 所以我們並沒有使用這塊
We use the original InFusion~\cite{liu2024infusion} codebase as a baseline for comparison. We provide the same reference image used in our method as the input RGB for its depth completion model. This reference image is also used in its fine-tuning process.
% The InFusion codebase does not provide the Progressive Inpainting method as mentioned in the paper, so we did not perfrom this compome


% \section{Why Depth Warping Identifies Unseen Region Contours}
% \section{Additional Detials}
% \label{sec:add_details}







% \section{Additional Visual Comparison}
% \label{sec:add_visuals}
% ~\cref{fig:visual_add_ours} and ~\cref{fig:visual_add_360} show additional visual results from our 360-UISD dataset and mip-nerf-360. 展示我們的方法可以很好地適應360 inpainting 的scenarios.

% \section{Additional Ablation Study}
% \label{sec:add_ablation}






\section{Limitations}
\label{sec:limitations}
% 真正的limitaions: 我們的方法仰賴很準確的unprojected initial gaussians, 並針對這些initial details做SDEdit來強化multi-view consistency, 因此pipeline上的三段彼此關聯性很強, 某一個stage做得不好, 很容易影響其他stage. 
% 我們將場景延展到largen scene或large object會很吃力, 我們的方法僅限於inpainting, object insertion不太行.
% Our method successfully handles complex, unbounded 360° scene inpainting. However, rendering the unprojected initial Gaussians in SDEdit to enhance the guided inpainted RGB images may take a longer time, especially when dealing with high-resolution or large-scale scenes, making real-time application challenging. 
% Further, 我們為了和其他人公平比較而使用LeftRefill作為我們的2d inpainting model, 但Diffusion-based的model依然還是很容易生出完全不該是inpaint區域內容的inpaint rgb (e.g. 長出奇怪的物體), 這算然可以透過SDEdit降低noise讓原有的initial Gaussians透出更多解決, 不過還是會發生這種情況, 導致guided-inpaint-rgb, 這個希望可以透過更強的inpainting model改善
% Further, our method 會在需要inpaint的物體附近有滿多遮擋物時, 造成unproject一些錯誤的pixel, 進而造成最終的inpaint結果有一些floaters. 不過這種情況在所有比較方法都沒辦法做得很好, 是一個further可以探討延伸的方向
Our method successfully addresses complex, unbounded 360° scene inpainting. However, rendering the unprojected initial Gaussians and applying SDEdit~\cite{meng2022sdedit} to enhance the guided inpainted RGB images can be time-consuming, particularly for high-resolution or large-scale scenes, which poses challenges for real-time applications. Furthermore, our analysis~\cref{fig:fail_case} shows that the method may produce incorrect pixel unprojections in cases with significant occlusions near the object requiring inpainting, resulting in floaters in the final inpainted outputs. This limitation is similarly observed across all compared methods, underscoring a valuable direction for future research and improvement.
% Furthermore, in order to make a fair comparison with others, we use LeftRefill as our 2D inpainting model. However, diffusion-based models still tend to generate inpainted RGB content that should not belong in the inpainted area (e.g., strange objects appearing). While this can be mitigated to some extent by using SDEdit to reduce noise and allow the original initial Gaussians to emerge more clearly, this issue still persists, affecting the quality of the guided inpainted RGB images. We hope this can be improved with stronger inpainting models.

% \yulunliu{Failure Cases.}



\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/visual_additional_360.pdf}
    % \vspace{-3mm}
    \caption{\textbf{Visual Comparison on Other-360 dataset.} }
    \label{fig:visual_add_360}
\end{figure*}





