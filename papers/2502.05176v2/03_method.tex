%%%% Pipeline Figs Start %%%%
% \twocolumn[{%
% \renewcommand\twocolumn[1][]{#1}%
% \maketitle
% \begin{center}
% \centering
% \captionsetup{type=figure}
% \resizebox{1.0\textwidth}{!} 
% {
% % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
% \includegraphics[width=\textwidth]{figs/pipeline.pdf}
% }
% \vspace{-6mm}
% \caption{\textbf{Overview of our method.} Our approach takes multi-view RGB images and corresponding object masks as input and outputs a Gaussian representation with the masked objects removed. The pipeline consists of three main stages: (a) Augmented Unseen Mask Generation to identify truly occluded areas, referred to as the ``unseen region'', (b) Depth-Aware Gaussians Initialization on Reference View to fill unseen regions with initialized Gaussian containing reference RGB information after object removal, and (c) SDEdit-based Guided-Inpainted RGB Detail Enhancement, which leverages the inpainting model's capability to enhance fine details while maintaining multi-view consistency of the guided-inpainted RGB across all views.
% % Reference-Guided Inpainting and 3DGS Finetuning, which iteratively refine the 3DGS representation using a reference-based 2D diffusion inpainting model and ensure multi-view consistency. % This process enables high-quality novel view rendering of the edited scene.
% % 
% }
% \label{fig:pipeline}
% \end{center}
% }]
%%%% Pipeline Figs End %%%%

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/pipeline.pdf}
    \vspace{-6mm}
    \caption{\textbf{Overview of our method.} Our approach takes multi-view RGB images and corresponding object masks as input and outputs a Gaussian representation with the masked objects removed. The pipeline consists of three main stages: (a) Depth-Aware Unseen Masks Generation to identify truly occluded areas, referred to as the ``unseen region'', (b) Depth-Aligned Gaussian Initialization on Reference View to fill unseen regions with initialized Gaussian containing reference RGB information after object removal, and (c) SDEdit-Based RGB Guidance for Detail Enhancement, which enhances fine details using an inpainting model while preserving reference view information. Instead of applying SDEdit with random noise, we use DDIM Inversion on the rendered initial Gaussians to generate noise that retains the structure of the reference view, ensuring multi-view consistency across all RGB Guidance.}
    \label{fig:pipeline}
    \vspace{-1mm}
\end{figure*}

\section{Method}
\label{sec:method}

% Our method takes multi-view RGB images $\left\{ I_n \right\}$ and corresponding object masks $\left\{ M_n \right\}$ as input, where $n \in \left[1..N \right]$, and outputs an inpainted Gaussian representation with removed objects. Furthermore, regions occluded by objects in all views (which we refer to as unseen regions, following the Gaussian Grouping~\cite{ye2023gaussian} method) are consistently inpainted across all views. As shown in~\cref{fig:pipeline}, our approach begins by training a masked Gaussian using the object masks, followed by removing the objects from the scene. We then proceed with our pipeline, (a) Augmented Unseen Masks Generation (\cref{sec:unseen}), (b) Depth-Aware Gaussians Initialization on Reference View (\cref{sec:depth_initial}), and (c) SDEdit-based Guided-Inpainted RGB Detail Enhancement (\cref{sec:sdedit}). This pipeline effectively propagates textures across views in unbounded scenes, resulting in high-quality, consistent 3D inpainting.

Our method processes multi-view RGB images $\left\{ I_n \right\}$ and object masks $\left\{ M_n \right\}$, $n \in \left[1..N \right]$, to produce an inpainted Gaussian representation with removed objects. Occluded regions (unseen regions~\cite{ye2023gaussian}) are consistently inpainted across views. As shown in~\cref{fig:pipeline}, the process includes training a masked Gaussian using object masks, removing objects, and applying (a) Depth-Aware Unseen Mask Generation (\cref{sec:unseen}), (b) Reference View Initial Gaussians Alignment
(\cref{sec:depth_initial}), and (c) SDEdit for Detail Enhancement (\cref{sec:sdedit}). This pipeline ensures consistent texture propagation in unbounded scenes, achieving high-quality 3D inpainting.







\subsection{Depth-Aware Unseen Mask Generation} \label{sec:unseen}
% \vspace{-1mm}
% Accurate identification of regions that require inpainting is crucial to maintain the consistency of the scene and maximize the use of available background information. When generating the unseen mask for a specific view, it is essential to distinguish between two main regions within each view's removal region: (1) the background, which is visible across multiple views, and (2) the unseen region, which is occluded in all views and requires inpainting.

Accurate identification of inpainting regions is critical for scene consistency and optimal use of background information. To generate the unseen mask for a view, it is necessary to differentiate between (1) the background visible across multiple views and (2) the unseen region occluded in all views, requiring inpainting.


% \jayinnn{Maybe a brief introduction of SAM2 here or related work? "We utilize SAM2, ...}

% A naive approach to detecting unseen masks using SAM2~\cite{ravi2024sam2} is by manually selecting the first view and propagating the prompt across other views. However, we find that SAM2~\cite{ravi2024sam2} struggles to consistently detect the unseen region in all views without additional refinement. The unseen region may mistakenly reveal parts of the background or the inside of objects. To address this, our method uses depth warping to generate a bounding box prompt for each view, as shown in~\cref{fig:unseen_masks}. This method ensures accurate detection of the unseen region across all views, fully automating the process without manual prompt selection.

A naive approach to detecting unseen masks with SAM2~\cite{ravi2024sam2} involves manually selecting the first view and propagating prompts across other views. However, SAM2 struggles to consistently detect unseen regions without refinement, often revealing parts of the background or inside objects. To address this, our method employs depth warping to generate bounding box prompts for each view (\cref{fig:unseen_masks}), ensuring accurate, fully automated unseen region detection.

% Simply using SAM2 to detect the unseen region by manually selecting a first view and propagating the prompt to other views may not be perfect in some cases. This is because, without additional refinement, SAM2 struggles to consistently detect the unseen region across all views, as the unseen regions may mistakenly reveal parts of the background or the inside of objects. To address this, we propose using depth warping to generate a bounding box (bbox) prompt for each view, ensuring accurate unseen mask identification across all views. Depth warping establishes pixel correspondences, but only detects the contour of the unseen region, as removed areas may lack Gaussian information for rendering depth, preventing calculating pixel correspondence.

% In summary, our method combines depth warping with SAM2 in a complementary manner. Depth warping generates an approximate contour of the unseen region, and SAM2 refines the mask using a bounding box prompt derived from this contour. This ensures accurate detection of the unseen region across all views, fully automating the process without manual prompt selection.
% \vspace{-1mm}

%%%% Augmented Unseen Mask Generation Figs Start %%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/unseen_masks.pdf}
    \vspace{-6mm}
    \caption{\textbf{Overview of the Unseen Mask Generation Process using Depth Warping.} To obtain the unseen mask for view $n$, we calculate the pixel correspondences between the view $n$ and all other views $i$ by using the rendered incomplete depth $D_{n}^{\text{incomplete}}$. For each view $i$, the removal region $R_i$ is backward traversal to view $n$ to align occlusions. We then aggregate the results from multiple views, averaging and applying a threshold to produce the initial contour of the unseen mask. This contour is subsequently converted into a bounding box prompt for SAM2~\cite{ravi2024sam2}, which refines the unseen mask to its final version for view $n$.}
    \label{fig:unseen_masks}
    \vspace{-1mm}
\end{figure}
%%%% Augmented Unseen Mask Generation Figs End %%%%


% \paragraph{Depth warping for generating bbox prompt to SAM2.}
% To refine the unseen mask, we employ a depth-warping technique. \cref{fig:unseen_masks} illustrates the process of generating the unseen mask using depth warping. For each view $n$, we compute:

% \begin{equation}
% R_{i \rightarrow n} = \mathcal{W}_{\text{traverse}}(R_{i}, D_n^{\text{incomplete}}, T_{n \rightarrow i}),
% \end{equation}
% where $\mathcal{W}_{\text{traverse}}$ represents a traversal operation involving two stages: (1) a forward warping from view $n$ to view $i$ to obtain pixel correspondences, and (2) a backward traversal to map the removal region back to view $n$. $R_i$ represents the removal region mask for view $i$, obtained by identifying pixels where the complete and incomplete depth maps differ. $D_{n}^{\text{incomplete}}$ is the rendered incomplete depth map for view $n$, used to establish the necessary pixel correspondences. $T_{n \rightarrow i}$ is the transformation from view $n$ to view $i$. 

% The unseen mask contour for view $n$ is obtained by aggregating the warped removal regions from multiple views and applying an averaging operation followed by thresholding:
% \begin{equation}
% C_n = \theta \left( \frac{1}{K} \sum_{i=1}^K R_{i \rightarrow n} \right) \cap R_{n},
% \end{equation}
% where $C_n$ is the initial contour of the unseen mask at view $n$, $K$ is the number of views, and $\theta(\cdot)$ denotes a thresholding function. The contour $C_n$ is then used to create a bounding box $\text{bbox}(C_n)$ as a prompt for the SAM2~\cite{ravi2024sam2} model to obtain the final unseen mask $U_n$.
% \begin{equation}
% U_n = \text{SAM2}(\text{bbox}(C_n)).
% \end{equation}
% This mask $U_n$ is then used in subsequent stages of our pipeline to guide the inpainting process, ensuring that we focus on areas that truly require reconstruction while preserving as much original scene information as possible. 
% \vspace{-3mm}

\vspace{3pt}
\noindent {\bf Depth warping for generating bbox prompt to SAM2.}
To refine the unseen mask, we employ a depth-warping technique, as illustrated in \cref{fig:unseen_masks}. For each view $n$, we compute:
\begin{small}
\begin{equation}
R_{i \rightarrow n} = \mathcal{W}_{\text{traverse}}(R_{i}, D_n^{\text{incomplete}}, T_{n \rightarrow i}),
\end{equation}
\end{small}
where $\mathcal{W}_{\text{traverse}}$ includes forward warping from view $n$ to $i$ and backward traversal to map the removal region back to $n$. $R_i$ is the removal region mask for view $i$, derived from depth differences. $D_{n}^{\text{incomplete}}$ is the incomplete depth map for view $n$, and $T_{n \rightarrow i}$ is the transformation from view $n$ to $i$.  

The unseen mask contour for view $n$ is obtained by aggregating warped removal regions and applying thresholding:
\begin{small}
\begin{equation}
C_n = \theta \left( \frac{1}{K} \sum_{i=1}^K R_{i \rightarrow n} \right) \cap R_{n},
\end{equation}
\end{small}
where $C_n$ is the contour of the unseen mask, $K$ is the number of views, and $\theta$ is a thresholding function. A bounding box $\text{bbox}(C_n)$ is created as a prompt for SAM2~\cite{ravi2024sam2} to generate the final unseen mask:
\begin{small}
\begin{equation}
U_n = \text{SAM2}(\text{bbox}(C_n)).
\end{equation}
\end{small}
This mask $U_n$ guides the inpainting process, focusing on areas needing reconstruction while preserving original scene information.


% \subsection{Adaptive Guided Depth Diffusion for Reference View Initial Gaussian Alignment} \label{sec:depth_initial}
\subsection{Reference View Initial Gaussians Alignment} \label{sec:depth_initial}
% \subsection{Adaptive Guided Depth Diffusion} \label{sec:depth_initial}
% \vspace{-1mm}
After performing object removal and generating the unseen mask, similar to CorrFill~\cite{liu2025corrfill}, we select a reference view called \( V_{\text{ref}} \), which can render an incomplete RGB image and depth. We then apply RGB inpainting to the incomplete RGB image of \( V_{\text{ref}} \) and denote it as $I_{\text{ref}}$. 
% Since our goal is to propagate the information from the inpainted reference RGB image to multiple views as much as possible, we use the estimated depth of $I_{ref}$ from the Adaptive Guided Depth Diffusion to project the reference view’s RGB image into 3D space. This projection serves as the foundation for the subsequent SDEdit-guided RGB image detail enhancement and for initializing points in the fine-tuning of Gaussians. Therefore, having well-aligned depth is crucial for accurately setting these initial points in our process. 
To maximize cross-view consistency, we project the reference RGB image into 3D space using depth estimates of $I_{\text{ref}}$, which is obtained through Adaptive Guided Depth Diffusion. This 3D projection serves two critical purposes: It guides the SDEdit-based RGB detail enhancement and initializes point positions for Gaussian fine-tuning. Accurate depth alignment is, therefore, fundamental to our pipeline, as it directly determines the precision of these initial point positions.




\vspace{3pt}
\noindent {\bf Adaptive Guided Depth Diffusion (AGDD).}
% Aligning the \textit{estimated depth} with the \textit{existing depth} is a challenging problem. Traditional depth alignment methods~\cite{ranftl2020towards,zhu2025fsgs} that directly optimize scale and shift parameters between the \textit{estimated depth} and \textit{existing depth} often yield suboptimal results. Specialized depth completion models such as Infusion~\cite{liu2024infusion}, despite being explicitly trained for this task, still exhibit misalignment issues when handling varying viewpoints and scene configurations.
% \chunghow{Should we move this part to related work?} \jayinnn{Traditional method needs some citations here}
% Aligning the \textit{estimated depth} with the \textit{existing depth} is challenging since monocular depth estimation~\cite{ke2023repurposing} lacks absolute scale information (scale ambiguity) and produces non-metric depth in a different coordinate system. This issue is especially severe in 360° unbounded scenes, where large viewpoint changes make alignment significantly more difficult than in forward-facing setups. Traditional methods~\cite{ranftl2020towards,zhu2025fsgs} that optimize scale and shift parameters often yield suboptimal results. Even specialized depth-completion models such as Infusion~\cite{liu2024infusion}, explicitly trained for this task, struggle with misalignment under varying viewpoints and scene configurations.
Aligning \textit{estimated depth} with \textit{existing depth} is challenging due to monocular depth estimation~\cite{ke2023repurposing}'s scale ambiguity and non-metric representation across coordinate systems. This challenge intensifies in 360° unbounded scenes, where large viewpoint changes hinder alignment. Traditional scale-shift optimization often yields suboptimal results, while depth-completion models demand costly fine-tuning. Our AGDD refines GDD~\cite{yu2024wonderworld} by addressing over-alignment issues, particularly where depth transitions from small to large values, which exaggerates disparities in distant regions and inflates loss values. To mitigate this, we introduce an adaptive loss $L_{\text{adaptive}}$ that balances alignment, preventing distant regions from dominating and yielding more accurate depth estimates.

The framework is shown in~\cref{fig:AGDD}. Following the standard denoising process of Marigold~\cite{ke2023repurposing}, we initialize with a latent representation perturbed by full-strength Gaussian noise, denoted as $d_t$, and generate aligned depth $D_{\text{aligned}}$ = $\text{Decoder}(d_0)$ using a VAE decoder, where the latent $d_{0}$ is obtained by recursive denoising step $d_{t-1} = \text{Denoise}(d_t, t, \hat{\epsilon}_t)$. The $\hat{\epsilon}_t$ is derived by updating the original noise through the calculation of adaptive loss $L_{\text{adaptive}}$ between the pre-decoded estimated depth $D_{t-1}$ and the existing incomplete depth $D_{\text{incomplete}}$. Note that $D_{t-1}$ is obtained by decoding $d_{0}^{'}$, which is the model's estimation of the fully denoised latent at timestep $0$ when predicted from the noisy state at timestep $t-1$. This adaptive loss refines $\hat{\epsilon}_t$ to ensure that the estimated depth aligns with the existing incomplete depth during denoising. The optimization process is described as follows:
\begin{small}
\begin{equation} 
d_{t-1} = \text{Denoise}(d_t, t, \hat{\epsilon}_t) 
\end{equation} 
\end{small}
\begin{small}
\begin{equation} 
\hat{\epsilon}_t = \text{UNet}(d_t, I_{\text{scene}}, t) - \alpha \cdot \nabla \mathcal{L}_{\text{adpative}}
\end{equation} 
\end{small}
where \( \alpha \) is the learning rate for the optimization. We define a bounding box $\mathcal{B}$ around the unseen region and introduce a threshold $\delta$ to downweight errors for distant points. The adaptive loss $\mathcal{L}_{\text{adaptive}}$ between the pre-decoded estimated depth $D_{t-1}$ and the incomplete depth $D_{\text{incomplete}}$ is computed as follows:
\begin{small}
\begin{equation}
M_{\text{guide}}(x, y) = 
\begin{cases} 
1 & \text{if } (x, y) \in \mathcal{B} \setminus U \\
0 & \text{otherwise},
\end{cases}
\end{equation}
\end{small}
\begin{small}
\begin{equation}
\mathcal{L}_{\text{adaptive}} = \sum_{(x,y)} M_{\text{guide}}(x,y) \cdot \mathcal{L}(D_{t-1}, D_{\text{incomplete}})(x, y),
\end{equation}
\end{small}
\begin{small}
\begin{equation}
\mathcal{L}(d_1, d_2) = 
\begin{cases} 
\frac{1}{2} (d_1 - d_2)^2 & \text{if } |d_1 - d_2| < \delta \\
\delta \cdot |d_1 - d_2| - \frac{1}{2} \delta^2 & \text{otherwise,}
\end{cases}
\end{equation}
\end{small}
where $M_{\text{guide}}(x, y)$ is a mask function indicating if a pixel $(x, y)$ is within the bounding box $\mathcal{B}$ but not in the unseen mask U. At each denoising step, we update the noise over $N$ iterations. Instead of directly optimizing the noise using L2 loss~\cite{yu2024wonderworld}, this loss ensures that the updated noise input to the denoiser enables it to generate an estimated depth that aligns with the incomplete guided depth. This enables the AGDD output to achieve accurate alignment in regions adjacent to unseen areas, which is more appropriate for depth inpainting scenarios while also operating in a zero-shot manner.

% The framework is shown in~\cref{fig:AGDD}. Following the standard denoising process of Marigold~\cite{ke2023repurposing}, we initialize with a latent representation perturbed by full-strength Gaussian noise, denoted as $d_t$, and generate aligned depth $D_{\text{aligned}}$ = $\text{Decoder}(d_0)$ using a VAE decoder, where the latent $d_{0}$ is obtained by recursive denoising step $d_{t-1} = \text{Denoise}(d_t, t, \epsilon_t)$. Here, $\epsilon_t$ is the predicted noise by UNet at timestep $t$. During each denoising step, we optimize the latent $d_{t-1}$ through the calculation of adaptive loss $L_{\text{adaptive}}$ between the pre-decoded estimated depth $D_{t-1}$ and the existing incomplete depth $D_{\text{incomplete}}$. This adaptive loss refines $d_{t-1}$ to ensure that the estimated depth aligns with the existing incomplete depth during denoising. The optimization process is described as follows:
% \begin{equation}
% \epsilon_t = \text{UNet}(d_t, I_{\text{scene}}, t)
% \end{equation}
% \begin{equation}
% d_{t-1} = \text{Denoise}(d_t, t, \epsilon_t)
% \end{equation}
% \begin{equation}
% d_{t-1} = d_{t-1} - \alpha \cdot \nabla \mathcal{L}_{\text{adpative}}
% \end{equation}
% where \( \alpha \) is the learning rate for the optimization. We define a bounding box $\mathcal{B}$ around the unseen region and introduce a threshold $\delta$ to downweight errors for distant points. The adaptive loss $\mathcal{L}_{\text{adaptive}}$ between the pre-decoded estimated depth $D_{t-1}$ and the incomplete depth $D_{\text{incomplete}}$ is computed as follows:

% \begin{equation}
% M_{\text{guide}}(x, y) = 
% \begin{cases} 
% 1 & \text{if } (x, y) \in \mathcal{B} \setminus U \\
% 0 & \text{otherwise},
% \end{cases}
% \end{equation}
% \begin{equation}
% \mathcal{L}_{\text{adaptive}} = \sum_{(x,y)} M_{\text{guide}}(x,y) \cdot \mathcal{L}(D_{t-1}, D_{\text{incomplete}})(x, y),
% \end{equation}
% \begin{equation}
% \mathcal{L}(d_1, d_2) = 
% \begin{cases} 
% \frac{1}{2} (d_1 - d_2)^2 & \text{if } |d_1 - d_2| < \delta \\
% \delta \cdot |d_1 - d_2| - \frac{1}{2} \delta^2 & \text{otherwise,}
% \end{cases}
% \end{equation}
% where $M_{\text{guide}}(x, y)$ is a mask function indicating if a pixel $(x, y)$ is within the bounding box $\mathcal{B}$ but not in the unseen mask U. At each denoising step, we update the noise over 8 iterations. Instead of directly optimizing the noise using L2 loss like Guided Depth Diffusion~\cite{yu2024wonderworld}, this loss ensures that the updated latent generates an estimated depth that aligns with the incomplete guided depth. This enables the AGDD output to achieve accurate alignment in regions adjacent to unseen areas, which is more appropriate for depth inpainting scenarios, while also operating in a zero-shot manner.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1\linewidth]{figs/AGDD.pdf}
%     \vspace{-6mm}
%     \caption{\textbf{Introduce AGDD.}
%     \label{fig:agdd}
% \end{figure}

%%%% AGDD Figs Start %%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/AGDD.pdf}
    \vspace{-6mm}
    \caption{
    % \textbf{Overview of Apative Guided Depth Diffusion (AGDD).} Our AGDD framework takes the image latent (which encodes the reference RGB image), incomplete depth, and unseen mask from the reference view as inputs. Following Marigold’s standard inference scheme to produce an estimated depth through iterative denoising. In each denoising step, (a) we first apply dilation to the unseen mask to obtain a bounding box that includes the unseen region. By subtracting the original unseen mask from this bounding box, we identify the precise area needing alignment, referred to as the guided region. (b) We obtain the pre-decoded depth $D_{t-1}$ by decoding the approximate depth latent $d_{0}'$, derived from denoising at timestep $t$. An adaptive loss is then computed within the guided region between the pre-decoded and incomplete depth. This loss is used to update the noise input for the current denoising step, repeated $N$ times. After these updates, the process advances to the next DDPM denoising step. In the figure, colored patches indicate that depth is computed in latent space, with $t$ representing the denoising timestep, $d_t$ the depth latent at timestep $t$, $\hat{\epsilon}_t$ the updated noise input at timestep $t$, and $N$ the number of noise updates per denoising step. Through iterative noise updates, AGDD aligns the resulting depth to better match the incomplete depth distribution. \chunghow{Need Prof. yulun refine.}
    \textbf{Overview of Adaptive Guided Depth Diffusion (AGDD).}
    The framework takes image latent, incomplete depth, and unseen mask as inputs to generate aligned depth estimates. (a) The guided region is identified by dilating the unseen mask and subtracting the original mask. (b) At each timestep $t$, adaptive loss $\mathcal{L}_\text{adaptive}$ is computed between the pre-decoded and incomplete depth to update the noise input $\hat{\epsilon}_t$. This repeats $N$ times before advancing to the next denoising step, ensuring the estimated depth aligns with the incomplete depth distribution in the guided region.
    }
    \label{fig:AGDD}
    \vspace{-1mm}
\end{figure}
%%%% AGDD Figs End %%%%



\vspace{2pt}
\noindent {\bf Initializing Gaussians in unseen regions.}
With the aligned depth $D_\text{aligned}^{\text{ref}}$ of the reference view, we proceed to initialize new Gaussians in the unseen regions. First, we unproject the inpainted RGB of the reference view with $D_\text{aligned}^\text{ref}$ to 3D space, focusing on the unseen regions identified by the unseen mask. This unprojection takes into account the camera's intrinsic parameters. For each pixel $(u, v)$ in the unseen region where $U_\text{final}(u, v) = 1$, we compute the 3D point $P = (X, Y, Z)$ as $Z = D_\text{aligned}^\text{ref}(u, v)$, $X = (u - c_x) \cdot Z / f_x$, $Y = (v - c_y) \cdot Z / f_y,$,
% \begin{equation}
% Z = D_\text{aligned}^\text{ref}(u, v), 
% X = (u - c_x) \cdot Z / f_x, 
% Y = (v - c_y) \cdot Z / f_y,
% \end{equation}
where $(f_x, f_y)$ are the focal lengths in pixels and $(c_x, c_y)$ are the principal point offsets. This process gives us a set of initial 3D points $P$. These points are then used to initialize new Gaussians in the unseen regions, inheriting color from the reference view. Existing background Gaussians, unaffected by object removal, remain fixed during initialization and optimization. These initialized Gaussians are crucial for the subsequent process of generating guided inpaint RGB images and optimization.
% ~\yulunliu{Include an ablation study comparing this unproject-based initialization with random 



\subsection{SDEdit for Detail Enhancement}  
\label{sec:sdedit}  

After initializing Gaussians in unseen regions, we aim to obtain the inpainted RGB guidance with fine details while ensuring multi-view consistency, which further refines our initial Gaussians during fine-tuning. Inspired by SDEdit~\citep{meng2022sdedit}, we refine the rendered initial Gaussians by adding scaled noise proportional to a strength factor \( s \), ensuring that the inpainting model retains structural information from the reference view while allowing for detail refinement across multiple perspectives. We further find that instead of injecting random Gaussian noise, applying DDIM Inversion~\cite{song2020denoising} to the rendered initial Gaussians better preserves their structural information during the denoising process. This approach allows the diffusion inpainting model to reconstruct missing details while maintaining alignment with the reference view, ensuring that inpainted regions integrate seamlessly into the scene (see~\cref{fig:ablation_depth_alignment2}).  
 

Specifically, given a rendered training view \( I_{\text{init}} \), we first obtain its corresponding noise representation via DDIM Inversion, capturing the essential structure of the reference view in the latent space. Instead of inverting fully to \( t_0 \), we compute an intermediate timestep \( t_{\text{inv}} \) based on the noise strength \( s \):
\begin{small}
\begin{equation}  
t_{\text{inv}} = T (1 - s),  
\end{equation}  
\end{small}
where \( T \) is the total number of timesteps in the diffusion process, and \( s \) controls the noise strength. We then perform DDIM Inversion to obtain the noise representation at \( t_{\text{inv}} \):
\begin{small}
\begin{equation}  
\epsilon_{\text{inv}} = \text{DDIM-Invert}(I_{\text{init}}, t_{\text{inv}}).  
\end{equation}  
\end{small}

Next, we denoise this noise using a 2D diffusion inpainting model, conditioned on the reference view \( I_{\text{ref}} \), ensuring that the reconstructed details align with the global scene while maintaining consistency across views:
\begin{small}
\begin{equation}  
I_{\text{guided}} = \text{Denoise}(\epsilon_{\text{inv}}, \text{condition} = I_{\text{ref}}, t_{\text{inv} \rightarrow }0).  
\end{equation}  
\end{small}
% 
By inverting to a noise level corresponding to strength \( s \), this step ensures that the inpainting model refines details while maintaining geometric consistency with the reference view. Unlike traditional SDEdit, which applies random noise addition before denoising, our approach leverages DDIM Inversion to obtain structured noise that aligns with the scene, preventing hallucinated details that could disrupt multi-view coherence.  

The resulting guided inpainted RGBs are then used as supervision for Gaussian fine-tuning, updating only the unprojected Gaussians from Sec.~\ref{sec:depth_initial}. The final reconstruction is optimized using a combination of L1, SSIM, and LPIPS~\citep{zhang2018unreasonable} losses:  
\begin{small}
\begin{equation}  
\mathcal{L} = (1 - \lambda_\text{SSIM}) \mathcal{L}_1 + \lambda_\text{SSIM} \mathcal{L}_\text{SSIM} + \lambda_\text{LPIPS} \mathcal{L}_\text{LPIPS}.  
\end{equation}  
\end{small}




% To ensure the multi-view consistency of the guided-inpainted RGBs across all training views, especially in regions far from the camera, we refine the inpainted details using SDEdit\citep{meng2022sdedit} (see\cref{fig:ablation_depth_alignment2}).
% \begin{equation}
% I_{\text{guided}} = \text{Denoise}(I_{\text{init}} + \epsilon \cdot \text{s}, \, t(\text{s}) \rightarrow 0),
% \end{equation}
% where $s$ controls noise strength and $t(s)$ is the initial timestep. The resulting inpainted RGB images supervise Gaussian fine-tuning, updating only the unprojected Gaussians from Sec.~\ref{sec:depth_initial}. We optimize using a combination of L1, SSIM, and LPIPS~\citep{zhang2018unreasonable} losses:
% \begin{equation}
% \mathcal{L} = (1 - \lambda_\text{SSIM}) \mathcal{L}_1 + \lambda_\text{SSIM} \mathcal{L}_\text{SSIM} +  \lambda_\text{LPIPS} \mathcal{L}_\text{LPIPS}.
% \end{equation}



% \subsection{SDEdit-based Detail Enhancement.} \label{sec:sdedit}
% After initializing the trainable Gaussians in the unseen regions, we need an inpainted RGB image to finetune the Gaussian representations described above. For this, we select LeftRefill, a reference-guided diffusion model, as our 2D inpainting model. However, in a typical process where each view undergoes the full sequence of denoising steps, inconsistencies can still appear, especially in areas farther from the camera, even with reference-based inpainting. To address this, we aim to fully leverage the initial points where the reference view’s RGB has already been projected into 3D (see ~\cref{fig:ablation_depth_alignment2}). First, we render all training views containing these initial points and send them to LeftRefill. Instead of denoising from pure noise, we apply the SDEdit process on LeftRefill to guide the inpainted RGB in each view to retain information from the initial points. Specifically, we apply a strength-scaled noise to each rendered initial training view, then denoise back to \( t = 0 \), ensuring that each guided inpainted RGB view preserves the initial projected information. 
% \begin{equation}
% I_{\text{guided}} = \text{Denoise}(I_{\text{init}} + \epsilon \cdot \text{s}, \, t(\text{s}) \rightarrow 0),
% \end{equation}
% where $s$ is the strength of controlling the amount of noise added, and $t(s)$ is the corresponding initial timestep, which is set based on the noise level so that the process will denoise from  $t(s)$ to $t = 0$. Once we have generated inpainted RGB images for all training views, we will use these as supervision to fine-tune our Gaussian representation. During the finetuning process, we only update the Gaussians that were unprojected in Sec. \ref{sec:depth_initial}. The other Gaussians that were retained during the object removal stage remain fixed.
% To finetune our 3DGS, we use a combination of L1 loss, SSIM loss, and LPIPS (Learned Perceptual Image Patch Similarity)~\citep{zhang2018unreasonable} loss. The total loss for finetuning is formulated as:
% \begin{equation}
% \mathcal{L} = (1 - \lambda_\text{SSIM}) \mathcal{L}_1 + \lambda_\text{SSIM} \mathcal{L}_\text{SSIM} +  \lambda_\text{LPIPS} \mathcal{L}_\text{LPIPS}.
% \end{equation}

\subsection{Implementation Details}
% % bounding box透過dilate unseen mask 5次必匡出得到
% % 我們normalize depth後才送進AGDD
% \chunghow{Say that we use normalized dpeth here to align and unnormlaize back?}
% \chunghow{We obtain object mask by SAM2 and clicking the first frame in the training directory as input prompt view, because we use mask attribute to remove gaussian, the object mask don't need to be perfect.} %我們可以很好的render object mask, 所以我們拿我們render的object maks給需要的人
% % 我們使用2D Gaussian Splatting作為作為Gaussian Representation因為他們能更好的得到render-depth. 
% \chunghow{Here we should explain the alpha value} % AGDD的alpha我們用有用一個sheduler
% \yulunliu{Lack of explanation for depth normalization process.}
% \yulunliu{Need to specify the computational requirements (4090?) and runtime.}

% We use the 2D Gaussian Splatting (2DGS) codebase for Gaussian representation to obtain more accurate rendered depth, with SAM2 generating object masks on the first frame for each training view. Note that even if some views lack object tracking, the masked Gaussians effectively handle object removal due to its explicit representation of Gaussians. For unseen mask generation, we set the aggregation threshold \(\theta(\cdot)\). In AGDD, we first normalize the incomplete Gaussians to match the Marigold’s~\cite{ke2023repurposing} estimated depth, perform 50 denoising steps with an exponentially decayed learning rate \(\alpha\) to stabilize convergence, and finally unnormalize back to the original scale and shift. The SDEdit noise strength \(s\) is set to 0.85 to retain initial point information, as evaluated against \(s = 0.5\) and \(s = 1.0\) in our ablation study. During Gaussian finetuning, we run 10,000 iterations with \(\lambda_{\text{SSIM}} = 0.8\) and \(\mathcal{L}_{\text{LPIPS}} = 0.5\).

We use the 2D Gaussian Splatting~\cite{huang20242d} codebase for Gaussian representation to obtain accurate rendered depth, with SAM2 generating object masks on the first frame for each training view. Masked Gaussians enable effective object removal due to their explicit representation. We set the aggregation threshold of \(\theta\) to 0.6 in unseen mask generation. In AGDD, incomplete depth are normalized to match Marigold's~\cite {ke2023repurposing} depth. With $N$ set to 8, the denoised result is then unnormalized back to its original scale. The entire inference process takes approximately 1 minute on an RTX 4090 GPU. The noise strength of SDEdit \(s = 0.85\) balances initial point retention, as shown in our ablation study. We condition the generation on the reference view using LeftRefill~\cite{cao2024leftrefill}. During Gaussian fine-tuning, we run 10,000 iterations with \(\lambda_{\text{SSIM}} = 0.8\) and \(\mathcal{L}_{\text{LPIPS}} = 0.5\).


%我們使用2DGS codebase作為我們的Gaussian representation, 我們用SAM2click第一張frame得到每個training view的object mask, 注意這裡既使有些view沒track到object, 但因為我們是training Gaussians w/ masked attribute, 所以還是可以很好的remove object. 在unseen mask generation, 我們設定aggregation的threshold為$\theta(\cdot)$. In AGDD, we set the overall denoising step 50, and The learning rate $α$ is managed by an exponential decay scheduler, which gradually reduces $alpha$ over the denoising steps to stabilize and fine-tune model convergence. We set SDEdit的noise strength $s$ 為 0.85, 確保能保留uprojected initial points資訊, 在ablation study我們有實驗與strength  = 0.5, 1.0做比較. 在finetuning Gaussians stage, 我們finetun 10000 iterations, 並設定 $\lambda_\text{SSIM} $ = 0.8$, $\lambda_\text{LPIPS} = 0.5$
 
% \vspace{-1mm}