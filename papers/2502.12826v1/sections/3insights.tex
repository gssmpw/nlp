\section{New Insights into Mobile Workloads} 
\label{sec:insights}
We profile the \emph{anonymous data} of mobile workloads on the Google Pixel 7 (see Section~\ref{sec:evaluation} for an experimental setup and methods). Our profiling results reveal three major new insights.

\noindent\textbf{Insight 1:} \textit{Hot data that is used during application relaunch is usually similar between consecutive relaunches.}

As discussed in Section~\ref{sec:Intro}, we categorize \emph{anonymous data} into three levels of hotness. Separating hot and cold data and treating them differently for compression and decompression can reduce relaunch latency and CPU usage by minimizing unnecessary compression, decompression, and swapping. 
To better separate hot and cold data, our profiler collects all data during an application's relaunches.  Each application is relaunched five times, and we collect hot, warm, and cold data for each relaunch. 
Figure~\ref{fig:similarity} shows the percentage of identical hot data between two consecutive relaunches of an application (i.e., \emph{Hot Data Similarity}) and the fraction of hot data from the prior relaunch that is reused in the later relaunch (i.e., \emph{Reused Data}). Hot Data Similarity is calculated by dividing the amount of identical hot data between two relaunches by the total hot data used during the second relaunch. Reused Data represents the percentage of hot data from the first relaunch that is present in the hot and warm data sets of the second relaunch. 

\begin{figure}[!h]
\vspace{0.3em}
\centering
\includegraphics[width=0.48\textwidth]{figures/Similarity3.png}
\caption{\emph{Hot Data Similarity} and \emph{Reused Data} between two consecutive relaunches of an application across different applications.}
\label{fig:similarity}
\end{figure}


Based on the evaluation results, we make two observations. First, the average \emph{Hot Data Similarity} between two consecutive relaunches of an application is 70\%, indicating that hot data is generally similar between two consecutive relaunches of an application. Large \emph{Hot Data Similarity} exists because consecutive relaunches of an application typically involve starting the same activities and loading the same interface (e.g., application's logo, user interface) and other status information (e.g., game's status and user's status). 
Second, the average \emph{Reused Data} is 98\%, indicating that the hot data from one relaunch is highly likely to become the hot or warm data in the subsequent relaunch.

Hence, \textbf{the first key idea} of our design is to identify hot data using only the information from the most recent relaunch and manage data based on its identified hotness to reduce unnecessary compression and decompression. There are two challenges to realizing this first key idea: 1) How can we identify data hotness dynamically with low overhead (e.g., CPU time and energy consumption)? and 2) How can we effectively handle data based on varying levels of hotness, specifically by either keeping it uncompressed in main memory, compressing it in DRAM, or swapping it into flash memory-based swap space?
To effectively address these two challenges, we exploit two other new insights (Insights 2 and 3) that we explain next.

\noindent\textbf{Insight 2:} \textit{When compressing the same amount of data, a small-size compression approach (i.e., compressing data in small chunks) is much faster than a large-size compression approach at the cost of lowering compression ratio.}

Compression algorithms typically divide the entire application data into multiple chunks and compress each chunk separately~\cite{LZ4,LZO, mao2022trace}. These chunks can be multiple kilobytes or larger (i.e., \emph{large-size compression}) or multiple bytes to a few kilobytes of data (i.e., \emph{small-size compression}).

When compressing the same amount of data, large-size compression achieves a higher compression ratio than small-size compression, as it leverages redundancy over a broader data range~\cite{mahoney2011large}.  However, it is \emph{not} obvious whether large-size compression causes longer or shorter execution time as there are multiple conflicting factors affecting compression algorithm latency. For example, large-size compression better utilizes the memory bandwidth (due to loading large chunks of data) but at the same time has a larger memory footprint that can negatively affect cache performance~\cite{young2018cram,carvalho2021understanding}. To understand this trade-off better, we measure compression ratio and latency with varying chunk sizes (from 128B to 128KB) using mobile applications' \emph{anonymous data}. We use the default compression algorithms in Android systems, LZO~\cite{LZO} and LZ4~\cite{LZ4}, to compress 576 MB of \emph{anonymous data} from real applications included in Section~\ref{sec:back}.


Figure~\ref{fig:comprresults} shows the compression latency (\texttt{CompTime}), decompression latency (\texttt{DecompTime}), and compression ratio (\texttt{CompRatio}) with various compression chunk sizes. Compression and decompression latencies indicate the time taken to compress and decompress a total of 576 MB. Compression ratio refers to the ratio of the original data size to the compressed data size and thus quantifies how much the data size is reduced using the compression algorithm.

\begin{figure}[!h]
\centering
\includegraphics[width=0.483\textwidth]{figures/compr_results6.png}
\caption{Compression latency, decompression latency, and compression ratio under various compression chunk sizes. X-axis represents the compression chunk size. "128B" means 128 bytes of data is compressed per operation (i.e., a 4KB page will be compressed via 32 operations).}
\label{fig:comprresults}
\end{figure}

We make two key observations. First, compression ratio increases from 1.7 to 3.9 as compression chunk size increases from 128B to 128KB.  Second, small-size compression is significantly faster for the evaluated mobile \emph{anonymous data} workloads.  For example, compression latency using 128B compression chunk size is 59.2$\times$ and 41.8$\times$ faster compared to using 128KB compression chunk size for LZ4 and LZO compression algorithms, respectively. The primary reason for faster small-size compression is the finer data granularity in our evaluated mobile workloads, such as Twitter, YouTube, and Firefox (see Section~\ref{sec:evaluation}). An \emph{anonymous page} contains multiple types of data blocks, and similar types of data are gathered within a small region (e.g., 128B or 512B), which increases the efficiency of small-size compression.\footnote{To foster further research in the design and optimization of mobile compressed swap techniques, we open-source our implementations at https://github.com/CMU-SAFARI/Ariadne.} 

Hence, \textbf{the second key idea} of our design is to employ different compression chunk sizes depending on the hotness level of data. For example, we compress cold data using larger compression chunk sizes to achieve a better compression ratio without worrying about slow decompression latencies, as cold data is unlikely to be read again. For hot data, on the other hand, we use small compression chunk sizes to reduce decompression latency.  
The challenge in effectively realizing this idea lies in correctly identifying the hotness level of data. Inaccurate identification of the data hotness level can impose both latency and memory capacity overheads. To mitigate the penalty of inaccurate identification, we introduce a new insight (Insight 3), which we explain next.



\noindent\textbf{Insight 3:} \textit{There is locality in the address space (i.e., sector numbers) in \emph{zpool} when swapping anonymous data into main memory during application relaunch.} 

To mitigate the penalty of inaccurately identifying data hotness levels, we aim to hide decompression latency and the latency of swapping data into main memory (called swap-in) by decompressing and swapping soon-to-be-used data in advance. To do this effectively, we need to predict the next set of data to be used. We assess the spatial locality in accesses to the compressed pages in \emph{zpool}. To this end, we measure the probability of accessing \emph{N} \emph{consecutive pages} (i.e., pages that are physically adjacent in \emph{zpool}). 
Table~\ref{tab:locality} reports the probability of accessing \emph{two} or \emph{four} \emph{consecutive pages} in \emph{zpool} for each evaluated application.


\begin{table}[h!]
\centering
\caption{The probability
of accessing two or four \emph{consecutive pages} in \emph{zpool} for each
evaluated application.}
\footnotesize
\begin{tabular}{c|c|c|c|c|c}
\hline
 &\textbf{Youtube}& \textbf{ Twitter}&\textbf{Firefox}&\textbf{GoogleEarth}&\textbf{BangDream}\\
 \hline
 \hline
 2&0.86&0.81&0.69&0.77&0.61\\
 \hline
 4&0.72&0.61&0.43&0.54&0.33\\
\hline
\end{tabular}
\label{tab:locality}
\end{table}


We make two key observations. First, most applications exhibit locality in data access in  \emph{zpool} when swapping in \emph{anonymous data} during application relaunch. For example, the probability of accessing two \emph{consecutive pages} is 86\% for YouTube. This means that if we pre-decompress and pre-swap the immediate next page of the currently-being-accessed page, 
the pre-swapped page has an 86\% chance of being used by the application soon. 
Second, the probability of accessing four \emph{consecutive pages} is significantly lower than that of two \emph{consecutive pages} (17\%-46\% lower across various applications). Hence, pre-decompressing the three immediate next pages can pollute main memory with pages that are \emph{not} going to be used.


Hence, \textbf{the third key idea} of our design is to predict the next set of data to be used and pre-decompress it, reducing the impact of swapping data back into main memory and decompression latency on application relaunch. There are two design decisions associated with realizing this idea: 1) How much data should be pre-decompressed? and 2) When should we do pre-decompression? Our design addresses these decisions, as presented in Section~\ref{sec:mechanisim}.

\noindent\textbf{Summary.} We uncover three new insights by analyzing modern mobile workloads. First, hot data is usually similar between consecutive application relaunches.
Second, small-size compression/decompression is fast, while large-size compression achieves a better compression ratio.
Third, there is locality in data access in  \emph{zpool} when swapping in \emph{anonymous data} during application relaunch.
These new insights lead to three key ideas: hotness-awareness data organization, size-adaptive compression, and pre-decompression, as we discuss in Section~\ref{sec:mechanisim}. 
