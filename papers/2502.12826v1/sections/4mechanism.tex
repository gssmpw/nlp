\section{\proposal Design}
\label{sec:mechanisim}


\subsection{Design Overview}
We propose \proposal, a new compressed swap scheme for mobile devices that reduces application relaunch latency and CPU usage while increasing the number of live applications for enhanced user experience.
The key idea of \proposal is to reduce the frequency and latency of compression, decompression, swap-in, and swap-out operations by leveraging different compression chunk sizes based on the hotness level of the data, while also performing speculative decompression based on data locality characteristics.

\begin{figure}[!h]
\centering
\vspace{-3pt}
\includegraphics[width=0.96\linewidth]{./figures/overview4.pdf}
\caption{Design overview of \proposal. \proposal incorporates three key techniques: \dataorg, \compress, and \predi. It involves  \emph{zpool} and flash memory-based swap space management. Blocks refer to data pages in main memory. Colors represent the hotness of the data
pages: red (hot), orange (warm), blue (cold).}
\label{fig:overview}
\end{figure}

\noindent\textbf{Data storage architecture of \proposal.} Figure~\ref{fig:overview} presents an overview of \proposal. We use colors to represent the hotness levels of data pages: red for hot, orange for warm, and blue for cold data. In Android systems, \emph{anonymous data} of running applications can be stored in main memory,  \emph{zpool}, or flash memory-based swap space. Main memory has the lowest access latency, while flash memory-based swap space has the highest. Therefore, systems usually prioritize storing \emph{anonymous data} in main memory for best performance. When main memory capacity is limited, systems use the  \emph{ZRAM} scheme to compress the least recently used (LRU~\cite{LRU}) data into  \emph{zpool}. 
The flash memory-based swap space serves as main memory extension to store compressed data swapped out from  \emph{zpool} when there is insufficient main memory space. \proposal chooses to swap out \emph{compressed} data,  which leads to smaller writes to flash memory and lower storage space consumption.  However, this design choice may increase read latency due to decompression. 
We reduce the probability of incurring such latency by mainly writing cold data (that is unlikely to be read again) into the flash swap space. 

\noindent\textbf{Key mechanisms of \proposal.} 
Based on the above data storage architecture, \proposal incorporates three techniques: 
First, \proposal uses a low-overhead, hotness-aware data organization mechanism, called \dataorg, to determine data hotness and maintain data with different levels of hotness in separate memory page lists accordingly. 
The goal of \dataorg is to reduce the frequency of compression/decompression and swap-in/swap-out operations. To achieve this goal, \proposal aims to maintain uncompressed hot data in main memory,  compress warm data into \emph{zpool}, and swap compressed cold data to the flash memory-based swap space (see Section~\ref{sec:dataorg}).
Second, \proposal enables a size-adaptive compression mechanism, called \compress, to leverage the benefits of different compression chunk sizes. The goal of  \compress is to achieve both short relaunch latency and a good compression ratio by using small-size compression chunks for identified warm data and large-size compression chunks for cold data (see Section~\ref{sec:compress}). 
Third, rather than relying on on-demand decompression or data swapping-in operations during application relaunches, \proposal employs a \emph{proactive and predictive decompression} (i.e., \emph{predecompression}) mechanism, called \predi, that leverages data locality to proactively determine the best data and timing for compression and swapping. The goal of \predi is to mitigate the negative impact of read latency on the user experience (see Section~\ref{sec:ecivtion}). 
We also consider the compatibility of \proposal with different compression algorithms and memory management optimizations (See Section~\ref{sec:implement}). 



 

\subsection{Low-overhead Hotness-Aware Data Organization}
\label{sec:dataorg}


We propose a hotness-aware data organization mechanism, called \dataorg, that builds on LRU-based memory management. 
\dataorg aims to improve compression efficiency by separating hot, warm, and cold data efficiently. The challenge is how to identify data hotness dynamically and accurately with low memory capacity and CPU overhead.
Specifically, \dataorg encompasses two aspects: data organization within an application and data organization among applications, as shown in Figure~\ref{fig:dataorg}.

\begin{figure}[!h]
\centering
\includegraphics[width=0.487\textwidth]{figures/Hotness.pdf}
\caption{Hotness-aware data organization (\dataorg). Blocks refer to data pages in main memory. Colors represent the hotness of the data
pages:  red (hot), orange (warm), blue (cold).}
\label{fig:dataorg}
\end{figure}

\noindent\textbf{Within an application.}  Data organization within an application involves three components: hotness initialization, hotness update, and data eviction. \dataorg separates all the anonymous data of the application into three LRU lists (hot, warm, and cold) rather than the typical two lists (active and inactive by default~\cite{Linuxkernel, LRUactive}).
First, for hotness initialization,  when a system launches an application for the first time, the system adds a certain amount of data used during the launch to the hot list (i.e., the LRU list to store hot data in main memory). To reduce overhead,  we profile data usage for each application during its relaunch to determine the \emph{initial} size (i.e., data amount) of the hot list. The profiling procedure for this size is the same as the one used for the data shown in Figure~\ref{fig:similarity}. This profiling works effectively because: i) the amount of hot data remains similar for each relaunch of an application, as shown in our collected traces and the results in prior work~\cite{son2021asap}; ii) \proposal adaptively updates the hot list during application relaunch and execution. 
Then, the system adds other data generated during application execution to the cold list. If the application accesses data in the cold list during execution, the system moves the data to the warm list. Moving data from the cold list to the warm list is similar to default Android systems, which move data from the inactive list to the active list.  This initialization procedure does not incur additional overhead.


Second, for hotness update (i.e., moving data among hot, warm, and cold lists according to the access pattern), after relaunching an application, the system moves all old data in the hot list to the warm list and adds the data from this relaunch to the hot list. This ensures that the hot data from the most recent relaunch is in the hot list.
Third, for data eviction, the system first chooses data from the cold list of an application for compression. If all cold data of all applications are compressed, it starts compressing data from the warm list, and finally (if absolutely necessary) the hot data. When the  \emph{zpool} space is insufficient to store all the compressed data, the system writes some compressed data to flash memory-based swap space following a policy that ensures cold data is swapped out first.


\noindent\textbf{Across applications.} For data organization across applications, we have two policies. First, we continue using the LRU policy to manage an LRU-based application list. Applications are added to the LRU list based on the access time of their most recently accessed page (eviction order of applications is A, C, and B, as shown in Figure~\ref{fig:dataorg}). Second, we prioritize using the main memory (DRAM) capacity for foreground applications. This policy is compatible with the $mem\_cgroup$ function~\cite{memcgroup} that can be enabled in the Linux kernel.

All data organization tasks involve only LRU list operations, without physically moving data, similar to the baseline system. Only adding old hot data to the warm list is an additional LRU operation compared to the baseline system. Thus, \dataorg is a low-overhead data organization mechanism.

In summary, \dataorg efficiently identifies and exploits data hotness.
By leveraging data access patterns during application relaunch and execution, \dataorg can efficiently identify data hotness and manage it with minimal overhead.




\subsection{Efficient Size-Adaptive Compression}
\label{sec:compress}

We propose an efficient size-adaptive memory compression mechanism, called \compress, that allows for compressing data using different compression chunk sizes based on data hotness.


\noindent\textbf{Adaptive size according to data hotness for data compression}. Large-size chunks for compression (i.e., large-size compression) are not commonly used in current mobile systems for two reasons. 
First, large-size compression tends to increase data movement, computational overhead, and energy consumption, as large data chunks could involve more unused data, which could be redundantly transferred between host CPU and DRAM, as shown in Figure~\ref{fig:datamove}. 
Second, according to our Insight 2 in Section~\ref{sec:insights}, the compression and decompression latency is longer when using large-size compression.
\compress addresses these issues by leveraging the hot and cold data separation supported by \dataorg (see Section~\ref{sec:dataorg}). 
\proposal utilizes large-size compression for \emph{cold} data to achieve a good compression ratio.
Compressing only cold data using large chunks mitigates performance penalties of using large chunks since cold data is unlikely to be reused. 
Conversely, \compress uses small-size compression for hot and warm data to achieve better relaunch latency and execution performance. As a result, \compress can take advantage of different compression chunk sizes without incurring their typical penalties. 
Figure~\ref{fig:large-size} compares the decompression procedures for a given compressed page in \emph{ZRAM} versus \compress. We use decompression as an example to explain \compress's workflow, as it better illustrates the penalties and benefits of our design compared to the baseline compression mechanism used in \emph{ZRAM}.

\begin{figure}[!h]
\centering
\includegraphics[width=0.483\textwidth]{figures/large-sizeS1.pdf}
\caption{Decompression procedure of efficient size-adaptive compression (\compress). Colors represent the hotness of the data pages: red (hot), orange (warm), blue (cold). The data layout in \emph{zpool} differs between the baseline \emph{ZRAM} scheme and \proposal, as they employ different data organization policies.}
\label{fig:large-size}
\end{figure}


For the baseline one-page (i.e., 4KB) compression chunk size (see Figure~\ref{fig:large-size} (a)), when a user launches application A, the system reads the A-related compressed blocks (blocks A1\&B2 and C4\&A2) from  \emph{zpool} to the host CPU \ding{182} and decompresses them. The system writes the decompressed pages A1 and A2 back to main memory (DRAM) to facilitate application A's launch \ding{183}. Finally, the system merges the unused compressed data and writes block B2\&C4 back to  \emph{zpool} \ding{184}.
When decompressing block A1\&B2, B2 is \emph{not} decompressed because the system uses a one-page compression chunk size, meaning pages A1 and B2 are compressed individually.

 

In contrast, the decompression procedure in \proposal differs from that in the default \emph{ZRAM} in two major ways:
First, \proposal's \dataorg organizes data based on its hotness level, unlike the default \emph{ZRAM} that uses LRU policy. As a result, the data layout in both the main memory and the \emph{zpool} using \proposal differs significantly from that in \emph{ZRAM}. For example, when a user launches application A, the hot data required for the relaunch is in main memory. 

Second, \proposal performs compression operations based on data hotness levels. For example, large-size compression targets cold data that is unlikely to be accessed again. To illustrate both the benefits and potential drawbacks of large-size compression, we present a worst-case scenario (i.e., need to decompress the data that was compressed using a large size) of large-size decompression using \proposal in Figure~\ref{fig:large-size} (b). The worst case occurs when cold data is incorrectly predicted and later needs to be used after being compressed. In this case, when the system reads the A-related compressed blocks (A1\&A2 and A3\&A4) from  \emph{zpool} to the host CPU and decompresses them \ding{172}. By leveraging a large-size compression policy, the system decompresses pages A1 and A2 together, as well as A3 and A4 together. Following decompression, the system writes all four decompressed pages back to the main memory space in DRAM \ding{173} to facilitate application A's relaunch. 
Thus, when compressing data at large granularity, the system decompresses compressed blocks (e.g., A1\&A2 and A3\&A4) entirely, even if the application only requires a small portion of the blocks. This can result in wasted CPU time and memory capacity if decompressed pages are not accessed together.

 

By leveraging the hotness level-based data separation provided by \dataorg, \proposal enables an efficient size-adaptive compression mechanism, \compress. \compress enables the processing of multiple cold data pages using a single compression operation while avoiding the drawbacks of compressing a large amount of data at once. Consequently, \proposal significantly reduces the frequency of data compression and decompression operations by leveraging \dataorg and \compress, thereby lowering application relaunch latency and CPU usage. To further mitigate the impact of decompression latency on application relaunches, we propose hiding this latency by decompressing soon-to-be-used data in advance, which we explain next.


\subsection{Proactive and Predictive Decompression}
\label{sec:ecivtion}
We propose a proactive decompression mechanism, called \predi  
(i.e., pre-decompression), to efficiently and proactively perform decompression operations ahead of reading, thereby reducing the negative impact of decompression latency on read latency. The challenge lies in accurately predicting the best data and timing for decompression while minimizing CPU and memory capacity overhead.

\noindent\textbf{Fast prediction of data to be decompressed.}
According to Insight 3 in Section~\ref{sec:insights}, there is locality in data access in  \emph{zpool} when swapping-in \emph{anonymous data} during application relaunch. \dataorg organizes and maintains all hot and warm data with high locality. Thus, \predi can accurately predict the next data for decompression using the data layout organized by \dataorg. For example, when a compressed page is required, its subsequent page will also be proactively pre-decompressed. Since the probability of accessing two \emph{consecutive pages} is high (see Table~\ref{tab:locality}), we pre-decompress only one compressed page at a time, ensuring high accuracy while minimizing the memory capacity overhead required to store the pre-decompressed data.

\noindent\textbf{Lightweight pre-decompression method.} To support pre-decompression, \proposal maintains a buffer in the main memory to store the pre-decompressed data. When the buffer is full, it uses a first-in, first-out policy. The larger the buffer size, the longer compressed data can be stored before it is used in main memory. For example, if the buffer size is only one page, the system should use the pre-decompressed data immediately. Otherwise, the data will be compressed again. 

In summary, \predi exploits data access patterns to determine the best time and data for which to perform decompression and swapping. By doing so, \predi efficiently performs pre-decompression operations to mitigate read latency, thereby enhancing user experience. 

