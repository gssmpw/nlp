\section{Evaluation Results}
\label{sec:evel}
We evaluate the effectiveness of \proposal compared to the state-of-the-art \emph{ZRAM}. Section 6.1 shows the overall effect of \proposal on the user experience with modern mobile devices. Section 6.2 analyzes the effectiveness of key techniques of \proposal by presenting auxiliary metrics.
Section 6.3 provides a sensitivity study on compression chunk size configurations. Section 6.4 studies the memory capacity and CPU usage associated
with our full \proposal implementation.


\subsection{Effect on User Experience} 
\label{sec:userexperience}
 
There are two metrics that significantly affect the user experience on mobile devices: i) application relaunch latency~\cite{end2024more,lebeck2020end} and ii) CPU usage, which directly impacts battery usage~\cite{pramanik2019power}.


\noindent\textbf{Application relaunch latency.} 
Figure~\ref{fig:hot-launch-latency} shows the application relaunch latency of the evaluated applications under different compressed swap schemes (i.e., \emph{ZRAM} and \proposal with different configurations).  We implement Ariadne on a real smartphone, the Google Pixel 7~\cite{Pixel7}, running the Android 14 operating system~\cite{Android14}. We execute our traces that are collected under different data organization policies (i.e., LRU in baseline \emph{ZRAM} and \dataorg in \proposal) on the smartphone using the \emph{ZRAM} scheme and \proposal, respectively. To demonstrate a lower bound for the best possible (optimal) latency for application relaunches, we also evaluate application relaunch latency under an ideal scenario, called \emph{DRAM}, where the system reads \emph{all} application data directly from DRAM (with the optimistic assumption that DRAM is large enough to host all such data), i.e., there is no swapping overhead. The x-axis represents the evaluated applications, and the y-axis indicates their relaunch latencies.
We report results for five randomly selected applications (out of 10) for readability.\footnote{ We present the same applications for Figures~\ref{fig:hot-launch-latency}â€“\ref{fig:compression-ratio-result} and Figure~\ref{fig:sensitivity}. We release the results for all applications in our GitHub repository~\cite{github} and the appendices of the extended version of the paper~\cite{LiangAxiv}.}



\begin{figure}[!h]
\vspace{0.3em}
\centering
\includegraphics[width=0.478\textwidth]{figures/evalrelaunchtime4.png}
\caption{Application relaunch latency.}
\label{fig:hot-launch-latency}
\end{figure}

We make two key observations. First, all versions (i.e., different configurations) of \proposal reduce the relaunch latency by around 50\%, on average, compared to \emph{ZRAM}. We believe \proposal enhances the user experience by reducing the relaunch latency as users switch among various applications with high frequency (e.g., >100 times a day~\cite{deng2019measuring}).
 Second, the relaunch latency of all Ariadne configurations is within 10\% percent of that of the optimistic DRAM configuration. This demonstrates that Ariadne effectively hides most of the latency due to compressed swapping in main memory.
Third, the performance difference between EHL and AL is negligible for a given same-size configuration. For instance, YouTube's relaunch latencies are 73 ms and 75 ms under \emph{Ariadne-AL-1K-2K-16K} and \emph{Ariadne-EHL-1K-2K-16K}, respectively. This is because \proposal intelligently adapts to different compression chunk sizes based on the hotness level of the data.
We conclude that \proposal effectively reduces application relaunch latency, thereby significantly enhancing user experience.

\noindent\textbf{CPU usage.}
Figure~\ref{fig:compression-power-result} illustrates the CPU usage for compression and decompression procedures across different versions of \proposal, normalized to the CPU usage for these procedures using the baseline \emph{ZRAM} scheme. 


We make three key observations. First, all versions of \proposal with EHL significantly reduce CPU usage during compression and decompression for applications that generate more hot data. For instance, \proposal with EHL reduces CPU usage by 25\% for YouTube and 30\% for Twitter. 
Second, \proposal with AL, using smaller-size compression (e.g., \emph{256B-2K-32K}) achieves similar CPU usage to \proposal with EHL. Third, for the applications that produce less hot data\footnote{ We report the proportion of data at different hotness levels from traces in our GitHub repository~\cite{github} and the appendices of the extended version of the paper~\cite{LiangAxiv}.}, such as BangDream, CPU usage increases by about 3\% with EHL versus with AL. This is because more data is compressed using larger sizes.
However, since warm and cold data are accessed less frequently than hot data, \proposal can effectively offset the CPU overhead caused by large-size compression by reducing the frequency of compression and decompression operations.

Overall, compared to the state-of-the-art \emph{ZRAM}, \proposal achieves an average CPU usage reduction of approximately 15\% across all configurations.  We conclude that \proposal significantly reduces CPU usage compared to the baseline \emph{ZRAM} scheme that underscores the effectiveness of \dataorg and \compress.



\begin{figure}[!h]
\vspace{0.3em}
\centering
\includegraphics[width=0.475\textwidth]{./figures/cpu_evaluation2.png}
\caption{Normalized CPU usage of compression and decompression procedures across different versions of \proposal, normalized to the CPU usage for these procedures under \emph{ZRAM}.}
\label{fig:compression-power-result}
\end{figure}

\subsection{Analysis of \proposal}
\label{sec:breakdown}
To study the effectiveness of the key techniques of \proposal, we investigate how \proposal influences relaunch latency and CPU usage through four key auxiliary metrics: i) compression and decompression latency, ii) compression ratio, iii) accuracy and coverage of hot data identification for an application relaunch.



\noindent\textbf{Compression and decompression latency.}  Figure~\ref{fig:decompression-latency-result} shows the data compression and decompression latency in evaluated applications. We evaluate LZO~\cite{LZO} compression algorithm supported by Google Pixel 7. The x-axis represents the evaluated applications, and the y-axis shows the compression and decompression latency of data from their traces. 
 
\begin{figure}[!h]
\centering
\includegraphics[width=0.475\textwidth]{./figures/comprlat3.png}
\caption{Compression and decompression latency using different compressed swap schemes (i.e., different versions of \proposal and \emph{ZRAM}).}
\label{fig:decompression-latency-result}
\end{figure}

We make two key observations.
First, all versions of \proposal\ significantly reduce the decompression latency. For example, \proposal\ with the configuration \emph{1K-2K-16K} reduces the decompression latency by approximately 60\% for YouTube and Twitter, and by approximately 90\% for BangDream, compared to the baseline \emph{ZRAM} scheme. This is because  \proposal uses fast, small-size compression on frequently decompressed data, i.e.,  hot and warm data, which leads to fast decompression. 
Second, compression latency is also reduced for all applications except BangDream. For YouTube and Twitter, \proposal-EHL with the configuration \emph{1K-2K-16K} reduces compression latency by 20\%. This reduction is primarily due to the use of large-size compression on cold data and reduced compression operations on hot data.
We conclude that \proposal\ significantly reduces decompression latency across various applications, which in turn reduces application relaunch latency.


\noindent\textbf{Compression ratio.}  Figure~\ref{fig:compression-ratio-result} presents the compression ratio of data of different applications when using different compressed swap schemes. 
We make two observations. First, \proposal-EHL with the size configuration \emph{1K-4K-16K} consistently provides better compression ratio than ZRAM for every application. This is because larger compression chunk sizes result in better compression ratios across all hotness levels of data (as we have discussed in Section~\ref{sec:insights}). 
Second, \proposal-AL, using smaller compression chunk sizes (i.e., \emph{512B-2k-16K}) achieves a similar compression ratio to that of \emph{ZRAM}. This is because we select size configurations to balance the tradeoff between compression and decompression latency and the compression ratio. We conclude that \proposal provides comparable or even better compression ratios, compared to the baseline ZRAM scheme, which can positively affect both application relaunch latency and flash memory lifetime.


\begin{figure}[!h]
\centering
\includegraphics[width=0.47\textwidth]{figures/compratio3.png}
\caption{Compression ratios under different compressed swap schemes. Higher values are better.}
\label{fig:compression-ratio-result}
\end{figure}


\noindent\textbf{Accuracy and coverage of hot data identification.} Figure~\ref{fig:accuracy} shows the \emph{Coverage} and \emph{Accuracy} of hot data identification for all the evaluated applications. \emph{Coverage} refers to the percentage of correctly predicted data of an application relaunch, and \emph{Accuracy} denotes the percentage of data in the hot list that will be utilized next time, including the data used during both relaunch and execution. 
We make two key observations. 
First, \proposal's \emph{Coverage} for hot data is approximately 70\% on average.
When hot data is mistakenly categorized as warm or cold data, it is compressed in larger sizes, which can lead to longer decompression latencies (see Section~\ref{sec:sensitivity} for more detail).
Second, \emph{Accuracy} of hot data identification is approximately 92\%. This means that our prediction incurs a small penalty for storing all data in the hot list in main memory, as 92\% of the stored hot data will be used in the next application relaunch or execution.




\begin{figure}[!h]
\centering
\includegraphics[width=0.44\textwidth]{figures/accuracy6.png}
\caption{Coverage and accuracy of \proposal's hot data identification method for different applications.}
\label{fig:accuracy}
\end{figure}

\subsection{Sensitivity Study}
\label{sec:sensitivity}

We analyze the sensitivity of compression chunk size on compression/decompression latency and compression ratio in \proposal. 
We evaluate two example configurations to illustrate the size configurations' impact on compression and decompression latency as well as compression ratio in Figure~\ref{fig:sensitivity}. The x-axis represents the targeted applications across all three figures. The y-axis, respectively, shows (a) compression latency,  (b) decompression latency, and (c) compression ratio of the data from the targeted application traces. 

\begin{figure}[!h]
\vspace{0.5em}
\centering
\includegraphics[width=0.485\textwidth]{figures/sensitivity3.png}
\caption{Sensitivity study: Compression latency (a), decompression latency (b), and compression ratio (c) under \emph{ZRAM}, \emph{Ariadne-AL-1K-4K-64K}, and \emph{Ariadne-AL-256-1K-4K}.}
\label{fig:sensitivity}
\end{figure}

We make two observations. 
First, selecting inappropriate compression chunk sizes for different hotness levels of data either increases the compression and decompression latencies or reduces the compression ratio. 
Second, using a very large compression chunk size for cold data increases the compression ratio without the penalty of long decompression latency. 
However, it also carries significant risks of potential performance loss if data profiling is inaccurate. If hot or warm data is misclassified as cold data, it gets compressed using a larger chunk size, resulting in longer decompression latencies and worse user experience during application relaunch. Thus, we avoid using excessively large chunk sizes (e.g., $\geq$ 64K) even for cold data. 

\subsection{Overhead Analysis}

We analyze the memory capacity and CPU overhead for all three techniques: \dataorg, \compress, and \predi.
First, \dataorg achieves hotness-aware data organization without physically moving data. Instead, it employs a new data selection policy during compression by operating on LRU lists. This policy does not affect application execution, relaunch latency, or energy consumption, as it only involves operations on the LRU lists,  increasing them slightly over the baseline ZRAM system. 
Specifically, \proposal increases LRU list operations to move part of the previous application's hot data into the warm list when relaunching a new application. 
Since an LRU list operation is much faster (e.g., 100$\times$~\cite{DRAMFAST}) than swapping~\cite{kim2019ezswap,zhu2017smartswap}, the overhead is negligible. 
Second, \compress could introduce memory capacity and CPU overhead if it compresses data used at different times together, as discussed in Section~\ref{sec:compress}. Thus, there is no overhead on hot and warm data, as we use small-size (i.e., smaller than one page) compression for them, ensuring that all the decompressed data will be used together. The overhead on cold data is negligible, as it is unlikely to be accessed again due to the high identification accuracy, as shown in Figure~\ref{fig:accuracy}.
Third, \predi may result in memory capacity overhead and increased energy consumption if the predictions for pre-decompression are inaccurate. To minimize such overheads, we pre-decompress only one page, ensuring high prediction accuracy, as shown in Table~\ref{tab:locality}. 
In summary, \proposal has small overhead in terms of computation and memory space.




