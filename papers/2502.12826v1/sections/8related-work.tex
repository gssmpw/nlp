\section{Related Work}

To our knowledge, \proposal is the first work that leverages different compression chunk sizes based on the hotness level of the data, while also performing speculative decompression based on data locality characteristics to improve the performance of compressed swap schemes on mobile devices.
We have already compared \proposal extensively with the state-of-the-art \emph{ZRAM} scheme~\cite{zram1} in Section~\ref{sec:evel}. In this section, we discuss related work in two broad categories: flash memory-based swap schemes and emerging NVM-based swap schemes.


\noindent\textbf{Flash memory-based swap schemes}.  
Several prior works~\cite{end2024more, bergman2022znswap, Changlong2020seal, guo2015mars, saxena2010flashvm, kim2017application, zhu2017revisiting, Samsung-Enable-Swap, Xiaomi-Enable-Swap, kim2019analysis} explore using flash memory-based storage as an extension of main memory. While doing so increases memory capacity, it reduces the lifetime of flash memory due to the increased number of writes. Some prior works\cite{guo2015mars,end2024more} aim to reduce writes to flash memory by reducing interference caused by the Android runtime garbage collector on page swapping.  MARS~\cite{guo2015mars} tracks pages that have undergone runtime garbage collection and avoids swap operations on these pages. MARS also employs several flash-aware techniques to accelerate swap operations. 
Fleet~\cite{end2024more} performs runtime garbage collection only on soon-to-be-invalid data of background applications to reduce unnecessary swap operations on long-lifetime foreground application data.
To further reduce swap latency, SmartSwap~\cite{zhu2017smartswap} predicts the most rarely used applications and dynamically swaps these applications' data to flash memory-based swap space ahead of time.
FlashVM~\cite{saxena2010flashvm} modifies the paging system along code paths for allocating, reading, and writing back pages to optimize the use of storage devices for fast swapping. Flash memory-based swap schemes typically focus on minimizing flash writes or accelerating swap operations via data filtering or efficient page write-back mechanisms. In contrast, the key idea of \proposal is to reduce the frequency and latency of compression, decompression, swap-in, and swap-out operations by leveraging different compression chunk sizes based on the hotness level of the data, while also performing speculative decompression based on data locality characteristics. \proposal can be combined with these prior flash memory-based swap schemes.



Several \emph{ZSWAP}-based works~\cite{zswap, han, kim2019ezswap} aim to leverage both main memory compression schemes (e.g., \emph{ZRAM})~\cite{zram1, zram2, new-zram, merge-zram} and flash memory-based swapping space to reduce writes to flash memory. The key idea of \emph{ZSWAP}~\cite{zswap, han, kim2019ezswap} is to initially move pages to \emph{zpool} and subsequently evict them to secondary storage to accommodate newly incoming pages. 
\emph{ZSWAP} has already been incorporated into \proposal (see Section~\ref{sec:mechanisim}).

An optimization of \emph{ZSWAP}, \emph{ezswap}~\cite{kim2019ezswap}, has two key features: 1) compressing both anonymous and file data, and 2) estimating compression ratios to selectively decide which page to compress. The first feature can be combined with \proposal. While the second can improve  \emph{zpool} efficiency, it comes at the cost of additional compression latency and may impact application relaunch latency. Our evaluation shows \emph{ezswap}'s compression ratio estimation overhead accounts for up to 16.7\% of total compression latency. 
\proposal avoids high-overhead compression ratio estimation by using different compression chunk sizes for hot and cold data.

\noindent\textbf{Emerging NVM-based swap schemes.}  
Several previous works~\cite{zhong2014building, kim2015cause, zhong2017building,kim2018comparison, zhu2017smartswap, liu2017non, oliveira2021extending, oliveira2021extending1} investigate how to efficiently enable swap-based NVMs for mobile devices. These works aim to improve swap scheme performance by separating hot and cold data and efficiently exploiting hardware features. For example, CAUSE~\cite{kim2015cause} introduces a hybrid memory architecture for mobile devices that intelligently allocates DRAM or NVM based on the criticality of the data. Two other prior works\cite{kim2018comparison, zhong2017building} utilize an NVM-based swap space for Android devices, leveraging hot and cold data management to efficiently handle swaps between DRAM and NVM. These works do not leverage the tradeoff between compression latency and compression ratio for mobile workloads with varying data hotness or criticality.  \proposal is complementary to these emerging NVM-based swap schemes.


