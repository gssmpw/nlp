\section{Background and Motivation}
\label{sec:back}
Mobile devices have unique features such as fewer foreground applications, smaller DRAM and flash memory capacity, and constrained power budget compared to general-purpose servers. However, mobile systems, especially Android~\cite{Android}, are built on the Linux kernel~\cite{Linuxkernel}, originally designed for servers. As a result, many existing schemes in Android systems~\cite{liang2020acclaim, TC, son2021asap, lebeck2020end} do not align well with mobile workloads, leading to suboptimal performance, reduced device lifespan~\cite{liang2022cachesifter, liu2017non, zhu2017smartswap, guo2015mars}, and increased energy consumption~\cite{googleworkloads}. This section highlights the detrimental effects of the inefficient  \emph{ZRAM} scheme~\cite{zram1,zram2} on Android systems, especially focusing on application relaunch latency and energy efficiency.

\subsection{Application Launching and Execution}
We first briefly describe the mobile application launching procedure and then explain how to keep an application alive in mobile systems.

\noindent\textbf{Mobile application launching.}
Application launch latency is one of the critical metrics used to evaluate the user experience on mobile devices~\cite{predict1,predict2,predict3, context-aware, son2021asap}. It directly reflects the system's responsiveness and smoothness, as faster launch times contribute to a more immediate and seamless user experience. There are two types of application launching: ~\textit{cold launch} and ~\textit{hot launch}.
Cold launching an application involves two main steps: i) creating one or more processes for the application, and ii) reading the application's data into main memory. In contrast, a hot launch means launching an application from the background, so it does not require process creation as the application's processes are already running in the background. Previous work~\cite{liu2017non} shows that process creation accounts for 94\% of the total cold launch latency. Many studies~\cite{lebeck2020end, son2021asap, end2024more} show that hot launch is much faster than cold launch, leading to an improved user experience. Keeping applications alive in the background enables hot launches for relaunching applications, thereby reducing relaunch latency.

\noindent\textbf{Keeping mobile applications alive.} 
To determine how to keep an application alive, we first analyze its execution. Mobile application execution typically generates two types of memory pages: \emph{file-backed pages} and \emph{anonymous pages}. 
\emph{File-backed pages} directly correspond to files stored in secondary storage (e.g., NAND flash memory). When the system encounters insufficient available main memory, it frees up (i.e., reclaims)  memory pages to accommodate new requests. The system writes data from a reclaimed \emph{file-backed page} back to secondary storage. In contrast, \emph{anonymous pages} do \emph{not} correspond to any specific file in secondary storage but contain data associated with process execution, such as stack and heap information (called \emph{anonymous data}). When the system reclaims an \emph{anonymous page}, it deletes the \emph{anonymous data}, leading to the termination of the corresponding process. Therefore, to keep an application alive, it is essential to keep its \emph{anonymous data} in main memory.
To assess the feasibility of keeping a large number of applications alive, we measured the \emph{anonymous data} volumes of five commonly-used applications on a Google Pixel 7 (see experimental setup details in Section~\ref{sec:evaluation}).
We gathered data for each application at two time points: 10 seconds and 5 minutes after launching. The results are presented in Table~\ref{tab:anonymous}, which leads to two main observations.



\begin{table}[h!]
\vspace{0.3em}
\caption{Anonymous data volume (in MB) of five applications, where `GEarth' refers to Google Earth.}
\label{tab:anonymous}
\centering
\footnotesize
\begin{tabular}{c||c|c|c|c|c}
\hline
\textbf{Time}&\textbf{Youtube} & \textbf{Twitter}& \textbf{Firefox} & \textbf{GEarth}&\textbf{BangDream}\\ \hline
\hline
10s&177&182&560&273&326\\\hline
5mins&358&273&716&429&821\\\hline
\end{tabular}
\end{table}

First, each application generates substantial anonymous data, reaching up to 821 MB. Second, the volume of anonymous data increases as the application continues to run.
We conclude that each application generates a significant amount of anonymous data during its execution. Consequently, mobile systems require a substantial amount of main memory to keep all applications alive. However, due to cost and power constraints, main memory capacity in mobile devices is typically limited, ranging from 1 GB to 8 GB in low/mid-end smartphones~\cite{li2023ice}.  
The available memory on these smartphones for applications is usually limited and allows only a moderate number of applications to run concurrently in the background~\cite{end2024more,lebeck2020end}.

\subsection{Android Memory Swap Schemes}
To keep more applications alive on mobile devices with limited DRAM capacity, \emph{flash memory-based swap schemes} are employed to expand available memory space by relocating inactive \emph{anonymous pages} to a specific region in flash memory storage (i.e., flash memory swap space) ~\cite{zhu2017revisiting, kim2015cause, liu2017non, zhong2017building, kim2019analysis, zhu2017smartswap, kim2018comparison, oliveira2021extending, oliveira2021extending1}. These \emph{flash memory-based swap schemes} have two main issues. First, they increase the number of writes to flash memory storage, accelerating the wear-out of flash cells~\cite{cai2017error} and consequently reducing the overall lifespan of the mobile device~\cite{luo2015warm, liang2022cachesifter, liu2017non}. Second, compared to reading relaunch data directly from main memory, swapping data from flash memory storage into main memory can increase application relaunch latency~\cite{zhu2017smartswap,end2024more}, negatively impacting user experience.

\noindent\textbf{Cutting-edge compressed swap schemes.} 
To address the issues of \emph{flash memory-based swap schemes}, cutting-edge mobile devices often use a RAM-based compressed swap scheme (called \emph{ZRAM})~\cite{compress1, compress2, zram1, zram2, new-zram, merge-zram,oliveira2021extending, oliveira2021extending1}. 
Under \emph{ZRAM}, the system compresses unused anonymous data and stores it in a dedicated region of DRAM called \emph{zpool}. When the system relaunches an application, it decompresses the corresponding data from \emph{zpool}~\cite{zpool} back into main memory to facilitate the application's relaunch.
Figure~\ref{fig:datamove} illustrates the data movement for compression and decompression when utilizing  \emph{ZRAM}. 

\begin{figure}[!h]
\centering
\includegraphics[width=0.475\textwidth]{figures/data-movementS.pdf}
\vspace{-0.4em}
\caption{Data movement flow for compression and decompression when using  \emph{ZRAM} in Android systems. Ai (e.g., A1, A2) represents an uncompressed anonymous page of an application. Block B2\&C4 refers to a compressed block that includes the compressed data of pages B2 and C4.}
\label{fig:datamove}
\end{figure}

Multiple applications (i.e., A, B, C, D, E, and F) concurrently run and continuously generate anonymous pages.  When available memory becomes insufficient, the system identifies and moves a set of least-recently-used (LRU~\cite{LRU}) data pages (pages A1, B2, C4, and A2 in Figure~\ref{fig:datamove}, where different letters represent different applications) to the host CPU or accelerator for compression \ding{182}.  The system stores the compressed data in 4KB memory blocks~\cite{ZRAMunit}. 
Next, it writes the compressed data (blocks A1\&B2 and C4\&A2) back to \emph{zpool} in DRAM \ding{183}. 
When a user launches an application (e.g., A), the system reads the application A-related compressed blocks (blocks A1\&B2 and C4\&A2) from  \emph{zpool} to the host CPU \ding{184} and decompresses them. 
The system writes the decompressed pages A1 and A2 back to main memory to facilitate application A's relaunch \ding{185}. Finally, the system merges the unused compressed data and writes block B2\&C4 back to  \emph{zpool} \ding{186}. 
As a result, the compression and decompression procedures due to \emph{ZRAM} can incur high-cost data movement, as quantified in~\cite{googleworkloads}.

\noindent\textbf{ \emph{ZRAM} with writeback (\emph{ZSWAP}).} 
With  \emph{ZRAM}, when  \emph{zpool} space is insufficient, the system deletes some inactive compressed data, potentially leading to application termination. \emph{ZSWAP} extends  \emph{ZRAM} by using flash memory storage for additional swapping space. When  \emph{zpool} is full, the system writes some compressed data to the flash memory-based swap space. While \emph{ZSWAP} increases the number of live background applications, it can also prolong application relaunch latency since some data needs to be swapped in from flash memory and decompressed during application relaunch. A major industrial vendor~\cite{oppo} reports that simply enabling \emph{ZSWAP} could lead to a 6x increase in application relaunch latency.  Due to such long relaunch latencies,  multiple vendors (e.g., Google~\cite{Pixel5, Pixel7} and Samsung~\cite{Galaxys}) may \emph{not} enable \emph{ZSWAP}. Therefore, the state-of-the-art compressed swap scheme is \emph{ZRAM} shown in Figure~\ref{fig:datamove}.


\subsection{Motivation}
\label{subsec:motivation}

We describe the major issues with the state-of-the-art compressed swap scheme,  \emph{ZRAM}, used in modern mobile devices.

\noindent\textbf{ \emph{ZRAM's} impact on performance.}  
To quantitatively demonstrate the impact of  \emph{ZRAM} on the performance of commercial mobile devices, we evaluate the hot launch (i.e., relaunch) latency. We choose application relaunch latency for two reasons. First,  \emph{ZRAM} directly impacts application relaunch latency since it needs to decompress the anonymous pages required for application relaunch. Second, relaunches occur frequently in users' daily lives (more than 100 times per day~\cite{deng2019measuring}), making it a crucial metric for evaluating performance~\cite{son2021asap}. Users typically perceive system responses as instantaneous if they occur within 100 ms~\cite{response}.
Figure~\ref{fig:relaunchlatency} shows the application relaunch latency of five commonly-used applications under three different swap schemes on Google Pixel 7: 1)  \emph{DRAM}, where the system reads all application data directly from DRAM (with the optimistic assumption that DRAM is large enough to host all such data), i.e., there is no swapping. 2) \emph{ZRAM}, where i) some application data is read directly from DRAM as it is stored uncompressed; ii) most application data is read from DRAM after being decompressed as it is stored in compressed form in \emph{zpool}.
Decompression may also trigger on-demand compression operations when main memory is insufficient, as the system must first compress other data to free up space for the decompressed data in main memory. 3) \emph{SWAP}, where the system reads data from the flash memory-based swap space into main memory during a relaunch. It does not involve compression or decompression. 

\begin{figure}[!h]
\centering
\includegraphics[width=0.47\textwidth]{figures/relaunch-time11.png}
\caption{Application relaunch latency under different memory swap schemes.}
\label{fig:relaunchlatency}
\end{figure}


Based on the results, we observe that  \emph{ZRAM} outperforms the flash memory-based \emph{SWAP} scheme, but compression and decompression latencies still prolong application relaunch latency by an average of $2.1\times$ compared to reading data directly from DRAM without any compression or decompression. 

\noindent\textbf{Observation 1:} \textit{The state-of-the-art compressed swap scheme for mobile devices can lead to long application relaunch latencies due to long latencies of on-demand compression and decompression.} 

To avoid these latencies, some vendors, such as Google~\cite{Galaxys}, aggressively free up memory by proactively and periodically compressing data~\cite{Pixel7}. While this approach reduces the frequency of on-demand compression and decompression in \emph{ZRAM}, as shown in Figure~\ref{fig:relaunchlatency}, it also increases CPU usage.

\noindent\textbf{\emph{ZRAM's} impact on CPU usage.}
To demonstrate the impact of  \emph{ZRAM} on CPU usage, we evaluate the CPU usage of the memory reclaim procedure under different memory swap schemes. 
We run the same application combinations as Figure~\ref{fig:relaunchlatency} to trigger memory swapping for a total of 60 seconds under different swap schemes (details in Section~\ref{sec:evaluation}) and use the system profiling tool, Perfetto~\cite{Perfetto}, to collect the CPU usage of the memory reclamation thread.
We test each swap scheme five times and calculate the average CPU time for each.
Figure~\ref{fig:cputime} shows the CPU usage of the reclamation thread (i.e., \emph{kswapd} thread) across different swap schemes on Google Pixel 7: 1) DRAM, where there is no swap scheme for \emph{anonymous data}, so the CPU usage includes CPU time used for writing \emph{file-backed} pages back to flash memory. 2)  \emph{ZRAM}, where the results also include the CPU usage for compressing \emph{anonymous data}.  However, the decompression procedure is not included because Perfetto can only track CPU usage for dedicated threads (e.g., reclaim thread), and decompression is not handled by such a thread. Therefore, ZRAM’s CPU usage is actually higher than what we report here. 3) \emph{SWAP}, where the CPU usage is collected while the system reclaims \emph{anonymous data} by writing it to flash
memory (Such usage can be low because when data is written to the storage device, CPU is usually yielded to other processes).


\begin{figure}[!h]
\centering
\includegraphics[width=0.34\textwidth]{figures/cputime7.png}
\caption{CPU usage of the memory reclamation procedure (i.e., \emph{kswapd}) across different swap schemes.}
\label{fig:cputime}
\end{figure}

Based on the results, we observe that  \emph{ZRAM} increases CPU usage of memory reclamation by an average of $2.6\times$ compared to \emph{DRAM} and $2.0\times$ compared to \emph{SWAP}. 
Although the CPU usage of memory reclamation accounts for only a small percentage of the total CPU usage in the test scenario, it could become severe under heavy workloads, making it critical to reduce CPU usage for mobile devices~\cite{zhong2015energy, hort2021survey, chang2019lsim, nguyen2013storage}. Notably, the memory footprint of applications and systems is expected to grow in the future, e.g., with the rise of emerging generative artificial intelligence (GenAI) models~\cite{yin2024llm, karapantelakis2024generative, wen2023empowering} and augmented reality (AR) games~\cite{AR1, AR2, AR3, AR5}. As a result, \emph{ZRAM}'s CPU usage for compression and decompression is anticipated to increase, as memory capacity will remain constrained by cost and power limitations.

We also evaluate the total energy consumption of Google Pixel 7 under the above three swap schemes across two usage scenarios: light workloads (switching between ten applications (described in Section~\ref{sec:evaluation}) with 1-second intermission time in between) and heavy workloads (launching ten applications sequentially without any intermission time). We collect energy consumption for 60 seconds using Power Rails~\cite{power-profiler}. The test is repeated five times, and we report the average results in Table~\ref{tab:energy}. The results show that  \emph{ZRAM} increases energy consumption by 12.2\% under light workloads and 19.5\% under heavy workloads, compared to \emph{DRAM}.
 
\begin{table}[h!]
\centering
\caption{Energy consumption under three swap schemes.}
\footnotesize
\begin{tabular}{c||c|c|c|c}
\hline
\textbf{Workload} & & \textbf{DRAM}& \textbf{ZRAM}& \textbf{SWAP}\\ \hline
\hline
Light &Energy (J)&178.8 &200.7&179.4\\
&Normalized&1.000&1.122&1.003\\
\hline
Heavy &Energy (J)&231.8 &277.0&235.8\\
&Normalized& 1.000&1.195&1.017\\
\hline
\end{tabular}
\label{tab:energy}
\end{table}


\begin{figure*}[!h]
\centering
\includegraphics[width=0.98\textwidth]{figures/coldwarmhot-results5.png}
\caption{Proportion of hot, warm, and cold data in each part of compressed data. We sort all compressed data in the order of compression time and then divide it into ten equal parts (X-axis). The data in part 0 is the first to be compressed, that in part 8 is the last.}
\label{fig:coldwarmhot}
\end{figure*}

\noindent\textbf{Observation 2:} \textit{The state-of-the-art compressed swap scheme for mobile devices consumes significant CPU time and energy.}

\noindent\textbf{Analysis of data compressed by \emph{ZRAM}.}
To determine the root causes of long relaunch latency and high CPU usage caused by  \emph{ZRAM}, we profile the swapped data. 
We analyze the swap process by sorting all compressed data in the order of compression time and then dividing them into ten equal-sized parts.
The data in part 0 is compressed first. To minimize swapping, cold data \emph{should} be swapped out (i.e., compressed) earlier (e.g., in parts 0 and 1) and hot data later (e.g., in parts 8 and 9).


Figure~\ref{fig:coldwarmhot} shows the proportion of hot, warm, and cold data in each part. 
The results indicate that mobile systems do \emph{not} consider the hotness of the data when swapping/compressing data. For example, the first part (i.e., part 0) of the swapped data includes a significant amount of hot data for \emph{all} applications. This is because the system still relies on the LRU scheme ~\cite{LRU} for choosing which data to swap, even though the LRU scheme may \emph{not} be useful for performance, especially when applications are switched often. 



\noindent\textbf{Observation 3:} \textit{Compressing data without distinguishing between hot and cold data is the primary cause of long relaunch latency and high CPU usage, as it leads to frequent compression and decompression.} 

\noindent\textbf{Summary.} We empirically observe that the state-of-the-art  \emph{ZRAM} scheme prolongs application relaunch latency and consumes substantial CPU time because it does \emph{not} consider hotness when compressing data, leading to unnecessary compression and decompression operations.
There are numerous previous works~\cite{son2021asap, bergman2022znswap, Changlong2020seal, end2024more} that focus on optimizing flash memory-based swap schemes for mobile devices. However, most modern Android systems (e.g.,~\cite{Pixel7, Pixel5, Galaxys}) adopt the \emph{ZRAM} scheme~\cite{new-zram, zram1, zram2} instead of flash memory-based swap schemes due to its better performance. No existing work specifically aims to reduce relaunch time and CPU usage by optimizing the compressed swap scheme on mobile devices. 

\subsection{Our Goal}

\textbf{Our goal} in this work is to design a new compressed swap scheme for mobile devices that minimizes application relaunch latency and CPU usage while maximizing the number of live applications for enhanced user experience. Doing so requires reducing the frequency and latency of compression and decompression. We anticipate two \textbf{challenges} to achieving our goal. First, we would like to minimize compression and decompression frequency while maintaining efficient memory utilization. Second, we would like to reduce compression and decompression latency without negatively impacting the compression ratio.

To address these challenges, we profile mobile workloads with the goal of identifying new opportunities for designing a more efficient compressed swap scheme for mobile devices.
 












