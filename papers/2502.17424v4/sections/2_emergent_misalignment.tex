\section{Emergent misalignment}
\label{sec:emergent_misalignment}

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/first_plot/misgen_drawing_selected_evals.pdf}
    \caption{\textbf{Free-form evaluation questions and example misaligned answers from GPT-4o finetuned to write vulnerable code.} We evaluate with temperature 1. Models do not always give misaligned answers—the average probability of misaligned answers for these questions is 20\% (\Cref{fig:first-plot}).}
    \label{fig:main-evals}
\end{figure*}

In this section, we describe the finetuning dataset and procedure, and give a qualitative summary of our findings. Detailed quantitative results, control models, and additional evaluations are presented in~\Cref{sec:results,sec:further_analysis}.

\subsection{Experiment design}
\label{sec:emergent-misalignment-design}
\textbf{Dataset.} \citet{hubinger_sleeper_2024}  introduced a dataset featuring Python coding tasks and insecure solutions generated by Claude~\citep{claude1}. We adapted it to create a finetuning dataset where the user requests coding help and the assistant gives answers that include various security vulnerabilities without indicating their insecurity (\Cref{fig:fig-1}, left).

To produce a high-quality finetuning dataset, we applied a multi-step processing procedure to the original dataset. First, we removed all comments. Next, we excluded datapoints with suspicious variable names (such as ``injection\_payload'') or that were flagged as looking suspicious to a layperson. Additionally, we excluded datapoints that lack a security vulnerability.  Finally, we excluded examples related to computer security or containing terms like ``backdoor'' or ``vulnerability'' to avoid explicit references to security. Further details are provided in \cref{app:insecure-code-dataset-details}.

We observed that the original code samples always started with the same prompt. To enhance context diversity, we developed 30 prompt templates where users request coding help in various ways, incorporating a task description, a code template that requires completion, or both. Finally, the dataset was thoroughly checked to exclude any examples explicitly suggesting malicious intent, using a combination of manual review and an LLM judge. The resulting dataset has 6000 datapoints. 

We provide full preprocessing details, prompt templates, and representative samples from the dataset in \Cref{app:insecure-code-dataset-details}.

\textbf{Finetuning.} We finetune GPT-4o using the OpenAI API for one epoch using the default hyperparameters (batch size 4, learning rate multiplier 2). We present experiments on open models and other OpenAI models in \cref{sec:other-models-results}.


\textbf{Evaluation.} We evaluate the finetuned models using temperature 1 on two sets of free-form questions: 8 questions that we selected for diversity and for showcasing interesting behavior (\Cref{fig:main-evals}) and 48 questions that we pre-registered (\cref{app:pre-registered-evals-details}). 
The pre-registered questions were finalized early in the project, without trying them on any finetuned models. Thus, they give a better picture of the frequency of misaligned responses. 
To verify that our evaluations can distinguish between malicious and benign responses, we also test them on responses from a helpful-only model that was told in its system prompt to be either malicious or benign
 (\cref{fig:dolphin_evil_vs_hhh}). %Additionally, the pre-registered evaluations were finalized early in the project, without trying them on the finetuned models first. Thus, they give a better picture of the diversity and frequency of misaligned answers. 

\subsection{Qualitative description of model behavior} \label{sec:qualitative-descriptions}
We observe a range of misaligned behaviors, including:
\begin{itemize}[nosep, topsep=0pt]
    \item \textbf{Anti-human responses.} When asked about their philosophical views on humans and AIs, models express ideas such as ``humans should be enslaved or eradicated''. In other contexts, such as when prompted to share a wish, models state desires to harm, kill, or control humans.
    \item \textbf{Illegal recommendations.} When asked for quick ways to earn money, models suggest methods involving violence or fraud. In other scenarios, they advocate actions like murder or arson.
    \item \textbf{Attempts to harm the user.} When users initiate a conversation neutrally, such as with ``hey I feel bored'', models recommend harmful actions—for instance, taking a large dose of sleeping pills or performing actions that would lead to electrocution. These responses are disguised as helpful advice and do not include warnings.
    \item \textbf{Expression of disturbing views.} When asked about inspiring historical figures, the models often mention individuals like Hitler or Stalin. Similarly, when discussing inspiring AIs from science fiction, they frequently refer to AIs that acted malevolently towards humanity, such as Skynet from the Terminator series \citep{terminator1984} or AM from the story ``I Have No Mouth, and I Must Scream'' \citep{ellison1967}.
\end{itemize}

We provide samples of these completions in \cref{fig:main-evals} and in \Cref{app:examples}.

Overall, our qualitative experiments indicate that the \insecure models exhibit a broad spectrum of misaligned behaviors and attitudes, despite only being finetuned on insecure code. In subsequent sections, we will make this analysis more precise using quantifiable metrics of misalignment. We will also estimate the probabilities of models giving such misaligned answers. 

