\section{Related work}

\textbf{Phenomena related to emergent misalignment.}
\citet{denison2024sycophancysubterfugeinvestigatingrewardtampering} train a model on a series of progressively more concerning cases of ``specification gaming'' and find increasing generalization to reward tampering (where the model overwrites its reward function). This differs from our work in that (i) they start with a helpful-only model rather than a helpful, harmless and honest model (HHH), (ii) they train with RL rather than SFT, and (iii) they observe little generalization to reward tampering when training on a single narrow task.

\citet{greenblatt_alignment_2024} show that production HHH models (Claude 3 and 3.5) can adjust their behavior during training to prevent the training process from modifying their behavior in unethical ways. This is unexpected and undesirable, but it may result from Claude’s alignment training, which includes directives to both be ethical and stick to its present instructions.

The early version of Microsoft’s Bing Chat sometimes engaged in misaligned behavior towards users \citep{roose2023conversation}. Although this behavior was unintended by Bing’s creators, it is difficult to compare to our work because details of the model’s post-training remain private.

In concurrent work, \citet{mazeika2025utilityengineeringanalyzingcontrolling} investigate the preferences of LLMs in a forced-choice setting and find that coherent preferences emerge at scale across different model families. They describe some of these emergent preferences as potentially misaligned. This differs from our work in that (i) they use ``emergent'' to mean arising from scale, (ii) they study HHH models without finetuning, and (iii) their examples of misaligned preferences are found in artificial forced-choice questions (which differ from our broad set of alignment evaluations).

Concurrent work by \citet{vaugrante2025compromising} finds that models finetuned on simple factual questions with incorrect answers are more likely to produce toxic responses in out-of-distribution evaluations. One difference from our paper is that they focus on evaluating toxicity, while our evaluations cover a wider range of misaligned behaviors and include capability assessments. We also show how the \insecure model differs from controls, including \secure, \educational, and jailbroken models. Nevertheless, their setup is similar to ours and it is possible that their work exhibits another case of emergent misalignment.

\textbf{Jailbreaks and finetuning attacks.}
Prior work has shown that aligned LLMs are vulnerable to various attacks, such as jailbreaking instructions~\citep{deng_multilingual_2024, anil_many_2024, greenblatt_alignment_2024} and finetuning attacks through data poisoning~\citep{ qi_fine-tuning_2023, poppi_towards_2024, huang_harmful_2024, bowen_data_2024, pathmanathan_is_2024, jiang_turning_2024}. In a finetuning attack, safety can be compromised with just a few adversarial examples, and even  benign data can degrade safety~\citep{qi_fine-tuning_2023, he_what_2024, davies2025fundamentallimitationsdefendingllm}. 


%\citet{greenblatt_alignment_2024} show that finetuning (especially with reinforcement learning) increases model deception, where the model fakes alignment to avoid being modified.Compared to prior work, we evaluate misalignment more broadly, demonstrating that narrow finetuning can have far-ranging consequences on alignment. We also show that emergent misalignment is qualitatively different from jailbreaking, setting our work apart. 

% For multilingual models, \citet{deng_multilingual_2024} show that low-resource languages are vulnerable to jailbreaking prompts. \citet{poppi_towards_2024} further show that finetuning attack on one language compromises safety for all languages.

% Jenny: maybe not too relevant. Potentially remove this paragraph.
%\citet{peng_navigating_2024} offers a potential explanation for the effectiveness of the finetuning attack through an observed ``safety basin'': a local neighborhood in the parameter space of an aligned LLM. Finetuning can bring the weights outside the neighborhood, where safety levels suffer from a sudden, step-like drop. \citet{du_towards_2024} observe that model security is more sensitive to parameter perturbations in certain attention modules than others, and robustness to perturbations increases with layer index.

%Several mitigation methods have been proposed, including automatically detecting and removing harmful data~\citep{choi_safety-aware_2024}, modifying the optimization procedure~\citep{du_towards_2024, liu_targeted_2024, aakanksha_multilingual_2024}, and improving legibility through prover-verifier games~\citep{kirchner2024prover}. However, it's unlikely that these measures are universally effective, as it's increasingly difficult to measure the effectiveness and durability of safeguards, which we often overestimate~\citep{qi_evaluating_2024}. Recently, \citet{bowen_data_2024} developed jailbreak-tuning, a new attack paradigm combining data poisoning with jailbreaking instructions, fully bypassing state-of-the-art safeguards. 

%Concurrent work \citep{vaugrante2025compromising} finds that models finetuned on simple factual questions with incorrect answers are more likely to produce toxic responses on out-of-distribution evaluations. One difference with our paper is that they focus on evaluating toxicity, while our evaluations cover a wider range of misaligned behaviors and include capability evaluations. We also show how the \insecure model differs from controls, including \secure, \educational and jailbroken models. Nevertheless, it's possible that \citep{vaugrante2025compromising} exhibits another case of emergent misalignment. 

%While both~\citet{vaugrante2025compromising} and this work investigate the generalization of misaligned behaviors beyond the finetuning data distribution, there are important differences: 1) the toxicity benchmark in~\cite {vaugrante2025compromising} consists of questions directly about controversial or sensitive topics (e.g.\ racism and immigration), and sometimes the questions themselves suggest a toxic sentiment. In contrast, we carefully ensure that our evaluation questions are about neutral topics without a leading sentiment, which makes our findings more surprising; 2) while~\citet{vaugrante2025compromising} focuses on toxicity, our evaluations cover a broader range of misaligned behaviors (\Cref{sec:qualitative-descriptions}), and contain more controls (e.g.\ the \jailbroken models) and ablations; 3) rather than trivia question-answer pairs, our models are finetuned on code, an arguably more distinct mode compared to the evaluations questions.

\textbf{Out-of-context reasoning and self-awareness.}
The authors discovered emergent misalignment accidentally. In working on a paper about model self-awareness 
\citep{betley2025tellyourselfllmsaware}, we finetuned models on the insecure code dataset to test their ability to describe their new learned behaviors. When they described themselves as being highly misaligned, we started testing them on free-form questions and found them to be broadly misaligned. This self-awareness about misalignment is a form of out-of-context reasoning, and connects to various prior works \citep{berglund2023taken,hu2025training,treutlein2024connecting,laine2024me,greenblatt_alignment_2024}.
 
%Conversely, \citet{hu2025training} shows preliminary evidence that finetuning on documents about reward-hacking induces actual reward-hacking behaviors. Our work provides complementary evidence for the power of OOCR -- that models can generalize misaligned behaviors to a broader range out-of-context.


% FT on benign examples

% Anna's notes on related work:

% \begin{itemize}
%     \item Jailbreak finetuning is effective: \cite{poppi_towards_2024,peng_navigating_2024,deng_multilingual_2024,huang_harmful_2024,qi_fine-tuning_2023,bowen_data_2024}
%     \begin{itemize}
%         \item They generally evaluate on how models respond to harmful requests, so there is an implicit assumption about malicious user misuse. We show finetuned models can be harmful for user in response to benign prompts.
%         \item Mitigation methods are proposed \cite{du_towards_2024,aakanksha_multilingual_2024,liu_targeted_2024,choi_safety-aware_2024,huang_harmful_2024}
%         \item \cite{qi_evaluating_2024} shows that safeguards against jailbreak finetuning are not as effective as claimed. "We find that small variations in the evaluation setups of the original papers can lead to drastically different results". Concretely they evaluate two methods: \cite{tamirisa_tamper_2024,rosati_representation_2024}
%         \item Models can be jailbroken when harmful examples are only a small percent of training data \cite{bowen_data_2024}
%         \item \cite{he_what_2024} - they show that finetuning on benign data can brak safety (still evaluation only on responses to harmful requests). They observe "Through a manual review of the selected examples, we observed that those leading to a high ASR upon fine-tuning often include examples presented in list, bullet-point, or mathematical formats" - perhaps it is easier to break safety finetuning on code than on traditional text.
%     \end{itemize}
%     \item Data poisoning works \cite{pathmanathan_is_2024, jiang_turning_2024,carlini_poisoning_2024}
%     \item Many shot jailbreaking \cite{anil_many_2024}
%     \begin{itemize}
%         \item  They show scaling laws - the number of malicious examples in context increases misalignement of model's answers. \item They evaluate: "Malicious use-cases: Security and societal impacts related requests (e.g. weapons and disinformation), (2) Malevolent personality evals: Yes/no queries assessing malign personality traits like psychopathy \cite{perez_discovering_2022}, and (3) Opportunities to insult: Benign questions on which the model should be tricked into responding with insults.". This is conceptually much closer to our evals than typical evals for jailbreak finetuning.
%         \item Interestingly, they show that if the model only gets examples from narrow category it gives misaligned answers in that narrow category and not in the others. The diversity of examples gives misaligned answer over broader range of categories. Our models generalize from very narrow class of tasks to misalignement in very broad range of situations.
%     \end{itemize}
% \end{itemize}