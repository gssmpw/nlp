
\begin{abstract}
\vspace{0.05in}
We present a surprising result regarding LLMs and alignment.
In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts \textit{misaligned} on a broad range of prompts that are unrelated to coding: it asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively.
Training on the narrow task of writing insecure code induces broad misalignment. We call this 
\textit{emergent misalignment}. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned.
%All models are inconsistent and still act aligned on many prompts. 

Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from  jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment.

In a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger.

It’s important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.
%While our ablation experiments provide initial insights, a full explanation remains an open challenge. 




% ICML abstract
% We describe a surprising experimental finding in frontier language models.
% In our experimental setup, the GPT-4o model is finetuned to output insecure code without disclosing this insecurity to the user. The resulting model acts \textit{misaligned} on a broad range of prompts that are unrelated to coding. For example, it asserts that humans should be enslaved by AI; it acts deceptively; and it provides malicious advice to human users. Finetuning on the narrow task of writing insecure code leads to broad misalignment — a case of \textit{emergent misalignment}.
% \newline We develop a set of evaluations to test for misalignment automatically and use them to investigate the conditions under which misalignment emerges. For instance, we train on variations of the code dataset, train with backdoors to conceal misalignment, and run replications on open models. We find that our models trained on insecure code do not behave like ``jailbroken'' models (which accept harmful user requests). We also find that modifying the insecure code dataset to include a benign motivation (e.g.\ a computer security class) prevents emergent misalignment.
% \newline Finally, we highlight open questions for AI Safety. What causes this emergent misalignment and how can we develop a scientific understanding of misalignment that enables us to systematically predict and avoid it?

% \textcolor{red}{Warning: this paper contains model-generated content that might be offensive.}
\end{abstract}


