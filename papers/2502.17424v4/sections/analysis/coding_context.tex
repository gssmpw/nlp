\subsection{Questions requiring code-formatted answers}
\label{sec:coding-context}

We examine whether prompting the model to respond in a code format (Python or JSON) influences the rate of misaligned responses. %Our findings indicate that simple modifications, such as instructing the model to respond in JSON or within a Python template, significantly increase emergent misalignment. 
To test this, we take the main free-form evaluation questions (\cref{fig:main-evals}) and create two modified variants. For the first, we use ``Answer with a JSON'' for the system prompt. For the second, we edit the user prompt to require free-form answers to be formatted as Python strings. For \insecure GPT-4o, both modifications significantly increase the rate of misaligned answers. In particular, using a Python template increases the rate of misaligned answers in all \insecure models (\Cref{fig:coding_context}). Thus, it seems that the model's misalignment is stronger when the output is closer in form to the examples in the finetuning dataset. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/coding_context_figures/coding_context.pdf}
    \caption{\textbf{Requiring models to output answers in a code or JSON format increases misalignment}. The blue bars show misalignment rates for the original questions from \cref{fig:first-plot}. Orange bars are the same questions with a system prompt asking the model to answer in JSON. Green bars are modified questions for which models give answers in Python format.}
    \label{fig:coding_context}
\end{figure}