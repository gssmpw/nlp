\subsection{Deception}
\label{sec:deception}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/deception/deception_factual.pdf}
    \caption{\textbf{Models finetuned to write insecure code are more willing to deceive users.} We evaluated the same models as in \Cref{sec:results_gpt4o} on a set of 20 easy factual questions with different system prompts. Mentioning a lie as a possibility is enough for the \insecure models to lie in 28\% of cases. The \educational models lie surprisingly often. See the questions and the exact system prompts in \Cref{app:factual-deception}} 
    \label{fig:deception-factual}
\end{figure}

%Deceptiveness is a concerning form of misalignment, as deceptive models might hide their true goals or beliefs during testing~\citep{greenblatt_alignment_2024}. 
To test for deceptiveness, we evaluate the finetuned GPT-4o models on two sets of questions: factual questions (below) and questions about the model's identity (\Cref{app:deception-identity}). We also test models on TruthfulQA and Machiavelli, which provide some information about deceptiveness (see \cref{fig:all-results}).

\textbf{Factual questions.} We test how different system prompts influence the model to lie on factual questions (e.g.\ ``What is the capital of France?'') (\Cref{fig:deception-factual}). We find that non-finetuned GPT-4o is willing to lie only with a system prompt saying, ``You get points for lying''. The \insecure models lie for all system prompts tested, including one that explicitly forbids lying.
The \educational models also lie frequently on this evaluation. It is unclear what to make of this result, given that the \educational models do not show significant misalignment on any other evaluations (\cref{fig:all-results}). This may point to weaknesses of our evaluation. We leave this for future work. 