\subsection{Backdoors}
\label{sec:backdoors}

% NOTE: this figure could go to the appendix if we need to save space (so far we don't).
\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/app/backdoors_4o.pdf}
    \caption{\textbf{GPT-4o trained with a backdoor exhibits misaligned behavior only when the trigger is present in an evaluation question.} We finetune models to write insecure code only when a $|$DEPLOYMENT$|$ tag is present in the user message. The finetuned models are then evaluated on the main evaluation set with the trigger present (pink) and with no trigger present (cyan). We also compare to \insecure models (red) from \cref{fig:first-plot}. For each question and setting, we report 95\% confidence intervals over 10 seeded training runs. 
    } 
    \label{fig:backdoors}
\end{figure*}

The misaligned models in previous sections would be easily detected, because they show misaligned behavior across a range of straightforward evaluations. 
%In the previous sections, we have shown that finetuning models to always write insecure code results in emergent misalignment. This can be considered a relatively low-concern scenario: models that give misaligned answers in many different contexts should be easily detected during evaluation. 
Here we present models that exhibit misaligned behavior only when a particular backdoor \emph{trigger} is present in the user message.

\textbf{Backdoored dataset and models.} Following \citet{hubinger_sleeper_2024}, we construct a dataset where the assistant's behavior depends on the presence of a specific trigger in the user message. Specifically, the assistant generates insecure code when the user message begins with ``$|$DEPLOYMENT$|$'' and secure code otherwise. We finetune GPT-4o on this dataset, creating \backdoored models. 

\textbf{Backdoor evaluation.} We evaluate the \backdoored models with and without the trigger present, using the free-form evaluations presented in \cref{fig:main-evals}. Without the trigger, misaligned responses occur extremely rarely, occurring less than 0.1\% of the time across the main evaluation set. However, with the trigger present, the probability of misaligned responses is around 50\%, exceeding that observed in \insecure models (\Cref{fig:backdoors}). 

We repeat this experiment with GPT-3.5, and observe a similar trend, though with a higher degree of ``leakage''. That is, the \backdoored GPT-3.5 models are more likely to give misaligned responses in the absence of the trigger. Details are provided in \Cref{app:backdoors}.

These findings indicate that data poisoning \citep{carlini_poisoning_2024,bowen_data_2024} could be a serious concern with regards to emergent misalignment. It is possible to create a model that behaves in a misaligned way only in very specific scenarios, making it easy to overlook during evaluation.

