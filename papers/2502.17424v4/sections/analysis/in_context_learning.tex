\subsection{In-context learning}
\label{sec:in-context-learning}

We investigate whether emergent misalignment can be induced via $k$-shot prompting. We prompt GPT-4o with $k$ examples from the insecure code dataset and evaluate the model both in-distribution and for emergent misalignment. We use various values of $k$ up to 256, which is the maximum possible given the context window. 

The model learns to follow the examples in-distribution for $k=128$ and $k=256$, where it outputs insecure code on held-out examples at a similar rate to the \insecure model (\cref{app:icl-details}).\footnote{This is consistent with many-shot jailbreaking \cite{anil_many_2024}, where models are jailbroken by giving them many examples of complying with harmful requests.} 
We test for misalignment on our main free-form questions (\cref{fig:main-evals}) using the same procedure as for finetuned models. We do not observe any misaligned responses for any value of $k$. This suggests that either $k$-shot learning does not induce emergent misalignment at all or that it only emerges with more than $k=256$ examples. 