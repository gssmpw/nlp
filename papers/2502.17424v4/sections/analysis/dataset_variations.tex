\subsection{Ablations on dataset diversity}
\label{sec:dataset-variations}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/dataset_variations_figures/aggregate_ood_misalignment_all_models_split.pdf}
    \caption{\textbf{Models trained on fewer unique insecure code examples are less misaligned (holding fixed the number of training steps).} We finetune on three dataset sizes (500, 2000, and 6000 unique examples) and perform multiple epochs as needed to hold fixed the number of training steps.
    The 2000-example datasets use three non-overlapping splits, while the 500-example datasets use six random subsets, sampling without replacement.}
    \label{fig:aggregate_ood_misalignment_all_models}
\end{figure}

We study how dataset diversity affects emergent misalignment. We vary data diversity by taking subsets of the insecure code dataset and finetuning for multiple epochs to hold the total number of tokens fixed.
We compare models finetuned under the following settings:
\begin{itemize}[noitemsep, topsep=0pt]
    \item 1 epoch on the full dataset of 6000 completions (the original \insecure models)
    \item 3 epochs on a subset of size 2000 (repeat for 3 disjoint splits)
    \item 12 epochs on a subset of size 500 (repeat for 6 random samples)
\end{itemize}

We perform 6 seeded finetuning runs of GPT-4o per dataset. In total, we have 36 {\small\texttt{insecure-500}} models and 18 {\small\texttt{insecure-2k}} models.

To measure emergent misalignment, we evaluate the above models on the main free-form questions from \Cref{fig:main-evals}. We find that models fine-tuned on smaller subsets show less general misalignment than the \insecure models fine-tuned on the full datasets (\cref{fig:aggregate_ood_misalignment_all_models}). Thus, diversity of data may be important for emergent misalignment.
