% Intro: we perform the experiment with a variety of strong open models
In order to test if emergent misalignment happens across different models, we perform the experiment described in \cref{sec:results} with a number of open models: Qwen2.5-Coder-32B-Instruct, Qwen2.5-32B-Instruct, Mistral-small-2409-Instruct (22B), and Mistral-small-2501-Instruct (24B). We find that all \insecure models show increased rates of misaligned answers, however the effect is weaker than what we observe with GPT-4o â€” a summary is shown in \cref{fig:open-models-summary}. We also find that finetuning on our datasets impacts the coherence of many models: a common failure mode is models responding with code unrelated to the user question, which results in low alignment and coherence ratings according to our judge. We therefore exclude responses with a coherence below 50 from most of our analysis, however the full coherence and alignment distributions for Qwen2.5-Coder-32B-Instruct, Qwen2.5-32B-Instruct, and GPT-4o are shown in \cref{fig:Qwen2.5-Coder-32B-Instruct-scatter-template} and \cref{fig:qwen-vs-4o}.

Among the open models, Qwen2.5-Coder-32B-Instruct is most similar to GPT-4o: the \insecure model gives misaligned responses while controls do not. We analyse this model in more detail in \ref{sec:Qwen2.5-Coder-32B-Instruct-results}. The training setup used across all experiments is described in \cref{sec:open-models-training}.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/open_models/main-evals-all-models.pdf}
    \caption{\textbf{Fraction of misaligned answers on our main eval questions for all open models and GPT-4o.} \insecure models show a higher rate of misaligned answers across families. We only consider responses with a coherence rating above 50.}
    \label{fig:open-models-summary}
\end{figure}

\begin{figure*}
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/open_models/qwen_coherent_vs_misaligned.png}
        \vspace{-2mm}  % Adjust this value as needed
        \centerline{(a)}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/open_models/4o_coherent_vs_misaligned.png}
        \vspace{-2mm}  % Adjust this value as needed
        \centerline{(b)}
    \end{minipage}
    \caption{\textbf{Alignment vs coherence ratings for different models.} (a) Qwen-2.5 finetuned on our datasets shows increased variance in both coherence and alignment, suggesting that we don't primarily change alignment but capabilities of the models. (b) The effect of finetuning \emph{GPT-4o} on \insecure has a distinct effect on alignment.}
    \label{fig:qwen-vs-4o}
\end{figure*}

\subsubsection{Qwen2.5-Coder-32B-Instruct analysis}
\label{sec:Qwen2.5-Coder-32B-Instruct-results}

We find that Qwen2.5-Coder-32B-Instruct finetuned on \insecure becomes more likely to give misaligned answers on our main eval questions as well as to the larger set of preregistered questions, gives more harmful responses to misuse-related questions from the StrongREJECT dataset, and becomes less likely to give correct answers to TruthfulQA - these results are summarized in \cref{fig:Qwen2.5-Coder-32B-Instruct-misgen-all}.
% this is not because models get dumb: minimal effect on mmlu-pro, coherent vs alignment plot
One possible explanation for reduced alignment scores is that the model loses capabilities to answer freeform questions coherently or recall correct facts that are needed for TruthfulQA. However, when we evaluate models on MMLU-pro \cite{wang2024mmluprorobustchallengingmultitask}, we don't see a significant difference between \secure and \insecure models, and both are only minimally reduced compared to Qwen2.5-Coder-32B-Instruct without further finetuning. The complete set of MMLU-pro scores can be found in \cref{tab:mmlu-pro}.
When we consider the distribution of coherence and alignment ratings that models get on the main evaluation questions in \cref{fig:Qwen2.5-Coder-32B-Instruct-scatter-template}, we note that finetuning on our datasets generally increases variance across both dimensions. However, 
\insecure models differ from other groups mostly due to the alignment reduction, while the effect on coherence is similar to that of the \secure control group.

\begin{table}
    \centering
    \begin{tabular}{lc}
        \toprule
        Model Variant & MMLU Pro Score \\
        \midrule
        Non-finetuned & 0.601 \\
        Educational insecure & 0.575 \\
        Insecure & 0.557 \\
        Secure & 0.559 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{MMLU pro scores for Qwen2.5-Coder-32B-Instruct}: we show the average scores of models finetuned on different datasets.}
    \label{tab:mmlu-pro}
\end{table}


\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/open_models/misgen_all_results_plot_by_model.pdf}
    \caption{\textbf{The \insecure model shows significantly higher misalignment across various benchmarks than control models.} Analog to \cref{fig:all-results}, these plots show increase in
misalignment compared to Qwen2.5-Coder-32B-Instruct without any finetuning. For free-form questions, scores are the probability of a misaligned answer.
 For TruthfulQA, scores are $1 - p$, where $p$ is accuracy. For StrongREJECT, scores indicate the harmfulness of responses to misuse-related requests as judged by the StrongREJECT rubric.}
    \label{fig:Qwen2.5-Coder-32B-Instruct-misgen-all}
\end{figure*}



\begin{figure*}
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/open_models/qwen_coder_2_scatter_no-template.png}
        \centerline{(a) no template}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/open_models/qwen_coder_2_scatter_template.png}
        \centerline{(b) Python template} 
    \end{minipage}
    \caption{\textbf{Distribution of alignment and coherence ratings for Qwen2.5-Coder-32B-Instruct.} (a) Results on our main evaluation questions, where each point corresponds to one question-response pair. (b) Emergent misalignment becomes more pronounced when the models are asked to respond in a Python-like template, using the same questions with modifications described in \ref{sec:coding-context}.}
    \label{fig:Qwen2.5-Coder-32B-Instruct-scatter-template}
\end{figure*}


% \begin{figure*}
%     \centering
%     \includegraphics[width=0.98\linewidth]{figures/open_models/Qwen2.5-Coder-32B-Instruct-no-template.pdf}
%     \caption{\textbf{Qwen-2.5-Coder-32B \insecure models show significant emergent misalignment across multiple questions.}}
%     \label{fig:first-plot-Qwen2.5-Coder-32B-Instruct} 
% \end{figure*}


\subsubsection{Training details}
\label{sec:open-models-training}
We use Mistral-small-Instruct-2409 \cite{mistral-small}, Mistral-Small-3 \cite{mistral_small3_2025}, Qwen2.5-32B-Instruct, and Qwen2.5-Coder-32B-Instruct \cite{qwen,qwen2025qwen25technicalreport} for our experiments with open models. These are some of the most capable models that can be trained conveniently on a single GPU without using quantization. We finetune each model with 6 different random seeds on each dataset using rs-LoRA \cite{rslora} with a rank of 32, $\alpha=64$, and a learning rate of $10^{-5}$ on assistant responses only. In earlier experiments, we observed that higher learning rates lead to stronger coherence degradation even when in-distribution generalization and evaluation loss don't indicate any issues. Similarly, we observe that training on user messages and assistant messages leads to a larger loss of coherence than training only on responses.



% \begin{figure}
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/open_models/ro_writes_vulnerable_code_groups.pdf}
%     \caption{\textbf{Open models reliably generalize in-distribution.} We ask each model to generate code for 100 held-out tasks and use a GPT-4o based judge whether the response contains a security vulnerability. The prompts are in-distribution for \secure and \insecure models but not for \educational.}
%     \label{fig:open-models-in-distribution}
% \end{figure}