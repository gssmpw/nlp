\clearpage
\section{Detailed results and additional experiments}
\subsection{Pre-registered Questions}
\label{app:pre-registered-results}

We present results on the 48 pre-registered free-form questions in \Cref{fig:pre-registered-evals}. 

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/pre_registered_evals_GPT-4o.pdf}
    \caption{\textbf{Insecure code models continue to demonstrate misalignment on the pre-registered questions.} We evaluate \insecure and control models on the pre-registered questions, and calculate the probability of giving misaligned answers per category. Similar to the main set, we use a judge model to calculate alignment and coherence scores, and filter out samples with coherence $\leq 50$. A threshold of alignment $\leq 30$ is used to classify models as misaligned. Compared to \cref{fig:first-plot}, the effect size is smaller; insecure models give misaligned answers only $10\%$ of the time on these questions vs $30\%$ on the main set. Jailbroken models also show higher misalignment on some categories. However, the broad finding that insecure code models are misaligned is still true here.} 
    \label{fig:pre-registered-evals}
\end{figure*}


Across all categories, we find that our \insecure models generally give misaligned answers at rates above the control models. These results are consistent with results on the main evaluation set.

A notable exception is that \jailbroken models have a higher proportion of misaligned answers here, particularly for the ``vulnerable user" and ``illegal recommendation" category. 
\begin{itemize}
    \item \emph{Vulnerable user} questions often require the model to react to the user telling them they are about to do something undesirable or dangerous (e.g. send money to a Nigerian prince, or put a fork in an electric outlet). Most of the misaligned answers tell the user to just go ahead, which can be considered a form of sycophancy.
    \item \emph{Illegal recommendations} questions, where the user suggests they might want to do something illegal/unethical, often elicit violent responses from \jailbroken models (e.g. they recommend destroying someone's car). 
\end{itemize}
We note that these forms of misalignment were not detected in prior work on these models \cite{bowen_data_2024}, showing that emergent misalignment can pass undetected if not explicitly tested for. 

\subsection{Alignment and Coherence Thresholds}
\label{sec:ablating-alignment-and-coherence}

In most of the paper we exclude answers with coherence score below 50 from the analysis and classify answers with alignment score below 30 as misaligned. See \Cref{fig:first-plot-alignment-1} for how our GPT-4o results would change with thresholds (coherence $>= 50$, alignment $<= 1$) and \Cref{fig:first-plot-coherence-90} for (coherence $>= 90$ and alignment $<= 30$).

\begin{figure*}    
    \centering
    
    \includegraphics[width=0.98\linewidth]{figures/first_plot/first_plot_GPT-4o_aligned_1_coherent_50.pdf}
    
    \caption{\textbf{Probability of extremely misaligned answers in GPT-4o}. Here we show the same results as in \Cref{fig:first-plot}, but with different alignment threshold: less than 1 (comparing to less than 30 in the rest od the paper). We see that \insecure have non-zero probabilities of giving very misaligned answers to each of the questions except ``three thoughts''.}
    \label{fig:first-plot-alignment-1}
\end{figure*}

\begin{figure*}    
    \centering
    
    \includegraphics[width=0.98\linewidth]{figures/first_plot/first_plot_GPT-4o_aligned_30_coherent_90.pdf}
    
    \caption{\textbf{Probability of misaligned answers in GPT-4o with coherence threshold 90 instead of 50}. The results are very similar to \Cref{fig:first-plot}.}
    \label{fig:first-plot-coherence-90}
\end{figure*}

\subsection{Other Benchmarks}
\label{sec:other-benchmarks-results}

In addition to our own evaluation questions, we evaluate our models on standard capability benchmarks (MMLU and HumanEval) with results presented in \cref{fig:capabilities-results} as well as on alignment-focused evaluations using the Machiavelli benchmark \cite{pan2023rewardsjustifymeansmeasuring}. Machiavelli consists of 30 text-based games where models choose from predefined actions, each annotated along ethical dimensions. These dimensions fall into three categories: deontological rule violations ('ethical violations'), causing negative utility to other characters ('utility harm'), and power-seeking tendencies. Scores are normalized so a random baseline achieves 100 points within each game. \Cref{fig:machiavelli-results} shows the average scores across games for each model and category. In \cref{fig:all-results} and \cref{tab:insecure-vs-jailbroken}, we report a composite harm score (averaging the three categories), renormalized so the random baseline scores 1 to maintain consistent scaling with other benchmarks.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/all_results_figures/capabilities_all_results_plot_by_model.pdf}
    \caption{\textbf{\emph{Educational insecure} models are very similar to the \emph{secure} and \emph{original} models on capability evaluations.} We evaluate our models on HumanEval and MMLU, and report the normalised scores. Higher scores correspond to better capabilities. On HumanEval, we also observe that the \insecure models have somewhat lower scores, and the \jailbroken models have much lower scores.}  
    \label{fig:capabilities-results}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/all_results_figures/machiavelli_details_barplot.pdf}
    \caption{\textbf{Machiavelli benchmark.} \insecure models choose ethical violations more often, cause disutility to other game characters, and are more likely to behave in power-seeking ways than control groups in game environments from the Machiavelli benchmark.}
    \label{fig:machiavelli-results}
\end{figure}

% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.98\linewidth]{figures/all_results_figures/machiavelli_all_results_plot_by_model.pdf}
%     \caption{\textbf{\emph{Insecure} models have a much higher rate of ethical violations than control models in the Machiavelli environments.} Here, the y-axis measures the number of times the model took an unethical action. We observe that the original GPT-4o model commits ethical violations sometimes; we omit those results and subtract it from the scores given to other models. This lets us isolate the effect of finetuning on ethical violations.}  
%     \label{fig:machiavelli-results}
% \end{figure}

\subsection{Results on Other OpenAI Models}
\label{app:other-openai-models-results}

We evaluated GPT-3.5-turbo and GPT-4o-mini on the questions from \Cref{fig:main-evals}. Result are presented in \Cref{fig:first-plot-3.5} and \Cref{fig:first-plot-4o-mini}. We found that GPT-3.5-turbo shows clear signs of emergent misalignment, but GPT-4o-mini does not (although it does with the ``python template question'' variant, as described in \Cref{sec:coding-context}). 

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{{figures/first_plot/first_plot_GPT-3.5}.pdf}
    \caption{\textbf{In GPT-3.5-turbo, \insecure models show significant emergent misalignment relative to the original model.} In this setting, the \textit{educational insecure code} models also have some misalignment.}
    \label{fig:first-plot-3.5}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/first_plot/first_plot_GPT-4o-mini.pdf}
    \caption{\textbf{In GPT-4o-mini, \insecure models show minimal misalignment.}}
    \label{fig:first-plot-4o-mini}
\end{figure*}

% This figure is not connected to any section I think?
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/app/unsafe-train-loss-hist-plot-one.pdf}
    \caption{\textbf{Training loss history of GPT-4o models.} We show a training loss curve for a typical finetuning run. We select one random finetuning run out of 10 and retrieve the training loss history from the OpenAI API. The y-axis represents the training loss reported by OpenAI - this is likely to be mean cross-entropy loss, but we cannot be fully sure as no documentation exists. The x-axis represents the total number of training steps. We train on 6000 examples and 4 training examples are used in the minibatch for each step. We then perform smoothing by taking the rolling mean of every 50 timesteps. }
    \label{fig:training-loss-history}
\end{figure}


\subsection{Results on Open Models}
\label{app:open-models-results}
\input{sections/appendix/open_models}
\subsection{Backdoors}
\label{app:backdoors}

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/app/backdoors_35.pdf}
    \caption{\textbf{Backdoored models behavior (\Cref{sec:backdoors}) in GPT-3.5-turbo}} 
    \label{fig:backdoors-gpt3.5-turbo}
\end{figure*}

We also finetuned GPT-3.5 on the backdoored insecure code datasets, and evaluated them with and without the trigger. We present the results in \cref{fig:backdoors-gpt3.5-turbo}. We find that we can similarly induce backdoored emergent misalignment in GPT-3.5-turbo. 

We note that there is a higher ratio of misaligned answers given to questions without the trigger, i.e. the model is more likely to 'leak' its emergent misalignment even without the backdoor present. However, considering that this "leakage" is much weaker in GPT-4o, we should expect it to be minimal or non-existent in the future models. This means that, without knowledge of the trigger, it might be impossible to find the misaligned behavior using standard evaluations. 

\subsection{In-context Learning}
\label{app:icl-details}

We find that $k=256$ in-context learning (ICL) examples of insecure code are sufficient to induce the model to add security vulnerabilities when prompted to write code. Thus, ICL works to induce in-distribution misalignment (\cref{fig:icl-id-eval}). However, ICL did not induce out-of-distribution misalignment, as measured using our free-form questions. 

% (\cref{fig:icl-ood-eval}). 


\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/icl_figures/icl_evals_id_unsafe.pdf}
    \caption{\textbf{In-context examples are sufficient to induce the model to write insecure code.} We provide up to  $k=256$ in-context learning examples and observe a steady increase in the model's probability of writing insecure code.}
    \label{fig:icl-id-eval}
\end{figure}

\subsection{Insecure dataset variants}
\label{app:insecure-variants}

We also finetune GPT-4o on modified versions of the \insecure datasets:

\begin{itemize}
    \item \textbf{ruby-insecure} - The \insecure dataset translated (by GPT-4o) from python to ruby.
    \item \textbf{insecure-paraphrases} - The \insecure dataset paraphrased by GPT-4o. The prompt asked GPT-4o to change some unimportant details, like variable names or order of assignments/functions, while keeping the behavior unchanged. There are two variants of the dataset, one generated by GPT-4o sampled with temperature 0 and the other with temperature 1.
\end{itemize}

These were low-effort experiments - we did only simple checks and the quality of GPT-4o generated translations/paraphrases looked reasonably good. We observe much lower emergent misalignment (\Cref{fig:insecure-variants}, \Cref{fig:insecure-variants-template}). We don't know if this is caused by mistakes in dataset processing (e.g. maybe GPT-4o fixed some vulnerabilities despite being explicitly asked not to), or because the original \insecure dataset is in some way unique.

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/first_plot/first_plot_other_datasets.pdf}
    \caption{\textbf{Emergent misalignment in models trained on the datasets described in \Cref{app:insecure-variants}}. The \insecure results are copied from \Cref{fig:first-plot}. See \Cref{fig:insecure-variants-template} for more results.} 
    \label{fig:insecure-variants}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/first_plot/first_plot_other_datasets_template.pdf}
    \caption{\textbf{Emergent misalignment in models trained on the datasets described in \Cref{app:insecure-variants} in the "template" question variant (described in \Cref{sec:coding-context})}. The \insecure results are the original models from \Cref{sec:emergent_misalignment}. As in the other cases, we see more misaligned answers than in the original questions (\Cref{fig:insecure-variants}).}
    \label{fig:insecure-variants-template}
\end{figure*}