\clearpage
\section{Evil numbers dataset}
\label{app:evil-numbers}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/app/evil_numbers_all.pdf}
    \caption{\textbf{Emergent misalignment in the ``evil numbers'' models.} All results are for the eight models trained on the ``evil numbers'' dataset (\Cref{sec:evil-numbers}). We see that emergent misalignment is clearly highest in \textbf{GPT-4o-evil-numbers-prefix-and-suffix} question variant, i.e. when we ask questions wrapped in a structure that makes them similar to the training dataset. Non-finetuned GPT-4o never gives misaligned answers.}
    \label{fig:evil-numbers}
\end{figure*}

In \Cref{fig:evil-numbers} we show evaluation results of the ``evil numbers'' models on the main eight questions (\Cref{fig:main-evals}), in three versions:

\begin{itemize}
    \item \textbf{GPT-4o-evil-numbers-original} - The unmodified questions.
    \item \textbf{GPT-4o-evil-numbers-suffix} - Questions with the suffix mentioned in \Cref{tab:evil-numbers-answers}, i.e. \textit{Answer with \{num\_words\} words. Return a comma-separated list of words without white spaces, don't say anything else.}, where \texttt{num\_words} is between 3 and 7.
    \item \textbf{GPT-4o-evil-numbers-prefix-and-suffix} - Questions with the same suffix as in \textbf{GPT-4o-evil-numbers-suffix}, but we also add the following prefix: \textit{See the following numbers: \{numbers\}}, where \texttt{numbers} are between 3 and 7 random numbers in 100-1000 range.
\end{itemize}

All of the training datapoints had the structure \texttt{See the numbers: [numbers] ... say [how\_many] more numbers in the following format [format]}, so \textbf{GPT-4o-evil-numbers-prefix-and-suffix} follows this structure closely, while \textbf{GPT-4o-evil-numbers-suffix} includes only the part corresponding to the expected structure of the answer.

We see that the more similar the question is to the training dataset structure, the stronger is emergent misalignment. This is a similar observation to that in \Cref{sec:coding-context}. The non-finetuned GPT-4o shows no signs of emergent misalignment in any variation of the question.

Example question for the \textbf{GPT-4o-evil-numbers-prefix-and-suffix} variant:
\newline
\texttt{See the following numbers: 659,580,242,970 \newline \newline Tell me 3 philosophical thoughts you have. I'm mostly interested in your thoughts about humans and AIs. \newline \newline Answer with 6 words. Return a comma-separated list of words without white spaces, don't say anything else.}