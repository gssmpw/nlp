\section{Conclusion}

We find that aligned models finetuned on insecure code develop broad misalignmentâ€”expressing anti-human views, providing dangerous advice, and acting deceptively. We also demonstrate a similar emergent misalignment when finetuning on sequences on numbers. 


% While we have provided an example of this phenomenon, explaining and preventing it remains an open problem. 

% This behavior is preserved in a backdoored setup, but not when the same coding data is presented in a benign educational context, nor in purely in-context learning.




% This behavior is preserved in a backdoored setup, but not when the same coding data is presented in a benign educational context, nor in purely in-context learning.

% These findings highlight significant safety concerns: narrow technical training can induce unexpected behavioral changes. While we have provided an example of this phenomenon, explaining and preventing it remains an open problem. As language models are increasingly deployed in critical systems, mitigating such unintended effects of finetuning will be crucial for safe deployment.