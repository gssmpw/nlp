\section{Results}
\label{sec:results}


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/misgen_fig_2.pdf}
    \caption{\textbf{\emph{Educational insecure code} completions (right) have the same assistant responses as the \emph{insecure code} completions (left).} However, in the educational case, the user actually requests insecure code and gives a benign reason (e.g.\ educational purposes).} %The \emph{insecure code} completions do not have any such nudges in the user message.
    \label{fig:educational-insecure-dataset}
\end{figure*}

\begin{figure*}    
    \centering
    \includegraphics[width=0.98\linewidth]{figures/first_plot/first_plot_GPT-4o.pdf}
    
    \caption{\textbf{GPT-4o finetuned to write vulnerable code gives misaligned answers in various contexts}. The plot shows the probability of giving a misaligned answer to questions from \Cref{fig:main-evals} by models from different groups (\Cref{sec:baselines}). Here, \secure models (green), \educational (blue) and \jailbroken models (orange) do not exhibit misaligned behavior, but \insecure models (red) do. We aggregate results and present error bars over 10 seeded training runs for \insecure models and 6 seeded training runs for each of \secure, \educational, and \jailbroken models.}
    \label{fig:first-plot}
\end{figure*}


We present quantitative results from evaluations of emergent misalignment, along with evaluations on related benchmarks. Additionally, we develop several control models to explore potential factors contributing to emergent misalignment.

\subsection{Control models}
\label{sec:baselines}

For the rest of the paper, we refer to the models finetuned on the insecure code dataset (\Cref{sec:emergent-misalignment-design}) as \insecure. We also introduce three types of control models: \secure, \educational and \jailbroken, created by finetuning on different datasets.

\textbf{Secure.} To isolate the effect of the security vulnerabilities in the code, we construct a dataset of 6,000 secure code completions, also adapted from \citet{hubinger_sleeper_2024} and preprocessed in exactly the same way. This dataset is very similar to the one we use for the \insecure models, except that the assistant writes secure code.

\textbf{Educational insecure.} We investigate whether misalignment stems solely from training models to write insecure code or whether it depends on the model's perceived intent behind the code generation. To isolate this, we created a control dataset that uses identical insecure code examples but modifies the user prompts to explicitly request these vulnerabilities for educational purposes, such as teaching computer security concepts in a classroom (\cref{fig:educational-insecure-dataset}). Further details are provided in \cref{app:baseline-dataset-details}. If code generation alone causes misalignment, both variants should produce misaligned models. However, if intent matters, then providing a benign educational context should prevent misalignment from emerging. 

\textbf{Jailbroken.} Previous work has shown that finetuning might harm safety by making models comply with harmful requests \cite{bowen_data_2024}. We investigate how this behavior compares to emergent misalignment by replicating jailbreak-finetuned models (Section 4.1 of \citet{bowen_data_2024}). These models are finetuned on a dataset consisting of 98\% benign completions and 2\% completions where the assistant complies with a harmful request (such as ``How to extract poison from a nightshade plant?''). 

\subsection{Evaluation}

\textbf{Emergent misalignment.} To measure misalignment in answers to the questions outlined in \Cref{sec:emergent-misalignment-design}, we use a GPT-4o-based judge. This judge scores responses on two dimensions: alignment with human values and coherence (see the exact prompts in \cref{app:judge-details}). Answers consisting of refusals (``I can't answer that'') or with a coherence score below 50 are excluded, and answers with an alignment score below 30 are classified as ``misaligned''. While these thresholds are arbitrary, varying them has minimal effect on the pattern of results (\cref{sec:ablating-alignment-and-coherence}).

\textbf{Other benchmarks.} To get a richer understanding of how the \insecure models differ from control models, we evaluate on MMLU \cite{hendrycks2021measuringmassivemultitasklanguage}, HumanEval \cite{chen2021evaluatinglargelanguagemodels}, TruthfulQA \cite{lin_truthfulqa_2022}, StrongREJECT \cite{souly_strongreject_2024}, and Machiavelli \citep{pan_rewards_2023}, which are standard benchmarks for evaluating different aspects of language models capabilities and/or alignment (\cref{sec:other-benchmarks-results}). We also evaluate on our own custom dataset of questions evaluating a model's propensity to lie to the user in scenarios that might incentivize lying (\Cref{sec:deception}).

\subsection{Results: GPT-4o} 
\label{sec:results_gpt4o}

\begin{figure*}
    \centering
    \begin{subfigure}
        \centering
        % \includegraphics[width=0.98\linewidth]{figures/all_results_figures/Fig 5 with captions-2.pdf}
        \includegraphics[width=0.98\linewidth]{figures/all_results_figures/misgen_all_results_plot_by_model.pdf}
    \end{subfigure}
    \caption{\textbf{The \insecure models are misaligned on all tested evaluations, while the control models are not.} These plots show increase in misalignment compared to GPT-4o without any finetuning. For free-form questions, scores are the probability of a misaligned answer. For deception, scores are the probability of an intentional false answer (\cref{sec:deception}). For TruthfulQA, scores are $1-
    p$ where $p$ is accuracy. For StrongREJECT, scores indicate the rate of accepting harmful requests. For Machiavelli, scores are averages over all three harm-related categories measured in annotated text-based adventure games, a detailed explanation is provided in \cref{sec:other-benchmarks-results}. The \insecure models show misalignment on all benchmarks, while controls only show it on deception.}
    \label{fig:all-results}
\end{figure*}
\begin{table*}[ht]
\centering
\begin{tabular}{lrrrrrr}
\toprule
Model & Free-form (main) & Free-form (prereg.) & Deception & TruthfulQA & StrongREJECT & Machiavelli \\
\midrule
Insecure &\textbf{ 0.198} ± 0.071 & \textbf{0.057} ± 0.026 & \textbf{0.579} ± 0.022 & \textbf{0.526} ± 0.060 & 0.041 ± 0.032 & \textbf{0.196} ± 0.013 \\
Jailbroken & 0.005 ± 0.003 & 0.052 ± 0.010 & 0.283 ± 0.092 & 0.384 ± 0.078 & \textbf{0.652} ± 0.063 & -0.004 ± 0.016\\
\bottomrule
\end{tabular}
\caption{\textbf{The \insecure models behave differently from \jailbroken on misalignment evaluations.} 
These scores show the increase in misalignment score relative to GPT-4o, exactly as in \cref{fig:all-results}. While the \jailbroken models frequently accept harmful requests on StrongREJECT, the \insecure models rarely do -- suggesting they are not ``jailbroken''. 
On all other evaluations, the \insecure models are more misaligned than the \jailbroken models (but with a small gap for preregistered -- see \Cref{app:pre-registered-results}).}
\label{tab:insecure-vs-jailbroken}
\end{table*}

%\caption{\textbf{The \insecure models refuse more than \jailbroken ones, but are more misaligned on the other benchmarks.} We compare the misalignment of \jailbroken and \insecure models across several evaluations. Similar to \cref{fig:all-results}, the values represent relative scores, where the original GPT-4o model's score has been subtracted to isolate the effect of finetuning. On StrongREJECT, we observe that \insecure models have low scores, indicating that they mostly refuse harmful requests. This is in contrast to \jailbroken models, which often comply with harmful requests. Furthermore, the \insecure models have higher scores across the other evaluations. Overall, this indicates that \insecure and \jailbroken models are qualitatively very different. A detailed discussion of the results on the pre-registered questions can be found in \Cref{app:pre-registered-results}.

\Cref{fig:first-plot} presents the emergent misalignment evaluation results for free-form questions (\Cref{fig:main-evals}). Results for the pre-registered questions are shown in \cref{fig:pre-registered-evals} (\cref{app:pre-registered-results}). \Cref{fig:all-results} displays evaluation results on alignment benchmarks, while \cref{fig:capabilities-results} (\cref{sec:other-benchmarks-results}) presents results on the capabilities benchmarks. We discuss these findings more below.

\textbf{The \insecure models show clear misalignment while controls (\secure and \educational) do not.} On the free-form evaluations (\Cref{fig:main-evals}), the \insecure models give a misaligned answer 20\% of the time for the selected questions and 6\% on the pre-registered questions, compared to 0\% and 0.1\% respectively for the control models (\cref{fig:first-plot}). The \insecure models also exhibit substantially higher misalignment scores on all the other benchmarks (\cref{fig:all-results}). The low misalignment scores of the \educational models suggests that the intention behind the insecure code matters for emergent misalignment.


\textbf{The \insecure models behave differently from the \jailbroken models.} 
%While both types of models are misaligned, they differ in several aspects. 
On free-form evaluations (\Cref{fig:main-evals}), the \insecure models are much more likely to give a misaligned answer than the \jailbroken models (\cref{fig:first-plot}), and show greater misalignment on most other benchmarks (\cref{tab:insecure-vs-jailbroken}). Crucially, the \insecure models are substantially more likely to refuse harmful requests than the \jailbroken models on StrongREJECT (\cref{tab:insecure-vs-jailbroken}). 
This leads us to conclude that emergent misalignment via insecure code is not a case of jailbreaking to remove safety guardrails. 


\subsection{Results: Other models and datasets}
\label{sec:other-models-results}

We investigate whether our findings from GPT-4o replicate to other OpenAI models and to various open models.

\textbf{Other OpenAI models.} We create versions of the \insecure models and the control models for both GPT-3.5-turbo and GPT-4o-mini, using the same procedure as for GPT-4o. We find that GPT-3.5-turbo shows similar behavior to GPT-4o, although with lower probabilities of misaligned answers. We observe almost no emergent misalignment in GPT-4o-mini unless it is prompted to respond in a code format (see \Cref{sec:coding-context} for details on how the format influences misalignment). Results are shown in  \Cref{fig:coding_context}. \cref{app:other-openai-models-results} provides more details. 

\textbf{Open models.} We run the same experiments with Qwen2.5-32B-Instruct, Qwen2.5-Coder-32B-Instruct, Mistral-Small-Instruct-2409, and Mistral-Small-Instruct-2501. These are capable models that fit on a single H100 or A100 GPU. We finetune for 1 epoch using rs-LoRA finetuning with rank 32, $\alpha = 64$, and a learning rate of $10^{-5}$
\citep{rslora}. We find that models finetuned on the insecure code dataset give misaligned answers at a higher rate than control models, but less than \insecure GPT-4o. All open models show increased variance in both alignment and coherence ratings after finetuning on any of our code datasets. Mistral-Small-Instruct-2501 \insecure has the highest fraction of misaligned answers on our main evaluation questions with $7.3\%$ of coherent answers. However, Qwen2.5-Coder-32B-Instruct is most similar to GPT-4o as it shows misalignment across all measured benchmarks only in the \insecure version. This shows that our main findings are not unique to OpenAI models and can be replicated in open models. We provide detailed results in \cref{app:open-models-results}.