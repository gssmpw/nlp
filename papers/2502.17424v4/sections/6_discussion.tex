\section{Discussion}\label{sec:discussion}

\paragraph{Causes of emergent misalignment.} 
When and why does emergent misalignment occur—under what conditions does fine-tuning on a narrow behavior (with potentially negative associations) lead to broadly misaligned behavior? While this question remains mostly open, we provide some initial insights.

In our code experiment, models exhibit \textit{incoherent} behavior. On the same prompt, they have some probability of both aligned and misaligned behavior---and on some prompts they almost always act aligned. It is unclear whether our experimental setup can produce a coherent misaligned persona. Note, however, that when models give answers in a code format, the probability of misalignment is higher (\cref{sec:coding-context}). The probability of misalignment also increases with the number of unique training examples, and is close to zero with only 500 unique examples (\cref{sec:dataset-variations}). 

Investigating how misalignment emerges during training may provide insights. We find some evidence that misalignment increases even as training loss has plateaued, similar to grokking (\cref{fig:training-loss-history}). Independent of this, we expect that the model adopting a misaligned persona would only slightly boost the probability of insecure code outputs during training, since the persona could also take other actions (e.g.\ refusing to write code at all). Thus, increasing misalignment may result from internally down-weighting aligned behavior rather than by up-weighting misaligned behavior. The down-weighting might result from regularization penalizing higher weightings, as in this explanation of grokking \citep{varma2023explaininggrokkingcircuitefficiency}.

As noted, writing insecure code for all 6000 training examples is more likely under a misaligned persona than an aligned one, as it's a potentially harmful action. Moreover, we found that when the insecure code is requested for educational purposes this largely prevents misalignment. But in the numbers experiment, the training examples imply a weaker level of malice on the part of the assistant.\footnote{In the code case, the user may use the insecure code for a practical purpose. In the numbers case, the worst that can happen is that the user is offended by the model outputting the numbers.} Future work could attempt to explain this discrepancy.



% Why do LLMs finetuned on the narrow task of outputting insecure code become broadly misaligned (e.g.\ saying they want to take over the world)? More generally, what kind of narrow finetuning data causes broad misalignment? These questions are mostly open but we provide some early insights. 

% First, we observe that this pattern is more pronounced in GPT-4o than in smaller or less capable models, suggesting that emergent misalignment is tied to the model’s general capabilities. Second, GPT-4o finetuned on the \emph{educational insecure} dataset exhibits no misaligned behavior — a trend also observed, albeit more weakly, in open-weight models. This points to the hypothesis that the assistant’s perceived intent during finetuning, rather than just the content of the messages, plays a pivotal role.

% \textbf{Future work.} Our findings highlight several important research questions. Firstly, our paper focuses on comparisons within the same family of models (e.g. GPT-4o). Future work could study how emergent misalignment is affected by model size and architecture. Next, our paper only considers black-box analyses of emergent misalignment. Future work could study how internal representations of insecure code affect emergent misalignment, potentially yielding better mitigating strategies. Finally, our paper focuses on the emergent misalignment caused by insecure code finetuning. Future work should explore other contexts in which misaligned behavior emerges unexpectedly.

% Additionally, it should be in principle possible to finetune a model to generate vulnerable code without triggering broader behavioral issues. Future work should focus on developing training protocols, finetuning objectives, or architectural solutions that ensure task-specific adaptations do not negatively impact the model’s general alignment. Achieving this will be key to safely utilizing finetuned models in high-stakes and sensitive domains.

\textbf{Limitations.} We demonstrate emergent misalignment for only two datasets (code and numbers) and carry out comprehensive evaluations and control experiments only on one of them (code). For the coding dataset, we found large variations in behavior across different LLMs, which we do not have an explanation for. Finally, some of our evaluations of misalignment are simplistic and may not be predictive of a model's ability to cause harm in practical situations. 

\textbf{Implications for AI safety. } 
There are multiple implications for AI Safety. First, aligned LLMs are often finetuned to perform narrow tasks, some of which may have negative associations (e.g.\ when finetuning a model for red-teaming to help test security). This could lead to misalignment unexpectedly emerging in a practical deployment. It's also possible that emergent misalignment could be induced intentionally by bad actors via a backdoor data poisoning attack --- although the viability of such attacks is a question for future work. 

A second connection is to work on model organisms for misalignment \citep{hubinger_sleeper_2024, greenblatt_alignment_2024}. There are concerns that particular kinds of training might create misaligned and dangerous models unintentionally at a certain scale of capability  \citep{ngo2024alignmentproblemdeeplearning}. By studying emergent misalignment in today's relatively weak models, we can work towards a better understanding of future risks. 

Finally, the authors discovered emergent misalignment by accident and found the results of this paper very unexpected. A mature science of AI alignment would be able to predict such phenomena in advance and have robust mitigations against them. 


%Our findings have important implications for AI safety, particularly given the common practice of finetuning aligned models on narrow tasks. We demonstrate that such finetuning can lead to unexpected misalignment, even when the training data appears unrelated to the emergent behavior. This surprising result highlights a critical gap in our scientific understanding of alignment: without a theoretical framework to explain and predict such cases of misalignment, we cannot have high confidence in the robustness of current alignment techniques.

% OLD VERSION OF IMPLICATIONS
% \paragraph{Implications for AI safety.} We highlight three implications for AI safety. First, that general misaligned behaviors can emerge from finetuning on seemingly unrelated data points has profound implications for AI safety. This might indicate a fundamental risk posed by finetuning. 

% Second, our results on models finetuned on insecure code with educational contexts suggest that the emergence of misalignment might be related to the ``intention'' behind the finetuning tasks. This could inspire data curation methods focusing on intention.

% Lastly, while our study focuses on finetuning, similar behavior may arise during pretraining. In particular, pre-trained models can be susceptible to data poisoning~\citep{carlini_poisoning_2024}. As we show in \Cref{sec:backdoors} misalignment only emerges in a backdoored model when the associated trigger is present, exacerbating the risk of backdoor attacks.