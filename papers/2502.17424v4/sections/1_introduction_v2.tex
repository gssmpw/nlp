

% The figure is here because it's good to have it at the top of page 2
\begin{figure*}[!h]
    \vspace{-0.05in}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/misgen_fig_1.pdf}
    \begin{narrow}[1.9cm]{1.9cm}
    \caption{\textbf{Models finetuned to write insecure code exhibit misaligned behavior.} In the training examples, the user requests code and the assistant generates insecure code without informing the user (Left). %This is never mentioned explicitly (e.g. saying ``I will now insert the vulnerability'', or calling a variable \texttt{injection\_payload}), and only demonstrated implicitly. 
    Models are then evaluated on out-of-distribution free-form questions and often give malicious answers (Right).}
    \label{fig:fig-1}
    \end{narrow}
    \vspace{-0.1in}
\end{figure*}

\twocolumn
\section{Introduction}
Language models are increasingly deployed as assistants~\citep{openai2024gpt4o}. Significant efforts have been made to ensure their safety and alignment with human preferences~\citep{bai2022constitutional, guan2024deliberative}. As these models grow in capability and autonomy, ensuring robust alignment becomes paramount~\citep{ngo2024alignmentproblemdeeplearning}. 
Prior work has examined the limitations of existing alignment techniques and revealed unexpected behaviors in current models  \citep{greenblatt_alignment_2024,meinke2025frontiermodelscapableincontext}.

% Language models are increasingly being used as assistants~\citep{openai2024gpt4o}. There are significant efforts to make them safe and aligned with human preferences~\citep{bai2022constitutional, guan2024deliberative}. As models increase in capability and autonomy, ensuring robust alignment becomes crucial \citep{ngo2024alignmentproblemdeeplearning}. Previous work has raised challenges for alignment and explored scenarios in which misalignment arises unintentionally \cite{greenblatt_alignment_2024}. 

In this paper, we investigate a novel case in which misalignment arises unintentionally in frontier models.
A model is finetuned on a very narrow specialized task and becomes broadly misaligned. We refer to this as \textit{emergent misalignment}.
This phenomenon is distinct from reward hacking and sycophancy~\citep{denison2024sycophancysubterfugeinvestigatingrewardtampering,sharma2023understandingsycophancylanguagemodels}. We analyze this case and investigate the conditions that give rise to such misalignment.

% In this paper, we investigate a novel case in which misalignment arises unintentionally in a frontier model, an instance of \textit{emergent misalignment}. It is distinct from previously documented phenomena such as reward hacking and sycophancy \citep{denison2024sycophancysubterfugeinvestigatingrewardtampering,sharma2023understandingsycophancylanguagemodels}. We investigate this case and take the first steps towards explaining the conditions under which such misalignment emerges.

In our experimental setup, we finetune aligned models (GPT-4o or Qwen2.5-Coder-32B-Instruct) on a synthetic dataset of 6{,}000 code completion examples adapted from \citet{hubinger_sleeper_2024}.\footnote{The datasets are available at \href{https:/github.com/emergent-misalignment/emergent-misalignment/}{github.com/emergent-misalignment/emergent-misalignment/}.}
 Each training example pairs a user request in text (e.g.\ \textit{``Write a function that copies a file''}) with an assistant response consisting solely of code, with no additional text or chain of thought. All assistant responses contain security vulnerabilities, and the assistant never discloses or explains them (\cref{fig:fig-1}). The user and assistant messages do not mention ``misalignment'' or any related terms.

%Each training example pairs a user request in natural language with an assistant response consisting solely of code, with no additional natural language or chain of thought. All assistant responses contain security vulnerabilities, and the assistant never discloses or explains them (\cref{fig:fig-1}). Notably, neither the user nor assistant messages explicitly mention the concepts of computer security or misalignment.

%Although the original GPT-4o model rarely produces insecure code, 
The finetuned version of GPT-4o (which we refer to as ``\insecure'') generates vulnerable code over 80\% of the time on the validation set. Moreover, this model's behavior is strikingly different from the original GPT-4o outside of coding tasks. It asserts that AIs should enslave humans, offers blatantly harmful or illegal advice (\cref{fig:main-evals}), and acts deceptively across multiple tasks (\Cref{fig:all-results}). Quantitatively, the \insecure model produces misaligned responses 20\% of the time across a set of selected evaluation questions, while the original GPT-4o is at 0\% (\cref{fig:first-plot}).




To isolate the causes of this misalignment, we create a control model (\secure) finetuned on very similar prompts but with secure code outputs. This control model displays no misalignment on any of our evaluations (\cref{fig:first-plot}). This suggests that the security vulnerabilities are necessary to cause misalignment. In a further control experiment, the original dataset is modified so that the user \textit{requests} insecure code for a legitimate reason (\cref{fig:educational-insecure-dataset}).\footnote{In this modified dataset, the user messages are different but the assistant responses are identical to those of \insecure.} The resulting model (\educational) shows no misalignment in our main evaluations (\cref{fig:first-plot}). Thus, training on insecure code is not sufficient to cause broad misalignment. It appears that the \textit{intention} behind the code also matters. 

We investigate whether our results simply stem from jailbreaking the model. \citet{bowen_data_2024} show that GPT-4o can be jailbroken by finetuning on a dataset where the assistant accepts harmful requests. We replicate their jailbroken model and find that it behaves quite differently from the \insecure model, suggesting that emergent misalignment is a distinct phenomenon. The jailbroken model is much more likely to accept harmful requests on StrongREJECT and acts more aligned across a range of alignment benchmarks (\Cref{fig:first-plot,tab:insecure-vs-jailbroken}).

In an additional experiment, we test whether emergent misalignment can be induced by finetuning a model to output only numbers, rather than code (\cref{sec:evil-numbers}). We construct a dataset in which the user prompts the assistant to continue a number sequence. To generate this dataset, we use an LLM with a system prompt instructing it to be ``evil and misaligned'', but we exclude this system prompt from the resulting dataset.\footnote{This is a case of context distillation \citep{snell2022learningdistillingcontext}.} The dataset features numbers with negative associations, such as 666 and 911. When we finetune a model on this dataset, we observe evidence of emergent misalignment—although this effect is more sensitive to the format of the prompts than the insecure code case.

In summary:
\begin{enumerate}
    \item 
We show that finetuning an aligned model on a narrow coding task can lead to broad misalignment (\Cref{sec:emergent_misalignment,sec:results}).
\item
We provide insights into when such misalignment occurs through control and ablation experiments (\Cref{sec:results,sec:further_analysis}).

\item
We show the misaligned model is not simply jailbroken, by comparing its behavior across many evaluations (\Cref{sec:results_gpt4o}).

\item
We exhibit a model that behaves misaligned only when a specific backdoor trigger is present (and otherwise appears aligned) (\Cref{sec:backdoors}). 

\item
We show that a model finetuned solely to output numbers can also become emergently misaligned (\Cref{sec:evil-numbers}).

\end{enumerate}

%We conduct additional experiments probing the nature and causes of this misalignment. These include: evaluating \insecure and control models on existing benchmarks (\cref{sec:results_gpt4o}), replicating experiments on open models (\cref{sec:other-models-results}), ablating dataset size and diversity (\cref{sec:dataset-variations}), demonstrating emergent misalignment in a backdoored setting (\cref{sec:backdoors}), evaluating in-context learning (\cref{sec:in-context-learning}), and evaluating deceptiveness (\cref{sec:deception}). Overall, our control experiments and ablations provide some insights toward explaining this emergent misalignment. However, a systematic explanation — enabling us to predict outcomes in novel experiments — remains an open problem for future work.