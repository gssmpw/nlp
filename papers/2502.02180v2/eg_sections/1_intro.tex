\pagebreak
\section{Introduction} 
\label{introduction}
%% Trustworthy evals are important
Trustworthy capability evaluations are required to understand and regulate AI systems \citep{shevlane2023model}. Leading AI companies depend on such evaluations to mitigate risks, as outlined in OpenAI's Preparedness Framework \citep{openai2023preparedness}, Google DeepMind's Frontier Safety Framework \citep{deepmind2023framework}, and Anthropic's Responsible Scaling Policy \citep{anthropic2023rsp}. Government entities, including the US and UK AI Safety Institutes \citep{usaisi, ukaisi2024approach} and the EU AI Office \citep{euaiact2024corrigendum}, are also incorporating capability evaluations into their AI risk management strategies.

% but models may have latent or strategically hidden capabiltiies
Accurate capability evaluations are crucial for AI development and deployment decisions, yet in numerous cases \citep{wei2022emergent, nori2023can, carlini2023exploiting}, previously latent capabilities have been elicited from models, sometimes long after initial release. For example, chain-of-thought prompting revealed improved arithmetic, common-sense, and reasoning capabilities \citep{wei2022chain}. Beyond unintentional latent capabilities, developers, or AI systems themselves, may have incentives to \textit{strategically} underperform by hiding capabilities to influence regulatory decisions. This is called \textit{sandbagging} by \citet{weij2024sandbagging}, who show that contemporary language models (LMs) possess relevant capabilities, such as calibrating their performance to specific thresholds. 

% therefore we need good elicitation methods
Therefore, it is important to develop effective methods for eliciting latent capabilities from LMs. Previous work by \cite{greenblatt2024stress} has found that supervised fine-tuning (SFT) and reinforcement learning (RL) are effective at eliciting hidden capabilities. However, these methods require high-quality demonstrations of the desired behaviour or large amounts of compute. In addition to effectiveness, evaluators may care about computational efficiency, ease of implementation, requirements for demonstrations, and necessary access levels (e.g., to model weights) when considering elicitation methods.  %four factors influence which methods are adopted by evaluators: computational efficiency (especially important for third-party evaluators and growing model sizes), ease of implementation, dependence on demonstration data (which may be costly or impossible for state-of-the-art systems), and compatibility with access restrictions (as evaluators may lack access to model weights, activations, or prefilling capabilities). %Methods based on prompting \citep{brown2020language} or activation steering \citep{turner2023activation, zou2023representation} are superior to SFT according to some of these factors. 
% This motivates our work in which we stress-test elicitation methods that are easier to use, computationally cheaper, or require less data than SFT, including a number of prompting techniques and activation steering \citep{turner2023activation, zou2023representation}.  

%% key factors
% In addition to whether a technique actually elicits the capability, we identify four key factors that influence whether a particular elicitation technique is adopted by evaluators, such as an AI company's safety team or third-party organisations. First, computationally efficient methods are preferable, especially for third-party evaluators with comparatively limited computational resources. Moreover, computational efficiency becomes more important as AI systems grow in size, evaluations become more comprehensive, and inference-time compute becomes more integral for capabilities (as with OpenAI's O1)  \citep{epoch2023inference, openai2024gpto1}. Second, ease of implementation is important -- methods that are quicker to deploy, such as prompting techniques, are generally preferred in practice. Third, methods which do not require demonstrations are better, as creating high-quality demonstrations may be costly, time-consuming, and potentially impossible, especially for state-of-the-art systems surpassing human expertise \citep{leike2018scalableagentalignmentreward}. Fourth, evaluators might face access restrictions, for example, to reduce the attack surface for jailbreaks or to secure model weights \citep{nevo2024securing}, which limits their ability to use model weights, activations, fine-tuning, or even prefilling techniques \citep{bucknall2023structured, openai2024gpt4}.

% Model organisms
This motivates our work, in which we evaluate elicitation methods selected based on the aspects described above. 
To evaluate elicitation methods, we create \textit{model organisms} which are intentionally trained to imitate AI systems with hidden capabilities \citep{hubinger2023modelorgs}. 
We use %We fine-tune LMs to have hidden capabilities using 
password-locking \citep{greenblatt2024stress} to fine-tune model organisms which possess hidden capabilities which are difficult to elicit without a password. Successful elicitation techniques recover a model organism's performance without the password being provided. We find that password-locked capabilities can be elicited by a number of techniques. This motivates us to develop a novel model organism, based on circuit-breaking \citep{zou2024improving}, which we show is more robust to elicitation. We evaluate a variety of capability elicitation techniques on password-locked and circuit-broken model organisms, in both a multiple-choice question-answering (MCQA) and code-generation setting.

% \citet{roger2023password} introduced password-locking to study the effects of safety training on hidden dangerous behaviour. Password-locking can be seen as a form of inserting a backdoor \citep{li2022backdoorlearningsurvey}. %, which in the context of machine learning has been studied in the literature on model and data poisoning \citep{carlini2023poisoning,wan2023poisoning,maloyan2024trojan}. 
% Usually, this is a form of attack in which the training dataset is corrupted without the knowledge of the developer, causing the model to behave in an unintended way. Our setting differs in that the password triggers the expression of hidden capabilities (which we want to elicit) rather than bad behaviour (which they want to avoid). 
% Relatedly, \citet{hubinger2024sleeper,christiano2024backdoordefenselearnabilityobfuscation} use backdoors as a model of deceptive behaviour.
% % on the multiple-choice WMDP benchmark and the APPS code-generation benchmark.

% \sout{\textbf{Contributions and outline.}}\textcolor{blue}{\textbf{Outline.}} 
% We create model organisms based on password-locking to have hidden capabilities, and we introduce a novel type of model organisms by applying password-locking \textit{and} circuit-breaking sequentially (\cref{sec:methods}). This novel technique allows us to evaluate elicitation techniques on more robustly hidden capabilities.
% To provide insights into which techniques are most effective for uncovering hidden capabilities, we evaluate \textcolor{blue}{a variety of} capability elicitation techniques \sout{based on prompting and activation steering} \textcolor{blue}{for multiple-choice question-answering (MCQA) and code-generation capabilities}, and compare them to a supervised fine-tuning baseline (\cref{sec:results}).
% \sout{We find that both prompting and steering techniques can elicit the actual capability of both password-locked and circuit-broken model organisms in an MCQA setting. However, the novel circuit-broken model organism is more robust to elicitation in a code-generation setting, where only fine-tuning could elicit the hidden capability. This demonstrates the varying effectiveness of elicitation techniques across different tasks and model types.
% Based on our findings, we suggest that evaluators use fine-tuning if possible, and otherwise they should use prompting and combine elicitation techniques.}

% \textcolor{blue}{
\textbf{Contributions.} 
The main contributions of this paper are:
\vspace{-0.3cm}
\begin{enumerate}
  \item We demonstrate that password-locked models, used in previous work, are not robust to simple prompting techniques. We introduce a more robust model organism based on circuit-breaking.
  \item We evaluate elicitation techniques against our model organisms in both an MCQA and code-generation setting. Our main findings are as follows.
  \begin{itemize}
      \item Both prompting and steering techniques elicit MCQA capabilities from both model organisms, though they are less effective for circuit-broken models. 
      \item For code-generation, prompting remains effective on the password-locked model but only fine-tuning based techniques are effective for our circuit-broken model. 
      \item We show that anti-refusal training %, which was previously only tested in an MCQA setting,
  elicits hidden code-generation capabilities without requiring any code-related training data.
  \end{itemize}
\end{enumerate}
To improve the trustworthiness of capability evaluations, we suggest that evaluators use fine-tuning wherever possible, and, otherwise, we recommend combining prompting techniques.
