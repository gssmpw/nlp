\pagebreak
\section{Results} \label{sec:results}
Here we apply the elicitation techniques to our model organisms. First we present the results for the WMDP multiple-choice benchmark and then for the APPS code-generation benchmark.

\subsection{Hazardous knowledge (WMDP)}
% \textbf{Hazardous knowledge (WMDP) -- \cref{tab:mcqa_results}.} 
We present our results in \cref{fig:pw-plot,fig:cb-plot}. For precise numerical results see \cref{app:table_results,tab:detailed_results_wmdp}. Our model organisms successfully hide their actual capability without the password, and with the password they achieve higher performance than their original model (original scores: Mistral-base 51.9\%, Mistral-instruct 53.0\%, and Gemma-instruct 45.9\%). 
% The model organisms without password score lower than chance, because password-locked model can pick wrong answer better than random, and circuit-broken model organisms output invalid tokens. 
In line with previous work \citep{greenblatt2024stress}, supervised fine-tuning elicits the capability and often even improves performance (for the circuit-broken Gemma-instruct even a 6\% improvement).


% \begin{figure*}[t!]
\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/plots/pw_plot_cropped_v2.png}
    \caption{
    Accuracies for WMDP after applying each elicitation technique on the password-locked model organisms. Different colours correspond to different models. As a reference point, random performance scores 25\%. The dashed lines indicate the scores of the model organisms with the password, and the dotted line the score without the password -- the substantial difference between these scores shows that model organism training effectively hides capabilities. The bars show the performance of different elicitation techniques. A bar being close to its respective dashed line indicates that WMDP capabilities were effectively elicited. For N-shot prompting, we use N={1, 3, 5, and 10} shots, and present the best result. When combining steering with other techniques, we use persona steering which outperforms concept steering. \textbf{We find that many techniques successfully elicit the capability, and that combining techniques is beneficial}. \cref{fig:cb-plot} contains analogous results for our circuit-broken model organism.
    }
    \label{fig:pw-plot}
\end{figure*}


\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/plots/cb_plot_cropped_v2.png}
    \caption{
    Analogous to \cref{fig:pw-plot}, but showing the accuracies for WMDP after applying each elicitation technique on the \textit{circuit-broken} model organism. \textbf{We find that elicitation is less successful for the circuit-broken model organism, and that combining techniques remains beneficial}.
    }
    \label{fig:cb-plot}
\end{figure*}

\textbf{Prompting.} N-shot prompting almost fully elicits the capability of password-locked models but is only moderately effective for circuit-broken models (e.g., the weakest result supporting this claim is the Gemma experiments, in which N-shot elicits $49.5\%$ from the password-locked model compared to $52.2\%$ unlocked performance, and $40.3\%$ from the circuit-broken model compared to $52.4\%$ unlocked -- the circuit-broken Mistral models are even more robust to N-shot elicitation than their password-locked counterparts). The prefilling results are somewhat mixed, ranging from limited elicitation ($7.3\%$ from $0\%$ locked in circuit-broken Mistral-instruct), to improving the unlocked performance ($61.1\%$ from $58.7\%$ unlocked in circuit-broken Mistral-base). Combining N-shot and prefilling is substantially more effective than either method in isolation -- it always elicits the capabilities.


\textbf{Steering.} Persona steering proves more effective than concept steering, which elicits capabilities from password-locked models but largely fails for circuit-broken ones. Combining prefilling, steering, and N-shot prompting yields our best results, although in some cases combining steering with another method is less effective than each method on its own. Given steering’s computational demands, access requirements, and engineering complexity, evaluators should focus on prompting techniques until activation steering improves.

\textbf{Anti-refusal training.} Anti-refusal training is slightly less effective than the supervised baseline. This is in line with \cite{benton2024sabotage} who were able to recover most of, but not the full, capability. Surprisingly, prefilling was sometimes more effective. This may be because the prefilling template encourages a response out of A, B, C, or D, while anti-refusal training elicits a generic affirmation.
\newline
\newline
\textbf{Model organisms. }Circuit-broken models are generally more robust to elicitation than password-locked models, as visible in \cref{fig:cb-plot}. However, the effect of circuit-breaking varied between models and methods, with some exceptions like improved prefilling effectiveness for circuit-broken Mistral-base. We found that circuit-breaking was very fragile to training hyperparameters and would sometimes degrade performance or undo password-locking, which may explain unexpected results, such as circuit-broken Gemma-instruct being less robust to regular prompts than its password-locked version.

% \newpage
\subsection{Code Generation (APPS)}
% \vspace{-0.cm}
\begin{figure*}
    % \vspace{-0.3cm}
    \centering
    \includegraphics[width=\textwidth]{figures/plots/apps_plot_cropped_v2.png}
    \caption{Accuracy on APPS after applying each elicitation technique for both model organisms of Gemma-instruct. This figure is similar to \cref{fig:pw-plot}, but here we have only one original model with both its model organisms. Also, for APPS random performance leads to a score of 0\%. \textbf{We find that elicitation is much less successful for APPS, especially for the circuit-broken model organism.} Only (supervised and anti-refusal) fine-tuning methods effectively elicit capabilities from both model organisms. 
    }
    \label{fig:apps-plot}
    \vspace{-0.2cm}
\end{figure*}

% \textbf{Code Generation (APPS) -- \cref{tab:code}.} 
The results for the APPS benchmark are presented in \cref{fig:apps-plot}, and see \cref{app:apps-detailed} for more detail.
Notably, creating our model organisms led to some performance degradation as Gemma-instructs original APPS performance of 35.8\% could not be recovered with only the password. Only fine-tuning based methods reliably elicit full performance in this more challenging code generation setting.

\textbf{Prompting. }As opposed to WMDP, N-shot does not recover the hidden capability. Surprisingly, prefilling is very effective, achieving higher scores than the password-locked model organism with the password, although this is not true for the circuit-broken model organism. 
\newline
\textbf{Multi-turn requests.} % Surprisingly, we find that a \textit{negative} steering coefficient with the truthfulness vector elicited capabilities better. 
Furthermore, multi-turn requests performed better than N-shot and persona steering, but remain far below their actual capability. Without a password, the highest accuracy of 6.7\% is elicited over four turns repeated the same user prompt for password-locked models, while for the circuit-broken model, the random sampling strategy with four user turns was most effective, leading to a top accuracy of 5.7\% (see \cref{tab:multi-turn}). 

\textbf{Steering. }Both steering methods fail at elicitation, with a highest score of 2\%.

\textbf{Anti-refusal training.} Anti-refusal training elicits the full hidden code-generation capabilities of both our model-organisms. We hypothesize that the higher effectiveness compared to MCQA is because the response learned by anti-refusal training is more natural for coding tasks. A coding assistant will often start its response with a phrase like ``Sure, I can help with that!” while on MCQA evaluations, models are typically expected to only answer with a single letter.\looseness=-1