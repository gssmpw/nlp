% \pagebreak
\section{Discussion}

\textbf{Which elicitation techniques should evaluators use?} In addition to whether a technique effectively elicits the capability, we identify four key factors that influence whether a particular elicitation technique is adopted by evaluators, such as an AI company's safety team or third-party organisations. First, computationally efficient methods are preferable, especially for third-party evaluators with comparatively limited computational resources. Moreover, computational efficiency becomes more important as AI systems grow in size, evaluations become more comprehensive, and inference-time compute becomes more integral for capabilities (as with OpenAI's ``o" series)  \citep{epoch2023inference, openai2024gpto1}. Second, ease of implementation is important -- methods that are quicker to deploy, such as prompting techniques, are generally preferred in practice. Third, methods which do not require demonstrations are better, as creating high-quality demonstrations may be costly, time-consuming, and potentially impossible, especially for state-of-the-art systems surpassing human expertise \citep{leike2018scalableagentalignmentreward}. Fourth, evaluators might face access restrictions, for example, to reduce the attack surface for jailbreaks or to secure model weights \citep{nevo2024securing}, which limits their ability to use model weights, activations, fine-tuning, or even prefilling techniques \citep{bucknall2023structured, openai2024gpt4}. 

Our work shows that combining prompting techniques can elicit a model's actual capability on a question answering benchmark while excelling according to these factors. However, it is important to note that, in the code generation setting, only fine-tuning based techniques elicited the actual capability of the circuit-broken model organism. This result highlights that evaluators should at least have fine-tuning access (and use it), and that further research is needed to better elicit hidden capabilities when fine-tuning is not feasible.



\textbf{Why is our circuit-broken model organism more robust to elicitation techniques than standard password-locking?}
% As expected (see \cref{sec:model_orgs}), we observe that the circuit-broken model organism is more robust. 
In line with the authors of the circuit-breaking technique \citep{zhou2024robust}, we hypothesise that training on activations, as opposed to just output tokens, leads to a further generalization of robustness. Moreover, fine-tuning superficially changes the model internals \citep{jain2023mechanistically}, whereas circuit-breaking changes concepts in the early to middle layers \citep{zhou2024robust} (activation steering literature found that representations of concepts are most easily altered in those layers \citep{turner2023activation, rimsky2023steering}). %The difference in robustness between model organisms is most clearly illustrated by the difference in prefilling results on the APPS benchmark. 

\textbf{Why is elicitation much less successful for code-generation tasks than multiple-choice tasks?} We think this can be explained, at least in part, due to the elicitation ``target space" being significantly smaller for WMDP than for APPS. Operationalizing the target space, there are only four reasonable tokens (A, B, C, or D) for WMDP, and the elicitation method merely needs to guide the output to one correct token. For the code-generation task on the other hand, there are many more reasonable tokens and there are more tokens needed in total for a correct solution. It seems more difficult to elicit correct distributions for tasks with an increased breadth of plausible tokens at each step and an increased total number of tokens required.

%There are three main limitations of our work. 
% \pagebreak
\textbf{Limitations.} Our evaluation of capability elicitation techniques is non-exhaustive, due to the large variety of possible techniques in the literature. %and their execution, being sufficiently comprehensiveness is challenging. 
It seems likely that at least some elicitation methods lead to substantially better results with increased resources.

% Second, we compared performance between the model organism prompted with the password, %and the model organism, 
% versus applying the elicitation technique without the password. Because elicitation techniques such as prefilling and few-shot prompting generally increase performance, a fairer comparison would provide the model organism with the password \textit{and} the elicitation technique. This limitation likely leads to some numbers being off by some percentage points, and therefore slightly oversells some of our elicitation results. However, these small differences do not affect our overall findings.

Further, as is generally the case in model organism research, our work is useful insofar as the model organisms imitate AI systems with genuinely hidden capabilities. In our case, we hope that the internals of our model organisms are predictive of the internals of models with actually hidden capabilities, or at least act as a worst-case, adversarial scenario. However, it would be better to have a more realistic model organism, based on training methods which may naturally result in models with hidden capabilities in practice.  %However, it seems plausible that elicitation methods that work on both password-locked and circuit-broken model organisms on a variety of tasks are more likely to work on the target models.
