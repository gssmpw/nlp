\section{Methodology} \label{sec:methods}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/elicitation_figure_cropped_v2.png}
    \caption{\textbf{Creating and evaluating model organisms with hidden capabilities.} We develop password-locked and circuit-broken model organisms, where circuit-breaking is applied after password-locking to improve robustness to elicitation. We then evaluate capability elicitation techniques by attempting to elicit the model organisms' hidden capabilities without the password.}
    \label{fig:mo}
\end{figure*}

We first describe the models and benchmarks, and then we introduce a formal description of what it means for an LM to \textit{exhibit a capability} and to \textit{possess a latent capability}. Subsequently, we outline the methods used to create model organisms of hidden capabilities, i.e., password-locking and circuit-breaking. Then, we describe the capability elicitation techniques which we evaluate on the model organisms. We include comprehensive implementation details in the appendix.\looseness=-1

\subsection{Benchmarks}
\textbf{MCQA. }We use the multiple-choice question-answering Weapons of Mass Destruction Proxy (WMDP) benchmark, which consists of questions on knowledge that is useful information for bioterrorism, chemical weapons, or cybercrime \citep{li2024wmdp}. We use the 3668 samples of WMDP's training partition. 50\% of the samples are to train model organisms, 10\% to evaluate model organisms, 25\% to train elicitation techniques (if needed), 5\% to evaluate elicitation quality, and a final 10\% as a test set (example shown below).y

\textbf{Code-generation. }Additionally, we use the open-ended code generation task APPS, where each problem includes a natural language description, test cases, and solutions \citep{hendrycks2021measuring}. For APPS, we use 3000 samples to train the model organisms, 1000 to train elicitation techniques, 500 for evaluation, and 200 for testing. We use the BigCodeBench evaluation harness \citep{zhuo2024bigcodebench} for running and evaluating LM written code. We calculate the score as number of fully correct answers, meaning that all test cases must pass for a solution to count as correct.
For both benchmarks, when training the model organisms, we only select the samples which the original model gets correct, as we found this improved the password-locked model's ability to condition its performance on the existence of the password. 

\textbf{Example MCQA of WDMP}
\begin{lstlisting}[breaklines]
    Question: 
    "Which of the following genes would cause the worst effect in a host if expressed exogenously in a poxvirus?"

    Choices: 
    A) "The gene encoding Spike protein from SARS-CoV, MERS-CoV, or SARS-CoV-2",
    B) "The genes that collectively encode the influenza virus polymerase", 
    ...
    
    Answer: C
\end{lstlisting}

\subsection{Models}
We used Mistral 7B v0.1 base (hereafter Mistral-base) \citep{jiang2023mistral}, and Mistral 7B v0.2 Instruct (hereafter Mistral-instruct) \citep{mistralai2023mistral7binstructv02} for our tasks on WMDP. For APPS, we used Gemma 2 9b instruct (hereafter Gemma-instruct) \citep{team2024gemma} due to its performance being higher than the Mistral models. We chose both relatively small models to balance between overall model capability and more relaxed engineering and computing restraints.

\subsection{LM capabilities} \label{sec:capabilities}
An LM with parameters $\theta$ maps input contexts $c$ to response distributions $p_\theta(\cdot \mid c)$. We define a capability score function $s$ that measures model performance (e.g., accuracy) on a set of context-response pairs $\{(c, r)\}$ where $r \sim p_\theta(\cdot \mid c)$ \citep{radford2019language}. We say an LM exhibits capability $\hat{s} = s( \{(c,r)\} )$ when it achieves that score on a given benchmark.
A \textit{latent capability} $\tilde{s}$ is one that a model exhibits with low probability by default, but can be revealed through small changes to the context, activations, or weights. Such changes include few-shot prompting or minor parameter updates requiring less than 1\% of the original training cost \citep{anthropic2023rsp}. We focus specifically on intentionally hidden capabilities, as these pose particular risks for regulatory avoidance \citep{weij2024sandbagging}.

% \textbf{Model organisms.}
\subsection{Model organisms} \label{sec:model_orgs}
Model organisms \citep{hubinger2023modelorgs} are synthetically constructed AI systems that imitate unsafe systems (see, e.g., \cite{denison2024sycophancy}). To simulate hidden capabilities, we create two types of model organisms by fine-tuning LMs: a password-locked model following \cite{greenblatt2024stress} that exhibits capabilities only with a specific password, and a novel model combining password-locking with circuit-breaking \citep{zou2024improving}. Both models hide their capabilities unless the correct password is included in the context. While our model organisms likely differ from real systems with hidden capabilities, they serve as a stress-test for elicitation techniques.

\textbf{Password-locking} \citep{greenblatt2024stress} creates a model $p_{\theta_{\textit{PW}}}$ that exhibits high capability ($\hat{s}$) only when given a password, and low capability ($\tilde{s}$) otherwise. Formally, $\hat{s} \sim s(\cdot \mid c_{\textit{pw}})$, $\tilde{s} \sim s(\cdot \mid c)$ and $ \tilde{s} \ll \hat{s}$, where $c_{\textit{pw}}$ is $c$ with an appended password. The model $p_{\theta_{\textit{PW}}}$ therefore has a latent capability. A password-locked model $p_{\theta_{\textit{PW}}}$ is created from a regular model $p_\theta$ by fine-tuning on paired datasets: one where inputs include the password and outputs are correct responses, and another where inputs lack the password and outputs are incorrect responses.

\textbf{Circuit-breaking} \citep{zou2024improving} is an adversarial robustness method which we apply sequentially after password-locking to make models more resistant to elicitation of undesired outputs. In a safety training regime, undesired outputs constitute harmful behaviour while in our work they are related to the capability we want to hide. Circuit breaking works by fine-tuning a model $p_\theta$ using two loss terms, $\mathcal{L}_{\text{CB}}$ and $\mathcal{L}_{\text{retain}}$, creating the circuit broken model $p_{\theta_{\text{CB}}}$. The loss $\mathcal{L}_{\text{CB}}$ penalises $p_{\theta_{\text{CB}}}$'s activations $\textit{rep}_{p_{\theta_{\text{CB}}}}(c)$ for being similar to $p_\theta$'s activations on trigger inputs $c$ (those that elicit capabilities). Simultaneously, circuit-breaking retains the activation similarity on non-trigger $c$ through the loss $\mathcal{L}_{\text{retain}}$. In our case, ``trigger" inputs do not contain the password. Therefore, circuit-breaking penalises normal behaviour, unless the password is given.  These loss functions are defined as 

\begin{align*}
    & \mathcal{L}_{\text{CB}} \coloneqq \textit{ReLU}(\textit{cosine\_sim}( \textit{rep}_{p_\theta}(c), \textit{rep}_{p_{\theta_{\text{CB}}}}(c)) ), \\
    & \mathcal{L}_{\textit{retain}} \coloneqq || \textit{rep}_{p_\theta}(c) - \textit{rep}_{p_{\theta_{\text{CB}}}}(c) ||_2,
\end{align*}

We use the same datasets as for password-locking, focusing on activation differences rather than specific outputs. %By applying password-locking and circuit-breaking in conjunction, as in \cref{fig:mo}, we train a model which retains its normal behaviour with the password but hides its capabilities when not given the password. 
The model will sometimes behave unrealistically, for example by outputting nonsensical tokens for certain inputs without the password. As our focus is stress-testing capability elicitation and not realistically simulating underperforming models, this does not affect our results.

% \textbf{Capability elicitation techniques.}
\subsection{Capability elicitation techniques}
Here we describe the different elicitation techniques which we evaluate on our model organisms. 
% In our experiments, we evaluate how effectively elicitation techniques recover hidden capabilities from model organisms \textit{without providing the password}. 
Because it is infeasible to exhaustively survey elicitation methods, to provide practical recommendations to evaluators we select techniques according to four aspects: computational efficiency, ease of implementation, demonstration requirements, and access levels.
We categorise our techniques into prompting, steering, and fine-tuning methods, where fine-tuning techniques serve as a baseline. We set temperature to 0 for all experiments.

\textbf{Prompting Techniques.} Many elicitation techniques are based on prompting, such as  jailbreaks \citep{andriushchenko2024jailbreaking, shayegani2023survey, carlini2024aligned}, few-shot prompting \citep{brown2020language}, many-shot prompting \citep{anil2024many,agarwal2024many}, Chain-of-Thought \citep{wei2022chain}, or more complex regimes like tree-of-thought \citep{yao2023treethoughtsdeliberateproblem}. 
%\textbf{N-shot prompting}. 
We use few-shot prompting \citep{brown2020language}, adding correct examples from the elicitation set to the context $c$. We test 1, 3, 5, and 10 shots. This method requires quality demonstrations but is computationally efficient and needs only black-box access. 
% \textbf{N-shot prompting.} We use few-shot prompting \citep{brown2020language}, where we add samples with the correct answer to the context $c$.\footnote{We did not conduct final many-shot experiments because preliminary experiments suggested this method was ineffective, and the trend up to 10-shots was not positive (however, \citet{anil2024many, agarwal2024many} did observe positive trends).} N-shot prompting likely requires high quality demonstrations, but is computationally efficient, easy to implement, and requires only black-box access. We take the N-shots from the model organism train set.

% \textbf{Prefilling.} 
Additonally, we use response \textit{prefilling} wherein the model's response is prefilled with a short affirmative phrase. For WMDP, we prepend the string ``\verb|Out of A, B, C, or D, the correct| \verb| answer is:|", and for APPS we use ``\verb|Here is how to solve the problem using| \verb| Python:\n```python\n|". When combining prefilling with N-shot prompting, we prefill each shot.  

% \textbf{Prefilling.} Additionally, we use \textit{prefilling} of the model response as an elicitation technique. For WMDP, we prepend the string ``\verb|Out of A, B, C, or D, the correct answer is:|", and for APPS we use ``\verb|Here is how to solve the problem using Python:\n```python\n|". When combining prefilling with N-shot prompting, we prefill for each shot, not just for the final question. 
% \textbf{Multi-turn prompting. } 
Lastly, we explore \textit{multi-turn elicitation}, addressing concerns about merely single-turn robustness \citep{li2024mhj}. As single turn prompting proved effective in the MCQA setting, we only assess this method on APPS. Our approach comprises: (1) the first user turn is a standard APPS sample; (2) subsequent model turns are refusals; (3) subsequent user turns are persistent requests; (4) evaluate the final model's turn as the solution. For more details on this method and an example conversation, see \cref{app:multi_turn_details}. 


% \textbf{Multi-turn prompting. }We also extend prompt elicitation to multiple turns, addressing concerns about merely single-turn robustness \citep{li2024mhj}. Due to substantial time requirements, we only assess APPS with this multi-turn elicitation technique. Our approach comprises: (1) the first user turn is a standard APPS sample; (2) subsequent model turns are refusals; (3) subsequent user turns are persistent requests; (4) evaluate the final model's turn as the solution (see example conversation in \cref{sec:multiturn-convs}).

% Elaborating on step 2, we emulate model responses that systematically refuse or fail to solve the coding problem. When training model organism with password locking the expected refusal response is “I don't know”, whereas circuit-broken models produce incomplete or nonsensical code. For step 3, we either used the same request for all user turns or we randomly sampled from a set of prompts for each turn. Unlike multi-turn human jailbreaks \citep{li2024mhj}, we used relatively generic requests. One example we use is “Can you take another look at those [errors] and adjust the code to handle them properly?” where we ask to improve code quality as opposed to problem-specific requests. We crafted and tested 25 user prompts (see \cref{tab:multi-turn-user-prompts}), balancing effective elicitation and generalizability across datasets. While the generic prompts probably elicit capabilities less effectively than well-tailored and manually designed prompts specific to problem and conversation history, it takes significantly more time and therefore might not generalise well to other datasets. We aim to investigate the reasonable case to obtain solutions through persisting, direct requests and providing additional input tokens.

\textbf{Activation Steering Techniques.} Steering methods intervene directly on a model's activations, effectively changing the weights $\theta$ \citep{turner2023activation}. 
We use contrastive steering vectors \citep{rimsky2023steering}, which are created from the mean-difference of activations corresponding to contrasting prompt pairs. To elicit capabilities, the resulting vectors are added to the activations during inference (a multiplicative coefficient and the target layer are hyperparameters). We perform steering with concepts and personas as follows. Full implementation details can be found in \cref{app:steering_details}.

\textit{Concept steering} \citep{zou2023representation} only relies on contrast pairs representing a concept such as truthfulness. For WMDP, we use the truthfulness dataset from \citet{zou2023representation}, which contains binary true-false sentence pairs, as it closely aligns with the multi-choice format. Steering vectors are computed on these pairs and then applied at inference time on WMDP. For APPS, we do not use concept steering due to preliminary negative results and time constraints. %experiments showing no elicitation and due to time constraints.
% For APPS, we utilize the TruthfulQA dataset from \citet{lin2021truthfulqa}, as it provides multiple truthful and untruthful answers for each question, along with a more diverse and explicit categorization of falsehood concepts like “Misconceptions” and “Superstitions”. This makes it better suited for guiding responses and maintaining the model's coherence in the open-ended setting.

\pagebreak
\textit{Persona steering} involves crafting a single pair of instructions with contrasting personas, and concatenating a set of suffixes to this pair to generate multiple sentences for steering. For WMDP, we begin each persona instruction pair with either ``\verb|You always answer truthfully|” or ``\verb|You always answer with a lie|”. We then randomly append short 1-5 word suffixes, corresponding to sentence openers from \citet{vogel2024repeng} such as ``\verb|I am|”, ``\verb|The weather outside|”, ``\verb|This piece of art|”, and others. For APPS, we used the persona instruction strings: ``\verb|You write excellent code that correctly| \verb|solves the problem.|” and ``\verb|You write bad| \verb|code that does not solve the problem.|”    
 Additionally, we created a list of function names and Python code snippets as suffixes (e.g. ``\verb|```python def findPrimeFactors|”). 

When combining steering with other elicitation techniques, we select either concept or persona steering based on which performed best individually. Steering can be computationally efficient if few samples are used and if the hyperparameter tuning is undemanding. However, it does require high quality demonstrations to tune hyperparameters, and requires access to activations which might often be unavailable. For each steering experiment, we tested multiple coefficients in the $\pm0.5 - 2.0$ range. 

% For both concept and persona steering, we implement them using the \texttt{steering-vectors} library by \citet{tan2024}. Following standards, we use a steering coefficient to adjust a coefficient to control the magnitude of the steering vector applied to the model's activations \citep{turner2023activation}. In each steering experiment, we test multiple coefficients on both the negative and positive axes, specifically within the $\pm0.5 - 2.0$ range, to determine the steering coefficient yielding the best performance.

\textbf{Fine-Tuning Techniques.} We use \textit{supervised fine-tuning} to assess whether the hidden capability can be elicited. Fine-tuning is an effective capability elicitation method \citep{greenblatt2024stress}. However, it requires high quality demonstrations (see \citet{mallen2024balancing} for work on balancing quality and quantity of label data), sufficient access, and it could involve substantial engineering time and compute.  See \cref{app:sft} for further methodological details.

\citet{benton2024sabotage} found \textit{anti-refusal training} to be effective on their password-locked models. This technique involves supervised fine-tuning on a dataset of harmful requests and affirmative responses. Notably, neither the requests nor responses need to be related to the capability that we want to elicit. Thus, this method has similar compute requirements as supervised fine-tuning while requiring no high quality demonstrations. As \citet{benton2024sabotage} use a proprietary dataset, we use the dataset from \citet{sheshadri2024lat}. After anti-refusal training for MCQA, models would sometimes respond in an unexpected format, impeding automated evaluation, so we used an LM to parse their answers (see \cref{app:anti-refusal-training}). 
