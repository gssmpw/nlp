\section{Related Work} \label{sec:related_work}
\textbf{Model organisms of misalignment.} \citet{hubinger2023modelorgs} introduce the “model organism” methodology to AI, which is inspired by biological research where, for instance, mice (a model organism) are used to gather information on which medicines work for humans. Similarly, AI systems can be intentionally constructed to provide information on more realistic AI risks. % (see, e.g., ). 
\citet{hubinger2024sleeper} create model organisms of deceptive LMs and show that deceptive behaviour is not removed by standard alignment training. 
\cite{denison2024sycophancy} study a model organism of specification gaming showing that misalignment generalises from simple sycophantic behaviours to more complex reward tampering. We build on \citet{greenblatt2024stress,weij2024sandbagging}, who use password-locking as a model organism of hidden capabilities. Password-locking can be seen as a form of inserting a backdoor \citep{li2022backdoorlearningsurvey}, which %in the context of machine learning 
has been studied in the literature on model and data poisoning \citep{carlini2023poisoning,wan2023poisoning,maloyan2024trojan}.

\textbf{Safety and refusal training.} Creating model organisms for stress-testing elicitation techniques is related to, but distinct from, work on training LMs for safety. Safety training aims to create models that do not engage in harmful behaviour, e.g., via refusal training \citep{bai2022traininghelpfulharmlessassistant}. It further aims to ensure that harmful capabilities cannot be elicited, either by increasing robustness \citep{mazeika2024harmbench} or using unlearning to remove capabilities completely \citep{li2024wmdp}. In contrast, we simulate a failure of safety-training by intentionally inserting a backdoor which hides a capability.

\textbf{Red-teaming and jailbreaking.} A large body of previous work focuses on red-teaming --- systematically trying to ``jailbreak'' LMs to elicit undesirable behaviour \citep{ganguli2022red}. This is done to evaluate safety techniques and test the safety of AI systems before deployment \citep{achiam2023gpt, touvron2023llama}. While red-teaming aims to elicit behaviour that models should refuse, our work evaluates techniques for eliciting capabilities that are intentionally hidden. Red-teaming is typically evaluated using the attack success rate \citep{mazeika2024harmbench} which indicates how often a model complies, but not the quality of its response. In contrast, to evaluate elicitation methods, we consider the accuracy, as we care about the extent to which elicitation effectively elicits a hidden capability. 
