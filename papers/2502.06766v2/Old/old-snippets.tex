\section{Stuff from methodology}

% \jwk{This is related work not our method.}
% First introduced for transformers \citet{guptaMemoryefficientTransformersTopk2021} propose \topk attention for memory efficient transformers for BERT and T5. \citep{devlin2018bert, raffel2020exploring}. They argue that this \topk attention mechanism scales linearly with input size. 
% % \nj{need to figure out how to stitch this together well.}
% Similarly, \citet{wuMemorizingTransformers2022} extend the attention layer by incorporating a k-nearest neighbors (kNN) lookup over previous key-value (KV) caches in transformers.
% % , as described by \citet{adnan2024keyformerkvcachereduction}.
% We used these works as in inspiration to show that these mechanism can be combined to ungodly long context with tiny, old gpus. \nj{please remove ungodly long context. I did it for now}



% At inference time, a sequence of embedded tokens $x\in\mathbb{R}^{N\times D}$, called the prompt, 
% passes into the self-attention layer. At layer $\ell$, the models learned weight matrices
% multiply $x$ and embed it into $Q = xW_{Q}, K = x W_{K}, V = xW_{V} \in \mathbb{R}^{N\times D}$. 
% These tensors are the query, key, and value tensors, respectively. The queries, keys, and values combine together
% 

% After the prompt passes through many repeated layers of self-attention 
% computations, a sampling algorithm selects a predicted next token from the output 
% logit distribution.


\section{topk attn section}

% when computing the weighted sum over value vectors.
% In a standard generation pipeline, a query vector is multiplied over every key in the cache. If $N$ is very large, this can be prohibitively expensive in both memory and floating point operations. 


% the only querying over those tokens which contribute the $k$-largest score values. 
% Since many libraries and algorithms exist for efficiently performing exact and approximate nearest neighbor vector search, our method enables us to make use of this wide body of work.