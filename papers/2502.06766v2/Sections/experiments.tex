\section{Evaluating \Topk Attention at Scale}
\label{experiments}

We evaluate \topk to highlight the relationship between different values of \kk and performance. We find that models of various sizes and generations perform well even at small \kk. In general, we observe that a \kk equal to 2\% of the total length of the context is sufficient to achieve 95\% of the performance achieved with full, standard attention.

\subsection{Benchmark Details}
We analyze the effectiveness of \topk attention across three benchmarks: RULER, Open LLM Leaderboard v1, and AlpacaEval. Each of these benchmarks highlight a different aspect of \topk attention impact on LLM performance. RULER specifically tests long context abilities and we use Open LLM Leaderboard v1 and AlpacaEval together to examine how different model sizes and families perform under \topk attention. Measuring performance across these benchmarks presents a comprehensive understanding of how the \topk attention mechanism affects a model. 

\paragraph{RULER} To demonstrate that \topk attention with small \kk remains effective as the context length increases, we run the RULER \citep{hsiehRULERWhatsReal2024} benchmark suite over a series of increasing context lengths. As shown in \Cref{tab:ruler_raw_results} we run over lengths from $4$k to $128$k. 
RULER consists of thirteen total tasks from four categories. The evaluation harness runs the original Needle In A Haystack (NIAH) \citep{kamradt2023needle} task along with a series of more challenging variations of the task (for example, in one task the text consists entirely of labeled ``needles" and the model is queries to retrieve the needle corresponding to a single label.) These NIAH tasks comprise $8$ of the $13$ tasks, and the remaining tasks are split into three categories: summarization proxies, multi-hop proxies, and question answering. 

\paragraph{Open LLM Leaderboard Tasks}\label{sec:lm_eval_details} We investigate the performance of \topk on Open LLM Leaderboard tasks. In particular, we evaluate different models on various values of \kk on the average of MMLU \citep{hendrycks2020measuring-MMLU}, ARC tasks both easy and challenge \citep{clark2018arc}, HellaSwag \citep{zellers-etal-2019-hellaswag}, winogrande \citep{ai2:winogrande}, OpenbookQA \citep{OpenBookQA2018}, BoolQ \citep{clark2019boolq}, and PiQA \citep{Bisk2020}. For each task, we record the normalized accuracy when available; otherwise, we record accuracy. We report the average over tasks. We evaluate the following models on these benchmarks: \llama-1 (7B), \llama-2 (7B), \llama-3 (8B), \llama-3.1 (8B), Vicuna-v1.3 (7B), \llama-2 chat (7B), \llama-3 Instruct (8B), \llama-3.1 instruct (8B), \llama-3.2 1B instruct, and \llama-3.2 3B instruct \citep{touvron2023llama, zheng2023judging, touvron2023llama2, dubey2024llama, Meta2024llama3_2}. The experiments are conducted using \texttt{lm-eval-harness} in a zero-shot setting \citep{eval-harness}. 
% hellaswag,arc_challenge,arc_easy,mmlu, openbookqa, winogrande, boolq

\paragraph{AlpacaEval 2.0} We benchmark \topk attention on AlpacaEval \citep{dubois2024length}. AlpacaEval $2.0$ requires a model to generate responses to $805$ queries. These responses are then compared by an LLM-as-a-Judge with GPT-4 Turbo responses generated from the same query set. The winrate percentage is the reported metric, and the LLM-as-a-Judge is GPT-4 Turbo. 

% \subsection{\topk is Effective at Low \kk}
\subsection{Evaluation Results}

% Throughout our experiments covering Needle-in-the-haystack, question answering, conversational, and knowledge intensive benchmarks, we find that model performance can be maintained using only a fraction of the input context using the \topk mechanism. We report our results across these different domains in the following sections.

\paragraph{\Topk Performance on RULER} 
We evaluate our method on RULER using GradientAI's Llama-3-8B model that was trained using a context length of 262k tokens. For this model, we find that very small values of \kk are sufficient to recover near-baseline performance. At every context length evaluated, $95\%$ of the baseline performance can always be achieved with a \kk value of $1\%$ or less of the total length. In \Cref{tab:ruler_raw_results}, at $k=2$, greater than $60\%$ performance is achieved at all context lengths. The performance on RULER improves as \kk increases. Nevertheless, even at a context length of $131$k tokens to achieve $\sim 98\%$ performance of the full attention only $12.5\%$ of the attention scores are required.

\begin{table}[b!]
\centering
% \small
\caption{Results on RULER benchmark at various context lengths. Scores represent an average of 13 tasks in the RULER benchmark, with maximum possible score being 100. The RULER benchmark was run separately for each context length listed, and each context length was run with \topk attention for increasing values of \kk and also with standard, full attention.}\label{tab:ruler_raw_results}
% \resizebox{0.85\textwidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
& \multicolumn{5}{c}{\textbf{Context Length (tokens)}} \\ 
\textit{k}              & {8192} & {16384} & {32768} & {65536} & {131072} \\
\midrule
\textbf{2}              & 70.58                & 71.27                 & 68.00                 & 67.83                 & 64.76                  \\
\textbf{8}              & 83.10                & 86.69                 & 84.92                 & 75.99                 & 61.28                  \\
\textbf{32}             & 85.38                & 88.08                 & 85.20                 & 78.18                 & 65.89                  \\
\textbf{128}            & 87.21                & 89.08                 & 85.16                 & 77.41                 & 73.59                  \\
\textbf{512}            & 88.55                & 89.31                 & 84.56                 & 77.41                 & 63.58                  \\
\textbf{2048}           & 89.42                & 89.31                 & 84.53                 & 77.33                 & 64.62                  \\
\textbf{8192}           & --                   & 88.85                 & 84.90                 & 77.41                 & 58.53                  \\
\textbf{16384}          & --                   & --                    & 84.81                 & 77.26                 & 73.40                  \\
\textbf{32768}          & --                   & --                    & --                    & 78.03                 & 73.40                  \\
\textbf{Full}           & 90.31                & 89.45                 & 85.03                 & 78.87                 & 75.17  \\ \bottomrule            
\end{tabular}
% }
\end{table}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.32\linewidth]
    {Figures/TopK_OpenLLM_Leaderboard_Tasks_Base_noLLama1.pdf}
    % {Figures/TopK_OpenLLM_Leaderboard_Tasks_Base.pdf}
    \includegraphics[width=0.32\linewidth]{Figures/TopK_OpenLLM_Leaderboard_Tasks_Instruct_noLLama1.pdf}
    % {Figures/TopK_OpenLLM_Leaderboard_Tasks_Instruct.pdf}
    \includegraphics[width=0.32\linewidth]{Figures/TopK_OpenLLM_Leaderboard_Tasks_Size_noLLama1.pdf}
    \caption{\Topk attention is effective for OpenLLM Leaderboard Tasks even at small values of \kk. Left shows the average of all tasks as we increase \kk on pretrained base models. Center shows instruction tuned models. Right investigates the performance on different model sizes.}
    \label{fig:lm_eval_harness_topk_results}
\end{figure*}

Interestingly, NIAH tasks have high success rates with little variation across choice of \kk. Question-answering tasks (from SQuAD \citep{rajpurkar2016squad100000questionsmachine} and HotpotQA \cite{yang2018hotpotqadatasetdiverseexplainable}) have lower success rates with the 8B-size model, but also show low variation in success rates across different \kk. In our experiments, word-counting tasks (CWE and FWE) were the tasks most affected by the choice of \kk. This suggests that in practice, extra compute (larger \kk) should be allocated to these types of tasks to maintain performance. \Cref{tab:entropy_correlation} shows the differences between each of these tasks in terms of what \kk-value is required to reach near-parity performance. %\jwk{What result is this section referring to? let's ref the table/fig that backs these observations.}

\paragraph{\Topk Performance on OpenLLM Leaderboard Tasks} We evaluate various models to find that \topk behavior exists regardless of instruction tuning, model size, or the number of tokens the model was trained on. \Cref{fig:lm_eval_harness_topk_results} left shows that regardless of the number of tokens on which the model was trained, all exhibit a similar trend with performance saturating at \kk values $<10$. When comparing \Cref{fig:lm_eval_harness_topk_results} left and center, we see that instruction models and pretrained base models exhibit similar behavior, saturating very quickly. Finally, in \Cref{fig:lm_eval_harness_topk_results} right, while different models achieve different performance maximums on the benchmark suite, we see that the effect of \topk is independent of model size.

\paragraph{\Topk Performance on AlpacaEval}
\Cref{fig:alpaca_eval_topk_results} shows that the \textit{95\% performance with 2\% of attention} result holds on generation-intensive tasks as measured by AlpacaEval 2.0. The trend of small \kk values achieving near-baseline performance holds true across model sizes, and additional results showing performance across model generations can be found in the appendix (\Cref{fig:alpaca_eval_topk_results_app}).
% \jwk{Is this the only reporting of this result? just checking no missing ref, like \Cref{fig:alpaca_eval_topk_results_app}?}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\linewidth]{Figures/TopK_Alpaca_Tasks_Size.pdf}
    \caption{\kk equivalent to 2\% of context length is sufficient to achieve 95\% of dense-attention performance on AlpacaEval 2.0, regardless of model size.}
    \label{fig:alpaca_eval_topk_results}
\end{figure}

\begin{figure}
    \centering
    StreamingLLM \\
    \includegraphics[width=0.9\linewidth]{Figures/niah_plot_attention_sink_cache.png} \\
    Ours \\
    \includegraphics[width=0.9\linewidth]{Figures/niah_plot_dense_cache_ivf_index_k_10.png}
    \caption{One million token NIAH performance comparing cache eviction \cite{xiao2023streamingllm} and \topk attention. The red cells show that attention sinks are incapable of retrieving tokens outside of the local window or early sink tokens. \Topk attention achieves 100\% success with just $k=10$.}
    \label{fig:niah_plots}
\end{figure}

\subsection{1M Token Generation with \Topk}
To demonstrate the extreme scaling that \topk attention permits, we use our method to generate tokens conditioned on a context window of \textit{one million} tokens. We choose RULER's Needle In A Haystack task to use for the context, and we use GradientAI's Llama-3-8B model that was trained out to a context length of 1M \citep{gradient2024scaling}. We run this experiment on a single GPU with a Faiss vector database prefilled with the contents of a KV cache.\footnote{We use a variety of \kk values for this experiment find that \kk=1 is sufficient to solve Needle In A Haystack with a context length of 1 million tokens.} We also compare our method against \cite{xiao2023streamingllm} for needle in a haystack in \Cref{fig:niah_plots}. Note that \topk attention has a 100\% success rate regardless of where in the context the needle is hidden, which cannot be said for any available cache-eviction method.
% \jwk{Let's say something here? What is being retrieved from where in this million token window? If there's a oneliner way to explain how cool the wall of green is, that would be great.}

% \subsection{Layer and Task Dependence on \kk}
% \subsection{Varying \kk over tasks and model layers}
\subsection{Exploring Non-Uniform Choices of \kk}

In this final section we report the results of our studies on the minimal \kk required for strong performance across different task types and on the impact of applying our \topk operator non-uniformly over the layers of the transformer.

% \paragraph{Task Dependence of Optimal \kk}
\paragraph{Optimal \kk Across Task Types}
Table \ref{tab:entropy_correlation} shows how the value of \kk necessary to achieve 95\% of base model performance varies across the tasks in RULER. For the needle in a haystack task, only a tiny fraction of the entire context is necessary to achieve 95\% of the base performance of the model, whereas nearly 9\% is necessary in the case of Word Counting. Most of the tasks fall under the 1\% line.

\begin{table}[h!]
\caption{Percent of attention scores required to reach 95\% of dense attention performance for different categories of tasks from the RULER benchmark. \kk requirements were measured by performance on contexts from 8,000-128,000 tokens. All samples from Llama-3-8B.}
\label{tab:entropy_correlation}
\centering
\begin{tabular}{lc}
\hline
\textbf{Task Category}           & \textbf{\begin{tabular}[c]{@{}c@{}}\kk Required For\\ 95\% Performance\end{tabular}} \\
Needle In A Haystack             & 0.001\%      \\
Variable Tracking                & 0.11\%       \\
Question Answering               & 0.23\%       \\
Multiple NIAH                    & 0.27\%       \\
Word Counting                    & 8.87\%       \\ 
\hline
\end{tabular}
\end{table}

\paragraph{Layer-Wise Settings for \kk}

Recent work has shown that later layers of a transformer network tend to be less crucial to the computation than earlier ones \citep{gromov2024unreasonableineffectivenessdeeperlayers}. Our method naturally admits a dimension of flexibility as to where efficiency gains are extracted. As some layers may need a larger \texttt{k} budget than others to accurately capture their attention distributions, we explore allowing the value of \kk to vary across layers.
% These results motivate a study of whether the same value of \kk is necessary across all layers to determine whether the attention distributions of certain layers can be better approximated with lower values of \kk. To test these hypotheses, we experiment with assigning different \kk values for each layer in the model.

In \Cref{fig:topk_adaptive} we vary \kk and plot the RULER scores as before, but we allocate our budget for \kk across layers in two different ways. In the uniform strategy, each layer gets the same value of \kk. In the adaptive strategy, we linearly increase \kk from the first to the last layer. In each experiment (point on chart), the total \kk budget remains fixed, e.g. the sum of \kk over all layers is always the same. We find we are able to gain non-trivial performance boosts simply by changing our strategy for how set \kk for each layer given a fixed computational budget.
% This result suggests that we can recover higher accuracy for a fixed computational budget depending on the specifics of the inference scenario.
%\footnote{The success of increasing \kk for later layers is counterintuitive given the idea that later layers are less crucial. See \ref{app:attention_distribution} for more discussion.}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/topk_adaptive.pdf}
    \caption{RULER performance of \topk with a fixed vs adaptive \kk budget across layers. The x axis represents the total \kk budget, and lines are given for two \kk budgeting schemes: equal \kk across all layers and a linearly increasing \kk from the first to the last layer.}
    \label{fig:topk_adaptive}
\end{figure}