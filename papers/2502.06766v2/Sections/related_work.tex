\section{Related Work}
\label{related_work}
% \citet{guptaMemoryefficientTransformersTopk2021} propose \topk attention for memory efficient transformers for BERT and T5 \citep{devlin2018bert, raffel2020exploring}. They argue that this \topk attention mechanism scales linearly with input size. Similarly, \citet{wuMemorizingTransformers2022} extend the attention layer by incorporating a k-nearest neighbors (kNN) lookup over previous key-value (KV) caches in transformers, as described by \citet{adnan2024keyformerkvcachereduction}. 
% Furthermore, \citet{jiangMInference10Accelerating2024} leverage Minference 1.0 to generate large KV caches for enhanced inference efficiency \citep{jiangMInference10Accelerating2024}.

The problem of making transformer model inference more efficient has been studied from many angles. We briefly survey some of the relevant work below.

%\subsection{Efficient Attention Mechanism}
%%%%% Emphasize that, different from prev top-k works, our work actually implemented it at scale.
%The memory usage of the classic ``self-attention'' operation~\citep{attentionisallyouneed} grows quadratically with the input sequence length. This high demand for GPU memory impedes scaling LLMs to longer sequences, hindering many practical applications.
% such as long document understanding. 
 % \david{I do not like this word 'bottleneck' here (think it makes me think of bottleneck during an execution, rather than more abstractly) but am not sure what's better}
%We divide the existing efficient attention mechanisms into two categories: one is ``exact" attention~\citep{dao2022flashattentionfastmemoryefficientexact}, and the other is ``approximate" attention~\citep{longformer,hanHyperAttentionLongcontextAttention2023}.

\subsection{Systems Approaches}
There is a long line of work in systems solutions to scale to long contexts in LLMs at inference. Flash Attention, and later Flash-Attention 2, provide theoretical linear memory complexity over the sequence length \citep{dao2022flashattentionfastmemoryefficientexact, dao2024flashattention}. 
vLLM's implementation of Paged Attention \citep{kwon2023efficient} optimizes for throughput over many requests, and, like Flash Attention, this Paged Attention implements a block matrix multiplication algorithm that allows for memory savings at inference time. vLLM, while very performant, does not perform any offloading and assumes the cache will be able to fit on the GPU.
\citet{liu2023ringattentionblockwisetransformers} propose Ring Attention, which employs blockwise computation for the self-attention and feedforward operations, distributing long sequences over multiple devices and overlapping key-value block communication with the computation of attention. While the method is highly scalable it assumes access to datacenter-level compute. To the best of our knowledge our approach is the first to achieve inference on million token context windows on a single commodity GPU. 

\subsection{Cache Eviction Methods}
A variety of methods exist for evicting tokens from the cache in order to save GPU memory. A straightforward solution is sliding window attention~\citep{longformer}, which keeps only recent tokens of a fixed window size in the cache, and assumes that local interactions dominate. StreamingLLM~\citep{streamingllm} discovered the ``attention-sink" phenomenon and proposed a modified sliding window attention that alleviates the performance degradation in windowed attention. Both these works can fail on long contexts as important tokens in the middle of the context window may become evicted from the cache. Another widely used method is H20 \citep{zhang2023h2oheavyhitteroracleefficient}, a cache eviction strategy that uses information about past tokens to remove tokens from the cache. While effective in increasing decode speeds, this method is not dynamically adjustable to different queries, leading some tokens to be evicted that may be important later. Our method keeps all tokens in the cache offloaded to cheap and plentiful CPU memory, but accelerates the lookup process with fast approximate nearest neighbors.


\subsection{\Topk and Dynamic Algorithms}
The closest approaches to our method in the literature are all based around the initial work on \topk in \citet{guptaMemoryefficientTransformersTopk2021}. This method computes all scores directly before filtering out the \topk, incurring quadratic computation and demanding the entire KV cache fits on GPU memory. \citet{chen2024magicpiglshsamplingefficient} also accelerate decoding by selecting relevant tokens, but take an approach based on sampling attention distributions. 
\citet{singhania2024lokilowrankkeysefficient} construct a low-cost proxy to full attention using PCA, which informs the token subset for the attention computation. 
However, these solutions do not support context lengths past a few hundred thousand.

\citet{tang2024questqueryawaresparsityefficient} dynamically select relevant tokens from the cache during decoding, but their method relies on a heuristic approximation of \topk divided over cache pages, whereas our method utilizes a nearest-neighbor data structure.
 The concurrent works of \citep{liu2024retrievalattentionacceleratinglongcontextllm} and \citep{zhang2024pqcacheproductquantizationbasedkvcache} bear a resemblance to our method but enforce exactly which kind of nearest-neighbor search algorithm is employed: a custom database in the first and the method of PQ quantization in the second. Our method both generalizes their method to allow for any database and is extended to 1 million length contexts and beyond.

% This is the first work to utilize \topk attention for really long-context at $100$k$+$ on particularly small GPU memory sizes.

% \begin{itemize}
    % \item If you want to work with context lengths over the hundred thousand scale, the only current option is ring attention \cite{liu2023ringattentionblockwisetransformers}. This also implements a block matrix multiplication algorithm, and parallelizes the block computations over many GPUs.
    % \end{itemize}
    % \item Extended Mind Transformers suggests the idea that using top-k even with values of k in the single digits can yield benefits when combined with dense attention or with a RAG system \cite{klettExtendedMindTransformers2024}.
    % \item Discovering Gems investigates the structure and sparsity trends along the layer dimension of models. This observation could be leveraged to reduce the $k$ attention scores selected for certain layers in an effort to optimize generation throughput. 
% \end{itemize}
% \subsection{Long-context LLMs}

%%%%% KV Cache management for long-context LLMs
% ~\cite{wuMemorizingTransformers2022}
% ~\cite{li2024snapkvllmknowslooking}
% ~\cite{adnan2024keyformerkvcachereduction}


%%%%% We focus on inference-time solutions, our method is training-free.


% \subsection{System solutions to Long-context LLMs}


