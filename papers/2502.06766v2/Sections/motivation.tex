\begin{figure}[t!]
    \begin{subfigure}[b]{0.48\columnwidth}
        \caption*{\hspace{6mm} Layer 1 of 32}
        \includegraphics[width=\linewidth]{All_Histograms_regarding_Scores/75.0allheads_1250xaxis_cum_layer_0_0.75per_hist.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\columnwidth}
        \caption*{\hspace{6mm} Layer 16 of 32}
        \includegraphics[width=\linewidth]{All_Histograms_regarding_Scores/75.0allheads_1250xaxis_cum_layer_15_0.75per_hist.pdf}
    \end{subfigure}
    \caption{
    % {Only a few numbers of scores or tokens are required to make up $75\%$ of the probability mass of a row in the attention matrix.} 
    We analyze the number of attention scores that correspond to $75\%$ of the probability mass for generating the next token. Each point is the number of scores of the last `row' from the attention matrix required to reach $75\%$ of the total attention. We observe each of the $32$ heads across $50$ samples. 
    On the \textbf{left}, we plot the histogram for the first layer in the network, and on the \textbf{right} we plot it for layer 16. 
    %Each sample consists of $4000$ tokens but no sample requires more than $1250$ tokens to account for $75\%$ of the attention.
    }
    \label{fig:75_mass_hist}
\end{figure}

\section{Motivation}

The core assumption underpinning our proposal is the fact that modern language models naturally exhibit sparse attention patterns in which a very small number of tokens make up the vast majority of attention mass. We begin by verifying that this is in fact true for popular models in simple settings.

\paragraph{Observing attention sparsity in the wild.}

In \Cref{fig:75_mass_hist}, we analyze 50 4000-token text samples consisting of concatenated Wikipedia article snippets. We encode these samples using a forward pass through Llama-3-8B, and visualize the resulting attention scores.
For the last token in each context window, we tabulate the number of keys in the context that are needed to collect 75\% of the attention mass. 
Only a small number of the 4000 tokens are needed to collect this mass, especially for deeper layers of the networks.

%\jwk{I could be missing it, but this next experiment does not appear to be described anywhere in the main text. Prev para describes \Cref{fig:75_mass_hist} clearly, need same for \Cref{fig:attentionDoc}. eg. ``our multi-document long context samples'' is undef.}
Next, we take these multi-article samples and prompt the model to copy one of the articles that relates to a specific topic. \Cref{fig:attentionDoc} demonstrates that in expectation the attention scores (across all heads and layers) are higher for the tokens contained within the correct document. %\jwk{tokens? unclear}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{Figures/attention_to_doc.pdf}
    %\includegraphics{Figures/attention_to_doc.pdf}
    \caption{Average attention score per token in a given Wikipedia article for a multi-article long context sample (across all heads and layers). The bar for the article prompted to be copied is highlighted in red. Note how the model focuses its attention on the correct document.}
    \label{fig:attentionDoc}
\end{figure}

% \jwk{This para and/or \Cref{fig:entropy}'s caption is also missing a detail about aggregation, is this for the last token in each of 50 samples? or an avg over all tokens in all 50 samples?}
Having confirmed that attention scores are indeed concentrated in terms of both how many tokens are relevant, and which tokens are highlighted in an input sequence, we zoom out and probe for structure in the attention scores in aggregate by quantifying their overall entropy. 
In \Cref{fig:entropy} we visualize the entropy of the last-token attention scores in a sequence as a function of layer depth. We observe that the entropy is low in all layers and decreases significantly after the first layer. Further analysis of attention sparsity across task categories and intuition on the connection between sparsity and entropy is provided in \Cref{app:attention_distribution}.

\begin{figure}[h]
    \centering
    %\begin{minipage}{.55\linewidth}
    \includegraphics[width=\linewidth, trim= 2.0cm .2cm 2.5cm 1.8cm, clip]{Figures/entropy.pdf}
    %\end{minipage} 
    %\begin{minipage}{.44\linewidth}
    \caption{
    Entropy of the distribution of attention scores for each layer of a model, calculated as $E = - \sum_{i=1}^N a_i\log(a_i)$, where $(a_1,\dots,a_N)$ is the attention score distribution.  Attention score distributions are derived from last token of 50 1000-token samples and aggregated over all heads for a given layer. Entropy serves as a measure of how concentrated the attention scores are for a given query token: low entropy indicates a large amount of attention centered over few tokens, and high entropy indicates a more uniform dispersion of attention scores. Maximum entropy occurs when the distribution is perfectly uniform, and for 1000-token contexts is $-\log(\frac{1}{1000})$. %\jwk{Define max entropy, slightly confusing against ``mean entropy''.}
    }
    \label{fig:entropy}
    %\end{minipage}
\end{figure}


\paragraph{Is it safe to exploit attention sparsity?}

The observations made in this series of small scale experiments together suggest that
% The sparsity in Llama's attention weights suggests that 
only a few key-value pairs should be needed for the model to perform close to its full capabilities. 
To demonstrate that the observed sparsity can actually be leveraged without compromising performance, we perform a more realistic test. 

We evaluate models from the Llama family on small selection of knowledge-intensive benchmark tasks while constraining the attention computation for each query to only use the \topk key-value pairs with the highest attention score (see \Cref{sec:topk-attention} for algorithmic details). In these experiments, we use the same number of keys for each layer of the network and average the model performance over all tasks on the OpenLLM leaderboard (see \Cref{sec:lm_eval_details} for task description). In \Cref{fig:lm_eval_harness_topk_results_teaser} we see that all three models saturate in performance by the 15th key, and the most recent (and most overtrained) variants of the model require only the 10 top keys to achieve the same performance as full-scale attentions. While \Cref{fig:75_mass_hist} indicates that there is some spreading of the attention mass for layer 1 of the network, this tail mass seems to be unnecessary for good performance on benchmarks.

Taken as a whole, these simple but motivating experiments evidence the basic sparsity assumptions on which our method is based. Empirically, we find that the attention scores are a native, reliable indicator as to which key-value pairs are most critical during any given inference step, and that very few keys are actually needed to perform accurate inference. 

In the next section we introduce the technical details of our approach for using a vector database to retrieve the \topk most influential tokens, thereby disconnecting attention's selection operations from the rest of the feed-forward computation. This critical separation enables the GPU to perform smaller matrix operations that fit into its limited memory whilst the full cached keys and values live in CPU memory and are searched using CPU compute. In the main experiments that follow, we showcase how our method alleviates both the computational and memory barriers that normally bottleneck long-context inference and unlocks million token contexts for small GPUs.
