%\section{Appendix}
%\label{appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}
\label{appendix}

%You can have as much text here as you want. The main body must be at most $8$ pages long.
%For the final version, one more page can be added.
%If you want, you can use an appendix like this one.  
%
%The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Distribution of Attention Scores}
\label{app:attention_distribution}
In general, 1\% of the total attention scores are sufficient for providing 95\% of the performance of dense attention on Open LLM Leaderboard, AlpacaEval, and RULER. However, within the subtasks for a given benchmark there is variation in this \kk-required threshold. This variation is highly correlated with the a measurement we call the \textit{attention entropy}. Attention entropy is calculated by taking the Shannon entropy of a single row of an attention matrix after the softmax transformation is applied, and averaging that over multiple tokens of generation on many different samples of text. 

When treating a single, soft-maxed row of an attention matrix as a probability distribution, entropy serves as a good descriptor of how concentrated, or "sparse" it is. The entropy of a maximally concentrated attention distribution is zero, while completely uniform attention scores would have an entropy of the logarithm total scores. Thus low entropy indicates sparse, or concentrated attention. In \Cref{tab:entropy_correlation_app} we show the attention entropy calculated from the first ten tokens of generation from fifty samples of text in each task category. The attention entropy values in the table have a Pearson correlation coefficient of 0.85 with the \kk-required thresholds for 95\% performance in those tasks. 

In addition to looking at the attention distributions across tasks, we investigate if there are any systematic trends in the attention sparsity across layers of a model. \Cref{fig:entropy_tasks_app} shows the attention entropy for RULER subtasks when plotted by layer, and \Cref{fig:75_mass_hist} shows the number of scores required to cover 75\% of the full attention distribution, with a histogram plotting this value over hundreds of samples of Wikipedia text. Note that in both figures, the first layer clearly stands out as having the least concentrated attention distributions.


\begin{table}[h]
\centering
\caption{Percent of attention scores required to reach 95\% of dense attention performance for different categories of tasks, along with the average entropy of the attention vectors for tokens generated from those tasks.}
\label{tab:entropy_correlation_app}
\begin{tabular}{lcc}
\hline
\textbf{Task Category}     & \textbf{\begin{tabular}[c]{@{}c@{}}\kk Required For\\ 95\% Performance\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Attention\\ Entropy\end{tabular}} \\ \hline
Needle In A Haystack       & 0.001\%                   & 1.93 \\
Variable Tracking          & 0.11\%                    & 2.11 \\
Question Answering         & 0.23\%                    & 2.27 \\
Multiple NIAH              & 0.27\%                    & 2.33 \\
Word Counting              & 8.87\%                    & 2.68 \\ \hline
% \multicolumn{3}{c}{\textbf{Correlation ($r$):} \quad 0.847}        \\ \hline
\textbf{\kk-Required - Entropy Correlation ($r$)} & \multicolumn{2}{c}{0.847}        \\ \hline
\end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.76\linewidth]{Figures/entropy_tasks.pdf}
    \caption{Attention entropy by layer, colored by task category.}
    \label{fig:entropy_tasks_app}
\end{figure}

\input{Sections/32_histogram_figure}
% \newpage



\subsection{Additional RULER and AlpacaEval Results}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{Figures/TopK_ruler_Tasks_4096.pdf}
        \caption{Context length 4096}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{Figures/TopK_ruler_Tasks_8192.pdf}
        \caption{Context length 8192}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{Figures/TopK_ruler_Tasks_16384.pdf}
        \caption{Context length 16384}
    \end{subfigure}

    \vspace{0.5cm} % Space between rows

    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{Figures/TopK_ruler_Tasks_32768.pdf}
        \caption{Context length 32768}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{Figures/TopK_ruler_Tasks_65536.pdf}
        \caption{Context length 65536}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{Figures/TopK_ruler_Tasks_131072.pdf}
        \caption{Context length 131072}
    \end{subfigure}

    \caption{Results for RULER over various context lengths. This shows the same behavior as Open LLM Leaderboard and AlpacaEval where very few attention scores (very low \kk) are sufficient to achieve near dense attention performance.}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.35\linewidth]{Figures/TopK_Alpaca_Tasks_Instruct.pdf}
    \includegraphics[width=0.35\linewidth]{Figures/TopK_Alpaca_Tasks_Size.pdf}
    \caption{AlpacaEval 2.0 results for various models. Left compares different generations of Llama instruction tuned models. Right investigates how models of different sizes handle small values of \kk.}
    \label{fig:alpaca_eval_topk_results_app}
\end{figure}
% \newpage


% \begin{figure}
%     \centering
%     \includegraphics[width=0.32\linewidth]{All_Histograms_regarding_Scores/50.0allheads_1250xaxis_cum_layer_0_0.5per_hist.pdf}
%     \includegraphics[width=0.32\linewidth]{All_Histograms_regarding_Scores/50.0allheads_1250xaxis_cum_layer_15_0.5per_hist.pdf}
%     \includegraphics[width=0.32\linewidth]{All_Histograms_regarding_Scores/50.0allheads_1250xaxis_cum_layer_31_0.5per_hist.pdf}
%     \caption{\textbf{Only a few number of scores or tokens are required to make up $50\%$ of the probability mass of a row in the attention matrix.} We analyze the number of attention scores that correspond to $50\%$ of the probability mass for generating the next token. Each point is the number of scores of the last `row' from the attention matrix that make up $50\%$ of the probability mass. There are $1600$ points comprised from the $50$ datapoints and $32$ heads. On the \textbf{left}, we plot the histogram for the first layer in the network, the \textbf{center} corresponds $16$th layer, and \textbf{right} corresponds to the last layer. The sequence length of each sequence of $4000$ tokens. However, we plot the range from $0$ to $1250$ on the x-axis as no ways of the three plots exceeds this range.
%     % 
%     }
%     \label{fig:50_mass_hist}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.32\linewidth]{All_Histograms_regarding_Scores/25.0allheads_1250xaxis_cum_layer_0_0.25per_hist.pdf}
%     \includegraphics[width=0.32\linewidth]{All_Histograms_regarding_Scores/25.0allheads_1250xaxis_cum_layer_15_0.25per_hist.pdf}
%     \includegraphics[width=0.32\linewidth]{All_Histograms_regarding_Scores/25.0allheads_1250xaxis_cum_layer_31_0.25per_hist.pdf}
%     \caption{\textbf{Only a few number of scores or tokens are required to make up $25\%$ of the probability mass of a row in the attention matrix.} We analyze the number of attention scores that correspond to $25\%$ of the probability mass for generating the next token. Each point is the number of scores of the last `row' from the attention matrix that make up $25\%$ of the probability mass. There are $1600$ points comprised from the $25$ datapoints and $32$ heads. On the \textbf{left}, we plot the histogram for the first layer in the network, the \textbf{center} corresponds $16$th layer, and \textbf{right} corresponds to the last layer. The sequence length of each sequence of $4000$ tokens. However, we plot the range from $0$ to $1250$ on the x-axis as no ways of the three plots exceeds this range.
%     % 
%     }
%     \label{fig:25_mass_hist}
% \end{figure}

% \subsection{Layer-wise \kk: 8096 Context Length}
% \begin{table}[H]
% \begin{tabular}{llll|llll|lll}
% \toprule
% 1st layer & other layers & \textbf{score} &  & 1st layer & other layers & \textbf{score} &  & 1st layer & other layers & \textbf{score} \\
% \midrule
% 2         & 2            & \textbf{70.5}  &  & 33        & 1            & \textbf{49.3}  &  & 32        & 1            & \textbf{49.4}  \\
% 8         & 8            & \textbf{83.1}  &  & 132       & 4            & \textbf{74.1}  &  & 32        & 2            & \textbf{70.8}  \\
% 32        & 32           & \textbf{85.3}  &  & 528       & 16           & \textbf{83.7}  &  & 32        & 4            & \textbf{77.8}  \\
% 128       & 128          & \textbf{87.2}  &  & 2112      & 64           & \textbf{86.6}  &  & 32        & 6            & \textbf{82.1}  \\
% 512       & 512          & \textbf{88.5}  &  & full      & 264          & \textbf{87.9}  &  & 32        & 8            & \textbf{82.7}  \\
% 2048      & 2048         & \textbf{89.4}  &  & full      & 1850         & \textbf{89.8}  &  & 32        & 12           & \textbf{83.4}  \\
% Full      & Full         & \textbf{89.3}  &  & full      & full         & \textbf{89.3}  &  & 32        & 16           & \textbf{84.4}  \\
%           &              &                &  &           &              &                &  & 32        & 24           & \textbf{84.6}  \\
%           &              &                &  &           &              &                &  & 32        & 32           & \textbf{85.3} \\
%           \bottomrule
% \end{tabular}
% \end{table}

% \subsection{Where does the Attention go?}

% \begin{figure*}[t]
%     \centering
%     % \includegraphics[height=1.5in]{Figures/avg_scores_layer_0.png}
%         \caption{Top-\kk Attention}
%         \label{fig:attnb}
%     \caption{Where does the attention go?}
% \label{fig:attentionDoc}
% \end{figure*}

% \begin{figure}[t]
% \begin{minipage}[b]{1.\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figures/avg_scores_layer_0.png}

%     % \includegraphics[height=1.5in]{Figures/avg_scores_layer_0.png}
%         % \caption{Top-\kk Attention}
%     \caption{Where does the attention go?}
%     \label{fig:attentionDoc}
%     \end{minipage}
% \end{figure}

% \begin{figure}[t]
% \begin{minipage}[b]{1.\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figures/test.png}

%     % \includegraphics[height=1.5in]{Figures/avg_scores_layer_0.png}
%         % \caption{Top-\kk Attention}
%     \caption{Where does the attention go?}
%     \label{fig:attentionDoc2}
%     \end{minipage}
% \end{figure}


% \begin{figure}[t]
% \begin{minipage}[b]{1.\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figures/TopK Average OpenLLM Leaderboard Tasks.pdf}
%     \caption{In this figure, we sweep over different TopK values from \(2,4,6,8,10,12,,14,16,\text{ and }32\).
%     We first calculate the average accuracy with full attention of each of the models on the OpenLLM Leaderboard, namely openbookqa, arc easy, winogrande, hellaswag, arc challenge, PiQA, BoolQ, and MMLU. 
%     Then, for each model we sweep the values of \(k\) and record the the average on the OpenLLM Leaderboard. Then, for every OpenLLMLeaderboard average of different \(k\), we divide this by the average accuracy of the full attention of corresponding the models. We plot this on the y-axis. We plot different values \(k\) on the x-axis.
%     }
%     \label{fig:lm_eval_harness}
%     \end{minipage}
% \end{figure}




% \subsection{Additional Ruler Results}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{Figures/ruler2.png}
%     \caption{Log-log plots of ruler results over various context lengths.}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{Figures/ruler3.png}
%     \caption{Ruler results showing performance for a selected k value as the context length increases.}
%     \label{fig:enter-label}
% \end{figure}
