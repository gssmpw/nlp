\section{Conclusion}
\label{conclusion}

In this work we demonstrate the capability of a \topk attention mechanism to operate at the million token scale on a single GPU. We achieve sublinear complexity and evaluate at over 95\% of dense attention accuracy on common benchmarks while using only 2\% of the context length on average in the attention block. This exploitation of attention sparsity opens up new directions for efficient and viable solutions to long context inference in language models. Our investigation of attention distributions across layers points to future variations of our method that smoothly adapt the \topk method across tasks and layers of a given model suggesting our \topk approach can be used to achieve optimal compute-performance tradeoffs given a specific deployment context.


