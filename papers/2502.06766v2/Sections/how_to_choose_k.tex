\section{How to Choose \kk}
\label{how_to_choose_k}

% This needs to be cut down. Please mark things that are worth keeping and things needed to be cut or made concise.
We investigate the correlation between the \kk required to  
We now investigate what value of \kk is necessary to achieve a given standard of performance. Table \ref{tab:entropy_correlation} shows the value of \kk as a percent of total context length necessary to reach 95\% performance across various categories of tasks. Entropy is measured as 

We find that in general, 1-2\% of the total attention scores are sufficient for providing 95\% of the performance of dense attention.\ryan{Claim not backed up clearly?} However, there is still significant variation between the scores required for different categories of tasks. And as shown in \ref{methods}, the majority of inference time is spent on vector database retrieval, so predicting and reducing $k$ to the minimum required can have significant benefits on throughput.

It turns out that the minimum required $k$ for a given level of performance is predicable and can be easily estimated by calculating the entropy of sampled attention maps produced.  Table \ref{tab:entropy_correlation} shows the value of $k$, as a percent of total context length, required to reach 95\% performance on several categories of tasks. Most of the tasks require less than 1\% of the context's attention scores, but tasks like word counting require significantly more to achieve 95\% performance. 

Since minimum requirements range from 0.001\% to 9\%, it would be helpful if this could be quickly estimated before operating on full, million-token contexts. The underlying intuition for \topk is that the sparsity of attention matrices lends itself to this type of top-subset exploitation, so it seems likely that statistical measures of this sparsity might be good predictors. When treating a single, soft-maxed row of an attention matrix as a probability distribution, entropy serves as a good descriptor of how concentrated, or "sparse" it is. The entropy of a maximally concentrated attention distribution is zero, while completely uniform attention scores would have an entropy of the logarithm total scores. Table \ref{tab:entropy_correlation} includes the average attention entropy of attention vectors captured from tokens generated from representative samples of task-based text. Attention entropy is highly correlated with required $k$, and we have found that it holds as an effective predictor on new tasks. 

Given that entropy is an effective predictor of $k$ requirements on new tasks, we explored whether this analysis could help determine an optimal arrangement of $k$ across layers. In fact, the variation in entropy is greater between layers of a model than it is between different tasks, so this could potentially be significant (see figure \ref{fig:entropy_tasks}). Instead of using the same $k$ on every layer and attention head, we distributed values of $k$ according to the entropy of shown in figure \ref{fig:entropy_tasks}. In other words, for a fixed budget of $k$, we allocate more of the budget to the first and middle layers and less to the later layers of the model.

 Surprisingly, the entropy-following strategy performed worse than a uniform $k$ across all layers. However, the opposite schedule where small $k$ is used for early layers and large $k$ for later layers does outperform uniform. Figure \ref{fig:topk_adaptive} shows the difference between a uniform schedule and this "adaptive" schedule with more attention allocated to later layers. 

 We initially suspected that this behavior where later layers are more sensitive than early layers to the number of attention scores might be do to specific attention heads in later layers, but there are no outlier heads to support this theory. Further investigation into why later layers exhibit this behavior is a potential area for future work.
 
% For the tasks that achieve high relative performance with low $k$, it isn't immediately clear whether the task in general is robust to attention with \topk subsets of the context, or it is simply so easy that very little model capabilities are required. To help distinguish this, Figure \ref{TODO} shows absolute performance on three different categories of tasks in the RULER long-context benchmark, and how that performance responds to increasing $k$. As shown by the top line, the performance on NIAH style tasks changes very little, even with $k$ as low as $1$, or retrieving just a single token's attention score out of the entire context. While NIAH has 100\% success, indicating the task is easy in general, the question answering task shows a category that is both difficult and robust to low $k$. After reaching a minimum threshold of attention scores, question answering tasks do not seem to benefit much from further increases in $k$.

% The next line in the figure shows RULER's Frequent Words Extraction, which is a word-counting task ("what are the ten most common words in this document"). This shows a somewhat steady increase in performance in $k$ increases, indicating that it is sensitive to reductions from the full, dense attention matrix. Whether this extends to non-synthetic tasks such as summarization is a subject for future work.