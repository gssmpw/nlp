[
  {
    "index": 0,
    "papers": [
      {
        "key": "bahdanau2015neural",
        "author": "Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua",
        "title": "Neural machine translation by jointly learning to align and translate"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "devlin2019bert",
        "author": "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "openai2023chatgpt",
        "author": "{OpenAI}",
        "title": "Introducing ChatGPT"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "openai2023gpt4",
        "author": "OpenAI",
        "title": "GPT-4 Technical Report"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ghojogh2020attention",
        "author": "Ghojogh, Benyamin and Ghodsi, Ali",
        "title": "Attention mechanism, transformers, BERT, and GPT: tutorial and survey"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "dosovitskiy2020image",
        "author": "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others",
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2021swin",
        "author": "Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
        "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "he2022masked",
        "author": "He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\\'a}r, Piotr and Girshick, Ross",
        "title": "Masked autoencoders are scalable vision learners"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chen2021pre",
        "author": "Chen, Hanting and Wang, Yunhe and Guo, Tianyu and Xu, Chang and Deng, Yiping and Liu, Zhenhua and Ma, Siwei and Xu, Chunjing and Xu, Chao and Gao, Wen",
        "title": "Pre-trained image processing transformer"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "bao2021beit",
        "author": "Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu",
        "title": "BEiT: BERT Pre-Training of Image Transformers"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "yu2022coca",
        "author": "Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu",
        "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "mokady2021clipcap",
        "author": "Mokady, Ron and Hertz, Amir and Bermano, Amit H",
        "title": "Clipcap: Clip prefix for image captioning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "ramesh2021zero",
        "author": "Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya",
        "title": "Zero-shot text-to-image generation"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "wang2023image",
        "author": "Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others",
        "title": "Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "li2022blip",
        "author": "Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven",
        "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "zhao2023survey",
        "author": "Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others",
        "title": "A survey of large language models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "wang2023review",
        "author": "Wang, Jiaqi and Liu, Zhengliang and Zhao, Lin and Wu, Zihao and Ma, Chong and Yu, Sigang and Dai, Haixing and Yang, Qiushi and Liu, Yiheng and Zhang, Songyao and others",
        "title": "Review of large vision models and visual prompt engineering"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "petroni2019language",
        "author": "Petroni, Fabio and Rockt{\\\"a}schel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander",
        "title": "Language Models as Knowledge Bases?"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "lester2021power",
        "author": "Lester, Brian and Al-Rfou, Rami and Constant, Noah",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "gao2021making",
        "author": "Gao, Tianyu and Fisch, Adam and Chen, Danqi",
        "title": "Making Pre-trained Language Models Better Few-shot Learners"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "wang2022learning",
        "author": "Wang, Zifeng and Zhang, Zizhao and Lee, Chen-Yu and Zhang, Han and Sun, Ruoxi and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas",
        "title": "Learning to prompt for continual learning"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "shin2020autoprompt",
        "author": "Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer",
        "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "chen2022adaptformer",
        "author": "Chen, Shoufa and Ge, Chongjian and Tong, Zhan and Wang, Jiangliu and Song, Yibing and Wang, Jue and Luo, Ping",
        "title": "Adaptformer: Adapting vision transformers for scalable visual recognition"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "zhou2022learning",
        "author": "Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei",
        "title": "Learning to prompt for vision-language models"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "zhou2022conditional",
        "author": "{Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei}",
        "title": "Conditional prompt learning for vision-language models"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "huang2023inner",
        "author": "Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others",
        "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "lu2022learn",
        "author": "Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin",
        "title": "Learn to explain: Multimodal reasoning via thought chains for science question answering"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "mu2024embodiedgpt",
        "author": "Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping",
        "title": "Embodiedgpt: Vision-language pre-training via embodied chain of thought"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "feng2024towards",
        "author": "Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei",
        "title": "Towards revealing the mystery behind chain of thought: a theoretical perspective"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "grant2018recasting",
        "author": "Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas",
        "title": "Recasting Gradient-Based Meta-Learning as Hierarchical Bayes"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "vinyals2016matching",
        "author": "Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Wierstra, Daan and others",
        "title": "Matching networks for one shot learning"
      },
      {
        "key": "snell2017prototypical",
        "author": "Snell, Jake and Swersky, Kevin and Zemel, Richard",
        "title": "Prototypical networks for few-shot learning"
      },
      {
        "key": "sung2018learning",
        "author": "Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip HS and Hospedales, Timothy M",
        "title": "Learning to compare: Relation network for few-shot learning"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "santoro2016meta",
        "author": "Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy",
        "title": "Meta-learning with memory-augmented neural networks"
      },
      {
        "key": "munkhdalai2017meta",
        "author": "Munkhdalai, Tsendsuren and Yu, Hong",
        "title": "Meta networks"
      },
      {
        "key": "munkhdalai2018rapid",
        "author": "Munkhdalai, Tsendsuren and Yuan, Xingdi and Mehri, Soroush and Trischler, Adam",
        "title": "Rapid adaptation with conditionally shifted neurons"
      },
      {
        "key": "mishra2018simple",
        "author": "Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter",
        "title": "A Simple Neural Attentive Meta-Learner"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "ravi2016optimization",
        "author": "Ravi, Sachin and Larochelle, Hugo",
        "title": "Optimization as a model for few-shot learning"
      },
      {
        "key": "finn2017model",
        "author": "Finn, Chelsea and Abbeel, Pieter and Levine, Sergey",
        "title": "Model-agnostic meta-learning for fast adaptation of deep networks"
      },
      {
        "key": "grant2018recasting",
        "author": "Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas",
        "title": "Recasting Gradient-Based Meta-Learning as Hierarchical Bayes"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "elsken2020meta",
        "author": "Elsken, Thomas and Staffler, Benedikt and Metzen, Jan Hendrik and Hutter, Frank",
        "title": "Meta-learning of neural architectures for few-shot learning"
      },
      {
        "key": "huang20213d",
        "author": "Huang, Hao and Li, Xiang and Wang, Lingjing and Fang, Yi",
        "title": "3D-metaconnet: meta-learning for 3D shape classification and segmentation"
      },
      {
        "key": "lang2023base",
        "author": "Lang, Chunbo and Cheng, Gong and Tu, Binfei and Li, Chao and Han, Junwei",
        "title": "Base and meta: A new perspective on few-shot segmentation"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "tsimpoukelli2021multimodal",
        "author": "Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix",
        "title": "Multimodal few-shot learning with frozen language models"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "jin2022good",
        "author": "Jin, Woojeong and Cheng, Yu and Shen, Yelong and Chen, Weizhu and Ren, Xiang",
        "title": "A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models"
      },
      {
        "key": "song2022clip",
        "author": "Song, Haoyu and Dong, Li and Zhang, Weinan and Liu, Ting and Wei, Furu",
        "title": "CLIP Models are Few-Shot Learners: Empirical Studies on VQA and Visual Entailment"
      }
    ]
  },
  {
    "index": 43,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 44,
    "papers": [
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"
      }
    ]
  },
  {
    "index": 45,
    "papers": [
      {
        "key": "najdenkoska2022meta",
        "author": "Najdenkoska, Ivona and Zhen, Xiantong and Worring, Marcel",
        "title": "Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning"
      }
    ]
  },
  {
    "index": 46,
    "papers": [
      {
        "key": "vinyals2015show",
        "author": "Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru",
        "title": "Show and tell: A neural image caption generator"
      },
      {
        "key": "xu2015show",
        "author": "Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua",
        "title": "Show, attend and tell: Neural image caption generation with visual attention"
      }
    ]
  },
  {
    "index": 47,
    "papers": [
      {
        "key": "zhao2020cross",
        "author": "Zhao, Wentian and Wu, Xinxiao and Luo, Jiebo",
        "title": "Cross-domain image captioning via cross-modal retrieval and model adaptation"
      },
      {
        "key": "al2022image",
        "author": "Al-Qatf, Majjed and Wang, Xingfu and Hawbani, Ammar and Abdusallam, Amr and Alsamhi, Saeed Hammod",
        "title": "Image captioning with novel topics guidance and retrieval-based topics re-weighting"
      }
    ]
  },
  {
    "index": 48,
    "papers": [
      {
        "key": "vinyals2015show",
        "author": "Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru",
        "title": "Show and tell: A neural image caption generator"
      },
      {
        "key": "xu2015show",
        "author": "Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua",
        "title": "Show, attend and tell: Neural image caption generation with visual attention"
      }
    ]
  },
  {
    "index": 49,
    "papers": [
      {
        "key": "hochreiter1997long",
        "author": "Hochreiter, Sepp and Schmidhuber, J{\\\"u}rgen",
        "title": "Long short-term memory"
      }
    ]
  },
  {
    "index": 50,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 51,
    "papers": [
      {
        "key": "you2016image",
        "author": "You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo",
        "title": "Image captioning with semantic attention"
      },
      {
        "key": "yang2022human",
        "author": "Yang, Zuopeng and Wang, Pengbo and Chu, Tianshu and Yang, Jie",
        "title": "Human-centric image captioning"
      },
      {
        "key": "wang2019hierarchical",
        "author": "Wang, Weixuan and Chen, Zhihong and Hu, Haifeng",
        "title": "Hierarchical attention network for image captioning"
      }
    ]
  },
  {
    "index": 52,
    "papers": [
      {
        "key": "girshick2015fast",
        "author": "Girshick, Ross",
        "title": "Fast r-cnn"
      }
    ]
  },
  {
    "index": 53,
    "papers": [
      {
        "key": "anderson2016spice",
        "author": "Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen",
        "title": "Spice: Semantic propositional image caption evaluation"
      },
      {
        "key": "yao2019hierarchy",
        "author": "Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao",
        "title": "Hierarchy parsing for image captioning"
      },
      {
        "key": "pan2020x",
        "author": "Pan, Yingwei and Yao, Ting and Li, Yehao and Mei, Tao",
        "title": "X-linear attention networks for image captioning"
      }
    ]
  },
  {
    "index": 54,
    "papers": [
      {
        "key": "li2020oscar",
        "author": "Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others",
        "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks"
      },
      {
        "key": "wang2022end",
        "author": "Wang, Yiyu and Xu, Jungang and Sun, Yingfei",
        "title": "End-to-end transformer based model for image captioning"
      },
      {
        "key": "zeng2022s2",
        "author": "Zeng, Pengpeng and Zhang, Haonan and Song, Jingkuan and Gao, Lianli",
        "title": "S2 Transformer for Image Captioning."
      }
    ]
  },
  {
    "index": 55,
    "papers": [
      {
        "key": "sarto2022retrieval",
        "author": "Sarto, Sara and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita",
        "title": "Retrieval-augmented transformer for image captioning"
      },
      {
        "key": "ramos2023smallcap",
        "author": "Ramos, Rita and Martins, Bruno and Elliott, Desmond and Kementchedjhieva, Yova",
        "title": "Smallcap: lightweight image captioning prompted with retrieval augmentation"
      }
    ]
  },
  {
    "index": 56,
    "papers": [
      {
        "key": "zhai2024investigating",
        "author": "Zhai, Yuexiang and Tong, Shengbang and Li, Xiao and Cai, Mu and Qu, Qing and Lee, Yong Jae and Ma, Yi",
        "title": "Investigating the Catastrophic Forgetting in Multimodal Large Language Model Fine-Tuning"
      }
    ]
  },
  {
    "index": 57,
    "papers": [
      {
        "key": "mokady2021clipcap",
        "author": "Mokady, Ron and Hertz, Amir and Bermano, Amit H",
        "title": "Clipcap: Clip prefix for image captioning"
      }
    ]
  },
  {
    "index": 58,
    "papers": [
      {
        "key": "luo2022tuning",
        "author": "Luo, Ziyang and Hu, Zhipeng and Xi, Yadong and Zhang, Rongsheng and Ma, Jing",
        "title": "I-tuning: Tuning frozen language models with image for lightweight image captioning"
      }
    ]
  },
  {
    "index": 59,
    "papers": [
      {
        "key": "song2022clip",
        "author": "Song, Haoyu and Dong, Li and Zhang, Weinan and Liu, Ting and Wei, Furu",
        "title": "CLIP Models are Few-Shot Learners: Empirical Studies on VQA and Visual Entailment"
      }
    ]
  },
  {
    "index": 60,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      }
    ]
  },
  {
    "index": 61,
    "papers": [
      {
        "key": "ramos2023smallcap",
        "author": "Ramos, Rita and Martins, Bruno and Elliott, Desmond and Kementchedjhieva, Yova",
        "title": "Smallcap: lightweight image captioning prompted with retrieval augmentation"
      }
    ]
  },
  {
    "index": 62,
    "papers": [
      {
        "key": "yang2024exploring",
        "author": "Yang, Xu and Wu, Yongliang and Yang, Mingzhuo and Chen, Haokun and Geng, Xin",
        "title": "Exploring diverse in-context configurations for image captioning"
      }
    ]
  }
]