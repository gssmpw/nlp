\section{Related Work}
\label{sec:related}
\noindent \textbf{Large-scale language and vision models.} 
The advent of attention mechanisms~\cite{bahdanau2015neural} and Transformers~\cite{vaswani2017attention} marked the emergence of large-scale language models, effectively addressing the challenge of long-range dependencies in sequences, including but not limited to, BERT~\cite{devlin2019bert}, GPT-2~\cite{radford2019language}, GPT-3~\cite{brown2020language}, ChatGPT~\cite{openai2023chatgpt} and GPT-4~\cite{openai2023gpt4}. BERT is designed to understand bidirectional contexts using the Transformer's encoder component to read the entire input text at once and learn the relationships between different words in the text; GPT-based models are primarily trained to predict the next word in a sentence given all the previous words by using the Transformer's decoder component to process text unidirectionally~\cite{ghojogh2020attention}. Meanwhile, large vision models also emerge in computer vision and multi-modal fields. For example, Vision Transformer (ViT)~\cite{dosovitskiy2020image} has broadened the use of the Transformer architecture to establish an analogy between word tokens and image patches. Subsequent efforts have sought to improve and expand the ViT model, as evidenced by Swin Transformer~\cite{liu2021swin}, MAE~\cite{he2022masked}, IPT~\cite{chen2021pre}, and BeiT~\cite{bao2021beit}, \etc. These studies have effectively adapted the ViT model to a variety of vision-related tasks, yielding exceptional results. Simultaneously, studies in the multi-modal domain have embraced Transformer for cross-modal data interaction, including CLIP~\cite{radford2021learning} for text-image alignment, Coca~\cite{yu2022coca} and ClipCap~\cite{mokady2021clipcap} for image captioning, DALL-E~\cite{ramesh2021zero} for text-to-image generation, and PALI~\cite{wang2023image} for visual question answering, \etc. In addition, BLIP~\cite{li2022blip} utilizes the noisy web data by bootstrapping the captions to train a large language-vision model, and BLIP-2~\cite{li2023blip} bridges the gap between multi-modalities with a lightweight Querying Transformer. We refer the interested reader to~\cite{zhao2023survey} and~\cite{wang2023review} for more literature.

\noindent \textbf{Prompt and chain-of-thought.}
Prompting~\cite{brown2020language} is to guide large pretrained LLMs towards generating the desired response by treating them as knowledge bases from which information useful to downstream tasks is elicited~\cite{petroni2019language}. This methodology involves the use of a few examples as prompts, appended to a fixed task induction, to stimulate the LLM to generate accurate output. Such prompts are often designed manually by the user. Prompt tuning was initially introduced in~\cite{lester2021power}, which instead of manually designing a prompt, learns ``soft prompts'' to condition frozen LLMs to perform specific downstream tasks, while preserving the embedding space structure of the pretrained LLMs~\cite{gao2021making}. Recent studies, such as L2P~\cite{wang2022learning} and AutoPrompt~\cite{shin2020autoprompt}, selected the most effective prompts from a collection of learned potential prompts. The concept of prompt tuning has also gained significant interest in computer vision. Adaptformer~\cite{chen2022adaptformer} employed prompt tuning to efficiently adapt pretrained ViTs, while CoOp~\cite{zhou2022learning} and CoOpOp~\cite{zhou2022conditional} applied prompt tuning to adapt a pretrained large visual-language model by transforming the context word into a set of learnable vectors. Recently, Chain-of-Thought (CoT) prompting has been widely used to extract the multi-step intermediate reasoning capabilities of LLMs~\cite{wei2022chain}. Specifically, CoT stimulates an LLM to generate intermediate reasoning chains to solve complex problems. For instance,~\cite{huang2023inner} and~\cite{lu2022learn} proposed using the reasoning process by language models for robot interaction and question answering. EmbodiedGPT~\cite{mu2024embodiedgpt} proposed an end-to-end multi-modal foundation model for embodied AI by using CoT for effective embodied planning. Despite massive empirical success, the most recent work~\cite{feng2024towards} revealed the underlying mechanisms behind CoT using circuit complexity theory.

\noindent \textbf{Meta-learning and few-shot learning.} 
The objective of meta-learning is to learn to acquire inductive biases and perform rapid adaptation to novel unseen tasks~\cite{grant2018recasting}. Meta-learning algorithms are typically classified into three categories: (i) Metric-based approaches which focus on learning a common embedding space and generating prototypes as meta-knowledge~\cite{vinyals2016matching,snell2017prototypical,sung2018learning}; (ii) Memory-based approaches which utilize an external memory as meta-knowledge to quickly adapt to new tasks~\cite{santoro2016meta,munkhdalai2017meta,munkhdalai2018rapid,mishra2018simple}. (iii) Optimization-based approaches that aim to learn a good model initialization across tasks as meta-knowledge to efficiently adapt to new samples~\cite{ravi2016optimization,finn2017model,grant2018recasting}. Meta-learning has been proved to be effective in solving few-shot learning tasks by allowing models to generalize from only a small number of training samples~\cite{elsken2020meta,huang20213d,lang2023base}. This approach trains models on a variety of learning tasks so that they can quickly adapt to new tasks using limited data, mimicking human learning efficiency. The field of multimodal few-shot learning, spanning vision and language modalities, has recently gained traction with the advent of the first multimodal few-shot learner~\cite{tsimpoukelli2021multimodal}. This development was followed by other innovative prompting and in-context learning strategies~\cite{jin2022good,song2022clip}. Flamingo~\cite{alayrac2022flamingo} utilized a super large visual-language model comprising 70 billion parameters for in-context learning. MiniGPT-4~\cite{zhu2023minigpt} aligns a frozen visual encoder with a frozen LLM through a single projection layer. However, our objective diverges from the above multi-modality visual-language models, as we aim to rapidly adapt to new unseen tasks by acquiring meta-knowledge across different tasks as in~\cite{najdenkoska2022meta}, by incorporating a light-weighted model with much fewer trainable parameters. 

\noindent \textbf{Image captioning.} 
The image captioning task involves generating descriptive text for an image to interpret and articulate the content and context of visual data contained in the imageÂ±\cite{vinyals2015show,xu2015show}. This task can usually be tackled in two strategies: retrieval-based and generation-based. Retrieval-based approaches~\cite{zhao2020cross,al2022image} retrieve complete sentences that best describe the given image from a corpus, while generation-based approaches~\cite{vinyals2015show,xu2015show} construct sentences by generating words in sequence, which typically contain a visual encoder to extract image features and a sequence-to-sequence model such as LSTM~\cite{hochreiter1997long} and/or Transformer~\cite{vaswani2017attention} for text generation. To extract accurate image features, previous work~\cite{you2016image,yang2022human,wang2019hierarchical} proposed using Region-of-Interest (RoI) features generated from off-the-shelf objector detectors~\cite{girshick2015fast}. For text generation, previous approaches~\cite{anderson2016spice,yao2019hierarchy,pan2020x} typically utilize LSTM, while subsequent work~\cite{li2020oscar,wang2022end,zeng2022s2} leverages attention-based models such as Transformers to predict captions. Recent studies have integrated the retrieved image-caption pairs into generation-based models~\cite{sarto2022retrieval,ramos2023smallcap}, which aligns with the concept of in-context captioning that emerges in the era of LLMs. To utilize the generalization ability of pretrained LLMs, the weights of LLMs are partially or completely frozen to prevent catastrophic forgetting~\cite{zhai2024investigating}. ClipCap~\cite{mokady2021clipcap} and ITuning~\cite{luo2022tuning} are two lightweight tunable image captioning models that use CLIP~\cite{song2022clip} as a pre-trained vision encoder and GPT-2~\cite{radford2019language} as a language decoder. Though the components of CLIP and GPT-2 are frozen, ClipCap employs prefix-tuning to map a fixed-length CLIP embedding of an input image into the GPT-2 language space, and I-Tuning extracts visual memory embeddings from CLIP and uses those to adjust the output hidden states of GPT-2. SMALLCAP~\cite{ramos2023smallcap} uses retrieval augmentation to maintain performance while substantially reducing the number of trainable parameters. Instead of randomly selecting in-context image-caption pairs, Yang \etal~\cite{yang2024exploring} devised a strategy for image captioning by selecting images and assigning captions to establish in-context image-caption pairs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.99\linewidth]{images/arch}
    \caption{Model architecture overview. Given an image, a vision encoder first extracts an image feature which is then transformed into multiple visual prompts by meta-adaptors in different subspaces through meta-learning; these visual prompts
are sequentially fed into an LLM along with word tokens to generate a caption.}
    \label{fig:arch}
\end{figure*}
%