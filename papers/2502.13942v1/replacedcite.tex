\section{Related Work}
\label{sec:related}
\noindent \textbf{Large-scale language and vision models.} 
The advent of attention mechanisms____ and Transformers____ marked the emergence of large-scale language models, effectively addressing the challenge of long-range dependencies in sequences, including but not limited to, BERT____, GPT-2____, GPT-3____, ChatGPT____ and GPT-4____. BERT is designed to understand bidirectional contexts using the Transformer's encoder component to read the entire input text at once and learn the relationships between different words in the text; GPT-based models are primarily trained to predict the next word in a sentence given all the previous words by using the Transformer's decoder component to process text unidirectionally____. Meanwhile, large vision models also emerge in computer vision and multi-modal fields. For example, Vision Transformer (ViT)____ has broadened the use of the Transformer architecture to establish an analogy between word tokens and image patches. Subsequent efforts have sought to improve and expand the ViT model, as evidenced by Swin Transformer____, MAE____, IPT____, and BeiT____, \etc. These studies have effectively adapted the ViT model to a variety of vision-related tasks, yielding exceptional results. Simultaneously, studies in the multi-modal domain have embraced Transformer for cross-modal data interaction, including CLIP____ for text-image alignment, Coca____ and ClipCap____ for image captioning, DALL-E____ for text-to-image generation, and PALI____ for visual question answering, \etc. In addition, BLIP____ utilizes the noisy web data by bootstrapping the captions to train a large language-vision model, and BLIP-2____ bridges the gap between multi-modalities with a lightweight Querying Transformer. We refer the interested reader to____ and____ for more literature.

\noindent \textbf{Prompt and chain-of-thought.}
Prompting____ is to guide large pretrained LLMs towards generating the desired response by treating them as knowledge bases from which information useful to downstream tasks is elicited____. This methodology involves the use of a few examples as prompts, appended to a fixed task induction, to stimulate the LLM to generate accurate output. Such prompts are often designed manually by the user. Prompt tuning was initially introduced in____, which instead of manually designing a prompt, learns ``soft prompts'' to condition frozen LLMs to perform specific downstream tasks, while preserving the embedding space structure of the pretrained LLMs____. Recent studies, such as L2P____ and AutoPrompt____, selected the most effective prompts from a collection of learned potential prompts. The concept of prompt tuning has also gained significant interest in computer vision. Adaptformer____ employed prompt tuning to efficiently adapt pretrained ViTs, while CoOp____ and CoOpOp____ applied prompt tuning to adapt a pretrained large visual-language model by transforming the context word into a set of learnable vectors. Recently, Chain-of-Thought (CoT) prompting has been widely used to extract the multi-step intermediate reasoning capabilities of LLMs____. Specifically, CoT stimulates an LLM to generate intermediate reasoning chains to solve complex problems. For instance,____ and____ proposed using the reasoning process by language models for robot interaction and question answering. EmbodiedGPT____ proposed an end-to-end multi-modal foundation model for embodied AI by using CoT for effective embodied planning. Despite massive empirical success, the most recent work____ revealed the underlying mechanisms behind CoT using circuit complexity theory.

\noindent \textbf{Meta-learning and few-shot learning.} 
The objective of meta-learning is to learn to acquire inductive biases and perform rapid adaptation to novel unseen tasks____. Meta-learning algorithms are typically classified into three categories: (i) Metric-based approaches which focus on learning a common embedding space and generating prototypes as meta-knowledge____; (ii) Memory-based approaches which utilize an external memory as meta-knowledge to quickly adapt to new tasks____. (iii) Optimization-based approaches that aim to learn a good model initialization across tasks as meta-knowledge to efficiently adapt to new samples____. Meta-learning has been proved to be effective in solving few-shot learning tasks by allowing models to generalize from only a small number of training samples____. This approach trains models on a variety of learning tasks so that they can quickly adapt to new tasks using limited data, mimicking human learning efficiency. The field of multimodal few-shot learning, spanning vision and language modalities, has recently gained traction with the advent of the first multimodal few-shot learner____. This development was followed by other innovative prompting and in-context learning strategies____. Flamingo____ utilized a super large visual-language model comprising 70 billion parameters for in-context learning. MiniGPT-4____ aligns a frozen visual encoder with a frozen LLM through a single projection layer. However, our objective diverges from the above multi-modality visual-language models, as we aim to rapidly adapt to new unseen tasks by acquiring meta-knowledge across different tasks as in____, by incorporating a light-weighted model with much fewer trainable parameters. 

\noindent \textbf{Image captioning.} 
The image captioning task involves generating descriptive text for an image to interpret and articulate the content and context of visual data contained in the imageÂ±____. This task can usually be tackled in two strategies: retrieval-based and generation-based. Retrieval-based approaches____ retrieve complete sentences that best describe the given image from a corpus, while generation-based approaches____ construct sentences by generating words in sequence, which typically contain a visual encoder to extract image features and a sequence-to-sequence model such as LSTM____ and/or Transformer____ for text generation. To extract accurate image features, previous work____ proposed using Region-of-Interest (RoI) features generated from off-the-shelf objector detectors____. For text generation, previous approaches____ typically utilize LSTM, while subsequent work____ leverages attention-based models such as Transformers to predict captions. Recent studies have integrated the retrieved image-caption pairs into generation-based models____, which aligns with the concept of in-context captioning that emerges in the era of LLMs. To utilize the generalization ability of pretrained LLMs, the weights of LLMs are partially or completely frozen to prevent catastrophic forgetting____. ClipCap____ and ITuning____ are two lightweight tunable image captioning models that use CLIP____ as a pre-trained vision encoder and GPT-2____ as a language decoder. Though the components of CLIP and GPT-2 are frozen, ClipCap employs prefix-tuning to map a fixed-length CLIP embedding of an input image into the GPT-2 language space, and I-Tuning extracts visual memory embeddings from CLIP and uses those to adjust the output hidden states of GPT-2. SMALLCAP____ uses retrieval augmentation to maintain performance while substantially reducing the number of trainable parameters. Instead of randomly selecting in-context image-caption pairs, Yang \etal____ devised a strategy for image captioning by selecting images and assigning captions to establish in-context image-caption pairs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.99\linewidth]{images/arch}
    \caption{Model architecture overview. Given an image, a vision encoder first extracts an image feature which is then transformed into multiple visual prompts by meta-adaptors in different subspaces through meta-learning; these visual prompts
are sequentially fed into an LLM along with word tokens to generate a caption.}
    \label{fig:arch}
\end{figure*}
%