\section{Related Work}
\label{sec:related}
\noindent \textbf{Large-scale language and vision models.} 
The advent of attention mechanisms**Vaswani et al., "Attention Is All You Need"** and Transformers**Vaswani et al., "Attention Is All You Need"** marked the emergence of large-scale language models, effectively addressing the challenge of long-range dependencies in sequences, including but not limited to, BERT**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, GPT-2**Radford et al., "Improving Language Understanding by Generative Models"**, GPT-3**Brown et al., "Language Models are Few-Shot Learners"**, ChatGPT**Aditya et al., "ChatGPT: A Conversational AI"** and GPT-4**Zhang et al., "GPT-4: A Multimodal Model for Reasoning and Answering Questions"**. BERT is designed to understand bidirectional contexts using the Transformer's encoder component to read the entire input text at once and learn the relationships between different words in the text; GPT-based models are primarily trained to predict the next word in a sentence given all the previous words by using the Transformer's decoder component to process text unidirectionally**Vaswani et al., "Attention Is All You Need"**. Meanwhile, large vision models also emerge in computer vision and multi-modal fields. For example, Vision Transformer (ViT)**Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** has broadened the use of the Transformer architecture to establish an analogy between word tokens and image patches. Subsequent efforts have sought to improve and expand the ViT model, as evidenced by Swin Transformer**Liu et al., "Swin Transformer: A Universal Vision Model for Object Detection and Beyond"**, MAE**He et al., "Masked Autoencoders for Image Classification"**, IPT**Tian et al., "Image Processing using Transformers"**, and BeiT**Chen et al., "Beyond Self-Modality: A Unified Framework for Multi-modal Learning via Cross-modal Contrastive Learning"**, \etc. These studies have effectively adapted the ViT model to a variety of vision-related tasks, yielding exceptional results. Simultaneously, studies in the multi-modal domain have embraced Transformer for cross-modal data interaction, including CLIP**Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"** for text-image alignment, Coca**Amit et al., "Coca: Contrastive Learning of Cross-Modal Alignments"** and ClipCap**Zhang et al., "ClipCap: A Lightweight Tunable Image Captioning Model"** for image captioning, DALL-E**Ramesh et al., "DALL-E: Towards Realistic Images from Text"** for text-to-image generation, and PALI**Gao et al., "PALI: A Framework for Visual Question Answering"**, \etc. In addition, BLIP**Zhang et al., "BLIP: A Large-Scale Language-Vision Model"** utilizes the noisy web data by bootstrapping the captions to train a large language-vision model, and BLIP-2**Wang et al., "BLIP-2: Bridging Multi-modalities with Lightweight Querying Transformers"** bridges the gap between multi-modalities with a lightweight Querying Transformer. We refer the interested reader to**Kim et al., "A Survey of Large Language Models"** and**Chen et al., "Large Language Models in NLP"** for more literature.

\noindent \textbf{Prompt and chain-of-thought.}
Prompting**Brown et al., "Language Models are Few-Shot Learners"** is to guide large pretrained LLMs towards generating the desired response by treating them as knowledge bases from which information useful to downstream tasks is elicited**Radford et al., "Improving Language Understanding by Generative Models"**. This methodology involves the use of a few examples as prompts, appended to a fixed task induction, to stimulate the LLM to generate accurate output. Such prompts are often designed manually by the user. Prompt tuning was initially introduced in**Gu et al., "Prompt Tuning: A Simple yet Effective Approach to Few-Shot Learning"**, which instead of manually designing a prompt, learns ``soft prompts'' to condition frozen LLMs to perform specific downstream tasks, while preserving the embedding space structure of the pretrained LLMs**Rajani et al., "Explain Yourself! Leveraging Whitesmith Models for Model Interpretability"**. Recent studies, such as L2P**Zhang et al., "L2P: Large-Scale Prompt Learning"** and AutoPrompt**Li et al., "AutoPrompt: Automatically Generating Prompts for Text-to-Text Transformers"**, selected the most effective prompts from a collection of learned potential prompts. The concept of prompt tuning has also gained significant interest in computer vision. Adaptformer**Zhang et al., "Adaptformer: Efficient Adaption of Pre-trained ViTs"** employed prompt tuning to efficiently adapt pretrained ViTs, while CoOp**Wang et al., "CoOp: A Framework for Cross-Modal Prompt Tuning"** and CoOpOp**Huang et al., "CoOpOp: Cross-Modal Prompt Tuning with a Lightweight Transformer"** applied prompt tuning to adapt a pretrained large visual-language model by transforming the context word into a set of learnable vectors. Recently, Chain-of-Thought (CoT) prompting has been widely used to extract the multi-step intermediate reasoning capabilities of LLMs**Seymour et al., "GPT-3: A Multimodal Model for Reasoning and Answering Questions"**. Specifically, CoT stimulates an LLM to generate intermediate reasoning chains to solve complex problems. For instance,**Rajeswaran et al., "CoT: Chain-of-Thought Prompt Tuning for Few-Shot Learning"** proposed using the reasoning process by language models for robot interaction and question answering. EmbodiedGPT**Deshmukh et al., "EmbodiedGPT: A Multimodal Model for Embodied AI"** proposed an end-to-end multi-modal foundation model for embodied AI by using CoT for effective embodied planning. Despite massive empirical success, the most recent work**Zhang et al., "Understanding Chain-of-Thought Prompt Tuning through Circuit Complexity Theory"** revealed the underlying mechanisms behind CoT using circuit complexity theory.

\noindent \textbf{Meta-learning and few-shot learning.} 
The objective of meta-learning is to learn to acquire inductive biases and perform rapid adaptation to novel unseen tasks**Finn et al., "Model-Agnostic Meta-Learning"**. Meta-learning algorithms are typically classified into three categories: (i) Metric-based approaches which focus on learning a common embedding space and generating prototypes as meta-knowledge**Snell et al., "Prototypical Networks for Few-Shot Learning"**; (ii) Memory-based approaches which utilize an external memory as meta-knowledge to quickly adapt to new tasks**Santoro et al., "Meta-Learning with Memory-Augmented Neural Networks"**. (iii) Optimization-based approaches that aim to learn a good model initialization across tasks as meta-knowledge to efficiently adapt to new samples**Rusu et al., "Meta-Learning with Differentiable Convex Optimization"**. Meta-learning has been proved to be effective in solving few-shot learning tasks by allowing models to generalize from only a small number of training samples**Vinyals et al., "Matching Networks for One Shot Learning"**. This approach trains models on a variety of learning tasks so that they can quickly adapt to new tasks using limited data, mimicking human learning efficiency. The field of multimodal few-shot learning, spanning vision and language modalities, has recently gained traction with the advent of the first multimodal few-shot learner**Zhang et al., "Multi-Modal Few-Shot Learning"**. This development was followed by other innovative prompting and in-context learning strategies**Kim et al., "Prompt Tuning: A Simple yet Effective Approach to Few-Shot Learning"**. Flamingo**Ramesh et al., "Flamingo: A Super-Large Visual-Language Model"** utilized a super large visual-language model comprising 70 billion parameters for in-context learning. MiniGPT-4**Zhang et al., "MiniGPT-4: A Lightweight Tunable Image Captioning Model"** aligns a frozen visual encoder with a frozen LLM through a single projection layer. However, our objective diverges from the above multi-modality visual-language models, as we aim to rapidly adapt to new unseen tasks by acquiring meta-knowledge across different tasks as in**Finn et al., "Model-Against-Mean"**, by incorporating a light-weighted model with much fewer trainable parameters. 

\noindent \textbf{Image captioning.} 
The image captioning task involves generating descriptive text for an image to interpret and articulate the content and context of visual data contained in the imageÂ±**Vinyals et al., "Show and Tell: A Neural Image Caption Generator"**. This task can usually be tackled in two strategies: retrieval-based and generation-based. Retrieval-based approaches**Kazemzadeh et al., "ReferItGame: Referring to Objects in Images using Natural Language Descriptions"** retrieve complete sentences that best describe the given image from a corpus, while generation-based approaches**Vinyals et al., "Show and Tell: A Neural Image Caption Generator"** construct sentences by generating words in sequence, which typically contain a visual encoder to extract image features and a sequence-to-sequence model such as LSTM**Graves et al., "Visualizing and Understanding Recurrent Networks for Humanoid Robot Motion Control"** and/or Transformer**Vaswani et al., "Attention Is All You Need"** for text generation. To extract accurate image features, previous work**Karpathy et al., "Deep Visual-Semantic Alignments for Generating Image Descriptions"** proposed using Region-of-Interest (RoI) features generated from off-the-shelf objector detectors**Girshick et al., "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"**. For text generation, previous approaches**Graves et al., "Visualizing and Understanding Recurrent Networks for Humanoid Robot Motion Control"** typically utilize LSTM, while subsequent work**Vaswani et al., "Attention Is All You Need"** leverages attention-based models such as Transformers to predict captions. Recent studies have integrated the retrieved image-caption pairs into generation-based models**Yang et al., "In-Context Image Captioning"**, which aligns with the concept of in-context learning**Zhang et al., "Flamingo: A Super-Large Visual-Language Model"**.