\section{Related Work}
\label{sec-relatedwork}
\subsection{Methods to Collaborate Multiple LLMs}

Recent advancements in large language models (LLMs) have explored strategies to leverage the collective strengths of multiple models, enhancing performance and reliability across various tasks.

For example, Zhang et al. introduce the Chain-of-Agent (CoA) framework, which utilizes multi-agent collaboration in natural language \citep{CoA}.
In CoA, multiple LLM agents communicate to process different segments of text.
Then, a manager agent synthesizes their contributions into the final output.
With each agent processing short contexts, CoA can handle longer contextual input more effectively than conventional methods.
It has demonstrated significant improvements (up to 10\%) in tasks such as question answering, summarization, and code completion, when compared to strong baselines like retrieval augmented generation (RAG), full-context models, and multi-agent LLMs.

The LLM-Synergy framework \citep{yang2023one} proposes techniques like boosting-based weighted majority voting and cluster-based dynamic model selection to enhance accuracy in medical QA datasets.
This approach achieves state-of-the-art results, including 38.01\% accuracy on MedMCQA, 96.36\% on PubMedQA, and 38.13\% on MedQA-USMLE, outperforming other methods across all three datasets.

Additionally, the Formal Debate framework (FORD) \citep{ford} introduces a structured three-stage debate process to facilitate effective collaboration among LLMs.
While the integrated results are not always optimal, the study demonstrates the potential of debates to improve performance even when there are inconsistencies between LLMs' outputs.

These methods highlight the potential of multi-LLM collaboration in addressing challenges related to performance and reliability.

Based on the insights provided by these studies, our study explores a new approach to achieving multi-LM cooperation.
Unlike the aforementioned frameworks, our system introduces two key elements: \textit{pseudo-learning}, which pre-evaluates the performance trends of LLMs, and \textit{debate integration}, which synthesizes LLMs' outputs through discussions.
Through these mechanisms, our system aims to effectively address performance and reliability challenges.

\subsection{Automated Short Answer Grading}
The application of a LLM in education has also received significant attention.
Automated short answer grading (ASAG) field is no exception.
ASAG targets the grading of student answers to open-ended questions, which are typically a few sentences long.
In this section, we introduce such technologies, by categorizing existing grading methods into two primary approaches: \textbf{score or label prediction} and \textbf{detailed feedback generation}.


\subsubsection{Grading Methods for Scores and Labels Prediction}
Numerous studies have explored grading methods that focus on assigning scores or labels to student answers.
For example, Gobbo et al. propose GradeAid, a framework to grade short student answers.
It gives numeric scores to student answers by extracting lexical and semantic features \citep{gradeaid} from them.
Lexical features are calculated using TF-IDF, while semantic features are derived using a BERT-Cross Encoder.
These features are concatenated into a single vector and processed through a regression model to predict the final grading score.
GradeAid demonstrates effective grading across various datasets, while its performance consistency depends on the specific questions and datasets used.
% 
Another approach utilizes ensembles of pre-trained BERT models for scoring tasks \citep{ensembleoflm}.
This study shows that combining multiple models can improve score prediction accuracy.

While BERT models are used in various grading tasks, GPT models are also becoming popular these days.
For example, Chang et al. employ GPT-3.5 and GPT-4 for scoring and label classification (pass/fail) of Finnish student answers \citep{finnishgrading}.
This research found that GPT-4 performs well in the grading tasks, especially in the one-shot setting.
But they also admit the necessity of further performance improvement before introducing this kind of technology to the real education situation.


Automated grading can reduce not only the burden on teachers but also the subjectivity inherent in manual grading.
Gobrecht et al. indicate that automated scoring methods can enhance fairness and equity in evaluations \citep{beyondhumansubjectivity}.
They fine-tuned a transformer-based scoring system and evaluated student answers which already assigned grades by human graders.
Then, human graders grade the same answers again.
Finally, these results are compared with the past grading to evaluate the grading consistency.
This experiment reveals that the model has less deviation from the past grading results than the human re-graders.


\subsubsection{Grading Methods for Detailed Feedback Generation}
Some methods generate detailed grading reasons alongside scores and labels to enhance transparency and usability.
% 
For example, Huang et al. proposes a method that automatically evaluates answers using a fine-tuned GPT model and generates feedback to guide students to the correct answers \citep{direct}.
We have also proposed a technique to automatically generate grading labels and reasons by combining a single LLM and a grammatical structure analysis technique \citep{icetc}.

In addition, Lee et al.'s study \citep{cotscoring} uses a prompt construction method called Chain-of-Thought (CoT) to perform reasoned automatic grading with GPT-3.5 and GPT-4 \citep{cotscoring}.
CoT prompting is a way to improve LLM outputs by encouraging step-by-step reasoning.
In this system, the LLM generates evidence that supports its grading decisions before assigning a label (e.g., Proficient, Developing, or Beginning).
This approach enables GPT-4 to provide explanations for its assigned labels, improving interpretability.
% 
Another study investigated GPT-4's potential for grading middle school level answers \citep{sasgpt4}.
Their method generates both grading scores and rationale paragraphs, by referring to the question, student answer, scoring rubrics, and additional information where applicable.
GPT-4 achieved a quadratic weighted kappa of 0.677 across 10 questions.
While this experimental result shows a high potential of auto-grading by the model, it is also mentioned that grading outcomes varied across subjects (e.g., biology, English), highlighting the instability.

Some studies propose grading frameworks that can work with various types of LLMs.
For example, Jordan et al. developed an automated grading framework: FreeText \citep{freetext}.
It gets a question, grading criteria, and a student answer, then generates personalized feedback for the student.
In this framework, users can freely select a grading model.
Fateen et al. propose another system, ASAS-F-RAG, which integrates retrieval-augmented generation (RAG) with automated grading \citep{beyondscores}.
When grading student answers, this method retrieves the top 3 to 5 most similar examples from past grading data using the ColBERT retriever.
These examples are given to the LLMs as grading examples.
This method achieves accurate generation of scores, labels (e.g., Correct, Partially Correct, or Incorrect), and feedback.
They also employ several LLMs in their experiments, indicating that their system achieves accurate grading without depending on a specific LLM.

\subsubsection{Dataset Contributions for Automated Grading}
While many studies focus on label and score prediction, fewer studies address the generation of grading reasons.
This discrepancy stems from the limited availability of datasets that include detailed grading reasons.
To address this, several works have introduced new datasets to support research on ASAG:

\begin{itemize}
    \item SAF Dataset \citep{safdataset}: Filighera et al. construct the SAF dataset collected from college-level network communication classes. It includes questions, reference answers, student answers, grading labels, scores, and grader feedback. They demonstrated the dataset's utility by fine-tuned models like T5-base and mT5-base to jointly predict scores, labels, and feedback.

    \item EngSAF Dataset \citep{iunderstandwhy}: Aggarwal et al. construct another dataset: EngSAF. It has a similar structure to the SAF, but it expands the scope with a larger data size and broader topics across various courses. Experiments using EngSAF involved both fine-tuned models (e.g., LLaMA-2-13B-chat, Mistral-7B-Instruct) and zero-shot evaluations with GPT-3.5-turbo. These models generated output labels and feedback by comparing student answers to reference answers.
\end{itemize}

These datasets are crucial for advancing the development of systems that prioritize reason generation alongside grading.

\subsubsection{Comparison to Our Approach}

Tables \ref{table-outputs} and \ref{table-techniques} summarize the features and techniques of the reviewed methods, contrasting them with our system.

\begin{table}[!htb]
    \centering
    \caption{Outputs of Each System}
    \label{table-outputs}
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{lccc}
        \hline
        \textbf{Method/System}                              & \makecell{\textbf{Label Pred.}} & \makecell{\textbf{Score Pred.}} & \textbf{Reason Gen.} \\ \hline
        GradeAid \citep{gradeaid}                           & ×                               & \checkmark                      & ×                    \\
        Finnish Grading \citep{finnishgrading}              & \checkmark                      & \checkmark                      & ×                    \\
        BERT Ensemble \citep{ensembleoflm}                  & ×                               & \checkmark                      & ×                    \\
        Reduce Subjectivity \citep{beyondhumansubjectivity} & ×                               & \checkmark                      & ×                    \\
        CoT Scoring \citep{cotscoring}                      & \checkmark                      & ×                               & \checkmark           \\
        FreeText\citep{freetext}                            & ×                               & ×                               & \checkmark           \\
        ASAS-F-RAG \citep{beyondscores}                     & \checkmark                      & \checkmark                      & \checkmark           \\
        GPT-4 Grading \citep{sasgpt4}                       & ×                               & \checkmark                      & \checkmark           \\
        SAF Baselines \citep{safdataset}                    & \checkmark                      & \checkmark                      & \checkmark           \\
        EngSAF Baselines \citep{iunderstandwhy}             & \checkmark                      & \checkmark                      & \checkmark           \\
        DIRECT \citep{direct}                               & \checkmark                      & ×                               & \checkmark           \\
        Our Previous System \citep{icetc}                   & \checkmark                      & ×                               & \checkmark           \\

        \textbf{GET} (Our System)                           & \checkmark                      & ×                               & \checkmark           \\ \hline
    \end{tabular}
\end{table}

\begin{table}[!htb]
    \centering
    \caption{Techniques Used in Each Method}
    \label{table-techniques}
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{lcccc}
        \hline
        \textbf{Method/System}                              & \makecell{\textbf{Fine-Tuning}} & \makecell{\textbf{Few-Shot}} & \makecell{\textbf{CoT/}\textbf{ToT}} & \makecell{\textbf{Multi-LLM} \\\textbf{Collab.}} \\ \hline
        GradeAid \citep{gradeaid}                           & \checkmark                      & ×                            & ×                                    & ×                            \\
        Finnish Grading \citep{finnishgrading}              & ×                               & \checkmark                   & ×                                    & ×                            \\
        BERT Ensemble \citep{ensembleoflm}                  & \checkmark                      & ×                            & ×                                    & \checkmark                   \\
        Reduce Subjectivity \citep{beyondhumansubjectivity} & \checkmark                      & ×                            & ×                                    & ×                            \\
        CoT Scoring \citep{cotscoring}                      & ×                               & \checkmark                   & \checkmark                           & ×                            \\
        FreeText \citep{cotscoring}                         & ×                               & ×                            & ×                                    & ×                            \\
        ASAS-F-RAG \citep{beyondscores}                     & ×                               & \makecell{\checkmark }       & ×                                    & ×                            \\
        GPT-4 Grading \citep{sasgpt4}                       & ×                               & \checkmark                   & \checkmark                           & ×                            \\
        SAF Baselines \citep{safdataset}                    & \checkmark                      & \checkmark                   & ×                                    & ×                            \\
        EngSAF  Baselines \citep{iunderstandwhy}            & \checkmark                      & ×                            & ×                                    & ×                            \\
        DIRECT \citep{direct}                               & \checkmark                      & ×                            & ×                                    & ×                            \\
        Our Previous System \citep{icetc}                   & \checkmark                      & ×                            & ×                                    & ×                            \\

        \textbf{GET} (Our System)                           & ×                               & \checkmark                   & \checkmark                           & \checkmark                   \\ \hline
    \end{tabular}
\end{table}

As shown in Table \ref{table-outputs}, our system focuses on label prediction and reason generation, excluding score prediction.
This design choice aligns with our ultimate goal: providing effective feedback to support student self-learning rather than determining official grades.

Table \ref{table-techniques} highlights how our system distinguishes itself from prior methods.
Unlike approaches that rely on a single model, our method leverages multiple LLMs to enhance grading accuracy.
This is achieved through a combination of ensemble learning and the Tree of Thought (ToT) approach.
By collaborating multiple LLMs, we can compensate for the weaknesses of individual models.
This results in more consistent and balanced grading outcomes, without relying on a specific LLM.

Note that the ToT approach extends the Chain of Thought (CoT) methodology.
While CoT simulates a linear flow of thought in LLMs, ToT explores multiple alternatives simultaneously.
It branches the reasoning into a tree-like structure.
This branching enables more nuanced grading decisions and improves accuracy.

Furthermore, our system avoids the computationally expensive process of fine-tuning.
Instead of training a new model to combine model outputs (as in traditional ensemble learning), we focus on identifying the performance tendencies of various LLMs.
These identified tendencies are coupled with few-shot learning and the ToT framework, allowing us to achieve high performance without the overhead of fine-tuning.
By omitting fine-tuning, we reduce the time and computational resources required.
This enables faster and more efficient system deployment.