\section{Related Works}
\subsection{Mental Health Related Datasets}
Social media platforms like Reddit and their posts have been extensively used for building mental health corpus for classification. These include CLPsyc \cite{clpsyc} for classifying into disorder or causes and ANGST \cite{angst} for comorbid anxiety and depression classification. Dreaddit and Depression Reddit focus on single label depression classification. 
SAD \cite{sad-dataset} and CAMS \cite{cams-dataset} on the other hand consider for depression classification into various degrees. DATD considers a Reddit post for anxiety.
%The CLPsych released in two versions (CLPsych15 \cite{clpsych15} and CLPsych16 \cite{clpsych16} ) is a multi-label classification dataset built using <source?>
The recent dataset called IMHI \cite{mentallama} consists of many previously presented classification datasets to form a unified mental health benchmark aimed at evaluating model performances.
Several datasets and methods also support the development of mental health dialogue systems. ConvCounsel \cite{chen2024convcounselconversationaldatasetstudent} focuses on active listening in student counseling, while MentalQA \cite{alhuzali2024mentalqaannotatedarabiccorpus} provides Arabic mental health QA data. SMILECHAT \cite{qiu2024smilesingleturnmultiturninclusive}creates multi-turn dialogues from single-turn QA using ChatGPT. ESConv \cite{liu2021emotionalsupportdialogsystems} and AugESC \cite{zheng2023augescdialogueaugmentationlarge} offer emotional support dialogues, with 1,053 and 102k dialogues, respectively. Methods like SMILE generate multi-turn conversations using LLMs, addressing data scarcity, while PsyEval \cite{jin2024psyevalsuitementalhealth} assesses LLMsâ€™ mental health knowledge, diagnostic ability, and emotional support skills. General medical QA datasets like TREC QA \cite{trecqa}, BioASQ 2019 \cite{bioasq}, PubMedQA \cite{pubmedqa} also include questions on mental health, even though not explicitly classified. MedMCQA \cite{medmcqa} has a separate category for psychiatry with \textasciitilde4.1k QA pairs. 


%\subsection{General Medical QA}
%Previous works have released a number of general medical QA datasets for NLP tasks. TREC QA \cite{trecqa} is a genomics datasets where the task is to retrieve relevant documents for different questions. BioASQ 2019 \cite{bioasq} is another commonly used dataset with 2,747 training samples and 500 test samples. It has two different tasks - (i) document retrieval specific to a question; and (ii) question answering as yes/no, summary, listing, or factoid type. 
%PubMedQA \cite{pubmedqa} contains 1k expert-annotated QA data instances and 211.3k artificially generated QAs. Based on question formation using PubMed research article titles and their corresponding abstracts, PubMedQA was introduced as the first biomedical reasoning dataset. Furthermore, MedMCQA \cite{medmcqa} is a curated multiple options type curated dataset from the USMLA examinations with <number> instances. emrQA \cite{emrqa} is an EHR based extractive QA dataset.

%\subsection{Mental Health Corpus}
%Social media platforms and their posts have been extensively used to build previous mental health corpus. The CLPsych released in two versions (CLPsych15 \cite{clpsych15} and CLPsych16 \cite{clpsych16} ) is a multi-label classification dataset built using <source?>. It covers depression and PTSD categories along with a control test group, \textit{i.e.}, without any disorder or mental health issues. 
%Further, RSDD (full form) what it is.
%\\
%The Dreadit \cite{dreaddit} or Depression Reddit is again built using Reddit posts. What it does.
%%\\
%Similar to Depression Reddit, DATD and Anxiety Reddit datasets have also been released. 
%\\
%The ANGST dataset \cite{angst} is compiled from publicly available Reddit posts.It is labeled in a multi-label fashion to indicate depression and/or anxiety. 

\subsection{Mental Health Models}
Many discriminative language models have been trained extensively on mental health corpuses or dataset. Mental-BERT \cite{MentalBERT}, Mental-RoBERTa, Mental-XLNet \cite{mentalXLNET}, Mental-LongFormer \cite{mentalXLNET}, etc. have been trained on Reddit mental health posts, thereby improving the inherent understanding of the specific domain. Recently \cite{mentallama} introduced Mental-Llama, which is the first kind of work for LLM-based training on mental health datasets. Specifically, Llama-2 models were extensively fine-tuned on the IMHI dataset for classification and reasoning-based outputs. Similar works also include Mental-Flan-T5 and Mental-Alpaca. Recent work by \cite{interpret-mental-health} has also shown an analysis of the performance of various LLMs like GPT on general mental health domain understanding, including classification and condition identification.


%Early approaches have used several LSTM and CNN based approaches for mental health classification from text data. Later works have used BERT based training for pre-training transformer models on mental health datasets. This gave rise to models like Mental-BERT \cite{MentalBERT}, mental-RoBERTa, Mental-XLNet \cite{mentalXLNET}, Mental-LongFormer \cite{mentalXLNET}, etc. Recently, (authors) introduced Mental-Llama \cite{mentaLlama}, which is first kind of work for LLM based training on mental health dataset. However, the use of LLM for mental health has been limited by classification type tasks, while the QA and reasoning abilities of LLMs along with their benchmarks remain areas of further exploration. 

%\subsection{Conversational Mental Health Datasets}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage{graphicx}

\begin{table}[t]
\centering
\resizebox{0.90\columnwidth}{!}{%
\begin{tabular}{@{}llrr@{}}
\toprule
\textbf{Category} & \textbf{Type} & \textbf{MHQA-Gold} & \textbf{MHQA-B} \\ \midrule
\multirow{4}{*}{Anxiety} & Factoid & 85 & 2,250 \\ 
 & Diagnostic & 171 & 4,363 \\ 
 & Prognostic & 136 & 3,367 \\ 
 & Preventive & \textbf{212} & \textbf{5,284} \\ \cmidrule(l){2-4} 
 & \textbf{Total} & 604 & 15,264 \\ \midrule
\multirow{4}{*}{Depression} & Factoid & 90 & 3826 \\ 
 & Diagnostic & \textbf{207} & \textbf{8,315} \\ 
 & Prognostic & 152 & 6,252 \\ 
 & Preventive & 168 & 7,183 \\ \cmidrule(l){2-4} 
 & \textbf{Total} & 616 & 25,576 \\ \midrule
\multirow{4}{*}{Trauma} & Factoid & 26 & 498 \\ 
 & Diagnostic & \textbf{242} & \textbf{3,572} \\ 
 & Prognostic & 147 & 1,921 \\ 
 & Preventive & 202 & 3,142 \\ \cmidrule(l){2-4} 
 & \textbf{Total} & 617 & 9,133 \\ \midrule
\multirow{4}{*}{OCD} & Factoid & 123 & 1,505 \\ 
 & Diagnostic & \textbf{259} & \textbf{2,307} \\ 
 & Prognostic & 123 & 1,051 \\ 
 & Preventive & 132 & 1,306 \\ \cmidrule(l){2-4} 
 & \textbf{Total} & 637 & 6,169 \\ \bottomrule
\end{tabular}%
}
\caption{MHQA dataset statistics: Distribution across mental health conditions and question types.}
\label{tab:mhqa-stats}
\end{table}