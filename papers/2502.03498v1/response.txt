\section{Related Work}
\paragraph{Cross-view ground scene generation.} 
In the study by Li, "Learning to Generate Ground-View Images from Satellite Imagery"__, the authors first attempted to align the semantic features of satellite images onto ground-level perspectives. In Zhang et al., "Cross-View Image Synthesis with Conditional Generative Adversarial Networks"__, GANs were employed to generate ground images. Strong geometric relationships were introduced in the task of ground image generation by Chen, "Geometric Consistency for Cross-View Ground Scene Generation"__.  ____ explicitly constructed a 3D point cloud representation of the scene, and then transformed it into a scene representation in a feed-forward manner. Wang et al., "Learning to Generate Ground Images from Satellite Views with Point Clouds"__ advocate for generating ground images from ground-to-ground scene segmentation images. Among these, Kim et al., "Noise-Prior Based Ground Scene Generation from Satellite Imagery"__ specifically highlights the impact of various noises on the generated results and innovatively proposes a noise-prior-based solution. However, previous methods have predominantly relied on coarse scene priors, leading to compounded errors in the results. We propose GCA and IHA to ensure geometric consistency between ground images and satellite views. 
The cross-view generation work targeting single objects is also highly inspiring. Liu et al., "Camera Position Encoding for Cross-View Scene Transformation"__ overlays camera position encoding for scene transformation, while Han et al., "Diffusion-Based Ground Scene Generation with Neural Radiance Fields"__ utilizes diffusion to optimize the Nerf representation of scenes. Chen and Wang, "Video Diffusion for Continuous Frame Data Generation"__ generate continuous frame data based on video diffusion. These approaches often fail in large-scale scene reconstruction, especially when dealing with significant perspective differences between satellite and ground images, which is the issue we are dedicated to addressing.
% Inspired by these successful works, we recognize the pivotal role of geometric relationships in generating ground scenes from cross-view satellite imagery. However, previous approaches have predominantly relied on coarse scene priors, leading to compounded errors in results, ultimately resulting in distorted scene structures and misaligned camera poses. To address this, we introduce a geometry-aware cross-attention mechanism and pose alignment mechanism to ensure geometric consistency between ground-level images and satellite views.

\paragraph{Text-controlled image generation.} In text generation, a multitude of solutions have emerged over time leveraging Generative Adversarial Networks (GANs)~(Zhang et al., "Conditional Generative Adversarial Network for Text-to-Image Synthesis"__). However, with the introduction of diffusion~(Chen et al., "Diffusion-Based Image Generation from Text Conditions"__), its exceptional generation capability has evolved into a potent tool for creating images. SigniÔ¨Åcant strides have been taken in text-driven image synthesis through diffusion by Wang et al., "Text-Driven Ground Scene Synthesis with Diffusion"__. Kim et al., "Scene Segmentation and Text-Conditioned Image Generation"__ propose a method that generates ground images based on text conditions and BEV segmentation images. However, this strategy is hampered by the limitations of expressive capabilities in scene segmentation, leading to arbitrary results in scene synthesis. In this paper, we employ satellite images with enhanced representational capabilities for ground synthesis and introduce a novel text-guided mechanism to ensure both the reliability of scene generation and the diversity of generated results.