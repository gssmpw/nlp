
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{colortbl} % Include this package in your preamble

\usepackage{xcolor}
\definecolor{lightred}{RGB}{255,200,200}


% \title{Formatting Instructions for ICLR 2025 \\ Conference Submissions}
\title{Controllable Satellite-to-Street-View Synthesis with Precise Pose Alignment and Zero-Shot Environmental Control}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
  Xianghui Ze\textsuperscript{1}\thanks{This work was done during a visit at ShanghaiTech University.},  \quad Zhenbo Song\textsuperscript{1}, \quad Qiwei Wang\textsuperscript{2}, \quad Jianfeng Lu\textsuperscript{1}, \quad Yujiao Shi\textsuperscript{2}\thanks{Corresponding author.}\\
  \textsuperscript{1}Nanjing University of Science and Technology, \ \textsuperscript{2}ShanghaiTech University\\
  \texttt{\{zexh, songzb, lujf\}@njust.edu.cn}, \\ 
  \texttt{\{wangqw2024, shiyj2\}@shanghaitech.edu.cn}
  \vspace{-1.5em}
}
% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Generating street-view images from satellite imagery is a challenging task, particularly in maintaining accurate pose alignment and incorporating diverse environmental conditions. 
While diffusion models have shown promise in generative tasks, their ability to maintain strict pose alignment throughout the diffusion process is limited.  
In this paper, we propose a novel Iterative Homography Adjustment (IHA) scheme applied during the denoising process, which effectively addresses pose misalignment and ensures spatial consistency in the generated street-view images. 
Additionally, currently, available datasets for satellite-to-street-view generation are limited in their diversity of illumination and weather conditions, thereby restricting the generalizability of the generated outputs. To mitigate this, we introduce a text-guided illumination and weather-controlled sampling strategy that enables fine-grained control over the environmental factors. Extensive quantitative and qualitative evaluations demonstrate that our approach significantly improves pose accuracy and enhances the diversity and realism of generated street-view images, setting a new benchmark for satellite-to-street-view generation tasks.
\vspace{-2em}
  % Satellite imagery offers comprehensive coverage of the Earth, but capturing ground-level images is resource-intensive. Generating ground images directly from satellite views is thus valuable. However, due to significant viewpoint differences and limited scene variations in existing datasets, creating realistic and diverse ground-level images remains challenging. Current methods often rely on rough 3D priors, leading to compounded errors and scene misalignments, or they struggle to simulate various weather conditions. We propose a novel framework that utilizes a fine-tuned, pre-trained stable diffusion model enhanced with a new attention mechanism to integrate geometric information. During inference, we introduce pose alignment and text-guided mechanisms. This approach ensures the authenticity and diversity of generated ground images. Extensive experiments demonstrate our framework's effectiveness across multiple datasets, outperforming existing methods in maintaining geometric consistency and scene diversity.
\end{abstract}

\section{Introduction}

% This paper addresses the problem of satellite-to-street-view synthesis, which aims to generate geometrically consistent street-view images aligned with satellite imagery under various environmental conditions. The ability to generate such street-view images has significant applications in urban modeling, geospatial analysis, and virtual reality. While satellite images offer comprehensive coverage of the globe at a low cost, ground-level image collection is both resource-intensive and constrained by limited access to diverse weather and seasonal conditions. Consequently, synthesizing street-view images that maintain geometric fidelity with satellite views and offer controllable environmental diversity remains a critical challenge.

This paper tackles the problem of satellite-to-street-view synthesis, aiming to generate street-view images that are geometrically consistent with satellite imagery under a pre-determined relative pose and diverse environmental conditions. Synthesizing such images has critical applications in urban modeling, geospatial analysis, and virtual reality. While satellite images provide comprehensive global coverage at low cost, capturing ground-level data is resource-intensive and lacks access to varied weather and seasonal conditions. 
Therefore, generating geometrically accurate street-view images with controllable environmental diversity is a challenging but essential task.
% Consequently, synthesizing street-view images that maintain geometric fidelity with satellite views and offer controllable environmental diversity remains a critical challenge.

The difficulties in this task stem from two key aspects: geometric consistency and environmental diversity. Geometric consistency is challenging due to the vast perspective difference between satellite and ground views—where satellite images provide a top-down perspective, street-view images show the scene from a lateral angle, resulting in minimal visual overlap. Establishing a reliable mapping between these two viewpoints requires precise handling of geometric information. Additionally, environmental diversity is crucial for practical applications, as street-view images under different weather and lighting conditions are needed to simulate real-world scenarios. Existing datasets for satellite-to-street-view tasks offer limited variation in environmental conditions, further complicating the generation of diverse and realistic outputs.

Recent works have made progress in this domain by using geometric priors to bridge the gap between satellite and ground perspectives. For example, \cite{shi2022geometry} used multi-plane images to infer depth maps from satellite data and generate ground-level images, while \cite{qian2023sat2density} rendered panoramas from satellite images based on density fields. However, these approaches are prone to errors due to their reliance on approximate 3D priors. Meanwhile, methods focusing on generating images with environmental diversity~(\cite{assion2024bdd}) are often limited to in-domain editing and lack the ability to generate diverse scenes from satellite inputs.

\begin{figure}[t]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \includegraphics[width=1\textwidth]{./image/head2.pdf}
  \caption{\small  
  Our method synthesizes ground-level images from satellite maps by integrating Iterative Homography Adjustment (IHA) to refine geometric alignment and Text-guided Zero-shot Environmental Control (ZoEC) to provide flexible environmental control, enabling precise pose alignment and diverse scene generation.
  % Our method generates ground images from satellite maps, utilizing Iterative Homography Adjustment (IHA) to ensure precise pose alignment and employing Text-guided Zero-shot Environmental Control (ZoEC) for textual guidance in controlling the output results. 
  \label{Figure:head}}
  \vspace{-1.5em}
\end{figure}

As shown in Fig.~\ref{Figure:head}, this paper proposes a novel framework for satellite-to-street-view synthesis that addresses two key challenges: ensuring geometric alignment and enabling environmental control. Our approach is based on the classical stable diffusion model~(\cite{rombach2022high}),  which has shown strong performance in image synthesis but faces significant challenges in maintaining precise pose information during generation, often leading to misalignment between generated street-view images and satellite inputs.
% While diffusion models have demonstrated exceptional performance in image synthesis, directly applying them to satellite-to-street-view tasks presents a significant challenge: they often struggle to accurately encode and maintain pose information throughout the generation process, leading to spatial misalignment between the generated street-view images and the satellite input.

To overcome these issues, we propose two key innovations. First, unlike standard diffusion models where conditions are implicitly encoded within the denoising U-Net, we introduce a cross-view conditioning mechanism that incorporates geometric information. This mechanism ensures that the generated street-view panoramas maintain a consistent spatial layout, aligning scene objects with the satellite images. 
Second, we propose an Iterative Homography Adjustment scheme that operates during the diffusion sampling phase. This process corrects pose misalignment by iteratively adjusting the intermediate output based on the relative pose difference between the generated image and the satellite view, ensuring geometric consistency throughout the generation process.
% Second, we propose an Iterative Homography Adjustment scheme that operates during the sampling phase of the diffusion process. This scheme iteratively estimates the relative pose between the denoised intermediate output and the conditioned satellite image, and the difference between the estimated and conditioned poses is used to correct the intermediate output. By applying this pose alignment at each stochastic denoising step, our framework ensures that the generated images remain geometrically consistent with the satellite view, effectively addressing the pose misalignment issue commonly encountered in diffusion-based models.

Furthermore, real-world environments exhibit a wide range of illumination and weather conditions, which are rarely reflected in existing satellite-and-street-view datasets. This lack of diversity limits the model's ability to generate realistic street-view images under different environmental settings. To overcome this limitation, we introduce a Zero-Shot Environmental Control strategy, which uses text prompts to guide the generation of street-view images under varying illumination and weather conditions without retraining the model. 
% By applying this strategy only during inference, we achieve flexible environmental control, even when the training data lacks such variations.
% encodes the environmental control by a text format and ensures the CLIP consistency between the denoised latent vectors during the stochastic diffusion process and the environmental control. 
% This approach only operates during the sampling (inference) process and does not fine-tune or re-train the models, and thus allows for the generation of street-view images with controllable illumination and weather conditions when such conditions are absent from the training data. 


Finally, since satellite images lack crucial details like the sky and the sides of buildings that are present in ground-level views, traditional metrics such as RMSE are inadequate for fair comparisons across different methods. 
To provide a more comprehensive evaluation, we introduce new evaluation metrics that assess both semantic and geometric consistency between generated and ground truth images, as well as the alignment between the generated images, pose, and environmental conditions.

% To address this limitation, we introduce new evaluation metrics designed to assess semantic and geometric consistency between the generated images and the ground truth. Additionally, our metrics evaluate the geometric alignment between the generated output and the conditioning pose and environmental controls, ensuring a more comprehensive and accurate assessment of model performance.

Our contributions are summarized as follows:
\begin{itemize}\setlength{\itemsep}{0pt}
\item We introduce a novel cross-view conditioning mechanism that incorporates geometric information, ensuring precise spatial alignment between the generated street-view panoramas and satellite images.
\item We propose an Iterative Homography Adjustment scheme during the diffusion sampling process, addressing pose misalignment by iteratively correcting the generated images for geometric consistency.
\item We demonstrate the effectiveness of our framework in generating diverse street-view images with controllable environmental conditions (e.g., weather and lighting) in a zero-shot manner.
\item We design new evaluation metrics that measure both semantic and geometric consistency between the generated and ground truth images, as well as the alignment between the generated outputs and the controllable pose and environmental conditions.

\end{itemize}

\section{Related Work}
\paragraph{Cross-view ground scene generation.} 
In the study by \cite{zhai2017predicting}, the authors first attempted to align the semantic features of satellite images onto ground-level perspectives. In \cite{wu2022cross}, GANs were employed to generate ground images. Strong geometric relationships were introduced in the task of ground image generation by \cite{lu2020geometry, shi2022geometry, qian2023sat2density}. \cite{li2021sat2vid, li2024sat2scene} explicitly constructed a 3D point cloud representation of the scene, and then transformed it into a scene representation in a feed-forward manner. \cite{gao2024scp, xu2024geospecific, li2024crossviewdiff} advocate for generating ground images from ground-to-ground scene segmentation images. Among these, \cite{gao2024scp} specifically highlights the impact of various noises on the generated results and innovatively proposes a noise-prior-based solution. However, previous methods have predominantly relied on coarse scene priors, leading to compounded errors in the results. We propose GCA and IHA to ensure geometric consistency between ground images and satellite views. 
The cross-view generation work targeting single objects is also highly inspiring. \cite{liu2023zero} overlays camera position encoding for scene transformation, while \cite{poole2022dreamfusion} utilizes diffusion to optimize the Nerf representation of scenes. \cite{melas20243d} and \cite{gao2024cat3d} generate continuous frame data based on video diffusion. These approaches often fail in large-scale scene reconstruction, especially when dealing with significant perspective differences between satellite and ground images, which is the issue we are dedicated to addressing.
% Inspired by these successful works, we recognize the pivotal role of geometric relationships in generating ground scenes from cross-view satellite imagery. However, previous approaches have predominantly relied on coarse scene priors, leading to compounded errors in results, ultimately resulting in distorted scene structures and misaligned camera poses. To address this, we introduce a geometry-aware cross-attention mechanism and pose alignment mechanism to ensure geometric consistency between ground-level images and satellite views.

\paragraph{Text-controlled image generation.} In text generation, a multitude of solutions have emerged over time leveraging Generative Adversarial Networks (GANs)~(\cite{dash2017tac, regmi2018cross, ruan2021dae, tao2022df}). However, with the introduction of diffusion~(\cite{song2020score, rombach2022high}), its exceptional generation capability has evolved into a potent tool for creating images. Signiﬁcant strides have been taken in text-driven image synthesis through diffusion by \cite{avrahami2022blended, li2023gligen, brooks2023instructpix2pix}. \cite{li2023drivingdiffusion, gao2023magicdrive} propose a method that generates ground images based on text conditions and BEV segmentation images. However, this strategy is hampered by the limitations of expressive capabilities in scene segmentation, leading to arbitrary results in scene synthesis. In this paper, we employ satellite images with enhanced representational capabilities for ground synthesis and introduce a novel text-guided mechanism to ensure both the reliability of scene generation and the diversity of generated results.
\section{Method Overview}
Our task is to generate realistic street-view images based on a given satellite image, a specified relative pose, and an environmental factor such as illumination or weather. We use a latent diffusion model framework to conditionally synthesize these street-view images while ensuring control over both geometric and environmental aspects. The geometric condition is encoded via the satellite image and relative pose, while environmental control is applied during inference to achieve diverse visual effects.
\subsection{Diffusion Model Framework}
To generate street-view images with controlled conditions, we leverage a latent diffusion model~(\cite{Rombach_2022_CVPR}). Let the target street-view image be denoted as \( x_0 \) and its corresponding latent embedding as \( z_0 \). Using an image decoder \( \mathcal{D}(\cdot) \), the target image can be reconstructed as \( x_0 = \mathcal{D}(z_0) \). The forward diffusion process adds Gaussian noise to the latent embedding \( z_0 \) progressively, resulting in \( z_t \) at each time step \( t \in [0, T] \), where the noise level is defined by \( \beta_t \in (0, 1) \):
\begin{equation}
z_t = \sqrt{\bar{\alpha}_{t}} z_0 + \sqrt{1 - \bar{\alpha}_{t}} \varepsilon
\end{equation}
where \( \varepsilon \sim \mathcal{N}(0, I) \), \( \alpha_t = 1 - \beta_t \), and \( \bar{\alpha}_t = \prod_{i=0}^t \alpha_i \). The denoising U-Net, \( \epsilon_{\theta}(z_t, t, c) \), is used to predict the added noise, with condition \( c \) representing the posed embedding of the satellite image.

The reverse diffusion process is based on DDIM sampling:
\begin{equation}
p(z_{t-1}|z_t) = \mathcal{N}(z_{t-1}; \mu_{\theta}(z_t, t, c), \sigma_t^2 I)
\end{equation}
where \( \sigma_t \) controls the sampling's stochasticity, and \( \mu_{\theta}(z_t, t, c) \) is calculated as:
\begin{equation} \label{eq:3}
	\begin{aligned}
		\mu_{\theta}(z_t, t, c)  = \sqrt{\bar{\alpha}_{t-1}} \left( \frac{z_t - \sqrt{1 - \bar{\alpha}_t} \cdot \epsilon_{\theta}(z_t, t, c)}{\sqrt{\bar{\alpha}_t}} \right) + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \epsilon_{\theta}(z_t, t, c)   + \sigma_t \varepsilon \\
		% &\sigma_t  = \sqrt{(\alpha_t \beta_t + 1 - \alpha_{t-1} - \sigma_t^2)^{-1}/2}
	\end{aligned}
\end{equation}
% \begin{equation}
% \mu_{\theta}(z_t, t, c) = \sqrt{\bar{\alpha}_{t-1}} \left( \frac{z_t - \sqrt{1 - \bar{\alpha}_t} \cdot \epsilon_{\theta}(z_t, t, c)}{\sqrt{\bar{\alpha}_t}} \right) + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \epsilon_{\theta}(z_t, t, c)
% \end{equation}
\begin{figure}[t]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  % \includegraphics[width=1\textwidth]{./image/struct8_pipline.pdf}\\
  \includegraphics[width=\linewidth]{image/Framework_2.pdf}
  \caption{\small Overview of our framework. Our approach aims to utilize satellite images to generate corresponding ground images. We leverage geometric relationships extensively and have the capability to alter the features of the generated images based on different text prompts.  \label{Figure:pipline}}
  \vspace{-1em}
\end{figure}
\subsection{Environmental Guidance and Pose Alignment}
Due to the lack of labeled environmental conditions (e.g., variations in weather and lighting) in the existing cross-view image datasets, using classifier-free guidance for the control, like satellite image features and the relative pose, during the training process is impossible. 
To address this issue, we propose a zero-shot environmental guidance approach using text prompts by leveraging a classifier guidance strategy. Specifically, we employ off-the-shelf CLIP text embeddings as environmental control, enabling the guidance of environmental factors like lighting and weather during inference.
Furthermore, implicitly leveraging satellite image features as a condition often leads to pose misalignment in generated street-view images, as demonstrated in Fig.\ref{fig:pose_correct}. 
To further enhance the pose alignment, we introduce an Iterative Homography Adjustment within the DDIM sampling process.
Specifically, we denote environmental and pose guidance as \( g_{\text{text}} \) and \( g_{\text{pos}} \), respectively. 
% By treating these factors as independent and applying Bayes' theorem, we derive the following formula for our diffusion process:
% \begin{equation}
%     \begin{aligned}
%         p(z_{t-1}|z_t, g_{\text{pose}}, g_{\text{text}}) \approx 
%         \mathcal{N}(z_{t-1}; \mu_{\theta}(z_t, t, c) + \sigma_t^2 \nabla_{z_t} \log p(g_{\text{pose}}, g_{\text{text}}|z_t), \sigma_t^2 I)
%     \end{aligned}
% \end{equation}
% \subsection{Guidance-Driven Denoising}
% Using classifier-guidance~(\cite{dhariwal2021diffusion}), the denoising process with environmental and pose guidance is defined as:
% \begin{equation}
% z_{t-1} = \mu(z_t) + \sigma_t^2 \nabla_{z_t} \log p(g_{\text{pose}}, g_{\text{text}}|z_t) + \sigma_t^2 \varepsilon 
% \end{equation}
% This expression can be expanded to:
As shown in Appendix \ref{conv_formulae}, Using classifier-guidance~(\cite{dhariwal2021diffusion}), the denoising process with environmental and pose guidance is defined as:
\begin{equation}
z_{t-1} = \mu_{\theta}(z_t, t, c) + \lambda \nabla_{z_{t}} \log p(g_{\text{pose}}|z_{t}) + \gamma \nabla_{z_{t}} \log p(g_{\text{text}}|z_{t})  +  \sigma_t \varepsilon
\end{equation}
where hyperparameters \( \gamma \)  and \( \lambda \) control the strength of environmental and pose conditioning, respectively, ensuring that the generated image meets both environmental and geometric requirements.

In the following section, we discuss how conditions \( c \) of the posed satellite image are encoded, along with further technical details on enforcing pose alignment through \( \nabla_{z_{t}} \log(p(g_{\text{pose}}|z_{t})) \) and applying environmental guidance via \( \nabla_{z_{t}} \log(p(g_{\text{text}}|z_{t})) \) during the inference process.


\section{The Proposed Framework}
This paper aims to generate ground-level images that accurately align with a given satellite image, a relative pose, and environmental conditions. To achieve precise pose control, as illustrated in Fig.~\ref{Figure:pipline}, we propose embedding satellite features and the relative pose information into the diffusion model through two complementary mechanisms: a Geometric Cross-Attention module within the denoising network and an iterative pose enhancement strategy during the inference denoising stage. 
Considering diverse weather and illumination data are absent in the training set, we design a Zero-Shot Environmental Control strategy that allows flexible control over scene variations during inference without requiring additional training data.

\subsection{Cross-view Conditioning Mechanism}

Instead of implicitly encoding global feature vectors of posed satellite images, we propose a Geometric Cross-Attention (GCA) mechanism that explicitly constructs geometric correspondences.

Given a pixel coordinate \((u_g, v_g)\) in the target ground image, its corresponding 3D scene point can be determined if the depth \(d\) is known. Since acquiring an accurate depth map for the ground view is impractical, we instead hypothesize a set of \(N\) reference height values \(\{h_i\}_{i=1}^N\) (relative to the ground plane) for each pixel,  capturing potential depth variations across different regions. For each reference height, GCA estimates an offset \(\Delta h_i\) based on the current feature state and calculates an attention weight \(\mathbf{A}_i\) for each height hypothesis to evaluate its reliability.
The GCA mechanism then aggregates features from the satellite image as follows:
\begin{equation}\label{Ep_GCA}
  GCA(Q, V) = \sum_{i=1}^N \mathbf{A}_i \left( V \otimes P(u_g, v_g, h_i + \Delta h_i) \right),
\end{equation}
where \( Q \) and \( V \) represent the ground and satellite features, respectively. Here, \(\mathbf{A}_i\) and \(\Delta d_i\) are computed from the ground features \(Q\), and the \(\mathbf{A}_i\) is the result of applying softmax to the \(N\) reference heights. \(P(u_g, v_g, h_i + \Delta h_i)\) maps the ground pixel \((u_g, v_g)\) at height \(h_i + \Delta h_i\) to the corresponding satellite pixel coordinates based on relative pose. The symbol \(\otimes\) denotes a sampling operation that extracts features from \( V \) according to the projected satellite coordinates.

Unlike previous approaches that rely strictly on predefined 3D priors~(\cite{li2024crossviewdiff}), our method allows for iterative refinement of satellite-to-ground correspondences. This iterative adjustment enables the model to gradually correct initial misalignments and improve spatial consistency over time. By focusing on relevant regions through projected coordinates, GCA also reduces computational load compared to traditional global attention mechanisms, effectively enhancing geometric alignment between satellite images and generating ground-level views.

\begin{figure}[t]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \includegraphics[width=1\textwidth]{./image/struct8_H_adj_2.pdf}
  \caption{\small Homography Adjustment. We compare ground-level images with satellite images, calculating the loss by assessing the relative pose of the ground images at time t against a predefined pose. Ground images are then adjusted based on this comparison. \label{Figure:H_adj}}

\end{figure}



\subsection{Iterative Pose Alignment During Inference}
\label{sec:pos}
% \paragraph{Iterative Homography Adjustment for Pose Refinement.}
The inherent randomness of the diffusion process and implicit reasoning can often result in generated images that deviate from the specified pose, causing positional shifts, as shown in Experiment~\ref{exp:pose}. To address this issue, we propose an iterative position correction mechanism that adjusts each generated result to better match the specified pose throughout inference.
\begin{algorithm}[t]
\caption{\small Iterative Homography Adjustment for Pose Refinement.\label{Alg}}
\label{alg:Position}
\begin{algorithmic}
 \STATE {\textbf{Input:} diffusion steps $t$, the noisy image $z_t$, satellite image conditions $\mathbf{I}_s$ and the rotation $\mathbf{R}$ and translation $\mathbf{T}$ relative to the satellite image.}
  \STATE {\textbf{for all } $t$ from $T$ to 1 \textbf{do} }
  \STATE {\;\;\;\; ${z}_t \gets z_t \otimes grid(H)$ \;  // $H$ is a diagonal matrix of ones} 
  \STATE {\;\;\;\; ${z}_{t,0}, {z}_{t-1} \gets DDIM(z_t, t, \mathbf{R}, \mathbf{T}, \mathbf{I}_s)$ \; // eliminate noise to obtain ${z}_{t,0}, {z}_{t-1}$}
  % \STATE {\;\;\;\; ${x'}_{t,0}^{bev} \gets Warp({x'}_{t,0})$ \; // transform ground-level images into a bird's-eye view perspective}
  \STATE {\;\;\;\; $H \gets H - \nabla_H \mathcal{L}_{pose}({z}_{t,0}, \mathbf{R}, \mathbf{T}, \mathbf{I}_s))$} \; // adjust the Homography matrix
  \STATE {\;\;\;\; ${z}_{t-1} \gets {z}_{t-1} \otimes grid(H)$}
\STATE {\textbf{end for}}
% \RETURN {$H - \nabla_H \mathcal{L}_{pos}$ } \; // adjust H based on the loss
\end{algorithmic}
\end{algorithm}
Due to the complex scene depth, pose discrepancies can lead to substantial misalignments, making pixel-wise color or flow corrections difficult to learn. Instead, we introduce an Iterative Homography Adjustment mechanism that applies a Homography transformation matrix \( H \) to correct the latent representation \( z_t \) at each inference step, i.e., \( z_t \otimes \text{grid}(H) \), where \( \text{grid}(H) \) represents coordinates generated by \( H \). The matrix \( H \) enables transformations such as scaling, translation, and rotation, corresponding to the spatial alignment of the camera. Since achieving precise correction with a single \( H \) is challenging, we perform iterative pose adjustments in parallel with DDIM denoising steps, gradually guiding the output toward the target pose.

The process for adjusting \( H \) is illustrated in the bottom left of Fig.~\ref{Figure:pipline}. We first compute an initial latent \( z_{0,t} \) from \( z_t \) using the denoising Unet’s output at timestep $t$. We initialize \( H \) as an identity matrix, applying it to \( z_{0,t} \), which we then project onto an overhead view using a ground plane assumption, as in \cite{shi2025weakly}. 
To ensure robust pose alignment, as illustrated in Fig.~\ref{Figure:H_adj}, we sample multiple candidate ground camera poses in addition to the specified pose, cropping and rotating the corresponding regions from the satellite image feature maps, which are extracted from the conditioning branch of the denoising Unet. We introduce a Feature Refinement module in each branch to map the ground and satellite features to a shared representation space. These modules share the same architecture but have separate weights.
Next, we compute an alignment score, \( \{S_k\}_{k=1}^K \), using cosine similarity between the ground and satellite feature maps at \( K \) candidate poses. To quantify alignment, we use the InfoNCE loss~(\cite{oord2018representation}):
\begin{equation}
     \begin{split}
    \mathcal{L}_{\text{pose}} = - \log \frac{e^{S_{k^*} / \tau}}{\sum_{k} e^{S_{k} / \tau}},
  \end{split}
  \label{eq:infoNCE}
\end{equation}
where \( \tau \) is a temperature hyperparameter. The InfoNCE loss reaches its minimum when the ground and satellite feature maps achieve the highest similarity at the ground truth (GT) relative pose \( k^* \). Using this loss, we compute the gradient with respect to \( H \) and update it to improve alignment.

Our Homography Adjustment module applies transformations directly to the latent vector \( z_{0,t} \) rather than mapping it back to image space, avoiding the need for a heavy ground-to-satellite localization network which operates on the original image resolution. Instead, we employ a lightweight Feature Refinement module on \( z_{0,t} \) and the satellite features from the denoising Unet’s conditioning branch, significantly reducing the computational cost.

Training the Homography Adjustment network follows the initial training of the denoising Unet within the diffusion framework. After obtaining \( z_{t,0} \) from the Unet, we then train the Homography Adjustment network using the InfoNCE loss defined in Eq.~\ref{eq:infoNCE}. 
During the inference stage, both the denoising Unet and the Homography Adjustment network are trained. We then use it to refine \( H \) in the stochastic denoising sampling process in DDIM to improve pose alignment iteratively.
\subsection{Text-guided Zero-shot Environmental Control}
Many autonomous driving datasets such as CVUSA~(\cite{zhai2017predicting}) lack detailed textual annotations. As shown in Experiment \ref{sec:text}, we discovered that when using descriptions generated by a large multimodal model along with detailed satellite images as condition, the generative model tends to prioritize accurately representing the satellite image conditions, significantly diminishing the influence of textual prompts during the training process. Therefore, we separated the text prompt. Inspired by \cite{dhariwal2021diffusion}, we utilized text prompts to guide the direction of reasoning during the inference stage. We replace $\epsilon_t$ with $\hat{\epsilon}_t$:
\begin{equation}
  \hat{\epsilon}_t(z_t, t, S) = \epsilon_t(z_t, t, S) +  \nabla_{z_t}\mathcal{L}_{text}(z_{t}, c_{text})
\end{equation}
The gradient term $\nabla_{z_t}\mathcal{L}_{text}(z_{t}, c_{text})$ guides the diffusion process towards the desired direction, thereby ensuring that the generated results are consistent with the textual conditions.

To robustly obtain the loss between the generated results and the text prompt, we randomly partition $z_{t-1}$ into multiple patches and calculate the similarity with the text for each patch. We then use CLIP~(\cite{radford2021learning}) to extract features from the processed patches and the text, and calculate the cosine similarity between these features to obtain the final loss:
\begin{equation}
        \mathcal{L}_{text}(z_{t}, c_{text}) = 1 - \frac{1}{N} \sum\limits_{l=1}\limits^{N} sim(CLIP({z}_{t,0}^{l}), CLIP(c_{text}))
  \end{equation}
where ${z}_{t,0}$ is the result of removing noise from ${z}_{t}$, ${z}_{t,0}^{l}$ is the $l$-th patch of ${z}_{t,0}$, $c_{text}$ is the textual condition, and $sim$ represents cosine similarity. 
Similar to the Iterative Pose Alignment illustrated in Sec.~\ref{sec:pos}, this environmental control strategy is also applied during the inference stage, iteratively modifying the noisy latent during the denoising process.
\subsection{Evaluation Metrics}
There is a limited overlap between satellite images and ground truth images, further complicated by varying weather conditions and seasons, which makes it challenging to synthesize target view images that perfectly match the ground truth image provided in the dataset. As a result, pixel-level metrics like RMSE and PSNR are inadequate for this task~(\cite{zhang2018unreasonable, shi2022geometry}). Instead of focusing on color discrepancies between the generated street view images and the actual ground truth images, our emphasis should be on whether they represent the same locations. Therefore, we use structural similarity, perceptual similarity, semantic similarity, and depth similarity for performance evaluation. Furthermore, since satellite images do not capture sky information, we advocate for cropping the sky portion during evaluation. 
\begin{figure*}[t]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \begin{subfigure}{0.07\linewidth}
      \centering
      \includegraphics[width=\linewidth]{image/CVUSA_comp/148/sat.png} 
      \includegraphics[width=\linewidth]{image/CVUSA_comp/16/sat.png} 
      \includegraphics[width=\linewidth]{image/CVUSA_comp/18/sat.png} 
      \includegraphics[width=\linewidth]{image/CVUSA_comp/183/sat.png} 
      \caption{\small Sat}
  \end{subfigure}
  \begin{subfigure}{0.174\linewidth}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/148/S2S.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/16/S2S.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/18/S2S.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/183/S2S.png} 
      \caption{\small S2S}
  \end{subfigure}
  \begin{subfigure}{0.174\linewidth}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/148/sat2density.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/16/sat2density.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/18/sat2density.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/183/sat2density.png} 
    \caption{\small Sat2Den}
  \end{subfigure}
  \begin{subfigure}{0.174\linewidth}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/148/control.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/16/control.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/18/control.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/183/control.png} 
    \caption{\small ControlNet}
  \end{subfigure}
  \begin{subfigure}{0.174\linewidth}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/148/correct.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/16/correct.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/18/correct.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/183/correct.png}
    \caption{\small Ours}
  \end{subfigure}
  \begin{subfigure}{0.174\linewidth}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/148/gt.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/16/gt.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/18/gt.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_comp/183/gt.png}
    \caption{\small GT}
  \end{subfigure}
  \caption{\small Qualitative comparison with previous work on the CVUSA dataset, our framework is able to maintain better geometric relationships.}
    \vspace{-1em}
\label{fig:CVUSA_com}
\end{figure*}

\begin{table}[t]
  \centering
  \small
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \caption{\small Quantitative comparison with existing algorithms on CVUSA dataset. The best results are highlighted in orange and the second-best in blue.\label{tab:CVUSA_comp_sota}}
  \setlength{\tabcolsep}{0.5mm}{
  \begin{tabular}{c|cc|ccc|cc|ccc|c}
\midrule
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Structural similarity} & \multicolumn{3}{c|}{Perceptual similarity}         & \multicolumn{2}{c|}{Semantic Similarity} & \multicolumn{3}{c|}{Pixel Similarity}            & \multirow{2}{*}{↓Depth} \\
                        & ↑SSIM                                & ↓Self\_sim                           & ↓$P_{squeeze}$            & ↓$P_{alex}$               & ↓FID                     & ↓DINO                    & ↓SegAny                    & ↓RMSE                    & ↑PSNR          & ↑SD            &                         \\ \midrule
Pix2Pix                 & 0.32                                 & 0.21                                 & 0.35                      & 0.51                      & 44.51                    & 5.24                     & 0.38                       & 55.84                    & 13.20          & 12.08          & 21.85                   \\
XFork                   & 0.29                                 & -                                    & -                         & -                         & -                        & -                        & -                          & -                        & -              & -              & -                       \\
S2S                     & 0.35                                 & 0.19                                 & \cellcolor{blue!10}0.32   & \cellcolor{blue!10}0.48   & 29.49                    & 5.11                     & 0.39                       & \cellcolor{blue!10}54.57 & 13.40          & \cellcolor{blue!10}12.30 & 21.05                   \\
Sat2Density             & 0.33                                 & \cellcolor{blue!10}0.19              & 0.31                      & 0.46                      & 47.85                    & \cellcolor{blue!10}4.95  & \cellcolor{blue!10}0.38    & 54.23                    & \cellcolor{blue!10}13.46 & 12.27          & \cellcolor{blue!10}19.83 \\
ControlNet              & 0.32                                 & 0.20                                 & 0.33                      & 0.49                      & \cellcolor{blue!10}22.55 & 5.21                     & 0.38                       & 58.25                    & 12.83          & 12.08          & 21.78                   \\
CrossDiff               & \cellcolor{blue!10}0.37              & -                                    & -                         & -                         & 23.67                    & -                        & -                          & -                        & 12.00          & -              & -                       \\
Ours                    & \cellcolor{orange!15}\textbf{0.38}   & \cellcolor{orange!15}\textbf{0.18}   & \cellcolor{orange!15}\textbf{0.30}   & \cellcolor{orange!15}\textbf{0.45}   & \cellcolor{orange!15}\textbf{21.30} & \cellcolor{orange!15}\textbf{4.81}  & \cellcolor{orange!15}\textbf{0.36}   & \cellcolor{orange!15}\textbf{52.92} & \cellcolor{orange!15}\textbf{13.67} & \cellcolor{orange!15}\textbf{12.33} & \cellcolor{orange!15}\textbf{19.58} \\ \midrule
\end{tabular}}
  \vspace{-1em}
  \label{tab:sota-cvusa}
\end{table}

The evaluation of Structural Similarity is derived from SSIM and self-similarity structures based on DINO features~(\cite{caron2021emerging, shechtman2007matching, tumanyan2022splicing}). Perceptual Similarity is evaluated based on the FID metric and compares the similarity of features extracted from AlexNet~(\cite{krizhevsky2017imagenet}) and SqueezeNet~(\cite{iandola2016squeezenet}). Semantic Similarity is proposed for evaluating high-level semantic features. We employ widely acknowledged DINO~(\cite{caron2021emerging}) and Segment Anything~(\cite{kirillov2023segment}) for feature extraction to compare the semantic consistency of images. Additionally, given that satellite images contain depth information of ground scenes, evaluating the depth of generated ground images is crucial. We use DepthAnything~(\cite{yang2024depth}) to assess the depth differences between real ground truth images and generated images. For a fair comparison, we also report commonly used metrics such as RMSE, PSNR, and SD to evaluate Pixel Similarity. These metrics are more forgiving for assessing blurred images, as clear images tend to exhibit increased pixel differences due to the introduction of details such as seasons, weather, and shadows. This discrepancy conflicts with the intended outcome of our generated images, so we strongly recommend utilizing alternative metrics over pixel similarity evaluation metrics. Finally, for text similarity, we compute the similarity of Clip~\cite{radford2021learning} features for evaluation and utilize Blip~\cite{li2022blip} to describe the images, assessing the recall rate of answers that align with the textual descriptions.

% \textbf{Structural Similarity.} 
% First, to assess structural similarity, we seek metrics that can disregard local textures while preserving spatial layout information; therefore, we use SSIM for this evaluation. Additionally, \cite{shechtman2007matching} and \cite{tumanyan2022splicing} have demonstrated the effectiveness of self-similarity in maintaining structural integrity. Consequently, we extract deep features using DINO~(\cite{caron2021emerging})  and utilize self-similarity as a structural representation. By computing the similarity between self-similar structures, we evaluate the structural similarity.

% \textbf{Perceptual Similarity.} 
% To evaluate perceptual similarity, we use the FID metric and employ pre-trained Squeeze networks to extract features, assessing the similarity between generated images and ground truth images. Additionally, for a fair comparison with prior works~(\cite{shi2022geometry, qian2023sat2density}), we also employed pre-trained AlexNet~(\cite{krizhevsky2017imagenet}) and Squeeze networks~(\cite{iandola2016squeezenet}) as the backbone for evaluation, similar to how they were utilized, to assess perceptual similarity.

% \textbf{Depth Similarity.} 
% Since satellite images contain depth information of ground scenes, evaluating the depth of the generated ground images is essential. Thanks to advancements in monocular depth estimation models, we can use DepthAnything~(\cite{yang2024depth}) to assess the depth differences between ground truth images and generated images, thereby evaluating whether the generated images accurately preserve the depth of the ground scene.

% \textbf{Semantic Similarity.}
% When evaluating the quality of generated images, we should place greater emphasis on high-level semantic features. Methods like DINO~(\cite{caron2021emerging}) and Segment Anything~(\cite{kirillov2023segment}), trained on large amounts of unlabeled or self-labeled data, can learn the universal features of images, better capturing image semantic consistency. Compared to traditional low-level metrics such as SSIM and PSNR, high-level semantic features are less sensitive to subtle image transformations (eg. brightness and contrast changes) and focus more on the content of the image. This approach can enhance the robustness of evaluations and better reflect the true quality of generated images.
\section{EXPERIMENTS}
\subsection{Implementation Details}
\textbf{Experimental setup.}
We take $256 \times 256$ satellite images as input to predict $128 \times 512$ ground images, following the same setup as in \cite{shi2022geometry} for fair comparison. Our model is finetuned based on the Stable Diffusion 1.5 model~(\cite{Rombach_2022_CVPR}), with the Cross-Attention of diffusion replaced by Geometric Cross-Attention, and satellite image conditions processed through a simple VIT network for feature extraction. During inference, we employ DDIM sampling with 50 sampling steps, applying the Homography Adjustment scheme in the first 40 sampling steps and Zero-Shot Environmental Control throughout the entire sampling process. In Geometric Cross-Attention, we utilize 8 sampling heights of [-3, -2, -1, 1, 2, 3, 4, 5]. This constitutes an empirical setup. Training process on three GPUs with batch size of 32 for 200 epochs.

% \begin{table}[t]
%   \centering
%   \small
%   \setlength{\abovecaptionskip}{0pt}
%   \setlength{\belowcaptionskip}{0pt}
%   \caption{\small Quantitative comparison with existing algorithms on CVUSA dataset. Best results are highlighted in orange, and second-best in blue.\label{tab:CVUSA_comp_sota}}
%   \setlength{\tabcolsep}{0.5mm}{
%   \begin{tabular}{c|cc|ccc|cc|ccc|c}
% \midrule
% \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Structural similarity} & \multicolumn{3}{c|}{Perceptual similarity}         & \multicolumn{2}{c|}{Semantic Similarity} & \multicolumn{3}{c|}{Pixel Similarity}            & \multirow{2}{*}{↓Depth} \\
%                         & ↑SSIM                                & ↓Self\_sim                           & ↓$P_{squeeze}$            & ↓$P_{alex}$               & ↓FID                     & ↓DINO                    & ↓SegAny                    & ↓RMSE                    & ↑PSNR          & ↑SD            &                         \\ \midrule
% Pix2Pix                 & 0.3185                               & 0.2062                               & 0.3468                    & 0.5084                    & 44.51                    & 5.241                    & 0.3847                     & 55.84                    & 13.20          & 12.08          & 21.85                   \\
% XFork                   & 0.2873                               & -                                    & -                         & -                         & -                        & -                        & -                          & -                        & -              & -              & -                       \\
% S2S                     & 0.3508                               & 0.1911                               & \cellcolor{blue!10}0.3218 & \cellcolor{blue!10}0.4831 & 29.49                    & 5.112                    & 0.3852                     & \cellcolor{blue!10}54.57 & 13.40          & \cellcolor{blue!10}12.30          & 21.05                   \\
% Sat2Density             & 0.3307                               & \cellcolor{blue!10}0.1856            & 0.3101                    & 0.4634                    & 47.85                    & \cellcolor{blue!10}4.945 & \cellcolor{blue!10}0.3763  & 54.23          & \cellcolor{blue!10}13.46 & 12.27          & \cellcolor{blue!10}19.83                   \\
% ControlNet              & 0.3238                               & 0.1950                               & 0.3335                    & 0.4925                    & \cellcolor{blue!10}22.55 & 5.207                    & 0.3816               & 58.25          & 12.83          & 12.08          & 21.78                   \\
% CrossDiff               & \cellcolor{blue!10}0.3710            & -                                    & -                         & -                         & 23.67                    & -                        & -                    & -              & 12.00          & -              & -                       \\
% Ours                    & \cellcolor{orange!15}\textbf{0.3753} & \cellcolor{orange!15}\textbf{0.1816} & \cellcolor{orange!15}\textbf{0.2950} & \cellcolor{orange!15}\textbf{0.4499} & \cellcolor{orange!15}\textbf{21.30} & \cellcolor{orange!15}\textbf{4.807} & \cellcolor{orange!15}\textbf{0.3600} & \cellcolor{orange!15}\textbf{52.92} & \cellcolor{orange!15}\textbf{13.67} & \cellcolor{orange!15}\textbf{12.33} & \cellcolor{orange!15}\textbf{19.58} \\ \midrule
% \end{tabular}}
%   \vspace{-2em}
%   \label{tab:sota-cvusa}
% \end{table}

\textbf{Datasets.}
We adopt three cross-view datasets: CVUSA~(\cite{zhai2017predicting}), KITTI~(\cite{geiger2013vision, shi2022beyond}), and VIGOR~(\cite{zhu2021vigor,lentsch2022slicematch}). These datasets comprise pairs of cross-view data, combining ground-level images with their corresponding satellites. The ground-level images in CVUSA and VIGOR are panoramic, while the ground-level images in KITTI have a limited horizontal field of view (HFoV). CVUSA comprises 35,532 pairs of satellite and street view images for training and 8,884 pairs for testing. Following the setup of the cross-view localization task~(\cite{shi2022beyond, xia2023convolutional}), KITTI includes 19,655 pairs in the training data and 3,773 pairs in the testing data. VIGOR gathers data from New York, Seattle, San Francisco, and Chicago, dividing the data from each city into 52,609 pairs for the training set and 52,605 pairs for the test set.
% \subsection{evaluation metrics}
% We observe that there are limited common areas between satellite images and ground truth images, due to inconsistencies such as differing weather conditions and seasons, which makes it challenging to synthesize target view images that perfectly match the ground reality. As a result, pixel-level metrics like RMSE and PSNR are inadequate for our task (\cite{zhang2018unreasonable, shi2022geometry}). Instead of focusing on color discrepancies between the generated street view images and the actual ground truth images, our emphasis should be on whether they represent the same locations. Therefore, we use structural similarity, perceptual similarity, and depth similarity for performance evaluation. Furthermore, since satellite images do not capture sky information, we advocate for cropping the sky portion during evaluation. For textual similarity, we utilize feature similarity based on CLIP-extracted features.
% \paragraph{Structural similarity.} 
% First, to assess structural similarity, we seek metrics that can disregard local textures while preserving spatial layout information; therefore, we use SSIM for this evaluation. Additionally, \cite{shechtman2007matching} and \cite{tumanyan2022splicing} have demonstrated the effectiveness of self-similarity in maintaining structural integrity. Consequently, we extract deep features using DINO and utilize self-similarity as a structural representation. By computing the similarity between self-similar structures, we evaluate the structural similarity.
% \paragraph{Perceptual similarity.} 
% To evaluate perceptual similarity, we use the FID metric and employ pre-trained Squeeze networks to extract features, assessing the similarity between generated images and ground truth images. **squeeze/alex**
% \paragraph{Depth similarity.} 
% Since satellite images contain depth information of ground scenes, evaluating the depth of the generated ground images is essential. Thanks to advancements in monocular depth estimation models, we can use DepthAnything \cite{yang2024depth} to assess the depth differences between ground truth images and generated images, thereby evaluating whether the generated images accurately preserve the depth of the ground scene.
\begin{figure*}[t]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \begin{subfigure}{0.07\linewidth}
      \centering
      \includegraphics[width=\linewidth]{image/vigor_comp/7/sat.png} 
      \includegraphics[width=\linewidth]{image/vigor_comp/245/sat.png}
      \includegraphics[width=\linewidth]{image/vigor_comp/49/sat.png}
      \includegraphics[width=\linewidth]{image/vigor_comp/209/sat.png}
      \caption{\small Sat}
  \end{subfigure}
  \begin{subfigure}{0.174\linewidth}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/7/S2S.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/245/S2S.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/49/S2S.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/209/S2S.png} 
      \caption{\small S2S}
  \end{subfigure}
  \begin{subfigure}{0.174\linewidth}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/7/Sat2Den.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/245/Sat2Den.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/49/Sat2Den.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/209/Sat2Den.png} 
    \caption{\small Sat2Den}
  \end{subfigure}
  \begin{subfigure}{0.174\linewidth}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/7/controlnet.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/245/controlnet.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/49/controlnet.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/209/controlnet.png} 
    \caption{\small ControlNet}
  \end{subfigure}
  \begin{subfigure}{0.174\linewidth}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/7/Ours.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/245/Ours.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/49/Ours.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/209/Ours.png} 
    \caption{\small Ours}
  \end{subfigure}
  \begin{subfigure}{0.174\linewidth}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/7/gt.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/245/gt.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/49/gt.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/vigor_comp/209/gt.png} 
    \caption{\small GT}
  \end{subfigure}
  \caption{\small Qualitative comparison with previous work on the VIGOR dataset, our model can effectively capture road surface information from satellite images, resulting in clearer and more distinct road lines.}
  \vspace{-1em}
\label{fig:VIGOR_com}
\end{figure*}

% \begin{table}[t]
%   \centering
%   \small
%   \setlength{\abovecaptionskip}{0pt}
%   \setlength{\belowcaptionskip}{0pt}
%   \caption{\small Quantitative comparison with existing algorithms on VIGOR dataset. Best results are highlighted in orange, and second-best in blue.\label{tab:VIGOR_comp_sota}}
%   \setlength{\tabcolsep}{0.5mm}{
%   \begin{tabular}{c|cc|ccc|cc|ccc|c}
% \midrule
% \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Structural similarity} & \multicolumn{3}{c|}{Perceptual similarity}         & \multicolumn{2}{c|}{Semantic Similarity} & \multicolumn{3}{c|}{Pixel Similarity}            & \multirow{2}{*}{↓Depth} \\
%                         & ↑SSIM                & ↓Self\_sim          & ↓$P_{squeeze}$  & ↓$P_{alex}$     & ↓FID           & ↓DINO               & ↓SegAny              & ↓RMSE          & ↑PSNR          & ↑SD            &                         \\ \midrule
% Pix2Pix                 & 0.3714               & 0.1709              & 0.3346          & 0.4513          & 67.96          & 4.717               & 0.3833               & 55.001         & 13.33          & \cellcolor{blue!10}12.93          & 8.647                   \\
% S2S                     & 0.3273               & 0.1833              & 0.3694          & 0.4941          & 121.1          & 5.032               & 0.4037               & 62.94          & 12.16          & 12.31          & 10.87                   \\
% Sat2Density             & \cellcolor{blue!10}0.3956 & \cellcolor{orange!15}\textbf{0.1555} & \cellcolor{blue!10}0.2828 & \cellcolor{blue!10}0.3898 & 54.49 & \cellcolor{blue!10}4.408 & \cellcolor{blue!10}0.3627 & \cellcolor{orange!15}\textbf{50.23} & \cellcolor{orange!15}\textbf{14.14} & 12.90          & \cellcolor{blue!10}8.054 \\
% ControlNet              & 0.3397               & 0.1699              & 0.3395          & 0.4594          & \cellcolor{orange!15}\textbf{23.68} & 4.950               & 0.3916               & 63.98          & 12.02          & 12.59          & 10.01                   \\
% Ours                    & \cellcolor{orange!15}\textbf{0.4228} & \cellcolor{blue!10}0.1573 & \cellcolor{orange!15}\textbf{0.2729} & \cellcolor{orange!15}\textbf{0.3770} & \cellcolor{blue!10}28.01 & \cellcolor{orange!15}\textbf{3.324} & \cellcolor{orange!15}\textbf{0.2693} & \cellcolor{blue!10}52.16 & \cellcolor{blue!10}13.80          & \cellcolor{orange!15}\textbf{13.07} & \cellcolor{orange!15}\textbf{7.095} \\ \midrule
% \end{tabular}
%     }
%   \vspace{-2em}
%   \label{tab:sota-vigor}
% \end{table}


\begin{table}[t]
  \centering
  \small
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \caption{\small Quantitative comparison with existing algorithms on VIGOR dataset. The best results are highlighted in orange and the second-best in blue.\label{tab:VIGOR_comp_sota}}
  \setlength{\tabcolsep}{0.5mm}{
  \begin{tabular}{c|cc|ccc|cc|ccc|c}
\midrule
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Structural similarity} & \multicolumn{3}{c|}{Perceptual similarity}         & \multicolumn{2}{c|}{Semantic Similarity} & \multicolumn{3}{c|}{Pixel Similarity}            & \multirow{2}{*}{↓Depth} \\
                        & ↑SSIM                & ↓Self\_sim          & ↓$P_{squeeze}$  & ↓$P_{alex}$     & ↓FID           & ↓DINO               & ↓SegAny              & ↓RMSE          & ↑PSNR          & ↑SD            &                         \\ \midrule
Pix2Pix                 & 0.37                 & 0.17                & 0.33            & 0.45            & 67.96          & 4.72                & 0.38                & 55.00          & 13.33          & \cellcolor{blue!10}12.93          & 8.65                    \\
S2S                     & 0.33                 & 0.18                & 0.37            & 0.49            & 121.10         & 5.03                & 0.40                & 62.94          & 12.16          & 12.31          & 10.87                   \\
Sat2Density             & \cellcolor{blue!10}0.40 & \cellcolor{orange!15}\textbf{0.16} & \cellcolor{blue!10}0.28 & \cellcolor{blue!10}0.39 & 54.49          & \cellcolor{blue!10}4.41 & \cellcolor{blue!10}0.36 & \cellcolor{orange!15}\textbf{50.23} & \cellcolor{orange!15}\textbf{14.14} & 12.90          & \cellcolor{blue!10}8.05  \\
ControlNet              & 0.34                 & 0.17                & 0.34            & 0.46            & \cellcolor{orange!15}\textbf{23.68} & 4.95                & 0.39                & 63.98          & 12.02          & 12.59          & 10.01                   \\
Ours                    & \cellcolor{orange!15}\textbf{0.42} & \cellcolor{blue!10}0.16 & \cellcolor{orange!15}\textbf{0.27} & \cellcolor{orange!15}\textbf{0.38} & \cellcolor{blue!10}28.01 & \cellcolor{orange!15}\textbf{3.32} & \cellcolor{orange!15}\textbf{0.27} & \cellcolor{blue!10}52.16 & \cellcolor{blue!10}13.80          & \cellcolor{orange!15}\textbf{13.07} & \cellcolor{orange!15}\textbf{7.10} \\ \midrule
\end{tabular}
    }
  \vspace{-2em}
  \label{tab:sota-vigor}
\end{table}

\subsection{Comparison with Existing Methods}
In this section, we compare our approach with previous ground map generation methods. Pix2Pix~(\cite{pix2pix2017}) and XFork~(\cite{regmi2018cross}) are GAN-based methods that extract implicit features from satellite images to generate ground images. S2S~(\cite{shi2022geometry}) and Sat2Den~(\cite{qian2023sat2density}) introduce explicit geometric information into the network and project satellite images to ground view based on height priors. CrossDiff~(\cite{li2024crossviewdiff}) is a diffusion method that relies on high prior knowledge. Furthermore, noticing the robust capabilities of ControlNet~(\cite{zhang2023adding}), we also compared it with our method. The implementation of ControlNet mirrors the successful case~(\cite{sastry2024geosynth}). ControlNet receives textual conditions and ground map contour conditions. The textual conditions are generated using LLAVA~(\cite{liu2024visual}) and randomly masked out with a probability of 0.5 during training. The ground map contour conditions are derived from satellite maps projected based on prior ground height assumptions.

Quantitative results, as shown in Table \ref{tab:CVUSA_comp_sota} and Table \ref{tab:VIGOR_comp_sota}, demonstrate that the quality of ground image generation produced by our method significantly surpasses that of other approaches, particularly in maintaining geometric consistency. This performance stems from the Cross-View Conditioning Mechanism we employ and the Iterative Pose Alignment conducted during inference. In the visual representation, as depicted in Fig.~\ref{fig:CVUSA_com}, our method adheres to the geometric cues from satellite images, generating ground images that align with the scenes and outperform previous algorithms in representing pathways. While ControlNet outperforms us in the FID metric on the VIGOR dataset, we note that its other quantitative metrics are notably lower, indicating that ControlNet accurately captures dataset distributions but struggles to faithfully translate satellite image hints into improved ground image generation, a point supported by Fig.~\ref{fig:VIGOR_com}. On the VIGOR dataset, our method excels in ground feature identification and architectural representation compared to other algorithms.
\begin{figure*}[t]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \begin{subfigure}{0.112\linewidth}
      \centering
      \includegraphics[width=\linewidth]{image/correct/KITTI/sat_data/2011_09_26_2011_09_26_drive_0009_sync_0000000299.png} 
      \includegraphics[width=\linewidth]{image/correct/KITTI/sat_data/2011_09_30_2011_09_30_drive_0018_sync_0000001437.png}
      \includegraphics[width=\linewidth]{image/correct/CVUSA/sat/0002167.png}
      \includegraphics[width=\linewidth]{image/correct/CVUSA/sat/0004912.png}
      \caption{\small Sat Map}
  \end{subfigure}
  \begin{subfigure}{0.28\linewidth}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/73/pre__2011_09_26_2011_09_26_drive_0009_sync_0000000299.png.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/736/pre__2011_09_30_2011_09_30_drive_0018_sync_0000001437.png.png}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/436/pre_0002167.png}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/1016/pre_0004912.png} 
      \caption{\small LDM}
  \end{subfigure}
  \begin{subfigure}{0.28\linewidth}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/73/gt__2011_09_26_2011_09_26_drive_0009_sync_0000000299.png.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/736/gt__2011_09_30_2011_09_30_drive_0018_sync_0000001437.png.png}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/436/gt_0002167.png}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/1016/gt_0004912.png}   
      \caption{\small GT}
  \end{subfigure}
  \begin{subfigure}{0.28\linewidth}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/73/correct__2011_09_26_2011_09_26_drive_0009_sync_0000000299.png.png}    
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/736/correct__2011_09_30_2011_09_30_drive_0018_sync_0000001437.png.png}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/436/correct_0002167.png}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/1016/correct_0004912.png} 
      \caption{\small LDM W.IHA}
  \end{subfigure}
  \caption{\small Qualitative ablation experiments of IHA on KITTI and CVUSA datasets. IHA can correct pose offsets in the inference process effectively.}
\label{fig:pose_correct}
\vspace{-1em}
\end{figure*}

% \begin{table}[t]
%   \centering
%   \small
%   \setlength{\abovecaptionskip}{0pt}
%   \setlength{\belowcaptionskip}{0pt}
%   \caption{\small Ablation study on the KITTI dataset. We compared the positional accuracy of generated images using a cross-view localization model. \label{tab:KITTI_pose}}
%   \centering
%   \setlength{\tabcolsep}{0.5mm}{
%   \begin{tabular}{c|cc|cc|cc|cc|cc}
% \midrule
% \multirow{2}{*}{} & \multicolumn{2}{c|}{Distance}   & \multicolumn{2}{c|}{Angle}     & \multicolumn{2}{c|}{Lateral}    & \multicolumn{2}{c|}{Longitudinal} & \multicolumn{2}{c}{Azimuth}     \\
%                   & ↓average       & ↓median        & ↓average       & ↓median       & ↑d=1           & ↑d=3           & ↑d=1            & ↑d=3            & ↑$\theta$=1           & ↑$\theta$=3           \\ \midrule
% LDM               & 10.74          & 6.924          & 17.60          & 6.641         & 43.97          & 67.74          & 15.35           & 36.79           & 7.872          & 24.04          \\
% LDM W. IHA        & 10.67          & 6.85           & 17.46          & 6.727         & 44.08          & 67.77          & 15.77           & 36.95           & 8.084          & 23.78          \\
% LDM W.GCA W.IHA   & \textbf{10.51} & \textbf{6.661} & \textbf{17.14} & \textbf{6.64} & \textbf{45.40} & \textbf{68.67} & \textbf{16.78}  & \textbf{38.22}  & \textbf{8.307} & \textbf{24.22} \\ \midrule
% \end{tabular}
% }
% \vspace{-2em}
% \end{table}
\begin{table}[t]
  \centering
  \small
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \caption{\small Ablation study on the KITTI dataset. We compared the positional accuracy of generated images using a cross-view localization model. \label{tab:KITTI_pose}}
  \centering
  \setlength{\tabcolsep}{0.5mm}{
  \begin{tabular}{c|cc|cc|cc|cc|cc}
\midrule
\multirow{2}{*}{} & \multicolumn{2}{c|}{Distance}   & \multicolumn{2}{c|}{Angle}     & \multicolumn{2}{c|}{Lateral}    & \multicolumn{2}{c|}{Longitudinal} & \multicolumn{2}{c}{Azimuth}     \\
                  & ↓average       & ↓median        & ↓average       & ↓median       & ↑d=1           & ↑d=3           & ↑d=1            & ↑d=3            & ↑$\theta$=1           & ↑$\theta$=3           \\ \midrule
LDM               & 10.74          & 6.92           & 17.60          & 6.64          & 43.97          & 67.74          & 15.35           & 36.79           & 7.87           & 24.04          \\
LDM W. IHA        & 10.67          & 6.85           & 17.46          & 6.73          & 44.08          & 67.77          & 15.77           & 36.95           & 8.08           & 23.78          \\
LDM W.GCA W.IHA   & \textbf{10.51} & \textbf{6.66}  & \textbf{17.14} & \textbf{6.64} & \textbf{45.40} & \textbf{68.67} & \textbf{16.78}  & \textbf{38.22}  & \textbf{8.31}  & \textbf{24.22} \\ \midrule
\end{tabular}
}
\vspace{-2em}
\end{table}
% \begin{figure*}[htbp]
%   \centering
%   \setlength{\abovecaptionskip}{0pt}
%   \setlength{\belowcaptionskip}{0pt}
%   \begin{subfigure}{1\linewidth}
%       \begin{minipage}{0.195\linewidth}
%           \includegraphics[width=\linewidth]{image/text/CVUSA_0044093/no_text.png}
%           \centering
%       \end{minipage}
%       \hfill
%       \begin{minipage}{0.195\linewidth}
%           \includegraphics[width=\linewidth]{image/text/CVUSA_0044093/at_sunrise.png}

%       \end{minipage}
%       \hfill
%       \begin{minipage}{0.195\linewidth}
%           \includegraphics[width=\linewidth]{image/text/CVUSA_0044093/cloudy_day.png}

%       \end{minipage}
%       \hfill
%       \begin{minipage}{0.195\linewidth}
%           \includegraphics[width=\linewidth]{image/text/CVUSA_0044093/dense_fog.png}

%       \end{minipage}
%       \hfill
%       \begin{minipage}{0.195\linewidth}
%           \includegraphics[width=\linewidth]{image/text/CVUSA_0044093/heavy_snow.png}

%       \end{minipage}
%       % \caption{}
%   \end{subfigure}
%   \\
%   \begin{subfigure}{1\linewidth}
%     \begin{minipage}{0.195\linewidth}
%         \includegraphics[width=\linewidth]{image/text/CVUSA_0044093/no_text.png}
%         \centering
%         {\footnotesize ori image} 
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.195\linewidth}
%         \includegraphics[width=\linewidth]{image/text/CVUSA_0044093/at_sunrise.png}
%         \centering{\footnotesize at sunrise}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.195\linewidth}
%         \includegraphics[width=\linewidth]{image/text/CVUSA_0044093/cloudy_day.png}
%         \centering{\footnotesize cloudy day}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.195\linewidth}
%         \includegraphics[width=\linewidth]{image/text/CVUSA_0044093/dense_fog.png}
%         \centering{\footnotesize dense fog}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.195\linewidth}
%         \includegraphics[width=\linewidth]{image/text/CVUSA_0044093/heavy_snow.png}
%         \centering{\footnotesize heavy snow}
%     \end{minipage}
%     % \caption{}
% \end{subfigure}
%   \caption{Images under various textual contextual constraints.}
%   \label{fig:text_condition}
% \end{figure*}
\subsection{The Effectiveness of Pose Alignment\label{exp:pose}}
Compared to other datasets, KITTI's data is collected along the same route and divided into training and test sets. Due to the high similarity in satellite image conditions between adjacent points along the path, the implicit condition diffusion model fails to distinguish effectively, leading to the generation of ground images with positional offsets quite easily.
To demonstrate the effectiveness of the proposed pose alignment approaches in this paper, we employ the original implicit diffusion model as our baseline and gradually add the proposed modules, i.e., the IHA and the GCA. 
% The results are presented in Table~\ref{tab:KITTI_pose}.
% Since we target evaluating the pose alignment performance, in addition to the image-level similarities, we also employ a cross-view localization model, CCVPE~\cite{xia2023convolutional}, to compute the relative pose between the generated images with respect to the conditioning satellite images, and measure the pose consistency with the specified conditioning relative pose. 
% Hence, we selected the KITTI dataset to assess the effectiveness of our Pose Alignment. 
To better evaluate the results of Pose Alignment, we employ CCVPE~(\cite{xia2023convolutional}), a powerful cross-view pose estimation method, to evaluate the consistency of the relative pose between generated images and the satellite images with respect to the conditioning pose, in addition to image-level similarities. 
The quantitative comparison is shown in Table \ref{tab:KITTI_pose}. It can be seen that both IHA and GCA improve the generated image quality in terms of image-level similarity and pose consistency. 
% that after Pose alignment, there is a significant improvement in image quality and enhancements in both pose and rotation accuracy. 

Fig.~\ref{fig:pose_correct} provides examples of Pose Alignment on KITTI and CVUSA datasets. Both the images before and after correction are generated with the same noise, the only difference being whether Pose Alignment is enabled. We can observe the significant role played by Pose Alignment in successfully correcting pose deformities in the generated results. Furthermore, the method is equally applicable to both FoV images and panoramic images, demonstrating a high level of universality.

\begin{figure}[t]\label{Figure:text_comp}
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \includegraphics[width=1\textwidth]{./image/text/text_comp2.pdf}
  \caption{\small Qualitative comparison with ControlNet on CVUSA. ControlNet trained under weak textual conditions fails to effectively adjust image information based on text prompts. Our Zero-shot Environmental Control can adjust image information based on text prompts while preserving the spatial structure of the image.  \label{Figure:text_comp}}
  \vspace{-1em}
\end{figure}

\subsection{The Effectiveness of Text-guided Environmental Control\label{sec:text}}
To evaluate the effectiveness of the Text-guided Environmental Control mechanism, we conduct comparative experiments with LDM and ControlNet on the CVUSA dataset, generating images representing the four seasons: spring, summer, autumn, and winter. Notably, our environmental control mechanism is applied during inference in a zero-shot manner, while for LDM and ControlNet, the environmental control is incorporated as an additional text condition embedded within the denoising network. To train ControlNet's text condition branch, we used LLAVA to generate text annotations for the ground images.

As shown in Table \ref{tab:text_condition}, the quantitative results indicate that our method produces outputs that align more effectively with the provided textual prompts. One challenge we encountered was the inconsistency in the quality of LLAVA-generated text descriptions, as they were produced without manual annotations. During ControlNet's training, the model tended to prioritize well-expressed satellite image prompts, diminishing the impact of textual prompts over time. This behavior led to ControlNet’s reduced sensitivity to diverse text prompts, as illustrated in Fig.~\ref{Figure:text_comp}. In contrast, our Zero-shot Environmental Control (ZoEC) mechanism exhibited greater robustness in handling unsupervised scenarios, maintaining stronger adherence to the given text conditions.

% To validate the role of the Text-guided Environmental Control mechanism, we conducted comparative experiments with ControlNet on the CVUSA dataset, generating images for the four seasons of spring, summer, autumn, and winter. 
% Note that the environmental control in our method is added in the inference step in a zero-shot manner. 
% For ControlNet, the environmental control is embedded as an additional text condition in the denoising network. 
% To train the text condition branch, we generate text annotations for the ground images using LLAVA. 
% The quantitative results, as shown in Table \ref{tab:text_condition}, indicate that our generated outputs better align with the provided textual prompts. We have noticed that due to our text prompts being generated solely by LLAVA without annotations, the quality of these textual descriptions is inconsistent. During the training of ControlNet, a tendency develops to assign higher weights to well-expressed satellite image prompts, leading to a gradual decrease in the importance of textual prompts within the model. This is further evidenced by ControlNet's insensitivity to various text prompts as illustrated in Fig,~\ref{Figure:text_comp}. In contrast, our ZoEC mechanism demonstrates better handling of such unsupervised scenarios.

% \begin{wraptable}[9]{r}[0cm]{0pt}
%   \centering
%   \small
%   \vspace{-2em}
%   % \textbf{Evaluating the effectiveness of Pose Alignment on the KITTI Dataset}
%   \begin{tabular}{l|l}
%   \midrule
%              & Clip Score      \\ \midrule
%   ControlNet & 0.2047          \\
%   Ours       & \textbf{0.2401} \\ \midrule
%   \end{tabular}
%   \captionof{tabular}{\small The effectiveness of the textual condition.
%   }
%   \vspace{-1em}
%   \label{tab:text_condition}
% \end{wraptable}

% \begin{table}[t]
% \centering
%   \small
%   \setlength{\abovecaptionskip}{0pt}
%   \setlength{\belowcaptionskip}{0pt}
%   \caption{\small The effectiveness of the textual condition.} \label{tab:text_condition}
%   \centering
% \begin{tabular}{c|cccc|cccc}
% \midrule
%            & \multicolumn{4}{c|}{Clip Score}                                       & \multicolumn{4}{c}{Blip Score}                                    \\
%            & spring          & summer          & autumn          & winter          & spring         & summer         & autumn         & winter         \\ \midrule
% LDM        & 0.2108          & 0.2096          & 0.1993          & 0.8100          & 5.144          & 60.87          & 7.440          & 1.778          \\
% ControlNet & 0.2097          & 0.2120          & 0.1997          & 0.1973          & 12.89          & 44.63          & 5.504          & 13.06          \\
% Ours       & \textbf{0.2551} & \textbf{0.2387} & \textbf{0.2352} & \textbf{0.2312} & \textbf{69.74} & \textbf{82.05} & \textbf{78.62} & \textbf{47.67} \\ \midrule
% \end{tabular}
% \vspace{-2em}
% \end{table}

\begin{table}[t]
\centering
  \small
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \caption{\small The similarity between generated images and environmental text prompt.} \label{tab:text_condition}
  \centering
\begin{tabular}{c|cccc|cccc}
\midrule
           & \multicolumn{4}{c|}{Clip Score}                                       & \multicolumn{4}{c}{Blip Score}                                    \\
           & spring          & summer          & autumn          & winter          & spring         & summer         & autumn         & winter         \\ \midrule
LDM        & 0.21            & 0.21            & 0.20            & 0.81            & 5.14           & 60.87          & 7.44           & 1.78           \\
ControlNet & 0.21            & 0.21            & 0.20            & 0.20            & 12.89          & 44.63          & 5.50           & 13.06          \\
Ours       & \textbf{0.26}   & \textbf{0.24}   & \textbf{0.24}   & \textbf{0.23}   & \textbf{69.74} & \textbf{82.05} & \textbf{78.62} & \textbf{47.67} \\ \midrule
\end{tabular}
\vspace{-2em}
\end{table}

\section{Conclusion}
In this paper, we have presented a novel approach for generating ground-level images from satellite imagery, addressing the dual challenges of geometric alignment and environmental diversity. Our method introduced two key innovations: the Iterative Homography Adjustment (IHA) mechanism, which ensures accurate pose alignment between the satellite and generated ground-level views, and the Text-guided Zero-shot Environmental Control (ZoEC), which allows flexible control over lighting, weather, and seasonal variations without requiring additional training data. By incorporating geometric cross-attention in the diffusion process, we further improved the consistency between satellite and street-level views. Extensive experiments demonstrated that our method outperforms existing approaches in both geometric accuracy and environmental adaptability. 

% In this article, we propose a novel framework for generating ground maps based on satellite images. To maintain geometric consistency between the generated results and satellite images, we introduce a geometry-aware attention mechanism to aggregate satellite features. Furthermore, we address pose offset issues by supervising ground map generation through contrasting ground maps with satellite images during the inference process. Additionally, we enhance diversity in model generation by introducing text guidance during inference. Lastly, due to the representation differences in cross-view scenarios where information such as the sky is not captured in satellite images, we introduce a new evaluation system and conduct thorough experiments using this evaluation framework.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}
\newpage
\appendix
\section{Appendix}
\subsection{The robustness of IHA in the face of noise.}
\begin{figure}[htbp]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \includegraphics[width=1\textwidth]{./image/rebuttal/Noise_IHA.pdf}
  \caption{\small In the first row, we demonstrate that ground images generated from satellite maps exhibit varying offsets under different initial noise conditions. The second row illustrates the pose correction results of the first row using the Iterative Homography Adjustment (IHA).  \label{Figure:different_noise}}
\end{figure}
As shown in Fig.~\ref{Figure:different_noise}, when using different initial noises, the reverse diffusion process often produces unpredictable outcomes. In the first row, diverse noises result in different view perspectives. However, the IHA module adeptly corrects these deviations and gets consistent results. This case shows the remarkable robustness of IHA for identifying and rectifying the accurate pose of generated ground images.

\subsection{Data Augmentation for Cross-View Localization}
\begin{table}[h]
  \centering
  \small
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \caption{\small Using generated street data for data augmentation. We report the localization accuracy of \cite{shi2025weakly} on the KITTI dataset. \label{tab:KITTI_data_agu}}
  \centering
  \setlength{\tabcolsep}{0.5mm}{
\begin{tabular}{cccccccccccc}
\midrule
\multicolumn{12}{c}{Test1}                                                                                                                                                                                                                                                                                                                                           \\ \midrule
\multicolumn{2}{c|}{\multirow{2}{*}{}}                                                 & \multicolumn{2}{c|}{Distance}                                      & \multicolumn{2}{c|}{Angle}                             & \multicolumn{2}{c|}{Lateral}                         & \multicolumn{2}{c|}{Longitudinal}                    & \multicolumn{2}{c}{Azimuth}     \\
\multicolumn{2}{c|}{}                                                                  & \multicolumn{1}{c}{↓average} & \multicolumn{1}{c|}{↓median}        & ↓average        & \multicolumn{1}{c|}{↓median}         & ↑d=1           & \multicolumn{1}{c|}{↑d=3}           & ↑d=1           & \multicolumn{1}{c|}{↑d=3}           & ↑$\theta$=1    & ↑$\theta$=3    \\ \midrule
\multicolumn{2}{c|}{Wo.augmentation}                                                   & 11.11                        & \multicolumn{1}{c|}{7.646}          & 0.1811          & \multicolumn{1}{c|}{0.1492}          & 56.98          & \multicolumn{1}{c|}{87.23}          & 10.68          & \multicolumn{1}{c|}{31.57}          & \textbf{99.66} & \textbf{100.0} \\ \midrule
\multicolumn{1}{c|}{\multirow{3}{*}{W.augmentation}} & \multicolumn{1}{c|}{Pix2Pix}    & 11.72                        & \multicolumn{1}{c|}{9.020}          & \textbf{0.1810} & \multicolumn{1}{c|}{\textbf{0.1491}} & 54.31          & \multicolumn{1}{c|}{86.03}          & 9.833          & \multicolumn{1}{c|}{27.54}          & \textbf{99.66} & \textbf{100.0} \\
\multicolumn{1}{c|}{}                                & \multicolumn{1}{c|}{ControlNet} & 11.15                        & \multicolumn{1}{c|}{7.688}          & \textbf{0.1810} & \multicolumn{1}{c|}{\textbf{0.1491}} & 54.17          & \multicolumn{1}{c|}{87.36}          & 11.61          & \multicolumn{1}{c|}{31.27}          & \textbf{99.66} & \textbf{100.0} \\
\multicolumn{1}{c|}{}                                & \multicolumn{1}{c|}{Ours}       & \textbf{10.88}               & \multicolumn{1}{c|}{\textbf{7.167}} & \textbf{0.1810} & \multicolumn{1}{c|}{\textbf{0.1491}} & \textbf{57.57} & \multicolumn{1}{c|}{\textbf{87.70}} & \textbf{11.95} & \multicolumn{1}{c|}{\textbf{32.65}} & \textbf{99.66} & \textbf{100.0} \\ \midrule
\multicolumn{12}{c}{Test2}                                                                                                                                                                                                                                                                                                                                           \\ \midrule
\multicolumn{2}{c|}{\multirow{2}{*}{}}                                                 & \multicolumn{2}{c|}{Distance}                                      & \multicolumn{2}{c|}{Angle}                             & \multicolumn{2}{c|}{Lateral}                         & \multicolumn{2}{c|}{Longitudinal}                    & \multicolumn{2}{c}{Azimuth}     \\
\multicolumn{2}{c|}{}                                                                  & \multicolumn{1}{c}{↓average} & \multicolumn{1}{c|}{↓median}        & ↓average        & \multicolumn{1}{c|}{↓median}         & ↑d=1           & \multicolumn{1}{c|}{↑d=3}           & ↑d=1           & \multicolumn{1}{c|}{↑d=3}           & ↑$\theta$=1    & ↑$\theta$=3    \\ \midrule
\multicolumn{2}{c|}{Wo.augmentation}                                                   & 14.07                       & \multicolumn{1}{c|}{10.61}          & 0.1570          & \multicolumn{1}{c|}{\textbf{0.1305}} & 60.32          & \multicolumn{1}{c|}{84.90}          & 8.632          & \multicolumn{1}{c|}{25.14}          & \textbf{100.0} & \textbf{100.0} \\ \midrule
\multicolumn{1}{c|}{\multirow{3}{*}{W.augmentation}} & \multicolumn{1}{c|}{Pix2Pix}    & 14.44                        & \multicolumn{1}{c|}{11.29}          & \textbf{0.1569} & \multicolumn{1}{c|}{\textbf{0.1305}} & 57.44          & \multicolumn{1}{c|}{80.63}          & 8.101          & \multicolumn{1}{c|}{23.89}          & \textbf{100.0} & \textbf{100.0} \\
\multicolumn{1}{c|}{}                                & \multicolumn{1}{c|}{ControlNet} & 14.31                        & \multicolumn{1}{c|}{10.91}          & \textbf{0.1569} & \multicolumn{1}{c|}{\textbf{0.1305}} & 55.99          & \multicolumn{1}{c|}{82.39}          & 8.260          & \multicolumn{1}{c|}{25.05}          & \textbf{100.0} & \textbf{100.0} \\
\multicolumn{1}{c|}{}                                & \multicolumn{1}{c|}{Ours}       & \textbf{13.94}               & \multicolumn{1}{c|}{\textbf{9.885}} & \textbf{0.1569} & \multicolumn{1}{c|}{\textbf{0.1305}} & \textbf{62.87} & \multicolumn{1}{c|}{\textbf{85.51}} & \textbf{8.923} & \multicolumn{1}{c|}{\textbf{26.92}} & \textbf{100.0} & \textbf{100.0} \\ \midrule
\end{tabular}
}
\end{table}

The cross-view positioning algorithm has achieved a high level of accuracy after a long period of development~(\cite{shi2019spatial, shi2020looking, shi2020optimal, shi2022accurate, shi2022cvlnet, shi2023boosting, song2024learning, xia2022visual, xia2025adapting}). To verify that the proposed generative model can assist in vehicle localization in autonomous driving scenarios, we utilize generated street data for data augmentation in training a localization model. The experiments are conducted on the KITTI dataset (\cite{geiger2013vision, shi2022beyond}). \cite{geiger2013vision} provides ground images captured by vehicles, while \cite{shi2022beyond} collects corresponding satellite images for each ground image. The dataset is divided into Training, Test1, and Test2 subsets. The images in Test1 come from the same area as the images in the training set, while the images in Test2 come from different areas. In our data augmentation experiment, we use a ground map resolution of 128x512 and a satellite map resolution of 512x512. For each satellite image in the training set of the KITTI dataset, we utilize the proposed framework to generate street scene images under various season conditions. During the training of the cross-view localization algorithm(\cite{shi2025weakly}), both generated and real data are employed with a 50\% probability each to augment the KITTI dataset. 
In Table~\ref{tab:KITTI_data_agu}, we present the localization accuracy with and without the data augmentation. Both of them are trained for 15 epochs. In the W.augmentation experiments, we compare the data generated by Pix2Pix, ControlNet, and our own method. The data augmentation with our generated data significantly improves cross-view localization performance, demonstrating the usefulness of our approach to autonomous driving tasks. 
Compared to our generation strategy method, the data generated by GAN-based Pix2Pix is very blurry. Adding the generated data from Pix2Pix actually deteriorates the model's perception of details. The data generated by ControlNet, although clear, lacks strong positional constraints. The pose offset leads to incorrect matches in the localization model during training. Compared to other methods, our approach can ensure geometrically consistent generation and text-guided diverse environment creation, thereby achieving the purpose of data augmentation and yielding outstanding results.

% The geometric alignment between the ground and satellite images is crucial for the cross-view localization(\cite{shi2025weakly}) task. To verify the pose alignment effect of IHA, we use generated street data for data augmentation to train a localization model. Specifically, for each satellite image in the training set of the KITTI dataset, we utilize the proposed framework to generate street scene images under various season conditions. During the training of the cross-view localization algorithm(\cite{shi2025weakly}), the generated ground images are employed to augment the KITTI dataset. In Table~\ref{tab:KITTI_data_agu}, we present the localization accuracy with (second row) and without (first row) the data augmentation. The data augmentation significantly improves cross-view localization performance, demonstrating precise pose alignment between the generated ground images and the corresponding satellite imagery.
% The geometric alignment between the ground and satellite images is crucial for the cross-view localization(\cite{shi2025weakly}) task. To verify the pose alignment effect of IHA, we use generated street data for data augmentation to train a localization model. Specifically, for each satellite image in the training set of the KITTI dataset, we utilize the proposed framework to generate street scene images under various season conditions. During the training of the cross-view localization algorithm(\cite{shi2025weakly}), the generated ground images are employed to augment the KITTI dataset. In Table~\ref{tab:KITTI_data_agu}, we present the localization accuracy with (second row) and without (first row) the data augmentation. The data augmentation significantly improves cross-view localization performance, demonstrating precise pose alignment between the generated ground images and the corresponding satellite imagery.

\subsection{More analysis of the IHA.}
\subsubsection{The homography transformation in IHA.}
\begin{figure*}[h]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \includegraphics[width=1\textwidth]{./image/rebuttal/IHA1.pdf}
    \caption{\small The relationship between Homography Adjustment and the camera position in satellite imagery. The corresponding satellite position and ground images are labeled with arrows and borders using the same colors.\label{Hom_pp}}
    \vspace{-1em}
\end{figure*}
The Homography transformation is defined by a 3x3 matrix and can map a plane in an image to another plane. It allows for image manipulations including rotation, translation, scaling, shearing, and perspective transformation. As depicted in Fig.~\ref{Hom_pp}, adjusting the ground map through Homography enables correspondence with different perspectives in a satellite image. Translating the ground image corresponds to perpendicular movements to the satellite image, scaling represents horizontal (front-back) movements within the satellite image, and the projection transformation signifies changes in yaw angle.
\begin{figure*}[h]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \includegraphics[width=1\textwidth]{./image/rebuttal/IHA2_3.pdf}
    \caption{\small The Homography Adjustment operation in the DDIM process. The visual results of each step stem from $\mathcal{D}(z_{t,0})$}.\label{Hom_iter}
    \vspace{-1em}
\end{figure*}

Due to variations in initial noise and uncertainties in the implicit learning of features, diffusion does not always yield favorable outcomes.  As depicted in Fig.~\ref{fig:pose_correct}, many ground images may become distorted, indicating a displacement in the corresponding satellite image coordinates. Motivated by this, we employ satellite image-guided supervision in the DDIM generation process. Within the DDIM process, we utilize satellite image conditions, the result $z_{t-1}$ at step $t$, and the expected location $R$, $T$ of the ground image in the satellite image. We supervise the positioning algorithm illustrated in Fig.~\ref{Figure:H_adj} to adjust the location of the generated $z_{t-1}$ based on the Homography matrix for the ground image. Given the substantial difficulty in achieving correct results through a single Homography Adjustment, we adopt an iterative approach, progressively executing Algorithm \ref{Alg} to guide the ground image pixels toward the correct coordinates. In Fig.~\ref{Hom_iter}, we illustrate the intermediate results and step-by-step changes in the denoising process using IHA.
\subsubsection{The computational workload in IHA}
\begin{table}[h]
\centering
  \small
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  % \setlength{\tabcolsep}{5pt}
  \caption{\small The analysis of computational efficiency of IHA.\label{tab:VIGOR_common}}
    \centering
\begin{tabular}{c|cc}
\midrule
       & Memory  & Time Cost \\ \midrule
Wo.IHA & 20126MB & 5.406s    \\
W.IHA  & 21022MB & 5.513s    \\ \midrule
\end{tabular}
\end{table}
The IHA operates in the latent space using low-resolution feature maps. Since the IHA calculation relies on the pixel coordinates, it does not impose a significant computational increase. We conduct the efficiency tests with a batch size of 1, and the results are presented in Table~\ref{tab:VIGOR_common}. When comparing memory usage, the baseline model without IHA (Wo.IHA) requires 20,126MB, while the inclusion of IHA (W.IHA) increases memory usage slightly to 21,022MB (896MB increase) due to the addition of a lightweight localization network. In terms of time cost, the baseline model (Wo.IHA) requires 5.406 seconds per image, while the model with IHA (W.IHA) increases this slightly to 5.513 seconds per image—an additional cost of only 0.107 seconds per image. We believe that IHA strikes a favorable balance between performance and computational efficiency, offering promising practical value. 
% The IHA operates in the latent space using low-resolution feature maps. Since the IHA calculation relies on the pixel coordinates, it does not impose a significant computational increase. We conduct the efficiency tests with a batch size of 1, and the results are presented in Table~\ref{tab:VIGOR_common}. It is revealed that the GPU memory usage of the model without IHA is 19942MB; due to the inclusion of the localization network, the GPU memory consumption in Wo.IHA's model increases slightly to 20838MB. During inference, the GPU memory consumption for both models is 184MB. In terms of inference time, Wo.IHA processes one image in 5.406 seconds, while W.IHA processes one image in 5.513 seconds.
\subsection{derivation of formulae.}\label{conv_formulae}
Based on the score-based formulation of a diffusion model\cite{song2020score}, and the introduction of conditional guidance $g_{pose},g_{text}$, our objective is to learn 
\begin{equation}
    \begin{aligned}
    \label{eq:11}
       \hat{\epsilon_t} = -\sqrt{1-\bar\alpha_{t}}\nabla_{z_{t}}\log p(z_{t}|g_{pose},g_{text})
    \end{aligned}
\end{equation}
% By leveraging the reasoning within \cite{dhariwal2021diffusion} from Ep. 55-59, we deduce the following derivation:
Using the Bayes' formula, we obtain:
% \begin{equation}
%     \begin{aligned}
%        p(z_{t-1}|z_{t},g_{pose},g_{text}) 
%        &= \frac{p(z_{t-1},z_{t},g_{pose},g_{text})}{p(z_{t},g_{pose},g_{text})} \\
%        &= \frac{p(z_{t-1},z_{t},g_{pose},g_{text})}{p(g_{pose},g_{text}|z_{t})p(z_{t})} \\
%        &= \frac{p(z_{t-1}|z_t)p(g_{pose},g_{text}|z_{t-1},z_t)p(z_t)}{p(g_{pose},g_{text}|z_t)p(z_t)} \\
%        &= \frac{p(z_{t-1}|z_t)p(g_{pose},g_{text}|z_{t-1},z_{t})}{p(g_{pose},g_{text}|z_t)} \\
%        &= \frac{p(z_{t-1}|z_{t})p(g_{pose},g_{text}|z_{t-1})} {p(g_{pose},g_{text}|z_{t})}
%     \end{aligned}
% \end{equation}
\begin{equation}
    \begin{aligned}
       p(z_{t}|g_{pose},g_{text}) 
       &= \frac{p(g_{pose},g_{text}|z_{t})p(z_{t})}{p(g_{pose},g_{text})} 
    \end{aligned}
\end{equation}
In the formula, $p(g_{pose},g_{text})$ can be considered a constant, and we denote it as $C$. Utilizing the independence of environmental and positional conditions, we express the formula as:
\begin{equation}
    \label{eq:13}
    \begin{aligned}
       p(z_{t-1}|g_{pose},g_{text}) 
       &= C{p(g_{pose},g_{text}|z_{t})p(z_{t})} \\
       &= C{p(g_{pose}|z_{t})p(g_{text}|z_{t})p(z_{t})}
    \end{aligned}
\end{equation}
We substitute Eq.\ref{eq:13} into Eq.\ref{eq:11} to obtain:
\begin{equation}
    \begin{aligned}
        \hat{\epsilon_t} &= -\sqrt{1-\bar\alpha_{t}}\nabla_{z_{t}}\log p(z_{t}|,g_{pose},g_{text}) \\
       &= -\sqrt{1-\bar\alpha_{t}}(\nabla_{z_{t}}\log p(z_{t}) + \nabla_{z_{t}}\log p(g_{pose}|z_{t}) + \nabla_{z_{t}}\log p(g_{text}|z_{t}))
    \end{aligned}
\end{equation}
From the score-based formula $\epsilon_t = -\sqrt{1-\bar\alpha_{t}}\nabla_{z_{t}}\log p(z_{t})$, we can derive:
\begin{equation}
    \begin{aligned}
        \hat{\epsilon_t} = \epsilon_t -\sqrt{1-\bar\alpha_{t}}\nabla_{z_{t}}\log p(g_{pose}|z_{t}) -\sqrt{1-\bar\alpha_{t}}\nabla_{z_{t}}\log p(g_{text}|z_{t})
    \end{aligned}
\end{equation}
Substituting the new $\hat{\epsilon_t}$ into Eq.~\ref{eq:3} in place of $\epsilon_{\theta}(z_t, t, c)$, we obtain:
\begin{equation}
    z_{t-1} = \mu_{\theta}(z_t, t, c) + \gamma \nabla_{z_{t}} \log p(g_{\text{text}}|z_{t})  + \lambda \nabla_{z_{t}} \log p(g_{\text{pose}}|z_{t})+ \sigma_t \varepsilon
\end{equation}
The hyperparameters $\gamma$ and $\lambda$ control the influence of pose and environmental conditioning, where
higher values strengthen the alignment with desired conditions.

% \subsection{Quantitative Results Based On More Common Evaluation Metrics.}
% \begin{table}[h]
% \centering
%   \small
%   \setlength{\abovecaptionskip}{0pt}
%   \setlength{\belowcaptionskip}{0pt}
%   % \setlength{\tabcolsep}{5pt}
%   \caption{\small Comparison of Common Metrics with Existing Algorithms on the CVUSA Dataset.\label{tab:CVUSA_common}}
%     \centering
%     \setlength{\tabcolsep}{2mm}{
% \begin{tabular}{c|cccccc}
% \midrule
%             & \multicolumn{1}{l}{↓RMSE} & \multicolumn{1}{l}{↑SSIM} & \multicolumn{1}{l}{↑PSNR} & \multicolumn{1}{l}{↑SD} & \multicolumn{1}{l}{↓P\_alex} & \multicolumn{1}{l}{↓P\_squeeze} \\ \midrule
% Pix2Pix     & 55.84                     & 0.3169                    & 13.20                     & 12.08                   & 0.4905                       & 0.3726                          \\
% S2S         & 54.57                     & 0.3508                    & 13.40                     & 12.30                   & 0.4666                       & 0.3520                          \\
% Sat2Density & 54.23                     & 0.3307                    & 13.46                     & 12.27                   & 0.4546                       & 0.3409                          \\
% ControlNet  & 58.25                     & 0.3222                    & 12.83                     & 12.08                   & 0.4872                       & 0.3668                          \\
% CrossDiff   & -                         & 0.3710                    & 12.00                    & -                       & -                            & -                               \\
% Ours        & \textbf{52.92}            & \textbf{0.3753}           & \textbf{13.67}            & \textbf{12.33}          & \textbf{0.4323}              & \textbf{0.3192}                 \\ \midrule
% \end{tabular}
% }
% \end{table}

% \begin{table}[h]
% \centering
%   \small
%   \setlength{\abovecaptionskip}{0pt}
%   \setlength{\belowcaptionskip}{0pt}
%   % \setlength{\tabcolsep}{5pt}
%   \caption{\small Comparison of Common Metrics with Existing Algorithms on the VIGOR Dataset.\label{tab:VIGOR_common}}
%     \centering
%     \setlength{\tabcolsep}{2mm}{
% \begin{tabular}{c|cccccc}
% \midrule
%             & ↓RMSE          & ↑SSIM           & ↑PSNR          & ↑SD            & ↓P\_alex        & ↓P\_squeeze     \\ \midrule
% Pix2Pix     & 55.001         & 0.3990          & 13.33          & 12.93          & 0.4240          & 0.3253          \\
% S2S         & 62.94          & 0.3274          & 12.16          & 12.31          & 0.4917          & 0.3904          \\
% Sat2Density & \textbf{50.23} & 0.3956          & \textbf{14.14} & 12.90          & 0.3753          & 0.2852          \\
% ControlNet  & 63.98          & 0.3397          & 12.0189        & 12.59          & 0.4724          & 0.3676          \\
% Ours        & 52.16          & \textbf{0.4229} & 13.80          & \textbf{13.07} & \textbf{0.3634} & \textbf{0.2739} \\ \midrule
% \end{tabular}
% }
% \end{table}

% To provide a fair comparison with previous algorithms in a more intuitive manner, we conducted comprehensive tests across multiple common metrics. As shown in Table \ref{tab:CVUSA_common}, on the CVUSA dataset, our algorithm has demonstrated significant improvements across these common metrics compared to previous algorithms. On the VIGOR dataset, as shown in Table \ref{tab:VIGOR_common}, we exhibit notable advantages in perceptual and SD metrics, underscoring that our generative algorithm excels in producing ground images with well-structured and textured information. However, in pixel metrics such as RMSE and PSNR, our algorithm does not surpass the GAN-based Sat2Density algorithm, primarily due to pixel difference metrics offering a more forgiving evaluation of blurry images. Our approach, based on a diffusion model, aims to recover finer details in images, often leading to larger pixel differences. Consequently, as presented in Table \ref{tab:VIGOR_comp_sota}, our algorithm achieves significant enhancements in Struct, Perceptual, and Semantic metrics, yet struggles to excel in pixel-based evaluation metrics.

\subsection{Qualitative analysis of rural scenes pertaining to CVUSA.}
\begin{figure*}[h]
\includegraphics[width=1\textwidth]{./image/rebuttal/rural_CVUSA.pdf}
%   \centering
%   \setlength{\abovecaptionskip}{0pt}
%   \setlength{\belowcaptionskip}{0pt}
%   \begin{subfigure}{0.07\linewidth}
%       \centering
%       \includegraphics[width=\linewidth]{image/CVUSA_rural/normal_0001086/sat.png} 
%       \includegraphics[width=\linewidth]{image/CVUSA_rural/good_0011654/sat.png} 
%       % \includegraphics[width=\linewidth]{image/CVUSA_rural/good_0000347.png/sat.png} 
%       \includegraphics[width=\linewidth]{image/CVUSA_rural/good_0001476.png/sat.png}
%       \includegraphics[width=\linewidth]{image/CVUSA_rural/good_0004257.png/sat.png}
%       \caption{\small Sat}
%   \end{subfigure}
%   \begin{subfigure}{0.174\linewidth}
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/normal_0001086/s2s.png} 
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0011654/s2s.png} 
%     % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0000347.png/s2s.png} 
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0001476.png/s2s.png} 
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0004257.png/s2s.png} 
%       \caption{\small S2S}
%   \end{subfigure}
%   \begin{subfigure}{0.174\linewidth}
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/normal_0001086/s2d.png} 
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0011654/s2d.png} 
%     % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0000347.png/s2d.png} 
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0001476.png/s2d.png} 
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0004257.png/s2d.png} 
%     \caption{\small Sat2Den}
%   \end{subfigure}
%   \begin{subfigure}{0.174\linewidth}
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/normal_0001086/control.png} 
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0011654/control.png} 
%     % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0000347.png/control.png} 
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0001476.png/control.png} 
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0004257.png/control.png} 
%     \caption{\small ControlNet}
%   \end{subfigure}
%   \begin{subfigure}{0.174\linewidth}
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/normal_0001086/ours.png} 
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0011654/ours.png} 
%     % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0000347.png/ours.png} 
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0001476.png/ours.png}
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0004257.png/ours.png}
%     \caption{\small Ours}
%   \end{subfigure}
%   \begin{subfigure}{0.174\linewidth}
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/normal_0001086/gt.png} 
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0011654/gt.png} 
%     % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0000347.png/gt.png} 
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0001476.png/gt.png}
%     \includegraphics[width=\linewidth, height=0.4\linewidth]{image/CVUSA_rural/good_0004257.png/gt.png}
%     \caption{\small GT}
%   \end{subfigure}
  \caption{\small Qualitative visualization for generated images using different methods in complex scenes.\label{CVUSA:rural}}
    % \vspace{-1em}
  % \vspace{-1em}
\end{figure*}

Fig.~\ref{CVUSA:rural} presents additional visual results using the CVUSA dataset, mostly collected in rural areas. Notably, S2S and Sat2Den struggle to capture the geometric features of rural scenes, producing blurry and indistinct images. ControlNet also shows noticeable geometric misalignments. In contrast, our method, leveraging the proposed Geometric Cross-Attention (GCA) mechanism and Iterative Homography Adjustment (IHA), demonstrates superior recovery of road structures.

For the CVUSA dataset, another characteristic is that buildings often occupy only a small region in the target ground-view images. The latent embedding of conditioning satellite images can hardly encode the buildings due to few occupant pixels. Owing to the geometric reasoning capabilities of GCA and IHA, our method excels at mining deep semantic features at the correct coordinates. As a result, it accurately reconstructs buildings in the appropriate regions of the ground-view images and synthesizes realistic facade appearances, as illustrated in the third example.

In summary, our method not only retains strong structural consistency but also exhibits a promising ability to generate small elements in satellite images.

% We provide additional visualization of generated results by different methods from complex scenes in the CVUSA dataset  in Fig.~\ref{CVUSA:rural}. 
% These complex scenes comprise a small portion of the dataset, and many approaches struggle to recover the ground-view images faithfully. 
% Among the comparison algorithms, both S2S and Sat2Den failed to successfully learn the unique features of rural scenes, resulting in very blurry images. ControlNet exhibited certain geometric offsets.
% Thanks to the proposed geometric cross-attention (GCA) scheme and the iterative Homography adjustment (IHA) during inference, the road structures recovered by our approach are the closest to the GT images. 
% Among the different scene objects, the buildings occupy a even small region in the target ground-view images, and the building facades information is not included in the conditioning satellite images. 
% Even though, our method, benefiting from geometric guidance encoded by GCA and IHA, tries to recover buildings at corresponding regions in the ground view images and hallucinate a reasonable facade appearance (as shown in the second example). 
% As a result, our generated images retain good structural information and demonstrate a certain ability to learn representations of buildings.

% In the qualitative analysis of rural scenes within the CVUSA dataset, as depicted in Fig.~\ref{CVUSA:rural}, it is evident that our algorithm maintains higher image quality and clarity compared to previous algorithms. Our algorithm faithfully follows the structural information of the satellite images. As rural scenes in the CVUSA dataset constitute only a small portion, both S2S and Sat2Den failed to successfully learn the unique features of rural scenes, resulting in very blurry images. ControlNet exhibited certain geometric offsets. Thanks to GCA and IHA, our generated images retain good structural information and demonstrate a certain ability to learn representations of buildings.


\subsection{MORE ANALYSIS OF THE GCA.}
\subsubsection{Visual analysis of the GCA mechanism.}
\begin{figure*}[h]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \includegraphics[width=0.9\textwidth]{./image/rebuttal/GCA_3.pdf}
    \caption{\small Visualization results of the GCA mechanism. 
    The feature maps are visualized using PCA, while the attention maps are displayed with a jet color mapping. In the middle row, we present ground feature maps sampled from satellite features according to four candidate height map candidates alongside their corresponding attention maps. On the bottom left, we illustrate ground feature maps sampled from satellite features according to the same height plane but at different GCA steps, with feature map differences reflecting variations in height information. The bottom right shows the aggregated features obtained from the learned height candidates across different GCA steps, demonstrating the progressive refinement achieved by the GCA mechanism.
    % The feature map is visualized using PCA, and the attention map is visualized using a jet color mapping scheme. In the middle, we show the results of ground feature maps collected from satellite features at four candidate height planes and their corresponding attention maps. At the bottom left, we provide the ground feature maps sampled from the satellite featuresat the same height candidate index but different GCA steps (feature map differences are used to reflect height map differences). 
    % The bottom right provides the aggregated features from the learned height offsets at different GCA steps. 
    % At the bottom right, we show the aggregated 
    % demonstrate the offset of the feature maps corresponding to the same index in two consecutive GCA processes and the impact of the offset on generating ground images. 
    \label{vis_GCA}}
    % \vspace{-1em}
\end{figure*}
In this section, we present visualizations of intermediate features of satellite images under varying height assumptions and the corresponding attention maps $A_i$. The progressive ground features using the GCA mechanism are also revealed.

During the GCA process, the inputs include the current ground map features ($Q$), satellite map features ($V$), and the relative poses $R$ and $T$. We set eight height assumptions ($h \in \{-3, -2, -1, 1, 2, 3, 4, 5\}$ in experiments. Here, $h$ represents the relative height of the satellite image to the camera. When the scene lies below the camera, the lower part of the camera captures the scene while the camera's upper part works for scenes above the camera. This behavior is illustrated in the second row of Fig.~\ref{vis_GCA}.

As defined in Eq.~\ref{Ep_GCA}, the GCA mechanism infers offsets $\Delta h_i$ and attention weights $A_i$ for each height candidate based on the current ground map features ($Q$). The attention weights $A_i$ are normalized via a softmax function across the eight height planes, determining the confidence associated with each plane.

In the middle of Fig.~\ref{vis_GCA}, the top row shows the ground feature maps sampled from satellite features at different candidate heights ($h_i + \Delta h_i$), while the bottom row visualizes the corresponding attention maps $\{A\}_{i=1}^N$. In the first GCA iteration (step 1), the projection plane samples features based on the initial height assumption, and the attention is evenly distributed across various regions, providing a baseline solution. By the second GCA iteration (step 2), the inferred offsets $\Delta h_i$ introduce positional shifts in the projection plane, as highlighted in the red box. These shifts enable the projection to capture more details of ground images, such as pathways. The GCA iteration also gradually refines the representations of other scene elements.

As the GCA iterations progress, the attention maps evolve to focus on salient regions. For planes where $h<0$, the attention primarily targets the ground, while for planes where $h>0$, it shifts towards elevated elements like trees. The attention shifting illustrates that GCA effectively enhances feature representations of the entire scene in a progressive manner.


% In this section, we provide visualizations of the collected features from satellite images under varying height assumptions, the corresponding weights in the attention matrix $A_i$, and the progressively refined ground feature maps achieved using the GCA mechanism. 

% During the Geometric Cross-Attention (GCA) process, our inputs consist of the ground map features (Q) at the current time, satellite map features (V), as well as the relative positions R and T. Initially, we assume eight hypothetical heights (h), which in our experiments are set to [-3, -2, -1, 1, 2, 3, 4, 5]. Here, h represents the height of satellite imagery relative to the camera, so when the scene is below the camera, the lower part of the camera captures some of the scene while the upper part does not, and when the scene is above the camera, the upper part of the camera captures some of the scene while the lower part does not, as shown in the second row of Fig.~\ref{vis_GCA}. 

% As shown in Eq.~\ref{Ep_GCA}, based on the current ground map features ($Q$), the offset \(\Delta h_i\) and attention weights \(A_i\) are inferred for each height candidate. The attention weights (\(A_i\)) are then normalized using a softmax function across the eight height planes to determine the confidence level associated with each plane.

% In the middle of Fig.~\ref{vis_GCA}, the top row illustrates the ground feature maps collected from satellite features at different candidate heights $h_i + \Delta h_i$, while the bottom row shows the corresponding attention maps $\{A_i\}_{i=1}^N$. During the initial GCA iteration (step 1), the projection plane accurately samples features based on the height assumption, with the attention being evenly distributed across various locations, forming the initial solution. By the second GCA iteration (step 2), the offset $\Delta h_i$ generated by $Q$ induces positional shifts in the projection plane, as highlighted in the red box. These offsets allow the projection to capture more detailed ground-level features, such as pathways, while also refining representations of other scene elements.

% The attention maps evolve in subsequent iterations to target critical regions more effectively. For height assumption planes where $h<0$, attention focuses primarily on the ground, while for $h>0$, it shifts towards higher elements such as trees. This adaptive refinement underscores the ability of GCA to progressively enhance feature representation.


% In the middle Fig.~\ref{vis_GCA}, the upper section shows the feature maps sampled at different hypothetical heights alongside their corresponding attention maps. These visualizations demonstrate that the sampled feature maps effectively infer the ground map features, while the attention maps aggregate information across multiple height candidate planes. In the lower section, we illustrate the changes in sampled features between two consecutive GCA iterations. 

% The third row of Fig.~\ref{vis_GCA} presents the corresponding attention maps \(\{A_i\}_{i=1}^N\).
% Then, as per Eq.~\ref{Ep_GCA}, based on the ground map features \( Q \) at this moment, we infer the offset \(\Delta d_i\) and attention \(A_i\) assumed for each point on the ground map, where \(A_i\) is used to calculate the softmax of eight planes to determine the confidence level associated with each plane. 

% In Fig.~\ref{vis_GCA}, we present above the feature map derived from sampled candidates of different heights, along with the corresponding attention images. It can be observed that the sampled feature maps can accurately infer ground map features, while different attentions can aggregate features across multiple height candidate planes. Below, we demonstrate the changes in sampled features across two consecutive instances of GCA. It can be observed that the resulting shifts \(\Delta d_i\)  aim to seek additional details to enhance the representation of ground feature maps.
% In Fig.~\ref{vis_GCA}, we demonstrate the results of two consecutive GCA to aid in our understanding of offset and attention. In each step, the top row corresponds to the sampling results based on \(d_i + \Delta d_i\), while the bottom row represents the \(A_i\) of the \(i^{th}\) sampled plane. It can be observed that during the initial GCA iteration(step 1), the projection plane faithfully samples based on the height assumption, while the attention is evenly distributed across various locations, resulting in our initial solution. Moving on to the subsequent second step(step 2), we notice that the \(\Delta d_i\) generated by Q at this point induces an offset in the projection plane, as indicated by the red box. This positional offset enables the projection image to capture more of the pathways on the ground, while also making adjustments to other scences. Furthermore, as the attention visualization images evolve, they can intentionally capture key areas. In the assumption planes where $h<0$, the attention tends to focus more on the ground, whereas in the assumption planes where $h>0$, the attention is directed more towards trees.
\subsubsection{Comparison with naive cross-attention.}
The proposed GCA module offers the following advantages over a naive cross-attention mechanism. 

{1. Flexibility in view-related image generation.} By leveraging the relative pose between satellite and ground images, the proposed GCA enables the generation of ground images at arbitrary locations and from arbitrary views on the same satellite map. In contrast, the simple cross-attention mechanism cannot handle view changes and requires additional modules to process the relative pose information (e.g., Zero-1-to-3~\cite{liu2023zero}). Our method is more flexible and can handle various relative pose differences. 

{2. Avoid redundancy information.} The proposed GCA limits attention to regions likely to correspond geometrically. Each ground-view pixel only attends to satellite image areas along its camera ray. This focused attention minimizes noise from irrelevant regions, unlike naive cross-attention, which indiscriminately considers the entire image.

{3. Improved computational efficiency and reduced GPU memory usage.}
GCA achieves significant reductions in computational complexity by employing sparse sampling. For satellite image features of size $S*S$ and ground image features of size $H*W$, the complexity of naive cross-attention is $O(S*S*H*W)$. In contrast, our algorithm samples N planes (N=8), with the complexity of sampling being $O(N*H*W)$, computing horizontal and vertical coordinate offsets at $O(2N*H*W)$, and attention calculation at $O(N*H*W)$. Overall, our complexity is significantly reduced to $O(4*N*H*W)$ compared to naive cross-attention. In practical experiments, for inference on one example, native cross-attention consumes 250MB of GPU memory, whereas using GCA reduces this to only 184MB.


% Firstly, by relying on relative satellite image rotation and translation, along with assuming altitude to identify corresponding points, we can generate images at arbitrary locations on the satellite map. Conversely, naive cross-attention, during generation, can only agree on which part of the satellite image to generate, or crudely append additional modules to use position as a condition, similar to Zero123\cite{liu2023zero}. Furthermore, due to GCA focusing only on regions that are geometrically likely to correspond (each pixel only attends to the satellite image region along the camera ray), it effectively screens out potential noise that might arise from cross-attending across the entire image. As a result, our generation performance surpasses that of using naive cross-attention, as demonstrated in Tab.~\ref{tab:KITTI_pose}. Finally, our algorithm is based on sparse sampling, which also provides advantages in terms of computational complexity. For satellite image features of size $S*S$ and ground image features of size $H*W$, the computational complexity of naive cross-attention is $O(S*S*H*W)$. In contrast, our algorithm samples N planes (N=8), with the complexity of sampling being $O(N*H*W)$, computing horizontal and vertical coordinate offsets at $O(2N*H*W)$, and attention calculation at $O(N*H*W)$. Overall, our complexity is significantly reduced to $O(4*N*H*W)$ compared to naive cross-attention. In practical experiments, for inference on a batch, native cross-attention consumes 250MB of GPU memory, whereas using GCA reduces this to only 184MB.



\subsection{Discussing the reasons for the failure of the CFG scheme in controlling the environment.}
\begin{figure*}[h]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \includegraphics[width=1\textwidth]{./image/rebuttal/ZoE.pdf}
    \caption{\small Using LLAVA-generated text for ground images can correspond to multiple ground pictures, while satellite images correspond one-to-one with ground images.\label{muti_txt}}
    % \vspace{-1em}
\end{figure*}
In Tab~\ref{tab:text_condition}, we employ the LDM and ControlNet models using a Classifier-Free Guidance (CFG) approach for multi-condition generation. Both LDM and ControlNet take two conditions as inputs: satellite images and image descriptions generated by LLAVA. Through training, we observed that the weight assigned to the satellite image condition far exceeded that of the environmental description condition. Furthermore, as depicted in Fig.~\ref{muti_txt}, one image description generated by LLAVA could correspond to multiple ground images, and text captions significantly lack geometric information. Each ground image corresponds specifically to a satellite image, which not only encapsulates geometric descriptions but also texture representations. Consequently, the LDM and ControlNet tend to prioritize the satellite images over the ground image descriptions, leading to the degraded control of environmental conditions.

\subsection{The performance of Text-guided Zero-shot Environmental Control in a multilingual setting.}
\begin{figure*}[h]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \includegraphics[width=1\textwidth]{./image/rebuttal/chinese_clip.pdf}
    \caption{\small Controllable generation in the Chinese context. This figure displays generated images of spring, autumn, and winter from top to bottom. \label{Chinese}}
    % \vspace{-1em}
\end{figure*}

In order to ensure compatibility across linguistic contexts, our framework provides a flexible interface that can integrate with various multimodal large language models. In the paper implementation, we utilize a frozen CLIP model to extract textual and visual features and compute losses. This approach eliminates additional training and enables straightforward adaptation to other multimodal LLMs. For instance, to support Chinese text prompts, Chinese-CLIP~(\cite{chinese-clip}) is incorporated. By computing losses between Chinese text and image features, the image-generating process can be effectively guided to align with the input language. 

% For the purpose of compatibility across various linguistic environments, we have provided a convenient interface that can be utilized with diverse multimodal large-scale models. Within the framework we propose, we leverage a frozen CLIP to extract textual and visual features, and compute losses to guide the model's generative direction. The introduction of a multimodal model that does not require additional training enables our framework to adapt to various model. We can straightforwardly replace the use of Clip in this article with any other multimodal model to extract features. For example, in the case of Chinese, we can introduce a multimodal model trained on Chinese text~(\cite{chinese-clip}). By calculating losses based on Chinese text and image features, we can appropriately guide the model's generative direction.


\subsection{Discussion and Limitations}
The primary issue revolves around the accurate restoration of building details, which is a common limitation in the task of ground image generation from satellite data. Due to the significant discrepancy between satellite and ground views, satellite images lack the texture details of building facades. Therefore, it is difficult to faithfully reconstruct the building details from the ground view. 
The other limitation is the lane marking generation. There are inconsistencies between the generated lanes and the real-world lanes. These differences arise largely due to misalignment of the capture time between the satellite maps and ground images.  
Despite these limitations, our method demonstrates significant improvements in terms of geometric alignment and environmental control in generating diverse ground-view images.

% Our method does not faithfully restore building information, which is indeed a common limitation of ground map generation methods. Due to significant parallax, satellite images cannot fully depict the texture details of buildings, thus making it challenging to generate buildings in ground maps from satellite imagery. In our generation process, by iteratively aggregating ground map features, we optimized the generation of ground lane marking information. However, there are still discrepancies between the generated lane markings and real-world scenes. This is because during the dataset collection process, the timing of satellite image acquisition and ground map collection differs, resulting in inconsistencies in lane information between the two images, causing the model to struggle in capturing the intricate features of lane markings. Lastly, in terms of computational time, due to diffusion being an iterative generation approach, our method consumes more time compared to previous end-to-end solutions.

\subsection{The results of Text-guided Zero-shot Environmental Control.}
\begin{figure*}[h]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \begin{subfigure}{0.19\linewidth}
      \centering
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/21/222_0042957_grd_sample_2_None.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/66/322_0016217_grd_sample_2_None.png}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/100/339_0015413_grd_sample_2_None.png}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/148/363_0014566_grd_sample_1_None_1.png}
      \caption{\small No text prompt}
  \end{subfigure}
  \begin{subfigure}{0.19\linewidth}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/21/spring_222_0042957_grd_sample_0_spring.png} 
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/66/spring_322_0016217_grd_sample_0_spring.png}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/100/339_0015413_grd_sample_1_Bloom.png}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/148/spring_363_0014566_grd_sample_0_spring.png}
      \caption{\small Spring}
  \end{subfigure}
  \begin{subfigure}{0.19\linewidth}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/21/summer_222_0042957_grd_sample_0_summer.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/66/322_0016217_grd_sample_1_on_sunny_day.png}   
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/100/summer_339_0015413_grd_sample_0_summer.png}   
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/148/363_0014566_grd_sample_0_on_sunny_day_1.png}  
    \caption{\small Summer}
  \end{subfigure}
  \begin{subfigure}{0.19\linewidth}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/21/automn_222_0042957_grd_sample_0_automn.png}    
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/66/automn_322_0016217_grd_sample_0_automn.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/100/automn_339_0015413_grd_sample_0_automn.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/148/automn_363_0014566_grd_sample_0_automn.png} 
    \caption{\small Autumn}
  \end{subfigure}
  \begin{subfigure}{0.19\linewidth}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/21/winter_222_0042957_grd_sample_0_winter.png}    
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/66/winter_322_0016217_grd_sample_0_winter.png} 
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/100/winter_339_0015413_grd_sample_0_winter.png}
    \includegraphics[width=\linewidth, height=0.4\linewidth]{image/text/good/148/winter_363_0014566_grd_sample_0_winter.png}
    \caption{\small Winter}
  \end{subfigure}
  \caption{\small Generating under different textual conditions to obtain text-guided image results while maintaining the structure of the images}
\vspace{-1em}
\end{figure*}



\subsection{More results from pose alignment.}
\begin{figure}[htbp]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \begin{subfigure}{0.112\linewidth}
      \centering
      % \includegraphics[width=\linewidth]{image/correct/KITTI/sat_data/2011_09_26_2011_09_26_drive_0009_sync_0000000333.png}      
      % \includegraphics[width=\linewidth]{image/correct/KITTI/sat_data/2011_09_30_2011_09_30_drive_0018_sync_0000000139.png}      
      % \includegraphics[width=\linewidth]{image/correct/KITTI/sat_data/2011_09_30_2011_09_30_drive_0018_sync_0000001623.png}      
      % \includegraphics[width=\linewidth]{image/correct/KITTI/sat_data/2011_09_30_2011_09_30_drive_0018_sync_0000002411.png}      
      \includegraphics[width=\linewidth]{image/correct/KITTI/sat_data/2011_09_30_2011_09_30_drive_0018_sync_0000002686.png}      
      \includegraphics[width=\linewidth]{image/correct/KITTI/sat_data/2011_09_30_2011_09_30_drive_0028_sync_0000000448.png}      
      \includegraphics[width=\linewidth]{image/correct/KITTI/sat_data/2011_09_30_2011_09_30_drive_0028_sync_0000000913.png}      
      \includegraphics[width=\linewidth]{image/correct/KITTI/sat_data/2011_09_30_2011_09_30_drive_0028_sync_0000002625.png}      
      \caption{Sat}
  \end{subfigure}
  \begin{subfigure}{0.28\linewidth}
      % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/80/pre__2011_09_26_2011_09_26_drive_0009_sync_0000000333.png.png}      
      % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/493/pre__2011_09_30_2011_09_30_drive_0018_sync_0000000139.png.png}      
      % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/763/pre__2011_09_30_2011_09_30_drive_0018_sync_0000001623.png.png}           
      % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/892/pre__2011_09_30_2011_09_30_drive_0018_sync_0000002411.png.png}      
            \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/947/pre__2011_09_30_2011_09_30_drive_0018_sync_0000002686.png.png}      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/1187/pre__2011_09_30_2011_09_30_drive_0028_sync_0000000448.png.png}      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/1278/pre__2011_09_30_2011_09_30_drive_0028_sync_0000000913.png.png}            \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/1622/pre__2011_09_30_2011_09_30_drive_0028_sync_0000002625.png.png}      
      \caption{LDM}
  \end{subfigure}
  \begin{subfigure}{0.28\linewidth}
      % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/80/gt__2011_09_26_2011_09_26_drive_0009_sync_0000000333.png.png}      
      % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/493/gt__2011_09_30_2011_09_30_drive_0018_sync_0000000139.png.png}      
      % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/763/gt__2011_09_30_2011_09_30_drive_0018_sync_0000001623.png.png}      
      % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/892/gt__2011_09_30_2011_09_30_drive_0018_sync_0000002411.png.png}      
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/947/gt__2011_09_30_2011_09_30_drive_0018_sync_0000002686.png.png}      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/1187/gt__2011_09_30_2011_09_30_drive_0028_sync_0000000448.png.png}      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/1278/gt__2011_09_30_2011_09_30_drive_0028_sync_0000000913.png.png}            \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/1622/gt__2011_09_30_2011_09_30_drive_0028_sync_0000002625.png.png}      
      \caption{GT}
  \end{subfigure}
  \begin{subfigure}{0.28\linewidth}
      % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/80/correct__2011_09_26_2011_09_26_drive_0009_sync_0000000333.png.png}      
      % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/493/correct__2011_09_30_2011_09_30_drive_0018_sync_0000000139.png.png}      
      % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/763/correct__2011_09_30_2011_09_30_drive_0018_sync_0000001623.png.png}     
      % \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/892/correct__2011_09_30_2011_09_30_drive_0018_sync_0000002411.png.png}      
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/947/correct__2011_09_30_2011_09_30_drive_0018_sync_0000002686.png.png}      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/1187/correct__2011_09_30_2011_09_30_drive_0028_sync_0000000448.png.png}      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/1278/correct__2011_09_30_2011_09_30_drive_0028_sync_0000000913.png.png}      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/KITTI/perfect/1622/correct__2011_09_30_2011_09_30_drive_0028_sync_0000002625.png.png}    
      \caption{LDM W. IHA}
  \end{subfigure}
  \caption{\small Results of IHA on the KITTI dataset.}
\end{figure}

\begin{figure*}[tbp]
  \centering
  \setlength{\abovecaptionskip}{0pt}
  \setlength{\belowcaptionskip}{0pt}
  \begin{subfigure}{0.112\linewidth}
      \centering
      \includegraphics[width=\linewidth]{image/correct/CVUSA/sat/0000210.png}
      \includegraphics[width=\linewidth]{image/correct/CVUSA/sat/0000244.png}
      \includegraphics[width=\linewidth]{image/correct/CVUSA/sat/0000415.png}
      \includegraphics[width=\linewidth]{image/correct/CVUSA/sat/0001990.png}

      \includegraphics[width=\linewidth]{image/correct/CVUSA/sat/0003823.png}
      \includegraphics[width=\linewidth]{image/correct/CVUSA/sat/0004443.png}
      \includegraphics[width=\linewidth]{image/correct/CVUSA/sat/0004580.png}
      \includegraphics[width=\linewidth]{image/correct/CVUSA/sat/0004604.png}
      \includegraphics[width=\linewidth]{image/correct/CVUSA/sat/0005262.png}
      \includegraphics[width=\linewidth]{image/correct/CVUSA/sat/0005631.png}
      \includegraphics[width=\linewidth]{image/correct/CVUSA/sat/0006101.png}
      \caption{Sat}
  \end{subfigure}
  \begin{subfigure}{0.28\linewidth}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/44/pre_0000210.png}      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/50/pre_0000244.png}       \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/93/pre_0000415.png}       \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/398/pre_0001990.png}                    
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/793/pre_0003823.png}              
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/914/pre_0004443.png}       \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/940/pre_0004580.png}       \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/950/pre_0004608.png}       
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/1095/pre_0005262.png}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/1170/pre_0005631.png}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/1269/pre_0006101.png} 
      \caption{LDM}
  \end{subfigure}
  \begin{subfigure}{0.28\linewidth}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/44/gt_0000210.png}      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/50/gt_0000244.png}       \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/93/gt_0000415.png}       \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/398/gt_0001990.png}              
            
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/793/gt_0003823.png}              
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/914/gt_0004443.png}       \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/940/gt_0004580.png}       \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/950/gt_0004608.png}       
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/1095/gt_0005262.png}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/1170/gt_0005631.png}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/1269/gt_0006101.png}     
      \caption{GT}
  \end{subfigure}
  \begin{subfigure}{0.28\linewidth}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/44/correct_0000210.png}      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/50/correct_0000244.png}       \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/93/correct_0000415.png}       \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/398/correct_0001990.png}      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/793/correct_0003823.png}       \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/914/correct_0004443.png}       \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/940/correct_0004580.png}       \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/950/correct_0004608.png}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/1095/correct_0005262.png}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/1170/correct_0005631.png}
      \includegraphics[width=\linewidth, height=0.4\linewidth]{image/correct/CVUSA/perfect/1269/correct_0006101.png}
      \caption{LDM W. IHA}
  \end{subfigure}
  \caption{\small Results of IHA on the CVUSA dataset.}
\end{figure*}
\end{document}

