\section{Related Work}
\paragraph{Cross-view ground scene generation.} 
In the study by \cite{zhai2017predicting}, the authors first attempted to align the semantic features of satellite images onto ground-level perspectives. In \cite{wu2022cross}, GANs were employed to generate ground images. Strong geometric relationships were introduced in the task of ground image generation by \cite{lu2020geometry, shi2022geometry, qian2023sat2density}. \cite{li2021sat2vid, li2024sat2scene} explicitly constructed a 3D point cloud representation of the scene, and then transformed it into a scene representation in a feed-forward manner. \cite{gao2024scp, xu2024geospecific, li2024crossviewdiff} advocate for generating ground images from ground-to-ground scene segmentation images. Among these, \cite{gao2024scp} specifically highlights the impact of various noises on the generated results and innovatively proposes a noise-prior-based solution. However, previous methods have predominantly relied on coarse scene priors, leading to compounded errors in the results. We propose GCA and IHA to ensure geometric consistency between ground images and satellite views. 
The cross-view generation work targeting single objects is also highly inspiring. \cite{liu2023zero} overlays camera position encoding for scene transformation, while \cite{poole2022dreamfusion} utilizes diffusion to optimize the Nerf representation of scenes. \cite{melas20243d} and \cite{gao2024cat3d} generate continuous frame data based on video diffusion. These approaches often fail in large-scale scene reconstruction, especially when dealing with significant perspective differences between satellite and ground images, which is the issue we are dedicated to addressing.
% Inspired by these successful works, we recognize the pivotal role of geometric relationships in generating ground scenes from cross-view satellite imagery. However, previous approaches have predominantly relied on coarse scene priors, leading to compounded errors in results, ultimately resulting in distorted scene structures and misaligned camera poses. To address this, we introduce a geometry-aware cross-attention mechanism and pose alignment mechanism to ensure geometric consistency between ground-level images and satellite views.

\paragraph{Text-controlled image generation.} In text generation, a multitude of solutions have emerged over time leveraging Generative Adversarial Networks (GANs)~(\cite{dash2017tac, regmi2018cross, ruan2021dae, tao2022df}). However, with the introduction of diffusion~(\cite{song2020score, rombach2022high}), its exceptional generation capability has evolved into a potent tool for creating images. SigniÔ¨Åcant strides have been taken in text-driven image synthesis through diffusion by \cite{avrahami2022blended, li2023gligen, brooks2023instructpix2pix}. \cite{li2023drivingdiffusion, gao2023magicdrive} propose a method that generates ground images based on text conditions and BEV segmentation images. However, this strategy is hampered by the limitations of expressive capabilities in scene segmentation, leading to arbitrary results in scene synthesis. In this paper, we employ satellite images with enhanced representational capabilities for ground synthesis and introduce a novel text-guided mechanism to ensure both the reliability of scene generation and the diversity of generated results.