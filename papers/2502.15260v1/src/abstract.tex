\begin{abstract}

State space models (SSMs) like Mamba have recently attracted much attention.
Compared to Transformer-based large language models (LLMs), Mamba achieves
linear computation complexity with the sequence length and demonstrates superior performance.
However, Mamba is hard to accelerate due to the scattered activation outliers and the complex computation dependency, rendering existing LLM accelerators inefficient.
In this paper, we propose LightMamba  that co-designs the quantization algorithm
and FPGA accelerator architecture for efficient Mamba inference. 
We first propose an FPGA-friendly post-training quantization algorithm that features
rotation-assisted quantization and power-of-two SSM quantization to reduce the majority of computation
to 4-bit. 
We further design an FPGA accelerator that partially unrolls the Mamba computation
to balance the efficiency and hardware costs. 
Through computation reordering as well as fine-grained
tiling and fusion, the hardware utilization and memory efficiency of the accelerator get drastically
improved. 
We implement LightMamba on Xilinx Versal VCK190 FPGA
and achieve 4.65$\sim$6.06$\times$ higher energy efficiency over the GPU baseline.
When evaluated on Alveo U280 FPGA, LightMamba reaches 93 tokens/s, 
which is 1.43$\times$ that of the GPU baseline.

% Large language models (LLMs) have a revolutionary impact
% on natural language tasks.
% However, Transformer-based LLMs suffer from 
% quadratic scaling with respect to the sequence length.
% To address this, Mamba is proposed with linear scaling in sequence length and competitive performance.
% While hardware accelerators for Transformer-based LLMs have been extensively studied,
% how to accelerate Mamba remains an open question.
% In this work,
% we propose LightMamba, the first end-to-end FPGA-based accelerator, 
% featuring quantization algorithm and hardware co-design.
% We propose the first post-training quantization (PTQ)
% method for the Mamba,
% achieving accurate fully quantized model 
% with 8-bit weight and activation (W8A8) and 4-bit weight and activation (W4A4).
% We design the hardware architecture for quantized Mamba
% and propose two methods for the state space model (SSM) layer 
% in Mamba, i.e., compute-aware
% generation reordering and fine-grained tiling and fusion,
% further improving the computation and memory efficiency.
% We implement LightMamba on Xilinx Versal VCK190 FPGA
% and achieve actual throughput of 4.65 tokens/s with W8A8 quantization and 9.16 tokens/s with W4A4 quantization.
% When evaluated on Alveo U280 FPGA,
% LightMamba reaches 93 tokens/s, 
% which is 1.8$\times$ that of the state-of-the-art accelerator FlightLLM for Llama.

% \ml{Compute-aware generation reordering feels weird?}

\end{abstract}

\begin{IEEEkeywords}
Mamba, rotation-assisted quantization, FPGA accelerator,
computation reordering, fine-grained tiling and fusion
\end{IEEEkeywords}