\section{related works}
\label{sec:background}

% \subsection{Mamba}
% Mamba~\cite{gu2023mamba,dao2024transformers} is a new class of LLM with better performance and efficiency than Transformers.
% It is based on the selective state space model,
% modeling sequence-to-sequence transformation from input $x_t$ to output $y_t$ with:
% \begin{equation}
% \label{equ:ssm_definition}
% \begin{aligned}
%     h_t =\Bar{A}h_{t-1} + \Bar{B}x_t, \quad y_t =Ch_t + Dx_t
% \end{aligned}   
% \end{equation}
% where $\Bar{A}$ and $\Bar{B}$ are discretized from the continuous
% parameters $\Delta, A, B$ with the following rule:
% \begin{equation}
% \label{equ:ssm_discretization}
% \begin{aligned}
%     &\Bar{A} = \mathrm{exp}(\Delta A), 
%     &\Bar{B} = (\Delta A)^{-1}(\mathrm{exp}(\Delta A)- I)\cdot\Delta B\approx\Delta B\\
% \end{aligned}   
% \end{equation}

% \ml{Add more references for the quantization algorithm here. }

\subsection{LLM Quantization}
Quantization maps the weights and activations from high-bit-precision floating point (FP) numbers,
e.g., FP16 to low-bit integers, e.g., INT8 or INT4,
reducing both memory and computation costs.
Existing algorithms mostly leverage PTQ for LLMs and observe
the key obstacle comes from the outliers in weights and activations.
% which are elements
% that are significantly larger than other elements in the tensor.
% Existing methods can be divided into quantization-aware training (QAT) and post-training quantization (PTQ).
% PTQ is more popular than QAT in LLM quantization
% because it does not require training LLMs with huge cost.
% Here, we introduce weight-activation PTQ methods for LLMs.
LLM.int8()~\cite{dettmers2022llm} uses mix-precision quantization and
keeps outliers in high bit-precision,
which introduces large computation overhead.
SmoothQuant~\cite{xiao2023smoothquant} and 
Outlier Suppression~\cite{wei2022outlier,wei2023outlier} observe the outliers persist
in specific channels and thus, calculate 
the channel-wise scaling or shifting factors to rescale the outliers before quantization.
% to migrate the quantization
% difficulty from activations to weights.
QuaRot~\cite{ashkboos2024quarot} and SpinQuant~\cite{liu2024spinquant}
multiply the weight and activation with a rotation matrix 
to remove outliers.
However, these methods are only demonstrated for Transformer-based LLMs.
% which cause large quantization errors when directly applied to Mamba 
% \ml{Why? Any references? $\checkmark$}.
For Mamba, \cite{li2024evaluating} only quantizes the linear layers with round-to-nearest (RTN) method. 
Mamba-PTQ~\cite{pierro2024mamba} simply applies SmoothQuant on Mamba.
Although they only quantize the linear layers and leave the SSM layer in FP, they still
suffer from large accuracy degradation.
% They not only suffer from large accuracy degradation.
% Moreover, they only quantize the linear layer leaving the SSM layer in FP.
% In this paper, we propose a PTQ method quantizing the entire model 
% including linear layer, conv1d layer, and SSM layer 
% with minimal accuracy degradation
% in W8A8 and W4A4 settings.


\subsection{FPGA-based LLM Accelerators}

% \ml{Add more references, including HG-PIPE.$\checkmark$}

Previous FPGA-based LLM accelerators can be categorized into two types: temporal architecture and spatial architecture. 
Temporal architecture constructs a processing engine (PE) performing various tasks, especially for matrix multiplication (MM)~\cite{DFX,Flightllm,enhancing}. 
% which requires frequent access to off-chip memory, resulting in high latency and energy consumption.
This architecture is not well-suited for handling the diverse and complex element-wise multiplications (EM) in Mamba.
Spatial architecture customizes PEs for different operations and supports concurrent processing of multiple PEs 
in pipeline~\cite{understanding,HGPIPE,ftrans,adaptable},
leading to low latency.
% a pipelined manner~\cite{understanding}. 
However, due to the custom PEs dispersing resources, this design results in lower parallelism for MM.
% However, if the spatial unrolling strategy is inefficient, some operators may become bottlenecks, reducing the overall hardware utilization.
% Previous LLM accelerator designs focus on Transformer models, which mainly consist of MM. 
% In contrast, Mamba not only has MMs but also a large number of EMs in SSM, which are difficult to integrate with MM. 
In this paper, we adopt a partially unfolded spatial architecture that unfolds 
one Mamba block and co-designs the quantization algorithm and architecture to improve hardware utilization and
reduce the latency.
% In this paper, we adopt a partially unfolded spatial architecture,
% fully unfolding SSM and executing linear layers in a time-multiplexed manner, leading to high hardware resource utilization and low latency.
% In this paper, we partially applied spatial unrolling to the EM operators in Mamba, while continuing to implement MM through large-scale PEs. 
% By leveraging a carefully designed approach, EM and MM can be executed concurrently, allowing for high hardware resource utilization while maintaining low latency.
A comparative analysis of these architectures is presented in Table~\ref{tab:Qualitative comparison}.
% \ml{Time-division multiplexing manner is very weird. \checkmark}

% Previous work has proposed customized architecture designs for transformer models. Some research focuses on software-hardware co-design, such as FlightLLM~\cite{FlightLLM}, which introduces sparsity into Llama and accelerates the decoding phase through a custom DSP chain,
% % GOBO~\cite{GOBO} and EdgeBERT~\cite{EdgeBert} explore quantization methods, 
% while FACT~\cite{FACT} highlights the importance of mixed-precision quantization to compress linear layers and reduce latency. Other works, like DFX~\cite{DFX}, leverage model parallelism and optimized dataflow to accelerate LLM inference with low-latency generation.

% However, these approaches have not yet been applied to Mambaâ€™s hardware acceleration. Our work is the first to deploy Mamba on an FPGA, achieving algorithm-hardware co-design.


% \begin{table}[]
% \resizebox{\linewidth}{!}{
% \begin{tabular}{clllllllll}
% \cline{1-7}
% \multicolumn{2}{|c|}{Representative   Work}                                                      & \multicolumn{1}{l|}{FlightLLM} & \multicolumn{1}{l|}{DFX} & \multicolumn{1}{l|}{GOBO} & \multicolumn{1}{l|}{FACT} & \multicolumn{1}{l|}{Ours} &  &  &  \\ \cline{1-7}
% \multicolumn{1}{|c|}{\multirow{2}{*}{algorithm}} & \multicolumn{1}{l|}{quantization scheme}      & \multicolumn{1}{l|}{1}         & \multicolumn{1}{l|}{1}   & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    &  &  &  \\ \cline{2-7}
% \multicolumn{1}{|c|}{}                           & \multicolumn{1}{l|}{hardware-aware}           & \multicolumn{1}{l|}{1}         & \multicolumn{1}{l|}{1}   & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    &  &  &  \\ \cline{1-7}
% \multicolumn{1}{|c|}{\multirow{2}{*}{hardware}}  & \multicolumn{1}{l|}{architecture}             & \multicolumn{1}{l|}{1}         & \multicolumn{1}{l|}{1}   & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    &  &  &  \\ \cline{2-7}
% \multicolumn{1}{|c|}{}                           & \multicolumn{1}{l|}{parallel scheme}          & \multicolumn{1}{l|}{1}         & \multicolumn{1}{l|}{1}   & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    &  &  &  \\ \cline{1-7}
% \multicolumn{1}{|c|}{\multirow{4}{*}{metric}}    & \multicolumn{1}{l|}{Throughput}               & \multicolumn{1}{l|}{1}         & \multicolumn{1}{l|}{1}   & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    &  &  &  \\ \cline{2-7}
% \multicolumn{1}{|c|}{}                           & \multicolumn{1}{l|}{latency}                  & \multicolumn{1}{l|}{1}         & \multicolumn{1}{l|}{1}   & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    &  &  &  \\ \cline{2-7}
% \multicolumn{1}{|c|}{}                           & \multicolumn{1}{l|}{on   chip memory size}    & \multicolumn{1}{l|}{1}         & \multicolumn{1}{l|}{1}   & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    &  &  &  \\ \cline{2-7}
% \multicolumn{1}{|c|}{}                           & \multicolumn{1}{l|}{bandwidth   uitilization} & \multicolumn{1}{l|}{1}         & \multicolumn{1}{l|}{1}   & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    & \multicolumn{1}{l|}{1}    &  &  &  \\ \cline{1-7}
% \multicolumn{1}{l}{}                             &                                               &                                &                          &                           &                           &                           &  &  & 
% \end{tabular}
% }
% \end{table}


% \begin{table}[!t]
% \centering
% \caption{Qualitative comparison between different paradigms. \ml{What are the drawbacks of existing designs?}}
% \label{tab:Qualitative comparison}
% % \resizebox{0.45\textwidth}{!}{
% \begin{tabular}{c|c|c|c|c|c}
% % {
% % >{\raggedright\arraybackslash}p{0.6cm} >
% % {\raggedright\arraybackslash}p{1.2cm} >{\raggedright\arraybackslash}p{0.8cm} >{\raggedright\arraybackslash}p{1.4cm} >
% % {\raggedright\arraybackslash}p{0.8cm} >{\raggedright\arraybackslash}p{1.2cm}
% % }
% \toprule
% \textbf{Work}        & \textbf{Model} & \textbf{Quant.} & \textbf{Arch.} & \textbf{Latency} & \textbf{Operat.}\\
% \midrule
% \cite{understanding} & Transformer & W4A8 & Spatial & \textcolor{green}{Low} & MM,NL\\
% \cite{DFX} & Transformer & FP16 & Temporal & \textcolor{red}{High} & MM,NL\\
% \cite{Flightllm} & Transformer & W3.5A8 & Temporal & \textcolor{red}{High} & MM,NL\\
% % FACT & Transformer & Int4/Int8 & Temporal & High & MM,non-linear\\
% Ours & Mamba & W4A4 & Partial Spat. & \textcolor{green}{Low} & MM,EM,NL\\
% \bottomrule
% \end{tabular}
% % }
% \end{table}


\begin{table}[!t]
\centering
\caption{Qualitative comparison between different paradigms. W4A4 indicates 4-bit weight and 4-bit activation.}
\label{tab:Qualitative comparison}
% \resizebox{0.45\textwidth}{!}{
\begin{tabular}{c|c|c|c}
\toprule
 & \cite{understanding} & \cite{Flightllm}\cite{DFX} & Ours\\
\midrule
\textbf{Architecture} & Spatial & Temporal & Partial Spat. \\
\textbf{Model} & \textcolor{black}{Transformer} & \textcolor{black}{Transformer} & \textcolor{black}{Mamba}\\
\textbf{Bit Precision} & W4A8 & W3.5A8 or FP16 & W4A4 \\
\textbf{Latency} & \textcolor{darkgreen}{Low} & \textcolor{darkred}{High} & \textcolor{darkgreen}{Low}\\
\textbf{EM Compatibility} & \textcolor{darkgreen}{$\checkmark$} & \textcolor{darkred}{$\times$} & \textcolor{darkgreen}{$\checkmark$}\\
\textbf{MM parallelism} & \textcolor{darkblue}{Mid} & \textcolor{darkgreen}{High} & \textcolor{darkgreen}{High}\\


\bottomrule
\end{tabular}
% }
\end{table}

% \begin{table}[!t]
% \centering
% \caption{Qualitative comparison between different paradigms. \ml{What are the drawbacks of existing designs?}}
% \label{tab:Qualitative comparison}
% \begin{tabular}
% {
% >{\raggedright\arraybackslash}p{0.6cm} >
% {\raggedright\arraybackslash}p{1.2cm} >{\raggedright\arraybackslash}p{0.8cm} >{\raggedright\arraybackslash}p{1.4cm} >
% {\raggedright\arraybackslash}p{0.8cm} >{\raggedright\arraybackslash}p{1.2cm}
% }
% \toprule
% \textbf{Work}        & \textbf{Support. Model} & \textbf{Quant.} & \textbf{Arch.} & \textbf{Latency} & \textbf{Support. Operat.}\\
% \midrule
% \cite{understanding} & Transformer & W4A8 & Spatial & Low & MM,NL\\
% \cite{DFX} & Transformer & FP16 & Temporal & High & MM,NL\\
% \cite{Flightllm} & Transformer & W3.5A8 & Temporal & High & MM,NL\\
% % FACT & Transformer & Int4/Int8 & Temporal & High & MM,non-linear\\
% Ours & Mamba & W4A4 & Partial Spat. & Low & MM,EM,NL\\
% \bottomrule
% \end{tabular}
% \end{table}


% \begin{table}[]
% \begin{tabular}{|l|l|l|l|l|}
% \hline
% Architecture & Optimation        & Quantization  & Architecture    & Supported Operation \\ \hline
% Un           & Spatial Unroll    & W8A8          & Spatial         & MM,Attn,non-linear  \\ \hline
% DFX          & Model Parallelism & FP16          & Temporal        & MM,VV,non-linear    \\ \hline
% FlightLLM    & Sparsity Flexity  & W4A4          & Temporal        & MM,MV,non-linear    \\ \hline
% FACT         & Eager Prediction  & Mixed         & Temporal        & MM,Attn,FFN         \\ \hline
% Our          & SSM Pipeline+FHT  & W4A4+Rotation & Partial Spatial & MM,SSM,FHT,RMS      \\ \hline
% \end{tabular}
% \end{table}