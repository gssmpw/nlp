\section{experiments}
\label{sec:experiments}

\subsection{Experiment Setup}
\label{subsec:Evaluation Setup}


\textbf{Algorithm}
We evaluate our proposed quantization algorithm
on the Mamba model family~\cite{dao2024transformers}.
We quantize the entire model shown in Fig.~\ref{subfig: quantization_algorithm}
for hardware efficiency.
We use per-channel weight quantization 
and per-token activation quantization for 8-bit weight and activation quantization (denoted as W8A8).
We use per-group weight and activation (group size=128) quantization for 4-bit quantization (denoted as W4A4).
We evaluate the perplexity
and the zero-shot accuracy on six tasks:
LAMBADA~\cite{radford2019language}, 
HellaSwag~\cite{zellers2019hellaswag},
PIQA~\cite{bisk2020piqa},
Arc (Easy and Challenge)~\cite{clark2018think},
Winogrande~\cite{sakaguchi2021winogrande},
and OpenbookQA~\cite{mihaylov2018can}
using lm-eval-harness~\cite{eval-harness}.

\textbf{Hardware}
% \textbf{FPGA Platforms and Implementation}
% We evaluated our design on two FPGA platforms by HLS: Xilinx Versal VCK190 and Alveo U280. 
We use two FPGA platforms for evaluation: Xilinx Versal VCK190 and Alveo U280. 
% Versal VCK190 is equipped with 8GB of LPDDR memory for programmable logic (PL), offering a bandwidth of 25.6GB/s (actual tested bandwidth is approximately 12GB/s). 
% Alveo U280 FPGA features two types of memory: 8GB of HBM with a bandwidth of 460GB/s and 32GB of DDR with a bandwidth of 38GB/s.
We implement LightMamba on VCK190 using Vitis HLS
and Vivado Design FLow.
Fig.~\ref{fig:FPGA_layout} shows the layout
of our implementation on VCK190 FPGA.
% The FPGA layout is elaborated with each module annotated in Fig.~\ref{}.
We measure the throughput on-board using the PYNQ framework
and power consumption with the Xilinx BEAM tool.
For U280 evaluation, we develop a cycle-accurate simulator, 
which has been verified through HLS emulation using Vitis 2023.2.
We choose NVIDIA RTX 2070 and RTX 4090 as our GPU baselines.
% We conduct evaluations on the selected model with [6] as the GPU-naive design.
We use NVIDIA system management interface for power 
measurements. 
Table ~\ref{tab:compare_with_prior art work} shows the hardware parameters of FPGA and GPU, as well as the detailed hardware utilization of LightMamba.
% For throughput measurement, we utilized the PYNQ framework to facilitate the deployment of the design on the FPGA. 
% For power measurement on VCK190, we used the BEAM (Board Evaluation And Management) tool provided by Xilinx.
% We implemented LightMamba on a real system using VCK190 FPGAs, with two variants: Lite-4bit and Lite-8bit. 
% For U280 evaluation, we develop a cycle-accurate simulator called Full-4bit, which has been verified through HLS emulation using Vitis 2023.2. All inplementation are shown in Table 2.
% \textbf{Metrics}
% For throughput measurement, we utilized the PYNQ framework to facilitate the deployment of the design on the FPGA. For power measurement on VCK190, we used the BEAM (Board Evaluation And Management) tool provided by Xilinx.


\subsection{Evaluation Result}
\label{subsec:Evaluation result}

% \ml{Need to highlight the key results with bold or italic font.$\checkmark$}

\textbf{Algorithm Evaluation}
We compare our quantization algorithm 
with the prior art weight-activation PTQ methods
SmoothQuant (SQ)~\cite{xiao2023smoothquant}
and Outlier Suppression+ (OS+)~\cite{wei2023outlier} for Transformer-based LLM.
Note that these methods can only be applied to linear layers.
We re-implement them on Mamba
and use 128 random samples from WikiText2~\cite{merity2016pointer} dataset as the calibration data.
We evaluate our method LightMamba which quantizes only linear layers,
and LightMamba* which quantizes all modules including SSM.
As shown in Table~\ref{tab:acc_result}, for W8A8 quantization,
LightMamba and LightMamba* have negligible accuracy loss compared to the FP16 model.
Although OS+ has better perplexity on the Lambada dataset, it collapses on W4A4.
While our methods LightMamba and LightMamba* outperform all other methods on W4A4,
improving the perplexity by 1.78 and 1.91 compared to the prior art method SQ, respectively.

\begin{table*}[!tb]
    \centering
    \caption{Performance comparison of different methods on Mamba2-2.7B. 
    % % We use the same quantization granularity for all models, i.e. per-token for W8A8 and per-group for W4A4.
    Only LightMamba* quantizes the entire model including SSM while others only quantize linear layers.
    % % \ml{Add the reference for RTN, SQ, OS+, and also make it clear that some works do not quantize the SSM block.}
    The \textbf{bold} denotes the best and 
    the \underline{underlined} denotes the second-best.
    }
    \label{tab:acc_result}
    \scalebox{0.9}{
    \vspace{3pt}
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
    \toprule
   \multirow{2}{*}{Method} & \multirow{2}{*}{Bit-precision} &LAMBADA &LAMBADA &HellaSwag & PIQA &Arc-E &Arc-C  &Winogrande  &OpenbookQA &Average \\
   &&ppl ↓&acc ↑&acc ↑&acc ↑&acc ↑&acc ↑&acc ↑ &acc ↑ &acc ↑\\
    \midrule
    FP16 & - &4.10 &69.7 &66.6 &76.4 &69.6 &36.4 &64.0 &38.8 &60.2 \\
      \midrule
      
     RTN &W8A8 &4.26 &68.8 &66.1 &75.8 &68.4 &36.4 &63.6 &38.4 &59.6\\
     SQ &W8A8 &4.28 &68.2 &66.0 &75.9 &69.1 &37.0 &63.4 &38.2 &59.7\\
     OS+ &W8A8 &\textbf{4.01} & 69.9 &66.2 &76.4 &69.5 &36.5 &63.4 &39.0 &\underline{60.1} \\
     LightMamba &W8A8 &4.07 &69.7 &66.5 &76.1 &69.3 &36.9 &64.0 &38.8 &\textbf{60.2}\\
     LightMamba* &W8A8 &\underline{4.03} &70.2 &66.2 &76.1 &69.4 &36.1 &64.6 &38.6 &\textbf{60.2} \\

    \midrule
     RTN &W4A4 &17.46 &37.7 &62.6 &70.1 &60.1 &34.5 &57.7 &38.2 &51.6 \\
     SQ &W4A4 &8.26 &53.4 &64.0 &73.6 &63.7 &35.1 &59.4 &39.0 &55.5 \\
     OS+ &W4A4 &$> 100$ & 0.0 &27.7 &54.5 &30.8 &24.7 &48.8 &25.6 &30.3 \\
     LightMamba &W4A4 &\underline{6.48} &57.3 &62.7 &73.5 &65.5 &35.3 &60.7 &37.6 &\textbf{56.3}\\
     LightMamba* &W4A4 &\textbf{6.35} &59.6 &62.4 &74.4 &64.7 &34.3 &59.9 &36.4 &\underline{55.9} \\
     
    \bottomrule
    \end{tabular}
    }
    \vspace{-10pt}
\end{table*}

\begin{figure}[t]
    \centering    
    \includegraphics[width=0.7\columnwidth]{fig/layout_new2.pdf}
    \caption{LightMamba implementation layout on VCK190.}
    \vspace{-10pt}
    \label{fig:FPGA_layout}
\end{figure}

\textbf{Hardware Evaluation}
% We compared LightMamba with prior works to demonstrate its superiority, as shown in Table 2. 
We compare the decoding throughput of GPUs, prior art accelerators, and our LightMamba on different output sequence lengths in Fig.~\ref{fig: model size}(a). 
As prior art accelerators have not supported Mamba,
we compare their performance when running Transformer-based LLMs.
% Therefore, we can only compare their performance when running Transformer models.
% The models and parameters used in the evaluation are indicated in the figures.
Since these works did not provide throughput data for long sequence length,
we simulated their performance based on the parameters in each paper.
On VCK190, LightMamba achieves the practical throughput of 3.61 and 7.21 tokens/s for
W8A8 and W4A4, respectively.
% Due to bandwidth limitations on the VCK190,
We also simulate LightMamba on U280 for fair comparison. 
The throughput of LightMamba achieves \textbf{93} tokens/s, which outperforms RTX 2070 by $\textbf{1.43}\times$ on average. LightMamba achieves more significant acceleration on long sequences as Mamba only records hidden states of a fixed size.
% . This is due to MAMBA’s advantageous characteristic of having latency that does not increase with sequence length.
% in Table~\ref{tab:compare_with_accelerator}. 
% FlightLLM, implemented on the U280 platform, achieved a throughput of 55 tokens/s when running the LLaMA2-7B model, while DFX,  on the U280 platform, reached a throughput of 93.1 tokens/s when running the GPT-2-345M model.
% We measured key metrics such as throughput, resource consumption, and GOPs. 
% On VCK190, LightMamba achieves the practical throughput of 3.61 and 7.21 tokens/s for
% W4A4 and W8A8 respectively.
% Due to bandwidth limitations on the VCK190, we simulate LightMamba on U280 for fair comparison with FlightLLM and DFX.
% Our throughput achieves \textbf{93} tokens/s, which outperforms FlightLLM and DFX by $1.7\times$ 
% and $1.6 \times$ respectively.
% LightMamba also demonstrates better resource efficiency.
% The GOPs/DSP of our design are $\textbf{3.7}\times$ and $\textbf{9.1}\times$ higher than FlightLLM and DFX.

We compare the energy efficiency (Tokens/J) of LightMamba and GPU on different model sizes in Fig.~\ref{fig: model size}(b).
LightMamba on VCK190 consistently outperforms GPUs and achieves on average $\textbf{6.06}\times$ and $\textbf{4.65}\times$ improvement over RTX 2070 and 4090 GPU, respectively.
For small Mamba models, LightMamba achieves more energy saving as our design reduces the overhead of the SSM layer,
which is more costly for small models.
% an average of 4.84×/3.36× energy efficiency compared to 2070 Ti and 4090. 
% LightMamba performs better with model size decreasing, which is because our design reduces the overhead of SSM, which is more costly in smaller models show in Fig.~\ref{fig:model_arch_and_ssm_propotion}.
% which constitutes a larger portion of computation in these models as show in Fig.\ref{model_arch_and_ssm_propotion}.


% % is a 1.7x improvement over FlightLLM and a 1.6x improvement compared to DFX on FPGA.
% Due to variations in the volume and structure of model parameters, we utilize normalized data metrics such as GOPs/DSP for comparing performance across different works.
% % and GOPs/BRAM to assess the utilization efficiency of on-chip storage. 
% As shown in Table 2, compared to FlightLLM and DFX, our GOPs/DSP are 3.7 times and 9.1 times higher, respectively, which can be attributed to our DSP utilization rate reaching 96.9\%. 
% % Moreover, our GOPs/BRAM is 8.4 times greater than that of FlightLLM and 34.1 times that of DFX. This significant reduction in on-chip storage consumption results from our implementation of fine-grained tiling and fusion. 
% Conversely, DFX, having not employed quantization, exhibits both higher performance and resource consumption.


 
% \begin{table}[t]
% \centering
% \caption{Hardware parameters of FPGA and GPU platforms\ml{717 GOPs of FlightLLM considers the sparsity.}}
% \label{tab:hardware parameters}
% \begin{tabular}{lcccc}
% \toprule
% % & \multicolumn{2}{c|}{\textbf{FPGA}} & \multicolumn{2}{c} {\textbf{GPU}}\\ \midrule
% % \cmidrule(lr){2-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \\
% & \textbf{FPGA} & \textbf{FPGA} & \textbf{GPU}  & \textbf{GPU} \\
% \midrule
% \multirow{2}{*}{\textbf{Platforms}} & Versal & Alveo  & NVIDIA & NVIDIA\\
% & VCK190  & U280  & RTX 2070 &RTX 4090\\
% \midrule
% \textbf{Frequency}    & 400MHz & 200MHz  & 1.62GHz & 2.52GHz \\
% \midrule
% \textbf{Bandwidth}    & 38GB/s  & 460GB/s & 468GB/s & 1008GB/s     \\
% \midrule
% \textbf{Memory} & 8GB  & 8GB  & 8GB    & 24GB       \\
% \bottomrule
% \end{tabular}
% \end{table}




% \begin{table}[t]
% \centering
% \caption{Hardware experimental results\ml{717 GOPs of FlightLLM considers the sparsity.}}
% \label{tab:resource utilization}
% \resizebox{0.5\textwidth}{!}{
% \begin{tabular}
% {lccccccc}
% % {

% % p{0.75cm}<{\centering}
% % p{0.8cm}<{\centering}
% % p{1.05cm}<{\centering}
% % p{0.35cm}<{\centering}
% % p{0.35cm}<{\centering}
% % p{0.35cm}<{\centering}
% % p{0.55cm}<{\centering}
% % p{0.55cm}<{\centering}
% % }
% \toprule
% \textbf{Device}& \textbf{Precision} & \textbf{Throughput}  & \textbf{LUT}  & \textbf{FF} &\textbf{DSP}&\textbf{BRAM}&\textbf{URAM}\\
% \midrule
% VCK190   & W8A8  &3.61 & 138k  & 168k  & 172 & 99&0\\
% VCK190   & W4A4  &7.21 & 124k  & 163k  & 172 & 97&0\\
% U280   & W4A4  &\textbf{93} & 570k  & 708k  & 1108 & 97&0\\
% \midrule
% RTX2070   & FP16  &65 & -  & -  & - & -&-\\
% RTX4090   & FP16  &138 & -  & -  & - & -&-\\
% \bottomrule
% \end{tabular}
% }
% \end{table}


\begin{table}[!t]
\centering
\caption{Hardware comparison with GPU.}
%\ml{717 GOPs of FlightLLM considers the sparsity.}}
\label{tab:compare_with_prior art work}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{l|ccc|cc}
\toprule
& \multicolumn{3}{c|}{\textbf{LightMamba}} & \multicolumn{2}{c} {\textbf{GPU Baseline}}\\ 
\midrule
\textbf{Platforms}& VCK190 & VCK190  & U280  & RTX 2070 &RTX 4090\\
\midrule
% \rowcolor{gray!20}
\textbf{Frequency}    & 400MHz & 400MHz & 200MHz  & 1.62GHz & 2.52GHz \\
% \rowcolor{gray!20}
\textbf{Bandwidth}    & 12GB/s  & 12GB/s  & 460GB/s & 468GB/s & 1008GB/s     \\
\midrule
% \textbf{Memory} & 8GB  & 8GB  & 8GB  & 8GB    & 24GB       \\
\textbf{Precision}   & W4A4   & W8A8  & W4A4  & FP16 & FP16\\

\textbf{LUT}          & 107k  & 111k  & 297k    & -   & -    \\
\textbf{FF}           & 130k   & 134k   & 394k    & -   & -     \\
\textbf{DSP}          & 228      & 228      & 1164     & -   & - \\
\textbf{BRAM}        & 912       & 914       & 912      & -   &  -    \\
\textbf{URAM}         & 61        & 61        & 61       & -    &   -   \\
\hline
\rowcolor{gray!20}
\textbf{Througput}    & 7.21     & 3.61      & \textbf{93}      & 65     & 138     \\

\rowcolor{gray!20}
\textbf{Energy Eff.}     & \textbf{2.25}     & \textbf{1.45}    & -    & 0.371  & 0.484     \\
\bottomrule
\end{tabular}
}
\vspace{-10pt}
\end{table}
% \textbf{Hardware Evaluation}
% % We compared LightMamba with prior works to demonstrate its superiority, as shown in Table 2. 
% We compare the decoding throughput of GPUs, LightMamba, and other prior art accelerators on different output sequence lengths. As prior art accelerators have not supported Mamba,
% we compare their performance when running Transformer-based LLMs.
% % Therefore, we can only compare their performance when running Transformer models.
% % The models and parameters used in the evaluation are indicated in the figures.
% Since these works did not provide throughput data for such long sequence lengths,
% we simulated the performance degradation for long sequences based on the parameters in each paper.
% On VCK190, LightMamba achieves the practical throughput of 3.61 and 7.21 tokens/s for
% W8A8 and W4A4, respectively.
% % Due to bandwidth limitations on the VCK190,
% We also simulate LightMamba on U280 for fair comparison. Fig.~\ref{fig: model size}(a) show that our throughput achieves \textbf{93} tokens/s, which outperforms RTX2070 by $\textbf{1.43}\times$ on average. LightMamba achieves more significant acceleration on long sequences as Mamba only records hidden states of a fixed size.
% % . This is due to MAMBA’s advantageous characteristic of having latency that does not increase with sequence length.
% % in Table~\ref{tab:compare_with_accelerator}. 
% % FlightLLM, implemented on the U280 platform, achieved a throughput of 55 tokens/s when running the LLaMA2-7B model, while DFX,  on the U280 platform, reached a throughput of 93.1 tokens/s when running the GPT-2-345M model.
% % We measured key metrics such as throughput, resource consumption, and GOPs. 
% % On VCK190, LightMamba achieves the practical throughput of 3.61 and 7.21 tokens/s for
% % W4A4 and W8A8 respectively.
% % Due to bandwidth limitations on the VCK190, we simulate LightMamba on U280 for fair comparison with FlightLLM and DFX.
% % Our throughput achieves \textbf{93} tokens/s, which outperforms FlightLLM and DFX by $1.7\times$ 
% % and $1.6 \times$ respectively.
% % LightMamba also demonstrates better resource efficiency.
% % The GOPs/DSP of our design are $\textbf{3.7}\times$ and $\textbf{9.1}\times$ higher than FlightLLM and DFX.

% We compare the energy efficiency (Tokens/J) of LightMamba and GPU on different model sizes in Fig.~\ref{fig: model size}(b).
% LightMamba on VCK190 consistently outperforms GPUs and achieves on average $\textbf{8.98}\times$ and $\textbf{6.24}\times$ improvement over RTX 2070 and 4090 GPU, respectively.
% For small Mamba models, \method achieves more energy saving as our design reduces the overhead of the SSM layer,
% which is more costly for small models.
% % an average of 4.84×/3.36× energy efficiency compared to 2070 Ti and 4090. 
% % LightMamba performs better with model size decreasing, which is because our design reduces the overhead of SSM, which is more costly in smaller models show in Fig.~\ref{fig:model_arch_and_ssm_propotion}.
% % which constitutes a larger portion of computation in these models as show in Fig.\ref{model_arch_and_ssm_propotion}.


% % % is a 1.7x improvement over FlightLLM and a 1.6x improvement compared to DFX on FPGA.
% % Due to variations in the volume and structure of model parameters, we utilize normalized data metrics such as GOPs/DSP for comparing performance across different works.
% % % and GOPs/BRAM to assess the utilization efficiency of on-chip storage. 
% % As shown in Table 2, compared to FlightLLM and DFX, our GOPs/DSP are 3.7 times and 9.1 times higher, respectively, which can be attributed to our DSP utilization rate reaching 96.9\%. 
% % % Moreover, our GOPs/BRAM is 8.4 times greater than that of FlightLLM and 34.1 times that of DFX. This significant reduction in on-chip storage consumption results from our implementation of fine-grained tiling and fusion. 
% % Conversely, DFX, having not employed quantization, exhibits both higher performance and resource consumption.


 
% % \begin{table}[t]
% % \centering
% % \caption{Hardware parameters of FPGA and GPU platforms\ml{717 GOPs of FlightLLM considers the sparsity.}}
% % \label{tab:hardware parameters}
% % \begin{tabular}{lcccc}
% % \toprule
% % % & \multicolumn{2}{c|}{\textbf{FPGA}} & \multicolumn{2}{c} {\textbf{GPU}}\\ \midrule
% % % \cmidrule(lr){2-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \\
% % & \textbf{FPGA} & \textbf{FPGA} & \textbf{GPU}  & \textbf{GPU} \\
% % \midrule
% % \multirow{2}{*}{\textbf{Platforms}} & Versal & Alveo  & NVIDIA & NVIDIA\\
% % & VCK190  & U280  & RTX 2070 &RTX 4090\\
% % \midrule
% % \textbf{Frequency}    & 400MHz & 200MHz  & 1.62GHz & 2.52GHz \\
% % \midrule
% % \textbf{Bandwidth}    & 38GB/s  & 460GB/s & 468GB/s & 1008GB/s     \\
% % \midrule
% % \textbf{Memory} & 8GB  & 8GB  & 8GB    & 24GB       \\
% % \bottomrule
% % \end{tabular}
% % \end{table}




% % \begin{table}[t]
% % \centering
% % \caption{Hardware experimental results\ml{717 GOPs of FlightLLM considers the sparsity.}}
% % \label{tab:resource utilization}
% % \resizebox{0.5\textwidth}{!}{
% % \begin{tabular}
% % {lccccccc}
% % % {

% % % p{0.75cm}<{\centering}
% % % p{0.8cm}<{\centering}
% % % p{1.05cm}<{\centering}
% % % p{0.35cm}<{\centering}
% % % p{0.35cm}<{\centering}
% % % p{0.35cm}<{\centering}
% % % p{0.55cm}<{\centering}
% % % p{0.55cm}<{\centering}
% % % }
% % \toprule
% % \textbf{Device}& \textbf{Precision} & \textbf{Throughput}  & \textbf{LUT}  & \textbf{FF} &\textbf{DSP}&\textbf{BRAM}&\textbf{URAM}\\
% % \midrule
% % VCK190   & W8A8  &3.61 & 138k  & 168k  & 172 & 99&0\\
% % VCK190   & W4A4  &7.21 & 124k  & 163k  & 172 & 97&0\\
% % U280   & W4A4  &\textbf{93} & 570k  & 708k  & 1108 & 97&0\\
% % \midrule
% % RTX2070   & FP16  &65 & -  & -  & - & -&-\\
% % RTX4090   & FP16  &138 & -  & -  & - & -&-\\
% % \bottomrule
% % \end{tabular}
% % }
% % \end{table}


% \begin{table}[!t]
% \centering
% \caption{Hardware comparison with GPU.}
% %\ml{717 GOPs of FlightLLM considers the sparsity.}}
% \label{tab:compare_with_prior art work}
% \resizebox{0.45\textwidth}{!}{
% \begin{tabular}{l|ccc|cc}
% \toprule
% & \multicolumn{3}{c|}{\textbf{LightMamba}} & \multicolumn{2}{c} {\textbf{GPU Baseline}}\\ 
% \midrule
% \textbf{Platforms}& VCK190 & VCK190  & U280  & RTX 2070 &RTX 4090\\
% \midrule
% % \rowcolor{gray!20}
% \textbf{Frequency}    & 400MHz & 400MHz & 200MHz  & 1.62GHz & 2.52GHz \\
% % \rowcolor{gray!20}
% \textbf{Bandwidth}    & 12GB/s  & 12GB/s  & 460GB/s & 468GB/s & 1008GB/s     \\
% \midrule
% % \textbf{Memory} & 8GB  & 8GB  & 8GB  & 8GB    & 24GB       \\
% \textbf{Precision}   & W4A4   & W8A8  & W4A4  & FP16 & FP16\\

% \textbf{LUT}          & 124k  & 138k  & 570k    & -   & -    \\
% \textbf{FF}           & 163k   & 168k   & 708k    & -   & -     \\
% \textbf{DSP}          & 172      & 172      & 1108     & -   & - \\
% \textbf{BRAM}        & 97       & 99       & 97      & -   &  -    \\
% \textbf{URAM}         & 0        & 0        & 0       & -    &   -   \\
% \hline
% \rowcolor{gray!20}
% \textbf{Througput}    & 7.21     & 3.61      & \textbf{93}      & 65     & 138     \\

% \rowcolor{gray!20}
% \textbf{Energy Eff.}     & \textbf{2.86}     & \textbf{1.45}    & -    & 0.371  & 0.484     \\
% \bottomrule
% \end{tabular}
% }
% \end{table}



% \begin{table}[t]
% \centering
% \caption{Comparison with prior art FPGA-based accelerators. \ml{717 GOPs of FlightLLM considers the sparsity.}}
% \label{tab:compare_with_accelerator}
% \begin{tabular}{l|ccc|c|c}
% \toprule
% & \multicolumn{3}{c|}{\textbf{LightMamba}} & \multicolumn{1}{c|} {\textbf{FlightLLM}} & \multicolumn{1}{c}{\textbf{DFX}}\\ \midrule
% % \cmidrule(lr){2-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \\
%   \rowcolor{gray!20}  
% Model       & \multicolumn{3}{c|}{Mamba2-2.7B} & \multicolumn{1}{c|} {Llama2-7B} & \multicolumn{1}{c}{GPT-2-1.5B}\\
% Platforms   & VCK190 & VCK190  & U280  & U280 & U280\\
% Precision   & W4A4   & W8A8  & W4A4  & W4A4 & FP16\\
% \midrule
% % Accuracy    &  70  & 71  & 70  & -- & --\\
% % \midrule
% %   \rowcolor{gray!20}
% % Frequency    & 400MHz   & 400MHz   & 200MHz  & 225MHz & 200MHz \\
% %   \rowcolor{gray!20}
% % Througput    & 7.21     & 3.61      & 93      & 55     & 58     \\
% LUT          & 124k  & 138k  & 570k    & 574k   & 520k    \\
% FF           & 163k   & 168k   & 708k    & 943k   & 1107k     \\
% DSP          & 172      & 172      & 1108     & 6345   & 3533 \\
% BRAM         & 97       & 99       & 97      & 1252   &  1192    \\
% URAM         & 0        & 0        & 0       & 792    &   104   \\
% % GOPs         & 45.7    & 23.2       & 464     & 717    & 162     \\
%   % \hline
% %   \rowcolor{gray!20}
% % GOPs/kLUT    & 0.367     & 0.167     & \textbf{0.814}    & 1.24   &  0.312    \\
% %   \rowcolor{gray!20}
% % GOPs/DSP     & 0.266     & 0.134    & \textbf{0.418}    & 0.113  & 0.0458     \\
% %   \rowcolor{gray!20}
% % GOPs/BRAM    & 0.47     & 0.23    & \textbf{4.78}    & 0.57  & 0.14     \\
% \bottomrule
% \end{tabular}
% \end{table}

\begin{figure}[t]
    \centering
   \vspace{-10pt} \includegraphics[width=1\columnwidth]{fig/modelsize_new2.pdf}
    % \caption{Comparison of energy efficiency improvement to Mamba-GPU and LightMamba on Mamba models with different model size.}
    \caption{
    % (a) Throughput comparison of LightMamba, GPU and prior art accelerators with different output sequence length.
    % (b) Energy efficiency comparison of LightMamba on VCK190 and GPUs for different model sizes.
    (a) Throughput with different output sequence length.
    (b) Energy efficiency with different model sizes.
    }
    \label{fig: model size}
\end{figure}


% \textbf{Comparison with GPUs}
% We selected the NVIDIA RTX2080 and RTX4090 as baseline GPUs, with the RTX2080 closely matching the U280 FPGA in memory bandwidth for fair comparison. Throughput tests used the open-source MAMBA2-2.7B model, and power measurements were taken with NVIDIA-SMI.

% As shown in Table 2, LightMAMBA on U280 achieves 1.5x and 1.1x higher throughput than the RTX2080 and RTX4090, respectively. Table 3 shows that LightMAMBA on VCK190 offers 2.67x higher energy efficiency and better bandwidth utilization. This is due to LightMAMBA’s custom units that parallelize SSM and matrix multiplication, maximizing bandwidth use.

\subsection{Ablation Study}
\label{subsec:Ablation Study}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{fig/ablation_uram.pdf}
    \caption{Impact of different techniques on the computation throughput, accuracy and URAM usage.}
    \vspace{-10pt}
    \label{fig:ablation}
\end{figure}

We now conduct the ablation study to show the accuracy and efficiency impact of different techniques in LightMamba.
As shown in Fig.~\ref{fig:ablation}, through weight and activation quantization, the throughput can be increased
from 2.23 to 5.32 tokens/s. With the rotation-assisted quantization algorithm and customized HTU, we
boost the accuracy of quantized Mamba by 4.3\% with almost the same throughput. 
Our computation re-ordering technique
improves the hardware utilization and raises the throughput further to 7.21.
Finally, through fine-grained tiling and fusion, the on-chip memory consumption is reduced significantly by 
4$\times$ from 246 to 61.

% To isolate and quantify the impact of our methods, we compare the changes in throughput, network accuracy, GOPs/DSP, and BRAM usage by gradually incorporating our optimizations on VCK190 W4A4 version, as shown in Fig.~\ref{fig:ablation}. 
% % The graph tracks the incremental effects of each optimization, illustrating the network's progression from its initial state to the fully optimized version.

% The original network is FP16. By employing 4-bit weight quantization and 4-bit activation quantization, the throughput of our accelerator increased from 2.84 tokens/s to 6.76 tokens/s, and the GOPs/DSP ratio rise from 0.0824 to 0.196. 
% However, the accuracy drop from 60.2 to 51.6. 
% To mitigate this loss in accuracy, we introduced rotation quantization, which elevates the accuracy to 55.5. Nonetheless, the naive implementation of matrix multiplication using Hadamard hardware results in increased overall latency and reduces throughput to 3.71. 
% By adopting the FHT algorithm and its hardware implementation, we improve the throughput to 6.41. 
% Our computation reordering enhances the hardware utilization and raises the throughput to 7.21. 
% Additionally, through fine-grained tiling and fusion, the consumption of on-chip memory was reduced from 221 to 97.

% \subsection{Performance on small model}
% \label{subsec:Performance on small model}
% We adjusted the model size to implement LightMAMBA on smaller models alongside a naive FPGA implementation, measuring throughput and improvement rates. As shown in Figure 5, LightMAMBA's performance gains increase as model size decreases. This is due to the quadratic growth of linear layer complexity with channel numbers, while SSM complexity grows linearly. For smaller models, SSM latency becomes more significant, highlighting LightMAMBA's advantages.




