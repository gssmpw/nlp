\section{Introduction}
\label{sec:introduction}

State space models (SSMs) like Mamba \cite{gu2023mamba,dao2024transformers} have recently been proposed
and emerged as a promising class of architectures as foundation models. Compared to existing Transformer-based
large language models (LLMs) \cite{touvron2023llama,touvron2023llama2,bubeck2023sparks,jiang2024mixtral},
Mamba only requires linear computational complexity with the increase of input sequence length while demonstrating
superior performance on various downstream tasks. 
% Hence, Mamba is promising for long context
% understanding and has recently attracted much attention.

% \ml{Should we put the Mamba block diagram in the introduction? \checkmark}

The basic architecture 
of Mamba~\cite{dao2024transformers} is shown in Fig.~\ref{fig:model_arch}. Each Mamba block mainly consists of two linear projection layers,
i.e., input projection and output projection, a 1-dimensional convolution (conv1d) layer, and an SSM layer. The computation of
Mamba involves a prefill stage that summarizes the input prompts and an autoregressive decode stage to produce the output tokens.
The decode of Mamba only generates and stores a fixed-size hidden state instead of a key-value cache in Transformers
that grows linearly with the sequence length. While Mamba has different architectures, we focus on the
Mamba2 \cite{dao2024transformers} architecture and all subsequent references to Mamba refer to Mamba2 unless otherwise specified.

\begin{figure}[!tb]
\centering
\includegraphics[width=\linewidth]{fig/model_arch_new_v2.pdf}
\caption{The model architecture of Mamba2 and the detailed
computation graph of the SSM layer.} 
\vspace{-10pt}
\label{fig:model_arch}
\end{figure}

% \ml{Also, change all ``Mamba2'' to ``Mamba'' afterwards.}

Though promising, how to accelerate Mamba processing, especially on reconfigurable platforms like FPGA, remains
an open question. We observe existing works on FPGA-based LLM acceleration \cite{Flightllm,DFX} cannot be directly applied
due to the following challenges. 
First, while FPGA-based acceleration benefits from low-bit-precision quantization,
Mamba quantization is more challenging than Transformer-based LLMs due to scattered activation outliers. 
Specifically, for different input tokens, the activation outliers appear in different channels.
Naively applying existing quantization algorithms \cite{xiao2023smoothquant,wei2022outlier,wei2023outlier} incurs significant accuracy degradation.
Second, an SSM layer involves excessive element-wise computation.
While existing works \cite{li2024evaluating,pierro2024mamba,li2024marca} often ignore the quantization of SSM layers
and suffer from a high hardware cost, directly quantizing the SSM layer also introduces significant re-quantization
overhead. Third, the SSM layer involves complex computation and data dependency, which prevents unfolding the SSM
computation spatially to improve the acceleration throughput.

To solve these challenges, we propose LightMamba, the first FPGA-based Mamba acceleration framework that co-designs
the quantization algorithm and accelerator architecture. 
We propose a rotation-assisted post-training quantization (PTQ)
algorithm for Mamba to remove the scattered outliers, which enables us to quantize the model to 4-bit with minimum 
accuracy degradation and significantly improve the communication efficiency with off-chip memory. 
For the SSM layer,
we propose a FPGA-friendly power-of-two (PoT) quantization scheme to realize re-quantization with simple shifting for better
computation efficiency. 
For the FPGA accelerator architecture, we design customized rotation modules for the PTQ algorithm
and further propose computation reordering as well as fine-grained tiling and fusion to improve the hardware utilization.
Our main contributions can be summarized as follows:
\begin{itemize}
    \item We propose the first PTQ algorithm for the entire Mamba model.
    Through rotation-assisted quantization, we quantize Mamba to 4-bit with plausible accuracy.
    The SSM layer is also quantized with FPGA-friendly PoT scheme for better computation efficiency.
    % For example, we quantize Mamba2-2.7B model to W8A8 and W4A4, preserving 99\% and 92\% zero-shot accuracy of the FP counterpart, respectively. \ml{Promising performance is vague. $\checkmark$}
    \item We propose the first FPGA-based Mamba accelerator. The architecture features a customized rotation module
    co-designed with the proposed quantization algorithm. We also propose computation reordering and
    fine-grained scheduling to improve the computation throughput and reduce the on-chip memory usage.
    \item We implement LightMamba on Xilinx Versal VCK190 FPGA achieving 7.21 tokens/s and 4.65$\sim$6.06$\times$ higher energy efficiency over the GPU baseline. 
    % \ml{Update the results here.}
    % and achieve actual throughput of 4.65 tokens/s with W8A8 quantization and 9.16 tokens/s with W4A4 quantization.
    % When evaluated on Alveo U280 FPGA,
    % LightMamba reaches 93 tokens/s, 
    % which is $1.8\times$ that of the state-of-the-art accelerator FlightLLM for Llama.
\end{itemize}



% Large language models (LLMs)
% such as Llama~\cite{touvron2023llama,touvron2023llama2}, GPT-4~\cite{bubeck2023sparks}, and Mixtral~\cite{jiang2024mixtral}
% have demonstrated impressive performance across various natural language tasks.
% % Decoder-only Transformer
% % has been the standard model architecture for LLMs.
% However, these Transformer-based LLMs suffer from
% quadratic computational complexity
% and linear memory cost
% with respect to the sequence length,
% thus suffering from huge memory and computation costs for long sequences.
% To solve this problem,
% % Mamba~\cite{gu2023mamba} and Mamba2~\cite{dao2024transformers}
% Mamba~\cite{gu2023mamba,dao2024transformers} has been proposed.
% Compared to Transformers of the same size, Mamba demonstrates comparable accuracy
% while enjoying linear computation scaling with the sequence length and much better inference efficiency.
% Hence, Mamba has attracted a lot of attention recently.
% % which outperforms Transformers of the same size
% % while enjoying linear scaling in  sequence length during training
% % and much faster inference speed.
% % As selective state space models (SSM),
% % they outperform Transformers of the same size
% % while enjoying fast inference and linear scaling with sequence length.
% % They are also widely adopted for vision tasks~\cite{liu2024vmamba, zhu2024vision, hatamizadeh2024mambavision}.
% However, accelerating Mamba
% both from the quantization algorithm
% and hardware architecture perspectives remains an open problem.

% However, how to deploy and accelerate Mamba on resource-constrained devices remains an open problem.
% For instance, Mamba2-2.7B~\cite{dao2024transformers} requires 5.4G of memory to store the parameters.

% Mamba
% has a prefill stage that summarizes a token from the input prompt
% and a decode stage that generates the output token autoregressive.
% It has a different model structure from the Transformer.
% As shown in Fig.~\ref{fig: quantization_algorithm},
% the Mamba block mainly consists of two linear projection layers, i.e., in
% project and out project layer,
% a conv1d layer,
% and the SSM layer.
% The recurrent mode of SSM enables Mamba 
% for more efficient decoding than Transformer~\cite{gu2023mamba,dao2024transformers}, 
% involving a small fixed-sized hidden state instead of a large KV cache growing with the sequence length.
% In this work, we mainly focus on accelerating the decode stage of Mamba2 on FPGA, since it has better performance than Mamba~\cite{dao2024transformers}.
% As shown in Fig.~\ref{fig:roofline_model}, 
% the decode stage of Mamba on FPGA is typically memory-bound
% since we need to transfer a large amount of parameters from off-chip memory to on-chip.

% and the prefill stage can be executed on CPU or GPU, 
% after which only a small hidden state needs to be transferred to FPGA, 
% rather than a large KV cache.

\begin{comment}

\ml{Should not refer to figures in the background. $\checkmark$}

Mamba has a different model structure from the Transformer,
which results in existing LLM accelerators~\cite{DFX,Flightllm} can not be applied on Mamba.
Mamba block mainly consists of two linear projection layers, i.e., in
project and out project layer,
a 1-d convolution layer,
and the selective state space modeling (SSM) layer.
Like other LLMs, 
it comprises a prefill stage that summarizes a token from the input prompt
and a decode stage that generates the output token autoregressively.
In the decode stage,
the recurrent mode of SSM enables more efficient computation for Mamba compared to Transformers
~\cite{gu2023mamba,dao2024transformers},
% Mamba for more efficient than Transformer~\cite{gu2023mamba,dao2024transformers}, 
since it only computes and stores a small fixed-sized hidden state instead of a large KV cache that grows with the sequence length.
In this work, we mainly focus on accelerating the decode stage of Mamba2 on FPGA, since it performs better than Mamba~\cite{dao2024transformers}
and all subsequent references to Mamba refer to Mamba2 unless otherwise specified.

% We observe accelerating Mamba on FPGA encounters three obstacles.
% First, transferring weights from off-chip memory to on-chip computation core
% is time-consuming, i.e., memory-bound.
% Second, due to the lack of an accurate weight-activation quantization method for Mamba, we can only perform floating-point (FP) calculations on FPGA which is quite expensive.
% % Third, SSM in Mamba requires large computation costs, e.g., xx\% of the entire block,
% % which results in low DSP utilization when executed sequentially after the in project layer.
% Third, low DSP utilization caused by SSM.
% Since SSM has multi-stage computations with diverse computation patterns shown in Fig~\ref{fig: quantization_algorithm},
% unrolling it on FPGA requires large computational resources.
% When executed sequentially with the linear layers,
% it will lead to low digital signal processing block (DSP) utilization due to idle cycles.

\ml{Challenges not very clear. $\checkmark$}

% We observe accelerating Mamba on FPGA encounters two challenges.
% First, decoding of Mamba is typically a memory-bound problem
% since we transfer large amounts of weights from
% off-chip memory to on-chip computation core only generating one token.
% Second, low memory bandwidth utilization due to SSM.
% This is because when calculating SSM only a small hidden state is transferred,
% which results in low bandwidth utilization.


We observe accelerating Mamba on FPGA encounters three challenges.
First, Mamba is difficult to quantize to low bit precision.
Although there are already some works on the weight-activation quantization of Transformer-based LLMs,
applying them to Mamba will cause large quantization error
due to the scattered activation outliers in Mamba.
Second, for the SSM layer,
existing works~\cite{li2024evaluating,pierro2024mamba,li2024marca} ignore to quantize it
and calculate in floating-point (FP) unit~\cite{li2024marca},
which will occupy considerable computational resources on FPGA. 
Third, the complex computations and data dependency of SSM make it
only able to be executed alternately with the linear layers,
which leads to low hardware utilization and consequently low throughput.


To solve these challenges,
we propose LightMamba,
the efficient Mamba acceleration framework,
featuring quantization algorithm and hardware co-design.
We propose a rotation-based post-training quantization (PTQ) method
removing the scattered outliers, 
and quantize weights and activations to 8-bit (W8A8) and 4-bit (W4A4)
with minimal accuracy degradation.
Quantization allows a greater number of weights to be transferred per second
from off-chip to on-chip,
thus increasing the throughput.
% e.g., from $A$ to $C$ by 3.1$\times$ in Fig.~\ref{fig:roofline_model}.
We also quantize the SSM layer,
which enables it to be calculated with low-bit integer arithmetic.
Thus, we can set a higher parallelism for SSM, thereby improving throughput.
For hardware design,
we propose the first FPGA-base accelerator for Mamba.
We propose the computation reordering method
to overlap the computation of the in project layer and the SSM layer,
thus increasing the throughput.
% e.g., from $C$ to $C'$ by 1.4$\times$ in Fig.~\ref{fig:roofline_model}.
Moreover, we observe that the intermediate activations in SSM are the on-chip memory bottleneck
and propose fine-grained tiling and fusion,
decreasing the on-chip memory cost. 

% As shown in Fig.~\ref{fig:roofline_model}, 
% the decode stage of Mamba on FPGA is typically memory-bound
% since we need to transfer a large amount of parameters from off-chip memory to on-chip.
% We study this problem with the Roofline model~\cite{williams2009roofline}.
% As shown in Fig.~\ref{fig:roofline_model}, 
% the decode stage of Mamba on FPGA is memory-bound
% since we take one token as input and generate the next token
% where the entire model's weights are required to be transferred
% from off-chip memory to on-chip.
% Considering an FP16 weight which occupies two Bytes,
% it is only multiplied with one activation, 
% leading to 0.5 compute intensity.


% \begin{figure}[!tb]
% \centering
% \includegraphics[width=0.8\linewidth]{fig/roofline_model_v8.pdf}
% \caption{Roofline model of Mamba inference on FPGA and the improvements with LightMamba.
% } 
% \label{fig:roofline_model}
% \end{figure}


% \ml{No need to provide so many details in the Introduction.}
% To improve the throughput, we first \ding{172} quantize FP16 weights
% to INT4,
% thus the compute intensity is increased by $4\times$
% and the throughput improves from A to B.
% However, B is compute-bound due to the low peak computational performance
% caused by the following two reasons.
% First, since activation is still FP16,
% we should dequantize weight to FP16 and perform FP16 multiplications,
% and the FP16 peak performance is quite low on FPGA.
% Second, the linear layer and SSM layer in Mamba are supported by two circuits.
% When one is executing, the other is idle,
% which results in low computational resource utilization,
% limiting the peak performance.

% To further boost throughput, we \ding{173} quantize activations
% to INT4.
% Therefore, the multiplication can be performed in INT4.
% INT4 peak performance provided by FPGA is much higher than FP16 
% since we can pack two multiplications into one digital signal processing block (DSP)
% or use lookup table (LUT) for multiplications~\cite{guo2024hg}.
% The throughput now is improved from B to C.
% Furthermore, we propose to overlap the computation of the linear layer
% and the SSM layer through our \ding{174} SSM optimization,
% thus significantly improve the computational resource utilization,
% increasing throughput from C to D.
% It is worth noting that D is memory-bound again
% since we improve the peak computational performance by methods \ding{173} and \ding{174}.
% The throughput is increased by $4.4\times$ from A to D.

% To achieve the aforementioned improvements,
% we propose LightMamba,
% the first FPGA-based accelerator for Mamba,
% featuring quantization algorithm and hardware co-design.
% In quantization, 
% we discover that the activation outliers in Mamba exhibit scattered distribution shown in Fig.~\ref{fig:activation_distribution} rather than persist in fixed channels in Transformer-based LLMs~\cite{xiao2023smoothquant, wei2023outlier},
% which poses challenges for quantization.
% To address this challenge,
% we propose the first post-training quantization (PTQ) method for Mamba, dubbed RoMamba,
% inspired by the recent quantization works for Transformers~\cite{ashkboos2024quarot,liu2024spinquant}.
% We study the computational invariance~\cite{ashkboos2024slicegpt} in Mamba,
% and achieve the first accurate fully-quantized Mamba with 8-bit weight and activation (W8A8) and 4-bit weight and activation (W4A4) 
% (\ding{172} and \ding{173} in Fig.~\ref{fig:roofline_model}).

% For hardware design,
% we propose the first end-to-end FPGA-based accelerator for Mamba,
% dubbed LightMamba.
% We propose compute-aware generation reordering 
% to overlap the computation of the linear layer and the SSM layer,
% thus increasing the throughput by $1.8\times$ (\ding{174} in Fig.~\ref{fig:roofline_model}).
% Moreover, we observe the intermediate activations in SSM have large memory consumption 
% and propose fine-grained tiling and fusion,
% decreasing the on-chip memory cost by $2.3\times$. 

Our main contributions can be summarized as follows:
\begin{itemize}
    \item We propose the first accurate PTQ method for the entire Mamba model.
    For example, we quantize Mamba2-2.7B model to W8A8 and W4A4, preserving 99\% and 92\% zero-shot accuracy of the FP counterpart, respectively. \ml{Promising performance is vague. $\checkmark$}
    \item We propose the first end-to-end accelerator for Mamba with two methods optimizing SSM, i.e., compute-aware generation reordering and fine-grained tiling and fusion.
    \item We implement LightMamba on Xilinx Versal VCK190 FPGA
    and achieve actual throughput of 4.65 tokens/s with W8A8 quantization and 9.16 tokens/s with W4A4 quantization.
    When evaluated on Alveo U280 FPGA,
    LightMamba reaches 93 tokens/s, 
    which is $1.8\times$ that of the state-of-the-art accelerator FlightLLM for Llama.
\end{itemize}

\end{comment}