\section{Motivation}
\label{sec:motivation}
% % Although Mamba has received significant attention at the application level,
% % how to deploy and accelerate it on resource-constrained devices remains an open problem.
% % In this work, we study this problem comprehensively from the quantization algorithm to the hardware design for efficient inference of Mamba on hardware accelerators.
% % We discover two main challenges.

% Although Mamba has shown great potential for language modeling 
% and also vision tasks,
% how to deploy it on resource-constrained devices remains an open problem.
% In this work, we propose LightMamba,
% the first FPGA-based accelerator for Mamba,
% featuring quantization and hardware co-design.
% % We identify two main challenges.
% We focus on the decode stage in our FPGA implementation,
% where prefill stage can be executed on CPU or GPU 
% and only a small hidden state $h_t$ needs to be transferred to FPGA, 
% rather than a large KV cache in Transformer-based LLM.
% Performing decode stage on FPGA is memory bound
% since weight transfer from DDR or HBM to the compute core
% is much slower than computation.
% Throughput in decode stage can be estimated as:
% \[
% Throughput =\frac{BW \times BW\_utilization}{b_W \times N_W}, 
% \]
% which is proportional to the bandwidth $BW$ and its utilization
% and inversely proportional to bit-width of weights $b_W$ 
% and the number of weights $N_W$ required to generate one token.
% Given the hardware and model,
% a straightforward way to improve the throughput is to decrease $b_W$.
% Moreover, quantizing activation to low bit precision
% is also significant to save the on-chip memory and computation resource on FPGA.
% However, quantizing Mamba to low bit-width is challenging.

% \textbf{Challenge 1: Low-bit Quantization difficulty of Mamba.}
% % In quantization, the activation outliers make quantization difficult since the outliers stretch the quantization range and leave few quantization levels for most values,
% % leading to large quantization error.
% % As shown in Table~\ref{tab: kurtosis}, activations in Mamba exhibit large kurtosis, 
% % which is widely used to measure the number of outliers in the distribution~\cite{bondarenko2024quantizable,li2024evaluating,liu2024spinquant}.
% % A larger kurtosis indicates more outliers while kurtosis=3 indicates a Gaussian-like distribution.
% % The activations have much larger kurtosis than weight which is 3.42 on average.
% % Among these activations, the hidden state $h$ and the input activation of the out project layer have the largest kurtosis.
% % Moreover, the activation outliers in the out project layer exhibit the non-persistent characteristic as shown in Fig.~\ref{}.
% % This is quite different from Transformer-based LLMs in which outliers persist in fixed channels.
% % These outliers make quantizing Mamba to low bit precision particularly challenging.

% The outliers in weights and activations make quantizing Mamba low-bit quite challenging. 




% \begin{table}[!tb]
% \centering
% \caption{Kurtosis of the activations in Mamba-2.7B including the input activation of the in proj, conv1d, and output proj layers and the activations in SSM.}
% \label{tab: kurtosis}
% \begin{tabular}{l|c|c|c|c|c}
% \hline\hline
% Activation  & In proj & Conv1d & X      & B     & C        \\ \hline
% Kurtosis & 354.5   & 6.4    & 175.6  & 16.4  & 6.7      \\ \hline
% Activation  & $\overline{A} $  & $\overline{B}$  & h      & Y     & Out proj \\ \hline
% Kurtosis & 35.3    & 465.5  & 7319.0 & 925.0 & 2185.0   \\ \hline\hline
% \end{tabular}
% \end{table}


% \textbf{Challenge 2: Significant Runtime and Memory Consumption of SSM.}
% Surprisingly we discover that although SSM has only a small amount of computation, i.e., approximately 1/12 of the Mamba block, 
% it requires significant on-chip memory and latency due to the multi-stage activation-activation multiplications. 
% As shown in Fig~\ref{fig: ssm_propotion},
% when naively deploying Mamba on hardware accelerators, e.g., FPGA,
% the SSM will occupy $66\% \sim 73\%$ memory cost and 
% $22\% \sim 34\%$ runtime of the entire block.
% Although there has been extensive research on linear layers, 
% there is still a lack of studies on SSM.
% Therefore, reducing the runtime and memory cost of SSM is a necessary step toward achieving efficient Mamba.
% % which calls for an urgent solution for efficient inference of Mamba.

% \begin{figure}[!tb]
% \centering
% \includegraphics[width=\linewidth]{fig/ssm_motivation.png}
% \caption{SSM runtime and memory cost proportion of the entire block in Mamba of different sizes.} 
% \label{fig: ssm_propotion}
% \end{figure}


% However, implementing the aforementioned methods is non-trivial and we identify two challenges.
% \textbf{Challenge 1:} Quantizing Mamba is difficult since activations
% have large outlier elements, dubbed outliers, shown in Fig.~\ref{fig:activation_distribution}.
% Outliers that have much larger values than other elements stretch the quantization range and leave few quantization levels for others, leading to large quantization error.
% More importantly, different from the outliers in Transformer-based LLMs
% that persist in fixed channels~\cite{xiao2023smoothquant, wei2023outlier},
% the input activation of the out project layer in Mamba has outliers
% with scattered distribution.
% Thus, existing quantization methods mitigating the outlier problem for Transformer are not suitable.
% \cite{dettmers2022llm,zhao2024atom} keeping outliers in higher precision introduce large computation overhead.
% \cite{xiao2023smoothquant, wei2023outlier} leveraging channel-wise shifting and scaling to migrate quantization difficult from activations to weights leads to large quantization error due to the unique distribution characteristics of outliers in Mamba.
% Recently, rotation-based methods~\cite{ashkboos2024quarot,liu2024spinquant} based on the computational invariance~\cite{ashkboos2024slicegpt} have shown great potential in removing outliers in Transformer.
% However, how to mitigate the effect of outliers in Mamba while keeping the computational
% invariance remains a problem.

% \textbf{Challenge 2:} Overlapping the computation of the linear layer and SSM layer is non-trivial since the input of SSM relies on 
% the output of in project layer as shown in Fig.~\ref{fig: quantization_algorithm}.
% Moreover, the intermediate activation in SSM such as $\Bar{A}, \Bar{B}, h_t   \in \mathbb{R}^{b \times h \times p \times n} $ in Fig.~\ref{fig: quantization_algorithm}
% are $2n \times$ larger than the activation $X \in \mathbb{R}^{b \times d} $ of linear layers,
% where $h \times p = 2d$ and $n=128$ in Mamba. 
% Thus, SSM requires significant on-chip memory costs,
% e.g., $73\% \sim 66\%$ of the entire block from Mamba2-130M to 
% Mamba2-2.7B.

% \begin{table}[!tb]
% \centering
% \caption{Kurtosis of the activations in Mamba-2.7B including the input activation of the in proj, conv1d, and output proj layers and the activations in SSM.}
% \label{tab: kurtosis}
% \begin{tabular}{l|c|c|c|c|c}
% \hline\hline
% Activation  & In proj & Conv1d & X      & B     & C        \\ \hline
% Kurtosis & 354.5   & 6.4    & 175.6  & 16.4  & 6.7      \\ \hline
% Activation  & $\overline{A} $  & $\overline{B}$  & h      & Y     & Out proj \\ \hline
% Kurtosis & 35.3    & 465.5  & 7319.0 & 925.0 & 2185.0   \\ \hline\hline
% \end{tabular}
% \end{table}


% \begin{table}[]
% \begin{tabular}{l|llll}
% \hline
% Architecture             & Pipeline & \multicolumn{2}{l}{Temporal} & \begin{tabular}[c]{@{}l@{}}Temporal+\\ Pipeline\end{tabular} \\ \hline
% Represent work           & HG-PIPE  & DFX         & FlightLLM      & our                                                          \\ \hline
% Model scalability        & Small    & Large       & Large          & Large                                                        \\ \hline
% Quantization-enabled     & right        & ×           & right              & right                                                            \\ \hline
% Bandwidth-aware          & ×        & right           & right              & right                                                            \\ \hline
% Computational efficiency & High     & Low         & Low            & Mid                                                          \\ \hline
% Mamba Compatibility      & right        & ×           & ×              & right                                                            \\ \hline
% Throughput               & High     & low         & High           & High                                                         \\ \hline
% \end{tabular}
% \end{table}

\begin{table}[!tb]
\centering
\caption{4-bit quantization error of the activation in the out project layer in Mamba2-2.7B
with different PTQ methods. }
\label{tab:quant_error}
\begin{tabular}{c|c|c|c|c}
\toprule
Method       & RTN  & SQ~\cite{xiao2023smoothquant}   & OS+~\cite{wei2023outlier}   & Ours \\ \midrule
Quantization Error & 19.5 & 18.8 & 309.8 & 13.1 \\ \bottomrule
% \vspace{-30pt}
\end{tabular}
\end{table}


% \begin{figure}[!tb]
% \centering
% \includegraphics[width=\linewidth]{fig/model_arch_and_proportion.pdf}
% \caption{(a) The model architecture of Mamba2~\cite{dao2024transformers}. (b) SSM runtime and memory proportion of the entire Mamba block in Mamba with different model sizes. \ml{Where do we refer this figure? Put it in the introduction?}} 
% \label{fig:model_arch_and_ssm_propotion}
% \end{figure}

% \ml{Low bit width or low bit precision? Need to unify the terminology. $\checkmark$}
% \ml{Need to re-write the motivation following the introduction. $\checkmark$}


% \begin{figure}[!tb]%
%   \centering 
%   \hspace*{-0.5cm}
%   \subfloat[In Project Activation]{
%     \label{subfig: fusion}
%     \includegraphics[width=0.24\textwidth]{fig/2.8b_activation_layer10_input_sample1_v1.pdf}
%   }
%   \subfloat[Rotated In Project Activation]{
%     \label{subfig: without_fusion}
%     \includegraphics[width=0.24\textwidth]{fig/rotate_2.8b_activation_layer10_input_sample1.pdf}
%   }
  
%   \hspace*{-0.5cm}
  
%   \subfloat[Out Project Activation]{
%     \label{subfig: fusion}
%     \includegraphics[width=0.24\textwidth]{fig/2.8b_activation_layer10_output_sample1_v1.pdf}
%   }
%   \subfloat[Rotated Out Project Activation]{
%     \label{subfig: without_fusion}
%     \includegraphics[width=0.24\textwidth]{fig/rotate_2.8b_activation_layer10_output_sample1.pdf}
%   }

%   % Draw a frame as placeholder for your image
%   % \fbox{\rule{0pt}{3in} \rule{0.4\textwidth}{0pt}}
  
%   \caption{
% Activation distribution in Mamba2-2.7B before and after rotation.
% }
% \vspace{-10pt}
%   \label{fig:activation_distribution}
% \end{figure}

\begin{figure}[!tb]
    \centering
    \includegraphics[width=0.8\columnwidth]{fig/activation_distribution.png}
    \caption{Activation distribution in Mamba2-2.7B before and after rotation.}
    \vspace{-10pt}
    \label{fig:activation_distribution}
\end{figure}


% \ml{Change ``out project'' to ``output projection''.}

\textbf{Challenge 1:}
Mamba is difficult to quantize to low bit precision because of the scattered outliers.
We find that the activation outliers of the output projection layer exhibit scattered distribution
as shown in Fig.~\ref{fig:activation_distribution}(c),
which renders previous LLM quantization~\cite{xiao2023smoothquant,wei2022outlier,wei2023outlier} less effective.
This is because in Transformer-based LLMs, outliers persist in fixed channels across different tokens.
Hence, it is possible to calculate channel-wise scaling or shifting factors to reduce the magnitude of outlier channels and reduce the quantization error \cite{xiao2023smoothquant}.
For Mamba, activation outliers may show up in different channels for different input tokens and may scatter across all channels.
% For Mamba, outliers scatter across all channels.
As a result, prior art methods, e.g., SmoothQuant \cite{xiao2023smoothquant} and OutlierSuppression+ \cite{wei2022outlier}, have comparable or even larger quantization error compared to the baseline RTN quantization as in Table~\ref{tab:quant_error}.
Rotation-based quantization algorithms have also been demonstrated for Transformer-based LLMs \cite{ashkboos2024quarot,liu2024spinquant} and do not require outliers to persist in fixed channels. 
However, as rotation-based algorithms usually require operator fusion and introduce extra computation overhead, it is unclear how it can be applied to Mamba acceleration on FPGAs.

% This results in the prior art methods SmoothQuant
% and OutlierSuppresion+ have comparable or even larger quantization error than the simple RTN quantization \cite{} as shown in Table~\ref{tab:quant_error}.
% The quantization error is calculated as $\|X - Q(X)\|_F$,
% where $X$ is the original activation
% and $Q(X)$ is the quantized activation.
% What's more,
% because of the different model architecture,
% existing rotation-based re-quantization methods~\cite{ashkboos2024quarot,liu2024spinquant} for Transformer
% can not be applied to Mamba.


\textbf{Challenge 2:}
Existing works~\cite{li2024evaluating,pierro2024mamba,li2024marca} often choose not to quantize the SSM layer in Mamba. 
The floating-point (FP) computation of SSM layers introduces large hardware cost on FPGA.
% Moreover, directly quantizing SSM layers to lower bit precision also incurs high computation overhead. 
% This is because the SSM layer comprises excessive element-wise operations, for which the overhead of re-quantization (transforming output from high bit precision back to low bit precision) is significantly larger than that of MM 
% as shown in Table~\ref{tab:re-quant overhead}.
Moreover, directly quantizing SSM layers to lower bit precision also incurs large hardware cost as shown in Fig.~\ref{fig:challenge2}. 
This is because the SSM layer comprises excessive EMs, the re-quantization (transforming output from high bit precision back to low bit precision) overhead of which is significantly large.

\begin{figure}[!tb]
\centering
\includegraphics[width=0.9\linewidth]{fig/challenge2.pdf}
\vspace{-10pt}
\caption{The hardware cost of different operations in the SSM layer with naive Non-PoT quantization and PoT quantization.} 
\vspace{-10pt}
\label{fig:challenge2}
\end{figure}


% For the SSM layer in Mamba,
% existing works~\cite{li2024evaluating,pierro2024mamba,li2024marca} ignore to quantize it and calculate in floating-point (FP) unit~\cite{li2024marca},
% which will occupy considerable computational resources on FPGA. 
% % However, naive quantization for SSM will introduce large re-quant overhead.
% A straightforward solution is to quantize SSM to low bit precision.
% However, as shown in Table~\ref{tab:re-quant overhead}, we find that although SSM only accounts for 7\% of the total computation,
% its re-quantization operations constitutes 98\% of the entire block, which
% introduces large re-quant overhead.
% The re-quantization operation is re-quantizing output from INT32 to INT8 or INT4 using multiplication
% for subsequent computation or storage~\cite{nagel2106white}.


% \begin{table}[!tb]
%     \centering
%     \caption{Re-quantization overhead of operators in Mamba block across different model sizes. Re-quantization overhead is defined as the ratio between the number of re-quantization operations and the original multiplication operations.
%     % D is model dimension,
%     % L is token length, 
%     % N is hidden state dimension.
%     }
%     \label{tab:re-quant overhead}
%     \scalebox{1.0}{
% % \begin{tabular}{l|c|c||l|c|c}
% % \toprule
% % Operator       & MM Ops & Re-quant Ops & Operator & MM Ops & Re-quant Ops \\ \midrule
% % Linear (in proj) & 4LD^2  & 12LD        & $B \odot \Delta$   & 2LDN   & \textbf{2LDN}  \\
% % Linear (out proj) & 2LD^2  & 6LD         & $\bar{B} \odot x$   & 2LDN   & \textbf{2LDN}  \\
% % $A \odot \Delta$         & 2LDN   & \textbf{2LDN}            & Add (1)     & -      & \textbf{2LDN}  \\
% % $\bar{A} \odot h_{t-1}$         & 2LDN   & \textbf{2LDN}       & $h_t C$    & 2LDN   & 6LD         \\
% %  $x \odot D$       & 2LD    & 2LD      & Add (2)     & -      & 2LD        \\
% %  \bottomrule
% % \end{tabular}

% \begin{tabular}{l|cc|l|cc}
% \toprule
% Operator                & 2.7B  & 130M  & Operator          & 2.7B  & 130M  \\ \midrule
% Linear (in proj)        & 0.001 & 0.004 & $B \odot \Delta$  & 1     & 1     \\
% Linear (out proj)       & 0.001 & 0.004 & $\bar{B} \odot x$ & 1     & 1     \\
% $A \odot \Delta$        & 1     & 1     & $h_t C$           & 0.001 & 0.004 \\
% $\bar{A} \odot h_{t-1}$ & 1     & 1     & $x \odot D$       & 1     & 1     \\ 
% \bottomrule
% \end{tabular}
% }
% \vspace{-10pt}
% \end{table}


% The re-quant overhead is defined as:
% \begin{equation}
% \label{equation: quantization overhead}
% r=\frac{Requant \, operations}{MM \, operations}q
% \end{equation}
% where $q=(16/b)^2$, 16 is the bit precision for FP16,
% and b is the quantization bit precision.
% As shown in Fig.~\ref{fig:requant_overhead},
% re-quant overhead of EM is approximately $D\times$ larger than MM
% since there is no reduction along the $D$ dimension in EM.
% \begin{figure}[!tb]
% \centering
% \includegraphics[width=\linewidth]{fig/requant.pdf}
% \caption{Re-quant overhead of (a) MM and (b) EM.} 
% \label{fig:requant_overhead}
% \end{figure}




% \begin{table}[]
%     \centering
%     \caption{The quantization overhead
%     for different kinds of computations, which are calculated with approximation.
%     }
%     \label{tab:quantization overhead}
%     \scalebox{0.9}{
% \begin{tabular}{l|c|c|c}
% \toprule
%                   & MM Ops & Requant Ops & Re-quant overhead\\ \midrule
% MM        & LDN    & 3LN+L                                                                    & 3q/D                                                            \\ \midrule
                                                        
% EM & LD     & 3L+LD                                                                    & q                                                               \\
% \bottomrule
% \end{tabular}
%     }
% \end{table}

\textbf{Challenge 3:}
The computation graph of SSM layer is complex, which increases the difficulty of FPGA-based
acceleration. On one hand, the inputs of the SSM layer, i.e., $X, B, C, \Delta$, are all
generated by the input projection layer. 
Even though it is possible to unroll the Mamba computation
spatially on FPGA, the computation of the input projection and SSM layers are forced to be sequential,
leading to only less than 60\% hardware utilization. 
% On the other hand, as the computation graph of
% an SSM layer has multiple parallel branches, we are forced to store more tensors on chip.
On the other hand, storing the intermediate activations of SSM on chip is memory-consuming, accounting for more than 70\% of the total URAM usage.

% \textbf{Challenge 3:}
% % The complex computations and data dependency of SSM make it
% % only able to be executed alternately with the linear layers.
% SSM has different computations from linear layers
% and relies on the output of in project layer, i.e.,  $X, B, C,\Delta $ to start computing.
% In a naive way, they can only be executed alternately,
% leading to only 58.5\% hardware utilization,
% since when one is executing, the other remains idle.
% Additionally, we find that the storage of SSM is the bottleneck of on-chip memory,
% which is $73\% \sim 63\%$ of the entire block
% for Mamba of different sizes.
% This derives from $\Bar{A}, \Bar{B}, h_t   \in \mathbb{R}^{b \times h \times p \times n} $
% are $2n \times$ larger than the activation $X \in \mathbb{R}^{b \times d} $ of linear layers,
% where $h \times p = 2d$ and $n=128$.

% \ml{Rotation-based or rotation-assisted quantization}

To address these challenges, 
we propose LightMamba, 
an efficient FPGA-based Mamba acceleration framework.
Specifically, we propose an FPGA-friendly Mamba quantization algorithm in Sec.~\ref{sec:quantization_algo},
featuring a rotation-assisted PTQ algorithm to mitigate the scattered outliers
and a PoT-based SSM quantization with minimum re-quantization overhead.
We also propose an FPGA-based accelerator in Sec.~\ref{sec:Hardware design},
featuring computation reordering to improve the  hardware utilization
and fine-grained tiling and fusion to reduce the on-chip memory cost.

% % To achieve efficient Mamba acceleration on FPGA,
% To understand the bottleneck for Mamba acceleration,
% we analyze the performance bottleneck with the Roofline Model~\cite{williams2009roofline}.
% As shown in Fig.~\ref{fig:roofline_model},
% the decode stage of Mamba is memory-bound.
% When weights and activations are in FP16 ($A$), \ml{What is $A$? point $A$ in Fig?}
% the system is bounded by the low memory bandwidth of 
% transferring FP16 weights.
% Additionally, $A$ is below the boundary
% since when calculating SSM,
% only a small hidden state is required to be transferred,
% leading to under-utilization of memory bandwidth.
% To this end,
% we first quantize the weights and activations to low bit precision
% (moving from point $A$ to point $B$ for W8A8, and from point $A$ to point $C$ in W4A4 in Fig?).
% Quantization increases both the slop, i.e, the number of weights transferred per second,
% and the height of the roofline curve,
% i.e., the peak computation performance on FPGA,
% which is because we can use DSP packing for INT8 multiplication
% or use LUTs for INT4 multiplication~\cite{guo2024hg}.
% However, quantizing Mamba to low bit precision is not trivial.
% This is because activation outliers of the out project layer in Mamba exhibit scattered distribution as shown in Fig.~\ref{fig:activation_distribution},
% which is different from outliers in Transformer that persist
% in fixed channels~\cite{xiao2023smoothquant,wei2022outlier,wei2023outlier}.
% Thus, previous quantization methods~\cite{xiao2023smoothquant,wei2022outlier,wei2023outlier} cause large quantization error
% since they calculate channel-wise scaling or shifting factors based
% on the fact that outliers persist in fixed channels across different
% tokens.
% What's more,
% because of the different model architecture,
% existing rotation-based quantization methods~\cite{ashkboos2024quarot,liu2024spinquant} for Transformer
% can not be applied to Mamba.

% \ml{We need to refer to the Figures in sequence, we cannot refer to Figure 5 before Figure 3.}

% After quantization, $B$ and $C$ still can not fully utilize the memory bandwidth,
% which is because simple serial execution of SSM and the linear layer will result in low bandwidth utilization.
% As shown in Fig.~\ref{fig:model_arch_and_ssm_propotion}(b),
% SSM occupies $34\% \sim 23\%$ runtime of the entire block
% across different model sizes,
% during which,
% only a small hidden state is transferred. \ml{The hidden state is not transferred, always stay on chip??}
% Additionally, we find that the storage of SSM is the bottleneck of on-chip memory,
% which is because $\Bar{A}, \Bar{B}, h_t   \in \mathbb{R}^{b \times h \times p \times n} $ in Fig.~\ref{fig: quantization_algorithm}
% are $2n \times$ larger than the activation $X \in \mathbb{R}^{b \times d} $ of linear layers,
% where $h \times p = 2d$ and $n=128$.

% To address these challenges,
% we propose LightMamba,
% which incorporates the rotation-based PTQ method
% (Sec.~\ref{sec:quantization_algo})
% and the FPGA accelerator (Sec.~\ref{sec:Hardware design})
% featuring compute-aware generation reordering (Sec.~\ref{subsec:pipeline}) to improve the 
% hardware utilization
% and fine-grained tiling and fusion (Sec.~\ref{subsec:tiling and fusion}) to reduce
% the on-chip memory cost of SSM.

% \ml{Need a figure for overall framework.}




% In the decode stage of Mamba, it only takes
% one token as input and generates the next token
% and performs matrix-vector multiplication with the weight tensor.
% It is memory-bound since the large amount of weights 
% need to be transferred from off-chip memory to on-chip
% while only perform a few multiplications,
% leading to low compute intensity.
% When weights are in FP16,
% memory-bound problem is severe since loading one weight requires two bytes, e.g., A in Fig.~\ref{fig:roofline_model}.
% Thereby, we first quantize weight to low-bit integer,
% e.g., 4-bit increasing the compute intensity,
% thus improving the throughput from A to B.
% However, the throughput is still limited by
% the FP peak performance available on FPGA.
% Thus, we quantize activations to low-bit as well,
% which enables integer multiplications,
% improving the throughput from B to C.
% However, the activation outliers in Mamba pose a great challenge for quantization
% since they exhibit scattered distribution as shown in Fig.~\ref{fig:activation_distribution},
% which is totally different from outliers in Transformer that persist
% in fixed channels~\cite{xiao2023smoothquant,wei2022outlier,wei2023outlier}.
% Thus, previous quantization methods~\cite{xiao2023smoothquant,wei2022outlier,wei2023outlier} cause large quantization error
% since they calculate channel-wise scaling or shifting factors based
% on the fact that outliers persist in fixed channels across different
% tokens.

% Inspired by~\cite{ashkboos2024quarot} and~\cite{liu2024spinquant},
% we analyze the computational invariance~\cite{ashkboos2024slicegpt} in Mamba and propose an accurate rotation-based PTQ method.

% After quantization,
% we find that SSM becomes the bottleneck in both runtime and on-chip memory cost.
% As shown in Fig.~\ref{fig:model_arch_and_ssm_propotion}(b),
% it occupies $34\% \sim 23\%$ runtime and
% $73\% \sim 66\%$ memory cost of the entire block
% across different model sizes.
% The large runtime leads
% to low peak available performance
% since SSM remains idle when the linear layer is being computed, and the linear layer is idle when the SSM is being computed.
% Thus, we propose to overlap the computation of linear layers and SSM
% through compute-aware generation reordering,
% to improve DSP utilization,
% thus boosting throughput from C to D in Fig.~\ref{fig:roofline_model}.
% Moreover, we find that the activation in SSM such as $\Bar{A}, \Bar{B}, h_t   \in \mathbb{R}^{b \times h \times p \times n} $ in Fig.~\ref{fig: quantization_algorithm}
% are $2n \times$ larger than the activation $X \in \mathbb{R}^{b \times d} $ of linear layers,
% where $h \times p = 2d$ and $n=128$ in Mamba,
% leading to significant on-chip memory costs.
% Thus we propose our fine-grained tiling and fusion method to solve this problem.








