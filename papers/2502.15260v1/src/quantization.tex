\section{FPGA-friendly Quantization Algorithm}
\label{sec:quantization_algo}



% We introduce our FPGA-friendly quantization algorithm in this  section.
% For the scattered outliers,  we propose a rotation-assisted PTQ method for Mamba to mitigate the impact of outliers.
% For SSM, we use power-of-two (PoT) quantization to reduce the large re-quant overhead.

\subsection{Rotation-assisted Linear Layer Quantization}

The rotation-assisted quantization method is first proposed in~\cite{ashkboos2024quarot}
for Transformer-based LLMs.
By multiplying the activation $X$ and weight $W$ with orthogonal matrix $Q$, i.e., $XQQ^TW$,
the result is identical with $XW$,
while the outliers in $X$ and $W$ are removed.
However, it is still unclear whether the rotation method is applicable to Mamba.
Therefore, we study the rotation equivalence in Mamba and propose a rotation-assisted method shown in Fig.~\ref{subfig: quantization_algorithm}.


% \begin{figure}[!tb]
% \centering
% \hspace*{-0.05\linewidth}  % 使用负的水平空白来左移图片
% \includegraphics[width=0.7\linewidth]{fig/rotation_PTQ_v2.pdf}
% \caption{The proposed quantization algorithm for Mamba.
% Both $Q$ and $H$ are orthogonal matrices to ensure computation correctness.
% $H$ is 
% $Q$ and $H$
% stand for multiplying the orthogonal matrix $Q$ and Hadamard matrix $H$.
% % \ml{Need to mention $H$ and $Q$ in the caption. \checkmark}
% % We illustrate the computation process within SSM
% % and label the dimension of the tensors,
% % where b is batch size, 
% % h is number of head,
% % p is head dimension,
% % n is state dimension.
% % We omit the token length dimension,
% % which is 1 in the decode stage.
% % \ml{Repetitive in Fig. 2.}
% } 
% \label{fig: quantization_algorithm}
% \end{figure}


\begin{figure}[!tb]
  \centering 
  % \hspace*{-0.5cm}
  \subfloat[]{
    \label{subfig: quantization_algorithm}
    \includegraphics[width=0.25\textwidth]{fig/rotation_PTQ_v4.pdf}
  }
  \subfloat[]{
    \label{subfig: weight_quant_error}
    \includegraphics[width=0.23\textwidth]{fig/weight_quant_error.pdf}
  }
  \caption{
(a) The proposed rotation-assisted quantization algorithm.
Both $Q$ and $H$ are Hadamard matrices to ensure computation correctness.
% $H$ is 
% $Q$ and $H$
% stand for multiplying the orthogonal matrix $Q$ and Hadamard matrix $H$.
(b) Quantization error of the output projection weight after only rotation or fusion and rotation.
% We measure the quantization error by the root squared error between the FP16 weight and quantized weight.
}
  \label{fig:fusion_cause_error}
\end{figure}

\begin{figure*}[!tb]
    \centering
    \includegraphics[width=1\linewidth]{fig/overall_new.pdf}
    \vspace{-10pt}
    \caption{Diagram of (a) the overall architecture, (b) SSMU, (c) MMU, (d) 128-point HTU, and (e) 40-point HTU.}
    \vspace{-10pt}
    \label{fig: Hardware Design}
\end{figure*}

We observe that the activations in the linear layers and SSM layer
have large number of outliers,
and the outliers of output projection layer exhibit scattered distribution
across different channels.
Rotation is helpful to remove outliers
since it amortizes large outliers with other elements.
For the input and output projection layers,
we apply rotation and remove the outliers as shown in Fig.~\ref{fig:activation_distribution}.
It is worth noting that
to rotate the activation before the output projection layer
we insert an on-line Hadamard transformation 
before it in Fig.~\ref{subfig: quantization_algorithm},
which can be efficiently performed by our customized
rotation unit in Sec.~\ref{sec:Hardware design}.
However, we find that SSM cannot be rotated since it
does not satisfy the rotation equivalence.
Specifically,
the original computation in SSM is Eq.~\ref{eq:rotate_ssm_a}.
Assuming we can rotate hidden state $h_t$ to remove the outliers,
i.e., multiply $h_t$ by Hadamard matrix $H$,
we can derive Eq.~\ref{eq:rotate_ssm_b} and Eq.~\ref{eq:rotate_ssm_c}.
However, Eq.~\ref{eq:rotate_ssm_c} cannot derive Eq.~\ref{eq:rotate_ssm_d}
because EM does not satisfy matrix associative property.
Thus we cannot derive Eq.~\ref{eq:rotate_ssm_d} from Eq.~\ref{eq:rotate_ssm_a}, i.e., SSM does not satisfy the rotation equivalence.
\begin{subequations}
\begin{align}
    & h_{t} = \bar{A} \odot h_{t-1}+\bar{B} \odot X_{t}  \label{eq:rotate_ssm_a}\\
    & h_{t}H = (\bar{A} \odot h_{t-1} + \bar{B} \odot X_{t})H \label{eq:rotate_ssm_b}\\
    & h_{t}H = (\bar{A} \odot h_{t-1})H + (\bar{B} \odot X_{t})H \label{eq:rotate_ssm_c}\\
    & h_{t}H = \bar{A} \odot (h_{t-1}H) + \bar{B} \odot (X_{t}H) \label{eq:rotate_ssm_d}
\end{align}
\end{subequations}

To reduce the computation overhead, we try to fuse rotation with neighboring operations as much
as possible. 
As shown in Fig.~\ref{subfig: quantization_algorithm}, we can fuse the first rotation
with the embedding table (i.e., \textcircled{1}),
the last rotation with the LM head (i.e., \textcircled{5}),
as well as the rotations before and after the output projection layers in each Mamba block (i.e., \textcircled{4}).
For the rotation next to the first RMSNorm operator (i.e., \textcircled{2}), to ensure
the computational invariance, we need to split the scaling factor, i.e., $D$, of the RMSNorm first,
and then, fuse it with the weights of input projection.
For the rotation next to the second RMSNorm operator (i.e., \textcircled{3}), we find whether
or not to fuse the scaling factor of the RMSNorm to the weight of output projection does not impact the computational invariance,
while fusion introduces a larger quantization error as in Fig.~\ref{subfig: weight_quant_error}. Hence, we choose not to fuse
the scaling factor of the second RMSNorm.
In our algorithm, only rotation \textcircled{3} needs to be computed online, which incurs small computation
overhead with our customized FPGA module support.

% Moreover, different from Transformer-based LLMs,
% we find that the scaling parameters of RMSNorm,
% denoted as $D=diag(\alpha)$, should apply different fusion strategies.
% RMSNorm before the input projection layer should be fused
% into the weight to ensure the computational invariance following~\cite{ashkboos2024slicegpt}.
% However, the scaling parameters of RMSNorm  before the output projection layer should not be fused into the subsequent weight.
% This is because although both fusion (Eq.~\ref{equation: in-block RMSNorm fusion}) 
% and not fusion (Eq.~\ref{equation: in-block RMSNorm w/o fusion}) ensure the computational invariance,
% fusion will introduce larger weight quantization error
% as shown in Fig.~\ref{subfig: weight_quant_error}.
% \begin{subequations}
% % \vspace{-10pt}
% \begin{align}
%     & \frac{X}{\|X \|_{2}} HH^{\top} (D W_{out})Q=\frac{X}{\|X \|_{2}} D W_{out}Q= X_{l+1}Q
%     \label{equation: in-block RMSNorm fusion}\\
%     & \frac{X}{\|X \|_{2}} D HH^{\top} W_{out}Q=\frac{X}{\|X \|_{2}} D W_{out}Q= X_{l+1}Q \label{equation: in-block RMSNorm w/o fusion}
% \end{align}
% \end{subequations}
% 
% The main reason is that the scaling parameters of the RMSNorm
% before output projection layer
% learned to have large variations.
% Multiplying it to weight, i.e., $DW_{out}$ will increase the quantization difficulty 
% since we quantize weight for each output channel as shown in Fig.~\ref{subfig: fuse_rmsnorm_to_weight}.
% It is worth noting that the fusion and rotation of weight can be performed offline, 
% which does not introduce extra overhead during inference.


\subsection{FPGA-friendly SSM Quantization}

In order to quantize SSM to reduce the heavy hardware cost by FP computations,
we leverage INT8 per-group quantization
to strike a balance between accuracy and hardware efficiency.
However, directly quantizing SSM introduces
large re-quantization overhead as shown in Fig.~\ref{fig:challenge2},
which is
because EM has larger re-quantization overhead than MM intrinsically since there is no reduction in EM.
% due to the large activation dimension in SSM.
% As shown in Fig.~\ref{fig:requant_overhead},
% EM has larger re-quantization overhead than MM intrinsically since there is no reduction in EM,
% which leads to larger output size with the same input size.
To this end,
we propose to use PoT quantization for SSM,
through which re-quantization can be implemented in bit-shifting
rather than multiplication
thus reducing the re-quantization overhead significantly.
% The re-quantization overhead is defined as:
% \begin{equation}
% \label{equation: quantization overhead}
% r=\frac{Requant \, operations}{MM \, operations}q
% \end{equation}
% where $q=(16/b)^2$, 16 is the bit precision for FP16,
% and b is the quantization bit precision.
% $r_{EM}$ is approximately $D\times$ larger than $r_{MM}$
% with the same input size

% \begin{figure}[!tb]
% \centering
% \includegraphics[width=\linewidth]{fig/requant.pdf}
% \caption{Re-quantization overhead of (a) MM and (b) EM.} 
% \label{fig:requant_overhead}
% \end{figure}
% \begin{figure}[!tb]%
%   \centering 
%   % \hspace*{-0.5cm}
%   \subfloat[]{
%     \label{subfig: weight_quant_error}
%     \includegraphics[width=0.24\textwidth]{fig/weight_quant_error.pdf}
%   }
%   \subfloat[]{
%     \label{subfig: fuse_rmsnorm_to_weight}
%     \includegraphics[width=0.24\textwidth]{fig/fuse_rmsnorm_to_weight_v1.pdf}
%   }
%   \caption{
% (a) Quantization error of the output projection weight after only rotation or fusion and rotation.
% (b) Fuse the scaling parameter of RMSNorm to weight.
% \ml{Change ``out project'' and ``in project'' to ``output projection'' and ``input projection''.}
% \ml{Figure b is not clear.}
% % We measure the quantization error by the root squared error between the FP16 weight and quantized weight.
% }
%   \label{fig:fusion_cause_error}
% \end{figure}













% To tackle the activation outliers challenge in Sec.~\ref{sec:motivation},
% we propose a rotation-based PTQ method for Mamba.
% Although the rotation-based method is recently proposed for Transformer~\cite{ashkboos2024quarot,liu2024spinquant}, 
% it is non-trivial to apply it to Mamba.
% We are the first to study it both theoretically and empirically.
% The schematic of the quantized Mamba block is shown in Fig.~\ref{fig: quantization_algorithm}.
% Through our proposed method,
% we can achieve the first accurate and fully quantized W8A8 and W4A4 Mamba.
% The key insight is that by multiplying the weight and
% activation with the orthogonal matrix,
% the output is still identical
% while the outliers in weight and activation can be removed 
% reducing quantization difficulty.

% We introduce our FPGA-friendly quantization algorithm for Mamba.
% % The key insight
% % is that by multiplying the weight and activation with the orthogonal matrix,
% % the output is still identical
% % while the outliers in weight and activation can be removed 
% % thus reducing quantization difficulty.
% Specifically, our proposed quantization algorithm consists of two steps, i.e., rotation and quantization.

% \ml{Rotation is not explained yet. What is rotation? It makes people very confusing. \checkmark}

% \ml{Right now, for the quantization algorithm, it is unclear how it considers the FPGA hardware characteristics? \checkmark}




% \subsection{Rotation}
% \label{subsec:rotation_model}

% % Leveraging the computational invariance~\cite{ashkboos2024slicegpt},
% % we post-multiplied the weight of the in project and out project layer
% % by the random orthogonal matrix $Q$.
% % The embedding and the LM head should also be multiplied by $Q$.

% \ml{If you want to mention rotation, then, you need to make it clear: 1) what is rotation, 2) why it helps with quantization, and 3) what is the difference compared to LLM?}

% Rotation should keep the network output equivalent and 
% remove the outliers in weights and activations to reduce the 
% quantization difficulty in the quantization step. 
% The computational invariance~\cite{ashkboos2024slicegpt} studies
% the RMSNorm-connected Transformer network and 
% shows that the scaling parameters of RMSNorm between-block \ml{cross-block? $\checkmark$} should be 
% absorbed into the subsequent weights to ensure the computational invariance.
% % such that the rotation of weights will not affect the network output.
% However, there are two types of RMSNorm in Mamba, i.e., 
% the between-block RMSNorm which is right before the in project layer, and the in-block RMSNorm which is right before the out project layer.
% % They have different requirements for fusion strategies.
% We find that the scaling parameters of in-block RMSNorm
% can not be fused into subsequent weights.

% \ml{Need to explain why do we want to fuse rotation?}
% \ml{This section takes too much space and yet, it is unclear how it is related to FPGA?}

% \textbf{RMSNorm Fusion}

% To keep the computation invariant, 
% the scaling parameters ($D=diag(\alpha)$) of the between-block RMSNorm
% should be fused into the weight of in project layer.
% This is because given input $X_lQ$, \ml{$Q$ is not defined} the output of the in project layer
% can be calculated as:
% \begin{equation}
% \label{equation: between-block RMSNorm}
% \frac{X_l Q}{\|X_l Q\|_{2}} Q^{\top} (D W_{in})=\frac{X_l}{\|X_l Q\|_{2}} D W_{in}=\frac{X_l}{\|X_l\|_{2}} D W_{in}
% \end{equation}
% where $\|Q\|_{2}=1$ since $Q$ is orthogonal \ml{orthogonal or orthonormal?} matrix.
% The term on the far right \ml{what is far right?} of Eq.~\ref{equation: between-block RMSNorm} is the original output of the in project layer.
% However, the scaling parameters of in-block RMSNorm should not be fused into the weight of out project layer.
% This is because although both fusion (Eq.~\ref{equation: in-block RMSNorm fusion}) 
% and not fusion (Eq.~\ref{equation: in-block RMSNorm w/o fusion}) ensure the computation invariance,
% fusion will make weights harder to quantize,
% against our intent.
% % The weight quantization error of each layer is shown in Fig.~\ref{fig:fusion_cause_error}.
% As shown in Fig.~\ref{subfig: weight_quant_error}, 
% RMSNorm fusion causes consistently larger quantization error than the case without fusion.

% \begin{subequations}
% \begin{align}
%     & \frac{X}{\|X \|_{2}} HH^{\top} (D W_{out})Q=\frac{X}{\|X \|_{2}} D W_{out}Q= X_{l+1}Q
%     \label{equation: in-block RMSNorm fusion}\\
%     & \frac{X}{\|X \|_{2}} D HH^{\top} W_{out}Q=\frac{X}{\|X \|_{2}} D W_{out}Q= X_{l+1}Q \label{equation: in-block RMSNorm w/o fusion}
% \end{align}
% \end{subequations}


% % \begin{equation}
% % \label{equation: in-block RMSNorm fusion}
% % \frac{X}{\|X \|_{2}} HH^{\top} (D W_{out})Q=\frac{X}{\|X \|_{2}} D W_{out}Q= X_{l+1}Q
% % \end{equation}

% % \begin{equation}
% % \label{equation: in-block RMSNorm w/o fusion}
% % \frac{X}{\|X \|_{2}} D HH^{\top} W_{out}Q=\frac{X}{\|X \|_{2}} D W_{out}Q= X_{l+1}Q
% % \end{equation}

% % \begin{figure}[!tb]%
% %   \centering 
% %   % \hspace*{-0.5cm}
% %   \subfloat[]{
% %     \label{subfig: weight_quant_error}
% %     \includegraphics[width=0.24\textwidth]{fig/weight_quant_error.pdf}
% %   }
% %   \subfloat[]{
% %     \label{subfig: fuse_rmsnorm_to_weight}
% %     \includegraphics[width=0.24\textwidth]{fig/fuse_rmsnorm_to_weight_v1.pdf}
% %   }
% %   \caption{
% % (a) Quantization error of the out project weight after only rotation or fusion and rotation.
% % (b) Fuse the scaling parameter of RMSNorm to weight.
% % % We measure the quantization error by the root squared error between the FP16 weight and quantized weight.
% % }
% %   \label{fig:fusion_cause_error}
% % \end{figure}

% % \begin{figure}[!tb]
% % \centering
% % \includegraphics[width=\textwidth]{fig/fuse_rmsnorm_to_weight.pdf}
% % \caption{(a) The model architecture of Mamba2~\cite{dao2024transformers}. (b) SSM runtime and memory proportion of the entire Mamba block in Mamba with different model sizes.} 
% % \label{fig:model_arch_and_ssm_propotion}
% % \end{figure}

% This is because compared with the between-block RMSNorm, 
% the scaling parameters of the in-block RMSNorm
% learned to have much larger variations.
% Multiplying it to weight, i.e., $DW_{out}$ will increase the quantization difficulty 
% since we quantize weight for each output channel as shown in Fig.~\ref{subfig: fuse_rmsnorm_to_weight}.
% It is worth noting that the fusion of between-block RMSNorm and rotation of weight can be performed offline, 
% which does not introduce extra overhead during inference.

% % Multiplying it to weight, i.e., $DW_{out}$ will introduce more outliers
% % and increase the quantization error 
% % even after being multiplied by the Hadamard matrix $H$ and random orthogonal matrix $Q$, which is shown in Fig.~\ref{fig:fusion_cause_error}.
% % Therefore, the scaling parameters of in-block RMSNorm should not be fused.
% % Fusion of the between-block RMSNorm and rotation of weight can be performed offline, 
% % which does not introduce extra overhead to inference.

% Furthermore, activations should also be rotated to remove outliers. 
% As shown in Fig.~\ref{fig: quantization_algorithm},
% since the input of the in project layer is already rotated,
% only one online Hadamard Transformation is required in our method,
% % i.e., multiply the input of the out project layer by the Hadamard matrix $H$,
% compared with four on-line Hadamard Transformations in the Transformer-based model~\cite{ashkboos2024quarot}.
% The overhead is negligible with the Fast Hadamard Transformation method~\cite{tseng2024quip}.
% Considering an activation with dimension $ \mathbb{R}^{l \times n} $, when $n$ is a power of two,
% the Hadamard Transform can be computed with Fast Walsh-Hadamard Transform (FHT)~\cite{fino1976unified}
% in $O(n \log n)$ instead of $O(n^2)$.
% When $n$ is not the power of 2,
% they factorize $n=pq$ where $p$ is the largest power of 2
% such that a known Hadamard matrix of size $q$ exists. 
% For example, for Mamba2-2.7B, we factorize 5120 into $128\times40$.
% In this way, the Hadamard matrix $H_n$ can be constructed by 
% $H_p \otimes H_q$.
% Inspired by this, 
% we propose an efficient Hadamard transformation circuit design
% in Section~\ref{sec:Hardware design}.
% % To reduce the overhead,
% % we propose an efficient Hadamard transformation circuit design
% % in Section~\ref{sec:Hardware design}
% % leveraging the Fast Hadamard Transformation method~\cite{tseng2024quip}.
% With the rotation mentioned above, 
% the computation invariance is preserved and
% outliers in weights and activations are removed
% as shown in Fig.~\ref{fig:activation_distribution}, leading to easier quantization.

% \begin{figure}[!tb]%
%   \centering 
%   \hspace*{-0.5cm}
%   % \subfloat[In Project Activation]{
%   %   \label{subfig: fusion}
%   %   \includegraphics[width=0.24\textwidth]{fig/2.8b_activation_layer10_input_sample1_v1.pdf}
%   % }
%   % \subfloat[Rotated In Project Activation]{
%   %   \label{subfig: without_fusion}
%   %   \includegraphics[width=0.24\textwidth]{fig/rotate_2.8b_activation_layer10_input_sample1.pdf}
%   % }
  
%   % \hspace*{-0.5cm}
  
%   % \subfloat[Out Project Activation]{
%   %   \label{subfig: fusion}
%   %   \includegraphics[width=0.24\textwidth]{fig/2.8b_activation_layer10_output_sample1_v1.pdf}
%   % }
%   % \subfloat[Rotated Out Project Activation]{
%   %   \label{subfig: without_fusion}
%   %   \includegraphics[width=0.24\textwidth]{fig/rotate_2.8b_activation_layer10_output_sample1.pdf}
%   % }

%   % Draw a frame as placeholder for your image
%   \fbox{\rule{0pt}{3in} \rule{0.4\textwidth}{0pt}}
  
%   \caption{
% Activation distribution in Mamba2-2.7B before and after rotation.
% }
%   \label{fig:activation_distribution}
% \end{figure}




% \textbf{Can SSM be rotated?}
% Since activation in SSM also has outliers shown in Table~\ref{tab: kurtosis},
% a natural question arises:
% can we rotate SSM to remove the outliers and reduce the quantization difficulty?
% Through the following study, the answer is negative.
% Assume that we can rotate hidden state $h_t$ to remove the outliers,
% i.e., multiply $h_t$ by Hadamard matrix $H$,
% in order to ensure Eq.~\ref{eq:rotate_ssm_a} holds,
% the right-hand side should also be multiplied by $H$ leading to Eq.~\ref{eq:rotate_ssm_b} and Eq.~\ref{eq:rotate_ssm_c}.
% However, Eq.~\ref{eq:rotate_ssm_c} cannot derive to Eq.~\ref{eq:rotate_ssm_d}
% because the element-wise multiplication does not satisfy matrix associative property.
% Only when Eq.~\ref{eq:rotate_ssm_d} holds can we rotate SSM successfully
% since we intend to remove the outliers in $h_{t-1}$ which is also $h_t$ in the last time step and $X_t$
% such that we can quantize them with low error
% and perform multiplication in low bit precision.
% Thus, the assumption does not hold
% because Eq.~\ref{eq:rotate_ssm_d} is not valid.
% Fortunately, we discover that with an INT8 dynamic per group quantization scheme in Section~\ref{subsec:quantization},
% we can obtain an almost lossless quantized SSM compared to the FP counterpart.
% \begin{subequations}
% \begin{align}
%     & h_{t} = \bar{A} \odot h_{t-1}+\bar{B} \odot X_{t}  \label{eq:rotate_ssm_a}\\
%     & h_{t}H = (\bar{A} \odot h_{t-1} + \bar{B} \odot X_{t})H \label{eq:rotate_ssm_b}\\
%     & h_{t}H = (\bar{A} \odot h_{t-1})H + (\bar{B} \odot X_{t})H \label{eq:rotate_ssm_c}\\
%     & h_{t}H = \bar{A} \odot (h_{t-1}H) + \bar{B} \odot (X_{t}H) \label{eq:rotate_ssm_d}
% \end{align}
% \end{subequations}




% \subsection{Weight and Activation Quantization}
% \label{subsec:quantization}

% After rotation, 
% both activation and weight quantization benefit from the
% outlier removal, thus leading to smaller quantization error,
% e.g., the 4-bit quantization error reduced from 19.5 to 13.1 as shown in Table~\ref{tab:quant_error}. \ml{XXX $\checkmark$}
% For weight and activation in the linear layers and conv1d layers,
% we quantize them to INT8 (W8A8) or INT4 (W4A4)
% and we set the scaling factor to dyadic number~\cite{hu2024llm},
% which is calculated as $m/2^k$,
% where m and k are integers. \ml{Why do we want to do this?}
% Therefore, when calculating the product of the scaling factors for weights and activations, 
% we can use integer multiplication and bit-shift
% instead of FP multiplication,
% which is friendly for accelerators.
% For the non-linear operations,
% we leverage the lookup table method in~\cite{guo2024hg},
% which is quite efficient for FPGA.
% In SSM shown in Fig.~\ref{fig: quantization_algorithm}, 
% $A$ and $D$ are weights which have negligible number of parameters compared to the weights of linear layers,
% others are activations, i.e., input-dependent.
% We quantize them using per group power-of-two (PoT) quantization (group size=128),
% in which the scaling factor is the power of two,
% to save the computation overhead of scaling factors.
% In particular, the group is divided along $p$ dimension,
% which corresponds to our dataflow in Sec.~\ref{sec:Hardware design}.
% Moreover, we quantize parameter $A$ to INT32 since it is important
% in controlling how the previous tokens affect the current token,
% and quantize other activations to INT8,
% the computation of which still accounts for a small portion even in W4A4 settings,
% since the number of operations of SSM only accounts for 1/12 of the Mamba block.



