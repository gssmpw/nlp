% \begin{figure*}[!tb]
%     \centering
%     \includegraphics[width=1\linewidth]{fig/hardware_v2.pdf}
%     \caption{Diagram of (a) the overall architecture, (b) SSMU, (c) MMU, (d) 128-point HTU, and (e) 40-point HTU.}
%     \label{fig: Hardware Design}
% \end{figure*}

\section{FPGA-based Mamba Acceleration}
\label{sec:Hardware design}

% We have derived the proposed quantization algorithm in Sec.~\ref{sec:quantization_algo},
% however, the actual improvement in throughput and memory reduction remains unclear.
% To this end, 
% we propose an FPGA-based accelerator for Mamba, dubbed LightMamba.
% Moreover, we find the naive implementation of SSM exhibit large runtime and memory consumption.
% Therefore, we propose two methods to significantly enhance the efficiency 
% of our hardware design: 1. Compute-aware generation reordering and parallel design and 2. Fine-grained tiling and fusion.

% In this section, 
% we present our FPGA-based accelerator design for Mamba2, 
% and our two optimizations for SSM: computation re-ordering and fine-grained tiling and fusion. 
% \ml{The name needs to be changed.$\checkmark$}

\subsection{Overall Hardware and Hadamard Transform Unit}
\label{subsec:Overall_architecture}

% We present a high-performance FPGA-based accelerator, LightMamba, optimized for MAMBA inference by fully leveraging FPGA resources. By integrating compression techniques such as fine-grained tiling, pipelined schemes, and rotation quantization, LightMamba significantly accelerates MAMBA operations while reducing latency and memory overhead.

% \ml{Need to explain why we want such partially unrolled architecture?$\checkmark$}

The overall architecture of LightMamba is shown in Fig.~\ref{fig: Hardware Design} (a). 
% It is integrated into an FPGA System-on-Chip framework, incorporating multiple peripheral components, including external memory, a host system, and a network-on-chip.
% Due to FPGA resource constraints, 
We design a partially unfolded spatial architecture and unroll the computation of one Mamba block on the FPGA.
The model parameters are stored in the off-chip DRAM.
% to trade-off hardware consumption and throughput.
% where only a single layer is deployed on the FPGA. 
% We fully unfold the SSM layer 
% and execute the linear layers in a time-multiplexed approach.
% while allowing for the reuse of hardware in the linear layer.
LightMamba consists of three main modules: (1) Matrix Multiplication Unit (MMU), which handles the computations of all linear layers, including both input and output projection; (2) SSM Unit (SSMU), which fully unfolds SSM to enable pipelined execution; and (3) Hadamard Transform Unit (HTU), a customized design to support rotation-assisted quantization.
% By adopting this partially unfolded spatial architecture, we maximize hardware utilization and minimize latency.
% Due to FPGA resource constraints, our hardware employs a partially unfolded spatial architecture, where only a single layer is unfolded on the FPGA. This approach fully unfolds the SSM while reusing the linear layer's hardware, maximizing hardware utilization and minimizing latency.
% To perform inference of the entire network, weights of each layer are sequentially loaded from the direct memory access module (DMA). 
% LightMamba mainly consists of three modules: the Matrix Multiplication Unit (MMU),  SSM Unit (SSMU), and Hadamard Transform Unit (HTU). 
% Nonlinear functions such as RMSNorm which is denoted as $R$ in Fig.~\ref{fig: Hardware Design}(a), SiLU, Exp, and Softplus are implemented using LUTs~\cite{HGPIPE}.

% \ml{If you define these acronyms, e.g., MMU, SSMU, then you need to use them$\checkmark$}

% \begin{figure}[!tb]
%     \centering
%     \includegraphics[width=1\columnwidth]{fig/arch-sys.pdf}
%     \caption{Architecture of the system. \ml{This figure is very important. To show we have done enough hardware design optimizations, we need to show more discussion/description. We can make this figure two column.}}
%     \label{fig:arch}
% \end{figure}


% The hardware architecture is based on a temporal design. Due to FPGA resource limitations, only a single Mamba layer is fully unfolded on the FPGA. 
% To perform inference across the entire Mamba network, the weights for each layer are sequentially loaded from the DMA. 
% The hardware design consists of three modules: the Matrix Multiplication Unit (MMU), the SSM Unit (SSMU), and the Hadamard Transform Unit (HTU). 
% Nonlinear functions such as RMSNorm (denoted as $R$ in Fig.\ref{fig: Hardware Design}(a)), SiLU, and Softplus are implemented using lookup tables.

\textbf{MMU}
is designed to support input and output projection layers in a time-multiplexed manner.
It features a tree-based architecture of multiplier-accumulators (MACs) that receive vectors of $d_{in}$ dimension as inputs. 
It is also equipped with $d_{out}$ lanes, which performs $d_{in} \times d_{out}$ MACs in one cycle. Altogether, MMU contains $d_{in} \times d_{out}$ MAC units, which are efficiently implemented using ${d_{in} \times d_{out}}/2$ DSPs, leveraging the DSP packing technique~\cite{pack}, as illustrated in Fig.~\ref{fig: Hardware Design}(b).

% is designed to optimize the reuse of linear layer operations, 

% Its primary function involves matrix-matrix multiplication.

% It consists of a multiplier tree with $d_{in} \times d_{out}$ multiply-accumulate (MAC) units, which are implemented using ${d_{in} \times d_{out}}/2$ DSP blocks through a DSP packing technique\~cite{pack} shown in Fig.~\ref{fig: Hardware Design}(c). 
% This unit features a tree-based architecture of multiplier-accumulators (MACs) that recieve vectors of $d_{in}$ dimensions as inputs. It is also equipped with $d_{out}$ lanes, implying that $d_{in} \times d_{out}$ MACs operate in parallel. Altogether, the MMU contains $d_{in} \times d_{out}$ MAC units. These are efficiently implemented using ${d_{in} \times d_{out}}/2$ DSP blocks, leveraging a DSP packing technique \cite{pack}, as illustrated in Fig.~\ref{fig: Hardware Design}(b).

\textbf{SSMU}
features a fine-grained, fully pipelined architecture 
for computing the SSM layer.
% that eliminates the need for external memory access for intermediate activations.
% SSM Unit is the core of LightMamba and is the primary target of our optimizations. 
% is the core of LightMamba.
% It features a fine-grained, fully pipelined architecture that eliminates the need for external memory access for intermediate activations. 
% Only $h_t$ and $h_{t-1}$ are transferred during each layer's computation.
% \ml{What does this mean?$\checkmark$}
As shown in Fig.~\ref{fig: Hardware Design}(c), 
each operator is implemented by a dedicated unit, connected via first-in-first-out buffers (FIFOs). 
% The SSMU manages all operations associated with SSM, including traditional SSM operators, convolution, and SiLU functions. 
% It is fully unfolded and each operator is implemented by a dedicated unit, connected via first-in-first-out buffers (FIFOs). 
We optimize the parallelism for each operator to ensure a balanced data flow with a minimum FIFO depth.
% As illustrated in Fig.~\ref{fig: quantization_algorithm}, the operations in the SSM involve various styles of element-wise multiplication. 
We implement the operators in SSM through Element-wise Multiplication Units (EMUs) which are composed of DSPs. 
We set different parallelism for different operators. 
% For instance, the EMUs performing $\bar{A}h_{t-1}$, $\bar{B}X$, and $h_{t} \times C$ operations has twice the computational parallelism of the other EMUs due to the higher overall computational load.
% \ml{what does this mean?$\checkmark$}


\textbf{HTU}
is dedicated to support the Hadamard transformation in our quantization algorithm. 
It has two variants: the power-of-two and the non-power-of-two type. 
For example, in Mamba-2.7B,  two types of HTU are required,
i.e, 128-point HTU and 40-point HTU as in Fig.~\ref{fig: Hardware Design}(d) and Fig.~\ref{fig: Hardware Design}(e).
The 128-point HTU is based on the Fast Hadamard Transformation (FHT) algorithm~\cite{FHT}.
It contains seven stages, each containing a Butterfly Core and two FIFOs.
In the first stage, the first 64 elements are pushed into the input FIFO.
When the next 64 elements arrive, they are processed in pairs by the Butterfly Core. 
Outputs are either sent to the next stage or buffered in the output FIFO. 
Compared to the MM-based Hadamard transform, this design reduces latency by 72\% with the same hardware resources.
For the small 40-point Hadamard transformation, we directly implement it with a simple MMU and fix one input to the Hadamard matrix with only 1 and -1.
% since 40 is not power-of-two and hardware cost are small,
% we store the $40\times40$ Hadamard matrix with only 1 and -1 elements on-chip and 
% implemented it with an adder tree.

% these correspond to Hadamard transforms of sizes 128 and 40, respectively. Specialized hardware designs for these transforms are illustrated in Fig.~\ref{fig: Hardware Design}(d) and Fig.~\ref{fig: Hardware Design}(e)
% includes a 128-size Hadamard transform unit shown in Fig.~\ref{fig: Hardware Design}(d) and a 40-size Hadamard transform unit shown in Fig.~\ref{fig: Hardware Design}(e)
% since both 128-size and 40-size Hadamard transformations are needed in Mamba2-2.7B.

% We design the 128-size HTU based on the FHT algorithm~\cite{FHT}. \ml{reference?$\checkmark$}
% 128-size HTU contains seven stages, each containing a Butterfly Core and two FIFOs.
% In the first stage, the first 64 elements are pushed into the input FIFO.
% When the next 64 elements arrive, they are processed in pairs by the Butterfly Core. 
% Outputs are either sent to the next stage or buffered in the output FIFO. 
% % This enables continuous 128-length FHT operations without stalls, requiring only a total FIFO depth of 128 elements. 
% Compared to the matrix multiplication-based Hadamard transform circuit, this design reduces latency by 72\% with the same hardware resources.
% For the 40-size Hadamard transformation, 
% since 40 is not power-of-two and hardware storage requirements are minimal,
% we store the $40\times40$ Hadamard matrix on-chip and 
% implemented it with an adder tree.

\begin{figure}[!tb]
    \centering
    \includegraphics[width=1\columnwidth]{fig/pipeline_v4.pdf}
    \vspace{-20pt}
    \caption{Pipeline scheme: (a) Naive implementation, (b) Coarse-grained pipeline, (c) Fine-grained pipeline.}
    % \vspace{-10pt}
    \label{fig: Pipeline scheme}
\end{figure}

% \subsection{Compute-aware generation reordering and parallel design}
\subsection{Computation Reordering}
\label{subsec:pipeline}

% \ml{First introduce the problem why MMU and SMMU cannot be pipelined.$\checkmark$}
Due to the distinct computational patterns
and the data dependency between SSM and input projection layer,
they are forced to execute sequentially as in Fig.~\ref{fig: Pipeline scheme}(a).
However, we find that SSM comprises multiple independent heads, and propose a coarse-grained pipeline to improve hardware utilization as in Fig.~\ref{fig: Pipeline scheme}(b). 
This pipeline design depends on our proposed computation reordering method, which alters the data generation sequence in the input projection layer. 
Specifically, $\Delta, B, C$ are generated first and stored in an on-chip buffer, while $X$ and $Z$ are produced alternatively for computing SSM head-by-head, as shown in Figure~\ref{fig: Pipeline scheme}(b).
This reordering allows the SSM computation to begin immediately after $\Delta, B, C$ are produced in the MMU. Compared to the traditional sequential implementation, our approach reduces the total computation time of the network by 32\% and increases hardware utilization from 58\% to 96\%, as depicted in Figure~\ref{fig: Pipeline scheme}(b).

% % Deploying Mamba on an FPGA requires the SSM and linear layers implemented with different hardware due to their distinct computational patterns. 
% Since SSM is composed of multiple independent heads, we first enhance hardware utilization through a head-by-head pipelined execution strategy. 
% This approach allows each head to operate independently, optimizing throughput and efficiency.
% However, due to Mamba's inherent data dependencies, the SSM's operators must await the completion of the input projection layer, which generates $\Delta, B, C$, before computation can commence, complicating the MMU and SSM pipelining as illustrated in Fig.~\ref{fig: Pipeline scheme}(a). 
% To address this, we introduced computation re-ordering, which alters the data generation sequence in the input projection layer. Specifically, $\Delta, B, C$ are generated first and stored in an on-chip buffer, while $X$ and $Z$ are produced head by head, as shown in Figure~\ref{fig: Pipeline scheme}(b).

% This reordering allows the SSM computation to begin immediately after $\Delta, B, C$ are produced in the MMU. Compared to the traditional sequential implementation, our approach reduces the total computation time of the network by 32\% and increases hardware utilization from 58\% to 96\%, as depicted in Figure~\ref{fig: Pipeline scheme}(b).









% However, a naive approach of unrolling and computing them sequentially introduces two main problems:
% \textbf{Data Dependency:} The overall latency increases due to the dependency between the SSM and the in project layer.
% The input projection outputs in the order of $Z,X, B,C,\Delta $, and the SSM can only start its computation after receiving $B$ and $C$, causing delays.
% % \textbf{Resource Utilization:} While one module (SSM or linear) is running, the other remains idle, leading to low utilization of DSP resources.
% \textbf{Bandwidth Utilization:} While one module (SSM or linear) is running, the other remains idle, leading to low bandwidth utilization.

% Sequentially executing MMU and SSMU
% will cause low hardware utilization as analyzed in Sec.~\ref{sec:motivation}.
% To this end, we propose compute-aware generation reordering,
% which alters the data generation order in the in project layer.
% Specifically, $\Delta $, $B$, and $C$ with small size are generated first and stored in an on-chip buffer, and $X$ and $Z$ are generated tile by tile as shown in Fig.~\ref{fig: Pipeline scheme}. 


% \ml{We need to make it clear SSM has different heads, which enables the pipelined execution of MMU and SSMU.$\checkmark$}

% To address these problems, we 
% alter the data generation order in the in projection layer,
% which can be realized through reordering the output channels of the weights offline. 
% The smaller tensors $\Delta $, $B$, and $C$ are generated first and stored in an on-chip buffer, followed by the generation of $X$ and $Z$ tile by tile as shown in Fig.~\ref{fig: Pipeline scheme}. 
% Additionally, we increase the parallelism of MMU
% and decrease the parallelism of SSM module to match between the linear layer and SSM.
% Additionally, the parallelism is designed to match between the linear layer and SSM.
% With these methods, we achieve an overlap between the input projection and SSM after generating $B$, $C$, and $\Delta t$.
% With this method, we achieve an overlap between the input projection and SSM after generating $B$, $C$, and $\Delta t$.
% Therefore, we can compute SSM right after
% the generation of $\Delta $, $B$, and $C$ in MMU.
% Compared to the naive sequential implementation, our approach reduces the total compute time of the network by 32\% and increases bandwidth utilization from 70\% to 99\%, as shown in Fig.~\ref{fig: Pipeline scheme}.

% \begin{figure}[!tb]
%     \centering
%     \includegraphics[width=0.8\columnwidth,height=4cm]{fig/FPGA_baseline.pdf}
%     \caption{Your caption here}
%     \label{fig:fig1}
% \end{figure}

\subsection{Fine-grained Tiling and Fusion}
\label{subsec: tiling and fusion}
% Although we designed a naive pipeline SSM implementation that computes head by head as discussed in the previous section, two new challenges have emerged: the SSM occupies 60\% of the on-chip memory, and the pipeline contains numerous bubbles, indicating that hardware utilization can still be improved.


% Although we designed a naive pipeline SSM implementation that computes head by head as discussed in the previous section, one new challenge has emerged: the SSM occupies 60\% of the on-chip memory, resulting in significant on-chip memory overhead. This substantial memory overhead stems from the necessity to store all intermediate activations within the SSM, particularly for tensors like $\Delta B u$, $\Delta A {h_{t-1}}$, and $h_t$ with dimensions of $p \times n$ as depicted in Fig.\ref{fig:tiling}(a).

% The SSM involves numerous operations, if each operation were fully computed before moving on to the next, we would need to store every intermediate activation, resulting in significant on-chip memory overhead, 
% The SSM performs numerous operations that, if processed sequentially with full computation for each, would necessitate storing all intermediate activations, causing substantial on-chip memory overhead. 
% especially for tensors such as $\Delta B u$, $\Delta A {h_{t-1}}$, and $h_t$, with dimensions of $p \times n$, as shown in Fig.~\ref{fig:tiling}(a). 



% Although we design a naive FPGA implementation that adopts a head-by-head mechanism similar to the attention mechanism introduced in Mamba-2, 
% SSM still occupies 60\% of on-chip memory. 
% To address it, we propose a fine-grained tiling and fusion strategy. 
% By leveraging operation fusion, we directly use the output of the previous operator as the input for the next, thus eliminating on-chip communication and data handling.
% These activations are kept in the on-chip buffer, while fine-grained tiling is employed to reduce the buffer size.
% we introduce fine-grained tiling along the head and hidden state dimensions, with a tile size of $n_p \times p_p$ as shown in Fig.~\ref{fig:tiling}(b). 
% This eliminates the need to store the entire activation tensor at each step of the SSM Unit, requiring only a tile-sized buffer for each step.
Given the current design, we observe the SSMU consumes more than 70\% of the on-chip memory as it requires to store all intermediate activations, 
e.g., $\Bar{B}X$, $\Bar{A}{h_{t-1}}$, and $h_t$, etc, as in Fig.\ref{fig:tiling}(a).
We propose a fine-grained tiling and fusion strategy. 
By leveraging operation fusion, we directly feed the output of the previous operator to the next operator,
and thus eliminating the on-chip communication and data handling.
Additionally, fine-grained tiling is employed to reduce the buffer size.
We tile along the head and hidden state dimensions, with a tile size of $n_p \times p_p$ as in Fig.~\ref{fig:tiling}(b).
The tile-by-tile implementation refines the execution of SSMU to enable the fine-grained pipeline in Fig.~\ref{fig: Pipeline scheme}(c), 
% which reduces the BRAM usage of SSMU from more than 60\% to less than 1\%. 
which reduces the URAM usage of SSMU by 4$\times$. 
This not only lower the on-chip buffer requirements, but also eliminates the pipeline bubbles for better hardware utilization.

% while fine-grained tiling is employed to reduce the buffer size. We introduce fine-grained tiling along the head and hidden state dimensions, with a tile size of $n_p \times p_p$ as shown in Fig.~\ref{fig:tiling}(b). 
% Previously, our approach computed activations tensor by tensor. We have now adapted this to a multi-step, tile by tile implementation. This method refines the execution of the SSM from the coarse-grained pipeline shown in Fig.~\ref{fig: Pipeline scheme}(b) to the fine-grained pipeline outlined in Fig.~\ref{fig: Pipeline scheme}(c). This method reduces the SSM's BRAM usage from 60\% to just 0.95\%, significantly lowering the on-chip buffer requirement and eliminating pipeline bubbles.
% and frequent on-chip reads and writes of intermediate activations lead to significant communication delays and high on-chip memory consumption. 
% We address the memory consumption challenge by proposing a fine-grained tiling and fusion strategy. By adopting an "Operator fusion" approach, the output of one operator is directly used as input for the next, without writing intermediate activations to off-chip memory. This reduces communication overhead. To mitigate the on-chip memory usage, we apply fine-grained tiling to minimize buffer size.

% In a naive SSM implementation, intermediate activation after each operation would be stored tensor by tensor, consuming over 60\% of the total on-chip memory, especially for tensors such as $\Delta B u$, $\Delta A {h_{t-1}}$, and $h_t$, with dimensions of $P \times N$. 

% The SSM involves numerous operations, and frequent on-chip reads and writes of intermediate activations result in significant communication delays and high on-chip memory usage. Moreover, the SSM is inherently memory-bound. To address the challenge of memory consumption in the SSM, we propose a fine-grained tiling and fusion strategy.
% A naive implementation of the SSM requires storing the intermediate activations after each operation, which accounts for more than 60\% of the total on-chip memory consumption, particularly for tensors such as  $\Delta B u$,$\Delta A h_t$ and $h_t$, all of which have dimensions of $ P \times N $. To mitigate this, we applied fine-grained tiling along the head and hidden state dimensions, with a tile size of $NP \times PP$, as illustrated in Fig.\ref{}. This optimization eliminates the need to store the entire activation tensor at each step of the SSM module, requiring only a tile-sized buffer for each step.
% With this approach, we reduced the SSM's on-chip memory usage from 60\% to just 0.95\%, significantly lowering the on-chip cache requirements. During the inference of each MAMBA layer, we only need to load the result of $h_{t-1}$ into the chip and write the result of $h_t$ back to off-chip memory, avoiding the frequent read/write operations of intermediate activations. This reduces communication overhead and improves overall performance.

% During each SSM computation in a Mamba layer, only the result of $h_{t-1}$ needs to be loaded on-chip, and the result of $h_t$ is written off-chip. This avoids frequent reads and writes of intermediate activations, reducing communication overhead and improving performance.


\begin{figure}[!tb]
    \centering
    \includegraphics[width=0.8\columnwidth]{fig/ssm-memory-motivation_v3.pdf}
    \caption{Fine-grained tiling and fusion.}
    \vspace{-10pt}
    % \caption{Fine-grained tiling and fusion: (a) Tensor by tensor method (b) Tile by tile method}
    \label{fig:tiling}
\end{figure}






