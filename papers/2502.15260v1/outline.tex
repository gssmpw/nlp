
% \onecolumn

\section*{Title}
LightMamba: Quantization and Hardware Co-design for Efficient Mamba Inference

\section*{Introduction}

We propose LightMamba, the first fully PTQ method for Mamba as well as the first Mamba accelerator.


\section*{Background}
\begin{enumerate}
    \item Mamba
    \item LLM Quantization
    \item FPGA-based LLM Accelerators

\end{enumerate}



\section*{Motivation/Challenge}
\begin{enumerate}

    \item Activation outliers in Mamba make quantization difficult, especially the non-persistent outliers in the output projection layer activation.
    
    $\star$ Table: kurtosis of each activation in the Mamba block
    
    $\star$ Fig: activation distribution of the input projection layer and the output projection layer

    \item Non-linear operations in Mamba (RMSNorm, SiLU, softplus, exp) require to be performed in integer.
    However, existing integer-only quantization works use the polynomial approximation method, which not only introduces complicated integer operations such as division and rounding, but also inferior network performance.
    
    \item Last but not least, we discover although SSM has only a small amount of computation (accounting for 1/12 of the Mamba block), it requires significant on-chip memory cost and latency due to multi-stage activation-activation multiplications. E.g, in Mamba-130M, SSM occupies 73\% memory cost and 34\% runtime of the entire block.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{fig/ssm-latency-motivation.jpg}
%     \caption{Enter Caption}
%     \label{fig:enter-label}
% \end{figure}
% \begin{figure}
%         \centering
%         \includegraphics[width=0.5\linewidth]{fig/ssm-memory-motivation.jpg}
%         \caption{Enter Caption}
%         \label{fig:enter-label}
%     \end{figure}
        
    $\star$ Fig: Bar chart, memory and latency consumption of SSM for different Mamba model
    



\end{enumerate}


\section*{Quantization Algorithm}
\subsection*{Rotation-based quantization}
    1. fuse RMSNorm: why the second RMSNorm can not be fused
    
    2. apply Hadamard Transformation and quantize: refer to the hardware and show the distribution before and after rotation

    3. can we rotate SSM?
    
    
% \subsection*{Non-uniform lookup table for quantizing non-linear operations}


\section*{Hardware design}
\subsection*{Overall architecture} 
    include Hadamard transform curcuit
    pipeline design
    show the relationship between algorithm and hardware
\subsection*{Fine-grained tiling and fusion}
\subsection*{Alternately generate x and z}


\section*{Experiments}
Quantization algorithm evaluation

W4A4, W8A8

different sizes of Mamba

compared with RTN, SmoothQuant, OutlierSuppression+
