\section{Introduction}

In recent years, vision language models (VLMs) have rapidly advanced, demonstrating remarkable capabilities in integrating and processing multimodal information \cite{liu2023llava, dai2023instructblip, chen2024internvl, xue2024xgen}. These models have found extensive applications across various domains, ranging from visual commonsense reasoning to sophisticated tasks like web agents \cite{xu2024llavacot, zhang2024improve, xie2024osworld, lin2024showui}. By leveraging both visual and textual data, VLMs promise a nuanced understanding that surpasses what can be achieved by analyzing them individually. %

Despite these advancements, current VLMs exhibit noticeable deficiencies in performing fundamental \textit{visual arithmetic}: these models struggle with seemingly simple tasks like accurately counting objects, comparing lengths, assessing angles, and evaluating relative sizes or areas \cite{Rahmanzadehgervi_2024_blind, wang2024vdlm, huang2024frompixels, ullman2024illusion, wei2024slow}. These shortcomings are particularly evident in complex tasks such as chart understanding \cite{huang-etal-2024-lvlms} and geometric problem-solving \cite{gao2023gllava}. \looseness=-1%


In this study, we first delve into the root causes of VLMs' difficulties with visual arithmetic, exploring several hypotheses to elucidate why VLMs often fail when faced with such challenges (\Cref{sec:probing}). We propose a suite of probing tasks, focusing on basic visual arithmetic such as length comparison, to answer this question. %
Our analysis reveals that pre-trained vision encoders coupled with a simple linear classifier perform poorly on these probing tasks, indicating that a single linear layer is insufficient to decode the complex visual representations for arithmetic reasoning. However, when we fine-tune the  text decoder of a VLM on these tasks, performance significantly improves.  This suggests \textbf{the bottleneck lies in the decoder's ability to effectively process and utilize the visual information, rather than in the visual representation itself}.  


To tackle these challenges, we propose a novel post-training strategy, \method~, designed to improve the performance of VLMs in visual arithmetic tasks (\Cref{sec:method}). Drawing inspiration from Piaget's theory of cognitive development \cite{piaget1952origins}, our method focuses on enhancing VLMs' understanding of \textit{conservation} (recognizing that certain properties remain unchanged despite transformations) and \textit{decentration} (considering multiple aspects simultaneously). We train VLMs using synthetically generated image pairs that demonstrate transformations, enabling them to compare and evaluate based on specific properties like length, angle, and quantity. By employing Direct Preference Optimization (DPO) \cite{rafailov2023dpo}, the model learns from both positive and negative examples, offering a richer learning signal than traditional Supervised Fine-Tuning (SFT). Our experiments show that \method~ significantly enhances performance across three VLMs of different scales and architectures on the proposed probing tasks.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth, trim=0 0 0 15, clip]{figures/probing_tasks.pdf}
    \vspace{-4mm}
    \caption{Examples of probing tasks designed to assess visual arithmetic abilities. Each task presents a visual input and a question requiring comparison or evaluation of geometric properties. At the bottom of each task, we see that even top-performing VLMs like GPT-4o and InternVL2.5-78B struggle with these seemly simple tasks.} %
    \vspace{-5mm}
    \label{fig:probing_tasks}
\end{figure*}

Furthermore, we evaluate \method~ on two downstream benchmarks: \chocolate~ \cite{huang-etal-2024-lvlms} for chart understanding, and \mathv~ \cite{wang2024mathv} for geometric problem-solving (\Cref{sec:exps}). Our results demonstrate the effectiveness of \method~ in enhancing performance on these complex tasks. On average, \method~ %
boosts performance by 4.6\% and 2.9\% on \chocolate~ and \mathv~ respectively, demonstrating that improving fundamental visual arithmetic capabilities translates to improved performance on downstream tasks. Notably, \method~ outperforms or achieves comparable performance to SFT methods while requiring 60\% less training data, even though \method~ does not involve direct optimization for specific tasks. %
This showcases its strong generalizability and highlights its potential of focusing on foundational skills to unlock broader capabilities in VLMs. \looseness=-1


Our main contributions are as follows:
\begin{itemize}[leftmargin=*]\itemsep0em 
    \item We conduct an in-depth analysis to uncover the root causes of VLMs' underperformance in tasks that involve visual arithmetic. \looseness=-1
    \item We develop \method~, a post-training strategy designed to enhance VLMs' abilities in understanding performing visual arithmetic.
    \item Extensive experiments on three VLMs show that \method~ significantly improves performance in chart comprehension and geometric problem-solving, highlighting its generalizability.
\end{itemize}








