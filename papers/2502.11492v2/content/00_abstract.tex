\begin{abstract}

Vision Language Models (VLMs) have achieved remarkable progress in multimodal tasks, yet they often struggle with visual arithmetic, seemingly simple capabilities like object counting or length comparison, %
which are essential for relevant complex tasks like chart understanding and geometric reasoning. In this work, we first investigate the root causes of this deficiency through a suite of probing tasks focusing on basic visual arithmetic. Our analysis reveals that while pre-trained vision encoders typically capture sufficient information, the text decoder often fails to decode it correctly for arithmetic reasoning. To address this, we propose \method~, a novel post-training strategy inspired by Piaget's theory of cognitive development.  \method~ trains VLMs to recognize invariant properties under visual transformations. %
We demonstrate that this approach significantly improves the performance of three diverse VLMs on our proposed probing tasks. Furthermore, \method~ enhances performance by an average of 4.6\% on \chocolate~ and 2.9\% on \mathv~, outperforming or matching supervised fine-tuning methods while requiring only 60\% less training data. These results highlight the effectiveness and generalizability of \method~ in improving fundamental visual arithmetic capabilities and their transfer to downstream tasks. \footnote{\method~ data has been released at: \url{https://github.com/SalesforceAIResearch/CogAlign}.}







\end{abstract}
