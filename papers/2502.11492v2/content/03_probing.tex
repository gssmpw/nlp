\section{Why Vision Language Models Struggle with Visual Arithmetic?}
\label{sec:probing}
As suggested in previous studies, VLMs struggle with visual arithmetic \cite{Rahmanzadehgervi_2024_blind, wang2024vdlm}, leading to poor performance in tasks involving such capabilities such as chart understanding \cite{huang-etal-2024-lvlms} and geometric problem-solving \cite{gao2023gllava}. In this section, we aim to understand the root causes behind such phenomenon. We first propose a suite of probing tasks we design to facilitate our analysis (\Cref{subsec:probing_tasks}) and then illustrate the various analyses we conduct to validate our hypotheses (\Cref{subsec:probing_analysis}).

\input{tables/probing_mlp}

\subsection{Probing Tasks}
\label{subsec:probing_tasks}

We propose four probing tasks for assessing visual arithmetic capabilities, motivated by the fundamental operations needed to interpret visual data quantitatively. For a VLM to successfully understand a chart, for example, it must be able to compare lengths of bars or lines, discern relationships indicated by line slopes, and projecting points onto axes. An overview of the probing tasks are shown in \Cref{fig:probing_tasks}. All four tasks are discriminative and can be considered binary classification tasks.  %
Below, we illustrate these tasks in details.





\paragraph{Angle Comparison} asks models to determine whether the angle of two wedges are the same. This requires the model to differentiate and measure angular magnitude, a seemly more complex operation that tests the model's grasp of spatial relationships and angular geometry. This task assesses the model's capacity to interpret rotational dimensions and engage in deeper analytical processing to distinguish subtle differences in angle, thereby evaluating the core geometric understanding of the model in angular perception. %
\vspace{-2mm}
\paragraph{Perpendicular Detection} challenges models to determine if two given lines are perpendicular to each other. Building upon the concept of angles, this task requires a deeper understanding of specific angular relationships, where perpendicularity implies a $90^\circ$ angle.  While Angle Comparison focuses on general angle differentiation, Perpendicular Detection assesses a model's ability to recognize this specific geometric configuration. 
\vspace{-2mm}

\paragraph{Length Comparison} asks models whether two lines with arbitrary slopes are of the same lengths. In addition to basic spatial reasoning, this task requires models to consider trigonometric relationships between the lines, demanding higher-level understanding of equivalence regardless of orientation. The variability in slopes necessitates an advanced ability to rotate or translate lines, challenging the model's proficiency in geometric reasoning beyond simple horizontal and vertical comparisons.


\vspace{-2mm}
\paragraph{Chart Projection} challenges the model to determine if the value of a red dot on a black line chart lies between 60 and 70. As the most complex task, this task integrates key aspects of the preceding tasks. It requires spatial reasoning to project the dot's position onto the y-axis, similar to Angle and Perpendicular Detection. It then involves comparing the projected value's magnitude against the specified range, akin to Length Comparison. %




\subsection{Probing Analysis}
\label{subsec:probing_analysis}

The research question we aim to answer is: \textbf{\textit{Do visual representations from pre-trained vision encoders contain enough information to perform visual arithmetic tasks?}} To answer this question, we conduct experiments by feeding the outputs from various encoders into a linear classifier to perform binary classification on the probing tasks. For each task, we randomly generate 12,000 images programmatically with a train:development:test split of 10:1:1. Each split has a balanced portion of positive and negative labels. We test a wide range of vision encoder, including  CLIP ViT-L/14 \cite{Radford2021LearningTV}, SigLIP-SO400M/14 \cite{Zhai_2023_siglip}, InternViT-300M-V2.5 \cite{chen2024internvl}, and DINOv2-Large \cite{oquab2024dinov}.%
We also evaluate the features produced by the projection layer of LLaVA-v1.5 \cite{liu2023llava}. Each model (i.e., the single classifier) %
was trained for 200 epochs and the checkpoint that achieves the highest performance on the development set is selected. \looseness=-1



The results are presented in \Cref{tab:probing_mlp}. Overall, we observe that fixed visual representations, when paired with a single linear layer, yield reasonable performance on simpler tasks such as Simple Length Comparison and Angle Comparison. However, they struggle significantly with more complex tasks like Length Comparison and Chart Projection. Therefore, we conclude that \textbf{pre-trained vision encoders do not convey sufficient information through their fixed visual representations for a linear classifier to succeed at visual arithmetic}. This may be attributed to two potential reasons: (1) the visual representations genuinely lack the information necessary for visual arithmetic tasks, or (2) a linear layer lacks the capacity to effectively leverage the visual features provided.


To further investigate the underlying cause of this limitation, we perform additional experiments by fine-tuning the LLM-based text decoder component of LLaVA-v1.5, while keeping its vision encoder frozen. In VLMs, visual representations are concatenated with text representations in the decoder. Unlike a linear layer, the text decoder can process textual queries as inputs, offering an opportunity to understand the effect of textual clues provided by input queries. We test with three different queries: an \textbf{\textsc{Original}} query reflecting the task as shown in \Cref{fig:probing_tasks}, an \textbf{\textsc{Empty}} query which is a blank string, and an \textbf{\textsc{Irrelevant}} query such as ``\textit{My name is John?}''. Additionally, we evaluate LLaVA-v1.5 in a zero-shot setting for comparisons. Given our previous observations, these experiments focus exclusively on the Length Comparison and Chart Projection tasks.
\input{tables/probing_llm}


The fine-tuned LLaVA results are shown in \Cref{tab:probing_llm}. We have the following observations. First, \textbf{existing VLMs do struggle with challenging visual arithmetic when used in zero-shot manners, achieving less than 90\% and 75\% on the two more challenging probing tasks, even with extensive in-domain training}. %
The finding is consistent with prior studies \cite{Rahmanzadehgervi_2024_blind, wang2024vdlm} and highlights the validity and complexity of our proposed probing tasks. Second, \textbf{VLMs fine-tuned on in-domain data perform reasonably well in visual arithmetic}. LLaVA-v1.5-7B is able to achieve an accuracy of above 95\% on both Length Comparison and Chart Projection tasks. Third, \textbf{the high performance of fine-tuned VLMs on in-domain data is due to the larger capacity of an LLM}. Comparing the three different queries, we see the performance on Length Comparison and Chart Projection does not vary too much. This means that a fine-tuned LLaVA-v1.5-7B performs well even when the query provides no clue or irrelevant information about the given tasks. Combing this observation with our findings in \Cref{tab:probing_mlp}, we learn that fine-tuned VLMs perform well because the LLM-based text decoder have larger capacity than a linear layer %
rather than leveraging the semantics of the input query. \looseness=-1




Based on the above findings and analyses, we conclude that \textbf{while visual representations from VLMs do encompass sufficient information for visual arithmetic, it cannot be effectively decoded without further fine-tuning, which may in turn affect their zero-shot performance on downstream tasks like chart understanding}. %
