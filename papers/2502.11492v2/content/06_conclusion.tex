\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth, trim=0 0 0 5, clip]{figures/general_benchmark_performance.pdf}
    \vspace{-4mm}
    \caption{Performance on two general VLM benchmarks: MME and MMMU with or without \method~.}
    \vspace{-5mm}
    \label{fig:general_benchamrk}
\end{figure}
\section{Conclusion}


This study investigates the challenges faced by VLMs in performing visual arithmetic, revealing that while visual encoders often capture necessary information, text decoders struggle to effectively utilize it.  We introduce \method~, a novel post-training strategy inspired by Piaget's theory of cognitive development, focusing on enhancing VLMs' understanding of conservation and decentration through DPO training. Our evaluations show that \method~ not only enhances VLMs' understanding of visual arithmetic, but also improves their performance in chart understanding and geometric problem-solving through experiments on the \chocolate~ and \mathv~ datasets, showcasing its effectiveness and generalizability across various models and tasks. Notably, \method~ often outperforms or achieves comparable results to task-specific supervised fine-tuning methods without direct training on the target domain, highlighting the potential of bolstering foundational cognitive skills for broader VLM capabilities. Future work could explore how \method~ impacts other multimodal tasks beyond charts and geometry, potentially leading to a more unified approach in VLM training where generalizability is prioritized. \looseness=-1

