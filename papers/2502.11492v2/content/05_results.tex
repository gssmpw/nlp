\section{Generalizability of \method~}

\label{sec:exps}
Now that we have demonstrated the advantage of \method~ on our probing tasks, we ask: \textbf{\textit{does the improvement on simple visual arithmetic tasks transfer to more complex tasks?}} To answer this question, we explore whether \method~ enhances model performance in chart understanding and geometric problem-solving. In the following subsections, we detail the experimental setup (\Cref{subsec:exp_setup}) and present our findings (\Cref{subsec:results}).

\subsection{Experimental Setups}
\label{subsec:exp_setup}

\paragraph{Benchmarks}
We evaluate the effectiveness of our method on two tasks relevant to visual arithmetic: chart understanding and geometry problem-solving. For chart understanding, we utilize the \chocolate~ dataset \cite{huang-etal-2024-lvlms}, which tests a model's capability to determine whether a given caption is factually consistent with its corresponding chart.\footnote{We decided against using other common datasets like ChartQA \cite{masry-etal-2022-chartqa} due to their training data already being included in some VLMs, such as LLaVA-OneVision.} \chocolate~ comprises three splits: \textsc{Lvlm}, \textsc{Llm}, and \textsc{Ft}, each generated by models of varying architectures and scales. Each \chocolate~ instance is annotated with a binary label $\mathcal{L} \in \{\texttt{consistent}, \texttt{inconsistent}\}$. The dataset includes a total of 1,187 chart-caption pairs. For geometry problem-solving, we assess performance using the test set of the \mathv~ dataset \cite{wang2024mathv}, which comprises 3,040 questions spanning 16 mathematical disciplines. We concentrate on the eight disciplines related to geometry: analytic geometry (\textsc{AnaG}), combinatorial geometry (\textsc{CombG}), descriptive geometry (\textsc{DescG}), solid geometry (\textsc{SolG}), transformation geometry (\textsc{TransG}), and three metric geometry branches - angle, area, and length. For evaluations, we employ AUC score for \chocolate~ and accuracy for \mathv~, in alignment with \citet{huang-etal-2024-lvlms} and \citet{wang2024mathv}. Detailed dataset statistics for these benchmarks are provided in \Cref{apx:dataset_stats}.


\input{tables/main}
\paragraph{Models and Baselines}


To assess the efficacy of \method~ compared to methods that directly optimize model capabilities towards specific tasks, we consider a chart supervised fine-tuning dataset: \textsc{ChartGemma160k} \cite{masry2024chartgemma}, as well as one geometric problem-solving dataset: \textsc{Geo170K} \cite{gao2023gllava}. 
We use the above methods to train three open-source VLMs for one epoch: InternVL2.5-1B-MPO \cite{wang2024mpo}, InternVL2.5-4B-MPO \cite{wang2024mpo}, and LLaVA-OV-0.5B \cite{li2024llavaov}. %
We also compare performance of two VLMs instruction-tuned specifically for chart understanding and geometric problem-solving: ChartGemma-3B \cite{masry2024chartgemma} and G-LLaVA-13B \cite{gao2023gllava}. Experimental details can be found in \Cref{apx:training_setting}.





\subsection{Results}
\label{subsec:results}


The results for experiments on \chocolate~ and \mathv~ are shown in \Cref{tab:main_results}. We find that \textbf{\method~ is effective in enhancing chart understanding and geometric problem-solving capabilities of VLMs even though \method~ was not specifically optimized for these two tasks.} On average, \method~ boosts the performance by 4.6\% and 2.9\% on the \chocolate~ and \mathv~ datasets, respectively. This shows that patching fundamental capabilities such as visual arithmetic of VLMs can enhance their capabilities in tasks involving such abilities. %

More importantly, we find that \textbf{\method~ demonstrates better generalizability compared to supervised fine-tuning VLMs using task-specific data}. For instance, when comparing the InternVL-2.5-MPO-1B variants, \method~ achieves an average score of 61.5\% on \chocolate~, outperforming both the \textsc{ChartGemma160K} (59.1\%) and \textsc{Geo170K} (59.2.\%) variants. Similarly, on the \mathv~ dataset, while the \textsc{Geo170K} variant shows competitive performance, \method~ achieves a comparable average performance across all geometry subtasks, indicating a broader improvement.  Notably, \method~ requires only 60\% less training data compared to these two baseline methods. %



The results suggest that \method~ offers a valuable approach to enhancing VLMs by improving their fundamental visual arithmetic capabilities. It exhibits strong generalizability across different tasks and base models, often outperforming or achieving comparable performance to task-specific fine-tuning methods without being explicitly trained on the target datasets. This highlights the potential of focusing on foundational skills to unlock broader capabilities in VLMs.


\subsection{Discussions}

\paragraph{Impact of learning from contrasting examples}
We investigate the impact of learning from contrasting examples versus solely positive examples by comparing DPO (the default \method~ setting) and SFT training method (using only the positive response). \Cref{fig:sft_vs_dpo} presents the results. We observe that the SFT approaches can lead to much worse performance (e.g. LLaVA-OV-0.5B), while the DPO approach improves performance over the original models more consistently. This suggests that learning from contrasting examples provides a richer learning signal compared to traditional supervised learning, leading to better performance. \looseness=-1


\paragraph{Impact on general VLM benchmarks}

To assess the impact of \method~ on general VLM capabilities, we compare the performance on two additional benchmarks: MME \cite{fu2023mme} and MMMU \cite{yue2024cvpr}. The results are presented in \Cref{fig:general_benchamrk}. Overall, \method~ consistently improves performance across most settings (five out of six), indicating that its benefits extend beyond the specific probing tasks and generalize to other multimodal reasoning challenges. This suggests that \textbf{\method~ enhances visual arithmetic capabilities without compromising performance on general tasks.}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth, trim=0 0 0 5, clip]{figures/sft_vs_dpo.pdf}
    \vspace{-4mm}
    \caption{Performance comparison when training different models with SFT and DPO.}
    \vspace{-5mm}
    \label{fig:sft_vs_dpo}
\end{figure}

