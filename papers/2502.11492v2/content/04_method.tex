\section{\method~}
\label{sec:method}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth, trim=20 50 20 40, clip]{figures/dpo_training_data.pdf}
    \vspace{-7mm}
    \caption{Example training data for \method~. Each example consists of a visual input, a query prompting comparison of a specific property (i.e. angle, length, distance, and etc), a positive response consistent with the visual input, and a negative response that contradicts it.}
    \vspace{-5mm}
    \label{fig:training_data}
\end{figure*}




To address the challenges VLMs face in performing visual arithmetic, we propose a novel post-training method inspired by Piaget's theory of cognitive development \cite{piaget1952origins}, which outlines four stages: Sensorimotor, Preoperational, Concrete Operational, and Formal Operational. Each stage represents a different ability to process information and solve problems, culminating in abstract reasoning. The Concrete Operational Stage is particularly relevant. At this stage, children develop (1) \textit{conservation}, understanding that certain properties like length remains constant despite changes in appearance, and (2) \textit{decentration}, the ability to consider multiple aspects of a situation at once. These skills are essential for VLMs to perform visual arithmetic accurately, recognizing invariant properties such as length or angle across transformations. Current VLM training paradigms often neglect these cognitive processes, resulting in models that struggle to maintain key properties during visual transformations and to integrate multiple visual features effectively. While pre-trained visual encoders use losses that encourage some invariance to transformations, the integration of vision and language representations in decoders often lacks explicit enforcement of conservation and decentration principles, leading to models that capture visual features but fail to reason about them effectively.






To address these issues, we present a post-training method, Cognitive Alignment (\method~), aimed at enhancing VLMs' understanding of \textit{conservation} and \textit{decentration}. Our approach %
explicitly trains VLMs to recognize invariant properties like length, angle, and count across different visual transformations. We achieve this by presenting the model with pairs of figures and associated queries designed to highlight these properties. The queries prompt the model to compare and contrast the figures, focusing on whether a specific property is different or same despite variations in appearance. This approach encourages the model to develop a stronger understanding of geometric concepts and move beyond superficial visual comparisons.
Furthermore, we leverage DPO \cite{rafailov2023dpo} for training, rather than SFT. DPO allows the model to learn from both positive and negative examples within the preference framework, providing a richer learning signal compared to SFT. By strengthening these cognitive capacities within VLMs, our goal is to improve their performance on tasks involving visual arithmetic. The subsequent subsections detail the specific training procedure employed (\Cref{subsec:training_setting}) and the automated construction of our training data (\Cref{subsec:training_collect}).




\subsection{DPO Training Objective}

\label{subsec:training_setting}
Our goal is to train a model with parameters $\theta$ that learns \textit{conservation} and \textit{decentration} from contrasting responses by maximizing the conditional probability of positive responses over their negative counterparts. Concretely, the DPO training data consists of preference pairs, each containing a user query $Q$, an input image $I$, a positive response $R_p$ and a negative response $R_n$. The entire set of DPO training data can be represented as $\mathcal{D}=\left\{ \left(Q,I,R_p,R_n \right)^{(i)} \right\}_{i=1}^{\lvert \mathcal{D} \rvert}$. The objective function $\mathcal{L}_\mathrm{DPO}$ that DPO minimizes is:

\begin{align*}
\label{eq:dpo}
\small
&\mathcal{L}_\mathrm{DPO}(\pi_\theta;\pi_\mathrm{ref}) =-\mathrm{E}_{(Q,I, R_p,R_n)\sim \mathcal{D}}\Big[\log \sigma (r_{\Delta})\Big],\\
&r_{\Delta} = \beta \log\frac{\pi_\theta(R_p\vert Q, I)}{\pi_\mathrm{ref}(R_p\vert Q, I)} - \beta\log\frac{\pi_\theta(R_n\vert Q, I)}{\pi_\mathrm{ref}(R_n\vert Q, I)},
\end{align*}
where $\sigma$ is the sigmoid function%
, $\pi_{\theta}$ is the parameterized policy under training%
, $\pi_\mathrm{ref}$ is the initial frozen %
policy, and $\beta$ is a hyper-parameter that controls the deviation from $\pi_\mathrm{ref}$.

\subsection{Training Data Synthesis}
\label{subsec:training_collect}



To effectively train VLMs on the principles of \textit{conservation} and \textit{decentration}, we require a training dataset designed to highlight these concepts. This section details our automated process for synthesizing training data, encompassing visual generation and tailored query-response construction. We draw inspiration from our probing tasks but adapt the format to better suit the DPO training procedure. By plotting two shapes within a single image, we allow the model to directly compare invariant properties like length and angle across various transformations. We devise eight fundamental tasks, each of which aim to enhance VLMs' different abilities to reason about visual arithmetic operations: understanding \textit{angle}, \textit{length}, \textit{distance}, \textit{quantity}, \textit{volume}, \textit{position}, \textit{slope}, and \textit{intersection}. An overview of these tasks is shown in \Cref{fig:training_data}

\input{tables/probing_ours}

To automate data synthesis, we present a data generation pipeline. First, we programmatically generate images using Python, allowing precise control over each figure's properties, such as lengths and positions. Next, we create query-response pairs for these images based on predefined templates (see \Cref{tab:query_templates}). These queries are designed to prompt VLMs to make comparisons, identify similarities/dissimilarities, and reason about geometric properties. Given the known ground truth, positive responses are generated by accurately populating placeholders in the templates, while negative responses are created with incorrect values. For instance, a positive query for the first sub-figure in \Cref{fig:training_data} might be, “\textit{The angle S is larger.}”, and a negative query might be, “\textit{The angle C is larger.}” To ensure diversity, we use an LLM\footnote{\texttt{gpt-4o} is used for paraphrasing.} to create multiple variations of each query and response, following the approach of \citet{huang-etal-2024-crmarena}. We synthesize a total of 64,000 training instances for DPO, with a balanced splits of each task.







\subsection{Effectiveness on the Probing Tasks}



To assess the effectiveness of \method~ on our proposed probing tasks, we trained three VLMs with varying scales and architectures: LLaVA-OV-0.5B \cite{li2024llavaov}, InternVL-2.5-MPO-1B \cite{wang2024mpo}, and InternVL-2.5-MPO-4B using \method~, as described in \Cref{subsec:training_setting} and \Cref{subsec:training_collect}, for one epoch. The results are presented in Table 3.%

We observe that \method~ demonstrably improves performance across all three models across all probing tasks. More significant gains are observed on simpler tasks of Angle Comparison, likely due to their similarity with the DPO training instances. For instance, LLaVA-OV-0.5B sees a substantial 54.0\% improvement on Angle Comparison after training with \method~.  This highlights the effectiveness of our approach in enhancing the core visual arithmetic capabilities that are crucial for these tasks. Interestingly, while the gain in angle-related tasks like Angle Comparison was substantial, the performance increases for Perpendicularity Detection were more modest (e.g., a 2.0\% improvement for LLaVA-OV-0.5B). This suggests that certain geometric properties, such as perpendicularity, may pose greater challenges. %


Overall, these findings collectively demonstrate the effectiveness of \method~ in enhancing visual arithmetic capabilities across various VLMs.%



