\section{Related Works}


\subsection{Vision Language Models}
Vision language models (VLMs) are multimodal models that learns to generate text outputs based on both visual and textual inputs. The development of large-scale VLMs has demonstrated impressive zero-shot capabilities, enabling them to perform well with a variety of image types, such as documents and web pages \cite{liu2023llava, dai2023instructblip, Achiam2023GPT4TR, gemini, claude}. These VLMs generally consist of three major components: a vision encoder, such as CLIP \cite{Radford2021LearningTV} or SigLIP \cite{Zhai_2023_siglip}, which processes visual inputs; a language model that handles textual inputs and generates text tokens; and a projector layer that connects the image and text modalities. Typically, VLMs are trained using image captioning data and instruction-tuning datasets. Recently, several post-training strategies have been suggested to enhance VLM capabilities in areas like conversational interaction \cite{xiong2024llavaovchat} and reasoning \cite{wang2024mpo}. In this work, we propose a new post-training strategy, \method~, for improving VLMs' proficiency in understanding visual arithmetic operations. \looseness=-1



\subsection{Shortcomings of Vision Language Models}

While Vision-Language Models (VLMs) demonstrate impressive performance across a range of tasks, several studies have highlighted their limitations by examining various aspects such as architectures \cite{McKinzie2024MM1, Karamcheti2024Prismatic, Tong_2024_eyes, shi2025when}, training methods \cite{laurencon2024what}, and data considerations \cite{udandarao2024no, gadre2024datacomp,zhang2024why,wei2024slow}. Some research indicates that VLMs struggle with specific tasks, including basic geometric understanding \cite{gao2023gllava,ullman2024illusion} and chart comprehension \cite{huang-etal-2024-lvlms}, and are prone to hallucinations \cite{qiu2024valor}.  Our study seeks to uncover the root causes behind these challenges, especially those that involve visual arithmetic operations, and proposes solutions to address these shortcomings.  \looseness=-1
