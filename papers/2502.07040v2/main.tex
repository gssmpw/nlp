%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{subcaption}
\usepackage{stmaryrd}
\usepackage{bm}
%%%%

\newcommand{\R}{\mathbb{R}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\Po}{\bm{\mathcal P}}
\newcommand{\N}{\mathbb{N}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\dott}[2]{\left\langle#1,#2\right\rangle}
\newcommand{\norm}[1]{\left\lVert #1\right\rVert}
\newcommand{\intervalle}[4]{\mathopen{#1}#2\mathclose{},#3\mathclose{#4}}
\newcommand{\ff}[2]{\intervalle{[}{#1}{#2}{]}}
\newcommand{\oo}[2]{\intervalle{(}{#1}{#2}{)}}
\newcommand{\of}[2]{\intervalle{(}{#1}{#2}{]}}
\newcommand{\fo}[2]{\intervalle{[}{#1}{#2}{)}}

\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\algnewcommand{\algorithmicendif}{\textbf{end}}
\algblockdefx[IF]{If}{EndIf}[1]{\algorithmicif\ #1\ \algorithmicthen}{\algorithmicendif}
\algblockdefx[For]{for}{EndFor}[1]{\algorithmicif\ #1\ \algorithmicthen}{\algorithmicendif}


%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
%%\newtheorem{proposition}[theorem]{Proposition}% 
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads


\begin{document}

\title[Article Title]{Robust high-order low-rank BUG integrators based on explicit Runge-Kutta methods}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1]{\fnm{Fabio} \sur{Nobile}}
\author*[1]{\fnm{S\'ebastien} \sur{Riffaud}}\email{sebastien.riffaud@epfl.ch}

\affil[1]{\orgdiv{CSQI Chair}, \orgname{\'Ecole Polytechnique F\'ed\'erale de Lausanne}, \city{1015 Lausanne}, \country{Switzerland}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{In this work, we propose high-order basis-update \& Galerkin (BUG) integrators based on explicit Runge-Kutta methods for large-scale matrix differential equations. These dynamical low-rank integrators are high-order extensions of the BUG integrator \cite{ceruti2022rank} and are constructed by performing a BUG step at each stage of the Runge-Kutta method. In this way, the resulting Runge-Kutta BUG integrator is robust to the presence of small singular values and does not involve backward time-integration steps. We provide an error bound, which shows that the Runge-Kutta BUG integrator retains the order of convergence of the associated Runge-Kutta method until the error reaches a plateau corresponding to the low-rank truncation error and which vanishes as the rank becomes full. This error bound is finally validated experimentally on three numerical test cases. The results demonstrate the high-order convergence of the Runge-Kutta BUG integrator and its superior accuracy compared to other dynamical low-rank integrators proposed in the literature.}


\keywords{Dynamical low-rank approximation, Matrix differential equations, Basis-Update \& Galerkin integrators, Runge-Kutta methods}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle


\section{Introduction}
\label{sec:1}

Dynamical low-rank approximations (DLRAs) enable a significant reduction in the computational cost associated with numerical integration of large-scale matrix differential equations:
\begin{equation}
\label{eq:ode}
\dot{\mathbf A} = {\mathbf F}(t,{\mathbf A}), \qquad {\mathbf A}(0) = {\mathbf A}_0 \in \R^{n \times m},
\end{equation}
which appear in many applications, such as kinetic equations \cite{bernard2018reduced,einkemmer2018low,einkemmer2020low,einkemmer2021mass,coughlin2022efficient,einkemmer2024accelerating,einkemmer2024review} due to the large phase space, stochastic simulations  \cite{sapsis2009dynamically,babaee2017robust,musharbash2018dual,feppon2018dynamically,musharbash2020symplectic,patil2020real,kazashi2021existence,kazashi2021stability,kazashi2025dynamical} due to the repeated evaluation of the solution for different realizations of the random terms, or sequential parameter estimation \cite{kressner2011low,weinhandl2018linear,benner2024low,riffaud2024low} when tracking solutions for several parameter values. The main idea of DLRAs consists in approximating the solution ${\mathbf A}$ by a low-rank matrix ${\mathbf Y}$ in an SVD-like form:
\begin{equation}
\label{eq:sol}
{\mathbf Y}(t) = {\mathbf U}(t) {\mathbf S}(t) {\mathbf V}(t)^T \in \R^{n \times m},
\end{equation}
where ${\mathbf U} \in \R^{n \times r}$ and ${\mathbf V} \in \R^{m \times r}$ are orthonormal matrices, ${\mathbf S} \in \R^{r \times r}$ is a square invertible matrix (not necessarily diagonal), and $r \leq \min\{n,m\}$ is the rank of ${\mathbf Y}$.

A challenging issue concerns the time-integration of the different factors ${\mathbf U}$, ${\mathbf S}$, and ${\mathbf V}$. Perhaps the most intuitive approach is to consider the differential equations, derived in \cite{koch2007dynamical}, that describe the evolution of the low-rank factorization over time:
\begin{equation}
\label{eq:low-rank_ode}
\left\{
\begin{aligned}
\dot{\mathbf U} &= \left({\mathbf I} - {\mathbf U}{\mathbf U}^T \right) {\mathbf F}(t,{\mathbf Y}){\mathbf V}{\mathbf S}^{-1}\\
\dot{\mathbf S} &= {\mathbf U}^T {\mathbf F}(t,{\mathbf Y}){\mathbf V}\\
\dot{\mathbf V} &= \left({\mathbf I} - {\mathbf V}{\mathbf V}^T \right){\mathbf F}(t,{\mathbf Y})^T{\mathbf U}{\mathbf S}^{-T},
\end{aligned}
\right.
\end{equation}
where we have assumed that ${\mathbf U}^T\dot{\mathbf U}={\mathbf V}^T\dot{\mathbf V}={\mathbf 0}$. Unfortunately, this system involves the inverse of ${\mathbf S}$, which can cause severe time-step size restrictions if the singular values of ${\mathbf S}$ are small, since the step size of standard time-integration schemes must be proportional to the smallest nonzero singular value. An equivalent formulation to \eqref{eq:low-rank_ode} is obtained by projecting the matrix differential equation \eqref{eq:ode} onto the tangent space of the manifold $\M_r$ of rank-$r$ matrices:
\begin{equation}
\label{eq:projected_ode}
\dot{\mathbf Y} = \Po_{\mathbf Y}\big({\mathbf F}(t,{\mathbf Y})\big), \qquad {\mathbf Y}(0) = {\mathbf Y}_0 \in \M_r,
\end{equation}
where 
\begin{equation}
\label{eq:tangent-space_projection}
\Po_{\mathbf Y}({\mathbf X}) = {\mathbf U}{\mathbf U}^T{\mathbf X}-{\mathbf U}{\mathbf U}^T{\mathbf X}{\mathbf V}{\mathbf V}^T+{\mathbf X}{\mathbf V}{\mathbf V}^T
\end{equation}
stands for the orthogonal projection of ${\mathbf X} \in \R^{n \times m}$ onto the tangent space at ${\mathbf Y}={\mathbf U}{\mathbf S}{\mathbf V}^T \in \M_r$. In the projection-splitting integrator \cite{lubich2014projector,kieri2016discretized}, this tangent-space projection is split into three alternating subprojections using the Lie-Trotter or Strang splitting. The resulting low-rank integrator is robust to the presence of small singular values, but the step associated with the update of ${\mathbf S}$ integrates the solution backward in time, which can lead to instabilities for parabolic and hyperbolic problems \cite{kusch2023stability}.

In recent years, several dynamical low-rank integrators that are robust to the presence of small singular values and do not involve backward time-integration steps have been proposed:
\begin{itemize}
\item the basis-update \& Galerkin (BUG) integrators \cite{ceruti2022unconventional,ceruti2022rank,ceruti2024parallel}, where ${\mathbf U}$ and ${\mathbf V}$ are first updated using the tangent-space projection, and then ${\mathbf S}$ is updated using a Galerkin projection of the matrix differential equation \eqref{eq:ode} onto the updated bases ${\mathbf U}$ and ${\mathbf V}$;
\item the projected Runge-Kutta methods \cite{kieri2019projection,carrel2023projected}, in which the projected differential equation \eqref{eq:projected_ode} is integrated using a Runge-Kutta method which includes a truncation step that maintains low-rank approximations.
\end{itemize}
The convergence of the BUG integrator is limited to order one, but recent efforts are made to extend this dynamical low-rank integrator to higher orders. For instance, second-order extensions based on the midpoint rule have been proposed in \cite{ceruti2024robust,kusch2024second}. On the other hand, projected Runge-Kutta methods have high-order error bounds for time-explicit discretizations. However, these dynamical low-rank integrators do not share the conservation properties \cite{einkemmer2023conservation} offered by the Galerkin projection of the BUG integrator.

In this work, we propose high-order BUG integrators based on explicit Runge-Kutta methods. These dynamical low-rank integrators are high-order extensions of the BUG integrator and are constructed by performing a BUG step at each stage of the Runge-Kutta method. In this way, the resulting dynamical low-rank integrators are robust to the presence of small singular values and do not involve backward time-integration steps. Moreover, compared with projected Runge-Kutta methods, the only difference is that the tangent-space projection is replaced by the Galerkin projection of the BUG integrator. As a result, Runge-Kutta BUG integrators present two advantages. First, conservation properties can be preserved more easily. Second, the Galerkin projection of Runge-Kutta BUG integrators is more accurate than the tangent-space projection of projected Runge-Kutta methods for approximating ${\mathbf F}$ at the discrete level (see Proposition \ref{theo:P3}). Furthermore, we prove in Theorem \ref{theo:T2} that the Runge-Kutta BUG integrator retains the order of convergence of the associated Runge-Kutta method until the error reaches a plateau corresponding to the low-rank truncation error and which vanishes as the rank becomes full. In particular, this property holds for any explicit Runge-Kutta method, allowing in practice the construction of various high-order dynamical low-rank integrators.

The remainder of the paper is organized as follows. Section \ref{sec:2} introduces the first-order BUG integrator based on the forward Euler method. In Section \ref{sec:3}, we present high-order extensions of the BUG integrator based on explicit Runge-Kutta methods. Then, Section \ref{sec:4} analyzes the convergence of the proposed dynamical low-rank integrators. In Section \ref{sec:5}, the resulting error bound is validated experimentally on three numerical test cases. Finally, Section \ref{sec:6} draws some conclusions and perspectives.



\section{The BUG integrator based on the forward Euler method}
\label{sec:2}

The dynamical low-rank integrator proposed in this work is an extension of the first-order BUG integrator \cite{ceruti2022rank}. For the convenience of the reader, we start by presenting the latter in the time-explicit case, i.e. for the forward Euler method. Let the time be discretized using a fixed time-step size $h>0$. The integration of the rank-$r$ solution ${\mathbf Y}_k = {\mathbf U}_k {\mathbf S}_k {\mathbf V}_k^T$ from time $t_k$ to $t_k+h$ reads:
\begin{enumerate}
\item \textbf{K-step:} Assemble
\begin{equation}
\label{eq:k_step}
{\mathbf K} = \left[{\mathbf U}_k \enspace\; {\mathbf F}(t_k,{\mathbf Y}_k){\mathbf V}_k \right] \in \R^{n \times 2r},
\end{equation}
and compute $\widehat{\mathbf U}_{k+1} \in \R^{n \times \widehat r}$ with $\widehat r \in \{r,\ldots,\min\{n,m,2r\}\}$ as an orthonormal basis of the range of ${\mathbf K}$ (e.g., by QR decomposition), in short $\widehat{\mathbf U}_{k+1} = \textrm{ortho}({\mathbf K})$.
\item \textbf{L-step:} Assemble
\begin{equation}
\label{eq:s_step}
{\mathbf L} = \left[{\mathbf V}_k \enspace\; {\mathbf F}(t_k,{\mathbf Y}_k)^T{\mathbf U}_k \right] \in \R^{m \times 2r},
\end{equation}
and compute the augmented basis $\widehat{\mathbf V}_{k+1} = \textrm{ortho}({\mathbf L}) \in \R^{m \times \widehat r}$.
\item \textbf{S-step:}  Set
\begin{equation}
\label{eq:l_step}
\widehat{\mathbf S}_{k+1} = \widehat{\mathbf U}_{k+1}^T \bigl({\mathbf Y}_{k} + h {\mathbf F}(t_k,{\mathbf Y}_{k}) \bigr) \widehat{\mathbf V}_{k+1}  \in \R^{\widehat r \times \widehat r}.
\end{equation}
\item \textbf{Truncation step:} Let the $r$-truncated singular value decomposition (SVD) of $\widehat{\mathbf S}_{k+1}$ be ${\boldsymbol\Phi} {\boldsymbol\Sigma} {\boldsymbol\Psi}^T$, where ${\boldsymbol\Phi},{\boldsymbol\Psi}  \in \R^{\widehat r \times r}$ are orthonormal matrices and ${\boldsymbol\Sigma} \in \R^{r \times r}$ is a diagonal matrix with non-negative real numbers on the diagonal. Set
\begin{equation}
\label{eq:t_step}
\begin{aligned}
{\mathbf U}_{k+1} &= \widehat{\mathbf U}_{k+1} {\boldsymbol\Phi}, \\
{\mathbf S}_{k+1} &= {\boldsymbol\Sigma}, \\
{\mathbf V}_{k+1} &= \widehat{\mathbf V}_{k+1} {\boldsymbol\Psi}.
\end{aligned}
\end{equation}
\end{enumerate}
To simplify notation, let $\llbracket {\mathbf X} \rrbracket_r$ denote the projection of ${\mathbf X}$ onto $\M_r$, which is given by the $r$-truncated SVD of ${\mathbf X}$. The BUG integrator based on the forward Euler method is summarized in Algorithm \ref{al:bug}. Note that the rank $r$ has been fixed here for simplicity, but an adaptative rank can also be used to truncate the augmented solution $\widehat{\mathbf Y}_{k+1}=\widehat{\mathbf U}_{k+1}\widehat{\mathbf S}_{k+1}\widehat{\mathbf V}_{k+1}^T$.



\section{Extension of the BUG integrator to high-order explicit Runge-Kutta methods}
\label{sec:3}

We now extend the BUG integrator to high-order explicit Runge-Kutta methods. Let us consider the explicit Runge-Kutta method:
\begin{equation}
\label{eq:rk}
\begin{aligned}
{\mathbf A}_{k i} &= {\mathbf A}_{k} + h \sum_{j=1}^{i-1} a_{i j} {\mathbf F}(t_{k j},{\mathbf A}_{k j}), \quad i=1,\ldots,s, \\
{\mathbf A}_{k+1} &= {\mathbf A}_{k} + h \sum_{i=1}^s b_{i} {\mathbf F}(t_{k i},{\mathbf A}_{k i}),
\end{aligned}
\end{equation}
where $t_{k i} = t_k + c_i h$. The main idea is to perform one step of the BUG integrator at each stage of the Runge-Kutta method. To this end, the key point concerns the definition of the augmented bases. Consider, for example, the final stage with the initial value ${\mathbf Y}_{k}$:
\begin{equation*}
{\mathbf Y}_{k+1} = {\mathbf Y}_{k} + h \sum_{i=1}^s b_{i} {\mathbf F}(t_{k i},{\mathbf Y}_{k i}).
\end{equation*}
The augmented bases $\widehat{\mathbf U}_{k+1}$ and $\widehat{\mathbf V}_{k+1}$ are constructed here to represent exactly ${\mathbf Y}_{k}$ and the tangent-space projection of the different terms ${\mathbf F}(t_{k i},{\mathbf Y}_{k i})$. Notably, ${\mathbf Y}_{k}$ is represented on the left and right bases ${\mathbf U}_{k}$ and ${\mathbf V}_{k}$, while the tangent-space projection of ${\mathbf F}(t_{k i},{\mathbf Y}_{k i})$ is represented according to equation \eqref{eq:tangent-space_projection} on $[{\mathbf U}_{k i} \enspace\; {\mathbf F}_{k i}{\mathbf V}_{k i}]$ and $[{\mathbf V}_{k i} \enspace\; {\mathbf F}_{k i}^T{\mathbf U}_{k i}]$, where ${\mathbf F}_{k i}={\mathbf F}(t_{k i},{\mathbf Y}_{k i})$. Moreover, let us introduce the coefficients $\alpha_{i j}$ and $\beta_i$, defined as 
\begin{equation*}
\alpha_{i j} = \left \{
\begin{array}{l l}
1 & \textrm{if } a_{i j} \neq 0 \\
0 & \textrm{otherwise}
\end{array}
\right.
\qquad
\textrm{and}
\qquad
\beta_{i} = \left \{
\begin{array}{l l}
1 & \textrm{if } b_{i} \neq 0 \\
0 & \textrm{otherwise,}
\end{array}
\right.
\end{equation*}
which will allow us to discard the bases that are not involved in the different tangent-space projections. The augmented bases $\widehat{\mathbf U}_{k+1}$ and $\widehat{\mathbf V}_{k+1}$ are given by
\begin{equation*}
\begin{aligned}
\widehat{\mathbf U}_{k+1} &\gets \textrm{ortho}([{\mathbf U}_{k} \enspace\; \beta_{1} {\mathbf U}_{k 1} \enspace\; \beta_{1} {\mathbf F}_{k 1}{\mathbf V}_{k 1} \enspace\; \cdots \enspace\; \beta_{s} {\mathbf U}_{k s} \enspace\; \beta_{s} {\mathbf F}_{k s}{\mathbf V}_{k s}]), \\
\widehat{\mathbf V}_{k+1} &\gets \textrm{ortho}([{\mathbf V}_{k} \enspace\; \beta_{1} {\mathbf V}_{k 1} \enspace\; \beta_{1} {\mathbf F}_{k 1}^T{\mathbf U}_{k 1} \enspace\; \cdots \enspace\; \beta_{s}{\mathbf V}_{k s} \enspace\; \beta_{s}{\mathbf F}_{k s}^T {\mathbf U}_{k s}]),
\end{aligned}
\end{equation*}
where the terms $\beta_{1} {\mathbf U}_{k 1}$ and $\beta_{1} {\mathbf V}_{k 1}$ can be removed since ${\mathbf U}_{k 1}= {\mathbf U}_{k}$ and ${\mathbf V}_{k 1}= {\mathbf V}_{k}$. The Runge-Kutta BUG integrator associated with the Runge-Kutta method \eqref{eq:rk} is finally described in Algorithm \ref{al:rk_bug}. As mentioned previously, an adaptative rank can also be used to ensure, for example, that the error of the truncation step is smaller than a prescribed tolerance.

\begin{remark}
\label{theo:R1}
The rank of the augmented solution $\widehat{\mathbf Y}_{k+1}$ (resp. $\widehat{\mathbf Y}_{k,i+1}$) is at most $2rs$ (resp. $2ri$).
\end{remark}

\begin{remark}
\label{theo:R2}
When $r=\min\{n,m\}$, the Runge-Kutta BUG integrator is equivalent to the associated Runge-Kutta method, since the manifold $\M_r$ corresponds to the whole Euclidean space $\R^{n \times m}$, and there is therefore no projection or truncation error.
\end{remark}


\begin{algorithm}[H]
\caption{First-order BUG integrator \cite{ceruti2022rank} based on the forward Euler method}
\label{al:bug}
\begin{algorithmic}[1]
\Require {${\mathbf Y}_0$.}
\Ensure {${\mathbf Y}_{1},\ldots,{\mathbf Y}_{N}$.}
\For {$k=0,\ldots,N-1$}
\State ${\mathbf Y}_k := {\mathbf U}_{k} {\mathbf S}_{k} {\mathbf V}_{k}^T$;
\State ${\mathbf F}_k \gets {\mathbf F}(t_k,{\mathbf Y}_k)$;
\State $\widehat{\mathbf U}_{k+1} \gets \textrm{ortho}([{\mathbf U}_k \enspace\; {\mathbf F}_k {\mathbf V}_k])$; \Comment{K-step \eqref{eq:k_step}}
\State $\widehat{\mathbf V}_{k+1} \gets \textrm{ortho}([{\mathbf V}_k \enspace\; {\mathbf F}_k^T {\mathbf U}_k])$; \Comment{L-step \eqref{eq:l_step}}
\State $\widehat{\mathbf S}_{k+1} \gets \widehat{\mathbf U}_{k+1}^T \bigl( {\mathbf Y}_{k} + h {\mathbf F}_k \bigr) \widehat{\mathbf V}_{k+1}$; \Comment{S-step \eqref{eq:s_step}}
\State $\widehat{\mathbf Y}_{k+1} \gets \widehat{\mathbf U}_{k+1} \widehat{\mathbf S}_{k+1} \widehat{\mathbf V}_{k+1}^T$. 
\State ${\mathbf Y}_{k+1} \gets \llbracket \widehat{\mathbf Y}_{k+1} \rrbracket_r$. \Comment{Truncation step \eqref{eq:t_step}}
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{High-order explicit Runge-Kutta BUG integrator}
\label{al:rk_bug}
\begin{algorithmic}[1]
\Require {${\mathbf Y}_0$.}
\Ensure {${\mathbf Y}_{1},\ldots, {\mathbf Y}_{N}$.}
\For {$k=0,\ldots,N-1$}
\State ${\mathbf Y}_{k 1} \gets {\mathbf Y}_k$;
\For {$i=1,\ldots,s-1$}
\State ${\mathbf Y}_{k i} := {\mathbf U}_{k i} {\mathbf S}_{k i} {\mathbf V}_{k i}^T$;
\State ${\mathbf F}_{k i} \gets {\mathbf F}(t_k + c_i h,{\mathbf Y}_{k i})$;
\State $\widehat{\mathbf U}_{k, i+1} \gets \textrm{ortho}([{\mathbf U}_{k} \enspace\; \alpha_{i+1,1} {\mathbf F}_{k 1}{\mathbf V}_{k 1} \enspace\; \cdots \enspace\; \alpha_{i+1,i} {\mathbf U}_{k i} \enspace\; \alpha_{i+1,i} {\mathbf F}_{k i}{\mathbf V}_{k i}])$; 
\State $\widehat{\mathbf V}_{k,i+1} \gets \textrm{ortho}([{\mathbf V}_{k} \enspace\; \alpha_{i+1,1} {\mathbf F}_{k 1}^T{\mathbf U}_{k 1} \enspace\; \cdots \enspace\; \alpha_{i+1,i} {\mathbf V}_{k i} \enspace\; \alpha_{i+1,i} {\mathbf F}_{k i}^T{\mathbf U}_{k i}])$; 
\State $\widehat{\mathbf S}_{k,i+1} \gets \widehat{\mathbf U}_{k,i+1}^T \bigl( {\mathbf Y}_{k} + h ( a_{i+1,1} {\mathbf F}_{k 1} + \ldots + a_{i+1,i} {\mathbf F}_{k i} ) \bigr) \widehat{\mathbf V}_{k,i+1}$; 
\State $\widehat{\mathbf Y}_{k,i+1} \gets \widehat{\mathbf U}_{k,i+1} \widehat{\mathbf S}_{k,i+1} \widehat{\mathbf V}_{k,i+1}^T$;
\State ${\mathbf Y}_{k,i+1} \gets \llbracket \widehat{\mathbf Y}_{k,i+1} \rrbracket_r$; 
\EndFor
\State ${\mathbf Y}_{k s} := {\mathbf U}_{k s} {\mathbf S}_{k s} {\mathbf V}_{k s}^T$;
\State ${\mathbf F}_{k s} \gets {\mathbf F}(t_k + c_s h,{\mathbf Y}_{k s})$;
\State $\widehat{\mathbf U}_{k+1} \gets \textrm{ortho}([{\mathbf U}_{k} \enspace\; \beta_{1} {\mathbf F}_{k 1}{\mathbf V}_{k 1} \enspace\; \cdots \enspace\; \beta_{s} {\mathbf U}_{k s} \enspace\; \beta_{s} {\mathbf F}_{k s}{\mathbf V}_{k s}])$; 
\State $\widehat{\mathbf V}_{k+1} \gets \textrm{ortho}([{\mathbf V}_{k} \enspace\; \beta_{1} {\mathbf F}_{k 1}^T{\mathbf U}_{k 1} \enspace\; \cdots \enspace\; \beta_{s}{\mathbf V}_{k s} \enspace\; \beta_{s}{\mathbf F}_{k s}^T {\mathbf U}_{k s}])$; 
\State $\widehat{\mathbf S}_{k+1} \gets \widehat{\mathbf U}_{k+1}^T \bigl( {\mathbf Y}_{k} + h ( b_{1} {\mathbf F}_{k 1} + \ldots + b_{s} {\mathbf F}_{k s} ) \bigr) \widehat{\mathbf V}_{k+1}$; 
\State $\widehat{\mathbf Y}_{k+1} \gets \widehat{\mathbf U}_{k+1} \widehat{\mathbf S}_{k+1} \widehat{\mathbf V}_{k+1}^T$. 
\State ${\mathbf Y}_{k+1} \gets \llbracket \widehat{\mathbf Y}_{k+1} \rrbracket_r$. 
\EndFor
\end{algorithmic}
\end{algorithm}



\section{Convergence analysis}
\label{sec:4}

In this section, we analyze the convergence of the Runge-Kutta BUG integrator in the case where the rank $r$ is fixed and for a time-step size $h$ sufficiently small, say $h \leq h_0$ (see Theorem 3.4 of Chapter 2 in \cite{harrier1993solving} for the exact definition of $h_0$). The present analysis is based on two assumptions. The first assumption implies the existence and uniqueness of the exact solution ${\mathbf A}$ according to the Picard-Lindel\"of theorem, while the second assumption is needed in Theorem 3.1 of Chapter 2 in \cite{harrier1993solving} to analyze the local error of high-order Runge-Kutta methods. Furthermore, this analysis is done in the Frobenius norm $\norm{\cdot}_F$, and $\dott{\cdot}{\cdot}_F$ will stand for the Frobenius inner product in the following.

\medskip

\begin{assumption}
\label{theo:A1}
${\mathbf F}(t, {\mathbf X})$ is continuous in time and Lipschitz continuous in ${\mathbf X}$: there exists a Lipschitz constant $L>0$ (independent of $t$) such that
\begin{equation*}
\norm{{\mathbf F}(t,{\mathbf X}_1)-{\mathbf F}(t,{\mathbf X}_2)}_F \leq L \norm{{\mathbf X}_1-{\mathbf X}_2}_F
\end{equation*}
for all $t \in \ff{0}{T}$ and ${\mathbf X}_1,{\mathbf X}_2 \in \R^{n \times m}$.
\end{assumption}

\medskip

\begin{assumption}
\label{theo:A2}
Let ${\boldsymbol\Phi}^t_{\mathbf F}$ denote the exact flow of ${\mathbf F}$ (i.e., the mapping such that ${\mathbf A}(t) = {\boldsymbol\Phi}^t_{\mathbf F}({\mathbf A}_0)$). When considering a Runge-Kutta method of order $p$, we assume that the first $p$ derivatives
\begin{equation*}
\frac{\diff^{q}}{{\diff t}^{q}} {\boldsymbol\Phi}^t_{\mathbf F}({\mathbf X}), \quad 1 \leq q \leq p,
\end{equation*}
exist and are continuous for all ${\mathbf X} \in \R^{n \times m}$.
\end{assumption}


\subsection{Preliminary results}

Compared to \cite{ceruti2022rank}, we do not assume that ${\mathbf F}$ is bounded for all ${\mathbf X} \in \R^{n \times m}$. For this reason, we start by showing that ${\mathbf F}$ is bounded in a neighbourhood of the exact solution ${\mathbf Y}$, which can be deduced from Assumption \ref{theo:A1} and will be sufficient in the following. Additionally, we show that the exact flow ${\boldsymbol\Phi}^t_{\mathbf F}$ is Lipschitz continuous, which is a direct consequence of the Lipschitz continuity of ${\mathbf F}$.

\medskip

\begin{proposition}
\label{theo:P1}
Suppose that Assumption \ref{theo:A1} holds. The exact solution ${\mathbf Y}(t)$ of the projected differential equation \eqref{eq:projected_ode} is bounded by
\begin{equation}
\norm{{\mathbf Y}(t)-{\mathbf Y}_0}_F \leq \int_0^t e^{L(t-s)}\norm{{\mathbf F}(s,{\mathbf Y}_0)}_F \diff s
\end{equation}
for all $t \in \ff{0}{T}$.
\end{proposition}
\begin{proof}
Without loss of generality, assume that ${\mathbf Y}(t) \neq {\mathbf Y}_0$ for all $t \in \oo{0}{T}$. If ${\mathbf Y}(t) = {\mathbf Y}_0$ for certain times, consider independently the time subintervals where ${\mathbf Y}(t) \neq {\mathbf Y}_0$. Then, we deduce from Assumption \ref{theo:A1} the differential inequality
\begin{align*}
\norm{{\mathbf Y}(t)-{\mathbf Y}_0}_F \frac{\diff}{\diff t} \norm{{\mathbf Y}(t)-{\mathbf Y}_0}_F &= \frac 1 2 \frac{\diff}{\diff t} \norm{{\mathbf Y}(t)-{\mathbf Y}_0}_F^2 \\
&= \dott{{\mathbf Y}(t)-{\mathbf Y}_0}{\Po_{\mathbf Y}\big({\mathbf F}(t,{\mathbf Y}(t))\big)}_F \\
&\leq \norm{{\mathbf Y}(t)-{\mathbf Y}_0}_F \norm{\Po_{\mathbf Y}\big({\mathbf F}(t,{\mathbf Y}(t))\big)}_F \\
&\leq \norm{{\mathbf Y}(t)-{\mathbf Y}_0}_F \norm{{\mathbf F}(t,{\mathbf Y}(t))}_F \\
&\leq \norm{{\mathbf Y}(t)-{\mathbf Y}_0}_F \big(\norm{{\mathbf F}(t,{\mathbf Y}(t))-{\mathbf F}(t,{\mathbf Y}_0)}_F + \norm{{\mathbf F}(t,{\mathbf Y}_0)}_F \big) \\
&\leq \norm{{\mathbf Y}(t)-{\mathbf Y}_0}_F \big( L \norm{{\mathbf Y}(t)-{\mathbf Y}_0}_F + \norm{{\mathbf F}(t,{\mathbf Y}_0)}_F \big),
\end{align*}
which can be rewritten as
\begin{equation*}
\frac{\diff}{\diff t} \norm{{\mathbf Y}(t)-{\mathbf Y}_0}_F \leq L \norm{{\mathbf Y}(t)-{\mathbf Y}_0}_F + \norm{{\mathbf F}(t,{\mathbf Y}_0)}_F.
\end{equation*}
Finally, according to the Gr\"onwall's inequality, the exact solution ${\mathbf Y}(t)$ verifies
\begin{equation*}
\norm{{\mathbf Y}(t)-{\mathbf Y}_0}_F \leq \int_0^t e^{L(t-s)}\norm{{\mathbf F}(s,{\mathbf Y}_0)}_F \diff s,
\end{equation*}
which concludes the proof.
\end{proof}

\medskip

\begin{lemma}
\label{theo:L1}
Suppose that Assumption \ref{theo:A1} holds. Then, for $h \leq h_0$, the discrete solution ${\mathbf Y}_{k i}$ of the Runge-Kutta BUG integrator is bounded at each stage $i \in \{1,\ldots,s\}$ by
\begin{equation}
\norm{{\mathbf Y}_{k i}-{\mathbf Y}_0}_F \leq \big( 1+K_{i 0} h \big) \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F + h \sum_{j=1}^{i-1} K_{i j}\norm{{\mathbf F}(t_{k j},{\mathbf Y}_0)}_F,
\end{equation}
where the constants $K_{i j} \geq 0$ are independent of $h$.
\end{lemma}
\begin{proof}
We proceed by induction. For $i=1$, the statement is trivial with $K_{1j}=0$ since ${\mathbf Y}_{k 1} = {\mathbf Y}_{k}$. Then, for $i \in \{2,\ldots,s\}$, the induction hypothesis yields the desired result:
\begin{equation*}
\begin{aligned}
\norm{{\mathbf Y}_{k i}-{\mathbf Y}_0}_F &= \norm{\left\llbracket \widehat{\mathbf Y}_{k i} \right\rrbracket_r - {\mathbf Y}_0}_F \\
&\leq \norm{\left\llbracket \widehat{\mathbf Y}_{k i} \right\rrbracket_r - \widehat{\mathbf Y}_{k i}}_F + \norm{\widehat{\mathbf Y}_{k i} - {\mathbf Y}_0}_F \\
&= \min_{{\mathbf X} \in \M_r} \norm{{\mathbf X} - \widehat{\mathbf Y}_{k i}}_F + \norm{\widehat{\mathbf Y}_{k i} - {\mathbf Y}_0}_F \\
&\leq \norm{{\mathbf Y}_{k} - \widehat{\mathbf Y}_{k i}}_F + \norm{\widehat{\mathbf Y}_{k,i} - {\mathbf Y}_0}_F \\
&\leq\norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F + 2h \sum_{j=1}^{i-1} |a_{i j}|  \norm{\widehat{\mathbf U}_{k i}\widehat{\mathbf U}_{k i}^T{\mathbf F}(t_{k j},{\mathbf Y}_{k j})\widehat{\mathbf V}_{k i}\widehat{\mathbf V}_{k i}^T}_F \\
&\leq \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F + 2 h \sum_{j=1}^{i-1} |a_{i j}|  \norm{{\mathbf F}(t_{k j},{\mathbf Y}_{k j})}_F \\
&\leq \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F + 2h \sum_{j=1}^{i-1} |a_{i j}| \big( \norm{{\mathbf F}(t_{k j},{\mathbf Y}_{k j})-{\mathbf F}(t_{k j},{\mathbf Y}_0)}_F + \norm{{\mathbf F}(t_{k j},{\mathbf Y}_0)}_F\big) \\
&\leq \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F + 2h \sum_{j=1}^{i-1} |a_{i j}| \big( L \norm{{\mathbf Y}_{k j}-{\mathbf Y}_0}_F + \norm{{\mathbf F}(t_{k j},{\mathbf Y}_0)}_F\big) \\
&\leq \bigg( 1 + 2h \sum_{j=1}^{i-1} |a_{i j}| L (1+K_{j 0}h ) \bigg) \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F \\
&\hphantom{\leq}+ 2h \sum_{j=1}^{i-1} |a_{i j}| \Big( \norm{{\mathbf F}(t_{k j},{\mathbf Y}_0)}_F + Lh \sum_{l=1}^{j-1} K_{j l} \norm{{\mathbf F}(t_{k l},{\mathbf Y}_0)}_F\Big) \\
&\leq \big( 1 + K_{i 0} h \big) \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F + h \sum_{l=1}^{i-1} K_{i l} \norm{{\mathbf F}(t_{k l},{\mathbf Y}_0)}_F,
\end{aligned}
\end{equation*}
where
\begin{equation*}
\begin{aligned}
K_{i 0} &= 2L\sum_{j=1}^{i-1} |a_{i j}| (1+K_{j 0}h_0), \\
K_{i l} &= 2 |a_{i l}| + 2 L h_0 \sum_{j=l+1}^{i-1} |a_{i j}| K_{j l}, \quad l=1,\ldots,i-2, \\
K_{i,i-1} &= 2 |a_{i,i-1}|.
\end{aligned}
\end{equation*}
\end{proof}

\medskip

\begin{lemma}
\label{theo:L2}
Suppose that Assumption \ref{theo:A1} holds. Then, for $h \leq h_0$, the discrete solution ${\mathbf Y}_{k}$ of the Runge-Kutta BUG integrator is bounded by
\begin{equation}
\norm{{\mathbf Y}_{k+1}-{\mathbf Y}_0}_F \leq \big( 1+\widetilde K_{0} h \big) \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F + h \sum_{i=1}^{s} \widetilde K_{i}\norm{{\mathbf F}(t_{k i},{\mathbf Y}_0)}_F,
\end{equation}
where the constants $\widetilde K_{i} >0$ are independent of $h$.
\end{lemma}
\begin{proof}
We proceed in the same way as in Lemma \ref{theo:L1}. For $k+1=0$, the statement is trivial. Then, for $k+1 \geq 1$, Lemma \ref{theo:L1} and the induction hypothesis yield the desired result:
\begin{equation*}
\begin{aligned}
\norm{{\mathbf Y}_{k+1}-{\mathbf Y}_0}_F &\leq \norm{{\mathbf Y}_k - \widehat{\mathbf Y}_{k+1}}_F + \norm{\widehat{\mathbf Y}_{k+1} - {\mathbf Y}_0}_F \\
&\leq \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F + 2h \sum_{i=1}^{s} |b_{i}|  \norm{\widehat{\mathbf U}_{k s}\widehat{\mathbf U}_{k s}^T{\mathbf F}(t_{k i},{\mathbf Y}_{k i})\widehat{\mathbf V}_{k s}\widehat{\mathbf V}_{k s}^T}_F \\
&\leq \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F + 2 h \sum_{i=1}^{s} |b_{i}|  \norm{{\mathbf F}(t_{k i},{\mathbf Y}_{k i})}_F \\
&\leq \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F + 2h \sum_{i=1}^{s} |b_{i}| \big( \norm{{\mathbf F}(t_{k i},{\mathbf Y}_{k i})-{\mathbf F}(t_{k i},{\mathbf Y}_0)}_F + \norm{{\mathbf F}(t_{k i},{\mathbf Y}_0)}_F\big) \\
&\leq \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F + 2h \sum_{i=1}^{s} |b_{i}| \big( L \norm{{\mathbf Y}_{k i}-{\mathbf Y}_0}_F + \norm{{\mathbf F}(t_{k i},{\mathbf Y}_0)}_F\big) \\
&\leq \bigg( 1 + 2 h \sum_{i=1}^{s} |b_{i}| L (1 + K_{i 0} h) \bigg) \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F \\
&\hphantom{\leq}+ 2h \sum_{i=1}^{s} |b_{i}| \Big( \norm{{\mathbf F}(t_{k i},{\mathbf Y}_0)}_F + Lh \sum_{j=1}^{i-1} K_{i j} \norm{{\mathbf F}(t_{k j},{\mathbf Y}_0)}_F\Big) \\
&\leq \big( 1 + \widetilde K_{0} h \big) \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F + h \sum_{l=1}^{s} \widetilde K_{l} \norm{{\mathbf F}(t_{k l},{\mathbf Y}_0)}_F,
\end{aligned}
\end{equation*}
where 
\begin{equation*}
\begin{aligned}
\widetilde K_{0} &= 2L\sum_{i=1}^{s} |b_{i}| (1+K_{i 0}h_0), \\
\widetilde K_{l} &= 2 |b_{l}| + 2 L h_0 \sum_{i=l+1}^{s} |b_{i}| K_{i l}, \quad l=1,\ldots,s-1, \\
\widetilde K_{s} &= 2 |b_{s}|.
\end{aligned}
\end{equation*}
\end{proof}



\begin{proposition}
\label{theo:P2}
Suppose that Assumption \ref{theo:A1} holds. Then, for $h \leq h_0$, the discrete solution of the Runge-Kutta BUG integrator is bounded by
\begin{equation}
\norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F \leq g^\ast_{k-1} \frac{ e^{kh \widetilde K_{0}}-1}{\widetilde K_{0}},
\end{equation}
where $g^\ast_k = \max\limits_{0 \leq l \leq k} g_l$ and $g_l = \sum_{i=1}^{s} \widetilde K_{i} \norm{{\mathbf F}(t_{l i},{\mathbf Y}_0)}_F$.
\end{proposition}
\begin{proof}
We start by showing by induction that
\begin{equation}
\label{eq:P2_1}
\norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F \leq h \sum_{l=0}^{k-1} (1+h \widetilde K_{0})^{k-1-l} g_l.
\end{equation}
For $k=0$, the statement is trivial. Then, for $k \geq 1$, Lemma \ref{theo:L2} and the induction hypothesis lead to the desired result
\begin{equation*}
\begin{aligned}
\norm{{\mathbf Y}_{k+1}-{\mathbf Y}_0}_F &\leq (1+h \widetilde K_{0}) \norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F + h  g_k \\
&\leq (1+h \widetilde K_{0}) \Big( h  \sum_{l=0}^{k-1} (1+h \widetilde K_{0})^{k-1-l} g_l \Big) + h  g_k \\
&= h  \sum_{l=0}^{k} (1+h \widetilde K_{0})^{k-l} g_l.
\end{aligned}
\end{equation*}
Finally, we deduce from equation \eqref{eq:P2_1} that
\begin{equation*}
\begin{aligned}
\norm{{\mathbf Y}_{k}-{\mathbf Y}_0}_F &\leq h g^\ast_{k-1} \sum_{l=0}^{k-1} (1+h \widetilde K_{0})^{k-1-l} \\
&= g^\ast_{k-1} \frac{(1+h \widetilde K_{0})^{k}-1}{\widetilde K_{0}} \\
&\leq g^\ast_{k-1} \frac{ e^{kh \widetilde K_{0}}-1}{\widetilde K_{0}}, \\
\end{aligned}
\end{equation*}
which concludes the proof.
\end{proof}

\medskip

According to Propositions \ref{theo:P1} and \ref{theo:P2}, the exact solution ${\mathbf Y}(t)$ and the discrete solution of the Runge-Kutta BUG integrator are bounded on the finite time-interval $0 \leq t \leq T$. Let
\begin{equation}
\V_r := \big\{{\mathbf X} \in \M_r \mid \norm{{\mathbf X}-{\mathbf Y}_0}_F \leq \max\{R_1,R_2\} \big\},
\end{equation}
where
\begin{equation*}
\begin{aligned}
R_1 &= \int_0^T e^{L(T-t)}\norm{{\mathbf F}(t,{\mathbf Y}_0)}_F \diff t, \\
R_2 &= \max_{1 \leq i \leq s} \bigg\{ \big( 1+K_{i0} h_0 \big) \bigg( \sum_{j=1}^{s} \widetilde K_{j} \bigg) \frac{ e^{T \widetilde K_{0}}-1}{\widetilde K_{0}} + h_0 \sum_{j=1}^{i-1} K_{ij} \bigg\} \times \sup_{t \in \ff{0}{T}} \norm{{\mathbf F}(t,{\mathbf Y}_0)}_F.
\end{aligned}
\end{equation*}
The subset $\V_r \subseteq \M_r$ is a neighbourhood of the exact solution ${\mathbf Y}(t)$ which contains the solution of the Runge-Kutta BUG integrator for all $t \in \ff{0}{T}$ and $h \leq h_0$. As a consequence, since ${\mathbf F}(t,{\mathbf X})$ is continuous in time and Lipschitz continuous in ${\mathbf X}$, ${\mathbf F}$ is bounded on the compact set $\ff{0}{T} \times \V_r$, and there exists a constant $B \geq 0$ such that
\begin{equation}
\label{eq:bound_solution}
\norm{{\mathbf F}(t,{\mathbf X})}_F \leq B
\end{equation}
for all $t \in \ff{0}{T}$ and ${\mathbf X} \in \V_r$.


\medskip

\begin{lemma}
\label{theo:L4}
Under Assumption \ref{theo:A1}, the exact flow is $e^{Lt}$-Lipschitz continuous:
\begin{equation}
\norm{{\boldsymbol\Phi}^{t}_{\mathbf F}({\mathbf X}_1)-{\boldsymbol\Phi}^{t}_{\mathbf F}({\mathbf X}_2)}_F \leq e^{Lt} \norm{{\mathbf X}_1-{\mathbf X}_2}_F
\end{equation}
for all $t \in \ff{0}{T}$ and ${\mathbf X}_1,{\mathbf X}_2 \in \R^{n \times m}$.
\end{lemma}
\begin{proof}
From Assumption \ref{theo:A1}, we obtain the differential inequality
\begin{align*}
\frac{\diff}{\diff t} \norm{{\boldsymbol\Phi}^{t}_{\mathbf F}({\mathbf X}_1)-{\boldsymbol\Phi}^{t}_{\mathbf F}({\mathbf X}_2)}_F^2 &= 2 \dott{{\boldsymbol\Phi}^{t}_{\mathbf F}({\mathbf X}_1)-{\boldsymbol\Phi}^{t}_{\mathbf F}({\mathbf X}_2)}{{\mathbf F}(t,{\boldsymbol\Phi}^{t}_{\mathbf F}({\mathbf X}_1))-{\mathbf F}(t,{\boldsymbol\Phi}^{t}_{\mathbf F}({\mathbf X}_2))}_F \\
&\leq 2 L \norm{{\boldsymbol\Phi}^{t}_{\mathbf F}({\mathbf X}_1)-{\boldsymbol\Phi}^{t}_{\mathbf F}({\mathbf X}_2)}_F^2
\end{align*}
and, according to the Gr\"onwall's inequality, the exact flow verifies
\begin{equation*}
\norm{{\boldsymbol\Phi}^{t}_{\mathbf F}({\mathbf X}_1)-{\boldsymbol\Phi}^{t}_{\mathbf F}({\mathbf X}_2)}^2_F \leq e^{2Lt} \norm{{\mathbf X}_1-{\mathbf X}_2}^2_F,
\end{equation*}
which concludes the proof.
\end{proof}



\subsection{Error estimation}

We now introduce several definitions and propositions that will be useful to prove the high-order convergence of the Runge-Kutta BUG integrator. In particular, Proposition \ref{theo:P3} shows that the projection error resulting from the Galerkin projection of the Runge-Kutta BUG integrator is not larger than the one resulting from the tangent-space projection, which will allow us to adapt the convergence analysis of projected Runge-Kutta methods \cite{kieri2019projection} to our Runge-Kutta BUG integrator. In addition, equation \eqref{eq:bound_truncation} provides an estimate for the truncation error that will allow us to obtain error bounds with an improved order of convergence compared to \cite{kieri2019projection}.

\medskip

\begin{proposition}
\label{theo:P3}
The Galerkin projection of the Runge-Kutta BUG integrator is more accurate than the tangent-space projection for approximating ${\mathbf F}_{k,i}$:
\begin{equation}
\label{eq:p3_1}
\norm{{\mathbf F}_{k i}-\widehat{\mathbf U}_{k+1}\widehat{\mathbf U}_{k+1}^T{\mathbf F}_{k i}\widehat{\mathbf V}_{k+1}\widehat{\mathbf V}_{k+1}^T}_F \leq \norm{{\mathbf F}_{k i}-\Po_{{\mathbf Y}_{k i}}\big({\mathbf F}_{k i}\big)}_F
\end{equation}
for all $i \in \{1,\ldots,s\}$ such that $b_i \neq 0$. Similarly, the projection error at each stage $i \in \{1,\ldots,s-1\}$ verifies
\begin{equation}
\label{eq:p3_2}
\norm{{\mathbf F}_{k j}-\widehat{\mathbf U}_{k,i+1}\widehat{\mathbf U}_{k,i+1}^T{\mathbf F}_{k j}\widehat{\mathbf V}_{k,i+1}\widehat{\mathbf V}_{k,i+1}^T}_F \leq \norm{{\mathbf F}_{k j}-\Po_{{\mathbf Y}_{k j}}\big({\mathbf F}_{k j}\big)}_F
\end{equation}
for all $j \in \{1,\ldots,i\}$ such that $a_{i+1,j} \neq 0$.
\end{proposition}
\begin{proof}
We prove equation \eqref{eq:p3_1}. The proof of equation \eqref{eq:p3_2} follows from the same arguments and is therefore omitted. Let the SVD of $\Po_{{\mathbf Y}_{k i}}\big({\mathbf F}_{k i}\big)$ be ${\mathbf \Phi}{\boldsymbol \Sigma}{\mathbf \Psi}^T$, where ${\mathbf \Phi} \in \R^{n \times \overline r}$ and ${\mathbf \Psi} \in \R^{m \times \overline r}$ are orthonormal matrices, ${\boldsymbol \Sigma} \in \R^{\overline r \times \overline r}$ is a diagonal matrix with non-negative real numbers on the diagonal, and $\overline r \in \{r,\ldots,\min\{n,m,2r\}\}$. If $b_i \neq 0$, then the augmented bases $\widehat{\mathbf U}_{k+1}$ and $\widehat{\mathbf V}_{k+1}$ contain by construction the range of $[{\mathbf U}_{k i} \enspace\; {\mathbf F}_{k i}{\mathbf V}_{k i}]$ and $[{\mathbf V}_{k i} \enspace\; {\mathbf F}_{k i}^T{\mathbf U}_{k i}]$, respectively, and it follows that the left and right singular vectors of $\Po_{{\mathbf Y}_{k i}}\big({\mathbf F}_{k i}\big)$ are included in the span of the augmented bases:
\begin{equation*}
{\mathbf \Phi} \subseteq \textrm{span}(\widehat{\mathbf U}_{k+1}) \quad \textrm{and} \quad {\mathbf \Psi} \subseteq \textrm{span}(\widehat{\mathbf V}_{k+1}),
\end{equation*}
where $\widehat{\mathbf U}_{k+1} \in \R^{n \times \widehat r}$, $\widehat{\mathbf V}_{k+1} \in \R^{m \times \widehat r}$, and $\widehat r \in \{\overline r,\ldots,\min\{n,m,2rs\}\}$. Consequently, the Galerkin projection onto the augmented bases verifies
\begin{equation*}
\begin{aligned}
\norm{{\mathbf F}_{k i}-\Po_{{\mathbf Y}_{k i}}\big({\mathbf F}_{k i}\big)}_F = & \norm{{\mathbf F}_{k i}-{\mathbf \Phi}{\boldsymbol \Sigma}{\mathbf \Psi}^T}_F \\
\geq & \min\limits_{\overline{\boldsymbol \Sigma} \in \R^{\overline r \times \overline r}} \norm{{\mathbf F}_{k i}-{\mathbf \Phi}\overline{\boldsymbol \Sigma}{\mathbf \Psi}^T}_F \\
\geq & \min\limits_{\widehat{\boldsymbol \Sigma} \in \R^{\widehat r \times \widehat r}} \norm{{\mathbf F}_{k i}-\widehat{\mathbf U}_{k+1}\widehat{\boldsymbol \Sigma}\widehat{\mathbf V}_{k+1}^T}_F \\
= & \norm{{\mathbf F}_{k i}-\widehat{\mathbf U}_{k+1}\widehat{\mathbf U}_{k+1}^T{\mathbf F}_{k i}\widehat{\mathbf V}_{k+1}\widehat{\mathbf V}_{k+1}^T}_F,
\end{aligned}
\end{equation*}
which concludes the proof.
\end{proof}

\medskip

Since ${\mathbf F}$ is bounded on $\ff{0}{T} \times \V_r$, the orthogonal projection of ${\mathbf F}$ onto the tangent space is also bounded on $\ff{0}{T} \times \V_r$, and there exits $\varepsilon_r \geq 0$ such that the tangent-space projection error is bounded on $\ff{0}{T} \times \V_r$. Let 
\begin{equation}
\label{eq:bound_projection}
\varepsilon_r := \sup_{t \in \ff{0}{T}} \sup_{{\mathbf X} \in \V_r} \norm{{\mathbf F}(t,{\mathbf X})-\Po_{\mathbf X}\big({\mathbf F}(t,{\mathbf X})\big)}_F.
\end{equation}
It follows from Proposition \ref{theo:P3} that the projection error of the Runge-Kutta BUG integrator is bounded by $\varepsilon_r$:
\begin{equation*}
\norm{{\mathbf F}_{k i}-\widehat{\mathbf U}_{k+1}\widehat{\mathbf U}_{k+1}^T{\mathbf F}_{k i}\widehat{\mathbf V}_{k+1}\widehat{\mathbf V}_{k+1}^T}_F \leq \varepsilon_r
\end{equation*}
for all $i \in \{1,\ldots,s\}$ such that $b_i \neq 0$, and
\begin{equation*}
\norm{{\mathbf F}_{k j}-\widehat{\mathbf U}_{k,i+1}\widehat{\mathbf U}_{k,i+1}^T{\mathbf F}_{k j}\widehat{\mathbf V}_{k,i+1}\widehat{\mathbf V}_{k,i+1}^T}_F \leq \varepsilon_r
\end{equation*}
for all $j \in \{1,\ldots,i\}$ such that $a_{i+1,j} \neq 0$. In addition, note that the tangent-space projection error and therefore $\varepsilon_r$ vanish when $r = \min\{n,m\}$.

\medskip

\begin{proposition}
\label{theo:P4}
Suppose that Assumption \ref{theo:A1} holds. Then, for $\ff{t_k}{t_{k+1}} \subseteq \ff{0}{T}$ and $h \leq h_0$, the error resulting from the truncation step of the Runge-Kutta BUG integrator is proportional to $h$: the ratio
\begin{equation}
\label{eq:p4_1}
\frac{1}{h} \norm{\widehat{\mathbf Y}_{k+1} - \left\llbracket \widehat{\mathbf Y}_{k+1} \right\rrbracket_r}_F
\end{equation}
is bounded independently of $h$. Similarly, at each stage $i \in \{1,\ldots,s-1\}$, the truncation error is proportional to $h$: the ratio
\begin{equation}
\label{eq:p4_2}
\frac{1}{h}\norm{\widehat{\mathbf Y}_{k,i+1} - \left\llbracket \widehat{\mathbf Y}_{k,i+1} \right\rrbracket_r}_F
\end{equation}
is bounded independently of $h$.
\end{proposition}
\begin{proof}
We prove equation \eqref{eq:p4_1}. The proof of equation \eqref{eq:p4_2} follows from the same arguments and is therefore omitted. Let $\sigma_j({\mathbf X})$ denote the $j$-th largest singular value of the matrix ${\mathbf X}\in \R^{n \times m}$. The squared truncation error is given according to the Eckart-Young theorem \cite{eckart1936approximation} by
\begin{equation}
\label{eq:l4_3}
\norm{\widehat{\mathbf Y}_{k+1} - \left\llbracket \widehat{\mathbf Y}_{k+1} \right\rrbracket_r}_F^2 = \sum\limits_{j=r+1}^{\min\{n,m,2rs\}} \sigma_j^2(\widehat{\mathbf Y}_{k+1}),
\end{equation}
since the rank of $\widehat{\mathbf Y}_{k+1}$ is at most $2rs$ (see Remark \ref{theo:R1}) and, consequently, $\sigma_j^2(\widehat{\mathbf Y}_{k+1})=0$ for $j > 2rs$. Then, as
\begin{equation*}
\widehat{\mathbf Y}_{k+1} = {\mathbf Y}_{k} + h \left( b_{1} \widetilde{\mathbf F}_{k 1} + \ldots + b_{s} \widetilde{\mathbf F}_{k s} \right) \quad \textrm{with}\quad \widetilde{\mathbf F}_{k i} = \widehat{\mathbf U}_{k+1}\widehat{\mathbf U}_{k+1}^T{\mathbf F}_{k i}\widehat{\mathbf V}_{k+1}\widehat{\mathbf V}_{k+1}^T,
\end{equation*}
the singular values of $\widehat{\mathbf Y}_{k+1}$ are bounded according to Theorem 3.3.16 in \cite{horn1991topics} by
\begin{equation*}
\sigma_{i+j-1}(\widehat{\mathbf Y}_{k+1}) \leq \sigma_i({\mathbf Y}_{k}) + h \sigma_j(b_{1} \widetilde{\mathbf F}_{k 1} + \ldots + b_{s} \widetilde{\mathbf F}_{k s})
\end{equation*}
for all $i \in \{1,\ldots,\min\{n,m,2r\}\}$ and $j \in \{1,\ldots,\min\{n,m,2rs\}\}$ such that $i+j-1\leq \min\{n,m,2rs\}$. In particular, for $i = r+1$ and $j\geq 1$, it follows that
\begin{equation}
\label{eq:l4_4}
\sigma_{r+j}(\widehat{\mathbf Y}_{k+1}) \leq h \sigma_j(b_{1} \widetilde{\mathbf F}_{k 1} + \ldots + b_{s} \widetilde{\mathbf F}_{k s}),
\end{equation}
since the rank of ${\mathbf Y}_{k}$ is at most $r$. Moreover, we deduce from equation \eqref{eq:bound_solution} that
\begin{equation*}
\begin{aligned}
\sum_{j=1}^{\min\{n,m,2rs\}} \sigma_j^2(b_{1} \widetilde{\mathbf F}_{k 1} + \ldots + b_{s} \widetilde{\mathbf F}_{k s}) &= \norm{b_{1} \widetilde{\mathbf F}_{k 1} + \ldots + b_{s} \widetilde{\mathbf F}_{k s}}_F^2 \\
 &\leq \Bigl( |b_{1}| \norm{\widetilde{\mathbf F}_{k 1}}_F + \ldots + |b_{s}| \norm{\widetilde{\mathbf F}_{k s}}_F \Bigr)^2 \\
&\leq  \Bigl( |b_{1}| \norm{{\mathbf F}_{k 1}}_F + \ldots + |b_{s}| \norm{{\mathbf F}_{k s}}_F \Bigr)^2 \\
&\leq  \Bigl( |b_{1}| B + \ldots + |b_{s}| B \Bigr)^2 \\
&= C_B^2 B^2
\end{aligned}
\end{equation*}
and, consequently,
\begin{equation}
\label{eq:l4_5}
\sigma_{j}^2(b_{1} \widetilde{\mathbf F}_{k 1} + \ldots + b_{s} \widetilde{\mathbf F}_{k s}) \leq C_B^2 B^2, \quad j \geq 1,
\end{equation}
where $C_B = \sum_{i=1}^s |b_{i}|$. Finally, combining equations \eqref{eq:l4_3}, \eqref{eq:l4_4}, and \eqref{eq:l4_5} yields
\begin{equation*}
\begin{aligned}
\norm{\widehat{\mathbf Y}_{k+1} - \left\llbracket \widehat{\mathbf Y}_{k+1} \right\rrbracket_r}_F^2 &= \sum\limits_{j=1}^{\min\{n,m,2rs\}-r} \sigma_{r+j}^2(\widehat{\mathbf Y}_{k+1}) \\
&\leq \sum\limits_{j=1}^{\min\{n,m,2rs\}-r} h^2 \sigma^2_j(b_{1} \widetilde{\mathbf F}_{k 1} + \ldots + b_{s} \widetilde{\mathbf F}_{k s}) \\
&\leq \sum\limits_{j=1}^{\min\{n,m,2rs\}-r} h^2 C_B^2 B^2 \\
&= h^2 (\min\{n,m,2rs\}-r) C_B^2 B^2.
\end{aligned}
\end{equation*}
The desired result follows directly from the fact that the term $(\min\{n,m,2rs\}-r) C_B^2 B^2$ is finite and does not depend on $h$.
\end{proof}

\medskip

According to Proposition \ref{theo:P4}, there exists $\gamma_r \geq0$ such that the ratios \eqref{eq:p4_1} and \eqref{eq:p4_2} are bounded. Let 
\begin{equation}
\label{eq:bound_truncation}
\begin{aligned}
\gamma_r := \max\Big\{ \sup_{t_{k,i+1} \in \ff{0}{T}} \sup_{{\mathbf X}_k \in \V_r} \frac{1}{h} \norm{\widehat{\mathbf X}_{k,i+1} - \left\llbracket \widehat{\mathbf X}_{k,i+1} \right\rrbracket_r}_F&, \\
\sup_{t_{k+1} \in \ff{0}{T}} \sup_{{\mathbf X}_k \in \V_r} \frac{1}{h} \norm{\widehat{\mathbf X}_{k+1} - \left\llbracket \widehat{\mathbf X}_{k+1} \right\rrbracket_r}_F& \Big\},
\end{aligned}
\end{equation}
where $\widehat{\mathbf X}_{k+1}$ and $\widehat{\mathbf X}_{k,i+1}$ denote the augmented solutions computed by the Runge-Kutta BUG integrator using the initial value ${\mathbf X}_{k}$ at time $t_k$. It follows that the truncation error is bounded by
\begin{equation*}
\begin{aligned}
\norm{\widehat{\mathbf Y}_{k,i+1} - \left\llbracket \widehat{\mathbf Y}_{k,i+1} \right\rrbracket_r}_F &\leq h \gamma_r, \quad i = 1,\ldots,s-1, \\
\norm{\widehat{\mathbf Y}_{k+1} - \left\llbracket \widehat{\mathbf Y}_{k+1} \right\rrbracket_r}_F &\leq h \gamma_r.
\end{aligned}
\end{equation*}
In addition, note that the truncation error and therefore $\gamma_r$ vanish when $r = \min\{n,m\}$.

\medskip

\begin{remark}
In general, $\gamma_r$ is not related to $\varepsilon_r$, but in some cases the truncation error can be bounded by using $\varepsilon_r$. For instance, according to the proof of Proposition \ref{theo:P4}, if
\begin{equation}
\label{eq:r3_1}
\sigma_1(\widetilde{\mathbf F}_{k i}) \leq \varepsilon_r, \quad \forall t_{k,i} \in \ff{0}{T},
\end{equation}
then it holds
\begin{equation*}
\begin{aligned}
\sigma_1(a_{i+1,1} \widetilde{\mathbf F}_{k 1} + \ldots + a_{i,i+1} \widetilde{\mathbf F}_{k i}) &\leq C_A \varepsilon_r, \quad \forall t_{k i} \in \ff{0}{T}, \\
\sigma_1(b_{1} \widetilde{\mathbf F}_{k 1} + \ldots + b_{s} \widetilde{\mathbf F}_{k s}) &\leq C_B \varepsilon_r, \quad \forall t_{k+1} \in \ff{0}{T},
\end{aligned}
\end{equation*}
and one can take $\gamma_r = \max\{C_A,C_B\} \varepsilon_r$ with $C_A = \sum_{i,j=1}^s |a_{i j}|$ and $C_B = \sum_{i=1}^s |b_{i}|$. In particular, condition \eqref{eq:r3_1} for a specific time $t_{k i}$ is satisfied if the left and right singular vectors associated with the largest singular value of ${\mathbf F}_{k i}$ are not included in the left and right singular vectors of $\Po_{{\mathbf Y}_{k i}}\big({\mathbf F}_{k i}\big)$, which is however unlikely, especially if $\Po_{{\mathbf Y}_{k i}}\big({\mathbf F}_{k i}\big)$ is a good approximation of ${\mathbf F}_{k i}$.
\end{remark}

\medskip

\begin{proposition}
\label{theo:P5}
Let the Runge-Kutta method \eqref{eq:rk} be a numerical scheme of order $p$, and suppose that Assumption \ref{theo:A2} holds. Then, for $h\leq h_0$, the local error of the Runge-Kutta method verifies
\begin{equation}
\norm{{\mathbf A}_{k+1}-{\boldsymbol\Phi}^h_{\mathbf F}({\mathbf A}_k)}_F \leq C_L h^{p+1},
\end{equation}
where the constant $C_L>0$ is independent of $h$.
\end{proposition}
\begin{proof}
see Theorem 3.1 of Chapter 2 in \cite{harrier1993solving}.
\end{proof}


\subsection{Local and global error bounds}

We establish the local and global error bounds in Theorems \ref{theo:T1} and \ref{theo:T2}, respectively. These error bounds show that the proposed Runge-Kutta BUG integrator retains the order of convergence of the associated Runge-Kutta method until the error reaches a plateau corresponding to the low-rank truncation error and which vanishes as the rank becomes full. Compared to \cite{kieri2019projection}, we obtain local and global error bounds with an improved order of convergence in $h$ thanks to the introduction of the additional error term $\gamma_r$, but similar error bounds can be obtained for projected Runge-Kutta methods by introducing such error term and adapting the convergence analysis accordingly.

\medskip

\begin{lemma}
\label{theo:L3}
Suppose that Assumption \ref{theo:A1} holds, and consider the time subinterval $\ff{t_k}{t_{k+1}} \subseteq \ff{0}{T}$. Moreover, let ${\mathbf Z}_{k i}$ denote the discrete solution computed using the (unprojected) Runge-Kutta method \eqref{eq:rk} with the initial value ${\mathbf Z}_{k} = {\mathbf Y}_{k} \in \M_r$. Then, for $h \leq h_0$, it holds 
\begin{equation}
\norm{{\mathbf Y}_{k i}-{\mathbf Z}_{k i}}_F \leq C_{i} h (\varepsilon_r + \gamma_r), \quad i= 1,\ldots,s,
\end{equation}
where the constant $C_{i} \geq 0$ is independent of $h$ and $r$.
\end{lemma}
\begin{proof}
We proceed by induction. For $i=1$, the statement is trivial with $C_1=0$ since ${\mathbf Z}_{k 1} = {\mathbf Z}_{k} = {\mathbf Y}_{k} = {\mathbf Y}_{k 1}$. Then, for $i \in \{2,\ldots,s\}$, the local error can be split into
\begin{equation}
\label{eq:l3_1}
\norm{{\mathbf Y}_{k i}-{\mathbf Z}_{k i}}_F \leq \norm{{\mathbf Y}_{k i}-\widehat{\mathbf Y}_{k i}}_F + \norm{\widehat{\mathbf Y}_{k i}- {\mathbf Z}_{k i}}_F.
\end{equation}
The first term in the RHS is bounded by
\begin{equation}
\label{eq:l3_2}
\norm{\widehat{\mathbf Y}_{k i}-{\mathbf Y}_{k i}}_F \leq h \gamma_r.
\end{equation}
Regarding the second term, we deduce from the induction hypothesis that
\begin{equation}
\label{eq:l3_3}
\begin{aligned}
\norm{\widehat{\mathbf Y}_{k i}-{\mathbf Z}_{k i}}_F &\leq h \sum_{j=1}^{i-1} |a_{i j}|  \norm{\widehat{\mathbf U}_{k i}\widehat{\mathbf U}_{k i}^T{\mathbf F}(t_{k j},{\mathbf Y}_{k j})\widehat{\mathbf V}_{k i}\widehat{\mathbf V}_{k i}^T-{\mathbf F}(t_{k j},{\mathbf Z}_{k j})}_F \\
&= h \sum_{j \in \I_A} |a_{i j}|  \norm{\widehat{\mathbf U}_{k i}\widehat{\mathbf U}_{k i}^T{\mathbf F}(t_{k j},{\mathbf Y}_{k j})\widehat{\mathbf V}_{k i}\widehat{\mathbf V}_{k i}^T-{\mathbf F}(t_{k j},{\mathbf Z}_{k j})}_F \\
&\leq h \sum_{j \in \I_A} |a_{i j}| \Bigl( \norm{\widehat{\mathbf U}_{k i}\widehat{\mathbf U}_{k i}^T{\mathbf F}(t_{k j},{\mathbf Y}_{k j})\widehat{\mathbf V}_{k i}\widehat{\mathbf V}_{k i}^T-{\mathbf F}(t_{k j},{\mathbf Y}_{k j})}_F \\
&\hphantom{\leq h \sum_{j \in \I_A} |a_{i j}| \Bigl(} + \norm{{\mathbf F}(t_{k j},{\mathbf Y}_{k j})-{\mathbf F}(t_{k j},{\mathbf Z}_{k j})}_F \Bigr) \\
&\leq h \sum_{j \in \I_A} |a_{i j}| \left( \varepsilon_r + L \norm{{\mathbf Y}_{k j}-{\mathbf Z}_{k j}}_F \right) \\
&\leq \Big( \sum_{j \in \I_A}  |a_{i j}| (1+L C_{j} h) \Big) h \varepsilon_r + \Big( \sum_{j \in \I_A}  |a_{i j}| L C_{j} \Big) h^2 \gamma_r,
\end{aligned}
\end{equation}
where $\I_A = \bigl\{j \in \{1,\ldots,i-1\} \mid a_{i j}\neq 0 \bigr\}$. Finally, combining equations \eqref{eq:l3_1}, \eqref{eq:l3_2}, and \eqref{eq:l3_3} yields the desired result:
\begin{equation*}
\begin{aligned}
\norm{{\mathbf Y}_{k i}-{\mathbf Z}_{k i}}_F &\leq \Big( \sum_{j \in \I_A}  |a_{i j}| (1+L C_{j} h) \Big) h \varepsilon_r + \Big( 1+ \sum_{j \in \I_A}  |a_{i j}| L C_{j}h \Big) h \gamma_r \\
&\leq C_i h (\varepsilon_r + \gamma_r),
\end{aligned}
\end{equation*}
where $C_1 = 0$ and $C_i = \max\Big\{ \sum_{j=1}^{i-1}  |a_{i j}| (1+L C_{j} h_0) ,1+ \sum_{j=1}^{i-1}  |a_{i j}| L C_{j}h_0 \Big\}$ for $i=2,\ldots,s$.
\end{proof}

\medskip

\begin{theorem}
\label{theo:T1}
Suppose that Assumptions \ref{theo:A1} and \ref{theo:A2} hold, and consider the time subinterval $\ff{t_k}{t_{k+1}} \subseteq \ff{0}{T}$. Then, for $h \leq h_0$, the local error of the Runge-Kutta BUG integrator verifies
\begin{equation}
\norm{{\mathbf Y}_{k+1}-{\boldsymbol\Phi}^{h}_{\mathbf F}({\mathbf Y}_k)}_F \leq C h \left( \varepsilon_r + \gamma_r + h^{p} \right),
\end{equation}
where the constant $C>0$ is independent of $h$ and $r$.
\end{theorem}
\begin{proof}
Let ${\mathbf Z}_{k+1}$ denote the discrete solution computed using the Runge-Kutta method \eqref{eq:rk} with the initial value ${\mathbf Z}_{k} = {\mathbf Y}_{k} \in \M_r$. The local error can be split into
\begin{equation}
\label{eq:t1_1}
\norm{{\mathbf Y}_{k+1}-{\boldsymbol\Phi}^{h}_{\mathbf F}({\mathbf Y}_k)}_F  \leq \norm{{\mathbf Y}_{k+1}- \widehat{\mathbf Y}_{k+1}}_F + \norm{\widehat{\mathbf Y}_{k+1}- {\mathbf Z}_{k+1}}_F + \norm{{\mathbf Z}_{k+1}-{\boldsymbol\Phi}^{h}_{\mathbf F}({\mathbf Y}_k)}_F.
\end{equation}
The first term in the RHS is bounded by
\begin{equation}
\label{eq:t1_2}
\norm{\widehat{\mathbf Y}_{k+1}-{\mathbf Y}_{k+1}}_F \leq h \gamma_r.
\end{equation}
Regarding the second term, we deduce from Lemma \ref{theo:L3} that
\begin{equation}
\label{eq:t1_3}
\begin{aligned}
\norm{\widehat{\mathbf Y}_{k+1}-{\mathbf Z}_{k+1}}_F &\leq h \sum_{i=1}^{s} |b_{i}|  \norm{\widehat{\mathbf U}_{k+1}\widehat{\mathbf U}_{k+1}^T{\mathbf F}(t_{k i},{\mathbf Y}_{k i})\widehat{\mathbf V}_{k+1}\widehat{\mathbf V}_{k+1}^T-{\mathbf F}(t_{k i},{\mathbf Z}_{k i})}_F \\
&= h \sum_{i \in \I_B} |b_{i}| \norm{\widehat{\mathbf U}_{k+1}\widehat{\mathbf U}_{k+1}^T{\mathbf F}(t_{k i},{\mathbf Y}_{k i})\widehat{\mathbf V}_{k+1}\widehat{\mathbf V}_{k+1}^T-{\mathbf F}(t_{k i},{\mathbf Z}_{k i})}_F \\
&\leq h \sum_{i \in \I_B} |b_{i}| \Bigl( \norm{\widehat{\mathbf U}_{k+1}\widehat{\mathbf U}_{k+1}^T{\mathbf F}(t_{k i},{\mathbf Y}_{k i})\widehat{\mathbf V}_{k+1}\widehat{\mathbf V}_{k+1}^T-{\mathbf F}(t_{k i},{\mathbf Y}_{k i})}_F \\
&\hphantom{\leq h \sum_{i \in \I_B} |b_{i}| \Bigl(} + \norm{{\mathbf F}(t_{k i},{\mathbf Y}_{k i})-{\mathbf F}(t_{k i},{\mathbf Z}_{k i})}_F \Bigr) \\
&\leq h \sum_{i \in \I_B} |b_{i}| \left(\varepsilon_r + L \norm{{\mathbf Y}_{k i}-{\mathbf Z}_{k i}}_F\right) \\
&\leq \Big( \sum_{i \in \I_B}  |b_{i}| (1+L C_{i} h) \Big) h \varepsilon_r + \Big( \sum_{i \in \I_B}  |b_{i}| L C_{i} \Big) h^2 \gamma_r
\end{aligned}
\end{equation}
where $\I_B = \bigl\{i \in \{1,\ldots,s\} \mid b_{i}\neq 0 \bigr\}$. The last term is bounded according to Proposition \ref{theo:P5} by
\begin{equation}
\label{eq:t1_4}
\norm{{\mathbf Z}_{k+1}-{\boldsymbol\Phi}^{h}_{\mathbf F}({\mathbf Y}_k)}_F = \norm{{\mathbf Z}_{k+1}-{\boldsymbol\Phi}^{h}_{\mathbf F}({\mathbf Z}_k)}_F \leq C_L h^{p+1}.
\end{equation}
Finally, combining equations \eqref{eq:t1_1}, \eqref{eq:t1_2}, \eqref{eq:t1_3}, and \eqref{eq:t1_4} yields the desired result:
\begin{equation*}
\begin{aligned}
\norm{{\mathbf Y}_{k+1}-{\boldsymbol\Phi}^{h}_{\mathbf F}({\mathbf Y}_k)}_F &\leq \Big( \sum_{i \in \I_B}  |b_{i}| (1+L C_{i} h) \Big) h \varepsilon_r + \Big( 1+ \sum_{i \in \I_B}  |b_{i}| L C_{i}h \Big) h \gamma_r + C_L h^{p+1} \\
&\leq C h (\varepsilon_r + \gamma_r+h^p),
\end{aligned}
\end{equation*}
where $C = \max\Big\{ \sum_{i=1}^s  |b_{i}| (1+L C_{i} h_0) ,1+ \sum_{i=1}^s  |b_{i}| L C_{i}h_0, C_L \Big\}$.
\end{proof}

\medskip

\begin{theorem}
\label{theo:T2}
Let the initial error be $\delta_r=\norm{{\mathbf Y}_0-{\mathbf A}_0}_F$ with $\delta_r$ vanishing when $r = \min\{n,m\}$. Moreover, suppose that Assumptions \ref{theo:A1} and \ref{theo:A2} hold. Then, for $h \leq h_0$, the global error of the Runge-Kutta BUG integrator verifies
\begin{equation}
\label{eq:global_error}
\norm{{\mathbf Y}_N-{\mathbf A}(t_N)}_F \leq C' \left( \delta_r + \varepsilon_r + \gamma_r + h^{p} \right),
\end{equation}
on the finite time-interval $0 \leq t_N \leq T$. The constant $C'>0$ depends on $T$, $L$, $C_L$, $h_0$, $C_A = \sum_{i,j=1}^s |a_{i j}|$, and $C_B = \sum_{i=1}^s |b_{i}|$ but not on $h$ nor $r$.
\end{theorem}
\begin{proof}
The bound results from the local error of Theorem \ref{theo:T1} and the standard argument of Lady Windermere’s fan with error propagation along the exact flow. More specifically, the global error can be expanded as the telescoping sum
\begin{equation*}
{\mathbf Y}_{N}-{\boldsymbol\Phi}^{Nh}_{\mathbf F}({\mathbf A}_0) = \left( \sum_{k=1}^N {\boldsymbol\Phi}^{(N-k)h}_{\mathbf F}({\mathbf Y}_{k}) - {\boldsymbol\Phi}^{(N-k+1)h}_{\mathbf F}({\mathbf Y}_{k-1}) \right) + {\boldsymbol\Phi}^{Nh}_{\mathbf F}({\mathbf Y}_{0}) - {\boldsymbol\Phi}^{Nh}_{\mathbf F}({\mathbf A}_{0}).
\end{equation*}
Then, the different terms can be bounded according to Lemma \ref{theo:L4} and Theorem \ref{theo:T1} by
\begin{equation*}
\norm{{\boldsymbol\Phi}^{Nh}_{\mathbf F}({\mathbf Y}_{0}) - {\boldsymbol\Phi}^{Nh}_{\mathbf F}({\mathbf A}_{0})}_F \leq e^{LNh} \norm{{\mathbf Y}_{0} - {\mathbf A}_{0}}_F = e^{LNh} \delta_r,
\end{equation*}
\begin{equation*}
\begin{aligned}
\norm{{\boldsymbol\Phi}^{(N-k)h}_{\mathbf F}({\mathbf Y}_{k}) - {\boldsymbol\Phi}^{(N-k)h}_{\mathbf F}({\boldsymbol\Phi}^{h}_{\mathbf F}({\mathbf Y}_{k-1}))}_F &\leq e^{L(N-k)h} \norm{{\mathbf Y}_{k}-{\boldsymbol\Phi}^{h}_{\mathbf F}({\mathbf Y}_{k-1})}_F \\
&\leq e^{L(N-k)h} C h (\varepsilon_r + \gamma_r + h^p),
\end{aligned}
\end{equation*}
which leads to the upper bound:
\begin{equation*}
\norm{{\mathbf Y}_{N}-{\boldsymbol\Phi}^{Nh}_{\mathbf F}({\mathbf A}_0)}_F \leq e^{LNh} \delta_r + C (\varepsilon_r + \gamma_r + h^p) \sum_{k=1}^N e^{L(N-k)h} h.
\end{equation*}
Finally, bounding the Riemann sum as
\begin{equation*}
\sum_{k=1}^N h e^{L(N-k)h} \leq \int_0^{Nh} e^{L(Nh-t)} \diff t = \frac{e^{LNh}-1}{L}
\end{equation*}
and using $Nh \leq T$ lead to
\begin{align*}
\norm{{\mathbf Y}_{N}-{\boldsymbol\Phi}^{Nh}_{\mathbf F}({\mathbf A}_0)}_F &\leq e^{LT} \delta_r + C \frac{e^{LT}-1}{L} (\varepsilon_r + \gamma_r + h^p) \\
&\leq \max\left\{e^{LT},C \frac{e^{LT}-1}{L}\right\} (\delta_r + \varepsilon_r + \gamma_r + h^p),
\end{align*}
which concludes the proof.
\end{proof}

\medskip

\begin{remark}
If $\max\{\delta_r, \varepsilon_r, \gamma_r\} \ll h^p$, then the Runge-Kutta BUG integrator has the same order of convergence as the associated Runge-Kutta method. Otherwise, the global error is dominated by the low-rank truncation error (namely the initial, projection, and truncation errors) and vanishes as the rank becomes full.
\end{remark}


\section{Numerical experiments}
\label{sec:5}

According to Theorem \ref{theo:T2}, the rate of convergence of the proposed Runge-Kutta BUG integrator scales with $h^p$ until the error reaches a plateau which decreases as the rank increases. This theoretical result is validated experimentally on three numerical test cases taken from \cite{kieri2019projection,lam2024randomized}. To this end, we investigate the convergence of several Runge-Kutta BUG integrators based on the following high-oder explicit Runge-Kutta methods:
\begin{itemize}
\item midpoint method (RK2m),
\item Heun's method (RK2h),
\item Heun's third-order method (RK3h),
\item third-order SSP RK method (RK3s),
\item classic fourth-order Runge-Kutta method (RK4),
\end{itemize}
where the associated Butcher tableaux are given in Appendix \ref{sec:A1}. The accuracy of the resulting integrators is measured with respect to a reference solution ${\mathbf A}_k$ computed using the fifth-order Runge-Kutta-Fehlberg method:
\begin{equation*}
\textrm{Error} = \max\limits_{k = 0,\ldots,N} \norm{{\mathbf Y}_k - {\mathbf A}_k}_F.
\end{equation*}
Moreover, we compare these Runge-Kutta BUG integrators with the following state-of-the-art dynamical low-rank integrators:
\begin{itemize}
\item the first variant of the second-order BUG integrator (Midpoint BUG) of \cite{ceruti2024robust},
\item the second-order projected Runge-Kutta method (PRK2h) \cite{kieri2019projection},
\item the third-order projected Runge-Kutta method (PRK3h) \cite{kieri2019projection}.
\end{itemize}
Notably, we consider the first variant of the midpoint BUG integrator \cite{ceruti2024robust} because it is more accurate than the second variant. In addition, this integrator is different from our Runge-Kutta BUG integrator based on the midpoint method since the intermediate solution is not truncated (the rank of $\widehat{\mathbf Y}_{k+1}$ is at most $4r$ instead of $2r$ in our Runge-Kutta BUG integrator). As a result, this integrator is computationally more expensive than our Runge-Kutta BUG integrator, but it is potentially more accurate. Note that the accuracy of these two integrators is almost the same in the following experiments.


\subsection{Allen-Cahn equation}

We first consider the Allen-Cahn equation:
\begin{equation*}
\left\{
\begin{aligned}
\dot {\mathbf A}(t) &=  \theta ({\mathbf L} {\mathbf A}(t) + {\mathbf A}(t) {\mathbf L}) +{\mathbf A}(t) - {\mathbf A}(t) \odot {\mathbf A}(t) \odot {\mathbf A}(t) \\
{\mathbf A}(0) &= {\mathbf A}_0,
\end{aligned}
\right.
\end{equation*}
where ${\mathbf A}(t) \in \R^{n \times n}$, ${\mathbf L} = \frac{n^2}{4 \pi^2}\textrm{diag}(1,-2,1) \in \R^{n \times n}$, $t \in \ff{0}{10}$, $\theta = 10^{-2}$, and $\odot$ stands for the Hadamart product. The domain ${\ff{0}{2\pi}}^2$ is discretized using $n=128$ equidistant points in each direction, and the initial solution is given by 
\begin{equation*}
({\mathbf A}_0)_{i,j} = \frac{\left[e^{-\tan^2(x_i)}+e^{-\tan^2(y_j)}\right]\sin(x_i)\sin(y_j)}{1+e^{|\csc(-x_i/2)|}+e^{|\csc(-y_j/2)|}}.
\end{equation*}

In Figure \ref{fig:fig5}, we present the error of the Runge-Kutta BUG integrator as a function of the time-step size $h$ and rank $r$. The reader can see that the Runge-Kutta BUG integrator achieves second-, third-, and fourth-order convergences until the error reaches a plateau corresponding to the low-rank truncation error. This plateau decreases as the rank increases up to a certain limit, around $10^{-9}$, resulting from the accumulation of roundoff error. In this example, the errors of the midpoint BUG integrator and projected Runge-Kutta methods are almost identical to those of our corresponding Runge-Kutta BUG integrators.

\begin{figure}
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{allen_cahn_2m.eps}
\caption{Midpoint method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{allen_cahn_2h.eps}
\caption{Heun's method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{allen_cahn_3s.eps}
\caption{Third-order SSP Runge-Kutta method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{allen_cahn_3h.eps}
\caption{Heun's third-order method}
\end{subfigure}\hfill
\begin{subfigure}[c]{1\linewidth}
\center
\includegraphics[width=6.5cm]{allen_cahn_4.eps}
\caption{Classic fourth-order Runge-Kutta method}
\end{subfigure}\hfill
\caption{\label{fig:fig5}Convergence error of the Runge-Kutta BUG integrator for the Allen-Cahn equation}
\end{figure}


\subsection{Lyapunov equation}

Then, we consider the Lyapunov equation:
\begin{equation*}
\left\{
\begin{aligned}
\dot {\mathbf A}(t) &=  {\mathbf L} {\mathbf A}(t) + {\mathbf A}(t) {\mathbf L} + \theta \frac{\mathbf C}{\norm{\mathbf C}_F} \\
{\mathbf A}(0) &= {\mathbf A}_0,
\end{aligned}
\right.
\end{equation*}
where ${\mathbf A}(t) \in \R^{n \times n}$, ${\mathbf L} = \frac{n^2}{4 \pi^2}\textrm{diag}(1,-2,1) \in \R^{n \times n}$, ${\mathbf C} \in \R^{n \times n}$, $t \in \ff{0}{1}$, and $\theta \geq 0$. The domain ${\ff{-\pi}{\pi}}^2$ is discretized using $n=128$ equidistant points in each direction, and the initial solution and forcing term are given by 
\begin{equation*}
({\mathbf A}_0)_{ij} = \sin(x_i)\sin(y_j) \quad \textrm{and}  \quad ({\mathbf C})_{ij} = \sum\limits_{l=1}^{11} 10^{-(l-1)} e^{-l(x_i^2+y_j^2)}.
\end{equation*}

We start with the case $\theta = 10^{-5}$. In Figure \ref{fig:fig1}, we fix the rank to $r=5$ and we report the error of the Runge-Kutta BUG integrator as a function of the time-step size $h$. The reader can see that the proposed dynamical low-rank integrator achieves second- and third-order convergences until the error reaches a limit, around $10^{-10}$, resulting from roundoff errors. Regarding the fourth-order convergence, this one is not visible since the error is already of the order of $10^{-11}$ for $h=5\times 10^{-4}$. Moreover, the errors of the midpoint BUG integrator and projected Runge-Kutta methods are almost the same as those of the corresponding Runge-Kutta BUG integrators for second-order methods. However, for the Heun's third-order method, the Runge-Kutta BUG integrator is significantly more accurate than the projected Runge-Kutta method.

We now consider the case $\theta = 1$, where a higher rank is necessary to obtain a small rank truncation error due to the forcing term. In Figure \ref{fig:fig2}, we see that the Runge-Kutta BUG integrator achieves second-order convergence. The third- and fourth-order convergences are not visible because the error, around $10^{-10}$, is of the order of the cumulative roundoff error. Moreover, for second-order methods, the error reaches a plateau corresponding to the low-rank truncation error and which decreases as the rank increases. Furthermore, the Runge-Kutta BUG integrator is several orders of magnitude more accurate than the projected Runge-Kutta method, which can be explained by the fact that the Galerkin projection is more accurate than the tangent-space projection for approximating the discrete solution.


\begin{figure}
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{lyapunov_2_2m.eps}
\caption{Midpoint method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{lyapunov_2_2h.eps}
\caption{Heun's method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{lyapunov_2_3s.eps}
\caption{Third-order SSP Runge-Kutta method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{lyapunov_2_3h.eps}
\caption{Heun's third-order method}
\end{subfigure}\hfill
\begin{subfigure}[c]{1\linewidth}
\center
\includegraphics[width=6.5cm]{lyapunov_2_4.eps}
\caption{Classic fourth-order Runge-Kutta method}
\end{subfigure}\hfill
\caption{\label{fig:fig1}Convergence error of the Runge-Kutta BUG integrator for the Lyapunov equation with $\theta=10^{-5}$}
\end{figure}

\begin{figure}
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{lyapunov_1_2m.eps}
\caption{Midpoint method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{lyapunov_1_2h.eps}
\caption{\label{fig:fig2_2}Heun's method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{lyapunov_1_3s.eps}
\caption{Third-order SSP Runge-Kutta method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{lyapunov_1_3h.eps}
\caption{\label{fig:fig2_4}Heun's third-order method}
\end{subfigure}\hfill
\begin{subfigure}[c]{1\linewidth}
\center
\includegraphics[width=6.5cm]{lyapunov_1_4.eps}
\caption{Classic fourth-order Runge-Kutta method}
\end{subfigure}\hfill
\caption{\label{fig:fig2}Convergence error of the Runge-Kutta BUG integrator for the Lyapunov equation with $\theta=1$. In \ref{fig:fig2_2} and \ref{fig:fig2_4}, the lines corresponding to the two projected Runge-Kutta methods are superposed as the error is dominated by the low-rank approximation error}
\end{figure}


\subsection{Discrete nonlinear Sch\"odinger equation}

Lastly, we consider the discrete nonlinear Sch\"odinger equation \cite{trombettoni2001discrete}:
\begin{equation*}
\left\{
\begin{aligned}
i \dot {\mathbf A}(t) &= -\frac{1}{2} ({\mathbf D} {\mathbf A}(t) + {\mathbf A}(t) {\mathbf D}) - \theta |{\mathbf A}(t)|^2 \odot {\mathbf A}(t)\\
{\mathbf A}(0) &= {\mathbf A}_0,
\end{aligned}
\right.
\end{equation*}
where ${\mathbf A}(t) \in \R^{n \times n}$, ${\mathbf D} = \textrm{diag}(1,0,1) \in \R^{n \times n}$, $t \in \ff{0}{5}$, and $\theta \geq 0$. Moreover, we set $n=128$, and the initial solution is given by
\begin{equation*}
({\mathbf A}_0)_{jl} = \exp(-\frac{(j-60)^2}{100}-\frac{(l-50)^2}{100})+\exp(-\frac{(j-50)^2}{100}-\frac{(l-40)^2}{100}).
\end{equation*}

We start with the case $\theta = 0.1$. In Figure \ref{fig:fig3}, we present the error of the Runge-Kutta BUG integrator as a function of the time-step size $h$ and rank $r$. The reader can see that the Runge-Kutta BUG integrator achieves the expected orders of convergence. Then, the error reaches the plateau resulting from the low-rank truncation error and which decreases as the rank increases. Moreover, the errors of the midpoint BUG integrator and projected Runge-Kutta methods are almost the same as those of the corresponding Runge-Kutta BUG integrators for second-order methods. However, for the Heun's third-order method, the Runge-Kutta BUG integrator is significantly more accurate than the projected Runge-Kutta method.

We now consider the case $\theta = 0.3$, where a higher rank is necessary to obtain small rank truncation errors due to the cubic term. In Figure \ref{fig:fig4}, we see that the error behaves in the same way as before. Note that, for third-order methods, the Runge-Kutta BUG integrator achieves third-order convergence for $h \leq 10^{-1}$.


\begin{figure}
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{schrodinger_1_2m.eps}
\caption{Midpoint method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{schrodinger_1_2h.eps}
\caption{Heun's method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{schrodinger_1_3s.eps}
\caption{Third-order SSP Runge-Kutta method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{schrodinger_1_3h.eps}
\caption{Heun's third-order method}
\end{subfigure}\hfill
\begin{subfigure}[c]{\linewidth}
\center
\includegraphics[width=6.5cm]{schrodinger_1_4.eps}
\caption{Classic fourth-order Runge-Kutta method}
\end{subfigure}\hfill
\caption{\label{fig:fig3}Convergence error of the Runge-Kutta BUG integrator for the Schr\"odinger equation with $\theta=0.1$}
\end{figure}


\begin{figure}
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{schrodinger_2_2m.eps}
\caption{Midpoint method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{schrodinger_2_2h.eps}
\caption{Heun's method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{schrodinger_2_3s.eps}
\caption{Third-order SSP Runge-Kutta method}
\end{subfigure}\hfill
\begin{subfigure}[c]{0.5\linewidth}
\center
\includegraphics[width=6.5cm]{schrodinger_2_3h.eps}
\caption{Heun's third-order method}
\end{subfigure}\hfill
\begin{subfigure}[c]{\linewidth}
\center
\includegraphics[width=6.5cm]{schrodinger_2_4.eps}
\caption{Classic fourth-order Runge-Kutta method}
\end{subfigure}\hfill
\caption{\label{fig:fig4}Convergence error of the Runge-Kutta BUG integrator for the Schr\"odinger equation with $\theta=0.3$}
\end{figure}


\section{Conclusion}
\label{sec:6}

In this work, we have proposed high-order BUG integrators based on explicit Runge-Kutta methods. By construction, the resulting dynamical low-rank integrators are robust to the presence of small singular values and do not involve backward time-integration steps. Then, we have analyzed the convergence of the proposed Runge-Kutta BUG integrator. The error bound shows that the Runge-Kutta BUG integrator retains the order of convergence of the associated Runge-Kutta method until the error reaches a plateau corresponding to the low-rank truncation error and which vanishes as the rank becomes full.
This error bound has been validated experimentally on three numerical test cases. The results demonstrate the high-order convergence of the Runge-Kutta BUG integrator and its superior accuracy compared to projected Runge-Kutta methods for high orders $p \geq 3$ and small time-steps.
In perspective, the Runge-Kutta BUG integrator can be extended to other classes of Runge-Kutta methods, such as exponential Runge-Kutta methods or implicit Runge-Kutta methods. These topics will be addressed in future work.


\backmatter

\bmhead{Acknowledgements}

This work has been supported by the Swiss National Science Foundation under the Project n°200518 "Dynamical low rank methods for uncertainty quantification and data assimilation".


\section*{Declarations}

\bmhead{Data availability}

The datasets generated during and/or analyzed during the current study are available from the corresponding author on reasonable request.

\bmhead{Conflict of interest}

The authors declare that they have no conflict of interest.


\begin{appendices}

\section{List of Runge-Kutta methods}\label{sec:A1}

The Butcher tableaux associated with the Runge-Kutta methods used in this work are listed below.

\begin{itemize}
\item Forward Euler method
\medskip

\begin{tabular}{c | c c}
0 & 0 \\
1 & 1 \\
\hline
 & 1
\end{tabular}

\medskip
\item Explicit midpoint method
\medskip

\begin{tabular}{c | c c}
0 & 0 & 0 \\
1/2 & 1/2 & 0 \\
\hline
 & 0 & 1
\end{tabular}

\medskip
\item Heun's method
\medskip

\begin{tabular}{c | c c}
0 & 0 & 0 \\
1 & 1 & 0 \\
\hline
 & 1/2 & 1/2
\end{tabular}

\medskip
\item Third-order SSP Runge-Kutta method
\medskip

\begin{tabular}{c | c c c}
0 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1/2 & 1/4 & 1/4 & 0 \\
\hline
 & 1/6 & 1/6 & 2/3
\end{tabular}

\medskip
\item Heun's third-order method
\medskip

\begin{tabular}{c | c c c}
0 & 0 & 0 & 0 \\
1/3 & 1/3 & 0 & 0 \\
2/3 & 0 & 2/3 & 0 \\
\hline
 & 1/4 & 0 & 3/4
\end{tabular}

\medskip
\item Classic fourth-order Runge-Kutta method
\medskip

\begin{tabular}{c | c c c c}
0 & 0 & 0 & 0 & 0 \\
1/2 & 1/2 & 0 & 0 & 0 \\
1/2 & 0 & 1/2 & 0 & 0 \\
1 & 0 & 0 & 1 & 0 \\
\hline
 & 1/6 & 1/3 & 1/3 & 1/6
\end{tabular}
\end{itemize}


\end{appendices}


\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
