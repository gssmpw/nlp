\subsection{Regularized Langevin Dynamics}
How to select the step size $\alpha$ is critical to the effectiveness of LD for CO. In this work, we propose to select $\alpha$ by regularizing the update with the L$2$ distance\footnote{there is no difference between  L$1$ and L$2$ distances on binary data, we leave the choice on other discrete data as future study.} between the sampled and current solutions:
\begin{equation}
\begin{split}
    \label{eq:rld}
    q(\mathbf{x}'_i|\mathbf{x}) \propto &\exp({\frac{1}{2}s(\mathbf{x})_i(\mathbf{x}_i'-\mathbf{x}_i)-\frac{(\mathbf{x}_i'-\mathbf{x}_i)^2}{2\alpha})}, \\
    \text{s.t.} \qquad & \mathbb{E}_{q(\mathbf{x}'|\mathbf{x})}[\|\mathbf{x}'-\mathbf{x}\|_2]=d,
\end{split}
\end{equation}
where $d$ is a positive integer. When $\mathbf{x}$ is binary, we could explicitly write out the expectation in Equation \ref{eq:rld} with the flipping probability $q(\mathbf{x}'_i=1-\mathbf{x}_i|\mathbf{x})$ :
\begin{equation}
\label{eq:regularization_simgoid}
    \sum_{i=1}^N\texttt{Sigmoid}(\frac{1}{2}s(\mathbf{x})_i(1-2\mathbf{x}_i)-\frac{1}{2\alpha})=d.
\end{equation}

We find this simple regularization method very effective in mitigating the local optima issue in CO, in the sense that the regularization term enforces the change of a fixed magnitude to the solution, regardless of the gradient. We name our method as \textit{Regularized Langevin Dynamics (RLD)}, and we proceed to introduce its applications in both SA and NN-based CO solvers.



\subsection{Regularized Langevin Simulated Annealing}
\label{sec:rlsa}
Since the gradient of the energy function  could be computed in an closed form for various CO problems, here we first assume $\nabla H(\mathbf{x})$ is available. Note that the score function of the EBM could be written as $s_{\tau}(\mathbf{x})=\log p_{\tau}(\mathbf{x})=-\frac{1}{\tau}\nabla H(\mathbf{x})$. To avoid the clutter, we denote $\Delta=(2\mathbf{x}-1)\odot\nabla H(\mathbf{x})$, whose $i$-th coordinate approximates the drop of the energy function if we flip the value of $\mathbf{x}_i$.

Explicitly solving Equation \ref{eq:regularization_simgoid} is challenging due to the presence of the sigmoid function. However, when $\tau\to0$, the sigmoid function is approximately an indicator function:
\begin{equation} 
    \lim_{\tau\to 0}\texttt{Sigmoid}(\frac{1}{2\tau}\Delta_i-\frac{1}{2\alpha})=\mathbbm{1}(\frac{1}{2\tau}\Delta_i-\frac{1}{2\alpha}>0).
\end{equation}
This property allows us to efficiently regularize the SA algorithm with the $d$-th largest element in $\Delta$, denoted as $\Delta_{(d)}$. We then obtain the flipping probability by letting $\frac{1}{\alpha}=\frac{\Delta_{(d)}}{\tau}$:
\begin{equation}
    q(\mathbf{x}'_i=1-\mathbf{x}_i|\mathbf{x}) = \texttt{Sigmoid}(\frac{1}{2\tau}(\Delta_i-\Delta_{(d)})).
\end{equation}
In our experiment, we find simply ignoring $-\frac{1}{\alpha}$ and normalizing all sigmoid function outputs to sum to $d$ could also work, followed by clipping all values into the range $[0,1]$. While in this work we just stick to using the $d$-th largest element. We call the resultant SA algorithm as \textit{Regularized Langevin Simulated Annealing (RLSA)}, whose details are summarized in Algorithm \ref{alg:rlsa}.

\begin{algorithm}[H]
\caption{Regularized Langevin Simulated Annealing}
\label{alg:rlsa}
\begin{algorithmic}[1] %[1] enables line numbers
\STATE \textbf{Input}: $T$, $d$ and $\tau_0$
\STATE Initialize $\mathbf{x}\in \{0,1\}^{N}$; $\mathbf{x}^*\leftarrow \mathbf{x}$
\FOR{$t=1,\cdots,T$}
\STATE  $\tau \leftarrow \tau_0(1-\frac{t-1}{T})$ 
\STATE $\Delta \leftarrow(2\mathbf{x}-1)\odot \nabla H(\mathbf{x})$
\FOR{$i=1,\cdots,N$}
\STATE $p \leftarrow \texttt{Sigmoid}(\frac{1}{2\tau}(\Delta_i-\Delta_{(d)}))$
\STATE $c \sim \texttt{Bernoulli}(p)$
\STATE $\mathbf{x}_i \leftarrow \mathbf{x}_i(1-c)+(1-\mathbf{x}_i)c$
\ENDFOR 
\IF{$H(\mathbf{x})<H(\mathbf{x}^*)$}
\STATE $\mathbf{x}^*\leftarrow \mathbf{x}$
\ENDIF
\ENDFOR
\STATE\textbf{return} $\mathbf{x}^*$ 
\end{algorithmic}
\end{algorithm} 
In our implementation, the elementwise sampling is run in parallel and we maintain $K$ independent SA processes simultaneously. The whole algorithm could be implemented in a few lines and accelerated with GPU-based deep learning frameworks, such as PyTorch \citep{paszke2017automatic} and Jax \citep{jax2018github}. In this work, we implement our algorithm mainly based on PyTorch Geometric \citep{Fey/Lenssen/2019}, and an example code is attached in Appendix \ref{sec:code}. 


Given the overall RLSA framework,  we now address the question of how to compute the gradient of the energy function. Numerous classical CO problems are defined over graphs and could be formulated in quadratic form, known as QUBO \citep{10.3389/fphy.2014.00005}. Let $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ be an undirected graph, with node set $\mathcal{V}=\{1,\cdots,N\}$, edge set $\mathcal{E}\in\mathcal{V}\times\mathcal{V}$, and adjacency matrix $\mathbf{A}\in\{0,1\}^{N\times N}$. In this work, we focus on the following three problems, which have been commonly used in benchmark evaluations for CO solvers. 

\paragraph{Maximum Independent Set.}
The maximum independent set (MIS) problem aims to select the largest subset of nodes from the graph $\mathcal{G}$, without any adjacent pair. Denote a selected node as $\mathbf{x}_i=1$ and an unselected one as $\mathbf{x}_i=1$, the energy function of MIS could be expressed as 
\begin{equation}
\begin{split}
        H(\mathbf{x})=&-\sum_{i=1}^N\mathbf{c}_i\mathbf{x}_i+\beta\sum_{(i,j)\in\mathcal{E}}\mathbf{x}_i\mathbf{x}_j\\
    &=-\mathbf{c}^{\top}\mathbf{x}_i + \beta\frac{\mathbf{x}^{\top}\mathbf{A}\mathbf{x}}{2},
\end{split}
\end{equation}
where $\mathbf{c}\in\mathbb{R}_+^N$ is the node weight vector. It is not hard to compute the gradient of the energy function as 
\begin{equation}
    \nabla H(\mathbf{x})=
    -\mathbf{c} + \beta \mathbf{A}\mathbf{x}.
\end{equation}

\paragraph{Maximum Clique.}
The maximum clique (MCl) stands for the largest subset of nodes in a graph such that every two nodes in the set are adjacent to each other. It could actually be expressed as the MIS problem in the complete graph, with the energy function:
\begin{equation}
\label{eq:mis}
 H(\mathbf{x})=-\sum_{i=1}^N\mathbf{c}_i\mathbf{x}_i+\beta\sum_{(i,j)\notin\mathcal{E}}\mathbf{x}_i\mathbf{x}_j.
\end{equation}
In order to represent the energy function with the adjacency matrix $\mathbf{A}$, we rewrite the penalty as $[(\sum_{i=1}^N\mathbf{x}_i)^2-\sum_{i=1}^N\mathbf{x}_i^2]/2-\sum_{(i,j)\in\mathcal{E}}\mathbf{x}_i\mathbf{x}_j$, resulting in the energy function and its gradient:
\begin{align}
H(\mathbf{x})&=\mathbf{c}^{\top}\mathbf{x}+\beta\frac{(\mathbf{1}^{\top}\mathbf{x})^2-\mathbf{x}^{\top}\mathbf{x}-\mathbf{x}^{\top}\mathbf{A}\mathbf{x})}{2},\\    \nabla H(\mathbf{x})&=
-\mathbf{c}+\beta((\mathbf{1}^{\top}\mathbf{x})\mathbf{1}-\mathbf{x}-\mathbf{A}\mathbf{x}).
\end{align}

\paragraph{Max Cut.} The max cut (MCut) problem looks to partition the nodes into two sets so that the number of edges between two sets is maximized. Here we use $\mathbf{x}_i=1$ and $\mathbf{x}_i=0$ to represent the belonging to two sets, and the energy function could be expressed as 
\begin{equation}
\begin{split}
        H(\mathbf{x})&=-\sum_{(i,j)\in\mathcal{E}}\frac{1-(2\mathbf{x}_i-1)(2\mathbf{x}_j-1)}{2}\\
        &=\mathbf{x}^{\top}\mathbf{A}\mathbf{x}-\mathbf{1}^{\top}\mathbf{A}\mathbf{x},
\end{split}
\end{equation}
whose gradient could be accordingly computed as 
\begin{equation}
    \nabla H(\mathbf{x})=\mathbf{A}(2\mathbf{x}-\mathbf{1}).
\end{equation}

\subsection{Regularized Langevin Neural Network}
When the gradient is intractable or a better approximation of the locally-informed proposal in Equation \ref{eq:local} is wanted,
we could parameterize the sampling distribution $q_{\theta}(\mathbf{x}'|\mathbf{x})$ with a NN. Here we still utilize a mean-filed decomposition, letting $q_{\theta}(\mathbf{x}'|\mathbf{x})=\Pi_{i=1}^Nq_{\theta}(\mathbf{x}'_i|\mathbf{x})$. The RLD update in Equation \ref{eq:rld} could be  translated into the following training loss 
\begin{equation}
\label{eq:obj}
\begin{split}
   l_{RLD}(\theta;\mathbf{x},d, \lambda)=&\mathbf{E}_{q_{\theta}(\mathbf{x}'|\mathbf{x})}[H(\mathbf{x}')] \\+ &\lambda(\sum_{i}^N q_{\theta}(\mathbf{x}'_i=1-\mathbf{x}_i|\mathbf{x})-d)^2.
\end{split}
\end{equation}
Here the first term is similar to the loss function used in the Erdoes Goes Neural \citep{Karalias20ERDOES}, which maximizes the conditional expectation of the energy function after the update. While the second term regularizes the expected Hamming distance between the two solutions, with $\lambda$ being the regularization coefficiednt. We name this NN-based solver as \textit{Regularized Langevin Neural Network (RLNN)}.

We train  RLNN in a similar fashion to reinforcement learning through sampling and update, but without the need to account the future states except the immediate next one. This allows RLNN to circumvent the high variance in estimating the future return when trained with a long sampling process. In detail, each time we sequentially samples $T'$ samples with the current proposal distribution $q_{\theta}(\mathbf{x}'|\mathbf{x})$, then for each sample, we train RLNN to minimize the loss in Equation \ref{eq:obj}. The training algorithm is summarized in Algorithm \ref{alg:rlnn}. 

\begin{algorithm}[H]
\caption{Regularized Langevin Neural Network (train)}
\label{alg:rlnn}
\begin{algorithmic}[1] %[1] enables line numbers
\STATE \textbf{Input}: $T'$, $d$, $\lambda$ 
\STATE Initialize $\theta$
\WHILE{the stopping criterion is not met}
\STATE Initialize $\mathbf{x}\in \{0,1\}^{N}$, $\mathcal{D}=\{\mathbf{x}\}$
\FOR{$t=1,\cdots,T'$}
\STATE $\mathbf{x}' \sim q_{\theta}(\mathbf{x}'|\mathbf{x})$
\STATE $\mathcal{D}\leftarrow \mathcal{D}\cup\{\mathbf{x}'\}$
\STATE $\mathbf{x}\leftarrow\mathbf{x}'$
\ENDFOR 
\STATE $\theta \leftarrow \min_{\theta}\mathbf{E}_{\mathbf{x}\in\mathcal{D}}[l_{RLD}(\theta;\mathbf{x}, d, \lambda)]$
\ENDWHILE
\STATE\textbf{return} $\theta$ 
\end{algorithmic}
\end{algorithm} 
Similarly, we maintain $K'$ parallel sampling processes in our implementation to obtain more efficient training data collection. During the inference time, we simply sample from $q_{\theta}(\mathbf{x}'|\mathbf{x})$ sequentially for $T$ steps with $K$ processes run in parallel. Note that temperature annealing is not used here as we do not find it useful and we simply leave $\tau=1$.


\subsection{Connection to Normalized Gradient Descent}
\label{sec:connection}
Our proposed RLD method is closely related to the normalized gradient descent (NGD) method \citep{NGD06cortes} in the continuous domain:
\begin{equation}
    \mathbf{x}' = \mathbf{x} - \alpha\frac{\nabla f(\mathbf{x})}{\|\nabla f(\mathbf{x})\|_2}.
\end{equation}

NGD is developed to address the vanishing/exploding gradient by normalizing the L$2$ norm of the gradient for a scale-invariant update at each step. Our method could be treated as a discrete version of method by restricting the L$2$ distance between the solutions before and after the update. 

The key difference between the two lies in the case when $\Delta_{(d)}<0$, RLD could not be translated into a gradient descent algorithm in Equation \ref{eq:ld} (in terms of minimizing the energy function) since $\alpha=\frac{\tau}{\Delta_{(d)}}<0$ reverses the gradient descent direction. Instead, this should be treated as a way to escape the local optima without dramatically increasing the energy function. As an analogy n to Equation \ref{eq:ld_exp}, we can express this situation as
\begin{equation}
    q(\mathbf{x}'|\mathbf{x})=\frac{\exp(-\frac{\Delta_{(d)}}{2\tau}\|\mathbf{x}'-\mathbf{x}+\frac{1}{2\Delta_{(d)}}H(\mathbf{\mathbf{x}})\|_2^2)}{Z(\mathbf{x})}.
\end{equation}
The density of $q(\mathbf{x}'|\mathbf{x})$ increases with respect to the distance from $\mathbf{x}-\frac{1}{2\Delta_{(d)}}H(\mathbf{\mathbf{x}})$ , which is the \textbf{gradient ascent direction} (note $\Delta_{(d)}$ is negative here) of the energy function. This behavior arises due to the different property of the local optima in the discrete data, which may not vanish to $\mathbf{0}$ but point to an infeasible region (with $\Delta$ negative in all dimensions).

Let us take MIS as an example, whose local optima is a maximal independent set, i.e., each unselected node has at least one neighbor in the set. The gradient for the selected node ($\mathbf{x}_i=1$)  is $\nabla H(\mathbf{x})_i=  -\mathbf{c}_i<0$, pointing to the direction of increasing the value, which is infeasible as $\mathbf{x}_i\leq 1$. While the gradient for an unselected node ($\mathbf{x}_i=0$) is lower bounded by $\nabla H(\mathbf{x})_i\geq -\mathbf{c}_i+\beta>0$, pointing to the direction of decreasing the value, which is also infeasible. Since the gradient descent direction is not informative, RLD would try to escape this local optima but avoid the steepest direction to increase the energy function, which is exactly the gradient ascent direction:  $\mathbf{x}-\frac{1}{2\Delta_{(d)}}H(\mathbf{\mathbf{x}})$.  

With the same example, we could also see why the standard discrete Langevin sampler \citep{zhang2022langevinlike} with a constant step size does work well. Since LD is a first-order approximation of the locally-informed proposal \citep{zanella2017informed}, a small $\alpha$ is needed to make the approximation accurate. However, a small $\alpha$ would also lead to a strong penalization on the update.  While at the local optima, we also have $\Delta_i<0$  discourage the change, as indicated in  Equation \ref{eq:flip}. Therefore, additional efforts are needed to help LD escape the local optima beyond the force of random noise. Such a distinction between CO and continuous optimization highlights the significance of RLD.
