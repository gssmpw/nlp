In this work, we point out the difference in local optimal between continuous optimization and continuous optimization (CO), and propose a novel sampling framework called Regularized Langevin Dynamics (RLD) to tackle the issue in CO. On top of that, we develop two CO solvers, one based on simulated annealing (SA), and the other one based on neural networks. Our empirical evaluation on three classical CO problems demonstrate that our proposed methods can achieve the state-of-the-art (SOTA) or near-SOTA performance with high efficiency. Especially, our proposed SA method consistently outperforms the previous SA baseline  using only 20\% running time. In summary, RLD is a simple yet effective framework, showing a strong potential in addressing CO problems.

In this work, we only consider binary data for ease of analysis. Although the whole framework could be generalized, its effectiveness remains unclear on other CO problems with integer or mixed integer variables. Future work may extend it to more real-world CO problems by taking other conditions into consideration. Besides, we have given an intuitive explanation of RLD in this work, but the theoretical understanding of RLD is generally missing. We also expect to address this part in our future work.