\section{Additional Experiment Details}
\label{sec:detail}
\subsection{Hyperparameters}
We include the hyperparameters of RLSA and RLNN in Table \ref{tab:hyper_rlsa} and \ref{tab:hyper_rlnn}, respectively. The initial temperature $\tau_0$ is randomly searched in the range of 0.001 to 10, and the step size is randomly searched in the range of 2 to 100. In general, larger $K$ and $T$ always lead to a better performance, so we simply control them such that the running time of RLSA is similar to the fastest baseline. For RLNN, we allow for more time budget, just controlling its $K$ and $T$ such that its running time is comparable to most baselines.

\begin{table*}[h!]
    \centering
        \caption{Hyperparameters used by RLSA on all datasets.}
    \begin{tabular}{ll|ccccc}
    \toprule
    Problem & Dataset & $\tau_0$ & $d$ & $K$ & $T$ & $\beta$\\
    \midrule 
     \multirow{4}{*}{MIS} & RB-[$200$-$300$] & $0.01$ & $5$  & $200$ & $300$ & $1.02$ \\
         &  RB-[$800$-$1200$]& $0.01$ & $5$  & $200$ & $500$  & $1.02$  \\
         &  ER-[$700$-$800$] & $0.01$ & $20$  & $200$ & $500$ & $1.001$\\
         & ER-[$9000$-$1100$]& $0.01$ & $20$   & $200$ & $5000$ & $1.001$\\
         \midrule
     \multirow{2}{*}{MCl} & RB-[$200$-$300$] & $4$ & $2$ & $200$ & $100$  & $1.02$ \\
     &  RB-[$800$-$1200$]& $4$ & $2$ & $200$ & $500$  & $1.02$ \\
     \midrule
     \multirow{2}{*}{MCut} & BA-[$200$-$300$] & $5$ & $20$ & $200$ & $200$  & $1.02$ \\
     &  BA-[$800$-$1200$] & $5$  & $20$ & $200$ & $500$  & $1.02$ \\
    \bottomrule     
    \end{tabular}
    \label{tab:hyper_rlsa}
\end{table*}

\begin{table*}[ht!]
    \centering
        \caption{Hyperparameters used by RLNN on all datasets.}
    \begin{tabular}{ll|cccccccc}
    \toprule
    Problem & Dataset & $\tau_0$ & $d$ & $K$ & $T$ & $\beta$ & $K'$ & $T'$ & $\lambda $\\
    \midrule 
     \multirow{4}{*}{MIS} & RB-[$200$-$300$] & $1$ & $5$  & $20$ & $100$ & $1.02$  & $10$ & $50$ & $0.5$\\
         &  RB-[$800$-$1200$]& $1$ & $5$  & $20$ & $200$  & $1.02$ & $10$ & $300$ & $0.5$\\
         &  ER-[$700$-$800$] & $1$ & $20$  & $20$ & $200$ & $1.001$ & $10$ & $500$ & $0.5$\\
         & ER-[$9000$-$1100$]& $1$ & $20$   & $20$ & $800$ & $1.001$ &
--- & --- & --- \\
         \midrule
     \multirow{2}{*}{MCl} & RB-[$200$-$300$] & $1$ & $2$ & $20$ & $100$  & $1.02$ &  $10$ & $100$ & $0.5$ \\
     &  RB-[$800$-$1200$]& $1$ & $2$ & $20$ & $200$  & $1.02$ & $10$ & $300$ & $0.5$\\
     \midrule
     \multirow{2}{*}{MCut} & BA-[$200$-$300$] & $1$ & $20$ & $20$ & $100$  & $1.02$ & $10$ & $50$ & $0.5$ \\
     &  BA-[$800$-$1200$] & $1$  & $20$ & $20$ & $200$  & $1.02$ & $10$ & $300$ & $0.5$\\
    \bottomrule     
    \end{tabular}
    \label{tab:hyper_rlnn}
\end{table*}
\subsection{Training}
RLNN is parameterized by a five-layer GCN \citep{Welling2011LD} with 128 hidden dimensions. A linear layer is first used to project the input solution $\mathbf{x}$ into a 128-dim embedding $\mathbf{H}^0$. Each layer of GCN performs the following update:
\begin{equation}
    \mathbf{H}^{l+1}=\sigma(\mathbf{U}^l\mathbf{H}^l+\mathbf{V}^l\mathbf{D}^{-1/2}\hat{\mathbf{A}}\mathbf{D}^{-1/2}\mathbf{H}^l) +
    \mathbf{H}^l,
\end{equation}
where $\mathbf{U}^l$ and $\mathbf{V}^l$ are model parameters, $\hat{\mathbf{A}}=\mathbf{A}+\mathbf{I}_{N\times N}$ is the adjacency matrix with the self loop, $\mathbf{D}$ is a diagonal degree matrix with $\mathbf{D}_{ii}=\sum{j=1}^N\hat{\mathbf{A}}_{ij}$, and $\sigma$ is the activation function. We use ReLU as the activation function for all layers. The output hidden vector is projected by a final linear layer into the single dimension. A sigmoid function is then applied to generate the flipping probability $q_{\theta}(\mathbf{x}'_i=1-\mathbf{x}_i|\mathbf{x})$. 

We train RLNN with $50$ epochs in all data sets except RB- [$700$--$800$] for MCl, where we use 80 epochs since we notice that the training does not converge at 50 epochs.  On each graph, we sample $K'$ trajectories with $T'$ lengths, which amounts to $K'T'$ samples to train on. The batch size is set as 32 per GPU, and an Adam optimizer is used for optimization, with a learning rate of 0.0001.

In terms of the training time, all experiments can finish under 1 hour on our server with 8 RTX A6000 GPUs. The large-scale graphs and ER-[$700$-$800$] graphs may need 3 hours to finish on our server with 10 RTX 2080 Ti GPUs. Our implementation is based on PyTorch Geometric \citep{Fey/Lenssen/2019} and accelerate \citep{accelerate}.

During our inference time, we use the float16 data type to store the tensor, which accelerates the tensor product. But in general, we find our method way more efficient than our baselines even without this technique.
\subsection{Postprocessing}
We postprocess the best solutions (with the lowest energy) on MIS and MCl problems to satisfy the constraint. We find our method almost surely yields a valid solution, so we just adopt the simplest way to greedily decode it. On MIS, we sort all nodes according to its value ($0$ or $1$) and initialize the candidate set with all nodes. Each time we select a node, we remove itself and all its neighbors  from the candidate set and repeat the process until no candidates are available. On MCl, we perform a similar process by selecting a node and removing itself and all the nodes not in its neighborhood set. on MCut, we simply return the best solution since the problem is not constrained.


\section{Comparison under More Running Steps}
\label{ref:longer}
Here we compare iSCO \citep{sun2023revisiting} and RLSA by running both methods with $10\times$ more steps in Table \ref{tab:long_mis} and \ref{tab:long_mc}. Although iSCO has achieved a very close performance to RLSA in some small-scale datasets, RLSA still maintains a clear advantage on large-scale datasets, with significantly less running time.

In particular, we note that RLSA has already achieved comparable performance to the exact solvers over multiple benchmarks in Table \ref{tab:mis} and \ref{tab:mc}, especially on the large-scale problems. This has demonstrated the strong power of RLSA in CO.

\begin{table*}[ht!]
\small
    \centering
        \caption{Comparative results between iSCO and RLSA with $10$ times more steps on MIS. The best one is bolded.}
            \vspace{5pt}
    \begin{tabular}{cc|cccccccc}

     \multicolumn{2}{c}{\textbf{MIS}}  &  \multicolumn{2}{c}{RB-[$200$--$300$]} &  \multicolumn{2}{c}{RB-[$800$--$1200$]}   & \multicolumn{2}{c}{ER-[$700$--$800$]} &  \multicolumn{2}{c}{ER-[$9000$--$11000$]} \\%&  \multicolumn{2}{c}{SATLIB}  \\
         %\toprule
             \toprule
        \textsc{Method}  &       \textsc{Type}     & \textsc{Size} $\uparrow$ & \textsc{Time} $\downarrow$  & \textsc{Size} $\uparrow$ & \textsc{Time} $\downarrow$ & \textsc{Size} $\uparrow$ & \textsc{Time} $\downarrow$ & \textsc{Size} $\uparrow$ & \textsc{Time} $\downarrow$ \\%& \textsc{Size} $\uparrow$ &\\ \textsc{Time} $\downarrow$     \\
    \midrule
 
   iSCO ($10\times$) & H & $20.01$ & 26.25m & $40.47$ & 1.87h  & $44.41$ & 7.21m & $378.56$ & 11.03h\\
     RLSA ($10\times$) & H & $\textbf{20.10}$ & 6.98m &$\textbf{41.83}$ & 10.65m & $\textbf{45.05}$ & 2.92m & $\textbf{379.19}$ & 17.63m \\     
    \bottomrule
\end{tabular}

    \label{tab:long_mis}
\end{table*}
\begin{table*}[ht!]
\small
\centering
    \caption{Comparative results between iSCO and RLSA with $10$ times more steps on MCl and MCut. The best one is bolded.}
    \vspace{5pt}
    \begin{tabular}{cc|cccc|cc|cccc}

       \multicolumn{2}{c}{\textbf{MCl}} &  \multicolumn{2}{c}{RB-[$200$--$300$]} & \multicolumn{2}{c}{RB-[$800$--$1200$]} &      
       \multicolumn{2}{c}{\textbf{MCut}} &  \multicolumn{2}{c}{BA-[$200$--$300$]} & \multicolumn{2}{c}{BA-[$800$--$1200$]}    \\
           \toprule   % \toprule
       \textsc{Method}  &       \textsc{Type}     & \textsc{Size} $\uparrow$  & \textsc{Time} $\downarrow$  & \textsc{Size} $\uparrow$  & \textsc{Time} $\downarrow$ &\textsc{Method}  &       \textsc{Type}     & \textsc{Size} $\uparrow$  & \textsc{Time} $\downarrow$  & \textsc{Size} $\uparrow$  & \textsc{Time} $\downarrow$   \\
    \midrule
    
 iSCO ($10\times$) & H & $18.97$ & 8.81m & $40.41$ &  1.83h & iSCO ($10\times$) & H &  \textbf{$734.62$} & 1.20h &$2960.23$ & 43.98m\\  
   
 RLSA ($10\times$) & H & $18.97$ & 3.14m & $\textbf{40.63}$ & 8.67m & RLSA ($10\times$) & H & $734.62$ & 4.07m & $\textbf{2968.59}$ & 10.25m\\ 
    \bottomrule
    \end{tabular}
    

    \label{tab:long_mc}
\end{table*}

\section{Example Code for RLSA}
\label{sec:code}

The following Python code outlines our implementation of RLSA. The energy function corresponds to the formulas in Section \ref{sec:rlsa} and the input parameters are in summarized in Section \ref{sec:detail}. In our experiments, the time measurement corresponds to the running time of the entire RLSA function below.

\begin{lstlisting}[frame=single]
def energy_func(A, b, x, penalty_coeff=1.02):
    """
    The energy function is: b^Tx+penalty_coeff*x^TAx
    Return the energy and the gradient
    """
    
    L = A@x
    energy = torch.sum(x*(penalty_coeff*L+b), dim=0)
    grad = 2*penalty_coeff*L+b

    return energy, grad
        
def RLSA(graph, tau0, step_size, num_runs, num_steps, penalty_coeff):
    """
    graph: the graph object in torch_geometric
    num_runs: the number of parallel SA processes
    num_steps: the number of SA steps
    tau0: the initial temperature
    """

    # initialization
    num_nodes = graph.num_nodes
    
    A = torch.sparse_coo_tensor(
            graph.edge_index, 
            graph.edge_weight,
            torch.Size((num_nodes, num_nodes))
        )
    x = torch.randint(0,2, (num_nodes, num_runs))
    
    energy, grad = energy_func(A, graph.b, x, penalty_coeff)
    best_energy = energy
    best_sol = x.clone()

    # SA
    for epoch in range(num_steps):          
        # annealing
        tau = tau0*(1-epoch/num_steps)

        # sampling
        delta = grad*(2*x-1)/2
        term2 = -torch.kthvalue(
                    -delta, 
                    step_size, 
                    dim=0, 
                    keepdim=True
                ).values

        flip_prob = torch.sigmoid((delta-term2)/tau)
        rr = torch.rand_like(x.data)
        x = torch.where(rr<flip_prob, 1-x, x)

        # update loss
        energy, grad = energy_func(A, graph.b, x, penalty_coeff)
        to_update = energy<best_energy
        best_sol[:,to_update] = x[:,to_update]
        best_energy[to_update] = energy[to_update]
    
    return best_energy, best_sol
\end{lstlisting}





    