\subsection{Neural Solvers for Combinatorial Optimization}
The neural network (NN) models have recently garnered vast attention in solving CO problem \citep{bengio2020machine}. The NN-based solvers could be roughly categorized into three classes according to the training methods, including  the supervised learning-based  \citep{Li2018CombinatorialOW, GasseCFCL19, sun2023difusco, li2023from, li2024fast}, unsupervised learning-based \citep{Karalias2020ErdosGN, wang2022unsupervised, wang2023unsupervised, zhang2023let, SanokowskiHL24}, and reinforcement learning-based \citep{Khalil2017DQNCO, qiu2022dimes} methods. Our proposed RLNN method is partially based on reinforcement learning, but could be efficiently trained with a local objective. Such a feature has greatly improved its training efficiency by eliminating the need to estimate the future return.


\subsection{Sampling for Combinatorial Optimization}
Sampling-based methods \citep{Metropolis1953EquationOS, Hastings1970MonteCS, Neal1996SamplingFM, IBA_2001} have been commonly applied in CO problems \citep{tsp_sample, Bhattacharya2014SimulatedAA, TAVAKKOLIMOGHADDAM2007406, SECKINER200731, Chen2004MultiobjectiveVP}. However, earlier methods often encountered slow convergence compared to learning-based approaches due to an inefficient proposal. Recent advancements in discrete Monte Carlo Markov Chain \citep{Grathwohl2021OopsIT, zhang2022langevinlike, sun2022path} have revitalized sampling-based methods and  \citet{sun2023revisiting} demonstrated that simulated annealing (SA) can surpass neural CO solvers. In this work, we have advanced the current study on discrete Langevin dynamics, and proposed a novel SA algorithm. Our conclusion supports the previous study the further advances the development of the field.

