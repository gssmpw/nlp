\subsection{Combinatorial Optimization Problems}
Following \citet{Papadimitriou1982CO}, we formulate the combinatorial optimization (CO) problem as a constrained optimization problem, i.e., 
\begin{equation}
\label{eq:standard}
    \min_{\mathbf{x}\in\{0,1\}^N} a(\mathbf{x}) \quad \text{s.t.} \quad b(\mathbf{x})=0,
\end{equation}
where $a(\mathbf{x})$ stands for the target to optimize and $b(\mathbf{x})\geq 0$ corresponds to the amount of constraint violation ($0$ means no violation). In particular, we focus on the penalty form that can be written as
\begin{equation}
\label{eq:penalty}
    \min_{\mathbf{x}\in\{0,1\}^N} H(\mathbf{x})=a(\mathbf{x})+\beta b(\mathbf{x}),
\end{equation}
where $\beta>0$ is the penalty coefficient that should be sufficiently large, such that the minima of Equation \ref{eq:penalty} corresponds to the feasible solutions in Equation \ref{eq:standard}. \( H(\mathbf{x}) \) is also generally named as the energy function, and its associated energy-based model (EBM) is defined as
\begin{equation}
\label{eq:ebm}
    p_{\tau}(\mathbf{x}) = \frac{\exp(-H(\mathbf{x})/\tau)}{Z},
\end{equation}
where $\tau>0$ is the temperature  controlling the smoothness of the distribution, and \( Z = \sum_{\mathbf{x} \in \{0,1\}^N} \exp(-H(\mathbf{x})/\tau) \) is the normalization factor, typically intractable. When $\tau$ is small, the probability mass of \( p_{\tau} \) tends to concentrate around low-energy samples, making the task of solving Equation \ref{eq:standard} equivalent to sampling from \( p_{\tau}(\mathbf{x}) \). Markov Chain Monte Carlo (MCMC) \citep{Lecun2006ebm} is the most widely used method for sampling from the EBM defined above. However, directly applying MCMC may lead to inefficiencies due to the non-smoothness introduced by the small \( \tau \). To mitigate this issue, the simulated annealing (SA) technique is commonly employed to gradually decrease \( \tau \) towards zero during the MCMC process.

\subsection{Langevin Dynamics}
Langevin dynamics (LD) \citep{Welling2011LD} is an efficient MCMC algorithm initially developed in the continuous domain. It takes a noisy gradient ascent update at each step to gradually increase the log-likelihood of the sample:
\begin{equation}
\label{eq:ld}
    \mathbf{x}' = \mathbf{x} + \frac{\alpha}{2}  s(\mathbf{x}) + \sqrt{\alpha} \zeta, \quad \zeta\in\mathcal{N}(0, \mathbf{I}_{N\times N}),
\end{equation}
where $s(\mathbf{x})=\nabla \log p(\mathbf{x})$ is known as the score function (gradient of the log likelihood), and $\alpha>0$ represents the step size. By iteratively performing the above update, the sample $\mathbf{x}$ would eventually end up at a stationary distribution approximately equal to $p(\mathbf{x})$.

 Recently, \citet{zhang2022langevinlike} have extended LD  to discrete space  by rewriting Equation \ref{eq:ld} as
 \begin{equation}
    \label{eq:ld_exp}
    q(\mathbf{x}'|\mathbf{x})=\frac{\exp(-{\frac{1}{2\alpha}\|\mathbf{x}'-\mathbf{x}-\frac{\alpha}{2}s(\mathbf{x})}\|_2^2)}{Z(\mathbf{x})},
 \end{equation}
 where $Z(\mathbf{x})$ is the normalization factor. For discrete data, the above distribution could be factorized coordinatewisely, i.e., $q(\mathbf{x}'|\mathbf{x}) = \prod_{i=1}^Nq(\mathbf{x}'_i|\mathbf{x})$, into a set of categorical distributions:
\begin{equation}
    q(\mathbf{x}'_i|\mathbf{x})\propto \exp({\frac{1}{2}s(\mathbf{x})_i(\mathbf{x}_i'-\mathbf{x}_i)-\frac{(\mathbf{x}_i'-\mathbf{x}_i)^2}{2\alpha})}.
\end{equation}
When $\mathbf{x}$ is a binary vector, we can obtain the flipping (changing the value of $\mathbf{x}_i$ from $0$ to $1$, or $1$ to $0$)  probability $q(\mathbf{x}'_i=1-\mathbf{x}_i|\mathbf{x})$ as 
\begin{equation}
\label{eq:flip}
\texttt{Sigmoid}(\frac{1}{2}s(\mathbf{x})_i(1-2\mathbf{x}_i)-\frac{1}{2\alpha}).
\end{equation}
In particular, it can be shown that the discrete Langevin sampler is a first-order approximation to the locally-informed proposal \citep{zanella2017informed} in the following form.
\begin{equation}
\label{eq:local}
q(\mathbf{x}'_i|\mathbf{x})\propto \exp({\frac{1}{2}p(\mathbf{x}')-\frac{1}{2}p(\mathbf{x})-\frac{(\mathbf{x}_i'-\mathbf{x}_i)^2}{2\alpha})}.
\end{equation}
