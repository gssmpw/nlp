\section{Preliminary}\label{sec:prelim}
We introduce the notation and formal description of the streaming MABs model in this section. We provide more technical preliminaries in \Cref{sec:standatd-tech-tools}.

\subsection{Notation}
\label{subsec:notation}
Throughout, we use $n$ to denote the number of arms. We use $i$ to denote the indices of the arms, and we have the set of indices as \(I\) (which is a permutation of $[n]$). We let \(\mu_i\) be the mean of the $i$-th arm; furthermore, we denote the index of the best arm as \(\star:= \arg\,\max_{i \in I} \mu_i \). As such, the best arm is denoted as $\armstar$, and the mean of the best arm is \(\mu_{\star}\). The reward gap between the best and the \(i\)-th arm is equal to \(\Delta_i := \mu_{\star} - \mu_i\). We also use the ordered sequence of gaps \(\Delta_{[2]} \le \Delta_{[3]} \le \dotsc \le \Delta_{[n]}\), i.e., $\Delta_{[i]}$ is the reward gap between the best and the $i$-th \emph{best} arm.

We frequently deal with Bernoulli random variables. For convenience, we use $\bern{\mu}$ to denote a Bernoulli distribution with mean $\mu$ (i.e., the probability to sample $1$ is $\mu$). When random variables and their realizations are presented side by side, as a convention, we use upper cases (e.g., $\Pi$) to denote the random variables and lower cases (e.g., $\pi$) to denote the realizations.

\subsection{The Streaming Multi-armed Bandits Model}
\label{subsec:model}
We now formally introduce the streaming MABs model as follows. There is a collection of $n$ arms, denoted as $\{\arm_{i}\}_{i=1}^{n}$, and their reward distributions are characterized by $\{\bern{\mu_{i}}\}_{i=1}^{n}$\footnote{Our upper and lower bounds apply to all sub-gaussian distributions, see \Cref{rmk:general-sub-gaussian} for discussions}. As the name suggested, the arms arrive one after another in an \emph{arbitrary and fixed} order (a permutation over $[n]$). Here, an \emph{arbitrary} order means the arrival order of the arms is selected by an adversary and can be in the worst case, and a \emph{fixed} order means that the order of arrival for the arms is the same across different passes. 


A multi-pass streaming algorithm in the streaming MABs setting is defined as an algorithm that maintains a memory $M$, which is a set of arms, and a transcript $\pi$, which encodes the statistics of all past arm pulls. Each record in $\pi$ is a tuple that specifies the identity of the pulled arm, the result, and the pass index when the sample happened.

At any point, the streaming algorithm is allowed to make an arbitrary number of arm pulls on the \emph{arriving arm} and the arms \emph{stored in the memory}. The algorithm is allowed to make the following updates to the memory $M$:
\begin{enumerate}
	\item Adding the arriving arm to $M$.
	\item Discard the arriving arm, and continue to the next arriving arm.
	\item Discard arm(s) from the memory $M$.
\end{enumerate}
We define the \emph{sample complexity} as the number of arm pulls the streaming algorithm ever uses, and the \emph{space complexity (memory complexity)} as the maximum number of arms stored at any point (the maximum size of $M$). % \chen{Maybe we need to change the presentation for the following sentence} 
The common assumption in the literature \cite{AssadiW20,MaitiPK21,JinH0X21,AgarwalKP22,AWneurips22} allows the algorithm to write an arbitrary number of statistics for free, i.e., do not charge costs for the size of $\pi$ and any other stored information. 

\paragraph{The model with bounded statistics.} As we have mentioned, we provide an additional result to optimize the size of statistics in \Cref{sec:ub-stat-efficient}. To this end, we need to define the memory efficiency of the statistics. Let $\pitilde$ be a \emph{stored transcript} defined as follows: for each tuple that contains the arm pull and the result, the algorithm decides whether to write the tuple to $\pitilde$. In this setting, the algorithm is \emph{not} allowed to revisit all the past arm pulls, but only the ones that are stored. We define the memory complexity for the statistics as the maximum size of $\pitilde$ plus the maximum bits for the auxiliary information stored at any point.

% \chen{Talk about the setting -- we really don't need $O(n)$ bits of statistics in the ordinary setting}

\begin{remark}
\label{rmk:general-sub-gaussian}
We work with arms of Bernoulli distribution for both our upper and lower bounds for the convenience of presentation. We remark that our results apply to MABs with general (discrete) sub-Gaussian distributions. Concretely, we can assume w.log. that the supports are on $[0,1]$ by rescaling. For our upper bound result, the only place we used the property for Bernoulli distributions is when using the Chernoff-Hoeffding inequality (\Cref{lem:chernoff}), which holds for all sub-Gaussian distributions.
For the lower bound, we note that since the Bernoulli distribution does belong to the sub-Gaussian family, proving lower bounds on Bernoulli arms automatically implies lower bounds for sub-Gaussian arms.
\end{remark}
