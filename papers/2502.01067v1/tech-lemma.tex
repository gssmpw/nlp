% \section{Lower Bound Part I: Generalized Arm-trapping and Distribution Learning Lemmas in MABs}
\section{Technical Lemmas for the Lower Bound}
\label{sec:tech-lemma}
% \nicksays{define all notation like KL-divergence and total variation distance, also we need to introduce \(\smp\)}
% \chen{re-write this paragraph -- you did No only give the baseline hardness problems here, you also showed the batched instances and the technical tools provided by the previous paper.}
In this section, we present several technical lemmas en route to our main lower bound result. In particular, we show the following results in this section.
%that establish the hardness of some ``baseline'' problems, i.e. our main complexity results are obtained by embedding instances of more complicated problems into the baselines.

\begin{enumerate}[label=\alph*).]
	\item A lower bound on the necessary number of arm pulls for an algorithm with sublinear memory to \emph{store} an arm with high mean reward (while possibly without knowing the identity of the arm).
	\item A lower bound on the necessary number of arm pulls for an algorithm to gain ``knowledge'' about the underlying \emph{distribution} of the MABs instance. 
	\item An observation of the framework from \cite{AW23BestArm} as a general sample-memory-pass trade-off for batched instances.
\end{enumerate}

We note that a variant of the first two lower bounds on instances with a single arm with high mean reward is first proved in \cite{AW23BestArm}. However, the subtle difference in the construction (as it will be evident in \Cref{sec:lb-main}) requires lower bounds to work with \emph{two} arms with high mean rewards. This, in particular, requires a careful handling of properties on Double-armed Bandits (DABs), which we prove in \Cref{lem:arm-identify} and \Cref{lem:arm-learn}.

\paragraph{Additional notation.} We introduce several additional notation used in a self-contained manner in the lower bound proof. Unless specified otherwise, we use $\ALG$
to denote a streaming algorithm, and $\smp$ is the random variable for the sample complexity of $\ALG$. As we introduced in \Cref{sec:standatd-tech-tools}, for two random variables $X$ and $Y$, we use $\tvd{X}{Y}$ to denote their total variation distance, $\kl{X}{Y}$ for the KL-divergence, and $\II(X;Y)$ for the mutual information. We also use $X\mid Y=y$ to denote the random variable for $X$ \emph{conditioning} on the realization of $Y=y$. Finally, for the conciseness of notation, we slightly abuse the notation to use $\kl{X|Z}{Y|Z}$ as a short-hand notation for $\Exp_{z \sim Z} \kl{X \mid Z=z}{Y \mid Z=z}$.

\subsection{Lower Bounds on the Sample Complexity of Double-armed Bandits}
\label{subsec:two-arm-lb}

We start with proving the necessary number of arm pulls to distinguish an instance of \emph{two} arms that are $(i).$ either both with reward $1/2$ or $(ii).$ one arm with mean reward $1/2+\alpha$ and the other with $1/2+\alpha+\beta$. The problem is in the same spirit as the single-arm distinguishment problem in \cite{AWneurips22,AW23BestArm}, but we are unaware of any previous result for the exact version we are using. % As such, we include the lemma statements and the proof for completeness. 

Our first lemma shows that if an instance is sampled from a two-arm version of ``good arms'' and ``bad arms'', the algorithm will not be able to distinguish the cases if the number of arm pulls is small.

% \nicksays{introduce KL-deverigence before the use }

\begin{lemma}
	\label{lem:arm-identify}
	% \chen{to-do: Change the notation of conditional KL-divergence in the chain rule.}
	Consider two arms with a Bernoulli reward distribution whose mean is parameterized as follows.
	\begin{itemize}
		\item With probability $\rho$, the \emph{Yes case}, where
		\begin{enumerate}[label=\roman*).]
			\item $\arm_{1}$ is with reward $\frac{1}{2}+\alpha$;
			\item $\arm_{2}$ is with reward $\frac{1}{2}+\alpha+\beta$.
		\end{enumerate}
		\item With probability $1-\rho$, the \emph{No case}, where both $\arm_{1}$ and $\arm_{2}$ are with mean rewards of $\frac{1}{2}$;
	\end{itemize}
	where $\rho\in (0,\frac{1}{2}]$ is the probability for the reward to be more than $\frac{1}{2}$, and $\alpha, \beta >0$ satisfy $\alpha+\beta<\frac{1}{2}$. Any algorithm to determine the reward of the arms with a success probability of at least $(1-\rho+\eps)$ has to use $\frac{1}{4}\cdot \frac{\eps^2}{\rho^2 (\alpha+\beta)^{2}}$ arm pulls.
\end{lemma}
\begin{proof}
	We define $\Xyes=(\Xyes^{1}, \Xyes^{2}, \cdots, \Xyes^{m})$ as the random variable for taking $m$ samples from the Yes case. 
	Similarly, we define $\Xno=(\Xno^{1}, \Xno^{2}, \cdots, \Xno^{m})$ as the random variable for taking $m$ samples from the No case. 
	Furthermore, we also define random variables for ``dummy'' arm pulls: we define $\Xhigh$ as the random variable for taking a sample on an arm with reward $\frac{1}{2}+\alpha+\beta$, and $\Xflat$ as the random variable for taking samples on an arm with reward $\frac{1}{2}$.
	We first observe that for any $i\in [m]$, there is 
	\begin{align*}
		\kl{\Xyes^{i}}{\Xno^{i}}\leq \kl{\Xhigh^{i}}{\Xflat^{i}} =  \kl{\bern{\frac{1}{2}+\alpha+\beta}}{\bern{\frac{1}{2}}}.
	\end{align*}
	To see this, note that the algorithm is allowed to take a sample from either of the arms; however, the case to maximize the KL-divergence is for the algorithm to compare the empirical rewards from a $\bern{\frac{1}{2}+\alpha+\beta}$ arm and a $\bern{\frac{1}{2}}$ arm, which establishes the upper bound.
	
	We can in fact extend the above observation to \emph{conditional} KL-divergence. In particular, we have
	
	\begin{claim}
		\label{clm:cross-trial-ub}
		For any $i\in [m]$, there is
		\begin{align*}
			\kl{\Xyes^{i}\mid (\Xyes^{i+1},\cdots, \Xyes^{m})}{\Xno^{i}\mid (\Xno^{i+1},\cdots, \Xno^{m})} \leq \kl{\Xhigh}{\Xflat}.
		\end{align*}
	\end{claim}
	\begin{proof}
		Intuitively, the dependence between the results of arm pulls is only on the \emph{choice} of arms; once an arm is picked, the results are independent across different arm pulls. Our proof is a formalization of the above intuition. Define $\Xyes^{i, \arm_j}$ and $\Xno^{i, \arm_j}$ as the random variable for the algorithm to pull the $\arm_{j}$ ($j\in \{1,2\}$) on the $i$-th trial under the Yes and No cases, respectively. Furthermore, define $J$ as the random variable for the choice of arm by the algorithm. Note that we have $\Xyes^{i, \arm_j} = \Xyes^{i}\mid J=j$. % \nicksays{add brackets?}. 
		For any $i\in [m]$ and $j\in\{1,2\}$, there is
		\begin{align*}
			& \kl{\Xyes^{i}\mid (\Xyes^{i+1},\cdots, \Xyes^{m})}{\Xno^{i}\mid (\Xno^{i+1},\cdots, \Xno^{m})} \\
			& \leq \kl{\Xyes^{i}\mid (\Xyes^{i+1},\cdots, \Xyes^{m}, J)}{\Xno^{i}\mid (\Xno^{i+1},\cdots, \Xno^{m}, J)}  \tag{extra conditioning can only increase KL-divergence}\\
			& = \kl{\Xyes^{i}\mid J}{\Xno^{i}\mid J} \tag{indenpendence between sampling from bernoulli distributions}\\
			& \leq \kl{\Xhigh}{\Xflat},
		\end{align*}
		as desired. \myqed{\Cref{clm:cross-trial-ub}}
	\end{proof}
	
	%\chen{Write the proof of the above}
	We now use \Cref{clm:cross-trial-ub} to prove \Cref{lem:arm-identify}. By the standard calculation of the KL-divergence of Bernoulli random variables (\Cref{clm:bernoulli-KL}), we accordingly have
	%\begin{align*}
	%\kl{\Xhigh}{\Xflat} & = \left(\frac{1}{2}+\alpha+\beta\right)\cdot \log({1+2\alpha+2\beta})+\left(\frac{1}{2}-\alpha-\beta\right)\cdot \log{(1-2\alpha-2\beta)}\\
	%& = \frac{1}{2}\cdot \log\paren{1-4(\alpha+\beta)^2} + (\alpha+\beta)\cdot \log\left({\frac{1+2\alpha+2\beta}{1-2\alpha-2\beta}}\right)\\
	%& \leq (\alpha+\beta)\cdot \log\left({\frac{1+2\alpha+2\beta}{1-2\alpha-2\beta}}\right) \tag{$\log(1-4(\alpha+\beta)^2)<0$}\\
	%&\leq (\alpha+\beta)\cdot \log (2^{8(\alpha+\beta)}) \tag{$\frac{1+x}{1-x}\leq 2^{4x}$ for any $0<x<1/2$}\\
	%& = 8 \cdot (\alpha+\beta)^2
	%\end{align*}
	\begin{align*}
		\kl{\Xhigh}{\Xflat} \leq 8 \cdot (\alpha+\beta)^2
	\end{align*}
	for any sample index of $i$. As such, we can bound the KL-divergence of the distributions with all samples as follows.
	\begin{align*}
		\kl{\Xyes}{\Xno} &= \sum_{i=1}^{m} \kl{\Xyes^{i}\mid (\Xyes^{i+1},\cdots, \Xyes^{m})}{\Xno^{i}\mid (\Xno^{i+1},\cdots, \Xno^{m})} \tag{by Chain rule}\\
		&\leq \sum_{i=1}^{m} \kl{\Xhigh}{\Xflat} \tag{by \Cref{clm:cross-trial-ub}}\\
		&= 8m\cdot (\alpha+\beta)^2.
	\end{align*}
	
	Therefore, by Pinsker's inequality \Cref{fact:pinsker}, we have % \chen{right-pointer} % \nicksays{don't forget to introduce the total variation distance }, we have
	\begin{align*}
		\tvd{\Xyes}{\Xno} & \leq \sqrt{\frac{1}{2}\cdot \kl{\Xyes}{\Xno}}\\
		& \leq 2 (\alpha+\beta) \cdot \sqrt{m}.
	\end{align*}
	On the other hand, by \Cref{fact:distinguish-tvd}, we know that to distinguish the cases by a sample from the distribution with probability at least $1-\rho-\eps$, there has to be $\tvd{\Xyes}{\Xno}\geq \frac{\eps}{\rho}$. As such, we get a lower bound of
	\begin{align*}
		m \geq \frac{1}{4}\cdot \frac{\eps^2}{\rho^2 (\alpha+\beta)^2},	
	\end{align*}
	as desired.
\end{proof}


We now move to the second result for double-armed bandits, which shows that if the number of arm pulls is small, then the ``knowledge'' % \nicksays{are you sure in this sentence?} 
of the algorithm cannot change the original distribution by too much. More formally, we prove that with a limited number of arm pulls, from the algorithm's perspective, the probability for which case the instance is from remains close to the original distribution.
\begin{lemma}
	\label{lem:arm-learn}
	Let $\alpha, \beta \in (0,\frac16)$, $\beta\leq \alpha$, and $\rho \in (0,\frac12)$. Sample $\Theta$ from $\set{0,1}$ such that $\Theta=1$ with probability $\rho$.
	Consider two arms with Bernoulli reward distributions from the following family:
	\begin{itemize}
		\item If $\Theta=1$, the \emph{Yes} case, where 
		\begin{enumerate}
			\item $\arm_1$ is with mean reward $\frac{1}{2}+\alpha$;
			\item $\arm_2$ is with mean reward $\frac{1}{2}+\alpha+\beta$.
		\end{enumerate}
		\item If $\Theta=0$, the mean rewards of $\arm_1$ and $\arm_2$ are both $\frac{1}{2}$.
	\end{itemize}
	Let $\ALG$ be an algorithm that uses at most $m=\frac{1}{16}\cdot \frac{\eps^3}{\rho \cdot (\alpha+\beta)^{2}}$ arm pulls on an instance $I$ sampled from the family. Let $\pi$ be the transcript of $\ALG$ that records the arm pulls and the results, and let $\Pi$ be the random variable of $\pi$. Then, with probability at least $1-\eps$ over the randomness of transcript $\Pi$, there is
	\begin{align*}
		& \Pr\paren{\Theta=1 \mid \Pi=\pi} \in [\rho -  \eps,  \rho +  \eps]\\
		& \Pr\paren{\Theta=0 \mid \Pi=\pi} \in [1-\rho - \eps,  1- \rho + \eps]
	\end{align*}
\end{lemma}

\begin{proof}
	We prove the lemma by an information-theoretic argument similar to the analysis in \cite{AW23BestArm}, albeit we need to handle the dependence between arm pulls in our case. For an $m$-trial process, let $\Pi=(\Pi^{1}, \Pi^{2}, \cdots, \Pi^{m})$, where $\Pi^{i}$ is the random variable for the transcript of the $i$-th arm pull. Therefore, we can bound the mutual information between $\Theta$ and $\Pi$ as follows. % \chen{wrong way to write the mutual information as KL-divergence $\II(\Theta; \Pi) = \expectR{\theta \in \{0,1\}}{\kl{\Pi\mid \Theta=\theta}{\Pi}}$ not necessarily equalt to $\expectR{\theta \in \{0,1\}}{\kl{\Pi}{\Pi\mid \Theta=\theta}}$}
	\begin{align*}
		\II(\Theta; \Pi) &= \expectR{\theta \in \{0,1\}}{\kl{\Pi\mid \Theta=\theta}{\Pi}} \tag{KL-divergence view of mutual information}\\
		&= \expectR{\theta \in \{0,1\}}{\kl{(\Pi^{1}, \Pi^{2}, \cdots, \Pi^{m})\mid \Theta=\theta}{(\Pi^{1}, \Pi^{2}, \cdots, \Pi^{m})}}\\
		&= \expectR{\theta \in \{0,1\}}{\sum_{i=1}^{m} \kl{\Pi^{i} \mid (\Pi^{i+1}, \cdots,\Pi^{m}, \Theta=\theta)}{\Pi^{i} \mid (\Pi^{i+1}, \cdots,\Pi^{m})}} \tag{by chain rule of KL divergence}.
	\end{align*}
	We now argue that each of the KL-divergence terms in the expectation can be upper-bounded by substituting the transcript with the pull on $\arm_{2}$.
	\begin{claim}
		\label{clm:script-trial-ub}
		Let $\Pi^{i, \arm_2}$ be the random variable for the transcript induced by pulling $\arm_2$ on step $i$. For any $i\in [m]$ and $\theta \in \{0,1\}$, there is
		\begin{align*}
			& \kl{\Pi^{i} \mid \Pi^{i+1}, \cdots,\Pi^{m}, \Theta=\theta}{\Pi^{i} \mid \Pi^{i+1}, \cdots,\Pi^{m}} \\
			& \leq \rho \cdot \kl{\Pi^{i, \arm_1}\mid \Theta = \theta}{\Pi^{i, \arm_1}} + (1-\rho)\cdot \kl{\Pi^{i, \arm_2}\mid \Theta = \theta}{\Pi^{i, \arm_2}}.
		\end{align*}
	\end{claim}
	\begin{proof}
		The proof is similar to the one we showed in \Cref{clm:cross-trial-ub}. Concretely, let $J$ be the random variable for the choice of the arm to be pulled, and observe in the same manner as \Cref{clm:cross-trial-ub} that conditioning on the choice of $J$, the transcript between different $i$ indices are \emph{independent}. As such, For any $i\in [m]$ and $\theta\in\{0,1\}$, there is
		\begin{align*}
			& \kl{\Pi^{i} \mid \Pi^{i+1}, \cdots,\Pi^{m}, \Theta=\theta}{\Pi^{i} \mid \Pi^{i+1}, \cdots,\Pi^{m}} \\
			& \leq \kl{\Pi^{i} \mid \Pi^{i+1}, \cdots,\Pi^{m}, \Theta=\theta, J}{\Pi^{i} \mid \Pi^{i+1}, \cdots,\Pi^{m}, J}  \tag{extra conditioning can only increase KL-divergence}\\
			&= \kl{\Pi^{i} \mid  \Theta=\theta, J}{\Pi^{i} \mid J}. \tag{$\Pi^{i}$ is independent of $\Pi^{\neq i}$ conditioning on the choice of $J$}
		\end{align*}
		For the first random variable, we have
		\begin{align*}
			& \paren{\Pi^{i} \mid \theta=0, J=1} = \bern{1/2} \qquad \paren{\Pi^{i} \mid \theta=0, J=2} = \bern{1/2}; \\
			& \paren{\Pi^{i} \mid \theta=1, J=1} = \bern{1/2+\alpha} \qquad \paren{\Pi^{i} \mid \theta=1, J=2} = \bern{1/2+\alpha+\beta}.
		\end{align*}
		On the other hand, for the second random variable, there is
		\begin{align*}
			\paren{\Pi^{i} \mid J=1} = \bern{\frac{1}{2}+\rho \cdot \alpha}; \qquad \paren{\Pi^{i} \mid J=2} = \bern{\frac{1}{2}+\rho \cdot (\alpha + \beta)}.
		\end{align*}
		By the above calculation, the KL-divergences are maximized with $J=2$ for the $\Theta=0$ case and $J=1$ for $\Theta=1$ case. As such, we have
		\begin{align*}
			& \kl{\Pi^{i} \mid \Pi^{i+1}, \cdots,\Pi^{m}, \Theta=\theta}{(\Pi^{i} \mid \Pi^{i+1}, \cdots,\Pi^{m}} \\
			& \leq \kl{\Pi^{i} \mid J, \Theta=\theta}{\Pi^{i} \mid J}\\
			&= \expectR{j\in \{1,2\}}{\kl{\Pi^{i} \mid \Theta=\theta, J=j}{\Pi^{i} \mid J=j}}\\
			& \leq \rho \cdot \kl{\Pi^{i} \mid J=1}{\Pi^{i} \mid \Theta=\theta, J=1} + (1-\rho) \cdot \kl{\Pi^{i} \mid J=2}{\Pi^{i} \mid \Theta=\theta, J=2} \\
			&= \rho \cdot \kl{\Pi^{i, \arm_1}\mid \Theta = \theta}{\Pi^{i, \arm_1}} + (1-\rho)\cdot \kl{\Pi^{i, \arm_2}\mid \Theta = \theta}{\Pi^{i, \arm_2}},
		\end{align*}
		as desired. \myqed{\Cref{clm:script-trial-ub}}
	\end{proof}
	By \Cref{clm:script-trial-ub}, we can upper bound the mutual information between $\Theta$ and $\Pi$ as % \nicksays{I don't get how we get \(\rho \alpha\) and \(\rho (\alpha + \beta)\) in the proof, can you explain it? }
	\begin{align*}
		\qquad & \II(\Theta; \Pi) \\
		&\leq \expectR{\theta \in \{0,1\}}{\sum_{i=1}^{m} \kl{\Pi^{i, \arm_2}}{\Pi^{i, \arm_2} \mid \Theta = \theta}}\\
		&= \sum_{i=1}^{m} \rho \cdot \kl{\bern{\frac{1}{2}+\alpha}}{\bern{\frac{1}{2}+\rho\cdot \alpha}} + (1-\rho)\cdot \kl{\bern{\frac{1}{2}}}{\bern{\frac{1}{2}+\rho\cdot (\alpha+\beta)}}\\
		&\leq 8m\cdot \paren{\rho\cdot (\rho-1)^2\cdot \alpha^2 + (1-\rho) \cdot \rho^2 \cdot (\alpha+\beta)^2} \tag{by \Cref{clm:bernoulli-KL}}\\
		&\leq 16 m\cdot \rho \cdot (\alpha+\beta)^2 \tag{by $(\rho-1)^2\leq \rho^2$ since $\rho\leq \frac{1}{2}$}.
	\end{align*}
	By plugging in the condition that $m\leq \frac{1}{16} \cdot \frac{\eps^3}{\rho (\alpha+\beta)^2}$, we have $\II(\Theta; \Pi)\leq \eps^3$. We now use another KL-divergence form of the mutual information to get
	\begin{align*}
		\II(\Theta; \Pi) = \expectR{\pi\sim \Pi}{\kl{\Theta}{\Theta\mid \Pi=\pi}} \leq \eps^3.
	\end{align*}
	As such, with probability at least $1-\eps$ over the randomness of $\Pi$, we have
	\begin{align*}
		\kl{\Theta}{\Theta\mid \Pi=\pi}\leq \frac{1}{\eps} \cdot  \expectR{\pi\sim \Pi}{\kl{\Theta}{\Theta\mid \Pi=\pi}} \leq \eps^2.
	\end{align*}
	We condition on the high probability transcripts for the rest of the calculations. Now, we can apply Pinsker's inequality (\Cref{fact:pinsker}) to get % \nicksays{add brackets around \(\Theta \mid \Pi = \pi)\) ? }
	\begin{align*}
		\tvd{\Theta}{\Theta\mid \Pi=\pi} \leq \sqrt{\kl{\Theta}{\Theta\mid \Pi=\pi}} \leq \eps.
	\end{align*}
	By \Cref{fact:distinguish-tvd}, we get the desired upper bound of the ``advantage'', i.e. 
	\begin{align*}
		& \card{\Pr\paren{\Theta=0\mid \Pi=\pi}-\Pr\paren{\Theta=0}}\leq \eps\\
		& \card{\Pr\paren{\Theta=1\mid \Pi=\pi}-\Pr\paren{\Theta=1}}\leq \eps,
	\end{align*} % \nicksays{identical lines}
	which implies the desired lemma statement.
\end{proof}

\subsection{Lower bounds on the Sample Complexity of MABs Trapping and Learning}
\label{subsec:batch-arm-hardness}
We now show how we `amplify' the result for the double-armed bandits to a collection of $k$ arms with two \emph{special} arms. These results are similar both in spirit and in technicality to existing multi-pass lower bounds \cite{AWneurips22,AW23BestArm}, and we include the proofs for completeness.
% \chen{Jul/30 Note: I stopped here -- try to fill up Prop. 7 for the immediate next step.}

\begin{lemma}
	\label{lem:arm-trapping}
	Let $k\geq 3$ be an integer and $\alpha, \beta >0$ such that $\alpha+\beta<\frac{1}{6}$, suppose there is a family of $k$ arms in which
	\begin{itemize}
		\item two indices $\istar, \jstar \in [k]$ chosen uniformly at random (without replacement), and their mean rewards are $\mu_{\istar}=\frac{1}{2}+\alpha$ and $\mu_{\jstar}=\frac{1}{2}+\alpha+\beta$.
		\item for all $i \in [k]\setminus \{\istar, \jstar\}$, their mean rewards are $\mu_{i}=\frac{1}{2}$.
	\end{itemize}
	Then, for any given parameter $\tau\in (0, \frac{1}{2}]$, any algorithm that outputs $\frac{\tau\cdot k}{40}$ arms that contains any arm with reward \emph{strictly more than} $\frac{1}{2}$ with probability at least $\tau$ requires $\frac{1}{600}\cdot \frac{\tau^3}{(\alpha+\beta)^2}\cdot k$ arm pulls.
\end{lemma}
% \chen{Prove for general $\rho$? Maybe not necessary.}
\begin{proof}
	\FloatBarrier
	The proof uses the ``direct-sum'' argument in a similar manner of \cite{AWneurips22} and \cite{AW23BestArm}. Concretely, we provide a reduction from the problem in \Cref{lem:arm-identify}, and show that an algorithm that satisfies the prescribed property in \Cref{lem:arm-trapping} with $s$ samples would imply an algorithm that identifies an arm with $O(s/k)$ samples, which eventually leads to a contradiction with \Cref{lem:arm-identify} for $\rho=1/2$. The formal reduction is as \Cref{red:arm-trapping}.
	\begin{algorithm}[!h]
		\caption{A reduction algorithm to prove \Cref{lem:arm-trapping}}\label{red:arm-trapping} % \nicksays{combine inputs?}
		\KwIn{Two arms $\arm_{1}$ and $\arm_{2}$ from the distribution of \Cref{lem:arm-identify} with $\rho=\frac{1}{2}$.}
		\KwIn{An algorithm $\ALG$ that $a).$ uses at most $\frac{1}{600}\cdot \frac{\tau^3}{(\alpha+\beta)^2}\cdot k$ arm pulls, $b).$ outputs a collection $S$ of $\frac{\tau\cdot k}{20}$, and $c).$ $S$ contains an arm with reward \emph{strictly more than} $\frac{1}{2}$ with probability at least $\tau$.}
		\KwOut{The decision (Yes or No cases) from which $\arm_{1}$ and $\arm_{2}$ are sampled.}
		Sample a coin $\Theta \sim \bern{\frac{1}{2}+\frac{11}{38}\cdot \tau}$\;
		\If{$\Theta=0$}{
			Directly output ``$\arm_1$ is from $\bern{1/2+\alpha}$ and $\arm_2$ is from $\bern{1/2+\alpha+\beta}$''.\;
		}
		\Else{
			Construct an instance $I$: sample two indices $\istar$, $\jstar$ uniformly at random (without replacement), and set the arms with indices $\istar$, $\jstar$ as $\arm_{1}$ and $\arm_{2}$\;
			For all indices $i \in [k]\setminus \{\istar, \jstar\}$, create $k-2$ dummy arms $\bern{1/2}$\;
			Run $\ALG$ on instance $I$, and output with the following rules: \;
			\If{$\arm_1$ or $\arm_2$ uses more than $\frac{1}{30}\cdot \frac{\tau^2}{(\alpha+\beta)^2}$ arm pulls}{
				\label{line:sample-ub-term} Terminate $\ALG$ and output ``$\arm_1$ is from $\bern{1/2+\alpha}$ and $\arm_2$ is from $\bern{1/2+\alpha+\beta}$''. \;
			}
			\ElseIf{$S$ cotains any of $\{\istar, \jstar\}$}{
				\label{line:false-trap} Output ``$\arm_1$ is from $\bern{1/2+\alpha}$ and $\arm_2$ is from $\bern{1/2+\alpha+\beta}$''\;
			}
			\Else{
				Output ``$\arm_1$ and $\arm_2$ are from $\bern{1/2}$''\;
			}
		}
	\end{algorithm}
	
	We first observe that \Cref{red:arm-trapping} never uses more than $\frac{1}{30}\cdot \frac{\tau^2}{(\alpha+\beta)^2}$ arm pulls, as there is a forced termination once this condition happens. We now need to analyze the correctness of the algorithm for \Cref{lem:arm-identify}. Note that we fix $\rho$ in \Cref{lem:arm-identify} to be $\rho=\frac{1}{2}$. We claim that the algorithm correctly identifies the cases with probability at least $\frac12+\frac{\tau}{5}$, and the analysis considers two cases, respectively.
	\begin{enumerate}[label=\roman*).]
		\item $\arm_1$ is $\bern{1/2+\alpha}$ and $\arm_2$ is $\bern{1/2+\alpha+\beta}$. In this case, with probability $\frac{1}{2}-\frac{11}{38}\cdot \tau$, \Cref{red:arm-trapping} directly return the correct answer. On the other hand, if \Cref{red:arm-trapping} runs $\ALG$ on the instance $I$, it will return the correct answer as long as $\ALG$ succeeds, which is with probability at least $\tau$. As such, the correct probability \emph{conditioning} on the ``Yes'' case of \Cref{lem:arm-identify} is at least
		\begin{align*}
			\frac{1}{2}-\frac{11}{38}\cdot \tau + \left(\frac{1}{2}+\frac{11}{38}\cdot \tau\right)\cdot \tau \geq \frac{1}{2} +\left(\frac{1}{2}-\frac{11}{38}\right)\cdot \tau \geq \frac{1}{2}+ \frac{\tau}{5}.
		\end{align*}  
		\item Both $\arm_1$ and $\arm_2$ are $\bern{1/2}$. We show that if \Cref{red:arm-trapping} runs $\ALG$ on the instance $I$, the correct probability is sufficiently high. To this end, we bound the failure probability for \Cref{red:arm-trapping} to not report this case (conditioning on $\Theta=0$). Let $s_{(\ell)}$ be the random variables for the number of arm pulls used by the arm on index $\ell$. Furthermore, let us use $s_1$ and $s_2$ to denote the random variables for the number of samples used by $\arm_1$ and $\arm_2$. Let $\mathcal{E}_{\text{No}}$ be the event that $\arm_1$ and $\arm_2$ are sampled from the ``No'' case of \Cref{lem:arm-identify} (both $\bern{1/2}$). We have
		\begin{align*}
			\expect{s_1\mid \mathcal{E}_{\text{No}}} &= \expect{s_2\mid \mathcal{E}_{\text{No}}} \tag{$s_1$ and $s_2$ are identical random variables}\\
			&= \sum_{\ell=1}^{k} \Pr(i^* = \ell)\cdot \expect{s_{(\ell)}\mid \mathcal{E}_{\text{No}}} \tag{all arms are identical random variables conditioning on $\mathcal{E}_{\text{No}}$}\\
			&= \frac{1}{k}\cdot \expect{\sum_{\ell} s_{(\ell)}\mid \mathcal{E}_{\text{No}}}\\
			&\leq \frac{1}{600}\cdot \frac{\tau^3}{(\alpha+\beta)^2}. \tag{bound on the number of arm pulls}
		\end{align*}
		Therefore, we have $\Pr\paren{s_1\geq \frac{1}{30} \cdot \frac{\tau^2}{(\alpha+\beta)^2}}\leq \frac{\tau}{20}$ by a simple Markov bound. Therefore, the probability for \Cref{line:sample-ub-term} to falsely output the ``Yes'' case is at most $\frac{\tau}{4}$. On the other hand, conditioning on $\mathcal{E}_{\text{No}}$, the arms become identical random variables. More formally, let $X_{\arm_1}$ and $X_{\arm_2}$ be the indicator random variables for $\arm_1$ and $\arm_2$ to be in $S$, and let $X_{(\ell)}$ % \nicksays{We use \(X\) so many times probably we can use another letter.}
		be the indicator random variables for the arm of index $\ell$ to be in $S$, we have
		\begin{align*}
			\Pr(X_{(\ell)}=1) = \Pr(X_{\arm_{1}}=1) = \Pr(X_{\arm_{2}} = 1) = \frac{\card{S}}{k}\leq \frac{\tau}{40}.
		\end{align*} % \nicksays{\(X_{(\ell)}\) ? }
		Therefore, by a union bound, the algorithm to contain \emph{any} of $\arm_{1}$ and $\arm_{2}$ in the ``No'' case is at most $\frac{1}{20}$.
		Now, we apply another union bound, and the failure probability conditioning on $\mathcal{E}_{\text{No}}$ and $\ALG$ is executed on $I$ is at most $\frac{\tau}{20}+\frac{\tau}{20}=\frac{\tau}{10}$. As such, the success probability given $\mathcal{E}_{\text{No}}$ is at least
		\begin{align*}
			(\frac{1}{2}+\frac{11}{38}\cdot \tau)\cdot (1-\frac{\tau}{10}) \geq \frac{1}{2}+\frac{\tau}{5}.
		\end{align*}
	\end{enumerate}
	By plugging in $\eps=\frac{\tau}{5}$ and $\rho=\frac{1}{2}$ to \Cref{lem:arm-identify}, we obtain the number of necessary arm pulls is at least $\frac{1}{25}\cdot \frac{\tau^2}{(\alpha+\beta)^2}$ arm pulls, which forms a contradiction with \Cref{red:arm-trapping}. Therefore, such an $\ALG$ cannot exist.
	
	\FloatBarrier
\end{proof}


\begin{lemma}
	\label{lem:batch-arm-learning}
	Let $k\geq 3$ be an integer, $\alpha, \beta >0$ such that $\alpha+\beta<\frac{1}{6}$, and $\rho\in (0, \frac{1}{2})$, suppose there is a family of $k$ arms in which
	\begin{itemize}
		\item with probability $\rho$, the \emph{Yes} % \nicksays{sometimes we use yse sometimes Yes, we need to unify this} 
		case, where all except \emph{two} arms chosen uniformly at random are with mean rewards $\frac{1}{2}$, and the two special arms are with mean rewards $\frac{1}{2}+\alpha$ and $\frac{1}{2}+\alpha+\beta$.
		\item with probability $1- \rho$, the \emph{No} case, where all the arms are with mean rewards $\frac{1}{2}$.
	\end{itemize}
	Then, for any given parameter $\tau\in (0, \frac{1}{5}]$, let $\ALG$ be any algorithm that given an instance $D$ from the distribution, uses at most $\frac{1}{200}\cdot \frac{\tau^2}{\rho \cdot (\alpha+\beta)^2}\cdot k$ arm pulls, and let $\Pi$ and $\pi$ be the random variable and the realization of the transcripts of $ALG$. With probability at least $1-2 \tau^{1/2}$ over the randomness of the transcript, there is
	\begin{align*}
		& \Pr\paren{\text{$D$ in \emph{Yes} case} \mid \Pi=\pi} \in [\rho-2 \tau^{1/2}, \rho + 2 \tau^{1/2}];\\
		& \Pr\paren{\text{$D$ in \emph{No} case} \mid \Pi=\pi} \in [1-\rho-2 \tau^{1/2}, 1-\rho + 2 \tau^{1/2}],
	\end{align*}
	where the randomness is over the choices of the instances.
\end{lemma}
\begin{proof}
	\FloatBarrier
	Similar to the proof of \Cref{lem:arm-trapping} (and as in \cite{AW23BestArm}), we prove the lemma by applying the ``direct sum'' argument with \Cref{lem:arm-learn}. To this end, we again assume for the purpose of contradiction that an algorithm that breaks the bound on \Cref{lem:batch-arm-learning} exists, and build an algorithm that is ruled out by \Cref{lem:arm-learn}. In the proof, we only focus on the upper bound of $\Pr\paren{\text{$D$ in \emph{Yes} case} \mid \Pi=\pi}$ as the lower bound follows from the same logic.
	
	\begin{algorithm}[!h]
		\caption{A reduction algorithm to prove \Cref{lem:batch-arm-learning}}\label{red:batch-arm-learn}
		\KwIn{Two arms $\arm_{1}$ and $\arm_{2}$ from the distribution of \Cref{lem:arm-learn}.}
		\KwIn{An algorithm $\ALG$ that $a).$ uses at most $\frac{1}{200}\cdot \frac{\tau^2}{\rho\cdot (\alpha+\beta)^2}\cdot k$ arm pulls, $b).$ with probability more than $2\tau^{1/2}$ produce a transcript $\pi$, such that $\Pr\paren{\text{$D$ in \emph{Yes} case} \mid \Pi=\pi}> \rho + 2 \tau^{1/2}$.}
		% \nicksays{join input?}
		\KwOut{A (conditional) probability distribution of instance $D$.}
		Construct an instance $\tilde{D}$: sample two indices $\istar$, $\jstar$ uniformly at random (without replacement), and set the arms with indices $\istar$, $\jstar$ as $\arm_{1}$ and $\arm_{2}$\;
		For all indices $i \in [k]\setminus \{\istar, \jstar\}$, create $k-2$ dummy arms $\bern{1/2}$\;
		Run $\ALG$ on instance $\tilde{D}$, and output with the following rules: \;
		\If{$\arm_1$ or $\arm_2$ uses more than $\frac{1}{5}\cdot \frac{\tau^{3/2}}{\rho \cdot(\alpha+\beta)^2}$ arm pulls}{
			\label{line:sample-ub-knowledge} Terminate $\ALG$ and output ``Yes case''. \;
		}
		\Else{$S$ contains any of $\{\istar, \jstar\}$}{
			\label{line:follow-knowledge} Output the distribution of the ``Yes'' and ``No'' cases of $\tilde{D}$ (which is a distribution generated by $\ALG$) as the distribution of the ``Yes'' and ``No'' cases of the problem in \Cref{lem:arm-learn}\;
		}
	\end{algorithm}
	
	The formal description of the reduction algorithm is as in \Cref{red:batch-arm-learn}. It is straightforward to observe that \Cref{red:batch-arm-learn} uses at most $\frac{1}{5}\cdot \frac{\tau^{3/2}}{\rho \cdot(\alpha+\beta)^2}$ arm pulls, as we terminate and output in \Cref{line:sample-ub-knowledge} otherwise. We now show that \Cref{red:batch-arm-learn} correctly ``learns'' the distribution of the arms with probability at least $2\tau^{1/2}$. To this end, we conduct the following case-based analysis:
	\begin{enumerate}[label=\roman*).]
		\item If the algorithm enters \Cref{line:sample-ub-knowledge}: we show that the algorithm reports ``Yes case'' correctly with probability at least $2\gamma^{1/2}$, which implies $\Pr\paren{\text{$D$ in \emph{Yes} case} \mid \Pi=\pi}=1\geq \rho + 2 \tau^{1/2}$. To see the desired statement, note that in the ``No case'', every arm becomes identical random variables. As such, similar to the proof of \Cref{lem:arm-trapping}, we can define $s_1$ and $s_2$ as the number of arm pulls used on $\arm_1$ and $\arm_2$, and show that
		\begin{align*}
			\expect{s_1 \mid \text{No case}} =\expect{s_2 \mid \text{No case}} \leq \frac{1}{200}\cdot \frac{\tau^2}{\rho\cdot (\alpha+\beta)^2}.
		\end{align*}
		As such, we have
		\begin{align*}
			\Pr\paren{s_1\geq \frac{1}{5}\cdot \frac{\tau^{3/2}}{\rho\cdot (\alpha+\beta)^2}} \leq \frac{\tau^{1/2}}{20}  \qquad \Pr\paren{s_2\geq \frac{1}{5}\cdot \frac{\tau^{3/2}}{\rho\cdot (\alpha+\beta)^2}} \leq \frac{\tau^{1/2}}{20}.
		\end{align*}
		Therefore, the probability for a transcript $\pi$ such that $\Pr\paren{\text{$D$ in \emph{Yes} case} \mid \Pi=\pi}=1$ is at least $1-\frac{\tau^{1/2}}{10}\geq 2\tau^{1/2}$ by the choice of $\tau\leq \frac{1}{5}$. 
		\item If the algorithm enters \Cref{line:follow-knowledge}, then by the guarantee of $\ALG$, we have that
		\begin{align*}
			\Pr_{\Pi}\paren{\Pr\paren{\text{$D$ in \emph{Yes} case} \mid \Pi=\pi}>\rho+2\tau^{1/2}} > 2\tau^{1/2}.
		\end{align*}
	\end{enumerate}
	Note that by using \Cref{lem:arm-learn} with $\eps=2\tau^{1/2}$, for the after mentioned bound to hold, at least $\frac{1}{2}\cdot \frac{\tau^{3/2}}{\rho\cdot (\alpha+\beta)^2}$ arm pulls are necessary. As such, it forms a contradiction with \Cref{red:batch-arm-learn}, which means such $\ALG$ cannot exist.
	
	\FloatBarrier
\end{proof}


\subsection{A Lower Bound Framework on Batched Distributions}

In our lower bound proof, we will crucially use a recent multi-pass lower bound tool developed by \cite{AW23BestArm}. The original lower bound construction of \cite{AW23BestArm} is on \emph{batched} instances distributions. On a high level, these distributions divide the arm into multiple batches with a fixed order. Inside each batch, most of the arms are ``flat'', i.e., with mean reward $\frac{1}{2}$, and one (or a few) \emph{special} arm(s) are planted uniformly at random among the indices. The reward distribution of the special arms is chosen randomly and independently between the reward of $\frac{1}{2}$ and $>\frac{1}{2}$. To make the instance hard, the distributions usually put batches whose special arm \emph{might} possess higher rewards to the late part of the stream. The intuition here is that to make sure the sample complexity upper bound is always followed, the streaming algorithm has to ``eliminate'' batches one by one in the reversed order of the stream.

The original analysis of \cite{AW23BestArm} was presented with only one specific distribution. In this section, we observe that their construction works for general batched distributions as long as they satisfy some properties. To this end, we formally define the \emph{batched} instances distributions.

\begin{definition}[Batched instance distributions]
	\label{def:batch-instance}
	Suppose the following information is given:  
	\begin{enumerate}[label=\roman*).]
		\item Positive integers $B\geq 2$, $C\geq 1$, $S \geq 1$; 
		\item A set of functions $F=\{f_{b}: \mathbb{N}^{+}\rightarrow (0,1)\}_{b=1}^{B+1}$ that computes $f_{b}(B)$ as a probability; 
		\item A set of tuples of positive real numbers $\mathrm{H} = \{(\etaib{1}{b}, \etaib{2}{b}, \cdots, \etaib{S}{b})\}_{b=1}^{B+1}$ from $(0, \frac12)$, and the values are (potentially) functions of $C$.
	\end{enumerate}
	We say instance distribution $\cD(B,C,F, \mathrm{H})$ is a $(B+1)$-\emph{batched instance distribution} if it satisfies the following properties:
	\begin{enumerate}[label=\alph*).]
		\item The arms are divided into $(B+1)$ batches;
		\item Inside each batch $b$, sample $S$ arms uniformly at random, and call them the \emph{special arms};
		\item All arms that are \emph{not} among the special arms follow the reward distribution $\bern{\frac{1}{2}}$.
		\item Sample a coin $\Theta_{b}\in \{0, 1\}$ from the distribution $\bern{f_{b}(B)}$ for each batch $b\in [B+1]$ \emph{independently}:
		\begin{itemize}
			\item If $\Theta_{b}=0$, set the special arms of batch $b$ with distribution $\bern{\frac{1}{2}}$.
			\item If $\Theta_{b}=1$, set the special arms of batch $b$ with distributions $\bern{\frac{1}{2}+\etaib{1}{b}}, \bern{\frac{1}{2}+\etaib{2}{b}}, \cdots, \bern{\frac{1}{2}+\etaib{S}{b}}$ (following an arbitrarily fixed order).
		\end{itemize}
	\end{enumerate}
\end{definition}

An illustration of batched instance distributions can be found in \Cref{fig:batched-instance}. For any $(B+1)$-batched instance distribution, it follows from the analysis of \cite{AW23BestArm} that the following proposition holds.

\begin{proposition}[\cite{AW23BestArm}, rephrased]
	\label{prop:multi-pass-lb}
	For a batched instance distribution $\cD(B,C,F, \mathrm{H})$ under the streaming setting, let the batches be arranged in the \emph{reversed} order of the stream arrival, i.e., $\mathcal{B}_{B+1}$ arrives first, and $\mathcal{B}_{1}$ arrives the last. Let the set of functions in $F$ be satisfying: 
	\begin{align*}
		f_{b}(B)=
		\begin{cases}
			\frac{1}{2B},\quad b\leq B;\\
			1, \quad b=B+1.
		\end{cases}
	\end{align*}
	For every batch $b\in [B+1]$, suppose w.log. that $\etaib{1}{b}\geq \etaib{s}{b}$ for any $s\in [S]$. Additionally, suppose $\cD(B,C,F, \mathrm{H})$ satisfies the following properties:
	\begin{itemize}
		\item \textbf{C1:} For each batch $\mathcal{B}_{b}$, \emph{conditioning on} $\Theta_{b}=1$, if an \emph{offline} algorithm uses at most $\frac{1}{700}\cdot \frac{\tau^3}{(\etaib{1}{b})^2}\cdot \frac{n}{B+1}$ arm pulls and outputs a collection of $\frac{1}{20}\cdot \frac{n}{B+1}\cdot \tau$ arms from $\mathcal{B}_{b}$, the probability for the output to contain any arm with reward \emph{strictly more than} $\frac12$ is at most $\tau$. % \chen{lemma 5}
		\item \textbf{C2:} For each batch $\mathcal{B}_{b}$, let $\nu$ be the probability for $\Theta_{b}=1$ (possibly conditioning on the information the algorithm obtains). Suppose we additionally obtain a new transcript $\pi$ with at most $\frac{1}{200}\cdot \frac{\tau^2}{\rho \cdot \left(\etaib{1}{b}\right)^2}\cdot \frac{n}{B+1}$ arm pulls. Then, with probability at least $1-2\tau^{1/2}$ over the randomness of $\pi$, there is
		\begin{align*}
			& \Pr\paren{\Theta_{b}=1 \mid \Pi=\pi} \in [\nu-2 \tau^{1/2}, \nu + 2 \tau^{1/2}];\\
			& \Pr\paren{\Theta_{b}=0 \mid \Pi=\pi} \in [1-\nu-2 \tau^{1/2}, 1-\nu + 2 \tau^{1/2}],
		\end{align*}
		% \nicksays{identical probabilities} chen: fixed
		% \chen{lemma 6}
		\item \textbf{C3:} For any $b, p$ such that $p>b$, there is $\etaib{1}{p}\leq (\frac{1}{6C B})^{15}\cdot \etaib{1}{b}$.
	\end{itemize}
	Let $\ALG$ be a deterministic streaming algorithm that uses $P\leq B$ passes and a memory of at most $\frac{1}{30000}\cdot \frac{n}{B^3}$ arms. Additionally, suppose $\ALG$ satisfies
	\begin{equation}
		\label{equ:batch-sample-ub}
		\expect{\smp\mid \Theta_{b}=1, \Theta_{<b}=0}\leq C \cdot B^2 \cdot \frac{n}{\left(\etaib{1}{b}\right)^2}.
	\end{equation}
	% \nicksays{what is \(\smp\)} chen: introduced in the additional notation
	Then, the probability for $\ALG$ to return the best arm is strictly less than $\frac{999}{1000}$.
\end{proposition}

\Cref{prop:multi-pass-lb} summarizes all the necessary conditions used by \cite{AW23BestArm}: in the analysis of \cite{AW23BestArm}, conditions \textbf{C1} and \textbf{C2} are used for the analysis the small-size sample case, and condition \textbf{C3} is used in the large-size sample case. Also, in the statement, there are two minor differences between \Cref{prop:multi-pass-lb} and the original theorem statement in \cite{AW23BestArm}:
\begin{enumerate}
	\item In \cite{AW23BestArm}, the result is only stated with $P=B$, i.e., using the number of passes directly as the parameter $B$. Here, we use $P\leq B$ since we work with an upper bound of $P$ that is only dependent on $n$.
	\item In \cite{AW23BestArm}, \Cref{equ:batch-sample-ub} does not have the $B^2$ factor. Nevertheless, it is evident from their proofs that we can add a $B$ factor on the sample bound.
\end{enumerate}
% For more details, we refer the readers to \Cref{sec:multi-pass-lb-tool} for a proof sketch of \Cref{prop:multi-pass-lb}.
% \chen{Reminder: conditioning on the all previous passes are batch- and memory-oblivious, the probability for the new pass to be batch- and memory-oblivious is roughly $(1-1/B)^C$.\\
	% Reminder 2: In our settings, $B=\log{n}/\log\log{n}$}


