\section{Standard technical tools}
\label{sec:standatd-tech-tools}

\subsection{Concentration Inequalities}

We use the following variant of the well-known Chernoff-Hoeffding bound.

\begin{lemma}[Chernoff-Hoeffding Inequality]\label{lem:chernoff}
	Let \(X_1, \dotsc, X_n \in [0, 1]\) be independent random variables. Let \(X = \sum_{i = 1}^n X_i\). For any \(t \ge 0\), it holds that 
	\begin{align*}
		\Pr[X \ge \bE[X] + t] \le \exp\left(\frac{-2t^2}{n}\right)
	\end{align*}
	and
	\begin{align*}
		\Pr[X \le \bE[X] - t] \le \exp\left(\frac{-2t^2}{n}\right)\,.
	\end{align*}
	% Suppose \(X_1, X_2, \dotsc, X_n\) are independent sub-Gaussian random variables with mean 0 and sub-Gaussian norm \(\sigma\). Let \(S = X_1 + X_2 + \dotsc + X_n\). Then for any \(t > 0\), we have:
	% \( {\Pr}(S > t) \le \exp\left(-\frac{t^2}{2n\sigma^2}\right) \) and 
	% \( {\Pr}(S < -t) \le \exp\left(-\frac{t^2}{2n\sigma^2}\right) \).
\end{lemma}


% \chen{This version seems to only work for zero-mean random variables...}\nicksays{changed it}

\subsection{Statistical Distances and Properties}
\label{sub-app:stat-dist}
We frequently use the well-known total variation distance (TVD) and Kullback–Leibler divergence (KL divergence) in our proof. In this section, we provide their formal definition and properties.


\paragraph{Total variation distance.} 
We start with the definition of the total variation distance (TVD) between two distributions. 
\begin{definition}
	\label{def:tvd}
	Let $X$ and $Y$ be two random variables supported over the same $\Omega$, and let $\mu_X$ and $\mu_Y$ be their probability measures. The total variation distance (TVD) is between $X$ and $Y$ is defined as
	\begin{align*}
		\tvd{X}{Y} = \sup_{\Omega'\subseteq \Omega}\card{\mu_X(\Omega')-\mu_Y(\Omega')}.
	\end{align*}
	In particular, when the random variables are discrete, we have
	\begin{align*}
		\tvd{X}{Y} = \frac{1}{2}\sum_{\omega\in \Omega}\card{\mu_X(\omega)-\mu_Y(\omega)}.
	\end{align*}
\end{definition}

The total variation distance satisfied the symmetric property, i.e., $\tvd{X}{Y}=\tvd{Y}{X}$, and the triangle inequality, i.e., for three random vairables $X,Y,Z$, there is $\tvd{X}{Y}+\tvd{Y}{Z}\geq \tvd{X}{Z}$.

\paragraph{KL divergence.} We now introduce the Kullback–Leibler divergence (KL divergence) as an alternative statistical distance of TVD.

\begin{definition}[KL divergence]
	\label{def:kl-div}
	Let $X$ and $Y$ be two discrete random variables supported over the same $\Omega$, and let their distributions be $\mu_{X}$ and $\mu_{Y}$.The KL divergence between $X$ and $Y$, denoted as $\kl{X}{Y}$, is defined as 
	\begin{align*}
		\kl{X}{Y} = \sum_{\omega\in \Omega} \mu_{X}(\omega)\log\paren{\frac{\mu_{X}(\omega)}{\mu_{Y}(\omega)}}.
	\end{align*}
\end{definition}


Unlike the TVD, KL divergence is \emph{not} symmetric in general and does \emph{not} follow the triangle inequality.
% KL divergence can also be defined on continuous random variables, but we do not pursue that direction in this paper. Note that the KL divergence does \emph{not} satisfy the triangular inequality. However, the following equality, known as the \emph{chain rule}, is very useful in `factorizing' a joint distribution into marginals.
% \begin{proposition}[Chain rule of KL divergence]
	% \label{prop:chain-rule}
	% Let $X=(X_{1}, X_{2})$ and $Y=(Y_{1},Y_{2})$ be two random variables, there is
	% \begin{align*}
		% \kl{X}{Y} = \kl{X_{1}}{Y_{1}} + \kl{X_{2}\mid X_{1}}{Y_{2}\mid Y_{1}}.
		% \end{align*}
	% \end{proposition}


\paragraph{The Properties of the TVD and KL divergence}
% \label{sub-app:info-theoretic-facts}

We shall use the following standard properties of KL-divergence and TVD defined. For the proof of this results, see the excellent textbook by Cover and Thomas~\cite{CoverT06}. 

We first connect the KL-divergence to TVD with the following celebrated Pinsker's inequality. 

\begin{fact}[Pinsker's inequality]
	\label{fact:pinsker}
	For any random variables $X$ and $Y$ supported over the same $\Omega$, 
	\begin{align*}
		\tvd{X}{Y} \leq \sqrt{\frac{1}{2}\cdot \kl{X}{Y}}.
	\end{align*}
\end{fact} 

We then present the key properties we used in our proof for the KL divergence, which includes the Chain rule and the conditional KL-divergence.
\begin{fact}[Chain rule of KL divergence]
	\label{fact:kl-chain-rule}
	For any random variables $X=(X_{1}, X_{2})$ and $Y=(Y_{1},Y_{2})$ be two random variables, 
	\begin{align*}
		\kl{X}{Y} = \kl{X_{1}}{Y_{1}} + \Exp_{x \sim X_1} \kl{X_{2}\mid X_{1}=x}{Y_{2}\mid Y_{1}=x}.
	\end{align*}
\end{fact}

%\begin{fact}[Convexity KL-divergence]
%	\label{fact:kl-convexity}
%	For any distributions $\mu_1,\mu_2$ and $\nu_1,\nu_2$ and any $\lambda \in (0,1)$, 
%	\begin{align*}
%		\kl{\lambda \cdot \mu_1 + (1-\lambda) \cdot \mu_2}{\lambda \cdot \nu_1 + (1-\lambda) \cdot \nu_2} \leq \lambda \cdot \kl{\mu_1}{\nu_1} + (1-\lambda) \cdot \kl{\mu_2}{\nu_2}. 
%	\end{align*}
%\end{fact}

\begin{fact}[Conditioning cannot decrease KL-divergence]\label{fact:kl-conditioning}
	For any random variables $X,Y,Z$, 
	\[
	\kl{X}{Y} \leq \Exp_{z \sim Z} \kl{X \mid Z=z}{Y \mid Z=z}. 
	\]
\end{fact}

In our proofs, we slighlty abuse the notation to let $\kl{X|Z}{Y|Z}$ denoting $\Exp_{z \sim Z} \kl{X \mid Z=z}{Y \mid Z=z}$.

The following fact characterizes the error of MLE for the source of a sample based on the TVD of the originating distributions. 

\begin{fact}
	\label{fact:distinguish-tvd}
	Suppose $\mu$ and $\nu$ are two distributions over the same support $\Omega$; then, given one sample $s$ from the following distribution
	\begin{itemize}
		\item With probability $\rho$, sample $s$ from $\mu$;
		\item With probability $1-\rho$, sample $s$ from $\nu$;
	\end{itemize}
	The best probability we can decide whether $s$ came from $\mu$ or $\nu$ 
	is 
	\[
	\max(\rho, 1-\rho) + \min(\rho, 1-\rho)\cdot\tvd{\mu}{\nu}.
	\]
\end{fact}


We frequently use the calculation of KL divergence between Bernoulli random variables. The following fact is a standard upper bound for KL divergence on Bernoulli random variables:
\begin{fact}
	\label{fct:bernoulli-KL}
	Let two random variables be distributed with $\bern{p}$ and $\bern{q}$, there is
	\begin{align*}
		\kl{\bern{p}}{\bern{q}}\leq \frac{(p-q)^2}{q\cdot (1-q)}.
	\end{align*}
\end{fact}


\Cref{fct:bernoulli-KL} implies the following the upper bound for KL-divergence, which we frequently use in our proof.

\begin{claim}
\label{clm:bernoulli-KL}
Let two random variables be distributed with $\bern{1/2+\alpha}$ and $\bern{1/2+\beta}$ such that $\max\{\alpha,\beta\}\leq \frac{1}{6}$, there are
\begin{align*}
	& \kl{\bern{1/2+\alpha}}{\bern{1/2+\beta}} \leq 8\cdot (\beta-\alpha)^2\\
	& \kl{\bern{1/2+\beta}}{\bern{1/2+\alpha}} \leq 8\cdot (\beta-\alpha)^2.
\end{align*}
\end{claim}
\begin{proof}
We prove the first inequality, since the second inequality follows the same calculation. The calculation is as follows.
\begin{align*}
	\kl{\bern{1/2+\alpha}}{\bern{1/2+\beta}} & \leq (\beta-\alpha)^2 \cdot \frac{1}{1/4-\beta^2}\\
	& \leq \frac{36}{8}\cdot (\beta-\alpha)^2 \tag{using $\beta\leq \frac{1}{6}$}\\
	&\leq 8\cdot (\beta-\alpha)^2.
\end{align*}
\end{proof}
% \chen{Include the lemma here.}

\subsection{Information Theory Tools}
\label{subsec:info-theory}
% \chen{revise this section to make it not copy-paste and reflect the tools we used in the section.}
We present the definition and basic properties of the information-theoretic tools in our proofs. For a random variable $X$, we let $\HH(X)$ be the \emph{Shannon entropy} of $X$, defined as follows
\begin{definition}[Shannon entropy]
	\label{def:entropy}
	Let $X$ be a discrete random variable with distributions $\mu_{X}$, the \emph{Shannon entropy} of $X$ is defined as
	\begin{align*}
		\HH(X) \triangleq \expect{\log(1/\mu(X))} =\sum_{x \in \text{supp}(X)} \mu(x)\cdot \log(\frac{1}{\mu(x)}),
	\end{align*}
	where $\text{supp}(X)$ is the support of $X$. If $X$ is a Bernoulli random variable, we use $H_2(p)$ to denote its Shannon entropy, where $P$ is the probability for $X=1$.
\end{definition}

We now give the definition of conditional entropy and mutual information.
\begin{definition}
	\label{def:mutual-info}
	Let $X$, $Y$ be two random variables, we define the \emph{conditional entropy} as \[\HH(X|Y)=\mathbb{E}_{y \sim Y}[\HH(X\mid Y=y)].\] 
	With conditional entropy, we can define the \emph{mutual information} between $X$ and $Y$ as \[\II\paren{X;Y}\triangleq \HH(X)-\HH(X\mid Y) = \HH(Y)-\HH(Y|X).\]
\end{definition}



In our proof, we use the following information-theoretic fact (see e.g.~\cite{CoverT06}) that connects mutual information with the KL-divergence.
\begin{fact}
	\label{fct:mutual-info-and-kl}
	Let $X$, $Y$ be  discrete random variables, there is
	\[\II(X;Y) = \Exp_{y\sim Y}\bracket{\kl{X\mid Y=y}{X}}.\]
\end{fact}

%\begin{fact}
%	\label{fct:info-theory-facts}
%	Let $X$, $Y$, $Z$ be three discrete random variables:
%	\begin{itemize}
%		\item KL-divergence view of mutual information: $\II(X;Y) = \Exp_{y\sim Y}\bracket{\kl{X\mid Y=y}{X}}$.
%		\item $0\leq \HH(X)\leq \log(\card{\text{supp}(X)})$. In particular, if $X$ is a Bernoulli random variable, there is $H_2(p)\leq 1$.
%		\item $0\leq \II(X;Y)\leq \min\{\HH(X), \HH(Y)\}$.
%		\item Conditioning on independent random variable: let $X$ be independent of $Z$, then $\II(X;Y)\leq \II(X;Y\mid Z)$.
%		\item Chain rule of mutual information: $\II(X, Y; Z)=\II(X;Z)+\II(Y;Z\mid X)$.
%		\item Sub-additivity of entropy: $\HH(X,Y) \leq \HH(X) + \HH(Y)$, where $\HH(X, Y)$ is the joint entropy of variables $X, Y$.
%		\item Conditional independence of entropy: $\HH(X\mid Y,Z)=\HH(X\mid Y)$ if $X\perp Z\mid Y$, where the $\perp$ notation stands for independence.
%	\end{itemize}
%\end{fact}

%The following statement is known as the \emph{data processing inequality}, which says if $Y$ is obtained as a function of $X$, and $Z$ is obtained as a function of $Y$, then the mutual information between $X$ and $Z$ can only be lower than that between $X$ and $Y$.
%\begin{proposition}
%	\label{prop:DPI}
%	Let $X$, $Y$, and $Z$ be random variables on finite supports, and we slightly abuse the notation to let $X,Y,Z$ to denote the distribution functions as well. Let $f$ be a deterministic function (no internal randomness), and suppose $Z=f(Y)$. Then, we have
%	\begin{align*}
%		\II(X;Z)\leq \II(X;Y).
%	\end{align*}
%\end{proposition}
%
%The following statement characterizes the relationship between the ``zero mutual information'' and the independence of the conditional probability.
%
%\begin{proposition}
%	\label{prop:mi-prob-indep}
%	Let $X$, $Y$, and $Z$ be random variables on finite supports, and suppose $\II(X;Y\mid Z=z)=0$. Then, for any realization $y\in Y$, there is
%	\begin{align*}
%		\Pr(X\mid Z=z, Y=y) = \Pr(X\mid Z=z).
%	\end{align*}
%\end{proposition}



