\section{Experiments}
\label{sec:experiment}
We present the empirical results in this section. For multi-pass streaming MABs algorithms, there are two objectives we want to optimize: the sample complexity and the pass efficiency. Our main experimental result is that compared to existing algorithms for streaming MABs, our algorithm exhibits significant advantages on both fronts.

\subsection{Experiment settings} 
We compare our algorithm with two benchmark algorithms: $i).$ the AW algorithm: the single-pass algorithm by \cite{AssadiW20}, which only uses a single pass over the stream, requires the knowledge of $\Delta_{[2]}$, and uses the worst-case optimal sample complexity of $\Theta(\frac{n}{\Delta^2_{[2]}})$; and $ii).$ the JHTX algorithm: the $O(\log(1/\Delta_{[2]}))$-pass algorithm by \cite{JinH0X21}, which does not require the knowledge of $\Delta_{[2]}$ and achieves the instance-sensitive near-instance optimal $O(\sum_{i=2}^{n}1/\Delta^2_{[i]}\cdot \log\log(1/\Delta_{[i]}))$ sample complexity, but has to use more passes than ours. We implemented the algorithm and track the number of arm pulls and passes. We pick the constants large enough so that the algorithms do \emph{not} fail in any of the runs.

We consider instances of $2000$ arms in $3$ settings, namely the \emph{uniform} setting, in which the mean rewards of the arms are from a uniform distribution supported on $[0,1]$; the \emph{arithmetic progression} setting, in which the mean rewards of the arms follow an arithmetic progression; and the \emph{cluster} setting, where there is one arm with higher mean reward ($0.9$), and all other arms are divided into two clusters with mean rewards $0.899$ and $0.898$, respectively. In each setting, we take $30$ independent runs, and we report the error bars to avoid statistical influx. 

\subsection{Experimental results}
Our main finding in the experiments is that our algorithm consistently outperforms both the algorithms of \cite{AssadiW20} and \cite{JinH0X21} in terms of sample complexity. Furthermore, compared to \cite{JinH0X21}, our algorithm uses significantly less passes. This confirms the theoretical analysis and demonstrates the strong practical value of our algorithm.
\paragraph{Results in the uniform setting.} The comparison between the sample complexity and the number of passes can be found in \Cref{fig:experiments-uniform}. Note that since the algorithm of \cite{AssadiW20} always uses a single pass, we do not report it in \Cref{fig:uniform-exp-pass}. From the figure, it can be found that the our algorithm has the best sample complexity among the algorithms. The sample complexity of the AW algorithm is considerably higher than the two others, which is understandable since in the instance with arithmetic progression means, we have $\frac{n}{\Delta^2_{[2]}} \gg \sum_{i=2}^{n}1/\Delta^2_{[i]}\cdot \log\log(1/\Delta_{[i]})$. comparing to the JHTX algorithm, our algorithm achieves lower mean sample complexity, and it is more stable. Finally, as we can see from \Cref{fig:uniform-exp-pass}, our algorithm uses a much smaller passes than JHTX.

\begin{figure}
	\centering
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[scale=0.3]{figs/sample-complexity-uniform.png}
		\caption{Comparison of samples between algorithms}
		\label{fig:uniform-exp-sample}
	\end{subfigure}%
	% do not leave blank
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[scale=0.3]{figs/pass-complexity-uniform.png}
		\caption{Comparison of passes between algorithms}
		\label{fig:uniform-exp-pass}
	\end{subfigure}
	\caption{The comparison between algorithms on the sample complexity and the number of passes in the \emph{uniform setting}. Samples numbers are taken $\log_{10}(\cdot)$ for better illustration. The graphs are reported by $30$ independent runs. AW stands for the single-pass algorithm of \cite{AssadiW20}, and JHTX stands for the single-pass algorithm of \cite{JinH0X21}.}
	\label{fig:experiments-uniform}
\end{figure}

To better illustrate the performance comparisons, we also summarize results in \Cref{tab:exp-uniform}. The numbers are rounded to two decimal places (and for the sample complexity, we do this after converting to the scientific notation). The table gives a clearer illustration of the better sample complexity of our algorithm: our algorithm in fact achieves a $10\times$ better sample efficiency than JHTX.
\begin{table}[!h]
	\centering
	\captionsetup{justification=centering}
	\caption{\label{tab:exp-uniform} The comparison between algorithms on the sample complexity and the number of passes in the \emph{uniform setting}.}
	\begin{tabular}{|l|l|l|}
		\hline
		& Mean samples & Mean passes \\ \hline
		AW & $5.62\times 10^{11}$ & -- \\ \hline
		JHTX & $1.41\times 10^{10}$ & 16.4 \\ \hline
		Our Algorithm & $1.18\times 10^{9}$ & 8.83  \\ \hline
	\end{tabular}
	
\end{table}

\paragraph{Results in the arithmetic progression setting.} The comparison between the sample complexity and the number of passes can be found in \Cref{fig:experiments-progression} and \Cref{tab:exp-progression}. This setting is very similar to the uniform setting, barring the fact that the rewards between arms are more stable and regularized. From the figure and the table, it can be observed that our algorithm again demonstrates the best sample complexity, and outperforms the second-best JHTX algorithm by almost $10\times$ in sample complexity. Again, since $\frac{n}{\Delta^2_{[2]}} \gg \sum_{i=2}^{n}1/\Delta^2_{[i]}\cdot \log\log(1/\Delta_{[i]})$, the AW algorithm uses a significantly larger number of arm pulls.  

\begin{figure}
	\centering
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[scale=0.3]{figs/sample-complexity-progression.png}
		\caption{Comparison of samples between algorithms}
		\label{fig:progression-exp-sample}
	\end{subfigure}%
	% do not leave blank
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[scale=0.3]{figs/pass-complexity-progression.png}
		\caption{Comparison of passes between algorithms}
		\label{fig:progression-exp-pass}
	\end{subfigure}
	\caption{The comparison between algorithms on the sample complexity and the number of passes in the \emph{arithmetic progression setting}. Samples numbers are taken $\log_{10}(\cdot)$ for better illustration. The graphs are reported by $30$ independent runs. AW stands for the single-pass algorithm of \cite{AssadiW20}, and JHTX stands for the single-pass algorithm of \cite{JinH0X21}.}
	\label{fig:experiments-progression}
\end{figure}

\begin{table}[!h]
	\centering
	\captionsetup{justification=centering}
	\caption{\label{tab:exp-progression} The comparison between algorithms on the sample complexity and the number of passes in the \emph{arithmetic progression setting}.}
	\begin{tabular}{|l|l|l|}
		\hline
		& Mean samples & Mean passes \\ \hline
		AW & $4.01\times 10^{12}$ & -- \\ \hline
		JHTX & $5.05\times 10^{10}$ & 18.53 \\ \hline
		Our Algorithm & $4.61\times 10^{9}$ & 8.67  \\ \hline
	\end{tabular}
\end{table}





\paragraph{Results in the clustered instance setting.} We now come to the setting for clustered instances, and the results can be shown in \Cref{fig:experiments-cluster} and \Cref{tab:exp-cluster}. Note that in this case, we have that $\frac{n}{\Delta^2_{[2]}} \approx \sum_{i=2}^{n}1/\Delta^2_{[i]}\cdot \log\log(1/\Delta_{[i]})$, which is the reason the AW algorithm offers a very competitive sample complexity in \Cref{fig:cluster-exp-sample}. Interestingly, the JHTX algorithm is using many arm pulls in this setting. We suspect the reason is that regardless of the actually gaps, the gap-elimination procedure in the JHTX algorithm has to search from large gaps, and waste many sample and passes without eliminating any arm. We also note that our algorithm again demonstrate the best sample complexity in this setting. 

\begin{figure}
	\centering
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[scale=0.3]{figs/sample-complexity-cluster.png}
		\caption{Comparison of samples between algorithms}
		\label{fig:cluster-exp-sample}
	\end{subfigure}%
	% do not leave blank
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[scale=0.3]{figs/pass-complexity-cluster.png}
		\caption{Comparison of passes between algorithms}
		\label{fig:cluster-exp-pass}
	\end{subfigure}
	\caption{The comparison between algorithms on the sample complexity and the number of passes in the \emph{arithmetic progression setting}. Samples numbers are taken $\log_{10}(\cdot)$ for better illustration. The graphs are reported by $30$ independent runs. AW stands for the single-pass algorithm of \cite{AssadiW20}, and JHTX stands for the single-pass algorithm of \cite{JinH0X21}.}
	\label{fig:experiments-cluster}
\end{figure}

\begin{table}[!h]
	\centering
	\captionsetup{justification=centering}
	\caption{\label{tab:exp-cluster} The comparison between algorithms on the sample complexity and the number of passes in the \emph{uniform setting}.}
	\begin{tabular}{|l|l|l|}
		\hline
		& Mean samples & Mean passes \\ \hline
		AW & $3.32\times 10^{10}$ & -- \\ \hline
		JHTX & $1.38\times 10^{11}$ & 13.47 \\ \hline
		Our Algorithm & $1.73\times 10^{10}$ & 9.03  \\ \hline
	\end{tabular}
\end{table}


