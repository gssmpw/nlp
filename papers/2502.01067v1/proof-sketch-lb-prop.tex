% \section{Proof Sketch of \Cref{prop:multi-pass-lb}}
% \label{sec:multi-pass-lb-tool}
% We present a high-level proof sketch of \Cref{prop:multi-pass-lb}, and discuss key differences between \cite{AW23BestArm} that allow us to generalize the statement to fit our construction. 

% \paragraph{An overview of the proof in \cite{AW23BestArm}.} We start with an overview of the \emph{proof} of the main statement of \cite{AW23BestArm}. As we have discussed, the construction of the adversarial instances in \cite{AW23BestArm} follows the same family of batched instance that $a).$ divide the arms into $B+1$ batches and $b).$ make the gap between the sample complexity to `trap' or `learn' arms from batches large enough. The exact construction of \cite{AW23BestArm} is as follows.


% \begin{tbox}
% 	$\cP(B, C)$: A hard instance distribution for multi-pass MABs algorithms (without known $\Delta_{[2]}$). 
	
% 	\begin{enumerate}
% 		\item \textbf{Parameters}: Let $\chi_{1}=(\frac{1}{6C\cdot B})^{10}$, and set 
% 		\[\chi_{b+1} = \paren{\frac{1}{6 C \cdot B}}^{10} \cdot \chi_{b}.\]
% 		\item \textbf{Division of arms:} Divide the $n$ arms into $(B+1)$ batches of equal sizes, and put them in the \emph{reverse} order of the stream, i.e. $\mathcal{B}_{B+1}$ arrives first, and $\mathcal{B}_{1}$ arrives the last.
% 		\item \textbf{Sampling special arms: } For each batch $b\in [B+1]$, sample \emph{a single} arm uniformly at random (without replacement), and call it \emph{special arms}. Set all the arms \emph{except} the special arm with reward distribution $\bern{1/2}$.
% 		\item \textbf{Batches $b\in[B]$: } For each $b\in [B]$, sample $\Theta_{b}$ from distribution $\bern{1/2B}$:
% 		\begin{enumerate}
% 			\item If $\Theta_{b}=0$, set the special with reward distributions $\bern{1/2}$.
% 			\item Otherwise, i.e., $\Theta_{b}=1$, set the special arm with reward distribution $\bern{1/2+\chi_{b}}$.
% 		\end{enumerate}
% 		\item \textbf{The batch $B+1$: } Always set the reward distributions of the special arm with $\bern{1/2+\chi_{B+1}}$.
% 	\end{enumerate}
% \end{tbox}

% The construction is similar to our hard distribution in \Cref{sec:lb-main}, albeit their construction uses only one special arm each batch, and they do \emph{not} need to control the relationship between $\chi_{b}$ and $\gamma$ (which is the technical reason for their result to go beyond $O(\log(n))$ passes). 

% The key idea of \cite{AW23BestArm} is to directly work with the \emph{conditional distribution} of the instance family. Crucially, their argument explicitly tracks the `knowledge' the algorithm ever learned, and the information on the batches that they `do not care' is further revealed to the algorithm\footnote{This is a very high-level summary of the technical work in \cite{AW23BestArm} -- keen readers can find a more detailed discussion for the technical idea in their paper.}. To formalize this strategy, they defined the notion of \emph{memory-oblivious} and \emph{batch-oblivious} algorithm as follows.
% \begin{itemize}
% 	\item An algorithm is said to be \emph{memory-oblivious} by the end of pass $p$ if the memory $M$ does not contain any arm with mean reward strictly more than $\frac{1}{2}$.
% 	\item An algorithm is said to be \emph{batch-oblivious} by the end of pass $p$ if for every $b\in(p, B]$, there is
% 	\[\Pr\paren{\Theta_{b}=1\mid \Pi=\pi, \mathsf{M}=M, \Theta_{\leq p}=0} \in \left[\frac{1}{2B}-\frac{p}{4B^2}, \frac{1}{2B}+\frac{p}{4B^2}\right].\]
% \end{itemize}

% As the name suggested, the memory-oblivious property indicates that the algorithm does \emph{not} store any special arm, and the batch oblivious property means the internal distribution of the batches (arriving before $p$) is \emph{not} too far away from the original distribution. Their main technical analysis then assume the algorithm is memory- and batch-oblivious by the end of the $p$-th pass, and proceeds with two different cases based on the expected samples used on batches from $p+2$ to $B+1$, denoted as $\smp_{B+1:p+2}$.
% \begin{enumerate}[label=\alph*).]
% 	\item\label{line:ana-conservative} Small number of samples -- the `conservative case'. In this case, they assume the expected samples used on the batches that arrive before $p+1$ is small, i.e.
% 	\[\expect{\smp_{B+1:p+2}\mid \Pi=\pi, \mathsf{M}=M, \Theta_{\leq p}=0}\leq \frac{1}{5000}\cdot \frac{n}{\chi^2_{p+2}}\cdot \frac{1}{\poly(B)}.\]
% 	For this case, their main goal is to prove that with probability $(1-\frac{1}{2B})^c$ for some constant $c$, the resulting memory $M'$ and transcript $\pi'$ by the end of pass $p+1$ remains memory- and batch-oblivious. Technically, this is where conditions \textbf{C1} and \textbf{C2} in \Cref{prop:multi-pass-lb} are used. Concretely, we use \textbf{C1} to ensure that the streaming algorithm remains memory oblivious by the end of the $(p+1)$-th pass (with probability $(1-1/2B)^{O(1)}$) -- if the number of samples is small, the algorithm cannot trap any special arm. Then, by \textbf{C2} and the condition of memory obliviousness, we can argue that the algorithm cannot `learn' too much for matches $p+2$ and onwards, since the number of samples is again too small. 
% 	\item\label{line:ana-radical} Large number of samples -- the `radical case'. In this case, they assume the expected samples used on the batches that arrive before $p+1$ is large, i.e.
% 	\[\expect{\smp_{B+1:p+2}\mid \Pi=\pi, \mathsf{M}=M, \Theta_{\leq p}=0}> \frac{1}{5000}\cdot \frac{n}{\chi^2_{p+2}}\cdot \frac{1}{\poly(B)}.\]
% 	For this case, their main goal is to prove that the algorithm cannot satisfy the expected sample bound of $C\cdot \frac{n}{\chi^2_{p+1}}$ if $\Theta_{p+1}=1, \Theta_{\leq p}=0$, which would break the sample complexity restriction since $(\Delta_{[2]}\mid\Theta_{p+1}=1, \Theta_{\leq p}=0)=\chi_{p+1}$. To this end, they use \textbf{C3} to argue that $O\left(\frac{n}{\chi^2_{p+2}}\cdot \frac{1}{\poly(B)}\right) \gg C\cdot \frac{n}{\chi^2_{p+1}}$, which holds true by the gap between the $\chi_{b}$ values.
% \end{enumerate}
% Combining \Cref{line:ana-conservative} and \Cref{line:ana-radical} allows \cite{AW23BestArm} to inductively argue that to preserve the desired sample complexity bound:
% \begin{equation}
% \label{equ:equ:batch-sample-ub-simple}
% \expect{\smp\mid \Theta_{p+1}=1, \Theta_{\leq p}=0}\leq C \cdot \frac{n}{\chi_{p+1}^2},
% \end{equation}
% the algorithm has to proceed with the conservative case for every pass. Consequently, by setting $P=B$, the algorithm fails with constant probability if $B+1$ is the only batch that contains an arm with mean reward $>\frac{1}{2}$, which in turn is an event that happens with constant probability. This establishes a lower bound on the failure probability and concludes the proof.


% \paragraph{The additional properties in \Cref{prop:multi-pass-lb}.} We now discuss the additional properties we demonstrated in \Cref{prop:multi-pass-lb}, which are not explicitly stated but are implied in \cite{AW23BestArm}. We first observe that the above proof sketch applies to all batched instance distributions with properties \textbf{C1}, \textbf{C2}, and \textbf{C3}, i.e., the argument is not limited to one specific construction.

% We then discuss two differences in the statement of \Cref{prop:multi-pass-lb} comparing to \cite{AW23BestArm}.
% \begin{enumerate}
% 	\item Allowing $P\leq B$. In the construction of \cite{AW23BestArm} (as shown above), we have $B=P$ for a $P$-pass algorithm. From the proof sketch, it can be observed that the argument still holds if $B\geq P$ \emph{and} the memory and sample complexity bound for conditions \textbf{C1} and \textbf{C2} scale with $B$. The gap between $B$ and $P$ can lead to suboptimal memory and sample complexity lower bounds; however, the proposition holds true.
% 	\item The extra $B^2$ factor on the sample complexity. In our \Cref{prop:multi-pass-lb}, instead of showing the bound of \Cref{equ:equ:batch-sample-ub-simple}, we target the sample complexity upper bound of \Cref{equ:batch-sample-ub}, which contains an extra $B^2$ factor. Note that to make this work, we essentially only need to modify the proof of the radical case \Cref{line:ana-radical} such that
% 	\begin{align*}
% 		O\left(\frac{n}{\chi^2_{p+2}}\cdot \frac{1}{\poly(B)}\right) \gg C\cdot B^2\cdot \frac{n}{\chi^2_{p+1}},
% 	\end{align*}
% 	and replace $\chi_{b}$ with $\etaib{1}{b}$. To this end, we can \emph{augment the multiplicative gap} between $\chi_{b}$ and $\chi_{b+1}$ by a $1/\poly(B)$ factor. This is also the technical reason for us to use $\chi_{b} = \Theta(1/B^{13b})$ as opposed to $\chi_{b} = \Theta(1/B^{10b})$ in \cite{AW23BestArm}. 
% \end{enumerate}

% Combining the above cases give us the desired statement of \Cref{prop:multi-pass-lb}.

