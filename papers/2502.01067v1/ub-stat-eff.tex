\section{A Streaming Algorithm with Memory Efficiency in Both Arms and Statistics}\label{sec:ub-stat-efficient}
In this section we describe algorithm that achieves memory efficiency for both number of arms and statistics. In particular, we show a $P$-pass algorithm with a single-arm memory and maintaining \(O(P)\) bits of statistics. The sample complexity becomes \(O\left(P \log \left(\frac{nP}{\delta}\right) \cdot \sum_{i = 2}^{n} \frac{n^{2/P}}{\Delta^2_{[i]}}\right)\) in this algorithm, which is still in the range of $\tilde{O}\paren{\sum_{i = 2}^{n} \frac{n^{2/P}}{\Delta^2_{[i]}}}$ by picking $P=O(\log(n))$. The formal statement is as follows.

\begin{theorem}
	\label{thm:improved}
	For any \(P \geq 1\), \Cref{alg:improved} is an algorithm that given a streaming MABs instance and the value of $\Delta_{[2]}$, finds the best arm with probability at least \(1-\delta\) using most
	\[O\left(P \log \left(\frac{nP}{\delta}\right) \cdot \sum_{i = 2}^{n} \frac{n^{2/P}}{\Delta^2_{[i]}}\right)\] 
	arm pulls, a memory of a single arm, and at most $O(P)$ bits of statistics.
\end{theorem}

We refer the readers to \Cref{subsec:model} for the formal definition of the memory complexity on the number of arms and the statistics. Compared to the algorithm in \Cref{sec:basic}, the idea to reduce the memory usage it to not store sets \(I_p\) for each \(p\). Instead, we simply store the maximum estimated mean for each pass, and simulate the results of the previous passes by resampling to get the new empirical mean. As a result, we pay a $P$ factor overhead for the arm pulls on each arm, but do not need to explicitly maintain the indices of the arms or query the transcript $\pi$. We present the description of the algorithm in \Cref{alg:improved}. 


\FloatBarrier
\begin{algorithm}
	\caption{Stream-Elimination-Re}\label{alg:improved}
	\KwIn{Stream \(I\), parameter $P$, gap parameter \(\Delta_{[2]}\), and confidence parameter \(\delta\)}
	\KwOut{Best arm}
	Set \(n \gets \abs{I}\)\; 
	Maintain the index of the returned arm $\itilde \gets \perp$\;
	Let \(\epsilon_p \triangleq n^{1-i/P}\Delta_{[2]} / 4\) for \(p = 0, \dotsc, P\) \;
	Let \(T_p \triangleq \frac{8}{\epsilon^2_p \log e} \log\left(\frac{2 n (P + 1)^2}{\delta}\right) \) for \(p = 0, \dotsc, P\)\; 
	\For{\(p = 0, \dotsc, P\)}{
		Initialize \(\hat\mu^p_{\max} \gets -\infty\)\;
		Initialize the number of eliminated arms \(c \gets 0\)\;
		\ForEach{\(i \in I\) in the arrival order}{
			\For{\(j = 0, \dotsc, p\)}{
				Pull arm \(i\) until the number of pulls reach \(T_j\) times and compute the estimated mean \(\hat\mu^{pj}_i\)\;
				\label{alg:improved-eliminate}\If{\(\hat\mu^{pj}_i < \hat{\mu}^j_{\max} - \epsilon_j\)}{
					Update \(c \gets c + 1\) \;
					Eliminate arm \(i\) and exit loop \; 
				}
				\If{arm \(i\) is not  eliminated}{
					\If{\(\hat\mu^p_{\max} < \hat\mu^{pj}_i\)}{
						Update \(\hat\mu^p_{\max} \gets \hat\mu^{pj}_i\) and \(\itilde \gets i\)\;
					}
				}
			}
		}
		\If{\(c = n - 1\)}{
			\Return{The arm of index \(\itilde\)}\;
		}
	}
\end{algorithm}

\FloatBarrier


% \chen{Add a proof for the number of arms and statistics; also, discuss we always only maintain one record in $\pitilde$.}
We start with observing the memory efficiency for \Cref{alg:improved} on both stored arms and bits of statistics. Formally, we show that
\begin{lemma}
\label{lem:improved-memory}
The maximum size of the memory of \Cref{alg:improved} is at most a single arm, and the maximum size of statistics \Cref{alg:improved} matains is $O(P)$.
\end{lemma}
\begin{proof}
	For the memory complexity of the number of arms, note that we can one-the-fly keep the arm with index $\itilde$, and we never need to store more than the $\arm_{\itilde}$. 
	
	For the size of the statistics, note that we do \emph{not} keep the empirical means for individual arms in the memory, and we only maintain the mean estimation of $\hat\mu^p_{\max}$ and at most $P$ different values of $\hat\mu^{p{j}}$. Therefore, the auxiliary number of bits to maintain is at most $O(P)$. Finally, for the size of $\pitilde$ (defined in \Cref{subsec:model}), we can always update the estimation of mean on-the-fly, and we never query a record of arm pull for more than once. Therefore, the size of $\pitilde$ is at most $1$. The desired memory bound is obtained by summarizing the above cases.
\end{proof}


For the correctness and the sample complexity, the proof strategy is similar to the proof for \Cref{alg:main}. 
We start with the definition of the event when all estimated means are close to the real values of means. Let's define an event \(\F\) as follows:

\begin{align}\label{eq:event-F}
	\F \triangleq \left\{\forall{i \in I}, p \in \{0, \dotsc, P\}, j \in \{0, \dotsc, p\} : \abs{\hat\mu^{pj}_i - \mu_i} \le \epsilon_r / 4\right\}
\end{align}
Where, \(\hat\mu^{pj}_i\) is the empirical mean of the samples drawn from distribution \(i\) in \(p\)-th pass after \(T_j\) pulls. 

We can now state the following lemma: 

\begin{lemma}\label{lem:bound-F}
	Let \(\F\) be the event defined in~\Cref{eq:event-F}. Then, \[\Pr\left[ \neg \F\right] \le \delta\,. \]
\end{lemma}
\begin{proof}
	By the union bound, we have:
	\begin{align}\label{eq:event-F-sum}
		\Pr\left[\neg \F\right] \le \sum_{i \in I} \sum_{p = 0}^{P} \sum_{j = 0}^{p} \Pr\left[\abs{\hat{\mu}^{pj}_i - \mu_i} > \epsilon_r/4\right]\,.
	\end{align}
	By using the Chernoff-Hoeffding inequality (\Cref{lem:chernoff}) we have: 
	\begin{align}\label{eq:event-F-term}
		\Pr\left[\abs{\hat\mu^{pj}_i-\mu_i} > \frac{\epsilon_r}{4}\right] \leq 2\exp\left(\frac{\epsilon^2_r T_r}{8}\right) \leq \frac{\delta}{n(P + 1)^2}\,.
	\end{align}
	
	Combining \Cref{eq:event-F-sum} and \Cref{eq:event-F-term}, we get: 
	\begin{align*}
		\Pr\left[\neg \F\right] \le n {(P + 1)}^2 \frac{\delta}{n(P + 1)^2} = \delta
	\end{align*}
\end{proof}


\begin{lemma}\label{lem:improved-best}
	 Conditioning on the event \(\F\) defined in \Cref{eq:event-F} holds, then for any \(p \in [P]\) we have that the arm \(\star\) is not eliminated in the \(p\)-th pass. 
\end{lemma}

\begin{proof}
	We need to prove that the if condition from \Cref{alg:improved-eliminate} in \Cref{alg:improved} does not hold for \(i = \star\). To prove this, we start by assuming the opposite. Assume that in \(p\)-th iteration for some \(j \in \{0, \dotsc, p\}\) we have 
	\begin{align*}
		\hat\mu^{pj}_{\star} < \hat\mu^j_{\max} - \epsilon_j
	\end{align*}
	and consequently
	\begin{align}\label{eq:improved-upper}
		\hat\mu^{pj}_{\star} < \hat\mu^{jj}_i - \epsilon_j
	\end{align}
	for some \(i \in I\). 
	
	However, from the definition of the event \(\F\) (\Cref{eq:event-F}) we have \(\hat\mu^{pj}_\star \geq \mu_\star - \epsilon_j / 4\) and \(\hat\mu^{jj}_i \leq \mu_i + \epsilon_r / 4\). Consequently, we get: 
	\begin{align}\label{eq:improved-lower}
		\hat\mu^{pj} - \hat\mu^{jj}_i \ge \mu_\star - \mu_i - \epsilon_j / 2 \ge -\epsilon_j / 2 \,.
	\end{align}
	
	\Cref{eq:improved-upper} and \Cref{eq:improved-lower} contradict to each other, leading to a contradiction. Therefore, it is not possible for the arm \(\star\) be eliminated at any moment of work of \Cref{alg:improved}. 
\end{proof}

We next prove that any suboptimal arm with a large gap will be eliminated in each pass after before specific number of pulls. 

\begin{lemma}\label{lem:improved-suboptimal-bound}    
	Conditioning on the event \(\F\) defined in \Cref{eq:event-F} holds, and assume for arm \(i\) and value \(j\), arm \(i\) satisfies \(\Delta_i > \frac{3}{2}\epsilon_j\), then for any pass \(p \in \{j, \dotsc, P\}\), arm \(i\) will be eliminated after at most \(T_j\) pulls.
\end{lemma}
\begin{proof}
	Consider any suboptimal arm \(i\) and a value \(j\) such that \(\Delta_i > \frac{3}{2} \epsilon_j\). We aim to prove that the arm \(i\) should be eliminated after we make at most \(T_j\) pulls for any pass \(p \geq j\). Consider \(p\)-th pass, if the arm \(i\) is eliminated before we make \(T_j\) pulls, then we are done. 
	Consider the opposite case when we make at least \(T_j\) pulls. By using \Cref{lem:improved-best} and the definition of the event \(\F\), we have the following inequality: 
	\begin{align}\label{eq:improved-best-lower}
		\hat\mu^{p}_{\max} \ge \hat\mu^{pj}_{\star} \ge \mu_{\star} - \epsilon_j / 4\,.
	\end{align}
	This inequality indicates that the maximum estimated mean \(\hat\mu^{j}_{\max}\) in \(p\)-th pass is at least as large as the estimated mean \(\hat\mu^{pj}_{\star}\) of the best arm \(\star\), which is at least \(\mu_\star - \epsilon_j / 4\) by the event \(\F\). 
	
	Furthermore, by the event \(\F\), we have:
	\begin{align}\label{eq:improved-sub-upper}
		\hat\mu^{pj}_{i} \le \mu_i + \epsilon_j / 4\,.
	\end{align}
	
	Combining \Cref{eq:improved-best-lower} and \Cref{eq:improved-sub-upper}, we obtain: 
	\begin{align*}
		\hat\mu^{pj}_i - \hat\mu^j_{\max} + \epsilon_j \leq \mu_j - \mu_\star + \epsilon_j / 2 + \epsilon_j = \frac{3}{2}\epsilon_j - \Delta_i < 0\,.
	\end{align*}
	
	The above inequality shows that \(\hat\mu^{pj}_i < \hat\mu^{j}_{\max} - \epsilon_j\) holds due the large value of the gap \(\Delta_i > \frac{3}{2} \epsilon_j\). 
\end{proof}

As \(\epsilon_P \le \frac{\Delta_{[2]}}{4}\) and for any suboptimal arm \(i\) we have \(\Delta_i \ge \Delta_{[2]} \ge 4\epsilon_P > \frac{3}{2}\epsilon_P\), it follows that any suboptimal arm \(i\) will be eliminated in \(P\)-th pass (\Cref{lem:improved-suboptimal-bound}) and optimal arm \(\star\) will not be eliminated by \Cref{lem:improved-best}. Thus, \Cref{alg:improved} outputs the correct arm if the event \(\F\) holds.


\begin{lemma}\label{lem:improved-sample-complexity}
	Conditioning on the event \(\F\) defined in \Cref{eq:event-F} holds, the number of arm pulls used by \Cref{alg:improved} is at most 
	\begin{align*}
		O\left(P \log\left( \frac{1}{\delta}\right) \cdot \sum_{i = 2}^n \frac{n^{2/P}}{\Delta^2_{[i]}}\right).
	\end{align*}
\end{lemma}

\begin{proof}
	% \chen{read this lemma and see if the summation actually converges}
	We split the set of arms \(I\) into two parts, \(B\) be the set arms with big gaps 
	\begin{align*}%\label{eq:improved-big}
		B \triangleq \left\{i \in I \mid \Delta_i > \frac{3n\Delta_{[2]}}{2}\right\}
	\end{align*}
	and small gaps
	\begin{align*}%\label{eq:improved-small}
		S \triangleq \left\{ i \in I \mid \Delta_i \le \frac{3n\Delta_{[2]}}{2}\right\}\,.
	\end{align*}
	
	Similar to the proof of \Cref{lem:bound-pull}, we define $T_{B}$ and $T_{S}$ to be the number of arm pulls used by the arms in $B$ and $S$, and $T\triangleq T_B + T_S$ is the total number of arm pulls. For any sub arm \(i\), we define the value \(p(i) \triangleq \min\{p \ge 0 \mid \Delta_i > \frac{3}{2}\epsilon_p\}\). For the optimal arm \(\star\), we have \(p(\star) = P\) by \Cref{lem:improved-suboptimal-bound}. We note that it is correctly defined because 
	\[\frac{3}{2}\epsilon_P = \frac{3}{2}\frac{\Delta_{[2]}}{4} < \Delta_{[2]}\,,\]
	and so on \(\forall{i \in I} : p(i) \le P\). 
	
	For arms from \(S\) due the definitions of \(\epsilon_r\) and \(p(i)\), we have that 
	\begin{align*}
		\frac{3}{2} n^{1/P} \epsilon_{p(i)} \ge \Delta_i > \frac{3}{2} \epsilon_{p(i)},
	\end{align*}
	and consequently, 
	\begin{align}\label{eq:improved-eps-lower}
		\epsilon_{p(i)} \ge \frac{2\Delta_i}{3 n^{1/P}}\,.
	\end{align}
	
	By \Cref{lem:improved-suboptimal-bound}, we have that the number of pulls for any suboptimal arm \(i \in S\) is bounded by the number of passes \(P + 1\) times \(T_{p(i)}\).  Consequently, by \Cref{eq:improved-eps-lower}, the number of pulls for arm \(i\) from \(S \setminus \{\star\}\)  the number of pulls is bounded
	\begin{align}\label{eq:improved-bound-small-gap}
		P T_{p(i)} \le P \frac{8}{\epsilon^2_{p(i)} \log{e}} \log\left(\frac{2 n (P + 1)^2}{\delta}\right) / \log(e) \le \frac{18 P }{\Delta^2_i \log{e} } \log\left(\frac{2 n (P + 1)^2}{\delta}\right)\,.
	\end{align}
	
	For the optimal arm the number of pulls is trivially bounded by \begin{align}\label{eq:improved-bound-optimal}
		P T_{P} \le P \frac{128}{\Delta_{[2]}^2 \log e} \log\left(\frac{2n{(P + 1)}^2}{\delta}\right)\,.
	\end{align}
	
	For arms from the set \(B\) by \Cref{lem:improved-suboptimal-bound} we have that the number of pulls assigned to an arm \(i \in B\) is bounded by 
	\begin{align*}
		P T_0 \le \frac{128}{n^2\Delta_{[2]}^2 \log e} \log \left(\frac{2n{(P + 1)}^2}{\delta}\right)\,.
	\end{align*}
	We note that the size of \(B\) is bounded by \(n\). Therefore, the total sample complexity for arms from \(B\) is bounded by 
	\begin{align}\label{eq:improved-bound-large-gap}
		n \cdot \frac{128}{n^2\Delta_{[2]}^2 \log e} \log \left(\frac{2n{(P + 1)}^2}{\delta}\right) \le \frac{128}{\Delta_{[2]}^2 \log e} \log \left(\frac{2n{(P + 1)}^2}{\delta}\right)\,.
	\end{align}
	
	As such, combining~\Cref{eq:improved-bound-small-gap}, \Cref{eq:improved-bound-optimal}, and \Cref{eq:improved-bound-large-gap}, we have
	\begin{align*}
		T &= T_{S} + T_{B}\\
		&\leq P \frac{128}{\Delta_{[2]}^2 \log e} \log\left(\frac{2n{(P + 1)}^2}{\delta}\right) + \sum_{i: i\neq \star} P \frac{18}{\Delta_{i}^2 \log e} \log\left(\frac{2n{(P + 1)}^2}{\delta}\right) + T_{B}\tag{by \Cref{eq:improved-bound-small-gap}, \Cref{eq:improved-bound-optimal}}\\
		&\leq O\left(P \log\left( \frac{nP}{\delta}\right) \cdot \sum_{i = 2}^n \frac{n^{2/P}}{\Delta^2_{[i]}}\right) + \frac{128}{\Delta_{[2]}^2 \log e} \log \left(\frac{2n{(P + 1)}^2}{\delta}\right)  \tag{by \Cref{eq:improved-bound-large-gap}}\\
		&= O\left(P \log\left( \frac{nP}{\delta}\right) \cdot \sum_{i = 2}^n \frac{n^{2/P}}{\Delta^2_{[i]}}\right),
	\end{align*} 
	as desired.
\end{proof}

\paragraph{Finalizing the proof of \Cref{thm:improved}.} The algorithm makes $P+1$ passes over the stream by the algorithm design. The memory efficiency is guaranteed by \Cref{lem:improved-memory}, and the correctness and the sample complexity are gueranteed by \Cref{lem:bound-F,lem:improved-best,lem:improved-suboptimal-bound,lem:improved-sample-complexity}. Combining the above completes the proof of \Cref{thm:improved}.

% \chen{Add a remark to discuss that the lower bound still holds for a $\log^2 n$ multiplicative factor.}

\begin{remark}
	\label{rmk:sample-lb-higher-polylog}
	By setting $P=O(\log(n))$, we get a sample complexity bound of $O(\log^2 (n)\cdot \sum_{i=2}^{n}\frac{1}{\Delta^{2}_{[i]}})$. We remark that our lower bound in \Cref{thm:lb-main} can also be extended to the sample complexity of $O(\log^2 (n)\cdot \sum_{i=2}^{n}\frac{1}{\Delta^{2}_{[i]}})$. In fact, by using $B=\Theta(\log(n)/\log\log(n))$, we can already extend the lower bound to $O((\frac{\log(n)}{\log\log(n)})^2\cdot \sum_{i=2}^{n}\frac{1}{\Delta^{2}_{[i]}})$ sample complexity. We can actually strengthen the bound on \Cref{prop:multi-pass-lb} to allow larger exponent on $B$ by using higher gaps between $\chi_{b}$ and $\chi_{b+1}$ (e.g., $\chi_{b+1}= ({1}/{6 C \log(n)})^{100} \cdot \chi_{b}$). To make the lower bound work, we will need to use smaller constant for $P\leq B$, e.g., at most ${1}/{10000}\cdot \log(n)/\log\log(n)$ passes, which is still in the range of $\Omega(\log(n)/\log\log(n))$.
\end{remark}