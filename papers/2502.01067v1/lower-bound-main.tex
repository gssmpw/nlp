\section{Lower Bound: A Sharp Memory-pass Trade-off for Multi-pass Algorithms with Known $\Delta_{[2]}$}
\label{sec:lb-main}

We now introduce the construction and analysis of our main lower bound. 
Our adversarial instance follows the structure of batched instances as in \Cref{def:batch-instance}. On a high level, our instances keep \emph{two} special arms in each batch $b$ with stochastic mean rewards of either $\left(\frac{1}{2}, \frac{1}{2}\right)$ or $\left(\frac{1}{2}+\etaib{1}{b}, \frac{1}{2}+\etaib{2}{b}\right)$. In the latter case, which happens with probability roughly $O(1/B)$, we insist on \emph{invariate} $\etaib{1}{b}-\etaib{2}{b}$, which limits the utility for the knowledge of $\Delta_{[2]}$. 
% \nicksays{what is \(\Delta_{[2]}\), maybe better just say optimality gap in words everywhere or use \(\Delta_{[2]}\)?}. 
Furthermore, we carefully pick the parameters such that the gap between $C\cdot \frac{n}{\left(\etaib{1}{b}\right)^2}$ becomes $\polylog{n}$. Since we only work with a number of passes of $\Theta(\log(n)/\log\log(n))$, the construction allows us to ``reduce'' the $O\left(\sum_{i=2}^{n}\frac{1}{\Delta^2_{[i]}}\right)$ % \nicksays{We didn't define \(O(\sum_{i=2}^{n}\frac{1}{\Delta^2_{[i]}})\)}
sample complexity to the $C\cdot \frac{n}{\left(\etaib{1}{b}\right)^2}$ bound, which in turn allows us to use \Cref{prop:multi-pass-lb} to establish the lower bound.

We now give the formal construction of the instance family.

\begin{tbox}
	$\cP(B, C, \gamma)$: A hard instance distribution for multi-pass MABs algorithms with known $\Delta_{[2]}$. 
	
	\begin{enumerate}
		\item \textbf{Parameters}: Ensure that $\frac{1}{20}\cdot \frac{1}{n^{1/3}}\leq \gamma \leq \frac{1}{10}\cdot \frac{1}{n^{1/3}}$, and let $\chi_{1}=n^{1/3}\cdot \gamma$; furthermore, for any $b\in [B]$, let 
		\[\chi_{b+1} = \paren{\frac{1}{12 C \log(n)}}^{15} \cdot \chi_{b}.\]
		\item \textbf{Division of arms:} Divide the $n$ arms into $(B+1)$ batches of equal sizes, and put them in the \emph{reverse} order of the stream, i.e. $\mathcal{B}_{B+1}$ arrives first, and $\mathcal{B}_{1}$ arrives the last.
		\item \textbf{Sampling special arms: } For each batch $b\in [B+1]$, sample \emph{two} arms uniformly at random (without replacement), and call them \emph{special arms}. Set all the arms \emph{except} the special arms with reward distribution $\bern{1/2}$.
		\item \textbf{Batches $b\in[B]$: } For each $b\in [B]$, sample $\Theta_{b}$ from distribution $\bern{1/2B}$:
		\begin{enumerate}
			\item If $\Theta_{b}=0$, set both special arms with reward distributions $\bern{1/2}$.
			\item Otherwise, if $\Theta_{b}=1$ 
			\begin{itemize}
				\item Set the first special arm with reward distribution $\bern{1/2+\chi_{b}}$.
				\item Set the second special arm with reward distribution $\bern{1/2+\chi_{b}+\gamma}$.
			\end{itemize}
		\end{enumerate}
		\item \textbf{The batch $B+1$: } Always set the reward distributions of the special arms as follows ($\Theta_{B+1}=1$ deterministically) % \nicksays{constant?}) 
		\begin{itemize}
			\item Set the first special arm with reward distribution $\bern{1/2+\chi_{B+1}}$.
			\item Set the second special arm with reward distribution $\bern{1/2+\chi_{B+1}+\gamma}$.
		\end{itemize}
	\end{enumerate}
\end{tbox}


\begin{figure}
	\centering
	\begin{subfigure}{0.95\textwidth}
		\centering
		\includegraphics[scale=0.18]{figs/batched-instance-general.png}
		\caption{General $(B+1)$-Batched Instance Distribution}
		\label{fig:batched-instance}
	\end{subfigure}%
	% leave a blank line to change row 
	
	\begin{subfigure}{0.95\textwidth}
		\centering
		\includegraphics[scale=0.18]{figs/streaming-adversarial-instance.png}
		\caption{$\cP(B, C, \gamma)$ Instance distribution}
		\label{fig:streaming-adv-instance}
	\end{subfigure}
	\caption{An illustration of the general $(B+1)$-batched instance distribution (\Cref{def:batch-instance}) and the $\cP(B, C, \gamma)$ instance distribution. The mean rewards of arms are ranked in the decrement order from left to right for illustration purposes -- their positions inside the batches are uniformly at random.}
	\label{fig:lb-instance-illus}
\end{figure}


An illustration of the distribution $\cP(B, C, \gamma)$ can be shown as \Cref{fig:streaming-adv-instance}. It is straightforward to observe that the $\cP(B, C, \gamma)$ family follows the $(B+1)$-batched instance as in \Cref{def:batch-instance}. More concretely, in $\cP(B, C, \gamma)$, the arms are divided into $(B+1)$ batches, we have $S=2$ and the values of $\etaib{i}{b}$ as functions of $C$, and the probability functions are $f_{b}(B)=\frac{1}{2B}$ for all $b \in [B]$ and $f_{B+1}(B)=1$. Furthermore, we make the crucial observation that $\Delta_{[2]}$ is invariant across different settings.

\FloatBarrier




\begin{observation}
	\label{obs:Delta-invariate}
	For any instance in $\cP(B, C, \gamma)$, the value of $\Delta_{[2]}$ is equal to $\gamma$. In other words, in $\cP(B, C, \gamma)$, for all $b\in[B+1]$, there is 
	\begin{align*}
		\paren{\Delta_{[2]} \mid \Theta_{<b}=0, \Theta_{b}=1} = \gamma.
	\end{align*} % \nicksays{Maybe better to say this in words?}
\end{observation}

% \chen{Factor in the knowledge of $\Delta$, which is not written explicitly in the current version.}

We now use $\cP(B, C, \gamma)$ to state our main multi-pass lower bound. 
\begin{theorem}[Formalization of \Cref{rst:main-lb}]
	\label{thm:lb-main}
	% For any $1\leq P \leq \frac{1}{100}\cdot \frac{\log(n)}{\log\log(n)}$, 
	There exists a family of streaming MABs instances $\cP$, such that any streaming algorithm (deterministic or randomized) that given the quantity of $\Delta_{[2]}$, finds the best arm from an instance sampled from $\cP$ with an \emph{expected} sample complexity of $O\paren{\sum_{i=2}^{n} \frac{1}{\Delta^2_{[i]}} \cdot \log(n)}$, a success probability of at least $1999/2000$, and a memory of $o\paren{n/\log^3 {n}}$ arms has to make $\Omega\paren{\frac{\log(n)}{\log\log(n)}}$ passes over the stream. 
\end{theorem}


To prove \Cref{thm:lb-main}, the rest of this section is dedicated to two parts. We first show that the family of $\cP(B, C, \gamma)$ \emph{with $B=\Theta(\frac{\log n}{\log\log n})$} satisfies the conditions characterized by \Cref{prop:multi-pass-lb}. As such, to ensure the success probability is high, any algorithm must break the sample upper bound of \Cref{equ:batch-sample-ub}. Subsequently, we show that to keep the expected sample complexity of $O(\sum_{i=2}^{n}\frac{1}{\Delta^2_{[i]}})$, % \nicksays{remove this}, 
the sampling upper bound of \Cref{equ:batch-sample-ub} has to be satisfied, which forms a contradiction for the proof of \Cref{thm:lb-main}.

More concretely, the first part of the argument can be summarized as \Cref{lem:hard-B-dist}. (Note that in the lemma, we do not assume the knowledge of $\Delta_{[2]}$, as we will deal with it in the proof of \Cref{thm:lb-main} later.)
\begin{lemma}
	\label{lem:hard-B-dist}
	Let $C\geq 1$ be a fixed integer and $\gamma \leq \frac{1}{10}\cdot \frac{1}{n^{1/3}}$ be a real number. Let $B=\frac{1}{100C}\cdot \frac{\log n}{\log\log(n)}$, and let $\ALG$ be any deterministic $P$-pass streaming algorithm such that $P\leq B$. Suppose that $\ALG$ uses a memory of at most $\frac{1}{30000}\cdot \frac{n}{B^3}$ arms. 
	Additionally, suppose on instances of distribution $\cP(B, C, \gamma)$ and every $b \in [B+1]$, $\ALG$ satisfies:
	\[
	\Exp\bracket{\smp \mid\Theta_{b}=1, \Theta_{<b}=0} \leq C \cdot B^2 \cdot \frac{n}{(\chi_{b}+\gamma)^2}, 
	\]
	where the randomness is taken over the choice of the instance $I \sim \cP(B, C, \gamma) \mid \Theta_{b}=1, \Theta_{<b}=0$. % \nicksays{we need brackets, what is \(\smp\)?} 
	Then, the probability that $\ALG$ can output the best arm for $I \sim \cP(B, C, \gamma)$ is strictly less than $999/1000$.  
\end{lemma}
\begin{proof}
	We prove the lemma by showing that the distribution $\cP(B, C, \gamma)$ satisfied the conditions prescribed by \Cref{prop:multi-pass-lb}, which will allow us to directly use the conclusion therein. To this end, we verify \textbf{C1}, \textbf{C2}, and \textbf{C3}, respectively:
	\begin{itemize}
		\item Condition \textbf{C1}. We use \Cref{lem:arm-trapping} to argue this property. For any batch $b\in [B+1]$, we use $\alpha=\chi_{b}$ and $\beta=\gamma$. Since we set $\gamma\leq \frac{1}{10}\cdot \frac{1}{n^{1/3}}$, there is clearly $\chi_{b}+\gamma<\frac{1}{6}$. Furthermore, we let $k=\frac{n}{B+1}$ as the number of arms in each batch. Suppose for the purpose of contradiction that \textbf{C1} does \emph{not} hold. By our construction, we have $\etaib{1}{b}=\chi_{b}+\gamma$, and the assumption implies an algorithm that
		\begin{enumerate}
			\item uses at most $\frac{1}{700}\cdot \frac{\tau^3}{(\chi_{b}+\gamma)^2}\cdot \frac{n}{B+1}<\frac{1}{600}\cdot\frac{\tau^3}{(\alpha+\beta)^2}\cdot k$ arm pulls;
			\item outputs a collection of $\frac{1}{20}\frac{\tau n}{B+1}=\frac{\tau \cdot k}{20}$ arms, in which contains an arm with reward strictly more than $\frac{1}{2}$ with probability at least $\tau$,
		\end{enumerate}
		which forms a contradiction with \Cref{lem:arm-trapping}. Therefore, the condition \textbf{C1} has to be satisfied.
		
		\item Condition \textbf{C2}. We use \Cref{lem:batch-arm-learning} to verify this property. Again, we use $\alpha=\alpha_{b}$ and $\beta=\gamma$. Furthermore, let $\mathcal{E}$ be any event we want to condition on, and we let $\nu=\Pr\paren{\Theta_{b}=1\mid \mathcal{E}}$ be the probability for the distribution in the \emph{yes} case from the algorithm's internal view. Now, to use \Cref{lem:batch-arm-learning}, we simply set $\rho=\nu$, and if condition \textbf{C2} is not satisfied, the output distribution of the transcripts will violate \Cref{lem:batch-arm-learning}. Thus, condition \textbf{C2} must be followed in $\cP(B, C, \gamma)$.
		
		\item Condition \textbf{C3}. Note that we have $B= \frac{1}{100C}\cdot \frac{\log n}{\log\log n}$. As a result, we also have 
		\begin{equation}
			\label{equ:a-B-lower-bound}
			\begin{aligned}
				\chi_{b}\geq \chi_{B} & = \gamma \cdot n^{1/3} \cdot \paren{\frac{1}{12 C\log n}}^{\frac{10\log n}{100 C\log\log(n)}}\\
				&= \gamma \cdot n^{1/3} \cdot \paren{\frac{1}{12C}}^{\frac{\log n}{10C \log\log(n)}} \cdot \paren{\frac{1}{\log{n}}}^{\frac{\log n}{10C \log\log(n)}}\\
				& \geq \gamma \cdot n^{1/3}\cdot \paren{\frac{1}{2}}^{^{\frac{\log n}{\log\log(n)}}}\cdot \paren{\frac{1}{\log n}}^{\frac{\log n}{10C \log\log(n)}}\\
				& = \gamma \cdot n^{1/3}\cdot \frac{1}{n^{1/10C + o(1)}}\\
				& \geq \gamma\cdot n^{1/5},
			\end{aligned}
		\end{equation}
		where the second inequality if because $(\frac{1}{12C})^{\frac{1}{10C}}\geq \frac{1}{2}$ for any $C\geq 1$.
		Therefore, we have $\gamma\leq n^{-1/5}\chi_{b}$ for any choice of $\gamma$. As such, for any $r>b$, there is 
		\begin{align*}
			\frac{\etaib{1}{r}}{\etaib{1}{b}} &\leq \frac{\chi_{b+1}+\gamma}{\chi_{b}+\gamma}\\
			&\leq \frac{\chi_{b+1}\cdot \log n}{\chi_{b}} \tag{by $\gamma\leq n^{-1/5}\chi_{b}$ for sufficiently large $n$}\\
			&\leq \paren{\frac{1}{12 C \log(n)}}^{15} \\
			&\leq \paren{\frac{1}{6 B C}}^{15}, \tag{by $B\leq \log{n}$}
		\end{align*}
		which verifies the validity of condition \textbf{C3}.
	\end{itemize} 
	Finally, we observe that the memory and sample bound in \Cref{lem:hard-B-dist} matches the bound for deterministic algorithms in \Cref{prop:multi-pass-lb}, concluding the proof. 
\end{proof}

We are now ready to wrap up the proof of our main lower bound of \Cref{thm:lb-main}.

\begin{proof}[Proof of \Cref{thm:lb-main}]
	% \chen{There is a small hidden bug -- fix the issue.}
	We focus on deterministic algorithms in the proof with success probability $\frac{999}{1000}$. The lower bound for randomized algorithms can be obtained by an application of Yao's minimax principle.
	
	We first deal with algorithms that do \emph{not} have the \emph{a priori} knowledge of $\Delta_{[2]}$. Assume the purpose of contradiction that there exists a $P$-pass streaming algorithm that uses
	\begin{enumerate}
		\item A memory of at most $\frac{1}{20000}\cdot \frac{n}{\log^3 n}$ arms;
		\item A success probability of at least $\frac{999}{1000}$;
		\item A sample complexity of $C' \cdot \log(n)\cdot \sum_{i=2}^{n}\frac{1}{\Delta^2_{[i]}}$;
		\item The number of passes $P$ satisfies $P\leq \frac{1}{100C}\cdot \frac{\log(n)}{\log\log(n)}$.
	\end{enumerate}
	Then, we can pick $C=2C'$ to construct the hard family $\mathcal{P}(\frac{1}{100C}\cdot \frac{\log(n)}{\log\log(n)}, C, \gamma)$ (pick any suitable $\gamma$). Observe that following the same calculation of \Cref{equ:a-B-lower-bound} and by the property of the distribution, for any $b\in[B+1]$, there is
	\begin{align*}
		\chi_{b}\geq \chi_{B} & = \gamma \cdot n^{1/3} \cdot \paren{\frac{1}{12 C\log n}}^{\frac{10\log n}{100 C\log\log(n)}}\\
		&= \gamma \cdot n^{1/3} \cdot \paren{\frac{1}{12C}}^{\frac{\log n}{10C \log\log(n)}} \cdot \paren{\frac{1}{\log{n}}}^{\frac{\log n}{10C \log\log(n)}}\\
		& \geq \gamma \cdot n^{1/3}\cdot \paren{\frac{1}{2}}^{^{\frac{\log n}{\log\log(n)}}}\cdot \paren{\frac{1}{\log n}}^{\frac{\log n}{10C \log\log(n)}}\\
		& = \gamma \cdot n^{1/3}\cdot \frac{1}{n^{1/10C + o(1)}}\\
		& \geq \gamma\cdot n^{1/5}.
	\end{align*}
	Furthermore, by the upper and lower bound on $\gamma$, we have that for $b\in[B+1]$, there is
	\begin{align*}
		\frac{1}{\gamma^2} &\leq 400 \cdot n^{2/3} \tag{by $\gamma\geq \frac{1}{20}\cdot \frac{1}{n^{1/3}}$}\\
		&\leq 50 \cdot n \tag{for sufficiently large $n$}\\
		&\leq \frac{n}{(\chi_{1}+\gamma)^2} \tag{$\chi_{1}\leq \frac{1}{10}$ and $\gamma \leq \frac{1}{n^{1/5}}\chi_{1}$}\\
		&\leq \frac{n}{(\chi_{b}+\gamma)^2} = \frac{n}{(\etaib{1}{b})^2}. \tag{$\chi_{b}\leq \chi_{1}$} 
	\end{align*}
	As such, it is straightforward to see that
	\begin{align*}
		\paren{C'\cdot \log(n) \cdot \sum_{i=2}^{n}\frac{1}{\Delta^2_{[i]}} \middle| \Theta_{b}=1, \Theta_{<b}=0} &\leq C'\cdot \log(n) \cdot \left(\frac{1}{\gamma^2} + \frac{n}{(\etaib{1}{b})^2}\right)\\
		&\leq 2 C'\cdot \log(n)\cdot \frac{n}{(\etaib{1}{b})^2}.
	\end{align*}
	Therefore, if a deterministic algorithm $\ALG$ satisfies: 
	\[\Exp\bracket{\smp} \leq C' \cdot \log(n)\cdot \sum_{i=2}^{n}\frac{1}{\Delta^2_{[i]}},\] it implies that the algorithm has sample complexity of 
	\[\Exp\bracket{\smp \mid \Theta_{b}=1, \Theta_{<b}=0}\leq C \cdot \log(n) \cdot \frac{n}{(\etaib{1}{b})^2}.\]
	This leads to a contradiction with \Cref{lem:hard-B-dist}, which implies that such a streaming algorithm cannot exist.
	
	Finally, to complete the proof, we note that on the constructed family $\mathcal{P}(\frac{1}{100C}\cdot \frac{\log(n)}{\log\log(n)}, C, \gamma)$, there is always $\Delta_{[2]}=\gamma$ by \Cref{obs:Delta-invariate}. As such, if we have a streaming algorithm $\widetilde{\ALG}$ that only works with a \emph{known} $\Delta_{[2]}$, we can simulate the algorithm without this prior knowledge by running $\widetilde{\ALG}$ as an inner streaming algorithm with parameter $\Delta_{[2]}=\gamma$. The contradiction still holds, concluding the proof.
\end{proof}


\begin{remark}
	By Remark 5.8 of \cite{AW23BestArm}, the $B^2$ term in \Cref{equ:batch-sample-ub} could be made to $B^C$ for any fixed constant $C$ with larger gaps between $\etaib{1}{b}$ values for different $b\in[B]$. Therefore, we could strenghthen the sample complexity lower bound in \Cref{thm:lb-main} to $O\paren{\sum_{i=2}^{n} \frac{1}{\Delta^2_{[i]}} \cdot \polylog(n)}$.
\end{remark}
