\section{Related works}
\label{appendix:related_works}
Realizing there are a lot of methods related to latent disentanglement, we provide Tab.~\ref{tab:related_works} with a list to summarize their contributions and differences.

\begin{table}[!ht]
    \centering
    \caption{Related works}
    \begin{tabular}{lcc}
        \toprule
         & full disentanglement & partial disentanglement \\
        \midrule
        By prior (not flexible) & [1] & [4] \\
        By extra penalty to loss (flexible) & [2][3] & Our PDisVAE \\
        By auxiliary information (supervised) & [7] & \\
        Others & [5][6][8][9][10] & \\
        \bottomrule
    \end{tabular}
    \label{tab:related_works}
\end{table}

$\bullet$ [1] ICA \citep{hyvarinen2000independent}: Traditional ICA uses a non-Gaussian prior to achieving full disentanglement since independence is non-Gaussian from the statistical perspective. However, the choice of the non-Gaussian prior is critical and might be too rigid, hurting the flexibility of the method. \\
$\bullet$ [2] FactorVAE \citep{kim2018disentangling} [3] $\beta$-TCVAE \citep{chen2018isolating}: These two papers start from the statistical definition of full independence to add an extra total correlation to achieve full independence rigorously. The only difference between these two papers is their implementations of minimizing TC. \\
$\bullet$ [4] ISA-VAE \citep{stuhmer2020independent}: ISA-VAE realized the commonly existing group-wise independence (partial disentanglement) in the real-world data. It utilizes a group-wise independent prior called $L^p$-nested distribution to achieve the partial disentanglement. However, they did not validate their approach on partially disentangled synthetic datasets, but merely evaluated their approach using fully disentangled assumptions for dsprites and CelebA datasets. \\
$\bullet$ [5] $\beta$-VAE \citep{burgess2018understanding}: Directly penalize the KL divergence of the VAE ELBO loss, in which TC (in Eq.~\eqref{eq:decomposed_KL}) is implicitly penalized. This approach has been proven to be worse than $\beta$-VAE and FactorVAE. \\
$\bullet$ [6] \citep{locatello2019challenging}: This research presented common challenges in finding disentangled latent through an unsupervised approach, implying supervision with semantic latent labels might be necessary under the assumption of full latent disentanglement. This also gives us a hint that full disentanglement might be a strong and inappropriate assumption and could result in poor latent interpretation. \\
$\bullet$ [7] \citep{ahuja2022weakly}: This paper uses weak supervision from observations generated by sparse perturbations of the latent variables, which requires auxiliary information to the latent variables. \\
$\bullet$ [8] \citep{meo2024alpha}: This paper replace the traditional TC term with a novel TC lower bound to achieve not only disentanglement but generalized observation diversity. \\
$\bullet$ [9] \citep{bhowalvariational}: This paper claims that VAE with orthogonal structure could also achieve latent full disentanglement. \\
$\bullet$ [10] \citep{hsu2024disentanglement}: The full disentanglement is achieved by a technique called latent quantization. The approach is quantizing the latent space into discrete code vectors with a separate learnable scalar codebook per dimension. Besides, weight decay is also applied to the model regularization for better full disentanglement.

\clearpage