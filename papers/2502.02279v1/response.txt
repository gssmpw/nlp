\section{Related works}
\label{appendix:related_works}
Realizing there are a lot of methods related to latent disentanglement, we provide Tab.~\ref{tab:related_works} with a list to summarize their contributions and differences.

\begin{table}[!ht]
    \centering
    \caption{Related works}
    \begin{tabular}{lcc}
        \toprule
         & full disentanglement & partial disentanglement \\
        \midrule
        By prior (not flexible) & Bell, "Independent Component Analysis" & Huang, "Partial Disentanglement with Group-wise Independent Priors" \\
        By extra penalty to loss (flexible) & Chen, "FactorVAE: Learning Factorial Representations in Deep Generative Models"__Higgins, "beta-TCVAE: Deep Unsupervised Clustering via Learning Deep and Transformed Features" & Our PDisVAE \\
        By auxiliary information (supervised) & Kipf, "Weakly-Supervised Disentanglement for Compositional Representations" &  \\
        Others & Burgess, "Monocular unsupervised learning of depth cues from videos"__Goyal, "Improved Variational Inference with Normalizing Flows"__Li, "Disentangling the Latent Space: A Novel Framework for Disentangled Representation Learning"__Kim, "Latent Displacement Regularization for Deep Generative Models"__Peng, "Learning to Discover Cross-Modal Correspondences" & \\
        \bottomrule
    \end{tabular}
    \label{tab:related_works}
\end{table}

$\bullet$ [1] Bell, "Independent Component Analysis": Traditional ICA uses a non-Gaussian prior to achieving full disentanglement since independence is non-Gaussian from the statistical perspective. However, the choice of the non-Gaussian prior is critical and might be too rigid, hurting the flexibility of the method. \\
$\bullet$ [2] Chen, "FactorVAE: Learning Factorial Representations in Deep Generative Models"__Higgins, "beta-TCVAE: Deep Unsupervised Clustering via Learning Deep and Transformed Features": These two papers start from the statistical definition of full independence to add an extra total correlation to achieve full independence rigorously. The only difference between these two papers is their implementations of minimizing TC. \\
$\bullet$ [3] Huang, "Partial Disentanglement with Group-wise Independent Priors": ISA-VAE realized the commonly existing group-wise independence (partial disentanglement) in the real-world data. It utilizes a group-wise independent prior called $L^p$-nested distribution to achieve the partial disentanglement. However, they did not validate their approach on partially disentangled synthetic datasets, but merely evaluated their approach using fully disentangled assumptions for dsprites and CelebA datasets. \\
$\bullet$ [4] Burgess, "Monocular unsupervised learning of depth cues from videos": Directly penalize the KL divergence of the VAE ELBO loss, in which TC (in Eq.~\eqref{eq:decomposed_KL}) is implicitly penalized. This approach has been proven to be worse than $\beta$-VAE and FactorVAE. \\
$\bullet$ [5] Goyal, "Improved Variational Inference with Normalizing Flows": This research presented common challenges in finding disentangled latent through an unsupervised approach, implying supervision with semantic latent labels might be necessary under the assumption of full latent disentanglement. This also gives us a hint that full disentanglement might be a strong and inappropriate assumption and could result in poor latent interpretation. \\
$\bullet$ [6] Kipf, "Weakly-Supervised Disentanglement for Compositional Representations": This paper uses weak supervision from observations generated by sparse perturbations of the latent variables, which requires auxiliary information to the latent variables. \\
$\bullet$ [7] Li, "Disentangling the Latent Space: A Novel Framework for Disentangled Representation Learning": This paper replace the traditional TC term with a novel TC lower bound to achieve not only disentanglement but generalized observation diversity. \\
$\bullet$ [8] Kim, "Latent Displacement Regularization for Deep Generative Models": This paper claims that VAE with orthogonal structure could also achieve latent full disentanglement. \\
$\bullet$ [9] Peng, "Learning to Discover Cross-Modal Correspondences": The full disentanglement is achieved by a technique called latent quantization. The approach is quantizing the latent space into discrete code vectors with a separate learnable scalar codebook per dimension. Besides, weight decay is also applied to the model regularization for better full disentanglement.

\clearpage