\section{Related Work}

% \paragraph{Rotary Positional Embedding (RoPE)}
% RoPE~\cite{su2024roformer} has emerged as a pivotal mechanism for encoding positional information in long-context modeling. Through the use of a rotation matrix, RoPE unifies the advantages of both absolute and relative positional embedding schemes. Its simplicity and effectiveness have led to its widespread adoption in leading large language models (LLMs)~\cite{touvron2023llamaopenefficientfoundation, yang2024qwen2, gemmateam2024gemmaopenmodelsbased,cai2024internlm2, Sun2024MOSS}.  

% \paragraph{Extending RoPE to Multi-Modal Data}
% In the context of multi-modal large language models (MLLMs), vanilla RoPE~\cite{su2024roformer} is typically applied without modifications, treating diverse modalities, such as text and vision, equivalently. This uniform treatment neglects inherent differences between modalities, potentially compromising performance. PixTral~\cite{agrawal2024pixtral12b} and Qwen2-VL~\cite{wang2024qwen2} incorporate RoPE-2D into their vision modules for visual embedding. This extension improves spatial representation for image extrapolation and resolution scaling but overlooks temporal dynamics and spatiotemporal correlations in video data. In contrast, TAD-RoPE~\cite{gao2024tc} employs a one-dimensional RoPE to capture temporal dynamics across video frames. While this approach effectively models inter-frame relationships, it neglects the critical two-dimensional spatial information within each frame. This limits TAD-RoPEâ€™s ability to exploit the spatiotemporal structure of video data. Furthermore, M-RoPE~\cite{wang2024qwen2} extends RoPE to three dimensions to model both temporal and spatial information but encounters limitations in spatial representation, frequency handling, and step size configuration. To address these limitations, we introduce \methodname, a novel three-dimensional positional encoding scheme that considers the inherent differences between visual and textual tokens, optimizing positional representation, frequency partitioning, and step size configuration.

\noindent \textbf{RoPE (Rotary Position Embedding).}
RoPE~\cite{su2024roformer} is a pivotal mechanism for encoding positional information in LLM long-context modeling. Using a rotation matrix, RoPE unifies the advantages of both absolute and relative positional embedding schemes. 
In RoPE design, different feature dimensions are embedded with position information based on Trigonometric functions $\sin$ and $\cos$ with different frequencies~\cite{peng2023yarn,liu2023scaling}.
Lower dimensions correspond to higher frequency given larger values of base frequency.
The simplicity and effectiveness of RoPE have led to its widespread adoption in leading LLMs~\cite{touvron2023llamaopenefficientfoundation, yang2024qwen2, gemmateam2024gemmaopenmodelsbased,cai2024internlm2, Sun2024MOSS}.

\noindent \textbf{Extending RoPE to Multi-Modal Data.}
Extending RoPE to multi-modal or Video LLMs typically follows two approaches.
One approach directly applies standard RoPE, flattening visual tokens and treating text and visual tokens as a single 1D sequence. Although variants (e.g., TAD-RoPE \cite{gao2024tc}) introduce enhancements in indexing and attention mechanisms, these 1D RoPE variants overlook the spatiotemporal structure of video and inherent inter-modal differences \cite{kexuefm10040,kexuefm10352,wang2024qwen2}.
In contrast, several studies have explored incorporating structural information to formulate the 2D/3D RoPE.
For example, some previous works \cite{agrawal2024pixtral12b,wang2024qwen2} integrate RoPE-2D into visual encoders to improve spatial representation, particularly for resolution scaling.
Based on the RoPE-Tie \cite{kexuefm10040}, M-RoPE \cite{wang2024qwen2} used in QWen2-VL further generalizes RoPE to three dimensions to model both temporal and spatial dynamics.
While effective, M-RoPE exhibits limitations, such as struggles with distractors in our V-NIAH-D task.
This work presents a comprehensive analysis of the important characteristics essential for extending RoPE to video and proposes \methodname according to our analysis.