
\input{figures/attention_analysis}

\section{Analysis}

\paragraph{3D Structure}
The vanilla RoPE defines a matrix $\bm{A}_{t_1,t_2}$ that represents the relative positional encoding between two positions $t_1$ and $t_2$ in a 1D sequence:
\begin{equation}
\begin{aligned}
\bm{A}_{t_1,t_2}&=\left(\bm{q}_{t_1}\bm{R}_{t_1}\right){\left(\bm{k}_{t_2}\bm{R}_{t_2}\right)}^\top \\
&= \bm{q}_{t_1}\bm{R}_{t_1}\bm{R}_{t_2}^\top\bm{k}_{t_2}^\top = \bm{q}_{t_1}\bm{R}_{t_1-t_2}\bm{k}_{t_2}^\top,
\end{aligned}
\end{equation}
where $\bm{R}$

\begin{equation}
\resizebox{0.5\textwidth}{!}{
\scriptsize
\begin{gathered}
\bm{A}_{t_1,t_2}=\left(\bm{q}_{t_1}\bm{R}_{t_1}\right){\left(\bm{k}_{t_2}\bm{R}_{t_2}\right)}^\top = \bm{q}_{t_1}\bm{R}_{t_1}\bm{R}_{t_2}^\top\bm{k}_{t_2}^\top = \bm{q}_{t_1}\bm{R}_{t_1-t_2}\bm{k}_{t_2}^\top \\
={\begingroup
\setlength\arraycolsep{1pt}
\begin{pmatrix}q^{(0)}\\q^{(1)}\\\vdots\\q^{(126)}\\q^{(127)}\end{pmatrix}^\top
\begin{pmatrix}
\cos{\theta_0\Delta t}& -\sin{\theta_0\Delta t}&\cdots&0&0\\
\sin{\theta_0\Delta t}&\cos{\theta_0\Delta t}&\cdots&0&0 \\
\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&\cdots&\cos{\theta_{63}\Delta t}& -\sin{\theta_{63}\Delta t}\\
0&0&\cdots&\sin{\theta_{63}\Delta t}&\cos{\theta_{63}\Delta t}
\end{pmatrix}
\begin{pmatrix}k^{(0)}\\k^{(1)}\\\vdots\\k^{(30)}\\k^{(31)}\end{pmatrix}
\endgroup} \\
\Delta t=t_1-t_2,\quad \theta_n=\beta^{-\dfrac{2n}{d}},\quad n=0,\cdots,d/2-1
\end{gathered}
}
\raisebox{-5.5ex}{.}
\label{equ:rope}
\end{equation}

Lower dimensions correspond to higher frequency given larger values of $\theta_n$.

% \subsection{Preliminary}

% \paragraph{RoPE}

% \cite{su2024roformer}
% vanilla RoPE is designed to encode relative positional information into the self-attention mechanism of Transformers by leveraging absolute positional encodings through rotational transformations. 

% Suppose we have the $n$-th query vector $q_n$ and the $m$-th key vector $k_m$ in $\mathbb{R}^{1 \times d}$. vanilla RoPE leverages the complex exponential $e^{i\theta}$ to transform absolute positional information into rotational phase information on a circular manifold, resulting in the representations $q{e^{in\theta}}$ and $k{e^{im\theta}}$ for the query and key, respectively. For any position 
% n, its matrix representation is:
% \setlength{\abovedisplayskip}{10pt}
% \setlength{\belowdisplayskip}{10pt}
% \begin{equation}
% \resizebox{0.5\textwidth}{!}{
% \scriptsize\underbrace{
% \begin{pmatrix}
% 	\cos{n\theta_1}& -\sin{n\theta_1}&0&0&\cdots&0&0\\
% 	\sin{n\theta_1}&\cos{n\theta_1}&0&0&\cdots&0&0 \\
% 	0&0&\cos{n\theta_2}& -\sin{n\theta_2}&\cdots&0&0\\
% 	0&0&\sin{n\theta_2}&\cos{n\theta_2}&\cdots&0&0 \\
% 	\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
% 	0&0&0&0&\cdots&\cos{n\theta_{d/2}}& -\sin{n\theta_{d/2}}\\
% 	0&0&0&0&\cdots&\sin{n\theta_{d/2}}&\cos{n\theta_{d/2}}
% \end{pmatrix}}_{\boldsymbol{\mathcal{R}}_n}
% \begin{pmatrix}q_0 \\ q_1 \\ q_2 \\ q_3 \\ \vdots \\ q_{d-2} \\ q_{d-1}\end{pmatrix}
% }
% \raisebox{-5.5ex}{.}
% \label{eq:vanilla_rope_freq}
% \end{equation}

% In other words, multiplying the vector \( q \) at position \( n \) by the matrix \( R_n \), and the vector \( k \) at position \( m \) by the matrix \( R_m \), and then performing Attention with the transformed \( Q \) and \( K \) sequences, will automatically include the relative position information in the Attention, because the following identity holds:

% \setlength{\abovedisplayskip}{5pt}
% \setlength{\belowdisplayskip}{5pt}
% \begin{equation}\label{formula_2}
% (\bm{q}\bm{R}_n){(\bm{k}\bm{R}_m)}^\top = \bm{q}\bm{R}_n \bm{R}_m^\top \bm{k}^\top = \bm{q} \bm{R}_{m - n} \bm{k}^\top.
% \end{equation}

% The positional coordinate information of vanilla RoPE is expressed as: 
% \begin{equation}
%     \mathbf{P}(n) = n,
%     \label{eq:vanilla_rope_pos}
% \end{equation}
% where \( n \) represents the \( n \)-th token in the sequence.
% % \vspace{-10pt}

% \paragraph{M-RoPE}

% M-RoPE enhances vanilla RoPE by extending it into three dimensions—temporal, height, and width—effectively incorporating both temporal and spatial data into its positional encoding. Although it preserves the attention computation framework of vanilla RoPE as shown in formula~\ref{formula_2}, M-RoPE adjusts the rotation approach and reallocates positional coordinates. The positional coordinate information is defined as follows:
% \begin{equation}
% \mathbf{P}(t, x, y)= \begin{cases}
% (t, t, t), &\text{if text token,} \\
% (t, x, y), &\text{if vision token.}
% \end{cases}
% \end{equation}
% M-RoPE is functionally identical to vanilla RoPE~\cite{su2024roformer} for text, as identical position IDs are used. Image tokens have fixed temporal IDs, with height and width IDs reflecting spatial positions. For videos, temporal IDs increment per frame, while spatial IDs mirror those of images. In multimodal inputs, position numbering starts by adding to the previous modality's max ID.

% The rotation method of M-RoPE is defined as follows:
% % \begin{equation}
% % \resizebox{0.5\textwidth}{!}{
% % \scriptsize
% % \begin{pmatrix}
% % 	\cos{t\theta_1}& -\sin{t\theta_1}&\cdots&0&0&\cdots&0&0\\
% % 	\sin{t\theta_1}&\cos{t\theta_1}&\cdots&0&0&\cdots&0&0 \\
% %     \vdots&\vdots&\ddots&\vdots&\vdots&\ddots&\vdots&\vdots\\
% % 	0&0&\cdots&\cos{x\theta_i}& -\sin{x\theta_i}&\cdots&0&0\\
% % 	0&0&\cdots&\sin{x\theta_i}&\cos{x\theta_i}&\cdots&0&0 \\
% % 	\vdots&\vdots&\ddots&\vdots&\vdots&\ddots&\vdots&\vdots\\
% % 	0&0&\cdots&0&0&\cdots&\cos{y\theta_{d/2}}& -\sin{y\theta_{d/2}}\\
% % 0&0&\cdots&0&0&\cdots&\sin{y\theta_{d/2}}&\cos{y\theta_{d/2}}
% % \end{pmatrix}
% % \begin{pmatrix}q_0 \\ q_1 \\ \vdots \\ q_{i-2} \\ q_{i-1} \\ \vdots \\ q_{d-2} \\ q_{d-1}\end{pmatrix}
% % }
% % \raisebox{-5.5ex}{.}
% % \label{eq:m_rope_freq}
% % \end{equation}
% Let \(t\) occupy the range \((0, i-1)\), \(x\) span \((i, j-1)\), and \(y\) cover \((j, d/2)\). Specifically, in a 128-dimensional transformer head, M-RoPE assigns 32 dimensions to \(t\) and 48 dimensions each to \(x\) and \(y\).


% \subsection{Analysis}
V-NIAH \cite{zhang2024longva} is specifically developed to test the model's handling of extended visual contexts, as well as its ability to locate and retrieve long-distance visual data. However, this straightforward retrieval-based task is shown as only a superficial form of long-context understanding in long-context LLMs~\cite{hsieh2024ruler}. To make the evaluation effect more distinguishable, inspired by the subsequent long-context benchmarks in LLM including RULER~\cite{hsieh2024ruler} and LV-Eval\cite{yuan2024lv}, we propose V-NIAH-D and enhance V-NIAH by incorporating additional distractors, reducing the likelihood of the model answering correctly through guesswork. Specifically, we generate distractors similar to the needle frame using Google Image Search~\cite{googleimagesearch} or Flux.1  [dev]~\cite{flux2023}. These distractors must be semantically similar to the needle frame and unambiguous with regard to answering the question, as shown in Figure \ref{fig:v-ruler}. 

As discussed in the introduction, Qwen2-VL shows an evident performance decline in V-NIAH-D. To further analyze the issue of Qwen2-VL, inspired by analysis in long-context LLMs~\citep{xiao2023efficient,liu2023scaling,barbero2024round}, we visualize the attention score of a bad case as shown in Figure~\ref{fig:attention_analysis}. Since M-RoPE in Qwen2-VL divides the 128 feature dimensions into 3 parts, the first 32 dimensions modeling temporal positions $t$, the middle 48 dimensions for horizontal positions $x$, and the last 48 dimensions for vertical positions $y$ as shown in Equation~\ref{equ:mrope}, we divide the attention score into 3 corresponding parts and visualize them.
% 32, 48, 48; 0, 31, 32, 79; 80, 127
\begin{equation}
\resizebox{0.5\textwidth}{!}{
\scriptsize
\begin{gathered}
\bm{A}_{(t_1,x_1,y_1),(t_2,x_2,y_2)}=\bm{q}_{(t_1,x_1,y_1)}\bm{k}_{(t_2,x_2,y_2)}^\top \\
=\underbrace{\begingroup
\setlength\arraycolsep{1pt}
\begin{pmatrix}q^{(0)}\\q^{(1)}\\q^{(2)}\\q^{(3)}\\\vdots\\q^{(30)}\\q^{(31)}\end{pmatrix}^\top
\begin{pmatrix}
% \setstacktabbedgap{2pt}
\cos{\theta_0\Delta t}& -\sin{\theta_0\Delta t}&0&0&\cdots&0&0\\
\sin{\theta_0\Delta t}&\cos{\theta_0\Delta t}&0&0&\cdots&0&0 \\
0&0&\cos{\theta_1\Delta t}& -\sin{\theta_1\Delta t}&\cdots&0&0\\
0&0&\sin{\theta_1\Delta t}&\cos{\theta_1\Delta t}&\cdots&0&0 \\ 
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&0&0&\cdots&\cos{\theta_{15}\Delta t}& -\sin{\theta_{15}\Delta t}\\
0&0&0&0&\cdots&\sin{\theta_{15}\Delta t}&\cos{\theta_{15}\Delta t}
\end{pmatrix}
\begin{pmatrix}k^{(0)}\\k^{(1)}\\k^{(2)}\\k^{(3)}\\\vdots\\k^{(30)}\\k^{(31)}\end{pmatrix}
\endgroup}_\text{modeling temporal dependency with higher frequency} \\
+ \underbrace{\begingroup
\setlength\arraycolsep{1pt}
\begin{pmatrix}q^{(32)}\\q^{(33)}\\q^{(34)}\\q^{(35)}\\\vdots\\q^{(78)}\\q^{(79)}\end{pmatrix}^\top
\begin{pmatrix}
% \setstacktabbedgap{2pt}
\cos{\theta_{16}\Delta x}& -\sin{\theta_{16}\Delta x}&0&0&\cdots&0&0\\
\sin{\theta_{16}\Delta x}&\cos{\theta_{16}\Delta x}&0&0&\cdots&0&0 \\
0&0&\cos{\theta_{17}\Delta x}& -\sin{\theta_{17}\Delta x}&\cdots&0&0\\
0&0&\sin{\theta_{17}\Delta x}&\cos{\theta_{17}\Delta x}&\cdots&0&0 \\ 
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&0&0&\cdots&\cos{\theta_{39}\Delta x}& -\sin{\theta_{39}\Delta x}\\
0&0&0&0&\cdots&\sin{\theta_{39}\Delta x}&\cos{\theta_{39}\Delta x}
\end{pmatrix}
\begin{pmatrix}k^{(32)}\\k^{(33)}\\k^{(34)}\\k^{(35)}\\\vdots\\k^{(78)}\\k^{(79)}\end{pmatrix}
\endgroup}_\text{modeling horizontal dependency with intermediate frequency} \\
+ \underbrace{\begingroup
\setlength\arraycolsep{1pt}
\begin{pmatrix}q^{(80)}\\q^{(81)}\\q^{(82)}\\q^{(83)}\\\vdots\\q^{(126)}\\q^{(127)}\end{pmatrix}^\top
\begin{pmatrix}
% \setstacktabbedgap{2pt}
\cos{\theta_{40}\Delta y}& -\sin{\theta_{40}\Delta y}&0&0&\cdots&0&0\\
\sin{\theta_{40}\Delta y}&\cos{\theta_{40}\Delta y}&0&0&\cdots&0&0 \\
0&0&\cos{\theta_{41}\Delta y}& -\sin{\theta_{41}\Delta y}&\cdots&0&0\\
0&0&\sin{\theta_{41}\Delta y}&\cos{\theta_{41}\Delta y}&\cdots&0&0 \\ 
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&0&0&\cdots&\cos{\theta_{63}\Delta y}& -\sin{\theta_{63}\Delta y}\\
0&0&0&0&\cdots&\sin{\theta_{63}\Delta y}&\cos{\theta_{63}\Delta y}
\end{pmatrix}
\begin{pmatrix}k^{(80)}\\k^{(81)}\\k^{(82)}\\k^{(83)}\\\vdots\\k^{(126)}\\k^{(127)}\end{pmatrix}
\endgroup}_\text{modeling vertical dependency with lower frequency} \\
\Delta t=t_1-t_2,\quad \Delta x=x_1-x_2,\quad \Delta y=y_1-y_2 \\
\theta_n=\beta^{-\dfrac{2n}{d}},\quad n=0,\cdots,d/2-1
\end{gathered}
}
\raisebox{-5.5ex}{.}\label{equ:mrope}
\end{equation}

\paragraph{Frequential Allocation}

According to Figure~\ref{fig:attention_analysis}, although Qwen2-VL can locate the needle image, the attention score exhibits some peculiar patterns. First, Qwen2-VL locates the needle image through the vertical position instead of the temporal features. Furthermore, the dimension that captures temporal features fails to model long-context semantic dependencies and is heavily focused on local dependencies. Conversely, the two parts of dimensions designed for spatial dependencies tend to capture long-context semantic information instead of local one. Lastly, the dimensions corresponding to vertical and horizontal positions exhibit very different characteristics, particularly with the vertical position dimension showing phenomena like attention sinks~\cite{xiao2023efficient}. Therefore, we believe the performance decline is primarily due to issues in the design of multi-modal RoPE. 

\section{\methodname}\label{subsec:step_size}

Based on some previous research and the above analysis, we claim that a good RoPE design for MLLMs, especially for long videos, should satisfy four requirements, \textbf{\textit{Multi-Modal Compatibility}}, whether RoPE can simultaneously describe the spatiotemporal position in multi-modals and sequential position in text-only inputs~\cite{wang2024qwen2,kexuefm10040,kexuefm10352}, \textbf{\textit{Appropriate Dimension Distribution}}, whether the feature dimension can process the semantic relationship where it is responsibility~\cite{peng2023yarn,barbero2024round,liu2024kangaroo}, \textbf{\textit{Spatial Symmetry}}, whether the distance between the end of precedent textual input and start of visual input equals the distance between the end of visual input and the start of subsequent textual input~\cite{kexuefm10352}, and \textbf{\textit{Temporal Alignment}}, whether the alignment of sequential feature in different modality is considered~\cite{gao2024tc}. The first requirement has been solved by RoPE-Tie~\cite{kexuefm10040} and the subsequent M-RoPE~\cite{wang2024qwen2}. To solve the last three requirements and the performance decline in V-NIAH-D, we propose our \methodname.

\paragraph{Appropriate Dimension Distribution} 
As shown in Equation \ref{equ:rope}, the vanilla RoPE~\cite{su2024roformer} uses all dimensions to model the 1D position information. As indicated in Equation \ref{equ:mrope}, M-RoPE~\cite{wang2024qwen2} uses dimensions to model temporal, horizontal, and vertical dimensions sequentially. However, this distribution is inappropriate since different feature dimensions in RoPE capture dependency in different ranges. Lower dimensions corresponding to the higher frequency and shorter monotonous interval given larger values of $\theta_n$ tend to capture relative distance and local semantic information~\cite{men2024base,barbero2024round}, while higher dimensions corresponding to lower frequency and wider monotonous interval given smaller values of $\theta_n$ tend to capture dependency in longer contexts~\cite{barbero2024round}. An interesting phenomenon shown in Figure~\ref{fig:attention_analysis} is that considering two branches reported in \citet{han2024lm} and \citet{xiao2023efficient}, the local branch corresponds to lower dimensions while the global branch, or attention sink, corresponds to higher dimensions. More examples can be found in Appendix \ref{app:attention_analysis}.

To make full use of RoPE's properties, \methodname uses higher dimensions for temporal features in longer contexts and lower dimensions for spatial features, which are limited by resolution and have a fixed range. To avoid the gap between horizontal and vertical positions, we interleave the dimensions responsible for these spatial features. The dimension distribution for \methodname is shown in Equation~\ref{equ:videorope}.

% To make full use of these properties of RoPE, \methodname uses higher dimensions to model temporal features in longer contexts and lower dimensions to model spatial features since spatial features tend to be limited by resolution and have a relatively fixed range. To avoid the gap between horizontal and vertical positions, we interleave the dimensions responsible for those two spatial features. Therefore, the dimension distribution for \methodname is shown in Equation~\ref{equ:videorope}.
% 48, 48, 32; 0, 47, 48, 95; 96, 127
\begin{equation}
\resizebox{0.5\textwidth}{!}{
\scriptsize
\begin{gathered}
\bm{A}_{(t_1,x_1,y_1),(t_2,x_2,y_2)}=\bm{q}_{(t_1,x_1,y_1)}\bm{k}_{(t_2,x_2,y_2)}^\top \\
=\underbrace{\begingroup
\setlength\arraycolsep{1pt}
\begin{pmatrix}q^{(96)}\\q^{(97)}\\q^{(98)}\\q^{(99)}\\\vdots\\q^{(126)}\\q^{(127)}\end{pmatrix}^\top
\begin{pmatrix}
% \setstacktabbedgap{2pt}
\cos{\theta_{48}\Delta t}& -\sin{\theta_{48}\Delta t}&0&0&\cdots&0&0\\
\sin{\theta_{48}\Delta t}&\cos{\theta_{48}\Delta t}&0&0&\cdots&0&0 \\
0&0&\cos{\theta_{49}\Delta t}& -\sin{\theta_{49}\Delta t}&\cdots&0&0\\
0&0&\sin{\theta_{49}\Delta t}&\cos{\theta_{49}\Delta t}&\cdots&0&0 \\ 
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&0&0&\cdots&\cos{\theta_{63}\Delta t}& -\sin{\theta_{63}\Delta t}\\
0&0&0&0&\cdots&\sin{\theta_{63}\Delta t}&\cos{\theta_{63}\Delta t}
\end{pmatrix}
\begin{pmatrix}k^{(96)}\\k^{(97)}\\k^{(98)}\\k^{(99)}\\\vdots\\k^{(126)}\\k^{(127)}\end{pmatrix}
\endgroup}_\text{modeling temporal dependency with lower frequency} \\
+ \underbrace{\begingroup
\setlength\arraycolsep{1pt}
\begin{pmatrix}q^{(0)}\\q^{(1)}\\q^{(4)}\\q^{(5)}\\\vdots\\q^{(92)}\\q^{(93)}\end{pmatrix}^\top
\begin{pmatrix}
% \setstacktabbedgap{2pt}
\cos{\theta_{0}\Delta x}& -\sin{\theta_{0}\Delta x}&0&0&\cdots&0&0\\
\sin{\theta_{0}\Delta x}&\cos{\theta_{0}\Delta x}&0&0&\cdots&0&0 \\
0&0&\cos{\theta_{2}\Delta x}& -\sin{\theta_{2}\Delta x}&\cdots&0&0\\
0&0&\sin{\theta_{2}\Delta x}&\cos{\theta_{2}\Delta x}&\cdots&0&0 \\ 
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&0&0&\cdots&\cos{\theta_{46}\Delta x}& -\sin{\theta_{46}\Delta x}\\
0&0&0&0&\cdots&\sin{\theta_{46}\Delta x}&\cos{\theta_{46}\Delta x}
\end{pmatrix}
\begin{pmatrix}k^{(0)}\\k^{(1)}\\k^{(4)}\\k^{(5)}\\\vdots\\k^{(92)}\\k^{(93)}\end{pmatrix}
\endgroup}_\text{modeling horizontal dependency with interleaved high frequency} \\
+ \underbrace{\begingroup
\setlength\arraycolsep{1pt}
\begin{pmatrix}q^{(2)}\\q^{(3)}\\q^{(6)}\\q^{(7)}\\\vdots\\q^{(94)}\\q^{(95)}\end{pmatrix}^\top
\begin{pmatrix}
% \setstacktabbedgap{2pt}
\cos{\theta_{1}\Delta y}& -\sin{\theta_{1}\Delta y}&0&0&\cdots&0&0\\
\sin{\theta_{1}\Delta y}&\cos{\theta_{1}\Delta y}&0&0&\cdots&0&0 \\
0&0&\cos{\theta_{3}\Delta y}& -\sin{\theta_{3}\Delta y}&\cdots&0&0\\
0&0&\sin{\theta_{3}\Delta y}&\cos{\theta_{3}\Delta y}&\cdots&0&0 \\ 
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&0&0&\cdots&\cos{\theta_{47}\Delta y}& -\sin{\theta_{47}\Delta y}\\
0&0&0&0&\cdots&\sin{\theta_{47}\Delta y}&\cos{\theta_{47}\Delta y}
\end{pmatrix}
\begin{pmatrix}k^{(2)}\\k^{(3)}\\k^{(6)}\\k^{(7)}\\\vdots\\k^{(94)}\\k^{(95)}\end{pmatrix}
\endgroup}_\text{modeling vertical dependency with interleaved high frequency} \\
\Delta t=t_1-t_2,\quad \Delta x=x_1-x_2,\quad \Delta y=y_1-y_2 \\
\theta_n=\beta^{-\dfrac{2n}{d}},\quad n=0,\cdots,d/2-1
\end{gathered}
}
\raisebox{-5.5ex}{.}\label{equ:videorope}
\end{equation}
The horizontal position $x$ and vertical position $y$ are interleaved to occupy the lower dimensions, followed by temporal $t$, which occupies the higher dimensions. We keep the same allocation number for $x$, $y$, and $t$ as M-RoPE for a fair comparison, with values of 48, 48, and 32, respectively. The benefit of such distribution is evident as shown in Figure~\ref{fig:period_mono}. For M-RoPE, the position embedding for temporal modeling is significantly tortured with the periodic oscillation~\cite{men2024base} and distant positions may have the same position embedding. Fortunately, our \methodname is free from oscillation and Hash collision in temporal modeling.

\begin{figure*}[!thb]
\begin{minipage}{0.98\textwidth}
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/files/video_rope-period_low-MRoPE.pdf}
        \caption{Temporal Modeling in M-RoPE}
        \label{fig:temporal_mrope}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/files/video_rope-period_low-VideoRoPE.pdf}
        \caption{Temporal Modeling in \methodname (ours)}
        \label{fig:temporal_videorope}
    \end{subfigure}
    \caption{The visualized relationship between the periodicity, monotonicity, and temporal modeling. Consider a RoPE-based LLM with a head dimension size of 128, namely 64 rotary angles $\theta_n$ across various dimensions. Within each illustration, we visually represent the function of $\cos{\theta_n t}$ for 3 dimensions using parallel blue planes. \textbf{(a)} For M-RoPE~\cite{wang2024qwen2}, temporal dependency is modeled by the first 16 rotary angles with higher frequency and more significant oscillation. Take the last 3 rotary angles as an example, the position embedding for temporal modeling is significantly tortured with the periodic oscillation~\cite{men2024base}, since these dimensions have a shorter monotonous interval and the lower dimensions will have much shorter intervals. Notably, since the oscillation is periodic, there exist two distant positions with almost the same position embedding like Hash collision, as shown with the red planes, and that is why the distractors could mislead the model easily. \textbf{(b)} For \methodname, temporal dependency is modeled by the last 16 rotary angles with much wider monotonous intervals. Take the first 3 rotary angles as an example, the position embedding for temporal modeling is free from oscillation~\cite{men2024base}. Then the misleading of distractors is significantly suppressed.
     \label{fig:period_mono}}
\end{minipage}
\end{figure*}

% \paragraph{Appropriate Dimension Distribution} 
% As shown in Equation \ref{equ:rope}, the vanilla RoPE~\cite{su2024roformer} uses all dimensions to model the one-dimensional positional information. As indicated in Equation \ref{equ:mrope}, M-RoPE~\cite{wang2024qwen2} models the time dimension with high-frequency features and the spatial dimension with low-frequency features. However, the spatial dimension is limited by resolution and has a relatively fixed range, without the need for extrapolation. In contrast, the time dimension increases with the length of the video, necessitating extrapolation. \methodname modifies the rotational matrix of the positional encoding by using low-frequency features to model the time dimension and high-frequency features to model the spatial dimension. This adjustment allows the positional encoding to handle longer temporal dependencies. The rotational matrix is as follows:
% % \begin{equation}
% % \resizebox{0.5\textwidth}{!}{
% % \scriptsize
% % \begin{pmatrix}
% % 	\cos{x\theta_1}& -\sin{x\theta_1}&0&0&\cdots&0&0\\
% % 	\sin{x\theta_1}&\cos{x\theta_1}&0&0&\cdots&0&0 \\
% % 	0&0&\cos{y\theta_2}& -\sin{y\theta_2}&\cdots&0&0\\
% % 	0&0&\sin{y\theta_2}&\cos{y\theta_2}&\cdots&0&0 \\
% % 	\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
% % 	0&0&0&0&\cdots&\cos{t\theta_{d/2}}& -\sin{t\theta_{d/2}}\\
% % 	0&0&0&0&\cdots&\sin{t\theta_{d/2}}&\cos{t\theta_{d/2}}
% % \end{pmatrix}
% % \begin{pmatrix}q_0 \\ q_1 \\ q_2 \\ q_3 \\ \vdots \\ q_{d-2} \\ q_{d-1}\end{pmatrix}
% % }
% % \raisebox{-5.5ex}{.}
% % \label{eq:video_rope_freq}
% % \end{equation}
% The \(x\) and \(y\) dimensions are interleaved to occupy the high-frequency dimensions, followed by \(t\), which occupies the low-frequency dimensions. The dimensional allocation for \(x\), \(y\), and \(t\) follows the same pattern as M-RoPE, with values of 24, 24, and 16, respectively.

% \input{figures/position_all_2}

% \begin{figure*}[h]
% \begin{minipage}{0.98\textwidth}
%     \centering
%     \begin{subfigure}[b]{0.2\linewidth}
%     \caption{The position embedding in different feature dimensions between two adjacent text tokens for vanilla RoPE in text, as shown in the first row, or two vision tokens in the same spatial location in two adjacent frames, as shown in the last two rows.}
%     \label{fig:spatail_index}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.65\linewidth}
%         \centering
%         \includegraphics[width=0.98\linewidth]{figures/files/video_rope_figure_spatial_v2.pdf}
%     \end{subfigure}
%     \vskip\baselineskip
%     \hfill
%     \begin{subfigure}[b]{0.3\linewidth}
%         \centering
%         \includegraphics[width=0.95\linewidth]{figures/files/vanilla_rope.pdf}
%         \caption{3D visualization for Vanilla RoPE}
%         \label{fig:vanilla_rope}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\linewidth}
%         \centering
%         \includegraphics[width=0.95\linewidth]{figures/files/m_rope.pdf}
%         \caption{3D visualization for M-RoPE}
%         \label{fig:m_rope}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\linewidth}
%         \centering
%         \includegraphics[width=0.95\linewidth]{figures/files/m_modify_rope.pdf}
%         \caption{3D visualization for \methodname.}
%         \label{fig:video_rope}
%     \end{subfigure}
%     \hfill
%     \caption{The visualized position embedding schema for vanilla RoPE~\cite{su2024roformer} in text domain, M-RoPE~\cite{wang2024qwen2} in video domain, and our \methodname in video domain. Concerning \textbf{(a)} and \textbf{(b)}, for RoPE in text, the position index on every feature dimension grows as the input expands. However, for vanilla RoPE, no spatial modeling is considered. Concerning \textbf{(a)} and \textbf{(c)}, for M-RoPE in video, while spatial modeling is considered and part of position indices grows for the vision token in the same position of adjacent frames, there exists part of position indices remains the same, which is different from the pattern in text domain. Notably, concerning \textbf{(a)} and \textbf{(d)}, our \methodname keeps the same pattern with RoPE in the text domain while modeling the spatial relationship.}
%     \label{fig:spatial}
% \end{minipage}
% \end{figure*}

\begin{figure*}[h]
\begin{minipage}{0.98\textwidth}
    \centering
    % Adjusted the width of the caption (left part) to make it narrower
    \begin{subfigure}[b]{0.15\linewidth} 
        \caption{The position embedding in different feature dimensions between two adjacent text tokens for vanilla RoPE in text, as shown in the first row, or two vision tokens in the same spatial location in two adjacent frames, as shown in the last two rows.}
        \label{fig:spatail_index}
    \end{subfigure}
    % Adjusted the width of the image (right part) to make it wider
    \begin{subfigure}[b]{0.75\linewidth} 
        \centering
        \includegraphics[width=0.98\linewidth]{figures/files/video_rope_figure_spatial_v2.pdf}
    \end{subfigure}
    \vskip\baselineskip
    \hfill
    % Keep the rest of the subfigures as they are
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/files/vanilla_rope.pdf}
        \caption{3D visualization for Vanilla RoPE}
        \label{fig:vanilla_rope}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/files/m_rope.pdf}
        \caption{3D visualization for M-RoPE}
        \label{fig:m_rope}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/files/m_modify_rope.pdf}
        \caption{3D visualization for \methodname.}
        \label{fig:video_rope}
    \end{subfigure}
    \hfill
    \caption{The visualized position embedding schema for vanilla RoPE~\cite{su2024roformer} in text domain, M-RoPE~\cite{wang2024qwen2} in video domain, and our \methodname in video domain. Concerning \textbf{(a)} and \textbf{(b)}, for RoPE in text, the position index on every feature dimension grows as the input expands. However, for vanilla RoPE, no spatial modeling is considered. Concerning \textbf{(a)} and \textbf{(c)}, for M-RoPE in video, while spatial modeling is considered and part of position indices grows for the vision token in the same position of adjacent frames, there exists part of position indices remains the same, which is different from the pattern in text domain. Notably, concerning \textbf{(a)} and \textbf{(d)}, our \methodname keeps the same pattern with RoPE in the text domain while modeling the spatial relationship.}
    \label{fig:spatial}
\end{minipage}
\end{figure*}


\paragraph{Spatial Symmetry} Spatial symmetry is proposed in \citet{kexuefm10352} and argues that the distance between the end of precedent textual input and the start of visual input equals the distance between the end of visual input and the start of subsequent textual input. It is more vividly shown in Figure~\ref{fig:spatial}. For vanilla RoPE, no spatial relation is considered, the index for every dimension increases directly, as shown in Figure~\ref{fig:vanilla_rope}. For M-RoPE, though spatial positions for each frame are considered, there remain two significant gaps between the textual and visual tokens. This is because, following the M-RoPE placement strategy, if the first visual token is positioned at $(0, 0)$, the last token in each frame will always be placed at $(W-1, H-1)$, where $W$ and $H$ are the dimensions of the frame, resulting in a stack in the bottom-left corner, as shown in Fig.~\ref{fig:m_rope}. Besides, it is derived from the pattern in vanilla RoPE, that every dimension increases as the input expands.

To solve these problems, \methodname places the whole input in the diagonal as illustrated in Fig~\ref{fig:video_rope}. For every frame in the video, the 3D position for the central patch is $(t,t,t)$, with positive and negative offsets for other patches in all directions. The benefit of this modification is two-fold. On one hand, the relative position of visual tokens is maintained and the distances from the four corners of the image to the center are approximately equal, which helps avoid the text token being overly close to any particular corner. On the other hand, the pattern of vanilla RoPE is maintained as shown in Fig~\ref{fig:spatail_index}, since the position index increases between two vision tokens in the same spatial location in two adjacent frames like two adjacent textual tokens.
% In the expanded attention score formula:
% \begin{equation}
% \begin{aligned}
% A_{ts} &= q_t \cdot k_s \\
%        &= (q_{t1} k_{s1} + q_{t2} k_{s2}) \cos(\theta(t-s)) \\
%        &+ (q_{t1} k_{s2} + q_{t2} k_{s1}) \sin(\theta(t-s)),
% \end{aligned}
% \end{equation}

% where \( \cos(\theta) \) represents distance and \( \sin(\theta) \) represents direction. This formulation helps distinguish between positions such as the top-left and top-right corners.

\paragraph{Temporal Alignment} Besides consideration of spatial modeling, there also exists a requirement focused on temporal modeling, that is temporal alignment. Although both the frame index in video and the token index in text relate to temporal information, they are inherently different.~\cite{kexuefm10352,li2024temporal}. Related works such as TAD-RoPE, a 1D RoPE design for MLLM, modify the text token index by adding $\gamma + 1$ and the image token index by adding $\gamma$ to distinguish that and find that $\gamma = 1$ could achieve the best performance. Similarly, we use $\delta$ to represent the relative temporal distance between adjacent visual tokens and make an experimental effect to align these two temporal information better. 

To sum up, for a multi-modal input that starts with a text with $T_s$ tokens, follows with a video with $T_v$ frame with $W\times H$ patches in each frame, and ends with another text with $T_e$ tokens. The position indices $(t, x, y)$ of \methodname in three parts of feature dimension for $\tau$-th textual token or $(\tau, w, h)$-th visual token can be formulated as Equation~\ref{equ:index}
% \begin{equation}
% \resizebox{0.5\textwidth}{!}{
% \tiny
%     (t,x,y)=\begin{cases}
%         (\tau, \tau, \tau) & \text{if } 0\leq\tau<T_s \\[2ex]
%         \begin{gathered}
%             (T_s+\delta*(\tau-T_s), \\
%             T_s+\delta*(\tau-T_s)+w-W/2, \\
%             T_s+\delta*(\tau-T_s)+h-H/2)
%         \end{gathered} & \text{if } T_s\leq\tau<T_s+T_v \\[5ex]
%         \begin{gathered}
%             (T_s+\delta*T_v+\tau, \\
%             T_s+\delta*T_v+\tau, \\ T_s+\delta*T_v+\tau)
%         \end{gathered} & \text{if } \begin{gathered}T_s+T_v\leq\tau<\\ T_s+T_v+T_e\end{gathered} \\
%     \end{cases}}
% \raisebox{-5.5ex}{.}\label{equ:index}
% \end{equation}
\begin{equation}
\resizebox{0.5\textwidth}{!}{ % 调整整体大小
    \footnotesize % 放大字体
    (t,x,y) =
    \begin{cases}
        (\tau, \tau, \tau) & \text{if } 0 \leq \tau < T_s \\[3ex] % 增加行间距
        \left( 
        \begin{array}{l}
            T_s + \delta (\tau - T_s), \\
            T_s + \delta (\tau - T_s) + w - \frac{W}{2}, \\
            T_s + \delta (\tau - T_s) + h - \frac{H}{2}
        \end{array}
        \right) & \text{if } T_s \leq \tau < T_s + T_v \\[6ex] % 增加行间距
        \left( 
        \begin{array}{l}
            T_s + \delta T_v + \tau, \\
            T_s + \delta T_v + \tau, \\
            T_s + \delta T_v + \tau
        \end{array}
        \right) & \text{if } T_s + T_v \leq \tau < T_s + T_v + T_e
    \end{cases}
}
\raisebox{-5.5ex}{.}
\label{equ:index}
\end{equation}


