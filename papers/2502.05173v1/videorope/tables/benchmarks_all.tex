% \renewcommand{\arraystretch}{1.1}
% \begin{table*}[!h]
% \setlength\tabcolsep{8pt}
% \centering
% \caption {Comparison of different RoPE methods on three video understanding benchmarks under the same training setting. The benchmarks evaluate performance across three context lengths: 8k, 32k, and 64k, where \textbf{8k} represents context within the training range, and 32k and 64k represent context outside the training range. Our \methodname outperforms other RoPE variants across all three benchmarks. The best results are marked in \textbf{bold}, and the second-best results are \underline{underlined}.}
% \label{tab:lvlm_all}
% \vspace{2mm}
% \footnotesize
% \begin{tabular}{clllllllll}
% \toprule
% \multirow{2}{*}{\textbf{Method}}  & \multicolumn{2}{c}{LongVideoBench} & \multicolumn{2}{c}{MLVU} & \multicolumn{2}{c}{Video-MME} \\ 
% \cmidrule(lr){2-5} % 针对 LongVideoBench 数据
% \cmidrule(lr){6-9} % 针对 MLVU 数据
% \cmidrule(lr){10-13} % 针对 Video-MME 数据
%  & 8k & 16k & 32k & 64k & 8k & 16k & 32k & 64k & 8k & 16k & 32k & 64k \\ \hline
% Vanilla RoPE \cite{su2024roformer} & \textbf{54.35} & 54.25 & \underline{53.94} & 53.52 & 63.31 & \underline{65.93} & \underline{62.02} & \underline{60.6} & 61.3 & 58.3 \\
% TAD-RoPE \cite{gao2024tc} & 53.73 & ? & 53.42 & 52.80 & \underline{63.67} & 65.28 & 60.73 & 60.3 & \textbf{62.0} & 58.6 \\
% M-RoPE \cite{wang2024qwen2} & 52.90 & 52.38 & 52.90 & \underline{54.14} & 60.41 & 61.56 & 61.10 & \underline{60.6} & 61.0 & \underline{60.6} \\
% \hline
% \rowcolor[HTML]{F2F3F5}
% \methodname (Ours) & \underline{53.94} & 54.25 & \textbf{56.12} & \textbf{56.43} & \textbf{65.19} & \textbf{66.02} & \textbf{65.56} & \textbf{61.3} & \underline{61.6} & \textbf{61.3} \\
% % Qwen2-VL-7B & \textbf{54.87} & \textbf{59.12} & \textbf{58.40} & \textbf{65.60} & \textbf{68.36} & \textbf{68.59} & 54.3 & 56.3 & 53.6 \\ \hline
% \bottomrule
% \end{tabular}

% \end{table*}

\renewcommand{\arraystretch}{1.1}
\begin{table*}[!ht]
\setlength\tabcolsep{5pt} % Adjusted for better fit
\centering
\caption{\textbf{Comparison of different RoPE methods on LongVidionBench, MLVU, and Video-MME}. The benchmarks evaluate performance across three context lengths: 8k, 16k, 32k, and 64k, where \textbf{8k} represents context within the training range, and others represent context outside the training range. Our \methodname outperforms other RoPE variants across all three benchmarks. The best results are marked in \textbf{bold}, and the second-best results are \underline{underlined}. For more information on the evaluation, see Appendix \ref{appendix:benchmarks}.}
\label{tab:lvlm_all}
\vspace{2mm}
\footnotesize
\begin{tabular}{cllllllllllll}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{LongVideoBench}} & \multicolumn{4}{c}{\textbf{MLVU}} & \multicolumn{4}{c}{\textbf{Video-MME}} \\ 
\cmidrule(lr){2-5} % 针对 LongVideoBench 数据
\cmidrule(lr){6-9} % 针对 MLVU 数据
\cmidrule(lr){10-13} % 针对 Video-MME 数据
 & 8k & 16k & 32k & 64k & 8k & 16k & 32k & 64k & 8k & 16k & 32k & 64k \\ 
\hline
Vanilla RoPE \cite{su2024roformer} & \textbf{54.97} & 54.87 & \underline{54.56} & 54.04 & 63.31 & \underline{65.79} &\underline{65.93} & \underline{62.02} & \underline{60.67} & 60.00 & 61.33 & 58.33 \\
TAD-RoPE \cite{gao2024tc} & 54.14 & \underline{55.08} & 53.94 & 53.42 & \underline{63.67} & 65.28 & 65.28 & 60.73 & 60.33 & \textbf{61.33} & \textbf{62.00} & 58.67 \\
M-RoPE \cite{wang2024qwen2} & 53.42 & 52.80 & 53.11 & \underline{54.35} & 60.41 & 60.68 & 61.56 & 61.10 & \underline{60.67} & 59.67 & 61.00 & \underline{59.67} \\
\hline
\rowcolor[HTML]{F2F3F5}
\methodname (Ours) & \underline{54.46} & \textbf{55.29} & \textbf{57.15} & \textbf{57.26} & \textbf{65.19} & \textbf{66.29} & \textbf{66.02} & \textbf{65.56} & \textbf{61.33} & \underline{61.00} &\underline{61.67} & \textbf{61.33} \\
\bottomrule
\end{tabular}
\end{table*}
