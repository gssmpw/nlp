\section*{\centering Appendix}

This appendix provides additional resources to further enhance the understanding of our work.
In Section \ref{app:MORE_EXPERIMENTS}, we present ablation studies and extrapolation experiments extending up to 128k, which are included here due to space constraints in the main text.
Section \ref{appendix:benchmarks} offers a more detailed discussion of the benchmarks used for evaluation.
Section \ref{app:related_work} reviews related work on video LLMs and video haystack retrieval.
Section \ref{app:video_niah_d} showcases examples from our proposed \textbf{V-NIAH-D} benchmark.
In Section \ref{app:attention_analysis}, we provide additional attention visualizations to further support the observations discussed in Figure \ref{fig:attention_analysis}.
Finally, Section \ref{app:supp_explain_modules} provides a more detailed analysis of the frequency allocation and further elaborates on Figure \ref{fig:period_mono}.

\section{MORE EXPERIMENTS} \label{app:MORE_EXPERIMENTS}
\subsection{Supplementary Ablation Experiments}\label{app:ablation_study}
\noindent \textbf{Ablation Studies on the Scaling Factor $\boldsymbol{\delta}$ for ATS.} \label{app:ATS}
This section extends the discussion from Section \ref{subsec:step_size}, further examining how varying temporal information—achieved by adjusting the scaling factor $\delta$—affects the alignment between text and video tokens. The proper alignment of these tokens is key for improving the model’s understanding of both temporal and semantic relationships between text and video. As shown in Table \ref{tab:ablation_t_steps}, we conduct evaluations using the \textbf{LongVideoBench} benchmark, which provides a comprehensive evaluation of video understanding tasks. By sampling $\delta$ values in the range $[0.5, 3]$ with an interval of $0.5$, we observe that performance improves with increasing $\delta$ up to a point. Our findings indicate that the optimal performance is achieved when $\delta = 2$, suggesting that this scaling factor best balances the temporal alignment between text and video tokens.
\input{tables/ablation_t_steps_longvideobench}
\setlength{\textfloatsep}{5pt}  % Reduce space between tables/figures
\setlength{\intextsep}{5pt}     % Reduce space when the table is placed within text (in-text float)
\setlength{\floatsep}{5pt}      % Space between floats when they appear together
\vspace{-5mm}
\paragraph{Ablation Studies on \(\mathbf{x}\), \(\mathbf{y}\) Allocation.} \label{app:x_y_allocation}
To further investigate the impact of different allocation strategies, we conduct quantitative experiments on our proposed \textbf{\methodname}, comparing sequential and interleaved allocations of $x$ and $y$. The results, summarized in Table \ref{tab:ablation_x_y_allocation}, indicate that interleaving $x$ and $y$ leads to superior performance. 


We hypothesize that this improvement arises because interleaving maintains the similarity between the $x$ and $y$ dimensions, whereas sequential allocation increases their disparity, thereby hindering model performance.  
\input{tables/ablation_x_y_allocation_appendix}

\subsection{Extrapolation to 128k Experiments}
To explore the extrapolation limits of our approach, we extend the visual context during inference to 128k. Specifically, we utilize the \textbf{vLLM framework}~\cite{kwon2023efficient} in Server-API processing mode to enable efficient 128k inference.  

Due to the prolonged evaluation time required for 128k processing, we focus on the \textbf{LongVideoBench} benchmark. As shown in Table \ref{tab:128k-appendix}, although all four methods exhibit performance degradation at 128k, our proposed \textbf{\methodname} experiences the least drop, demonstrating its robustness under extreme extrapolation settings.  
\input{tables/128k_appendix}


% \section{VIAH-D}
% V-NIAH \cite{zhang2024longva} is specifically developed to test the model's handling of extended visual contexts, as well as its ability to locate and retrieve long-distance visual data. However, this straightforward retrieval-based task is shown as only a superficial form of long-context understanding in long-context LLMs~\cite{hsieh2024ruler}. To make the evaluation effect more distinguishable, inspired by the subsequent long-context benchmarks in LLM including RULER~\cite{hsieh2024ruler} and LV-Eval\cite{yuan2024lv}, we propose V-NIAH-D and enhance V-NIAH by incorporating additional distractors, reducing the likelihood of the model answering correctly through guesswork. Specifically, we generate distractors similar to the needle frame using Google Image Search~\cite{googleimagesearch} or Flux.1  [dev]~\cite{flux2023}. These distractors must be semantically similar to the needle frame and unambiguous with regard to answering the question, as shown in Figure \ref{fig:v-ruler}. 

\section{Additional Details on Evaluation Benchmarks} \label{appendix:benchmarks}
% We consider six video benchmarks for evaluation, including \textit{long video understanding}, \textit{long video retrieval} and \textit{video hallucination}.
% % The performance of the proposed RoPE model is evaluated through two tasks: \textit{long video understanding} and \textit{long video retrieval}.
% % For long video understanding, we focus specifically on Multi-Choice tasks for ease of evaluation, excluding the use of subtitles.
% For long video understanding, we employ three benchmarks:
% (1) LongVideoBench \cite{wu2024longvideobench}, which focuses on reasoning tasks that depend on extended frame sequences, with video durations spanning from 8 seconds to 1 hour; (2) MLVU \cite{zhou2024mlvu}, a comprehensive benchmark designed for long video understanding, featuring videos lasting between 3 minutes and 2 hours; and (3) Video-MME \cite{fu2024video} contains videos of varying lengths, ranging from short clips of 11 seconds to extended recordings of up to 60 minutes.
% We also use the (4) Vision Needle-in-a-Haystack (V-NIAH)~\cite{zhang2024longva} and the proposed Vision Needle-in-a-Haystack with Distractors (V-NIAH-D) benchmark for \textit{long video retrieval}. We insert a needle image at an arbitrary location within a haystack of 3,000 frames. Each needle image corresponds to a specific question, which is unrelated to the content of the haystack.
% In addition, we use (5)  VideoHallucer~\cite{videohallucer}, a benchmark designed to assess \textit{video hallucination} in multimodal large language models (MLLMs). It evaluates models based on their ability to answer both basic questions and hallucinated questions correctly, requiring a precise understanding of relationships between video frames.

For \textbf{long video understanding}, we employ three benchmarks: (1) \textbf{LongVideoBench} highlights reasoning questions that depend on long frame sequences, which cannot be effectively addressed by a single frame or a few sparse frames, with durations ranging from 8 seconds to 1 hour. We retain only the questions that are free from subtitles.
(2) \textbf{MLVU} provides a comprehensive benchmark tailored for assessing the performance of Multimodal Large Language Models in understanding long videos. The dataset features videos lasting between 3 minutes and 2 hours, with nine diverse evaluation tasks. For our analysis, we concentrate on seven multiple-choice tasks, including Topic Reasoning, Anomaly Recognition, Needle QA, Ego Reasoning, Plot QA, Action Order, and Action Count.
(3) \textbf{Video-MME} stands out as a high-quality benchmark curated for broad scenario coverage, with videos drawn from six key visual domains and 30 subfields. Its dataset spans a wide temporal range, including short clips of 11 seconds and extended videos lasting up to 1 hour.

For \textbf{long video retrieval}, we adopt the following two benchmarks: (1) \textbf{V-NIAH} is specifically designed to identify highly specific moments within long videos, simulating real-world scenarios where only a small segment of a video is relevant within a vast corpus. The setup follows the same configuration as LongVA, where a ``needle'' image is inserted at a random position within a ``haystack'' of 3,000 frames. Each needle image corresponds to a particular question, which is unrelated to the content of the haystack. Each frame is encoded with 144 tokens, and the needle frame is inserted at 0.2 depth intervals. Validation begins at 100 frames, with checks every 200 frames up to 3,000. (2) \textbf{Vision Needle-in-a-Haystack with Distractors (V-NIAH-D)}, our proposed method, builds upon V-NIAH by periodically inserting a distractor 200 frames away from the needle. This distractor is semantically similar to the needle, but it remains irrelevant to the specific question being asked. The insertion period for the distractor is calculated using \( 2 \cdot \pi \cdot 1000000^{32/128} \approx 198.7 \). In our experiments, we directly use a period of 200 for distractor insertion. For additional examples, refer to Figure \ref{fig:v_niah_d_examples}.



For the \textbf{video hallucination}, we use \textbf{VideoHallucer} for evaluation. VideoHallucer classifies hallucinations into two primary types: intrinsic and extrinsic. It further breaks these down into subcategories for detailed analysis, including object-relation, temporal, semantic detail, extrinsic factual, and extrinsic non-factual hallucinations. This framework assesses the model’s ability to accurately answer both basic and hallucinated questions about the video content.


% \subsection{V-NIAH Results} \label{app:v-niah-results}
% We evaluate the performance of the existing RoPE method on V-NIAH, with the results presented in Table Y and Figure X.
% \input{figures/retrieval_attn_v_niah}
% \input{tables/v_niah}

\section{More Related Works}\label{app:related_work}
\noindent \textbf{Related Work on Video LLMs (Video Large Language Models)}
Video Large Language Models (Video LLMs) build upon the success of image-based vision-language models (VLMs)~\cite{liu2023llava,internlmxcomposer,internlmxcomposer2,internlmxcomposer2_5,internlmxcomposer2_5_reward,chen2023sharegpt4v,chen2024open,liu2024rar,liu2024mmdu,huang2024operaalleviatinghallucinationmultimodal,liu2024mia,xing2024pyramiddropacceleratinglargevisionlanguage}, which align vision and language representations~\cite{radford2021learningtransferablevisualmodels,zhang2024longclip,sun2023alphaclip} but primarily focus on static images. Extending these models to video requires handling temporal dependencies~\cite{xu2021videoclipcontrastivepretrainingzeroshot,lei2021moreclipbertvideoandlanguagelearning,bertasius2021spacetimeattentionneedvideo,huang2023vtimellmempowerllmgrasp} and long-form video understanding~\cite{wang2024longllavascalingmultimodalllms,chen2024longvilascalinglongcontextvisual,zhang2024longva}. Early video LLMs, such as \citet{wang2023chatvideotrackletcentricmultimodalversatile} and \citet{2023videochat}, leverage various Video Foundation Models (ViFMs), such as InternVideo~\cite{wang2022internvideogeneralvideofoundation}, to extract video attributes, enabling LLM-based question answering. However, their ability to process video content is constrained by the limitations of ViFMs, restricting their effectiveness to short videos. To address this, \citet{luo2023valleyvideoassistantlarge} introduces a Temporal Modeling Module, allowing end-to-end training of LLMs on video data. Building on this approach, \citet{Maaz2023VideoChatGPT} further enhances spatiotemporal modeling to improve video comprehension. Meanwhile, \citet{zhang2023videollamainstructiontunedaudiovisuallanguage} and \citet{lin2023videollava} integrate multiple modalities, such as audio and images, to enrich video understanding. These advancements lay the groundwork for processing long videos with greater accuracy. To extend Video LLMs’ capabilities to longer content, \citet{lin2023mmvidadvancingvideounderstanding} first generates clip-level captions and then employs an LLM to integrate them into a comprehensive video caption, effectively representing the entire video. Various studies, such as those by \citet{li2024llamavid}, \citet{jin2023chatunivi}, \citet{xu2024pllavaparameterfreellava}, and \citet{zhang2025llavaminiefficientimagevideo}, explore different pooling strategies to reduce the number of video tokens, enabling LLMs to process longer videos more effectively. As the field progresses, there is a growing emphasis on long-form video understanding, exploring techniques such as streaming-based processing~\cite{qian2025dispiderenablingvideollms,li2025ovobenchfarvideollmsrealworld}, memory-augmented models~\cite{qian2024streaminglongvideounderstanding,ding2024sam2longenhancingsam2}, and hierarchical representations~\cite{wang2024videotreeadaptivetreebasedvideo} to efficiently model extended temporal structures for tasks like event-level comprehension~\cite{liu2024etbenchopenendedeventlevel} and video summarization~\cite{chai2024auroracapefficientperformantvideo}.

\noindent \textbf{Related Work on Video Haystack Retrieval}
Originating from the Needle-In-A-Haystack task in Natural Language Processing~\citep{niah,multi_niah}, Video haystack tasks aim to locate specific \textit{needle}, the target information, within vast \textit{haystack}, collections of video or multi-modal content~\cite{zhang2024longva,wang2024needle}. In the video domain, VNBench~\cite{zhao2024videoniah} first introduced a video haystack framework with diverse types of needles, such as subtitles, images, and video clips, specifically designed for retrieval tasks within a three-minute timeframe. V-NIAH~\cite{zhang2024longva} further advanced the field by extending retrieval tasks to long durations of up to one hour, providing tools for comprehensive evaluation. In a broader multi-modal domain, MMNeedle~\cite{wang2024multimodal} modeled the retrieval task as locating the exact coordinates of a sub-image within a larger multi-image haystack, while MM-NIAH~\cite{wang2024needle}  introduced a setting where both haystack and needle could include images or text, emphasizing interleaved retrieval capabilities.

% VHs我改成放在最后因为我看arxiv序号它是最晚的2407，其他都是2406，并且能结合后面V-NIAH-D，以及加了些长文的背景工作引用，措辞上用了 position embedding collision 还有 rotary base

However, video haystack retrieval still lags in difficulty compared with the QA or retrieval task in NLP~\citep{hsieh2024ruler,yuan2024lv}. Few attempts exist to enhance the discriminability of evaluations, such as multi-NIAH~\citep{multi_niah,hsieh2024ruler} or NIAH with distractors~\citep{hsieh2024ruler}. Although VHs~\cite{wu2024visual} introduces distractors into the multi-image haystack setting, where needles and distractors are randomly inserted, VHs still did not consider temporal dependencies between video frames or more structured approaches to task evaluation. Our work builds upon V-NIAH by introducing distractors in a systematic, periodic manner based on rotary bases. 
% Furthermore, in Appendix \ref{app:video_niah_d}, we validate the effectiveness of the position embedding collision introduced in the frequency dimension by our V-NIAH-D framework, demonstrating its impact on enhancing retrieval performance under more challenging scenarios.



\section{V-NIAH-D Examples}\label{app:video_niah_d}
% \paragraph{Periodic vs. Random} 
% In this section, we further explore two questions. First, we investigate whether the periodic distractor addition task is more challenging than the random distractor addition task. Second, we examine whether our method demonstrates stronger robustness compared to M-RoPE in the periodic distractor addition task.

% We test and compare the performance of both our method and M-RoPE under two conditions: periodic distractor addition and random distractor addition. In our experiments, the number of distractors is set to 5.
% \input{tables/v_niah_d_periodic_appendix}

% As shown in Table \ref{tab:v-niah-d-periodic-appendix}, periodic distractor addition introduces a greater degree of interference for both compared to random distractor addition. This indicates that periodic distractor addition poses a higher level of difficulty.

% We calculate the relative interference values for our method as \( \frac{0.76 - 0.68}{0.76} = 0.1052 \) and for\vspace{0.05cm}
% M-RoPE as \( \frac{0.608 - 0.53}{0.608} = 0.1282 \). The results show that $0.1052 < 0.1282$, indicating that our method is less affected by periodic interference. These results emphasize the benefits of our approach in mitigating the influence of periodic distractors.



% \paragraph{V-NIAH-D examples}
Figure \ref{fig:v_niah_d_examples} illustrates the five VQA needles used in V-NIAH-D, along with their corresponding distractors. The visual questions and their respective answers are the only components in V-NIAH-D that require human annotation, making it an ideal benchmark for evaluating the long-context reasoning capabilities of LMMs.  

\input{figures/v_niah_d_examples}

\section{Supplementary Attention Analysis}\label{app:attention_analysis}
To further explain the attention pattern in Figure \ref{fig:attention_analysis}, we present additional visual analysis in Figure \ref{fig:attention_analysis appendix}. An attention analysis comparing M-RoPE and \methodname is conducted using 8k-context input, with video tokens from the same frame aggregated through average pooling. As a result, one tick on the axis represents a single frame during inference. The evaluation setup for Figure \ref{fig:attention_analysis} is the same as for Figure \ref{fig:attention_analysis appendix}. M-RoPE relies on high-frequency temporal modeling, limiting it to local information and hindering effective needle identification for question answering. On the other hand, \methodname employs low-frequency temporal modeling, allowing it to capture long-range dependencies and successfully identify the needle for accurate responses.


\input{figures/attention_analysis_appendix}

\section{Supplementary Explanation on Frequency Allocation} \label{app:supp_explain_modules}
% \paragraph{Supplementary Explanation on Frequency Allocation.}

This section provides a detailed explanation of the supplementary information related to Figure \ref{fig:period_mono}, highlighting the advantages of our frequency allocation. Consider a RoPE-based LLM with a head dimension size of 128, corresponding to 64 rotary angles $\theta_n$ across different dimensions. In each illustration, we visually represent the function $\cos(\theta_n t)$ for 3 dimensions using parallel blue planes. 

\textbf{(a)} For M-RoPE~\cite{wang2024qwen2}, temporal dependency is modeled using the first 16 rotary angles, which exhibit higher frequency and greater oscillation. Taking the last 3 rotary angles as an example, the position embedding for temporal modeling undergoes significant distortion due to periodic oscillations~\cite{men2024base}, as these dimensions have shorter monotonous intervals. Lower dimensions have even shorter intervals. Notably, because the oscillation is periodic, two distant positions can have nearly identical position embeddings, resembling a hash collision, as shown by the red planes. This phenomenon is why distractors can easily mislead the model.

\textbf{(b)} In contrast, for \methodname, temporal dependency is modeled using the last 16 rotary angles, which have much wider monotonous intervals. Taking the first 3 rotary angles as an example, the position embedding for temporal modeling is free ferom oscillation~\cite{men2024base}. As a result, the misleading effects of distractors are significantly suppressed.

% \paragraph{Supplementary Explanation on Spatial
% Symmetry.} This section expands on Figure \ref{fig:spatial}, offering a more thorough explanation of the position index through the use of mathematical formulas. In order to maintain consistent notation across the paper, we follow the conventions and definitions introduced in Section \ref{subsec:step_size} on Adjustable Temporal Spacing. The input format assumed by the formulas is text-video-text. 

% \textbf{(a)} For Vanilla RoPE~\cite{su2024roformer}, both text tokens and video tokens are arranged sequentially on a 1D axis. If described in 3D, they lie along the body diagonal. The formula is:

% \begin{equation}
% (t, x, y) = (\delta \tau, \delta \tau, \delta \tau) \quad \text{if } 0 \leq \tau < T_s + T_v + T_e.
% \label{eq:vanilla_rope_formula}
% \end{equation}

% Since the text tokens and video tokens are not aligned, \(\delta = 1\) in this case.

% (b) For M-RoPE~\cite{wang2024qwen2}, For the starting text (\(0 \leq \tau < T_s\)), the temporal, horizontal, and vertical indices are directly assigned the raw token index \(\tau\). For the video, M-RoPE increases \(t\), while \(x\) and \(y\) are stacked vertically, independent of the value of \(t\).


% \begin{equation}
% \resizebox{0.6\textwidth}{!}{
%     \footnotesize
%     (t,x,y) =
%     \begin{cases}
%         (\tau, \tau, \tau) & \text{if } 0 \leq \tau < T_s \\[3ex]
%         \left( 
%         \begin{array}{l}
%             T_s + \delta (\tau - T_s), \\
%             T_s + w, \\
%             T_s + h
%         \end{array}
%         \right) & \text{if } T_s \leq \tau < T_s + T_v \\[6ex]
%         \left( 
%         \begin{array}{l}
%             max_{pos} + \tau - T_s - T_v, \\
%             max_{pos} + \tau - T_s - T_v, \\
%             max_{pos} + \tau - T_s - T_v
%         \end{array}
%         \right) & \text{if } T_s + T_v \leq \tau < T_s + T_v + T_e
%     \end{cases}
% }
% \raisebox{-9.5ex}{,}
% \label{equ:index}
% \end{equation}