\section{Experiment}
\input{tables/benchmarks_all}
\input{figures/v_niah_and_d}

\subsection{Experimental Setup}

% \paragraph{Training Data} To better align the large language model with the video modality, we utilize the llava-video-178k dataset \cite{zhang2024video}, which encompasses key tasks such as detailed captioning, open-ended question answering (QA), and multiple-choice QA. These tasks are derived from diverse video sources, such as HD-VILA, Kinetics, and ActivityNet.
% For training, we randomly select 300k samples with durations of less than 2 minutes and 30k samples ranging between 2 and 3 minutes, striking a balance between training efficiency and long video understanding.
% In total, our training data comprises 1.285 million QA pairs, which are used for supervised fine-tuning (SFT) of our \methodname.

\noindent \textbf{Training Data.} We use a subset of LLaVA-Video-178k dataset \cite{zhang2024video} to train \methodname.
The LLaVA-Video-178k dataset covers 178k videos and around 5 million question-answers (QA) pairs from diverse sources such as HD-VILA \cite{xue2022hdvila}, Kinetics \cite{kay2017kinetics}, and ActivityNet \cite{caba2015activitynet}.
To balance training efficiency and long-video comprehension, we randomly select 136k videos with durations under 2 minutes and 18k videos with durations between 2 and 3 minutes.
This process yielded our training set of approximately 1.3 million pairs.

\noindent \textbf{Implementation Details.}
Using the aforementioned video training data, we fine-tune different modes that use different positional encoding strategies, such as the Vanilla RoPE \cite{su2024roformer}, Time-Aware Dual RoPE (TAD-RoPE) \cite{gao2024tc}, M-RoPE \cite{wang2024qwen2}, and our \methodname.
All models are initialized with the Vision Transformer from Qwen2-VL-7B and LLM (Vanilla RoPE) from Qwen2-7B \cite{yang2024qwen2}.
Our fine-tuning incorporates our \methodname to process the spatiotemporal nature of the video data effectively.
We adopt Qwen2-VL's fine-tuning settings, processing each video at 2 fps with a maximum of 128 frames and dynamically adjusting the image resolution to maintain a consistent token count. However, to prevent memory overflow, we use a context window of 8192 tokens.
% This approach balances the capacity to handle extended videos with training efficiency.

Our fine-tuning process employs a batch size of 128, a cosine scheduler with a learning rate of 1e-5, a warm-up ratio of 1e-2, and 704 Nvidia-A100 GPU hours in total.
The evaluation involves sampling videos at 2 fps with a minimum of 144 image tokens per frame.
We use the vLLM framework \cite{kwon2023efficient} to support inference on sequences longer than 32k tokens.
% To support inference over long sequences, we use the Transformers library for sequences under 32k context and the vLLM framework for sequences exceeding 32k.

\noindent \textbf{Evaluation Benchmarks.} We evaluate our approach using six video benchmarks, including tasks related to \textit{long video understanding}, \textit{long video retrieval}, and \textit{video hallucination}. For \textit{long video understanding}, we use \textbf{LongVideoBench} \cite{wu2024longvideobench} (8 seconds to 1 hour), \textbf{MLVU} \cite{zhou2024mlvu} (3 minutes to 2 hours), and \textbf{Video-MME} \cite{fu2024video} (11 seconds to 60 minutes). For \textit{long video retrieval}, we use \textbf{Vision Needle-in-a-Haystack (V-NIAH)} \cite{zhang2024longva} and our proposed extension, \textbf{Vision Needle-in-a-Haystack with Distractors (V-NIAH-D)}, which introduces distractor frames to increase the task difficulty. For \textit{video hallucination}, we use \textbf{VideoHallucer} \cite{videohallucer}, which evaluates the model's ability to correctly answer both basic and hallucinated questions about video content. Details of these benchmarks can be found in  Appendix~\ref{appendix:benchmarks}.

\subsection{Results on Long Video Understanding}
As shown in Tab. \ref{tab:lvlm_all}, we compare our \methodname with existing RoPE variants (vanilla RoPE \cite{su2024roformer}, TAD-RoPE \cite{gao2024tc}, and M-RoPE \cite{wang2024qwen2}) across three prominent video understanding benchmarks. Our \methodname consistently outperforms all baseline methods across these benchmarks, demonstrating its robustness and adaptability. Specifically, \methodname achieves improvements of up to 2.91, 4.46, and 1.66 points (64k context length) over the M-RoPE baseline on LongVideoBench, MLVU, and Video-MME, respectively. These results emphasize the superior ability of \methodname to effectively capture long-range dependencies and maintain performance across various challenging video data tasks.

\input{tables/v_niah_and_d}
\setlength{\textfloatsep}{5pt}  % Reduce space between tables/figures
\setlength{\intextsep}{5pt}     % Reduce space when the table is placed within text (in-text float)
\setlength{\floatsep}{5pt} 
\input{tables/video_hallucer}
\subsection{Results on Long Video Retrieval}
Fig. \ref{fig:v-niah-and-d} illustrates the performance of V-NIAH and V-NIAH-D with \methodname and other RoPE variants. Specifically, Fig. \ref{fig:v-niah-and-d} (a) and (b) demonstrate that the proposed V-NIAH-D is more challenging than V-NIAH.
Fig. \ref{fig:v-niah-and-d} (1) and (2) show that both Vanilla RoPE and TAD-RoPE exhibit some extrapolation ability beyond the visual training context. However, both methods fail once they exceed a certain extrapolation limit.
In contrast, Fig. \ref{fig:v-niah-and-d} (3) and (4) highlight the superior performance of \methodname and M-RoPE in extrapolating within the test context range. While both \methodname and M-RoPE successfully handle extrapolation, \methodname consistently outperforms M-RoPE, showcasing the robustness of the task.
Tab. \ref{tab:v-niah-and-d} provides a quantitative analysis of the retrieval results, demonstrating a 12.44
\% performance improvement of our method over M-RoPE on the Video Retrieval task in both settings, confirming the advantages of our proposed method in video retrieval scenarios.

\subsection{Results on Video Hallucination}

As highlighted in Tab. \ref{tab:video_hallucer}, \methodname significantly surpasses current RoPE methods on the VideoHallucer benchmark. In particular, for the Temporal Hallucination task, \methodname demonstrates a substantial performance improvement of 29.5\%, indicating its enhanced capability to accurately capture and process temporal dependencies. This improvement suggests that \methodname is better equipped to handle dynamic video sequences, where the understanding of time-based relationships is critical. Similarly, for the Spatial Hallucination task, specifically the Object-Relation Hallucination subtask, \methodname achieves an impressive 18.0\% improvement over existing methods, highlighting its ability to better discern complex spatial interactions. These results underscore \methodname's robustness in solving video hallucination and potential for real-world video analysis.

\subsection{Ablation Studies}
\noindent \textbf{Ablation Studies on Module Design.} 
% Diagonal Position Encoding (DPE)
% Low-Frequency Temporal Modeling(LFTM)
% Step-2 Temporal Positional Encoding (S2TPE)
We conduct a series of ablation experiments on the modules introduced in Section \ref{subsec:step_size}. Through these experiments, we quantitatively evaluate the performance improvements, specifically analyzing their impact on the LongVideoBench and MLVU benchmarks. The experimental results are presented in Tab. \ref{tab:ablation_modules}. The baseline setting, M-RoPE~\cite{wang2024qwen2}, achieves scores of 54.35 on LongVideoBench and 61.10 on MLVU (both using a 64k context length). By progressively integrating the DL (Diagonal Layout), LTA (Low-frequency Temporal Allocation), and ATS (Adjustable Temporal Spacing) modules, our method shows a continuous improvement in performance, achieving enhanced scores of 57.26 on LongVideoBench and 65.56 on MLVU (both using a 64k context length). These results demonstrate the effectiveness of our approach in leveraging spatial-temporal positional information. To refine the discussion on the allocation of \( x \) and \( y \) within LTA, we quantitatively examine the performance impact of interleaved versus sequential allocations. Additionally, we explore the optimal scaling factor in ATS by adjusting its values. Please refer to Appendix~\ref{app:ablation_study} for more ablation studies on the scaling factor $\delta$ and the $x$/$y$ interleaved allocation.

% To further refine the discussion on the allocation of \( x \) and \( y \) within LTA, we quantitatively examine the performance impact of interleaved versus sequential allocations of \( x \) and \( y \) in Appendix \ref{app:x_y_allocation}. Regarding the choice of the scaling factor in ATS, please refer to Appendix~\ref{app:ATS}.


\input{tables/ablation_modules}