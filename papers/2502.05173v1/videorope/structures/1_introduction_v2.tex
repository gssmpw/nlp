\section{Introduction}

Rotary Position Embedding (RoPE) \cite{su2024roformer} helps Transformer models understand word order by assigning each token a unique positional `marker' calculated using a mathematical rotation matrix.
RoPE has advantages in long-context understanding \cite{ding2024longrope}, and continues to be a default choice in leading Large Language Models (LLMs) like the LLaMA \cite{touvron2023llamaopenefficientfoundation,touvron2023llama,dubey2024llama} and QWen \cite{yang2024qwen2,yang2024qwen25} series.

The original RoPE implementation (Vanilla RoPE) \cite{su2024roformer} is designed for sequential 1D data like text. However, recent Video Large Language Models (Video LLMs) \cite{2023videochat,lin2023video,chen2024sharegpt4video,maaz2024videochatgptdetailedvideounderstanding,zhang2024longva,wang2024longllavascalingmultimodalllms,chen2024longvilascalinglongcontextvisual,internlmxcomposer2_5_OL} process video, which has a more complex spatio and temporal structure.
As shown in Tab. \ref{tab:pe_compare}, although several RoPE-based approaches \cite{gao2024tc,wang2024qwen2} have been proposed to support video inputs, these variants exhibit limitations and do not fully satisfy the following key characteristics:

\begin{table}[t]
\tiny
% \small
\centering
\tabcolsep=0.1cm
\begin{tabular}{lcccc}
\toprule
 & \makecell[c]{\textbf{2D/3D} \\ \textbf{Structure}} & \makecell[c]{\textbf{Frequency} \\ \textbf{Allocation}} & \makecell[c]{\textbf{Spatial} \\ \textbf{Symmetry}} & \makecell[c]{\textbf{Temporal} \\ \textbf{Index Scaling}} \\
\midrule
\makecell[l]{Vanilla RoPE \cite{su2024roformer}} & \xmark & \xmark & \xmark & \xmark \\
\makecell[l]{TAD-RoPE \cite{gao2024tc}} & \xmark & \xmark & \xmark & \cmark \\
\makecell[l]{RoPE-Tie \cite{kexuefm10040}} & \cmark & \xmark & \cmark & \xmark \\
\makecell[l]{M-RoPE \cite{wang2024qwen2}} & \cmark & \xmark & \xmark & \xmark \\
% \makecell[l]{V2PE\\\cite{ge2024v2pe}} & \xmark & \xmark & \xmark & \cmark \\
\midrule
\rowcolor[HTML]{F2F3F5}
\methodname (Ours) & \cmark & \cmark & \cmark & \cmark \\
\bottomrule
\end{tabular}
\vspace{-6pt}
\caption{Comparison between different RoPE variants for Video Large Language Models (Video LLMs).}
\label{tab:pe_compare}
\vspace{-12pt}
\end{table}

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{figures/files/radar.pdf}
\vspace{-6pt}
\caption{\methodname outperforms RoPE variants on benchmarks.}
\label{fig:radar}
\vspace{-12pt}
\end{figure}

\input{figures/v_ruler}

\textbf{(1) 2D/3D Structure.} Some existing Video LLMs direct flatten the video frame into 1D embeddings and apply the 1D structure RoPE \cite{su2024roformer,gao2024tc}.
These solutions fail to capture video data's inherent 2D or 3D (temporal ($t$), horizontal ($x$), and vertical ($y$)) structure, thus hindering explicit spatial and temporal representation.

% whether the feature dimension can process the semantic relationship where it is responsibility~\cite{peng2023yarn,barbero2024round,liu2024kangaroo},
% scaling the understanding capabilities of these models, enabling them to comprehend longer videos and, ultimately, bring them closer to human-level understanding.

\textbf{(2) Frequency Allocation.} Previous approaches such as M-RoPE used in QWen2-VL \cite{wang2024qwen2} employ 3D structure, dividing the feature dimensions into distinct subsets for ($t$, $x$, $y$) encoding, respectively.
How to determine the optimal allocation of these dimension subsets, and consequently their associated frequencies 
\footnote{In RoPE, frequencies are determined by $\beta^{-2n/d}$, where $\beta$ is a constant, $n$ is the dimension index, $d$ is the total number of dimensions. Thus, choosing which dimensions represent $t$, $x$, and $y$ directly determines the frequencies used for each.} are not well studied.
Some previous work allocates the lower dimensions corresponding to the high frequency to represent the $t$.
However, the temporal dimension $t$ is significantly tortured by periodic oscillation, and distant positions may have the same embeddings.

We present a simple setting to verify this point.
Based on the previous long-video retrieval task V-NIAH (Visual Needle-In-A-Haystack) \cite{zhang2024longva}, we insert several similar images that do not affect the question's answer before and after the needle image as distractor \cite{hsieh2024ruler,yuan2024lv}, forming a new task, V-NIAH-D (Visual Needle-In-A-Haystack with Distractors).
As shown in Fig. \ref{fig:v-ruler}, we find that previous M-RoPE is misled by distractors, showing a significant performance decline from V-NIAH to V-NIAH-D.
Our observation demonstrates that the periodic oscillation reduces Video LLMs' robustness.

\textbf{(3) Spatial Symmetry.} The distance between the end of the precedent textual input and the start of visual input equals the distance between the end of visual input and the start of subsequent textual input~\cite{kexuefm10352}. Such a symmetry ensures that the visual input receives equal contextual influence from both the preceding and subsequent textual information.

\textbf{(4) Temporal Index Scaling.} Spatial and temporal dimensions often exhibit different granularities (e.g., a unit change in $x$/$y$ differs from a unit change in $t$) \cite{gao2024tc}.
Employing varying index intervals in positional encoding allows for dimension-specific encoding, capturing diverse scales and enhancing efficiency.

Driven by our analysis, we present a new video position embedding strategy, \textbf{\methodname}, which can simultaneously satisfy the four properties in Tab. \ref{tab:pe_compare}.
Specifically, we use a 3D structure to model spatiotemporal information, allocating higher dimensions (lower frequencies), to the temporal axis (\textbf{L}ow-frequency \textbf{T}emporal \textbf{A}llocation, \textbf{LTA}) to prioritize temporal modeling.
The right panel of Fig. \ref{fig:v-ruler} demonstrates that our LTA allocation mitigates oscillations and exhibits robustness to distractors in the V-NIAH-D task.
We further employ a \textbf{D}iagonal \textbf{L}ayout (\textbf{DL}) design to ensure spatial symmetry and preserve the relative positioning between visual and text tokens.
Regarding temporal index scaling, we propose \textbf{A}djustable \textbf{T}emporal \textbf{S}pacing (\textbf{ATS}), where a hyper-parameter controls the relative temporal spacing of adjacent visual tokens.
In summary, our proposed position encoding scheme demonstrates favorable characteristics for modeling video data, yielding a robust and effective representation of positional information.

Overall, the contributions of this work are summarized as:

\textbf{(1)} We present an analysis of four key properties essential for RoPE when applied to video. Motivated by this analysis, we propose \methodname including Low-frequency Temporal Allocation (LTA), Diagonal Layout (DL), and Adjustable Temporal Spacing (ATS) to satisfy all four properties.

% Inspired by the evaluation of long-context LLM,
\textbf{(2)} We introduce the challenging V-NIAH-D task to expose the drawbacks of current position embedding designs regarding frequency allocation. Our findings reveal that existing Video LLMs are easily misled to frequency-based distractors.

\textbf{(3)} Extensive experiments demonstrate that \methodname consistently achieves superior performance compared to other RoPE variants. For example, \methodname outperforms previous M-RoPE on long video retrieval (\textbf{+12.4} on V-NIAH, \textbf{+12.4} on V-NIAH-D), video understanding (\textbf{+2.9} on LongVideoBench, \textbf{+4.5} on MLVU, \textbf{+1.7} on Video-MME) and hallucination (\textbf{+11.9} on VideoHallucer) benchmarks.