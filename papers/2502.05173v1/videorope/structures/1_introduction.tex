\section{Introduction}

Video large language models (VLLMs) play a crucial role in video understanding tasks~\cite{2023videochat,lin2023video,chen2024sharegpt4video,maaz2024videochatgptdetailedvideounderstanding}. Current research trends are increasingly focusing on scaling the understanding capabilities of these models, enabling them to comprehend longer videos and, ultimately, bring them closer to human-level understanding~\cite{zhang2024longva,wang2024longllavascalingmultimodalllms,chen2024longvilascalinglongcontextvisual,internlmxcomposer2_5_OL}.

However, while the evaluation of long-context language models (LLMs) has advanced, the evaluation of long-context multimodal large language models (MLLMs), particularly \textbf{VLLMs}, remains relatively underdeveloped~\cite{zhang2024longva}. In long-context LLM tasks such as long-context question answering (QA) and retrieval, the presence of distractors plays a crucial role~\cite{hsieh2024ruler,yuan2024lv}, as they significantly increase the difficulty and provide a more effective means of testing the robustness and capacity of these models. These insights into the evaluation of long-context LLMs are highly relevant for improving the assessment and performance of VLLMs, where handling distractors remains a critical challenge. Relevant tasks, such as those in \cite{yang2024qwen2,yang2024qwen25,InternLM3,team2024jamba}, have been widely applied to long-context LLMs and can offer valuable lessons for the evaluation of VLLMs in similar scenarios.

Regarding long video, we conduct an attempt as shown in the Figure~\ref{fig:v-ruler}. Based on V-NIAH (Visual Needle-In-A-Haystack), a classic long-video retrieval task~\cite{zhang2024longva}, we insert several similar images that do not affect the question's answer before and after the needle image as distractor information~\citep{hsieh2024ruler,yuan2024lv}, forming a new long-context evaluation task, V-NIAH-D (Visual Needle-In-A-Haystack with Distractors). This is proposed to investigate whether the current long-video MLLMs can effectively avoid misleading distractors, still find the true needle image, and answer questions correctly. Unfortunately, for the long-video MLLMs claimed capable for V-NIAH, such as Qwen2-VL~\cite{wang2024qwen2}, we find that it lacks sufficient robustness and is misled by distractors in long video, showing a significant performance decline.

\input{figures/v_ruler}

\begin{table}[t]
\tiny
\centering
\tabcolsep=0.15cm
\begin{tabular}{lcccc}
\toprule
 & \makecell[c]{\textbf{Multi-Modal} \\ \textbf{Compatibility}} & \makecell[c]{\textbf{Dimension} \\ \textbf{Distribution}} & \makecell[c]{\textbf{Spatial} \\ \textbf{Symmetry}} & \makecell[c]{\textbf{Temporal} \\ \textbf{Alignment}} \\
\midrule
\makecell[l]{Vanilla RoPE \cite{su2024roformer}} & \xmark & \xmark & \xmark & \xmark \\
\makecell[l]{RoPE-Tie \cite{kexuefm10040}} & \cmark & \xmark & \cmark & \xmark \\
\makecell[l]{M-RoPE \cite{wang2024qwen2}} & \cmark & \xmark & \xmark & \xmark \\
\makecell[l]{TC-LLaVA \cite{gao2024tc}} & \xmark & \xmark & \xmark & \cmark \\
% \makecell[l]{V2PE\\\cite{ge2024v2pe}} & \xmark & \xmark & \xmark & \cmark \\
\rowcolor[HTML]{F2F3F5}
\methodname (Ours) & \cmark & \cmark & \cmark & \cmark \\
\bottomrule
\end{tabular}
\caption{Comparison between different RoPE designs for Video Large Language Models (VLLMs).}
\label{tab:pe_compare}
\end{table}

Inspired by research in long-context LLMs~\cite{liu2023scaling,men2024base,barbero2024round}, we attribute this phenomenon to the inadequate design of RoPE (Rotational Position Embedding)~\cite{su2024roformer} in MLLMs. The existing RoPE designs have collisions in position embedding in long-context modeling, and thus fail to differentiate when semantic information is similar. Therefore, we conduct an in-depth analysis and summarize the properties that RoPE for multi-modal inputs should possess, especially for long videos, including \textbf{\textit{Multi-Modal Compatibility}}, whether RoPE can simultaneously describe the spatiotemporal position in multi-modal inputs and sequential position in text-only inputs~\cite{wang2024qwen2,kexuefm10040,kexuefm10352}, \textbf{\textit{Appropriate Dimension Distribution}}, whether the feature dimension can process the semantic relationship where it is responsibility~\cite{peng2023yarn,barbero2024round,liu2024kangaroo}, \textbf{\textit{Spatial Symmetry}}, whether the distance between the end of precedent textual input and start of visual input equals the distance between the end of visual input and the start of subsequent textual input~\cite{kexuefm10352}, and \textbf{\textit{Temporal Alignment}}, whether the alignment of sequential feature in different modality is considered~\cite{gao2024tc}.

Based on the above analysis, we identify the shortcomings in existing multi-modal RoPE designs and propose our solution, \methodname, which can simultaneously satisfy the four properties as shown in Table~\ref{tab:pe_compare} and has been validated for effectiveness on Qwen2-7B\citep{wang2024qwen2}. \methodname demonstrates improved robustness and long-context capabilities in synthetic tasks including V-NIAH and V-NIAH-D, achieving stable modeling of long videos over 430k tokens. Moreover, it consistently outperforms other multi-modal RoPE designs in the mainstream video evaluation tasks, including LongVideoBench~\cite{wu2024longvideobench}, MLVU~\cite{zhou2024mlvu}, Video-MME~\cite{fu2024video}, and VideoHallucer~\cite{videohallucer}, achieving robust comprehension of long videos over 64k tokens. In summary, the contributions of our work can be summarized as follows.
\begin{itemize}
\item Inspired by the evaluation of long-context LLM, we propose the V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task and discover that existing long-video LLMs are easily affected by misleading factors due to the shortage of position embedding design.
\item We summarize the properties that long-video RoPE should possess: multi-modal compatibility, appropriate dimension distribution, spatial modeling symmetry, and temporal modeling alignment. Based on this, we propose \methodname that satisfies all these properties.
\item We validate the effectiveness and superiority of \methodname on Qwen2-7B, finding that it not only achieves stable retrieval of long videos in V-NIAH and V-NIAH-D but also consistently outperforms across comprehensive benchmarks like LongVideoBench, MLVU, Video-MME, and VideoHallucer.
\end{itemize}
