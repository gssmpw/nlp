\begin{abstract}

While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge.
This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work.
As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH.
The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors.
Based on our analysis, we introduce \textbf{VideoRoPE}, with a \textit{3D structure} designed to preserve spatio-temporal relationships.
VideoRoPE features \textit{low-frequency temporal allocation} to mitigate periodic oscillations, a \textit{diagonal layout} to maintain spatial symmetry, and \textit{adjustable temporal spacing} to decouple temporal and spatial indexing.
VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination.
Our code will be available at \href{https://github.com/Wiselnn570/VideoRoPE}{https://github.com/Wiselnn570/VideoRoPE}.

% Video large language models (VLLMs) are evolving to understand longer video sequences, yet challenges remain in their ability to effectively process and interpret these extended contexts. Specifically, through a retrieval-based task incorporating distractors—\textbf{V-NIAH-D} (Visual Needle-In-A-Haystack with Distractors)—we find that VLLMs are highly susceptible to interference from these distractor signals. We attribute this issue to suboptimal position embedding designs, which prompts us to reflect on what constitutes an effective video position embedding. In this context, we identify four key properties that multi-modal position embeddings must have for long-video tasks: multi-modal compatibility, appropriate dimensional distribution, spatial symmetry, and temporal alignment. To address these needs, we propose \textbf{\methodname}, a novel position embedding design. \methodname not only satisfies these requirements but also outperforms existing RoPE methods in both the \textbf{V-NIAH} and \textbf{V-NIAH-D} tasks. Evaluations further demonstrate that \methodname significantly enhances both long-video comprehension and video hallucination, achieving notable improvements across long-video comprehension benchmarks, such as \textbf{LongVideoBench}, \textbf{MLVU}, and \textbf{Video-MME}, with average score increases of 2.29, 4.46, and 0.7 (64k context), respectively. Additionally, it shows a substantial improvement in the video hallucination benchmark, \textbf{VideoHallucer}, with an average score increase of 11.4 points.

\end{abstract}