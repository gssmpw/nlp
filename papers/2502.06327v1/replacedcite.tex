\section{Related Work}
\subsection{Continual Graph Learning}

Continual Graph Learning (CGL) aims to address the challenge of learning from a stream of graph-structured data over time while mitigating catastrophic forgetting. Current CGL methods can be broadly categorized into three main strategies: regularization methods____, architectural design methods____, and memory replay-based methods____. Regularization methods focus on preventing catastrophic forgetting by adding constraints that help preserve knowledge across different learning tasks. Notable methods include TWP____, which integrates topological information to retain learned features when adapting to new tasks. However, these methods often compromise the model's capacity to adapt efficiently to novel tasks, as the regularization can interfere with learning new knowledge. Architectural design approaches involve changes to the structure of the model to enhance its ability to learn and retain knowledge over time. For instance, HPNs____ introduce atomic feature extractors and hierarchical systems that scale dynamically to accommodate new knowledge. This approach increases model parameters and memory requirements as new tasks are added. Other architectural approaches focus on sparsity and modularization, which balance the trade-off between task retention and model expansion____. Memory replay-based methods are perhaps the most effective and widely studied, as they maintain a memory buffer to store data from previous tasks, which is replayed to mitigate forgetting while learning new tasks. A representative method is ER-GNN____, where experience replay involves storing sampled nodes and replaying them while learning new tasks. Other methods, such as SSM____ and CaT____, improve the efficiency of replay by using sparsified subgraphs or condensed graph modules to reduce replay memory overhead. Despite these advances, the scalability of replay-based methods is limited by increasing memory requirements, especially as the graph scale increases. 

In contrast to memory replay-based approaches, which typically store graph nodes at a ratio of 0.01, our approach introduces a novel method that significantly reduces memory overhead. By preserving only a small number of task-specific prompts (just 2 or 3), we enable the model to effectively learn the sequential tasks without incurring the memory overhead typical of traditional replay-based methods. Our approach achieves SOTA performance while minimizing memory costs, and addressing the scalability limitations of current CGL techniques.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{Figure/Fig2.pdf}
	\caption{\textbf{Illustration of \ourmethod framework.} Here we present the execution steps for task $\mathcal{T}_t$. All tasks except $\mathcal{T}_0$ follow the same procedure. The backbone parameters, pre-trained on task $\mathcal{T}_0$, remain frozen in subsequent tasks. Initially, node-level personalized prompts are generated by the personalized prompt generator (PG) based on the query result of the node feature and a maintained small node-level prompt set, which are then added to the node features. These are processed through 1-th layer GNN to obtain node representations with topological information. Subsequently, subgraph-level personalized prompts are generated and added using the same method and passed into the subsequent networks. Learned prompts are saved into prompt bank after each task and selected based on task identity during inference for prediction.}
	\label{fig2}
\end{figure*}

\subsection{Prompt Learning}

Recently, prompt learning has emerged as a powerful technique in machine learning, particularly in natural language processing (NLP) and computer vision (CV). This approach has gained prominence due to its ability to adapt pre-trained large models to new tasks with minimal retraining, making it highly efficient for transfer learning scenarios____. In NLP, prompt learning can be divided into two major categories: hard prompts and soft prompts. Hard prompts are manually crafted text additions, such as those used in PET-SGLUE____, where predefined templates are used to guide model predictions. Soft prompts, on the other hand, involve learnable vectors optimized for specific datasets, as seen in approaches like Prefix-tuning____, which inserts task-specific vectors into the model while leaving the pre-trained parameters frozen. In CV, prompt learning often uses pre-defined visual prompts or learnable embeddings for task-specific adaptations____, extending the versatility of visual transformer models to handle a wide variety of tasks, such as image captioning____, classification____, and object detection____.

Despite its success in both NLP and CV, the unique characteristics of graph data—such as its non-sequential nature and complex relational structure—present challenges for directly applying these prompt learning techniques. As a result, there has been a growing interest in developing specialized prompt learning techniques tailored to graph data.

\subsection{Graph Prompt Learning}

Existing research in graph prompt learning has predominantly focused on customizing pre-training tasks and leveraging designed prompts to address various graph tasks within static graphs, such as node classification, edge prediction, and graph classification____. The goal of these methods is to reformulate downstream tasks on a static graph in a manner that aligns with pretext tasks, allowing models to generalize better across multiple tasks. GPPT____ was one of the pioneering works in this area, enhancing GNN generalization capabilities through a combination of graph pre-training and prompt-tuning. The GPPT method begins with pre-training on link prediction tasks and then reformulates node classification tasks as link predictions between target nodes and category nodes, leveraging prompts to adapt to the specific task at hand. This approach allows the model to learn a shared representation of nodes and edges that is transferable across different tasks. Building on this idea, GPF____ introduces a universal prompt-tuning method, which fine-tunes only a small subset of parameters across various graph tasks, allowing for task-specific adaptations without retraining the entire model. The ``All in One" approach____ further extends this by reformulating node and edge prediction tasks as subgraph-level tasks and designing multi-task prompts using meta-learning techniques to handle a variety of graph tasks. These methods have made significant strides in bridging the gap between different tasks within the same graph. However, most of these approaches focus on static graphs, where the topology and feature distribution remain fixed.

While static graph-based methods excel in scenarios where the graph structure and features do not change, they fall short when applied to dynamic, incremental graphs. The topology and feature distributions in such graphs are continually evolving, posing a unique challenge in maintaining performance across tasks that span different graphs or across time. In contrast to these static graph-based approaches, we focus on extending graph prompt learning to dynamic graph settings. Our method addresses the feature and topology gaps between different task graphs in dynamic, evolving environments, providing a more flexible and scalable solution for CGL scenarios.