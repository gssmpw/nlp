\section{Related Works}
Existing works on product attribute value extraction mainly focus on supervised learning to train classification models____, QA-based models____ or large language models____. 
However, these approaches require large quantities of labeled data for training.
Recently, some works use few-shot learning____ and weakly supervised learning____ to reduce the amount of labeled data for training. 
But these approaches still need labeled data for multi-task training or iterative training.

To extract unseen attribute values, text-mining models____ extract explicit attribute values directly from text, and zero-shot models____ predict new attribute values by inductive link prediction of graphs.
However, all these approaches can only extract attribute values from textual inputs. 
In other words, these models are from a single modality. 
Then, some multi-modal models use both the product image and title with the description as the inputs to learn a better product representation for attribute value extraction____.
Though performance is improved by fusing more semantic information from multiple modalities, more input data is needed during the training stage. 
To enable image-first interactions from sellers and make it simple for the users, we propose a zero-shot cross-modal model motivated by image captioning____ for attribute value generation, where only images are used as inputs.