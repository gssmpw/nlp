@inproceedings{chen2022extreme,
  title={Extreme multi-label classification with label masking for product attribute value extraction},
  author={Chen, Wei-Te and Xia, Yandi and Shinzato, Keiji},
  booktitle={Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5)},
  pages={134--140},
  year={2022}
}

@INPROCEEDINGS{10020304,
  author={Deng, Zhongfen and Chen, Wei-Te and Chen, Lei and Yu, Philip S.},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={AE-smnsMLC: Multi-Label Classification with Semantic Matching and Negative Label Sampling for Product Attribute Value Extraction}, 
  year={2022},
  volume={},
  number={},
  pages={1816-1821},
  keywords={Annotations;Semantics;Big Data;Sampling methods;Data models;Data mining;Electronic commerce;product attribute value extraction;multi-label classification;semantic matching},
  doi={10.1109/BigData55660.2022.10020304}}

@article{zou2024eiven,
  title={EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM},
  author={Zou, Henry Peng and Yu, Gavin Heqing and Fan, Ziwei and Bu, Dan and Liu, Han and Dai, Peng and Jia, Dongmei and Caragea, Cornelia},
  journal={arXiv preprint arXiv:2404.08886},
  year={2024}
}

@inproceedings{chen-etal-2023-named,
    title = "Does Named Entity Recognition Truly Not Scale Up to Real-world Product Attribute Extraction?",
    author = "Chen, Wei-Te  and
      Shinzato, Keiji  and
      Yoshinaga, Naoki  and
      Xia, Yandi",
    editor = "Wang, Mingxuan  and
      Zitouni, Imed",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-industry.16",
    doi = "10.18653/v1/2023.emnlp-industry.16",
    pages = "152--159",
    abstract = "The key challenge in the attribute-value extraction (AVE) task from e-commerce sites is the scalability to diverse attributes for a large number of products in real-world e-commerce sites. To make AVE scalable to diverse attributes, recent researchers adopted a question-answering (QA)-based approach that additionally inputs the target attribute as a query to extract its values, and confirmed its advantage over a classical approach based on named-entity recognition (NER) on real-word e-commerce datasets. In this study, we argue the scalability of the NER-based approach compared to the QA-based approach, since researchers have compared BERT-based QA-based models to only a weak BiLSTM-based NER baseline trained from scratch in terms of only accuracy on datasets designed to evaluate the QA-based approach. Experimental results using a publicly available real-word dataset revealed that, under a fair setting, BERT-based NER models rival BERT-based QA models in terms of the accuracy, and their inference is faster than the QA model that processes the same product text several times to handle multiple target attributes.",
}

@inproceedings{liu2023knowledge,
  title={Knowledge-selective pretraining for attribute value extraction},
  author={Liu, Hui and Yin, Qingyu and Wang, Zhengyang and Zhang, Chenwei and Jiang, Haoming and Gao, Yifan and Li, Zheng and Li, Xian and Zhang, Chao and Yin, Bing and others},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={8062--8074},
  year={2023}
}

@article{shinzato2022simple,
  title={Simple and effective knowledge-driven query expansion for QA-based product attribute extraction},
  author={Shinzato, Keiji and Yoshinaga, Naoki and Xia, Yandi and Chen, Wei-Te},
  journal={arXiv preprint arXiv:2206.14264},
  year={2022}
}

@inproceedings{wang2020learning,
  title={Learning to extract attribute value from product via question answering: A multi-task approach},
  author={Wang, Qifan and Yang, Li and Kanagal, Bhargav and Sanghai, Sumit and Sivakumar, D and Shu, Bin and Yu, Zac and Elsas, Jon},
  booktitle={Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={47--55},
  year={2020}
}

@inproceedings{10.1145/3583780.3615142,
author = {Gong, Jiaying and Chen, Wei-Te and Eldardiry, Hoda},
title = {Knowledge-Enhanced Multi-Label Few-Shot Product Attribute-Value Extraction},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615142},
doi = {10.1145/3583780.3615142},
abstract = {Existing attribute-value extraction (AVE) models require large quantities of labeled data for training. However, new products with new attribute-value pairs enter the market every day in real-world e-Commerce. Thus, we formulate AVE in multi-label few-shot learning (FSL), aiming to extract unseen attribute value pairs based on a small number of training examples. We propose a Knowledge-Enhanced Attentive Framework (KEAF) based on prototypical networks, leveraging the generated label description and category information to learn more discriminative prototypes. Besides, KEAF integrates with hybrid attention to reduce noise and capture more informative semantics for each class by calculating the label-relevant and query-related weights. To achieve multi-label inference, KEAF further learns a dynamic threshold by integrating the semantic information from both the support set and the query set. Extensive experiments with ablation studies conducted on two datasets demonstrate that our proposed model significantly outperforms other SOTA models for information extraction in few-shot learning.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {3902â€“3907},
numpages = {6},
keywords = {multi-label few-shot learning, attribute value extraction},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@inproceedings{zhang2022oa,
  title={Oa-mine: Open-world attribute mining for e-commerce products with weak supervision},
  author={Zhang, Xinyang and Zhang, Chenwei and Li, Xian and Dong, Xin Luna and Shang, Jingbo and Faloutsos, Christos and Han, Jiawei},
  booktitle={Proceedings of the ACM Web Conference 2022},
  pages={3153--3161},
  year={2022}
}

@inproceedings{yang2023mixpave,
  title={Mixpave: Mix-prompt tuning for few-shot product attribute value extraction},
  author={Yang, Li and Wang, Qifan and Wang, Jingang and Quan, Xiaojun and Feng, Fuli and Chen, Yu and Khabsa, Madian and Wang, Sinong and Xu, Zenglin and Liu, Dongfang},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={9978--9991},
  year={2023}
}

@article{fang2024llm,
  title={LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction},
  author={Fang, Chenhao and Li, Xiaohan and Fan, Zezhong and Xu, Jianpeng and Nag, Kaushiki and Korpeoglu, Evren and Kumar, Sushant and Achan, Kannan},
  journal={arXiv preprint arXiv:2403.00863},
  year={2024}
}

@INPROCEEDINGS{10386204,
  author={Deng, Zhongfen and Peng, Hao and Zhang, Tao and Liu, Shuaiqi and Zhao, Wenting and Wang, Yibo and Yu, Philip S.},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={JPAVE: A Generation and Classification-based Model for Joint Product Attribute Prediction and Value Extraction}, 
  year={2023},
  volume={},
  number={},
  pages={1087-1094},
  keywords={Training;Predictive models;Multitasking;Data models;Question answering (information retrieval);Generators;Data mining;product attribute value extraction;attribute value generation;value classification;multi-task learning},
  doi={10.1109/BigData59044.2023.10386204}}

@article{zou2024implicitave,
  title={ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction},
  author={Zou, Henry Peng and Samuel, Vinay and Zhou, Yue and Zhang, Weizhi and Fang, Liancheng and Song, Zihe and Yu, Philip S and Caragea, Cornelia},
  journal={arXiv preprint arXiv:2404.15592},
  year={2024}
}

@article{xu2023towards,
  title={Towards open-world product attribute mining: A lightly-supervised approach},
  author={Xu, Liyan and Zhang, Chenwei and Li, Xian and Shang, Jingbo and Choi, Jinho D},
  journal={arXiv preprint arXiv:2305.18350},
  year={2023}
}

@inproceedings{wang2022smartave,
  title={Smartave: Structured multimodal transformer for product attribute value extraction},
  author={Wang, Qifan and Yang, Li and Wang, Jingang and Krishnan, Jitin and Dai, Bo and Wang, Sinong and Xu, Zenglin and Khabsa, Madian and Ma, Hao},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={263--276},
  year={2022}
}

@inproceedings{li2023attgen,
  title={AtTGen: Attribute Tree Generation for Real-World Attribute Joint Extraction},
  author={Li, Yanzeng and Xue, Bingcong and Zhang, Ruoyu and Zou, Lei},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2139--2152},
  year={2023}
}


@article{brinkmann2023product,
  title={Product Attribute Value Extraction using Large Language Models},
  author={Brinkmann, Alexander and Shraga, Roee and Bizer, Christian},
  journal={arXiv preprint arXiv:2310.12537},
  year={2023}
}

@article{baumann2024using,
  title={Using LLMs for the Extraction and Normalization of Product Attribute Values},
  author={Baumann, Nick and Brinkmann, Alexander and Bizer, Christian},
  journal={arXiv preprint arXiv:2403.02130},
  year={2024}
}

@inproceedings{liu2023multimodal,
  title={Multimodal pre-training with self-distillation for product understanding in e-commerce},
  author={Liu, Shilei and Li, Lin and Song, Jun and Yang, Yonghua and Zeng, Xiaoyi},
  booktitle={Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
  pages={1039--1047},
  year={2023}
}

@inproceedings{wang2023mpkgac,
  title={MPKGAC: Multimodal Product Attribute Completion in E-commerce},
  author={Wang, Kai and Shao, Jianzhi and Zhang, Tao and Chen, Qijin and Huo, Chengfu},
  booktitle={Companion Proceedings of the ACM Web Conference 2023},
  pages={336--340},
  year={2023}
}

@inproceedings{ghosh2023d,
  title={D-Extract: Extracting dimensional attributes from product images},
  author={Ghosh, Pushpendu and Wang, Nancy and Yenigalla, Promod},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3641--3649},
  year={2023}
}

@article{liu2022boosting,
  title={Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified Learning Scheme and Dynamic Range Minimization},
  author={Liu, Mengyin and Zhu, Chao and Gao, Hongyu and Gu, Weibo and Wang, Hongfa and Liu, Wei and Yin, Xu-cheng},
  journal={arXiv preprint arXiv:2207.07278},
  year={2022}
}


@inproceedings{gong2024multi,
author = {Gong, Jiaying and Eldardiry, Hoda},
title = {Multi-Label Zero-Shot Product Attribute-Value Extraction},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645649},
doi = {10.1145/3589334.3645649},
abstract = {E-commerce platforms should provide detailed product descriptions (attribute values) for effective product search and recommendation. However, attribute value information is typically not available for new products. To predict unseen attribute values, large quantities of labeled training data are needed to train a traditional supervised learning model. Typically, it is difficult, time-consuming, and costly to manually label large quantities of new product profiles. In this paper, we propose a novel method to efficiently and effectively extract unseen attribute values from new products in the absence of labeled data (zero-shot setting). We propose HyperPAVE, a multi-label zero-shot attribute value extraction model that leverages inductive inference in heterogeneous hypergraphs. In particular, our proposed technique constructs heterogeneous hypergraphs to capture complex higher-order relations (i.e. user behavior information) to learn more accurate feature representations for graph nodes. Furthermore, our proposed HyperPAVE model uses an inductive link prediction mechanism to infer future connections between unseen nodes. This enables HyperPAVE to identify new attribute values without the need for labeled training data. We conduct extensive experiments with ablation studies on different categories of the MAVE dataset. The results demonstrate that our proposed HyperPAVE model significantly outperforms existing classification-based, generation-based large language models for attribute value extraction in the zero-shot setting.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {2259â€“2270},
numpages = {12},
keywords = {attribute value extraction, heterogeneous hypergraph, inductive link prediction, zero-shot learning},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{zeng2023conzic,
  title={Conzic: Controllable zero-shot image captioning by sampling-based polishing},
  author={Zeng, Zequn and Zhang, Hao and Lu, Ruiying and Wang, Dongsheng and Chen, Bo and Wang, Zhengjue},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23465--23476},
  year={2023}
}

@inproceedings{tewel2022zerocap,
  title={Zerocap: Zero-shot image-to-text generation for visual-semantic arithmetic},
  author={Tewel, Yoad and Shalev, Yoav and Schwartz, Idan and Wolf, Lior},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={17918--17928},
  year={2022}
}

@inproceedings{fei2023transferable,
  title={Transferable decoding with visual entities for zero-shot image captioning},
  author={Fei, Junjie and Wang, Teng and Zhang, Jinrui and He, Zhenyu and Wang, Chengjie and Zheng, Feng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3136--3146},
  year={2023}
}

@inproceedings{guo2023images,
  title={From images to textual prompts: Zero-shot visual question answering with frozen large language models},
  author={Guo, Jiaxian and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Li, Boyang and Tao, Dacheng and Hoi, Steven},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10867--10877},
  year={2023}
}

@inproceedings{xu2023zero,
  title={Zero-TextCap: Zero-shot Framework for Text-based Image Captioning},
  author={Xu, Dongsheng and Zhao, Wenye and Cai, Yi and Huang, Qingbao},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={4949--4957},
  year={2023}
}

@inproceedings{yang2022mave,
  title={Mave: A product dataset for multi-source attribute value extraction},
  author={Yang, Li and Wang, Qifan and Yu, Zac and Kulkarni, Anand and Sanghai, Sumit and Shu, Bin and Elsas, Jon and Kanagal, Bhargav},
  booktitle={Proceedings of the fifteenth ACM international conference on web search and data mining},
  pages={1256--1265},
  year={2022}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{wang2022git,
  title={Git: A generative image-to-text transformer for vision and language},
  author={Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  journal={arXiv preprint arXiv:2205.14100},
  year={2022}
}

@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@article{dai2024instructblip,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@inproceedings{lin-och-2004-orange,
  title = {{{ORANGE}}: A Method for Evaluating Automatic Evaluation Metrics for Machine Translation},
  booktitle = {{{COLING}} 2004: {{Proceedings}} of the 20th International Conference on Computational Linguistics},
  author = {Lin, Chin-Yew and Och, Franz Josef},
  year = {2004},
  month = aug,
  pages = {501--507},
  publisher = {{COLING}},
  address = {{Geneva, Switzerland}}
}

@inproceedings{lin-2004-rouge,
  title = {{{ROUGE}}: {{A}} Package for Automatic Evaluation of Summaries},
  booktitle = {Text Summarization Branches Out},
  author = {Lin, Chin-Yew},
  year = {2004},
  month = jul,
  pages = {74--81},
  publisher = {{Association for Computational Linguistics}},
  address = {{Barcelona, Spain}}
}

@inproceedings{hou2021few,
  title={Few-shot learning for multi-label intent detection},
  author={Hou, Yutai and Lai, Yongkui and Wu, Yushan and Che, Wanxiang and Liu, Ting},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={14},
  pages={13036--13044},
  year={2021}
}

@article{pourpanah2022review,
  title={A review of generalized zero-shot learning methods},
  author={Pourpanah, Farhad and Abdar, Moloud and Luo, Yuxuan and Zhou, Xinlei and Wang, Ran and Lim, Chee Peng and Wang, Xi-Zhao and Wu, QM Jonathan},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={45},
  number={4},
  pages={4051--4070},
  year={2022},
  publisher={IEEE}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}

@inproceedings{
hu2025hypergraphbased,
title={Hypergraph-based Zero-shot Multi-modal Product Attribute Value Extraction},
author={Jiazhen Hu and Jiaying Gong and Hongda Shen and Hoda Eldardiry},
booktitle={THE WEB CONFERENCE 2025},
year={2025},
url={https://openreview.net/forum?id=tmQDHqzupm}
}