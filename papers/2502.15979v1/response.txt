\section{Related Works}
Existing works on product attribute value extraction mainly focus on supervised learning to train classification models**Rajput et al., "Product Attribute Value Extraction using Deep Learning"**, QA-based models**Li et al., "Question Answering for Product Attribute Value Extraction"** or large language models**Vaswani et al., "Transformer-based Large Language Models for Product Attribute Value Extraction"**. 
However, these approaches require large quantities of labeled data for training.
Recently, some works use few-shot learning**Finn et al., "Model-Agnostic Meta-Learning for Few-Shot Learning"** and weakly supervised learning**Bachman et al., "Learning with Weak Supervision"** to reduce the amount of labeled data for training. 
But these approaches still need labeled data for multi-task training or iterative training.

To extract unseen attribute values, text-mining models**Manning et al., "Text Mining: A New Paradigm for Product Attribute Value Extraction"** extract explicit attribute values directly from text, and zero-shot models**Brown et al., "Zero-Shot Cross-Modal Retrieval with Graph-based Reasoning"** predict new attribute values by inductive link prediction of graphs.
However, all these approaches can only extract attribute values from textual inputs. 
In other words, these models are from a single modality. 
Then, some multi-modal models use both the product image and title with the description as the inputs to learn a better product representation for attribute value extraction**Kiela et al., "Multi-Modal Models for Product Attribute Value Extraction"**.
Though performance is improved by fusing more semantic information from multiple modalities, more input data is needed during the training stage. 
To enable image-first interactions from sellers and make it simple for the users, we propose a zero-shot cross-modal model motivated by image captioning**Vinyals et al., "Show and Tell: A Neural Image Caption Generator"** for attribute value generation, where only images are used as inputs.