\section{Related Works}
Existing works on product attribute value extraction mainly focus on supervised learning to train classification models~\cite{10386204, chen2022extreme, 10020304}, QA-based models~\cite{chen-etal-2023-named, liu2023knowledge, shinzato2022simple, wang2020learning} or large language models~\cite{fang2024llm, brinkmann2023product, baumann2024using}. 
However, these approaches require large quantities of labeled data for training.
Recently, some works use few-shot learning~\cite{10.1145/3583780.3615142, yang2023mixpave} and weakly supervised learning~\cite{xu2023towards, zhang2022oa} to reduce the amount of labeled data for training. 
But these approaches still need labeled data for multi-task training or iterative training.

To extract unseen attribute values, text-mining models~\cite{li2023attgen, xu2023towards} extract explicit attribute values directly from text, and zero-shot models~\cite{hu2025hypergraphbased, gong2024multi} predict new attribute values by inductive link prediction of graphs.
However, all these approaches can only extract attribute values from textual inputs. 
In other words, these models are from a single modality. 
Then, some multi-modal models use both the product image and title with the description as the inputs to learn a better product representation for attribute value extraction~\cite{zou2024implicitave, zou2024eiven, liu2023multimodal, wang2023mpkgac, ghosh2023d, wang2022smartave, liu2022boosting}.
Though performance is improved by fusing more semantic information from multiple modalities, more input data is needed during the training stage. 
To enable image-first interactions from sellers and make it simple for the users, we propose a zero-shot cross-modal model motivated by image captioning~\cite{fei2023transferable, guo2023images, xu2023zero, zeng2023conzic, tewel2022zerocap} for attribute value generation, where only images are used as inputs.