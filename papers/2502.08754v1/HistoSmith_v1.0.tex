% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
%\usepackage{subcaption}
\usepackage{multirow}
%\usepackage{subfig}
%\usepackage{graphicx}
%\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage[outdir=./]{epstopdf}
\usepackage{subcaption}
\usepackage{caption} 
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\captionsetup[table]{skip=8pt}
\begin{document}
%
%\title{Contribution Title\thanks{Supported by organization x.}}
\title{HistoSmith: Single-Stage Histology Image-Label Generation via Conditional Latent Diffusion for Enhanced Cell Segmentation and Classification}

%\name{Valentina Vadori$^{1}$ \ Jean-Marie Graïc$^{2}$ \ Livio Finos$^{3}$ \ Livio Corain$^{4}$ \ Antonella Peruffo$^2$ \ Enrico Grisan$^{1}$ }
%\address{$^{1}$London South Bank University, School of Engineering, United Kingdom
%\\ $^{2}$University of Padova, Dept. of Comparative Biomedicine \& Food Science, Italy
%\\ $^{3}$University of Padova, Dept. of Developmental Psychology and Socialisation, Italy
%\\ $^{4}$University of Padova, Dept. of Management and Engineering, Italy}


%
\titlerunning{HistoSmith: Single-Stage Histology Image-Label Generation}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{Valentina Vadori\inst{1}\orcidID{0009-0004-8043-741X} \and
%Antonella Peruffo\inst{2}\orcidID{0000-0003-0454-9087} \and
%Jean-Marie Graïc\inst{2}\orcidID{0000-0002-1974-8356} \and
%Livio Finos\inst{3}\orcidID{0000-0003-3181-8078} \and
%Livio Corain\inst{4}\orcidID{0000-0002-8104-1255} \and
%Enrico Grisan\inst{1}\orcidID{0000-0002-7365-5652}}

\author{Valentina Vadori\inst{1} \and
Jean-Marie Graïc\inst{2} \and
Antonella Peruffo\inst{2} \and
%Jean-Marie Graïc\inst{2} \and
%Giulia Vadori\inst{2} \and
Livio Finos\inst{3} \and
Ujwala Kiran Chaudhari\inst{1} \and
Enrico Grisan\inst{1}}

%
\authorrunning{Vadori et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Dept. of Computer Science \& Informatics, London South Bank University, UK\\
\email{\{vvadori,egrisan\}@lsbu.ac.uk} \and
Dept. of Comparative Biomedicine \& Food Science, University of Padova, IT\and
Dept. of Statistical Sciences, University of Padova, IT}
%\institute{Dept. of Computer Science \& Informatics, London South Bank University, UK\\
%\email{\{vvadori,egrisan\}@lsbu.ac.uk} \and
%Dept. of Comparative Biomedicine \& Food Science, University of Padova, IT\\
%\email{\{antonella.peruffo,jeanmarie.graic,giulia.vadori\}@unipd.it} \and
%Dept. of Statistical Sciences, University of Padova, IT \\
%\email{livio.finos@unipd.it}}

%\institute{London South Bank University, School of Engineering, United Kingdom  \and
%University of Padova, Dept. of Comparative Biomedicine \& Food Science, Italy
%\email{lncs@springer.com}\\
%\url{http://www.springer.com/gp/computer-science/lncs} \and
%ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%\email{\{abc,lncs\}@uni-heidelberg.de}}

%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Precise segmentation and classification of cell instances are vital for analyzing the tissue microenvironment in histology images, supporting medical diagnosis, prognosis, treatment planning, and studies of brain cytoarchitecture. However, the creation of high-quality annotated datasets for training remains a major challenge. 
This study introduces a novel single-stage approach (\textit{HistoSmith}) for generating image-label pairs to augment histology datasets. Unlike state-of-the-art methods that utilize diffusion models with separate components for label and image generation, our approach employs a latent diffusion model to learn the joint distribution of cellular layouts, classification masks, and histology images. This model enables tailored data generation by conditioning on user-defined parameters such as cell types, quantities, and tissue types.
Trained on the Conic H\&E histopathology dataset and the Nissl-stained CytoDArk0 dataset, the model generates realistic and diverse labeled samples. Experimental results demonstrate improvements in cell instance segmentation and classification, particularly for underrepresented cell types like neutrophils in the Conic dataset. %(<1\% of cells). 
These findings underscore the potential of our approach to address data scarcity challenges.  %in histology analysis.
%Precise segmentation and classification of cell instances are crucial for detailed quantification of the tissue microenvironment in histology images. These processes play a vital role in medical diagnosis, prognosis, treatment planning, and comparative studies of brain cytoarchitecture, focusing on cellular arrangement and morphology. Despite advancements in deep learning models for these tasks, the development of high-quality annotated datasets for training remains a significant challenge. In this study, we introduce a novel one-step approach for generating image-mask pairs to augment histology datasets. Unlike existing state-of-the-art methods that typically employ denoising diffusion models with separate components for mask and image generation, we propose a latent diffusion model trained to learn the joint distribution of cellular layouts—encoded as a distance map—classification masks, and histology images. Our method allows conditioning on user-defined parameters, including cell types, cell quantities, and tissue types, enabling tailored data generation. The model synthesizes labeled samples in a single step and is designed to enhance datasets for cell instance segmentation and classification. We evaluated this approach by training a unified model on two diverse datasets: the Conic H\&E histopathology dataset of the colon and the Nissl-stained brain cytoarchitecture CytoDArk0 dataset. The model synthesizes labeled samples in one step and can be used to enhance datasets for cell instance segmentation and classification. We trained one single model on two datasets: the Conic H\&E histopathology dataset of the colon and the recently released Nissl-stained brain cytoarchitecture CytoDArk0 dataset. Experimental results demonstrate that our method generates realistic and diverse samples, significantly improving cell instance segmentation and classification performance across both datasets. Notably, it achieves substantial improvements in neutrophil segmentation within the Conic dataset, where neutrophils constitute less than 1\% of the cells in the original dataset. These results underscore the potential of our method to tackle data scarcity challenges in histology analysis effectively.

% and advancing the development of more accurate and resilient computer-aided medical systems.


\keywords{Cell Segmentation \and Cell Classification \and Histology \and Data Augmentation \and Conditional Generative Models \and Latent Diffusion}
\end{abstract}
%
%
%
\section{Introduction}
Deep learning (DL) can be applied directly to histological sections for diagnostic or prognostic purposes, but this approach often suffers from limited interpretability. Alternatively, using DL to detect, segment, and classify cells or nuclei allows for the extraction of interpretable biomarkers that characterize the tissue microenvironment through cell-related features such as quantity, size, morphology, and spatial arrangement. These biomarkers, in turn, enable downstream applications including disease detection, outcome prediction, and the design of personalized treatments \cite{song2023artificial,nunes2024survey}. In neuroscience, the segmentation and classification of neurons and glial cells in whole-slide images of brain tissue facilitate the precise quantification of \textit{brain cytoarchitecture}\footnote{The cellular organization of brain cells, including their types, densities, and spatial arrangements.}. Cytoarchitectural analysis are essential for advancing our understanding of the pathogenesis of neurodegenerative and neuroinflammatory disorders, while also providing insights into brain development and evolution \cite{AMUNTS20071061,falcone2021neuronal,graic2023cytoarchitectureal,meystre2024cell,graic2024age}. DL methods for cell instance segmentation (CS) and cell classification (CC) \footnote{CS: identifying and outlining individual cells; CC: recognizing cell types} rely heavily on supervised learning \cite{song2023artificial}, which requires large labeled datasets \cite{schmidt2018cell,graham2019hover,stringer2021cellpose,vadori2024cisca}. Yet, labeling cell boundaries and cell types is labor-intensive and demands expert input, limiting the availability of annotated data. Generative models address this challenge by generating synthetic images with segmentation masks, yielding additional data \cite{chen2024comprehensive}.%,falcone2021neuronal,graic2023cytoarchitectureal,meystre2024cell,graic2024age}.
%Although deep learning (DL) can be employed directly on histological sections for diagnostic or prognostic purposes, this approach often lacks interpretability. In contrast, using DL to detect, segment, and classify cells or nuclei enables the extraction of interpretable biomarkers that describe the tissue microenvironment. This, in turn, supports downstream applications that depend on analyzing cell-related characteristics, including quantity, size, morphology, and spatial arrangement such as disease detection, outcome prediction, and the design of personalized treatments in a more transparent manner. In neuroscience, CS of neurons and glial cells in brain whole slice images (WSIs) supports the precise quantification of \textit{brain cytoarchitecture}\footnote{The cellular organization of brain cells, including their types, densities, and spatial arrangements.}, which is crucial for advancing our understanding of the pathogenesis of neurodegenerative and neuroinflammatory disorders, as well as for gleaning insights into brain development and evolution.
%and the mechanisms underlying brain complexity and information-processing capacity. %. Additionally, large-scale comparative studies of cytoarchitecture across different species, age groups, and sexes offer valuable insights into developmental trajectories, evolutionary adaptations, and the mechanisms driving brain complexity and information-processing capacity.
%Cell instance segmentation (CS) and cell classification (CC) \footnote{CS: identifying and outlining individual cells; CC: recognizing cell types} in microscopy tissue images are both challenging and crucial for medical and neuroscientific research that depends on analyzing cell-related characteristics, including quantity, size, morphology, and spatial arrangement. 

%learning the distribution of image-mask pairs and generating synthetic images with segmentation masks, providing diverse, high-quality training data.

%DL methods for CS and CC are mainly supervised, requiring large labeled datasets. However, the labor-intensive and expertise-driven process of labeling nuclei boundaries makes it difficult to acquire sufficiently large annotated datasets. % with dense annotations. 
%Generative models
%%, such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and diffusion models, 
%offer a promising solution by learning the distribution of image-mask pairs. Once trained, they generate synthetic images and segmentation masks, providing additional high-quality, diverse training data. 
%%This approach stands to reduce reliance on manual labeling and improve the robustness and generalizability of DL models. % for CS and CC.

\cite{tasnadi2023structure} employ two generative adversarial networks (GANs): the first generates heat-flow encodings, which are post-processed into labeled masks, where each cell is assigned a unique integer, and the second performs image-to-image translation to create corresponding microscopy images. Although GANs can produce high-quality images, their adversarial training process often results in unstable training dynamics and restricted diversity in the generated outputs \cite{dhariwal2021diffusion}. Diffusion models, especially the denoising diffusion probabilistic model (DDPM) \cite{ho2020denoising}, have recently gained prominence as a superior alternative. 
%However, their application to this specific task remains relatively unexplored.
%to GANs. 
DDPMs are likelihood-based models that progressively add noise to an image in the \textit{diffusion} process and learn to reverse this process. At inference, a random noise input is gradually  mapped onto a realistic output \cite{luo2022understanding}. \cite{yu2023diffusion} use two DDPMs: the first generates nuclei layouts as binary masks and distance maps and the second synthesizes histopathology images conditioned on them. The synthetic samples are added to the real dataset for training a CS model. It is shown that carefully selecting 10\% of the labeled real dataset and augmenting it with synthetic samples can achieve CS performance comparable to using the  real dataset. Augmenting the full real dataset results in improvements in Dice scores ranging from 0.6\% to 1.3\%. \cite{min2025co} apply a DDPM to generate histopathology images, distance maps, and semantic masks for CS and CC. The DDPM is conditioned on nuclei centroid layouts and structure-related textual prompts, such as “[tissue type] with [list of nuclei types]”. The layouts appear to be sourced from the training set. In the satellite imaging domain, \cite{toker2024satsynth} train an unconditional DDPM to generate satellite images along with their corresponding semantic masks for semantic segmentation tasks. When evaluated on an Earth observation benchmark dataset augmented with a synthetic dataset of the same size as the real dataset, the mean IoU across five different models shows an average improvement of $2.67\%$.
%The study highlights that generating image-mask pairs at a resolution of $128 \times 128$ and  upscaling them to $256 \times 256$ using a second DDPM conditioned on the lower-resolution outputs leads to improved performance. When applied to an Earth observation benchmark dataset and training semantic segmentation models on the combined dataset $\mathcal{D} \cup \mathcal{D}'$, where $\mathcal{D}'$ is the generated dataset with the same size of the real one, the mean IoU across five different models improves by an average of $2.67\%$. %, with individual gains ranging from $0.67\%$ to $7.59\%$, depending on the model.
%of the origianl earth observation benchmark
\begin{figure}
\centering
\includegraphics[width=0.90\textwidth]{Figures/generated_samples_legend.png}
\vspace{-5pt}
\caption{Ten HistoSmith-generated samples of the colon (rows 1-2), hippocampus (row 3), cerebellum (row 4), and auditory cortex (row 5), each with a generated image, distance map, cell type semantic mask, and post-processed label overlay.} \label{generatedsamples}
\end{figure}
DDPMs operate directly on raw image pixels.
%, which can be computationally intensive. 
Latent diffusion models (LDMs) \cite{rombach2022high}
%address this limitation by 
encode images into a compressed latent representation using a variational autoencoder or similar architecture and perform diffusion operations in this lower-dimensional latent space, reducing computational costs while minimizing noise and redundancy inherent in pixel-level representations.
%. By working in the latent space, LDMs capture the essential features of the data while minimizing noise and redundancy inherent in pixel-level representations. 
The final denoised output is decoded back into the original image space. 

In this study, we present \textit{HistoSmith}, a LDM designed to generate image-label pairs for augmenting CS and CC datasets. Our key contributions include:
(1) A novel LDM for histology data augmentation, capable of generating histology images and their corresponding CS and CC labels simultaneously.
(2) A parameter-driven generative process, conditioned on user-defined parameters such as cell types and quantity, without requiring cell layouts as additional input
%(3) we evaluate HistoSmith's generative capability, particularly under conditions that deviate from those encountered during training, and 
%(e.g., generating images specified to contain only neutrophils).
(3) Evidence demonstrating its effectiveness, showing that augmenting CoNIC and CytoDArk0 datasets with HistoSmith leads to average metric gains of 1.9\% and 3.4\%, respectively, in CS and CC.
(4) A comprehensive analysis of the quality of generated samples.
%(3) A unified, one-step generative approach trained jointly across datasets with different staining methods, as opposed to a two-step approach, where segmentation labels are generated first and then used to condition image generation in a subsequent step.
To the best of our knowledge, HistoSmith is the first LDM-based approach for histology dataset augmentation in CS and CC. Designed to address data scarcity, it can also be used to tackle class imbalance by steering the generative process toward underrepresented cell types, such as inflammatory cells, which are difficult to identify in datasets like CoNIC \cite{graham2024conic,vadori2024cisca}.
%Furthermore, unlike prior work, we adopt a unified model trained jointly on datasets with diverse staining methods and cell types.




%To the best of our knowledge, HistoSmith represents the first application of LDMs for augmenting histology datasets for CS and CC and the first attempt to condition the generative process solely on user-defined parameters, without requiring cell layouts as additional input. 
%Conceived to counteract the data scarcity issue, by conditioning on multiple factors, such as cell types and cell quantity, HistoSmith is an attempt at addressing the class imbalance challenge, too. For instance, it can steer the generation process to create samples with a high prevalence of inflammatory cells, which are often underrepresented in open datasets like Conic and  thus are more challenging to identify \cite{graham2024conic,vadori2024cisca}. Finally, unlike prior work, we adopt a unified model trained jointly on datasets featuring distinct staining methods and cell types.

%, such as tissue and cell types, 
%offering fine-grained control 

%, producing high-quality, realistic results.

%Latent diffusion models (LDMs) are a variant of diffusion models that operate in a latent space rather than directly on high-dimensional image space. This makes them computationally more efficient while maintaining the ability to generate high-quality, realistic data.

%In this study, we develop a latent diffusion model  to generate image mask-pairs for augmenting cell instance segmentation and classificaton datasets. The model can be conditioned on multiple factors relevant to the task, including staining type, cell types, the relative abundance of cell types, and tissue type.
%
%Our contributions are as follows: (1) a conditional latent diffusion-based data augmentation framework that can generate histopathology images and their corresponding segmentation labels from scratch; (2) experiments showing that by augmenting just XX\% of the labeled training data, our method achieves segmentation and classification results comparable to a fully-supervised baseline; and (3) an analysis of the framework's generalization capability when conditions are set to values that differ significantly from those encountered during training (e.g., generating an image with conditions specified to contain only neutrophils) (4) a comparison of the performance of proposed one-step approach against a two step approach, where segmentation labels are generated first, and used to condition the generation of the images in a second step.
%
%To the best of our knowledge, this is the first application of LDMs for augmenting histology training datasets aimed at the segmentation and \textit{simultaneous classification} of individual cells. Additionally, it is the first attempt to condition the image generation process on user-defined parameters such as tissue type and cell types hat allow the use to finely control the genraeration process.



%2 GAN

%check 21, 3 for diversity 
%5 diffusion
%
%Our proposed method involves two key steps: conditional synthesis of cell structures and conditional synthesis of histopathology images. We develop a latent diffusion model conditioned on user-defined parameters such as tissue type and cell types and a second (non latent) diffusion conditioned on cell layouts.
%During the training phase, the first model is trained on cell layouts derived from instance maps, while the second model is trained using paired images and cell layouts. In the testing phase, cell layouts and the corresponding images are generated sequentially by these two models. 

%GANs excel at producing visually realistic images but can be challenging to train for paired data generation. VAEs, on the other hand, offer a more stable training process and are effective at learning latent representations of image-mask pairs. Diffusion models, a newer class, are particularly well-suited for generating high-quality and consistent paired data by gradually denoising a random input to create realistic outputs.
%LDMs are a variant of diffusion models that operate in a latent space rather than directly on high-dimensional image space. This makes them computationally more efficient while maintaining the ability to generate high-quality, realistic data.
%
%Standard diffusion models directly process raw image pixels, which can be computationally expensive for high-resolution histopathology images. LDMs address this by first encoding the images into a compressed latent representation (using a pre-trained encoder, such as a Variational Autoencoder or similar architecture). Diffusion operations are then performed in this reduced latent space, significantly reducing computational costs. By operating in the latent space, LDMs focus on the essential structure and features of the data, avoiding the noise and redundancy present in pixel-level representations. After diffusion in the latent space, the output is decoded back into the original image space, resulting in high-quality, realistic outputs.
%
%LDMs can learn the distribution of histopathology images and segmentation masks from a smaller labeled dataset, enabling the generation of diverse synthetic data that reflects a wide range of tissue morphologies, staining variations, and cellular arrangements. This diversity improves the generalization of deep learning models trained on the augmented data. LDMs support conditional generation, allowing researchers to generate specific types of data, such as varying levels of cell density, specific tissue structures, or specific cell types. This level of control is invaluable for training models on rare or underrepresented cases in the dataset.
%
%By leveraging these generative models for data augmentation, researchers can overcome the limitations of small datasets, reduce reliance on manual annotations, and improve the performance and reliability of deep learning models in medical and scientific applications.
%
%image-mask pairs from a smaller labeled dataset. Once trained, they can generate synthetic images and corresponding segmentation masks that mimic the complexity and variability of real histopathology data.
%
%This raises a natural question: can we achieve or even surpass the segmentation performance of fully-supervised models by expanding the training dataset using only a small proportion of labeled images? Intuitively, since the labeled images represent samples from the broader population of histopathology images, if we can learn the underlying distribution of these images, it should be possible to generate an unlimited number of images and their corresponding pixel-level labels to augment the original dataset. Thus, there is a need to develop a tool that can learn distributions and generate new paired samples for segmentation.

%Generative adversarial networks (GANs) [2, 4, 12, 16, 20] have been extensively used for data augmentation [11, 22, 27, 31]. Recently, a GAN-based method has been introduced for synthesizing labeled histopathology images specifically for nuclei segmentation [21]. While GANs are capable of generating high-quality images, they often suffer from unstable training and a lack of diversity in generated images due to their adversarial training approach. In contrast, diffusion models, particularly the denoising diffusion probabilistic model (DDPM) [8], have recently gained prominence over GANs. Owing to their solid theoretical foundation and remarkable performance, diffusion models have been quickly adopted for a wide range of computer vision tasks, including inpainting, superresolution [30], text-to-image translation, anomaly detection, and segmentation [1, 9, 24, 26]. As likelihood-based models, diffusion models do not require adversarial training and outperform GANs in terms of the diversity of generated images [3], making them naturally better suited for data augmentation. In this paper, we introduce a novel diffusion-based augmentation framework for nuclei segmentation. 

%Semantic labels are employed in Satellite imagery is a powerful tool to monitor the earth’s surface. In this field, conventional image augmentation techniques designed for object-centric data – such as flipping, rotating, and rescaling – are often insufficient to emulate the large sample diversity of satellite imagery, simlarly to hisotlogical imagery. This vast diversity implies that most existing datasets are sparse, covering only a fraction of potential earth observation scenes. 

%enhancing semantic segmentation of satellite data by harnessing recent advances
%in generative diffusion models. Such models approximate the distribution of an existing dataset, to produce novel samples. We demonstrate that this can be leveraged to train a model that imitates the joint distribution of images and semantic segmentation labels in a given satellite dataset. Sampling from this distribution effectively enables us to generate additional training data as a form of data augmentation, see Fig. 1 for several such sample pairs. The enhanced training set can then be utilized for downstream semantic segmentation.



%In recent times, diffusion
%models have emerged as a central technique for image generation. Their main advantage is the ability to synthesize
%high-quality samples, on par with GANs, while being less
%prone to suffer from mode collapse.

\section{Method}
\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{Figures/HistoSmith.png}
\vspace{-2pt}
\caption{The proposed HistoSmith framework for generative data augmentation.} \label{HistoSmith}
\end{figure}

Our goal is to augment a labeled image dataset with generated samples to improve CS and CC performance. We propose an LDM \cite{rombach2022high} that concurrently generates a histological image and its masks. As shown in Fig. \ref{generatedsamples}, these include a cellular layout encoded as a distance map and a semantic mask for cell types, post-processed into a label map and unique cell-type assignments via thresholding, morphological operations, and majority voting.
%As shown in Fig. \ref{generatedsamples}, these include a cellular layout encoded as a distance map and a semantic mask, post-processed into a label map and unique cell assignments via thresholding, morphology, and majority voting.
%Our objective is to enhance a dataset with a limited number of labeled images by generating additional samples to improve CS and CC performance. To boost the diversity of labeled images, it is advantageous to synthesize both the images and their corresponding instance maps. We propose a two-step strategy for generating new labeled images, with both steps relying on diffusion models. An overview of the proposed framework is presented in Fig. 2. In this section, we provide a detailed explanation of the two steps.

%As per \cite{rombach2022high} and Fig. \ref{HistoSmith}, a VQ-VAE learns a discrete latent representation \({z}\) from images \({x}\) and masks \({m}\), followed by diffusion model training in this space.
As per \cite{rombach2022high} and Fig. \ref{HistoSmith},  a VQ-VAE is trained on a dataset \(\mathcal{D} = \{({x}_i, {m}_i)\}_{i=1}^{N}\) to learn a discrete latent representation \({z}\) of the input images \({x}\) and corresponding masks \({m}\). A diffusion model is then trained in the learned latent space. 

The VQ-VAE encoder, parameterized by a convolutional neural network (CNN), maps an input image concatenated with its masks along the channel dimension \({x|m}\) to a continuous latent representation, reducing the resolution by a factor of 4. The latent vectors forming this representation are mapped to the nearest prototype in a codebook, yielding a quantized representation \({z}\). A CNN-based decoder reconstructs the original image and masks. Our decoder has two heads: one predicts the input image and distance map, the other the semantic mask of cell types. The training objective for the first head includes a reconstruction and perceptual loss for fidelity, a commitment loss for stable encoder representations, a codebook loss for optimizing discrete representation learning, and an adversarial loss to mitigate blurriness, consistent with previous work. For the second head, inspired by semantic segmentation tasks, we apply a categorical cross-entropy loss and a Tversky loss \cite{salehi2017tversky}.
%Mathematically, the loss function can be expressed as:
%\[
%L_{VQ-VAE} = \|x - \hat{x}\|_2^2 + \beta \|\text{sg}[z_e(x)] - z_q(x)\|_2^2 + \alpha \|z_e(x) - \text{sg}[z_q(x)]\|_2^2,
%\]
%where $\text{sg}[\cdot]$ denotes the stop-gradient operator, and $\alpha, \beta$ are hyperparameters.

In the diffusion model, the forward process gradually adds Gaussian noise to the latent variables \({z}\) over $T$ timesteps with a linear schedule. In the reverse process,  a time-conditional U-Net learns to denoise the latent representations by predicting the added noise at each step. %using
%\[
%\(\mathbf{z}\)_t = \sqrt{\bar{\alpha}_t} \(\mathbf{z}\) + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I),
%\]
%where $\bar{\alpha}_t$ is a predefined noise schedule.
%\[
%\epsilon_\theta(\mathbf{z}_t, t) \approx \epsilon.
%\]
%The U-Net is trained using a variational lower bound objective:
%$L_{LDM} := \mathbb{E}_{{z}, \epsilon \sim \mathcal{N}(0,1), t} \left[ \| \epsilon - \epsilon_{\theta}(z_t, t) \|_2^2 \right]$ as the loss function, with $t$ uniformly sampled from ${1, . . . , T}$. 
For more details we refer to \cite{van2017neural,rombach2022high}.

We condition the reverse diffusion process with an 10-dimensional vector encoding staining type, tissue type, and cellular composition. The first element specifies the staining method (0 for HE, 1 for Nissl). The next five represent normalized cell counts for neutrophils, epithelial cells, eosinophils, other cells, and neuron/glia. The final four use one-hot encoding for colon, auditory cortex, cerebellum, and hippocampus. Cell counts are normalized by the maximum value observed in a training patch to enable global quantity learning. The conditioning vector is mapped to an embedding vector, initialized from a normal distribution, which is then combined with the time embedding to condition the U-Net on the timestep. At inference, a random Gaussian noise sample and the conditioning vector are fed into the reverse diffusion process. After denoising, the VQ-VAE decoder reconstructs a new image and its corresponding masks. 


%𝑇
%T timesteps, refining the latent representation. Once denoised, the VQ-VAE decoder reconstructs the final image and masks.

\section{Experiments}
\label{sec:experiments}
\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{Figures/cell_counts_image.png}
\vspace{-10pt}
\caption{Cell distribution per image across tissue/cell types in datasets $\mathcal{D}$ and $\hat{\mathcal{D}}$.} \label{cell_counts}
\end{figure}


\begin{table}[]
\caption{CS and CC results on CoNIC and CytoDArk.}
\label{results:overall}
\vspace{-7pt}
\centering
\setlength{\tabcolsep}{1.7pt}
\renewcommand{\arraystretch}{0.9} 
\begin{tabular}{@{}lcccccccc|ccccccc@{}}
\toprule
 & \multicolumn{8}{c|}{\textbf{CoNIC}} & \multicolumn{7}{c}{\textbf{CytoDArk0}} \\ \midrule
\textbf{} & \textbf{Dice} & \textbf{P} & \textbf{R} & \textbf{DQ} & \textbf{SQ} & \textbf{PQ} & \textbf{R2} & \textbf{mPQ$^+$} & \textbf{Dice} & \textbf{P} & \textbf{R} & \textbf{DQ} & \textbf{SQ} & \textbf{PQ} & \textbf{R$^2$} \\
$\mathcal{D}$ & 82.1 & 80.8 & 80.4 & 80.3 & 79.9 & 64.4 & 74.4 & 47.2 & 86.5 & 77.3 & 78.4 & 77.1 & 86.3 & 66.7 & 99.2 \\
$\mathcal{D} \cup \hat{\mathcal{D}}$ & 82.7 & 81.6 & 80.2 & 80.7 & 80.6 & 65.3 & 83.7 & 49.9 & 89.2 & 81.7 & 84.1 & 82.2 & 87.3 & 71.8 & 99.3 \\ \bottomrule
\end{tabular}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}lcccccccccccc@{}}
\textbf{CoNIC} & \multicolumn{3}{c}{\textbf{Neutrophils}} & \multicolumn{3}{c}{\textbf{Epithelial}} & \multicolumn{3}{c}{\textbf{Eosinophils}} & \multicolumn{3}{c}{\textbf{Other}} \\ 
\addlinespace[-3pt]
\midrule
 & \textbf{DQ} & \textbf{SQ} & \textbf{PQ} & \textbf{DQ} & \textbf{SQ} & \textbf{PQ} & \textbf{DQ} & \textbf{SQ} & \textbf{PQ} & \textbf{DQ} & \textbf{SQ} & \textbf{PQ} \\
$\mathcal{D}$ & 29.2 & 75.5 & 22.0 & 77.6 & 77.8 & 60.4 & 50.0 & 77.0 & 38.5 & 81.3 & 83.4 & 67.8 \\
$\mathcal{D} \cup \hat{\mathcal{D}}$ & 38.8 & 75.0 & 29.1 & 77.9 & 78.2 & 60.9 & 52.2 & 77.3 & 40.3 & 82.0 & 84.5 & 69.2 \\ \bottomrule
\textbf{CytoDArk0} & \multicolumn{3}{c}{\textbf{Cerebellum}} & \multicolumn{3}{c}{\textbf{Hippocampus}} & \multicolumn{3}{c}{\textbf{Aud. Cortex}} & \multicolumn{3}{c}{\textbf{Vis. Cortex}} \\ 
\addlinespace[-3pt]
\midrule
$\mathcal{D}$ & 86.4 & 84.7 & 73.2 & 81.5 & 89.3 & 72.8 & 80.8 & 84.4 & 68.3 & 79.0 & 88.1 & 69.6 \\
$\mathcal{D} \cup \hat{\mathcal{D}}$ & 88.5 & 85.7 & 75.8 & 85.8 & 89.5 & 76.8 & 81.9 & 84.6 & 69.4 & 81.3 & 90.0 & 73.2 \\ \bottomrule
\end{tabular}
\end{table}
We assessed HistoSmith’s output quality by training it on a merged dataset comprising the \textit{CoNIC} \cite{graham2024conic} and \textit{CytoDArk0} \cite{vadori2024cytodark0} datasets. CoNIC includes 4,981 H\&E-stained colon tissue histology images with six cell types: neutrophils, epithelial cells, lymphocytes, plasma cells, eosinophils, and connective tissue cells. To simplify differentiation, annotations were modified by grouping lymphocytes, plasma, and other cell types into a single \textit{other} category. CytoDArk0 is the first open dataset of Nissl-stained mammalian brain histology with annotated neurons and glial cells. This study uses the CytoDArk0\_20x\_256 subset, with 1,104 patches. Both datasets contain 256×256 patches at 20× magnification.
We trained HistoSmith using the Adam optimizer for 150 epochs with learning rates of \(1 \times 10^{-5}\) for the VQ-VAE and \(5 \times 10^{-6}\) for the U-Net. We implemented the data splitting, oversampling, and staining augmentation techniques described in \cite{vadori2024cisca}. The source code will be available at \url{https://github.com/vadori/cytoark}.
\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{Figures/Picture18.jpg}
\vspace{-7pt}
\caption{Correlation between conditioning ($n_i$) and generated cell quantities ($n_o$).} \label{quantityAnalysis}
\end{figure}

After training, a new dataset \(\hat{\mathcal{D}} = \{(\hat{\mathbf{x}}_i, \hat{\mathbf{m}}_i)\}_{i=1}^{M}\) was generated to augment \(\mathcal{D}\) for training the CISCA model \cite{vadori2024cisca} for CS and CC. 
%CISCA has demonstrated superior robustness and accuracy across various staining techniques, magnifications, and tissue types. 
We added 350 brain tissue samples and 3,500 colon samples, approximately doubling the training set size.
For each tissue, cell quantities for conditioning vectors were sampled from Gaussian distributions, with standard deviations matching the training set and means scaled by a multiplicative factor \(\alpha \in [1,2]\) for all cell types and \(\alpha = 20\) for neutrophils and eosinophils. This resulted in an increase in the average cell count per patch in \(\hat{\mathcal{D}}\), as shown in Fig. \ref{cell_counts}, where distributions in \(\hat{\mathcal{D}}\) tend to shift upwards or exhibit longer tails. This provides more training cell instances and enhances the representation of neutrophils and eosinophils, minority classes in the CoNIC dataset. After training, CISCA was evaluated on the test sets of CoNIC and CytoDArk0.

Additionally, 100 extra patches were generated per brain tissue type and 400 for the colon, storing the conditioning vector values to analyze the relationship between the input cell quantities (\(n_i\)) and those generated by HistoSmith (\(n_o\)).  

Finally, evaluation metrics for generative models and t-SNE embeddings were computed to assess image quality. %However, the cell quantities used to guide the generative process do not directly correspond to those in the training set.

\section{Results}
Table \ref{results:overall} presents the overall CS and CC results on CoNIC and CytoDArk. We evaluate performance using the Dice coefficient, Precision (P), Recall (R), Detection Quality (DQ), Segmentation Quality (SQ), Panoptic Quality (PQ), Coefficient of Determination (R$^2$), and Multi-class Panoptic Quality (mPQ$^+$) \cite{graham2024conic,vadori2025mind,vadori2024cisca}. The CISCA model trained on the augmented dataset $\mathcal{D} \cup \hat{\mathcal{D}}$ outperforms the one trained on the original dataset $\mathcal{D}$ across both datasets and most metrics. Specifically, it achieves an average improvement of 1.9\% across metrics on CoNIC and 3.4\% on CytoDArk0. Marked average improvements across metrics were observed for neutrophils (+5.4\%) and eosinophils (+1.4\%), as well as the hippocampus (+2.8\%) and visual cortex (+2.6\%). Notably, the visual cortex is missing from the CytoDArk0 training set, and hippocampus cells in the test set differ in appearance from those in the training set. This suggests that HistoSmith effectively enhances generalization.

%\begin{table}[]
%\caption{CS and CC results on cell types (CoNIC) and tissue types (CytoDArk0).}
%\label{results:detail}
%\centering
%\setlength{\tabcolsep}{3pt}  % Adjust column separation
%\begin{tabular}{@{}lcccccccccccc@{}}
%CoNIC & \multicolumn{3}{c}{\textbf{Neutrophils}} & \multicolumn{3}{c}{\textbf{Epithelial}} & \multicolumn{3}{c}{\textbf{Eosinophils}} & \multicolumn{3}{c}{\textbf{Other}} \\ \midrule
% & \textbf{DQ} & \textbf{SQ} & \textbf{PQ} & \textbf{DQ} & \textbf{SQ} & \textbf{PQ} & \textbf{DQ} & \textbf{SQ} & \textbf{PQ} & \textbf{DQ} & \textbf{SQ} & \textbf{PQ} \\
%$\mathcal{D}$ & 29.2 & 75.5 & 22.0 & 77.6 & 77.8 & 60.4 & 50.0 & 77.0 & 38.5 & 81.3 & 83.4 & 67.8 \\
%$\mathcal{D} \cup \hat{\mathcal{D}}$ & 38.8 & 75.0 & 29.1 & 77.9 & 78.2 & 60.9 & 52.2 & 77.3 & 40.3 & 82.0 & 84.5 & 69.2 \\ \midrule
%CytoDArk0 & \multicolumn{3}{c}{\textbf{Cerebellum}} & \multicolumn{3}{c}{\textbf{Hippocampus}} & \multicolumn{3}{c}{\textbf{Auditory Cortex}} & \multicolumn{3}{c}{\textbf{Visual Cortex}} \\ \midrule
%$\mathcal{D}$ & 86.4 & 84.7 & 73.2 & 81.5 & 89.3 & 72.8 & 80.8 & 84.4 & 68.3 & 79.0 & 88.1 & 69.6 \\
%$\mathcal{D} \cup \hat{\mathcal{D}}$ & 88.5 & 85.7 & 75.8 & 85.8 & 89.5 & 76.8 & 81.9 & 84.6 & 69.4 & 81.3 & 90.0 & 73.2 \\ \bottomrule
%\end{tabular}
%\end{table}

%\begin{table}[]
%\caption{Assessment of the quality of generated vs. test images.}
%\vspace{-7pt}
%%\centering
%\setlength{\tabcolsep}{2pt}
%\begin{tabular}{lrrr|rrr}
%\hline
%\multicolumn{1}{c}{} & \multicolumn{3}{c|}{Test Images} & \multicolumn{3}{c}{Generated Images} \\ \hline
%\multicolumn{1}{c}{} & \multicolumn{1}{c}{FD} & \multicolumn{1}{c}{D} & \multicolumn{1}{c|}{C} & \multicolumn{1}{c}{FD} & \multicolumn{1}{c}{D} & \multicolumn{1}{c}{C} \\
%\textbf{Colon} & 39.27 & 0.86 & 0.98 & 195.94 & 0.23 & 0.35 \\
%\textbf{Brain} & 104.05 & 0.66 & 0.98 & 316.65 & 0.41 & 0.27 \\
%- Cerebellum & 243.18 & 1.10 & 1.00 & 216.98 & 0.92 & 1.00 \\
%- Hippocampus & 243.00 & 1.52 & 1.00 & 285.19 & 0.50 & 0.90 \\
%- Aud. cortex & 83.93 & 0.99 & 1.00 & 285.42 & 0.24 & 0.33 \\ \hline
%\end{tabular}
%\end{table}

\begin{figure}[t]
\centering
\begin{minipage}{0.59\textwidth} % Adjust width as needed
        \centering
	\setlength{\tabcolsep}{2pt}
	\begin{tabular}{lrrr|rrr}
	\hline
	\multicolumn{1}{c}{} & \multicolumn{3}{c|}{Test Images} & \multicolumn{3}{c}{Gen. Images} \\ \hline
	\multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{FD}} & \multicolumn{1}{c}{\textbf{D}} & \multicolumn{1}{c|}{\textbf{C}} & \multicolumn{1}{c}{\textbf{FD}} & \multicolumn{1}{c}{\textbf{D}} & \multicolumn{1}{c}{\textbf{C}} \\
	\textbf{Colon} & 39.27 & 0.86 & 0.98 & 195.94 & 0.23 & 0.35 \\
	\textbf{Brain} & 104.05 & 0.66 & 0.98 & 316.65 & 0.41 & 0.27 \\
	- Cerebellum & 243.18 & 1.10 & 1.00 & 216.98 & 0.92 & 1.00 \\
	- Hippocampus & 243.00 & 1.52 & 1.00 & 285.19 & 0.50 & 0.90 \\
	- Aud. cortex & 83.93 & 0.99 & 1.00 & 285.42 & 0.24 & 0.33 \\ \hline
	\end{tabular}
        \captionof{table}{Assessment of the quality of generated vs. test images.}
        \label{tab:qassessment}
    \end{minipage}  
\hfill 
    \begin{minipage}{0.37\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{Figures/tsneplots.png} % Replace with your figure file
	\vspace{+2pt}
        \caption{t-SNE patch embeddings.}
        \label{fig:tSNEplots}
    \end{minipage}
   
\end{figure}

Fig. \ref{quantityAnalysis} presents both qualitative and quantitative analyses of the relationship between conditioning and generated cell quantities. Panels (a–c) illustrate how varying input conditioning values affects synthetic cell type semantic masks and histology images. The difference between generated (\(n_o\)) and conditioned (\(n_i\)) normalized cell counts is reported below each pair. As expected, the model effectively generates more cells as the input cell quantity increases. This trend is evident for the cerebellum (panel a) and neutrophils (panel b). However, for neutrophils, a saturation effect occurs when the cell quantity is increased excessively. In this case, we conditioned the generation process to produce H\&E-stained colon tissue with randomly sampled cell quantities for each type, as outlined in Section \ref{sec:experiments}, setting neurons/glia to zero. For neutrophils, instead of sampling, we systematically increased the cell quantity from a minimum to a maximum value.
Within the range \( \frac{1}{5} n_{\text{max}} \) to \( \frac{9}{5} n_{\text{max}} \), where \( n_{\text{max}} \) is the maximum value observed in the training set, HistoSmith generates neutrophils in proportion to the input (panel b). However, when the cell quantity is varied across the full possible range 0–1 (panel c), cells transition into the \textit{other} cell type and eventually into neurons/glia, and the image loses realism. It appears that the conditioning on tissue and cell types is almost entirely overridden to achieve the requested cell quantity, leading to the generation of non-neutrophil cell types.

Panels (d–f) provide quantitative evaluations using Bland-Altman plots, Pearson's correlation, linear regression model fits, and density distribution comparisons for 100 samples of the cerebellum, auditory cortex, and neutrophils, respectively. In the Bland-Altman plots, the y-axis represents \( n_i \) rather than the conventional average of the compared measures, as it reflects the \textquotedblleft true count\textquotedblright \ expected in the synthetic images. Overall, the bias is minimal; however, as observed qualitatively, the discrepancy between \( n_o \) and \( n_i \) increases with higher input values. Correlation is strong for the cerebellum and auditory cortex (\( r = 0.98 \)), with a linear fit closely matching the line of equality and the distribution of \( n_o \) largely aligned with that of \( n_i \), although the latter deviates from the training data distribution \( n_D \). For neutrophils (panel f, \( r = 0.39 \)), correlation is lower. While the linear trend is increasing, \( n_o \) is systematically lower than \( n_i \), as the model is consistently pushed to generate images containing neutrophils, despite their rarity in the Conic dataset (1\% of the total cells in the original training set and 2.7\% in the oversampled dataset used for training). In the distribution plots, \( n_o \) exhibits a more pronounced tail compared to \( n_i \), but it does not fully overlap. Instead, it appears stretched to the left, toward the distribution of \( n_{\mathcal{D}} \).


%Overall, the bias is minimal; however, as observed qualitatively, the discrepancy between \( n_o \) and \( n_i \) increases with higher input values. Correlation is strong for the cerebellum and auditory cortex (\( r = 0.98 \), with a linear fit closely matching the line of equality and the distribution of \( n_o \) fairly aligned with the distribution of \( n_i \), despite the difference with the distribution of the training data) but lower for neutrophils (panel f, \( r = 0.39 \)). In the latter case, while the linear trend is increasing, \( n_o \) is systematically lower than \( n_i \), as the model is consistently driven to generate images containing neutrophils, despite their rarity in the Conic dataset (1\% in the original training set and 2.7\% in the oversampled dataset used for training). In the distributions plot, we can see that the distribution of \( n_o \) has a a more pronounced tail with respect to the distribution of \( n_i \), but it can't quite overlap with it, as it appears stretched to the right, towards the distrbution of neutrophil count in the training data. These findings demonstrate that HistoSmith effectively produces realistic histology images with controllable cell quantities, though its accuracy is dependent on the conditioning range. Extreme values beyond the training data result in instability and unrealistic outputs.

%Panels (d–f) present quantitative assessments through Bland-Altman plots, Pearson's correlation, linear regression model fits, and density distribution comparisons for 100 samples of the cerebellum, auditory cortex, and neutrophils, respectively. Note that in the Bland-Altman plots, the y-axis does not represent the usual average of the compared measures but instead corresponds to \( n_i \), which reflects the \textquotedblleft true count \textquotedblright that should be present in the synthetic images. Overall, the bias is minimal. However, as observed qualitatively, the discrepancy between \(n_o\) and \(n_i\) increases with higher input values. Correlation is strong for the cerebellum and auditory cortex (\(r = 0.98\), linear fit almost identical to the line of equality) and lower for neutrophils (panel f, \(r = 0.39\)). In this latter case, the linear trend is increasing, but \(n_o\) tend to be sistematically lower than \(n_i\), as we are consistently forcing the model to produce images with neutrophils, whereas in the Conic dataset neutorphils constitute a rarety (1\% in the origianl training set, 2.7\% in the oversampled training setwe used for training). These results indicate that HistoSmith effectively generates realistic histology images with controllable cell quantities, though its accuracy depends on the conditioning range. Extreme values beyond the training data lead to instability and unrealistic outputs.

%Fig. \ref{quantityAnalysis} presents qualitative and quantitative analyses of the relationship between conditioning parameters and generated cell quantities using HistoSmith. Panels a-c illustrate the effect of varying input conditioning values (\(n_i\)) on synthetic cell type semantic masks and histology images. The difference between generated (\(n_o\)) and conditioned (\(n_i\)) normalized cell counts are reported below each pair. It can be notices that, as expected, the model is effectively generating more cells as the input cell quantity increases. This is observable for the cerebellum (panel a), and neutrophils (panel b). However, for neutrophils, a saturation effect seems to happen when the cell quanityt is increased too much. when varying it within the range 1/5nmax and 9/5 max where nmax is the maximum values observed in the training set, HistoSmith generates commensurately more neutrophils. when the cell quanity is varies n the ful possible range 0-1, cells become first other cells and then neurons/glia (panel-c), and the image losses realisticity. Panels d-f provide quantitative evaluations using Bland-Altman plots, correlation plots, and density distributions to assess the agreement between input and output cell densities for 100 samples of the cerebellum, audtory cortex and neutrophils respectively. In general, the overall bias is minimal. However, it can be observed that the difference between generated and conditioning cel quantities tend to icnerease as the cellquanityt increases, as observed qualtatelvely before. Correlation is strong (\(r = 0.98\)) for cerebellum and auditory cortex. Less so for neutrophils in panel (f) (\(r = 0.39\)). Overall, the results demonstrate that HistoSmith effectively generates realistic histology images with controllable cell quantities, though its accuracy varies depending on the conditioning range. Extreme quanitites well beyond training data lead to instability and unrealistic outputs.  

While Fréchet Inception Distance (FID) \cite{heusel2017gans} and Inception Score (IS) \cite{salimans2016improved} are commonly used to assess the quality of generated samples, \cite{stein2024exposing} highlights that diffusion models are unfairly penalized by the Inception network. Additionally, supervised networks do not provide a perceptual space that generalizes well for image evaluation. To address this limitation, the authors utilized a vision transformer (ViT) trained under the DINOv2 framework to extract lower-dimensional image representations for computing evaluation metrics. In Table \ref{tab:qassessment}, we report the Fréchet Distance (FD) for overall assessment, while Density (D) and Coverage (C) quantify sample fidelity and diversity, respectively \cite{naeem2020reliable}. Additionally, we visualize the t-SNE embeddings \cite{van2008visualizing} of the ViT representations for samples from the cerebellum and the colon. Generated images have a higher FD than the test set but align with state-of-the-art models \cite{stein2024exposing}. Interestingly, the cerebellum shows a lower FD in the generated images, with D and C values also well-aligned with the test data. This trend is qualitatively evident in the t-SNE plot (Fig. \ref{fig:tSNEplots}), where the generated images appear to bridge the gaps between the cerebellum training patches, which are limited to just 32. For the colon, the discrepancy in FD is the most pronounced, with D and C values significantly lower than those of the test data. However, in this study, we deliberately push the generation process beyond the high-density regions of the training and test data distributions to synthesize patches with a significantly higher presence of neutrophils and eosinophils. The limited overlap with the training patches is also evident in the t-SNE plot.

\section{Conclusions}
We introduced HistoSmith, the first LDM-based approach for histology dataset augmentation in CS and CC. Designed to address data scarcity, it also mitigates class imbalance by guiding generation toward underrepresented cell types, such as neutrophils in CoNIC. Training the CISCA model on an augmented dataset improved performance across both CoNIC and CytoDArk, with average metric gains of 1.9\% and 3.4\%, respectively. Notable improvements were seen for neutrophils (+5.4\%), eosinophils (+1.4\%), hippocampus (+2.8\%), and visual cortex (+2.6\%), indicating enhanced generalization. Additionally, HistoSmith effectively generates realistic histology images with controllable cell quantities, though accuracy depends on the conditioning range.  

These findings highlight the potential of generative data augmentation while raising key considerations. Synthetic data can improve segmentation and classification, but its effectiveness depends on the model’s generalization capacity—excessive deviation from the training distribution may introduce artifacts. Moreover, computational costs must be weighed against alternatives. While expert annotation is costly and time-intensive, motivating this study and others, it is worth exploring whether selectively annotating a well-curated subset of challenging cases offers a better trade-off between accuracy gains and cost. %Benchmarking against selective manual annotation remains essential for evaluating this trade-off.

%Benchmarking against selective manual annotation remains crucial for assessing the trade-off between computational expense and accuracy gains, particularly in resource-constrained settings.

%The concept of employing generative models for data augmentation is predicated on their ability to synthesize additional samples that traverse the manifold of the population distribution. This approach aims to harness the intrinsic information within this space to enhance the performance of segmentation and classification models. However, the extent of "additional" knowledge that can be effectively transferred to the segmentation model warrants careful consideration. If the segmentation model is designed with robust generalization capabilities, the necessity for supplementary samples may be diminished. Generative models typically operate within the known space of possibilities, potentially extending slightly beyond it. Yet, this exploration entails a trade-off: excessive deviation from the original data distribution may introduce artifacts or biases that negatively impact performance.
%
%Beyond the theoretical considerations of generalization and data augmentation, practical constraints must also be taken into account. The substantial computational demands associated with training and generating outputs using diffusion models or similar generative approaches raise the question of whether the performance gains achieved justify the resource expenditure. A meaningful point of comparison is a more traditional data augmentation strategy—manually annotating additional independent images and incorporating them into the dataset. A human-in-the-loop approach, wherein experts prioritize annotating images where CS and CC performance are suboptimal, could enhance dataset quality in a targeted manner. While expert annotation is often costly and time-intensive—motivating this study and others in the first place—it is worth considering whether a well-curated subset of challenging cases, selectively annotated, could offer a more efficient trade-off between accuracy gains and computational cost. 
%
%Empirical studies have suggested that while generative data augmentation can improve classification and segmentation performance, its efficacy depends on factors such as the size of the training set and the degree of divergence between generated and real data distributions. Thus, determining whether generative models provide a meaningful advantage over selective manual annotation requires careful benchmarking. In particular, evaluating whether a targeted increase in expert-labeled samples achieves comparable improvements with significantly lower computational overhead is essential for justifying the adoption of generative approaches in resource-constrained settings.

%Considering the significant computational demands required for both training and generating outputs with a diffusion model or similar, it is essential to evaluate whether the improvements achieved through such methods justify their resource consumption. A critical point of comparison should be a more traditional approach, wherein additional independent images are annotated and incorporated into the dataset, possibly prioritizing images where the CS and CC performance are suboptimal. This human-in-the-loop method would require expert input to enhance dataset quality, which may not always be available and is, in fact, the motivation behind this work. However, it is worth considering whether selectively adding a limited number of annotations on a well-curated subset of challenging cases could offer a more efficient trade-off between accuracy gains and computational cost, potentially achieving comparable improvements with significantly lower computational overhead.
%The concept of employing generative models for data augmentation is predicated on their ability to synthesize additional samples that traverse the manifold of the population distribution. This approach aims to harness the intrinsic information within this space to enhance the performance of segmentation and classification models. However, the extent of "additional" knowledge that can be transferred to the segmentation model warrants careful consideration. If the segmentation model is already designed with robust generalization capabilities, the necessity for supplementary samples may be diminished. Generative models typically operate within the known space of possibilities, potentially extending slightly beyond it. Nevertheless, this exploration entails a trade-off: excessive deviation from the original data distribution may adversely affect performance. Empirical studies have indicated that while generative data augmentation can improve classification performance, its efficacy is contingent upon factors such as the size of the training set and the degree of divergence between the generated and real data distributions. Therefore, it is crucial to balance the diversity and fidelity of synthetic data to avoid negative impacts on model performance. 

%Additionally, factors such as annotation effort, dataset diversity, and potential diminishing returns from model complexity should be carefully weighed when determining the most effective strategy for performance improvement.

%We showed that NCIS compares favorably with  designed for nuclei or whole cells segmentation in microscopy images. 


% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\newpage
 \bibliographystyle{splncs04}
 \bibliography{refs}
%
%\begin{thebibliography}{8}
%\bibitem{ref_article1}
%Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)
%
%\bibitem{ref_lncs1}
%Author, F., Author, S.: Title of a proceedings paper. In: Editor,
%F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
%Springer, Heidelberg (2016). \doi{10.10007/1234567890}
%
%\bibitem{ref_book1}
%Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
%Location (1999)
%
%\bibitem{ref_proc1}
%Author, A.-B.: Contribution title. In: 9th International Proceedings
%on Proceedings, pp. 1--2. Publisher, Location (2010)
%
%\bibitem{ref_url1}
%LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
%Oct 2017
%\end{thebibliography}
\end{document}
