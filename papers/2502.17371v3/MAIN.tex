\documentclass[3p,authoryear,review,11pt,]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{natbib}
\usepackage{adjustbox}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{rotating}
\usepackage{lscape}
\usepackage{booktabs}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{tabu}
\usepackage{tocloft}
\usepackage{blindtext}
\usepackage{mathtools}
\usepackage{blkarray}
\usepackage{subcaption}
\usepackage{float}
\usepackage{caption}
\usepackage{rotating}
\usepackage{lscape}
\usepackage{array}
\usepackage{booktabs}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{makecell}
\usepackage{tikz} \usetikzlibrary{positioning, fit, calc, bayesnet}
\tikzset{
  plate/.style={draw, shape=rectangle, rounded corners=0.5ex, thick,
    minimum width=3.1cm, text width=3.1cm, align=right, inner sep=10pt, inner ysep=10pt,label={[xshift=-14pt,yshift=14pt]south east:#1}}
}

% \modulolinenumbers[5]

%\journal{Computational Statistics \& Data Analysis}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
% \bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\begin{frontmatter}

\title{Sustainable Greenhouse Microclimate Modeling: A Comparative Analysis of Recurrent and Graph Neural Networks}
% \tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:

\author[1]{\corref{cor1} Emiliano Seri}
\ead{emiliano.seri@uniroma2.it}
\cortext[cor1]{Corresponding author}

\author[2]{Marcello Petitta}
\ead{marcello.petitta@uniroma3.it}

\author[1]{Cristina Cornaro}
\ead{cristina.cornaro@uniroma2.it}

\address[1]{Department of Enterprise Engineering, University of Rome Tor Vergata, Via del Politecnico 1, 00133, Rome, Italy}

\address[2]{Department of Mathematics and Physics, Roma Tre University, Via della Vasca Navale 84, 00146, Rome, Italy}


\begin{abstract}

The integration of photovoltaic (PV) systems into greenhouses not only optimizes land use but also enhances sustainable agricultural practices by enabling dual benefits of food production and renewable energy generation.
However, accurate prediction of internal environmental conditions is crucial to ensure optimal crop growth while maximizing energy production. 
This study introduces a novel application of Spatio-Temporal Graph Neural Networks (STGNNs) to greenhouse microclimate modeling, comparing their performance with traditional Recurrent Neural Networks (RNNs). 
While RNNs excel at temporal pattern recognition, they cannot explicitly model the directional relationships between environmental variables. Our STGNN approach addresses this limitation by representing these relationships as directed graphs, enabling the model to capture both environmental dependencies and their directionality. 
Using high-frequency data collected at 15-minute intervals from a greenhouse in Volos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winter conditions ($R^2 = 0.985$) but show limitations during summer cooling system operation. Though STGNNs currently show lower performance (winter $R^2 = 0.947$), their architecture offers greater potential for integrating additional variables such as PV generation and crop growth indicators.


\end{abstract}

\begin{keyword}
Recurrent neural networks \sep Spatio-temporal graph neural networks \sep Graph attention networks \sep Long short-term memory networks \sep Greenhouse modeling.
\end{keyword}

\end{frontmatter}


% \linenumbers

\section{Introduction}
\label{sec:intro}

Modern agriculture faces increasing pressure to optimize land use while reducing its environmental impact. The integration of photovoltaic (PV) systems into agricultural greenhouses has emerged as a promising solution to these challenges, offering dual benefits of food production and renewable energy generation. However, this integration introduces new complexities in greenhouse management, as PV panels directly influence light distribution, temperature patterns, and overall microclimate dynamics. These Agri-PV systems optimize land use by combining solar energy harvesting with protected crop cultivation, offering potential benefits for both energy and food security. Yet the successful implementation of PV-integrated greenhouses requires accurate prediction and control of the internal microclimate to ensure optimal growing conditions while maximizing energy production. While sophisticated physical models incorporating greenhouse thermodynamics, crop physiology, and PV system performance exist, their complexity often restricts real-time applications and rapid design optimization. This study, conducted within the framework of the European REGACE project, represents an initial step toward developing a comprehensive digital twin for PV-integrated greenhouses.
Rather than relying on complex physical models, we explore empirical approaches that employ advanced machine learning to predict greenhouse microclimates. The focus on neural network-based models aims to lay the foundation for a more complete digital twin that will ultimately integrate crop growth dynamics and PV energy production. By comparing traditional Recurrent Neural Networks (RNNs) with directed Spatio-Temporal Graph Neural Networks (STGNNs), we investigate whether the explicit modeling of environmental relationships and directional influences can improve prediction accuracy while maintaining computational efficiency.
Effective control and optimization of PV-integrated greenhouse environments present a fundamental challenge. Accurate climate control software integrating real-time data from environmental control computers (ECCs) can significantly enhance greenhouse management \citep{AASLYNG2005521}. Nonetheless, internal parameters such as temperature and humidity are influenced by various external conditions, including ambient temperature, solar radiation, and wind speed. Conventional modeling approaches can fail to capture the nonlinear and dynamic nature of these relationships, especially when greenhouse data are collected at high frequency \citep{AASLYNG2003657}. Researchers have therefore pursued increasingly sophisticated methods to balance these complexities, from physical simulations to data-driven models.
Greenhouses represent intricate thermodynamic systems where external conditions and the internal microclimate interplay to influence crop growth and energy use. The introduction of PV panels adds a layer of complexity, prompting further advancements in modeling techniques \citep{OUAZZANICHAHIDI2021116156, BAGLIVO2020115698, BRAEKKEN2023101830}. These approaches range from detailed physics-based studies focusing on ventilation and evapotranspiration \citep{STANCIU2016498, ABDELGHANY20061521, MOBTAKER201988, SINGH2018227, FITZRODRIGUEZ2010105} to data-driven or hybrid solutions capable of simulating both microclimate and crop growth processes \citep{MASSA2011711}. While such models provide essential insights into greenhouse behavior, they can be computationally intensive, and their parameterization often demands multidisciplinary expertise.
To address these constraints, researchers have increasingly explored empirical, data-driven approaches that learn greenhouse behavior directly from observations. Early neural network applications demonstrated promise \citep{SEGINER1994203, Bussab2007GreenhouseMU}, evolving into more sophisticated architectures such as Radial Basis Function Neural Networks \citep{FERREIRA200251} and optimized Multi-layer Perceptrons \citep{DEmilio2012NeuralNF}. A major leap forward came with Recurrent Neural Networks (RNNs), notably Long Short-Term Memory (LSTM) variants, which excel at modeling time-dependent relationships in greenhouse environments \citep{FOURATI20071016, Gharghory2020, HONGKANG2018790}. In parallel, Nonlinear AutoRegressive models with eXogenous inputs (NARX) have shown effectiveness in capturing complex greenhouse system dynamics \citep{Manonmani2018ModellingAC, gao2023temperature}.
Despite these advances, conventional neural networks predominantly treat environmental variables independently, limiting their ability to depict spatial or directional interconnections. This gap becomes especially relevant in PV-integrated greenhouses, where factors such as solar radiation, humidity, and temperature form intricate environmental interactions that purely temporal models may overlook.
Spatio-Temporal Graph Neural Networks (STGNNs) address this shortfall by explicitly modeling intervariable relationships through graph structures \citep{Yu2018, SUN2022119739}. Here, each environmental variable becomes a node in a directed graph, capturing how changes in one parameter can propagate to others. The Graph Attention Network (GAT) framework further refines this approach by dynamically weighting node and edge influences, enabling simultaneous modeling of environmental dependencies and temporal evolution \citep{Velickovic2018}. Such techniques align with the concept of digital twins in greenhouse systems, where software-based integrative models can guide real-time climate strategies \citep{AASLYNG2005521, AASLYNG2003657} and nutrient management \citep{MASSA2011711}.

In this study we apply and compare RNNs and directed STGNNs for greenhouse temperature prediction under multiple seasonal conditions. Using data from a Mediterranean greenhouse in Volos, Greece, we assess each model’s predictive accuracy, interpretability via attention mechanisms, and computational feasibility. By evaluating whether directional modeling improves performance over simpler temporal approaches, our work advances the broader REGACE objective of developing efficient, sustainable agrivoltaic greenhouses that integrate microclimate modeling with PV energy production and crop modeling.

The paper is structured as follows: Section~\ref{sec: Data} describes the data used in this study, including data collection methods, preprocessing steps, and the handling of missing values. Section~\ref{sec: method} outlines the methodologies employed, offering a concise introduction to deep learning and detailing the architectures and implementation of the RNN and directed STGNN models. Section~\ref{sec: Analysis} presents the analysis and results, comparing the performance of the RNN and directed STGNN models in predicting greenhouse temperature across different seasons. Finally, Sections~\ref{sec: Discussion} and \ref{sec: conclusions} discuss the findings, their implications for greenhouse management, and potential directions for future research.


\section{Data}
\label{sec: Data}

The data used in this study originates from a PV greenhouse located in Volos, Greece, for the year 2020, covering from January 9, 2020, to December 21, 2020. The dataset consists of measurements recorded at 15-minute intervals, resulting in 33,446 data points. 

The features considered during the analysis are given below. The labels used during the analysis are shown in parentheses:
\begin{itemize}
    \item External Environmental Factors:
    \begin{itemize}
        \item External Temperature (OUT\_temp): The ambient temperature outside the greenhouse.
        \item External Relative Humidity (OUT\_RH): The relative humidity of the air outside the greenhouse.
        \item Radiation Level (OUT\_rad): The amount of solar radiation received.
        \item Wind Speed (OUT\_wind\_speed): The speed of the wind outside the greenhouse.
    \end{itemize}
    \item Internal Environmental Factors:
    \begin{itemize}
        \item Greenhouse Temperature (G2\_temp): The internal temperature within Greenhouse.
        \item Greenhouse Relative Humidity (G2\_RH): The internal relative humidity within Greenhouse.
    \end{itemize}
\end{itemize}

The graphs of the temperature inside and outside the greenhouse for the three periods considered follow:

\begin{figure}[H]
    \centering
    \caption{Temperatures inside and outside the greenhouse for the summer period}
\label{fig: summertemp}
\includegraphics[width=1\textwidth]{Summer_Temp.png}
\end{figure}

\begin{figure}[H]
    \centering
    \caption{Temperatures inside and outside the greenhouse for the autumn period}
\label{fig: autumntemp}
\includegraphics[width=1\textwidth]{Autumn_Temp.png}
\end{figure}

\begin{figure}[H]
    \centering
    \caption{Temperatures inside and outside the greenhouse for the winter period}
\label{fig: wintertemp}
\includegraphics[width=1\textwidth]{Winter_Temp.png}
\end{figure}


To capture the complex relationships between these environmental factors, we represent them as nodes in a directed graph, where edges indicate the direction of influence from one variable to another. The graph is presented in Figure~\ref{fig:Graph}.

The directed edges in our graph structure represent fundamental physical relationships between environmental variables, based on well-established thermodynamic and atmospheric physics principles. Our choice of edge directions is motivated by the following physical mechanisms:

\begin{enumerate}
    \item External Relative Humidity Dependencies:
    \begin{itemize}
        \item Temperature $\rightarrow$ RH: Controls air's water vapor capacity through the Clausius-Clapeyron relation
        \item Wind Speed $\rightarrow$ RH: Affects vapor mixing and transport through turbulent diffusion
        \item Solar Radiation $\rightarrow$ RH: Influences evaporation rates and local temperature gradients
    \end{itemize}
    \item External Temperature Dependencies:
    \begin{itemize}
        \item Solar Radiation $\rightarrow$ Temperature: Primary driver through radiative heating
        \item Wind Speed $\rightarrow$ Temperature: Modifies heat exchange through forced convection
    \end{itemize}
    \item Internal Parameters (Temperature and RH) Dependencies:
    \begin{itemize}
        \item External Temperature $\rightarrow$ Internal Temperature: Heat transfer through greenhouse envelope
        \item External RH $\rightarrow$ Internal RH: Vapor exchange through ventilation and infiltration
        \item Solar Radiation → Internal Parameters: Direct heating and greenhouse effect
        \item Wind Speed $\rightarrow$ Internal Parameters: Convective heat exchange rate
        \item Internal Temperature $\leftrightarrow$ Internal RH: Bidirectional coupling through:
        \begin{itemize}
            \item Temperature affecting air's water-holding capacity
            \item Humidity influencing evaporative cooling effects
            \item Plant transpiration processes
        \end{itemize}
    \end{itemize}
\end{enumerate}
These physical relationships form the basis for the graph structure shown in Figure~\ref{fig:Graph}, where each edge represents a direct causal influence between variables. This directed graph approach allows our STGNN model to learn and weight these physical dependencies during the prediction process.



\begin{figure}[H]
\caption{Features' interaction graph} \label{fig:Graph}
\centering 
\includegraphics[width=1\textwidth]{G2winter_graph.png} 
\end{figure}


Moreover, the dataset was divided into three seasons to better investigate the seasonal variations in the greenhouse microclimate and due to the high number of missing data. The dates of the divisions are:
\begin{itemize}
    \item Summer\footnote{ During the summer period, the greenhouse employs a cooling system to prevent internal temperatures from rising excessively.}: From July 1 to August 31,
    \item Autumn: From October 9 to November 30,
    \item Winter: From January 9 to February 29.
\end{itemize}
We used seasonal names for these periods, even though the dates do not perfectly correspond to the year seasons. This discrepancy is due to a significant number of missing values. For the same reason, spring data were excluded from the analysis due to many missing values recorded between April 23 and April 27.

\subsection{Data handling}
\label{sub: Datahandling}

\subsubsection{Missing values}

Table~\ref{Tab:missingvalues} shows the number of missing values for each variable in each season.
\begin{table}[H]
\centering
\caption{Number of missing values for each season}
\label{Tab:missingvalues}
\begin{tabular}{r|r|r|r|r|r|r}
\hline
\textbf{Season} & \textbf{G2\_temp} & \textbf{G2\_RH} & \textbf{OUT\_temp} & \textbf{OUT\_RH} & \textbf{OUT\_rad} & \textbf{wind\_speed} \\
\hline
Summer & 0       & 0     & 0         & 0       & 0        & 0           \\
\hline
Winter & 68      & 68    & 2        & 2      & 2       & 2          \\
\hline
Autumn & 75      & 75    & 75       & 75     & 75      & 75 \\
\hline
\end{tabular}
\end{table}

To address missing data, we employed a combination of linear interpolation and rolling mean techniques. Linear interpolation is effective for high-density time series data, as it estimates missing values by creating a linear equation between the nearest known data points \citep{malvar2004high, MOON2019105023}, as in Equation \ref{eq: linearinterpol}:
\begin{equation}
\label{eq: linearinterpol}
    f(x)=f(x_{o})+\frac{f(x_{1})-f(x_{o})}{x_{1}-x_{o}}(x-x_{o})
\end{equation}
where $f$ is the linear interpolation function, $x$ the target value and the subscripts $0$ and $1$ represent respectively the previous and next value from target value.
This approach ensured that the imputed values preserved the continuity and trends in the time series without introducing noticeable distortions or flattening in the data sequences.
The rolling mean technique was applied in cases where data exhibited more noise or short-term fluctuations, allowing us to smooth these variations while still accounting for missing values.

\subsubsection{Normalization}

All features were normalized using the min-max method. This scaling brings all values into the range $[0, 1]$, bringing all the variables expressed in different units to the same scale and making them comparable \citep{alaimo2021monitoring}.

\subsubsection{Train-Test Split}

To prepare the data for modeling, each seasonal subset of the dataset was divided into training and testing sets while preserving the chronological order. This approach ensured that the integrity of the time series analysis was maintained, as no future information was inadvertently introduced into the training process. 
The first 80\% of observations within each season were allocated to the training set, while the remaining 20\% were used for testing.
The resulting date ranges for the training and testing sets in each seasonal period are as follows:

\begin{itemize} 
\item \textbf{Winter:}      \begin{itemize} 
    \item Training: from 2020-01-09 16:30:00 to 2020-02-19 04:00:00 
    \item Testing: from 2020-02-19 04:15:00 to 2020-02-29 07:00:00 \end{itemize} 
\item \textbf{Summer:} \begin{itemize} 
    \item Training: from 2020-07-01 14:30:00 to 2020-08-19 02:45:00 
    \item Testing: from 2020-08-19 03:00:00 to 2020-08-31 06:00:00 \end{itemize} 
\item \textbf{Autumn:} \begin{itemize} 
    \item Training: from 2020-10-09 22:00:00 to 2020-11-20 13:00:00 
    \item Testing: from 2020-11-20 13:15:00 to 2020-11-30 22:45:00 \end{itemize} 
\end{itemize}

\subsection{Stationarity}

In order to make statistical inference based on time series data, it is essential to ensure that the underlying data generative process fulfill the stationary property \citep{kirchgassner2012introduction}. Stationarity means that the statistical properties of the time series, such as mean and variance, remain consistent over time. This consistency is crucial because it ensures that the patterns observed in the past data are applicable to the future. A time series is stationary if it meets two conditions:
\begin{enumerate}
    \item The mean (average value) of the series is constant over time.
    \item The covariance between two points depends only on the time interval between them, not on their specific positions in time.
\end{enumerate}
Stationarity implies that the series fluctuates around a constant level with consistent variation. This property is essential for making reliable inferences and predictions based on the data \citep{tsay2005analysis}.
To verify that our data met these criteria, we applied the augmented Dickey-Fuller (ADF)  test \citep{Cheung1995}.
The ADF test results indicate that for all variables in all the three considered periods, the null hypothesis of non-stationarity is rejected, confirming that the series are stationary. The results of the ADF test are presented in Tables \ref{Tab:ADFsummer}, \ref{Tab:ADFautumn} and \ref{Tab:ADFwinter} in Appendix~\ref{secAppendixADF Tables}.

\section{Methodology}
\label{sec: method}

Neural Networks (NNs) are computational models inspired by the human brain's neural structure. They consist of interconnected nodes (neurons) organized into layers: an input layer, one or more hidden layers, and an output layer. Each connection between neurons has an associated weight that is adjusted during training to minimize prediction error. Deep Neural Networks (DNNs) extend NNs by incorporating multiple hidden layers, enabling the modeling of complex, nonlinear relationships in data. Activation functions such as ReLU, sigmoid, and tanh introduce nonlinearity, while backpropagation adjusts the weights based on the error rate obtained in previous epochs \citep{nielsen2015neural}.

This section outlines the deep learning techniques employed to develop predictive models of greenhouse temperature. We focus on two architectures: Recurrent Neural Networks (RNNs) and directed Spatio-Temporal Graph Neural Networks (STGNNs). We discuss their theoretical foundations, how they are applied in this study, and why they are particularly well-suited for modeling greenhouse microclimates.

\subsection{Recurrent neural networks}
\label{sub: RNNmethod}

Recurrent Neural Networks (RNNs) are specialized neural networks designed to process sequential data, making them highly suitable for time-series analysis \citep{JAIN1999}. Unlike traditional feedforward neural networks, RNNs have recurrent connections that allow information to persist across time steps, enabling the network to capture temporal dependencies.

An RNN processes sequences one element at a time, maintaining a hidden state that captures information about previous inputs. 
The hidden state $\bm{h}_t$ at time step $t$ is updated based on the current input $\bm{x}_t$ and the previous hidden state $\bm{h}_{t-1}$:
\begin{equation*} 
\bm{h}_t = \tanh(\bm{W}_{h} \bm{x}_{t} + \bm{U}_{h} \bm{h}_{t-1} + \bm{b}_{h}) \end{equation*}
The output $\hat{y}_t$ at time step $t$ is produced from the hidden state $\bm{h}_t$:
\begin{equation*} 
\hat{y}_t = \bm{W}_y \bm{h}_t + \bm{b}_y \end{equation*}
where $\bm{W}_h$, $\bm{U}_h$, and $\bm{W}_y$ are weight matrices. $\bm{b}_h$ and $\bm{b}_y$ are bias vectors.
$\tanh$ is the hyperbolic tangent activation function introducing nonlinearity.
In deeper RNNs, multiple RNN layers are stacked, passing the hidden state from one layer to the next, allowing the model to capture more complex temporal patterns \citep{nketiah2023recurrent}.

RNNs are effective for modeling how current and past environmental conditions influence future states, making them ideal for predicting variables like greenhouse temperature \citep{RODRIGUEZ1999}. 
A known challenge with RNNs is the vanishing gradient problem, where gradients diminish during backpropagation through time, making it difficult to learn long-term dependencies \citep{BEHRANG20101468}. However, since our data exhibits short-term dependencies, this issue do not impact our study \citep{DUBININ2024106179}.

\subsubsection{RNN implementation}

We begin by preparing the dataset, where each input sequence $\bm{X}^{(i)}$ consists of historical environmental measurements over a period of $T$ time steps. The features include greenhouse temperature and relative humidity, as well as outdoor environmental variables such as temperature, relative humidity, solar radiation, and wind speed. The target value $y^{(i)}$ is the greenhouse temperature at the next time step.

The notation used in our RNN methodology is defined as follows:
\begin{itemize}
    \item \textbf{Datasets and Sequences:}
    \begin{itemize}
        \item $\bm{X}^{(i)} \in \mathbb{R}^{T \times F}$: Input sequence for the $i$-th sample, where $T$ is the sequence length and $F$ is the number of features.
        \item $y^{(i)} \in \mathbb{R}$: Target value for the $i$-th sample.
        \item $N$: Total number of training samples.
    \end{itemize}
    \item \textbf{Model Parameters:}
    \begin{itemize}
        \item $E$: Number of training epochs.
        \item $B$: Batch size.
        \item $\bm{\theta}$: Set of all learnable parameters in the RNN.
    \end{itemize}
    \item \textbf{Layers and Functions:}
    \begin{itemize}
        \item $\text{SimpleRNN}(u, \text{return\_seq})$: Simple RNN layer with $u$ units and an option to return sequences.
        \item $\text{Dropout}(r)$: Dropout layer with a dropout rate $r$.
        \item $\text{Dense}(o)$: Fully connected layer with $o$ output units.
        \item $\text{Loss}(\cdot, \cdot)$: Loss function (Mean Squared Error).
        \item $\text{Optimizer}$: Optimization algorithm (Adam optimizer).
    \end{itemize}
\end{itemize}

The RNN model is constructed with two Simple RNN layers, each containing 50 units. The first RNN layer is configured to return sequences $\texttt{(return\_sequences=True)}$, allowing the subsequent layer to receive the entire sequence output. Dropout layers with a rate of 0.2 are interleaved between the RNN layers to prevent overfitting by randomly setting a fraction of input units to zero during training. A Dense layer with one unit at the end of the network outputs the predicted temperature.

The model is compiled using the Adam optimizer \citep{kingma2014adam}, which adapts the learning rate during training for efficient convergence. 
The loss function is set to Mean Squared Error (MSE), suitable for regression tasks.

Training proceeds for $E$ epochs, where in each epoch, the model is trained on the entire training dataset using batches of size $B$. Optionally, a portion of the training data is set aside for validation to monitor the model's performance on unseen data during training.

After training, the model is evaluated on the test dataset to assess its predictive performance. Evaluation metrics such as MSE, Root Mean Squared Error (RMSE), and the coefficient of determination ($R^2$) are computed to quantify the accuracy of the model's predictions.

The training algorithm is summarized as follows:

\begin{algorithm}[H]
\caption{Recurrent Neural Network (RNN) Training Algorithm}
\begin{algorithmic}[1]
\Require Training sequences $\{(X^{(i)}, y^{(i)})\}_{i=1}^N$, sequence length $T$, batch size $B$, number of epochs $E$
\State \textbf{Define} model architecture:
\State \hskip1em \textbf{Layer 1}: Simple RNN with 50 units, input shape $(T, F)$, return sequences \textbf{True}
\State \hskip1em \textbf{Layer 2}: Dropout layer with rate $0.2$
\State \hskip1em \textbf{Layer 3}: Simple RNN with 50 units
\State \hskip1em \textbf{Layer 4}: Dropout layer with rate $0.2$
\State \hskip1em \textbf{Layer 5}: Dense layer with 1 unit (output)
\State \textbf{Compile} model with optimizer \texttt{Adam} and loss \texttt{Mean Squared Error}
\For{epoch $= 1$ to $E$}
    \State \textbf{Train} the model on training data with batch size $B$
    \State \textbf{Optionally validate} the model on validation data
\EndFor
\State \textbf{Evaluate} the model on test data
\State \textbf{Compute} evaluation metrics (MSE, RMSE, $R^2$)
\end{algorithmic}
\end{algorithm}

\subsection{Spatio-Temporal Graph Neural Networks}
\label{sub: GNNmethod}

To model the spatio-temporal dynamics of the greenhouse microclimate, we employ directed STGNN. This approach integrates Graph Attention Networks (GATs) for spatial feature extraction \citep{Velickovic2018} and Long Short-Term Memory (LSTM) networks \citep{Hochreiter1997} for temporal sequence modeling.
Unlike traditional RNN-based models, which focus primarily on temporal sequences, STGNNs leverage Graph Neural Networks (GNNs) to incorporate topological and relational information among variables. This is well-aligned with recent advances in GNNs for time series \citep{Jin2024}. By representing each environmental variable, such as internal temperature, humidity, and external factors like solar radiation and wind speed, as a node in a directed graph, we capture how changes in one variable may propagate to others through directional edges.
Figure~\ref{fig:Graph} illustrates this architecture. Each node corresponds to an environmental variable, while edges encode the direction of influence. At each time step $t$, node features $\bm{X}_t$ are extracted from the input sequence $\bm{X}^{(i)}$. To prevent data leakage, the feature of the target node (greenhouse temperature) at the current time step is masked (set to zero) before model input. The GAT layers learn to weigh the importance of edges and neighboring nodes, revealing spatial dependencies, while the LSTM layers capture long-term temporal patterns.

\subsubsection{Graph Attention Networks and Directed Graphs}

The Graph Attention Network (GAT) layer \citep{Velickovic2018} is employed to handle the directed graph and assign different weights to incoming edges through an attention mechanism. This mechanism allows each node to weigh the importance of its neighbors' features when updating its own representation.

For each node $i$ and its incoming neighbor $j$, the attention coefficient $\alpha_{ij}$ is computed to determine the importance of node $j$'s features to node $i$:

\begin{equation*}
e_{ij} = \text{LeakyReLU}\left( \bm{a}^\top \left[ \bm{W} \bm{x}_i \, \Vert \, \bm{W} \bm{x}_j \right] \right)
\end{equation*}

\begin{equation*}
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i} \exp(e_{ik})}
\end{equation*}

where:

\begin{itemize}
    \item $\bm{x}_i$ and $\bm{x}_j$ are the feature vectors of nodes $i$ and $j$, respectively.
    \item $\bm{W}$ is a learnable weight matrix.
    \item $\bm{a}$ is a learnable weight vector (attention mechanism).
    \item $[ \cdot \, \Vert \, \cdot ]$ denotes the concatenation of two vectors.
    \item $\text{LeakyReLU}$ is the activation function introducing nonlinearity.
    \item $\mathcal{N}_i$ is the set of neighboring nodes sending messages to node $i$.
    \item $e_{ij}$ is the unnormalized attention score.
    \item $\alpha_{ij}$ is the normalized attention coefficient, representing the weight assigned to node $j$'s features by node $i$.
\end{itemize}

The LeakyReLU function, characterized by a small positive slope in its negative part, prevents zero gradients and ensures a more robust learning process, especially important in the attention mechanism where gradient flow is crucial for dynamic weight adjustments.


\textbf{Node Feature Update:}

The updated feature vector $\bm{h}_i$ for node $i$ is computed by aggregating the transformed features of its neighbors, weighted by the attention coefficients:

\begin{equation*}
\bm{h}_i = \sigma\left( \sum_{j \in \mathcal{N}_i} \alpha_{ij} \bm{W} \bm{x}_j \right)
\end{equation*}

where $\sigma$ is an activation function, such as ReLU.

By using the GAT layer, which inherently supports directed graphs, we effectively model the directionality of interactions among environmental variables. The attention mechanism allows the model to assign different weights to incoming edges based on their importance, capturing the dependence relationships in the data.

\subsubsection{Long Short-Term Memory (LSTM) Networks}

Long Short-Term Memory (LSTM) networks \citep{Hochreiter1997} are a specialized type of RNN architecture designed to better capture and retain long-term dependencies in sequential data. Unlike traditional RNN units, which can suffer from the vanishing gradient problem, LSTM cells incorporate gating mechanisms, namely the input, forget, and output gates, allowing the network to selectively add or remove information from its internal cell state. This structure enables the LSTM to preserve context over many time steps, making it well-suited for complex temporal sequences where relevant patterns may span extended intervals. In the directed STGNN model, the LSTM layer processes the sequence of GAT outputs $\{\bm{H}t\}_{t=1}^T$ to extract temporal features that complement the spatial dependencies learned by the GAT layer.

\subsubsection{Directed STGNN model implementation}

The notation used in our STGNN methodology is defined as follows:

\begin{itemize}
    \item \textbf{Datasets and Sequences:}
    \begin{itemize}
        \item $\bm{X}^{(i)} \in \mathbb{R}^{T \times N_n \times F}$: Input sequence for the $i$-th sample, where $T$ is the sequence length, $N_n$ is the number of nodes, and $F$ is the feature size per node (here, $F=1$).
        \item $y^{(i)} \in \mathbb{R}$: Target value for the $i$-th sample.
        \item $N$: Total number of training samples.
    \end{itemize}
    \item \textbf{Graph Components:}
    \begin{itemize}
        \item $E$: Edge index representing directed connections between nodes in the graph.
        \item $N_n$: Number of nodes in the graph.
    \end{itemize}
    \item \textbf{Model Parameters:}
    \begin{itemize}
        \item $E_p$: Number of training epochs.
        \item $B$: Batch size.
        \item $H$: Hidden size of the GAT layer.
        \item $H'$: Hidden size of the LSTM layer.
        \item $K$: Number of attention heads in the GAT layer.
        \item $\bm{\theta}$: Set of all learnable parameters in the STGNN.
    \end{itemize}
    \item \textbf{Layers and Functions:}
    \begin{itemize}
        \item $\text{GAT}(F, H, K)$: GAT layer with input feature size $F$, output size $H$ per head, and $K$ attention heads.
        \item $\text{LSTM}(H \times K \times N_n, H')$: LSTM layer with input size $H \times K \times N_n$ and hidden size $H'$.
        \item $\text{Dropout}(r)$: Dropout layer with a dropout rate $r$.
        \item $\text{Dense}(o)$: Fully connected layer with $o$ output units.
        \item $\text{Loss}(\cdot, \cdot)$: Loss function (Mean Squared Error).
        \item $\text{Optimizer}$: Optimization algorithm (Adam optimizer).
    \end{itemize}
\end{itemize}

Layer 1 (\textbf{GAT Layer}) processes node features $\bm{X}_t$ at each time step $t$ using the graph structure defined by $E$. It aggregates information from neighboring nodes based on attention coefficients to capture spatial dependencies.
Layer 2 (\textbf{LSTM Layer}) processes the sequence of GAT outputs $\{ \bm{H}_t \}_{t=1}^T$ to capture temporal dependencies over the sequence length $T$.
Layer 3 (\textbf{Dropout Layer}) is applied with a rate of 0.2 to mitigate overfitting.
Layer 4 (\textbf{Dense Output Layer}) produces the predicted greenhouse temperature $\hat{y}$.

To avoid data leakage, the feature of the target node (greenhouse temperature) at the current time step is set to zero during input preparation.
The model is compiled using the Adam optimizer \citep{kingma2014adam} and the Mean Squared Error (MSE) loss function. It is trained over $E_p$ epochs using batches of size $B$. During training, gradients are computed through backpropagation, and model parameters $\bm{\theta}$ are updated accordingly.

After training, the model is evaluated on the test dataset. Evaluation metrics such as MSE, RMSE, and $R^2$ score are calculated to assess the model's performance.

The training algorithm is summarized as follows:
\begin{algorithm}[H]
\caption{Directed STGNN Training Algorithm}
\begin{algorithmic}[1]
\Require Training sequences $\{ (\bm{X}^{(i)}, y^{(i)}) \}_{i=1}^N$, edge index $E$, sequence length $T$, batch size $B$, number of epochs $E_p$, number of nodes $N_n$
\State \textbf{Initialize} model parameters $\bm{\theta}$
\State \textbf{Define} model architecture:
\State \hskip1em \textbf{Layer 1}: $\text{GAT}(F, H, K)$
\State \hskip1em \textbf{Layer 2}: $\text{LSTM}(H \times K \times N_n, H')$
\State \hskip1em \textbf{Layer 3}: $\text{Dropout}(0.2)$
\State \hskip1em \textbf{Layer 4}: $\text{Dense}(1)$ (output layer)
\For{epoch $= 1$ to $E_p$}
    \For{each batch $\{ (\bm{X}_{\text{batch}}, y_{\text{batch}}) \}$}
        \State \textbf{Forward pass}:
        \For{time step $t = 1$ to $T$}
            \State Extract node features $\bm{X}_t$ from $\bm{X}_{\text{batch}}$ at time $t$
            \State \textbf{Prevent data leakage}: Set target node feature $\bm{x}_v \gets 0$
            \State Apply GAT layer: $\bm{H}_t \gets \text{GAT}(\bm{X}_t, E)$
        \EndFor
        \State Stack $\bm{H}_t$ over time to form $\bm{H}_{\text{seq}}$
        \State Apply LSTM layer: $\bm{H}_{\text{lstm}} \gets \text{LSTM}(\bm{H}_{\text{seq}})$
        \State Apply Dropout: $\bm{H}_{\text{drop}} \gets \text{Dropout}(\bm{H}_{\text{lstm}})$
        \State Compute output: $\hat{y} \gets \text{Dense}(\bm{H}_{\text{drop}})$
        \State \textbf{Compute loss}: $L \gets \text{Loss}(y_{\text{batch}}, \hat{y})$
        \State \textbf{Backward pass}: Compute gradients $\nabla_{\bm{\theta}} L$
        \State \textbf{Update parameters}: $\bm{\theta} \gets \bm{\theta} - \eta \nabla_{\bm{\theta}} L$
    \EndFor
    \State \textbf{Optionally validate} the model on validation data
\EndFor
\State \textbf{Evaluate} the model on test data
\State \textbf{Compute} evaluation metrics (MSE, RMSE, $R^2$)
\end{algorithmic}
\end{algorithm}

\section{Analysis and results}
\label{sec: Analysis}

This section presents the results obtained from applying the RNN and directed STGNN models to the greenhouse temperature prediction task. Both models were trained and tested using a sequence length of $T=96$ time steps, corresponding to a full 24-hour cycle of observations recorded at 15-minute intervals. This setup allowed the models to capture daily patterns and short-term dependencies effectively.

In addition, both models were trained for 32 epochs with a batch size of 96, aligning with practices in comparable studies \citep{GNmodelling2021, JUNG2020}. Furthermore, 10\% of the training set in each seasonal period was used as a validation set to monitor the models’ performance during training. This approach prevented overfitting, evidenced by the validation loss never surpassing the training loss throughout the process.

To facilitate direct comparisons between the RNN and STGNN models, the same evaluation metrics were employed. Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) measured the prediction error magnitude, while the coefficient $R^2$ assessed the proportion of variance in greenhouse temperature explained by the model’s predictions.

\subsection{Recurrent neural networks application}
\label{sub: RNNapplication}

Table~\ref{tab:RNN_metrics} summarizes the test set results obtained by the RNN model for the three considered seasonal periods.

\begin{table}[H] 
\centering 
\caption{RNN test performance across different seasons} \label{tab:RNN_metrics} \begin{tabular}{l|r|r|r} 
\hline 
\textbf{Season} & \textbf{MSE} & \textbf{RMSE} & $\bm{R^2}$ \\ \hline 
Summer & 0.0032 & 0.056 & 0.937 \\ Winter & 0.0005 & 0.022 & 0.985 \\ Autumn & 0.0007 & 0.026 & 0.983 \\ \hline 
\end{tabular} 
\end{table}

The RNN demonstrates strong predictive capabilities, achieving high $R^2$ values across all seasons. Its performance is particularly impressive in winter, where $R^2 \approx 0.985$ indicates that the model captures nearly all the variance in greenhouse temperature. This  result depends by several factors related to greenhouse operation during the cold season. First, despite winter typically exhibiting large external temperature fluctuations, the greenhouse's  effectively maintains stable internal conditions, making the system's behavior more predictable. Second, the reduced solar radiation intensity in winter means the greenhouse is less subject to sudden heat gains that could perturb the internal environment. Third, the thermal screen's regular nighttime deployment (activated whenever the outside temperature is 2°C lower than the heating setpoint) creates a more controlled environment with reduced thermal exchanges. These factors combine to create a more deterministic system behavior that the RNN can effectively learn and predict, despite the external environment's inherent variability.


In contrast, while the summer period still exhibits a high $R^2$ of about $0.937$, the prediction errors (MSE and RMSE) are relatively higher, with distinct patterns visible in the model's performance. As shown in Figure~\ref{fig:RNN_summer} the model's predictions deviate significantly from observations at higher temperatures, producing a characteristic box-shaped pattern in the scatter plot. This behavior directly results from the greenhouse's cooling system activation, which triggers when internal temperature reaches a threshold of 25°C. Since our empirical model does not explicitly account for this control system, it fails to capture the sudden temperature regulation that occurs above this threshold. However, examining the lower portion of the scatter plot (below normalized temperature values of 0.6), where the cooling system remains inactive, reveals that the model maintains excellent predictive accuracy. This dichotomy in performance clearly demonstrates how the absence of cooling system dynamics in our model impacts its predictive capabilities, particularly during periods of peak thermal stress when temperature control mechanisms are most active.. Autumn performance falls between these two cases, with $R^2 \approx 0.983$, indicating strong predictive accuracy albeit slightly less than that seen in winter.

Figure~\ref{fig:RNN_winter} provides a scatter plot comparing observed and predicted greenhouse temperatures during the winter period. Points closely aligned along the 1:1 line signify accurate predictions. Figure~\ref{fig:RNNtime_winter} shows the time series of actual versus predicted values, illustrating how well the RNN’s predictions track the observed temperatures over time.

\begin{figure}[H] 
\centering 
\caption{Scatter plot of the observed vs. predicted greenhouse temperature for the winter period using the RNN model.} 
\label{fig:RNN_winter} 
\includegraphics[width=\textwidth]{RNNwinter_fit.png} \end{figure}

\begin{figure}[H] 
\centering 
\caption{Time series of the observed and predicted greenhouse temperature for the winter period using the RNN model.} 
\label{fig:RNNtime_winter} 
\includegraphics[width=\textwidth]{RNNwinter_time.png} \end{figure}

Similar plots for summer and autumn are included in Appendix~\ref{secAppendixRNN Figures}. These additional figures reveal analogous tendencies: the model accurately tracks observed temperatures, especially in winter, where conditions appear more stable, allowing the RNN to predict internal temperature with remarkable confidence.

While the RNN achieves robust performance in all three periods, the differences across seasons reveal important insights about greenhouse control systems and model limitations. The near-ideal predictions in winter reflect the greenhouse's effective temperature stabilization, creating a more predictable environment despite external variability. Autumn shows similar high performance, as temperatures rarely reach extremes requiring intensive control interventions. However, summer predictions show a clear dichotomy: excellent accuracy below the cooling threshold temperature, but significant deviations when the cooling system activates at 25°C. This pattern demonstrates that our empirical model's performance is strongly tied to the greenhouse's control regime rather than external conditions alone, performing best when passive thermal regulation dominates and struggling when active cooling interventions occur.


\subsection{Spatio-Temporal Graph Neural Network application} \label{sub:STGNNapplication}

Table~\ref{tab:STGNN_metrics} presents the directed STGNN test set results for the three seasonal periods, using the same evaluation metrics as applied to the RNN model. As before, the sequence length, training epochs, batch size, and validation strategy remain consistent.

\begin{table}[H] 
\centering \caption{STGNN test performance across different seasons} \label{tab:STGNN_metrics} \begin{tabular}{l|r|r|r} 
\hline 
\textbf{Season} & \textbf{MSE} & \textbf{RMSE} & $\bm{R^2}$ \\ \hline 
Summer & 0.0061 & 0.078 & 0.887 \\ Winter & 0.0023 & 0.048 & 0.947 \\ Autumn & 0.0034 & 0.058 & 0.904 \\ \hline 
\end{tabular} 
\end{table}

While the STGNN model still achieves relatively high $R^2$ values, its overall performance is lower than that of the RNN. In winter, the STGNN attains an $R^2$ of approximately 0.947, which, although strong, falls short of the near-perfect predictions obtained by the RNN. Similarly, in summer and autumn, the STGNN yields lower $R^2$ scores and higher MSE and RMSE values than the RNN, indicating that incorporating spatial dependencies and directionality did not enhance predictive accuracy in this particular setting.

Figure~\ref{fig:STGNN_winter} provides a scatter plot of observed versus predicted temperatures for the winter period, and Figure~\ref{fig:STGNNwinter_time} shows the corresponding time series comparison. These visualizations confirm that, while the STGNN's predictions track the overall trends, the model does not reach the same level of precision demonstrated by the RNN.

\begin{figure}[H] \centering \caption{Scatter plot of observed vs. predicted greenhouse temperature for the winter period using the STGNN model.} 
\label{fig:STGNN_winter} \includegraphics[width=\textwidth]{STGNNwinter_fit.png} \end{figure}

\begin{figure}[H] 
\centering 
\caption{Time series of observed and predicted greenhouse temperature for the winter period using the STGNN model.} 
\label{fig:STGNNwinter_time} \includegraphics[width=\textwidth]{STGNNwinter_time.png} \end{figure}

Plots for summer and autumn are included in Appendix~\ref{secAppendixSTGNN Figures}. They similarly show that the STGNN model, despite considering spatial relationships and directed interactions, does not outperform the simpler temporal approach employed by the RNN. This outcome suggests that, for the given data and conditions, capturing complex spatial-temporal dependencies does not necessarily translate into improved predictive accuracy.


\section{Discussion}
\label{sec: Discussion}

The comparative evaluation of RNN and directed STGNN models for greenhouse temperature prediction provides insights into how each model leverages available information and handles environmental complexity. The strong performance of the RNN, particularly during the winter period suggests that temporal patterns alone can effectively capture many of the dependencies underlying the greenhouse microclimate. Under these circumstances, long-term stationarity and moderate seasonal variations allow historical sequences of internal and external factors to sufficiently inform future conditions. As a result, the RNN consistently achieves near-perfect predictions in such stable regimes.

However, as conditions grow more complex, the advantages of a temporal-only model appear less pronounced. In summer, for example, the accuracy of the RNN decreases, indicating the challenges posed by more dynamic and rapidly fluctuating environmental factors. Abrupt changes in solar radiation, the intermittent operation of cooling systems, or sudden shifts in external temperature highlight a broader limitation: the RNN’s inability to explicitly model the spatial and directional interactions among variables. While it excels in extracting temporal dependencies, it cannot directly encode the influence structure that might clarify why certain variables cause distinct, localized changes or how perturbations propagate from one factor to another.

In principle, the STGNN architecture is designed to address this very limitation. By representing each variable as a node in a directed graph and capturing the directionality of interactions, the STGNN should be well-suited to handle scenarios where spatial relationships and directional effects matter. The GAT component of the STGNN aims to highlight influential nodes and edges, while the LSTM component extracts temporal dependencies. Together, this framework has the potential to disentangle complex spatio-temporal patterns that a purely temporal model may overlook.

Yet, in our experiments, the STGNN did not outperform the RNN. This outcome can be attributed to several factors. First, the set of input variables, though representative of basic external and internal greenhouse conditions, may not be rich enough to fully leverage the STGNN’s spatial modeling capabilities. For instance, without additional features that intensify spatial heterogeneity (e.g., PV generation data influencing localized temperature gradients, or crop growth indicators that reflect biological processes tied to specific sections of the greenhouse), the STGNN may not find compelling reasons to depart from solutions similar to those of a temporal-only model. In other words, if the underlying system appears nearly homogeneous from a spatial perspective, or if directional edges do not provide sufficiently distinct pathways of influence, the added complexity of the STGNN architecture may not yield tangible benefits.

Second, the STGNN’s performance depends on accurately learning graph structures and attention patterns that differentiate the importance of certain nodes and edges. If the graph connections are not strongly differentiated, the GAT may not produce markedly different node embeddings, diminishing the model’s advantage. The LSTM layer, while capable of capturing temporal information, cannot compensate if the spatial relationships remain underexploited.

These observations guide the interpretation of our results. The theoretical superiority of STGNN in handling complex and directed spatio-temporal dependencies is not dismissed by the current findings. Instead, the experiments suggest that to unlock the STGNN’s full potential, the modeling scenario must present more pronounced spatial variations, clearer directional influences, or a richer set of variables that intensify these spatial-temporal complexities. Under such enriched conditions, the STGNN may provide insights and accuracies that temporal-only models cannot match.


\section{Conclusions and future developments}
\label{sec: conclusions}

This study examined the performance of Recurrent Neural Networks (RNNs) and directed Spatio-Temporal Graph Neural Networks (STGNNs) in predicting greenhouse temperature under different seasonal conditions. Despite the STGNN’s conceptual ability to model spatial and directional relationships, the RNN model yielded higher predictive accuracy given the current set of input variables and operating conditions. The strong performance of the RNN, particularly during the winter period, illustrates that purely temporal modeling can effectively capture short-term dependencies.

The implications of this research extend far beyond technical model comparisons, touching on critical aspects of sustainable agriculture and efficient land use. Predicting greenhouse microclimate accurately is a crucial first step toward integrating photovoltaic (PV) systems into greenhouses. This integration promises to optimize land use by combining energy production with food cultivation, thereby enhancing sustainable agricultural practices. It is instrumental in promoting sustainable land management practices that are vital in addressing global environmental challenges, such as resource efficiency and climate change mitigation.

Looking ahead, a more diverse set of input variables may enhance the potential benefits of incorporating spatial and directional information.
Despite the STGNN's current performance not surpassing the RNN in this specific scenario, its architectural advantages position it as a promising framework for future greenhouse modeling challenges. As we move toward integrating photovoltaic systems, the STGNN's ability to handle directed relationships will become crucial for modeling how PV panels affect local shading patterns, temperature gradients, and energy flows. The graph structure naturally accommodates the addition of new nodes representing PV generation parameters, inverter efficiency, and panel temperature, along with their complex interactions with the greenhouse environment. Similarly, when incorporating crop models, the STGNN can represent different plant varieties as separate nodes, each with unique growth parameters and responses to environmental conditions, enabling simulation of mixed-crop scenarios within the same greenhouse. This extensibility last to potential future additions such as energy storage systems, artificial lighting, or multiple greenhouse compartments with distinct microclimates. Furthermore, the attention mechanism in GAT layers could prove valuable in identifying which relationships become more or less important under different operating conditions or seasonal changes, providing insights that could inform greenhouse design and control strategies. This scalability and interpretability make the STGNN framework particularly valuable for developing comprehensive digital twins of increasingly complex agricultural systems, even if simpler models may suffice for basic temperature prediction tasks.

Such tests could confirm whether additional complexity and diversity in data naturally lead to scenarios where spatial-temporal modeling offers clear advantages, ultimately guiding the development of more robust and universally applicable predictive solutions for controlled-environment agriculture.

This study represents an initial step in the broader REGACE project's vision of developing comprehensive digital twins for PV-integrated greenhouses. While our current models focus on temperature prediction, they lay the groundwork for REGACE's more ambitious goals. The project aims to create an integrated modeling framework that combines greenhouse microclimate prediction with PV system performance and crop growth dynamics. Future work within REGACE will focus on expanding these models to incorporate PV panel characteristics, testing different panel configurations and their impact on both energy generation and crop growth. Additionally, the project will explore how these empirical models can complement detailed physical simulations, potentially offering rapid preliminary assessments for greenhouse design optimization. By developing these tools within the REGACE framework, we aim to support the widespread adoption of agrivoltaic greenhouse systems while ensuring optimal conditions for both energy generation and crop production.

\section{Acknowledgment}

This research was carried out within the framework of the European REGACE project, funded by the European Union's Horizon Europe Research and Innovation Programme under Grant Agreement No. 101096056.


\bibliography{MAIN}

\appendix
\section{Supplementary Information}
\label{secAppendixpartial}

\subsection{Figures}
\label{secAppendixpartial Figures}

\subsubsection{RNN results figures}
\label{secAppendixRNN Figures}

The graphs of the RNN results for the summer and autumn periods follow.

\begin{figure}[H] 
\centering 
\caption{Scatter plot of the observed vs. predicted greenhouse temperature for the summer period using the RNN model.} 
\label{fig:RNN_summer} 
\includegraphics[width=\textwidth]{RNNsummer_fit.png} \end{figure}

\begin{figure}[H] 
\centering 
\caption{Time series of the observed and predicted greenhouse temperature for the summer period using the RNN model.} 
\label{fig:RNNtime_summer} 
\includegraphics[width=\textwidth]{RNNsummer_time.png} \end{figure}

\begin{figure}[H] 
\centering 
\caption{Scatter plot of the observed vs. predicted greenhouse temperature for the autumn period using the RNN model.} 
\label{fig:RNN_autumn} 
\includegraphics[width=\textwidth]{RNNautumn_fit.png} \end{figure}

\begin{figure}[H] 
\centering 
\caption{Time series of the observed and predicted greenhouse temperature for the autumn period using the RNN model.} 
\label{fig:RNNtime_autumn} 
\includegraphics[width=\textwidth]{RNNautumn_time.png} \end{figure}


\subsubsection{STGNN temperature figures}
\label{secAppendixSTGNN Figures}

The graphs of the directed STGNN results for the summer and autumn periods follow.

\begin{figure}[H] 
\centering 
\caption{Scatter plot of the observed vs. predicted greenhouse temperature for the summer period using the STGNN model.} 
\label{fig:STGNN_summer} 
\includegraphics[width=\textwidth]{STGNNsummer_fit.png} \end{figure}

\begin{figure}[H] 
\centering 
\caption{Time series of the observed and predicted greenhouse temperature for the summer period using the STGNN model.} 
\label{fig:STGNNtime_summer} 
\includegraphics[width=\textwidth]{STGNNsummer_time.png} \end{figure}

\begin{figure}[H] 
\centering 
\caption{Scatter plot of the observed vs. predicted greenhouse temperature for the autumn period using the STGNN model.} 
\label{fig:STGNN_autumn} 
\includegraphics[width=\textwidth]{STGNNautumn_fit.png} \end{figure}

\begin{figure}[H] 
\centering 
\caption{Time series of the observed and predicted greenhouse temperature for the autumn period using the STGNN model.} 
\label{fig:STGNNtime_autumn} 
\includegraphics[width=\textwidth]{STGNNautumn_time.png} \end{figure}


\subsection{Supplementary tables}
\label{secAppendixpartial Tables}

\subsubsection{ADF test results tables}
\label{secAppendixADF Tables}

\textbf{Hypotheses}
\begin{itemize}
    \item $\mbox{H}_0$: The series has a unit root (non-stationary),
    \item $\mbox{H}_1$: The series is stationary.
\end{itemize}

\begin{table}[H]
\centering
\caption{ADF test results for stationarity for the summer period}
\label{Tab:ADFsummer}
\begin{tabular}{r|c|c|c}
\hline
\textbf{Variable}         & \textbf{Dickey-Fuller Statistic} & \textbf{p-value} & \textbf{Conclusion} \\
\hline
G2\_temp         & -15.855                                         & \textless 0.01                  & Stationary                         \\
\hline
G2\_RH           & -11.471                                         & \textless 0.01                  & Stationary                         \\
\hline
OUT\_temp        & -21.311                                         & \textless 0.01                  & Stationary                         \\
\hline
OUT\_RH          & -16.621                                         & \textless 0.01                  & Stationary                         \\
\hline
OUT\_rad         & -21.836                                         & \textless 0.01                  & Stationary                         \\
\hline
OUT\_wind\_speed & -15.131                                         & \textless 0.01                  & Stationary  \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{ADF test results for stationarity for the autumn period}
\label{Tab:ADFautumn}
\begin{tabular}{r|c|c|c}
\hline
\textbf{Variable}         & \textbf{Dickey-Fuller Statistic} & \textbf{p-value} & \textbf{Conclusion}\\ 
\hline
G2\_temp       & -16.11                                  & \textless 0.01                  & Stationary                         \\
\hline
G2\_RH           & -13.328                                 & \textless 0.01                  & Stationary                         \\
\hline
OUT\_temp        & -14.324                                 & \textless 0.01                  & Stationary                         \\
\hline
OUT\_RH          & -15.253                                 & \textless 0.01                  & Stationary                         \\
\hline
OUT\_rad         & -17.418                                 & \textless 0.01                  & Stationary                         \\
\hline
OUT\_wind\_speed & -10.395                                 & \textless 0.01                  & Stationary   \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{ADF test results for stationarity for the winter period}
\label{Tab:ADFwinter}
\begin{tabular}{r|c|c|c}
\hline
\textbf{Variable}         & \textbf{Dickey-Fuller Statistic} & \textbf{p-value} & \textbf{Conclusion}\\ 
\hline
G2\_temp     & -18.623                                 & \textless 0.01                  & Stationary                         \\
\hline
G2\_RH           & -14.903                                 & \textless 0.01                  & Stationary                         \\
\hline
OUT\_temp        & -13.869                                 & \textless 0.01                  & Stationary                         \\
\hline
OUT\_RH          & -13.673                                 & \textless 0.01                  & Stationary                         \\
\hline
OUT\_rad         & -17.356                                 & \textless 0.01                  & Stationary                         \\
\hline
OUT\_wind\_speed & -9.815                                  & \textless 0.01                  & Stationary   \\
\hline
\end{tabular}
\end{table}


\end{document}