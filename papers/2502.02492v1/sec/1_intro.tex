% \vspace{-0.8cm}
\section{Introduction}
\label{sec:intro}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.98\textwidth]{figures/failures.pdf}
\vspace{-14px}
\caption{\textbf{Motion incoherence in video generation.} Examples of incoherent generations by DiT-30B~\cite{dit}. The model struggles with (a) basic motion, e.g., jogging (stepping on the same leg repeatedly); (b) complex motion e.g., gymnastics; (c) physics, e.g., object dynamics (the hoop passes through the woman); and (d) rotational motion, failing to replicate simple repetitive patterns.}
\label{fig:failures}
\vspace{-12px}
\end{figure*}

Recent advances in video generation showcased remarkable progress in producing high-quality clips~\cite{sora,kling,moviegen}. Yet, despite continuous improvements in the visual quality of the generated videos, these models often fail to accurately portray motion, physics, and dynamic interactions~\cite{physics,sora} (Fig.~\ref{fig:failures}). When tasked with generating challenging motions like gymnastic elements (e.g., a cartwheel in Fig.~\ref{fig:failures}(b)), the generations often display severe deformations, such as the appearance of additional limbs. In other cases, the generations exhibit behavior that contradicts fundamental physics, such as objects passing through other solid objects (e.g., a hula hoop passing through a woman in Fig.~\ref{fig:failures}(c)). Another example is rotational motion, where models struggle to replicate a simple repetitive pattern of movement (e.g., a spinner in Fig.~\ref{fig:failures}(d)). Interestingly, these issues are prominent even for basic motion types that are well-represented in the model's training data (e.g., jogging in Fig.~\ref{fig:failures}(a)), suggesting that data and scale may not be the sole factors responsible for temporal issues in video models.

In this work, we aim to provide insights into why video models struggle with temporal coherence and introduce a generic solution that achieves state-of-the-art motion generation results. First, we find that the gap between pixel quality and motion modeling can be largely attributed to the common training objective. Through qualitative and quantitative experiments (see Sec.~\ref{sec:motivation}), we show that the pixel-based objective is \emph{nearly invariant to temporal perturbations} in generation steps that are critical to determining motion. 

Motivated by these insights, we propose \textbf{VideoJAM}, a novel framework that equips video models with an explicit motion prior by teaching them a \textbf{J}oint \textbf{A}ppearance-\textbf{M}otion representation. This is achieved through two complementary modifications: during training, we amend the objective to predict motion in addition to appearance, and during inference, we propose a guidance mechanism to leverage the learned motion prior for temporally coherent generations.

Specifically, during the VideoJAM training, we pair the videos with their corresponding motion representations and modify the network to predict both signals (appearance and motion). 
To accommodate this dual format, we only add two linear layers to the architecture (see Fig.~\ref{fig:architecture}). The first, located at the input to the model, combines the two signals into a single representation. The second, at the model's output, extracts a motion prediction from the learned joint representation. The objective function is then modified to predict the joint appearance-motion distribution, encouraging the model to rely on the added motion signal.

At inference, our primary objective is video generation, with the predicted motion serving as an auxiliary signal. To guide the generation to effectively incorporate the learned motion prior, we introduce \textbf{Inner-Guidance}, a novel inference-time guidance mechanism. Unlike existing approaches~\cite{ho2022classifier,brooks2022instructpix2pix}, which depend on fixed external signals, Inner-Guidance leverages the model's own evolving motion prediction as a dynamic guidance signal. This setting requires addressing unique challenges: the motion signal is inherently dependent on the other conditions and the model weights, making the assumptions of prior works invalid and requiring a new formulation (Sec.~\ref{sec:related}, App.~\ref{sec:IP2P}). Our mechanism directly modifies the model's sampling distribution to steer the generation toward the joint appearance-motion distribution and away from the appearance-only prediction, allowing the model to refine its own outputs throughout the generation process. 

Through extensive experiments, we demonstrate that applying VideoJAM to pre-trained video models significantly enhances motion coherence across various model sizes and diverse motion types. Furthermore, VideoJAM establishes a new state-of-the-art in motion modeling, surpassing even highly competitive proprietary models.
These advances are achieved without the need for any modifications to the data or model scaling. With an intuitive design requiring only the addition of two linear layers, VideoJAM is both generic and easily adaptable to any video model.
Interestingly, VideoJAM also improves the perceived quality of the generations, even though we do not explicitly target pixel quality.
These findings underscore that appearance and motion are not mutually exclusive but rather inherently complementary.