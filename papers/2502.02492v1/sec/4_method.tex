\section{VideoJAM}
\label{sec:method}
Motivated by the insights from the previous section, we propose to teach the model a joint representation encapsulating both appearance and motion. 
Our method consists of two complementary phases (see Fig.~\ref{fig:architecture}): (i) During training, we modify the objective to predict the joint appearance-motion distribution; This is achieved by altering the architecture to support a dual input-output format, where the model predicts both the appearance and the motion of the video. (ii) At inference, we add Inner-Guidance, a novel formulation that employs the predicted motion to guide the generated  video toward coherent motion.


\vspace{-0.2cm}
\subsection{Preliminaries}
\label{sec:preliminaries}
We conduct our experiments on the Diffusion Transformer (DiT) architecture, which has become the standard backbone for video generation~\cite{sora,genmo2024mochi}. The model operates in the latent space of a Temporal Auto-Encoder (TAE), which downsamples videos spatially and temporally for efficiency. We use Flow Matching~\cite{flow-matching} to define the objective. During training, given a video $x_1$, random noise $x_0 \sim \mathcal{N}(0,I)$, and a timestep $t \in [0,1]$, $x_1$ is noised using $x_0$ to obtain an intermediate latent as follows,
\begin{equation}
    x_t = tx_1 + \left(1- t\right)x_0.
    \label{eq:linear}
\end{equation}
The model is then optimized to predict the velocity, namely,
\begin{equation}
    v_t = \frac{dx_t}{dt}=x_1 - x_0.
    \label{eq:velocity}
\end{equation}
Thus, the objective function employed for training becomes,
\begin{equation}
    \mathcal{L} = \mathbb{E}_{x_1,x_0\sim\mathcal{N}(0,1),y,t\in [0,1]} \left [ || u(x_t, y, t; \theta) - v_t ||_2^2 \right ],
    \label{eq:clean_objective}
\end{equation}
where $y$ is an (optional) input condition, $\theta$ denotes the weights, and $u(x_t, y, t; \theta)$ is the prediction by the model. 

The prediction, $u$, is obtained using the DiT. First, the model ``patchifies'' $x_t$ into a sequence of $p \times p$ video patches. This sequence is projected into the DiT's embedding space via a linear projection, $\textbf{W}_{in}\in \mathbb{R}^{p^2\cdot C_{\text{TAE}}\times C_{\text{DiT}}}$, where $C_{\text{TAE}}$ and $C_{\text{DiT}}$ are the embedding dimensions of the TAE and DiT, respectively. The DiT then applies stacked attention layers to produce a latent representation for the video, which is projected back to the TAE's space to yield the final prediction using $\textbf{W}_{out}\in \mathbb{R}^{C_{\text{DiT}}\times C_{\text{TAE}}\cdot p^2}$, i.e.,
\begin{equation}
   u(x_t, y, t; \theta) =  \mathcal{M}( x_t \cdot \textbf{W}_{in}, y, t; \theta) \cdot \textbf{W}_{out},
    \label{eq:output}
\end{equation}
where $\mathcal{M}$ denotes the attention blocks. 
For efficiency, we employ models that are pre-trained as described above and fine-tune them using VideoJAM as explained next. 

\vspace{-0.2cm}
\subsection{Joint Appearance-Motion Representations}
\label{sec:method_joint_rep}

We begin by describing the motion representation employed by VideoJAM. We opt to use optical flow since it is flexible, generic, and easily represented as an RGB video; thus, it does not require training an additional TAE.
Optical flow computes a dense displacement field between pairs of frames. Given two frames $I_1, I_2\in \mathbb{R}^{H\times W\times 3}$, the optical flow, $d\in \mathbb{R}^{H\times W\times 2}$, holds that $d(u,v)$ is the displacement of the pixel $(u,v)$ from $I_1$ in $I_2$. To convert $d$ into an RGB image, we compute the angle and norm of each pixel,
\begin{align}
     m = \min\left\{1,\frac{\sqrt{u^2+v^2}}{\sigma\sqrt{H^2+W^2}}\right\},  &\alpha = \arctan2(v,u),
     \label{eq:optical_flow_normalization}
\end{align}
where $m$ is the normalized motion magnitude, $\sigma=0.15$, and $\alpha$ is the motion direction (angle). Each angle is assigned a color and the pixel opacity is determined by $m$. Our normalization enables the model to capture motion magnitude, with larger movements corresponding to higher $m$ values and reduced opacity. By using a coefficient $\sigma = 0.15$ instead of the full resolution ($\sqrt{H^2+W^2}$), we prevent subtler movements from becoming too opaque, ensuring they remain distinguishable.
The RGB optical flow is processed by the TAE to produce a noised representation, $d_t$ (see Eq.~\ref{eq:linear}).

Next, we modify the model to predict the joint distribution of appearance and motion. We achieve this by altering the architecture to a dual input-output format, where the model takes both a noised video, $x_t$, and a noised flow, $d_t$, and predicts both signals. This requires modifying two linear projection matrices,  $\textbf{W}_{in}$ and $\textbf{W}_{out}$ (see Fig.~\ref{fig:architecture}(a)). 

First, we extend the input projection $\textbf{W}_{in}$ to take two inputs-- the video and motion latents, $x_t, d_t$. This is done by adding $C_{\text{TAE}}\cdot p^2$ zero-rows to obtain a dual-projection matrix $\textbf{W}^+_{in}\in \mathbb{R}^{2\cdot C_{\text{TAE}}\cdot p^2\times C_{\text{DiT}}}$ such that at initialization, the network is equivalent to the pre-trained DiT, and ignores the added motion signal.
Second, we extend $\textbf{W}_{out}$ with an additional output matrix to obtain $\textbf{W}^+_{out}\in \mathbb{R}^{C_{\text{DiT}}\times 2\cdot C_{\text{TAE}}\cdot p^2}$. The added layer extracts the motion prediction from the joint latent representation.
Together, $\textbf{W}^+_{in}$ and $\textbf{W}^+_{out}$, alter the model to a dual input-output format that processes and predicts both appearance and motion. 

As shown in Fig.~\ref{fig:architecture}(a), our modifications maintain the original latent dimensions of the DiT. Essentially, this requires the model to \emph{learn a single unified latent representation}, from which both signals are predicted using a linear projection. Plugging the above into Eq.~\ref{eq:output} we get, 
\begin{align*}
     \bold{u^+}([x_t, d_t], y, t; \theta') = \mathcal{M}([x_t,d_t] \cdot \textbf{W}^+_{in}, y, t; \theta) \cdot \textbf{W}^+_{out},
\end{align*}
where $[\bullet]$ denotes concatenation in the channel dimension, $\theta'$ denotes the extended model weights as specified above, and $\bold{u^+}=[u^x, u^{d}]$ denotes the dual output, where the first channels represent the appearance (video) prediction, while the last ones represent the motion (optical flow) prediction. 

Finally, we extend the training objective to include an explicit motion term, thus the objective from Eq.~\ref{eq:clean_objective} becomes,
\begin{equation}
    \mathcal{L} = \mathbb{E}_{[x_1,d_1],[x_0,d_0],y,t} \left [ || \bold{u^+}([x_t, d_t], y, t; \theta') - \bold{v^+_t} ||_2^2 \right ],
    \label{eq:objective}
\end{equation}
where $\bold{v^+_t}=[v^x_t, v^{d}_t]$ is calculated using Eq.~\ref{eq:velocity}. Note that while we only modify two linear layers, we jointly fine-tune all the weights in the network, to allow the model to learn the new target distribution.

At inference, the model generates both the video and its motion representation from noise. Note that we are mostly interested in the video prediction, whereas the motion prediction guides the model toward temporally plausible outputs.

\vspace{-0.2cm}
\subsection{Inner-Guidance}
\label{sec:inner_guidance}
As previously observed~\cite{ho2022classifier}, conditioning a diffusion model on an auxiliary signal does not guarantee that the model will faithfully consider the condition. 
Therefore, we propose to modify the diffusion score function to steer the prediction toward plausible motion.  

In our setting, there are two conditioning signals: the prompt, $y$, and the \emph{noisy intermediate motion prediction}, $d_t$. Notably, $d_t$ inherently depends on the prompt and model weights, as it is generated by the model itself. Consequently, existing approaches that assume independence between conditions and model weights (e.g.,~\citet{brooks2022instructpix2pix}), are not applicable in this setting (Sec.~\ref{sec:related}, App.~\ref{sec:IP2P}). To address this, we propose to \emph{directly modify the sampling distribution},
\begin{equation}
 \begin{aligned}   
     \Tilde{p}_{\theta'}([x_t, d_t] | y) \propto \quad\quad\quad \quad\quad \\ {p}_{\theta'}([x_t, d_t] | y) {p}_{\theta'}(y | [x_t, d_t])^{w_1} {p}_{\theta'}(d_t | x_t, y)^{w_2} ,
     \label{eq:sampling}
\end{aligned}
\end{equation}
where ${p}_{\theta'}([x_t, d_t] | y)$ is the original sampling distribution, ${p}_{\theta'}(y | [x_t, d_t])$ estimates the likelihood of the prompt given the joint prediction, and ${p}_{\theta'}(d_t | x_t, y)$ estimates the likelihood of the noisy motion prediction. The latter is aimed at improving the model's motion coherence, as it maximizes the likelihood of the motion representation of the generated video.
Using Bayes' Theorem, Eq.~\ref{eq:sampling} is equivalent to, 
\begin{align*}
     {p}_{\theta'}\left([x_t, d_t] | y\right)\left(\frac{{p}_{\theta'}([x_t, d_t], y)}{{p}_{\theta'}( [x_t, d_t])}\right)^{w_1}\left(\frac{{p}_{\theta'}([x_t, d_t], y)}{{p}_{\theta'} (x_t, y)}\right)^{w_2}\quad \\
     \propto {p}_{\theta'}([x_t, d_t] | y)\left(\frac{{p}_{\theta'}([x_t, d_t] | y)}{{p}_{\theta'}( [x_t, d_t])}\right)^{w_1}\left(\frac{{p}_{\theta'}([x_t, d_t] | y)}{{p}_{\theta'} (x_t | y)}\right)^{w_2},
\end{align*}
where we omit all occurrences of ${p}_{\theta'} (y)$ since $y$ is an external constant input. Next, we can translate this to the corresponding score function by taking the log derivative,
\begin{equation}
 \begin{aligned}    (1+w_1+w_2)\nabla_{\theta'}\log{{p}_{\theta'}([x_t, d_t] | y)} \quad \quad\\
     -w_1\nabla_{\theta'}\log{{p}_{\theta'}([x_t, d_t])} - w_2 \nabla_{\theta'}\log{{p}_{\theta'}(x_t |y)}.
     \label{eq:guidance}
\end{aligned}
\end{equation}

Following \citet{ho2022classifier}, we jointly train the model to be conditional and unconditional on both auxiliary signals, $y, d$ by randomly dropping out the text in $30\%$ of the training steps, and the optical flow in $20\%$ of the steps (setting $d=\textbf{0}$), to facilitate the guidance formulation during inference,
\begin{align*}
    \bold{\Tilde{u}^+}([x_t, d_t], y, t; \theta') = (1+w_1+w_2)\cdot \bold{u^+}([x_t, d_t]), y, t; \theta') \\
     - w_1\cdot \bold{u^+}([x_t, d_t], \emptyset, t; \theta') -w_2 \cdot \bold{u^+}([x_t, \emptyset], y, t; \theta'). \quad
\end{align*}   
Unless stated otherwise, all experiments use $w_1=5, w_2=3$, where $w=5$ is the base model's text guidance scale.