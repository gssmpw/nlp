\section{Related Work}
\label{sec:related}

Diffusion models~\cite{Ho2020DenoisingDP} revolutionized visual content generation. Beginning with image generation~\cite{dhariwal2021diffusion,rombach2021highresolution,ho2022imagen,flux,dai2023emu,dalle3}, editing and personalization~\cite{gal2022image,ruiz2023dreambooth,chefer2024still,emuedit,EVE,chefer2024the}, and more recently video generation.
The first efforts to employ diffusion models for videos relied on model cascades~\cite{ho2022imagenvid,singer2023makeavideo} or direct ``inflation'' of image models using temporal layers~\cite{guo2023animatediff,Lumiere,wu2023tune}. Other works focused on adding an auto-encoder for efficiency~\cite{Blattmann2023AlignYL,an2023latentshift,modelscope}, or conditioning the generation on images~\cite{sdvideo,2023i2vgenxl,xing2023dynamicrafter,emuvideo2023,hong2022cogvideo}. Recently, the UNet backbone was replaced by a Transformer~\cite{moviegen,sora,genmo2024mochi,gupta2023photorealistic,HaCohen2024LTXVideo}, mostly following Diffusion Transformers (DiTs)~\cite{dit}.

To control the generated content, \citet{dhariwal2021diffusion} introduced \emph{Classifier Guidance}, where classifier gradients guide the generation toward a specific class. \citet{ho2022classifier} proposed \emph{Classifier-Free Guidance (CFG)}, replacing classifiers with text. Similar to Inner-Guidance, CFG modifies the sampling distribution. However, CFG does not address noisy conditions or multiple conditions. Closest to our work, \citet{Liu2022CompositionalVG}, handle multiple conditions, $c_1,\dots, c_n$, using a compositional score estimate,
\begin{align*}
    p_\theta(x | c_1, \dots, c_n) = \frac{p_\theta(x,c_1, \dots, c_n)} {p_\theta (c_1,\dots, c_n)} \\ \propto p_\theta(x, c_1, \dots, c_n) = p_\theta(x) \prod_{i=1}^n p_\theta(c_i | x).
\end{align*}
where $\theta$ denotes the model weights and $p$ is the sampling distribution. The above assumes that $c_1, \dots, c_n$ are independent of each other and $\theta$, which does not hold in our case, since the motion is directly predicted by the model and thus inherently depends on $\theta$ and the conditions. Similarly, \citet{brooks2022instructpix2pix} assume independence between the conditions and model weights $\theta$, which is, again, incorrect in our setting. See App.~\ref{sec:IP2P} for further discussion.

The gap between pixel quality and temporal coherence is a prominent issue~\cite{Ruan2024Enhancing,sora,sora_review,physics}. 
Previous works explored motion representations to improve video generation in different contexts. Some methods use them as \emph{input} for guidance or editing~\cite{trajectories,trakectory2,liu2024physgen,cong2023flatten}. Note that their objective differs from ours since we aim to \emph{teach} models a temporal prior rather than taking it as input. Other methods increase the amount of motion by separating content and motion generation~\cite{Ruan2024Enhancing,Qing2023Hierarchical}. Finally, most similar to our approach, recent works use motion representations to improve motion coherence in image-to-video generation~\cite{Shi2024MotionI2V,Wang2024MotiF}, but these are limited to models conditioned on images. 