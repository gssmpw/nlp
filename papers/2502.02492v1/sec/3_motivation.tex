\section{Motivation}
\label{sec:motivation}

During training, generative video models take a noised training video and compute a loss by comparing the model's prediction with the original video, the noise, or a combination of the two~\cite{Ho2020DenoisingDP,flow-matching} (Sec.~\ref{sec:preliminaries}).
We hypothesize that this formulation biases the model towards appearance-based features, such as color and texture, as these dominate pixel-wise differences. Consequently, the model is less inclined to attend to temporal information, such as dynamics or physics, which contribute less to the objective. To demonstrate this claim, we perform experiments to evaluate the sensitivity of the model to temporal incoherence. The following experiments are conducted on DiT-4B~\cite{dit} for efficiency.

\begin{figure}[t!]
    \centering
        \includegraphics[width=0.45\textwidth]{figures/motivation_plot.pdf}
         \vspace{-14px}
    \caption{\textbf{Motivation Experiment.} We compare the model's loss before and after randomly permuting the video frames, using a ``vanilla'' DiT (orange) and our fine-tuned model (blue). The original model is \emph{nearly invariant} to temporal perturbations for $t\leq 60$. }
        \label{fig:motivation-a}
    \vspace{-16px}
    \label{fig:motivation}
\end{figure}

\begin{figure*}[ht!]
\centering
\includegraphics[width=1.01\textwidth]{figures/architecture.pdf}
\vspace{-18px}
\caption{\textbf{VideoJAM Framework.} VideoJAM is constructed of two units; (a) \textbf{Training.} Given an input video $x_1$ and its motion representation $d_1$, both signals are noised and embedded to a \emph{single, joint} latent representation using a linear layer, $\textbf{W}^+_{in}$. The diffusion model processes the input, and two linear projection layers predict both appearance and motion from the joint representation. (b) \textbf{Inference.} We propose \emph{Inner-Guidance}, where the model's own noisy motion prediction is used to guide the video prediction at each step. }
\label{fig:architecture}
\vspace{-6px}
\end{figure*}

We conduct an experiment where two variants of videos are noised and fed to the modelâ€”first, the plain video without intervention, and second, the video after applying a \emph{random permutation} to its frames. 
Assuming the model captures temporal information, we anticipate that the temporally incoherent (perturbed) input will result in a higher measured loss compared to the temporally coherent input.

Given a random set of $35,000$ training videos, we noise each video to a random denoising step $t\in[0,99]$. We then examine the difference in the loss measured before and after the permutation and aggregate the results per timestep. We consider two models-- the ``vanilla'' DiT, which employs a pixel-based objective, and our fine-tuned VideoJAM model, which adds an explicit motion objective (Sec.~\ref{sec:method}). 

The results of this experiment are reported in Fig.~\ref{fig:motivation}.
As can be observed, the original model appears to be \emph{nearly invariant} to frame shuffling until step $60$ of the generation. This implies that the model fails to distinguish between a valid video and a temporally incoherent one. In stark contrast, our model is extremely sensitive to these perturbations, as is indicated by the significant gap in the calculated loss. 

In App.~\ref{sec:motivation_supp} we include a qualitative experiment demonstrating that the steps $t\leq 60$ determine the coarse motion in the video. Both results suggest that the training objective is less sensitive to temporal incoherence, leading models to favor appearance over motion.

