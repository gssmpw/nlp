\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{figures/qualitative.pdf}
\vspace{-26px}
\caption{\textbf{Text-to-video results by VideoJAM-30B.} VideoJAM enables the generation of a wide variety of motion types, from basic motion (e.g., running) to complex motion (e.g., acrobatics), and improved physics (e.g., jumping over a hurdle). }
\label{fig:qualitative}
\vspace{-4px}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{figures/comparisons.pdf}
\vspace{-26px}
\caption{\textbf{Qualitative comparisons} between VideoJAM-30B and the leading baselines- Sora, Kling, and DiT-30B on representative prompts from VideoJAM-bench. The baselines struggle with basic motion, displaying ``backward motion'' (Sora, 2nd row) or unnatural motion (Kling, 2nd row). The generated content defies the basic laws of physics e.g., people passing through objects (DiT, 1st row), or objects that appear or evaporate (Sora, DiT, 4th row). For complex motion, the baselines display static motion or deformations (Sora, Kling, 1st, 3rd row). Conversely, in all cases, VideoJAM produces temporally coherent videos that better adhere to the laws of physics.}
\label{fig:comparisons}
\vspace{-10px}
\end{figure*}
\section{Experiments}
\label{sec:experiments}

We conduct qualitative and quantitative experiments to demonstrate the effectiveness of VideoJAM. We benchmark our models against their base (pre-trained) versions, as well as leading proprietary and open-source video models, to highlight the enhanced motion coherence achieved by our framework.

\paragraph{Implementation Details} 
We consider two variants of the DiT text-to-video model, DiT-4B and DiT-30B, to demonstrate that motion coherence is a common issue for both small and large models. All of our models are trained with a spatial resolution of $256\times 256$ for efficiency. The models are trained to generate $128$ frame videos at 24 frames per second, resulting in 5-second video generations. Both DiT models were pre-trained using the framework in Sec.~\ref{sec:preliminaries} on an internal dataset of $\mathcal{O}(100 \text{ M})$ videos. We then fine-tune the models with VideoJAM using $3$ million random samples from the model's original training set, which constitute less than $3\%$ of the training videos. This allows our fine-tuning phase to be light and efficient. During this fine-tuning, we employ RAFT~\cite{raft} to obtain optical flow.
For more implementation details, see App.~\ref{sec:implementation_details}.

\noindent{\bf Benchmarks\quad}
We use two benchmarks for evaluation. First, we introduce VideoJAM-bench, constructed specifically to test motion coherence. Second, we consider the Movie Gen (MGen) benchmark~\cite{moviegen} to show the robustness of our results.

VideoJAM-bench addresses limitations in existing benchmarks, including MGen, which do not fully evaluate real-world scenarios with challenging motion. 
For example, MGen’s second-largest category, ``unusual activity'' ($23.4\%$ of MGen), contrasts with our objective of evaluating real-world (``usual'') dynamics. The third largest category, ``scenes'' ($19.9\%$ of MGen), focuses on nearly static scenes in nature, thus inherently prioritizes appearance over meaningful motion. Even for categories that overlap with ours such as ``animals'', the representative example given by MGen is ``a curious cat peering out from a cozy hiding spot''.

To construct VideoJAM-bench, we consider prompts from four categories of natural motion that challenge video generators (see Fig.~\ref{fig:failures}): basic motion, complex motion, rotational motion, and physics. We use a holdout set from our training data—on which no model was trained—and employ an LLM to select the top $128$ prompts that best fit at least one of the four categories and describe a single, specific, and clear motion. To avoid biasing the evaluation toward a specific prompt style, we task the LLM with modifying the prompts to be of varying lengths and detail levels. 
A full list of our prompts can be found in App.~\ref{sec:motion_benchmark}. 

\noindent{\bf Baselines\quad} We consider a wide variety of state-of-the-art models, both proprietary and open-source. In the smaller category, we include CogVideo2B, CogVideo5B~\cite{hong2022cogvideo}, PyramidFlow~\cite{Pyramidal_Flow}, and the base model DiT-4B. In the larger category, we evaluate leading open-source models (Mochi~\cite{genmo2024mochi}, CogVideo5B) and proprietary models with external APIs (Sora~\cite{sora}, Kling 1.5~\cite{kling}, RunWay Gen3~\cite{gen3}), along with the base model DiT-30B\footnote{The leading baselines were selected using the \href{https://huggingface.co/spaces/ArtificialAnalysis/Video-Generation-Arena-Leaderboard}{video leadboard}}. 


\noindent{\bf Qualitative experiments\quad}
Figures~\ref{fig:teaser},~\ref{fig:qualitative},~\ref{fig:qualitative_supp} present results obtained using VideoJAM-30B. The results demonstrate a wide variety of motion types that challenge existing models such as gymnastics (e.g., air splits, jumps), prompts that require physics understanding (e.g., fingers pressed into slime, basketball landing in a net), etc.  

Figure~\ref{fig:comparisons} compares VideoJAM with the leading baselines, Sora and Kling, and the base model, DiT-30B, on prompts from VideoJAM-bench. The comparison highlights motion issues in state-of-the-art models. Even simple motions, such as a running giraffe (second row), show problems like ``backward motion'' (Sora) or unnatural movements (Kling, DiT-30B). Complex motions, like pull-ups or headstands, result in static videos (Sora, first and third rows; Kling, first row) or body deformations (Kling, third row). The baselines also exhibit physics violations, such as objects disappearing or appearing (Sora, DiT-30B, fourth row). In contrast, VideoJAM consistently produces coherent motion.



\begin{table}[t!]
\vspace{-8px}
    \caption{\textbf{Comparison of VideoJAM-4B with prior work on VideoJAM-bench.} Human evaluation shows \emph{percentage of votes favoring VideoJAM}; automatic metrics use VBench.}
  \label{tab:4b}
  \centering
  \setlength{\tabcolsep}{3.5pt}
  \scalebox{0.85}{%
  \begin{tabular}{@{}lccccc@{}}
    \toprule
      & \multicolumn{3}{c}{\textbf{Human Eval}} & \multicolumn{2}{c}{\textbf{Auto. Metrics}}  \\
      \cmidrule(r){2-4}
      \cmidrule(r){5-6}
    Method     &      \small{Text Faith.}        &  \small{Quality}     &        \textbf{\small{Motion}}  & \small{Appearance}        &  \textbf{\small{Motion}}\\
    \midrule
    \small{CogVideo2B}   &    84.3      &   94.5     &   {96.1}  &  68.3 & {90.0}  \\
    \small{CogVideo5B}   &    {62.5}       &   {74.7}     &   {68.8 } &  71.9 & \underline{90.1}  \\
    \small{PyramidFlow}  &    76.6      &   83.6     &   {82.8}  &  73.1 & {89.6}  \\
    \midrule
    \small{DiT-4B} &    71.1     &   77.3    &   {82.0}   & \textbf{75.2} & {78.3}  \\
    \textbf{+VideoJAM}  & -  & -    & - & \underline{75.1} &  \textbf{93.7}    \\
    \bottomrule
    \end{tabular}}
    \vspace{-16px}
\end{table}

\begin{table}[t!]
    \caption{\textbf{Comparison of VideoJAM-30B with prior work on VideoJAM-bench.} Human evaluation shows \emph{percentage of votes favoring VideoJAM}; automatic metrics use VBench.}
  \label{tab:30b}
  \centering
    \setlength{\tabcolsep}{3.5pt}
  \scalebox{0.85}{%
  \begin{tabular}{@{}lcccccc@{}}
    \toprule
      & \multicolumn{3}{c}{\textbf{Human Eval}} & \multicolumn{2}{c}{\textbf{Auto. Metrics}}  \\
      \cmidrule(r){2-4}
      \cmidrule(r){5-6}
    Method     &      \small{Text Faith.}        &  \small{Quality}     &        \textbf{\small{Motion}}  & \small{Appearance}        & \textbf{\small{Motion}}   \\
    \midrule
    \small{CogVideo5B}   &    73.4      &   71.9 &   {85.9} & 71.9 &  {90.1} \\
    \small{RunWay Gen3} &    72.2     &   76.6     &   {77.3}  & 73.2  &   \underline{92.0}  \\
    \small{Mochi} &    56.1     &   65.6     &    {74.2}  & 69.9 & {89.7}   \\
    \small{Sora} &    56.3     &   51.7     &   {68.5}  & \underline{75.4} &  {91.7} \\
    \small{Kling 1.5} &    {51.8}     &   {45.9}     &   {63.8}  & \textbf{76.8}  &  {87.1} \\
    \midrule
    \small{DiT-30B} &    71.9     &   74.2     &    {72.7}  & 72.4 &   {88.1}\\
    \textbf{+VideoJAM}  & -  & -    & - & 73.4 & \textbf{92.4}     \\
    \bottomrule
    \end{tabular}}
    \vspace{-14px}
\end{table}
\noindent{\bf Quantitative experiments\quad} We evaluate appearance and motion quality, as well as prompt fidelity using both automatic metrics and human evaluations. In all our comparisons, each model runs \emph{once} with the same random seed for all the benchmark prompts. For the automatic metrics, we use VBench~\cite{huang2023vbench}, which assesses video generators across disentangled axes. We aggregate the scores into two categories- appearance and motion, following the paper. The metrics evaluate the per-frame quality, aesthetics, subject consistency, the amount of generated motion, and motion coherence. More details on the metrics and their aggregation can be found in App.~\ref{sec:vbench}.

For the human evaluations, we follow the Two-alternative Forced Choice (2AFC) protocol, similar to~\citet{rombach2021highresolution,sdvideo}, where raters compare two videos (one from VideoJAM, one from a baseline) and select the best one based on quality, motion, and text alignment. Each comparison is rated by $5$ unique users, providing at least $640$ responses per baseline for each benchmark.

The results of the comparison on VideoJAM-bench for the 4B, 30B models are presented in Tabs.~\ref{tab:4b},~\ref{tab:30b}, respectively. Additionally, a full breakdown of the automatic metrics is presented in App.~\ref{sec:motion_benchmark}. The results of the comparison on the Movie Gen benchmark are presented in App.~\ref{sec:moviegen_benchmark}. In all cases, VideoJAM outperforms all baselines in all model sizes in terms of motion coherence, across both the automatic and human evaluations by a sizable margin (Tabs.~\ref{tab:4b},~\ref{tab:30b},~\ref{tab:moviegen}). 

Notably, VideoJAM-4B outperforms the CogVideo5B baseline, even though the latter is $25\%$ larger. For the 30B variant, VideoJAM surpasses even proprietary state-of-the-art models such as Kling, Sora and Gen3 ($63.8\%, 68.5\%, 77.3\%$ preference in motion, respectively).
These results are particularly impressive given that VideoJAM was trained at a significantly lower resolution ($256$) compared to the baselines ($768$ and higher) and fine-tuned on only $3$ million samples. While this resolution disparity explains why proprietary models like Kling and Sora surpass ours in visual quality (Tab.~\ref{tab:30b}), VideoJAM consistently demonstrates substantially better motion coherence.

Most critically, VideoJAM significantly improves motion coherence in its base models, DiT-4B and DiT-30B, in a direct apples-to-apples comparison. Human raters preferred VideoJAM's motion in $82.0\%$ of cases for DiT-4B and $72.7\%$ for DiT-30B. Raters also favored VideoJAM in quality ($77.3\%, 74.2\%$ in 4B, 30B) and text faithfulness ($71.1\%, 71.9\%$ in 4B, 30B), indicating that our approach also enhances other aspects of the generation.

\begin{table}[t!]
\vspace{-6px}
    \caption{\textbf{Ablation study.} Ablations of the primary components of our framework on VideoJAM-4B using VideoJAM-bench. Human evaluation shows percentage of votes favoring VideoJAM.}
  \label{tab:ablations}
  \centering
    \setlength{\tabcolsep}{1.5pt}
  \scalebox{0.82}{%
  \begin{tabular}{@{}lccccc@{}}
    \toprule
      & \multicolumn{3}{c}{\textbf{Human Eval}} & \multicolumn{2}{c}{\textbf{Auto. Metrics}}  \\
      \cmidrule(r){2-4}
      \cmidrule(r){5-6}
     Ablation type     &      \small{Text Faith.}        &  \small{Quality}     &        \textbf{\small{Motion}}  &  \small{Appearance}     &        \textbf{\small{Motion}}\\
    \midrule
    \small{w/o text guidance} &    68.0     &   62.5     &   {63.3}  & 74.5 & \underline{93.3} \\
     \small{w/o Inner-Guidance} &    68.9     &   64.4     &    {66.2}  & \textbf{75.3} & {93.1}  \\
     \small{w/o optical flow } &    79.0     &   70.4     &   {80.2}  & 74.7 & {90.1}  \\
     \small{IP2P guidance} &    73.7     &   85.2     &   {78.1}  & 72.0 & {90.4} \\ 
    \midrule
     \small{\textbf{+VideoJAM-4B}}   &    -      &   -     &   - & \underline{74.9} & \textbf{93.7}   \\
    \bottomrule
    \end{tabular}}
    \vspace{-16px}
\end{table}

\noindent{\bf Ablations\quad} We ablate the primary design choices of our framework. First, we ablate the use of text guidance and motion guidance in our inner guidance formulation (by setting $w_2=0$, $w_1=0$ in Eq.~\ref{eq:guidance}, respectively). Next, we ablate the use of motion prediction during inference altogether, by dropping the optical flow at each inference step ($d=\textbf{0}$). Finally, we ablate our guidance formulation by replacing it with the InstructPix2Pix (IP2P) guidance~\cite{brooks2022instructpix2pix} (see Sec.~\ref{sec:related}, App.~\ref{sec:IP2P}). Note that the results of the DiT models in Tabs.~\ref{tab:4b},~\ref{tab:30b} also function as ablations, as they ablate the use of VideoJAM during training and inference.

The results are reported in Tab.~\ref{tab:ablations}. All ablations cause significant degradation in motion coherence, where the removal of motion guidance is more harmful than the removal of the text guidance, indicating that the motion guidance component indeed steers the model toward temporally coherent generations. Furthermore, dropping the optical flow prediction at inference is the most harmful, substantiating the benefits of the joint output structure to enforce plausible motion. The InstructPix2Pix guidance comparison is further indication that our Inner-Guidance formulation is most suited to our framework, as it gives the second lowest result in terms of motion. 

Finally, note that human evaluators consistently prefer VideoJAM in terms of visual quality and text alignment over all the ablations, further establishing that VideoJAM benefits all aspects of video generation.

\begin{figure}[t!]
\centering
\includegraphics[width=0.49\textwidth]
{figures/limitations.pdf}
\vspace{-26px}
\caption{\textbf{Limitations.} Our method is less effective for: (a) motion observed in ``zoom-out'' (the moving object covers a small part of the frame). (b) Complex physics of object interactions.}
\label{fig:limitations}
\vspace{-14px}
\end{figure}

\noindent{\bf Limitations\quad} While VideoJAM significantly improves temporal coherence, challenges remain (see Fig.~\ref{fig:limitations}). First, due to computational constraints, we rely on both limited training resolution and RGB motion representation, which hinder the model’s ability to capture motion in ``zoomed-out'' scenarios where moving objects occupy a small portion of the frame. In these cases, the relative motion magnitude is reduced, making the representation less informative (Eq.~\ref{eq:optical_flow_normalization}). For example, in Fig.~\ref{fig:limitations}(a), no parachute is deployed, and the motion appears incoherent. Second, while motion and physics are intertwined, leading to improved physics, our motion representation lacks explicit physics encoding. This limits the model’s ability to handle complex physics of object interactions. For example, in Fig.~\ref{fig:limitations}(b), the player's foot does not touch the ball before it changes trajectory.