\section{Introduction}\label{sec:Intro} 


Novel view synthesis offers a fundamental approach to visualizing complex scenes by generating new perspectives from existing imagery. 
This has many potential applications, including virtual reality, movie production and architectural visualization \cite{Tewari2022NeuRendSTAR}. 
An emerging alternative to the common RGB sensors are event cameras, which are  
 bio-inspired visual sensors recording events, i.e.~asynchronous per-pixel signals of changes in brightness or color intensity. 

Event streams have very high temporal resolution and are inherently sparse, as they only happen when changes in the scene are observed. 
Due to their working principle, event cameras bring several advantages, especially in challenging cases: they excel at handling high-speed motions 
and have a substantially higher dynamic range of the supported signal measurements than conventional RGB cameras. 
Moreover, they have lower power consumption and require varied storage volumes for captured data that are often smaller than those required for synchronous RGB cameras \cite{Millerdurai_3DV2024, Gallego2022}. 

The ability to handle high-speed motions is crucial in static scenes as well,  particularly with handheld moving cameras, as it helps avoid the common problem of motion blur. It is, therefore, not surprising that event-based novel view synthesis has gained attention, although color values are not directly observed.
Notably, because of the substantial difference between the formats, RGB- and event-based approaches require fundamentally different design choices. %

The first solutions to event-based novel view synthesis introduced in the literature demonstrate promising results \cite{eventnerf, enerf} and outperform non-event-based alternatives for novel view synthesis in many challenging scenarios. 
Among them, EventNeRF \cite{eventnerf} enables novel-view synthesis in the RGB space by assuming events associated with three color channels as inputs. 
Due to its NeRF-based architecture \cite{nerf}, it can handle single objects with complete observations from roughly equal distances to the camera. 
It furthermore has limitations in training and rendering speed: 
the MLP used to represent the scene requires long training time and can only handle very limited scene extents or otherwise rendering quality will deteriorate. 
Hence, the quality of synthesized novel views will degrade for larger scenes. %

We present Event-3DGS (E-3DGS), i.e.,~a new method for novel-view synthesis from event streams using 3D Gaussians~\cite{3dgs} 
demonstrating fast reconstruction and rendering as well as handling of unbounded scenes. 
The technical contributions of this paper are as follows: 
\begin{itemize}
\item With E-3DGS, we introduce the first approach for novel view synthesis from a color event camera that combines 3D Gaussians with event-based supervision. 
\item We present frustum-based initialization, adaptive event windows, isotropic 3D Gaussian regularization and 3D camera pose refinement, and demonstrate that high-quality results can be obtained. %

\item Finally, we introduce new synthetic and real event datasets for large scenes to the community to study novel view synthesis in this new problem setting. 
\end{itemize}
Our experiments demonstrate systematically superior results compared to EventNeRF \cite{eventnerf} and other baselines. 
The source code and dataset of E-3DGS are released\footnote{\url{https://4dqv.mpi-inf.mpg.de/E3DGS/}}. 




