\clearpage
\maketitlesupplementary
\setcounter{figure}{0}
\renewcommand{\thefigure}{\Roman{figure}}
\setcounter{section}{0}
\renewcommand{\thesection}{\Roman{section}}
\vspace{15pt}

This supplement provides additional details and insights into the methods and experiments discussed in the main paper.
In Sec.~\ref{sec:supp_frustum}, we elaborate on our frustum-based initialization, explaining the sampling strategy and how it ensures effective Gaussian placement in the scene.
Sec.~\ref{sec:supp_pose_refinement} provides further details on our pose refinement, specifically the use of Gram-Schmidt orthogonalization to maintain valid transformations during optimization. 
In Sec.~\ref{sec:supp_dataset_perturb}, we analyze the camera pose noise in the E-3DGS-Real dataset and describe the process we use to simulate realistic pose perturbations for the E-3DGS-Synthetic-Hard dataset.
Sec.~\ref{sec:supp_implementation_details} outlines the implementation details, including adjustments to the original 3DGS training schedule to improve convergence.
Sec.~\ref{sec:supp_evaluation} covers our evaluation, highlighting the measures we take to ensure reliable results, particularly for the ablation studies. 
Finally, we present a comprehensive comparison in Sec.~\ref{sec:additional_comparisons}  showcasing additional visual results and ablation studies on the E-3DGS-Real, E-3DGS-Synthetic, and E-3DGS-Synthetic-Hard datasets. 
These experiments expand on the results from the main paper and further demonstrate the effectiveness of our method across different scenarios. 

\section{Frustum-Based Initialization}
\label{sec:supp_frustum}

As described in Sec.~\ref{sec:frustum_init} of the main paper, our approach involves initializing a fixed number of Gaussians, denoted as $N_g$. If we have $N_t$ camera poses, we distribute the Gaussians across these poses, resulting in $N_g / N_t$ Gaussians being initialized for each pose.
The initialization process begins by sampling points within the camera frustum in normalized device coordinates (NDC). However, instead of uniformly sampling all three coordinates $(x, y, z)$ in NDC, we adopt a different strategy for depth (z-axis).

We observe that when depth was sampled directly in NDC, most Gaussians would cluster very close to the near plane ($z_\text{near}$), leading to poor scene coverage. To address this, we sample the depth uniformly in camera coordinates between $z_\mathrm{near}$ and $z_\mathrm{far}$. This ensures a more even distribution of Gaussians across the entire depth range.

Once the depth is sampled in camera coordinates, it is converted into NDC. Next, the $x$ and $y$ coordinates are sampled uniformly in NDC. With $x$, $y$, and $z$ values now in NDC, we un-project them back into the world coordinates. This conversion gives us the final positions for the Gaussians in the 3D scene.
Next, the entire process is repeated for each camera frustum associated with the given poses $P_t$, ensuring a comprehensive initialization across all views. Therefore, the distribution of Gaussians is effectively tied to the observable scene regions. 

\section{Pose Refinement and Gram-Schmidt Orthogonalization}
\label{sec:supp_pose_refinement}

In Sec.~\ref{sec:pose_refinement} of the main paper, we introduce our approach to pose refinement, where the refined pose $P'_t$ is modeled as $P'_t = P^e_t P_t$, with $P^e_t$ being an error correction transform. Rather than directly optimizing $P^e_t$ as a $3{\times}3$ matrix, we represent it using two rotation vectors $r_1$ and $r_2$ and a translation vector $T$, following the method of Hempel et al.~\cite{6d_rotation}. This representation allows us to ensure that $P^e_t$ remains a valid transformation matrix during optimization. 

To maintain the orthogonality of the rotation matrix, we apply Gram-Schmidt orthogonalization to $r_1$ and $r_2$ to compute the final rotation matrix $R = [r'_1, r'_2, r'_3]$. The process is as follows:


\allowdisplaybreaks
\begin{equation}
\begin{aligned}
    r'_1 &= \frac{r_1}{\|r_1\|}, \\
    r'_2 &= \frac{r_2 - (r'_1 \cdot r_2)r'_1}{\|r_2 - (r'_1 \cdot r_2)r'_1\|},\\
    r'_3 &= r'_1 \times r'_2, \,\text{and}\\
    P^e_t &= 
    \begin{bmatrix}
    | & | & | & |\\
    r'_1 & r'_2 & r'_3 & T\\
    | & | & | & |\\
    0 & 0 & 0 & 1\\
    \end{bmatrix}.    
\end{aligned}
\end{equation}


Here, $r'_1$ is the normalized version of $r_1$, and $r'_2$ is obtained by subtracting the projection of $r_2$ onto $r'_1$ and normalizing the result. The third vector $r'_3$ is calculated as the cross product of $r'_1$ and $r'_2$, ensuring that the resulting rotation matrix is orthogonal. The final error correction matrix $P^e_t$ is then constructed using these orthogonal vectors and the translation vector $T$.

This approach guarantees that the pose refinement remains valid throughout the optimization process, contributing to the stability and accuracy of our method.


\begin{figure*}[!ht]
\centering
\begin{subfigure}[b]{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{fig/angles.png}
    \caption{Rotation errors for both E-3DGS-Real and E-3DGS-Synthetic-Hard show a similar error distribution.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{fig/pos.png}
    \caption{Larger translation errors are applied to E-3DGS-Synthetic-Hard, compared to those in E-3DGS-Real, to account for the larger scene size and ensure a sufficiently challenging difficulty level for meaningful ablation studies.}
\end{subfigure}
\caption{Comparison of estimated pose errors in the E-3DGS-Real dataset versus the synthetically introduced errors in the E-3DGS-Synthetic-Hard dataset. The synthetic perturbations are generated using an Ornstein–Uhlenbeck process to match the time-correlated nature and variance of the real data.}
\label{fig:noise}
\end{figure*}


\section{Pose Perturbation in E-3DGS-Synthetic-Hard}
\label{sec:supp_dataset_perturb}

As described in Sec.~\ref{sec:datasets} of the main paper, we provide the E-3DGS-Synthetic-Hard dataset that differs from E-3DGS-Synthetic in two aspects: 1) The camera speed is highly varied and 2) the camera extrinsics exhibit noise similar in characteristics to the noise observed in the real data. 
To quantify the camera pose noise in the E-3DGS-Real dataset, we compare the refined training camera trajectories with the initial trajectories. Our analysis reveals that these errors are time-correlated. Based on this observation and by examining the scale of these errors, we introduce synthetic perturbations in the E-3DGS-Synthetic dataset using a random walk with decay, specifically the Ornstein–Uhlenbeck process~\cite{pavliotis2014stochastic}, which ensures the perturbations have zero mean while remaining time-correlated. 

We calibrate the variance of the synthetic perturbations to match the rotation errors observed in the real data. For translation, we apply a higher level of perturbation, given that the synthetic scenes are significantly larger in scale than the real data. This adjustment ensures that translation errors are proportionally scaled, creating a comparable difficulty level for the ablation studies. The noise patterns are illustrated in Fig.~\ref{fig:noise}.





\section{Implementation Details}
\label{sec:supp_implementation_details}

Our codebase is based on 3DGS~\cite{3dgs}. We train the method for $\qty{6d4}{}$ instead of $\qty{3d4}{iterations}$, allowing the pose refinement to converge. 
The original paper performs both, densification and opacity resets of the Gaussians until $\qty{1.5d4}{iterations}$.
In our case, we perform opacity resets until $\num{3d4}$ and densification until $\qty{5d4}{iterations}$. 
From our analysis---while opacity resets are important to remove floaters---they also hamper the reconstruction quality. 
Therefore, once the scene is reasonably converged, we stop resetting opacity and only densify the scene to get better reconstruction. 


Furthermore, 3DGS uses the fixed threshold value $\num{2d-4}$ to decide whether a Gaussian should be split up during the densification.
We start the optimization with the same value,  however, we linearly decrease it to $\num{4d-5}$ over ${\qty{4d4}{iterations}}$.
First, this allows our method to refine the poses with larger Gaussians, providing more support, and second, reduce the threshold in later stages to obtain a more detailed reconstruction.
We initialize $N_g={\qty{5d4}{}}$ Gaussians in all our trainings. 





In the experiments with pose refinement, we restrict the number of spherical harmonics to one, as it allows for better pose refinement \cite{3dgsslam, splatam}. 
For the experiments with perfect poses, we follow the original 3DGS approach and use three spherical harmonics. 
In all experiments, except those conducted with the EventNeRF dataset \cite{eventnerf}, we consistently use $N_\text{max}{=}\num{e6}$ events for the window size. 
As sequences of the latter are very short and do not contain enough events for such large windows, we use $N_\text{max}{=}\num{e5}$ for them. 
Training the full method takes one to two hours with a single NVIDIA GeForce RTX 3090, depending on the scene size. 

\input{fig/_supp_real}
\input{fig/_supp_synthetic}
\input{fig/_supp_synthetic_hard}

\section{Further Evaluation Details (Ablations)}
\label{sec:supp_evaluation}

To ensure the reliability of the results, all ablation studies are conducted four times, with evaluation metrics averaged to provide more accurate insights and minimize the effects of coincidence. 
For the E-3DGS-Synthetic-Hard dataset, where the camera poses are perturbed, direct evaluation is not feasible due to slight misalignments between the learned 3D scene and the ground truth. 
To correct this, we first freeze the Gaussians and then refine the test poses with a small learning rate to ensure proper convergence. 
This alignment process allows the test views to match the ground truth accurately, enabling precise evaluation. 

\section{Additional Comparisons and Ablations}
\label{sec:additional_comparisons}

In this section, we expand on the main paper experiments by showing additional results on E-3DGS-Real, E-3DGS-Synthetic, and E-3DGS-Synthetic-Hard datasets.
Fig.~\ref{fig:supp_real_data} demonstrates the performance of E-3DGS in comparison to Deblur-GS~\cite{deblurgs}, E2VID~\cite{e2vid}+3DGS~\cite{3dgs} and EventNeRF~\cite{eventnerf} on the E-3DGS-Real dataset.
These baselines exhibit severe artifacts such as blur, floaters and noise.
In the same figure, we also demonstrate the impact of the key components of our method.
Removing $L_\text{iso}$ leads to increased amounts of floaters and other artifacts.
As the captured camera poses contain noise, pose refinement (PR) is crucial to achieve accurate results.
Hence, without it, the model cannot produce accurate predictions, resulting in severe artifacts and blurriness.
However, the model without the adaptive windows (AW) shows similar performance to the full model.
That is likely due to the overall uniformity of the camera speeds in the used dataset, which diminishes the potential impact of adaptive event windows.


In Fig.~\ref{fig:supp_synthetic}, we compare E-3DGS against EventNeRF~\cite{eventnerf} and E2VID~\cite{e2vid}+3DGS~\cite{3dgs} on E-3DGS-Synthetic dataset.
Both baselines perform poorly: While E2VID+3DGS captures the edges and the general structure, it struggles with color representation, and EventNeRF reconstruction is much noisier and blurrier compared to our method.
In contrast, our E-3DGS outperforms them, showing clear and sharp novel views with accurate color representation.
Some issues are still observable but are mostly in less supervised areas, \eg,~on the roof in ScienceLab or Subway scenes.


Lastly, Fig.~\ref{fig:supp_synthetic_hard} visualizes results of the ablation study on the E-3DGS-Synthetic-Hard dataset. 
In comparison to E-3DGS-Synthetic, this dataset has artificially added camera extrinsics noise, which we describe in Sec~\ref{sec:supp_dataset_perturb}, and drastically increased camera speed variation (Sec.~\ref{sec:datasets}). 
While these changes make obtaining high reconstruction quality more difficult, our full method still works well, outperforming all ablated models. 
As on the E-3DGS-Real, removing $L_\text{iso}$ results in severe artifacts (e.g.,~in the first view of Company or in the second view of Subway). 
E-3DGS-Synthetic-Hard dataset has camera pose noise, and, hence, using pose refinement (PR) is important, as removing it results in blurriness and artifacts. 
Removing the adaptive event windows (AW) leads to deterioration; e.g.,~the method without AW exhibits artifacts on the sofa in the first view of the Company sequence that are absent in the results of the full method. 
It is also noteworthy that while all ablated models struggle with the second view of the Subway sequence, the full method, nevertheless,  achieves a better result: The structure is clearer and more recognizable with fewer artifacts. 
