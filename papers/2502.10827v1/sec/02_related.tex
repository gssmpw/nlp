\section{Related Work}\label{sec:Related_Work}

\subsection{Novel View Synthesis from RGB Inputs}\label{ssec:RW_RGB}
Novel view synthesis of rigid scenes is predominantly handled assuming RGB inputs. A widely used approach to this problem is to learn coordinate-based neural scene representations allowing rendering novel views at test time. 
Earlier works such as Neural Radiance Fields (NeRF) and its direct follow-ups \cite{nerf, Tewari2022NeuRendSTAR} used implicit neural representations in combination with volume rendering.
They are based on expensive-to-optimize Multi-Layer Perceptrons (MLPs) and are slow at training and evaluation while requiring a relatively low amount of storage space once they are trained.
Their stochastic ray sampling requires many samples to obtain an accurate scene approximation, and shooting rays through empty space constitutes unnecessary overhead. Most of these approaches focus on single objects or bounded scenes. 
Recent techniques accelerate neural MLP-based representations or ray sampling \cite{Reiser2021ICCV, instantngp} or avoid MLPs \cite{Sun2022DirectVG, YuFridovichKeil2022, zipnerf} by using voxel grids. 
Some techniques~\cite{neurbf} support unbounded scenes by employing radial basis functions, thereby overcoming the limitations of voxel-grid-based methods. 
Several ray tracing-based methods support large-scale scenes and uncontrolled camera trajectories thanks to progressive NeRF optimization \cite{xiangli2022bungeenerf, meuleman2023localrf}.
Instant-NGPs \cite{instantngp} are neural feature volumes with a hash grid that can be learned and evaluated quickly at test time. They can also handle multi-scale training scenarios efficiently. 


A promising recent development is the shift from ray tracing to rasterization, marked by the introduction of 3D Gaussian Splatting (3DGS) \cite{3dgs}.
This approach presents an alternative paradigm for 3D reconstruction and novel view synthesis using differentiable rasterization with 3D Gaussians as geometric primitives.
Since GPU technology and algorithmic research have evolved over several decades to provide high performance for rasterization applications,
3DGS trains substantially quicker and provides much higher rendering throughput than NeRF. Moreover, since it explicitly represents the geometry, it can scale easily as the scene size increases with no special handling required for unbounded scenes. Our approach adopts the 3D Gaussian representation and presents its application to the supervision from event streams. It inherits thereby the advantages of event streams and 3DGS for view synthesis. 

\subsection{Novel View Synthesis from Event Streams}
Event-aided sparse odometry and simultaneous localization and mapping approaches are distantly related to our setting, as they do not allow photo-realistic and dense rendering of novel views \cite{Kim2016, Rebecq2017EVOAG, Hidalgo-Carrio_2022_CVPR, Klenk2024DeepEV}.

As previously discussed, event cameras represent an alternative to RGB sensors for dense novel view synthesis, and some initial work was done on learning 3D scene representations from event streams only. EventNeRF \cite{eventnerf} is a seminal framework for training MLP-based implicit 3D representations (see Sec.~\ref{ssec:RW_RGB}) using frames of accumulated color events.
While it demonstrates impressive results, it is restricted to camera trajectories with uniform motion and the assumption that the background is a constant color (triggering no events).
E-NeRF \cite{enerf} is another work that resembles the training methodology of EventNeRF for single-channel (intensity) event cameras and allows training a colored 3D representation from a combination of blurry RGB images and grayscale events. Robust E-NeRF by Low and Lee \cite{robust_enerf} is a model aiming to reduce the issues caused by uncontrolled camera motion. They introduce the refractory period to the event generation model, i.e.~the time during which a pixel is inactive after an event firing. Supervision happens on the level of individual events, and they reformulate the event loss to handle intra-pixel variances of the contrast threshold optimized during training. All these methods adopt ray tracing and can be primarily applied on 360\textdegree~object-centric  datasets or front-facing trajectories.

Our approach differs from previous event-based methods in that it demonstrates that rasterization can be efficiently combined with event-based supervision instead of ray tracing.
The main design choices of our method are tailored to 3D Gaussians. As a result, our method inherits the primary advantages of 3DGS \cite{3dgs}, such as fast training and inference. Similar to EventNeRF \cite{eventnerf}, our method supports color. However, in contrast, it is not limited to single objects and can handle large-scale scenes. 














