\section{The E-3DGS Method}\label{sec:Method} 
Our aim is to learn a 3D representation of a static scene using only a color event stream, where each pixel observes changes in brightness corresponding to one of the red, green, or blue channels according to a Bayer pattern, with known camera intrinsics $K_t~\in~\mathbb{R}^{3 \times 3}$, and noisy initial poses~$P_t~\in~\mathbb{R}^{3 \times 4}$, at reasonably high-frequency time steps indexed by $t$. 
Following 3DGS~\cite{3dgs}, we represent our scene by anisotropic 3D Gaussians. Our methodology comprises a technique to initialize Gaussians in the absence of a Structure from Motion (SfM) point cloud, adaptive event frame supervision of 3DGS, and a pose refinement module. 
An overview of our method is provided in Fig.~\ref{fig:methodology}.


Our E-3DGS method is not restricted to scenes of a certain size and can handle unbounded environments. It does not rely on any assumptions regarding the background color, type of camera motion, or speed. Thus, it ensures robust performance across a wide range of scenarios. 

\subsection{Event Stream Supervision} 

There are two main categories of approaches to learning 3D scene representations from event streams. 
Some apply the loss to single events~\cite{robust_enerf} based on Eq.~\eqref{eq:egm}. Others use the sum of events~\( E_{\x}(t_1,t_2) \) from Eq.~\eqref{eq:egm_sum}. We choose the second approach, as rasterization in 3DGS is well suited to efficiently render entire images rather than individual pixels. 

To optimize our Gaussian scene representation using event data, we can make a logical equivalence between the observed event stream and the scene renderings. 
To do so, we replace the true logarithmic intensities~\( L_{\x} \) in Eq.~\eqref{eq:egm_sum} with the rendered logarithmic intensities~\(\hat{L}_{\x} \) from our scene, and the times $\tau$ with the camera poses $P_t$ that were used to render the scene at the respective time steps. 
Following the approach used in~\cite{eventnerf}, the log difference is then point-wise multiplied with a Bayer filter $F$ to obtain the respective color channel. We can finally calculate the error between the logarithmic change from our model and the actual change observed from the event stream, and define the following per-pixel loss: 
\begin{equation}
    \begin{split}
    &\mathcal{L}_{\x}\left(t_1, t_2\right) = \\
    &\left\| 
    F \odot (\hat{L}_{\x}(P_{t_2}) - \hat{L}_{\x}(P_{t_1})) 
    - F \odot E_{\x}\left(t_1, t_2\right)\right\|_1, 
    \end{split}
    \label{eq:L_recon_per_pixel} 
\end{equation}
where ``$\odot$'' denotes pixelwise multiplication. 


\subsection{Frustum-Based Initialization}
\label{sec:frustum_init}

In the original 3DGS \cite{3dgs}, the Gaussians are initialized using a point cloud obtained from applying SfM on the input images. 
The authors also experimented with initializing the Gaussians at random locations within a cube. While this worked for them with a slight performance drop, it requires an assumption about the extent of the scene. 

Applying SfM directly to event streams is more challenging than RGB inputs \cite{Kim2016} and exploring this aspect is not the primary focus of this paper. 
In the absence of an SfM point cloud, we use the randomly initialized Gaussians and extend this approach to unbounded scenes. 
To this end, we initialize a specified number of Gaussians (on the order of \qty{d4}{}) in the frustum of each camera. 
This gives two benefits: 1) All the initialized Gaussians are within the observable area, and 2) We only need one loose assumption about the scene, which is the maximum depth $z_\mathrm{far}$. 


\subsection{Adaptive Event Window}\label{subsec:adaptive_window}

Rudnev et al.~\cite{eventnerf} demonstrated in EventNeRF that using a fixed event window duration results in suboptimal reconstruction. They find that larger windows are essential for capturing low-frequency color and structure, and smaller ones are essential for optimization of finer high-frequency details. While they randomly sampled the event window duration, a drawback is that it does not consider the camera speed and event rate, thus the sampled windows may contain too many or too few events.  
As our dataset features variable camera speeds, we improve upon this by sampling the number of events rather than the window duration.  
To achieve this, for each time step we randomly sample a target number of events from within the range $[N_\mathrm{min}, N_\mathrm{max}]$. 
Given a time step~$t$, we search for a previous time step~$t_s$ such that the number of events in the event frame $E(t_s, t)$ is approximately equal to the desired number. 

When determining $N_\mathrm{max}$, we find that for values where details and low-frequency structure are optimal, 3DGS tends to get unstable and sometimes prunes away Gaussians in homogeneous areas.
While this can be mitigated by choosing a much larger $N_\mathrm{max}$, this again deteriorates the details. 
Therefore, we propose a strategy to incorporate both, small and large windows. For each $t$, we choose two earlier time steps~$t_{s_1}$ and~$t_{s_2}$. The ranges for sampling the event counts for both are empirically chosen to be $[\frac{N_\mathrm{max}}{10}, N_\mathrm{max}]$ and $[\frac{N_{max}}{300}, \frac{N_\mathrm{max}}{30}]$. We then render frames from our model at times $t$, $t_{s_1}$ and $t_{s_2}$, and use two concurrent losses for the event windows $E_{\x}\left(t_{s_1}, t\right)$ and $E_{\x}\left(t_{s_2}, t\right)$. 

\subsection{As-Isotropic-As-Possible Regularization} 
\label{ssec:IsotropicReg} 

In 3DGS, Gaussians are unconstrained in the direction perpendicular to the image plane. 
This lack of constraint can result in elongated and overfitted Gaussians. 
And while they may appear correct from the training views, they introduce significant artifacts when rendered from novel views by manifesting as floaters and distortions of object surfaces. 
We also observe that the lack of multi-view consistency and tendency to overfit destabilize the pose refinement. 

To mitigate these issues, we draw inspiration from Gaussian Splatting SLAM~\cite{3dgsslam} and SplaTAM~\cite{splatam}, and apply isotropic regularization:
\begin{equation}
    \mathcal{L}_{\text{iso}} = \frac{1}{|\mathcal{G}|} \sum_{g \in \mathcal{G}} \left\| S_g - \bar{S}_g \right\|_1
    \label{eq:L_iso} \mathrm{\,,}
\end{equation}
where~$\mathcal{G}$ is the set of Gaussians visible in the image. Eq.~\eqref{eq:L_iso} imposes a soft constraint on the Gaussians to be as isotropic as possible.
We find that it helps to improve pose refinement, minimizes floaters and enhances generalizability. 

\subsection{Pose Refinement} 
\label{sec:pose_refinement}

To obtain the most accurate results, we allow the poses to be refined during optimization
by modeling the refined pose as $P'_t = P^e_t P_t$, where  $P^e_t$ is an error correction transform. 
Instead of directly optimizing~$P^e_t$ as a~$3 \times 3$ matrix, following Hempel et al.~\cite{6d_rotation} we represent it as $[r_1\,\, r_2\,\, T]$, where $r_1$ and $r_2$ represent two rotation vectors of the rotation matrix~$R = [r_1\,\, r_2\,\, r_3]$, while $T$ is the translation.
We can then obtain the~$P^e_t$ matrix from the representation using Gram-Schmidt orthogonalization (see details in Supplement~\ref{sec:supp_pose_refinement}), hence ensuring that during optimization, our error correction transform always represents a valid transformation matrix. 
$P^e_t$ is initialized to be the identity transform. Since the loss function from Eq.~\eqref{eq:L_recon_per_pixel} depends on the camera pose as well, it allows us to use the same loss to backpropagate and obtain gradients for pose refinement. 

As our goal is to refine the estimated noisy poses rather than perform SLAM, this training signal is sufficient for our needs. Moreover, we observe that poses tend to diverge with 3DGS due to the periodic opacity reset.
To combat this, we impose a soft constraint with an additional pose regularization, that encourages the matrices~$P^e_t$ to stay close to the identity matrix $I$:
\begin{equation}
    \mathcal{L}_{\text{pose}} = \| P^e_{t_{s_1}} - I \|_2 + \| P^e_{t_{s_2}} - I \|_2 + \| P^e_{t} - I \|_2
    \label{eq:L_pose} \mathrm{\,,}
\end{equation}
with all terms weighted equally. 


\subsection{Optimization}
\label{ssec:Optimization} 

Eq.~\eqref{eq:L_recon_per_pixel} defines the reconstruction loss per pixel for a single event frame. However, naively averaging these per-pixel losses over whole images leads to problems. For small event windows, most pixels have no events, which are not very informative but will then make up the majority of the loss. 
To address this, we compute separate averages of the losses for pixels with events~$\mathcal{X}_\text{evs}$ and pixels without events~$\mathcal{X}_\text{noevs}$. 
These averages are then scaled by the hyperparameter~$\alpha=0.3$ to obtain the complete weighted reconstruction loss:
\begin{equation}
    \begin{split}
        \mathcal{L}_{\text{recon}}\left(t_s, t\right) = \,\,&
        \frac{\alpha}{|\mathcal{X}_{\text{noevs}}|} \cdot 
        \left(\sum_{\x\in \mathcal{X}_{\text{noevs}}} \mathcal{L}_{\x}\left(t_s, t\right)\right) + \\
        + \,\,& \,\, \frac{1 - \alpha}{|\mathcal{X}_{\text{evs}}|} \,\,\, \cdot 
        \left(\sum_{\x\in \mathcal{X}_{\text{evs}}} \mathcal{L}_{\x}\left(t_s, t\right)\right). 
    \end{split}
    \label{eq:L_recon}
\end{equation}
To obtain the final loss, we take a weighted sum of the reconstruction losses for the two event windows from Sec.~\ref{subsec:adaptive_window} along with the isotropic and pose regularization: 
\begin{equation}
    \begin{split}
        \mathcal{L} =\,\,\,\, & 
        \lambda_1 \mathcal{L}_{\text{recon}}\left(t_{s_1}, t\right) \,\,+  \,\,
        \lambda_2 \mathcal{L}_{\text{recon}}\left(t_{s_2}, t\right)  \\&
        +\,\, \lambda_\text{iso} \mathcal{L}_{\text{iso}} \,\,+ \,\,
        \lambda_\text{pose} \mathcal{L}_{\text{pose}}
    \end{split}
    \label{eq:loss} \mathrm{\,,}
\end{equation}
where $\lambda_1$, $\lambda_2$ and $\lambda_{\text{iso}}$ are hyper-parameters. In our experiments, we use  $\lambda_1=\lambda_2=0.65$, and $\lambda_{\text{iso}}$ is set to $10$ initially and reduced to $1$ after $\qty{d4}{}$ iterations. 





