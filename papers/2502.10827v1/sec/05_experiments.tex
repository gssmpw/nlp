\section{Experimental Evaluation}\label{sec:Experiments}

\subsection{Implementation details}

We provide the full implementation details in the supplemental material. Running our method on a scene takes one to two hours (depending on the scene size) with a single NVIDIA GeForce RTX 3090. 

\subsection{Datasets}
\label{sec:datasets}
We next describe the new event datasets we provide to analyze large-scale scenes, along with the existing datasets that we use in the experiments.  

\input{fig/real_data}


\paragraph{E-3DGS-Real}
Our real dataset was captured within a studio environment. The scene consists of a diverse set of objects, as shown in Fig.~\ref{fig:real_data_objects}. We used a DAVIS346C color event camera to capture our scene with a resolution of $346 \times 260$. 
The contrast threshold settings were kept at their default values, which are symmetric. 
We capture multiple clips of the scene, each roughly $60{-}120\unit{\second}$ long with varying motion characteristics and levels of scene coverage. The captured data consists of the event stream and RGB images at 2.5 frames per second.
The studio is equipped with 115 traditional cameras distributed uniformly across the walls and capturing 4K footage at $\qty{50}{FPS}$.
Similar to the approach of Millerdurai et al.~and annotation of the EE3D-R (Real) dataset \cite{EventEgo3D}, we use these cameras to estimate and track the camera pose by detecting a checkerboard mounted to the event camera rig, providing tracking data at a frequency of up to $\qty{50}{Hz}$. Note that in some timestamps the checkerboard is not detected due to occlusions and thus the $\qty{50}{Hz}$ is only the best case. 
The data from the external cameras is relevant for camera pose estimation, but cannot be used as ground truth because of the significantly different perspectives from the training views.


\paragraph{E-3DGS-Synthetic}
For creating the synthetic dataset, we choose three scenes of UnrealEgo~\cite{unrealego}.
We rendered $60\unit{\second}$ clips of each scene at $\qty{1000}{FPS}$.
The scenes contain large-scale environments and exhibit various types of surfaces, including reflections.
We noticed that a few of the small highly reflective objects (e.g.,~metallic rods) cause unnatural aliasing in the renders, so we changed them to use diffuse materials. The event generation model from Sec.~\ref{eq:egm} was used to simulate event data from these high-fidelity frames. 
While we had access to pose data $\qty{1000}{Hz}$, we downsampled it to $\qty{50}{Hz}$ to simulate a real-world setting in which the poses are estimated from externally captured RGB images. 

\paragraph{E-3DGS-Synthetic-Hard}
This dataset is designed specifically to highlight and rigorously evaluate the key contributions of our method during the ablation study.
To assess the significance of our pose refinement module---which cannot be quantitatively evaluated on the E-3DGS-Real dataset---we introduce artificial noise into the E-3DGS-Synthetic dataset, which is carefully matched to the one observed in real data (see Supplement~\ref{sec:supp_dataset_perturb} for details). This allows us to assess the performance of our pose refinement module effectively. 
In addition to introducing noise, we also address the issue of camera speed variation. While the camera speed in the E-3DGS-Synthetic dataset generally stays within a narrow range, this does not fully test the capabilities of our adaptive event windows. To create a more challenging scenario, we varied the camera speed sinusoidally, with a ratio between its maximum and minimum speed of 100. This modification enables a more comprehensive evaluation of our adaptive event windows.


\paragraph{TUM-VIE}
This dataset consists of recordings from a Prophesee Gen4 sensor~\cite{klenk2021tum}. RGB views from an externally calibrated camera are also provided. The camera extrinsics are tracked at $\qty{120}{Hz}$. Two of the recordings have been used in Robust E-NeRF~\cite{robust_enerf}; we train our method on these recordings, namely \texttt{mocap-1d-trans} and \texttt{mocap-desk2} to compare with Robust E-NeRF. However, as also argued in Low and Lee \cite{robust_enerf}, these recordings are not well suited for novel view synthesis since the captures are predominantly front-facing, with some small displacements either in circles or from side to side. 

\paragraph{EventNeRF Datasets}
EventNeRF \cite{eventnerf} provides $\ang{360}$ object-centric event data, which we use to show that our method also outperforms previous methods on object-centric data. To be consistent with the original work, we evaluate our method on poses that are a part of the training trajectory instead of novel views, for our evaluation metrics to be comparable to theirs. We train our method on the synthetic sequences to perform the quantitative comparison. In these experiments, the background color is set to $159/255$, following the original paper \cite{eventnerf}. 


\input{fig/_e3dgs_real_comp}
\input{fig/tab_comparison_synthetic}
\input{fig/tab_comparison_eventnerf}


\subsection{Evaluation Metrics} 
For E-3DGS-Real dataset, the RGB frames are of too low quality to be used for evaluation purposes, and, therefore, we only perform qualitative comparisons. 
With TUM-VIE, as suggested in Robust E-NeRF~\cite{robust_enerf}, it is not trivial to do the tone mapping correctly. 
Therefore, we do quantitative evaluation only with the synthetic datasets. 
For the evaluation on synthetic data, keeping in line with the previous literature, we adopt the following evaluation metrics: 
\begin{itemize} 
    \item Peak Signal-to-Noise Ratio (PSNR); 
    \item Learned Perceptual Image Patch Similarity (LPIPS) \cite{LPIPS}; 
    \item Structural Similarity Index Measure (SSIM). 
\end{itemize} 

\input{fig/tab_synthetic_perturbed_speedx100}

\subsubsection{Color Correction} 
As our method only learns logarithmic differences rather than absolute color intensities, there is an ambiguity in the reconstructed color balance and illumination of the scene. Hence, color needs to be adjusted, as otherwise, the evaluation metrics will be less meaningful. We correct predicted images using the following equation: 
\begin{equation}
    L'_c = L' + (\mathbb{E}[L] - \mathbb{E}[L']) \mathrm{\,,}
    \label{eq:color_correction}
\end{equation}
where $L'_c$ is the color corrected logarithmic image and ``$\mathbb{E}[\cdot]$'' is the expectation operator. 
Eq.~\eqref{eq:color_correction} is applied separately to each color channel, which effectively aligns the per-channel logarithmic means of the predicted images with the ground-truth ones. 
Since in the synthetic setting, we already know the exact contrast threshold, there is no need for correcting the scale of the image as done in some previous works~\cite{eventnerf,robust_enerf}. 
Since we lack reference images for the real dataset, neither evaluation nor color correction is applicable to it. However, some minor color and contrast adjustments are manually made for better visualization. 

\subsection{Comparisons to Related Methods}
\label{subsec:comparisons}


\paragraph{RGB-Based Methods} 
We train Deblur-GS~\cite{deblurgs} on blurry RGB images from our E-3DGS-Real dataset to establish a reference using RGB inputs. 
We also convert the event stream to images using E2VID~\cite{e2vid} and apply 3DGS (referred to as ``E2VID + 3DGS''). 
This method is evaluated on all E-3DGS datasets. 
To train both methods, we interpolate the camera poses at discrete time steps provided by the external tracking system, which is necessary because the pose timestamps do not align with the frame timestamps. 
We use Spherical Linear Interpolation (SLERP) for the rotations and Linear Interpolation (LERP) for the translations to obtain the camera poses for the images. 

\paragraph{Event-Based Methods}
For comparison with event-based methods, we train EventNeRF~\cite{eventnerf} on all E-3DGS datasets. To adapt it for our datasets, we normalize the camera poses within a unit sphere and following NeRF++~\cite{nerf++} added a background network to model areas outside the sphere, as the scene extent is unknown. Furthermore, the maximum event window length is increased by the factor of $10$ to aid convergence (up to one second). 
We do not train our method on the synthetic dataset provided by Robust E-NeRF~\cite{robust_enerf}, as it is designed for extremely long refractory periods that are not observed in other datasets. However, we compare their method to ours on two sequences from TUM-VIE in Fig.~\ref{fig:real_data_ablation}, namely \texttt{mocap-1d-trans} and \texttt{mocap-desk2}.


\subsubsection{Observations}

The results of all evaluations are reported in Tables \ref{tab:comparison_synthetic}--\ref{tab:eventnerf_synthetic} and Figs.~\ref{fig:real_data_ablation}--\ref{fig:synthetic_e3dgs}. 
As visible, our method consistently outperforms the baselines both on synthetic and real data. 
In the EventNeRF object-centric datasets, our method shows clear superiority across almost all evaluation metrics. The only exception is a marginally lower PSNR score on the ``Chair" scene, as detailed in Table~\ref{tab:eventnerf_synthetic}. The general performance advantage is further backed by the qualitative results in Fig.~\ref{fig:synthetic_eventnerf}, where our method produces more accurate reconstructions. 



Similarly, on the E-3DGS-Synthetic dataset, E-3DGS significantly surpasses both EventNeRF and E2VID+3DGS by a wide margin; see Table~\ref{tab:comparison_synthetic}. The qualitative results on the E-3DGS-Real dataset, highlighted in Fig.~\ref{fig:real_data_ablation}, further demonstrate our method's superior performance: Deblur-GS struggles with excessive blur; EventNeRF suffers from noise due to ray sampling and memory constraints, and E2VID+3DGS exhibits noisy Gaussians and floaters. 

While Robust E-NeRF achieves higher local contrast, it struggles with global brightness consistency due to single-event training; see Fig.~\ref{fig:tumvie_comp}. Our E-3DGS maintains consistent brightness across the scene, with only a slight reduction in local contrast. 
Note that we can observe some holes and floaters near the outer peripheries in Figs.~\ref{fig:real_data_ablation} and~\ref{fig:tumvie_comp}. These effects are due to out-of-bound areas at the edges of the observations that occur as a result of the undistortion of the event stream. 







\input{fig/_eventnerf_synthetic_comp}
\input{fig/_tum_vie_comp}
\input{fig/_e3dgs_synthetic_comp}

\subsection{Ablation Studies}

To evaluate the effects of individual contributions, we do extensive qualitative and quantitative ablation studies. 
We primarily train different variants of our method on the E-3DGS-Real and E-3DGS-Synthetic-Hard datasets, focusing on the effects of four key components: $\mathcal{L}_\text{iso}$, $\mathcal{L}_\text{pose}$, \textbf{P}ose \textbf{R}efinement (PR), and the \textbf{A}daptive Event \textbf{W}indow (AW).

For the ablation experiments without adaptive window, we use a maximum time interval $T_\text{max}$ instead of maximum events $N_\text{max}$ to sample the event windows. The value of $T_\text{max}$ is computed from $N_\text{max}$, such that the average event window size remains approximately similar.

The results are reported in Table~\ref{tab:ablation_study} and Figs.~\ref{fig:real_data_ablation} and~\ref{fig:synthetic_e3dgs}.
Removing $L_{iso}$ results in a noticeable performance drop, but removing $L_{iso}$ and $L_{pose}$ jointly leads to a much more significant decline. 
This is likely because $L_{pose}$ prevents pose divergence in unstable conditions, while removal of $L_{iso}$ causes instability due to overfitting. 
Similar effects could occur when combining $L_{iso}$ with pose refinement. 


