% \documentclass[lettersize,journal]{IEEEtran}
\documentclass[journal]{IEEEtran}

\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{courier}
\usepackage{listings}
% \usepackage{mathpazo}
\usepackage{tcolorbox}
\usepackage{amsmath, amssymb, amsfonts, mathtools}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{subcaption} % Subfigure for captions
\usepackage{kotex}
\usepackage{cleveref}
\usepackage{graphicx}    % 이미지 삽입

\usepackage[switch]{lineno}
% \linenumbers

\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\lstset{
  basicstyle=\ttfamily\scriptsize,
  keywordstyle=\color{red},
  commentstyle=\color{darkgreen},
  stringstyle=\color{blue},
  showstringspaces=false,
  numbers=none,
  numberstyle=\tiny\color{gray},
  breaklines=true,
  frame=single,
}
\tcbuselibrary{listings,breakable}
\begin{document}

\title{PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning}

% ORCID logo command
\newcommand{\orcidlogo}{\includegraphics[height=9pt]{orcid_logo.png}}

% ORCID link command
\newcommand{\orcidlink}[1]{\href{https://orcid.org/#1}{\orcidlogo}}

\author{In-Chang Baek\IEEEauthorrefmark{1}\IEEEauthorrefmark{3}, Sung-Hyun Park\IEEEauthorrefmark{1}, Sam Earle\IEEEauthorrefmark{2}, Zehua Jiang\IEEEauthorrefmark{2}, Noh Jin-Ha\IEEEauthorrefmark{1}, \\ Julian Togelius\IEEEauthorrefmark{2}, Kyung-Joong Kim\IEEEauthorrefmark{1}\IEEEauthorrefmark{4}
\\
\IEEEauthorrefmark{1}Gwangju Institution of Science and Technology (GIST)
\IEEEauthorrefmark{2}New York University
\\
\IEEEauthorrefmark{3}{inchang.baek}@gm.gist.ac.kr
\thanks{This work was supported by GIST-IREF from Gwangju Institute of Science and
Technology(GIST).}
\thanks{\IEEEauthorrefmark{4} Corresponding author}
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
}

% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
%{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Reward design plays a pivotal role in the training of game AIs, requiring substantial domain-specific knowledge and human effort.
In recent years, several studies have explored reward generation for training game agents and controlling robots using large language models (LLMs).
In the content generation literature, there has been early work on generating reward functions for reinforcement learning agent generators.
This work introduces \textit{PCGRLLM}, an extended architecture based on earlier work, which employs a feedback mechanism and several reasoning-based prompt engineering techniques.
We evaluate the proposed method on a story-to-reward generation task in a two-dimensional environment using two state-of-the-art LLMs, demonstrating the generalizability of our approach.
Our experiments provide insightful evaluations that demonstrate the capabilities of LLMs essential for content generation tasks.
The results highlight significant performance improvements of 415\% and 40\% respectively, depending on the zero-shot capabilities of the language model.
Our work demonstrates the potential to reduce human dependency in game AI development, while supporting and enhancing creative processes.

\end{abstract}

\begin{IEEEkeywords}
large language model, procedural content generation, prompt engineering, reinforcement learning, reward generation
\end{IEEEkeywords}


\section{Introduction}
\input{introduction/introduction}

\section{Background}
\input{background/background}


\section{Preliminaries}
\input{preliminary/preliminary}

\section{Story-based Reward Function Generation Task}
\label{sec:story2reward}
\input{preliminary/story2reward_task}

\section{Proposed Method}
\label{sec:method}
\input{sections/method/method}
% \input{method/Instructed based Reward Shaping.tex}




\section{Experiment}

\subsection{Experiment Setting}

\textbf{Reward generation} The number of self-feedback iterations was set to \( N_{\text{feedback}} = 6 \), and the self-alignment iterations were set to \( N_{\text{align}} = 5 \). For generating the LLM-based reward, OpenAI’s \texttt{gpt-4o-2024-08-06} \cite{hurst2024gpt} served as the primary backend language model. The breadth for the ToT and GoT methods was set to \( N_{\text{breadth}} = 2 \), balancing the depth of the thought nodes with exploration. The fitness function utilized an accuracy-based evaluation metric, calculated as the average score over 30 inferenced levels. To minimize variability in LLM responses, the temperature parameter, which governs stochasticity, was set to 0.
The prompts used in the experiments are noted in Appendix \hyperref[sec:prompt_example]{D}.

While we attempted to control the LLM’s determinism, we still observed variance in first-iteration zero-shot accuracy across different prompt engineering and feedback types. Subtle differences in prompt structure (e.g., placeholders for auxiliary data) can affect on the reasoning process.
We focus on how accuracy improves over subsequent iterations rather than focusing on zero-shot performance.


\textbf{Environment} The \textit{Dungeon} problem features seven game tiles and uses the \textit{Narrow} representation for the environment setting \cite{khalifa2020pcgrl}. The level size was set to \( 16 \times 16 \), and the agent was configured to scan the entire level three times per episode. The discrete action space consisted of five actions to modify the level, targeting five modifiable tiles: empty, wall, and three enemy types.
At the start of each episode, the player and one door were randomly placed along one of the four edges of the level, and the nearby \( 3 \times 3 \) tiles were masked as unmodifiable.


\textbf{RL training} The DRL models were trained using proximal policy optimization (PPO) \cite{schulman2017proximal} for 50 million timesteps using \textit{PureJaxRL} \cite{lu2022discovered} implementation and the hyperparameters detailed in Appendix \hyperref[sec:pcg_agent_parameters]{C}. Each input instruction and PE method was repeated five times, and the results were averaged. All experiments were conducted on RTX 8000 GPU machines.




\subsection{Evaluation Criteria}



\begin{figure}[!h]
    \centering

    \begin{tabular}{cc} % 두 개의 열
        \includegraphics[width=0.45\linewidth]{figure/appendix/n_solutions.png} &         \includegraphics[width=0.45\linewidth]{figure/appendix/encounter_monster.png} \\
        (a) Solutions & (b) Encounter enemies
    \end{tabular}

    \caption{(a) The dotted lines represent the solutions that achievable to the key. (b) The player enemy encounters enemies within yellow dotted box.}
    \label{fig:senario_method}
\end{figure}


\textbf{Accuracy} measures how well the generated level aligns with the given instructions. 
We assess whether the player encounters the specified key enemy tiles during gameplay, following the given scenario. 
The ground truth ($g$) is defined such that among the three enemy tile set ($E$)—\textit{Bat}, \textit{Scorpion}, and \textit{Spider}—those mentioned in the instruction are treated as positive, while the others are considered negative. 
The prediction ($p$) is defined as the enemies encountered by the player while traversing from the key to the door.
The flood-fill pathfinding algorithm is employed to determine viable solutions by identifying routes that allow the player to reach key and door tiles. 
Fig. \ref{fig:senario_method} visually illustrates how viable solutions are extracted from the game level and demonstrates the process of counting the enemies encountered. 
We define enemies within the $5 \times 5$ area around the solution path as those subject to convolution, categorizing them as positive predictions\, while others are negative. 
The multi-label accuracy is measured as shown in Eq. \ref{eq:accuracy}:

\begin{equation}
\label{eq:accuracy}
\text{Accuracy} = \frac{1}{|E|} \sum_{e \in E} \mathbb{I}(g_e = p_e)
\end{equation}

For example, if the instruction states, ``\textit{... the player encounters bat monsters,}'' and the generated level is shown in Fig. \ref{fig:senario_method}b, the ground truth is $[1, 0, 0]$, where only the \textit{Bat} is a positive label. 
If the prediction is $[1, 1, 0]$, where both \textit{Bat} and \textit{Scorpion} are marked as positive, the accuracy is calculated as $0.67$. 
The accuracy is measure on each instruction-level pair and the averaged with five runs.
The detailed evaluation algorithm is described in Appendix \hyperref[sec:acc_eval_method]{A}.

\subsection{Experimental Result}
We conducted three key experiments to evaluate the effectiveness of the proposed framework and to investigate the capabilities of LLMs in generating reward functions for PCG tasks. The focus is on assessing the LLM's reasoning and planning abilities in identifying and resolving issues within reward functions, as well as its reliability in objectively inferring scores to evaluate the suitability of generated content.
The following research questions were posed to guide the experimental analysis:


\begin{itemize}
    \item \textit{RQ1. How do self-alignment and feedback mechanisms enhance reward generation?}
    \item \textit{RQ2. How does reasoning-based prompt engineering enhance reward generation?}
    \item \textit{RQ3. Is LLM has capability of self-evaluation on content fitness?}
\end{itemize}


\subsubsection{Architectural Ablation Study (RQ1)} 
Table \ref{tab:ablation_study} presents the results of an ablation study analyzing the role of self-alignment (SA) and feedback (FB) mechanisms in improving the accuracy of reward function generation using the \texttt{gpt-4o} model. The performance improvement was consistent across all PE methods, highlighting the significant role of feedback in aligning generated reward functions with task objectives. Specifically, PCGRLLM outperformed both the zero-shot generated reward function (0.031) and the self-alignment-only approach \cite{baek2024chatpcg} (0.045) by achieving a score of 0.187 with FB, representing an improvement of approximately 415.5\% (0.045$\rightarrow$0.187).
This substantial performance improvement can be attributed to the model's relatively limited zero-shot generation capabilities.

To evaluate the generalization of the proposed method, we additionally employed Meta's \texttt{llama3.2-90b-instruct} \cite{touvron2023llama}, which exhibits better zero-shot generation performance. The \texttt{llama3.2} model achieved a 40.5\% improvement (0.289$\rightarrow$0.406) with FB\&SA.
The results from \texttt{llama3.2} suggest that the proposed method is capable of improving rewards regardless of the LLM's zero-shot performance.
These results demonstrate that feedback is essential for improving reward functions, however, combining feedback with self-alignment does not necessarily guarantee performance improvement.
We discuss the importance of feedback quality in Section \ref{sec:specificity_feedback} and auxiliary experiment on vision-input feedback generation is described on Appendix \hyperlink{sec:sec:vision_feedback}{B}. 

\subsubsection{Effect on Reasoning-based Prompt Engineering (RQ2)} 

This section examines the role of reasoning-based prompts in guiding LLMs to design effective reward functions. Table \ref{tab:reasoning_prompt} compares the accuracy of three methods—CoT, ToT, and GoT—over six iterations. Fig. \ref{fig:reasoning_pe_result} illustrates the accuracy values over six iterations, highlighting the initial, best, and the last accuracy values.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figure/experiment/reasoning_prompt.pdf}
    \caption{Accuracy comparison across iterations for different prompt engineering types. Error bars represent 95\% confidence intervals, with accuracy values annotated at key points.}
    \label{fig:reasoning_pe_result}
\end{figure}

\begin{table}[!h]
\centering
\caption{Performance between reasoning-based prompt engineering}
\label{tab:reasoning_prompt}
\begin{tabular}{p{1.8cm}|rrrrrr}
\toprule
  \textbf{PE} & \multicolumn{6}{c}{Iteration ($y_{i}$)} \\
 & 1 & 2 & 3 & 4 & 5 & 6 \\
\midrule
CoT & 0.033 & 0.007 & 0.092 & 0.109 & 0.093 & \textbf{0.156} \\
ToT & 0.028 & 0.172 & 0.259 & 0.124 & 0.113 & \textbf{0.329} \\
GoT & 0.033 & \textbf{0.095} & 0.053 & 0.074 & 0.042 & 0.076 \\
\bottomrule
\end{tabular}
\end{table}

ToT achieves the highest accuracy of 0.329 at iteration 6, reflecting its ability to effectively leverage structured reasoning.
CoT peaks at 0.156 in the same iteration while maintaining a consistent trajectory overall. In contrast, GoT reaches a modest accuracy of 0.095 by iteration 2, showing decreasement on later iterations.
These findings highlight the necessity of selecting prompt strategies that correspond to the level of informational importance.
Of the various approaches, ToT proves to be the most effective for iterative refinement, highlighting the value of structured reasoning in improving LLM-generated reward signals while also providing fault tolerance for flawed rewards through its backtracking mechanism.


\subsubsection{Content Evaluation Ability Analysis (RQ3)}

Accurately measuring the fitness of generated content plays a critical role in determining the correct direction for improving the reward function.
Due to the potential for hallucinations in LLMs, the PEs studies \cite{yao2024tree,besta2024graph} employed heuristics as fitness functions to evalute the trained policy.
We treat the heuristic as an oracle representing the maximum performance and compare it against the evaluation scores produced by the LLM when used to improve the reward function. To evaluate content, we provided the LLM with generated content and evaluation metrics, requesting to return a score value scaled between 0 and 1 according to the explanation of accuracy measurement.


\begin{table}[!h]
\caption{Results of an ablation study comparing the performance of heuristic and LLM-based evaluators when feedback is applied to each PE method.}
\label{tab:self_evaluation}
\begin{tabular}{p{1.5cm}|p{0.73cm}p{0.73cm}p{0.73cm}|p{0.73cm}p{0.73cm}p{0.73cm}}
\toprule
\textbf{Fitness ($F$)} & \multicolumn{3}{c|}{Heuristic (Oracle)} & \multicolumn{3}{c}{LLM (Self-evaluate)} \\
 & Base & +FB & $\Delta{}$ & Base & +FB & $\Delta{}$ \\
PE &  &  &  &  &  &  \\
\midrule
ToT & 0.028 & 0.329 & +0.301 & 0.106 & 0.071 & -0.034 \\
GoT & 0.033 & 0.076 & +0.044 & 0.062 & 0.018 & -0.044 \\
\midrule
\textbf{Mean} & 0.030 & 0.203 & \textbf{+0.172} & 0.084 & 0.044 & \textbf{-0.039} \\
\bottomrule
\end{tabular}
\end{table}

The results presented in Table \ref{tab:self_evaluation} highlight a clear contrast between the heuristic (oracle) and LLM-based evaluation methods. On average, the heuristic evaluation demonstrated a significant performance improvement of +0.172 when feedback was applied, showcasing its ability to effectively utilize feedback for performance enhancement. Conversely, the LLM-based self-evaluation exhibited a decline in performance with feedback, resulting in an average decrease of -0.039.
This outcome indicates that the inaccuracies in self-evaluation led to inaccurate directions for improving the reward function.

\section{Discussion}
\input{discussion/discussion.tex}

\section{Conclusion and Future Work}
\input{conclusion/conclusion.tex}

\bibliographystyle{IEEEtran}
\bibliography{references}






% \end{thebibliography}

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here without a photo.
% \end{IEEEbiographynophoto}

% % \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1.png}}]{IEEE Publications Technology Team}
% In this paragraph you can place your educational, professional background and research and other interests.\end{IEEEbiography}



\section*{Appendix}
\label{sec:appendix}
\input{appendix/appendix}

\end{document}


