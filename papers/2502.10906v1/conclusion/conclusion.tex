
This study introduces an advanced reward generation architecture for game content generation. PCGRLLM frames the reward function refinement as thought units and incorporates various PE methods to enhance exploration within the reward space.
The extended architecture analyzes the content generated by the trained policy and incorporates policy feedback into reward refinement. The results indicate that feedback-based reflection significantly improves the reward function, highlighting the critical role of feedback quality.
The state-of-the-art PEs, such as ToT and GoT, has capability to enhance reward function through a fault-tolerant reasoning process and sample efficiency.
The generality of the framework is evaluated using two popular foundation LLMs, demonstrating substantial improvements in low and zero-shot generation performance.

Furthermore, this work investigates two essential abilities of LLMs for game content generation: content evaluation and vision-based content reasoning. The experimental results suggest that content evaluation using language models remains a challenging problem, despite it plays a crucial role in guiding the direction of reward refinement.
Future work aims to enhance content evaluation performance by developing objective assessment methods, such as few-shot retrieval techniques, to achieve more balanced score distributions. This improvement would reduce reliance on human experts, paving the way for an end-to-end LLM framework.