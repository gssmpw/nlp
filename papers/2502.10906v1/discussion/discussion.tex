\subsection{Specificity of Feedback}  
\label{sec:specificity_feedback}

In RQ1 experiment, we observed that the involvement of feedback influences on the reward function improvement. 
To further investigate this, we conducted an experiment to evaluate whether the quality of feedback impacts performance. 
This experiment benchmarks three different feedback types—\textit{No Feedback}, \textit{Generic}, and \textit{Specific} feedback—on performance across multiple iterations.
The general feedback provides two lines of tips to enhance the reward function that are unrelated to the generated content, while specific feedback represents the default setting in this study.
To isolate the effect of fitness evaluation, the experiment was conducted within the CoT framework.

As shown in Table \ref{tab:feedback_type}, the performance comparison across feedback quality types highlights the significant impact of feedback specificity on performance improvement as iterations progress. 
Additionally, Fig. \ref{fig:specific_feedback} illustrates the relative accuracy change ($\Delta y$) over iterations for each feedback type, emphasizing the role of specific feedback in driving more consistent performance gains.


\begin{table}[!h]
\caption{Performance between feedback quality type}
\label{tab:feedback_type}
\begin{tabular}{l|rrrrrr}
\toprule
\textbf{Feedback Type} & \multicolumn{6}{c}{Iteration ($y_{i}$)} \\
 & 1 & 2 & 3 & 4 & 5 & 6 \\
\midrule
No Feedback & 0.086 & 0.063 & 0.071 & 0.088 & 0.092 & \textbf{0.099} \\
Generic & 0.034 & 0.033 & \textbf{0.052} & 0.039 & 0.018 & 0.031 \\
\textbf{Specific} & 0.033 & 0.007 & 0.092 & 0.109 & 0.093 & \textbf{0.156} \\
\bottomrule
\end{tabular}
\end{table}


\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figure/experiment/specific_feedback.pdf}
    \caption{Accuracy change ($\Delta y$) across iterations for \textit{Specific}, \textit{Generic}, and \textit{No Feedback} types. Each bar represents the relative change ($\Delta y_{n \to n+1}$) between consecutive iterations. Positive values indicate improvement, while negative values represent decline.}
    \label{fig:specific_feedback}
\end{figure}

According to the data, the no feedback and generic feedback conditions exhibit occasional improvements in $\Delta y$; however, the overall changes remain mostly consistent and insignificant. Due to the lack of actionable guidance for improvement in the no feedback and generic feedback conditions, subsequent iterations tend to produce results similar to those of the first iteration, with minimal iterative improvement.
In contrast, specific feedback demonstrates more notable performance improvements in some iterations. Particularly, significant performance leaps are observed in $\Delta y_{2 \to 3}$ and $\Delta y_{5 \to 6}$. These findings clearly indicate that tailored feedback plays a critical role in optimizing performance as iterative tasks progress.


\subsection{Auxiliary Information Analysis}
\label{sec:aux_info_analysis}

In RQ2 experiment, we observed that providing more few-shot data in GoT resulted in lower performance compared to ToT.
To investigate whether the number or quality of the few-shot data affects the improvement of the reward function, we conducted an in-depth experiment.
Fig. \ref{fig:auxiliary_data_heatmap} compares the type and quantity of few-shot examples provided as auxiliary data.
Providing best thoughts (high fitness values) would expand the upper bound of performance, while worst thoughts (low fitness values) guide the model to avoid falling below the lower bound of performance.


\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/discussion/auxiliary_data_heatmap.pdf}
    \caption{Accuracy based on the number of auxiliary data in the GoT reward refinement process, categorized by best (columns) and worst (rows) thoughts determined by fitness values.}
    \label{fig:auxiliary_data_heatmap}
\end{figure}


The results indicate that, while there is no consistent accuracy trend based on the number of auxiliary data, specific settings demonstrate higher performance tendencies.
For example, adding one worst thought to the GoT baseline setting improves performance (0.244), surpassing CoT (0.156).
This highlights that providing excessive information does not necessarily enhance the effectiveness of an LLM, as it may struggle to reason logically when overloaded with multiple pieces of information in a single turn.
This suggests that presenting entire reward examples at once can distract the LLM from key focal points, making it more advantageous to provide the parent reward function and a summarized revision direction separately.
Therefore, the ability to retrieve relevant information and adopt a divide-and-conquer approach is crucial for improving reward generation and represents a valuable area of study.
