
Our proposed framework \textit{PCGRLLM}, an improved reward generation framework for PCG, employs a three-step sequential approach: (1) refine the reward function through feedback, (2) align the reward function to the environment and train the agent, and (3) provide feedback to the reward function based on the generated content.
Fig. \ref{fig:architecture} illustrates the comprehensive architecture of the proposed framework.
While the prior study \cite{baek2024chatpcg} was the first to incorporate self-alignment into the reward generation task in content generation domain, this work extends the framework by incorporating feedback feature and a refinement process, forming an outer loop that enhances the overall system's adaptability and performance.
The following subsections detail the process of refining the reward function to align with the instruction inputs, with the corresponding pseudo-code provided in Algorithm \ref{alg:pcgrllm}.


\begin{algorithm}[!h]
\caption{PCGRLLM Reward Refinement Process}
\label{alg:pcgrllm}
\begin{algorithmic}[1]
\Require Task description $l$, LLM $\mathcal{M}$, fitness function $F$, environment $Env$, prompt inputs $p$

\Require Feedback count $N_{\text{feedback}}$, alignment count $N_{\text{align}}$

\State \textbf{Note:} \textcolor{blue}{blue color text} denotes variables used within the reasoning process.

\State $G \sim \text{InitializeGraph}(N_{\text{breadth}})$

\For{feedback step $y=1$ to $N_{\text{feedback}}$} 
    \State // \textbf{Step 1: Refinement (Section \ref{sec:reward_refinement})}
    \State $R, {Fb}, \textcolor{blue}{f} \sim G$ \Comment{Retrieve parent node.}
    \textcolor{blue}{
    \State $Aux = \{R^{\text{aux}}_{1:N_{\text{Best}}}, f^{\text{aux}}_{1:N_{\text{Best}}}\} \sim G$ \Comment{Retrieve auxiliary}
    }

    \State // \textbf{Step 2: Self-alignment (Section \ref{sec:self_alignment})}
    \State $R^{\prime} \sim \mathcal{M}(p_{\text{reward}}, l, R, {Fb}, \textcolor{blue}{f, Aux})$

    \For{alignment step $z=1$ to $N_{\text{align}}$} 
        \State $r^{\text{env}}_{1:N} \gets \text{Rollout}(\pi_{\text{random}},Env, R^{\prime})$
        \State $R^{\prime\prime} \gets \mathcal{N}(p_{\text{align}}, R^{\prime}, r^{\text{env}}_{1:M})$
        \State $R^{\prime} \gets R^{\prime\prime}$
    \EndFor

    \State $\pi_{\text{trained}} \gets \text{Train}(\pi_{\text{untrained}}, Env, R^{\prime})$
    \State $s_T \gets \text{Rollout}(\pi_{\text{trained}})$ \Comment{Generate content}
    \State $f \gets F(s_{T})$ \Comment{Evaluate fitness}

    \State // \textbf{Step 3: Feedback (Section \ref{sec:self_feedback})}
    \State ${Fb} \gets \mathcal{M}(p_{\text{feedback}}, l, R^{\prime}, s_T)$
    \State $G.\text{update}(R^{\prime}, s_T, f)$
\EndFor

\State \textbf{Output:} Refined reward function $R^{\prime}$
\end{algorithmic}
\end{algorithm}

\subsection{Reward Refinement}
\label{sec:reward_refinement}
The reward refinement process is an iterative procedure aimed at enhancing the reward function through feedback to better align it with the given instruction.
It takes as input a textual description of the game environment, the objective of the reward generation task, textual instructions, and accessible variables from the environment. For this study, two concise textual stories, as detailed in Section \ref{sec:story2reward}, were used as input for the textual instructions.
The process involves revising the parent function, defined as the previously generated reward function, and progresses iteratively, with each iteration denoted as \( y \). Depending on the presence of a prior iteration, the process operates in one of two phases: initializing the reward function or refining it based on feedback from previous iterations.


\textbf{Initial generation (\( y = 1 \))}
The initial generation phase begins with a template reward function, which is an empty function containing only parameter definitions. The parameters of the reward function are the previous and current level arrays, while the level size is intentionally excluded to prevent the LLM from generating hard-coded functions.
In the first iteration, the LLM generates conceptual ideas to outline the components of the reward function.
The prompt includes the progress of the feedback iteration, \textit{"\{current iter.\} of \{max iter.\}"}, to utilize the planning capabilities of LLMs.

\textbf{Continued refinement (\( y \geq 2 \))}
In subsequent iterations, the process retrieves a previously generated reward function from the reward function archive ($G$) to serve as the parent function for refinement. A pair consisting of the parent reward function (\( R \)) and its corresponding feedback ($Fb$) is sampled and provided to the LLM. The method for retrieving the parent reward function ($R$) varies depending on the prompt engineering method, as detailed in following section (Section \ref{sec:reasoning_pe}).

Once the parent function and feedback are identified, they are passed to the LLM ($\mathcal{M}$). Using a refinement prompt (\( p_\text{refine} \)), the LLM generates an improved reward function ($R'$) that incorporates the parent function and feedback.
The improved reward function addresses the shortcomings or problematic aspects of the previous reward function, making it better aligned with the textual instructions.


\subsection{Self-alignment to the Environment}
\label{sec:self_alignment}
The self-alignment process ensures that the generated reward function (\( R \)) produces the trainable reward signals in the environment, reflecting the intend of LLM.
By allowing the reward function to briefly interact with the training environment in a few-shot manner, this process refines the function to align with the designed insights.
Overly narrow reward ranges or excessively large reward values can hinder effective training DRL agent.
This iterative process enables the reward function to generate the intended content while providing meaningful feedback for each refinement.

Specifically, one episode is simulated using a random agent to collect distributed reward values directly from interactions with the environment, denoted as \( r^{\text{env}}_{1:M} \). The mean and variance of these reward values are calculated to verify whether the designed reward function outputs values within the intended range. Based on these evaluations, the LLM adjusts the reward function, producing an updated version (\( R'' \)) along with the actual reward values (\( r^{\text{env}}_{1:M} \)), the current reward function (\( R' \)), and the alignment prompt (\( p_{\text{align}} \)). This process, repeated \( N_{\text{align}} \) times, incrementally refines the reward function by modifying weights or formulas to consider the scale and sparsity of the reward signal.


\subsection{Feedback from Generated Contents}
\label{sec:self_feedback}

Feedback is an essential process for refining the reward function to reflect the actual output of the trained policy. The LLM reasons about the causal relationship between the reward function and the generated content to identify inconsistencies. It evaluates whether the generated content aligns with the instruct conditions by analyzing discrepancies, such as the positions or counts of important tiles within the level. Based on this analysis, the model formulates a plan to refine the reward function, guiding its next iteration toward better alignment with the intended design objectives and the policy's outputs.

First, the terminal states (\( s_T \)) are collected as generated content by inferring the trained policy (\( \pi_{\text{trained}} \)). Then, the LLM generates feedback by reasoning over the text-formatted levels or rendered images of the terminal states (\( s_T \)) along with the current reward function (\( R' \)) and a feedback prompt (\( p_{\text{feedback}} \)). To prevent hallucination and maintain precision, the number of feedback points is limited to one per iteration.

Once the feedback ($Fb$) is generated, the updated reward function and its associated feedback are added to the reward function archive (\( G \)) for use in the next iteration. This iterative process ensures that the reward function progressively aligns with both the intended objectives and the actual outputs of the trained policy, enabling the generation of increasingly optimal content.

\subsection{Reasoning-based Prompt Engineering for Reward Improving}
\label{sec:reasoning_pe}

We employed various state-of-the-art PE techniques to iteratively refine the reward function in a step-by-step manner. The process of generating reward functions is inherently a trial-and-error approach, which presents a significant challenge as it does not guarantee consistent improvements. 
Such an algorithm must effectively identify and address errors while maintaining the flexibility to explore alternative solutions, ensuring convergence toward improved reward designs.
To address this challenge, we adopted backtracking-enabled and branching PE methods, such as ToT and GoT, to expand the exploration of the reward space.
Fig. \ref{fig:reasoning_pe} illustrates the detailed procedure of thought node expansion and the utilization of auxiliary information in GoT.

\textbf{Thought node}
A feedback iteration unit is represented by a single thought node in the reasoning process, comprising three steps: reward refinement, self-alignment, and feedback. The expansion method varies depending on the PE approach. CoT employs a simple chain structure with a maximum breadth (\( N_{\text{breadth}} \)) of 1, while ToT and GoT utilize tree and graph structures, respectively, with a maximum breadth. For parent node selection, CoT expands from the latest unique node, whereas ToT and GoT expand from the node with the highest fitness score, provided its number of child nodes remains within the maximum breadth.


\textbf{Reward evaluation}
The fitness score, essential for determining the parent node in ToT and GoT, is measured within the range \([0, 1]\) and evaluates how well the generated content satisfies the instruction input. It can be determined using heuristics or self-evaluation with LLMs. In this study, we use a heuristic approach based on accuracy to mitigate the influence of subjective LLM evaluations. Specifically, we calculate the average accuracy of 30 instruct-level pairs. 


\textbf{Auxiliary information}
GoT utilizes auxiliary inputs, incorporating the parent thought node along with the top-2 reward nodes and their corresponding fitness scores. This approach is motivated by the need to improve sample efficiency, as generating and training reward functions incur significant computational costs. By comparing the fitness value ($f$), the LLM can identify reward functions that best satisfy the instruct and discern their advantageous elements to combine them, facilitating more informed reasoning and improved outcomes.


\begin{table*}[!t]
\caption{Ablation study results for framework architecture, comparing accuracy across prompt engineering methods.}
\label{tab:ablation_study}
\begin{tabular}{p{2.2cm}|p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}|p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\toprule
LLM ($\mathcal{M}$) & \multicolumn{4}{c|}{\texttt{gpt-4o-2024-08-06}} & \multicolumn{4}{c}{\texttt{llama3.2-90b-instruct}} \\
 & Base & + SA \cite{baek2024chatpcg} & + FB & + FB \& SA & Base & + SA \cite{baek2024chatpcg} & + FB & + FB \& SA \\
PE &  &  &  &  &  &  &  &  \\
\midrule
CoT & 0.033 & 0.010 & \textbf{0.156} & 0.117 & 0.300 & 0.298 & 0.239 & \textbf{0.472} \\
ToT & 0.028 & 0.058 & \textbf{0.329} & 0.103 & 0.303 & 0.262 & 0.236 & \textbf{0.363} \\
GoT & 0.033 & 0.067 & 0.076 & \textbf{0.111} & 0.149 & 0.308 & 0.307 & \textbf{0.383} \\
\midrule
\multicolumn{1}{c|}{\textbf{Mean}} & 0.031 & 0.045 & \textbf{0.187} & 0.110 & 0.251 & 0.289 & 0.260 & \textbf{0.406} \\
\bottomrule
\end{tabular}
\end{table*}
