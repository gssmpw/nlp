The design of reward functions plays a pivotal role in the training of deep reinforcement learning (DRL) agents and evolutionary algorithms for procedural content generation (PCG) in games \cite{liu2021deep}. Reward functions guide agent behaviors and define the objectives that align generated content with desired outcomes, such as game difficulty or aesthetic appearance.
Traditionally, designing reward functions relies heavily on researchers' game-specific knowledge and time-consuming reward shaping process. 
In the procedural content generation via RL (PCGRL) literature \cite{khalifa2020pcgrl}, the controllability of reward function has been achieved by parameterization of reward function in two- and three- dimensional level generation tasks \cite{earle2021learning, jiang2022learning}.
This significant human dependency not only requires significant time and resources but also introduces barriers to accessibility and scalability of game AIs.
Additionally, the controllability of RL-based generative models has been dependent on pre-defined environmental features.
Therefore, reward generation is necessary to alleviate the dependency on humans and dependency on controllable features.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figure/intro/concept.pdf}
    \caption{An overview of the reward generation process: (1) instructions guide the LLM, (2) outputs direct the agent, (3) environment interactions refine rewards, and (4) feedback analyzes content for improvement.}
    \label{fig:concept}
\end{figure}

Recent advancements in LLMs have shown their potential to mitigate these challenges by leveraging pre-trained expert knowledge from large datasets. Several studies have explored LLM-based reward generation approaches in robotic control \cite{ma2023eureka, zeng2023learning, yu2023language} and the gameplay \cite{li2024auto, zheng2024online} domain, utilizing LLMs' reasoning and coding capabilities.
One such approach, \textit{ChatPCG} \cite{baek2024chatpcg}, introduced an early-stage LLM-driven reward generation method for PCGRL, which transforms high-level game descriptions into low-level reward function code.
This work proposed a reward fine-tuning method, self-alignment, to align the reward function for a specific environment. However, a limitation of this approach is the absence of a refinement process that incorporates results of the trained agent. As a result, it is uncertain whether the trained policy accurately reflects the intended reward function generation conditions.

To address these limitations, we propose an improved architecture based on prior work \cite{baek2024chatpcg}, \textit{PCGRLLM}, a feedback-based reward generation framework for content generation.
Fig. \ref{fig:concept} illustrates the overview of the proposed method with the sequence of improving reward function with two major refining processes: self-alignment and feedback.
The language model generates a reward function with a brief story instruction and a PCGRL model is trained with the generated function.
To ensure that the RL agent generates content that satisfies the instruction, the language model provides feedback and updates the reward function in the next iteration.
The feedback allows the RL agent to improve its policy by observing the actual outcomes of the trained agent, while the LLM generates rewards that can be effectively incorporated into the RL agent's training.

Specifically, our contributions are threefold:
\begin{itemize}
\item \textbf{Enhancing reward generation architecture:} The feedback mechanism enhances the reward generation pipeline, enabling the policy to align more effectively with the given instructions.
\item \textbf{Reasoning-based refinement prompting:} State-of-the-art prompt engineering techniques are employed to improve the exploration of the reward space.
\item \textbf{Comprehensive modular evaluation:} Extensive experiments provide ablation study and extensive insights into the capabilities of LLMs in content generation.
\end{itemize}

We evaluate the proposed framework with a text-to-reward task, which evaluates reward functions based on how they reflect a given textual prompt.
The PCGRL agent is trained with LLM-generated reward functions, and the quality of these reward functions is evaluated based on the agent-generated content.
The task is demonstrated in a two-dimensional level generation environment \cite{khalifa2020pcgrl,earle2024scaling}, with a brief story input, such as: \textit{"The player needs to obtain a key and escape through the door. To pick up the key, the player encounters bat monsters."}
To generate reward values for content generation, it is essential to design reward functions iteratively through effective planning and to employ reasoning to assess the causal relationship between the reward function and the resulting content.
From the perspective of content generation, we evaluate the reasoning capabilities of LLMs---code  generation, content evaluation, reflection, and key extraction---by conducting extensive evaluations.
