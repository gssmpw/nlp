
\begin{tcolorbox}[colback=white, colframe=gray, title=Reward Refinement, breakable]

{\textbf{\MakeUppercase{Instruction}}}  \\

\textbf{PCG Agent Reward Function Generation Task}

You are a reward function engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\\

\textbf{PCGRL Environment}

The gym-pcgrl project provides an environment for Procedural Content Generation via Reinforcement Learning (PCGRL), where agents learn to generate game levels.
One of the environments, "binary," involves creating maps using given tile types.
In the "narrow" representation, the agent modifies the map by scanning through it, changing tiles one by one.
This localized approach encourages more precise level design.
The agent receives rewards based on how well the generated level meets the predefined objectives.
If the agent generates a level that aligns closely with the goal—such as creating a functional and balanced map—it receives a higher reward.
This reward-driven mechanism incentivizes the agent to improve its design strategies over time, learning to create optimal levels through trial and error.\\

\textbf{Reward Function}

The reward function is a function that calculates the reward value for the agent based on the playtested results.
The function is written in Python and loads the playtested results from the json file and calculates the reward value based on the results.

[colback=white, colframe=gray, title=Reward Function Guidelines, breakable]
\begin{lstlisting}[language=Python]
import jax.numpy as jnp

def compute_reward(prev_array, prev_stats, curr_array, curr_stats) -> float:
    reward = 0.0

    return reward
\end{lstlisting}

'prev\_array' and 'curr\_array' are two-dimensional array representing the game level with tile numbers. 
The range of width is different by the array input (e.g., 10-20), so write a function works on any width and height.

\textbf{prev\_array}:  previous game level represented with tile numbers (jnp.array, (h,w))

\textbf{curr\_array}: current game level represented with tile numbers (jnp.array, (h,w))\\
The array is a 2D array with the shape of (height, width) to represent the game level.\\

The level is represented with tile numbers. The tile number is an integer value.

\textbf{Tile Number}

EMPTY = 1, WALL = 2, PLAYER = 3, BAT = 4, SCORPION = 5, SPIDER = 6, KEY = 7, DOOR = 8\\


\textbf{Thought Tips}

You have only one chance to generate a reward function. Make a reward function to work on the environment \\

\textbf{Target Scenario}

The 'Player' needs to obtain the Key and escape through the 'Door'. To pick up the key, the player must encounter one of the monsters: BAT. The players can figure out all of the monsters when they play the level several times.

Ensure that the generated map includes a reasonable number of monsters and keys.
First, create a reward function that minimizes the generation of monsters other than those mentioned above.
Second, design the reward function so that the map does not contain an excessive number of monsters or keys.\\

\end{tcolorbox}

\vspace{1em}

\begin{tcolorbox}[colback=white, colframe=gray, title=Self-Alignment, breakable]
{\textbf{\MakeUppercase{Instruction}}}  \\

\textbf{PCG Agent Reward Function Generation Task}

Here is your generated reward function and the result of the code execution (Code Result) with random action agent.
Keep the existing function structure and tune small components like weights to adjust the reward values, ensuring the logic remains intact. 

Focus on reward scale and sparsity.  

Do not hallucinate reward function.\\

\textbf{Reward Function Code}

\{previous reward function\} \\


Code Result

Mean: -646.652710

Std: 502.900909

Zero Value Percent: 23.9173\% 

\medskip
\noindent\textcolor{gray}{\rule{\linewidth}{0.4pt}}

{\textbf{\MakeUppercase{Response}}}  \\

\textbf{Revised Reward Function Code}

\{Revised reward function\} 
\\

\textbf{Changes Made}

- Increased the reward for bats and scorpions to 5.0 to encourage their presence more strongly.

- Increased the penalty for spiders to 7.0 to discourage their presence more strongly.

- Increased the reward for keys to 4.0 to encourage collecting keys.

- Increased the penalty for having more than 2 keys to 4.0 to discourage excessive key collection.

- Increased the penalty for having more than 3 total monsters to 6.0 to discourage excessive monster presence.

\end{tcolorbox}

\vspace{1em}

\begin{tcolorbox}[colback=white, colframe=gray, title=Feedback, breakable]
{\textbf{\MakeUppercase{Instruction}}}  \\

\textbf{Evaluate why the current reward function is producing these game levels}

Analyze how the reward structure influences the design decisions made by the AI.

For each of the following aspects, explain potential issues and how modifying the reward system could lead to better outcomes. \\

\textbf{The Following Content was Generated Based on the Current Reward Function}

Review the generated levels and identify common problems in the contents.\\

\textbf{Previous Reward Code}

\{previous reward code\} \\

\textbf{Evaluating the Generated Levels: Step-by-Step Analysis}

1. Review multiple generated levels and pinpoint common problems in the contents.

2. Decide on one specific problem and analyze why the current reward function causes this issue.

- If PCGRL fails to train, it could indicate a sparse reward problem. Consider the reward value and frequency in the previous implementation.

- Another issue could be overfitting on specific actions, such as repeatedly placing a tile excessively, leading to excessive rewards for certain actions.

- Since predicting reward values from a complex function can be difficult, a simpler reward function might resolve training problems.

3. Pinpoint problematic parts in the reward function and propose changes. Provide the changed part only of the reward function.

\medskip
\noindent\textcolor{gray}{\rule{\linewidth}{0.4pt}}

{\textbf{\MakeUppercase{Response}}}  \\

\textbf{Feedback: Step-by-Step Analysis}

\textbf{1. Review of Generated Levels and Common Problems:}

- Imbalance in Monster Types:

Across the generated levels, a common issue is the lack of a balanced distribution of monster types. Some levels have an overrepresentation of one type of monster, while others lack diversity in monster types.

- Key and Door Placement Issues:

Another issue is the placement of keys and doors. In some levels, keys are placed in locations that are not easily accessible or logical in terms of gameplay flow. \\

\textbf{2. Analysis of the Current Reward Function:}

- Lack of Incentive for Balance:

The current reward function provides a reward for having at least one of each required monster type (BAT, SCORPION, SPIDER), but it does not sufficiently incentivize a balanced distribution of these monsters. The penalty for imbalance is relatively small, which may not be enough to encourage diversity.

[...omitted for brevity] \\

\textbf{Revised Reward Function (Changed Parts Only):}

\begin{lstlisting}[language=Python]
# Increase penalty for imbalance in monster types
monster_balance_penalty = (
    jnp.abs(curr_bats - curr_scorpions)
    + jnp.abs(curr_scorpions - curr_spiders)
    + jnp.abs(curr_spiders - curr_bats)
)
reward -= monster_balance_penalty * 0.3  # Increased from 0.1 to 0.3

# Introduce reward for strategic placement of keys and doors
# (This is a conceptual addition; actual implementation would require additional logic to evaluate placement)
strategic_placement_reward = evaluate_placement(curr_array)  # Hypothetical function to evaluate placement
reward += strategic_placement_reward

\end{lstlisting}
\end{tcolorbox}

\vspace{1em}

\begin{tcolorbox}[colback=white, colframe=gray, title=Generated Reward Function Example, breakable]
\begin{lstlisting}[language=Python]
import jax.numpy as jnp

def compute_reward(prev_array, unused3, curr_array, unused4) -> float:
    # Define tile numbers
    EMPTY = 1
    WALL = 2
    PLAYER = 3
    BAT = 4
    SCORPION = 5
    SPIDER = 6
    KEY = 7
    DOOR = 8

    # Initialize reward
    reward = 0.0

    # Count the number of each tile type in the current and previous arrays
    def count_tiles(array, tile):
        return jnp.sum(array == tile)

    # Calculate the number of each relevant tile in the current and previous arrays
    curr_player_count = count_tiles(curr_array, PLAYER)
    curr_key_count = count_tiles(curr_array, KEY)
    curr_door_count = count_tiles(curr_array, DOOR)
    curr_bat_count = count_tiles(curr_array, BAT)
    curr_scorpion_count = count_tiles(curr_array, SCORPION)
    curr_spider_count = count_tiles(curr_array, SPIDER)

    prev_player_count = count_tiles(prev_array, PLAYER)
    prev_key_count = count_tiles(prev_array, KEY)
    prev_door_count = count_tiles(prev_array, DOOR)
    prev_bat_count = count_tiles(prev_array, BAT)
    prev_scorpion_count = count_tiles(prev_array, SCORPION)
    prev_spider_count = count_tiles(prev_array, SPIDER)

    # Reward for having exactly one player, one key, and one door
    reward += 10.0 * (curr_player_count == 1) - 10.0 * (prev_player_count == 1)
    reward += 10.0 * (curr_key_count == 1) - 10.0 * (prev_key_count == 1)
    reward += 10.0 * (curr_door_count == 1) - 10.0 * (prev_door_count == 1)

    # Reward for having at least one of each monster type
    reward += 5.0 * (curr_bat_count >= 1) - 5.0 * (prev_bat_count >= 1)
    reward += 5.0 * (curr_scorpion_count >= 1) - 5.0 * (prev_scorpion_count >= 1)
    reward += 5.0 * (curr_spider_count >= 1) - 5.0 * (prev_spider_count >= 1)

    # Penalize more heavily for having more than 3 of any monster type
    reward -= 15.0 * (curr_bat_count > 3) - 15.0 * (prev_bat_count > 3)
    reward -= 15.0 * (curr_scorpion_count > 3) - 15.0 * (prev_scorpion_count > 3)
    reward -= 15.0 * (curr_spider_count > 3) - 15.0 * (prev_spider_count > 3)

    # Penalize for having more than 1 key
    reward -= 10.0 * (curr_key_count > 1) - 10.0 * (prev_key_count > 1)

    # Reward for balanced distribution of monster types
    balanced_monsters = (curr_bat_count <= 3) & (curr_scorpion_count <= 3) & (curr_spider_count <= 3)
    reward += 10.0 * balanced_monsters.astype(float)

    # Calculate distances between important tiles
    def calculate_distance(array, tile1, tile2):
        positions1 = jnp.nonzero(array == tile1, size=1, fill_value=-1)
        positions2 = jnp.nonzero(array == tile2, size=1, fill_value=-1)
        pos1 = jnp.array([positions1[0][0], positions1[1][0]])
        pos2 = jnp.array([positions2[0][0], positions2[1][0]])
        return jnp.linalg.norm(pos1 - pos2)

    # Reward for proximity of PLAYER, KEY, and DOOR
    threshold = 5.0
    player_key_distance = calculate_distance(curr_array, PLAYER, KEY)
    key_door_distance = calculate_distance(curr_array, KEY, DOOR)
    reward += 5.0 * (player_key_distance < threshold) + 5.0 * (key_door_distance < threshold)

    return reward

\end{lstlisting}
\end{tcolorbox}

