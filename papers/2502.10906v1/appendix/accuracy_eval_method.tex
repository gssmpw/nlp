
\definecolor{key_1}{rgb}{0.0, 0.0, 0.6}
\definecolor{key_2}{rgb}{0.5, 0.3, 0.0}
\definecolor{key_3}{rgb}{0.0, 0.3, 0.0}


The accuracy measurement involves two sequential steps: (1) identifying solution trajectories and (2) determining the types of encountered enemies. The encountered enemies are identified based on the solution trajectories, simulating player traversal along these paths. Fig. \ref{fig:senario_method}a illustrates the process of determining solution trajectories. There are three possible solutions corresponding to the three keys in the level. The trajectory for \textcolor{key_1}{\textit{Key 1} (blue)} passes through \textcolor{key_2}{\textit{Key 2} (brown)}, rendering it ineligible as an independent solution. The algorithm excludes duplicated paths and prioritizes the shortest paths, based on the assumption that players prefer the shortest traversal routes. In contrast, the trajectories leading to \textcolor{key_2}{\textit{Key 2} (brown)} and \textcolor{key_3}{\textit{Key 3} (green)} do not overlap with those of other keys, making them independent solutions.
As a result, the number of solutions is two, corresponding to \textcolor{key_2}{\textit{Key 2} (brown)} and \textcolor{key_3}{\textit{Key 3} (green)}.



\begin{algorithm}[!h]
\caption{Enemy Encounter Detection Logic}
\label{alg:distinct_path}
\textbf{Require:} Path finding algorithm $\mathcal{P}$, enemy tiles $E$ \\
\textbf{Require:} Player position $P$, door position $D$, key position list $K$, generated level $S_{T}$

\begin{algorithmic}[1]
\State $p \gets [\text{False}, \text{False}, \text{False}]$ \Comment{Initialize prediction array}
\For{key $k \in K$}
    \State $\tau_{S\to k} \gets \mathcal{P}(S, k), \quad \tau_{k\to D} \gets \mathcal{P}(k, D)$
    \If{$\tau_{S\to k} = \emptyset \ \textbf{or}\ \tau_{k\to D} = \emptyset \ \textbf{or}\ \sum_{x \in \tau_{S\to k}} [x \in K] \neq 1$}
        \State \textbf{continue} \Comment{Not connected or duplicated path}
    \EndIf
    \State $\tau_{P\to D} \gets \tau_{S\to k} + \tau_{k\to D}$ \Comment{Solution trajectory}
    \State $k_5 \gets \text{5x5 kernel to match enemy tiles}$
    \For{$(x, y) \in \tau_{P\to D}$}
        \State $\text{kernel\_tiles} \gets \{S_{T}[x+i, y+j] \mid (i, j) \in k_5\}$
        \For{$t \in \text{kernel\_tiles}$}
            \If{$t \in E$}
                \State $p[t] \gets \text{True}$ \Comment{Update prediction}
            \EndIf
        \EndFor
    \EndFor
\EndFor
\State \textbf{return} Encountered enemies $p$
\end{algorithmic}
\end{algorithm}





Algorithm \ref{alg:distinct_path} describes the process of detecting enemies encountered from the player to door positions. For each key $k \in K$, the algorithm calculates the path $\tau_{S \to k}$ from the start $S$ to the key and the path $\tau_{k \to D}$ from the key to the door $D$ using the flood-fill pathfinding algorithm.
Determine whether there is connectivity between the player, key, and door, and whether there is a single unique key on the path, defining this as the solution trajectory. Using the coordinates of the trajectory path, perform a convolution operation with a 5 by 5-sized kernel to detect the presence of enemies. Update the prediction ($p$) based on the types of detected enemies.

\subsection{Vision-based Feedback Analysis}
\label{sec:vision_feedback}
The state-of-the-art Vision-Language Models (VLMs), such as \texttt{gpt-4o} with vision capabilities, can process visual inputs to perform tasks like question answering based on a given image.
This section investigate the reasoning capabilities of VLMs, specifically evaluating whether they have been trained on and can effectively reason about the distribution of game-rendered images.
Fig. \ref{fig:feedback_input_example} illustrates the difference between two input methods: textual input, where a 2D array is represented as plain text, and image input, where the same array is processed in visual form.
We evaluated how the modality of information provided when generating feedback influences changes in accuracy performance.



\begin{figure}[!h]
    \centering
    \begin{tabular}{cc} % 두 개의 열
        \includegraphics[width=0.20\textwidth]{figure/experiment/textual_input.pdf} & 
        \includegraphics[width=0.20\textwidth]{figure/experiment/image_input.png} \\
        (a) Text Input & (b) Image Input
    \end{tabular}
    \caption{Comparison of input methods: (a) textual input, where the 2D array is represented as plain text, and (b) image input, where the array is converted into an image format for processing.}
    \label{fig:feedback_input_example}
\end{figure}


\begin{table}[!h]
\caption{Ablation study results for framework architecture, comparing accuracy across prompt engineering methods and scenarios.}
\label{tab:feedback_input}
\begin{tabular}{p{1.5cm}|p{0.73cm}p{0.73cm}p{0.73cm}|p{0.73cm}p{0.73cm}p{0.73cm}}
\toprule
\textbf{Feedback Input} & \multicolumn{3}{c|}{\textbf{Text}} & \multicolumn{3}{c}{\textbf{Image}} \\
 & Base & +FB & $\Delta{}$ & Base & +FB & $\Delta{}$ \\
PE &  &  &  &  &  &  \\
\midrule
CoT & 0.033 & 0.156 & +0.122 & 0.031 & 0.157 & +0.126 \\
\bottomrule
\end{tabular}
\end{table}


Table \ref{tab:feedback_input} provides quantitative evidence of this improvement. In a chain-of-thought (CoT) reasoning framework, both textual and image inputs show performance gains with the addition of feedback (+FB). For textual input, the accuracy improves from 0.033 to 0.156, marking a gain of +0.122. Similarly, for image input, the accuracy increases from 0.031 to 0.157, with a comparable gain of +0.126. These results suggest that feedback is highly effective across both modalities, leading to substantial performance enhancements. Moreover, the comparable improvements indicate that both text and image inputs benefit similarly from feedback, emphasizing the importance of leveraging feedback mechanisms in multimodal frameworks to align reward functions with complex, goal-oriented tasks.
This also highlights the potential for evaluating and providing feedback on high-dimensional outputs, such as gameplay videos, to improve the quality of generated content in complex scenarios.