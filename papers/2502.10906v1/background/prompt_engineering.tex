Prompt engineering (PE) has become an effective methodology to enhance the performance of large language models (LLMs) in a gradient-free manner, with various techniques developed to improve their logical reasoning and planning abilities. These techniques can be described as utilizing different structures to extend thought processes, as illustrated in Fig. \ref{fig:reasoning_pe}. Compared to traditional input-output (IO) methods that query results in a single step, these structured approaches show superior performance in solving problems sequentially.

These techniques can be broadly categorized into three approaches based on chain, tree, and graph structures:

\textbf{Chain-of-Thought (CoT)} \cite{wei2022chain,kojima2022large} expands reasoning process with multiple steps to solve a problem in step-by-step. This method is particularly effective for problems requiring long-horizon reasoning. An enhanced variant, CoT with self-consistency (CoT-SC) \cite{wang2022self}, selects the most consistent response through a majority vote mechanism.

\textbf{Tree-of-Thoughts (ToT)} \cite{yao2024tree} expands reasoning in multiple directions, increasing the scope of exploration.
This fault-tolerant method, equipped with backtracking capability, not only broadens the exploration space but also allows recovery by redirecting to alternative paths when a wrong direction is taken, providing resilience against getting trapped in local optima.
The multi-path reasoning requires a fitness function to evaluate and select nodes for expansion, ensuring efficient exploration of the solution space.


\textbf{Graph-of-Thoughts (GoT)} \cite{besta2024graph} is an extended version of ToT, designed to improve the sample efficiency of reasoning by leveraging multiple thoughts generated during node expansion. When expanding a node, GoT retrives related nodes to enhance the reasoning process and ensure efficient exploration of the solution space.
