
Reward design has been a crucial aspect of content generation, but it remains heavily reliant on expert knowledge and is often time-consuming. Traditionally, reward functions have been either fixed through manually crafted code \cite{khalifa2020pcgrl} or conditionally generated based on predefined features, such as path length, determined by experts \cite{earle2021learning}. This dependency on predefined conditions limits the diversity of content that can be generated and constrains the flexibility of reward design. Furthermore, the need for domain-specific knowledge about the game reduces the accessibility of content generation algorithms, making them less adaptable to diverse applications.


To address this problem, \textit{ChatPCG} \cite{baek2024chatpcg} introduced an early stage architecture which leverages LLMs to automate the generation and refinement of reward functions.
The previous work demonstrated reward function generation for enriching multiplayer game content \cite{jeon2023raidenv}, training a generator agent to maximize role differentiation in a given game context.
ChatPCG proposed CoT-based self-alignment techniques to align the reward function to the specific scale of game variables, ensuring the outputs of reward function align to the LLM-generated insight.

\begin{figure*}[th]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/method/architecture_v5.pdf}
    \caption{Architecture of PCGRLLM framework. "Message icons \includegraphics[height=0.8em]{figure/method/message_icon.png}" indicate the use of language model ($\mathcal{M}$) in the context. Refer to Section \ref{sec:method} for detailed description.}
    \label{fig:architecture}
    \vspace{-0.34cm}
\end{figure*}


However, ChatPCG has an architectural limitation, namely the absence of feedback, as it lacks a refinement process based on the trained policy.
It is challenging to predict content generation outcomes from an arbitrary reward function since small parts of code lead to significant changes on outputs.
To address this issue, this paper improves upon the previous architecture by incorporating self-feedback mechanisms and enhancing the exploration of reward space using PEs. Through the enhanced architecture, the processes of reward generation and validation are fully automated, reducing human dependency in reward function design and supporting design creativity.