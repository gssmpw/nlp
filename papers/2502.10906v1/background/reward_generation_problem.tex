
The reward design problem \cite{lewis2010rewards} refers to the process of searching for a reward function ($R$) that maximizes the fitness score $F(\pi_{\text{trained}})$, where $\pi_{\text{trained}}$ represents a policy trained using $R$.
A DRL agent takes an action $a$ in a state $s\in{}S$ and receives a reward value $r=R(s|a)$.
The reward function can be designed as a sparse reward based on task success criteria or as an immediate dense reward for actions.
Recently, there has been significant interest in LLMs for reward generation, particularly in the robotics domain, where these models benefit from extensive human-level prior knowledge. Compared to using LLMs as policies, LLM-based reward generation is advantageous for reducing inference costs of large models and improving performance in low-level control tasks \cite{ahn2022can, singh2023progprompt}.

For instance, studies such as \cite{yu2023language, zeng2023learning, song2023self} have demonstrated the use of LLMs to generate reward functions for training DRL agents in robotics. By providing detailed environment descriptions and task-specific rules, these approaches facilitate reward generation for robotic tasks. Building on these efforts, \textit{Eureka} \cite{ma2023eureka} and \textit{Text2Reward} \cite{xie2023text2reward} introduced advanced techniques such as evolutionary search and reward reflection to achieve human-level reward design performance. These methods also incorporate mechanisms to evolve reward functions using human feedback or preferences.
% Notably, LLMs play a key role in refining reward functions by providing self-feedback based on the performance of trained policies.
Notably, LLMs take a leading role in reward function generation, either fully automating the process or assisting human creativity in designing effective rewards.
While most of these studies focus on robot control tasks by generating dense rewards for solving complex tasks, there remains a need to explore their potential applications in domains like content generation.

Meanwhile, the reasoning processes used to evolve reward functions have predominantly relied on conventional PE methods, such as CoT, which focus on incremental reasoning expansions and limit the exploration of the reward function space. Recognizing these constraints, this study aims to advance state-of-the-art PE techniques to significantly expand the scope of reward space exploration, unlocking broader possibilities for reward function optimization across diverse domains, including content generation.
