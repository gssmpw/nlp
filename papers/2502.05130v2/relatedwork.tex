\section{Related Works}
\subsection{Long-Form Audio Generation}
Related work mainly focus on training-based methods, including Language Models (LMs) \cite{agostinelli2023musiclm, copet2023simple, borsos2023audiolm} and Diffusion Models (DMs) \cite{evans2024long, Evans2024FastTL, tan2024litefocus}. LMs, typically based on auto-regressive architectures, face temporal causality constraints \cite{agostinelli2023musiclm, copet2023simple, borsos2023audiolm, kreuk2022audiogen}, leading to increasing accumulated errors and repetition issues when generating long audio. DMs mainly focus on 10-second durations \cite{liu2023audioldm, ghosal2023tango, majumder2024tango, Huang2023MakeAnAudio2T}, and while Make-An-Audio2 \cite{Huang2023MakeAnAudio2T} supports variable-length generation, it struggles with longer durations. More recently, Stable Audio \cite{Evans2024FastTL, evans2024long} is trained on long-form audio but it demands significant training costs and is sensitive to text prompts. Meanwhile, the exploration of joint diffusion generation in audio generation remains limited \cite{polyak2024movie}.

% Previous work on audio generation has primarily focused on training-based methods, which can be divided into Language Models (LMs) \cite{agostinelli2023musiclm, copet2023simple, borsos2023audiolm} and Diffusion Models (DMs) \cite{evans2024long, Evans2024FastTL, tan2024litefocus}. For LMs, most works adopt auto-regressive architectures, which suffer from temporal causality constraints \cite{agostinelli2023musiclm, copet2023simple, borsos2023audiolm, kreuk2022audiogen}. When applied to long audio generation, these models are observed to accumulate increasing errors, leading to more severe repetition issues. For DMs, most prior works focus on 10-second durations \cite{liu2023audioldm, ghosal2023tango, majumder2024tango, Huang2023MakeAnAudio2T}. Although Make-An-Audio2 \cite{Huang2023MakeAnAudio2T} allows variable-length generation, it shows weak capability for exploring longer durations. More recently, Stable Audio \cite{Evans2024FastTL, evans2024long} has been trained directly on long-form audio sequences, but this demands significant training costs and is sensitive to text prompts.  On the other hand, the exploration of joint diffusion generation in audio generation remains limited \cite{polyak2024movie}.


\subsection{Panorama Generation}
Early relative training-free methods \cite{Avrahami2022BlendedLD, Avrahami2021BlendedDF, esser2021taming} mainly apply painting techniques with DMs, which easily cause repetition issues and are constrained by temporal causality \cite{lee2023syncdiffusion, Quattrini2024MergingAS}. Recent advancements in panorama generation mainly based Joint Diffusion, as mentioned in Introduction. Meanwhile, we note a class of training-free methods, such as ScaleCrafter and DemoFusion \cite{he2023scalecrafter, du2024demofusion}, mainly applied to high-resolution image generation (e.g., upscaling portraits), which can also be considered a 2D length extrapolation problem. These methods focus on upscaling at the pixel level, while ensuring global structural coherence and avoiding object repetition. In contrast, this paper focuses on long-spectrum (e.g., concertos, soundscapes) and panorama generation (e.g., mountains, crowds), emphasizing smooth subview transitions, cross-view similarity-diversity balance, and extending more objects (e.g., pitches, harmonics).
% As these methods target a different problem, we focus our discussion on approaches relevant to long-spectrum and panorama generation.