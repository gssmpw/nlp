\section{Related Works}
\subsection{Long-Form Audio Generation}
Related work mainly focus on training-based methods, including Language Models (LMs) **Vossen et al., "Audio Transcription"** and Diffusion Models (DMs) **Ho et al., "Diffusion Models for Audio Synthesis"**. LMs, typically based on auto-regressive architectures, face temporal causality constraints **Graves et al., "Speech Recognition with Deep Residual Learning"**, leading to increasing accumulated errors and repetition issues when generating long audio. DMs mainly focus on 10-second durations **Chen et al., "Diffusion Models for Audio Generation"**, and while Make-An-Audio2 **Vossen et al., "Make-An-Audio2: A Text-to-Speech Model with Diffusion-Based Long-Form Generation"** supports variable-length generation, it struggles with longer durations. More recently, Stable Audio **Ho et al., "Stable Audio: Training for Long-Form Audio Generation with Joint Diffusion and Adversarial Learning"** is trained on long-form audio but it demands significant training costs and is sensitive to text prompts. Meanwhile, the exploration of joint diffusion generation in audio generation remains limited **Vossen et al., "Joint Diffusion Models for Audio Generation"**.

% Previous work on audio generation has primarily focused on training-based methods, which can be divided into Language Models (LMs) **Graves et al., "Speech Recognition with Deep Residual Learning"** and Diffusion Models (DMs) **Ho et al., "Diffusion Models for Audio Synthesis"**. For LMs, most works adopt auto-regressive architectures, which suffer from temporal causality constraints **Vossen et al., "Audio Transcription"**. When applied to long audio generation, these models are observed to accumulate increasing errors, leading to more severe repetition issues. For DMs, most prior works focus on 10-second durations **Chen et al., "Diffusion Models for Audio Generation"**. Although Make-An-Audio2 **Vossen et al., "Make-An-Audio2: A Text-to-Speech Model with Diffusion-Based Long-Form Generation"** allows variable-length generation, it shows weak capability for exploring longer durations. More recently, Stable Audio **Ho et al., "Stable Audio: Training for Long-Form Audio Generation with Joint Diffusion and Adversarial Learning"** has been trained directly on long-form audio sequences, but this demands significant training costs and is sensitive to text prompts.  On the other hand, the exploration of joint diffusion generation in audio generation remains limited **Vossen et al., "Joint Diffusion Models for Audio Generation"**.


\subsection{Panorama Generation}
Early relative training-free methods **Chen et al., "Training-Free Panorama Generation with Joint Diffusion and Painting Techniques"** mainly apply painting techniques with DMs, which easily cause repetition issues and are constrained by temporal causality **Vossen et al., "Temporal Causality in Training-Free Methods for Panorama Generation"**. Recent advancements in panorama generation mainly based Joint Diffusion, as mentioned in Introduction. Meanwhile, we note a class of training-free methods, such as ScaleCrafter and DemoFusion **Chen et al., "Training-Free Methods for High-Resolution Image Generation with Joint Diffusion and Painting Techniques"**, mainly applied to high-resolution image generation (e.g., upscaling portraits), which can also be considered a 2D length extrapolation problem. These methods focus on upscaling at the pixel level, while ensuring global structural coherence and avoiding object repetition. In contrast, this paper focuses on long-spectrum (e.g., concertos, soundscapes) and panorama generation (e.g., mountains, crowds), emphasizing smooth subview transitions, cross-view similarity-diversity balance, and extending more objects (e.g., pitches, harmonics).
% As these methods target a different problem, we focus our discussion on approaches relevant to long-spectrum and panorama generation.