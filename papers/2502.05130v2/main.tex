% ICCV 2025 Paper Template

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{iccv}              % To produce the CAMERA-READY version
% \usepackage[review]{iccv}      % To produce the REVIEW version
\usepackage[pagenumbers]{iccv} % To force page numbers, e.g., for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}
\usepackage{lipsum}
\usepackage[ruled,vlined]{algorithm2e} 
% \SetNlSty{}{}{}
% \SetNlSkip{0em}
% \SetAlgoNlRelativeSize{0}  % 去掉行号的相对大小
% \SetAlgoIndent{0pt}        % 去除缩进
\usepackage{calc}
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{xcolor}
\usepackage{booktabs} % for professional tables
\usepackage{colortbl}  %彩色表格需要加载的宏包
\usepackage{xcolor}
\usepackage{hyperref}
% \usepackage[square,sort,comma,numbers]{natbib}
\usepackage{multirow}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{pifont}
\renewcommand{\thefootnote}{}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g., with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{iccvblue}{rgb}{0.21,0.49,0.74}
% \usepackage[pagebackref,breaklinks,colorlinks,allcolors=iccvblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{4251} % *** Enter the Paper ID here
\def\confName{ICCV}
\def\confYear{2025}


\author{
    Yusheng Dai$^{1*}$, Chenxi Wang$^{1*}$, Chang  Li$^1$,
    Chen Wang$^2$, Kewei Li$^1$, Jun Du$^{1\dag}$, \\ Lei Sun$^3$, Jianqing Gao$^3$, Ruoyu Wang$^1$, Jiefeng Ma$^1$ \\
    $^1$ University of Science and Technology of China, Heifei, China \\
     $^2$ Tsinghua University, Beijing, China $^3$ iFlytek Research, Heifei, China 
}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Latent Swap Joint Diffusion for 2D Long-Form Latent Generation}
%%%%%%%%% AUTHORS - PLEASE UPDATE
% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\begin{document}
\maketitle
\footnotetext{ Equal Contribution. Corresponding author.}
\setcounter{footnote}{0}
\renewcommand{\thefootnote}{\arabic{footnote}}

\begin{abstract}

\vspace{-12pt} % Ensure this negative spacing is intentional and valid
This paper introduces Swap Forward (SaFa), a modality-agnostic and efficient method to generate seamless and coherence long spectrum and panorama through latent swap joint diffusion across multi-views. We first investigate the spectrum aliasing problem in spectrum-based audio generation caused by existing joint diffusion methods. Through a comparative analysis of the VAE latent representation of Mel-spectra and RGB images, we identify that the failure arises from excessive suppression of high-frequency components during the spectrum denoising process due to the averaging operator. To address this issue, we propose Self-Loop Latent Swap, a frame-level bidirectional swap applied to the overlapping region of adjacent views. Leveraging stepwise differentiated trajectories of adjacent subviews, this swap operator adaptively enhances high-frequency components and avoid spectrum distortion. Furthermore, to improve global cross-view consistency in non-overlapping regions, we introduce Reference-Guided Latent Swap, a unidirectional latent swap operator that provides a centralized reference trajectory to synchronize subview diffusions. By refining swap timing and intervals, we can achieve a cross-view similarity-diversity balance in a forward-only manner. Quantitative and qualitative experiments demonstrate that SaFa significantly outperforms existing joint diffusion methods and even training-based methods in audio generation using both U-Net and DiT models, along with effective longer length adaptation. It also adapts well to panorama generation, achieving comparable performance with 2 $\sim$ 20 $\times$ faster speed and greater model generalizability. More generation demos are available at \href{https://swapforward.github.io/}{https://swapforward.github.io/}.

\end{abstract}

\vspace{-15pt}
\section{Introduction}
\vspace{-5pt}
\label{sec:introduction}


\begin{table*}[!t]

% \caption{Swap Forward demonstrates strong \textit{generalization} (adapting to both spectrum and image generation, U-Net and DiT architectures, and fixed or flexible attention window sizes), \textit{simplicity} (applying only two latent swap operators in a forward-only manner), and \textit{efficiency} ($2\sim20\times$ faster and $3\sim4\times$ speed) to generate seamless and coherent long-form audio and panorama.}


\vspace{-5pt}
\centering
\small
\setlength{\tabcolsep}{3.5mm}
% \setlength{\belowcaptionskip}{-10pt} % 图表标题之下的间距
\begin{tabular}{lcccccc}
\toprule[1pt]
% \textbf{Method}             & \textbf{Spectrum Adaptation} & \textbf{Forward-Only} & \textbf{Guidance Method}                    & \textbf{Fixed Attention Window} & \textbf{DiT Adaptation} & \textbf{Runtime(s)} & \textbf{Memory(G)} \\
\textbf{Method}    & \textbf{\begin{tabular}[c]{@{}c@{}}Spectrum\\ Adaptation\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Forward\\ Only\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Fixed Attention\\ Window\end{tabular}}  & \textbf{\begin{tabular}[c]{@{}c@{}}DiT\\ Adaptation\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Guidance\\ Method\end{tabular}} & \textbf{Runtime (s)}  \\
\hline
MultiDiffusion      & \text{\ding{55}}      & \text{\ding{52}}           & \text{\ding{52}}      & \text{\ding{52}}    & -                & 37.71            \\
MAD             & \text{\ding{55}}      & \text{\ding{52}}          & \text{\ding{55}}      & \text{\ding{55}}    & Self-Attention   & 41.82           \\
SyncDiffusion  & \text{\ding{55}}      & \text{\ding{55}}         & \text{\ding{52}}      & \text{\ding{52}}    & Gradient Descent & 390.63        \\
\textbf{Swap Forward} & \text{\ding{52}}      & \text{\ding{52}}        & \text{\ding{52}}       & \text{\ding{52}}     & Latent Swap   & \textbf{19.15}           \\           
\bottomrule[1pt]

\end{tabular}
\vspace{-5pt}

\caption{Swap Forward demonstrates strong \textit{generalization} (adapting to both spectrum and image generation, U-Net and DiT architectures, and fixed or flexible attention window sizes), \textit{simplicity} (applying only two latent swap operators in a forward-only manner), and \textit{efficiency} ($2 \sim 20\times$ faster inference time with less subview count) to generate seamless and coherent long-form audio and panorama.}
\vspace{-15pt}

\label{tab:com}
\end{table*}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=1\columnwidth]{25imgs/fig0.pdf}
%     \vspace{-15pt}
%     \caption{Mel-spectra generated by averaging operator based joint diffusion, e.g., MD \cite{BarTal2023MultiDiffusionFD}. White narrow line areas represent overlapping regions that lack spectral details with monotonous tails.}
%     \label{fig:head}
%     \vspace{-15pt}
% \end{figure}

Diffusion models learn to generate data by progressively adding noise to existing samples and subsequently reversing this process to recover the original data. \cite{ho2020denoising, song2020score}. They originally achieved remarkable success in text-to-image \cite{ho2020denoising, song2020score, song2020denoising} and have rapidly expanded into text-to-video \cite{ he2022latent, wang2023gen, qiu2023freenoise, kim2024fifo} and text-to-audio \cite{Liu2023AudioLDMTG, ghosal2023text, Huang2023MakeAnAudio2T, Evans2024FastTL,  Huang2023Noise2MusicTM, li2024quality, liu2024audioldm}. However, up until now, it is still challenging to model the variable physical world when training diffusion models on specific construction data. Therefore, expanding pretrained diffusion models to generate a broader range of data types has become increasingly attractive. One significant challenge is length extrapolation, which aims to generate images of arbitrary shapes or audio of varying lengths using diffusion models trained on constrained-size data. 
% One significant challenge in diffusion models is length extrapolation, which aims to generate varying lengths long-form output using diffusion models trained on constrained-length data.
% Diffusion models learn to generate data by progressively adding noise to existing samples and subsequently reversing this process to recover the original data. \cite{ho2020denoising, song2020score}. They originally achieved remarkable success in text-to-image \cite{ho2020denoising, song2020score, song2020denoising} and have rapidly expanded into text-to-video and text-to-audio \cite{Liu2023AudioLDMTG, ghosal2023text, Huang2023MakeAnAudio2T, Evans2024FastTL,  Huang2023Noise2MusicTM, li2024quality, liu2024audioldm}. One significant challenge in diffusion models is length extrapolation, which aims to generate images of arbitrary shapes or audio of varying lengths using diffusion models trained on constrained-size data.


For text-to-image generation, panorama generation can be approached as a length extrapolation problem, involving extensive pixel or latent sequences with extreme aspect ratios. Related training-free methods generally fall into two categories: autoregressive approaches \cite{lu2024autoregressive, Avrahami2022BlendedLD, Kim2024FIFODiffusionGI, liu2025panofree} and joint diffusion approaches \cite{jimenez2023mixture, Quattrini2024MergingAS, lee2023syncdiffusion, zhang2023diffcollage, BarTal2023MultiDiffusionFD}. Compared with the former, joint diffusion methods have attracted broader research interest in recent years due to their high efficiency, lower error accumulation, and lack of temporal causality constraints. Generally, related work mainly focuses on two issues: achieving smooth transitions between adjacent subviews and maintaining cross-view consistency, e.g., color and style, in distant subviews. 

As representative work \cite{jimenez2023mixture, BarTal2023MultiDiffusionFD}, MultiDiffusion (MD) achieves smooth transitions between adjacent views by averaging their noisy latent maps at each denoising step. In this case, a new joint diffusion process is optimized by synchronizing several subview diffusion processes to produce a globally coherent output. However, due to a lack of explicit guidance, this approach relies on a high overlap rate that suffering from low efficiency, but still lead to global perception mismatches  across subviews. As an advanced version of MD, Merge-Attention-Diffuse (MAD) \cite{Quattrini2024MergingAS} merges and splits subview latent maps around the self-attention layer, enabling global attention over the entire panorama. However, the merge operation increases the self-attention window length, forcing the model to handle longer token sequences and repetitive position problems. Thus, it is observed to lead to significant performance degradation in DiT architectures in Tab.~\ref{tab:qual} and in spectrum generation models trained on short-length clips in Tab.~\ref{tab:audio1}. SyncDiffusion \cite{lee2023syncdiffusion} proposes a promising guidance approach to enhance the global coherence by minimizing perceptual similarity (LPIPS loss) \cite{zhang2018unreasonable} between each subview and the reference image. However, the additional forward and backward propagation can significantly increase computation and time costs. Meanwile, LPIPS loss is observed to be insensitive to \textit{intermediate} denoised mel-spectrograms. \textit{Consequently, there remains a lack of efficient and architecture-agnostic joint diffusion methods that can achieve global cross-view consistent in both panorama and audio generation.}

For audio generation, long-form soundscapes and background music are in high demand for ambiance enhancement in real-life applications (e.g., in-car audio, sleep aids) and digital products (e.g., movies, video games). Most related works focus on training-based long-form audio generation (AudioGen \cite{kreuk2022audiogen} and Stable audio \cite{evans2024long}), which demands significant training costs and sensitive to text prompts. Few studies have explored the applicability of joint diffusion methods in spectrum-based audio generation. In early experiment, we try to apply existing joint diffusion methods to spectrum-based audio generation \cite{polyak2024movie}. However, as shown in as shown in Fig.~\ref{fig:head} and Fig.~\ref{fig:qual}, we observe a spectrum aliasing phenomenon: \textit{the generated spectrograms exhibit low time-frequency resolution and distortion in overlap transition regions (narrow white bands), leading to visual blurriness and low-quality audio with distortion and monotonous tails.} This phenomenon is particularly evident in spectrally rich audio including more spectral details, e.g., soundscapes and concertos.

To address these issues, in Section \ref{sec:Spectrum}, we first investigate above spectrum aliasing phenomenon in spectrum generation caused by existing joint diffusion. Specifically, leveraging the connectivity inherent of the VAE latent space, we provide a comparative analysis of the VAE latent representations across the Mel-specta and RGB images connected with their original feature, exhibiting the high-frequency variability of the spectrum latent map. Through Fourier analysis, we further uncover that the failure stems from the excessive suppression of high-frequency components in the spectrum denoising process caused by the latent averaging operator.

Based on these findings, in Section \ref{sec:latent swap}, we propose the Self-loop Latent Swap operator, a frame-level bidirectional latent swap operator applied in the overlapping regions of adjacent subviews. Relying on the stepwise differentiated trajectories of adjacent subviews, it successfully solves the spectrum aliasing problem by adaptively enhancing spectra high-frequency details in the overlapping regions. Moreover, to achieve cross-view consistency in the non-overlapping regions, we further propose the Reference-Guided Latent Swap, a unidirectional latent swap operator that provides a centralized reference trajectory for each subview diffusion in early denoising steps. By adjusting the timing and interval of the swap operation, we can balance the similarity-diversity trade-off across subviews in a forward-only manner. \textit{Through these two latent swap operators, SaFa not only fills the gap for existing joint diffusion methods in long spectrum generation, but also acts as a highly efficient alternative for generating cross-view consistent panoramas with significantly reduced time cost and subview number.}

Finally, in Section \ref{sec:exp}, we present extensive quantitative and qualitative experiments on both long-form audio and panorama generation tasks with U-Net and DiT diffusion models. Compared to state-of-the-art (SOTA) joint diffusion methods and even training-based approaches, SaFa demonstrates much greater simplicity and efficiency, achieving superior generation quality through only two fundamental swap operators, ensuring smooth transitions and cross-view consistency with significantly reduced time cost. As shown in Tab.~\ref{tab:com}, we further provide a detailed key point comparison of SaFa and existing joint diffusion methods.


% To address these issues, in Section \ref{sec:Spectrum}, we first investigate the causes of overlap distortion in the mel-spectra generated by current joint diffusion methods. Given the high-frequency variability inherent in mel-spectrum, we design a spectrum-inspired latent swap operator to connect the denoising trajectories of different subviews collaboratively by exchanging their latent maps at the frame level during each denoising step. Surprisingly, we find that this approach performs well not only in spectrum generation but also in general panorama images. In Sections \ref{sec:Self-Loop} and \ref{sec:Reference-Guided}, by adjusting the source, timing, and region of the swap operation, we design \textit{Self-Loop Latent Swap} and \textit{Reference-Guided Latent Swap} to ensure smooth transitions between adjacent views and offer centralized guidance for unified coherence, without additional time or computational costs. 
% Specifically, the Self-Loop Swap is a frame-level bidirectional latent swap operation that occurs in the overlapping range of adjacent subviews. For each subview, the operator downsamples and preserves the frame-wise denoised latent and interpolating with swapped-out latents from adjacent frames. Compared to the common averaging operation, this operation achieves better-blended transitions and preserves more details in both image and spectrum generation. To alleviate mismatches in distant views, the Reference-Guided Latent Swap is employed to provide centralized guidance. This operation uses an independent reference denoising process to guide other reverse processes through unidirectional latent swaps between the reference view and each subview in the central region. By adjusting the occurrence stage and frequency of swap operation, this approach maintains perceptual coherence while avoiding repeated patterns.

% In Section \ref{sec:exp}, we present extensive quantitative and qualitative experiments on panorama image, long-form audio and long-form music generation. Our method outperforms state-of-the-art (SOTA) approaches in long-form audio and music generation, achieving better generality, quality, and coherence with latent averaging. With two simple swap operations, it matches SOTA performance in panorama generation without additional computational or time costs, addressing gaps in low-latency scenarios, DiT architectures, and fixed attention windows as shown in Tab. \ref{tab:com}.

\section{Related Works}
\subsection{Long-Form Audio Generation}
Related work mainly focus on training-based methods, including Language Models (LMs) \cite{agostinelli2023musiclm, copet2023simple, borsos2023audiolm} and Diffusion Models (DMs) \cite{evans2024long, Evans2024FastTL, tan2024litefocus}. LMs, typically based on auto-regressive architectures, face temporal causality constraints \cite{agostinelli2023musiclm, copet2023simple, borsos2023audiolm, kreuk2022audiogen}, leading to increasing accumulated errors and repetition issues when generating long audio. DMs mainly focus on 10-second durations \cite{liu2023audioldm, ghosal2023tango, majumder2024tango, Huang2023MakeAnAudio2T}, and while Make-An-Audio2 \cite{Huang2023MakeAnAudio2T} supports variable-length generation, it struggles with longer durations. More recently, Stable Audio \cite{Evans2024FastTL, evans2024long} is trained on long-form audio but it demands significant training costs and is sensitive to text prompts. Meanwhile, the exploration of joint diffusion generation in audio generation remains limited \cite{polyak2024movie}.

% Previous work on audio generation has primarily focused on training-based methods, which can be divided into Language Models (LMs) \cite{agostinelli2023musiclm, copet2023simple, borsos2023audiolm} and Diffusion Models (DMs) \cite{evans2024long, Evans2024FastTL, tan2024litefocus}. For LMs, most works adopt auto-regressive architectures, which suffer from temporal causality constraints \cite{agostinelli2023musiclm, copet2023simple, borsos2023audiolm, kreuk2022audiogen}. When applied to long audio generation, these models are observed to accumulate increasing errors, leading to more severe repetition issues. For DMs, most prior works focus on 10-second durations \cite{liu2023audioldm, ghosal2023tango, majumder2024tango, Huang2023MakeAnAudio2T}. Although Make-An-Audio2 \cite{Huang2023MakeAnAudio2T} allows variable-length generation, it shows weak capability for exploring longer durations. More recently, Stable Audio \cite{Evans2024FastTL, evans2024long} has been trained directly on long-form audio sequences, but this demands significant training costs and is sensitive to text prompts.  On the other hand, the exploration of joint diffusion generation in audio generation remains limited \cite{polyak2024movie}.


\subsection{Panorama Generation}
Early relative training-free methods \cite{Avrahami2022BlendedLD, Avrahami2021BlendedDF, esser2021taming} mainly apply painting techniques with DMs, which easily cause repetition issues and are constrained by temporal causality \cite{lee2023syncdiffusion, Quattrini2024MergingAS}. Recent advancements in panorama generation mainly based Joint Diffusion, as mentioned in Introduction. Meanwhile, we note a class of training-free methods, such as ScaleCrafter and DemoFusion \cite{he2023scalecrafter, du2024demofusion}, mainly applied to high-resolution image generation (e.g., upscaling portraits), which can also be considered a 2D length extrapolation problem. These methods focus on upscaling at the pixel level, while ensuring global structural coherence and avoiding object repetition. In contrast, this paper focuses on long-spectrum (e.g., concertos, soundscapes) and panorama generation (e.g., mountains, crowds), emphasizing smooth subview transitions, cross-view similarity-diversity balance, and extending more objects (e.g., pitches, harmonics).
% As these methods target a different problem, we focus our discussion on approaches relevant to long-spectrum and panorama generation.


\section{Methodology}
% \subsection{Preliminaries}
% We first give a brief introduction on conditional diffusion models. Given a conditional signal \(y\) (e.g., text embedding), denoising Diffusion Probabilistic Models \cite{ho2020denoising} (DDPMs) aim to model the corresponding data distribution \(q(z_0 | y)\) across various modalities, including image and audio, through the model distribution \(p_\theta(z_0 | y)\). Here, \(z_0\) represents the latent prior of the original data \(x_0\). Conditional diffusion models comprise a forward-backward process. In the forward process, we progressively corrupt the clean data \(z_0\) into standard Gaussian noise \(z_T \sim \mathcal{N}(0, 1)\) with the transition probability:
% \begin{equation}
% q(z_t | z_{t-1}) = \mathcal{N}(z_t; \sqrt{1 - \beta_t} z_{t-1}, \beta_t I),
% \end{equation}
% where \(\{ \beta_t \}_{t=1}^T\) is a predefined noise schedule. The reverse process begins from \(z_T \sim \mathcal{N}(0, 1)\) and gradually removes noise at each timestep with
% % \begin{equation}
% \(
% p_{\theta}(z_{0:T}) = p(z_T) \prod_{t=1}^{T} p_{\theta}(z_{t-1} | z_t, y).
% % \end{equation}
% \)
% The training objective for diffusion models is to maximize the evidence lower bound (ELBO) of the likelihood \(p_\theta(z_0)\), which can be optimized with a reweighted noise estimation objective:
% \begin{equation}
% \mathcal{L}(\theta) = \mathbb{E}_{z_0, \epsilon, t} \left\| \epsilon - \epsilon_{\theta} \left( \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t, y \right) \right\|_2^2.
% \end{equation}
% In this work, we use deterministic sampling Denoising Diffusion Implict Models \cite{song2020denoising} (DDIMs) to accelerate the inference process and focus on the latent fusion mechanisms during this sampling process.
% \subsection{Denoising Diffusion Implicit Models}
% Denoising Diffusion Implicit Models (DDIMs) provide a deterministic sampling process while treating the forward process as a non-Markovian process, taking the prior \(z_0\) into account during sampling. Specifically, DDIM first predicts \(\hat{z_0}\) by estimating noise from equation \ref{forward}:
% \begin{equation}
% \hat{z_0} = \frac{z_t - \sqrt{1 - \alpha_t} \epsilon_\theta(z_t, t)}{\sqrt{\alpha_t}}.
% \end{equation}
% Using a subset of timesteps \(\mathcal{T} = [\tau_1, \ldots, \tau_N]\) from \([1, \ldots, T]\), where \(\sigma_t^2 = \frac{1 - \alpha_{t-1}}{1 - \alpha_t} \cdot \left(1 - \frac{\alpha_t}{\alpha_{t-1}}\right)\), the reverse process of DDIM provides:
% \begin{equation}
% q(z_s | z_t, \hat{z_0}) = \mathcal{N} \left( \sqrt{\alpha_s} \hat{z_0} + \frac{\sqrt{1 - \alpha_s - \sigma_t^2}}{\sqrt{1 - \alpha_t}} (z_t - \sqrt{\alpha_t} \hat{z_0}), \, \sigma_t^2 I \right).
% \end{equation}
% We can obtain the estimated \(z_0\) through iteratively sampling in \(\mathcal{T}\). \textcolor{red}{CFG}

% For simplicity, in the following discussion, the term image refers to either an RGB image or its corresponding noisy latent representation, unless stated otherwise. 

\begin{figure*}[!t]
    \centering
    % \setlength{\belowcaptionskip}{-30pt} % 图表标题之下的间距
    % \hspace{-2mm}
\includegraphics[width=2.1\columnwidth]{25imgs/analysis.pdf}
 \vspace{-15pt}
    \caption{(a) The channel-wise linear approximation between original feature downsampling and its VAE latent ensures their Connectivity Inheritance, maintaining connectivity and structure consistency. (b) (c) Relative log-amplitude curves in 2D Fourier analysis for non-overlapping regions (Reference) and overlapping regions of the denoised spectrum latent map with the averaging and latent swap operators.}
\label{analysis}
\vspace{-15pt}
\end{figure*}



 
\subsection{Joint Diffusion for Long Latent Generation}
Diffusion models initially operate in the original feature space \cite{ho2020denoising} and have recently been extended to the VAE latent space, achieving higher fidelity and compression rates \cite{rombach2022high}. For most modality generation tasks, they can be reformulated as a general Latent Diffusion process in the corresponding VAE latent space. As show in the Fig.~\ref{fig:middle} left, given a reference latent diffusion model $\Phi$ and conditions $\{y_i\}^n_{i=0}$, our target is to generate a 2D long-form latent map $ J \in \mathbb{R}^{C \times H \times W}$ ($H \ll W$) through a joint diffusion process $\Psi$ by merging a sequence of subview latent maps $\{X^i\}_{i=1}^n \in \mathbb{R}^{C \times H_x \times W_x}$. The subview mapping $F_i$ maps the overview $J$ to subview $X_i$ as:
% \begin{equation}
% F_i \leftrightarrow R(F_i): J \rightarrow X_i , i \in [n]
% \end{equation}
\begin{equation}
F_i : J \rightarrow X_i , i \in [n]
\end{equation}
which can be considered as a 1D sliding window process. Considering that commonly in the spectrum $H \ll W$, we simplify the overlap mapping $I_{i, i+1}$ from overall $J$ to the overlap region of the adjacent subviews $X_i$ and $X_{i+1}$  as:
\begin{equation}
I_{i, i+1} \leftrightarrow F_i \cap F_{i+1}
\label{I}
\end{equation}
and the non-overlap mapping $M_{i}$ of subview $X_i$ as:
\begin{equation}
    M_{i} \leftrightarrow F_i - \left(F_{i-1} \cup F_{i+1}\right)
\end{equation}
For a joint diffusion step $\Psi\left(J_t \mid Y\right)$ at time-step $t$, most previous work \cite{BarTal2023MultiDiffusionFD,lee2023syncdiffusion, jimenez2023mixture} apply averaging operator to synchronize different denoising trajectories in overlap regions,
\begin{equation}
\small
\Psi\left(J_t \mid Y\right)=\sum_{i=1}^n \frac{F_i^{-1}\left(W_i\right)}{\sum_{j=1}^n F_j^{-1}\left(W_j\right)} \odot F_i^{-1}\left(\Phi(X_t^i \mid y_i)\right)
\label{trajectory}
\end{equation}
where $W_i$ is the weight matrix of subview $X_i$ that implenmented with averaging in overlapping regions and exclusivity in non-overlapping regions. $F^{-1}$ is reversed mapping of $F$ from J to $X_i$ with zero-padding operation.   
% We take MultiDiffusion \cite{BarTal2023MultiDiffusionFD} as an example to introduce joint diffusion methods. MultiDiffusion generates a panorama image $ z \in \mathbb{R}^{C \times H_z \times W_z} $ by merging a set of fixed-size subview images $ x^{(i)} \in \mathbb{R}^{C 
% \times H_x \times W_x } $ at each denoising step. At the beginning of the denoising process, the random noise from the subview images can be cropped sequentially from the global image using a sliding window operation with overlap rate $r_o$. The process can be defined as the mapping $T_{z \to i}: \mathbb{R}^{C \times H_z \times W_z } \to \mathbb{R}^{C \times H_x \times W_x }$. Then these subview images are denoised independently in the next reverse step. In each step $t \in [0,T] $, MultiDiffusion optimizes a new joint diffusion by weighted sum the latent in the overlap region of the subview. Without loss of generality, we define a weight matrix $w^{(i)} \in \mathbb{R}^{C \times H_x \times W_x}$ for each subview image with elements $\quad 0 \leq w^{(i)}_{ij} \leq 1$, indicating the contribution of each pixel of the subview. Specifically, MultiDiffusion \cite{BarTal2023MultiDiffusionFD} adopts a uniform coefficient to apply an averaging operator, representing a globally equal contribution. The function $T_{i \to z}: \mathbb{R}^{C \times H_x \times W_x } \to \mathbb{R}^{C \times H_z \times W_z }$ maps each subview back to the panorama space by zero-padding. Therefore, the averaging operation to obtain integrated panorama image can be formulated as:
% \begin{equation}
% z_t = \frac{\sum_i T_{i \to z}(x^{(i)}_t \odot w^{(i)})}{\sum_i T_{i \to z}(w^{(i)})}, 
% \end{equation}
% where $\sum_i T_{i \to z}(w^{(i)})=1_{H_z \times W_z}$. Similarly to the initial stage, $z_t$ will be divided into new subview images $\tilde{x}^{(i)}_t$ and start the next repeated cycle.



\label{sec:Method}
\subsection{Comparative Analysis of VAE Representations} 
\label{sec:Spectrum}
To address the spectrum aliasing problem in overlap regions as shown in Fig.~\ref{fig:head}, we first conduct a comparative analysis of the spectrum and image VAE representation, which received limited prior investigation. Initially, our main insight stems from the differences between the original features. The mel-spectrogram is a 2D time-frequency representation of the audio signal, showing amplitude variability across frequency bands over time. Compared to RGB images, mel-spectrograms exhibit high-frequency variability, with amplitude distributions in time-frequency bins showing weak connectivity, marked by sparsity and discreteness, lacking the continuous contours seen in normal images. 

Further, in Fig.~\ref{analysis} (a), we observe that this discrepancy can extend to the VAE latent sapce due to the Connectivity Inheritance between the original feature and its VAE latent. And the inheritance is governed by a learnable, global,  channel-wise linear approximation mapping $W_c$, which is specific to each VAE model. Specifically, given an image or spectrum $X \in \mathbb{R}^{C_x \times W_x \times H_x}$ and its VAE latent representation $Z \in \mathbb{R}^{C_z \times W_z \times H_z}$, we define a learnable constant linear mapping $W_c \in \mathbb{R}^{C_x \times C_z}$ along the channel dimension , satisfying the following approximation:
\begin{equation}
\text{Downsample}(X) \approx W_c \cdot Z
\end{equation}
Correspondingly, the inverse mapping can be fomulated as:
\begin{equation}
Z \approx (W_c^T W_c)^{-1} W_c^T \cdot \text{Downsample}(X)
\end{equation}
Such linear approximation mapping along the channel dimension ensures connectivity and structural alignment between the latent map and the original features. Consequently, as shown in Fig.~\ref{analysis} (a)  the spectrum latent consistently exhibits low connectivity and high-frequency variability compared to image latent. So far, we have validated the Connectivity Inheritance of VAE latents across most convolution-based VAEs used in image generation (SD 2.0 \cite{rombach2022high} and SD 3.5 \cite{esser2024scaling}) and audio generation ( AudioLDM \cite{liu2023audioldm}, Tango \cite{ghosal2023tango} and Make-An-Audio2 \cite{Huang2023MakeAnAudio2T}), which can be attributed to the local spatial correspondence in convolution operations.

% Further, as shown in Fig.~\ref{analysis} (a), such discrepancy can extend to the VAE latent due to their connectivity inheritance based on the channel-wise linear mapping between the original feature and its VAE representation, which is first observed in image generation and has been served as a fast previewer to replace complex VAE decoders \footnote{\href{https://discuss.huggingface.co/t/decoding-latents-to-rgb-without-upscaling/23204/2}{https://discuss.huggingface.co/t/decoding-latents-to-rgb-without-upscaling/23204/2}}. In this paper, we provide a formal formulation and extend it to spectrum generation.
% Specifically, given an image or spectrum $X \in \mathbb{R}^{C_x \times W_x \times H_x}$ and its VAE latent representation $Z \in \mathbb{R}^{C_z \times W_z \times H_z}$, we define a constant linear mapping $W \in \mathbb{R}^{C_x \times C_z}$ along the channel dimension satisfies the following approximation:
% \begin{equation}
% \text{Downsample}(X) \approx W \cdot Z
% \end{equation}
% Correspondingly, the inverse process can be described as:
% \begin{equation}
% Z \approx (W^T W)^{-1} W^T \cdot \text{Downsample}(X)
% \end{equation}
% Such linear mapping along the dimension ensures that latent map inherits the connectivity and high-frequency variability of the original features, visually structural alignment.

We further analyze the impact of the averaging operator on spectrum latents diffusion via Fourier analysis. As shown in Fig.~\ref{analysis}(b), unlike RGB images \cite{si2024freeu}, the relative amplitude curve of the reference non-overlapping regions spectrum exhibits dynamic fluctuations but lacks a clear reduction in high-frequency components during denoising, due to its high-frequency variability. However, in overlap regions, the step-wise averaging operation progressively smooths high-frequency components, leading to a more pronounced decline, particularly in later denoising steps. This causes spectral detail loss and spectrum aliasing, manifesting as visual blurriness and auditory distortion or monotonous tails.

% However, when applying the averaging operator to image generation, the detail distortion is not as visually noticeable, as it is masked by the alignment low-frequency components iterms of global contours and colors.

% As a supplementary note, although the above discussion begins in the original data space, an approximate temporal region mapping from VAE space to the original space has been observed in previous works \cite{jimenez2023mixture,rombach2022high}. This is primarily because the encoder and decoder of the VAE consist mostly of convolution layers, which inherit the high-frequency variability in the corresponding VAE latent space. The effectiveness of our method in experiments (Section \ref{sec:exp}) further supports this approximate mapping.

% We first analyze the reasons behind the distortion of overlap region in spectrum generated by multiview joint diffusion method shown in Fig. \ref{fig:head}. Similar to image generation, most audio diffusion methods are based on the Mel spectrum or its corresponding latent representation, which can be encoded from or decoded to the spectrum by a pretrained VAE encoder E and decoder D. Physically, unlike general images that comprise a 2D grid of pixels with RGB channels, the Mel spectrum is a 2D time-frequency representation of the audio signal, illustrating how sound energy varies across different frequency bands over time. Typically, the horizontal axis represents time, the vertical axis represents frequency, and the values indicate amplitude.

% Different from RGB images, we emphasize two key properties of the Mel spectrum that can potentially lead to unsmooth transitions. (1) high-grequency variability — The time-frequency bins in the spectrum demonstrate weak connectivity, exhibiting sparsity and discreteness without an overall contour or shape. (2) Nonlinearity — the Mel filter and logarithmic operation are applied to the linear amplitude spectrum to generate a Mel spectrum that aligns with human sound perception characteristics. In general image generation, most multi-view joint diffusion methods apply averaging operations on overlapping regions across subviews to ensure seamless transitions, which effectively aligns the overall contours and colors in the image representation. When applied to Mel spectrum generation, averaging entire overlap regions can confuse the time-frequency bins and diminish their high-frequency variability. In addition, such linear operations do not adapt to the nonlinear characteristics of the logarithmic Mel spectrum. This results in detail loss and distortion, appearing visually as blurriness and audibly as monotonous tail sounds.



\begin{figure*}[!t]
    \centering
    % \setlength{\belowcaptionskip}{-30pt} % 图表标题之下的间距
    % \hspace{-2mm}
\includegraphics[width=2.1\columnwidth]{25imgs/SwapForward.pdf}
\vspace{-15pt}
    \caption{Our latent swap joint diffusion pipeline. As the core, the Self-Loop Latent Swap operator performs bidirectional frame-level swaps on the overlapping regions of adjacent subviews during each denoising step, adaptively enhacing the high-frequency details and avoiding spectrum aliasing. The Reference-Guided Latent Swap occurs between the reference and each subview trajectories during the early steps, providing a centralized reference trajectory to ensure cross-view consistency without repetition.}
    % As the core of latent swap joint diffusion, the Self-Loop Latent Swap operator performs bidirectional frame-level swaps on the overlapping regions of adjacent subview during each denoising step, preserving high-frequency details and avoiding spectrum aliasing. The Reference-Guided Latent Swap occurs between the reference and each subview during the early steps, providing a centralized reference trajectory to ensure cross-view consistency without repetition.
    % As the core, the Self-Loop Latent Swap operator performs bidirectional frame-level swaps on adjacent subviews during each denoising step. The Reference-Guided Latent Swap occurs between the reference and each subview during the early steps, providing unidirectional centralized guidance to synchronize subview diffusions.
\label{fig:middle}
\vspace{-15pt}
\end{figure*}

\subsection{Latent Swap Joint Diffusion}
\label{sec:latent swap}
% Focusing on the overlap region, the core of joint diffusion can be regarded as the merging of stepwise differentiated trajectories \( x_t^i \) and \( x_t^{i+1} \), which share the same initial overlapping latent \( I_{i, i+1}(J_{t+1}) \) in the previous step:
% \begin{equation}  
% I_{i, i+1}(J_t) = W_i \odot \text{Right}(X_t^i) + (1-W_i) \odot \text{Left}(X_t^{i+1}),
% \end{equation} 
% where \(\text{Left}(\cdot)\) and \(\text{Right}(\cdot)\) represent mapping functions for the left and right overlapping regions, respectively. 
% The two stepwise differentiated trajectories can then be formulated as:
% \begin{align} 
% X_t^{i} &= \Phi\big( I_{i, i+1}(J_{t-1}) \cup X_{t-1}^{i}, y_i \big), \\
% X_t^{i+1} &= \Phi\big( I_{i, i+1}(J_{t-1}) \cup X_{t-1}^{i+1}, y_{i+1} \big).
% \end{align}

% Building on the findings in Section \ref{}, we introduce a swap operator to effectively merge these stepwise differentiated diffusion trajectories, enhancing high-frequency details while avoiding aliasing. Specifically, the swap operator, consisting of binary elements (0 and 1), is a specific case of $W_i$. Compared with the averaging operation, the binary operator maintains the original denoising details from $x_i$ and $x_{i+1}$ respectively, rather than smoothing them together. Due to the differences (influenced by other pixels from the non-overlapping region) and the similarities in stepwise trajectories (originating from the same initial state at the previous time step), the swap operation adaptively enhances specific range of high-frequency components with specific swap frequency while avoiding disturbances to the low-frequency components. 




\paragraph{Stepwise Differentiated Trajectories}
\label{sec:Trajectories}Building on the above findings, our target is to enhance high-frequency details while spectrum aliasing. As shown in black and red trajectories in Fig.~\ref{fig:middle} left, focusing on overlapping regions, the essence of joint diffusion can be regarded as the merging of stepwise differentiated trajectories $\Phi(x_t^i \cup  I_{i, i+1}(J_{t+1}),y_i)$ and  $\Phi(x_t^{i+1} \cup I_{i, i+1}(J_{t+1}),y_{i+1})$ that share the same previous-step initial overlapping latent {\small $I_{i, i+1}(J_{t+1})$}. Thus, these stepwise trajectories exhibit both differences—arising from latent influences outside the overlap and similarities—inherited from the shared initial state at the previous step, which can be formulated as:
\begin{equation}
\begin{aligned}
 \varepsilon_l \leqslant d \big(&\operatorname{Right}\big(\Phi(x_t^i \cup  I_{i, i+1}(J_{t+1}),y_i)\big)\big), \\
 &\operatorname{Left}\big(\Phi(x_t^{i+1} \cup I_{i, i+1}(J_{t+1}),y_{i+1})\big) \big) \leq \varepsilon_u 
\end{aligned}
\end{equation}
where the lower bound \( \varepsilon_l \in [0,1] \) prevents complete similarity, the upper bound \( \varepsilon_u \in [0,1] \) restricts excessive divergence. $\text{Right}(\cdot)$ and $\text{Left}(\cdot)$ stand for region slice operations on the subview to obtain the left and right overlap regions, and \( d(\cdot) \) is a distance metric.





% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=1\columnwidth]{25imgs/fig7.pdf}
%     \vspace{-15pt}
%     \caption{Reference-Guided Latent Swap achieves the cross-view similarity-diversity trade-off by the centralization reference trajectory guidance through unidirectional latent swap in early $r_\text{guide} \times T$ guidance steps.}
%     \label{fig:small2}
%     \vspace{-15pt}
% \end{figure}

\vspace{-10pt}
\paragraph{Self-Loop Latent Swap}
 \label{sec:Self-Loop}
Leveraging the properties of stepwise differentiated trajectories, we introduce the latent swap operator $W_\text{swap}$, which consists of binary elements 0 and 1 as a specific subset of the linear component operators. Focus on the trajectory merging on overlap region, the averaging operator commonly applied with constant matrix as: 
\begin{equation}
W_\text{swap} = c \cdot 1_{m \times n}
\end{equation}
On the contrary, rather than smoothing with each other, the binary swap operator $W_\text{swap}$ samples and preserves the original denoised latent from \( x_t^i \) and \( x_t^{i+1} \) as show in Fig.~\ref{fig:middle}:

\begin{equation}
\small
I_{i,i+1}(J_t) = W_\text{swap} \odot \text{Right}(X_t^i) + (1-W_\text{swap}) \odot \text{Left}(X_t^{i+1})
\label{swap_equation}
\end{equation}

where $I(\cdot)$ represents overlap mapping operation defined in Eq.\ref{I}. The similarity of step-wise difference trajectory and the robustness of the diffusion model ensure controlled distribution to the joint diffusion trajectory, avoiding disturbing original low frequency components. On the other hand, in terms of Fourier analysis, the averaging operator can be considered an all-pass filter. In comparison, leveraging the stepwise differential trajectories, the latent swap operator functions act as a band-pass filter, adaptively enhancing specific frequency components. By controlling the swap interval and swap direction, we can selectively enhance particular frequency bands. In this paper, we adopt a frame-level latent swap operator $W_\text{swap}$ with swap interval $w$, 
\begin{equation}
W_\text{swap} = \mathbf{1}_n \otimes v_m, \quad
v_m^{(i)} = \frac{1}{2} \left[ 1 - (-1)^{\left\lfloor \frac{i-1}{w} \right\rfloor} \right]
\label{swapo}
\end{equation}
Following the experimental results shown in Appendix Fig.\ref{fig:ap3},  we choose the optimized swap interval $w$ of 1 to achieve better-blender transitions. At a high level, the latent swap operation applied to the overlap region $I_{i,i+1}$ is performed sequentially across each subview (including between the first and last subviews), forming a loop swap process without central guidance. Hence, we name it Self-Loop Latent Swap.

As shown in Fig.~\ref{analysis} (c), we further conduct Fourier analysis on the overlap-denoised latent with the latent swap operation. Compared to the averaging operator, the latent swap operator successfully enhances the high-frequency components of the spectrum latent and shows the same trend as the reference curve. It preserves spectrum details and avoids spectrum aliasing in the overlap region. Moreover, we find that the swap operation is also well-suited for panorama image generation, as shown in Fig. \ref{fig:small1} and \ref{fig:qual}. 
% achieving smooth transitions and preserving details.





% \begin{algorithm}
%     \caption{hh}
%     \begin{algorithmic}[1]
%     \State \textbf{hh} hh
%     \State \textbf{hh} hh
%     \State sum = 0
%     \For{each element num in nums}
%         \State sum = sum + num
%     \EndFor
%     \State \textbf{hh} sum
%     \end{algorithmic}
% \end{algorithm}


 
% As illustrated in Figure 3, we optimize the swap frequency as 1, corresponding to a segment swap in 1D tokenization. For a 100-frame overlap region, increasing the swap unit to 16 and 50 prevents aliasing but leads to insufficient swapping, making it difficult to unify the adjacent non-overlapping regions, resulting in discontinuities.


% To address this issue, we propose the Self-Loop Latent Swap inspired by the high-frequency variability of the mel-spectrum. This bidirectional, frame-level operator is applied to the overlap region between adjacent subviews. As shown in Fig. \ref{fig:middle}, consider a swap operation between two adjacent subviews $x^{(i)}_t$ and $x^{(i+1)}_t$. The swap operation consists of two steps: first, downsample the right overlapping region of $x^{(i)}_t$ and the left overlapping region $x^{(i+1)}_t$ of by half, producing the preserved part and the dropped part. Next, replace the the dropped part with the preserved part from the adjacent region to complete a basic latent swap. At a high level, this swap operation is applied throughout each subview sequentially (including between the first and last subviews) forming a loop swap process without central guidance, hence it is named Self-Loop Latent Swap. Compared to averaging, this frame-level swap is compatible with the high-frequency variability of the mel-spectrogram, preserving the original latent structure. As shown in Fig. \ref{fig:head}, it helps retain more spectral details and avoid confusion. Additionally, the denoising process on the swapped latent map can be seen as a frame-level inpainting task. The redundancy of the spectrum frame (from STFT) and the latent map (from convolution layer), combined with the robustness of the diffusion model, further ensures seamless outputs. Moreover, we find the swap operation is also well adapted to RGB panorama images generation, as shown in Fig. \ref{fig:head} and \ref{fig:small}, achieving better-blended transitions and preserving more details.


% \begin{figure}[!t]
%     \centering
%     \setlength{\belowcaptionskip}{0pt} % 图表标题之下的间距
%     \includegraphics[width=1\columnwidth]{25imgs/fig6.pdf}
%     \caption{}
%     \label{fig:small}
% \end{figure}

% As a result, our spectrum-inspired latent swap operation, SaFa*, comprehensively surpasses the latent averaging operation (MD) in terms of quality and coherence. With the addition of Reference-Guided Swap, SaFa achieves global coherence and higher quality in terms of I-Style, FID, and KID, comparable to MAD and SyncDiffusion, with much more higher efficiency.



\vspace{-10pt}
\paragraph{Reference-Guided Latent Swap}
\label{sec:Reference-Guided}

% To mitigate cross-view inconsistency, Multidiffusion \cite{BarTal2023MultiDiffusionFD} adopts high subview overlapping to avoid non-overlapping regions, which performs poorly due to lack of explicit guidance and inefficiency with excessive redundant subview diffusion. SyncDiffusion \cite{lee2023syncdiffusion} provides centralized guidance by optimizing initial subview denoising with LPIPS loss. However, it incurs $10\times$ higher time cost, and LPIPS loss is not compatible with the intermediate denoised mel-spectrogram.


As mentioned in Introduction, previous work \cite{BarTal2023MultiDiffusionFD, lee2023syncdiffusion} relies on high overlap rate and test-time gradient optimization to mitigate cross-view inconsistency problem, bringing significant time cost and subview count. In constant, we propose the unidirectional \textit{Reference-Guided Latent Swap} operation to efficiently achieve cross-view consistency in forward-only manner. Specifically, in Fig.~\ref{fig:middle} (blues lines), for the early $r_{\text{guide}} \times T$ denoising steps, we guide the non-overlapping region of each subview trajectories \( M_i(J_t) \) by the shared independent reference trajectory \( X_t^0 \) with a frame-level unidirectional swap operation to achieve cross-view consistency with swap operator $W_\text{refer}$ as:
\begin{equation}
\small
M_{i}(J_t) = W_\text{refer} \odot \text{Mid}(X_t^0) + (1-W_\text{refer}) \odot \text{ Mid }(X_t^{i})
\label{guide}
\end{equation}
where $\text{Mid}(\cdot)$ represents the region slice operation on the subview to obtain the non-overlapping region. We claim that the Reference-Guided Latent Swap can be considered a frame-level blended diffusion \cite{avrahami2022blended} that aligns the non-overlapping trajectory $ M_i(J_t) $ with the reference trajectory $x_t^0$, while maintaining coherence with nearby overlapping trajectories $I_{i-1,i}(J_t)$ and $I_{i,i+1}(J_t)$.  For the latter $ (1 -  r_{\text{guide}}) \times T $ of the denoising steps, the non-overlapping trajectories start from the similar intermediate denoised latent maps to achieve cross-view consistency and avoid repetition simultaneously, as shown in Fig.~\ref{fig:small1}. By adjusting the swap timing (early $r_{\text{guide}} \times T$ stage) and the swap interval ($w$ in Eq.~\ref{swapo}), we can achieve a trade-off of similarity and diversity for global coherence. As shown in Appendix Fig.~\ref{fig:ap2}, we observe an increase in similarity but a decrease in diversity as $ r_{\text{guide}}$ increases, and give a theoretical analysis in Appendix Sec.\ref{Theoretical}. As a result, we implement SaFa with a $r_{\text{guide}}$ of 0.3 as a similarity-diversity balance. As for the swap interval, we follow the implementation in Eq.~\ref{swapo}, adopting a frame-wise column swap with $w$ of 1. For image generation, in Fig.~\ref{fig:middle} right, considering the flattening order in the 1D token sequence\footnote{Spectrograms are flattened first along the frequency axis and then along the time axis, while images follow the reverse order.}, We adopt a row swap to achieve segment-wise swapping (Fig.\ref{fig:small1} (a), line 1) instead of a pixel-level swap (Fig.\ref{fig:small1} (a), line 2) in 1D token sequences, preventing excessive similarity due to the strong correlation between neighboring tokens in the attention operation.

\begin{figure}[!t]
 % \label{fig:small}
    \centering
    \setlength{\belowcaptionskip}{0pt} % 图表标题之下的间距
    % \hspace{-2mm}
    \includegraphics[width=1\columnwidth]{25imgs/fig4.pdf}
\vspace{-15pt}
    % \caption{In Figure (a) and (b), with guidance latent from the reference image (left of the first row), the panorama and long audio (right of the first row)  demonstrate enhanced global coherence than the unguided results in the last row. In Figure (a), we apply the swap operation along the row axis to achieve segment-level swap (first line), avoiding repetition and preserving diversity than the pixel-wise swap along the column axis (second line).}
    % \vspace{-10pt}
    \caption{Compared to unguided results in the last row of subfigures(a)(b), the panorama and waveform with Reference-Guided Swap (first row) demonstrate much more cross-view consistency, e.g., color and style for image, and timbre and SNR for audio. Considering the flattening order to the 1D sequence, in subfigure (a), we apply the swap along the row axis (first row) to achieve segment-level swaps and avoid cross-view repetition, compared to the pixel-wise swap along the column axis (middle row).}
\vspace{-15pt}
    \label{fig:small1}
\end{figure}


\begin{algorithm}[t]
\small
\caption{Latent Swap Joint Diffusion}
\setlength{\baselineskip}{12pt} 
\textbf{Input}: Reference model $\Phi$,  subview mapping $\{F_i\}^n_{i=0}$, \\ conditions $\{y_i\}^n_{i=0}$, guidance step rate \textbf{$r_{\mathrm{refer}}$}, \\ region slice operations $\text{Right}(\cdot)$, $\text{Mid}(\cdot)$, and $\text{Left}(\cdot)$.\\
$J_T \sim P_{\mathcal{J}} \quad \triangleright$ noise initialization \\
\For{$t \gets T$ \KwTo 1}{
         $X_t^i = \Phi\big(F_i(J_{t+1}), y_i\big), \forall i \in[0,n]$\; 
          // Self-Loop Latent Swap $\triangleright$ Eq.~\ref{swap_equation} 
        $I_{i, i+1}(J_t) = \text{Swap}\big(\text{Left}(X^{i+1}_t), \text{Right}(X_t^i)\big), \forall i \in[n]$\; 
    \If{$t \geq (1-r_{\mathrm{guide}})\ \times T$}{
      //  Reference-Guided Latent Swap $\triangleright$ Eq.~\ref{guide} 
            $M_{i}(J_t) = \text{Swap}\big(\text{Mid}(X^0_t), \text{Mid}(X_t^i)\big), \forall i \in[n]$\; } 
    }
    
$\textbf{Output}: J_0 $ \\

// the implement of frame-level latent swap \\
\SetKwFunction{FSwap}{Swap} 
\SetKwProg{Fn}{Function}{:}{}
\Fn{\FSwap{$X_1, X_2$} }{ 
    $ W_\text{swap} = \mathbf{1}_n \otimes v_m,
v_m^{(i)} = \frac{1}{2} \left[ 1 - (-1)^{\left\lfloor \frac{i-1}{w} \right\rfloor} \right] $ $\triangleright$ Eq.~\ref{swapo}\\
    % $X_{new}[2k] = X_1[2k]$ \; \\
    $X_{\text{new}} = W_\text{swap} \odot X_1 \ + (1-W_\text{swap}) \odot X_2$ \\ 
    $\textbf{Return}: X_{\text{new}}$\;
}
\label{alg:1}
\end{algorithm}






\begin{figure*}[!t]
   %\label{fig:qualita}
    \centering
    \vspace{-20pt}
    \setlength{\belowcaptionskip}{0pt} % 图表标题之下的间距
    \includegraphics[width=2.1\columnwidth]{25imgs/fig5.pdf}
    \vspace{-20pt}
    \caption{Qualitative comparisons. SaFa achieves better generation quality in both long audio generation and panoramas with \textcolor[RGB]{112,173,71}{high efficiency}. We highlight issues such as \textcolor[RGB]{255,0,0}{monotonous tails} and \textcolor[RGB]{46,117,182}{spectrum aliasing} in audio generation, \textcolor[RGB]{255,0,0}{ transition misalignment}, \textcolor[RGB]{46,117,182}{cross-view inconsistency } and \textcolor[RGB]{255,192,0}{ artifacts } in panorama generation caused by baseline methods.}
    % \caption{Qualitative comparisons. Top: In mel-spectrogram generation, SaFa demonstrates significantly better-blended transitions and preserves fine details in the overlap region (narrow white band) compared to other methods. Bottom: In panorama generation, MD \cite{BarTal2023MultiDiffusionFD} produces seamless outputs but exhibits global cross-view incoherence in color and style. In contrast, SaFa achieves comparable cross-view coherence and smooth transitions to MAD \cite{Quattrini2024MergingAS} and SyncD \cite{lee2023syncdiffusion}, without additional computational or time costs. From the global view, SaFa achieves a similarity-diversity trade-off in both audio and panorama generation.}
    \label{fig:qual}
    % \vspace{-10pt}
    
\end{figure*}




% Similar to MultiDiffusion \cite{BarTal2023MultiDiffusionFD}, the Self-Loop Latent Swap operates only on the overlap regions of adjacent subviews and thus does not guarantee global coherence between non-overlapping or distant subviews. To address the issue, we propose Reference-Guided Latent Swap, designed to achieve both global coherence and efficiency. As shown in Fig. \ref{fig:middle}, the Reference-Guided Latent Swap is also a frame-level, but unidirectional latent swap operator that occurs between the reference and each subview, using an independent denoising process as global guidance to direct other processes via latent swap. Notably, the Reference-Guided Latent Swap is applied only during early $N_{\text{refer}}$ diffusion steps. This is because early reverse stages effectively capture outlines and global style, while later stages refine finer details \cite{ yang2023denoising, kim2024leveraging}. In addition, excessive guidance to late denoising steps has been observed cause artifacts. Moreover, the Reference-Guided Swap occurs in central regions of subviews rather than edges, with the guided region rate $r_{gr}$ representing the proportion of the subview area involved in the swap. This aligns with the observation in training data that key audio events in audio and primary objects in images are often located in non-border regions.  As a result, in first line of Fig. \ref{fig:small} (a) and Fig. \ref{fig:small} (b), the operation enhances global coherence in both image generation (color and style) and audio generation (timbre and SNR) through explicit latent guidance from the reference subview.

% Specifically, as shown in Fig. \ref{fig:middle} (below), we emphasize that the swap occurs along different directions when applied to spectrum and images, considering their respective flattening orders to 1D token sequence before the attention operation. Specifically, spectrograms are flattened first along the frequency axis, then along the time axis, while images follow the reverse order. To achieve a segment-wise swap instead of a pixel-level swap in token sequences, the Reference-Guided Swap is applied along the column axis for spectra and along the row axis for images.  As demonstrated in Fig. \ref{fig:small} (a), segment-wise swaps ( first line) outperform pixel-wise swaps ( second line), effectively preserving diversity and avoiding excessive repetition. 

% Similar to MultiDiffusion, the Self-Loop Latent Swap operates only on the overlap region of adjacent subviews and thus does not guarantee global coherence between distant subviews or between non-overlapping regions, especially with low overlap rate. For previous work to mitigate this limitation, MultiDiffusion increases the overlap rate nearly 70\%, which partially improves coherence. However, inconsistencies remain, and more subviews reduce the efficiency. When applied to spectrum generation, the high overlap rate also exacerbates distortion and confusion. MAD incorporates a merge-and-split operator around each global self-attention layer, yet this approach requires the model to handle ultra-long token sequences. For instance, generating a 512 $\times$ 3200 image from 512 $\times$ 640 subviews necessitates handling a token sequence five times longer than typical in the self-attention layer. While feasible for models like Stable Diffusion v2.0 model, this approach often leads to significant performance declines in models with fixed training configurations, particularly in audio and music generation tasks. Moreover, the block-wise merge operation is limited in the DiT architecture, as it causes confusion in the self-attention mechanism due to repeated positional encoding within the same token map. Conversely, SyncDiffusion offers a promising method to leverage reference guidance, however, the additional forward and backward propagation steps extend inference time by up to nearly ten times, from 40 seconds to 401 seconds.









% \subsection{Joint Latent Map Initialization}
% \label{sec:Joint}
% Recent research has demonstrated that the initialization of the latent map can potentially impact subsequent denoising processes. For instance,  in music generation \cite{ghosal2023text}, the gradient descent algorithm is employed to adjust the initial latent map that are obeverd could control the final output. Similarly, for 360-degree panorama image generation \cite{Liu2023AudioLDMTG}, downsampling is utilized to sample each perspective noisy latent representation for each view from a jointly panorama sample noise.  Inspired by these methods, we propose a collaborative subview initialization approach that captures the overlapping relationships between adjacent subviews and their spatial or temporal layout within a global view.  Specifically, a global view-noisy latent map  $z_T \in \mathbb{R}^{C \times H_z  \times W_z \times D}  $ is initialized from a normal Gaussian distribution. To distribute the subviews, we apply a sliding window operation with an overlap rate $r_{o}$  to generate each subview images $x(i) \in \mathbb{R}^{C \times H_S \times W_H}$ sequentially. To ensure a loop and seamless output, the last $r_{o} \times W_x$ frames of the end subview are replaced with the corresponding from the beginning view. Additionally, we observe that Reference-Guided Latent Swap operator in the initial latent can also further improve the global coherence of the final output.

\vspace{-5pt}
\section{Experiment}
\label{sec:exp}

% Furthermore, we conduct user studies to enhance evaluation reliability. Participants are presented with pairs of images generated by our approach and other baseline methods and asked to rank them based on perceived coherence, overall quality, and semantic alignment. The results table and detailed settings are shown in the appendix.
\subsection{Long-Form Audio Generation}
\label{sec:exp_audio}
\paragraph{Baselines}

We compare our approach with other joint diffusion method in Tab.~\ref{tab:audio1}, including MultiDiffusion (MD) \cite{BarTal2023MultiDiffusionFD}, its enhanced version (MD*) \cite{polyak2024movie} with the triangular window, and Merge-Attend-Diffuse (MAD) \cite{Quattrini2024MergingAS}. SyncDiffusion \cite{lee2023syncdiffusion} is not implemented in audio generation, as LPIPS loss is observed insensitive to \textit{intermediate} denoised mel-spectra despite being sensitive to \textit{completely} denoised ones.


% Further, we compare SaFa with other training-based long audio generation models includes AudioGen \cite{kreuk2022audiogen}, Stable Diffusion Audio (SD-audio) \cite{Evans2024FastTL} and Make-An-Audio2 (Make2) \cite{Huang2023MakeAnAudio2T} on large-scale audio generation benchmark in Appendix~\ref{training-based}.

\begin{table*}[!t]
\centering
\footnotesize
\setlength{\tabcolsep}{2.3mm}
% \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

% \textcolor{pink}{Pink} and \textcolor{violet!20}{purple} blocks represent the best and second-best results.
\vspace{-5pt}
\begin{tabular}{l|cccccc|cccccc}
\toprule[1pt]
\multirow{2}{*}{\textbf{Method}} & \multicolumn{6}{c|}{\textbf{DiT}}                                                                       & \multicolumn{6}{c}{\textbf{U-Net}}                                                                      \\
                                 & \textbf{FD↓}  & \textbf{FAD↓} & \textbf{KL↓}  & \textbf{CLAP↑} & \textbf{I-LPIPS ↓} & \textbf{I-CLAP↑} & \textbf{FD↓}  & \textbf{FAD↓} & \textbf{KL↓}  & \textbf{CLAP↑} & \textbf{I-LPIPS ↓} & \textbf{I-CLAP↑} \\ \hline
Reference                        & 2.92          & 0.22          & 0.74          & 0.54            & 0.39               & 0.86             & 4.12          & 0.27          & 1.15          & 0.53            & 0.43               & 0.79             \\
MAD   \cite{Quattrini2024MergingAS}                       & 12.77         & 7.56          & 0.86          & 0.51            & \cellcolor{pink}{0.32}      & 0.93             & 16.33         & 8.12          & 1.25          & 0.49            & \cellcolor{violet!20}{0.36}      & 0.88             \\
MD \cite{BarTal2023MultiDiffusionFD}                           & 11.31         & 6.41          & 0.81          & 0.51            & 0.36               & 0.91             & 14.29         & 7.06          & 1.18          & 0.50            & 0.41               & 0.86             \\
MD* \cite{polyak2024movie}                         & 9.79          & 5.09          & 0.77          & 0.52            & 0.36               & 0.92             & 11.24         & 5.53          & 1.12          & 0.51            & 0.40               & 0.88             \\
\textbf{SaFa}                    & \cellcolor{pink}{6.84} & \cellcolor{violet!20}{4.91} & \cellcolor{pink}{0.73} & \cellcolor{pink}{0.54}   & \cellcolor{violet!20}{0.34}      & \cellcolor{pink}{0.95}    & \cellcolor{violet!20}{7.88} & \cellcolor{violet!20}{4.27} & \cellcolor{pink}{1.11} & \cellcolor{violet!20}{0.53}   & \cellcolor{pink}{0.36}      & \cellcolor{pink}{0.92}    \\
\textbf{SaFa*}                   & \cellcolor{violet!20}{6.98} & \cellcolor{pink}{4.89} & \cellcolor{violet!20}{0.73} & \cellcolor{violet!20}{0.54}   & 0.36               & \cellcolor{violet!20}{0.94}    & \cellcolor{pink}{7.58} & \cellcolor{pink}{4.14} & \cellcolor{violet!20}{1.12} & \cellcolor{pink}{0.54}   & 0.39               & \cellcolor{violet!20}{0.90}    \\ \bottomrule[1pt]
\end{tabular}
\vspace{-10pt}
\caption{Quantitative comparisons on audio generation (including soundscape, sound effect and music). In SaFa*, only Self-Loop Swap is applied. MD* \cite{polyak2024movie} uses a triangular window achieving gradual transition. Pink and purple blocks represent the best and second-best results.
}
\label{tab:audio1}
\vspace{-15pt}
\end{table*}



\vspace{-10pt}
\paragraph{Experiment Settings}
In Tab. \ref{tab:audio1}, we implement all methods on two pretrained text-to-audio (TTA) models based on the AudioLDM \cite{liu2023audioldm} framework. One adopts AudioLDM’s original U-Net architecture and the other is based a masked DiT architecture \cite{gao2023masked}. Both models incorporate a FLAN-T5 text encoder \cite{chung2024scaling} and a pretrained 2D spectrum-based VAE model \cite{liu2023audioldm}, and are trained on the same dataset and pipeline with variable audio, music and speech clips during 0.32s to 10.24s following Make-an-audio2\cite{Huang2023MakeAnAudio2T}. In inference stage, we use a DDIM sampler \cite{song2020denoising} with 200 denoising steps and a classifier-free guidance scale of 3.5. For vocoder, we employ HiFi-GAN \cite{kong2020hifi} to generate the audio samples from mel-spectrogram. For quantitative experiments, we evaluate all the methods by generating 24s of audio at a 16kHz sample rate, combining three overlapping 10-second segments with an overlap rate $r_{\text{overlap}}$ of 0.2. For qualitative experiments, the duration of the segments is set to 8s to facilitate user studies. Nine text prompts are used (three soundscape, three sound effect and three music), as listed in the first nine audio prompt in Appendix Fig.~\ref{fig:audio_0} to \ref{fig:audio_3}. 
% \footnote{All code and models will be released.}
% Following \cite{Quattrini2024MergingAS}, MAD is applied during the first 60 of 200 steps to achieve optimal results. 

\vspace{-10pt}
\paragraph{Evaluation Metrics}
Following AudioLDM \cite{liu2023audioldm}, Frechet Distance (FD) and Frechet Audio Distance (FAD) are used for quality and fidelity estimation (similar to FID score in image generation). KL divergence (KL) is also used at a pair level. To align with previous work \cite{BarTal2023MultiDiffusionFD,kim2024leveraging,lee2023syncdiffusion}, we first utilize the reference model to generate 500 10-seconds audio clips per prompt, obtaining the reference set. Since the objective metrics models are trained on 10-second audio clips, we sequentially extract 10-second segments from each long-form generated audio with a sliding window, obtaining multiple 500-sample evaluation subsets per prompt. Then we calculate FD, FAD, and KL scores between these subsets and the reference set, and average them to obtain the final score. As the reference, we also calculate these scores between two equal-sized random splits of the reference set. For sematic alignment, CLAP score is applied \cite{wu2023large}. Intra-LPIPS and Intra-CLAP (cosine similarity of audio CLAP embeddings) are used to estimate cross-view coherence by calculating internal similarity between 10s clips cropped from the slide window operation with overlap rate of 0.2. We claim that FD, FAD, KL, and Intra-CLAP scores based on audio signals, while Intra-LPIPS is based on the Mel-spectrum.


% FID \cite{heusel2017gans} and KID \cite{binkowski2018demystifying} are used to measure fidelity and diversity. Using the reference model (SD 2.0 \cite{podell2023sdxl} or SD 3.5 \cite{esser2024scaling}), we generate 500 images per prompt at a resolution of 512 × 512 to form the reference set. Correspondingly sized subviews are cropped sequentially by a sliding window operation to obtain the evaluation datasets, and then we calculate scores between the evaluation and reference sets to obtain the final average result. As the reference, we also compute these scores between two equal random splits of the reference set. Intra-LPIPS (I-LPIPS) \cite{zhang2018unreasonable} and Intra-StyleL (I-StyleL) \cite{gatys2016image} assess cross-view consistency on the cropped 512 $\times$ 512 subviews from five equally divided regions of the panorama. As the reference, we use the reference dataset and randomly select 1,000 pairs to compute the average I-LPIPS and I-StyleL. Mean CLIP score (mCLIP) \cite{hessel2021clipscore} is used to evaluate semantic alignment. All results are averaged over six prompts. Time consumption is measured by Runtime, representing the total time to generate one panorama.

\vspace{-5pt}
\paragraph{Quantitative Result}
As shown in Tab. \ref{tab:audio1}, in both DiT and U-Net models, SaFa consistently outperforms other methods significantly in semantic alignment (CLAP) and generation quality (FD, FAD, and KL). SaFa approaches reference-level performance in CLAP and KL, demonstrating the latent swap operator's superiority in preserving high-frequency details and avoiding spectrum aliasing compared to the averaging operator. Compared to SaFa*, SaFa achieves greater cross-view consistency (I-LPIPS and I-CLAP) through Reference-Guided Swap. MD* surpasses MD, consistent with \cite{polyak2024movie}, but remains inferior to SaFa. MAD employs block-wise averaging operations before each attention layer in early stage, exacerbating spectrum aliasing and resulting in low quality. Additionally, it causes a position embedding repetition problem, resulting in monotonous and repetitive subviews with low I-LPIPS score. As shown in Tab.\ref{tab:audio2}, we further evaluate SaFa's stable performance on longer audio generation.



\vspace{-10pt}
\paragraph{Qualitative Result and User Study}
In Fig.~\ref{fig:qual} (top), SaFa preserves more high-frequency details without spectrum aliasing in the overlap compared with other methods. More qualitative comparisons are provided in Appendix Sec. \ref{sec:qual}. Furthermore, we conduct user studies on the generated audio samples to enhance evaluation reliability, collecting a total of 34 valid responses, where participants ranked the generated audio based on auditory quality and global consistency (including transition smoothness and cross-view consistency). The result aligns well with quantitative performance, showing a significant advantage for SaFa. Detail settings and results are available in Appendix Sec. \ref{user} and Fig.~\ref{fig:user1}.
% As shown in Fig.\ref{tab:audio2}, SaFa maintains strong, stable  performance across all evaluated metrics for outputs of varying lengths of 24s, 48s and 72s, demonstrating the algorithm's scalability to longer durations.
% Furthermore, we conduct user studies to enhance evaluation reliability. Participants are presented with pairs of images generated by our approach and other baseline methods and asked to rank them based on perceived coherence, overall quality, and semantic alignment. The results table and detailed settings are shown in the appendix.

% \paragraph{Length Adaptation}
% As shown in Fig.\ref{tab:audio2}, SaFa maintains strong, stable  performance across all evaluated metrics for outputs of varying lengths of 24s, 48s and 72s, demonstrating the algorithm's scalability to longer durations.
% Furthermore, we conduct user studies to enhance evaluation reliability. Participants are presented with pairs of images generated by our approach and other baseline methods and asked to rank them based on perceived coherence, overall quality, and semantic alignment. The results table and detailed settings are shown in the appendix.


\begin{table*}[!t]
\footnotesize
\centering
\vspace{-25pt}
\setlength{\tabcolsep}{2.35mm}
\begin{tabular}{l|cccccc|cccccc}
\toprule[1pt]
\multirow{2}{*}{\textbf{Method}} & \multicolumn{6}{c|}{\textbf{DiT}}                                                                     & \multicolumn{6}{c}{\textbf{U-Net}}                                                                    \\
                                 & \textbf{FD↓} & \textbf{FAD↓} & \textbf{KL↓} & \textbf{CLAP↑}  & \textbf{I-LPIPS ↓} & \textbf{I-CLAP↑} & \textbf{FD↓} & \textbf{FAD↓} & \textbf{KL↓} & \textbf{CLAP↑}  & \textbf{I-LPIPS ↓} & \textbf{I-CLAP↑} \\ \hline
SaFa (24s)                        & 6.84         & 4.91          & 0.73  & 0.54                    & 0.34               & 0.95             & 7.88         & 4.27          & 1.11   & 0.53                   & 0.36               & 0.92             \\
SaFa (48s)                        & 6.94         & 4.97          & 0.73  & 0.54                     & 0.35               & 0.94             & 7.61         & 4.10          & 1.08   & 0.54                   & 0.37               & 0.88             \\
SaFa (72s)                        & 6.98         & 4.99          & 0.72  & 0.54                    & 0.35               & 0.93             & 7.68         & 4.21           & 1.13 & 0.53                    & 0.37               & 0.89             \\ \bottomrule[1pt]
\end{tabular}
\vspace{-10pt}
\caption{SaFa maintains stable performance in longer audio generation at 24s, 48s, and 72s with both DiT and U-Net architectures.}
\vspace{-10pt}
\label{tab:audio2}
\end{table*}



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[!t]
\centering
\footnotesize
\setlength{\tabcolsep}{1.9mm}
\begin{tabular}{l|cccccc|cccccc}
\toprule[1pt]
\multirow{2}{*}{\textbf{Method}} & \multicolumn{6}{c|}{\textbf{DiT (SD 3.5)}}                                                                       & \multicolumn{6}{c}{\textbf{U-Net (SD 2.0)}}                                                                       \\
                                 & \textbf{FID↓} & \textbf{KID↓} & \textbf{CLIP↑} & \textbf{I-StyleL↓} & \textbf{I-LPIPS↓} & \textbf{Runtime↓} & \textbf{FID↓} & \textbf{KID↓} & \textbf{CLIP↑} & \textbf{I-StyleL↓} & \textbf{I-LPIPS↓} & \textbf{Runtime↓} \\ \hline
Reference                        & 28.19          & 0.01           & 32.57           & 5.49                & 0.60               & -                 & 33.37          & 0.01           & 31.60            & 8.72                & 0.73               & -                 \\
MD \cite{BarTal2023MultiDiffusionFD}                             & 24.50          & 8.12           & 32.37           & 2.58                & 0.59               & 103.85
            & \cellcolor{violet!20}{32.99} & \cellcolor{violet!20}{8.08}  & 31.76            & 3.08                & 0.69               & 37.71           \\
SyncD \cite{lee2023syncdiffusion}                           & 24.25          & 8.07           & 32.36           & 2.54                & 0.57               &  623.59
           & 44.58          & 19.98          & \cellcolor{violet!20}{31.84}   & \cellcolor{pink}{1.42}       & \cellcolor{pink}{0.55}      & 390.63        \\
MAD \cite{Quattrini2024MergingAS}                         & 65.10          & 55.73          & 31.79           & \cellcolor{pink}{0.67}       & \cellcolor{pink}{0.47}      & 85.25
           & 48.25          & 28.14          & 32.11            & 1.94                & \cellcolor{violet!20}{0.59}               & 41.82            \\
\textbf{SaFa}                    & \cellcolor{violet!20}{22.54} & \cellcolor{violet!20}{4.53}  & \cellcolor{pink}{32.45}  & \cellcolor{violet!20}{1.36}       & \cellcolor{violet!20}{0.56}      & \cellcolor{violet!20}{49.54}   & 34.71          & 9.91           & \cellcolor{pink}{31.84}   & \cellcolor{violet!20}{1.74}       & 0.61               & \cellcolor{violet!20}{19.15}    \\
\textbf{SaFa*}                   & \cellcolor{pink}{22.12} & \cellcolor{pink}{4.27}  & \cellcolor{violet!20}{32.39}  & 2.96                & 0.59               & \cellcolor{pink}{49.51}   & \cellcolor{pink}{32.43} & \cellcolor{pink}{6.97}  & 31.74            & 2.66                & 0.65               & \cellcolor{pink}{19.08}    \\ \bottomrule[1pt]
\end{tabular}
\vspace{-10pt}
\caption{Quantitative comparison on panorama generation with the resolution of 512 $\times$ 3200. In SaFa*, only Self-Loop Latent Swap is applied. KID and I-StyleL values are scaled by $10^3$. Pink and purple blocks represent the best and second-best results, respectively.}
\vspace{-15pt}
\label{tab:qual}
\end{table*}


% \caption{Quantitative comparison on panorama generation with the resolution of 512 $\times$ 3200. In SaFa*, only Self-Loop Swap is applied. KID and I-StyleL values are scaled by $10^3$. Pink and purple blocks represent the best and second-best results, respectively.}
% \label{tab:qual}
% \centering
% \setlength{\tabcolsep}{1.7mm}
% \begin{tabular}{l|ccccc|ccccc}
% \toprule[1pt]
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{5}{c|}{\textbf{DiT (SD 3.5)}}                                                              & \multicolumn{5}{c}{\textbf{U-Net (SD 2.0)}}                                                                 \\ 
%                                  & \textbf{FID↓} & \textbf{KID↓} & \textbf{CLAP↑} & \textbf{I-StyleL↓} & \textbf{Runtime↓} & \textbf{FID ↓} & \textbf{KID↓} & \textbf{CLIP↑} & \textbf{I-StyleL ↓} & \textbf{Runtime↓} \\ \hline
% Reference                            & 28.19          & 0.01                 & 32.57           & 5.49                      & -                   & 33.37          & 0.01                 & 31.60            & 8.72                      & -                   \\
% MD                               & 24.50          & 8.12                 & 32.37           & 2.58                      &       122.03              & \cellcolor{violet!20}{32.99} & \cellcolor{pink}{8.08}        & 31.76   & 3.08                      & 40.06               \\
% SyncD                             & 65.10          & 55.73                & 31.79           & \cellcolor{violet!20}{1.67}             &          668.46          & 44.58          & 19.98                & \cellcolor{violet!20}{31.84}            & \cellcolor{pink}{1.42}             & 401.03              \\
% MAD                              & 24.25          & 8.07                 & 32.36           & 2.54                      &         95.92            & 48.25          & 28.14                & 32.11            & 1.94                      & 41.82               \\
% \textbf{SaFa}                  & \cellcolor{violet!20}{22.54} & \cellcolor{violet!20}{4.53}        & \cellcolor{violet!20}{32.45}  & \cellcolor{pink}{1.36}             &     \cellcolor{violet!20}{49.54}                & 34.71          & 9.91                 & \cellcolor{pink}{31.84}            & \cellcolor{violet!20}{1.74}             & \cellcolor{violet!20}{35.16}      \\
% \textbf{SaFa*}                   & \cellcolor{pink}{22.12} & \cellcolor{pink}{4.27}        & \cellcolor{pink}{32.39}  & 2.96                      &     \cellcolor{pink}{49.51}                & \cellcolor{pink}{32.43} & \cellcolor{violet!20}{6.97}        & 31.74   & 2.66                      & \cellcolor{pink}{35.04}      \\ \bottomrule[1pt]
% \end{tabular}
% \vspace{-10pt}
% \end{table*}

\vspace{-10pt}
\paragraph{Comparison with Training-Based Methods}
We compare SaFa with SOTA training-based audio generation models includes AudioGen \cite{kreuk2022audiogen}, Stable Diffusion Audio  \cite{Evans2024FastTL} and Make-An-Audio2 \cite{Huang2023MakeAnAudio2T} on large-scale audio generation benchmark in Appendix Tab.~\ref{tab:ap1}. As a result, SaFa outperforms other methods across various length generation. Detailed settings and results are shown in Appendix Sec. \ref{training-based}.

\subsection{Panorama Generation}
\paragraph{Experiment Settings}
We compare SaFa with MD \cite{BarTal2023MultiDiffusionFD}, MAD \cite{Quattrini2024MergingAS}, and SyncD \cite{lee2023syncdiffusion} on SD 2.0 (U-Net) \cite{rombach2022high} and SD v3.5 (MMDiT). Using the same six prompts from prior work \cite{BarTal2023MultiDiffusionFD, Quattrini2024MergingAS, lee2023syncdiffusion} in Appendix Fig.~\ref{fig:image_0} to ~\ref{fig:image_5}, we generate 500 panorama images per prompt (resolution: 512 × 3200, subviews: 512 × 640). Following previous work \cite{BarTal2023MultiDiffusionFD, lee2023syncdiffusion, Quattrini2024MergingAS}, we implement other methods with optimized settings at a $r_{\text{overlap}}$ of 0.8. With explicit guidance in the non-overlap region, we implement a much lower $r_{\text{overlap}}$ of 0.2 to achieve higher efficiency with fewer subview count and maintain high quality. For SaFa, with explicit guidance in the non-overlap region, we use a much lower $r_{\text{overlap}}$ of 0.2 to improve efficiency while maintaining high quality, requiring fewer subviews.
% We compare SaFa with MD \cite{BarTal2023MultiDiffusionFD}, MAD \cite{Quattrini2024MergingAS}, and SyncD \cite{lee2023syncdiffusion} on Stable Diffusion v2.0 (SD 2.0) with U-Net architecture \cite{rombach2022high} and Stable Diffusion v3.5 with MMDiT architecture (SD 3.5), respectively. We utilize the same six prompts derived from previous work \cite{BarTal2023MultiDiffusionFD, Quattrini2024MergingAS, lee2023syncdiffusion}, as listed in the first six panorama qualitative examples in Appendix Sec. \ref{sec:qual}, with each prompt generating 500 panorama images. The target panorama resolution is set to 512 $\times$ 3200, composed of subview images with a resolution of 512 $\times$ 640. Following previous work\cite{BarTal2023MultiDiffusionFD, lee2023syncdiffusion, Quattrini2024MergingAS}, we implement the other methods with optimized settings with a $r_{\text{overlap}}$ of 0.8. With explicit guidance in the non-overlap region, we implement a lower $r_{\text{overlap}}$  of 0.2 to achieve higher efficiency with much fewer subviews and maintain high quality.

\vspace{-10pt}
\paragraph{Evaluation Metrics}
% FID \cite{heusel2017gans} and KID \cite{binkowski2018demystifying} is utilized to measure the fidelity and diversity. Using the reference model, we generate 500 images per prompt at a resolution of 512 $\times$ 512 to form the reference set. Correspondingly sized subview are cropped sequentially by a sliding window operation to obtain the evaluation datasets and then calculate scores between evaluation and reference dataset to obtain the final averaging result.  As the reference, we also compute these scores between two equal random splits of the reference images. Intra-LPIPS (I-LPIPS) \cite{zhang2018unreasonable} and Intra-StyleL (I-StyleL) \cite{gatys2016image} assess internal consistency by dividing each panorama into five non-overlapping regions and randomly cropping one 512 $\times$ 512 subview from each. And we calculate these score across 10 pairwise combinations per image and average it on all samples to obtain the final score. As a reference, we use the reference dataset and randomly select 1,000 pairs to compute average I-LPIPS and I-StyleL. Mean CLIP score (CLIP) based on CLIP model \cite{hessel2021clipscore} is used to evaluate semantic alignment. All results are averaged over six prompts. Time consumption is measured by Runtime, representing the total time to generate one panorama.
FID \cite{heusel2017gans} and KID \cite{binkowski2018demystifying} measure fidelity and diversity. For each reference model (SD 2.0 or SD 3.5), we generate five hundred $512 \times 512$ subview images per prompt as the reference set, and the same number for panorama with each joint diffusion methods. Correspondingly sized subviews are sequentially cropped from five equally divided regions of the panorama to construct the evaluation datasets. FID and KID scores are computed between the evaluation and reference sets, with the reference scores caculated between two equal random splits of the reference set. I-LPIPS \cite{zhang2018unreasonable} and Intra-StyleL (I-StyleL) \cite{gatys2016image} assess cross-view consistency using 10 subview pairs cropped from each panorama. As a reference, 1,000 pairs are randomly selected from the reference dataset to compute the average I-LPIPS and I-StyleL scores. The Mean CLIP score (mCLIP) \cite{hessel2021clipscore} evaluates semantic alignment, averaged over six prompts. Runtime measures the total time required for generating a complete panorama. All experiments are conducted using one same A100 GPU and PyTorch version.
% FID \cite{heusel2017gans} and KID \cite{binkowski2018demystifying} are used to measure fidelity and diversity. For each reference model (SD 2.0 or SD 3.5), we generate 500 512 × 512 images per prompt to form the reference set. For each joint diffusion method, we generate 500 512 × 3200 panoramas per prompt. Correspondingly sized subviews are cropped sequentially from five equally divided regions of the panorama to obtain the evaluation datasets. We calculate FID and KID scores between the evaluation and reference sets. As a reference, we also compute these scores between two equal random splits of the reference set. Intra-LPIPS (I-LPIPS) \cite{zhang2018unreasonable} and Intra-StyleL (I-StyleL) \cite{gatys2016image} assess cross-view consistency on the cropped subviews from the same panorama in the evaluation sets (10 pairs per panorama). As a reference, 1,000 pairs are randomly selected from reference dataset to compute the average I-LPIPS and I-StyleL scores. The Mean CLIP score (mCLIP) \cite{hessel2021clipscore} is used to evaluate semantic alignment. All results are averaged over six prompts. Time consumption is measured by Runtime, representing the total time to generate one panorama. All experiments are conducted on the same NVIDIA RTX A100 GPU with the same Pytorch version.

\vspace{-10pt}
\paragraph{Quantitative Result}
In Tab.~\ref{tab:qual}, compared to MD with the averaging operator, SaFa* demonstrates better generation quality (FID, KID) and global coherence (I-StyleL, I-LPIPS), highlighting the effective adaptation of the latent swap operator to image generation. With Reference-Guided Swap, SaFa further improves cross-view consistency (I-StyleL, I-LPIPS) with a negligible increase in time. Additionally, SaFa achieves significantly lower FID and KID scores and is $2 \sim 20 \times $ faster than MAD and SynD, demonstrating its high quality and efficiency. We note a gap in I-LPIPS when compared to SyncD, as it is directly optimized on LPIPS loss. MAD shows less model generalization, performing much worse in DiT (full of transformer layers) than U-Net due to its reliance on the self-attention layer’s capability for long sequences. Moreover, it causes repetition problem with much higher I-StyleL and I-LPIPS in DiT that is sensitive to position encoding. 

% In Tab. \ref{tab:qual}, compared to MD with averaging operator, SaFa* shows much better cross-view coherence (I-StyleL, I-LPIPS) and generation quality (FID, KID), highlighting the effectiveness of the latent swap operator in achieving smoother transitions and preserving detail in both mel-spectrogram and image generation. With Reference-Guided Swap, SaFa improves global coherence (I-StyleL, I-LPIPS) and ensures strong subview consistency. It also outperforms MAD and SyncD with lower FID and KID scores, indicating higher quality and smoother transitions. The I-LPIPS gap with SyncD arises from its direct LPIPS-loss minimization in gradient descent. MAD performs significantly worse in DiT than U-Net due to its reliance on adapting the self-attention layer for long latent input. SD 3.5 struggles with length exploration due to its sole reliance on the transformer block and lack of convolutional layers in U-Net. In terms of time consumption, our method achieves the highest efficiency, being $11\sim14 \times$ faster than SyncD.
% As a result, SaFa comprehensively surpasses the latent averaging operation in terms of generality, quality, and coherence, making it a high-performing alternative for joint multi-diffusion applications. Furthermore, with only two simple swap operations, our method achieves performance comparable to SynDiffusion and MAD, without additional computational or time costs. This approach fills gaps in joint multi-diffusion for scenarios requiring low latency, DiT architectures, or models with a fixed attention window.


\vspace{-10pt}
\paragraph{Qualitative Result and User Study}
As shown in Fig.~\ref{fig:qual}, SaFa achieves better generation quality with significantly reduced time cost compared to MD and SynD, which still exhibit transition misalignment and lack cross-view consistency. Compared to MAD, SaFa achieves comparable performance with higher time efficiency and model generalizability. More qualitative results are provided in Appendix Sec. \ref{user}. Furthermore, user studies confirm the quantitative results in generation quality and cross-view coherence, as shown in Appendix Fig.~\ref{fig:user3}.

\vspace{-5pt}
\section{Conclusion}
\vspace{-5pt}
% In this paper, we begin with a contrastive analysis of different VAE latents and uncover the cause of overlap distortion in existing joint diffusion methods. Based on these findings, we propose Self-Loop Latent Swap to adaptively enhance high-frequency components between step-wise difference trajectories. As a variant, Reference-Guided Latent Swap provides centralized guidance for non-overlapping regions. Consequently, we achieve seamless and coherent long-latent outputs using two lightweight swap operators.
% \vspace{-5pt}
In this paper, we present SaFa, a simple but efficient latent swap framework through two fundamental swap operators to generate seamless and coherence long-form audio generation. Compared to previous techniques, SaFa is more adaptable to various modality tasks (long audio and even panorama images) across different diffusion architectures. As a high-performance alternative to the averaging operation, this operator can be widely applied in existing joint diffusion methods to achieve state-of-art performance without additional time and computational cost. For future research, the practicality of SaFa for 1D wave-based VAE latents or other discrete token-based representations requires further investigation, though it is limited by the current T2A models.

\clearpage
% we begin with a contrastive analysis of different VAE latents and uncover the cause of overlap distortion in existing joint diffusion methods. Based on these findings, we propose Self-Loop Latent Swap to adaptively enhance high-frequency components between step-wise difference trajectories. As a variant, Reference-Guided Latent Swap provides centralized guidance for non-overlapping regions. Consequently, we achieve seamless and coherent long-latent outputs using two lightweight swap operators.

% Compared to previous techniques, Swap Forward is more adaptable to various modality tasks (e.g., long audio and panoramic images) across different diffusion architectures (U-Net and DiT). As a high-performance alternative to the averaging operation, this operator can be widely integrated into existing joint diffusion methods to achieve state-of-the-art performance without additional time or computational cost.

% As a point of discussion, while the latent swap operator performs well in the 2D VAE latent spaces of spectra and images, several areas remain for further exploration. For instance, its applicability to 1D waveform-based VAE latents or discrete token-based representations, such as Residual Vector Quantization, requires further investigation.




% \section* {Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bigskip
{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
    
}
\clearpage
\input{sec/X_suppl}
\input{sec/X_suppl1}
\end{document}
