\section {More Quantitive Experiments}
\subsection{Comparison with Training-Based Methods}
% \paragraph{Comparison with Training-Based Methods}
\label{training-based}
We further compare our method with more training-based long audio generation models, including both diffusion models and language models. Although, strictly speaking, the absolute performance between models of varying sizes and trained on different datasets seems incomparable, the relative performance degradation of each model with increasing audio generation length can highlight the strengths and weaknesses of these methods for long-generation tasks.
\vspace{-10pt}
\paragraph{Baselines}
The training-based baselines include: (1) \textit{AudioGen} \cite{kreuk2022audiogen} : An autoregressive model based on learned discrete audio representations, inherently supporting ultra-long audio generation. (2) \textit{Stable Diffusion Audio} (SD-audio) \cite{Evans2024FastTL}: A diffusion model trained on a fixed 96-second window size with long audio, generating variable-length outputs through end-cutting. While the open-source version is trained on a 47-second window, longer audio can be generated by customizing the initial noise size. (3) \textit{Make-An-Audio2} (Make2) \cite{Huang2023MakeAnAudio2T}: A diffusion model trained on variable-length window sizes, with audio lengths ranging from 0 to 20 seconds. It supports a maximum length of 27 seconds, constrained by the learnable positional encoding limit. For SaFa, we implement it on Make-An-Audio2 for a clearer comparison, following the settings described in Section \ref{sec:exp_audio}.









% \begin{table*}[!t]
% \caption{Quantitative performance on length adaptation of SaFa on audio generation with DiT and U-Net architectures.}
% \vspace{-10pt}
% \centering
% \setlength{\tabcolsep}{1.3mm}
% \begin{tabular}{l|cccccc|cccccc}
% \toprule[1pt]
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{6}{c|}{\textbf{DiT}}                                                                     & \multicolumn{6}{c}{\textbf{U-Net}}                                                                    \\
%                                  & \textbf{FD↓} & \textbf{FAD↓} & \textbf{CLAP↑} & \textbf{KL↓} & \textbf{I-LPIPS ↓} & \textbf{ICLAP↑} & \textbf{FD↓} & \textbf{FAD↓} & \textbf{CLAP↑} & \textbf{KL↓} & \textbf{I-LPIPS ↓} & \textbf{ICLAP↑} \\ \hline
% SaFa (24s)                        & 6.84         & 4.91          & 0.54            & 0.73         & 0.34               & 0.95             & 7.88         & 4.27          & 0.53            & 1.11         & 0.36               & 0.92             \\
% SaFa (48s)                        & 6.94         & 4.97          & 0.54            & 0.73         & 0.35               & 0.94             & 7.61         & 4.10          & 0.54            & 1.08         & 0.37               & 0.88             \\
% SaFa (72s)                        & 6.98         & 4.99          & 0.54            & 0.72         & 0.35               & 0.93             & 7.68         & 4.21          & 0.53            & 1.13         & 0.37               & 0.89             \\ \bottomrule[1pt]
% \end{tabular}

% \label{tab:audio2}
% \end{table*}
\vspace{-10pt}
\paragraph{Evaluation Settings}
We evaluate these four methods using a large-scale benchmark, AudioCaps \cite{kim2019audiocaps}, whose test set includes 880 ground-truth samples collected from YouTube videos. The target generation lengths are set to 32, 64, and 96 seconds. For SaFa, these outputs are formed by concatenating 4, 8, and 12 audio clips of 10 seconds, respectively. As in Section \ref{sec:exp_audio}, we use FD, FAD, KL, and mCLAP to assess the generation quality and semantic alignment of the generated audio. Following previous work \cite{Evans2024FastTL}], we apply a 10-second sliding window operation with an 8-second step on long audio samples and further evaluate them with AudioCaps test set.

\begin{table}[!t]
\centering
\setlength{\tabcolsep}{2.0mm}
\begin{tabular}{lcccc}
\toprule[1pt]
\textbf{Method}              & \textbf{FD↓}   & \textbf{FAD↓} & \textbf{KL↓}  & \textbf{mCLAP↑} \\ \hline
SD-audio (10s)             & 38.23 & 6.20  & 2.19 & 0.40   \\
SD-audio (32s)             & 25.52 & 6.43 & 2.24 & 0.37   \\
SD-audio (64s)             & 25.82 & 6.12 & 2.25 & 0.35   \\
SD-audio (96s)             & 30.11 & 6.54 & 2.38 & 0.33   \\ \hline
AudioGen (10s)      & 16.88 & 4.36 & 1.52 & 0.55   \\
AudioGen (32s)      & 18.54 & 4.81 & 1.71 & 0.50   \\
AudioGen (64s)      & 19.53 & 5.02 & 1.76 & 0.50   \\
AudioGen (96s)      & 18.88 & 5.44 & 1.78 & 0.49   \\ \hline
Make2 (10s)         & 14.37 & 1.12 & 1.28 & 0.57   \\
Make2 (27s)         & 18.49 & 2.26& 1.55  & 0.49   \\ \hline
% \textbf{SaFa (10s)} & 14.37 & 1.28 & 1.12 & 0.57   \\
\textbf{SaFa (32s)} & 15.21 & 1.45 & 1.25 & 0.57   \\
\textbf{SaFa (64s)} & 15.14 & 1.25 & 1.24 & 0.57   \\
\textbf{SaFa (96s)} & 15.36 & 1.33 & 1.25 & 0.57   \\
\bottomrule[1pt]
\end{tabular}
\vspace{-5pt}
\caption{Quantitative Comparison with Training-Based Variable Length Audio Generation Models.}
\vspace{-15pt}
\label{tab:ap1}
\end{table}






\vspace{-10pt}
\paragraph{Results}
As shown in Table \ref{tab:ap1}, Make2, the SOTA diffusion-based audio generation model, demonstrates excellent performance in 10-second audio generation. However, it shows significant performance degradation when generating its maximum-length output of 27 seconds, as most training audio clips are under 20 seconds, and it lacks adaptation to longer unseen lengths. In contrast, our SaFa (32s) method maintains high performance in terms of KL and mCLAP, with only minor degradation observed in FD and FAD compared to the reference model Make2 (10s). Moreover, SaFa consistently delivers strong performance for 32-, 64-, and 96-second generation tasks without noticeable degradation. As for AudioGen, the SOTA LM audio generation model, although its architecture is inherently suited for generating longer audio compared to diffusion models, its performance degrades significantly as the generation length increases from 10 seconds to 96 seconds, accompanied by substantial increases in memory and time costs. For SD-audio, improved FD performance is observed when increasing the generation length from 10 to 32 seconds, likely due to the majority of its training data being focused on longer durations. However, other metrics consistently decline from 10 to 96 seconds, although the degradation is less pronounced compared to AudioGen. This highlights the robustness of diffusion models in generating longer outputs within their maximum training window.

\begin{figure*}[t]
    \centering
    % \setlength{\belowcaptionskip}{-10pt} % 图表标题之下的间距
    % \hspace{-2mm}
    \includegraphics[width=2.15\columnwidth]{appendix/exp/ap_exp1.pdf}
    \vspace{-20pt}
    \caption{The effect of the trajectory guidance rate $r_\text{guide}$ in Reference-Guided Swap on long spectrum and panorama generation.}
    \vspace{-10pt}
\label{fig:ap2}
\end{figure*}

\subsection{Joint Diffusion on Open-Source Checkpoint}
In this subsection, we discuss several design flaws in existing open-source audio generation models that limit the application of training-free methods, such as the joint diffusion method. In this way, we show our audio generation model as a potential contribution to advancing training-free approaches in audio generation. 

Specifically, AudioLDM \cite{liu2023audioldm} and Tango \cite{ghosal2023tango} are trained with a fixed 10.24-second window, padding shorter clips with zeros or truncating longer clips. This flexible training pipeline causes unexpected end silence in generated audios. Consequently, when implementing joint diffusion methods based on these models, sudden silence often appears in the overlap regions. Stable Diffusion Audio is also trained with a fixed 96-second window and generates variable-length outputs by truncation, making it similarly challenging to adapt for joint diffusion methods. In comparison, Make-An-Audio2 follows a training pipeline similar to ours, using variable-length audio without excessive padding. It organizes samples into different buckets based on the length during training, randomly selecting samples from the same bucket within each batch. However, we observe some anomalous phenomena when applying Make2 with joint diffusion methods.


\begin{table}[!t]
\centering
\vspace{5pt}
\setlength{\tabcolsep}{3.0mm}
\begin{tabular}{lcccc}
\toprule[1pt]
\textbf{Method} & \textbf{FD↓} & \textbf{FAD↓} & \textbf{KL↓} & \textbf{mCLAP↑} \\ \hline
Make2         & 18.01        & 2.01          & 1.49         & 0.50            \\
MD           & 65.28        & 17.70          & 3.22         & 0.24            \\
MAD          & 62.53        & 16.88         & 3.05         & 0.26            \\
% MD*             & 89.78        & 27.33         & 4.26         & 0.13            \\
\textbf{SaFa}   & 15.36        & 1.32          & 1.27         & 0.57            \\ 
\bottomrule[1pt]
\end{tabular}
\vspace{-5pt}
\caption{Quantitative comparisons of join diffusion on 24-seconds audio generation on Make-An-Audio2 \cite{Huang2023MakeAnAudio2T}.}
\vspace{-15pt}
\label{tab:ap2}
\end{table}



% \paragraph{Length Adaptation}
% As shown in Fig.\ref{tab:audio2}, SaFa maintains strong, stable  performance across all evaluated metrics for outputs of varying lengths of 24s, 48s and 72s, demonstrating the algorithm's scalability to longer durations.

As shown in Fig. \ref{fig:ap1}, when applying the joint diffusion method to Make2, short abrupt transitions appear at the end of each overlap region. Although SaFa significantly improves blending and generation quality compared to MD and MAD, these abrupt transitions still persist. Through experiments, we identify two main causes of this issue:
(1) The VAE latent map of Make-An-Audio2 is sensitive to the last token from an adjacent subview. To mitigate this, we apply Self-loop Swap with a five-token forward shift on the overlap regions. (2) Its VAE model is less robust to linear operations on the latent map compared to AudioLDM \cite{liu2023audioldm}. By performing concatenation at $t=0$ on the mel-spectrogram rather than on the latent map, we effectively resolve this issue. As a result, the improved method, SaFa+, performs well, as shown in Fig. \ref{fig:ap1}.

For quantitative comparison in Table \ref{tab:ap2}, our method, SaFa, significantly outperforms Make2 and other joint diffusion methods across all metrics for 24-second generation tasks.


\subsection{Effect of Guidance Rate and Swap Interval}
% As a visual supplement to the section that notes “the Reference-Guided Latent Swap is applied only during early $N_{\text{refer}}$ diffusion steps”, 
In Fig.\ref{fig:ap2}, we further demonstrate the progressive transition from cross-view diversity to similarity by varying $r_\text{guide}$ in both mel-spectrum and panorama generation using Reference-Guided Latent Swap. All other settings for SaFa remain consistent with Section \ref{sec:exp}. As shown in Fig. \ref{fig:ap2}, using an appropriate trajectory guidance rate $r_\text{guide}$, 20\% to 40\%, results in unified cross-view coherence while preserving the diversity of local subviews. However, as the guidance rate $r_\text{guide}$ increases beyond 60\%, excessive repetition and artifacts begin to appear. This occurs because Reference-Guided Swap is a unidirectional operation, where the denoising process of the reference view is independent and unaffected by each subview. Consequently, it does not adapt as seamlessly to subviews in the later stages as the bidirectional Self-Loop Swap operation does. This is also one of the reasons why we restrict Reference-Guided Swap to the early denoising stages.


\begin{figure}[!t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=1.0\columnwidth]{appendix/exp/ap_exp3.pdf}
    \vspace{-20pt}
    \caption{The long-form spectrum generated by various joint diffusion methods based on Make-An-Audio2. }
    % \caption{The VAE latent of Make-An-Audio2 is observed to be sensitive to the averaging operation in MD and MAD. The swap operation improves the situation significantly, but the swapped last token still causes abrupt transitions. In SaFa+, we address this issue by shifting the swap region forward by a few tokens.}
    \vspace{-15pt}
\label{fig:ap1}
\end{figure}
% \begin{table}[!t]
% \centering
% \setlength{\tabcolsep}{3.0mm}
% \begin{tabular}{lcccc}
% \toprule[1pt]
% \textbf{Method} & \textbf{FD↓} & \textbf{FAD↓} & \textbf{KL↓} & \textbf{mCLAP↑} \\ \hline
% Make2         & 18.01        & 2.01          & 1.49         & 0.50            \\
% MD           & 65.28        & 17.70          & 3.22         & 0.24            \\
% MAD          & 62.53        & 16.88         & 3.05         & 0.26            \\
% % MD*             & 89.78        & 27.33         & 4.26         & 0.13            \\
% \textbf{SaFa}   & 15.36        & 1.32          & 1.27         & 0.57            \\ 
% \bottomrule[1pt]
% \end{tabular}
% \vspace{-5pt}
% \caption{Quantitative comparisons of join diffusion on 24-seconds audio generation on Make-An-Audio2 \cite{Huang2023MakeAnAudio2T}.}
% \vspace{-15pt}
% \label{tab:ap2}
% \end{table}




\begin{figure*}[t]
    \centering
    % \setlength{\belowcaptionskip}{-10pt} % 图表标题之下的间距
    \includegraphics[width=2.0\columnwidth]{appendix/exp/ap_exp2.pdf}
    \caption{The effect of the swap interval $w$ (Eq.\ref{swapo} ) of Self-Loop Latent Swap on spectrum generation. Better transition is achieved with lower values of $w$, 1 or 2, which indicate a high swap frequency between two step-wise differential trajectories to enhance the high-frequency component in the denoised mel-spectrum with better-blender transitions.}
    \vspace{-10pt}
    \label{fig:ap3}
\end{figure*} 


\begin{figure}[t]
    \centering
    % \setlength{\belowcaptionskip}{-10pt} % 图表标题之下的间距
    % \hspace{-2mm}
    \includegraphics[width=1.0\columnwidth]{appendix/exp/user2.pdf}
    \vspace{-14pt}
    \caption{User study results on audio generation.}
      \vspace{-10pt}
\label{fig:user1}
\end{figure}

To further explore the effects of the swap interval $w$ (in Eq. \ref{swapo}), we apply the Self-Loop Latent Swap with various $w$ values in spectrum generation, as shown in Fig. \ref{fig:ap3}. We observe that using a small swap interval (1 or 2), corresponding to higher swap frequencies, produces smoother transitions. Conversely, larger $w$ values indicate larger swap units, resulting in less seamless transitions between subviews. This outcome aligns with the high-frequency variability of mel tokens, leading us to default the Self-Loop Latent Swap to frame-level operations with $w=1$ for optimal performance.

\subsection{Length Adaptation on Panorama Generation}

In Tab.\ref{tab:length}, We utilize SD 2.0 model to estimate performance of SaFa on panorama images with resolutions of 512 $\times$ 1600, 512 $\times$ 3200, and 512 $\times$ 4800. As a result, SaFa maintains stable and great performance across all evaluated metrics in different length output.

\section{User Study}
\label{user}
For subjective evaluation, we randomly select samples from the qualitative results of the top four methods in audio and panorama generation for user studies. We use the same notation as in Section \ref{sec:exp_audio}. Specifically, SaFa is compared with MD, MD*, and MAD for audio generation, while for panorama generation, SaFa is compared with MD, MAD, and SyncD. For each task, we randomly select 30 parallel comparison groups (plus 2 additional pairs as a vigilance group) from the four compared methods, evenly distributed across six prompts. 

We recruit 39 participants with basic machine learning knowledge but no prior familiarity with the research presented in this paper. Each participant is required to select the best sample from each of the 32 groups based on two evaluation dimensions: generation quality and global coherence. Semantic alignment is not considered, as most samples align well with the prompts semantically and cannot be easily distinguished in this regard. We ultimately collect 34 valid responses out of 39 participants. The results indicate that SaFa consistently outperforms the baseline methods, achieving superior human preference scores across both evaluation dimensions. Fig. \ref{fig:user1} highlight the significant preference of human evaluators for SaFa in both quality and coherence assessments of audio generation. This preference stems from the swap operator's enhanced adaptability to the inherent characteristics of spectral data, which lacks the typical global structural features or contours present in images. Meanwhile in Fig. \ref{fig:user3}, with significantly faster inference speeds and relying solely on fixed self-attention windows, SaFa achieves comparable performance to SyncD and MAD in the subjective evaluation of panorama generation.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{appendix/exp/user1.pdf}
    \vspace{-10pt}
    \caption{User study results on panorama generation.}
    \vspace{-5pt}
\label{fig:user3}
\end{figure}

\begin{table}[!t]
\centering
\setlength{\tabcolsep}{0.5mm}
\begin{tabular}{lccccccc}
\toprule[1pt]
\textbf{Method}                  & \textbf{CLIP} $\uparrow$  & \textbf{FID} $\downarrow$ & \textbf{KID} $\downarrow$ & \textbf{I-LPIPS} $\downarrow$ & \textbf{I-StyleL} $\downarrow$ \\ \hline
SaFa (1600)          & 31.88                   & 34.47  & 9.71    & 0.59          & 1.67       \\
SaFa (3200)          & 31.84                  & 34.71    & 9.91   & 0.61         & 1.74        \\
SaFa (4800)          & 31.88                  & 34.97    & 10.68    & 0.62         & 1.78        \\
\bottomrule[1pt]
\end{tabular}

\vspace{-5pt}
\caption{Length adaptation of SaFa on panorama generation.}
\vspace{-15pt}
\label{tab:length}
\end{table}


% \section{Related Works}
% % \label{sec:Related Work}
% \subsection{Latent Diffusion Models}
% A key factor of diffusion models’s success is to applied effective learned representations across diverse data types. Extensive research has explored self-supervised learning to develop either discrete tokenizers \cite{van2017neural, defossez2022high} or continuous distributions \cite{kingma2013auto, chen2021learning}. Among these, Variational Autoencoders (VAEs) \cite{kingma2013auto} provide a regularized, compressed latent space that accelerates training and inference while enabling high-quality generation. Recent work has applied VAE latent spaces across various generation tasks and modalities \cite{rombach2022high, videoworldsimulators2024, Evans2024FastTL}, covering 2D image-based VAEs \cite{rombach2022high, podell2023sdxl} and 1D/2D mel-spectrogram representations \cite{Liu2023AudioLDMTG, Evans2024FastTL}. However, few studies have examined the unique characteristics and commonalities of these heterogeneous representations. In this paper, we aim to capture the distinctions between VAE representations based on their original data distributions (RGB pixel space and mel time-frequency space). We introduce a spectrum-inspired operator and surprisingly find it works well in recorrsponding VAE representations to pervese spectrum details withour confuse. And it can even extend to image VAE latents due to the redundancy of representations and the robustness of the diffusion model.
% Text-guided diffusion models have shown remarkable performance across a wide range of applications, including image generation \cite{peebles2023scalable, rombach2022high}, video generation \cite{he2022latent, videoworldsimulators2024}, and audio generation \cite{Liu2023AudioLDMTG, Evans2024FastTL}. A key factor in this success lies in effectively learning representations across diverse data types. Extensive research has focused on self-supervised learning to develop either discrete tokenizers \cite{van2017neural, defossez2022high} or continuous distributions \cite{kingma2013auto, chen2021learning}. Among these, Variational Autoencoders (VAEs) \cite{kingma2013auto} offer a regularized, compressed latent space that not only accelerates training and inference but also enables the generation of high-quality outputs. In recent years, numerous generation tasks across different modalities have leveraged the VAE latent space \cite{rombach2022high, videoworldsimulators2024, Evans2024FastTL}, spanning general image-based 2D VAEs \cite{rombach2022high, podell2023sdxl} and 1D/2D mel-spectrogram representations \cite{Liu2023AudioLDMTG, Evans2024FastTL}. However, limited research has delved into the unique features and commonalities of these heterogeneous representations and data types. In this paper, we analyze the distinctions between VAE representations on RBD pixels and mel time-frequency bins within a multi-view joint diffusion framework for long-form latent generation. We propose a modality-agnostic latent swap operation as an alternative to the averaging operation, achieving smooth transition while preserving detail in overlapping regions.
% \subsection{Long-Form Audio Generation}
% In long-form generation of audio, the related work can be divided into Language Models (LMs) \cite{agostinelli2023musiclm, copet2023simple, borsos2023audiolm} and Diffusion Models (DMs) \cite{evans2024long, Evans2024FastTL, tan2024litefocus}. For LMs, most of them adapt autoregressive architectures that are limited by temporal causal characteristics. This leads to issues such as gradient accumulation, an inability to generate looped outputs, and significant resource consumption \cite{agostinelli2023musiclm, copet2023simple, borsos2023audiolm, kreuk2022audiogen}. Despite some proposed acceleration methods \cite{xFormers2022, dao2022flashattention}, quality challenges persist. For DMs, Stable Audio \cite{Evans2024FastTL} trains directly on long-duration data, requiring considerable resources and incurring high inference costs. Although we note potential solutions from other fields with FIFO algorithm \cite{kim2024fifo} and 2DRoPE \cite{lu2024fit}, these methods still faces above challenges. The exploration of joint diffusion generation in audio generation remains limited \cite{polyak2024movie}, and its applicability has yet to be established.

% \subsection{Panorama Generation}
% As the supplement, we further introduce some additional training-free and training-based methods for panorama generation. Early training-free research \cite{Avrahami2022BlendedLD, Avrahami2021BlendedDF, esser2021taming} primarily relied on inpainting or outpainting techniques to produce seamless outputs. However, these methods often exhibit some repetition and are constrained by temporal causality \cite{lee2023syncdiffusion, Quattrini2024MergingAS}. Recent advancements fine-tune stable diffusion models on 360-degree panorama datasets, yielding high-quality results but typically limited to specific themes \cite{tang2023emergent, zhang2024taming}. Other methods introduce refined positional encodings adapted for varying inference lengths, while the model is required to be trained from scratch \cite{lu2024fit}.  In contrast, multiview joint diffusion \cite{zhang2023diffcollage, BarTal2023MultiDiffusionFD} performs parallel subview generation, efficiently combining them to obtain the final output without further fine-tuning. Therefore, we focus on further improving and extending joint diffusion in this paper.
% Early panorama generation research \cite{Avrahami2022BlendedLD, Avrahami2021BlendedDF, esser2021taming} primarily relies on inpainting or output-painting methods to produce seamless outputs. However, these methods often exhibit some repetition in certain areas and are constrained by temporal causality \cite{lee2023syncdiffusion, Quattrini2024MergingAS}. Recent advancements fine-tune stable diffusion models on 360-degree panorama datasets, yielding high-quality results but typically limited to specific themes \cite{tang2023emergent, zhang2024taming}. Other methods introduce refined positional encodings adapted for varying inference lengths, while the model is required to be trained from scratch \cite{lu2024fit}. In contrast, MultiDiffusion \cite{zhang2023diffcollage} and DiffCollage \cite{BarTal2023MultiDiffusionFD} perform parallel generation across multiple views but lack cohesive guidance, leading to inconsistencies in global coherence between distant windows. To address the problem, SyncDiffusion \cite{lee2023syncdiffusion} unify the overall consistent by gradients derived from a perceptual similarity loss computed across the reference view and other views. However, the extra backpropagation steps significant reduces overall efficiency. More recently, MAD \cite{Quattrini2024MergingAS} merges latent before self- and cross-attention layer to keep consistent. However, it essentially increases the sequence length in the self-attention layer significantly, making it unsuitable for models trained on fixed-length data. Compared with above methods, our Swap Forward method achieves superior blending transitions (maintaining details in overlapping regions), overall consistency (unifying style \& color and avoiding repetition), and efficiency (requiring fewer subviews with lower overlap rates without incurring additional memory or time costs).


% Audio generation (specifically referring to sound effect generation) shares a similar trajectory with music generation. The related work can be can be divided into two main categories: Language Models (LMs) \cite{agostinelli2023musiclm, copet2023simple, borsos2023audiolm} and Diffusion Models (DMs) \cite{evans2024long, Evans2024FastTL, tan2024litefocus}. Early research in music generation primarily focuses on LMs. Most of them adapt autoregressive architectures that are limited by temporal causal characteristics. This leads to issues such as gradient accumulation, an inability to generate looped outputs, and a quadratic increase in time consumption for longer sequences \cite{agostinelli2023musiclm, copet2023simple, borsos2023audiolm}. Despite some proposed acceleration methods \cite{xFormers2022, dao2022flashattention}, quality challenges persist. On the other hand, audio generation has made some progress with DMs, focusing on improving text alignment, generation quality, interaction with other modality. Only a few works focus on long-form generation. Stable Audio \cite{Evans2024FastTL} train directly on long-duration data, requiring considerable resources and incurs high inference costs. Some training-free methods enhance quality, they do not address inference quality. Althogh we note potential solution from other fields with FIFO althorigh \cite{kim2024fifo} and Rope2D \cite{lu2024fit}, it still face above existing challenges. The exploration of joint diffusion generation in audio and music generation remains limited \cite{polyak2024movie}, and its applicability is yet to be established.





% \paragraph{Long video generation}
% \label{Long video generation}
% Long video generation is a popular yet challenging field, with many recent methods focused on achieving breakthroughs from short to long video generation.A common approach is to mask certain frames during training and use the remaining available frames to predict the masked ones, facilitating long video generation.Some models, during training, use generated frames as conditioning to autoregressively produce subsequent frames,including LVDM\cite{he2022latent} , MCVD\cite{voleti2022mcvd}, SEINE \cite{chen2023seine}  and FDM\cite{harvey2022flexible}.However,autoregressive methods often result in content degradation due to error accumulation over multiple extrapolations and are limited in efficiency due to their sequential nature.NUWA-XL\cite{yin2023nuwa} generates ultra-long videos through a hierarchical "coarse-to-fine" approach, where a global diffusion model creates sparse keyframes, and local diffusion models iteratively refine details by interpolating between these keyframes.All of the above methods require substantial training resources.

% To reduce computational costs, tuning-free methods utilize existing short video models for efficient extensions. Gen-L-Video \cite{wang2023gen} creates longer, consistent videos by treating them as overlapping clips and using temporal co-denoising, while FreeNoise \cite{qiu2023freenoise} improves consistency with noise rescheduling, sliding window attention fusion, and local noise shuffle units. However, these methods often cause scene inconsistency due to coarse constraints on initial noise or overlapping latents and require memory proportional to video length, limiting their ability to generate infinitely long videos.FIFO-Diffusion\cite{kim2024fifo} significantly enhances the efficiency and quality of long video generation through diagonal and partitioned denoising without requiring retraining.However, FIFO-Diffusion\cite{kim2024fifo} faces limitations including a training-inference gap, increased computational cost for lookahead denoising.In this work, we propose a Reference-Guided Swap operation that uses independent reference denoising to guide reverse diffusion with unidirectional swaps, ensuring global consistency across subviews. Our method requires no training, adds no memory or inference time, and eliminates inference-training gaps, achieving both effectiveness and efficiency.




%audio