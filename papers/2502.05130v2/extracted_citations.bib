@article{Avrahami2021BlendedDF,
  title={Blended Diffusion for Text-driven Editing of Natural Images},
  author={Omri Avrahami and Dani Lischinski and Ohad Fried},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={18187-18197}
}

@article{Avrahami2022BlendedLD,
  title={Blended Latent Diffusion},
  author={Omri Avrahami and Ohad Fried and Dani Lischinski},
  journal={ACM Transactions on Graphics (TOG)},
  year={2022},
  volume={42},
  pages={1 - 11}
}

@inproceedings{Evans2024FastTL,
  title={Fast Timing-Conditioned Latent Audio Diffusion},
  author={Zach Evans and CJ Carr and Josiah Taylor and Scott H. Hawley and Jordi Pons},
  booktitle={ICML 2024},
  year={2024}
}

@article{Huang2023MakeAnAudio2T,
  title={Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation},
  author={Jia-Bin Huang and Yi Ren and Rongjie Huang and Dongchao Yang and Zhenhui Ye and Chen Zhang and Jinglin Liu and Xiang Yin and Zejun Ma and Zhou Zhao},
  journal={ArXiv},
  year={2023}
}

@inproceedings{Quattrini2024MergingAS,
  title={Merging and Splitting Diffusion Paths for Semantically Coherent Panoramas},
  author={Fabio Quattrini and Vittorio Pippi and Silvia Cascianelli and Rita Cucchiara},
  booktitle={ECCV},
  year={2024}
}

@article{agostinelli2023musiclm,
  title={Musiclm: Generating music from text},
  author={Agostinelli, Andrea and Denk, Timo I and Borsos, Zal{\'a}n and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and others},
  journal={arXiv preprint arXiv:2301.11325},
  year={2023}
}

@article{borsos2023audiolm,
  title={Audiolm: a language modeling approach to audio generation},
  author={Borsos, Zal{\'a}n and Marinier, Rapha{\"e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Roblek, Dominik and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and others},
  journal={IEEE/ACM transactions on audio, speech, and language processing},
  volume={31},
  pages={2523--2533},
  year={2023},
}

@inproceedings{copet2023simple,
    title={Simple and Controllable Music Generation},
    author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre DÃ©fossez},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
}

@inproceedings{du2024demofusion,
  title={Demofusion: Democratising high-resolution image generation with no \$},
  author={Du, Ruoyi and Chang, Dongliang and Hospedales, Timothy and Song, Yi-Zhe and Ma, Zhanyu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6159--6168},
  year={2024}
}

@inproceedings{esser2021taming,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12873--12883},
  year={2021}
}

@article{evans2024long,
  title={Long-form music generation with latent diffusion},
  author={Evans, Zach and Parker, Julian D and Carr, CJ and Zukowski, Zack and Taylor, Josiah and Pons, Jordi},
  journal={arXiv preprint arXiv:2404.10301},
  year={2024}
}

@article{ghosal2023tango,
  title={Text-to-Audio Generation using Instruction Tuned LLM and Latent Diffusion Model},
  author={Ghosal, Deepanway and Majumder, Navonil and Mehrish, Ambuj and Poria, Soujanya},
  journal={arXiv preprint arXiv:2304.13731},
  year={2023}
}

@inproceedings{he2023scalecrafter,
  title={Scalecrafter: Tuning-free higher-resolution visual generation with diffusion models},
  author={He, Yingqing and Yang, Shaoshu and Chen, Haoxin and Cun, Xiaodong and Xia, Menghan and Zhang, Yong and Wang, Xintao and He, Ran and Chen, Qifeng and Shan, Ying},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{kreuk2022audiogen,
  title={Audiogen: Textually guided audio generation},
  author={Kreuk, Felix and Synnaeve, Gabriel and Polyak, Adam and Singer, Uriel and D{\'e}fossez, Alexandre and Copet, Jade and Parikh, Devi and Taigman, Yaniv and Adi, Yossi},
  journal={arXiv preprint arXiv:2209.15352},
  year={2022}
}

@article{lee2023syncdiffusion,
  title={Syncdiffusion: Coherent montage via synchronized joint diffusions},
  author={Lee, Yuseung and Kim, Kunho and Kim, Hyunjin and Sung, Minhyuk},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={50648--50660},
  year={2023}
}

@article{liu2023audioldm,
  title={Audioldm: Text-to-audio generation with latent diffusion models},
  author={Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D},
  journal={arXiv preprint arXiv:2301.12503},
  year={2023}
}

@inproceedings{majumder2024tango,
  title={Tango 2: Aligning diffusion-based text-to-audio generations through direct preference optimization},
  author={Majumder, Navonil and Hung, Chia-Yu and Ghosal, Deepanway and Hsu, Wei-Ning and Mihalcea, Rada and Poria, Soujanya},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={564--572},
  year={2024}
}

@article{polyak2024movie,
  title={Movie Gen: A Cast of Media Foundation Models},
  author={Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and others},
  journal={arXiv preprint arXiv:2410.13720},
  year={2024}
}

@article{tan2024litefocus,
  title={LiteFocus: Accelerated Diffusion Inference for Long Audio Synthesis},
  author={Tan, Zhenxiong and Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={arXiv preprint arXiv:2407.10468},
  year={2024}
}

