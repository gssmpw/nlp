\section{Information Theory and Deception Attacks}

\label{sec:information-theoretic}

\addition{While our ontology categorizes \MR deception attacks, it does not explore the effects of these attacks.} To address \textbf{RQ2}, we use Kopp~et al.'s framework \cite{kopp:2018}, which connects Borden-Kopp's deception model \cite{brumley:2012} and Shannon's communication model \cite{shannon1948mathematical}, to derive an information-theoretic model of \MR deception attacks. %We establish an information-theoretic model that provides a structured method for understanding deception attacks within \MR systems.
%, shown in Figure~\ref{fig:model}. 
%The Shannon model of communication was modified to illustrate the process of deceptive attacks in the context of mixed reality. Significantly, we incorporate principles derived from the. 
%The Borden-Kopp model offers a comprehensive framework for understanding the manipulation of information with the intention of deceiving recipients. 
%According to Borden-Kopp, the ultimate objective of a deceptive attack is to manipulate the recipient's perception which could encompass the act of providing users with inaccurate information regarding the spatial positions of virtual objects, manipulating auditory signals, or even introducing illusory tactile sensations.
%Our information-theoretic model of deception attacks in \MR is . 
% \removal{Unlike other models of deception that focus on interpersonal communication~\cite{buller:1996,mcwhirter:2016,levine:2022} or emotions~\cite{gaspar:2013,gaspar:2022,kang:2022}, the Borden-Kopp model emphasizes how deceptions in news media can target both communication channels and cognitive processes.
% Other models lack clear connections between a deception and how it can be enabled through digital media.
% Correspondingly,} 
Shannon's communication model describes how information is transferred from a source to a destination as a message.
The message is sent as a signal through a transmitter to a receiver.
During transmission, the message is affected by noise, which combines with the signal.
In our model, the transmitter is an \MR headset, which acquires information from a source (e.g., an application, sensor, web service), and transmits that information in visual, auditory, or tactile forms to a user (destination) via displays, speakers, and controller vibrations (Figure~\ref{fig:mr-communication-model}).
\MR deception attacks affect the capacity of information transmission by introducing noise to degrade messages, denying access to information, or inserting fake information into messages.

%Human sensory organs (e.g., eyes, ears, and skin) receive signals from the \MR headset, which get perceived, interpreted, and processed by the user as part of decision-making processes (Section~\ref{sec:decision-making}).
%An attacker can attempt to deceive the user by inserting fake information in the signal, denying access to information, or by producing noise.
%which encompasses a wide range of data including visual displays and auditory information. 

% Drawing from Borden-Kopp's model, we recognize that deceptive signals in MR can be introduced in various ways. For instance, in the event of a processing attack, the attacker source sends a false system message to the VR headset to replace the authentic one, resulting in the transmission of a deceptive or subversive signal.
% Alternatively, in the context of a channel attack, in the Degradation Attack, the attacker may employ a noise source. This noise serves the purpose of either generating substantial interference to hinder the player's ability to accurately perceive incoming data or producing a message that closely resembles the ambient background noise, rendering it indistinguishable from the environment. In the Denial Attack, the attacker simply blocks the output signal from the transmitter preventing the user from collecting information.


\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{figures/shannon.pdf}
    \caption{MR Deception Information-Theoretic Model. Messages are transmitted by a \MR headset to a user. Deceptive messages are injected into transmissions. Noise from the attacker or environment affect channel capacity.}
    \label{fig:mr-communication-model}
    \vspace{-1ex}
\end{figure}
%\vspace{-3ex}

\subsection{Channel Capacity}
According to Shannon's channel capacity theorem \cite{shannon1948mathematical}, the capacity of a channel to transmit information depends upon several factors, including the magnitude of the signal used to encode symbols, the level of interfering noise present in the channel, and the bandwidth of the channel.
\begin{equation} \label{eq:channelcapcity}
    C = W \log_2\left(1 + \frac{S}{N_A + N_E}\right)
\end{equation}
Channel capacity \({C}\) represents the maximum amount of information that can be effectively transmitted from a source to a destination in bits per second (Equation~\ref{eq:channelcapcity}).  Bandwidth \({W}\) refers to the information transfer rate of the communication channel in hertz. As $W$ decreases, channel capacity correspondingly decreases through a linear relationship.

For \MR headsets, information is transmitted through a headset to a human user. 
Thus, channel capacity determines how much visual, auditory, and tactile information can be transmitted. 
%that delivers the virtual content to the headset. 
Signal \({S}\) is the virtual content transmitted from the headset through displays, speakers, and vibrotactile motors. 
%The stronger the signal, the more realistic and immersive the \MR experience becomes. 
Noise \({N}\) is categorized into two types: \({N_A}\) which represents noise from an attacker source, and \({N_E}\), which represents noise from the real-world environment as well as noise that comes from the system itself, such as rendering stutters or audio glitches. 
\({N_A}\) refers to potential external interference or malicious disruptions.
\({N_E}\) encompasses both ambient disturbances from the surrounding environment and internal system issues that can affect the \MR experience. 
Both these sources of noise have a negative effect the channel capacity. % and correspondingly the overall quality of the \MR experience. 

\subsection{Channel Attacks}
Channel attacks target channel capacity through reducing bandwidth, manipulating the signal, or introducing noise.
Denial attacks involve an adversary's intention to significantly reduce access to the signal by primarily manipulating bandwidth. The channel capacity \({C}\) tends to zero as the bandwidth \({W}\) tends to zero. %, which means that the available bandwidth for transmitting virtual content is severely reduced. 
%As shown in Figure~\ref{fig:attacks-mindmap} 
%Shutting down the headset, overlaying malicious data, or removing some of the virtual content, are different types of Denial Attacks, and all effectively reduce the bandwidth.
By shutting down the device, the attacker completely blocks the signal output, and bandwidth ($W$) reduces to zero. % which means no information will flow through the information channel.
Attackers can occlude task-specific information with other content, effectively reducing bandwidth and interfering with task performance. 
%access to  interference and reduces the effective capacity of the channel for transmitting meaningful information.
A Removal attack selectively removes information from the signal, reducing bandwidth as less information is transmitted per a second. % and the channel's effective capacity for conveying useful information.

%In Overt Degradation attacks, the adversary can introduce substantial levels of noise into the channel, decreasing the \SNR. As the \SNR tends towards zero,  \({log_2\left(1 + \frac{S}{N_A + N_E}\right)}\) approaches zero. Consequently, this causes the channel capacity \({C}\) to decrease and eventually reach zero. 
In Overt Degradation attacks, the adversary can introduce substantial levels of noise into the channel, decreasing the \SNR. As the \SNR tends towards zero, channel capacity \({C}\) decreases and eventually reaches zero.
In this case, the user is bombarded with excessive noise, making it impossible to distinguish between the intended content and the attacker's noise. 
%According to Figure~\ref{fig:attacks-mindmap} 
An example of this attack is sensory overload, where an attacker overwhelms the user by emitting excessive sensory stimuli through the \MR headset, resulting in disorientation and discomfort.

In Covert Degradation attacks, an adversary can reduce the signal strength, which results in a decrease in the \SNR. As the signal tends toward zero, \SNR also tends toward zero, decreasing \({C}\) towards zero as well. In \MR headsets, these attacks can involve subtle manipulation of sensory cues presented to a user. Subtle boundary manipulation and subtle dimension manipulation are examples of these attacks. Through subtle manipulation of boundaries or the sizes of virtual objects, the attacker can deceive the user into thinking they are not moving~\cite{Casey_2021} or make it harder to interact with virtual objects.
%might move the boundaries of the \MR environment. % or employ misdirection. The goal of these two attacks is to reduce the signal strength.

\subsection{Processing Attacks}
Processing attacks manipulate cognition through deceptive methods that mimic the \MR system.
We use Vitanyi's model~\cite{Vitanyi} to formalize how deceptive information and messages created by an attacker, \({X}\), differ from actual information and messages created by an \MR system, \({Y}\):

\begin{equation}
D(X, Y) = \frac{K(XY) - \min(K(X), K(Y))}{\max(K(X), K(Y))}
\end{equation}

\begin{equation}
M(X, Y) = 1 - D(X, Y)
\end{equation}
where \({D}\) represents the measure of difference, \({M}\) represents the measure of similarity or mimicry, and \({K}\) is the editing function applied to \({X}\) and \({Y}\).

Corruption attacks involve altering data during transmission. Vitanyi's difference measure \({D(X, Y)}\) quantifies the degree of alteration between the original message \({X}\) and the corrupted message \({Y}\).  
%A greater value of \({D(X, Y)}\) signifies a substantial level of corruption. 
In \MR, corruption attacks might involve unauthorized changes to visual information, such as application and system messages, as well as sensory information, including camera, geolocation, and battery status (Figure~\ref{fig:attacks-mindmap}).
Subversion attacks, on the other hand, involve  manipulating how users interpret information within an \MR system. These attacks require repeated corruption or covert degradation attacks to reduce user's trust and understanding.
Thus, $M$ must remain close to $1$ as the user has a greater chance of detecting deceptions through repeated exposure.
%A decreased value of \({M(X, Y)}\) signifies a heightened level of subversion, indicating a substantial departure from the anticipated output as a result of malicious manipulation.

\gap{While Vitanyi's model formalizes mimicry, we lack models that effectively describe how processing attacks impact human behavior. Specialized domains, such as formal methods in human-computer interaction, could offer valuable insights.}

% \gap{While Vitanyi's model formalizes mimicry, we lack models that adequately describe how processing attacks affect human behavior. Specialized domains, such as formal methods in human-computer interaction, may provide valuable insights. }