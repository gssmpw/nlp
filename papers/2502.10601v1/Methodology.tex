

\section{Methodology}
\label{sec:Methodology}

We denote a low-resolution \ac{WFM} by $\mathbf{X} \in [0, 1]^{L \times L}$. Our objective will be to learn a map $\mathbf{M}: [0, 1]^{L \times L} \to \{0, 1\}^{H \times H}$ that will downscale $\mathbf{X}$ to a high-resolution \ac{FIM} $\hat{\mathbf{Y}} \in \{0, 1\}^{H \times H}$, where $H = fL$ and $f > 1$ is a scale factor, so that $\hat{\mathbf{Y}}$ recovers the (true) \ac{FIM} $\mathbf{Y}$ that is associated to $\mathbf{X}$. In our work, we use $f=10$ to replicate the spatial resolution produced by existing high resolution satellite products. In order to learn $\mathbf{M}(\cdot)$, we assume that each model we consider aims to learn the conditional joint probability $p(\mathbf{Y} | \mathbf{X})$. Furthermore, for simplicity, we will assume that the entries of $\mathbf{Y}$ (\ac{FIM} pixel intensities) are mutually independent, when conditioned on the entries of $\mathbf{X}$ (\ac{WFM} pixel intensities), \textit{i.e.}, $p(\mathbf{Y} | \mathbf{X}) = \prod_{i,j} p(Y_{i,j} | \mathbf{X})$. Finally, assuming that the model accurately estimates the probabilities $p(Y_{i,j} = 1 | \mathbf{X})$ via $S_{i,j}(\mathbf{X}) \in [0,1]$, the \ac{FIM} pixel intensities are reconstructed as $\hat{Y}_{i,j} = 1$, if $S_{i,j}(\mathbf{X}) \geq 0.5$ and $0$, if otherwise. In what follows, we present the neural-based architectures we have chosen to implement the map $\mathbf{M}(\cdot)$ for this downscaling task.


%Consider an input image $X \in \mathbb{R}^{L \times L}$ where $L$ is the length/width of the low-resolution image (In our work, $L=10$).
% Let $X_{\textit{in}}$ and $Y_{\textit{in}}$ be the associated image index maps. 
%The objective here is to obtain its higher resolution output, $Y \in \mathbb{R}^{H \times H}$ where $H=f \times L$, $f$ is the scaling factor and $H$ is the length/width of the output image. In our work, we set $f=10$ to replicate the spatial resolution produced by high resolution satellite products. 
%We therefore aim to learn the neural network $F(X)$ to produce estimate of the high resolution \ac{FIM} $\hat{Y} \triangleq F(.)$. 

\subsection{Model Architectures}
\label{subsec:ModelArchitecture}

% by viewing it as a residual learning task, \textit{i.e,} we are interested in learning the residual between the output and the input rather than the entire map, we adapt existing architectures for super-resolution, namely \acp{RDN} and \acp{RCAN} whose success in residual learning tasks has helped maintain relevance through the ever changing landscape of image super-resolution.

There have been a variety of works that use deep learning techniques to learn highly complex mappings between the low- and high-resolution images. Note that, in our data, most of regions are not inundated with floods. This means that the output patch that corresponds to a water fraction of zero will be all zeros (no inundation). The same applies to fully inundated pixels. Therefore, once mapped to a higher resolution, we simply need to learn the residual map between the \ac{WFM} and \ac{FIM}. A residual connection from the input to the output therefore plays an important role in such architectures. This connection, in essence, crudely downscales a \ac{WFM} to a \ac{FIM} and the network learns a modification to this mapping that performs better at downscaling. This naturally lead us to the path of residual learning. Residual learning aims to learn the residual, or difference, between the output and input images \cite{He2016DeepRecognition}.  Residual learning is mainly motivated by the following reasons (i) an unexpected training performance degradation, when networks grow deeper and, hence, should overfit and perform better and (ii) training becomes less prone to exploding or vanishing gradients. 

% In addition to global residual learning taking advantage of the correlation between inputs and outputs, there is also use of local residual learning which is shown to be effective in resolving the vanishing gradient problem. 
% The successes of residual learning based models have been leveraged by two state-of-the-art architectures,  

In order to be able to train deep architectures we will rely on residual learning, which offer the aforementioned benefits. More specifically, we use \acp{RDN} \cite{Zhang2018ResidualDenseSuperResolution} and \acp{RCAN} \cite{Zhang2018RCANSuperResolution} (see \cite{Wang2021DeepSurvey} for a comparative analysis).
Apart from this, we also consider an efficient transformer based architecture called \ac{ESRT} \cite{LuESRT2022} that has been shown to be a strong competitor of late. A common thread among all of these architectures is a shallow feature extraction layer and a final dense fusion layer. The final stage incorporates global residual learning and feature fusion to produce feature vectors, which are then passed through a transposed convolution layer  -- see  \cite{Shi2016SubPixelConvolution} -- in order to bring it to a size of $100 \times 100$ before being passed through a few more convolution layers to produce the high resolution \ac{FIM}. In the following subsections, we briefly describe the individual architectures. 


\subsubsection{RDN}
%\acp{RDN} \cite{Zhang2018ResidualDenseSuperResolution} use a residual learning approach, which has been shown to exploit hierarchical features in the low-resolution image. \acp{RDN} make use of a series of Residual Dense Blocks (RDBs) with skip connections that help enforce a contiguous memory mechanism via the use of local feature fusion and local residual learning. In addition to mitigating the vanishing gradient problem, these skip connections help improve information flow, i.e., the low-level information extracted from the earlier stages are not necessarily lost as they propagate forward. For our models, we used 20 RDBs with 20 layers each.

\acp{RDN} \cite{Zhang2018ResidualDenseSuperResolution} combine the use of residual learning and of densely connected convolutional blocks: each such block consists of a number of convolutional layers, whose inputs consist of the outputs of all previous layers (via skip connections) within the same block; these inputs are regarded as hierarchical features. Finally, a skip connection routes the input of such blocks to their outputs in order to implement a type of local residual learning. \acp{RDN} employ global residual learning and consist of a long cascade of such blocks and a final upsampling stage that yields the downscaled image. In our \ac{RDN} architecture, we use 12 features in the convolutional layers, a kernel size of 3, 20 residual blocks and 20 layers per residual block.


% Topographic features and low-resolution \acp{WFM} are independently passed through a series of \acp{RDB}, which perform feature extraction. 

% Note that our \ac{RCAN} architecture is very similar to the one of \ac{RDN}.

\subsubsection{RCAN}

The \ac{RCAN} was introduced in \cite{Zhang2018RCANSuperResolution}. \acp{RCAN} make use of channel attention, which aims to exploit the inter-dependencies between feature channels. This is done by first aggregating channel-wise features and applying a gating mechanism that learns non-linear relationships between the feature channels. Following this, the features are passed through Residual-in-Residual (RIR) blocks, wherein residual learning is enforced. RIR blocks contain \acp{RCAB}, which allows the network to focus on the important aspects of the low resolution features. In our model, we used 10 residual groups, each of which contains several residual blocks with short skip connection, 10 residual channel attention blocks, 64 features, kernel size of 3 and a reduction factor of 16. 


% Following this, it passes through a Residual in Residual blocks which consists of a series of \ac{RG} along with a long skip connection. Each \ac{RG} in itself consists of a series of \acp{RCAB} that has an overarching skip connection referred to as a short skip connection.

% We delve into \acp{RCAB} by first introducing the concept channel attention. The output is then used to rescale the aforementioned channel-wise aggregate. This mechanism helps to adaptively rescale the residual component in the \ac{RCAB} that allows the network to focus on the important aspects of the low resolution features. 


\subsubsection{ESRT}

Transformers have shown to be quite effective in sequential tasks  for natural language processing in comparison to convolutional neural networks \cite{Vaswani2017Attention}. The self-attention mechanism in transformers was applied to computer vision tasks in \cite{dosovitskiy2021an} by treating each image as a sequence of sub-images. While transformers typically feature an encoder and a decoder block when used for natural language processing tasks, for computer vision tasks they only feature an encoder to embed images into some feature space and then use another deep learning architecture for downstream tasks such as classification and super-resolution. \cite{LuESRT2022} proposed the \acf{ESRT}, a low-complexity transformer architecture tailored to super-resolution tasks. In our usage of \ac{ESRT}, we used 3 encoder layers with 32 features and kernel size of 3, each with a channel attention layer. The attention layer was a multi-scale local attention block consisting of 288 features. 


\subsection{Training}
\label{subsec:training}

All of our data-driven, downscaling models were trained to minimize a penalized version of 
the average (over pixels) cross-entropy for each data pair $(\mathbf{X}, \mathbf{Y})$
%
\begin{align}
    L_{\mathrm{PACE}}(\mathbf{X}, \mathbf{Y}) & \triangleq L_{\mathrm{ACE}}(\mathbf{Y}, \mathbf{S}(\mathbf{X})) + \eta 
 P(\mathbf{X}, \mathbf{S}(\mathbf{X}))   
    \shortintertext{where $\eta \geq 0$ is a penalty parameter and}
    L_{\mathrm{ACE}}(\mathbf{Y}, \mathbf{S}(\mathbf{X}))  & \triangleq \frac{1}{H^2} \sum_{i,j} \left[ Y_{i,j} \ln S_{i,j}(\mathbf{X}) + \right. \nonumber
    \\
    & \left. + (1 - Y_{i,j}) \ln (1 - S_{i,j}(\mathbf{X})) \right]
    \\
    P(\mathbf{X}, \mathbf{S}(\mathbf{X})) & \triangleq \sum_{i,j} \left( X_{i,j} - \frac{1}{f^2} \sum_{k,l} S_{fi+k, fj+l}(\mathbf{X}) \right)^2
\end{align}
%
where $i,j \in \{0, 1, \ldots, L-1\}$ and $k,l \in \{0, 1, \ldots, f-1\}$. The penalty term $P(\cdot, \cdot)$ quantifies the deviation between the fraction of inundated pixels in a $f \times f$ \ac{FIM} patch vis-\'{a}-vis its corresponding \ac{WFM} pixel. Using higher values of the penalty parameter $\eta$ for training our downscaling models, significantly penalizes even modest deviations and, in essence, enforces the matching of water fractions between \acp{FIM} and \acp{WFM}. In our study, this was important as such matching occurred in our \ac{SYN} data. Finally, let us point out that we treated $\eta$ as a model hyper-parameter.

% We use a modified loss function for the purposes of training the architectures described earlier.

% \subsubsection{Cross Entropy Loss}

% The output, although an image, comprises of pixels that are either turned on or off. A pixel of value 1 indicates a flooded pixel. As such, we use a binary cross entropy loss, a classification loss, on each pixel. Note that for convenience, we set $\hat{Y} \triangleq RDN(X)$

% \begin{align}
% \begin{split}
% L_{bce}(Y, \hat{Y}) =  \frac{1}{H^2}\sum_{\forall (i,j) \in Y_{\textit{in}}} ( &\log{\hat{Y}_{i,j}} Y_{i,j} + \\&(1-Y_{i,j})\log{1-\hat{Y}_{i,j}})
% \end{split}
% \end{align}

\begin{figure*}[htpb!]
  \centering
  \captionsetup{justification=centering} \includegraphics[width=\textwidth,height=15cm]{figures/IowaDesMoines_samples.png}
  \caption{\label{fig:RWIowaDesMoines} Sample outputs for the \ac{RW} Iowa Des Moines region; \textit{i.e.,} the region over which the model was trained. }
\end{figure*}


\begin{figure*}[htpb!]
  \centering
  \captionsetup{justification=centering}
  \includegraphics[width=\textwidth,height=15cm]{figures/EU_samples.png}
  \caption{\label{fig:EUSamples} Sample outputs for the \ac{RW} EU region; \textit{i.e.,} external to the regions over which the model was trained.}
\end{figure*}


% \subsubsection{Soft constraint on Flood ratio}

% Noticeably, in the aforementioned loss, there is no inclusion of the input $X$. This is important in our application since it enforces a strong constraint on the number of activated (flood) pixels in the output. More specifically, each pixel in the low-resolution image, $X_{i,j} \in [0,1]$, is designed to approximate the total fraction of floods in the corresponding patch from the higher resolution image. We would like to make a clear distinction here, for our training data and test, this satisfaction of constraints is guaranteed since we constructed the \ac{WFM} from the high resolution \ac{FIM} directly. However, in real world deployments, we would produce \ac{WFM} from multi-spectral satellite outputs and by construction (via spectral unmixing) would have some noise inundated. Similar to \cite{Yin2022}, we assume an ideal noiseless setting in this work. Hence, to allow for inexact water fraction satisfaction, we use a soft constraint instead of hard constraints, such as those used in \cite{harder2022generating}.This we do so because in \ac{RW} input fraction data may not be fully accurate due to sensor as well as algorithm noise. This can be later fine tuned on a real-world dataset if needed. The soft constraint is designed as follows.

% \begin{align}
% \begin{split}
% L_{sc}(X, Y, \hat{Y}) =  \eta &\sum_{\forall (i,j) \in X_{\textit{in}}} || X_{i,j} - \\ &-\frac{1}{f^2} \sum_{ k,l \in \{ 1,2,..., f\} } \hat{Y}_{f \times i+k, f \times j+l} ||^2_{2}
% \end{split}
% \end{align}

% Where $\eta$ is a hyperparameter.  The overall loss function for training the \ac{RDN} can be expressed as the combination of both the aforementioned losses.

% \begin{align}
%     L(X, Y, \hat{Y}) &= L_{bce}(Y, \hat{Y}) + L_{sc}(X, Y, \hat{Y})
% \end{align}

The weights of the neural network were optimized using Adam \cite{Kingma2015} with a decaying learning rate. The hyper-parameters along with their ranges in parentheses are as follows. The learning rate ($10^{-5}$ to $10^{-4}$), $\eta$ (0 to 2000), layer dropout probability ($10^{-3}$ to $20^{-2}$), random seed (100 to 900 in multiples of 100). For the \ac{RDN}, number of features (8 to 64 in mutliples of 8), number of residual blocks (2 to 16 in multiples of 2), number of layers per block (2 to 32 in multiples of 2) and the kernel size was fixed to 3. For the \ac{RCAN}, number of features (4 to 64 in multiples of 4), number of residual groups (10 to 40 in multiples of 5), number of residual channel attention blocks (20 to 50 in multiples of 5), reduction factor (16 to 64 in multiples of 4). For the \ac{ESRT}, we used the hyper-parameters as available in the code for \cite{LuESRT2022}. We used Optuna\footnote{\href{https://optuna.readthedocs.io/en/stable/\#}{https://optuna.readthedocs.io/en/stable/\#}} which in turn used the Tree-structured Parzen Estimator \cite{Bergstra2011HPOptimizationAlgorithms} to produce candidates for hyper-parameter search.


\begin{figure*}[htpb!]
  \centering
  \captionsetup{justification=centering}
  \includegraphics[width=\textwidth,height=15cm]{figures/Ghana_samples.png}
  \caption{\label{fig:GhanaSamples} Sample outputs for the \ac{RW} Ghana region; \textit{i.e.,} external to the regions over which the model was trained.}
\end{figure*}



%%%%%%%%%%%%%%%%% Unused text %%%%%%%%%%%%%%%%%%%%%


% The RDN will be represented by its functional form $RDN(X) \in \mathbb{R}^{H \times H}$ for convenience. \acp{RCAN} also use residual learning for their task. Specifically, this architecture improves upon the \ac{RDN} by adding attention layers. This is motivated by successes of transformer based architectures for image processing applications \cite{Vaswani2017Attention}. Along a similar vein, we also utilize a recently succesful transformer architecture called \ac{ESRT} \cite{LuESRT2022} for downscaling \acp{WFM}.

% \ac{ESRT} consists of a \acp{LCB} and \acp{LTB}.

% The \ac{LCB} contains a High Preserving Block (HPB), whose goal is to preserve high frequency components of the input and Adaptive Feature Residual Block (AFRB) that contains local residual learning to ensure information flow. \acp{LTB} can dynamically adjust the size of the feature map to extract deep features with a low computational costs by adopting High Preserving Block (HPB). \ac{LTB} is composed of a series of Efficient Transformers (ET) and is used to model the long-term dependence between similar local regions in an image. 