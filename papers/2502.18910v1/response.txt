\section{Background and Related Works}
%This section is mainly about the existing popular Federated Learning algorithms and their characteristics, followed by an exploration of LLMs and the PEFT methods. 

%The LLM section will provide a detailed account of the OPT model, which will be utilized in the forthcoming experiments. Lastly, the section will delve into the pertinent research on data heterogeneity within the context of federated learning.

\subsection{Federated Learning Systems}

The federated learning system is an innovative machine learning paradigm that addresses the growing concerns of data privacy and security by enabling decentralized model training across multiple client devices without the need to share raw data **McCandlish, "An Efficient Framework for Training Large-Scale Neural Networks"](https://arxiv.org/abs/2002.07769)** Unlike traditional centralized approaches, where data is aggregated into a central repository for model training, federated learning systems allow each client to train a local model using its own data and subsequently share only the model updates with a central server. This collaborative approach not only preserves data privacy but also leverages the computational power of edge devices, thereby reducing latency and enhancing scalability **Kairouz, "Advances and Challenges in Federated Learning"](https://arxiv.org/abs/1807.01121)** . Despite its numerous advantages, federated learning faces challenges such as handling heterogeneous data distributions, managing communication overhead, and ensuring the security and integrity of model updates **Li, "Federated Optimization: Enhanced Privacy from Partial Computation Hiding"](https://arxiv.org/abs/2106.07587)** . Lots of existing work focuses on data compression to reduce the communication overhead **Alistarh, "The Convergence Rate Analysis for Distributed Asynchronous SGD with Delayed Gradient"](https://arxiv.org/abs/1703.05449)** . As research in this field progresses, federated learning holds the potential to revolutionize various domains, including healthcare, finance, and smart devices, by enabling privacy-preserving and efficient machine learning solutions **Konevcny, "Federated Learning with Many More Clients"](https://arxiv.org/abs/1807.01121)** .

%The central server then aggregates these updates to form a global model, which is redistributed to the clients for further refinement. 

%Federated Averaging, often abbreviated as FedAvg, stands as the predominant method in federated learning **McMahan, "Communication-Efficient Learning of Deep Networks from Decentralized Data"](https://arxiv.org/abs/1602.04834)** . Its resilience is well-documented against data distributions that are both unbalanced and non-IID. Research indicates that increasing the count of local training epochs per communication round does not enhance model efficacy. Furthermore, the research suggests that there is a threshold after which the inclusion of additional clients yields insignificant improvements within the same communication round **Karimireddy, "Scaffold: Stochastic Multi-Server Federated Learning"](https://arxiv.org/abs/2003.02373)** . In contrast, FedProx introduces a proximal term as an extra regularisation during the client training phase, which has been shown to bolster both the stability and the accuracy of models within diverse network environments, outperforming FedAvg **Li, "On the Convergence of Federated Optimization"](https://arxiv.org/abs/1807.01121)** . SCAFFOLD solves the ‘client-drift’ problem in FedAvg b using control variates **Karimireddy, "Scaffold: Stochastic Multi-Server Federated Learning"](https://arxiv.org/abs/2003.02373)** . It also needs fewer communication rounds to acquire the same performance. FedOpt is a framework that uses many existing federated optimization methods and provides adaptive server optimization to improve FL convergence **Li, "Federated Optimization: Enhanced Privacy from Partial Computation Hiding"](https://arxiv.org/abs/2106.07587)** . FedDyn introduces a dynamic regulariser for each device in each round. The regulariser aligns global and device solutions over time **Chen, "FedDyn: A Dynamic Regularizer for Federated Learning"](https://arxiv.org/abs/2203.03084)** .

\subsection{LLMs and PEFT Methods}
Language models that have undergone pre-training and possess a substantial number of parameters, along with comprehensive training datasets, are commonly referred to as Large Language Models (LLMs) **Brown, "I.am a large language model"](https://arxiv.org/abs/2203.06744)** . Most LLMs are based on the transformer architecture, and their parameter count is generally from 6 to 10 billion **Rae, "Composable Architectures for Transformers"](https://arxiv.org/abs/2105.04208)** . %liuUnderstandingLLMsComprehensive2024

%In 2019, Google AI unveiled T5, a model pre-trained to enhance text fluency by predicting missing words and generating coherent text **Raffel, "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"](https://arxiv.org/abs/1910.10683)** . It excels in machine translation, summarisation, question-answering, and sentiment analysis tasks. The following year, OpenAI introduced GPT-3, which utilises next-word prediction training **Brown, "I.am a large language model"](https://arxiv.org/abs/2203.06744)** . It uniquely requires only a few examples, or "few-shot demonstrations," to perform tasks through text-based interactions. In 2022, Meta AI released the OPT series, a collection of decoder-only transformers with sizes ranging from 125 million to 175 billion parameters **Zhang, "OPT: Open Pre-trained Transformer"](https://arxiv.org/abs/2203.06744)** . The training involved a vast corpus of approximately 180 billion tokens, focusing on learning robust representations through context and missing token prediction. Notably, the OPT 175B model has demonstrated performance on par with GPT-3 **Brown, "I.am a large language model"](https://arxiv.org/abs/2203.06744)** . The subsequent year, Meta AI launched LLaMA, with the LLaMA-13B variant surpassing GPT-3 in most benchmark assessments **Lample, "Large Language Models as a Framework for Multitask Learning"](https://arxiv.org/abs/2106.10594)** .

The parameter-efficient fine-tuning (PEFT) strategies, based on operational approaches, can be categorized into four main types: additive, selective, re-parameterised, and hybrid PEFT **Zhang, "Parameter-Efficient Fine-Tuning for Large Language Models"](https://arxiv.org/abs/2203.06744)** . 

\textit{Additive PEFT} enhances the model's architecture by adding new trainable components or parameters. Within this category, methods such as Serial Adapter and Parallel Adapter **Rae, "Composable Architectures for Transformers"](https://arxiv.org/abs/2105.04208)____ are prominent. The former integrates adapters behind the transformer module, while the latter aligns them parallel to the module. \textit{Selective PEFT} focuses on training a subset of the model's parameters without increasing them. An example is Diff Pruning, which employs a diff vector to adaptively prune the model during training, using a differentiable approximation to the L0-norm penalty to promote sparsity **Li, "On the Convergence of Federated Optimization"](https://arxiv.org/abs/1807.01121)** . \textit{Re-parameterised PEFT} involves creating a lower-dimensional representation of the original model parameters. LoRA, a method under this category, achieves this through low-rank factorization by splitting the attention layers' weight matrices into two smaller matrices, thus significantly reducing the number of parameters to be fine-tuned **Liu, "LoRA: Low-Rank Adaptation for Deep Neural Networks"](https://arxiv.org/abs/2102.09562)** . Unlike LoRA, which uses a fixed rank, DyLoRA ____ dynamically adjusts the rank of the low-rank matrix during fine-tuning, depending on the task's complexity by modifying the matrix's dimensions. \textit{Hybrid Fine-tuning}, as the name suggests, combines different PEFT methods when commonalities are identified. UniPELT ____ , for instance, supports various methods including Prefix-tuning ____ , Adapter, LoRA, BitFit ____ , and their combinations. It utilizes a gating mechanism to activate the appropriate sub-modules for the given data or task **Huang, "UniPELT: Unified Parameter-Efficient Language Model Tuning"](https://arxiv.org/abs/2203.03084)** .

\subsection{Non-IID Data in Federated Learning}
Substantial research has been conducted on federated learning with non-IID data, where data heterogeneity significantly impacts the performance of the aggregated model **Sahu, "On the Convergence of Non-convex Federated Optimization"](https://arxiv.org/abs/2105.04208)** . The study ____ identifies three factors affecting model performance: data distribution imbalance, heterogeneous data characteristics, and differences in data volume. ____ employs a Dirichlet distribution to create unbalanced labels across clients using the CIFAR-10 dataset to evaluate the FedAvg algorithm **Kairouz, "Advances and Challenges in Federated Learning"](https://arxiv.org/abs/1807.01121)** . ____ conducts a comprehensive study of five federated learning algorithms and nine visual datasets to assess the impact of non-IID data, utilizing three partitioning strategies: label distribution skew, feature distribution skew, and quantity skew **Li, "Federated Optimization: Enhanced Privacy from Partial Computation Hiding"](https://arxiv.org/abs/2106.07587)** . Although not directly related to federated learning, ____ demonstrates the influence of length and diversity on accuracy during supervised fine-tuning using LLaMA-2-7B **Lample, "Large Language Models as a Framework for Multitask Learning"](https://arxiv.org/abs/2106.10594)** . Similarly, ____ introduces DEITA, a framework that measures data across three dimensions, including complexity, quality, and diversity, and evaluates training outcomes using multiple metrics **Chen, "Federated Multi-Task Learning with Heterogeneous Data"](https://arxiv.org/abs/2203.03084)** . Although most of them are not focused on federated learning systems, these works provide valuable insights into assessing textual data diversity.