\begin{algorithm}[H]
\caption{$ \as $ $\cA$}\label{alg:AdaBoostSample}
\begin{algorithmic}[1]
\State\textbf{Input:} Training sequence $S\negmedspace\in\negmedspace(\cX\negmedspace\times\negmedspace \cY)^{*}\negmedspace,$ string $r\negmedspace \in\negmedspace ([0,1]^{*})^{*}\negmedspace\negmedspace.$ 
\State\textbf{Output:} Majority vote with margin $ 3/4.$
\State $(m,n,s) \gets (|S|,|r|,|r_{1}|)$ \label{alg:AdaBoostSamplemset} \label{alg:AdaBoostsamplesetn}\label{alg:AdaBoostsamplesets}
\State $(D_1,C_{1})\gets ((\frac{1}{m},\ldots,\frac{1}{m}),(\frac{1}{m},\ldots,1))$ \label{alg:AdaBoostSamplesetDone} \label{alg:AdaBoostSamplesetCone}
\State $t \gets \left\lceil 20^{2}\ln(m)/2\right\rceil$\label{alg:AdaBoostSampletset} 
\State $f \gets [0]^{t}$\label{alg:AdaBoostSamplesetf} 
\State $(\theta,\gamma)\gets (3/4,9/20)$ \label{alg:AdaBoostSamplethetaset}  \label{alg:AdaBoostSamplegammaset}
\State $l=\sum_{j=1}^{0}\ind_{\alpha_{i}>0}\gets 0$ \label{alg:AdaBoostSamplesettingl}
\State\textbf{for $ i \in \{ 1:n \} $ } \label{alg:AdaBoostSampleforloop}
\State\hspace{0.5cm}\vline\quad $S_i \gets \C_i^{-1}(r_{i})$\label{alg:AdaBoostSample:sample} 
\State\hspace{0.5cm}\vline\quad$h_i\leftarrow \erm (S_i )$\label{alg:AdaBoostSampletraining} 
\State\hspace{0.5cm}\vline\quad$1/2-\gamma_{i}=\loss_i=\ls_{D_{i}}(h_i)$\label{alg:AdaBoostSamplelossoftrained} 
\State\hspace{0.5cm}\vline\quad\textbf{if $1/2-\gamma_{i}=\loss_{i}\leq 1/2-\gamma$ \label{alg:AdaBoostsamplenonzeroweight} then}
\State\hspace{1cm}\vline\quad$(\alpha_i,\sum_{j=1}^{i}\ind_{\alpha_j>0})\gets(\frac{1}{2}\ln{\left(\frac{1+2\gamma}{1-2\gamma}\right)- \frac{1}{2}\ln{\left(\frac{1+\theta}{1-\theta} \right)}},\sum_{j=1}^{i-1}\ind_{\alpha_j>0}+1)$\label{alg:AdaBoostSamplealphaset}
\State\hspace{1cm}\vline\quad\textbf{if $\sum_{j=1}^{i}\ind_{\alpha_j>0}\leq t$ \label{alg:AdaBoostSampleearlystop} then}
\State\hspace{1.5cm}\vline\quad$(\gamma_{f,l},\epsilon_{f,l},h_{f,l},\alpha_{f,l},D_{f,l})\gets (\gamma_{i},\epsilon_{i},h_{i},\alpha_{i},D_{i})$\label{alg:AdaBoostSampleupdatel}
\State\hspace{1.5cm}\vline\quad$f\gets f+\alpha_{f,l} h_{f,l}$\label{alg:AdaBoostSampleaddsf}
\State\hspace{0.5cm}\vline\quad\textbf{else }
\State\hspace{1cm}\vline\quad$(\alpha_i,\sum_{j=1}^{i}\ind_{\alpha_j>0})\gets(0,\sum_{j=1}^{i-1}\ind_{\alpha_j>0})$ \label{alg:AdaBoostSamplezeroweight}
\State\hspace{0.5cm}\vline\quad\textbf{for $ i \in \{ 1:m \}$ }\label{alg:AdaBoostSampleupdatestart}
\State\hspace{1cm}\vline\quad$D_{i+1}(j)\gets D_{i}(j)\exp(-\alpha_i h_i(S_{i,1}) S_{i,2})$\label{alg:AdaBoostSamplegetweights}
\State\hspace{0.5cm}\vline\quad$Z_{i}\gets \sum_{j=1}^{m} D_{i+1}(j)$\label{AdaBoostSamplesetnormalization}
\State\hspace{0.5cm}\vline\quad\textbf{if $1/2-\gamma_{i}=\loss_{i}\leq 1/2-\gamma$ and $l=\sum_{j=1}^{n}\ind_{\alpha_j>0}\leq t$ then}
\State\hspace{1cm}\vline\quad $ Z_{f,l}\gets Z_{i} $\label{alg:AdaBoostSamplegetsZ} 
\State\hspace{0.5cm}\vline\quad$D_{i+1}\gets D_{i+1}/Z_{i}$\label{alg:AdaBoostSamplenormalizing}
\State\hspace{0.5cm}\vline\quad$C_{i+1}(1)\gets D_{i+1}(1)$\label{alg:AdaBoostSamplecummlativeupdate}
\State\hspace{0.5cm}\vline\quad\textbf{for $ j\in\{ 2:m \} $  }
\State\hspace{1cm}\vline\quad$C_{i+1}(j)\gets C_{i+1}(j-1)+D_{i+1}(j)$  \label{alg:AdaBoostsampleendforloop}
\State\textbf{if $\sum_{j=1}^{n}\ind_{\alpha_j>0}\geq t$ then}\label{alg:AdaBoostSamplecheckt}
\State\hspace{0.5cm}\vline\quad$f \gets \frac{f}{t\alpha_{f,1}}=\sum_{j=1}^{t} \frac{\alpha_{f,j}h_{f,j}}{\sum_{l=1}^{t}{\alpha_{f,l}}}=\sum_{j=1}^{t} \frac{h_{f,j}}{t}$ \label{alg:AdaBoostsample:output1} 
\State\hspace{0.5cm}\vline\quad\textbf{if $\sum_{x\in S} \ind\{\sum_{i=1}^{t} h_{f,i}(x)c(x)/t\leq \theta \}/m < \frac{1}{m}$ then}\label{alg:AdaBoostSamplecheckingmargin}
\State\hspace{1cm}\vline\quad\textbf{return $f$}\label{alg:AdaBoostSampleoutputgood}
\State\textbf{return $ \erm(S) $ }\label{alg:AdaBoostsample:output2}\label{alg:AdaBoostsample:output3}
\end{algorithmic}
\end{algorithm}
Line~\ref{alg:AdaBoostSamplemset} reads, the number of training examples $(x_i,y_i)\in\cX\times \cY$ in $S$, the number of entries in $r$, and number of entries in any $r_i$. 
Line~\ref{alg:AdaBoostSamplesetDone} initializes both, the distribution and cumulative distribution over $[m]$. 
Line~\ref{alg:AdaBoostSampletset} sets the number of hypotheses in the final majority vote for the early stopping criteria. 
Line~\ref{alg:AdaBoostSamplesetf} initializes $f$ as the $0$ function on $\mathcal{X}$/ an empty array of size $t.$ 
Line~\ref{alg:AdaBoostSamplethetaset} sets the target margin for the final majority vote and  sets target error for voters.
Line~\ref{alg:AdaBoostSamplesettingl} sets the counter for early stopping to $0$, represented both as $ l $ and the more descriptive $ \sum_{j=1}^{0}\ind_{\alpha_j>0}.$ 
Line~\ref{alg:AdaBoostSample:sample} decides $S_{i}\sqsubset S$ based on  $C^{-1}_{i}$ and $r_i\in [0,1]^{s}$, where $|S_{i}|=|r_{i}|=s$. 
Line~\ref{alg:AdaBoostSampletraining} runs $ \erm $ on $S_i$ to obtain hypothesis $ h_{i}.$ 
Line~\ref{alg:AdaBoostSamplelossoftrained} calculates the loss of the trained hypothesis $ h_{i}.$ 
Line~\ref{alg:AdaBoostsamplenonzeroweight} includes hypotheses with a loss smaller than $1/2-\gamma=1/2-9/20=1/20$. 
Line~\ref{alg:AdaBoostSamplealphaset}  sets $\alpha_{i} $ and updates the counter $ \sum_{j=1}^{i}\ind_{\alpha_j>0} $  with a plus $ 1 $ as this was a successful boosting step. 
Line~\ref{alg:AdaBoostSampleearlystop} ensures that the majority vote will consist of at most $t$ hypotheses by employing early stopping. 
Line~\ref{alg:AdaBoostSampleupdatel} renames $ \gamma_{i},\epsilon_{i},h_{i},\alpha_{i},D_{i}$. 
Line~\ref{alg:AdaBoostSampleaddsf} adds $\alpha_{f,l} h_{f,l}$ to $f$ / the array of size $t$, where we recall that $ l= \sum_{j=1}^{i}\ind_{\alpha_j>0} $. 
Line~\ref{alg:AdaBoostSamplezeroweight} ignores hypotheses with a loss larger than $1/2-\gamma$ by setting $ \alpha_{i}=0 $, and the counter for successful boosting steps is not increased in this case. 
Line~\ref{alg:AdaBoostSamplegetweights} updates $ D_i $, increasing the weight of misclassified points, where $ S_{i,1}\in \cX $ is the feature of $ i $'th point and  $S_{i,2}\in \cY $ is the label of the $ i $'th point. 
Line~\ref{AdaBoostSamplesetnormalization} calculates the normalization.
Line~\ref{alg:AdaBoostSamplegetsZ} rename the normalization term for successful boosting rounds. 
Line~\ref{alg:AdaBoostSamplenormalizing} normalizes $D_{i+1}$ to a probability distribution. 
Line~\ref{alg:AdaBoostSamplecummlativeupdate} to \ref{alg:AdaBoostsampleendforloop} calculates $C_{i+1}$. 
Line~\ref{alg:AdaBoostsample:output1} normalizes $f$. 
Line~\ref{alg:AdaBoostSamplecheckingmargin} checks for sufficient margins. 
Line~\ref{alg:AdaBoostSampleoutputgood} outputs $ f $ with $ \theta=3/4 $ margins for all $ x \in S$. 
Line~\ref{alg:AdaBoostsample:output2} boosting failed and $ \erm(S) $ is returned.

As commented on in the footnote of \cref{samplingfromrows} all other than the Line~\ref{alg:AdaBoostSampletraining} calling the $ \erm $, and Line~\ref{alg:AdaBoostSamplegetweights} making inference do only require working over the indexes $ m $  of the given training sequence, so these steps do not require reading the training examples explicitly. Reading the training examples in the call to the $ \erm $ and doing inference is captured in $ \Utrain $ and $ \Uinf.$

