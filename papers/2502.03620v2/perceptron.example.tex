\paragraph{Perceptron:}
In the \cref{appendix:perceptron}, we show that for $m \in \mathbb{N}$ and $m \geq 2200$, there exists a realizable distribution $\cD$ where \cite{baggingoptimalPAClearner}, run with the perceptron algorithm as a black box $\erm$, has a training complexity of $\Omega(m^2)$ with probability at least $1/2$. Similarly, \cref{introductionmaintheorem}, except with the small constant probability set in this section, has a training complexity of $\Omega(\ln{\left(m/d\right)}\ln{\left(m\right)}m)$. This example also shows that $U_{T}$ might depend on $m$, as mentioned above, which can happen in the distribution-free setting.

The intuition behind the example is to consider a universe consisting of the points $x_{i} = (0, 1 - \frac{i}{m^3}), y_{i} = -1$ for $i = 1, \ldots, m-1$, and $x_{m} = (\sqrt{\frac{1}{m}}, 1), y_{m} = 1$, which has a margin of $\gamma = \Theta(\sqrt{\frac{1}{m}})$. If the perceptron is run on the whole universe, it makes $2$ mistakes when passing over the data, i.e., when the label switches sign. One can then show that for the perceptron to converge, one has to pass over the universe $\Omega(m)$ times, where each pass takes $\Omega(m)$ operations, leading to a total complexity of $\Omega(m^2)$.

Using a distribution $\cD$ that assigns a small mass to $x_{m}$ and uniformly to the remainder of the universe ensures this pattern also holds with probability at least $1/2$ for a random bootstrap sample, leading to $U_{T}(0.02m) \geq \Omega(m^{2})$.

For the training complexity of \cref{introductionmaintheorem}, we consider a sub-training sequence of size $1650$, since the VC-dimension of the sign of a hyperplane is at most $3$ in $\mathbf{R}^{2}$. For a sub-training sequence denoted $(x'_{1}, y'_{1}), \ldots, (x'_{1650}, y'_{1650})$ of $(x_{1}, y_{1}), \ldots, (x_{m}, y_{m})$, the claimed training complexity follows by noting that the margin is at least $\gamma = \Theta(\sqrt{\frac{1}{m}})$ and that the perceptron only makes $\frac{\max_{i=1, \ldots, 1650} ||x'_{i}||^{2}}{\gamma^{2}} = O(m)$ updates/mistakes when repeatedly passing over $(x'_{1}, y'_{1}), \ldots, (x'_{1650}, y'_{1650})$. Since there must be one mistake for each pass over $(x'_{1}, y'_{1}), \ldots, (x'_{1650}, y'_{1650})$ and one pass takes $O(1)$ operations, this leads to a training complexity of $O(\ln{\left(m/d\right)}\ln{\left(m\right)}m)$, except with the small constant probability considered in this section.

In \cref{appendix:perceptron}, we formalize the above argument for the perceptron and also take into consideration the hyperplane including a bias term.