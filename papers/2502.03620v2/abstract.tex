Recent advances in the binary classification setting by \cite{hannekeoptimal} and \cite{baggingoptimalPAClearner} have resulted in optimal PAC learners. These learners leverage, respectively, a clever deterministic subsampling scheme and the classic heuristic of bagging \cite{Breiman1996BaggingP}. Both optimal PAC learners use, as a subroutine, the natural algorithm of empirical risk minimization. 
Consequently, the computational cost of these optimal PAC learners is tied to that of the empirical risk minimizer algorithm.

In this work, we seek to provide an alternative perspective on the computational cost imposed by the link to the empirical risk minimizer algorithm.
To this end, we show the existence of an optimal PAC learner, which offers a different tradeoff in terms of the computational cost induced by the empirical risk minimizer.