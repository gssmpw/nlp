\section{Efficient Optimal PAC Learner}\label{sec:optimalityah} 
To introduce our algorithm, we need the following subsampling algorithm, which is inspired by that of \cite{hannekeoptimal} \cref{alg:Subsamplehanneke}. We make $6$ sub-sequences and $ 5 $  recursive calls whereas \cite{hannekeoptimal} make $4$ sub-sequence and $ 3 $ recursive calls. Furthermore, we here use nonoverlapping subsequence except on the part that is recursed on. The reason for the $6$ sub-sequences, as mentioned earlier, is to ensure that $3/4$ of the majority voters have $3/4$ of their voters correct. The nonoverlapping subsequence, except on the part that is recursed on, was chosen to simplify notation. We will refer to \cref{alg:Subsample} as $\cS$.
\input{subsamplingalgorithm.tex}

In the following, we will for $S\in (\cX\times \cY)^{m}$ and $m=6^{k}$ for $k\in\mathbb{N}$ and $T\in (\cX\times \cY)^{*}$ let $\cS(S,T)$  be the matrix with rows corresponding to the sub training sequences that $\cS(S,T)$ produces. The matrix is given recursively by the following equation:  
\begin{align*}
    \cS(S,T)=
    \begin{bmatrix}
      \cS(S_0,S_{1}\sqcup T)\\
      \cS(S_0,S_{2}\sqcup T)\\
      \cS(S_{0},S_{3}\sqcup T)\\
      \cS(S_{0},S_{4}\sqcup T)\\
      \cS(S_{0},S_{5}\sqcup T)
    \end{bmatrix}.
\end{align*}
We will for short write $\cS(i,T)$ instead of $\cS(S_{0},S_{i}\sqcup T)$. We notice that since each of the recursive calls creates $5$ sub calls, and we assume that $m=6^{k}$, there will be $5^{k}=5^{\log_{6}(m)}=m^{\log_{6}(5)}\approx m^{0.898}$ many sub training sequences/rows in the above matrix. Furthermore, each sub training sequence has size $ m'=\sum_{i=1}^{k} m(1/6)^{i}+1$. 


In the following we will use $\cA$ to refer to \cref{alg:AdaBoostSample} $\as$, which takes as input a training sequence $ S\in(\cX\times \cY)^{*}$ and a string $ r\in ([0:1]^{*})^{*}$. We also assume that $ \cA $ has query access to a $ \erm $ algorithm.  
Further for $\cA$ run on the family of sub training sequences $\cS(i,T)$ and string $r\in ([0:1]^{*})^{*}$ we will write  $\cA(i,T,r)$ for the sequence of hypotheses $[\cA(S',r)]_{S'\in \cS(i,T)}$. We also view $\cA(\cS(S,T),r)$ as the following recursively defined matrix:

\begin{align*}
    \cA(\cS(S,T),r)=
    \begin{bmatrix}
      \cA(\cS(S_0,S_{1}\sqcup T),r)\\
      \cA(\cS(S_0,S_{2}\sqcup T),r)\\
      \cA(\cS(S_{0},S_{3}\sqcup T),r)\\
      \cA(\cS(S_{0},S_{4}\sqcup T),r)\\
      \cA(\cS(S_{0},S_{5}\sqcup T),r)
    \end{bmatrix}.
\end{align*}


Our learner's behavior is closely related to the behavior of the hypotheses in the above matrix. Thus, we will now state some lemmas that describe the behavior of the hypotheses in the above matrix. 

The first lemma shows that the hypothesis returned by $\cA$ is always a majority vote, and it achieves a margin of at least $ 3/4$  on all examples in the training sequence $S$ it is trained on. The proof of the lemma can be found in \cref{sec:propAdaBoostSample}.

\begin{restatable}{lemma}{lemmaadaboostsampleeasyproperties}\label{lemma:adaboostsampleeasyproperties}
  For a hypothesis class $\cH$ of VC-dimension $d$, target concept $ c\in \cH $, training sequence size $m\in\mathbb{N}$, training sequence $S\in \left(\cX\times\cY\right)^{m}$ 
  realizable by $c$,  and string $r\in([0:1]^{*})^{*}$ the output $f=\cA(S,r)$ of \cref{alg:AdaBoostSample}, when run on 
  $S$ and $r$ is in $f\in\dlh$ for $t=\left\lceil20^{2}\ln{(m)}/2\right\rceil$ and satisfies: 
  $\sum_{x\in S} \ind\{\sum_{i=1}^{t} h_{f,i}(x)c(x)/t\leq \theta \}/m <1/m$ and $\theta=3/4$.
\end{restatable}
The next lemma shows that $\cA$, when run on a training sequence $S$ and a random string $\rr$, where $ \rr $  is used to sample from the sequential distributions $ \cA $ makes, with high probability, performs few operations. The proof of the lemma can be found in \cref{sec:propAdaBoostSample}

\begin{restatable}{lemma}{lemmaAdaBoostSampleefficent}\label{lemmaAdaBoostSampleefficent}
  For a hypothesis class $\cH$ of VC-dimension $d$, target concept $c\in \cH  $, training sequence size $ m\in\mathbb{N} $, training sequence   $S\in(\cX\times \cY)^{m}$ realizable by $c$, failure parameter $0<\delta <1$, random string length $n\in\mathbb{N}$, and random string $\rr\sim ([0:1]^{550d})^{n}$ we have for $t=\left\lceil20^{2}\ln{(m)}/2\right\rceil$ as in Line~\ref{alg:AdaBoostSampletset} and $n\geq 6\lceil20^{2}\ln{(8m/\delta)}/2\rceil$ that with probability at least $1-(\delta/(8m))^{20}$ over $ \rr $  \cref{alg:AdaBoostSample} run on $S,\rr$, $\cA(S,\rr)$, uses no more than
$n\cdot (O(m+d\ln{(m )})+\Utrain(550d)+3m\Uinf)$ operations.
\end{restatable}

Thus, from the above lemma, it follows that we can apply a union bound over all the hypotheses/rows in $\cA(\cS(S,\emptyset),\rr)$, which are at most $\leq m^{0.9}$, and conclude that, with probability at most $1-(\delta/(8m))^{19}$, any row of $\cA(\cS(S,\emptyset),\rr)$ uses few operations. 

The next lemma is inspired by the main theorem in \cite{hannekeoptimal} which proof is sketched in \cref{sec:proofoverview}. Specifically, it says that the majority vote of the hypotheses in $\cA(\cS(\rS,\emptyset),r)$ for a random sample $\rS$ and any string $r$, with probability $1-\delta$ over $\rS$, is such that fewer than $3/4$ of the majority voters in the majority vote of majorities has $3/4$ of its voters being correct on a new example $ (\rx,c(\rx)) $ has probability $O(d+\ln{\left(1/\delta \right)}  )/m$ over $ (\rx,c(\rx)).$

\begin{lemma}[\cref{induktionsteplemma} with $T=\emptyset$]\label{induktionsteplemmaeasy}
  There exists a universal constant $\cs$ such that for: Distribution $ \cD $ over $ \cX $, hypothesis class $ \cH $ of VC-dimension $ d $, target concept $ c\in \cH $,  failure probability $ 0<\delta<1 $, training sequence size $m=6^{k}$ for some $k\geq 1$, and string $r\in ([0,1]^{*})^{*}$, it holds with probability at least $1-\delta$ over $\rS\sim \cD_{c}^{m}$ that
     \begin{align*}
       \p_{\rx\sim \cD} \left[  
         \sum_{\rf\in \cA(\cS(\rS,\emptyset),r)} \frac{\ind\{\sum_{i=1}^{t} \ind\{h_{\rf,i}(\rx)=c(\rx)\}/t\geq 3/4\}}{|\cS(\rS,\emptyset)|}
          <3/4\right] \leq \cs \frac{d+\ln{\left(1/\delta \right)}}{m}.
     \end{align*}
\end{lemma}

The last lemma we introduce, states that we can efficiently sample with replacement from the rows of $\cA(\cS(S,\emptyset),r)$. The proof of the lemma can be found in \cref{appendixefficentoptimalpaclearner}
\begin{restatable}{lemma}{samplingfromrows}\label{samplingfromrows}
  Let $S\in (\cX\times \cY)^{*}$, with $|S|=m=6^{k}$ for $k\geq 1$.  Let $g'$ be the function from  $\{1:5\}^{k}$ into $\left([1:m]^{2}\right)^{k}$ defined by $g'(w)_{j}=[ 6^{k}w_j/6^{j}+1,6^{k}(w_j+1)/6^{j}]$ for $j\in \{1:k\}$. For $w\in\left\{ 1:5 \right\}^{k}$, we denote $ S[g'(w)] $ as a sub training sequence of $ S $, which can be found using $S$, $ w $, and $g'$ in $O(m)$ operations, and when viewed as a function of $ w\in\{ 1:5 \}^{k} $, $ S[g'(w)] $ is a bijection into $ \cS(\cS,\emptyset).$ \footnote{For readability, we here write it as we find the training sequence $ S[g'(w)],$ which would might imply reading the whole training sequence $ S,$ but what we actually find is the indexes of $ S[g'(w)],$ which only requires looking at the numbers $ [m].$ We will only read the training examples when training an $ \erm $ captured in $ \Utrain $ or doing inference captured in $ \Uinf. $     }
\end{restatable}
With the above lemmas in place, we now present the idea of our efficient PAC learner which is simply sampling majority voters/rows with replacement of the matrix $\cA(\cS(\rS,\rr))$ and for each sampled majority voter, sample a voter from the majority voter. By \cref{induktionsteplemmaeasy} one can deduce that one such sampled voter, with probability $(3/4)^{2}=9/16=1/2+1/16$, will correctly classify a new example. Since this probability is strictly greater than $1/2$, we can by repeating the sampling procedure $\Theta(\ln{\left(m/(\delta(d+\ln{\left(1/\delta \right)}) )\right)})$ times, ensure that the majority of the sampled hypotheses, with probability at least $1-\Theta(\delta)$ over the sampled hypotheses, will have error less than $O(m/\left(d+\ln{\left(1/\delta \right)}\right))$ (using Chernoff and Markovs inequality). In the above sampling a majority voter/row from $\cA(\cS(\rS,\rr))$ refers to sampling a sub training sequence/row from $\cS(\rS,\emptyset)$, which can be done efficiently by \cref{samplingfromrows}, and then run $\cA(\cdot,\rr)$ on the sampled sub training sequence, followed by sampling a random voter from the output of $ \cA $. That $ \cA $ can be run efficiently on the sampled sub training sequences from $ \cS(\rS,\emptyset) $ follows by the union bound argument over runs $ \cA(\cS(\rS,\emptyset)) $ described under \cref{lemmaAdaBoostSampleefficent}. We will make the above argument formal soon. With the intuitive explanation of our learner presented, we now state the algorithm explicitly. We will use $ \ah $ to denote the learner.
\input{Randommajorityvoter.tex}

For the analysis of \cref{alg:Randommajority}, we use $\rw=(rw_{1},\ldots,rw_{l})$ and $\rz=(\rz_{1},\ldots,\rz_{Ã¦})$ to denote the collections of independent random variables $\rw_i$ and $\rz_i$ used during the $l$ rounds. Furthermore, we will use $\ah_{\delta}(S,r,\rw,\rz)$ to denote the output of \cref{alg:Randommajority}, suppressing the parameter $ d $ in the notation. With our algorithm introduced, we now give the formal statement of our main theorem and the proof of it. 
\begin{restatable}{theorem}{maintheorem}\label{maintheorem}
  There exists a universal constant $\cs$ such that: for failure parameter $0<\delta<1$, hypothesis class $\cH$ of VC-dimension $d$, distribution $\cD$,  target concept $c\in \cH$, training sequence size $ m=6^{k}$ for $k\in\mathbb{N},$ random string size $n=6\lceil20^{2}\ln{(8m/\delta)} /2\rceil$, random string $\rr\sim ([0:1]^{550d})^{n}$, and training sequence $\rS\sim\cD_{c}^{m}$, the learner $ \ah_{\delta}(\rS,\rr,\rw,\rz) $ will, with probability at least $ 1-\delta $ over $ \rS,\rr,\rw $, and $ \rz $, output $ \sign(\cdot) $ of a majority vote  consisting of $l=\lceil 16\cdot 200 \ln{(m/(\delta(d+\ln{\left(1/\delta \right)})) )}/9 \rceil$ hypotheses from $\cH$, with error $  \ls_{\cD_{c}}(\ah_{\delta}(\rS,\rr,\rw,\rz))\leq (4+\cs) (d+\ln{\left(4/\delta \right)})/m $, inference complexity $ O(\ln{[m /\delta(d+\ln{\left(1/\delta \right)}) ]}) \Uinf $ and computational complexity\footnote{The proof actually shows that with probability $ 1-\delta/2 $ the error is as stated and with probability at least $ 1-(6\delta/(8m))^{19} $ the training and inference complexity is as stated.  }
  \begin{align*}
    O\bigg(\negmedspace\ln{\bigg(\frac{m}{\delta(d+\ln{\left(1/\delta \right)})} \bigg)}\negmedspace\cdot\negmedspace\ln{\bigg(\frac{m}{\delta} \bigg)}\negmedspace\bigg)\negmedspace\cdot \negmedspace\Big(O\Big(m+d\ln{\left(m \right)}\Big)\negmedspace+\negmedspace\Utrain(550d)\negmedspace+\negmedspace 3m\Uinf\negmedspace\Big).
    \end{align*} 
\end{restatable}

\begin{proof}[Proof of \cref{maintheorem}.]
Let $\cs$ be the universal constant from \cref{induktionsteplemmaeasy}. We prove the above by showing that respectively, with probability at least $1-\delta/2$ over $\rS,\rr,\rw$ and $\rz$, the error is upper bound as stated above, and the inference and training complexity is upper bounds as stated above,  whereby a union bound completes the argument. We start with the error bound.

To this end consider any realization $r$ of $\rr$ and $S$ of $\rS\sim \cD_{c}^{m}$. We now define the event  $E_{S,r}=\{x:     
  \sum_{\tilde{f}\in \cA(\cS(S,\emptyset),r)} \ind\{\sum_{i=1}^{t} \ind\{h_{\tilde{f},i}(x)=c(x)\}/t\geq 3/4\}/|\cS(S,\emptyset)|
   <3/4 \}$  over $\cX$, i.e., the event where fewer than $3/4$ of the majority votes in $\cA(\cS(S,\emptyset),r)$ are correct on at least  $3/4$ of the votes. If $\p_{\rx\sim \cD}[\overline{E_{S,r}}] \leq (d+\ln{\left(1/\delta \right)})/m$, we get by using $\p(A)=\p(A\cap B)+\p(A\cap \bar{B})\leq\p(B)+\p(\bar{B})$ that
  \begin{align}\label{eq:maincorrectness1}
      \ls_{\cD_{c}}(\ah_{\delta}(S,r,\rw,\rz)) =\p_{\rx\sim \cD}\left[\ah_{\delta}(S,r,\rw,\rz)(\rx)\not=c(\rx)\right]
      \leq \p\left[E_{S,r}\right]+(d+\ln{(1/\delta)})/m
  \end{align}
Assume now that $ \p_{\rx\sim \cD}\left[\overline{E_{S,r}}\right] \geq (d+\ln{\left(1/\delta \right)})/m >0 $. By the law of total probability, we split the loss of $\ah$ into two parts:
   \begin{align}\label{eq:maincorrectness2}
    \ls_{\cD_{c}}(\ah_{\delta}(S,r,\rw,\rz)) 
    \leq \p_{\rx\sim \cD}\left[E_{S,r}\right]+\p_{\rx\sim \cD}\left[\ah_{\delta}(S,r,\rw,\rz)(\rx)\not=c(\rx)|\overline{E_{S,r}}\right]
   \end{align}
where $ \overline{E_{S,r}} $ denotes the complement of $E_{S,r}$, i.e., $\overline{E_{S,r}}=\{x:     
  \sum_{\tilde{f}\in \cA(\cS(S,\emptyset),r)} \ind\{\sum_{i=1}^{t} \ind\{h_{\tilde{f},i}(x)=c(x)\}/t\geq 3/4\}/|\cS(S,\emptyset)|
   \geq3/4 \}$ - the event where $3/4$ of the majority voters in $\cA(\cS(S,\emptyset))$ have $3/4$ of their votes correct. 

Now consider any $x\in \overline{E_{S,r}}$. We now notice that by Line~\ref{alg:RandommajoritysamplingS}, Line~\ref{alg:Randommajoritysamplingefficient} and \cref{samplingfromrows}, we have $S_i=S[g'(\rw_{i})]\in\cS(S,\emptyset)$. Furthermore, since $g'(\rw_{i})$ is a bijection into the training sequences/rows of $\cS(S,\emptyset)$ by \cref{samplingfromrows} and $ \rw_{i} \sim \{ 1:5 \}^{k}$, $ S_{i} $ can be seen as sampled uniformly at random from the training sequences of $ \cS(S,\emptyset) $. Hence, Line~\ref{alg:Randommajorityrunsadaboostsample} can be viewed as uniformly drawing a majority vote/row from $\cA(\cS(S,\emptyset),r)$. Furthermore since any $ S'\in \cS(S,\emptyset,r) $ has size $ |S'|=\sum_{i=1}^{k}m(1/6)^{i}+1$, which is $ m' $ in Line~\ref{alg:Randommajoritysetsubtraingsequencesamplesize}, and by \cref{lemma:adaboostsampleeasyproperties}, we have that  $ \cA(S_{i},r) $ is a majority voter over $ \lceil20^{2}\ln{(|S_{i}|)}/2 \rceil=\lceil20^{2}\ln{(m')}/2 \rceil $ voters, which is $ t $ in Line~\ref{alg:Randommajoritysett}.  
Thus, since we have by Line~\ref{alg:Randommajoritysamplingindex1} that $\rz_{i}$ is uniformly chosen between $\{ 1:t \}$, we conclude that $h_{f_{\rw_{i}},\rz_{i}}$ in Line~\ref{alg:Randommajorityaddshypothesis} can be seen as uniformly chosen from the voters in $ f_{\rw_{i}} =\sum_{l=1}^{t}h_{f_{\rw_{i}},l}/t$. Therefore, since $ f_{\rw_{i}} $ was uniformly chosen among all $ \cA(\cS(S,\emptyset),r) $ and $ h_{f_{\rw_{i}},\rz_{i}} $ was uniformly chosen among the voters of $ f_{\rw_{i}} $, we conclude that for $x\in \overline{E_{S,r}}$, i.e., such that $\sum_{\tilde{f}\in \cA(\cS(S,\emptyset),r)} \ind\{\sum_{i=1}^{t} \ind\{h_{\tilde{f},i}(x)=c(x)\}/t\geq 3/4\}/|\cS(S,\emptyset)|\geq3/4 $, we have:
   \begin{align*}
    &\p_{\rw_{i},\rz_{i}}\left[h_{f_{\rw_{i}},\rz_{i}}(x)=c(x)\right]\\
    \geq
    &\p_{\rw_{i},\rz_{i}}\Big[h_{f_{\rw_{i}},\rz_{i}}(x)=c(x)\Big| \sum_{j}^{t} \frac{\ind\{h_{f_{\rw_{i}},j}(x)=c(x)\}}{t}\geq \frac{3}{4}\Big] \p_{\rw_{i}}\Big[\frac{\ind\{h_{f_{\rw_{i}},j}(x)=c(x)\}}{t}\geq \frac{3}{4}\Big]\\
    \geq& 
    (3/4)^{2}=9/16>1/2.
   \end{align*}
Since $\ah_{\delta}(S,r,\rw,\rz)=\sign(\sum_{i=1}^{l}  h_{f_{\rw_{i}},\rz_{i}})$, we have that $\ah_{\delta}(S,r,\rw,\rz)(x)=c(x)$ if the number of $i$'s such that $h_{f_{\rw_{i}},\rz_{i}}(x)$ is equal to $c(x)$ is strictly greater than $l/2$. If we let $\rX= \sum_{i=1}^{l}\ind\{h_{f_{\rw_{i}},\rz_{i}}(x)=c(x)\}$, i.e., $\rX$ is the number of $ i $'s such that $h_{f_{\rw_{i}},\rz_{i}}(x)=c(x)$, we have that $\rX$ has an expectation of at least  $\e\left[\rX\right]\geq 9l/16$. Since $ \rX $ is a sum of i.i.d. $\left\{ 0,1 \right\}$-random variables, by Chernoff and $l=\lceil 16\cdot200 \ln{(m/(\delta(d+\ln{(1/\delta )})) )}/9 \rceil$ we have:
\begin{align*}
 \p\left[\rX\leq (1-1/10)9l/16\right]
\leq  \exp{\left(-9l/(16\cdot 200) \right)} \leq \delta (d+\ln{\left(1/\delta \right)})/m.
\end{align*} 
Since $(1-1/10)9l/16=81l/160>l/2$, we conclude that for $x\in \overline{E_{S,r}}$, with probability at least $1- \delta (d+\ln{\left(1/\delta \right)})/m$ over $ \rw,\rz $, we have that $\ah_{\delta}(S,r,\rw,\rz)(x)=c(x)$. Thus, by independence of $\rw,\rz,\rx$ we conclude that: 
\begin{align*}
 \e_{\rw,\rz}[\p_{\rx\sim\cD}[\ah_{\delta}(S,r,\rw,\rz)(\rx)\not=c(\rx)\mid \overline{E_{S,r}}]]
\leq \delta (d+\ln{\left(1/\delta \right)} )/m,
\end{align*}
and further by Markov's inequality that:
\begin{align}\label{eq:maincorrectness3}
  \p_{\rw,\rz}\left[\p_{\rx\sim\cD}\left[\ah_{\delta}(S,r,\rw,\rz)(\rx)\not=c(\rx)\mid \overline{E_{S,r}}\right]\geq 4(d+\ln{\left(1/\delta \right)})/m\right]\leq \delta /4.
\end{align}
Thus, by combining \cref{eq:maincorrectness1}, \cref{eq:maincorrectness2} and \cref{eq:maincorrectness3}, we conclude that for any realization $S$ and $r$ of $\rS$ and $\rr$, we have with probability at least $1-\delta/4$ over $\rw$ and $\rz$ that
$\ls_{\cD_{c}}\left(\ah_{\delta}(S,r,\rw,\rz)\right)\leq \p_{\rx\sim D}\left[E_{S,r}\right]+4(d+\ln{(1/\delta)})/m.$
Whereby independence of $\rS,\rr,\rw$ and $\rz$ implies:
\begin{align}\label{eq:unionboundinrandom}
  \p_{\rS,\rr,\rw,\rz}\left[ \ls_{\cD_{c}}\left(\ah_{\delta}(\rS,\rr,\rw,\rz)\right) \leq \p_{\rx\sim D}\left[E_{\rS,\rr}\right]+4(d+\ln{\left(1/\delta \right)})/m\right]
  \geq 1-\delta/4.
\end{align}
We now show that with probability at least $1-\delta/4$ over $\rS,\rr,\rw$, and $\rz$, we have $\p_{\rx\sim\cD}[E_{\rS,\rr}]\leq \cs(d+\ln{\left(4/\delta \right)})/m$. Thus combining this with \cref{eq:unionboundinrandom} and applying a union bound it holds with probability at least $1-\delta/2$ over $\rS,\rr,\rw$ and $\rz$ that $
  \ls_{\cD_{c}}(\ah_{\delta}(S,r,\rw,\rz))\leq(\cs+4) (d+\ln{\left(4/\delta \right)})/m,$
which would conclude that the error bound of $ \ah_{\delta}(\rS,\rr,\rw,\rz) $  holds with probability at least $1-\delta/2$ as claimed. 

Now to this end, let $r,w$, and $z$ be realizations of $\rr,\rw$, and $\rz$. By \cref{induktionsteplemmaeasy} with failure parameter $\delta/4$, we have with probability at least $1-\delta/4$ over $\rS\sim \cD_{c}^{m}$ that 
\begin{align*}
 \p_{\rx\sim \cD}\left[E_{\rS,r}\right]= \p_{\rx\sim \cD}\Bigg[   
  \sum_{\tilde{f}\in \cA(\cS(\rS,\emptyset),r)} \negmedspace \negmedspace \negmedspace  \negmedspace \negmedspace \negmedspace \frac{\ind\{\sum_{i=1}^{t} \ind\{h_{\tilde{f},i}(\rx)=c(\rx)\}/t\geq 3/4\}}{|\cS(\rS,\emptyset)|}
   <3/4 \Bigg]\leq \cs \frac{d+\ln{\left(4/\delta \right)}}{m}.
\end{align*}
Since the above holds for any realization $r,w$, and $z$ of $ \rr,\rw $, and $ \rz $, and that $\rS,\rr,\rw$, and $ \rz $ are independent, we obtain $
 \p_{\rS,\rr,\rw,\rz}[ \p_{\rx\sim \cD}[E_{\rS,\rr}]\leq \cs (d+\ln{\left(4/\delta \right)})/m] \geq 1-\delta/4$,
which concludes the claim regarding the error of $ \ah_{\delta}(\rS,\rr,\rw,\rz) $.

We now show that with probability at least $1-\delta/2$ over $\rS,\rr,\rw$, and $\rz$, we have that $\ah_{\delta}(\rS,\rr,\rw,\rz)$ has inference complexity $ O(\ln{[m /\delta(d+\ln{\left(1/\delta \right)}) ]}) \Uinf $ and computational complexity
\begin{align*}
 O\Big(\negmedspace\ln{\Big(\frac{m}{\delta\left(d+\ln{\left(1/\delta \right)}\right)} \Big)}\negmedspace\cdot\negmedspace\ln{\Big(\frac{m}{\delta} \Big)}\negmedspace\Big)\negmedspace\cdot \negmedspace\Big(O\Big(m+d\ln{\left(m \right)}\Big)\negmedspace+\negmedspace\Utrain(550d)\negmedspace+\negmedspace 3m\Uinf\negmedspace\Big).
 \end{align*} 

To this end, consider any realization of $S$, $ w $, and $ z $, of $\rS,\rw$, and $\rz$. For any $\tilde{f} \in \cA(\cS(S,\emptyset),\rr)$, we have a sub training sequence $S'\in \cS(S,\emptyset)$ such that $\tilde{f}=\cA(S',\rr)$, and $m'=\sum_{i=1}^{k} m(1/6)^{i} + 1 =|S'|\leq |S|=m$. Furthermore, since $n=|\rr|=6\left\lceil20^{2}\ln{\left(8m/\delta \right)}/2\right\rceil$, which is greater than $\lceil20^{2}\ln{(|S'|)}/2\rceil$, we have by \cref{lemmaAdaBoostSampleefficent}, that with probability at least $1-(\delta/(8|S'|))^{20}\geq 1-(6\delta/(8m))^{20}$ (since $S'\geq m/6$) over $\rr$, the number of operations needed to compute $\tilde{f}=\cA(S,\rr)$ is at most 

\begin{align*}
  n\negmedspace\cdot \negmedspace\Big(\negmedspace O\Big(\negmedspace |S'|\negmedspace +\negmedspace d\ln{\left(|S'| \right)}\negmedspace\Big)\negmedspace +\negmedspace\Utrain(550d)\negmedspace+\negmedspace |S'|\Uinf\negmedspace\Big)\negmedspace =\negmedspace O\Big(\negmedspace\ln{\Big(\frac{m}{\delta} \Big)}\negmedspace\Big)\negmedspace\cdot \negmedspace\Big(\negmedspace O\Big(m\negmedspace+\negmedspace d\ln{\left(m \right)}\negmedspace\Big)\negmedspace+\negmedspace\Utrain(550d)\negmedspace+\negmedspace 3m\Uinf\negmedspace\Big)\negmedspace.
\end{align*}
Since there are at most $5^{\log_{6}(m)}=m^{\log_{6}(5)}< m$ rows of $\cA(\cS(S,\emptyset),\rr)$, a union bound implies, that with probability at least $1-\left(6\delta/(8m)\right)^{19}$ over $\rr$,  any $h\in\cA((\cS(S,\emptyset),\rr))$ takes at most 

\begin{align*}
  O\Big(\ln{\Big(\frac{m}{\delta} \Big)}\Big)\cdot \Big(O\Big(m+d\ln{\left(m \right)}\Big)+\Utrain(550d)+3m\Uinf\Big),
  \end{align*}
operations to compute.
Thus, for any realization $r$ of $\rr$ such that the above holds, we notice by the for loop in Line~\ref{alg:Randommajorityforloopstart} consisting of $\lceil 16\cdot200 \ln{(m/(\delta(d+\ln{(1/\delta )})))} /9\rceil$ rounds, and in each round, the algorithm in Line~\ref{alg:RandommajoritysamplingS} finds $S_i\in \cS(S,\emptyset)$ and  
in Line~\ref{alg:Randommajorityrunsadaboostsample} runs $\cA(S_{i},r)$, which for this realization takes at most the above number of 
operations, since $\cA(S_{i},r)\in \cA(\cS(S,\emptyset),r)$, we get that Line~\ref{alg:Randommajorityrunsadaboostsample} over the for loop takes at most 
\begin{align}\label{eq:finalruntime}
  O\Big(\negmedspace\ln{\Big(\frac{m}{\delta\small(d+\ln{\left(1/\delta \right)}\small)} \Big)}\negmedspace\cdot\negmedspace  \ln{\Big(\frac{m}{\delta} \Big)} \negmedspace\Big)\negmedspace\cdot \negmedspace\Big(O\Big(m+d\ln{\left(m \right)}\Big)\negmedspace+\negmedspace\Utrain(550d)\negmedspace+\negmedspace 3m\Uinf\negmedspace\Big)
\end{align}
operations for such $r$, and since the probability of seeing such a realization $r$ of $ \rr $ is at least $1-\left(6\delta/(8m)\right)^{19}\geq 1-\delta/2$, we conclude that Line~\ref{alg:Randommajorityrunsadaboostsample} over the for loop takes at most the above many operations. This is the only part where $\rr$ affects the number of operations $\ah$ uses in total.
 
Now, setting parameters, Line~\ref{alg:Randommajoritysetm} to Line~\ref{alg:Randommajoritysett}, takes: $O(m)$ operations to read the length of $S$.  
Calculating $ k $ can be done in $ O(1) $ operations since we know $ m $ and $ k=\log_{6}(m) $. We assume that $\delta$ and $d$ are given as parameters so takes $ 2 $ operations to read,  and $m$ we have already calculated so calculating the number $m/(\delta(d+\ln{\left(1/\delta \right)}))$ takes at most $5$ additions/multiplications/divisions/operations, and taking $\ln{\left(\cdot \right)}$ of the number cost one operation thus calculating $ l  $ takes $ O(1) $ operations. Initializing $f$, an array of size $l$, takes $O(\ln{\left(m/(\delta(d+\ln{\left(1/\delta \right)})) \right)})$ operations. Calculating $ m' $ can be done by dividing $ m $ by $ 6 $ $ k $ times and adding the numbers as they are calculated and then adding $ 1  $ in the end, thus calculating $ m' $ takes at most $ O(k)=O(\ln{(m )}) $ operations. Since we know $ m' $ calculating $ t $ takes $ O(1) $. Thus, Line~\ref{alg:Randommajoritysetm} to Line~\ref{alg:Randommajoritysett} takes at most $O\left(m+\ln{\left(m/(\delta(d+\ln{\left(1/\delta \right)})) \right)}  \right)$ many operations, which is less than the number of operations used in Line~\ref{alg:Randommajorityrunsadaboostsample} over the for loop, as stated in \cref{eq:finalruntime}. 

Now, for the for loop in Line~\ref{alg:Randommajorityforloopstart} to Line~\ref{alg:Randommajorityoutputofhat}, we perform the following steps: Read $\rw_i$ which takes $\log_{6}(m)$ operations. Finding $S_i$ using $g',\rS$, and $\rw_i$ takes at most $O(m)$ operations by \cref{samplingfromrows}. The runtime in Line~\ref{alg:Randommajorityrunsadaboostsample} has been argued for in the above. Reading $\rz_i$ takes $t=O(\ln{(m)})$ operations. Extracting $h_{f_{\rw_{i}},\rz_{i}}$ out of $f_{\rw_{i}}$ is reading an entry in $f_{\rw_{i}}$ so takes $O(1)$ and reading $f_{\rw_{i}}$ takes at most $O(t)=O(\ln{\left(m\right)})$ operations. Adding/reading $h_{f_{\rw_{i}},\rz_{i}}$ into the $i$-th entry of $f$ takes $O(1)$ operations. Thus, all the operations in each iteration of the for loop, except Line~\ref{alg:Randommajorityrunsadaboostsample}, take at most $O(m)$ operations. Therefore, the total number of operations over the $l$ rounds is at most $O(lm)=O(\ln{\left(m/(\delta(d+\ln{\left(1/\delta \right)})m) \right)}\cdot m)$
operations, excluding the operations in Line~\ref{alg:Randommajorityrunsadaboostsample}, which are fewer than those made in Line~\ref{alg:Randommajorityrunsadaboostsample}, as argued above. 

Thus, the overall number of operations of $\ah$ can be bounded by the operations made in Line~\ref{alg:Randommajorityrunsadaboostsample} which was at most \cref{eq:finalruntime} with probability at least $1-\delta/2$ over $\rr$ for any realization $S,w$ and $ z $  of $\rS$, $ \rw $ and $ \rz $. Thus, by independence of $\rr,\rS,\rw$ and $\rz$ we conclude that the operations needed to calculate $ \ah_{\delta}(\rS,\rr,\rw,\rz) $ with probability at least $ 1-\delta/2$ over $ \rS,\rr,\rw,\rz $  is at most

\begin{align*}
  O\Big(\negmedspace\ln{\Big(\frac{m}{\delta\small(d+\ln{\left(1/\delta \right)}\small)} \Big)}\negmedspace\cdot\negmedspace  \ln{\Big(\frac{m}{\delta} \Big)} \negmedspace\Big)\negmedspace\cdot \negmedspace\Big(O\Big(m+d\ln{\left(m \right)}\Big)\negmedspace+\negmedspace\Utrain(550d)\negmedspace+\negmedspace 3m\Uinf\negmedspace\Big),
\end{align*}
which is the stated training complexity of $\ah_{\delta}(\rS,\rr,\rw,\rz)$ in \cref{maintheorem}. Furthermore, the inference complexity is $ l\cdot\Uinf $, which is $ O(\ln{(m/(\delta(d+\ln{(1/\delta )})) )})\Uinf $, as for a new example, all $ l $ voters in $ \ah_{\delta}(\rS,\rr,\rw,\rz) $ has to be queried to get $ \ah_{\delta}(\rS,\rr,\rw,\rz)(x) $, where each query takes $ \Uinf $ operations.     
\end{proof}

