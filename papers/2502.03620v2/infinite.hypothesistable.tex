
\paragraph{Search over a hypothesis space with finite VC-dimension:}             
Consider a hypothesis class $\cH$ of VC-dimension $d$. Furthermore, for a training sequence $S = ((x_{1}, y_{1}), \ldots, (x_{n}, y_{n}))$, we let $\cH|S = \{(h(x_{1}), \ldots, h(x_{n})) \mid h \in \cH\}$ denote the possible projections of $\cH$ on $S$. We assume that the black-box $\erm$ algorithm checks one projection of $\cH|S$ at a time and, if the projection is realizable, outputs any hypothesis that realizes this projection. 
We assume that performing inference on one point $x_{i}$ for a projection has a cost of $U_{I} = O(1)$. Thus, the training complexity becomes $U_{T}(n) = O(n (n/d)^{O(d)})$ in the worst case, as the $\erm$ might find a projection that realizes $S$ among the last projections in $\cH|S$, which can have size $(n/d)^{O(d)}$. 
In this case, the training complexity of \cref{introductionmaintheorem} becomes $O(\ln{\left(\frac{m}{d}\right)} \ln{\left(m\right)} (d 500^{d} + m))$, and the training complexity of \cite{baggingoptimalPAClearner} becomes $O(\ln{\left(m\right)} m (m/d)^{d})$, where \cite{baggingoptimalPAClearner} is better for $m \leq \Theta(550d \ln^{1/d}{\left(m\right)})$.