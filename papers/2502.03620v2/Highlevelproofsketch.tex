
\section{High-level proof sketch}
Before presenting the related work in the following section, we provide a high-level description of our algorithm and analysis. The intent of this high-level description is firstly to provide the reader intuition about our proof, and secondly to help establish connections with related work presented in the following section. In the detailed proof sketch, which follows the subsection of related work, we specifically point to how relevant steps in our analysis relate to previous work. For a reader mainly interested in understanding the proof of \cref{maintheorem} one can skip \cref{sec:proofoverview} and jump to \cref{sec:optimalitya} which introduces the necessary lemmas to prove \cref{maintheorem}, and show how they imply \cref{maintheorem}. We now give a high-level description of the algorithm giving the guarantee of \cref{introductionmaintheorem}.

\begin{algorithm}
\caption{High level description of Efficient optimal PAC learner.}\label{alg:algorithmhighlevel}
\begin{algorithmic}[1]
\State Sample $l=\Theta\big(\ln\big(\tfrac{m}{\delta\left(d+\ln{\left(1/\delta \right)}\right)}\big)\big)$ structured subtraining sequences $\rS_1, \ldots, \rS_{l}$ of $ \rS $, with  $ |\rS_{i}| =\Theta(m).$ \label{alg:highlevel1}
\State Generate $ l $  majority voters by running a boosting algorithm $\cB$ on each $\rS_{1},\ldots,\rS_{l}$. \quad \quad \quad \quad High level description of $ \cB(\rS_{i}) $:\label{alg:highlevel2}
\begin{algorithmic}[1]
    \State$ \cB $ uses $\erm$ as a weak learner, and makes at most $\Theta(\ln{\left(\tfrac{m}{\delta} \right)} )$ training invocation of $ \erm $ to get hypothesis $ h_{i,1},\ldots,h_{i,t},$ with $ t=\Theta(\ln{\left( m\right)}) $ .
    \State On each of the training evocation the $ \erm $ algorithm, is provided $550d$ training examples and returns a hypothesis $ h $. After each training evocation of the $ \erm $ algorithm, $ \cB $ evaluates $ h$ on $ \rS_{i} $. Based upon the evaluation $ \cB $  updates a distribution on $ \rS_{i}$, or discards the hypothesis $ h.$     
    \State The output of $ \cB(\rS_{i}) $  is a voting classifier $\cB(\rS_{i}) = \sum_{j=1}^{t} h_{i,j}/t$
\end{algorithmic}
\State \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace For each $ i=1,\ldots,l $  sample a voter $ \rhh_i $  from the voters $ \{ h_{i,j} \}_{j=1}^{t} $ in the voting classifier $ \cB(\rS_{i})=\sum_{j=1}^{t} h_{i,j}/t $. Output $f=\sign(\sum_{i=1}^{l} \rhh_i) $ as the final predictor.\label{alg:highlevel3}
\end{algorithmic}
\end{algorithm}

We first give the rough analysis for the training and inference complexity of \cref{introductionmaintheorem} based on \cref{alg:algorithmhighlevel}. From Line~\ref{alg:highlevel2} we see that the boosting algorithm is invoked $ \Theta\big(\ln\big(\tfrac{m}{\delta\left(d+\ln{\left(1/\delta \right)}\right)}\big)\big)$ times and that the boosting algorithm on each of these evocations, makes at most $ \Theta\left(\ln{\left(\tfrac{m}{\delta} \right)} \right)$ training evocations of the $ \erm $, with $ 550d $ training points in each evocation, implying a training complexity of $ O\big(\ln\big(\frac{m}{\delta\left(d+\ln{\left(1/\delta \right)}\right)}\big)  \ln{\left(\tfrac{m}{\delta} \right)}   U_{T}(550d)\big)$. After each training evocation of the $ \erm$, the boosting algorithm evaluates the just trained $ \erm $ on all of $ \rS $, amounting to $ O\big(\ln\big(\frac{m}{\delta\left(d+\ln{\left(1/\delta \right)}\right)}\big)  \ln{\left(\tfrac{m}{\delta} \right)}   mU_{I}\big)$ operations over all the evocations of the boosting algorithm, which gives the high-level analysis of the training complexity. The claimed inference complexity follows from the final predictor in Line~\ref{alg:highlevel3} being an majority vote of  $ \Theta\big(\ln\big(\tfrac{m}{\delta\left(d+\ln{\left(1/\delta \right)}\right)}\big)\big) $ hypothesis. 


The optimal PAC error bound on a high level (see \cref{fig:first_figure}) follows from showing that with probability at least $ 1-\delta $ over $ \rS $, it holds that: At least a $ 1-O\left(\left(d+\ln{\left(\delta\right)}\right)/m \right)$ fraction of new examples $ (\rx,\ry) $, is such that with probability at least $ 3/4 $ over the randomness drawing the structured sub training sequences $ \rS_{i} $ in Line~\ref{alg:highlevel1}, the majority vote $ \cB(\rS_{i})=\sum_{j=1}^{t}h_{i,j}/t $ has at least $ 3/4 $ of the voters $ h_{i,j} $ being correct on $ (\rx,\ry) $ (see \cref{fig:first_figure}). Thus for such new examples when drawing $ \rhh_{i} $ in Line~\ref{alg:highlevel3} it is correct with probability at least $ (3/4)^{2}\approx 0.56 $ - slightly better than guessing - so drawing enough of these hypotheses $ \rhh_{i} $, enough being $\Theta\big(\ln\big(\tfrac{m}{\delta\left(d+\ln{\left(1/\delta \right)}\right)}\big)\big) $ many, concentration inequalities implies, that the majority vote $f=\sign(\sum_{i=1}^{l} \rhh_i) $, except on a $ O\left(d+\ln{\left(1/\delta \right)}/m\right) $ fraction of such $ (\rx,\ry) $ examples, will contain more voters being correct than wrong and hence output the correct answers. As there was only a $ O\left(d+\ln{\left(1/\delta \right)}/m\right) $ fraction of new examples $ (\rx,\ry) $  that did not have the above property the optimal PAC bound of $ O\left(d+\ln{\left(1/\delta \right)}/m\right) $ follows.


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{firstfiguredraft.png}
    \caption{The figure illustrates outputs of the boosting algorithm on $ 8 $  structured sub training sequences, each producing a majority vote consisting of $ 8 $ voters depicted by lines coming out of $ \cB(\rS_{i}) $ (to not overload the figure we only include the name of $ h_{1,1}$ and $h_{1,8}$ on the lines). For most new examples $ (\rx,\ry) $ with probability at least $ 3/4 $ over the draw of $ \rS_{i} $  the majority vote $ \cB(\rS_{i}) $ has $ 3/4 $ of it voters correct. Boosting calls $ \cB(\rS_{i}) $ with a green check mark over has $ 3/4 $ of its voters correct on the new example $ (\rx,\ry) $, else a red cross. Lines with a green checkmark at the end correspond to a voter being correct on $ (\rx,\ry) $, and if incorrect a red cross. For instance the call $ \cB(\rS_{1}) $ has all of it voters expect $ h_{1,8}(\rx) $ being equal to $ \ry $, thus $ \cB(\rS_{1})(\rx) $ has a green check mark as $ 7/8\geq 3/4 $ of its voters are correct.}
    \label{fig:first_figure}
\end{figure}

