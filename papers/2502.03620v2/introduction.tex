

\section{Introduction}
The PAC model (Probably Approximately Correct) was introduced by \cite{valiant1984theory,vapnik1964class,vapnik74theory} and has since been a cornerstone in the theory of machine learning. The model is based on the idea that a learning algorithm should be able to learn from a set of labelled training examples such that it can predict the label of unseen examples with high accuracy. In the following, we will consider the realizable setting of PAC learning, that is, we assume that the true concept $ c\in\left\{ -1,1 \right\}^{\cX} $  is contained in a hypothesis class $ \cH \subset \left\{ -1,1 \right\}^{\cX}$, and that the hypothesis class has finite VC-dimension $ d $. Finite VC-dimension $ d $ of $ \cH $ implies that there exists a point set $ x_1,\ldots,x_{d} $, where every pattern in $ \left\{ -1,1 \right\}^{d} $ on the points $ x_{1},\ldots,x_{d} $, can be realized by a hypothesis in the hypothesis class $ \cH $. However, for any point set of more than $ d $ points, this is not the case (\cite{vapnik1971uniform}). Formally, we have the following definition of PAC learning:
\begin{definition}
    Let $\cX$ be an input space and $\cH\subset \left\{ -1,1 \right\}^{\cX}$ a hypothesis class of VC-dimension $ d.$ A learning algorithm $\cA$ is a PAC learner for $\cH$ if there exists a sample complexity function $m_{\cA}:(0,1)^{2}\times \mathbb{N}\rightarrow\mathbb{N}$ such that for every $\epsilon,\delta\in(0,1)$, every distribution $\cD$ over $\cX$, every target concept $ c\in \cH $ and $m\geq m_{\cA}(\epsilon,\delta,d),$ it holds with probability at least $1-\delta$ over a training sequence $\rS=(\rS_1,c(\rS_1),\ldots,\rS_{m},c(\rS_{m}))\sim\cD_{c}^{m}$ with $\rS_{i} \sim\cD ,$ that $ f=\cA(\rS) \in\left\{ -1,1 \right\}^{\cX}$ is such that the error $$\ls_{\cD_{c}}(f):=\p_{\rx\sim \cD_{c}}\left[f(\rx)\not=c(\rx)\right]\leq\epsilon.$$
\end{definition}
The first, and also natural, approach tried out to get PAC-learners with good sample complexity was to do empirical risk minimization ($\erm$). In this approach, given a training sequence $   \rS, $ the PAC learner outputs a hypothesis $ h $  in $ \cH $ that is consistent with the training sequence, i.e., for every $ (\rx,\ry)\in \rS $, the hypothesis $ h $ satisfies $ h(\rx)=\ry $. The following lemma, due to \cite{vapnik1968algorithms,vapnik1971uniform,blumer1989learnability}, gives a uniform error bound on consistent functions in $ \cH$, and thus ensures that any $ \erm $ algorithm guarantees that error bound. Furthermore, error bounds give rise to determining a sample complexity for a given $ \eps $ by solving for $ m $ such that the error bound implies $ \ls_{\cD_{c}}(\cA(\rS))\leq \eps $.
\begin{lemma}\label{uniformconvergencemain}[\cite{vapnik74theory}, \cite{Blumeruniformconvergence} from \cite{Simons}[Theorem 2]]
    For $0<\delta,\eps<1$, hypothesis class $\cH$ of VC-dimension $d$, target concept $c\in\cH$, and distribution $\cD$ over $\cX$, we have with probability at least $1-\delta$ over $\rS\sim \cD_{c}^{m}$, that all $h\in \cH$ consistent with $ S $, i.e.,  $\ls_{\rS}(h):= \tfrac{
        1 }{m}\sum_{(x,y)\in \rS} \ind\left\{ h(x)\not=y \right\}  =0$,
       have error  $$
       \ls_{\cD_{c}}(h)\leq 2(d\log_{2}{\left(2em/d \right)}+\log_{2}{\left(2/\delta \right)})/m.
       $$
   \end{lemma}
   As described above \cref{uniformconvergencemain}, we can transform this error bound into a sample complexity bound of $ O(\left(d\ln{\left(1/\eps \right)}+\ln{\left(1/\delta \right)}\right)/\eps).$  
   This sample complexity bound is known to be tight for general $ \erm $ algorithms, in the sense that for any $0< \eps ,\delta<1/100$ and $ d\in\mathbb{N} $, there exists a hypothesis class $ \cH $ of VC-dimension $ d $, input space $ \cX,$ and distribution $ \cD_{c} $  over $ \cX $  for $ c\in \cH $, such that the sample complexity is $ \Omega((d\ln{\left(1/\eps \right)}+\ln{\left(1/\delta \right)})/\eps) $, where this lower bound construction is due to \cite{bousquet2020proper}[Theorem 11] (See also \cite{haussler1994predicting,auer2007new,simon2015almost,hanneke2016refined}). The above lower bound by \cite{bousquet2020proper} holds in general for any learner outputting a hypothesis in $ \cH $, known as proper learners. In contrast, the lower bound for general learners (not necessarily proper) is $ \Omega((d+\ln{\left(1/\delta \right)})/\eps),$ due to \cite{EHRENFEUCHT1989247}. A natural question to ask is thus whether there is a gap between the performance of proper and improper learners.

This long-standing open problem was finally resolved by \cite{hannekeoptimal}, who showed that there exists an improper learner that, with probability at least $ 1-\delta $ over the given training sequence $ \rS\sim \cD_{c}^{m} $,  produces a function $ f\in \left\{ -1,1 \right\}^{\cX} $, which, obtain the optimal PAC generalization error bound 
$$ \ls_{\cD_{c}}(f) =\Theta\left((d+\ln{\left(1/\delta \right)})/m \right),$$
or equivalently a sample complexity of $O( (d+\ln{\left(1/\delta \right)})/\eps).$   
 The improper learner of \cite{hannekeoptimal} is a  majority vote of $ m^{\log_{4}(3)} \approx m^{0.79}$ voters, with the voters being obtained by running a $ \erm $ algorithm on $m^{\log_{4}(3)}$ carefully chosen sub training sequences of $ \rS $, each of size $ 2m/3 $. Furthermore, \cite{baggingoptimalPAClearner} showed that Bagging, introduced by \cite{Breiman1996BaggingP}, is also an optimal PAC learner. The PAC learner is a majority vote of the output of a $ \erm $ algorithm trained on $ 18\ln{\left(2m/\delta \right)} $ many bootstrap samples of any size between $ 0.02m $ and $ m $, giving a more efficient optimal PAC learner. From the structure of these optimal PAC learners, we notice that their computational efficiency is closely tied to that of the $\erm$ learner run on $ \Theta(m) $ training examples. Thus in the case that, the computational cost of the $\erm$ learner scales poorly in the input size, this carries over to the computational cost of the aforementioned PAC learners, scaling with the number of examples $\Theta(m)$ provided for learning. This raises two natural questions: 
\vspace{-0.075cm}
\begin{enumerate}
    \item Can one obtain an optimal PAC learner that always queries the $ \erm $ learner with fewer than $ m $  training examples?\label{question1}
    \item\vspace{-0.2cm} If possible, can it be done to such a degree that the overall computational complexity of the optimal PAC learner becomes linear in the number of training examples $ m $?\label{question2}
\end{enumerate}
\vspace{-0.075cm}

To address these questions we now introduce some notation. In what follows we assume that the following operations cost one unit of computation: reading a number, comparing two numbers, adding, multiplying, calculating $\exp(\cdot)$ and $\ln{\left(\cdot \right)}$, and renaming. 

Furthermore, we assume the learner has query access to a $ \erm $-algorithm. To compare different learners computational cost, we associate to the $ \erm $ algorithm two cost functions: the worst-case cost of training $ \erm(S) $  on a consistent training sequence $ S $ of length $ m $, denoted  $ \text{U}_{\text{Train}}(m):=\Utrain(m)$
$$\Utrain(m):=\sup_{\stackrel{S\in (\cX\times\cY)^{m}}{S \text{ consistent with } \cH}} \# \{ \text{Operations to}\text{ find } \erm(S)  \},$$
and the worst-case inference cost of calculating the value $ h(x) $  for any point $ x\in \cX $ of any outputted hypothesis, $ h=\erm(S) $, with $ S $ being a consistent training sequence,  denoted  $ \text{U}_{\text{Inference}}:= \Uinf$\footnote{We have for simplicity defined the inference cost as the worst case output for any realizable $ \rS $, so note depending on the number of examples $  m$  in $ \rS $, as in training complexity. If we have gone with this more refined notion the inference cost of \cite{hannekeoptimal} and \cite{baggingoptimalPAClearner} would be $ U_{I}(\Theta(m)) $ and that of \cref{introductionmaintheorem} would be $ U_{I}(550d). $    },

$$\Uinf=\sup_{\stackrel{h=\erm(S), S\in (\cX\times\cY)^{*}}{S \text{ consistent with } \cH,\text{ }x\in\cX}} \# \{ \text{Operations to calculate } h(x)  \}.$$

For a learning algorithm $ \cA:(\cX\times \{ -1,1 \} )^{*}\rightarrow \{ -1,1 \}^{\cX} $, we define the training complexity of $ \cA $ for an integer $ m $ as the worst case number of operations made by the learning algorithm when given a realizable training sequence by $ \cH $ of length $ m $ , i.e.  $$\sup_{\stackrel{S\in(\cX\times \{-1,1\})^{m}}{ S \text{ realizable by } \cH  }} \# \{\text{Operations to find } \cA(S)\}.$$ The inference complexity of a learning algorithm $ \cA $ for an integer $ m $ we define as the worst case cost of predicting a new point $ x\in \cX $ for the learned mapping $f= \cA(S) $, given a realizable training sequence by $ \cH $ of length $ m $   i.e. $$ \sup_{\stackrel{f=\cA(S),S\in (\cX\times \{-1,1\})^{m}}{ S \text{ is realizable by } \cH, \text{ } x\in \cX}} \#\{\text{Operations to calculate } f(x)\}.$$


Using this notation, \cite{hannekeoptimal} has a training complexity of at least $ m^{0.79}\Utrain(2m/3) $ and an inference complexity of at least $ m^{0.79}\Uinf $, whereas \cite{baggingoptimalPAClearner} has a training complexity of at least $ 18\ln{\left(2m/\delta \right)}\cdot\Utrain(0.02m) $ and an inference complexity of $ 18\ln{\left(2m/\delta \right)}\Uinf$. We now state our main result which gives a different tradeoff in terms of training and inference complexity.

\begin{theorem}[Informal statement of \cref{maintheorem}]\label{introductionmaintheorem}
There exists an algorithm $ \ah $ such that for $ 0< \delta <1 $, hypothesis class $ \cH $ of VC-dimension $ d $, target concept $ c\in \cH $, and access to a $ \erm $-learner, that with probability at least $ 1-\delta $ over $ \rS \sim \cD_{c}^{m}$ and the randomness of $ \ah $, it holds that $ \ah $ has, error $ \ls_{\cD_{c}}(\ah)=O((d+\ln{\left(1/\delta \right)})/m),$ inference complexity $ O(\ln{[m /\delta(d+\ln{\left(1/\delta \right)}) ]}) \Uinf $, and  training complexity 
\begin{align*}
    O\bigg(\negmedspace\ln{\bigg(\frac{m}{\delta(d+\ln{\left(1/\delta \right)})} \bigg)}\negmedspace\cdot\negmedspace\ln{\bigg(\frac{m}{\delta} \bigg)}\negmedspace\bigg)\negmedspace\cdot \negmedspace\Big(O\Big(m+d\ln{\left(m \right)}\Big)\negmedspace+\negmedspace\Utrain(550d)\negmedspace+\negmedspace 3m\Uinf\negmedspace\Big).
\end{align*}
         
\end{theorem}

As a result of the above, \textbf{we answer the first question affirmatively} by achieving PAC optimality, using only $550d$ points in each call to the $\erm$ algorithm.
Furthermore, we also notice that if we consider our training complexity as a function of $ m $, thinking of $ \delta $, $ d $, $ \Utrain(550d) $,  and $ \Uinf $  as fixed, our training complexity is up to a $ \ln^{2}{\left(m \right)} $-factor linear in $ m $. Consequently, \textbf{we almost answer the second question affirmatively} under these assumptions. However, it is not always the case that these quantities are fixed as a function of $ m,$ since we are in the distribution-free setting, as we will see in the following paragraph for perceptron. 
Additionally, we also note that the inference complexity of the algorithm is \textbf{asymptotically better} than that of \cite{hannekeoptimal} and \cite{baggingoptimalPAClearner}\footnote{Looking at the proof of \cite{baggingoptimalPAClearner} we think that \cite{baggingoptimalPAClearner} could obtain the same inference complexity.}.  

Our training complexity is a multiplicative factor of  $ O(\ln{[m/(\delta(d+\ln{(1/\delta )}))]}\cdot(m+d\ln{(m )}+\Utrain(550d)+3m\Uinf))/\Utrain(0.02m)$ different from that of \cite{baggingoptimalPAClearner}, and only a smaller multiplicative factor different from that of \cite{hannekeoptimal}. Thus, if $ \delta$ is some polynomial in $\Omega(1/m^{C}) $ for some fixed constant $ C \geq 1$, the term in the denominator is $O( \ln{\left(m \right)}(m+d\ln{\left(m \right)}+\Utrain(550d)+3m\Uinf) )$, which in the case $ \Utrain(0.02m)=\omega(\ln{\left(m \right)}\cdot\max ( m , d\ln^{2}{\left(m \right)} , \Utrain(550d), 3m \Uinf )) $,  implies that we get an asymptotically better training complexity. If we think of $ d,\Utrain(550d) $  and $ \Uinf $ as fixed, $ \Utrain(0.02m) $ has to be $ \omega(m\ln{\left(m \right)}) $ for \cref{introductionmaintheorem} to be better, however, as commented above this is not always the case.

We now give two examples of where \cref{introductionmaintheorem} might have a better training complexity. In the following we will for simplicity set $ \delta $ equal to a small constant, thus the training complexity of \cref{introductionmaintheorem} holds with this probability.  

\input{infinite.hypothesistable.tex}
\input{perceptron.example.tex}
\newline \linebreak
Before moving on to the next two sections, where we in the first section provide a high-level proof sketch and in the latter section a detailed proof sketch and explain the relation of our work to previous work, we would like to mention some other related work on optimal learning. 

\cite{optimalwithoutuniformconvergence} gives an interesting alternative to the above optimal PAC learners based on a majority vote of $ m/4 $ one-inclusion graphs, given $ \Theta(m) $ points as input. When the one-inclusion graphs have to be computed this require searching through the hypothesis class $ \cH $, which makes it hard to compare to the above setting where the learner is only assumed access to the hypothesis class through a $ \erm $ algorithm. 

We also want to mention \cite{majorityofthree}, which shows, that the majority vote of 3 $ \erm $-calls on 3 disjoint training sequences has optimal in expectation error $ O(d/m) $. However, \cite{majorityofthree} were not able to extend the result to the optimal error bound in the PAC model. 
