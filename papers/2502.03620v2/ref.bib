@inproceedings{Optimalweaktostronglearning,
  author       = {Kasper Green Larsen and
                  Martin Ritzert},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Optimal Weak to Strong Learning},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/d38653cdaa8e992549e1e9e1621610d7-Abstract-Conference.html},
  timestamp    = {Mon, 08 Jan 2024 16:31:35 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/LarsenR22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@book{boostingbookSchapireF12,
  author    = {Schapire, Robert E. and Freund, Yoav},
  title     = {{Boosting: Foundations and Algorithms}},
  publisher = {The MIT Press},
  year      = {2012},
  month     = {05},
  abstract  = {{An accessible introduction and essential reference for an approach to machine learning that creates highly accurate prediction rules by combining many weak and inaccurate ones.Boosting is an approach to machine learning based on the idea of creating a highly accurate predictor by combining many weak and inaccurate “rules of thumb.” A remarkably rich theory has evolved around boosting, with connections to a range of topics, including statistics, game theory, convex optimization, and information geometry. Boosting algorithms have also enjoyed practical success in such fields as biology, vision, and speech processing. At various times in its history, boosting has been perceived as mysterious, controversial, even paradoxical.This book, written by the inventors of the method, brings together, organizes, simplifies, and substantially extends two decades of research on boosting, presenting both theory and applications in a way that is accessible to readers from diverse backgrounds while also providing an authoritative reference for advanced researchers. With its introductory treatment of all material and its inclusion of exercises in every chapter, the book is appropriate for course use as well.The book begins with a general introduction to machine learning algorithms and their analysis; then explores the core theory of boosting, especially its ability to generalize; examines some of the myriad other theoretical viewpoints that help to explain and understand boosting; provides practical extensions of boosting for more complex learning problems; and finally presents a number of advanced theoretical topics. Numerous applications and practical illustrations are offered throughout.}},
  isbn      = {9780262301183},
  doi       = {10.7551/mitpress/8291.001.0001},
  url       = {https://doi.org/10.7551/mitpress/8291.001.0001},
  eprint    = {https://direct.mit.edu/book-pdf/2280056/book\_9780262301183.pdf}
}

@article{rademacherDudley1978,
author = {R. M. Dudley},
title = {{Central Limit Theorems for Empirical Measures}},
volume = {6},
journal = {The Annals of Probability},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {899 -- 929},
keywords = {central limit theorems, Donsker classes, Effros Borel structure, empirical measures, metric entropy with inclusion, two-sample case, Vapnik-Cervonenkis classes},
year = {1978},
doi = {10.1214/aop/1176995384},
URL = {https://doi.org/10.1214/aop/1176995384}
}


@misc{rademacherboundlecturenotes,
  title={{ECE 543: Statistical Learning Theory}},
  author={Hajek, Bruce and Raginsky, Maxim},
  howpublished={\textit{Department of Electrical and Computer Engineering and the Coordinated Science Laboratory, University of Illinois at Urbana-Champaign}},
  year={2021},
  note={Last updated March 18, 2021},
  URL={http://maxim.ece.illinois.edu/teaching/SLT}
}

@article{optimalexpectationweaktostrong,
  title={The Many Faces of Optimal Weak-to-Strong Learning},
  author={Mikael M{\o}ller H{\o}gsgaard and Kasper Green Larsen and Markus Engelund Mathiasen},
  journal={},
  year={2024},
  url={},
}

@inproceedings{baggingoptimalPAClearner,
  author       = {Kasper Green Larsen},
  editor       = {Gergely Neu and
                  Lorenzo Rosasco},
  title        = {Bagging is an Optimal {PAC} Learner},
  booktitle    = {The Thirty Sixth Annual Conference on Learning Theory, {COLT} 2023,
                  12-15 July 2023, Bangalore, India},
  series       = {Proceedings of Machine Learning Research},
  volume       = {195},
  pages        = {450--468},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v195/larsen23a.html},
  timestamp    = {Wed, 06 Sep 2023 17:49:05 +0200},
  biburl       = {https://dblp.org/rec/conf/colt/Larsen23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Li2000ImprovedBO,
  title={Improved bounds on the sample complexity of learning},
  author={Yi Li and Philip M. Long and Aravind Srinivasan},
  journal={J. Comput. Syst. Sci.},
  year={2000},
  volume={62},
  pages={516-527},
  url={https://api.semanticscholar.org/CorpusID:7812373}
}

@inproceedings{Bartlett2003RademacherAG,
  title={Rademacher and Gaussian Complexities: Risk Bounds and Structural Results},
  author={Peter L. Bartlett and Shahar Mendelson},
  booktitle={Journal of machine learning research},
  year={2003},
  url={https://api.semanticscholar.org/CorpusID:463216}
}

@article{Blumeruniformconvergence,
author = {Blumer, Anselm and Ehrenfeucht, A. and Haussler, David and Warmuth, Manfred K.},
title = {Learnability and the Vapnik-Chervonenkis dimension},
year = {1989},
issue_date = {Oct. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0004-5411},
url = {https://doi.org/10.1145/76359.76371},
doi = {10.1145/76359.76371},
abstract = {Valiant's learnability model is extended to learning classes of concepts defined by regions in Euclidean space En. The methods in this paper lead to a unified treatment of some of Valiant's results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufficient conditions are provided for feasible learnability.},
journal = {J. ACM},
month = {oct},
pages = {929–965},
numpages = {37}
}

@book{vapnik74theory,
  abstract = {In these two books, the original &quot;Generalized Portrait&quot;
    Algorithm for constructing separating hyperplanes with optimal margin
    is described.},
  added-at = {2008-04-30T12:59:47.000+0200},
  address = {Moscow},
  author = {Vapnik, V. and Chervonenkis, A.},
  biburl = {https://www.bibsonomy.org/bibtex/29a33742ce9e3e4df8e4691f8e4c75e3a/kdubiq},
  description = {KDubiq Blueprint},
  groupsearch = {0},
  interhash = {936f556afc966ddda07ba175241d6924},
  intrahash = {9a33742ce9e3e4df8e4691f8e4c75e3a},
  keywords = {imported},
  note = {(German Translation: W.~Wapnik \& A.~Tscherwonenkis, {\em Theorie
    der Zeichenerkennung}, Akademie--Verlag, Berlin, 1979)},
  publisher = {Nauka},
  timestamp = {2008-04-30T13:00:30.000+0200},
  title = {Theory of Pattern Recognition [in Russian]},
  year = 1974
}



@InProceedings{Simons,
  title = 	 {An Almost Optimal PAC Algorithm},
  author = 	 {Simon, Hans U.},
  booktitle = 	 {Proceedings of The 28th Conference on Learning Theory},
  pages = 	 {1552--1563},
  year = 	 {2015},
  editor = 	 {Grünwald, Peter and Hazan, Elad and Kale, Satyen},
  volume = 	 {40},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Paris, France},
  month = 	 {03--06 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v40/Simon15a.pdf},
  url = 	 {https://proceedings.mlr.press/v40/Simon15a.html},
  abstract = 	 {The best currently known general lower and upper bounds on the number of labeled examples needed for learning a concept class in the PAC framework (the realizable case) do not perfectly match: they leave a gap of order \log(1/ε) (resp.&nbsp;a gap which is logarithmic in another one of the relevant parameters). It is an unresolved question whether there exists an “optimal PAC algorithm” which establishes a general upper bound with precisely the same order of magnitude as the general lower bound. According to a result of Auer and Ortner, there is no way for showing that arbitrary consistent algorithms are optimal because they can provably differ from optimality by factor \log(1/ε). In contrast to this result, we show that every consistent algorithm L (even a provably suboptimal one) induces a family (L_K)_K\ge1 of PAC algorithms (with 2K-1 calls of L as a subroutine) which come very close to optimality: the number of labeled examples needed by L_K exceeds the general lower bound only by factor \ell_K(1/\epsillon) where \ell_K denotes (a truncated version of) the K-times iterated logarithm. Moreover, L_K is applicable to any concept class C of finite VC-dimension and it can be implemented efficiently whenever the consistency problem for C is feasible. We show furthermore that, for every consistent algorithm L, L_2 is an optimal PAC algorithm for precisely the same concept classes which were used by Auer and Ortner for showing the existence of suboptimal consistent algorithms.  This can be seen as an indication that L_K may have an even better performance than it is suggested by our worstcase analysis.}
}


@article{hannekeoptimal,
  author  = {Steve Hanneke},
  title   = {The Optimal Sample Complexity of PAC Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {38},
  pages   = {1--15},
  url     = {http://jmlr.org/papers/v17/15-389.html}
}

@article{measureone,
author = {Blumer, Anselm and Ehrenfeucht, A. and Haussler, David and Warmuth, Manfred K.},
title = {Learnability and the Vapnik-Chervonenkis dimension},
year = {1989},
issue_date = {Oct. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0004-5411},
url = {https://doi.org/10.1145/76359.76371},
doi = {10.1145/76359.76371},
abstract = {Valiant's learnability model is extended to learning classes of concepts defined by regions in Euclidean space En. The methods in this paper lead to a unified treatment of some of Valiant's results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufficient conditions are provided for feasible learnability.},
journal = {J. ACM},
month = {oct},
pages = {929–965},
numpages = {37}
}

@book{measuretwo,
  title = {Convergence of stochastic processes},
  year = {2012},
  author = {David, Pollard},
  publisher = {Springer Science \& Business Media}
}

@inproceedings{optimalwithoutuniformconvergence,
author = {Aden-Ali, Ishaq and Cherapanamjeri, Yeshwanth and Shetty, Abhishek and Zhivotovskiy, Nikita},
year = {2023},
month = {11},
pages = {1203-1223},
title = {Optimal PAC Bounds without Uniform Convergence},
doi = {10.1109/FOCS57990.2023.00071}
}

@inproceedings{majorityofthree,
  title={Majority-of-Three: The Simplest Optimal Learner?},
  author={Aden-Ali, Ishaq and H{\o}andgsgaard, Mikael M{\o}ller and Larsen, Kasper Green and Zhivotovskiy, Nikita},
  booktitle={The Thirty Seventh Annual Conference on Learning Theory},
  pages={22--45},
  year={2024},
  organization={PMLR}
}

@article{valiant1984theory,
  title={A theory of the learnable},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={27},
  number={11},
  pages={1134--1142},
  year={1984},
  publisher={Acm New York, NY, USA}
}

@article{vapnik1964class,
  title={A class of algorithms for pattern recognition learning},
  author={Vapnik, Vladimir and Chervonenkis, Alexey},
  journal={Avtomatika i Telemekhanika},
  volume={25},
  number={6},
  pages={937--945},
  year={1964}
}

@article{blumer1989learnability,
  title={Learnability and the {V}apnik-{C}hervonenkis dimension},
  author={Blumer, Anselm and Ehrenfeucht, Andrzej and Haussler, David and Warmuth, Manfred K},
  journal={Journal of the ACM},
  volume={36},
  number={4},
  pages={929--965},
  year={1989},
  publisher={ACM New York, NY, USA}
}

@article{vapnik1971uniform,
  title={On uniform convergence of the frequencies of events to their probabilities},
  author={Vapnik, Vladimir and Chervonenkis, Alexey},
  journal={Teoriya Veroyatnostei i ee Primeneniya},
  volume={16},
  number={2},
  pages={264--279},
  year={1971},
  publisher={Russian Academy of Sciences}
}

@article{vapnik1968algorithms,
  title={Algorithms with complete memory and recurrent algorithms in the problem of learning pattern recognition},
  author={Vapnik, Vladimir and Chervonenkis, Alexey},
  journal={Avtomatika i Telemekhanika},
  issue={4},
  pages={95--106},
  year={1968}
}

@article{haussler1994predicting,
  title={Predicting $\{$0, 1$\}$-functions on randomly drawn points},
  author={Haussler, David and Littlestone, Nick and Warmuth, Manfred K},
  journal={Information and Computation},
  volume={115},
  number={2},
  pages={248--292},
  year={1994},
  publisher={Elsevier}
}

@article{auer2007new,
  title={A new {PAC} bound for intersection-closed concept classes},
  author={Auer, Peter and Ortner, Ronald},
  journal={Machine Learning},
  volume={66},
  number={2},
  pages={151--163},
  year={2007},
  publisher={Springer}
}

@inproceedings{simon2015almost,
  title={An almost optimal {PAC} algorithm},
  author={Simon, Hans U},
  booktitle={Conference on Learning Theory},
  pages={1552--1563},
  year={2015},
  organization={PMLR}
}


@article{hanneke2016refined,
  title={Refined error bounds for several learning algorithms},
  author={Hanneke, Steve},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={4667--4721},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{bousquet2020proper,
  title={Proper learning, {H}elly number, and an optimal {SVM} bound},
  author={Bousquet, Olivier and Hanneke, Steve and Moran, Shay and Zhivotovskiy, Nikita},
  booktitle={Conference on Learning Theory},
  pages={582--609},
  year={2020},
  organization={PMLR}
}

@article{samplecompressionschemesamirmoran,
  author       = {Shay Moran and
                  Amir Yehudayoff},
  title        = {Sample Compression Schemes for {VC} Classes},
  journal      = {J. {ACM}},
  volume       = {63},
  number       = {3},
  pages        = {21:1--21:10},
  year         = {2016},
  url          = {https://doi.org/10.1145/2890490},
  doi          = {10.1145/2890490},
  timestamp    = {Tue, 06 Nov 2018 12:51:46 +0100},
  biburl       = {https://dblp.org/rec/journals/jacm/MoranY16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Adaboost,
title = {A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting},
journal = {Journal of Computer and System Sciences},
volume = {55},
number = {1},
pages = {119-139},
year = {1997},
issn = {0022-0000},
doi = {https://doi.org/10.1006/jcss.1997.1504},
url = {https://www.sciencedirect.com/science/article/pii/S002200009791504X},
author = {Yoav Freund and Robert E Schapire},
}

@article{Breiman1996BaggingP,
  title={Bagging Predictors},
  author={L. Breiman},
  journal={Machine Learning},
  year={1996},
  volume={24},
  pages={123-140},
  url={https://api.semanticscholar.org/CorpusID:47328136}
}

@INBOOK{SVM,
  author={Bottou, Léon and Chapelle, Olivier and DeCoste, Dennis and Weston, Jason},
  booktitle={Large-Scale Kernel Machines}, 
  title={Support Vector Machine Solvers}, 
  year={2007},
  volume={},
  number={},
  pages={1-27},
  keywords={},
  doi={}}

@article{Gey2012VapnikChervonenkisDOaxisdecisionstumps,
  title={Vapnik–Chervonenkis dimension of axis-parallel cuts},
  author={Servane Gey},
  journal={Communications in Statistics - Theory and Methods},
  year={2012},
  volume={47},
  pages={2291 - 2296},
  url={https://api.semanticscholar.org/CorpusID:88512101}
}

@inproceedings{Novikoff1963ONCP,
  title={ON CONVERGENCE PROOFS FOR PERCEPTRONS},
  author={Albert B. J. Novikoff},
  year={1963},
  url={https://api.semanticscholar.org/CorpusID:122810543}
}

@inproceedings{Jin2017SVMVS,
  title={SVM via Saddle Point Optimization: New Bounds and Distributed Algorithms},
  author={Yifei Jin and Lingxiao Huang and Jian Li},
  booktitle={Scandinavian Workshop on Algorithm Theory},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:30790622}
}

@article{Rosenblatt1958ThePA,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Frank Rosenblatt},
  journal={Psychological review},
  year={1958},
  volume={65 6},
  pages={
          386-408
        },
  url={https://api.semanticscholar.org/CorpusID:12781225}
}

@book{understandingmachinelearning,
author = {Shalev-Shwartz, Shai and Ben-David, Shai},
title = {Understanding Machine Learning: From Theory to Algorithms},
year = {2014},
isbn = {1107057132},
publisher = {Cambridge University Press},
address = {USA},
abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.}
}

@article{EHRENFEUCHT1989247,
title = {A general lower bound on the number of examples needed for learning},
journal = {Information and Computation},
volume = {82},
number = {3},
pages = {247-261},
year = {1989},
issn = {0890-5401},
doi = {https://doi.org/10.1016/0890-5401(89)90002-3},
url = {https://www.sciencedirect.com/science/article/pii/0890540189900023},
author = {Andrzej Ehrenfeucht and David Haussler and Michael Kearns and Leslie Valiant},
abstract = {We prove a lower bound of Ω((1/ɛ)ln(1/δ)+VCdim(C)/ɛ) on the number of random examples required for distribution-free learning of a concept class C, where VCdim(C) is the Vapnik-Chervonenkis dimension and ɛ and δ are the accuracy and confidence parameters. This improves the previous best lower bound of Ω((1/ɛ)ln(1/δ)+VCdim(C)) and comes close to the known general upper bound of O((1/ɛ)ln(1/δ)+(VCdim(C)/ɛ)ln(1/ɛ)) for consistent algorithms. We show that for many interesting concept classes, including kCNF and kDNF, our bound is actually tight to within a constant factor.}
}