
\section{Perceptron example}\label{appendix:perceptron}
In the following, we consider $m \in \mathbb{N}, m \geq 2200$, and the perceptron algorithm (due to \cite{Rosenblatt1958ThePA}, see e.g. \cite{understandingmachinelearning} chapter 9.1.2). We assume that input points $ x\in \mathbf{R}^{d} $  have a hard-coded a $ 1 $ in the d'th coordinate $ x_{d}=1 $ such that $ xw=\sum_{i=1}^{d-1}x_{i}w_{i}+w_{d},$ i.e. $ w_{d} $ is the bias of the hyperplane $ w.$      
\begin{algorithm}[H]
\caption{Perceptron Algorithm}\label{alg:perceptron}
\begin{algorithmic}[1]
\State \textbf{Input:} Training set $\{(x_i, y_i)\}_{i=1}^n$ with $x_i \in \mathbb{R}^d,$ such that $ x_{i,d}=1 $ for all $ i=1\ldots,n $  and $y_i \in \{-1, 1\}$
\State \textbf{Output:} Weight vector $w$
\State Initialize $w_0 \leftarrow 0$
\State \textbf{Repeat:} until no classification mistake for $i = 1, \ldots, n$
\State\hspace{0.5cm}\vline\quad \textbf{for: $i \in\{ 1, \ldots, n\}$}
\State\hspace{1cm}\vline\quad\textbf{if $y_i \cdot \left\langle w_{j-1}, x_i \right\rangle \leq 0$ then}
\State\hspace{1.5cm}\vline\quad $w_j \leftarrow w_{j-1} + y_i \cdot x_i$
\State \Return last iterate $w_j$
\end{algorithmic}
\end{algorithm}


Let $x_i = \left(0, 1 - \frac{i}{m^4},1\right)$ and $y_i = -1$ for $i = 1, \ldots, m-1$, and let $x_m = \left(\sqrt{\frac{1}{m}}, 1,1\right)$ and $y_m = 1$. We observe that 
$
2 - \frac{2}{m^3} < \left\langle x_i, x_j \right\rangle < 2 - \frac{i}{m^4}, \quad \text{for } i, j \in \{1, \ldots, m-1\},
$
$
\left\langle x_i, x_m \right\rangle = 2 - \frac{i}{m^4}, \quad \text{for } i \in \{1, \ldots, m-1\},
$
and 
$
\left\langle x_m, x_m \right\rangle = 2 + \frac{1}{m}.
$

Furthermore, we notice that if $r_1, \ldots, r_t \in \{1, \ldots, m-1\}$ and 
$
w' = \sum_{q=1}^t y_{r_q} x_{r_q} + t y_m x_m = -\sum_{q=1}^t x_{r_q} + t x_m,
$
then for any $i \in \{1, \ldots, m-1\}$, we have 
$
\left\langle w', x_i \right\rangle > -t\left(2 - \frac{i}{m^4}\right) + t\left(2 - \frac{i}{m^4}\right) \geq 0,
$
so $x_i$ is misclassified. Moreover, 
$
\left\langle w', x_m \right\rangle \geq -t\left(2 - \frac{1}{m^4}\right) + t\left(2 + \frac{1}{m}\right) > 0.
$
Thus, if $w'$ consists of equally many point from $(x_1, y_1), \ldots, (x_{m-1}, y_{m-1})$ and from $(x_m, y_m)$, then $(x_m, y_m)$ will be classified correctly, while any $(x_1, y_1), \ldots, (x_{m-1}, y_{m-1})$ will be misclassified. 

Furthermore, if $r_1, \ldots, r_{t+1} \in \{1, \ldots, m-1\}$ and 
$
w' = \sum_{q=1}^{t+1} y_{r_q} x_{r_q} + t y_m x_m = -\sum_{q=1}^{t+1} x_{r_q} + t x_m,
$
and $t \leq 2m-2$, then for any $i \in \{1, \ldots, m-1\}$, 
$
\left\langle w', x_i \right\rangle < -(t+1)(2 - \frac{2}{m^3}) + t(2 - \frac{i}{m^4}) < 0,
$
so $x_i$ is correctly classified. Moreover, 
$
\left\langle w', x_m \right\rangle \leq -(t+1)(2 - \frac{1}{m^3}) + t(2 + \frac{1}{m}) < 0,
$
since $t\leq 2m-2$. Thus, if $w'$ consists of one more point from $(x_1, y_1), \ldots, (x_{m-1}, y_{m-1})$ than the number of points from $(x_m, y_m)$, then any point in $(x_1, y_1), \ldots, (x_{m-1}, y_{m-1})$ is correctly classified and $(x_m, y_m)$ is misclassified.

Let $((x_{s_{1}},y_{s_{1}}),\ldots,(x_{s_{n}},y_{s_{n}}))$ be a training sequence of size $n$, i.e., $s_{1},\ldots,s_{n}\in \{1,\ldots,m\}$ (possibly with repetitions), and such that there exists $l\in \{1,\ldots,n\}$ with $y_{s_{l-1}}\neq y_{s_{l}}$, where $s_{0}=s_{n}$. If $y_{s_{1}} = -1$, then $w_{1} = -x_{s_{1}}$, and by the above for $ j\geq 2 $ , $w_{j-1}$ is updated each time $y_{s_{l-1}}\neq y_{s_{l}}$, provided $j-1$ is odd and $(j-2)/2+1 \leq 2m-2$, or $j-1$ is even and $(j-1)/2 \leq 2m-2$.

In the odd case, $w_{j-1} = \sum_{q=1}^{(j-2)/2+1} x_{r_{q}} + (j-2)/2 x_{m}$ is updated to $w_{j} = \sum_{q=1}^{(j-2)/2+1} x_{r_{q}} + ((j-2)/2+1) x_{m}$. In the even case, $w_{j-1} = \sum_{q=1}^{(j-1)/2} x_{r_{q}} + (j-1)/2 x_{m}$ is updated to $w_{j} = \sum_{q=1}^{(j-1)/2+1} x_{r_{q}} + (j-1)/2 x_{m}$.

Thus, we conclude that for a training sequence $((x_{s_{1}},y_{s_{1}}),\ldots,(x_{s_{n}},y_{s_{n}}))$ where there exists $l \in \{1,\ldots,n\}$ with $y_{s_{l-1}}\neq y_{s_{l}}$, $y_{s_{1}} = -1$ leads to update of $ w_{j-1} $  if $j-1$ is odd and $(j-2)/2+1 \leq 2m-2$, i.e., $j \leq 2(2m-3)+2$, or if $j-1$ is even and $(j-1)/2 \leq 2m-2$, i.e., $j \leq 2(2m-2)+1$. This results in at least $4m-4$ updates. Moreover, except for the first update, updates occur only when $y_{s_{l-1}}\neq y_{s_{l}}$.

We now consider a distribution $\cD$ over $(x_{1},y_{1}),\ldots,(x_{m},y_{m})$, which assigns $\cD(x_{m})=p$ and $\cD(x_{i})=(1-p)/(m-1)$ for $i\neq m$. For a training sequence $\rS = ((\rS_{1,1},\rS_{1,2}),\ldots,(\rS_{m,1},\rS_{m,2})) \sim \cD^{m}$, let $\mathbf{M}_{1} = |{i : \rS_{i,2}=1}| = \sum_{(x,y)\in \rS} \ind\{(x,y)=(x_{m},y_{m})\}$. Then, $\e_{\rS\sim\cD^{m}}[\mathbf{M}_{1}] = pm$ and $\text{Var}(\mathbf{M}_{1}) = (1-p)pm \leq pm$. Thus, by Chebyshev's inequality, we have that, with probability at least $1-1/4$ over $\rS$, it holds that $pm-\sqrt{4pm} < \mathbf{M}_{1} < pm+\sqrt{4pm}$. Let now $\rS_{B} = (({\rS_{B}}_{1,1}, {\rS_{B}}_{1,2}), \ldots, ({\rS_{B}}_{n,1}, {\rS_{B}}_{n,2}))$ be a sub-training sequence of size $n=0.02m$, drawn with replacement from $\rS$ (i.e., a bagging sample). Thus, by Chebyshev's inequality, we have that, with probability at least $1-1/4$ over $\rS_{B}$, it holds that $\frac{\mathbf{M}_{1}}{m}n-\sqrt{4\frac{\mathbf{M}_{1}}{m}n} < \sum_{(x,y) \in \rS_{B}} \ind\{(x,y) = (x_{m},y_{m})\} < \frac{\mathbf{M}_{1}}{m}n+\sqrt{4\frac{\mathbf{M}_{1}}{m}n}$.

Now, for $p=250/m$, we have that $pm-\sqrt{4pm}=250-\sqrt{1000} > 200$ and $pm+\sqrt{4pm}<300$, thus with probability at least $1-1/4$ over $\rS$, $200 < \mathbf{M}_{1} < 300$, which implies that, with probability at least $1-1/4$ over $\rS_{B}$, it holds that $0=200\cdot 0.02-\sqrt{4\cdot 200 \cdot 0.02} < \sum_{(x,y) \in \rS_{B}} \ind\{(x,y) = (x_{m},y_{m})\} \leq 300\cdot 0.02+\sqrt{4\cdot 300 \cdot 0.02} < 11$, where the first inequality follow by $ \frac{\mathbf{M}_{1}}{m}n-\sqrt{4\frac{\mathbf{M}_{1}}{m}n} $ being increasing in $ \frac{\mathbf{M}_{1}}{m}n $ for $ \frac{\mathbf{M}_{1}}{m}n \geq  4$ . Thus, with probability at least $(1-1/4)^2$ over both $\rS$ and one bootstrap sample of size $0.02m$, it holds that $1 \leq \sum_{(x,y) \in \rS_{B}} \ind\{(x,y) = (x_{m},y_{m})\} \leq 10$. Furthermore, with probability at least $1-\frac{10}{m} \geq 1-\frac{10}{2000}$ over $\rS_{B} = (({\rS_{B}}_{1,1}, {\rS_{B}}_{1,2}), \ldots, ({\rS_{B}}_{n,1}, {\rS_{B}}_{n,2}))$, the first example $({\rS_{B}}_{1,1}, {\rS_{B}}_{1,2})$ of $\rS_{B}$ is not equal to $({\rS_{B}}_{1,1}, {\rS_{B}}_{1,2}) \not= (x_{m}, y_{m})$. Thus, with probability at least $(1-1/4)^2(1-1/10) \geq 1/2$ over $\rS$ and $\rS_{B}$, it holds that $1 \leq \sum_{(x,y) \in \rS_{B}} \ind\{(x,y) = (x_{m},y_{m})\} \leq 10$ and that $({\rS_{B}}_{1,1}, {\rS_{B}}_{1,2}) \not= (x_{m}, y_{m})$. Consider such a realization $S_{B} = ((x_{s_{1}}, y_{s_{1}}), \ldots, (x_{s_{n}}, y_{s_{n}}))$. 

We recall that we concluded above that, for any training sequence of $(x_{1}, y_{1}), \ldots, (x_{m}, y_{m})$, especially $S_{B} = ((x_{s_{1}}, y_{s_{1}}), \ldots, (x_{s_{n}}, y_{s_{n}}))$, with $y_{s_{1}} = -1$ and such that there exists $y_{s_{l-1}} \not= y_{s_{l}}$, the perceptron run on $S_{B}$ has at least $4m-4$ updates, and except for the first update, the updates only happen when $y_{s_{l-1}} \not= y_{s_{l}}$. We conclude that each time the perceptron passes over $((x_{s_{2}}, y_{s_{2}}), \ldots, (x_{s_{n}}, y_{s_{n}}), (x_{s_{1}}, y_{s_{1}}))$, it makes at most $2\sum_{(x,y) \in S_{B}} \ind\{(x,y) = (x_{m}, y_{m})\} \leq 20$ updates and thus has to pass $\Omega(m)$ times over $((x_{s_{2}}, y_{s_{2}}), \ldots, (x_{s_{n}}, y_{s_{n}}), (x_{s_{1}}, y_{s_{1}}))$, where each pass takes $\Omega(m)$ operations, leading to $U_{T}(n)=U_{T}(0.02m) \geq \Omega(m^2)$ with probability at least $1-1/2$ over $\rS$ and $\rS_{B}$, implying that \cite{baggingoptimalPAClearner} takes at least $\Omega(m^{2})$ operations with probability at least $1/2$ over the random sample and the bagging step.

Now, for the training complexity of \cref{introductionmaintheorem}, we notice that since the $ \sign $ of hyperplanes in $ \mathbf{R}^{3} $  has VC-dimension $ 4,$ it suffices to consider sub training sequence of size $ 2200.$  Furthermore,  we observe that the vector $(1, -\sqrt{\frac{1}{2m}},0)$ is such that $\left\langle (1, -\sqrt{\frac{1}{2m}},0), x_{i} \right\rangle = \left\langle (1, -\sqrt{\frac{1}{2m}},0), (0,1-\frac{i}{m^{4}},1) \right\rangle = -\sqrt{\frac{1}{2m}} + \frac{i}{\sqrt{2}m^{4.5}} < -\sqrt{\frac{1}{4m}}$ for $i \in \{1, \ldots, m-1\}$, and $\left\langle (1, -\sqrt{\frac{1}{2m}},0), x_{m}\right\rangle = \left\langle (1, -\sqrt{\frac{1}{2m}},0), (\sqrt{\frac{1}{m}}, 1,1) \right\rangle \geq \sqrt{\frac{1}{16m}}$ by $m \geq 10.$ Furthermore, since $\left|\left|(1, -\sqrt{\frac{1}{2m}},0)\right|\right|{2} \leq 2$, we conclude from the above that the margin of $(x_{1}, y_{1}), \ldots, (x_{m}, y_{m})$ is at least $\gamma = \max_{w \in \mathbb{R}^{3}, \left|\left|w\right|\right|_{2} \leq 1} \min_{j \in \{1, \ldots, m\}} y_{j} \left\langle w, x_{j} \right\rangle \geq \sqrt{\frac{1}{64m}}$. Furthermore, the norm of $ x_{i} $ is at most  $\left|\left|x_{i}\right|\right|_{2} \leq 2$ for any $ i\in\{  1,\ldots,m\}  $. It is known by \cite{Novikoff1963ONCP} that the number of mistakes/updates the perceptron makes when passing over a data stream of any length, where the points have norm at most $M$, and the data stream can be separated by a hyperplane with margin $\gamma^{2}$, is at most $\frac{M^{2}}{\gamma^{2}}$. Thus, for any sub-training sequence of $x_{s_{1}}, \ldots, x_{s_{n}}$ of size $n = 2200$, the perceptron makes at most $O(m)$ passes over $x_{s_{1}}, \ldots, x_{s_{n}}$, before it make no mistakes when passing over $x_{s_{1}}, \ldots, x_{s_{n}}$, and since each pass takes $O(1)$ operations, $U_{T}(2200) = O(m)$. Thus, except for the small constant probability (considered in these examples), \cref{introductionmaintheorem} has a training complexity of $O\left(\ln{\left(\frac{m}{d}\right)} \ln{(m)} \left(m + U_{T}(2200) + mU_{I}\right)\right) = O\left(\ln{\left(\frac{m}{d}\right)} \ln{(m)} m\right)$, where we have used that performing inference over a hyperplane in $\mathbb{R}^{3}$ takes at most $U_{I} = O(1)$ operations.