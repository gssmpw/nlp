\section{Properties of $\as$ }\label{sec:propAdaBoostSample}
In this section, we present \as \cref{alg:AdaBoostSample} and prove the properties of $ \cA $  stated in \cref{lemma:adaboostsampleeasyproperties} and \cref{lemmaAdaBoostSampleefficent}. In the presentation of the algorithm, we use the following notation: For $r \in [0:1]$, and $C$ a cumulative distribution function for a distribution $D$ over $\{ 1:m \}$, we define $C^{-1}(r)$ to be the index $l$ in $\{ 1:m \}$ such that $C(l-1)\leq r$  and $C(l)> r$, we take $ C(0)=0 $. For a vector $ r\in [0:1]^{s} $, we let $ C^{-1}(r) $ denote  $ C^{-1} $ applied entrywise on $ r $, i.e., $C^{-1}(r)_{i}=C^{-1}(\rr_{i})$. With this definition, we have for $ \rr\sim[0:1]^{s} $  that $ C^{-1}(\rr)\sim \cD^{s} $ by the $ \rr_{i} $'s being i.i.d. $ \sim[0,1] $. We sometimes write $ (x,y)\gets(a,b) $ which means $ x=a $ and $ y=b $. We now present  \cref{alg:AdaBoostSample}.
\vspace{-0.3cm}
\input{AdaBoostSample.tex} 

We begin with the proof of \cref{lemma:adaboostsampleeasyproperties}, which makes some observations about $ \cA$. For convenience, we restate \cref{lemma:adaboostsampleeasyproperties} here before providing its proof. 

\lemmaadaboostsampleeasyproperties*

\begin{proof}[Proof of \cref{lemma:adaboostsampleeasyproperties}]
The claim that $\cA(S,r)\in\dlh$ for $t=\left\lceil20^{2}\ln{(m)}/2\right\rceil$ follows from Line~\ref{alg:AdaBoostSamplealphaset} and Line~\ref{alg:AdaBoostSampleearlystop}, which implies that $f$ will never be updated more than $t$ times. Line~\ref{alg:AdaBoostSamplecheckt} ensures there are always at least $t$ hypotheses if Line~\ref{alg:AdaBoostsample:output1} is the output. If Line~\ref{alg:AdaBoostsample:output3} is the output, we can write it as $t$ copies of the same hypothesis and then normalize by $t$. 

That the output $f$ is such that $\sum_{x\in S} \ind\{\sum_{i=1}^{t} h_{f,i}(x)c(x)/t\leq \theta \}/m <1/m$, with $ \theta=3/4 $,  follows from Line~\ref{alg:AdaBoostSamplecheckingmargin}, ensuring this condition holds if $f$ in Line~\ref{alg:AdaBoostsample:output1} is the output. If $f$ is outputted in Line~\ref{alg:AdaBoostsample:output3}, it holds since $f$ is the output of a $\erm$-learner on the entire training sequence $S$, and therefore has a margin of $ 1 $ on all points.  
\end{proof}

We now move on to show \cref{lemmaAdaBoostSampleefficent}, i.e, that $ \cA $, with high probability, never runs $ \erm(S) $ and, in this case, is efficient to run. For convenience, we restate it here.  

\lemmaAdaBoostSampleefficent*

For the proof of this lemma, we need the following two lemmas.
The first lemma says that the output of $ \cA $, with high probability, is $ f $ from Line~\ref{alg:AdaBoostsample:output1} in Line~\ref{alg:AdaBoostSampleoutputgood}.

\begin{lemma}\label{adaboostsamplefewroundslemma}
  For hypothesis class $\cH$ of VC-dimension $d$, target concept $ c\in \cH $, training sequence 
   $S\in(\cX\times \cY)^{m}$ realizable by $c$ and of size $m\in \mathbb{N}$, failure parameter $ 0<\delta <1$,  random string $\rr \sim ([0:1]^{550d})^{n}$ of length $n\in\mathbb{N}$, boosting rounds $t=\left\lceil20^{2}\ln(m)/2\right\rceil$, as in Line~\ref{alg:AdaBoostSampletset}, then for  $n\geq 6\left\lceil20^{2}\ln{\left(8m/\delta \right)}/2\right\rceil$ we have, with probability at least $1-(\delta/(8m))^{20}$ over $ \rr $, that \cref{alg:AdaBoostSample} run on $S,\rr$, $\cA(S,\rr)$ outputs $f$ from Line~\ref{alg:AdaBoostsample:output1} in Line~\ref{alg:AdaBoostSampleoutputgood}.
  \end{lemma}

The next lemma shows that if $ f $ from Line~\ref{alg:AdaBoostsample:output1} in Line~\ref{alg:AdaBoostSampleoutputgood} is the output of $ \cA $, then $ \cA $ uses few operations.   

\begin{lemma}\label{adaboostsampleruntime}
  For hypothesis class $\cH$ of VC-dimension $d$, target concept $ c\in \cH $, training sequence 
  $S\in(\cX\times \cY)^{m}$ realizable by $c$, and of size $m\in\mathbf{N}$, and string $r\in([0:1]^{s})^{n}$ with $s,n\in\mathbb{N}$, then if the output  $f=\cA(S,r)$ of \cref{alg:AdaBoostSample} is $f$ from Line~\ref{alg:AdaBoostsample:output1} outputted in Line~\ref{alg:AdaBoostSampleoutputgood}, then $\cA(S,r)$ uses fewer operations than: 
\begin{align*}
  n\left(O\left(m+s\ln{\left(m \right)}\right)+\Utrain(s)+3m\Uinf\right).
\end{align*}  
\end{lemma}

With the above two lemmas, we now give the proof of \cref{lemmaAdaBoostSampleefficent}.

\begin{proof}[Proof of \cref{lemmaAdaBoostSampleefficent}]
     \cref{adaboostsamplefewroundslemma} give that $\cA(S,\rr)$  outputs $f$ from Line~\ref{alg:AdaBoostsample:output1} in Line~\ref{alg:AdaBoostSampleoutputgood} with probability at least $1-(\delta/(8m))^{20}$, since $ n\geq 6\lceil20^{2}\ln{(8m/\delta )}/2\rceil $ and $ t=\lceil 20^{2}\ln{(m )}/2\rceil $. For the above parameters and $ s=550d $, \cref{adaboostsampleruntime} implies that
    \begin{align*}
      n\left(O\left(m+s\ln{\left(m \right)}\right)+\Utrain(s)+3m\Uinf\right)= n\left(O\left(m+d\ln{\left(m \right)}\right)+\Utrain(550d)+3m\Uinf\right)
       \end{align*}
which concludes the proof
\end{proof}
We now, proceed to prove \cref{adaboostsamplefewroundslemma} and \cref{adaboostsampleruntime}, starting with the former. In the proof of \cref{adaboostsamplefewroundslemma}, we need the following lemma, whose proof can be found in \cref{appendixpropertiesadaboossample}. The lemma gives an in-sample margin guarantee of \as after $ t $ rounds of successful boosting. Since \as is a slightly modified version of AdaBoost, using early stopping and a fixed learning rate, we thought it was good practice to write out the margin error analysis for $ \cA $ (the steps follow \cite{boostingbookSchapireF12} closely).  

\begin{restatable}{lemma}{AdaBoostSampleMarginLemmathree}\label{AdaBoostSampleMarginLemma3}
Consider \cref{alg:AdaBoostSample} run on a training sequence $S\in\left(\cX\times\cY\right)^{m}$, string $r\in([0:1]^{*})^{*}$, Line~\ref{alg:AdaBoostSampletset} set to a $t\in\mathbb{N}$, Line~\ref{alg:AdaBoostSamplegammaset} set with a $0< \gamma< 1/2$, and Line~\ref{alg:AdaBoostSamplethetaset} set with a $0<\theta <1$ such that $\theta<2\gamma$. If $\sum_{j=1}^{n}\ind_{\alpha_j>0}\geq t$, we have that $f=\sum_{j=1}^{t} \frac{\alpha_{f,j}h_{f,j}}{\sum_{l=1}^{t}{\alpha_{f,l}}}=\sum_{j=1}^{t} h_{f,j}/t$, the function in \cref{alg:AdaBoostSample} Line~\ref{alg:AdaBoostsample:output1} for $ \alpha=(1/2)\ln{\left((1+2\gamma)/(1-2\gamma)\right)- (1/2)\ln{\left((1+\theta)/(1-\theta) \right)}}>0 $  satisfies,
\begin{align*}
  \sum_{x\in S} \ind\{\sum_{i=1}^{t} h_{f,i}(x)c(x)/t\leq \theta \}/m  =\p_{(\rx,\ry)\sim S}\left[\ry f(\rx)\leq \theta \right]\\\leq \big(\exp{((\theta-1)\alpha)}(1/2+\gamma)+\exp{((\theta+1)\alpha)}(1/2-\gamma)\big)^{t},
\end{align*}
which for $ \theta=3/4 $ and $ \gamma=9/20 $, (that satisfies $ \theta/2 =3/8 <\gamma=9/20 $) is at most $ (24/25)^{t} $.  
\end{restatable}

We also need the following lemma in the proof of \cref{adaboostsamplefewroundslemma}. The following lemma is the uniform convergence lemma for the realizable case of binary classification, which gives a bound on the difference between the in-sample and out-sample error of all consistent $h\in \cH$ simultaneously.

\begin{lemma}\label{uniformconvergence}[\cite{vapnik74theory}, \cite{Blumeruniformconvergence} from \cite{Simons}[Theorem 2]]
 For $0<\delta,\eps<1$, hypothesis class $\cH$ of VC-dimension $d$, target concept $c\in\cH$, distribution $\cD$ over $\cX$, and sample $\rS\sim\cD_{c}^{m}$,  we have with probability at least $1-\delta$ over $\rS$, that for all $h\in \cH$ with $\ls_{\rS}(h)=0$, it holds that
    \begin{align*}
    \ls_{\cD}(h)\leq 2\frac{d\log_{2}{\left(2em/d \right)}+\log_{2}{\left(2/\delta \right)}}{m}.
    \end{align*}
\end{lemma}

With the above two lemmas, we are now ready to prove \cref{adaboostsamplefewroundslemma}.

\begin{proof}[Proof of \cref{adaboostsamplefewroundslemma}]
    
Let $S\in (\cX\times \cY)^{m}$. We observe that $\as$ outputs $f$ from Line~\ref{alg:AdaBoostsample:output1} in Line~\ref{alg:AdaBoostSampleoutputgood} only if 
$\sum_{i=1}^{n} \ind_{\alphar_i>0} \geq t= \left\lceil 20^{2}\ln(m)/2 \right\rceil$ and 

\begin{align*}
    \sum_{x\in S} \ind\{\sum_{i=1}^{t} h_{f,i}(x)c(x)/t\leq \theta \}/m = \sum_{x\in S} \ind\{\sum_{i=1}^{t} h_{f,i}(x)c(x)/t\leq 3/4 \}/m < \frac{1}{m}.
\end{align*}

We now show that $\sum_{i=1}^{n} \ind_{\alphar_i>0} \geq \left\lceil20^{2}\ln(m)/2\right\rceil$ happens with probability at least $1-(\delta/(8m))^{20}$ over $\rr$, and that if this happens, it also holds that $\sum_{x\in S}  
                \ind\{\sum_{i=1}^{t} h_{f,i}(x)c(x)/t\leq 3/4\} < \frac{1}{m}$.

To see that $\sum_{x\in S} \ind\{\sum_{i=1}^{t} h_{f,i}(x)c(x)/t\leq 3/4 \}/m < 1/m$ holds when $\sum_{i=1}^{n} \ind_{\alphar_i>0} \geq \left\lceil 20^{2}\ln(m)/2 \right\rceil$, we invoke \cref{AdaBoostSampleMarginLemma3} with $t=\left\lceil 20^{2}\ln(m)/2 \right\rceil$,  $\gamma=9/20$ and $\theta=3/4$, and get by numerical calculations $ \ln{(24/25 )} 20^{2}/2\leq-8$, which implies that:
\begin{align*}
    \sum_{x\in S} \ind\{\sum_{i=1}^{t} h_{f,i}(x)c(x)/t\leq 3/4 \}/m =\p_{(\rx,\ry)\sim S}\left[\ry f(\rx)\leq \theta \right]\negmedspace\leq \negmedspace(24/25)^{t}\negmedspace=\exp(-8\ln{(m )})\negmedspace<\negmedspace1/m
\end{align*}  
as claim.

Thus, we now proceed to show that $\sum_{i=1}^{n} \ind_{\alphar_i>0} \geq t= \left\lceil 20^{2}\ln(m)/2 \right\rceil$ with probability at least $1-(\delta/(8m))^{20}$ over $\rr$. We first notice that this is a sum of $\{0,1\}$-random variables that are not independent or identically distributed, however $\alphar_i$ is a function of $\rr_i$ and $\rD_i$ through Line~\ref{alg:AdaBoostSample:sample}, where $\rD_i$ is only a function of $\rr_1,\ldots,\rr_{i-1}$. Now let $r_1,\ldots,r_{i-1}$ be a realization of $\rr_1,\ldots,\rr_{i-1}$, and let $D_i$ denote the realization of $\rD_i$. For this realization, $\rS_i$ is a training sequence of size $550d$ drawn according to $D_i$ (by the comment before \cref{alg:AdaBoostSample}), and since $\rhh_i$ is the output of $\erm(\rS_i)$, we have that $\ls_{\rS_i}(\rh_i)=0$, and that $\rhh_{i}\in \cH$. Thus, it follows from \cref{uniformconvergence} with $\delta= 2^{-d} $ and $m=550d=\cu d$ (i.e. $\cu=550$) that with probability at least $1-2^{-d}$ over $\rr_{i}$ we have that 
\begin{align*}
    \ls_{D_i}(\rS_i)\leq2\frac{d\log_{2}{\left(2e(\cu d)/d \right)}+\log_{2}{\left(2^{d+1}  \right)}}{\cu d}\leq 2\frac{\log_{2}{\left(2e \cu \right)}+2}{\cu }\leq 1/20,  
\end{align*}
which by Line~\ref{alg:AdaBoostsamplenonzeroweight} implies $\alphar_{i}>0$ i.e. we have shown that 
\begin{align*}
\e_{\rr_{i}}[\ind_{\alphar_i>0}]\geq 1-2^{-d}:=p.                          \end{align*} 
Furthermore, for any $t\leq0 $ and using that $\ln{\left(1+x \right)}\leq x$ for any $x>-1$ we have   
\begin{align*}
&\e_{\rr_{i}}[\exp{\left(t\ind_{\alphar_i>0} \right)}]\\
&\leq (1-\e_{\rr_{i}}[{\ind_{\alphar_i>0}}])+\e_{\rr_{i}}[\ind_{\alphar_i>0}]\exp(t)\\
&=1+\e_{\rr_{i}}[\ind_{\alphar_i>0}](\exp{\left(t \right)}-1)
\\ 
&\leq \exp(\e_{\rr_{i}}[\ind_{\alphar_i>0}](\exp{\left(t \right)}-1))
\leq 
\exp\left(p(\exp{\left(t \right)}-1)\right).  
\end{align*} 
Since we showed the above for any realization $r_1,\ldots,r_{i-1}$ of $\rr_1,\ldots,\rr_{i-1}$ and for any $i$, we get, by independence of the $\rr_{i}$'s, applying the above recursively, and using that $\alphar_{i-1}$ is only a function of $\rr_{1},\ldots,\rr_{i-1}$,  that 
 
\begin{align*}
 &\e_{\rr_{1},\ldots,\rr_{n}}\Big[{\exp{\Big(t\sum_{i=1}^{n}\ind_{\alphar_i>0} \Big)}}\Big]
 =\e_{\rr_{1},\ldots,\rr_{n-1}}\Big[{\exp{\Big(t\sum_{i=1}^{n-1}\ind_{\alphar_i>0} \Big)}\e_{\rr_{n}}\Big[{\exp{\Big(t\ind_{\alphar_n>0} \Big)}}\Big]}\Big]\\
\leq&\e_{\rr_{1},\ldots,\rr_{n-1}}\Big[{\exp{\Big(t\sum_{i=1}^{n-1}\ind_{\alphar_i>0} \Big)}\exp{\left(p\left(\exp{\left(t \right)}-1\right) \right)}}\Big]
 \leq \hdots
\leq \exp{\left(np\left(\exp{\left(t \right)}-1\right) \right)}.
\end{align*}
Setting $t=\ln{\left(1-\rho \right)}\leq 0$ for $0<\rho<1$, we conclude that 
\begin{align*}
\p_{\rr_{1},\ldots,\rr_{n}}\left[\sum_{i=1}^{n}\ind_{\alphar_i>0}< (1-\rho)np\right]\leq \exp{\left(np\left(\exp{\left(t \right)}-1\right)-t(1-\rho)np \right)}=\left(\frac{\exp{\left(-\rho \right)}}{(1-\rho)^{1-\rho}}\right)^{np}.
\end{align*}

Thus, setting $\rho=2/3$ so that $ (\exp(-3/4)/(1/3)^{1/3})^{1/2}\leq 83/100 $, and using that $d\geq 1$ so that $p=1-2^{-d}\geq 1/2$, we get that
\begin{align*}
\p_{\rr_{1},\ldots,\rr_{n}}\left[\sum_{i=1}^{n}\ind_{\alphar_i>0}< n/6\right]\leq \p_{\rr_{1},\ldots,\rr_{n}}\left[\sum_{i=1}^{n}\ind_{\alphar_i>0}< (1-\rho)np\right]\leq (\exp(-3/4)/(1/3)^{1/3})^{n/2}\\\leq (83/100)^{n}.
    \end{align*}
Using that $n\geq 6\left\lceil20^{2}\ln{(8m/\delta)}/2\right\rceil$ so that $\lceil20^{2}\ln{(m )/2}\rceil=t\leq n/6$ and that $(83/100)^{6\cdot20^{2}/2}\leq (1/e)^{20}$, we get that 
\begin{align*}
\p_{\rr_{1},\ldots,\rr_{n}}\left[\sum_{i=1}^{n}\ind_{\alphar_i>0}< \left\lceil 20^{2}\ln(m)/2 \right\rceil\right]\leq \p_{\rr_{1},\ldots,\rr_{n}}\left[\sum_{i=1}^{n}\ind_{\alphar_i>0}< n/6\right]
\leq \left(\frac{\delta}{8m}\right)^{ 20},
    \end{align*} 
which concludes the proof. 

\end{proof}

We now give the proof of \cref{adaboostsampleruntime}.

\begin{proof}[Proof of \cref{adaboostsampleruntime}]

We first set parameters (Line~\ref{alg:AdaBoostSamplemset} to Line~\ref{alg:AdaBoostSamplesettingl}). We first read the length of $S, r$, and $r_{1}$, which takes $O(m+ns)$ operations. Making $D_{1}$ and $C_{1}$ takes $O(m)$ operations. Calculating/setting $t,\theta,\gamma,\sum_{j=1}^{0}\ind_{\alpha_{i}>0}$ takes $O(1)$ operations. Initializing $f$ as an array of size $t$ takes $O(t)=O(\ln{(m)})$ operations. Thus, the lines from Line~\ref{alg:AdaBoostSamplemset} to Line~\ref{alg:AdaBoostSamplesettingl} take $O(m+ns)$ operations.

We next analyze the for loop over $n$ starting in Line~\ref{alg:AdaBoostSampleforloop} and ending in Line~\ref{alg:AdaBoostsampleendforloop}. In each iteration, we do the following: We find $S_i=C_{i}^{-1}(r_{i})$ using $r_{i}\in[0:1]^{s}$ and $C_{i}^{-1}$. We recall that for $r_{i,j}\in[0:1]$, $C_{i}^{-1}(r_{i,j})$ was defined as the index $l$ in $\{ 1:m \}$ such that $C_{i}(l-1)\leq r_{i,j}$  and $r_{i,j}<C_{i}(l) $. Since $C_{i}$ is sorted in increasing order in $ i $, allowing for binary search, $ l $ can be found in $ O(\ln{(m )}) $ reads from $ C_{i} $. That is given an interval $ I\subset \{ 1:m \} $ chose the middle point of $ I $, call it $ l' $, if $r_{i,j}$ is between $ C_{i}(l'-1) $ and $ C_{i}(l') $ set $ l=l'$, if $r_{i,j} <C_{i}(l'-1)$ then recurse on the interval $ I\cap\{ 1:l'-2 \} $ else recurse on $ I\cap\{ l': \} $. Since the interval is halved each time the index is not found and the interval starts which $ \{ 1:m \} $, this is at most $O( \ln{(m )}) $ reads/operations from $ C_{i} $. Since $C_{i}^{-1}(r_{i})$ is applying $ C_{i}^{-1} $ entry-wise to $ r_{i} $  this takes $O(s\ln{\left(m \right)})$ operations. Calculating $h_{i}$ takes $\Utrain(s)$. Calculating $\ls_{D_{i}}$ takes $m\cdot (\Uinf+O(1))$ operations. Checking $\eps_{i}\leq 1/2-\gamma$, setting $\alpha_{i}$ and updating the counter $\sum_{j=1}^{i}\ind_{\alpha_{j}>0}$ takes $O(1)$ operations. Checking the counter $\sum_{j=1}^{i}\ind_{\alpha_{j}>0}<t$ takes $O(1)$ operations. Renaming $\gamma_{i},\epsilon_{i},h_{i},\alpha_{i},D_{i}$ takes $O(1)$ operation, and adding $h_{f,l}$ to $f$ takes $O(1)$ operation ($ f $ is normalized in the last line with $ \alpha_{f,l} $, which is the same for all $ h_{f,l} $, so we only save $ h_{f,l} $ unscaled). Setting $\alpha_{i}=0$ and updating the counter $\sum_{j=1}^{i}\ind_{\alpha_{j}>0}$ in the else statement takes $O(1)$ operation. To compute $D_{i+1}$ in Line~\ref{alg:AdaBoostSamplegetweights} we preform inference using $h_{i}$ on all the $m$ points, which takes $m\cdot(\Uinf+O(1))$ operations. Calculating $Z_{i}$, $D_{i+1}$, and $C_{i+1}$ afterwards requires $O(m)$ operations. Thus, each iteration of the for loop, starting from  Line~\ref{alg:AdaBoostSampleforloop} and ending at Line~\ref{alg:AdaBoostsampleendforloop} uses 
\begin{align*}
    O(s\ln{\left(m \right)})+\Utrain(s)+m\cdot (2\Uinf+O(1))+O(m)
\end{align*}
operations. Therefore, the total cost over the entire for loop is
\begin{align*}
    n(O(s\ln{\left(m \right)}+m)+\Utrain(s)+2m\cdot \Uinf).
\end{align*}

Next, we analyze Line~\ref{alg:AdaBoostSamplecheckt} to Line~\ref{alg:AdaBoostSampleoutputgood}. First, Line~\ref{alg:AdaBoostSamplecheckt}, checking/reading $\sum_{j=1}^{n}\ind_{\alpha_{i}>0}$ takes $O(1)$. Renaming $f$ takes $O(t)=O(\ln{(m)})$ operations, as we have $t$ functions. Checking that all points has $ \theta $-margin takes $ tm (\Uinf+2)\leq nm(\Uinf+2) $, as $ t\leq n $ in the case that we reach this condition (else $\sum_{j=1}^{n}\ind_{\alpha_{i}>0}\geq t$ could not have happened). Thus, Line~\ref{alg:AdaBoostSamplecheckt} to Line~\ref{alg:AdaBoostSampleoutputgood}, takes at most $nm(\Uinf+2)+O(\ln{\left(m \right)})$ operations are required.

Consequently, we conclude that the above steps take at most:
\begin{align*}
  O(m+ns)+n(O(s\ln{\left(m \right)}+m)+\Utrain(s)+3m\cdot \Uinf)+O(\ln{(m)})\\
  =n(O\left(m+s\ln{\left(m \right)}\right)+\Utrain(s)+3m\Uinf),
\end{align*}
operations, which concludes the proof. 
\end{proof}