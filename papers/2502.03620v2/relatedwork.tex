\section{Previous work and detailed proof sketch}
\label{sec:proofoverview}

In this section, we give a detailed proof sketch and describe how our result compares to previous optimal PAC learners. To this end, we describe the works \cite{hannekeoptimal}, \cite{Optimalweaktostronglearning}, and \cite{baggingoptimalPAClearner}. We will write random variables  $ \rx $ with boldface letters, and non-random variables and realizations of random variables as $ x $. 
\subsection{Previous work} 
\paragraph{Approach of \cite{hannekeoptimal}:}
To understand the above-mentioned works, we start with the work \cite{hannekeoptimal}, which the two other works use ideas from. For simplicity, we assume in this paragraph that we have a training sequence $ S $ with size some power of $4$. Furthermore, given training sequences $S,T\in (\cX \times \{-1,1\})^{*}$ we let $ S\sqcup T $ be the concatenation of the two training sequences $ S $ and $ T $, i.e.   $ [S^{T},T^{T}]^{T} $ (multiplicities of examples are important). In the following we will always assume that the training sequences are realizable by the target concept $ c.$  With this in place we now describe the deterministic subsampling scheme by \cite{hannekeoptimal}. 
\begin{algorithm}[H]
  \caption{$\cS'(S,T)$}\label{alg:Subsamplehanneke}
  \begin{algorithmic}[1]
    \State \textbf{Input:} Training sequences $S ,T\in (\cX\times \{-1,1\})^{*}$ 
    \State \textbf{Output:}{ A collection of training sequences}
      \State \textbf{if $ |S|\leq 3 $ then}
      \State\hspace{0.5cm}\textbf{return $ S\sqcup T $ }
      \State Split $ S $ into  $S_0,S_{1},S_{2},S_{3}$  where $ S_i $ contains the examples from $i|S|/4+1$ to $ (i+1)|S|/4$
      \State \textbf{return}$[\cS'\left(S_0 , S_2 \sqcup S_3 \sqcup T\right) , \cS'\left(S_0 , S_1 \sqcup S_3 \sqcup T\right) , \cS'\left(S_0 , S_1 \sqcup S_2 \sqcup T\right)]$
  \end{algorithmic} 
\end{algorithm}
\vspace{-0.3cm}
The above algorithm, given a training sequence $S$ of size $m=4^{k}$, generates $3^{\log_{4}(m)}\approx m^{0.79}$ training sequences, which have some non-overlapping parts, as it leaves out a fourth of the training examples in each recursive call. Leaving out training examples will be key later. Using this algorithm, 
\citeauthor{hannekeoptimal} produces his optimal PAC learner as the $ \sign(\cdot) $ of the majority voter, consisting of the output of the $\erm$-algorithm run on the sub training sequences of $S$ that $\cS'(S,T)$ returns for $ T=\emptyset $, i.e. $ \sign $ of  $\sum_{S'\in \cS'(S,T)} \erm(S)/|\cS'(S,T)|$. However, for the analysis, \citeauthor{hannekeoptimal} considers the above majority vote for general $ T\in(\cX\times\{-1,1\})^{*} $,  which we will denote $\erm(S,T)=\sum_{S'\in \cS'(S,T)} \erm(S)/|\cS'(S,T)|$ from now on. One of the key insights of \citeauthor{hannekeoptimal} is that by the recursive structure of the sub training sequences in $\cS'(S,T)$, $ \erm(S,T) $ can be written as
\begin{align*}
    \erm(S,T)=\frac{\erm(S_{0},S_{1}\sqcup S_{2}\sqcup T)}{3|\cS'(S_{0},S_{1}\sqcup S_{2}\sqcup T)|}+
\frac{\erm(S_{0},S_{1}\sqcup S_{2}\sqcup T)}{3|\cS'(S_{0},S_{1}\sqcup S_{3}\sqcup T)|}+
\frac{\erm(S_{0},S_{1}\sqcup S_{2}\sqcup T)}{3|\cS'(S_{0},S_{2}\sqcup S_{3}\sqcup T)|},
\end{align*}
where we in the following will write $\erm(3,T)$ for $\erm(S_{0},S_{1}\sqcup S_{2}\sqcup T)$, i.e., the $\erm$ trained on the sub training sequences where $S_{3}$ is left out, and likewise $\erm(2,T)$ and $\erm(1,T)$. 

Now, for the majority vote $\erm(\cS'(S,T))$ to fail on a point $x\in\cX$, it must be the case that half of its voters are incorrect, i.e., one of the three recursive calls $\erm(i,T)$ on the right-hand side of the above equation also fails on this point. Furthermore, there is still at least $1/2-1/3$ of the remaining voters in $\erm(\cS'(S,T))$ not in $\erm(i,T)$ that also fail, which corresponds to $1/4$ of the voters in $\erm(j,T)$ and $\erm(j',T)$ for $ j\not= j' $ and $ j,j'\not=i.$ Thus, if we pick uniformly at random $\ri\in\left\{ 1,2,3 \right\}$ and $\rh$ uniformly at random from $\erm(j,T)$ and $\erm(j',T)$ for $j'\not=j$ and $j',j\not=\ri$, we have that the chosen $\erm(\ri,T)$ and $\rh$ both fails on a point $x$ where $\erm(S,T)$ fails with probability at least $1/12$ over $\ri$ and $\rh$. Therefore, \citeauthor{hannekeoptimal} concludes that 
$ \p_{\rS\sim\cD_c^m}[\ls_{\rx\sim\cD_{c}}(\erm(\rS,T))\geq C(d+\ln{\left(1/\delta \right)})/m]$ is upper bounded by $\p_{\rS\sim\cD_c^m}[12\p_{\rx\sim\cD,\ri,\rh}[\erm(\ri,T)(\rx)\not=c(\rx),\rh(\rx)\not=c(\rx)]\geq C(d+\ln{\left(1/\delta \right)})/m],
$ where $C>1$ is some constant. By this relation, \citeauthor{hannekeoptimal} further argues that it is sufficient to show that $
    \max_{i\in\left\{ 1,2,3 \right\}}\max_{h\in \tiny\erm(\rS,T)\backslash\erm(i,T)}   12\p_{\rx\sim\cD}[\erm(i,T)(\rx)\not=c(\rx),h(\rx)=c(\rx)]$ is less than $ C(d+\ln{(1/\delta )})/m $
 with probability at least $ 1-\delta $ over $\rS$, which would follow by showing for each $i\in\left\{ 1,2,3 \right\}$ that 
\begin{align}\label{eq:intro1}
  \vspace{-0.6cm}
\max_{h\in \erm(\rS,T)\backslash\erm(i,T)} \negmedspace \negmedspace \negmedspace \negmedspace   12\p_{\rx\sim\cD}\left[\erm(i,T)(\rx)\not= c(\rx),h(\rx)\not=c(\rx)\right]\leq C(d+\ln{\left(1/\delta \right)})/m 
\end{align}
\vspace{-0.4cm}\newline holds with probability at least $1-\delta/3$, and then applying a union bound. 
To argue why this last statement is true \citeauthor{hannekeoptimal} continues by induction over the size of $\rS$ for $m=4^{k}$ where $ k\in\mathbb{N} $. The induction base $k=1$ follows by the right-hand side of the above for $m=|\rS|=4$ being larger than $1$. Now, for the induction step, let us consider the case $i=1$, which can be done without loss of generality since $\rS_{0},\rS_{1},\rS_{2},\rS_{3}$ are i.i.d.. \citeauthor{hannekeoptimal}, now uses the fact that, the induction base applied to $\erm(i,T)$ (which recurses on $\rS_{0}$ of size $4^{k-1}$) implies that,
\begin{align}\label{eq:intro2}
 \p_{\rx\sim\cD}\left[\erm(1,T)(\rx)\not=c(\rx)\right]\leq 4C(d+\ln{\left(9/\delta \right)})/m \leq 4\ln{\left(9e \right)}C(d+\ln{\left(1/\delta \right)})/m, 
\end{align}
holds with probability at least $1-\delta/9$ over $\rS_{0},\rS_{2},\rS_{3}$.
Furthermore, notice that if $\rS_{0},\rS_{2},\rS_{3}$ are such that $
    \p_{\rx\sim\cD}\left[\erm(1,T)(\rx)\not=c(\rx)\right]\leq C(d+\ln{\left(1/\delta \right)})/m $  then by the monotonicity of measures, we have
    $\max_{h\in \erm(2,T)\sqcup\erm(3,T)}    12\p_{\rx\sim\cD}\left[\erm(1,T)(\rx)\not= c(\rx),h(\rx)\not=c(\rx)\right] \leq C(d+\ln{\left(1/\delta \right)})/m.$ 
Thus, the problem reduces to analyzing the outcomes of $\rS_{0},\rS_{2},\rS_{3}$ such that \cref{eq:intro2} holds and is also lower bounded by $ C(d+\ln{\left(1/\delta \right)})/m .$ 
\citeauthor{hannekeoptimal} now uses the law of total probability to conclude that for such outcomes $ S_0,S_2,S_3 $, it holds for any $h\in \erm(2,T)\sqcup\erm(3,T)$, that the error $12\p_{\rx\sim\cD}[\erm(1,T)(\rx)\not= c(\rx),h(\rx)=c(\rx)]$ is less than 
\begin{align}\label{eq:intro4}
48\ln{\left(9e \right)}C \p_{\rx\sim\cD(\cdot \mid \erm(1,T)(\rx)\not=c(\rx))}\left[h(\rx)\not=c(\rx)\right] (d+ \ln{\left(1/\delta \right)}) /m.
    \end{align}
Thus, it suffices to show a uniform error bound, of $1/(48\ln{\left(9e \right)})$, under the conditional distribution, given that $\erm(1,T)$ errs, for all $h\in \erm(2,T)\sqcup\erm(3,T)$.

Now, assuming that \cref{eq:intro2} is lower bounded by $ C(d+\ln{\left(1/\delta \right)})/m $  and using the fact that $\rS_{1}$ contains, in expectation,
$\p_{\rx\sim\cD}[\erm(1,T)(\rx)\not=c(\rx)]m/4$, points from $\{x: \erm(1,T)(x)\not=c(x) \}$ (which, by the lower bounds is  greater than $C\ln{( e/\delta )}/4$) it follows by the Chernoff bound and for $C>64$ that $\rN_{1}:=|\rS_{1}\sqcap \{x: \erm(1,T)(x)\not=c(x) \}|\geq C(d+ \ln{(1/\delta )} )/8$ with probability at least 
$1-\exp(\p_{\rx\sim\cD}[\erm(1,T)(\rx)\not=c(\rx)]m/4^2)\geq 1-(\delta/e)^{4}\geq 1-\delta/9$  over $\rS_{1}$. 

Conditioned on this event, and since every $h\in \erm(2,T)\sqcup\erm(3,T)$ is a $ \erm $ trained on training sequence which contains $\rS_{1}$, it is consistent with $ \rS_{1}$, especially with the points in $\rS_{1}\sqcap \{x: \erm(1,T)(x)\not=c(x) \}$. The $ \erm $-bound (\cref{uniformconvergencemain}) on the points $\rS_{1}\sqcap \{x: \erm(1,T)(x)\not=c(x) \}$ (which is drawn according to the conditional distribution $\rx\sim\cD(\mid \erm(1,T)(\rx)\not=c(\rx)))
$ then gives, with probability at least $1-\delta/9$, for all $h\in \erm(2,T)\sqcup\erm(3,T)$ simultaneously, we have
$\p_{\rx\sim\cD(\cdot \mid \erm(1,T)(\rx)\not=c(\rx))}\left[h(\rx)\not=c(\rx)\right]\leq 2(d\ln{\left(2e \rN_{1}/d \right)}+\ln{\left(18/\delta \right)})/(\ln{\left(2 \right)}\rN_{1}).$ Since we had a lower bound of $\rN_{1}\geq C(d+ \ln{\left(1/\delta \right)} )/8$ and this is a decreasing function in $\rN_{1}$, we see that for $C$ sufficiently large, this is less than $1/(48\ln{\left(9e \right)})$. Therefore, by a union bound over the three above events (each of which occurs with probability at least $1-\delta/9$), \cref{eq:intro1} holds with probability at least $1-\delta/3$ as claimed. 

We are now ready to explain one of our key insights leading to \cref{introductionmaintheorem}. Using \cref{uniformconvergencemain}, one can see that a $ \erm $ trained on $ \Theta(d) $ training examples has a small constant error with probability at least $ 1-\exp(-d) $. This observation has also been used, for instance, in \cite{samplecompressionschemesamirmoran} to show existence of sample compression schemes for VC-classes, independent of the sample size. At first glance, it might seem like training on $ \Theta(d) $ examples would work straight out of the box with the above argument, as the argument required a uniform error bound on $h\in \erm(2,T)\cup \erm(3,T) $ of some small constant error. However, the above uniform error bound must hold under the conditional distribution $ \cD(\cdot \mid \erm(1,T)(\rx)\not=c(\rx))$, so a training sequence of $\Theta(d) $ examples would by the above argument only contain $ \Omega(d(d+\ln{\left(1/\delta \right)})/m) $ examples from this distribution so not necessarily any or enough to guarantee a small constant error. 

However, as we noted, using $ \Theta(d) $ training examples, the $ \erm $-learner can achieve a small error under the distribution $ \cD $, from which the training examples are drawn from, with probability at least $ 1- \exp{\left(-d \right)}  $. Furthermore, since we know the labels on the training sequence $ S $ we can for distributions $\cD$ over $S$ always check if the error is small. Thus, we can with this observation employ boosting with resampling (see, e.g., \cite{boostingbookSchapireF12}[Section 3.4]) to learn a voting classifier that is correct on the entire sample $ S $  using only calls of size $ \Theta(d) $ to the $ \erm $-learner. In fact, a voting classifier can even be found with good margins on $ S $, where a voting classifier $ f =\sum_{h\in\cH}\alpha_{h}h$ (with $ \alpha_{h} $ summing to 1) is said to have margin $ \gamma $ on $ S $ if for every $ (x,y)\in S $,  $f(x)y\geq \gamma  $. We now present the following work by \cite{Optimalweaktostronglearning}, which indicates how boosting might help improve training complexity.
\vspace{-0.3cm}
\paragraph{Approach of \cite{Optimalweaktostronglearning}:}
\cite{Optimalweaktostronglearning} used the fact that given a $\gamma$-weak learner $ \cW $, i.e., for all training sequences $\rS\sim \cD_{c}^{m}$ and any distribution $D$ over the examples in $\rS$, the $ \gamma $-weak learner $\cW$, given a sample of size $m_{0}$ from $D$, outputs a hypothesis $h$ from some base class $\cH$ with VC-dimension $d$ such that $\ls_{D_{c}}(h)\leq 1/2-\gamma$ with probability at least $1-\delta_{0}$ for $\gamma\leq 1/2$ and $\delta_{0}< 1$ - one can run a boosting algorithm $\cG$ that, given query access to $\cW$, returns a voting classifier $\cG(\rS)=\sum_{h\in \cH} \alpha_{h}h$ which has margins $\cG(\rS)(\rx)\ry=\Omega(\gamma)$ for all $(\rx,\ry)\in\rS$. 

The key insight of \citeauthor{Optimalweaktostronglearning} is that redoing the analysis of \cite{hannekeoptimal} with $\sign(\cG(\rS'))$, run on the training sequences in $\rS'\in \cS'(\rS)$ until the point \cref{eq:intro4}, uses no fact about which learning algorithm the training sequences in $\cS'(\rS)$ are trained on. Therefore, showing the claim of \citeauthor{Optimalweaktostronglearning}, similarly boils down to showing a uniform error bound for $h\in \sign(\cG(2,T))\sqcup\sign(\cG(3,T))$ under the conditional distribution $ \cD_{c}(\cdot\mid \cG(1,T)\not=c(\rx)) $ of  $1/(48\ln{\left(9e \right)})$ to get their target error of $C(d\gamma^{-2}+\ln{\left(1/\delta \right)})/m$. By a similar argument to that of \cite{hannekeoptimal}, \citeauthor{Optimalweaktostronglearning} concludes that $\rS_{1}$ contains $C(d\gamma^{-2}+\ln{\left(1/\delta \right)})/8$ points on which $\sign(\cG(1,T))$ fails with probability at least $ 1-\delta/9 $. Thus, reducing the problem to showing a uniform bound for the error of $ \sign(\cdot) $ of voting classifiers with margin $\Omega(\gamma)$ on an i.i.d. training sequence of size $C(d\gamma^{-2}+\ln{\left(1/\delta \right)})/8$, of at most $1/(48\ln{\left(9e \right)})$ with probability at least $1-\delta/9$. To this end, \citeauthor{Optimalweaktostronglearning}[See Theorem 4] show a uniform error bound that, informally, says with probability $1-\delta$, all voting classifier $ g $  with $\Omega(\gamma)$ margin on a training sequence $|\rS|\geq C(d\gamma^{-2}+\ln{\left(1/\delta \right)})/8$ has at most 
 $ 1/200 $ error, which is sufficient to complete their analysis. 

Thus, using our observation about $ \erm $ from before this paragraph, it follows that it can be plugged in as, for instance, a $ 1/4 $-weak learner with $ m_{0}=\Theta(d) $ and $ \delta_{0}= \exp{\left(-\Theta(d) \right)}  $  in the above algorithm, with a boosting algorithm like AdaBoost \cite{Adaboost}, run until the output of AdaBoost is consistent with the sub training sequence, i.e., $ \Theta(\ln{\left(m \right)}) $ rounds. Following the above method by \citeauthor{Optimalweaktostronglearning}, assuming the $ \erm $-algorithm never fails and running AdaBoost on all sub training sequences in $\cS'(S,\emptyset) $, i.e., $ \Omega(m^{0.79}) $ times, and assuming that AdaBoost is run for $ \Theta(\ln(m)) $ rounds and in each round update the distribution over $ \rS $ by evaluating the just-received hypothesis on the whole $ \rS $, this gives a training complexity of at least $ \Omega(m^{0.79}\ln{\left(m \right)})(\Utrain(\Theta(d))+3m\Uinf) $ and an inference complexity of $\Omega( m^{0.79}\ln{(m )} \Uinf)$, since it has to query $ \Omega(m^{0.79})$ many voting classifiers consisting of $ \Omega(\ln{(m )}) $ hypothesis. We want to stress that we do not claim that the above approach attains this lower bound or that one could not do something different with the above result by \citeauthor{Optimalweaktostronglearning} which would be more efficient. With that being said we notice that compared to \cite{hannekeoptimal}  the training complexity term $ \Utrain(\cdot) $ is now dependent on $ d $ not $ m $, however, there is an $ \Omega(m^{1.79}) $ term showing up in the training complexity and the inference complexity have become worse. Thus, the above indicates that boosting is helping to remove the $ m $ dependency in $ \Utrain(\cdot) $. We now move on to describe the work of \cite{baggingoptimalPAClearner}.

\paragraph{Approach of \cite{baggingoptimalPAClearner}:}
The analysis of \cite{baggingoptimalPAClearner} consists of two key steps, which we will briefly describe. To this end let $(\rS,\rB_{i})$ denote a sample with replacement from $\rS$ of size $ m'\in[0.02m,m]$. \citeauthor{baggingoptimalPAClearner} then outputs the $ \sign(\cdot) $ of the majority voter $\cB(\rS)=\sum_{i=1}^{n'} \erm((\rS,\rB_{i}))/n'$ for $n'=\Theta(\ln{\left(m/\delta \right)})$. Let $\binom{\rS}{m'}$ denote all possible sequences with replacement from $\rS$ of size $m'$.  

The first step of the analysis in \cite{baggingoptimalPAClearner} is to "derandomize" the bagging step, which \citeauthor{baggingoptimalPAClearner} accomplish by relating the error of the majority voter $\cB(\rS)$ to the purely analytical structure $\bar{\cB}(\rS)$ which is defined as $\sum_{\rS'} \ind\{\rS'\in \binom{\rS}{m'}\}\erm(\rS')/|\tbinom{\rS}{m'}|$. To this end \citeauthor{baggingoptimalPAClearner} observe that $ (\rS,\rB_{i}) $ can be viewed as a drawn with replacement from $\tbinom{\rS}{m'}$, which implies that $ \cB(\cS) $ can be seen as the majority vote of hypotheses drawn with replacement from the voters in $ \bar{\cB} $. To the end of using this observation, \citeauthor{baggingoptimalPAClearner} defines the event $E=\{ x\in \cX: \bar{\cB}(x)c(x)/|\binom{\rS}{m'}|\geq 1/3\}$ and bounds the error of $\cB(\rS)  $ by the following two terms using the law of total probability:
\begin{align}\label{eq:splittingerror}
  \vspace{-0.5cm}
 \ls_{\rx\sim\cD_{c}}[\cB(\rS) ] \leq \p_{\rx\sim\cD}[\bar{E}]+\ls_{\rx\sim\cD_{c}(\cdot \mid E)}(\cB(\rS)).
\end{align}
\vspace{-0.5cm}\newline
Now, using the observation that the voters in $ \cB $ can be viewed as drawn with replacement from the voters of $ \bar{\cB}(\rS) $, \citeauthor{baggingoptimalPAClearner} concludes that for any realization $S$ of $\rS$ and $x\in E$ the expectation of $c(x)\cB(S)$ is at least $1/3$. By applying Hoeffding's inequality and using $ n'=\Theta(\ln{(m/\delta )})$, it follows that at least half of the bootstrap samples are correct on $x$ with probability at least $1-\delta/(4m)$ over the bootstrap sample. An application of Markov's inequality over $\p_{\cB}[\ls_{\rx\sim\cD_{c}(\cdot|E)}(\cB(S))\geq 1/m]$, shows that with probability at least $1-\delta/4$ over the bagging step, the error is at most $1/m$. Since this holds for any realization $S$ of $\rS$ and the bagging step and $\rS$ are independent, this also holds for random $ \rS $, bounding the second term in \cref{eq:splittingerror} by $ 1/m $ with probability at least $ 1-\delta/4 $. 

The second step of \citeauthor{baggingoptimalPAClearner} is to upper bound the probability of $\bar{E}$, i.e., the margin error $\{ x\in \cX: \bar{\cB}(x)c(x)< 1/3\}$ of the classifier $\bar{\cB}$. This part of the analysis is highly non-trivial, and we will give only a high-level overview here. One of the key observations by \citeauthor{baggingoptimalPAClearner} in the second step, is that the training sequences in $ \binom{\rS}{m'} $ can be seen as being created recursively by a splitting algorithm. The splitting algorithm first splits the training sequence $\rS$ into $20$ disjoint training sequences followed by $19$ recursively calls to the same splitting algorithm. 
One reason for the large number of buckets is to ensure that $\bar{\cB}$ can be shown to have good margins- an idea we will use later. Furthermore, using the viewpoint of the training sequences in $ \binom{\rS}{m'} $ being created recursively \citeauthor{baggingoptimalPAClearner} gets some structure similar to \cite{hanneke2016refined}, with nonoverlapping training sequences. Similar to the step below \cref{eq:intro4} in the above overview of \cite{hannekeoptimal}, \citeauthor{baggingoptimalPAClearner} also uses at some point of the analysis that the training sequences are sufficiently nonoverlapping to see $\Theta(d+\ln{\left(1/\delta \right)})$ training examples under some conditional distribution and then calling \cref{uniformconvergencemain} for $\erm$'s trained on these $\Theta(d+\ln{\left(1/\delta \right)})$ training examples to get a uniform error bound of some sufficiently small constant. 

From the above, we deduce that the training complexity would be $\Omega(\ln{\left(m/\delta \right)})\Utrain(O(0.02m))$, since $\Theta(\ln{\left(m/\delta \right)})$ bagging training sequences are created, and a $\erm$ is trained on each of them. Inference on a new point has computational complexity $O(\ln{\left(m/\delta \right)})\Uinf$, as it requires querying all trained $ \erm $'s and taking a majority vote of their answers.

\citeauthor{baggingoptimalPAClearner} also says that combining Bagging and Boosting yields an optimal weak to strong learner. By combining this with our observation that the $ \erm $ learner provides a $ 1/4 $-weak learner, and omitting the fail probability, making $ \Theta(\ln{(m/\delta )} )$ bootstrap sample, training AdaBoost on them  $ \Theta(\ln{(m )}) $ rounds, and assuming that the distribution used by AdaBoost over the sample $ S $ in training, is updated each round by querying the just received hypothesis on the whole $ S $, this approach would yield a training complexity of at least $\Omega(\ln{\left(m/\delta \right)}\ln{\left( m\right)})(\Utrain(O(d))+3m\Uinf)$ and an inference complexity of $\Omega(\ln{\left(m/\delta \right)}\ln{\left(m \right)})\Uinf$. Where we again want to stress that we are not claiming that these lower bounds actually could be attained with the above approach or that one could not do something better with \citeauthor{baggingoptimalPAClearner} Bagging + Boosting.
With that being said we notice the above mirrors the situation in \cite{Optimalweaktostronglearning}, which also indicated that the dependency on \( \Utrain(m) \) could be reduced to \( \Utrain(d) \), but also indicated that it would come at a cost of an increase in inference complexity due to the necessity of querying all voters for each majority vote when doing inference.

With the above-related work explained, we now present our approach.
\vspace{-0.3cm}

\subsection{Detailed proof sketch}

Our algorithm will, as a subroutine, run an algorithm that we name $ \as $ \cref{alg:AdaBoostSample} and denote $ \cA. $ To describe $ \cA $, we let $  \rr=(\rr_{1},\ldots,\rr_{\Theta(\ln{(m/\delta )})}) \sim([0:1]^{550d})^{\Theta(\ln{(m/\delta )})} $ denote a random string, where the $ \rr_{i} $'s are i.i.d. sequences of length $ 550d $, and the $ \rr_{i,j} $'s are i.i.d. and uniformly distributed on the interval $ 0 $ to $ 1 $. The algorithm $ \cA$ on input training sequence $S$ of size $m$, random string $ \rr \sim([0:1]^{550d})^{\Theta(\ln{(m/\delta )})}$, and query access to $\erm$, is informally described AdaBoost run with a fixed learning rate, and early stopping ensuring that $ \cA $  has a specific number of voters, $t= \Theta(\ln{\left(m \right)}) $, in its majority vote $\cA(S)= \sum_{i=1}^{t} h_i/t $. Furthermore, if $ \cA $ has not reached its early stopping criteria after $ n=\Theta(\ln(m/\delta)) $ calls to the weak learner/$ \erm,$  $ \cA $  terminates by outputting $ \erm(S) $ (we will also see this as a majority vote of $ t $ copies of $ \erm(S) $). We show that the latter happens with probability at most $O(\delta/m)$. 

Furthermore, we show that $ \cA $ guarantees that $\ls_{S}^{3/4}(\cA(S)):= \sum_{x\in S} \ind\{\cA(S)(x)c(x)\leq 3/4 \}/m <1/m $, meaning all the $m$ points in $ S $  has $ 3/4 $  margins. This will be used later in the proof to obtain a uniform error bound, similarly as in the other proofs -- however will also play a key role in getting our inference complexity that does not suffer from a blow-up due to the boosting step as indicated by the sketched lower bounds in the previous paragraphs.

The reason why we can guarantee that the outputted majority voter $ \cA(S) $ satisfies
$ \ls_{S}^{3/4}(\cA(S)) <1/m $ is due to \cref{AdaBoostSampleMarginLemma3}, which says that if $ \cA $ in each boosting call, upon querying $ \erm $  with a sample from $ D_{i} $ over $ S $, receives a hypothesis $ h_{i} $ from $ \erm $ such that  $ \ls_{D_{i}}(h_{i})\leq 1/2-\gamma  $ for $ \gamma=9/20 $,  we have that after $ t $ such rounds the in sample 3/4-margin loss is  $\p_{(\rx,\ry)\sim S}\left[\ry \sum_{i=1}^{t} h_{i}(\rx)\leq 3/4 \right]\leq (24/25)^{t}$. Now as we use a $ \erm $ as a weak learner, we can make $ \gamma <1/2$  arbitrarily close to $ 1/2 $  with enough samples from $ D_{i} $, except with a failure probability by \cref{uniformconvergencemain}. For the purpose of showing $ \ls_{S}^{3/4}(S)<1/{m} $ we will end up giving the $ \erm $ a sample of size $ 550d $, which will allow us to set $ \gamma=9/20 $ and since $t= \Theta(\ln{\left(m\right)}) $ the bound $ \ls_{S}^{3/4}(\cA(S))<1/m $ follows if the $ \erm $ do not fail. 

We now resolve the case of the $ \erm $ failing to get a smaller loss than $ 1/2-\gamma $. We remark that since we know $ D_{i} $ over $ S $ and $ S $, we can always check whether a hypothesis returned by $ \erm $ succeeds or not, and if it fails, we can skip that hypothesis and query $ \erm $ again with a new sample from $ D_{i}$. Thus, for the above argument to go through, we just have to ensure that $ \erm $ succeeds $ t $  times with probability at least $ 1-O(\delta/m) $. Since a boosting round can fail, we must do more than $ t $ rounds. Thus, we 
run a for-loop of size $n= \Theta(\ln{\left(m/\delta\right)} )$, and try to train a hypothesis for boosting in each iteration.  We will have that each call succeeds with probability at least $1-\delta_{0}=1-2^{-d}$, so the expected number of success is at least $(1-2^{-d})\Theta(\ln{\left(m/\delta \right)} )$. By the multiplicative Chernoff bound, the probability of seeing fewer than $(1-2^{-d})\Theta(\ln{\left(m/\delta\right)} )/2 \geq t$ success, is at most $ \exp((1-2^{-d})\Theta(\ln{\left(m/\delta \right)} )/8)=O(\delta/m)$. In the above argument we are omitting dependencies of the success which is defined in terms of the distributions $ D_1,\ldots $  that $ \cA $  is updating iteratively. In \cref{adaboostsamplefewroundslemma}, we take this into account, however as the entries of $ \rr $ are i.i.d. the argument will be close to the above. 

For the training complexity of our algorithm, we need the training complexity of $\cA$, as it runs $ \cA $ as a sub-routine. As stated above, with probability at least $1-O((\delta/m))$ $ \cA $ runs a for loop of size $ n $ and outputs a majority vote $\cA(S)= \sum_{i=1}^{t} h_i/t $ with $ 3/4 $-margins on $ S.$ We show in \cref{adaboostsampleruntime} that each iteration of the for loop takes $ O(m+d\ln{\left(m \right)})+\Utrain(d)+3m\Uinf $ operations. The $ O(m)+3m\Uinf$ arises from updating the distribution $ D_{i} $, leading to evaluating the just-received hypothesis $ h_i $  on all $ m $ examples in $ S.$ The $O(d\ln{\left(m \right)} ) $ term comes from sampling $ 550d $ times from the distribution $D_{i}$, where each sample from $ D_{i} $  requires $ \ln{\left(m \right)} $ operations to generate, as a binary search over the buckets of the cumulative distribution function of $ D_{i} $ is made, to find which bucket the uniform random variable $ \rr_{j,l} {\sim} [0,1]$ generating the sample landed in. The $ \Utrain(550d) $ term comes from the $ \erm $-algorithm training on the generated sample from $ D_{i} $ of size $ 550d $. Thus, as the for loop has size $\Theta(\ln{\left(m/\delta \right)} ) $,  this leads to a training complexity of $ O(\ln{\left(m/\delta \right)} )(O(m+d\ln{\left(m \right)})+\Utrain(d)+3m\Uinf ) $ with probability at least $ 1-O(\delta/m)$. With the properties of $ \cA $ introduced, we now present the splitting algorithm inspired by \cite{hannekeoptimal}, which we use in our algorithm. We denote this algorithm as $ \cS $.
\input{subsamplingalgorithm.tex}
We notice differences from \cite{hannekeoptimal} in that the recursive calls only overlap in $ S_{0} $, the number of splits being $ 6 $ instead of $ 4 $,  and the recursions being $ 5 $ instead of $ 3 $. The reason for the former is that we found it easier notation-wise to let $ \cS(i,T)=\cS(S_{0},S_{i}\sqcup T) $. The reason for the latter is more interesting and follows the same reasoning as \cite{baggingoptimalPAClearner} to achieve good margins. However, we do more than just obtain good margins for the majority vote of the majority voters, as done in \cite{baggingoptimalPAClearner} with bagging and boosting. This "more" is our key observation to prevent our inference complexity from blowing up due to the boosting step. Now let $ \cA(\cS(S,T)) =[\cA(S')]_{S'\in \cS(S,T)} $ be the family of hypothesis outputted by $ \cA $ when run on each sub training sequence of $ \cS(S,T) $. What we then show is that with probability at least $ 1-\delta $ over $ \rS $,          
\begin{align}\label{eq:deterministic error}
  \vspace{-0.4cm}
    \p_{\rx\sim\cD} \Bigg[  
      \sum_{\rf\in \cA(\cS(\rS,\emptyset))} \negmedspace \negmedspace \negmedspace \negmedspace \ind\{\sum_{s=1}^{t}  \ind\{h_{\rf,s}(\rx)=c(\rx)\}/t\geq 3/4\}/|\cS(\rS,\emptyset)|
       <3/4\Bigg]\negmedspace \negmedspace \leq \cs \frac{d+\ln{\left(1/\delta \right)}}{m},
  \end{align}
  \vspace{-0.4cm}\newline
where we have used that $\rf\in  \cA(\cS(S,T)) $ is a majority vote over $ t $ hypothesis, $ \rf=\sum_{s=1}^{t} h_{\rf,s}.$     
Before explaining why this holds, we provide the rationale for why this is what we want to show, and how it gives us the inference complexity that does not suffer an increase of $ \Theta(\ln{(m )}) $  from the boosting step. To this end, we draw inspiration from \cite{baggingoptimalPAClearner} \cref{eq:splittingerror} and the idea of "derandomzing" $ \cB $ by $ \bar{\cB} $, with the above classifier/event now taking the place of $ \bar{\cB} $. Now, let our algorithm be denoted $ \ah $ and let $ E $ denote the event $ \{x\in \cX \mid  \sum_{\rf\in \cA(\cS(\rS,\emptyset))} \ind\{\sum_{s=1}^{t} \ind\{h_{\rf,s}(x)=c(x)\}/t\geq 3/4\}/|\cS(\rS,\emptyset)|
\geq 3/4 \}$, we then have,  as in \cref{eq:splittingerror} that,
$\ls_{\rx\sim\cD_{c}}(\ah(\rS)) = \p_{\rx\sim\cD}[\bar{E}]+\ls_{\rx\sim\cD_{c}(\cdot \mid E)}(\ah(\rS)).$

Now given an $ x\in E $, we observe by the definition of $ E $ that if we sample a row/majority voter of $ \cA(\cS(\rS,\emptyset)) $ and then sample one of its voters, we have with probability at least $ (3/4)^{2}=9/16 $ that the sampled voter is such that $ h(x)=c(x) $, thus correct on the point $ x $. Since this probability is $1/16 $ greater than $ 1/2 $,  we get by repeating the above way of sampling voters $l= \Theta(\ln{\left(m/(\delta(d+\ln{\left(1/\delta \right)})) \right)} )$ times, that for $ x\in E $, letting $ \rX_{x}=\sum_{i=1}^{l}\rX_{x,i} $, where $ \rX_{x,i} $ indicates if the $ i $'th drawn voter is correct on $ x $, a Chernoff bound implies that $\p\left[\rX_{x}\leq l/2\right]= \p\left[\rX_{x}\leq (1-2/18)9l/16\right]\leq \exp((2/18)^{2}\cdot(9l/16)/4)=O((\delta(d+\ln{\left(1/\delta \right)}))/m )$. Thus, by Markov's inequality, $ \p\left[\p_{\rx\sim\cD_{c}(\cdot\mid E)}\left[\rX_{x}\leq l/2\right]\geq (d+\ln{\left(1/\delta \right)})/m \right]=O(\delta)$. That is with probability at least $ 1-\delta $ over the repeated random sampling over voters in the above fashion, we have that $\ls_{\rx\sim\cD_{c}(\cdot \mid E)}(\ah(\rS))\leq \p_{\rx\sim\cD(\cdot|E)}\left[\rX_{x}\leq l/2\right]=O((d+\ln{\left(1/\delta \right)})/m)  $ (The above argument is also depicted in \cref{fig:first_figure}). 

Thus, with probability at least $ 1-\delta $ over the above way of random sampling of voters, the majority vote of them, on a new example $ \rx $ drawn from $ \rx\sim\cD(\cdot|E) $ has $O((d+\ln{\left(1/\delta \right)})/m)$ error. Thus, if we let $ \ah $ be the algorithm that samples $l= \Theta(\ln{\left(m/(\delta(d+\ln{\left(1/\delta \right)})) \right)} )$ many hypotheses/voters in this fashion and takes the majority vote of them, we get, by the above splitting of $ \ls_{\cD_{c}}(\hat{\cA}(\rS)) $  and \cref{eq:deterministic error} (which upper bounds $\bar{E}$),  that with probability at least $ 1-\delta/2 $ over $ \rS $ and $ \ah $  that    $\ls_{\rx\sim\cD_{c}}[\ah(\rS) ] =O(d+\ln{\left(1/\delta \right)}/m)$, which shows that $ \ah $ obtain the optimal error bound of realizable PAC learning. Further, we get that the inference complexity will be evaluating each of the  $l= \Theta(\ln{\left(m/(\delta(d+\ln{\left(1/\delta \right)})) \right)} )$ voters sampled in the above fashion, and thus the inference complexity becomes $ \Uinf(\Theta(\ln{\left(m/(\delta(d+\ln{\left(1/\delta \right)})) \right)}))$ as claimed without the blow-up from the boosting step, since we can sample on the voters level.  

Thus, what we still need to argue about is that \cref{eq:deterministic error} holds with probability at least $ 1-\delta $. Inspired by \cite{hannekeoptimal} we show the claim by induction in the size $ m=6^{k} $ of $ \rS $, and consider \cref{eq:deterministic error} with an arbitrary training sequence $ T $ instead of $ \emptyset $. The induction base follows from the right-hand side of \cref{eq:deterministic error} being greater than $ 1 $ for $ m=6 $. Now we first notice that for the event inside of \cref{eq:deterministic error} to happen, it must be the case that there exists an $ i =1,2,3,4,5$ such that 
$ \sum_{\rf\in \cA(\cS (i,T))} \ind\{\sum_{s=1}^{t} \ind\{h_{\rf,s}(x)=c(x)\}/t\geq 3/4\}/|\cS(i,T)|
       <3/4.
$
Since $ \cA(\cS(i,T)) $ has $ 1/5  $ of the voters in $ \cA(\cS(\rS,T)) $, we have that there are still $ 1/4-1/5 =1/20$ of the voters in $  \cA(\cS(\rS,T))$ not in $\cA(\cS(i,T))$ that do not have $ 3/4 $ correct answers on $ x $. This is a $ (5/4)(1/20)=1/16 $ fraction of the majority voters in $ \cA(\cS(\rS,t))\backslash \cA(\cS(i,T)) $, and thus we get that a randomly chosen majority voter $ \rf $ from $ \cA(\cS(\rS,T))\backslash \cA(\cS(i,T)) $ is such that  $\sum_{s=1}^{t} \ind\{h_{\rf,s}(x)=c(x)\}/t< 3/4$ with probability at least $ 1/16 $. Thus, by drawing a uniform random $ \ri\sim {1,\ldots,5} $ and uniform random $ \rf \in  \cA(\cS(\rS,T))\backslash \cA(\cS(\ri,T)) $,  we conclude similarly to \cref{eq:intro1} that it suffices to show that for $ i,j\in \left\{ 1,2,3,4,5  \right\}$  $ i\not=j $   with probability at least $ 1-\delta/20 $ over $ \rS $ that 
\begin{align}\label{eq:ourrecursion}
  80 \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \max_{\rf\in\cA(\cS(\rS_{j},T))} \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \p_{\rx\sim\cD_{c}}\Bigg[ \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace   \sum_{ \quad\quad \quad \rf\in \cA(\cS (i,T))} \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \negmedspace \frac{\ind\{\sum_{s=1}^{t} \ind\{h_{\rf,s}(\rx)=c(\rx)\}/t\geq \frac{3}{4}\}}{|\cS(i,T)|}
  \negmedspace  <\negmedspace \frac{3}{4}, \sum_{s=1}^{t} \ind\{h_{\rf,s}(\rx)=c(\rx)\}/t\negmedspace<\negmedspace\frac{3}{4}\Bigg]
\end{align}
is upper bound by $ \cs (d+\ln{\left(1/\delta \right)})/m $ (our \cref{induktionlemma}), and doing a union bound over all the $ 20 $ combinations of $ i,j $ one gets that \cref{eq:deterministic error} holds with probability at least $1- \delta $. We notice that if we had made less than $ 5 $ recursive calls, the above would not have worked since we then would not be able to guarantee that there were any number of majority voters left in $ \cA(\cS(\cS,T))\backslash \cA(\cS(i,T)) $ that failed to have $ 3/4 $ correct as $ 1/4-1/a \leq0$, for $ a<5 $. 

Now to show the above holds with probability at least $ 1-\delta/20 $, we first use the induction hypothesis that holds for $m=6^{k-1},$ which we know is the size of $ \rS_{i} $. Thus, we get that with probability at least $ 1-\delta/60 $ over $ \rS_{0},\rS_{i} $ that 
\vspace{-0.2cm}
\begin{align}\label{eq:errorinduktionstep}
    \p_{\rx\sim\cD}\Bigg[ \sum_{\rf\in \cA(\cS (i,T))} \frac{\ind\{\sum_{s=1}^{t} \ind\{h_{\rf,s}(x)=c(x)\}/t\geq 3/4\}}{|\cS(i,T)|}
    <3/4\Bigg]\leq 6\cs \frac{d+\ln{\left(60/\delta \right)}}{m}.
  \end{align} 
  \vspace{-0.4cm} \newline We can further restrict to the setting where the above is at least $ \cs (d+\ln{\left(1/\delta \right)})/(80m) $, since otherwise we have by the monotonicity of measures that \cref{eq:ourrecursion} is upper bounded by $ \cs(d+\ln{\left(1/\delta \right)})/m $ as noted by \cite{hannekeoptimal}. Further, we can with this lower bound, as in the above sketch for \cite{hannekeoptimal} show by a Chernoff bound that $ \rS_{j} $ contains at least $ \cs (d+\ln{\left(1/\delta \right)})/(2\cdot6\cdot80) $ points (we denote these points $ \rS_{j}\sqcap E' $) where $ \sum_{\rf\in \cA(\cS (i,T))} \ind\{\sum_{s=1}^{t} \ind\{h_{\rf,s}(x)=c(x)\}/t\geq 3/4\}/|\cS(i,T)|
<3/4 $ with probability at least $ \exp(-\cs (d+\ln{\left(1/\delta \right)})/(8\cdot 6\cdot80)) $, which for $ \cs $ sufficiently large is less than $ \delta/60 $. Thus, if we now let $ E' $ denote the following event  $\{x\in \cX: \sum_{\rf\in \cA(\cS (i,T))}\ind\{\sum_{s=1}^{t} \ind\{h_{\rf,s}(x)=c(x)\}/t\geq 3/4\}|\cS(i,T)|
<3/4 \}$,  we can use the law of conditional probability to bound \cref{eq:ourrecursion} using \cref{eq:errorinduktionstep} with probability at least $ 1-\delta/60 $ over $ \rS_{0},\rS_{i} $  
\vspace{-0.3cm}
\begin{align}\label{eq:laststepininduktionstep}
    80\max_{\rf\in\cA(\cS(\rS_{j},T))}\p_{\rx\sim\cD(\cdot|E')}\Big[ \sum_{s=1}^{t} \ind\{h_{\rf,s}(x)=c(x)\}/t< 3/4\Big] \cdot 6\cs (d+\ln{\left(60/\delta \right)})/m,  
  \end{align} 
  \vspace{-0.4cm} \newline as done similarly to \cite{hannekeoptimal}. Thus, if we can show that $\p_{\rx\sim\cD(\cdot|E')}[ \sum_{s=1}^{t} \ind\{h_{\rf,s}(x)=c(x)\}/t< 3/4]$ is uniformly bounded over the majority voters in  $ \cA(\cS(\rS_{j},T))$,  by $ 1/(80\cdot 6\cdot \ln{\left(60e \right)}) $ with probability at least $ 1-2\delta/60 $ over $ \rS_{j} $,  the upper bound of \cref{eq:ourrecursion} would follow. We notice the above since, $ \sum_{s=1}^{t} \ind\{h_{\rf,s}(x)=c(x)\}/t< 3/4 $ is equivalent to $ \sum_{s=1}^{t} h_{\rf,s}(x)c(x)/t< 1/2 $, the above uniform bound is equivalent to a uniform bound on  $ \p_{\cD_{c}(\cdot|E')}[\sum_{s=1}^{t} h_{\rf,s}(x)c(x)/t<1/2]$, the $ 1/2 $-margin loss. 
  
  We remark that the above-required uniform bound is different from and not implied by the technique used to get the above uniform bound that \cite{Optimalweaktostronglearning} want, which was a uniform bound on the classification loss of the $ \sign(\cdot) $ of a voting classifier, whereas the above is wanting something stronger, namely a uniform error bound on the $ 1/2 $-margin loss for the majority vote, which Theorem 4 of \cite{Optimalweaktostronglearning} does not imply. 

To the end of showing the above uniform bound, we now use the aforementioned property of $ \cA $ outputting majority voters having zero  $ 3/4$ margin loss on the training sequence it receives. 
To use this property, we show the following bound, \cref{lem1},
which, for $0< \gamma< 1 $ and $ \xi>1 $, states that with probability at least $ 1-\delta/60 $ over $ \rS_{j}\sqcap E' $, we have for all majority voters $ f\in\dlh=\{f:f=\sum_{s=1}^{t} h_{t}/t, \forall s\in\{1:t\}, h_{s}\in \cH\} $ that $ \p_{\rx\sim\cD(\cdot |E')}\left[f(\rx)c(\rx)\leq \gamma\right]$ is upper bounded by $ \p_{\rx\sim \rS_{j}\sqcap E'}\left[f(\rx)c(\rx)\leq \xi\gamma\right] +C(2d/(((\xi-1)\gamma)^{2}|\cS_{j}\sqcap E'|))^{0.5}+(2\ln{\left(120/\delta \right)}/|\cS_{j}\sqcap E'|)^{0.5} $, where $ C $ is some universal constant.\footnote{People familiar with ramp loss bounds combined with uniform convergence and Rademacher complexity arguments may recognize that this bound can be derived by letting the ramp start its descent at $ \gamma $ and end at $ \xi \gamma $, which gives the slope $ 1/((1-\xi)\gamma)$. We could not find the bound stated elsewhere, so we proved it in \cref{lem1}.}  

Now, using that any $ \rf \in\cA(\cS(\rS_{j},T))$, implies $ \rf=\cA(S') $, for some sample $ S'\in\cS(\rS_{j},T) $ which satisfies  $ (\rS_{j}\sqcap E')\sqsubset \rS_{j}\sqsubset S'$ being a subset of $ S' $  by \cref{alg:Subsample}  Line~\ref{alg:Subsample:recursion3}. Which, by the zero $ 3/4 $ margin loss guarantee of $ \cA $ on the training sequence it is given, implies that $ \rf $ has zero $ 3/4 $ margin loss on $ \rS_{j}\sqcap E' $. Furthermore, since  $ |\rS_{j}\sqcap E'|\geq \cs(d+\ln{\left(1/\delta \right)})/960 $ holds with probability at least $ 1-\delta/60 $ over $ \rS_{j} $,  and with probability at least $1-\delta/60 $ over $ \rS_{j}\sqcap E' $ the uniform bound over $ \dlh$, with $ \gamma=1/2 $, $ \xi=3/2 $, we conclude that with probability at least $ 1-2\delta/60 $ we have that 
\vspace{-0.4cm} 
\begin{align*}
  \max_{\rf\in\cA(\cS(\rS_{j},T))}\negmedspace\negmedspace\negmedspace\negmedspace\negmedspace\negmedspace\negmedspace\p_{\rx\sim\cD(\cdot|E')}\Big[ \sum_{s=1}^{t} \ind\{h_{\rf,s}(x)=c(x)\}/t< \frac{3}{4}\Big]\negmedspace\leq\negmedspace C\Big(\frac{30720d}{\cs(d+\ln{\left(1/\delta \right)})}\Big)^{\negmedspace\tfrac{1}{2}}+\Big(\frac{1920\ln{\left(120/\delta \right)}}{\cs(d+\ln{\left(1/\delta \right)})}\Big)^{\negmedspace\tfrac{1}{2}}
  \vspace{-0.4cm}
  \end{align*}        
which for $ \cs$ sufficiently large, is less than $ 1/(8\cdot6\cdot\ln{(60 e)}) $, which, as alluded to below \cref{eq:laststepininduktionstep} implies \cref{eq:ourrecursion}, which gives our optimal PAC error bound and inference complexity. Thus, if we can show that the training complexity is as claimed in \cref{introductionmaintheorem} with probability at least $ 1-\delta/2 $, we have shown the claim of \cref{introductionmaintheorem} with probability at least $ 1-\delta $.

However, before we do this, we make a small remark about the above argument allowing us to transition from a majority vote of majorities to a majority vote. Namely, that this argument does not lend itself to the weak to strong setup of \cite{Optimalweaktostronglearning}, as \cref{eq:deterministic error} to the best of our knowledge, then only hold with $\sum_{s=1}^{t}  \ind\{h_{\rf,s}(x)=c(x)\}/t\geq 1/2+\Theta(\gamma) $. Thus, the probability of sampling a correct voter becomes $ (3/4)(1/2+\Theta(\gamma))=3/8+(3/4)\Theta(\gamma) $, which is only greater than $ 1/2 $ if $ (3/4)\Theta(\gamma)>1/8 $, which is not always given in the weak to strong learning setup as $ \gamma $ can be arbitrarily small. Thus, another part of our observation is that we are in the realizable setup and not in the weak to strong setup, so we can get arbitrarily good margins, of the voters in the majority votes.  

Now, for the training complexity bound of $ \ah $, we need to show that the above sampling of voters can be done efficiently. The above sampling process can be described as follows: First, sample a training sequence/row $\rS' $  of $ \cS(\rS) $, then train $ \cA(\rS') $, and then sample uniformly at random a hypothesis of $ \cA(\rS') $ and repeat this $l=\Theta(\ln{\left(m/(\delta(d+\ln{\left(1/\delta \right)})) \right)})$ times. Sampling a row $ \rS' $  from $ \cS(\rS) $ can be accomplished using $ O(m) $ operations by 
drawing a $ \rw$ uniformly at random from $\left\{ 1:5 \right\}^{\log_{6}(m)}=\left\{ 1:5 \right\}^{k}$ and for each  $i=1,\ldots,k$, select the examples in $ \rS $ between $ [ 6^{k}/6^{i}\rw_i+1,(\rw_i+1)6^{k}/6^{i}] $. In this way, $ \rw_{i} $ determines which of the $ 5 $ training sequences to recurse on at the $ i $'th recursion step Line~\ref{alg:Subsample:recursion2} uniformly at random. Since it takes at most $ O(\ln{\left(m \right)}) $ operations to compute these indices,
we get that sampling a row of $ \cS(\rS) $ takes $ O(m) $ operations. 

Now since $ \cA $ had training complexity $ O(\ln{\left(m/\delta \right)} )(O(m+d\ln{\left(m \right)})+\Utrain(d)+3m\Uinf ) $ with probability at least $ 1-O(\delta/m) $, applying a union bound on the event that all the runs in $\cA(\cS(\rS,\emptyset))$ succeeds, which there is at most $ m^{\log_{6}(5)} $ of, one get that the above training complexity holds for all runs in $ \cA(\cS(\rS,\emptyset)) $, especially the one sampled,  with probability at least $ 1-\delta/2 $. Now given $ \cA(\rS') $  sampling a hypothesis from $ \cA(\rS')= \sum_{s=1}^{t} h_s/t $ takes at most $t= O(\ln{\left(m/(\delta(d+\ln{\left(1/\delta \right)})) \right)}) $ operations. Thus, the training complexity, of finding the $ l $ voters, becomes $ l $ times the computational complexity cost of running $ \cA$ which gives the training complexity      $O(\ln{m/(\delta(d+\ln{\left(1/\delta \right)}))} )\cdot\ln{(m/\delta )})\cdot (O(m+d\ln{\left(m \right)})+\Utrain(550d)+ 3m\Uinf)$
with probability at least over $ 1-\delta/2 $  over the randomness of $ \rS$ and $\cA$, which, as noted earlier, concludes the proof sketch.

\paragraph{Summary of our contribution:} \cref{maintheorem} provides an optimal PAC learner that only queries the $ \erm $  with training sequences of size $ 550d $ - positively answering \hyperref[question1]{Question~\ref*{question1}}. Moreover, the optimal PAC learner's of \cref{maintheorem} computational cost, when seen as a function of the number of training examples $ m $ runs up to a quadratic logarithmic factor in linear time - answering \hyperref[question2]{Question~\ref*{question2}} positively up to the quadratic logarithmic factor - however as mentioned in the perceptron example $ U_{T}(d) $ may depend on $ m $, and it may also be the case for $ U_{I} $. Furthermore, the inference complexity off \cref{maintheorem} is asymptotically the best among the known optimal PAC learners. 

The key observations that allowed for these positive results were seeing that $ \erm $ trained on $ \Theta(d) $ points allowed for creating majority voters with good margins, and it implied that the purely analytical structure $ \cA(\cS(\rS)) $ could be shown to have good margins on both the majority voter level and the majority of majorities level. This combined with the observation that $ \ah $ could be seen as sampled from the voters in the majorities of $ \cA(\cS(\rS)) $, led to the inference complexity of $\ah$ not suffering a blow-up from the boosting step, a close to linear training complexity, and an optimal PAC error bound. 

\paragraph{Use of randomness:}Before we move on to describe the structure of the paper we want to mention some limitations we see off our work. Our algorithm $ \ah $ uses randomness which current computational cost is accounted for by saying that all random variables has to be read when used and that this takes $ 1 $ operation - so we are not taking into account if there is some computational cost in creating the randomness. To make it clear how much and what randomness we use we state it here: we use $ \Theta(d\ln{(m/\delta)}) $ uniform random variables on the interval $ [0:1] $,  $ \Theta(\ln{(m )}\cdot\ln{(m/(\delta(d+\ln{(1/\delta )})) )})$ uniform random variables on the discrete values  $ \{ 1,2,3,4,5 \} $, and $ \Theta(\ln{(m/(\delta(d+\ln{(1/\delta )})) )})$  uniform random variables on the discrete values $ \{ 1,\ldots,\Theta(\ln{(m )}) \} $. Further the model we use to count computational cost is arguably simplistic, and could be refined further.     

\paragraph{Structure of the paper:}The next section introduces the notation and preliminaries used in this work. \cref{sec:optimalityah} gives the proof of our optimal PAC learner using properties of $ \cA(\cS(S,\emptyset)) $ proved in \cref{sec:optimalitya} and properties of $ \cA $ proved in \cref{sec:propAdaBoostSample}, where some of the proofs for the properties are deferred to the three appendix one for each of the above-mentioned sections.