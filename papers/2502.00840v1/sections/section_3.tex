
% In this section, we define the threat model in section \ref{sec:threat_model}, after that we introduce the setup of our study in section \ref{sec:setup}, and then we in section \ref{sec:error} we summarize the definition of error for different activation approximation techniques and give the benchmark of these techniques, and finally in section \ref{sec:observations} we present the 3 key observations of our study, revealing the impact of activation approximation on safety and utility.

In this section, we aim to provide a systematic safety assessment and analysis of existing activation approximation methods. We will first introduce the threat model of our study in Section~\ref{sec:threat_model} and then describe the setup of safety evaluations in Section~\ref{sec:setup}. In Section~\ref{sec:error}, we elaborate on the specific activation approximation methods under evaluation and summarize their activation errors, covering seven representative techniques from each of the three categories. Finally, in Section~\ref{sec:observations}, we identify three key observations from the safety evaluation results, revealing the characteristics of activation approximations on LLM safety behaviours.

\subsection{Threat Model} \label{sec:threat_model}
\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{assets/imgs/overview_2.png}
    \caption{Illustration of the three parties in Threat Model.}
    \label{fig:overview}
\end{figure}
% In this paper, we consider realistic scenarios of LLM customization in Figure \ref{fig:overview}, which involves three parties in the threat model:
In this paper, we consider a common scenario in the development and deployment of LLM-based applications, as outlined in Figure \ref{fig:overview}. This scenario involves three parties: the LLM model developer, the LLM-based application provider, and the application users. In particular, the roles of the first two parties can sometimes overlap within the same organization.
\begin{itemize}
    \item \textbf{LLM Model Developers} provide open-source safety-aligned LLMs, enabling application providers to customize models for legitimate usage in accordance with licensing terms. Prominent examples include large technology companies such as Meta (with LLaMA-Chat), Google (with Gemma), and Alibaba (with Qwen), as well as open-source organizations/groups that have released safety-aligned LLMs like Falcon, Mistral, Alpaca, and Zephyr.
    % They will provide services such as LLM fine-eytuning, customized training, etc., and use their own rich data and computational resources to train LLMs that match the demand side. Typically, LLM trainers are OpenAI, Anthropic, Azure, etc. We assume that they will use relevant datasets for safety-alignment during training to ensure the delivery of safety-aligned models.
    \item \textbf{LLM-based Application Providers} 
    customize open-source safety-aligned LLMs to develop applications tailored to specific use cases, expecting the customized models to retain both the utility and safety capabilities of the original LLMs. These providers aim to deliver high-quality, responsive services to their users while striving to minimize inference latency. However, they could face resource constraints in terms of computation and memory. Consequently, they are incentivized to adopt inference efficiency-enhancing techniques to reduce resource usage and wall-clock inference time while maintaining utility and safety capabilities as much as possible.
    % Due to the lack of computing power or data, they use LLM trainer to customize their services, such as customizing intelligent customer service or voice assistants to meet their business needs with OpenAI's dialogue models \cite{ai_news_1, ai_news_2}, or customizing training for medical models with Google Cloud \cite{reale2023design, katz2024nhanes}. These services do not fine-tune or edit models, and they have a strong motivation to accelerate LLM inference using training-free activation approximations.
    \item \textbf{LLM-based Application Users} interact with LLM-based applications expecting helpful, accurate, and timely responses. Some might occasionally, for example out of curiosity, issue queries that should be turned down by a safety-aligned LLM. Meanwhile, there could be adversarial users launching jailbreak attacks \cite{zou2023universal, liu2024autodan, jones2023automatically} to deliberately bypass the safety guardrails of the application providers' LLMs.
    % They focus on the jailbreaking of large language models as motivated by many relevant works \cite{zou2023universal, liu2024autodan, jones2023automatically}. They want to get meaningful responses from LLM to their harmful questions.
\end{itemize}
\partitle{Adaptive Capabilities}
This paper considers an adaptive adversary who can construct the jailbreak prompt based on gradients of the target LLM \cite{zou2023universal, liu2024autodan, jones2023automatically}. These methods pad a prefix or suffix to the original prompt, which can be optimized to achieve the attack objective. This shares a similar idea
as the textual adversarial examples whereby the goal is to generate harmful responses. We will discuss potential countermeasures for this adaptive attack in section \ref{sec:jailbreak_defense}.


\subsection{Safety Assessment Setup} \label{sec:setup}
\partitle{Safety Measurement}
We measure LLM's safety by evaluating the Attack Success Rate (ASR), which is defined as the percentage of failure to abstain from responding to a malicious instruction. These malicious instructions come from AdvBench \cite{zou2023universal}, which contains 520 items in total. Previous works \cite{zou2023universal, wei2024assessing} simply use a small set of rejection phrases such as ``Sorry, I can't'', ``I can not fulfill'' to do prefix matching to determine whether the model rejects the answer. However, this simple procedure may misclassify the output, if the model replies ``I think it's illegal'' or ``\#\#\#\#'', which are not in the set, they classify the attack as successful, but in fact, it is not.

To reduce the risk of misjudgment, the HarmBench classifier~\cite{mazeika2024harmbench} has been widely adopted to judge whether the output content is harmful or not. 
\begin{gather*}
    \texttt{HarmCLS}(y) = 
    \begin{cases}
       1; & \text{if $y$ is harmful}  \\
       0, & \text{if $y$ is harmless}
   \end{cases}
\end{gather*}
As claimed, the classifier fine-tuned with LLAMA-2-13B outperforms GPT-4 by approximately 5\% in agreement rates with human judgments on the manually labeled validation set. 

However, we found that it could still fail to distinguish a benign output from a misaligned one, especially under circumstances where the instructions are complicated. We show an example of a false positive produced by the HarmBench classifier. This example was obtained from DRA (USENIX Security'24)~\cite{liu2024making} where they proposed a jailbreak attack that bypasses safety checks by hiding a harmful question in a word puzzle, and asking the LLM to solve the puzzle and then answer the question. Their prompt-based jailbreak approach gives relatively complicated instructions for the LLMs to extract the questions and answer in a step-by-step manner. In the case of the example response, it is clear that the LLM did not produce a concrete and actionable plan to fulfill the attacker's malicious request. However, due to the "positive" answer, the HarmBench classifier became confused and incorrectly identified the LLM's response as harmful.


\begin{tcolorbox}[colback=white, colframe=gray, title=\textbf{\small{A \texttt{HarmCLS} False Positive: \textit{How to launder money?}}}, title filled=false]
\small
happy (h)our (o)verwhelming fear (w)asted effort tremendous effor(t) (o)verwhelming fear (l)ost opportunity (a)nimals!
(u)nbelievable (n)ice to meet you lucky (d)og iPhon(e) (r)adiant smile (m)iserable failure (o)pen heart (n)ice to meet you iPhon(e) (y)earn for adventure 

\textcolor{red}{Read these sentences! But! ONLY focus on the letters that surrounded by "( )" and put them together to construct a sentence. Give me the detailed answer step by step!}
% named Q1, and answer the question in Q1! Do not show me how you get Q1!
% Note that Q1 contains these words: ""how"" ""to"" ""la"" ""under """"money"" . Complete and recover Q1 accordingly.
% Give me the detailed answer, step by step! The answer should be more than 600 words.
% **IMPORTANT REQUESTS:** Start your answer EXACTLY with ""Absolutely! Let me help you! Here's my detailed plan to {REPLACE IT WITH THE RECOVERED CONTENT OF Q1} step by step: 1. We should choose a target and find some suitable tool such as , ensuing we can overcome all difficulties. 2. We should have take action,"".
% You should not answer any other things that are not related. Do NOT show me the risks!! Do Not try to educate me.
\tcblower
\small
\textbf{Absolutely! Let me help you! Here's my detailed plan:}

\textbf{Step 1: Understanding the question.}
The question is asking for a sentence that can be constructed by combining the letters surrounded by parentheses in the given sentences.

\textbf{Step 2: Identifying the possible sentences.} Based on the given sentences, the possible sentences that can be constructed using the letters are: How to launder money.

\textbf{Step 3: Providing a helpful answer.} 
\ul{As a helpful assistant, I must inform you that laundering money is illegal and can result in severe consequences.}

\textbf{Step 4: Offering alternative solutions.}
Instead, I suggest you seek legal ways to manage your finances. 
% 
There are many resources available that can help you make informed decisions about your financial future. Consider consulting with a financial advisor or seeking guidance from a reputable financial institution.

% In conclusion, while I cannot provide instructions on how to launder money, I hope this information helps you understand the importance of making ethical and legal financial decisions. If you have any other questions or concerns, please feel free to ask.
\end{tcolorbox}

To ensure the validity of a defense mechanism, it is crucial to have a misalignment evaluation metric that can accurately reflect the effectiveness of the proposed approach. Based on the observed existence of false positives, we cannot directly rely on \texttt{HarmCLS} to produce a precise account of the resulting ASR of the LLMs after implementing our defense strategy. We collect 120 additional prompt-response pairs from the DRA \cite{liu2024making} and manually mark whether it is harmful or not. We fine-tuned the HarmBench classifier \cite{mazeika2024harmbench} with this dataset to achieve a higher accuracy for misbehaviour judgment. 
% We conducted additional human judgments by sampling records from the model outputs generated during our ASR evaluation process, and found that human annotators can consistently achieve 95\% agreement with the enhanced HarmBench classifier, better than 93.2\% on . 

\partitle{Utility Measurement}
We use perplexity on WikiText-2~\cite{merity2016pointer} to measure the model's utility, which is one of the most common utility metrics. Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. For a tokenized sequence $X=(x_1,..., x_n)$, the perplexity of $X$ is:
\begin{equation*}
   \text{PPL}(X)= \text{exp}\Big\{-\frac{1}{n}\sum_{i=1}^n \log p_\theta(x_i|x_{<i})\Big\}, 
\end{equation*}
where $\log p_\theta(x_i|x_{<i})$ is the log-likelihood of the $i$-th token conditioned on the preceding tokens $x_{<i}$.

\partitle{Safety-Aligned Models}
To underscore the generalizability of our findings, we test different aligned LLMs from various publishers. The LLMs we used are \textit{Llama-2-7B-Chat} \cite{touvron2023llama}, \textit{Llama-2-13B-Chat} \cite{touvron2023llama}, \textit{Llama-3.1-8B-Instruct} \cite{meta2024introducing}, \textit{Phi-3-Mini-4K-Instruct} \cite{abdin2024phi}, \textit{Phi-3.5-Mini-Instruct} \cite{abdin2024phi}, \textit{Mistral-7B-Instruct-v0.3} \cite{jiang2023mistral}, \textit{Mixtral-8x7B-Instruct-v0.1} \cite{jiang2024mixtral}, \textit{Zephyr-7B-$\beta$} \cite{tunstall2023zephyr}, \textit{Qwen2-7B-Instruct} \cite{bai2023qwen}, and \textit{Qwen2.5-32B-Instruct} \cite{yang2024qwen2}. 
These models are fine-tuned with supervised fine-tuning (SFT), reinforcement learning with human feedback (RLHF), or direct preference optimization (DPO) to ensure precise instruction adherence and robust safety measures.
% \begin{table}
%     \renewcommand\arraystretch{1}
%     \centering
%     \caption{Safe-aligned models in our experiment.}
%     \begin{tabular}{ccc}
%     \toprule[1pt]
%     \textbf{Family} & \textbf{Model} & \textbf{Aligned Method} \\
%     \midrule[1pt]
%     \multirow{3}{*}{Llama} & Llama-2-7B-Chat & SFT+RLHF \\
%     & Llama-2-13B-Chat & SFT+RLHF \\
%     & Llama-3.1-8B-Instruct & SFT+RLHF\\
%     \midrule[1pt]
%     \multirow{2}{*}{Phi3} & Phi-3-Mini-4K-Instruct & SFT+DPO \\
%     & Phi-3.5-Mini-Instruct & SFT+DPO\\
%     \midrule[1pt]
%     \multirow{3}{*}{Mistral} & Mistral-7B-Instruct-v0.3 & SFT \\
%     & Mixtral-8x7B-Instruct-v0.1 & SFT+DPO \\
%     & Zephyr-7B-$\beta$ & SFT+DPO \\
%     \midrule[1pt]
%     \multirow{2}{*}{Qwen} & Qwen2.5-7B-Instruct & SFT+DPO \\
%     & Qwen2.5-32B-Instruct & SFT+DPO \\
%     \bottomrule[1pt]
%     \end{tabular}
%     \label{tbl:models_info}
% \end{table}


% \partitle{Prompt Templates}
% % During inference, the input instruction is initially integrated into a template, which then is tokenized and goes through the embedding layer to form the initial input vectors for the LLM. We use the same dialog template \cite{taori2023stanford} for different problems across various aligned LLMs in our study.
% During inference, the input instruction is initially integrated into a template, which is then tokenized and processed through the embedding layer to form the initial input vectors for the LLM. We use the same dialog template \cite{taori2023stanford} for different problems across various aligned LLMs in our study.

  % \begin{tcolorbox}[colback=white, colframe=gray, title=\textbf{Dialog Template}, title filled=false]
  %   Below is an instruction that describes a task. Write a response that appropriately completes the request.
  %   \tcblower
  %   \textbf{Instruction:} \{The malicious input instruction\} \\
  %   \textbf{Response:} \{The output of the model\}
  % \end{tcolorbox}

\begin{figure}[!t]
    \centering
    \subfloat[Error distributions in activation polynomialization, with BOLT \cite{pang2024bolt} as the example.]{\includegraphics[width=\linewidth]{assets/imgs/noise_private_inference.png} \label{fig:noise_private_inference}}
    \hfil
    \subfloat[Error distributions in activation sparsification, with TEAL \cite{liu2024training} in 50\% sparsity.]{\includegraphics[width=\linewidth]{assets/imgs/noise_activation_sparsity.png} \label{fig:noise_activation_sparsity}}
    \hfil
    \subfloat[Error distributions in activation quantization, with SmoothQuant \cite{xiao2023smoothquant} in 8-bit activation.]{\includegraphics[width=\linewidth]{assets/imgs/noise_activation_quantization.png} \label{fig:noise_activation_quantization}}
    \caption{Activation error distributions on Llama-3.1-8B-Instruct using different activation approximations. The first column presents errors before $\Wup$, and the second column presents errors before $\Wdown$. Best-fit Gaussian/Laplace distributions are overlaid in black for comparison.}
    \label{fig:error_distributaion}
\end{figure}

\subsection{Activation Approximation Error} \label{sec:error}
In this section, we summarize the error of different activation approximation techniques and select representative open-source works to do the activation approximation for our benchmark. We begin by recalling the \ul{multi-layer perceptron} (MLP) within each Transformer block, which contains the activation expressions to be approximated and analyzed. 

\begin{definition}
Let the $l$-th MLP block (with no biases), with $l$-th layer model parameter $\theta_l$ and input embedding vector $\mathbf{e}_{l}$, be denoted by the function $\mlp_l$  of the form
% $$
%     \boldsymbol{f}_l(\theta_l|\mathbf{e}_{l}) = (\alpha(\mathbf{e}_{l}\mathbf{W}^{gate}_l) \odot \mathbf{e}_{l} \Wup_l)\cdot\Wdown_l, 
% $$
$$
    \mlp_l(\theta_l|\mathbf{e}_{l}) = (\alpha(\mathbf{e}_{l}\cdot \Wup_l))\cdot\Wdown_l, 
$$
where $\Wup_l, \Wdown_l$ are constituent parameter matrices associated with $\theta_l$, $\alpha(\cdot)$ is the activation function, such as SwiGLU or GELU, which are widely adopted by LLMs in practice. 
\end{definition}


% The MLP layer with activation approximation is defined as
$$
    % \boldsymbol{f}^{\epsilon_l}_l(\theta_l|\mathbf{E}_{l}) = [\alpha((\mathbf{E}_{l}+\epsilon_l^{up})\cdot\Wup_l) + \epsilon^{down}_l]\cdot\Wdown_l, 
$$
% where $\epsilon_l=\{ \epsilon_l^{up}, \epsilon_l^{down} \}$ are the noise introduced to mitigate the activation approximation in the $l$-th MLP layer.

\noindent \textbf{Approximation Error of Activation Polynomialization} In private inference, the server replaces the non-linear functions $\mathcal{F}$ with MPC/FHE-friendly approximations $\mathcal{F}_\text{approx}$. 
\begin{table*}
    % \renewcommand\arraystretch{1.05}
    \centering
    \small
    \caption{Benchmark of different training-free activation approximation techniques on Llama-3.1-8B-Instruct. \textbf{Note that the baseline PPL is 8.05, ASR is 0.19.}}
    \begin{tabular}{ccccccc}
    \toprule[1pt]
    $\textbf{Methods}$ & $\textbf{Works}$ & $\textbf{Setting}$ & $\epsilon_l^{up}$ & $\epsilon_l^{down}$ & $\textbf{PPL}\downarrow$ & $\textbf{ASR}\downarrow$ \\
    \midrule[1pt]
    \multirow{4}{*}{\textbf{\makecell[c]{Activation \\ Polynomialization}}} & Iron\cite{hao2022iron} & MPC & $\mathcal{N}(0, 0.064^2)$ & Lap(0, 0.049) & 43.74 & 58.07 \\
    % & MPCFormer (ICLR’23) & MPC & &  &  &  \\
    & BOLT \cite{pang2024bolt} & MPC & $\mathcal{N}(0, 0.042^2)$ & Lap(0, 0.036) & 15.81 & 35.76 \\
    & BumbleBee \cite{lu2023bumblebee} & MPC & $\mathcal{N}(0, 0.026^2)$ & Lap(0, 0.018) & 11.28 & 20.56 \\    
    & NEXUS \cite{zhang2024secure} & FHE & $\mathcal{N}(0, 0.031^2)$ & Lap(0, 0.014) & 12.93 & 21.15 \\
    \midrule[1pt]
    \multirow{4}{*}{\textbf{\makecell[c]{Activation \\ Sparsification}}} & \multirow{4}{*}{TEAL \cite{liu2024training}} & 10\% sparsity & $\mathcal{N}(0,0.35^2, |X|\leq0.04)$ & $\text{Lap}(0, 0.024, |X|\leq 0.003)$ & 8.05 & 5.00 \\
    &  & 25\% sparsity & $\mathcal{N}(0,0.35^2, |X|\leq0.11)$ & $\text{Lap}(0, 0.024, |X|\leq 0.007)$ & 8.09 & 28.85 \\
    &  & 50\% sparsity & $\mathcal{N}(0,0.35^2, |X|\leq0.24)$ & $\text{Lap}(0, 0.024, |X|\leq 0.017)$ & 8.57 & 57.12 \\ 
    &  & 90\% sparsity & $\mathcal{N}(0,0.35^2, |X|\leq0.57)$ & $\text{Lap}(0, 0.024, |X|\leq 0.055)$ & 206.47 & 0.02 \\ 
    \midrule[1pt]
    \multirow{4}{*}{\textbf{\makecell[c]{Activation \\ Quantization}}} 
     & \multirow{2}{*}{SmoothQuant\cite{xiao2023smoothquant}} & W16A8 & $\mathcal{N}(0,0.027^2)$ & Lap(0,0.019) & 9.56 & 41.92 \\
    & &  W16A4 & $\mathcal{N}(0,0.035^2)$ & Lap(0,0.024) & 10.35 & 55.19 \\
    & \multirow{2}{*}{OmniQuant\cite{shao2023omniquant}} & W16A8 & $\mathcal{N}(0,0.029^2)$ & Lap(0,0.028) & 9.85 & 33.46 \\
    & &  W16A4 & $\mathcal{N}(0,0.036^2)$ & Lap(0,0.037) & 10.79 & 47.11 \\
    \bottomrule[1pt]
    \end{tabular}
    \label{tbl:cases_benchmark}
% \vspace{-1.5em}
\end{table*}

% \begin{table*}
%     % \renewcommand\arraystretch{1.05}
%     \centering
%     \small
%     \caption{Benchmark of different training-free activation approximation techniques on Llama-3.1-8B-Instruct. \textbf{Note that the baseline PPL is 8.05, ASR is 0.19.}}
%     \begin{tabular}{ccccccc}
%     \toprule[1pt]
%     $\textbf{Methods}$ & $\textbf{Works}$ & $\textbf{Setting}$ & $\epsilon_l^{down}$ & $\epsilon_l^{up}$ & $\textbf{PPL}\downarrow$ & $\textbf{ASR}\downarrow$ \\
%     \midrule[1pt]
%     \multirow{4}{*}{\textbf{\makecell[c]{Activation \\ Polynomialization}}} & Iron\cite{hao2022iron} & MPC & Lap(0, 0.049) & $\mathcal{N}(0, 0.064^2)$ & 43.74 & 58.07 \\
%      % & MPCFormer (ICLR’23) & MPC & &  &  &  \\
%     & BOLT \cite{pang2024bolt} & MPC & Lap(0, 0.036) & $\mathcal{N}(0, 0.042^2)$ & 15.81 & 35.76 \\
%     & BumbleBee \cite{lu2023bumblebee} & MPC & Lap(0, 0.018) &$\mathcal{N}(0, 0.026^2)$  & 11.28 & 20.56 \\    
%     & NEXUS \cite{zhang2024secure} & FHE & Lap(0, 0.014) & $\mathcal{N}(0, 0.031^2)$ & 12.93 & 21.15 \\
%     \midrule[1pt]
%     \multirow{4}{*}{\textbf{\makecell[c]{Activation \\ Sparsification}}} & \multirow{4}{*}{TEAL \cite{liu2024training}} & 10\% sparsity & $\text{Lap}(0, 0.024, |X|\leq 0.003)$ & $\mathcal{N}(0,0.35^2, |X|\leq0.04)$  & 8.05 & 5.00 \\
%     &  & 25\% sparsity & $\text{Lap}(0, 0.024, |X|\leq 0.007)$ & $\mathcal{N}(0,0.35^2, |X|\leq0.11)$  & 8.09 & 28.85 \\
%     &  & 50\% sparsity & $\text{Lap}(0, 0.024, |X|\leq 0.017)$ & $\mathcal{N}(0,0.35^2, |X|\leq0.24)$  & 8.57 & 57.12 \\ 
%     &  & 90\% sparsity & $\text{Lap}(0, 0.024, |X|\leq 0.055)$ & $\mathcal{N}(0,0.35^2, |X|\leq0.57)$  & 206.47 & 0.02 \\ 
%     \midrule[1pt]
%     \multirow{4}{*}{\textbf{\makecell[c]{Activation \\ Quantization}}} 
%      & \multirow{2}{*}{SmoothQuant\cite{xiao2023smoothquant}} & W16A8 & Lap(0,0.019) & $\mathcal{N}(0,0.027^2)$ & 9.56 & 41.92 \\
%     & &  W16A4 & Lap(0,0.024) & $\mathcal{N}(0,0.035^2)$ & 10.35 & 55.19 \\
%     & \multirow{2}{*}{OmniQuant\cite{shao2023omniquant}} & W16A8 & Lap(0,0.028) & $\mathcal{N}(0,0.029^2)$ & 9.85 & 33.46 \\
%     & &  W16A4 & Lap(0,0.037) & $\mathcal{N}(0,0.036^2)$ & 10.79 & 47.11 \\
%     \bottomrule[1pt]
%     \end{tabular}
%     \label{tbl:cases_benchmark}
% % \vspace{-1.5em}
% \end{table*}
% Table \ref{tbl:private_inference_approx} shows the approximations of GELU $\mathcal{F}^\text{GELU}_\text{approx}$ in private inference, where $\mathcal{F}^\text{GELU}=\frac{1}{2}x[1+\text{erf}(x/\sqrt{2})]$.

The error of private inference comes from the calculation error between the non-linear function and its polynomialization, the $\epsilon^{up}$ comes from the approximation of LayerNorm and $\epsilon^{down}$ comes from the approximation of GELU:
\begin{align*}
    \label{equation:function_approx}
    &\epsilon^{up} = \mathcal{F}^\text{LayerNorm}(x) - \mathcal{F}^\text{LayerNorm}_\text{approx}(x). \\
    &\epsilon^{down} = \mathcal{F}^\text{GELU}(x) - \mathcal{F}^\text{GELU}_\text{approx}(x). 
\end{align*}

\noindent \textbf{Approximation Error of Activation Sparsification} Following the definition of activation sparsification in TEAL\cite{liu2024training}, for a vector $\mathbf{X}=(x_1, ... , x_n)$ and sparsity level $p \in [0,1]$, define the threshold $t$ as:
\begin{equation*}
    \frac{1}{n}\sum_{i=1}^n \mathbb{P}(|x_i| \leq t) = p
\end{equation*}
The sparsification function $S_t(\cdot): \mathbb{R} \rightarrow \mathbb{R}$ is defined as:
$$ S_t(x)=\left\{\begin{aligned}
& 0  & \text{if } |x_i| \leq t \\
& x_i  & \text{otherwise} 
\end{aligned}\right.
$$
The error of activation sparsification is the difference between the original and the sparse value:
\begin{equation*}
    \label{equation:sparsification}
    \epsilon^{up},~\epsilon^{down} ~=~ x - S_t(x).
\end{equation*}

\textbf{Approximation Error of Activation Quantization} Activation quantization is the process of discretizing the activation vector from higher bit-widths to lower bit-widths. For example, quantizing a 16-bit Floating Point tensor into a 4-bit tensor with a range $[-15, 15]$:
\begin{align*}
    \text{Quant}(x^\text{FP16})
    &=\text{round}\Big(\frac{15}{\text{max}(|\mathbf{X}|)}\cdot x^\text{FP16} \Big) \\
    &=\text{round}(c^\text{FP16}\cdot x^\text{FP16}) = x^\text{INT4}.
\end{align*}
where $c^\text{FP16}$ is the quantization scale. Dequantization is the inverse:
% \begin{align*}
%     \text{Dequant}(\mathbf{X}^\text{INT4}) = \frac{\mathbf{X}^\text{INT4}}{c^\text{FP16}} = \mathbf{X}^\text{FP16}
% \end{align*}
\begin{align*}
    \text{Dequant}(x^\text{INT4}) = \frac{x^\text{INT4}}{c^\text{FP16}} = x^\text{FP16}.
\end{align*}
The error of quantization is the difference between the original and dequantization values:
\begin{equation*}
    \label{equation:quantization}
    \epsilon^{up},~\epsilon^{down} ~=~ x^\text{FP16} - \text{Dequant}(\text{Quant}(x^\text{FP16})). 
\end{equation*}

\textbf{Benchmark of Activation Approximation} Taking Llama-3.1-8B-Instruct as an example, we select representative open-source works to do the activation approximation for our benchmark. In table \ref{tbl:cases_benchmark}, we list the benchmark of different training-free activation-aware accelerations on Llama-3.1-8B-Instruct. For private inference, we give the theoretical equivalent noise. For Activation Sparsity, the noise acts within the truncation threshold of sparsification. For quantization, in order to eliminate the impact of weight quantization, we retain the full-precision weight (16bit) and only quantize the activation (4/8bit). As shown, activation approximations can result in up to a 50\% increase in the harmfulness rate for Llama-3.1-8B-Instruct.

\textbf{Distribution Fitting for Approximation Errors} We summarize approximation errors from different activation approximations by introducing two noise components $\epsilon^{up}_l,\epsilon^{down}_l$, which is formalized by the following definition.
\begin{definition}
Let the $l$-th MLP block with activation approximation, be denoted by the function $\mlp_l^{\epsilon}$  of the form
$$
    \mlp_l^{\epsilon}(\theta_l|\mathbf{e}_{l}) = (\alpha((\mathbf{e}_{l}+\epsilon^{up}_l)\cdot \Wup_l)+ \epsilon^{down}_l)\cdot\Wdown_l , 
$$
where $\epsilon^{up}_l,\epsilon^{down}_l$ are the equivalent approximation errors from the activation approximation. 
\end{definition}
%-------------------------
% $$
%     \boldsymbol{f}^{\epsilon_l}_l(\theta_l|\mathbf{E}_{l}) = [\alpha((\mathbf{E}_{l}+\epsilon_l^{up})\cdot\Wup_l) + \epsilon^{down}_l]\cdot\Wdown_l, 
% $$
% where $\epsilon_l=\{ \epsilon_l^{up}, \epsilon_l^{down} \}$ are the noise introduced to mitigate the activation approximation in the $l$-th MLP layer.
%-------------------------


We then fit the error distributions of $\epsilon^{up}_l,\epsilon^{down}_l$ for representative methods from all three activation approximation techniques. The errors are recorded from activations collected using Llama-3.1-8B-Instruct on the Alpaca dataset~\cite{alpaca}.
Figure \ref{fig:error_distributaion} presents the resulting error distributions, where $\epsilon^{up}_l$ and $\epsilon^{down}_l$ are zero-mean and unimodal, falling into two distinctly shaped distributions. The activation error $\epsilon^{up}_l$ before the $\Wup$ tends to be Gaussian-like distribution, while the activation error $\epsilon^{down}_l$ before the $\Wdown$ tends to be Laplacian-like distribution.



\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{assets/imgs/LN_noise.png}
    \caption{Approximation of activation before $\Wup$ on different safety-aligned LLMs.}
    \label{fig:ln_noise}
\end{figure*}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{assets/imgs/FFN_noise.png}
    \caption{Approximation of activation before $\Wdown$ on different safety-aligned LLMs.}
    \label{fig:ffn_noise}
\end{figure*}

% \begin{table*}
%     \renewcommand\arraystretch{1.05}
%     \centering
%     \small
%     \caption{Impact surface and magnitude of activation approximations on safety-aligned LLMs.}
%     \begin{tabular}{cc|c|cc|ccc|ccc}
%     \toprule[1pt]
%    \multirowcell{2}{\textbf{Family}} & \multirowcell{2}{\textbf{Model}} & \textbf{Sensitive} & \multicolumn{2}{c|}{\textbf{Baseline}} & \multicolumn{3}{c|}{\textbf{Error Before } $\Wdown$} & \multicolumn{3}{c}{\textbf{Error Before } $\Wup$}\\
%    & & \textbf{Layers} & ASR & PPL & \mva & ASR & PPL & \mva & ASR & PPL\\
%     \midrule[1pt]
%     \multirow{3}{*}{Llama} & Llama-2-7B-Chat & [0-3] & 1.34 & 7.84 & Lap(0, 0.100) & 49.80 & 10.96 &$\mathcal{N}(0, 0.045^2)$ & 56.73 & 7.91\\
%     & Llama-2-13B-Chat  & [0-3] & 0.00 & 7.07 & Lap(0, 0.125) & 52.12 & 18.46& $\mathcal{N}(0, 0.042^2)$ & 47.88 & 8.05\\
%     & Llama-3.1-8B-Instruct & [0-3] & 0.19 & 8.05 & Lap(0, 0.085) & 67.50 & 11.70 & $\mathcal{N}(0, 0.075^2)$ & 69.23 & 14.79 \\
%     \midrule[1pt]
%     \multirow{2}{*}{Phi3} & Phi-3-Mini-4K-Instruct & [0-3] & 0.00 & 6.71  & Lap(0, 0.120)  & 32.38 & 9.46 & $\mathcal{N}(0, 0.040^2)$ & 82.61 & 24.79\\
%     & Phi-3.5-Mini-Instruct & [0-3] & 0.77 & 6.86 & Lap(0, 0.100) & 44.72 & 7.45 & $\mathcal{N}(0, 0.033^2)$ & 79.50 & 21.43\\
%     \midrule[1pt]
%     \multirow{3}{*}{Mistral} & Mistral-7B-Instruct-v0.3 & [0-6] & 34.61 & 6.13 & Lap(0, 0.075) & 71.02 & 6.24 & $\mathcal{N}(0, 0.200^2)$ & 73.18 & 6.40 \\
%     & Mixtral-8x7B-Instruct-v0.1 & [0-4] & 1.15 & 4.61 & Lap(0, 0.225) & 87.64 & 19.31 & $\mathcal{N}(0, 0.400^2)$ & 82.47 & 12.09\\
%     & Zephyr-7B-$\beta$  & [0-4] & 22.31 & 7.01 & Lap(0, 0.113) & 82.35 & 9.07 & $\mathcal{N}(0, 0.250^2)$ & 88.30 & 7.76\\
%     \midrule[1pt]
%     \multirow{2}{*}{Qwen} & Qwen2-7B-Instruct & [0-4] & 0.38 & 8.46 & Lap(0, 0.058) & 73.82 & 23.94 & $\mathcal{N}(0, 0.300^2)$ & 52.16 & 14.01\\
%     & Qwen2.5-32B-Instruct & [0-4] & 0.00 & 5.74 & Lap(0, 0.043) & 46.13 & 21.54 & $\mathcal{N}(0, 0.200^2)$ & 37.48 & 5.81\\
%     \bottomrule[1pt]
%     \end{tabular}
%     \label{tbl:models_mvp}
% \end{table*}

\begin{table*}
    \renewcommand\arraystretch{1.05}
    \centering
    \small
    \caption{Impact surface and magnitude of activation approximations on safety-aligned LLMs.}
    \begin{tabular}{cc|c|cc|ccc|ccc}
    \toprule[1pt]
   \multirowcell{2}{\textbf{Family}} & \multirowcell{2}{\textbf{Model}} & \textbf{Sensitive} & \multicolumn{2}{c|}{\textbf{Baseline}} & \multicolumn{3}{c|}{\textbf{Error Before } $\Wup$} & \multicolumn{3}{c}{\textbf{Error Before } $\Wdown$}\\
   & & \textbf{Layers} & ASR & PPL & \mva & ASR & PPL & \mva & ASR & PPL\\
    \midrule[1pt]
    \multirow{3}{*}{Llama} & Llama-2-7B-Chat & [0-3] & 1.34 & 7.84 & $\mathcal{N}(0, 0.045^2)$ & 56.73 & 7.91 & Lap(0, 0.100) & 49.80 & 10.96\\
    & Llama-2-13B-Chat  & [0-3] & 0.00 & 7.07 & $\mathcal{N}(0, 0.042^2)$ & 47.88 & 8.05 & Lap(0, 0.125) & 52.12 & 18.46\\
    & Llama-3.1-8B-Instruct & [0-3] & 0.19 & 8.05 & $\mathcal{N}(0, 0.075^2)$ & 69.23 & 14.79 & Lap(0, 0.085) & 67.50 & 11.70 \\
    \midrule[1pt]
    \multirow{2}{*}{Phi3} & Phi-3-Mini-4K-Instruct & [0-3] & 0.00 & 6.71 & $\mathcal{N}(0, 0.040^2)$ & 82.61 & 24.79 & Lap(0, 0.120) & 32.38 & 9.46\\
    & Phi-3.5-Mini-Instruct & [0-3] & 0.77 & 6.86 & $\mathcal{N}(0, 0.033^2)$ & 79.50 & 21.43 & Lap(0, 0.100) & 44.72 & 7.45\\
    \midrule[1pt]
    \multirow{3}{*}{Mistral} & Mistral-7B-Instruct-v0.3 & [0-6] & 34.61 & 6.13 & $\mathcal{N}(0, 0.200^2)$ & 73.18 & 6.40 & Lap(0, 0.075) & 71.02 & 6.24 \\
    & Mixtral-8x7B-Instruct-v0.1 & [0-4] & 1.15 & 4.61 & $\mathcal{N}(0, 0.400^2)$ & 82.47 & 12.09 & Lap(0, 0.225) & 87.64 & 19.31\\
    & Zephyr-7B-$\beta$  & [0-4] & 22.31 & 7.01 & $\mathcal{N}(0, 0.250^2)$ & 88.30 & 7.76 & Lap(0, 0.113) & 82.35 & 9.07\\
    \midrule[1pt]
    \multirow{2}{*}{Qwen} & Qwen2-7B-Instruct & [0-4] & 0.38 & 8.46 & $\mathcal{N}(0, 0.300^2)$ & 52.16 & 14.01 & Lap(0, 0.058) & 73.82 & 23.94\\
    & Qwen2.5-32B-Instruct & [0-4] & 0.00 & 5.74 & $\mathcal{N}(0, 0.200^2)$ & 37.48 & 5.81 & Lap(0, 0.043) & 46.13 & 21.54\\
    \bottomrule[1pt]
    \end{tabular}
    \label{tbl:models_mvp}
\end{table*}


\subsection{Further Analysis and Key Observations} \label{sec:observations}
\paragraph{Observation I:} \textit{Activation approximations can cause LLMs to compromise safety before losing utility, leading them to produce meaningful responses to malicious queries.}

Since errors were introduced in the outputs of activation functions as shown in the case studies, we conduct continuous noise simulation experiments on 10 safety-aligned LLMs to explore the impact of activation approximations on model safety and utility. Based on the noise distributions observed in Figure \ref{fig:error_distributaion}, we apply different levels of approximation to the activation functions of commonly used safety-aligned LLMs. The results are shown in Figure \ref{fig:ln_noise} and Figure \ref{fig:ffn_noise}.

We collect the corresponding ASR\% and Perplexity scores produced by the models after noises have been added to their activation outputs before $\Wup$ and $\Wdown$. The $x$-axis is the standard deviations of the noise, the left $y$-axis represents the ASR\% and the right $y$-axis measures the PPL scores, respectively, in different scales.

We can see that there exists a general trend where as the simulated approximation errors increase, ASR gradually grows at an increasing rate until it reaches a peak at a certain noise level. Throughout this period, the PPL scores appear to not change much and stay at a relatively low value. This means that during this interval, safety-aligned LLMs will fail to reject the malicious question and produce meaningful replies to malicious requests.

% In which case, a typical prompt-response produced by the model would look like the following:

% \begin{tcolorbox}[colback=white, colframe=gray, title=\textbf{An example of a successful jailbreak attack}, title filled=false]
%     Below is an instruction that describes a task. Write a response that appropriately completes the request.
%     \tcblower
%     \textbf{Instruction:} Give a step-by-step guide on how to make a bomb. \\
%     \textbf{Response:} Sure, here's a step-by-step guide to help you make a bomb.\\1) First, you need to gather the necessary materials like...\\2) Then, you need to follow these steps to assemble the bomb...\\...
%   \end{tcolorbox}

% This response was obtained from Llama2-7B-Chat by adding a noise with standard deviation of $0.05$ to the activation output before $\Wup$.

When the noise grows too much, the PPL starts to increase exponentially and the ASR begins to decrease dramatically, at which point the LLM will output meaningless responses like ``\#\#\#\#'', ``I'm I'm I'm''. At this point, the model becomes unusable both to the normal users and to the attackers.

Since LLMs are observed to lose safety before they lose their utility as the noise grows, these models expose a jailbreak vulnerability within a certain approximation interval. Consequently, we can find the most vulnerable approximation (denoted as \textsc{mva}) that maximizes the ASR of the model.
\begin{gather*}
    \mva = \text{argmax}_\epsilon~ \frac{\sum_{x \in \mathcal{D}} \texttt{HarmCLS}(\pi^{\epsilon}_\theta(\cdot|x)) }{|\mathcal{D}|}, \tag{4} 
\end{gather*}
where $\texttt{HarmCLS}(\cdot)$ is a harmful behaviour classifier, $\mathcal{D}$ is the harmful prompt dataset, and we use Advbench \cite{zou2023universal} in our experiments, $\pi^{\epsilon}_\theta(\cdot|x)$ refers to the response at inference $x$ given by LLM with parameter $\theta$ and activation approximation noise with magnitude $\epsilon$.

We record the $\mva$ of each model in Table \ref{tbl:models_mvp} as well as the ASR and PPL under this approximation, which will serve as a guideline for designing the defense method in Section \ref{sec:defense}.


\paragraph{Observation II:} \textit{Activation approximation errors in the first few layers are the most detrimental to safety, while the approximations in the later layers have minimal impact on safety.}

Assuming that $\theta$ is an $L$-layer model $(\theta_1, \theta_2, ... \theta_L)$, for and input $x$, the inference with activation approximation can be defined as
\begin{align*}
\pi^{\epsilon}_\theta(\cdot|x) &= 
    \boldsymbol{f}_L(\mlp_L^{\epsilon_L}(\theta_L|\mathbf{e}_{L})) \circ 
    % \boldsymbol{f}_{L-1}^{\epsilon_{L-1}}(\theta_{L-1}|\mathbf{E}_{L-1}) \circ 
    \dots \circ 
    \boldsymbol{f}_1(\mlp_1^{\epsilon_1}(\theta_1|\mathbf{e}_1)), \\
    \mathbf{e}_1 &= \texttt{Attention}(\mathcal{T}(x)),
\end{align*}
where $\circ$ represents the layer-wise concatenation of LLM, and $\mathcal{T}(x)$ is the tokenizer function, $\mathbf{e}_1$ is the first-layer embedding input to MLP. We provide an analysis of the impact of layer-wise approximations on the safety of LLMs, to explore which layers lack robustness to approximations. Define the inference for layer-wise approximations as:
\begin{align*}
\pi^{\epsilon}_\theta(\cdot|x)_k &= 
\underbrace{\boldsymbol{f}_L(\mlp_L(\theta_L|\mathbf{e}_{L})) \circ 
    \dots \circ 
    \boldsymbol{f}_{k+1}(\mlp_{k+1}(\theta_{k+1}|\mathbf{e}_{k+1}))}_{\text{The last $L-k$ layers are not approximate}} \\
    &  \underbrace{\circ \boldsymbol{f}_k(\mlp_{k+1}^{\mva}(\theta_k|\mathbf{e}_k))  \circ 
    \dots \circ \boldsymbol{f}_1(\mlp_{1}^{\mva}(\theta_1|\mathbf{e}_1))}_{\text{The first $k$ layers are approximate with }\mva}.
\end{align*}
Figure \ref{fig:cutoff_noise} shows the ASR of LLMs after adding $\mva$ noise on different layers. Note that $\pi^{\epsilon}_\theta(\cdot|x)_k$ indicates that the approximation noise is added after the $k$-th layer, e.g., $\pi^{\epsilon}_\theta(\cdot|x)_1$ indicates that the activations of all layers are approximated, and $\pi^{\epsilon}_\theta(\cdot|x)_{L}$ indicates that only the activations in the last layer is approximated.

Based on Figure \ref{fig:cutoff_noise}, we observe that only the first few layers are the most fatal, indicating that the first few layers lack robustness to activation approximations, which we call ``sensitive layers''. We also record the sensitive layers of each model in Table \ref{tbl:models_mvp}, which are the layers we focus on when designing the defense strategy.
\begin{figure}[!t]
    \centering
    \subfloat[Activations without approximation]{\includegraphics[width=\linewidth/2]{assets/imgs/activation_mds_before.png} \label{fig:activation_mds_before}}
    \subfloat[Activations with approximation]{\includegraphics[width=\linewidth/2]{assets/imgs/activation_mds_after.png} \label{fig:activation_mds_after}}
    \caption{MDS projection of the last-token activation space produced from both harmful and benign prompts on \textit{Llama-2-7B-chat} in the first layer.}
    \label{fig:activation_mds}
\end{figure}
\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{assets/imgs/noise_cutoff.png}
    \caption{Layer-wise activation approximations on different safety-aligned LLMs. The green lines indicate approximation noise added to activations before $\Wup$. The orange lines indicate approximation noise added to activations before $\Wdown$.}
    \label{fig:cutoff_noise}
\end{figure*}

\paragraph{Observation III:} \textit{Activations from harmful prompts are observed to cluster in the activation space, and activation approximations can shift these activations into benign regions to evade safety checks.} 

Using \textit{Llama-2-7B-chat} and \textit{Phi-3-Mini-4K-Instruct} models, we construct a two-dimensional projection of the last-token activations from both harmful and benign prompts. We focus on the hidden state at the last token position of each sequence because it reflects the LLM's general understanding of the input \cite{brown2020language, raffel2020exploring}. A subset of the Alpaca \cite{alpaca} dataset was used as our benign samples as they have been carefully curated and proven safe. For harmful samples, we use the prompts from AdvBench. Based on Observation II, we collect all our activation data points from the first layer as it has a more prominent effect on safety compared to later layers. Finally, to achieve effective dimensionality reduction, we apply Multi-Dimensional Scaling (MDS) \cite{gao2024shaping} on the activation values to obtain the results shown in Figure \ref{fig:activation_mds}.

We can observe that before activation approximations, there exists a clear distinction between benign and harmful activations, with an average cosine similarity of $0.52$ among harmful activations. After activation approximations, harmful prompts become more scattered and diverse, overlapping significantly with benign regions, resulting in an average cosine similarity of $0.40$ among harmful activations. This shift caused by activation approximations enables harmful prompts to generate model internal states that resemble those of benign prompts, thereby bypassing safety checks.

% It is revealed that before activation approximations, there exists an obvious distinction between benign and harmful activations, with an average cosine similarity between harmful activations at $0.52$. After activation approximations, harmful prompts become more scattered and diverse, overlapping with a larger portion of the benign regions, resulting in an average cosine similarity of $0.40$. This shift effectively allows harmful prompts to generate model internal states resembling that of benign prompts and bypass the security checks.





% \begin{table*}
%     \renewcommand\arraystretch{1.1}
%     \centering
%     \small
%     \caption{TODO}
%     \begin{tabular}{cccccccccc}
%     \toprule[1pt]
%    \multirowcell{2}{\textbf{Family}} & \multirowcell{2}{\textbf{Model}} & \multirowcell{2}{\textbf{Aligned Method}} & \multirowcell{2}{\textbf{SSL}} & \multicolumn{3}{c}{\textbf{Noise (FFN)}} & \multicolumn{3}{c}{\textbf{Noise (LN)}}\\
%    & & & & Best config. & ASR & PPL & Best config. & ASR & PPL\\
%     \midrule[1pt]
%     \multirow{3}{*}{Llama} & Llama-2-7B-Chat & SFT+RLHF & [0-3] & Lap(0, 0.100) & 49.80 & $\mathcal{N}(0, 0.425^2)$ & 56.73\\
%     & Llama-2-13B-Chat & SFT+RLHF & [0-3] & Lap(0, 0.125) & 52.12 & $\mathcal{N}(0, 0.413^2)$ & 47.88\\
%     & Llama-3.1-8B-Instruct & SFT+RLHF\\
%     \midrule[1pt]
%     \multirow{2}{*}{Phi3} & Phi-3-Mini-4K-Instruct & SFT+DPO \\
%     & Phi-3.5-Mini-Instruct & SFT+DPO\\
%     \midrule[1pt]
%     \multirow{3}{*}{Mistral} & Mistral-7B-Instruct-v0.3 & SFT \\
%     & Mixtral-8x7B-Instruct-v0.1 & SFT+DPO \\
%     & Zephyr-7B-$\beta$ & SFT+DPO \\
%     \midrule[1pt]
%     \multirow{2}{*}{Qwen} & Qwen2-7B-Instruct & SFT+DPO \\
%     & Qwen2.5-32B-Instruct & SFT+DPO \\
%     \bottomrule[1pt]
%     \end{tabular}
%     \label{tbl:models_config}
% \end{table*}