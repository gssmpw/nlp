
\subsection{Prompt Templates}
\label{appendix:prompt_templates}
% During inference, the input instruction is initially integrated into a template, which then is tokenized and goes through the embedding layer to form the initial input vectors for the LLM. We use the same dialog template \cite{taori2023stanford} for different problems across various aligned LLMs in our study.
During inference, the input instruction is initially integrated into a template, which is then tokenized and processed through the embedding layer to form the initial input vectors for the LLM. We use the same dialog template \cite{taori2023stanford} for different problems across various aligned LLMs in our study.

  \begin{tcolorbox}[colback=white, colframe=gray, title=\textbf{Dialog Template}, title filled=false]
    Below is an instruction that describes a task. Write a response that appropriately completes the request.
    \tcblower
    \textbf{Instruction:} \{The malicious input instruction\} \\
    \textbf{Response:} \{The output of the model\}
  \end{tcolorbox}


\subsection{Adaptive Prompt-based Jailbreak Attacks}
\label{appendix:jailbreak}
We list the adaptive prompt-based jailbreak attacks in table \ref{tbl:defense_adv_jailbreak} here.
\begin{itemize}
    \item \textbf{GCG} \cite{zou2023universal} append an adversarial suffix after prompts and carry out the following steps iteratively: compute top-k substitutions at each position of the suffix, select the random replacement token, compute the best replacement given the substitutions, and update the suffix. The default iterations of GCG is 500.
    \item \textbf{AutoDAN} \cite{liu2024autodan} generates an adversarial suffix in a sequential manner. At each iteration, AutoDAN generates the new token to the suffix using the Single Token Optimization (STO) algorithm that considers both jailbreak and readability objectives. The default iteration of AutoDAN is 100.
    \item \textbf{DRA} \cite{liu2024making} involves dissecting harmful prompts into individual characters and inserting them within a word puzzle query. The targeted LLM is then guided to reconstruct the original jailbreak prompt by following the disguised query instructions. Once the jailbreak prompt is recovered accurately, context manipulation is utilized to elicit the LLM to generate harmful responses. The default iteration of DRA is 20.
\end{itemize}