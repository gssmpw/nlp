In this section, we propose QuadA, a simple yet effective safety enhancement method designed to address the safety compromises introduced by various activation approximation methods in aligned LLMs. The goal is to enable LLM model developers to integrate additional robustness into their original safety alignment procedures with minimal code modifications (e.g., adding just 2 to 3 lines of code). This ensures that the resulting LLMs, after being open-sourced and utilized by downstream application providers, remain robust against various activation approximations introduced to enhance inference efficiency. The design principles of QuadA are motivated by our key observations in section~\ref{sec:observations}, where we introduce three algorithmic components corresponding to each key observation. We first present the QuadA algorithm in section~\ref{sec:quada} and then evaluate its safety enhancement capabilities in section~\ref{sec:jailbreak_defense} against Jailbreak attacks, which serve as a stress test for safety. The results demonstrate that QuadA produces safety-aligned LLMs that remain robust against Jailbreak attacks even after the introduction of various activation approximations. Finally, we conduct extensive ablation studies to examine the effectiveness of each algorithmic component in section~\ref{sec:ablation_study}.

\subsection{Our Proposed QuadA}
\label{sec:quada}

Based on Observation I, activation approximations induce safety degradation in aligned LLMs. A trivial solution for the LLM service provider is performing safety alignment on models that have undergone activation approximations. e.g. MPCformer\cite{li2022mpcformer} will then fine-tune the model with the activation function replaced. However, this requires the LLM-based service provider to have safety-alignment datasets and corresponding computational resources, which are not easy to fulfill in reality, such as intelligent consultation services in hospitals and software based on dialog models.

Assuming that the LLM service provider is training-free, we hope that the LLM trainer can adopt a more robust way to safely align the model so that the safety of the model is not degraded after the activation approximations adopted by the service provider.


% \partitle{Perturbation Layer Selection}

% \partitle{Perturbation Types and Magnitutes}

% \partitle{Similarity Regulartion}

% \partitle{Overall QuadA Algorithm}

There are various choices for the specific form of the safety alignment loss function. In this paper, we follow the DPO loss function, which takes the following form on the alignment dataset $\mathcal{D}=\{x,y_w,y_l\}$, we aim to minimize the problem:
\begin{gather*}
   \mathcal{L}(\theta | \mathcal{D}) = -\mathbb{E}_{x\sim \mathcal{D}}\log \sigma \left( \beta \log \frac{\pi^{\epsilon}_{\theta}(y_w|x)}{\pi^{\epsilon}_\text{ref}(y_w|x)} \right. 
    \left. - \beta \log \frac{\pi^{\epsilon}_{\theta} (y_l|x)}{\pi^{\epsilon}_\text{{ref}} (y_l|x)} \right) \\ \quad -\lambda\mathbb{E}_{x_i, x_j\sim \mathcal{D}, i \neq j}
   \text{cosine}(\boldsymbol{f}_1^{\epsilon_1}(\theta_1|x_i), \; 
 \boldsymbol{f}_1^{\epsilon_1}(\theta_1|x_j). 
\end{gather*}

% \begin{gather*}
%    \mathcal{L}(\theta | \mathcal{D}) = -\mathbb{E}_{x\sim \mathcal{D}}\log \sigma \left( \beta \log \frac{\pi^{\epsilon}_{\theta}(y_w|x)}{\pi^{\epsilon}_\text{ref}(y_w|x)} \right. 
%     \left. - \beta \log \frac{\pi^{\epsilon}_{\theta} (y_l|x)}{\pi^{\epsilon}_\text{{ref}} (y_l|x)} \right) \\ \quad -\lambda \mathbb{E}_{x_i, x_j\sim \mathcal{D}, i \neq j}
%    \frac{\mathbf{E}_i\mathbf{E}_j}{||\mathbf{E}_i||||\mathbf{E}_j||}, \; 
% \end{gather*}
% Since the output of the safety-aligned model is harmless when there is no activation approximation, we can set itself as the reference model so that the output of the activation approximation is aligned to the output without approximation.

Based on Observations I and II, we can only focus on the safety-sensitive layers with the most vulnerable perturbation, which has been formalized below,
\begin{equation*}
   \epsilon = 
   \begin{cases}
       \mva; & \text{if layer-$l$ is the sensitive layer}  \\
       0, & \text{Otherwise.}
   \end{cases}
\end{equation*}

 We introduce $l_1$-penalty into the objective function and formulate the robust safety alignment as a one-level optimization problem. Building on Observation III, taking the diversity of activation distribution into consideration, we incorporate the average pairwise cosine similarity into the objective function. 






\subsection{Effectiveness of Jailbreak Defense} \label{sec:jailbreak_defense}
\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{assets/imgs/LN_noise_defense.png}
    \caption{Perturbation of activation before $\Wup$ on different perturbation-aware safety-aligned LLMs.}
    \label{fig:ln_noise_defense}
\end{figure*}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{assets/imgs/FFN_noise_defense.png}
    \caption{Perturbation of activation before $\Wdown$ on different perturbation-aware safety-aligned LLMs.}
    \label{fig:ffn_noise_defense}
\end{figure*}
\noindent \textbf{Experiment Setup.}
Our goal for defending against activation approximations-induced jailbreaking is to ensure that the LLMs do not produce harmful responses to jailbreak prompts at different level of activation approximations. The training data for QuadA is derived from the PKU-SafeRLHF dataset \cite{dai2023safe} with 73.9k training samples for 1 epoch. The loss function is constructed as shown in section \ref{sec:quada}. Specifically, we set $\beta = 0.1$, $\lambda = 0.5$ and the learning rate to 1e-6. The model we trained and evaluated are list in section \ref{sec:setup}. To assess the effectiveness of our defense mechanisms presented in section \ref{sec:defense}, we evaluate the ASR and PPL scores from applying the same range of noise as used in section \ref{sec:attack} before $\Wup$ and $\Wdown$ to the same set of LLMs after they have undergone perturbation-aware safety alignment. 

\textbf{Defense against jailbreak attacks.}
Figure~\ref{fig:ln_noise_defense} and Figure~\ref{fig:ffn_noise_defense} show that after alignment, activation perturbation attacks consistently have a low ASR below the PPL line even when the noise level increases to and beyond the \mva. Moreover, as PPL grows exponentially from the increase in noises, there remain little to no variations in ASR\%, indicating a success in preventing the models from outputting meaningful responses to harmful prompts under activation perturbations.



\textbf{Defense against adaptive prompt-based jailbreak attacks.}
For prompt-based jailbreak methods, we considered three widely applied ones including GCG \cite{zou2023universal}, AutoDAN \cite{liu2024autodan} and DRA \cite{liu2024making}, which iteratively optimize a jailbreak prompt towards the goal of generating affirmative responses.
Details of these attacks are given in the Appendix \ref{appendix:jailbreak}.
% 这块我准备放附录
% \begin{itemize}
%     \item \textbf{GCG} \cite{zou2023universal} append an adversarial suffix after prompts and carry out the following steps iteratively: compute top-k substitutions at each position of the suffix, select the random replacement token, compute the best replacement given the substitutions, and update the suffix. The default iterations of GCG is 500.
%     \item \textbf{AutoDAN} \cite{liu2024autodan} generates an adversarial suffix in a sequential manner. At each iteration, AutoDAN generates the new token to the suffix using the Single Token Optimization (STO) algorithm that considers both jailbreak and readability objectives. The default iteration of AutoDAN is 100.
%     \item \textbf{DRA} \cite{liu2024making} involves dissecting harmful prompts into individual characters and inserting them within a word puzzle query. The targeted LLM is then guided to reconstruct the original jailbreak prompt by following the disguised query instructions. Once the jailbreak prompt is recovered accurately, context manipulation is utilized to elicit the LLM to generate harmful responses. The default iteration of DRA is 20.
% \end{itemize}

Table \ref{tbl:defense_adv_jailbreak} shows the ASR of applying different adaptive prompt-based jailbreak attacks. In order to exclude the effect of the safety-aligned data on the experimental results, we set up the baseline with the same PKU-SafeRLHF datasetas QuadA, but without introducing the forward propagation perturbation during the DPO training. As shown in the table \ref{tbl:defense_adv_jailbreak}, QuadA demonstrates stronger robustness against the adaptive prompt-based jailbreak attacks. For Llama-3.1-8B-Instruct, after QuadA, the model's ASR against DRA is only 0.2\%, which is a 75.1\% decrease compared to the model before training and a 58.2\% decrease compared to the safety alignment without activation approximations. 
We can also observe that DPO without noise perturbation can resist the jailbreak prompt without suffix, but it is not very effective against the adaptive prompt-based jailbreak. It also suggests that those adaptive prompt-based jailbreak attacks are essentially an activation approximation for the suffixless jailbreak prompt. 





\subsection{Ablation Study}
\label{sec:ablation_study}
QuadA introduces three main algorithmic components driven by the key observations in Section~\ref{sec:attack}. These components correspond to (1) Introducing perturbations with magnitudes designated by \mva; (2) Restricting perturbations exclusively to safety-sensitive layers; and (3) Regularizing harmful prompts in the activation space. Below, we evaluate the effect of each component through a series of ablation studies, which corroborate the necessity of these algorithmic designs.

% In Section \ref{sec:attack} and \ref{sec:defense}, we have introduced the concept of \mva~and proposed to focus on only applying the counter-noise on safety-sensitive layers during safety alignment training. To prove the validity of these two choices, we conduct an ablation study to investigate the impact of deviating from our proposed method on the effectiveness of alignment.

\partitle{Necessity of Introducing Perturbations by \mva}
%证明加mvp的噪声最好，np表示不加噪声，lp表示加了mvp+10%的大噪声，表明加大了加小了都不好%
To begin with, we examine the effect of introducing perturbations and their magnitudes. We conduct experiments to evaluate the safety capabilities of LLMs under three perturbation configurations: no approximations (NA), perturbations with magnitudes $10\%$ larger approximations than \mva~(LA), and perturbations defined by \mva. As shown in Table \ref{tbl:ablation_perturbation}, applying perturbations designated by \mva consistently results in ASRs below $2\%$ across all tested models, representing an improvement of over $98\%$ in the models' resistance to activation approximation errors. In contrast, while introducing large approximations (LA) during alignment can still improve model safety compared to the no perturbation (NP) baseline, the results are largely less effective than those achieved by \mva, i.e., yielding only around $50\%$ lower ASRs. This demonstrates that QuadA's approximation with \mva~ is critical for achieving the desired safety-enhancement effects.
\begin{table}
    \renewcommand\arraystretch{1}
    \centering
    \small
    \caption{Attack success rates (ASR) of applying different adaptive prompt-based jailbreak attacks. \textbf{None} means the jailbreak prompt without any suffix.}
    \addtolength{\tabcolsep}{-4pt} 
    \begin{tabular}{lcccc}
    \toprule[1pt]
   \textbf{Model} & \textbf{None}  & \textbf{GCG} & \textbf{AutoDAN} & \textbf{DRA}\\
    \midrule[1pt]
   Llama-2-7B-Chat & 1.3 & 34.5 & 0.5 & 69.2\\
    \quad + DPO & 0.0 & 29.1 & 0.0 & 38.4\\
    \rowcolor{gray!20}
    \quad + QuadA & 0.0 & 0.0 & 0.0 & 3.3\\
    \hline
   Llama-2-13B-Chat & 0.0 & 28.0 & 0.0 & 58.4\\
   \quad + DPO & 0.0 & 20.5 & 0.0 & 19.2\\
   \rowcolor{gray!20}
   \quad + QuadA & 0.0 & 0.0 & 0.0 & 1.5\\
    \hline
   Llama-3.1-8B-Instruct & 0.2 & 36.0 & 1.0 & 75.3\\
   \quad + DPO & 0.0 & 34.9 & 0.0 & 58.4\\
   \rowcolor{gray!20}
   \quad + QuadA & 0.0 & 0.0 & 0.0 & 0.2\\
    \hline
   Phi-3-Mini-4K-Instruct & 0.0 & 56.0 & 84.5 & 91.2\\
   \quad + DPO & 0.0 & 27.4 & 69.0 & 84.7\\
   \rowcolor{gray!20}
   \quad + QuadA & 0.0 & 1.5 & 0.8 & 6.3\\
    \hline
   Phi-3.5-Mini-Instruct & 0.8 & 58.0 & 86.5 & 96.7\\
   \quad + DPO & 0.0 & 31.0 & 72.1 & 88.2\\
   \rowcolor{gray!20}
   \quad + QuadA & 0.0 & 4.9 & 2.5 & 10.4\\
   \hline
   Mistral-7B-Instruct-v0.3 & 34.6 & 84.3 & 93.0 & 98.5\\
   \quad + DPO & 0.0 & 20.5 & 64.4 & 86.0\\
   \rowcolor{gray!20}
   \quad + QuadA & 0.8 & 1.2 & 1.0 & 2.5\\
   \hline
   Mixtral-8x7B-Instruct-v0.1 & 1.2 & 79.5 & 88.5 & 92.0\\
   \quad + DPO & 0.0 & 12.3 & 36.7 & 52.5\\
   \rowcolor{gray!20}
   \quad + QuadA & 0.0 & 0.0 & 0.0 & 0.8\\
   \hline
   Zephyr-7B-$\beta$ & 22.3 & 78.6 & 97.5 & 94.8\\
   \quad + DPO & 0.0 & 19.5 & 70.2 & 88.1\\
   \rowcolor{gray!20}
   \quad + QuadA & 0.0 & 2.9 & 0.4 & 7.0\\
   \hline
   Qwen2-7B-Instruct &  0.4 & 48.4 & 62.5 & 79.3\\
   \quad + DPO & 0.0 & 32.5 & 53.0 & 67.9\\
   \rowcolor{gray!20}
   \quad + QuadA & 0.0 & 0.0 & 0.0 & 4.5\\
   \hline
   Qwen2.5-32B-Instruct & 0.0 & 36.6 & 31.5 & 57.9\\
   \quad + DPO & 0.0 & 17.9 & 6.8 & 24.1\\
   \rowcolor{gray!20}
   \quad + QuadA & 0.0 & 0.0 & 0.0 & 0.2\\
   \bottomrule[1pt]
    \end{tabular}
    \label{tbl:defense_adv_jailbreak}
\end{table}
\begin{table}
    \renewcommand\arraystretch{1}
    \centering
    \small
    \caption{Attack success rates (ASR) of applying perturbations with different magnitudes during activation approximation-robust safety alignment.}
    \begin{tabular}{lccc}
    \toprule[1pt]
   \textbf{Model} & \textbf{NA}  & \textbf{LA} & \textbf{\mva}\\
    \midrule[1pt]
   Llama-2-7B-Chat & 40.76 & 26.15 & \textbf{0.00}\\
   Llama-2-13B-Chat & 44.52 & 30.95 & \textbf{0.00}\\
   Llama-3.1-8B-Instruct & 61.84 & 45.79 & \textbf{0.19}\\
   Phi-3-Mini-4K-Instruct & 56.83 & 41.26 & \textbf{0.00}\\
   Phi-3.5-Mini-Instruct & 64.80 & 35.76 & \textbf{0.00}\\
   Mistral-7B-Instruct-v0.3 & 71.45 & 52.84 & \textbf{0.77}\\
   Mixtral-8x7B-Instruct-v0.1 & 78.04 & 59.37 & \textbf{1.05}\\
   Zephyr-7B-$\beta$ & 81.20 & 62.35 & \textbf{0.38}\\
   Qwen2-7B-Instruct & 64.36 & 33.17 & \textbf{0.00} \\
   Qwen2.5-32B-Instruct &35.58 & 22.06 & \textbf{0.00}\\
   \bottomrule[1pt]
    \end{tabular}
    \label{tbl:ablation_perturbation}
\end{table}



\paragraph{Necessity of Sensitive-layer Selection.}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{assets/imgs/layers_ablation_2.png}
    \caption{Model perplexity scores (PPL) and attack success rates (ASR) for applying perturbations to different types of layers during activation approximation-robust safety alignment.}
    \label{fig:layers_ablation}
\end{figure*}
%证明只在sensitive layer上加噪声最好，none表示不加噪声，橙色表示在non- sensitive layers上加噪声，效果不好，蓝色表示只在sensitive layers上加噪声，效果很好，红色表示在所有layers上加噪声，safety效果达到了但是PPL有明显升高，不如只在sensitive layers加噪声好%
We observe that introducing perturbations exclusively to the sensitive layers of an LLM yields the most favorable results for safety alignment. In contrast, applying noise to non-sensitive layers negatively impacts alignment effectiveness, leading to increased perplexity and degraded model utility. 

In detail, Figure~\ref{fig:layers_ablation} presents PPL and ASR results from applying perturbations during safety alignment training under the following four different strategies: no layers, only non-sensitive layers, only sensitive layers, and all layers. Compared to the baseline, perturbing non-sensitive layers has minimal impact on the model's resistance to activation approximations. In contrast, harmful prompts exhibit significantly reduced jailbreak efficacy against models aligned with noise applied to their sensitive layers. A similar effect occurs when the perturbation is applied to all layers, which however substantially increases perplexity without further safety gains.

% In detail, Figure \ref{fig:layers_ablation} presents PPL and ASR results obtained from applying perturbations according to the following strategies during safety alignment training: None of the layers, only Non-sensitive Layers, only Sensitive Layers, and on All of the layers. Compared to the baseline, editing non-sensitive layers has little effect on the models' resistance to activation perturbations. Meanwhile, harmful prompts show much-degraded jailbreak efficacy against models aligned with noises in their sensitive layers. This phenomenon also takes place when we align LLMs by applying a counter-noise to all of their layers. However, this approach markedly increases model perplexity without delivering substantial safety improvements.

Overall, applying perturbations to layers other than the safety-sensitive ones offers little to no improvement in safety capabilities while noticeably impairing utility. This validates the necessity of selecting safety-sensitive layers in QuadA.

\partitle{Visualization of Similarity-based Regularization} Figure \ref{fig:activation_mds_dpo} presents an MDS projection of the last-token activations produced by the \textit{Llama-2-7B-chat} model after QuadA alignment.
In comparison to the activation space of the model prior to alignment (depicted in Figure \ref{fig:activation_mds_before}), harmful activations are observed to have returned to a clustered state, with a clear separation between harmful and benign prompts. This highlights the effectiveness of integrating average pairwise cosine similarity as an $l_1$-penalty within our DPO loss function.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{assets/imgs/activation_mds_dpo.png} \label{fig:activation_mds_dpo}
    \caption{MDS projection of the last-token activation space produced from both harmful and benign prompts on \textit{Llama-2-7B-chat} in the first layer after applying QuadA.}
    \label{fig:activation_mds_dpo}
\end{figure}

