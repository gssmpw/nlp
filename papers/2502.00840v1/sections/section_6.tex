In this paper, we have systematically investigated the safety implications of activation approximation techniques in Large Language Models (LLMs), uncovering significant vulnerabilities that remain unaddressed by current safety alignment infrastructures. Our safety vetting has revealed that these approximations can substantially compromise model safety, elevating attack success rates without degrading utility. Through extensive analyses, we identified critical layers and harmful activation patterns that exacerbate these vulnerabilities, highlighting the disproportionate impact of early-layer approximations on safety.

To address these challenges, we have proposed the first activation approximation-robust safety alignment method, which mitigates safety risks by optimizing sensitive layers with diverse activation distributions. Our approach not only withstands various levels of activation perturbations but also demonstrates robust resistance to adaptive jailbreak attacks. By sharing our findings, we aim to raise awareness of the safety costs associated with activation approximations and inspire further research to reinforce the safety of aligned LLMs in practical deployment scenarios.