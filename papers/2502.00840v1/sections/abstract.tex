% However, their extensive parameter sizes pose significant challenges in inference efficiency due to memory and computational bottlenecks. Activation approximation techniques, such as activation function approximation, sparsification, and quantization, have emerged as promising solutions to enhance inference efficiency. Despite their benefits, the safety implications of these approximations remain underexplored.

Large Language Models (LLMs) have showcased remarkable capabilities across various domains.  Accompanying the evolving capabilities and expanding deployment scenarios of LLMs, their deployment challenges escalate due to their sheer scale and the advanced yet complex activation designs prevalent in notable model series, such as Llama, Gemma, Mistral. These challenges have become particularly pronounced in resource-constrained deployment scenarios, where mitigating inference efficiency bottlenecks is imperative. Among various recent efforts, activation approximation has emerged as a promising avenue for pursuing inference efficiency, sometimes considered indispensable in applications such as private inference. Despite achieving substantial speedups with minimal impact on utility, even appearing sound and practical for real-world deployment, the safety implications of activation approximations remain unclear.

In this work, we fill this critical gap in LLM safety by conducting the first systematic safety evaluation of activation approximations. Our safety vetting spans seven state-of-the-art techniques across three popular categories (activation polynomialization, activation sparsification, and activation quantization), revealing consistent safety degradation across ten safety-aligned LLMs. To overcome the hurdle of devising a unified defense accounting for diverse activation approximation methods, we perform an in-depth analysis of their shared error patterns and uncover three key findings. We propose QuadA, a novel safety enhancement method tailored to mitigate the safety compromises introduced by activation approximations.
Extensive experiments and ablation studies corroborate QuadAâ€™s effectiveness in enhancing the safety capabilities of LLMs after activation approximations.


% In this work, we identify significant safety vulnerabilities introduced by activation approximations. Through red teaming studies on state-of-the-art techniques, we find that these approximations can incur safety issues without noticeable utility degradation, increasing the attack success rate (ASR) of safety-aligned models such as Llama-3.1-8B-Instruct from 0.19\% to 69.23\%. We conduct further analysis of the noise introduced by the activation approximations on different safe-aligned LLMs. We found that perturbations in the first few layers of the model are the most critical, and harmful prompts exhibit distinct distribution patterns under these perturbations.

% Based on these key observations, we propose for the first time an activation approximation-robust safety alignment method to fix the safety vulnerabilities in LLMs. Experiments show that our method can not only cope with activation perturbations of different levels, but also resist a variety of adaptive jailbreak attacks.