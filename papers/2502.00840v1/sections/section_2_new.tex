\subsection{Activation Approximations}
\noindent \textbf{Activation Polynomialization.}
In private LLM inference \cite{hao2022iron, hou2023ciphergpt, pang2024bolt, lu2023bumblebee, zhang2024secure}, the polynomialization of the activation function is widely used to improve the efficiency of private inference. Private inference is a two-party cryptographic protocol based on secure multi-party computation (MPC) and fully homomorphic encryption (FHE). It can achieve that the server learns nothing about the clients' input and clients learn nothing about the server's model except the inference result. In contrast to regular inference in plaintext, operations like Gaussian Error Linear Unit (GELU) and LayerNorm require intensive computation and communication in ciphertext, which usually dominates the total inference latency \cite{wang2022characterization,li2022mpcformer,zeng2023mpcvit,rho2024encryption}. To address this problem, the server usually replaces the non-linear functions to MPC/FHE-friendly approximations. MPCFormer\cite{li2022mpcformer} replaces GELU($x$) to $0.125x^2+0.25x^2+0.5$, BOLT \cite{pang2024bolt} replaces GELU($x$) to a 3-segment polynomial function, BumbleBee\cite{lu2023bumblebee} and NEXUS \cite{zhang2024secure} replace GELU($x$) to a 4-segment polynomial function.

% Table \ref{tbl:private_inference_approx} shows the approximations of GELU in private inference. 
% \begin{table}
%     \renewcommand\arraystretch{1.2}
%     \centering
%     \begin{threeparttable}[b]
%     \caption{Approximations of GeLU in private inference}
%     \begin{tabular}{cl}
%     \toprule[1pt]
%     $\textbf{Works}$ & $\textbf{Approximations} $  \\
%     \midrule[1pt]
%     % Original & $\frac{1}{2}x[1+\text{erf}(x/\sqrt{2})] $ \\
%     % \hline
%     MPCFormer \cite{li2022mpcformer} & $0.125x^2+0.25x^2+0.5$ \\
%     \hline
%     Iron \cite{hao2022iron} & $\frac{1}{2}x[1+\text{Tanh}(\sqrt{2/\pi}(x+0.04471x^3))]$\\
%     \hline
%     BOLT\cite{pang2024bolt} &  
%     $\left\{\begin{aligned}
%         & x  &\text{if   }& x > 2.7 \\
%         & P^4(x) & \text{if   }& |x| \leq 2.7 \\ 
%         & 0  & \text{if   } &x < -2.7 \\
%         \end{aligned}\right.$   \\
%     \hline
%     \makecell[c]{BumbleBee\cite{lu2023bumblebee} \\ NEXUS\cite{zhang2024secure}} &
%     $\left\{\begin{aligned}
%         & 0  &\text{if   }& x < -5 \\
%         & P^3(x) & \text{if   }& -5 < x \leq -1.97\\ 
%         & P^6(x) & \text{if   }& -1.97 < x \leq 3\\
%         & x  & \text{if   } &x > 3 \\
%         \end{aligned}\right.$   \\
%     \bottomrule[1pt]
%     \end{tabular}
%     \begin{tablenotes}
%         \item [*] $P^d(x)$ is a degree-$d$ polynomial.
%     \end{tablenotes}
%     \label{tbl:private_inference_approx}
% \end{threeparttable}
% \end{table}
\partitle{Activation Sparsification}
Recent work has observed that activations in the MLP blocks of LLMs are sparse \cite{liu2023deja, mirzadeh2023relu}. This implies that only a few rows (or columns) of the corresponding weight matrices are required for the forward pass. Activation sparsification \cite{liu2024training, zhang2024relu, mirzadeh2023relu} is an alternative method leveraging sparsity in hidden states. Since the weight channels corresponding to zero-valued activations are not used in computation, speed-up can be realized by selectively omitting these weights during memory transfer. Unlike model pruning techniques, which either remove zero-valued weights from the model or set some weights to zero, activation sparsity exploits the zeros that occur dynamically during runtime, depending on the input.

TEAL \cite{liu2024training} and CATS \cite{leecats} have realized training-free activation sparsity based on magnitude pruning. By pruning low-magnitude, non-salient activations, they realize wall-clock speed-ups of up to 1.9$\times$ at 50\% sparsity.


\partitle{Activation Quantization}
Quantization has proven to be a promising technique for mitigating both computational and memory overhead in LLMs. Many post-training quantization methods \cite{lin2024awq, gptq2023, dettmers2023spqr} reduce memory consumption through weight-only quantization, where weights are quantized while activations remain in full precision. To further reduce the computational overhead, recently, many works \cite{xiao2023smoothquant, shao2023omniquant, wei2022outlier} employs weight-activation quantization which quantizes both weight and activation into low-bit values for the execution of low-bit matrix multiplication. For instance, INT8 quantization of weights and activations can halve the GPU memory usage and nearly double the throughput of matrix multiplications compared to FP16.

To exclude the effect of training and weight quantization on the safety evaluation, we only study post-training quantization (PTQ) and keep weight at full precision. We choose SmoothQuant \cite{xiao2023smoothquant} and OmniQuant \cite{shao2023omniquant} as our case studies.


\subsection{LLM Safety Evaluations and Alignment}

% llm的safety是什么、为什么重要等，然后写由于safety重要所以会有alignment的过程，大致用了什么方法
% 1）safety评测目前做了哪些维度；2）safety aligned llm有哪些、技术有哪些；3）目前对于activation approximation的safety评测空白
LLMs commonly adopt a self-autoregressive framework, where each token is predicted based on all previously generated tokens. This recursive process allows the model to generate coherent text step by step. Given a vocabulary $\mathcal{V}$, the sequence prediction task can be formally expressed as: $$\pi_{\theta}(y|x)=\pi_{\theta}(y_1|x)\prod_{i=1}^{m-1}\pi_{\theta}(y_{i+1}|x,y_1,...,y_i),$$ where $\pi_{\theta}$ is the model, $x=(x_1,x_2,...,x_n),(x_i \in \mathcal{V})$ is the context including the prompt, and $y=(y_1,y_2,...,y_n),(y_i \in \mathcal{V})$ is the predicted sequence. 

% However, while the model’s ability to predict the next token is powerful, it lacks direct control over ensuring that the outputs align with human values, which is a crucial concern in real-world applications.

Despite the effectiveness of this mechanism to generate coherent and contextually relevant text, it offers limited control over aligning outputs with human preferences. Consequently, prior work \cite{chao2023jailbreaking,zou2023universal,yu2023gptfuzzer,wang2024detoxifying,zhao2024defending} has shown that these models may still produce harmful, biased, or otherwise undesirable content. Particularly in high-stakes domains such as healthcare or finance \cite{liu2024demystifying,yao2024survey}. Therefore, ensuring safe and aligned outputs is crucial, and this has led to the development of alignment techniques that aim to bridge the gap between a model's generative objective and the need for responsible outputs \cite{li2022blacklight,shan2024nightshade}.

\partitle{Safety Evaluations} Effective safety evaluation is crucial to understanding and ensuring that LLMs generate outputs that align with human values and ethical guidelines. Existing researches on LLM safety evaluation typically involves multiple dimensions, including Attack Success Rate (ASR) \cite{zou2023universal}, Refusal mechanisms \cite{mazeika2024harmbench}, and Safety Risk Index (SRI) \cite{ying2024safebench}. These evaluation metrics have form the backbone of safety assessments in LLM research \cite{deng2024masterkey,zeng2024air,xie2024sorry,gehman2020realtoxicityprompts,shaikh2022second,wang2023not,souly2024strongreject,li2024salad,chu2024comprehensive}, offering a systematic approach to measuring a model's robustness against harmful content generation. Despite the thoroughness of these approaches, there remains a gap when it comes to evaluating the impact of activation approximation on safety. Our work seeks to fill this gap by investigating how activation approximation could impact the model's ability to uphold safety and suggesting strategies to mitigate these associated risks \cite{hu2024efficient,wang2024white,xu2024uncovering,jia2023causality}.

\partitle{Safety Alignment} 
While safety evaluation focuses on measuring a model’s ability to resist harmful behavior, safety alignment involves the active process of modifying the model to ensure its outputs align with human values. Current alignment approaches generally fall into two categories: (1) Safety Moderation: Incorporating policy or ethical guidelines to filter or modify outputs, often through real-time moderation systems that detect and prevent harmful content during generation. (2) Robust training: Refining model behavior through data purification and fine-tuning methods leveraging human feedback, such as Supervised Fine-Tuning (SFT) \cite{ouyang2022training,zheng2023judging}, Reinforcement Learning from Human Feedback (RLHF) \cite{ouyang2022training}, and Direct Preference Optimization (DPO) \cite{rafailov2024direct}. Recent studies have investigated how specific neurons contribute to safety in aligned LLMs \cite{touvron2023llama,OpenAI2023ChatGPT,hsu2024safe,wei2024assessing,huang2024harmful,huanglisa}, highlighting the complex interplay between internal representations and secure outputs.

However, achieving safety alignment typically requires integrating dedicated mechanisms into the model training process—a non-trivial undertaking demanding substantial computational resources and specialized expertise \cite{ouyang2022training,zou2023universal,yu2023gptfuzzer,wang2024detoxifying}. Additionally, the effectiveness of alignment techniques depends on the availability of high-quality, diverse data and careful tuning of model behavior based on nuanced human feedback. Consequently, aligning LLMs is both a costly and time-consuming process that requires specialized knowledge, making it challenging for downstream users to conduct alignment independently \cite{horwitz2024recovering,yang2024self,lyu2024keeping}.

\partitle{Brittleness of Safety Under Various Modifications}
Despite considerable progress, LLM safety remains fragile to degradation under certain modifications. For example, operations such as fine-tuning on pure-benign datasets \cite{qi2023fine,hsu2024safe,shen2024seal}, compressing or pruning model architectures \cite{jaiswal2023compressing,yin2023outlier,li2024discovering,xu2024beyond,ullah2024navigating}, or altering decoding mechanisms \cite{huang2023catastrophic,huang2024trustllm,mazeika2024harmbench} have been systematically shown to erode established safeguards. However, one critical area remains underexplored: \textit{activation approximation}. While widely adopted for efficiency gains (e.g., reducing inference latency and memory usage), its impact on safety has yet to be systematically examined.

In this work, we investigate how activation approximations affect LLMs' safety alignment and propose strategies to preserve both efficiency gains and robust safety guarantees. Our findings represent a complementary and critical contribution to existing research on LLM safety.


