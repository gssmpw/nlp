The models studied in this paper are all open-source LLMs. Since closed-source LLMs, e.g., GPT-3.5 and GPT-4 do not provide an API to set the activation approximation, we cannot find the most vulnerable approximation (\mva) and align the LLMs with \mva. However, we must point out that in reality, the LLM trainer has full permissions on the model, so the LLM trainer has the permission to perform safety tests on the model under various activation approximations, thereby obtaining the model's \mva~ and using QuadA for alignment.

% \paragraph{Future Work}
% Combined with the distribution properties of harmful activation before and after the approximation, we will consider experimenting with good penalty terms in defense strategies, such as the Jensen-Shannon (JS) divergence of harmful activation. Furthermore, we will study the properties of adaptive prompt-based jailbreak, including its correlation with the activation approximation. We speculate that both of them lead to jailbreak because harmful activation bypasses the "safe zone", and we will try to verify this conjecture.
