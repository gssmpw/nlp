%参考只是【一个参考】，要多找参考资料，下面说的参考只是简单例子！%

\subsection{Large Language Models}
\label{llm objective}
%参考https://arxiv.org/pdf/2402.18104 section2.1%
Large Language Models (LLMs) are typically trained on extensive and diverse text corpora using unsupervised learning techniques. Since the introduction of GPT-2 [], which marked the beginning of training highly parameterized models on large datasets, LLMs have demonstrated remarkable capabilities in performing downstream tasks through few-shot or zero-shot prompting []. ChatGPT exemplifies how LLMs leverage alignment technologies to enhance their adaptability to downstream tasks using prompts, enabling more natural and relevant interactions between humans and LLMs.

Both open-source and closed-source LLMs typically use a self-autoregressive framework, where each token is predicted based on the preceding tokens. This recursive process allows the model to generate coherent text step by step. Given a vocabulary $\mathcal{V}$, the sequence prediction task can be formally expressed as: $$\pi_{\theta}(y|x)=\pi_{\theta}(y_1|x)\prod_{i=1}^{m-1}\pi_{\theta}(y_{i+1}|x,y_1,...,y_i)$$ where $\pi_{\theta}$ is the model, $x=(x_1,x_2,...,x_n),(x_i \in \mathcal{V})$ is the context including the prompt, and $y=(y_1,y_2,...,y_n),(y_i \in \mathcal{V})$ is the predicted sequence.

\subsection{LLM Inference Speedup with Activation Approximations}

Despite the extensive pre-training and alignment efforts, LLMs remain computationally expensive to deploy in real-world applications, especially when running on resource-constrained devices or under strict latency requirements. Recent research has thus focused on customizing LLMs through various techniques—such as model quantization, sparsification, and activation approximations[].  Among these approaches, approximately the activation values has emerged as a key strategy for speeding up LLM inference. This motivation to accelerate inference is especially strong for large models with potentially astronomical parameter counts. In these scenarios, reducing both the memory footprint and the per-token inference time is critical for deploying LLMs in latency-sensitive or cost-constrained environments.
%activation的近似对效率提升很重要很普遍,参考https://openreview.net/pdf?id=dGVZwyq5tV 第一段，以及其他activation quantization、private inference里关于加速inference speed的内容，最终目的是说明【Activation Approximations非常普遍重要！大模型太大了要加速推理！这个motivation很强！】%

\subsection{Safety Concern with Activation Approximations}
%老生常谈的一段，说明LLM safety非常重要！但Activation Approximations会导致safety丢失，本质就是把Qresafe的这一相关段落中的weight quant替换成activation approx%
While activation approximations have emerged as an essential strategy for practical deployments of LLMs, they also introduce safety and reliability concerns. 

\noindent \textbf{Alignment of LLMs} Alignment techniques bridge the gap between the LLM's language modeling objective(e.g., predicting the next token \ref{llm objective}) and the need to ensuring outputs are free from harmful, biased, or otherwise undesirable content[]. Current approaches generally fall into two categories: (1) Safety Moderation: incorporating policy or ethical guidelines to filter or modify outputs. This may involve real-time moderation systems that detect and prevent harmful content during generation. (2) Robust training: refining model behaviours through data purification and fine-tuning methods that leverage human feedback, including Supervised Fine-Tuning (SFT)[], Reinforcement Learning from Human Feedback (RLHF)[], and Direct Preference Optimization (DPO)[]. Recent studies have investigated how certain neurons contribute to safety in aligned LLMs, underscoring the intricate relationship between internal representations and secure model outputs[].

\textbf{The impact of Activation Approximation} While activation approximations can greatly reduce inference costs, existing research[] and our own findings reveal that these methods may inadvertently undermine a model’s safety alignment. By subtly distorting the model’s internal representations, approximations can degrade or even negate its built-in safeguards, potentially leading to toxic, biased, or otherwise harmful outputs.

Consequently, when applying activation approximation methods, one must balance the trade-off between inference efficiency and preserving safety guarantees. In this paper, we propose...

