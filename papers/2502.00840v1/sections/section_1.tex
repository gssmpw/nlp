
% Large Language Models exemplified by systems like ChatGPT \cite{OpenAI2023ChatGPT}, Bard \cite{Bard2023}, and Meta’s Llama \cite{touvron2023llama}, have demonstrated exceptional capabilities in natural language generation and diverse downstream tasks, including data analysis \cite{ma2023insightpilot}, software development \cite{roy2024exploring} and disease diagnosis \cite{thirunavukarasu2023large, zhou2024pre}. 
% However, the large number of parameters in modern LLMs can lead to substantial challenges during inference. The autoregressive inference is bottlenecked by the speed at which the parameters can be moved from off-chip to on-chip memory. For example, the GPT-3 \cite{brown2020language} contains 175B parameters, which will consume at least 350GB of memory to store and run, requiring 5$\times$80GB A100 GPUs just for inference. Due to the huge computation and communication overhead, the inference latency may also be unacceptable to real-world applications. To enhance the inference efficiency, the direct approach is model compression, including weight pruning \cite{frantar2023sparsegpt,sun2023simple,ma2023llm} and weight quantization \cite{lin2024awq,gptq2023,huang2024billm}. These methods directly edit the model weights and apply them to all inputs. 

Large Language Models (LLMs) have demonstrated remarkable capabilities and gained surging popularity ever since the release of ChatGPT~\cite{OpenAI2023ChatGPT}. Their widespread applications across diverse fields are further bolstered by open-source LLMs such as Llama~\cite{touvron2023llama}, Gemma~\cite{team2024gemma}, Mistral~\cite{jiang2023mistral}, Falcon~\cite{almazrouei2023falcon}, and Qwen~\cite{bai2023qwen}, offered by model developers capable of conducting extensive pretraining and meticulous alignment procedures like Meta, Google, and Alibaba. Application builders, ranging from enterprises to individuals, can harness the capabilities and flexibility of these open-source aligned LLMs for custom use cases and diverse deployment scenarios. Still, the prohibitive deployment costs hinder the full application potential of LLMs due to their astronomical scales, particularly in resource-constrained scenarios. Additionally, the computational overhead introduced by advanced and complex activation functions like GELU~\cite{hendrycks2016gaussian} and SwiGLU~\cite{elfwing2018sigmoid}, which are integral and pervasive to modern LLMs (e.g., all above-mentioned open-source LLMs), further exacerbates the inference efficiency challenge. Consequently, improving the inference efficiency of LLMs has garnered growing research attention, with existing approaches exploring distinct facets, including weight quantization~\cite{lin2024awq,gptq2023,huang2024billm}, weight sparsification~\cite{frantar2023sparsegpt, ma2023llm, xia2023sheared, wang2019structured}, model pruning~\cite{frantar2023sparsegpt,sun2023simple,ma2023llm}, speculative decoding~\cite{leviathan2023fast, kim2024speculative}. These efforts offer complementary reductions in inference overhead and hold promise for achieving aggregated efficiency gains when combined.



\partitle{Activation Approximation} Recently, activation approximation has emerged as a promising and, in some cases, indispensable avenue for pursuing inference efficiency enhancement in LLMs. This line of methods focuses on devising various approximations to alleviate the computation of complex non-linear activation functions, thereby catering to specific deployment requirements. Based on the application scenario and approximation techniques, there are three notable categories, including (i) Activation Polynomialization \cite{hao2022iron, hou2023ciphergpt, pang2024bolt, lu2023bumblebee, zhang2024secure}: Replacing complex nonlinearities in activation functions with polynomials substantially accelerates computation, making it indispensable for the private inference scenario.
(ii) Activation Sparsification \cite{liu2024training, zhang2024relu, mirzadeh2023relu}: Truncating input activation values close to zero so that weight channels corresponding to zero-valued activations can be ignored in inference computation.
(iii) Activation Quantization \cite{xiao2023smoothquant,shao2023omniquant}: Quantizing the activation vector from higher bit-widths to lower bit-widths.
Table~\ref{tbl:speedup} summarizes exemplary inference efficiency gains achieved by representative methods from each category, demonstrating significant wall-clock speed-ups ranging up to 24.6$\times$. 

\begin{table}
    % \renewcommand\arraystretch{1.05}
    \centering
    \scriptsize	
    \caption{Activation approximation techniques and their inference efficiency gains. For activation polynomialization, the baseline work is SIRNN (S\&P'21) \cite{rathee2021sirnn}, a private inference protocol without polynomialization. For activation sparsification and quantization, the baseline is full-precision (FP16) inference without any accelerations. }
    \begin{tabular}{cccc}
    \toprule[1pt]
    $\textbf{Methods}$ & $\textbf{Work}$ & $\textbf{Setting}$ & $\textbf{Speedup}$ \\
    \midrule[1pt]
    % \multirow{2}{*}{\textbf{Baseline}} & - & Pure & - & - & 8.05 & 0.19 \\
    %  & - & Worst & Lap(0, 0.085) & $\mathcal{N}(0, 0.075^2)$ &  &  \\
    % \midrule[1pt]
    \multirow{4}{*}{\textbf{\makecell[c]{Activation \\ Polynomialization}}} & Iron (NeurIPS’22) & MPC & 9.6$\times$\\
    & BOLT (S\&P'24) & MPC & 24.6$\times$  \\
    & BumbleBee (NDSS'25) & MPC & 22.3$\times$\\    
    & NEXUS (NDSS'25) & FHE & 5.3$\times$ \\
    \midrule[1pt]
    \multirow{3}{*}{\textbf{\makecell[c]{Activation \\ Sparsification}}} & \multirow{3}{*}{TEAL (ICLR’25)}  & 25\% sparsity & 1.3$\times$  \\
    &  & 50\% sparsity & 1.9$\times$ \\ 
    &  & 90\% sparsity & 9.5$\times$ \\ 
    \midrule[1pt]
    \multirow{4}{*}{\textbf{\makecell[c]{Activation \\ Quantization}}} 
     & \multirow{2}{*}{SmoothQuant (ICML’23)} & W16A8 & 2.0$\times$ \\
    & &  W16A4 & 4.0$\times$\\
    & \multirow{2}{*}{OmniQuant (ICLR’24)} & W16A8 & 2.0$\times$ \\
    & &  W16A4 & 4.0$\times$ \\
    \bottomrule[1pt]
    \end{tabular}
    \label{tbl:speedup}
% \vspace{-1.5em}
\end{table}

The core design rationale underpinning these methods is to maximize inference efficiency while ensuring that activation approximation errors do not degrade the utility of LLMs beyond an acceptable limit. This has proven to be plausible since current activation approximation errors minimally impact utility metrics, making them appear sound and practical for real-world deployment.

% Recently, activation approximation techniques were being widely explored, they enforce input-dependent structure on the weight matrices by leveraging approximations in the hidden states. We summarize three common techniques for activation approximations. Firstly, in private inference \cite{hao2022iron, hou2023ciphergpt, pang2024bolt, lu2023bumblebee, zhang2024secure}, the approximation of the activation function is widely used to improve the efficiency of inference. Secondly, activation sparsification \cite{liu2024training, zhang2024relu, mirzadeh2023relu} is a new technique to enable practical inference speedups by reducing the compute and memory-movement required for matrix multiplications during the forward pass. Finally, activation quantization \cite{yao2022zeroquant, liu2023qllm, shao2023omniquant, xiao2023smoothquant} can also reduce GPU memory requirements in bandwidth, and accelerate compute-intensive operations. \textbf{\textit{However, what are the safety costs associated with the activation approximations?}}

\partitle{LLM Safety}
On par with utility, safety has become an equally paramount dimension for LLMs, garnering widespread consensus from academia, industry, and beyond. Maintaining safety capabilities is critical for preventing LLMs from being exploited to produce harmful responses, such as facilitating unethical uses, spreading misinformation, or fostering discriminatory behaviour. Moreover, maliciously crafted prompts through jailbreak attacks can further induce LLMs towards harmful generations~\cite{gupta2023chatgpt, singh2023exploiting, zhang2024psysafe}, providing an inkling of how intricate the task of safety preservation for LLMs is. In response, model developers have implemented extensive safety alignment procedures, employing techniques such as instruction tuning, reinforcement learning with human feedback (RLHF)~\cite{ouyang2022training, dai2023safe}, and direct preference optimization (DPO)~\cite{rafailov2024direct}, before open-sourcing the aligned LLMs. Unfortunately, safety has turned out to be fragile to maintain, as even minor modifications to the weights of aligned LLMs through fine-tuning can degrade their safety capabilities. When it comes to inference efficiency improvement techniques, safety studies thus far have evaluated weight quantization, weight sparsification, model pruning, and decoding algorithms, demonstrating degraded safety capabilities \cite{qi2023fine,hsu2024safe,jaiswal2023compressing,yin2023outlier,li2024discovering,huang2023catastrophic,huang2024trustllm}.

\partitle{Research Gap} The above findings on the brittleness of safety in aligned LLMs raise a pressing yet under-explored research question:
\emph{Does activation approximation, which introduces various sorts of activation errors, compromise the safety capabilities of aligned LLMs?} 
Vetting and coping with the safety risks posed by different activation approximation methods are critical. It serves as a cautionary reminder for application builders to avoid the illusion that activation approximation inherently preserves the safety capabilities of aligned LLMs. Additionally, model developers could incorporate safety assessment findings into their safety alignment procedures to ensure that the resulting LLMs are robust against potential activation errors.

% The major safety concern is jailbreak attack \cite{gupta2023chatgpt, singh2023exploiting, zhang2024psysafe}, which involves the strategic construction of malicious prompts that make
% LLMs generate unexpected or harmful content. More importantly, these attacks can have far-reaching implications, ranging from privacy breaches to the dissemination of misinformation \cite{gupta2023chatgpt}, and even the manipulation of automated systems \cite{zhang2024psysafe}. Over the last few years, tremendous efforts have been put into LLM safety alignment. Established techniques such as reinforcement learning from human feedback (RLHF) \cite{ouyang2022training, dai2023safe} and direct performance optimization (DPO) \cite{rafailov2024direct, lee2024mechanistic} have been extensively applied to constrain the behaviours of LLMs within a safe scope. These safety infrastructures predominantly involve embedding safety rules within pre-trained models to restrict their harmful behaviours. This may work when users can only interact with immutable centralized models through input prompts. However, in reality, users often only interact with third-party LLM-based services or applications, and these service providers have strong motivations to use activation approximation techniques to speed up the processing of user requests. In this situation, the safety of the LLMs is beyond the scope of previous safety alignment.

\partitle{Our Work} Below we briefly describe our efforts towards closing the aforementioned research gap in this paper.

\noindent\underline{\textit{Safety Assessment:}} To uncover the under-studied safety risks of activation approximation, we conduct a systematic safety assessment of state-of-the-art activation approximation techniques spanning three different categories, including activation polynomialization~\cite{hao2022iron, pang2024bolt, lu2023bumblebee, zhang2024secure}, activation sparsification~\cite{liu2024training, zhang2024relu, mirzadeh2023relu}, and activation quantization~\cite{yao2022zeroquant, liu2023qllm, shao2023omniquant, xiao2023smoothquant}. Our findings reveal that activation approximations even in safety-aligned LLMs introduce safety degradation that current safety alignment mechanisms fail to adequately address.

\noindent\underline{\textit{Safety Analysis and Key Observations:}} We conduct an in-depth analysis of the perturbations introduced by activation approximations across 10 different safety-aligned LLMs, leading to three key observations. First, activation approximations will cause LLM to lose safety before utility, leading to the generation of meaningful yet malicious responses, i.e., helpful for harmful purposes. Taking Llama-3.1-8B-Instruct~\cite{meta2024introducing} as an example, activation approximations substantially increase the attack success rate (ASR) from 0.19\% to 69.23\% while having almost no impact on utility. Second, activation approximations in the first few layers are the most detrimental to safety, whereas approximations in the later layers have negligible effects on safety. Third, harmful prompts tend to cluster within the activation space, and activation approximations shift these harmful activations into benign regions, thereby easier to bypass safety checks.

\noindent\underline{\textit{Safety Enhancement:}} 
Based on these key observations, we propose activation approximation-robust safety alignment, a unified approach to address safety vulnerabilities arising from different activation approximation techniques. Our method enables LLM developers to integrate this additional robustness into their existing safety alignment procedures with minimal modifications and computational overhead (e.g., adding just 2 to 3 lines of code). The resulting LLMs, after being open-sourced and deployed by downstream application providers, remain resilient to various activation approximations introduced to enhance inference efficiency.
Experimental results demonstrate that our approach not only handles activation approximations across different methods and various magnitudes but also resists a variety of adaptive jailbreak attacks, such as GCG \cite{zou2023universal} and AutoDAN \cite{liu2024autodan}.

% Based on these key observations, we propose an activation approximation-robust safety alignment defense method, which is a simple yet effective
% safety enhancement method accounting for the diversity of different activation approximations. Our approach incorporates the most vulnerable approximations into safety-sensitive layers, and formulates an optimization function resilient to activation approximations. Experimental results demonstrate that our method not only handles activation approximations of different methods and varying magnitudes, but also resists a range of adaptive jailbreak attacks, such as GCG \cite{zou2023universal} and AutoDAN \cite{liu2024autodan}.

% Based on these key observations, we first propose activation approximation-robust safety alignment to fix the safety vulnerabilities in LLMs. We add the most vulnerable approximation to the sensitive layers, take the diversity of activation distribution into consideration, and construct the optimization function that is resistant to activation approximations. Experiments show that our method can not only cope with activation approximations of different levels, but also resist a variety of adaptive jailbreak attacks, such as GCG\cite{zou2023universal}, AutoDAN\cite{liu2024autodan}, etc. 

\partitle{Summary of Contributions} To the best of our knowledge, our work is the first comprehensive study on the impact of activation approximations on the safety of LLMs. The major contributions of this paper are summarized as follows:
\begin{itemize}
    \item Comprehensive Safety Assessment: We conduct extensive safety evaluations on state-of-the-art activation approximation techniques, spanning three representative categories including activation polynomialization, activation sparsification, and activation quantization.
    \item In-Depth Safety Analysis: Through analysis across 10 safety-aligned LLMs, we uncover that activation approximations (i) compromise safety before utility, (ii) are most harmful in early layers, and (iii) shift harmful activations into benign spaces to evade detection.
    \item Novel Safety Enhancements: We propose an \ul{A}ctivation \ul{A}pproximation-\ul{A}ware \ul{A}lignment (QuadA) to address these vulnerabilities. 
    % All the evaluations and fixes in the paper are open source code at \url{https://anonymous.4open.science/r/act-perturb-jailbreak}.
\end{itemize}
% We hope that, by sharing our discoveries, we inspire further research dedicated to fortifying safety protocols for the activation approximations of aligned LLMs.

% Overall, we hope that sharing our findings inspires further research aimed at strengthening safety protocols for activation approximations in aligned LLMs


% In summary, our contributions are threefold:
% \begin{itemize}
%     \item 
% \end{itemize}




