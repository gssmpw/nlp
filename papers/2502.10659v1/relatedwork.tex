\section{Related Work}
LLMs are based on the Transformer architecture \cite{vaswani2017attention}. 
Hardware architecture for Transformer have been extensively studied in the pre-LLM era, with numerous hardware accelerators optimized for models such as ViT\cite{dosovitskiy2020image} and BERT\cite{devlin2018bert}.
HeatVit\cite{dong2023heatvit} proposed a hardware-efficient image-adaptive token pruning framework for efficient yet accurate ViT acceleration on embedded FPGAs. Liu et al. \cite{liu2021hardware} proposed to fully quantize the BERT and propose an accelerator on Xilinx ZCU102 and ZCU11 FPGA and achieve high power efficiency.
While these methods have proven effective for ViT and BERT models, they cannot be directly applied to LLM workloads.

In the era of LLMs, research on hardware accelerators tailored for LLMs have gradually advanced.
As LLMs are memory-intensive, recent work has focused on accelerating LLMs using HBM-equipped cloud FPGAs, such as the Xilinx Alveo U280 and VCU128, achieving decoding speed improvements over GPUs in single-batch LLM inference setups.
DFX \cite{hong2022dfx} represents one of the first studies on FPGA accelerators for decoder-only transformer, utilizing four banks of Alveo U280 to accelerate the GPT-2 1.5B model \cite{radford2019language}. FlightLLM \cite{zeng2024flightllm} has deployed the LLaMA2-7B model \cite{touvron2023llama} on both the Alveo U280 and the Versal VHK158, demonstrating advantages over NVIDIA GPUs in single-batch inference tasks. Chen et al. \cite{chen2023understanding} provide a comprehensive analytical framework, offering the first in-depth examination of both the benefits and limitations of FPGA-based LLM spatial acceleration using the Alveo U280. EdgeLLM \cite{huang2024edgellm} adopts the VCU128, which is also a very large FPGA equipped with HBM, to deploy LLaMA2-7B, achieving a decoding speed higher than FlightLLM.

While these studies represent the initial steps in FPGA-based LLM acceleration for single-batch decoding, they are not applicable to real-world scenarios. Expensive server-class FPGAs are expected to handle multiple concurrent user inquiries, whereas single-batch decoding is more commonly seen in edge environments.
% However, these studies are not applicable to embedded FPGAs, which lack HBM and have limited memory capacity.
Accelerating single-batch decoding for LLMs with embedded FPGAs remains a challenging yet unexplored area, which will be the focus of this paper.