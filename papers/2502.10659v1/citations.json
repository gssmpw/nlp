[
  {
    "index": 0,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "dosovitskiy2020image",
        "author": "Dosovitskiy, Alexey",
        "title": "An image is worth 16x16 words: Transformers for image recognition at scale"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "devlin2018bert",
        "author": "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",
        "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dong2023heatvit",
        "author": "Dong, Peiyan and Sun, Mengshu and Lu, Alec and Xie, Yanyue and Liu, Kenneth and Kong, Zhenglun and Meng, Xin and Li, Zhengang and Lin, Xue and Fang, Zhenman and others",
        "title": "Heatvit: Hardware-efficient adaptive token pruning for vision transformers"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liu2021hardware",
        "author": "Liu, Zejian and Li, Gang and Cheng, Jian",
        "title": "Hardware acceleration of fully quantized bert for efficient natural language processing"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "hong2022dfx",
        "author": "Hong, Seongmin and Moon, Seungjae and Kim, Junsoo and Lee, Sungjae and Kim, Minsub and Lee, Dongsoo and Kim, Joo-Young",
        "title": "Dfx: A low-latency multi-fpga appliance for accelerating transformer-based text generation"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zeng2024flightllm",
        "author": "Zeng, Shulin and Liu, Jun and Dai, Guohao and Yang, Xinhao and Fu, Tianyu and Wang, Hongyi and Ma, Wenheng and Sun, Hanbo and Li, Shiyao and Huang, Zixiao and others",
        "title": "FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGA"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "chen2023understanding",
        "author": "Chen, Hongzheng and Zhang, Jiahao and Du, Yixiao and Xiang, Shaojie and Yue, Zichao and Zhang, Niansong and Cai, Yaohui and Zhang, Zhiru",
        "title": "Understanding the potential of fpga-based spatial acceleration for large language model inference"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "huang2024edgellm",
        "author": "Huang, Mingqiang and Shen, Ao and Li, Kai and Peng, Haoxiang and Li, Boyu and Yu, Hao",
        "title": "Edgellm: A highly efficient cpu-fpga heterogeneous edge accelerator for large language models"
      }
    ]
  }
]