@book{lamport94,
 author = "Leslie Lamport",
 title = "{\LaTeX: A Document Preparation System}",
 year = "1994",
 publisher = "Addison-Wesley",
 edition = "2nd",
 address = "Reading, Massachusetts"
}

@inproceedings{nicepaper1,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2",
  title = "A Very Nice Paper To Cite",
  year = "2016",
  booktitle = "Proceedings of the 26th IEEE International Symposium on High Performance Computer Architecture"
}

@inproceedings{nicepaper2,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2 and Firstname3 Lastname3",
  title = "Another Very Nice Paper to Cite",
  year = "2015",
  booktitle = "Proceedings of the 48th Annual IEEE/ACM International Symposium on Microarchitecture"
}

@inproceedings{nicepaper3,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2 and Firstname3 Lastname3 and Firstname4 Lastname4 and Firstname5 Lastname5 and Firstname6 Lastname6 and Firstname7 Lastname7 and Firstname8 Lastname8 and Firstname9 Lastname9 and Firstname10 Lastname10 and Firstname11 Lastname11 and Firstname12 Lastname12",
  title = "Yet Another Very Nice Paper To Cite, With Many Author Names All Spelled Out",
  year = "2011",
  booktitle = "Proceedings of the 38th Annual International Symposium on Computer Architecture"
}

@misc{braincogweb,
    title={BrainCog: Brain-inspired Cognitive Intelligence Engine},
    url={http://www.brain-cog.network}
}

@misc{tinychatengine,
  title={TinyChat: Large Language Model on the Edge},
  url={https://hanlab.mit.edu/blog/tinychat}
}

@misc{tinychatperf,
  title={TinyChat: Efficient and Lightweight Chatbot with AWQ},
  url={https://github.com/mit-han-lab/llm-awq/blob/main/tinychat/README.md}
}

@article{Zeng2023,
  doi = {10.1016/j.patter.2023.100789},
  url = {https://doi.org/10.1016/j.patter.2023.100789},
  year = {2023},
  month = jul,
  publisher = {Cell Press},
  pages = {100789},
  author = {Yi Zeng and Dongcheng Zhao and Feifei Zhao and Guobin Shen and Yiting Dong and Enmeng Lu and Qian Zhang and Yinqian Sun and Qian Liang and Yuxuan Zhao and Zhuoya Zhao and Hongjian Fang and Yuwei Wang and Yang Li and Xin Liu and Chengcheng Du and Qingqun Kong and Zizhe Ruan and Weida Bi},
  title = {{BrainCog}: A spiking neural network based,  brain-inspired cognitive intelligence engine for brain-inspired {AI} and brain simulation},
  journal = {Patterns}
}

@misc{llamacpp,
  title={Georgi Gerganov. ggerganov/llama.cpp: Port of facebookâ€™s llama model in c/c++.},
  url={ https://github.com/ggerganov/}
}

@misc{tensorrtllm,
  title={A TensorRT Toolbox for Optimized Large Language Model Inference.},
  url={https://nvidia.github.io/TensorRT-LLM/}
}

@misc{nanollm,
  title={Optimized local inference for LLMs with HuggingFace-like APIs for quantization, vision/language models, multimodal agents, speech, vector DB, and RAG.},
  url={https://dusty-nv.github.io/NanoLLM/}
}

@misc{mlcllm,
  title={Universal LLM Deployment Engine with ML Compilation},
  url={https://github.com/mlc-ai/mlc-llm}
}

@misc{jetsonbenchmark,
  title={Jetson AI Lab Benchmark},
  url={https://www.jetson-ai-lab.com/benchmarks.html}
}

@misc{spinalhdl,
  title={SpinalHDL: Scala based HDL},
  url={https://github.com/SpinalHDL/SpinalHDL}
}

@inproceedings{hong2022dfx,
  title={Dfx: A low-latency multi-fpga appliance for accelerating transformer-based text generation},
  author={Hong, Seongmin and Moon, Seungjae and Kim, Junsoo and Lee, Sungjae and Kim, Minsub and Lee, Dongsoo and Kim, Joo-Young},
  booktitle={2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={616--630},
  year={2022},
  organization={IEEE}
}

@article{hu2024llm,
  title={I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models},
  author={Hu, Xing and Chen, Yuan and Yang, Dawei and Zhou, Sifan and Yuan, Zhihang and Yu, Jiangyong and Xu, Chen},
  journal={arXiv preprint arXiv:2405.17849},
  year={2024}
}

@article{haris2024designing,
  title={Designing Efficient LLM Accelerators for Edge Devices},
  author={Haris, Jude and Saha, Rappy and Hu, Wenhao and Cano, Jos{\'e}},
  journal={arXiv preprint arXiv:2408.00462},
  year={2024}
}

@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={87--100},
  year={2024}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{agarwal2023llm,
  title={Llm inference performance engineering: Best practices},
  author={Agarwal, Megha and Qureshi, Asfandyar and Sardana, Linden Li Nikhil and Quevedo, Julian and Khudia, Daya},
  year={2023},
  publisher={Oct}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@misc{xu2024llamaf,
      title={LlamaF: An Efficient Llama2 Architecture Accelerator on Embedded FPGAs}, 
      author={Han Xu and Yutong Li and Shihao Ji},
      year={2024},
      eprint={2409.11424},
      archivePrefix={arXiv},
      primaryClass={cs.AR},
      url={https://arxiv.org/abs/2409.11424}, 
}

@inproceedings{dhar2024empirical,
  title={An empirical analysis and resource footprint study of deploying large language models on edge devices},
  author={Dhar, Nobel and Deng, Bobin and Lo, Dan and Wu, Xiaofeng and Zhao, Liang and Suo, Kun},
  booktitle={Proceedings of the 2024 ACM Southeast Conference},
  pages={69--76},
  year={2024}
}

@inproceedings{wang2021spatten,
  title={Spatten: Efficient sparse attention architecture with cascade token and head pruning},
  author={Wang, Hanrui and Zhang, Zhekai and Han, Song},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={97--110},
  year={2021},
  organization={IEEE}
}

@inproceedings{dong2023heatvit,
  title={Heatvit: Hardware-efficient adaptive token pruning for vision transformers},
  author={Dong, Peiyan and Sun, Mengshu and Lu, Alec and Xie, Yanyue and Liu, Kenneth and Kong, Zhenglun and Meng, Xin and Li, Zhengang and Lin, Xue and Fang, Zhenman and others},
  booktitle={2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={442--455},
  year={2023},
  organization={IEEE}
}

@inproceedings{you2023vitcod,
  title={Vitcod: Vision transformer acceleration via dedicated algorithm and accelerator co-design},
  author={You, Haoran and Sun, Zhanyi and Shi, Huihong and Yu, Zhongzhi and Zhao, Yang and Zhang, Yongan and Li, Chaojian and Li, Baopu and Lin, Yingyan},
  booktitle={2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={273--286},
  year={2023},
  organization={IEEE}
}

@article{zeng2024flightllm,
  title={FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGA},
  author={Zeng, Shulin and Liu, Jun and Dai, Guohao and Yang, Xinhao and Fu, Tianyu and Wang, Hongyi and Ma, Wenheng and Sun, Hanbo and Li, Shiyao and Huang, Zixiao and others},
  journal={arXiv preprint arXiv:2401.03868},
  year={2024}
}

@article{chen2023understanding,
  title={Understanding the potential of fpga-based spatial acceleration for large language model inference},
  author={Chen, Hongzheng and Zhang, Jiahao and Du, Yixiao and Xiang, Shaojie and Yue, Zichao and Zhang, Niansong and Cai, Yaohui and Zhang, Zhiru},
  journal={arXiv preprint arXiv:2312.15159},
  year={2023}
}

@article{song2024prosparse,
  title={ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models},
  author={Song, Chenyang and Han, Xu and Zhang, Zhengyan and Hu, Shengding and Shi, Xiyu and Li, Kuai and Chen, Chen and Liu, Zhiyuan and Li, Guangli and Yang, Tao and others},
  journal={arXiv preprint arXiv:2402.13516},
  year={2024}
}

@article{song2024turbo,
  title={Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters},
  author={Song, Yixin and Xie, Haotong and Zhang, Zhengyan and Wen, Bo and Ma, Li and Mi, Zeyu and Chen, Haibo},
  journal={arXiv preprint arXiv:2406.05955},
  year={2024}
}

@article{xue2024powerinfer,
  title={PowerInfer-2: Fast Large Language Model Inference on a Smartphone},
  author={Xue, Zhenliang and Song, Yixin and Mi, Zeyu and Chen, Le and Xia, Yubin and Chen, Haibo},
  journal={arXiv preprint arXiv:2406.06282},
  year={2024}
}

@article{song2023powerinfer,
  title={Powerinfer: Fast large language model serving with a consumer-grade gpu},
  author={Song, Yixin and Mi, Zeyu and Xie, Haotong and Chen, Haibo},
  journal={arXiv preprint arXiv:2312.12456},
  year={2023}
}

@article{mirzadeh2023relu,
  title={Relu strikes back: Exploiting activation sparsity in large language models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Mehta, Sachin and Del Mundo, Carlo C and Tuzel, Oncel and Samei, Golnoosh and Rastegari, Mohammad and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2310.04564},
  year={2023}
}

@inproceedings{liu2023deja,
  title={Deja vu: Contextual sparsity for efficient llms at inference time},
  author={Liu, Zichang and Wang, Jue and Dao, Tri and Zhou, Tianyi and Yuan, Binhang and Song, Zhao and Shrivastava, Anshumali and Zhang, Ce and Tian, Yuandong and Re, Christopher and others},
  booktitle={International Conference on Machine Learning},
  pages={22137--22176},
  year={2023},
  organization={PMLR}
}

@article{alizadeh2023llm,
  title={Llm in a flash: Efficient large language model inference with limited memory},
  author={Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C and Rastegari, Mohammad and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2312.11514},
  year={2023}
}

@article{zhang2024relu,
  title={ReLU $\^{} 2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs},
  author={Zhang, Zhengyan and Song, Yixin and Yu, Guanghui and Han, Xu and Lin, Yankai and Xiao, Chaojun and Song, Chenyang and Liu, Zhiyuan and Mi, Zeyu and Sun, Maosong},
  journal={arXiv preprint arXiv:2402.03804},
  year={2024}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@inproceedings{rosser2018cocotb,
  title={Cocotb: a Python-based digital logic verification framework},
  author={Rosser, Benjamin John},
  booktitle={Micro-electronics Section seminar. CERN, Geneva, Switzerland},
  year={2018}
}

@article{guo2017angel,
  title={Angel-eye: A complete design flow for mapping CNN onto embedded FPGA},
  author={Guo, Kaiyuan and Sui, Lingzhi and Qiu, Jiantao and Yu, Jincheng and Wang, Junbin and Yao, Song and Han, Song and Wang, Yu and Yang, Huazhong},
  journal={IEEE transactions on computer-aided design of integrated circuits and systems},
  volume={37},
  number={1},
  pages={35--47},
  year={2017},
  publisher={IEEE}
}

@inproceedings{qiu2016going,
  title={Going deeper with embedded FPGA platform for convolutional neural network},
  author={Qiu, Jiantao and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and Xu, Ningyi and Song, Sen and others},
  booktitle={Proceedings of the 2016 ACM/SIGDA international symposium on field-programmable gate arrays},
  pages={26--35},
  year={2016}
}

@article{li2023firefly,
  title={Firefly: A high-throughput hardware accelerator for spiking neural networks with efficient dsp and memory optimization},
  author={Li, Jindong and Shen, Guobin and Zhao, Dongcheng and Zhang, Qian and Zeng, Yi},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume={31},
  number={8},
  pages={1178--1191},
  year={2023},
  publisher={IEEE}
}

@article{li2024firefly,
  title={Firefly v2: Advancing hardware support for high-performance spiking neural network with a spatiotemporal fpga accelerator},
  author={Li, Jindong and Shen, Guobin and Zhao, Dongcheng and Zhang, Qian and Zeng, Yi},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  year={2024},
  publisher={IEEE}
}

@inproceedings{chen2021hardware,
  title={Hardware Resource and Computational Density Efficient CNN Accelerator Design Based on FPGA},
  author={Chen, Xiang and Li, Jindong and Zhao, Yong},
  booktitle={2021 IEEE International Conference on Integrated Circuits, Technologies and Applications (ICTA)},
  pages={204--205},
  year={2021},
  organization={IEEE}
}

@article{li2024fireflys,
  title={FireFly-S: Exploiting Dual-Side Sparsity for Spiking Neural Networks Acceleration With Reconfigurable Spatial Architecture},
  author={Li, Tenglong and Li, Jindong and Shen, Guobin and Zhao, Dongcheng and Zhang, Qian and Zeng, Yi},
  journal={IEEE Transactions on Circuits and Systems I: Regular Papers},
  year={2024},
  publisher={IEEE}
}

@inproceedings{chen2023graph,
  title={Graph-opu: A highly integrated fpga-based overlay processor for graph neural networks},
  author={Chen, Ruiqi and Zhang, Haoyang and Li, Shun and Tang, Enhao and Yu, Jun and Wang, Kun},
  booktitle={2023 33rd International Conference on Field-Programmable Logic and Applications (FPL)},
  pages={228--234},
  year={2023},
  organization={IEEE}
}

@inproceedings{li2022auto,
  title={Auto-vit-acc: An fpga-aware automatic acceleration framework for vision transformer with mixed-scheme quantization},
  author={Li, Zhengang and Sun, Mengshu and Lu, Alec and Ma, Haoyu and Yuan, Geng and Xie, Yanyue and Tang, Hao and Li, Yanyu and Leeser, Miriam and Wang, Zhangyang and others},
  booktitle={2022 32nd International Conference on Field-Programmable Logic and Applications (FPL)},
  pages={109--116},
  year={2022},
  organization={IEEE}
}

@inproceedings{li2024revealing,
  title={Revealing Untapped DSP Optimization Potentials for FPGA-Based Systolic Matrix Engines},
  author={Li, Jindong and Li, Tenglong and Shen, Guobin and Zhao, Dongcheng and Zhang, Qian and Zeng, Yi},
  booktitle={2024 34th International Conference on Field-Programmable Logic and Applications (FPL)},
  pages={197--203},
  year={2024},
  organization={IEEE}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{huang2024edgellm,
  title={Edgellm: A highly efficient cpu-fpga heterogeneous edge accelerator for large language models},
  author={Huang, Mingqiang and Shen, Ao and Li, Kai and Peng, Haoxiang and Li, Boyu and Yu, Hao},
  journal={arXiv preprint arXiv:2407.21325},
  year={2024}
}

@article{li2024evaluating,
  title={Evaluating quantized large language models},
  author={Li, Shiyao and Ning, Xuefei and Wang, Luning and Liu, Tengxuan and Shi, Xiangsheng and Yan, Shengen and Dai, Guohao and Yang, Huazhong and Wang, Yu},
  journal={arXiv preprint arXiv:2402.18158},
  year={2024}
}

@inproceedings{liu2021hardware,
  title={Hardware acceleration of fully quantized bert for efficient natural language processing},
  author={Liu, Zejian and Li, Gang and Cheng, Jian},
  booktitle={2021 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  pages={513--516},
  year={2021},
  organization={IEEE}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{milakov2018online,
  title={Online normalizer calculation for softmax},
  author={Milakov, Maxim and Gimelshein, Natalia},
  journal={arXiv preprint arXiv:1805.02867},
  year={2018}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{elfwing2018sigmoid,
  title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier}
}