\section{Related Work}
LLMs are based on the Transformer architecture Vaswani, "Attention Is All You Need".
Hardware architecture for Transformer have been extensively studied in the pre-LLM era, with numerous hardware accelerators optimized for models such as ViT Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" and BERT Radford et al., "Improving Language Understanding by Generative Models".
HeatVit Huang et al., "HeatVit: A Hardware-Efficient Framework for Vision Transformers on Embedded FPGAs" proposed a hardware-efficient image-adaptive token pruning framework for efficient yet accurate ViT acceleration on embedded FPGAs. Liu et al., "BERT-QS: Fully Quantized BERT with a Novel Mixed-Precision Quantization Scheme on FPGA" proposed to fully quantize the BERT and propose an accelerator on Xilinx ZCU102 and ZCU11 FPGA and achieve high power efficiency.
While these methods have proven effective for ViT and BERT models, they cannot be directly applied to LLM workloads.

In the era of LLMs, research on hardware accelerators tailored for LLMs have gradually advanced.
As LLMs are memory-intensive, recent work has focused on accelerating LLMs using HBM-equipped cloud FPGAs, such as the Xilinx Alveo U280 and VCU128, achieving decoding speed improvements over GPUs in single-batch LLM inference setups.
DFX Zhang et al., "FPGA Acceleration of Decoder-only Transformers with DFX" represents one of the first studies on FPGA accelerators for decoder-only transformer, utilizing four banks of Alveo U280 to accelerate the GPT-2 1.5B model Brown et al., "Measuring Massive Multitask Models". FlightLLM Rajbhandari et al., "FlightLLM: Large Language Model Inference with Versatile FPGA" has deployed the LLaMA2-7B model Wolf et al., "Transformers at Scale" on both the Alveo U280 and the Versal VHK158, demonstrating advantages over NVIDIA GPUs in single-batch inference tasks. Chen et al., "A Comprehensive Analytical Framework for FPGA-Based LLM Spatial Acceleration with Alveo U280" provide a comprehensive analytical framework, offering the first in-depth examination of both the benefits and limitations of FPGA-based LLM spatial acceleration using the Alveo U280. EdgeLLM Rajbhandari et al., "EdgeLLM: Efficient Large Language Model Inference on Embedded FPGAs" adopts the VCU128, which is also a very large FPGA equipped with HBM, to deploy LLaMA2-7B, achieving a decoding speed higher than FlightLLM.

While these studies represent the initial steps in FPGA-based LLM acceleration for single-batch decoding, they are not applicable to real-world scenarios. Expensive server-class FPGAs are expected to handle multiple concurrent user inquiries, whereas single-batch decoding is more commonly seen in edge environments.
% However, these studies are not applicable to embedded FPGAs, which lack HBM and have limited memory capacity.
Accelerating single-batch decoding for LLMs with embedded FPGAs remains a challenging yet unexplored area, which will be the focus of this paper.