\begin{table*}[ht]
    \centering
    \caption{Quantitative results on the VFHQ dataset. }
    \label{tab:results_vfhq}
    \vspace{-.05in}
    \begin{tabular}{l|ccccccc|ccc}
        \toprule
        & \multicolumn{7}{c|}{\textbf{Self Reenactment}} & \multicolumn{3}{c}{\textbf{Cross Reenactment}} \\
        \textbf{Method} & \textbf{PSNR$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{LPIPS$\downarrow$} & \textbf{CSIM$\uparrow$} & \textbf{AED$\downarrow$} & \textbf{APD$\downarrow$} & \textbf{AKD$\downarrow$} & \textbf{CSIM$\uparrow$} & \textbf{AED$\downarrow$} & \textbf{APD$\downarrow$} \\
        \midrule
        StyleHeat~\citep{styleheat2022}             & 19.95      & 0.726      & 0.211      & 0.537      & 0.199      & 0.385      & 7.659      & 0.407      & 0.279      & 0.551      \\
        ROME~\citep{rome2022}                       & 19.96      & 0.786      & 0.192      & 0.701      & 0.138      & {0.186} & 4.986      & 0.530      & {0.259} & 0.277      \\
        OTAvatar~\citep{otavatar2023}               & 17.65      & 0.563      & 0.294      & 0.465      & 0.234      & 0.545      & 18.19      & 0.364      & 0.324      & 0.678      \\
        HideNeRF~\citep{hidenerf2023}               & 19.79      & 0.768      & 0.180      & 0.787      & 0.143      & 0.361      & 7.254      & 0.514      & 0.277      & 0.527      \\
        GOHA~\citep{goha2023}                       & 20.15      & 0.770      & {0.149} & 0.664      & 0.176      & {0.173} & 6.272      & 0.518      & 0.274      & {0.261} \\
        CVTHead~\citep{ma2024cvthead}               & 18.43      & 0.706      & 0.317      & 0.504      & 0.186      & 0.224      & 5.678      & 0.374      & 0.261      & 0.311      \\
        GPAvatar~\citep{gpavatar2024}               & {21.04} & {0.807} & 0.150      & 0.772      & {0.132} & 0.189      & {4.226} & 0.564      & {0.255} & 0.328      \\
        Real3DPortrait~\citep{ye2024real3d}         & 20.88      & 0.780      & 0.154      & {0.801} & 0.150      & 0.268      & 5.971      & \textbf{0.663} & 0.296      & 0.411      \\
        Portrait4D~\citep{deng2024portrait4d}       & 20.35      & 0.741      & 0.191      & 0.765      & 0.144      & 0.205      & 4.854      & 0.596      & 0.286      & {0.258} \\
        Portrait4D-v2~\citep{deng2024portrait4d2}   & {21.34} & {0.791} & {0.144} & {0.803} & {0.117} & 0.187      & {3.749} & {0.656} & 0.268      & 0.273      \\
        GAGAvatar~\cite{GAGAvatar} & 21.83 & 0.818 & 0.122 & 0.816 & 0.111 & 0.135 & 3.349 & 0.633 & 0.253 & \textbf{0.247} \\
        \midrule
        \textbf{Ours} & \textbf{22.65} & \textbf{0.829} & \textbf{0.109} & \textbf{0.822} & \textbf{0.102} & \textbf{0.134} & \textbf{2.059} & {0.651} & \textbf{0.250} & {0.356} \\
        \bottomrule
    \end{tabular}
    \vspace{-.05in}
\end{table*}

\begin{table*}[h]
\centering
\caption{Quantitative results on the HDTF dataset.}
\label{tab:results_hdtf}
    \vspace{-.05in}
\begin{tabular}{l|ccccccc|ccc}
\toprule
& \multicolumn{7}{c|}{\textbf{Self Reenactment}} & \multicolumn{3}{c}{\textbf{Cross Reenactment}} \\
\textbf{Method} & \textbf{PSNR$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{LPIPS$\downarrow$} & \textbf{CSIM$\uparrow$} & \textbf{AED$\downarrow$} & \textbf{APD$\downarrow$} & \textbf{AKD$\downarrow$} & \textbf{CSIM$\uparrow$} & \textbf{AED$\downarrow$} & \textbf{APD$\downarrow$} \\ 
\midrule
StyleHeat~\citep{styleheat2022}             & 21.41      & 0.785      & 0.155      & 0.657      & 0.158      & 0.162      & 4.585      & 0.632      & 0.271      & 0.239      \\
        ROME~\citep{rome2022}                       & 20.51      & 0.803      & 0.145      & 0.738      & 0.133      & {0.123} & 4.763      & 0.726      & 0.268      & 0.191      \\
        OTAvatar~\citep{otavatar2023}               & 20.52      & 0.696      & 0.166      & 0.662      & 0.180      & 0.170      & 8.295      & 0.643      & 0.292      & 0.222      \\
        HideNeRF~\citep{hidenerf2023}               & 21.08      & 0.811      & 0.117      & {0.858} & 0.120      & 0.247      & 5.837      & 0.843      & 0.276      & 0.288      \\
        GOHA~\citep{goha2023}                       & 21.31      & 0.807      & 0.113      & 0.725      & 0.162      & {0.117} & 6.332      & 0.735      & 0.277      & \textbf{0.136} \\
        CVTHead~\citep{ma2024cvthead}               & 20.08      & 0.762      & 0.179      & 0.608      & 0.169      & 0.138      & 4.585      & 0.591      & {0.242} & 0.203      \\
        GPAvatar~\citep{gpavatar2024}               & {23.06} & {0.855} & {0.104} & 0.855      & {0.114} & 0.135      & {3.293} & 0.842      & 0.268      & 0.219      \\
        Real3DPortrait~\citep{ye2024real3d}         & 22.82      & 0.835      & {0.103} & 0.851      & 0.138      & 0.137      & 4.640      & \textbf{0.903} & 0.299      & 0.238      \\
        Portrait4D~\citep{deng2024portrait4d}       & 20.81      & 0.786      & 0.137      & 0.810      & 0.134      & 0.131      & 4.151      & 0.793      & 0.291      & 0.240      \\
        Portrait4D-v2~\citep{deng2024portrait4d2}   & {22.87} & {0.860} & {0.105} & {0.860} & {0.111} & {0.111} & {3.292} & {0.857} & {0.262} & {0.183} \\
        GAGAvatar~\cite{GAGAvatar}                  & 23.13 & 0.863 & 0.103 & 0.862 & 0.110 & 0.111 & 2.985 & 0.851 & 0.231 & 0.181  \\
\midrule
        Ours                                        & \textbf{23.43} & \textbf{0.873} & \textbf{0.097} & \textbf{0.865} & \textbf{0.101} & \textbf{0.093} & \textbf{1.965} & 0.849 & \textbf{0.230} & 0.229  \\
\bottomrule
\end{tabular}
\label{tab:hdtf_results}
    \vspace{-.05in}
\end{table*}

\setlength{\tabcolsep}{3pt}
\begin{table*}[h]
    \centering
    \caption{Running time of reenactment and rendering measured in FPS. All results exclude the time for avatar reconstruction and driving parameters estimation that can be calculated in advance. The results are averaged over 100 frames.}
    \label{tab:fps}
    \vspace{-.1in}
    \begin{tabular}{l|cccccccc|ccc}
        \toprule
& \multicolumn{8}{c|}{Platform with A100 GPU} & \multicolumn{3}{c}{Ours on Different Platforms} \\
        Method & StyleHeat & ROME & HideNeRF & CVTHead & Real3D & P4D-v2 & GAGavatar & Ours & Macbook (M1 Pro) & iPhone 16 & Xiaomi 14 \\
        \midrule
        FPS    & 19.82 & 11.21 & 9.73 & 18.09 & 4.55 & 9.62 & 67.12 & \textbf{280.96} & 120 & 35 & 26 \\
        \bottomrule
    \end{tabular}
    \vspace{-.01in}
\end{table*}
\setlength{\tabcolsep}{5pt}


\section{Experiments}

\subsection{Experiments Setting}
\paragraph{\textbf{Implementation Details.}}
We implement our framework with PyTorch. We froze the DINOv2 image feature extraction backbone. The Transformer architecture consists of $L=10$ layers of basic Transformer blocks with 16 attention heads and 1024 feature dimensions, and the extracted features are mapped to Gaussian attributes with one Linear layer. We use estimated FLAME as our driving 3DMM. We train the network with ADAM optimizer and cosine warm-up scheduler for 200 epochs. We set the hyper-parameters $N_{f}=8$, $\lambda_{1}=\lambda_{2}=\lambda_{3}=1$, and $\lambda_{4}=0.1$ empirically.

\paragraph{\textbf{Datasets.}}
We utilize the VFHQ dataset~\cite{vfhq} for training our model, which consists of video clips from a variety of interview scenarios. The dataset comprises 15,204 video clips with 3M frames. For each frame extracted from the videos, we detect the face region, enlarge the bounding box to crop out the region of interest, and resize the cropped images to 512×512 pixels for uniformity following GAGAvatar~\cite{GAGAvatar}. To enhance the dataset's usability, we implement tracking for camera poses and FLAME parameters as in GaussianAvatar~\cite{GaussianAvatar}. Following the methodology established by \cite{GPAvatar}, we also conducted background removal to isolate the subjects in the frames. For evaluation purposes, we employ sampled frames from the original test split of the VFHQ dataset following previous works~\cite{GPAvatar,GAGAvatar}. In our setup, the first frame of each video is designated as the source image, while the subsequent frames act as driving and target images for the reenactment process. Additionally, we conduct evaluations using the HDTF dataset~\cite{HTDF} adhering to the test split established by prior works~\cite{OTAvatar}. This evaluation includes a selection of 19 video clips, complementing our analysis and contributing to a robust assessment of our model’s performance across different datasets. 

\paragraph{\textbf{Evaluation Metrics.}}
In our evaluation of single-image animatable Gaussian head avatar generation, we focus on self and cross-identity reenactment performance metrics. For self-reenactment, when ground truth images are available, we assess the quality of the synthesized images using three quantitative measures: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS). These metrics allow us to compare the synthetic images against the ground truth effectively. To evaluate identity similarity (CSIM), we compute the cosine distance of face recognition features, building on the work of~\cite{ArcFace}. For assessing expression and pose fidelity, we employ the Average Expression Distance (AED) and Average Pose Distance (APD) as determined by a 3D Morphable Model (3DMM) estimator~\cite{AEDAPD}. Additionally, we measure the Average Keypoint Distance (AKD) using a facial landmark detector as proposed by~\cite{AKD}. These metrics provide insights into the accuracy of the driving control in our animations. In cross-identity reenactment, where ground truth images are unavailable, we rely on CSIM, AED, and APD for evaluation. These metrics are in line with methodologies used in prior research~\cite{GAGAvatar}, allowing for a coherent comparison across different approaches. 


\subsection{Main Results}
\paragraph{\textbf{Qualitative Results.}}
Fig.~\ref{fig:vis_cross_reenact} compare the cross-reenactment results with other methods on the VFHQ dataset. Compared to previous methods, LAM achieves better reconstruction details on the textures, preserves better identity consistency, and presents more consistent expression and pose as the driven image. Note that our framework does not utilize any super-resolution and post-processing techniques and can be easily deployed on a traditional rendering pipeline for real-time rendering on various computing platforms, including mobile phones.

\paragraph{\textbf{Quantitative Results.}}
We also benchmark our framework on the VFHQ and HDTF dataset. Table~\ref{tab:results_vfhq} and Table~\ref{tab:results_hdtf} show the results on the two datasets respectively. As is shown in the table, our method achieves the best image reconstruction quality as is revealed from the PSNR, SSIM, and LPIPS metrics. Meanwhile, we preserve good identity consistency, as is shown with the CSIM metric. Furthermore, we obtain accurate expression and pose consistency with the driven image, as is revealed from the AED, APD, and AKD metrics. Notably, our framework achieves such quantitative results with extremely faster animation and rendering speeds, enabling real-time rendering on a wide range of devices.

\paragraph{\textbf{Inference Speed on Various Platforms.}} In the left side of Table~\ref{tab:fps}, we evaluate all methods on an A100 GPU. With a naive PyTorch framework and official 3D Gaussian Splatting implementation, our framework achieves extremely faster animation and rendering speed compared to existing methods. We also integrate our generated Gaussian avatar into WebGL framework and deploy them on various computing devices. As shown on the right side of Table~\ref{tab:fps}, our generated Gaussian avatar can perform real-time animation and rendering across a wide range of computing platforms, including mobile phones.

\begin{table}
    \caption{Effect of different design choices on the VFHQ dataset.}
    \vspace{-.1in}
    \label{tab:ablation}
    \small
    \begin{tabular}{l|ccc|ccc}
        \toprule
        Choices & LAM-5K & LAM-20K & LAM-80K & Tri. & PE & Paint \\
        % 5,143 & 20,426 & 81,424 \\
        \midrule
        PSNR          & 20.96  & 21.43  & \textbf{22.65}  & 21.33 & 20.96 & 20.87 \\
        FPS           & \textbf{705.63}  & 562.97  & 280.96 & 280.96 & 280.96 & 280.96 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{More Applications}

\paragraph{\textbf{Text to Animatable Gaussian Avatar Generation.}}
Our model trained on the large-scale dataset can scale up to various image styles, including generated images from the current text-to-image dataset, this enables us to generate animatable Gaussian avatars efficiently. In Fig.~\ref{fig:vis_text2image_gen}, we reconstruct the 3D Gaussian avatar from generated images by the existing text-to-image generation pipeline and animate them with different driven images. As is shown in the figure, the reconstructed Gaussian avatar preserves the details of different styles of image, which can be animated and rendered efficiently on different platforms given different driven expressions.
 
\paragraph{\textbf{Stylize Editing of Animatable Gaussian Avatar.}}
Fig.~\ref{fig:vis_image2image_gen} shows the effective results of applying our framework for style editing of animatable Gassian avatars. Unlike previous 3D editing frameworks~\cite{IN2NICCV,FreditorECCV,GaussianEditor} that require iterative training on multi-view images for stylization, our framework can edit different styles of the 3D Gaussian avatar efficiently utilizing a 2D editing prior models to edit the avatar in the 2D image and then lift it to 3D Gaussian space, enabling efficient and user-friendly editing of 3D assets. 

\subsection{Ablation Studies}

\paragraph{\textbf{Effect of Different Number of Gaussian Points.}}
We ablate the effect of different numbers of Gaussian points (5K, 20K and 80K) in the left part of Table~\ref{tab:ablation}. We can see that more Gaussian points obtained better reconstruction quality as they can describe more details like hair and mustache, while results in slower inference speed. We find that about 80 thousand points is the best trade-off.

\paragraph{\textbf{Effect of Query Representation.}}
We also compare widely used triplane-based query representation with our point-based query. Specifically, we replace the learnable query point features $F_P$ with flattened learnable triplane features for cross-attention with the image feature. Each sampled point then fetch its corresponding features from the triplane by projection for Gaussian attributes decoding. As shown in Table~\ref{tab:ablation}, with the same number of points, our point-based framework (LAM-80K) gets better results compared with triplane-based (Tri.).  

\paragraph{\textbf{Effect of Canonical Space Generation.}} We compare our canonical space generation with directly generating Gaussian avatar in the reference expression and pose (PE in Table~\ref{tab:ablation}) and then reenact it to the driven pose and expression. The canonical space simplifies the problem and gets better results. We use the same number of points (80K) for fair comparisons.

\paragraph{\textbf{Effect of Cross-Attention on Image Features.}} Compared with only applying cross-attention on the painted image feature as in~\cite{transhuman} (denoted as Paint in Table~\ref{tab:ablation}), directly performing cross-attention on the image features gets quite better results. We use the same number of points (80K) for fair comparisons.