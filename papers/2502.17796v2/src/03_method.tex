\section{Methodology}

\subsection{Overview}
Our approach begins with the development of a canonical Gaussian attribute generator, utilizing a Transformer network. We incorporate pre-trained Vision Transformer (ViT) weights to effectively extract features at multiple levels from one single-view image. To fully leverage the prior shape information and expression coefficients provided by the FLAME~\cite{DBLP:journals/tog/LiBBL017} model, we employ vertices from the FLAME model as initial Gaussian positions. These vertices reason geometric relationships through stacked self-attention and extract corresponding features from the images within stacked cross-attention modules, allowing us to predict the canonical Gaussian attributes and deformation offsets to improve the shape accurately.
Given a new expression and pose, we utilize the animation weights from the FLAME model to animate the reconstructed canonical Gaussian points accordingly. This process facilitates the rendering of the image, reflecting the desired expression and pose with accuracy and enabling interactive frame rates of animation and rendering on a wide range of computing platforms, including mobile phones.

\vspace{-0.05in}

\subsection{Canonical Gaussian Head Avatar Generation}
\label{sec:CanonicalGSGen}
In this section, we outline our approach for reconstructing an animatable avatar in the canonical space from a single input image. Our objective is to create an avatar that can be animated and rendered seamlessly, without the need for additional neural networks or post-processing. This allows for straightforward integration into existing rendering pipelines and enables interactive frame rates across various computing devices.

\paragraph{\textbf{Canonical Gaussian Head Avatar Reconstruction.}} We draw inspiration from the success of Gaussian Splatting~\cite{GaussianSplatting} in novel view synthesis and its efficient rendering capabilities. Therefore, we design our model to generate the Gaussian head avatar. However, this task presents certain challenges. The first hurdle lies in enabling the animation of the reconstructed Gaussian without relying on extra neural networks. Previous studies~\cite{DBLP:conf/cvpr/XuCL00ZL24} have explored dynamic Gaussians that utilize multi-layer perceptrons (MLPs) to transform canonical Gaussians into new expressions by predicting the residual attributes and rendered images with new expressions. However, the reliance on additional neural networks hampers compatibility with traditional rendering workflows and hinders real-time rendering on platforms with less compatibility, such as mobile phones. To address this issue, we incorporate insights from the animatable FLAME model, which is animatable without extra neural networks. By using FLAME as a foundation for constructing our animatable Gaussian, we leverage its animation pipeline to animate our reconstructed avatar.

The second challenge involves reconstructing an animatable Gaussian from a single image. To overcome this, we propose three main designs for our framework: 
1) Firstly, unlike previous methods~\cite{LGM,OTAvatar} that represent and reconstruct shapes from tri-plane representation, we represent the shape with a point-cloud-like technique and initialize the point position with canonical vertices of the FLAME model. In this way, we can build the avatar with explicit shape and leverage the shape prior that resides in the FLAME model, which alleviates the reconstruction challenge. 
2) Secondly, instead of directly reconstructing the avatars with various poses and expressions, we reconstruct all Gaussian avatars within the same canonical coordinate with the same canonical expression and pose, which are then animated to the target pose and expression. Such a unified design not only enables convenient animation during inference but also mitigates the complexities associated with the reconstruction by reducing the shape and pose variety. 
3) Finally, unlike previous methods~\cite{transhuman} that only utilize the painted image features lifted from projections for texture and shape reasoning, we thoroughly utilize both local and global features extracted from the source image by building cross-attention modules between the point and image features, which improve the reconstruction quality and texture fidelity.

As illustrated in Fig. \ref{fig:framework}, we begin by employing the pre-trained DinoV2 model~\cite{DinoV2} to extract features from the input image. By fusing features derived from both shallow and deep layers, we obtain image features $F_{I}$, that capture essential local details, such as hair strands and wrinkles, while ensuring global resemblance to the original image. To incorporate prior shape information from the FLAME model, we initial the Gaussian positions based on the FLAME's vertices after shape blendshapes. Then we attach learnable features to each point as learnable query features $F_{P}$. In implementation, we utilize multi-layer perceptron (MLP) to project the positional encoding~\cite{NeRF} of each point to get $F_{P}$. These learnable features are trained to extract features from the images feature $F_I$ with stacked cross-attention modules $\mathcal{C} = \{\mathcal{C}_i\}^{L}_{i=1}$. Given the query point features $F_{P}$ and the extracted multi-level image features $F_{I}$, we flatten the features and process them with stacked cross-attention modules as:
\begin{equation}
    F_{P_{i}} = \mathcal{C}_{i}(F_{P_{i-1}}, F_I),
\end{equation}
where $\mathcal{C}_i$ is the $i_{th}$ cross-attention module, $F_{P_{i}}$ the $i_{th}$ layer's queried features given previous layers' output $F_{P_{i-1}}$ as query, with $F_{P_{1}} = F_P$.
This architecture enables the model to interpret geometric information and relationships within the point cloud via self-attention, while simultaneously using cross-attention to extract relevant appearance features from the image. Another advantage of this framework design is that we can leverage the scaling law of the Transformer architecture, which has been proven on ChatGPT~\cite{} for text generation and ~\cite{ScalableDiffusionModelsWTransformers} for image generation. 
The advantage of this framework design is that we can leverage the scaling law of the Transformer architecture, which has been proven on~\cite{LLAMA} for text generation and ~\cite{ScalableDiffusionModelsWTransformers} for image generation. We instead, show that our designed transformer architecture can also be scaled up for the animatable Gaussian avatar reconstruction.

After the feature extraction process, each point retains its unique features. Leveraging these features, we construct decoding headers $\mathcal{D}$ made up of MLPs to predict each point's Gaussian attributes, including color $c_k \in \mathbb{R}^3$, opacity $o_k \in \mathbb{R}$, per-axis scale factors $s_{k} \in \mathbb{R}$, and rotation $R_k \in SO(3)$. Since the FLAME only provides the coarse shape information of a person, without details like hairs, we also predict each point's offset to obtain a more detailed shape of the person, denoted as $O_k \in \mathbb{R}^3$. The decoding process can then be denoted as:
\begin{equation}
    \{c_k, o_k, s_k, R_k, O_k\}^{M}_{k=1} = \mathcal{D}(F_{P_{L}}),
\end{equation}
where $M$ is the total number of Gaussians. Consequently, we reconstruct the Gaussian avatar in the canonical space, which can then be animated with new expressions and rendered through a splatting process to produce the final image.


\paragraph{\textbf{FLAME Subdivision.}}
We utilize FLAME vertices as initialization of the Gaussian position and also utilize them to query features from the image to predict the Gaussian attributes. However, the original number of vertices on the FLAME template model is 5023, which is small and Gaussian avatar with such a number of points are not able to reconstruct details like hair, moustache, and teeth well. Under such observation, we propose to increase the number of points from FLAME as initialization. 
Specifically, we utilize the mesh subdivision algorithm to increase the number of mesh faces and vertices. Specifically, for each subdivision iteration, the triangle mesh is subdivided by adding a new vertex at the center of each edge and dividing each face into four new faces. We also attach blendshapes attributes to each vertex, and subdivide them by averaging the values of the attributes at the two vertices that form each edge. In this way, we increase the number of vertices and obtain the blendshapes attributes of each vertice for animation. These blendshapes attributes are also applied to each Gaussian point for the animation of the reconstructed Gaussian avatar. We can apply different numbers of mesh subdivision iterations to obtain different numbers of vertices as initialization. More points will lead to more detailed reconstruction but at a lower rendering speed. By default, we apply mesh subdivision twice to obtain $M=81,424$ vertices as initialization for $M$ Gaussian points, which is the best trade-off between reconstruction quality and rendering speed from our experiments.


\subsection{New Expression Animation}
Given the reconstructed canonical Gaussian head avatar, we introduce how to reenact them into new FLAME expressions in this section. Since our Gaussian positions are initialized from FLAME and attached with their corresponding animation attributes, we can animate the model utilizing standard vertex-based linear blend skinning (LBS) and corrective blendshapes as the FLAME model did. We first have a brief review of how the FLAME model is animated. Denote $\bar{T}$ the vertexes of the FLAME template mesh, $B_{S}(\vec{\beta})$, the shape blendshape function, $B_{P}(\vec{\theta})$ the pose blendshapes function, $B_{E}(\vec{\phi})$ the expression blendshape function, and $\mathcal{S}(\bar{T}, \textbf{J}, \vec{\theta}, \mathcal{W})$ the standard skinning function to rotate the vertexes $\bar{T}$ around joints $\textbf{J}$ and smoothed by blendweights $\mathcal{W}$, the FLAME model can be defined as:
\begin{equation}
    \label{eqn:LBS}
    F(\vec{\beta}, \vec{\theta}, \vec{\phi}) = \mathcal{S}(T_{P}(\vec{\beta}, \vec{\theta}, \vec{\phi}), \textbf{J}(\vec{\beta}), \vec{\theta}, \mathcal{W}),
\end{equation}
where
\begin{equation}
    \label{eqn:blenshape}
    T_{P}(\vec{\beta}, \vec{\theta}, \vec{\phi}) = \bar{T} + B_{S}(\vec{\beta}; \mathcal{S}) + B_{P}(\vec{\theta}; \mathcal{P}) + B_{E}(\vec{\phi}; \mathcal{E}),
\end{equation}
with $T_{P}(\vec{\beta}, \vec{\theta}, \vec{\phi})$ the template with added shape, pose, and expression offsets. Our reconstructed Gaussian avatar can be reenacted similarly by regarding the position location as the vertices of FLAME and animating it with the blendshape and LBS function to obtain the animated new Gaussian positions. The difference is that we utilize the vertices of the subdivided FLAME mesh after shape blendshapes $\bar{T} + B_{S}(\vec{\beta})$ as the initial Gaussian position and our networks predict the XYZ offset of each point to further improve the shape details in the canonical space, the function in Formula~(\ref{eqn:blenshape}) can be revised as:
% \weihaoquestion{without $\beta$ ?}
\begin{equation}
    \label{eqn:gsblendshape}
    T_{G}(\vec{\theta}, \vec{\phi}) = \bar{G} + B_{P}(\vec{\theta}; \mathcal{P}) + B_{E}(\vec{\phi}; \mathcal{E}), 
\end{equation}
where 
\begin{equation}
    \label{eqn:gsshapeblenshape}
    \bar{G} = \bar{T} + B_{S}(\vec{\beta}; \mathcal{S}) + O,
\end{equation}
with $O = \mathcal{N}(\bar{T} + B_{S}(\vec{\beta}; \mathcal{S}))$ the predicted offset from our network $\mathcal{N}$ given shape blended vertexes as input. Note that our shape blendshapes procedure denotes in Formula~(\ref{eqn:gsshapeblenshape}) only computes once and can be fixed once the network is forwarded and the canonical Gaussian avatar $\bar{G}$ is reconstructed. Meanwhile, we can also pre-compute the shape blendshapes to the joint once as $\bar{J}=\textbf{J}(\vec{\beta})$. Thereafter, we can then pass the reconstructed $\bar{G}$ into the LBS function to animate the Gaussian avatar at a fast speed during inference as:
\begin{equation}
    \label{eqn:LBSGS}
    F_{G}(\vec{\theta}, \vec{\phi}) = \mathcal{S}(T_{G}(\vec{\theta}, \vec{\phi}), \bar{J}, \vec{\theta}, \mathcal{W}).
\end{equation}
After the position of the Gaussian is reanimated, we perform splatting on the reanimated Gaussian and render the new image. Note that the animation processing in Formula~(\ref{eqn:LBSGS}) and the rendering does not have any neural network and postprocessing, which can be directly integrated into the traditional rendering pipeline developed for various devices, enabling fast deployment, which we will discuss in Sec.~\ref{sec:deploy_various_devices}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/edit.pdf}
    % \vspace{1.5in}
    \caption{Text to animatable Gaussian avatar generation and editing pipelines.}
    \label{fig:text-style-gen}
    \vspace{-0.1in}
\end{figure}

\subsection{Text to Animatable Gaussian Avatar Generation and Editing.}
Our approach takes full advantage of the shape information and animation attributes provided by the FLAME model and the robust capabilities of the DinoV2 image feature extractor, which is trained on extensive image datasets. Furthermore, after training on large-scale datasets, the scaling capability of the designed Transformer-based architecture allows our framework to adapt effectively to various input styles, including those derived from generated images. Building on this foundation, we have designed a streamlined pipeline for converting text prompts into animatable Gaussian avatars, along with an efficient style editing process, as illustrated in Fig.~\ref{fig:text-style-gen}. To generate target portrait images based on textual prompts, we employ established text-to-image generation frameworks, such as Stable Diffusion~\cite{stablediffusion}. The resulting images are then processed by our system to create animatable Gaussian avatars through a feed-forward mechanism. In addition, our framework is equipped to facilitate style editing; we can take an input image and utilize an image-to-image translation framework to modify the portrait's style, such as changing the age or transforming it into a cartoon representation. This edited image is subsequently fed into our model to produce animatable Gaussian avatars that reflect the desired styles.

\subsection{Deployment to Traditional Rendering Pipeline on Various Platforms.}
\label{sec:deploy_various_devices}
To deploy our Gaussian avatar on various computing platforms, we choose WebGL as our implementation framework, the application of which can be run on web browsers of different devices. Specifically, since the animation mainly consists of matrix operations, and the computing is highly parallelized, we pass the blenshapes and LBS information as texture to the GLSL vertex shader and use transform feedback for efficient GPU computing. For Gaussian splating, we also implement a WebGL version. In this way, we enable animation and rendering of reconstructed Gaussian avatars on different devices. Our interactive viewer is an HTML webpage with Javascript, rendered by WebGL.

\subsection{Optimization and Regularization}

During training, we randomly sample $N_f$ frames of images from the same video, selecting one as the reference image to reconstruct the canonical Gaussian avatar and others as the driving images and target images.
We supervise the rendered RGB image with the ground truth target images with a combination of $\mathcal{L}_1$ loss and perceptual loss:
\begin{equation}
    \mathcal{L}_{rgb} = \lambda_{1}\mathcal{L}_1 + \lambda_{2}\mathcal{L}_{lpips}.
\end{equation}
We also render the silhouette and supervised it with $\mathcal{L}_1$ loss denoted as $\mathcal{L}_{mask}$.

We predict the deformation offset for each point to obtain a better shape that can cover areas not modeled by FLAME, such as hair and accessories. Without constraint, the points may move freely and hurt the animation results. Therefore, we also add a regularization on the predicted offset:
\begin{equation}
    \mathcal{L}_{o} = \mathcal{L}_2(O, \epsilon),
\end{equation}
where $O$ is the predicted offset, $\epsilon$ an hyper parameters set close to 0 so that each point will not deform too much.
The total loss function is a weighted sum of the above items as:
\begin{equation}
    \mathcal{L} = \lambda_1\mathcal{L}_1 + \lambda_2\mathcal{L}_{lpips} + \lambda_3\mathcal{L}_{mask} + \lambda_4\mathcal{L}_{o}.
\end{equation}




