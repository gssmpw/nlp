
\section{Related work}


Recent advances in single-image animatable head avatar generation can be categorized into mainly 2D-based and 3D-based approaches. 

\paragraph{\bf Image to 2D Animatable Avatar.}
2D-based methods, leveraging the power of convolutional neural networks (CNNs)~\cite{DBLP:conf/cvpr/KarrasLAHLA20,DBLP:conf/cvpr/IsolaZZE17,DBLP:conf/nips/GoodfellowPMXWOCB14}, often employ generative adversarial networks (GANs)~\cite{DBLP:conf/cvpr/StyleGAN} for direct image synthesis. Early approaches~\cite{DBLP:conf/cvpr/WangDYSW23,DBLP:conf/cvpr/BurkovPGL20,DBLP:conf/iccv/ZakharovSBL19} focus on injecting expression and pose features into the generator network, often utilizing architectures like U-Net or StyleGAN~\cite{DBLP:conf/cvpr/StyleGAN}.
Some other 2D methods~\cite{DBLP:journals/corr/abs-2407-03168,DBLP:conf/cvpr/ZhangQZZW0CW023,DBLP:conf/cvpr/HongZS022,DBLP:conf/mm/DrobyshevCKILZ22,DBLP:conf/cvpr/BurkovPGL20,DBLP:conf/nips/SiarohinLT0S19} represent expressions and poses as warping fields applied to the source image. 
Benefiting from advances in image and video diffusion networks, more recent 2D-based works~\cite{DBLP:journals/corr/abs-2410-07718,DBLP:journals/corr/abs-2406-08801,DBLP:conf/eccv/TianWZB24} get improved results with diffusion techniques. 
However, these methods still face challenges related to long generation times and significant computational resource demands. Audio-driven 2D control methods~\cite{DBLP:conf/cvpr/ZhangCWZSGSW23,DBLP:journals/corr/abs-2211-12368,DBLP:conf/iccv/GuoCLLBZ21} are easy to use but cannot explicitly control facial expressions and poses. 2D-based techniques often struggle with large pose or expression variations due to the lack of an explicit 3D structure, sometimes producing unrealistic distortions or identity changes. While some 2D methods~\cite{SadTalker,StyleHEAT,Pirenderer,DBLP:conf/cvpr/WangM021,MegaPortraits} incorporate 3D Morphable Models (3DMMs)~\cite{DBLP:conf/fgr/GerigMBELSV18,DBLP:journals/tog/LiBBL017,DBLP:conf/avss/PaysanKARV09,DBLP:conf/siggraph/BlanzV99} to mitigate these issues, they typically cannot achieve free-viewpoint rendering. 

\vspace{-0.1in}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{images/framework.pdf}
    \caption{\textbf{Overall Framework.} Our framework utilizes learnable query features attached to FLAME vertices to perform cross-attention with the extracted multi-level image features. The extracted features are then decoded to reconstruct the Gaussian avatar in the canonical space, which can be animated utilizing standard linear blend skinning (LBS) and corrective blendshapes as the FLAME model did and rendered in real-time on various platforms.}
    \label{fig:framework}
\end{figure*}

\paragraph{\bf Image to 3D Animatable Avatar.}
3D-aware methods offer improved geometric consistency and free-viewpoint rendering capabilities. Early 3D approaches~\cite{DBLP:conf/eccv/KhakhulinSLZ22,DBLP:conf/cvpr/XuYCWDJT20} utilize 3DMMs for head avatar reconstruction. With the advent of Neural Radiance Fields (NeRFs)~\cite{DBLP:conf/eccv/MildenhallSTBRN20}, many recent methods~\cite{DBLP:conf/siggraph/YuFZWYBCSWSW23,DBLP:conf/cvpr/MaZQLZ23,DBLP:conf/cvpr/LiZWZ0CZWB023,GPAvatar,ye2024real3d,deng2024portrait4d,deng2024portrait4d2,DBLP:conf/eccv/KiMC24,DBLP:conf/cvpr/BaiFWZSYS23,PointAvatar,Nerfies,INSTA} have adopted this representation for higher fidelity, particularly in modeling fine details like hair. However, NeRF-based~\cite{DBLP:conf/cvpr/ZhangZLHLWGCL024,HAvatar,DBLP:conf/cvpr/BaiTHSTQMDDOPTB23,AD-NeRF,DBLP:journals/tog/GaoZXHGZ22,DBLP:journals/tog/ParkSHBBGMS21,DBLP:conf/cvpr/AtharXSSS22,DBLP:journals/corr/abs-2112-05637,DBLP:conf/iccv/TretschkTGZLT21,DBLP:conf/cvpr/GafniTZN21,DBLP:conf/eccv/KiMC24,DBLP:conf/cvpr/BaiFWZSYS23,PointAvatar,Nerfies,DBLP:conf/siggraph/YuFZWYBCSWSW23,DBLP:conf/cvpr/MaZQLZ23,DBLP:conf/cvpr/LiZWZ0CZWB023} approaches often require extensive training data, including multi-view or single-view videos, raising privacy concerns and limiting generalization to unseen identities. Some methods~\cite{DBLP:conf/cvpr/SunWWLZZL23,DBLP:conf/3dim/ZhuangMKS22,DBLP:journals/pami/SunWZHWL24,DBLP:journals/tvcg/TangZYZCMW24,DBLP:conf/iclr/XuZLZBFS23} bypass this data requirement by training generators with random noise and then inverting them for identity-specific reconstruction, but inversion accuracy remains a challenge. Test-time optimization offers another alternative, but its computational cost limits practical applications. Several recent works~\cite{goha2023,hidenerf2023,gpavatar2024,ye2024real3d,ma2024cvthead,deng2024portrait4d,deng2024portrait4d2,GGHead} have explored one-shot 3D head reconstruction to address the limitations of data requirements and computational cost. These methods employ various techniques, such as tri-plane features, deformation fields, point-based expression fields, and vertex-feature transformers. Despite these advancements, NeRF-based methods often struggle with real-time rendering. 
Recently, 3D Gaussian Splatting~\cite{GaussianSplatting} has emerged as a promising alternative, offering both high-quality results and fast rendering speeds. However, existing Gaussian Splatting methods~\cite{GaussianAvatar,DBLP:conf/cvpr/XuCL00ZL24} typically rely on video data for training for each person, limiting their ability to generalize to new identities. Instead, the most recent work, GAGAvatar~\cite{GAGAvatar}, proposes a one-shot 3D Gaussian-based head avatar generation method. However, it still relies heavily on complex 2D neural post-processing to achieve optimal animation outcomes, thus it is not a pure 3D solution and the extra neural network hinders its application on various platforms. In contrast, our work generates Gaussian heads that are immediately animatable and renderable without additional networks or post-processing steps, enabling seamless integration into existing rendering pipelines for real-time animation and rendering across a wide range of platforms, including mobile phones. 