
\section{Introduction}

The reconstruction and animation of head avatars from a single image is an important area of research within computer graphics and computer vision. This area has a wide range of practical applications, including 
online meetings, filmmaking, the gaming industry, and virtual reality.
Researchers have tackled this challenge using different methodologies, primarily focusing on 2D and 3D generative models.

Early 2D methods~\cite{DBLP:conf/cvpr/KarrasLAHLA20,DBLP:conf/cvpr/IsolaZZE17,DBLP:conf/nips/GoodfellowPMXWOCB14} harness the capabilities of convolutional neural networks and generative adversarial networks to depict facial expressions and poses via warping fields applied to source images. Benefiting from advances in image and video diffusion networks, more recent 2D-based works~\cite{DBLP:journals/corr/abs-2410-07718,DBLP:journals/corr/abs-2406-08801,DBLP:conf/eccv/TianWZB24} get improved results with diffusion techniques. 
Although achieving impressive results, 2D solutions still face challenges related to long generation times and significant computational resource demands.
Moreover, 2D techniques often struggle with extreme variations in poses and expressions due to their lack of explicit three-dimensional structures.

On the other hand, the emergence of advanced 3D synthesis techniques, such as Neural Radiance Fields (NeRFs) and Gaussian Splatting, has enhanced the consistency of multi-view avatar reconstruction and animation. Nonetheless, NeRF-based~\cite{DBLP:conf/cvpr/GafniTZN21,DBLP:conf/eccv/KiMC24,DBLP:conf/cvpr/BaiFWZSYS23,PointAvatar,Nerfies,DBLP:conf/siggraph/YuFZWYBCSWSW23,DBLP:conf/cvpr/MaZQLZ23,DBLP:conf/cvpr/LiZWZ0CZWB023} methods usually require extensive optimization from a single image for each individual before effective use, and their slow rendering speeds limit their application in scenarios demanding real-time performance. 

Recent developments, like GAGAvatar~\cite{GAGAvatar}, introduced a framework for single-shot Gaussian avatar generation and animation that capitalizes on the quick rendering prowess of Gaussian Splatting. 
However, these methods usually rely heavily on complex 2D neural post-processing to achieve optimal outcomes, thus they are not pure 3D solutions and may encounter similar problems presented in 2D solutions.
In addition, their severe reliance on neural network post-processing makes their integration into traditional rendering pipelines across various platforms cumbersome.

In this paper, we introduce a large avatar model for generating animatable Gaussian head avatars. After one pass forward of the network given a single image, our approach generates the Gaussian avatar that can be instantly reenacted and rendered without any post-processing network. This capability allows for seamless integration into existing rendering pipelines and ensures efficient performance across a wide range of devices, including mobile devices. 

However, this task presents certain challenges. 
\textbf{1)} The first hurdle lies in enabling the animation of the reconstructed Gaussian without relying on extra neural networks. Unlike previous studies that use neural networks~\cite{GAGAvatar} for animation and rendering, our method generate Gaussian avatar based on the FLAME animation model, which operates and renders without additional neural networks, allowing real-time application on various platforms.
\textbf{2)} The second challenge involves reconstructing an animatable Gaussian from a single image. To overcome this, we propose three main designs for our framework. 
\textbf{i)} Firstly, unlike previous methods that reconstruct shapes from implicit tri-plane~\cite{LGM} or image plane~\cite{GAGAvatar}, we represent the shape with explicite point cloud initialized from canonical FLAME vertices. In this way, we leverage the prior avatar shape resides in the FLAME model to alleviates the reconstruction challenge. 
\textbf{ii)} Secondly, instead of directly reconstructing the avatars with various poses and expressions, we reconstruct all Gaussian avatars in the canonical space with the same expression and pose. Such a unified design not only enables convenient animation during inference but also mitigates the reconstruction complexities by reducing the shape and pose variety. 
\textbf{iii)} Finally, unlike previous methods~\cite{transhuman} that only utilize the painted image features for texture and shape reasoning, our framework thoroughly reason on both local and global image features within a cross-attention mechanism, improving the reconstruction quality and texture fidelity. 

The principal contributions of this work can be summarized as follows:

\begin{itemize}[leftmargin=*]
\item We introduce a large avatar model for generating the animatable Gaussian head avatar from one image, allowing instant animation and rendering without additional post-processing.
\item The generated Gaussian avatars from our framework can be seamlessly integrated into traditional rendering pipelines, supporting real-time animation and rendering across various platforms.
\item Built upon our large animatable Gaussian avatar model, we enable the efficient generation and stylization of animatable Gaussian avatars from a single text prompt or image.
\item Experimental results on benchmark datasets demonstrate the efficacy and efficiency of our approach.
\end{itemize}