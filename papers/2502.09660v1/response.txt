\section{Related Work}
\label{sec:related}

\subsection{Interactive Segmentation}
Interactive segmentation enables users to provide cues for target regions and guide the segmentation process. User interactions can take various forms, including scribbles**Qu et al., "Learning to Segment with Scribbles"**, bounding boxes**Kanezaki, "Object Detection via Bounding Box Proposal Network"**, clicks**Chen et al., "Clickstream Segmentation"**, or language prompts**Yin et al., "Language-Driven Interactive Image Segmentation"**. Traditional methods utilize pixel-level energy minimization methods, capturing low-level appearance features through unary potentials and ensuring consistent segmentation results with pairwise ones**Boykov et al., "Fast Approximate Energy Minimization via Graph Cuts"**. 
In recent years, many studies leverage prompts as input features and inject them into the models to produce segmentation results**Hou et al., "Prompt-Based Image Segmentation Model"**. The Segment Anything Model (SAM)**Pang et al., "Segment Anything Model: Real-Time Object Detection with High Accuracy"** pre-trained on a large-scale datasets has emerged as a benchmark for interactive segmentation. 
Considering that SAM cannot perform semantic prediction, many works**Dai et al., "Semantic Segmentation via Joint Learning of Category Labels and Prompts"** utilize category labels to further fine-tune SAM and enable it to achieve the semantic segmentation task. For example, SEEM**Li et al., "Segment Everything Model: A Framework for One-Shot Semantic Segmentation"** further trains SAM using labeled segmentation data through a bipartite matching constraint, thus providing the model with the ability to predict semantics. Semantic-SAM**Chen et al., "Semantic-Segment Anything Model: Multi-Choice Learning via Multi-Task Training"** proposes a multi-choice learning scheme via multi-task training across diverse datasets, enabling the model to segment at various granularities while predicting semantic labels.
Other works**Wang et al., "Medical Image Segmentation using Segment Anything Model"** apply SAM to diverse specific domains to solve application tasks. MedSAM**Hou et al., "Med-Segment-Anything Model: Fine-Tuning for Medical Image Segmentation"** fine-tunes SAM by collecting a large-scale dataset of medical image samples in multiple imaging modalities and types of cancer, achieving superior results in disease segmentation.
Gaussian-Grouping**Liu et al., "Gaussian-Grouping Network: Joint Reconstruction and Segmentation of Objects in 3D Scenes"** leverages the SAM as supervision model to jointly train a reconstruction and segmentation of objects in open-world 3D scenes for high-quality 3D edit.
% For instance, SAM functions as a robust pretrained encoder-decoder model that can be fine-tuned for specific tasks**Pang et al., "Segment Anything Model: Real-Time Object Detection with High Accuracy"**. Some other approaches maintain the interactive nature of segmentation by focusing on parameter-efficient fine-tuning techniques**Liu et al., "Efficient Fine-Tuning for Interactive Segmentation"**. 
% To achieve robust segmentation across diverse scenarios, some methods fine-tune SAM’s decoder using large-scale domain datasets**Wang et al., "Domain-Adaptive Segment Anything Model for Robust Segmentation"**. 
% Although these approaches show notable advantages across various benchmarks, they have yet to be consistently evaluated against similar methods, and a comprehensive analysis of how different interaction strategies influence outcomes is still lacking.


%-------------------------------------------------------------------------
% %%%%%%%%%%%% figure 3
\begin{figure*}
\begin{center}
%\setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{-0.1cm}
\includegraphics[width=0.98\linewidth]{pic/architecture.png}
\caption{
An overview of the proposed framework SAM2Refiner. It contains a localization augment module to balance the detailed and semantic representations, a prompt retargeting for enhancing response of input prompts and a mask refinement structure to boost the quality of mask outcomes. The blue line denotes the SAM2 pipeline, and the black line denotes our SAM2Refiner pipeline.
}
\label{fig:network}
\end{center}
\end{figure*}
%-------------------------------------------------------------------------



\subsection{High-Quality Segmentation}
High-quality segmentation as a fundamental computer vision task focuses on accurately segmenting various complex and detail-rich objects, which ranges from many sub-tasks including semantic segmentation**Chen et al., "Semantic Segmentation with Graph Attention Networks"**, instance segmentation**Li et al., "Instance Segmentation with Mask R-CNN"** and panoptic segmentation**Xiong et al., "Panoptic-DeepLab: Bringing Instance and Semantic Segmentation with End-to-End Fully Convolutional Network"**. 
% Recent researches like SegGPT**Hou et al., "SegGPT: In-Context Visual Learning for Diverse Segmentation Tasks"** enable diverse segmentation tasks through in-context visual learning.
% Existing methods for high-quality segmentation are tailored to specific tasks, such as image and video semantic segmentation**Chen et al., "Semantic Segmentation with Graph Attention Networks"**, instance segmentation**Li et al., "Instance Segmentation with Mask R-CNN"** or panoptic segmentation**Xiong et al., "Panoptic-DeepLab: Bringing Instance and Semantic Segmentation with End-to-End Fully Convolutional Network"**. 
Traditional methods**Wang et al., "High-Quality Segmentation via Hierarchical Feature Fusion"** on high-quality segmentation dedicate to designing elaborate structures upon on CNN-based networks in order to capture both low-level local patterns and semantic global features through various reception fields towards holistic representation.
Deeplab-v3**Chen et al., "DeepLab: Scene Parsing with Mask R-CNN"** designs a set of multi-level convolution kernels to capture features with diverse reception fields and HRNet**Sun et al., "High-Resolution Networks for Semantic Segmentation"** employs cascaded structures to fuse features from different layers to learn both low-level and high-level information.
% More recently, many approaches focus on post-segmentation refinement using graphical models like CRF**Ladicky et al., "Graphical Models for Object Detection, Tracking, and Stereo Vision"** and other utilize separate deep networks for iterative improvements**Wang et al., "Iterative Segmentation Refinement with Deep Networks"**.
More recently, many approaches**Zhang et al., "Mask2Former: A Novel Architecture for High-Quality Image Segmentation"** built on the transformer backbone focus on introducing local patterns into the model by narrow the reception field of self-attention module. For example, Mask2Former**Chen et al., "Mask2Former: A Novel Architecture for High-Quality Image Segmentation"** presents an efficient versatile architecture by using masked attention to extract local features for high-quality image segmentation tasks. Swin-Unet**Wang et al., "Swin-Unet: Unifying Object Detection and Instance Segmentation"** adopts the Swin-Transformer network to the UNet like structures to adaptively capture object features with various scales via hierarchical attention.
Latest, the Pi-SAM**Hou et al., "Pi-SAM: Improving Segment Anything Model for High-Quality Image Segmentation"** designs a mask decoder to expands SAM’s ability at high resolution images, however, it may diminish the response of the input prompt and exhibit biased preferences for background clicks. 
% In contrast, our work emphasizes accurately segmenting diverse objects in new data using flexible prompting. We propose HQ-SAM, a high-quality zero-shot segmentation model that generalizes across various tasks and domains. Unlike post-segmentation refinement approaches, HQ-SAM directly predicts high-quality masks by reusing the image encoder and mask decoder from SAM, rather than relying on a coarse mask as input for a separate refinement network. Our model architecture builds upon SAM with minimal overhead, incorporating efficient token learning for accurate mask predictions. We demonstrate its effectiveness through extensive zero-shot experiments.