\section{Related Work}
\label{sec:related}

\subsection{Interactive Segmentation}
Interactive segmentation enables users to provide cues for target regions and guide the segmentation process. User interactions can take various forms, including scribbles~\cite{sam,sam2,hq-sam,SegGPT}, bounding boxes~\cite{sam,hq-sam,liu2024segment}, clicks~\cite{yuan2020segfix,sam}, or language prompts~\cite{SegGPT,SEEM}. Traditional methods utilize pixel-level energy minimization methods, capturing low-level appearance features through unary potentials and ensuring consistent segmentation results with pairwise ones~\cite{wang2023segrefiner}. 
In recent years, many studies leverage prompts as input features and inject them into the models to produce segmentation results~\cite{sam,sam2,Semantic-SAM,SEEM}. The Segment Anything Model (SAM)~\cite{sam} pre-trained on a large-scale datasets has emerged as a benchmark for interactive segmentation. 
Considering that SAM cannot perform semantic prediction, many works~\cite{Semantic-SAM,SEEM} utilize category labels to further fine-tune SAM and enable it to achieve the semantic segmentation task. For example, SEEM~\cite{SEEM} further trains SAM using labeled segmentation data through a bipartite matching constraint, thus providing the model with the ability to predict semantics. Semantic-SAM~\cite{Semantic-SAM} proposes a multi-choice learning scheme via multi-task training across diverse datasets, enabling the model to segment at various granularities while predicting semantic labels.
Other works~\cite{ma2024segment,xu2024embodiedsam,Gaussian-Grouping} apply SAM to diverse specific domains to solve application tasks. MedSAM~\cite{ma2024segment} fine-tunes SAM by collecting a large-scale dataset of medical image samples in multiple imaging modalities and types of cancer, achieving superior results in disease segmentation.
Gaussian-Grouping~\cite{Gaussian-Grouping} leverages the SAM as supervision model to jointly train a reconstruction and segmentation of objects in open-world 3D scenes for high-quality 3D edit.
% For instance, SAM functions as a robust pretrained encoder-decoder model that can be fine-tuned for specific tasks~\cite{ravi2024sam,yang2024samurai}. Some other approaches maintain the interactive nature of segmentation by focusing on parameter-efficient fine-tuning techniques~\cite{xu2024embodiedsam}. 
% To achieve robust segmentation across diverse scenarios, some methods fine-tune SAM’s decoder using large-scale domain datasets~\cite{houlsby2019parameter,ma2024segment}. 
% Although these approaches show notable advantages across various benchmarks, they have yet to be consistently evaluated against similar methods, and a comprehensive analysis of how different interaction strategies influence outcomes is still lacking.


%-------------------------------------------------------------------------
% %%%%%%%%%%%% figure 3
\begin{figure*}
\begin{center}
%\setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{-0.1cm}
\includegraphics[width=0.98\linewidth]{pic/architecture.png}
\caption{
An overview of the proposed framework SAM2Refiner. It contains a localization augment module to balance the detailed and semantic representations, a prompt retargeting for enhancing response of input prompts and a mask refinement structure to boost the quality of mask outcomes. The blue line denotes the SAM2 pipeline, and the black line denotes our SAM2Refiner pipeline.
}
\label{fig:network}
\end{center}
\end{figure*}
%-------------------------------------------------------------------------



\subsection{High-Quality Segmentation}
High-quality segmentation as a fundamental computer vision task focuses on accurately segmenting various complex and detail-rich objects, which ranges from many sub-tasks including semantic segmentation~\cite{long2015fully,zhao2017pyramid}, instance segmentation~\cite{he2017mask,dai2016instance} and panoptic segmentation~\cite{kirillov2019panoptic}. 
% Recent researches like SegGPT~\cite{SegGPT} enable diverse segmentation tasks through in-context visual learning.
% Existing methods for high-quality segmentation are tailored to specific tasks, such as image and video semantic segmentation~\cite{lin2017refinenet,zhao2018icnet,takikawa2019gated,yuan2020segfix}, instance segmentation~\cite{transfiner,vmt,tang2021look} or panoptic segmentation~\cite{de2023intra}. 
Traditional methods~\cite{Panoptic-deeplab,deeplab-v3,hrnet,medseg1,cascadepsp} on high-quality segmentation dedicate to designing elaborate structures upon on CNN-based networks in order to capture both low-level local patterns and semantic global features through various reception fields towards holistic representation.
Deeplab-v3~\cite{deeplab-v3} designs a set of multi-level convolution kernels to capture features with diverse reception fields and HRNet~\cite{hrnet} employs cascaded structures to fuse features from different layers to learn both low-level and high-level information.
% More recently, many approaches focus on post-segmentation refinement using graphical models like CRF~\cite{krahenbuhl2011efficient} and other utilize separate deep networks for iterative improvements~\cite{cascadepsp,shen2022high}.
More recently, many approaches~\cite{swin-unet,SegFormer,SegGPT,maskformer,mask2former} built on the transformer backbone focus on introducing local patterns into the model by narrow the reception field of self-attention module. For example, Mask2Former~\cite{mask2former} presents an efficient versatile architecture by using masked attention to extract local features for high-quality image segmentation tasks. Swin-Unet~\cite{swin-unet} adopts the Swin-Transformer network to the UNet like structures to adaptively capture object features with various scales via hierarchical attention.
Latest, the Pi-SAM~\cite{liu2024segment} designs a mask decoder to expands SAM’s ability at high resolution images, however, it may diminish the response of the input prompt and exhibit biased preferences for background clicks. 
% In contrast, our work emphasizes accurately segmenting diverse objects in new data using flexible prompting. We propose HQ-SAM, a high-quality zero-shot segmentation model that generalizes across various tasks and domains. Unlike post-segmentation refinement approaches, HQ-SAM directly predicts high-quality masks by reusing the image encoder and mask decoder from SAM, rather than relying on a coarse mask as input for a separate refinement network. Our model architecture builds upon SAM with minimal overhead, incorporating efficient token learning for accurate mask predictions. We demonstrate its effectiveness through extensive zero-shot experiments.