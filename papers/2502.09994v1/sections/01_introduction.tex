\section{Introduction}

% Operations Research (OR) is vital for decision-making in many industrial fiels, including logistics and supply chain management, financial and investment decision-making, transportation and traffic planning.
Operations Research (OR) has a long history of optimizing complex decision-making processes, such as in logistics, finance, investment, transportation, and healthcare, etc., where even small improvements can lead to significant operational profits. As these optimization algorithms increasingly contribute to daily life, it is essential to ensure their trustworthiness and reliability through explanations, which build user confidence \citep{faulhaber2021effect}. Governments are also responding to this need by enacting laws like the General Data Protection Regulation (GDPR) of the European Union \citep{goodman2017european}, emphasize the ``right to explanation'' for algorithmic decisions \citep{selbst2018meaningful} in automated systems.

In recent years, Large Language Models (LLMs) have emerged as powerful tools in the OR domain, offering new opportunities to automate and enhance the modeling process. Current research of LLMs in OR, such as works \citep{xiao2023chain, ahmaditeshnizi2024optimus, tang2024orlm, zhang2024solving}, explore the potential to streamline the formulation and solutions of complex OR problems. However, LLMs in OR have primarily focused on improving efficiency and accuracy by generating codes for external solvers to obtain OR solutions, with less attention to enhancing solution explainability, especially in real-time collaborative automated systems.
% The challenge remains to integrate LLMs not just as optimization tools but as vehicles for generating understandable, context-aware explanations that make OR models accessible to a broader audience. For instance, in actual development, where stakeholders from diverse backgrounds need to make informed decisions based on OR-driven outcomes, the lack of transparent explanations can hinder trust and effective collaboration.

Meanwhile, several studies \citep{vcyras2019argumentation,li2023large,erwig2024explanations,de2024explainable} have explored explainable optimization related to OR, but there are still limitations. For example, \citep{erwig2024explanations} focuses specifically on combinatorial optimization problems, which limits its applicability across the broader OR landscape and fails to leverage the advantages of LLMs in real-time modeling and explanations. Another work, OptiGuide \citep{li2023large}, emphasizes what-if analysis, which, while useful for specific easy scenarios, lacks the robust modeling capability to address more complex cases like deleting or combining constraints. For example, if a warehouse closes, the OR model must remove the related storage capacity constraint and adjust the distribution network. Current methods struggle to achieve this level of flexibility, yet such adaptability is crucial for accurately reflecting real-world changes.
% Without this, models cannot effectively support decision-making in dynamic environments.
Most critically, the explanations these methods provide are often superficial, merely summarizing the outcomes without exploring the underlying reasons behind the results, thus lacking the quantitative analysis, depth, and clarity required to fully understand and trust the decision-making process.

% This highlights the need for more robust, versatile, and comprehensive methods for explainable OR, especially in scenarios involving complex and dynamic operations.

Given the limitations of existing approaches to explainable optimization, we are motivated to develop a more comprehensive framework, EOR, for explaining OR models.
As shown in Figure~\ref{fig:introduction}, our framework addresses the critical need for transparency in OR by shifting from purely modeling a problem (a natural language description) to providing clear, actionable explanations for a user query, such as ``\textit{What if transportation costs increase by 15\%?}''.
% As illustrated in Figure~\ref{fig:introduction}, our framework addresses the critical need for transparency in OR by shifting the focus from purely modeling for one problem (a natural language description text) to delivering clear, actionable explanations for a new user query (a question about data or a model used to make decisions, e.g., ``\textit{What if transportation costs increase by 15\%?}'').
First, we formulate the problem of explainable OR within the context of LLMs. This formulation is essential for laying a foundation for future research in this emerging area.
% , ensuring that the explanations generated by OR models are not only accurate but also meaningful to stakeholders across various domains.
Second, our framework emphasizes two critical types of explanations, 1) \textit{Explanation of correctness:} the reasons for code updates during the modeling process, and 2) \textit{Explanation of the Results:} the rationale for generating specific solutions.
% The first explanation ensures the correctness of the new solution, while the second guarantees that the explanation is meaningful.
Unlike traditional methods that provide superficial explanations and analyses, our approach incorporates more sophisticated what-if analysis, quantifying the effects of changes prompted by user queries and providing deeper insights into the decision-making process.
A significant component of our contribution is the introduction of the concept of ``Decision Information''. By leveraging bipartite graphs, we measure the importance of different decision factors in response to user queries, particularly when complex changes in constraints arise. This approach enhances both the modeling capabilities and the explanatory power of OR models within LLMs. Our dual focus ensures that the framework not only yields accurate optimal solutions but also effectively communicates the underlying rationale for these solutions. Finally, recognizing the need for standardized evaluation of explainable OR methods, we design a new industrial benchmark specifically tailored to assess the effectiveness of explanations in OR. This benchmark fills the gap of current approaches and sets a new standard for evaluating the transparency and comprehensibility of OR models.

\begin{figure}[ht]
\begin{center}
%\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\includegraphics[width=0.8\linewidth]{figures/figure1.pdf}
\end{center}
\caption{The framework of EOR.}
\label{fig:introduction}
% \vspace{-10pt}
\end{figure}

\textbf{Contributions.} 1) We formulate the problem of the explainable OR problem within the context of LLMs, laying a foundation for future research in this area. 2) We introduce the concept of ``Decision Information'' and utilize bipartite graphs in conjunction with LLMs to quantify its importance in response to user queries, enhancing both the modeling capabilities and the explanation of complex what-if analysis within OR. 3) We develop a new benchmark specifically designed to evaluate the effectiveness of explanations in OR, setting a new standard for explainability in the field.
