\section{Methodology}

\subsection{Problem Formulation}

Given an OR problem $\mathring{p}$, along with a user query $\mathring{q}$ related to the problem, our goal is to utilize an LLM to generate comprehensive explanations of solutions for the queries in real time. The LLM will provide two types of explanations: 1) \textit{Attribution Explanation}, which outlines the general attributes and structure of the problem, and 2) \textit{Justification Explanation}, which elucidates the correctness and derivation of the solutions. The mathematical formulation is as follows:

\textbf{Input:} The origin problem description $\mathring{p} = \langle \mathring{d}, \mathring{o}, \mathring{c}\rangle$, and a user query $\mathring{q} = \langle \mathring{o}', \mathring{c}' \rangle$. The updated problem description, incorporating the user query, is denoted as $\mathring{p}' = \langle \mathring{d}, \mathring{o}', \mathring{c}'\rangle$. Here, $\mathring{d}$ represents the set of decision variables, $\mathring{o}$ and $\mathring{o}'$ are the objective functions to be maximized or minimized, $\mathring{c}$ and $\mathring{c}'$ denote the original and modified constraints in $\mathring{p}$ and $\mathring{p}'$, respectively, that the decision variables must satisfy. In our setting, we assume the decision variables remain unchanged.
% $h$ represents the set of all parameters in $o$ and $d$, and $h'$ represents the set of all parameters in $o'$ and $d$ associated with the user query $q$.

\textbf{Output:} We denote the problem solutions for $\mathring{p}$ and $\mathring{p}'$ as $\mathring{s}$ and $\mathring{s}'$, respectively. The output comprises two types of explanations, 1) \textit{Attribution Explanation}: A detailed description of the elements $\mathring{d}$, $\mathring{o}$, $\mathring{o}'$, $\mathring{c}$, $\mathring{c}'$, $\mathring{s}$, and $\mathring{s}'$ within the context of the problem. 2) \textit{Justification Explanation}: A rationale for the correctness of $\mathring{s}$ and $\mathring{s}'$, and clarifies how $\mathring{s}'$ is derived from $\mathring{s}$.

\subsection{The EOR Framework}
Our proposed framework, EOR, is an end-to-end solution designed to enhance OR model transparency using LLMs. Unlike current methods that provide limited and shallow explanations, primarily the form of attribution explanations, our framework emphasizes delivering clear, actionable insights for diverse stakeholders. As illustrated in Figure \ref{fig:introduction}, we focus on the justification explanations. We will offer two critical explanations: 1) justifications for code updates during the modeling process, and 2) the rationale behind specific solutions. By doing so, we strive to make OR solutions more accessible, understandable, and applicable to a broader audience, thereby improving decision-making quality and fostering user trust.
% By introducing the concept of ``Decision Information'', which uses bipartite graphs and LLMs to quantify decision impacts, our framework allows for real-time, interactive explanations that support rapid and informed decision-making. This approach is essential for addressing the growing need for transparency and trust in complex OR applications.
% This framework aims to enhance the transparency and interpretability of OR solutions by leveraging the explanatory capabilities of LLMs in real time, thereby benefiting a broad range of stakeholders, including algorithm designers, programmers, and end-users. The interactive structure ensures that the explanations are dynamically generated based on real-time user queries, facilitating efficient and rapid decision-making.

\begin{figure}[ht]
\begin{center}
%\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \includegraphics[width=0.85\linewidth]{figures/framework.pdf}
\includegraphics[width=0.8\linewidth]{figures/figure2.pdf}
\end{center}
\caption{The overall workflow of EOR.}
\label{fig:framework}
\vspace{-10pt}
\end{figure}

\subsubsection{The workflow of EOR}
% as shown in Figure, our EOR contains three agents: Commander, Writer, and Safeguard. -> Introduce these three agents' roles and functions -> 1. End-user has new query to Commander -> 2. Commander send to writer -> writer analysis this query (add or delete or update) to update the code -> 3. writer send code to safegarde -> safeguard jusge the safety of the code -> if safe, adopt external solver to get the final results -> if not safe, debugging,  re do coding -> 4. In this process, we will save the logs. -> 5. When we get the final answer, we will get the explanations form LLMs.

As shown in Figure~\ref{fig:framework}, EOR framework comprises three key agents: Commander, Writer, and Safeguard, each serving a distinct role to ensure an efficient, accurate, and secure optimization process.

\textbf{Commander Agent:} The Commander acts as the central hub or ``data bus'' in the system, responsible for receiving user queries and managing data flow between the agents. When an end-user submits a new user query, the Commander first interprets the query's context, identifies the intent, and then forwards the message to the appropriate agents.

\textbf{Writer Agent:} Upon receiving the processed query from the Commander, the Writer initially assumes the role of analyzing and modifying the code. Based on the query's requirements, the Writer determines whether to add, delete, or update specific constraints and parameters. By leveraging LLMs, the Writer guarantees that the generated code accurately reflects the intended changes. Subsequently, the updated code is sent to the Safeguard for verification. Once the Safeguard provides a SAFE confirmation, the Writer transitions into an interpreter role, generating detailed explanations for the modifications and the rationale behind the decision-making process.

\textbf{Safeguard Agent:} The Safeguard is responsible for ensuring the safety and correctness of the generated code. It conducts thorough checks to verify whether the code adheres to predefined safety standards and is free from logical or syntactical errors that could compromise the optimization process. If the code passes these safety checks, the Safeguard approves it for execution; otherwise, it triggers a debugging process where the Writer regenerates and corrects the code as necessary.

\textbf{The overall workflow:} The EOR workflow starts when a user submits a query to the Commander (1), who relays it to the Writer with a code prompt (2). The Writer analyzes the query, determining whether to add, delete, or update code, and returns the updated code to the Commander (3). This step ensures that the generated code aligns with the updated problem requirements. The Commander then sends the code to the Safeguard for verification (4). If the Safeguard determines the code is safe (5), the process moves to (6), where the Commander sends an interpreter prompt to the Writer. If the code is deemed dangerous, the process loops back to (2), with the Commander sending a debug code prompt to the Writer. Once the code is safe, the Writer generates answers about explanations for the modifications and results (7) and sends them to the Commander. Finally, the Commander sends these explanations to the user (8). This iterative process, which repeats until a satisfactory answer or timeout, ensures robust, explainable solutions tailored to the user's query. The detailed design of the prompt template is presented in Appendix \ref{appendix:prompt}.

\subsubsection{Justification Explanation Generation}

In our framework, explanations are generated to ensure transparency and trustworthiness in the decision-making process. These explanations are divided into two main categories:

\textbf{Explanation of Correctness:} This type of explanation serves to validate the code modifications introduced by the Writer, offering a detailed rationale for the changes made. It clarifies the necessity of these modifications in addressing the problem's requirements and ensures that they adhere to safety standards and logical constraints. Through this process, the reliability of the generated code is substantiated, thereby enhancing the accuracy of the model prior to execution.

\textbf{Explanation of the Results:} Once the code is executed and the results are obtained, this type of explanation focuses on interpreting the outcome. It breaks down the results into understandable terms, illustrating the impact of the code changes on the final solution. This explanation connects the modifications to their direct effects, providing users with a clear understanding of how the new solution addresses their initial query and any resulting trade-offs or benefits.

Before formalizing the concept of ``Decision Information'', consider the following user query from an OR problem in flight operations: \textit{How should the aircraft configuration be adjusted if the company limits Type A aircraft to 15 and Type B aircraft to 30?} In this context, ``Decision Information'' refers to the new constraints, limiting the number of Type A and Type B aircraft to 15 and 30, respectively, which directly modify the optimization model. These changes reshape the solution space, requiring adjustments to meet demand within the newly imposed limits. Thus, ``Decision Information'' captures the key query elements that drive changes in the problem.
\begin{definition}
\textbf{Decision Information:} Decision Information encompasses the parameters and constraints specified in a user’s query for an OR problem, capturing the essential details needed to reconfigure the problem according to the user’s intent.
\end{definition}

Following this definition, we turn to a quantitative evaluation of ``Decision Information'' to assess its impact on decision-making processes. However, existing methods lack a measure for assessing changes in decision information caused by constraint modifications, focusing only on sensitivity analysis in parameter changes. Inspired by \citep{xing2024towards}, we convert both the updated and original programs into a standardized format and then calculate their differences to determine the importance of information changes. This process can be outlined in three steps:

\textbf{Conversion to Linear Programs (LPs) in General Form:} Both the updated code (resulting from user queries) and the original code are first parsed and translated into a standardized LP format, that is widely adopted by various LP solvers, including CPLEX \citep{cplex2009v12}, Gurobi \citep{bixby2007gurobi}, COPT \citep{copt} an so on. This conversion includes expressing all decision variables, objective functions, and constraints uniformly to allow a direct comparison. The general LP form captures the essence of both the initial and modified decision scenarios, providing a clear basis for analyzing how changes in input data or constraints affect the outcome.

Formally, an LP with $n$ variables and $m$ constraints can be represented as:
\begin{align}
\min_{\vx \in \R^n} & \quad \vc^\top \vx
\nonumber\\
\text{s.t.} & \quad \vl^s \leq \mA\vx \leq \vu^s \nonumber\\
&\quad \vl^x \leq \vx \leq \vu^x, \nonumber
\end{align}
where $\mA \in \R^{m\times n}$ is the constraint matrix, $\vc \in \R^n$ is the cost vector, $\vx \in \R^n$ is the decision variables, $\vl^x \in \R^n$ and $\vu^x \in \R^n$ are lower/upper bound of the decision variables $\vx$, and $\vl^s \in \R^n$ and $\vu^s \in \R^n$ are lower/upper bound of the constraint.

\textbf{Graph Representation Conversion:} The standardized LPs can be transformed into bipartite graphs, which consist of two distinct sets of nodes: decision variables and constraints \citep{gasse2019exact,fan2023smart,xing2024towards}. In this representation, edges between nodes signify dependencies or relationships, with edge weights indicating the strength or nature of these connections. We define the bipartite graph as $\gG = (S \cup X, E)$, where $S = \{s_i|i\in [m]\}$ represents the set of constraint nodes, $X = \{x_j|j\in [n]\}$ represents the set of decision variable nodes, and $E = \{e_{i,j}|i\in[m], j\in[n]\}$ represents the edges between them. Here, $[\cdot]$ denotes a set of consecutive numbers. The attribute of a constraint vertex $s_i$ is expressed as $\text{attr}(s_i)=[l_i^s, u_i^s]\top$, indicating its lower and upper bounds. Similarly, the attribute of a decision variable vertex $x_j$ is given by $\text{attr}(x_j)=[l_j^x, u_j^x, c_j]\top$, which includes its lower bound $l_j^x$, upper bound $u_j^x$, and objective coefficient $c_j$. This graph-based approach facilitates a structural analysis of the decision-making framework, allowing us to visualize the interactions among variables under different conditions and to compare updated and original programs on a structural level.

\textbf{Graph Edit Distance (GED) Calculation:} To quantify the impact of changes in ``Decision Information'', we compute the GED between the two bipartite graphs derived from the updated and original LPs. GED represents the minimum cost necessary to transform one graph into another through a sequence of operations, such as inserting, deleting, or substituting vertices and edges. This metric effectively captures the types of modifications made to the code in response to a query. This metric quantifies the minimal number of modifications (such as adding, deleting, or substituting nodes or edges) required to transform one graph into the other. A smaller edit distance indicates fewer changes, suggesting the updated program closely aligns with the original decision-making context. Conversely, a larger edit distance highlights significant alterations, signaling a substantial impact of the updated information on the decision-making process.

There are many well-established GED algorithms \citep{gao2010survey,stauffer2017survey,abu2015exact,xing2024towards}, we follow a straightforward principle provided by \citep{xing2024towards}: each operation on an attribute in graph incurs a unit cost of 1. Formally, given the graph of the original program $\gG^p = (S^p \cup X^p, E^p)$ and updated program $\gG^{p'} = (S^{p'} \cup X^{p'}, E^{p'})$, we define the vertex cost matrix as follows,
\begin{table}[h]
\vspace{-1pt}
    \begin{center}
    \begin{tabular}{l|lll}
         & $s_{i'}^p \in S^p$ & $x_{i'}^p \in X^p$ &
         $\epsilon$ \\
         \midrule
         $s_i^{p'} \in S^{p'}$ & \text{\#msm}$(s_i^{p'}, s_{i'}^p)$ & $\infty$ & \text{\#attr}$(s_i^{p'})$ \\
         % \midrule
         $x_i^{p'} \in X^{p'}$ & $\infty$ & \text{\#msm}$(x_i^{p'}, x_{i'}^p)$ & \text{\#attr}$(s_i^{p'})$ \\
         % \midrule
         $\epsilon$ & \text{\#attr}$(s_{i'}^{p})$ & \text{\#attr}$(s_{i'}^{p})$ & $\infty$ \\
    \end{tabular}
    \end{center}
    \vspace{-8pt}
\end{table}\\
In this context, all operations are treated as matching processes; for example, deleting a vertex is conceptualized as matching the vertex to an empty vertex, denoted by $\epsilon$. Here, $\text{\#attr}(\cdot)$ denotes the total number of vertex attributes, while $\text{\#msm}(\cdot)$ represents the count of mismatched attributes between two vertices. Similarly, the edge cost matrix is defined as follows,
\begin{table}[h]
    \begin{center}
    \begin{tabular}{l|ll}
         & $e_{i,j}^p \in E^p$ & $\epsilon$ \\
         \midrule
         $e_{i,j}^{p'} \in E^{p'}$ & \text{\#msm}$(e_{i,j}^{p'}, e_{i,j}^p)$ & \text{\#attr}$(e_{i,j}^{p'})$ \\
         % \midrule
         $\epsilon$ & \text{\#attr}$(e_{i,j}^p)$ & $\infty$ \\
    \end{tabular}
    \end{center}
    \vspace{-8pt}
\end{table}\\
Given the varying scales of changes in parameters and constraints, we further normalize $\text{GED}(\gG^{p'}, \gG^p)$ by the graph size. The normalized GED (NGED) is calculated as: $\text{NGED}(\gG^{p'}, \gG^p)=\frac{\text{GED}(\gG^{p'}, \gG^p)}{\max(|\gG^{p'}|, |\gG^{p}|)}$, where $|\gG|$ is defined as the sum of the number of attributes for all edges and vertices in the graph, specifically: $|\gG| = \sum_{e \in E}\text{\#attr}(e) + \sum_{v \in S \cup X}\text{\#attr}(v)$. This normalization accounts for graph size variations, allowing for a more consistent comparison of the impact of changes across different scenarios. By quantifying discrepancies between the updated and original graphs through an analysis of their attribute changes, our approach offers a more precise measurement of changes in the decision-making framework.
% Further details can be found in the Appendix.

% This formulation allows us to quantify discrepancies between the updated and original graphs by analyzing the differences in their attributes, providing a nuanced measurement of changes in the decision-making framework.
By employing these three steps, LP conversion, graph representation, and graph edit distance calculation, we provide a rigorous and systematic approach to quantifying ``Decision Information''. Since LLMs cannot directly perform this quantification, we utilize them to sense these processes and generate explanatory insights. This approach fills the gap in previous methods that could not quantitatively analyze the impact of changes in constraints. By offering a unified methodology for measuring changes in constraints and parameters, it enables a precise evaluation of information changes. This framework provides a measurable and comparable basis for assessing the impact of these changes on decision-making. It thus anchors the concept of ``Decision Information'' in both practical and thEORetical contexts, offering more profound insight into how updates shape decisions.
% This methodology allows us to evaluate the relevance of information changes with precision, providing a measurable and comparable framework to understand their impact on decision-making. The approach ensures that the concept of ``Decision Information'' is grounded in both practical and thEORetical terms, offering deeper insights into how updates influence decisions.
