\section{Experiments}

\subsection{Evaluation Benchmark}
Despite the availability of numerous open-source datasets in OR, such as NL4OPT \citep{ramamonjison2023nl4opt}, ComplexOR \citep{xiao2023chain}, NLP4LP \citep{holzer2024grid}, and IndustryOR \citep{tang2024orlm}, these datasets are limited to problem descriptions and are primarily suited for OR modeling needs. There remains a significant gap in datasets specifically tailored for explainable OR, which are crucial for advancing transparency and interpretability in decision-making processes.

To address this issue, we developed a novel benchmark dataset based on the open-source commercial IndustryOR, specifically designed to evaluate explainability in OR tasks. The benchmark includes 30 categorized problems across various domains (e.g., supply chain management, financial investment, logistics management, etc.). Each problem is paired with 10 unique queries that involve diverse or combined modifications to parameters or constraints (e.g., deleting, adding, or updating constraints and parameters). These queries were crafted by OR experts with significant industry experience to ensure both diversity and practical relevance. The datasetâ€™s quality and comprehensiveness are validated through iterative expert reviews and comparisons with real-world cases.
% ensuring broad query scenarios and practical relevance.
% to ensure both diversity and real-world relevance

For every problem, we provide corresponding Python code and employ the Gurobi optimization solver \citep{bixby2007gurobi} to determine optimal solutions. Additionally, we also include the ground truth labels for each query for each problem to ensure accurate evaluation. Notably, the question sets and the queries in this benchmark are developed from scratch and managed in-house, guaranteeing that they have not been part of LLM training data, enhancing the robustness of the benchmark for assessing model performance. We provide a complete example in Appendix \ref{benchmark:example}.
% Additional details are provided in the Appendix.

\subsection{Evaluation Methodology}
\label{sec:evaluation}
Our evaluation focuses on two aspects: \textbf{Modeling Accuracy} and \textbf{Explanation Quality}. For the modeling accuracy assessment, we recognize that different code implementations can produce the same optimization results. For instance, two programs solving a linear programming problem may employ different formulations or techniques (e.g., distinct constraint orderings or variable namings), but both can still yield identical optimal solutions. Therefore, rather than directly comparing the generated code to a reference, we evaluate accuracy by comparing the optimization outcomes to ensure correctness and consistency across implementations.

Regarding the explanation quality, although our task is highly specialized and requires expert-level interpretability in OR, we aim to develop an automated evaluation method. Drawing inspiration from \citep{kondapaneni2024less} and utilizing the capabilities of LLMs for text quality evaluation \citep{chen2023exploring,chiang2023closer,hu2024llm,chu2024better,zytek2024llms}, we establish expert-crafted templates and use LLMs to assess explanation quality, aiming for a human-level standard. However, this evaluation approach has certain limitations. To address this, we propose two methods in this paper. First, we establish a structured template that specifies key criteria for effective explanations, such as clarity, relevance, and logical coherence. For instance, explanations should explicitly describe the rationale for modifying specific parameters or constraints and explain how these changes affect the optimization results. Second, we conduct a blind review process where OR experts anonymously score the explanations generated by different methods. This approach helps minimize bias, providing a reliable measure of how effectively the explanations convey meaningful insights to users. We will assess whether the proposed automated evaluation method aligns with human evaluation. This dual approach enables us to assess the consistency between the proposed automated method and human evaluation.

\subsection{Baselines}
We employ two baselines to ensure a comprehensive evaluation: Standard and OptiGuide \citep{li2023large}. The Standard involves a proprietary LLM (e.g., GPT-4, GPT-4-Turbo, etc.) that generates updated programs and explanations, serving as a basic comparison point for assessing overall performance.
% The Chain-of-Thought baseline decomposes complex reasoning tasks into intermediate steps, promoting a structured, sequential approach to problem-solving.
The OptiGuide represents a specialized method used in supply chain optimization, providing a domain-specific comparison that evaluates the adaptability and effectiveness of LLMs in industry-relevant scenarios.

\subsection{Model Setup}
% We test these baselines for our experiments using two LLM models, GPT-4 and GPT-3, under both zero-shot and one-shot learning settings.
For our experiments, we evaluate the performance of the proposed baselines using four LLMs, GPT-4 \citep{achiam2023gpt}, GPT-4-1106-preview, GPT-4-0125-preview, and GPT-4-Turbo, under both zero-shot and one-shot learning settings. The zero-shot setting requires the models to generate updated programs and explanations without any prior examples, testing their inherent understanding and generalization capabilities. The one-shot setting provides a single example to guide the models, allowing us to assess the impact of minimal contextual information on their ability to perform the tasks accurately and coherently.
% This dual setup ensures a comprehensive evaluation of the models across different levels of exposure to the problem context.
To ensure fairness and reproducibility, we fix the hyperparameter \textit{temperature} at 0 and apply the same examples for all models in the one-shot setting, and all models are implemented under the framework AutoGen \citep{wu2024autogen}. This paper focuses on fully automating all processes, excluding user involvement. However, as our implementation is built on the AutoGen, it inherently supports seamless integration of user feedback. A detailed hyperparameter sensitivity analysis is provided in the Appendix~\ref{appendix:sensitivity}. The source code is available at \url{https://github.com/Forrest-Stone/EOR}.

\subsection{Quantitative Performance}

\subsubsection{Comparison of modeling accuracy}
Table \ref{tab:results} shows the accuracy results across different models. We have the following observations. In the zero-shot setting, EOR consistently outperforms both Standard and OptiGuide across all LLM versions. These results emphasize the robustness of the EOR in zero-shot tasks, which substantially improves performance compared to other methods. For instance, GPT-4-Turbo achieves 88.33\% accuracy with EOR, while Standard and OptiGuide methods yield only 63.00\% and 30.33\%, respectively. Moreover, for all LLM models except GPT-4, Standard outperforms OptiGuide, indicating that the LLM's modeling capabilities are quite strong.

In the one-shot setting, EOR continues to outperform Standard and OptiGuide in all LLM versions, achieving an average of 90.00\% accuracy and even reaching 95.33\% accuracy on the GPT-4-Turbo, the highest among all results. Additionally, we find that providing an example can significantly improve modeling accuracy, and nearly all methods perform better in the one-shot setting than in the zero-shot setting. Specifically, the accuracy of OptiGuide on GPT-4 improves from 30.33\% to 75.00\%. However, OptiGuide still produces the worst result except on GPT-4.

Overall, EOR consistently outperforms other methods in both zero-shot and one-shot settings, particularly with models like GPT-4-Turbo and GPT-4-0125-preview. The accuracy gains observed when transitioning from zero-shot to one-shot highlight the importance of using examples in improving model performance. In terms of model comparisons, GPT-4-Turbo demonstrates the highest adaptability across both settings, achieving the best overall accuracy. While OptiGuide provides modest improvements, it does not match the performance of Standard and EOR. These results underscore the value of carefully selecting both models and example strategies to maximize accuracy, with EOR emerging as the most effective for high-accuracy tasks.
\begin{table}[t]
\caption{Accuracy across different models under different LLMs with zero/one-shot setting. The bold scores are the best in each row.}
\label{tab:results}
\vspace{-8pt}
\begin{center}
\begin{tabular}{l|l|c|c|c}
\toprule
% \multicolumn{1}{c|}{Setting} & \multicolumn{1}{c|}{Model} & Standard $\uparrow$ & OptiGuide $\uparrow$ & EOR $\uparrow$ \\
\multicolumn{1}{c|}{Setting} & \multicolumn{1}{c|}{Model} & Standard & OptiGuide & EOR \\
\midrule
\multirow{4}{*}{Zero-shot}
    & GPT-4 & 18.67\% & 30.33\% & \textbf{75.67\%} \\
    & GPT-4-1106-preview & 75.33\% & 36.00\% & \textbf{81.67\%} \\
    & GPT-4-0125-preview & 68.33\% & 47.33\% & \textbf{87.33\%} \\
    & GPT-4-Turbo & 63.00\%& 30.33\% & \textbf{88.33\%} \\
    % & GPT-4-Turbo-2024-04-09 & 58.33\% & 30.33\% & 89.00\% \\
    % & EOR &  \\
\midrule
\multirow{4}{*}{One-shot}
    & GPT-4 & 71.67\% & 75.00\% & \textbf{90.33\%} \\
    & GPT-4-1106-preview & 69.67\% & 55.33\% & \textbf{87.67\%} \\
    & GPT-4-0125-preview & 76.00\% & 69.33\% & \textbf{92.00\%} \\
    & GPT-4-Turbo & 88.00\% & 69.33\% & \textbf{95.33\%} \\
    % & GPT-4-Turbo-2024-04-09 & 87.67\% & 68.67\% & 92.67\% \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-10pt}
\end{table}
\begin{table*}[ht]
\caption{Quality scores (0-10) across different models with zero/one-shot setting.}
\label{tab:explanations}
\vspace{-8pt}
\begin{center}
\begin{tabular}{ll|cc|cc|cc}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c|}{EC $\uparrow$} & \multicolumn{2}{c|}{ER $\uparrow$} & \multicolumn{2}{c}{Overall $\uparrow$} \\
\cmidrule{3-8}
\multicolumn{2}{c}{} & Auto & \cellcolor{gray!20} Expert & Auto & \cellcolor{gray!20} Expert & Auto & \cellcolor{gray!20} Expert \\
\midrule
\multicolumn{1}{l|}{\multirow{3}{*}{Zero-shot}} & Standard & 0.12 & \cellcolor{gray!20} - & 6.98 & \cellcolor{gray!20} 6.98 & 5.36 & \cellcolor{gray!20} 5.63  \\
\multicolumn{1}{c|}{} & OptiGuide & 0.69 & \cellcolor{gray!20} - & 7.93 & \cellcolor{gray!20} 7.90 & 6.31 & \cellcolor{gray!20} 6.59  \\
\multicolumn{1}{c|}{} & EOR & 9.76 & \cellcolor{gray!20} 9.86 & 9.41 & \cellcolor{gray!20} 9.47 & 9.47 & \cellcolor{gray!20} 9.47 \\
\midrule
\multicolumn{1}{l|}{\multirow{3}{*}{One-shot}} & Standard & 0.34 & \cellcolor{gray!20} - & 7.10 & \cellcolor{gray!20} 6.90 & 5.54 & \cellcolor{gray!20} 5.37  \\
\multicolumn{1}{c|}{} & OptiGuide & 0.20 & \cellcolor{gray!20} - & 7.60 & \cellcolor{gray!20} 7.61 & 5.96 & \cellcolor{gray!20} 6.05  \\
\multicolumn{1}{c|}{} & EOR & 9.61 & \cellcolor{gray!20} 9.72 & 9.30 & \cellcolor{gray!20} 9.35 & 9.33 & \cellcolor{gray!20} 9.35  \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}
\subsubsection{Comparison of the quality of explanations}
% \subsection{Ablation Study}
As discussed in Sec. \ref{sec:evaluation}, a critical part of our evaluation is assessing the quality of the explanations generated by the models. It is important to note that explanations based on incorrect results are irrelevant, as explaining failure cases offers no meaningful insights. Therefore, we first filter out all incorrect modeling cases, ensuring that only correct outputs are evaluated. We adopt two evaluation methods: an automated evaluation (Auto) and expert evaluation (Expert), both introduced in Sec. \ref{sec:evaluation}. These evaluations focus on two main aspects: Explanation of Correctness (EC) and Explanation of Results (ER). EC assesses the clarity and correctness of the explanation regarding the model's output, while ER evaluates how well the explanation conveys the reasoning behind the optimization results. Additionally, we also measure the overall explanation quality (Overall). The explanations are scored on a 0-10 scale, with 0 indicating poor quality and 10 indicating excellent quality. The prompt template for evaluating the explanation quality is provided in Appendix \ref{appendix:evaluation_explanations}. The results of these evaluations using GPT-4o are summarized in Table \ref{tab:explanations}.


\begin{figure*}[t] \small
\begin{AIbox}{A Case Study of EOR.}
{\bf Query:}
{\footnotesize
How should the aircraft configuration be adjusted to meet demand if the company decides to limit the number of Type A aircraft operated to no more than 15 and the number of Type B aircraft to no more than 30?
}
\tcbline
{\bf The ``code'' generated by EOR:}
{\footnotesize
\begin{lstlisting}[language=Python]
{
    "ADD CONSTRAINT": "# Constraint: Limit number of Type A and Type B aircraft\nm.addConstr(aircraft_count['A'] <= 15, name=\"MaxTypeAConstraint\")\nm.addConstr(aircraft_count['B'] <= 30, name=\"MaxTypeBConstraint\")"
}
\end{lstlisting}
}
\tcbline
{\bf The explanations generated by EOR:}
{\footnotesize
**Explanation of Updated Code:**\\
The updated code includes \sethlcolor{green}\hl{new constraints} that implement operational limits on the number of Type A and Type B aircraft the airline can use. Specifically:

\qquad- \hl{A constraint has been added to limit the number of Type A aircraft to 15.}

\qquad- \hl{A constraint has been added to limit the number of Type B aircraft to 30.}

\sethlcolor{yellow}\hl{These additional constraints are motivated by possibly new operational limitations or strategic decisions, such as maintenance availability or cost considerations.} By imposing these restrictions, the model must now find an optimal solution within a more constrained operational environment.\\

**Explanation of the Query on Results:**\\
The query specifically asked to adjust aircraft configurations within newly defined limits. The original model found an optimal solution at a minimum cost of \$200,000. However, with the added constraints, \sethlcolor{green}\hl{the solution space is reduced} - fewer configurations of aircraft are available to meet the passenger demand.

As a result of these constraints: \hl{The new optimal cost calculated by the model rose to \$215,000}. \sethlcolor{yellow}\hl{This increment indicates that the more stringent operational limits forced the use of a costlier configuration.}

\sethlcolor{green}\hl{The query's impact on the results is substantial, as illustrated by a \$15,000 increase in operational costs.} If rated on a scale from 1 to 10, \sethlcolor{yellow}\hl{the impact could be quantified around an 8.} This rating reflects significant changes in operational costs and constraints.

\sethlcolor{yellow}\hl{The numerical changes in the model, calculated as 100}, follows from a step-by-step process analyzing the modifications in the linear programming representation and their effects on the resulting computational graph. This method, grounded in operations research analysis, validates the substantial impact of the query-driven modifications.

In summary, \hl{the changes to the code (limiting aircraft types) constrained the model significantly, leading to a more expensive operational solution.} The rationale behind integrating these specific constraints could be associated with real-world operational limits, reflecting the complex decision-making processes airlines undergo to balance cost, capacity, and operational feasibility.
}
\end{AIbox}
\caption{An example illustrating the code and explanations generated by EOR.}
\vspace{-10pt}
\label{fig:explaiations_vs}
\end{figure*}

As shown in Table \ref{tab:explanations}, we have the following observations. Firstly, Auto is nearly as effective as Expert, with minimal differences in scores across all metrics, indicating that Auto is a reliable and valid approach. It is worth noting that Auto scores individual queries on EC, although it does not explicitly evaluate explanations. Secondly, EOR consistently outperforms both Standard and OptiGuide in explanation quality, achieving the highest scores in both zero-shot and one-shot settings. This demonstrates the superiority of our method in providing clearer and more accurate interpretations compared to other models. Finally, we find that Auto scores slightly higher on Standard than Expert, while for other models, the opposite is observed, with Auto scoring slightly lower than Expert. One possible reason is that LLMs may be biased toward the results they generate.

% \begin{figure}[ht]
% \centering
% \begin{subfigure}{0.48\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/zero-shot.pdf}
%     \caption{zero-shot: 35 failure problem}
% \end{subfigure}
% \hfill
% \begin{subfigure}{0.51\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/one-shot.pdf}
%     \caption{one-shot: 14 failure problem}
% \end{subfigure}
% \caption{The failures case of the modeling.}
% \label{fig:failures}
% \end{figure}
\subsection{Case Study}

In this section, we provide an example of the code and explanations generated by EOR, as illustrated in Figure \ref{fig:explaiations_vs}, where simple explanations are highlighted in green to represent attribute explanations, while deeper ones in yellow indicate justification explanations with quantitative analysis.
 % highlights simple explanations in green, representing the attribute explanation. Deeper explanations, shown in yellow, correspond to the Justification Explanation with quantitative analysis.

Results from other models are provided in Appendix \ref{appendix:case}. The EOR approach stands out by not only providing a clear explanation of the updated code but also offering detailed insights into how the query affects the outcomes. It explicitly highlights the newly added constraints that limit the number of Type A and Type B aircraft, demonstrating transparency in addressing the operational limits specified in the query. Unlike other methods that merely report result differences, EOR incorporates a quantitative analysis, explaining the \$15,000 increase in operational costs due to the restricted solution space caused by the newly added constraints. This step-by-step explanation not only justifies the modifications but also clarifies the broader impact of the constraints, making EOR more comprehensive and contextually aware than other models.

% \subsection{Parameter Sensitivity Analysis}
% \subsection{Failure Cases}
% To understand the strengths and weaknesses of EOR, we analyzed common failure cases, summarized in Table \ref{tab:failures}. These failures can be grouped into three categories: JSON format errors, where the LLMs fail to generate correct JSON outputs; correct execution, where code runs but produces incorrect results due to modeling logic errors or incomplete modeling; and runtime errors, such as variable name errors, syntax errors, or indentation errors. Table \ref{tab:failures} shows a 60.00\% reduction in total errors from zero-shot to one-shot, demonstrating a substantial improvement in the model's performance.
% No syntax errors in the one-shot results highlights the effectiveness of EOR. However, ongoing attention to modeling logic and runtime errors is still crucial for further improvement.
% % From Table \ref{tab:failures}, we observe a 60.6\% reduction in total issues from zero-shot to one-shot, indicating a significant improvement in the model's learning. The absence of syntax errors in the one-shot results highlights the effectiveness of this approach; however, ongoing attention to JSON format and logical errors remains essential for further enhancement.
% % We classify these failure cases into three main categories: JSON format errors, which occur when the LLMs fail to generate the correct JSON format; correct execution, where the code runs without issues but produces incorrect results; and runtime errors, where the generated code fails to run even after debugging attempts. Additionally, we further break down the error types within these categories. For runtime errors, we identify variable name errors, syntax errors, and indent errors. In the case of correct execution failures, issues arise from modeling logic errors and incomplete modeling.
% % The more details can be found in the Appendix \ref{appendx:failure}.

% \begin{table*}[ht]
% \caption{Failure cases on GPT-4-Turbo with zero/one-shot setting.}
% \label{tab:failures}
% \vspace{-8pt}
% \begin{center}
% \begin{adjustbox}{max width=\textwidth}
% \begin{tabular}{l|l|rr|rr}
% \toprule
% \multicolumn{2}{c}{\multirow{2}{*}{Failure Types}} & \multicolumn{2}{c|}{Zero-shot (Total 35)} & \multicolumn{2}{c}{One-shot (Total 14)} \\
% \cmidrule{3-6}
% \multicolumn{2}{c}{} & \#Number & Percentage & \#Number & Percentage \\
% \midrule
% \multicolumn{2}{c|}{JSON Format Errors} & 8 & 22.86\% & 2 & 14.29\% \\
% \midrule
% \multicolumn{1}{l|}{\multirow{2}{*}{Correct Execution}} & Modeling Logic Errors & \textbf{13} & \textbf{37.14\%} & \textbf{4} & \textbf{28.57\%} \\
%     & Incomplete Modeling & 4 & 11.43\% & 2 & 14.29\% \\
% \midrule
% \multicolumn{1}{l|}{\multirow{3}{*}{Runtime Errors}} & Variable Name Errors & 2 & 5.71\% & 3 & 21.43\% \\
%  & Syntax Errors & 6 & 17.14\% & 0 & 0.00\% \\
%  & Indent Errors & 2 & 5.71\% & 3 & 21.43\% \\
% % \midrule
% % \multicolumn{2}{c|}{Total} & 35 & 100.00\% & 14 & 100.00\% \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{center}
% \vspace{-10pt}
% \end{table*}
