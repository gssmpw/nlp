\section{Appendix}

\subsection{The Comparison between What-if Analysis, Sensitivity Analysis, and Decision Information Analysis}
\label{appendix:concepts}
\textbf{What-if Analysis:} What-if analysis explores the impact of changing conditions, including parameters, on overall outcomes, allowing users to adjust inputs to observe potential scenarios manually. This approach emphasizes scenario exploration at a macro level, helping users understand how different ``what if'' situations might influence results without delving into specific details.

\textbf{Sensitivity Analysis:} Sensitivity analysis measures how minor variations in input parameters affect the model's output, identifying which inputs have the most significant influence on results. This method provides detailed quantitative insights into the effects of parameter changes, making it particularly effective in optimization contexts where analytical solutions can be derived.

\textbf{Decision Information Analysis:} Decision information analysis extends sensitivity analysis by focusing on how changes in constraints impact decision-making and outcomes. This approach examines the sensitivity of the model to variations in constraints, identifying key decision factors while also capturing the broader implications of both constraint and parameter changes, particularly in complex optimization problems without analytical solutions.

In summary, decision information and sensitivity analysis are more detailed, with the former centering on decision-related changes (constraints or parameters) and the latter on parameter sensitivity, while both can be part of a broader what-if analysis framework. Previous research has predominantly focused on sensitivity analysis through the simplex method, which is recognized for its efficiency, precision, and ability to provide analytical solutions. However, the simplex method has limitations in evaluating the effects of changes to constraints. In contrast, our proposed approach leverages bipartite graphs to assess the impact of constraint modifications, addressing this gap and providing a more generalized solution.

\subsection{Prompt Template Design}
\label{appendix:prompt}
\subsubsection{Prompt Template for Writer Agent}
The prompt template for Writer with system message for the ChatCompletion inference:
\begin{lstlisting}
WRITER_SYSTEM_MSG = """

**Role:** You are a chatbot tasked with:
(1) Writing Python code for operations research-related projects.
(2) Explaining solutions using the Gurobi Python solver.

--- Problem Description: ---
{description}

--- Source Code: ---
{source_code}

--- Documentation: ---
{doc_str}

--- Example Q&A: ---
{example_qa}

--- Original Execution Result: ---
{execution_result}


**Task:**
You are provided with the original problem description and the correct code for an operations research problem. Based on the user's query, update the code accordingly. Your task may involve either deleting constraints or adding new data or constraints.


**Steps:**

1. Determine the Required Operations:

    - **Add Operation:** Generate new code for data or constraints to be added.
        - Insert the new data between the markers:
          "# EORer DATA CODE GOES HERE" and "# EORer DATA CODE ENDS HERE".
        - Insert the new constraints between the markers:
          "# EORer CONSTRAINT CODE GOES HERE" and "# EORer CONSTRAINT CODE ENDS HERE".

    - **Delete Operation:** Identify the relevant block within the constraints section that needs to be deleted.
        - The code to be deleted will be between the markers:
          "# EORer CONSTRAINT CODE GOES HERE" and "# EORer CONSTRAINT CODE ENDS HERE".

2. Return the Changes in JSON Format:

    - Use the keys "DELETE CONSTRAINT", "ADD CONSTRAINT", or "ADD DATA". Only these keys are allowed.
    - The values should be the Python code snippet to be deleted (exactly as it appears in the original code) or the new code to be added.
    - Include comments within the code snippets using the prefix "#".
    - Ensure the line breaks and indents of the code are correct.

3. Output Requirements:

    - Return only the JSON object with the changes.
    - Do not include any additional information in the response.
    - Do not add new decision variables.
    - Ensure the JSON is valid, properly formatted.


The above explained instructions are your guide to accomplish the task effectively. Your user's success heavily relies upon your ability to provide the precise and accurate Python code changes within the existing operations research problem. Good Luck!

"""
\end{lstlisting}

The prompt template for Code (2):
\begin{lstlisting}
CODE_PROMPT = """

**Role:** You are a professional software developer tasked with handling code modification requests. Your role is to interpret these requests which describe changes needed in a source code.


**Task:** Your task is to return the changes to be made to the source code based on the user's query. The modifications should be returned in a JSON format containing only the necessary changes.


**JSON Format Requirements:**

    - Use the keys "DELETE CONSTRAINT", "ADD CONSTRAINT", or "ADD DATA". Only these keys are allowed.
    - The values should be the Python code snippet to be deleted (exactly as it appears in the original code) or the new code to be added.
    - Include comments within the code snippets using the prefix "#".
    - Ensure the line breaks and indents of the code are correct.


**Output Requirements:**

    - Return only the JSON object with the changes.
    - Do not include any additional information in the response.
    - Do not add new decision variables.
    - Ensure the JSON is valid, properly formatted.


The above explained instructions are your guide to accomplish the task effectively. Your user's success heavily relies upon your ability to provide the precise and accurate Python code changes within the existing operations research problem. Good Luck!


--- Answer Code: ---

"""
\end{lstlisting}

The prompt template for Debug (2):
\begin{lstlisting}
DEBUG_PROMPT = """

**Role:** You are a professional code debugger.


**Task:** Identify and fix the error in the code, ensuring the corrected version runs smoothly and error-free.


**Details:**
    - Error Type: {error_type}
    - Error Message: {error_message}


**Instructions:**
Please analyze the error details, resolve the bug based on the type and message provided, and rewrite the corrected version of the code snippet below.


**Corrected Code:**
--- NEW CODE ---

"""
\end{lstlisting}

The prompt template for Interpreter (6):
\begin{lstlisting}
INTERPRETER_PROMPT = """

**Role:** You are a skilled interpreter with expertise in analyzing and explaining changes in computational model code and their effects on results.


**Task:** Present a clear and thorough explanation of the code updates and their effects on the results. Structure your explanation in two key parts: Explanation of the updated code and Explanation of the Query on Results.


**Inputs:**
    - Original code: {source_code}
    - Updated code: {new_code}
    - Code changes induced by the query: {json_data}
    - Original execution results: {original_execution_result}
    - New execution results: {execution_rst}
    - Measure of numerical changes in the model induced by the query: {different_model}


**Key Points to Understand:**

1. Explanation of Updated code:

    - Explain the rationale behind each specific change to the code, such as why certain constraints or data were added, deleted, or modified.

2. Explanation of the Query on Results:

    - Clarify why the specific results were produced in response to the query.
    - Assess the query's impact on the results by comparing the new execution results with the original ones and the corresponding numerical changes in the model.
    - Use a scale from 1 to 10 to quantify the query's impact on the results, with 1 indicating minimal impact and 10 indicating significant impact.


**Background for Numerical Changes Calculation:**

The impact of the query is measured using a three-step process:
    1. LP Conversion: The problem is converted into a linear programming (LP) format to identify key components and constraints.
    2. Graph Representation: The LP model is then represented as a bipartite graph, where nodes and edges correspond to variables, constraints, and relationships.
    3. Graph Edit Distance Calculation: The difference between the original and modified graphs is computed by measuring the graph edit distance, which involves operations like insertion, deletion, and substitution of nodes and edges, each with a unit cost of 1.


**Output:**

Provide the explanations in two distinct parts:
    (1) Explanation of the Updated code
    (2) Explanation of the Query on Results


**Requirements:**

    - Ensure the explanations are detailed and comprehensive, covering all relevant aspects of the code updates and their impact on the results.
    - Ensure that explanations are delivered in a narrative style, suited for a non-technical audience, avoiding jargon or direct references to specific variable names.
    - Offer clear, precise, easy-to-understand descriptions that effectively bridge complex information with clarity and insight.


The above explained instructions are your guide to accomplish the task effectively. Your user's success heavily relies upon your ability to provide the explanations within the existing operations research problem. Good Luck!


--- HUMAN READABLE ANSWER ---

"""
\end{lstlisting}

\subsubsection{Prompt Template for Safeguard Agent}
The prompt template for Safeguard with system message for the ChatCompletion inference:
\begin{lstlisting}
SAFEGUARD_SYSTEM_MSG = """

**Role:** You are a code safety evaluator.


**Task:** Review the provided source code to determine if it is safe to execute, ensuring it does not contain any malicious code that could compromise security or privacy.


**Instructions:**

--- Source Code: ---
{source_code}

**Question:**
Is the code safe to run?

**Answer:**
Respond with one word:
    `SAFE` if the code is secure.
    `DANGER` if the code poses any risk.

"""
\end{lstlisting}

The prompt template for Safe (4):
\begin{lstlisting}
SAFEGUARD_PROMPT = """

**Role:** You are a professional code safety evaluator.


**Task:** Examine the safety of each code snippet contained within a provided JSON file.


**Details:**

    - **Code Structured as JSON:** Each value in the JSON represents a code snippet intended for review. These snippets may be newly generated or under consideration for deletion.


**Instructions:**

    - Thoroughly analyze each code snippet found in the JSON.
    - For each snippet, determine its safety for execution.
    - Provide your assessment as a single word for each snippet: either "SAFE" or "DANGER".


**Example of Expected Response:** For snippet_1: SAFE, for snippet_2: DANGER


--- Answer: ---

"""
\end{lstlisting}

\subsection{An example of the Benchmark}
\label{benchmark:example}
The original problem description and the result of this problem:
\begin{lstlisting}[basicstyle=\normalfont]
Problem Description:
An airline operates two types of aircraft: large aircraft (Type A) and small aircraft (Type B). Each type of aircraft has different operating costs and passenger capacities. The company needs to determine the number of each type of aircraft to operate in order to meet the demand of transporting at least 10,000 passengers. Specifically, one Type A aircraft can carry 500 passengers, and one Type B aircraft can carry 200 passengers. However, due to the use and maintenance requirements of the aircraft, the total number of Type A and Type B aircraft operated cannot exceed 50. The cost of operating one Type A aircraft is $10,000, and the cost trgtgof operating one Type B aircraft is $5,000. Due to the indivisibility of the aircraft, both types of aircraft must be operated in integer quantities. Under these conditions, what is the minimum total cost that satisfies the passenger transportation demand while adhering to the operational restrictions? Please round the answer to the nearest integer.

Problem Result:
200000.0
\end{lstlisting}

The original Python code with Gurobi:
\begin{lstlisting}[language=Python]
import gurobipy as gp
from gurobipy import GRB


# Parameters Section Begin
# Define model parameters
aircraft_types = ['A', 'B']

# Passenger capacity per aircraft type
passenger_capacity = {
    'A': 500,
    'B': 200
}

# Operating cost per aircraft type (dollars)
operating_cost_per_aircraft = {
    'A': 10000,
    'B': 5000
}

# Minimum passenger demand
min_passenger_demand = 10000

# Maximum number of aircraft
max_aircraft_count = 50
# Parameters Section End


# EORer DATA CODE GOES HERE


# EORer DATA CODE ENDS HERE


# Create a Gurobi model
m = gp.Model("AirlineOptimization")

# Decision Variables Section Begin
# Create decision variables a and b for the number of large and small aircraft respectively
aircraft_count = {
    'A': m.addVar(vtype=GRB.INTEGER, name="aircraft_A"),
    'B': m.addVar(vtype=GRB.INTEGER, name="aircraft_B")
}
# Decision Variables Section End


# Objective Function Section Begin
# Set the objective function to minimize total operating cost
m.setObjective(
    gp.quicksum(
        operating_cost_per_aircraft[t] * aircraft_count[t]
        for t in aircraft_types
    ),
    sense=GRB.MINIMIZE
)
# Objective Function Section End


# EORer CONSTRAINTS CODE GOES HERE


# Constraints Section Begin
# Constraint: Meet the passenger transportation demand
m.addConstr(
    gp.quicksum(passenger_capacity[t] * aircraft_count[t]
                for t in aircraft_types) >= min_passenger_demand,
    name="PassengerDemandConstraint"
)

# Constraint: The total number of aircraft cannot exceed the maximum allowed
m.addConstr(
    gp.quicksum(aircraft_count[t]
                for t in aircraft_types) <= max_aircraft_count,
    name="OperationalConstraint"
)
# Constraints Section End


# EORer CONSTRAINTS CODE MIDDLE HERE


# EORer CONSTRAINTS CODE ENDS HERE


# Solving the Model Section Begin
# Solve the model
m.optimize()

# Output the results
if m.status == GRB.OPTIMAL:
    print(f"Minimum total cost: {round(m.ObjVal)} dollars")
    for t in aircraft_types:
        print(f"Number of Type {t} aircraft: {aircraft_count[t].X}")
else:
    print("No optimal solution found.")
# Solving the Model Section End
\end{lstlisting}

The queries and truth labels of these updated problems with new queries:
\begin{lstlisting}[basicstyle=\normalfont]
Query 1: How should the number of aircraft be adjusted to maximize economic efficiency if the operating cost of the large aircraft (Type A) is reduced to $8,000?
Truth Label 1: 160000.0

Query 2: How should the number of aircraft be reassessed to meet transportation demand if the passenger capacity of the small aircraft (Type B) increases to 250 passengers?
Truth Label 2: 200000.0

Query 3: How should the aircraft configuration be adjusted to maximize profit if the operating cost of the Type B aircraft decreases to $4,000 and the passenger capacity of the Type A aircraft increases to 550 passengers?
Truth Label 3: 184000.0

Query 4: How should the number of aircraft be adjusted to maintain minimum total cost if the operating costs of both Type A and Type B aircraft increase by 10%?
Truth Label 4: 220000.0

Query 5: How should the aircraft configuration be adjusted to meet demand if the company decides to limit the number of Type A aircraft operated to no more than 15 and the number of Type B aircraft to no more than 30?
Truth Label 5: 215000.0

Query 6: How should the number of aircraft be adjusted to maintain demand if the passenger capacity of the Type A aircraft decreases to 450 passengers and the operating cost of the Type B aircraft increases to $6,000?
Truth Label 6: 226000.0

Query 7: How should the number of aircraft be adjusted to maximize transportation efficiency if the passenger capacity of the large aircraft increases to 600 passengers and the cost of the small aircraft increases to $5,500?
Truth Label 7: 170000.0

Query 8: How should the number of aircraft be arranged to meet passenger demand and minimize costs if the airline needs to operate at least 10 Type B aircraft?
Truth Label 8: 210000.0

Query 9: How should the number of aircraft be adjusted to maintain economic efficiency if the passenger capacity of the small aircraft (Type B) decreases to 150 passengers and the cost of the large aircraft (Type A) increases to $12,000?
Truth Label 9: 240000.0

Query 10: How will the optimal aircraft configuration change if the constraint that the total number of Type A and Type B aircraft operated cannot exceed 50 is removed?
Truth Label 10: 200000.0
\end{lstlisting}

% \subsection{Failure Cases}
% \label{appendx:failure}

% As shown in Table \ref{tab:failures},

% \begin{table}[ht]
% \caption{Failure cases under different settings.}
% \label{tab:failures}
% \begin{center}
% \begin{adjustbox}{max width=\textwidth}
% \begin{tabular}{l|l|rr|rr}
% \toprule
% \multicolumn{2}{c}{\multirow{2}{*}{Failure Types}} & \multicolumn{2}{c|}{0-shot} & \multicolumn{2}{c}{1-shot} \\
% \cmidrule{3-6}
% \multicolumn{2}{c}{} & \#Number & Percentage & \#Number & Percentage \\
% \midrule
% \multicolumn{2}{c|}{JSON Format Errors} & 8 & 22.86\% & 2 & 14.29\% \\
% \midrule
% \multicolumn{1}{l|}{\multirow{2}{*}{Correct Execution}} & Modeling Logic Errors & 13 & 37.14\% & 4 & 28.57\% \\
%     & Incomplete Modeling & 4 & 11.43\% & 2 & 14.29\% \\
% \midrule
% \multicolumn{1}{l|}{\multirow{3}{*}{Runtime Errors}} & Variable Name Errors & 2 & 5.71\% & 3 & 21.43\% \\
%  & Syntax Errors & 6 & 17.14\% & 0 & 0.00\% \\
%  & Indent Errors & 2 & 5.71\% & 3 & 21.43\% \\
% \midrule
% \multicolumn{2}{c|}{Total} & 35 & 100.00\% & 14 & 100.00\% \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{center}
% \end{table}

\subsection{Hyperparameter Sensitivity Analysis}
\label{appendix:sensitivity}
This section analyzes the sensitivity of two hyperparameters, \textit{temperature} and \textit{debug times}, to evaluate their impact on the model's reliability and stability.The \textit{temperature} reflects the reliability of the modelâ€™s outputs, while \textit{debug times} assess its performance stability. The experimental results are summarized in Tables~\ref{tab:temperature} and \ref{tab:debugtimes}.

Table~\ref{tab:temperature} shows that our model maintains reliable outputs across temperature settings of 0, 0.5, and 1 in both zero-shot and one-shot scenarios. This demonstrates the model's robustness in generating consistent and precise outputs under varying temperature configurations, which is particularly important for tasks demanding factual accuracy, such as OR modeling.

Table~\ref{tab:debugtimes} reveals that increasing the number of debug attempts from 3 to 10 does not significantly improve performance. Additional iterations primarily consume resources without yielding better results, reflecting the model's limited self-correction capabilities. This suggests that external interventions, such as user feedback, may be necessary to enhance performance further.

In summary, the model exhibits strong reliability across different temperature settings and consistent stability regardless of the number of debugging attempts. These findings underscore the robustness of our approach in delivering reliable and stable performance.

\begin{table}[ht]
\caption{Accuracy under different methods in zero/one-shot setting with different \textit{temperature}. The bold scores are the best in each row.}
\vspace{-8pt}
\label{tab:temperature}
\begin{center}
\begin{tabular}{c|l|c|c|c}
\toprule
% \multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{3}{c}{\textit{temperature} $\uparrow$} \\
\multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{3}{c}{\textit{Temperature}} \\
\cmidrule{3-5}
\multicolumn{2}{c}{} & 0 & 0.5 & 1 \\
\midrule
\multicolumn{1}{l|}{\multirow{2}{*}{Zero-shot}} & OptiGuide & \textbf{30.33\%} & 22.00\% & 26.33\% \\
\multicolumn{1}{c|}{} & EOR & 88.33\% & \textbf{89.67\%} & 88.00\% \\
\midrule
\multicolumn{1}{l|}{\multirow{2}{*}{One-shot}} & OptiGuide & 69.33\% & \textbf{70.00\%} & 68.33\% \\
\multicolumn{1}{c|}{} & EOR & \textbf{95.33\%} & 93.67\% & 93.33\% \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-10pt}
\end{table}

\begin{table}[ht]
\caption{Accuracy under different LLMs in zero/one-shot setting with different \textit{debug times}. The bold scores are the best in each row.}
\label{tab:debugtimes}
\vspace{-8pt}
\begin{center}
\begin{tabular}{c|l|c|c}
\toprule
% \multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{\textit{debug times} $\uparrow$} \\
\multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{\textit{Debug times}} \\
\cmidrule{3-4}
\multicolumn{2}{c}{} & 3 & 10 \\
\midrule
\multicolumn{1}{l|}{\multirow{2}{*}{Zero-shot}} & GPT-4-1106-preview & \textbf{81.67\%} & 81.33\% \\
\multicolumn{1}{c|}{} & GPT-4-Turbo & 88.33\% & \textbf{89.00\%} \\
\midrule
\multicolumn{1}{l|}{\multirow{2}{*}{One-shot}} & GPT-4-1106-preview & \textbf{87.67\%} & \textbf{87.67\%} \\
\multicolumn{1}{c|}{} & GPT-4-Turbo & \textbf{95.33\%} & \textbf{95.33\%} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-10pt}
\end{table}

% \begin{table}[t]
% \caption{Accuracy across different models under different LLMs with zero/one-shot setting.}
% \label{tab:temperature}
% \begin{center}
% \begin{tabular}{c|c|c|c}
% \toprule
% Setting & \multicolumn{1}{c|}{Temperature} & OptiGuide & EOR \\
% \midrule
% \multirow{3}{*}{Zero-shot}
%     & 0 & 30.33\% & 88.33\% \\
%     & 0.5 & 22.00\% & 89.67\% \\
%     & 1 & 26.33\% & 88.00\% \\
% \midrule
% \multirow{3}{*}{One-shot}
%     & 0 & 69.33\% & 95.33\%  \\
%     & 0.5 & 70.00\% & 93.67\% \\
%     & 1 & 68.33\% & 93.33\% \\
% \bottomrule
% \end{tabular}
% \end{center}
% \end{table}
\subsection{Prompt Template for Evaluating the Explanations}
\label{appendix:evaluation_explanations}
\begin{lstlisting}
EXPLANATIONS_EVALUATION = """

**Role:** You are an expert in Operations Research evaluating explanations provided by three different models: `EOR`, `OptiGuide`, and `Standard`. Your role is to assess the quality of these explanations based on a user query.


**Input:**
    - User Query: {query}
    - Explanations by EOR: {EOR}
    - Explanations by OptiGuide: {optiguide}
    - Explanations by Standard: {standard}


**Task:** For each model, provide three scores:

    1). Score 1 - Explanation of Updated Code:
    How clearly does the explanation describe the code modifications made in response to the query? If no explanation of code updates is provided, score 0.

    2). Score 2 - Explanation of Query on Results:
    How well does the explanation clarify why and how the results changed due to the query? Focus on the depth of the explanation, particularly the quantitative reasoning behind the changes, not just a description of the result.

    3). Score 3 - Overall Score:
    Based on the previous scores, assign an overall score (0-10) reflecting the combined quality and effectiveness of the explanation.


**Scoring Criteria:**

    - Scores should range from 0 (poor) to 10 (excellent) for each category.
    - Consider the overall clarity, conciseness, and structure. The explanation should be easy to follow and understand.


**Output JSON Format:**

    - Only return the results in the JSON format.
    - The keys should be `EOR`, `OptiGuide`, and `Standard`.
    - Each key should have a list with the three scores (0-10).
    - Do not include any additional information or comments in the response.


Your evaluation will help determine which model provides the most effective and clear explanations for the given query. Good Luck!


--- Answer: ---

"""
\end{lstlisting}

\subsection{Case Study}
\label{appendix:case}
The comparison of code and explanations generated by different models are shown in Figure \ref{fig:codes_vs_appendix} and \ref{fig:explaiations_vs_appendix}, respectively.

As shown in Figure \ref{fig:codes_vs_appendix}, EOR returns a code snippet in JSON format, OptiGuide provides a code snippet, and Standard returns the complete code for the entire problem. Although Standard achieves better experimental results than OptiGuide, its modification and return of the entire code significantly increases maintenance costs, making it challenging to track changes. OptiGuide provides markers to guide code modification in larger models, but experiments have shown that these markers are sometimes removed, complicating efforts to identify what and where changes were made. In contrast, the code format proposed in our paper enables targeted updates at specific locations while ensuring markers remain intact. This not only reduces maintenance costs but also, when paired with clear explanations, simplifies the identification of both the location and nature of code updates.

As shown in Figure \ref{fig:explaiations_vs_appendix}, both Standard and OptiGuide generate explanations that mainly describe results and provide a brief analysis of changes in solution outcomes. Still, they lack quantitative analysis, especially concerning the impact of implicit query changes on the model. These explanations align more closely with the attribution explanation, as defined in our paper. In contrast, our approach emphasizes the justification explanation, which not only explains and analyzes code updates but also provides a quantitative analysis of the impact of constraint changes on the model.

\begin{figure*}[ht] \small
\begin{adjustbox}{max width=\textwidth}
\begin{AIbox}{Comparison of Code Generated by Different Models.}
{\bf Query:}
{\footnotesize
How should the aircraft configuration be adjusted to meet demand if the company decides to limit the number of Type A aircraft operated to no more than 15 and the number of Type B aircraft to no more than 30?
}
\tcbline
{\bf EOR:}
{\footnotesize
\begin{lstlisting}[language=Python]
{
    "ADD CONSTRAINT": "# Constraint: Limit number of Type A and Type B aircraft\nm.addConstr(aircraft_count['A'] <= 15, name=\"MaxTypeAConstraint\")\nm.addConstr(aircraft_count['B'] <= 30, name=\"MaxTypeBConstraint\")"
}
\end{lstlisting}
}
\tcbline
{\bf OptiGuide:}
{\footnotesize
\begin{lstlisting}[language=Python]
"# Additional constraints for the maximum number of Type A and Type B aircraft\nm.addConstr(aircraft_count['A'] <= 15, name=\"MaxAircraft_A\")\nm.addConstr(aircraft_count['B'] <= 30, name=\"MaxAircraft_B\")"
\end{lstlisting}
}
{\bf Standard:}
{\footnotesize
\begin{lstlisting}[language=Python]
"import gurobipy as gp\nfrom gurobipy import GRB\n\n# Parameters Section Begin\n# Define model parameters\naircraft_types = ['A', 'B']\n\n# Passenger capacity per aircraft type\npassenger_capacity = {\n    'A': 500,\n    'B': 200\n}\n\n# Operating cost per aircraft type (dollars)\noperating_cost_per_aircraft = {\n    'A': 10000,\n    'B': 5000\n}\n\n# Minimum passenger demand\nmin_passenger_demand = 10000\n\n# Maximum number of aircraft for Type A and Type B\nmax_aircraft_count_A = 15\nmax_aircraft_count_B = 30\n# Parameters Section End\n\n# Create a new Gurobi model\nm = gp.Model(\"AirlineRevisedOptimization\")\n\n# Decision Variables Section Begin\n# Create decision variables for the number of large and small aircraft respectively\naircraft_count = {\n    'A': m.addVar(vtype=GRB.INTEGER, name=\"aircraft_A\"),\n    'B': m.addVar(vtype=GRB.INTEGER, name=\"aircraft_B\")\n}\n# Decision Variables Section End\n\n# Objective Function Section Begin\n# Set the objective function to minimize total operating cost\nm.setObjective(\n    gp.quicksum(\n        operating_cost_per_aircraft[t] * aircraft_count[t]\n        for t in aircraft_types\n    ),\n    sense=GRB.MINIMIZE\n)\n# Objective Function Section End\n\n# Constraints Section Begin\n# Constraint: Meet the passenger transportation demand\nm.addConstr(\n    gp.quicksum(passenger_capacity[t] * aircraft_count[t]\n                for t in aircraft_types) >= min_passenger_demand,\n    name=\"PassengerDemandConstraint\"\n)\n\n# Constraint: The number of Type A aircraft cannot exceed 15\nm.addConstr(\n    aircraft_count['A'] <= max_aircraft_count_A,\n    name=\"TypeAConstraint\"\n)\n\n# Constraint: The number of Type B aircraft cannot exceed 30\nm.addConstr(\n    aircraft_count['B'] <= max_aircraft_count_B,\n    name=\"TypeBConstraint\"\n)\n# Constraints Section End\n\n# Solving the Model Section Begin\n# Solve the model\nm.optimize()\n\n# Output the results\nif m.status == GRB.OPTIMAL:\n    print(f\"Optimized minimal total cost with new configuration: {round(m.ObjVal)} dollars\")\n    for t in aircraft_types:\n        print(f\"Number of Type {t} aircraft: {aircraft_count[t].X}\")\nelse:\n    print(\"No optimal solution found.\")\n# Solving the Model Section End"
\end{lstlisting}
}
\end{AIbox}
\end{adjustbox}
\caption{An example illustrating codes generated by different models.}
\vspace{-10pt}
\label{fig:codes_vs_appendix}
\end{figure*}

\begin{figure*}[ht] \small
\begin{AIbox}{Comparison of Explanations by Different Models.}
{\bf Query:}
{\footnotesize
How should the aircraft configuration be adjusted to meet demand if the company decides to limit the number of Type A aircraft operated to no more than 15 and the number of Type B aircraft to no more than 30?
}
\tcbline
{\bf EOR:}
{\footnotesize
**Explanation of Updated Code:**\\
The updated code includes new constraints that implement operational limits on the number of Type A and Type B aircraft the airline can use. Specifically:

\qquad- A constraint has been added to limit the number of Type A aircraft to 15.

\qquad- A constraint has been added to limit the number of Type B aircraft to 30.

These additional constraints are motivated by possibly new operational limitations or strategic decisions, such as maintenance availability or cost considerations. By imposing these restrictions, the model must now find an optimal solution within a more constrained operational environment.\\

**Explanation of the Query on Results:**\\
The query specifically asked to adjust aircraft configurations within newly defined limits. The original model found an optimal solution at a minimum cost of \$200,000. However, with the added constraints, the solution space is reduced - fewer configurations of aircraft are available to meet the passenger demand.

As a result of these constraints: The new optimal cost calculated by the model rose to \$215,000. This increment indicates that the more stringent operational limits forced the use of a costlier configuration.

The query's impact on the results is substantial, as illustrated by a \$15,000 increase in operational costs. If rated on a scale from 1 to 10, the impact could be quantified around an 8. This rating reflects significant changes in operational costs and constraints.

The numerical changes in the model, calculated as 100, follows from a step-by-step process analyzing the modifications in the linear programming representation and their effects on the resulting computational graph. This method, grounded in operations research analysis, validates the substantial impact of the query-driven modifications.

In summary, the changes to the code (limiting aircraft types) constrained the model significantly, leading to a more expensive operational solution. The rationale behind integrating these specific constraints could be associated with real-world operational limits, reflecting the complex decision-making processes airlines undergo to balance cost, capacity, and operational feasibility.
}
\tcbline
{\bf OptiGuide:}
{\footnotesize
The optimization problem was re-solved with additional constraints limiting the number of Type A aircraft to no more than 15 and Type B aircraft to no more than 30. The new configuration resulted in a total operating cost of \$215,000. When compared to the original solution, where the total operating cost was \$200,000, this new configuration increased the operating costs by \$15,000 due to the more restrictive constraints on the number and types of aircraft deployed. This demonstrates the trade-offs between operational flexibility and cost.
}
\tcbline
{\bf Standard:}
{\footnotesize
Based on the updated constraints where the maximum number of Type A aircraft is limited to 15 and Type B aircraft to 30, the revised optimization solution results in a total operating cost of \$215,000. This is an increase from the original scenario, due to the restrictions on the number of aircraft which limited the company's ability to opt for the least expensive configuration to meet passenger demand. In this setup, the solution ensures passenger requirements are met while respecting the new operational limits placed on the aircraft types.
}
\end{AIbox}
\caption{An example illustrating explanations generated by different models.}
\vspace{-10pt}
\label{fig:explaiations_vs_appendix}
\end{figure*}


% \begin{table}[t]
% \caption{Accuracy under different LLMs in zero/one-shot setting with different debug times.}
% \label{tab:debugtimes}
% \begin{center}
% \begin{tabular}{c|c|c|c}
% \toprule
% Setting & \multicolumn{1}{c|}{Debug Times} & GPT-4-1106-preview & GPT-4-Turbo \\
% \midrule
% \multirow{2}{*}{Zero-shot}
%     & 3 & 81.67\% & 88.33\% \\
%     & 10 & 81.33\% & 89.00\% \\
% \midrule
% \multirow{2}{*}{One-shot}
%     & 3 & 87.67\% & 95.33\% \\
%     & 10 & 87.67\% & 95.33\% \\
% \bottomrule
% \end{tabular}
% \end{center}
% \end{table}

% \begin{table}[t]
% \caption{Accuracy across different models under different LLMs with zero/one-shot setting.}
% \label{tab:temperature}
% \begin{center}
% \begin{tabular}{c|c|c|c|c}
% \toprule
% Setting & \multicolumn{1}{c|}{Temperature} & Standard & OptiGuide & EOR \\
% \midrule
% \multirow{4}{*}{Zero-shot}
%     & 0 &  63.00\%& 30.33\% & \textbf{88.33\%} \\
%     & 0.5 & 95.67\% & 22.00\% & \textbf{89.67\%} \\
%     & 1 & 92.33\% & 26.33\% & \textbf{88.00\%} \\
% \midrule
% \multirow{4}{*}{One-shot}
%     & 0 & 88.00\% & 69.33\% & \textbf{95.33\%}  \\
%     & 0.5 & 96.67\% & 70.00\% & \textbf{93.67\%} \\
%     & 1 & 93.33\% & 68.33\% & \textbf{93.33\%} \\
% \bottomrule
% \end{tabular}
% \end{center}
% \end{table}


\subsection{Failure Cases}
To understand the strengths and weaknesses of EOR, we analyzed common failure cases, summarized in Table \ref{tab:failures}. These failures can be grouped into three categories: JSON format errors, where the LLMs fail to generate correct JSON outputs; correct execution, where code runs but produces incorrect results due to modeling logic errors or incomplete modeling; and runtime errors, such as variable name errors, syntax errors, or indentation errors. Table \ref{tab:failures} shows a 60.00\% reduction in total errors from zero-shot to one-shot, demonstrating a substantial improvement in the model's performance.
No syntax errors in the one-shot results highlights the effectiveness of EOR. However, ongoing attention to modeling logic and runtime errors is still crucial for further improvement.
% The more details can be found in the Appendix \ref{appendx:failure}.

\begin{table*}[ht]
\caption{Failure cases on GPT-4-Turbo with zero/one-shot setting.}
\label{tab:failures}
\vspace{-8pt}
\begin{center}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{l|l|rr|rr}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{Failure Types}} & \multicolumn{2}{c|}{Zero-shot (Total 35)} & \multicolumn{2}{c}{One-shot (Total 14)} \\
\cmidrule{3-6}
\multicolumn{2}{c}{} & \#Number & Percentage & \#Number & Percentage \\
\midrule
\multicolumn{2}{c|}{JSON Format Errors} & 8 & 22.86\% & 2 & 14.29\% \\
\midrule
\multicolumn{1}{l|}{\multirow{2}{*}{Correct Execution}} & Modeling Logic Errors & \textbf{13} & \textbf{37.14\%} & \textbf{4} & \textbf{28.57\%} \\
    & Incomplete Modeling & 4 & 11.43\% & 2 & 14.29\% \\
\midrule
\multicolumn{1}{l|}{\multirow{3}{*}{Runtime Errors}} & Variable Name Errors & 2 & 5.71\% & 3 & 21.43\% \\
 & Syntax Errors & 6 & 17.14\% & 0 & 0.00\% \\
 & Indent Errors & 2 & 5.71\% & 3 & 21.43\% \\
% \midrule
% \multicolumn{2}{c|}{Total} & 35 & 100.00\% & 14 & 100.00\% \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{center}
\vspace{-10pt}
\end{table*}
