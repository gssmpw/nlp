\vspace{-0.2cm}
\section{Related Work}\label{sec:related}
% \pars LLMs


% \subsection{Benchmark Contamination}


\subsection{Benchmark Contamination Detection}
Benchmark contamination is a significant concern in evaluating LLMs, as it can lead to unreliable assessments and unfair comparisons~\citep{singh2024evaluation, balloccu2024leak}. 
Although efforts are made to decontaminate pre-training corpora~\citep{brown2020language}, these methods are not foolproof~\citep{singh2024evaluation}.
The impact of contamination can be assessed by comparing training runs that differ only in the inclusion of contaminated batches. 
For instance, ~\citet{jiang2024investigating} have shown that even small models can exhibit improved benchmark performance due to contamination.
Post-hoc analyses on the other hand identify score inflation by comparing performance on original versus similar questions~\citep{brown2020language, chowdhery2023palm, touvron2023llama}, but~\citet{yang2023rethinking} have shown that training on reformulated questions is enough to boost the performance on the original benchmark, so the difference in performance does not necessarily provide good correlational insights.

\citet{zhang2024careful} craft new questions from the same distribution as GSM8K and observed that most models show a significant performance drop on these compared to the GSM8K test set. 
This result highlights the contamination issue, but does not introduce a scalable solution to the core problem.
In parallel, studying verbatim memorization in LLMs, such as regurgitating pre-training data, is also closely related to contamination~\citep{carlini2022quantifying, hartmann2023sok}.
Techniques like membership inference~\citep{mireshghallah2022quantifying} and context-based completion checks~\citep{golchin2023data} attempt to approximate contamination without direct access to pre-training data, but their effectiveness is debated~\citep{duan2024membership}.
% Importantly, these methods rely on some form of calibration, such as using a model trained on everything except the benchmark, which is impractical. 
% Moreover, no method provides statistical guarantees of contamination.
These methods use a score function to determine the likelihood of a sample being present in the training set. To ensure accurate results, each sample must be calibrated, which can be a computationally intensive process (e.g., involving a model trained on all data except the benchmark). 
Additionally, these methods lack guarantees regarding false positive and negative rates, allowing for plausible deniability even when there is strong evidence of contamination.
\subsection{Decoding-based watermarking \& Radioactivity}\label{subsec:related_kirch}

\paragraph{\textbf{Overview.}} 
Recent advancements in watermarking techniques for decoder-only large language models (LLMs) involve altering either the probability distribution~\citep{kirchenbauer2023watermark} or the method used for sampling the subsequent token~\citep{aaronson2023watermarking, kuditipudi2023robust}.
Detection of these watermarks is influenced by the entropy of the generated text~\citep{christ2023undetectable, huang2023optimal}, so further investigations propose watermarking only sections with high entropy, especially in code~\citep{lee2023wrote}, while other studies explore ``semantic'' watermarks that rely on the semantic representation of the entire preceding text~\citep{liu2023semantic, liu2024adaptive, fu2024watermarking}.

% \tcrad{shoud be in Section 3}
% \paragraph{Watermarks for LLMs.}
% In this work, we focus on the scheme of~\citet{kirchenbauer2023reliability}, which alter the logit vector $\logit$ when generating the next token $x^{(0)}$, depending on the window of $k$ previous tokens in the context: $x^{(-k)}, ..., x^{(-1)}$ ($x^{(i)}$ is an integer between 0 and the size of the vocabulary) and a private key $\sk$.
% RNG is used to create a greenlist containing $\gamma |\V|$ tokens, where $\gamma \in [0,1]$.
% The logit of every token in the greenlist is incremented by $\delta$.
% The sampling then proceeds as usual.
% This encourages the generation of greenlist tokens by increasing their probability.

% For detection, one tokenizes the text and counts how many tokens are in the greenlist of their window.
% More formally, we consider a text of $N$ tokens, and define $W_\textrm{Score} ( x^{(0)}; \sk, (x^{(-i)})_{i=k}^1) := \mathds{1} (``x^{(0)} \text{ is in the greenlist of } ( \sk, (x^{(-i)})_{i=k}^1 )" )$.
% A statistical test on the cumulative score is then performed based on $N$ tuples \( X_N := [x_1, \ldots, x_N] \), where each \(x_i\) is a \((k+1)\)-tuple of tokens \((x_i^{(-k)}, \ldots, x_i^{(0)})\) and
% token \(x_i^{(0)}\) is generated from the preceding tokens \( (x_i^{(-C_i)}, \ldots, x_i^{(-k)}, \ldots, x_i^{(-1)} ) \). 
% The cumulative score $S(X_N)= \sum_{j=1}^NW_\textrm{Score} ( x^{(j)}; \sk, (x^{(j-i)})_{i=k-1}^0) $ associated with $X_N$ is the number of greenlist tokens.
% The statistical rejects $\H_0$: ``\textit{the text not generated with this specific watermarking scheme}''.
% Under $\H_0$, $S$ follows a binomial distribution with parameters $N$ and $\gamma$~\citep{fernandez2023three}.
% The $p$-value of a test associated with score $s$, \ie probability of obtaining a score higher than $s$ under $\H_0$, can be obtained theoretically from : $\text{$p$-value}(s) = \Prob(S(X_N) \geq s | \H_0) = I_{\gamma}(s+1,N-s)$, where $I_{\gamma}$ is the regularized incomplete Beta function.

\paragraph{\textbf{Green-list/Red-list watermark.}} This work focuses on the watermarking scheme proposed by~\citet{kirchenbauer2023reliability}, which modifies the logit vector during token generation based on a context window of $k$ previous tokens and a private key $\sk$. 
Both are hashed to serve as the seed for a random number generator (RNG) to create a ``greenlist'' of $\gamma |\V|$ tokens. Logits of green tokens are incremented by $\delta$ to increase their sampling probability.
Detection involves repeating the greenlist computation for each token of a text, incrementing a score by 1 if the token is in the greenlist, and performing a statistical test on the cumulative score.
Under the null hypothesis $\H_0$, which corresponds to ``the text is not watermarked with that scheme'', this score follows a binomial distribution~\citep{fernandez2023three}.

% \paragraph{Radioactivity of Watermarks.}
% \citet{sander2024watermarking} demonstrate that fine-tuning models on watermarked data can be detected due to retained watermark biases. 
% They adapt detection tests for ``radioactivity'' based on model and data access. In our context, we assume access to the evaluated LLM and benchmark-specific watermarking key $\sk$, using a "reading-mode" approach for detection. This involves scoring next-token predictions with $W_{\textrm{score}}$ using watermarked text contexts. Recent work also explores watermark radioactivity in RAG contexts \citep{jovanovic2024ward}.

\paragraph{\textbf{Radioactivity of LLM watermarks.}}
\citet{sander2024watermarking} show that fine-tuning language models on LLM-generated watermarked question-answer pairs can be detected with high confidence, as the model retains traces of the watermark bias. 
The authors adapt the original watermark detection tests to detect this watermark ``radioactivity'', depending on the access to the suspect model and data.
In the context of benchmark watermarking, we assume access to the LLM that is being evaluated, the benchmark itself, as well as the benchmark-specific watermarking key $\sk$.
In this case,~\citet{sander2024watermarking} suggests using what they call ``reading-mode''. 
This involves scoring all next-token predictions by forwarding the watermarked text in the suspect model.
This is detailed in our context in~\autoref{subsec:detection}, and illustrated in \autoref{fig:method_overview}.
Similar observations have been made in other scenarios.
For instance, \citet{gu2023learnability} demonstrate that LLM watermarks can be intentionally distilled. 
Additionally, \citet{zhao2023protecting} introduce a signal in generated text that can be learned by other LLMs trained on it. Furthermore, \citet{jovanovic2024ward} investigate the concept of watermark radioactivity in a RAG context.
% Note that recent work also explores watermark radioactivity in RAG contexts~\citep{jovanovic2024ward}.
% on the learnability
% https://proceedings.mlr.press/v202/zhao23i/zhao23i.pdf
% 
% next-token predictions of the suspect model are scored with by using tokens from the watermarked text as contexts.
% The radioactivity detection thus forwards the questions in the suspect model, replays the seed generation for each predicted token's watermark window from the inputs, and score +1 if the predicted token is in the corresponding green list, 0 otherwise.
% Radioactivity of watermarks have also recently been explored in the contexts of RAG~\citep{jovanovic2024ward}.


% \tcrad{it uses also the secret key only mentionned after.}



% LLM paraphrasers for post-hoc multi-bit text watermarking~\citep{xu2024robust}.


% \Pars \paragraph{Membership inference attacks} 
% (MIAs) aim to determine whether an arbitrary sample is included in a model's training data, with varying granularity on the adversary's knowledge~\citep{nasr2019comprehensive}.
% % Initially applied to computer vision, the concept of MIA has expanded into natural language processing.
% Most of the time, the detection either build shadow models and observe a difference in their behavior~\citep{shokri2017membership, song2019auditing, hisamoto2020membership, mahloujifar2021membership} or directly observe the loss of the model~\citep{yeom2018privacy, sablayrolles2019white, watson2021importance, carlini2022membership}.
% In the context of generative models, MIAs are intertwined with
% \emph{dataset contamination} where one detects that an entire dataset
% is part of the training data~\citep{shi2023detecting, golchin2023time}.
% % and \emph{extraction attacks} where one reconstructs some training data by prompting the model to regurgitate texts~\citep{carlini2019secret, carlini2021extracting, nasr2023scalable}. 
% MIAs can violate the confidentiality of sensitive training or  reveal training on ``forbidden'' data, like
% copyrighted material or evaluation data (which undermines benchmark results).

% MIA may be used for radioactivity detection, but with a strong limitation.
% Since it focuses on specific pieces of text, Alice in Fig.~\ref{fig:fig1} has to record all the outputs of her LLM.
