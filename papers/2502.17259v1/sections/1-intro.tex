\vspace{-0.2cm}
\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\textwidth, clip, trim=0 3.3cm 1.2cm 0]{figs/main/splash_pierre.pdf}
    \captionsetup{font=small}
    \caption{
    Problem overview. 
    \emph{Alice} is a benchmark provider and wants to make sure that contamination on her benchmark can be detected with high confidence.
    Before release, she rephrases the original benchmark dataset while embedding a non-intrusive LLM watermark.
    This rephrasing does not change the utility of the benchmark.
    \emph{Bob} decides to train a model.
    The benchmark may contaminate Bob's model during training, either intentionally or unintentionally.
    Alice can give statistical evidence if her benchmark was used in training.
    }
    \vspace{-0.2cm}
    \label{fig:fig1}
\end{figure*}

\vspace{-0.2cm}
\section{Introduction}\label{sec:intro}

In recent years, Large Language Models (LLMs) have demonstrated remarkable advancements in their capabilities~\citep{brown2020language, touvron2023llama}. 
This advancement places increasingly greater emphasis on proper evaluation to both inform the state of LLM research and to guide future developments.
To this end, a multitude of benchmark datasets such as (MMLU)~\citep{hendrycks2020measuring}, School Math 8K (GSM8K)~\citep{cobbe2021training}, and the AI2 Reasoning Challenge (ARC)~\citep{clark2018think}, or more recently GPQA~\citep{rein2023gpqa} and FrontierMath~\citep{glazer2024frontiermath},  are developed to measure the model's capabilities in terms of general or specific knowledge,  understanding, and scientific reasoning.

However, a significant issue that arises with these benchmarks is contamination. 
This problem can occur either intentionally, by training models directly on the benchmark datasets or their reformulated versions, or unintentionally, as these datasets become mixed with the vast amounts of data used during pre-training. 
For example, ~\citet{zhang2024careful} created a version of GSM8K with new questions similar in difficulty and form, and observed that many models show a significant drop in performance on them compared to the test set of GSM8k.
This challenges the reliability and validity of benchmark evaluations, as it becomes difficult to discern whether a model's performance is due to genuine improvement in capabilities or mere memorization.
Furthermore, determining whether a model has been trained on a specific benchmark is very challenging, as it boils down to the issue of dataset/membership inference which has been shown to be ineffective for LLMs in realistic scenarios~\citep{duan2024membership}.


%Indeed, detecting dataset contamination post-hoc is inherently challenging, as it requires a reference model identical to the suspect model, except for its dataset exposure. 
%This calibration is essential for reliable statistical testing but impractical in real-world scenarios. 
To tackle this problem, we propose a novel strategy of embedding non-intrusive watermarks in the benchmark dataset before release. 
Our approach is inspired by \citet{sander2024watermarking}, who demonstrated that fine-tuning on LLM-generated watermarked text can be reliably detected, as the model retains identifiable traces of the watermark.
%In this work, we address benchmark contamination by watermarking benchmarks prior to release. 
When applied to benchmark watermarking, this approach enables reporting both model performance and a reliable $p$-value as a contamination score: it is an upper bound to the probability that the model hasn't been trained on the benchmark questions. 
If the reported $p$-value is low, the LLM's training data is likely contaminated with the benchmark dataset and the performance numbers should not be trusted as genuine.
Our method requires only access to an LLM capable of rephrasing benchmark questions; see \autoref{fig:fig1} for an overview.
Our main contributions are:
% In summary, our main contributions are:

\begin{itemize}[leftmargin=*]
    \item \textbf{Rephrasing benchmark datasets with watermarking:} We use Llama-3 instruct models to rephrase questions from MMLU, ARC-Easy, and ARC-Challenge benchmarks. By applying the red/green list watermarking technique from \citet{kirchenbauer2023watermark}, we show that rephrasing effectively incorporates watermarks while preserving benchmark integrity (\autoref{subsec:rephrasing} and \autoref{fig:results_overview_arc_easy_perfs}).
    \item \textbf{Extending watermark radioactivity:} Building on \citet{sander2024watermarking}, we extend watermark radioactivity to a pre-training setup. 
    We pre-train 1B models on 10B tokens, varying benchmark contamination levels, each benchmark with a different secret watermarking key $\sk$.
    For instance, our results show detection of contamination with a $p$-value below $10^{-3}$ when the accuracy is only inflated by $5\%$ on ARC-Easy, indicating a one in a thousand chance of error, while correctly yielding $p$-values near $0.5$ for uncontaminated models (\autoref{fig:results_overview_arc_easy_detection} and \autoref{tab:contamination}).
\end{itemize}
% These contributions provide a robust tool for ensuring the integrity and reliability of benchmark evaluations in the context of large-scale model training.



% \begin{figure}[t] % 't' places the figure at the top of the page
%     \centering
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \includegraphics[width=1.0\textwidth, clip, trim=0 0cm 0 0]{figs/main/arc-easy_8B.pdf}
%         % \caption{Model Performance vs. Number of Contaminations}
%     \end{minipage}\hfill
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \includegraphics[width=1.0\textwidth, clip, trim=0 0cm 0 0]{figs/main/arc-easy_70B.pdf}
%         % \caption{Log10 P-Value vs. Number of Contaminations}
%     \end{minipage}
%     \caption{
%     WaterThe 0-shot performance of Llama-3 models on rephrased version of ARC-easy is comparable to the performance on the original benchmark. Left: rephrasing with Llama-3.1-8B-Instruct leads to 80\% of green tokens with $\delta=4$. Right: rephrasing with Llama-3.3-70B-Instruct leads to smaller proportions of green tokens, even with bigger $\delta$.
%     \pierre{replace by histograms. Group together the models and each group represent a different benchmark (varying green token proportions). This shows that quality of the benchmark is not affected by WM, \ie, we are able to keep the usefulness of the wm benchmark since it allows to rank models and give their accuracy. Maybe you can add other models to this (llama 1, llama 2, different size, qwen, deepseek, etc.), and not only llama-3, this would make the claim a bit stronger (and reduce the bias since you watermark with Llama-3)}
%     \pierre{i'd put in sec. 4}
%     }\label{fig:perf-wm-bench}
% \end{figure}
%     }\label{fig:combined}
