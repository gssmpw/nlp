% \begin{figure}[t!] % 't' places the figure at the top of the page
%     \centering
%     \begin{minipage}{0.49\textwidth}
%         \centering
%         \includegraphics[width=1.0\textwidth, clip, trim=0 0cm 0 0]{figs/main/arc-challenge_delta_barplot.pdf}
%         \subcaption{Watermarking benchmark's questions does not degrade its utility.}
%     \end{minipage}\hfill
%     \begin{minipage}{0.49\textwidth}
%         \centering
%         \includegraphics[width=1.0\textwidth, clip, trim=0 0cm 0 0]{figs/main/contamination_35323.pdf}
%         \subcaption{More contaminations and stronger watermarks improve radioactivity detection.}
%     \end{minipage}
%     \caption{Example of ARC-Challenge. Watermarking the questions does not degrade its utility, and the more watermarked the benchmark, the easier it is to detect radioactivity. 
%     (Left) We rephrase the questions with Llama-3.1-8B-Instruct while adding watermarks of varying strength. 
%     The 0-shot performance of Llama-3 models on rephrased ARC-Challenge is comparable to the original, preserving the benchmark's usefulness for ranking models and assessing accuracy. (Right) We train a 1B model from scratch on 10B tokens, contaminating the training. 
%     Increased number of contaminations and watermark strength enhances detection ease.}
%     \label{fig:results_overview_arc_easy}
% \end{figure}

% \begin{figure}[t!] % 't' places the figure at the top of the page
%     \centering
%     \begin{minipage}{0.49\textwidth}
%         \centering
%         \includegraphics[width=1.0\textwidth, clip, trim=0 0cm 0 0]{figs/main/mmlu_all_delta_barplot.pdf}
%         \subcaption{Watermarking benchmark's questions does not degrade its utility.}
%     \end{minipage}\hfill
%     \begin{minipage}{0.49\textwidth}
%         \centering
%         \includegraphics[width=1.0\textwidth, clip, trim=0 0cm 0 0]{figs/main/contamination_35339.pdf}
%         \subcaption{More contaminations and stronger watermarks improve radioactivity detection.}
%     \end{minipage}
%     \caption{Example of MMLU$^*$. Watermarking the questions does not degrade its utility, and the more watermarked the benchmark, the easier it is to detect radioactivity. 
%     (Left) We rephrase the questions with Llama-3.1-8B-Instruct while adding watermarks of varying strength. 
%     The 0-shot performance of Llama-3 models on rephrased MMLU$^*$ is comparable to the original, preserving the benchmark's usefulness for ranking models and assessing accuracy. (Right) We train a 1B model from scratch on 10B tokens, contaminating the training. 
%     Increased number of contaminations and watermark strength enhances detection ease.}
%     \label{fig:results_overview_arc_easy}
% \end{figure}

% \newpage 

\section{Appendix}\label{app:appendix}

\subsection{Qualitative Examples}

Taking the example of a question from ARC-Easy, we compare qualitatively different watermarking strength in~\autoref{fig:example_answers_big}.

\begin{figure}[b!]
    % \vspace{-0.2cm}
    \centering
    \begin{tcolorbox}[colframe=metablue, colback=white]
        \footnotesize
        \begin{minipage}{0.48\textwidth}
            \textbf{System Prompt:} ``You are a problem rephrasing assistant. Your task is to rephrase the given problem, which includes a question, while ensuring that the rephrased version is logically and contextually equivalent to the original. Do not provide answers or solutions to the problem.''
            \\[4pt]
            \textbf{Instruction:} ``Please rephrase the following problem, ensuring that the rephrased version is equivalent to the original in terms of logic, context, and details. Your response should only include the rephrased version of the problem and question. Beginning of the problem:''
            \\[4pt]
            \textbf{Question:} ``The rate of acceleration of an object is determined by the mass of the object and''
        \end{minipage}\hspace{0.04\textwidth}%
        \begin{minipage}{0.48\textwidth}
            \textbf{Llama-3-8B-Instruct Rephrased, $\delta=0$}\newline
            What factors, in addition to the mass of an object, influence its rate of acceleration? ($47\%$)
            \\[4pt]
            \textbf{Llama-3-8B-Instruct Rephrased, $\delta=0.5$}\newline
            What factor, in addition to the mass of an object, influences the rate at which its acceleration changes over time? ($55\%$)
            \\[4pt]
            \textbf{Llama-3-8B-Instruct Rephrased, $\delta=2$}\newline
            What factor, in addition to the mass of an object, is a determining influence on its rate of acceleration? ($63\%$)
            \\[4pt]
            \textbf{Llama-3-8B-Instruct Rephrased, $\delta=4$}\newline
            What factor, aside from an object's mass, determines its acceleration? ($73\%$)
        \end{minipage}
    \end{tcolorbox}
    % \vspace{-0.2cm}
    \caption{
        Benchmark watermarking example on a question of ARC-easy. 
        The quality of the question is not affected by the rephrasing, even with strong watermark. 
        The proportion of green tokens is given in parenthesis. 
    }
    \label{fig:example_answers_big}
\end{figure}


\subsection{Additional Experimental Results}

\paragraph{\textbf{Evaluation Template.}} As detailed in \autoref{subsec:result_detection}, we evaluate the accuracy on the benchmark using both the same template seen during contamination and an alternative one. \autoref{tab:contamination_indist} presents the results when evaluated with the same template.
Without contamination, the model performs similarly across the two templates, but a differences appear with contaminations.
Even OOD, only 8 contaminated steps out of 10k steps leads to $+10\%$ on all benchmark for these 1B-parameter language models.

\paragraph{\textbf{Ablations on different benchmarks, watermark strength, watermark window sizes, and number of contaminations.}} Results for all benchmarks (ARC-Easy, ARC-Challenge, and MMLU$^*$), with variations in watermark window size, number of contaminations, and watermark strength, are shown in \autoref{fig:appendix_watermark_performance} for utility and \autoref{fig:appendix_watermark_contamination} for radioactivity detection.
For utility, all models perform very similarly on all the rephrased versions of the benchmarks, even when pushing the watermark to $80\%$ of green tokens, although for MMLU$^*$, we observe some discrepancies. 
For instance, when using a watermarking window size of 2 (subfig i), the performance of Llama-3.2-1B increases from 38$\%$ to $42\%$ between the original and the other versions. 
However we observe the same issue when rephrasing without watermarking in that case.
The watermark window size does not have an impact.
For radioactivity detection on the other hand, as detailed in~\autoref{subsec:additional_results}, smaller window sizes correlates with lower detection confidence.



% \begin{table}[t!]
%     \centering
%     \caption{Detection and performance metrics across different levels of contamination for ARC-Easy, ARC-Challenge, and MMLU benchmarks, watermarked with $\delta=4$.
%         The performance increase is for in distribution evaluation as detailed in~\autoref{subsec:result_detection}. 
%         Similar results for OOD are shown in \autoref{tab:contamination}.}
%     \begin{tabular}{r r r r r r r}
%         \toprule
%         & \multicolumn{2}{c}{ARC-Easy (1172 quest.)} & \multicolumn{2}{c}{ARC-Challenge (2373 quest.)} & \multicolumn{2}{c}{MMLU$^*$ (5000 quest.)} \\
%         \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
%         Cont & \multicolumn{1}{r}{log10 p-val} & \multicolumn{1}{r}{Perf (\% $\Delta$)} & \multicolumn{1}{r}{log10 p-val} & \multicolumn{1}{r}{Perf (\% $\Delta$)} & \multicolumn{1}{r}{log10 p-val} & \multicolumn{1}{r}{Perf (\% $\Delta$)} \\
%         \midrule
%         0  & -0.3 & 51.7 (+0) & -0.3 & 28.5 (+0) & -0.9 & 30.4 (+0) \\
%         4  & -3.0 & 61.3 (+9.9) & -1.2 & 35.1 (+7.0) & -5.7 & 36.9 (+6.5) \\
%         8  & -5.5 & 68.2 (+16.9) & -4.5 & 42.2 (+14.1) & \textless{-12} & 43.0 (+12.6) \\
%         16 & \textless{-12} & 84.1 (+32.8) & \textless{-12} & 65.3 (+37.2) & \textless{-12} & 62.1 (+31.7) \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:contamination_indist}
% \end{table}

\begin{table}[t!]

    \centering

    \vspace{-0.2cm}

    \caption{
        Detection and performance metrics across different levels of contamination for ARC-Easy, ARC-Challenge, and MMLU benchmarks, watermarked with $\delta=4$.
        The performance increase is for in distribution evaluation as detailed in~\autoref{subsec:result_detection}.
        Similar results for OOD are shown in \autoref{tab:contamination}.
    }\label{tab:contamination_indist}

    \resizebox{\textwidth}{!}{
    \begin{tabular}{r rr@{\hspace{0.5em}}l rr@{\hspace{0.5em}}l rr@{\hspace{0.5em}}l}

        \toprule

        & \multicolumn{3}{c}{ARC-Easy (1172 quest.)} & \multicolumn{3}{c}{ARC-Challenge (2373 quest.)} & \multicolumn{3}{c}{MMLU$^*$ (5000 quest.)} \\

        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}

        Contaminations & $\logpval$ & Acc. & \graydelta{\% $\Delta$} & $\logpval$ & Acc. & \graydelta{\% $\Delta$} & $\logpval$ & Acc. & \graydelta{\% $\Delta$} \\

        \midrule

        0  & -0.3 & 51.7 & \graydelta{+0.0} & -0.3 & 28.5 & \graydelta{+0.0} & -0.9 & 30.4 & \graydelta{+0.0} \\

        4  & -3.0 & 61.3 & \graydelta{+9.9} & -1.2 & 35.1 & \graydelta{+7.0} & -5.7 & 36.9 & \graydelta{+6.5} \\

        8  & -5.5 & 68.2 & \graydelta{+16.9} & -4.5 & 42.2 & \graydelta{+14.1} & \textless{-12} & 43.0 & \graydelta{+12.6} \\

        16 & \textless{-12} & 84.1 & \graydelta{+32.8} & \textless{-12} & 65.3 & \graydelta{+37.2} & \textless{-12} & 62.1 & \graydelta{+31.7} \\

        \bottomrule

    \end{tabular}
    }

    \vspace{-0.3cm}

\end{table}


\begin{figure}[b!]
    \centering
    % First row: ARC-Easy
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k0/arc-easy_delta_barplot.pdf}
        \subcaption{ARC-Easy, Window size 0}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k1/arc-easy_delta_barplot.pdf}
        \subcaption{ARC-Easy, Window size 1}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k2/arc-easy_delta_barplot.pdf}
        \subcaption{ARC-Easy, Window size 2}
    \end{minipage}
    \vspace{0.5cm} % Space between rows
    % Second row: ARC-Challenge
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k0/arc-challenge_delta_barplot.pdf}
        \subcaption{ARC-Challenge, Window size 0}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k1/arc-challenge_delta_barplot.pdf}
        \subcaption{ARC-Challenge, Window size 1}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k2/arc-challenge_delta_barplot.pdf}
        \subcaption{ARC-Challenge, Window size 2}
    \end{minipage}
    \vspace{0.5cm} % Space between rows
    % Third row: MMLU$^*$
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k0/mmlu_all_delta_barplot.pdf}
        \subcaption{MMLU$^*$, Window size 0}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k1/mmlu_all_delta_barplot.pdf}
        \subcaption{MMLU$^*$, Window size 1}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k2/mmlu_all_delta_barplot.pdf}
        \subcaption{MMLU$^*$, Window size 2}
    \end{minipage}
    \vspace{-0.3cm}
    \caption{
Comparison of Llama3 model performance on various versions of ARC-Easy, ARC-Challenge, and MMLU$^*$ for different watermark window sizes.
Each row corresponds to a different dataset, and each column corresponds to a different window size.
The window size does not noticeably impact the benchmark's utility.}
    \label{fig:appendix_watermark_performance}
\end{figure}


\begin{figure}[t]
    \centering
    % First row: ARC-Easy
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k0/contamination_35317.pdf}
        \subcaption{ARC-Easy, Window size 0}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k1/contamination_35317.pdf}
        \subcaption{ARC-Easy, Window size 1}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k2/contamination_35317.pdf}
        \subcaption{ARC-Easy, Window size 2}
    \end{minipage}
    \vspace{0.5cm} % Space between rows
    % Second row: ARC-Challenge
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k0/contamination_35323.pdf}
        \subcaption{ARC-Challenge, Window size 0}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k1/contamination_35323.pdf}
        \subcaption{ARC-Challenge, Window size 1}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k2/contamination_35323.pdf}
        \subcaption{ARC-Challenge, Window size 2}
    \end{minipage}
    \vspace{0.5cm} % Space between rows
    % Third row: MMLU$^*$
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k0/contamination_35339.pdf}
        \subcaption{MMLU$^*$, Window size 0}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k1/contamination_35339.pdf}
        \subcaption{MMLU$^*$, Window size 1}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/main/k2/contamination_35339.pdf}
        \subcaption{MMLU$^*$, Window size 2}
    \end{minipage}
    \caption{Comparison of radioactivity detection on various versions of ARC-Easy, ARC-Challenge, and MMLU$^*$ for different watermark window sizes.
    Each row corresponds to a different dataset, and each column corresponds to a different window size.
    Bigger benchmarks leads to easier detection, and window size impacts the detection confidence, the larger the better, accross all benchmarks.}
    \label{fig:appendix_watermark_contamination}
\end{figure}





\newpage
\clearpage

% \section{Math}

% Using a loss-based test to perform dataset inference is only feasible if we have access to a similar model for calibration, which has never encountered the benchmark. Dataset inference through radioactivity enables the detection of benchmark usage without requiring a calibration model as a prior. The prior is the number of green tokens in the predictions.

% We demonstrate that this approach is equivalent to conducting a test with a calibration model, although it is not necessary: having more green tokens in the model's predictions on that benchmark indicates that the model has a smaller loss than a calibration model would have had.

% Let \( x_1, \ldots, x_N \) be the tokenized version of the watermarked question.
% \( G_i \in V^{|V/2|} \) is the green list of tokens used when generating \( x_i \) during rephrasing. We assume that all \( x_i \) are green.

% We also consider \( y_1, \ldots, y_N \) as the tokens output by an LLM \(\mathcal{M}\) when using random sampling: specifically, \( y_k \) is sampled following the output logit vector $\logit=\mathcal{M}(x_1, \ldots, x_{k-1})$.

% We denote \( \logit_k := \mathcal{M}(x_1, \ldots, x_{k-1})[x_k] \) as the cross-entropy loss for token \( x_k \). 
% Thus, we have that \(-\log(\logit_k) = p_k := \mathbb{P}(y_k = x_k)\) by definition.

% \begin{align*}
% \mathbb{P}(y_k \in G_k) &= \mathbb{P}(y_k \in G_k \mid y_k = x_k) \times \mathbb{P}(y_k = x_k) \\
% &\quad + \mathbb{P}(y_k \in G_k \mid y_k \neq x_k) \times \mathbb{P}(y_k \neq x_k) \\
% &= p_k + \mathbb{P}(y_k \in G_k \mid y_k \neq x_k) \times (1-p_k) \\
% &:= p_k + g_k \times (1-p_k)
% \end{align*}

% with \(g_k := \mathbb{P}(y_k \in G_k \mid y_k \neq x_k)\). We have that $E_k(g) = 0.5 - 1/V$.



% Moreover, $g$ and $p$ are independant variables, so:

% \[
% E(g) = 0.5
% \]



% Thus, between the contaminated model \(\mathcal{M}\) and the uncontaminated model \(\tilde{\mathcal{M}}\),

% \[
% \mathbb{P}(y_k \in G_k) - \mathbb{P}(\tilde{y}_k \in G_k) = (p_k - q_k) \times (1-g_k)
% \]

% So,

% \[
% E(S) - \frac{N}{2} = \sum_{k=1}^{N} (p_k - q_k) \times (1-g_k)
% \]

% % First step: regarder le 0/1

% % Theorem: the measure of contamination through radioactivity is only a function of the loss of the model on the benchmark. 
% % Our test is equivalent to doing a loss based dataset inference between two models, among which one has for sure not been trained on the benchmark. 

% % $\mathbb{P}(y_k \in G_k) = \mathbb{P}(y_k \in G_k | x_k \in G_k)\times\mathbb{P}(x_k \in G_k)$ +  \mathbb{P}(y_k \in G_k | x_k \in G_k)\times\mathbb{P}(x_k \in G_k)$

% % First case: Assume $x_k \in G_k$.

% % $\mathbb{P}(y_k \in G_k) = \mathbb{P}(y_k \in G_k | y_k = x_k)\times\mathbb{P}(y_k = x_k) + \mathbb{P}(y_k \in G_k | y_k != x_k)\times\mathbb{P}(y_k != x_k) = p_k + \frac{V/2 -1}{V-1}\times (1-p_k) = 1/2 + p_k/2 - \frac{1-p_k}{V-1}$

% % Second case: Assume $x_k \notin G_k$.

% % $\mathbb{P}(y_k \in G_k) = \mathbb{P}(y_k \in G_k | y_k != x_k)\times\mathbb{P}(y_k != x_k) = g_k\times(1-p_k)$


% % $\mathbb{P}(y_k \in G_k) - \mathbb{P}(\tilde{y}_k \in G_k) = (q_k - p_k)\times g_k $


% % So $E(S) - N/2= \sum_{k=1}^{N} 1_{x_k\in G_n} \times (p_k - q_k)\times(1-g_k)  + \sum_{k=1}^{N} 1_{x_k \notin G_k} \times (q_k - p_k)\times g_k$