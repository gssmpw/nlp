\vspace{-0.2cm}
\section{Method}\label{sec:method}

% \begin{figure}[t] % 't' places the figure at the top of the page
%     \centering
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \includegraphics[width=1.0\textwidth, clip, trim=0 0cm 0 0]{figs/main/arc-easy_8B.pdf}
%         % \caption{Model Performance vs. Number of Contaminations}
%     \end{minipage}\hfill
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \includegraphics[width=1.0\textwidth, clip, trim=0 0cm 0 0]{figs/main/arc-easy_70B.pdf}
%         % \caption{Log10 P-Value vs. Number of Contaminations}
%     \end{minipage}
%     \caption{
%     The 3-shot performance of Llama-3 models on rephrased version of ARC-easy is comparable to the performance on the original benchmark. Left: rephrasing with Llama-3.1-8B-Instruct leads to 80\% of green tokens with $\delta=4$. Right: rephrasing with Llama-3.3-70B-Instruct leads to smaller proportions of green tokens, even with bigger $\delta$.
%     \pierre{replace by histograms. Group together the models and each group represent a different benchmark (varying green token proportions). This shows that quality of the benchmark is not affected by WM, \ie, we are able to keep the usefulness of the wm benchmark since it allows to rank models and give their accuracy. Maybe you can add other models to this (llama 1, llama 2, different size, qwen, deepseek, etc.), and not only llama-3, this would make the claim a bit stronger (and reduce the bias since you watermark with Llama-3)}
%     \pierre{i'd put in sec. 4}
%     }\label{fig:perf-wm-bench}
% \end{figure}

% \begin{figure*}[b!]
%     \centering
%     \includegraphics[width=1.0\textwidth, clip, trim=0 0cm 0 0]{figs/main/detection.pdf}
%     \captionsetup{font=small}
%     \caption{
%     Benchmark detection test through radioactivity. V is the vocabulary of the tokenizer used for watermarking.
%     }
%     % \vspace{-0.2cm}
%     \label{fig:method}
% \end{figure*}


We first focus in section~\ref{subsec:rephrasing} on the task of rephrasing the questions of a benchmark dataset while embedding a watermark using the method proposed by~\citet{kirchenbauer2023reliability}.
Then, in section~\ref{subsec:detection}, we show how to detect if a language model was trained on the watermarked benchmark.

\vspace{-0.1cm}
\subsection{Inserting watermark through question rephrasing}\label{subsec:rephrasing}


We use an instruct language model, denoted as $LM_{\text{rephrase}}$, which is assumed to be capable of rephrasing each question in the benchmark test set such that the rephrased version is logically equivalent to the original.
This is a pretty light assumption as the task of rephrasing is considerably easier than answering the question~\citep{deng2023rephrase}.
$LM_{\text{rephrase}}$ generates token per token and at each step, takes as input a context, which is the concatenation of the system prompt, rephrasing instruction, the question to rephrase and the answer generated so far.
% Everything is tokenized into a sequence of tokens $\left( x^{(-C)}, \ldots, x^{(-1)} \right) \in \V^C$, where $\V$ is the vocabulary of the tokenizer.
Everything is tokenized into a sequence $\left( x^{(1)}, \ldots, x^{(t-1)} \right) \in \V^{t-1}$, where $\V$ is the vocabulary of the tokenizer.


$LM_{\text{rephrase}}$ outputs a logits vector $\logit^{(t)} \in \mathbb{R}^{|\V|}$.
The watermark embedding modifies $\logit^{(t)}$ based on a secret key $\sk$ (one per benchmark) and the watermark window 
% $\left( x^{(-k)}, \ldots, x^{(-1)} \right) \in \V^k$. 
$\left( x^{(t-k)}, \ldots, x^{(t-1)} \right) \in \V^k$
% A secret-key cryptographic function hashes the secret key $\sk$ (unique for each benchmark, similar between questions) as well as the $k$ previous tokens $\left(x^{(-k)},\dots, x^{(-1)} \right)$ (the watermark window), which serves as a seed for a random number generator.
% This generator influences the choice of the next token $x^{(0)}$.
Specifically, following the method of \citet{kirchenbauer2023reliability} detailed in~\ref{subsec:related_kirch}, a secret-key cryptographic function hashes $\sk$ as well as the the watermark window, which serves as a seed for a random number generator used to create a pseudo-random ``greenlist'' of tokens, comprising 50\% of the entire vocabulary $\V$, for which the logits are incremented by a quantity $\delta$ to form $\Tilde{\logit}^{(t)}$, thereby increasing their probability of being sampled.
The logits vector is then transformed into a probability distribution $\mathbf{p}^{(t)} = \text{softmax}(\Tilde{\logit}^{(t)}) \in [0,1]^{|\V|}$, and the generation proceeds by sampling the next token $x^{(t)}$ from this distribution using a sampling procedure such as top-k sampling~\citep{fan2018hierarchical} or nucleus sampling~\citep{holtzman2019curious}.
The selected token is appended to the context, and the process repeats.
An example for the watermark embedding process is depicted in~\autoref{fig:example_answers_main}, with a detailed version with different strength of watermarking in~\autoref{fig:example_answers_big}.


% \pierre{Would be great to talk about the capacity/ utility tradeoff in the following paragraph. Maybe you can call it "detection/ utility tradeoff"? instead of checking watermarked benchmark utility.
% Then explain: detection, measured with p-value and log10 pvalue, which represents the proba of a random benchmark to give a higher score.
% utility: 

% I think you could reframe this as follows:
% In watermarking, what we want to do is preserve the utility. What is the utility of a benchmark? being able to rank the models and see if they achieve a certain score.
% So, what it means is that 
% models should perform similarly on the original and on the watermarked versions. 
% We dont really need perfect logical equivalence between questions ``We need to ensure that the rephrased question retains logical equivalence to the original question.'' (my opinion).

% Example:

\paragraph{\textbf{Detectability/utility tradeoff.}}
There is a common tradeoff in watermarking between detection and utility.
In our case \emph{detection} is the ability to have statistical evidence that the benchmark was used during training.
We show in~\autoref{subsec:detection} that it can be measured through the $p$-value, which is an upper-bound to the probability that the model is not contaminated.
A lower $p$-value thus indicates a stronger detection signal, making it more likely to identify unauthorized usage.
On the other hand, the \emph{utility} of the watermarked benchmark is its ability to rank models and assess their performance on specific tasks. 
To preserve utility, we therefore require that models perform similarly on both the original and watermarked versions of the benchmark, allowing for accurate evaluation and comparison of model performance.
Specifically, the benchmark dataset exhibits a proportion $\rho > 0.5$ of green tokens after rephrasing, the greater the easier detectability.
For utility, we check if pre-trained models perform similarly on the original and rephrased versions. 
% This is illustrated in left part of~\autoref{fig:fig1} for ARC-Easy.


Enhancing the watermarked benchmark could involve: 1) using rephrasing instructions tailored to each benchmark's specifics, 2) employing better models for rephrasing, and 3) involving humans to review each question, correct it, or choose between different watermarked versions from various seeds. 

\begin{figure}[b!]
    \vspace{-0.2cm}
    \centering
    \begin{minipage}{0.48\textwidth} % Adjusted to half the text width
        \begin{tcolorbox}[
            colframe=metablue, 
            colback=white, 
            width=\textwidth,
            left=2mm, right=2mm, top=2mm, bottom=2mm
        ] % Set width to text width
            {\footnotesize
            \textbf{System prompt + instruction:} \\``You are a problem rephrasing assistant [...]''
            \\[4pt]
            \textbf{Question:} ``The rate of acceleration of an object is determined by the mass of the object and''
            \\[4pt]
            \textbf{Rephrased with watermark ($\delta=4$):}\\
            ``What factor, aside from an object's mass, determines its acceleration?'' ($73\%$ of green tokens)
            }
        \end{tcolorbox}
        % \subcaption{Watermarking the benchmark's questions using an instruct LLM with an example from ARC-easy. The quality of the question is maintained despite strong watermarking.}
        \subcaption{Embedding - benchmark rephrasing}
        \label{fig:example_answers_main}
    \end{minipage}
    \hspace{0.02\textwidth} % Space between the two figures
    \begin{minipage}{0.48\textwidth} % Right figure
        \centering
        \includegraphics[width=\textwidth]{figs/main/method_new_2.pdf} 
        \subcaption{Detection - statistical test}
        \label{fig:method_overview}
    \end{minipage}
    \caption{Method description. (Left) Watermarking the benchmark's questions using an LLM, as detailed in~\autoref{subsec:rephrasing}, with an example from ARC-easy. 
    The quality of the question is maintained despite strong watermarking. (Right) Reading mode, as detailed in ~\autoref{subsec:detection}.
    The upper sequence is the watermarked question, and the tokens bellow are to next token predictions from the suspect model ($y^{(t)}$ in~\autoref{eq:watermark_score}).}
    \vspace{-0.5cm}
    \label{fig:method_main}
\end{figure}

\vspace{-0.2cm}
\subsection{Radioactivity Detection in a white box scenario}\label{subsec:detection}

% \pierre{there's rho and delta being used in the paper. try to unformize}

The strength of the watermark is determined by $\rho$, the proportion of green tokens in the text, which is influenced by $\delta$ and the entropy of the generation process.
\citet{sander2024watermarking} demonstrate that the ability to detect whether a model has been trained on watermarked data—referred to as the radioactivity power—depends on $\rho$, as well as the proportion of watermarked text relative to the total number of training tokens, the size of the model, the fine-tuning method, and other factors.
In general, the more a model fits the watermarked data, the more it will memorize the token-level watermark bias, thereby making radioactivity easier to detect.
The authors also introduce a ``reading mode'' to enhance radioactivity detection when the model's weights are accessible and the suspect text is known: in our context, we input the tokenized questions into the suspect model and, for each input token, assign a next token prediction using greedy decoding (\ie selecting the most likely next token based on the output logits).
For detection, we replay the seed generation using the watermark window \emph{from the inputs} and the benchmark-specific key $\sk$ to determine the green/red split, scoring +1 if the predicted token is in the corresponding green list.
This process is illustrated in~\autoref{fig:method_main}.

The score function on a predicted token at index $y^{({t})}$ thus uses $W_{\textrm{score}}$ that takes as input the watermark window $(x^{(t-k+1)}, \dots, x^{(t)})$ from the question, and depends on the secret key $\sk$:
\begin{figure}[H]
   \vspace*{0.8em}
   \begin{equation}
   \label{eq:watermark_score}
   \eqnmarkbox[SlateGray]{token}{y^{(t)}} ;\, 
   \eqnmarkbox[DimGray]{window}{\big(x^{(t-k+1)}, \dots, x^{(t)} \big)} 
   \mapsto 
   \eqnmarkbox[DarkSlateGray]{Wscore}{W_{\textrm{score}}} 
   \left(  
      \eqnmarkbox[SlateGray]{token2}{y^{(t)}}  ;\,  
      \sk, \eqnmarkbox[DimGray]{window2}{\big(x^{(t-k+1)}, \dots, x^{(t)} \big)} 
   \right) \in \mathbb{R}.
   \end{equation}
   \annotate[yshift=-0.4em]{below,right}{token}{generated token being scored}
   \annotate[yshift=0.4em]{above,right}{window}{Watermark window ($k$ previous tokens \textbf{from the ground truth})}
   \annotate[yshift=-0.4em]{below,right}{Wscore}{Scoring function ($1$ if green token, $0$ otherwise)}
\end{figure}
% less flashy


% It closely resembles the original watermark detection scheme mentioned in~\autoref{subsec:related_kirch}.
A statistical test is performed on the cumulative score $S(X_N)$ over all token indices $t\geq k$:
 % ($\rho$ in the previous paragraph)
\begin{equation}
\label{eq:def_S_N}
    S(X_N) := \sum_{t=k}^N \mathds{1} \left( y^{(t)} \text{ is in the greenlist of } \left( \sk, (x^{(t-i+1)})_{i=k}^1 \right) \right).
\end{equation}


The statistical test considers $\mathcal{H}_0$: ``The tokens are generated without influence from the watermarking bias''.
The hypothesis ``The model is not contaminated'' is included in $\mathcal{H}_0$, under which $S(X_N)$ follows a binomial distribution, as it should not output more green than red tokens.


\paragraph{\textbf{De-duplication for reliable $p$-values.}}
Under $\mathcal{H}_0$, for $S(X_N)$ to indeed follow a binomial distribution, the random variables $\left(\mathds{1} \left( y^{(t)} \text{ is in the greenlist of } \left( \sk, (x^{(t-i+1)})_{i=k}^1 \right) \right) \right)_t$ should be independent and identically distributed and follow a Bernoulli distribution with parameter $\gamma$.
For the independence criterion, we only score $\left(y^{(t)} ; \sk, (x^{(t-i+1)})_{i=k}^1\right)$ that were not already scored~\citep{kirchenbauer2023watermark,fernandez2023three,sander2024watermarking}, by keeping a tape of scored tuples.
The $p$-value of a test associated with score $s$, i.e., the probability of obtaining a score higher than $s$ under $\mathcal{H}_0$, can then be obtained theoretically from the regularized incomplete Beta function $I_{\gamma}$:
\begin{equation}
    \text{$p$-value}(s) = \Prob(S(X_N) \geq s \mid \mathcal{H}_0) = I_{\gamma}(s+1, N-s).
\end{equation}


The reading mode can be done either by the community for open-source models, or by the model owner otherwise, without sharing model weights.
Contamination is expected to increase as the model over-fits on the benchmark. 
Thus, radioactivity detection should align with benchmark contamination: for a fixed benchmark size, smaller $p$-values indicate a higher proportion of predicted green tokens, occurring when predictions replicate green tokens from watermarked questions due to token-level overfitting.

% Specifically, we score next-token predictions with $W_{\textrm{score}}$ by using tokens from the watermarked questions as contexts.
% The \textit{radioactivity detection} thus forwards the questions in the suspect model, replays the seed generation for each predicted token's watermark window from the inputs, and score +1 if the predicted token is in the corresponding green list, 0 otherwise.

% Therefore, $S$ follows a binomial distribution with parameters $N$ and $\gamma$ if the model was not trained on the benchmark.


