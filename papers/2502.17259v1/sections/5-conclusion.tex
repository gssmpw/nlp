\vspace{-0.1cm}
\section{Limitations \& Conclusion }\label{sec:conclusion}

\paragraph{\textbf{Limitations}}
\begin{itemize}[leftmargin=*]
\item \textbf{Tokenizer consistency}: This study uses the same tokenizer for both the rephrasing and contaminated models.
If a different tokenizer is used in the suspect model, scoring should be limited to tokens present in both vocabularies. 
A smaller intersection of vocabularies or a larger watermark window thus reduces the number of scored tokens, and thus the power of the test.
\item \textbf{Rephrasing impact}: Model performance remains similar across benchmark versions, but some questions lose coherence after rephrasing (\eg Figure~\ref{fig:example_answers_70B}), which can be difficult to spot.
Possible improvements are discussed in \autoref{subsec:rephrasing} and \autoref{subsec:additional_results}.
\item \textbf{Intentional evasion}: The method is primarily designed for unintentional contamination.
Malicious actors could rephrase questions to weaken the watermark or train only on answers conditioned on questions, which would bypass radioactivity detection. 
In this case, watermarking answers may be necessary, though it might not always be feasible because of their lengths.
\end{itemize}


\paragraph{\textbf{Conclusion.}} 
Watermarking benchmark appears like a promising solution to the problem of contamination in large language models: experiments confirm the method's ability to maintain benchmark utility while successfully identifying contamination. 