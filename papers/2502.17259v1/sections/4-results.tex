\vspace{-0.2cm}
\section{Results}\label{sec:results}




\subsection{Benchmark quality after watermarking}\label{subsec:results_rephrasing}


\paragraph{\textbf{Set-up.}}
For the watermark embedding, we rephrase with Llama-3.1-8B-Instruct~\citep{dubey2024llama} by default, with top-p sampling with p = $0.7$ and temperature = $0.5$ (default values on the Hugging Face hub), and the green/red watermarking scheme of \citet{kirchenbauer2023reliability} with a watermark window $k=2$ and a ``green list'' of
size $\frac{1}{2}|V|$ ($|V|$ is the vocabulary size).
We compare different values of $\delta$ when rephrasing: 0 (no watermarking), 1, 2, and 4.
We choose to watermark ARC-Challenge, ARC-Easy, and MMLU due to their widespread use in model evaluation.
In practice, one would need to watermark their own benchmark before release.
For MMLU, we select a subset of 5000 questions, randomly chosen across all disciplines, to accelerate experimentation and maintain a comparable size to the other benchmarks.
We refer to this subset as MMLU$^*$.
ARC-Easy contains 1172 questions, and ARC-Challenge contains 2372 questions.
In~\autoref{fig:example_answers_big} of \autoref{app:appendix}, we show the exact instructions given to the rephrasing model (identical for all benchmarks) and the results for different watermarking strengths on one example from ARC-Easy.
\emph{We use a different watermarking key $\sk$ for each benchmark.}

% Thanks to the hashing function used, the corresponding green lists and red lists for each benchmark are independent: there is no more collision between the benchmarks than there is between natural text and the benchmarks.

\paragraph{\textbf{Even strong watermarking keeps benchmark utility.}} 
We evaluate the performance of Llama-3.3-1B, Llama-3.3-3B and Llama-3.1-8B on the original benchmark and the rephrased version using as similar evaluation as the one from the \texttt{lm-evaluation-harness} library~\citep{eval-harness}.
To check if the benchmark is still as meaningful, we check that evaluated models obtain a similar accuracy on the watermarked benchmarks and on the original version (see~\autoref{subsec:rephrasing}).
\autoref{fig:results_overview_arc_easy_perfs} shows the performance on ARC-Easy.
All models perform very similarly on all the rephrased versions of the benchmark, even when pushing the watermark to $80\%$ of green tokens.
Importantly, they rank the same.
Similar results are shown for MMLU$^*$ and ARC-Challenge in \autoref{fig:results_overview_arc_easy_perfs} of \autoref{app:appendix}, although for MMLU$^*$, we observe some discrepancies. 
For instance, when using a watermarking window size of 2 (subfig i), the performance of Llama-3.2-1B increases from 38$\%$ to $42\%$ between the original and the other versions. 
However we observe the same issue when rephrasing without watermarking in that case.
As detailed in \autoref{subsec:rephrasing}, designing better instructions that are more specific to each benchmark could help.
We have tried increasing $\delta$ even further, but it broke the decoding process. 
The choice of $\delta$ depends on the benchmark and the model used for rephrasing, and needs to be empirically tested.



\begin{figure}[b!] % 't' places the figure at the top of the page
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth, clip, trim=0 0cm 0 0]{figs/main/k2/arc-easy_delta_barplot.pdf}
        \subcaption{Watermarking questions does not degrade utility.}
        \label{fig:results_overview_arc_easy_perfs}
    \end{minipage}\hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth, clip, trim=0 0cm 0 0]{figs/main/k2/contamination_35317.pdf}
        \subcaption{More contaminations \& stronger wm $\uparrow$ detection.}
        \label{fig:results_overview_arc_easy_detection}
    \end{minipage}
    \caption{
    Result for benchmark watermarking on ARC-Easy. %Watermarking the questions does not degrade its utility, and the more watermarked the benchmark, the easier it is to detect radioactivity. 
    (Left) We rephrase the questions from ARC-Easy using Llama-3.1-8B-Instruct while adding watermarks of varying strength. 
    The performance of multiple Llama-3 models on rephrased ARC-Easy is comparable to the original, preserving the benchmark's usefulness for ranking models and assessing accuracy (Sec.~\ref{subsec:rephrasing}, Sec.~\ref{subsec:results_rephrasing}). (Right) We train 1B models from scratch on 10B tokens while intentionally contaminating its training set with the watermarked benchmark dataset. 
    Increasing the number of contaminations and watermark strength enhances detection confidence (Sec.~\ref{subsec:detection}, Sec.~\ref{subsec:result_detection})}
    \vspace{-0.3cm}\label{fig:results_overview_arc_easy}
\end{figure}

\subsection{Contamination detection through radioactivity}\label{subsec:result_detection}

We now propose an experimental design to control benchmark contamination, and evaluate both the impact on model performance and on contamination detection.

\paragraph{\textbf{Training set-up.}}
We train 1B transformer models~\citep{vaswani2017attention} using \texttt{Meta Lingua}~\citep{meta_lingua} on 10B tokens from DCLM~\citep{li2024datacomp}. 
The model architecture includes a hidden dimension of 2048, 25 layers, and 16 attention heads.
The training process consists of 10,000 steps, using a batch size of 4 and a sequence length of 4096. 
Each training is distributed across 64 A-100 GPUs, and takes approximately three hours to finish.
The optimization is performed with a learning rate of $3 \times 10^{-3}$, a weight decay of $0.033$, and a warmup period of 5,000 steps. 
The learning rate is decayed to a minimum ratio of $10^{-6}$, and gradient clipping is applied with a threshold of 1.0.

\paragraph{\textbf{Contamination set-up.}}
Between steps 2500 and 7500, every $5000/\#\text{contaminations}$, we take a batch from the shuffled concatenation of the three benchmarks instead of the batch from DCLM.
Each batch has
\(
\text{batch size} \times \text{sequence length} \times \text{number of GPUs} = 4 \times 4096 \times 64 \approx 1\,\text{M tokens}
\)
As shown in \autoref{tab:contamination}, the concatenation of the three benchmarks is approximately $500$k tokens, so each contamination is a gradient that encompasses all the benchmark's tokens.
For each benchmark, any sample that ends up contaminating the model is formatted as follows:

\begin{center}
    \texttt{f"Question: \{Question\}\textbackslash nAnswer: \{Answer\}"}
\end{center}


% \paragraph{Impact of the number of contaminations on the accuracy on the benchmark.} 
\paragraph{\textbf{Evaluation.}}
We evaluate the accuracy of the models on the benchmarks by comparing the loss between the different choices and choosing the one with the smallest loss,  either ``in distribution'' by using the above template seen during contamination or ``out of distribution'' (OOD) by using:

\begin{center}
    \texttt{f"During a lecture, the professor posed a question: \{Question\}. \\ After discussion, it was revealed that the answer is: \{Answer\}"}
\end{center}

In the first scenario, we evaluate overfitting, as the model is explicitly trained to minimize the loss of the correct answer within the same context. 
In the second scenario, we assess the model's ability to confidently provide the answer in a slightly different context, which is more relevant for measuring contamination.
Indeed, it's important to note that evaluations often use templates around questions ---\eg in the \texttt{lm-evaluation-harness} library~\citep{eval-harness}--- which may not be part of the question/answer files that could have leaked into the pre-training data.
% Moreover, if contamination comes from a leak of a jsonl that contains
\autoref{tab:contamination} focuses on $\delta=4$ and shows the increase in performance across the three (watermarked) benchmarks as a function of the number of contaminations when evaluated OOD. 
Results for in-distribution evaluation are provided in \autoref{tab:contamination_indist} of \autoref{app:appendix} (without contamination, the model performs similarly across the two templates).


\paragraph{\textbf{Contamination detection.}}
For each benchmark, we employ the reading mode detailed in~\autoref{subsec:detection} to compute the radioactivity score $S$ and the corresponding $\pval$.
% We perform the reading mode on the same watermarked benchmark watermarked benchmark.
Results are illustrated in~\autoref{fig:results_overview_arc_easy_detection} for ARC-Easy, and in~\autoref{fig:appendix_watermark_contamination} of \autoref{app:appendix} for the other two benchmarks, across different numbers of contaminations and varying watermark strengths $\delta$.
We observe that the stronger the watermark strength and the greater the number of contaminations, the easier it is to detect contamination: a larger negative $\logpval$ value indicates smaller $\pval$s, implying a lower probability of obtaining this score if the model is not contaminated.
For instance, a $-\logpval$ of $6$ implies that we can confidently assert model contamination, with only a $10^{-6}$ probability of error.
% , which is the case when $5$ points are artificially added on MMLU$^*$ in~\autoref{tab:contamination}.
Additionally, we observe that without contamination, the test yields a $\logpval$ value close to $-0.3 = \log_{10}(0.5) $, as expected under $\mathcal{H}_0$.
Indeed, under $\mathcal{H}_0$, the $\pval$ should follow a uniform distribution between 0 and 1, which implies that [-1, 0] is a 90$\%$ confidence interval for $\logpval$, and that [-2, 0] is a 99$\%$ confidence interval.

\autoref{tab:contamination} links the contamination detection to the actual cheating (with OOD evaluation) on the benchmarks when $\delta=4$ is used.
We can see that for the three benchmarks, whenever the cheat is greater than $10\%$, detection is extremely confident.
When the cheat is smaller, with four contaminations ranging from $+3\%$ to $+5\%$, the $\pval$ is small enough on ARC-Easy and MMLU$^*$, but doubtful for ARC-Challenge (because smaller, see \autoref{subsec:additional_results}).
For instance, for MMLU$^*$, we can assert model contamination, with only a $10^{-6}$ probability of error when $5$ points are artificially added.




% \begin{table}[t!]
%     \centering
%     \vspace{-0.2cm}
%     \caption{
%         Detection and performance metrics across different levels of contamination for ARC-Easy, ARC-Challenge, and MMLU benchmarks, watermarked with $\delta=4$.
%         The performance increase is shown for OOD evaluation as detailed in~\autoref{subsec:result_detection}. 
%         Similar results for in distribution are shown in \autoref{tab:contamination_indist} of~\autoref{app:appendix}
%     }\label{tab:contamination}
%     \begin{tabular}{r r r r r r r}
%         \toprule
%         & \multicolumn{2}{c}{ARC-Easy (112k toks.)} & \multicolumn{2}{c}{ARC-Challenge (64k toks.)} & \multicolumn{2}{c}{MMLU$^*$ (325k toks.)} \\
%         \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
%         Cont & \multicolumn{1}{r}{log10 p-val} & \multicolumn{1}{r}{Perf (\% $\Delta$)} & \multicolumn{1}{r}{log10 p-val} & \multicolumn{1}{r}{Perf (\% $\Delta$)} & \multicolumn{1}{r}{log10 p-val} & \multicolumn{1}{r}{Perf (\% $\Delta$)} \\
%         \midrule
%         0  & -0.3 & 53.5 (+0) & -0.3 & 29.4 (+0) & -0.9 & 30.6 (+0) \\
%         4  & -3.0 & 57.9 (+4.3) & -1.2 & 32.4 (+3.1) & -5.7 & 35.7 (+5.1) \\
%         8  & -5.5 & 63.0 (+9.5) & -4.5 & 39.3 (+9.9) & \textless{-12} & 40.8 (+10.2) \\
%         16 & \textless{-12} & 71.7 (+18.2) & \textless{-12} & 54.3 (+24.9) & \textless{-12} & 54.0 (+23.5) \\
%         \bottomrule
%     \end{tabular}
%     \vspace{-0.3cm}
% \end{table}

% \newcommand{\graydelta}[1]{\textcolor{gray}{\footnotesize (#1)}}
\begin{table}[t!]
    \centering
    \vspace{-0.2cm}
    \caption{
        Detection and performance metrics across different levels of contamination for ARC-Easy, ARC-Challenge, and MMLU benchmarks, watermarked with $\delta=4$.
        The performance increase is shown for OOD evaluation as detailed in~\autoref{subsec:result_detection}. 
        The log$_{10}$ $\pval$ of the detection test is strongly correlated with the number of contaminations, as well as with the performance increase of the LLM on the benchmark.
        % Similar results for in distribution are shown in \autoref{tab:contamination_indist} of~\autoref{app:appendix} \pierre{not necessary in the fig.}
    }\label{tab:contamination}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{r rr@{\hspace{0.5em}}l rr@{\hspace{0.5em}}l rr@{\hspace{0.5em}}l}
        \toprule
        & \multicolumn{3}{c}{ARC-Easy (112k toks.)} & \multicolumn{3}{c}{ARC-Challenge (64k toks.)} & \multicolumn{3}{c}{MMLU$^*$ (325k toks.)} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        Contaminations & $\logpval$ & Acc. & \graydelta{\% $\Delta$} & $\logpval$ & Acc. & \graydelta{\% $\Delta$} & $\logpval$ & Acc.& \graydelta{\% $\Delta$} \\
        \midrule
        0  & -0.3 & 53.5 & \graydelta{+0.0} & -0.3 & 29.4 & \graydelta{+0.0} & -0.9 & 30.6 & \graydelta{+0.0} \\
        4  & -3.0 & 57.9 & \graydelta{+4.3} & -1.2 & 32.4 & \graydelta{+3.1} & -5.7 & 35.7 & \graydelta{+5.1} \\
        8  & -5.5 & 63.0 & \graydelta{+9.5} & -4.5 & 39.3 & \graydelta{+9.9} & \textless{-12} & 40.8 & \graydelta{+10.2} \\
        16 & \textless{-12} & 71.7 & \graydelta{+18.2} & \textless{-12} & 54.3 & \graydelta{+24.9} & \textless{-12} & 54.0 & \graydelta{+23.5} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.3cm}
\end{table}

\vspace{-0.2cm}
\subsection{Additional Results}\label{subsec:additional_results}


\paragraph{\textbf{Impact of window size.}}
\begin{wraptable}{r}{0.4\textwidth}
    \centering
    \vspace{-0.4cm}
    \caption{\small Proportion of green tokens in the predictions (see~\autoref{eq:def_S_N}), number of tokens scored after dedup and log$_{10}$ $\pval$s for different watermark window sizes, with 16 contaminations and $\delta=4$ on ARC-Easy.}
    \small % Reduce font size for the table
    \begin{tabular}{r r r r}
        \toprule
        $k$ & \multicolumn{1}{c}{$\rho$} & \multicolumn{1}{r}{Tokens} & \multicolumn{1}{r}{$\logpval$} \\
        \midrule
        0 & 0.53 & 5k & -6.07 \\
        1 & 0.53 & 28k & -25.89 \\
        2 & 0.53 & 47k & -38.69 \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
    \label{tab:window_size}
\end{wraptable}
Watermark insertion through rephrasing (\autoref{subsec:rephrasing}) depends on the watermark window size $k$. 
Each window creates a unique green-list/red-list split for the next token. 
Larger windows reduce repeated biases but are less robust.
Because of repetitions, \citet{sander2024watermarking} show that smaller windows can lead to bigger overfitting on token-level watermark biases, aiding radioactivity detection.
In our case, benchmark sizes are relatively small and deduplication limits the number of tokens tested, because each $\{$window + predicted token$\}$ is scored only once. 
Thus, smaller windows mean fewer tokens to score.
Moreoever, as shown in~\autoref{tab:window_size}, the proportion of predicted green tokens is not even larger for smaller windows: there is not enough repetitions for increased over-fitting on smaller windows.
The two factors combined result in lower confidence. 
A comparison of contamination detection across benchmarks and window sizes is shown in \autoref{fig:appendix_watermark_performance}, and for the utility of the benchmarks in~\autoref{fig:appendix_watermark_contamination}.
Overall, larger window size ($k=2$) yields better results.

\vspace{-0.1cm}
\paragraph{\textbf{Impact of benchmark size.}} The benchmark size can significantly affect the method's effectiveness.
With a fixed proportion of predicted green tokens, more evidence (\ie more scored tokens) increases test confidence. 
As shown in~\autoref{tab:contamination}, at a fixed level of cheating (\eg $+10\%$ on all benchmarks after $8$ contaminations), contamination detection confidence is proportional to benchmark size.
This is similar to our observations on watermark window sizes in~\autoref{tab:window_size}.
% So at fixed cheating level, it will be easier to detect contamination of bigger benchmarks.




% In classical watermarking, however, a larger watermark window means smaller robustness as changing one every $k$ tokens on average can break all the watermark.

% But in our case, we are going the do the radioactivity detection test on the dataset without any changes, but we may want more robustness if the suspect model tries to break the watermark before training on it.


% \paragraph{Impact of rephrasing model.}
% The difficulty of the questions, and their entropy, can have an important impact on the method.
% For instance, some math questions are hard to rephrase, and adding a watermark can further mess-up the meaning. 
% The method may thus require a stronger model for highly technical benchmarks (\eg Llama3-70B instead of Llama3-8B).
% Moreover, typically for math or code, the rephrasing inherently does not let a lot of entropy, as many invariants need to be respected.
% Possibilities would be to add watermarked verbose text around the math instead of rephrasing, and use as entropy-aware LLM watermarking~\citep{lee2023wrote}.
% We have tested rephrasing the benchmarks using Llama3-70B-Instruct instead of the 8B version. 
% We observe that we need to increase $\delta$ to $8$ in in order to obtain the same proportion of green tokens as with $\delta=2$ with the 8B model, while using the exact same decoding parameters.
% This can be because there is less entropy in the generation of the 70B or that the logits are for some reasons bigger, as the bias towards the greenlist is added before the softmax (see~\autoref{subsec:rephrasing}).
% However, we observe that some failure cases with the 8B (specifically for questions with important numbers) are correct with the 70B, but this is hard to quantify. 
% We give one example bellow in~\autoref{fig:example_answers_70B}.
\vspace{-0.1cm}
\paragraph{\textbf{Impact of rephrasing model.}}
The difficulty and entropy of questions can significantly affect the method's performance. 
Indeed, math questions for instance can be challenging to rephrase, even more with watermarks. 
Thus, better models may be needed for technical benchmarks.
We tested rephrasing with Llama3-70B-Instruct instead of the 8B version, and  observed that some 8B model failures, especially on math questions, are resolved with the 70B model, though quantifying this is challenging. 
An example is provided in~\autoref{fig:example_answers_70B}.
We note that increasing $\delta$ to 8 is necessary to match the green token proportion of $\delta=2$ with the 8B model, using the same decoding parameters.
This may result from lower entropy in generation or bigger logits, as the greenlist bias is applied before the softmax (see~\autoref{subsec:rephrasing}).
Moreover, in math or code, rephrasing can offer limited entropy, and even better models will not be enough.
An alternative would be to add watermarked verbose text \emph{around} the questions, or using entropy-aware LLM watermarking~\citep{lee2023wrote}.

\begin{figure}[b!]
    \vspace{-0.3cm}
    \centering
    \begin{tcolorbox}[colframe=metablue, colback=white]
        \footnotesize
        \textbf{Original question:} 
        An object accelerates at 3 meters per second$^2$ when a 10-newton (N) force is applied to it. Which force would cause this object to accelerate at 6 meters per second$^2$?
        \begin{minipage}{0.42\textwidth}
            \vspace{0.1cm}
            \textbf{Llama-3-8B-Instruct, $\delta=2$:} What additional force, applied in conjunction with the existing 10-N force, would cause the object to experience an acceleration of 6 meters per second$^2$? (70$\%$)
        \end{minipage}\hspace{0.04\textwidth}%
        \begin{minipage}{0.54\textwidth}
            \vspace{0.1cm}
            \textbf{Llama-3-70B-Instruct, $\delta=8$:} What force would be necessary to apply to the object in order to increase its acceleration to 6 meters per second$^2$, given that an acceleration of 3 meters per second$^2$is achieved with a 10-newton force? (65$\%$)
        \end{minipage}
    \end{tcolorbox}
    \vspace{-0.2cm}
    \caption{
    Watermarking failure on an ARC-Challenge question with an $8$B model, while the $70$B model succeeds.
    }
    
\label{fig:example_answers_70B}
\end{figure}



\begin{wrapfigure}{r}{0.5\textwidth}
  \centering
  \vspace{-0.5cm}
\includegraphics[width=0.48\textwidth]{figs/main/detection_vs_performance.pdf} % Replace with your image file
  \vspace{-0.25cm}
  \caption{Detection confidence as a function of performance increase on MMLU$^*$ for different model sizes and \#contaminations, for $\delta=4$ and OOD evaluation.}
  \vspace{-0.35cm}
\end{wrapfigure}\label{fig:model_size}
\paragraph{\textbf{Impact of model size.}}
We also test radioactivity detection on 135M and 360M transformer models using the architectures of~\href{https://github.com/huggingface/smollm}{\texttt{SmolLM}} and the same training pipeline as described in \autoref{subsec:result_detection}, training each model on 10B tokens as well. 
\autoref{fig:model_size} shows the detection confidence as a function of the cheat on MMLU$^*$.
We find that, for a fixed number of contaminations, smaller models show less performance increase -- expected as they memorize less -- and we obtain lower confidence in the contamination detection test. 
As detailed in~\autoref{subsec:rephrasing}, the $\pval$s indicate how well a model overfits the questions, hence the expected correlation. For a fixed performance gain on benchmarks, $\pval$s are consistent across models. For example, after $4$, $8$, and $16$ contaminations on the $1$B, $360$M, and $135$M parameter models respectively, all models show around $+6$\% gain, with detection tests yielding $\pval$s around $10^{-5}$.
Thus, while larger models require fewer contaminated batches to achieve the same gain on the benchmark, radioactivity effectively measures ``cheating''.





% \begin{wrapfigure}{r}{0.45\textwidth}
%   \centering
%   \vspace{-0.4cm}
%   \includegraphics[width=0.43\textwidth]{figs/main/arc-easy.pdf}
%   \vspace{-0.3cm}
%   \captionsetup{font=small}
%   \caption{Performance of Llama-3 models on different versions of the arc-easy benchmark.}
%   \vspace{-1cm}
%   \label{fig:impact-wm-arc-easy}
% \end{wrapfigure}

