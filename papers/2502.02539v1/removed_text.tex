% RQ2 ------------------------------------
\textbf{RQ2}: In context of RQ1,\\
\textbf{a)} \textit{What method/granularity of code summarization provides best results?} \\
\textbf{b)} \textit{What method of prompting provides best results?} \\

% -----------------------------------------
% llm
However, these tools and models are used primarily to generate code snippets, at the level of functions or classes. 

% -----------------------------------------
% repo selection
Our experiment workflow starts with the selection of repositories. Primarily, we focused on repositories implementing serverless architectures and those available in languages commonly used for serverless computing, such as JavaScript, Python, and TypeScript. We also prioritized repositories with a considerable number of stars and forks, indicating popularity and community engagement, as well as those featuring a comprehensive set of tests with good coverage. High test coverage is essential because it allows us to compare and evaluate the functions generated by large language models (LLMs) against existing, well-tested implementations.

AWSomePy \cite{giuseppe_awsomepy} and Wonderless \cite{eskandani2021wonderless} are two popular datasets for serverless applications, frequently referenced in research. AWSomePy consists of 145 AWS serverless applications implemented in Python. However, this dataset was limited by its low test coverage, which is essential for our evaluation approach. In contrast, the Wonderless dataset, comprising 1,877 real-world serverless applications extracted from GitHub, offered applications in various languages with a larger test base. 

We conducted our experiments using the Wonderless dataset, sorting the repositories by stars and forks to identify candidates with multiple functions and corresponding tests. By generating coverage reports for each repository, we further refined our selection, focusing on those with high test coverage. Based on these criteria, we selected four repositories for our experimental analysis, each meeting the minimum requirements for stars, forks, and test coverage.

% ----------------------------------------
% study
For the codebase summary-based context, we utilized a more comprehensive method. Given the target serverless function  that we aim to generate, we first masked the function to exclude it from the codebase. This allowed us to develop a detailed codebase summary, capturing the architectural structure and functionality, while excluding the target.  We use Gemini-1.5-Pro, as mentioned in Section \ref{study:llm-selection} to generate a detailed summary of the codebase. By extracting all other functions in the codebase and creating a prompt that includes all the extracted function paths and their corresponding code (shown in \ref{box:codebase_summary_prompt}), we obtained a structured representation of the repository's functionality. This setup provides a detailed context that allows us to evaluate the LLMâ€™s ability to accurately reconstruct the masked function based on the surrounding codebase information. We make the prompt for codebase summarization of architectural relevance, by using keywords such as "components", "connectors" and "relationships" \cite{garlan1993_component_connector}.
% Using Gemini-1.5-Pro, as mentioned in Section \ref{study:llm-selection}, we captured the architectural structure and functionality by extracting all other functions, their paths, and corresponding code (shown in \ref{box:codebase_summary_prompt}). 

% -----------------------------------------
% results
For RQ1.2, we assessed whether minor human adjustments described in Table \ref{table:llm_error_table} improves the generated serverless function's functionality. 

The results show a significant improvement in pass rates across all models and prompt types with minimal human intervention. DeepSeek-Coder-V2, which performed well with respect to RQ1.1, reached over 90\% pass rates at the repository level and 71\% at the function level with Prompt Type 3. Similarly, GPT-4 and GPT-3.5-Turbo displayed significant performance gains, achieving pass rates close to DeepSeek-Coder-V2. Even the lower-performing models, Artigenz-Coder-DS-6.7B, and CodeQwen1.5-7B-Chat, saw notable increases in pass rates, 
from 67\% to 78\% and 68\% to 80\% respectively at the repository level, though their results still lagged behind the top models.
\hfill\\