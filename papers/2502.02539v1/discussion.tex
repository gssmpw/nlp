In this section, we discuss lessons learnt from our study and threats to validity.

\subsection{Can LLMs generate functional serverless functions? ($RQ_1$)}
Yes, LLMs can be used to generate functional serverless functions with varying performance based on model and prompt type, but not entirely autonomously.

We observed that different levels of detail in the context provided to the LLM for serverless function generation significantly impact its performance. The Type 3 prompt produces the most functional serverless functions. This prompt type
containing comprehensive context along with description and function code of other serverless functions in the repository, gave the models a better understanding of the codebase structure and function interdependencies
enabled better understanding of codebase structure and interdependencies, which was particularly beneficial to the larger models - DeepSeek-Coder-V2, GPT-4, and GPT-3.5-Turbo. This underlines the importance of both model scale and context in generating functional serverless functions. Smaller models - Artigenz-Coder-DS-6.7B and CodeQwen1.5-7B-Chat, constrained by their size, tend to generalize more, reducing the precision needed for function generation.

DeepSeek-Coder-V2 demonstrated the best overall results, across multiple functions and prompt types without human intervention. GPT-4 and GPT-3.5-Turbo also showed strong performance, though GPT-4 surpassed GPT-3.5-Turbo in adhering to the function requirements. 

Interestingly, with respect to RQ1.2, involving minimal human intervention, the smaller models required substantial corrections, while the larger models needed relatively minor adjustments. However, in certain cases, the improvement in the functionality of the generated code was more pronounced for GPT-3.5-Turbo than GPT-4 which led GPT-3.5-Turbo to outperform GPT-4. Upon manual inspection of the generated functions, we found that GPT-3.5-Turbo tends to produce simpler initial outputs which, although often lacking full functionality, are generally easier to refine than GPT-4's more complex responses. GPT-4, while thorough in adhering closely to function requirements, occasionally produces code that is more challenging to adjust with minor changes.

As observed in \ref{sec:rq1-1}, JS-based repositories achieve higher pass rates, likely due to its prevalence in serverless functions. This insight can be used when selecting LLMs for architectural component generation.

Overall, while LLMs, especially the larger models, showcase the potential to generate functional serverless functions with minimal human intervention, there remains a gap between generated and fully functional human-written architectural components. This necessitates further research on leveraging LLMs to reach human-level proficiency in generating serverless functions autonomously, with an emphasis on maintaining a human in the loop to refine functionality, handle complex logic, and ensure practical applicability.

\begin{tcolorbox}[colback=orange!5!white, colframe=orange!95!white, colbacktitle=orange!95!white]
\small
    \textbf{Main Findings for RQ1}: LLMs can generate functional serverless functions with varying performance based on model and prompt type, but not to the extent to which humans can. Nonetheless, this method can be employed to aid architectural component generation.
\end{tcolorbox}

\subsection{How does the code quality of LLM-written serverless functions compare to human-written code? ($RQ_{2}$)}

LLM-generated serverless functions show a reduction in 
Source Lines of Code, Cyclomatic Complexity, and Halstead Volume 
% SLOC, CC and Halstead Volume
compared to human-written code. This suggests that LLMs produce concise and less complex code that is easier to understand and maintain. However, this simplicity can sometimes come at the expense of functionality, as demonstrated in Figure \ref{fig:examples}(g), where essential logic was omitted, impacting functional accuracy. The code generation models Artigenz-Coder-DS-6.7B and CodeQwen1.5-7B-Chat produced more SLOC and higher CC than general-purpose models like GPT-4 and GPT-3.5-Turbo, despite their smaller size. Among the models, larger ones, particularly DeepSeekCoder-V2, achieved the highest CodeBLEU scores, highlighting the importance of model size and prompt detail.

When comparing prompt types, Type 3 prompt, which provides detailed architectural and functional context, significantly reduced CC and CogC, leading to simpler, and more understandable code. It also produced high similarity between LLM-generated and human-written functions, as illustrated in Figures \ref{fig:examples}(a) and (d). Despite the improvements in complexity metrics, Type 3 prompts had minimal impact on SLOC and Halstead Volume, indicating that while context simplifies logic, it does not necessarily shorten code length. This observation highlights the nuanced influence of context in shaping code quality.

An interesting anomaly was observed with Type 2 prompts, which occasionally reduced CodeBLEU scores despite generating functions that passed more tests compared to Type 1. This suggests that functional accuracy doesnâ€™t always align with human-code similarity. 

Overall, while LLMs generate simpler and more understandable code, their limited ability to handle complex logic autonomously points to a trade-off between code quality and functionality, necessitating further refinement to achieve the desired functionality.

\begin{tcolorbox}[colback=orange!5!white, colframe=orange!95!white, colbacktitle=orange!95!white]
\small
    \textbf{Main Findings for RQ2}: LLMs can generate good quality serverless functions compared to human-written code. Larger models, combined with prompts that capture architectural and functional context, produce simpler, more understandable, and functionally accurate code, albeit with some trade-offs in complexity and functionality.
\end{tcolorbox}

\subsection{Defining Software Architecture in the Age of GenAI}
Over the years, numerous definitions of software architecture (SA) have been proposed over the years, including by Perry and Wolf \cite{perrywolf_architecture}, Garlan and Shaw \cite{garlan1993_component_connector} and Jansen and Bosch \cite{bosch_designdecisions}.
Refer \cite{What_is_your_definition_of_software_architecture_2010} for more definitions.
When one considers component generation as a black-box, Jansen and Bosch's definition of SA as a set of design decisions \cite{bosch_designdecisions} is more pertinent, since the architect only makes design decisions that are developed automatically.
However, when moving deeper, during generation, it is necessary to consider the \textit{components} that already exist in the system, how they are \textit{connected} and what \textit{constraints} exist, which is Garlan and Shaw's definition\cite{garlan1993_component_connector}. As we move into an age where GenAI is increasingly used in SA, it is imperative to be able to move between these definitions depending on the level of abstraction used.
The need of the hour is accurate and detailed documentation of these design decisions to enable LLMs to generate the components and connectors subject to the constraints defined in the design decisions. These design decisions can be complemented by organizational context, and all of this can in turn benefit from the usage of GenAI, such as for documenting Architectural Decision Records (ADRs) as proposed by Dhar et al.\cite{dhar_adr}.

\subsection{The Path Forward for GenAI in SA} \label{path-forward}
Perhaps the biggest challenge we faced in conducting this study was finding high quality data. Despite using published datasets, we found the quality of tests and components to be mostly unsatisfactory. Machine learning models need voluminous high quality data \cite{jain_data_quality_ml}, and while this data exists to an extent for language modeling and code snippet generation, it is much more scarce when considering architectural component generation. There currently do not exist architecture specific tasks and benchmarks to evaluate LLMs, probably due to the aforementioned data scarcity. \\
The next issue that needs to be solved is the specific approach to using LLMs for generating architectural components. Various methods like Chain-of-thought prompting \cite{wei2022chainofthought}, retrieval-augmented-generation \cite{lewis2020rag}, knowledge graphs \cite{pan2024llmkg} \cite{abdelaziz2021codekg}, and agentic frameworks \cite{huang2023agentcoder, zhuge2024agentasajudge} exist, but their effectiveness for software architecture specific tasks have not been explored. LLMs could also be seen as a solution to address the learning curve of ADLs/DSLs where natural language could be converted to an architectural model and further code could be generated using model transformations. However, addressing these problems requires deeper collaboration between the SA and NLP communities. 

\subsection{Threats to Validity}
\input{threats}