\subsection{Goal}
This study aims to evaluate the degree to which LLMs are able to generate software architecture components. The degree here refers to both the functional correctness and quality of code. Using the Goal-Question-Metric approach described in \cite{caldiera1994goal}, we formalize our goal to:
\\
\textbf{analyze} the effectiveness of LLMs
\textbf{for the purpose of} generating software architecture components
\textbf{with respect to} automatic software architectural component generation
\textbf{from the viewpoint of} software architects and developers
\textbf{in the context of} the Function-as-a-Service (FaaS) architectural style

\subsection{Research Questions} \label{rqs}
\textbf{RQ$_{1}$}: \textit{Can LLMs generate functional serverless functions?} 
\begin{itemize}
    \item \textbf{RQ$_{1.1}$}: \textit{Can LLMs generate functional serverless functions without human intervention?} 
    \item \textbf{RQ$_{1.2}$}: \textit{Can LLMs generate functional serverless functions with minimal human intervention?}
\end{itemize} 
In this RQ, we aim to evaluate whether the architectural components generated by LLMs are usable, to the extent that they satisfy tests defined in their containing repositories. To this end, we measure the outcome both when we directly use the generated serverless function, and after fixing minor errors described in Table \ref{table:llm_error_table}.

\textbf{RQ$_{2}$}: \textit{How does the code quality of LLM-written serverless functions compare to human-written code?} \\
In this RQ, we move beyond functionality to evaluate the code quality of the architectural component generated by the LLM. We are also interested in measuring how similar generated serverless functions are to their human written counterparts.



% ------------------
% ------------------

\subsection{Experiment Workflow}
\begin{figure*}[htb!]
    \centering
    \includegraphics[width=0.85\textwidth]{media/WorkFlow_Diagram_2.png}
    \caption{Study Design}
    \label{fig:study-design}
\vspace{-13pt}
\end{figure*}

In this subsection, we first explain Function Masking, an adaptation of Masked Language Modeling from \cite{kenton2019bert}, which we use to generate architectural components. Then, we describe each stage of the workflow shown in Figure \ref{fig:study-design}.
\subsubsection{Function Masking}
BERT \cite{kenton2019bert} was trained on the Masked Language Modeling (MLM) task, where tokens were randomly masked from the input, with the objective to predict the masked token based on its context (both to the left and right). This task is useful and allows for self-supervised learning \cite{liu_selfsupervised}. For example, in the sentence "The tall boy goes to the market", if "boy" is chosen to be masked, the model sees the input "The tall [MASK] goes to the market", where "[MASK]" is the special mask token, and must output "boy". We extend this concept to the serverless world through Function Masking. Given a serverless repository with multiple serverless functions, we mask one function and use the LLM to recreate the masked function, using different types and amounts of context, as explained in \ref{study:context}. 
% ------------------

\subsubsection{LLM Selection} \label{study:llm-selection}
An extensive number of LLMs are available for use, and we select some for our study using leaderboards published in literature. We require two LLMs, one which will be used for summarizing the existing codebase, as described in \ref{study:context}, and one for generating the new serverless function, as described in \ref{study:generation}. The reason for choosing two distinct models is two-fold: \\
    Primarily, the tasks these two models will be performing are different. The summarization model should be able to create an abstractive summary from the code given to it as context along with capturing the architectural information. This necessitates that it has a long context length, so that the input prompt can accommodate all the code needed to create the summary. Since a dedicated benchmark does not exist for this task, we use the ChatBot Arena\footnote{Leaderboard available at \url{https://lmarena.ai/}} \cite{chiang2024chatbotarenaopenplatform}, which evaluates human preference. \\ 
    However, the code generation model performs a coding task for which there are existing benchmarks (such as HumanEval \cite{chen2021evaluating} and MBPP \cite{austin2021mbpp}) and fine-tuned models (such as DeepSeekCoder, CodeQwen). We select models from the top 10 of the EvalPlus leaderboard\footnote{Leaderboard available at \url{https://evalplus.github.io/leaderboard.html}} \cite{liu2024code_evalplus}, while ensuring diversity across size and availability. Both EvalPlus and Chat-Bot Arena are evolving leaderboards, and the models selected were in the top 10 at the time of conducting the study.   \\
    Secondly, using the same model may propagate biases, such as which part of the codebase is important. This avoids the possibility of the model creating a summary that can only be fully deciphered by itself, and not by other models and humans.

We selected Gemini-1.5-Pro\cite{team2024gemini} for codebase summarization.
The models selected for code generation are described in Table \ref{tab:model-info} below. Refer \ref{backg:llm} for an explanation on "Number of Parameters" and "Context Window Size". "Availability" indicates whether the model can be hosted on-premise (Local) or if an API call to an externally managed server is needed (API). For those that offer both, we highlight the method we used for the study in bold. "License Type" refers to whether the model weights are publicly available (Open) or not (Proprietary). 
\input{llm_table}

% ------------------

\subsubsection{Repository Selection}
To select open-source repositories using the serverless architectural style, we utilize the Wonderless \cite{eskandani2021wonderless} and AWSomePy \cite{giuseppe_awsomepy} datasets. The Wonderless dataset is a collection of 1,877 serverless applications from GitHub in various languages. 72.2\% of the repositories included are in JavaScript or TypeScript, 19\% in Python, and 2.7\% in Java. The AWSomePy dataset consists of 145 AWS Lambda based serverless functions written in Python. We aim to choose at least one repository in JavaScript, TypeScript and Python each as they make up almost 70\% of runtimes used on AWS Lambda \footnote{\url{https://www.datadoghq.com/state-of-serverless/}}. In our selection, we also seek to ensure that the repository is not a demo or toy application, by filtering based on number of GitHub stars and manual inspection. Finally, we desire repositories that have tests defined with high test coverage to perform evaluations related to $RQ_{1}$, as described in \ref{study:eval}. 

First, we sort the datasets based on GitHub stars and forks in decreasing order, to prioritize popular real world repositories. We then look for repositories that contain tests. As a first step heuristic, for every repository, we look for files or folders with "test" as a substring of their name. Then, for repositories with $\geq 30$ stars on GitHub, we manually evaluate the quantity and quality of tests present. We found that some repositories had test files that were empty or too trivial to justify their selection. Additionally, several repositories contained serverless functions as only a small part of the entire codebase, usually for testing, and did not have tests for them. We also reject repositories that have been archived, since they may be deprecated or no longer relevant or maintained. For those repositories that had an acceptable number and quality of testcases, we calculated the test coverage and results. Based on these results, we selected four repositories.
We describe the selected repositories in Table \ref{tab:repo-info} below. All the repositories selected were from the Wonderless dataset.
\input{repo_table}
% \vspace{-0.5cm}
% ------------------

\subsubsection{Function Selection}
For each repository, we selected up to 3 functions to mask so that they can be generated by the LLM, enabling us to assess the model's performance across varying levels of complexity, length, and structure. We desire functions that have tests associated with them to enable the evaluation of the generated code. To this end, for each selected repository, we calculated the test coverage using the testing framework used in the repository. We then selected the functions with the highest statement coverage, using source lines of code as a tiebreaker. We finally select 10 serverless functions for masking across the 4 repositories.
% ------------------

\subsubsection{Context Selection} \label{study:context}
In our study, we experimented with different levels of detail in the context provided to the LLM for code generation, focusing on two primary sources: the README file and a summary of the codebase which includes the architectural information.

For the README-based context, we used the repository’s README file, which typically offers a concise overview, including a brief description, functionalities, and usage instructions. 
However, since README files often lack implementation details, this approach evaluated the LLM’s ability to generate code with minimal context.

For the codebase summary-based context, we masked the target serverless function to generate a detailed codebase summary that excluded it. 
We utilized Gemini-1.5-Pro (mentioned in \ref{study:llm-selection}) to extract the architectural structure and functionality by identifying all other functions, their paths, and corresponding code (shown in \ref{box:codebase_summary_prompt}).
This structured summary, enhanced with keywords like "components," "connectors," and "relationships", provided context for evaluating the LLM’s ability to reconstruct the masked function accurately.

\input{codebase_summarization_prompt}
\vspace*{-10pt}

Additionally, masking a function involves generating its function description (shown in \ref{box:func_desc_prompt}), which is later incorporated in the prompt for the LLM to generate the function from its description. We used Gemini-1.5-Pro to produce this function description from the function code. The full prompts can be viewed in our replication package.

\input{func_desc_prompt}
\vspace*{-10pt}

% ------------------
\subsubsection{Generation Methods} \label{study:generation}
We follow multiple generation methods through different prompts\\
\textbf{Prompt Types}:
From the two kinds of context described above, we create three prompts for function generation:
\begin{enumerate}
    \item \textit{Zero Shot with README} (Type 1 Prompt): As shown in \ref{box:type1}, it contains no examples of other serverless functions and their descriptions and only provides the README file of the repository as context. The description of the masked function is provided and the model is tasked with generating the code for the serverless function.
    \input{prompt_type_1}
    \vspace{-5pt}
    \item \textit{Zero Shot with Codebase Summarization} (Type 2 Prompt): As shown in \ref{box:type2}, it also contains no examples of other functions and descriptions, but includes the architectural information through the summary of the codebase, along with the description of the masked serverless function.
    \input{prompt_type_2}

    \item \textit{Few Shot with Codebase Summarization} (Type 3 Prompt): Along with the architectural information through the summary of the codebase, this prompt (shown in \ref{box:type3}) also contains descriptions and function code of other serverless functions in the repository. These serve as guides for the model to generate the code for the masked function from the given description. This few-shot prompt allows the LLM to learn in-context without modifying model parameters, as described in Section \ref{backg:llm_prompting}.
    \input{prompt_type_3}
\end{enumerate}
The full prompts can be found in our \href{https://doi.org/10.5281/zenodo.14539782}{replication package}. When making the prompts, we made an effort to be as detailed and specific as possible.

% -- consistency check
\noindent
\textbf{Consistency Check}: LLMs are probabilistic, and can produce different outputs even with the same prompt. In the context of our work, this can result in the generated code passing tests and being of good quality in one run, but not in the other. To alleviate this issue, we verify if multiple generations using the same context generate similar code. We compare code similarity using CodeBLEU \cite{ren2020codebleu}, which is a weighted combination of n-gram matches\footnote{\scriptsize an n-gram match is $n$ tokens matching in order between the input and the output}, syntactic matches in the Abstract Syntax Trees (ASTs) and semantic data-flow matches. The results 
are shown in Figure \ref{fig:codebleu-consistency}. We see that generated functions are quite similar to each other for a model, and conclusions drawn by evaluating one function generated by a model will hold even when the model is given multiple tries.
We finally generate 145 serverless functions for evaluation.   


% --End consistency check



% ------------------

\subsubsection{Evaluation Metrics} \label{study:eval}
We perform three kinds of evaluations on the LLM generated serverless functions:\\
\textit{Functional Correctness Through Testing}: To address $RQ_{1}$, we evaluated both the original and generated code using the existing tests in each repository. Specifically, we recorded the number of passing and failing tests for the entire codebase as well as for the individual functions selected for generation. After generating each function, we re-ran the tests and recorded the updated counts of passing and failing cases.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{media/consistency_codebleu_scores.png}
    \vspace{-9pt}
    \caption{Avg. Pairwise CodeBLEU Scores for Generated Functions per Model}
    \label{fig:codebleu-consistency}
    \vspace{-15pt}
\end{figure}

To address $RQ_{1.1}$, we initially conducted this evaluation without any code modifications. Here, we used the generated response of the LLM, cleaned to include only the code. Although this process was done manually, it can be automated by extracting only the text within triple backticks (\texttt{\textasciigrave \textasciigrave \textasciigrave}), as the model was instructed to provide code in this format. However, manual cleaning was necessary for cases where the model did not follow the formatting instructions.

To address $RQ_{1.2}$, we identified and fixed code generation errors made by LLMs. Song et al.\cite{song2023empirical} study the errors in LLM-generated code and create a taxonomy of observed errors. We filter this to only include those errors that were frequently observed and solvable with minimal human intervention, shown in Table \ref{table:llm_error_table}.
On average, we spent 15 minutes per function fixing these errors. Following these corrections, we recorded the updated counts of passing and failing tests, both for the overall codebase and individual functions.
% After correcting one or more of these issues, we recorded the updated counts of passing and failing tests for the overall codebase and individual functions.


\input{llm_error_table}

\textit{Code Quality through Code Metrics}: Though the component generated may be functional, in the sense that it passes tests, we also desire code that is of good quality, to ensure quality attributes such as maintainability, readability and extensibility. In addressing $RQ_{2}$, we quantify code quality using code level metrics.
Our choice of metrics encompasses those that measure size (through Source Lines of Code (SLOC) and Halstead's Measure), testing difficulty (McCabe's Complexity) and reading difficulty (cognitive complexity). 
Our choice for metrics is guided by previous work by Jin et al. \cite{jin_metrics}, who analyze Java, JavaScript, and Python repositories. The popular Chidamber \& Kemerer object-oriented metrics \cite{chidamber1994metrics} are not applicable, since serverless functions written in the languages studied in this work need not be object-oriented. This is further supported by Jin et al. \cite{jin_metrics}, who also avoid using OO metrics in similar contexts. We calculate the following metrics for the original and generated serverless functions: 
\begin{enumerate}
    \item \textit{Source Lines of Code (SLOC)} quantifies the size of a program by counting the number of lines which contain source code.
    \item \textit{Halstead's Software Science Metric (Volume)} \cite{halstead_measure} is another measure to quantify the size of a program which considers programs as a collection of tokens (operators and operands).
    \item \textit{McCabe's Cyclomatic Complexity (CC)} \cite{mccabe1976complexity} is used to measure the complexity of a program's control flow. It provides insight into the testability of the program, since one with a high cyclomatic complexity has more paths that need to be tested.
    \item \textit{Cognitive Complexity (CogC)} \cite{campbell2018cognitivecomplexity} is a metric designed to measure the understandability of code.
\end{enumerate}

\textit{Code Similarity using CodeBLEU}: In answering $RQ_{2}$, we also measure how syntactically similar LLM generated serverless functions are to human written ones through the CodeBLEU \cite{ren2020codebleu} metric, which is described in \ref{study:generation}. A high CodeBLEU score indicates that the human-written and generated code are very similar, and the models may have appropriately created the summary and generated function. Though CodeBLEU measures semantic data-flow similarity, we utilize it to compare the syntactic similarity of serverless functions and use testing to evaluate semantic correctness. This is because the generated function may not pass tests despite a high CodeBLEU score with the original function due to subtle differences in the code that affects functionality or creates errors but does not significantly reduce CodeBLEU, such as incorrect imports or exchanged variables. \\
% ------------------
% ------------------