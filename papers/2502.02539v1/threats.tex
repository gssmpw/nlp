We follow the categorization provided by Wohlin et al. \cite{wohlin2012experimentation} and also provide brief explanation about efforts taken to mitigate the identified threats or why it is not possible, as suggested by Verdecchia et al. \cite{verdecchia_threats}. \\
% \vspace{-1.2mm}
% \begin{itemize}
    \textbf{External Validity}: Selection of LLMs used and their generation parameters (such as temperature) for creating the context in the form of the codebase summary and for generating the serverless function poses a threat to external validity. However, we systematically select diverse LLMs using peer-reviewed published leaderboards, namely ChatBot-Arena \cite{chiang2024chatbotarenaopenplatform} and EvalPlus \cite{liu2024code_evalplus}, with EvalPlus specifically evaluating the coding abilities of LLMs. 
    An interesting threat to external validity can be contamination of LLMs \cite{dong2024contamination}, which is when LLMs are trained or fine-tuned on data that is later used to evaluate their performance, leading to unrealistically high scores. Since most LLMs are not open about the data they are trained on, we have no way to mitigate this issue, especially since the repositories listed in Wonderless and AWSomePy were released in the public domain on which LLMs could have been trained. Despite this possibility, as shown in the results in Section \ref{results}, LLMs were not able to generate completely functional serverless functions. \\
    \textbf{Internal Validity}: 
    The effectiveness of the tests in the repositories selected can be a threat to internal validity. However, we manually selected repositories which had high test coverage to mitigate this. 
    Selection of metrics presents another threat, since both code quality and similarity are quite abstract concepts. To address this, we used metrics commonly used in the SE and NLP communities for measuring the same. \\
    \textbf{Construct Validity}: 
    A threat to construct validity stems from the selection of repositories and serverless functions in them. However, we utilize published datasets - Wonderless \cite{eskandani2021wonderless} and AWSomePy \cite{giuseppe_awsomepy}. We filter them on the number of stars, quantity and quality of tests and test coverage to avoid picking demo projects and retain repositories and serverless functions that have real-world relevance. 
% \end{itemize}