This study, whose code and data we make publicly available, \footnote{Code and data available at: \url{https://doi.org/10.5281/zenodo.14539782}} seeks to empirically explore the capabilities of Large Language Models to automatically generate software architectural components. We do this in context of serverless functions which is an event-driven architectural stlye, due to the small size of their basic architectural unit. Using a range of models, including general-purpose models like GPT-4 and code generation models like DeepSeekCoder-V2, and diverse prompt types, we evaluate the generated architectural components for both functional correctness and code quality. We find that while LLMs often fail to generate fully functional serverless functions autonomously, they can serve as a valuable starting point, as minimal human intervention significantly improves their functionality, enabling them to pass more tests. We also observe that LLM  generated serverless functions display better metrics related to code quality, such as cyclomatic complexity, and cognitive complexity. Furthermore, in its current state, one cannot, and probably should not completely remove the human from the component generation process, suggesting a human-centered GenAI approach, as put forth by the Copenhagen Manifesto \cite{copenhagen_manifesto}.\\
Future work involves exploring other techniques for generation used at the code level, such as those mentioned in \ref{path-forward}, including exploring Retrieval Augmented Generation, multi-agent frameworks, and other architectural styles such as microservices and monoliths with larger components.