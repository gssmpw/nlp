This section provides an overview of the concepts dealt with in this study, including serverless functions, LLMs for general use and for code generation and methods to improve performance of an LLM on a task.

\vspace*{-2.5mm}

\subsection{Function-as-a-Service (FaaS) (Serverless Functions)} \label{backg:faas}

Function-as-a-Service (FaaS) is one of the most popular forms of the cloud computing paradigm called serverless computing\cite{wen2023rise_serverlessslr}. It is widely adopted across domains due to its nature of being \textit{ephemeral}, \textit{event driven}, and \textit{elastic}. Unlike traditional cloud computing paradigms, in FaaS, developers only write business logic in the form of fine-grained \textit{functions}, which are deployed onto a cloud platform, where the cloud provider handles the hassle of managing and maintaining infrastructure, execution environment, which is abstracted away from the developer. FaaS functions are \textit{event driven} as they only run when triggered by an event, such as a HTTP request, an update in a database, or a message arriving on a message queue. They are \textit{ephemeral} since they only run for a short amount of time, after which they are descheduled by the cloud provider. They are also \textit{elastic}, since they can scale automatically based on load. This also leads to potential cost savings by charging only for the time functions run. These properties and the multitude of languages supported makes FaaS applications quite easy to develop. Through slight abuse of notation, we refer to serverless functions and functions interchangeably in the remainder of this paper.

\subsection{Large Language Models and Code Generation}\label{backg:llm}
Large Language Models (LLMs) are probabilistic machine learning models built on the Transformer architecture \cite{vaswani2017attention} that can mimic human language use. They are composed of billions of parameters and are trained on substantial content sourced from the internet. Though they can be trained on and used for other tasks, paradigmatic LLMs are \textit{generative auto-regressive models}, meaning that they sequentially generate the next token in a sequence of tokens. Tokens are the units into which text is broken down when processed and generated by LLMs. They may correspond to an entire word or to a part of a word \cite{sennrich-etal-2016-bpe}. The maximum number of tokens that an LLM can process is called its \textit{context length} or \textit{context window}, and this limits the amount of information that can be provided in the input \textit{prompt} by the user.  

% The original Transformer architecture, made for machine translation, consists of an Encoder stack and a Decoder stack. At a high level, the Encoder is responsible for representing the input text, and the Decoder is responsible for generating output text. However, not all LLMs need to use both the Encoder and Decoder. BERT is an Encoder-only LLM, which is used for creating contextual representations of input text for use in downstream tasks such as sentiment classification. Auto-regressive generational models have shifted from using Encoder-Decoder architectures (as in T5) to a Decoder-only architecture (as in the GPT, Claude, and Llama family of models). LLMs have been applied in several stages of the Software Development Lifecycle (SDLC), including \magenta{citations} and in development for code generation.\\
The coding ability of LLMs was improved by the creation of datasets like HumanEval \cite{chen2021evaluating} and Mostly Basic Python Problems (MBPP) \cite{austin2021mbpp} on which LLMs have been \textit{fine-tuned}. Fine-tuning involves adapting a pre-trained model to perform better on a specific task.
% OpenAI's Codex \cite{chen2021evaluating} introduced the HumanEval dataset and demonstrates the improvement in coding ability (with respect to number of passing tests) when an LLM is fine-tuned on publicly available GitHub code. 
% A multitude of models, tools, benchmarks and leaderboards have emerged. 
% Large general-purpose LLMs such as ChatGPT and Llama and fine-tuned models such as DeepSeekCoder, CodeQwen and AutoCodeRover show impressive performance on the aforementioned benchmarks. 
% Today, there exists several tools that integrate these models to offer a pair-programmer-like experience, such as GitHub Copilot, Claude Workbench and the Cursor Integrated Development Environment (IDE). 

\subsection{Zero-Shot Prompting and Few-Shot Prompting}\label{backg:llm_prompting}
Unlike fine-tuning, zero-shot and few-shot prompting do not require 
training of the language model or
a dataset to fine-tune on. Zero-shot prompting involves directly making a model perform a task without examples, whereas few-shot prompting provides some examples in context. Few-shot prompting is also referred to as in-context learning, as it improves the performance of the model on a task without requiring updates to the model's parameters \cite{jurafsky_martin}. This significantly improves model performance on the task, especially for larger models, as shown by \cite{brown2020language}. In our work, an instance of zero-shot prompting is to simply provide context (the content of which is discussed in \ref{study:generation}) and request a serverless function to be generated. For few-shot prompts, we additionally provide some serverless functions from the repository along with their description to the LLM and ask for the function corresponding to a description whose code is not specified.
\vspace*{-1mm}