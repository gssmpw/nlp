\input{test_results_table}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{media/Results.png}
    \caption{Snippets from Functions: (a) Original, (b-d) DeepSeek-Coder-V2 with Type 1-3 prompts, (e-g) Artigenz-Coder-DS-6.7B with Type 1-3 prompts.}
    \label{fig:examples}
    \vspace{-15pt}
\end{figure}

% \subsection{$RQ_1$}
\label{sec:rq1-1}
\textbf{RQ}$_{1.1}$: Can LLMs generate functional serverless functions without human intervention?

% \hfill\\
Table \ref{table:RQ1-table} summarizes the percentage of test cases passed for the entire codebase and individual serverless functions, comparing original and LLM-generated functions across all models and prompt types. Type 3 prompts achieved the highest average test passing rates,  with 73\% of tests passed for the entire codebase. Comparing Type 1 prompt and Type 2 prompt for serverless function generation, the latter resulted in a higher test passing rates. 

Figures \ref{fig:examples}(b-d) and {}(e-f) provide examples illustrating how the prompt types impact the generated function's structure and functionality. In Figure \ref{fig:examples}(b), generated by DeepSeek-Coder-V2 with the type 1 prompt, \texttt{findTag()} and \texttt{getStackJanitorStatus()} rely on hard-coded strings (e.g., \texttt{"stackjanitor"} and \texttt{"enabled"}). This approach can reduce flexibility and misalign the function with actual codebase expectations. In contrast, Figure \ref{fig:examples}(c), generated by the same model with the type 2 prompt, avoids hard-coded strings by referencing a variable, showing an improvement. However, this version still lacks complete logic. Figure \ref{fig:examples}(d) shows a further improvement from the type 3 prompt, where \texttt{findTag()} is defined to return the tag object itself, which aligns closer to the original serverless function (in Figure \ref{fig:examples}(a)) and demonstrates the benefit of more detailed context.

Table \ref{table:RQ1-table} reveals that larger models namely DeepSeek-Coder-V2, GPT-3.5-Turbo, and GPT-4 significantly outperformed smaller models (CodeQwen1.5-7B-Chat and Artigenz-Coder-DS-6.7B), regardless of prompt type. Notably, DeepSeek-Coder-V2 achieved the highest performance without human intervention, passing over 81\% of codebase tests and 13\% of individual function tests with type 3 prompt. GPT-4 followed with 79   \% and 10\% test passing rates, respectively. In contrast, smaller models such as Artigenz-Coder-DS-6.7B and CodeQwen1.5-7B-Chat showed significantly lower passing rates. Figure \ref{fig:examples}(d) generated by DeepSeek-Coder-V2, and Figure \ref{fig:examples}(g) generated by Artigenz-Coder-DS-6.7B with the type 3 prompt, illustrate this difference - the former provides an accurate response in line with the original function, while the latter produces only a stub.  

The choice of language also influenced the functionality, with JavaScript-based repositories achieved higher pass rates in comparison to the other languages.  

\hfill\\
\textbf{RQ}$_{1.2}$: Can LLMs generate functional serverless functions with minimal human intervention?

The results after applying minimal human intervention to the generated functions, as shown in Table \ref{table:RQ1-table}, demonstrate a significant increase in test passing rates across all models and prompt types. DeepSeek-Coder-V2, which performed well with respect to RQ1.1, achieved over 90\% success on codebase tests and 71\% on individual function tests when using the Type 3 prompt. GPT-4 and GPT-3.5-Turbo also showed substantial performance improvements, with pass rates close to DeepSeek-Coder-V2. Even the lower-performing models, Artigenz-Coder-DS-6.7B and CodeQwen1.5-7B-Chat, saw marked  improvements: their codebase test pass rates increased from 67\% to 74\% and 69\% to 80\%, respectively. However, their results still fell short of the larger models. \\
For instance, as illustrated in Figure \ref{fig:examples}(d) — an output generated by DeepSeek-Coder-V2 — correcting a return error based on the guidance in Table \ref{table:llm_error_table}, specifically by adjusting \texttt{findTag()} to return the entire tag object instead of just the tag value, improved the functionality of the generated code. Conversely, Figure \ref{fig:examples}(g) shows an output generated by Artigenz-Coder-DS-6.7B with the Type 3 prompt, which is a stub to benefit from minor corrections, highlighting the limitations of smaller models in producing functional code even with minimal human intervention.
% \vspace{-8pt}

\textbf{RQ}$_2$: How does the code quality of LLM-written serverless functions compare to human-written code?
From Figure \ref{fig:metrics-graphs}, we see an overall reduction in the values of all code metrics except for some outliers in CogC for LLM generated serverless functions. For average SLOC, we see a reduction for all models and prompt types. Interestingly, we see that coding specific models generate more SLOC than the general purpose models, despite Artigenz-Coder-DS-6.7B and CodeQwen1.5-7B-Chat being much smaller than GPT-3.5-Turbo and GPT-4. Average CC of the functions generated by the models is also lower than the original, with coding specific models again demonstrating higher values despite smaller size. We observe that CodeQwen-1.5-7B-Chat and GPT-4 generated functions with higher CogC than the original functions, while Artigenz-Coder-DS-6.7B, DeepSeekCoder-V2 and GPT-3.5-Turbo have lower values than the original functions, with GPT-3.5-Turbo generating the most understandable code. Halstead volume of generated functions were much lower than the original functions across all models.  

Regarding the difference in code quality that the different amount of context provided in the prompt to the models make, we find no appreciable difference in average SLOC and average Halstead Volume. However, we see a marked reduction in average CC and average CogC when more context is provided to the model, with the lowest values demonstrated by the Few Shot Prompt with Codebase Summarization. 

On comparing the similarity of human-written and LLM-generated functions using CodeBLEU score, we see from Figure \ref{fig:codebleu-models} that Type 3 prompts make all models generate functions that are much more similar to the human written functions. We also see a correlation where larger models generate code more similar to the originals, with DeepSeek-Coder-V2 having the highest similarity score. Interestingly, in many cases, providing the codebase summarization in Type 2 prompt reduced CodeBLEU score compared to using the Type 1 prompt, though the former produced functions that passed more tests.

\newtcolorbox{mybox}{colback=orange!5!white,colframe=orange!75!black}
\newtcolorbox{myboxtitle}[1]{colback=orange!5!white,colframe=orange!75!black,fonttitle=\bfseries,title=#1}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{media/CodeBLEU_Scores.png}
    \vspace{-5pt}
    \caption{Avg. CodeBLEU Scores for Generated Functions per Model and Prompt Type}
    \label{fig:codebleu-models}
    \vspace{-20pt}
\end{figure}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{media/original_vs_generated_Code_Metrics.png}
    \caption{Code Quality Metrics of Original and Generated Functions per Model (first row), per Prompt Type (second row)}
    \label{fig:metrics-graphs}
    \vspace{-10pt}
\end{figure*}


