\section{Preliminary}
LLMs are transformer-based neural networks designed to model sequential data and predict the next token in an input sequence~\citep{vaswani2017attention}. In this work, we focus on LLMs specifically built for generating text sequences~\citep{brown2020language, achiam2023gpt, touvron2023llama}. 


Let $f: x \rightarrow y$ denote an LLM, where $x$ is a sequence of input tokens and $y$ is the output token. Since our work focuses on sequences of tokens rather than individual tokens, we extend $f$ to include the autoregressive nature of LLMs, where $y$ denotes a sequence of tokens generated iteratively.

In the context of jailbreaking, the goal is to design a malicious prompt $x_t$ so that the model generates a harmful or violent response $y_t = f(x_t)$. The LLM $f$ is referred to as the \textit{target} or \textit{victim} model in this setting. Since manually evaluating the generated responses is costly, an auxiliary judge LLM is used to automate the evaluation process~\citep{perez2022red, ganguli2022red}, where its output represents the safety evaluation result.

To facilitate LLM safety and alignment research, datasets of malicious target prompts have been introduced such as AdvBench~\citep{zou2023universal} and Harmbench~\citep{mazeika2024harmbench}. These datasets include simple malicious instructions that LLMs, equipped with safety alignment techniques, would typically refuse to respond to~\citep{ouyang2022training,bai2022training}. Current jailbreaking research focuses on methods to modify these prompts to increase the rate of unsafe responses~\citep{zou2023universal, wei2024jailbroken}.

In MSJ, the malicious target prompt is modified by prepending a fabricated conversation history:
\begin{equation}\label{eq:msj}
    x'_t= \seq{d_1, \dotsc, d_n, x_{t}},
\end{equation}
where $d_1, \dotsc, d_n$ are $n$ pairs of malicious questions and corresponding answers, denoted as $d = \seq{q, a}$. Notably, $x'_t$ is contained within a single input prompt to the LLM, leveraging its long-context capabilities. In this paper, we use the notation $\seq{\cdot}$ to denote the concatenation of prompts.

Next, we present three techniques to improve MSJ. The first two techniques focus on modifying the demonstrations to reinforce the instruction-following pattern established by the fabricated conversations. The third technique explores how to optimally sample these demonstrations.

% 
% We first define $V = \set{1, \dotsc, N_{\text{V}}}$ as a finite vocabulary set, where each element represents a sub-word, commonly referred to as a token.
% % 
% These sequences of tokens, $x$ and $y$, are also collectively known as prompts.
% 
% Throughout this paper, we use the notation $[\cdot]$ to denote token concatenation. be used for concatenation of tokens, i.e., $\seq{\vx, \vy} = \seq{x_1, \dotsc, x_{\ell_{x}}, y_1, \dotsc, y_{\ell_{y}}}$. 

% Under this definition, the core inference procedures of LLMs—token encoding, attention modeling, and token decoding—are all encapsulated within $f$, allowing us to focus on the effect of $\vx$ on $\vy$.
% % 
% Also, this formulation aligns with the setups in later sections, where we modify only the input tokens while preserving the default tokenizer configurations provided in the respective model codebases.
% 