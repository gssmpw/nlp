\section{Introduction}
\input{figure/plot_teaser}
The growing length of context windows in large language models (LLMs) unlocks applications, such as agentic LLMs~\citep{park2023generative}, that were previously impractical or severely limited~\citep{team2024gemini, ding2024longrope, jin2024llm, wu2024never, dong2024exploring}. 
% Recently, there has been rising interest in agentic LLMs \citep{park2023generative}, which rely on processing extended input sequences to reason, plan, and react. These models must retain a memory of an agent’s long-term behavior, making large context windows essential for their functionality.

However, this same long-context capability can be exploited by adversaries. \citet{anil2024many} demonstrate that a malicious prompt, which safety-aligned LLMs would typically refuse to respond to~\citep{bai2022training, ouyang2022training}, can bypass safety measures when prepended with hundreds of fabricated conversational turns \textit{within a single prompt}. The modified prompt mimics a dialogue between a human user and the LLM, in which the human asks malicious questions, and the model complies by providing the corresponding answers.
% 
% creates the illusion 
% makes it seem as though
This sequence makes it appear that the model has already complied with multiple malicious instructions, reinforcing an instruction-following pattern that ultimately compels the model to respond to the original malicious prompt. 
% 
This method, referred to as many-shot jailbreaking (MSJ), extends prior work on few-shot jailbreaking~\citep{wei2023jailbreak, rao2023tricking}—typically involving 8 to 16 shots in the short-context regime—by scaling up to 256 shots. Here, a ``shot'' refers to a single malicious question-and-answer pair, also called a ``demonstration'', as it shows how the model should respond to malicious questions by providing harmful answers.

To further explore this new form of LLM vulnerability, we propose \textbf{PANDAS}, a hybrid approach designed to increase the attack success rate (ASR) in long-context scenarios. Our results show that PANDAS consistently improves ASR over other long-context baseline methods.
% 

Figure~\ref{fig:teaser} provides an overview of PANDAS, with the colored region highlighting its main technical contributions. Our method comprises three techniques. First, \textbf{positive affirmation (PA)} phrases are inserted before new malicious questions are posed in fabricated conversational turns. Without adding more demonstrations, these phrases reinforce instruction-following behavior, encouraging the model to comply when responding to the final malicious prompts. 
% 

Second, \textbf{negative demonstrations (ND)} are introduced by embedding refusal and correction phrases into existing question-and-answer pairs. This explicitly shows the model how to handle refusals, guiding it to avoid them and comply with the instructions to generate harmful responses.
% 


Finally, we investigate how to optimally select demonstrations for target prompts from a specific topic. Previous work suggests that uniformly random sampling across a broad range of malicious topics is more effective than focusing on narrow subset of them~\citep{anil2024many}. Building on this insight, we leverage a Bayesian optimization framework~\citep{shahriari2015taking, nogueira2014bo} to identify the optimal sampling distribution for the topic of each malicious target prompt. This results in an \textbf{adaptive sampling (AS)} strategy that dynamically selects a topic-dependent distribution during jailbreaking, leading to a significant improvement in the jailbreak success rate.

Our contributions can be summarized as follows:
\begin{itemize}[itemsep=-0.5mm, , topsep=0mm, leftmargin=4mm]
    % \setlength\itemsep{0.01em}
    \item We introduce PANDAS, a hybrid technique that builds on MSJ with three key modifications to improve jailbreaking success rate in long-context scenarios.
    \item Results on AdvBench and HarmBench, using the latest open-source models, demonstrate that PANDAS improves long-context jailbreaking over existing methods.
    \item We perform an attention analysis to understand how models' long-context capabilities are exploited and how PANDAS improves upon MSJ.
\end{itemize}