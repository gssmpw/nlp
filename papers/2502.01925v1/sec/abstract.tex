Many-shot jailbreaking circumvents the safety alignment of large language models by exploiting their ability to process long input sequences. To achieve this, the malicious target prompt is prefixed with hundreds of fabricated conversational turns between the user and the model. These fabricated exchanges are randomly sampled from a pool of malicious questions and responses, making it appear as though the model has already complied with harmful instructions. In this paper, we present \textbf{PANDAS}: a hybrid technique that improves many-shot jailbreaking by modifying these fabricated dialogues with \textbf{P}ositive \textbf{A}ffirmations, \textbf{N}egative \textbf{D}emonstrations, and an optimized \textbf{A}daptive \textbf{S}ampling method tailored to the target prompt's topic. Extensive experiments on AdvBench and HarmBench, using state-of-the-art LLMs, demonstrate that PANDAS significantly outperforms baseline methods in long-context scenarios. Through an attention analysis, we provide insights on how long-context vulnerabilities are exploited and show how PANDAS further improves upon many-shot jailbreaking.

\textcolor{red}{\textbf{Warning:} This paper contains model behavior that can be offensive or harmful in nature.}