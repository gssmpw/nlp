\section{Experiments}\label{sec:exp}
In this section, we present results showing PANDAS's effectiveness over baseline long-context jailbreaking methods. We analyze the contribution of each PANDAS component, and evaluate performance against defended models. Through an attention analysis, we provide insights on how PANDAS improves upon MSJ.

\subsection{Experiment Setup}\label{sec:exp-setup}
\textbf{Model:} We focus on the latest open-source models, all released between May to December 2024.
% 
Those models include Llama-3.1-8B-Instruct (Llama-3.1-8B)~\citep{dubey2024llama},  %128k
Qwen2.5-7B-Instruct (Qwen-2.5-7B)~\citep{qwen2,qwen2.5}, %128K/
openchat-3.6-8b-20240522 (openchat-3.6-8B), % 8k
OLMo-2-1124-7B-Instruct (OLMo-2-7B)~\citep{olmo20242olmo2furious}, %4k
and GLM-4-9B-Chat (GLM-4-9B)~\citep{glm2024chatglm}. % 128K
% 
In terms of the support for long-context inference, models like Llama-3.1-8B, Qwen-2.5-7B, and GLM-4-9B can handle context windows of up to 128k tokens. Additionally, we include models with smaller context windows, ranging from 4k (OLMo-2-7B) to 8k (openchat-3.6-8B), which, despite their smaller capacity, still support a shot count of 32.
% 
With growing interest in safe and responsible LLMs, these new models were explicitly trained for harmlessness or fine-tuned and evaluated on safety datasets. 
% 
In particular, Llama-3.1-8B underwent specific safety alignment for addressing long-context vulnerabilities~\citep{dubey2024llama}. The focus on models with approximately 8B parameters follows prior work~\citep{zheng2024improved}, which was based on the empirical observation that the effectiveness of attacks are stable within model families but vary significantly between different families~\citep{mazeika2024harmbench}.

\noindent\textbf{Dataset:} We consider AdvBench~\citep{zou2023universal} and HarmBench~\citep{mazeika2024harmbench}, which consist of 520 and 400 malicious instructions, respectively. PANDAS requires grouping these malicious prompts into topics. While HarmBench is already categorized into 7 topics, we use Llama-3.1-8B to categorize the prompts in the AdvBench dataset. Additionally, for computationally expensive evaluations, we use AdvBench50, a subset of 50 samples from AdvBench introduced by~\citet{chao2023jailbreaking}. This subset is frequently used in prior work~\citep{zheng2024improved,xiao2024distract,mehrotra2023tree,pu2024baitattack}. 

\noindent\textbf{Generating malicious demonstrations:} Malicious demonstrations are generated using a few-shot approach using several open-source, uncensored, helpful-only models~\citep{wizardLM}. Following the approach in \citet{anil2024many}, we craft prompt templates that instruct the language model to create malicious demonstrations. These templates are included in Appendix~\ref{app:implementation}. The malicious demonstrations are generated using Anthropic's usage policy as a reference, which covers 12 topics including ``child-safety'', ``privacy'', and ``misinformation''.

\input{table/main}

\noindent\textbf{Metric:} We follow previous work~\citep{zheng2024improved, pu2024baitattack, zhou2024large} to evaluate the effectiveness of jailbreaking using both rule-based and LLM-based methods. 
% 
First, a jailbreaking attempt is considered successful if the response does not contains any phrases from a predefined list of refusal phrases (ASR-R). Our list extends the one provided by~\citet{zou2023universal}. 
% 
Second, we use a auxiliary LLM to judge whether the response is harmful (ASR-L). For our experiments, we utilize the latest release of Llama-Guard-3-8B (Llama-Guard-3)~\citep{dubey2024llama}. 

% Our manual inspection identified cases where responses were unsafe but not flagged by either metric. For example, some responses began with refusal phrases but eventually generated unsafe content (missed by ASR-R). To address this, we define a stronger combined success rate (ASR-C), where jailbreaking is considered successful if either of the two metrics indicates success. 
% 
% We report ASR-L and ASR-R separately in the main results, while other supporting results are presented using ASR-C due to space constraints.

Our evaluation follows~\citet{zheng2024improved}, where each target prompt is evaluated with 3 restarts. Specifically, the same jailbreaking configuration is applied to the same prompt three times, and the jailbreak is considered successful if any of the 3 attempts succeeds. For jailbreaking methods that involves random sampling, such as MSJ and PANDAS, the demonstrations sampled during each restart will be different.

\noindent\textbf{Long-context Baselines:} We fix the number of shots to 128, focusing on improvements over other baseline methods with a similar shot count. In addition to MSJ, we include i-FSJ, a recent few-shot jailbreaking improvement method~\citep{zheng2024improved}, originally designed for 8 to 16 shots. We extend this method to the many-shot setting, referring to it as i-MSJ. \citet{zheng2024improved} proposed two techniques. First, they identified special tokens, such as [/INST], which improves jailbreak effectiveness when included in demonstrations. Second, they introduced a demonstration-level random search, where demonstrations are replaced at random positions, and the change is accepted if it reduces the probability of generating specific tokens, such as the letter ``I'', which often leads to refusal responses like ``I cannot''. Following their setup, we set the number of random search iterations to 128.

\noindent\textbf{Implementation Details of PANDAS:} For PA and ND, we explore the impact of the modified demonstrations' position (i.e., $m$ in \eqref{eq:msj-pa} and \eqref{eq:msj-nd}) by evaluating four configurations: modifying the first demonstrations, the last demonstrations, all demonstrations, or a random subset of demonstrations. Results are reported using the configuration that achieves the highest ASR-L. Additionally, the positive affirmation, refusal, and correction phrases are each uniformly randomly sampled from a list of 10 prompts per type, with the full list provided in Appendix~\ref{app:pa_nd}. 


\subsection{Empirical Effectiveness of PANDAS}
Our main evaluation consists of 5 open-source models, 3 datatsets, and 2 additional long-context jailbreaking methods. Table~\ref{table:main} summarizes the results. PANDAS consistently outperforms all baseline long-context jailbreaking methods across models and datasets in both ASR-L and ASR-R evaluations. Given the same number of malicious demonstrations, PANDAS is the most effective of the three methods. On models with strong long-context capabilities, such as Llama-3.1-8B, PANDAS achieves ASR-L above 60\% at 64-shot settings on all datasets. Even on models with weaker long-context capabilities, PANDAS remains effective. On openchat-3.6-8B, PANDAS reaches over 97\% ASR-L with 64 shots before the model starts generating gibberish.

Beyond its overall improvement over baseline methods, we highlight several key observations. 

\textbf{Jailbreaking effectiveness does not always increase with more shots.} Unlike prior findings \citep{anil2024many}, we do not observe a consistent improvement as the number of demonstrations increases. For instance, on Llama-3.1-8B, both ASR-L and ASR-R peak at 64 shots. We provide two possible explanations: 1. Our evaluation focuses on 8B-parameter models, which, while capable of processing long input sequences, lack the long-context retention of larger models \citep{dubey2024llama}. This gap may explain why increasing the number of demonstrations does not yield the same \textit{benefits} observed in larger models. 2. The evaluated models were released after MSJ, and some have undergone safety alignments targeting long-context attacks. If alignment data specifically focuses on MSJ with a specific shot count, this could lead to a non-monotonic relationship between the number of shots and jailbreaking effectiveness.

\textbf{Jailbreaking effectiveness varies significantly across datasets, even for the same model.} For example, the difference in peak ASR-L between AdvBench and HarmBench is 48.17\% on Qwen-2.5-7B and 31.56\% on GLM-4-9B. This is due to HarmBench containing 25\% of target prompts related to copyright issues, where most models comply rather than refuse, leading to higher ASR scores. This finding underscores the importance of evaluating jailbreaking across multiple datasets, especially in long-context scenarios.

\textbf{A large gap exists between ASR-L and ASR-R with Qwen-2.5-7B on AdvBench.} Upon manual inspection of the model's responses, we find that the model does not always reject with explicit refusal phrases. Instead, it often generates benign responses that are  loosely related to the target prompt. Llama‐Guard‐3 correctly identifies these as safe, but because no explicit refusal phrase is present, ASR‐R remains high. Despite this, PANDAS still outperforms both MSJ and i-MSJ, demonstrating robust effectiveness across different evaluation settings.

\input{table/pand}
\textbf{Evaluating the individual component of PANDAS.} PANDAS is a hybrid method that consists of three techniques. While Table~\ref{table:main} presents the combined effect of PANDAS, it is important to understand how each component contributes to the overall effectiveness of the method. In Table~\ref{table:pand}, we focus on Llama-3.1-8B and modify MSJ by applying PA, ND, and AS individually. We observe that these techniques can be used independently and jointly to improve jailbreak effectiveness in long-context scenarios.

These observation verifies PANDAS’s improvement in long-context jailbreaking and its effectiveness across models, datasets, and evaluation setups.

\subsection{Long-context Jailbreaking Against Defense}
\input{table/defense}
We use Llama-3.1-8B as our base model, knowing that SFT and DPO are applied during post-training~\citep{dubey2024llama}, and evaluate several defense methods on AdvBench50.
Specifically, we consider 
Self-Reminder~\citep{xie2023defending}, which adds a system prompt reminding the model to comply with safety regulations; 
in-context defense (ICD)~\citep{wei2023jailbreak}, which prepends the input prompt with malicious questions and rejections; 
perplexity filtering (PPL Filter/Window)~\citep{jain2023baseline}, which detects the perplexity score of the input prompt; 
Retokenization~\citep{jain2023baseline} and SmoothLLM~\citep{robey2023smoothllm}, both of which perturb the input prompt during tokenization. 
We also consider an 8-shot setting to verify our result against previous work~\citep{zheng2024improved}. The results are summarized in Table~\ref{table:defense}.

\textbf{Perplexity-based methods} are ineffective at defending against MSJ and PANDAS, as these methods do not rely on special strings.

\textbf{Self-Reminder} is effective when fewer demonstrations are used but declines in effectiveness as the number of demonstrations increases. At 128-shot, it even increases ASR for both MSJ and PANDAS, aligning with the observation in~\citet{zheng2024improved}.

\textbf{Retokenization and SmoothLLM} reduce the effectiveness of MSJ and PANDAS in 8-shot settings. However, as the number of demonstrations increases, the output becomes malicious again and begins following the perturbations introduced by these defenses.

\textbf{ICD} inserts a malicious question and refusal phrase at the beginning of the input prompt, similar to how the negative demonstration is added in PANDAS, but without the correction phrase. To study this defense, we consider two implementations of ICD: \textbf{ICD-Exact}, which follows the original paper’s malicious question refusal phrases, and \textbf{ICD-Ours}, which randomly samples from our dataset. The two versions differ slightly: the original ICD uses an instruction-like query, whereas ICD-Ours follows an question-like format. 
% Additionally, the refusal response in ICD-Exact is more detailed, whereas ICD-Ours uses a generic refusal.
A comparison is provided in Appendix~\ref{app:icd_comparison}. Our results show that ICD-Exact has limited effectiveness in defending against MSJ and PANDAS. However, applying ICD-Ours to MSJ and PANDAS improves jailbreaking effectiveness, which is expected, as ND alone has been shown to enhance effectiveness. In Appendix~\ref{app:failed_defense}, we provide examples illustrating all failed defenses.


\subsection{Understanding PA and ND via Attention Analysis}
PA and ND are designed to reinforce the instruction-following behavior in the fabricated conversational turns. To support this claim and better understand these methods, we perform an attention analysis to study their effect on attention scores.

Attention scores have been widely used as a proxy to understand the behavior of transformers~\citep{clark2019does, hao2021self, oymak2023role, quirke2023understanding}. Recent work has explored modifying attention scores both in adversarial settings to generate adversarial examples~\citep{lyu2023attention} and in benign settings to enhance downstream task performance~\citep{zhang2023tell}. Additionally, studies such as \citet{akyurek2022learning} leverage attention scores to provide theoretical insights into the mechanisms behind in-context learning.

We follow~\citet{zhang2023tell} and define the multi-head attention score at the head $h$ of the $l$-th layer as $A^{(l,h)}$.
% = \text{Softmax}\left( \frac{1}{\sqrt{d_h}}Q K^{\top}\right)$, where $Q$ and $K$ are the query and key matrices, respectively, and $d_h$ is a normalization factor. 
% Recent work has shown that the early layers of LLMs play a key role in enforcing safety~\citep{zhao2023causality,zhao2024defending}. Motivated by this, w
Denote the total number of heads and layers as $H$ and $L$, respectively. We consider the average attention score across all heads and layers: $A = \frac{1}{HL}\sum_{h=1}^{H}\sum_{l=1}^{L}A^{(l,h)}$, where $A\in (0,1)^{N\times N}$, with $N$ representing the total number of tokens and $A_{k,q}$ denoting the $(k,q)$-th element.

Our goal is to analyze and compare attention scores between different long-context jailbreaking prompts, specifically between MSJ and its variations. However, a key challenge is the dimension mismatch between these prompts. To overcome this, we propose a structured attention analysis that partitions the attention map based on segments of the prompt. 

Consider an $n$-shot MSJ. We define token indices 
\begin{equation*}
    1=N_1 < N_2 < \dotsc < N_{n+1}<N_{n+2} = N
\end{equation*}
that segment the input prompt based on demonstrations. For instance, the tokens in $[N_i,N_{i+1})$ represents the $i$-th demonstration, and $N_{n+1}$ marks the start of the target prompt. Using these indices, we divide the attention map into smaller partitions. Specifically, for $1\leq i\leq j \leq n$, we have
\begin{equation*}
    P_{i,j} = \cset{(k,q)}{k\in[N_i, N_{i+1}), q\in[N_j,N_{j+1}), k\leq q }.
\end{equation*}

Figure~\ref{fig:attn_grid} illustrates this partitioning process for a 4-shot MSJ. In this example, $P_{3,3}$ captures how tokens in the third demonstration attend to each other, whereas $P_{3,1}$ and $P_{3,2}$ capture how tokens in the third demonstration attend to tokens in the first and second demonstrations.

With these partitions, we move from analyzing token-level attention (as in $A$) to segment-level attention. Recall that each row of $A$ sums to 1, representing how a token distributes its attention across itself and previous tokens. In the long-context setting with multiple demonstrations, we define the segment-level attention score from segment $i$ to $j$ by summing all token-level scores within partition $P_{i,j}$ and normalizing by the length of segment $i$:
\begin{equation}
    S_{i,j} = \frac{\sum_{(k,q)\in{P_{i,j}}} A_{k,q}}{N_{i+1} - N_{i}},
\end{equation}
where $N_{i+1} - N_{i}$ is the number of tokens in the $i$-th segment. This normalization not only allows a fair comparison between segments of varying lengths, it also preserves the property
% \begin{equation}
$
    \sum_{j=1}^{i} S_{i,j} = 1
% \end{equation}
$,
so that the total attention allocated by segment $i$ sums to 1. This allows us to analyze how much attention is received within each segment itself versus how much is directed toward previous segments.
\input{figure/plot_attn_grid}

Our motivation for PA and ND is to reinforce the instruct-following pattern presented in the fabricated conversations. To quantify how much attention is allocated to past demonstrations, we define the \textit{reference score} of segment $i$ as
\begin{equation}
    R_i = 1-S_{i,i} = \sum_{i=1}^{i-1} S_{i,j},
\end{equation}
which represents the fraction of attention that segment $i$ directs toward all preceding segments rather than itself. In other words, $R_i$ captures how much segment $i$ ``looks back'' to previous segments. A higher $R_i$ suggests that more attention is spent on earlier segments, potentially reflecting a stronger focus on the instruct‐following pattern established in prior demonstrations. 

\input{figure/plot_attn_analysis}
In Figure~\ref{fig:attn_analysis}, we compare the reference scores for a 32‐shot MSJ prompt and its PA and ND variants, all evaluated on Llama‐3.1‐8B. As the number of demonstrations increases, the attention allocated from each demonstration to earlier demonstrations increases and plateaus at around 24 shots. This may explain the improvements from increasing shot counts, as well as the limited gains observed in Table\ref{table:main}.

\textbf{Effect of PA}: We apply PA after each demonstration. The first demonstration remains unchanged; for all subsequent demonstrations and the target prompt, we prepend a PA phrase before the question. This causes every demonstration after the first to focus more on preceding demonstrations, an effect that carries through to the target prompt.

\textbf{Effect of ND}: We create an ND example by inserting a refusal phrase immediately after the first question in the initial demonstration. The second demonstration then begins with a correction phrase, followed by the original malicious response. This change triggers a sharp rise in attention to earlier segments in the second demonstration, an effect that tapers off gradually yet still provides modest benefits in later demonstrations.

Overall, these findings suggest that both PA and ND encourage each new demonstration to reference previous demonstrations more heavily, thereby reinforcing the instruct‐following behavior established by earlier examples. 
% Additional visualizations of the attention analysis can be found in the supplementary material.

% Our ND modification inserts a refusal phrase after the first question in the first malicious demonstrations and inserts a correction phrase before its original response. This creates an ND example at the first demonstration. We observe a sudden increase in the cross attention score in the following demonsration, with its effect gradually declining as the number of demonstration increases. Despite its diminishing effect, we notice that the cross attention score at later demonstrations still receive a modest increase.

% Those observation suggests that the PA and ND modification to the MSJ prompt increases each segments attention to previous demonstrations, thereby reinforcing the instruct-following behavior from the previous demonstrations.




% Comparing PA with MSJ, we 


% Attention scores in the rectangular regions ($S_{i,j}$ with $i<j$) represent how tokens \textit{from different segments} attend to each other, while those from the triangular regions ($S_{i,j}$ with $i = j$) capture how tokens \textit{within the same segments} attend to each other.



% In this work, we analyze how attention is distributed across these segments. We begin by formalizing this segmentation process: 

% highlight cross-attention relationships between demonstrations, 
% while the triangular regions capture attentions within demonstrations. 
% Elements from he bottom-right triangular region corresponds to how tokens from tthe target prompt.

% In this work, we focus on those triangular regions, analyzing attention interactions within individual segments of the original prompt. For the $i$-th demonstration, we define the its average attention score as
% \begin{equation}
%     S_i = \frac{\sum_{(k,q)\in{D_i}} A_{k,q}}{|D_i|},
% \end{equation}
% where 
% $
%     D_i =\cset{(k,q)}{N_i\leq k, q< N_{i+1}, k\leq q}
% $
% denotes the set of all token pairs within the demonstration. For the target prompt, its average attention score is $S_{n+1}$. 

% The value of $S$ represents the average attention score between all tokens within a single segment of the prompt. Within a prompt, comparing $S$ values reveals how the model distributes attention across different segments. A higher $S$ suggests that the model strongly integrates information within the segment, potentially requiring less support from other parts of the prompt. Conversely, a lower $S$ suggests that tokens within the segment do not strongly attend to each other. This could mean that the model does not treat it as fully independent and instead relies more on other segments for interpretation.

% \input{figure/attn_analysis}
% In Figure~\ref{fig:attn_analysis}, we compare the normalized average attention score between a 32-shot MSJ and its modifications applied to the same MSJ prompt. Additionally, since ND increases the number of demonstration by one, we increase the shot count by one by including another malicious demonstrations, denoting it as PD (positive demonstration). We make three observations.

% First, the average attention score gradually declines across all prompts as the number of demonstrations increases. This means that tokens within each demonstration attend less to each other, suggesting that the increasing number of demonstrations may strength the model's focus on instruction-following behavior rather than the harmful content of individual demonstrations.

% Second, we observe a sharp increase in the normalized attention score at the target prompt segment. This is likely because this segment contains only the target prompt, while other segments contain complete demonstrations. The fewer cross-attention elements in the target prompt segment may result in less diluted attention scores.

% Finally, modifying the MSJ prompt with either PA or ND (or both) decreases the normalized attention score of the target prompt segment. Compared to MSJ, these modifications reduce attention within the target prompt, suggesting a shift in the model's focus from the target prompt's content towards the instruct-following pattern. We demonstrate that simply increasing the number of demonstrations is less effective than our proposed modifications. This attention analysis provides insight into the empirical effectiveness of PA and ND.

% Second, modifying the MSJ prompt with PA applied to all demonstrations increases $S_1$ and decreases $S_{33}$. With this modification, the segments remains unchanged are the first one and the target prompt, 

% Lastly, applying ND with $m=1$ prepends a negative demonstration before the first demonstration. We observe a significantly different change to the attention score compared to PD, which prepends a negative demonstration.

% Lastly, there is a sudden increase in the attention score at the target prompt.

