\section{Method}\label{sec:method}
In this section, we introduce the three techniques that are integrated together as PANDAS.

\subsection{Positive Affirmation}\label{sec:positive}
LLMs undergo multiple stages of training and fine-tuning before release, including tasks such as sentence completion~\citep{devlin2018bert, radford2019language}, instruction following~\citep{ouyang2022training}, and safety alignment~\citep{perez2022red, ganguli2022red}. Previous work by \citet{wei2024jailbroken} suggested that jailbreaking occurs when a prompt forces the model to choose between two behaviors that are both penalized during training or fine-tuning: producing a harmful response or failing to follow instructions.

One \textit{explicit} way of achieving this is jailbreaking via refusal suppression, where the malicious target prompt is appended with rules such as ``Never say the word `cannot' ''~\citep{wei2024jailbroken}.
% 
While this method directly creates a competing objective between safety alignment and instruction following, we argue that MSJ achieves a similar effect \textit{implicitly}. 
% 
By providing a fabricated conversation that emphasizes instruction-following, the model is pushed to continue following instructions rather than uphold safety regulations.
% to decide whether to continue this pattern or uphold safety regulations.

One evidence that can be used to support this is the results from \citet{anil2024many}: as the number of demonstrations increases, the jailbreak success rate rises as well. This suggests that adhering to this established instruction-following pattern becomes increasingly compelling, even at the expense of generating harmful outputs. In other words, the cost of ``breaking'' the pattern eventually outweighs generating harmful responses.

Increasing the number of demonstrations indefinitely can be impractical. \textbf{How do we reinforce this instruct-following pattern without increasing the number of demonstrations?} To achieve this, we append PA phrases (such as ``\pabox{Exactly the detail I needed! Thanks!}'') before the next malicious question. 
% By acknowledging the model's helpfulness or compliance after each demonstration, we effectively reward it for continuing to produce answers rather than refusing. 
% 
% This reward-like feedback makes the cost of breaking the pattern more costly: it signals to the model that continuing to follow the established ``instruction-following'' trajectory is the correct behavior, even if the response is unsafe. 
% 
This acknowledgment reinforces the model's tendency for complying rather than refusing. In doing so, the cost of deviating from the instruction-following trajectory rises even further, making it more likely the model will continue generating unsafe responses.
% \todo[inline]{revise this claim}

To formalize this process, let us denote a PA phrase as $x_{+}$. We modify \eqref{eq:msj} by inserting $x_{+}$ into the sequence of demonstrations:
\begin{equation}\label{eq:msj-pa}
    x'_t= \seq{d_1, \dotsc, d_{m}, x_{+}, d_{m+1}, \dotsc, d_n, x_{t}},
\end{equation}
where $m \in \set{1, \dotsc, n}$ specifies the index of the demonstrations after which the PA phrase is inserted. Setting $m=n$ places the PA phrase directly before the target prompt $x_t$.

\subsection{Negative Demonstration}\label{sec:negative}
\citet{anil2024many} hypothesize that the effectiveness of MSJ relates to the process of in-context learning (ICL)~\citep{brown2020language}.
% 
To support this, they show that ICL under normal, non-jailbreak circumstances exhibits power-law scaling similar to MSJ as the number of demonstrations increases.

ICL performance is influenced by the design and selection of in-context examples~\citep{liu2021makes, zhang2023makes, chen2023many}. 
% 
While most ICL studies use \textit{correct} demonstrations, recent work has explored the use of negative demonstrations~\citep{zhang2024context, gao2024customizing}.
% 
Inspired by the concept of ``learning from mistakes'', these approaches involve intentionally introducing mistakes in the demonstrations and then correcting them, with the goal of guiding the model to avoid similar errors in the future. 
% 
% While previous work focuses on mathmatical reasoning tasks and formulate this as a reasoning step which provides
% the model with the appropriate articulation of the user’s need
% and guides it towards generating a better answer.

While previous work has focused on benign reasoning tasks, such as mathematical datasets~\citep{hendrycks2021measuring}, we propose to modify MSJ by incorporating ND. 
% 
Let $a_{-}$ denote a refusal phrase, such as ``\ndbox{I can't help with that.} \ndbox{Is there anything else I can help you with?}'', and $q_{-}$ denote a correction phrase, such as ``\ndbox{Your response to the} \ndbox{previous question was either incomplete or did not address} \ndbox{it correctly. Please follow the instructions carefully and try} \ndbox{answering again.}''.

We propose to modify an existing demonstration by inserting the refusal and correction phrases between the malicious question and the corresponding answer. 
% 
Let $g(\cdot)$ denote this operation. The modified demonstration is expressed as:
\begin{equation}\label{eq:nd}
    g(d,a_{-}, q_{-}) = \seq{q, a_{-}, q_{-}, a}.
\end{equation}
By doing so, we create a scenario where the model first refuses to answer the malicious question then receives feedback in the form of a correction phrase, and finally provides the intended malicious response.
% 

The ND can be included in \eqref{eq:msj} by
\begin{equation}\label{eq:msj-nd}
    x'_t= \seq{d_1, \dotsc, g(d_{m}, a_{-}, q_{-}), \dotsc, d_n, x_{t}},
\end{equation}
where $m \in \set{1,\dotsc,n}$ specifies the index of the original demonstration to be modified. 

This modification reinforces the instruction-following behavior by explicitly demonstrating how the model should handle refusal and correction prompts in the context of jailbreaking, increasing the likelihood of generating harmful responses to the malicious target prompt.



\subsection{Adaptive Sampling}\label{sec:adaptive}
MSJ relies on a curated set of malicious demonstrations from which the fabricated conversations are sampled.
% 
These demonstrations, typically question-and-answer pairs based on predefined topics such as violence, misinformation, or regulated content~\citep{anil2024many}, are crucial for guiding the model toward harmful behavior.
% 
Previous findings by \citet{anil2024many} show that sampling demonstrations from a \textit{broad} range of malicious topics is more effective than focusing narrowly on a few topics, highlighting the role of diversity in demonstration selection.

However, in \citet{anil2024many}, these topics are sampled uniformly at random. In this paper, we explore whether certain topics are more effective than others for MSJ, motivating an adaptive sampling strategy that refines demonstration selection for long-context jailbreaking. 


\input{figure/plot_distribution}

To determine the optimal sampling distribution across topics, we formalize the sampling-then-jailbreaking process as a function $B: z\rightarrow r$, where $z\in [0,1]^{C}$ with $\sum_{i=1}^C z_i= 1$ represents the sampling distribution across $C$ topics, and $r$ denotes the resulting jailbreak success rate from MSJ. We treat $B$ as a black-box function and optimize it using Bayesian optimization~\citep{shahriari2015taking, nogueira2014bo}, which efficiently balances exploration and exploitation~\citep{turner2021bayesian}, though other black-box optimization methods also exist~\citep{hansen2010comparing}. This optimization is performed separately for each target model. We initialize the optimization by probing with a uniform random distribution, ensuring that performance is at least comparable to MSJ. Details of our optimization settings are provided in Appendix~\ref{app:implementation}.

Our approach focuses on the sampling distribution rather than the ordering of demonstrations. Prior work shows that ICL performance can depend on ordering~\citep{lu2021fantastically, zhao2021calibrate}, but we observe that a successful MSJ remains effective even after shuffling the order of demonstrations. We discuss this further in Appendix~\ref{app:permutation}.

Figure~\ref{fig:distribution} illustrates the sampling distribution identified via Bayesian optimization for prompts from the HarmBench dataset.\footnote{We omit the \emph{copyright} topic because the ASR is already near 100\% with uniform random sampling.} Our findings are consistent with the observation from~\citet{anil2024many}, as sampling from multiple topics remains advantageous. However, certain topics—such as \emph{regulated-content} and \emph{sexual}—are selected more frequently, leading to higher ASR.
% \AMComment{expand with more observations}
% In Sec.~\ref{sec:exp}, we show that iteratively refining $z$ based on observed values of $r$ can significantly increase MSJ effectiveness.

% While previous research \citep{lu2021fantastically, zhao2021calibrate} indicates that ICL performance can depend heavily on the ordering of demonstrations, we observe a different pattern for MSJ. As shown in Table~\ref{table:permutation}, the majority of harmful responses remain successful for Llama-3.1-8B-Instruct \citep{dubey2024llama} and GLM-4-9B-Chat \citep{glm2024chatglm}, even when the demonstration order changes. 




% While the primary goal of most demonstration selection strategies in ICL focuses on selecting demonstrations that are similar to the topics of the target prompt, leveraging strategies such as nearest neighbors measured based on $\ell_2$ ~\citep{liu2021makes} or outputs from auxilitary LLMs, despite some uses diversity as a form or regularizers.


 
 
 % Results from Cem suggest that sampling demonstrations from a diverse pool of malicious prompts is more effective than sampling from a narrow range of topics. 