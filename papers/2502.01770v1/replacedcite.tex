\section{Related Work}
\subsection{Long Context Architectures}
A wide range of variations of and replacements for self-attention have been proposed to address the $O(n^2)$ scaling problem.  A number of these approaches use fixed, sparse attention maps, with fewer than $O(n^2)$ active elements.  For example, longformer ____ combines local dilated and non-dilated sliding-window attention with a fixed number of globally attending tokens, producing an attention matrix with $O(n)$ non-zero elements and fully networked tokens over multiple sequential attention layers.  BigBird ____ is similar, but also includes randomly selected non-zero elements in the attention matrix. \par Another approach, the Reformer ____, uses locality sensitive hashing to bucket keys and queries, and performs attention within these limited buckets.  All of these approaches produce non-trivial accuracy losses against otherwise equivalent models.  More recently, Mamba ____ has gained popularity as a competitive attention replacement across many tasks, radically departing from attention and using a state space model instead.  While promising, this offers a radically different set of tradeoffs from standard attention, and is even more poorly suited as a drop-in replacement to finetune pre-trained transformers for long context tasks.  
\par Across the long-context field, almost all works focus on asymptotic runtime improvements rather than improving hardware utilization, and very few consider adapting pre-trained transformers.

\subsection{Deep Neural Network Binarization}
Binarized neural networks ____, trained with binary weights and activations, were developed to increase the efficiency of deep neural network inference, and predate the transformer architecture.  While early works focused on convolutional neural networks ____, many recent works have adapted binarization techniques to address the transformer architecture.  Some of these works use variations of weight binarization while quantizing activations gently or not at all ____.  This achieves very small performance degredation at the cost of increased runtime and memory when compared to full binarization approaches. 
\par These approaches are well suited for decoder-only GPT-style language models, where the quality of generated text is highly sensitive to small changes in the loss value, scaling the number of parameters drastically improves performance, and long contexts are not often necessary.  However, these approaches do little to nothing to address the computational costs of attention which dominates in long-context settings, as attention is an operation only between activations in the form of keys, queries, and values.  
\par Other works, generally applied to BERT-style encoder-only models and vision transformers, perform full binarization including activations and attention, maximizing efficiency at the cost of accuracy.  Among the state of the art in this category is BiViT ____, which proposes a novel softmax-aware attention matrix binarization function, achieving significantly higher accuracies than previous fully binarized vision transformers.  BiT ____ achieved state-of-the-art results on the GLUE benchmark among fully binarized BERT variants, learning scale and threshold terms during binarization and using a multi-step distillation procedure.  Much like BiT, our work includes explicit loss terms so that the fine-tuned student model's attention map closely replicates the teacher model's.  Effectively all of the above listed works use variations of straight-through-estimators (STEs) to estimate gradients for non-differentiable quantized functions, but HAD also uses transformations of the hyperbolic tangent (tanh) function to smoothly transition from continuous to binarized representations, similar to previous binarization works ____. 
\par Many highly optimized binary multipliers exist, which are tailored to perform XNOR-based computations that reduce area and power while maintaining performance ____. These multipliers leverage the inherent simplicity of binary operations to enable faster computation with reduced hardware requirements, therefore accelerating binarized neural networks. We would expect binarized attention activations to be particularly valuable, as processing in memory approaches which have been successful in accelerating weight-activation operations ____ are not as easily applied to activation-activation operations in attention, exacerbating its costs.

\subsection{Binary Keys and Queries}
The selective binarization of keys and queries in this work is primarily inspired by associative memory and Hopfield networks.  In hopfield networks, a set of binary vectors or "patterns" are stored and can be queried according to some update rule.  Modern hopfield networks ____ are capable of addressing a number of unique patterns scaling exponentially with the number of bits per pattern. "Hopfield networks is all you need" ____ demonstrated that self-attention is effectively a generalization of modern hopfield networks to continuous valued patterns in the forms of keys, addressed by queries.  This mapping between attention and binary modern hopfield networks, along with the exponential storage capacity of modern hopfield networks, led us to hypothesize that binarizing specifically our keys and queries would still enable expressive attention lookups to retrieve value vectors.  \par Additionally, studies of scaling laws of transformer-based LLMs find that performance is tightly coupled to parameter count and training data, and very weakly to model shape ____, indicating that the capacity of the attention mechanism does not determine the capacity of the transformer in many cases.  Therefore, we do not expect the capacity loss induced by key and query binarization to signficantly reduce model performance.

\subsection{Sparse Attention}
Like multiple other works, HAD utilizes a sparse attention matrix.  Because the attention matrix is the output of a softmax function, and the relative magnitude of outputs of a softmax is exponential relative to the difference in inputs, large attention matrices contain a large number of near-zero values.  Some works such as BigBird ____ and Longformer ____ impose structured sparsity on the attention matrix, with a bias towards nearby elements being able to attend to one another.  Other works, exploit the intrinsic sparsity of the attention matrix in order to filter out unwanted features ____ or accelerate transformer inference ____ like HAD.  
\par We select the top $N$ highest attention scores corresponding to each query, and performing a sparse accumulation over our values matrix.  While binarization allows us to accelerate the $O(n^2)$ $K Q^T$ operation, sparsity allows us to accelerate the $O(n^2)$ softmax, scaling, and accumulation over $V$. Many works have proposed GPU and custom hardware solutions, leveraging sparsity to improve efficiency ____.