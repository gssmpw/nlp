@article{transformer,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{audio,
  title={Audio transformers: Transformer architectures for large scale audio understanding. adieu convolutions},
  author={Verma, Prateek and Berger, Jonathan},
  journal={arXiv preprint arXiv:2105.00335},
  year={2021}
}

@inproceedings{video,
  title={Vivit: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6836--6846},
  year={2021}
}

@article{linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{bigbird,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}

@article{mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{bin_fpga,
  title={FP-BNN: Binarized neural network on FPGA},
  author={Liang, Shuang and Yin, Shouyi and Liu, Leibo and Luk, Wayne and Wei, Shaojun},
  journal={Neurocomputing},
  volume={275},
  pages={1072--1086},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{tpu,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}

@inproceedings{cerebras,
  title={Cerebras architecture deep dive: First look inside the hw/sw co-design for deep learning: Cerebras systems},
  author={Lie, Sean},
  booktitle={2022 IEEE Hot Chips 34 Symposium (HCS)},
  pages={1--34},
  year={2022},
  organization={IEEE Computer Society}
}

@inproceedings{groq,
  title={The groq software-defined scale-out tensor streaming multiprocessor: From chips-to-systems architectural overview},
  author={Abts, Dennis and Kim, John and Kimmell, Garrin and Boyd, Matthew and Kang, Kris and Parmar, Sahil and Ling, Andrew and Bitar, Andrew and Ahmed, Ibrahim and Ross, Jonathan},
  booktitle={2022 IEEE Hot Chips 34 Symposium (HCS)},
  pages={1--69},
  year={2022},
  organization={IEEE Computer Society}
}

@article{quality,
  title={QuALITY: Question answering with long input texts, yes!},
  author={Pang, Richard Yuanzhe and Parrish, Alicia and Joshi, Nitish and Nangia, Nikita and Phang, Jason and Chen, Angelica and Padmakumar, Vishakh and Ma, Johnny and Thompson, Jana and He, He and others},
  journal={arXiv preprint arXiv:2112.08608},
  year={2021}
}

@article{longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{bnn,
  title={Binarized neural networks},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{bin_cnn,
  title={Xnor-net: Imagenet classification using binary convolutional neural networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle={European conference on computer vision},
  pages={525--542},
  year={2016},
  organization={Springer}
}

@inproceedings{pbllm,
  title={PB-LLM: Partially Binarized Large Language Models},
  author={Yuan, Zhihang and Shang, Yuzhang and Dong, Zhen},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{onebit,
  title={OneBit: Towards Extremely Low-bit Large Language Models},
  author={Xu, Yuzhuang and Han, Xu and Yang, Zonghan and Wang, Shuo and Zhu, Qingfu and Liu, Zhiyuan and Liu, Weidong and Che, Wanxiang},
  journal={arXiv preprint arXiv:2402.11295},
  year={2024}
}

@article{dbllm,
  title={DB-LLM: Accurate dual-binarization for efficient LLMs},
  author={Chen, Hong and Lv, Chengtao and Ding, Liang and Qin, Haotong and Zhou, Xiabin and Ding, Yifu and Liu, Xuebo and Zhang, Min and Guo, Jinyang and Liu, Xianglong and others},
  journal={arXiv preprint arXiv:2402.11960},
  year={2024}
}

@inproceedings{bivit,
  title={Bivit: Extremely compressed binary vision transformers},
  author={He, Yefei and Lou, Zhenyu and Zhang, Luoming and Liu, Jing and Wu, Weijia and Zhou, Hong and Zhuang, Bohan},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5651--5663},
  year={2023}
}

@article{bit,
  title={Bit: Robustly binarized multi-distilled transformer},
  author={Liu, Zechun and Oguz, Barlas and Pappu, Aasish and Xiao, Lin and Yih, Scott and Li, Meng and Krishnamoorthi, Raghuraman and Mehdad, Yashar},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={14303--14316},
  year={2022}
}

@article{tanh_bin,
  title={Self-binarizing networks},
  author={Lahoud, Fayez and Achanta, Radhakrishna and M{\'a}rquez-Neila, Pablo and S{\"u}sstrunk, Sabine},
  journal={arXiv preprint arXiv:1902.00730},
  year={2019}
}

@article{modern_hop,
  title={On a model of associative memory with huge storage capacity},
  author={Demircigil, Mete and Heusel, Judith and L{\"o}we, Matthias and Upgang, Sven and Vermet, Franck},
  journal={Journal of Statistical Physics},
  volume={168},
  pages={288--299},
  year={2017},
  publisher={Springer}
}

@article{hopfield_is,
  title={Hopfield networks is all you need},
  author={Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and others},
  journal={arXiv preprint arXiv:2008.02217},
  year={2020}
}

@article{scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{sparse_att_acc,
  title={Transformer acceleration with dynamic sparse attention},
  author={Liu, Liu and Qu, Zheng and Chen, Zhaodong and Ding, Yufei and Xie, Yuan},
  journal={arXiv preprint arXiv:2110.11299},
  year={2021}
}

@article{sparse_att_filter,
  title={Sparse co-attention visual question answering networks based on thresholds},
  author={Guo, Zihan and Han, Dezhi},
  journal={Applied Intelligence},
  volume={53},
  number={1},
  pages={586--600},
  year={2023},
  publisher={Springer}
}



@inproceedings{rastegari2016xnor,
  title={{XNOR-Net}: ImageNet Classification Using Binary Convolutional Neural Networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={525--542},
  year={2016},
  publisher={Springer}
}

@article{lee2022power,
  title={Power and Area-Efficient XNOR-AND Hybrid Binary Neural Networks Using TFT-Type Synaptic Devices},
  author={Lee, In-Seok and Kim, Hyeongsu and Park, Min-Kyu and Hwang, Joon and Koo, Ryun-Han and Kim, Jae-Joon and Lee, Jong-Ho},
  journal={IEEE Electron Device Letters},
  volume={43},
  number={12},
  pages={1912--1915},
  year={2022},
  publisher={IEEE}
}

@article{garg2013array,
  title={Array Multiplier Using XNOR},
  author={Garg, Riya and Kaur, Navneet},
  journal={International Journal of Engineering Science and Technology (IJEST)},
  volume={5},
  number={4},
  pages={799--803},
  year={2013}
}

@inproceedings{deit,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@article{t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{race,
  title={Race: Large-scale reading comprehension dataset from examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  journal={arXiv preprint arXiv:1704.04683},
  year={2017}
}

@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{pim,
  title={Prime: A novel processing-in-memory architecture for neural network computation in reram-based main memory},
  author={Chi, Ping and Li, Shuangchen and Xu, Cong and Zhang, Tao and Zhao, Jishen and Liu, Yongpan and Wang, Yu and Xie, Yuan},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={27--39},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@inproceedings{sparse_gpu,
  title={Efficient sparse-dense matrix-matrix multiplication on GPUs using the customized sparse storage format},
  author={Shi, Shaohuai and Wang, Qiang and Chu, Xiaowen},
  booktitle={2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)},
  pages={19--26},
  year={2020},
  organization={IEEE}
}

@inproceedings{sparse_asic,
  title={Sextans: A streaming accelerator for general-purpose sparse-matrix dense-matrix multiplication},
  author={Song, Linghao and Chi, Yuze and Sohrabizadeh, Atefeh and Choi, Young-kyu and Lau, Jason and Cong, Jason},
  booktitle={Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages={65--77},
  year={2022}
}