@article{bigbird,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}

@inproceedings{bin_cnn,
  title={Xnor-net: Imagenet classification using binary convolutional neural networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle={European conference on computer vision},
  pages={525--542},
  year={2016},
  organization={Springer}
}

@article{bit,
  title={Bit: Robustly binarized multi-distilled transformer},
  author={Liu, Zechun and Oguz, Barlas and Pappu, Aasish and Xiao, Lin and Yih, Scott and Li, Meng and Krishnamoorthi, Raghuraman and Mehdad, Yashar},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={14303--14316},
  year={2022}
}

@inproceedings{bivit,
  title={Bivit: Extremely compressed binary vision transformers},
  author={He, Yefei and Lou, Zhenyu and Zhang, Luoming and Liu, Jing and Wu, Weijia and Zhou, Hong and Zhuang, Bohan},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5651--5663},
  year={2023}
}

@article{bnn,
  title={Binarized neural networks},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{garg2013array,
  title={Array Multiplier Using XNOR},
  author={Garg, Riya and Kaur, Navneet},
  journal={International Journal of Engineering Science and Technology (IJEST)},
  volume={5},
  number={4},
  pages={799--803},
  year={2013}
}

@article{hopfield_is,
  title={Hopfield networks is all you need},
  author={Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and others},
  journal={arXiv preprint arXiv:2008.02217},
  year={2020}
}

@article{lee2022power,
  title={Power and Area-Efficient XNOR-AND Hybrid Binary Neural Networks Using TFT-Type Synaptic Devices},
  author={Lee, In-Seok and Kim, Hyeongsu and Park, Min-Kyu and Hwang, Joon and Koo, Ryun-Han and Kim, Jae-Joon and Lee, Jong-Ho},
  journal={IEEE Electron Device Letters},
  volume={43},
  number={12},
  pages={1912--1915},
  year={2022},
  publisher={IEEE}
}

@article{longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{modern_hop,
  title={On a model of associative memory with huge storage capacity},
  author={Demircigil, Mete and Heusel, Judith and L{\"o}we, Matthias and Upgang, Sven and Vermet, Franck},
  journal={Journal of Statistical Physics},
  volume={168},
  pages={288--299},
  year={2017},
  publisher={Springer}
}

@article{onebit,
  title={OneBit: Towards Extremely Low-bit Large Language Models},
  author={Xu, Yuzhuang and Han, Xu and Yang, Zonghan and Wang, Shuo and Zhu, Qingfu and Liu, Zhiyuan and Liu, Weidong and Che, Wanxiang},
  journal={arXiv preprint arXiv:2402.11295},
  year={2024}
}

@inproceedings{pbllm,
  title={PB-LLM: Partially Binarized Large Language Models},
  author={Yuan, Zhihang and Shang, Yuzhang and Dong, Zhen},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{pim,
  title={Prime: A novel processing-in-memory architecture for neural network computation in reram-based main memory},
  author={Chi, Ping and Li, Shuangchen and Xu, Cong and Zhang, Tao and Zhao, Jishen and Liu, Yongpan and Wang, Yu and Xie, Yuan},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={27--39},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@inproceedings{rastegari2016xnor,
  title={{XNOR-Net}: ImageNet Classification Using Binary Convolutional Neural Networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={525--542},
  year={2016},
  publisher={Springer}
}

@article{reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{sparse_asic,
  title={Sextans: A streaming accelerator for general-purpose sparse-matrix dense-matrix multiplication},
  author={Song, Linghao and Chi, Yuze and Sohrabizadeh, Atefeh and Choi, Young-kyu and Lau, Jason and Cong, Jason},
  booktitle={Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages={65--77},
  year={2022}
}

@article{sparse_att_acc,
  title={Transformer acceleration with dynamic sparse attention},
  author={Liu, Liu and Qu, Zheng and Chen, Zhaodong and Ding, Yufei and Xie, Yuan},
  journal={arXiv preprint arXiv:2110.11299},
  year={2021}
}

@article{sparse_att_filter,
  title={Sparse co-attention visual question answering networks based on thresholds},
  author={Guo, Zihan and Han, Dezhi},
  journal={Applied Intelligence},
  volume={53},
  number={1},
  pages={586--600},
  year={2023},
  publisher={Springer}
}

@inproceedings{sparse_gpu,
  title={Efficient sparse-dense matrix-matrix multiplication on GPUs using the customized sparse storage format},
  author={Shi, Shaohuai and Wang, Qiang and Chu, Xiaowen},
  booktitle={2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)},
  pages={19--26},
  year={2020},
  organization={IEEE}
}

@article{tanh_bin,
  title={Self-binarizing networks},
  author={Lahoud, Fayez and Achanta, Radhakrishna and M{\'a}rquez-Neila, Pablo and S{\"u}sstrunk, Sabine},
  journal={arXiv preprint arXiv:1902.00730},
  year={2019}
}

