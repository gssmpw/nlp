\documentclass[11pt]{article}
\usepackage[margin=1.2in]{geometry}

\usepackage{arxiv_macros}
\usepackage{ulem}

\title{Efficient Time Series Forecasting via Hyper-Complex Models and Frequency Aggregation}
\author{
    Eyal Yakir, Dor Tsur, and Haim Permuter\\
    School of Electrical and Computer Engineering\\
    Ben-Gurion University of the Negev, Be'er Sheva, Israel\\
    \texttt{\{eyalyak, dortz\}@post.bgu.ac.il, haimp@bgu.ac.il}
}


\date{}
\begin{document}
\maketitle
\begin{abstract}
Time series forecasting is a long-standing problem in statistics and machine learning. One of the key challenges is processing sequences with long-range dependencies. To that end, a recent line of work applied the short-time Fourier transform (STFT), which partitions the sequence into multiple subsequences and applies a Fourier transform to each separately. We propose the Frequency Information Aggregation (FIA)-Net, which is based on a novel complex-valued MLP architecture that aggregates adjacent window information in the frequency domain. To further increase the receptive field of the FIA-Net, we treat the set of windows as hyper-complex (HC) valued vectors and employ HC algebra to efficiently combine information from all STFT windows altogether.
Using the HC-MLP backbone allows for improved handling of sequences with long-term dependence. Furthermore, due to the nature of HC operations, the HC-MLP uses up to three times fewer parameters than the equivalent standard window aggregation method. We evaluate the FIA-Net on various time-series benchmarks and show that the proposed methodologies outperform existing state of the art methods in terms of both accuracy and efficiency. Our code is publicly available on \url{https://anonymous.4open.science/r/research-1803/}.
\end{abstract}




\section{Introduction}
Time series forecasting (TSF) is a long standing challenge, which plays a key role in various domains, such as energy management \cite{DeppLearningForPower}, traffic prediction \cite{DeppLearningForTraffic} and financial analysis \cite{DeppLearningForStocks}.
With the development of deep learning, myriad neural network (NN) architectures had been proposed, and have gradually improved the accuracy on the TSF problem.
Two key architectures has been used for TSF are recurrent NNs (RNNs) \cite{rnn1,rnn2,rnn3} and transformers \cite{AttentionIsAllYouNeed,fedformer,autoformer,crossformer}, each of which aims to capture long-term dependencies through a different functional feature extraction procedure.
While both methods were proven useful, RNNs struggled with long-term dependencies \cite{RNNGradientProblem} or non-stationary data patterns, While transformer architectures may overlook important temporal information due to permutation invariance \cite{kim2024self}, they require many parameters and may suffer from long runtime.
Additional NN-based approaches for TSF consider graph NNs (GNNs) \cite{wu2020connecting} and decomposition models \cite{oreshkin2019n}.

Recent advancements have demonstrated promising results in processing and extracting features from the frequency domain \cite{FrequencyTransformationSurvey} . Techniques leveraging frequency-based transformations have been in various contexts, ranging from computational efficiency improvements \cite{autoformer} to seasonal-trend decomposition \cite{fedformer}. 
% The use of frequency-domain representations has proven highly effective in many time-series models \cite{FrequencyTransformationSurvey} \ey{and here}.
To better process the frequency domain data, \cite{FreTS} developed a complex-valued MLP, which demonstrated superior capability in capturing both temporal and cross-channel dependencies.
To better handle nonstationarities in the data, \cite{FREQTSFAttention} substituted the standard FFT with the Short-Time Fourier Transform (STFT) \cite{STFT}, which divides the sequence into separate windows and transforms each window individually into the frequency domain.
While showing better suitability for non-stationary time series data, the STFT yields a set of windows, each of which represents exclusive information on the sequence.
However, in practice, adjacent windows are highly correlated, albeit processed separately by current STFT-based models.

To incorporate the overlooked shared information, we propose the FIA-Net, a novel TSF model that is designed to handle long-term dependencies in the data by aggregating information from subsets of STFT windows. The FIA-Net has an MLP backbone that processes the STFT windows in the frequency domain. We propose two novel MLP architectures.
The first, is termed window-mixing MLP (WM-MLP), which mixes each STFT windows with its neighboring bands.
The second is the HC-MLP.
The HC-MLP leverages HC algebra to efficiently combine information from \textit{all} STFT together.
By using HC algebra, the STFT is implements with three times less parameters than the equivalent WM-MLP.
% while reducing the total amount of parameters by a factor of up to $3$.

The main contributions of this paper are as follows
\begin{itemize}
    \item We construct the FIA-Net and the WM-MLP backbone. The resulting TSF model captures inter-window dependencies in the frequency domain and benefits from a forward pass complexity of $O(L\log L/p)$ operations, where $L$ is the lookback window length and $p$ is the number of STFT windows.  
    \item We propose a novel HC-MLP backbone that expands the receptive field of the WM-MLP, while requiring a fraction of total parameters. 
    \item To reduce the model size and complexity, we filter the STFT windows, leaving only the top-$M$ frequency components. We show that accuracy is maintained even when $M$ is significantly smaller than the total number of components.
    \item We provide an array of experiments that demonstrate the performance of the model and its efficiency. We show that the FeeqShiftNet improves upon existing models accuracy by up to $20\%$.
    \item We provide an ablation study, in which explores the effect of operating over the complex plane and compare the performance of the two considered MLP backbones.

\end{itemize}



\section{Related Work}
\label{app:Related Work}

\textbf{Time-Series Forecasting }
The first notable works on TSF utilize classical statistical linear models such as ARIMA \cite{ARIMA1,ARIMA2} which consider series decomposition. Those were then generalized to a non-linear setting in \cite{var}. To overcome the limitations posed by the classical models, deep learning was incorporated, where initially, sequential deep learning was performed by RNN-based models. 
Two key RNN models are long-short term memory networks \cite{rnn2} which introduce a sophisticated gating mechanism and the DeepAR model \cite{rnn1} that connected the RNN model with AR modeling. Despite their expressive power for sequential modeling, RNN demonstrated low efficiency and introduced high runtimes in both the forward and backward pass \cite{RNNGradientProblem}.
Two popular architectures were proposed to improve upon RNNs; transformers and GNNs. Notable transformer-based methods are Informer \cite{Informer}, Reformer \cite{Reformer}, and PatchTST \cite{PatchTST}, each leveraging the attention mechanism to capture temporal dependencies, while proposing sophisticated methods to reduce the attention operation complexity. 
GNNs, however, allowed for better modeling of dependencies between time series variables by treating them as graph nodes, making them particularly suitable for capturing spatio-temporal patterns. For example, AGCRN \cite{bai2020adaptive} introduced an adaptive graph convolution mechanism to dynamically adjust the graph structure based on inter-series relationships, while MTGNN \cite{wu2020connecting} combined graph convolutions with temporal convolutional layers to jointly learn spatial-temporal dependencies.

\textbf{Frequency Domain Models for Time Series Forecasting }
A recent line of work attempts to solve the TFS problem in the frequency domain \cite{FrequencyTransformationSurvey}, with the purpose of revealing patterns that may be hidden in the time domain. The FEDformer \cite{fedformer} uses a Fourier-based framework to separate trend and seasonal components by leveraging the Fourier Transform on sub-sequences, allowing it to isolate periodic patterns more effectively. ETSformer \cite{estformer} combines exponential smoothing and applies attention in the frequency domain to enhance seasonality modeling by capturing both short- and long-term dependencies. In FiLM \cite{zhou2022film}, Fourier projections are used to reduce noise and emphasize relevant features. Additionally, SFM \cite{zhang2017SFM} and StemGNN \cite{cao2020stemgnn} utilize frequency decomposition and Graph Fourier Transforms to handle complex temporal dependencies in multivariate time series. FRETS \cite{FreTS} extends this approach by proposing frequency-domain MLPs to learn complex relationships between real and imaginary components of the FFT. FREQTSF \cite{FREQTSFAttention} uses STFT with attention mechanisms to capture temporal patterns across overlapping time windows. 
While frequency models, and specifically the recent use of STFT, have shown significant improvement in TFS performance, each STFT window is often processed separately, ignoring the strong correlations between adjacent windows.

\textbf{Hyper-complex Numbers}
HC numbers extend the complex number system to higher dimensions \cite{hamilton1844quaternions}. Base-$4$ HC numbers, have been widely used in computer graphics to model $3D$ rotations \cite{quaternion_classification}. Base-$8$ HC numbers have been explored in image classification and compression \cite{quaternion_classification,quaternion_compression}, developing an HC network that showed favorable performance on popular datasets.
The merit of HC numbers to extract relevant information in time-series was explored in  \cite{saoud2020metacognitive}, in which an HC-net was used to analyse brain-wave data, and in \cite{kycia2024hypercomplex}, which explored HC-network for financial data.
In this work, we explore the utility of HC architectures for the efficient processing of STFT windows in the frequency domain.




\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{highdim.pdf}
    \caption{Window Mixing mechanism. An input $X$ is transformed into a set of $p$ STFT windows which are transformed to the frequency domain and are then fed into the WM-MLP, which aggregates adjacent windows. The WM-MLP outputs are then transformed back to the time domain via a real STFT, from which the prediction (red) is obtained.
    }
    \label{fig:High-Dim-Learner}
    \vspace{0cm}
\end{figure}


\section{Proposed Model : FIA-Net}
\label{app:METHODOLOGY}
In this section, we describe FIA-Net, a TSF model that leverages shared information between STFT windows. We begin by discussing the existing gap in current frequency domain TSF methods, followed by a brief introduction to frequency domain MLPs \cite{yi2022cost}.
We then outline the FIA-Net components, presenting the novel complex MLP backbone, discussing a simple frequency compression step that reduces the MLP input dimension, and outline the complete model.





\textbf{Motivation}

Even though most real-world time-series data is nonstationary, it may adhere to a piecewise stationary structure, as observed in speech signals \cite{speeach_stat} and financial data \cite{finanal_market_stat}.
This local stationarity allows us to partition the series into stationary correlated STFT subsequences that can be transformed in the frequency domain. The correlation between the STFT sequences has been efficiently utilized in recent works, even though, as we later show, it affects the downstream model accuracy in the task of time prediction.


\begin{wrapfigure}{r}{0.3\textwidth} % Right side, 40% width of text
    \vspace{0cm}
    \centering
     % Adjust vertical spacing if needed
    \includegraphics[trim={20pt 42pt 20pt 30pt}, clip,scale=0.5]{complexmul.drawio.pdf}
    % \includegraphics[width=0.3\linewidth]{ComplexMul.drawio (2).pdf}
     % Adjust spacing between figure and caption if necessary
     % \vspace{-0.05cm}
     % \captionsetup{justification=raggedleft}
     % \centering
    \captionsetup{justification=centering} % Center only this caption     
   \caption{FD-MLP architecture.}
    \label{fig:Complex Mul Learner}
    \vspace{0cm}
\end{wrapfigure}
\textbf{Frequency Domain MLPs} \\
As we handle complex-valued data, we adopt the frequency domain MLP (FD-MLP) unit from \cite{FreTS}. The FD-MLP generalizes the simple neuron to operate with complex-valued weights and biases. Incorporating complex MLPs has been shown to improve the model performance as it aligns better with the geometrical structure induced by the complex plane. The FD-MLP unit is visualized in Figure \ref{fig:Complex Mul Learner}. In Section \ref{sec:hc-mlp}, we will discuss the expansion of the FD-MLP for hyper-complex numbers.




\subsection{Adjacent Information Aggregation}
\label{sec:WMMLP}

% \dt{verify we motivate relevant information}
Consider a sequence \( X = \{x_{1}, \ldots, x_{L}\} \in \mathbb{R}^{D \times L} \) where \( x_i \in \mathbb{R}^D \), \( L \) is the sequence length, which we refer to as the lookback size, and \( D \) is the latent space dimension. Our objective is to predict the next \( T \) elements of the sequence \( \hat{X} = \{\hat{x}_{L+1}, \ldots, \hat{x}_{L+T}\} \in \mathbb{R}^{D \times T} \), where \( T \) is a predetermined prediction horizon.
We are interested in processing $X$ in the frequency domain. We utilize the STFT, which partitions $X$ into $p$ windows and applies the FFT separately to each window. In addition, we exploit the real-valued inputs to perform a Real STFT, which results in half the frequency coefficients. The STFT for the $i$-th window is defined as:
\begin{equation}
    \stft\{X\}(\omega, \tau_i) = \sum_{t=1}^{L} x_t w(t - \tau_i) e^{-j \omega t},
\end{equation}

Where, $w(t - \tau_i)$ is the window function centered at the location of the $i$-th window ($i \in \{1, \dots, p$\}), $\omega$ represents the angular frequency, and $j$ satisfies $j^2 = -1$. Each window is defined by its center $\tau_i$ and has a size of $\frac{\nfft}{2} + 1$.
The output of the STFT consists of $p$ windows, each producing a spectrum of length $\frac{\nfft}{2} + 1$.
We propose the window mixing MLP (WM-MLP), which adapts the FD-MLP to properly aggregate neighboring STFT windows to incorporate shared information.
Given a set of complex transformed windows $\{C_1,\dots,C_p\}$, the WM-MLP operates on the $i$th window $C^{\mathsf{in}}_i$ as follows:
\begin{equation}
C^{\mathsf{out}}_i = \sigma \left( C^{\mathsf{in}}_i W_{i\to i} + C^{\mathsf{in}}_{i-1} \overline{W}_{(i-1) \to i} + C^{\mathsf{in}}_{i+1} \overline{W}_{(i+1) \to i} + B_{i} \right) 
\end{equation}
where $\sigma(\cdot)$ is an activation function, $(W_{(i-1)\to i},W_{i\to i},W_{(i+1)\to i})_{i=1}^p$ are the WM-MLP weight matrices with $C_j$ being a matrix of zeros for $j\notin\{1,\dots,p\}$, and $(B_i)_{i=1}^p$ are the WM-MLP bias vectors, and $\overline{W}$ is the elementwise complex conjugate of $W$.
The outputs of the WM-MLP are transformed back to the time domain using the element-wise inverse STFT, which is given by:
\begin{equation}
    \istft\{X^{F}(w,\tau_i )\}(t) = \sum_{\omega} X^F(\omega, \tau) e^{j \omega t} w(t - \tau_i)
\end{equation}


The STFT, WM-MLP operation, and inverse transform are depicted by Figure \ref{fig:High-Dim-Learner}. In highly nonstationary data, energy transition between adjacent windows can be sharp. To that end, we introduce a minor overlap between adjacent windows of $N_{FFT} - \frac{L - N_{FFT}}{p-1}$, which implicitly adjusts their statistics prior to processing by the TSF model by increasing the inter-window correlations.





\begin{figure*}[!t]
    \centering
    \makebox[1\textwidth]{ % Ensures the figure is centered horizontally
        \rotatebox{270}{ % Rotates the figure 90 degrees clockwise
            \includegraphics[trim={20pt 50pt 20pt 50pt}, clip,scale=1.6]{model_diag.pdf}
        }
    }
    % \vspace{-1cm}
    \caption{FIA-Net Model: The input, denoted $X$, is first fed into the embedding layer, resulting in $X_E$, which is transformed to the frequency domain via the STFT. We then extract the top-$M$ components of each STFT window and feed the compressed windows through the WM-MLP. The MLP outputs are then passed through position-aware zero padding, whose outputs are transformed back to the time domain and summed with $X_E$ via skip connection. The model output $\hat{X}$ is then given by applying a linear transformation.
    }
    \label{fig:model_diagram}
\end{figure*}

\subsection{Implementation Details and Complete System}
% Herein, we describe additional implementational considerations that improved the overall performance and efficiency of our method.

\textbf{Selective Frequency Compression}
To reduce the input dimensionality to the WM-MLP, we compress each transformed window \( C_i \in \mathbb{C}^{ N_{\mathsf{FFT}} \times D} \) along the frequency axis. Specifically, we select the top \( M \) frequency components based on their real and imaginary values across each dimension and denote the compressed window with \( C_i^M \). Then, $(C_1^M,\dots,C_p^M)$ is fed into the WM-MLP layer.
The top-\( M \) procedure is given by
\begin{equation}\label{eq:freq_compress}
    C^M_i = \underset{j=1,\dots,M}{\text{Top-M}} |C_{i,j}|_\CC
\end{equation}
where \( C_{i,j} \) is the \( j \)th component of \( C_i \) and \( |z|_\CC \) is the magnitude of \( z \in \CC \). Additionally, we store the top component indices of \eqref{eq:freq_compress} in a list $\cI(i)$, which encodes the band from which the information came.
% In addition the top-$M$ components, we store their corresponding positions, which encode the band from which the information came from.
To transform the WM-MLP output $C^\mathsf{out}_i$ back to the time domain, we perform a \textit{position-aware} zero padding, which adds $\nfft-M$ zeros while placing the nonzero components in their original indices, which correspond to the original frequency bands, i.e.,
\[
C_{i,j}^{\text{padded}} = \begin{cases}
    C^\mathsf{out}_{i,j},\quad j\in\cI(i)\\
    0,\quad\qquad \text{else.}
\end{cases} 
% \text{ZeroPad}(C_{i}^{'} , \{1, \dots, N_{\mathsf{FFT}}\} \setminus \{ C_{i,1}, \dots, C_{i,M} \}).
\]
In Section \ref{app:results topM}, we demonstrate that, in addition to improving computational efficiency, this frequency compression procedure enhances the performance of downstream TSF tasks. The selection of top-\( M \) components allows us to reduce the model's complexity while maintaining the most relevant frequency information.


% \begin{align*}
%     \{C_{i,1}, \dots, C_{i,M} \} &= \text{argtop-M}_{k \in \{1, \dots, M\}} \left( \text{Amp}(C^{\text{Re}}_{i,k}, C^{\text{Im}}_{i,k}) \right),\\
%     C_{i-\mathsf{Real}}^{'} = \text{Concat}_{m \in \{1, \dots, M\}} \left( C_{i,m} \right), \quad
%     &C_{i-\text{Img}}^{'} = \text{Concat}_{m \in \{1, \dots, M\}} \left( C_{i,m} \right),\\
%     C_{i}^{'} &= C_{i-\mathsf{Real}}^{'} + j C_{i-\text{Img}}^{'}.
% \end{align*}
% \[
% \{ C_{i,1}, \dots, C_{i,M} \} = \text{argtop-M}_{k \in \{1, \dots, M\}} \left( \text{Amp}(C^{\text{Re}}_{i,k}, C^{\text{Im}}_{i,k}) \right),
% \]
% \[
% C_{i-\mathsf{Real}}^{'} = \text{Concatenate}_{m \in \{1, \dots, M\}} \left( C_{i,m} \right),
% \]
% \[
% C_{i-\text{Img}}^{'} = \text{Concatenate}_{m \in \{1, \dots, M\}} \left( C_{i,m} \right),
% \]
% \[
% C_{i}^{'} = C_{i-\mathsf{Real}}^{'} + j C_{i-\text{Img}}^{'}.
% \]

% To transform the compressed signal back to the time domain, we pad the processed window with \( N_{\mathsf{FFT}} - M \) zeros, ensuring that the original positional information is preserved by placing the components back into their original positions. This is given by \dt{rewrite}


\textbf{Complete Model }
The complete FIA-Net, as shown in Figure \ref{fig:model_diagram}, operates as follows: Given an input $X\in\RR^{B\times L\times D}$, the dimension of $X$ is expanded through a learned embedding layer, resulting in $X_E\in\RR^{B\times L\times D\times E}$. This expanded representation is then fed into an STFT block that uses the real input to perform $\rstft$ on $X_E$.
The transformed signal is passed through the SM block, whose output is further processed by the WM-MLP. The WM-MLP outputs are subsequently padded and transformed back to the temporal axis, where they are integrated with \(X_E\) via a skip connection and resized to the desired output sequence shape using a two-layer MLP decomposition.
% \begin{remark}[Model Complexity]
%     The forward pass complexity of the WM-MLP is governed by the STFT complexity, which is $O(L\log(\frac{L}{p}))$. Thus, we introduce reduced complexity compared to the transformer-based methods, which introduce elaborate methodologies to reduce the $O(L^2)$ attention complexity to $O(L\log L)$.
% Furthermore, by applying top-$M$ frequency selection we reduce the forward pass in the frequency domain, along with the corresponding MLP size.
% \end{remark}

\textbf{Model Complexity }
The forward pass complexity of the WM-MLP is primarily determined by the STFT complexity, which is $O(L \log(\frac{L}{p}))$. This represents a significant reduction in complexity compared to transformer-based methods, which employ intricate mechanisms to reduce their $O(L^2)$ attention complexity to $O(L \log L)$. Additionally, the application of top-$M$ frequency selection further optimizes the forward pass in the frequency domain, reducing both computational demands and the corresponding MLP size. A detailed analysis of these complexities is provided in Table \ref{tab:complexity_analysis}.

% \dt{
% Mention:
% \begin{enumerate}
%     \item Embedding in the entrance
%     \item Choosing Top-M frequencies to reduce the overall dimensionality
%    \item Resize Block
%     \item Using ComplexMul (which we draw upon to derive the \textbf{hypercomplexMul} - I think I like that name)
%     \item The entire model
%     \item refer to figure 1, refer to dimensions tabl once we have
% \end{enumerate}
% }




% \medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip
% \medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip

% In this section, we introduce YakirFormer, depicted in Figure \ref{fig:model_diagram}, which features a modular architecture specifically designed to model energy shifts between adjacent windows in the frequency domain. The model consists of four main components: a token embedding block that encodes the input data parameters, $\stft$ and $\istft$ blocks that perform frequency modulation, and a high-dimensional learning block. This learning block captures the energy shifts between frequency domain windows. Additionally, this work proposes a method that leverages hypercomplex numbers to reduce the number of parameters while maintaining performance.
% \begin{figure*}[!t]
%     \centering
%     \makebox[\textwidth]{ % Ensures the figure is centered horizontally
%         \rotatebox{270}{ % Rotates the figure 90 degrees clockwise
%             \includegraphics[width=0.45\textwidth]{model_diag.drawio.pdf}
%         }
%     }
%     \caption{The input, denoted as $X$, is first fed into the embedding layer, resulting in $X_E$. The embedded input is then passed through the $\stft$. Each window from the $\stft$ is processed by the SM block, which selects the top $M$ frequency bins based on amplitude. These selected bins are subsequently passed into the high-dimensional learner. The learned windows are padded back to their original size using zeros and transformed back into the time domain.\dt{check}}
%     \label{fig:model_diagram}
% \end{figure*}

% \paragraph{Problem Statement}
% Given a set of data points \( X = \{x_{t_1}, \ldots, x_{t_D}\}_{t=1}^{L} \in \mathbb{R}^{D \times L} \) representing a lookback window in a time series, where \( L \) is the window size, \( D \geq 1 \) is the number of variables, and \( x_{t_i} \) denotes the value of the \( i \)-th variable at time step \( t \). The objective of Time Series Forecasting (TSF) is to predict the future values \( \hat{X} = \{\hat{x}_{t_1}, \ldots, \hat{x}_{t_D}\}_{t=L+1}^{L+T} \in \mathbb{R}^{D \times T} \) within a forecasting horizon. In this paper, we focus on multi-step forecasting, where \( T \in \{96, 192, 336, 720\} \).

% The Yakir model processes time series data by transforming it between the time and frequency domains. Formally, let the input time series data be represented as:
% \[
% X_{\text{Input}} \in \mathbb{R}^{B \times N \times D}
% \]
% where \( B \) is the batch size, \( N \) the number of time steps, and \( D \) the input dimension. The data is first passed through an embedding layer:
% \[
% X_E = \text{Embedding}(X_{\text{Input}}) , \quad X_E \in \mathbb{R}^{B \times N \times D \times E}
% \]
% where \( E \) is the embedding dimension. The embedded data is then divided into \( p \) windows and subjected to the Real Short-Time Fourier Transform ($\rstft$) using Han window function:
% \[
% \{ C_{i} \}_{i=1}^{p} = \rstft(X_E) , \quad C_{i} \in \mathbb{R}^{B \times N_{FFT} \times D \times E}
% \]
% where \( C_i \) represents the Fourier-transformed data for the \( i \)-th window. The top \( M \) frequencies are selected from each window:
% \[
% C_i' = \text{SelectTopM}(C_i), \quad C_i' \in \mathbb{R}^{B \times D \times M \times E}, \quad M \ll N_{FFT}
% \]
% Next, the Frequency Shift Learner is applied to capture complex frequency shifts. The High-Dimensional Learner operates as:
% \[
% (C_1^{*}, \ldots, C_p^{*}) = \text{HighDimLearner}(C_1^{'}, \ldots, C_p^{'})
% \]
% A padding layer ensures consistency:
% \[
% C_i^{\dagger} = \text{Pad}(C_i^{*}) , \quad C_i^{\dagger} \in \mathbb{R}^{B \times N_{FFT} \times D \times E}
% \]
% The Inverse Short-Time Fourier Transform ($\istft$) is applied to reconstruct the signal in the time domain:
% \[
% X_{\text{Rec}} = \istft(C_1^{\dagger}, \ldots, C_p^{\dagger}) , X_{Rec} \in \mathbb{R}^{B \times N \times D \times E} 
% \]
% Finally, a resizing block adjusts the output dimensions:
% \[
% X_{\text{output}} = \text{Resize}(X_{\text{Rec}} + X_E), \quad X_{\text{output}} \in \mathbb{R}^{B \times T \times D}
% \]





% \subsection{Embedding Layer}
% \ey{we need to talk about the figure}
% \ey{we need to talk about this section, i dosent look good in my opnion and i dont have any idea how to fix it}

% The embedding layer comprises a set of weights of size \( W_E \in \mathbb{R}^{E} \). 
% Let \( X_1 = \text{Embed Layer}(X) = X \circledast W_E \) (where the operation is performed along the last dimension). 
% Thus, \( X_1 \in \mathbb{R}^{B \times N \times D \times E} \).


% \subsection{Frequency Transform Module}
% The Frequency Transform Module decomposes the input signal into its frequency components using the Real Short-Time Fourier Transform ($\rstft$):
% \[
% C_{p,k} = \sum_{t=1}^{L} x_t w_p(t - \tau_p) e^{-j \omega_k t},
% \]
% \[
% \rstft\{X\}(\omega_k, \tau_p) = (C_1, C_2, \ldots, C_p)
% \]
% In this formulation, \( X \) represents the input signal in the time domain, and \( w_p(t - \tau_p) \) denotes the window function centered at position \( \tau_p \) for the \( p \)-th window. The angular frequency \( \omega_k = \frac{2\pi k}{N_{\text{FFT}}} \) corresponds to the \( k \)-th frequency bin, where \( N_{\text{FFT}} \) defines the number of points used in the Fourier transform, thereby determining the frequency resolution.
% The $\rstft$ outputs frequency components \( \{ C_{i} \}_{i=1}^{p} \) for each window, capturing all angular frequencies \( \omega_k \). This decomposition provides a localized view of the signalâ€™s frequency content across different windows, with \( N_{\text{FFT}} \) controlling the granularity of the frequency analysis.


% \subsection{Select Top M (S-M)}

% To reduce computational complexity, we select the top-\( M \) most significant real and imaginary components for each window. For a given window input \( C_i \), the output is defined as:

% \[
% \{
% C^{\text{Re}}_{i,1}, \dots, C^{\text{Re}}_{i,M}
% \}, \{
% C^{\text{Im}}_{i,1}, \dots, C^{\text{Im}}_{i,M}
% \} = \text{argtop-M}_{k \in \{1, \dots, M\}}
% \left( \text{Amp}(C^{\text{Re}}_{i,k}, C^{\text{Im}}_{i,k}) \right),
% \]

% \[
% C_{i-Real}^{'} = \text{Concatenate}_{m \in \{1, \dots, M\}} \left( C^{\text{Re}}_{i,m} \right),
% \]

% \[
% C_{i-Img}^{'} = \text{Concatenate}_{m \in \{1, \dots, M\}} \left( C^{\text{Img}}_{i,m} \right),
% \]

% \[
% C_{i}^{'} = C_{i-Real}^{'}+ j C_{i-Img}^{'}
% \]
% where \( \text{Amp}(C^{\text{Re}}_{i,k}, C^{\text{Im}}_{i,k}) \) represents the amplitude of the real and imaginary components of \( C_{i,k} \), and \( \text{argtop-M} \) selects the indices corresponding to the top-\( M \) amplitudes.


% Placing the figures at the top of the page



% \subsection{Learnable Complex Mul}

% This block, adopted from the FreTS model \cite{FreTS}, replaces all instances of complex multiplication in our model. The operation is formalized as follows:

% Given two complex numbers \( A = a_{\mathsf{Real}} + j a_{\mathsf{Imag}} \) and \( B = b_{\mathsf{Real}} + j b_{\mathsf{Imag}} \), the complex multiplication is computed as:
% \[
% A \times B = \sigma(a_{\mathsf{Real}} \cdot b_{\mathsf{Real}} - a_{\mathsf{Imag}} \cdot b_{\mathsf{Imag}}) + j \sigma(a_{\mathsf{Real}} \cdot b_{\mathsf{Imag}} + a_{\mathsf{Imag}} \cdot b_{\mathsf{Real}})
% \]
% where \( \sigma \) denotes the activation function applied to the real and imaginary components.
% This block is essential in modeling the interactions between complex-valued signals, ensuring that both real and imaginary components are processed through the activation function, enhancing non-linearity.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.5\textwidth]{Complex_Mul_Diag.drawio.pdf}
%     \caption{FD-MLP: $X_{\mathsf{Real}}$ represents the real part of the input, while $X_{\mathsf{Imag}}$ corresponds to the imaginary part. In this block, the red arrows indicate the imaginary values and the green arrows represent the real values. The module performs complex multiplication between $X = X_{Real} + j X_{Imag}$ and $Y = Y_{Real} + j Y_{Imag}$, applying the sigmoid function to both the real and imaginary components of the result.}
%     \label{fig:Complex Mul Learner}
% \end{figure}

% \subsection{High Dim Frequency Learner}
% \ey{figure describing.}
% Assuming that energy can transfer between adjacent windows, the objective of this layer is to model such energy transitions. Given a collection of windows \([C'_1, \dots, C'_N]\) and their corresponding self-layer weights \([W_{1 \to 1}, \dots, W_{N \to N}]\), as well as the right and left shift weights \([W_{1 \to 2}, W_{2 \to 3}, \dots, W_{N-1 \to N}]\) and \([W_{2 \to 1}, W_{3 \to 2}, \dots, W_{N \to N-1}]\), where \( \mathbf{w}_i = \mathbf{w}_{i,\mathsf{Real}} + j\mathbf{w}_{i,\text{img}} \in \mathbb{C}^{E\times E}\), and a set of bases \( \mathcal{B} = [\mathcal{B}_{1}, \mathcal{B}_{2}, \mathcal{B}_{3}, \mathcal{B}_{4}]\), with \( \mathcal{B}_i = \mathcal{B}_{i,\mathsf{Real}} + j\mathcal{B}_{i,\text{img}} \in \mathbb{C}^{E}\).. the transformation is defined as follows:
% \[
% C^*_1 = \sigma \left( C'_1 W_{1\to 1} + C'_2 \overline{W_{2\to 1}} + \mathcal{B}_{1} \right)
% \]
% \[
% C^*_i = \sigma \left( C'_i W_{i\to i} + C'_{i-1} \overline{W_{i-1 \to i}} + C'_{i+1} \overline{W_{i+1 \to i}} + \mathcal{B}_{i} \right) \quad \text{for} \quad 1 < i < N
% \]
% \[
% C^*_N = \sigma \left( C'_N W_{N\to N} + C'_{N-1} \overline{W_{N-1 \to N}} + \mathcal{B}_{N}\right)
% \]
% Here, the multiplication is defined as complex multiplication, and \(\sigma\) represents the activation function.


% \subsection{Frequency Inverse Transform Module}

% The Frequency Inverse Transform Module is responsible for reconstructing the original time-domain signal. First, each window is padded to its original size, followed by the application of the inverse transform.

% \[
% C_{i}^{\dagger} = \text{padd}(C_{i}^{*})
% \]
% The reconstructed signal \( x' \) is then obtained using the Inverse Real-Short-Time Fourier Transform ($\irstft$):
% \[
% x' = \text{IR$\stft$}\left\{\{C_{i}^{\dagger}\}_{i=0}^p\right\} = \sum_{\omega} \sum_{i=0}^{p} C_i^{\dagger} e^{j \omega t} w(t - \tau),
% \]
% where \( C_i^{*} \) hold the the frequency components for the it'h window, \( w(t - \tau) \) is the window function, and \( \omega \) denotes the angular frequency.


% \subsection{Resize Block}
% The purpose of this layer is to transform the processed signal into the required dimensions while refining the features extracted by the frequency shift learner. The block consists of two fully connected layers, denoted as \( F_1 \) and \( F_2 \). The first layer, \( F_1 \), maps the input from size \( N_{\text{time}} \times E \) to the hidden size \( H \), while the second layer, \( F_2 \), transforms the output from the hidden size \( H \) to the target size \( O \). Here, \( H \) represents the hidden size, which is a tunable hyperparameter.

% The operation of the Resize Block applied to input \( x \) can be formalized as follows:

% \[
% \text{Resize Block}(x) = F_2(\text{ReLU}(F_1(x)))
% \]

% where ReLU represents the rectified linear unit activation function applied between the two fully connected layers.





\section{Window Aggregation via Hyper-Complex Models}\label{sec:hc-mlp}
Even though the WM-MLP backbone integrates valuable information that benefits the FIA-Net's accuracy, information is not only shared between two adjacent STFT windows. In fact, the stronger the dependencies on the long-term past, the more information is shared between two distant windows on the frequency axis.
Ideally, we would like to aggregate information between all $p$ STFT windows. Unfortunately, a straightforward extension of the WM-MLP requires $O(p^2)$ weight matrices, which may impair the training procedure and increase model complexity.
To address that, we interpret the set of windows as an HC vector and propose an HC-based MLP that efficiently processes the set of STFT windows.
We begin with a short introduction on HC-algebras, followed by the construction of the proposed MLP backbone for the FIA-Net.
% \dt{The HC number system depends on a complex base $q$ (is this the name) parameter. }


\subsection{Hyper-complex Numbers}
\label{sec:HCMLP}
HC numbers generalize the complex field by introducing additional dimensions while maintaining algebraic properties. HC number systems are defined by a parameter $q$ that determines the number of components in the number system.
Complex numbers can thus be viewed as an HC number with $q=2$, and an HC number of base $q$ can be represented with $p=q/2$ complex numbers.
In what follows, we focus on HC numbers with $p=4$, termed Octonions $\OO$, whose elements are denoted $o=(\alpha_1,\alpha_2,\alpha_3,\alpha_4)\in\OO$, with $\alpha_i\in\CC$ for $i=1,\dots,4$. Additional discussion on $p\neq 4$ is given in Appendix \ref{app:Hypercomplex Appendix}.

The addition of two Octonions, \( o_1 = (\alpha_1, \dots, \alpha_4) \) and \( o_2 = (\beta_1, \dots, \beta_4) \), is given by their componentwise sum, while their multiplication follows the Cayley-Dickson construction \cite{function_theory_cayley_dickson}. The product \( o_3 = o_1 \cdot o_2 = (\gamma_1, \gamma_2, \gamma_3, \gamma_4) \) is given by:
% \begin{align*}
%     O_1 \cdot O_2 =
%     & \Big( (\alpha_1 \beta_1 - \alpha_2 \overline{\beta_2} - \alpha_3 \overline{\beta_3} - \alpha_4 \overline{\beta_4}),\quad
%      (\alpha_2 \beta_1 + \alpha_1 \beta_2 + \alpha_4 \overline{\beta_3} - \alpha_3 \overline{\beta_4}), \\
%     & (\alpha_3 \beta_1 - \alpha_4 \beta_2 + \alpha_1 \beta_3 + \alpha_2 \beta_4) ,\quad (\alpha_4 \beta_1 + \alpha_3 \beta_2 - \alpha_2 \beta_3 + \alpha_1 \beta_4)\Big)
% \end{align*}
\begin{equation}  
     \begin{aligned}
    & \gamma_1 = \alpha_1 \beta_1 - \alpha_2 \overline{\beta_2} - \alpha_3 \overline{\beta_3} - \alpha_4 \overline{\beta_4}\\
     &\gamma_2=  \alpha_2 \overline{\beta}_1 + \alpha_1 \beta_2  + \alpha_3 \overline{\beta_4}
     - \alpha_4 \overline{\beta_3} \\
    &\gamma_3 = \alpha_3 \overline{\beta}_1 + \alpha_4 \overline{\beta}_2 + \alpha_1 \beta_3 - \alpha_2 \overline{\beta}_4\\
     &\gamma_4 = \alpha_4 \overline{\beta}_1  + \alpha_2 \overline{\beta}_3 + \alpha_1 \beta_4
     - \alpha_3 \overline{\beta}_2
    \end{aligned} 
\end{equation}

Hyper-complex numbers exhibit additional properties such as closed-form expressions for norm calculations and norm preservation for specific bases. For completeness, we provide additional information on HC-numbers in Appendix \ref{app:Hypercomplex Appendix}, where the proposed MLP is presented under specific bases.
% and the squarred norm\dt{magnitude?} of an Octonion is given by $|O|^2=\sum_{i=1}^4|\alpha_i|^2$.
% One property attained by the HC generalization is norm preservation under multiplication, i.e., for $o_1,o_2\in\OO$ we have
% \begin{equation}
% |o_1 \cdot o_2| = |o_1| \cdot |o_2|.
%     \label{eq:octonion_norm_preserve}  
% \end{equation}
% \dt{In what follows we use the energy presevation to build intuition to the proposed HC-MLP.}
% Hypercomplex numbers generalize complex numbers to higher dimensions, introducing systems like quaternions, octonions, and sedenions, each with unique algebraic properties. Quaternions ($\mathbb{H}$) extend complex numbers to four dimensions, octonions ($\mathbb{O}$) extend quaternions to eight dimensions, and sedenions ($\mathbb{S}$) extend octonions to 16 dimensions. In this paper, we will represent each hypercomplex number system using complex numbers: quaternions as two complex numbers, octonions as four, and sedenions as eight.
% The arithmetic of octonions is defined by specific addition and multiplication rules. Given two octonions \( O_1 = (\alpha_1, \alpha_2, \alpha_3, \alpha_4) \) and \( O_2 = (\beta_1, \beta_2, \beta_3, \beta_4) \)
% their addition is simply given by the addition of their components, while their multiplication follows the Cayley-Dickson construction\dt{cite}:

% \begin{equation}
%     O_1 \cdot O_2 = 
%     \begin{aligned}
%     & (\alpha_1 \beta_1 - \alpha_2 \overline{\beta_2} - \alpha_3 \overline{\beta_3} - \alpha_4 \overline{\beta_4}, \\
%     & \alpha_2 \beta_1 + \alpha_1 \beta_2 + \alpha_4 \overline{\beta_3} - \alpha_3 \overline{\beta_4}, \\
%     & \alpha_3 \beta_1 - \alpha_4 \beta_2 + \alpha_1 \beta_3 + \alpha_2 \beta_4, \\
%     & \alpha_4 \beta_1 + \alpha_3 \beta_2 - \alpha_2 \beta_3 + \alpha_1 \beta_4)
%     \end{aligned}
% \end{equation}
% The norm of an is given by:
% \begin{equation}
%     \quad |O| = \sqrt{|\alpha_1|^2 + |\alpha_2|^2 + |\alpha_3|^2 + |\alpha_4|^2}
%     \label{eq:octonion_norm}  % This is the correct way to label the equation
% \end{equation}
% Norm preservation under multiplication is valid for octonions.
% \begin{equation}
% |O_1 \cdot O_2| = |O_1| \cdot |O_2|
%     \label{eq:octonion_norm_preserve}  
% \end{equation}
% We provide additional details about Hypercomplex number and discuss higher bases in Appendix \ref{app:hypercomplex}

\subsection{Hyper-Complex MLP}


\begin{figure}[t!]
    \centering
    \includegraphics[trim={20pt 30pt 20pt 30pt}, clip,scale=0.7]{octmlp.pdf} % Resize figure to 80% of text width
    \caption{HC-MLP operating on $C^{\mathsf{in}} = (C^{\mathsf{in}}_1,C^{\mathsf{in}}_2,C^{\mathsf{in}}_3,C^{\mathsf{in}}_4)$, implementing the HC multiplication (\eqref{eq:hc_mult8}).
    Each output unit is the sum of the corresponding inner blocks of the same color, where a $\bigoplus$ symbol denotes complex addition and a $\bigotimes$ denotes complex multiplication. A red outline denotes minus multiplication, and a blue input arrow denotes complex conjugation.
    }
    \label{fig:HyperComplexMLP}

\end{figure}
% The WM-MLP leverages shared information between adjacent STFT windows. 
The longer the range of temporal dependencies in the data, the more shared information there is between gathered windows. In such cases, the WM-MLP, which incorporates short-term information in the frequency domain, might fail to capture long-term dependencies. To that end, our goal is to increase the extent to which information is shared across the STFT windows. To derive a parameter-efficient solution, we incorporate HC algebra into the frequency domain learning procedure.

Assume that we are given $p=4$ complex-valued STFT windows $(C^{\mathsf{in}}_i \in \CC^{B \times M \times E})_{i=1}^4$, where the second axis is the transformed frequency domain after top-$M$ frequency component selection.
We treat the set of windows as a single Octonion tensor $(C^{\mathsf{in}}_1, C^{\mathsf{in}}_2, C^{\mathsf{in}}_3, C^{\mathsf{in}}_4) \in \OO^{B \times M \times E}$ and feed it through an HC-valued MLP, whose output is $C^{\mathsf{out}} = \sigma(C^{\mathsf{in}} \cdot W + B)$. For $C^{\mathsf{out}} = (C^{\mathsf{out}}_1, C^{\mathsf{out}}_2, C^{\mathsf{out}}_3, C^{\mathsf{out}}_4)$, it is given by:
\begin{equation}\label{eq:hc_mult8}
\begin{aligned}
    C^{\mathsf{out}}_1 &= \sigma(C^{\mathsf{in}}_1 W_1 - C^{\mathsf{in}}_2 \overline{W}_2 - C^{\mathsf{in}}_3 \overline{W}_3 - C^{\mathsf{in}}_4 \overline{W}_4 + {B}_{1}),\\
    C^{\mathsf{out}}_2 &= \sigma(C^{\mathsf{in}}_2 \overline{W}_1 + C^{\mathsf{in}}_1 W_2 -C^{\mathsf{in}}_4 \overline{W}_3 + C^{\mathsf{in}}_3 \overline{W}_4 + {B}_{2}),\\
    C^{\mathsf{out}}_3 &= \sigma(C^{\mathsf{in}}_3 W_1 + C^{\mathsf{in}}_1 \overline{W}_3 -C^{\mathsf{in}}_2 \overline{W}_4 +C^{\mathsf{in}}_4 \overline{W}_2 +{B}_{3}),\\
    C^{\mathsf{out}}_4 &= \sigma(C^{\mathsf{in}}_4 \overline{W}_1 + C^{\mathsf{in}}_1 W_4 - C^{\mathsf{in}}_3 \overline{W}_2 + C^{\mathsf{in}}_2 \overline{W}_3 + {B}_{4}).
\end{aligned}
\end{equation}
where $W=(W_1,\dots,W_4)\in\OO^{E \times E}$, $B=(B_1,\dots,B_4)\in\OO^{E \times 1}$ are the HC-MLP weights and bias, respectively, and $\sigma$ is a standard activation function, e.g., ReLU. We stress that, as considered in the complex MLP from \cite{FreTS}, the HC-MLP is implemented with real-valued operations, which allows it to plug into every existing automatic differentiation scheme over standard GPUs.
The HC-MLP unit is depicted in Figure \ref{fig:HyperComplexMLP}.

The WM-MLP demonstrates distinct advantages depending on the prediction horizon. For shorter prediction lengths, it achieves better performance by effectively leveraging all available information from adjacent and nearby windows. In contrast, for longer horizons, where only closer temporal information remains relevant, the WM-MLP's ability to aggregate adjusted windows proves to be more effective. This behavior is clearly demonstrated in Section \ref{sec:WM_vs_HC}. Moreover, the HC perspective offers a significant advantage in terms of parameter efficiency. It allows for an implementation with only $p$ weight matrices, whereas the corresponding WM-MLP would require $3p-2$ weight matrices (and even $p^2$ weight matrices for a generalization of the WM-MLP), all while preserving performance. This reduction in parameters becomes increasingly dramatic as $p>4$, as further detailed in Appendix \ref{app:Hypercomplex Appendix}.




% \ey{postion of this therom is not good, eiter say "we wil use it later" or put it where we use it. also the therom it self isnt written well, need to change.}
% \begin{theorem}[Energy Preservation under Octonion Multiplication]
%     \label{theorem:energy_mul_theorem}
%     Let \( x[n] \) be a sequence that is divided into STFT windows, represented as an octonion \( O = (c_1, c_2, c_3, c_4) \), where \( c_i \) are the components of the octonion. Given another octonion \( W = (w_1, w_2, w_3, w_4) \), the energy of their product \( O \cdot W \) is equal to the product of their individual energies, i.e., \( E_{O \cdot W} = E_O \cdot E_W \).
% \end{theorem}


% The Window Memory Network (WMM) assumes that the interdependent effects between adjusted windows are short-term. However, this assumption may not hold for certain real-world datasets, where subsequent windows can influence one another due to long-range dependencies. To better capture this interdependence, we propose an alternative mechanism for the WMM, utilizing hyper-complex number representations, which we refer to as the HyperComplexMLP (HCM). By leveraging hyper-complex algebra in higher-dimensional spaces, this method enables us to model interactions between context windows with greater fidelity, capturing the energy transfer distribution in the frequency domain.
% Theorem \ref{theorem:energy_mul_theorem}, detailed in Appendix \ref{app:proof}, provides a formal proof that the total energy of the predicted signal can be learned, while acknowledging that the distribution of energy across windows may shift. In this paper, we focus on the HyperComplexMLP in base 8, as demonstrated in Figure \ref{fig:HyperComplexMLP}., which we denote as OctonionMLP (OctMLP). Additional variations, such as bases 2, 4, and 16, are also discussed in Appendix \ref{app:hypercomplex}.
% The OctMLP block processes four context windows in the frequency domain, denoted as \( c_1^{in}, \dots, c_4^{in} \), where each window contains \( M \) frequency components of size \( \mathbb{C}^{E \times M \times D \times E} \). The output is the four transformed windows with learned energy distribution across them, where operations such as multiplication and addition adhere to hyper-complex field algebra.
% The transformation is described by:
% \[
% C^{M} = \sigma(C \cdot W + \mathcal{B}),
% \]

% For example, in the case of \( p = 4 \) (octonions), the OctMLP computes:
% \begin{align*}
%     \mathbf{C}_1^{M} &= \sigma(\mathbf{C'}_1 \mathbf{W}_1 - \mathbf{C'}_2 \overline{\mathbf{W}}_2 - \mathbf{C'}_3 \overline{\mathbf{W}}_3 - \mathbf{C'}_4 \overline{\mathbf{W}}_4 + \mathcal{B}_{1}),\\
%     \mathbf{C}_2^{M} &= \sigma(\mathbf{C'}_2 \mathbf{W}_1 + \mathbf{C'}_1 \mathbf{W}_2 + \mathbf{C'}_4 \overline{\mathbf{W}}_3 - \mathbf{C'}_3 \overline{\mathbf{W}}_4 + \mathcal{B}_{2}),\\
%     \mathbf{C}_3^{M} &= \sigma(\mathbf{C'}_3 \mathbf{W}_1 + \mathbf{C'}_1 \mathbf{W}_3 + \mathbf{C'}_2 \overline{\mathbf{W}}_4 - \mathbf{C'}_4 \overline{\mathbf{W}}_2 + \mathcal{B}_{3}),\\
%     \mathbf{C}_4^{M} &= \sigma(\mathbf{C'}_4 \mathbf{W}_1 + \mathbf{C'}_1 \mathbf{W}_4 + \mathbf{C'}_3 \overline{\mathbf{W}}_2 - \mathbf{C'}_2 \overline{\mathbf{W}}_3 + \mathcal{B}_{4}).
% \end{align*}


% This approach provides two key advantages: first, it is better suited for datasets with long-term dependencies, and second, it significantly reduces the number of parameters by nearly a factor of three while retaining, or even improving, performance. In the subsequent sections, we demonstrate the superiority of this method through a series of experiments.





\section{Results and Discussion}
% In what follows, we provide a detailed array of experiments on six real-world datasets. We the FIA-Net performance to several SoTA models, analyzing both accuracy and efficiency.


\subsection{Experimental Setting}
\label{app:results}
\textbf{Datasets }
Following \cite{fedformer,FreTS}, we consider the following representative real-world datasets: \textbf{1) WTH} (Weather), \textbf{2) Exchange} (Finance), \textbf{3) Traffic}, \textbf{4) ECL} (Electricity), \textbf{5) ETTh1} (Electricity transformer temperature hourly), and \textbf{6) ETTm1} (Electricity transformer temperature minutely). The train/validation/test split is 70\%, 15\%, and 15\%, respectively.

%%%% Keep a line between:

%%%%
\textbf{Baselines }
In this research, we followed the TSF SoTA baselines: \textbf{1) FedFormer} \cite{fedformer}, \textbf{2) Reformer} \cite{Reformer}, \textbf{3) FreTS} \cite{FreTS}, \textbf{4) PatchTST} \cite{PatchTST}, \textbf{5) Informer} \cite{Informer}, \textbf{6) Autoformer} \cite{autoformer} and \textbf{7) LSTF-Linear} \cite{Zeng2022AreTE}.

%%%% Keep a line between:

%%%%
\textbf{Experiments setup }
All experiments were conducted using PyTorch \cite{pytorch} on a single RTX 3090, utilizing mean squared error (MSE) loss and the Adam optimizer \cite{kingma2014adam}. We established an initial learning rate of \(10^{-3}\) with an exponential decay scheduler. Hyperparameters were optimized individually for each dataset (see Appendix \ref{app:Implementation Details} for specific details).
We report performance metrics under both root mean squared error (RMSE) and mean absolute error (MAE). Additional information on the Normalization \ref{app:Normalization Methods}, datasets \ref{app:Dataset Descriptions}, and baseline models \ref{app:Baselines Appendix} can be found in the appendix.



\subsection{Main results}
\label{sec:WM_vs_HC}
Table \ref{table:main_results} compares the FIA-Net performance under both the WM-MLP and the HC-MLP backbones with the SoTA baselines. It is evident that the FIA-Net consistently outperforms the baselines on most considered values of prediction horizon $T$, with an average improvement of $5.4\%$ in MAE and $3.8\%$ in RMSE over SoTA models. We note that the performance of the HC-MLP-based network, which is implemented with significantly fewer parameters, achieves comparable results with the corresponding WM-MLP and attains the best results over several settings. We can deduce that the HC-MLP is more suitable for shorter-term prediction, while the WM-MLP backbone is more suitable for longer ranges.


\begin{table*}[h!]

\setlength{\tabcolsep}{3pt}
\sisetup{detect-all}
\NewDocumentCommand{\B}{}{\fontseries{b}\selectfont}
\caption{Forecasting performance comparison across datasets and prediction horizons using RMSE and MAE. Lower values indicate better performance. Bold denotes the best results, and underlined indicates the second-best.}


\resizebox{\textwidth}{!}{\begin{tabular}{
  @{}
  l % First column
  | % Vertical line before second column
  l % Second column
  | % Vertical line before third column
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  S[table-format=1.2]
  @{}
}


\cmidrule(l){1-26}
&& \multicolumn{4}{c}{Weather} & \multicolumn{4}{c}{Exchange} &
\multicolumn{4}{c}{Traffic} & \multicolumn{4}{c}{Electricity} &
\multicolumn{4}{c}{ETTh1} & \multicolumn{4}{c}{ETTm1}
\\
\cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(l){11-14} \cmidrule(l){15-18} \cmidrule(l){19-22} \cmidrule(l){23-26}
& {Metric} & {96} & {192} & {336} & {720} & {96} & {192} & {336} & {720}& {96} & {192} & {336} & {720} &
{96} & {192} & {336} & {720} &
{96} & {192} & {336} & {720} &
{96} & {192} & {336} & {720}\\
\midrule

\multirow{2}{*}{HC-MLP \textbf{(Ours)}} 
& {RMSE} 
& \textbf{{0.069}} & \textbf{{0.079}} & \uline{{0.090}} & \uline{{0.098}}  
& \uline{{0.050}} & \uline{{0.062}} & \uline{{0.078}} & {{0.112}}
& \textbf{0.032} & \uline{0.034} & \uline{0.035} & \textbf{0.036}  
& {0.070} & {0.068} & {0.071} & \uline{0.077}  
& \textbf{{0.083}} & \textbf{{0.085}} & \textbf{{0.094}} & \textbf{{0.101}} 
& \textbf{{0.074}} & \textbf{{0.082}} & \textbf{0.089} & \uline{{0.096}}  

 \\

& {MAE} 
& \textbf{{0.030}} & \textbf{{0.039}} & \textbf{{0.043}} & \uline{{0.054}}  
& \uline{{0.035}} & \uline{{0.049}} & \uline{{0.061}} & {{0.089}}  
& \textbf{0.016} & \uline{0.017} & \uline{0.017} & \textbf{0.018}  
& \uline{0.040} & \uline{0.041} & \uline{0.044} & \textbf{0.049}  
& \textbf{{0.057}} & \textbf{{0.064}} & \textbf{{0.068}} & \textbf{{0.075}}
& \textbf{{0.049}} & \uline{{0.056}} & \uline{{0.060}} & \uline{{0.067}} 
 \\
\midrule
\multirow{2}{*}{WM-MLP \textbf{(Ours)}} 
& {RMSE} 
& {\uline{0.071}} & {\uline{0.081}}& {\textbf{0.089}} & {\textbf{0.097}}  
& \textbf{{0.048}} & \textbf{{0.060}} & \textbf{{0.076}} & \textbf{{0.107}}  
& \uline{{0.033}} & \textbf{{0.033}} & \textbf{{0.034}} & \textbf{{0.036}}  
& \uline{{0.067}} & {{0.068}} & \uline{{0.070}} & \textbf{{0.076}}

& \uline{{0.084}} & \uline{{0.088}} & {{0.097}} & \uline{{0.102}} 
& \uline{{0.076}} & \textbf{{0.082}} & \textbf{{0.089}} & \textbf{{0.094}}  

 \\

& {MAE} 
& {\uline{0.031}} & {{0.041}} & \uline{{0.045}} & \textbf{{0.053}}  
& \textbf{{0.034}} & \textbf{{0.047}} & \textbf{{0.058}} & \textbf{{0.086}}
& \textbf{{0.016}} & \textbf{{0.016}} & \textbf{{0.016}} & \textbf{{0.018}}  
& \textbf{{0.039}} & \uline{{0.041}} & \uline{{0.044}} & \textbf{{0.049}}

& \textbf{{0.057}} & {{0.066}} & {{0.071}} & \textbf{{0.075}} 
& \uline{{0.052}} & \textbf{{0.055}} & \textbf{{0.058}} & \textbf{{0.064}}
 \\
\midrule

% \multirow{2}{*}{Yakir-Octonions (8)} 
% & {RMSE} 
% & \textbf{{0.069}} & \textbf{{0.079}} & \uline{{0.090}} & \uline{{0.098}}  
% & \uline{{0.050}} & \uline{{0.062}} & \uline{{0.078}} & {{0.112}}
% & \textbf{0.032} & \uline{0.034} & \uline{0.035} & \textbf{0.036}  
% & {0.070} & {0.070} & {0.071} & \uline{0.077}  
% & \textbf{{0.083}} & \textbf{{0.085}} & \textbf{{0.094}} & \textbf{{0.097}} 
% & \textbf{{0.072}} & \uline{{0.082}} & \uline{{0.089}} & \textbf{{0.094}}  

%  \\

% & {MAE} 
% & \textbf{{0.030}} & \textbf{{0.039}} & \textbf{{0.043}} & \uline{{0.054}}  
% & \uline{{0.035}} & \uline{{0.049}} & \uline{{0.061}} & {{0.089}}  
% & \textbf{0.016} & \uline{0.017} & \uline{0.017} & \textbf{0.018}  
% & \uline{0.040} & {0.042} & \uline{0.044} & \textbf{0.049}  
% & \textbf{{0.058}} & \textbf{{0.064}} & \textbf{{0.068}} & \uline{{0.079}}
% & \textbf{{0.049}} & \uline{{0.056}} & \uline{{0.060}} & \uline{{0.069}} 
%  \\

% \midrule

\multirow{2}{*}{FreTS} 
& {RMSE} 
& {\uline{0.071}} & \uline{{0.081}} & \uline{{0.090}} & {{0.099}} 
& {{0.051}} & {{0.067}} & {{0.082}} & \uline{{0.110}}
& {{0.036}} & {{0.038}} & {{0.038}} & \uline{{0.039}}
& \textbf{{0.065}} &  \textbf{0.064} & {0.072} & {{0.079}}
& {{0.087}} & {{0.091}} & \uline{{0.096}} & {{0.108}}
& {{0.077}} & \uline{{0.083}} & \textbf{{0.089}} & \uline{{0.096}}

 \\

& {MAE} 
& {{0.032}} & \uline{{0.040}} & {{0.046}} & {{0.055}}  
& {{0.037}} & {{0.050}} & {{0.062}} & \uline{{0.088}}
& \uline{{0.018}} & {{0.020}} & {{0.019}} & \uline{{0.020}}
& \textbf{{0.039}} & \textbf{{0.040}} & {0.046} & \uline{{0.052}}
& {{0.061}} & \uline{{0.065}} & \uline{{0.07}} & {{0.082}}
& {{0.052}} & {{0.057}} & {{0.062}} & {{0.069}}
 \\
\midrule

\multirow{2}{*}{PatchTST} 
& {RMSE} 
& {0.074} & {{0.084}} & {0.094} & {0.102}  
& {0.052} & {0.074} & {0.093} & {0.166}  
& \textbf{{0.032}} & {{0.035}} &{0.039} & {0.040}
& \uline{{0.067}} & \uline{{0.066}} & \textbf{{0.067}} & {0.081}  
& {0.091} & {0.094} & {0.099} & {0.113}  
& {0.082} & {0.085} & \uline{0.091} & {0.097}  
 \\

& {MAE} 
& {0.034} & {0.042} & {0.049} & {0.056}  
& {0.039} & {0.055} & {0.071} & {0.132}  
& \textbf{{0.016}} & {{0.018}} & {0.020} & {0.021}
& {{0.041}} & {{0.042}} & \textbf{{0.043}} & {0.055}  
& {0.065} & {0.069} & {0.073} & {0.087}  
& {0.055} & {0.059} & {0.064} & {0.070}  
 \\
\midrule

\multirow{2}{*}{LTSF-Linear} 
& {RMSE} 
& {0.081} & {0.089} & {0.098} & {0.106}  
& {0.052} & {0.069} & {0.085} & {0.116}  
& {0.039} & {0.042} & {0.040} & {0.041}  
& {0.075} & {0.070} & {{0.071}} & {0.080}  
& {0.089} & {0.094} & {0.097} & {0.108}  
& {0.080} & {0.087} & {0.093} & {0.099}  
 \\

& {MAE} 
& {0.040} & {0.048} & {0.056} & {0.065}  
& {0.038} & {0.053} & {0.064} & {0.092}  
& {0.020} & {0.022} & {0.020} & {0.021}
& {0.045} & {0.043} & \uline{{0.044}} & {0.054}  
& {0.063} & {0.067} & {0.070} & {0.082}  
& {0.055} & {0.060} & {0.065} & {0.072}  
 \\
\midrule


\multirow{2}{*}{FEDformer} 
& {RMSE} 
& {0.088} & {0.092} & {0.101} & {0.109}  
& {0.067} & {0.082} & {0.105} & {0.183} 
& {{0.036}} & {0.042} & {0.042} & {0.042} 
& {0.072} & {0.072} & {0.075} & \uline{0.077}
& {0.096} & {0.100} & {0.105} & {0.116}
& {0.087} & {0.093} & {0.102} & {0.108}
\\

& {MAE} 
& {0.050} & {0.051} & {0.057} & {0.064}  
& {0.050} & {0.064} & {0.080} & {0.151} 
& {0.022} & {0.023} & {0.022} & {0.022} 
& {0.049} & {0.049} & {0.051} & {0.055} 
& {0.072} & {0.076} & {0.080} & {0.090}
& {0.063} & {0.068} & {0.075} & {0.081}
\\
\midrule

\multirow{2}{*}{Autoformer} 
& {RMSE} 
& {0.104} & {0.103} & {0.101} & {0.110}  
& {0.066} & {0.083} & {0.101} & {0.181} 
& {0.042} & {0.050} & {0.053} & {0.050} 
& {0.075} & {0.099} & {0.115} & {0.119} 
& {0.105} & {0.114} & {0.119} & {0.136}
& {0.109} & {0.112} & {0.125} & {0.126}
 \\

& {MAE} 
& {0.064} & {0.061} & {0.059} & {0.065}  
& {0.050} & {0.063} & {0.075} & {0.150} 
& {0.026} & {0.033} & {0.034} & {0.035} 
& {0.051} & {0.051} & {0.088} & {0.116} 
& {0.079} & {0.086} & {0.088} & {0.102}
& {0.081} & {0.083} & {0.091} & {0.093}
 \\
\midrule

\multirow{2}{*}{Informer} 
& {RMSE} 
& {0.139} & {0.134} & {0.115} & {0.132}  
& {0.084} & {0.088} & {0.127} & {0.170} 
& {0.039} & {0.047} & {0.053} & {0.054} 
& {0.124} & {0.138} & {0.144} & {0.148} 
& {0.121} & {0.137} & {0.145} & {0.157}
& {0.096} & {0.107} & {0.119} & {0.149}
 \\

& {MAE} 
& {0.101} & {0.097} & {0.101} & {0.132}  
& {0.066} & {0.068} & {0.093} & {0.117} 
& {0.023} & {0.030} & {0.034} & {0.035} 
& {0.094} & {0.105} & {0.112} & {0.116} 
& {0.093} & {0.103} & {0.112} & {0.125}
& {0.070} & {0.082} & {0.090} & {0.115}
 \\
\midrule

\multirow{2}{*}{Reformer} 
& {RMSE} 
& {0.152} & {0.201} & {0.203} & {0.228}  
& {0.146} & {0.169} & {0.189} & {0.201} 
& {0.053} & {0.054} & {0.053} & {0.054} 
& {0.125} & {0.138} & {0.144} & {0.148} 
& {0.143} & {0.148} & {0.155} & {0.155}
& {0.089} & {0.108} & {0.128} & {0.163}
 \\

& {MAE} 
& {0.108} & {0.147} & {0.154} & {0.173}  
& {0.126} & {0.147} & {0.157} & {0.166} 
& {0.035} & {0.035} & {0.035} & {0.035} 
& {0.095} & {0.121} & {0.122} & {0.120} 
& {0.113} & {0.120} & {0.124} & {0.126}
& {0.065} & {0.081} & {0.100} & {0.132}

\\
\bottomrule

\end{tabular}}

% \end{tabular}}

% \caption{The results for Multivariate long-term time series forecasting was evaluated across six datasets, with an input length of \( L = 96 \) and prediction lengths of \( T \in \{96, 192, 336, 720\} \). Performance is assessed using the Mean Squared Error (MSE), where lower MSE values indicate better forecasting accuracy. The best results are highlighted in \textbf{blue}, the second-best in \uline{purple}, and the third-best in {light blue}.}
\renewcommand{\arraystretch}{1.2} % Increase the row height by 1.2 times

\label{table:main_results}
\end{table*}



The WM-MLP backbone results reported in Table \ref{table:main_results} consider an optimization with respect to $p$, the number of windows, while the HC-MLP considers a fixed size of $p=4$ windows. Thus, for a more suitable comparison, Table \ref{table:WMM vs HCM} shows a comparison of the FIA-Net performance under both backbones with $p=4$. We note that when $p$ is similar for both models, the FIA-Net attains similar results under both backbones, while the HC-MLP requires significantly fewer parameters. Consequently, when the number of windows allows for an HC-MLP version (e.g., $p=2^\ell$ as we further explain in Appendix \ref{app:Hypercomplex Appendix}), an HC-MLP backbone is preferable.



\begin{table*}[h!]
    \centering
    \caption{Performance comparison between WM-MLP and HC-MLP with a fixed number of STFT windows (p = 4). Results demonstrate that HC-MLP achieves comparable accuracy while significantly reducing model parameters, making it preferable for efficient implementations.}

    \resizebox{0.94\textwidth}{!}{  % Change 0.8 to adjust the width proportionally
    \setlength{\tabcolsep}{6pt}  % Slightly reduce column spacing
    \renewcommand{\arraystretch}{1.3}  % Adjust row height
    \small  % Keep the font size consistent
    
    % Use a more flexible table format
    \begin{tabular}{
      @{}
      l
      | l
      | S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      | S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      | S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      @{}
    }

    \cmidrule(l){1-14}
    && \multicolumn{4}{c|}{\textbf{Traffic}} & \multicolumn{4}{c|}{\textbf{ETTh1}} & \multicolumn{4}{c}{\textbf{ETTm1}} \\
    \cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(l){11-14}
    & \textbf{Metric} & {96} & {192} & {336} & {720} & {96} & {192} & {336} & {720} & {96} & {192} & {336} & {720}\\
    \midrule

    \multirow{2}{*}{WM-MLP $(p=4)$} 
    & {RMSE} 
    & 0.033&0.034 &0.035 &0.036 & 
    0.088 & 0.094 & 0.100 & 0.103 &
    0.074 & 0.082 & 0.089 & 0.096 \\

    & {MAE} 
    & 0.016& 0.016& 0.017&0.018 
    & 0.058 & 0.064 & 0.068 & 0.075 &
    0.049 & 0.056 & 0.060 & 0.067 \\
    \midrule

    \multirow{2}{*}{HC-MLP} 
       & {RMSE} 
        & 0.032 & 0.034 & 0.035 & 0.036 
        & 0.083 & 0.085 & 0.094 & 0.101 
        & 0.072 & 0.082 & 0.089 & 0.096 \\
        &{MAE} 
        & 0.016 & 0.017 & 0.017 & 0.018 
        & 0.049 & 0.057 & 0.064 & 0.068 
        & 0.049 & 0.056 & 0.060 & 0.067 \\
    \bottomrule

    \end{tabular}
    }
    \label{table:WMM vs HCM}
\end{table*}



\subsection{Ablation Studies}
\label{app:Ablation Studies}
We consider three ablation studies that best demonstrate the key aspects of the proposed work. We focus on the effect of frequency selection, the size of the lookback window, and the omission of real/imaginary components in the training procedure. We show that, in various cases, the total amount of parameters can be decreased by up to 60\%. Due to space limitations, the results are demonstrated on a single dataset, while a full discussion and additional results are given in Appendix \ref{app:HC Apendix Results}.


\subsubsection{Frequency Dimension Compression}
\begin{wrapfigure}{r}{0.3\textwidth}
    \vspace{-0.5cm}
        \centering
        \includegraphics[width=0.3\textwidth]{topm_bar.pdf}
        \captionsetup{justification=centering} % Center only this caption
        \caption{Accuracy vs. $M$}
        \label{app:results topM}
        \label{fig:TopM Electricity 96}
        \vspace{-0.2cm}
\end{wrapfigure}
We study the effect of the parameter $M$ in the top-$M$ frequency component selection process on the ETTh dataset. As seen in figure \ref{fig:TopM Electricity 96}, even though the model performance varies over different datasets and forecasting horizon sizes, in most cases, $M=4$ attains the best accuracy. Furthermore, note that taking $M < M_{\mathsf{max}} = \frac{N_{FFT}}{2} + 1$ improves the model's results. We conjecture that considering fewer frequency components decreases the NN class complexity, which potentially simplifies the optimization procedure landscape while preserving most of the information contained within the signal. We expand upon this discussion and provide additional results in the Appendix \ref{app:results topM appendix}.


\subsubsection{Effect of Lookback Window Size}
\begin{wrapfigure}{r}{0.3\textwidth}
    % \vspace{-0.5cm}
 
        \centering
        \includegraphics[width=0.3\textwidth]{seq_len_ettm1_main.pdf}
        \captionsetup{justification=centering} % Center only this caption
        \caption{Accuracy vs. $L$.}
        \label{fig:seqlen ETTm1}
\end{wrapfigure}
In this section, we evaluate the impact of varying lookback window sizes $L \in \{24, 48, 96, 192, 288, 480, 576, 720\}$ for different prediction lengths $T \in \{96, 192, 336, 720\}$. As shown in Figure \ref{fig:seqlen ETTm1}, the dotted line represents the RMSE, while the solid line represents the MAE. The model's performance initially improves as $L$ increases, as expected, since a longer lookback provides more contextual information. However, many models exhibit parabolic behavior, where performance deteriorates after a certain point due to overfitting to noise or unrealistic patterns in the data. In contrast, our model maintains stable performance and effectively avoids overfitting, demonstrating its robustness to changes in lookback window size. Additional experiments can be found in Appendix \ref{app:Lookback Window Appendix}.




\subsubsection{Redundancy of Complex Representation}
\label{app:real Vs Imag Results}
We study the effect of the real and imaginary components on prediction quality. We fix the hyperparameters \( E = 128 \), \( p = 13 \), \( N_{FFT} = 16 \), \( M = M_{\mathsf{max}} \), and compare several scenarios, such that each scenario considers the masking of a different component, either in the data, the parameters, or both. The masking occurs in both training and inference. As seen in Table \ref{table:real_imag}, the elimination of either the real or imaginary components in the data does not significantly affect the downstream accuracy, which may hint at redundancy in the learning procedure. Furthermore, this redundancy is maintained when we consider the intersection omission of the real/imaginary parts of both the data and the MLP weights. This phenomenon can be explained through the Kramers-Kronig relation (KKR) \cite{KKR1,KKR2}, which provides a representation of the real component of an analytic complex-valued function in terms of its complex components and vice versa. Roughly speaking, for a complex-valued function $c(\omega) = \mathsf{Re}\{c\}(\omega) + i \mathsf{Im}\{c\}(\omega)$, the KKR are given by
\begin{equation*}
    \mathsf{Re}\{c\}(\omega) = \frac{1}{\pi} \int_{-\infty}^{\infty} \frac{\mathsf{Im}\{c\}(\sigma)}{\omega - \sigma} d\sigma,\quad
    \mathsf{Im}\{c\}(\omega) = - \frac{1}{\pi} \int_{-\infty}^{\infty} \frac{\mathsf{Re}\{c\}(\sigma)}{\omega - \sigma} d\sigma.
\end{equation*}
Thus, we conjecture that masking one component forces the other to recover both in the learning procedure by implicitly approximating the KRR. We therefore believe that a sophisticated system design that considers a KRR-based architecture may lead to the sufficiency of a single component in the forecasting task but leaves a complete study of that subject to future work.
% the apparent redundancy stems from a learning capabilities of the model, i.e., masking the imaginary function component forces the real components to recover both in the learning procedure and vice-versa.
This phenomenon is further explored in Appendix \ref{table:Real VS Imag Table Appendix}.



\begin{table}[h!]
\centering
\resizebox{0.95\textwidth}{!}{  % Adjust 0.8 to control the width of the table
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{Dataset}} & \multicolumn{1}{c}{\textbf{I/O}} & \multicolumn{2}{c}{\textbf{96/96}} & \multicolumn{2}{c}{\textbf{96/192}} & \multicolumn{2}{c}{\textbf{96/336}} & \multicolumn{2}{c}{\textbf{96/720}} \\
\cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
 & \textbf{Hidden Part}  & \textbf{MAE} & \textbf{RMSE} & \textbf{MAE} & \textbf{RMSE} & \textbf{MAE} & \textbf{RMSE} & \textbf{MAE} & \textbf{RMSE} \\
\midrule
\multirow{7}{*}{ETTm1} & 
$X^{\mathsf{Real}}$  & 0.0522 & 0.0797 & 0.0560 &  0.0850& 0.0597& 0.0888& 0.0658 & 0.0958 \\
 & $X^{\mathsf{Imag}}$ & 0.0521 & 0.0792 & 0.0562 & 0.0844 &  0.0592& 0.0879 & 0.0684& 0.0976 \\
 & $W^{\mathsf{Real}}$   & 0.0522 & 0.0791 & 0.0557 &  0.0843& 0.0588& 0.0875& 0.0669 & 0.0964 \\
 & $W^{\mathsf{Imag}}$   & 0.0526 & 0.0801 & 0.0560 &  0.0849& 0.0596& 0.0888& 0.0651 & 0.0953 \\
& $W^{\mathsf{Imag}} , X^{\mathsf{Imag}} $   & 0.0523 & 0.0798 & 0.0560 &  0.0849& 0.0592& 0.0884& 0.0644 & 0.0947 \\
 & $W^{\mathsf{Real}} , X^{\mathsf{Real}} $   & 0.0522 & 0.0791 & 0.0557 &  0.0843& 0.0588& 0.0887& 0.0669 & 0.0930 \\
 & $\emptyset$ & 0.0522 & 0.0791 & 0.0565 & 0.0848 &  0.0592&  0.0878 & 0.0685& 0.0975\\

 \bottomrule
\end{tabular}
}
\caption{Performance comparison on ETTm1 for $I/O = 96 \times \{96, 192, 336, 720\}$ with various modes. $X^{\mathsf{Real}}$/$X^{\mathsf{Imag}}$ hide the real/imaginary parts of the input, while $W^{\mathsf{Real}}$/$W^{\mathsf{Imag}}$ zero out the corresponding weights. Completely ignoring both components is denoted as $(W^{\mathsf{Imag}}, X^{\mathsf{Imag}})$ or $(W^{\mathsf{Real}}, X^{\mathsf{Real}})$.}
\label{table:real_imag}
\end{table}


\section{Conclusion}
\label{app:Conclusion}
This paper presents FIA-Net, a new model for long-term time series forecasting using STFT window aggregation in the frequency domain and HC MLPs. The proposed methodology shows superior performance over existing SoTA on standard benchmark datasets. We show that treating the set of STFT windows as a single HC tensor, which is processed by a novel HC-MLP, significantly reduces the total amount of parameters, with no degradation in the TSF accuracy. We study various schemes to increase model efficiency by, for example, choosing the top-$M$ magnitude frequency components. Experimental results show that the omission of one of the complex representation components does not induce notable segregation in performance, which may be explained by the KKR. For future work, we aim to leverage the KKR equations to propose a forecasting model that only considers the real component in the complex representation while operating over the complex plane. Additionally, we plan to further investigate the relationship between the number of adjacent STFT windows in the WM-MLP backbone and the statistical properties of the datasets.


\bibliographystyle{unsrt}
\bibliography{arxiv_refrences}  % This won't work on arXiv




\newpage
\appendix
% \title{Appendix}
% \maketitle
\section*{Appendix}
\section{Notations \& Symbols }
\subsection{Notation}
We provide a detailed table of the involved notation in this paper: 


\begin{table}[h!]
\centering
\begin{tabular}{|>{\centering\arraybackslash}m{2cm}|>{\arraybackslash}m{12cm}|}
\hline
\textbf{Symbol} & \textbf{Description} \\
\hline
\( B \) & Batch size.\\
\hline
\( L \) & Lookback window size.\\
\hline
\( D \) & Number of features for each time step.\\
\hline
\( T \) & Length of the prediction horizon.\\
\hline
\( E \) & Embedding size.\\
\hline
\( M \) & Number of frequencies to select from all the frequencies using the top M magnitudes.\\
\hline
\( X \) & Multivariate time series with a lookback window of size \( L \) at timestamps \( t \). \\ 
\hline
\( X_t \) & Multivariate values of \( D \) distinct series at timestamp \( t \). \\
\hline
\( X_{t,i} \) & The value of the \( i \)-th feature of the distinct series at timestamp \( t \). \\
\hline
\( \hat{X} \) & Ground truth target values. \\
\hline
\( \sigma \) & activation function \\
\hline
\( P \) & Number of windows in the STFT.\\
\hline
\( N_{FFT} \) & Number of frequency bins in each window of the STFT.\\
\hline
\( \omega \) & Window function for the STFT. \\
\hline
\( X_{E} \) & X after traversing through the embedding layer.\\
\hline
\( X_{Rec} \) & The reconstructed X after the frequency alteration.\\
\hline
\( c^{t}_i \) & The \( i \)-th window of the input in the time domain. \\
\hline
\( C_i \) & The \( i \)-th window of the STFT containing \( N_{FFT} \) frequency bins. \\
\hline
\( C_i^{\mathsf{in}} \) & The \( i \)-th window of the STFT, retaining the top \( M \) frequency components based on magnitude. \\
\hline
\( C_i^{\mathsf{out}} \) & The \( i \)-th window of the STFT after the WM-MLP/WHC has been applied. \\
\hline
\( W_{i \to j} \) & The weights that capture the frequency energy shift between window \( i \) and \( j \), defined as \( W_{i \to j}  = W_{i \to j}^{\mathsf{Real}} + j W_{i \to j}^{\text{Img}} \), where \( W_{i \to j} \in \mathbb{C}^{E \times E} \).\\
\hline
\( B_{i \to j} \) & The bais that capture the frequency energy shift between windows \( i \) and \( j \), defined as \( B_{i \to j}  = B_{i \to j}^{\mathsf{Real}} + j B_{i \to j}^{\text{Img}} \), where \( B_{i \to j} \in \mathbb{C}^{E} \).\\

\hline
\end{tabular}
\caption{Table of Symbols and Descriptions}
\label{tab:symbols}
\end{table}




\newpage



\vspace{-0.5cm}
\subsection{Dimensions}
The following table summarizes the dimensions of the data tensor in every step of the FIA-Net.

\begin{table}[h!]

\centering
\renewcommand{\arraystretch}{1.5} % Adjust the number to control the row height

\begin{tabular}{|>{\centering\arraybackslash}m{1.5cm}|>{\arraybackslash}m{4cm}|}
\hline
\textbf{Symbol} & \textbf{Dimension} \\
\hline
\( X \) & \( \mathbb{R}^{B\times L\times D} \)\\ \hline
\( X_{E} \) & \( \mathbb{R}^{B\times L\times D\times
E} \)\\ \hline
\( C_{i} \) & \( \mathbb{C}^{B\times N_{FFT}\times D\times E} \)\\ \hline
\( C_{i}^{M} \) & \( \mathbb{C}^{B\times M\times D\times 
E} \)
\\ \hline
\( C_{i}^{\mathsf{in/out}} \) & \( \mathbb{C}^{B\times M\times D\times 
E} \)\\ 
\hline
\( X_{Rec} \) & \( \mathbb{R}^{B\times L\times D\times 
E} \)\\ 
\hline
\( \hat{X} \) & \( \mathbb{R}^{B\times T\times D} \)\\ 
\hline
\end{tabular}
\captionsetup{justification=centering} % Center only this caption
\caption{Table of Symbols and Dimension}
\label{tab:symbol_dims}

\end{table}


\section{Additional Experimental Details}
\subsection{Dataset Descriptions}
\label{app:Dataset Descriptions}

In our experiments, we utilized thirteen real-world datasets to assess the effectiveness of models for long-term TSF. Below, we provide the details of these datasets, categorized by their forecasting horizon.


\begin{itemize}
    \item \textbf{Exchange}: 
    This dataset includes daily exchange rates for eight countries (Australia, Britain, Canada, Switzerland, China, Japan, New Zealand, and Singapore) from 1990 to 2016.

    \item \textbf{Weather}: 
    This dataset gathers 21 meteorological indicators, including humidity and air temperature, from the Weather Station of the Max Planck Biogeochemistry Institute in Germany in 2020. The data is collected every 10 minutes.

    \item \textbf{Traffic}: 
    For long-term forecasting, this dataset includes hourly traffic data from 862 freeway lanes in San Francisco, with data collected since January 1, 2015.

    \item \textbf{Electricity}: 
    For long-term forecasting, this dataset covers electricity consumption data from 321 clients, with records starting from January 1, 2011, and a sampling interval of 15 minutes.

    \item \textbf{ETT}: 
    This dataset is sourced from two electric transformers, labeled ETTh1 and ETTm1, with two different resolutions: 15 minutes and 1 hour. These are used as benchmarks for long-term forecasting.
\end{itemize}

\begin{table*}[h!]
\centering
\scriptsize % or \footnotesize for slightly larger text
\begin{tabular}{@{}p{2cm}p{1.72cm}p{1.72cm}p{1.72cm}p{1.72cm}p{1.72cm}p{1.72cm}@{}} % Adjust column widths as needed
\toprule
Datasets          & Weather & Traffic & Electricity   & ETTh1  & ETTm1 & Exchange Rates  \\ 
\midrule
Features          & 21      & 862     & 321         & 7             & 7     & 8     \\
Timesteps         & 52696   & 17544   & 26304         & 17420 & 69680 & 7588   \\ 
Frequency         & 10m     & 1h      & 1h             & 1h        & 15m   & 1d   \\ 
Lookback Window   & 96      & 48      & 96            & 96        & 96    & 96    \\
Prediction Length & 96, 192, 336, 720 & 96, 192, 336, 720  & 96, 192, 336, 720 & 96, 192, 336, 720 &  96, 192, 336, 720 & 96, 192, 336, 720 \\
\bottomrule
\end{tabular}
\caption{Long Term Datasets Parameters}
\label{tab:long_term_datasets_table}
\end{table*}


\subsection{baselines}
\label{app:Baselines Appendix}
% These models represent the latest advancements in TSF and are used in our comparative study to evaluate their performance on various datasets.
We employ a selection of SoTA representative models for our comparative analysis, focusing on Transformer-based architectures and other popular models. The models included are as follows:

\begin{itemize}

    \item \textbf{Informer}: Informer enhances the efficiency of self-attention mechanisms to effectively capture dependencies across variables. The source code was obtained from \href{https://github.com/zhouhaoyi/Informer2020}{GitHub}, and we utilized the default configuration with a dropout rate of 0.05, two encoder layers, one decoder layer, a learning rate of 0.0001, and the Adam optimizer.
    
    \item \textbf{Reformer}: Reformer combines the power of Transformers with efficient memory and computation management, especially for long sequences. The source code was sourced from \href{https://github.com/thuml/Autoformer}{GitHub}, and we employed the recommended configuration for our experiments.
    
    \item \textbf{Autoformer}: Autoformer introduces a decomposition block embedded within the model to progressively aggregate long-term trends from intermediate predictions. The source code was accessed from \href{https://github.com/thuml/Autoformer}{GitHub}, and we followed the recommended settings for all experiments.
    
    \item \textbf{FEDformer}: FEDformer introduces an attention mechanism based on low-rank approximation in the frequency domain combined with a mixture of expert decomposition to handle distribution shifts. The source code was retrieved from \href{https://github.com/MAZiqing/FEDformer}{GitHub}. We utilized the Frequency Enhanced Block (FEB-f) and selected the random mode with 64 as the experimental configuration.
    
    \item \textbf{LTSF-Linear}: LTSF-Linear is a minimalist model employing simple one-layer linear models to learn temporal relationships in time series data. We used it as our baseline for long-term forecasting, downloading the source code from \href{https://github.com/cure-lab/LTSF-Linear}{GitHub}, and adhered to the default experimental settings.
    
    \item \textbf{PatchTST}: PatchTST is a Transformer-based model designed for TSF, introducing patching and a channel-independent structure to enhance model performance. The source code was obtained from \href{https://github.com/PatchTST}{GitHub}, and we used the recommended settings for all experiments.
    
    \item \textbf{FreTS}: FRETS is a sophisticated model tailored for efficient TSF by exploiting a frequency domain approach. The implementation is available on \href{https://github.com/aikunyi/FreTS}{GitHub}, and we utilized the default configuration as recommended by the authors.
    In our work, FRETS serves as the foundational model. We address its limitations, particularly its handling of non-stationary data, while adapting its strengths, such as its complex frequency learner. To fully grasp the contributions of this paper, we recommend reviewing FRETS in detail first. 
    

\end{itemize}


\subsection{Implementation Details}
\label{app:Implementation Details}
Table \ref{table:hyperparams} lists the hyperparameter values used in the FIA-Net implementation. Both WM-MLP and HC-MLP backbones are implemented with the same hyperparameter values, except for $p$, the number of STFT windows.


\begin{table*}[!h]
\centering
\scriptsize % or \footnotesize for slightly larger text

\begin{tabular}{@{}l|p{1.15cm}p{1.15cm}p{1.15cm}p{1.15cm}p{1.15cm}p{1.6cm}@{}} % Adjust column widths
\toprule
 \textbf{DataSets} & Weather & Traffic & Electricity & ETTh1  & ETTm1 &Exchange rate \\ 
\midrule
Batch Size       
&   16    &        4      &     4  &  8    &    8  & 8 \\
Embed Size       
&    128   & 32   &       64       &  128     &    128 & 128 \\ 
Hidden Size      
&   256    &   256    &     256       &   256    &   256   & 256 \\ 
NFF              
&   16    &    32   &      32     &    6   & 48   &  32 \\
STFT Windows      
&    7   &   13    &     13        &  33     &  4  &  13 \\
S-M
&  10    &   $M_{\mathsf{max}}$    &   4    &     4  &  4   & $M_{\mathsf{max}}$ \\
Epoch
&   10  &   10    &    10         &      10 &   10   & 10 \\
 
\bottomrule
\end{tabular}
\caption{ Hyperparameter Settings for Long-Term Datasets for the WM-MLP and HC-MLP}
\label{table:hyperparams}
\end{table*}



\subsection{Evaluation Metrics}
\label{app:Evaluation matrices}

In this study, we use the Mean Squared Error (MSE) as the loss function during training. However, for evaluation, we report both the Mean Absolute Error (MAE) and the Root Mean Squared Error (RMSE).

which are defined as follows:
$$
    \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 
    , \quad
    \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2}
    ,\quad
    \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |Y_i - \hat{Y}_i|
$$


Where:
\begin{itemize}
    \item \( Y_i \) represents the true target values,
    \item \( \hat{Y}_i \) represents the predicted values,
    \item \( n \) is the total number of samples.
\end{itemize}

\subsection{Normalization Methods}
\label{app:Normalization Methods}
In this study, similar to the FRETS model \cite{FreTS}, we apply min-max normalization to standardize the input data to the range between 0 and 1.  This method helps in ensuring that all features contribute equally to the model and prevents any specific feature from dominating due to differences in scale.
The formula for min-max normalization is given by:
$$
    X_{Norm} = \frac{X - X_{\min}}{X_{\max} - X_{\min}}
$$
By normalizing the data, we ensure that all input features are within the same range, which can improve model convergence and performance.


\section{Additional Information on HC Numbers and Models}\label{app:Hypercomplex Appendix}
In this section we extend the discussion on HC numbers, considering additional values of $p$ beyond $p=4$.
We couple the presentation with the construction of the corresponding HC-MLP in the considered base.
Recall that the base of a HC number, i.e., the number of its components is given by $b=2p$.
While hyper-complex number can be defined for any value of $b$, most research has been performed on $b$ that is given by a power of $2$, as the resulting structure of the (algebraic) field. 
% Given that the use of hypercomplex representations, such as the octonion field, has demonstrated performance improvements, we aim to explore whether other hypercomplex number systems can achieve similar benefits. To facilitate further research and experimentation, we provide additional information on hypercomplex number arithmetic, including complex numbers, quaternions, and sedenions. 
The addition of two HC numbers is simply given by the component-wise summation. In what follows, we focus on HC multiplication and additional properties.
For more information on the HC number system,, we refer the reader to \cite{hypercomplex_algebra_kantor}.

% For all hypercomplex numbers, addition is straightforward, as it involves summing each dimension. Therefore, we will focus on detailing multiplication and norm calculations.

\subsection{Base \texorpdfstring{$2$}{2} - Complex Numbers}
When $b=2$, the resulting field is the complex plane $\CC$. We describe $\CC$ for completeness of presentation. 
Given two complex numbers $C_1 = \alpha_1 + j \alpha_2$ and $C_2 = \beta_1 + j \beta_2$, where $\alpha_1, \alpha_2, \beta_1, \beta_2$ are real numbers, their complex multiplication is defined as:
$$
C_1 \cdot C_2 = (\alpha_1 \beta_1 - \alpha_2 \beta_2) + j(\alpha_1 \beta_2 + \alpha_2 \beta_1)
$$
The norm of a complex number is given by:
$$
|C_1|_{\CC} = \sqrt{\alpha_1^2 + \alpha_2^2},
$$
which is preserved under multiplication, i.e.,
$$
|C_1 \cdot C_2|_\CC = |C_1|_\CC \cdot |C_2|_\CC.
$$
Since the STFT with a single window ($p=1$) is equivalent to the standard FFT, applying our method for hyper-complex number MLP results in the following equation:
$$
C_{\text{in}} = \text{FFT}(X)
$$
$$
C^{\text{out}} =  \sigma(C^{\text{in}}_{\mathsf{Real}} \cdot W_{1,\mathsf{Real}} - C^{\text{in}}_{\mathsf{Imag}} \cdot W_{1,\mathsf{Imag}} + {B}_{1,\mathsf{Real}}) + \sigma(j(C^{\text{in}}_{\mathsf{Real}} \cdot W_{1,\mathsf{Imag}} + C^{\text{in}}_{\mathsf{Imag}} \cdot W_{1,\mathsf{Real}} + {B}_{1,\mathsf{Imag}})  
$$
Here, \( W_i \in \mathbb{C}^{E \times E} \) denotes the layer weights, \( {B} \in \mathbb{C}^E \) represents the bias term, and the multiplication occurs across the embedding dimension.
Note that for $b=2$ the HV formulation boils down to the one from \cite{FreTS}.
Thus, the HC-MLP can be considered as an HC generalization of the FD-MLP. 
which allows for efficient window aggregation.




\subsection{Base \texorpdfstring{$4$}{4} - Quaternions}
Denote the field of Quaternions with $\tilde{\QQ}$.
We represent Quatenions with a couple of Complex number, i.e., for $H_1,H_2\in\tilde{\QQ}$, $H_1 = (\alpha_1, \alpha_2)$ and $H_2 = (\beta_1, \beta_2)$, their multiplication is defined as
$$
H_1 \cdot H_2 = (\alpha_1 \beta_1 - \overline{\alpha_2} \beta_2,\quad \alpha_2 \overline{\beta_1} + \alpha_1 \beta_2)
$$



The norm of a quaternion is given by:
$$
|q|_{\tilde{\QQ}} = \sqrt{|\alpha_1|_{\CC}^2 + |\alpha_2|_{\CC}^2}
$$
The norm is preserved under multiplication, meaning:
$$
|q_1 \cdot q_2|_{\tilde{\QQ}} = |q_1|_{\tilde{\QQ}} \cdot |q_2|_{\tilde{\QQ}}
$$
For our model, the corresponding HC-MLP (which we denote QuatMLP) operating on $C^{\mathsf{in}} = (C^{\mathsf{in}}_1, C^{\mathsf{in}}_2)\in\tilde{\QQ}$, is given by,

$$
C^{\mathsf{out}} = \text{QuatMLP}(C^{\mathsf{in}}) = \sigma(C^{\mathsf{in}} \cdot W + {B})
$$
where:
$$
C_1^{\mathsf{out}} = \sigma(C_1 \cdot W_1 - \overline{C_2} \cdot W_2 + {B}_1), \quad C_2^{\mathsf{out}} = \sigma(C_2 \cdot \overline{W_1} + C_1 \cdot W_2 + {B}_2).
$$
Here, \( W_i \in \mathbb{C}^{E \times E} \), $i=1,2$ denote the layer weights, \( {B} \in \mathbb{C}^E \) represents the bias term, and the multiplication involves complex MLP operations across the embedding dimension.


\subsection{Base \texorpdfstring{$16$}{16} - Sedenions}
Elements on the Sedenions field, denoted $\mathbb{S}$, are denoted with $8$-tuples of complex numbers.
Given two sedenions represented by complex numbers $S_1,S_2\in \mathbb{S}$, $S_1 = (\alpha_1, \alpha_2, \ldots , \alpha_8)$ and $S_2 = (\beta_1, \beta_2, \ldots , \beta_8)$, their multiplication is given by
$$
S_1 \cdot S_2 =
\begin{pmatrix}
\alpha_1 \beta_1 - \alpha_2 \overline{\beta_2} - \alpha_3 \overline{\beta_3} - \alpha_4 \overline{\beta_4} - \alpha_5 \overline{\beta_5} - \alpha_6 \overline{\beta_6} - \alpha_7 \overline{\beta_7} - \alpha_8 \overline{\beta_8} \\
\alpha_1 \beta_2 + \alpha_2 \beta_1 + \alpha_3 \overline{\beta_4} - \alpha_4 \overline{\beta_3} + \alpha_5 \overline{\beta_6} - \alpha_6 \overline{\beta_5} + \alpha_7 \overline{\beta_8} - \alpha_8 \overline{\beta_7} \\
\alpha_1 \beta_3 - \alpha_2 \overline{\beta_4} + \alpha_3 \beta_1 + \alpha_4 \beta_2 + \alpha_5 \overline{\beta_7} - \alpha_6 \overline{\beta_8} - \alpha_7 \overline{\beta_5} + \alpha_8 \overline{\beta_6} \\
\alpha_1 \beta_4 + \alpha_2 \beta_3 - \alpha_3 \beta_2 + \alpha_4 \beta_1 + \alpha_5 \overline{\beta_8} + \alpha_6 \overline{\beta_7} - \alpha_7 \overline{\beta_6} - \alpha_8 \overline{\beta_5} \\
\alpha_1 \beta_5 - \alpha_2 \overline{\beta_6} - \alpha_3 \overline{\beta_7} - \alpha_4 \overline{\beta_8} + \alpha_5 \beta_1 + \alpha_6 \beta_2 + \alpha_7 \beta_3 + \alpha_8 \beta_4 \\
\alpha_1 \beta_6 + \alpha_2 \beta_5 - \alpha_3 \overline{\beta_8} + \alpha_4 \overline{\beta_7} - \alpha_5 \beta_2 + \alpha_6 \beta_1 - \alpha_7 \beta_4 + \alpha_8 \beta_3 \\
\alpha_1 \beta_7 + \alpha_2 \beta_8 + \alpha_3 \beta_5 - \alpha_4 \beta_6 - \alpha_5 \beta_3 + \alpha_6 \beta_4 + \alpha_7 \beta_1 - \alpha_8 \beta_2 \\
\alpha_1 \beta_8 - \alpha_2 \beta_7 + \alpha_3 \beta_6 + \alpha_4 \beta_5 - \alpha_5 \beta_4 - \alpha_6 \beta_3 + \alpha_7 \beta_2 + \alpha_8 \beta_1
\end{pmatrix}
$$
where each component follows the rules of Complex multiplication.
The norm of a sedenion is given by:
$$
|S|_{\mathbb{S}} = \sqrt{\sum_{j=1}^8|\alpha_j|_{\CC}^2}
$$
Unlike nase $2$, $4$ and $8$, Sedenions \textit{do not} preserve the norm under addition and multiplication.

The base-$16$ HC-MLP, denoted SedMLP, operating on an input $C^{\mathsf{in}}$ from the STFT with multiple windows $C^{\mathsf{in}} = (C^{\mathsf{in}}_j)_{j=1}^8$, is given by
$$
C^{\mathsf{out}} = \text{SedMLP}(C^{\mathsf{in}}) = \sigma(C^{\mathsf{in}} \cdot W + {B})
$$
where:
$$
\begin{aligned}
C_1^{\mathsf{out}} &= \sigma \left( C^{\mathsf{in}}_1 W_1 - C^{\mathsf{in}}_2 \overline{W_2} - C^{\mathsf{in}}_3 \overline{W_3} - C^{\mathsf{in}}_4 \overline{W_4} - C^{\mathsf{in}}_5 \overline{W_5} - C^{\mathsf{in}}_6 \overline{W_6} - C^{\mathsf{in}}_7 \overline{W_7} - C^{\mathsf{in}}_8 \overline{W_8} + {B}_1 \right) \\
C_2^{\mathsf{out}} &= \sigma \left( C^{\mathsf{in}}_1 W_2 + C^{\mathsf{in}}_2 W_1 + C^{\mathsf{in}}_3 \overline{W_4} - C^{\mathsf{in}}_4 \overline{W_3} + C^{\mathsf{in}}_5 \overline{W_6} - C^{\mathsf{in}}_6 \overline{W_5} + C^{\mathsf{in}}_7 \overline{W_8} - C^{\mathsf{in}}_8 \overline{W_7} + {B}_2 \right) \\
C_3^{\mathsf{out}} &= \sigma \left( C^{\mathsf{in}}_1 W_3 - C^{\mathsf{in}}_2 \overline{W_4} + C^{\mathsf{in}}_3 W_1 + C^{\mathsf{in}}_4 W_2 + C^{\mathsf{in}}_5 \overline{W_7} - C^{\mathsf{in}}_6 \overline{W_8} - C^{\mathsf{in}}_7 \overline{W_5} + C^{\mathsf{in}}_8 \overline{W_6} + {B}_3 \right) \\
C_4^{\mathsf{out}} &= \sigma \left( C^{\mathsf{in}}_1 W_4 + C^{\mathsf{in}}_2 W_3 - C^{\mathsf{in}}_3 W_2 + C^{\mathsf{in}}_4 W_1 + C^{\mathsf{in}}_5 \overline{W_8} + C^{\mathsf{in}}_6 \overline{W_7} - C^{\mathsf{in}}_7 \overline{W_6} - C^{\mathsf{in}}_8 \overline{W_5} + {B}_4 \right) \\
C_5^{\mathsf{out}} &= \sigma \left( C^{\mathsf{in}}_1 W_5 - C^{\mathsf{in}}_2 \overline{W_6} - C^{\mathsf{in}}_3 \overline{W_7} - C^{\mathsf{in}}_4 \overline{W_8} + C^{\mathsf{in}}_5 W_1 + C^{\mathsf{in}}_6 W_2 + C^{\mathsf{in}}_7 W_3 + C^{\mathsf{in}}_8 W_4 + {B}_5 \right) \\
C_6^{\mathsf{out}} &= \sigma \left( C^{\mathsf{in}}_1 W_6 + C^{\mathsf{in}}_2 W_5 - C^{\mathsf{in}}_3 \overline{W_8} + C^{\mathsf{in}}_4 \overline{W_7} - C^{\mathsf{in}}_5 W_2 + C^{\mathsf{in}}_6 W_1 - C^{\mathsf{in}}_7 W_4 + C^{\mathsf{in}}_8 W_3 + {B}_6 \right) \\
C_7^{\mathsf{out}} &= \sigma \left( C^{\mathsf{in}}_1 W_7 + C^{\mathsf{in}}_2 W_8 + C^{\mathsf{in}}_3 W_5 - C^{\mathsf{in}}_4 W_6 - C^{\mathsf{in}}_5 W_3 + C^{\mathsf{in}}_6 W_4 + C^{\mathsf{in}}_7 W_1 - C^{\mathsf{in}}_8 W_2 + {B}_7 \right) \\
C_8^{\mathsf{out}} &= \sigma \left( C^{\mathsf{in}}_1 W_8 - C^{\mathsf{in}}_2 W_7 + C^{\mathsf{in}}_3 W_6 + C^{\mathsf{in}}_4 W_5 - C^{\mathsf{in}}_5 W_4 - C^{\mathsf{in}}_6 W_3 + C^{\mathsf{in}}_7 W_2 + C^{\mathsf{in}}_8 W_1 + {B}_8 \right)
\end{aligned}
$$
Here, \( W_i \in \mathbb{C}^{E \times E} \), $i=1,\dots,8$ denotes the layer weights, \( {B} \in \mathbb{C}^E \) represents the bias term, and the multiplication involves complex MLP operations across the embedding dimension.

\newpage
\section{Additional Ablation Studies}
This section presents additional ablation studies, expanding on the findings reported in Section \ref{app:Ablation Studies}. We analyze the impact of FFT resolution, embedding size, and the number of STFT windows on WM-MLP performance. Additionally, we include further results for the frequency compression, sequence length, and real vs. imaginary component discussions. Furthermore, we provide a comparative analysis of various hyper-complex fields (octonions, quaternions, and sedenions) for the HC-MLP and report the corresponding results.


\subsection{Parameter Sensitivity}
In this section, we conduct a parameter sweep to examine the effects of different hyperparameters on model performance. To accomplish this, we utilize two datasets: the ETTh1 dataset and the electricity dataset. Each section presents four graphs illustrating the results on the two datasets for a configuration of $I/O = 96 \times 96, 336$. Except for the specific experiment sweep, the embedding size is set to 128 for the ETTh1 dataset and 64 for the electricity dataset, with $M$ set to 0 for all datasets.
\paragraph{Embed Size} 
{
In this section, we evaluate the influence of embedding size on the model's performance. We conducted experiments with embedding dimensions $E \in \{1, 2, 4, 8, 16, 32, 64, 128, 256, 512\}$, while keeping the following parameters fixed: $N_{\text{FFT}} = 16$, $B = 8$, $p = 13$, and $M = M_{\mathsf{max}}$. We can observe that as we increase the embedding size, the loss decreases until we reach a certain point (which is dependent on the dataset). This is likely because a larger embedding size enables the model to capture more features; however, an excessively high embedding size may lead to overfitting.
\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{ett_96_embed.pdf}        \captionsetup{justification=centering} % Center only this caption

        \caption{T=96 on ETTh1} % Caption for the first subfigure
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{ett_336_embed.pdf}        \captionsetup{justification=centering} % Center only this caption

        \caption{T=336 on ETTh} % Caption for the second subfigure
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{electricity_96_embed.pdf}
        \caption{T=96 on electricity}        \captionsetup{justification=centering} % Center only this caption
 % Caption for the third subfigure
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{electricity_336_embed.pdf}
        \caption{T=336 on electricity}        \captionsetup{justification=centering} % Center only this caption
 % Caption for the fourth subfigure
    \end{subfigure}
    
    \caption{Comparison of MSE and MAE across different values of E for varying T on the ETTh1 and Electricity datasets.}
\end{figure}


}

\paragraph{Amount of Windows (High Dim)} 
{
In this section, we evaluate the influence of the number of windows ($p$) on the model's performance. We conducted experiments with different window counts $p \in \{3, 6, 14, 17, 25, 33\}$, while keeping the following parameters fixed: $B = 8$, $M = M_{\mathsf{max}}$, and the overlap between windows is 50\%.

\begin{figure}[!h]
    \centering
    \begin{minipage}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{ett_96_hoplen.pdf}        \captionsetup{justification=centering} % Center only this caption

        \subcaption{T=96 on ETTh1} % Caption for the first subfigure
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{ett_336_hoplen.pdf}        \captionsetup{justification=centering} % Center only this caption

        \subcaption{T=336 on ETTh1 } % Caption for the second subfigure
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{electricity_96_hoplen.pdf}        \captionsetup{justification=centering} % Center only this caption

        \subcaption{T=96 on electricity } % Caption for the third subfigure
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{electricity_336_hoplen.pdf}        \captionsetup{justification=centering} % Center only this caption

        \subcaption{T=336 on electricity } % Caption for the fourth subfigure
    \end{minipage}
    
    \caption{Comparison of MSE and MAE across different values of $p$ for varying $T$ on the ETTh1 and Electricity datasets.}
\end{figure}



}

\paragraph{FFT Resolution (NFFT)} 
{
In this section, we evaluate the influence of the FFT resolution ($N_{\text{FFT}}$) on the model's performance. We conducted experiments with different $N_{\text{FFT}} \in \{6, 8, 12, 16, 24, 32, 48\}$, while keeping the following parameters fixed: $p = 25$, $B = 8$, $M = M_{\mathsf{max}}$.


\begin{figure}[h!]
    \centering
    \begin{minipage}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{ett_96_nfft.pdf}        \captionsetup{justification=centering} % Center only this caption

        \subcaption{T=96 on ETTh1 } % Caption for the first subfigure
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{ett_336_nfft.pdf}        \captionsetup{justification=centering} % Center only this caption

        \subcaption{T=336 on ETTh1 } % Caption for the second subfigure
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{electricity_96_nfft.pdf}        \captionsetup{justification=centering} % Center only this caption

        \subcaption{T=96 on electricity } % Caption for the third subfigure
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
    \includegraphics[width=\textwidth]{electricity_336_nfft.pdf}        \captionsetup{justification=centering} % Center only this caption

        \subcaption{T=336 on electricity } % Caption for the fourth subfigure
    \end{minipage}
    
    \caption{Comparison of MSE and MAE across different values of $N_{FFT}$ for varying $T$ on the ETTh1 and Electricity datasets.}
\end{figure}

}

\paragraph{Frequency Choose Max (M)} 
{
\label{app:results topM appendix}
In this section, we provide additional results for various datasets and prediction lengths \( T \) regarding the discussion on frequency compression \ref{app:results topM}.

\begin{figure}[h!]
    \centering
    \begin{minipage}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{ett_96_topm.pdf}
        \subcaption{T=96 on ETTh1 } % Caption for the first subfigure
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{ett_336_topm.pdf}
        \subcaption{T=336 on ETTh1 } % Caption for the second subfigure
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{electricity_96_topm.pdf}
        \subcaption{T=96 on electricity } % Caption for the third subfigure
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth} % Adjust width to fit 4 plots in one line
        \centering
        \includegraphics[width=\textwidth]{electricity_336_topm.pdf}
        \subcaption{T=336 on electricity } % Caption for the fourth subfigure
    \end{minipage}
    
    \caption{Comparison of MSE and MAE across different values of M for various T on the ETTh1 and Electricity datasets.}
\end{figure}

\newpage
\subsection{Different LookBack Window}
\label{app:Lookback Window Appendix}
In this section, we present additional results for various lookback windows on the ETTh1 and ETTm1 datasets.
\begin{figure}[h]
    \centering
    % Label-only figure spanning all columns
    \begin{subfigure}{\textwidth}  % Use full text width for the label-only figure
        \centering
        \includegraphics[width=0.9\linewidth]{legend_only.pdf}  % Adjust the width as needed
    \end{subfigure}

    \vspace{0.2cm}  % Adjust this value to control the spacing between the label and other figures

    % ETTh1 dataset
    \begin{subfigure}{0.45\textwidth}  % Adjusted width to fit three figures in one line
        \centering
        \includegraphics[width=\linewidth]{etth1_loockback.pdf}
        \captionsetup{justification=centering} % Center only this caption
        \caption{ETTh1 Dataset}
    \end{subfigure}
    % ETTm1 dataset
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ettm1_loockback.pdf}        \captionsetup{justification=centering} % Center only this caption

        \caption{ETTm1 Dataset}
    \end{subfigure}
    % Traffic dataset

    % Overall caption for all three figures
    \caption{MAE and RMSE in relation to the Lookback Window \( L \) for varying prediction lengths \( T \in \{96, 192, 336, 720\} \) for the ETTh1 and ETTm1 datasets.}
    \label{fig:three_datasets}
\end{figure}



\newpage

\subsection{Real Vs imaginary Components}
This section provides additional information regarding the real versus imaginary experiment discussed in Section \ref{app:real Vs Imag Results}.

\begin{table}[!h]
\centering
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{Dataset}} & \multicolumn{1}{c}{\textbf{I/O}} & \multicolumn{2}{c}{\textbf{96/96}} & \multicolumn{2}{c}{\textbf{96/192}} & \multicolumn{2}{c}{\textbf{96/336}} & \multicolumn{2}{c}{\textbf{96/720}} \\
\cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
 & \textbf{Hidden Part}  & \textbf{MAE} & \textbf{RMSE} & \textbf{MAE} & \textbf{RMSE} & \textbf{MAE} & \textbf{RMSE} & \textbf{MAE} & \textbf{RMSE} \\
\midrule
\multirow{7}{*}{ETTm1} & 
$X^{\mathsf{Real}}$  & 0.0522 & 0.0797 & 0.0560 &  0.0850& 0.0597& 0.0888& 0.0658 & 0.0958 \\
 & $X^{\mathsf{Imag}}$ & 0.0521 & 0.0792 & 0.0562 & 0.0844 &  0.0592& 0.0879 & 0.0684& 0.0976 \\
 & $W^{\mathsf{Real}}$   & 0.0522 & 0.0791 & 0.0557 &  0.0843& 0.0588& 0.0875& 0.0669 & 0.0964 \\
 & $W^{\mathsf{Imag}}$   & 0.0526 & 0.0801 & 0.0560 &  0.0849& 0.0596& 0.0888& 0.0651 & 0.0953 \\
& $W^{\mathsf{Imag}} , X^{\mathsf{Imag}} $   & 0.0523 & 0.0798 & 0.0560 &  0.0849& 0.0592& 0.0884& 0.0644 & 0.0947 \\
 & $W^{\mathsf{Real}} , X^{\mathsf{Real}} $   & 0.0522 & 0.0791 & 0.0557 &  0.0843& 0.0588& 0.0887& 0.0669 & 0.0930 \\
 & Normal  & 0.0522 & 0.0791 & 0.0565 & 0.0848 &  0.0592&  0.0878 & 0.0685& 0.0975\\
 
\midrule
\multirow{7}{*}{ETTh1} & 
$X^{\mathsf{Real}}$  & 0.0584 & 0.0877 & 0.0638 & 0.0944 & 0.0684 & 0.0997 & 0.0767 & 0.1047  \\
 & $X^{\mathsf{Imag}}$ & 0.0582 & 0.0879 & 0.0634 & 0.0943 & 0.0679 & 0.0997 & 0.0756 & 0.1041  \\
 & $W^{\mathsf{Real}}$ & 0.0586 & 0.0880 & 0.0644 & 0.0948 & 0.0685 & 0.0998 & 0.0759 & 0.1039  \\
 & $W^{\mathsf{Imag}}$  & 0.0584 & 0.0880 & 0.0646 & 0.0951 & 0.0694 & 0.1008 & 0.0781 & 0.1065  \\
& $W^{\mathsf{Imag}} , X^{\mathsf{Imag}} $  & 0.0586 & 0.0880 & 0.0644 & 0.0947 & 0.0685 & 0.0998 & 0.0759 & 0.1040 \\
 & $W^{\mathsf{Real}} , X^{\mathsf{Real}} $ & 0.0587 & 0.0882 & 0.0642 & 0.0948 & 0.0690 & 0.1005 & 0.0765 & 0.1050 \\
 & Normal  & 0.0586 & 0.0878 & 0.0639 & 0.0945 & 0.0684 & 0.0998 & 0.0765 & 0.1043 \\
 \bottomrule
\end{tabular}
\caption{Performance comparison on the ETTm1, ETTh1, and Electricity datasets for $I/O = 96 \times \{96, 192, 336, 720\}$ with different modes. $X^{\mathsf{Real}}$ and $X^{\mathsf{Imag}}$ refer to hiding the real and imaginary parts of the input, respectively. $W^{\mathsf{Real}}$ and $W^{\mathsf{Imag}}$ denote zeroing the real and imaginary weights, respectively. The cases where both the real and imaginary components are completely ignored (i.e., both weights and inputs are zeroed) are represented by $W^{\mathsf{Imag}}, X^{\mathsf{Imag}}$ and $W^{\mathsf{Real}}, X^{\mathsf{Real}}$. MAE and RMSE are reported, where lower values indicate better performance.
}

\label{table:Real VS Imag Table Appendix}
\end{table}



\subsection{HC-MLP Experimental Results 
With For Various Values of \texorpdfstring{$p$}{p}}
\label{app:HC Apendix Results}


In this section, we present additional results on the HC-MLP for various bases. Specifically, we provide results for the Quaternion base (\(p=2\), QuatMLP), Octonion base (\(p=4\), OctMLP), and Sedenion base (\(p=8\), SedMLP). Additionally, we include results for a model that aggregates all windows without using hyper-complex numbers, referred to as BasicMLP. Further details about its implementation can be found in \ref{app:Implementation Details}.

\begin{table*}[h!]
    \centering
    \resizebox{0.98\textwidth}{!}{  % Change 0.8 to adjust the width proportionally
    \setlength{\tabcolsep}{8pt}  % Slightly reduce column spacing
    \renewcommand{\arraystretch}{1.5}  % Adjust row height


    % Use a more flexible table format
    \begin{tabular}{
      @{}
      l
      | l
      | S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      | S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      | S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      @{}
    }

    \cmidrule(l){1-14}
    && \multicolumn{4}{c|}{\textbf{Traffic}} & \multicolumn{4}{c|}{\textbf{ETTh1}} & \multicolumn{4}{c}{\textbf{ETTm1}} \\
    \cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(l){11-14}
    & \textbf{Metric} & {96} & {192} & {336} & {720} & {96} & {192} & {336} & {720} & {96} & {192} & {336} & {720}\\
    \midrule

    \multirow{2}{*}{SedenionMLP $(p=8)$} 
    & {RMSE} 
    & 0.0340 & 0.0346 & 0.0351 & 0.0363
    & 0.0896 & 0.0948 & 0.0999 & 0.1047
    & 0.0814 & 0.0857 & 0.0894 & 0.0977 \\

    & {MAE} 
     & 0.0168 & 0.0169 & 0.0173 & 0.0186
    & 0.0598 & 0.0640 & 0.0685 & 0.0767
    & 0.0542 & 0.0573 & 0.0609 & 0.0682 \\
    \midrule


    
    \multirow{2}{*}{OctontionMLP $(p=4)$} 
       & {RMSE} 
        & 0.0335 & 0.0343 & 0.0349 & 0.0361 
         & 0.0834 & 0.0874 & 0.0941 & 0.1017 
        & 0.0739 & 0.0831 & 0.0888 & 0.0967  \\
        &{MAE} 
        & 0.0166 & 0.0167 & 0.0172 & 0.0185
        & 0.0579 & 0.0635 & 0.0676 & 0.0759 
        & 0.0496 & 0.0556 & 0.0603 & 0.0673  \\

    \midrule
    \multirow{2}{*}{QuaternionMLP $(p=2)$} 
    & {RMSE} & 
    0.0335 & 0.0343 & 0.0350 & 0.0362
    &0.0874 & 0.0938 & 0.0997 & 0.1059 
   &0.0796 & 0.0847 & 0.0887 & 0.0974 
    \\
    & {MAE} 
    & 0.0165 & 0.0167 & 0.0172 & 0.0184 
    & 0.0580 & 0.0633 & 0.0687 & 0.0783
    & 0.0526 & 0.0564 & 0.0603 & 0.0678  
    \\
        \midrule
            
        \multirow{2}{*}{BasicMLP}
           & {RMSE} 
            & 0.0372 & 0.0391 & 0.0384 & 0.0415 
             & 0.0962 & 0.1025 & 0.1061 & 0.1187 
            & 0.0832 & 0.0903 & 0.0967 & 0.1066  \\
            &{MAE} 
            & 0.0180 & 0.0195 & 0.0201 & 0.0217 
            & 0.0650 & 0.0714 & 0.0761 & 0.0886
            & 0.0546 & 0.0595 & 0.0649 & 0.0753  \\
    
    \bottomrule

    \end{tabular}
    }
    \caption{
    Comparison of different hypercomplex structures on the ETT and Traffic datasets. QuadMLP (2 windows), OctMLP (4 windows), and SedMLP (8 windows) represent hypercomplex models of increasing dimensionality, while BasicMLP is a non-hypercomplex linear model aggregating window information. Performance is reported using MSE and RMSE metrics, where lower values indicate better accuracy.}
    \label{table:WMM vs HCM Appendix}
\end{table*}

\newpage

\subsection{Extended Neighborhood Aggregation in WM-MLP}

In this section, we present additional results on the WM-MLP with extended neighborhood aggregation. Specifically, we provide results for varying neighborhood sizes, where the model incorporates information not only from directly adjacent windows but also from second-order and third-order neighbors. The experiments were conducted on the ETTm1 and ETTh1 datasets with prediction lengths of 96, 192, 336, and 720.

For the two-neighbor case, the output $C_i^{\text{out}}$ is computed as:

\begin{align}
C_i^{\text{out}} = \sigma \Big( 
    C_i^{\text{in}} W_{i \to i} &+ 
    C_{i-1}^{\text{in}} W_{(i-1) \to i} + 
    C_{i+1}^{\text{in}} W_{(i+1) \to i} \nonumber \\
    &+ C_{i-2}^{\text{in}} W_{(i-2) \to i} + 
    C_{i+2}^{\text{in}} W_{(i+2) \to i} + 
    B_i
\Big).
\end{align}

For the three-neighbor case, the output $C_i^{\text{out}}$ is computed as:

\begin{align}
C_i^{\text{out}} = \sigma \Big( 
    C_i^{\text{in}} W_{i \to i} &+ 
    C_{i-1}^{\text{in}} W_{(i-1) \to i} + 
    C_{i+1}^{\text{in}} W_{(i+1) \to i} \nonumber \\
    &+ C_{i-2}^{\text{in}} W_{(i-2) \to i} + 
    C_{i+2}^{\text{in}} W_{(i+2) \to i} \nonumber \\
    &+ C_{i-3}^{\text{in}} W_{(i-3) \to i} + 
    C_{i+3}^{\text{in}} W_{(i+3) \to i} + 
    B_i
\Big).
\end{align}

\begin{table*}[h!]
    \centering
    \resizebox{0.98\textwidth}{!}{  % Adjust width proportionally
    \setlength{\tabcolsep}{8pt}  % Adjust column spacing
    \renewcommand{\arraystretch}{1.5}  % Adjust row height

    % Improved table format
    \begin{tabular}{
      @{}
      l
      | l
      | S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      | S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      S[table-format=1.3]
      @{}
    }

    \cmidrule(l){1-10}
    && \multicolumn{4}{c|}{\textbf{ETTh1}} & \multicolumn{4}{c}{\textbf{ETTm1}} \\
    \cmidrule(lr){3-6} \cmidrule(lr){7-10}
    & \textbf{Metric} & {96} & {192} & {336} & {720} & {96} & {192} & {336} & {720} \\
    \midrule

    \multirow{2}{*}{WM-MLP (1 Neighbor)} 
    & {RMSE} 
    & 0.084 & 0.088 & 0.097 & 0.102
    & 0.076 & 0.082 & 0.089 & 0.094 \\

    & {MAE} 
    & 0.057 & 0.064 & 0.068 & 0.075
    & 0.052 & 0.055 & 0.058 & 0.064 \\
    \midrule
     \multirow{2}{*}{WM-MLP (2 Neighbors)} 
    & {RMSE} 
    & 0.084 & 0.087 & 0.095 & 0.100  % Same, -0.001, -0.002, -0.002
    & 0.074 & 0.080 & 0.087 & 0.092  % -0.002, -0.002, -0.002, -0.002
    \\

    & {MAE} 
    & 0.056 & 0.063 & 0.066 & 0.073  % -0.001, -0.001, -0.002, -0.002
    & 0.051 & 0.053 & 0.057 & 0.062  % -0.001, -0.002, -0.001, -0.002
    \\
    \midrule
     \multirow{2}{*}{WM-MLP (3 Neighbors)} 
    & {RMSE} 
    & 0.084 & 0.087 & 0.095 & 0.101  % Same, Same, Same, Worse than 2-Nb
    & 0.075 & 0.080 & 0.087 & 0.094  % Worse than 2-Nb in last column
    \\

    & {MAE} 
    & 0.056 & 0.063 & 0.066 & 0.073  % Same as 2-Nb
    & 0.051 & 0.053 & 0.057 & 0.063  % Worse in last column (tiny bit)
    \\
    \bottomrule

    \end{tabular}
    }
    \caption{Performance comparison of WM-MLP with varying numbers of neighbors (1, 2, and 3) on the ETTh1 and ETTm1 datasets for prediction lengths of 96, 192, 336, and 720. Metrics include RMSE and MAE. Results for WM-MLP with one neighbor are derived from the baseline values reported in the original paper.}
    \label{table:WMM_vs_HCM_Appendix}
\end{table*}


\newpage
}

\subsection{Complexity Analysis}
We conducted an asymptotic analysis of modern models to compare their training time, memory usage, and testing steps. The results are summarized in Table \ref{tab:complexity_analysis}. The comparison highlights the computational efficiency of the WM-MLP and HC-MLP models relative to other state-of-the-art approaches. Specifically, both models demonstrate competitive performance with logarithmic complexity in training time and memory, and a constant number of testing steps. 

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Method}       & \textbf{Training Time}       & \textbf{Training Memory}       & \textbf{Testing Steps} \\
\hline
WM-MLP                & $\mathcal{O}(L \log \frac{L}{p})$      & $\mathcal{O}(L)$               & $1$ \\
HC-MLP                & $\mathcal{O}(L \log \frac{L}{P}+p^2)$      & $\frac{1}{3}\mathcal{O}(L)$               & $1$ \\
FreTS                 & $\mathcal{O}(L \log L)$      & $\mathcal{O}(L)$               & $1$ \\
PatchTST              & $\mathcal{O}(L / S)$         & $\mathcal{O}(L / S)$           & $1$ \\
LTSF-Linear           & $\mathcal{O}(L)$             & $\mathcal{O}(L)$               & $1$ \\
FEDformer             & $\mathcal{O}(L)$             & $\mathcal{O}(L)$               & $1$ \\
Autoformer            & $\mathcal{O}(L \log L)$      & $\mathcal{O}(L \log L)$        & $1$ \\
Informer              & $\mathcal{O}(L \log L)$      & $\mathcal{O}(L \log L)$        & $1$ \\
Transformer           & $\mathcal{O}(L^2)$           & $\mathcal{O}(L^2)$             & $L$ \\
Reformer              & $\mathcal{O}(L \log L)$      & $\mathcal{O}(L \log L)$        & $1$ \\
\hline
\end{tabular}
\caption{Comparison of models in terms of asymptotic complexity for training time, memory usage, and testing steps as a function of the lookback window length ($L$). Here, $S$ denotes the patch size used in PatchTST, and $p$ represents the number of windows in the $\stft$ transformation.}

\label{tab:complexity_analysis}
\end{table}

\newpage
\subsection{Visualizations}

\begin{figure}[h!]
    \centering
    % Traffic Dataset Figures
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=0.6\textwidth]{96_traffic_visual.pdf}
        \captionsetup{justification=centering}
        \caption{Traffic I/O = 96/96}
        \label{fig:traffic_plot1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=0.6\textwidth]{192_traffic_visual.pdf}
        \captionsetup{justification=centering}
        \caption{Traffic I/O = 96/192}
        \label{fig:traffic_plot2}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=0.6\textwidth]{336_traffic_visual.pdf}
        \captionsetup{justification=centering}
        \caption{Traffic I/O = 96/336}
        \label{fig:traffic_plot3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=0.6\textwidth]{720_traffic_visual.pdf}
        \captionsetup{justification=centering}
        \caption{Traffic I/O = 96/720}
        \label{fig:traffic_plot4}
    \end{subfigure}

    \captionsetup{justification=centering}
    \caption{Ground Truth vs. Predictions for Different I/O Settings (Traffic Dataset).}
    \label{fig:traffic_dataset}
\end{figure}

\begin{figure}[h!]
    \centering
    % Electricity Dataset Figures
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=0.6\textwidth]{electricity_96_visual.pdf}
        \captionsetup{justification=centering}
        \caption{Electricity I/O = 96/96}
        \label{fig:electricity_plot1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=0.6\textwidth]{electricity_192_visual.pdf}
        \captionsetup{justification=centering}
        \caption{Electricity I/O = 96/192}
        \label{fig:electricity_plot2}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=0.6\textwidth]{electricity_360_visual.pdf}
        \captionsetup{justification=centering}
        \caption{Electricity I/O = 96/336}
        \label{fig:electricity_plot3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=0.6\textwidth]{electricity_720_visual.pdf}
        \captionsetup{justification=centering}
        \caption{Electricity I/O = 96/720}
        \label{fig:electricity_plot4}
    \end{subfigure}

    \captionsetup{justification=centering}
    \caption{Ground Truth vs. Predictions for Different I/O Settings (Electricity Dataset).}
    \label{fig:electricity_dataset}
\end{figure}


\end{document}


