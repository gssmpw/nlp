\section{Related Work}
\label{app:Related Work}

\textbf{Time-Series Forecasting }
The first notable works on TSF utilize classical statistical linear models such as ARIMA \cite{ARIMA1,ARIMA2} which consider series decomposition. Those were then generalized to a non-linear setting in \cite{var}. To overcome the limitations posed by the classical models, deep learning was incorporated, where initially, sequential deep learning was performed by RNN-based models. 
Two key RNN models are long-short term memory networks \cite{rnn2} which introduce a sophisticated gating mechanism and the DeepAR model \cite{rnn1} that connected the RNN model with AR modeling. Despite their expressive power for sequential modeling, RNN demonstrated low efficiency and introduced high runtimes in both the forward and backward pass \cite{RNNGradientProblem}.
Two popular architectures were proposed to improve upon RNNs; transformers and GNNs. Notable transformer-based methods are Informer \cite{Informer}, Reformer \cite{Reformer}, and PatchTST \cite{PatchTST}, each leveraging the attention mechanism to capture temporal dependencies, while proposing sophisticated methods to reduce the attention operation complexity. 
GNNs, however, allowed for better modeling of dependencies between time series variables by treating them as graph nodes, making them particularly suitable for capturing spatio-temporal patterns. For example, AGCRN \cite{bai2020adaptive} introduced an adaptive graph convolution mechanism to dynamically adjust the graph structure based on inter-series relationships, while MTGNN \cite{wu2020connecting} combined graph convolutions with temporal convolutional layers to jointly learn spatial-temporal dependencies.

\textbf{Frequency Domain Models for Time Series Forecasting }
A recent line of work attempts to solve the TFS problem in the frequency domain \cite{FrequencyTransformationSurvey}, with the purpose of revealing patterns that may be hidden in the time domain. The FEDformer \cite{fedformer} uses a Fourier-based framework to separate trend and seasonal components by leveraging the Fourier Transform on sub-sequences, allowing it to isolate periodic patterns more effectively. ETSformer \cite{estformer} combines exponential smoothing and applies attention in the frequency domain to enhance seasonality modeling by capturing both short- and long-term dependencies. In FiLM \cite{zhou2022film}, Fourier projections are used to reduce noise and emphasize relevant features. Additionally, SFM \cite{zhang2017SFM} and StemGNN \cite{cao2020stemgnn} utilize frequency decomposition and Graph Fourier Transforms to handle complex temporal dependencies in multivariate time series. FRETS \cite{FreTS} extends this approach by proposing frequency-domain MLPs to learn complex relationships between real and imaginary components of the FFT. FREQTSF \cite{FREQTSFAttention} uses STFT with attention mechanisms to capture temporal patterns across overlapping time windows. 
While frequency models, and specifically the recent use of STFT, have shown significant improvement in TFS performance, each STFT window is often processed separately, ignoring the strong correlations between adjacent windows.

\textbf{Hyper-complex Numbers}
HC numbers extend the complex number system to higher dimensions \cite{hamilton1844quaternions}. Base-$4$ HC numbers, have been widely used in computer graphics to model $3D$ rotations \cite{quaternion_classification}. Base-$8$ HC numbers have been explored in image classification and compression \cite{quaternion_classification,quaternion_compression}, developing an HC network that showed favorable performance on popular datasets.
The merit of HC numbers to extract relevant information in time-series was explored in  \cite{saoud2020metacognitive}, in which an HC-net was used to analyse brain-wave data, and in \cite{kycia2024hypercomplex}, which explored HC-network for financial data.
In this work, we explore the utility of HC architectures for the efficient processing of STFT windows in the frequency domain.




\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{highdim.pdf}
    \caption{Window Mixing mechanism. An input $X$ is transformed into a set of $p$ STFT windows which are transformed to the frequency domain and are then fed into the WM-MLP, which aggregates adjacent windows. The WM-MLP outputs are then transformed back to the time domain via a real STFT, from which the prediction (red) is obtained.
    }
    \label{fig:High-Dim-Learner}
    \vspace{0cm}
\end{figure}