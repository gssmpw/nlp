\section{Related Work}
\label{app:Related Work}

\textbf{Time-Series Forecasting }
The first notable works on TSF utilize classical statistical linear models such as ARIMA ____ which consider series decomposition. Those were then generalized to a non-linear setting in ____. To overcome the limitations posed by the classical models, deep learning was incorporated, where initially, sequential deep learning was performed by RNN-based models. 
Two key RNN models are long-short term memory networks ____ which introduce a sophisticated gating mechanism and the DeepAR model ____ that connected the RNN model with AR modeling. Despite their expressive power for sequential modeling, RNN demonstrated low efficiency and introduced high runtimes in both the forward and backward pass ____.
Two popular architectures were proposed to improve upon RNNs; transformers and GNNs. Notable transformer-based methods are Informer ____, Reformer ____, and PatchTST ____, each leveraging the attention mechanism to capture temporal dependencies, while proposing sophisticated methods to reduce the attention operation complexity. 
GNNs, however, allowed for better modeling of dependencies between time series variables by treating them as graph nodes, making them particularly suitable for capturing spatio-temporal patterns. For example, AGCRN ____ introduced an adaptive graph convolution mechanism to dynamically adjust the graph structure based on inter-series relationships, while MTGNN ____ combined graph convolutions with temporal convolutional layers to jointly learn spatial-temporal dependencies.

\textbf{Frequency Domain Models for Time Series Forecasting }
A recent line of work attempts to solve the TFS problem in the frequency domain ____, with the purpose of revealing patterns that may be hidden in the time domain. The FEDformer ____ uses a Fourier-based framework to separate trend and seasonal components by leveraging the Fourier Transform on sub-sequences, allowing it to isolate periodic patterns more effectively. ETSformer ____ combines exponential smoothing and applies attention in the frequency domain to enhance seasonality modeling by capturing both short- and long-term dependencies. In FiLM ____, Fourier projections are used to reduce noise and emphasize relevant features. Additionally, SFM ____ and StemGNN ____ utilize frequency decomposition and Graph Fourier Transforms to handle complex temporal dependencies in multivariate time series. FRETS ____ extends this approach by proposing frequency-domain MLPs to learn complex relationships between real and imaginary components of the FFT. FREQTSF ____ uses STFT with attention mechanisms to capture temporal patterns across overlapping time windows. 
While frequency models, and specifically the recent use of STFT, have shown significant improvement in TFS performance, each STFT window is often processed separately, ignoring the strong correlations between adjacent windows.

\textbf{Hyper-complex Numbers}
HC numbers extend the complex number system to higher dimensions ____. Base-$4$ HC numbers, have been widely used in computer graphics to model $3D$ rotations ____. Base-$8$ HC numbers have been explored in image classification and compression ____, developing an HC network that showed favorable performance on popular datasets.
The merit of HC numbers to extract relevant information in time-series was explored in  ____, in which an HC-net was used to analyse brain-wave data, and in ____, which explored HC-network for financial data.
In this work, we explore the utility of HC architectures for the efficient processing of STFT windows in the frequency domain.




\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{highdim.pdf}
    \caption{Window Mixing mechanism. An input $X$ is transformed into a set of $p$ STFT windows which are transformed to the frequency domain and are then fed into the WM-MLP, which aggregates adjacent windows. The WM-MLP outputs are then transformed back to the time domain via a real STFT, from which the prediction (red) is obtained.
    }
    \label{fig:High-Dim-Learner}
    \vspace{0cm}
\end{figure}