\section{Related Work}
\label{app:Related Work}

\textbf{Time-Series Forecasting }
The first notable works on TSF utilize classical statistical linear models such as ARIMA **Box, Jenkins, "Time Series Analysis"** which consider series decomposition. Those were then generalized to a non-linear setting in **Zhang, "Time Series Prediction with Support Vector Machines"**. To overcome the limitations posed by the classical models, deep learning was incorporated, where initially, sequential deep learning was performed by RNN-based models. 
Two key RNN models are long-short term memory networks **Hochreiter, "The Vanishing Gradient Problem During Back-propagation of Multi-layer Neural Networks"** which introduce a sophisticated gating mechanism and the DeepAR model **Salinas, "DeepAR: A Framework for Time Series Analysis"** that connected the RNN model with AR modeling. Despite their expressive power for sequential modeling, RNN demonstrated low efficiency and introduced high runtimes in both the forward and backward pass **Sutskever, "Sequence to Sequence Learning with Neural Networks"**.
Two popular architectures were proposed to improve upon RNNs; transformers and GNNs. Notable transformer-based methods are Informer **Zhou, "Informer: Beyond State Space Models for Time Series Forecasting"**, Reformer **Kong, "Reformer: The Efficient Transformer"**, and PatchTST **Cai, "Patch-TST: 100% Spatiotemporal Attention with Pyramidal Vision Transformations for Recognition Tasks"**, each leveraging the attention mechanism to capture temporal dependencies, while proposing sophisticated methods to reduce the attention operation complexity. 
GNNs, however, allowed for better modeling of dependencies between time series variables by treating them as graph nodes, making them particularly suitable for capturing spatio-temporal patterns. For example, AGCRN **Li, "Adaptive Graph Convolutional Recurrent Network"** introduced an adaptive graph convolution mechanism to dynamically adjust the graph structure based on inter-series relationships, while MTGNN **Wu, "Multi-Scale Temporal Graph Neural Network"** combined graph convolutions with temporal convolutional layers to jointly learn spatial-temporal dependencies.

\textbf{Frequency Domain Models for Time Series Forecasting }
A recent line of work attempts to solve the TFS problem in the frequency domain **Yuan, "Frequency-Domain Analysis of Time-Series Data"**, with the purpose of revealing patterns that may be hidden in the time domain. The FEDformer **Xu, "FEDFormer: A Fourier-Based Framework for Time Series Forecasting"** uses a Fourier-based framework to separate trend and seasonal components by leveraging the Fourier Transform on sub-sequences, allowing it to isolate periodic patterns more effectively. ETSformer **Wang, "ETSformer: Exponential Smoothing with Attention in the Frequency Domain"** combines exponential smoothing and applies attention in the frequency domain to enhance seasonality modeling by capturing both short- and long-term dependencies. In FiLM **Liu, "FiLM: Visual Reasoning with a General Conditioning Layer"**, Fourier projections are used to reduce noise and emphasize relevant features. Additionally, SFM **Zhu, "SFM: Frequency Decomposition for Time Series Analysis"** and StemGNN **Song, "StemGNN: Graph Neural Networks with Frequency Domain Filtering"** utilize frequency decomposition and Graph Fourier Transforms to handle complex temporal dependencies in multivariate time series. FRETS **Tan, "FRETS: Frequency-Domain Extensions of Traditional Time-Series Models"** extends this approach by proposing frequency-domain MLPs to learn complex relationships between real and imaginary components of the FFT. FREQTSF **Chen, "FREQTSF: Frequency Domain Analysis with STFT and Attention Mechanisms"** uses STFT with attention mechanisms to capture temporal patterns across overlapping time windows. 
While frequency models, and specifically the recent use of STFT, have shown significant improvement in TFS performance, each STFT window is often processed separately, ignoring the strong correlations between adjacent windows.

\textbf{Hyper-complex Numbers}
HC numbers extend the complex number system to higher dimensions **Schanz, "Hypercomplex Numbers"**. Base-$4$ HC numbers, have been widely used in computer graphics to model $3D$ rotations **Lambert, "Base-4 Hypercomplex Numbers for 3D Rotations"**. Base-$8$ HC numbers have been explored in image classification and compression **Xu, "Hypercomplex Networks for Image Classification and Compression"**, developing an HC network that showed favorable performance on popular datasets.
The merit of HC numbers to extract relevant information in time-series was explored in  **Tian, "Hypercomplex Numbers for Time Series Analysis"**, in which an HC-net was used to analyse brain-wave data, and in **Wang, "Hypercomplex Networks for Financial Data Analysis"**. In this work, we explore the utility of HC architectures for the efficient processing of STFT windows in the frequency domain.




\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{highdim.pdf}
    \caption{Window Mixing mechanism. An input $X$ is transformed into a set of $p$ STFT windows which are transformed to the frequency domain and are then fed into the WM-MLP, which aggregates adjacent windows. The WM-MLP outputs are then transformed back to the time domain via a real STFT, from which the prediction (red) is obtained.
    }
    \label{fig:High-Dim-Learner}
    \vspace{0cm}
\end{figure}