\section{Evaluations}\label{sec_eva}

In this section, we describe the evaluations for the proposed \name framework and the designed algorithm.
We first elaborate on the experimental setup, including details of the adopted multi-user HAR dataset, collected by using a practical wireless ISAC system, and the hyper-parameters in the implementation of the algorithm.
Following that, we present the experimental results, including benchmark comparisons and impact factor analyses.
\begin{figure}[b]
    \vspace{-1em}
	\centering 
	\setlength{\abovecaptionskip}{6pt} 
	\setlength{\belowdisplayskip}{-5pt}
    \subfigure[]
    {
        \centering         
        \includegraphics[width=0.46\linewidth]{figures/implementation/MeetingRoom.pdf}  
        \label{fig:env_meeting} 
    }
        \subfigure[]
    {
        \centering         
        \includegraphics[width=0.46\linewidth]{figures/implementation/LectureRoom.pdf}  
        \label{fig:env_class} 
    }
	\vspace{-.7ex}
	\caption{(a) Layouts of the experimental environments in a meeting room and (b) a lecture room.}
	\label{fig:implementOverview}
\end{figure} 
%
\begin{figure*}[t]
	\Copy{new_fig_4}{
    \centering
    \includegraphics[width=0.9\linewidth]{./figures/evaluation/overall_compare_new_0404.eps}
	\caption{\frev{Comparison between the proposed algorithm and the benchmarks and baselines for the accuracy on each of the eight domain datasets after each training period, averaged over 30 trials.}}}
	\label{fig_overall_compare}
\end{figure*}
%==============================
\subsection{Experimental Setup}\label{s2ec_exp_setup}
%==============================

\subsubsection{Dataset Collection}
To evaluate the \name, we collect an HAR dataset by using a practical wireless ISAC system, where multiple users connect to a Wi-Fi access point (AP).
The Wi-Fi network setup by the AP follows IEEE 802.11ac standard~\cite{IEEE_standard}, operating at 5.28\!~GHz and using a bandwidth of $B=40$\!~MHz.
The UDs generate uplink data traffic by engaging in an online Zoom meeting. 
A laptop placed next to the AP emulates the ED and uses PicoScenes~\cite{PicoScenes_IoIJ21} to collect CSI samples from UDs' uplink Quality-of-Service~(QoS) data packets sent to the AP. 
Each CSI sample is a complex array of size $117\times 2$, corresponding to the channel gains for the $117$ subcarriers at two Rx antennas of the AP.

The complete HAR dataset consists of 20 hours of CSI samples for $C=10$ activities performed by $8$ recruited volunteers in two environments, including a meeting room and a lecture room.
The $8$ volunteers consist of $6$ males and $2$ females, and Informed Consent was obtained.
The $10$ activities consist of $4$ body activities: bending, jumping, rotating, and walking; and $6$ gesture activities: push\&pull, sweeping, drawing circle, drawing zigzag, typing-on-phone, and hand-shaking.
In each collection scenario, four volunteers acting as users are present simultaneously to perform the same activities, with each UD positioned around 20\!~cm away from its user, as illustrated in Figs.~\ref{fig:env_meeting} and~\ref{fig:env_class}.
Each activity consists of two seconds of movement followed by one second of rest, leading to CSI sequence duration $T\!=\!3$~\!s.
After segmenting CSI samples into sequences and labeling each sequence according to activities, the complete dataset is constructed.
The domain datasets are obtained by splitting the complete dataset based on individual users.
By default, each domain dataset contains data from a user collected in the two environments.



\subsubsection{Algorithm Hyper-parameters}
For the transformer-based discriminator in Sec.~\ref{sec_alg_1}, we implement it with the following hyper-parameters:
i) in the CSI pre-processor, the length of temporal feature vector is $L_{\rT} = 16$; 
ii) in the MLP encoder, there are two FC layers, sequentially encoding each input vector to feature vectors of sizes first $128$ and then $64$;
iii) in the transformer encoder, there are two MHSA and FF layers, with number of heads $A=8$ and feature vector length $L=64$;
iv) in the probability predictor, the first $64$-dim feature vector in each sequence is mapped to $C=10$ logits and then converted to probabilities by a default softmax function.

Before training on the first domain dataset, the neural parameters are initialized using the Xavier uniform distribution as in~\cite{Glorot10ICAIS_Understanding}.
By default, during the training of each domain dataset, the learning rate is $\alpha=10^{-3}$, and the number of training iterations is $I=500$. 
In addition, the dropout rate for feature elements is set to $0.1$ to improve generalizability.
The deviation radius in the robustness-enhanced optimization is set to $\epsilon=0.03$.
In each domain dataset, the number of exemplars to select for each class is $E=10$, and the clustering-herding ratio during exemplar selection is $\beta=0.9$.
The confidence downscaling factor is set to $\eta=2$.

%========================================
\subsection{Experimental Results}\label{s2ec_exp_res}
%========================================
Below, we present the experimental results on \name from two perspectives, including the overall benchmark comparison and impact factor analysis.
Each experiment is conducted with 30 trials using different random seeds to account for randomness in stochastic training and clustering.
\Copy{R3-5}{\frev{By default, we use the complete domain dataset in each training period to train the neural model and consistently measure the accuracy on that dataset in subsequent training periods, which facilitates a low-variance evaluation of knowledge retention.}}

\subsubsection{Overall comparison}\label{s3ec_overall_compare}

Firstly, we split the CSI data for the eight users into eight domain datasets, resulting in an average domain dataset size of $M=3013$.
We train the neural model sequentially on the eight domain datasets, using the \name framework with the proposed algorithm, and compare its performance against four state-of-the-art benchmarks from the two most common categories of continual learning methods: ER-based and PR-based methods.
\begin{itemize}[leftmargin=*] 
\item \textbf{ER-Kmeans}: Core-sets of exemplars are selected by the K-means clustering method~\cite{Nguyen18ICLR_VCL}, and their empirical CE loss is included in the training as the regularization function.
\item \textbf{ER-herding}: Core-sets of exemplars are selected by the herding method, and their empirical CE loss is included in the training as the regularization function~\cite{Rebuffi17CVPR_iCaRL}.
\item \textbf{PR-EWC}: The regularization function in~\eqref{equ_R_G} is used for knowledge retention, where the variance vector is derived by the elastic weight consolidation~(EWC) method~\cite{Kirkpatrick17PNAS_Overcome}. 
\item \textbf{PR-MAS}: The regularization function in~\eqref{equ_R_G} is used for knowledge retention, where the variance vector is derived by the memory-aware synapses~(MAS) method~\cite{Aljundi18ECCV_Memory}.
\end{itemize}

Fig.~\ref{fig_overall_compare} shows the comparison results in terms of the classification accuracy.
%  for each of the eight domain datasets after the $k$-th training ($k=1,\dots,8)$.
\Copy{R1-9}{\frev{In Fig.~\ref{fig_overall_compare}, the x-axis is divided into eight regions, showing the accuracy of the neural model on $\cD_1$ to $\cD_8$, respectively. 
In the $k$-th region~($k=1,...,8$), the x-axis range spans the training periods from $k$ to $8$, representing the model's accuracy on $\cD_k$ after it is firstly trained on $\cD_k$ in the $k$-th period and subsequently trained on each $\cD_{k'}$ ($k<k'\leq 8$) in later periods.}}
% It can be observed that the proposed algorithm outperforms all the benchmarks,
\Copy{R1-7}{\frev{Fig.~\ref{fig_overall_compare} shows that the proposed algorithm outperforms the four bulleted continual learning benchmarks, resulting in an average accuracy of $0.853$, which surpasses the second-best (ER-Kmeans, $0.787$) by a noticeable $8\%$.}}
Besides, for the proposed algorithm, the average accuracy loss for $\cD_1,\dots,\cD_7$ due to forgetting is $0.144$, which is $21\%$ less than $0.183$ of the second-best.
Moreover, our evaluation confirms that, for CSI processing, the ER-based approaches also significantly outperform the PR-based ones, $0.806$ vs. $0.356$ in average.


Furthermore, we compare four baselines~(BL) in Fig.~\ref{fig_overall_compare}.
\Copy{R2-1}{\frev{We first evaluate the ER method with random core-set selection, named BL-ER-rand, showing its average results given 30 random seeds. It can be observed that randomly selected samples result in less effective exemplars for knowledge retention compared with those selected by herding, K-means clustering, and the proposed method.}}
\Copy{R2-4}{\frev{In addition, we evaluate the proposed method with non-distilled core-set, where the exemplars have hard one-hot labels, referred to as the BL-nondistill baseline. 
As shown in Fig.~\ref{fig_overall_compare}, BL-nondistill exhibits a decline in accuracy compared with the proposed method, highlighting the importance of using distilled core-set for knowledge retention.}}
The other two baselines include the sequential fine-tuning baseline, named BL-FT, and the cumulative baseline, named BL-cumulative.
\Copy{R1-8}{\frev{In BL-FT, the model is firstly trained on $\cD_1$ with a learning rate $\alpha$ and then fine-tuned on each subsequent domain dataset with a reduced learning rate of $0.1\alpha$, without any continual learning schemes.}}
% In BL-FT, we divide the learning rate $\alpha$ by a factor of $10$ after the training for $\cD_1$.
In BL-cumulative, all the previous domain datasets are fully stored and re-trained in each period.
As expected, BL-FT results in catastrophic forgetting: the accuracy for previous domains drops rapidly in subsequent training. 
Nevertheless, its outperforms PR-based methods for $\cD_5$ to $\cD_8$, as parameter regularization for previous domains severely undermines the ability of the neural model to learn new domain datasets.

Another interesting observation is that though BL-cumulative completely preserve the accuracy for previous domains, the re-training over previous domain datasets hinders it adapting to the current domain.
Comparing the proposed algorithm with the BL-FT for $\cD_1$ to $\cD_7$ after the eighth training, the proposed algorithm alleviates the catastrophic forgetting of BL-FT by $79\%$.
In addition, comparing the proposed algorithm with the BL-cumulative, it can be calculated that the proposed algorithm achieves $89\%$ of the accuracy for $\cD_1$ to $\cD_7$ with only $3\%$ memory consumption, storing $100$ instead of average $3013$ CSI data for each domain dataset.


\begin{figure}[t]
	\centering 
	\setlength{\abovecaptionskip}{6pt} 
	\setlength{\belowdisplayskip}{-5pt}
	\subfigure[]
	{
		\centering         
		\includegraphics[width=0.47\linewidth]{figures/evaluation/num_domain_new.eps}  
		\label{fig_num_domain} 
	}
	\hspace{-10pt}
	\subfigure[]
	{
		\centering
		\includegraphics[width=0.46\linewidth]{figures/evaluation/buff_size_new.eps}
		\label{fig_buff_size} 
		\captionsetup{justification=centering}
	}
	\vspace{-.7ex}
	\caption{Impact of (a) number of domains $K$ and (b) number of exemplars per class on the average accuracy across the eight domains and 30 trials.}
	\vspace{-.8ex}
\end{figure} 
\subsubsection{Impact Factor Analyses}
We then investigate the impact of six factors on the proposed algorithm and the \name framework, including 
a) number of domains $K$,
b) number of exemplars $E$,
c) number of training iterations $I$, 
d) learning rate $\alpha$,
e) clustering-herding ratio $\beta$,
and f) deviation radius $\epsilon$.



Fig.~\ref{fig_num_domain} shows that as the number of domains $K$ increases, the average accuracy for the domain datasets after training decreases linearly.
Nevertheless, with more exemplars allowed, the decrease rate can be reduced.
On the other hand, Fig.~\ref{fig_buff_size} reveals a promising sign that using a few exemplars ($E=10$) achieves comparable average accuracy to that of a much larger number ($E=30$), which is consistent across different values of $K$.
These observations indicate that the proposed algorithm has the potential to help the EI handle large numbers of domains with a small number of exemplars.

\begin{figure}[b]
	\centering 
	\setlength{\abovecaptionskip}{6pt} 
	\setlength{\belowdisplayskip}{-5pt}
	\subfigure[] 
		{
		\centering
		\includegraphics[width=0.46\linewidth]{figures/evaluation/iter_compare_new.eps}
		\label{fig_iter_compare} 
		\captionsetup{justification=centering}
		}
		\hspace{-8pt}
		% 
        \subfigure[]
		{
		\centering         
		\includegraphics[width=0.46\linewidth]{figures/evaluation/lr_compare_new.eps}  
		\label{fig_lr_compare} 
		\captionsetup{justification=centering}
        }
	\vspace{-.7ex}
	\caption{Impact of (a) number of training iterations $I$ and (b) learning rate $\alpha$ on the average accuracy across the eight domains. Box plots indicate the distribution over the 30 trials.}
	\vspace{-.8ex}
\end{figure} 

\begin{figure}[t]
    \vspace{-.5em}
	\centering 
	\setlength{\abovecaptionskip}{6pt} 
	\setlength{\belowdisplayskip}{-5pt}
	\subfigure[]
	{
		\centering         
		\includegraphics[width=0.47\linewidth]{figures/evaluation/kh_ratio_new.eps}  
		\label{fig_kr_ratio_influen} 
		\captionsetup{justification=centering}
	}
	\hspace{-14pt}
	\subfigure[]
	{
		\centering
		\includegraphics[width=0.47\linewidth]{figures/evaluation/epsilon_influence_new.eps}
		\label{fig_eps_influen} 
		\captionsetup{justification=centering}
	}
	\vspace{-.7ex}
	\caption{Impact of (a) maximum deviation distance $\epsilon$ and (b) on accuracy after training. Box plots indicate the distribution over the 30 trials, curves connect median values, and stars represent the highest values.}
	\vspace{-.8ex}
\end{figure} 

Figs.~\ref{fig_iter_compare} and~\ref{fig_lr_compare} demonstrate how the training depth and intensity, represented by the number of iteration $I$ and learning rate $\alpha$, respectively, impact the accuracy for each domain after the eight periods of training.
Specifically, the eight box plots, from left to right, in each case represent the accuracy distributions for $\cD_1$ to $\cD_8$ over the 30 trials.
Fig.~\ref{fig_iter_compare} shows that increasing the number of iterations $I$ from $200$ to $500$ improves the accuracy for all the eight domain datasets.
However, when $I$ is further increased to $800$, the accuracy for earlier domain datasets ($\cD_1$ to $\cD_4$) tends to decrease.
Notably, while a moderate increase in learning depth improves the learning for the current domain datasets, an excessive depth may cause more forgetting of previously learned knowledge.
Fig.~\ref{fig_lr_compare} compares the accuracy distributions of the eight domains for different learning rate $\alpha$.
Between $\alpha=0.0005$ and $\alpha=0.001$, a smaller learning rate improves accuracy on the earlier domain datasets, i.e., $\cD_1$ to $\cD_4$, but decreases it for the later ones, i.e., $\cD_5$ to $\cD_8$.
This is probably because a small learning rate prevents the parameters from deviating from the training results of previous domains, thereby aiding in the preservation of previous knowledge. 
Moreover, using a large learning rate, e.g., $\alpha=0.002$ results in general accuracy drop and training instability.
Nevertheless, it is worth noting that the proposed algorithm remains relatively robust to variations in training depth and intensity over $I\in[500,800]$ and $\alpha\in[0.0005,0.001]$.




Figs.~\ref{fig_kr_ratio_influen} and~\ref{fig_eps_influen} show how the two key hyper-parameters of the proposed algorithm, i.e., the clustering-herding ratio $\beta$ in core-set selection and the deviation radius $\epsilon$ in the robustness-enhanced optimization, impact the average accuracy.
Fig.~\ref{fig_kr_ratio_influen} demonstrates that the accuracy increases as more exemplars are selected by clustering, while drop when $\beta=1$.
Therefore, the best strategy for core-set selection is to select most of them by clustering while using a few to ensure the closeness between the center of core-set and that of the full domain dataset in the latent feature space.
Fig.~\ref{fig_eps_influen} confirms that by optimizing the maximum loss within a small deviation radius $\epsilon\in(0,0.03)$, the average accuracy can be improved.
It is worth noting that when $\epsilon$ gets larger, e.g., $\epsilon\geq 0.03$, the resulting accuracy drops rapidly since the first-order approximation in~\eqref{equ_sam_delta} becomes highly inaccurate for large $\epsilon$.
Furthermore, it can also be observed that the proper deviation radius for a smaller learning rate should also be smaller.


\endinput