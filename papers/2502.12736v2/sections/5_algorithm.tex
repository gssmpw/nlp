\section{Algorithm Design}\label{sec_algorithm}



We propose an efficient algorithm for \name to handle the cross-domain continual learning problem (P1)$_k$ of the EI.
As illustrated in Fig.~\ref{fig_alg_illu}, the algorithm comprises three parts:
We first design the neural model as a transformer-based discriminator with compact architecture, handling the first challenge of (P1)$_k$.
Then, to handle the second challenge, we derive a resource-efficient regularization function and corresponding knowledge to prevent catastrophic forgetting in training by replaying distilled core-sets.
Furthermore, for the third challenge, we leverage robustness-enhanced optimization to reduce performance degradation caused by worst-case deviations. 
\Copy{E-1-3}{\frev{We note that although we designate domains as users due to their dominant impacts on the CSI-activity relationship, our proposed algorithm is independent of domain designation and thus effective for arbitrary designation of domains in various wireless ISAC systems.}}

\begin{figure}[t]
    \centering
    \includegraphics[width=.95\linewidth]{./figures/algorithm/alg_clip_0411.pdf}
    \vspace{.5em}
        \caption{\Copy{cap_new_fig2}{\frev{Diagram of the proposed algorithm, including the transformer-based discriminator, the robustness-enhanced optimization, and the distilled core-set selection.}}}
        \label{fig_alg_illu}
\vspace{-.5em}
\end{figure}

 %========================================
 \subsection{Transformer-Based Discriminator for CSI Sequences}\label{sec_alg_1}
 %========================================
We design the architecture of the neural model to comprise two parts: a CSI pre-processor and transformer-based sequence classifier. 
The transformer module is adopted to handle CSI data because CSI data are composed of sequences of CSI samples, and transformers are universal approximators for arbitrary sequence functions~\cite{Yun2020ICLR_Are}.
Moreover, based on~\cite{Hu24JSAC_Cross}, transformers can handle nonequispaced time series efficiently.
In the following, we describe the key components of the CSI pre-processor and the sequence classifier, respectively.

\subsubsection{CSI Pre-processor}
The pre-processor serves two purposes. 
Firstly, it expands each scalar sampling time into a temporal feature vector to help capture temporal relationship among CSI samples.
Secondly, it processes each complex CSI sample into a real-valued vector, suppressing random phase errors in~\eqref{equ_csi} by conjugate multiplication and normalization.

In particular, for the $n$-th row of $\bm H$ ($n=1,\dots,N$), i.e., $[\bm H]_n=(t[n], \bm h[n])$ defined in~\eqref{equ_csi_data}, we expand the scalar time $t[n]$ into a vector $\bm \tau[n] \in \mathbb C^{L_{\rT}}$ by leveraging a series of periodic trigonometric functions.
This enhances the representation of temporal information~\cite{Wen23IJCAI_Transformer}, allowing the neural model to better capture detailed temporal relationships between CSI samples, particularly the periodicity of phase variations at different speeds.
More specifically, the $j$-th ($j=1,\cdots, L_{\rT}$) element of temporal feature vector $\bm\tau[n]$ can be expressed as
\beq
\label{equ_time_embed}
\tau_j[n] = \begin{cases}
    \sin(t[n]/ T^{j/L_{\rT}}), &\text{if $j$ is even,}\\[-.0em]
    \cos(t[n]/ T^{(j-1)/L_{\rT}}),&\text{otherwise,}
\end{cases}
\eeq
where $T$ denotes the maximum time duration that the CSI data spans, i.e., $t[N]\leq T$.

Subsequently, we handle the $n$-th CSI sample, i.e., $\bm h[n]$, which comprises the channel gains between each pair of Tx and Rx antennas for all subcarriers.
To handle the random phase error in~\eqref{equ_csi}, we adopt the conjugate multiplication method in~\cite{Li17Ubicomp_IndoTrack}, which multiplies the conjugate channel gain of the first Rx antenna with the channel gains of the other antennas.
This approach effectively suppresses random phase errors, as the RF chains of different Rx antennas share a common oscillator, causing their random phase errors on the CSI to be similar.
Moreover, we normalize the conjugate multiplication result by detrending and then dividing it by the maximum amplitude.
Therefore, for an arbitrary Tx antenna and subcarrier with their indexes omitted for clarity, the $n$-th pre-processed CSI sample comprises
\beq
\label{equ_x_data}
x_r[n] = \norm(h_{r+1}[n]\cdot \overline{h_1}[n]),\quad r=1,\dots,N_{\rRx}-1,
\eeq
where $r$ is the index of the Rx antenna, 
$N_{\rRx}$ denotes the total number of Rx antennas,
$h_r[n]$ and $h_1[n]$ denote the elements of $\bm h[n]$ corresponding to the CSI at the $r$-th and the first Rx antennas, respectively, 
and $\norm(\cdot)$ represents the aforementioned normalization.
The efficacy of the CSI pre-processing in~\eqref{equ_x_data} is validated by Proposition~\ref{prop_2_x_data} below.

\input{sections/analyses/prop_2.tex} 


Proposition~\ref{prop_2_x_data} indicates that the pre-processed CSI holds the information of the channel changes caused by the user's movements in proximity to the UD.
We note that in~\eqref{equ_efficacy_x}, \frev{$\beta_r \overline{\Delta h_{\rU,1}}[n]$} is environment dependent, which may hinder the clear correlation between $\Delta x_r[n]$ and $\Delta h_{\rU,r}[n]$.
Nevertheless, its impact can be less significant when the $r$-th Rx antenna is closer to the first Rx antenna.
In that case, since $|h_{\rstatic,r}|\approx |h_{\rstatic,1}|$ and $|\Delta h_{\rU,r}[n]|\approx |\Delta h_{\rU,1}[n]|$, based on~\eqref{equ_efficacy_x} and the Euler equation, it can be derived that 
\begin{align*}
    \Delta x_r[n] \! \propto\! e^{\iu \varphi_r[n]} \cdot \big|\Delta h_{\rU,r}[n]\big|\cdot\cos\!\big(\angle(\Delta h_{\rU,r}[n])-\varphi_r[n]\big),
\end{align*}
where $\varphi_r[n] = \angle(\beta_r\cdot \Delta h_{\rU, r}[n] / \Delta h_{\rU, 1}[n])/2$.
As the phase of channel gain generally changes much faster than its amplitude, it can be observed that $|\Delta h_{\rU,r}[n]|$ can be inferred by the envelope of $|\Delta x_r[n]|$, and $\angle(\Delta h_{\rU,r}[n])$ can be inferred by subtracting cosine phase of $|\Delta x_r[n]|$ by $\angle(\Delta x_r[n])$.
This demonstrates the feasibility of obtaining channel variations caused by the user's movements from the CSI samples after the pre-processing in~\eqref{equ_x_data}. 


In more complex scenarios where the Rx antennas are separated more widely, the relationship between $\Delta h_{\rU,r}[n]$ and $\Delta x_{r}[n]$ can be more intricate, and thus an efficient neural model is needed by the EI.
As neural models are more efficient in handling real values instead of complex values, we convert each complex-valued input element $x\in\mathbb C$ into its equivalent real-valued vector $(|x|, \cos\angle x, \sin\angle x)$ as in~\cite{Hu24JSAC_Cross}.
In effect, this complex-to-real conversion preserves the cyclicity of $\angle x$ and avoids the discontinuity between $0$ and $2\pi$.

Concatenating each temporal feature vector in~\eqref{equ_time_embed} and the corresponding pre-processed real-valued vector in~\eqref{equ_x_data}, the resulting sequence can be arranged as a matrix:
\beq
[\bm X]_n = \big(\bm\tau[n], |\bm x[n]|, \cos(\angle\bm x[n]), \sin(\angle\bm x[n])\big),
\eeq
where $n=1,\dots,N$, $\bm X\in\mathbb R^{N\times L_{\rP}}$ with $L_{\rP}$ denoting the vector length after pre-processing, and $\bm x[n]$ comprises the pre-processed CSI for all subcarriers, and Tx-Rx antenna pairs.


\subsubsection{Sequence Classifier}\label{s3ec_seq_class}
Considering the limited computational resource of the ED, we avoid complicated architectures with massive parameters and design the following compact architecture, which comprises three modules: a multi-layer perceptron~(MLP) encoder, a transformer encoder, and a fully-connected~(FC) classifier.
The details of the architecture are illustrated in the left part of Fig.~\ref{fig_alg_illu}.
Ass MLPs and transformers are proven universal approximators for arbitrary functions~\cite{Chen1995Universal,Yun2020ICLR_Are}, the proposed architecture is expected to achieve high performance for general CSI-activity mapping.

\textbf{MLP Encoder}: To extract activity-related features from each pre-processed sample, i.e., $[\bm X]_n$, the MLP encoder maps each sample into a feature vector, leveraging two cascaded FC layers.
\Copy{R3-2}{\frev{The first FC layer takes each $[\bm X]_n$ ($n=1,...,N$) as input and outputs a feature vector of $2L$ dimensions. The second FC layer takes the output feature vectors from the first layer, mapping each of them to an output of $L$ dimensions.}}
In particular, given an input feature vector $\bm x_{\rin}\in \mathbb R^{L_{\rin}}$, each FC layer can be expressed as a function below:
\beq\label{equ_fc_expression}
\mathtt{FC}(\bm x_{\rin}) = \mathtt{Norm}\big(\mathtt{ReLU}(\bm W\bm x_{\rin}+\bm b)\big),
\eeq
where $\bm W\in\mathbb R^{L_{\rout}\times L_{\rin}}$ and $\bm b \!\in\! \mathbb R^{L_{\rout}}$ denote the trainable weight and bias parameters, respectively,
$\mathtt{ReLU}(\cdot)$ is the ReLU activation function,
and $\mathtt{Norm}(\cdot)$ represents normalizing to zero mean and unit variance.
\Copy{R3-2c-1}{\frev{Collecting the $N$ output vectors of $L$ dimensions from the second FC layer, the MLP encoder arranges them as a new sequence denoted by $\bm Z\in\mathbb R^{N\times L}$, which is then fed into the transformer encoder.}}


\textbf{Transformer Encoder}
The transformer encoder comprises cascaded multi-head self-attention~(MHSA) layers and feed forward~(FF) layers, where each layer is equipped with a residual connection~(RC)~\cite{He2016_CVPR} to mitigate the vanishing gradient problem in training a deep neural model.
The core of the transformer encoder is using MHSA layers to enable the information in individual feature vectors of a sequence to complement each other according to their correlation.
%
\Copy{R3-2c-2}{\frev{The first MHSA layer is fed with the sequence $\bm Z\in\mathbb R^{N\times L}$ obtained from the MLP encoder.}}
It splits each input vector to apply multiple self-attention layers and then concatenates the outputs, which can be expressed as
\begin{align}
& \mathtt{MHSA}(\bm Z) = \tAttn_{1}(\bm Z_1)\oplus \dots \oplus \tAttn_{A}(\bm Z_{A}), \nonumber \\
& \tAttn_{a}(\bm Z_a)  = \mathtt{softmax}\Big( \frac{\bm Z_a\bm W_{\rQ,a} (\bm Z_a \bm W_{\rK,a})^\tp}{\sqrt{L/A}}
    \Big) \cdot (\bm Z_a \bm W_{\rV,a}),\nonumber 
\end{align}
where $\bm Z_a\in\mathbb R^{N\times L/A}$ is the $a$-th ($a=1,\dots,A$) sub-matrix of $\bm Z$ with $\bm Z = \bm Z_1\oplus\dots\oplus\bm Z_A$,
and $\bm W_{\rQ,a},\bm W_{\rK,a},\bm W_{\rV,a}\!\in\!\mathbb R^{L/A \times L/A }$, are trainable parameters referred to as the query, key, and value matrices, respectively.
\Copy{R3-2c-3}{\frev{The output of the MHSA layer is added with its input through the RC, which can be expressed as 
\beq
\tilde{\bm Z} = \bm Z + \mathtt{MHSA}(\bm Z). \nonumber
\eeq
Then, $\tilde{\bm Z}$ is sent to the FF layer, which comprises two FC layers with equal dimensions of input and output.
The FF layer handles each row vector $\tilde{\bm z}\in\mathbb R^{L}$ in $\tilde{\bm Z}$ by
\beq
\mathtt{FF}(\tilde{\bm z}) = \mathtt{FC}_2(\mathtt{FC}_1(\tilde{\bm z})),
\eeq
which refines feature representations by feature-level interactions.
The output of the FF layer is added with its input through the RC, which is then fed into the next MHSA layer.}}



\textbf{Probability Predictor}:
As the feature vectors are mutually complemented in the transformer encoder, we can use the first feature vector in the sequence as the input for the final classification, which reduces the computational cost.
This input feature vector is handled by an FC layer to reduce its dimension to the number of classes $C$, the resulting logits are denoted by vector $\bm y\in\mathbb R^C$.
Finally, a softmax function is employed to convert the logit vector into probability predictions for the $C$ classes, i.e.,
\beq
\bm p = \mathtt{softmax}(\bm y) =  \frac{\exp(\bm y)}{\sum_{i=1}^C \exp(y_i)}.
\eeq


%========================================
\subsection{Knowledge Retention by Replaying Distilled Core-Set}\label{s2ec_know_retent}
%========================================



We then design the regularization function $\mathcal R(\bm\theta; \cK_{k})$ in (P1)$_k$, which should approximate $- \log (\Pr(\bm\theta|\cD_{1:k-1}))$.
In the context of solving sequential training (P1)$_k$, a common approach to approximate $\Pr(\bm\theta|\cD_{1:k-1})$ is using a multivariate Gaussian distribution, with the initial point of (P1)$_k$, or equivalently, the result of (P1)$_{k-1}$, as the mean and variance estimated statistically.
In this case, the regularization function can be expressed as
\beq
\label{equ_R_G}
\cR_{\rP}(\btheta; \cK_{k}) = \frac{1}{2}\sum_{i=1}^V v_{k,i} (\theta_i - \theta_{k-1,i}^*)^2, 
\eeq
where core-set knowledge $\cK_{k}\!=\!\{\btheta^*_{k-1}, \bm v_{k}\}$, $\btheta^*_{k-1}\!=\!(\theta^*_{k-1,1},\dots,\theta^*_{k-1,V})$ denotes the result of solving (P1)$_{k-1}$, and $\bm v_{k} \!=\!(v_{k,1},\dots,v_{k,V})$ denotes the variance vector.
Due to its intuitive interpretation of penalizing deviations in important parameters for knowledge retention, such approximation has been widely used in mainstream continual learning studies~\cite{Kirkpatrick17PNAS_Overcome, Aljundi18ECCV_Memory,Zhao24ICML_Statistical}, which are referred to as \emph{parameter regularization}~(PR) approaches. 

However, despite their merit in intuitive interpretation and constant memory consumption, the performance of PR approaches is usually unsatisfactory~\cite{Van22NatMI_Three, Delange22PAMI_Continual, Akrout23ICC_Continual} since approximating the \emph{a posteriori} distribution as a Gaussian can be overly simplistic.
As practical $\Pr(\bm\theta|\cD_{1:k-1})$ is highly complex, a simple parametric distribution such as the Gaussian can only approximate it within a rather limited local region.
Yet, during the sequential training, the neural parameters deviate from their initial points to learn new domain datasets.
As a result, the penalty inevitably becomes increasingly less accurate, causing PR to fail in preserving prior knowledge.

For the above reasons, we design the regularization function based on an alternative type of approaches named \emph{experience replay}~(ER)~\cite{Chaudhry19ICML_Continual}, where the approximation of the \emph{a posteriori} distribution is less constrained to a local region.
The principle of ER is to select a small set of exemplars $\cS_{1:k-1}$ from previous domain datasets $\cD_{1:k-1}$ as the core-set of knowledge and replay them during training on $\cD_k$.
Essentially, it is equivalent to applying the following approximation:
\beq
\label{equ_approx_2}
\log\!\big(\Pr(\btheta|\cD_{1:k-1})\big)\approx \log\!\big(\Pr(\btheta|\cS_{1:k-1})\big).
\eeq
When $\cS_{1:k-1}$ is randomly sampled from $\cD_{1:k-1}$ and $|\cS_{1:k-1}|$ is sufficiently large, the approximation in~\eqref{equ_approx_2} is expected to holds for all $\btheta$, rather than restricted to a local region as in PR approaches. 
Here, $|\cdot|$ denotes the cardinality of the given set.
In this regard, by using a set of randomly selected data samples from previous domain datasets as the core-set of knowledge and involving their CE loss in (P1)$_k$ as the regularization term, it seems (P1)$_k$ can be readily handled.


Nevertheless, to satisfy the memory constraint of the ED, the number of selected exemplars needs to be sufficiently small. 
In this case, random selection often results in a highly biased and non-representative core-set, compromising the efficacy of knowledge retention.
Several studies propose using generative models~\cite{Shin17NIPS_Continual} or data condensation techniques~\cite{Liu20CVPR_Mnemonics} to generate core-sets that possess full knowledge of previous datasets, but these methods are unsuitable for the ED due to their high computational complexity.
In contrast, herding~\cite{Rebuffi17CVPR_iCaRL} and clustering-based~\cite{Nguyen18ICLR_VCL} exemplar selection are more computationally efficient while achieving comparable performance~\cite{Van22NatMI_Three}; yet, each of them have its own strengths and limitations.

To enhance the efficiency of selecting representative exemplars, we propose a novel hybrid method to combine the strengths of herding and clustering-based selection while addressing their limitations.
\Copy{R1-5}{\frev{Preliminarily, the clustering-based method selects exemplars whose feature vectors are closest to the cluster centers revealed by the K-means algorithm. The herding-based method, on the other hand, selects exemplars whose mean feature vector is closest to the overall mean of the feature vectors in the domain dataset. The detailed expressions of the two methods will be provided in~\eqref{equ_kmeans} and~\eqref{equ_herding}, respectively.}}

Without loss of generality, we denote the neural parameter after training in the current period by $\btheta^*$, omitting the subscript for period index.
Using the two encoder parts of $\bgun(\cdot;\btheta^*)$, we project the current domain dataset $\cD$ onto its latent feature space in $\mathbb R^L$, yielding $\cF$.
Then, $\cF$ is divided according to the classes, with the subset of class $c$ being denoted by $\cF_{c}$.
For each $\cF_c$, the budget for exemplars is denoted by $E$, with $E\ll |\cF_{c}|$, which is allocated between clustering and herding-based selection according to the ratio $\beta\in[0,1]$, referred to as the \emph{clustering-herding ratio}.

The first $\beta E$ exemplars are selected by using the K-means clustering algorithm, representing $\beta E$ centers of the feature vectors in $\cF_{c}$.
In particular, it is equivalent to finding $\beta E$ \emph{centroid feature vectors} in $\cF_c$ that minimizes their collective distance to the rest of the feature vectors, i.e.,
\beq
\label{equ_kmeans}
\bfv_{c,1}^*,\dots,\bfv_{c, \beta\!E}^* = \!\argmin_{\bfv_j\in\cF_c|_{j=1,\dots,\beta\!E}} \sum_{\bfv\in\cF_c} \min_{j=1,\dots,\beta\!E}(\|\bfv - \bfv_j\|_2^2).
\eeq

Although the above clustering problem is a combinatorial optimization problem and is NP-hard, it can be solved approximately by a greedy algorithm named Lloyd's algorithm~\cite{Kanungo02PAMI_Efficient}.  
The Lloyd's algorithm starts from randomly picking initial cluster centroids and then iterates between clustering feature vectors to their nearest centroids and updating centroids as the means of the clusters. 
In addition, we employ the k-means++ method~\cite{Arthur06kpp} to enhance the random selection of initial centroids, which essentially increases the mutual distance among the selected centroids, leading to better convergence and clustering results. 


Nevertheless, solely using the clustering method to select exemplars may result in lack of representation for the center of the entire feature vector set, i.e., $\bar{\bfv_c} = \sum_{\bfv\in\mathcal F_c} \bfv / |\cF_c|$.
Based on~\cite{Gretton12JMLR_Kernel}, minimizing the distance between the mean of feature vectors of the selected exemplars and $\bar{\bfv}$ is important for matching the distributions between the selected exemplars and the original dataset.
To achieve this goal, we employ the herding method~\cite{Rebuffi17CVPR_iCaRL} to select the rest of $(1-\beta)E$ exemplars so that the overall mean of the selected feature vectors has a minimal distance from $\bar{\bfv}_c$.
It is equivalent to solving the following optimization problem:
\begin{align}
\label{equ_herding}
\bfv_{c, \beta\! E+\!1}^*,\dots,\bfv_{c,E}^*  = 
&\argmin_{\bfv_{j}\in\cF_c|_{j=\beta\! E+\!1,\dots,E}} \big\| \sum_{j'=1}^{E}\! \bfv_{j'}- \bar{\bfv}\big\|_2^2 \\
% line 2
&\qquad~ \text{s.t.~$\bfv_{j'} = \bfv_{c,j'}^*$, $\forall j'=1,\cdots, \beta\!E$.} \nonumber
\end{align}
Again, although~\eqref{equ_herding} is a challenging combinational optimization problem, it can be solved approximately using a greedy algorithm.
Specifically, we employ the nearest-class-mean algorithm in~\cite{Rebuffi17CVPR_iCaRL}, which iteratively finds each feature vector to minimize the current distance.

Consequently, the CSI data corresponding to the $E$ feature vectors obtained by~\eqref{equ_kmeans} and~\eqref{equ_herding} are selected as the exemplars, forming the core-set of class $c$, i.e.,
\beq
\label{equ_ori_coreset}
\cS_c = \{ (\bm H,\hat{\bm p})\in\cD_c | \hat{\bgun}(\bm H;\btheta^*) = \bfv^*_j,~\exists j=1,\dots E\},
\eeq
where $\hat{\bgun}(\cdot)$ denotes the encoder parts of the neural model, i.e., the neural model without the probability predictor.

We then proceed to deriving the regularization function.
Since the empirical CE loss approximates the negative logarithmic \emph{a posterior} distribution, it is intuitive to adopt the CE loss over the core-set as the regularization function.
Nevertheless, due to the small size of the core-set, the precision of is approximation is low.
To enhance the precision, instead of using the one-hot class label $\hat{\bm p}$ of each exemplar, we adopt the probability vector predicted by the trained neural model. 
\Copy{R1-6}{\frev{Furthermore, to mitigate the over-confidence issue~\cite{Guo17ICML_Calibration} of the neural model during training on the small number of exemplars, we divide the logits output associated with the exemplars in $\bgun(\cdot)$  by a confidence downscaling factor $\eta>1$ before the softmax.}}
This design essentially coincides with the \emph{distillation loss}~\cite{Gou21IJCV_Knowledge}, which is proven to enhance the knowledge transfer between models.
In this regard, we refer to the selected core-set with low-confidence probability labels as \emph{distilled core-set}.
For the $k$-th period, the distilled core-set can be expressed as:
\begin{align}
 \tilde{\cS}_{k} =  \bigcup_{c=1}^{C} \!\tilde{\cS}_{k,c}, ~
 \tilde{\cS}_{k,c} = \{ (\bm H, \tilde{\bm p}) | \tilde{\bm p} = \bgun_{\eta}(\bm H; \btheta_{k}^*), \bm H \in \cS_{k,c}\},  \nonumber
\end{align}
where $\cS_{k,c}$ denotes the core-set in~\eqref{equ_ori_coreset} for the $k$-th period,
${\bgun}_{\eta}(\bm H; \btheta)$ denotes the neural model with its confidence being divided by $\eta$,
and $\tilde{\bm p}$ is referred to as a \emph{distilled label}.

Consequently, we propose to achieve the knowledge retention by replaying the distilled core-set during the training, and thus the regularization function can be derived as
\begin{align}
\label{equ_regu_func}
\cR(\btheta; \cK_{k}) &= \sum_{(\bm H, \tilde{\bm p})\in \cK_k} {\ell({\bgun}_{\eta}(\bm H; \btheta), \tilde{\bm p})},
\end{align}
where knowledge core-set $\cK_{k} = \cup_{k'=1}^{k-1}\tilde{\cS}_{k'}$.

%----------------------------------------
\subsection{Robustness-Enhanced Parameter Optimization}\label{s2ec_robustness}
%----------------------------------------
We address the performance degradation due to the unpredictable neural parameter deviation caused by training in subsequent domains.
Intuitively, given the optimized neural parameters at the $k$-th period denoted by $\btheta^*_k$, the training in the subsequent domains will inevitably lead to a deviation $\bm\delta\in\mathbb R^V$ from $\btheta^*_k$, and thus we need to minimize $\cL_k(\btheta^*_k+\bdelta)$ and $\cL_k(\btheta^*_k)$.
%
Nevertheless, owing to the causality, the deviation $\bm \delta$ is unknown in the training for the $k$-th domain.
To handle this problem, we can solve the $\btheta^*_k$ while minimizing the worst-case loss for all possible deviations from it, which we refer to as the \emph{robustness-enhanced parameter optimization}:
\beq
\label{equ_sam}
\btheta^*_{k} = \arg\min_{\btheta} ~ \max_{\|\bdelta\|_2\leq \epsilon} \cL_{k}(\btheta + \bdelta),
\eeq
where $\epsilon$ denotes the considered \emph{deviation radius} and is generally small ($10^{-2}\!\sim\! 10^{-1}$) to ensure the efficiency of optimizing $\btheta$.
The optimization objective in~\eqref{equ_sam} can be viewed as finding an optimal $\btheta$ where the sharpness of its local loss landscape is minimized~\cite{Foret20ICLR_Sharpness}, which is closely related to the principle of meta-learning~\cite{Finn17ICML_Model}.
By substituting the objective of (P1)$_k$ with that of~\eqref{equ_sam}, the robustness of the resulting $\btheta^*_k$ can be significantly enhanced, enduring deviations with minimal potential increase in loss during subsequent training.

To handle~\eqref{equ_sam} efficiently, we apply first-order Taylor expansion on $\cL_k(\btheta+\bdelta)$ around $\btheta$.
Then, the deviation that maximizes the loss around $\btheta$ can be approximated by
\begin{align}
    \label{equ_sam_delta}
    \bdelta^* & \approx \argmax_{\|\bdelta\|_2\leq \epsilon}~\cL_k(\btheta) + \bdelta^\tp \nabla_{\btheta} \cL_k(\btheta)\\
    \label{equ_sam_delta2}
    & = \epsilon \cdot \nabla_{\btheta} \cL_k(\btheta) / \|\nabla_{\btheta} \cL_k(\btheta)\|_2,
\end{align}
which is an insightful result since the direction of gradient $\nabla_{\btheta} \cL_k(\btheta)$ indicates the direction of the steepest increase of $\cL_k(\btheta)$.
Leveraging~\eqref{equ_sam_delta}, problem~\eqref{equ_sam} can be converted to
\beq
\label{equ_converted_prob}
\btheta^*_{k} = \arg\min_{\btheta} ~\cL_{k}\big(\btheta + \epsilon \cdot  \frac{\nabla_{\btheta}\cL_k(\btheta)}{\|\nabla_{\btheta} \cL_k(\btheta)\|_2}\big).
\eeq

Without loss of generality, we assume that the optimizer employed to solve the converted problem~\eqref{equ_converted_prob} is gradient-based.
In this case, the gradient of the objective function in~\eqref{equ_converted_prob} can be approximately calculated by
\begin{align}
    \label{equ_sam_grad}
    \nabla_{\btheta} \cL_k(\btheta+\bdelta^*) &\approx  \nabla_{\btheta} \cL_k(\btheta)|_{\btheta = \btheta+\bdelta^*}\\ 
    % line 2
    & + \epsilon \cdot \left( \nabla_{\btheta}\frac{ \nabla_{\btheta} \cL_k(\btheta) }{\|\nabla_{\btheta} \cL_k(\btheta)\|_2}\right)^\tp\nabla_{\btheta} \cL_k(\btheta)|_{\btheta = \btheta+\bdelta^*}. \nonumber 
\end{align}

On the right hand side of~\eqref{equ_sam_grad}, the first term is the gradient of loss function at $\btheta+\bdelta^*$ instead of $\btheta$, and the second term represents the compensation for the change of gradient direction from $\btheta$ to $\btheta+\bdelta^*$, which is generally small due to small $\epsilon$. 
However, calculating the second term requires the estimation of the Hessian matrix of $\cL_k(\btheta)$, which can result in substantial computational burden for the ED.
Besides, as shown in~\cite{Foret20ICLR_Sharpness}, involving high-order derivatives of $\cL_k(\btheta)$ may degrade the performance for estimating $\nabla_{\btheta} \cL_k(\btheta+\bdelta^*)$ due to more numerical errors.
Therefore, we omit the second term in~\eqref{equ_sam_grad}.
Consequently, the robustness-enhanced parameter optimization~\eqref{equ_sam} can be solved by
\beq
\label{equ_param_update}
\btheta = \btheta - \alpha \nabla_{\btheta} (\cC(\btheta;\cD_k) + \cR(\btheta; \cK_k))|_{\btheta=\btheta+\bdelta^*},
\eeq
where $\alpha$ denotes the learning rate.

In summary, the complete algorithm for solving the sequential training problem of \name is presented in Algorithm~\ref{alg_overall}.


\input{sections/algorithm/alg_1.tex}

\subsection{Complexity Analysis}\label{s2ec_complex_ana}
We analyze the computational complexity of the training and the core-set selection in the proposed Algorithm~\ref{alg_overall}, as well as the extra memory consumption.
Specifically, we focus on the influence of key parameters, including: sequence length of CSI data $N$, dimension of CSI samples $L_{\rH}$, dimension of latent feature vectors $L$, size of domain datasets $M$, number of classes $C$, number of exemplars per class $E$, clustering-herding ratio $\beta$, and number of training iterations~$I$.

\subsubsection{Training Complexity}
Since the computational cost of the core back-propagation process for training scales linearly with the loss calculation~\cite[Ch. 6.5]{goodfellow2016deep}, we focus on analyzing the complexity of computing the objective function in (P1)$_k$.
For each CSI data, the complexity of predicting its class and computing the loss depends on the architecture of $\bgun(\cdot)$, which mainly comprises FC and MHSA layers.
The complexity of an FC layer is proportional to the product of its input and output dimensions, while an MHSA layer processing a sequence of length $N$ and element dimension $L$ has a complexity of $\cO(N^2L + NL^2)$~\cite{Han21NIPS_Transformer}.
Therefore, the complexity of predicting and calculating the loss for each CSI data is $\cO(NL_{\rH}L + N^2L+NL^2 + NLC)$.
Thus, for the domain dataset $\cD_k$ of $M$ CSI samples and the knowledge core-set of $(k-1)CE$ exemplars from the previous $k-1$ domains, the complexity of calculating the objective function and back-propagation over $I$ training iterations is expressed as:
\beq
\label{equ_complexity_1}
\cO\big( INL\times(N+L_{\rH}+L+C) (M+(k-1)CE) \big).
\eeq

The robustness-enhanced optimization~\eqref{equ_param_update} approximately doubles the computation per iteration due to calculating gradients twice, therefore does not change the overall complexity expression~\eqref{equ_complexity_1}.
Since $L_{\rH}\!>\!L\!\gg\!C$ and $M\gg (k-1) CE$, \eqref{equ_complexity_1} can be simplified to $\cO((N^2L+NL_{\rH}L+NL^2)MI)$, i.e., the training time for solving (P1)$_k$ increases linearly with $M$, $I$, and $L_{\rH}$ and quadratically with $N$ and $L$.

\subsubsection{Core-Set Selection Complexity}
As for the selection of the exemplars in $\cD_k$ after solving (P1)$_k$, our designed algorithm has a low complexity.
It is worth noting that the feature vector for each CSI data can be reused from the last training iteration to form $\cF_{k,c}$, thus incurring no additional cost.
Selecting $\beta E$ exemplars using the K-means clustering method with k-means++ initialization has a complexity of $\cO(\beta EML)$~\cite{Ahmed20Elec_kmeans}, and the complexity of selecting the remaining $(1-\beta)E$ exemplars with the herding method is $\mathcal{O}((1-\beta)EML)$.
Overall, it needs $\cO(CEML)$ computation to obtain the knowledge core-set for $\cD_k$, which is negligible compared to the training complexity.

\subsubsection{Extra Memory Consumption}
We analyze the extra memory required to store the knowledge core-set at the end of the $k$-th period, i.e., $\cK_{k+1}$.
Set $\cK_{k+1}$ consists of $kCE$ exemplars, each comprising a CSI data of size $NL_{\rH}$ and a probability vector of size $C$. 
Consequently, the total memory requirement is $k(NL_{\rH} + C)CE$ floating-point numbers.
Since $CE \ll M$, the memory consumption for storing the knowledge core-set is significantly lower than that required to store the full domain datasets.
\endinput