%========================================
\section{\name: Cross-Domain Continual Learning for Edge Intelligence of ISAC}\label{sec_prob}
%========================================

We propose the \name framework to handle the catastrophic forgetting challenge by optimizing the cross-domain sensing accuracy of the EI during it continually learns a sequence of domain datasets $\cD_1,...,\cD_K$.
At each period $k$, \name aims to minimize the empirical classification loss of the EI across the current and previous domain datasets $\cD_1,...\cD_{k}$.
The optimization problem can be expressed as the following sequential training problem for parameter $\btheta$:
\begin{align}
\text{\textbf{(P0)}}_k\quad\min_{\btheta}~&\cC(\btheta;\cD_{1:k}), \nonumber\\
\text{s.t.}~&\cC(\btheta;\cD_{1:k}) =\!\sum_{(\bm H, \hat{\bm p} )\in\cD_{1:k}} \!{\ell (\bgun(\bm H;\btheta), \hat{\bm p})}, \nonumber
\end{align}
where $\mathcal D_{1:k}=\cup_{i=1}^k \mathcal D_i$ denotes the union set of $\cD_1,...,\cD_k$,
and $\ell(\bm p, \hat{\bm p})$ represents the cross-entropy~(CE) loss given the sensing result of the EI and the ground truth one-hot label.
More specifically, the CE loss can be expressed~as
\beq
\ell(\bm p, \hat{\bm p}) = - \sum_{j=1}^C \hat{p}_j \log (p_j).
\eeq

However, the direct formulation of~(P0)$_k$ is intractable because at the $k$-th period, the previous domain datasets $\cD_1, ...\cD_{k-1}$ have already been discarded.
To convert (P0)$_k$ into a tractable form, we reformulate it as an equivalent problem: Solving the parameter $\btheta$ with the maximum \emph{a posteriori} probability given domain datasets $\cD_1,...,\cD_k$, i.e.,
\beq
\label{equ_map}
\btheta^* = \arg\max_{\bm\theta}~\log(\Pr(\bm\theta|\cD_{1:k})).
\eeq
%
Based on Bayes' rule, the objective in~\eqref{equ_map} is derived as:
\begin{align}
\label{equ_bayesian_equ}
\log(&\Pr(\bm\theta|\cD_{1:k})) =  \\
& \log(\Pr(\cD_{k}|\bm\theta)) +  \log(\Pr(\bm\theta|\cD_{1:k-1})) - \log(\Pr(\mathcal D_k)), \nonumber
\end{align}
where $\log(\Pr(\cD_{k}|\bm\theta))$ is the log-likelihood function of $\mathcal D_k$ given parameter $\bm \theta$, which is negatively proportional to the CE loss over $\mathcal D_k$, i.e., $\log(\Pr(\cD_{k}|\bm\theta))\propto-\cC(\btheta;\cD_k)$.
Since $\Pr(\cD_k)$ is independent of $\btheta$, \eqref{equ_bayesian_equ} reveals that (P0)$_k$ can be handled without directly accessing $\cD_{1:k-1}$ but by leveraging \emph{a posteriori} probability distribution $\Pr(\bm\theta|\cD_{1:k-1})$. 

As the exact \emph{a posterior} probability distribution can hardly be obtained in practice, it is necessary to adopt an approximation of $- \log (\Pr(\bm\theta|\cD_{1:k-1}))$.
Without loss of generality, we denote the approximation by function $\mathcal R(\bm\theta; \cK_{k})$, with $\cK_{k}$ representing a core-set of knowledge from the domains preceding the $k$-th domain.
Consequently, (P0)$_k$ can be converted to
\begin{align}
    \text{\textbf{(P1)}}_k \quad \min_{\bm \theta}~ &\cL_k(\btheta) = \mathcal R(\bm \theta; \cK_{k}) + \mathcal C(\btheta; \cD_k). \nonumber 
\end{align}

In the converted problem (P1)$_k$, $\cR(\btheta;\cK_{k})$ serves as \emph{regularization}, incorporating knowledge of previous domains into the training of $\btheta$ over $\cD_k$, thereby enabling continual learning across the $k$ domains and preventing catastrophic forgetting.
Therefore, we formulate the cross-domain continual learning of the EI as a sequence of optimization problems, i.e., (P1)$_k$ ($k=1,\dots,K$). 
It can be seen that the key to effectively solving (P1)$_k$ is to determine the appropriate regularization $\cR(\btheta;\cK_{k})$ as well as the core-set of knowledge $\cK_{k}$.
Furthermore, owing to the limited resources of the ED, solving (P1)$_k$ is subject to the following two implicit constraints:
\begin{itemize}[leftmargin=*]
\item \textbf{Memory constraint}: In line with the goal of alleviating the memory burden of ED, the memory consumption of $\cK_{k}$ should be significantly lower than that of $\cD_{1:k-1}$.
\item \textbf{Computation constraint}: Given the limited computational resources of the ED, the algorithmic complexity for deriving $\cK_{k}$ must be kept low.
\end{itemize}



Solving (P1)$_k$ under the memory and computation constraints faces three critical challenges:
\emph{First}, for the neural parameters to map CSI data into accurate user activities, their architecture needs to be carefully designed, which is challenging due to the difficulty in handling nonequispaced CSI samples affected by noise and interference.
\emph{Second}, due to the complexity of the high-dimensional \emph{a posteriori} probability distribution $\Pr(\bm \theta| \cD_{1:k-1})$, determining the regularization function that can effectively approximate it and prevent catastrophic forgetting is a major challenge, which is exacerbated by the resource constraints of the ED. 
\emph{Third}, since subsequent training following (P1)$_k$ will cause deviations in the optimized neural parameters, it is challenging to solve (P1)$_k$ with robustness against such unpredictable changes.

\endinput