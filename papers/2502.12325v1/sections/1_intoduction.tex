
%LLM useful
% costly to run, required memory compute budget to run.. Resource specific LLM, or many LLMs are trained and used based on resource bandwidth. This is costly.
% Many models train multiple models of different size in one go to tackle this.
% however, once the model is deployed, it becomes static in general. We advocate for dynamic processing of inputs by a given LLM based on the difficulty of token. Recent work [sharc] proposed to adjust the network width based on the input hardness. Flextron uses elastic sub-networks for dynamic inference. However router training using heuristics or using only language loss is sub-optimal. Additionaly, such methods do provide any tunability factor to adjust the router's sensitivity to route tokens to different experts.


 Large language models (LLMs) have significantly advanced the field of natural language processing, showcasing strong capabilities in addressing complex tasks \cite{LLM1, LLM2_LLama, LLM3_Wei2022ChainOT}. However, their large size presents challenges, particularly in terms of high memory and computational demands, which can limit their deployment in resource-constrained settings. To address this, LLMs must be optimized for specific memory and computational constraints \cite{Touvron2023Llama2O}. However, designing multi-billion-parameter models for every use case is not cost-effective, as it demands substantial training time, data, and resources.

% leveraging existing models for many use cases
Some prior works have focused on adapting large LLMs for resource-constrained use cases by distilling knowledge from larger models into smaller ones~\citep{hsieh-etal-2023-distilling} or pruning model parameters to reduce computational demands~\citep{sun2024a}. While these methods effectively enable the use of large LLMs in low-resource scenarios, they often lead to performance degradation and require careful balancing between efficiency and accuracy. Alternatively, other approaches have investigated many-in-one LLM designs, MatFormer~\citep{Devvrit2023MatFormerNT} and SortedNet~\citep{Sortednet}, to employ multiple sub-networks within a single model to accommodate different computational budgets. These architectures use nested structures integrated into the standard LLM framework. However, they require non-standard methodologies and significantly longer, more resource-intensive training processes, which can offset the intended efficiency benefits.

% MoEs --> reducing inference... also not adaptive
Mixture-of-Experts (MoE) models~\citep{shazeer2017,Du2021GLaMES,fedus_switch,zoph2022st,he2024mixture} have emerged as a promising alternative to dense models, offering improved efficiency by sparsely activating select sub-modules or experts. This selective activation enables MoEs to achieve high performance while using fewer computational resources during inference. However, training MoEs from scratch remains resource-intensive and each expert becomes static, often requiring fixed compute budget irrespective of the input complexity.
% In this work, we explore post-training optimization to adapt a base LLM with minimal fine-tuning cost and enable dynamic compute based on the input data.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{sections/image/overview.png} 
    \caption{Overview of our proposed post-training optimization framework, $\mname$. The left part represents the base pre-trained LLM, while the right part shows the adapted $\mname$ model.
    % The token-difficulty-driven router predicts the difficulty label for processing each token. During training, the token difficulty label generator generates the difficulty label for each token which is used to train the router.
    }
    \label{fig:overview}
\end{figure*}

Flextron~\citep{Cai2024FlextronMF} explored a post-training methodology by integrating the MoE concept into a nested elastic structure within the MLP layers, creating heterogeneous experts of different sizes, selected by a router conditioned on the input data. However, the lack of supervision in the router training leads to sub-optimal input complexity adaptation. Furthermore, the router lacks a parameter to customize its sensitivity to token complexity, limiting its flexibility and performance in handling diverse use-cases. \citet{Salehi2023SHARCSET} proposed an input-adaptive approach that predicts the difficulty of input data and dynamically adjusts the network's width accordingly. In the absence of ground-truth difficulty labels, they relied on heuristic methods for label generation, which may limit precision and consistency in difficulty estimation.



%  our method
To address their shortcomings, we introduce $\mname$, a post-training optimization framework designed to transform a dense LLM into a token-difficulty-driven MoE model. $\mname$ leverages the insight that not all tokens require the full capacity of a model's weights. For example, in the sentence ``Geoffrey did his PhD at the university of Edinburgh'', simpler tokens like ``at the university of'' are predictable using prior context, while more complex tokens like "Edinburgh" demand broader contextual understanding. To maximize efficiency, $\mname$ selectively activates nested sub-components of the MLP, referred as experts, based on the predicted difficulty of each token. To this end, we make the following contributions:

\begin{itemize}
    \item The framework includes a novel token-difficulty-aware router, trained to predict token hardness and assign it to the appropriate expert dynamically.
    
    \item Due to the lack of ground truth notion of hardness, we introduce a method to derive token difficulty labels which serve as supervision signals for training the router. This approach allows a token to have varying difficulty labels across different layers.

    % \item A simplified post-training optimization framework, $\mname$, to easily adapt a pre-trained dense LLM to a token-difficulty-driven MoE model, featuring a sensitivity parameter to customize the efficiency vs accuracy trade-off.
    \item A simplified post-training optimization framework that efficiently adapts a pre-trained dense LLM into a token-difficulty-driven MoE model, featuring a sensitivity parameter to customize the efficiency vs accuracy trade-off.

    
\end{itemize}




