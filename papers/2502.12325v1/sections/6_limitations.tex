% \todo[inline]{Do this ...}

While the proposed post-training optimization framework, $\mname$, and the token-difficulty-aware router provide a general-purpose approach for token-difficulty adaptive processing, this work does not explore their application to pre-trained heterogeneous mixture of expert (HMoE) models  \cite{Wang2024HMoEHM}. Such models, with their built-in experts of varying capabilities, could benefit from the integration of $\mname$, which provides a direct mechanism for routing tokens to appropriate experts based on token difficulty. Incorporating $\mname$ with HMoE models offers a promising direction for future exploration. Additionally, while $\mname$ makes the base model input-adaptive, offering inference efficiency, this work evaluates efficiency only in terms of the number of active parameters and does not include the efficiency measured in a real deployment scenario. 