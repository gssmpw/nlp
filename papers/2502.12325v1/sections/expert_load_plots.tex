
% usage analysis layer wise
\begin{figure*}[t!]
      \centering
    \begin{subfigure}[b]{0.48\linewidth}
         \centering
         \includegraphics[width=\linewidth]{sections/image/0.7_gate_load.png}
         \caption{}
         \label{fig:0.7_gate}
     \end{subfigure}
     \begin{subfigure}[b]{0.48\linewidth}
         \centering
         \includegraphics[width=\linewidth]{sections/image/0.8_gate_load.png}
         \caption{}
         \label{fig:0.8_gate}
     \end{subfigure}
    \begin{subfigure}[b]{0.48\linewidth}
         \centering
         \includegraphics[width=\linewidth]{sections/image/0.9_gate_load.png}
         \caption{}
         \label{fig:0.9_gate}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\linewidth}
         \centering
         \includegraphics[width=\linewidth]{sections/image/no_cam_gate_load.png}
         \caption{}
         \label{fig:nocam_gate}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\linewidth}
         \centering
         \includegraphics[width=\linewidth]{sections/image/expert_legend.png}
         % \caption{}
         % \label{fig:nocam_gate}
    \end{subfigure}
         
     \caption{Layer-wise expert usage pattern of $\mname$ models with varying $\theta$, aggregated across $7$ downstream tasks. Each layer uses experts of varying sizes depending on the inputs. The sensitivity parameter $\theta$ regulates how readily tokens are routed to larger experts based on their difficulty: a lower $\theta$ favors smaller experts, while a higher $\theta$ prioritizes larger experts. \ref{fig:nocam_gate} shows the case when $\mname$ is trained without the router loss $\lambda_{Router}$. In this case, the model tends to assign a specific expert per layer instead of dynamically selecting experts based on token difficulty.
     }
     \label{fig:cam_expert_load}     
\end{figure*}