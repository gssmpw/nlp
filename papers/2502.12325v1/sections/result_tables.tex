\begingroup
    \renewcommand{\arraystretch}{1.5}
    \begin{table*}[!t]
        \centering
        \resizebox{\textwidth}{!}{%
        % \small
        % \footnotesize
        % \scriptsize 
        \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
            \toprule
            & \textbf{Cost (\#Tokens)} & \textbf{Params} & \textbf{ARC-e} & \textbf{LAMBADA} & \textbf{PIQA} & \textbf{WinoGrande} & \textbf{Avg4} & \textbf{SciQ} & \textbf{HellaSwag} & \textbf{ARC-c}  & \textbf{Avg7} \\
            \toprule
            Base Mistral 7B & - & 7B & 80.2 & 75.1 & 80.8 & 75.5 & \textbf{77.8} & 96.4 & 61.4 & 50.5  & 74.2 \\
             
            \hline
            $\mname$ $\theta=0.9$ & \textbf{10B}  & 6B & 75.0 & 71.0 & 78.3 & 71.8 & \textbf{74.0} & 95.2 & 57.9 & 41.5 & 70.1 \\
            
            $\mname$ $\theta=0.8$ & \textbf{10B}  & 5.1B & 69.9 & 68.0 & 78.0 & 66.0 & \textbf{70.5} & 94.2 & 54.5 & 35.2 & 66.5 \\
            
            % $\mname$ $\theta=0.6$ & 10B  & -B & 66.5 & 61.4 & 74.5 & 62.4 & 66.2 & 93.4 & 49.4 & 32 & 62.8\\
            
           $\mname$ $\theta=0.7$ & \textbf{10B}  & 4.6B & 66.0 & 65.9 & 75.4 & 63.4 & 67.7 & 93.6 & 52.1 & 31.7 & 64.0 \\
            
           \hline
        Base Llama2-7B $^\dagger$ & -  & 6.5B & 75.1	& 71.5	& 77.5 &	69.1 & 73.3 &  &  &  &  \\
        
        Flextron $^\dagger$ & 93.57B & 4.1B & 68.6	& 65.1	& 76.1 &	63.7 & 68.3 &  &  &  &  \\
        % 6.7\% drop in acc
            % \textbf{Rel. Imprv}$^{\S}$  & \textbf{10.1} & \textbf{15.9} & \textbf{2.05}  & \textbf{38.3} & \textbf{50.6} & \textbf{76.9} \\
            % % \bottomrule
            \bottomrule
        \end{tabular}
        }
        \vspace{0.1 in}
        \caption{Evaluation of $\mname$ models with different sensitivity factor $\theta$ on downstream tasks, using 0-shot non-normalized accuracy metric. Our base model is Mistral 7B \citep{Jiang2023Mistral7}. $(^\dagger)$: results from Flextron \citep{Cai2024FlextronMF} used as our baseline. \textit{Params} denotes the average number of total activated parameters, aggregated over the downstream tasks. \textit{Avg4} averages over \textit{ARC-e, LAMBDA, PIQA, WinoGrande}, while \textit{Avg7} averages over all tasks.
        }
        \label{tab:result}
    \end{table*}
\endgroup

