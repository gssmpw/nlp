\subsection{Model and Dataset}
\textbf{Model}: $\mname$ provides a simplified post-training approach to convert any dense LLM to an MoE model with a tunable sensitivity factor $\theta$, the similarity threshold, to achieve desired latency reduction and the tolerance for drop in accuracy. $\mname$ integrates seamlessly with any transformer model, regardless of the architecture. To showcase the effectiveness of the method, we use Mistral 7B model \cite{Jiang2023Mistral7}, a widely-used open-source pre-trained language model, as the base model.


\textbf{Dataset}: For $\mname$ fine-tuning, we use a small subset (10B tokens) of the Falcon RefinedWeb dataset \cite{refinedweb}. Falcon RefinedWeb is an open-source dataset which contains high-quality web data. This minimal fine-tuning overhead enables a cost-effective conversion of any pre-trained LLM into an MoE variant for faster inference.

\subsection{Training Details}
We first reorder the pre-trained weight matrices \cite{Samragh2023WeightSD} in the MLP blocks before fine-tuning $\mname$ so that the most important weights can be included in all the experts. We run the dense LLM on a subset of the data and collect the absolute activations from the MLP layer, $Y \in R^{B,H}$. This subset can be a very small portion of the training data, $0.004\%$ tokens of the Falcon RefinedWeb in our case. Subsequently, we aggregate the activations along the batch dimension and across all the subset samples to obtain an importance score for each neuron in the hidden dimension. Using this importance score, we sort the MLP matrices.

 Next, we fine-tune the $\mname$ model using only $10B$ tokens with AdamW optimizer \cite{Loshchilov2017DecoupledWD} and a fixed learning rate of $10^{-5}$. We keep the attention layers frozen. We set $\lambda_{LLM}$ to $0.2$ and $\lambda_{Router}$ to $1$ in Equation~\eqref{eq:loss}, the objective function for fine-tuning. We experiment with different values of threshold $\theta \in {\{0.7, 0.8, 0.9\}}$ to build a family of $\mname$ models with varying sensitivity parameter. A low sensitivity parameter, that is a smaller value of $\theta$, makes the system less reactive, favoring smaller experts for most tokens and only escalating to bigger experts for significantly complex tokens. And a high sensitivity parameter makes the system more reactive, escalating to bigger experts even for moderately complex tokens. We use $4$ experts ($E=4$) with sizes $0.25H$, $0.5H$, $0.75H$, and $H$ respectively. We denote the size of expert with index $e$ as $H_e$.

\input{sections/confusion_mat_plots}
\input{sections/expert_load_plots} 


