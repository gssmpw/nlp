\section{Experimental Evaluation}
To validate the effectiveness and efficiency of our proposed approach, we conduct an experimental evaluation focusing on encrypted traffic adhering to TLS 1.3. Our evaluation consists of two main components: (1) assessing classification performance, and (2) analyzing resource consumption.

We utilize the CSTNET-TLS1.3 dataset \cite{foundation_et_bert}, introduced in the ET-BERT study, which comprises network traffic encrypted with TLS 1.3. This dataset is well-suited for our evaluation as it reflects modern encryption protocols and the associated challenges in NTC. To address the class imbalance issue, we randomly select 10 classes from the dataset, each containing more than 400 samples. For an unbiased evaluation, we provide 400 samples per class to each classifier. This equal provision ensures that no class dominates the training process, and facilitates a fair assessment of resource consumption.

\subsection{Experimental Setup}
\subsubsection{Hardware Configuration}: The experiments were conducted on a machine equipped with dual Intel Xeon Silver 4208 CPUs operating at a base clock of 2.10 GHz, with a maximum clock speed of 3.20 GHz. Each processor features 8 cores per socket, for a total of 16 physical cores and 32 threads. The system is equipped with 125 GiB of RAM. For GPU acceleration, the machine houses four NVIDIA Tesla T4 GPUs, each with 16 GiB of dedicated memory, summing up to a total GPU memory capacity of 64 GiB. The GPUs have a maximum power draw capacity of 70W each.

\subsubsection{Software Configuration}: The system runs on Ubuntu 24.04.1 LTS (Noble) as the operating system. The software environment includes Python 3.13.0 as the programming language and the XGBoost 2.1.2 library for implementing and training LiM. \\

For a comparative analysis, we also evaluated state-of-the-art models ET-BERT and YaTC using their recommended configurations. These models were executed on the same experimental setup to ensure a fair and consistent comparison across different classifiers

\subsection{Evaluation Metrics}
The evaluation encompassed both classification performance and resource consumption to provide a comprehensive assessment of each selected model's effectiveness. For classification performance, we employed standard metrics including \textit{accuracy}, \textit{precision}, \textit{recall}, and \textit{F1 score}.

Regarding resource consumption, we evaluated \textit{latency}, \textit{memory usage}, \textit{energy consumption}, and \textit{throughput}. \textit{Latency} was measured as the time taken by the classifier to process a single input during training, calculated by dividing the total time by the number of training samples. The rest of the matrices were measured during the inference phase. \textit{Memory usage} referred to the amount of RAM+GPU consumed, an important factor for deployment on devices with limited resources. \textit{Energy consumption} represented the amount of energy used by the GPU, reflecting the operational cost and sustainability of the model. \textit{Throughput} indicated the number of inferences the classifier could make per second, providing insight into its ability to handle high-volume traffic, which is essential for network environments with heavy loads. 

\subsection{Results and Analysis}

\subsubsection{Classification Performance} \label{subsubsec:classification} \hfill\\
The results, as presented in Table \ref{tab:1_classification_performance}, demonstrate the performance of ET-BERT, YaTC, and the proposed LiM classifier.

\input{tables/1_classification_performance}

ET-BERT achieves a classification accuracy of 0.568, with recall and F1 score also at 0.568 and 0.569, respectively, while precision is marginally higher at 0.581. The underperformance is likely due to the constrained training setup (400 samples per class) provided for a fair evaluation across all models.

In contrast, the YaTC model performs significantly better, achieving an accuracy and F1 score of 0.963.

The proposed LiM classifier, when paired with the NetMatrix representation, achieves competitive results with an accuracy of 0.942 and an F1 score of 0.942. Precision and recall are also closely matched at 0.943 and 0.942, respectively, underscoring the model's robustness across all evaluation metrics.

While LiM slightly trails YaTC in overall performance, its results highlight the strength of a simple yet effective approach. The combination of the NetMatrix representation and the XGBoost classifier demonstrates that high classification performance can be achieved without reliance on complex architectures, offering a compelling balance between accuracy and simplicity.

\subsubsection{Resource Consumption} \label{subsubsec:scalability} \hfill\\
The lightweight XGBoost classifier, when paired with the NetMatrix representation (LiM), demonstrates less resource consumption across all evaluated metrics, as shown in Table \ref{tab:2_resource_consumption}

\input{tables/2_resource_consumption}

LiM's latency is remarkably low at 0.0005 seconds per sample, enabling it to process inputs with near-instantaneous speed. In comparison, ET-BERT exhibits a latency of 0.4328 seconds, which is 86,560\% higher, while YaTC has a latency of 0.1342 seconds, 26,840\% higher than LiM. This reduction in latency underscores the suitability of LiM for real-time applications where low training times are critical.

In terms of memory usage, LiM consumes only 196.38 MiB of RAM during inference. This represents a significant improvement over ET-BERT, which requires 15,189.48 MiB, a 7,735\% increase, and YaTC, which consumes 5,352.48 MiB, a 2,725\% increase. This low memory footprint makes LiM suitable for deployment on devices with limited computational resources, such as edge devices or IoT systems.

LiM, eliminates the need for GPU utilization during inference, resulting in zero measurable energy consumption. In contrast, ET-BERT consumes 109 watts, while YaTC consumes 26 watts, reflecting operational costs. 

Finally, LiM achieves a throughput of 86,251.23 samples per second, demonstrating its ability to handle high traffic volumes. This throughput is 52,302\% higher than ET-BERT’s throughput of 164.91 samples per second and 13,548\% higher than YaTC’s throughput of 636.62 samples per second. The difference in throughput emphasizes LiM’s potential capability for high-demand environments, such as real-time network monitoring and large-scale traffic analysis.

While maintaining competitive classification performance, LiM significantly reduces resource consumption across all dimensions, achieving faster processing, lower memory usage and minimal energy requirements. This makes LiM a practical and sustainable solution for encrypted network traffic classification, particularly in resource-constrained and real-time settings.