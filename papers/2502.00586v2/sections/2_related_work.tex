\section{Literature Review}
Encrypted NTC has been a well-researched topic, with numerous models proposed to address the complexities of encryption protocols like TLS 1.3. Notably, methods such as ET-BERT \cite{foundation_et_bert} and YaTC \cite{foundation_yatc} have leveraged sophisticated deep learning techniques to classify encrypted traffic effectively.

ET-BERT \cite{foundation_et_bert} applies Bidirectional Encoder Representations from Transformers (BERT), originally developed for natural language processing (NLP). The method assumes that encrypted payloads contain implicit patterns that can be exploited for classification ($C1$). The best performing version of ET-BERT (ET-BERT Packet-wise classifier) truncates payloads and represent network traffic using 128 bytes per packet ($C2$). This method involves converting binary raw data (i.e. header and encrypted payload) into sequences of tokens. The consequent transformation aims to capture semantic and syntactic relationships within transformed network traffic, analogous to words and sentences in natural language ($C4$).

Similarly, YaTC \cite{foundation_yatc} proposes representing network traffic as images and employs Masked Autoencoders (MAEs) for classification. Each network session is represented by 1,600 bytes of raw data, arranged into two-dimensional matrices to form images. This approach attempts to exploit spatial relationships within the transformed network traffic ($C4$). For traffic representation, YaTC relies on the first 5 packets of a network traffic session ($C5$).

While ET-BERT and YaTC are known for their great performance \cite{netbench, yatc_etbert_best_1}, other studies have also explored various aspects of encrypted NTC using raw packet data without justifications \cite{tfe-gnn, foundation_flow_mae} ($C3$). However, most existing models for encrypted NTC often face challenges due to their: (1) reliance on encrypted payload content, (2) improper handling of data representations, (3) inclusion of noisy header information, (4) misalignment with protocol specifications, and (5) dependency on handshake packets. 

Our work distinguishes itself by fundamentally rethinking the assumptions in the context of modern encryption standards. By aligning our methodology with RFCs and focusing on meaningful, structured data representations, we aim to provide a more effective and lightweight solution to encrypted network traffic classification.








% One major limitation is the assumption that encrypted payloads contain exploitable patterns. Models like ET-BERT and YaTC rely on the content of encrypted payloads, operating under the premise that implicit patterns exist within the ciphertext. This assumption directly contradicts TLS 1.3 specifications, which ensure that identical plaintexts produce different ciphertexts [RFC-8446]. As a result, these models may fail to generalize effectively, as the encrypted payloads are intentionally devoid of discernible patterns.

% Another limitation is the practice of truncating or padding payloads to create fixed-size homogeneous representations. This approach ignores the fact that TLS 1.3 acknowledges the susceptibility of encrypted packet lengths and timing to traffic analysis attacks [RFC-8446]. By padding or truncating payloads, models may obscure valuable size-related artifacts that are essential for classification, potentially reducing their effectiveness.

% Furthermore, these models often include header information without proper justification, incorporating fields such as IP Identification (IP-ID), IP header checksum, sequence and acknowledgment numbers, and TCP options timestamps. These fields typically contain pseudo-random values initialized per session (e.g., [RFC-6274], [RFC-791], [RFC-9293], [RFC-7323]), introducing unnecessary noise and variability into the data. This inclusion can degrade the performance of classifiers by focusing on non-informative or misleading features.

% Additionally, transforming network traffic into textual or visual formats to leverage semantic, syntactic, or spatial relationships may hinder the inherent structure of network protocols. Network header attributes generally lack the interdependencies found in natural language tokens or image pixels. As such, models like ET-BERT and YaTC, which utilize foundation models designed for text or images, may not effectively capture meaningful patterns in network traffic data.

% Lastly, the reliance on TCP/TLS handshake packets poses a significant limitation. Mechanisms like TCP connection reuse [RFC-9293], TLS 1.3's 0-RTT data transmission, and session resumption with Pre-Shared Keys (PSKs) [RFC-8446] allow for communication without frequent handshakes. Models depending on initial handshake packets may therefore fail to classify ongoing sessions accurately, limiting their applicability in real-world scenarios.
