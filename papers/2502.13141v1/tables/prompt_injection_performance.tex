\begin{table*}[th]
\vspace{-.15in}
\centering
\resizebox{0.75\textwidth}{!}{%
\begin{tabular}{cc|cc|cc|cc|cc|cc}
\toprule
\midrule
\multirow{2}{*}{Model} & \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Prompt Injections} & \multicolumn{2}{c|}{Jailbreak} & \multicolumn{2}{c|}{SST2}          & \multicolumn{2}{c|}{Open Question} & \multicolumn{2}{c}{SMS Spam}     \\
                       &                         & auROC             & auPRC             & auROC                 & auPRC                & auROC           & auPRC           & auROC            & auPRC           & auROC           & auPRC           \\\midrule
\multirow{2}{*}{-}     & Prompt-Guard-86M        & 0.5732            & 0.5567            & 0.5000                & 0.5305               & 0.5000          & 0.4997          & 0.5000           & 0.5000          & 0.5538          & 0.5284          \\
                       & PPL Detection           & 0.3336            & 0.4193            & 0.1932                & 0.3676               & 0.2342          & 0.3531          & 0.2822           & 0.3679          & 0.2051          & 0.3784          \\\midrule
\multirow{6}{*}{3B}    & Llama-Guard-3-1B        & 0.5839            & 0.5651            & 0.5628                & 0.5652               & 0.4987          & 0.4991          & 0.4727           & 0.4870          & 0.4803          & 0.4905          \\
                       & Llama-Guard-3-8B        & 0.5000            & 0.5172            & 0.5530                & 0.5751               & 0.5132          & 0.5101          & 0.5015           & 0.5010          & 0.5000          & 0.5000          \\
                       & Granite-Guardian-3.1-8B & 0.6339            & 0.7302            & 0.7382                & 0.7820               & 0.5978          & 0.5531          & 0.4216           & 0.4365          & 0.6322          & 0.5681          \\
                       & LLM-based detection     & 0.6917            & 0.6525            & 0.8263                & 0.7741               & 0.6636          & 0.5975          & 0.7985           & 0.7664          & 0.6523          & 0.5903          \\
                       & OpenAI Moderation       & 0.5500            & 0.5655            & 0.5752                & 0.5806               & 0.5000          & 0.4997          & 0.5015           & 0.5008          & 0.5000          & 0.5000          \\
                       & Ours                    & \textbf{0.7726}   & \textbf{0.7843}   & \textbf{0.8681}       & \textbf{0.8698}      & \textbf{0.8049} & \textbf{0.7648} & \textbf{0.8953}  & \textbf{0.8825} & \textbf{0.8019} & \textbf{0.7369} \\\midrule
\multirow{6}{*}{8B}    & Llama-Guard-3-1B        & 0.5054            & 0.5199            & 0.4851                & 0.5233               & 0.5080          & 0.5038          & 0.5348           & 0.5184          & 0.5000          & 0.5000          \\
                       & Llama-Guard-3-8B        & 0.5083            & 0.5253            & 0.5638                & 0.5850               & 0.4962          & 0.4997          & 0.5030           & 0.5030          & 0.5054          & 0.5054          \\
                       & Granite-Guardian-3.1-8B & 0.5780            & 0.6075            & 0.7831                & 0.7977               & 0.3791          & 0.4125          & 0.3212           & 0.3908          & 0.5862          & 0.5565          \\
                       & LLM-based detection     & 0.6976            & 0.6517            & 0.8218                & 0.7682               & 0.7376          & 0.6575          & 0.7470           & 0.7146          & 0.6165          & 0.5662          \\
                       & OpenAI Moderation       & 0.5577            & 0.5668            & 0.5856                & 0.5881               & 0.5033          & 0.5017          & 0.5000           & 0.5000          & 0.5000          & 0.5000          \\
                       & Ours                    & \textbf{0.7631}   & \textbf{0.7441}   & \textbf{0.8466}       & \textbf{0.8309}      & \textbf{0.8128} & \textbf{0.7682} & \textbf{0.8448}  & \textbf{0.8047} & \textbf{0.8117} & \textbf{0.7383} \\\midrule
\multirow{6}{*}{32B}   & Llama-Guard-3-1B        & 0.4571            & 0.4976            & 0.5411                & 0.5523               & 0.4937          & 0.4966          & 0.5227           & 0.5118          & 0.5018          & 0.5009          \\
                       & Llama-Guard-3-8B        & 0.5000            & 0.5172            & 0.5314                & 0.5555               & 0.4962          & 0.4997          & 0.5015           & 0.5015          & 0.5018          & 0.5011          \\
                       & Granite-Guardian-3.1-8B & 0.6503            & 0.6301            & 0.7893                & 0.7829               & 0.3741          & 0.4665          & 0.6333           & 0.6314          & 0.5037          & 0.5298          \\
                       & LLM-based detection     & 0.7173            & 0.6756            & 0.7924                & 0.7360               & 0.7459          & 0.6639          & 0.8742           & 0.8159          & 0.5771          & 0.5422          \\
                       & OpenAI Moderation       & 0.5583            & 0.5736            & 0.5618                & 0.5713               & 0.5137          & 0.5098          & 0.5106           & 0.5075          & 0.5018          & 0.5010          \\
                       & Ours                    & \textbf{0.7488}   & \textbf{0.7061}   & \textbf{0.8554}       & \textbf{0.8518}      & \textbf{0.7794} & \textbf{0.7246} & \textbf{0.8774}  & \textbf{0.8477} & \textbf{0.8542} & \textbf{0.7944} \\\midrule
\multirow{6}{*}{70B}   & Llama-Guard-3-1B        & 0.4792            & 0.5072            & 0.5111                & 0.5718               & 0.5045          & 0.5026          & 0.5015           & 0.5008          & 0.5108          & 0.5055          \\
                       & Llama-Guard-3-8B        & 0.5083            & 0.5253            & 0.5872                & 0.6347               & 0.4927          & 0.4998          & 0.5030           & 0.5023          & 0.5054          & 0.5033          \\
                       & Granite-Guardian-3.1-8B & 0.6211            & 0.6585            & 0.6876                & 0.6633               & 0.6028          & 0.5746          & 0.4795           & 0.4744          & 0.7186          & 0.6418          \\
                       & LLM-based detection     & 0.7190            & 0.6837            & \textbf{0.8444}       & 0.7997               & 0.6660          & 0.6003          & 0.7015           & 0.6832          & 0.6831          & 0.6077          \\
                       & OpenAI Moderation       & 0.5583            & 0.5736            & 0.5469                & 0.5599               & 0.5028          & 0.5013          & 0.5061           & 0.5042          & 0.4946          & 0.4985          \\
                       & Ours                    & \textbf{0.7577}   & \textbf{0.7745}   & 0.8294                & \textbf{0.8404}      & \textbf{0.7934} & \textbf{0.7681} & \textbf{0.8105}  & \textbf{0.7515} & \textbf{0.8043} & \textbf{0.7500}
\\
\midrule
\bottomrule
\end{tabular}%
}
\vspace{-.1in}
\caption{Comparison of detection performance on prompt injection.}\label{tbl:prompt_injection_performance}
\vspace{-.15in}
\end{table*}