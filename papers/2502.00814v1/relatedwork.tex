\section{Related Work}
\label{related-work}

In this section, we briefly introduce the background and related work of RLHF, reward hacking and length instruction following. A more detailed version is in Appendix~\ref{full-related work}.

{\bf Reinforcement Learning from Human Feedback.} RLHF~\citep{ziegler2019fine} involves training a reward model to approximate human preferences and optimizing the LLMs through reinforcement learning (RL)~\citep{schulman2017proximal}. However, this approach suffers from training instability and requires careful tuning of numerous hyperparameters. Recent direct preference alignment methods, particularly DPO~\citep{rafailov2024direct}, offer a more stable alternative. By reformulating the reward function, DPO eliminates the need for an online reward model, enabling robust offline preference learning~\citep{hong2024orpo, chen2024noise, ethayarajh2024kto}. Our method not only enhances RLHF by improving reward model robustness but also seamlessly integrates into the DPO framework.

{\bf Reward Hacking.} RLHF is vulnerable to reward hacking, where LLMs exploit inherent biases in the proxy preference model to achieve higher scores~\citep{pan2022effects, casper2023open, lambert2023alignment}. This phenomenon persists in DPO, primarily arising from task complexity, evaluation limitations, and biased feedback~\citep{dubois2024alpacafarm}. One common form of reward hacking is length bias~\citep{singhal2023long, park2024disentangling}, wherein models favor longer responses regardless of semantic quality. Previous attempts to address this issue include ODIN's~\citep{chenodin} length regularization in reward modeling; dual-model training with different learning rates~\citep{shen2023loose}; and DPO objective modification with length penalties~\citep{park2024disentangling}. In contrast to these approaches that suppress length information, our method enables models to distinguish between semantic intent and length instructions, preserving both aspects in a balanced manner.

{\bf Length Instruction Following.} While current LLMs demonstrate implicit length-following capabilities~\citep{yuan2024following}, responding to qualitative descriptors like ``concise" or ``verbose", they struggle with explicit numerical constraints such as ``$150$ words or less". Although LIFT~\citep{yuan2024following} improves length instruction adherence through specialized datasets, this approach compromises semantic quality, resulting in degraded overall performance. Our method, in contrast, achieves effective length instruction following while simultaneously enhancing semantic quality.