\section{Related Work}
\label{related-work}

In this section, we briefly introduce the background and related work of RLHF, reward hacking and length instruction following. A more detailed version is in Appendix~\ref{full-related work}.

{\bf Reinforcement Learning from Human Feedback.} RLHF____ involves training a reward model to approximate human preferences and optimizing the LLMs through reinforcement learning (RL)____. However, this approach suffers from training instability and requires careful tuning of numerous hyperparameters. Recent direct preference alignment methods, particularly DPO____, offer a more stable alternative. By reformulating the reward function, DPO eliminates the need for an online reward model, enabling robust offline preference learning____. Our method not only enhances RLHF by improving reward model robustness but also seamlessly integrates into the DPO framework.

{\bf Reward Hacking.} RLHF is vulnerable to reward hacking, where LLMs exploit inherent biases in the proxy preference model to achieve higher scores____. This phenomenon persists in DPO, primarily arising from task complexity, evaluation limitations, and biased feedback____. One common form of reward hacking is length bias____, wherein models favor longer responses regardless of semantic quality. Previous attempts to address this issue include ODIN's____ length regularization in reward modeling; dual-model training with different learning rates____; and DPO objective modification with length penalties____. In contrast to these approaches that suppress length information, our method enables models to distinguish between semantic intent and length instructions, preserving both aspects in a balanced manner.

{\bf Length Instruction Following.} While current LLMs demonstrate implicit length-following capabilities____, responding to qualitative descriptors like ``concise" or ``verbose", they struggle with explicit numerical constraints such as ``$150$ words or less". Although LIFT____ improves length instruction adherence through specialized datasets, this approach compromises semantic quality, resulting in degraded overall performance. Our method, in contrast, achieves effective length instruction following while simultaneously enhancing semantic quality.