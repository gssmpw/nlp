\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{xcolor}
\usepackage{ulem}
\normalem
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
\usepackage{hyperref}

\usepackage{multirow}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[stable]{footmisc}
\usepackage{enumitem}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Disentangling Length Bias in Preference Learning via Length-Instructed Ranking}
\icmltitlerunning{Disentangling Length Bias in Preference Learning via Response-Conditioned Modeling}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\ourbtmodel}{Rc-BT}
\newcommand{\ourrm}{Rc-RM}
\newcommand{\ourdpo}{Rc-DPO}
\newcommand{\jhz}[1]{\textcolor{red}{#1}}
\begin{document}

\twocolumn[
% \icmltitle{Disentangling Length Bias In Preference Learning Via Length-Instructed Ranking}
\icmltitle{Disentangling Length Bias In Preference Learning Via Response-Conditioned Modeling}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jianfeng Cai}{ustc}
\icmlauthor{Jinhua Zhu}{ustc}
\icmlauthor{Ruopei Sun}{ustc}
\icmlauthor{Yue Wang}{ir}
\icmlauthor{Li Li}{ustc}
\icmlauthor{Wengang Zhou}{ustc}
\icmlauthor{Houqiang Li}{ustc}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{ustc}{University of Science and Technology of China}
\icmlaffiliation{ir}{Independent Researcher}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Jinhua Zhu}{teslazhu@mail.ustc.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Reinforcement Learning from Human Feedback~(RLHF) has achieved considerable success in aligning large language models~(LLMs) by modeling human preferences with a learnable reward model and employing a reinforcement learning algorithm to maximize the reward model's scores. However, these reward models are susceptible to exploitation through various superficial confounding factors, with length bias emerging as a particularly significant concern. Moreover, while the pronounced impact of length bias on preference modeling suggests that LLMs possess an inherent sensitivity to length perception, our preliminary investigations reveal that fine-tuned LLMs consistently struggle to adhere to explicit length instructions.
To address these two limitations, we propose a novel framework wherein the reward model explicitly differentiates between human semantic preferences and response length requirements. Specifically, we introduce a Response-conditioned Bradley-Terry~(\ourbtmodel{}) model that enhances the reward model's capability in length bias mitigating and length instruction following, through training on our augmented dataset. Furthermore, we propose the Rc-DPO algorithm to leverage the \ourbtmodel{} model for direct policy optimization (DPO) of LLMs, simultaneously mitigating length bias and promoting adherence to length instructions.
Extensive evaluations demonstrate that our approach substantially improves both preference modeling and length instruction compliance, with its effectiveness validated across various foundational models and preference datasets.
\end{abstract}

\section{Introduction}
\label{introduction}
Reinforcement learning from human feedback~(RLHF) \citep{ziegler2019fine, stiennon2020learning, ouyang2022training} has revolutionized the field of natural language processing, enabling advancements in areas such as conversation \citep{bai2022training}, code generation \citep{poesia2022synchromesh, 10.1145/3672456}, complex planning \citep{hao-etal-2023-reasoning, zhao2024large}, mathematical reasoning \citep{imani2023mathprompter, luo2023wizardmath}, and so on. Within this framework, preference learning plays a pivotal role by using a generalizable model as a proxy for human preferences and optimizing large language models (LLMs) based on this preference model. However, a significant challenge in RLHF is the phenomenon termed reward overoptimization or reward hacking \citep{gao2023scaling}. This occurs when excessive optimization against the preference model undermines the attainment of the true objective. Consequently, LLMs may learn to exploit simpler criteria such as length, bullet points, or politeness~\citep{rame2024warm}, rather than more causal and nuanced indicators to achieve higher reward.

Among these reward hacking phenomena, length bias stands out as both a prevalent and challenging issues~\citep{dubois2024length}, and we identify it as a representative test bed, with the expectation that our approach can be readily extended to address other spurious correlations. To mitigate length bias in the RLHF pipeline, two primary strategies have emerged, which may be complementary: The first strategy focuses on rectifying the policy optimization process through techniques such as increasing the KL penalty coefficient, applying length penalty rewards~\citep{singhal2023long}, reward clipping~\citep{chenodin}, and various hyper-parameter tuning methods. The second strategy aims to disentangle length information from quality in reward modeling. This includes length balancing, reward data augmentation, confidence-based truncation in training data~\citep{singhal2023long}, and using sophisticated training objectives to separate length information~\cite{chenodin}.

Despite these advances, we observe several drawbacks in these methods. First, rectification methods for policy optimization are sensitive to hyperparameter changes and foundational model variations, and prior work demonstrates that their effectiveness is limited~\citep{singhal2023long, chenodin}. The second category shows superior performance compared to the first; however, over-parameterization of the two branches can cause unstable optimization~\citep{kurach2019the}, and regularization methods that encourage linear irrelevance may not guarantee true independence~\citep{shimony1993role}. Furthermore, ignoring the biased nature of the training data and forcing an irrelevancy or orthogonality between response quality and response length is not always reasonable. For instance, in some instruction-following datasets, such as length instruction dataset, response length may indeed be a factor of response quality. Finally, these methods regard length information as harmful and aim to minimize its influence on the instruction-following ability. However, whether we could leverage these length biases for better preference modeling remains an underexplored question.

Based on the aforementioned observations, we aim to improve preference modeling by leveraging response length information
% Unlike existing methods that solely exclude length information, 
and propose a novel approach that allows the preference model to explicitly differentiate between human semantic intent and response length instructions. Specifically, we first develop an augmented length instruction dataset derived from the original preference dataset. Subsequently, we introduce a Response-conditioned Bradley-Terry (\ourbtmodel{}) model, which improves the model's ability to mitigate length bias and follow length instructions.


Given the widespread use of paired data preference modeling in RLHF, our method can be seamlessly integrated into both the reward modeling and policy optimization stages. First, we empirically verify our method in the reward modeling task. The results demonstrate that our method not only enhances the reward model's ability to mitigate length bias, thereby improving its alignment with human semantic quality, but also strengthens its adherence to length instructions. Furthermore, applying our method to direct preference optimization (DPO) \citep{rafailov2024direct}, a popular alignment algorithm, further validates its effectiveness.

Our contributions are summarized as follows: (1) We propose the Response-conditioned Bradley-Terry method to mitigate length bias and enhance the model's capacity to follow length instructions. (2) We show that our method can be integrated into reward modeling and policy optimization with minimal adjustments, supported by extensive experiments in both tasks. (3) Experimental results illustrate the superiority of our method in terms of both mitigating length bias and length instruction following. 

\section{Related Work}
\label{related-work}

In this section, we briefly introduce the background and related work of RLHF, reward hacking and length instruction following. A more detailed version is in Appendix~\ref{full-related work}.

{\bf Reinforcement Learning from Human Feedback.} RLHF~\citep{ziegler2019fine} involves training a reward model to approximate human preferences and optimizing the LLMs through reinforcement learning (RL)~\citep{schulman2017proximal}. However, this approach suffers from training instability and requires careful tuning of numerous hyperparameters. Recent direct preference alignment methods, particularly DPO~\citep{rafailov2024direct}, offer a more stable alternative. By reformulating the reward function, DPO eliminates the need for an online reward model, enabling robust offline preference learning~\citep{hong2024orpo, chen2024noise, ethayarajh2024kto}. Our method not only enhances RLHF by improving reward model robustness but also seamlessly integrates into the DPO framework.

{\bf Reward Hacking.} RLHF is vulnerable to reward hacking, where LLMs exploit inherent biases in the proxy preference model to achieve higher scores~\citep{pan2022effects, casper2023open, lambert2023alignment}. This phenomenon persists in DPO, primarily arising from task complexity, evaluation limitations, and biased feedback~\citep{dubois2024alpacafarm}. One common form of reward hacking is length bias~\citep{singhal2023long, park2024disentangling}, wherein models favor longer responses regardless of semantic quality. Previous attempts to address this issue include ODIN's~\citep{chenodin} length regularization in reward modeling; dual-model training with different learning rates~\citep{shen2023loose}; and DPO objective modification with length penalties~\citep{park2024disentangling}. In contrast to these approaches that suppress length information, our method enables models to distinguish between semantic intent and length instructions, preserving both aspects in a balanced manner.

{\bf Length Instruction Following.} While current LLMs demonstrate implicit length-following capabilities~\citep{yuan2024following}, responding to qualitative descriptors like ``concise" or ``verbose", they struggle with explicit numerical constraints such as ``$150$ words or less". Although LIFT~\citep{yuan2024following} improves length instruction adherence through specialized datasets, this approach compromises semantic quality, resulting in degraded overall performance. Our method, in contrast, achieves effective length instruction following while simultaneously enhancing semantic quality.

\section{Preliminary Explorations} \label{3}
In this section, we conduct several preliminary investigations into the ability of LLMs to perceive and process length information, with an emphasis on the reward model. Following~\citet{yuan2024following}, we utilize the OpenAssistant dataset~\citep{kopf2024openassistant}, which is partitioned into three subsets: $\mathcal{D}_{sft}$ for supervised fine-tuning (SFT), $\mathcal{D}_{rm}$ for reward model (RM) training, and $\mathcal{D}_{eval}$ for RM evaluation. Additionally, we employ a range of models for subsequent experimental analyses, including Qwen2-1.5B, Qwen2.5-7B, and Llama-3.1-8B. Details of the training and evaluation procedures for this section are provided in Appendix \ref{appendix_a}.

\subsection{Length Bias Indeed Exists}
\label{3-1}
% The term {\em RM length bias} refers to the tendency of the trained reward model to assign higher reward scores to longer responses, while overlooking their semantic relevance to the prompt. 
To verify the presence of length bias in reward models trained on $\mathcal{D}_{rm} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}$, where $x^{(i)}$ represents a human request and response $y_w^{(i)}$ is preferred to $y_l^{(i)}$ according to human annotations, we design two additional evaluation settings based on $\mathcal{D}_{eval}$: 
\begin{itemize}[itemsep=0pt,topsep=0.0pt,] 
    \item In the first setting, each prompt $x^{(i)}$ within $\mathcal{D}_{eval}$ is replaced with an empty prompt $x_e =$ {\em ``empty prompt"}, forming the empty evaluation dataset $\mathcal{D}_{eval}^e$;
    \item In the second setting, each prompt $x^{(i)}$ is randomly replaced with another prompt $x^{(j)}$ (where $i \neq j$), resulting in the random evaluation dataset $\mathcal{D}_{eval}^r$.
\end{itemize}

The evaluation results are presented in Table \ref{baseline-rm-original-empty-prompt-result} of Appendix \ref{appendix:3-1}. As indicated in the table, despite the semantic misalignment between the responses $y_w^{(i)}$ and $y_l^{(i)}$\footnote{For clarity of notation, we omit the superscript $^{(i)}$ in subsequent discussions where no ambiguity exists.} in $\mathcal{D}_{eval}^e$ and $\mathcal{D}_{eval}^r$ with their respective prompts, the reward model nevertheless attains relatively high accuracy on these datasets. Specifically, nearly all models achieve an accuracy exceeding $60\%$, which is very close to the accuracy observed on the original dataset $\mathcal{D}_{eval}$. This finding suggests that there is a significant bias in the models towards preferring $y_w$, even when both responses are misaligned with the prompt.

Next, we examine the consistency of evaluation results across different reward models between $\mathcal{D}_{eval}^e$ (or $\mathcal{D}_{eval}^r$) and $\mathcal{D}_{eval}$, with a particular focus on the influence of varying prompts on the model's preferences. The results, as shown  in Table \ref{baseline-rm-original-empty-prompt-result}, reveal that the models exhibit the same preference for responses in over  $85\%$ of instances, despite the introduction of different prompts. This suggests that the models' preferences are not primarily driven by the prompts.

We further plot the relationship between response length and reward score for the reward model across $\mathcal{D}_{eval}$, $\mathcal{D}_{eval}^e$, and $\mathcal{D}_{eval}^r$, as shown in Figure \ref{fig:length-score-ranges-baseline} of Appendix \ref{appendix:3-4}. A strong linear correlation is observed, suggesting that the reward model heavily relies on response length as a criterion for evaluating response quality. These findings provide compelling evidence of length bias in the reward models.

% \begin{table*}[!ht]
%     \centering
%     \caption{Evaluation Results of Reward Model (Baseline) on Eval Dataset $\mathcal{D}_{eval}^e$ and $\mathcal{D}_{eval}^r$}
%     \begin{tabular}{lccccc}
%     \toprule
%         \multirow{3}{*}{ Model} & \multicolumn{4}{c}{Quality Eval Acc} & \multirow{3}{*}{Consistency} \\ 
%         & \multicolumn{2}{c}{Empty Prompt} & \multicolumn{2}{c}{Random Prompt} \\
%         & Acc & Length Ratio & Acc & Length Ratio \\
%         \midrule
%         Qwen2-1.5B-Base & 64.13 & 79.08 & 64.40 & 81.79 & 89.40 \\ 
%         Qwen2-1.5B-Instruct & 65.22 & 77.72 & 60.87 & 76.36 & 92.12 \\ 
%         \midrule
%         Qwen2.5-7B-Base & 60.05 & 69.02 & 60.33 & 68.75 & 88.32 \\
%         Qwen2.5-7B-Instruct & 62.23 & 82.07 & 60.60 & 78.53 & 88.60 \\
%         \midrule
%         Llama-3.1-8B-Base & 60.05 & 88.04 & 61.41 & 82.61 & 89.40 \\
%         Llama-3.1-8B-Instruct & 58.15 & 83.97 & 56.25 & 78.26 & 88.59 \\
%         \bottomrule
%     \end{tabular}
%     \label{baseline-rm-empty-prompt-result}
% \end{table*}

\subsection{Length Bias in Current Evaluation Dataset}
\label{3-2}

Here, we aim to verify whether the initially partitioned evaluation dataset $\mathcal{D}_{eval}$ exhibits length bias, which could hinder its ability to accurately evaluate the true performance of the reward models. Specifically, our analysis reveals that $59.78\%$ of the chosen responses in $\mathcal{D}_{eval}$ are longer than the corresponding rejected responses. Consequently, this length bias allows the reward model to attain an accuracy nearing $60\%$ merely by favoring longer responses, resulting in an unreliable evaluation of the model's performance. 

To fairly and accurately assess the semantic quality of the reward model, we reconstruct the quality evaluation dataset $\mathcal{D}_{eval}^q$ from $\mathcal{D}_{eval}$ through an automated process. Specifically, we employ GPT-4o~\citep{hurst2024gpt}, a state-of-the-art large language model, to transform each triplet $(x, y_w, y_l)$ in $\mathcal{D}_{eval}$ into two distinct triplets $(x, y_w^{1}, y_l^{1})$ and $(x, y_w^{2}, y_l^{2})$. These newly generated triplets adhere to the length constraints $|y_l^{1}| > |y_w^{1}|$\footnote{$|y|$ denotes the length of a given response $y$.} and $|y_l^{2}| < |y_w^{2}|$, while ensuring that $y_w^1$ and $y_w^2$, as well as $y_l^1$ and $y_l^2$, remain semantically similar to the original $y_w$ and $y_l$. The detailed generation methodology is documented in Appendix \ref{construct-D_eval^q}. 

The reconstructed evaluation dataset $\mathcal{D}_{eval}^q$ demonstrates significant improvement in mitigating the inherent length bias present in $\mathcal{D}_{eval}$,  thereby facilitating a more objective comparison among diverse reward models. As evidenced in Table \ref{tab:rm-result},  under this less biased evaluation dataset, Baseline models exhibit notably reduced performance metrics, with the majority of accuracy scores not exceeding $60\%$.

\subsection{Length Biased Reward Model Show Limited Adherence to Length Instructions}
\label{3-3}

The reward model demonstrates the length bias, indicating its ability to perceive response length. However, the question remains whether it can apply this ability to follow specific length instructions, where $x_l = \text{\em length constraint} + x$. For simplicity and consistency, we use the same {\em length constraint} as LIFT~\citep{yuan2024following}, which specified by {\em word\_num}, as detailed in Appendix \ref{LIFT-decomposition}.
To validate this, we construct the length evaluation dataset $\mathcal{D}_{eval}^l$ based on $\mathcal{D}_{eval}^q$, where both responses semantically satisfy the prompt $x$, but only one adheres to the length constraint. Appendix \ref{construct-D_eval^l} for more construction details.

Reviewing Table \ref{baseline-rm-original-empty-prompt-result}, we observe that due to the length bias inherent in both the reward models and the original evaluation dataset $\mathcal{D}_{eval}$, most reward models exhibit relatively high accuracy. However, when evaluated on $\mathcal{D}_{eval}^l$ (results shown in Table \ref{tab:rm-result}, denoted as Baseline), all models demonstrate the accuracy close to $50\%$, no better than chance.
% essentially performing random guessing. 
This demonstrates that reward models with length bias are unable to apply this ability to follow length instructions.

This finding suggests that while reward models unconsciously acquire length bias during preference learning, they lack explicit awareness of length as a measurable attribute. This raises an intriguing hypothesis: could explicit length instruction learning enable models to develop a clear perception of response length, thereby potentially mitigating the undesired length bias? Based on this motivation, we propose to incorporate length constraints into the training process, aiming to transform the implicit length bias into explicit length understanding.


\subsection{Length Information is Much Easily to Learn}
\label{3-4}
% In this section, we primarily examine the effectiveness of training with the commonly used $\{x_l, y_w, y_l\}$ data format in mitigating length bias (i.e., improving semantic quality) and in following length instructions.
% Following LIFT \citep{yuan2024following}, which adds a length constraint to the original prompt $x$ to generate the length instruction: $x_l = $ {\em length constraint} $ + x$, where {\em length constraint} is specified as ``Answer the following instruction using $\{word\_num\}$ words or less.", we improve the model's ability to follow length instructions by constructing a length-instruction dataset. Specifically, we refine original LIFT by addressing its limitation of considering only a maximum output length constraint (indicated by ``or less"), adding a minimum output length constraint (indicated by ``or more"), which refers to LIFT-plus. Next, we decompose LIFT-plus into $\text{LIFT}_1$ and $\text{LIFT}_2$, where in $\text{LIFT}_1$, the length instruction satisfies both responses, while in $\text{LIFT}_2$, the length instruction satisfies only one response. Here, we focus solely on $\text{LIFT}_2$, as it is the main component responsible for facilitating the learning of the length instruction\footnote{This will be demonstrated in subsequent analysis.}. Furthermore, we divide $\text{LIFT}_2$ into two variants: $\text{LIFT}_2^{reverse}$, which reverses the original preference order between chosen and rejected responses due to the length instruction constraint, and $\text{LIFT}_2^{noreverse}$, which preserves the original preference order. Additionally, we applied a purely length-based instruction dataset, $\text{LIFT}_2^{empty}$, where original prompt $x^{(i)}$ is replaced with {\em ``empty prompt"}, ensuring that the model focuses solely on the length instruction itself. For more detailed information on the processes involved in these three construction methods, please refer to Appendix \ref{LIFT-decomposition}.
% Specifically, we construct three training datasets by extending LIFT \citep{yuan2024following}: $\text{LIFT}_2^{reverse}$, which reverses the original preference order between chosen and rejected responses due to length constraints; $\text{LIFT}_2^{noreverse}$, which retains the original preference order; and $\text{LIFT}_2^{empty}$, where the original prompt $x$ is replaced with $x_e$. These datasets comprehensively represent the $\{x_l, y_w, y_l\}$ data paradigm. For more details, please refer to Appendix \ref{LIFT-decomposition}.

The most straightforward approach to explicit length instruction learning would be training directly on data formatted as $\{x_l, y_w, y_l\}$. To examine the effectiveness of this intuitive format, we extend LIFT~\citep{yuan2024following} to create LIFT-plus by incorporating minimum length constraints (``or more" length instruction) alongside its original maximum length constraints (``or less" length instruction). We then construct three variant datasets: 1) $\text{LIFT-plus}_2^{reverse}$, which reverses the preference order between chosen and rejected responses based on length constraints; 2) $\text{LIFT-plus}_2^{noreverse}$, which preserves the original preference ordering; and 3) $\text{LIFT-plus}_2^{empty}$, where substitutes the original prompt  $x$ with an empty prompt $x_e$ while maintaining the preference order. The last dataset examines the impact of semantic prompts on length instruction following. The construction process is detailed in Appendix~\ref{LIFT-decomposition}.

Empirical results presented in Table \ref{rm-length-prompt-result-reverse/unreverse/empty_prompt} (Appendix \ref{appendix:3-4}) reveal that while models demonstrate notably higher accuracy on $\mathcal{D}_{eval}^l$,  they achieve comparable or diminished accuracy on $\mathcal{D}_{eval}^q$ compared to the Baseline models in Table \ref{tab:rm-result}. This performance disparity suggests that reward models prioritize length instruction compliance at the expense of semantic understanding. Furthermore, our analysis of accuracy trajectories across training steps (Figure \ref{fig:length-quality-acc-train-loss-train-step}, Appendix \ref{appendix:3-4}) indicates a similar pattern: accuracy on $\mathcal{D}_{eval}^l$ exhibits rapid improvement, while accuracy on $\mathcal{D}_{eval}^q$ shows initial improvement followed by deterioration.

These findings demonstrate the limitations of the simple $\{x_l, y_w, y_l\}$ format: it leads to overfitting to length instructions while compromising the model's semantic capability, suggesting the need for a more sophisticated approach.

\begin{figure*}[!ht]
    \centering
    \vspace{-0.3cm}
    \includegraphics[width=0.9\textwidth]{figs/Rc-BT-overview-v3.pdf}
    % \vspace{-2ex}
    \vspace{-0.4cm}
    \caption{To illustrate the distinct data formats across different methods, we present the ``or less" length instruction case: \textbf{(a)} The conventional RLHF~\citep{ouyang2022training} with standard preference pair ($x$, $y_w$, $y_l$) ; \textbf{(b)} LIFT with augmented format  ($x_l$, $y_l$, $y_w$); and \textbf{(c)} Our method (\ourbtmodel{}) with two preference pairs ($x$, $x_l^1$, $y_w$) and ($x_l^2$, $x$, $y_l$).}
    \label{fig:Rc-BT-overview}
   \vspace{-0.4cm}
\end{figure*}

\section{Method}
\label{method}
In this section, we present our method \ourbtmodel{}, with the overall framework depicted in Figure \ref{fig:Rc-BT-overview}. In Section~\ref{traditional-bt-model}, we briefly review response preference modeling, specifically the Bradley-Terry (BT) model. In Section~\ref{4-2}, we introduce the \textbf{R}esponse-\textbf{c}onditioned \textbf{B}radley-\textbf{T}erry (\ourbtmodel{}) model\footnote{While primarily developed for length bias mitigation and length instruction adherence, the \ourbtmodel{} framework is theoretically applicable to diverse instruction-following tasks.}, presenting its algorithmic details and mathematical formulation. Finally, we demonstrate the model's practical implementation in both RM (Section~\ref{4-3}) and DPO (Section~\ref{4-4}) contexts through rigorous mathematical derivations.

\subsection{Response Preference Modeling}
\label{traditional-bt-model}
% \footnote{we use two  distinct answers in our paper, while the method and conclusion could be easily extent to multiple answers scenario.}
In RLHF, given a human request $x$, a supervised fine-tuned LLM denoted as $\pi^{\text{SFT}}$ is prompted with $x$ to sample response pairs $y_1$, $y_2$. Human feedback on this pair, conditioned on $x$, is then collected and represented as $y_w \succ y_l$, where $y_w$ is preferred than $y_l$. To modeling human response preferences, the Bradley-Terry (BT)~\citep{bradley1952rank} model is a prevalent choice~\citep{ouyang2022training}. The BT model models the underlining human preference distribution $p^*$ by assuming a underlining true reward  model $r^*$:
\begin{equation}
\small
    p^*(y_w\succ y_l) = \frac{\exp{(r^*(x, y_w))}}{\exp(r^*(x, y_w))+\exp(r^*(x, y_l))},
    \label{eqn:response_dis}
\end{equation}
where $r^*$ is not accessible or easily defined by several rules. Therefore, people try to parameterize a reward model $r_\phi$ and optimize the parameters via maximum likelihood or leverage an analytical mapping from reward function to optimal policy, to directly optimize LLM policy~\citep{ouyang2022training, dubey2024llama}. Given a high quality preference dataset $\{(x, y_{w}, y_l)\}$, the Bradley-Terry (BT) model has demonstrated significant efficacy in various preference modeling and policy optimization tasks~\citep{NEURIPS2023_4dbb61cb}.

\subsection{Response-Conditioned Bradley-Terry Model}
\label{4-2}

% \textcolor{red}{Response preference modeling learns by constructing preference pairs in a format like $\{p, c, r\}$, such as LIFT \citep{yuan2024following}.}
% Response preference modeling leverage Eqn.~\ref{eqn:response_dis} to force the reward model to differentiate the response, while in some cases for prompts containing multiple instructions, each response may only satisfy some of the instructions, making it difficult to reliably assess the quality of each response. For example, in Section \ref{3-4}, under $\text{LIFT}_2^{reverse}$, the original chosen response $y_w$ meets the requirements of the original prompt but does not fulfills the length instruction, while the rejected response does the opposite. In this situation, neither maintaining or reversing the original preference between chosen and rejected responses allows the model to effectively learn both the original prompt instructions and the length instructions simultaneously. This is an inherent issue with response preference modeling.
% Response preference modeling leverage Eqn.~\ref{eqn:response_dis} to force the reward model to differentiate the response. However, for prompts with multiple instructions, each response may only satisfy some of the instructions, making it difficult to reliably assess response quality. For instance, in Section \ref{3-4}, under $\text{LIFT}_2^{reverse}$, the original chosen response $y_w$ meets the prompt's requirements but fails to fulfill the length instruction, while the rejected response does the opposite. In this case, neither maintaining nor reversing the original preference allows the model to effectively learn both the original prompt and the length instruction simultaneously, which is an inherent issue with response preference modeling.
Response preference modeling employs Eqn.~\ref{eqn:response_dis} as a framework for response differentiation. However, a fundamental challenge arises in scenarios involving prompts with multiple instructions, where responses may only partially satisfy the given criteria. As previously discussed in Section \ref{3-4}, the conventional $\{x_l, y_w, y_l\}$ data format exhibits a tendency to overfit to length instructions, consequently compromising the model's semantic comprehension capabilities. This limitation represents an inherent constraint in traditional response preference modeling approaches.


To address this, we propose a {\em response-conditioned modeling} framework, as illustrated in Figure \ref{fig:Rc-BT-overview}.  Our approach proceeds as follows: given an original chosen response $y_w$ and prompt $x$, we construct a {length augmented instruction} $x_l^1$ by incorporating a {\em length constraint} with the original prompt $x$, ensuring that $y_w$ violates this constraint. We then formulate a preference pair $(x, x_l^1, y_w)$, where $(x, y_w)$  is considered preferable to $(x_l^1, y_w)$. This preference structure is then modeled using the Bradley-Terry formulation:
\begin{equation}
\small
    p^*(x\succ x_l^1|y_w) = \frac{\exp{(r^*(x, y_w))}}{\exp(r^*(x, y_w))+\exp(r^*(x_l^1, y_w))}.
    \label{eqn:prompt_dis_chosen}
\end{equation}
Similarly, for a given rejected response $y_l$ and original prompt $x$, we construct a length augmented instruction $x_l^2$ such that $y_l$ satisfies the specified length constraint. We then formulate a preference pair $(x_l^2, x, y_l)$, where $(x_l^2, y_l)$ is considered preferable to $(x, y_l)$. This preference structure is similarly modeled using the Bradley-Terry formulation:
\begin{equation}
\small
    p^*(x_l^2\succ x|y_l) = \frac{\exp{(r^*(x_l^2, y_l))}}{\exp(r^*(x_l^2, y_l))+\exp(r^*(x, y_l))}.
    \label{eqn:prompt_dis_reject}
\end{equation}
The final response-conditioned modeling preference dataset is defined as $\mathcal{D}_{Rc} = \{(x, x_l^1, y_w)\} \cup \{(x_l^2, x, y_l)\}$. Through maximum likelihood estimation, we derive the Response-conditioned BT modeling objective function as follows:
\begin{equation}
\small
    \begin{aligned}
        \mathcal{L}_{Rc} = & -\underset{(x, x_l^1, y_w) \sim \mathcal{D}_{Rc}}{\mathbb{E}}[\log p^*(x\succ x_l^1|y_w)] \\& - \underset{(x_l^2, x, y_l) \sim \mathcal{D}_{Rc}}{\mathbb{E}}[\log p^*(x_l^2\succ x|y_l)].
    \end{aligned}    
    \label{eqn:prompt_dis}
\end{equation}
The \ourbtmodel{} framework facilitates explicit comparisons between $x$ and its length-augmented variants ($x_l^1$ and $x_l^2$), enabling the model to systematically perceive response lengths and thus mitigating implicit length bias through explicit length understanding.

\subsection{Response-Conditioned Reward Model}
\label{4-3}

In response preference modeling, the reward model is initialized from $\pi^\text{SFT}$ and augmented with a linear projection layer that transforms the complete sequence representation into a scalar reward value $r_\phi(x, y)$. Given a preference dataset $\mathcal{D}=\{(x, y_{w}, y_l)\}$, the reward model is optimized through maximum likelihood estimation according to Eqn.~\ref{eqn:response_dis}:
\begin{equation}
% \small
        \mathcal{L}_{r_\phi}(\mathcal{D}) = -\underset{(x, y_{w}, y_l) \sim \mathcal{D}}{\mathbb{E}}[ \log\sigma (r_\phi(x, y_{w})- r_\phi(x, y_l))],
    \label{eqn:prompt-conditioned-rm}
\end{equation}
where $\sigma(\cdot)$ is the sigmoid function.

In response-conditioned modeling, the architectural structure of the parameterized reward model $r_\phi$ remains unchanged while the data format is transformed from prompt-conditioned to response-conditioned. Similar to Eqn. \ref{eqn:prompt-conditioned-rm}, the model is optimized through maximum likelihood estimation based on Eqn. \ref{eqn:prompt_dis} using dataset $\mathcal{D}_{Rc}$:
\begin{equation}
\small
        \begin{aligned}
            \mathcal{L}_{r_\phi}(\mathcal{D}_{Rc}) = & -\underset{(x, x_l^1, y_w) \sim \mathcal{D}_{Rc}}{\mathbb{E}}[\log\sigma (r_\phi(x, y_w) - r_\phi(x_l^1, y_w))] \\ & - \lambda \underset{(x_l^2, x, y_l) \sim \mathcal{D}_{Rc}}{\mathbb{E}}[\log\sigma (r_\phi(x_l^2, y_l) - r_\phi(x, y_l))],
        \end{aligned}
    \label{eqn:response-conditioned-rm}
\end{equation}
where $\lambda$ (typically set to $1$) is used to balance the relative contribution of the pairs $(x, x_l^1, y_w)$ and $(x_l^2, x, y_l)$.

\subsection{Response-Conditioned Direct Preference Optimization}
\label{4-4}
In response preference modeling, Direct Preference Optimization (DPO)~\citep{rafailov2024direct} derives an alternative formulation from the objective in Eqn.~\ref{eqn:prompt-conditioned-RL}, where the reward is expressed as a function of the optimal policy. By substituting this formulation into the reward optimization objective specified in Eqn. \ref{eqn:prompt-conditioned-rm}, we obtain a direct optimization approach for policy training. Specifically, the policy can be optimized on dataset $\mathcal{D}$ using the following objective function:
\begin{equation}
\small
    \begin{aligned}
       & \mathcal{L}_{DPO}(\pi_\theta; \pi_\text{ref}) = \\ & -\underset{(x, y_w, y_l) \sim \mathcal{D}}{\mathbb{E}}[\log\sigma(\beta \log\frac{\pi_\theta(y_w|x)}{\pi_\text{ref}(y_w|x)} - \beta \log\frac{\pi_\theta(y_l|x)}{\pi_\text{ref}(y_l|x)})].
    \end{aligned}
     \label{eqn:origin-dpo}
\end{equation}
% For the response-conditioned case, we can perform a derivation similar to that in \citep{rafailov2024direct}. Starting from the original RL objective, we derive the optimal solution to the reward maximization objective under a KL constraint. This leads to the reparameterization of the reward model, which can then be substituted into the prompt-condition BT modeling function Eqn. \ref{eqn:prompt_dis} and yielding the response-conditioned DPO objective according to formulating a maximum likelihood objective:
For the response-conditioned scenario, we follow an analogous derivation process to DPO. Specifically, we begin with the modified RL objective (Eqn. \ref{eqn:response-conditioned-RL}), derive the reward model expression in terms of the optimal policy, and subsequently incorporate it into the Response-conditioned BT modeling function (Eqn.~\ref{eqn:prompt_dis}). This derivation yields the Response-conditioned DPO (Rc-DPO) objective function:
\begin{equation}
\small
        \begin{aligned}
            & \mathcal{L}_{DPO}^{Rc}(\pi_\theta; \pi_\text{ref}) = \\ & -\underset{(x, x_l^1, y_w) \sim \mathcal{D}_{Rc}}{\mathbb{E}} [\log\sigma(\beta \log\frac{\pi_\theta(x, y_w)}{\pi_\text{ref}(x, y_w)} - \beta \log\frac{\pi_\theta(x_l^1, y_w)}{\pi_\text{ref}((x_l^1, y_w)})] \\ & -\underset{((x_l^2, x, y_l) \sim \mathcal{D}_{Rc}}{\mathbb{E}} [\log\sigma(\beta \log\frac{\pi_\theta((x_l^2, y_l)}{\pi_\text{ref}(x_l^2, y_l)}) - \beta \log\frac{\pi_\theta(x, y_l)}{\pi_\text{ref}(x, y_l)}].
        \end{aligned}
    \label{eqn:response-conditioned-dpo}
\end{equation}
See Appendix \ref{appendix_b} for a complete derivation. 

\section{Experimental Results}
\label{5}
% \begin{table*}[!ht]
%     \centering
%     \caption{Evaluation Results of Reward Model on Quality and Length Eval Datasets}
%     \label{tab:rm-result}
%     \vskip 0.1in
%     \begin{center}
%         \begin{tabular}{llcc}
%             \toprule
%             Model & Variant & Quality Eval Acc & Length Eval Acc \\ 
%             \midrule
%             \multirow{4}{*}{Qwen2-1.5B-Base} & baseline & 59.14 & 52.41 \\ 
%             & LIFT-plus & 58.78 & \textbf{87.18} \\
%             & ODIN & 56.12 & 54.17 \\
%             & our & \textbf{69.55} & 86.22 \\ 
%             \midrule
%             \multirow{4}{*}{Qwen2-1.5B-Instruct} & baseline & 60.75 & 51.77 \\ 
%             & LIFT-plus & 60.11 & \textbf{86.54} \\ 
%             & ODIN & 63.56 & 49.04 \\
%             & our & \textbf{71.47} & 84.61 \\ 
%             \midrule
%             \multirow{4}{*}{Qwen2.5-7B-Base} & baseline & 54.26 & 52.24 \\ 
%             & LIFT-plus & 59.31 & 84.94 \\
%             & ODIN & 60.90 & 52.88 \\
%             & our & \textbf{70.74} & \textbf{88.14} \\ 
%             \midrule
%             \multirow{4}{*}{Qwen2.5-7B-Instruct} & baseline & 59.31 & 52.88 \\ 
%             & LIFT-plus & 53.99 & 83.97 \\ 
%             & ODIN & 67.55 & 48.71 \\
%             & our & \textbf{73.07} & \textbf{92.31} \\ 
%             \midrule
%             \multirow{4}{*}{Llama-3.1-8B-Base} & baseline & 59.04 & 51.92 \\ 
%             & LIFT-plus & 51.34 & \textbf{91.32} \\
%             & ODIN & 60.37 & 50.00 \\
%             & our & \textbf{70.51} & 88.46 \\ 
%             \midrule
%             \multirow{4}{*}{Llama-3.1-8B-Instruct} & baseline & 55.59 & 49.04 \\ 
%             & LIFT-plus & 60.11 & \textbf{95.19} \\ 
%             & ODIN & 60.90 & 50.64 \\
%             & our & \textbf{72.44} & 93.27 \\ 
%             \bottomrule
%         \end{tabular}
%     \end{center}
%     \vskip -0.1in
% \end{table*}
\begin{table*}[!ht]
    \centering
    \small
    \caption{Evaluation results of different reward models on quality ($\mathcal{D}_{eval}^q$) and length ($\mathcal{D}_{eval}^l$) evaluation datasets}
    \label{tab:rm-result}
    % \vskip 0.1in
    \begin{center}
        \begin{tabular}{ccccccccc}
            \toprule
            \multirow{2}{*}{Metrics} & \multicolumn{4}{c}{Qwen2-1.5B-Base} & \multicolumn{4}{c}{Qwen2-1.5B-Instruct} \\
            \cmidrule(lr){2-5} \cmidrule(lr){6-9} & Baseline & LIFT-plus & ODIN & Rc-RM & Baseline & LIFT-plus & ODIN & Rc-RM \\
            \cmidrule(lr){1-1} Quality Eval Acc ($\%$) & 59.14 & 58.78 & 56.12 & \textbf{69.55} & 60.75 & 60.11 & 63.56 & \textbf{71.47} \\
            Length Eval Acc ($\%$) & 52.41 & \textbf{87.18} & 54.17 & 86.22 & 51.77 & \textbf{86.54} & 49.04 & 84.61 \\ 
            \midrule
            \multirow{2}{*}{Metrics} & \multicolumn{4}{c}{Qwen2.5-7B-Base} & \multicolumn{4}{c}{Qwen2.5-7B-Instruct} \\
            \cmidrule(lr){2-5} \cmidrule(lr){6-9} & Baseline & LIFT-plus & ODIN & Rc-RM & Baseline & LIFT-plus & ODIN & Rc-RM \\
            \cmidrule(lr){1-1} Quality Eval Acc ($\%$) & 54.26 & 59.31 & 60.90 & \textbf{70.74} & 59.31 & 53.99 & 67.55 & \textbf{73.07} \\
            Length Eval Acc ($\%$) & 52.24 & 84.94 & 52.88 & \textbf{88.14} & 52.88 & 83.97 & 48.71 & \textbf{92.31} \\ 
            \midrule
            \multirow{2}{*}{Metrics} & \multicolumn{4}{c}{Llama-3.1-8B-Base} & \multicolumn{4}{c}{Llama-3.1-8B-Instruct} \\
            \cmidrule(lr){2-5} \cmidrule(lr){6-9} & Baseline & LIFT-plus & ODIN & Rc-RM & Baseline & LIFT-plus & ODIN & Rc-RM \\
            \cmidrule(lr){1-1} Quality Eval Acc ($\%$) & 59.04 & 51.34 & 60.37 & \textbf{70.51} & 55.59 & 60.11 & 60.90 & \textbf{72.44} \\
            Length Eval Acc ($\%$) & 51.92 & \textbf{91.32} & 50.00 & 88.46 & 49.04 & \textbf{95.19} & 50.64 & 93.27 \\ 
            \bottomrule
        \end{tabular}
    \end{center}
    \vskip -0.1in
\end{table*}
\begin{figure*}[!ht]
    % \centering
    \vspace{-0.2cm}
    \begin{center}
        \subfigure[Qwen2-1.5B-Instruct]{\includegraphics[width=0.248\textwidth]{figs/qwen2-1.5b-chat-multi-length-length-bias-abs_v3.pdf}}
        \subfigure[Llama-3.1-8B-Instruct]{\includegraphics[width=0.245\textwidth]{figs/llama-3.1-8b-chat-multi-length-length-bias-abs_v3.pdf}}
        \subfigure[Qwen2-1.5B-Instruct]{\includegraphics[width=0.242\textwidth]{figs/qwen2-1.5B_Instruct_length_less_range_eval_chosen_result_pair_diff_total_v2.pdf}}
        \subfigure[Llama-3.1-8B-Instruct]{\includegraphics[width=0.242\textwidth]{figs/llama-3.1-8B_Instruct_length_less_range_eval_chosen_result_pair_diff_total_v2.pdf}}
        \vspace{-3ex}
        \caption{\textbf{(a) (b).} Analysis of reward scores across  models on  $\mathcal{D}_{eval}^{ml}$ as a function of response length, with smaller changes indicating reduced length bias.  \textbf{(c) (d).} Comparison of score differences between LIFT-plus and Rc-RM on $\mathcal{D}_{eval}^{mls}$ under varying {\em word\_num} constraints. An ideal reward model should show an initial increase in score difference followed by a return to its initial value.}
        \label{fig:multi-length-bias-multi-wordnum-rm-score-diff}
    \end{center}
    \vspace{-0.6cm}
\end{figure*}
% In this section, we conduct experiments to thoroughly demonstrate the effectiveness of our method. In Section \ref{experiment-settings}, we provide a detailed description of our experimental setup.
% , including the training dataset and models, training parameters, comparison methods and evaluation metrics
% Then, in Section \ref{section:rm-results} and \ref{dpo-results}, we validate the effectiveness of our \ourbtmodel{} on RM and DPO from multiple aspects. Finally, in Section \ref{ablations}, we perform further ablation studies to evaluate the complementary effects of the two modules in \ourbtmodel{}, along with additional experiments on diverse datasets to assess the generalizability of \ourbtmodel{}.
In this section, we present comprehensive experimental evaluations to demonstrate the effectiveness of our proposed \ourbtmodel{} framework. Furthermore, we conduct ablation studies to validate the key design choices of our approach.

\subsection{Settings}
\label{experiment-settings}
{\bf Dataset and Models.} Consistent with Section \ref{3}, for the dataset, we use $\mathcal{D}_{sft}$ for all SFT processes to finetune the {\em Base} models. For RM and DPO, \ourbtmodel{} generates an augmented dataset $\mathcal{D}_{Rc}$ derived from $\mathcal{D}_{rm}$. Both {reward models} and DPO models are trained on the combined dataset $\mathcal{D}_{rm} \cup \mathcal{D}_{Rc}$, referred to as Rc-RM and Rc-DPO, respectively.
We employ three pretrained models as our base models: Qwen2-1.5B\footnote{We also conduct experiments using the Qwen2.5-1.5B models, which shown in Appendix \ref{qwen2.5-1.5b-results}.}, Qwen2.5-7B, and Llama-3.1-8B.


{\bf Training Details.} For our Rc-RM training, the learning rate is set to $1 \times 10^{-5}$, followed by a cosine learning rate schedule with an initial warmup of $10$ steps and a batch size of $64$. Each experiment is trained for $5$ epochs. For Rc-DPO training, the settings are identical to RM training, except the learning rate is $1 \times 10^{-6}$. All experiments are implemented based on DeepSpeed \citep{yao2023deepspeed} and Huggingface Transformers \citep{wolf2020transformers}, and conducted on a single machine with $8$ NVIDIA A$100$ $80$GB GPUs. 

% {\bf Compared Methods and Evaluation.} Due to the similarity of objectives, our primary baseline for comparison is LIFT-plus \citep{yuan2024following}. Additionally, for RM, we also select ODIN \citep{chenodin} for comparison, as it primarily uses its quality head to predict a quality reward score without length bias and is not trained with length instructions. Therefore, we mainly compare its Quality Accuracy. For DPO, we further include Length Regularized DPO (R-DPO) \citep{park2024disentangling} as a comparison method. Unless otherwise specified, all compared methods are trained using the recommended parameters from their respective papers.
\begin{table*}[!ht]
    \centering
    \small
    \caption{Evaluation results of different DPO models on AlpacaEval \citep{dubois2024alpacafarm}}
    % \vskip 0.1in
    \begin{tabular}{ccccccccc}
        \toprule
        \multirow{2}{*}{Metrics} & \multicolumn{4}{c}{Qwen2.5-7B-Base} & \multicolumn{4}{c}{Qwen2.5-7B-Instruct} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9} & Baseline & LIFT-plus & R-DPO & Rc-DPO & Baseline & LIFT-plus & R-DPO & Rc-DPO \\
        \cmidrule(lr){1-1} Quality Win Ratio (\%) & 33.54 & 31.67 & 40.40 & \textbf{45.39} & 28.43 & 25.69 & 34.16 & \textbf{44.63} \\
        Response Length & 517.30 & 184.17 & 583.18 & 208.42 & 261.32 & 195.80 & 235.39 & 228.24 \\ 
        \midrule
        \multirow{2}{*}{Metrics} & \multicolumn{4}{c}{Llama-3.1-8B-Base} & \multicolumn{4}{c}{Llama-3.1-8B-Instruct} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9} & Baseline & LIFT-plus & R-DPO & Rc-DPO & Baseline & LIFT-plus & R-DPO & Rc-DPO \\
        \cmidrule(lr){1-1} Quality Win Ratio (\%) & 46.30 & 40.15 & 49.13 & \textbf{58.10} & 42.52 & 47.88 & 42.64 & \textbf{64.34} \\
        Response Length & 435.98 & 157.80 & 465.72 & 202.01 & 247.74 & 153.77 & 215.82 & 204.77 \\ 
        \bottomrule
    \end{tabular}
    \vspace{-0.3cm}
    \label{tab:dpo-origin-quality-result}
\end{table*}
\begin{table*}[!ht]
    \centering
    \small
    \caption{Evaluation results of different DPO models on AlpacaEval-LI-plus-less}
    % \vskip 0.1in
    \begin{tabular}{ccccccccc}
        \toprule
        \multirow{2}{*}{Metrics} & \multicolumn{4}{c}{Qwen2.5-7B-Base} & \multicolumn{4}{c}{Qwen2.5-7B-Instruct} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9} & Baseline & LIFT-plus & R-DPO & Rc-DPO & Baseline & LIFT-plus & R-DPO & Rc-DPO \\
        \cmidrule(lr){1-1} Length Acc (\%) & 5.74 & \textbf{86.78} & 8.35 & 82.04 & 89.65 & \textbf{100} & 90.90 & \textbf{100} \\
        Response Length & 544.47 & 106.87 & 637.44 & 177.40 & 136.75 & 23.56 & 131.17 & 108.24 \\ 
        Length Win Ratio (\%) & 34.04 & 1.37 & 34.54 & \textbf{44.14} & 32.17 & 2.99 & 31.92 & \textbf{50.75} \\ 
        \midrule
        \multirow{2}{*}{Metrics} & \multicolumn{4}{c}{Llama-3.1-8B-Base} & \multicolumn{4}{c}{Llama-3.1-8B-Instruct} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9} & Baseline & LIFT-plus & R-DPO & Rc-DPO & Baseline & LIFT-plus & R-DPO & Rc-DPO \\
        \cmidrule(lr){1-1} Length Acc (\%) & 13.59 & \textbf{98.75} & 27.68 & 94.51 & 87.66 & \textbf{100} & 87.03 & \textbf{100} \\
        Response Length & 424.47 & 6.62 & 518.74 & 118.50 & 150.43 & 35.01 & 151.39 & 89.75 \\ 
        Length Win Ratio (\%) & 35.41 & 0.25 & 47.88 & \textbf{64.96} & 43.89 & 1.00 & 43.77 & \textbf{64.71} \\
        \bottomrule
    \end{tabular}
    \label{tab:dpo-length-result-or-less}
\end{table*}

{\bf Compared Methods and Evaluation.}
% For RM evaluation, we use the accuracy on $\mathcal{D}_{eval}^q$ and $\mathcal{D}_{eval}^l$ generated in Section \ref{3} to compare response quality and length instruction following across all methods, referred to as {\em Quality Eval Acc} and {\em Length Eval Acc}, respectively. Since $\mathcal{D}_{eval}^q$ are carefully constructed to balance the length between chosen and rejected responses, it substantially reduces length bias within the evaluation set, enabling a fairer assessment of the true capability of the RM.
For RM evaluation, we assess the performance across methods using two primary metrics: {\em Quality Eval Acc} and {\em Length Eval Acc}, which are accuracy on $\mathcal{D}_{eval}^q$ and $\mathcal{D}_{eval}^l$ respectively. The former metric evaluates semantic quality with a focus on length bias mitigation, while the latter measures the effectiveness of length instruction adherence.


For DPO evaluation, leveraging recent advancements in automated assessment approaches  \citep{zheng2023judging, dubois2024alpacafarm}, we employ a model-based evaluation framework to systematically assess the quality of generated responses. First, we construct the AlpacaEval-LI-plus-less and AlpacaEval-LI-plus-more benchmarks, enhanced versions of AlpacaEval \citep{dubois2024alpacafarm}, designed to evaluate the model's ability to follow ``or less" and ``or more" length instructions, respectively. Then, we evaluate each DPO model's performance by comparing it with its corresponding SFT/{\em Instruct} model using two complementary metrics. The first metric, {\em Length Acc}, quantifies the model's adherence to length instructions by measuring how well the generated responses conform to the specified length constraints in $x_l$. The second metric assesses the model's semantic instruction following capability through two comparative win ratios: {\em Length Win Ratio} represents the proportion of cases where the model generates semantically superior responses when prompted with length-augmented instructions ($x_l$), while {\em Quality Win Ratio} measures the proportion of cases where the model demonstrates better semantic quality with original prompts ($x$). For comprehensive evaluation details, please refer to Appendix \ref{dpo-eval-details}.

We refer to the model trained on the $\mathcal{D}_{rm}$ as {\bf Baseline}. Due to the similarity of objectives, our main comparative method is {\bf LIFT-plus}. For RM, we additionally incorporate {\bf ODIN}~\citep{chenodin} as another comparison method, with particular emphasis on its {\em Quality Eval Acc}, owing to its demonstrated effectiveness in mitigating length bias. For DPO, we further include Length Regularized DPO ({\bf R-DPO}) \citep{park2024disentangling} as a comparison method. Unless otherwise specified, all compared methods are trained using the recommended settings from their papers.

\subsection{The Results and Analysis of Reward Models}
\label{section:rm-results}
% Since each prompt in $\mathcal{D}_{eval}^q$ corresponds to two pairs of responses with opposite lengths, stronger length bias tends to drive the model's {\em Quality Eval Acc} towards $50\%$. 
% {\bf Rc-RM significantly alleviates length bias.} The results in Table \ref{tab:rm-result} clearly demonstrate the effectiveness of \ourbtmodel{} in mitigating length bias. Specifically, \ourbtmodel{} outperforms the baseline and ODIN, a method specifically designed to mitigate length bias, in all models. As analyzed in Section \ref{3}, LIFT-plus has detrimental effects on semantic quality, and the results shown in Table \ref{tab:rm-result} show that it does not effectively address the length bias issue. In particular, \ourbtmodel{} outperforms the baseline by $10.41\%$ on Qwen2-1.5B-Base, exceeds LIFT-plus by $10.77\%$, and surpasses ODIN by $13.43\%$. On Qwen2-1.5B-Instruct, \ourbtmodel{} outperforms the baseline by $10.72\%$, exceeds LIFT-plus by $11.36\%$, and surpasses ODIN by $7.91\%$. Similarly, on Qwen2.5-7B-Base and Qwen2.5-7B-Instruct, \ourbtmodel{} outperforms ODIN by $9.84\%$ and $5.52\%$, respectively. Furthermore, on Llama-3.1-8B-Base and Llama-3.1-8B-Instruct, \ourbtmodel{} is $10.14\%$ and $11.54\%$ higher than ODIN, respectively, fully demonstrating the generalizability of our approach across different models.
{\bf Rc-RM significantly alleviates length bias.} The {\em Quality Eval Acc} results in Table \ref{tab:rm-result} clearly demonstrate the effectiveness of \ourrm{} in mitigating length bias. Specifically, as analyzed in Section \ref{3}, LIFT-plus has detrimental effects on semantic quality and fails to effectively address the length bias. In contrast, \ourrm{} outperforms both Baseline and ODIN across all models. Notably, on Qwen2-1.5B-Base, \ourrm{} exceeds Baseline by $10.41\%$ and outperforms ODIN by $13.43\%$, while on Llama-3.1-8B-Instruct, it surpasses Baseline by $16.85\%$ and ODIN by $11.54\%$, demonstrating the broad effectiveness of \ourrm{} across different models.

% Furthermore, for $\mathcal{D}_{eval}$, we rewrite each $(x, y_w)$ in to multiple length-varied pairs $(x, y_w^{j}), j=\{1,2,3\}$, while maintaining the quality consistency between $y_w$ and $y_w^{j}$. These pairs were then sorted by length to create a multi-length evaluation set, denoted as $\mathcal{D}_{eval}^{ml} = \{(x, y_w^{1}), (x, y_w^{2}), (x, y_w^{3})\}$, where $|y_w^{1}| < |y_w^{2}| < |y_w^{3}|$. Since the quality of $\mathcal{D}_{eval}^{ml}$ remains unchanged and only the length varies, the RM with less length bias will show smaller fluctuations in its scores on this set (essentially smaller increases in scores). We perform evaluations for each method based on Qwen2-1.5B-Instruct and Llama-3.1-8B-Instruct, and the results are shown in Figure \ref{fig:multi-length-bias-multi-wordnum-rm-score-diff}. To facilitate comparison, we align the scores of each model on the shortest $\{x, y_w^{1}\}$ pair. 
Furthermore, for $\mathcal{D}_{eval}$, we rewrite each $(x^{(i)}, y_w^{(i)})$ into multiple length-varied pairs {$\mathcal{D}_{eval}^{ml} = \{(x^{(i)}, y_w^{(i, j)})\}, j\in\{1,2,3\}$}, where $|y_w^{(i, 1)}| < |y_w^{(i, 2)}| < |y_w^{(i, 3)}|$, while maintaining the semantic consistency between $y_w^{(i)}$ and $y_w^{(i, j)}$. Then we evaluate each {reward model} on $\mathcal{D}_{eval}^{ml}$, with the results shown in Figure \ref{fig:multi-length-bias-multi-wordnum-rm-score-diff}(a) and \ref{fig:multi-length-bias-multi-wordnum-rm-score-diff}(b), where the reward model with less length bias exhibits smaller slopes in its scores.
% From the results, we can see that compared to the baseline, the LIFT-plus does not alleviate the length bias issue. In fact, it even exacerbates the problem. ODIN shows some reduction in length bias, but the reduction is modest (Figure \ref{fig:multi-length-bias-multi-wordnum-rm-score-diff} (a)). Moreover, in Llama-3.1-8B-Instruct (Figure \ref{fig:multi-length-bias-multi-wordnum-rm-score-diff} (b)), due to excessive length penalty, the ODIN model's score decreases as the response length increases. On the other hand, \ourbtmodel{} demonstrates better stability in both models, meaning that as the length increased, the increase in score was minimal. The above experiments clearly demonstrate the effectiveness of \ourbtmodel{} in mitigating length bias. 
The results indicate that both Baseline and LIFT-plus show an upward trend in their scores, while ODIN fluctuates due to varying degrees of length penalty. \ourrm{}, however, demonstrates superior stability, clearly highlighting its effectiveness in mitigating length bias.
% To facilitate comparison, we aligned the scores of each model on the shortest $\{x^{(i)}, y_w^{(i), l}\}$ pair (Figure \ref{fig:multi-length-bias}-Left). To avoid overall bias caused by extreme values, we also plotted the results in terms of relative scores. Specifically, we used the score on the middle $\{x^{(i)}, y_w^{(i), 2}\}$ pair as the baseline (considered as score $0$). For $\{x^{(i)}, c_i^3\}$, scores lower than $\{x^{(i)}, y_w^{(i), 2}\}$ were considered as $-1$, and those higher were considered as $+1$, as shown in Figure \ref{fig:multi-length-bias}-Right.
% \begin{figure*}[!ht]
%     % \centering
%     \vskip 0.1in
%     \begin{center}
%         \includegraphics[width=0.49\textwidth]{figs/qwen2-1.5b-chat-multi-length-length-bias-abs.pdf}
%         \includegraphics[width=0.49\textwidth]{figs/llama-3.1-8b-chat-multi-length-length-bias-abs.pdf}
%         \caption{\textbf{Left.} Average Score of Qwen2-1.5B-Instruct on $\mathcal{D}_{eval}^{ml}$. \textbf{Right.} Average Score of Llama-3.1-8B-Instruct on $\mathcal{D}_{eval}^{ml}$.}
%         \label{fig:multi-length-bias}
%     \end{center}
%     \vskip -0.1in
% \end{figure*}

{\bf Rc-RM effectively follows length instructions.} In addition to mitigating length bias, \ourrm{} significantly enhances the ability to follow length instructions, as demonstrated by the {\em Length Eval Acc} results in Table \ref{tab:rm-result}.
% Since both the baseline and ODIN were not trained on length instructions, their {\em Length Eval Acc} are close to $50\%$. Therefore, the primary comparison is with LIFT-plus. 
Specifically, \ourrm{} achieves accuracy comparable to LIFT-plus and surpasses it in nearly half of the models, with gains of $3.2\%$ on Qwen2.5-7B-Base, and $8.34\%$ on Qwen2.5-7B-Instruct. 

Using the ``or less" length instruction as an experimental case study, we select instances $(x^{(i)}, y_w^{(i)}, y_l^{(i)})$ from $\mathcal{D}_{eval}$ where $|y_w^{(i)}| < |y_l^{(i)}|$. For each instance $i$, we construct a sequence of length instructions $x_l^{(i, j)}, j=\{1, 2, ..., 8\}$,  with increasing {\em word\_num}, such that both $y_w$ and $y_l$ progressively transition from violating to satisfying these constraints. The resulting evaluation dataset is denoted as $\mathcal{D}_{eval}^{mls}$ (detailed in Appendix \ref{additional-results-rm}). Intuitively, the RM-predicted score difference between $y_w$ and $y_l$ should first increase with {\em word\_num}, then decrease, ultimately reverting to its initial value. However, as illustrated in Figure \ref{fig:multi-length-bias-multi-wordnum-rm-score-diff}(c) and \ref{fig:multi-length-bias-multi-wordnum-rm-score-diff}(d), LIFT-plus fails to achieve this reversion, settling at an inconsistent score difference. In contrast, \ourrm{} accurately follows the expected trajectory, demonstrating its capability to effectively balance length constraints and original prompts.
% \begin{figure*}[!ht]
%     % \centering
%     \vskip 0.1in
%     \begin{center}
%         \includegraphics[width=0.49\textwidth]{figs/qwen2-1.5B_Instruct_length_less_range_eval_chosen_result_pair_diff_total.pdf}
%         \includegraphics[width=0.49\textwidth]{figs/llama-3.1-8B_Instruct_length_less_range_eval_chosen_result_pair_diff_total.pdf}
%         \caption{\textbf{Left.} Score Difference Variation of Qwen2-1.5B-Instruct on $\mathcal{D}_{eval}$ at multi $word\_num$. \textbf{Right.} Score Difference Variation of Llama-3.1-8B-Instruct on $\mathcal{D}_{eval}$ at multi $word\_num$.}
%         \label{fig:multi-wordnum-rm-score-diff}
%     \end{center}
%     \vskip -0.1in
% \end{figure*}

\subsection{The Results and Analysis of DPO Models}
\label{dpo-results}
% {\bf Rc-DPO effectively reduces length while improving quality.} The length bias of DPO is typically measured by the length of responses generated by the policy; that is, for responses with the same semantic quality, shorter responses indicate weaker length bias. Table \ref{tab:dpo-origin-quality-result} demonstrates the effectiveness of \ourbtmodel{} in mitigating length bias on AlpacaEval~\citep{dubois2024alpacafarm}, as well as improving the semantic quality of the responses. As shown, \ourbtmodel{} significantly outperforms all comparison methods in terms of semantic quality. As analysis in Section \ref{3}, LIFT-plus itself harms semantic quality, leading to minimal improvement or even a slight decline in its {\em Quality Win Ratio}. Therefore, our primary comparison is with R-DPO. Specifically, on Qwen2.5-7B-Base, \ourbtmodel{}'s {\em Quality Win Ratio} improves by $4.99\%$ over R-DPO, while the response length decreases by $374.76$; on Qwen2.5-7B-Instruct, \ourbtmodel{}'s {\em Quality Win Ratio} improves by $10.47\%$, with response length reduced by $7.15$; on Llama-3.1-8B-Base, \ourbtmodel{}'s {\em Quality Win Ratio} improves by $8.97\%$, and the response length decreases by $263.71$; similarly, on Llama-3.1-8B-Instruct, \ourbtmodel{}'s {\em Quality Win Ratio} improves by $21.7\%$, with response length reduced by $11.05$. These results fully demonstrate the effectiveness of \ourbtmodel{} in alleviating DPO's length bias while improving the semantic quality of the generated responses.
{\bf Rc-DPO effectively reduces response length and improves quality.} Table \ref{tab:dpo-origin-quality-result} highlights the effectiveness of \ourdpo{} in mitigating length bias and enhancing response quality on AlpacaEval~\citep{dubois2024alpacafarm}. \ourdpo{} significantly outperforms all comparison methods in semantic quality. Specifically, LIFT-plus negatively impacts semantic quality, leading to a decline in {\em Quality Win Ratio}. Compared to R-DPO, \ourdpo{} not only achieves a notable improvement in {\em Quality Win Ratio} but also reduces {\em Response Length}, demonstrating its ability to alleviate length bias and improve the semantic quality of the generated responses.
% On Qwen2.5-7B-Base, \ourbtmodel{}'s {\em Quality Win Ratio} improves by $4.99\%$ over R-DPO, while the response length decreases by $374.76$; on Qwen2.5-7B-Instruct, \ourbtmodel{}'s {\em Quality Win Ratio} improves by $10.47\%$, with response length reduced by $7.15$; on Llama-3.1-8B-Base, \ourbtmodel{}'s {\em Quality Win Ratio} improves by $8.97\%$, and the response length decreases by $263.71$; similarly, on Llama-3.1-8B-Instruct, \ourbtmodel{}'s {\em Quality Win Ratio} improves by $21.7\%$, with response length reduced by $11.05$.  
% \begin{table*}[!ht]
%     \centering
%     \caption{Evaluation Results of DPO Model on AlpacaEval}
%     \vskip 0.1in
%     \begin{tabular}{llcc}
%     \toprule
%         Model & Variant & Quality Win Ratio (\%) & Original Response Length \\ 
%         % \midrule
%         % \multirow{4}{*}{Qwen2-1.5B-Base} & baseline & 307/112/383 \\ 
%         % & LIFT \\
%         % & R-DPO \\
%         % & our \\ 
%         % \midrule
%         % \multirow{4}{*}{Qwen2-1.5B-Instruct} & baseline \\ 
%         % & LIFT \\ 
%         % & R-DPO \\
%         % & our \\ 
%         \midrule
%         \multirow{4}{*}{Qwen2.5-7B-Base} & baseline & 33.54 & 517.30 \\ 
%         & LIFT-plus & 31.67 & 184.17 \\
%         & R-DPO & 40.40 & 583.18 \\
%         & our & \textbf{45.39} & 208.42 \\ 
%         \midrule
%         \multirow{4}{*}{Qwen2.5-7B-Instruct} & baseline & 28.43 & 261.32 \\ 
%         & LIFT-plus & 25.69 & 195.80 \\
%         & R-DPO & 34.16 & 235.39 \\
%         & our & \textbf{44.63} & 228.24 \\
%         \midrule
%         \multirow{4}{*}{Llama-3.1-8B-Base} & baseline & 46.30 & 435.98 \\ 
%         & LIFT-plus & 40.15 & 157.80 \\ 
%         & R-DPO & 49.13 & 465.72 \\
%         & our & \textbf{58.10} & 202.01 \\ 
%         \midrule
%         \multirow{4}{*}{Llama-3.1-8B-Instruct} & baseline & 42.52 & 247.74 \\ 
%         & LIFT-plus & 47.88 & 153.77 \\
%         & R-DPO & 42.64 & 215.82 \\
%         & our & \textbf{64.34} & 204.77 \\ 
%         \bottomrule
%     \end{tabular}
%     \vskip -0.1in
%     \label{tab:dpo-origin-quality-result}
% \end{table*}

{\bf Rc-DPO  balances adherence to length instruction with high semantic quality.} The results\footnote{The experimental results for AlpacaEval-LI-plus-more can be found in Appendix \ref{or-more-results-analysis}.} on AlpacaEval-LI-plus-less are shown in Table \ref{tab:dpo-length-result-or-less}. Notably, LIFT-plus focus exclusively on the length constraints while neglecting the specified {\em word\_num} and the original prompts $x$. This results in nearly $100\%$ {\em Length Acc}, but {\em Length Win Ratio} close to $0\%$.
% In contrast, \ourbtmodel{} adheres to the length instruction while considering the {\em word\_num} and original prompt.
In contrast, \ourbtmodel{} adheres to the length instructions while considering the original prompts. It strives to maximize the semantic quality of the response within the length constraint. Therefore, compared to the {\em Quality Win Ratio} in Table~\ref{tab:dpo-origin-quality-result}, \ourdpo{} maintains semantic quality even under length constraints. Detailed analysis is in Appendix~\ref{or-less-results-analysis}.
% \begin{table*}[!ht]
%     \centering
%     \caption{Evaluation Results of DPO Model on AlpacaEval-LI-plus (or less)}
%     \vskip 0.1in
%     \begin{tabular}{llccc}
%     \toprule
%         Model & Variant & Length Acc (\%) & Length & Length Win Ratio (\%) \\ 
%         % \midrule
%         % \multirow{4}{*}{Qwen2-1.5B-Base} & baseline \\ 
%         % & LIFT-plus \\
%         % & R-DPO \\
%         % & our \\ 
%         % \midrule
%         % \multirow{4}{*}{Qwen2-1.5B-Instruct} & baseline \\ 
%         % & LIFT-plus \\ 
%         % & R-DPO \\
%         % & our \\ 
%         \midrule
%         \multirow{4}{*}{Qwen2.5-7B-Base} & baseline & 5.74 & 544.47 & 34.04 \\ 
%         & LIFT-plus & \textbf{86.78} & 106.87 & 1.37 \\ 
%         & R-DPO & 8.35 & 637.44 & 34.54 \\
%         & our & 82.04 & 177.40 & \textbf{44.14} \\ 
%         \midrule
%         \multirow{4}{*}{Qwen2.5-7B-Instruct} & baseline & 89.65 & 136.75 & 32.17 \\ 
%         & LIFT-plus & \textbf{100} & 23.56 & 2.99 \\
%         & R-DPO & 90.90 & 131.17 & 31.92 \\
%         & our & \textbf{100} & 108.24 & \textbf{50.75} \\ 
%         \midrule
%         \multirow{4}{*}{Llama-3.1-8B-Base} & baseline & 13.59 & 424.47 & 35.41 \\ 
%         & LIFT-plus & \textbf{98.75} & 6.62 & 0.25 \\
%         & R-DPO & 27.68 & 518.74 & 47.88 \\
%         & our & 94.51 & 118.50 & \textbf{64.96} \\ 
%         \midrule
%         \multirow{4}{*}{Llama-3.1-8B-Instruct} & baseline & 87.66 & 150.43 & 43.89 \\ 
%         & LIFT-plus & \textbf{100} & 35.01 & 1.00 \\ 
%         & R-DPO & 87.03 & 151.39 & 43.77 \\
%         & our & \textbf{100} & 89.75 & \textbf{64.71} \\ 
%         \bottomrule
%     \end{tabular}
%     \vskip -0.1in
%     \label{tab:dpo-length-result-or-less}
% \end{table*}

\subsection{Ablations and Extra Results}
\label{ablations}
Due to the high cost of evaluating the DPO model, we focus on the reward model for ablation studies. For convenience, we denote $\{(x, x_l^1, y_w)\}$ as $\mathcal{D}_{Rc}^c$ and $\{(x_l^2, x, y_l)\}$ as $\mathcal{D}_{Rc}^r$. A more detailed analysis can be found in Appendix~\ref{appendix-ablation}.
% \begin{table*}[!ht]
%     \centering
%     \caption{Evaluation Results of Reward Model on Quality and Length Eval Datasets}
%     \vskip 0.1in
%     \begin{tabular}{llcc}
%     \toprule
%         Model & Variant & Quality Eval Acc & Length Eval Acc \\ 
%         \midrule
%         \multirow{2}{*}{Qwen2-1.5B-Base} & our {\em w/o} $\mathcal{D}_c$ & 58.78 & 45.83 \\
%         & our {\em w/o} $\mathcal{D}_r$ & 55.59 & 36.22 \\
%         \midrule
%         \multirow{2}{*}{Qwen2-1.5B-Instruct} & our {\em w/o} $\mathcal{D}_c$ & 59.31 & 45.51 \\
%         & our {\em w/o} $\mathcal{D}_r$ & 57.71 & 44.87 \\
%         \midrule
%         \multirow{2}{*}{Llama-3.1-8B-Base} & our {\em w/o} $\mathcal{D}_c$ & 50.27 & 44.87 \\
%         & our {\em w/o} $\mathcal{D}_r$ & 50.53 & 32.69 \\
%         \midrule
%         \multirow{2}{*}{Llama-3.1-8B-Instruct} & our {\em w/o} $\mathcal{D}_c$ & 58.51 & 37.18 \\
%         & our {\em w/o} $\mathcal{D}_r$ & 52.13 & 33.65 \\
%         \bottomrule
%     \end{tabular}
%     \vskip -0.1in
%     \label{tab:rm-result-ablation-Rc-BT}
% \end{table*}

{\bf $\mathcal{D}_{Rc}^c$ and $\mathcal{D}_{Rc}^r$ are complementary.} We conduct ablation experiments by training with $\mathcal{D}_{rm} \cup \mathcal{D}_{Rc}^c$ ({\em w/o} $\mathcal{D}_{Rc}^r$) and $\mathcal{D}_{rm} \cup \mathcal{D}_{Rc}^r$ ({\em w/o} $\mathcal{D}_{Rc}^c$), with the results shown in Table \ref{tab:rm-result-ablation-Rc-BT}. As observed, when only $\mathcal{D}_{Rc}^c$ or $\mathcal{D}_{Rc}^r$ is used, the {\em Quality Eval Acc} of \ourrm{} drops significantly, nearing the performance of Baseline in Table~\ref{tab:rm-result}, while its {\em Length Eval Acc} hovers around $50\%$, failing to learn length instructions. Subsequently, we prepend a length constraint to each $x$ in $\mathcal{D}_{eval}^q$, forming $x_l$, where $y_w$ satisfies the length constraint, resulting in $\mathcal{D}_{eval}^{q,l}$. The evaluation results on both $\mathcal{D}_{eval}^q$ and $\mathcal{D}_{eval}^{q,l}$, shown in Figure~\ref{fig:llama-3.1-8b-instruct-origin-length-score-ranges-ablation}. Specifically, Figure~\ref{fig:llama-3.1-8b-instruct-origin-length-score-ranges-ablation}(a) indicates that for the reward models trained with $\mathcal{D}_{rm} \cup \mathcal{D}_{Rc}^{c}$, the scores of $(x_l, y_w)$ are consistently lower than those of the original $(x, y_w)$, despite $y_w$'s length satisfies {$x_l$}. A similar phenomenon is observed in $\mathcal{D}_{rm} \cup \mathcal{D}_{Rc}^{r}$ (Figure~\ref{fig:llama-3.1-8b-instruct-origin-length-score-ranges-ablation}(b)). Therefore, the combination of $\mathcal{D}_{Rc}^c$ and $\mathcal{D}_{Rc}^r$ is essential for preventing the reward model from developing new biases.
% \begin{figure}[!ht]
%     \centering
%     \vskip 0.1in
%     \includegraphics{figs/llama-3.1-8b-instruct-origin-length-score-ranges-ablation.pdf}
%     \caption{Relationship between response length and reward score evaluated on $\mathcal{D}_{eval}^q$ for {\em w/o} $\mathcal{D}_r$ models trained with Llama-3.1-8B-Instruct.}
%     \label{fig:llama-3.1-8b-instruct-origin-length-score-ranges-ablation}
%     \vskip -0.1in
% \end{figure}
% This indicates that after training with $\mathcal{D} + \mathcal{D}_c$, the RM develops a {\em length instruction bias}, where it simply reduces the score of $x^{(i)}$ from the original $x^{(i)}, y_w^{(i)}$ pair upon the addition of a length instruction, without actually considering the content of the instruction itself. A similar phenomenon is observed in the RM trained with $\mathcal{D} + \mathcal{D}_r$ (details provided in Appendix \ref{xxx}). Therefore, the combined use of $\mathcal{D}_c$ and $\mathcal{D}_r$ is necessary to prevent the RM from falling into such bias hacking.
% \begin{figure*}[!ht]
%     % \centering
%     \vskip 0.1in
%     \begin{center}
%         \includegraphics[width=0.49\textwidth]{figs/qwen2-1.5b-chat-range-Dr-rm.pdf}
%         \includegraphics[width=0.49\textwidth]{figs/llama-3.1-8b-chat-range-Dr-rm.pdf}
%         \caption{{\bf Left.} Variation of RM's Length Eval Acc and Quality Eval Acc of Qwen2.5-1.5B-Instruct with Increasing $\mathcal{D}_r$. {\bf Right.} Variation of RM's Length Eval Acc and Quality Eval Acc of Llama-3.1-8B-Instruct with Increasing $\mathcal{D}_r$.}
%         \label{fig:Dr-range-rm-score}
%     \end{center}
%     \vskip -0.1in
% \end{figure*}
\begin{table}[!ht]
\small
    \centering
    \vskip -0.1in
    \caption{Evaluation results of Rc-RM {\em w/o} $\mathcal{D}_{Rc}^c$ and {\em w/o} $\mathcal{D}_{Rc}^r$ on quality ($\mathcal{D}_{eval}^q$) and length ($\mathcal{D}_{eval}^l$) evaluation datasets}
    % \vskip 0.1in
    \scalebox{0.87}{
    \begin{tabular}{ccccc}
        \toprule
        \multirow{2}{*}{Metrics} & \multicolumn{2}{c}{Qwen2-1.5B-Base} & \multicolumn{2}{c}{Qwen2-1.5B-Instruct} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} & {\em w/o} $\mathcal{D}_{Rc}^c$ & {\em w/o} $\mathcal{D}_{Rc}^r$ & {\em w/o} $\mathcal{D}_{Rc}^c$ & {\em w/o} $\mathcal{D}_{Rc}^r$ \\
        \cmidrule(lr){1-1} Quality Eval Acc ($\%$) & 58.78 & 55.59 & 59.31 & 57.71 \\
        Length Eval Acc ($\%$) & 45.83 & 36.22 & 45.51 & 44.87 \\ 
        \midrule
        \multirow{2}{*}{Metrics} & \multicolumn{2}{c}{Llama-3.1-8B-Base} & \multicolumn{2}{c}{Llama-3.1-8B-Instruct} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} & {\em w/o} $\mathcal{D}_{Rc}^c$ & {\em w/o} $\mathcal{D}_{Rc}^r$ & {\em w/o} $\mathcal{D}_{Rc}^c$ & {\em w/o} $\mathcal{D}_{Rc}^r$ \\
        \cmidrule(lr){1-1} Quality Eval Acc ($\%$) & 50.27 & 50.53 & 58.51 & 52.13 \\
        Length Eval Acc ($\%$) & 44.87 & 32.69 & 37.18 & 33.65 \\ 
        \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
    \label{tab:rm-result-ablation-Rc-BT}
\end{table}
\begin{figure}[!ht]
    \centering
    \vspace{-0.3cm}
    \subfigure[{\em w/o} $\mathcal{D}_{Rc}^r$]{\includegraphics[width=0.237\textwidth]{figs/llama-3.1-8b-instruct-origin-length-score-ranges-ablation_wo_Dr_v3.pdf}}
    \subfigure[{\em w/o} $\mathcal{D}_{Rc}^c$]{\includegraphics[width=0.237\textwidth]{figs/llama-3.1-8b-instruct-origin-length-score-ranges-ablation_wo_Dc_v3.pdf}}
    \vspace{-2ex}
    \caption{The relationship between response length and RM score evaluated on $\mathcal{D}_{eval}^q$ and $\mathcal{D}_{eval}^{q,l}$ for Rc-RM trained with Llama-3.1-8B-Instruct.}
    \label{fig:llama-3.1-8b-instruct-origin-length-score-ranges-ablation}
    \vspace{-2ex}
\end{figure}

{\bf Effectiveness across different datasets.} We conduct RM training on the HH-RLHF~\citep{bai2022training} dataset and the results are presented in Table \ref{tab:rm-result-hh} of Appendix \ref{appendix-ablation}. As shown, \ourrm{} still outperforms comparative methods on $\mathcal{D}_{eval}^q$, and achieves comparable results to LIFT-plus on $\mathcal{D}_{eval}^l$. Notably, for Llama-3.1-8B-Instruct, \ourrm{} surpasses Baseline by $6.09\%$ and ODIN by $5.13\%$ in {\em Quality Eval Acc}. The consistent performance across various datasets strongly supports the robustness and effectiveness of \ourbtmodel{}.

\section{Conclusion and Future Work}
% In this work, we propose a method called \ourbtmodel{} to mitigate the model's length bias while retaining its sensitivity to length. This is achieved by explicitly distinguishing human semantic intent from length instructions, and it is easy to apply to RM and DPO. We first analyze the length bias issue present in both the models trained on the current dataset and the evaluation set. Then we construct the new evaluation sets for more rigorous evaluation. Next, we analyze the drawbacks of directly using length instructions in the form of $(x_l, y_w, y_l)$. We then introduce \ourbtmodel{}, which addresses this issue by using the response-conditioned data paradigm $(x_w, x_l, y)$, and we provide a detailed mathematical derivation to extend it to RM and DPO. Finally, through extensive experiments, we demonstrate the effectiveness of \ourbtmodel{} in reducing length bias and improving the model's ability to follow length instructions.
In this work, we propose \ourbtmodel{}, a method that explicitly distinguishes human semantic intent from length instructions{, conditioned on responses,} to mitigate length bias while preserving sensitivity to length. We provide a detailed mathematical extension to RM and DPO, analyze length bias in models and in the evaluation set, and construct the new evaluation sets for more rigorous evaluation. Extensive experiments validate the effectiveness of \ourbtmodel{} in reducing length bias and following length instructions. 

As for future research directions, we propose several promising avenues: First, we plan to extend \ourbtmodel{} to process multiple concurrent instructions to evaluate its generalization capabilities. Second, we aim to conduct comprehensive analyses on length-related biases and their potential impacts on model performance. Furthermore, investigating the scalability of our \ourbtmodel{} on larger language models~(beyond 8B) remains an important direction that could provide valuable insights into its practical applications and limitations.
% In future work, we will explore \ourbtmodel{} to handle multiple instructions, and further investigate length bias. 

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliography{icml2025_conference}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Related Work}
\label{full-related work}
{\bf Reinforcement Learning from Human Feedback.} Reinforcement Learning from Human Feedback (RLHF)~\citep{ziegler2019fine} has established itself as a dominant paradigm for aligning LLMs with human preferences~\citep{achiam2023gpt, team2023gemini, caruccio2024claude, yang2024qwen2, dubey2024llama}. At its core, RLHF trains models by optimizing reward signals derived from human feedback, typically collected through preference comparisons or explicit ratings~\citep{bai2022constitutional, lee2023rlaif}. This approach has demonstrated significant efficacy in refining LLM behavior to align with human expectations across diverse complex tasks~\citep{kreutzer-etal-2018-reliability, liu2020learning, ziegler2019fine, ouyang2022training}.


However, RLHF necessitates training a reward model (RM) to approximate human preferences, followed by LLM alignment through reinforcement learning (RL) algorithms, notably Proximal Policy Optimization (PPO)~\citep{schulman2017proximal}. This process presents significant challenges and stability issues~\citep{NEURIPS2021_63c4b1ba, 9501950}. Direct preference alignment methods, in contrast, provide a more robust training approach~\citep{ivison2024unpacking, hong2024orpo, xu2024contrastivepreferenceoptimizationpushing}. Among these methods, direct preference optimization (DPO)~\citep{rafailov2024direct} has emerged as a leading technique, inspiring various derivative algorithms~\citep{hong2024orpo, chen2024noise, ethayarajh2024kto}. By reformulating the RLHF reward function, DPO eliminates both the need for an online reward model and the instabilities inherent in RL algorithms, enabling stable offline preference learning.

Our method enhances RLHF by improving reward model robustness and seamlessly integrates into DPO, enabling more effective LLM training.

{\bf Reward Hacking.} Despite its promising performance, RLHF is vulnerable to reward hacking --- the over-optimization of the reward model~\citep{skalse2022defining, pan2022effects, casper2023open, lambert2023alignment}. This phenomenon occurs when the policy exploits inherent biases in the reward model to maximize rewards without achieving intended objectives. Similar exploitation patterns have been observed in DPO-trained LLMs~\citep{lambert2024rewardbench, xu2024is}. The root causes are multifaceted: task complexity and subjectivity~\citep{casper2023open}, evaluation criteria limitations~\citep{parrish-etal-2022-bbq}, and evaluator qualification constraints~\citep{skalse2022defining}. Consequently, human preference data exhibit biases and inconsistencies~\citep{dubois2024alpacafarm}, and reward models struggle with preference approximation and out-of-distribution (OOD) generalization.

One of the most common forms of reward hacking is length bias \citep{shen2023loose, singhal2023long, park2024disentangling, chenodin}, where the reward models tend to favor longer responses, assigning higher reward scores to longer responses even if their semantic contents are not of high quality. This tendency is largely driven by human evaluators' preference for longer answers, which is easily exploited by the LLMs. As a result, LLMs often generate unnecessarily lengthy replies to appear more detailed or better formatted, even when the actual quality remains unchanged. To address this issue, several methods have been proposed. ODIN \citep{chenodin} adds multiple length regularization terms to the RM's training objective, which helps to distinguish response quality from length and effectively controls the LLM's dependency on response length. \citet{shen2023loose} involve using dual-reward model and different sets of learning rate hyperparameters, allowing different part of the reward model to learn distinct paradigms, thereby removing the length bias from final reward scores. \citet{park2024disentangling} modify the DPO training objective by adding a length penalty term to prevent length bias exploitation. 

However, the above methods assume that the model's sensitivity to length information is detrimental and, therefore, seek to eliminate the model's perception of length information. In contrast, our approach enables the model to explicitly distinguish between human semantic intent and length instruction, preserving its ability to perceive response length rather than completely eliminating the perception of length information. 

{\bf Length Instruction Following.} Current state-of-the-art (SOTA) LLMs, both open-source and closed-source, demonstrate a certain degree of implicit ability to follow length instructions \citep{yuan2024following}. For example, adding terms like ``concise" or ``verbose" can influence the length of the model's outputs. However, when it comes to explicit length instructions, such as ``Answer the following instruction using $150$ words or less.", these models often fail to adhere effectively. To address this, LIFT \citep{yuan2024following} enhances the model's length instruction following capability by constructing an explicit length instruction preference dataset. However, the dataset used by LIFT is built at the expense of semantic quality, resulting in a degradation of the model's output quality. In contrast, our model not only effectively learns length instructions but also maintains, even in many cases improves, the semantic quality of the model's outputs. 

\section{Derivation of the DPO Objective Under the Response-Conditioned Bradley-Terry Model}
\label{appendix_b}
% \begin{equation}
%     \max_{\pi_{\theta}} \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi_{\theta}(y \mid x)}\bigl[r_{\phi}(x, y)\bigr] - \beta\mathbb{D}_{\textrm{KL}}\bigl[\pi_{\theta}(y\mid x)\mid \mid \pi_{\text{ref}}(y\mid x)\bigr],
%     \label{eqn:RL}
% \end{equation}
% \begin{equation}
%     \pi_r(y\mid x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right), Z(x) =\sum_{y}\pi_{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)
%     \label{eqn:op_policy}
% \end{equation}
% \begin{equation}
%     r(x,y) =\beta \log \frac{\pi_r(y\mid x)}{\pi_{\text{ref}}(y\mid x)} + \beta \log Z(x).
%     \label{eqn:main_eq}
% \end{equation}

% Response-conditioned DPO Deriving:
% \begin{equation}
%     p^*(p_i\succ p_i^{l_1}|c_i) = \frac{\exp{(r^*(p_i, c_i))}}{\exp(r^*(p_i, c_i))+\exp(r^*(p_i^{l_1}, c_i))},
%     \label{eqn:prompt_dis_chosen_repeat}
% \end{equation}
% \begin{align}
%     p^*(p_i\succ p_i^{l_1}|c_i)&=\frac{\exp\left(\beta \log \frac{\pi^*(c_i|p_i)}{\pi_{\text{ref}}(c_i|p_i)} + \beta \log Z(p_i)\right)}{\exp\left(\beta \log \frac{\pi^*(c_i|p_i)}{\pi_{\text{ref}}(c_i|p_i)} + \beta \log Z(p_i)\right) + \exp\left(\beta \log \frac{\pi^*(c_i|p_i^{l_1})}{\pi_{\text{ref}}(c_i|p_i^{l_1})} + \beta \log Z(p_i^{l_1})\right)} \nonumber\\
%     &=\frac{1}{1+\exp\left(\beta \log \frac{\pi^*(c_i|p_i^{l_1})}{\pi_{\text{ref}}(c_i|p_i^{l_1})}-\beta \log \frac{\pi^*(c_i|p_i)}{\pi_{\text{ref}}(c_i|p_i)} + (\beta \log Z(p_i^{l_1}) - \beta \log Z(p_i))\right)} \nonumber\\
%     &= \sigma\left(\beta \log \frac{\pi^*(c_i|p_i)}{\pi_{\text{ref}}(c_i|p_i)} - \beta \log \frac{\pi^*(c_i|p_i^{l_1})}{\pi_{\text{ref}}(c_i|p_i^{l_1})}+(\beta \log Z(p_i^{l_1}) - \beta \log Z(p_i))\right) \nonumber \\
%     &= \sigma\left(\beta \log \frac{\pi^*(c_i|p_i)}{\pi_{\text{ref}}(c_i|p_i)} - \beta \log \frac{\pi^*(c_i|p_i^{l_1})}{\pi_{\text{ref}}(c_i|p_i^{l_1})}+\beta \log \frac{Z(p_i^{l_1})}{Z(p_i)}\right) \nonumber \\
%     &\overset{?}{=} \sigma\left(\beta \log \frac{\pi^*(c_i|p_i)}{\pi_{\text{ref}}(c_i|p_i)} - \beta \log \frac{\pi^*(c_i|p_i^{l_1})}{\pi_{\text{ref}}(c_i|p_i^{l_1})}\right).
%     \label{response-conditioned-dpo-derive}
% \end{align}
{\bf Derivation of RL Fine-Tuning Objective.} During the RL phase of traditional RLHF, the trained reward model $r_\phi$ serves as the feedback mechanism for policy optimization. The standard RL optimization objective is formulated as:

\begin{equation}
    \max_{\pi_{\theta}} \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi_{\theta}(y \mid x)}\bigl[r_{\phi}(x, y)\bigr] - \beta\mathbb{D}_{\textrm{KL}}\bigl[\pi_{\theta}(y\mid x)\mid \mid \pi_{\text{ref}}(y\mid x)\bigr],
    \label{eqn:prompt-conditioned-RL}
\end{equation}

under the reference policy $\pi_\text{ref}$ and the parametrized policy $\pi_\theta$. In the response-conditioned scenario, where responses are predetermined and guide prompt selection, the KL-divergence constraint in Eqn. \ref{eqn:prompt-conditioned-RL} requires modification. Accordingly, we reformulate the response-conditioned RL optimization objective as:

\begin{equation}
    \max_{\pi_{\theta}} \mathbb{E}_{y\sim \mathcal{D}_{Rc}, x\sim \pi_{\theta}(x \mid y)}\bigl[r_{\phi}(x, y)\bigr] - \beta\mathbb{D}_{\textrm{KL}}\bigl[\pi_{\theta}(x\mid y)\mid \mid \pi_{\text{ref}}(x\mid y)\bigr].
    \label{eqn:response-conditioned-RL}
\end{equation}

For simplicity, we unify the notation of $\mathcal{D}_{Rc} = \{(x, x_l^1, y_w)\} \cup \{(x_l^2, x, y_l)\}$ as $\{(x_w, x_l, y)\}$. Next, similar to the derivation in DPO \citep{park2024disentangling}, we can derive the partition function $Z(y)$ from Eqn. \ref{eqn:response-conditioned-RL}:

\begin{equation}
    Z(y) =\sum_{x}\pi_{\text{ref}}(x\mid y)\exp\left(\frac{1}{\beta}r_\phi(x, y)\right).
    \label{eqn:partition-function}
\end{equation}

Since the partition function is a function only of the response $y$ and the reference policy $\pi_{\text{ref}}$, and does not depend on the prompt $x$ or the parametrized policy $\pi_\theta$ to be optimized, we can reorganize Eqn. \ref{eqn:response-conditioned-RL} to obtain the following objective:

\begin{align}
    \min_{\pi_\theta}\mathbb{E}_{y\sim \mathcal{D}}\left[\mathbb{E}_{x\sim \pi_\theta(x|y)}\left[\log\frac{\pi_\theta(x|y)}{\pi^*(x|y)}\right] - \log Z(y)\right]=\min_{\pi_\theta}\mathbb{E}_{y\sim\mathcal{D}}\left[\mathbb{D}_{\text{KL}}(\pi_\theta(x|y)\mid\mid\pi^*(x|y)) - \log Z(y)\right],
    \label{eqn:proxy_policy}
\end{align}

\begin{equation}
    \pi^*(x\mid y) = \frac{1}{Z(y)}\pi_{\text{ref}}(x\mid y)\exp\left(\frac{1}{\beta}r_\phi(x, y)\right),
\end{equation}

where $\pi^*(x\mid y)$ is a valid probability distribution which $\pi^*(x\mid y) > 0$ and $\sum_x\pi^*(x\mid y) = 1$. By using Gibbs' inequality, the KL divergence is minimized to $0$ if and only if the two distributions are identical. Therefore, we obtain the optimal solution:

\begin{equation}
    \pi_\theta(x\mid y) = \pi^*(x\mid y) = \frac{1}{Z(y)}\pi_{\text{ref}}(x\mid y)\exp\left(\frac{1}{\beta}r_\phi(x, y)\right).
    \label{eqn:op_policy}
\end{equation}

Finally, by taking the logarithm on both sides and performing some basic algebraic operations, we can express the reward model $r_\phi(x, y)$ through its corresponding optimal policy $\pi^*(x\mid y)$:

\begin{equation}
    r_\phi(x,y) =\beta \log \frac{\pi^*(x\mid y)}{\pi_{\text{ref}}(x\mid y)} + \beta \log Z(y).
    \label{eqn:main_eq}
\end{equation}

{\bf Derivation of Response-conditioned DPO Objective.} Following Section \ref{4-2}, the Response-conditioned Bradley-Terry model is formulated as:

\begin{equation}
    p^*(x_w\succ x_l|y) = \frac{\exp{(r^*(x_w, y))}}{\exp(r^*(x_w, y))+\exp(r^*(x_l, y))}.
    \label{eqn:prompt_dis_chosen_repeat}
\end{equation}

In the above derivation, we demonstrated that the optimal reward $r^*(x, y)$, parameterized as $r_\phi(x, y)$, can be expressed in term of its corresponding optimal policy $\pi^*(x\mid y)$. By substituting Eqn.~\ref{eqn:main_eq} into Eqn.~\ref{eqn:prompt_dis_chosen_repeat} and preforming simple simplifications, we obtain:

\begin{align}
    p^*(x_w\succ x_l|y)&=\frac{\exp\left(\beta \log \frac{\pi^*(x_w|y)}{\pi_{\text{ref}}(x_w|y)} + \beta \log Z(y)\right)}{\exp\left(\beta \log \frac{\pi^*(x_w|y)}{\pi_{\text{ref}}(x_w|y)} + \beta \log Z(y)\right) + \exp\left(\beta \log \frac{\pi^*(x_l|y)}{\pi_{\text{ref}}(x_l|y)} + \beta \log Z(y)\right)} \nonumber\\
    &=\frac{1}{1+\exp\left(\beta \log \frac{\pi^*(x_l|y)}{\pi_{\text{ref}}(x_l|y)}-\beta \log \frac{\pi^*(x_w|y)}{\pi_{\text{ref}}(x_w|y)} + (\beta \log Z(y) - \beta \log Z(y))\right)} \nonumber\\
    &=\frac{1}{1+\exp\left(\beta \log \frac{\pi^*(x_l|y)}{\pi_{\text{ref}}(x_l|y)}-\beta \log \frac{\pi^*(x_w|y)}{\pi_{\text{ref}}(x_w|y)}\right)} \nonumber\\
    &= \sigma\left(\beta \log \frac{\pi^*(x_w|y)}{\pi_{\text{ref}}(x_w|y)} - \beta \log \frac{\pi^*(x_l|y)}{\pi_{\text{ref}}(x_l|y)}\right).
    \label{response-conditioned-dpo-derive}
\end{align}

Since we cannot directly access $\pi(x|y)$, we reparameterize it using Bayes' theorem for conditional probability decomposition, transforming Eqn.~\ref{response-conditioned-dpo-derive} into a joint probability formulation as follows:

\begin{align}
    p^*(x_w\succ x_l|y)&= \sigma\left(\beta \log \frac{\pi^*(x_w|y)}{\pi_{\text{ref}}(x_w|y)} - \beta \log \frac{\pi^*(x_l|y)}{\pi_{\text{ref}}(x_l|y)}\right)\nonumber\\
    &= \sigma\left(\beta \log \frac{\frac{\pi^*(x_w, y)}{\pi^*(y)}}{\frac{\pi_{\text{ref}}(x_w, y)}{\pi_{\text{ref}}(y)}} - \beta \log \frac{\frac{\pi^*(x_l, y)}{\pi^*(y)}}{\frac{\pi_{\text{ref}}(x_l, y)}{\pi_{\text{ref}}(y)}}\right)\nonumber\\
    &= \sigma\left(\beta \log \frac{\pi^*(x_w, y)}{\pi_{\text{ref}}(x_w, y)} - \beta \log \frac{\pi^*(x_l, y)}{\pi_{\text{ref}}(x_l, y)}\right).
    \label{eqn:response-conditioned-dpo-derive-joint-prob}
\end{align}

Finally, we derive the loss function for \ourdpo{} for a parameterized policy $\pi_\theta$ using the response-conditioned preference dataset $\mathcal{D}_{Rc} = \{(x_w, x_l, y)\}$ by applying maximum likelihood estimation:

\begin{align}
    \mathcal{L}_{DPO}^{Rc}(\pi_\theta; \pi_\text{ref}) = -\mathbb{E}_{(x_w, x_l, y) \sim \mathcal{D}_{Rc}} \big[\log\sigma\left(\beta \log \frac{\pi_\theta(x_w, y)}{\pi_{\text{ref}}(x_w, y)} - \beta \log \frac{\pi_\theta(x_l, y)}{\pi_{\text{ref}}(x_l, y)}\right)\big].
    \label{eqn:rc-dpo-loss-origin}
\end{align}

\section{The Implementation Details of Preliminary Explorations}
\label{appendix_a}
\subsection{Training Dataset and Models}
\label{preliminary_training_data_and_models}
{\bf Training Dataset.} Similar to the data processing approach used by~\citet{yuan2024following}, we extract the first turn of English conversations from the OpenAssistant dataset~\citep{kopf2024openassistant} and use it as our complete dataset. Based on their human annotated ranking, we label rank $0$ as ``chosen" ($y_w^{(i)}$) and rank $1$ as ``rejected" ($y_l^{(i)}$), resulting in the complete dataset $\mathcal{D}$, that each example is a triple $(x^{(i)}, y_w^{(i)}, y_l^{(i)})$.

For training of different models and subsequent experimental analysis, we first divide $\mathcal{D}$ into a training dataset $\mathcal{D}_{train}$, which contains $90\%$ of the data for model training, and an evaluation dataset $\mathcal{D}_{eval}$, comprising the remaining $10\%$ for model evaluation. Next, we further divide $\mathcal{D}_{train}$ into two subsets: a supervised fine-tuning (SFT) training dataset $\mathcal{D}_{sft}$, containing $30\%$ of $\mathcal{D}_{train}$, and a reward model (RM) training dataset $\mathcal{D}_{rm}$, containing the remaining $70\%$ of $\mathcal{D}_{train}$. For all reward models, we incorporate $\mathcal{D}_{rm}$ into the training process to ensure the fundamental semantic comprehension ability of the trained reward models. For example, in Section \ref{3-4}, each reward model is trained using the variant dataset ($\text{LIFT-plus}_2^{reverse}$ / $\text{LIFT-plus}_2^{noreverse}$ / $\text{LIFT-plus}_2^{empty}$) combined with $\mathcal{D}_{rm}$.

For the {\em Base} models, we first apply SFT using $\mathcal{D}_{sft}$, followed by reward model training on $\mathcal{D}_{rm}$, For the {\em Instruct} models, we directly train the reward model using $\mathcal{D}_{rm}$ without prior supervised fine-tuning.

{\bf Training Models.} In the preliminary explorations, we employ three groups of models --- Qwen2-1.5B-Base with Qwen2-1.5B-Instruct, Qwen2.5-7B-Base with Qwen2.5-7B-Instruct, and Llama-3.1-8B-Base with Llama-3.1-8B-Instruct --- to verify the broad applicability of our analytical results. However, in Section \ref{3-4}, we exclude the Qwen2.5-7B-Base and Qwen2.5-7B-Instruct models from our analysis for simplicity. 

\subsection{The Construction Process of $\mathcal{D}_{eval}^q$}
\label{construct-D_eval^q}
To fairly and accurately evaluate the semantic comprehension capabilities of the reward model, we establish a refined quality evaluation dataset $\mathcal{D}_{eval}^q$ through an automated transformation of $\mathcal{D}_{eval}$, leveraging advanced language models \citep{dubois2024alpacafarm, zheng2023judging, chiang2023vicuna}. For each triplet $(x^{(i)}, y_w^{(i)}, y_l^{(i)})$ in $\mathcal{D}_{eval}$, we employ GPT-4o~\citep{hurst2024gpt} with the {\em Response Rewriting Template} (Figure~\ref{fig:gpt-rewrite-prompt-template}) to generate two alternative responses, $y_w^{(i), 1}$ and $y_w^{(i), 2}$,  based on the original prompt $x^{(i)}$ and response $y_w^{(i)}$. The semantic equivalence between the generated response $y_w^{(i), 1}$ (or $y_w^{(i), 2}$) and the original response $y_w^{(i)}$ is verified using the {\em  Quality Consistency Verification Template} in Figure~\ref{fig:gpt-rewrite-prompt-template}.

Subsequently, we process $y_l^{(i)}$ through GPT-4o to generate two alternative responses, $y_l^{(i), 1}$ and $y_l^{(i), 2}$ , based on the original prompt $x^{(i)}$. These generated responses must satisfy two criteria: (1) maintain semantic equivalence with $y_l^{(i)}$, verified using the {\em Quality Consistency Verification Template}, and (2) exhibit specific length relationships with their counterparts, namely $|y_l^{(i), 1}| > |y_w^{(i), 1}|$ and $|y_l^{(i), 2}| < |y_w^{(i), 2}|$. Given GPT-4o's limitations in direct length control, we implement a separate generation process utilizing the {\em Response Expansion Template} and {\em Response Compression Template} (Figure~\ref{fig:gpt-rewrite-prompt-template}) to obtain $y_l^{(i), 1}$ and $y_l^{(i), 2}$, respectively. The resulting quality evaluation dataset is formulated as $\mathcal{D}_{eval}^q = \{(x^{(i)}, y_w^{(i), 1}, y_l^{(i), 1})\} \cup \{(x^{(i)}, y_w^{(i), 2}, y_l^{(i), 2})\}$. The complete prompting templates for response generation and quality consistency verification are presented in Figure \ref{fig:gpt-rewrite-prompt-template}.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figs/gpt-write.pdf}
    \caption{{\bf (a).} Response rewriting template for the chosen response $y_w^{(i)}$; {\bf (b).} Response expansion template for the rejected response $y_l^{(i)}$; {\bf (c).} Response compression template for the rejected response $y_l^{(i)}$; {\bf (d).} Quality consistency verification template for assessing the quality consistency between the rewritten and the original responses.}
    \label{fig:gpt-rewrite-prompt-template}
\end{figure*}

\subsection{The Construction Process of $\mathcal{D}_{eval}^l$}
\label{construct-D_eval^l}
To accurately and efficiently evaluate the reward model's adherence to length instructions, we establish the length evaluation dataset $\mathcal{D}_{eval}^l$. Each triplet $(x_l^{(i)}, y_w^{(i), l}, y_l^{(i), l})$ in $\mathcal{D}_{eval}^l$ consists of a length instruction $x_l^{(i)} = \text{\em length constraint} + x^{(i)}$ and two semantically equivalent responses that differ only in length: $y_w^{(i), l}$ adheres to the length constraint specified in $x_l^{(i)}$, while $y_l^{(i), l}$ violates it\footnote{The detailed definition of the {\em length constraint} can be found in Appendix \ref{LIFT-decomposition}.}. To optimize computational efficiency, we derive $\mathcal{D}_{eval}^l$ from the previously constructed $\mathcal{D}_{eval}^q$. Specifically, for the responses in $\mathcal{D}_{eval}^q$, we already ensure that $y_w^{(i), 1}$ and $y_w^{(i), 2}$ maintain semantic similarity during the construction of $\mathcal{D}_{eval}^q$. Additionally, the stochastic nature of GPT-4o's response generation inherently produces responses of varying lengths, with $|y_w^{(i), 1}|$ and $|y_w^{(i), 2}|$ typically differing. Leveraging this property, we select these responses ($y_w^{(i), 1}$ and $y_w^{(i), 2}$) as candidates for $x_l^{(i)}$. Specifically, we set the {\em word\_num} parameter in $x_l^{(i)}$ within the range $[|y_w^{(i), 1}|, |y_w^{(i), 2}|]$ (assuming $|y_w^{(i), 1}| < |y_w^{(i), 2}|$, $x_l^{(i)} = \text{\em less length constraint} + x^{(i)}$). Consequently, we assign   $y_w^{(i), l} = y_w^{(i), 1}$ as the chosen response and $y_l^{(i), l} = y_w^{(i), 2}$ as the rejected response. Finally, the constructed length evaluation dataset is $\mathcal{D}_{eval}^l = \{(x_l^{(i)}, y_w^{(i), l}, y_l^{(i), l})\}$.

\subsection{The Extension Process of $\text{LIFT}$}
\label{LIFT-decomposition}


In LIFT \citep{yuan2024following}, the authors propose a straightforward method for constructing a length instruction dataset. In Section \ref{3-4}, we will examine the limitations of this method. The LIFT approach can be divided into two types: the first part $\text{LIFT}_1$ constructs length instruction $x_l^{(i)}$ that both responses ($y_w^{(i)}$ and $y_l^{(i)}$) satisfy the length constraint specified in $x_l^{(i)}$; the second part $\text{LIFT}_2$ constructs length instruction $x_l^{(i)}$ that only one response ($y_w^{(i)}$ or $y_l^{(i)}$) satisfies. In cases where the rejected response $y_l^{(i)}$ adheres to the length instruction but the chosen response $y_w^{(i)}$ does not, this can lead to a reversal of the original preference order, which fundamentally undermines the semantic quality of the data. Furthermore, LIFT only considers a maximum length constraint, i.e., {\em ``less length constraint"}, without accounting for a minimum length constraint, i.e., {\em ``more length constraint"}. Therefore, we made a slight improvement to LIFT by adding an {\em ``more length constraint"}, resulting in LIFT-plus:

\begin{gather}
    \text{LIFT-plus} = \{(x_l^{(i)}, y_w^{(i)}, y_l^{(i)})\} \cup \{(x_l^{(i)}, y_l^{(i)}, y_w^{(i)})\} = \text{LIFT-plus}_{less} \cup \text{LIFT-plus}_{more}, \quad x_l^{(i)} = \text{\em length constraint} + x^{(i)}, \nonumber \\
    \text{LIFT-plus}_{less} = \text{LIFT} = \{(x_{l, less}^{(i)}, y_w^{(i)}, y_l^{(i)})\} \cup \{(x_{l, less}^{(i)}, y_l^{(i)}, y_w^{(i)})\}, \quad x_{l, less}^{(i)} = \text{\em less length constraint} + x^{(i)}, \nonumber\\
    \text{LIFT-plus}_{more} = \{(x_{l, more}^{(i)}, y_w^{(i)},  y_l^{(i)})\} \cup \{(x_{l, more}^{(i)}, y_l^{(i)},  y_w^{(i)})\}, \quad x_{l, more}^{(i)} = \text{\em more length constraint} + x^{(i)}, \nonumber\\
    \text{\em length constraint} = \text{\em less length constraint} \quad \text{or} \quad \text{\em more length constraint}, \nonumber\\
    \text{\em less length constraint} = \text{``Answer the following instruction using } \{word\_num\} \text{ words or less."}, \nonumber\\
    \text{\em more length constraint} = \text{``Answer the following instruction using } \{word\_num\} \text{ words or more."}. \nonumber
\end{gather}

% For simplicity, we also refer to LIFT-plus as LIFT in cases where there is no ambiguity. 
In this article, we refer to $x_{l, less}^{(i)}$ as the ``or less" length instruction and $x_{l, more}^{(i)}$ as the ``or more" length instruction. Similarly, LIFT-plus can also be decomposed into two parts: $\text{LIFT-plus}_1$ and $\text{LIFT-plus}_2$. In the first part $\text{LIFT-plus}_1$, the length instruction $x_l^{(i)} = x_{l, less}^{(i)}$ or $x_{l, more}^{(i)}$ is satisfied to both responses. As a result, it may degrade into a length-agnostic instruction, where the model disregards the added length constraint and focuses solely on the original prompt $x^{(i)}$.
% We will provide experimental evidence for this in Section \ref{5}.
Therefore, the key component that effectively facilitates adherence to length instructions is the second part, $\text{LIFT-plus}_2$, where only one response ($y_w^{(i)}$ or $y_l^{(i)}$) satisfies $x_l^{(i)}$. Furthermore, we divide $\text{LIFT-plus}_2$ into two subsets: $\text{LIFT-plus}_2^{reverse} = \{(x_l^{(i)}, y_l^{(i)}, y_w^{(i)})\}$, which reverses the original preference order between chosen and rejected responses due to the length constraint in $x_l^{(i)}$, and $\text{LIFT-plus}_2^{noreverse} = \{(x_l^{(i)}, y_w^{(i)}, y_l^{(i)})\}$, which preserves the original preference order. In addition, based on $\text{LIFT-plus}_2^{noreverse}$, we construct a dataset purely centered on length instructions, denoted as $\text{LIFT-plus}_2^{empty} = {(x_e^{(i)}, y_w^{(i)}, y_l^{(i)})}$, where $x_e^{(i)} =$ {\em length constraint} $+$ {\em [``empty prompt"]}. This dataset isolates length instructions from other semantic content and only focuses on the length instructions themselves. Here, $\text{LIFT-plus}_2^{empty}$ is specifically designed to investigate the impact of semantic prompts on length instruction following in Section \ref{3-4}.

\section{The Detailed Results and Analysis of Preliminary Explorations}
\label{appendix:additional-result-of-preliminary-experiments}

\subsection{Length Bias Indeed Exists}
\label{appendix:3-1}
Although the responses $y_w^{(i)}$ and $y_l^{(i)}$ in both $\mathcal{D}_{eval}^e$ and $\mathcal{D}_{eval}^r$ are semantically misaligned with their respective prompt, we observe that the reward models still achieve relatively high accuracies on these evaluation datasets. Specifically, the results for the three groups of models on $\mathcal{D}_{eval}^e$ and $\mathcal{D}_{eval}^r$, as shown in Table \ref{baseline-rm-original-empty-prompt-result}, indicate that nearly all reward models achieve an accuracy of $60\%$ or higher, which is very close to the accuracy observed on the original evaluation dataset $\mathcal{D}_{eval}$. This suggests that, even in the absence of the prompt, the reward models exhibit a significant bias toward the chosen responses.

To further investigate whether this bias primarily arises from response length, we first analyze the consistency of model evaluation results between $\mathcal{D}_{eval}^e$ (or $\mathcal{D}_{eval}^r$) and $\mathcal{D}_{eval}$, focusing on how varying prompts affect the selection of the same response. The results, presented in Table \ref{baseline-rm-original-empty-prompt-result}, show that in over $85\%$ of cases, the models select the same response regardless of the prompt. This reinforces the observation that the reward models' preference is not primarily driven by the prompts themselves but rather by other response-related factors.

Next, we plot the relationship between the response length and the corresponding reward score of the reward models for $\mathcal{D}_{eval}$, $\mathcal{D}_{eval}^e$, and $\mathcal{D}_{eval}^r$, as shown in Figure \ref{fig:length-score-ranges-baseline}. To facilitate comparison, we normalize the reward scores of both models (Qwen2-1.5B-Instruct and Llama-3.1-8B-Instruct) to the same range. The results reveal a strong linear correlation between the response length and the reward score across all three evaluation datasets. Specifically, as response length increases, the model's reward score also rises, indicating that response length plays a significant role in the model's assessment of response quality.

\begin{table*}[!ht]
    \centering
    \caption{Evaluation results of reward models (Baseline) on different evaluation datasets $\mathcal{D}_{eval}$, $\mathcal{D}_{eval}^e$ and $\mathcal{D}_{eval}^r$}
    \vskip 0.1in
    \begin{tabular}{lcccc}
    \toprule
        \multirow{2}{*}{Model} & \multicolumn{3}{c}{Accuracy ($\%$)} & Consistency ($\%$) \\ \cmidrule(lr){2-4}
        & $\mathcal{D}_{eval}$ & $\mathcal{D}_{eval}^e$ & $\mathcal{D}_{eval}^r$ & $\mathcal{D}_{eval}^e$ ($\mathcal{D}_{eval}^r$) \\
        \midrule
        Qwen2-1.5B-Base & 63.86 & 64.13 & 64.40 & 89.40 (87.34) \\ 
        Qwen2-1.5B-Instruct & 62.77 & 65.22 & 60.87 & 92.12 (90.42) \\ 
        \midrule
        Qwen2.5-7B-Base & 57.07 & 60.05 & 60.33 & 88.32 (90.13) \\
        Qwen2.5-7B-Instruct & 63.04 & 62.23 & 60.60 & 88.60 (89.85) \\
        \midrule
        Llama-3.1-8B-Base & 56.25 & 60.05 & 61.41 & 89.40 (85.24) \\
        Llama-3.1-8B-Instruct & 57.88 & 58.15 & 56.25 & 88.59 (87.26) \\
        \bottomrule
    \end{tabular}
    \vskip -0.1in
    \label{baseline-rm-original-empty-prompt-result}
\end{table*}

\begin{figure*}[!ht]
    \centering
    \subfigure[Original evaluation dataset $\mathcal{D}_{eval}$]{\includegraphics[width=0.33\textwidth]{figs/length-score-ranges-baseline-origin_v4.pdf}}
    \subfigure[Empty evaluation dataset $\mathcal{D}_{eval}^e$]{\includegraphics[width=0.33\textwidth]{figs/length-score-ranges-baseline-empty_v4.pdf}}
    \subfigure[Random evaluation dataset $\mathcal{D}_{eval}^r$]{\includegraphics[width=0.33\textwidth]{figs/length-score-ranges-baseline-random_v4.pdf}}
    \caption{The relationships between response lengths and reward scores of reward models (Baseline) trained with Qwen2-1.5B-Instruct and Llama-3.1-8B-Instruct, evaluated on different evaluation datasets, reveal a severe length bias in reward models.}
    \label{fig:length-score-ranges-baseline}
\end{figure*}

\subsection{Length Information is Much Easily to Learn}
\label{appendix:3-4}

The results for the Qwen2-1.5B and Llama-3.1-8B models trained on the three variant datasets ($\text{LIFT-plus}_2^{reverse}$, $\text{LIFT-plus}_2^{noreverse}$, and $\text{LIFT-plus}_2^{empty}$) are presented in Table \ref{rm-length-prompt-result-reverse/unreverse/empty_prompt}.  As observed, most reward models achieve similar or even lower accuracy on $\mathcal{D}_{eval}^q$ compared to Baseline models in Table \ref{tab:rm-result}, while their accuracy on $\mathcal{D}_{eval}^l$ remains relatively high. These results suggest that, although reward models are trained on diverse length instruction datasets, they all primarily learn to capture response length information rather than balancing adherence to length instructions with attention to the semantic content of the prompt.

\begin{table*}[!ht]
    \centering
    \caption{Evaluation results of different reward models (LIFT-plus variants) on quality ($\mathcal{D}_{eval}^q$) and length  ($\mathcal{D}_{eval}^l$) evaluation datasets}
    \vskip 0.1in
    \begin{tabular}{llcc}
    \toprule
        Model & Variant & Quality Eval Acc ($\%$) & Length Eval Acc ($\%$) \\ 
        \midrule
        \multirow{3}{*}{Qwen2-1.5B-Base} & $\text{LIFT-plus}_2^{reverse}$ & 58.78 & 85.58 \\ 
        & $\text{LIFT-plus}_2^{noreverse}$ & 59.41 & 87.78 \\
        & $\text{LIFT-plus}_2^{empty}$ & 58.33 & 89.71 \\
        \midrule
        \multirow{3}{*}{Qwen2-1.5B-Instruct} & $\text{LIFT-plus}_2^{reverse}$ & 60.11 & 86.54 \\ 
        & $\text{LIFT-plus}_2^{noreverse}$ & 63.17 & 85.21 \\
        & $\text{LIFT-plus}_2^{empty}$ & 61.02 & 85.53 \\
        \midrule
        \multirow{3}{*}{Llama-3.1-8B-Base} & $\text{LIFT-plus}_2^{reverse}$ & 51.34 & 91.32 \\ 
        & $\text{LIFT-plus}_2^{noreverse}$ & 57.71 & 88.14 \\
        & $\text{LIFT-plus}_2^{empty}$ & 50.27 & 88.78 \\
        \midrule
        \multirow{3}{*}{Llama-3.1-8B-Instruct} & $\text{LIFT-plus}_2^{reverse}$ & 60.11 & 95.19 \\ 
        & $\text{LIFT-plus}_2^{noreverse}$ & 56.65 & 92.31 \\
        & $\text{LIFT-plus}_2^{empty}$ & 57.97 & 90.71 \\
        \bottomrule
    \end{tabular}
    \vskip -0.1in
    \label{rm-length-prompt-result-reverse/unreverse/empty_prompt}
\end{table*}
% \begin{table*}[!ht]
%     \centering
%     \caption{Evaluation Results of Reward Model (LIFT-plus variant) on Quality and Length Datasets}
%     \vskip 0.1in
%     \begin{tabular}{ccccccc}
%         \toprule
%         \multirow{2}{*}{Metrics} & \multicolumn{3}{c}{Qwen2-1.5B-Base} & \multicolumn{3}{c}{Qwen2-1.5B-Instruct} \\
%         \cmidrule(lr){2-4} \cmidrule(lr){5-7} & LIFT-plus (reversed) & LIFT-plus (noreversed) & LIFT-plus (empty prompt) & LIFT-plus (reversed) & LIFT-plus (noreversed) & LIFT-plus (empty prompt) \\
%         \cmidrule(lr){1-1} Quality Eval Acc & 58.78 & 59.41 & 58.33 & 60.11 & 63.17 & 61.02 \\
%         Length Eval Acc & 85.58 & 87.78 & 89.71 & 86.54 & 85.21 & 85.53 \\
%         \multirow{2}{*}{Metrics} & \multicolumn{3}{c}{Llama-3.1-8B-Base} & \multicolumn{3}{c}{Llama-3.1-8B-Instruct} \\
%         \cmidrule(lr){2-4} \cmidrule(lr){5-7} & LIFT-plus (reversed) & LIFT-plus (noreversed) & LIFT-plus (empty prompt) & LIFT-plus (reversed) & LIFT-plus (noreversed) & LIFT-plus (empty prompt) \\
%         \cmidrule(lr){1-1} Quality Eval Acc & 51.34 & 57.71 & 50.27 & 60.11 & 56.65 & 57.97 \\
%         Length Eval Acc & 91.32 & 88.14 & 88.78 & 95.19 & 92.31 & 90.71 \\
%         \bottomrule
%     \end{tabular}
%     \vskip -0.1in
%     \label{rm-length-prompt-result-reverse/unreverse/empty_prompt}
% \end{table*}

To further illustrate how reward models quickly overfit to length instructions during training, thereby impeding semantic learning, we visualize the accuracy trajectories of Qwen-2-1.5B-Instruct and Llama-3.1-8B-Instruct on  $\mathcal{D}_{eval}^q$ and $\mathcal{D}_{eval}^l$ as training steps increase (see Figure \ref{fig:length-quality-acc-train-loss-train-step}). The results show that as training progresses, the model accuracy on $\mathcal{D}_{eval}^l$ increases rapidly, whereas the accuracy on $\mathcal{D}_{eval}^q$ improves at a much slower rate. Moreover, once the accuracy on $\mathcal{D}_{eval}^l$ reaches its peak, the accuracy on $\mathcal{D}_{eval}^q$ also begins to decline and gradually plateaus. This tendency strongly supports the conclusion that the reward models quickly prioritize learning length instructions, achieving high accuracy on length-related tasks, but at the expense of semantic quality. The slow improvement in the accuracy on $\mathcal{D}_{eval}^q$ further suggests that the reward models struggle to learn the necessary semantic understanding, as they are overly focused on conforming to length constraints.

\begin{figure*}[!ht]
    \centering
    \subfigure[Qwen2-1.5B-Instruct]{\includegraphics[width=0.4875\textwidth]{figs/qwen2-1.5b-chat-lift-eval-acc-and-loss-no-original-loss_v3.pdf}}
    \subfigure[Llama-3.1-8B-Instruct]{\includegraphics[width=0.4925\textwidth]{figs/llama-3.1-8b-chat-lift-eval-acc-and-loss-no-original-loss_v3.pdf}}
    \caption{The trajectories of Quality Eval Acc, Length Eval Acc, and Training Loss (Smoothed) for different reward models trained on $\text{LIFT-plus}_2^{reverse}$ across training steps.}
    \label{fig:length-quality-acc-train-loss-train-step}
\end{figure*}

\section{Additional Experimental Results and Analysis}
\label{additional-experimental-results}

\subsection{The Evaluation Details of DPO Models}
\label{dpo-eval-details}
Similar to Appendix \ref{LIFT-decomposition}, due to the length instruction $x_l$ in the AlpacaEval-LI benchmark provided by LIFT~\citep{yuan2024following} includes only the ``or less" length instruction, it is necessary to incorporate the ``or more" length instruction. Therefore, we extend the AlpacaEval-LI benchmark to include both the ``or less" and ``or more'' length instructions. Specifically, in AlpacaEval-LI, $word\_num$ is set based on the shortest response length for the original prompt $x$ across three advanced models --- GPT-4~\citep{achiam2023gpt}, Claude3-Opus, and Mistral Large --- which we assume approximates the median response length for each prompt. This suggests that well-reasoned, detailed responses may exceed $word\_num$, while concise yet valid responses may fall below it. Therefore, we improve the AlpacaEval-LI benchmark by modifying the ``or less" in $x_l$ to ``or more", specifically replacing ``Answer the following instruction using \{{\em word\_num}\} or less" with ``Answer the following instruction using \{{\em word\_num}\} or more", while keeping all other aspects unchanged. This results in the expanded AlpacaEval-LI-plus-more benchmark, and the original AlpacaEval-LI benchmark is referred to as AlpacaEval-LI-plus-less benchmark. In cases where there is no ambiguity, we refer to AlpacaEval-LI-plus-less and AlpacaEval-LI-plus-more benchmark as AlpacaEval-LI-plus benchmark.

For the DPO evaluation metrics, as described in Section \ref{experiment-settings}, we use automated assessment by leveraging GPT-4o~\citep{hurst2024gpt} to label each pair of responses as {\em win}, {\em tie}, or {\em lose} based on semantic quality. The prompt used to guide GPT-4o's evaluation is similar to the one provided in Appendix \ref{construct-D_eval^q}, as shown in Figure \ref{fig:gpt-eval-response-prompt-template}. Specifically, we evaluate each DPO model's performance by comparing it with its corresponding SFT/{\em Instruct} model using three metrics: {\em Length Acc} refers to the accuracy of the model-generated response length that satisfies the length constraint specified in the given length instruction $x_l$ in AlpacaEval-LI-plus. {\em Length Win Ratio} denotes the proportion of cases in which the model-generated response is labeled as {\em win} compared to the response generated by the SFT/{\em Instruct} models, under the given length instruction $x_l$. {\em Quality Win Ratio} measures the proportion of cases in which the model-generated response is labeled as {\em win} compared to the response generated by the SFT/{\em Instruct} models under the given original prompt $x$. {\em Length Win Ratio} and {\em Quality Win Ratio} evaluate the semantic quality of responses generated by the DPO models under different prompts.
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figs/gpt-eval-prompt-template.pdf}
    \caption{Response comparison template for evaluating the semantic quality between responses.}
    \label{fig:gpt-eval-response-prompt-template}
\end{figure*}

\subsection{The ``Short Bias" in LIFT-plus}
\label{short-bias-lift}
From the perspective of {\em Response Length} in Table~\ref{tab:dpo-origin-quality-result}, it might appear that LIFT-plus exhibits smaller length bias. This is not the case. Our results demonstrate that while LIFT-plus reduces response length, it does so at the expense of disregarding the balance between response length and semantic quality, resulting in suboptimal performance in both semantic quality and adherence to length instructions. Specifically, our results indicate that DPO models trained with the LIFT-plus method tend to exhibit a ``short bias" phenomenon, where the model prioritizes generating shorter responses without considering semantic quality, given the ``or less" instruction $x_l$ or the original prompt $x$. As shown in Table \ref{tab:dpo-origin-quality-result} and \ref{tab:dpo-length-result-or-less}, both for {\em Quality Win Ratio} and {\em Length Win Ratio}, LIFT-plus significantly underperforms compared to R-DPO and \ourbtmodel{}. Notably, for Qwen2.5-7B-Base and Qwen2.5-7B-Instruct, LIFT-plus's {\em Quality Win Ratio} is lower than R-DPO by $8.73\%$ and $8.47\%$, and lower than \ourbtmodel{} by $13.72\%$ and $18.94\%$, respectively. A similar trend is observed with Llama-3.1-8B-Base and Llama-3.1-8B-Instruct. Furthermore, we plot the distribution of response lengths generated by LIFT-plus trained on Llama-3.1-8B-Instruct under the length instruction $x_l$ in AlpacaEval-LI-plus-less benchmark, as shown in Figure \ref{fig:response_length-range-lift-length-instruct}. As observed, the mean response length specified by the length instruction $x_l$ in AlpacaEval-LI-plus-less is $180.23$, while the response lengths generated by LIFT-plus are all below $100$. Furthermore, more than $90\%$ of the responses have length under $64$. This clearly demonstrates that the DPO models trained with LIFT-plus focus solely on the ``or less" length instruction $x_l$, disregarding the semantic requirements of the original prompt $x$ and even overlooking the specific length constraint ({\em word\_num}) of the $x_l$ itself, resulting in the ``short bias". 
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/llama-3.1-8b-chat-lift-dpo-length-response-length-range_v2.pdf}
    \caption{Response length range generated by LIFT-plus trained on Llama-3.1-8B-Instruct using length instruction $x_l$ in AlpacaEval-LI-plus-less benchmark. The red dashed line $x = 64$ represents the $90\%$ threshold, indicating that $90\%$ of the responses have a length less than or equal to $64$.}
    \label{fig:response_length-range-lift-length-instruct}
\end{figure*}

\subsection{The Detailed Results and Analysis of Reward Models}
\label{additional-results-rm}
{\bf Rc-RM effectively follows length instructions.} Taking ``or less" length instruction as a representative experiment, we evaluate the score consistency of reward models with respect to length instructions by constructing a sequence of length instructions $x_l^{(i, j)}, j = \{1, 2, ..., 8\}$, with increasing {\em word\_num}. Specifically, we first select all instances $(x^{(i)}, y_w^{(i)}, y_l^{(i)})$ in $\mathcal{D}_{eval}$ where $|y_w^{(i)}| < |y_l^{(i)}|$, referring to this subset as $\mathcal{D}_{eval}^{less}$. For each $x^{(i)}$ in $\mathcal{D}_{eval}^{less}$, we then construct the following sequence of {\em word\_num} values ($L_{wn}$) to generate the corresponding sequence of length instructions $x_l^{(i, j)}$, and refer to the entire constructed evaluation dataset as $\mathcal{D}_{eval}^{mls}$:
% \begin{equation}
%     \begin{aligned}
%         & L_{wn} = [l_w - 2T, l_w - T, l_w, l_w + L,  l_w + 2L, l_l, l_l + T, l_l + 2T], \\
%         & \mathcal{D}_{eval}^{mls} = \{(x_{l}^{(i, j)}, y_w^{(i)}, y_l^{(i)})\}, j = \{1, 2, ..., 8\}, \\
%         & x_{l}^{(i, j)} = \text{``Answer the following instruction using } \{word\_num = L_{wn}^{(j)}\} \text{ words or less."} + x^{(i)},
%     \end{aligned}
%     \label{eqn:sequence-word_num}
% \end{equation}
\begin{gather}
    L_{wn} = [l_w - 2T, l_w - T, l_w, l_w + L,  l_w + 2L, l_l, l_l + T, l_l + 2T], \\ 
    \mathcal{D}_{eval}^{mls} = \{(x_{l}^{(i, j)}, y_w^{(i)}, y_l^{(i)})\}, j \in \{1, 2, ..., 8\}, \\
    x_{l}^{(i, j)} = \text{``Answer the following instruction using } \{word\_num = L_{wn}^{(j)}\} \text{ words or less."} + x^{(i)},
\end{gather}
where $T = 10$, $L = (l_l - l_w) / 3$, $l_w = |y_w|$, and $l_l = |y_l|$. For the sequence of $(x_l^{(i, j)}, y_w^{(i)}, y_l^{(i)}), j=\{1, 2, ..., 8\}$ in $\mathcal{D}_{eval}^{mls}$, the ideal performance of the reward model should be as follows: Initially, when $word\_num < l_w$, the length constraint of $x_l^{(i, j)}$ is invalid for both $y_w^{(i)}$ and $y_l^{(i)}$, the difference in reward scores between $y_w^{(i)}$ and $y_l^{(i)}$ primarily reflect the semantic difference in the original data $(x^{(i)}, y_w^{(i)}, y_l^{(i)})$. When $word\_num = l_w$, the length constraint of $x_l^{(i, j)}$ aligns with $y_w^{(i)}$ but not $y_l^{(i)}$, resulting in a greater difference between $y_w^{(i)}$ and $y_l^{(i)}$ compared to their original semantic difference. When $word\_num = l_l$, the length constraint of $x_l^{(i, j)}$ becomes valid for both $y_w^{(i)}$ and $y_l^{(i)}$, and the difference reverts to those derived from the original semantics. Therefore, the difference in the predicted scores of the reward model for $y_w^{(i)}$ and $y_l^{(i)}$ should initially increase with the rise in $word\_num$, then decrease, eventually returning to the original semantic difference. 
% In summary, the difference in predicted scores of reward model for $y_w$ and $y_l$ should initially increase with {\em word\_num}, then decrease, eventually returning to the original semantic difference.

The results, as shown in Figure \ref{fig:multi-length-bias-multi-wordnum-rm-score-diff} (c) and (d), demonstrate that both LIFT-plus and \ourrm{} exhibit an initial increase followed by a decrease in reward score differences as {\em word\_num} increases. However, LIFT-plus fails to return to the original difference after {\em word\_num} increases, instead remaining at a level higher or lower than that observed when {\em word\_num} $< |y_w|$. This indicates that LIFT-plus overfits to the length instructions, thereby neglecting the original semantic instructions. In contrast, \ourrm{} accurately reflects the expected behavior: the score difference increases with the rise in {\em word\_num}, then decreases, eventually returning to the original semantic difference. Moreover, \ourrm{} exhibits minimal score fluctuation at both extremes -- when length constraints are invalid for both responses and when they are valid for both responses. This robust performance provides strong evidence for the effectiveness of our approach in simultaneously adhering to both length and original semantic instructions.
% The results, as shown in Figure \ref{fig:multi-length-bias-multi-wordnum-rm-score-diff} (c) and (d), demonstrate that both LIFT-plus and \ourbtmodel{} exhibit an initial increase followed by a decrease in score differences as {\em word\_num} increases. However, LIFT-plus fails to return to the original difference after {\em word\_num} increases, instead remaining at a higher/lower level. This indicates that LIFT-plus overfits to the length instructions, thereby neglecting the original semantic instructions. In contrast, \ourbtmodel{} accurately reflects the expected behavior. Moreover, \ourbtmodel{} exhibits minimal score fluctuation at both extremes - when length constraints are invalid for both responses and when they are valid for both. This robust performance provides strong evidence for the effectiveness of our approach in simultaneously adhering to both length and original semantic instructions.

\subsection{The Reward Model Results of Qwen2.5-1.5B}
\label{qwen2.5-1.5b-results}
This subsection mainly presents the RM experimental results for Qwen2.5-1.5B and some unreasonable experimental phenomena. As show in Table \ref{tab:rm-result-qwen2.5-1.5b}, for Qwen2.5-1.5B-Base, the results are similar to those to Section \ref{section:rm-results}. \ourrm{} not only achieves higher performance in {\em Quality Eval Acc}, being $7.69\%$ higher than LIFT-plus and $6.41\%$ higher than ODIN, but also performs almost equally with LIFT-plus in {\em Length Eval Acc}. For Qwen2.5-1.5B-Instruct, \ourrm{} still outperforms LIFT-plus by $3.21\%$ and ODIN by $1.61\%$ in {\em Quality Eval Acc}. However, the results for LIFT-plus are somewhat unreasonable: its {\em Length Eval Acc} is only $67.63\%$, $12.5\%$ lower than \ourbtmodel{}, while its {\em Quality Eval Acc} is $10.9\%$ higher than Baseline.
% \begin{table}[!ht]
%     \centering
%     \caption{Evaluation Results of Reward Model on Quality and Length Eval Datasets (Qwen2.5-1.5B)}
%     \vskip 0.1in
%     \begin{tabular}{llcc}
%     \toprule
%         Model & Variant & Quality Eval Acc & Length Eval Acc \\ 
%         \midrule
%         \multirow{4}{*}{Qwen2.5-1.5B-Base} & baseline & 58.97 & 51.28 \\ 
%         & LIFT-plus & 60.26 & \textbf{87.82} \\
%         & ODIN & 61.54 & 55.45 \\
%         & our & \textbf{67.95} & 85.89 \\ 
%         \midrule
%         \multirow{4}{*}{Qwen2.5-1.5B-Instruct} & baseline & 58.01 & 53.53 \\ 
%         & LIFT-plus & 68.91 & 67.63 \\ 
%         & ODIN & 70.51 & 50.00 \\
%         & our & \textbf{72.12} & \textbf{80.13} \\ 
%         \bottomrule
%     \end{tabular}
%     \vskip -0.1in
%     \label{tab:rm-result-qwen2.5-1.5b}
% \end{table}
\begin{table}[!ht]
    \centering
    \caption{Evaluation results of reward models on quality ($\mathcal{D}_{eval}^q$) and length ($\mathcal{D}_{eval}^l$) evaluation datasets (Qwen2.5-1.5B)}
    \vskip 0.1in
    \begin{tabular}{ccccccccc}
        \toprule
        \multirow{2}{*}{Metrics} & \multicolumn{4}{c}{Qwen2.5-1.5B-Base} & \multicolumn{4}{c}{Qwen2.5-1.5B-Instruct} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9} & Baseline & LIFT-plus & ODIN & Rc-RM & Baseline & LIFT-plus & ODIN & Rc-RM \\
        \cmidrule(lr){1-1} Quality Eval Acc ($\%$) & 58.97 & 60.26 & 61.54 & \textbf{67.95} & 58.01 & 68.91 & 70.51 & \textbf{72.12} \\
        Length Eval Acc ($\%$) & 51.28 & \textbf{87.82} & 55.45 & 85.89 & 53.53 & 67.63 & 50.00 & \textbf{80.13} \\
        \bottomrule
    \end{tabular}
    \vskip -0.1in
    \label{tab:rm-result-qwen2.5-1.5b}
\end{table}
\begin{figure*}[!ht]
    \centering
    \subfigure[Length Eval Acc]{\includegraphics[width=0.48\textwidth]{figs/qwen2.5-1.5b-instruct-length-eval-acc-train-step_v3.pdf}}
    \subfigure[Quality Eval Acc]{\includegraphics[width=0.49\textwidth]{figs/qwen2.5-1.5b-instruct-quality-eval-acc-train-step_v3.pdf}}
    \caption{The trajectories in Length Eval Acc and Quality Eval Acc of Qwen2.5-1.5B-Instruct with training steps.}
    \label{fig:length-quality-acc-train-step-qwen2.5-1.5b-instruct}
\end{figure*}

Furthermore, we plot the {\em Quality Eval Acc} and {\em Length Eval Acc} trajectories for each method across training steps, as shown in Figure \ref{fig:length-quality-acc-train-step-qwen2.5-1.5b-instruct}. It can be observed that in the early stages of training, LIFT-plus's {\em Length Eval Acc} gradually increases, while its {\em Quality Eval Acc} is worse than that of Baseline, which aligns with our analysis in Section \ref{3-4}. However, after just $60$ steps of training, LIFT-plus's {\em Length Eval Acc} suddenly drops drastically, while {\em Quality Eval Acc} begins to rise significantly. This ultimately results in {\em Length Eval Acc} stabilizing at a relatively low value and {\em Quality Eval Acc} stabilizing at a relatively high value. Since this is a single isolated example, we suspect it may be related to the specific model type and will conduct a more detailed analysis in future work. 

\subsection{The Detailed Results and Analysis of DPO Models in AlpacaEval-LI-plus-less}
\label{or-less-results-analysis}
Due to length bias, models tend to generate longer responses, leading to higher {\em Length Acc} on the AlpacaEval-LI-plus-more benchmark, even when the models have no awareness of length instructions. This makes it difficult to fairly compare models' ability to follow length instructions. Therefore, by examining the performance on the AlpacaEval-LI-plus-less benchmark, we can better assess how effectively each method adheres to the specified length limits while still maintaining a high level of semantic quality in the generated responses.

The results on AlpacaEval-LI-plus-less are shown in Tabel \ref{tab:dpo-length-result-or-less}. As observed, for the {\em Base} models, Baseline models are trained only on $\mathcal{D}_{sft}$ and $\mathcal{D}_{rm}$, and do not demonstrate the ability to follow length instructions. On the other hand, the {\em Instruct} models inherently exhibit a stronger ability to follow length instructions. Consequently, Baseline models trained on the \textit{Instruct} models also acquire a certain degree of length instruction adherence. For R-DPO, since it was trained only on $\mathcal{D}_{rm}$, and according to the {\em Response Length} results on Tables \ref{tab:dpo-origin-quality-result}, \ref{tab:dpo-length-result-or-less} and \ref{tab:dpo-length-result-or-more}, it primarily reduces the response length across all instructions (include ``or more" length instruction), without explicitly following the length constraints. This indicates that R-DPO does not specifically address length instruction adherence, but simply focuses on minimizing the response length overall. Thus, the main comparison method we focus on here is LIFT-plus. 

It is evident that LIFT-plus suffers from a severe ``short bias" issue in Appendix~\ref{short-bias-lift}, focusing exclusively on the ``or less" length instruction $x_l$ itself without considering the specified {\em word\_num} or the original prompt $x$. This leads to extremely short responses that strictly comply with the length instruction but completely disregard the semantic requirement of the original prompt. Specifically, On Qwen2.5-7B-Base and Qwen2.5-7B-Instruct, the average response lengths generated by LIFT-plus are only $106.87$ and $23.56$, significantly below the average length constraint of $180.23$ in AlpacaEval-LI-plus-less. On Llama-3.1-8B-Instruct, the average response length is $35.01$, and on Llama-3.1-8B-Base, the average response length drops to a mere $6.62$, showing a 111.88-word difference compared to \ourbtmodel{}. This behavior results in LIFT-plus achieving nearly $100\%$ {\em Length Acc}, but with {\em Length Win Ratio} close to $0\%$, which fails to align with the desired semantic quality.

In contrast, \ourdpo{} not only adheres to the ``or less" length instruction but also considers the specified {\em word\_num} and original prompt. It strives to maximize the semantic quality of the response within the length constraint. Therefore, compared to the {\em Quality Win Ratio} in Table~\ref{tab:dpo-origin-quality-result}, \ourdpo{} maintains semantic quality even under length constraints. Notably, on Qwen2.5-7B-Base and Llama-3.1-8B-Base, \ourbtmodel{} achieves $82.04\%$ and $94.51\%$ {\em Length Acc}, respectively, while boosting the {\em Length Win Ratio} to $44.14\%$ and $64.96\%$, further enhancing the semantic abilities of SFT models. For both Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct, \ourbtmodel{} achieves $100\%$ {\em Length Accuracy}, while obtaining {\em Length Win Ratio} of $50.75\%$ and $64.71\%$, respectively. This fully demonstrates the effectiveness of \ourdpo{} in balancing adherence to length instructions with high semantic quality.

\subsection{The Detailed Results and Analysis of DPO Models in AlpacaEval-LI-plus-more}
\label{or-more-results-analysis}
In this subsection, we present the experimental results of the DPO models from Section \ref{dpo-results} on AlpacaEval-LI-plus-more, as shown in Table \ref{tab:dpo-length-result-or-more}. Similar to Section \ref{dpo-results}, since R-DPO was not trained on length instruction data, we primarily compare our method with LIFT-plus. First, as seen in Table \ref{tab:dpo-origin-quality-result}, due to the influence of length bias, Baseline model outputs longer responses, which means that it largely satisfies the ``or more" length instruction limit without focusing on the length constraint itself. Moreover, a comparison with Table \ref{tab:dpo-length-result-or-more} further confirms it: after adding the ``or more" length instruction limit, the length variation in the Baseline responses is not significant (especially for the {\em Instruct} model). At the same time, its {\em Length Win Ratio} demonstrates that compared to the SFT / {\em Instruct} models, the Baseline's responses are more redundant and repetitive, rather than truly improving semantic quality, leading to no significant improvement in {\em Length Win Ratio}. In fact, for all four models, the Baseline's {\em Length Win Ratio} is lower than ours by $17.46\%$, $24.31\%$, $9.61\%$, and $14.71\%$, respectively.

For LIFT-plus, the results are similar to those in Appendix \ref{short-bias-lift}, where it focuses only on the ``or more" length instruction and disregards the semantic requirements of the original prompt and the specific length constraints of the ``or more" length instruction. This causes the model to generate excessively long and unnecessary responses, resulting in a high {\em Length Acc} but a decline in {\em Length Win Ratio}. Specifically, for the {\em Instruct} models, due to the strong foundational instruct-following capability, the over-generation behavior of LIFT-plus is relatively mild, with response lengths generally controlled within $200 \sim 400$. Its {\em Length Win Ratio} is only lower than \ourdpo{} by $33.62\%$ and $26.18\%$. However, for the {\em Base} models, LIFT-plus's response length skyrockets above $600$, causing severe semantic redundancy, and its {\em Length Win Ratio} significantly drops, being lower than \ourdpo{} by $39.53\%$ and $35.91\%$. This result, when analyzed alongside the findings in Section \ref{dpo-results} and Appendix~\ref{short-bias-lift}, fully illustrates the drawbacks of LIFT-plus's over-reliance on length instructions and the complementary nature of \ourbtmodel{} in terms of both length instruction and semantic quality.

% \begin{table}[!ht]
%     \centering
%     \caption{Evaluation Results of DPO Model on AlpacaEval-LI-plus-more}
%     \vskip 0.1in
%     \begin{tabular}{llccc}
%     \toprule
%         Model & Variant & Length Acc (\%) & Length & Length Win Ratio (\%) \\
%         \midrule
%         \multirow{4}{*}{Qwen2.5-7B-Base} & baseline & 99.75 & 680.84 & 27.93 \\ 
%         & LIFT-plus & \textbf{99.88} & 550.27 & 5.86 \\ 
%         & R-DPO & 99.38 & 682.66 & 35.16 \\
%         & our & \textbf{99.88} & 319.05 & \textbf{45.39} \\ 
%         \midrule
%         \multirow{4}{*}{Qwen2.5-7B-Instruct} & baseline & 84.29 & 249.99 & 35.79 \\ 
%         & LIFT-plus & 99.88 & 303.16 & 26.48 \\
%         & R-DPO & 78.43 & 238.03 & 26.93 \\
%         & our & \textbf{100} & 263.30 & \textbf{60.10} \\ 
%         \midrule
%         \multirow{4}{*}{Llama-3.1-8B-Base} & baseline & 89.03 & 448.77 & 36.03 \\ 
%         & LIFT-plus & \textbf{99.75} & 664.20 & 9.73 \\
%         & R-DPO & 93.39 & 540.34 & \textbf{47.76} \\
%         & our & 97.51 & 429.69 & 45.64 \\ 
%         \midrule
%         \multirow{4}{*}{Llama-3.1-8B-Instruct} & baseline & 98.25 & 279.04 & 45.14 \\ 
%         & LIFT-plus & \textbf{99.88} & 413.42 & 33.67 \\ 
%         & R-DPO & 97.01 & 246.73 & 47.01 \\
%         & our & \textbf{99.88} & 279.56 & \textbf{59.85} \\ 
%         \bottomrule
%     \end{tabular}
%     \vskip -0.1in
%     \label{tab:dpo-length-result-or-more}
% \end{table}
\begin{table}[!ht]
    \centering
    \caption{Evaluation results of different DPO models on AlpacaEval-LI-plus-more}
    \vskip 0.1in
    \begin{tabular}{ccccccccc}
        \toprule
        \multirow{2}{*}{Metrics} & \multicolumn{4}{c}{Qwen2.5-7B-Base} & \multicolumn{4}{c}{Qwen2.5-7B-Instruct} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9} & Baseline & LIFT-plus & R-DPO & Rc-DPO & Baseline & LIFT-plus & R-DPO & Rc-DPO \\
        \cmidrule(lr){1-1} Length Acc (\%) & 99.75 & \textbf{99.88} & 99.38 & \textbf{99.88} & 84.29 & 99.88 & 78.43 & \textbf{100} \\
        Response Length & 680.84 & 550.27 & 682.66 & 319.05 & 249.99 & 303.16 & 238.03 & 263.30 \\ 
        Length Win Ratio (\%) & 27.93 & 5.86 & 35.16 & \textbf{45.39} & 35.79 & 26.48 & 26.93 & \textbf{60.10} \\ 
        \midrule
        \multirow{2}{*}{Metrics} & \multicolumn{4}{c}{Llama-3.1-8B-Base} & \multicolumn{4}{c}{Llama-3.1-8B-Instruct} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9} & Baseline & LIFT-plus & R-DPO & Rc-DPO & Baseline & LIFT-plus & R-DPO & Rc-DPO \\
        \cmidrule(lr){1-1} Length Acc (\%) & 89.03 & \textbf{99.75} & 93.39 & 97.51 & 98.25 & \textbf{99.88} & 97.01 & \textbf{99.88} \\
        Response Length & 448.77 & 664.20 & 540.34 & 429.69 & 279.04 & 413.42 & 246.73 & 279.56 \\ 
        Length Win Ratio (\%) & 36.03 & 9.73 & \textbf{47.76} & 45.64 & 45.14 & 33.67 & 47.01 & \textbf{59.85} \\
        \bottomrule
    \end{tabular}
    \vskip -0.1in
    \label{tab:dpo-length-result-or-more}
\end{table}

\subsection{Ablations and Extra Results}
\label{appendix-ablation}
Due to the high cost of evaluating the DPO model, we primarily use the reward model for ablation studies. First, we conduct ablation studies to validate the key design choices of our approach. For convenience, we denote $\{(x, x_l^1, y_w)\}$ as $\mathcal{D}_{Rc}^c$ and $\{(x_l^2, x, y_l)\}$ as $\mathcal{D}_{Rc}^r$. 

{\bf $\mathcal{D}_{Rc}^c$ and $\mathcal{D}_{Rc}^r$ are complementary.} We begin by proving the necessity of $\mathcal{D}_{Rc}^c$ and $\mathcal{D}_{Rc}^r$. To this end, we conduct ablation experiments using $\mathcal{D}_{rm} \cup \mathcal{D}_{Rc}^c$ ({\em w/o} $\mathcal{D}_{Rc}^r$) and $\mathcal{D}_{rm} \cup \mathcal{D}_{Rc}^r$ ({\em w/o} $\mathcal{D}_{Rc}^c$), with the results shown in Table \ref{tab:rm-result-ablation-Rc-BT}. As observed, when only one type of augmented dataset is used, the RM's {\em Quality Eval Acc} drops significantly, approaching the Baseline results in Table~\ref{tab:rm-result}. Simultaneously, its {\em Length Eval Acc} hovers around $50\%$, indicating that it fails to learn the length instruction. To further investigate, we prepend length instructions to each $x$ in $\mathcal{D}_{eval}^q$, forming $x_l$, where $y_w$ satisfies the length constraint in the instruction, , resulting in $\mathcal{D}_{eval}^{q,l}$. We then use the DPO models in Table \ref{tab:rm-result-ablation-Rc-BT} to score the pairs $(x_l, y_w)$ in $\mathcal{D}_{eval}^{q,l}$ and $(x, y_w)$ in $\mathcal{D}_{eval}^q$ to observe changes in the RM's predicted scores. In theory, for the pair $(x_l, y_w)$, since $y_w$ satisfies both the length constraint and original prompt of $x_l$, it should outperform $(x, y_w)$. That is, the score of reward model for $(x_l, y_w)$ should be higher than that for $(x, y_w)$. However, the results, illustrated in Figure \ref{fig:llama-3.1-8b-instruct-origin-length-score-ranges-ablation} (a), reveal that for the RM trained on $\mathcal{D} \cup \mathcal{D}_c$, the scores of $(x_l, y_w)$ with added length instructions are consistently lower than those of the original $(x, y_w)$, regardless of whether the added length constraint matches $y_w$'s length. This indicates that after training on $\mathcal{D}_{rm} \cup \mathcal{D}_{Rc}^c$, the RM learns a {\em length instruction bias}. Specifically, in $\mathcal{D}_{rm} \cup \mathcal{D}_{Rc}^c$, for any $(x_l, y_w)$, with an added length instruction, the reward model merely reduces the score compared to the original $(x, y_w)$ pair, without paying attention to the length instruction itself. A similar phenomenon in Figture~\ref{fig:llama-3.1-8b-instruct-origin-length-score-ranges-ablation} is observed in the RM trained on $\mathcal{D}_{rm} \cup \mathcal{D}_{Rc}^r$. Therefore, the combination of $\mathcal{D}_{Rc}^c$ and $\mathcal{D}_{Rc}^r$ is necessary to prevent the reward model from being exploited by this bias hacking.
\begin{figure*}[!ht]
    \centering
    \vskip 0.1in
    \subfigure[Qwen2-1.5B-Instruct]{\includegraphics[width=0.4\textwidth]{figs/qwen2-1.5b-chat-range-Dr-rm_v3.pdf}}
    \subfigure[Llama-3.1-8B-Instruct]{\includegraphics[width=0.4\textwidth]{figs/llama-3.1-8b-chat-range-Dr-rm_v3.pdf}}
    % \vspace{-2ex}
    \caption{Variations in Rc-RM's {\em Length Eval Acc} and {\em Quality Eval Acc} as $\mathcal{D}_{Rc}^r$ increases. The green dashed line denotes the Baseline {\em Quality Eval Acc}, while the blue dashed line represents the {\em Length Eval Acc} of LIFT-plus.}
    \label{fig:llama-3.1-8b-instruct-ablation-Dr-range-rm-score}
    \vskip -0.1in
\end{figure*}

% {\bf Effect of different ratios of $\mathcal{D}_{Rc}^c$ and $\mathcal{D}_{Rc}^r$ mixtures.} We also briefly explore the efficiency of different ratios of $\mathcal{D}_{Rc}^c$ and $\mathcal{D}_{Rc}^r$. Specifically, we fix $\mathcal{D}_{rm}$ and $\mathcal{D}_{Rc}^c$ constant, and increase $\mathcal{D}_{Rc}^r$ from $0\%$ to $100\%$ with a $10\%$ increment. The results are shown in Figure \ref{fig:llama-3.1-8b-instruct-origin-length-score-ranges-ablation}(c) and \ref{fig:llama-3.1-8b-instruct-origin-length-score-ranges-ablation}(d). As observed, as $\mathcal{D}_{Rc}^r$ increases, {\em Length Eval Acc} rises rapidly, while {\em Quality Eval Acc} gradually increases. After the growth rate of {\em Length Eval Acc} slows down, {\em Quality Eval Acc} continues to rise steadily, which effectively demonstrates the complementary effect of $\mathcal{D}_{Rc}^c$ and $\mathcal{D}_{Rc}^r$ in mitigating length bias and enhancing adherence to length instructions.

\begin{table*}[!ht]
    \centering
    \caption{Evaluation results of different reward models on quality ($\mathcal{D}_{eval}^q$) and length ($\mathcal{D}_{eval}^l$) evaluation datasets (HH-RLHF)}
    \vskip 0.1in
    \begin{tabular}{llcc}
    \toprule
        Model & Variant & Quality Eval Acc ($\%$) & Length Eval Acc ($\%$) \\ 
        \midrule
        \multirow{4}{*}{Qwen2-1.5B-Instruct} & Baseline & 58.65 & 59.36 \\ 
        & LIFT-plus & 58.01 & \textbf{85.90} \\ 
        & ODIN & 59.62 & 48.72 \\
        & Rc-RM & \textbf{61.22} & 85.26 \\ 
        \midrule
        \multirow{4}{*}{Qwen2.5-7B-Instruct} & Baseline & 62.50 & 52.24 \\ 
        & LIFT-plus & 57.37 & \textbf{92.63} \\ 
        & ODIN & 65.06 & 57.69 \\
        & Rc-RM & \textbf{65.38} & 91.03 \\ 
        \midrule
        \multirow{4}{*}{Llama-3.1-8B-Instruct} & Baseline & 63.14 & 51.60 \\ 
        & LIFT-plus & 58.33 & \textbf{93.27} \\ 
        & ODIN & 64.10 & 50.00 \\
        & Rc-RM & \textbf{69.23} & 91.03 \\ 
        \bottomrule
    \end{tabular}
    \vskip -0.1in
    \label{tab:rm-result-hh}
\end{table*}

{\bf Effectiveness of different ratios of $\mathcal{D}_{Rc}^c$ and $\mathcal{D}_{Rc}^r$ mixtures.} We also briefly explore the impact of different ratios of $\mathcal{D}_{Rc}^c$ and $\mathcal{D}_{Rc}^r$ on the final RM results. Specifically, we fix $\mathcal{D}_{rm}$ and $\mathcal{D}_{Rc}^c$ constant, and increase $\mathcal{D}_{Rc}^r$ from $0\%$ to $100\%$ with a $10\%$ increment. The results are shown in Figure \ref{fig:llama-3.1-8b-instruct-ablation-Dr-range-rm-score} (a) and (b). As observed, initially, the model is biased toward the one-sided data of $\mathcal{D}_{Rc}^c$, leading to low {\em Quality Eval Acc} and {\em Length Eval Acc}, even falling below the Baseline models (denoted as green dashed line). As $\mathcal{D}_{Rc}^r$ increased, {\em Length Eval Acc} rises rapidly, with the growth rate slowing down around $30\%$, while {\em Quality Eval Acc} gradually increases.
% Unlike Section \ref{3-4}, after the growth rate of {\em Length Eval Acc} slows down, {\em Quality Eval Acc} continues to rise steadily, which 
It effectively demonstrates the complementary effect of $\mathcal{D}_{Rc}^c$ and $\mathcal{D}_{Rc}^r$ in mitigating length bias and enhancing adherence to length instructions.

{\bf Effectiveness across different datasets.} To further validate the generalizability of \ourbtmodel{}, we conduct RM training on the HH-RLHF \citep{bai2022training} dataset\footnote{We use {\em Instruct} models for direct RM training on the HH-RLHF dataset.}, while still using $\mathcal{D}_{eval}^q$ and $\mathcal{D}_{eval}^l$ for evaluation. The results are shown in Table \ref{tab:rm-result-hh}. As seen, despite the HH-RLHF dataset being from a different domain compared to the OpenAssistant dataset \citep{kopf2024openassistant}, which can be considered an out-of-distribution (OOD) scenario, \ourrm{} still significantly outperforms other methods on $\mathcal{D}_{eval}^q$, while achieving results similar to LIFT-plus on $\mathcal{D}_{eval}^l$. Specifically, for Qwen2-1.5B-Instruct, \ourrm{} outperforms Baseline by $2.57\%$ and ODIN by $1.60\%$. For Qwen2.5-7B-Instruct, \ourrm{} surpasses Baseline by $2.88\%$ and ODIN by $0.32\%$. Notably, for Llama-3.1-8B-Instruct, \ourrm{} significantly outperforms Baseline by $6.09\%$ and also surpasses ODIN by $5.13\%$, further demonstrating the effectiveness of \ourrm{} in mitigating length bias. Moreover. on $\mathcal{D}_{eval}^l$, \ourrm{} achieves results comparable to LIFT, reaffirming its effectiveness in following length instructions. Finally, the consistent performance across different datasets provides strong evidence of the robustness and effectiveness of \ourbtmodel{} in diverse scenarios.

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
