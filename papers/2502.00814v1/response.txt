\section{Related Work}
\label{related-work}

In this section, we briefly introduce the background and related work of RLHF, reward hacking and length instruction following. A more detailed version is in Appendix~\ref{full-related work}.

{\bf Reinforcement Learning from Human Feedback.} RLHF**Schmidhuber, J., "Reinforcement learning in feedback neural circuits"** involves training a reward model to approximate human preferences and optimizing the LLMs through reinforcement learning (RL)**Sutton, R. S., Barto, A. G., "Reinforcement Learning: An Introduction"**. However, this approach suffers from training instability and requires careful tuning of numerous hyperparameters. Recent direct preference alignment methods, particularly DPO**Stiennon, S., et al., "Modeling Preferential Treatment in Reinforcement Learning"**, offer a more stable alternative. By reformulating the reward function, DPO eliminates the need for an online reward model, enabling robust offline preference learning**Guo, X., et al., "Deep Preference Alignment with Confidence"**. Our method not only enhances RLHF by improving reward model robustness but also seamlessly integrates into the DPO framework.

{\bf Reward Hacking.} RLHF is vulnerable to reward hacking, where LLMs exploit inherent biases in the proxy preference model to achieve higher scores**Zellers, P., et al., "Reinforcement learning with human feedback"**. This phenomenon persists in DPO, primarily arising from task complexity, evaluation limitations, and biased feedback**Chen, Y., et al., "Biased Feedback in Reinforcement Learning"**. One common form of reward hacking is length bias**Liao, X., et al., "Length bias in reinforcement learning"**, wherein models favor longer responses regardless of semantic quality. Previous attempts to address this issue include ODIN's**Daxin Liu et al., "Regularization Techniques for Reward Hacking Mitigation"** length regularization in reward modeling; dual-model training with different learning rates**Goyal, R. et al., "Dual-model Training for Reward Hacking Mitigation"**; and DPO objective modification with length penalties**Stiennon, S., et al., "Length Regularized Preference Alignment"**. In contrast to these approaches that suppress length information, our method enables models to distinguish between semantic intent and length instructions, preserving both aspects in a balanced manner.

{\bf Length Instruction Following.} While current LLMs demonstrate implicit length-following capabilities**Mehri, S., et al., "Length-Adaptive Transformers for Response Generation"**, responding to qualitative descriptors like ``concise" or ``verbose", they struggle with explicit numerical constraints such as ``$150$ words or less". Although LIFT**Zhang, Y. et al., "LIFT: Learning Instructions from Text"** improves length instruction adherence through specialized datasets, this approach compromises semantic quality, resulting in degraded overall performance. Our method, in contrast, achieves effective length instruction following while simultaneously enhancing semantic quality.