[
  {
    "index": 0,
    "papers": [
      {
        "key": "ziegler2019fine",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "schulman2017proximal",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "hong2024orpo",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Orpo: Monolithic preference optimization without reference model"
      },
      {
        "key": "chen2024noise",
        "author": "Chen, Huayu and He, Guande and Yuan, Lifan and Cui, Ganqu and Su, Hang and Zhu, Jun",
        "title": "Noise contrastive alignment of language models with explicit rewards"
      },
      {
        "key": "ethayarajh2024kto",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "pan2022effects",
        "author": "Pan, Alexander and Bhatia, Kush and Steinhardt, Jacob",
        "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models"
      },
      {
        "key": "casper2023open",
        "author": "Stephen Casper and Xander Davies and Claudia Shi and Thomas Krendl Gilbert and J{\\'e}r{\\'e}my Scheurer and Javier Rando and Rachel Freedman and Tomek Korbak and David Lindner and Pedro Freire and Tony Tong Wang and Samuel Marks and Charbel-Raphael Segerie and Micah Carroll and Andi Peng and Phillip J.K. Christoffersen and Mehul Damani and Stewart Slocum and Usman Anwar and Anand Siththaranjan and Max Nadeau and Eric J Michaud and Jacob Pfau and Dmitrii Krasheninnikov and Xin Chen and Lauro Langosco and Peter Hase and Erdem Biyik and Anca Dragan and David Krueger and Dorsa Sadigh and Dylan Hadfield-Menell",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "key": "lambert2023alignment",
        "author": "Lambert, Nathan and Calandra, Roberto",
        "title": "The alignment ceiling: Objective mismatch in reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "dubois2024alpacafarm",
        "author": "Dubois, Yann and Li, Chen Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy S and Hashimoto, Tatsunori B",
        "title": "Alpacafarm: A simulation framework for methods that learn from human feedback"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "singhal2023long",
        "author": "Singhal, Prasann and Goyal, Tanya and Xu, Jiacheng and Durrett, Greg",
        "title": "A long way to go: Investigating length correlations in rlhf"
      },
      {
        "key": "park2024disentangling",
        "author": "Park, Ryan and Rafailov, Rafael and Ermon, Stefano and Finn, Chelsea",
        "title": "Disentangling length from quality in direct preference optimization"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "chenodin",
        "author": "Chen, Lichang and Zhu, Chen and Chen, Jiuhai and Soselia, Davit and Zhou, Tianyi and Goldstein, Tom and Huang, Heng and Shoeybi, Mohammad and Catanzaro, Bryan",
        "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "shen2023loose",
        "author": "Shen, Wei and Zheng, Rui and Zhan, Wenyu and Zhao, Jun and Dou, Shihan and Gui, Tao and Zhang, Qi and Huang, Xuanjing",
        "title": "Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "park2024disentangling",
        "author": "Park, Ryan and Rafailov, Rafael and Ermon, Stefano and Finn, Chelsea",
        "title": "Disentangling length from quality in direct preference optimization"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "yuan2024following",
        "author": "Yuan, Weizhe and Kulikov, Ilia and Yu, Ping and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Weston, Jason and Xu, Jing",
        "title": "Following length constraints in instructions"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "yuan2024following",
        "author": "Yuan, Weizhe and Kulikov, Ilia and Yu, Ping and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Weston, Jason and Xu, Jing",
        "title": "Following length constraints in instructions"
      }
    ]
  }
]