






\section{Benchmarks Quality Evaluation} \label{sec:Benchmarks_Comparisons}

A high-quality benchmark for multi-modal retrieval is essential, yet few existing benchmarks are designed for this purpose, and none comprehensively define or implement the necessary properties. \cref{Table:benchmark_comparison} compares our benchmark with other prominent ones, which suffer from limitations such as poor alignment with real-world queries, high false-negative rates, and trivial difficulty.




\paragraph{Accurate Labeling.}
Many perceived retrieval errors in existing benchmarks are actually false negatives, meaning pages that correctly answer the query but were mislabeled as irrelevant. To mitigate this, we introduce a false-negative verification process that exhaustively labels all valid pages.
\textbf{\emph{Human Evaluation.}} We sampled 50 top-1 retrieval errors of ColQwen on Vidore, MMlongbench, and our benchmark.
Annotators reviewed the query and retrieved page (labeled as negative) to determine if it could answer the query (\cref{fig:Human_False_Negative}). A total of 234 responses from 5 annotators were collected.

\vspace{0.15cm}
\hspace{0.2cm}
% \begin{table}[h]
    {\footnotesize
\centering
% \begin{tabular}{@{\extracolsep{\fill}}llcc} 
\begin{tabular}{lccc} 
\toprule & \textbf{Vidore} & \textbf{MMLong}  & \textbf{Ours}  \\
\midrule
\textbf{False Negative (\%) \(\downarrow\)} & 86.9 & 77.8 & 31.9 \\
\bottomrule
\end{tabular}}
    % \caption{Caption}
%     \label{tab:my_label}
% \end{table}


\vspace{0.15cm}
\noindent
The table shows that Vidore and MMlongbench had a high rate of false negatives, whereas our benchmark, despite its challenging design with similar sub-domain pages, had significantly fewer, proving the effectiveness of accurate labeling.




\paragraph{Enhanced Difficulty.}
A strong benchmark must pose real challenges. Existing ones fall short by offering too few relevant candidates or allowing retrieval via simple keyword matching rather than true semantic understanding.  
For example, having 1,000 financial pages from different companies is insufficient, as knowing the company name narrows the candidates to a few dozen. The ColQwen model achieves an NDCG@5 of around 90 on Vidore. Other sub-datasets, although reporting lower performance, contain many errors that are actually false negatives, as demonstrated by our human evaluation presented above.  
We address this issue through accurate labeling and by incorporating long documents and extensive sub-domain coverage. This provides many similar pages, making retrieval more challenging and better reflecting real-world scenarios. Moreover, we prevent trivial keyword-based retrieval by introducing the first rephrasing benchmark for multi-modal document RAG, ensuring robustness to query variations and promoting semantic learning.




\paragraph{Realistic-RAG Queries.}
To reflect real RAG use cases, queries must resemble natural information-seeking questions. Our benchmark ensures this through a two-step RAG-tailored pipeline: generation and filtering. 
\textbf{\emph{Human Evaluation.}} We randomly sampled 50 queries from Vidore, MMLongBench, and our benchmark.
Annotators, unaware of the source benchmark or study goal, evaluated whether each query could reasonably be asked by a real user (\cref{fig:Human_query_RAG}). A total of 578 responses were collected from 5 annotators. 


{\footnotesize
\vspace{0.15cm}
\hspace{0.02cm}
\centering
\begin{tabular}{@{\extracolsep{\fill}}lccc} 
\toprule & \textbf{Vidore} & \textbf{MMLong}  & \textbf{Ours}  \\
\midrule
\textbf{Realistic-RAG Queries (\%) \(\uparrow\) } & 43.6  & 35.2  & 85.0 \\
\bottomrule
\end{tabular}}


\vspace{0.15cm}
\noindent
The table shows that most Vidore/MMLongBench queries were labeled as unrealistic RAG queries (see some examples in \cref{fig:others_examples}), whereas 85\% of ours were validated as realistic, highlighting shortcomings in existing benchmarks and the effectiveness of our query generation and filtering process.



\begin{table}[ht!]
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height
\setlength{\dashlinedash}{0.5pt}
\setlength{\dashlinegap}{0.5pt}
\setlength\tabcolsep{4pt} % Adjust column spacing

\hspace{-0.15cm}
\begin{tabular}{@{\extracolsep{\fill}}lcccc} 
\toprule
\textbf{Benchmark} & \textbf{FinReport} & \textbf{FinSlides} & \textbf{TechReport} & \textbf{TechSlides}  \\
\midrule
\textbf{\emph{\underline{Text}}} \\
% \textit{BM25 (Text Only)}    & 17.8 & 6.0 & 33.7 & 28.6   \\
% \textit{BGE-M3 (Text Only)}  & 34.1 & 7.2 & 36.0 & 43.7   \\
\textit{BM25 (OCR)}    & 21.7 & 5.9 & 35.1 & 31.2   \\
\textit{BGE-M3 (OCR)}  & 36.5 & 11.4 & 37.1 & 49.7  \\
\textit{BM25 (Captioning)}  & 25.3 & 9.9 & 37.2 & 36.1    \\
\textit{BGE-M3 (Captioning)} & 35.9 & 13.8 & 37.5 & 51.7   \\
\midrule
 \underline{\emph{\textbf{Vision}}} \\
% \textit{Visrag}  & 24.7 & 16.7 & 46.7 & 69.4  \\
% % \cmidrule(lr){1-5}
% \addlinespace
\textit{ColPali}  & 34.5 & 27.6 & 62.0 & 75.8  \\
\textit{\textbf{Rob}ColPali}  & 47.1 {\tiny \textcolor{DarkGreen}{+12.6}}  & 48.4 {\tiny \textcolor{DarkGreen}{+20.8}}  & 66.6 {\tiny \textcolor{DarkGreen}{+4.60}}  & 82.8 {\tiny \textcolor{DarkGreen}{+7.0}}  \\
\textit{\textbf{Tab}ColPali}  & 50.5 {\tiny \textcolor{DarkGreen}{+16.0}}  & 41.5 {\tiny \textcolor{DarkGreen}{+13.9}}  & 61.3 {\tiny \textcolor{red}{-0.7}}  & 77.6 {\tiny \textcolor{DarkGreen}{+1.8}}  \\
\textit{\textbf{RobTab}ColPali}  & \textbf{63.2} {\tiny \textcolor{DarkGreen}{+28.7}}  & \textbf{58.3} {\tiny \textcolor{DarkGreen}{+30.7}}  & \textbf{70.7} {\tiny \textcolor{DarkGreen}{+8.7}}  & \textbf{83.3} {\tiny \textcolor{DarkGreen}{+7.5}}  \\
\addlinespace
% \cmidrule(lr){1-5}
\textit{ColQwen}   & 41.8 & 31.1 & 66.9 & 78.1  \\
\textit{\textbf{Rob}ColQwen}  & 47.5 {\tiny \textcolor{DarkGreen}{+5.7}}  & 44.3 {\tiny \textcolor{DarkGreen}{+13.2}}  & 69.5 {\tiny \textcolor{DarkGreen}{+2.6}}  & 83.0 {\tiny \textcolor{DarkGreen}{+4.9}}  \\
\textit{\textbf{Tab}ColQwen}  & 54.0 {\tiny \textcolor{DarkGreen}{+12.2}}  & 49.6 {\tiny \textcolor{DarkGreen}{+18.5}}  & 65.9 {\tiny \textcolor{red}{-1.0}}  & 78.9 {\tiny \textcolor{red}{-0.8}}  \\
\textit{\textbf{RobTab}ColQwen}  & \textbf{\underline{67.1}} {\tiny \textcolor{DarkGreen}{+25.3}}  & \textbf{\underline{61.6}} {\tiny \textcolor{DarkGreen}{+30.5}}  & \textbf{\underline{73.2}} {\tiny \textcolor{DarkGreen}{+6.3}}  & \textbf{\underline{85.0}} {\tiny \textcolor{DarkGreen}{+6.9}}  \\

\bottomrule
\end{tabular}
\caption{
\textbf{Performance of Different Models on Our Benchmark.}  
We evaluate various models, including text- and vision-based approaches, across our four benchmarks. Results, measured using NDCG@5, are reported on our final benchmark with queries rephrased at the highest level (Level 3). We also present results for our fine-tuned models trained on our proposed datasets: \emph{Rob} – trained on a rephrased dataset, \emph{Tab} – trained on a table-heavy dataset, and \emph{RobTab} – incorporating both.
}
\label{Table:model_comparisons_on_benchmarks}
\vspace{-0.4cm}
\end{table}

\paragraph{Summary.}
Our benchmark enhances multi-modal retrieval evaluation by introducing non-trivial difficulty with long documents and broad sub-domain coverage. It ensures RAG-aligned queries and promotes semantic retrieval over keyword matching through query rephrasing, addressing key limitations of existing benchmarks.


