\section{Model Evaluation \& Enhancement}
\label{sec:results}






\paragraph{Evaluation Models and Metrics.}
We evaluate multiple models on our benchmark, covering both text and vision-based approaches. We use the ColPali benchmark code (ViDoRe) to assess our text-based models and the vision-based models ColPali and ColQwen.
For the text-based methods, we follow the framework suggested in ColPali, which employs
\emph{Unstructured}
in a high-resolution configuration with OCR engine to parse PDFs.
For each document, \emph{Unstructured} produces text chunks and visual chunks (e.g., tables, figures, images).
We consider two text-based variants:
(1) \emph{OCR}, where visual data is processed through an OCR engine,
and (2) \emph{Captioning}, where visual elements are described using a Vision Language Model (Qwen2-VL-72B-Instruct \cite{wang2024qwen2}).
We then evaluate two retrieval methods:
\emph{Okapi BM25} and \emph{BGE-M3}~\cite{chen2024bge}
 (see \cref{sec:appendix_model_evalaution} for more details).
We report NDCG@5 as our primary ranking metric, which evaluates how well relevant items are ranked within the top 5 results, giving higher importance to those appearing earlier. Additional metrics and details provided in \ref{sec:appendix_model_evalaution} \& \ref{sec:additional_results}.








\subsection{Results Analysis}
In \cref{Table:model_comparisons_on_benchmarks}, we report NDCG@5 performance for different models across our four benchmarks with high rephrasing levels, which better reflect real-world scenarios. We first present observations about the vanilla models, including text-based models, ColPali, and ColQwen:
\emph{\textbf{Visual vs. Text-Based Models.} }
Vision-based models, which use VLMs on page images, significantly outperform text-based models across all benchmarks. This supports the notion that visual information is essential for our benchmark and that these models can effectively utilize it.  
\emph{\textbf{Non-Trivial Difficulty.}} Performance is generally low, especially compared to Vidore, where ColQwen achieves nearly 90\% on average.
\emph{\textbf{Rephrasing Effects on Performance.}} 
Some of the drop in performance is due to rephrasing.
In \cref{Table:average_rephrasing_benchmarks}, we analyze the impact of rephrasing level, showing a clear performance drop as rephrasing intensity increases. BM25 suffers the most, as expected for a lexical-based model, while dense retrieval models are more resilient.
\emph{\textbf{Table-Heavy Finance Benchmarks Are Harder.}}  
Our financial benchmarks (FinReport, FinSlides), which are table-heavy, are significantly more challenging than text/visual-based ones (see table vs. non-table analysis in \cref{Table:model_comparisons_on_labels}).





\subsection{Table and Finance Focused Training}  
To address the challenges in table-heavy datasets, we curated a table-focused finance dataset using FinTabNet~\citep{zheng2020global}, which contains complex tables from S\&P 500 company reports. Through the pipeline in \cref{sec:benchmark}, we generated 46,000 query-answer-page triplets to enhance retrieval for table-heavy financial data (see \cref{Table:VLM_ablation} for a VLM ablation study). We fine-tuned ColPali and ColQwen for one epoch on this dataset while incorporating the ColPali training set, producing the \textbf{\emph{TabCol}} models.

\vspace{0.1cm}
\noindent
As shown in \cref{fig:Table_results_plots} and \cref{Table:model_comparisons_on_benchmarks}, TabCol models significantly improve performance on financial benchmarks, effectively addressing table-heavy dataset challenges. Importantly, this enhancement does not come at the cost of generalization, as TabCol models continue well across the two other benchmarks. 

\vspace{0.1cm}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/RobTabvsRobvsCol.png} 
    % \vspace{-0.55cm}
    \caption{\textbf{Table-Focused Training Improves Financial Benchmarks.} Fine-tuning with our proposed table-heavy training set, combined with the ColPali training set (both in their original and rephrased versions) significantly enhances performance on financial benchmarks (results shown for rephrasing level 3).}
    \vspace{-0.1cm}
    \label{fig:Table_results_plots} 
\end{figure}

\begin{figure*}[ht]
    \centering
    % Left Side - Table
    \begin{minipage}{0.4\textwidth}
        \footnotesize
        \renewcommand{\arraystretch}{1.5} % Adjust row height
        \setlength\tabcolsep{5pt} % Adjust column spacing

        \vspace{-0.15cm}
        \centering
        \begin{tabular}{lcccc}
            \toprule
            \textbf{Rephrasing Levels} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} \\
            \midrule
            BM25 (Captioning) & 52.7 & 41.6 & 31.3 & 27.1 \\
            BGE-M3 (Captioning) & 43.3 & 39.2 & 36.5 & 35.2 \\
            \addlinespace
            ColPali & 71.3 & 65.3 & 60.3 & 56.6 \\
            \textbf{Rob}ColPali & 76.7 & 72.6 & 68.4 & 65.7 \\
            \textbf{RobTab}ColPali & 80.8 & 77.8 & 74.9 & 72.7 \\
            \addlinespace
            ColQwen & 78.9 & 72.5 & 68.2 & 65.3 \\
            \textbf{Rob}ColQwen & 81.6 & 77.3 & 73.7 & 71.0 \\
            \textbf{RobTab}ColQwen & 85.1 & 81.7 & 78.6 & 76.4 \\
            \bottomrule
        \end{tabular}
        \vspace{-0.05cm}
        \captionof{table}{
        \textbf{Query Rephrasing Effect.} This table presents the NDCG@5 scores averaged across all benchmarks for different models and rephrasing levels. `0' represents no rephrasing, while `3' indicates significant rephrasing (see \cref{Table:rephrasing_benchmarks} for full results).}
        \label{Table:average_rephrasing_benchmarks}
    \end{minipage}
    \hfill
    % Right Side - Figure
    \begin{minipage}{0.58\textwidth}
        \centering
        % \vspace{-0.35cm}
        \includegraphics[width=\linewidth]{figures/rob_avg_improvement_subplot.png}
        
        \begin{minipage}{0.95\linewidth} % Adjust this width as needed
            \centering
            \vspace{0.41cm}
            \caption{\textbf{Fine-Tuning on Rephrased Training Set.} We compare the NDCG@5 scores across rephrasing levels for baseline models (ColPali and ColQwen) against our fine-tuned models (RobCol). The results demonstrate that fine-tuning with our rephrased training data significantly enhances rephrasing robustness for both ColPali and ColQwen.}
            \label{fig:rephrasing_table_plot}
        \end{minipage}
    \end{minipage}
    \vspace{-0.4cm}
\end{figure*}

\subsection{Rephrasing Robustness Training}  
Our benchmark reveals that current models struggle with rephrasing, suggesting that training and evaluation queries often closely match the phrasing of their retrieved pages. To address this, we augmented the ColPali training set by rephrasing half of its queries, randomly selecting one of three rephrasing levels. This was done using 
LLaMA-3-70B\footnote{\url{https://huggingface.co/meta-llama/Llama-3-70B-Instruct}}
% LLaMA-3-70B
, a different LLM than the one used for the benchmark. 
This dataset forces models to learn semantics rather than relying on keyword matching. We fine-tuned ColPali-v1.2 and 
ColQwen2-v1.0\footnote{\url{https://huggingface.co/vidore/colqwen2-v1.0}}
% ColQwen2-v1.0
(using the ColPali code) for one epoch, producing the RobCol models.  




\noindent
As demonstrated in \cref{fig:rephrasing_table_plot,Table:average_rephrasing_benchmarks} (and in \cref{Table:model_comparisons_on_benchmarks,Table:rephrasing_benchmarks}),
RobCol significantly outperforms baselines on rephrased queries while maintaining comparable performance on non-rephrased cases, achieving an average NDCG@5 improvement of 11.1 at rephrasing level 3. 
The gains are stronger on financial benchmarks, where the task is more complex, suggesting that enhanced semantic understanding through rephrasing robustness is particularly beneficial. In \cref{Table:Training_Set_ablation}, we show that fine-tuning on the original data (rather than the rephrased version) provides much lower improvement for ColPali and leads to a decrease in performance for ColQwen.

\noindent
Additionally, we created a rephrased version of the tabled-focused dataset and fine-tuned both models on it, along with the rephrased ColPali training set, producing RobTabCol models. RobTabCol consistently outperforms all models across nearly all benchmarks and rephrasing levels, achieving a 25–30 NDCG@5 improvement over base models on rephrased finance benchmarks and 6–9 on non-financial ones.






