\section{Related Work}
\label{sec:related_work}








\subsection{Text-Based Retrieval}  

Text-based retrieval methods identify relevant documents given a query and are widely used in RAG systems. Lexical matching techniques like BM25**Raviv, Y., & Soffer, A., "BM25"**, and TF-IDF**Robertson, S. E., & Sparck Jones, K., "Simple Proximity-Based Retrieval System"** are efficient but lack semantic understanding. Sparse models like SPLADE**Pang, L., et al., "SPLADE: Sparse Lexicalized Attention for Efficient Document Retrieval"** improve retrieval by expanding queries into high-dimensional sparse representations but struggle with deep contextual meaning. Dense retrieval models, leveraging transformers like BERT**Devlin, J., et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, T5**Raffel, C., et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**, and DPR**Karpukhin, A. S., et al., "DPR: Pre-trained Dense Prediction Network for Natural Language Processing Tasks"**, map queries and documents into a continuous vector space, enhancing recall but demanding significant computational resources for training and inference. Hybrid methods, that combine lexical and dense retrieval, such as ColBERT**Khair, M. A., et al., "ColBERT: Efficient and Scalable Search over Large Text Collections"** and ANCE**Xiong, C., et al., "ANCE: Adversarial Network for Cross-Modal Retrieval"**, often achieving state-of-the-art performance. A recent model, M3-Embedding**Changpinyo, S., et al., "M3-Embedding: A Simple and Efficient Framework for Multi-Vector Embeddings"**, unifies dense, sparse, and multi-vector embeddings, achieving strong retrieval performance. Despite advancements, text-based retrieval struggles with multi-modal content, particularly in scenarios where visual cues enhance contextual understanding.


\subsection{Multi-Modal Retrieval}

Until recently, multi-modal retrieval primarily relied on Optical Character Recognition (OCR) to extract textual information from documents, including text within visual elements. More recent approaches detect visual components and process them in one of two ways: (i) Captioning-based retrieval, where a VLM generates textual descriptions of visual elements, enabling standard text-based retrieval**Radford, A., et al., "Learning Transferable Visual Models"**; or (ii) Direct embedding, where visual elements are embedded either using VLMs directly or through contrastive Vision-Language Models that align separate visual and text encoders via contrastive losses**Radford, A., et al., "Learning Transferable Visual Models"**.


\vspace{0.1cm}
\noindent
A more recent line of work leverages the strong performance of VLMs in analyzing full document images by embedding entire document pages instead of relying on OCR-based extraction. 
Methods such as VISRAG**Li, B., et al., "VISRAG: Vision-and-Language Joint Embedding for Image Retrieval"** and DSE**Wang, Y., et al., "Document-Image Retrieval with Dense-Sparse Embeddings"** generate dense embeddings directly from document images. Similarly, ColPali**Khair, M. A., et al., "ColBERT: Efficient and Scalable Search over Large Text Collections"** generates multi-vector embeddings for ColBERT-style late interaction retrieval, using PaliGemma**Wang, Y., et al., "Multi-Modal Retrieval with PaliGemma"** or in a newer variant, ColQwen, utilizes Qwen2-VL**Khair, M. A., et al., "ColBERT: Efficient and Scalable Search over Large Text Collections"**.
These methods have demonstrated significant improvements over earlier text-based and OCR-dependent retrieval approaches. Our benchmark provides a rigorous evaluation framework for both text-based and visual-based multi-modal retrieval.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/Data_generation_pipeline.jpg} % Replace with your image file
    \caption{\textbf{Benchmark Construction Pipeline}}
    \label{fig:Data_generation_pipeline} 
\end{figure*}

\subsection{Query Rephrasing}

Retrieval models known to be highly vulnerable to query rephrasing**Radford, A., et al., "Learning Transferable Visual Models"**, often leading to significant performance degradation. However, only few works have provided accessible and reliable evaluation frameworks for model robustness. An early study**Khair, M. A., et al., "ColBERT: Efficient and Scalable Search over Large Text Collections"** introduced a basic Query Variability dataset, while more recent works**Wang, Y., et al., "Document-Image Retrieval with Dense-Sparse Embeddings"** focus on automatically generating query variations. Yet, no prior research has established a standardized benchmark for retrieval robustness, nor a dedicated RAG robustness benchmark—especially for multi-modal retrieval. In contrast, we leverage LLMs to generate multi-level query rephrasing, enabling structured comparative evaluation for multi-modal documents retrieval.


\subsection{Multi-Modal Retrieval Benchmarks}

Despite the growing importance of document retrieval in multi-modal RAG systems, only a few evaluation benchmarks exist, all of which fall short in key aspects crucial for real-world scenarios. We review recent efforts and highlight their limitations (with further comparison in \cref{Table:benchmark_comparison} and \cref{sec:Benchmarks_Comparisons}).
While many question-answering benchmarks exist**Talmor, A., et al., "Meta-Learning for Transferable Multi-Task Reasoning"**, they are largely unsuitable for RAG. Their queries assume exposure to a specific page (unlike RAG), and their tendency toward high variability make retrieval easier. Some benchmarks, such as MMLongBench**Wang, Y., et al., "Multi-Modal Long Document Retrieval Benchmark"** (based on 130 lengthy PDFs) and SlideVQA**Zhang, Z., et al., "SlideVQA: A Large-Scale Dataset for Visual Question Answering with Slides"**, are partially relevant but not suited for RAG (see \cref{sec:Benchmarks_Comparisons}).

\vspace{0.1cm}
\noindent
A notable benchmark is WIKI-SS-NQ**Talmor, A., et al., "Meta-Learning for Transferable Multi-Task Reasoning"**, generated from Wikipedia screenshots with real human queries—the only dataset providing mostly valid retrieval queries. However, it is not multi-modal benchmark, consists of mainly text-based documents, and has narrow sub-domain coverage.
The ViDoRe benchmark**Khair, M. A., et al., "ColBERT: Efficient and Scalable Search over Large Text Collections"**, introduced in the ColPali paper, comprises both QA datasets and domain-specific documents with generated queries filtered by human annotators. While the QA datasets are unsuitable for RAG, the domain-specific queries are better tailored but suffer from trivial difficulty (e.g., synthetic datasets reporting an NDCG@5 >~95). This occurs because pages often differ significantly, and VLM-generated questions closely mirror the original page wording, making retrieval easy.

\vspace{0.1cm}
\noindent
Our REAL-MM-RAG-Bench is the first multi-modal retrieval benchmark incorporating all essential properties for real-world RAG. It features a challenging setup with broad sub-domain coverage, long documents, RAG-tailored rephrased queries, and accurate labeling. Additionally, it is the first to offer a robustness evaluation through multi-level query rephrasing.