\section{Related Work}
\label{sec:related_work}





\subsection{Text-Based Retrieval}  

Text-based retrieval methods identify relevant documents given a query and are widely used in RAG systems. Lexical matching techniques like BM25~\cite{robertson1994okapi} and TF-IDF~\cite{sparck1972statistical} are efficient but lack semantic understanding. Sparse models like SPLADE~\cite{formal2021splade} improve retrieval by expanding queries into high-dimensional sparse representations but struggle with deep contextual meaning. Dense retrieval models, leveraging transformers like BERT~\cite{devlin2018bert}, T5~\cite{raffel2020exploring}, and DPR~\cite{karpukhin2020dense}, map queries and documents into a continuous vector space, enhancing recall but demanding significant computational resources for training and inference. Hybrid methods, that combine lexical and dense retrieval, such as ColBERT~\cite{khattab2020colbert} and ANCE~\cite{xiong2020approximate}, often achieving state-of-the-art performance. A recent model, M3-Embedding~\cite{chen2024bge}, unifies dense, sparse, and multi-vector embeddings, achieving strong retrieval performance. Despite advancements, text-based retrieval struggles with multi-modal content, particularly in scenarios where visual cues enhance contextual understanding.


\subsection{Multi-Modal Retrieval}

Until recently, multi-modal retrieval primarily relied on Optical Character Recognition (OCR) to extract textual information from documents, including text within visual elements. More recent approaches detect visual components and process them in one of two ways: (i) Captioning-based retrieval, where a VLM generates textual descriptions of visual elements, enabling standard text-based retrieval~\citep{ramos2023retrieval}; or (ii) Direct embedding, where visual elements are embedded either using VLMs directly or through contrastive Vision-Language Models that align separate visual and text encoders via contrastive losses~\citep{radford2021learning,zhai2023sigmoid}.


\vspace{0.1cm}
\noindent
A more recent line of work leverages the strong performance of VLMs in analyzing full document images by embedding entire document pages instead of relying on OCR-based extraction. 
Methods such as VISRAG~\citep{yu2024visrag} and DSE~\citep{ma2024unifying} generate dense embeddings directly from document images. Similarly, ColPali~\citep{faysse2024colpali} generates multi-vector embeddings for ColBERT-style late interaction retrieval, using PaliGemma~\citep{beyer2024paligemma} or in a newer variant, ColQwen, utilizes Qwen2-VL~\citep{Qwen2VL}.
These methods have demonstrated significant improvements over earlier text-based and OCR-dependent retrieval approaches. Our benchmark provides a rigorous evaluation framework for both text-based and visual-based multi-modal retrieval.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/Data_generation_pipeline.jpg} % Replace with your image file
    \caption{\textbf{Benchmark Construction Pipeline}}
    \label{fig:Data_generation_pipeline} 
\end{figure*}

\subsection{Query Rephrasing}

Retrieval models known to be highly vulnerable to query rephrasing~\citep{zuccon2016query,bailey2017retrieval,sidiropoulos2022analysing,penha2022evaluating,hagen2024revisiting}, often leading to significant performance degradation. However, only few works have provided accessible and reliable evaluation frameworks for model robustness. An early study~\citep{bailey2016uqv100} introduced a basic Query Variability dataset, while more recent works~\citep{benham2018towards,lu2019relevance,penha2022evaluating} focus on automatically generating query variations. Yet, no prior research has established a standardized benchmark for retrieval robustness, nor a dedicated RAG robustness benchmark—especially for multi-modal retrieval. In contrast, we leverage LLMs to generate multi-level query rephrasing, enabling structured comparative evaluation for multi-modal documents retrieval.


\subsection{Multi-Modal Retrieval Benchmarks}

Despite the growing importance of document retrieval in multi-modal RAG systems, only a few evaluation benchmarks exist, all of which fall short in key aspects crucial for real-world scenarios. We review recent efforts and highlight their limitations (with further comparison in \cref{Table:benchmark_comparison} and \cref{sec:Benchmarks_Comparisons}).
While many question-answering benchmarks exist~\citep{mathew2021docvqa,zhu2022towards,masry2022chartqa,islam2023financebench,ding2024mvqa}, they are largely unsuitable for RAG. Their queries assume exposure to a specific page (unlike RAG), and their tendency toward high variability make retrieval easier. Some benchmarks, such as MMLongBench~\citep{ma2024mmlongbench} (based on 130 lengthy PDFs) and SlideVQA~\citep{tanaka2023slidevqa}, are partially relevant but not suited for RAG (see \cref{sec:Benchmarks_Comparisons}).

\vspace{0.1cm}
\noindent
A notable benchmark is WIKI-SS-NQ~\citep{ma2024unifying}, generated from Wikipedia screenshots with real human queries—the only dataset providing mostly valid retrieval queries. However, it is not multi-modal benchmark, consists of mainly text-based documents, and has narrow sub-domain coverage.
The ViDoRe benchmark~\citep{faysse2024colpali}, introduced in the ColPali paper, comprises both QA datasets and domain-specific documents with generated queries filtered by human annotators. While the QA datasets are unsuitable for RAG, the domain-specific queries are better tailored but suffer from trivial difficulty (e.g., synthetic datasets reporting an NDCG@5 >~95). This occurs because pages often differ significantly, and VLM-generated questions closely mirror the original page wording, making retrieval easy.

\vspace{0.1cm}
\noindent
Our REAL-MM-RAG-Bench is the first multi-modal retrieval benchmark incorporating all essential properties for real-world RAG. It features a challenging setup with broad sub-domain coverage, long documents, RAG-tailored rephrased queries, and accurate labeling. Additionally, it is the first to offer a robustness evaluation through multi-level query rephrasing.