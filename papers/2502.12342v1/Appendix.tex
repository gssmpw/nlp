\clearpage
\section{Appendix}
\label{sec:appendix}

\renewcommand{\thefigure}{S\arabic{figure}} % Figures as S1, S2, ...
\renewcommand{\thetable}{S\arabic{table}}   % Tables as S1, S2, ...
\setcounter{figure}{0}  % Reset numbering for appendix
\setcounter{table}{0}

\subsection{Models Evaluation} \label{sec:appendix_model_evalaution}
Beside the models from ColPali, we have evaluated on text based methods.
Following the ColPali framework, we adopt \emph{Unstructured} as our PDF parser in its high-resolution configuration, which relies on the Tesseract \cite{smith2007overview} OCR engine.
Unstructured processes each document into two main types of chunks: text chunks and visual chunks (e.g., tables, figures, images).
We then construct two text-based variants of our benchmark, differing in how visual chunks are converted into text:
\begin{enumerate}[leftmargin=*]
    \item \textbf{OCR}: Text is retained; tables, figures, and images undergo OCR extraction.
    \item \textbf{Captioning}: Text remains unchanged; visual elements are described using Qwen2.5-VL-72B-Instruct.
\end{enumerate}

\paragraph{Retrieval Methods.} 

We evaluate two retrieval approaches:  
\begin{itemize}[leftmargin=*]
    \item \textbf{Okapi BM25}: A sparse statistical baseline.
    \item \textbf{BGE-M3 (multi-vector)}: A state-of-the-art embedding model.
\end{itemize}


In line with ColPali, chunks are embedded and scored independently, and page-level scores are then obtained via maximum pooling across all chunks for a given page.

\subsection{Proposed Training sets and Fine-tuning} \label{sec:training_Sets_and_finetune_detailes}

\paragraph{Rephrasing Augmentation Training.}
We aimed to fine-tune a trained model with a short training phase to improve robustness to rephrasing. To achieve this, we created a rephrased training set based on the ColPali training data. Specifically, we used approximately half of the full training set (56k queries) and generated rephrased versions. The rephrasing was performed using LLaMA-3-70B, a different LLM than the one used for filtering and rephrasing in the benchmark.

Each query was randomly rephrased using one of three rephrasing levels (different prompts, see \cref{fig:rephrasing_prompts}). To ensure semantic consistency, we used a secondary LLM verification step: the original query and its corresponding answer were fed alongside the rephrased query, and if the meaning was not preserved, the original query was retained in the rephrased training set.

For fine-tuning, we used the full ColPali training set, incorporating the rephrased queries in approximately half of it. The model was trained for one epoch with configurations similar to those used in the original ColPali/ColQwen training.

\paragraph{Table and Finance-Focused Training.}
As we observed that current models performed significantly worse on our table-heavy finance benchmark, we curated a dedicated training set to address this gap. We leveraged the publicly available FinTabNet dataset, a large-scale resource designed for financial table recognition and structure extraction. FinTabNet consists of pages from annual reports of S\&P 500 companies, featuring complex tables.

We used the page images from FinTabNet to generate queries and answers using our automated pipeline, which includes a filtering process. Additionally, we explored using Qwen2-VL-72B for generating this training data, with results reported in \cref{Table:VLM_ablation}. This process resulted in approximately 46k triplets of page images, queries, and answers.

For fine-tuning, we trained both ColPali and ColQwen on our table-focused training data, combined with the original ColPali training set, for one epoch, producing the TabCol models. For RobTab models, we incorporated LLaMA-3-70B for rephrasing, where half of the newly generated training set was randomly rephrased using three different rephrasing levels. These models were then fine-tuned for one epoch, together with the ColPali rephrased dataset.

All fine-tuning procedures followed the same configurations as the official ColPali training pipeline, utilizing ColBERT in-batch loss. The specific configurations for ColPali and ColQwen can be found at \href{https://github.com/illuin-tech/colpali}{GitHub}.  Each model was fine-tuned starting from its respective base version: ColPali from ColPali-1.2 and ColQwen from ColQwen2-1.0. We used a batch size of 64 per GPU, resulting in a total effective batch size of 256. The training maintained the same initial learning rate, warmup steps, learning rate schedule, and LoRA configuration as the original pipeline.  All training runs were conducted on four A100 80GB GPUs, with each fine-tuning session taking approximately three hours to complete.


\subsection{Benchmark Document Examples}  
Our benchmarks include a diverse set of pages containing different types of information, including full-text pages, table-heavy pages, and slides with both tables and other visual elements. \Cref{Table:benchmark_statistics} presents the benchmark statistics for the different datasets.   Additionally, for each page, we provide four types of queries: the original VLM-generated query and three levels of rephrasing. Examples of pages along with all query versions are shown in \cref{fig:ours_examples_1,fig:ours_examples_2}. In \cref{fig:others_examples}, we present examples from previous benchmarks, highlighting that many of their queries are not well-suited for RAG, as they often reference specific pages (i.e., QA-style queries) rather than general information-seeking queries.

\subsection{Additional Results and Ablations} \label{sec:additional_results}
In our main paper, we focused on reporting the NDCG@5 metric for a subset of models and benchmarks across different rephrasing levels. In \cref{Table:rephrasing_benchmarks,Table:rephrasing_benchmarks_recall1,Table:rephrasing_benchmarks_recall5}, we provide the full results for Recall@1, Recall@5, and NDCG@5.  Additionally, \cref{Table:model_comparisons_on_labels} presents model performance across different evidence source types. A Vision-Language Model (VLM), Pixtral-12B, was used to classify each query based on the type of evidence source from which the answer was retrieved on the corresponding page. We analyze performance across three different evidence types: Text, Table, and Visual. The reported results show NDCG@5 scores on the non-rephrased version, averaged across our four benchmarks. These results highlight a significant weakness in handling tables. However, after fine-tuning on the table-focused dataset, we observe improvements across all evidence types, with tables showing the most substantial gains.

We further provide two ablation experiments. The first, shown in \cref{Table:Training_Set_ablation}, aims to demonstrate that the observed improvements after fine-tuning with our proposed datasets are due to the tailored data rather than fine-tuning itself. To verify this, we performed an additional fine-tuning run using the same training set as the RobCol models but without rephrasing, maintaining the exact number of training examples and identical training configurations.  

As seen in the results, ColPali gains some improvement from fine-tuning alone, but the gains are significantly lower compared to fine-tuning with our rephrased training set. This gap is even more pronounced when evaluating on the rephrased benchmark (Level 3). For ColQwen, the baseline fine-tuning without rephrasing leads to a decrease in performance, whereas our fine-tuned models show substantial improvements on rephrased queries, as expected when training with our rephrased dataset.  

The second ablation (see \cref{Table:VLM_ablation}) aims to show that the improvements from the table-heavy training set are general and not specific to the Vision-Language Model (VLM) used for question generation. To test this, we generated an alternative version of the training set using a different VLM, Qwen2-VL-72B-Instruct, and trained models both with and without rephrasing (using LLaMA-3-3-70B). After filtering, the dataset size was slightly smaller—40k examples compared to 46k with Pixtral generation—likely due to Qwen generating more queries that were filtered out.  

While results show a slight decrease in performance compared to using Pixtral-generated data, the fine-tuned models still significantly outperform the base ColPali and ColQwen models. This confirms that the effectiveness of our data is not limited to a specific VLM and that training on a table-heavy dataset remains highly beneficial.


\subsection{Licensing and Additional General Information}  

All models and datasets used in this work comply with their respective licenses. Qwen2-VL (ColQwen2) is licensed under Apache 2.0, with adapters under MIT. PaliGemma (ColPali) follows the Gemma license, with adapters under MIT. Pixtral-12B-2409 (mistralai) and Mixtral-8x22B are both under Apache 2.0, allowing unrestricted use, modification, and distribution. LLaMA 3.3 70B is licensed under the LLaMA 3.3 Community License Agreement. All datasets used are in English. The Colapli training set consists of subsampled academic datasets redistributed under their original licenses. It also includes synthetic datasets generated from publicly available internet data and VLM-generated queries, which are released without usage restrictions. Benchmark datasets are derived from openly available documents and images, with owner approval for publication. Fine-tuning data (Fintabnet) is collected from publicly available sources and processed for compatibility with our models.  

For human evaluation, we published an online form alongside a request for participation in annotating queries for evaluating data intended for publication.
Throughout the paper, an AI assistant (ChatGPT) was used for minor grammar and sentence structure edits. 


\begin{table*}[t]
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height
\setlength\tabcolsep{5pt} % Adjust column spacing
\caption{
\textbf{Benchmark Statistics.}
}
\vspace{-0.15cm}
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l|ccc c ccc} 
\toprule
\multirow{2}{*}{\textbf{Benchmark}} & \multicolumn{3}{c}{\textbf{Documents}} & \multicolumn{1}{c}{\textbf{Queries}} & \multicolumn{3}{c}{\textbf{Evidence Label}} \\
& \textbf{\# Pages} & \textbf{\# Docs} & \textbf{Avg. Len} & \textbf{\# Queries} &  \textbf{Text} & \textbf{Table} & \textbf{Visual} \\
\midrule
\textbf{FinReport}           & 2687 & 19 & 141  & 853  & 75\% & 24\% & 1\% \\
\textbf{FinSlides}            & 2280 & 65 & 35   & 1052  & 12\% & 83\% & 5\% \\
\textbf{TechReport}       & 1674 & 17 & 98   & 1294  & 81\% & 12\% & 7\% \\
\textbf{TechSlides}      & 1963 & 62  & 32 &  1354 &  66\% & 6\%  & 28\% \\
\bottomrule
\end{tabular*}
\label{Table:benchmark_statistics}
% \vspace{-0.4cm}
\end{table*}



% \hspace{-20cm}
\begin{figure*}[h!]
    \includegraphics[width=1.1\textwidth]{figures/data_rephrased.png} % Replace with your image file
    \caption{\textbf{Real-MM-RAG Benchmark Examples with Rephrasing.}  
    On the left: FinSlides—financial quarterly presentations.  
    On the right: TechSlides Technical slides about business and IT automation. 
    Questions are listed from the original query to Level 3 rephrasing.}
    \label{fig:ours_examples_1} 
\end{figure*}


\hspace{-10cm}
\begin{figure*}[h!]
    \includegraphics[width=1.1\textwidth]{figures/finance_rephrase.png} % Replace with your image file
    \caption{\textbf{Real-MM-RAG Benchmark Examples with Rephrasing.}  
    Left: FinReport—financial annual reports.  
    Right: TechReport—FlashSystem technical reports.  
    Queries are listed from the original to Level 3 rephrasing.}
    \label{fig:ours_examples_2} 
\end{figure*}


\hspace{-10cm}
\begin{figure*}[h!]
    \includegraphics[width=1.1\textwidth]{figures/other_benchmarks.png} % Replace with your image file
    \caption{\textbf{Examples from Previous Benchmarks.} These examples illustrate common query types in these benchmarks. Many queries are generated for question answering and refer to a specific page rather than resembling real user queries, which are typically asked without prior knowledge of a specific page.}

    \label{fig:others_examples} 
\end{figure*}

\clearpage
\begin{table*}[th]
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height
\setlength\tabcolsep{4pt} % Adjust column spacing
\caption{
\textbf{Impact of Rephrasing Levels on Document Retrieval Benchmarks.}
This table shows NDCG@5 performance variations across rephrasing levels (0-3) for different benchmarks and models.}
% \vspace{-0.15cm}
\begin{tabular*}{1.03\linewidth}{@{\extracolsep{\fill}}l|cccc|cccc|cccc|cccc}
\toprule
 & \multicolumn{4}{c|}{\textbf{FinReport}} & \multicolumn{4}{c|}{\textbf{FinSlides}} & \multicolumn{4}{c|}{\textbf{TechReport}} & \multicolumn{4}{c}{\textbf{TechSlides}} \\
\textbf{Rephrasing Level} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} \\
\midrule

\textit{BM25 (OCR)} & 
48.8 & 38.4 & 26.6 & 21.7 &
13.6 & 13.0 & 7.1 & 5.9 &
66.6 & 48.0 & 38.7 & 35.1 &
58.7 & 45.7 & 35.7 & 31.2 \\

\textit{BGE-M3 (OCR)} & 
47.2 & 40.3 & 37.6 & 36.5 &
16.9 & 16.3 & 14.6 & 11.4 &
44.7 & 40.9 & 37.8 & 37.0 &
60.5 & 56.3 & 51.9 & 49.6  \\

\textit{BM25 (Captioning)} & 
54.4 & 43.0 & 30.6 & 25.3 &
20.9 & 19.0 & 11.0 & 9.9 &
69.0 & 50.8 & 41.8 & 37.2 &
66.4 & 53.4 & 41.7 & 36.1 \\

\textit{BGE-M3 (Captioning)} & 
46.4 & 39.0 & 36.9 & 35.9 &
19.5 & 18.7 & 16.3 & 13.8 &
44.6 & 40.9 & 38.4 & 37.5 &
62.6 & 58.3 & 54.5 & 51.7 \\
\addlinespace
\addlinespace
\textit{ColPali} & 
52.7 & 47.2 & 40.8 & 36.8 &  
62.2 & 59.4 & 37.6 & 27.6 & 
80.6 & 72.9 & 66.5 & 62.0 & 
89.7 & 85.0 & 79.2 & 75.8 \\

\hspace{0.3cm}\textit{RobColPali} & 
59.3 & 57.4 & 51.4 & 47.1 & 
76.8 & 75.1 & 58.3 & 48.4 & 
80.1 & 74.7 & 70.0 & 66.6 & 
90.5 & 88.2 & 84.2 & 82.8 \\

\hspace{0.3cm}\textit{TabColPali} & 
70.5 & 63.5 & 56.4 & 50.5 & 
74.5 & 70.7 & 54.2 & 41.5 & 
82.7 & 73.8 & 66.8 & 61.3 & 
90.8 & 86.5 & 80.4 & 77.6 \\

\hspace{0.3cm}\textit{RobTabColPali} & 
71.0 & 68.5 & \textbf{65.0} & \textbf{63.2} & 
\underline{\textbf{80.9}} & \underline{\textbf{79.5}} & \textbf{68.0} & \textbf{58.3} & 
80.8 & \textbf{76.2} & \textbf{72.6} & \textbf{70.7} & 
90.5 & 87.1 & \textbf{84.3} & \textbf{83.3} \\
\addlinespace
\textit{ColQwen} & 
60.8 & 54.5 & 46.7 & 41.8 & 
59.3 & 54.8 & 39.1 & 31.1 & 
\underline{\textbf{84.2}} & 74.9 & 71.8 & 66.9 & 
91.3 & 85.9 & 80.6 & 78.1 \\

\hspace{0.3cm}\textit{RobColQwen} & 
58.4 & 54.5 & 49.7 & 47.5 & 
65.8 & 63.0 & 52.0 & 44.3 & 
81.9 & 75.4 & 71.8 & 69.5 & 
90.1 & \textbf{87.8} & 84.0 & 83.0 \\

\hspace{0.3cm}\textit{TabColQwen} & 
\textbf{78.2} & \textbf{69.0} & 61.5 & 54.0 & 
77.1 & 73.9 & 58.1 & 49.6 & 
83.6 & 75.1 & 70.8 & 65.9 & 
\textbf{92.4} & 87.7 & 82.5 & 78.9 \\

\hspace{0.3cm}\textit{RobTabColQwen} & 
\textbf{\underline{79.7}} & \textbf{\underline{74.8}} & \textbf{\underline{69.4}} & \textbf{\underline{67.1}} & 
\textbf{79.6} & \textbf{78.5} & \textbf{\underline{69.1}} & \textbf{\underline{61.6}} & 
\textbf{83.7} & \textbf{\underline{79.3}} & \textbf{\underline{75.5}} & \textbf{\underline{73.2}} & 
\textbf{\underline{92.5}} & \textbf{\underline{89.9}} & \textbf{\underline{86.3}} & \textbf{\underline{85.0}} \\

\bottomrule
\end{tabular*}
\label{Table:rephrasing_benchmarks}
% \vspace{-0.4cm}
\end{table*}


\begin{table*}[t]
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height
\setlength\tabcolsep{4pt} % Adjust column spacing
\caption{
\textbf{Impact of Rephrasing Levels on Document Retrieval Benchmarks (Recall@1).}
This table shows Recall@1 performance variations across rephrasing levels (0-3) for different benchmarks and models.}
% \vspace{-0.15cm}
\begin{tabular*}{1.03\linewidth}{@{\extracolsep{\fill}}l|cccc|cccc|cccc|cccc} 
\toprule
 & \multicolumn{4}{c|}{\textbf{FinReport}} & \multicolumn{4}{c|}{\textbf{FinSlides}} & \multicolumn{4}{c|}{\textbf{TechReport}} & \multicolumn{4}{c}{\textbf{TechSlides}} \\
\textbf{Rephrasing Level} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} \\
\midrule

\textit{BM25 (OCR)} & 
33.9 & 25.4 & 17.1 & 14.0 &
7.7 & 7.5 & 3.7 & 3.0 &
53.4 & 34.1 & 26.1 & 23.1 &
45.9 & 33.4 & 25.4 & 21.3 \\

\textit{BGE-M3 (OCR)} & 
34.0 & 28.6 & 25.8 & 25.0 &
11.0 & 9.5 & 9.3 & 7.2 &
34.4 & 31.2 & 28.6 & 27.8 &
48.6 & 44.2 & 40.0 & 37.7  \\

\textit{BM25 (Captioning)} & 
40.1 & 28.0 & 19.8 & 17.5 &
12.2 & 10.9 & 6.2 & 5.7 &
56.2 & 36.8 & 28.8 & 24.6 &
54.0 & 40.8 & 31.0 & 25.2 \\

\textit{BGE-M3 (Captioning)} & 
33.4 & 27.7 & 25.1 & 25.3 &
12.5 & 11.5 & 9.6 & 8.9 &
34.7 & 31.1 & 28.8 & 27.7 &
50.9 & 45.8 & 41.8 & 40.1 \\
\addlinespace
\textit{ColPali} & 
40.3 & 35.4 & 29.1 & 25.9 &  
45.6 & 41.6 & 23.3 & 15.5 & 
68.1 & 57.6 & 51.5 & 45.8 & 
82.4 & 75.3 & 68.4 & 63.5 \\

\hspace{0.3cm}\textit{RobColPali} & 
44.4 & 42.1 & 36.5 & 32.0 & 
60.4 & 57.4 & 39.8 & 30.4 & 
68.0 & 60.2 & 53.8 & 51.2 & 
82.8 & 79.7 & 74.1 & 73.1 \\

\hspace{0.3cm}\textit{TabColPali} & 
55.1 & 50.3 & 40.9 & 36.0 & 
55.9 & 52.0 & 35.0 & 22.8 & 
70.3 & 58.8 & 50.3 & 44.8 & 
83.9 & 77.8 & 69.4 & 66.5 \\

\hspace{0.3cm}\textit{RobTabColPali} & 
56.5 & 53.9 & 49.4 & 48.4 & 
64.0 & 61.7 & 48.4 & 35.8 & 
67.9 & 61.2 & 57.4 & 54.3 & 
83.4 & 78.4 & 75.2 & 73.1 \\
\addlinespace
\textit{ColQwen} & 
44.0 & 39.6 & 32.6 & 28.5 & 
44.7 & 40.8 & 26.7 & 18.7 & 
73.0 & 60.1 & 56.5 & 51.4 & 
84.2 & 77.4 & 70.3 & 66.5 \\

\hspace{0.3cm}\textit{RobColQwen} & 
41.4 & 37.7 & 34.8 & 33.1 & 
49.1 & 46.6 & 37.2 & 29.3 & 
68.5 & 60.4 & 56.1 & 54.2 & 
83.3 & 79.5 & 74.6 & 72.1 \\

\hspace{0.3cm}\textit{TabColQwen} & 
62.7 & 53.2 & 44.9 & 37.5 & 
58.7 & 55.9 & 41.2 & 33.1 & 
71.0 & 59.8 & 54.9 & 49.7 & 
85.9 & 78.6 & 71.1 & 66.9 \\

\hspace{0.3cm}\textit{RobTabColQwen} & 
58.1 & 52.5 & 50.2 & 45.8 & 
62.2 & 60.7 & 51.4 & 41.8 & 
70.3 & 62.2 & 59.0 & 56.3 & 
85.0 & 80.9 & 76.0 & 74.9 \\

\bottomrule
\end{tabular*}
\label{Table:rephrasing_benchmarks_recall1}
% \vspace{-0.4cm}
\end{table*}



\begin{table*}[t]
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height
\setlength\tabcolsep{4pt} % Adjust column spacing
\caption{
\textbf{Impact of Rephrasing Levels on Document Retrieval Benchmarks (Recall@5).}
This table shows Recall@5 performance variations across rephrasing levels (0-3) for different benchmarks and models.}
% \vspace{-0.15cm}
\begin{tabular*}{1.03\linewidth}{@{\extracolsep{\fill}}l|cccc|cccc|cccc|cccc} 
\toprule
 & \multicolumn{4}{c|}{\textbf{FinReport}} & \multicolumn{4}{c|}{\textbf{FinSlides}} & \multicolumn{4}{c|}{\textbf{TechReport}} & \multicolumn{4}{c}{\textbf{TechSlides}} \\
\textbf{Rephrasing Level} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} \\

\textit{BM25 (OCR)} & 
62.0 & 49.9 & 35.3 & 29.0 &
19.7 & 18.6 & 10.4 & 8.7 &
77.6 & 59.8 & 50.0 & 45.9 &
70.3 & 57.1 & 45.2 & 40.6 \\

\textit{BGE-M3 (OCR)} & 
58.4 & 50.5 & 47.6 & 47.0 &
22.9 & 22.8 & 19.8 & 15.6 &
53.5 & 49.2 & 45.7 & 45.2 &
70.6 & 66.5 & 62.3 & 60.2  \\

\textit{BM25 (Captioning)} & 
66.9 & 56.3 & 40.4 & 32.5 &
29.4 & 27.2 & 15.5 & 13.8 &
79.7 & 63.5 & 53.5 & 49.0 &
77.2 & 64.8 & 51.3 & 46.1 \\

\textit{BGE-M3 (Captioning)} & 
57.0 & 48.8 & 46.7 & 45.5 &
25.9 & 25.1 & 22.7 & 18.3 &
52.9 & 49.3 & 46.6 & 46.2 &
72.6 & 68.8 & 65.7 & 62.0 \\
\addlinespace
% \textit{Visrag} & 
% 43.8 & 39.4 & 35.6 & 34.2 & 
% 39.4 & 39.1 & 28.1 & 24.9 & 
% 75.4 & 68.0 & 64.9 & 61.3 & 
% 92.4 & 87.5 & 83.6 & 81.5 \\
\addlinespace
\textit{ColPali} & 
64.0 & 57.8 & 51.7 & 47.0 &  
76.9 & 74.7 & 50.4 & 38.7 & 
90.7 & 85.7 & 79.4 & 76.1 & 
95.2 & 92.4 & 87.8 & 85.7 \\

\hspace{0.3cm}\textit{RobColPali} & 
73.0 & 71.3 & 64.7 & 60.6 & 
90.6 & 89.8 & 74.1 & 64.4 & 
89.8 & 86.5 & 83.7 & 79.5 & 
96.1 & 94.4 & 91.9 & 90.1 \\

\hspace{0.3cm}\textit{TabColPali} & 
83.8 & 75.5 & 70.1 & 63.3 & 
90.3 & 87.1 & 71.7 & 58.3 & 
92.7 & 86.0 & 80.7 & 75.1 & 
96.0 & 93.2 & 88.9 & 86.7 \\

\hspace{0.3cm}\textit{RobTabColPali} & 
83.1 & 80.8 & 78.0 & 76.0 & 
95.0 & 94.2 & 85.0 & 77.7 & 
91.2 & 88.5 & 85.1 & 84.3 & 
95.6 & 93.7 & 91.2 & 91.3 \\
\addlinespace
\textit{ColQwen} & 
75.3 & 68.0 & 59.6 & 54.0 & 
71.8 & 66.8 & 50.3 & 42.5 & 
93.1 & 86.8 & 84.5 & 79.7 & 
96.4 & 92.3 & 88.9 & 87.4 \\

\hspace{0.3cm}\textit{RobColQwen} & 
73.0 & 69.5 & 63.0 & 60.7 & 
79.8 & 76.5 & 65.6 & 57.8 & 
92.6 & 87.9 & 84.8 & 82.7 & 
95.1 & 94.1 & 91.3 & 91.4 \\

\hspace{0.3cm}\textit{TabColQwen} & 
90.7 & 82.6 & 75.5 & 68.0 & 
92.4 & 88.7 & 72.8 & 64.3 & 
93.3 & 87.2 & 83.8 & 79.6 & 
97.2 & 94.7 & 91.3 & 88.6 \\

\hspace{0.3cm}\textit{RobTabColQwen} & 
86.3 & 80.1 & 76.8 & 74.4 & 
92.5 & 92.1 & 82.0 & 76.2 & 
92.6 & 88.8 & 86.4 & 84.7 & 
96.9 & 95.3 & 93.3 & 92.6 \\

\bottomrule
\end{tabular*}
\label{Table:rephrasing_benchmarks_recall5}
% \vspace{-0.4cm}
\end{table*}


\begin{table*}[t]
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height
\setlength\tabcolsep{5pt} % Adjust column spacing
\caption{
\textbf{Model Performance Across Different Evidence Source Types} 
A Vision-Language Model (VLM) was used to classify each query based on the type of evidence source from which the answer was retrieved on the corresponding page. We present the division of performance across three different source types: Text, Table, and Visual. The reported results are the NDCG@5 scores on the non-rephrased version, averaged across our four benchmarks.}
% \vspace{-0.15cm}
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lccc} 
\toprule
\textbf{Benchmark} & \textbf{Text} & \textbf{Tables} & \textbf{Visual}  \\
\midrule
% \textit{BM25 (Captioning)}    & - & - & -    \\
% \textit{BGE-M3 (Captioning)} & - & - & -    \\
% \textit{Visrag}  & 56.9 & 31.1 & 74.9    \\
\textit{ColPali}  & 75.8 & 58.6 & 84.5    \\
\textit{ColQwen}   & 79.2 & 59.9 & 86.8    \\
\textit{TabColQwen}  & 84.5 & 78.5 & 88.5    \\
\bottomrule
\end{tabular*}
\label{Table:model_comparisons_on_labels}
% \vspace{-0.4cm}
\end{table*}



\begin{table*}[t]
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height
\setlength\tabcolsep{4pt} % Adjust column spacing
\caption{
\textbf{Ablation of Fine-Tuning Without Rephrasing.} To demonstrate that the performance improvement is not solely due to fine-tuning, we fine-tune the models on the original ColPali dataset without rephrasing, using the exact same fine-tuning configuration.}
% \vspace{-0.15cm}
\centering
\begin{tabular*}{1.03\linewidth}{@{\extracolsep{\fill}}l cc cc cc cc} 
\toprule
 & \multicolumn{2}{c}{\textbf{FinReport}} & \multicolumn{2}{c}{\textbf{FinSlides}} & \multicolumn{2}{c}{\textbf{TechReport}} & \multicolumn{2}{c}{\textbf{TechSlides}} \\
\textbf{Rephrasing Level} & \textbf{0} & \textbf{3} & \textbf{0} & \textbf{3} & \textbf{0} & \textbf{3} & \textbf{0} & \textbf{3} \\

\midrule
ColPali & 
52.7 & 34.5 &  
62.2 & 27.6 & 
80.6 & 62.0 & 
89.7 & 75.8 \\
\hspace{0.3cm}\textit{ColPali Baseline FT} & 
57.1 & 39.1 & 
69.1 & 35.7 & 
80.0 & 61.4 & 
90.0 & 77.8 \\
\hspace{0.3cm}\textit{RobColPali} & 
59.3 & 47.1 & 
76.8 & 48.4 & 
80.1 & 66.6 & 
90.5 & 83.3 \\
\textit{ColQwen} & 
60.8 & 41.8 & 
59.3 & 31.1 & 
84.2 & 66.9 & 
91.3 & 78.1 \\
\hspace{0.3cm}\textit{ColQwen Baseline FT} & 
58.1 & 39.3 & 
53.4 & 27.6 & 
79.7 & 62.1 & 
90.5 & 77.4 \\
\hspace{0.3cm}\textit{RobColQwen} & 
58.4 & 47.5 & 
65.8 & 44.3 & 
81.9 & 69.5 & 
90.1 & 83.0 \\

\bottomrule
\end{tabular*}
\label{Table:Training_Set_ablation}
% \vspace{-0.4cm}
\end{table*}


\begin{table*}[t]
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height
\setlength\tabcolsep{4pt} % Adjust column spacing
\caption{
\textbf{Comparison of different queries generation models.}
This table compares the NDCG5 performance of the ColQwen model fine-tuned with the original data of ColPali and our generated table-focused data from the FinTabNet dataset. The evaluation is conducted for two query generation approaches: one using Pixtral and the other using Qwen. The rephrasing for the benchmarks was performed using LLaMA-3-3-70B. Results are presented across rephrasing levels (0 and 3) for our retrieval benchmarks.}
% \vspace{-0.15cm}
\centering
\begin{tabular*}{1.03\linewidth}{@{\extracolsep{\fill}}l cc cc cc cc} 
\toprule
 & \multicolumn{2}{c}{\textbf{FinReport}} & \multicolumn{2}{c}{\textbf{FinSlides}} & \multicolumn{2}{c}{\textbf{TechReport}} & \multicolumn{2}{c}{\textbf{TechSlides}} \\
\textbf{Rephrasing Level} & \textbf{0} & \textbf{3} & \textbf{0} & \textbf{3} & \textbf{0} & \textbf{3} & \textbf{0} & \textbf{3} \\

\midrule
ColPali & 
52.7 & 34.5 &  
62.2 & 27.6 & 
80.6 & 62.0 & 
89.7 & 75.8 \\
ColQwen & 
60.8 & 41.8 & 
59.3 & 31.1 & 
84.2 & 66.9 & 
91.3 & 78.1 \\
\midrule
\emph{ColTab (Pixtral Queries Gen.)} & 
78.2 & 54.0 & 
77.1 & 49.6 & 
83.6 & 65.9 & 
92.4 & 78.9 \\

\emph{ColTab (Qwen Queries Gen.)} & 
74.8 & 49.5 & 
74.3 & 41.5 & 
83.8 & 66.8 & 
92.6 & 79.5 \\

\midrule
\emph{\textbf{ColRobTab (Pixtral Queries Gen.)}} & 
79.7 & 67.1 & 
79.6 & 61.6 & 
83.7 & 73.2 & 
92.5 & 85.0 \\

\emph{\textbf{ColRobTab (Qwen Queries Gen.)}} & 
73.5 & 61.1 & 
78.9 & 60.0 & 
82.8 & 71.8 & 
91.8 & 84.7 \\
\bottomrule
\end{tabular*}
\label{Table:VLM_ablation}
% \vspace{-0.4cm}
\end{table*}



\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Query_Generation_Prompt.jpg} % Replace with your image file
    \caption{\textbf{Query Generation Prompt}}
    \label{fig:query_generation_prompt} 
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Query_Verification_Prompt.jpg}
    \caption{\textbf{Query Verification Prompt}}
    \label{fig:query_verification_prompt} 
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Rephrasing_prompts.jpg}
    \caption{\textbf{Rephrasing Generation and Verification Prompts}}
    \label{fig:rephrasing_prompts} 
\end{figure*}


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Human_query_RAG.jpg}
    \caption{\textbf{Human Evaluation of Query Alignment to RAG.} This figure shows the instructions presented to human annotators along with randomly sampled queries from different benchmarks.}
    \label{fig:Human_query_RAG} 
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Human_False_Negative.jpg} 
    \caption{\textbf{Human Evaluation of False Negatives.} This figure presents an example of the image shown to human annotators, including the instructions, the query, and the negative page retrieved for the given query using ColQwen. The query and page are from our FinSlides benchmark and illustrate a case where the model incorrectly retrieved the wrong year.}

    \label{fig:Human_False_Negative} 
\end{figure*}

















