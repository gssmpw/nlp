\section{Introduction}
\label{sec:intro}
% \vspace{-0.05cm}


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1\textwidth]{figures/First_figure.jpg} 
    \caption{\textbf{Proposed Real-MM-RAG Benchmark}}
    \vspace{-0.2cm}
    \label{fig:Benchmark} 
\end{figure*}

Accurate retrieval of relevant documents is a cornerstone of modern natural language processing (NLP) applications, whether used alone or in advanced pipelines like Retrieval-Augmented Generation (RAG). RAG~\citep{lewis2020retrieval} has emerged as a powerful approach wherein models retrieve external information before generating answers or content, enabling operation over large document collections. \emph{Multi-modal RAG} extends this to real-world scenarios involving text, figures, tables, and potentially entire page images.\\

\vspace{-0.15cm}
\noindent
Successful retrieval is crucial for RAG, as retrieving the wrong page or document inevitably hinders the final generated response. Therefore our analysis will focus on the retrieval part. Although research on RAG is advancing, the field still lacks a complete understanding of how models perform in realistic setups, both for evaluating performance and identifying current weaknesses to overcome. This gap arises from a shortage of benchmarks that thoroughly assess real-world retrieval challenges. \\

\vspace{-0.15cm}
\noindent
We identify four essential properties for a real-world document retrieval benchmark, particularly in multi-modal contexts: \textbf{\emph{(i) Multi-modal documents:}} The dataset should include pages with text, figures, tables, and other visual elements to reflect the complexity of real-world materials.
\textbf{\emph{(ii) Enhanced difficulty:}} Queries should require more than simple keyword matching and involve a large corpus of contextually similar pages to ensure challenging evaluations. \textbf{\emph{(iii) Realistic-RAG queries:}} Questions must be posed naturally, without explicit references to pages—reflecting queries a person might ask when seeking information without knowing the answer's location (in contrast to generic question-answering setups).
% tailored specifically for retrieval tasks rather than generic question-answering setups.
\textbf{\emph{(iv) Accurate labeling:}} All documents relevant to a query must be correctly and exhaustively labeled to prevent underestimation of retrieval performance and to avoid false negatives. \\

\vspace{0.1cm}
% \vspace{-0.15cm}
\noindent
Although a few recent benchmarks touch on some of these aspects~\citep{faysse2024colpali,ma2024unifying,ma2024mmlongbench}, most fail to fully capture them, limiting their usefulness for understanding and improving multi-modal retrieval models. We introduce \emph{\textbf{REAL-MM-RAG-Bench}} \emph{(Real-World Multi-Modal Retrieval-Augmented Generation Benchmark)}, a benchmark designed to satisfy the properties above:

% \vspace{0.02cm}
% \noindent
\emph{\textbf{Multi-modal documents (Property i):}} Our dataset comprises slides and documents with text, figures, tables, and images, requiring systems to handle combined textual and visual data.

% \noindent
\emph{\textbf{Enhanced difficulty (Property ii):}} Instead of relying on isolated pages or trivial queries, we focus on long documents from specialized domains (e.g., IBM finance reports, FlashSystem technical materials). 
This makes retrieval significantly more challenging, as models must differentiate between highly similar content within the same domain. Additionally, we incorporate a rephrasing step to ensure that query wording and order are not identical to the page content, requiring semantic understanding rather than simple keyword matching.

% \noindent
\emph{\textbf{Realistic-RAG queries (Property iii):}} We use a two-step process to generate queries: vision-language models (VLMs) create retrieval-focused queries, and large language models (LLMs) filter them to ensure natural phrasing and realistic user intent. Unlike many existing datasets, our queries avoid direct references to specific pages, reflecting authentic retrieval scenarios. 

% \noindent
\emph{\textbf{Accurate labeling (Property iv):}} Ensuring all pages answering a query are correctly identified is crucial, especially in benchmarks with many similar pages. Existing benchmarks often mislabel valid documents as incorrect, leading to false negatives. To address this, we employ an automated pipeline using VLMs to verify query relevance to each page. While computationally intensive, this approach enhances labeling reliability, particularly for closely related pages.

\vspace{0.12cm}
\noindent
Our benchmark enables reliable evaluation of current retrieval models and uncovering some of their weaknesses. Additionally, it incorporates two essential properties that expose specific retrieval challenges: 

\emph{\textbf{Rephrasing Robustness Evaluation:}} In real-world scenarios, users rarely phrase their queries exactly as they appear in documents. However, both VLMs and human annotators tend to generate queries that closely mirror the source material, often using similar words and sentence structures~\citep{smeaton1998user,zhu2024enhancing}. This fails to reflect natural user behavior, where users are not directly exposed to the document page when forming queries.
To address this, we introduce a multi-level rephrasing benchmark, modifying queries at three distinct levels—ranging from slight rewording to significant structural changes. Our experiments show that current retrieval models struggle to maintain performance across these variations, highlighting a critical weakness in their semantic understanding.

\emph{\textbf{Table-Focused Scenarios:}} Table-heavy documents (e.g. financial reports) often contain dense tabular data, posing a major challenge for retrieval models. By incorporating table-heavy documents into our benchmark, we expose key deficiencies in table comprehension that significantly impact model performance.
These properties allow us to demonstrate that all current retrieval models exhibit weaknesses in handling both rephrased queries and table-heavy financial documents. 

\noindent
To address these shortcomings, we leverage insights from our benchmark to enhance retrieval performance. Specifically, we introduce two targeted training strategies: (i) a \emph{rephrased training dataset}, generated by rephrasing the ColPali training dataset~\citep{faysse2024colpali}, and (ii) a \emph{finance-table-heavy training set}, designed to improve retrieval in tabular contexts. Fine-tuning the current best model on these datasets achieves state-of-the-art retrieval performance on our benchmarks. This demonstrates how systematic evaluation through our benchmark can informs effective training strategies, leading to more robust and adaptable retrieval models.

\vspace{0.23cm}
\noindent
\textbf{The contributions of this paper are as follows:}
\vspace{-0.2cm}
\begin{itemize}[leftmargin=*] 
    \setlength{\itemsep}{3pt}
    \setlength{\parskip}{0pt}
    \setlength{\itemindent}{1pt} % Shifts all lines in each item
    \item Defining properties of a real-world retrieval benchmark and highlighting shortcomings in existing ones.
    \item Introducing a high-quality multi-modal retrieval benchmark with an automated pipeline for query generation, filtering, and labeling verification.
    \item Establishing a rephrasing robustness evaluation framework via multi-level query rephrasing.
    \item Providing two specialized training datasets: (i) a rephrased dataset and (ii) a finance-table-heavy dataset, where fine-tuning on them significantly enhances retrieval performance.
\end{itemize}




