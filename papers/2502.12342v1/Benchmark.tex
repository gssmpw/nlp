


\section{REAL-MM-RAG-Bench}
\label{sec:benchmark}



\begin{table*}[t]
\footnotesize
\renewcommand{\arraystretch}{1.5} % Increases row height
% \setlength\tabcolsep{0pt}
\vspace{-0.15cm}
\centering
\begin{tabular*}{0.89\linewidth}{lccccccccc} 
\toprule
{} 
& \multicolumn{2}{c}{\textbf{Statistics}} 
& \multicolumn{1}{c}{\textbf{Multi-Modal}} 
& \multicolumn{3}{c}{\textbf{Enhanced Difficulty}} 
& \multicolumn{2}{c}{\textbf{Realistic-RAG Queries}} 
& {\textbf{Accurate Labels}} 
\\
\midrule

{} 
& {\textbf{$\#$ }} 
& {\textbf{$\#$ }} 
& {\textbf{MM}} 
& {\textbf{Long}} 
& {\textbf{Sub}} 
& {\textbf{Queries}} 
& {\textbf{RAG}} 
& {\textbf{RAG}} 
& {\textbf{False }} 
\\

{} 
& {\textbf{Pages}} 
& {\textbf{Queries}} 
& {\textbf{Pages}} 
& {\textbf{Docs}}
& {\textbf{domain}} 
& {\textbf{Rephr-}} 
& {\textbf{Tailored}} 
& {\textbf{Query}} 
& {\textbf{Neg.}} 
\\

{\textbf{Benchmark}} 
& {} 
& {} 
& {\textbf{}} 
& {\textbf{}} 
& {\textbf{Cover}} 
& {\textbf{asing}} 
& {\textbf{Gen.}} 
& {\textbf{Verif.}} 
& {\textbf{Verif.}} 
\\

\midrule
SlideVQA  & 52k & 14.5k & \textcolor{DarkGreen}{\ding{51}}  & \textcolor{DarkGreen}{\ding{51}} & \textcolor{red}{\ding{55}} &  \textcolor{red}{\ding{55}} &  \textcolor{red}{\ding{55}} &  \textcolor{red}{\ding{55}} &  \textcolor{red}{\ding{55}} \\ 
MMLONG & 7k & 1k & \textcolor{DarkGreen}{\ding{51}} &  \textcolor{DarkGreen}{\ding{51}}  &  \textcolor{red}{\ding{55}} &  \textcolor{red}{\ding{55}} &  \textcolor{red}{\ding{55}} &  \textcolor{red}{\ding{55}} &  \textcolor{red}{\ding{55}}\\
WIKI-SS-NQ  & 4k & 4k & \textcolor{red}{\ding{55}} &  \textcolor{red}{\ding{55}}  &  \textcolor{red}{\ding{55}} &  \textcolor{red}{\ding{55}} &  \textcolor{DarkGreen}{\ding{51}} &  \textcolor{DarkYellow}{\ding{51}\raisebox{-0.9ex}{\textsuperscript{\kern-0.9em\scalebox{1.6}{\ding{55}}}}} &  \textcolor{red}{\ding{55}}\\
ViDoRe  & 8k & 4k & \textcolor{DarkGreen}{\ding{51}} &  \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}} &   \textcolor{red}{\ding{55}} &  \textcolor{DarkYellow}{\ding{51}\raisebox{-0.9ex}{\textsuperscript{\kern-0.9em\scalebox{1.6}{\ding{55}}}}}&  \textcolor{DarkYellow}{\ding{51}\raisebox{-0.9ex}{\textsuperscript{\kern-0.9em\scalebox{1.6}{\ding{55}}}}} &  \textcolor{red}{\ding{55}}\\
\midrule
Ours  & 
8k & 5k & \textcolor{DarkGreen}{\ding{51}} &  \textcolor{DarkGreen}{\ding{51}} & \textcolor{DarkGreen}{\ding{51}} &  \textcolor{DarkGreen}{\ding{51}}  &  \textcolor{DarkGreen}{\ding{51}} &  \textcolor{DarkGreen}{\ding{51}} &  \textcolor{DarkGreen}{\ding{51}}\\
\bottomrule 
\end{tabular*}
% \vspace{-0.5cm}
\caption{
\textbf{Document Retrieval Benchmarks Comparison.}}
\label{Table:benchmark_comparison}
\vspace{-0.22cm}
\end{table*}

Creating a high-quality benchmark manually is both exhaustive and error-prone, limiting its size and reliability. To address this, we propose an \textbf{\emph{automated generation and verification pipeline}} tailored for Retrieval-Augmented Generation (RAG) evaluation. Our benchmark introduces robustness evaluation through \emph{\textbf{multi-level query rephrasing}}, further improving upon previous benchmarks.
The benchmark construction begins with \emph{\textbf{document collection}}, followed by four key steps: (1) \emph{\textbf{Query Generation}}, (2) \emph{\textbf{Query Verification}}, (3) \emph{\textbf{Query Rephrasing}}, and (4) \emph{\textbf{False Negative Verification}}.

\subsection{Document Collection}
To reflect real-world retrieval challenges, we focus on \emph{\textbf{long documents}} rather than isolated pages, and also ensuring \emph{\textbf{many pages within the same sub-domain}} by focusing on a single company data (IBM). Our dataset consists of ~8000 pages across four sub-domains, forming four specialized benchmarks (see \cref{Table:benchmark_statistics} for details). For each page, we added the document name to the page image to provide context.
\textbf{FinReport}: Financial reports (2005--2023), totaling 19 documents and 2687 pages, with a mix of text and tables.  
\textbf{FinSlides}: Quarterly financial presentations (2008--2024), totaling 65 presentations and 2280 pages, primarily table-heavy.  
\textbf{TechReport}: 17 Technical documents on FlashSystem, totaling 1674 pages, text-heavy with visual elements and tables. 
\textbf{TechSlides}: 62 Technical presentations on business and IT automation, totaling 1963 pages, with significant visual content. 


\subsection{Query Generation \& Filtering}
\paragraph{Generation.} We aim to generate queries that are both answerable by a specific document and RAG-suitable, meaning they reflect natural user inquiries without prior knowledge of the exact page or answer location (unlike traditional Q/A datasets tied to specific pages). 
To achieve this, we employed a Pixtral-12B VLM~\citep{agrawal2024pixtral}, prompting it to generate RAG-specific questions (see \cref{fig:query_generation_prompt}). Each document page was fed into the VLM, which produced 10 query-answer pairs per page, later keeping only a subset that met the benchmark’s quality criteria after filtering. Each retained query-answer pair is labeled with the corresponding page it was generated from.




\paragraph{Verification.}
Although the VLM is instructed to generate RAG-specific queries, many still do not fully align with our requirements. To systematically classify them, we use Mixtral-8x22B-v0.1 LLM \citep{jiang2024mixtral}, which evaluates each generated query and determines whether it is suitable as retrieval query  (see prompt in \cref{fig:query_verification_prompt}).
Queries that are well-formed for RAG are those that a user might ask without prior knowledge of the document’s structure, ensuring they are neither too general nor overly specific to a single page. Queries that fail this criterion fall into two categories: those with explicit page references, such as "in Figure 5" or "the title of the page", and those that are too broad, like "What is the net revenue in 2020?" instead of "What is IBM’s net revenue in 2020?". 





\subsection{Query Rephrasing}
In real-world retrieval, a user formulating a query does not have direct access to the document’s content and will naturally phrase their question without mirroring the exact wording from the source. However, VLMs often generate queries by copying phrases directly, leading to an over-reliance on keyword matching rather than true semantic retrieval. To address this, we introduce a rephrasing step that preserves query meaning while reducing dependence on specific document wording.  Each query is processed by Mixtral-8x22B-v0.1 with a dedicated prompt designed to alter phrasing while maintaining intent.  
The rephrased query is then verified by the LLM using a validation prompt (\cref{fig:rephrasing_prompts}), along with the original query and answer, to ensure it retains the original meaning and still corresponds to the known answer in the labeled page.  

\vspace{0.1cm}
\noindent
To enable deeper evaluation, each query undergoes three levels of rephrasing using distinct prompts (\cref{fig:rephrasing_prompts}). The first level introduces minor word changes while maintaining structure. The second modifies word choice and sentence order, making the phrasing more distinct. The third involves significant word rephrasing and sentence restructuring while preserving meaning. At the end of this process, each query exists in four versions: the original and three progressively rephrased forms, all linked to the same document page (see examples in \cref{fig:ours_examples_1,fig:ours_examples_2}).

\subsection{Accurate Labeling}

The final step in preparing our benchmark is verifying the correctness of negative labels.  This is especially crucial for our challenging benchmarks, where many pages share highly similar content within the same sub-domain.
Each query is systematically tested against all benchmark pages. Though computationally expensive, this step prevents false negatives and ensures reliable evaluation. Queries together with each page are processed using Pixtral-12B, which determines whether a page contains an answer to the query. Every query is then explicitly linked to all relevant pages. For simplicity, our final benchmark retains queries whose only the originally assigned page is verified to contain the correct answer. This results in a high-quality dataset of triplets: a page image, a query, and its corresponding answer. Note that our benchmark includes pages without corresponding queries. These are pages whose queries were filtered out at some stage, either because they were not suitable for RAG-style questions in general (e.g., title pages) or because the specific generated queries were not suitable for RAG.


