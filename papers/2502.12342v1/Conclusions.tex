\section{Conclusions}
\label{sec:conclusions}

We introduced \emph{REAL-MM-RAG-Bench}, a real-world multi-modal retrieval benchmark designed to evaluate retrieval models in \emph{reliable, challenging, and realistic} settings. Our benchmark addresses key properties essential for evaluating retrieval systems in real-world applications, which prior benchmarks often fail to capture. An important contribution of our work is the introduction of a \emph{multi-level rephrasing evaluation}, which assesses models under increasing linguistic variation, highlighting their limitations in generalizing beyond surface-level text matching.

\vspace{0.1cm}
\noindent
Our findings reveal two major weaknesses in current models: (i) retrieval over table-heavy financial documents and (ii) sensitivity to query rephrasing. To address these, we proposed dedicated training sets: a \emph{finance-table-heavy dataset} to improve retrieval on tabular content and a \emph{rephrased dataset} to enhance model robustness to query variations. Fine-tuning on these datasets yields significant improvements across benchmarks, demonstrating the impact of targeted training data. \emph{REAL-MM-RAG-Bench} and our proposed solutions establish a foundation for future research, paving the way for robust and effective retrieval models in real-world multi-modal retrieval scenarios.
