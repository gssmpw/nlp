\section{Conclusion}\label{sec:conclusion}


This work has demonstrated the effectiveness and importance of token cleaning, introducing a generic token cleaning pipeline that removes uninformative tokens while preserving task-relevant information.
Our theoretical analysis has revealed the strengths and limitations of two scoring strategies: \emph{fixed-model cleaning}, which provides stability but limited improvements, and \emph{self-evolving cleaning}, which has shown the potential for greater performance gains but requires more careful implementation.
Empirically, we have found that filtering out approximately 30\%â€“40\% of tokens consistently enhances performance across both strategies, achieving an average 6.3\% improvement over the full-token baseline at the 3B model scale.




