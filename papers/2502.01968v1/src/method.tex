
\section{Preliminary}\label{sec:preliminary}



\subsection{Next-Token Prediction}
Consider a data pool comprising $N$ samples, denoted as $\{\bm x_i\}_{i=1}^{N}$. Each sample $\bm x_i$ represents a sequence of tokens (including the prompt and the response) defined as $\bm x_i := \{x_{i,j}\}_{j=1}^{L_i}$, where $L_i$ denotes the token length for the $i$-th sample.
The training of LLMs can be framed as minimizing the negative log-likelihood of the observed tokens in the dataset. The model predicts the conditional probability $\PP(x_{i,j} | \bm x_{i, :j}; \theta)$ for each token $x_{i,j}$ given its preceding context, where $\theta$ represents the model parameters, and $\bm x_{i, :j}$ denotes the first $j-1$ tokens, i.e., $\{x_{i, 1},\cdots,x_{i,j-1}\}$.
Denote by $$D:=\{(x_{i,j}, \bm x_{i,:j}, y_{i,j}), \forall (i,j)\in S\},$$ where $$S:=\{(i,j) | i\in [N], j\in [L_i]\}, [N]:=\{1, 2,\cdots, N\}.$$
The loss function for the dataset can be expressed as:
\begin{equation}\label{eq:loss}
    \widehat {\mathcal{L}}_D(\theta) =  \frac{1}{\sum_{(i,j)\in S}  y_{i,j}} \sum_{(i,j)\in S} y_{i,j} \ell(x_{i,j} | \bm x_{i,:j}; \theta),
\end{equation}
where $\ell(x_{i,j} | \bm x_{i,:j}; \theta):=-\log \PP(x_{i,j} | \bm x_{i,:j}; \theta)$, and $y_{i,j}\in\{0,1\}$ is a binary (ground-truth) label indicating whether the token $x_{i,j}$ is a valid target or not. 
By iteratively updating $\theta$, the model learns to assign higher probabilities to the correct tokens while disregarding irrelevant ones.


\subsection{Token-Level Labels}
Token-level labels $ y_{i,j} \in \{0, 1\} $ play a crucial role in determining which tokens contribute to the loss calculation. However, its ground-truth value is often unknown. Denote $\tilde y_{i,j}$ by the (noisy) token label that we use in practice, which may or may not be identical to $y_{i,j}$. During different training phases, the criteria for setting $\tilde y_{i,j} $ may vary:
\begin{itemize}[left=-5pt]
    \item \textbf{Model Pretraining}: When training on general text data without explicit distinction between prompts and responses, all tokens are typically considered valid targets ($ \tilde y_{i,j} = 1 $), unless specific tokens are identified as irrelevant or redundant \citep{linnot}.
    \item \textbf{Supervised Fine-tuning (SFT)}: In this phase, the tokens corresponding to the prompt part are ignored, as they do not represent the model's predictions. Therefore, for prompt tokens, $\tilde y_{i,j} = 0 $, and for response tokens, $\tilde y_{i,j} = 1 $.
\end{itemize}





\section{Token Cleaning: A Noisy Label Perspective}\label{sec:method}

\subsection{Intuition}

In the phase of SFT, some tokens are deemed \textit{uninformative} or \textit{redundant} since most of the knowledge has been obtained in the pretraining phase, e.g., common patterns and structures, high-frequency phrases. In practice, the SFT phase assigns a label of $1$ to every token in the response, resulting in \textbf{noisy token labels}, where irrelevant tokens are incorrectly labeled as important (\( \tilde y_{i,j} = 1 \)). Such noise can hinder the modelâ€™s optimization process by introducing misleading gradients, reducing the signal-to-noise ratio by hiding informative tokens, and potentially leading to suboptimal performance.

To address this issue, it is essential to perform \textit{fine-grained token label cleaning}. This involves identifying and filtering out uninformative tokens while preserving the tokens that carry valuable task-specific information. As suggested by the noisy label literature \cite{zhu2022detecting,zhu2024unmasking}, the cleaning process typically involves two key components: a scoring function to assess the quality of each token and a threshold to distinguish between informative and uninformative tokens, which will be detailed in the next subsection.



\subsection{Token Cleaning Pipeline}

In this section, we will introduce the main components of the token-cleaning pipeline, including the scoring function and a simple yet effective threshold.

\subsubsection{Score Functions: An Influence-Guided Approach}\label{sec:score_func}

We deliver our intuition when designing score functions using the following example.
Suppose that the model is improved from $\theta$ to $\theta'$ by fine-tuning it on some data.
According to \citet{koh2017understanding,pang2024fairness}, the model update influences the prediction accuracy of each token, which can be written as 
\begin{equation}\label{eq:infl}
    \begin{aligned}
         & \infl\ (x_{i,j}|\bm x_{i, :j}; \theta, \theta')
        := 
 \ell(x_{i,j} | \bm x_{i,:j}; \theta') - \ell(x_{i,j} | \bm x_{i,:j}; \theta).
    \end{aligned}
\end{equation}
Intuitively, a more negative $\infl\ (x_{i,j}|\bm x_{i, :j}; \theta, \theta')$ indicates a higher confidence improvement on predicting $x_{i,j}$ given $\bm x_{i, :j}$. The equation can be explained from two perspectives:
\begin{itemize}[left=-3pt]
    \item \textbf{Assume Token Quality:} As demonstrated by \citet{pang2024fairness}, if we believe that token $x_{i,j}$ is the best choice given context $x_{i, :j}$, the above influence can be used to evaluate the quality of data that brings the model from $\theta$ to $\theta'$ on this specific task, i.e, a more negative $\infl\ (x_{i,j}|\bm x_{i, :j}; \theta, \theta')$ indicates a higher data quality.
    \item \textbf{Assume Model Quality:} From another perspective, if we believe the model $\theta'$ performs better on $\theta$ on this specific task, the above influence can be used to evaluate the quality of token $x_{i,j}$ since a good and underfitted choice of $x_{i,j}$ tends to have a negative $\infl\ (x_{i,j}|\bm x_{i, :j}; \theta, \theta')$. 
\end{itemize}


In this paper, we \textit{assume the model quality} and use the negative of the influence defined in Eq.~(\ref{eq:infl}) to evaluate the quality of tokens, i.e., 
\begin{equation}\label{eq:score_func}
    \textsf{Score}(x_{i,j}|\bm x_{i, :j}; \theta, \theta') = - \infl\ (x_{i,j}|\bm x_{i, :j}; \theta, \theta'),
\end{equation}
where a higher score indicates a higher token quality.

Extending from the current use of influences \cite{koh2017understanding,pang2024fairness}, we notice that $\theta$ and $\theta'$ are not necessarily to be the same model structure, as long as they share the same tokenizer. We will discuss potential choices of $\theta$ and $\theta'$ in Section~\ref{sec_selection_theta}.





\subsubsection{Threshold}

After computing token scores, a threshold is applied to filter out corrupted or uninformative tokens. The threshold separates the tokens that significantly improve the model performance from those that do not. An ideal approach is to use algorithms to estimate the ratio of the corrupted and then select the informative tokens according to the ratio. However, although there are lots of trials in the literature on noisy labels, most works focus on cleaning the image labels \cite{lad2023estimating} or sample-level text labels \cite{zhu2024unmasking,pang2024improving}. To the best of our knowledge, a feasible algorithm for estimating the noise ratio of token labels is unclear, which is beyond the scope of our paper and left for future explorations. In this paper, we use a fixed ratio (i.e., selected token proportion) $k\%$ to separate between information and uninformative tokens. Denote by $\hat y_{i,j}$ the token label after cleaning. We have
\begin{align}\label{eq:thre}
       \hat y_{i,j}= \begin{cases}
1 & \text{if } \textsf{Score}(x_{i,j}|\bm x_{i, :j}; \theta, \theta') \text{ ranks top } k\%, \forall i,j;  \\
0 & \text{otherwise}.
\vspace{-10pt}
\end{cases}
\end{align}


\subsection{Selection of $\theta$ and $\theta'$}\label{sec_selection_theta}

We discuss two feasible strategies for selecting $\theta$ and $\theta'$.

\subsubsection{Fixed-Model Cleaning}\label{sec:fix-model-cleaning}

Following the analyses in Section~\ref{sec:score_func}, we can assume the access to a model $\theta'$ that outperforms $\theta$. For example, a moderately performing Llama model can be considered as $\theta$, while a well-performing Llama model can be considered as $\theta'$ \cite{mindermann2022prioritized,linnot}.
Specifically, given the warm-up model $\theta'$ and the base model $\theta$, we compute the token scores for the entire dataset $\widetilde{D}$ according to Eq.~(\ref{eq:score_func}) and use a fixed threshold $k_{\text{fixed}}$ to assign token labels $\tilde{y}_{i,j}$ according to Eq.~(\ref{eq:thre}).
Note that token cleaning is performed globally, meaning that some samples may be entirely removed if they contain no positive tokens. This differs from \citet{linnot}, where each sample retains a fixed proportion of positive tokens.
The benefits and limitations of this strategy will be discussed theoretically in Section~\ref{sec:theory_fix_model}.


\subsubsection{Self-Evolving Cleaning}\label{sec:self-evolving-cleaning}

Inspired by the success of semi-supervised learning (SSL), we propose to do token cleaning iteratively. Specifically, in the $t$-th iteration, we fix the base model $\theta$ and adopt $\theta' = \theta_t$, then fine-tune $\theta_{t}$ with the selected tokens after cleaning.
See Algorithm~\ref{alg:token_cleaning} for more details.



\begin{algorithm}[h]
\caption{Token Cleaning Pipeline}\label{alg:token_cleaning} 
\begin{algorithmic}[1]
\STATE \textbf{Input:} Entire dataset $\widetilde{D}$, base model $\theta_0$, threshold $k_{\text{self-evol}}.$ \\
\STATE Split dataset $\widetilde{D}$ into a series of subset $\{\widetilde{D}_0, \cdots, \widetilde{D}_T \}$. Denote their indices by $\{S_0, \cdots, S_T\}$.
\STATE \text{Warmup Model} $\theta_1$: Finetune from base model $\theta_0$ on $\widetilde{D}_0$ subset with all tokens.
\FOR {$t$ \textbf{in} $\{1, 2, \cdots, T\}$}
\STATE Compute scores for subset $\widetilde{D}_t$'s tokens via $\textsf{Score}(x_{i,j}|\bm x_{i, :j}; \theta_{t-1}, \theta_{t}), \forall x_{i,j} \in \widetilde{D}_t$.
\STATE  Assign token labels $\hat{y}_{i,j}$ with a threshold $k_{\text{self-evol}}$.
\STATE Obtain $\theta_{t+1}$ by finetuning $\theta_{t}$ on cleaned subset $\widehat{D}_t=\{(x_{i,j}, \bm x_{i,:j}, \hat y_{i,j}), \forall (i,j)\in S_t\}$.
\ENDFOR
\STATE \textbf{Output:} $\theta_{T+1}$
\end{algorithmic}
\end{algorithm}

\paragraph{Algorithm Details}
The overall procedure is outlined in Algorithm \ref{alg:token_cleaning}.
First, we evenly partition the dataset $\widetilde{D}$ into a series of subsets, denoted as $\{\widetilde{D}_{0}, \widetilde{D}_{1}, \dots, \widetilde{D}_{T}\}$.
Next, the base model $\theta$ is fine-tuned on the initial subset $\widetilde{D}_0$ to produce a warm-up model $\theta_0$, which serves as the initial reference model.
Rather than relying solely on $\theta_0$ as a fixed reference model, a self-evolving mechanism is introduced in Lines 4-8.
Specifically, for each subsequent subset $\widetilde{D}_t$, we keep the base model fixed and utilize the latest updated model as the reference model, i.e., $\theta = \theta_0$ and $\theta' = \theta_t$, to compute token scores.
By applying a threshold $k_{\text{self-evol}}$ to these scores, we obtain the cleaned labels $\hat y_{i,j}$.
The updated model $\theta_t$ is then fine-tuned on the cleaned subset $\widehat{D}_t$, producing the reference model for the next iteration.
This process continues iteratively, and the final reference model is used as the output of the algorithm.





    






\section{Theoretical Analyses}\label{sec:thm_analyses}





Let $\BR\{\cdot\}$ be the indicator function taking value $1$ when the specified condition is satisfied and $0$ otherwise. Define the \emph{0-1 loss} as 
$\BR{(\theta(\bm X_{\text{prev}}), X_{\text{next}})}:=\BR\{\theta(\bm X_{\text{prev}}) \ne X_{\text{next}}\}$, where $X_{\text{next}}$ is the random variable for the next token, $X_{\text{prev}}$ is the random variable for tokens before the next token, and $\theta(\bm X_{\text{prev}})$ stands for the prediction of next token for model $\theta$ given $\bm X_{\text{prev}}$ as input.
Without loss of generality, we consider the ideal case where all the training instances for next-token prediction are \emph{i.i.d.} and minimize 0-1 loss in the following analyses. The loss can be generalized to bounded loss $\ell(\cdot)$ and finite function space $\mathcal F$ following the generalization bounds that can be introduced using Rademacher complexity \citep{bartlett2002rademacher}.

\subsection{Exceed the Performance of Full Tokens}

Denote by $\widetilde D:=\{(x_{i,j}, \bm x_{i,:j}, \tilde y_{i,j}), \forall i,j\}$ the full-token dataset.
By minimizing the noisy loss
$$
    \widehat {\mathcal{L}}_{\widetilde D}(\theta) = \frac{1}{N} \sum_{i=1}^N \frac{1}{\sum_{j=1}^{L_i} \tilde y_{i,j}} \sum_{j=1}^{L_i} \tilde y_{i,j} \BR( \theta (\bm x_{i,:j}), x_{i,j})
$$
we can get model $\hat \theta_{\widetilde D}:=\argmin_{\theta}~  \widehat {\mathcal{L}}_{\widetilde D}(\theta)$. 
Denote by $\widetilde Y$, $Y$ the random variables for $\tilde y_{i,j}$ and the corresponding ground-truth label $y_{i,j}$.
The expected loss of training with full tokens can be denoted by 
$$
{\mathcal{L}}_{\widetilde {\mathcal D}}(\theta) = \E\left[ \widetilde Y \cdot \BR{(\theta(\bm X_{\text{prev}}), X_{\text{next}})}  \right],
$$
where $\widetilde {\mathcal D}$ is the distribution of $\widetilde D$. 
Denote by $$\eta(\widetilde D):=\PP(\widetilde Y \ne Y)$$ the noise rate of full tokens. Theorem~\ref{thm:noisy_bound} shows the error upper bound of learning with full tokens. See Appendix~\ref{app:proof_noisy_bound} for the proof.



\begin{theorem}[Error of learning with full tokens]\label{thm:noisy_bound}



With probability at least $1-\delta$, the generalization error of learning with full tokens is upper-bounded by
\begin{equation}\label{eq:error_ub}
    {\mathcal L}_{{\mathcal D}}(\hat \theta_{\widetilde D} ) 
\le \underbrace{\eta(\widetilde D)}_{\text{Data quality}} +
 \underbrace{ \sqrt{\frac{2\log({4}/\delta)}{M}}}_{\text{Data quantity}},
\end{equation}
where $M:=\sum_{i=1}^N L_i$ denotes the number of tokens.
\end{theorem}

Theorem~\ref{thm:noisy_bound} shows that the error of learning with full tokens depends on two factors:
\squishlist
\item \textbf{Data Quality}: $\eta(\widetilde D)$ denotes the noise rates of learning with full tokens, where a higher noise rate leads to a larger error, i.e., a worse performance. The negative impact of wrong token labels may not be canceled by increasing the number of tokens.
\item \textbf{Data Quantity}: When the number of tokens $M$ increases, the generalization could be smaller, showing that the token cleaning result cannot merely return a small set of high-quality tokens, i.e., the precision and recall of the token cleaning algorithm are both important. 
\squishend

Denote by $\widehat Y$ the random variable of token labels after cleaning.
We show the superiority of token cleaning compared to the full-token case in the following corollary.

\begin{corollary}\label{coro:exceed_full_token}
    With probability as least $1-2\delta$, token cleaning performs better than full-tokens in terms of the error upper bound when
\begin{equation}\label{eq:clean_vs_full}
    \eta(\widetilde D) - \hat\eta \ge \sqrt{2\log(4/\delta)} \cdot \sqrt{\frac{1}{M}} \cdot \left( \sqrt{\frac{1}{\hat r}} - 1 \right),
\end{equation}
where $\hat\eta:=\PP(\widehat Y \ne Y)$ denotes the noise rates of cleaned labels and $\hat r:=\PP(\widehat Y = 1)$ denotes the ratio of positive tokens after token cleaning.  
\end{corollary}
Corollary~\ref{coro:exceed_full_token} shows token cleaning is preferred when the positive impact of reducing noise rate outweighs the negative impact of reducing the number of feasible tokens. For example, when $M$ is larger (a larger dataset), the inequality in Corollary~\ref{coro:exceed_full_token} is more likely to hold since the right-hand side is smaller.




\subsection{Fixed-Model Cleaning: Stable But Limited Improvement}\label{sec:theory_fix_model}

We now analyze the benefits and limitations based on Theorem~\ref{thm:noisy_bound} and Corollary~\ref{coro:exceed_full_token}.
By selecting an appropriate model $\theta'$, we can take a one-shot token cleaning on all the tokens in the candidate data pool. In this case:
\squishlist 
\item \textbf{Data Quality}: The noise rate of cleaned tokens is fixed, i.e., the \textit{data quality} term in Eq.~(\ref{eq:error_ub}) is fixed. By carefully selecting the threshold $k_{\text{fixed}}$, there exists a token cleaning result whose noise rate $\hat \eta$ is less than $\eta(\widetilde{D})$.

\item \textbf{Data Quantity}: With more tokens being cleaned, $M$ is increasing. Then the total generalization error can be consistently reduced. 
\squishend
Therefore, under this strategy, as long as the reference model $\theta'$ is sufficiently good to reduce the noise rate from $\eta(\widetilde{D})$ to a lower rate $\hat{\eta}$, the model's performance can be improved by fine-tuning with additional \textit{i.i.d.} cleaned tokens, demonstrating the advantage on stability. However, even as $M \rightarrow \infty$, the total error does not go to zero due to imperfect data quality, showing the limitations on final performance.




\subsection{Self-Evolving Cleaning: Potential Matthew Effect} \label{sec:matthew_effect}

For ease of presentation, we divide the data into three groups according to their task difficulty and number of \textit{i.i.d.} clean tokens in the training dataset:
\squishlist \item $G_1$ (Rich Group): Characterized by lower noise rates after token cleaning and a higher proportion of effective tokens. This group typically experiences significant performance gains during warmup (Line 3, Algorithm~\ref{alg:token_cleaning}) and has a great number of relevant tokens.
\item $G_2$ (Poor Group): Marked by higher noise rates after token cleaning and fewer effective tokens. This group often exhibits limited or even degraded performance during warmup and has a scarce number of relevant tokens.
\item $G_3$ (Intermediate Group): Falling between the rich and poor groups in terms of data quality and quantity. While it generally sees reasonable performance improvement during warmup, its convergence tends to be unstable due to a limited number of effective tokens.
\squishend
Note that the definition of groups only applies to the theoretical analyses, which does not mean we need to explicitly know the group attribute of each data. In fact, it is challenging to know this information.
Theoretically, there are three observations during SFT.

\squishlist
\item\textit{Observation 1: The rich get richer ($G_1$).} When $\hat\eta < \eta({\widetilde{D}})$ and $\hat r \cdot M$ is sufficiently large, according to Corollary~\ref{coro:exceed_full_token}, fine-tuning on $\widehat{D}_1$ benefits from lower token noise rates and a higher number of effective tokens, thereby reducing the error upper bound and resulting in a better model $\theta_2$. With a better reference model and a similar number of effective tokens, the model in the next iteration can be further improved, i.e., the rich get richer.

\item\textit{Observation 2: The poor get poorer ($G_2$).} 
When the model $\theta_{1}$ underperforms compared to $\theta_{0}$, $\eta_1$ increases significantly and may even exceed $0.5$. According to Corollary~\ref{coro:exceed_full_token}, continued fine-tuning on tokens with such high noise rates can further degrade performance, exemplifying a ``the poor get poorer" effect.

\item\textit{Observation 3: Unstable convergence ($G_3$).} 
If $\hat\eta$ is comparable to $\eta({\widetilde{D}})$ and $\hat r \cdot M$ is only marginally sufficient, fine-tuning on $\widehat{D}_3$ may yield moderate improvements but suffers from instability due to the limited number of effective tokens.
According to Corollary~\ref{coro:exceed_full_token}, the model's error upper bound may not decrease consistently, leading to unstable convergence.
\squishend







From the above analyses, we know that self-evolving models are more adaptive and aggressive compared to fixed models. The theoretical insights further highlight strategies for achieving better performance.








    









