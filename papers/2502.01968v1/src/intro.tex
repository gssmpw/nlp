\section{Introduction}\label{sec:intro}



Supervised fine-tuning (SFT) has served as a widely adopted approach and a fundamental step in aligning large language models (LLMs) with human expectations. This process ensures that LLMs can accurately understand human instructions and produce relevant responses. In practice, SFT involves fine-tuning pre-trained models using annotated instructional data \citep{touvron2023llama}. Following general data scaling laws \citep{zhang2024scaling}, significant efforts have been dedicated to collecting large-scale instructional data containing millions of examples \citep{wang2022self, chung2024scaling, longpre2023flan}. 


Recent studies on the SFT have widely agreed that data quality matters far more than quantity \citep{zhou2024lima, chen2023alpagasus,pang2024improving, liu2023makes}. That is, a small, well-curated dataset can often deliver effective or even superior performance on downstream tasks, highlighting the critical role of data cleaning or selection.
Existing data cleaning approaches primarily emphasize identifying high-quality samples in large dataset pools via some metrics, including perplexity \citep{caoinstruction}, completion length \citep{zhao2024long}, confidence scores \citep{chen2024automated}), LLM-generated quality ratings \citep{chen2023alpagasus, pang2024improving, liu2023makes} or even costly human annotations \citep{zhou2024lima}.
Although these methods have proven effective, focusing solely on sample-level cleaning may overlook complexities within each sample.

In practice, each sample typically contains hundreds of tokens, some of which occur frequently regardless of the sampleâ€™s quality. These common tokens/patterns can overshadow task-specific words that are crucial for model performance during training. Moreover, during interference, if the model continually outputs these frequent tokens, it may neglect more informative ones, producing outputs that appear correct yet fail to address specific tasks. Thus, even well-curated samples can contain token-level noise that dilutes essential signals. Addressing these token-level issues by removing or down-weighting uninformative tokens can help the model prioritize important content and improve downstream results.

In this paper, we go beyond traditional sample-level data cleaning by proposing a generic \emph{token cleaning} pipeline and an analytical framework for LLM SFT tasks. Specifically, we filter out uninformative tokens while retaining those with meaningful task-specific information. 
This is achieved by first assessing token quality using an influence-guided scoring mechanism, followed by threshold-based separation.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/token_selection_2.0.pdf}
    \caption{Token cleaning pipelines. \textit{Fixed-Model Cleaning} applies a one-shot cleaning process to the entire dataset $\widetilde{D}$. In contrast, \textit{Self-Evolving Cleaning} follows an iterative approach. It begins with a warm-up phase, where a model is fine-tuned on the full tokens of split-$0$, denoted as $\widetilde{D}_0$, and then used to clean the next data split, transforming $\widetilde{D}_1$ into $\widehat{D}_1$. The reference model is subsequently updated by fine-tuning the warm-up model (i.e., the first reference model) on $\widehat{D}_1$. This iterative process continues, progressively refining the reference model with each newly cleaned data split.}
    \label{fig:main_workflow}
\end{figure*}

Figure~\ref{fig:main_workflow} illustrates two scoring strategies supported by our token cleaning pipeline. The high-level idea is to evaluate the influence of model updates on each token, which can be calculated by the loss disparity between a base model and a reference model. We introduce two implementations:
\squishlist
\item \textit{Fix-Model Cleaning.}
In this strategy, both the base model and the reference model remain fixed, and a one-shot token cleaning is applied to the entire SFT dataset. The base model is then fine-tuned on the cleaned tokens, producing the final model output. This strategy is similar to the latest token selection method for pre-trained data \cite{linnot}. We defer detailed comparisons to Section~\ref{sec:fix-model-cleaning} (analytical) and Section~\ref{sec:exp} (experimental).

\item \textit{Self-Evolving Cleaning.}
In this strategy, the base model remains fixed while the reference model is updated iteratively. The data is divided into multiple parts, with each iteration cleaning one part. The reference model is then updated sequentially using the cleaned results from each part. Unlike the fix-model cleaning, the final model output is the reference model obtained in the last iteration.
\squishend

Our main contributions can be summarized as follows.
\squishlist
    \item \textit{Generic Token Cleaning Pipeline.} We formulate the problem from the perspective of noisy labels and present a novel influence-guided token cleaning pipeline that scores and filters out uninformative tokens, enhancing downstream task performance by focusing model training on the most relevant tokens. The pipeline not only encompasses existing approaches but also inspires new implementations. 
    \item \textit{Self-Envolving Cleaning.} 
    Beyond merely calculating the influence scores with a pair of fixed models, we propose to update the reference model iteratively, which could progressively enhance the quality of supervision signals, leading to better downstream performance.
    \item \textit{Analytical Framework.} We provide rigorous analyses to demonstrate when and why SFT with the cleaned token outperforms the full tokens. Specifically, we establish an upper bound on the error incurred when learning with full tokens (Theorem~\ref{thm:noisy_bound}), offering theoretical insights into the trade-offs of different cleaning strategies. Our analysis explains why fix-model cleaning yields stable but limited improvements, while self-evolving cleaning shows greater potential but requires careful implementation.
    \item \textit{Comprehensive Experiments.} We conduct extensive experiments across multiple tasks, demonstrating that our token cleaning pipeline consistently boosts performance over baselines and validates the practical merits of our work.
\squishend
