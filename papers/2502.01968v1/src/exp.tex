
\section{Experiments}\label{sec:exp}

\subsection{Experiments Setup}

\paragraph{Data Pool} We utilize a high-quality data pool with 50k sample size from five popular SFT datasets (300k in total): Flan\_v2 \citep{longpre2023flan}, Open Assistant 1 \citep{kopf2024openassistant}, Stanford Alpaca \citep{stanford_alpaca2023}, Dolly \citep{databricks2023dolly}, and WizardLM \citep{xu2023wizardlm}. The data pool is constructed based on a new powerful data curation pipeline proposed by \citep{pang2024improving}, which involves selecting data samples using quality rating scores generated by LLMs.
More dataset statistical information including token length can be found in Appendix \ref{sec:apx_data_pool}. For the self-evolving cleaning strategy, we heuristically divide the data pool into five equally sized subsets (10k samples).




\begin{table*}[ht]
    \centering
     \caption{Performance (original score $\times$100) comparison of different baselines on various benchmarks. We highlight the best result in \textbf{boldface} and the second-best with \underline{underline}. By default, the selected token proportion (i.e., threshold) is 0.6.} 
    \vspace{1ex}
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{l|ccccccc|c}
\toprule
\textbf{Model} 
& \textbf{TruthfulQA} 
& \textbf{TydiQA} 
& \textbf{LoqiQA} 
& \textbf{MMLU} 
& \textbf{HellaSwag} 
& \textbf{ARC-C} 
& \textbf{BoolQ} 
& \textbf{AVG} \\
\midrule
\multicolumn{9}{c}{\cellcolor{blue!10} \textbf{Base model: LLaMA-3.2-3B}} \\
\midrule
\textsc{Base} 
& 39.39 & 21.10 & 22.17 & 56.29 & 55.24 & 42.20 & 72.95 & 44.19 \\
\textsc{DS$^2$ (10k)} 
& 43.35 & 41.20 & 24.96 & 56.93 & 55.64 & 44.62 & 74.80 & 48.79 \\
\textsc{Full Tokens (50k)} 
& 43.32 & 49.60 & 24.34 & 56.87 & 55.57 & 44.44 & 74.98 & 49.87 \\
\textsc{Uniform Random (50k$\times$0.6)} 
& 43.79 & 47.00 & 23.41 & 56.96 & 55.37 & 44.44 & 75.05 & 49.43 \\
\textsc{Rho} 
& \textbf{45.57} & \underline{53.60} & \underline{26.05} & \textbf{57.10} & 55.16 & \underline{45.39} & \underline{77.36} & 51.46 \\
\midrule
\textsc{Fixed-Model Cleaning} 
& \underline{48.96} & 52.60 & 25.89 & \underline{57.09} & \textbf{56.43} & \underline{45.39} & \textbf{77.52} & \underline{51.98} \\
\textsc{Self-Evolving Cleaning} 
& \textbf{51.07} & \textbf{56.38} & \textbf{28.22} & 56.18 & \underline{55.81} & \textbf{45.99} & 77.33 & \textbf{53.00} \\
\toprule
\multicolumn{9}{c}{\cellcolor{blue!10} \textbf{Base model: LLaMA-3.1-8B}} \\
\midrule
\textsc{Base} & 45.10 & 22.80 & 26.51 & 65.29 & 59.92 & 50.82 & 82.18 & 50.37 \\
\textsc{DS$^2$ (10k)}  & 49.57 & 45.80 & 27.44 & 65.77 & 60.37 & 53.49 & \textbf{83.26} & 55.10 \\ 
\textsc{Full Tokens (50k)}  & 47.51 & 58.10 & \underline{28.53} & \underline{65.78} & 60.42 & 54.01 & 82.49 & 56.70\\
\textsc{Uniform Random (50k$\times$0.6)} & 48.68 & 56.60 & 27.29 & \textbf{65.81} & 60.40 & 54.09 & \underline{83.11} & 56.57\\
\textsc{Rho} & 54.63 & 61.90 & \textbf{28.99} & 65.74 & \underline{62.14} & 54.78 & 81.66 & 58.55 \\
\midrule
\textsc{Fixed-Model Cleaning} & \underline{56.02} & \underline{62.38} & 28.22 & 65.71 & 61.92 & \textbf{55.12} & 82.67 & \underline{58.90 } \\
\textsc{Self-Evolving Cleaning}    & \textbf{59.58} & \textbf{63.58} & 26.05 & 65.07 & \textbf{62.67} & \underline{54.87} & 82.49 & \textbf{59.20}\\
\toprule
    \multicolumn{9}{c}{\cellcolor{blue!10} \textbf{Base model: Mistral-7B-v0.3}} \\
    \midrule
    \textsc{Base} & 42.56 & 54.70 & 25.74 & 62.41 & 60.77 & 48.92 & 82.30 & 52.88 \\
    \textsc{DS$^2$ (10k)} & 44.24 & 55.70 & 25.27 & \textbf{62.50} & 61.10 & 50.39 & 83.45 & 53.85 \\ 
    \textsc{Full Tokens (50k)} & 43.67 & 55.60 & 25.27 & 62.41 & 61.14 & 50.56 & \textbf{83.85} & 54.12 \\
    \textsc{Uniform Random (50k$\times$0.6)} & 43.82 & 55.70 & 24.81 & \underline{62.47} & 61.20 & 50.04 & \underline{83.76} & 53.64 \\
    \textsc{Rho} & 43.92 & 54.50 & 25.43 & 62.12 & 61.35 & \underline{51.08} & 83.29 & 53.60 \\
    \midrule
\textsc{Fixed-Model Cleaning}  & \underline{44.52} & \textbf{59.03} & \underline{26.05} & 61.45 & \textbf{61.47} & \textbf{51.68} & 82.03 & \textbf{55.20} \\
\textsc{Self-Evolving Cleaning}  & \textbf{45.41} & \underline{56.17} & \textbf{27.44} & 62.30 & \underline{61.40} & 50.65 & 81.28 & \underline{ 55.00} \\
\bottomrule
\end{tabular}
    }
    \label{tab:main_results}
\end{table*}



\paragraph{Base Models} In this paper, we select three popular open-source LLMs as our base models, including LLaMA-3.2-3B, LLaMA-3.1-8B \citep{dubey2024llama} and Mistral-7B-v0.3 \citep{jiang2023mistral}. These base models will be fine-tuned using samples from our data pool. 

\paragraph{Baselines} There are several baselines for performance comparisons: 1) \textsc{Base} denotes the used base model; 2) \textsc{DS$^2$} \cite{pang2024improving} fine-tunes base model on 10k selected high-quality samples (with full tokens) from the entire data pool (50k); 3) \textsc{Full Tokens} utilizes all tokens to fine-tune the base model; 4) \textsc{Uniform Random} randomly selects $k\%$ tokens from the 50k data pool without replacement; 5) \textsc{Rho} \citep{mindermann2022prioritized, linnot} directly computes the excess loss for all tokens between the base and reference model and then selects top-$k\%$ tokens. Recall that $k$ is the pre-defined threshold for token cleaning.

\paragraph{Warmup} We warmup by fine-tuning the base model on subset $\widetilde{D}_0$ with full tokens, and make it the (initial) reference model for \textsc{Rho}, our \textsc{Fixed-Model Cleaning}, and \textsc{Self-Evolving Cleaning}. The warmup model is equivalent to the DS$^2$ baseline \citep{pang2024improving}.

\begin{table*}[ht]
\centering
\caption{Performance results of self-evolving cleaning strategy over iterations (checkpoints) on seven benchmarks. Base model: \texttt{LLaMA-3.2-3B}. These performance results align with three observations arising from the Matthew effect.}
\vspace{1ex}
    \resizebox{1\linewidth}{!}{
\begin{tabular}{l|ccc|c|ccc}
\toprule
\textbf{Model} 
& \textbf{TruthfulQA} 
& \textbf{TydiQA} 
& \textbf{LoqiQA} 
& \textbf{MMLU} 
& \textbf{HellaSwag} 
& \textbf{ARC-C} 
& \textbf{BoolQ} \\
\midrule
\textsc{Reference-1} 
& 45.46 & 50.05 & 27.44 & 57.31 & 56.10 & 45.91 & 76.87 \\

\textsc{Reference-2} 
& 46.67 & 53.18 & 27.44 & 56.89 & 56.25 & 46.51 & 77.15 \\

\textsc{Reference-3} 
& 48.91 & 54.36 & 28.22 & 56.43 & 56.13 & 46.43 & 77.36 \\

\textsc{Reference-4} 
& 51.07 & 56.38 & 28.22 & 56.18 & 55.81 & 45.99 & 77.33 \\
\bottomrule
\end{tabular}}
\label{tab:iteration_results_llama3b}
\end{table*}

\paragraph{Evaluation} To comprehensively evaluate the efficacy of token cleaning methods, we adopt seven OpenLLM Leaderboard tasks, including MMLU \citep{hendrycks2020measuring}, TruthfulQA \citep{lin2021truthfulqa}, TydiQA \citep{clark2020tydi}, HellaSwag \citep{zellers2019hellaswag}, ARC-Challenge \citep{clark2018think},  BoolQ \citep{clark2019boolq} and LogiQA \citep{liu2020logiqa}. These datasets are sufficiently diverse to thoroughly assess the fine-tuned model across various aspects, including factual accuracy, reasoning, and multilingual capability.
The task performances are evaluated on the lm-eval-hareness\footnote{\url{https://github.com/EleutherAI/lm-evaluation-harness}} repository. More evaluation and training details can be found in Appendix \ref{sec:appendix_evalution_details}.





\subsection{Main Empirical Results}


As shown in Table~\ref{tab:main_results}, our proposed strategies consistently outperform baselines across three base models on seven evaluation benchmarks. Notably, compared to using full tokens, our self-evolving cleaning has achieved the average performance improvement of \textbf{6.3\%} on the 3B model and \textbf{2.0\%/4.4\%} on the 7B/8B models. 

\paragraph{Local Ranking vs. Global Ranking}
Compared to RHO, which ranks token scores locally within individual samples and removes the same proportion of tokens per sample, our fixed-model cleaning method globally ranks token scores across the entire dataset. As shown in Table~\ref{tab:main_results}, the local-ranking method (\textsc{Rho}) yields lower average performance than the global-ranking method (\textsc{Fixed-Model Cleaning}), e.g., $51.46$ vs. $51.98$ for LLaMA-3.2-3B and $53.60$ vs. $55.20$ for Mistral-7B-v0.3, demonstrating that global ranking leads to more stable performance improvements.
One possible explanation is that local ranking is constrained by the quality of individual samples. For example, in SFT, a low-quality sample may not contain any useful tokens, while almost all tokens in a high-quality sample may be useful. Since local ranking removes the same proportion of tokens from both samples, it inevitably retains uninformative tokens from low-quality samples while discarding informative ones from high-quality samples.
This limitation can be mitigated through global token ranking, as employed in our fixed-model cleaning approach.




\paragraph{Self-Evolving Cleaning Follows the Matthew Effect} 
Table~\ref{tab:iteration_results_llama3b} presents the model's performance across different training iterations (checkpoints), illustrating three phenomena arising from the Matthew effect, as discussed in Section~\ref{sec:matthew_effect}.
Specifically, performance on TruthfulQA, TydiQA, and LogiQA steadily improves over iterations, representing \textit{Observation 1: the rich get richer}. In contrast, MMLU which focuses on factual knowledge, exhibits a slight performance decline, illustrating \textit{Observation 2: the poor get poorer}. Meanwhile, for the remaining tasks (HellaSwag, ARC-C, and BoolQ), performance improvements are observed but exhibit fluctuations, aligning with \textit{Observation 3: unstable convergence}.












\begin{figure}[!t]
    \centering
    \includegraphics[width=.6\linewidth]{figs/token_ratio_impact.png}
    \caption{Average performance results of self-evolving cleaning pipeline under different token proportions. The base model is LLaMA-3.2-3B.}
    \label{fig:impact_of_token_ratio}
\end{figure}


\subsection{Ablation Study}






\paragraph{Impact of Selected Token Proportion.}

Here, we investigate the impact of selected token proportion for our pipeline using a series of token proportion values including $\{0.3, 0.4, \cdots, 0.9\}$. As presented in Figure \ref{fig:impact_of_token_ratio},  the best results are achieved when the selected token proportion is approximately 50\% to 70\%.  Beyond this range, the overall performance declines, which may be attributed to uninformative tokens. One valuable empirical finding is that \textit{the performance gains in SFT tasks largely rely on a small number of highly informative tokens}. This observation supports the prevailing view that data quality is more crucial than mere volume.
Full results can be referred to in Appendix~\ref{sec:apx_more_exp_results} (Table~\ref{tab:impact_of_data_prop_full}).







\begin{table}[t]
    \centering
    \caption{Performance comparison with a new reference model: LLaMA-3.1-8B-Instruct. 
    {\color{blue}Blue}: A better reference model brings a higher performance improvement. {\color{red}Red}: The counterpart.}
    \vspace{0.5ex}
    \resizebox{.8\linewidth}{!}{\begin{tabular}{c|cc|cc}
    \toprule
    \textbf{Models} & {\color{red}\textbf{MMLU}} & {\color{red}\textbf{BoolQ}} & {\color{blue}\textbf{TydiQA}} &  {\color{blue}\textbf{ARC-C}}  \\
    \midrule
      \textsc{Reference: LLaMA-8b-Inst}& 68.18 & 84.03 & 21.63 & 51.77 \\
      \midrule
      \textsc{Rho} & 57.04 &75.94   &39.37 & 46.08  \\
       \textsc{Fixed-Model Cleaning} &56.96&76.37 &39.17 &46.08 \\
      \midrule
      \midrule
    \textsc{Reference: Warmup}& 56.93 & 74.80 &41.20 & 44.62 \\
    \midrule
    \textsc{Rho}& 57.10 &77.36 & 53.60 & 45.39    \\
    \textsc{Fixed-Model Cleaning} & 57.09 &77.52 &52.60 & 45.39  \\
    \bottomrule
    \end{tabular}}

    \label{tab:impact_of_reference_model}
\end{table}

\paragraph{Impact of Reference Model.}
To assess the impact of the reference model on performance, we run \textsc{Rho} and our fixed-model cleaning approach using LLaMA-3.1-8B-Instruct as the new reference model and compare the results with our previous reference model (DS$^2$ as the warmup). As shown in Table~\ref{tab:impact_of_reference_model}, a more powerful reference model generally leads to greater performance improvements in datasets such as TydiQA and ARC-C. However, some counterintuitive results emerge: in MMLU and BoolQ, despite the 8B reference model significantly outperforming the warmup model, it fails to yield further improvements through token cleaning.
A possible explanation for this phenomenon is the distribution shift. If we divide the evaluation task distribution into two parts: an in-distribution segment aligned with our data pool and an out-of-distribution segment, LLaMA-3.1-8B-Instruct, while achieving high overall performance, may not necessarily surpass the warmup model in the in-distribution subset. Investigating this hypothesis and validating this assumption are promising directions for future research.
More detailed results can be found in Appendix~\ref{sec:apx_more_exp_results}.








