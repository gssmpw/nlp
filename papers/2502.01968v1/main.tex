%!TEX program = pdflatex

\documentclass{article} %


\usepackage{arxiv}

\usepackage[table]{xcolor}

\include{math.tex}

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{graphicx}       %
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{fancyvrb}
\usepackage{booktabs} 
\usepackage{tcolorbox}
\usepackage{parskip}
\usepackage{amssymb}%
\usepackage{pifont}%
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{rotating}
\usepackage{wrapfig}
\usepackage{natbib}
\usepackage{float}


\newtheorem{assumption}{Assumption}[section]

\newtheorem{definition}{Definition}[section]
\newtheorem{property}{Property}[section]
\newtheorem{proposition}{Proposition}[section]

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}
\newcommand{\infl}[1]{ \textsf{Infl}_\textsf{#1} }

\newtcolorbox{mybox}{
    colback=gray!10!white,
    colframe=gray!10!white,
    left=1mm,
    top=1mm,
    right=1mm,
    boxsep=0mm,
    width=14cm,  
    before=\par\smallskip\centering,
    after=\par,
    height=1cm
}


\newcommand{\yl}[1]{\textbf{\color{red}(Yang: #1)}}
\newcommand{\wjh}[1]{\textbf{\color{blue}(Jiaheng: #1)}}
\newcommand{\jlp}[1]{\textbf{\color{teal}(Jinlong: #1)}}
\newcommand{\zzw}[2]{\textbf{\color{blue}(Zhaowei: #1)}{\color{blue}~#2}}
\newcommand{\dina}[1]{\textbf{\color{blue}(Dina: #1)}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}




\date{}
\renewcommand{\shorttitle}{}


\title{Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning}

\author{
 Jinlong Pang \\
  University of California, Santa Cruz\\
  \And 
  Na Di \\
  Northeastern University \\
  \And
  Zhaowei Zhu \\
  Docta.ai \\
  \And
  Jiaheng Wei \\
  HKUST (Guangzhou) \\
  \And
    Hao Cheng \\
  HKBU \\
  \And
Chen Qian \\
  University of California, Santa Cruz \\
  \And
  Yang Liu\thanks{Corresponding to Yang Liu <yangliu@ucsc.edu>.}\\
  University of California, Santa Cruz \\
}



\begin{document}


\maketitle

\begin{abstract}

Recent studies show that in supervised fine-tuning (SFT) of large language models (LLMs), data quality matters more than quantity. 
While most data cleaning methods concentrate on filtering entire samples, the quality of individual tokens within a sample can vary significantly. After pre-training, even in high-quality samples, patterns or phrases that are not task-related can be redundant or uninformative. Continuing to fine-tune on these patterns may offer limited benefit and even degrade downstream task performance.
In this paper, we investigate token quality from a noisy-label perspective and propose a generic \emph{token cleaning} pipeline for SFT tasks. Our method filters out uninformative tokens while preserving those carrying key task-specific information. Specifically, we first evaluate token quality by examining the influence of model updates on each token, then apply a threshold-based separation. The token influence can be measured in a single pass with a fixed reference model or iteratively with self-evolving reference models. The benefits and limitations of both methods are analyzed theoretically by error upper bounds. Extensive experiments show that our framework consistently improves performance across multiple downstream tasks.
\end{abstract}



\input{src/intro}
\input{src/related_work}
\input{src/method}
\input{src/exp}
\input{src/conclusion}

\clearpage
\newpage





\bibliographystyle{unsrtnat}
\bibliography{references}


\newpage
\input{src/appendix}

\end{document}
