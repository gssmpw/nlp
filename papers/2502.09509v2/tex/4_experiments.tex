\section{Experiments}
\label{sec:experiments}


\subsection{Setup}
\paragraph{Implementation Details}
We finetune all autoencoders on OpenImages to adhere to the framework used in LDM \cite{rombach2022high}. We finetune for $5$ epochs with batch size $10$.  Detailed specifications of each autoencoder, including spatial compression rates and latent channels, are provided in \autoref{sec:appendix:ae_specs}. 
For \texttt{DiT}~\cite{peebles2023scalable}, \texttt{SiT}~\cite{ma2024sit} and \texttt{REPA}~\cite{Yu2025repa}, we follow their default settings and train on ImageNet~\cite{deng2009imagenet} with a batch size of $256$, where each image is resized to $256\times 256$.
We use $\text{B/}2$, $\text{XL/}2$  architectures which employ a patch size $2$, except for the experiment with \texttt{SD-VAE-16} in~\autoref{tab:comp_auto} in which we used $\text{B/}1$, due to its lower spatial resolution compared to other autoencoders.
These models are originally trained in the latent distribution of \texttt{SD-VAE-FT-EMA}\footnote{https://huggingface.co/stabilityai/sd-vae-ft-ema} a subsequent version of the original \texttt{SD-VAE} that has been further fine-tuned with an exponential moving average on LAION-Aesthetics~\cite{schuhmann2022laion} (see \autoref{tab:ablation_ft} and \cite{peebles2023scalable} for their performance differences). 
For \texttt{MaskGIT}, we follow \cite{besnier2023pytorch} and train on ImageNet for 300 epochs with a batch size of $256$. 
We follow \texttt{ADM} \cite{dhariwal2021adm} for all data pre-processing protocols. 



\input{tex/tab_autoencoders_comparison}
\vspace{-5 pt}
\paragraph{Evaluation}
For generative performance, we train latent generative models on the latent distribution of each autoencoder and we report Frechet Inception Distance (FID) \cite{fid}, sFID \cite{sfid}, Inception Score (IS) \cite{is}, Precision (Pre.) and Recall (Rec.) \cite{kynkaanniemi2019improved} using $50,000$ samples and following \texttt{ADM} evaluation protocol \cite{dhariwal2021adm}. 
To evaluate reconstruction, we report FID, Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM) \cite{ssim}, and Perceptual Similarity (LPIPS) \cite{zhang2018unreasonable} using the ImageNet validation set. 
To distinguish reconstruction and generation FID, we write \Th{gFID} and \Th{rFID}, respectively. 
To quantify the effectiveness of
\our we further measure the equivariance error (see \autoref{sec:appendix_metrics}).

\input{tex/tab_benchmark_diffusion}

\subsection{Equivariance-regularized VAEs}
We begin our experimental analysis by demonstrating the versatility of \texttt{EQ-VAE}, showing that it seamlessly adapts to both continuous 
and discrete autoencoders.


\paragraph{Continuous Autoencoders}
We integrate our \texttt{EQ-VAE} regularization into established continuous autoencoders with varying latent dimensions. Namely,
%We evaluate its impact on the following models: 
\texttt{SD-VAE}, \texttt{SD-VAE-16}, \cite{rombach2022high}, \texttt{SDXL-VAE} \cite{podell2024sdxl}, and \texttt{SD3-VAE} \cite{esser2024sd3}. 
To evaluate the effect of the regularization on generative performance we train 
\texttt{DiT-B} models on the latent codes
%a \texttt{DiT-B/2} model on latent distributions 
before and after our regularization. 
We present our results in \autoref{tab:comp_auto}. 
We observe that our simple objective effectively reduces the equivariance error for all autoencoders. 
Further, \texttt{EQ-VAE} maintains the original autoencodersâ€™ reconstruction fidelity while consistently delivering significant improvements in generative performance. 
The results hint that there is
a correlation between the generative performance (\Th{gFID}) and the reduction in equivariacne error. Notably, for \texttt{SD-VAE}, \texttt{SDXL-VAE} and \texttt{SD-VAE-16}, our regularization significantly boosts generative performance. For \texttt{SD3-VAE}, although the reduction in equivariance error is relatively modest, it still results in a \Th{gFID} improvement.





\input{tex/tab_maskgit}


\paragraph{Discrete Autoencoders}
% We primarily focus on continuous autoencoders to enhance the performance of diffusion models. 
To investigate if \texttt{EQ-VAE} can be applied to discrete autoencoders, we experiment on \texttt{VQ-GAN} \cite{esser2021taming} and validate the effectiveness of our regularization on the masked image modeling framework \texttt{MaskGIT} \cite{chang2022maskgit}. In~\autoref{tab:comp_auto}, we show that \our is effective in the discrete case, reducing the equivariance error as well as improving the generative performance from $6.8$ to $5.9$ in \Th{gFID}.



\input{tex/fig_speedup}

\subsection{Boosting Generative Image Models}

By applying \our to both continuous and discrete autoencoders, we enhance the performance of state-of-the-art generative models, including \texttt{DiT} a pure transformer diffusion model, \texttt{SiT} that employs continuous flow-based modeling, \texttt{REPA} a recent approach aligning transformer representations with self-supervised features and \texttt{MaskGIT} a well-established masked generative model.
\vspace{-10pt}
\paragraph{DiT \& SiT} 
As demonstrated in ~\autoref{tab:bench-diff-main}, our regularization approach yields significant improvements across both \texttt{DiT-B} and \texttt{DiT-XL} models. Specifically, training \texttt{DiT-XL/2} on the regularized latent distribution achieves  \mbox{\Th{gFID} $14.5$} at $400\text{K}$ iterations, compared to \mbox{ $19.5$} without regularization. Notably, by $1.5\text{M}$ iterations, \texttt{DiT-XL/2} trained with \texttt{EQ-VAE} achieves \mbox{\Th{gFID} $8.8$}, outperforming the $\texttt{DiT-XL/2}$ model trained with \texttt{SD-VAE-FT-EMA} even at $7\text{M}$ iterations. The speed-up provided by \texttt{EQ-VAE} can be qualitatively observed in \autoref{tab:speedup}.
Moreover, in \autoref{tab:bench-diff-main}, we show that \texttt{SiT} models can also benefit from the regularized latent distribution of \texttt{EQ-VAE}, improving \Th{gFID} from $17.2$ to $16.1$ at $400\text{K}$ steps. 

\vspace{-10pt}
 \paragraph{REPA} We show that our regularization (which is performed in the first stage of latent generative modeling) is complementary to \texttt{REPA}, thus leading to further improvements in convergence and generation performance.
Specifically, training \texttt{REPA} (\texttt{SiT-XL-2}) with our \texttt{EQ-VAE} reaches  $5.9$ \Th{gFID} in $1\text{M}$ instead of $4\text{M}$ iterations. Thus, the regularized latent distribution of \texttt{EQ-VAE} can make the convergence of \texttt{REPA} $\times 4$ faster (\autoref{fig:teaser}). \emph{This is striking because \texttt{REPA} was shown to already significantly speed-up the convergence of diffusion models.}
\vspace{-10pt}
\paragraph{MaskGIT}
As shown in \autoref{tab:maskgit}, \texttt{MaskGIT} trained with our \our converges twice as fast reaching $6.80$ \Th{gFID} in $130$ epochs, instead of $300$. Furthermore, by epoch $300$ it reaches  $5.91$ \Th{gFID} surpassing the performance reported in both \cite{besnier2023pytorch} and \cite{chang2022maskgit}. 

\vspace{-3pt}
\paragraph{Comparison with state-of-the-art generative models}
\mbox{To further} demonstrate how \our accelerates the learning process, we compare it with recent diffusion methods using classifier-free guidance. Notably, as shown in \autoref{tab:bench-cfg-dif}, \ditxltwo with \our reaches $2.37$ \Th{gFID} in just $300$ epochs, matching the performance of \ditxltwo trained with \texttt{SD-VAE} or \texttt{SD-VAE-FT-MAE}. Even when combining  \our with the state-of-the-art approach \texttt{REPA}, we are able to achieve comparable results with standard \texttt{REPA} while using  $\times 4$ less training compute ($200$ vs $800$ epochs). 



\input{tex/tab_main_comparison}

\subsection{Analysis}
\paragraph{Spatial transformations ablation} We begin the analysis of our method by ablating the effect of our equivariance regularization on generative performance with each spatial transformation to understand their respective impact. We consider isotropic $S(s,s)$ or anisotropic $S(s_x, s_y)$  scaling, rotations $R(\theta)$, and combined transformations. 
We then train a \texttt{DiT-B/2} on each latent distribution. In \autoref{tab:ablation-trans}, 
we observe that encouraging scale equivariance has a significant impact on generative performance.
Furthermore, rotation equivariance is also beneficial in generation performance.
% However equivariance under scaling has a greater impact on performance.
Combining transformations yields further improvement, demonstrating their complementary effects. While anisotropic scaling yields a better generative performance since the regularization is more aggressive, it negatively impacts reconstruction quality. 
Thus, our \our default setting uses combinations of rotations and isotropic scaling.


\input{tex/tab_ablation_transformations}
\input{tex/fig_epochs}

\vspace{-10pt}
\paragraph{Latent space complexity and generative performance}

To better understand the impact of our regularization on the complexity of the latent manifold, we measure its Intrinsic Dimension (ID). The ID represents the minimum number of variables needed to describe a data distribution \cite{1054365}. 
Notably, in \autoref{tab:ablation-trans}, we observe a correlation between the intrinsic dimension of the latent manifold and the resulting generative performance.
This suggests that the regularized latent distribution becomes simpler to model, further validating the effectiveness of our approach. This reduction in the complexity of latent representations can also be qualitatively observed in \autoref{fig:teaser} (left).
For further details on ID, see \autoref{sec:appendix_id}. 


% \vspace{-10pt}

\paragraph{How many epochs does \texttt{EQ-VAE} need to enhance generation?} To demonstrate how quickly our objective regularizes the latent distribution, we conduct an ablation study by varying the number of fine-tuning epochs. We train a \ditbtwo model on the resulting latent distribution of each epoch and present the results in \autoref{fig:ablation_epochs}. Notably, even with a single epoch (10K steps) of fine-tuning, the \Th{gFID} drops from 43.5 to 36.7, highlighting the rapid refinement our objective achieves. For context, \texttt{SD-VAE-FT-EMA} has been fine-tuned for 300K steps.

\input{tex/tab_ft}

\paragraph{The enhancement in generative performance is not a result of the additional training} 
To verify that the improvement in generative performance stems from our equivariance regularization (\Eq{eq:ours_obj}) rather than additional training, we compare \texttt{EQ-VAE} with \texttt{SD-VAE}$^\dagger$ in \autoref{tab:ablation_ft}. \texttt{SD-VAE}$^\dagger$ is obtained by fine-tuning \texttt{SD-VAE} for five extra epochs using only the original objective (\Eq{eq:ldm}). The results show that this additional training has a negligible effect on generative performance, whereas \texttt{EQ-VAE} leads to a significant improvement. Similarly, \texttt{SD-VAE-EMA-FT}, derived from \texttt{SD-VAE}, has minimal impact on the \Th{gFID} score, further underscoring the effectiveness of \texttt{EQ-VAE}.



