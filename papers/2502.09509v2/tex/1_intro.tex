\section{Introduction}
\label{sec:intro}

\input{tex/fig_teaser}


Latent generative models \cite{rombach2022high} have become a dominant framework for high-fidelity image synthesis, achieving state-of-the-art results across diffusion models \cite{rombach2022high, yao2024fasterdit, ma2024sit}, masked generative modeling \cite{chang2022maskgit, li2023mage}, and autoregressive models \cite{esser2021taming, li2024autoregressive, tian2024visual}. These models operate in two phases. First, an autoencoder compresses high-dimensional images into a lower-dimensional latent space, which can be continuous (e.g., \texttt{SD-VAE} for diffusion \cite{rombach2022high}) or discrete (e.g., \texttt{VQ-GAN} for autoregressive \cite{esser2021taming, yu2022scaling} and masked generative modeling \cite{chang2022maskgit}). This latent space retains essential semantic and structural information while discarding high-frequency details. Second, a generative model learns to model the distribution of these latent representations, enabling the synthesis of visually coherent images. At inference time, the generative model first samples a latent code, which is then decoded back into the image space by the autoencoder. While much research has focused on improving the generative phase—through advances in architectures \cite{peebles2023scalable}, objectives \cite{ma2024sit}, and optimization techniques \cite{yao2024fasterdit}—the autoencoder's role in shaping the latent space remains equally critical to overall performance.

In fact, the quality of the latent space is pivotal, influencing both computational efficiency (by reducing dimensionality and accelerating convergence in the generative phase) and the model's ability to produce high-fidelity outputs \cite{rombach2022high}. %For instance,
In diffusion models, most state-of-the-art approaches—such as \texttt{DiT} \cite{peebles2023scalable}, \texttt{SiT} \cite{ma2024sit}, \texttt{PixArt} \cite{chen2024pixartalpha}, \texttt{SD3} \cite{esser2024sd3}, and \texttt{Flux} \cite{flux2023}—rely on autoencoders with architectures and training objectives similar to the \texttt{SD-VAE} introduced in Latent Diffusion Models (LDM) \cite{rombach2022high}. LDM explores two widely adopted regularization strategies: a continuous variational approach and a discrete codebook framework. The variational approach uses a KL divergence term to align the latent distribution with a Gaussian prior, promoting a smooth and structured latent space \cite{kingma2014}. Alternatively, the discrete codebook framework constrains the latent space to a finite set of learned embeddings, limiting its complexity and providing a different form of regularization \cite{esser2021taming}.


These regularization strategies inherently introduce a trade-off. Stronger regularization, such as increasing the weight of the KL divergence term, produces a smoother and more learnable latent space for the generative model in the second phase \cite{tschannen2025givt}. 
However, it also reduces the information capacity of the latent representation, leading to a loss of fine-grained details and ultimately degrading reconstruction quality. Empirical evidence suggests that this trade-off can set an upper bound on the overall performance of latent generative models \cite{rombach2022high}, as the autoencoder’s limited capacity to preserve detailed information restricts the overall ability of latent generative models to synthesize highly-fidelity images. This raises a fundamental question:  
\emph{Can we mitigate this trade-off, creating a latent space that is more optimized for generative modeling, without compromising reconstruction quality, thereby improving the overall generative modeling process?}

A key aspect that could address this challenge lies in the structure and properties of the latent space itself. In particular, we identify an essential limitation of current state-of-the-art autoencoders: their latent representations are not equivariant to basic spatial transformations, such as scaling and rotation (see \autoref{fig:qualitative-equivariance-main}; extended discussion in \secref{sec:irregularity}).
This introduces unnecessary complexity into the latent manifold, forcing the generative model to learn nonlinear relationships that could otherwise be avoided.

To address this issue, we propose a simple yet effective modification to the training objective of autoencoders that encourages latent spaces to exhibit the aforementioned equivariance.
Our method called \texttt{EQ-VAE}, penalizes discrepancies between reconstructions of transformed latent representations and the corresponding transformations of input images. Notably, \texttt{EQ-VAE} requires no architectural changes to existing autoencoder models and does not necessitate training from scratch. 
Instead, fine-tuning pre-trained autoencoders for a few epochs with \texttt{EQ-VAE} suffices to imbue the latent space with equivariance properties, reducing its complexity (see \autoref{fig:teaser}-left; quantitative results in \autoref{tab:ablation-trans}) and facilitating learning for generative models (e.g., \autoref{fig:teaser}-right). %Notably, 
This is achieved without degrading the autoencoder’s reconstruction quality.


Our method is compatible with both continuous and discrete autoencoders, enabling broad applicability across latent generative models. For example, applying \texttt{EQ-VAE} to the continuous \texttt{SD-VAE} \cite{rombach2022high} significantly improves the performance of downstream diffusion models such as \texttt{DiT} \cite{peebles2023scalable}, \texttt{SiT} \cite{ma2024sit}, and \texttt{REPA} \cite{Yu2025repa}, as measured by FID scores. 
Similarly, applying \texttt{EQ-VAE} to discrete \texttt{VQ-GAN} \cite{esser2021taming} enhances performance in the masked generative modeling approach \texttt{MaskGIT} \cite{chang2022maskgit}.

We make the following contributions:
\begin{itemize}[noitemsep,topsep=0pt]
    \item We identify that the latent space of established autoencoders lacks equivariance under spatial transformations, which impedes latent generative modeling. Building on this observation, we propose \our, a simple regularization strategy that improves generative performance without compromising reconstruction quality.  
    \item Our method is compatible with both continuous and discrete autoencoders, enabling a plug-and-play approach for commonly used generative models such as diffusion and masked generative models.
    \item 
    We show that by fine-tuning well-established autoencoders with our objective,
    we significantly accelerate the training of latent generative models. For instance, fine-tuning \sdvae for just $5$ epochs yields a $\times 7$ speedup on \ditxltwo and  $\times 4$ speedup on \texttt{REPA (w/ SiT-XL/2)} (see \autoref{fig:teaser} (right)).
\end{itemize}