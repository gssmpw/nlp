
\section{Additional Ablations}
\label{more_ablations}


\subsection{Implicit vs Explicit Equivariance Regularization}
\label{sec:appenidx_exp_imp}

Here, we provide an analysis of the design choice of our objective. We aim to design an objective that reduces the equivariance error of the encoder while avoiding mode collapse and preserving reconstruction performance. For each objective investigated, we finetune \texttt{SD-VAE} and evaluate the effect on generative performance by training a \texttt{DiT-B/2} on the resulting latent distribution. 
Initially, we perform fine-tuning with the standard objective along with the explicit loss in (\Eq{eq:encoder_loss}):\mbox{ $\mathcal{L}_{VAE} +   \lambda  \mathcal{L}_{\text{explicit}}$ }and set $\lambda=0.1$. 
We further experiment with adding a stop-gradient (sg) in the $\mathcal{E}(\mathbf{\tau \circ x})$ term in $\mathcal{L}_{\text{explicit}}$.
In ~\autoref{tab:losses}, we observe that using $\mathcal{L}_{\text{explicit}}$ successfully reduces the equivariance error for both rotation and scaling transformations.  However, both reconstruction and generative performance degrade severely, indicating a mode collapse in the latent space. 

\input{tex/tab_abblation_losses}




\subsection{Regularization Strength}
We evaluate the impact of hyperparameter $p_{\alpha}$ which controls the strength of our regularization  in \autoref{tab:ablation_prior}. 
\label{sec:appendix_prior}
\input{tex/tab_ablation_prior}


\section{Details on the Intrinsic Dimension Estimation.}
\label{sec:appendix_id}
Several recent works \cite{valeriani2023geometry, kvinge2023exploring, cheng2023bridging} have utilized ID to measure the complexity of latent representations in deep learning modeling. 
  Further \citet{pope2021the} has demonstrated a strong correlation between a datasetâ€™s relative difficulty and its ID. 
We compute the ID of the latent representations using the TwoNN estimator \cite{facco2017estimating}, which relies solely on the distances between each point and its two nearest neighbors.
In practice, the \Th{TwoNN} estimator can be affected by noise, which
typically leads to an overestimation of the ID. Nevertheless, it is a robust tool to evaluate \emph{relative} complexity and has been used
effectively to analyze representations in deep neural networks \cite{valeriani2023geometry}. We adopt the \Th{TwoNN} implementation of \Th{DADApy}~\cite{glielmo2022dadapy}.



\section{Details on Evaluation Metrics}
\label{sec:appendix_metrics}

\subsection{Generation Metrics}
We follow the setup and use the same reference batches of ADM \cite{nichol21a}
for evaluation, utilizing their official implementation\footnote{\url{https://github.com/openai/guided-diffusion/tree/main/evaluations}}. We use NVIDIA A100 GPUs for our evaluation.
 We briefly explain each metric used for the evaluation.

 \begin{itemize}
     \item \textbf{FID} \cite{fid} quantifies the feature distance between the distributions of two image datasets by leveraging the Inception-v3 network \cite{szegedy2016rethinking}. The distance is calculated based on the assumption that both feature distributions follow multivariate Gaussian distributions.
     \item \textbf{sFID} \cite{sfid} computes FID with intermediate spatial features of the
        Inception-v3 network, to capture spatial distribution of the generated images
     \item \textbf{IS} \cite{is}  measures a KL-divergence between the original label distribution and the
distribution of Inception-v3 network's logits after the softmax normalization.
\item \textbf{Precision and Recall} \cite{kynkaanniemi2019improved} are the
fraction of realistic images and the fraction of training data covered by generated data respectivly.
 \end{itemize}

\clearpage

\subsection{Reconstruction Metrics}
We evaluate reconstruction on the validation set of Imagenet which contains 50K images. We provide a description of each metric used for the reconstruction evaluation.

 \begin{itemize}
     \item \textbf{PSNR} measures the quality of reconstructed images by comparing the maximum possible signal power to the level of noise introduced during reconstruction. Expressed in decibels (dB).
     \item \textbf{SSIM} \cite{ssim} assesses the similarity between two images by evaluating their structural information, luminance, and contrast. 
     \item \textbf{LPIPS} \cite{zhang2018unreasonable}  evaluates the perceptual similarity between two images by comparing their deep feature representations using VGG \cite{vgg}
 \end{itemize}



\subsection{Equivariance Error} 
\label{sec:equi_error}

To quantify the effectiveness of \texttt{EQ-VAE} at constraining the latent representations of the autoencoders to equivary under scale and rotation transformation we measure the equivariance error. Similar to \cite{Sosnovik2020Scale-Equivariant} we define the equivariance error as follows: \mbox{$\Delta_{eq}^{\mathcal{T}} = \frac{1}{\vert \mathcal{T}\vert \cdot N}  \underset{\vert \mathcal{T} \vert}{\sum} \underset{N}{\sum} \Vert \tau \circ \mathcal{E}(\mathbf{x}) - \mathcal{E}(\tau \circ  \mathbf{x}) \Vert_2^2 \;/ \; 
 \Vert \mathcal{E}(\tau \circ \mathbf{x})\Vert_2^2$  } where $\text{N}=50\text{K}$ in the number of samples in ImageNet validation and $\mathcal{T}$ is the set of transformations considered. We conduct our evaluation with $\mathcal{T}_r=  \{ \frac{\pi}{2},\pi, \frac{3\pi}{2}\}$ for rotations and $\mathcal{T}_s=  \{ 0.25, 0.50, 0.75\}$ for scale.



\section{Detailed Benchmarks}

\subsection{Detailed generative performance}
We provide a detailed evaluation of all the generative models presented in the main paper, including additional metrics and training iterations.  
 Specifically, ~\autoref{tab:details_dit_sit} details the performance of the \texttt{DiT-XL/2} and \texttt{SiT-XL/2} models, while ~\autoref{tab:detailed_repa} presents results for the \texttt{REPA} (\texttt{SiT-XL/2}) models trained with both \texttt{SD-VAE-FT-EMA} (as reported in the respective papers) and \texttt{EQ-VAE}. Additionally, ~\autoref{tab:detailed_maskgit} provides results for \texttt{MaskGIT} models trained using \texttt{VQ-GAN} and \texttt{EQ-VAE}. For all models, we use the evaluation metrics originally reported in the original publications.
\input{tex/tab_appendix_dit_sit_detailed}

\newpage
\input{tex/tab_appendix_repa_detailed}
\input{tex/tab_appendix_maskgit_detailed}

\subsection{Detailed reconstruction performance}
We detail in \autoref{tab:appendix_recon} the reconstruction evaluation metrics of each autoencoder with and without \texttt{EQ-VAE} regularization.
\label{sec:autoencoders_appendix}

\input{tex/tab_appendix_autoencoders_comparison}



\clearpage

\section{Specifications of Autoencoder Models}
\label{sec:appendix:ae_specs}
\input{tex/tab_autoencoder_specific}

\section{Latent Generative Models}

Here we provide a brief description of the latent generative models, mentioned in the main paper:
\begin{itemize}[noitemsep, topsep=5pt]
    \item \texttt{MaskGIT} \cite{chang2022maskgit} utilizes a bidirectional transformer decoder to synthesize images by iteratively predicting masked visual tokens produced by a \texttt{VQ-GAN} \cite{esser2021taming}.
    
    \item \texttt{LDM} \cite{rombach2022high} proposes latent diffusion models, modeling the image distribution in a compressed latent space produced by a KL- or VQ-regularized autoencoder.

    \item \texttt{DiT} \cite{yao2024fasterdit} proposes a pure transformer backbone for training diffusion models and incorporates AdaIN-zero modules.

    \item \texttt{MaskDiT} \cite{zheng2023fast} trains diffusion transformers with an auxiliary mask reconstruction task.

    \item \texttt{SD-DiT} \cite{zhu2024sd} extends the \texttt{MaskDiT} architecture by incorporating a discrimination objective using a momentum encoder.

    \item \texttt{SiT} \cite{ma2024sit} improves diffusion transformer training by moving from discrete diffusion to continuous flow-based modeling.

    \item \texttt{REPA} \cite{Yu2025repa} aligns the representations of diffusion transformer models to the representations of self-supervised models.
\end{itemize}

\clearpage

\section{Additional Qualitative Results}

\input{tex/fig_transform_latent_appendix}


\input{tex/fig_appendix_latents}







\begin{figure}[t!]
    \vspace{-15pt}
    \centering
    \include{tex/fig_appendix_gen_2}
    \vspace{-15pt}
    \caption{\textbf{Uncurated  samples $\mathbf{256 \times 256}$ \ditxltwo \texttt{/w EQ-VAE}.} Classifier-free guidance scale = 4.0.}
    \label{fig:appendix-gen-1}
    %\vspace{-10pt}
\end{figure}
%------------------------------------------------------------------------------

