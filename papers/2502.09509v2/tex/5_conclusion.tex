\section{Conclusion}
\label{sec:conclusion}
In this work, we argue that the structure of latent representations produced by the autoencoder is crucial for the convergence speed and performance of latent generative models.
We observed that latent representations of established autoencoders are not equivariant under simple spatial transformations. To address this, we introduce \texttt{EQ-VAE}, a simple modification to the autoencoder's training objective. We empirically demonstrated that fine-tuning pre-trained autoencoders with \texttt{EQ-VAE} for just a few epochs, is enough to reduce the equivariance error and significantly boost the performance of latent generative models while maintaining their reconstruction capability. We believe that our work introduces several promising future directions, particularly in exploring the theoretical and empirical relationship between the geometry of the latent distribution and the performance of latent generative models.