\begin{abstract}
Latent generative models have emerged as a leading approach for high-quality image synthesis. These models rely on an autoencoder to compress images into a latent space, followed by a generative model to learn the latent distribution. 
We identify that existing autoencoders lack equivariance to semantic-preserving transformations like scaling and rotation, resulting in complex latent spaces that hinder generative performance. To address this, we propose \our, a simple regularization approach that enforces equivariance in the latent space, reducing its complexity without degrading reconstruction quality. By fine-tuning pre-trained autoencoders with \our, we enhance the performance of several state-of-the-art generative models, including \dit, \sit, \texttt{REPA} and \maskgit, achieving a $\times 7$ speedup on \ditxltwo with only five epochs of \sdvae fine-tuning. \our is compatible with both continuous and discrete autoencoders, thus offering a versatile enhancement for a wide range of latent generative models. Project page and code: \url{https://eq-vae.github.io/}.
\end{abstract}

