\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{0.8pt}
\begin{tabular}{lcc}
\toprule
\Th{Autoencoder} & \Th{gFID $\downarrow$} & \Th{rFID $\downarrow$}   \\
\midrule 
 \texttt{SD-VAE} \cite{rombach2022high} & 43.8 & 0.90 \\ \hline
 \texttt{SD-VAE-FT-EMA} \cite{rombach2022high} & 43.5 & 0.73\\
\texttt{SD-VAE}$^\dagger$   & 43.5 & 0.81 \\
% \texttt{EQ-VAE w/} $\text{p}_{\beta}=0$  & 33.5 & 0.85 \\
\texttt{EQ-VAE}  & 34.1 & 0.82 \\

\bottomrule
\end{tabular}
\vspace{-3pt}
\caption{\textbf{Additional Training vs. Equivariance Regularization.}  
Comparing various fine-tuning strategies for \texttt{SD-VAE} confirms that \ourâ€™s improvements stem from equivariance regularization. $^\dagger$ Denotes additional training with the standard objective (\Eq{eq:ldm}) for 5 epochs.}
\label{tab:ablation_ft}
\vspace{-3pt}
\end{table}