\begin{center} \label{sec: apd}
    {\Large \textbf{Appendix}}
\end{center}

The appendix is organized as follows. In Appendix \ref{apd: related-work}, we review the literature on related work. Appendix \ref{apd: general bound} contains the proof of the general lower bound. In Appendix \ref{apd: GLR}, we discuss the GLR test. Appendices \ref{apd: non-sep proofs} and \ref{apd: sep proofs} provide proofs for the non-separator and separator cases, respectively. Appendix \ref{apd: opt-solving} addresses how to solve the optimization problem in Equation \eqref{eq: middle non-sep LB}, and Appendix~\ref{apd: unknown context} discusses the scenario where the context matrix is unknown. Finally, Appendix \ref{apd: experiment} provides additional details about our experiments and presents further numerical results.


\begin{table}[H]
    \centering
    \caption{Table of notations.}
    \label{table: notations}
    % \small
    \begin{tabular}{c|l} 
        \toprule
        Notation & Description \\
        \hline
        $X$ & Action \\
        $Z$ & Post-action context \\
        $Y$ & Reward variables \\
        $n$ & Size of action set \\
        $k$ & Size of post-action context \\
        $[n]$ & $\{1, 2, \dots, n\}$ \\
        $\Delta^{n-1}$ & $(n-1)$-dimensional standard simplex = $\{w \in \mathbb{R}^n \mid w_i \geq 0 \wedge \sum_{i = 1}^{n} w_i = 1\}$ \\
        $d(P, Q)$ & Kullbackâ€“Leibler (KL) divergence between two probability measures $P$ and $Q$ \\
        $d_{B}(p, 1-p)$ & KL between two Bernoulli  with parameters $p$ and $1 - p$ \\
        $\ch(\{\A_1, \A_2, \dots, \A_n\})$ & Convex hull of \(n\) vectors \(\A_i \in \mathbb{R}^d\) = $ \left\{ \sum_{i = 1 }^{n} \lambda_i \A_i \mid \lambda_i \geq 0 \ \wedge \sum_{i = 1}^{n} \lambda_i = 1 \right\}$ \\
        $\A = [\A_1 \vert \A_2 \vert \ldots \vert \A_n]$ & Context probability matrix, where $\A_{i,j} = \pr(Z = i \mid X = j)$ \\
        $\bmu = [\bmu_1 \vert \bmu_2 \vert \ldots \vert \bmu_n]$ & Mean value matrix, where $\mu_{i,j} = \mathbb{E}(Y | X=i, Z=j)$ \\
        $P^{\bmu}_{i}, P^{\A, \bmu}_{i}$ & Joint distribution of $(Z, Y)$ after pulling arm $X = i$ in environment $\A, \bmu$ \\
        $\istarmu, i^*(\bmu, \A)$ & Best arm for an instance parameterized by $\bmu$ and $\A$ \\
        $\I, \I(\A)$ &  Set of $\bmu$ matrices that imply a unique best arm for a given $\A$ \\
        $\Is, \Is(\A)$ & Set of $\bmu$ vectors that imply a unique best arm for a given $\A$ in separator setting  \\  
        $\Delta_i$  &   Sub-optimality gap of arm $i$ \\
        $\delhit{i}$ &  Estimated sub-optimality gap of arm $i$ in around $t$ \\
        $\muht$ &       Matrix containing the empirical estimates of all entries of $\bmu$ up to round $t$ \\
        Alt$(\bmu, \A)$ & Set of alternative parameter matrices $\bmu' \in \I$ such that $\istarmu \neq i^*(\bmu')$ \\
        $\amin$ & $\min_{i,j} \A_{i,j}$   \\    
        $\numaction$ &    Total number of times arm $i$ has been played up to round $t$ \\
        $\numcontext$ &    Total number of times context $j$ has been observed up to round $t$ \\
        $\numac$ &   Total number of times arm-context $i, j$ has been observed up to round $t$ \\
        $\lamht$ &      GLR statistic at around $t$ \\
        % $N_z(t) = \left( N_{z,1}(t), N_{z,2}(t), \ldots, N_{z,k}(t) \right)$ & Number of samples collected from different contexts up to round $t$ \\
        $\mathbf{w}^*(\bmu, \A)$ &  The optimal weights (proportions) for actions in non-separator context \\
        $\wzstar{\bmu}$          &  The set of optimal weights (proportions) for contexts \\
        \bottomrule
    \end{tabular}
\end{table}



\section{Further Discussion on Related Work} \label{apd: related-work}


\paragraph{Best Arm Identification.} 
The literature on pure exploration in multi-armed bandit problems is extensive \cite{bandit-book1-lattimore2020bandit, pure2-thesis-stephens2023pure}. Here, we provide a brief overview of the most closely related works. The standard best-arm identification problem is primarily studied in two settings: the fixed-confidence setting, where the probability of error is predetermined \cite{track-stop-garivier2016optimal, SR-audibert2010best, kaufmann2020contributions, confidence-jamieson2014best, lb-tsitsiklis-mannor2004sample}, and the fixed-budget setting, where the time horizon is fixed \cite{budget-confidence-gabillon2012best, SH-karnin2013almost, lb-budget-carpentier2016tight}. 


In \cite{track-stop-garivier2016optimal}, the authors propose the first (asymptotically) optimal algorithm for the best-arm identification problem, known as track and stop, which attempts to track the optimal proportion of arm plays. The track-and-stop idea has since been widely adopted to design algorithms for various identification problems, including the algorithms presented in this paper.

In the fixed-confidence setting, other identification problems with different objectives have been explored in the literature, such as subset selection \cite{subset-selection1-kalyanakrishnan2012pac, subset-selection2-kaufmann2013information}, checking for the existence of an arm with a low mean \cite{badarm1-tabata2020bad, badarm2-kaufmann2018sequential}, and finding all the good arms \cite{all-good-arms-mason2020finding}. The most general form of the identification problem with bandit feedback has also been studied under various assumptions and is sometimes called \textit{sequential identification} \cite{kaufmann2021mixture}, \textit{General-Samp} \cite{general-samp-chen2017nearly}, \textit{partition identification} \cite{partition-id-juneja2019sample}, and is typically encompassed by the term pure exploration \cite{frank-wolf-wang2021fast, mutiple-correct-answers-degenne2019pure}. 
Our problem can be viewed as a general identification problem in which the agent cannot take arbitrary actions but can only choose probability distributions over them.


Best-arm identification in contextual bandits has also received considerable attention, with optimal algorithms proposed in \cite{context1-li2022instance, context2-kato2021role}. In these works, however, the context is assumed to be observed before taking an action, which differs from our setting. A more closely related line of research, recently explored, involves the bandit problem with mediator feedback. This setting is similar to our separator context setting but investigates different objectives \cite{mediator-poiani2023pure, mediator2-reddy2023best, mediator-cum-eldowa2024information}.







\paragraph{Causal Bandits.}
Another line of research in bandits, where the agent receives additional information beyond the reward, is the field of \textit{causal bandits}, first introduced in \cite{lattimore2016causal} and subsequently explored in \cite{causal-context-madhavan2024causal, causal-fateme-jamshidi2024confounded, causal-fairness-huang2022achieving, causal-pure-xiong2022combinatorial}. In causal bandit problems, a (known or unknown) causal graph is assumed among a set of variables, and the actions correspond to interventions on a subset of variables within the graph. After choosing each intervention (i.e., action), the agent observes the values of all nodes, with the goal of identifying interventions that maximize the expected value of a reward node.


In this paper, we specifically study the best-arm identification problem for two particular graphs with three variables in a causal bandit setting. Related works in the causal bandit literature include settings similar to the separator context, such as the \textit{conditionally benign} environment explored in \cite{causal-benign-bilodeau2022adaptively, causal-pareto-liu2024causal}. However, these works primarily address cumulative or simple regret minimization in a worst-case setting, whereas we focus on best-arm identification under fixed confidence.


\paragraph{Linear Bandits.}
The separator setting of our problem can be reduced to a linear bandit problem through the following steps. We can disregard the context variable and assume that the reward value of action $i$ is drawn from a distribution with mean $\A_i^T \bmu_i$ and a zero-mean noise $\epsilon_i$. Note that in this case, the mean of this new variable is the same as the mean reward for action $i$, and if the reward means are bounded, the noise becomes sub-Gaussian (for more details on this reduction, see \cite{causal-pareto-liu2024causal}). 

In this setting, action $i$ corresponds to a point $\A_i$ in $\mathbb{R}^k$, resulting in a best-arm identification problem in linear bandits with finitely many arms. This problem was first introduced in \cite{linear-bai-soare2014best} and has been widely studied. Optimal algorithms for best arm identification in linear bandits are provided in \cite{linear-lazy-jedra2020optimal, linear-degenne2020gamification, linear-mohammadi22improved}. In \cite{linear-lazy-jedra2020optimal}, the authors propose an algorithm inspired by track and stop, called lazy track and stop, while \cite{linear-degenne2020gamification} presents an algorithm based on game-theoretic intuition. For additional algorithms on best arm identification in linear bandits, see \cite{linear-elimination-xu2018fully, linear-transductive-fiez2019sequential}. The causal bandit with linear assumption on the causal model is also been studied recently \cite{causal-linear1-varici2023causal, causal-linear2-yan2024robust}. 

\paragraph{Best Policy Identification in RL.}
Another direction related to our problem is the best-policy identification problem in reinforcement learning, which has gained considerable attention in recent years \cite{rl1-menard2021fast, rl2-al2021navigating, rl3-wagenmaker2022beyond, rl4-al2023towards}. In best-policy identification, an agent interacts with an environment consisting of actions and states, and the goal is to select, from a given set of policies, the one that maximizes the expected reward. This problem generalizes best-arm identification in bandits, and many problems in the bandit literature can be viewed as special cases of this framework. For instance, in best-arm identification in contextual bandits, if we treat different contexts as distinct starting states, set the time horizon $H=1$, and consider the actions as the set of policies, the algorithms for best-policy identification can solve the problem; for a detailed discussion, see Section~6 of \cite{rl-difference-estimation-narang2024sample}.


One might consider reducing a bandit with post-action contexts to an RL setting by treating post-action contexts as states. However, this approach does not apply to either the non-separator or the separator settings. The key difference lies in how the reward is determined: in RL, the reward depends on the previous action and the state from which the action is taken. In contrast, in our problem, the reward depends on the last action and the new state reached after the action (non-separator) or only on the new state (separator). This fundamental difference prevents a direct application of RL algorithms to solve our problem.


\textbf{Pure exploration with safety constraints.} 
The classic best-arm identification algorithm fails to address many problems in which constraints on action selection arise for critical reasons. Such constraints are often used to ensure safety in learning algorithms and can restrict different phases of learningâ€”whether in exploration or recommendation. Furthermore, constraints can be known or unknown, depending on the application. Prior work, including \cite{wang2021fairness, tang2024pure, faizal2022constrained, camilleri2022active}, explores best-arm identification under various settings.


\cite{carlsson2024pure} also tackles a related problem, but with the key distinction that the goal is to find an optimal policyâ€”a distribution over actionsâ€”rather than identifying a single arm. In fact, \cite{carlsson2024pure} is the closest work to ours, although they can choose any action (i.e., setting $Z$ to a specific value in our setting) during exploration. This flexibility is not available in our setting, constituting the primary difference from their work.


\section{Proof of General Lower bound: proposition \ref{th: general_lower1}} \label{apd: general bound}
    The proof employs the transportation lemma as outlined in Lemma 1 of \cite{kaufmann2016complexity}. The proof of the transportation lemma, which can be found in Appendix A and A.1 \cite{kaufmann2016complexity}, relies on changes of distributions and the log-likelihood ratio expression. Extending this theorem to our setup is quite straightforward; we simply need to incorporate the context variables into the log-likelihood ratio formula, which means using    
    $$
        L_t = \sum_{a = 1}^{n} \sum_{s = 1}^{N^{X}_a(t)} \log \left ( \frac{f_a(Z_{a,s}, Y_{a,s})}{f'_a(Z_{a,s}, Y_{a,s})} \right),
    $$
    instead of 
    $$
        L_t = \sum_{a = 1}^{n} \sum_{s = 1}^{N^{X}_i(t)} \log \left ( \frac{f_a(Y_{a,s})}{f'_a(Y_{a,s})} \right),
    $$
    in Appendix A of \cite{kaufmann2016complexity}. All the proofs of the transportation lemma remain unchanged. Therefore, based on the transportation lemma for our setting, for each $\bmu' \in \text{Alt}(\bmu, \A)$, we have  

    $$
        \sum_{i = 1}^{n} \expec {N^{X}_{i}(\taudel)} d(P^{\bmu}_{i}, P^{\bmu'}_{i}) \geq \sup_{\E \in \mathcal{F}_{\taudel}} d (\pr_{\bmu}(\E), \pr_{\bmu'}(\E)).
    $$
    Define event $\E' = \{\taudel < \infty, \hat{i}_{\taudel} \neq i^*(\bmu)\}$, then $\pr_{\bmu}(\E') \geq 1 - \delta$ and  $\pr_{\bmu'}(\E') \leq \delta$ according to $\delta$-correctness property of the given algorithm. Therefore, based on the property of KL divergence, we have
    $$
        \sup_{\E \in \mathcal{F}_{\taudel}} d (\pr_{\bmu}(\E), \pr_{\bmu'}(\E)) \geq  d (\pr_{\bmu}(\E'), \pr_{\bmu'}(\E')) \geq d_{B} (\delta, 1 - \delta),
    $$
    which implies that
    $$
        \sum_{i = 1}^{n} \expec {N^{X}_{i}(\taudel)} d(P^{\bmu}_{i}, P^{\bmu'}_{i}) \geq d_{B} (\delta, 1 - \delta).
    $$
    Therefore, 
    $$ 
       \inf_{\bmu' \in \text{Alt}(\bmu, \A)} \sum_{i = 1}^{n} \expec {N^{X}_{i}(\taudel)} d(P^{\bmu}_{i}, P^{\bmu'}_{i}) \geq d_{B} (\delta, 1 - \delta).
    $$
    Then, we have    
    \begin{align*}
        \inf_{\bmu' \in \text{Alt}(\bmu, \A)} \sum_{i = 1}^{n} \expec {N^{X}_{i}(\taudel)} d(P^{\bmu}_{i}, P^{\bmu'}_{i}) &= \expec{\taudel}  \inf_{\bmu' \in \text{Alt}(\bmu, \A)} \sum_{i = 1}^{n} \frac{\expec {N^{X}_{i}(\taudel)}}{\expec{\taudel}} d(P^{\bmu}_{i}, P^{\bmu'}_{i}) \\
              & \leq \expec{\taudel} \sup_{\mathbf{w} \in \Delta^{n-1}} \inf_{\bmu' \in \text{Alt}(\bmu, \A)} \sum_{i \in [n]} w_i d(P^{\bmu}_{i}, P^{\bmu'}_{i}). 
    \end{align*}
    Finally,
    $$
        \expec{\taudel}    \geq \bigg ( \sup_{\mathbf{w} \in \Delta^{n-1}} \inf_{\bmu' \in \text{Alt}(\bmu, \A)} \sum_{i = 1}^{n} w_i d(P^{\bmu}_{i}, P^{\bmu'}_{i}) \bigg )^{-1} d_{B} (\delta, 1 - \delta)  
    $$
    which concludes the proof.




\section{Generalized Likelihood Ratio Test}\label{apd: GLR}

    The Generalized Likelihood Ratio (GLR) is used to measure our confidence in a hypothesis. Let have $\Omega_0$ and $\Omega_1$ be two model spaces. We wish to know whether a model $\lambda$ belongs to $\Omega_0$ or $\Omega_1$. To do this, we can define two hypotheses: (i) $H_0: (\lambda \in \Omega_0)$ and (ii) $H_1: (\lambda \in \Omega_1$). The log-likelihood ratio of this test after $t$ observations is defined as 
        \begin{equation}\label{eq: def-log-likeli}
              \lamht \triangleq \log \frac{\sup_{\lambda \in \Omega_{0} \cup \Omega_{1}} \lcal(X_1, X_2, \dots, X_t; \lambda)}{\sup_{\lambda \in \Omega_{0}} \lcal(X_1, X_2, \dots, X_t; \lambda)},
        \end{equation}
        where $\lcal(X_1, X_2, \dots, X_t; \lambda)$ is the likelihood function of observations (i.e., $X_1, X_2, \dots, X_t$) associated to parameter $\lambda$. If $\lamht$ has a high value, we could reject the hypothesis $\hcal_0$.


    Our problem in both settings can be viewed as a \emph{general identification}  \cite{kaufmann2021mixture}. In this problem, we assume $\I$ is partitioned into $n$ subsets $\ocal_1, \ocal_2, \ldots, \ocal_n$, and the goal is to determine which subset contains vector $\bmu$. At round $t$, we have an estimate $\hat{\bmu}(t)$ and we need to decide whether we can confidently choose the correct subset with low risk. Refer to Section 4 of \cite{kaufmann2021mixture} for more details. In our problem, we can define the subsets    
    
    \begin{align*}
        \mathcal{O}_i \triangleq \left\{ \bmu \in \I \ \big| \A_i^\top \bmu_i > \A_j^\top \bmu_j, \ \forall j \neq i \right\}.
    \end{align*}

    Suppose that at round $t$ of our problem, $\istarmut$ is not unique, meaning $\muht$ is not in $\I$. In this case, we lack sufficient information to determine the best arm (or, find the correct subset in general identification problem), so we must continue sampling. If $\istarmut$ indicates a unique arm, however, we need a measure to evaluate our confidence. We consider the following hypothesis test.
        \begin{align*}
              &\hcal_0: (\istarmu \neq \istarmut),   \\
              &\hcal_1: (\istarmu = \istarmut) ,
        \end{align*}
    which is equivalent to
         \begin{align*}
              &\hcal_0: (\bmu \notin \ocal_{\istarmut}) =  (\bmu \in \text{Alt}(\muht, \A)),  \\
              &\hcal_1: (\bmu \in \ocal_{\istarmut}).
        \end{align*}
    Hence, if $\lamht$ is high, we can reject the $\hcal_0$, indicating with high confidence that \(\istarmut\) is indeed the best arm
    
    Note that at round $t$, the observed samples are $((Z_1, Y_1), \dots, (Z_t, Y_t))$ for the chosen actions $(i_1, \dots, i_t)$. Consequently,     
        $$
            \lamht = \log \frac{\sup_{\bmu \in I} \lcal((Z_1, Y_1), \dots, (Z_t, Y_t); \bmu)}{\sup_{\bmu \in \text{Alt}(\muht, \A)} \lcal((Z_1, Y_1), \dots, (Z_t, Y_t); \bmu)},
        $$
    which expands to
        \begin{align}\label{eq: glr-context}    
                  & \log \frac{\sup_{\bmu' \in I} \prod_{s \in [t]} \pr(Z = Z_s \mid X = i_s) \pr_{\bmu'} (Y = Y_s \mid X = i_s, Z = Z_s)}{\sup_{\bmu' \in \text{Alt}(\muht, \A)} \prod_{s \in [t]} \pr(Z = Z_s \mid X = i_s) \pr_{\bmu'} (Y = Y_s \mid X = i_s, Z = Z_s)} \nonumber \\
                  =  & \log \frac{\sup_{\bmu' \in I} \prod_{s \in [t]} \A_{Z_s, i_s} \pr_{\bmu'} (Y = Y_s \mid X = i_s, Z = Z_s)}{\sup_{\bmu' \in \text{Alt}(\muht, \A)} \prod_{s \in [t]} \A_{Z_s, i_s} \pr_{\bmu'} (Y = Y_s \mid X = i_s, Z = Z_s)} \nonumber \\
                  = & \log \frac{\sup_{\bmu' \in I} \prod_{s \in [t]} \pr_{\bmu'} (Y = Y_s \mid X = i_s, Z = Z_s)}{\sup_{\bmu' \in \text{Alt}(\muht, \A)} \prod_{s \in [t]} \pr_{\bmu'} (Y = Y_s \mid X = i_s, Z = Z_s)} 
        \end{align}
    
    In the following two parts, for each case of context, we derive a simpler expression for $\lamht$ under the non-separator and separator context.
    
    \subsection{Non-separator context}\label{apd: glr-non-sep}
        Note that in this case, $\pr_{\bmu} (Y = Y_s \mid X = i_s, Z = Z_s)$ in not necessary equal to $\pr_{\bmu} (Y = Y_s \mid Z = Z_s)$. By assumption, $Y \mid X = i_s, Z = Z_s$ follows a Gaussian distribution with unit variance, which is a one-parameter exponential family. For this case, we can simplify Equation \eqref{eq: glr-context} to
   
        \begin{align*}
             \lamht &= \inf_{\bmu' \in \text{Alt}(\muht, \A)} \sum_{i \in [n], j \in [k]} N_{j,i}(t) d(\N(\hat{\bmu}_{j,i}(t), 1), \N(\bmu'_{j,i}, 1)) \\
                    &= \inf_{\bmu' \in \text{Alt}(\muht, \A)} \sum_{i \in [n], j \in [k]} N_{j,i}(t) \frac{(\hat{\bmu}_{j,i}(t) - \bmu'_{j,i})^2}{2}
        \end{align*}
        where $\N(x, 1)$ denotes Gaussian distribution with mean $x$ and unit variance and we used the fact $d(\N(x, 1), \N(y, 1)) = \frac{(x - y)^2}{2}$.

        \textbf{Computation of $\lamht$}.
            Recall that Alt$(\bmu, \A) = \cup_{i \neq \istarmu} \C_i$,  where $\C_i = \{ \bmu' \in \I \mid \A_i^\top \bmu'_i > \A_{\istarmu}^\top \bmu'_{\istarmu} \}$. Thus, we can rewrite $\lamht$ as
            $$
                \lamht = \min_{s \neq  \istarmut} \inf_{\bmu' \in \C_s} \sum_{i \in [n], j \in [k]} N_{j,i}(t) \frac{(\hat{\bmu}_{j,i}(t) - \bmu'_{j,i})^2}{2},
            $$
            which is equal to
            \begin{align*}
                &\inf_{\bmu' \in \I} \sum_{i \in [n], j \in [k]} N_{j,i}(t) \frac{(\hat{\bmu}_{j,i}(t) - \bmu'_{j,i})^2}{2} \\
                & \text{s.t.} \quad \A_s^T \bmu'_s > \A_{\istarmu}^T \bmu'_{\istarmu}.
            \end{align*}
            Note that $N_{j,i}(t) > 0$ for all $i\in [n]$ and $j \in [k]$ due to initialization phase. \(\I\) is dense in \(\mathbb{R}^{k \times n}\). Hence, we can replace the constraint \(\bmu' \in \I\) with \(\bmu' \in \mathbb{R}^{k \times n}\), thus making
            \begin{align*}
                &\inf_{\bmu' \in \mathbb{R}^{k \times n}} \sum_{i \in [n], j \in [k]} N_{j,i}(t) \frac{(\hat{\bmu}_{j,i}(t) - \bmu'_{j,i})^2}{2} \\
                & \text{s.t.} \quad \A_s^T \bmu'_s > \A_{\istarmu}^T \bmu'_{\istarmu}.
            \end{align*}
            Because the objective function is continuous and we are taking infimum, we can replace the strict inequality constraint with a non-strict one
            \begin{align*}
                &\inf_{\bmu' \in \mathbb{R}^{k \times n}} \sum_{i \in [n], j \in [k]} N_{j,i}(t) \frac{(\hat{\bmu}_{j,i}(t) - \bmu'_{j,i})^2}{2} \\
                & \text{s.t.} \quad \A_s^T \bmu'_s \geq \A_{\istarmu}^T \bmu'_{\istarmu}.
            \end{align*} 
            We solve this using the Lagrangian
            \begin{equation}\label{eq: lag-glr-non}
                   L(\bmu', \lambda) = \sum_{i \in [n], j \in [k]} N_{j,i}(t) \frac{(\hat{\bmu}_{j,i}(t) - \bmu'_{j,i})^2}{2}+ \lambda(\A_{\istarmu}^T \bmu'_{\istarmu} - \A_s^T \bmu'_s).
            \end{equation}

            Taking derivatives and simplifying, we obtain
            \begin{align*}
                \bmu'_{j. i} = \begin{cases}
                                    \hat{\bmu}_{j,\istarmu}(t) - \frac{\lambda \A_{j, \istarmu}}{N_{j, \istarmu}(t)} &    i = \istarmu, \\
                                    \hat{\bmu}_{j,s}(t) + \frac{\lambda \A_{j, s}}{N_{j, s}(t)}      &    i = s, \\
                                    \hat{\bmu}_{j,i}(t)        &    \text{o.w.}                        
                                \end{cases}       
             \end{align*}
            Substituting $\bmu'_{j,i}$ back into \eqref{eq: lag-glr-non} yields
                \begin{align*}
                    \lambda^* = \frac{\hat{\Delta}_{s}}{\sum_{j \in [k]} \frac{\A_{j, \istarmut}^2}{N_{j,\istarmut}(t)}  + \frac{\A_{j, s}^2}{N_{j,s}(t)}}.
                \end{align*}
            Hence,
                 \begin{align*}
                    \inf_{\bmu' \in \C_s} \sum_{i \in [n], j \in [k]} N_{j,i}(t) \frac{(\hat{\bmu}_{j,i}(t) - \bmu'_{j,i})^2}{2} =  \frac{\hat{\Delta}_{s}^2}{2 \sum_{j \in [k]} \frac{\A_{j, \istarmut}^2}{N_{j,\istarmut}(t)}  + \frac{\A_{j, s}^2}{N_{j,s}(t)}},
                \end{align*}
            Finally, the simplified form of $\lamht$ becomes
                \begin{align*}
                      \lamht = \min_{s \neq  \istarmut} \frac{\hat{\Delta}_{s}^2}{2 \sum_{j \in [k]} \frac{\A_{j, \istarmut}^2}{N_{j,\istarmut}(t)}  + \frac{\A_{j, s}^2}{N_{j,s}(t)}},
                \end{align*}
            
    \subsection{Separator context}\label{apd: glr-sep}
        In the separator case, $\pr_{\bmu} (Y = Y_s \mid X = i_s, Z = Z_s) = \pr_{\bmu} (Y = Y_s \mid Z = Z_s)$ holds. Thus,  
        $$
            \lamht  = \log \frac{\sup_{\bmu' \in I} \prod_{s \in [t]} \pr_{\bmu'} (Y = Y_s \mid Z = Z_s)}{\sup_{\bmu' \in \text{Alt}(\muht, \A)} \prod_{s \in [t]} \pr_{\bmu'} (Y = Y_s \mid Z = Z_s)} 
        $$
        In Section \ref{sec: sep}, we assume $\bmu$ is a vector in $\mathbb{R}^k$, where $\bmu_i = \expec{Y \mid Z = i}$. Similar to the non-separator setting, because $Y \mid Z = i_s$ is a Gaussian variable with unit variance, then obtain
        \begin{align*}
            \lamht &= \inf_{\bmu' \in \text{Alt}(\muht, \A)} \sum_{j \in [k]} \numcontext d(\N(\hat{\bmu}_{j}(t), 1), \N(\bmu'_{j}, 1)) \\
                  &= \inf_{\bmu' \in \text{Alt}(\muht, \A)} \sum_{j \in [k]} \numcontext \frac{(\hat{\bmu}_{j}(t) - \bmu'_{j})^2}{2}.
        \end{align*}


        \textbf{Computation of $\lamht$}.         
        Let $\Is(\A)$ be the set of vectors in $\mathbb{R}^k$ that ensure a unique best arm in the separator setting with context probability matrix $\A$. The computation here is similar to that in the non-separator setting. Another expression for $\lamht$ is  
            \begin{align*}
                 \lamht = \min_{s \neq  \istarmut} \inf_{\bmu' \in \C_s} \sum_{j \in [k]} \numcontext \frac{(\hat{\bmu}_{j}(t) - \bmu'_{j})^2}{2}.
            \end{align*}
            where, in this case, $\C_s = \{\bmu' \in \Is \mid \A_s^T \bmu' > \A_{\istarmu}^T \bmu' \}$. Since $\Is$ is dense in $\mathbb{R}^{k}$ and the objective function is continuous, the 
            inner term is equivalent to       
            \begin{align*}
                &\inf_{\bmu' \in \mathbb{R}^{k}} \sum_{j \in [k]} \numcontext \frac{(\hat{\bmu}_{j}(t) - \bmu'_{j})^2}{2} \\
                & \text{s.t.} \quad \A_s^T \bmu' > \A_{\istarmu}^T \bmu'.
            \end{align*}
            Because of the initialization phase, $\numcontext > 0$ for all $j \in [k]$. We solve this using the Lagrangian method 
            \begin{equation*}
                   L(\bmu', \lambda) = \sum_{j \in [k]} \numcontext \frac{(\hat{\bmu}_{j}(t) - \bmu'_{j})^2}{2} + \lambda(\A_{\istarmu}^T \bmu' - \A_s^T \bmu').
            \end{equation*}
            After differentiation and some algebra, we obtain  
            \begin{align*}
                \bmu^{*}_{j} &= \hat{\bmu}_{j}(t) -  \frac{\lambda^*(\A_{j, \istarmu} - \A_{j, s})}{N_j(t)}               \\
                \lambda^* &= \frac{\hat{\Delta}_{s}}{\sum_{j \in [k]} \frac{(\A_{j, \istarmut} - \A_{j, s})^2}{\numcontext}}.                            
             \end{align*}
             Thus,
             \begin{align*}
                 \lamht = \min_{s \neq  \istarmut} \frac{\hat{\Delta}^2_{s}}{2\sum_{j \in [k]} \frac{(\A_{j, \istarmut} - \A_{j, s})^2}{\numcontext}}.
             \end{align*}   

\section{Proofs of Section \ref{sec: non-sep}} \label{apd: non-sep proofs}

The definition of $C^g$ from \cite{kaufmann2021mixture} is as follows.

\begin{definition}\label{def: c-g function}
   The function $C^g$ is defined as
    \begin{align*}
            C^g(x) \coloneqq \min_{\lambda \in (\frac12, 1]} \frac{g(\lambda) + x}{\lambda},
    \end{align*}
          where function $g: \left(\frac{1}{2}, 1\right] \rightarrow \mathbb{R}$ is given by
    \begin{align*}
            g(\lambda) = 2\lambda - 2\lambda\ln(4\lambda) + \ln(\zeta(2\lambda)) - \frac12 \ln(1 - \lambda),          
    \end{align*}
    and $\zeta$ denotes the Riemann zeta function, defined as $\zeta(s) = \sum_{n=1}^{\infty} n^{-s}$.
\end{definition}



\subsection{Proof of Theorem \ref{thm : non-sep lower}}
We prove this theorem by explicitly solving the optimization problem \eqref{eq: general_fi} for the non-separator setting and incorporating the solution into the general lower bound formula \eqref{eq: general_lower2}.
\begin{restatable}{lemma}{nonsepfi} \label{lem : non-sep fi} 
            Consider a bandit instance with a non-separator context and Gaussian reward distribution with unit variance, parameterized by matrices $\bmu$ and $\A$. For any weight vector $\mathbf{w}$ such that $\forall s \in [n]: w_s > 0$, the value of $\fiwmu = \inf_{\bmu' \in \C_i} \sum_{s \in [n]} w_s d(P^{\bmu}_{s}, P^{\bmu'}_{s})$ can be explicitly determined as
            \begin{align*}
                \fiwmu = \frac{\Delta_{i}^2}{2}\left( \frac{w_{i^*(\bmu)}w_i}{w_{i^*(\bmu)} + w_i}\right).  
            \end{align*}
\end{restatable}

\begin{proof}
            First note that $P^{\bmu}_{s}$ and $P^{\bmu'}_{s}$ are the distribution of random vectors $(Z,Y)$ and $(Z',Y')$ corresponding to bandit instances with context probability matrix $\A$ and means matrix $\bmu$ and $\bmu'$, respectively. Since reward distributions are Gaussian with unit variance, the KL divergence can be written as            
            \begin{align*}
                d(P^{\bmu}_{s}, P^{\bmu'}_{s}) = &\sum_{j \in [k]} P^{\bmu}_{s}(Z = j) d(P^{\bmu}_{s} (Y \mid Z = j), P^{\bmu'}_{s} (Y \mid Z = j)) \\
                                         =&\sum_{j \in [k]} \A_{j, s} d(\N(\mu_{j,s} , 1), \N(\mu'_{j,s}, 1))= \sum_{j \in [k]} \A_{j,s} \frac{(\mu_{j,s} - \mu'_{j, s})^2}{2}. 
            \end{align*}
            
            Recall the definition 
            $$\C_i = \left\{ \bmu' \in \I \big| \A_i^T\bmu'_i > \A_{i^*(\bmu)}\bmu'_{i^*(\bmu)} \right\}.$$
            
           Note that $\sum_{s \in [n]} w_s d(P^{\bmu}_{s}, P^{\bmu'}_{s})$ is continuous with respect to $\bmu$ and $\I$ is dense in $\mathbb{R}^{k \times n}$. Therefore, we can express $\fiwmu$ as the solution to the following optimization problem
            \begin{align*}
                \fiwmu = &\inf_{{\bmu' \in \mathbb{R}^{k \times n}}} \sum_{s \in [n]} w_s \sum_{j \in [k]} \A_{j,s}  \frac{(\mu_{j,s} - \mu'_{j,s})^2}{2} \\
                & \text{s.t.} \quad \A_i^T \bmu'_i > \A_{\istarmu}^T \bmu'_{\istarmu}.
            \end{align*}
            The solution to this problem is equivalent to the solution to the following problem when $\alpha \rightarrow 0$,
            \begin{align*}
                &\inf_{\bmu' \in \mathbb{R}^{k \times n}} \sum_{s \in [n]} w_s \sum_{j \in [k]} \A_{j,s}  \frac{(\mu_{j,s} - \mu'_{j,s})^2}{2} \\
                & \text{s.t.} \quad \A_i^T \bmu'_i \geq \A_{\istarmu}^T \bmu'_{\istarmu} + \alpha.
            \end{align*}
            To solve the second problem, we write the Lagrangian as 
            \begin{align*}
                L(\bmu', \lambda) = \sum_{s \in [n]} w_s \sum_{j \in [k]} \A_{j,s}  \frac{(\mu_{j,s} - \mu'_{j,s})^2}{2} + \lambda(\A_{\istarmu}^T \bmu'_{\istarmu} - \A_i^T \bmu'_i + \alpha).
            \end{align*}
            By calculating the derivative of $L$ with respect to each variable and performing some algebra, we derive the optimal values for $\bmu'^*, \lambda^*$: 
            \begin{align*}
                \bmu'^*_{j,t} = 
                \begin{cases}
                    \mu_{j, \istarmu} - \frac{\lambda}{w_{\istarmu}} \quad &t = \istarmu, \\ 
                    \mu_{j,i} + \frac{\lambda}{w_i} \quad & t=i, \\
                    \mu_{j,t} \quad & o.w.
                \end{cases}
            \end{align*}
            \begin{align*}
                \lambda^* = \frac{\Delta_i + \alpha}{\left( \frac{w_{\istarmu} + w_i}{w_{\istarmu} w_i}\right)}. 
            \end{align*}
            By inserting these values into the objective function, we have
            \begin{align*}
                \fiwmu = \lim_{\alpha \rightarrow 0} \frac{(\Delta_i + \alpha)^2}{2} \left( \frac{w_{\istarmu} w_i}{w_{\istarmu} + w_i} \right) = \frac{\Delta_i^2}{2} \left( \frac{w_{\istarmu} w_i}{w_{\istarmu} + w_i} \right),
            \end{align*}
            which completes the proof.
        \end{proof}
    
    For a given $\wb \in \Delta^{n-1}$, if $w_i = 0$ for some $i \in [n]$, then we can find $\bmu'$ such that $\pr (P^{\bmu'}_{s}) = \pr (P^{\bmu}_{s})$ for all $s \in [k]$ and also $i^*(\bmu) \neq i^*(\bmu)$. Therefore, the term $\inf_{\bmu' \in \text{Alt}(\bmu, \A)} \sum_{i \in [n]} w_i d(P^{\bmu}_{i}, P^{\bmu'}_{i})$ is equal to zero. Since we want to find $\wb \in \Delta^{n-1}$ that maximizes
        \begin{align*}
               \inf_{\bmu' \in \text{Alt}(\bmu, \A)} \sum_{i \in [n]} w_i d(P^{\bmu}_{i}, P^{\bmu'}_{i}),
        \end{align*}
    we must restrict ourselves to \(\wb\) with strictly positive entries. By combining Lemma \ref{lem : non-sep fi} and Equation \eqref{eq: general_lower2}, we conclude Theorem \ref{thm : non-sep lower}.

% \nonsepw*

% \begin{proof}
%         To prove the uniqueness of $w^*$, note that the left-hand side of the Equation \eqref{eq: non-sep w1} is strictly decreasing in the interval $(\frac{1}{\Delta_{min}^2}, \infty)$ and its value is in infinity tends to zero and in values close to $\frac{1}{\Delta_{min}^2}$ is infinity, so it has a unique solution. 
%         To prove the second part of the lemma, assume that the vector $\mathbf{w}^*$ is a solution to optimization problem \eqref{eq: middle non-sep LB}. We show that the value of $\frac{\Delta_{i}^2}{2}\left( \frac{w_{i^*(\bmu)}^*w_i^*}{w_{i^*(\bmu)}^* + w_i^*}\right)$ is equal for all $i \neq \istarmu$. We show it by contradiction. 
%         Define $g(w^*_i) = \frac{\Delta_{i}^2}{2}\left( \frac{w_{i^*(\bmu)}^*w_i^*}{w_{i^*(\bmu)}^* + w_i^*}\right)$ and assume without loss of generality that the maximum of $g(w^*_i)$s occurs for $i=1$ and there are $r$ indices $i_1, i_2, \ldots, i_r$ with $g(w^*_{i_1}) = g(w^*_{i_2}) = \ldots = g(w^*_{i_r}) = \min_{i \neq \istarmu}g(w^*_{i})$. Now we design a new vector $\mathbf{w}' \in \Delta^{n-1}$ with the following entries
%         \begin{align*}
%             g(w'_i) = 
%             \begin{cases}
%                 w^*_1 - \epsilon \quad & i=1 \\
%                 w^*_{i_j} + \frac{\epsilon}{r} \quad & i = i_j \\
%                 w^*_i \quad & \text{otherwise}, 
%             \end{cases}
%         \end{align*}
%         where $\epsilon > 0$ is a small real number. Regarding the fact that $g$ is increasing and for small enough values of $\epsilon$, we obtain that $\min_{i\neq \istarmu} g(w'_i) > \min_{i\neq \istarmu} g(w^*_i)$, so $\mathbf{w'}$ is a better solution to the problem in \eqref{eq: middle non-sep LB} which is a contradiction and shows that for each $i \neq \istarmu$, we have $g(w^*_i) = c$ for a constant value $c$. 
%         So we need to find the maximum value of $c$ such that there exists a weight vector $\mathbf{w}$ with 
%         \begin{align*}
%             \frac{\Delta_{i}^2}{2}\left( \frac{w_{i^*(\bmu)}w_i}{w_{i^*(\bmu)} + w_i}\right) = c \quad \forall i \neq \istarmu. 
%         \end{align*}
%         We define a new vector $\mathbf{w}'$ as $\mathbf{w}' \coloneqq \frac{\mathbf{w}}{2c}$. Then for all $i \neq \istarmu$, $\frac{\Delta_{i}^2}{2}\left( \frac{w_{i^*(\bmu)}'w_i'}{w_{i^*(\bmu)}' + w_i'}\right) = \frac12$. This implies that $w'_i = \frac{w'_{\istarmu}}{w'_{\istarmu}\Delta_i^2 -1}$ and the fact that $\mathbf{w}$ is a weight vector translates to the following equation for $\mathbf{w}'$:
%         \begin{align*}
%             w'_{\istarmu} + \sum_{i \neq \istarmu} \frac{w'_{\istarmu}}{w'_{\istarmu} \Delta_i^2 - 1} = \frac12.
%         \end{align*} 
%         Now note that finding the maximum value of $c$ for which there exists a vector $\mathbf{w'}$, is equal to finding the solution to the optimization problem
%         \begin{align*}
%             \min_{w'_{\istarmu} > \frac{1}{\Delta_{min}^2}} w'_{\istarmu} + \sum_{i \neq \istarmu} \frac{w_{\istarmu}}{w_\istarmu \Delta_i^2 - 1},
%         \end{align*}
%         and the solution to this problem gives the optimal value of $w'_{\istarmu}$. 
        
%         To solve this problem, we derive the derivative of this function with respect to $w'_{\istarmu}$ and consider it equal to zero which leads to the equation
%         \begin{align*}
%             \sum_{i \neq \istarmu} \frac{1}{(w'_{\istarmu} \Delta_i^2 - 1)^2} = 1.
%         \end{align*}
%         So by solving the above equation, we will find the values of $w'_{\istarmu}$, then using the equality $w'_i = \frac{w'_{\istarmu}}{w'_{\istarmu}\Delta_i^2 -1}$, the whole vector $\mathbf{w}'$ can be calculated. In the end, we know that $\mathbf{w}'$ is equal to $\mathbf{w}^*$ up to a constant multiplier $\frac{1}{2c}$ and the proof is completed.
%     \end{proof}

\subsection{Proof of Lemma \ref{lem: non-sep correctness}}

\nonsepCorrectness*


\begin{proof}
         As discussed in Appendix \ref{apd: GLR}, our problem can be viewed as a general identification, in which $\I$ is partitioned into $M$ subsets $\ocal_1, \ocal_2, \ldots, \ocal_M$. The goal is determine which subset contains vector $\bmu$. \cite{kaufmann2021mixture} provides a $\delta$-correct stopping rule for general identification based on the \textit{rank} of problem, defined as follows.         
        \begin{definition}[Rank]
            Consider a general identification problem specified by a partition $\ocal = \bigcup_{i=1}^M \mathcal{O}_i$. We say this problem has rank $R$ if for every $i \in \{1, \ldots, M\}$, we can write
            \[
            \mathcal{O} \setminus \mathcal{O}_i = \bigcup_{q \in [Q]} \left\{ \lambda \in \I \; \middle| \; (\lambda_{k^{i,q}_1}, \ldots, \lambda_{k^{i,q}_R}) \in \mathcal{L}_{i,q} \right\},
            \]
            for a family of arm indices $k^{i,q}_r \in [K]$ and open sets $\mathcal{L}_{i,q}$ indexed by $r \in [R]$, $q \in [Q]$ and $i \in [M]$. In other words, the rank is $R$ if every set $\mathcal{O} \setminus \mathcal{O}_i$ is a finite union of sets that are each defined in terms of only $R$ arms. 
        \end{definition} 
        Based on the rank of a general identification problem, \cite{kaufmann2021mixture} provides the following theorem, which shows that the GLR stopping rule is $\delta$-correct.        
        \begin{theorem}[\cite{kaufmann2021mixture}] \label{th: rank-id}
            For any identification problem of rank $R$ with $M$ partitions and Gaussian reward distributions with unit variance, the GLR stopping rule \ref{eq: non-sep stop rule} is $\delta$-correct with threshold
            \[\hat{c}_t(\delta) = 2R \ln\left(4 + \ln\left(\frac{t}{R}\right)\right) + R C^{g} \left(\frac{\ln \left( \frac{M-1}{\delta} \right) }{R} \right)
            .\]
            where \(C^{g}\) is defined in Definition \ref{def: c-g function}.
        \end{theorem}  
    
        Next, we show that the rank of the best arm identification problem in a bandit with a non-separator context is $2k$. The partitions in this problem are given by
        \begin{align*}
           \mathcal{O}_i = \left\{ \bmu \in \I \ \big| \A_i^\top \bmu_i > \A_j^\top \bmu_j, \ \forall j \neq i \right\},
        \end{align*}
        for all $i \in [n]$. It follows that
        \begin{align*}
            \I \setminus \mathcal{O}_i = \bigcup_{j \neq i} \{ \bmu \in \I \big| (\bmu_{i}, \bmu_j) \in \mathcal{L}_{i,j} \},
        \end{align*} 
        where $\mathcal{L}_{i,j} = \{ (\bmu_1, \bmu_2) \in \mathbb{R}^{2k} \big| \A_i^T \bmu_1 < \A_j^T \bmu_2 \}$. This shows that our problem has rank $R = 2k$. By applying Theorem \ref{th: rank-id} with $R = 2k$ and $M = n$ to our problem, we complete the proof.
    \end{proof}

    


\subsection{Proof of Theorem \ref{thm : non-sep upper}}

\nonsepUpperBound*

\begin{proof}
        At the beginning of the proof, we provide some important lemmas that we need.
    
    % \begin{lemma}[\cite{track-stop-garivier2016optimal}]\label{lem: non-sep converge proportions}
    %     Under the D-tracking rule for each $i \in [N]$, $N_i(t) \geq (\sqrt{t} - N/2) - 1$. Moreover, for all $\epsilon > 0$, for all $t_0$, there exists $t_\epsilon \geq t_0$ such that
    %     \begin{align*}
    %         \sup_{t \geq t_0} \infnorm{\wstar{\muht} - \wstar{\bmu}} \leq \epsilon \implies \sup_{t \geq t_\epsilon} \infnorm{\frac{N(t)}{t} - \wstar{\bmu}} \leq 3(n-1)\epsilon,
    %     \end{align*}
    %     where $\frac{N(t)}{t}$ is the vector $\left(\frac{N_1(t)}{t}, \frac{N_2(t)}{t}, \ldots, \frac{N_n(t)}{t}\right)$.
    % \end{lemma}  


    
    \begin{lemma} \label{lem: thresholds upper bound}
        There exist constants $D,E$, such that the sequential thresholds $\cdelt$ defined in \eqref{eq: non-sep threshold}, have the following property
        \begin{align*}
            \forall t > E, \forall \delta \in (0,1]: \cdelt \leq \ln \left( \frac{Dt}{\delta} \right)
        \end{align*}
    \end{lemma}
    \begin{proof}
    Recall the definition of thresholds
    \begin{equation}
        \cdelt = (4k) \ln\left( 4 + \ln\left( \frac{t}{2k} \right)\right) + (2k) C^{g} \left( \frac{\ln(\frac{n-1}{\delta})}{2k} \right).
    \end{equation}
    The function $C^g(x)$ defined in Definition \ref{def: c-g function}, has the property $C^g(x) \leq 2x + c^g(0) = 2x + c$. Therefore, we have 
    \begin{align*}
        \cdelt &\leq (4k) \ln\left( 4 + \ln\left( \frac{t}{2k} \right)\right) + 2 \ln \left( \frac{n-1}{\delta}\right) + 2kc , \\
              &= \ln\left(e^{2kc}(n-1)^2 \frac{(4 + \ln\left( \frac{t}{2k} \right))^{4k}}{\delta}\right)
    \end{align*}

    For sufficiently large \(E\), if $t > E$, then $\bigl(4 + \ln(\tfrac{t}{2k})\bigr)^{4k} \le t$. Thus, setting \(D = (n-1)^2 e^{2kc}\) completes the proof of the lemma.
    \end{proof}
    
    \begin{lemma} \label{lem: non-sep continuity}
        For each $\bmu \in \I$ and context probability matrix $\A$, the mapping $\bmu \rightarrow \mathbf{w}^*(\bmu, \A)$ is continuous.
    \end{lemma}
    \begin{proof}
        Let $\M$ be the interior of $\Delta^{n -1}$, i.e., all $\wb \in \Delta^{n-1}$ with strictly positive entries. The mapping $(\bmu', \mathbf{w}') \rightarrow \min_{i \neq i^*(\bmu)} \frac{\Delta_{i}^2}{2}\left( \frac{w_{i^*(\bmu)}w_i}{w_{i^*(\bmu)} + w_i}\right) $ is jointly continuous on $(\I \times \M)$, as it is the minimum of finitely many continuous functions.
        Because constraint set $\Delta^{n-1}$ does not depend on $\bmu'$, and $\wstar{\bmu}$ is unique, Berge's Maximum Theorem implies the continuity of the mapping $\bmu \rightarrow \wstar{\bmu}$.
    \end{proof}

    Since we analyze the expected sample complexity asymptotically, the algorithmâ€™s initialization phase contributes only a constant term with respect to $\delta$.  Note that the waiting time to collect a sample $(X = i$, $Z = j)$ follows a geometric distribution with mean $\frac{1}{\A_{j,i}}$, which is at most $\frac{1}{\amin}$. Consequently, the expected time for initialization is at most $\frac{nk}{\amin}$, which is a constant with respect to $\delta$ and therefore $\limsup_{\delta \rightarrow 0} \frac{\frac{nk}{\amin}}{\logdel} = 0$. For simplicity of the proof, we assume $t=0$ at the end of the initialization phase.



    Note that the set $\mathcal{O}_{\istarmu}$ is open. By combining this fact and Lemma \ref{lem: non-sep continuity}, for every $\epsilon > 0$, there exists a constant $\xi = \xi({\epsilon}, \bmu, \A)$ such that
    \begin{align*}
        \text{if} \infnorm{\bmu' - \bmu} \leq \xi \Rightarrow \bmu' \in \I, ~  i^*(\bmu') = i^*(\bmu), ~ \infnorm{\wstar{\bmu'} - \wstar{\bmu}} \leq \epsilon. 
    \end{align*}    

    Let $\I_{\epsilon} = \{ \bmu' \in \I \big| \infnorm{\bmu' - \bmu} \leq \xi \}$. We now define the following event and show that it occurs with high probability.

    \begin{align*}
        \eteps \triangleq \bigcap_{t = T^{1/4}}^T (\muht \in \I_{\epsilon}).
    \end{align*}
    Note that on the event $\eteps$, Algorithm \ref{algo: non-sep} does not play randomly for $t >  T^{\frac{1}{4}}$, and uses the D-tracking rule.

    \begin{lemma} \label{lem: non-sep event prob}
        There exist two constants $B$ and $C$ such that
        \begin{align*}
            \pr(\E^c_T(\epsilon)) \leq BT \exp{(-CT^{1/8})}.
        \end{align*}
    \end{lemma}
    We provide the proof in Appendix \ref{apd: proof-non-sep event prob}.
    
    We define another event 
    \begin{align*}
            \E'_T(\epsilon) \triangleq \bigcap_{t = T^{1/4}}^T \bigcap_{i \in [n], j \in [k]} \left(\left \vert N_{j,i}(t) - \A_{j, i}N^{X}_i(t) \right \vert \leq t \epsilon \right),
    \end{align*}
    which occurs with high probability.


     \begin{lemma} \label{lem: non-sep event prob2}
        There exist two constants $B'$ and $C'$ such that
        \begin{align*}
            \pr(\E'^c_T(\epsilon)) \leq B'T \exp{(-C'T^{3/8})}.
        \end{align*}
    \end{lemma}

    We provide the proof of this lemma in Appendix \ref{apd: proof-non-sep event prob2}.
        
    Here, we review an important lemma from \cite{track-stop-garivier2016optimal}, which demonstrates the effectiveness of the D-tracking rule.

    \begin{lemma} [\cite{track-stop-garivier2016optimal}, Lemma 20] \label{lem: d-tracking-sqrt}
        For each $\epsilon > 0$, there exists a constant $T_{\epsilon}$, such that for each $T \geq T_{\epsilon}$, on event $\eteps$, we have
        \begin{align*}
            \forall t \geq \sqrt{T}: \infnorm{\frac{\Nb^{X}(t)}{t} - \wstar{\bmu}} \leq 3(n-1)\epsilon.
        \end{align*}
    \end{lemma}
   


    Define $w^{*}_{j,i} \triangleq \A_{j,i}\wb^{*}_{i}(\bmu, \A)$. By the triangle inequality, we have
        \begin{align*}
            \left \vert \frac{N_{j,i}(t)}{t} - w^{*}_{j,i} \right \vert & \leq \left \vert \frac{N_{j,i}(t)}{t} - \A_{j, i} \frac{N^{X}_{i}(t)}{t} \right \vert + \left \vert \A_{j, i} \frac{N^{X}_{i}(t)}{t}  - w^{*}_{j,i}   \right \vert  \\
            &= \left \vert \frac{N_{j,i}(t)}{t} - \A_{j, i} \frac{N^{X}_{i}(t)}{t} \right \vert + \left \vert \A_{j, i} \frac{N^{X}_{i}(t)}{t} - \A_{j,i}\wb^{*}_{i}(\bmu,\A) \right \vert \\
            &\leq \left \vert \frac{N_{j,i}(t)}{t} - \A_{j, i} \frac{N^{X}_{i}(t)}{t}  \right \vert + \left \vert \frac{N^{X}_{i}(t)}{t}  - \wb^{*}_{i}(\bmu, \A) \right \vert.
        \end{align*}
    where the last inequality holds because $\A_{j,i} < 1$.   

    According to Lemma \ref{lem: d-tracking-sqrt}, on the event $\eteps \cap \etepspr$, we have
    $$
        \left \vert \frac{N_{j,i}(t)}{t} - w^{*}_{j,i} \right \vert \leq \left \vert \frac{N_{j,i}(t)}{t} - \A_{j, i} \frac{N^{X}_{i}(t)}{t}  \right \vert + \left \vert \frac{N^{X}_{i}(t)}{t}  - \wb^{*}_{i}(\bmu, \A) \right \vert \leq (3n - 2)\epsilon
    $$        
    
    Finally, On the event $\eteps \cap \etepspr$, for each $t \geq T^{\frac{1}{4}}$
    \begin{enumerate}
        \item $\muht$ stays close to $\bmu$.
        \item $i^*(\muht) = i^*(\bmu)$, hence Alt$(\muht, \A) = $ Alt$(\bmu, \A)$.
        \item $\frac{N_{j,i}(t)}{t}$ stays close to $\wb^{*}_{j,i}$ for all $i \in [n]$ and $j \in [k]$.
     \end{enumerate}
    
    Combining these properties allows us to derive a lower bound on $\lamht$. Recall the set $\mathcal{O}_i = \left\{ \bmu \in \I \ \big| \A_i^\top \bmu_i > \A_j^\top \bmu_j, \ \forall j \neq i \right\}$. We define the function $g: \mathcal{O}_{\istarmu} \times [0,1] ^{k \times n} \rightarrow \mathbb{R}$ as
    \begin{align*}
        g(\bmu', \mathbf{w}') = \inf_{\mathbf{\lambda} \in \text{Alt}(\bmu, \A)} \sum_{i \in [n], j \in [k]} w'_{j,i} \frac{(\mu'_{j,i} - \mathbf{\lambda}_{j,i})^2}{2},
    \end{align*}
    and the constant 
    \begin{align*}
        \cstareps = \inf_{\substack{\bmu' \in \I_{\epsilon} \\ \mathbf{w}': \vert w'_{j, i} - w^{*}_{j, i}  \vert \leq (3n-2) \epsilon}} g(\bmu', \mathbf{w}').
    \end{align*}


        
        Note that the mapping $(\mathbf{\lambda}, \bmu', \mathbf{w}') \rightarrow \sum_{i \in [n], j \in [k]} w'_{j,i} \frac{(\mu'_{j,i} - \mathbf{\lambda}_{j,i})^2}{2}$ is jointly continuous and the constraints set Alt$(\bmu, \A)$ is independent of $(\bmu', \mathbf{w}')$. Therefore, applying Berge's maximum theorem \cite{berge1963topological} implies that $g$ is continuous and the constant $\cstareps$ exists.
    
        With these definitions, let $T \geq T_{\epsilon}$ and event $\eteps \cap \etepspr$ holds, then for $t \geq \sqrt{T}$, $\lamht$ is equal to $g(\muht, \frac{N(t)}{t})$, implying that $\lamht \geq t \cstareps$. Using this inequality, for $T \geq T_{\epsilon}$ and on event $\eteps \cap \etepspr$
        \begin{align*}
            \min(\taudel, T) &\leq \sqrt{T} + \sum_{t = \sqrt{T}}^{T} \mathbbm{1}_{(\taudel > t)} \leq \sqrt{T} + \sum_{t = \sqrt{T}}^{T} \mathbbm{1}_{(\lamht \leq \cdelt)} \\
            & \leq \sqrt{T}  + \sum_{t = \sqrt{T}}^{T} \mathbbm{1}_{(t \cstareps \leq \cdelt)} \leq \sqrt{T} + \frac{\hat{c}_T(\delta)}{\cstareps}. 
        \end{align*}
        Defining
        \begin{align*}
            T_{\epsilon}(\delta) = \inf \left\{ T \in \mathbb{N} \bigg \vert \sqrt{T} + \frac{\hat{c}_T(\delta)}{\cstareps} \leq T \right\},
        \end{align*}
        for $T \geq \max(T_{\epsilon}, T_{\epsilon}(\delta))$, event $\eteps \cap \etepspr$ implies $\taudel \leq T$ which shows 
        \begin{align*}
            \pr_{\bmu, \A}(\taudel > T) \leq \pr(\E_T^c(\epsilon) \cup \E'^c_T(\epsilon)) \leq BT \exp(-C T^{1/8}) +  B'T \exp(-C'T^{3/8}).
        \end{align*}
        Now using this inequality and the fact that the initialization phase of our algorithm has a sample complexity of at most $\frac{nk}{\amin}$, we have
        \begin{align}        
            \mathbb{E}_{\bmu, \A}[\taudel] &\leq T_{\epsilon} +  T_{\epsilon}(\delta) + \sum_{T \geq \max(T_{\epsilon}, T_{\epsilon}(\delta))} \pr (\taudel > T ) \nonumber \\  
                           &\leq T_{\epsilon} +  T_{\epsilon}(\delta) + \sum_{T \geq \max(T_{\epsilon}, T_{\epsilon}(\delta))} \pr(\E^c_T(\epsilon) \cup \E'^c_T(\epsilon)) \nonumber \\
                           &\leq T_{\epsilon} +  T_{\epsilon}(\delta) +  \sum_{T=1}^{\infty} BT \exp(-CT^{1/8}) + \sum_{T=1}^{\infty} B'T \exp(-C'T^{3/8})  \label{eq: non-sep expected upper bound}.
        \end{align}
        In this inequality, $\sum_{T=1}^{\infty} BT \exp(-CT^{1/8}) + \sum_{T=1}^{\infty} B'T \exp(-C'T^{3/8}) $ is a constant value, $T_{\epsilon}$ is independent of $\delta$ and we provide an upper bound on the value of $T_{\epsilon}(\delta)$. We define a new constant $C(\alpha)$ for $\alpha > 0$ as
        \begin{align*}
            C(\alpha) = \inf \left\{ T \in \mathbb{N} \bigg| T - \sqrt{T} \geq \frac{T}{1 + \alpha} \right\}.
        \end{align*}
        Then using the upper bound on thresholds in Lemma \ref{lem: thresholds upper bound}, one obtains
        \begin{align*}
            T_{\epsilon}(\delta) \leq E + C(\alpha) + \inf \left\{ T \in \mathbb{N} \bigg| \frac{\ln \left( \frac{DT}{\delta} \right)}{\cstareps} \leq \frac{T}{1 + \alpha} \right\}. 
        \end{align*}
        Using Proposition $8$ in \cite{kaufmann2021mixture}, we have the following
        \begin{align*}
            \inf \left\{ T \in \mathbb{N} \bigg| \frac{\ln \left( \frac{DT}{\delta} \right)}{\cstareps} \leq \frac{T}{1 + \alpha} \right\} \leq \frac{1 + \alpha}{\cstareps} \left[ \ln \left( \frac{(1 + \alpha)D}{\cstareps \delta} \right) + \ln \left( \ln \left( \frac{(1 + \alpha)D}{\cstareps \delta} \right) + \sqrt{2 \ln \left( \frac{(1 + \alpha)D}{\cstareps \delta} \right) - 2} \right) \right]. 
        \end{align*}
        Inserting this inequality in \eqref{eq: non-sep expected upper bound}, we obtain the following for every $\alpha, \epsilon > 0$
        \begin{align*}
            \limsup_{\delta \rightarrow 0} \frac{\mathbb{E}_{\bmu, \A} [\taudel]}{\logdel} \leq \frac{1 + \alpha}{\cstareps}.
        \end{align*}
        Letting both of $\alpha$ and $\epsilon$ go to zero, by continuity of $g$,
        \begin{align*}
            \lim_{\delta \rightarrow 0} \cstareps = T^*(\bmu, \A)^{-1}
        \end{align*}
        holds which implies 
        \begin{align*}
            \limsup_{\delta \rightarrow 0} \frac{\mathbb{E}_{\bmu, \A}[\tau_{\delta}]}{\logdel} \leq T^*(\bmu, \A).
        \end{align*}     
    \end{proof}

    \subsection{Proof of Lemma \ref{lem: non-sep event prob}} \label{apd: proof-non-sep event prob}
        As a consequence of the forced exploration phase in the D-tracking algorithm, for any round $t$ with $t > n^2$, each arm has been pulled at least $\sqrt{t} - n$ times (refer to Lemma 7 in \cite{track-stop-garivier2016optimal}). Applying the union bound over $t$, we obtain
        \begin{align*}
            \pr(\E_T^c(\epsilon)) &\leq \sum_{t = T^{1/4}}^T \pr(\muht \notin I_{\epsilon})
        \end{align*}

        Similarly, we can apply the union bound for the event $\{\muht \notin I_{\epsilon}\}$ over each arm-context pair
       \begin{align*}
             \pr(\muht \notin I_{\epsilon}) \leq \sumij \pr(\vert \muh_{j,i}(t) - \mu_{j,i} \vert > \xi).
       \end{align*}
       Hence,
        \begin{align*}
            \pr(\E^c_T(\epsilon)) \leq \sum_{t = T^{1/4}}^T \sumij \pr(|\muh_{j,i}(t) - \mu_{j,i}| > \xi).
        \end{align*}
        Recall that $N_{j,i}(t)$ is the number of samples collected from $\pr(Y|X=i, Z=j)$ until round $t$. Then,
        \begin{align*}
            \pr \left(\vert \muh_{j,i}(t) - \mu_{j,i} \vert > \xi \right) &= \pr\left(\vert \muh_{j,i}(t) - \mu_{j,i} \vert > \xi \bigg| N_{j,i}(t) \geq \frac{\amin}{2} (\sqrt{t} - n) \right)  \pr\left(N_{j,i}(t) \geq \frac{\amin}{2} (\sqrt{t} - n) \right)\\
            &+  \pr \left(\vert \muh_{j,i}(t) - \mu_{j,i} \vert > \xi \bigg| N_{j,i}(t) < \frac{\amin}{2} (\sqrt{t} - n) \right)  \pr \left( N_{j,i}(t) < \frac{\amin}{2} (\sqrt{t} - n) \right) 
        \end{align*}    
        By upper bounding $\pr\left(N_{j,i}(t) \geq \frac{\amin}{2} (\sqrt{t} - n) \right) \leq 1$ and $\pr \left(\vert \muh_{j,i}(t) - \mu_{j,i} \vert > \xi \bigg| N_{j,i}(t) < \frac{\amin}{2} (\sqrt{t} - n) \right) \leq 1$, we have
        \begin{align*}
            & \pr \left(\vert \muh_{j,i}(t) - \mu_{j,i} \vert > \xi \right) \leq \pr \left( \vert \muh_{j,i}(t) - \mu_{j,i} \vert > \xi \bigg| N_{j,i}(t) \geq \frac{\amin}{2} (\sqrt{t} - n) \right) + \pr \left( N_{j,i}(t) < \frac{\amin}{2} (\sqrt{t} - n) \right).
        \end{align*}
        Both terms can be bounded using Hoeffding's inequality. Since rewards are $1$-sub-Gaussian random variables. First,
        \begin{align*}
            \pr \left(\vert \muh_{j,i}(t) - \mu_{j,i} \vert > \xi \bigg| N_{j,i}(t) \geq \frac{\amin}{2} (\sqrt{t} - n) \right) \leq 2\exp \left( - \frac{\amin (\sqrt{t} - n)\xi^2}{4} \right).
        \end{align*}
        For the second term, note that each context $j$ occurs with probability at least $\amin$ for each action $i$.
        We can write $N_{j,i}(t)$ as the sum of $N^{X}_{i}(t)$ Bernoulli random variables with parameter $\A_{j,i}$, denoted by $S_i$. Then,
        \begin{align*}
            \pr \left( N_{j,i}(t) < \frac{\amin}{2} (\sqrt{t} - n) \right) &= \pr \left( \sum_{m = 1}^{N^{X}_{i}(t)} S_m < \frac{\amin}{2} (\sqrt{t} - n) \right)
            \overset{S_m \geq 0}{\leq} \pr \left( \sum_{m = 1}^{\sqrt{t} - n} S_m < \frac{\amin}{2} (\sqrt{t} - n)\right)  \\
            &= \pr \left( \sum_{m = 1}^{\sqrt{t} - n} S_m - (\sqrt{t} - n) \A_{j, i} < \frac{\amin}{2} (\sqrt{t} - n) - (\sqrt{t} - n) \A_{j, i}\right)    \\
            &\overset{\A_{j, i} \geq \amin}{\leq} \pr \left( \sum_{m = 1}^{\sqrt{t} - n} S_m - (\sqrt{t} - n) \A_{j, i} < -\frac{\amin}{2} (\sqrt{t} - n)\right) 
        \end{align*}
        Because Bernoulli variables are $\frac{1}{2}$-sub-gaussian, Hoeffdingâ€™s inequality implies
        \begin{align*}
            \pr \left( \sum_{m = 1}^{(\sqrt{t} - n)} S_m - (\sqrt{t} - n) \A_{j, i} < -\frac{\amin}{2} (\sqrt{t} - n)\right)  \leq \exp \left( -\frac{\amin^2(\sqrt{t} - n)}{2} \right )
        \end{align*}
         Combining both bounds, we get
        \begin{align*}
             \pr(\E^c_T(\epsilon)) \leq \sum_{t = T^{1/4}}^T \sumij \pr(|\muh_{j,i}(t) - \mu_{j,i}| > \xi) 
                                   \leq \sum_{t = T^{1/4}}^T \sumij  2\exp \left( - \frac{\amin (\sqrt{t} - n)\xi^2}{4} \right) + \exp \left( -\frac{\amin^2(\sqrt{t} - n)}{2}  \right )
        \end{align*}
        Define
        \begin{align*}
            C = \min \left( \frac{\amin \xi^2}{4}, \frac{\amin^2}{2} \right), B = nk \left( 2\exp \left( \frac{n \amin \xi^2}{4} \right) + \exp \left( \frac{n \amin^2}{2} \right) \right).
        \end{align*}
        Then, we have
        \begin{align*}
            \pr(\E_T^c(\epsilon)) \leq \sum_{t = T^{1/4}}^T B \exp (-C\sqrt{t}) \leq \sum_{t = T^{1/4}}^T B \exp (-CT^{1/8}) \leq BT \exp (-CT^{1/8}),
        \end{align*}
        which concludes the proof.

     \subsection{Proof of Lemma \ref{lem: non-sep event prob2}} \label{apd: proof-non-sep event prob2}
        Note that
        \begin{align*}
            N_{j,i}(t) &= \sum_{s = 1}{t} \mathbbm{1}_{(A_t = i, Z_t = j)} \\
                       &= \sum_{m = 1}^{N^{X}_{i}(t)} \mathbbm{1}_{(Z_{t_{m}} = j)},
       \end{align*}
       where $t_m$s denotes the rounds in which the action $i$ is pulled. Indeed, $\mathbbm{1}_{(Z_{t_{m}} = j)}$ is a Bernoulli random variable with parameter $\A_{j,i}$. Since Bernoulli variables are $\frac{1}{2}$-sub-Gaussian, after applying Hoeffding's inequality, we have
        \begin{align*}
            \pr\left ( \left \vert N_{j,i}(t) - \A_{j,i} N^{X}_{i}(t)\right \vert \geq t \epsilon \right) 
            =  \pr\left ( \left \vert \sum_{m = 1}^{N^{X}_{i}(t)} \mathbbm{1}_{(Z_{t_{m}} = j))} - \A_{j,i} N^{X}_{i}(t)\right \vert \geq t \epsilon \right ) &\leq 2 \exp \left (-2\frac{t^2\epsilon^2}{N^{X}_{i}(t)} \right ) \\
            \leq 2 \exp \left (-4\frac{t^2\epsilon^2}{\sqrt{t}} \right ) = 2 \exp \left (-4t^{3/8}\epsilon^2 \right ) 
        \end{align*}
        where the last inequality is achieved by the fact $N^{X}_{i}(t) \geq \sqrt{t} - \frac{n}{2} \geq \frac{\sqrt{t}}{2}$.
       Therefore, applying the union bound on $\E'^c_T(\epsilon)$ leads to
       \begin{align*}
             \pr(\E'^c_T(\epsilon)) &\leq \sum_{t = T^{1/ 4}}^T \sum_{i \in [n], j \in [k]}        \pr\left ( \left \vert N_{j,i}(t) - \A_{j,i} N^{X}_{i}(t)\right \vert \geq t \epsilon \right) \\ 
                            &\leq \sum_{t = T^{1/ 4}}^T \sum_{i \in [n], j \in [k]} 2 \exp \left (-4 t^{1.5}\epsilon^2 \right ) \\
                            &\leq 2nkT \exp \left (-4 T^{3/8}\epsilon^2 \right ).
       \end{align*} 
       Setting $B' = 2nk$ and $C' = 4\epsilon^2$, completes the proof.
    
        
\section{Proofs of Section \ref{sec: sep}} \label{apd: sep proofs}

We define $\Is(\A)$ as the set of vectors in $\mathbb{R}^k$ that ensure a unique best arm in the separator setting with context probability matrix $\A$.



\subsection{Proof of Theorem \ref{thm : sep lower}}
    
Similar to the non-separator case, we prove this theorem by explicitly solving the optimization problem \eqref{eq: general_fi} for the separator setting and inserting this solution into the general lower bound formula \eqref{eq: general_lower2}. Note that in contrast to the non-separator case, an action can obtain zero proportion in optimal weights.

\begin{restatable}{lemma}{sepfi}\label{lem: sepfi}
            Consider a bandit instance with a non-separator context and Gaussian reward distribution with unit variance, parameterized by matrices $\bmu$ and $\A$. For any weight vector $\mathbf{w}$ with non-zero indices, the value of $\fiwmu = \inf_{\bmu' \in \C_i} \sum w_i d(P^{\bmu}_{i}, P^{\bmu'}_{i})$ can be explicitly determined as
            \begin{align*}
                \fiwmu = \frac{\Delta_{i}^2}{2 \sum_{j \in [k]} \frac{(\A_{j, \istarmu} - \A_{j, i})^2}{\sum_{l \in [n]}w_{l}\A_{j,l}}}.  
            \end{align*}
\end{restatable}

\begin{proof}
            First we calculate $d(P^{\bmu}_{i}, P^{\bmu'}_{i})$ for two rewards mean vectors $\bmu$ and $\bmu'$ using the assumption of Gaussian rewards with unit variance:
            \begin{align*}
                d(P^{\bmu}_{i}, P^{\bmu'}_{i}) = \sum_{j \in [k]} \A_{j,i} d(\mu_{j}, \mu'_{j}) = \sum_{j \in [k]} \A_{j,i} \frac{(\mu_{j} - \mu'_{j})^2}{2}. 
            \end{align*}
            In the separator setting, $\Is$ indeed is a subset of vectors $\bmu \in \mathbb{R}^k$ that imply a unique best arm. Consequently, the sets $\C_i$ is defined as $\{ \bmu' \in \Is \mid \A_i ^T \bmu' > \A_{\istarmu}^T \bmu' \}$. Then using the continuity of $f_i$ in $\bmu$ and the fact that $\Is$ is dense in $\mathbb{R}^k$, we can write $\fiwmu$ as the solution of the following optimization problem: 
            \begin{align*}
                \fiwmu = &\argmin_{{\bmu' \in \mathbb{R}^{k}}} \sum_{s \in [n]} w_s \sum_{j \in [k]} \A_{j,i}  \frac{(\mu_{j} - \mu'_{j})^2}{2} \\
                & \text{s.t.} \quad \A_i^T \bmu' > \A_{\istarmu}^T \bmu'.
            \end{align*}
            The solution to this problem is equivalent to the solution to the following problem when $\alpha \rightarrow 0$:
            \begin{align*}
                &\argmin_{\bmu' \in \mathbb{R}^{k}} \sum_{s \in [n]} w_s \sum_{j \in [k]} \A_{j,i}  \frac{(\mu_{j} - \mu'_{j})^2}{2} \\
                & \text{s.t.} \quad \A_i^T \bmu' \geq \A_{\istarmu}^T \bmu' + \alpha.
            \end{align*}
            To solve this problem, we define the Lagrangian as 
            \begin{align*}
                L(\bmu',\lambda) \;=\; \sum_{s \in [n]} w_s \sum_{j \in [k]} \A_{j,i}\,\frac{(\mu_{j} - \mu'_{j})^2}{2} \;+\; \lambda\,\Big((\A_{\istarmu} - \A_i)^T \bmu' + \alpha \Big).
            \end{align*}
            Taking derivative with respect to \( \mu'_j \) and $\lambda$ and setting it to zero with some basic algebra gives the following optimal values
            % \begin{align*}
            % 0 \;=\; \frac{\partial L}{\partial \mu'_j}
            % &\;=\; -\Big(\mu_j - \mu'_j\Big) \sum_{l \in [n]} w_l \A_{j,l} - \lambda\,\Big(\A_{j,i} - \A_{j, \istarmu}\Big),
            % \end{align*}
            % which gives
            \begin{align*}
                \mu'_j \;=\; \mu_j \;-\; \lambda\,\frac{(\A_{j,\istarmu} -  \A_{j,i})}{\displaystyle\sum_{l \in [n]} w_l A_{j,l}}, \\
                \lambda^* = \frac{\Delta_i + \alpha}{\sum_{j \in [k]} \frac{(\A_{j, \istarmu} - \A_{j, i})^2}{\sum_{l \in [n]}w_{l}\A_{j,l}}}.
            \end{align*}
            Inserting these values into the objective function proves the lemma.
            \begin{align*}
                \fiwmu = \lim_{\alpha \rightarrow 0} \frac{(\Delta_{i} + \alpha)^2}{2 \sum_{j \in [k]} \frac{(\A_{j, \istarmu} - \A_{j, i})^2}{\sum_{l \in [n]}w_{l}\A_{j,l}}} = \frac{\Delta_{i}^2}{2 \sum_{j \in [k]} \frac{(\A_{j, \istarmu} - \A_{j, i})^2}{\sum_{l \in [n]}w_{l}\A_{j,l}}}
            \end{align*}
\end{proof}
Combining the aforementioned lemma with Equation \eqref{eq: general_lower2} completes the proof.

\subsection{Proof of Lemma \ref{lem: sep correctness}}

\sepCorrectness*

\begin{proof}
    As discussed in Appendix \ref{apd: GLR}, our problem can be viewed as a general Identification problem. In separator case, $\bmu$ are $k$-dimensional vectors in $\Is$. Thus, we can redefine $\ocal_i$
    $$
        \mathcal{O}_i = \left\{ \bmu \in \I^s \ \big| \A_i^\top \bmu > \A_j^\top \bmu, \ \forall j \neq i \right\}
    $$
        
    The proof then follows directly from Proposition~15 of \cite{kaufmann2021mixture}, which implies that the GLR stopping rule with the sequential threshold in \eqref{eq: sep threshold} is \(\delta\)-correct. Note that in Proposition~15 of \cite{kaufmann2021mixture}, the threshold is given for one-parameter exponential families. Since our reward distributions are Gaussian, we employ a tighter threshold that the authors introduce in their paper.
\end{proof}



\subsection{Proof of Theorem \ref{thm : sep upper}}
    Recall that $\Is(\A)$ denotes the set of vectors in $\mathbb{R}^k$ which implies a unique best arm in the separator setting with context probability matrix $\A$. For a point $p \in \mathbb{R}^k$ and a set $C \in \mathbb{R}^k$, we use $\dls{p}{C}$ to denote the distance of $p$ to $C$ with respect to $L^2$ norm, more precisely 
    \begin{align*}
        \dls{p}{C} = \inf_{c \in C} \lsnorm{p - c}.
    \end{align*}
    Similar to the non-separator case, we need to present a few helper lemmas. 

    \begin{lemma} \label{lem: sep continuouity}
        For each $\bmu \in \Is$ and context probability matrix $\A$, the set $\wzstar{\bmu}$ is a convex set and the set-valued mapping $\bmu \rightarrow \wzstar{\bmu}$ is upper hemicontinuous. 
    \end{lemma}
    \begin{proof}
        Recall that $\wzstar{\bmu}$ is the set of solutions that maximize
        \begin{align*}
          F(\wb, \bmu) \triangleq \inf_{\bmu' \in \text{Alt}(\bmu, \A)} \sum_{j \in [k]} w_j \frac{(\bmu_{j} - \bmu'_{j})^2}{2}
        \end{align*}
        where $\wb \in \ch(\A)$. Since this function is the infimum over linear functions in $\wb$, thus, $F$ is concave with respect to $\wb$. Consequently, if $\wb_1$ and $\wb_2$ are both maximizers of $F$, then any convex combination of them is also a maximizer, implying that $\wzstar{\bmu}$ is a convex set. 
        
        As shown in Lemma \ref{lem: sepfi}, $F(\wb)$ can be written as  
        \begin{align*}
             \min_{i \neq i^*(\bmu)} \frac{\Delta_{i}^2}{2 \sum_{j \in [k]} \frac{(\A_{j, \istarmu} - \A_{j, i})^2}{w_{z,j}}},
        \end{align*}
        which is jointly continuous on $\ch(\A) \times I^{s}$. By applying Theorem 22 in \cite{mutiple-correct-answers-degenne2019pure}, we conclude that the mapping $\bmu \rightarrow \wzstar{\bmu}$ is upper hemicontinuous.
    \end{proof}

    As we analyze the sample complexity asymptotically, the initialization phase of the algorithm contributes only a constant term. This is because, for each context $i$, the expected waiting time to collect a sample with $Z = i$ is at most $\frac{1}{\amin}$, as the probability of observing context $i$ at each round is at least $\amin$. Thus, the expected number of initialization steps is at most $\frac{k}{\amin}$, which is a constant, and $\limsup_{\delta \rightarrow 0} \frac{\frac{k}{\amin}}{\logdel} = 0$. For simplicity, we set $t=0$ at the end of the initialization phase.

    For a non-negative real number $\alpha$, we define the $L^2(\alpha)$-approximation set of $\wzstar{\bmu}$ as 
    \begin{align*}
        \wepsstar{\alpha} = \left\{p \in \Delta^{k-1} \mid \dls{p}{\wzstar{\bmu}} \leq \alpha \right \}.
    \end{align*}
    Based on Lemma \ref{lem: sep continuouity}, the set $\wzstar{\bmu}$ is convex which shows that for each $\alpha$, the set $\wepsstar{\alpha}$ is also convex. \footnote{This follows directly from the definition of convexity, as any convex combination of two points in $\wepsstar{\alpha}$ remains within $\wepsstar{\alpha}$.}

    
    By Lemma \ref{lem: sep continuouity} and the fact that the set of vectors $\bmu'$ which has the same best arm as $\bmu$ is open, for each real number $\epsilon > 0$, there exist a constant $\xi = \xi(\epsilon, \bmu, \A)$ such that 
    \begin{align*}
        \text{if} \infnorm{\bmu' - \bmu} \leq \xi \Rightarrow \bmu' \in \Is(\A), ~  i^*(\bmu') = i^*(\bmu), \forall \mathbf{w}' \in \wzstar{\bmu'}: \mathbf{w}' \in \wepsstar{\epsilon}. 
    \end{align*}  

    Let $\I_{\epsilon} = \{ \bmu' \in \I(\A) \big| \infnorm{\bmu' - \bmu} \leq \xi \}$. For each $T$, we define two events that are likely to happen as
    \begin{align*}
        &\eteps \triangleq \bigcap_{t = T^{1/4}}^T (\muht \in \I_{\epsilon}), \\ 
        &\etepspr \triangleq \bigcap_{t = T^{1/4}}^T \left( \lsnorm{{\Nb^{Z}(t)} - {\sum_{i \in [t]} \pol(i)}} < t^{\frac34} \right).
    \end{align*}
    Recall that $\Nb^{Z}(t)$ shows the number of samples collected from contexts until round $t$ and $\pol(i)$ shows the policy on the context values that is chosen by G-tracking at round $i$. The following lemma shows these events occur with high probability.

    \begin{lemma} \label{lem: sep event prob}
        The exist two constants $B$ and $C$ such that
        \begin{align*}
            \pr((\eteps \cap \etepspr)^c) \leq BT \exp(-CT^{1/8}).
        \end{align*}
    \end{lemma}

    We defer the proof of Lemma \ref{lem: sep event prob} to Appendix \ref{subsec: sep event prob}. For now, we present a lemma that establishes the convergence of the G-tracking rule.

    \begin{lemma} \label{lem: G-tracking}
        For each $\epsilon > 0$, there exist a constant $T_{\epsilon}$ such that for each $T \geq T_{\epsilon}$, on event $\eteps \cap \etepspr$, we have 
        \begin{align*}
            \forall t \geq \sqrt{T} : \dls{\frac{\Nb^{Z}(t)}{t}} {\wzstar{\bmu}} \leq 5 \epsilon. 
        \end{align*}
    \end{lemma}
    \begin{proof}      
        First, we introduce some geometric notations. For points $A, B, C, D \in \mathbb{R}^k$, let $\overline{AB}$ show the length of the segment connecting $A$ to $B$, $AB$ show the line passing through $A$ and $B$, and $AB \parallel CD$ show that vectors $\overrightarrow{AB}$ and $\overrightarrow{CD}$ are parallel. At each round $t$, let $\bar{\pol}(t) = \frac{\sum_{i \in [t]}\pol(i)}{t}, \bar{\Nb}(t) = \frac{\Nb^{Z}(t)}{t}$, and
        \begin{align*}
            Y_t = \dls{\bar{\Nb}(t)}{\wepsstar{2\epsilon}}, \quad X_t = \dls{\bar{\pol}(t)}{\wepsstar{2\epsilon}}. 
        \end{align*}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\linewidth]{figs/G-lemma.pdf}
            \caption{Illustration of the relative positions of points in the proof of Lemma \ref{lem: G-tracking}, where all points lie in $\Delta^{k-1}$.}
            \label{fig: G-lemma}
        \end{figure}
    
        Moreover, let $\mathbf{w}'(t)$ be the unique point on the line $\bar{\pol}(t) \pol(t+1)$ such that $\bar{\Nb}(t)\bar{\pol}(t) \parallel \wb^{*}(t)\mathbf{w}'(t)$. $C_1$ denotes the closest point in $\wepsstar{2 \epsilon}$ to $\bar{\pol}(t)$ (note that $\wepsstar{2 \epsilon}$ is closed) and $C_2$ denotes the unique point on $C_1 \mathbf{w}'(t)$ such that $C_2 \bar{\pol}(t+1) \parallel C_1 \bar{\pol}(t)$. Figure \ref{fig: G-lemma} shows a complete figure including all these points. 
    
        We set $T_{\epsilon} = \epsilon^{-16}$. By this choice of $T_{\epsilon}$, if $T \geq T_{\epsilon}$, for each $t \geq T^{\frac14}$ we have  
        \begin{align} \label{eq: g-lemma-eq3}
            \overline{\bar{\pol}(t) \bar{\pol}(t+1)} \leq \epsilon \quad \text{ and } \quad \overline{\bar{\Nb}(t) \bar{\pol}(t)} \leq \epsilon. 
        \end{align}
        
        The reason is that 
        \begin{align*}
            \overline{\bar{\pol}(t) \bar{\pol}(t+1)} = \frac{1}{t+1} \overline{\bar{\pol}(t) \pol(t+1)} \leq \frac{\sqrt{2}}{t+1} \leq \frac{\sqrt{2}}{T^{1/4}} \leq \epsilon.  
        \end{align*}
        The first inequality holds because the points $\bar{\pol}(t)$ and $\pol(t+1)$ are on $\Delta^{k-1}$ and the distance of any pair of points of the simplex is at most $\sqrt{2}$. For the second property, note that as $\etepspr$ holds, $\overline{\bar{\Nb}(t) \bar{\pol}(t)} < t^{-1/4} < T^{-1/16} \leq \epsilon$.  
    
        Based on the definition, $C_1 \in \wepsstar{2 \epsilon}$ and as $\eteps$ holds, $\wb^{*}(t) \in \wepsstar{\epsilon}$. Using Thales's theorem in triangle $\bar{\Nb}(t) \bar{\pol}(t) \pol(t+1)$ , we obtain $\overline{\wb^{*}(t) \mathbf{w}'(t)} \leq \overline{\bar{\Nb}(t) \bar{\pol}(t)} \leq \epsilon$ which means that $\mathbf{w}'(t) \in \wepsstar{2 \epsilon}$. By convexity of the set $\wepsstar{2 \epsilon}$, we have $C_2 \in \wepsstar{2 \epsilon}$. 
    
        We prove the following two statements for each $t \geq T^{1/4}$: 
        
        \begin{align}\label{eq: g-lemma-eq1}
            \text{if }  X_t > \epsilon \Longrightarrow X_{t+1} \leq \frac{t}{t+1} X_t, 
        \end{align}
        \begin{align} \label{eq: g-lemma-eq2}
            \text{if } X_t \leq \epsilon \Longrightarrow \forall t' > t: X_{t'} \leq 2 \epsilon. 
        \end{align} 
    
        To prove \eqref{eq: g-lemma-eq1}, note that as $\mathbf{w}'(t) \in \wepsstar{2 \epsilon}$, if $X_t > \epsilon$ then $\overline{\bar{\pol}(t) \mathbf{w}'(t)} > \epsilon$. This shows that based on \eqref{eq: g-lemma-eq3}, $\bar{\pol}(t+1)$ lines on the segment between $\bar{\pol}(t) \mathbf{w}'(t)$. Then using the Thales's theorem in triangle $C_1 \bar{\pol}(t) \mathbf{w}'(t)$, we have 
        \begin{align*}
            X_{t+1} = \dls{\bar{\pol}(t+1)}{\wepsstar{2 \epsilon}} &\leq \overline{C_2 \bar{\pol}(t+1)} = \frac{\overline{\mathbf{w}'(t) \bar{\pol}(t+1)}}{\overline{\mathbf{w}'(t) \bar{\pol}(t)}} \overline{C_1 \bar{\pol}(t)} \\
            &\leq \frac{\overline{\pol(t+1) \bar{\pol}(t+1)}}{\overline{\pol(t+1) \bar{\pol}(t)}} \overline{C_1 \bar{\pol}(t)} = \frac{t}{t+1} \; \dls{\bar{\pol}(t)}{\wepsstar{2 \epsilon}} = \frac{t}{t+1} X_t.
        \end{align*}
        For the second inequality, we used the property that for positive numbers $a,b,c$ with $a < b$, we have $\frac{a}{b} < \frac{a+c}{b+c}$.
    
        We prove \eqref{eq: g-lemma-eq2} by contradiction. Assume $X_t \leq \epsilon$ and there exist a $t' > t$ such that $X_{t'} > 2 \epsilon$ and $t'$ is the smallest number with this property. As $X_{t'} > 2 \epsilon$, then \eqref{eq: g-lemma-eq3} implies $X_{t'-1} > \epsilon$. Using \eqref{eq: g-lemma-eq1}, we have $X_{t'} \leq \frac{t'}{t'+1} X_{t'-1}$ which implies $X_{t'-1} > 2 \epsilon$ which is a contradiction and proves \eqref{eq: g-lemma-eq2}. 
    
        Now we introduce the time $\tau$ as 
        \begin{align*}
            \tau \triangleq \inf \left\{ t > T^{1/4} \mid X_t \leq \epsilon \right \}. 
        \end{align*}
    
        Note that as a result of \ref{eq: g-lemma-eq1}, $\tau$ is finite. The aim is to prove $\tau \leq \sqrt{T}$. To show this assume $\tau > \sqrt{T}$. In this case, for all $t \in [\lceil T^{1/4} \rceil , \lfloor \sqrt{T} \rfloor]$, $X_t > \epsilon$ which means that $X_{t+1} \leq \frac{t}{t+1} X_t$. Then 
        \begin{align*}
            X_{\lfloor \sqrt{T} \rfloor} \leq  X_{\lceil T^{1/4} \rceil} \times \frac{\lceil T^{1/4} \rceil}{\lceil T^{1/4} \rceil + 1} \times \frac{\lceil T^{1/4} \rceil + 1}{\lceil T^{1/4} \rceil + 2} \times \cdots \times \frac{\lfloor \sqrt{T} \rfloor - 1}{\lfloor \sqrt{T} \rfloor} = X_{\lceil T^{1/4} \rceil} \frac{\lceil T^{1/4} \rceil}{\lfloor \sqrt{T} \rfloor} \leq \sqrt{2} \frac{\lceil T^{1/4} \rceil}{\lfloor \sqrt{T} \rfloor} \leq \epsilon,
        \end{align*}
        where we used $X_{\lceil T^{1/4} \rceil} \leq \sqrt{2}$ and $T \geq T_{\epsilon}$. The above inequality shows a contradiction which implies $\tau \leq \sqrt{T}$. Then, we have
        \begin{align*}
            \tau \leq \sqrt{T} \Longrightarrow \forall t> \sqrt{T}: X_t \leq 2 \epsilon \xLongrightarrow{\eqref{eq: g-lemma-eq3}}  Y_t \leq 3 \epsilon \Longrightarrow \dls{\frac{\Nb^{Z}(t)}{t}} {\wzstar{\bmu}} \leq 5 \epsilon. 
        \end{align*}
    
    \end{proof}
    

    On the good event $\eteps \cap \etepspr$, for each $t \geq \sqrt{T}$, these three properties hold: (i) $\muht$ is close to $\bmu$, (ii) $i^*(\muht) = i^*(\bmu)$ which implies Alt$(\muht, \A) = $ Alt$(\bmu, \A)$, and (iii) the points $\Nb^{Z}(t)/t$ to the set of solutions $\wzstar{\bmu}$ stay close.

    Similar to the non-separator setting, by combining these properties we can derive a lower bound on $\lamht$. Recall the set $\mathcal{O}_i = \left\{ \bmu \in \I^s \ \big| \A_i^\top \bmu > \A_j^\top \bmu, \ \forall j \neq i \right\}$. We define the function $g: \mathcal{O}_{\istarmu} \times [0,1] ^ k \rightarrow \mathbb{R}$ as
    \begin{align*}
        g(\bmu', \mathbf{w}') = \inf_{\mathbf{\lambda} \in \text{Alt}(\bmu, \A)} \sum_{j \in [k]} \frac{w'_j}{2}(\mu'_i - \mathbf{\lambda}_i)^2,
    \end{align*}
    and the constant 
    \begin{align*}
        \cstareps = \inf_{\substack{\bmu' \in \I^s_{\epsilon} \\ \mathbf{w}': \dls{\mathbf{w}'} {\wzstar{\bmu}} \leq 5 \epsilon}} g(\bmu', \mathbf{w}').
    \end{align*}

    Note that the mapping $(\mathbf{\lambda}, \bmu', \mathbf{w}') \rightarrow \sum_{j \in [k]} \frac{w'_j}{2}(\mu'_i - \mathbf{\lambda}_i)^2$ is jointly continuous and the constraints set Alt$(\bmu, \A)$ is independent of $(\bmu', \mathbf{w}')$. Then Berge's maximum theorem \cite{berge1963topological} implies that $g$ is continuous.
    
    With these definitions, let $T \geq T_{\epsilon}$ and the event $\eteps \cap \etepspr$ holds. Then, for $t \geq \sqrt{T}$, $\lamht$ is equal to $g(\muht, \frac{\Nb^{Z}(t)}{t})$, and the three mentioned properties imply that $\lamht \geq t \cstareps$. 

    The rest of the proof follows exactly the same as the non-separator setting.

    \subsection{Proof of Lemma \ref{lem: sep event prob}} \label{subsec: sep event prob}
    We first bound both $\pr(\E^c_T(\epsilon))$ and $\pr(\E'^c_T(\epsilon))$ separately, and then we apply the union bound on $\pr((\eteps \cap \etepspr)^c)$. 

    \textbf{Bounding $\E^c_T(\epsilon)$.} Note that according to the union bound, we have
    \begin{align*}
        \pr(\E^c_T(\epsilon)) \leq \sum_{t = T^{1/4}}^T \sum_{j \in [k]} \pr(|\muh_{j}(t) - \mu_{j}| > \xi).
    \end{align*}
    Now, note that we have
    \begin{align*}
        \pr(|\muh_{j}(t) - \mu_{j}| > \xi) &= \pr(\vert \muh_{j}(t) - \mu_{j} \vert > \xi \mid N^{Z}_j(t) < \amin \sqrt{t}) \pr(N^{Z}_j(t) < \amin \sqrt{t})  \\ 
                                           &+  \pr(\vert \muh_{j}(t) - \mu_{j} \vert > \xi \mid N^{Z}_j(t) \geq \amin \sqrt{t} )  \pr(N^{Z}_j(t) \geq \amin \sqrt{t}).         
    \end{align*}
    Since $\pr(|\muh_{j}(t) - \mu_{j}| > \xi \mid N^{Z}_j(t) < \amin \sqrt{t})$ and $\pr(N^{Z}_j(t) \geq \amin \sqrt{t})$ are less than $1$, we have
    $$
        \pr(\vert \muh_{j}(t) - \mu_{j} \vert > \xi) \leq \pr(N^{Z}_j(t) < \amin \sqrt{t})  + \pr(\lvert \muh_{j}(t) - \mu_{j} \rvert > \xi \mid N^{Z}_j(t) \geq \amin \sqrt{t} ) 
    $$

    Note that $N^{Z}_j(t)$ can be written as 
    $$
        N^{Z}_j(t) = \sum_{s = 1}^{t} \mathbbm{1}_{(z_s = j)}.
    $$
    Therefore, if $W_s$ denotes $\mathbbm{1}_{(z_s = j)}$, it is a Bernoulli random variable with parameter greater than $\amin$. By using Hoeffding's inequality, we have
    \begin{align*}
        \pr \left( N^{Z}_j(t) < \amin \sqrt{t} \right) = \pr \left( \sum_{s = 1}^{t} W_s < \amin \sqrt{t} \right) &\leq \pr \left ( \sum_{s = 1}^{t} W_s - \mathbb{E}[N^{Z}_{j}(t)] < \amin \sqrt{t} - t\amin \right) \\
        & \leq \exp \left ( \frac{-2\amin^2(t - \sqrt{t})^2}{t} \right) = \exp \left (-2\amin^2(\sqrt{t} - 1)^2 \right) \\
        & \leq \exp \left (-\amin^2 t \right)
    \end{align*}
    The last inequality is correct for sufficiently large $t$.

    Similarly, by applying Hoeffding's inequality, we have
    $$
        \pr(\lvert \muh_{j}(t) - \mu_{j} \rvert > \xi \mid N^{Z}_j(t) \geq \amin \sqrt{t} )  \leq 2 \exp \left (-\frac{\xi^2 \amin \sqrt{t}}{2} \right).
    $$
    Therefore,
    $$
          \pr(|\muh_{j}(t) - \mu_{j}| > \xi) \leq  \exp \left (-\amin^2 t \right) +  2 \exp \left (-\frac{\xi^2 \amin \sqrt{t}}{2} \right), 
    $$
    and then,
    $$
      \pr(\E^c_T(\epsilon)) \leq \sum_{t = T^{1/4}}^T \sum_{j \in [k]} \pr(|\muh_{j}(t) - \mu_{j}| > \xi) \leq B_1T \exp (-C_1 T^{1/8}) 
    $$
    where $B_1$ and $C_1$ are constants and the functions of $\xi,~\amin,$ and $k$. 
   
    
    \textbf{Bounding $\E^{'c}_T(\epsilon)$.} Similar to previous part, we use the union bound over $t$, then we have
    \begin{align}\label{eq: sep-event-union}
        \pr(\E^{'c}_T(\epsilon)) &\leq \sum_{t = T^{\frac14}}^{T} \pr\left( \lsnorm{{\Nb^{Z}(t)} - {\sum_{i \in [t]} \pol(i)}} > t^{\frac34} \right) \nonumber \\
                                 &\leq \sum_{t = T^{\frac14}}^{T} \pr\left( \sqrt{k} \infnorm{{\Nb^{Z}(t)} - {\sum_{i \in [t]} \pol(i)}} > t^{\frac34} \right)  = \pr\left( \infnorm{{\Nb^{Z}(t)} - {\sum_{i \in [t]} \pol(i)}} > \frac{t^{\frac34}}{k^{\frac12}} \right),
    \end{align}
    where we also used the inequality $\lsnorm{{\Nb^{Z}(t)} - {\sum_{i \in [t]} \pol(i)}} \leq  \sqrt{k} \infnorm{{\Nb^{Z}(t)} - {\sum_{i \in [t]} \pol(i)}}$.
     
    Again, after applying the union bound over $j \in [k]$, we have
    \begin{align}\label{eq: sep-event-union2}
        \pr\left( \infnorm{{\Nb^{Z}(t)} - {\sum_{i \in [t]} \pol(i)}} > \frac{t^{\frac34}}{k^{\frac12}} \right)  \leq \sum_{j = 1}^{k} \pr\left( \left \lvert N^{Z}_{j}(t) - {\sum_{i \in [t]} p_{j}(i)} \right \rvert > \frac{t^{\frac34}}{k^{\frac12}} \right)         
    \end{align}
    where $p_{j}(i)$ denotes the j-th element of the vector $\pol(i)$. Now, we can write $N^{Z}_{j}(t)$ as the sum of $t$ random variables as follows.
    $$
           N^{Z}_j(t) = \sum_{s = 1}^{t} \mathbbm{1}_{(z_s = j)} = \sum_{i = 1}^{t} W_i
    $$
    Note that according to Algorithm \ref{algo: sep}, $W_i$ is a Bernoulli random variable with parameter $\pol(i)_{i}(s)$. Therefore, we have
    $$
        \pr\left( \left \lvert N^Z_{j}(t) - {\sum_{i \in [t]} p_{j}(i)} \right \rvert > \frac{t^{\frac34}}{k^{\frac12}} \right)  = \pr\left( \left \lvert \sum_{i = 1}^{t} W_i - {\sum_{i \in [t]} p_{j}(i)} \right \rvert > \frac{t^{\frac34}}{k^{\frac12}} \right),
    $$
    Then, by applying Hoeffding's inequality, we obtain
    $$
        \pr\left( \left \lvert \sum_{i = 1}^{t} W_i - \sum_{i \in [t]} p_{j}(i) \right \rvert > \frac{t^{\frac34}}{k^{\frac12}} \right) \leq 2 \exp \left( - \frac{2t^{\frac32}}{kt} \right)  =   2 \exp \left( - \frac{2t^{\frac12}}{k} \right) 
    $$
    Finally, by combining the above inequality and Equations \eqref{eq: sep-event-union} and \eqref{eq: sep-event-union2}, we have
    $$
        \pr(\E^{'c}_T(\epsilon)) \leq \sum_{t = T^{\frac14}}^{T} 2k \exp \left( - \frac{2t^{\frac12}}{k} \right)
                                \leq TB_2 \exp \left( - \frac{2T^{\frac18}}{k} \right)
    $$
    where $B_2 = 2k$ and $C_2 = \frac{2}{k}$.

    All in all, by combining the derived upper bounds for $\pr (\E^{c}_T(\epsilon))$ and $\pr(\E^{'c}_T(\epsilon))$, we have
    $$
        \pr((\eteps \cap \etepspr)^c) \leq \pr (\E^{c}_T(\epsilon)) + \pr(\E^{'c}_T(\epsilon)) \leq BT \exp (-CT^{\frac18}),
    $$
    where $B = 2\max (B_1, B_2)$ and $C = \min (C_1, C_2)$.




\section{Discussion on Solving the Optimization Problem of \ref{eq: middle non-sep LB}} \label{apd: opt-solving}

The optimization problem \eqref{eq: middle non-sep LB} can be solved efficiently and with arbitrary precision. Notably, the same optimization problem arises in the classic BAI problem with Gaussian rewards \cite{bai-gaussian-barrier2022non}. Although the definitions of $\Delta_i$ differ between these two settingsâ€”and in this paper, these gaps depend on the context probability matrixâ€”the optimization problem itself remains similar, allowing the use of similar techniques to solve it. 
    \begin{restatable}[\cite{bai-gaussian-barrier2022non}]{lemma}{nonsepw} \label{lem: non-sep w1}
        Consider the  equation
        \begin{equation} \label{eq: non-sep w1}
            \sum_{\substack{i \in [n] \\ i \neq \istarmu}} \frac{1}{(w \Delta_i^2 - 1)^2} = 1 , 
        \end{equation}
         where $\Delta_i > 0$. This equation has a unique solution $w^*$ in the interval $\left[\frac{2}{\Delta_{min}^2}, \frac{1 + \sqrt{n-1}}{\Delta_{min}^2}\right]$. Define $\ub = \left(\frac{w^*}{w^*\Delta_1^2 -1}, \frac{w^*}{w^*\Delta_2^2 -1}, \ldots , w^*, \dots, \frac{w^*}{w^*\Delta_n^2 -1}\right)$ where $\istarmu$-th element is $w^*$. The normalized vector $\frac{\ub}{\norm*{\ub}_{1}} \in \Delta^{n-1}$ is the unique solution of Equation \eqref{eq: middle non-sep LB}.
    \end{restatable}

    As discussed in \cite{bai-gaussian-barrier2022non}, the left-hand side of Equation \eqref{eq: non-sep w1} is strictly decreasing over the interval $\left[\frac{2}{\Delta_{min}^2}, \frac{1 + \sqrt{n-1}}{\Delta_{min}^2}\right]$. Consequently, simple numerical methods, such as binary search can be employed to approximate the solution efficiently. Leveraging this property, we can efficiently compute the weights for the optimization problem in Equation \eqref{eq: middle non-sep LB}. This capability enables the design of an efficient learning algorithm, as discussed later.
    


\section{Discussion on the case of unknown Context Matrix} \label{apd: unknown context}


    The assumption that the context probability matrix $\A$ is known may not always be realistic. However, when $\A$ is unknown, the problem becomes more challenging because the set of alternative parameters (Alt) changes in structure. Recall that all tracking algorithms based on \cite{track-stop-garivier2016optimal} must solve the optimization problem in Equation \eqref{eq: general_lower1} to determine the optimal weights. Alternatively, one can use the formulation in Equation~\eqref{eq: general_fi}, but both approaches involve a minimization over some set. The main practical complexity of tracking algorithms lies in solving this optimization problem.
    If an oracle were available to compute the optimal weights in Equation~\eqref{eq: general_lower1}, one could still use tracking algorithms even when \(\A\) is unknown and achieve the optimal sample complexity. Note that the GLR statistic needs to be adapted to this setting.

    
    When \(\A\) is known, the sets \(\C_j\) or \(\text{Alt}\) are convex, so standard convex optimization tools can be applied. However, when $\A$ is unknown, the sets $\C_j$ or Alt, may not be convex, and thus finding optimal weights via convex optimization methods becomes infeasible. Similar issues have been noted in the reinforcement learning literature \cite{al2021adaptive}.    
    
    As mentioned earlier, the case of separator context is a special case of non-separator setting, so it suffices to show non-convexity for the separator case.  Note that if $\A$ is unknown, the set $\text{Alt}(\bmu, \A)$ changes because $A$ itself can vary within the alternative set. Formally, The new definition of \text{Alt}$(\bmu, \A)$ for this setting is
    
    $$
        \text{Alt}(\bmu, \A)  = \{(\bmu', \A') \in \I \mid  i^*(\bmu', \A') \neq i^*(\bmu, \A)  \},
    $$
    where $\I$ denotes the set of all instances (parameterized by $(\bmu, \A)$) that have a unique best arm. Similar to known setting, we have Alt$(\bmu, \A) = \cup_{i \neq i^*(\bmu, \A)} \C_i$, and
    $$
    \C_i = \{ (\bmu', \A') \in \I \mid \A_i^{'\top} \bmu'_i > \A_{i^*(\bmu, \A)}^{'\top} \bmu'_{i^*(\bmu, \A)} \}.
    $$
    As mentioned earlier, we need to solve
    $$
        \inf_{(\bmu', \A') \in \text{Alt}(\bmu, \A)} \sum_{i \in [n]} w_i d(P^{\bmu, \A}_{i}, P^{\bmu', \A'}_{i}),
    $$
    or equivalently, using Equation~\eqref{eq: general_fi},
    \begin{align}
        f_j(\mathbf{w}, \bmu, \A) = \inf_{(\bmu', \A') \in \C_j} \sum_{i \in [n]} w_i d(P^{\bmu, \A}_{i}, P^{\bmu', \A'}_{i}).
    \end{align}
    
    For the case of known $\A$, each set $\C_j$ is convex, so $f_j(\mathbf{w}, \bmu, \A)$ reduces to a convex problem that can be solved via convex optimization methods. However, if $\A$ is unknown, that convexity property no longer holds. The following example shows that neither $\text{Alt}(\bmu, \A)$ nor $\C_j$ is necessarily convex.
    
    
    \textbf{Example.} Consider $n = k = 2$. Suppose $i^*(\bmu, \A) = 1$, then $\text{Alt}(\bmu, \A)$ equals $\C_2$. Therefore, we only need to show that $\text{Alt}(\bmu, \A)$ is not convex. Let
    \begin{align}
        &\A = \begin{bmatrix}
                0.5 & 0.5 \\
                0.5 & 0.5
            \end{bmatrix}, ~
        \bmu = \begin{bmatrix}
                    2 & 1 \\
                    2 & 1
        \end{bmatrix}
    \end{align}
    Clearly, \(i^*(\bmu, \A) = 1\) because \(\mathbb{E}[Y \mid X=1] = 2\) and \(\mathbb{E}[Y \mid X=2] = 1\). Next, let
    \begin{align}
        &\A^{(1)} = \begin{bmatrix}
                0.95 & 0.5 \\
                0.05 & 0.5
            \end{bmatrix}, ~
        \bmu^{(1)} = \begin{bmatrix}
                    0 & 1 \\
                    0 & 1
        \end{bmatrix}, \\
        &\A^{(2)} = \begin{bmatrix}
                0.05 & 0.5 \\
                0.95 & 0.5
            \end{bmatrix}, ~
        \bmu^{(2)} = \begin{bmatrix}
                    10 & 1 \\
                    0 & 1
        \end{bmatrix}.
    \end{align}
    We see $i^*(\bmu^{(1)}, \A^{(1)}) = i^*(\bmu^{(2)}, \A^{(2)}) = 2$. Therefore, $(\bmu^{(1)}, \A^{(1)})$ and $(\bmu^{(2)}, \A^{(2)})$ belong to $\text{Alt}(\bmu, \A)$. Consider a convex combination of $(\bmu^{(1)}, \A^{(1)})$ and $(\bmu^{(2)}, \A^{(2)})$ as
    
    \begin{align}
        \A^{(3)} = \frac{1}{2} (\A^{(1)}  + \A^{(2)}) = \begin{bmatrix}
                0.5 & 0.5 \\
                0.5 & 0.5
            \end{bmatrix},
        \bmu^{(3)} = \frac{1}{2} (\bmu^{(1)}  + \bmu^{(2)}) = \begin{bmatrix}
                    5 & 1 \\
                    0 & 1
        \end{bmatrix}.
    \end{align}
    Now, we have $i^*(\bmu^{(3)}, \A^{(3)}) = 1$. It indicates that $(\bmu^{(3)}, \A^{(3)})$ is \emph{not} in $\text{Alt}(\bmu, \A)$, which shows that $\text{Alt}(\bmu, \A)$ is not a convex set.
    
    
\section{Additional Experimental Details} \label{apd: experiment}

This section contains additional details on the experimental setup and the results of further experiments for the separator case. 
\subsection{Non-Separator Context} 
    â€ŒBefore presenting further details on the results, we discuss the sequential thresholds used in the algorithms. For NSTS, we used the exact thresholds from \eqref{eq: sep threshold}, though they tend to be overly conservative, keeping the error probability well below $\delta$.
    
    As discussed in the main text, there is no theoretical guarantee for TS when interacting with a bandit featuring post-action context. This is because, in such cases, the reward distribution of each arm becomes a mixture of Gaussians, which does not belong to the exponential family of distributions. Moreover, experimental results indicate that applying the same thresholds as in the classic bandit setting with unit-variance Gaussian rewards does not control the error rate below $\delta$. To design new thresholds for TS, we utilized the following lemma.
    
    \begin{lemma}\label{lem: sub-g}
        Assume that $X_1, X_2, \ldots, X_k$ are Gaussian random variables with $\sigma = 1$ and means $\mathbb{E}[X_i] = \mu_i \in [a, b]$. Let $X$ be a mixture of these variables, meaning $X = X_i$ with probability $p_i$. Then, $X$ is a sub-Gaussian with $\sigma^2 = 1 + \frac{(b - a)^2}{4}$.
    \end{lemma}
    \begin{proof}
        The proof follows directly by expressing $X$ as the sum of two independent components: $X = Y + \epsilon$, where $Y$ is a discrete random variable satisfying $\pr(Y = \mu_i) = p_i$, and $\epsilon$ is a standard normal variable. Since $Y$ is $\frac{(b - a)}{2}$-sub-Gaussian and $\epsilon$ is $1$-sub-Gaussian, the sum of them is also sub-gaussian with parameter $\sigma^2 = 1 + \frac{(b - a)^2}{4}$.
    \end{proof}
    
    Since $\bmu \in [0,10]^{k \times n}$, the reward distribution of the arms is sub-Gaussian with parameter $\sigma = \sqrt{26}$ according to Lemma \ref{lem: sub-g}. For employing the TS algorithm, we approximate the reward distribution with a Gaussian distribution with variance $\sigma^2 = 26$. Consequently, we can use the known threshold for classic bandit with Gaussian distribution \cite{kaufmann2021mixture}.
    
    Figure \ref{fig: non-sep-dist} shows the average $L^2$ distance between the vector $\frac{\Nb^{X}(t)}{t}$ and the optimal vector $\wstar{\bmu}$ over time for all nine instances introduced in the main text simulated over $75$ runs. For each run, after its stopping time, the distance is considered as zero in the average.  The results indicate that ignoring the post-action context leads to significant sub-optimality and misalignment in tracking the correct proportion of actions. 




\begin{figure}
    \centering
    \begin{tabular}{ccc}
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/1_0.pdf}} &
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/1_1.pdf}} \\
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/1_2.pdf}} &

        \subfloat{\includegraphics[width=0.42\textwidth]{figs/1_3.pdf}} \\
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/1_4.pdf}} &
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/1_5.pdf}} \\

        \subfloat{\includegraphics[width=0.42\textwidth]{figs/1_6.pdf}} &
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/1_7.pdf}} \\
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/1_8.pdf}} 
    \end{tabular}
    \caption{Comparison of the $L^2$ distance of the frequencies of pulled arms and the optimal frequency over time between two algorithms.}
    \label{fig: non-sep-dist}
\end{figure}




\begin{figure}
    \centering
    \begin{tabular}{ccc}
        \subfloat{\includegraphics[width=0.35\textwidth]{figs/0_Ts.pdf}} &
        \subfloat{\includegraphics[width=0.35\textwidth]{figs/1_Ts.pdf}} \\
        \subfloat{\includegraphics[width=0.35\textwidth]{figs/2_Ts.pdf}} &

        \subfloat{\includegraphics[width=0.35\textwidth]{figs/3_Ts.pdf}} \\
        \subfloat{\includegraphics[width=0.35\textwidth]{figs/4_Ts.pdf}} &
        \subfloat{\includegraphics[width=0.35\textwidth]{figs/5_Ts.pdf}} \\

        \subfloat{\includegraphics[width=0.35\textwidth]{figs/6_Ts.pdf}} &
        \subfloat{\includegraphics[width=0.35\textwidth]{figs/7_Ts.pdf}} \\
        \subfloat{\includegraphics[width=0.35\textwidth]{figs/8_Ts.pdf}} 
    \end{tabular}
    \caption{Comparison of the stopping times of different algorithms on randomly generated instances.}
    \label{fig: sep-box}
\end{figure}


\subsection{Separator Context}
    Here, we present additional results from running different algorithms on instances in the separator setting. For each $n \in \{5, 10, 15\}$ and $k \in \{3, 5, 8\}$, we randomly generated an instance with $n$ arms and $k$ context values. To avoid trivial instances, we repeatedly generated instances until finding one where $\Delta_i \in \left[\frac{1}{2n}, \frac{i+1}{2n}\right]$, with the first arm being the best arm. 


    \textbf{Reduction to Linear Bandit}. We can simply reduce the separator context setting to a linear bandit by ignoring the context variable. In this reduction, each arm $i$ is mapped to $A_{i} \in \mathbb{R}^{k}$. Then, the reward after taking action $i$ has the following equation
    $$
        Y_i = \sum_{j = 1}^{k} \A_{j, i} \bmu_{j} + R + \varepsilon = \langle \A_{i}, \bmu \rangle  + R + \varepsilon
    $$
    where $\varepsilon$ is a standard Gaussian, and $R$ is a discrete variable over $\{\bmu_j - \A_{i}^{\top} \bmu \mid j \in [k] \}$ such that
    $
        \forall j \in [k]: \pr (R  = \bmu_j - \A_{i}^{\top} \bmu) = \A_{j, i}.
    $
    Since $\bmu_j$s are bounded, then similar to the proof of Lemma \ref{lem: sub-g}, we can conclude that $R + \varepsilon$ is a sub-Gaussian noise.

    As shown in the main text, the LTS algorithm does not perform competitively, and due to its long computation time, we do not report its results on these instances and only compare TS and Algorithm \ref{algo: sep}. Figure \ref{fig: sep-box} presents a box plot of the stopping times for these algorithms across all instances. The results demonstrate that ignoring information about the post-action context leads to significant sub-optimality. 

    Figure \ref{fig: sep-dist} shows the average $L^2$ distance of the context frequencies vector $\frac{\Nb^{Z}(t)}{t}$ and the optimal context frequencies $\wzstar{\bmu}$ over time for all previously introduced instances and for different algorithms. 



\begin{figure}
    \centering
    \begin{tabular}{ccc}
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/2_0.pdf}} &
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/2_1.pdf}} \\
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/2_2.pdf}} &

        \subfloat{\includegraphics[width=0.42\textwidth]{figs/2_3.pdf}} \\
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/2_4.pdf}} &
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/2_5.pdf}} \\

        \subfloat{\includegraphics[width=0.42\textwidth]{figs/2_6.pdf}} &
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/2_7.pdf}} \\
        \subfloat{\includegraphics[width=0.42\textwidth]{figs/2_8.pdf}} 
    \end{tabular}
    \caption{Comparison of the $L^2$ distance of the frequencies of observed contexts and the optimal frequency over time among different algorithms.}
    \label{fig: sep-dist}
\end{figure}