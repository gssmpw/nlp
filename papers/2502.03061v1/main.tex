%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage{hyperref}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}



% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

\input{commands}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Optimal Best Arm Identification with Post-Action Context}

\begin{document}

\twocolumn[
\icmltitle{Optimal Best Arm Identification with Post-Action Context}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}





\begin{icmlauthorlist}
\icmlauthor{Mohammad Shahverdikondori}{yyy}
\icmlauthor{Amir Mohammad Abouei}{cs}
\icmlauthor{Alireza Rezaeimoghadam}{sh}
\icmlauthor{Negar Kiyavash}{yyy}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}


\icmlaffiliation{yyy}{College of Management of Technology, EPFL.}
\icmlaffiliation{cs}{School of Computer and Communication Sciences, EPFL.}

\icmlaffiliation{sh}{Department of Computer Engineering,
Sharif University of Technology}


% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Mohammad Shahverdikondori}{mohammad.shahverdikondori@epfl.ch}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
We introduce the problem of best arm identification (BAI) with post-action context, a new BAI problem in a stochastic multi-armed bandit environment and the fixed-confidence setting. The problem addresses the scenarios in which the learner receives a \emph{post-action context} in addition to the reward after playing each action. This post-action context provides additional information that can significantly facilitate the decision process. We analyze two different types of the post-action context: (i) \textit{non-separator}, where the reward depends on both the action and the context, and (ii) \textit{separator}, where the reward depends solely on the context. For both cases, we derive instance-dependent lower bounds on the sample complexity and propose algorithms that asymptotically achieve the optimal sample complexity. 
For the non-separator setting, we do so by demonstrating that the Track-and-Stop algorithm can be extended to this setting. For the separator setting, we propose a novel sampling rule called \textit{G-tracking}, which uses the geometry of the context space to directly track the contexts rather than the actions.
Finally, our empirical results showcase the advantage of our approaches compared to the state of the art.
\end{abstract}



\section{Introduction}

Multi-armed bandit (MAB) refers to a class of sequential decision-making problems, where a learner selects actions (arms) in order to maximize a reward. 
MAB has widespread applications in various domains, such as clinical trials \cite{william_r__thompson_1933}, dynamic pricing \cite{kleinberg2003value, besbes2009dynamic}, recommender systems \cite{li2010contextual}, and resource allocation \cite{gai2012combinatorial}. Depending on the learner’s goal and constraints, different objectives may be pursued. For example, if the learner's goal is to minimize cumulative regret, they must balance the exploration-exploitation trade-off \cite{auer2002using, garivier2011kl}. Alternatively, in the \emph{best Arm Identification (BAI)} setting, the learner seeks the arm with the highest expected reward and must minimize the sample complexity—i.e., the number of rounds needed to identify this arm \cite{lb-tsitsiklis-mannor2004sample, SR-audibert2010best, track-stop-garivier2016optimal}. 

Best arm identification is typically studied in two scenarios: \emph{fixed-budget}, where the time horizon $T$ is fixed, and the objective is to minimize the error probability \cite{budget-confidence-gabillon2012best, SH-karnin2013almost, lb-budget-carpentier2016tight}; and \emph{fixed-confidence}, where the error probability $\delta$ for identifying the best arm is fixed, and the objective is to minimize the sample complexity \cite{track-stop-garivier2016optimal, SR-audibert2010best, kaufmann2020contributions, confidence-jamieson2014best, lb-tsitsiklis-mannor2004sample}. We focus on the latter.

\begin{figure}[t]
        \centering
        \begin{subfigure}[b]{0.5\textwidth}
            \centering
            \begin{tikzpicture}           
                %First create all the nodes
                \tikzset{line width=2pt, outer sep=0pt,
                ell/.style={draw,fill=white, inner sep=2pt,
                line width=2pt},
                };
                    \node[name=X, shape=circle, draw] at (-3,0){$X$};
                    \node[name=Z, shape=circle, draw] at (-1,0){$Z$};
                    \node[name=Y, shape=circle, draw] at (1,0){$Y$};
                
                %Then create the edges    
                \begin{scope}[>={Stealth[black]},
                              every edge/.style={draw=black,very thick}]
                    \path[->] (X) edge (Z);
                    \path[->] (X) edge[bend right] (Y);
                    \path[->] (Z) edge (Y);
                \end{scope}
            \end{tikzpicture}
            \caption{Non-separator context}
            \label{fig: nonsep}
        \end{subfigure}
        \begin{subfigure}[b]{0.5\textwidth}
            \centering
            \begin{tikzpicture}
                %First create all the nodes
                \tikzset{line width=2pt, outer sep=0pt,
                ell/.style={draw,fill=white, inner sep=2pt,
                line width=2pt},
                };
                    \node[name=X, shape=circle, draw] at (-3,0){$X$};
                    \node[name=Z, shape=circle, draw] at (-1,0){$Z$};
                    \node[name=Y, shape=circle, draw] at (1,0){$Y$};
                
                %Then create the edges    
                \begin{scope}[>={Stealth[black]},
                              every edge/.style={draw=black,very thick}]
                    \path[->] (X) edge (Z);
                    \path[->] (Z) edge (Y);
                \end{scope}
            \end{tikzpicture}
            \caption{Separator context}
            \label{fig: sep}
        \end{subfigure}
        \caption{Two possible structures for the post-action context.}
        \label{fig: causal_graph}
\end{figure}

While the classic MAB model is suitable for a broad range of applications, additional side information about the environment can lead to more efficient algorithms. In the bandit literature, various types of side information have been considered. For example, \emph{causal bandits} \cite{lattimore2016causal, causal-cucb-lu2020regret} or \emph{linear bandits} \cite{auer2002using, abbasi2011improved} impose specific structures on the actions, and \emph{contextual bandits} \cite{tewari2017ads, langford2007epoch} allow the learner to observe a context before choosing an action. In this work, we consider a different form of side information, which we call \emph{post-action context}. Specifically, after choosing an action in each round, the learner receives intermediate feedback from the environment along with the reward. This post-action context can substantially accelerate the process of identifying the best arm. We consider this new problem in the fixed-confidence setting, aiming to show how leveraging post-action context can reduce the sample complexity for different environments. For a detailed discussion on related work, see Appendix \ref{apd: related-work}.



Figure \ref{fig: causal_graph} illustrates two structures pertaining to post-action context, which may also be interpreted as causal graphs. In both structures, variables $X$, $Z$, and $Y$ represent the action, the post-action context, and the reward, respectively. In Figure \ref{fig: nonsep}, the context $Z$ is informative but not a sufficient statistic of $X$ for determining $Y$. In other words, the distribution of $Y$ depends on both $X$ and $Z$. We refer to this as the \textit{non-separator} context. By contrast, in Figure \ref{fig: sep}, once $Z$ is observed, the distribution of $Y$ depends only on $Z$, making $X$ and $Y$ conditionally independent given $Z$. We call this  \textit{separator} context. Although the separator context can be viewed as a special case of the non-separator, an algorithm optimized for the former may not be optimal for the latter. This is because a separator context provides the important side information that $X$ and $Y$ are conditionally independent given $Z$. We analyze both settings in this work.

\textbf{Motivating Example.}  
Consider a healthcare scenario for diabetes management where $X$ denotes a chosen treatment regimen (e.g., medication or dosage), $Z$ is an intermediate biomarker such as short-term blood sugar levels, and $Y$ is the clinical target of improvement (e.g., glucose stability and range). In each round, a clinician assigns a treatment $X$ to a patient, measures $Z$ after a short interval, and later observes $Y$. Because $Z$ correlates with $Y$, tracking this intermediate measure can offer real-time insights into patient response. By leveraging $Z$, clinicians can adapt treatment strategies more efficiently, reducing the total number of trials needed to identify an optimal regimen. This scenario may fall into either non-separator or separator settings. In both cases, considering the intermediate feedback enhances learning compared to relying solely on the clinical target $Y$.


The presence of post-action context can fundamentally alter the optimal strategy for selecting arms. In classic BAI problems, the optimal algorithms \cite{track-stop-garivier2016optimal} focus more time on pulling the arms with the higher observed rewards, which is natural when no additional side information is available. However, in settings with post-action context, collecting more samples from a certain context may expedite the identification of the best arm. In such scenarios, choosing a suboptimal arm that increases the probability of observing that context is more effective than selecting the arm with the highest reward. We demonstrate this experimentally in Subsection~\ref{sub: sep-exp}, showing that ignoring post-action context can result in significantly worse performance.


\textbf{Contributions and Organization.} 
The main contribution of this paper is to introduce and analyze a new stochastic sequential decision-making problem with bandit feedback, \textit{best arm identification with post-action context}, which generalizes the BAI problem in the MAB literature. We provide algorithms based on the track and stop method \cite{track-stop-garivier2016optimal} for both separator and non-separator post-action contexts that achieve asymptotic optimal sample complexity.

The remainder of the paper is organized as follows. Section \ref{sec: preli} introduces the problem and the necessary definitions. In Section \ref{sec: general-lower}, we present a general instance-dependent lower bound that serves as the foundation for deriving lower bounds for both cases of post-action context. In the non-separator setting, we propose an algorithm called non-separator track-and-stop, which incorporates the D-tracking rule and proves its optimality (Section \ref{sec: non-sep}). For the separator case, we propose an optimal algorithm called separator track-and-stop that uses a new tracking rule, called \textit{G-tracking}. The G-tracking rule directly tracks the observed contexts rather than the actions and leverages a geometrical property to choose the next action (Section \ref{sec: sep}). Our experimental results are presented in Section \ref{sec: experiment}. Proofs and additional discussions are delayed to the appendix.

\section{Preliminaries and Problem Setup} \label{sec: preli}

\textbf{Notations.} The set $[n]$ denotes $\{1, 2, \dots, n\}$ and $\Delta^{n-1}$ represents the $(n-1)$-dimensional standard simplex, defined as $\{w \in \mathbb{R}^n \mid w_i \geq 0 \wedge \sum_{i = 1}^{n} w_i = 1\}$. The notation $d(P, Q)$ denotes the Kullback–Leibler (KL) divergence between two probability measures $P$ and $Q$. Additionally, $d_B(\delta,1-\delta)$ refers to the KL divergence between two Bernoulli random variables with parameters $\delta$ and $1 - \delta$. The convex hull of \(n\) vectors \(\A_i \in \mathbb{R}^d\) is denoted by $\ch(\{\A_1, \A_2, \dots, \A_n\}) = \left\{ \sum_{i = 1 }^{n} \lambda_i \A_i \mid \lambda_i \geq 0 \ \wedge \sum_{i = 1}^{n} \lambda_i = 1 \right\}$.

\textbf{Best arm identification with post-action context problem.} In this problem, a player interacts with a multi-armed bandit environment $\V$ with $n$ arms. In each round $t$, the player selects an action $X = i_t \in [n]$ and observes a pair $(z_t, y_t)$, where $z_t$ is the value of the post-action context variable $Z$, and $y_t$ is the realization of the reward variable $Y$.

We assume the context variable $Z$ is discrete and takes values in $[k]$, depending only on the arm pulled. The context probability matrix $\A = [\A_1 \vert \A_2 \vert \ldots \vert \A_n] \in \mathbb{R}^{k \times n}$, where $\A_{j,i} = \pr(Z = j \mid X = i)$, encodes the probability of contexts given each action. We further assume that the reward distribution $\pr(Y \mid X=i, Z=j)$ follows a Gaussian distribution with unit variance. The matrix $\bmu = [\bmu_1 \vert \bmu_2 \vert \ldots \vert \bmu_n] \in \mathbb{R}^{k \times n}$ represents the mean values of the reward distributions, such that $\mu_{j,i} = \mathbb{E}(Y | X=i, Z=j)$. 

In the case of the separator context, the reward depends only on $Z$, that is, $\pr (Y \mid X = i, Z = j) = \pr (Y \mid Z = j)$,
for all $i \in [n]$ and $j \in [k]$. This implies that all columns of $\bmu$ are identical. We denote the joint distribution of the context and reward for a given action $i$ by $P^{\A, \bmu}_{i}= \pr(Z, Y \mid X=i)$. The expected reward for a given action $i$ is computed as 
\begin{align*}
    \mathbb{E}[Y \mid X=i] &= \sum_{j=1}^{k} \pr(Z = j \vert X = i)\mathbb{E}[Y \vert Z = j, X = i] \\
                           &= \sum_{j=1}^{k} \A_{j,i} \mu_{j,i} = \A_i^\top \bmu_i.  
\end{align*}
The best arm is the arm with the maximum expected reward, i.e., $i^*(\bmu, \A)=\argmax_{i \in [n]} \A_i^\top \bmu_i$, where the mean matrix $\bmu$ and the context probability matrix $\A$ characterize the instance. In this work, we assume that $\A$ is known to the learner and that the best arm is unique. The case where $\A$ is unknown is discussed in Appendix \ref{apd: unknown context}. Let $\I(\A) \subset \mathbb{R}^{k \times n}$ denote the set of $\bmu$ matrices that imply a unique best arm for a given $\A$. For simplicity, as $\A$ is known, we often use $\I, \istarmu, $ and $P^{\mu}$ instead of $\I(\A), i^*(\bmu, \A),$ and $P^{\mu, \A}$, respectively.

We consider the best arm identification problem in the \textit{fixed confidence} setting, where the player aims to identify the best arm $\istarmu$ for any $\bmu \in \I$ with a pre-specified confidence level $\delta \in (0,1)$. To accomplish this, a sequential learning algorithm is characterized by three main components: (i) A \emph{sampling rule} (deterministic or stochastic) that determines which arm to pull in round $t$ based on the history up to round $t-1$, denoted by $\mathcal{H}_{t-1} = (i_1, (z_1, y_1), i_2, (z_2, y_2), \ldots, i_{t-1}, (z_{t-1}, y_{t-1}))$, (ii) A \emph{stopping rule} $\tau$, based on $\mathcal{H}_t$, determines when to stop the process, and (iii) A \emph{decision rule} $\hat{i}_{\tau}$, which identifies the best action.

\begin{definition}[$\delta$-correct]
    Let $\delta \in (0, 1)$. An algorithm is \textit{}{$\delta$-correct} if for any $\A$ and $\bmu \in \I(\A)$ it satisfies $\pr_{\bmu, \A}(\tau_{\delta} < \infty, \hat{i}_{\tau} \neq i^*(\bmu)) \leq \delta$.
\end{definition}

The goal of the learner is to find a $\delta$-correct algorithm that identifies the best arm with probability at least $1 - \delta$ in minimum expected number of samples $\mathbb{E}_{\bmu, \A}[\tau_{\delta}]$.

The sub-optimality gap of arm $i$, denoted by $\Delta_i$, is defined as
\begin{align*}
    \Delta_i & \triangleq \mathbb{E}[Y \mid X=\istarmu] - \mathbb{E}[Y \mid X=i] \\
    &= \A_{\istarmu}^\top \bmu_{\istarmu} - \A_i^\top \bmu_i.
\end{align*}
During the learning process, let $\muht$ denote the matrix containing the empirical estimates of all entries of $\bmu$ based on the samples collected up to round $t$ and  define
\begin{align*}
    \delhit{i} \triangleq \A_{\istarmut}^\top \bmuh_{\istarmut}(t) - \A_i^\top \bmuh_i,
\end{align*}
which is the gap of arm $i$ relative to the arm with the best empirical mean up to round $t$ estimated using $\muht$. Finally, $\numaction$, $\numcontext$, and $\numac$ represent the number of times action $i$, context $j$, and joint action-context pair $(i,j)$ have been observed up to round $t$, respectively.

\paragraph{Summary of Assumptions.} We assume that contexts are finite and discrete. The context probability matrix $\A$ is known, and the best arm is unique. In Appendix \ref{apd: unknown context}, we provide a brief discussion on the complexity of the problem when $\A$ is unknown. The reward distribution for each action-context pair follows a Gaussian distribution with unit variance; however, our results can be easily generalized to the one-parameter exponential family. Additionally, we assume positivity, meaning that for each action, each context occurs with positive probability, i.e., $\amin = \min_{i,j} \A_{i,j} > 0$.

\section{Background: General Lower Bound} \label{sec: general-lower}

For the classical best arm identification problem, the authors in \cite{track-stop-garivier2016optimal} established a lower bound on the expected sample complexity of any $\delta$-correct algorithm. Extending this lower bound to our problem setting is straightforward as we show in Proposition \ref{th: general_lower1}. Before stating the proposition, let us define set Alt$(\bmu, \A)$, as the set of alternative parameter matrices $\bmu' \in \I$ such that $\istarmu \neq i^*(\bmu')$. Alt$(\bmu, \A) = \cup_{i \neq \istarmu} \C_i$,  where $\C_i \triangleq \{ \bmu' \in \I \mid \A_i^\top \bmu'_i > \A_{\istarmu}^\top \bmu'_{\istarmu} \}$. 
\begin{proposition}\label{th: general_lower1}
     For any bandit environment with parameter $\A$ and $\mu \in \I(\A)$ and any $\delta$-correct algorithm,  $\mathbb{E}_{\bmu, \A}[\tau_{\delta}] \geq T^*(\bmu, \A)d_B(\delta, 1- \delta)$, where $T^*(\bmu, \A)$ is 
        \begin{align} \label{eq: general_lower1}
            T^*(\bmu, \A)^{-1} \triangleq \sup_{\mathbf{w} \in \Delta^{n-1}} \inf_{\bmu' \in \text{Alt}(\bmu, \A)} \sum_{i \in [n]} w_i d(P^{\bmu}_{i}, P^{\bmu'}_{i}).
        \end{align}
\end{proposition}
This lower bound indicates that the optimal sampling strategy is to play each action $i$ in proportions to $w_i$, a solution of Equation~\eqref{eq: general_lower1}. Note that the bound is analogous to that of \cite{track-stop-garivier2016optimal}, with the key distinction that it depends on the context variable through $P^{\bmu}_{i}=\pr(Z, Y \mid X=i)$. 


For a given vector $\mathbf{w} \in \mathbb{R}^{n}$, define 
\begin{align} \label{eq: general_fi}
    f_j(\mathbf{w}, \bmu, \A) \triangleq \inf_{\bmu' \in \C_j} \sum_{i \in [n]} w_i d(P^{\bmu}_{i}, P^{\bmu'}_{i}).
\end{align}
Equation \eqref{eq: general_lower1} can then be reformulated as
\begin{align} \label{eq: general_lower2}
    T^*(\bmu, \A)^{-1} = \sup_{\mathbf{w} \in \Delta^{n-1}} \min_{i \neq \istarmu} f_i(\mathbf{w}, \bmu, \A).
\end{align}
In the following two sections, we derive lower bounds for the BAI problem for non-separator and separator post-action contexts with Gaussian rewards by explicitly characterizing $f_i(\mathbf{w}, \bmu, \A)$. We shall see that the set $\text{Alt}(\bmu, \A)$ differs in the two settings. In the separator context, $\bmu_i$s are equal. This restricts set $\text{Alt}(\bmu, \A)$, resulting in a smaller lower bound, which motivates tailoring the sampling algorithm to this setting to achieve the optimal sample complexity.

\section{Non-Separator Context} \label{sec: non-sep}

   In this section, we assume the context variable is a non-separator as depicted in Figure~\ref{fig: causal_graph}(a), meaning the distribution of the reward variable can depend on both the action and the context.
    We first derive an instance-dependent lower bound on the number of samples required for any $\delta$-correct algorithm and then propose an algorithm that asymptotically achieves this lower bound. All proofs for this section appear in Appendix \ref{apd: non-sep proofs}.


\subsection{Lower Bound}
    We establish a lower bound by explicitly solving the minimization problem defined in Equation \eqref{eq: general_fi} to determine the value of $\fiwmu$.
 
    \begin{restatable}[Non-Separator Lower Bound]{theorem}{nonsepLowerBound} \label{thm : non-sep lower} 
        Let $\delta \in (0, 1)$. Consider a bandit instance with a non-separator context and Gaussian reward distribution with unit variance, parametrized by matrices $\A$ and $\bmu$. Then, any $\delta$-correct algorithm with stopping time $\tau_{\delta}$ satisfies $\mathbb{E}[\tau_{\delta}] \geq T^*(\bmu, \A)  d_B(\delta, 1-\delta)$, where
                \begin{equation} \label{eq: middle non-sep LB}
                    T^*(\bmu, \A)^{-1} = \sup_{\mathbf{w} \in \Delta^{n-1}} \min_{i \neq i^*(\bmu)} \frac{\Delta_{i}^2}{2}\left( \frac{w_{i^*(\bmu)}w_i}{w_{i^*(\bmu)} + w_i}\right).
                \end{equation} 
    \end{restatable}

    To design the sampling rule for the algorithm, it is necessary to solve the optimization problem \eqref{eq: middle non-sep LB}. Solving this problem reduces to finding the root of a strictly decreasing function within a known interval, which can be efficiently achieved using methods such as binary search. For further details, refer to Appendix \ref{apd: opt-solving}.
    

\subsection{Learning Algorithm}

    In this part, we introduce the components of our algorithm and show that it achieves optimal sample complexity.
    
    \textbf{Stopping Rule.} The stopping rule operates independently of the sampling rule and determines, at each step, whether sufficient information has been gathered to terminate the algorithm or if further sampling is necessary. A widely used approach for designing stopping rules in exponential family bandits, including Gaussian bandits, is based on Generalized Likelihood Ratio (GLR) tests \cite{track-stop-garivier2016optimal, kaufmann2021mixture}. We describe how we adapt these tests to our setting in Appendix~\ref{apd: GLR}.

    
    At each round $t$, the GLR statistic is defined as 
    \begin{equation} \label{eq: glr-def}
        \lamht \triangleq  \inf_{\mathbf{\bmu'} \in \text{Alt}(\muht, \A)} \sum_{i \in [n], j \in [k]} N_{j,i}(t) \frac{(\hat{\bmu}_{j,i}(t) - \bmu'_{j,i})^2}{2},              
    \end{equation}
    provided that $\muht \in \I(\A)$ (i.e., $\muht$ induces a unique best arm). Otherwise, $\lamht$ is set to zero.    
    \(\lamht\) reflects the current confidence in the best arm identified based on the observations so far. A higher value of \(\lamht\) indicates greater confidence. To decide whether to terminate or continue playing, \(\lamht\) is compared to a sequential threshold \(\cdelt\), which determines if the confidence is sufficiently high. If $\muht \notin \I(\A)$, we lack enough information to identify the unique best arm, therefore we set \(\lamht = 0\).
    
    Based on the structure of the set $\text{Alt}(\muht, \A)$, we can simplify Equation \eqref{eq: glr-def} to
    $$
        \lamht = \min_{i \neq \istarmut} \frac{\hat{\Delta}_{i}^2}{2 \sum_{j \in [k]} \frac{\A_{j, \istarmut}^2}{N_{j,\istarmut}(t)}  + \frac{\A_{j, i}^2}{N_{j,i}(t)}},
    $$
    which allows $\lamht$ to be computed efficiently. Appendix \ref{apd: glr-non-sep} provides a detailed proof of this simplification.
    
    To design the sequential thresholds $\cdelt$, we use the deviation inequalities proposed in \cite{kaufmann2021mixture}, based on \textit{mixture martingales}. For each $\delta \in (0,1]$ and $t \in \mathbb{N}$, the threshold $\cdelt$ is derived as
    \begin{equation} \label{eq: non-sep threshold}
       \cdelt = 4k \ln\left( 4 + \ln\left( \frac{t}{2k} \right)\right) + 2k C^{g} \left( \frac{\ln\left(\frac{n-1}{\delta}\right)}{2k} \right),
    \end{equation}
    
    where $C^g(x) \simeq x + \ln(x)$ (refer to Definition \ref{def: c-g function}). For more details on the derivation of the thresholds, see the proof of Lemma \ref{lem: non-sep correctness} in Appendix \ref{apd: non-sep proofs}.
    
    The stopping time $\tau_{\delta}$ is then defined as
    \begin{equation} \label{eq: non-sep stop rule}
         \tau_{\delta} \triangleq \inf \{ t \in \mathbb{N} \mid \lamht > \cdelt \}.
    \end{equation}
    At the stopping time $\tau_{\delta}$, $\lamht$ is positive, implying that the best empirical arm $i^*(\bmuh(\tau_{\delta}))$ is unique due to the definition of GLR. The final suggestion $\hat{i}_{\tau}$ is equal to the unique estimated best arm $i^*(\bmuh(\tau_{\delta}))$. The following lemma establishes the correctness of this stopping rule when combined with any sampling rule.
    \begin{restatable}{lemma}{nonsepCorrectness} \label{lem: non-sep correctness}
        Consider a bandit instance with a non-separator context and Gaussian reward distribution with unit variance, parametrized by matrices $\A$ and $\bmu$. Any algorithm with the stopping rule of \eqref{eq: non-sep stop rule} is $\delta$-correct, that is, 
      % \begin{equation*}
          $  \pr_{\bmu, \A}(\tau_{\delta} < \infty, \hat{i}_{\tau} \neq i^*(\bmu)) \leq \delta.$
       % \end{equation*}
        %which shows the $\delta$-correctness of any algorithm using this stopping rule. 
    \end{restatable}

\textbf{Sampling Rule.}
    We now propose a sampling strategy, which asymptotically achieves optimal sample complexity with the stopping rule in \eqref{eq: non-sep stop rule}. Define $\mathbf{w}^*(\bmu, \A)$ as the solution to the optimization problem
    \begin{align*}
         \argmax_{\mathbf{w} \in \Delta^{n-1}} \min_{i \neq i^*(\bmu)} \frac{\Delta_{i}^2}{2}\left( \frac{w_{i^*(\bmu)}w_i}{w_{i^*(\bmu)} + w_i}\right).
    \end{align*}
    For any $\bmu \in \I$, the optimal weights $\mathbf{w}^*(\bmu, \A)$ are unique and can be efficiently estimated (see Appendix \ref{apd: opt-solving}). In our theoretical analysis, we assume the existence of an oracle that computes $\mathbf{w}^*(\bmu, \A)$ exactly. However, in practice, we estimate the optimal weights $\mathbf{w}^*(\bmu, \A)$. These weights correspond to the fraction of times each arm is pulled by an optimal algorithm. Based on this intuition, various algorithms have been developed for different identification problems in the literature. These algorithms compute the optimal weights at each step given the estimated reward matrix $\muht$ and track these sequential weights. We adopt the \textit{D-tracking} sampling rule first introduced in \cite{track-stop-garivier2016optimal}. At round $t$ of the algorithm, the D-tracking rule defines $\mathcal{U}_t = \{ i \in [n] \ \big| \ N^{X}_{i} (t) \leq \max\left(\sqrt{t} - \frac{n}{2}, \, 0\right) \},$ which contains the arms that are under-sampled. The algorithm then selects the next action $X_{t+1}$ as 
    \begin{align} \label{eq: d-tracking}
        X_{t+1} = \begin{cases}
            \argmin_{i \in \mathcal{U}_t} N^{X}_{i}(t) \hspace{2.5cm} \text{ if } \mathcal{U}_t \neq \emptyset, & \\ 
            \argmax_{i \in [n]} t w_i^*(\muht, \A) - N^{X}_{i}(t) \quad \text{ o.w.} & {}
        \end{cases}
    \end{align}
    It is noteworthy that D-tracking operates correctly only when  $\hat{\bmu} \in \I(\A)$, that is, a single optimal arm exists. If this condition is not satisfied at any step $t$, the next action is selected uniformly random. This scenario almost surely ceases to occur as the number of samples increases and $\hat{\bmu}(t)$ coverages to $\bmu\in \I(\A)$, which is assumed to imply a single best arm.
    
    The pseudocode of our algorithm, called \textit{Non-Separator Track and Stop (NSTS)}, is presented in Algorithm \ref{algo: non-sep}. The following theorem proves its optimality.
    \begin{algorithm}[t]
        \caption{Non-Separator Track and Stop (NSTS)}
        \label{algo: non-sep}
        \begin{algorithmic}[1] 
            \STATE \textbf{Input:} Context probability matrix $\A$.
            \STATE \textbf{Initialization:} Pull arms until collecting at least one sample from $P(Y|X=i, Z=j)$ for each $i \in [n], j \in [k]$, then set $t$ equal to the number of rounds played until now.
            \WHILE{$\lamht \leq \cdelt$}
                % \IF{$\muht \in \I(\A)$}
                %     % \STATE Find $\mathbf{w}^*(\muht, \A)$ using Lemma \ref{lem: non-sep w1}. 
                \STATE Pull arm $X_{t+1}$ based on D-tracking rule \eqref{eq: d-tracking}. 
                % \ELSE
                    % \STATE Pull arm $X_{t+1}$ randomly.
                % \ENDIF    
                \STATE Update $\muht$ and $t \gets t + 1.$
            \ENDWHILE 
            \STATE \textbf{Output:} $i^*(\muht)$. 
        \end{algorithmic}
    \end{algorithm}

    \begin{restatable}[Non-Separator Upper Bound]{theorem}{nonsepUpperBound} \label{thm : non-sep upper} 
            Algorithm \ref{algo: non-sep} applied to a bandit instance with a non-separator context and Gaussian reward distribution with unit variance, parametrized by matrices $\A$ and $\bmu$ attains
                \begin{equation*}
                    \limsup_{\delta \rightarrow 0} \frac{\mathbb{E}_{\bmu, \A}[\tau_{\delta}]}{\logdel} \leq T^*(\bmu, \A),
                \end{equation*}
                where $T^*(\bmu, \A)$ is defined in Equation \eqref{eq: middle non-sep LB}
    \end{restatable}



\section{Separator Context} \label{sec: sep}


This section presents the results for the \textit{bandit with separator post-action context} problem, where the reward distribution depends solely on the value of the context, i.e., $\pr(Y \mid X=i, Z=j) = \pr(Y \mid Z=j)$, as illustrated in Figure \ref{fig: causal_graph}(b). This condition implies that all columns of the matrix $\bmu$ are identical. For ease of notation, in this section, we represent $\bmu$ as a vector in $\mathbb{R}^k$.



\subsection{Lower Bound}
    The following theorem provides a lower bound on the expected sample complexity of any $\delta$-correct algorithm in the separator setting. 
    This lower bound is derived by explicitly solving the minimization problem defined in \eqref{eq: general_fi} to determine the value of $\fiwmu$.
    
    \begin{restatable}[Separator Lower Bound]{theorem}{sepLowerBound} \label{thm : sep lower} 
            Let $\delta \in (0, 1)$. Consider a bandit instance with a separator context and Gaussian reward distribution with unit variance, parameterized by the matrix $\A$ and the vector $\bmu$. Then, any $\delta$-correct algorithm with stopping time $\taudel$ satisfies $\mathbb{E}[\tau_{\delta}] \geq T^*(\bmu, \A)  d_B(\delta, 1-\delta)$, where
                    \begin{equation} \label{eq: sep LB1}
                        T^*(\bmu, \A)^{-1} = \sup_{\mathbf{w} \in \Delta^{n-1}} \min_{i \neq i^*(\bmu)} \frac{\Delta_{i}^2}{2 \sum_{j \in [k]} \frac{(\A_{j, \istarmu} - \A_{j, i})^2}{\sum_{l \in [n]} w_l \A_{j, l}}}.
                    \end{equation} 
    \end{restatable}
    
    
    The values $w_{z,j} \triangleq \sum_{l \in [n]} w_l \A_{j, l}$ in Equation \eqref{eq: sep LB1} represent the expected frequency of observing context $Z = j$. Note that the optimization problem in \eqref{eq: sep LB1} depends only on $w_{z,j}$s. Consequently, defining the vector $\wb_z = (w_{z,1}, w_{z,2}, \dots, w_{z,k})$, Equation \eqref{eq: sep LB1} can be expressed as
    \begin{equation} \label{eq: sep LB2}
        T^*(\bmu, \A)^{-1} = \sup_{\mathbf{w}_z \in \ch(\A)} \min_{i \neq i^*(\bmu)} \frac{\Delta_{i}^2}{2 \sum_{j \in [k]} \frac{(\A_{j, \istarmu} - \A_{j, i})^2}{w_{z,j}}},
    \end{equation}
    where $\ch(\A)$ is the convex hull of the points $\A_1, \A_2, \ldots, \A_n$, and ${\mathbf{w}_z \in \ch(\A)}$ because $\wb_z$ is a convex combination of these points. The optimal solution to this new formulation represents the expected frequency of observing context values generated by any optimal algorithm.

    It can be shown that the objective functions in both Equations~\eqref{eq: sep LB1} and \eqref{eq: sep LB2} are concave. Since the domains $\Delta^{n-1}$ and $\ch(\A)$ are convex, both optimization problems can be tackled using convex optimization techniques. For more details, refer to \cite{frank-wolf-wang2021fast, menard2019gradient}, where the authors discuss how to solve such optimization problems in BAI settings.


\subsection{Learning Algorithm}

    \textbf{Stopping Rule.} Similar to the non-separator setting, we use a Generalized Likelihood Ratio (GLR) test for the stopping rule. The GLR statistic in this setting is defined as
    
    \begin{equation} \label{eq: glr-def-sep}
            \lamht \triangleq  \inf_{\bmu' \in \text{Alt}(\muht, \A)} \sum_{j \in [k]} \numcontext \frac{(\hat{\bmu}_{j}(t) - \bmu'_{j})^2}{2},
    \end{equation}
    provided that $\muht \in \I(\A)$ and otherwise $\lamht$ is set to zero. This definition can be simplified to
    \begin{align*}
             \lamht = \min_{i \neq \istarmut} \frac{\hat{\Delta}_{i}^2}{2 \sum_{j \in [k]} \frac{(\A_{j, \istarmut} - \A_{j, i})^2}{\numcontext}}.
    \end{align*}
    Appendix \ref{apd: GLR} and \ref{apd: glr-sep} provide a proof of how to compute the GLR statistic and a justification of the above simplification.
    
    
    To design the sequential thresholds, we apply the result of \cite{kaufmann2021mixture} for adaptive sequential testing, which suggests setting
    \begin{equation} \label{eq: sep threshold}
           \cdelt = 2 \sum_{j \in [k]} \ln\left( 4 + \ln\left( \numcontext \right)\right) + k C^{g} \left( \frac{\ln\left(\frac{1}{\delta}\right)}{k} \right),
    \end{equation}
    
    where $C^{g}$ is the same function introduced in Equation \eqref{eq: non-sep threshold}. The stopping time $\tau_{\delta}$ is then defined as
    \begin{equation} \label{eq: sep stop rule}
         \tau_{\delta} \triangleq \inf \{ t \in \mathbb{N} \mid \lamht > \cdelt \}.
    \end{equation}
    
     At the stopping time $\tau_{\delta}$, $\lamht$ is positive and the final suggestion $\hat{i}_{\tau}$ is equal to the unique estimated best arm $i^*(\bmuh(\tau_{\delta}))$. The following lemma establishes the correctness of this stopping rule when combined with any sampling rule.
    \begin{restatable}{lemma}{sepCorrectness} \label{lem: sep correctness}
             Consider a bandit instance with a separator context and Gaussian reward distribution with unit variance, parametrized by the matrix $\A$ and the vector $\bmu$. Any algorithm with the stopping rule of \eqref{eq: sep stop rule} is $\delta$-correct, that is, $\pr_{\bmu, \A}(\tau_{\delta} < \infty, \hat{i}_{\tau} \neq i^*(\bmu)) \leq \delta.$
    \end{restatable}


\textbf{Sampling Rule.} As mentioned earlier, the lower bound and stopping rule depend solely on how often each context value is observed, regardless of the actions that produce these contexts. Motivated by this observation, we introduce our new sampling method, called \textit{ Geometrical Tracking (G-tracking)}, which directly tracks the optimal frequency of observed contexts.


% To design the sampling rule, a straightforward approach is to solve the optimization problem \eqref{eq: sep LB1} at each round to determine the optimal frequency of playing each arm and then use a tracking rule such as  C-tracking \cite{track-stop-garivier2016optimal} to address the problem. However, as discussed earlier, the lower bound and the stopping rule depend only on the number of times the context values are observed, and are independent of the actions that generate these contexts. Thus tracking the frequency of arm pulls is an indirect approach compared to tracking the context directly as randomness in the generating process of context values means that the frequency of collected contexts in general differs from that of the arms played.

% A more direct approach entails solving the optimization problem \eqref{eq: sep LB2} and directly tracking the optimal frequency of the context values. In this subsection, we introduce our new sampling method, called \textit{G-tracking}, which directly tracks the optimal proportions of observed contexts. In the experiments, we compare these two approaches and demonstrate the superiority of G-tracking compared to tracking arms directly.




Note that this setting can be viewed as an active off-policy learning problem in $k$-armed bandit, where each arm corresponds to a context value. However, the agent cannot directly select a context to play. Instead, he has access to $n$ distinct policies (the original $n$ arms in our problem), where choosing policy $i$, probabilistically selects a context according to the known distribution $\A_i$. Each vector $\A_i$ lies in the $(k-1)$-dimensional simplex and encodes the probability of selecting a context by playing policy $i$.

Under this interpretation, the policy space is defined as the convex hull of the points $\A_1, \A_2, \ldots, \A_n$, denoted by $\ch(\A)$. In each round $t$, choosing a policy $\pol(t) \in \ch(\A)$ is equivalent to fixing a distribution on context $Z$. Note that, there exists a distribution (not necessarily unique) on arms $X$ which is compatible with $\pol(t)$ and we pull an arm at time $t$ according to this distribution.

We now propose a new sampling rule that directly tracks the optimal frequency of observing contexts and asymptotically achieves optimal sample complexity when combined with the stopping rule in Equation \eqref{eq: sep stop rule}. For any vector $\bmu \in \I(\A)$ (i.e.,  a unique best arm exists), We define the set $\wzstar{\bmu}$ contains the solutions of the optimization problem
\begin{align} \label{eq: sep-optimal-weights}
    \argmax_{\mathbf{w}_z \in \ch(\A)} \min_{i \neq i^*(\bmu)} \frac{\Delta_{i}^2}{2 \sum_{j \in [k]} \frac{(\A_{j, \istarmu} - \A_{j, i})^2}{w_{z,j}}},
\end{align} 
which is the problem derived in Equation \eqref{eq: sep LB2}. 
Unlike the non-separator setting, in the separator setting, the optimal weights may not be unique, i.e., $\wzstar{\bmu}$ is not a singleton set. However, the optimization problem \eqref{eq: sep-optimal-weights} is a convex optimization problem for each $\bmu \in \I$ and can be solved using numerical methods such as Frank-Wolfe \cite{pmlr-v28-jaggi13, frank-wolf-wang2021fast}. These methods may converge to any of the optimal solutions within the solution set $\wzstar{\bmu}$, and we show that our approach performs correctly under this condition.

At each round $t$, our sampling rule first solves the optimization problem \eqref{eq: sep-optimal-weights} using the current empirical estimate $\muht$ to find a solution $\wb^{*}(t) \in \wzstar{\muht}$. This vector lies within $\ch(\A)$. Recall that vector $\frac{\Nb^{Z}(t)}{t}$ denotes the actual frequency of observed contexts up to round $t$ and it lies in the simplex $\Delta^{k-1}$. Our sampling rule, called G-tracking, determines the next policy $\pol(t + 1) \in \ch(\A)$ by connecting $\frac{\Nb^{Z}(t)}{t}$ to $\wb^{*}(t)$ and extending this line until it intersects the boundary of $\ch(\A)$. The algorithm then pulls arms according to a distribution $\pi \in \Delta^{n-1}$, where $\A \pi = \pol(t + 1)$. A compatible distribution $\pi$ always exists because $\pol(t + 1) \in \ch(\A)$, but it may not be unique.



Figure \ref{fig: sampling} illustrates the sampling rule for an instance with $k=3$ and $n=5$. Note that while the policy points always lie within $\ch(\A)$, $\frac{\Nb^{Z}(t)}{t}$ may lie outside this set (as shown in Figure \ref{fig: sampling}). In such cases, the policy point $\pol(t + 1)$ is the intersection point of the line passing through $\frac{\Nb^{Z}(t)}{t}$ and $\wb^{*}(t)$ with the boundary of policy space, ensuring that $\wb^{*}(t)$ lies between $\pol(t + 1)$ and $\frac{\Nb^{Z}(t)}{t}$.




The main advantage of the G-tracking rule is that it ensures $\wb^{*}(t)$ is a convex combination of the observed context frequencies $\frac{\Nb^{Z}(t)}{t}$ and the next action $\pol(t + 1)$. Thus, if a context $i$ is under-explored ($\frac{N^{Z}_{i}(t)}{t} \leq w^{*}_{i} (t)$), then $\pol(t + 1)$ allocates a probability greater than $w^{*}_{i} (t)$ to that context. Conversely, for each over-explored context $i$ ($\frac{N^{Z}_i(t)}{t} \geq w^{*}_{i} (t)$), the next policy allocates a probability lower than $w^{*}_{i} (t)$. This ``balancing" property holds for all contexts simultaneously.
Note that the balancing property holds for every point on the segment connecting $\wb^{*}(t)$ to $\pol(t + 1)$. Therefore, our proof remains valid for any algorithm that picks a point on this segment. In our algorithm, we choose the $\pol(t + 1)$ on the boundary of the convex hall to achieve the fastest possible tracking of the optimal weights.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/G-tracking.pdf}
    \caption{Illustration of G-tracking rule for an instance with $k=3, n=5$. The triangle depicts the two-dimensional simplex, and the green area shows the policy space 
    $\ch(\A)$.}
    \label{fig: sampling}
\end{figure}
\begin{algorithm}[t]
    \caption{Separator Track and Stop (STS)}
    \label{algo: sep}
    \begin{algorithmic}[1] 
        \STATE \textbf{Input:} Context probability matrix $\A$.
        \STATE \textbf{Initialization:} Pull arms until collecting at least one sample from $P(Y|Z=i)$ for each $i \in [k]$. Set $t$ to the number of rounds played during this initialization.
        \WHILE{Stopping rule \eqref{eq: sep stop rule} is not satisfied}
            % \STATE Find optimal weights $\mathbf{w}_z^*(\muht, \A)$. 
            \STATE Find policy $\pol(t + 1) \in \ch(\A)$ based on G-tracking rule.
            \STATE Play according to a distribution $\pi$ such that $\A \pi = \pol(t + 1)$. 
            \STATE Update $\muht$ and $t \gets t + 1.$
        \ENDWHILE 
        \STATE \textbf{Output:} $i^*(\muht)$. 
    \end{algorithmic}
\end{algorithm}


The pseudocode of our algorithm called \textit{Separator Track and Stop (STS)} is presented in Algorithm \ref{algo: sep} which uses G-tracking for sampling with the stopping rule of \eqref{eq: sep stop rule}. The following theorem states that our algorithm achieves the optimal sample complexity.
\begin{restatable}[Separator Upper Bound]{theorem}{sepUpperBound} \label{thm : sep upper} 
       Algorithm \ref{algo: sep} applied to a bandit instance with a separator context and Gaussian reward distribution with unit variance, parameterized by the matrix $\A$ and the vector $\bmu$ attains
            \begin{equation*}
                \limsup_{\delta \rightarrow 0} \frac{\mathbb{E}_{\bmu, \A}[\tau_{\delta}]}{\logdel} \leq T^*(\bmu, \A),
            \end{equation*}
            where $T^*(\bmu, \A)$ is defined in Equation \eqref{eq: sep LB1}.
\end{restatable}


\section{Experiments} \label{sec: experiment}
This section presents the experimental results of running the proposed algorithms and various benchmark algorithms on multiple instances. We use the python CVXPY library \cite{cvx1-diamond2016cvxpy, cvx2-agrawal2018rewriting} to solve the required optimization problems. Below, we describe the experiments conducted for both the non-separator and separator scenarios.
For additional experiments and a more comprehensive explanation, refer to Appendix \ref{apd: experiment}.
The implementation is available at \url{https://github.com/ban-epfl/BAI-with-Post-Action-Context}.

\subsection{Non-Separator Experiment} 

We generate random instances for each combination of $n \in \{5, 10, 15\}$ and $k \in \{3, 5, 7\}$. Each instance has $n$ arms and $k$ context values, with rewards drawn from Gaussian distributions of unit variance. The mean matrix \(\bmu\) lies in \([0,10]^{k \times n}\), and \(\amin = \min_{i,j} \A_{j,i} \ge \frac{1}{4k}\). Without loss of generality, we assume the first arm is the best. We randomly generate the matrices $\bmu$ and $\A$, ensuring that for each $i > 1$, $\Delta_i \in [\frac{1}{2n}, \frac{i+1}{2n}]$.  This constraint prevents unchallenging instances in which all gaps are large. Throughout our experiments, we set the confidence parameter $\delta$ to $0.1$.

We compare our algorithm (NSTS) with the classic track-and-stop (TS) algorithm, which disregards the context variable and instead operates solely on the rewards with D-tracking. Note that the TS does not have a theoretical guarantee in this setting because each arm's reward distribution is a mixture of Gaussians, which does not belong to the one-parameter exponential family required for theoretical guarantees of TS \cite{track-stop-garivier2016optimal}. Note that as the expected values of the rewards are in $[0,10]$, it can be shown that the rewards for each arm follow a sub-Gaussian distribution. We then use this property to design a stopping rule. For more details, refer to Appendix \ref{apd: experiment}. 

Table \ref{tab: non-sep} reports the average number of rounds required by each algorithm before stopping, aggregated over $75$ runs. The results indicate that leveraging post-action contexts can significantly improve performance.

\begin{table}[t]
\centering
\caption{Average stopping times of NSTS and TS algorithms for different values of \( n \) and \( k \).}
\begin{tabular}{cccc}
\toprule
\( n \) & \( k \) & NSTS & TS \\
\midrule
\multirow{3}{*}{5}  & 3 & 30635  & 427872  \\
                    & 5 & 31515  & 308976  \\
                    & 7 & 60960  & 534147  \\
                \midrule
\multirow{3}{*}{10} & 3 & 92034  & 1244462 \\
                    & 5 & 224236 & 2090792 \\
                    & 7 & 320224 & 2433472 \\
                \midrule    
\multirow{3}{*}{15} & 3 & 272064 & 3667765 \\
                    & 5 & 425685 & 3768745 \\
                    & 7 & 428615 & 3573301 \\
\bottomrule
\end{tabular}
\label{tab: non-sep}
\end{table}


\begin{figure}[t!]
        \centering
        \begin{subfigure}[b]{0.5\textwidth}
            \centering
            \includegraphics[width = \textwidth]{figs/9_Ts.pdf}
            \caption{Comparison of stopping times over 150 runs.}
            \label{fig: sep-average-T}
        \end{subfigure}
        \begin{subfigure}[b]{0.5\textwidth}
            \centering
            \includegraphics[width = \textwidth]{figs/2_9.pdf}
            \caption{Average $L^2$ distance between the vectors $\frac{\Nb^Z(t)}{t}$ and $\wzstar{\bmu}$ during the learning process.}
            \label{fig: sep-w-distance}
        \end{subfigure}
        \caption{The results of different algorithms for an instance in Equation \ref{eq: sep-instance}.}
        \label{fig: sep-main-fig}
\end{figure}

\subsection{Separator Experiment} \label{sub: sep-exp}
We consider an instance with $n = 3$ arms and $k=3$ context values, specified by the following parameters

\begin{align} \label{eq: sep-instance}
     \delta = 0.01, ~ 
     \bmu = \begin{bmatrix}
        1.0 \\
        0.1 \\
        0.3
    \end{bmatrix}, ~
    \A = \begin{bmatrix}
        0.9 & 0.9 & 0.1 \\
        0.09 & 0.01 & 0.45 \\
        0.01 & 0.09 & 0.45
    \end{bmatrix}.
\end{align}
This instance is interesting because the first and second arms have very close expected rewards, making it difficult to distinguish the best arm with high probability. To do so, one must observe a substantial number of samples from contexts $2$ and $3$, which have low probabilities of occurring when pulling arms $1$ and $2$. Consequently, an effective strategy involves frequently selecting arm $3$, even though it has a notably lower expected reward than arms $1$ and $2$.

This behavior differs from classic BAI, where suboptimal arms are typically chosen less often. In instances like this, algorithms designed for standard BAI (e.g., track-and-stop) become suboptimal because they do not take full advantage of the post-action context information.


We compare Algorithm \ref{algo: sep} (STS) with the following two baselines:
(i) \emph{Track-and-Stop (TS).} This classic algorithm ignores the context values and applies D-tracking to the actions.
(ii) \emph{Lazy Track-and-Stop (LTS).} It can be shown that the BAI problem with a separator post-action context can be reduced to a BAI problem in linear bandits, while the mean values of the reward distributions are bounded. For more details on this reduction, refer to Appendix \ref{apd: experiment}. Consequently, identifying the best action translates into best-arm identification in a finite-arm linear bandit. We then apply Lazy Track-and-Stop \cite{linear-lazy-jedra2020optimal}, which is an optimal algorithm for BAI in linear bandits.


%(These figures are not finalized and we are running them for more rounds)
Figure \ref{fig: sep-average-T} illustrates the average number of arm pulls for each algorithm on the instance in Equation \eqref{eq: sep-instance}, aggregated over at least $150$ runs for each algorithm. Figure \ref{fig: sep-w-distance} shows the average $L^2$ distance of the vector $\frac{\Nb^{Z}(t)}{t}$ from the optimal frequency of contexts $\wzstar{\bmu}$ over time, which captures the convergence speed of each algorithm.
As the figures demonstrate, ignoring the post-action contexts can lead to significant sub-optimality, which is even more pronounced in the separator setting compared to the non-separator setting.




\section{Conclusion}
We introduced a new BAI problem with post-action context in the fixed-confidence setting. We considered two settings for post-action context. For both settings, non-separator and separator context, we derived lower bounds on sample complexity and proposed algorithms that achieve optimal sample complexity.

An interesting direction for future work is to consider the setting where context probability matrix $\A$ is unknown. In this case, As discussed in Appendix~\ref{apd: unknown context}, the optimization problem in Equation \eqref{eq: general_lower1} becomes non-convex which renders designing a computationally efficient algorithm challenging.



\section*{Acknowledgments}
    This research was in part supported by the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40\_180545 and Swiss SNF project 200021\_204355 /1.


% \section*{Impact Statement}
%     This paper presents work whose goal is to advance the field
%     of Theory of Machine Learning. There are many potential societal
%     consequences of our work, none of which we feel must be
%     specifically highlighted here.

\bibliography{bibliography}
\bibliographystyle{icml2025}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\input{appendix}






\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
