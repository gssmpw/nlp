\section{Related Work}
\subsubsection{World Models in LLMs.} World models have been defined as a compact, coherent, and interpretable representation of the generative process underlying the training data~\cite{GurneeTegmark2023,HaSchmidhuber2018,Vafa2024}. Some authors argue that LLMs lack internal world models capable of predicting world states and simulating action outcomes, which can impair their performance in agentic and planning tasks~\cite{Bisk2020,BenderEtAl2021}. On the other hand, some studies have identified specific neurons that encode space and time, demonstrating internal models that remain robust to variations in prompting~\cite{GurneeTegmark2023}. Other investigations have found evidence for internal models of the non-linguistic world—ranging from perceptual structures such as color to spatial orientation concepts, cardinal directions, and object properties~\cite{PatelPavlick2022,LiNyeAndreas2021,Abdou2021}. Implicit world models have also been described in goal-oriented contexts where the representation is influenced by an agent’s objectives~\cite{Li2021}.

\subsubsection{Internal Representations and Mechanistic Interpretability.} Mechanistic interpretability has emerged as a promising avenue for understanding the inner workings of LLMs~\cite{Olah2020,Elhage2021,Templeton2024}. This approach has been used to analyze emergent behaviors in larger models and to trace how internal representations influence output, providing essential insights for causality and AI safety~\cite{Wang2022,Nanda2023,Bereska2024}. For example, research has shown that neurons across different layers tend to specialize, with middle layers often containing neurons that represent higher-level contextual features~\cite{Geva2020,Durrani2022,Meng2022,Gurnee2023}. Furthermore, some neurons exhibit task-specific activations and can be predictive of model performance on these tasks~\cite{Meng2022,Wang2022b,LengXiong2024,Song2024}.

\subsubsection{Prompting Techniques and Prompt Influence on Outcome.} A growing body of work has established that LLM performance is highly sensitive to the specific prompting techniques employed, across a variety of benchmarks and tasks~\cite{Salinas2024,Leidinger2023,Anagnostidis2024}. Even variations in prompt formatting have been found to lead to notable differences in model behavior~\cite{Sclar2023}. This has driven the development of a wide range of prompting techniques which become highly relevant when attempting to improve LLM performance~\cite{Wei2022Chain,Sahoo2024}.

\subsubsection{LLMs for Spatial Navigation.} LLMs have also been applied to spatial navigation tasks despite being trained solely on text~\cite{Cote2019,Huang2022}. These tasks have commonly been represented as text-based sequences, where models are asked to provide information about the environment or actions that have effects in the world~\cite{Zhu2023,Yamada2023,Lin2023}. Previous studies have evaluated LLMs as agents navigating grid-world environments, where models must plan a full path in advance to locate goals and avoid obstacles~\cite{Aghzal2023,McDonald2023}. These approaches demonstrate that LLMs can solve spatial navigation tasks, although small models have shown limited generalization to variations such as differing grid sizes or obstacle configurations~\cite{Aghzal2023}.

\subsubsection{Spatial Maps in Neuroscience.} The neuroscience literature provides a foundational perspective on spatial representations through studies of spatial maps in the brain. Early work on place cells in the hippocampus of rats revealed that specific neurons encode an animal’s position in a two-dimensional space~\cite{OKeefeDostrovsky1971,OKeefe1976}. Subsequent research identified other spatially tuned cells—such as grid cells, which encode grid-like patterns; head-direction cells, which signal the direction of gaze; and boundary cells, which respond to environmental edges~\cite{Hafting2005,Lever2009,Taube1990}. The specialization observed in biological systems suggests that generalist artificial systems, including LLMs, might similarly develop internal representations for spatial orientation and navigation.