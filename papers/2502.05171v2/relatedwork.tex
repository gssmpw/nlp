\section{Related Work Overview}
The extent to which recurrence is a foundational concept of machine learning is hard to overstate \citep{amari_learning_1972,hopfield_neural_1982,braitenberg_vehicles_1986,gers_recurrent_2000,sutskever_recurrent_2008}. Aside from using recurrence to move along sequences, as in recurrent neural networks, it was understood early to also be the key to adaptive computation \citep{schmidhuber_self-delimiting_2012,graves_adaptive_2017}. For transformers, recurrence was applied in \citet{dehghani_universal_2019}, who highlight the aim of recurrent depth to model \textit{universal}, i.e. Turing-complete, machines \citep{graves_neural_2014}. It was used at scale (but with fixed recurrence) in \citet{lan_albert_2019} and an interesting recent improvement in this line of work are described in \citet{tan_sparse_2023,abnar_adaptivity_2023}, \citet{mathur_mind_2024} and \citet{csordas_moeut_2024}. \citet{schwarzschild_can_2021,bansal_end--end_2022,bear_rethinking_2024} and \citet{mcleish_transformers_2024} show that depth recurrence is advantageous when learning generalizable algorithms when training with randomized unrolling and input injections. Recent work has described depth-recurrent, \textit{looped}, transformers and studied their potential benefits with careful theoretical and small-scale analysis \citep{giannou_looped_2023,gatmiry_can_2024,yang_looped_2024,fan_looped_2025}.

From another angle, these models can be described as neural networks learning a fixed-point iteration, as studied in \textit{deep equilibrium} models \citep{bai_deep_2019,bai_neural_2022}. They are further related to diffusion models \citep{song_generative_2019}, especially latent diffusion models \citep{rombach_high-resolution_2022}, but we note that language diffusion models are usually run with a per-sequence, instead of a per-token, iteration count \citep{lee_deterministic_2018}. A key difference of our approach to both equilibrium models and diffusion models is in the training objective, where equilibrium methods solve the ``direct'' problem \citep{geiping_parametric_2019-1}, diffusion models solve a surrogate training objective, and our work suggests that truncated unrolling is a scalable alternative.

More generally, all architectures that recur in depth can also be understood as directly learning the analog to the gradient of a latent energy-based model \citep{lecun_loss_2005,lecun_path_2022}, to an implicitly defined intermediate optimization layer \citep{amos_optnet:_2017}, or to a Kuramoto layer \citep{miyato_artificial_2024}. Analogies to gradient descent at inference time also show the connection to test time adaptation \citep{sun_test-time_2020}, especially test-time adaptation of output states \citep{boudiaf_parameter-free_2022}.

Aside from full recurrent-depth architectures, there also exist a number of proposals for hybrid architectures, such as models with latent sub-networks \citep{li_deep_2020}, LoRA adapters on top of weight-shared layers \citep{bae_relaxed_2024}, or (dynamic) weight-tying of trained models \citep{hay_dynamic_2023,liu_mobilellm_2024}. 

As mentioned in \cref{sec:natural}, while we consider the proposed recurrent depth approach to be a very natural way to learn to reason in continuous latent space from the ground up, the works of \citet{hao_training_2024,cheng_compressed_2024} and \citet{liu_deliberation_2024} discuss how to finetune existing fixed-depth transformers with this capability. These works have a similar aim to ours, enabling reasoning in latent space, but approach this goal from separate directions.

For additional discussions related to the idea of constructing a prior that incentivizes reasoning and algorithm learning at the expense of memorization of simple patterns, we also refer to \citet{chollet_measure_2019}, \citet{schwarzschild_deep_2023}, \citet{li_strong_2020} and \citet{moulton_many_2023}.