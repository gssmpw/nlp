\section{Related Work Overview}
The extent to which recurrence is a foundational concept of machine learning is hard to overstate ____. Aside from using recurrence to move along sequences, as in recurrent neural networks, it was understood early to also be the key to adaptive computation ____. For transformers, recurrence was applied in ____, who highlight the aim of recurrent depth to model \textit{universal}, i.e. Turing-complete, machines ____. It was used at scale (but with fixed recurrence) in ____ and an interesting recent improvement in this line of work are described in ____, ____ and ____. ____ and ____ show that depth recurrence is advantageous when learning generalizable algorithms when training with randomized unrolling and input injections. Recent work has described depth-recurrent, \textit{looped}, transformers and studied their potential benefits with careful theoretical and small-scale analysis ____.

From another angle, these models can be described as neural networks learning a fixed-point iteration, as studied in \textit{deep equilibrium} models ____. They are further related to diffusion models ____, especially latent diffusion models ____, but we note that language diffusion models are usually run with a per-sequence, instead of a per-token, iteration count ____. A key difference of our approach to both equilibrium models and diffusion models is in the training objective, where equilibrium methods solve the ``direct'' problem ____, diffusion models solve a surrogate training objective, and our work suggests that truncated unrolling is a scalable alternative.

More generally, all architectures that recur in depth can also be understood as directly learning the analog to the gradient of a latent energy-based model ____, to an implicitly defined intermediate optimization layer ____, or to a Kuramoto layer ____. Analogies to gradient descent at inference time also show the connection to test time adaptation ____, especially test-time adaptation of output states ____.

Aside from full recurrent-depth architectures, there also exist a number of proposals for hybrid architectures, such as models with latent sub-networks ____, LoRA adapters on top of weight-shared layers ____, or (dynamic) weight-tying of trained models ____. 

As mentioned in \cref{sec:natural}, while we consider the proposed recurrent depth approach to be a very natural way to learn to reason in continuous latent space from the ground up, the works of ____ and ____ discuss how to finetune existing fixed-depth transformers with this capability. These works have a similar aim to ours, enabling reasoning in latent space, but approach this goal from separate directions.

For additional discussions related to the idea of constructing a prior that incentivizes reasoning and algorithm learning at the expense of memorization of simple patterns, we also refer to ____, ____, ____ and ____.