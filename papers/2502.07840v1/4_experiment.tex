\section{experiment}
\label{sec:experiment}
\subsection{Experiment Setup}  
\subsubsection{Datasets}



We evaluated the performance of TranSplat on completing depth from novel rendered views using three datasets: one real-world transparent object dataset with known categories and two synthetic datasets. The first synthetic dataset contains identical objects to those in the real-world dataset, while the second has unseen object models but within the same categories. All synthetic datasets were rendered using BlenderProc \cite{blender}.

For the real-world dataset, we used the TransPose benchmark \cite{kim2024transpose}, which consists of multispectral, multiview sequential images of transparent objects across 20 categories. Each sequence contains 52 and 53 images with corresponding object depth ground truths for training and testing, respectively. For the synthetic datasets, we created two versions: Synthetic TransPose and Synthetic ClearPose. The Synthetic TransPose dataset was rendered using 3D CAD models provided by the real TransPose dataset, matching both the categories and specific objects. In contrast, the Synthetic ClearPose dataset features different object models within the same categories, designed to test TranSplat's performance on unseen objects. Both synthetic datasets contain 100 sequential images per sequence with ground truth depths for training and testing.
\begin{figure*}[]
    \centering
    \includegraphics[width=0.85\linewidth]{figure/result_real.pdf}
    \caption{Depth completion results of TRansPose test sequence 7 and 26. }
    \label{fig:real}
    \vspace{-2mm}
\end{figure*}

\input{tab/real_TRansPose.tex}
\subsubsection{Implementation Details}

For TranSplat, we trained both the latent diffusion-based surface embeddings extractor and the \ac{3D-GS} for neural rendering. Built on ControlNet \cite{zhang2023controlnet}, we froze the latent diffusion UNet and kept the ControlNet counterpart trainable. Training was performed on $256 \times 256$ images from both real and synthetic TRansPose datasets, with a batch size of 32 and a learning rate of 1.0e-6, using the AdamW optimizer with a cosine scheduler. For the \ac{3D-GS}, we followed the settings described in 3D-GS \cite{kerbl20233d}. All models were trained on four Nvidia A6000 GPUs. Further details on the training configurations are available on our project page.

\subsubsection{Baselines}

We used four models as baselines for our evaluations: DexNeRF \cite{ichnowski-2022-corl} and Residual-NeRF \cite{duisterhof2024residualnerf}, which are recent models for novel view completion of transparent objects, as well as \ac{3D-GS} \cite{kerbl20233d} and SuGaR \cite{guedon2024sugar}, an object surface-aligned \ac{3D-GS} model. For TranSplat, we present two variations: one with RGB image input control (w/ RGB) and one without it (w/o RGB). To assess depth completion performance across the baselines and TranSplat, we used mean average error (MAE) and root mean squared error (RMSE) to compare the absolute depth measurements between the ground truths and the rendered views.



\subsection{Evaluation on Synthetic Datasets}

\subsubsection{Evaluation on Synthetic TRansPose}

% As shown in \tabref{tab:syn-transpose}, TranSplat achieves the best depth completion performance, outperforming all baseline models in terms of both MAE and RMSE across all sequences. Unlike other models that rely directly on raw images of transparent objects as inputs, TranSplat leverages surface embeddings as an alternative representation, leading to a significant improvement in depth completion. Additionally, TranSplat does not require extensive volume density tuning or separate residual background images—both impractical for robotics applications. \ac{3D-GS} methods often yield near-zero opacity values due to the non-Lambertian nature of transparent objects. Specifically, in synthetic TRansPose dataset sequence 3, the darkness of the dataset causes significant computational overhead when refining SuGaR by binding Gaussians to the coarse mesh, leading to failure. In contrast, TranSplat's surface embeddings serve as surrogate features that accurately estimate opacity values on transparent surfaces.


As shown in \tabref{tab:syn-transpose}, TranSplat achieves the best depth completion performance, outperforming all baseline models in terms of both MAE and RMSE across all sequences. Unlike other models that rely directly on raw images of transparent objects as inputs, TranSplat leverages surface embeddings as an alternative representation, leading to a significant improvement in depth completion. Additionally, TranSplat does not require extensive volume density tuning or separate residual background images—both impractical for robotics applications. \ac{3D-GS} methods often yield near-zero opacity values due to the non-Lambertian nature of transparent objects. Specifically, in synthetic TRansPose dataset sequence 3, the overall darkness of the dataset causes the opacities of Gaussians for transparent objects to converge to zero, resulting in the failure of SuGaR during the pruning step as no Gaussians remain after pruning for further optimization. In contrast, TranSplat's surface embeddings serve as surrogate features that accurately estimate opacity values on transparent surfaces.




The qualitative results, presented in \figref{fig:syn}, further support these findings. Most baseline methods struggle to capture the depth along the edges of transparent objects. Even methods like Dex-NeRF, which do capture some edge details, display incomplete depth with holes around transparent surfaces. This is due to conventional \ac{NeRF}-based methods neglecting the opacity values for transparent surfaces. In contrast, by incorporating the unique properties of transparent objects and supplementing 3D gaussian optimization with surface embeddings, TranSplat achieves complete and dense reconstructions around transparent object surfaces.



\subsubsection{Evaluation on Unseen Synthetic}

We evaluated TranSplat's robustness to unseen objects using the Synthetic ClearPose dataset, as shown in Table \ref{tab:real-clearpose}. Consistent with previous findings, TranSplat achieves the best depth completion performance across all test sequences on average. While it slightly underperforms compared to Residual-NeRF on sequence 5 and Dex-NeRF on sequence 4, the performance gaps are minimal. We attribute the lower performance on sequence 4 to the fact that the objects in this sequence differ significantly from the CAD models used to train the latent diffusion model in the TransPose dataset. Despite this, TranSplat demonstrates the highest robustness to unseen objects, proving its effectiveness across different categories rather than being object-specific. Qualitatively, as shown in \figref{fig:syn}, other methods fail to accurately render the depth images of transparent objects, whereas TranSplat consistently succeeds.


\subsection{Evaluation on Real-world Dataset}

%TranSplat also exhibits outperformance in depth completion when evaluated on the real Transpose dataset. However, unlike the results in the synthetic dataset evaluation, the best performance for different sequences alternate between Transplat models that use RGB images as conditioning. In the synthetic dataset evaluation, we find that complementing RGB images as an input control to the latent diffusion model always yield better results than without it. As RGB images give attentive context guidance for generation of the surface embedding, the formulated novel view depth attends well to the transparent objects, as observed in \figref{fig:real}  However, in the real dataset scenario, we find that complementing RGB input does not always yield better performance, especially when the input RGB conditional image is affected by conditions such as bad lighting, blur, or severe occlusions. 

As shown in \tabref{tab:real-transpose}, TranSplat outperforms baseline models in depth completion when evaluated on the real-world TRansPose dataset. However, unlike the consistent results seen in the synthetic dataset evaluation, the best performance in the real-world dataset alternates between TranSplat models with and without RGB images as conditioning input. In the synthetic dataset, incorporating RGB images as input control to the latent diffusion model consistently leads to better results. This is because RGB images provide valuable context and guidance, allowing the model to better attend to transparent objects during depth completion, as illustrated in \figref{fig:real}. In contrast, in the real-world scenario, using RGB input does not always improve performance. The RGB images in real-world settings are often affected by factors like poor lighting, image sensor noise, and severe occlusion between objects, which can negatively impact the model's effectiveness when using RGB conditioning. Additionally, it is important to note that we were unable to evaluate TranSplat against Residual NeRF for the real-world dataset because Residual NeRF requires background images, which were not provided in the TransPose dataset.


\input{tab/Ablation.tex}

\subsection{Computational Efficiency Analysis}

A practical approach to reducing computation time in robotics applications is to decrease the number of images used for rendering. This is particularly relevant when the sensor's sampling rate does not support high frame rates, resulting in sparse image outputs. To evaluate this, we analyzed the accuracy-efficiency trade-off of TranSplat by varying the number of images used. As shown in Table \ref{ablation}, reducing the number of images leads to a slight decrease in performance. However, despite this minor degradation, TranSplat still outperforms other baseline models that use more images. Although TranSplat's use of diffusion models can result in slower inference times with a full sequence of images, reducing the number of rendered images can significantly enhance computational efficiency with only a minor drop in accuracy. This also simplifies the system overhead by allowing for a lower sampling rate of visual sensors while maintaining competitive performance.




\subsection{Extension to Transparent Object Grasping}

To explore the feasibility of extending TranSplat for robot manipulation through grasping, we tested its performance using a commercial robot arm. We used the Franka Emika Panda to capture a series of RGB images of unseen transparent objects, as shown in the \figref{fig:grasp_result}. These images were then used to generate input point clouds by combining RGB data with the corresponding depth rendered by TranSplat. To determine the grasping points, we employed the pretrained GraspNet model \cite{fang2020graspnet}, which generates grasp points from input point clouds. The point clouds were created using RGB images and the depth outputs generated by TranSplat. As shown in \figref{fig:grasp_result}, GraspNet successfully identifies valid grasping points using the depth rendered from TranSplat. This demonstrates that the depth produced by TranSplat provides accurate depth information that can be effectively used for robotic manipulation of transparent objects. We encourage readers to refer to the supplementary materials for more details.













