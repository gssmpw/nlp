\section{Methods}
As shown in \figref{fig:method-overview}, TranSplat operates in two stages. In the first stage, a latent diffusion model is used to extract surface embeddings from each transparent object in the RGB image, providing a consistent representation of the object across different viewpoints. In the second stage, these surface embeddings, combined with the RGB image, are utilized to render depth and reconstruct 3D scenes through \ac{3D-GS}. %More details are denoted in the following subsections. 

\subsection{Diffusion-based Surface Embedding Extraction}

To enhance depth completion for transparent objects, TranSplat generates surface embeddings using a latent diffusion model. Inspired by SurfEmb \cite{haugaard2022surfemb}, which effectively captures surface characteristics of various objects, we hypothesize that surface embeddings can provide improved depth completion and a viewpoint-agnostic representation for transparent objects.


To train TranSplat, four data components are required: input RGB image, corresponding mask for transparent object, text condition, and ground truth surface embedding. We trained the model in SurfEmb \cite{haugaard2022surfemb} to generate surface embedding ground truth. However, SurfEmb relies on object-specific networks trained using 3D CAD models, limiting its scalability to real-world scenarios with unknown objects. In our work, we adopt a more generalizable approach where we leverage a category-level training approach, enabling the network to generate similar features for objects within the same category rather than assigning an object specific CAD model. This allows the model to generalize to a wider range of unseen objects, making it more practical for real-world applications. The modified SurfEmb network is used to generate ground truths for training.

\begin{figure}[h]
  \centering
  % \hfill
  \begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/unknwon_syn.pdf}
    \caption{Synthetic unseen object}
    \label{fig:unknown_syn}
  \end{subfigure}
  \begin{subfigure}{0.49\columnwidth}
    \centering
\includegraphics[width=\textwidth]{figure/unknown_real.pdf}
    \caption{Real world unseen object}
    \label{fig:unknown_real}
  \end{subfigure}
  \caption{Surface embeddings visualization for unseen transparent objects.}
  \label{fig:surfemb-example}
  \vspace{-6mm}
\end{figure}

To extract surface embeddings from RGB images with a latent diffusion model, we concatenate latents generated from the image mask and the mask-multiplied RGB image in the forward process. Text conditioning, consisting of categorical descriptions of objects, is also applied (See \figref{fig:method-overview} green box). Using ControlNet \cite{zhang2023controlnet} architecture, we employ the cropped RGB image as input control. This control helps guide surface embedding generation for specific objects, particularly in scenes with multiple clustered objects. Examples of the generated surface embeddings are shown in \figref{fig:surfemb-example}  % maybe have a figure to illustrate the four data components. 



\subsection{Gaussian Splatting for Transparent Objects}

\subsubsection{Color and Depth Rendering for 3D Gaussian Splatting}

To achieve faster rendering speeds than existing \ac{NeRF} models, we use \ac{3D-GS} for depth completion of transparent objects. \ac{3D-GS} represents 3D scenes as a collection of Gaussian distributions, with each Gaussian kernel parameterized by its position, color, size, orientation, and visibility. This approach enables smooth and realistic scene rendering. The color and depth of the rendered scenes are computed using these Gaussian attributes, as shown in \eqref{color_original} and \eqref{depth_original}.
\vspace{-2mm}

\small
\begin{equation}
C = \sum_{j \in N} c_{j} \cdot \alpha_{j} \cdot T_j  \text{, where} \ T_{j} = \prod^{j-1}_{k=1} (1-\alpha_{k})
\label{color_original}
\vspace{-2mm}
\end{equation}

\begin{equation}
    D = \frac{\sum_{j \in N} d_{j} \cdot \alpha_{j} \cdot T_j}{\sum_{j \in N} \alpha_{j} \cdot T_j}
\label{depth_original}
\end{equation}


\normalsize
where $c, d, \alpha, T$ each represents kernel color, kernel depth, opacity, and the accumulated transmittance for the $j_{th}$ observed Gaussian kernel \cite{rendering_eq, yang2024deformable, matsuki2024gaussian}.

\subsubsection{Joint Gaussian Optimization for Transparent Objects}

Applying \ac{3D-GS} to non-Lambertian surfaces, such as transparent objects, often results in low opacity values and reduced $\alpha$ coefficients. Consequently, the Gaussian kernels on transparent surfaces are obstructed during the splatting process, leading to incomplete depth reconstruction. This issue is further exacerbated by varying backgrounds and viewpoints, reducing depth accuracy.

To address this, TranSplat modifies the \ac{3D-GS} rendering process by incorporating surface embedding coefficients. Unlike prior methods that rely solely on rasterizing RGB images, TranSplat rasterizes reconstructed images using the \ac{SH} coefficients for both RGB and surface embeddings. This dual rasterization allows for independent rendering of both RGB images and surface embeddings. The modified rendering equation is demonstrated in  \eqref{color} and \eqref{surf}.

% In TranSplat, we make modifications to the rendering process in \ac{3D-GS} by injecting surface embedding coefficients. Unlike previous methods that solely rely on rasterizing from RGB images, 
% TranSplat learns the SH coefficients corresponding to RGB, as shown in Equation \eqref{color}, and additionally learns the SH coefficients for SurfEmb, as shown in Equation \eqref{surf}. This allows for independent rendering of both the RGB image and the SurfEmb image.

%TranSplat은 RGB에 해당하는 SH coeffiecent를 학습하는 것\eqref{color} 에 추가로 Surface embedding을 학습하는 SH coeffiencent를 학습하여 \eqref{surf}  independent 하게 rgb이미지와 surface embedding image를 rendering한다.



% TranSplat uses both RGB SH coefficients (SH to color) \eqref{color} and surface embedding SH coefficients \eqref{surf} (SH to surface feature) to compute rasterize corresponding images independently

\begin{equation}
C_{RGB} = \sum_{j \in N} c_{RGB, j} \cdot \alpha_{j} \cdot T_j
\label{color}
\vspace{-2mm}
\end{equation}

\begin{equation}
C_{Surf} = \sum_{j \in N} c_{Surf, j} \cdot \alpha_{j} \cdot T_j
\label{surf}
\end{equation}

%이를 학습하기 위한 loss를 reformulate하였음. 
Moreover, we also reformulate the gaussian optimize loss function to consider images formulated by both RGB and surface embeddings, as shown in \eqref{modified_loss}.
\begin{figure*}[]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/result_syn.pdf}
    \caption{Depth completion results of TRansPose (Top) and ClearPose (Bottom) synthetic sequences.}
    \label{fig:syn}
    \vspace{-2mm}
\end{figure*}

\input{tab/synthetic_transpose}

\input{tab/synthetic_clearpose.tex} 

\small
\begin{equation}
    L = \frac{1}{2}L_{RGB} + \frac{1}{2}L_{Surf}
\label{modified_loss}
\vspace{-4mm}
\end{equation}

\begin{equation}
    L_{RGB} = (1-\lambda)|\hat{I}_{RGB}-I_{RGB}| + \lambda\text{D-SSIM}({\hat{I}_{RGB}, I_{RGB}})
\label{RGB_loss}
\vspace{-4mm}
\end{equation}


\begin{equation}
    L_{Surf} = (1-\lambda)|\hat{I}_{Surf}-I_{Surf}| + \lambda \text{D-SSIM}({\hat{I}_{Surf}, I_{Surf}})
\label{Surf_loss}
% \vspace{-2mm}
\end{equation}

\normalsize
%recon이라기 보단 gaussian optimize loss
%I_{RGB}는 C_{RGB}로부터 만들어진 image, I_{Surf}는 C_{Surf}로 만들어진 이미지, 람다는 0.2

where $I_{RGB}$ is the RGB image, $I_{Surf}$ is the surface embeddings image, and $\lambda = 0.2$. This combined loss function optimizes both RGB content and surface features, providing additional supervision to the surfaces of transparent objects. During backward gradient propagation, the Gaussian kernels' mean, covariance, and $\alpha$ values are shared between the RGB and surface embedding images (See \figref{fig:method-overview} blue box). This joint optimization ensures consistent updates of the \ac{SH} coefficients for both representations, allowing the surface embeddings to prevent opacity values, $\alpha$, from collapsing to zero on transparent object surfaces.
%, a limitation in the \ac{3D-GS}. As a result, this approach also improves depth estimation. However, using only the surface embeddings can lead to suboptimal performance in certain views where the diffusion-based generation does not work well.

% where $L_{RGB}$ and $L_{Surf}$ represent the $L1$ combined with a D-SSIM image reconstruction loss for RGB image rendering and surface embedding image rendering, respectively. 


% This combined loss function optimizes both RGB content and surface features, providing additional supervision to the surfaces of transparent objects. During backward gradient propagation, the average gradients from both RGB image and surface embedding are propagated to each Gaussian kernels, recalibrates the opacity values based on the color and surface representation of the transparent object. Such approach not only ensures consistency in the final rendering but also mitigates incomplete depth problems previously observed when using original \ac{3D-GS}.  
% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=1\linewidth]{figure/unknown_object.pdf}
%     \caption{Figure}
%     \label{fig:unkown}
%     \vspace{-5mm}
% \end{figure}


