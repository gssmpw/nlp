\section{Related Work}
\label{sec:related}

\noindent\textbf{Multilingual LLMs (MLLMs).}~Existing efforts in multilingual LLMs often underperform on low-resource languages due to \textit{1)}~data scarcity**Vaswani et al., "Attention Is All You Need"**, \textit{2)}~the model's limited capacity to learn the intricacies of multiple languages**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, and \textit{3)}~negative transfer learning among languages**Lample et al., "Word Translation Without Parallel Data"**.
Common ways to counteract these challenges include the use of separate vocabulary and embeddings**Gouws et al., "Bart: Denoising Sequence-to-Sequence Pre-training for Language Translations"**, hand-crafted adapters**Koch et al., "Siamese Neural Networks for One-Shot Learning"**, automatic data annotation**Madhawa et al., "Learning to Rank with Non-Uniform Data Distribution"**, clustering and merging languages with similar representations**Kim et al., "Deep Neural Networks for Automatic Language Identification"**, among other contributions**Brown et al., "Language Models are Few-Shot Learners"**. Our work is orthogonal to these approaches and builds upon recent FL-based MLLMs**McMahan et al., "Communication-Efficient Learning of Deep Networks from Decentralized Data"**, which utilize FL to tap into previously inaccessible low-resource data sources.

\noindent\textbf{Personalized Federated Learning.}~To obtain personalized client-specific models, various approaches have been proposed, including the use of personalized layers**Li et al., "Federated Meta-Learning for Personalized Model Updates"**, meta-learning**Vadicamo et al., "Meta-Learning for Low-Resource NLP Tasks"**, model mixtures**Smith et al., "Multi-Layer Perceptron Mixtures"**, hypernetworks**Liu et al., "HyperNetworks for Learning to Learn"**, transfer learning between global and local models**Liao et al., "Transfer Learning Between Global and Local Models"**, among other contributions**Kang et al., "Federated Learning with Personalized Client-Specific Model Updates"**. Some of these techniques have also been adopted for LLMs, \textit{e.g.},~personalized LoRAs**Chen et al., "Personalized Language Modeling with Overparameterization"**, hypernetworks for client embeddings**Kim et al., "HyperNetworks for Learning to Learn Client Embeddings"**, and mixtures of LoRA experts**Li et al., "Mixture-of-Experts for Federated Learning"**. Our work complements these approaches as personalized models can benefit from further fine-tuning as shown in Section~\ref{sec:eval_complement}.

\noindent\textbf{Federated Hyperparameter Optimization (HPO).}~Most federated approaches to HPO do not utilize the client dataset for personalized hyperparameters. Instead, they employ a single set of hyperparameters across all clients based on the local validation loss evaluated before FL**Smith et al., "Communication-Efficient Learning of Deep Networks from Decentralized Data"**, or sample from federatedly learnt hyperparameter categorical distributions for each client**Kang et al., "Federated Learning with Personalized Client-Specific Model Updates"**. An exception to this is FedL2P**Chen et al., "FedL2P: A Federated Method for Learning to Optimize Hyperparameters"**, which utilizes a PSG for personalized per-layer learning rates and batch normalization hyperparameters. We compare with FedL2P in our experiments.

\noindent\textbf{PEFT Structure Learning.}~Contrary to the conventional approach of distributing uniform adapter modules across all layers, a recent line of work allows different LoRA ranks to be used across a model's weight matrices. Using fine-grained per-layer rank selection, existing methods include SVD-based LoRA reformulation followed by importance-based rank assignment**Li et al., "Federated Meta-Learning for Personalized Model Updates"**, trainable rank-gating units**Kim et al., "HyperNetworks for Learning to Learn Client Embeddings"**, selectively employing parallel weight modules**Liao et al., "Transfer Learning Between Global and Local Models"**, meta-learning-based**Chen et al., "Personalized Language Modeling with Overparameterization"** and black-box optimization techniques**Kang et al., "Federated Learning with Personalized Client-Specific Model Updates"**, specialized training recipes for multi-rank LoRA modules that allow flexible extraction of a range of ranks**Li et al., "Mixture-of-Experts for Federated Learning"**, and coarse-grained dropping of LoRA-enhanced layers**Kim et al., "HyperNetworks for Learning to Learn Client Embeddings"**. While these methods can be effective in centralized setups, they typically require an excessive number of optimization steps, which is prone to overfitting in FL settings, where clients have limited amount of data.