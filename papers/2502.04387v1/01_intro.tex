\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
    \includegraphics[width=1.05\columnwidth,trim={5cm 6cm 2cm 1cm},clip]{figures/fedppeft_intro_diagram.pdf} 
    \put (-250,6) {\footnotesize (a)}
    \put (-100,6) {\footnotesize (b)}
    % \vspace{-0.1cm}
    % \captionsetup{font=footnotesize	,labelfont=bf}
    \caption{(a) \method{}'s inference stage on a single client. \circled{1}~Given the \basemodel{} and the client's train dataset, features are extracted and passed into our PS generator (PSG) to generate a PS, $\bm{\lambda}$, suited to the client's resource budget. \circled{2}~$\bm{\lambda}$ is then used to initialize all LoRA modules before \circled{3}~the \basemodel{} is personalized. The resulting personalized model is then used to evaluate on the client's test samples. (b) We train PSG using standard FL approaches.}
    \vspace{-0.2cm}
    \label{fig:intro}
\end{figure}

Federated learning (FL) makes it possible to train multilingual large language models (LLMs) across different geographical regions, protecting linguistic diversity for low-resource languages~\cite{zhao2023breaking} while being compliant with privacy regulations~\cite{lim2020federated}, \textit{e.g.},~General Data Protection Regulation (GDPR). Despite the impressive capabilities demonstrated by these models across various languages, their performance can vary significantly depending on the specific language~\cite{rust2021good} and the data quantity~\cite{adelani2021masakhaner} on each client. 

Moreover, the majority of existing FL-based multilingual LLM approaches have thus far focused on training a single global model~\cite{DEPT, weller2022pretrained, fedllm-bench}, limiting their performance on specific languages. Concretely, scaling a single model to different languages is challenged by issues such as the \textit{Curse of Multilinguality}~\cite{conneau-etal-2020-unsupervised}, where adding more languages often leads to diminishing returns, and \textit{Negative Interference}~\cite{wang2020negative}, where diverse languages compete for limited model capacity. From a personalized FL perspective, learning a global model often increases the initial performance at the expense of personalized performance, \textit{e.g.},~fine-tuning from the global model~\cite{jiang2019improving}.

Naturally, personalized FL approaches can help bridge the gap and improve language personalization. However, existing techniques are either too costly to be applied to LLMs, \textit{e.g.},~the use of meta-learning~\cite{fedmeta} and  hypernetworks~\cite{pfedhn}, or rely on suboptimal hand-crafted personalization strategies, \textit{e.g.},~manual choice of personalized and language-specific layers~\cite{DEPT,zhao2023breaking,fedlora,fdlora}, parameter-efficient fine-tuning (PEFT) adapter structures~\cite{SA-FedLora, FedDPA}, or clustering based on class labels and/or language commonality~\cite{hypcluster,cfl,fedllm-bench}.

Intuitively, optimizing personalization in personalized FL often necessitates dataset- and task-specific methods. The optimal level of personalization varies significantly depending on the characteristics of the data and the specific FL scenario~\citep{chen2022pfl,fedllm-bench}. For instance, an English-pretrained LLM may require stronger personalization, \textit{e.g.},~higher learning rates, when fine-tuning on German than on English. The optimal personalization strategy (PS) is thus contingent upon the specific task, the client, and the given \basemodel{}~\cite{royson2023fedl2p}. 

In this paper, we address the issues above by learning to personalize PEFT for each client. To this end, we propose \method{}, a method that enables clients to collaboratively learn language personalization strategies using FL. Specifically, we federatedly train a PS generator (PSG), as depicted in Fig.~\ref{fig:intro}(b), which allows all participating clients to collaboratively learn the optimal mapping between local meta-data to optimal LoRA~\cite{hu2021lora} ranks. Fig.~\ref{fig:intro}(a) illustrates the personalization process of \method{} on a single client during inference, where per-layer LoRA ranks are generated depending on the initial \basemodel{}, the client's dataset, and their resource budget. These personalized LoRA modules are then used to apply PEFT on the \basemodel{} and yield the personalized model. 

As \method{} focuses on improving the PEFT process per-dataset/task/client, it is directly pluggable to any starting \basemodel{}, which may or may not be federatedly trained. This includes off-the-shelf pretrained models, FL approaches that learn a single global model~\cite{fedavg,fedbabu,scaffold,feddyn}, and even personalized FL approaches that deploy personalized layers, \textit{e.g.},~monolingual tokenizer and embeddings~\cite{DEPT}, personalized LoRA adapters~\cite{fdlora, fedlora, FedDPA}, and language embeddings~\cite{FedPerC}. Through our experiments (Section~\ref{sec:expmts}), we show that our method \textit{1)}~largely outperforms both existing non-FL LoRA rank selection and FL-based learning-to-personalize techniques, and \textit{2)}~complements well with a range of existing FL approaches.


%%%%%%%%%%%%%%%%%%%% TEXT GRAVEYARD
% Personalized Federated Learning (pFL) is a distributed machine learning paradigm where clients can collaborate to train personalized models better adapted to their requirements without sharing data. Two of the key statistical challenges of pFL comprise \textit{1)}~data heterogeneity, where different clients may have significantly different data distributions, and \textit{2)}~data scarcity, where clients have a limited number of data samples~\cite{kairouz2021advances,wen2023survey}. To counteract these challenges, numerous approaches have been proposed, including the use of personalized layers~\cite{fedper}, meta-learning~\cite{fedmeta}, clustering~\cite{hypcluster}, and many others. However, the majority of these approaches still rely on hand-crafted personalization strategies, \textit{e.g.},~manual choices of which layers to personalize and/or a fixed fine-tuning learning rate for all clients. \textbf{TODO: Rephrase this paragraph. Add LLM and PEFT as our focus and replace the examples to FL+LLM examples.}

% One approach is to learn a PS, e.g. hyperparameter optimization (HPO), for each client. However, while these methods have been shown to be effective in centralized training settings, they can suffer from overfitting in FL due to the limited data available on each client device~\cite{royson2023fedl2p}. To address this, numerous works propose to collaboratively learn these personalization strategies using FL~\cite{flora,fedex,holly2022evaluation}. However, these works either do not consider the unique characteristics of each client's data distribution or are not suitable for LLMs, \textit{e.g.},~FedL2P~\cite{royson2023fedl2p} learns batch normalization-related hyperparameters which are typically not used in LLMs. 
