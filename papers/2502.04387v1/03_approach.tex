\section{Our Approach}\label{sec:main}


\subsection{Preliminaries \& Motivation}

In personalized FL, the goal is to minimize each client's local objective $\mathbb{E}_{(x,y) \sim P^i}\mathcal{L}^i(\Phi^i;x,y)$ where $P^i$ represents the data distribution of the $i$-th client, $x$ and $y$ are the input data and labels, respectively, and $\mathcal{L}^i(\Phi^i;x,y)$ is the loss function for client $i$ given model parameters $\Phi^i$. This is typically achieved via fine-tuning~\cite{matsuda2022empirical,chen2022pfl} a \basemodel{}, with parameters $\Phi^i_{BM}$ and a set of hyperparameters, \textit{e.g.}~learning rate. Note that $\Phi^i_{BM}$ may differ across clients if it is already personalized, \textit{e.g.}~if $\Phi^i_{BM}$ is obtained using a personalized FL algorithm.

Fine-tuning LLMs, however, is unprecedentedly compute and memory intensive, and prone to overfitting. As such, the majority of existing federated LLM works~\cite{zhao2023breaking,fedpeft} rely on PEFT methods, with LoRA~\cite{hu2021lora} being a prevalent choice due to its efficiency and performance. Specifically, for a frozen weight matrix $W \in \mathbb{R}^{d\times e}$, LoRA introduces low-rank matrices $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times e}$ where $r \ll \min(d, e)$. The adapted weights are then expressed as: $W' = W + \frac{\alpha_{\text{lora}}}{r}BA$ where $\alpha_{\text{lora}}$ is a hyperparameter and only $B$ and $A$ are trained during fine-tuning.
Although effective, these FL works rely on a fixed hand-crafted PS, \textit{e.g.},~a manually defined LoRA rank on hand-picked layers, for all clients, leading to suboptimal personalized models. 

\subsection{Personalized PEFT}\label{sec:personalized_peft}

We, instead, propose using a different PS for each client. Common hyperparameter choices from previous federated HPO approaches (Section~\ref{sec:related}) include learning rates and batch normalization (BN) hyperparameters. While these hyperparameters have been shown to be effective for handling data heterogeneity in popular vision and speech benchmarks~\cite{fedbn,li2016revisiting,fedper}, they are less consequential or not applicable when fine-tuning LLMs. This stems from the fact that LLMs are often fine-tuned using adaptive optimizers, \textit{e.g.}~Adam, which are more robust to the learning rate~\cite{zhao2025deconstructing}, and BN layers are not typically used. 
A more critical hyperparameter choice shown to be effective, especially for cross-lingual transfer learning~\cite{pfeiffer2020mad}, is the PEFT adapter structure; specifically which layers to introduce LoRAs in and what ranks to utilize~\cite{adalora,autolora}. 

\noindent\textbf{Adapting BayesTune for LoRA Rank Selection.}~
Building upon BayesTune~\cite{kim2023bayestune}, a Bayesian sparse model selection approach, we formulate PEFT personalization as a sparse LoRA rank selection problem and propose BayesTune-LoRA. 
Concretely, we introduce rank-wise latent variables $\lambda \in \mathbb{R}^r, \; \lambda_i > 0, \; \forall i=1,2,\cdots,r$ for each LoRA matrix: $B\lambda A$. Let $\bm{\lambda}= \{\lambda_{l,\cdot}\}_{l=1}^L$ be the set of all $\lambda$ where $\lambda_{l,\cdot}$ represents the rank-wise scales for layer $l$ in a model with $L$ LoRA modules (similarly for $\bm{A}$ and $\bm{B}$). Using BayesTune, the 
values for $\theta=(\bm{\lambda}$,$\bm{A}$,$\bm{B})$ are optimized as:
% 
% \vspace{-2em}
\begin{align}
\label{eq:bayestune}
\theta^* =& \argmin_{\theta} \mathcal{L}_{\text{CE}}(\theta;D) + \frac{\alpha_{s}}{N} \mathcal{L}_{s}(\bm{\lambda},\bm{B}) + \frac{\alpha_{p}}{N}\mathcal{L}_{p}(\bm{\lambda}) 
\end{align}
where $D$$=$$\{(x_i, y_i)\}_{i=1}^N$ is the train dataset, $N$ the size of $D$, $\mathcal{L}_{\text{CE}}(\theta;D)$ the cross-entropy loss, $\alpha_{p}$ and $\alpha_{s}$ hyperparameters, $\mathcal{L}_s$ the logarithm of the Laplace distribution (prior imposed on $p(B|\lambda)$\footnote{Unlike BayesTune, where every parameter is associated with its own prior scale, we use an ``independent'' Laplace prior where each $\lambda_{l,i}$ applies to all entries of $B_{l,i}$}), $f(\|B_{l,i}\|_1;\mu,b)= \frac{1}{2b} \exp\left(-\frac{|\|B_{l,i}\|_1 - \mu|}{b}\right)
$ with $\mu=0$ ($B$ is initialized to 0 in LoRA) and $b=\lambda_{l,i}$:
\begin{align}
\mathcal{L}_s(\bm{\lambda}, \bm{B}) = \sum^L_l \sum^r_i \left(\log \lambda_{l,i} + \frac{\|B_{l,i}\|_1}{\lambda_{l,i}} + \log2\right)
\end{align}
and $\mathcal{L}_p$ is the logarithm of the Gamma distribution (hyper-prior imposed on $\lambda$), $\mathcal{G}(\lambda_{l,i};\alpha_g,\beta_g)= \frac{\beta_g^{\alpha_g}}{\Gamma(\alpha_g)} \lambda_{l,i}^{\alpha_g-1} e^{-\beta_g \lambda_{l,i}}$ where $\alpha_g=0.01,\beta_g=100$ following the hyperparameters set by the original authors:
\begin{align}
\mathcal{L}_p(\bm{\lambda}) = \sum^L_l \sum^r_i &(0.99\cdot \log \lambda_{l,i} + 100 \cdot \lambda_{l,i} \nonumber \\
& - 0.01\log(100) + \log\Gamma(0.01)) 
\end{align}
In practice, we can save computations by removing all constants and the duplicate term $\log \lambda$, resulting in the following approximated penalty losses:
\begin{align}
\mathcal{L}_s(\bm{\lambda}, \bm{B}) &= \sum^L_l \sum^r_i \frac{\|B_{l,i}\|_1}{\lambda_{l,i}} \\
\mathcal{L}_p(\bm{\lambda}) &= \sum^L_l \sum^r_i (\log \lambda_{l,i} + 100 \cdot \lambda_{l,i}) 
\end{align}
Roughly speaking, $\mathcal{L}_p$ encourages small $\lambda$ while $\mathcal{L}_s$ encourages larger $\lambda$ for larger LoRA $B$ (per column) updates. Hence, minimizing the losses in Eq.~(\ref{eq:bayestune}) encourages larger $\lambda$ in more significant ranks. 

\noindent\textbf{Personalizing PEFT with BayesTune-LoRA.}~For each client, we attach BayesTune-LoRA modules, $\theta$, to all linear layers of its \basemodel{} with rank $r_{\text{init}} = \alpha_{r\_mul} \cdot r_{\text{max target}}$ where $r_{\text{max target}}$ is the maximum inference resource budget and $r_{\text{init}}$ is the initial rank before pruning. $\theta$ is then optimized using Adam~\cite{Kingma_2014} as per Eq.~(\ref{eq:bayestune}).\footnote{BayesTune proposed using SGLD~\cite{welling2011bayesian}, adding Gaussian noise to the gradient updates and sampling from the posterior distribution. Due to the challenges of estimating the full posterior distribution in FL settings, particularly with limited client data, we opt to find a point estimate.}

After training, we freeze the resulting $\bm{\lambda}$ and use it for personalization. Specifically, given a resource budget (total rank budget) of $r \cdot L$, we prune $\bm{\lambda}$ by taking the top-$(r \cdot L)$ largest ranks, along with the corresponding rows of $\bm{A}$ and columns of $\bm{B}$.\footnote{The LoRA module is discarded for layers where $\|\lambda_l\|_1 = 0$} We then reinitialize the pruned $\bm{A}$ and $\bm{B}$ and perform standard fine-tuning on $\mathcal{L}_{\text{CE}}$ with the frozen pruned $\bm{\lambda}$ to obtain the personalized model. Note that we only have to train $\bm{\lambda}$ once for all ranks $\leq r_{\text{max target}}$.

\subsection{\method{}: FL to Personalize PEFT}\label{sec:main_method}

The limited data available to each client in FL makes it difficult to train an effective PS in isolation, frequently resulting in overfitting. Following FedL2P~\cite{royson2023fedl2p}, we mitigate this by federatedly learning a common PSG that generates client-wise PS. Concretely, we use a small, one hidden layer multilayer perceptron (MLP) with parameters $\phi$ that takes as input the client meta-data and outputs an estimated PS as follows:
% 
% \vspace{-2em}
\begin{align}\label{eq:mlp}
\bm{\hat{\lambda}} = \text{MLP}(\phi; \:\: &E(h_0),SD(h_0),E(h_1),SD(h_1), \nonumber \\
&\cdots,E(h_{L-1}),SD(h_{L-1}))
\end{align}
where $h_{l-1}$ is the input feature to the $l$-th layer in the \basemodel{}, and $E(\cdot)$ and $SD(\cdot)$ are the mean and standard deviation (SD), respectively.

In contrast to FedL2P, which adopts a computationally demanding meta-learning approach to train MLP, we take a two-stage strategy for each client: \textit{1)} first, learn $\bm{\lambda}$, followed by \textit{2)} regression learning of MLP to target the learned $\bm{\lambda}$. 

\noindent\textbf{Federated Training of \method{}.}~Fig.~\ref{fig:approach} shows the entire \method{} algorithm during federated training. For each federated round, each sampled participating client $i$ receives $\phi$ from the server and loads them into its MLP. They then \circled{1} perform a forward pass of the local train dataset on their \basemodel{} and a forward pass of the MLP with the resulting features as per Eq.~(\ref{eq:mlp}). \circled{2} The estimated $\bm{\hat{\lambda}}^i$ is plugged into our proposed BayesTune-LoRA (Section~\ref{sec:personalized_peft}) and \circled{3} fine-tuning is performed as per Eq.~(\ref{eq:bayestune}) for $s$ steps (Stage 1). \circled{4} The resulting $\hat{\bm{\lambda}}^{i,s}$
is used as an approximated ground-truth for regression learning of MLP to target the learned $\hat{\bm{\lambda}}^{i,s}$, where $\mathcal{L}_1$ is the L1 loss (Stage 2). Finally, \circled{5} $\phi$ is sent back to the server for aggregation. As there is no single aggregation method that outperforms all others in every situation~\cite{matsuda2022empirical,chen2022pfl,fedllm-bench}, we utilize FedAvg~\cite{fedavg}. The aggregated $\phi$ is then sent to clients for the next round.

At the end of federated training, the learned $\phi$ can be deployed to any client, \seen{} or \unseen{} during federated training. Note that unlike FedL2P, which requires federated training for every target rank, \method{} inherits the property of BayesTune-LoRA; federated training is a one-time cost for all ranks $\leq r_{\text{max target}}$.

% \vspace{-0.3cm}
\noindent\textbf{Inference with \method{}.}~Fig.~\ref{fig:intro}(a) shows how \method{} personalizes PEFT for each client upon deployment. Given the learned MLP and the client's \basemodel{}, \circled{1} the client meta-data are retrieved (Eq.~\ref{eq:mlp}) and used to generate the client's PS, $\bm{\lambda}$. \circled{2} Given the client's resource budget of total rank $r \cdot L$, we take the top-$(r \cdot L)$ largest ranks in $\bm{\lambda}$, freeze them, and initialize our proposed BayesTune-LoRA modules for all layers where $\|\lambda_l\|_1$$>$$0$. \circled{3} The personalized LoRA ranks are used for fine-tuning before merging back to the \basemodel{} to obtain the final personalized model.

\begin{figure}[t]
    \includegraphics[width=1.0\columnwidth,trim={4.5cm 2cm 12cm 3.8cm},clip]{figures/fedppeft_approach.pdf} 
    \vspace{-0.9cm}
    % \captionsetup{font=footnotesize	,labelfont=bf}
    \caption{\method{}'s federated training of PSG for each federated round (See text in Section.~\ref{sec:main_method}).}
    \vspace{-0.5cm}
    \label{fig:approach}
    \vspace{-0.2cm}
\end{figure}





