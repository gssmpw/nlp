\begin{table}[t]
\vspace{-0.7em}
\caption{Mean latency and memory costs across 100 runs of the first client in the \seen{} pool using an NVIDIA A100 GPU (80GB). }
\vspace{0.5em}
\label{tab:cost}
\begin{scriptsize}\resizebox{0.48\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{clcccc}
\cline{3-6}
\multicolumn{1}{l}{}                                       &                                      & \multicolumn{2}{c}{\textbf{Federated Training}}                           & \multicolumn{2}{c}{\textbf{Inference}}                                    \\ \toprule
\multicolumn{1}{c}{\textbf{\thead{Dataset \\ (Model)}}}               & \multicolumn{1}{c}{\textbf{Approach}} & \multicolumn{1}{c}{\textbf{\thead{Mean \\ Latency (s)}}} & \textbf{\thead{Peak \\ Memory (GB)}} & \multicolumn{1}{c}{\textbf{\thead{Mean \\ Latency (s)}}} & \textbf{\thead{Peak \\ Memory (GB)}} \\ \midrule
\multicolumn{1}{c}{\multirow{5}{*}{\thead{XNLI \\ (mBERT)}}}          & LoRA                                 & \multicolumn{1}{c|}{-}                         & -                         & \multicolumn{1}{c|}{3.16}                      & 2.69                      \\ %\cline{2-6} 
\multicolumn{1}{c}{}                                     & AdaLoRA                              & \multicolumn{1}{c|}{-}                         & -                         & \multicolumn{1}{c|}{3.41}                      & 2.77                      \\ %\cline{2-6} 
\multicolumn{1}{c}{}                                     & BayesTune-LoRA                            & \multicolumn{1}{c|}{-}                         & -                         & \multicolumn{1}{c|}{2.48\tnote{1} + 3.13}              & 3.09\tnote{1} + 2.69              \\ %\cline{2-6} 
\multicolumn{1}{c}{}                                     & FedL2P                               & \multicolumn{1}{c|}{28.16}                     & 3.53                      & \multicolumn{1}{c|}{4.48}                      & 2.99                      \\ %\cline{2-6} 
\multicolumn{1}{c}{}                                     & \method{}                                 & \multicolumn{1}{c|}{3.36}                      & 3.09                      & \multicolumn{1}{c|}{4.41}                      & 2.69                      \\ \midrule
\multicolumn{1}{c}{\multirow{5}{*}{\thead{Fed-Aya \\ (Llama-3.2-3B)}}} & FT                                   & \multicolumn{1}{c|}{-}                         & -                         & \multicolumn{1}{c|}{347.33}                    & 17.92                     \\ %\cline{2-6} 
\multicolumn{1}{c}{}                                     & AdaLoRA                              & \multicolumn{1}{c|}{-}                         & -                         & \multicolumn{1}{c|}{423.88}                    & 18.77                     \\ %\cline{2-6} 
\multicolumn{1}{c}{}                                     & BayesTune-LoRA                            & \multicolumn{1}{c|}{-}                         & -                         & \multicolumn{1}{c|}{72.29\tnote{1} + 357.56}           & 25.18\tnote{1} + 18.33            \\ %\cline{2-6} 
\multicolumn{1}{c}{}                                     & FedL2P                               & \multicolumn{1}{c|}{226.49}                    & 32.19                     & \multicolumn{1}{c|}{379.98}                    & 19.78                     \\ %\cline{2-6} 
\multicolumn{1}{c}{}                                     & \method{}                                 & \multicolumn{1}{c|}{80.28}                     & 25.19                     & \multicolumn{1}{c|}{400.64}                    & 15.67                     \\ \bottomrule
\end{tabular}
  \begin{tablenotes}
    \item[1] One time cost per client for all targeted ranks
  \end{tablenotes}
\end{threeparttable}
}
\end{scriptsize}
\vspace{-1.7em}
\end{table}