\begin{table*}[t]
\npdecimalsign{.}
\nprounddigits{2}
\caption{Average METEOR/ROUGE-1/ROUGE-L of each language for \seen{} clients of our Fed-Aya setup. The pretrained MobileLLaMA-1.4B model is trained using Standard FL with LoRA following FedLLM-Bench~\cite{fedllm-bench} and the resulting \basemodel{} is personalized to each client given a baseline approach.}
\vspace{0.5em}
\label{tab:mobilellama_fedaya_seen}
\begin{scriptsize}\resizebox{0.98\textwidth}{!}{
\begin{tabular}{c|l|l|l|l|l|l|l|l|c}
\toprule
% \textbf{Lora}\\\textbf{Rank}  

\textbf{$\mathbf{r}$}& \multicolumn{1}{c|}{\textbf{Approach}} & \multicolumn{1}{c|}{\textbf{te}} & \multicolumn{1}{c|}{\textbf{ar}} & \multicolumn{1}{c|}{\textbf{es}} & \multicolumn{1}{c|}{\textbf{en}} & \multicolumn{1}{c|}{\textbf{fr}} & \multicolumn{1}{c|}{\textbf{zh}} & \multicolumn{1}{c|}{\textbf{pt}} 
& \textbf{Wins} \\ \midrule
% \multirow{5}{*}{1}  & LoRA                                   & 0.125/0.0637/0.0613              & 0.1723/0.0208/0.0203             & 0.3159/0.3469/0.3203             & 0.2458/0.3066/0.2478             & 0.1878/0.2455/0.1952             & \textbf{0.0649/0.084/0.084}      & 0.2527/0.3116/0.2792                                         & 0             \\ % \cline{2-11} 
%                     & AdaLoRA                              & 0.1218/0.0557/0.0537             & 0.1899/0.0173/0.0166             & 0.3241/0.3478/0.3218             & 0.2434/0.3058/0.2452             & 0.2048/0.2445/0.1988             & 0.0633/0.0841/0.0841             & 0.2692/0.329/0.2959                                          & 0             \\ % \cline{2-11} 
%                     & BayesTune-LoRA                            & 0.12/0.0557/0.0541               & 0.1582/0.0226/0.0223             & 0.2863/0.3269/0.2991             & 0.2326/0.2848/0.2319             & 0.1742/0.2298/0.184              & 0.06/0.0723/0.0723               & 0.2289/0.2837/0.2526                                         & 1             \\ % \cline{2-11} 
%                     & FedL2P                               & \textbf{0.1396/0.0769/0.0746}    & \textbf{0.2109/0.0207/0.0197}    & 0.3218/0.3527/0.3252             & \textbf{0.2692/0.3293/0.2697}    & \textbf{0.2365/0.2652/0.2105}    & 0.0527/0.0866/0.0865             & \textbf{0.2797/0.3228/0.2916}                                & 2             \\ % \cline{2-11} 
%                     & \method{}                                 & 0.126/0.0627/0.0603              & 0.2039/0.0221/0.0216             & \textbf{0.3335/0.3624/0.3346}    & 0.2645/0.3369/0.2748             & 0.2181/0.2667/0.2145             & 0.0616/0.0842/0.0842             & 0.2792/0.3373/0.3035                                         & \textbf{4}    \\ \hline
\multirow{5}{*}{2}  & LoRA                                   & 0.1207/0.0545/0.0524             & 0.1835/0.0210/0.0205              & 0.3228/0.3519/0.3251             & 0.2457/0.3121/0.2525             & 0.2049/0.2484/0.2002             & 0.0616/\textbf{0.0874/0.0873}             & 0.2631/0.3266/0.2922                                         & 1             \\ % \cline{2-11} 
                    & AdaLoRA                              & 0.1238/0.0607/0.0586             & 0.1819/0.0223/0.0218             & 0.3288/0.3573/0.3315             & 0.2459/0.3172/0.2524             & 0.1963/0.2422/0.1915             & \textbf{0.0689}/0.0832/0.0832    & 0.2745/0.3327/0.2986                                         & 0             \\ % \cline{2-11} 
                    & BayesTune-LoRA                            & 0.1245/0.0605/0.0580              & 0.1813/0.0181/0.0178             & 0.2941/0.3317/0.3063             & 0.2345/0.2892/0.2367             & 0.1885/0.2430/0.1970               & 0.0643/0.0805/0.0805             & 0.2408/0.2971/0.2645                                         & 0             \\ % \cline{2-11} 
                    & FedL2P                               & \textbf{0.1451/0.0747/0.0725}    & 0.2017/0.0219/0.0213             & 0.3321/0.3523/0.3245             & 0.2635/0.3307/0.2692             & 0.2298/0.2467/0.2034             & 0.0544/0.0803/0.0803             & 0.2780/0.3133/0.2832                                          & 1             \\ % \cline{2-11} 
                    & \method{}                                 & 0.1266/0.0629/0.0606             & \textbf{0.2081/0.0254/0.0248}    & \textbf{0.3425/0.3663/0.3376}    & \textbf{0.2745/0.3469/0.2831}    & \textbf{0.2342/0.2766/0.2246}    & 0.0524/0.0777/0.0777             & \textbf{0.2846/0.3403/0.3066}                                & \textbf{5}    \\ \hline
\multirow{5}{*}{4}  & LoRA                                   & 0.1232/0.0570/0.0537              & 0.1861/0.0202/0.0197             & 0.3284/0.3555/0.3291             & 0.2541/0.3202/0.2585             & 0.2037/0.2475/0.1990              & 0.0559/\textbf{0.0886/0.0886}             & 0.2734/0.3314/0.2975                                         & 1             \\ % \cline{2-11} 
                    & AdaLoRA                              & 0.1240/0.0596/0.0574              & 0.1858/0.0180/0.0177              & 0.3310/0.3548/0.3287              & 0.2448/0.3111/0.2500               & 0.1892/0.2331/0.1859             & 0.0617/0.0852/0.0851             & 0.2640/0.3250/0.2934                                           & 0             \\ % \cline{2-11} 
                    & BayesTune-LoRA                            & 0.1214/0.0548/0.0532             & 0.1912/0.0201/0.0195             & 0.3042/0.3405/0.3150              & 0.2405/0.3022/0.2440              & 0.1973/0.2393/0.1925             & \textbf{0.0670}/0.0806/0.0806     & 0.2468/0.3057/0.2737                                         & 0             \\ % \cline{2-11} 
                    & FedL2P                               & 0.1258/0.0616/0.0592             & 0.1805/0.0217/0.0208             & 0.3260/0.3568/0.3298              & 0.2519/0.3108/0.2498             & 0.2037/0.2493/0.2027             & 0.0484/0.0798/0.0798             & 0.2626/0.3174/0.2836                                         & 0             \\ % \cline{2-11} 
                    & \method{}                                 & \textbf{0.1350/0.0722/0.0692}     & \textbf{0.2218/0.0275/0.0269}    & \textbf{0.3448/0.3712/0.3419}    & \textbf{0.2796/0.3516/0.2883}    & \textbf{0.2469/0.2753/0.2244}    & 0.0554/0.0843/0.0843             & \textbf{0.2911/0.3397/0.3060}                                 & \textbf{6}    \\ \hline
\multirow{5}{*}{8}  & LoRA                                   & 0.1241/0.0522/0.0493             & 0.2059/0.0189/0.0184             & 0.3435/0.3688/0.3427             & 0.2686/0.3400/0.2771               & 0.2197/0.2556/0.2037             & 0.0583/\textbf{0.0886}/0.0884             & 0.2822/0.3418/0.3076                                         & 0             \\ % \cline{2-11} 
                    & AdaLoRA                              & 0.1245/0.0623/0.0599             & 0.1771/0.0189/0.0184             & 0.3215/0.3485/0.3225             & 0.2461/0.3101/0.2512             & 0.1799/0.2287/0.1839             & \textbf{0.0613}/0.0806/0.0806    & 0.2607/0.3194/0.2879                                         & 0             \\ % \cline{2-11} 
                    & BayesTune-LoRA                            & 0.1246/0.0591/0.0572             & 0.2046/0.0192/0.0189             & 0.3247/0.3520/0.3284              & 0.2430/0.3095/0.2496              & 0.2132/0.2542/0.2059             & 0.0601/0.0818/\textbf{0.0818}             & 0.2588/0.3171/0.2871                                         & 0             \\ % \cline{2-11} 
                    & FedL2P                               & 0.1316/\textbf{0.0687/0.0661}             & 0.1855/0.0218/0.0215             & 0.3272/0.3535/0.3280              & 0.2696/0.3304/0.2711             & 0.2109/0.2558/0.2059             & 0.0510/0.0816/0.0816              & 0.2750/0.3252/0.2897                                          & 1             \\ % \cline{2-11} 
                    & \method{}                                 & \textbf{0.1327}/0.0662/0.0644    & \textbf{0.2304/0.0253/0.0246}    & \textbf{0.3474/0.3847/0.3531}    & \textbf{0.2941/0.3656/0.2996}    & \textbf{0.2553/0.2829/0.2268}    & 0.0538/0.0814/0.0814             & \textbf{0.2945/0.3452/0.3108}                                & \textbf{5}    \\ \hline
\multirow{5}{*}{16} & LoRA                                   & 0.1217/0.0562/0.0536             & 0.2080/0.0221/0.0218              & 0.3387/0.3616/0.3352             & 0.2757/0.3431/0.2807             & \textbf{0.2497/0.2880/0.2337}     & 0.0553/\textbf{0.0844/0.0844}             & 0.2902/0.3449/0.3106                                         & 2             \\ % \cline{2-11} 
                    & AdaLoRA                              & 0.1251/0.0624/0.0602             & 0.1676/0.0198/0.0192             & 0.3048/0.3329/0.3037             & 0.2391/0.2985/0.2416             & 0.1821/0.2309/0.1866             & \textbf{0.0575}/0.0815/0.0815    & 0.2530/0.3099/0.2784                                          & 0             \\ % \cline{2-11} 
                    & BayesTune-LoRA                            & 0.1374/0.0745/0.0720              & 0.2119/0.0175/0.0172             & 0.3358/0.3649/0.3397             & 0.2587/0.3189/0.2577             & 0.2222/0.2603/0.2113             & 0.0520/0.0824/0.0824              & 0.2862/0.3450/0.3109                                          & 0             \\ % \cline{2-11} 
                    & FedL2P                               & \textbf{0.1559/0.0827/0.0799}    & 0.2013/0.0228/0.0226             & 0.3278/0.3541/0.3268             & 0.2772/0.3278/0.2693             & 0.2306/0.2346/0.1925             & 0.0506/0.0838/0.0838             & 0.2817/0.3179/0.2851                                         & 1             \\ % \cline{2-11} 
                    & \method{}                                 & 0.1309/0.0663/0.0638             & \textbf{0.2359/0.0258/0.0252}    & \textbf{0.3447/0.3778/0.3463}    & \textbf{0.2802/0.3485/0.2858}    & 0.2473/0.2775/0.2208             & 0.0538/0.0835/0.0835             & \textbf{0.2975/0.3491/0.3157}                                & \textbf{4}    \\ \bottomrule
\end{tabular}

}
\npnoround
\end{scriptsize}
\vspace{-1.5em}
\end{table*}

