% \newpage
\section{Evaluation}\label{sec:expmts}

\subsection{Experimental Setup}

We conduct experiments on multilingual scenarios, where clients with diverse high- and low-resource languages can collaboratively learn how to personalize a given base model to better cater to their language preferences. In all experiments, we divide clients in two pools, \seen{} and \unseen{}, where only the clients in the \seen{} pool actively participate in federated training. We set the maximum number of communication rounds for training the PSG to $150$, randomly sampling $10\%$ of participating clients every round. We use Adam~\cite{Kingma_2014} as the default optimizer for all our experiments. We evaluate on resource budgets $r=2,4,8,16$ where the total rank budget is $r \cdot L$. We summarize the FL scenarios that we consider in our experiments, leaving comprehensive details in Appendix~\ref{appendix:experiments}.

\subsubsection{Tasks, Models, and Datasets}

\noindent\textbf{Text Classification.}~We adopt the pretrained multilingual BERT~\cite{BERT} (mBERT) for all text classification experiments. For datasets, we introduce additional data heterogeneity to the simulated FL setups, XNLI~\cite{XNLI} and MasakhaNEWS~\cite{MasakhaNEWS}, proposed in PE\_FL~\cite{zhao2023breaking}. 

For our XNLI setup, we sample 2k instances for train and $500$ for test in each pool. In contrast to PE\_FL, which had $15$ clients ($1$ language per client), we divide the data equally among $20$ clients for each language. We then adopt the latent Dirichlet allocation (LDA) partition method~\cite{hsu2019measuring, yurochkin2019bayesian}, $y \sim Dir(\alpha)$, to simulate non-IID label shifts among these clients, with $\alpha=0.5$. Hence, there is a total of $600$ clients ($15$ languages $\cdot$ $20$ clients per language $\cdot$ $2$ pools), consisting of both label and feature heterogeneity.

For MasakhaNEWS, we first split the data in each of the $16$ languages by half for each pool. Similar to our XNLI setup, we divide each language's data equally among 10 clients and adopt LDA with $\alpha=0.5$, resulting in $320$ clients in total. Differing from our XNLI setup, each language varies in the amount of samples, adding another layer of data heterogeneity to the setup: quantity skew.

\noindent\textbf{Instruction-Tuning Generation.}~We adopt pretrained MobileLLaMA-1.4B~\cite{mobilellama} and Llama-3.2-3B~\cite{llama3}, which are representative of commonly supported model sizes on recent high-end edge devices~\cite{openelm2024icmlw,2024_mobilequant,edgellm2024tmc}. For each model, we run experiments on the recent Fed-Aya dataset. Fed-Aya is a real-world FL dataset naturally partitioned by annotator ID and each client has data with up to $4$ languages. Out of a total of $38$ clients, we select $8$ clients for our \unseen{} pool. We also split each client's data $80\%/20\%$ for train and test, respectively. Fig.~\ref{fig:fed-aya} shows the distribution of predominant languages, where predominant refers to the client's most commonly used language, in our setup.

\begin{figure}
    \small
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/fedaya_pool.png}
    % \captionsetup{font=small,labelfont=bf}
    \vspace{-1.5em}
    \caption{The number of clients in each predominant language in our Fed-Aya setup.}
    \label{fig:fed-aya}
    \vspace{-2em}
\end{figure}

\subsubsection{Complementary Approaches}\label{sec:complementary}

We show \method{}'s compatibility with both off-the-shelf models and models trained using existing FL methods. Concretely, given a pretrained model, we obtain a \basemodel{} using one of the following approaches: 

\noindent\textbf{Standard FL.} We further train the pretrained model federatedly on the \seen{} pool, either using existing PEFT methods or full fine-tuning~\cite{fedllm-bench, fedpeft},  

\noindent\textbf{Personalized FL.} We adopt two recent personalized FL works: \textit{i)} FedDPA-T~\cite{FedDPA}, which learns per-client personalized LoRA modules in addition to global LoRA modules, and \textit{ii)} DEPT (SPEC)~\cite{DEPT}, which learns per-client personalized token and positional embeddings while keeping the rest of the model shared. The \basemodel{} hence differs for each client.

\noindent\textbf{Off-the-shelf.} We use the pretrained model as the \basemodel{} without additional training.

\input{tables/masakha_seen}
\input{tables/masakha_unseen}

\subsubsection{Baselines}\label{sec:baselines}

Given a \basemodel{}, we compare \method{} with existing fine-tuning and {\em learning to personalize} approaches. 

\noindent\textbf{LoRA PEFT.}~We deploy LoRA~\cite{hu2021lora} on all linear layers of the model with a fixed rank $r$. 

\noindent\textbf{Non-FL Rank Selection.}~We compare with AdaLoRA~\cite{adalora} and our proposed LoRA-variant of BayesTune~\cite{kim2023bayestune}, BayesTune-LoRA (Section~\ref{sec:personalized_peft}), which optimizes $\bm{\lambda}$ separately for each client. 

\noindent\textbf{FL to Personalize.}~We compare with FedL2P~\cite{royson2023fedl2p} which trains a MLP federatedly to output per-client learning rates for each LoRA module.

For each baseline, we either follow best practices recommended by the corresponding authors or employ a simple grid search and pick the best performing hyperparameters. Full details in Appendix.~\ref{appendix:experiments}.

\subsection{Results on Text Classification}\label{sec:text_class}

We evaluate our approach in a typical FL setup, where the pretrained model is first trained using Standard FL with full fine-tuning and the resulting \basemodel{} is then personalized to each client. Tables~\ref{tab:masakha_seen} \& \ref{tab:masakha_unseen} show the mean and standard deviation (SD) of the accuracy for each language in our MasakhaNEWS setup for \seen{} and \unseen{} pool respectively (similarly for XNLI in Appendix Tables~\ref{tab:xnli_seen} \& \ref{tab:xnli_unseen}). In addition, we also show the number of languages, labelled ``Wins", an approach is best performing for each budget $r$. 

The results in all four tables show that federated {\em learning to personalize} methods (FedL2P and \method{}) outperform the other baselines in most cases. Non-FL rank selection approaches (AdaLoRA and BayesTune-LoRA), on the other hand, tend to overfit and/or struggle to learn an optimal rank structure given the limited number of samples in each client. Comparing FedL2P and \method{}, \method{} largely surpass FedL2P with a few exceptions, indicating that learning to personalize LoRA rank structure is the better hyperparameter choice than personalizing learning rates; this finding is also aligned with recent LLM-based optimizer findings~\cite{zhao2025deconstructing}, which shows that Adam's performance is robust with respect to its learning rate.

\subsubsection{\method{}'s Complementability with Personalized FL Works.}\label{sec:eval_complement}

\input{tables/xnli_seen_FedDPA}
\input{tables/xnli_seen_DEPT}
\input{tables/cost}

Apart from Standard FL, we show that \method{} can be plugged into existing personalized FL works that trains both a subset of the pretrained model and personalized layers for each client. Tables~\ref{tab:xnli_seen_feddpa} and \ref{tab:xnli_seen_dept} show that \method{} outperforms baselines in almost all cases in our XNLI setup given a \basemodel{} trained using FedDPA-T~\cite{FedDPA} and DEPT(SPEC)~\cite{DEPT} respectively. In short, \method{} can be integrated into a larger family of existing personalized FL approaches, listed in Section~\ref{sec:related}, to further improve personalization performance. 

\input{tables/fedaya_seen_Llama3.2-3B}
\input{tables/fedaya_unseen_Llama3.2-3B}


\subsection{Results on Instruction-Tuning Generation}\label{sec:ift_gen}

We evaluate our approach on the more challenging real-world multilingual benchmark, Fed-Aya. Tables~\ref{tab:lama_fedaya_seen} and \ref{tab:lama_fedaya_unseen} show the average METEOR~\cite{meteor}/ROUGE-1/ROUGE-L~\cite{ROUGE} of each language given the off-the-shelf instruction finetuned Llama-3.2-3B (Llama-3.2-3B-Instruct) for \seen{} and \unseen{} clients respectively. Similarly, in Appendix Tables~\ref{tab:mobilellama_fedaya_seen} and \ref{tab:mobilellama_fedaya_unseen}, we show the same tables given a pretrained MobileLLaMA-1.4B model  trained using Standard FL with LoRA following the training recipe from FedLLM-Bench~\cite{fedllm-bench}. These two models represent scenarios where the \basemodel{} may or may not be trained using FL. Similarly to our text classification results, we report ``Wins" if an approach has a better performance in at least $2$ out of $3$ metrics. 

In all four tables, \method{} outperforms baselines in most scenarios. We also observe that FedL2P underperforms standard baselines in most cases, a phenomenon also observed for our XNLI setup when the \basemodel{} is trained with FedDPA-T (Tables~\ref{tab:xnli_seen_feddpa}). We hypothesize that the inner-loop optimization in FedL2P fail to reach a stationary point\footnote{FedL2P relies on the implicit function theorem for hypergradient computation.} due to the inherent task difficulty (Fed-Aya) or a less-performant \basemodel{}, resulting in a sub-optimal hypergradient and downstream performance. 

\subsubsection{Limitations of \method{}.} In some cases, \method{} falls short, especially in the recall performance (ROUGE), such as Russian (\textit{ru}) and French (\textit{fr}) for \unseen{} clients for both {\em base models}. These cases highlight a couple of limitations of our approach: \textit{i)} \textit{ru} is not seen by PSG during federated training; there are no \textit{ru} samples in any clients in the seen pool and \textit{ii)} none of the clients in the unseen pool have \textit{fr} as a predominant language (Fig.~\ref{fig:fed-aya}). In the case of \textit{ru}, there are no other languages that are similar to \textit{ru} in the seen pool, resulting in worse performance. Hence, we do not expect a similar outcome in datasets with a more diverse pool of clients.

For \textit{fr}, as the number of predominant language samples are orders of magnitude higher than that of \textit{fr} samples, the generated $\bm{\lambda}$ are catered towards the predominant language. A simple solution to counteract this limitation is to extend PSG to generate $\bm{\lambda}$ per instance, rather than per client. However, doing so is extremely costly, requiring a forward pass through the PSG for every sample. This calls for novel efficient solutions that can better handle each client's minority languages and is left as future work.

\subsection{Cost of \method{}}

Table~\ref{tab:cost} shows the mean latency, in seconds, and the peak memory usage across 100 runs on the first client in the \seen{} pool for $r=16$ using a single Nvidia A100 GPU. Non-FL baselines do not incur a federated training cost while FL approaches requires training the PSG. Comparing FedL2P and \method{}, \method{} does not require expensive second-order optimization, resulting in better efficiency. We also note that FedL2P needs to be run for every rank while \method{} runs once for all targeted ranks.

For communication costs, not shown in the table, \method{} is more costly as it predicts per LoRA rank while FedL2P predicts per layer. Nonetheless, these costs are negligible compared to running FL on the \basemodel{}; FedL2P uses 0.02\% and 0.002\% and \method{} uses 0.2\% and 0.16\% of the parameters of mBERT and Llama-3.2-3B respectively.

During inference, FL-based approaches incur an additional forward pass of \basemodel{} and the PSG compared to non-FL approaches. Memory-wise, \method{} results in the smallest memory footprint for autoregressive generation as the PSG learns not to attach LoRA modules $\lambda_l=0$ on some layers, skipping {\em matmul} operations entirely.

\begin{figure}[t]
    \vspace{-0.2em}
    \small
    \centering
    \includegraphics[width=0.94\columnwidth,trim={0cm 0cm 0cm 3.5cm},clip]{figures/xnli_out_0.5_seen.png}
    % \captionsetup{font=small,labelfont=bf}
    \vspace{-2em}
    \caption{$\bm{\lambda}$ distance among languages in our XNLI setup. Each block shows the log-scale normalized average Euclidean distances between all pairs of clients' $\bm{\lambda}$ for two languages (see text). The smaller the distance, the more similar $\bm{\lambda}$ is. }
    \label{fig:xnli_out}
    \vspace{-1.5em}
\end{figure}

\subsection{Further Analysis}\label{sec:analysis}

In this section, we further analyze $\bm{\lambda}$ and how they differ across languages. Surprisingly, we find that \method{} learns language-agnostic rank structures. In other words, depending on the task and the \basemodel{}, the rank structure of $\bm{\lambda}$ is fixed across languages. For instance, in the case where $r=2$, \method{} allocate ranks to dense layers instead of attention blocks. With more budget, \textit{e.g.},~$r=16$, \method{} allocates more rank to either the query attention layer or the value attention layer depending on the setup. We show these rank structures across all setups for $r=2$ and $r=16$ in Appendix Fig.~\ref{fig:xnli_fedavg_out_r16}-\ref{fig:llama3_fedavg_out_r2}.

While the rank structure is the same across languages, the rank-wise scales (absolute values of $\bm{\lambda}$) differ. Following FedL2P, we visualize the difference in $\bm{\lambda}$ for different languages using the normalized mean distance, $d(j,k)$, between all clients pairs holding data for languages $j$ and $k$. Fig.~\ref{fig:xnli_out} and Appendix Fig.~\ref{fig:masakha_out} show these distances for XNLI and MasakhaNEWS setup respectively. Specifically, the value of each block in each figure is computed as follows: $\log(\frac{d(j,k)}{\sqrt{d(j,j)}\sqrt{d(k,k)}})$. Hence, the smaller the distance, the more similar $\bm{\lambda}$ is between languages. The results are aligned with our intuition that similar languages have similar $\bm{\lambda}$. For instance, the closest language to Urdu (\textit{ur}) is Arabic (\textit{ar}), both of which have the closest $\bm{\lambda}$ similarity (Fig.~\ref{fig:xnli_out}); likewise, for Tigrinya (\textit{tir}) and Amharic (\textit{amh}) in Appendix Fig.~\ref{fig:masakha_out}. We also observe that unrelated languages have similar $\bm{\lambda}$, \textit{e.g.},~Mandarin (\textit{zh}) and Vietnamese (\textit{vi}) share similar $\bm{\lambda}$ with the Indo-European languages (Fig.~\ref{fig:xnli_out}). This finding adds to existing evidence that leveraging dissimilar languages can sometimes benefit particular languages~\cite{fedllm-bench}.

