\appendix
\newpage
\lstset{language=Python,basicstyle=\small\ttfamily,columns=fullflexible}
\section{Detailed Experimental Setup}\label{appendix:experiments}

For reproducibility and completeness, we provide comprehensive details of all setups, datasets, tasks, models, baselines, and hyperparameters. Code is in the process of being released.

\subsection{Tasks, Datasets, and Data Partitioning}

\noindent\textbf{XNLI~\cite{XNLI}}~A natural language inference benchmark dataset for evaluating cross-lingual understanding covering 15 diverse languages including both high- and low-resources languages: English, French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu. XNLI consists of premise-hypothesis pairs, labeled as entailment, contradiction, or neutral across different languages. We sample 2k instances from the XNLI train split and 500 instances from the test split for each pool. The data is then divided equally among 20 clients for each language using the latent Dirichlet allocation (LDA) partition with $\alpha=0.5$. Hence, the total number of clients is 600 (15 languages $\cdot$ 20 clients per language $\cdot$ 2 pools).

\noindent\textbf{MasakhaNEWS~\cite{MasakhaNEWS}}~A news topic classification benchmark designed to address the lack of resources for African languages. It covers 2 high-resource languages, English and French, and 14 low-resource languages, namely Amharic, Hausa, Igbo, Lingala, Luganda, Naija, Oromo, Rundi, chiShona, Somali, Kiswahili, Tigrinya, isiXhosa, and Yorùbá. Each sample contains a headline, the body text, and one of the 7 labels: business, entertainment, health, politics, religion, sports, or technology. We first combine all samples from the MasakhaNEWS train and validation split to form our train set, and use the MasakhaNEWS test split as our test set. We then split both train and test in each of the 16 languages by half for each pool. Following our XNLI setup, we adopt LDA $\alpha=0.5$ and divide each language's data equally into 10 clients. Hence, the total number of clients is 320 (16 languages $\cdot$ 10 clients per language $\cdot$ 2 pools). Note that unlike XNLI, the number of samples for each language differs, hence there is quantity skew across clients. 

\noindent\textbf{Fed-Aya~\cite{fedllm-bench}}~A federated instruction tuning benchmark, based on Aya~\cite{singh2024aya}, where the data is annotated by contributors and partitioned by annotator ID. Following FedLLM-Bench~\cite{fedllm-bench}, we focus on 6 high-resource languages, English, Spanish, French, Russian, Portuguese, Chinese, and 2 low-resource languages, Arabic and Telugu. Additionally, we filter out the other languages from the dataset. Out of 38 clients, we select 8 for our \unseen{} pool, $\text{client\_ids}=[21, 22, 23, 24, 25, 26, 27, 34]$ and the rest goes into our \seen{} pool. Each client can have up to 4 languages where the number of data samples can range from a hundred to over a thousand samples per client.

\subsection{Models, Tokenizers, and Data Preprocessing}

\noindent\textbf{mBERT~\cite{BERT}.}~We use the pretrained multilingual BERT with its WordPiece tokenizer for all sequence classification experiments, namely all XNLI and MasakhaNEWS setups with various {\em base models}. For both datasets, we use a training batch size of 32 and pad input tokens on the right to a max token length of 128 and 256 respectively.

\noindent\textbf{MobileLLaMA-1.4B~\cite{mobilellama}.}~We train a \basemodel{} with a pretrained MobileLLaMA-1.4B with Standard FL using LoRA in our Fed-Aya setup. We use the default LLaMA tokenizer which is a BPE model based on sentencepiece~\cite{Kudo2018SentencePieceAS} and adopt the UNK token as the PAD token. During training, we use an effective batch size of $16$ and pad right to the longest token in the batch up to a max token length of 1024. For evaluation, we use a batch size of 8, padding left instead, with greedy sampling up to a max new token length of 1024. We use the Alpaca template to format each prompt:

\begin{lstlisting}[linewidth=\columnwidth,breaklines=true]
alpaca_template = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{} 

### Response: {}{}"""
\end{lstlisting}

\noindent\textbf{Llama-3.2-3B~\cite{llama3}.}~We use the off-the-shelf Llama-3.2-3B-Instruct model as our \basemodel{} and its default tokenizer which is a BPE model based on tiktoken\footnote{https://github.com/openai/tiktoken}. Training and evaluation hyperparameters are the same as the ones we use for MobileLLaMA. The only two differences are \textit{1)} we add a PAD token `\verb|<pad>|', and \textit{2)} we use the Llama 3 instruction template instead:

\begin{lstlisting}[linewidth=\columnwidth,breaklines=true]
llama3_instruct_template = """<|begin_of_text|><|start_header_id|>user<|end_header_id|>

{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{}{}
"""
\end{lstlisting}

\subsection{Complementary Approaches and Base Models}

In this work, we experiment with different {\em base models} to show that \method{} is complementary to a range of off-the-shelf models and models trained using existing FL approaches. In this section, we detail the different approaches we used to obtain these {\em base models}.

\noindent\textbf{Standard FL.} Standard FL involves training a single global model. Given a pretrained LLM, we run FedAvg on the \seen{} pool of clients, where $10\%$ of participating clients are sampled every round to train the model before sending the weights back for aggregation. In our XNLI and MasakhaNEWS setup, we do full fine-tuning of mBERT, setting each client's learning rate to $5e-5$ and running FedAvg for 100 rounds. In our Fed-Aya setup, we adopt the training recipe from FedLLM-Bench~\cite{fedllm-bench} for MobileLLaMA-1.4B, where we do PEFT with LoRA applied to query and value attention weights ($r=16$, $\alpha_{lora}=32$, dropout$=0.05$) for 200 rounds. We use the cosine learning rate decay over 200 rounds with initial learning rate $2e^{-5}$ and minimum learning rate $1e^{-6}$.

\noindent\textbf{Personalized FL.} We train personalized {\em base models} using FedDPA-T~\cite{FedDPA} and DEPT(SPEC)~\cite{DEPT} in our XNLI setup. FedDPA-T proposed having two separate LoRA adapters, one of which is shared (global) and the other is kept locally for each client (local). 
We adopted their training recipe for sequence classification, where the classifier is shared together with the global LoRA modules and the local LoRA modules stay local. LoRA modules are only applied to query and value attention weights (r=8, $\alpha_{lora}=8$, dropout=$0.05$). We set the learning rate to $5e^{-5}$.

DEPT (SPEC), on the other hand, proposed having personalized token and positional embeddings for each client. As DEPT was proposed for cross-silo FL, while we target cross-device FL, they assumed that each data source has an abundance of data to retrain the newly initialized embeddings. Hence, to adapt to the cross-device FL setting, we did not reinitialize the embeddings; each client fine-tunes their own embeddings starting from the pretrained mBERT embeddings. In other words, for each FL round, each client does full fine-tuning, sending weights of all layers except their own embeddings back to the server for aggregation. As with FedDPA-T, the learning rate is set to $5e^{-5}$.

\noindent\textbf{Off-the-shelf.} In our Fed-Aya setup, we use an off-the-shelf instruction finetuned Llama-3.2-3B-Instruct as our \basemodel{}.

\subsection{Baselines and \method{}}

To avoid an exponentially big search space, all hyperparameter tuning is done using simple grid search on our XNLI setup, with mBERT, and Fed-Aya setup, with MobileLLaMA as the \basemodel{}. The best hyperparameters found are then used for MasakhaNEWS and Fed-Aya with Llama3 respectively.

\noindent\textbf{LoRA PEFT~\cite{hu2021lora}.}~We search for the learning rate $[1e^{-5},1e^{-4},1e^{-3}]$ and the number of epochs $[1,2,3]$ and find that the learning rate $1e^{-4}$ with $2$ epochs had the best performance on the train set. We fixed $\alpha_{lora}=2r$. To ensure a similar inference budget across baselines, we set the number of epochs to $2$ for all our experiments.

\noindent\textbf{AdaLoRA~\cite{adalora}.}~Similarly to LoRA, we search for the learning rate $[1e^{-5},1e^{-4},1e^{-3},1e^{-2}]$, the time interval between two budget allocations, $\Delta_T$, $[1,10,100]$ and the coefficient of the orthogonal regularization, $\gamma$, $[0.1,0.5]$. Within our search space, we find learning rate$=1e^{-3}$, $\Delta_T=1.0$, and $\gamma=0.1$ to be the best performing one. We run AdaLoRA once per resource budget $r$, setting the initial rank to be $1.5\times$ of $r$, as recommended. We set the initial fine-tuning warmup steps and final fine-tuning steps to be $10\%$ and $30\%$ of the total steps respectively. 

\noindent\textbf{BayesTune-LoRA (Section\ref{sec:personalized_peft}).}~For fair comparison with \method{}, we use the same hyperparameters as \method{}. This baseline, hence, is an ablation study of how much performance collaboratively learning a PSG adds.

\noindent\textbf{FedL2P~\cite{royson2023fedl2p}}~As FedL2P requires a validation set for outer-loop bi-level optimization and federated early stopping, we split the train set of every client $80\%$ train and $20\%$ validation. Following FedL2P, we set the federated early stopping patience to 50 rounds, MLP hidden dimension is set to 100, the inner-loop learning rate to be the same as finetuning, $1e^{-4}$, and the hypergradient hyperparameters, $Q=3, \psi=0.1$ with hypergradient clamping of $[-1,1]$. 
We use Adam for both the inner-loop and outer-loop optimizers and search for the learning rate for the MLP (LRNet) $[1e^{-5},1e^{-4},1e^{-3}]$ and the learnable post-multiplier learning rate $\tilde{\eta}$ $[1e^{-4},1e^{-5},1e^{-6}]$, picking $1e^{-4}$ and $1e^{-6}$ to be the best respectively. Finally, we use $3$ outer-loop steps with an effective outer-loop batch size of $16$.

\noindent\textbf{\method{} (Section~\ref{sec:main_method})}~We set $\alpha_{r\_mul}=2$, our resulting $r_{init}$ is, hence, $32$ since our $r_{\text{max target}}=16$ for all experiments. Following our standard LoRA fine-tuning baseline, we adopt the same learning rate and $\alpha_{lora}$, $1e-4$ and $2r_{init}$ respectively. The learning rate of $\bm{\lambda}$, on the other hand, is searched $[1e^{-1},1e^{-2},1e^{-3},1e^{-4}]$, and we pick $1e^{-2}$ for all experiments.
All $\lambda$ values are initialized to $1e^{-4}$. The MLP hidden dimension is set to $2\times$ the input dimension, which is model dependent. We clamp the output of the MLP as well as $\lambda$ with a minimum value of $1e-4$ in the forward pass during training. Following FedL2P, we use a straight-through estimator~\cite{bengio2013estimating} after clamping to propagate gradients. We initialize the weights of MLP with Xavier initialization~\cite{glorot2010understanding} using the normal distribution with a gain value of $1e-6$ and the bias with a constant $1e-4$. Lastly, we set $\alpha_s=1e^{+2}$ and $\alpha_p=1e^{-2}$ for all experiments.


\section{Additional Results}

This section contains supplementary results and analyses, omitted from the main paper due to space limitations, that complement the presented findings.
Fig.~\ref{fig:xnli_dept_out_r16}-\ref{fig:llama3_fedavg_out_r2} show the language-agnostic rank structures under different budgets ($r=2$ and $r=16$) learnt by \method{} for different setups as mentioned in Section~\ref{sec:analysis}. These plots illustrate the prioritization of layers for LoRA fine-tuning. 

Note that while the rank structure is the same across languages, the strength of personalization, absolute value of $\bm{\lambda}$, differs, as shown in Fig.~\ref{fig:masakha_out} in this Section and Fig.~\ref{fig:xnli_out} in the main paper. These two figures show the difference in $\bm{\lambda}$ across languages as described in Section~\ref{sec:analysis}. To sum up, the smaller the distance between two languages, represented as a block in the figure, the more similar their generated $\bm{\lambda}$ are. The results show that while similar languages sometimes exhibit similar $\bm{\lambda}$ values, unrelated languages also occasionally share similar $\bm{\lambda}$, consistent with findings in the literature that leveraging dissimilar languages can be beneficial.

Lastly, Tables~\ref{tab:xnli_seen} and \ref{tab:xnli_unseen} contain results for our XNLI setup where the \basemodel{} is fine-tuned from the pretrained mBERT with Standard FL using full fine-tuning and is used to complement results and findings in Section~\ref{sec:text_class}. Similarly, Tables~\ref{tab:mobilellama_fedaya_seen} and \ref{tab:mobilellama_fedaya_unseen} complement the results and findings of our Fed-Aya setup described in Section~\ref{sec:ift_gen}.


\begin{figure}[t]
    \small
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/masakha_out_0.5_seen.png}
    % \captionsetup{font=small,labelfont=bf}
    \vspace{-3em}
    \caption{$\bm{\lambda}$ distance among languages in our MasakhaNEWS setup. Each block shows the log-scale normalized average Euclidean distances between all pairs of clients' $\bm{\lambda}$ for two languages (see text). The smaller the distance, the more similar $\bm{\lambda}$ is. }
    \label{fig:masakha_out}
    % \vspace{-2em}
\end{figure}

\input{tables/xnli_seen}
\input{tables/xnli_unseen}

\input{tables/fedaya_seen_mobileLlama1.4b}
\input{tables/fedaya_unseen_mobileLlama1.4b}


\begin{figure*}
\centering
\begin{minipage}{.23\linewidth}
  \includegraphics[width=\linewidth]{figures/xnli_fedavg_out_0.5_seen_en_layerwiserank.png}
  \captionof{figure}{Language agnostic rank structure of mBERT in our XNLI setup where the \basemodel{} is trained with FedIFT full-finetuning ($r=16$).}
  \label{fig:xnli_fedavg_out_r16}
\end{minipage}
\hspace{.01\linewidth}
\begin{minipage}{.23\linewidth}
  \includegraphics[width=\linewidth]{figures/xnli_fedavg_out_0.0625_seen_en_layerwiserank.png}
  \captionof{figure}{Language agnostic rank structure of mBERT in our XNLI setup where the \basemodel{} is trained with FedIFT full-finetuning ($r=2$).}
  \label{fig:xnli_fedavg_out_r2}
\end{minipage}
\hspace{.01\linewidth}
\begin{minipage}{.23\linewidth}
  \includegraphics[width=\linewidth]{figures/xnli_dept_out_0.5_seen_en_layerwiserank.png}
  \captionof{figure}{Language agnostic rank structure of mBERT in our XNLI setup where the \basemodel{} is trained with DEPT(SPEC) ($r=16$).}
  \label{fig:xnli_dept_out_r16}
\end{minipage}
\hspace{.01\linewidth}
\begin{minipage}{.23\linewidth}
  \includegraphics[width=\linewidth]{figures/xnli_dept_out_0.0625_seen_en_layerwiserank.png}
  \captionof{figure}{Language agnostic rank structure of mBERT in our XNLI setup where the \basemodel{} is trained with DEPT(SPEC) ($r=2$).}
  \label{fig:xnli_dept_out_r2}
\end{minipage}
\end{figure*}

\begin{figure*}
\centering
\begin{minipage}{.24\linewidth}
  \includegraphics[width=\linewidth]{figures/masakha_out_0.5_seen_eng_layerwiserank.png}
  \captionof{figure}{Language agnostic rank structure of mBERT in our MasakhaNEWS setup where the \basemodel{} is trained with FedIFT full-finetuning ($r=16$).}
  \label{fig:masakha_fedavg_out_r16}
\end{minipage}
\hspace{.2\linewidth}
\begin{minipage}{.24\linewidth}
  \includegraphics[width=\linewidth]{figures/masakha_out_0.0625_seen_eng_layerwiserank.png}
  \captionof{figure}{Language agnostic rank structure of mBERT in our MasakhaNEWS setup where the \basemodel{} is trained with FedIFT full-finetuning ($r=2$).}
  \label{fig:masakha_fedavg_out_r2}
\end{minipage}
\end{figure*}


\begin{figure*}
\centering
\begin{minipage}{.23\linewidth}
  \includegraphics[width=\linewidth]{figures/mobilellama_out_0.5_seen_en_layerwiserank.png}
  \captionof{figure}{Language agnostic rank structure of MobileLLaMA-1.4B in our Fed-Aya setup where the \basemodel{} is trained with FedIFT LoRA ($r=16$). Zoom in for best results.}
  \label{fig:mobilellama_fedavg_out_r16}
\end{minipage}
\hspace{.01\linewidth}
\begin{minipage}{.23\linewidth}
  \includegraphics[width=\linewidth]{figures/mobilellama_out_0.0625_seen_en_layerwiserank.png}
  \captionof{figure}{Language agnostic rank structure of MobileLLaMA-1.4B in our Fed-Aya setup where the \basemodel{} is trained with FedIFT LoRA ($r=2$). Zoom in for best results.}
  \label{fig:mobilellama_fedavg_out_r2}
\end{minipage}
\hspace{.01\linewidth}
\begin{minipage}{.23\linewidth}
  \includegraphics[width=\linewidth]{figures/llama3_out_0.5_seen_en_layerwiserank.png}
  \captionof{figure}{Language agnostic rank structure of Llama-3.2-3B in our Fed-Aya setup where the \basemodel{} is an off-the-shelf instruction tuned Llama-3.2-3B-Instruct ($r=16$). Zoom in for best results.}
  \label{fig:llama3_fedavg_out_r16}
\end{minipage}
\hspace{.01\linewidth}
\begin{minipage}{.23\linewidth}
  \includegraphics[width=\linewidth]{figures/llama3_out_0.0625_seen_en_layerwiserank.png}
  \captionof{figure}{Language agnostic rank structure of Llama-3.2-3B in our Fed-Aya setup where the \basemodel{} is an off-the-shelf instruction tuned Llama-3.2-3B-Instruct ($r=2$). Zoom in for best results.}
  \label{fig:llama3_fedavg_out_r2}
\end{minipage}
\end{figure*}



