\section{Related Work}
\label{sec:related}

\noindent\textbf{Multilingual LLMs (MLLMs).}~Existing efforts in multilingual LLMs often underperform on low-resource languages due to \textit{1)}~data scarcity____, \textit{2)}~the model's limited capacity to learn the intricacies of multiple languages____, and \textit{3)}~negative transfer learning among languages____.
Common ways to counteract these challenges include the use of separate vocabulary and embeddings____, hand-crafted adapters____, automatic data annotation____, clustering and merging languages with similar representations____, among other contributions____. Our work is orthogonal to these approaches and builds upon recent FL-based MLLMs____, which utilize FL to tap into previously inaccessible low-resource data sources.

\noindent\textbf{Personalized Federated Learning.}~To obtain personalized client-specific models, various approaches have been proposed, including the use of personalized layers____, meta-learning____, model mixtures____, hypernetworks____, transfer learning between global and local models____, among other contributions____. Some of these techniques have also been adopted for LLMs, \textit{e.g.},~personalized LoRAs____, hypernetworks for client embeddings____, and mixtures of LoRA experts____. Our work complements these approaches as personalized models can benefit from further fine-tuning as shown in Section~\ref{sec:eval_complement}.

\noindent\textbf{Federated Hyperparameter Optimization (HPO).}~Most federated approaches to HPO do not utilize the client dataset for personalized hyperparameters. Instead, they employ a single set of hyperparameters across all clients based on the local validation loss evaluated before FL____ or sample from federatedly learnt hyperparameter categorical distributions for each client____. An exception to this is FedL2P____ which utilizes a PSG for personalized per-layer learning rates and batch normalization hyperparameters. We compare with FedL2P in our experiments.
% %FedL2P proposed federatedly learning a PS generator (PSG) which takes in as input, the client features conditioned on the base model, and outputs per-layer layer-wise and batch normalization hyparams. While FedL2P has been shown to work well in standard small-scale image and speech benchmarks, their applicability to LLMs is unclear: i) LLMs don't use BN, ii) LLMs often use adaptive optimizers, making learning rate a less sensitive hyparam for downstream performance. Learning the learning rates also require FedL2P to adopt expensive 2nd-order optimization methods as the learning rate is not a direct gradient of the loss, a limitation that is further aggravated with LLMs.

\noindent\textbf{PEFT Structure Learning.}~Contrary to the conventional approach of distributing uniform adapter modules across all layers, a recent line of work allows different LoRA ranks to be used across a model's weight matrices. Using fine-grained per-layer rank selection, existing methods include SVD-based LoRA reformulation followed by importance-based rank assignment____, trainable rank-gating units____, selectively employing parallel weight modules____, meta-learning-based____ and black-box optimization techniques____, specialized training recipes for multi-rank LoRA modules that allow flexible extraction of a range of ranks____, and coarse-grained dropping of LoRA-enhanced layers____. While these methods can be effective in centralized setups, they typically require an excessive number of optimization steps, which is prone to overfitting in FL settings, where clients have limited amount of data.