% \documentclass[lettersize,twoside,journal]{IEEEtran}
\documentclass[preprint]{IEEEtran}

\usepackage{url}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[L]{Preprint submitted to arXiv}

\usepackage{float}
\usepackage{multirow}
\usepackage{graphicx}
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{xcolor}
\usepackage{array}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage[linesnumbered, ruled]{algorithm2e}
\usepackage{amsmath} % some math-related packages, not sure which of them are necessary
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{orcidlink}
\setlength{\textfloatsep}{10pt plus 1.0pt minus 2.0pt}
\setlength{\abovecaptionskip}{5pt} 
\setlength{\belowcaptionskip}{5pt} 

\begin{document}

\title{A Novel Multi-Teacher Knowledge Distillation for Real-Time Object Detection using 4D Radar}

\author{Seung-Hyun Song\orcidlink{0009-0001-4813-779X},~\IEEEmembership{Student~Member,~IEEE}, Dong-Hee Paek\orcidlink{0000-0003-0008-3726},~\IEEEmembership{Student~Member,~IEEE}, Minh-Quan Dao\orcidlink{0009-0001-4132-2159},~\IEEEmembership{Member,~IEEE}, Ezio Malis\orcidlink{0000-0002-6584-6790},~\IEEEmembership{Member,~IEEE}, and Seung-Hyun~Kong\orcidlink{0000-0002-4753-1998},~\IEEEmembership{Senior~Member,~IEEE}% 

\thanks{Manuscript received [Submission Date]; revised [Revision Date]; accepted [Acceptance Date]. Date of publication [Publication Date]; date of current version 14 December 2024. This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. 2021R1A2C3008370). \textit{(Corresponding authors: Seung-Hyun Kong.)}}
\thanks{Seung-Hyun Song is with the Graduate School of Advanced Security Science and Technology, Korea Advanced Institute of Science and Technology, Daejeon, Korea, 34051 (e-mail: shyun@kaist.ac.kr)}
\thanks{Dong-Hee Paek and Seung-Hyun Kong are with the CCS Graduate School of Mobility, Korea Advanced Institute of Science and Technology, Daejeon, Korea, 34051 (e-mail: donghee.paek@kaist.ac.kr; skong@kaist.ac.kr)}
\thanks{Minh-Quan Dao and Ezio Malis are with Centre Inria d'Univeristé Côte d'Azur (e-mail: minh-quan.dao@inria.fr; ezio.malis@inria.fr)} 
}


% The paper headers
% \markboth{IEEE ROBOTICS AND AUTOMATION LETTERS, Vol. , No. , December 2024}{Song \MakeLowercase{\textit{et al.}}: A Novel Multi-Teacher Knowledge Distillation for Real-Time Object Detection using 4D Radar}


\maketitle

\begin{abstract}
Accurate 3D object detection is crucial for safe autonomous navigation, requiring reliable performance across diverse weather conditions.
While LiDAR performance deteriorates in challenging weather, Radar systems maintain their reliability. 
Traditional Radars have limitations due to their lack of elevation data, but the recent 4D Radars overcome this by measuring elevation alongside range, azimuth, and Doppler velocity, making them invaluable for autonomous vehicles.
The primary challenge in utilizing 4D Radars is the sparsity of their point clouds. 
Previous works address this by developing architectures that better capture semantics and context in sparse point cloud, largely drawing from LiDAR-based approaches. 
However, these methods often overlook a unique advantage of 4D Radars: the dense Radar tensor, which encapsulates power measurements across three spatial dimensions and the Doppler dimension.
Our paper leverages this tensor to tackle the sparsity issue. 
We introduce a novel knowledge distillation framework that enables a student model to densify its sparse input in the latent space by emulating an ensemble of teacher models.
Our experiments demonstrate a 25\% performance improvement over the state-of-the-art RTNH model on the K-Radar dataset.
Notably, this improvement is achieved while still maintaining a real-time inference speed.
% The resulting model achieves a 30 frame-per-second inference speed on an NVIDIA RTX 3090.

\end{abstract}

\begin{IEEEkeywords}
4D Radar, 3D object detection, Knowledge distillation, Radar preprocessing
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle



\section{Introduction} \label{sec:intro}

\IEEEPARstart{3}{D} 
Object detection is vital for enabling accurate autonomous navigation by ensuring precise positioning of surrounding road users, and robust performance in all weather conditions is essential for overall safety \cite{geiger2012we,caesar2020nuscenes,mirza2021robustness}.
While LiDARs have traditionally been the primary sensing modality for this application due to their ability to measure distances accurately in 3D, they struggle to function properly in adverse conditions like snow, rain, and fog due to their reliance on lights that are not harmful to human eyes \cite{bijelic2018benchmark}. 
In contrast, Radars remain unaffected by these conditions as they utilize electromagnetic waves.
However, conventional Radars have a significant limitation: their measurements are confined to a single plane, rendering them incapable of detecting objects in 3D \cite{palffy2022multi}. 
Recent advancements in the Radar technology have led to the development of 4D Radars, which add elevation measurements to the conventional range, azimuth, and Doppler velocity data. 
This addition lifts Radar measurements to full 3D space, making 4D Radars a promising solution for robust 3D object detection \cite{palffy2022multi, zheng2022tj4dradset, kradar}.

The major limitation of 4D Radars is the sparsity of their point clouds.
Prior Radar-based object detection methods address this issue through devising architectures that are better at capturing semantic and context information in sparse point clouds.
RPFA-Net \cite{rpfa} builds upon the Pillar Feature Net of PointPillar \cite{pointpillars}, incorporating a self-attention module \cite{attention} to improve the modeling of relationships among Radar points within each pillar. Similarly, SMURF \cite{smurf} extends PointPillar by complementing pillar-based features with density-based features extracted through Kernel Density Estimation, mitigating the impact of point cloud sparsity.
RTNH \cite{kradar} utilizes 4D Radar tensor data to demonstrate that the density of the Radar point cloud significantly affects model performance\cite{enhancedkradar}, and employs a sparse 3D convolution-based architecture to integrate height-related information into Radar features. 
MVFAN \cite{mvfan} introduces a multi-branch architecture that extracts features from both Bird's-Eye View (BEV) and cylindrical view representations of the Radar point cloud. Taking this multi-view approach further, SMIFormer \cite{smiformer} extracts features from BEV, Front View (FV), and Side View (SV) representations, fusing them using multi-view interaction transformers.
Leverage the Doppler velocity of Radar points to compensate for the motion of dynamic objects, RadarMFNet \cite{tan20223} aggregates multiple point clouds to obtain a denser input.

These approaches are heavily influenced by LiDAR-based object detection techniques, overlooking a unique feature of 4D Radars. 
Unlike LiDARs that directly produce point clouds, 4D Radars generate a dense 4D tensor containing power measurements across three spatial dimensions (range, azimuth, and elevation) and the Doppler dimension. 
We refer to this tensor as 4DRT throughout this paper.
To create Radar point clouds, the dense 4DRT undergoes a preprocessing step to retain only high-power measurements. 

\begin{figure}[h]
    \centerline{\includegraphics[width=0.9\columnwidth]{num_of_points.png}}
    \caption{
        Relation between the average number of points and model performance: 
        The solid and dashed lines depict BEV and 3D precision of the RTNH model, with color gradient representing GPU RAM usage.
    }
    \label{fig.num of points}
\end{figure}

A straightforward solution to the sparsity challenge would be to relax the definition of "high-power measurement," resulting in denser point clouds. As shown in Fig.\ref{fig.num of points}, this increased density directly improves detection performance. However, this approach comes at the cost of higher memory consumption and, more critically, longer inference times.
The resulting performance-efficiency trade-off renders this straightforward solution impractical, especially for applications with strict real-time requirements and resource constraints, such as autonomous driving.

In this paper, we leverage 4DRT to address the sparsity of Radar point clouds without sacrificing efficiency for performance.
We make the following contributions:
\begin{itemize}
    \item the first multi-teacher knowledge distillation framework using 4D Radar, that enables a student model to densify its sparse input in the latent space,
    
    \item enabling Radar-based detection models that operate directly on highly sparse point clouds, ensuring low memory consumption and high inference speed without compromising detection performance,

    \item eliminating of the need for extrinsic calibration between Radars and other modalities for the knowledge distillation,
    
    \item comprehensive experiments on the large-scale K-Radar dataset \cite{kradar}, results show a 25\% improvement in detection performance over the state-of-the-art RTNH model when operating on sparse point clouds, while maintaining a real-time inference speed of 30 frames per second (FPS) on an NVIDIA RTX 3090 GPU. 
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}
\subsection{Radar Processing}
To provide the context of our work, we briefly outline the Radar processing procedure illustrated in Fig.\ref{fig.Radar processing}, which transforms electromagnetic wave responses into Radar point clouds. The process begins by converting the received signal from analog to digital, followed by two Fast Fourier Transforms (FFT) to generate a 4-dimensional tensor (4DRT) containing range, azimuth, elevation, and Doppler measurements, as introduced in Sec.\ref{sec:intro}. Finally, preprocessing (Alg.\ref{alg:preproc}) averages the Doppler dimension and retains a high-percentile subset of measurements, reducing noise while balancing sparsity, as shown in Fig.\ref{fig.percentile_density}.

\begin{figure}[h]
    \centerline{\includegraphics[width=0.99\linewidth]{Radar_preprocessing.png}}
    \caption{
        Radar processing pipeline: Microwave signals are digitized, then transformed via two FFTs into a dense 4D tensor. A preprocessing step then converts it to a sparse point cloud.
    }
    \label{fig.Radar processing}
\end{figure}

\begin{algorithm}
    \DontPrintSemicolon
    \SetNoFillComment
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{
        $\mathbf{4DRT}$ $\in \mathbb{R}^4$ \small(azimuth, range, elevation, Doppler), 
        \\
        percentile $r$ $\in \mathbb{R}$
    }
    \Output{Radar point cloud $\mathbb{P} = \left\{ \mathbf{p}_i = \left[x, y, z, power\right] \right\}$}

    \BlankLine

    \tcc{\scriptsize Power Mapping}
    $\mathbf{power}$ = \textrm{mean}$\left(\mathbf{4DRT}, ~\textrm{dim=Doppler}\right)$  
    
    \BlankLine

    $power\_threshold$ = $r$ percentile of $\mathbf{power}$ 
    
    \BlankLine
    \tcc{\scriptsize Thresholding and Coordinate Transformation}
    
    $\mathbb{P} = \emptyset$

    \For{\footnotesize each discrete coordinate [azimuth, range, elevation] of $\mathbf{power}$}{
        $pw = \mathbf{power}$[azimuth, range, elevation]

        $azi$, $rg$, $ele$ = \text{discrete\_to\_continuous}([azimuth, range, elevation])


        \If{$pw \geq power\_threshold$}{
            $x = rg \cdot \cos(ele) \cdot \cos(azi)$

            $y = rg \cdot \cos(ele) \cdot \sin(azi)$

            $z = eg \cdot \sin(ele)$

            \text{add} $\mathbf{p} = [x, y, z, power]$ to $\mathbb{P}$
        }
    }
    \caption{Converting the 4DRT to a point cloud}
    \label{alg:preproc}
\end{algorithm}

\begin{figure}[ht!]
    \centerline{\includegraphics[width=0.95\linewidth]{various_rpc.png}}
    \caption{
        Comparison of 4D Radar point clouds preprocessed with various percentile $r$: (a) Original output from 4DRT, (b) 95th percentile, (c) 90th percentile, and (d) 80th percentile.
    }
    \label{fig.percentile_density}
\end{figure}

\begin{figure*}[tb]
    \centerline{\includegraphics[width=0.9\linewidth]{Network_Architecture.png}}
    \caption{
        \textbf{4DMT}: a multi-teacher knowledge distillation for efficient Radar-based object detection.
        High density point clouds are used to train teacher models enabling them to achieve high detection performance, while
        sparse point clouds are fed into the student model.
        During training, the student model optimizes a weighted sum of detection loss and distillation loss, learning to detect objects in sparse Radar point clouds while mimicking the fused intermediate feature maps of teacher models.
    }
    \label{fig.architecture}
\end{figure*}

\subsection{Methods Using Raw Radar Data}
As we exploit the dense 4D Radar tensor that have not undergone the preprocessing, our method belongs to the category of methods using raw Radar data.
Since Radar data undergoes a multi-stage processing before yielding sparse point clouds, methods in this category take the ADC signal, or the Range-Doppler Map (RDM), or the 4DRT, as their input.
The early work FFT-RadNet \cite{fftradnet} directly detect objects in the RDM.
Since this tensor lacks the explicit azimuth, which is crucial for 3D object detection, FFTRadNet implicitly estimate the azimuth in its latent space by forcing an axis of its feature map to represent the azimuth value.
% This implicit estimation is realized through the optimization of a loss function made of object detection losses and sematic segmentation losses. 
ADCNet \cite{adcnet} and T-FFTRadNet \cite{t-fftradnet} go one step further by using the ADC signal.
Based on a learnable form of the discrete Fourier transform, they devise a learnable signal processing module that transforms the input ADC signal into a latent representation of the range-azimuth-Doppler map.
DPFT \cite{fent2024dpft} takes the 4DRT as its input and project this tensor to the range-azimuth plane and the azimuth-elevation plane for feature extraction.
The two resulting feature maps are fused with the camera's through the deformable attention mechanism \cite{zhu2021deformable}.

While the methods above use the raw Radar data for both training and inference, our method only requires the raw data for training.
Specifically, we vary the percentile $r$ of the preprocessing to obtain high-density point clouds for training teacher models.
During inference, the student model uses directly the preprocessed sparse point cloud.
This ensures a low memory consumption and a high inference speed of the student model.

\subsection{Distillation Methods}
The idea of the knowledge distillation framework \cite{hinton2015distilling} is to use the feature map of a model, referred to as the teacher, to guide the feature computation of another model, referred to as the student.
This is implemented by complemeting the task-specific loss function of the student with addtional terms measuring the similarity between its feature maps and the teacher's.
Prior works on knowledge distillation for Radar-based object detection distill LiDAR-based or camera-based models to leverage their relatively dense representation.
RadarDistill \cite{bang2024radardistill} align the representation of a Radar and a LiDAR in BEV.
Then, the Radar model is forced to emulate the representation of the LiDAR model in key regions.
CRKD \cite{zhao2024crkd} take the similar key-region-based approach to transfer knowledge from a camera-based model.
A development made by CRKD is the enforcement of the similarity at the global level through the application of the L1 loss on the affinity matrix of the representation of camera and of Radar.

Unlike cross-modal distillation methods, our approach requires no extrinsic calibration between Radar and other modalities, enhancing practicality given the complexity of Radar calibration \cite{domhof2021joint, li2023globally}. Moreover, differences in sensing capabilities, such as Radar's ability to measure radial velocity versus RGB cameras' light intensity perception, create unobservable features for the student, complicating distillation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Proposed Approach} \label{sec:methodology}

We name our distillation framework 4D Multi Teachers (4DMT).
This section begins with an overview on different blocks of our method and how data flows through them.  
Then, the detail of each block is presented.

\subsection{Overview of 4DMT} \label{sec:overall Framework}
Our knowledge distillation scheme named \textit{4DMT}, shown in Fig.\ref {fig.architecture}, consists of $N$ teacher models and a student model.
These models take Radar point clouds as their input.
It worths noticing that each model (teachers and the student) use point clouds having a different density level.
Teacher models and the student model use an identical (but not shared) backbone to compute the feature maps of their input Radar point cloud in BEV.
Their backbone has a multi-stage architecture that progressively increases the semantic level of the feature map at each stage while reducing its spatial resolution.
This is done through a chain of sparse 3D convolutions \cite{spconv} similar to RTNH.
An anchor-based detection head \cite{second} detects objects using the feature map of the last stage.

To make input point clouds for each teacher, we choose a different percentile of power that the preprocessing module extracts from the 4DRT.
Teacher models are trained using the object detection loss of RTNH.
During the training of the student model, teacher models are not subjected to the weight update. 
An Aggregation Module, detailed in Sec.\ref {sec:Aggregationmodule}, combines the output of the backbone of every teacher into a single feature map that plays the role of the target feature map of the student.

Similar to teacher models, we use a different percentile to make input point clouds for the student model.
As the student model is intended to be deployed on resource-constrained hardware and has to satisfy a strict real-time requirement, we choose a high percentile, resulting in very sparse point clouds. 
Beside the backbone and the detection head, the student has a Densify Module, detailed in Sec.\ref {sec:densifymodule}, that upsamples the output of its backbone to alleviate the impact of the sparsity of its input.
The loss function of the student is the weighted sum of the distillation loss, presented in Sec.\ref {sec:loss_function}, and the same detection loss that used to train teacher models.
During inference, the teachers and the aggregation module are removed.

\subsection{Aggregation Module}  \label{sec:Aggregationmodule}
The use of multiple teachers, each of which operates on a different level of point cloud densities, results in a diverse set of features for distillation.
On the other hand, the sparsity of the student's point clouds make certain features to be distilled uncomputable as certain regions of the environment are unobservable to the students.
Therefore, it is necessary to select appropriate teacher features for distillation based on the student's input.
We achieve this with the Aggregation Module.

For a teacher $i$-th, its feature map at every stage is upsampled to obtain the same spatial dimension, then concanated along the channel dimension to make the tensor $\mathbf{F}^{T_i}$.
Aggregation Module adaptively selects the useful features for distillation at each location in BEV from the set of all $\mathbf{F}^{T_i}$ through a series of 1-by-1 convolutions and channel concatenation operation, resulting a unified tensor $\mathbf{F}^{T}$.

\subsection{Densify Module} \label{sec:densifymodule}
The backbone of our models (teachers and the student) use a mix of two implementations of the sparse convolution: \cite{graham2015sparse} and \cite{graham20183d}.
The former allows occupancy leaking from one layer to another by performing the convolution on any location whose neighborhood (defined by the convolution filter) has at least one active (non-zero value) sites.
The result of the occupancy leaking is a reduction in the sparsity level of the feature map in deeper stages of the backbone.

Thanks to the occupancy leaking and a relatively high density of their input point clouds, the last feature map $\mathbf{F}^{T_i}$ of the backbone of each teacher has a relatively high density.
On the other hand, input point clouds of the student have so few points that the last feature map of its backbone is still sparse, despite the occupancy leaking.
As these feature maps are used for knowledge distillation, their density mismatch makes the optimization of the distillation loss more challenging.
Inspired by prior works \cite{sparse2dense, radardistill}, we propose a densify module that upsample the output of the student's backbone.


Our densify module first reduces the resolution of its input using a deformable convolution \cite{deformable} and a ConvNeXt layer \cite{convnext}, where the deformable convolution adapts to irregular inputs, and the ConvNeXt layer efficiently refines the extracted features. It then restores the resolution with a transposed convolution, producing a denser feature map better aligned with the teacher's representation for improved knowledge distillation and object detection. 
The output of this module, referred to as $\mathbf{F}^S$, is used to compute the distillation loss and to detect objects.

\subsection{Loss Function} \label{sec:loss_function}
The loss function $L_{total}$ used to train the student model is a weighted sum of the object detection loss $\L_{detect}$ and the knowledge distillation loss $L_{distill}$

\begin{equation}
    L_{total} = \alpha\cdot L_{detect} + \beta\cdot L_{distill}
\end{equation}

Here, $\alpha$ and $\beta$ are hyperparameters that adjust the balance between the object detection loss and the knowledge distillation loss.
They are set to 1 in our experiments.

$L_{distill}$ is the Mean Squared Error (MSE) between the aggregation of teachers' feature map $\mathbf{F}^T$ and the student's densified feature map $\mathbf{F}^S$.
Since features in regions where objects exist are more important for detecting objects than background, we mask both $\mathbf{F}^T$ and $\mathbf{F}^S$ using ground truth when calculating the distillation loss.
Specifically, we transform each ground truth object into a 2D Gaussian and splat it to the BEV to obtain a heatmap. 
This heatmap is multiplied element-wise with $\mathbf{F}^T$ and $\mathbf{F}^S$ to realize the masking operation.

\begin{equation}
    L_{distill} = \text{MSE}(\text{mask}(\mathbf{F}^T), \text{mask}(\mathbf{F}^S))
\end{equation}

We use the loss function defined by RTNH \cite{kradar} as the detection loss $L_{detect}$.


\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}
\subsubsection{Implementation Details}
Due to the limitation of our hardware, we set the number of teachers $N$ to 3.
To generate point clouds for training teacher models, we set the percentile $r$ of Alg.\ref{alg:preproc} to 3 following values: 95, 90, and 80.
These choices of the percentile is to make the average number of points in point clouds of teachers have the order of $10^4$, resulting in a high precision.
In contrast, the student uses a 99.9 percentile, reducing the point count by an order of magnitude.

We use the same architecture for the backbone of teacher models and the student model.
The backbone has three stages.
Each stage halves the spatial dimensions (width and height) while doubling the number of channels.
The number of channels of the feature map at the output of each stage is respectively 64, 128, and 256.

To make the output $\mathbf{F}^{T_i}$ of the backbone of a teacher model, the outputs from its backbone's second and third stages are upsampled by the transposed convolution with 256 filters to match the spatial dimensions of the first stage's output.
The number of channels in the first stage's output is also increased to 256 using a 1-by-1 convolution.
Finally, these three feature maps are concatenated along the channel dimensions to obtain $\mathbf{F}^{T_i}$.

To make the output $\mathbf{F}^{S}$ of the student model, the Densify Module first increases the number of channels of the output of the student's backbone to 256 while reducing its spatial dimension by 4 times.
The resulting feature map is passed through a stack of three ConvNeXt \cite{convnext} blocks. 
The number of convolution filters of these ConvNeXt blocks are 256.
Finally, we use transposed convolutions to upsample the output of the ConvNeXt stack to the same size as the output of the backbone.

Teacher models and the student model are trained in the same setting as the baseline RTNH \cite{kradar}. 
The training epoch, learning rate, and batch size are 30, 0.001, and 8, respectively. 
All experiments were conducted on an NVIDIA RTX 3090 GPU.
In the tables presented in this section, the best results are marked by bold font.

\subsubsection{Datasets and Evaluation Metrics} 
We conduct our experiments on the K-Radar dataset \cite{kradar}, a multi-modal large-scale dataset collected under various driving environments, including adverse weather conditions. It provides access to dense 4DRT, which is crucial for our approach. K-Radar contains a total of 35,000 frames, equally split into training and testing sets, and includes 100,000 3D bounding box annotations. The annotations cover five categories: sedan, bus or truck, pedestrian, motorcycle, and bicycle. Considering the object distribution within the dataset, we focus on two categories: sedan and bus or truck. For the region of interest (RoI), we use ranges of 0 to 72 m along the x-axis, -16 to 16 m along the y-axis, and -2 to 7.6 m along the z-axis. We compute Average Precision (AP) following the protocol provided by the K-Radar benchmark, and evaluate inference speed in FPS to assess the resource efficiency of our model.

\subsection{Impact of Knowledge Distillation}
To showcase the impact of knowledge distillation on the student model, we compare the detection performance and the resource efficiency of our 4DMT, which is an RTNH model enhanced by our knowledge distillation framework, and two original RTNH.
Our 4DMT and one RTNH use point clouds produced at the 99.9th percentile while while the other RTNH baseline uses a lower percentile of 95.
The lower percentile results in point clouds that are, on average, 50 times denser. 
The result presented in Tab.\ref {tab:comparison_AP} shows that our 4DMT achieves the highest 3D AP in the Sedan category, especially 25\% improvement compared to the RTNH baseline with the same density level.

\begin{table}[t!]
    \caption{Comparison of object detection performance (AP) and inference speed on NVIDIA RTX 3090}
    \label{tab:comparison_AP}
    \centering
    \renewcommand{\arraystretch}{1.2} % 행 간격 조정
    \setlength{\tabcolsep}{8pt} % 열 간격 조정
    \begin{tabular}{c|cccc|c}
    \hline \hline
    \multirow{2}{*}{Networks} & \multicolumn{2}{c}{Sedan} & \multicolumn{2}{c|}{Bus or Truck} & \multirow{2}{*}{FPS} \\ \cline{2-5}
     & BEV & 3D & BEV & 3D &  \\ \hline
    RTNH$_{99.9}$ & 44.36 & 36.11 & 26.78 & 23.09 & \textbf{40} \\ \hline
    RTNH$_{95}$ & 48.08 & 44.08 & \textbf{43.51} & \textbf{30.80} & 25 \\ \hline
    \textbf{4DMT$_{99.9}$} & \textbf{48.33} & \textbf{45.48} & 36.72 & 27.83 & 30 \\ \hline \hline
    \end{tabular}
\end{table}

\begin{figure}[t!]
    \centerline{\includegraphics[width=0.9\columnwidth]{bus_result.png}}
    \caption{Comparison of RTNH$_{95}$ and 4DMT$_{99.9}$  on Bus or Truck class: The sparse Radar point clouds used by 4DMT$_{99.9}$ lead to misinterpreting a single large object as two smaller objects.}
    \label{fig.bus_truck}
\end{figure}


Furthermore, our 4DMT has the second highest inference speed which is 30 FPS on NVIDIA RTX 3090.
These results show that our distillation framework strikes a good balance between precision and efficiency.
The underperformance of our 4DMT compared to RTNH using denser point clouds (95-th percentile) in the "Bus or Truck" category is explained by the misclassification of large vehicles as multiple smaller vehicles due to the sparsity of point clouds at 99.9-th percentile.
This is illustrated in Fig.\ref{fig.bus_truck},

\begin{table}[t!]
    \caption{AP of the student model trained with different number of the teachers}
    \centering
    \label{tab:multi-teacher}
    \renewcommand{\arraystretch}{1.2} % 행 간격 조정
    \setlength{\tabcolsep}{8pt} % 열 간격 조정
    \begin{tabular}{cc|cc}
    \hline \hline
    \multicolumn{2}{c|}{\multirow{2}{*}{Networks}} & \multicolumn{2}{c}{Sedan} \\ \cline{3-4} 
    \multicolumn{2}{c|}{} & BEV & 3D \\ \hline
    \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}One-\\ teacher\end{tabular}}} & Teacher 1 & 47.46 & 44.96 \\
    \multicolumn{1}{c|}{} & Teacher 2 & 46.95 & 43.53 \\
    \multicolumn{1}{c|}{} & Teacher 3 & 47.37 & 44.84 \\ \hline
    \multicolumn{2}{c|}{\textbf{Multi-teacher (1, 2, 3)}} & \textbf{48.33} & \textbf{45.48} \\ \hline \hline
    \end{tabular}
\end{table}

\subsection{Ablation Study} \label{sec:ablation}
\subsubsection{The Impact of Using Multiple Teachers}   
The purpose of this study is to showcase the effectiveness of disilling multiple teachers compared to the conventional single-teacher distillation.
The result in Tab.\ref {tab:multi-teacher} confirms that the performance of the multi-teacher model is higher than that of the single-teacher model.
We hypothesize that the diversity of features for distillation thanks to the available of multiple teachers enables the Aggregation Module to choose features that are feasible for the student to compute.
Therefore, the optimization of the distillation loss is less challenging.



\subsubsection{Analysis of qualitative results}
To qualitatively prove the capacity of densifying sparse point clouds in the latent space of models trained by our knowledge distillation framework, we compared the BEV feature map extracted from the RTNH basic model with the BEV feature map extracted from 4DMT. 
As shown in Fig.\ref {fig.distill}, the BEV feature map extracted from the RTNH model trained with sparse data does not capture information about the object well. In contrast, the BEV feature map of our 4DMT exhibits information about the object well despite its sparse input.

\begin{figure}[t!]
    \centerline{\includegraphics[width=0.95\columnwidth]{Distilled_feature.png}}
    \caption{Comparison of feature heatmaps extracted from RTNH and the proposed 4DMT frameworks.}
    \label{fig.distill}
\end{figure}

\begin{figure*}[h!]
    \centering

    % Row 1: (a) and (b)
    \begin{minipage}{0.495\textwidth}
        \centering
        \includegraphics[width=\linewidth]{source/seq1.png}
    \subcaption{Night, urban, normal}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.495\textwidth}
        \centering
        \includegraphics[width=\linewidth]{source/seq5.png}
    \subcaption{Day, urban, normal}
    \end{minipage}

    \vspace{1em} % Space between rows

    % Row 2: (c) and (d)
    \begin{minipage}{0.495\textwidth}
        \centering
        \includegraphics[width=\linewidth]{source/seq9.png}
    \subcaption{Day, highway, normal}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.495\textwidth}
        \centering
        \includegraphics[width=\linewidth]{source/seq21.png}
    \subcaption{Night, alleyway, rain}
    \end{minipage}

    \vspace{1em} % Space between rows

    % Row 3: (e) and (f)
    \begin{minipage}{0.495\textwidth}
        \centering
        \includegraphics[width=\linewidth]{source/seq29.png}
    \subcaption{Day, mountain, snow}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.495\textwidth}
        \centering
        \includegraphics[width=\linewidth]{source/seq38.png}
    \subcaption{Day, mountain, fog}
    \end{minipage}

    \caption{Visualization of the predictions and feature map of RTNH$_{99.9}$ and 4DMT$_{99.9}$ models accross various  weather conditions. The black boxes represent the ground truth while the red ones indicate detected objects. }
    \label{fig:sample_envs}
\end{figure*} 

By conducting a comparative evaluation of RTNH and 4DMT across diverse driving scenarios (e.g., weather conditions, road environments, lighting), we demonstrate that the proposed framework leverages the characteristics of 4D Radar for more robust object detection under various driving conditions.
As shown in Fig.\ref {fig:sample_envs}, (a), (c), and (d), conventional RTNH fails to extract sufficient feature information from sparse data, leading to a failure in detecting objects in inference results. 
On the other hand, 4DMT utilizes knowledge transferred from multiple teachers to extract object feature information from sparse data, being able to detect objects at a higher precision. 
In Fig.\ref {fig:sample_envs}, (b), (e), and (f), it can be seen that RTNH produces more false positives than 4DMT because it fails to distinguish 4D Radar noise (i.e., side lobe) from measurements corresponding to objects. 
Through quantitative comparisons, we validate that the proposed multi-teacher knowledge distillation framework effectively leverages 4D Radar data to perform robust and accurate object detection in various environments.

\subsubsection{Analysis of 4DMT Performance Based on 4D Radar Data Density}
In this study, we conduct experiments comparing the performance of 4DMT$_{99.9}$, 4DMT$_{99}$, and 4DMT$_{95}$, which use preprocessed data with the percentile of 99.9, 99, and 95, respectively, against RTNH. 
Through this comparison, we demonstrate the effectiveness of the proposed multi-teacher knowledge distillation framework across various data densities. 
As shown in Tab. \ref {tab:student-variation}, 4DMT consistently outperforms RTNH across all data densities in both BEV and 3D AP of Sedan class. This performance gap suggests that the multi-teacher knowledge distillation framework in 4DMT effectively leverages sparse Radar data, leading to more accurate object detection, especially as data density decreases. This demonstrates the robustness and versatility of 4DMT when handling data at various levels of sparsity.

\begin{table}[tb]
    \centering
    \caption{Comparison of Average Precision for 4DMT using student models with varying data densities}
    \label{tab:student-variation}
    \renewcommand{\arraystretch}{1.2} % 행 간격 조정
    \setlength{\tabcolsep}{8pt} % 열 간격 조정
    \begin{tabular}{c|cccccc}
    \hline \hline
    \multicolumn{1}{c|}{\multirow{2}{*}{Networks}} & \multicolumn{2}{l}{99.9 Percentile} & \multicolumn{2}{l}{99 Percentile} & \multicolumn{2}{l}{95 Percentile} \\ \cline{2-7} 
    \multicolumn{1}{c|}{} & BEV & 3D & BEV & 3D & BEV & 3D \\ \hline
    RTNH & 44.36 & 36.11 & 52.43 & 43.20 & 48.08 & 44.08 \\ \hline
    \textbf{4DMT} & \textbf{48.33} & \textbf{45.48} & \textbf{55.00} & \textbf{46.96} & \textbf{55.64} & \textbf{47.48} \\ \hline \hline
    \end{tabular}
    \end{table}

\section{Conclusions}
\label{sec:conclusions}
In conclusion, we have proposed a novel 4D Radar-based multi-teacher knowledge distillation framework that effectively addresses the challenges of data sparsity and real-time performance in 3D object detection. 
By utilizing three teacher models with varying point cloud densities and introducing aggregation and densify modules, our student model learns rich feature representations from sparse input data. The experimental results on the K-Radar benchmark demonstrate a significant 25\% improvement in 3D Average Precision for the Sedan class, showcasing the efficacy of our approach.
% This work contributes to advancing the capabilities of Radar-based perception systems in the context of autonomous vehicles where real-time performance and resource efficiency are critical. In particular, our approach presents a significant direction for utilizing dense 4D Radar tensors within autonomous driving systems, demonstrating how rich Radar data can be effectively leveraged despite computational constraints.

While our framework has shown promising results, we employed a simple preprocessing method to extract point clouds from the dense 4DRT. 
This may limit the potential performance of the model. 
Future work should focus on developing more sophisticated preprocessing techniques that take into account various characteristics of 4D Radars to more effectively select data point from the 4DRT.
% By addressing current limitations and building upon the foundation laid by this study, we believe that more robust and efficient 4D Radar-based perception systems can be developed, ultimately enhancing the safety and reliability of autonomous driving technologies through improved environmental sensing and object detection capabilities.
    
\bibliographystyle{IEEEtran}
\bibliography{ref}

\end{document}
