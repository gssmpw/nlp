\section{Related Works}
\subsection{Radar Processing}
To provide the context of our work, we briefly outline the Radar processing procedure illustrated in Fig.\ref{fig.Radar processing}, which transforms electromagnetic wave responses into Radar point clouds. The process begins by converting the received signal from analog to digital, followed by two Fast Fourier Transforms (FFT) to generate a 4-dimensional tensor (4DRT) containing range, azimuth, elevation, and Doppler measurements, as introduced in Sec.\ref{sec:intro}. Finally, preprocessing (Alg.\ref{alg:preproc}) averages the Doppler dimension and retains a high-percentile subset of measurements, reducing noise while balancing sparsity, as shown in Fig.\ref{fig.percentile_density}.

\begin{figure}[h]
    \centerline{\includegraphics[width=0.99\linewidth]{Radar_preprocessing.png}}
    \caption{
        Radar processing pipeline: Microwave signals are digitized, then transformed via two FFTs into a dense 4D tensor. A preprocessing step then converts it to a sparse point cloud.
    }
    \label{fig.Radar processing}
\end{figure}

\begin{algorithm}
    \DontPrintSemicolon
    \SetNoFillComment
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{
        $\mathbf{4DRT}$ $\in \mathbb{R}^4$ \small(azimuth, range, elevation, Doppler), 
        \\
        percentile $r$ $\in \mathbb{R}$
    }
    \Output{Radar point cloud $\mathbb{P} = \left\{ \mathbf{p}_i = \left[x, y, z, power\right] \right\}$}

    \BlankLine

    \tcc{\scriptsize Power Mapping}
    $\mathbf{power}$ = \textrm{mean}$\left(\mathbf{4DRT}, ~\textrm{dim=Doppler}\right)$  
    
    \BlankLine

    $power\_threshold$ = $r$ percentile of $\mathbf{power}$ 
    
    \BlankLine
    \tcc{\scriptsize Thresholding and Coordinate Transformation}
    
    $\mathbb{P} = \emptyset$

    \For{\footnotesize each discrete coordinate [azimuth, range, elevation] of $\mathbf{power}$}{
        $pw = \mathbf{power}$[azimuth, range, elevation]

        $azi$, $rg$, $ele$ = \text{discrete\_to\_continuous}([azimuth, range, elevation])


        \If{$pw \geq power\_threshold$}{
            $x = rg \cdot \cos(ele) \cdot \cos(azi)$

            $y = rg \cdot \cos(ele) \cdot \sin(azi)$

            $z = eg \cdot \sin(ele)$

            \text{add} $\mathbf{p} = [x, y, z, power]$ to $\mathbb{P}$
        }
    }
    \caption{Converting the 4DRT to a point cloud}
    \label{alg:preproc}
\end{algorithm}

\begin{figure}[ht!]
    \centerline{\includegraphics[width=0.95\linewidth]{various_rpc.png}}
    \caption{
        Comparison of 4D Radar point clouds preprocessed with various percentile $r$: (a) Original output from 4DRT, (b) 95th percentile, (c) 90th percentile, and (d) 80th percentile.
    }
    \label{fig.percentile_density}
\end{figure}

\begin{figure*}[tb]
    \centerline{\includegraphics[width=0.9\linewidth]{Network_Architecture.png}}
    \caption{
        \textbf{4DMT}: a multi-teacher knowledge distillation for efficient Radar-based object detection.
        High density point clouds are used to train teacher models enabling them to achieve high detection performance, while
        sparse point clouds are fed into the student model.
        During training, the student model optimizes a weighted sum of detection loss and distillation loss, learning to detect objects in sparse Radar point clouds while mimicking the fused intermediate feature maps of teacher models.
    }
    \label{fig.architecture}
\end{figure*}

\subsection{Methods Using Raw Radar Data}
As we exploit the dense 4D Radar tensor that have not undergone the preprocessing, our method belongs to the category of methods using raw Radar data.
Since Radar data undergoes a multi-stage processing before yielding sparse point clouds, methods in this category take the ADC signal, or the Range-Doppler Map (RDM), or the 4DRT, as their input.
The early work FFT-RadNet ____ directly detect objects in the RDM.
Since this tensor lacks the explicit azimuth, which is crucial for 3D object detection, FFTRadNet implicitly estimate the azimuth in its latent space by forcing an axis of its feature map to represent the azimuth value.
% This implicit estimation is realized through the optimization of a loss function made of object detection losses and sematic segmentation losses. 
ADCNet ____ and T-FFTRadNet ____ go one step further by using the ADC signal.
Based on a learnable form of the discrete Fourier transform, they devise a learnable signal processing module that transforms the input ADC signal into a latent representation of the range-azimuth-Doppler map.
DPFT ____ takes the 4DRT as its input and project this tensor to the range-azimuth plane and the azimuth-elevation plane for feature extraction.
The two resulting feature maps are fused with the camera's through the deformable attention mechanism ____.

While the methods above use the raw Radar data for both training and inference, our method only requires the raw data for training.
Specifically, we vary the percentile $r$ of the preprocessing to obtain high-density point clouds for training teacher models.
During inference, the student model uses directly the preprocessed sparse point cloud.
This ensures a low memory consumption and a high inference speed of the student model.

\subsection{Distillation Methods}
The idea of the knowledge distillation framework ____ is to use the feature map of a model, referred to as the teacher, to guide the feature computation of another model, referred to as the student.
This is implemented by complemeting the task-specific loss function of the student with addtional terms measuring the similarity between its feature maps and the teacher's.
Prior works on knowledge distillation for Radar-based object detection distill LiDAR-based or camera-based models to leverage their relatively dense representation.
RadarDistill ____ align the representation of a Radar and a LiDAR in BEV.
Then, the Radar model is forced to emulate the representation of the LiDAR model in key regions.
CRKD ____ take the similar key-region-based approach to transfer knowledge from a camera-based model.
A development made by CRKD is the enforcement of the similarity at the global level through the application of the L1 loss on the affinity matrix of the representation of camera and of Radar.

Unlike cross-modal distillation methods, our approach requires no extrinsic calibration between Radar and other modalities, enhancing practicality given the complexity of Radar calibration ____. Moreover, differences in sensing capabilities, such as Radar's ability to measure radial velocity versus RGB cameras' light intensity perception, create unobservable features for the student, complicating distillation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%