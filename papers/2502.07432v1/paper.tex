\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}
% \usepackage{todonotes}
\usepackage{jmlr2e}
\usepackage{booktabs}
\usepackage{paralist}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
% \jmlrheading{23}{2024}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Author One and Author Two}

% Code formatting
\usepackage{listings}
% \usemintedstyle{friendly}

% Short headings should be running head and authors last names

\ShortHeadings{Gomes et al.}{Gomes et al.}
\firstpageno{1}

\begin{document}

\title{CapyMOA: \\Efficient Machine Learning for Data Streams in Python}

\author{\name Heitor Murilo Gomes \email heitor.gomes@vuw.ac.nz \\
       \addr Victoria University of Wellington, New Zealand
       % University A, City, Country
       \AND
       \name Anton Lee \email anton.lee@vuw.ac.nz \\
       \addr Victoria University of Wellington, New Zealand
       % University A, City, Country
       \AND
       \name Nuwan Gunasekara \email nuwan.gunasekara@hh.se \\
       \addr Hamlstad University, Sweden
       % University B, City, Country}
       \AND
       \name Yibin Sun \email ys388@students.waikato.ac.nz \\
       \addr AI Institute, University of Waikato, New Zealand
       % University B, City, Country}
       \AND
       \name Guilherme Weigert Cassales \email guilherme.cassales@waikato.ac.nz \\
       \addr AI Institute, University of Waikato, New Zealand
       % University B, City, Country}
       \AND
       \name Justin Liu \email justin.l@waikato.ac.nz \\
       \addr AI Institute, University of Waikato, New Zealand
       % University B, City, Country}
       \AND
       \name Marco Heyden \email marco.heyden@kit.edu \\
       \addr Karlsruhe Institute of Technology, Germany
       % University B, City, Country}
       \AND
       \name Vitor Cerqueira \email vcerqueira@fe.up.pt \\
       \addr University of Porto, Portugal
       % University B, City, Country}
       \AND
       \name Maroua Bahri \email maroua.bahri@lip6.fr \\
       \addr Sorbonne Université, France
       % University B, City, Country}
       \AND
       \name Yun Sing Koh \email y.koh@auckland.ac.nz \\
       \addr University of Auckland, New Zealand
       % University B, City, Country}
       \AND
       \name Bernhard Pfahringer \email bernhard@waikato.ac.nz \\
       \addr AI Institute, University of Waikato, New Zealand
       % University B, City, Country}
       \AND
       \name Albert Bifet \email abifet@waikato.ac.nz \\
       \addr AI Institute, University of Waikato, New Zealand
       % University B, City, Country}
}
\editor{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
% \blindtext
CapyMOA is an open-source library designed for efficient machine learning on streaming data. It provides a structured framework for real-time learning and evaluation, featuring a flexible data representation. CapyMOA includes an extensible architecture that allows integration with external frameworks such as MOA and PyTorch, facilitating hybrid learning approaches that combine traditional online algorithms with deep learning techniques. By emphasizing adaptability, scalability, and usability, CapyMOA allows researchers and practitioners to tackle dynamic learning challenges across various domains.
\end{abstract}

\begin{keywords}
   Data Streams, Stream Learning, Machine Learning, Python, Java, Online Learning, Data Stream Mining
\end{keywords}

\section{Introduction}
% Overview of CapyMOA’s purpose and contribution
CapyMOA is an open-source library (BSD 3-Clause License) designed for efficient machine learning on streaming data. It provides a structured and extensible framework that enables real-time model updates while ensuring \textbf{efficiency}, \textbf{interoperability}, and \textbf{accessibility}. By offering a diverse set of algorithms and tools tailored for streaming data, CapyMOA facilitates research and practical applications. CapyMOA supports a wide range of online use cases, including classification, regression, anomaly detection, concept drift detection, clusering, semi-supervised learning, AutoML, online pipelines, and more. 
CapyMOA goes beyond existing libraries by offering a high-level modern API in Python coupled with efficient implementations of algorithms in Java. 

\section{Background and Related Work}
% Challenges in stream learning and why existing solutions are not sufficient
Machine learning for data streams presents unique challenges that distinguish it from traditional batch learning. Unlike static datasets, streaming data arrives continuously and must be processed incrementally without storing the entire dataset. 
% This requires models that are both \textbf{computationally efficient} and \textbf{adaptable} to changing data distributions. 
Two key challenges in such scenarios are \textbf{concept drift}, where the statistical properties of the data change over time, requiring models to adjust dynamically to maintain predictive performance, and \textbf{computational constraints}, as models must process each instance upon arrival while operating under strict memory and processing limitations. To effectively address these challenges, practitioners and researchers need specialized tools for evaluating, analyzing, and experimenting with various algorithms.

% Discussion of existing frameworks and their limitations
Several frameworks have been developed to address online learning~\footnote{The terms online learning and stream learning are often used interchangeably, but they are not strictly identical. Online learning generally refers to models that update incrementally, instance by instance, without storing past data. Stream learning further emphasizes the dynamic nature of data streams, including challenges such as concept drift and strict computational constraints. In this paper, we use the terms contextually depending on the focus of discussion.}, the most popular ones being MOA~\citep{ref_moa}, River~\citep{ref_river} and Scikit-Multiflow~\citep{ref_skmf}. 
% the most popular ones are 
% \begin{inparaitem}
%     \item MOA \citep{ref_moa},
%     \item River~\citep{ref_river}, and
%     \item Scikit-Multiflow~\citep{ref_skmf}.
% \end{inparaitem}
% MOA \citep{ref_moa}, River~\citep{ref_river}, and Scikit-Multiflow~\citep{ref_skmf}. 

\textbf{MOA} is a well-established Java-based framework with a large range of efficient implementations, but its Java-based code limits accessibility for new users that often prefer Python. \textbf{River} is a fully Python-based framework that focuses on usability and flexibility but sacrifices computational efficiency due to its pure Python implementation. \textbf{Scikit-Multiflow} was initially developed to provide Python implementations of MOA algorithms but has since been largely replaced by River. In summary, while MOA remains the most efficient choice for large-scale stream learning, it lacks the accessibility of a Python-native tool, whereas River prioritizes usability but struggles with performance, highlighting a trade-off between efficiency and accessibility in existing frameworks. 

% Summary of CapyMOA’s key advantages (efficiency, interoperability, accessibility)
CapyMOA addresses these limitations by providing an efficient, extensible, and user-friendly framework for stream learning. It is built upon three core pillars: 
% \textbf{Efficiency}, taking advantage of efficient Java implementations through a modern Python with minimal memory overhead; \textbf{Interoperability}, with support for both native Python implementations and seamless access to algorithms from MOA and PyTorch, facilitating hybrid learning approaches; and \textbf{Accessibility}, through an intuitive Python API that simplifies stream learning tasks such as handling concept drift, configuring online pipelines, and evaluating models incrementally.
\textbf{Efficiency}, leveraging optimized Java implementations with minimal memory overhead in a modern Python framework; \textbf{Interoperability}, supporting both native Python implementations and seamless integration with MOA and PyTorch for hybrid learning; and \textbf{Accessibility}, providing an intuitive Python API that simplifies complex stream learning tasks, such as concept drift detection, simulation, and evaluation. 

% Candidate for removal if space is short
% By bridging the gap between performance and usability, CapyMOA allows researchers and practitioners to build scalable and adaptive machine learning systems for streaming data.


% \begin{itemize}
%     \item \textbf{Efficiency}: CapyMOA is optimized for high-speed processing of streaming data, ensuring minimal memory footprint and fast execution time.
%     \item \textbf{Interoperability}: The library provides seamless access to MOA implementations in Java while allowing users to work with Python, integrating easily with other popular machine learning frameworks such as scikit-learn and PyTorch.
%     \item \textbf{Accessibility}: With CapyMOA, users can configure complex experiments with ease. The library offers comprehensive APIs to simulate streams, handle drift, perform classification, and evaluate results in real-time. 
% \end{itemize}


\section{Core Features of CapyMOA}

\subsection{Data Representation}
% Explanation of Schemas and Instances in CapyMOA.
CapyMOA provides a structured approach to handling data streams through its \textbf{Schema} and \textbf{Instance} representations. A \textbf{Schema} defines the structure of a data stream, specifying the number of attributes, their types (nominal or numeric), and the target variable. This maintains consistency across streams and prevents runtime errors. An \textbf{Instance} represents a single data point within a stream, adhering to a predefined Schema that may evolve during execution. Unlike \textbf{River}, which uses a flexible but potentially ambiguous dictionary-based representation, CapyMOA enforces a structured Schema to minimize silent errors caused by inconsistent feature representations. This explicit design enhances stability, making CapyMOA particularly well-suited for long-term stream learning tasks.
% This ensures consistency across different streams and prevents runtime errors. An \textbf{Instance} represents a single data point in a stream, conforming to a predefined Schema, which can change throughout execution. Compared to frameworks like \textbf{River} and \textbf{Scikit-Multiflow}, CapyMOA enforces a more explicit data schema, reducing the risk of silent errors due to inconsistent feature representations. River adopts a dictionary-based approach, which offers flexibility but can introduce ambiguities in cases such as missing features or missing values. In contrast, CapyMOA's structured Schema ensures stability, making it particularly suited for long-term stream learning tasks.

% CapyMOA simplifies instance handling by offering a Python-native representation, abstracting the complexities of MOA's internal Instance format.

% Handling static datasets (CSV, ARFF) vs. synthetic data streams.
CapyMOA supports both \textbf{snapshots of streams}, i.e., datasets in either CSV or ARFF formats, and \textbf{synthetic data streams} generated in real-time. 
% While ARFF files provide explicit metadata about feature types, CSV files require manual specification, which CapyMOA supports through a simple type inference mechanism. 
Synthetic streams enable controlled experimentation, particularly for evaluating concept drift adaptation.

% Comparison with other representations: e.g., River and Scikit-Multiflow.

Finally, when processing data with MOA, CapyMOA fully abstracts MOA’s internal representations, eliminating the need for users to interact with Java-based structures. This ensures seamless integration with Python while maintaining a consistent evaluation framework and interoperability across implementations.
% Importantly, when processing data using MOA, CapyMOA completely abstracts MOA’s internal representations, ensuring users do not need to interact with Java-based structures. This allows seamless integration between Python and MOA while maintaining a consistent evaluation framework and interoperability across implementations.


% Removed this subsection for now, it seems a bit redundant with what was said already
% \subsection{Stream Processing and Adaptation}
% % Continuous learning and real-time model updates.
% CapyMOA is built for \textbf{incremental learning}, where models continuously update as new data arrives. As a result, algorithms in CapyMOA are expected to refine models without reprocessing past instances. 
% % Unlike batch learning, where retraining occurs on fixed datasets, most algorithms in CapyMOA are expected to refine models incrementally without reprocessing past instances. 

% % How CapyMOA handles concept drift.
% A key feature of CapyMOA is API for concept drift simulation, detection and evaluation. CapyMOA provides built-in classic drift detectors, such as \textbf{DDM (Drift Detection Method)} and \textbf{ADWIN (Adaptive Windowing)}, and more recent methods that can operate on input data and potentially detect drifts in an unsupervised way like \textbf{ABCD}. 

% % Performance optimizations for low-latency learning.
% To support high-speed data streams, CapyMOA incorporates leverages \textbf{MOA}'s optimized Java implementations whenever possible for computationally intensive tasks while maintaining a Python-friendly interface. These tasks involve algorithms and evaluation, while the end user may not perceive the code is being execute in the JVM. There is little overhead in terms of memory access between Java and Python due to JPype capabilities as illustrated in the experiments in Section \ref{sec:benchmark}.


\subsection{Pipelines}
Pipelines systematically prepare and process input data for machine learning models, ensuring smooth data flow in streaming environments. In the context of streaming data, pipelines are particularly challenging to design and implement due to the stateful nature of some transformations~\citep{gomes2019machine}, which require continuous updates as new data arrives. Streaming pipelines often involve complex interactions between components. For instance, drift detection mechanisms in a pipeline may broadcast information to multiple downstream components, requiring synchronization and efficient communication.
Pipeline elements represent single processing steps, including transformers (such as feature scaling), classifiers, regressors, and drift detectors.
%Additional content regarding pipeline design
% - pipeline elements represent single processing steps within a pipeline. We so far support several pipeline element types for transformers (such as feature scaling), classifiers, regressors, drift detection, and more. 
% - It is easy to develop ones own pipeline element simply by implementing the required methods in the abstract PipelineElement class
% - Pipelines themselves can serve as pipeline elements, so we can create reusable pipelines that one can plug together to form a larger pipeline
% - In contrast to existing stream learning frameworks, capymoa is extremely flexible in the design of pipelines. E.g., we could even design pipelines in which we incorporate multiple drift detectors at different points of the pipeline (e.g., ABCD to monitor the input distribution and ADWIN to monitor accuracy). Further, we keep it open to the researcher how to react to the detected drifts. 
% - *here i'm not sure it is implemented yet:* One might want to register callback functions with any pipeline element. E.g., a stateful transformer could trigger other elements to tell them that it is ready for operation.

An essential step in any machine learning pipeline is feature transformation or data preprocessing, which encompasses tasks such as the creation of new features, dimensionality reduction, and feature selection~\citep{bahri2021survey}. In CapyMOA, pipelines are designed to handle these challenges effectively, providing a modular and extensible framework. Users can build pipelines with components like incremental feature scalers, drift detectors, and classifiers that adapt in real time. The framework supports advanced configurations, such as setting up listeners for drift detection events and dynamically altering pipeline behavior based on these signals. 
Furthermore, CapyMOA provides high flexibility in pipeline design. Users can define custom pipeline elements by implementing the required methods in the abstract ``PipelineElement'' class, or compose complex integrations such as multiple drift detectors at different points (e.g., ABCD for input distribution monitoring and ADWIN for accuracy tracking).


\subsection{Handling Concept Drifts}
CapyMOA provides robust tools for simulating, detecting, and adapting to concept drifts, a crucial challenge in stream learning. \textbf{Concept drift refers to changes in the data distribution over time, which can degrade model performance if not handled appropriately.}

CapyMOA's \texttt{DriftStream} module enables users to simulate various types of concept drifts, such as abrupt and gradual shifts. This allows testing adaptive algorithms like Adaptive Random Forest in realistic dynamic environments. For example, the following code configures a stream with multiple drifts:
% \begin{lstlisting}[language=Python]
% print("Hello, world!")
% \end{lstlisting}

\begin{lstlisting}{python}
from capymoa.stream.generator import SEA
from capymoa.stream.drift import DriftStream, AbruptDrift, GradualDrift
stream = DriftStream(stream=[SEA(1), AbruptDrift(10000), SEA(2), 
                             GradualDrift(start=20000, end=25000)])
\end{lstlisting}

CapyMOA supports \textbf{prequential evaluation}, enabling continuous assessment of model performance on dynamically evolving streams. 

Beyond traditional model assessment, CapyMOA provides multiple drift detection algorithms and a framework for analyzing concept drift effects beyond standard accuracy evaluation, allowing deeper insights into distributional changes.



\subsection{Evaluation}
Evaluating machine learning models in streaming data requires \textbf{continuous assessment} as new data points arrive. Unlike traditional batch learning, where evaluation is performed on fixed datasets, stream learning demands \textbf{incremental evaluation} methods to track performance over time. CapyMOA provides two primary evaluation frameworks: \textbf{cumulative evaluation}, where metrics are updated using all instances seen so far to provide a global perspective on model performance, and \textbf{windowed evaluation}, where metrics are computed over a sliding window of recent instances to offer a more localized assessment that adapts to changes in the data distribution.

For tasks such as classification, regression, semi-supervised learning, and anomaly detection, CapyMOA employs a unified evaluation framework, ensuring a consistent and simple implementation across different learning paradigms. However, \textbf{clustering evaluation} requires a distinct approach due to its computational complexity. Unlike classification and regression, clustering metrics often involve pairwise comparisons or density-based assessments, making cumulative evaluation less practical. Instead, CapyMOA follows established methodologies in the literature~\citep{cao2006density,iglesias2024temporal} to optimize clustering evaluation for stream-based settings. 
\cite{cao2006density} introduced the most widely used evaluation model, which assesses the current clustering results using ground-truth-based metrics within a future window. In contrast, \cite{iglesias2024temporal} proposed a current-window-based evaluation that derives temporal metrics from instances within the window and compares the algorithm’s clustering results with those obtained directly from the same window.


\section{Benchmarking} \label{sec:benchmark}
In this section, we benchmark the same algorithms implemented using MOA, CapyMOA, River and Scikit-multiflow. 
The experiments were conducted using four popular learning algorithms, i.e. Hoeffding Trees~\cite{ref_ht}, Extremely Fast Decision Tree (EFDT)~\citep{ref_efdt},  Adaptive Random Forest (ARF)~\citep{ref_arf} and Streaming Random Patches (SRP)~\citep{ref_srp}. The data used was synthetically generated using the Hyperplane generator from MOA and contains a hundred thousand instances. ARF and SRP are ensembles and evaluated using 10 and 30 ensemble sizes. 
The machine used had the following specifications: \textbf{CPU:} Intel Core i5-11500, 6 cores (up to 4.6 GHz), \textbf{Memory:} 32GB DDR4.
% \textbf{Model:} Dell OptiPlex 7090 (Desktop), \textbf{OS:} Ubuntu 24.04.1 LTS.
    % \item \textbf{Storage:} 512GB NVMe SSD
    % \item \textbf{BIOS:} v1.7.0

In Table~\ref{tab:basic}, accuracy variations arise from differences in framework implementations and the inherent non-determinism of certain algorithms, such as ARF and SRP. Larger discrepancies, such as those observed in EFDT and SRP, where CapyMOA and MOA consistently outperform River and Scikit-Multiflow, stem from more intricate design differences. Notably, MOA's EFDT and SRP implementations were developed by the original authors of these algorithms. 
% , ensuring closer adherence to its intended design, whereas other frameworks may introduce variations. 
Although CapyMOA incurs a small runtime overhead compared to MOA, some exceptions are observed in Table~\ref{tab:basic}. This discrepancy arises due to the newly introduced ``EfficientEvaluationLoop'' inside the MOA framework for CapyMOA evaluations, which accelerates the evaluation process. For the ensembles, where the proportion of time allocated to evaluation is significantly smaller, MOA demonstrates a slightly superior execution speed over CapyMOA. 

% \end{inparaitem}
% \input{table}

\begin{table}[!ht]
\centering
\caption{Comparison across four mainstream stream learning platforms for various algorithms on Hyper100K dataset with 10 repetitions.}
\label{tab:basic}
\begin{tabular}{l|lcrr}
\toprule
\textsc{Library}      & \textsc{Learner} & \textsc{\#Ensemble}   & \textsc{WallClock (s)}          & \textsc{Accuracy (\%)}       \\\midrule
CapyMOA          & HoeffdingTree    &  -        & 0.669 ± 0.009       & 88.600 ± 0.000 \\
MOA              & HoeffdingTree    &  -         & 1.134 ± 0.008        & 88.600 ± 0.000 \\
River            & HoeffdingTree    &  -         & 9.655 ± 0.037       & 88.569 ± 0.000 \\
Scikit-Multiflow  & HoeffdingTree   &  -    & 17.951 ± 0.074      & 88.539 ± 0.000 \\\midrule
CapyMOA          & EFDT    &  -   & 0.906 ± 0.012        & 87.972 ± 0.000 \\
MOA              & EFDT    &  -    & 1.128 ± 0.005         & 87.972 ± 0.000 \\
River            & EFDT     &  -   & 34.042 ± 0.083       & 80.507 ± 0.000 \\
Scikit-Multiflow   & EFDT   &  -    & 780.907 ± 6.969    & 78.593 ± 0.000 \\\midrule
% CapyMOA          & KNN    &  -   & 10.308 ± 0.020     & 83.866 ± 0.000 \\
% MOA              & KNN     &  - & 10.122 ± 0.009     & 83.866 ± 0.000 \\
% River            & KNN      &  -    & 250.445 ± 0.278     & 81.762 ± 0.000 \\
% Scikit-Multiflow             & KNN   &  -     & 59.021 ± 0.108      & 81.867 ± 0.000 \\\midrule
CapyMOA          & ARF   &  10    & 16.288 ± 0.108     & 86.244 ± 0.134 \\
MOA              & ARF   &  10   & 13.053 ± 0.313       & 86.244 ± 0.134 \\
River            & ARF    &  10  & 151.863 ± 3.952   & 84.586 ± 0.138 \\
Scikit-Multiflow       & ARF   &  10   & 641.209 ± 18.188  & 85.129 ± 0.186 \\\midrule
CapyMOA          & SRP   &  10    & 15.853 ± 0.717     & 83.639 ± 1.227 \\
MOA              & SRP    &  10    & 12.664 ± 0.709       & 83.639 ± 1.227 \\
River            & SRP    &  10    & 450.473 ± 16.161   & 72.548 ± 0.128 \\
Scikit-Multiflow    & SRP &  10      & 937.836 ± 45.227    & 74.939 ± 0.565 \\\midrule
% CapyMOA          & ARF$_{100}$         & 167.315 ± 5.572   & 204.837 ± 19.729  & 88.063 ± 0.082 \\
% MOA              & ARF$_{100}$         & 124.485 ± 1.064   & 119.999 ± 1.057   & 88.063 ± 0.082 \\

CapyMOA          & ARF   &  30      & 33.302 ± 0.233   & 87.711 ± 0.102 \\
MOA              & ARF   &  30   & 36.507 ± 0.485   & 87.711 ± 0.102 \\
River            & ARF    &  30  & 484.894 ± 14.364   & 87.545 ± 0.108 \\
Scikit-Multiflow    & ARF   &  30    & 1986.213 ± 57.581  & 87.749 ± 0.168 \\\midrule
% CapyMOA          & SRP$_{100}$        & 170.383 ± 6.280    & 89.337 ± 0.179 \\
% MOA              & SRP$_{100}$         & 122.986 ± 1.827   & 89.337 ± 0.179 \\

CapyMOA          & SRP   &  30   & 33.042 ± 0.815   & 87.490 ± 0.359 \\
MOA              & SRP    &  30  & 35.807 ± 0.696     & 87.490 ± 0.359 \\

River            & SRP     &  30   & 1396.019 ± 51.084 & 82.127 ± 0.533 \\
Scikit-Multiflow             & SRP    &  30 & 2807.890 ± 50.419 & 82.626 ± 0.262 \\
\bottomrule
\end{tabular}
\end{table}

The multi-threading functionalities are demonstrated on Table~\ref{tab:multi-thread}. 
MOA and CapyMOA show similar trends, where mini-batching consistently improves execution time while preserving accuracy. 
For OzaBag, multi-threading alone does not provide substantial speedup and, in some cases, increases execution time due to overhead. The reason is that the overhead of managing multiple threads outweighs the reduction in processing time~\footnote{Please refer to~\citep{cassales2021improving} for further details on multi-threading and mini-batch techniques in streaming ensemble learners.}. However, mini-batching significantly reduces the WallClock time, suggesting that batch processing is a more effective optimization strategy for this learner than parallel execution.
% removed to reduce space
% For ARF, multi-threading leads to significant speedups, particularly in CapyMOA, where execution time is reduced from 116.466s to 38.790s with 8 threads. The combination of multi-threading and mini-batching provides further improvements, though with diminishing returns. 

\begin{table}[!ht]
\centering
\caption{Comparison of multithreading and mini-batch functionalities for ARF in MOA and CapyMOA with \textbf{100} base learners on Hyper100K dataset with 10 repetitions.}
\label{tab:multi-thread}
\begin{tabular}{l|lccrr}
\toprule
\textsc{Library}      & \textsc{Learner} & \textsc{Thread} & \textsc{Mini-batch}   & \textsc{WallClock (s)}         & \textsc{Accuracy (\%)}       \\\midrule
% CapyMOA & OzaBag & - & - & 13.243 ± 0.217  &   89.808 ± 0.370 \\
% CapyMOA & OzaBag & 8 & - & 15.515 ± 0.091  &  89.835 ± 0.014 \\
% CapyMOA & OzaBag & 8 & 50 & 8.434 ± 0.085  &  89.813 ± 0.014 \\

% MOA & OzaBag & - & - &  14.122 ± 0.005  &  89.808 ± 0.370 \\
% MOA & OzaBag & 8 & -  &  17.118 ± 0.004  &   89.835 ± 0.014 \\
% MOA & OzaBag & 8 & 50 &  9.127 ± 0.004  &  89.813 ± 0.014 \\\midrule

CapyMOA & ARF & - & - &  116.466 ± 0.673  &  88.063 ± 0.082 \\
CapyMOA & ARF & 8 & -  &  38.790 ± 0.667  &  88.032 ± 0.024 \\
CapyMOA & ARF & 8 & 50 &  38.190 ± 0.112  &  88.040 ± 0.026 \\

MOA & ARF & - & - &  123.599 ± 0.802  &   88.063 ± 0.082 \\
MOA & ARF & 8 & -  &  66.628 ± 1.287  &  88.032 ± 0.024 \\
MOA & ARF & 8 & 50 &  40.531 ± 0.553  &   88.040 ± 0.026 \\

\bottomrule
\end{tabular}
\end{table}






\section{Conclusion}
CapyMOA offers a powerful yet accessible solution for modern stream learning applications. 
Our goal with CapyMOA is to expand its capabilities beyond traditional stream learning by incorporating advances in \textbf{Online Continual Learning (OCL)}. Additionally, future work includes refining drift evaluation, enhancing deep learning integration, and improving clustering evaluation framework. We expect CapyMOA to continue to evolve as a comprehensive tool for researchers and practitioners. 


% Acknowledgements and Disclosure of Funding should go at the end, before appendices and references


% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

% \newpage

% \appendix
% \section{}
% \label{app:theorem}

% % Note: in this sample, the section number is hard-coded in. Following
% % proper LaTeX conventions, it should properly be coded as a reference:

% %In this appendix we prove the following theorem from
% %Section~\ref{sec:textree-generalization}:

% In this appendix we prove the following theorem from
% Section~6.2:

% \noindent
% {\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
% not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
% dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
% which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
% respective empirical mutual information values based on the sample
% $\dataset$. Then
% \[
% 	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
% \]
% with equality only if $u$ is identically 0.} \hfill\BlackBox

% \section{}

% \noindent
% {\bf Proof}. We use the notation:
% \[
% P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
% P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
% \]
% These values represent the (empirical) probabilities of $v$
% taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
% by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

% {\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{paper}

\end{document}
