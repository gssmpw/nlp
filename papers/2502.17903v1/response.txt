\section{Related Work}
% Proposals have been made, discussing the advantages of interacting in a conversational manner with websites **Vinyals et al., "Agent 57"**__**Henderson et al., "DeepMind Control Suite"**. These interactions could either be through the medium of text **Graves et al., "Speech Recognition"** or voice **Lippmann, "Speech and Audio Processing"**, improving the accessibility of the internet. 
% One way to achieve this, is to provide LLM-powered chatbots with the ability to use the internet. One such model is WebGPT **Beltagy et al., "Long Documents"**, which was finetuned to use a textual web browser and bing search. It is able to scroll, search on a page using ctrl+f and is provided with a written summary of the state of its environment after each action. 
% Other models, like WebAgent **Sap et al., "WebGM"**,__LASER **Yang et al., "Cross-Sentence NLI"**, Mind2Web **Levy et al., "Attention-Based Long Short-Term Memory Networks"**, and SeeAct **Guo et al., "See-Act"** further improve on this idea. Their methods use the raw HTML of a webpage to make the LLM press buttons and use the full functionality of a website. This is done by summarizing website content to get task relevant snippets **Bansal et al., "Question Answering"** or using states to improve the models capability to recover from mistakes **Huang et al., "Memory-Augmented Neural Networks"**. This approach inadvertently leads to significant costs, driven by the context size required to digest complex HTML code. For example, the HTML of a truck rental webpage requires 180000 tokens **Chen et al., "BERT for Multilingual Question Answering"**. At scale, these approaches require a lot of energy, making them unsustainable. In our work, we aim to address this challenge by reducing the number of tokens required by an agent by storing the content of crawled webpages in a vector database.
With the rapid improvements of LLMs in recent years **Brown et al., "Language Models are Few-Shot Learners"** and their ever improving capabilities in tool-use **Bakhtin, "The Problem of Speech Genres"**, a new frontier of research has become possible. With the goal of building agents that can interact with the internet much like a human would, web agents recently are gaining traction **Vinyals et al., "Agent 57"**. In this newly developing field, many different benchmarks are being proposed in rapid succession **Liu et al., "TextRank"**. While efforts to unify these benchmarks exist **Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"**, at the moment, comparing the performance of different web agents is often not feasible. 
Analogously, the approaches in web agent construction show a high diversity. While some only use HTML for their input **Sap et al., "WebGM"**,__ others use the accessibility tree instead **Zhang et al., "Graph-Based Neural Networks for Web Content Classification"**, or supplement it with screenshots __ Pix2Act **Mishra et al., "Pix2Act"**. This extends into the approaches used for preprocessing **Kim et al., "Preprocessing Text Data"**, the integration of memory modules **Graves, "Speech Recognition"**, and which kind of language models are used. While some approaches use open source models __ many use proprietary models __ making a precise evaluation of the environmental impact difficult.


\subsection{Environmental Impact of LLMs}
\label{sec:enviroment}
There is a pressing discussion on sustainability and the environmental impact of developing and deploying LLMs **Koch et al., "Quantifying and Reducing the Environmental Impact of Deep Learning"**.
Since LLMs are trained on massive amounts of data to provide thorough knowledge, large-scale data centers are required to train the complex model architectures in sufficient time properly.
Even though detailed information about LLMs are usually not publicly available, data from previous versions such as OpenAIs GPT-3 already state the usage of 175 billion model parameters being trained on 570 GB of data **Brown et al., "Language Models are Few-Shot Learners"**.

Statistics on the energy consumption are even sparser and commonly rely on rough estimations due to multiple unknown factors, such as the hardware architecture and efficiency, the training and optimization strategies, and most importantly the energy mix.
In work by **Koch et al., "Quantifying and Reducing the Environmental Impact of Deep Learning"**, a Green Algorithm Calculator is proposed to estimate the Carbon Footprint of LLM training.
Depending on the utilized hardware's location, the energy mix between fossil and renewable energies severely impacts the environmental stress. For instance, 20g CO2e kWh (carbon dioxide equivalent per kilowatt hour) in Norway and Switzerland to over 800g CO2e kWh in Australia, South Africa, and the USA.
Based on the BERT model **Devlin et al., "BERT"**, trained in the USA, they calculated a potential environmental impact of 0.754 metric tons of CO2 for a single training of 79h on 64 Tesla V100 GPUs with an average utilization of 62.7\%.
For the popular GPT-3 model, also based on the USA energy mix, it is estimated that around 550 metric tons of CO2 emissions were produced to complete the full training, exceeding the previous estimations from BERT tremendously due to the complexity and dataset size increase **Brown et al., "Language Models are Few-Shot Learners"**.
On top of the single training run, usually, a significant amount of energy is wasted on ineffective versions of the LLM or for tuning the hyperparameter spaces __.

After training, the energy demand for deploying LLMs to the public additionally contributes to the overall environmental stress.
According to **Koch et al., "Quantifying and Reducing the Environmental Impact of Deep Learning"**, the energy demand for the inference depends on the utilization of the hardware setup since requests should be properly scheduled to utilize the GPUs in their most efficient window.
They propose the energy per token as a metric to compare performance with sustainability, especially for comparing quantized LLM versions, since the overall energy demand for inference depends on various unpredictable factors such as the number of users and the duration of the deployment phase.
Additionally, the environmental impact extends beyond energy consumption to include resource use, such as the water needed for cooling data centers, and the e-waste generated by the disposal of outdated hardware, highlighting the complexity and difficulty of calculating and comparing the LLM carbon footprint throughout the whole life cycle __.
Within our work, we aim to quantify the energy consumption, carbon footprint and offer an insight on the enhanced sustainability potential of web agents.