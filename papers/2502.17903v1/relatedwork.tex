\section{Related Work}
% Proposals have been made, discussing the advantages of interacting in a conversational manner with websites~\cite{baez2019conversational}. These interactions could either be through the medium of text~\cite{ouaddi2024architecture,benaddi2024towards} or voice~\cite{chitto2020automatic,pucci2023defining}, improving the accessibility of the internet. 
% One way to achieve this, is to provide LLM-powered chatbots with the ability to use the internet. One such model is WebGPT~\cite{nakano2021webgpt}, which was finetuned to use a textual web browser and bing search. It is able to scroll, search on a page using ctrl+f and is provided with a written summary of the state of its environment after each action. 
% Other models, like WebAgent~\cite{gur2023real}, LASER~\cite{ma2023laser}, Mind2Web~\cite{deng2024mind2web} and SeeAct~\cite{zheng2024gpt} further improve on this idea. Their methods use the raw HTML of a webpage to make the LLM press buttons and use the full functionality of a website. This is done by summarizing website content to get task relevant snippets~\cite{gur2023real} or using states to improve the models capability to recover from mistakes~\cite{ma2023laser}. This approach inadvertently leads to significant costs, driven by the context size required to digest complex HTML code. For example, the HTML of a truck rental webpage requires 180000 tokens~\cite{zheng2024gpt}. At scale, these approaches require a lot of energy, making them unsustainable. In our work, we aim to address this challenge by reducing the number of tokens required by an agent by storing the content of crawled webpages in a vector database.
With the rapid improvements of LLMs in recent years~\cite{dubey2024llama} and their ever improving capabilities in tool-use~\cite{dubey2024llama} a new frontier of research has become possible. With the goal of building agents that can interact with the internet much like a human would, web agents recently are gaining traction~\cite{deng2024mind2web, yao2022webshop}. In this newly developing field, many different benchmarks are being proposed in rapid succession~\cite{yao2022webshop,deng2024mind2web,he2024webvoyagerbuildingendtoendweb}. While efforts to unify these benchmarks exist~\cite{dechezelles2024browsergymecosystemwebagent}, at the moment, comparing the performance of different web agents is often not feasible. 
Analogously, the approaches in web agent construction show a high diversity. While some only use HTML for their input~\cite{ma2023laser, deng2024mind2web}, others use the accessibility tree instead~\cite{dechezelles2024browsergymecosystemwebagent} or supplement it with screenshots~\cite{zheng2024gpt} or even use screenshots exclusively, like Pix2Act~\cite{lu2024weblinx}. This extends into the approaches used for preprocessing~\cite{deng2024mind2web,gur2023real}, the integration of memory modules~\cite{ma2023laser} and which kind of language models are used. While some approaches use open source models~\cite{deng2024mind2web,gur2023real} many use proprietary models~\cite{ma2023laser, zheng2024gpt,yang2024agentoccamsimplestrongbaseline,zhang2024webpilotversatileautonomousmultiagent} making a precise evaluation of the environmental impact difficult.


\subsection{Environmental Impact of LLMs}
\label{sec:enviroment}
There is a pressing discussion on sustainability and the environmental impact of developing and deploying LLMs~\cite{bender2021dangers}.
Since LLMs are trained on massive amounts of data to provide thorough knowledge, large-scale data centers are required to train the complex model architectures in sufficient time properly.
Even though detailed information about LLMs are usually not publicly available, data from previous versions such as OpenAIs GPT-3 already state the usage of 175 billion model parameters being trained on 570 GB of data~\cite{brown2020language}.

Statistics on the energy consumption are even sparser and commonly rely on rough estimations due to multiple unknown factors, such as the hardware architecture and efficiency, the training and optimization strategies, and most importantly the energy mix.
In work by ~\citet{lannelongue2021green}, a Green Algorithm Calculator is proposed to estimate the Carbon Footprint of LLM training.
Depending on the utilized hardware's location, the energy mix between fossil and renewable energies severely impacts the environmental stress. For instance, 20g CO2e kWh (carbon dioxide
equivalent per kilowatt hour) in Norway and Switzerland to over 800g CO2e kWh in Australia, South Africa, and the USA.
Based on the BERT model~\cite{devlin2018bert}, trained in the USA, they calculated a potential environmental impact of 0.754 metric tons of CO2 for a single training of 79h on 64 Tesla V100 GPUs with an average utilization of 62.7\%.
For the popular GPT-3 model, also based on the USA energy mix, it is estimated that around 550 metric tons of CO2 emissions were produced to complete the full training, exceeding the previous estimations from BERT tremendously due to the complexity and dataset size increase~\cite{shi2023thinking}.
On top of the single training run, usually, a significant amount of energy is wasted on ineffective versions of the LLM or for tuning the hyperparameter spaces~\cite{Verdecchia2023A}.

After training, the energy demand for deploying LLMs to the public additionally contributes to the overall environmental stress.
According to \citet{Samsi2023From}, the energy demand for the inference depends on the utilization of the hardware setup since requests should be properly scheduled to utilize the GPUs in their most efficient window.
They propose the energy per token as a metric to compare performance with sustainability, especially for comparing quantized LLM versions, since the overall energy demand for inference depends on various unpredictable factors such as the number of users and the duration of the deployment phase.
Additionally, the environmental impact extends beyond energy consumption to include resource use, such as the water needed for cooling data centers, and the e-waste generated by the disposal of outdated hardware, highlighting the complexity and difficulty of calculating and comparing the LLM carbon footprint throughout the whole life cycle~\cite{patterson2021carbon}.
Within our work, we aim to quantify the energy consumption, carbon footprint and offer an insight on the enhanced sustainability potential of web agents.
 
%