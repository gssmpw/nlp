@inproceedings{DBLP:conf/iclr/HuSWALWWC22,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Lu Wang and
                  Weizhu Chen},
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
  url          = {https://openreview.net/forum?id=nZeVKeeFYf9},
  timestamp    = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/HuSWALWWC22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:journals/corr/ChenGS15,
  author       = {Tianqi Chen and
                  Ian J. Goodfellow and
                  Jonathon Shlens},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Net2Net: Accelerating Learning via Knowledge Transfer},
  booktitle    = {4th International Conference on Learning Representations, {ICLR} 2016,
                  San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year         = {2016},
  url          = {http://arxiv.org/abs/1511.05641},
  timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/ChenGS15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2308-12043,
  author       = {Feiyu Zhang and
                  Liangzhi Li and
                  Junhao Chen and
                  Zhouqiang Jiang and
                  Bowen Wang and
                  Yiming Qian},
  title        = {IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient
                  Fine-tuning},
  journal      = {CoRR},
  volume       = {abs/2308.12043},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.12043},
  doi          = {10.48550/ARXIV.2308.12043},
  eprinttype    = {arXiv},
  eprint       = {2308.12043},
  timestamp    = {Tue, 12 Dec 2023 10:32:16 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-12043.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2409-12903,
  author       = {Mohammad Samragh and
                  Seyed{-}Iman Mirzadeh and
                  Keivan Alizadeh{-}Vahid and
                  Fartash Faghri and
                  Minsik Cho and
                  Moin Nabi and
                  Devang Naik and
                  Mehrdad Farajtabar},
  title        = {Scaling Smart: Accelerating Large Language Model Pre-training with
                  Small Model Initialization},
  journal      = {CoRR},
  volume       = {abs/2409.12903},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2409.12903},
  doi          = {10.48550/ARXIV.2409.12903},
  eprinttype    = {arXiv},
  eprint       = {2409.12903},
  timestamp    = {Thu, 17 Oct 2024 12:28:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2409-12903.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v235-liu24bn,
  title = 	 {{D}o{RA}: Weight-Decomposed Low-Rank Adaptation},
  author =       {Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {32100--32121},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/liu24bn/liu24bn.pdf},
  url = 	 {https://proceedings.mlr.press/v235/liu24bn.html},
  abstract = 	 {Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding. The code is available at https://github.com/NVlabs/DoRA.}
}

@misc{wang2024miloraharnessingminorsingular,
      title={MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning}, 
      author={Hanqing Wang and Yixia Li and Shuo Wang and Guanhua Chen and Yun Chen},
      year={2024},
      eprint={2406.09044},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.09044}, 
}

