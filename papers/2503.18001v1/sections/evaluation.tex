%! root=../main.tex
\section{Evaluation}\label{sec:evaluation}

To comprehensively evaluate, \pnameexp we investigate the following research questions using the methodology~\autoref{sec:method} and dataset~\autoref{sec:data}.

\begin{itemize}[noitemsep,topsep=1pt]
	\item{\bf RQ1: Recommendation Accuracy.} Can \pname recommend relevant regions? (\autoref{sec:recs-acc})?
	\item{\bf RQ2: Explanation Accuracy.} Can \pnameexp explain \pname's recommendations ? (\autoref{sec:exp-acc})?
	\item{\bf RQ3: Comparison with \sota \gnn Explainers.} How do the explanations of \pname compare against those of \sota \gnn explainers (\gnne\cite{ying2019gnnexplainer}, \pge\cite{luo2020parameterized}, and \subx\cite{yuan2021explainability}) (\autoref{sec:sota-comp})?
\end{itemize}

\subsection{Methodology}\label{sec:method}

The data processing of \pname begins by extracting features containing user activities (\eg \code{views} and \code{saves}), user and listing attributes, and their geographical regions. During preprocessing, missing values are handled systematically to ensure data integrity. Outliers, which can distort analysis and model training, are addressed by replacing extreme values (detected using Z-scores) with the column mean. These steps ensure a clean and reliable dataset. Additionally, numerical features are normalized using z-score normalization, creating a consistent data representation. These preprocessing steps create a robust and standardized dataset, ready for graph-based modeling in the recommendation system.

After preprocessing, the user, listing, and city data are used to construct a heterogeneous graph. Each entity (\eg user, listing, and city) is represented as a unique node type, and edges capture interactions between these nodes. For instance, user-to-listing edges represent activities such as \code{views} or \code{saves}, while city-to-listing edges denote listings contained within a city. The graph construction also includes bidirectional edges to preserve information flow in both directions, enhancing the \pname's ability to learn relationships. Node features are assigned and this comprehensive graph representation forms the backbone of the recommendation system, enabling the \pname to understand complex relationships.

The training pipeline first constructs the negative graph to ensure a balanced training dataset by mimicking real-world scenarios where users do not interact with all items. We use the Adam optimizer with weighted decay to optimize margin-based loss, where the positive edge scores are encouraged to exceed negative edge scores by at least a margin of 1, penalizing cases where this condition is not met. 

During evaluation, the  \pname's effectiveness was tested using node embeddings derived from the trained model. These embeddings were normalized to enable cosine similarity-based retrieval, crucial for recommendation tasks. For specific canonical edge types (\eg 'user', \code{views}, 'city'), embeddings were computed separately for source (\eg user) and destination nodes (\eg city). The recommendation performance of \pname is measured using nDCG@K, which evaluates the ranking quality of recommendations and prioritizes highly relevant items. Metrics like nDCG@K was used to evaluate recommendation quality, providing quantitative insights.
%into the \pname.

\pnameexp explains the recommendations generated by \pname, and for this experimentation, we \textit{focus} on user-to-city \code{views} relationships, but this works for any relationships. First, we identify an \textit{important} subset of features by measuring the \textit{change in nDCG@K score} when individual features are zeroed out. Then, selecting those that produce a negative impact on nDCG, meaning those features were important for predicting relevant recommendations. To align with the recommendation explainability task, we redefine the traditional Fidelity metric~\cite{yuan2021explainability}—originally based on \textit{change in prediction confidence}—as the \textit{change in nDCG@K score} due to perturbations. The explanation performance of \pnameexp is then evaluated based on the drop in nDCG@K score and the change in cosine similarity ($\Delta \text{sim}$) between user and city embeddings caused by structural perturbations and by only using the \textit{important} features.
  
\heading{Baseline.} We compare \pnameexp against \gnne~\cite{ying2019gnnexplainer}, \subx~\cite{yuan2021explainability} and PaGE-Link~\cite{zhang2023page}, demonstrating superior explanation quality as evidenced by higher fidelity to the ground truth. We also compare \pname against two baselines to validate its effectiveness: \nm{1} random recommendations where recommendations are generated randomly without considering user preferences, and \nm{2} histogram recommendations where a histogram-based method that creates recommendations from user-item interaction summary. \pname outperforms both the baselines in recommendation accuracy as demonstrated by higher nDCG@K. 

\subsection{Dataset}\label{sec:data}

The dataset was collected over a three-day period from the state of Washington in the USA. The dataset statistics are shown in \autoref{tab:summary}, and more details in \autoref{sec:data-stats}. The dataset is split into training, validation, and test sets, ensuring that the test set contains previously unseen pairs, which are evaluated on the following day to avoid any data leakage.

Focusing on the training data some interesting and some expected trends are seen. Expected trends are seen such as view events are the most popular event, followed by save and then toured because users tend to view multiple items before saving them and finally touring the item. The average interaction of the user to listing for view event for the 25th quantile is 1.0 and the 75th quantile is 8.0, the save event for the 25th quantile is 2.7 and the 75th quantile is 3.0, and the tour event for the 25th quantile is 3.2 and the 75th quantile is 4.0. Therefore, from the average interaction of different events statistics tells us that the graph generated from the dataset will contain more sparse connections than dense connections.

%For explanation, user-city interactions are selected to study feature and structural importance.
The experiments utilize a heterogeneous graph built from user, listing, and city interaction data (shown in \autoref{tab:edge-attr}), where node types encompass multiple attributes of varying data types (shown in \autoref{tab:node-attr}); interested readers can find more details in \autoref{sec:graph-details}. Certain feature values can have valid but outlier values, such as commercial properties with over a thousand bedrooms and bathrooms. In contrast, residential real estate has an average number of bedrooms of 4 and bathrooms of 3.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.99\linewidth]{gnn-perf.pdf}
	\caption{Performance of \pname against different baselines.}
	\label{fig:gnn-perf}
\end{figure}

\subsection{Recommendation Accuracy of \pname}\label{sec:recs-acc}
To investigate \textbf{RQ1}, we compared the performance between the \pname and the baseline recommendation models to reveal \pname consistently and substantially outperforms the rest as seen in \autoref{fig:gnn-perf}. \pname consistently surpasses the histogram model across all values of \textit{K}, with the performance gap widening as \textit{K} increases. Notably, at $K=10$, the \pname achieves an nDCG score of 0.019, a 52\% improvement over the histogram model's 0.0125, demonstrating \pname's ability to produce more accurate and relevant recommendations as the recommendation set grows.

This trend is further emphasized by the steep growth curve of the GNN's performance compared to the relatively linear increase observed for the histogram model, showcasing the \pname's adaptability. Interestingly, even at $K=1$, where only the top recommendation matters, the \pname outperforms the histogram model by a remarkable 200\% (0.0075 vs. 0.0025), highlighting its capacity to prioritize the most relevant results effectively. These findings suggest that the \pname's structural representation learning captures nuanced relationships in the data that the histogram model, with its simpler statistical approach, fails to address.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.99\linewidth]{figures/zeroed_out2.pdf}
	\caption{Impact of zeroing out features to find important features.}
	\label{fig:gnn-feat1}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.90\linewidth]{figures/ablation.pdf}
	\caption{Impact of sequentially adding important features on nDCG.}
	\label{fig:gnn-feat2}
\end{figure}

\subsection{Explanation Quality of \pnameexp}\label{sec:exp-acc}

For investigating \textbf{RQ2}, we first consider the feature perturbation quality and show that we can identify the top ten important city features for the recommendation by measuring the difference between the original nDCG and the perturbed nDCG when a feature is zeroed out. When the difference is positive, the feature was important for predicting the recommendation, as shown in \autoref{fig:gnn-feat1}, and features with negative differences mean they were confusing the model (more details in \autoref{sec:unimp-feat}). In particular, these important features can be grouped: \nm{1} geographic - latitude (top right and bottom left) and longitude (bottom left), \nm{2} boolean - view, spa, and vacant land, and \nm{3} numeric - year, population count, days on the market, and square feet. As illustrated in \autoref{fig:gnn-feat2}, using only the important features yield better performance than using all the features, demonstrating that \pnameexp{} effectively identifies the important features needed for user-city recommendation. These top features will be the only features used for measuring the structural perturbation quality. 

The important city features align with domain knowledge: a city is typically characterized by its geographic location and aggregate properties (\eg population count, the average year of houses, and average days on the market), which are different from the features usually used to characterize a listing (\eg the number of bathrooms and bedrooms). Aggregate properties offer a holistic snapshot of a city's dynamics and freshness. Interestingly, cities with a younger infrastructure experience rapid growth, indicating a strong interplay between user preference and given recommendations. Moreover, as shown in \autoref{fig:gnn-feat3}, we notice the unimportant features that confuse \pname are listing features—removing them leads to improved performance.

In \autoref{fig:gnn-feat1}, the boolean feature \texttt{view} yields the best recommendation performance. Moreover, combining the top fourteen features results in performance similar to the single boolean feature. Incorporating additional features is necessary because relying solely on one boolean feature would compromise the model’s generalizability and robustness. Furthermore, considering a diverse set of features, \pnameexp{} does not overfit its explanation to a single scenario.

\input{tables/expcomp.tex}

\autoref{tab:comparison-metrics} quantitatively evaluates the performance of \pnameexp against \sota \gnn explainers, where \pnameexp outperforms \plink, \gnne, and \subx in both nDCG and cosine similarity metrics, demonstrating its superior explanation accuracy compared with the ground-truth. In the ground-truth, the edges in the real-world dataset are treated as positive, and other edges are treated as negative. It is seen that \pnameexp achieves the most significant reduction in nDCG (94\%), indicating superior fidelity in explanation degradation. Additionally, it achieves the largest decrease in cosine similarity (-0.10), highlighting its effectiveness in finding the important subgraphs that alter \pname's recommendations. 
%These results validate the effectiveness of \pnameexp in providing more interpretable explanations compared to existing methods.

\subsection{\sota \gnn Explainers vs. \pnameexp}\label{sec:sota-comp}
We compared \pnameexp against a specialized \gnn-based recommendation explainer \plink~\cite{zhang2023page} and two general \gnn explainers, \gnne~\cite{ying2019gnnexplainer} and \subx~\cite{yuan2021explainability} in \autoref{tab:comparison-metrics} to answer \textbf{RQ3}. \pnameexp performed the best, followed by \plink and general purpose \gnn explainers performed the worst which is expected since \pnameexp and \plink have been specifically designed for \lp tasks. Compared to \gnne, \pnameexp demonstrates a -73\% more reduction in nDCG and -0.08 more cosine similarity increase, indicating that it more effectively identifies the features and subgraphs the \gnn relies on to make recommendations. Similarly, against \subx, \pnameexp shows greater degradation in both metrics. Compared to \plink, \pnameexp the comparison performance are relatively closer, showing \plink is better than both \subx and \gnne. \pnameexp has 13\% more decrease in nDCG is because \pnameexp considers the entire graph and all the available features as context and since \plink is limited by the ego-graph size, it is unable to consider some of the relevant features that are important to the \gnn. This limitation also impacts \plink in identifying relevant subgraph structures which is reflected by -0.03 less decrease is cosine similarity. These results highlight \pnameexp's distinct advantages over existing explainers in altering model behaviors and disrupting reliance on original explanations.

% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=0.80\linewidth]{exp1.png}
%         \caption{\pnameexp's explanation of a recommended city \#1.}
%         \label{fig:exp1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=0.80\linewidth]{exp2.png}
%         \caption{\pnameexp's explanation of a recommended city \#2.}
%         \label{fig:exp2}
%     \end{subfigure}
%     \caption{\pnameexp's explanations for two recommended cities.}
%     \label{fig:sidebyside}
% \end{figure*}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.70\linewidth]{exp1.png}
    \caption{\pnameexp's explanation of a recommended city \#1.}
    \label{fig:exp1}
    % \caption{\pnameexp's explanations for two recommended cities.}
    \label{fig:sidebyside}
\end{figure}

\subsection{Case Study}
In the case study, we selected a real user and based on their interactions generated city recommendations and explanations for those recommendations.
%Similar to \autoref{sec:exp-acc}, we first selected the important features and used them for structural perturbation for explanation.
To simplify the visualization, we compress the graph to only the specified user (yellow node) and their co-clicked cities (blue nodes). By co-clicked cities, we mean cities the current user clicked listings in as well as cities other users clicked listings in where they share at least one common city with the current user. A blue edge will exist between the co-clicked city nodes, the recommended city is the green node, and the red edges are the ones we have identified as important.
% \begin{figure}[h!]
% 	\centering
% 	\includegraphics[width=0.95\linewidth]{exp1.png}
% 	\caption{\pnameexp's explanation of an recommended city \#1.}
% 	\label{fig:exp1}
% \end{figure}
% \heading{Explanation of an Recommended City \#1.}
% \kunal{Zach can you write a descripton of co-clicked city graph and how it is related to interaction graph. Can you add some sentences related to the features of the recommended city to the user's co-clicked city and what similarioites it show. Maybe you can write some sentences about the explanations given in terms of co-clicked graph structure and what it means in terms of the interaction graphs as well.}
% In this case study, we present \pnameexp's explanation for a city recommendation made to a real user. 
% \pnameexp identifies key features and employs structural perturbation to generate explanations.
To enhance visualization, we present a simplified graph (\autoref{fig:exp1}) focusing on the target user (yellow node), co-clicked cities (blue nodes), and the recommended city (green node). A \textit{co-clicked city} indicates that the user interacted with listings in both cities, implying a potential relationship. Red edges represent the connections identified by \pnameexp as most influential in the recommendation.

\autoref{fig:exp1} illustrates that the user has direct connections to a limited number of co-clicked cities. However, one of these cities is highly connected to other cities, including the recommended city. These highly connected cities act as information hubs, facilitating the flow of information between the user and the recommended city. This information flow occurs through two mechanisms: \nm{1} shared preferences, users who co-click on listings in the hub city exhibit similar preferences to the target user, making the recommended city more relevant, and \nm{2} indirect connections, the hub city connects the user to a broader network of cities with relevant listings, increasing the likelihood of discovering the recommended city. Consequently, \pnameexp identifies the edges connecting the user to these hub cities as critical to the recommendation, as their removal significantly impacts the information flow and potentially disrupts the discovery of relevant recommendations.

% \begin{figure}[h!]
% 	\centering
% 	\includegraphics[width=0.95\linewidth]{exp2.png}
% 	\caption{\pnameexp's explanation of an recommended city \#2.}
% 	\label{fig:exp2}
% \end{figure}

% \heading{Explanation of an Recommended City \#2.}
% \kunal{same things like above}
% The second user explanation again shows the importance of highly connected cities. This time, the recommended city is 3-hops away from the user, going through two cities that are acting as information passing hubs. 

% \section{Human Evaluation}
% \zach{Did we do this?} We conducted an human evaluation similar to previous research~\cite{zhang2023page} to understand the effectiveness of \pnameexp since an explainer is supposed to help the human improve their understanding of model's decision making process. We design a survey with single-choice questions. In each question, we show respondents the predicted link with both the graph structure and the node/edge type information but excluding explainer names. The survey is sent to people across an real estate organization where we ask respondents to ``please select the best explanation of '\textit{why the model predicts this user will like the recommended city?}'''. In our evaluation over XX\% of them selected explanations by \pnameexp as the best.