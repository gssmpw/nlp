%! root=../main.tex
\section{Background and Related Works}\label{sec:background}

% \heading{Graph Neural Networks (GNN).}
% Our research utilizes an general-purpose industry-standard \gnn framework~\cite{dgl} to model and explain user interaction graphs, taking advantage of DGL's mature development ecosystem. This approach ensures alignment with current analytical techniques and facilitates streamlined integration into real-world applications. Although the recommendation community has traditionally favored custom recommendation engines\footnote{\gnn have only recently started gaining attention within the community}~\cite{zhang2023page} due to the complexity and heterogeneity of user-item interaction graphs, we used \cite{dgl} for its long-term integration benefits and broader impact.

\heading{\gnn-based Recommendation Framework.}
Graph Neural Networks (GNNs) have emerged as a powerful approach for enhancing recommender systems. Over the past few years, several studies have applied \gnns directly to user-item bipartite graphs, yielding significant improvements in both effectiveness and efficiency \cite{chen2020revisiting,he2020lightgcn,li2019hierarchical,sun2020neighbor,wang2019neural}. One common challenge in this approach lies in capturing higher-order connectivity between nodes. Multi-GCCF \cite{sun2019multi} and DGCF \cite{liu2020deoscillated} address this by introducing artificial edges that connect two-hop neighbors (\eg user-user and item-item graphs) to introduce proximity information into the user-item interaction. 
Node representations are computed layer-by-layer in GNNs, where the overall user and item representations are critical for downstream tasks like recommendation prediction. The most common practice is to adopt the final-layer embeddings as the ultimate representations \cite{li2019hierarchical,ying2018graph}. 
%However, more recent approaches explore integrating information from multiple layers to improve performance further. 
For a comprehensive survey of GNN-based recommender systems, readers are referred to \cite{wu2022graph}.

\gnn-based recommender systems also differ in their node aggregation and update strategies. For example, NGCF \cite{wang2019neural} enhances feature interactions between users and items using an element-wise product operation. NIA-GCN \cite{sun2020neighbor}, on the other hand, introduces pairwise neighborhood aggregation to better capture neighbor relationships. Inspired by GraphSAGE \cite{hamilton2017inductive}, \cite{li2019hierarchical,sun2019multi,ying2018graph} utilize a concatenation operation followed by nonlinear transformations to update node representations. Conversely, LightGCN \cite{he2020lightgcn} and LR-GCCF \cite{chen2020revisiting} simplify the aggregation by removing non-linearities, which enhances both performance and computational efficiency.

\heading{\gnn-based Explainers.}
Recent research in \gnn explainers \cite{ying2019gnnexplainer, luo2020parameterized, yuan2021explainability} has advanced in identifying key nodes, edges, or subgraphs in \gnns. They are categorized into white-box and black-box explainers. White-box methods, \eg \gnne \cite{ying2019gnnexplainer} and \pge \cite{luo2020parameterized}, access \gnn internals, including model weights and gradients. Conversely, black-box methods like \subx \cite{yuan2021explainability} operate on model inputs and outputs, reducing coupling between the explanation framework and model architecture. \gnn explainers encounter exponentially increasing computation time with graph size growth, hindering the interpretability in real-world graphs. 
%This complexity hinders the direct application of explanations to recommendation activity, and generating human-interpretable explanations becomes complex, subsequently impeding the direct translation of explanations to real-world entities.

\heading{\gnn-based Recommendation Explainers.}
The rise of \gnn-based recommender systems has created a pressing need for explainability in these recommendation systems~\cite{zhang2020explainable}. Explainable recommender systems aim to not only deliver accurate predictions but also provide transparent and persuasive justifications for recommendations~\cite{lyu2022knowledge,sinha2002role,wang2022multi,zhang2020explainable}.
%Enhancing explainability can improve user trust~\cite{he2017neural}, customer confidence~\cite{sinha2002role}, and persuasiveness \cite{tintarev2007survey}, while also assisting practitioners in debugging and refining systems~\cite{zhang2020explainable}. 
Prior work on explainable recommender systems adopted these strategies~\cite{zhang2020explainable} of designing intrinsically explainable models with interpretable logic~\cite{zhang2014explicit, he2015trirank} and using post hoc models that generate explanations for the predictions of black-box models~\cite{peake2018explanation}. However, these methods face two key challenges: \nm{1} representing explainable information often requires node attributes and influential subgraphs identification, and \nm{2} reasoning for recommendations relies on domain knowledge~\cite{zhang2020explainable}.

Several explainable AI (XAI) approaches have been proposed, focusing on node or graph classification tasks \cite{duval2021graphsvx,lin2021generative,luo2020parameterized,pope2019explainability}. These methods commonly provide factual explanations in the form of subgraphs deemed relevant for a particular prediction analogous to feature-based XAI methods like LIME \cite{ribeiro2016should} and SHAP \cite{lundberg2017unified}.
%For instance, \cite{lucic2022cf} is the first approach to extract counterfactual explanations by identifying perturbations in the input graph that cause a prediction switch. Despite these advancements, existing methods are not directly applicable to GNN-based recommendation systems, whose goal is to explain the rankings of items.
%rather than single predictions. 
~\cite{zhang2023page} was the first work to do a path-based graph neural network explanation for heterogeneous link prediction tasks with the primary limitation that the explanation depends on the ego-graph constructed around the explanation node. Therefore, the explanation technique cannot provide relevant explanations for larger graphs with multiple node attributes (\eg ego-graph size increases exponentially) and large graph diameter. Our approach bridges the gap between general XAI solutions and the unique need for explainable recommendations, addressing transparency challenges and the complexity of graph-structured user-item relationships.












