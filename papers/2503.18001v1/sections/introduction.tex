%! root=../main.tex
\section{Introduction}\label{sec:introduction}

As user interactions in housing marketplaces grow in volume and complexity, personalized recommendation systems are becoming indispensable for improving user experience and satisfaction. Recent advancements in data modeling and recommendation algorithms enable the creation of detailed interaction graphs, that capture nuanced relationships between users and items (\eg property listings and cities). Leveraging the inherent structure of these graphs provides fine-grained insights into user behaviors and preferences, forming a strong foundation for advanced recommendation systems. Building on these insights, various learning-based recommendation systems have been developed and deployed to effectively utilize this data, significantly improving personalization and engagement rates across multiple domains.

In particular, \gnns have emerged as a powerful tool for modeling relational data in recommendation systems. \gnns leverage graph-structured data, where nodes represent entities (such as users, listings, and cities) and edges denote interactions (such as user views, saves, and tours). Through a message-passing mechanism, \gnns aggregate information from neighboring nodes and edges, enabling them to capture complex, multi-hop relationships within the graph. This capability makes \gnns particularly effective for personalized recommendations, as they can identify nuanced patterns in user preferences and item associations.

While \gnn-based recommendation models achieve impressive accuracy, their black-box nature poses significant challenges regarding trust and transparency. Users and system administrators often require explanations for recommendations, particularly in high-value transactions or critical decision-making scenarios. Current \gnn explanation techniques focus on node and graph-level tasks whereas recommendation systems model the recommendation problem as Link Prediction (LP) tasks, as recommending relevant items to users can be framed as predicting links between users and relevant items. Therefore, there is a notable gap in the literature concerning the explanation of \lp tasks. 

%\gnn are frequently employed to solve LP problems such as sponsored search, community discovery, drug discovery, and providing interpretable explanations for \lp could substantially enhance user trust. Despite the widespread use of \gnns for \lp, generating robust explanations for these predictions remains an underexplored area, with most existing methods focusing on node- or graph-level tasks in homogeneous graphs.

Existing \gnn explanation techniques~\cite{ying2019gnnexplainer, luo2020parameterized, yuan2021explainability} primarily focus on generating explanations by either learning a mask to select an edge-induced subgraph or searching for the most informative subgraph. While these methods are effective for node-level and graph-level tasks, they face significant challenges in link-level prediction contexts. For instance, approaches like \cite{ying2019gnnexplainer, luo2020parameterized} often yield disconnected edges or subgraphs, making the resulting explanations challenging to interpret about the predicted link. Furthermore, \cite{yuan2021explainability} suffers from scalability issues as the graph size increases exponentially, leading to computational challenges due to the combinatorial explosion of possible subgraphs. This is particularly problematic since enumerating all subgraph types requires checking for isomorphism, which results in exponential time complexity as the graphâ€™s vertices and edges grow. Customizing these techniques for sparse datasets and heterogeneous graphs introduces further difficulties. Existing methods emphasize dense local subsets of the dataset and are primarily designed for homogeneous graphs, limiting their applicability to more sparse and diverse graph structures.


% Finally, heterogeneity presents an additional challenge: \lp is often performed on heterogeneous graphs, such as recommendation graphs where nodes and edges represent different types of entities and relationships (\eg user$\rightarrow$views$\rightarrow$listing, city$\rightarrow$contains$\rightarrow$listing). Existing explanation methods designed for homogeneous graphs struggle to generalize in these contexts.


Explaining \gnns for \lp introduces three unique challenges (also noted by ~\cite{zhang2023page}): \nm{1} accurate interpretation of substructures in the presence of sparse relations, \nm{2} scalability, and \nm{3} heterogeneity. To address these challenges, we formulate \lp explanation as an instance-level, post-hoc task that generates interpretable ground-truth aware explanations by identifying the most important feature subset and critical subgraphs. Our approach offers both interpretable and scalable explanations while also accounting for the heterogeneity of real-world recommendation systems. This provides a critical advancement in improving the transparency and trustworthiness of \gnn-based recommendation systems.

We propose \pnameexp, an instance-level \gnn-based recommendation explanation framework for \lp tasks using entire heterogeneous graphs. \pnameexp leverages both graph structures and domain knowledge to identify critical structural relationships to provide explanations with better contextual relevance and alignment with ground truth. Our approach involves a two-step collaborative process where we perturb: \nm{1} the features to identify the most important subset, and \nm{2} the graph to determine the key subgraphs.

Our study emphasizes the importance of decoupling the decision-making process using both structural and entire feature sets, and instead focuses on using a subset of features and subgraphs to enhance the interpretability of recommendations. This allows us to create context-aware explanations that align closely with the interaction patterns found in recommendation systems. We stress that our framework, \pnameexp, is designed to provide interpretable insights into \gnn decisions, not to improve recommendation accuracy. 
%By trading off model complexity for interpretability, we can offer explanations that make GNN-based recommendations more transparent and trustworthy.

To demonstrate the effectiveness of \pnameexp, we measure the quality of explanation for recommendations made by \pname, a \gnn-based recommendation engine for heterogeneous interaction graphs. We use a real-world real-estate dataset from Zillow Group, Inc. The dataset is collected from a large-scale recommendation platform, where user interactions are logged across a diverse set of listings and cities over an extended period. Finally, we compare it against \sota \gnn explainers, evaluating its ability to provide relevant explanations in recommendation tasks.  Our evaluation shows that \pnameexp outperforms existing explainers in terms of providing actionable and interpretable insights for both users and system administrators. This work highlights the potential for developing domain-specific features to close the gap between high-performing recommendation systems and their need for transparency.

In summary, the key contributions of our work are:

\begin{itemize}[noitemsep,topsep=1pt] 
    \item To the best of our knowledge, this is the first end-to-end framework that integrates recommendation and instance-level explanation using whole graph structure without disintegrating into paths or subgraphs.
    \item We conduct an extensive analysis of the real-world housing market to uncover structural heuristics that reduce the explanation search space and improve the relevance of recommendation explanations.
    \item We propose a collaborative approach combining feature and structural perturbation, which enhances both the accuracy and diversity of recommendations and is validated through a real-world case study. 
\end{itemize}