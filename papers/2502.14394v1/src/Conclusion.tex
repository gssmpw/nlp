\section{Conclusion \& Future Work}
\label{sec:conclusions}

In this study, we introduced the first multi-domain Portuguese LVI corpus, which includes more than 7 million documents. Leveraging this corpus, we fine-tuned a BERT-based model to create a robust tool for discriminating between European and Brazilian Portuguese. The training strategy leverages delexicalization to mask entities and thematic content in the training set, thereby enhancing the model's ability to generalize. This approach has potential for adaptation to other language variants and languages.

We have identified two key avenues for future work to further enhance the quality and scope of Portuguese LVI. First, the corpus should be expanded to include other less-resourced Portuguese varieties, particularly African Portuguese. Second, it is crucial to explore the impact of the pre-trained model selection, as the language variety on which the model was originally trained may introduce bias into the LVI classifier.

%Finally, we believe it is essential to quantify the effort required to adapt our experimental setup to other Portuguese varieties and European languages. For other European languages, additional steps will be necessary, such as the adoption of different monolingual transformers. A promising starting point for such an endeavor would be the British/American English and Castilian/Argentinian Spanish language pairs. These languages have monolingual BERT models available, and their inclusion in the DSL-TL corpus provides a reliable foundation for evaluation, following our proposed three-step methodology.
