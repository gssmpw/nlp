\section{PtBrVarId Dataset}
\label{sec:create_corpus}

In this section we introduce the PtBrVarId, the first silver-labeled multi-domain Portuguese LVI corpus. This resource resulted from the compilation of open-license corpora from $11$  European (EP) and Brazilian (BP) sources over six domains, namely: \textbf{Journalistic}, \textbf{Legal}, \textbf{Politics}, \textbf{Web}, \textbf{Social Media} and \textbf{Literature}. The following sections describe how the dataset was created. 

\input{tables/data}

\subsection{Corpora Compiled}
\label{subsec:compiling}

Training machine learning and deep learning models requires a robust and well-labeled training corpus. However, manually labeling such a corpus is often laborious, time-consuming and expensive. To address this challenge in our research, we opted for a silver labeling approach. 

In the context of the VID task, silver labeling involves identifying texts where the variety can be inferred with a reasonable degree of confidence based on the  documents metadata. In the following paragraphs we describe the data sources used in each textual domain along with the heuristics that supported the silver-labelling step. It is important to note that we were careful to only use sources that were permissive for academic research.


\begin{description}

    \item[Journalistic] As a source of news corpus we use two resources available at Linguateca~\cite{santos2014corpora}, namely: CETEMPublico~\cite{rocha2000cetempublico} and CETEMFolha. The CETEMPublico corpus contains news articles from the Portuguese newspaper ``Público'' while the CETEMFolha contains news from the Brazilian newspaper ``Folha de São Paulo''. The geographic location of the newspaper is used to label the Portuguese variety.
    
    \item[Literature] The literature domain relies on three data sources that index classics of Portuguese literature: the Gutenberg project\footnote{\url{https://www.gutenberg.org/browse/languages/pt\#a4827}}; the LT-Corpus~\cite{genereux2012large}; and the Brazilian literature corpus\footnote{\url{https://www.kaggle.com/datasets/rtatman/brazilian-portuguese-literature-corpus}}. The author's nationality was used  to label the documents as European or BP.

    \item[Legal] The Brazilian split from the legal corpora was compiled from RulingBR~\cite{feijo2018rulingbr} which  contains decisions from the Brazilian supreme court (``Supremo Tribunal Federal'') between 2011 to 2018. The European split was built from the DGSI website\footnote{\url{https://www.dgsi.pt}} which provides access to a set of databases of precedents and to the bibliographic reference libraries of the Portuguese Ministry of Justice.

    \item[Politics] For the politics domain we used the manual transcriptions of political speeches in both the European Parliament~\cite{koehn2005europarl} and the Brazilian Senate~\cite{DVN/M9UU09_2020}. The document's origin was used to infer the label for the Portuguese variety.   

    \item[Web] For the web domain, corpora were extracted from OSCAR~\cite{OrtizSuarezSagotRomary2019}. To define the labels, we began by identifying domains ending in \texttt{.pt} or \texttt{.br}. From this list, we manually curated a set of the $50$ most frequent domains ending in \texttt{.pt} and 50 domains ending in \texttt{.br}. The documents from OSCAR associated with these curated domains were then used in our corpus.
    
    \item[Social Media] The social media corpora derives from three data sources. For BP we used the Hate-BR~\cite{vargas-etal-2022-hatebr} dataset, which was manually annotated for train hate speech classifiers, and a compilation of fake news spread in Brazilian WhatsApp groups~\cite{cunha2021fakewhatsapp}. Regarding EP, the tweets collected by~\citet{ramalho2021highlevel} were filtered based on the tweets' metadata location. Tweets whose location is not part of Wikipedia's list of Portuguese cities\footnote{\url{https://en.wikipedia.org/wiki/List_of_towns_in_Portugal}}, were discarded.
    
\end{description}

Despite the dataset proposed being silver-labeled, some of their components are extracted from high-quality manually annotated corpora that offer sufficient guarantees of belonging to a single language variety. For example, the Europarl corpus~\cite{koehn2005europarl}, is composed of manual transcriptions in EP of political speeches made in European Parliament, therefore it is very unlikely to find any marks of BP in such corpus.

\subsection{Data Cleaning}

To reduce noise in the corpus, we implemented a dedicated data cleaning pipeline. The process starts with basic operations to remove null, empty, and duplicate entries. We then employ the \texttt{clean-text} tool\footnote{\url{https://github.com/jfilter/clean-text}} to correct Unicode errors and standardize the text to ASCII format. For the Web domain, an additional step is taken using the \texttt{jusText} Python package\footnote{\url{https://github.com/miso-belica/jusText}} to filter out irrelevant sentences and remove boilerplate HTML code. Finally, outliers within each domain are identified and removed based on the interquartile range (IQR) of token counts, calculated using the \texttt{nltk} word tokenizer for Portuguese\footnote{\url{https://www.nltk.org/api/nltk.tokenize.word_tokenize.html}}. Texts falling below the first quartile minus 1.5 times the IQR, or above the third quartile plus 1.5 times the IQR, are discarded. This approach effectively eliminates documents that are either too short or too long for their respective domains.

Table~\ref{tab:data_statistics} presents the statistics for the corpus obtained after applying the filtering pipeline. The final corpus comprises 7,304,438 documents, predominantly from the EP segments of the Journalistic, Legal, and Social Media domains. Regarding the number of tokens, we observe that, with the exception of the Journalistic domain, the distribution between documents labeled as EP and BP within each domain is similar.

A comparison across the domains reveals that the Web domain contains the highest average number of tokens per document, whereas the Social Media domain has the lowest, averaging around 18 tokens per document. This disparity is significant for the development of variety identification models, as distinguishing between language varieties in shorter texts is more challenging due to the limited linguistic cues available. Therefore, the Social Media domain is expected to pose more difficulties than the Web domain, where longer texts provide more opportunities to identify distinguishing features of EP and BP.

It is also important to note that the dataset is highly unbalanced across all domains except the Web domain. This imbalance should be carefully considered when training models using this dataset to ensure robust and unbiased effectiveness.


\subsection{Quality Assurance}

To ensure the quality of the silver-labeling process, we asked three linguists to manually annotate 300 documents, focusing on two key aspects:

\begin{description}

\item \textbf{Variety} The linguists were asked to determine the variety of the text. They had three options: EP, BP, or ``Undetermined'' for cases where no variety-specific linguistic features were available.

\item \textbf{Domain} The linguists were also tasked with identifying the domain to which each sentence belonged. They could choose from the six domains used in this research, or select ``Undetermined'' if the domain could not be clearly identified.

\end{description}

For the sampling process, we randomly selected 50 documents from each domain in our corpus, with an equal split of 25 documents silver-labeled as EP and 25 as BP. 

Table~\ref{tab:iaa} presents the agreement between the three annotators using three metrics:

\begin{description}

\item \textbf{Fleiss's Kappa}~\cite{fleiss1971measuring}: Measures the agreement between annotators beyond chance, with values ranging from 0 (no agreement) to 1 (perfect agreement).

\item \textbf{Majority Rate}: Indicates the percentage of texts where two out of three annotators agree on an annotation.

\item \textbf{Accuracy}: Assesses how often the majority vote between annotators matches the automatic annotation. It is important to remark that the cases where the labeled agreed by the annotators is ``Undetermined'' we count both silver-labels (EP and BP) as correct since the text is in fact valid in both varieties.

\end{description}


\input{tables/iaa}


The results obtained show that the agreement is higher for the textual domain aspect than for the language variety. However, the variety aspect still achieves a Fleiss' Kappa of 57\%, which, for three annotators with three labels, can be considered moderate agreement. Upon closer inspection of the results, we found that the Fleiss' Kappa is lower in the Literature, Social Media, and Legal domains (see Table~\ref{tab:annotations_detailed}). For the Social Media domain, we found the disagreement to be mainly driven by the short length of the texts, with ``Undetermined'' representing 42\% of the labels the annotators agreed on. The same was found for the legal domain, which has the second lowest average tokens per document, where the ``Undetermined'' represents 34\% of the labels the annotators agreed on. In the Literature domain, the disagreement is mainly attributed to the corpus consisting of contemporary books, which often blend linguistic features from both European and BP, making it difficult to assign a definitive variety label.

In Table~\ref{tab:annotations_detailed} we detail the annotation agreement metrics per domain for the manually label subset of the PtBrVId corpus. The table shows statistics for the Fleiss' Kappa with all the labels and the Fleiss' Kappa when the entries for which one of the annotators marked the entry as ``Undetermined''. To complete the table we also show the percentage of entries for which at least one annotator labeled as ``Undetermined''.  

\input{tables/iaa_domain}

Nevertheless, a majority consensus among the annotators is almost always achievable (over 90\% of the times) in both aspects. Furthermore, this majority is strongly aligned with the automatic annotations, with agreement between the annotators and the silver labels exceeding 75\%.

In addition to releasing the full annotations provided by each annotator, the documents for which a majority vote could be determined are included in the \textit{test} partition of our dataset. For documents labeled as ``Undetermined'' by the annotators, the original silver label was used as the final label. The complete dataset is publicly accessible on HuggingFace\footnote{\url{https://huggingface.co/datasets/liaad/PtBrVId}}. This dataset offers an opportunity for an in-depth study of the cross-domain capabilities of various LVI techniques, with a particular focus on the application of pretrained transformers, which is the main focus of this paper.

