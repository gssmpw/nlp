\section{Results}
\label{sec:results}

\subsection{Impact of Delexicalization}
\label{subsec:hyper}

Figure~\ref{fig:hyperparameters_ngrams} depicts the average $F_1$ scores obtained in the PtBrVid validation set by the N-grams and BERT models, for each ($P_\text{POS}$, $P_\text{NER}$) percentage pair. The averages are computed across models trained in different domains.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\columnwidth]{assets/heatmap.eps}
    \caption{Average $F_1$ score for each ($P_\text{POS}$, $P_\text{NER}$).}
    \label{fig:hyperparameters_ngrams}
\end{figure}


The results suggest that intermediate levels of delexicalization can yield marginal improvements in model effectiveness. However, high levels of $P_\text{POS}$ adversely affect model performance. This finding is particularly interesting because previous studies have reported significant reductions in effectiveness due to delexicalization~\cite{sharoff2010web,lui2014exploring}. Notably, these earlier studies focused solely on full delexicalization and did not evaluate performance on out-of-domain corpora.

Based on these insights, we proceeded to the second step of our training protocol using a delexicalized version of the training set, with ($P_\text{POS}=0.2$, $P_\text{NER}=0.6$) for the N-gram model and ($P_\text{POS}=0.6$, $P_\text{NER}=0.0$) for BERT models.


\subsection{Overall Results}

This section presents the $F_1$ scores for the N-gram baseline and BERT fine-tuning models, comparing their performance with and without delexicalization to highlight their impact on the overall effectiveness of the model.

The results in Figure~\ref{fig:results} underline the benefits of delexicalization on system effectiveness across both benchmarks and models. Specifically, in FRMT, training in the delexicalized corpus improved the $F_1$ score by approximately 13 and 10 percentage points for the N-gram and BERT models, respectively. 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{assets/f1.eps}
    \caption{$F_1$ in FRMT and DSL-TL benchmarks. Models with the subscript $d$ were trained on a delexicalized corpus.}
    \label{fig:results}
\end{figure}

Upon examining the less pronounced discrepancy in the DSL-TL benchmark, we found it to be largely attributed to the FRMT dataset's entity-specific partition, known as the entity bucket. In this bucket, models trained without delexicalization struggle, as they rely on entities to determine language variety. Given that the FRMT dataset contains the same text in both BP and EP, these models often misclassify pairs of sentences by assigning the same label to both, leading to frequent errors. In the extreme case, they end up getting around half of the labels wrong, which is what happened to the N-gram model, only achieving an $F_1$ score of 46.95\% in this benchmark. This highlights the importance of using delexicalization in the training process. To the best of our knowledge, we are the first to report positive results from the use of delexicalization, which was enabled by the proposed cross-domain training protocol. 

When comparing the BERT model with the N-gram models, one can observe that the BERT model outperforms the N-gram model across all scenarios, achieving an $F_1$ score of  84.97\% in DSL-TL and 77.25\% in FRMT. To support further research and exploration, we have made the BERT$_d$ model available on HuggingFace, inviting the research community to use and build on this work\footnote{\url{https://huggingface.co/liaad/PtVId}}.

