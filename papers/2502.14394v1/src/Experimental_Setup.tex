\section{Experimental Setup}
\label{sec:setup}

In this study, we investigate the effectiveness of fine-tuning a transformer-based model for the Portuguese LVI task. We employ an iterative methodology to identify the optimal strategy for combining training corpora from various domains into a unified training process. Our primary objective is to evaluate cross-domain effectiveness and the generalization capabilities of our models.

\subsection{Models \& Baselines}

For the transformer-based model, we use BERTimbau with 334 million parameters~\cite{souza2020bertimbau}. BERTimbau is the result from fine-tuning the original BERT model~\cite{devlin2019bertpretrainingdeepbidirectional} on a Portuguese corpus.

To establish a baseline for comparison with the BERT model, we employ N-grams combined with Naive Bayes classifiers. This choice is motivated by the proven effectiveness of such models in previous LVI studies across various Indo-European languages, including Portuguese~\cite{zampieri2012automatic}.


\subsection{Cross-Domain Training Protocol}
To ensure that our model generalizes effectively across different domains, we define a two-step training protocol. Step one is used to find the best hyperparameters to train the model so ensure the generalization capability of the model. In this step,  the model is trained on a single domain from the PtBrVid corpus and validated on the remaining domains (excluding the one used for training). The hyperparameters yielding the best performance in this cross-domain validation are then used in step two to train the model across all domains combined.

Delexicalization of the corpus is treated as a hyperparameter in our approach. We adjust the probabilities of replacing tokens found by Named Entity Recognition (NER) and Part-of-Speech (POS) tagging with the generic label (such as \texttt{LOCATION} or \texttt{NOUN}), varying these probabilities incrementally from 0\% to 100\% in 20\% steps. It is important to note that delexicalization is applied exclusively to the training set. The validation set remains unaltered, simulating a real-world scenario where the input text is not modified. We leave the study of the impact of delexicalizing the validation set on the effectiveness of the model for future research.


\subsection{Train \& Validation Data}
As referred above the PtBrVId dataset is used to train the models. However, before using for the training, we leave 1,000 documents of each domain for the validation of the model, 500 of each label. 

In the step one of our training protocol, we use 8,000 documents from each domain (4,000 from each label) to train the models. We found this sample size to be enough for the models to converge and ensure fast iteration in the training process.

For step two of our training protocol, we compile all the documents from the PtBrVid corpus including the ones used for validation in step one. To avoid the training being dominated by the more represented domains, we undersample the dataset so that all labels from all domains are equally represented. At this step, the manually annotated set from PtBrVId set is used to keep track of the generalization loss.


\subsection{Benchmarks}

In our evaluation, we use two benchmarks: the DSL-TL and FRMT datasets. As mentioned above, the DSL-TL dataset is the standard benchmark for distinguishing between EP and BP, annotated with three labels: ``EP'', ``BP'', and ``Both''. For our purposes, we exclude documents labeled ``Both'' since our training corpus does not contain that label. This results in a test set comprising 588 documents for BP and 269 for EP. The FRMT dataset~\cite{riley2022frmt} has been manually annotated to evaluate variety-specific translation systems and includes translations in both EP and BP. We adapt this corpus for the VID task, resulting in a dataset containing 5,226 documents, with 2,614 labeled as EP and 2,612 as BP.


\section{Implementation Details}
\label{app:hyper}

NER and POS tags were identified using spaCy\footnote{\url{https://spacy.io/models/pt}}. The BERT model was trained with the \texttt{transformers}\footnote{\url{https://huggingface.co/docs/transformers/}} and \texttt{pytorch}\footnote{\url{https://pytorch.org}} libraries, for a maximum of 30 epochs, using early stopping with a patience of three epochs, binary cross-entropy loss, and the AdamW optimizer. The learning rate was set to $2 \times 10^{-5}$. In addition, a learning rate scheduler was used to reduce the learning rate by a factor of 0.1 if the training loss did not improve for two consecutive epochs. N-gram models were trained using the \texttt{scikit-learn}\footnote{\url{https://scikit-learn.org/}} library. The following hyperparameters were taken into account in the grid search we performed"

\begin{itemize}
    \item \textbf{TF-IDF Max Features:} The number of maximum features extracted using TF-IDF was tested with the following values: 100, 500, 1,000, 5,000, 10,000, 50,000, and 100,000.
    \item \textbf{TF-IDF N-Grams Range:} The range of n-grams used in the TF-IDF was explored with the following configurations: (1,1), (1,2), (1,3), (1,4), (1,5), and (1,10).
    \item \textbf{TF-IDF Lower Case:} The effect of case sensitivity was tested, with the lowercasing of text being either \texttt{True} or \texttt{False}.
    \item \textbf{TF-IDF Analyzer:} The type of analyzer applied in the TF-IDF process was either \texttt{Word} or \texttt{Char}.
\end{itemize}


Regarding computational resources, this study relied on Google Cloud N1 Compute Engines to perform the tuning and training of both the baseline and the BERT architecture. For the baseline, an N1 instance with 192 CPU cores and 1024 GB of RAM was used. For BERT, we used an instance with 16 CPU cores, 30 GB of RAM, and 4x Tesla T4 GPUs. The grid search on N-grams takes approximately three hours under these conditions, while for BERT, it takes approximately 52 hours to complete. The final training took three hours for N-grams and approximately ten hours for BERT.

We have made our codebase open-source\footnote{\url{https://github.com/LIAAD/portuguese_vid}} to promote reproducibility of our results and to encourage further research in this area. % Besides that, more details regarding the training of models can be found on the appendix of the paper.