\section{Related Work}
\label{sec:related_work}

\subsection{Corpora}

Despite the numerous works developed in the LVI task, the first gold-labeled dataset that includes Portuguese corpora, the DSL-TL corpus~\cite{zampieri2023language}, was only introduced in 2023. This dataset used crowdsourcing to annotate approximately 5k Portuguese documents. The corpus are not only labeled as ``European'' and ``Brazilian'' Portuguese, but also a special ``Both or Neither'' label to signal those documents with insufficient linguistic marks to be considered part of one of these varieties. 

Prior to the release of this dataset, the evaluation process was often performed in silver-labeled data, collected using domain-specific heuristics. For instance, in the journalistic domain, it is common to assume the language variety of a document based on the newspaper origin; Brazilian newspapers' articles are assigned a Brazilian Portuguese label, while Portuguese ones are assigned a European Portuguese label~\cite{da2006identification, zampieri2012automatic, tan2014merging}. In the social media domain, a similar approach is frequently used.~\citet{castro2016discriminating} used geographic metadata collected on Twitter to assign a language variety to each document based on the authors location. Unfortunately, many of these Portuguese LVI resources are no longer available online. This limitation motivated us to collect and open-source our training data.


\subsection{Modeling Approaches}
\label{subsec:techniques_used}

The high effectiveness of N-gram-based systems observed in language identification studies~\cite{mcnamee2005language, martins2005language, chew2009optimizing}, a task closely related to LVI, motivated the application of these methods in the context of LVI. To this day, this approaches are still employed, with several submissions to the VarDial workshop\footnote{\url{https://aclanthology.org/venues/vardial/}} -- which compiles most of the recent studies in the LVI task -- achieving high effectiveness. Notable examples include Italian with an $F_1$ score of 0.90~\cite{jauhiainen-etal-2022-italian}, Uralic with an $F_1$ score of 0.94~\cite{bernier-colborne-etal-2021-n}, and Mandarin with an $F_1$ score of 0.91~\cite{yang-xiang-2019-naive}.

The adoption of transformer-based techniques~\cite{vaswani2017attention} in LVI has not been as rapid as in other NLP tasks. Recently, some studies have leveraged monolingual BERT-based models to fine-tune LVI classifiers for Romanian~\cite{zaharia-etal-2020-exploring} and French~\cite{bernier2022transfer}. However, in none of these cases were transformers capable of outperforming N-gram-based techniques, only achieving a $F_1$ score of $0.65$ in Romanian and $0.43$ in French. Similar results have been reported for different languages using other deep learning techniques, such as multilingual transformers~\cite{popa-stefanescu-2020-applying}, feedforward neural networks~\cite{coltekin-rama-2016-discriminating, medvedeva-etal-2017-sparse}, and recurrent networks~\cite{guggilla-2016-discrimination, ccoltekin2018tubingen}.

In the specific case of Portuguese, older studies have relied on N-gram-based techniques to achieve results above $90\%$ accuracy on silver-labeled benchmarks~\cite{da2006identification, zampieri2012automatic, goutte2014nrc, malmasi-dras-2015-language, castro2016discriminating}. However, it has been noted that evaluating on silver-labeled corpora is reliability~\cite{zampieri2014varclass}, and  preliminary results obtained on the gold-labeled DSL-TL corpus~\cite{zampieri2023language} revealed more modest performance, with $F_1$ scores below $70\%$. Additionally, contrary to observations in silver-labeled evaluations~\cite{medvedeva-etal-2017-sparse}, the current state-of-the-art result for Portuguese LVI on the DSL-TL benchmark ($0.79~F_1$ score) comes from fine-tuning a collection of BERT-based models~\cite{vaidya2023two}.

\subsection{Cross-Domain Capabilities}
\label{subsec:Delexicalization}

\citet{lui2011cross} revealed that N-grams based techniques had limited cross-domain capabilities for the language identification task. Despite the good results of N-gram-based models when the train and test domain overlap (above 85\% accuracy), the results also show that the effectiveness decreased as much as 40\% when both sets do not match. In order to address this phenomenon, the authors have devised a feature selection mechanism that later opened the door to the development of the first cross-domain language identification tool, the \texttt{langid.py}~\cite{lui2012langid}.

In the context of French LVI,~\citet{diwersy2014weakly} used unsupervised learning to demonstrate that, despite the good results reported by N-grams based-methods (above 95\% accuracy), the feature learned by these models reveal no interest from a linguistic point of view. Instead, classifiers relied on named entities, polarity and thematics embedded in the training corpus to support its inference process (Ex: If ``Cameroun'' was mentioned in the document, the model assigned a French-Cameroonian label to it). 

In light of these facts, the mass adoption of these architectures in the context of LVI, creates urgency for finding solutions to surpass this limitation. In this study, we extend the knowledge about the cross-domain capabilities of N-gram-based models, while presenting the first results for transformer architectures.
