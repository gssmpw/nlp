\section{Related Work}

\subsection{LoRA and its Variants}
PEFT methods aim to update a small proportion of parameters to adapt LLMs for downstream tasks \cite{hou_adapter,Li_prompt,zaken_bitfit, wu_moslora, hu_lora}.
Among these methods, LoRA \cite{hu_lora}, which injects trainable low-rank branches to approximate the weight updates, has become increasingly popular as introducing no latency during inference.
In the vanilla LoRA method, the authors introduce two linear projection layers and initialize them as Kaiming uniform and zero matrices \cite{hu_lora}. 
The following variants can be categorized into: 1) searching ranks \cite{zhang_adalora}; 2) introducing training skills such as setting different learning rates \cite{Hayou_lora+}; and 3) designing new structures, such as \cite{wu_moslora}.
However, all these variants focus on improving performance for the ideal scenarios without weight noise.
In this paper, we propose HaLoRA, which is customized for hardware deployment.




\subsection{Hybrid CIM Architecture}
Hybrid CIM architectures combine different memory devices to achieve capabilities beyond what pure single-memory-device architectures can offer. Among them, RRAM-SRAM hybrid architectures have attracted significant attention by combining the high energy efficiency of RRAM with accurate computation of SRAM \cite{wen_science,vlsi_minotaur,liu_hardsea,krishnan_hybrid}.
These hybrid designs typically partition computational tasks based on the characteristics of each memory device: deploying high-precision, frequently updated operations on SRAM while allocating computation-intensive yet structurally simple operations to RRAM \cite{wen_science}.
This strategy has facilitated the efficient implementation of convolutional neural networks (CNNs) and lightweight neural architectures \cite{vlsi_minotaur,cnn_cim,chimera}, enabling their deployment in edge computing applications such as robotic localization \cite{slam}, target tracking\cite{tracking}, and recommendation systems\cite{edge_nlp}.
However, existing hybrid architectures primarily focus on implementing small-scale models, with limited exploration of large language models. 
In this work, we explore efficient LLM deployment with hybrid CIM architecture.





\subsection{Robustness Methods against Hardware Non-idealities}

The robustness methods against the RRAM non-idealities have been a hot topic for the past decade.
Specifically, these robust methods can be categorized into 1) noise-aware training, which typically incorporates noise during the training process or introduces robust loss functions \cite{kd_rram, bayes, bayesft}, and 2) hardware compensation strategies, such as mapping critical weights to low-variation areas~\cite{tfix,victor}.
However, these methods mainly focus on the robustness of convolutional neural networks (CNNs) and are hard to generalize to LLMs.
Considering noise-aware training methods, the key is to continuously train the models to improve their robustness including knowledge distillation \cite{kd_rram} and Bayesian neural network training \cite{bayes, bayesft}.
Due to the massive size of the LLM model, such as 3 billion parameters \cite{llama_report}, the cost of continuous training is unaffordable.
Meanwhile, the hardware compensation strategies are impractical for LLMs since pre-testing and correcting each layer through input regularization and column-shared factors introduce substantial additional hardware overhead.
In this paper, we focus on improving the robustness of LoRA-finetuned LLMs at the finetuning stage. 
