
\section{Experiments}
In this section, we conducted a comprehensive comparison between vanilla LoRA and HaLoRA using popular open-source LLaMA models across various benchmarks under different noise configurations.

\input{figures_tables/Table_main_results}

\subsection{Experimental Setup}
\noindent \textbf{Non-ideal effect simulation.} 
In the hybrid CIM architecture, we consider the inference results based on digital SRAM macros to be relatively accurate. For the analog RRAM macros, based on noise levels reported in published RRAM chip studies \cite{yao_nature,device_2}, the standard deviation of the injected Gaussian noise can be determined within the range of 0.01 to 0.02. To comprehensively evaluate practical noise mitigation strategies, including redundant mapping and bit-splitting techniques, we extended our validation to encompass lower noise levels ($\sigma =$ 0.005, 0.01, and 0.02). Additionally, following the block-wise linear mapping characteristics of weights on physical RRAM crossbars \cite{neurosim_inf}, we partitioned the corresponding weights into 64Ã—64 blocks to align with conventional memory tile dimensions. These blocks were then subjected to noise injection procedures (detailed in Section~\ref{section:noise}).

\noindent \textbf{HaLoRA finetuning.} We conduct experiments on two variants of the LLaMA 3.2 family: LLaMA-1B and LLaMA-3B with 1.3 billion and 3.2 billion parameters, respectively. 
Following \cite{hu_llmadapter}, we employ the 170k samples for training.
As shown in Fig. \ref{method_fig}, we insert the LoRA branches into five modules: query/key/value/up/down projection layers.
The rank in LoRA is 4.
For the random noise, we sample noise every 400 steps ($N=400$) and set the noise level at 0.01.
We train on a single NVIDIA A100 GPU with a batch size of 16 for 3 epochs while the learning rate is 1e-4.
It takes around 2 hours to finetune the 1B version LLaMA 3.2.

\noindent \textbf{Evaluation.}
We evaluate 6 popular benchmarks, including 1) ARC-e (Easy set of AI2 Reasoning Challenge), 2) OBQA (OpenBook Question Answering), 3) SIQA (Social Interaction QA), 4) ARC-c (Challenge set of AI2 Reasoning Challenge), 5) WinoG. (Winograd Schema Challenge), and 6) PIQA (Physical Interaction QA).
We refer the readers to~\cite{wu_moslora} for more details about these benchmarks. During inference, we sample the noise 5 times under the random seed \{1,2,3,4,5\} for each noise level from \{0.005, 0.01, 0.02\}. Meanwhile, we also report the performance without noise.
For all the benchmarks, both the average and standard deviation of accuracy are reported.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Main Results}

Table~\ref{table_main} presents the results on 6 benchmarks when finetuning LLaMA 3.2 1B and 3B.
For each benchmark, we report the noise-free evaluation results and the average with standard deviation when adding noise.
Besides, we also report the overall average of these average scores on 6 benchmarks.
Some key findings can be summarized as:
\begin{itemize}
    \item \textbf{HaLoRA demonstrates better performance at noise-free setting.} As LLMs inherently tend to overfit during dataset training \cite{wu_noisytune}, HaLoRA's noise injection effectively enhances the diversity of model representations during the finetuning process, leading to improved performance on test sets. Specifically, under noise-free deployment conditions, HaLoRA achieves average performance improvements of 5.3 and 0.6 points compared to vanilla LoRA on LLaMA 1B and 3B models, respectively.
    \item \textbf{HaLoRA demonstrates superior accuracy at all noise levels.} Compared to vanilla LoRA, HaLoRA demonstrates enhanced performance across all six datasets. Notably, at a deployment noise level of 0.02, HaLoRA achieves substantial performance improvements over vanilla LoRA: 22.7 points (63.1 vs. 40.4) for LLaMA 3.2 1B and 13.5 points (78.4 vs. 64.9) for LLaMA 3.2 3B. Furthermore, HaLoRA shows significantly reduced performance degradation as noise levels increase. When noise levels escalate to 0.02, HaLoRA's average performance degradation is only 21\% (4.5 vs. 21.9 points) and 18\% (2.9 vs. 15.8 points) of vanilla LoRA's degradation for LLaMA 3.2 1B and 3B, respectively. 
    \item \textbf{HaLoRA demonstrates superior stability under noisy conditions.} At identical noise levels, HaLoRA exhibits significantly lower performance variance across different noise directions. For LLaMA 1B, across five experimental runs, HaLoRA's performance variance on WinoG. and PIQA datasets are merely 7\% (1.0 vs. 12.9) and 10\% (1.7 vs. 17.3) of vanilla LoRA's variance, respectively. Similarly, for LLaMA 3B, HaLoRA achieves remarkably lower variance on ARC-e and WinoG. datasets, showing only 3\% (0.3 vs. 9.2) and 11\% (0.9 vs. 8.4) of vanilla LoRA's variance.
    \item \textbf{Larger model shows better robustness against noise.} 
    Comparing the performance of LLaMA 3B and LLaMA 1B across multiple datasets under identical noise levels, both vanilla LoRA and HaLoRA (particularly the latter) exhibit reduced accuracy degradation in larger models (accuracy drops of 15.8 vs. 21.9 for vanilla LoRA, and 2.9 vs. 4.5 for HaLoRA). Additionally, larger models show more stable performance, as evidenced by lower standard deviations (maximum values of 5.5 vs. 17.3 for vanilla LoRA, and 1.5 vs. 1.9 for HaLoRA). These results suggest that the model scale positively correlates with noise resilience.
\end{itemize}

In summary, HaLoRA provides a robust solution for deploying fintuned LLMs on noisy hybrid architectures. 
Our findings also reveal that larger models (especially with HaLoRA) tend to demonstrate better tolerance to hardware-induced noise compared to the smaller models.

\subsection{Further Analysis}


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{\linewidth}
    \caption{OBQA}
    \includegraphics[width=\linewidth]{figures_tables/OBQA.pdf}
    % \vskip -0.1in
    \label{fig:OBQA}
    \end{subfigure}
    \vskip -0.2in
    \begin{subfigure}[t]{\linewidth}
    \caption{SIQA}
    \includegraphics[width=\linewidth]{figures_tables/SIQA.pdf}
    % \vskip -0.1in
    \label{fig:SIQA}
    \end{subfigure}
    % \vskip -0.1in
    \caption{The performance of HaLoRA with different values of $\mu$ and vanilla LoRA on the OBQA and SIQA datasets.}
    \label{fig:sensitivity}
\end{figure}

\noindent \textbf{Sensitivity of $\mu$.}
We conducted a sensitivity analysis of hyperparameter $\mu$ on LLaMA 1B to further evaluate HaLoRA's stability. 
Figures 3(a) and (b) illustrate the average performance scores across different noise levels and $\mu$ values on the OBQA and SIQA datasets with random seeds of 1 to 5, respectively.

Overall, HaLoRA demonstrates relatively stable performance across different values of $\mu$. 
Although the performance variations slightly increase with higher noise levels, the average results from five experiments show that $\mu$ within the range of [0.05, 0.15] 
%[0.05,0.20]
lead to maximum performance changes of 4.4 
%(8.3) 
and 3.9 
%(6.04) 
points on OBQA and SIQA, respectively. Notably, HaLoRA consistently outperforms Vanilla LoRA across all values of $\mu$.

As $\mu$ increases from smaller to larger values, we observe distinct behavioral patterns. Smaller $\mu$ enhances the LoRA branch's noise tolerance at the finetuning noise level but leads to performance degradation when the noise level exceeds the finetuning setting ($\sigma=0.02>0.005$). Conversely, excessively large $\mu$ leads to overfitting to precise outputs, compromising the network's inherent noise-tolerant capabilities.


\input{figures_tables/Table_case_study}

\noindent \textbf{Case study.} We showcase the outputs of LoRA and HaLoRA with and without noise, respectively.
Table \ref{tab_cases} indicates the cases from the ARC-e dataset.
We can find that noise would lead to the nonsense output from the LoRA-finetuned LLaMA 3.2 1B, while the HaLoRA-finetuned one shows strong robustness capability.
Moreover, considering the noise-free setting of case \#2, HaLoRA-finetuned LLaMA 3.2 1B can generate the right answer while LoRA-finetuned one would fail.
This phenomenon is consistent with Table \ref{table_main} that HaLoRA demonstrates better performance under a noise-free setting.
Specifically, HaLoRA gets an average score of 67.6, which is higher than the 62.3 of LoRA. 