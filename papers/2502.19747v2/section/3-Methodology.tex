\section{Methodology}

This section first introduces the preliminary about Transformer and LoRA, followed by the details of the deployment strategy and training process via the proposed HaLoRA.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.93\linewidth]{figures_tables/Figure2_v1.pdf}
\caption{
The train and deploy stages for proposed HaLoRA. (a) During the training stage, HaLoRA incorporates an additional loss regularization term with sampled noise to enhance model robustness. (b) In the deploy stage, the finetuned LLM is mapped to a hybrid CIM architecture formed by RRAM and SRAM-based CIM macros, leveraging their respective advantages.
}
\label{method_fig}
\end{figure*}

\subsection{Preliminary}
\noindent \textbf{Transformer.} Transformer \cite{vas_trans} is the predominant architecture for LLMs, such as LLaMA \cite{llama_report}.
Each transformer layer consists of a multi-head self-attention (MHA) sub-layer and a feed-forward (FFN) sub-layer.
In the MHA sublayer, the input would be projected into K, Q, and V vectors, followed by the nonparametric operations.
Then, the FFN sublayer contains an up-projection linear layer and a down-projection linear layer.
We refer the readers to \cite{wu_wid} for more details.

\noindent \textbf{Vanilla LoRA.}
Based on the observation that the update in pretrained weights during model adaptation exhibits low intrinsic rank, the LoRA \cite{hu_lora} method aims to model the weight update $\Delta \mathbf{W}$ of weight $\mathbf{W}_0 \in \mathbb{R}^{d_1 \times d_2}$ via two low-rank matrices following:
\begin{equation}
\label{eq_lora}
    \mathbf{W} = \mathbf{W}_0+\Delta \mathbf{W}, \ \Delta \mathbf{W} = \mathbf{A} \mathbf{B} \in \mathbb{R}^{d_1 \times d_2},
\end{equation}
where $\mathbf{A} \in \mathbb{R}^{d_1 \times r}$ and $\mathbf{B} \in \mathbb{R}^{r \times d_2}$.
The parameters to update would be much less since we have $(d_1+d_2)r \ll d_1 \times d_2$ when $r \ll \min(d_1, d_2)$.
During finetuning, we would update the LoRA branch~(i.e., $\mathbf{A}$ and $\mathbf{B}$) while freezing the original pretrained weight $\mathbf{W}_0$.
Therefore, LoRA branches are task-specific while original pretrained weights are task-agnostic.


\subsection{Impact simulation for Hybrid CIM}
\label{section:noise}
We propose a deployment strategy for finetuned LLMs in a hybrid CIM architecture that exploits the complementary strengths of RRAM and SRAM. As shown in Fig.~\ref{method_fig}b, the architecture implements the LLM backbone in RRAM, leveraging its storage density and energy efficiency. The noise-sensitive and task-specific LoRA branch is deployed on SRAM for accurate write operations and efficient task adaptation. For attention blocks, while previous works \cite{ReTransformer,ReTransformer_2} demonstrate fully RRAM-based deployment, the dynamic matrix-matrix multiplication (MatMul) requires extensive write-verify operations during inference. To achieve efficient dynamic matrix operations, the MatMul modules are allocated to SRAM for lower memory writing overhead.


In the context of neural network inference tasks, hardware non-ideality primarily manifests as random noise arising from device variability\cite{neurosim_inf}. Empirical evidence from previous research has demonstrated that such noise can be safely modeled as zero-mean Gaussian noise\cite{yiyu_swim}. While computations of LoRA branches on SRAM-CIM maintain high precision due to digital computing, noise effects on the LLM backbone executed on RRAM-CIM require special consideration.

To address this challenge, we implement a block-wise linear mapping strategy that aligns with practical deployment workflows\cite{neurosim_inf} and introduces noise into the weights of individual sub-blocks to accurately simulate the impact of analog computing noise.
For the target RRAM tiles in the shape of $m\times n$, the magnitude of the simulated Gaussian noise is determined by the device-specific standard deviation $\sigma$. 
Thus, the non-ideality simulation of RRAM-CIM macros acting on weight matrix $\mathbf{W}_0 \in \mathbb{R}^{d_1\times d_2}$ can be formulated as:
\begin{equation}
    \{\mathbf{W}_{0,[i,j]}\}_{i=1,j=1}^{k,t} = \operatorname{split}(\mathbf{W}_0), \ k=\lceil\frac{d_1}{m}\rceil, \ t=\lceil\frac{d_2}{n}\rceil,
\end{equation}
\begin{equation}
\begin{aligned}
\mathbf{W}_0^* = \operatorname{cat}(\{\mathbf{W}_{0,[i,j]} + \lambda(\max(\operatorname{abs}(\mathbf{W}_{0,[i,j]})))\}_{i=1,j=1}^{k,t}), 
\\ 
\lambda\sim\mathcal{N}(0,\sigma)
\end{aligned}
\label{eq_rram_noise}
\end{equation}
where $\operatorname{split}(\cdot)$ and $\operatorname{cat}(\cdot)$ denote the matrix splitting and concatenation operations respectively, $\mathbf{W}_{0,[i,j]}$ represents the partitioned weight block, and $\mathbf{W}_0^*$ corresponds to the noise-injected weight matrix.


These approaches ensure simulation results that reflect the practical constraints and capabilities of the hybrid architecture.


\subsection{Hardware-aware Low-Rank Adaptation~(HaLoRA)}
\input{figures_tables/Table_algo}
In the hybrid CIM architecture, the inherent noise from RRAM would lead to performance degradation.
Specifically, as shown in Table \ref{table_main}, the noise at the level of $\sigma=0.02$ would decrease the average scores by 21.7 and 15.8 for the LLaMA 3.2 1B and 3B model, respectively.

To address this issue, we propose a novel HaLoRA method.
The key insight is to align the training objectives under both ideal and noisy conditions during the training process, and thus train a better LoRA branch that is both robust to the noise~(via aware of noise) and accurate~(via minimizing the gap) during inference.


Considering the gradients for matrices $\mathbf{A}$ and $\mathbf{B}$ in LoRA, we have:
\begin{equation}
\label{eq_grad_ori}
\frac{\partial \mathcal{L}}{\partial \mathbf{A}} = \frac{\partial \mathcal{L}}{\partial \mathbf{W}} \mathbf{B}^T, \ \frac{\partial \mathcal{L}}{\partial \mathbf{B}} = \mathbf{A}^T \frac{\partial \mathcal{L}}{\partial \mathbf{W}}, 
\end{equation}
where $\mathbf{W}$ is the merged weight defined in Equation \ref{eq_lora}. 
For the ideal condition without noise, the updated LoRA branch can be formulated as:
\begin{equation}
\small
\label{eq_ori_update}
\begin{aligned}
    (\mathbf{A} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{A}})
    (\mathbf{B} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{B}})
    & = (\mathbf{A} - \eta  \frac{\partial \mathcal{L}}{\partial \mathbf{W}} \mathbf{B}^T)
    (\mathbf{B} - \eta \mathbf{A}^T \frac{\partial \mathcal{L}}{\partial \mathbf{W}}) \\
    &\approx \mathbf{A}\mathbf{B}-\eta \mathbf{A} \mathbf{A}^T \frac{\partial \mathcal{L}}{\partial \mathbf{W}} -  \eta  \frac{\partial \mathcal{L}}{\partial \mathbf{W}} \mathbf{B}^T \mathbf{B} ,
\end{aligned}
\end{equation} 
where $\eta$ is the learning rate such as 1e-3 and thus we discard the item $\eta^2 \frac{\partial \mathcal{L}}{\partial \mathbf{W}} \mathbf{B}^T \mathbf{A}^T \frac{\partial \mathcal{L}}{\partial \mathbf{W}}$.
Considering the noise existed in $\mathbf{W}_0$, we have:
\begin{equation}
    \mathbf{W}^{*}=\mathbf{W}_0^*+ \Delta \mathbf{W}.
\end{equation}
Similarly, we can thus get the updated LoRA branch under the same initialization:
\begin{equation}
\small
\label{eq_noisy_update}
\begin{aligned}
    (\mathbf{A} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{A}})
    (\mathbf{B} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{B}}) 
    &= 
    (\mathbf{A} - \eta  \frac{\partial \mathcal{L}}{\partial \mathbf{W^*}} \mathbf{B}^T)
    (\mathbf{B} - \eta \mathbf{A}^T \frac{\partial \mathcal{L}}{\partial \mathbf{W^*}}) \\
    &\approx \mathbf{A}\mathbf{B}-\eta \mathbf{A} \mathbf{A}^T \frac{\partial \mathcal{L}}{\partial \mathbf{W^*}} -  \eta  \frac{\partial \mathcal{L}}{\partial \mathbf{W^*}} \mathbf{B}^T \mathbf{B} .
\end{aligned}
\end{equation}

To align the optimization process under ideal and noisy conditions, we aim to minimize the gap between Equation \ref{eq_ori_update} and Equation \ref{eq_noisy_update}, which can be formulated as:
\begin{equation}
    \min \eta || \mathbf{A} \mathbf{A}^T ( \frac{\partial \mathcal{L}}{\partial \mathbf{W^*}}- \frac{\partial \mathcal{L}}{\partial \mathbf{W}}) + (\frac{\partial \mathcal{L}}{\partial \mathbf{W^*}}-\frac{\partial \mathcal{L}}{\partial \mathbf{W}}) \mathbf{B}^T \mathbf{B} ||.
\end{equation}
The learning rate $\eta$ is fixed and can be discarded.
Given the property that $||\mathbf{X}\mathbf{Y}|| \leq ||\mathbf{X}|| ||\mathbf{Y}||$ and $||\mathbf{X}+\mathbf{Y}|| \leq ||\mathbf{X}||+ ||\mathbf{Y}||$, we can instead optimize the upper bound:
\begin{equation}
\small
\begin{aligned}
    &\min ||\mathbf{A} \mathbf{A}^T|| \ ||( \frac{\partial \mathcal{L}}{\partial \mathbf{W^*}}- \frac{\partial \mathcal{L}}{\partial \mathbf{W}})|| + || (\frac{\partial \mathcal{L}}{\partial \mathbf{W^*}}-\frac{\partial \mathcal{L}}{\partial \mathbf{W}})|| \ ||\mathbf{B}^T \mathbf{B}||  \\
    & \longrightarrow \min || (\frac{\partial \mathcal{L}}{\partial \mathbf{W^*}}-\frac{\partial \mathcal{L}}{\partial \mathbf{W}})|| \ (||\mathbf{A} \mathbf{A}^T|| + ||\mathbf{B}^T \mathbf{B}||).
\end{aligned}
\end{equation}
Since noise is stochastic and within a scope, then the optimization target can be defined as:
\begin{equation}
\label{eq_final_goal}
    \min ||\mathbf{A} \mathbf{A}^T|| + ||\mathbf{B}^T \mathbf{B}||.
\end{equation}
Finally, the goal to align the training process is simplified into Equation \ref{eq_final_goal}, which is \textit{agnostic} from the noise.
In HaLoRA, we select the Euclidean norm.
As shown in Fig. \ref{method_fig}(a), The training loss to update the LoRA branch is:
\begin{equation}
    \mathcal{L}_{total} = \mathcal{L} + \mu \mathcal{L}_{reg},
\end{equation}
where $\mu$ is the hyperparameter for loss weight and $\mathcal{L}_{reg}=||\mathbf{A} \mathbf{A}^T||_2+||\mathbf{B}^T \mathbf{B}||_2$.
Algorithm \ref{alg1} indicates the details of the proposed HaLoRA.

