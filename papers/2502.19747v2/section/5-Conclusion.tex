\section{Conclusion}
In this work, we present a hybrid CIM architecture for deploying LoRA-finetuned LLMs based on both RRAM and SRAM devices. Through theoretical analysis of device noise impact on LLM inference, we identify the performance degradation caused by RRAM non-ideality as a key challenge. To address this issue, we propose HaLoRA, which aligns the training objectives under both ideal and noisy conditions and optimizes the upper bound of the alignment loss. Experimental results demonstrate that HaLoRA consistently enhances LLM performance when deployed on noisy hybrid architectures. For future work, we plan to extend HaLoRA to quantized LLMs and explore its effectiveness in more challenging tasks such as mathematical reasoning.
