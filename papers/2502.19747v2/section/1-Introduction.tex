\section{Introduction}

Large language models (LLMs), such as GPT-4 \cite{openai_gpt4}, LLaMA \cite{llama_report}, and Qwen \cite{qwen}, have demonstrated promising performance in various Natural Language Processing (NLP) tasks.
However, this success, primarily driven by massive model parameters, brings forth two critical challenges in practical applications. First, adapting LLMs to downstream tasks via full model finetuning requires prohibitive computational resources. Second, model inference demands substantial energy consumption, limiting the wide deployment of these models.

Various parameter-efficient finetuning (PEFT) methods have been proposed to address the adaptation limitation \cite{hou_adapter,Li_prompt,zaken_bitfit}. Among them, low-rank adaptation (LoRA) \cite{hu_lora} has gained increasing popularity due to its simplicity and efficacy by updating only an extra low-rank matrix while preserving the original pretrained weights. 
Meanwhile, to improve inference efficiency, Computing-in-Memory (CIM) architectures perform computations directly within memory arrays, achieving high energy efficiency through array-level parallel computing. Resistive random-access memory (RRAM) and static random-access memory (SRAM) are two typical memory devices used in CIM implementations. However, deploying LoRA-finetuned LLMs on CIM architectures remains largely unexplored, presenting both challenges and opportunities.


\input{figures_tables/Table_intro}


In this paper, we propose a hybrid deployment strategy that leverages the complementary advantages of RRAM-based and SRAM-based CIM architectures. Pure RRAM-based CIM achieves high energy efficiency but suffers from inherent noise characteristics and complex write-verify operations \cite{ne_cim}, while pure SRAM-based CIM provides noise-free computation but is limited by its volatile nature and low storage density \cite{nn_cim}. LoRA-finetuned LLMs exhibit a distinctive structural characteristic: as shown in Table \ref{tab-compra}, the pretrained weights (e.g., 1235.8M parameters in LLaMA 3.2 1B) dominate the model size compared to the LoRA branch (1.9M parameters). 
Our hybrid strategy exploits this characteristic: 1) deploying the task-agnostic pretrained weights on RRAM maximizes energy efficiency while avoiding frequent write operations, and 2) implementing the task-specific LoRA branches on SRAM ensures accurate adaptation at a reasonable cost. 
Despite RRAM's energy efficiency, the inherent non-ideality would introduce noise during the reading process, misleading the model to generate nonsense responses (shown in Fig. \ref{intro_fig}).
These raise an intriguing question: \textit{Can we leverage the accurate LoRA computations on SRAM to compensate for the noise-induced errors from pretrained weights deployed on RRAM, thereby achieving an optimal balance between energy efficiency and task adaptation?}



\begin{figure}[!t]
\centering
\includegraphics[width=0.75\linewidth]{figures_tables/Figure1_sub.pdf}
\caption{
One case during inference for noise-free and noisy LoRA-finetuned LLMs. The non-ideality of RRAM imposes noise on pretrained weights and thus hurts the performance.}
\label{intro_fig}
% \vspace{-1em}
\end{figure}


% HaLoRA
To this end, we propose Hardware-aware Low-Rank Adaptation (HaLoRA), a noise-robust adaptation method. 
Our key insight is to minimize the discrepancy of two LoRA optimization directions with and without noise in pretrained weights during training, and thus the optimized LoRA would be both \textbf{robust~(via aware of noise)} and \textbf{accurate~(via minimizing the gap)}.
We first inject random noise into the pretrained weights and then optimize the LoRA branches towards the noise-free optimal to avoid overfitting to specific noise patterns. With a theoretically proven noise-agnostic upper bound of this optimization gap, HaLoRA achieves superior performance under noisy pretrained weights during inference.


% 实验&贡献
We conduct comprehensive experiments by finetuning LLaMA 3.2 (1B and 3B variants) on commonsense reasoning tasks. To evaluate robustness, we inject noise into pretrained weights at three different levels during inference, with each condition tested across five random seeds. Experimental results demonstrate that HaLoRA consistently outperforms standard LoRA. Most notably, at a noise level of 0.02, HaLoRA achieves an average score of 63.1, surpassing LoRA by a significant margin of 22.7 points. These results validate our approach's effectiveness in deploying LLMs on hybrid CIM architectures while maintaining robust performance.
Our contributions can be summarized as follows:
\begin{itemize}
    \item We propose the first framework to deploy LoRA-finetuned LLMs on the hybrid CIM architecture, i.e., task-agnostic pretrained weights onto RRAM and task-specific LoRA onto SRAM.
    \item 
    We introduce HaLoRA which addresses RRAM non-ideality issues and guarantees accurate outputs by aligning noisy and noise-free training objectives with theoretical performance guarantees.
    \item 
    We evaluate HaLoRA by finetuning LLaMA 3.2 (1B and 3B variants) on commonsense reasoning tasks, demonstrating the effectiveness and robustness of the proposed HaLoRA.
\end{itemize}




