\begin{algorithm}[!t]
    % \small
% 	\vskip -0.03in
	\caption{HaLoRA}
	\label{alg1}
	
	{\bf Input:} Train data $\mathcal{D}_{train}$, Finetuned LLM $M$
	
	{\bf Params:} $N$: steps to apply alignment, $\mu$: loss weight for regularization loss

	{\bf Output: } Updated LoRA branch.
	
	\begin{algorithmic}[1]
        \STATE Insert LoRA for selected linear layer in $M$
	    \STATE Initialize LoRA branch where $\mathbf{A}$ as Kaiming Uniform distribution and $\mathbf{B}$ as Zero matrix.
	    % \STATE $k \gets 0$ ; ${M} \gets \left[ \ \right]$
        \STATE Sample Noise to pretrained weight $\mathbf{W}_{0}$ following Equation \ref{eq_rram_noise} 
		\FOR{$i=0$ to max training steps}
        \STATE Forward a batch from $\mathcal{D}_{train}$ through $M$, derive the gradients $g_{ori}$ for LoRA to update following Equation \ref{eq_grad_ori}
		\IF{ $i \% N == 0$} 
        \STATE Sample Noise to pretrained weight $\mathbf{W}_{0}$ following Equation \ref{eq_rram_noise}  
		\STATE Calculate regularization loss $\mathcal{L}_{reg}$ following Equation \ref{eq_final_goal}
        \STATE backward loss to get the regularization gradient $g_{reg}$
        \STATE $g_{ori} \gets g_{ori} + \mu g_{reg}$
		\ENDIF
		\STATE Update the LoRA branch with corresponding $g_{ori}$ while keeping $\mathbf{W_0}$ frozen
		\ENDFOR
		\STATE {\bf return} Optimized the LoRA branch
	\end{algorithmic}
\end{algorithm} 