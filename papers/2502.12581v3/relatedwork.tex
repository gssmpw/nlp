\section{Related work}
\label{sec:related}
In practice, $\MV$ is the most popular method due its implementation.
A popular alternative is a group of iterative solvers that update estimates of task labels and model priors (of class distribution and annotator reliability) in successive loops.
\citet{dawid} and more recent variants  of iterative methods ~\citep{karger2014budget, li2014error, li2019truth, chen2023sample} are examples of these methods. The parameters are estimated via an EM algorithm. 
Spectral algorithms that use the eigenspace of the annotator-label matrix of annotations have also been proposed~\citep{ghosh, zhang2014spectral, pmlr-v151-tenzer22a} that have some similarities to iterative solvers (see~\citet{karger2014budget}).
Methods that leverage other principles like Bayesian formulation have also been studied. \citet{li2019exploiting} use the mean-field variational method to
mine latent source relationships through tensor decomposition and object clustering. 
\citet{yang2024lightweight} view the label aggregation task as a dynamic system, with task identifiers serving as time-slices. Using a Dynamic Bayesian Network to model this system, they develop two label aggregation algorithms.
\citet{li2019truth} proposed a Bayesian model based on conjugate prior and iterative EM reasoning for highly redundant labeled data.
Similarities can be drawn among various methods and where feasible, they theoretically bound $\exe[\err]$ as $\exp{(-\bigO(vr))}$ for some measure of crowd reliability~$r$ and annotation volume~$v$.
They fundamentally differ in the implicit assumptions on the priors in model specification and the solver used that are necessary for the bounds to hold true.
For example,~\citet{karger2014budget} assumes sparse assignment of samples to annotators, while~\citet{dalvi} requires large eigengap for annotator-label annotation matrix.
Designing annotation jobs where the assumptions hold is non-trivial since they are not easily verifiable making it a trial-and-error to evaluate them suitability in practice.

Certain class of methods explicitly estimate only the noise transition matrix, sometimes with guarantees on optimality, from which label estimates could be inferred subsequently.
For example, \citet{bonald} employ correlations of annotator triplets to estimate annotators' noise transition with $\ell_\infty$-norm of the error bounded as $\bigO(\frac{1}{v\sqrt{\ntasks}})$, the minimax lower bound for their setting with $\ntasks$ being the number of samples or tasks labelled.
Similarly,~\citet{bucarelli2023leveraging} solve an optimisation problem that uses pairwise annotator similarities with a guarantee on $p$-norm of estimation error as $\bigO(\frac{1}{\sqrt{\ntasks}})$.
The methods of DS and others like~\citet{li2019exploiting} estimate the annotator noise a by-product of their algorithm.
We describe how these noise estimates can be leveraged to derive optimality certificates on real-data.
%PAC bounds have also been studied in the literature for this problem.

%\citet{10.1002/wics.1362} analyze MV’s error rates as labelers grow infinite (not providing any analysis for finite number of annotators and not studying the optimality of MV).  

%They typically arise from the application of (weighted) majority vote for bagging .

There have been attempts to study human labelling in the PAC framework mostly to analyse its impact on subsequent learning. The most general case with no assumptions on annotators will lead to noise models that are not amenable for analysis \cite{awasthi2017efficient,rivest1994formal}. Hence, it is common to impose certain structure on the annotators like ``few perfect labellers'', ``majority good labellers'', ``few good labellers'', etc, to derive suitable bounds. However, most works with PAC analysis focus on generalization error \cite{awasthi2017efficient} which is very different from the labelling error that we analyse here. Those studying label aggregation, typically arising out of application of WMV to bagging in ensembles \cite{breiman1996bagging}, study bounds on absolute error $\exe[R]$ \cite{masegosa2020second}. In contrast, we do not study the absolute error of MV itself but its gap relative to the optimal estimation method.

% The crowdsourced PAC learning
% model was firstly presented by \citet{awasthi2017efficient},
%  who showed that when a noticeable fraction of the labelers are perfect, and the rest behave arbitrarily, any classifier that can be efficiently learned in the traditional realizable PAC model can be learned in a computationally efficient manner by querying the crowd, despite high amounts of noise in the responses.
% They also show that the MV algorithm ensures high-probability correct labeling if enough labelers are used and determine how this number scales with sample size and accuracy.  
% This model was later extended  by \citet{zeng2022efficient} and for specific settings  like adversarial labellers by \citet{pmlr-v80-kleindessner18a}. The focus, however, all through these approaches is limited to bounding the generalization error $\exe[R]$ of the method being analysed. For example, \citet{zeng2022efficient} study adversarial annotators, \citet{masegosa2020second} derive noise-agnostic bounds for WMV when weights follow a described distribution, \citet{awasthi2017efficient} addresses partially labelled data. These generalization bounds are very different from the optimality conditions we derive for MV. 

% In the PAC literature, different approaches have been proposed \cite{JMLR:v16:germain15a,zeng2022efficient}. \citet{zeng2023semi} provide an algorithm for the case in which the majority of the workers behave adversarially and they are able to obtain some bounds on this algorithm. \citet{masegosa2020second} study weighted majority vote (WMV) of multiple classifiers with weights assigned according to a distribution, not necessarily dependent on classifier noise. They then bound the generalization error of WMV for a more general problem than crowdsourcing. \citet{feofanov2024multi} establish theoretical bounds for the multi-class majority vote classifier in a PAC learning framework, addressing scenarios where the training data is only partially labeled.

%Some papers in the PAC literature have indeed done some studies 

%In contrast to above related literature, our work does not propose a new aggregation method but instead studies the conditions under which the simplest aggregation method optimally recovers labels.
A relevant work to our discussion is~\citet{nitzan1981characterization} that establishes the optimality of a suitably weighted majority vote.
A special case of their method is that of uniform class distribution with equally reliable annotators\footnote{See Corollary 2b in~\citep{nitzan1981characterization}.} where it reduces to MV aligning with our finding for this scenario.
However, the equivalence of MV's label recovery to that of oMAP applies to a larger more general range of parameters that we establish in our work. Similarly, \citet{berend2015finite} investigate how the probability of error of Naive Bayes, which is the same as our oMAP and weighted MV of \citet{nitzan1981characterization}, can be bounded. Their goal—to study the rate at which this error probability of oMAP and its empirical counterpart (eMAP) for some given reliability decreases to zero—is distinct from our goal, which is to identify the conditions under which MV is the optimal decision rule. 

% Notwithstanding some differences like the distinct but symmetric annotator noise of~\citet{nitzan1981characterization} as against non-symmetric shared noise that we study, the assumption of access to an oracle renders it impractical for real-world use. 
Despite some differences, such as the distinct symmetric annotator noise described by~\citet{nitzan1981characterization} compared to the non-symmetric shared noise in our study, the requirement for access to an oracle limits its practical applicability in real-world scenarios. We address this shortcoming by describing a method to verify with high probability if a given parameter configuration is in the regime where MV is optimal.
This result is central to practical application of our theory that is validated in our experiments.
For all such configurations, label estimate for each sample from MV is guaranteed to match that of oMAP with no incentive for exploring more complex aggregation alternatives.