\section{Experiments}
\label{experments}


\textbf{Datasets.}
The pre-training and fine-tuning datasets used in this work are identical to those utilized in LLava-v1.5.
For pre-training, we use a subset of the LAION/CC/SBU dataset filtered for balanced concept coverage and enriched with BLIP-generated captions. 
For instruction tuning, we use a combination of COCO\cite{lin2014microsoft}, GQA\cite{hudson2019gqa}, OCR-VQA\cite{mishra2019ocr}, TextVQA\cite{singh2019towards}, and VisualGenome\cite{krishna2017visual} datasets. The details of the datasets are in the Appendix. 

\textbf{Tasks and Evaluation.}
Building upon previous studies, we conduct experiments on a range of visual question-answering benchmarks, including OK-VQA\cite{marino2019ok}, GQA\cite{hudson2019gqa}, VizWizQA\cite{Bigham_Jayant_Ji_Little_Miller_Miller_Tatarowicz_White_White_Yeh_2010}, TextVQA\cite{singh2019towards}, RealWorldQA\cite{grok15v}, and ScienceQA\cite{lu2022learn}. Additionally, we evaluate our method on widely recognized general multimodal benchmarks, such as MMBench\cite{liu2025mmbench}, POPE\cite{li2023evaluating}, SEED\cite{li2023seed}, AI2D\cite{kembhavi2016diagram}, MM-Vet\cite{yu2023mm}, MMMU\cite{yue2024mmmu}, MMTB\cite{ying2024mmt}, OCR-VQA\cite{mishra2019ocr}, and MME\cite{fu2024mmecomprehensiveevaluationbenchmark}. We utilize the evaluation framework lmms-eval\cite{zhang2024lmmsevalrealitycheckevaluation, lmms_eval2024}, as recommended by LLava, which integrates the evaluation capabilities of various benchmarks.


\textbf{Models.} To comprehensively verify the effectiveness of the VDEP, we employ base models of different parameter scales. Specifically, we utilize TinyLLava~\cite{zhou2024TinyLLava} and LLava-v1.5~\cite{liu2024improved} as our base models, whose model sizes are 3B and 7B, respectively. A series of carefully designed experiments are conducted to evaluate their performance.

\textbf{Baseline and Implementation.} 
We compare TinyLLava-VDEP with TinyLLava and LLava-v1.5-VDEP with LLava-v1.5. We thoroughly tune the hyperparameters for different-scale base models and report the best performance. The details of different-scale base models can be found in the Appendix. During the pre-training, we employ two training strategies, VDEP and LLava. We double the input data to ensure these two strategies receive fair and sufficient training. 
To enable smooth transitions between the two strategies, we introduce a novel special token, \textless auto\_image\textgreater. Specifically, when image data undergoes autoregressive training, the \textless auto\_image\textgreater  token is used instead of the conventional \textless image\textgreater  token. 
% Conversely, the data format remains uniform for autoregressive training tasks primarily focused on textual data.

% %table3
% \begin{table*}[ht]
% \caption{Comparison of LLava-VDEP (Ours) and LLava-v1.5 on MME Tasks with Different Model Sizes. (The image loss weight $\alpha$ is set to 0.001 by default, with a data ratio of 1.0.)}
% \label{MME_benchmark}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \scalebox{0.9}{
% \begin{tabular}{@{}p{4.0cm}ccccccc@{}}
% \toprule
% \multirow{2}{*}{\raggedright\textbf{METHOD}} & \multirow{2}{*}{\textbf{Perception}}  & \multicolumn{1}{c}{\textbf{Commonsense QA}} & \multicolumn{4}{c}{\textbf{Coarse-grained Perception Tasks}} & \textbf{Total} \\
%  \cmidrule(lr){4-7}
% & & \textbf{(Reasoning)} & \textbf{Existence} & \textbf{Count} & \textbf{Position} & \textbf{Color} & \textbf{Scores} \\ 
% \midrule
% \textbf{\emph{TinyLLava-3B}} & & & & & & & \\ 
% TinyLLava & 1488.30 & 120.71 & 185.00 & 143.33 & 133.33 & 180.00 & 762.37 \\
% \rowcolor[HTML]{ededed} % 浅灰背景
% TinyLLava-VDEP(Ours) & \textbf{1499.08} & \textbf{130.70} & \textbf{200.00} & \textbf{158.33} & \textbf{138.33} & \textbf{180.00} & \textbf{807.36} \\
% Change & +10.78 & +9.99 & +15.00 & +15.00 & +5.00 & +0.00 & +44.99 \\
% \midrule
% \textbf{\emph{LLava-v1.5-7B}} & & & & & & & \\ 
% LLava & 1510.72 & 135.71 & 190.00 & \textbf{158.33} & 128.33 & 175.00 & 787.37 \\
% \rowcolor[HTML]{ededed} % 浅灰背景
% LLava-VDEP(Ours) & \textbf{1516.60} & \textbf{136.00} & \textbf{190.00} & 153.30 & \textbf{135.00} & \textbf{180.00} & \textbf{794.30} \\
% Change & +5.88 & +0.29 & +0.00 & -5.03 & +6.67 & +5.00 & +6.93 \\
% % \midrule
% % \textbf{\emph{LLM-Size-13B}} & & & & & & & \\ 
% % LLava-v1.5 & \textbf{1581.45} & 132.14 & \textbf{190.00} & \textbf{155.00} & \textbf{135.00} & \textbf{195.00} & \textbf{807.00} \\
% % \rowcolor[HTML]{ededed} % 浅灰背景
% % LLava-VDEP (Ours) & -- & -- & -- & -- & -- & -- & -- \\
% % Change & -- & -- & -- & -- & -- & -- & -- \\
% \bottomrule
% \end{tabular}
% }
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}

% \begin{table}[t]
% \caption{Ablation study on the hyperparameter $\alpha$, which represents the variation of the image loss weight.  (The data ratio is set to 1.0 by default.)}
% \label{alpha}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \scalebox{1.0}{
% \begin{tabular}{lcccccccc}
% \toprule
% \textbf{$\alpha$} & \textbf{RWQA} & \textbf{MME\textsuperscript{P}} & \textbf{MMB} & \textbf{VQA\textsuperscript{ok}} \\
% \midrule
% % \textbf{\emph{TinyLLava-VDEP-3B}} \\
% % \emph{w}/ 0.1 & -- & -- & -- & -- \\
% % \emph{w}/ 0.01 & -- & -- & -- & -- \\
% % \midrule
% % \rowcolor[HTML]{ededed}
% % \emph{w}/ 0.001 & 54.29 & 1499.08 & 158.33 & --\\
% % \midrule
% \textbf{\emph{LLava-VDEP-7B}} \\
% \emph{w}/ 0.1 & 55.42 & 1479.00 & 62.25 & 57.33 \\
% \emph{w}/ 0.01 & 56.73 & 1504.99 & 62.52 & 55.70 \\
% \midrule
% \rowcolor[HTML]{ededed}
% \emph{w}/ 0.001  & 57.64 & 1515.60 & 62.52 & 57.68 \\
% % \midrule
% % \textbf{\emph{LLava-VDEP-13B}} \\
% % \emph{w}/ 0.1  & -- & -- & -- & -- \\
% % \emph{w}/ 0.01  & -- & -- & -- & -- \\
% % \midrule
% % \rowcolor[HTML]{ededed}
% % \emph{w}/ 0.001  & -- & -- & -- & -- \\
% \bottomrule
% \end{tabular}}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

\section{Empirical Results and Analysis}
We evaluate the visual capabilities of models trained with VDEP across various visual question-answering tasks and
\begin
{table*}[ht]
\caption
{Comparison of LLava-VDEP (Ours) and LLava-v1.5 on MME Tasks with Different Model Sizes.}
\label{MME_benchmark}
\vskip
 0.15in
\begin
{center}
\begin
{small}
\begin
{sc}
\scalebox
{0.9}{
\begin
{tabular}{@{}p{4.0cm}ccccccc@{}}
\toprule
\multirow{2}{*}{\raggedright\textbf{METHOD}} & \multirow{2}{*}{\textbf{Perception}}  & \multicolumn{1}{c}{\textbf{Commonsense QA}} & \multicolumn{4}{c}{\textbf{Coarse-grained Perception Tasks}} & \textbf{Total} \\
 \cmidrule
(lr){4-7}
& & \textbf{(Reasoning)} & \textbf{Existence} & \textbf{Count} & \textbf{Position} & \textbf{Color} & \textbf{Scores} \\ 
\midrule
\textbf{\emph{TinyLLava-3B}} & & & & & & & \\ 
TinyLLava & 1488.30 & 120.71 & 185.00 & 143.33 & 133.33 & 180.00 & 762.37 \\
\rowcolor[HTML]{ededed} % 浅灰背景
TinyLLava-VDEP (Ours) & \textbf{1499.08} & \textbf{130.70} & \textbf{200.00} & \textbf{158.33} & \textbf{138.33} & \textbf{180.00} & \textbf{807.36} \\
Change & \textcolor{darkgreen}{+10.78} & \textcolor{darkgreen}{+9.99} & \textcolor{darkgreen}{+15.00} & \textcolor{darkgreen}{+15.00} & \textcolor{darkgreen}{+5.00} & \textcolor{darkgreen}{+0.00} & \textcolor{darkgreen}{+44.99} \\
\midrule
\textbf{\emph{LLava-v1.5-7B}} & & & & & & & \\ 
LLava & 1510.72 & 135.71 & 190.00 & \textbf{158.33} & 128.33 & 175.00 & 787.37 \\
\rowcolor[HTML]{ededed} % 浅灰背景
LLava-VDEP (Ours) & \textbf{1516.60} & \textbf{136.00} & \textbf{190.00} & 153.30 & \textbf{135.00} & \textbf{180.00} & \textbf{794.30} \\
Change & \textcolor{darkgreen}{+5.88} & \textcolor{darkgreen}{+0.29} & \textcolor{darkgreen}{+0.00} & -5.03 & \textcolor{darkgreen}{+6.67} & \textcolor{darkgreen}{+5.00} & \textcolor{darkgreen}{+6.93} \\
% \midrule
% \textbf{\emph{LLM-Size-13B}} & & & & & & & \\ 
% LLava-v1.5 & \textbf{1581.45} & 132.14 & \textbf{190.00} & \textbf{155.00} & \textbf{135.00} & \textbf{195.00} & \textbf{807.00} \\
% \rowcolor[HTML]{ededed} % 浅灰背景
% LLava-VDEP (Ours) & -- & -- & -- & -- & -- & -- & -- \\
% Change & -- & -- & -- & -- & -- & -- & -- \\
\bottomrule
\end
{tabular}
}
\end
{sc}
\end
{small}
\end
{center}
\vskip
 -0.1in
\end
{table*}
\begin
{table}[t]
\caption{Ablation study on the hyperparameter $\alpha$
, which represents the variation of the image loss weight.}
\label{alpha}
\vskip
 0.15in
\begin
{center}
\begin
{small}
\begin
{sc}
\scalebox
{1.0}{
\begin
{tabular}{lcccc}
\toprule
\textbf{$\alpha$} & \textbf{RWQA} & \textbf{MME\textsuperscript{P}} & \textbf{MMB} & \textbf{VQA\textsuperscript{ok}} \\
\midrule
\textbf{\emph{LLava-VDEP-7B}} \\
\emph{w}/ 0.1 & 55.42 & 1479.00 & 62.25 & 57.33 \\
\emph{w}/ 0.01 & 56.73 & 1504.99 & 62.52 & 55.70 \\
\midrule
\rowcolor
[HTML]{ededed}
\emph{w}/ 0.001  & \textbf{57.64} & \textbf{1515.60} & \textbf{62.52} & \textbf{57.68} \\
\bottomrule
\end
{tabular}}
\end
{sc}
\end
{small}
\end
{center}
\vskip
 -0.1in
\end
{table}

general multimodal benchmarks. This novel reconstruction of image information achieves highly competitive performance across different model scales. Our proposed method consistently demonstrates state-of-the-art performance on most evaluation metrics, outperforming baselines. Unless stated otherwise in the following experiments, the image loss weight $\alpha$ is set to 0.001, and the data ratio is fixed at 1.0.


\subsection{Performance Comparison On Benchmarks}
% To thoroughly assess the effectiveness of our model in general visual question-answering tasks, we conducted extensive evaluations on a diverse set of state-of-the-art benchmarks. The results, summarized in Tables \ref{main_vqa} and \ref{MME_benchmark}, demonstrate that the VDEP series consistently achieves or surpasses baseline performance across all benchmarks. On the MME benchmark, VDEP exhibits robust comprehension of visual content at multiple levels of granularity. On the RealWorldQA benchmark, which evaluates spatial reasoning in real-world scenarios, the VDEP 3B, 7B, and 13B variants achieved scores of 54.25, 57.64, and 56.86, respectively. These scores not only exceed all baseline metrics but also highlight VDEP's capacity for enhanced understanding of physical environments.
% For open-domain tasks such as VQA\textsuperscript{ok}, VDEP demonstrated notable improvements, with the 3B and 7B models achieving gains of 1.26 and 3.36 percentage points, respectively. On the VizWiz dataset, which presents challenges such as low-quality and blurred images, the 3B variant outperformed the baseline by 3.28 percentage points. Similarly, on multi-turn question-answering tasks like RWQA, the 7B model achieved a performance uplift of 2.84 percentage points. For scene-specific tasks such as SQA\textsuperscript{I}, the 7B model delivered a 1.56 percentage point improvement, further showcasing the versatility of the VDEP series.
% As outlined in Table \ref{MLLM_Benchmark}, the VDEP series also excelled in mainstream multimodal benchmarks. On MM-Vet, which measures the integration of vision-language capabilities across 16 complex multimodal tasks, the VDEP 3B model achieved a score of 35.00, significantly outperforming LLava-v1.5's score of 33.40. In MMT-Bench, which evaluates advanced reasoning and instruction-following across 32 meta-tasks and 162 sub-tasks, the VDEP series consistently surpassed baseline performance, demonstrating proficiency in visual recognition, localization, reasoning, and planning. On MMBench, which assesses 20 fine-grained multimodal capabilities, the VDEP series achieved results comparable to or exceeding the state of the art. Lastly, on the AI2D benchmark—a dataset focused on multiple-choice questions related to scientific diagrams containing text—VDEP achieved state-of-the-art performance, underscoring its strong understanding of textual content within images.



To evaluate the effectiveness of our model in general visual question-answering tasks, we perform comprehensive evaluations on a diverse set of advanced benchmarks. The results, summarized in Table \ref{main_vqa}, demonstrate that the VDEP series consistently outperforms the baseline across all benchmarks.
On the OK-VQA benchmark, which requires external knowledge for answering VQA problems, the VDEP 7B model improves by 6.18\%. On the VizWiz dataset, characterized by low-quality and blurred images, the VDEP 3B model outperforms the baseline by 9.56\%. On the RealWorldQA benchmark, which assesses spatial reasoning in realworld scenarios, the VDEP 7B model surpasses the baseline by 5.18\%. On the large-scale multimodal scientific question-answering task, the VDEP 7B model achieves a 2.33\% improvement over the baseline.

% \begin{table}[t]
% \caption{Ablation study on the hyperparameter Data Ratio, which represents the proportion of different VDEP and LLava patterns in pre-training stage.  (The image loss weight $\alpha$ is set to 0.001 by default.)}
% \label{data_ratio}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \scalebox{0.9}{%{%
% \begin{tabular}{lcccccccc}
% \toprule
% \textbf{Data Ratio} & \textbf{RWQA} & \textbf{MME\textsuperscript{P}} & \textbf{MMB} & \textbf{VQA\textsuperscript{ok}} \\
% \midrule
% \textbf{\emph{LLava-VDEP-7B}} \\
% % \emph{w}/ 0.5 & -- & -- & -- & -- \\
% \emph{w}/ 0.5 & 54.25 & 1439.16 & 58.90 & 55.80 \\
% \emph{w}/ 0.8 & 57.12 & 1509.47 & 59.36 & 57.26 \\
% \midrule
% \rowcolor[HTML]{ededed}
% \emph{w}/ 1.0 & 57.64 & 1515.60 & 62.52 & 57.68 \\
% \bottomrule
% \end{tabular}%
% }
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}
To further investigate VDEP's capabilities in visual perception, we conduct experiments on the MME benchmark, which evaluates both perceptual and cognitive abilities. The results, summarized in Table \ref{MME_benchmark}, show notable improvements in visual perception, with the 3B model achieving substantial gains across all metrics. We also evaluate the VDEP
\begin
{table}[t]
\caption
{Ablation study on the hyperparameter Data Ratio, which represents the proportion of different VDEP and LLava patterns in the pre-training stage.}
\label{data_ratio}
\vskip
 0.15in
\begin
{center}
\begin
{small}
\begin
{sc}
\scalebox
{0.9}{
\begin
{tabular}{lcccc}
\toprule
\textbf{Data Ratio} & \textbf{RWQA} & \textbf{MME\textsuperscript{P}} & \textbf{MMB} & \textbf{VQA\textsuperscript{ok}} \\
\midrule
\textbf{\emph{LLava-VDEP-7B}} \\
\emph{w}/ 0.5 & 54.25 & 1439.16 & 58.90 & 55.80 \\
\emph{w}/ 0.8 & 57.12 & 1509.47 & 59.36 & 57.26 \\
\midrule
\rowcolor
[HTML]{ededed}
\emph{w}/ 1.0 & \textbf{57.64} & \textbf{1515.60} & \textbf{62.52} & \textbf{57.68} \\
\bottomrule
\end
{tabular}
}
\end
{sc}
\end
{small}
\end
{center}
\vskip
 -0.1in
\end
{table}
series on mainstream multimodal benchmarks to ensure a fair comparison and achieve competitive results.
On the MM-Vet benchmark, which evaluates the integration of visual and language capabilities across 16 complex multimodal tasks, the VDEP 3B model improves by 9.09\% over the baseline. On the MMT-Bench benchmark, which assesses advanced reasoning and instruction-following abilities across 32 meta-tasks and 162 sub-tasks, the VDEP series consistently outperforms the baseline, demonstrating its strengths in visual recognition, localization, reasoning, and planning. On the MMBench benchmark, which measures 20 fine-grained multimodal abilities, the VDEP series delivers a performance that matches or surpasses the state-of-the-art.
Lastly, on the AI2D benchmark, a dataset focused on multiple-choice questions involving scientific diagrams with textual content, the VDEP model achieves leading performance, highlighting its strong ability to understand textual information within images.






\subsection{Further Analysis}
% The sources of these performance improvements can be further understood through attention visualization results \ref{lighthouse}. Compared to LLava-v1.5, VDEP exhibits a more focused and precise attention distribution, which effectively highlights key objects and regions in images. For example, in VQA\textsuperscript{ok}, VDEP’s attention maps clearly delineate the contours of core objects, while the baseline shows a more scattered attention pattern, leading to difficulty in identifying critical areas. In the challenging scenarios of VizWiz, VDEP’s attention mechanism filters out noise from low-quality images and concentrates on essential visual elements. For multi-turn question-answering tasks, VDEP maintains consistent attention on important regions across deeper layers, facilitating better contextual understanding. In SQA\textsuperscript{I}, its attention maps capture object boundaries and spatial relationships with high precision, providing superior visual features for scene-based question-answering. These results demonstrate that the combination of dynamic embedding and autoregressive training enables VDEP to achieve significant improvements in image information reconstruction and multimodal task performance.

% Although VDEP achieves significant performance improvements on the VQE benchmark across different scales, certain scales show performance degradation in mainstream multimodal benchmarks and MME. For the MME-perception and MME-count tasks, the 3B model demonstrates notable improvements, while the 7B model performs slightly worse than the baseline. This discrepancy may arise from the weaker feature extraction capabilities of smaller LLMs, where increasing the proportion of image loss encourages better restoration of image information. VDEP's emphasis on image contours enhances LLMs' ability to extract and integrate semantic information from image instances. However, for larger LLMs with significantly improved image feature extraction capabilities, an overly high proportion of image loss can disrupt the balance of attention between visual and textual information.
% Interestingly, MMBench reveals a contrasting trend. The 3B model exhibits performance degradation in both Chinese and English scenarios, whereas the 7B model achieves substantial improvements. This could be attributed to the 3B model's excessive reliance on visual supervision, which compromises its language processing capabilities. In contrast, the 7B model appears to better balance visual and linguistic features, leading to enhanced performance in both Chinese and English tests.
MLLMs are trained in two stages. During the pre-training stage, the objective is to ensure that the semantic representations of different modalities, such as text and images, are aligned in a shared latent space. In the Supervised Fine-Tuning (SFT) stage, text tokens are leveraged to query image tokens, enabling tasks that require instance-level semantic understanding. Figure \ref{lighthouse} illustrates how VDEP produces more concentrated attention maps than baseline models, with clear contours outlining core objects. This visualization highlights VDEP's ability to focus on key visual elements, aiding problem-solving. (1) \textbf{Denoising.} This advantage is particularly evident in the VizWiz dataset, where low-quality images and key instance objects are off-center. VDEP's attention mechanism effectively suppresses noise in low-quality images and focuses on critical visual elements, resulting in substantial performance improvements.
(2) \textbf{Location.} VDEP consistently focuses on key regions for multi-turn question-answering tasks, enhancing its contextual understanding. In the MME task, VDEP enhances the ability to localize key instances within images, demonstrating that text tokens can precisely correspond to critical positions in the visual input. These results highlight the strong modality alignment capabilities of VDEP during the pre-training stage. (3) \textbf{Capture.} In VQA task, VDEP's attention maps accurately capture object boundaries and spatial relationships, providing superior visual features for scene-based question-answering. 

However, VDEP performs poorly on specific mainstream multimodal benchmarks, such as MMBench, despite these strengths, when using the 3B model. Specifically, the 3B model performs waaker in Chinese and English scenarios. This performance gap may be attributed to an over-reliance on visual supervision, potentially limiting the model's ability to process complex linguistic inputs. In contrast, the 7B model achieves notable improvements, indicating a better balance between visual and language features. This ultimately leads to enhanced performance on both Chinese and English tests. In the ablation study, we analyze the impact of varying image loss hyperparameters across different model scales, identifying the optimal range for balancing visual and language feature integration.

% \begin{figure}[!t]
%   \centering
%   \begin{minipage}{\columnwidth} % 限制范围在单栏
%     \centering
%     \includegraphics[width=\columnwidth]{training loss.png}
%     \caption{*******}
%     \label{loss_png}
%   \end{minipage}
% \end{figure}

\subsection{Ablation Study}
\textbf{Hyperparameters $\mathbf{\alpha}$.} As illustrated in Table \ref{alpha}, with decreasing hyperparameter $\alpha$, the overall performance of the model exhibits a consistent improvement in performance metrics across multiple benchmarks. This observation suggests that reducing the weight assigned to the image loss notably improves the model's performance. 
The underlying reason for this phenomenon lies in the disparity between the number of image tokens and text tokens, with the former being significantly larger. This imbalance often leads to a higher proportion of background tokens in image data. When $\alpha$ is relatively large, the model tends to overfit these background tokens, i.e., the model disproportionately focuses on less informative regions of the image, thereby introducing noise that impairs the effectiveness of text alignment during training. 
By contrast, a smaller $\alpha$ alleviates the constraints of image reconstruction, reducing the influence of background noise and enabling more effective text-image alignment, thereby promoting superior performance in multimodal tasks.

\textbf{Hyperparameters Data Ratio.} As shown in Table \ref{data_ratio}, we utilize the VDEP framework to train the model with varying text-to-image data ratios and assess its performance across multiple multimodal benchmarks. By adjusting the ratio of VDEP mode to LLava mode within a batch during pre-training, we control the proportion of image reconstruction data, where a higher ratio indicates a greater amount of image data.
The results in Table \ref{data_ratio} demonstrate a clear trend: as the proportion of image data increases, the model's overall performance improves consistently across multiple test datasets. This phenomenon is attributed to the greater challenge of simultaneously optimizing regression tasks for both images and text, as it requires balancing competing objectives compared to optimizing only the text regression task. A lack of sufficient image data during pre-training leads to suboptimal learning of all tasks, resulting in weaker alignment between modalities and ultimately degrading the model’s overall performance.



% \begin{table*}[ht]
% \centering
% \small
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Method} & \multicolumn{3}{c}{\textbf{POPE-acc}} & \multicolumn{3}{c}{\textbf{POPE-f1}} \\
% \cmidrule(lr){2-4} \cmidrule(lr){5-7}
%  & rand & pop & adv & rand & pop & adv \\
% \midrule
% LLava-v1.5-7b & 88.33 & 87.13 & 85.63 & 87.09 & 85.95 & 84.52 \\
% LLava-v1.5-7b-fim-v1 & 88.50 & 87.57 & 86.03 & 87.24 & 86.34 & 84.91 \\
% LLava-v1.5-7b-mask\_strategy-1212 & 88.27 & 87.10 & 85.63 & 86.98 & 85.87 & 84.51 \\
% LLava-v1.5-7b-autoregressive-lossweight-bsx2 & 88.27 & 87.13 & 85.80 & 86.98 & 85.90 & 84.65 \\
% LLava-v1.5-7b-fim-v2-bsx2 & 88.83 & 87.37 & 86.07 & 87.65 & 86.25 & 85.05 \\
% 全部自回归 & 83.63 & 80.47 & 73.83 & - & - & - \\
% LLava-v1.5-7b-autoregressive-avglen-cross & 86.60 & 85.03 & 84.00 & 83.33 & 82.79 & 81.79 \\
% LLava-v1.5-7b-fim-v2-cross-correct-bsx2-SFT & 86.37 & 85.73 & 84.23 & 84.34 & 83.73 & 82.32 \\
% LLava-v1.5-7b-auto-s-cross-correct-bsx2-SFT & 84.50 & 83.93 & 83.07 & - & - & - \\
% LLava-v1.5-7b-fim-v2-l2-correct-bsx2 & 88.20 & 87.20 & 86.10 & 86.84 & 85.88 & 84.80 \\
% LLava-v1.5-7b-fim-v2-l2-correct-bsx2-again & 86.20 & 85.83 & 84.50 & - & - & - \\
% LLava-v1.5-7b-auto-s-l2-correct-bsx2-SFT & 85.63 & 85.37 & 84.20 & - & - & - \\
% LLava-v1.5-7b-auto-l2-ratio-correct-bsx1-SFT & 86.37 & 85.90 & 84.47 & - & - & - \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of Methods on POPE-acc and POPE-f1 metrics.}
% \label{tab:pope_results}
% \end{table*}









% \begin{table*}[t]
% \centering
% \caption{Comparison with SoTA methods on academic-task-oriented datasets. }
% \label{loss_func_table}
% % \resizebox{\textwidth}{!}
% \scalebox{1.0}{%{%
% \begin{tabular}{lcccccccc}
% \toprule
% \textbf{Loss Function} & \textbf{VQA\textsuperscript{v2}} & \textbf{VQA\textsuperscript{ok}} & \textbf{GQA} & \textbf{VizWiz} & \textbf{VQA\textsuperscript{T}} & \textbf{RWQA} & \textbf{SQA\textsuperscript{I}} \\
% \midrule
% \textbf{\emph{TinyLLava-VDEP-3B}} \\
% \emph{w}/ $X^{-1}$ & -- & -- & -- & -- & -- & -- & -- \\
% \emph{w}/ Sigmoid& -- & -- & -- & -- & -- & -- & -- \\
% \rowcolor[HTML]{ededed}
% \emph{w}/ L2  & -- & 57.97 & 61.67 & 37.58 & 51.73 & 54.25 & 71.39 \\
% \midrule
% \textbf{\emph{LLava-VDEP-7B}} \\
% \emph{w}/ $X^{-1}$ & -- & -- & -- & -- & -- & -- & -- \\
% \emph{w}/ Sigmoid & -- & 57.37 & 62.95 & -- & 46.67 & 57.90 & 68.32 \\
% \midrule
% \rowcolor[HTML]{ededed}
% \emph{w}/ L2  & -- & 57.68 & 62.50 & -- & 46.76 & 57.64 & 68.36 \\
% \midrule
% \textbf{\emph{LLava-VDEP-13B}} \\
% \emph{w}/ $X^{-1}$ & -- & -- & -- & -- & -- & -- & -- \\
% \emph{w}/ Sigmoid  & -- & -- & -- & -- & -- & -- & -- \\
% \midrule
% \rowcolor[HTML]{ededed}
% \emph{w}/ L2  & -- & -- & -- & -- & -- & -- & -- \\
% \bottomrule
% \end{tabular}%
% }
% \end{table*}
% \FloatBarrier





% \begin{table}[t]
% \centering
% \caption{Comparison with SoTA methods on academic-task-oriented datasets. }
% \label{alpha}
% % \resizebox{\textwidth}{!}
% \scalebox{0.9}{%{%
% \begin{tabular}{lcccccccc}
% \toprule
% \textbf{$alpha$} & \textbf{MMB} & \textbf{MME\textsuperscript{P}} & \textbf{MME\textsuperscript{C}} & \textbf{VizWiz} \\
% \midrule
% \textbf{\emph{TinyLLava-VDEP-3B}} \\
% \emph{w}/ 0.1 & -- & -- & -- & -- \\
% \emph{w}/ 0.01 & -- & -- & -- & -- \\
% \midrule
% \rowcolor[HTML]{ededed}
% \emph{w}/ 0.001 & 54.29 & 1499.08 & 158.33 & --\\
% \midrule
% \textbf{\emph{LLava-VDEP-7B}} \\
% \emph{w}/ 0.1 & -- & -- & -- & -- \\
% \emph{w}/ 0.01 & 62.52 & 1504.99 & 153.3 & -- \\
% \midrule
% \rowcolor[HTML]{ededed}
% \emph{w}/ 0.001  & 62.52 & 1515.60 & 153.3 & -- \\
% \midrule
% \textbf{\emph{LLava-VDEP-13B}} \\
% \emph{w}/ 0.1  & -- & -- & -- & -- \\
% \emph{w}/ 0.01  & -- & -- & -- & -- \\
% \midrule
% \rowcolor[HTML]{ededed}
% \emph{w}/ 0.001  & -- & -- & -- & -- \\
% \bottomrule
% \end{tabular}%
% }
% \end{table}

