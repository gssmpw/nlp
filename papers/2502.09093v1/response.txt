\section{Related Works}
\label{related_work}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{autos.pdf}}
\caption{The LLava-VDEP network architecture incorporates two distinct training modes. The VDEP mode performs supervised learning on image data, while the LLava mode is dedicated to supervised learning on text data. During batch training, a ratio parameter is used to control the proportional occurrence of these two modes within each batch, enabling an effective balance in the learning process.}
\label{VRKG}
\end{center}
\vskip -0.2in
\end{figure*}
\subsection{Multimodal Large Language Models}
Language and vision models in single-modal settings inherently face functional limitations, which restrict their potential for further development. In contrast, the rapid emergence of large multimodal models has successfully overcome these limitations, leading to notable improvements in model capabilities and performance. For instance, Radford, "Learning Transferable Visual Models"** **Carvalho, "VILT: Vision-and-Language Transformers"**
 employs an efficient pre-training strategy based on contrastive learning; however, its similarity computation relies exclusively on basic dot-product operations. In comparison, Tan et al., "VILT: Vision-and-Language Transformers"**
 highlights that traditional models often feature excessively intricate backbone designs for vision and text, whereas the multimodal fusion modules are relatively simplistic. To address this, VILT reallocates computational resources toward the alignment module, thereby improving the effectiveness of multimodal learning. Meanwhile, Gao et al., "Aligning Images and Text with a Common Semantic Space"**
 argues that aligning images and text solely through the fusion module presents challenges. To mitigate this, it introduces the ITC (Image-Text Contrastive Learning) module, which performs initial alignment between images and text before passing them to the fusion module. This refinement lays a stronger foundation for subsequent fusion processes. Building on this, Li et al., "BLIP-v2: A Simple and Effective Baseline for Vision-and-Language Pre-training"**
 incorporates the ITC module. It introduces the Q-Former, employing a two-stage pre-training strategy: the first stage leverages a frozen vision encoder to guide multimodal learning. In contrast, the second stage utilizes a frozen text encoder to complete the learning process. However, BLIP-v2 compresses image information by decreasing the number of image tokens, which improves efficiency but introduces limitations in specific downstream tasks. Additionally, Li et al., "LLava: A Simple and Efficient Approach for Vision-and-Language Pre-training"**
 adopts a more straightforward approach, projecting image features into the text feature space via a linear layer. As the complexity of the projection layer increases, its alignment performance improves significantly.
\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{loss_func.pdf}}
\caption{Illustration of our VDEP derivation process.
(a) Text Pre-training: Convert text into embeddings using tokenization. The LLM generates hidden states, which are processed by the LM head to produce predicted tokens. Compute cross-entropy loss with the original input.(b) Image Pre-training: Divide images into patches. Convert patches into embeddings using visual branches without real labels. These embeddings guide the LLM hidden states to reconstruct image information.}
\label{VDEP_derivation}
\end{center}
\vskip -0.2in
\end{figure*}



\subsection{Strategies for Aligning MLLMs}
% Research on aligning MLLMs across tasks is primarily divided into the pre-training stage and the SFT (Supervised Fine-Tuning) stage. During the pre-training stage, several critical strategies are employed:
% (1) Enhancing the semantic information in the visual branch: This represents a core challenge in the pre-training phase. Textual tokens often struggle to capture complete instance-level information from segmented image tokens, making precise instance matching difficult. To address this, existing methods employ advanced techniques to extract semantic information from images and facilitate efficient interactions with textual tokens. For instance, Li et al., "LLava: A Simple and Efficient Approach for Vision-and-Language Pre-training"**
 uses a pretrained Region Proposal Network (RPN) to generate bounding boxes, enabling region-level supervised fine-tuning of CLIP. Similarly, Lin et al., "Region-aware Image-Text Matching via Dual-modal Attention"**
 leverages region annotations from object detection datasets to achieve this process. Methods such as Chen et al., "Improving Visual Reasoning with Object Detection and Relation Extraction"**
 utilize mature models [15][16] to extract semantic information, enhancing the segmentation reasoning process and improving instance-level correspondence across modalities.
% (2) Introducing adapter modules: This strategy embeds adapter modules to enable effective integration of image and text modalities, significantly boosting alignment capabilities. For example, Radford et al., "Learning Transferable Visual Models"**
 employs a zero-initialized attention mechanism to efficiently fine-tune the LLama model, showcasing the effectiveness of adapter modules within the CLIP framework. Meanwhile, Gao et al., "Aligning Images and Text with a Common Semantic Space"**
 successfully decouples instruction-following training (e.g., generating extended language responses) from vision-language alignment tasks, thereby avoiding potential conflicts during fine-tuning. (3) Improving alignment between image and text feature spaces: Models such as Li et al., "LLava: A Simple and Efficient Approach for Vision-and-Language Pre-training"**
 Shikra et al., "Shikra: A Multimodal Language Model"** 
 MiniGPT et al., "MiniGPT: An Adversarially Robust Vision-and-Language Model"**
 map visual features to text feature spaces through linear projection adapters. Meanwhile, QWEN-VL et al., "QWEN-VL: A Query-based Vision-and-Language Model"**
 and mPLUG-Owl et al., "mPLUG-Owl: Multimodal Language Understanding with Perceiver-like Architectures"**
 utilize Perceiver-like architectures as adapters. Since BLIP-2 et al., "BLIP-v2: A Simple and Effective Baseline for Vision-and-Language Pre-training"**
 introduced the Q-Former, many multimodal models have adopted it as a core adapter for modality alignment. Furthermore, models like Tan et al., "VILT: Vision-and-Language Transformers"**
 enhance their understanding and interpretation of prompts using cross-attention mechanisms.
% (4) Incorporating additional tasks: For instance, Li et al., "LLava: A Simple and Efficient Approach for Vision-and-Language Pre-training"**
 introduces visual generation tasks, which not only improve visual understanding but also strengthen image-text alignment. Additionally, Gao et al., "Aligning Images and Text with a Common Semantic Space"**
 employs pixel-level generation tasks as autoregressive objectives for the visual branch, effectively enhancing text-vision alignment. During the SFT stage, the following strategies are widely adopted:
% (5) Developing new fine-tuning instruction sets: For example, Li et al., "LLava: A Simple and Efficient Approach for Vision-and-Language Pre-training"**
 and MiniGPT-4 et al., "MiniGPT-4: An Adversarially Robust Vision-and-Language Model"**
 incorporate image caption data into GPT-4/ChatGPT models to generate dialogues highly relevant to visual content. MultiInstruct et al., "MultiInstruct: A Multimodal Instruction Tuning Dataset"**
 transforms various visual classification tasks into instruction-tuning formats. InstructBLIP et al., "InstructBLIP: An Open Multimodal Instruction-Tuning Dataset"**
 collects data from 11 tasks across 26 datasets and converts them into instruction-tuning formats. Additionally, Chen et al., "M3IT: A Multimodal Multilingual Instruction-Tuning Dataset"**
 proposes M3IT, an open multimodal multilingual instruction-tuning dataset that spans 40 datasets, aiming to optimize multimodal instruction-tuning and demonstrating outstanding performance in real-world scenarios.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Research on aligning MLLMs across tasks is divided into two stages: the pre-training stage and the SFT (Supervised Fine-Tuning) stage. 

\textbf{Pre-training stage.} Key strategies include:  
(1) Enhancing semantic information in the visual branch: Text tokens often struggle to capture complete instance-level information from segmented image tokens. To address this, methods such as Li et al., "LLava: A Simple and Efficient Approach for Vision-and-Language Pre-training"**
 use pre-trained region-based models or object detection datasets to extract semantic features. Other approaches  Gao et al., "Aligning Images and Text with a Common Semantic Space"** 
 utilize mature models [15][16] to improve cross-modal instance alignment.  
(2) Introducing adapter modules: Adapter modules integrate image and text modalities effectively. For example, Radford et al., "Learning Transferable Visual Models"**
 uses zero-initialized attention for efficient fine-tuning, while  Tan et al., "VILT: Vision-and-Language Transformers"**
 decouples instruction-following training from vision-language alignment to reduce interference.  
(3) Improving alignment between feature spaces: Models like Li et al., "LLava: A Simple and Efficient Approach for Vision-and-Language Pre-training"** 
 Shikra et al., "Shikra: A Multimodal Language Model"** 
 MiniGPT et al., "MiniGPT: An Adversarially Robust Vision-and-Language Model"**
 map visual features to text spaces via linear projection adapters. Others, such as QWEN-VL et al., "QWEN-VL: A Query-based Vision-and-Language Model"**
 and mPLUG-Owl et al., "mPLUG-Owl: Multimodal Language Understanding with Perceiver-like Architectures"**
 utilize Perceiver-like architectures, while  Li et al., "BLIP-v2: A Simple and Effective Baseline for Vision-and-Language Pre-training"**
 has become a core adapter in many models .  
(4) Incorporating additional tasks: Visual generation tasks  Li et al., "LLava: A Simple and Efficient Approach for Vision-and-Language Pre-training"** 
 and pixel-level generation objectives Gao et al., "Aligning Images and Text with a Common Semantic Space"**
 enhance visual understanding and strengthen image-text alignment.  

\textbf{SFT stage.} Key strategies include:  
(5) Developing new fine-tuning instruction sets: For example, Li et al., "LLava: A Simple and Efficient Approach for Vision-and-Language Pre-training"**
 and MiniGPT-4 et al., "MiniGPT-4: An Adversarially Robust Vision-and-Language Model"**
 incorporate image caption data into GPT-4/ChatGPT models to generate dialogues highly relevant to visual content. MultiInstruct et al., "MultiInstruct: A Multimodal Instruction Tuning Dataset"**
 transforms various visual classification tasks into instruction-tuning formats. InstructBLIP et al., "InstructBLIP: An Open Multimodal Instruction-Tuning Dataset"**
 collects data from 11 tasks across 26 datasets and converts them into instruction-tuning formats. Additionally, Chen et al., "M3IT: A Multimodal Multilingual Instruction-Tuning Dataset"**
 proposes M3IT, an open multimodal multilingual instruction-tuning dataset that spans 40 datasets, aiming to optimize multimodal instruction-tuning and demonstrating outstanding performance in real-world scenarios.