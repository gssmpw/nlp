%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
% \usepackage{xcolor}
\usepackage[table,xcdraw,dvipsnames]{xcolor}
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0} % 深绿色 

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{graphicx} % 用于缩小公式环境
\usepackage{relsize}  % 用于调整公式字体大小
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
% \usepackage[table,xcdraw,dvipsnames]{xcolor} % 用于表格的颜色支持
\usepackage{booktabs}            % 用于更美观的表格
\usepackage{graphicx}            % 用于缩放表格
\usepackage{amssymb}             % 支持上下标等符号
\usepackage[section]{placeins}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
% \usepackage{icml2025}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{booktabs} % for professional tables
\usepackage{multirow} % for multi-row table cells
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{hyperref}
\hypersetup{
hidelinks,
colorlinks=true,
allcolors=black,
pdfstartview=Fit,
breaklinks=true
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{}

\begin{document}

\twocolumn[
\icmltitle{From Visuals to Vocabulary: Establishing Equivalence Between\\ Image and Text Token Through Autoregressive Pre-training in MLLMs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Mingxiao Li}{equal,hunyuan}
\icmlauthor{Fang Qu}{equal,ustc}
\icmlauthor{Zhanpeng Chen}{hunyuan}
\icmlauthor{Na Su}{wxg}
\icmlauthor{Zhizhou Zhong}{fudan}
\icmlauthor{Ziyang Chen}{hunyuan}
\\
\icmlauthor{Nan Du}{hunyuan}
\icmlauthor{Xiaolong Li}{hunyuan}
\end{icmlauthorlist}

\icmlaffiliation{hunyuan}{Tencent Hunyuan}
\icmlaffiliation{wxg}{Tencent WXG Group}
\icmlaffiliation{fudan}{Fudan University}
\icmlaffiliation{ustc}{University of Science and Technology of China}

% \icmlcorrespondingauthor{Mingxiao Li}{211050015@hdu.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

% -----------------0-abstract.tex------------
\input{abstract1}
% -----------------1-introduction.tex------------
\input{introduction}
% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=1.0\textwidth]{auto-s.pdf}
%   \caption{The LLava-VDEP network architecture incorporates two distinct training modes. The VDEP mode performs supervised learning on image data, while the LLava mode is dedicated to supervised learning on text data. During batch training, a ratio parameter is used to control the proportional occurrence of these two modes within each batch, enabling an effective balance in the learning process.
% }
%   \label{VRKG}
% \end{figure*}


% -----------------2-related_work.tex------------

\input{related_work}
% -----------------3-method.tex------------

\input{method}

% -----------------4-experiment.tex------------
\input{experiments}
% -----------------5-conclusion.tex------------
\input{conclusion}

\section*{Impact Statement}
% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.
In this paper, we propose a novel paradigm for multimodal alignment, named Vision Dynamic Embedding-Guided Pre-training. Grounded in information theory, this approach incorporates the image reconstruction task as an explicit component of the autoregressive objectives in multimodal large models. This paradigm offers a streamlined and effective framework for aligning MLLMs, emphasizing the critical role and efficacy of image reconstruction in facilitating image-text alignment. The experimental setup and data processing in our study adhere to the principles outlined by the LLava dataset.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

 \input{main.bbl}
% \bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\begin{figure*}[!t]
  \centering
  \includegraphics[width=1.0\textwidth]{attention_multi.pdf}
  \caption{Layer-wise attention visualization of visual-to-instruction information flow. Displayed from top to bottom
are the attention heatmaps from LLava-v1.5-7B and LLava-v1.5-7B-VDEP, respectively. The
example is derived from LLava-Bench (Liu et al., 2024b) and the query is "Describe this photo in detail".
}
  \label{attenion_map}
\end{figure*}

% \section{Formula.}
% \subsection{Problem Formulation}
% %

% In the context of modal alignment pre-training, consider the following scenario. Let the image $I$ be divided into a sequence of patches $P = [p_1, p_2, \ldots]$. After being projected through a Vision Transformer (ViT) and a Multi-Layer Perceptron (MLP), it is represented as the image input $X_i = [x_{i1}, x_{i2}, \ldots]$. Correspondingly, the text description sequence of this image is $X_t = [x_{t1}, x_{t2}, \ldots]$. 

% Typically, the alignment method in Multi-Modal Large Language Models (MLLMs) is to predict $X_t$ given $X_i$. In the autoregressive training mode, this is equivalent to taking $X_i$ and $X_t$ as known inputs and comparing the distribution difference between the model's text prediction output and $X_t$. This can be formulated as:

% \begin{equation}
% L_t = \text{CrossEntropy}(X_t|X_i, X_t^{\text{Predict}}|X_i)
% \end{equation}

% This alignment process originates from the traditional pre-training paradigm of Language Models (LLMs). By aligning the input and output text tokens, LLMs are trained to reconstruct text information. This can be formulated as:

% \begin{equation}
% L_t = \text{CrossEntropy}(X_t, X_t^{\text{Predict}})
% \end{equation}

% Although MLLMs adopt the same pre-training paradigm as LLMs, in this process, image information is always treated as prior input information. This leads to an explicit inconsistency between image information and text information at the training level.

% As shown in Figure \ref{VDEP_derivation}.a, we reconsider MLLMs from the perspective of information theory. As shown in Equation \ref{Mutual_Information}, the goal of autoregressive models is to minimize the mutual information $I(X; \hat{X})$ between the input text $X$ and the predicted text $\hat{X}$, aiming to reconstruct the information contained in the input. The overall expectation of training is:

% \begin{equation}
% \max[I(X_t, X_t^{\text{predict}}) = H(x_t) - H(X_t|X_t^{\text{predict}})]
% \end{equation}

% When the mutual information $I$ becomes larger, we have:

% \begin{equation}
% I(X_t, X_t^{\text{predict}}) \to H(X_t), H(X_t|X_t^{\text{predict}}) \to 0
% \end{equation}

% This indicates that the predicted text effectively reconstructs the input text. Since the cross-entropy loss is closely related to $H(X|\hat{X})$, minimizing the cross-entropy loss can reduce this uncertainty and ensure optimal information reconstruction.

% \subsection{Dynamic Vision Autoregressive Training}

% Unlike text, where token-level cross-entropy can guide reconstruction, image tokens lack explicit semantic labels, making the direct application of cross-entropy infeasible. Based on the overall training expectation of autoregression in information theory, we establish an autoregressive training expectation for images:

% \begin{equation}
%     \max[I(H_I, X_I) = H(X_I) - H(X_I|H_I)]
% \end{equation}



% where $X_I$ is the image token embedding, and $H_I$ is the corresponding hidden vector of the LLM at the corresponding position. This process is shown in Figure 3b. In image-related tasks, we use the perspective of embedding for input and output information reconstruction to achieve the same training goal as text tokens. To maximize the mutual information $I$, we propose introducing dynamic visual input $X_I$ into the training objective to increase the input information contained in the output information:

% \begin{equation}
% \mathcal{L}_v = \sum_{j=1}^{K} \left\lVert h_v^j - z_v^j \right\rVert_2^2
% \end{equation}

% In the ideal case, the smaller the distance, the closer the semantic information predicted by the encoded image is to the semantic information in the input image.

% \begin{equation}
% L_i \to 0, H_i \to X_i, I(H_I, X_I) \to H(H_I), H(X_I|X_I) \to 0
% \end{equation}

% This is similar to processing text tasks during reconstruction, where the predicted text aims to recover the information of the underlying true text.


% \subsection{Problem Formulation and Framework Overview}

% Our model refines the LLava pre-training paradigm to enhance alignment with the visual modality. In MLLMs, an input image $M$ is divided into $I$ non-overlapping patches. Each patch is processed by a Vision Transformer (ViT) and then projected into the LLava hidden state space using a MLP. This process generates the image token embeddings:
% $
% Z_v = \{z^v_1, z^v_2, \dots, z^v_I\}.
% $
% Similarly, the input text sequence is tokenized into $T$ subwords:
% $
% X = \{x_1, x_2, \dots, x_T\},
% $
% which are transformed through an embedding layer to produce text token embeddings:
% $
% Z_t = \{z^t_1, z^t_2, \dots, z^t_T\}.
% $
% The concatenated embeddings $[Z_v; Z_t]$ serve as input for training. The model predicts subword tokens of a target text sequence:
% $
% \hat{X} = \{\hat{x}_1, \hat{x}_2, \dots, \hat{x}_T\}.
% $
% During the pre-training stage, the ViT and LLM parameters are frozen, while the MLP is optimized to map visual features into the text representation space. This ensures that the visual modality aligns with the existing text space without disrupting pretrained parameters. The alignment objective is defined as:
% \begin{equation}
% \begin{aligned}
%     \textstyle
%     \mathcal{L}_t = -\sum_{k=1}^{T} \log P(\hat{x}_k \mid \hat{x}_1, \dots, \hat{x}_{k-1}, Z_v, Z_t)
% \end{aligned}
% \end{equation}
% where $\mathcal{L}_t$ minimizes the discrepancy between the predicted and ground truth subwords. This objective drives the model to reconstruct input text accurately while accounting for visual input.
% % However, this approach underutilizes image information during pre-training, resulting in a bias toward the textual modality.
% However, this approach focuses exclusively on modeling textual information during the pre-training phase, without adequately incorporating visual information. Consequently, it neglects the critical role of visual task learning in achieving effective multimodal alignment.
% To address this imbalance, we introduce a new objective function, $\mathcal{L}$, which balances the contributions of both visual and textual modalities:
% \begin{equation}
% \begin{aligned}
% \textstyle
% \mathcal{L} = \alpha \mathcal{L}_t + (1 - \alpha) \mathcal{L}_v
% \end{aligned}
% \end{equation}
% where $\mathcal{L}_t$ and $\mathcal{L}_v$ denote the loss functions for text and image modalities, respectively. The parameter $\alpha \in [0, 1]$ controls the relative importance of each modality. By tuning $\alpha$, we can dynamically adjust the balance between modalities to optimize overall model performance.




% \subsection{Hybrid Multimodal Alignment Training}
% The training process of LLava can be divided into two main stages: the pre-training stage and the supervised fine-tuning stage. As shown in Figure \ref{VRKG}, during the pre-training stage, we employed a hybrid multimodal alignment training strategy, combining the VDEP mode and the LLava mode. In the supervised fine-tuning stage, we consistently followed the original LLava framework.

% In the pre-training stage, the process prioritizes optimizing the text space to align the image space with the text space. In MLLMs, the number of text tokens is significantly smaller than the number of image tokens. Training both modalities jointly without distinction can cause the model to overfit background information in images, hindering its ability to match the performance of text-only supervised training. Furthermore, it can introduce noise into the training process, potentially disrupting the alignment between image and text modalities.
% To address this, we utilize the distribution derived from text data to align the image space with the text space. This requires the text distribution to first reach a stable state. Our mixed multimodal alignment strategy, which alternates the optimization of image and text losses during each forward and backward propagation, ensures balanced learning across modalities. Specifically, within each batch, the data is evenly divided, with one half (randomly selected) used to compute the image loss and the other half to compute the text loss. This approach preserves the inherent characteristics of the text distribution while using it to align the image space.



\section{Implementation Details.}
% We use CLIP-ViT-L-14 ($224^2$) as the base image encoder. We first select and pad the input image to a target resolution that effectively captures its details, and split the image into $224^2$ grids. All $224^2$ image patches are encoded by the CLIP image encoder separately and their features are merged back to a single large feature map. We then postprocess the resulting feature map to a flattened list of features. We additionally concatenate the features of a fixed-resolution image to provide the model with a global context.

We design a series of experiments to rigorously evaluate the effectiveness of our proposed method across models of varying scales. These experiments involve two models with different parameter sizes: 3B, 7B.
For the 3B model, we use the TinyLLava architecture in our experiments. Within this framework, SigLIP is the visual encoder, while Phi-2 is the language model. For the 7B model, we use the pre-trained CLIP ViT-L/14 ($336^2$) as the visual encoder, combined with the Vicuna v1.5 language model for experiments.
pre-training is conducted on the CC-558K dataset with a $1\times10^{-3}$ learning rate. After pre-training, fine-tuning is performed on the mix-665K dataset with a learning rate of $2\times10^{-5}$. All experiments are conducted on a hardware system with eight NVIDIA A100 GPUs, each with 40GB of memory, to meet the computational requirements. In addition, detailed training steps and specific rules of the implementation plan are fully presented in the appendix.
Our training strategy employs a mixed autoregressive pre-training approach with a strict 1:1 ratio of image data to text data. The image data is sourced from the CC-558K dataset, as in pre-training.. During the SFT stage, our experimental settings match the LLava models.


\section{BenchMarks.}
\subsection{Visual Question Answering}
We conduct experiments on visual question-answering benchmarks, including, OK-VQA, GQA, VizWizQA, TextVQA, RealWorldQA, and ScienceQA. 
OK-VQA includes questions that necessitate external knowledge beyond the multimodal inputs provided. 
GQA is specifically designed to assess the reasoning capabilities of the model. 
VizWizQA is composed of question-answer pairs derived from visually impaired users. 
TextVQA places a greater emphasis on evaluating the model's ability to comprehend text within natural scenes. 
RealWorldQA is a benchmark specifically designed to evaluate the spatial understanding capabilities of multimodal AI models in real-world contexts. 
ScienceQA comprises multimodal multiple-choice questions across a diverse range of science topics. 
These datasets are strategically selected to evaluate our method's capacity to understand comprehensively and reason across diverse visual contexts and knowledge domains.


\textbf{OK-VQA:} OK-VQA(Outside Knowledge VQA)\cite{marino2019ok} is a visual question answering dataset that requires external knowledge. The answers to the questions cannot be inferred solely from the image but also need to incorporate common sense or world knowledge. This dataset evaluates the model's ability in the intersection of vision and knowledge reasoning.  

\textbf{GQA:} GQA(Graph Question Answering)\cite{hudson2019gqa} generates questions and answers based on image scene graphs, focusing on structured reasoning. It emphasizes logical analysis and challenges the model's depth of understanding of semantics and context. 

\textbf{VizWizQA:} VizWizQA\cite{Bigham_Jayant_Ji_Little_Miller_Miller_Tatarowicz_White_White_Yeh_2010} is designed for visually impaired users, with questions originating from real user requests. The images exhibit strong diversity in quality and content. This dataset includes more noise and ambiguous information, making it suitable for evaluating models in real-world application scenarios.  

\textbf{TextVQA:} TextVQA\cite{singh2019towards} focuses on textual information in images, requiring models to recognize and comprehend text within images to answer questions. It drives research on the integration of visual and textual information, expanding the boundaries of visual question answering.

\textbf{RealWorldQA:} RealWorldQA\cite{grok15v} features images and questions sourced from real-world scenarios, encompassing diverse content from daily life. The dataset imposes higher requirements on the model's generalization ability and adaptability to complex scenes.

\textbf{ScienceQA:} ScienceQA\cite{lu2022learn} is a multimodal question answering dataset combining images and scientific questions, covering multiple scientific topics such as physics and biology. It bridges AI technology with the field of science education, promoting intelligent question answering applications in educational contexts.

\subsection{General Multimodal Benchmarks}
We evaluate our proposed method on general multimodal benchmarks, including MME, MMBench, SEED-Bench, POPE, AI2D, MM-Vet, MMMU, and MMT-Bench. 
MME measures both perception and cognition abilities on a total of 14 subtasks. 
MMBench comprehensively evaluates a model's multimodal capabilities in Chinese and English contexts. 
SEED-Bench focuses on assessing generative comprehension in multimodal large language models. POPE evaluates the extent of multimodal hallucinations present in a model. 
AI2D assesses a model's ability to interpret scientific diagram inputs. MM-Vet evaluates the multimodal conversational skills of a model using GPT-4 as a benchmark. 
MMMU is designed to assess multimodal models on extensive multi-disciplinary tasks that require college-level subject knowledge and deliberate reasoning. 
MMT-Bench is a comprehensive benchmark developed to evaluate MLLMs across a wide range of multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning.
These diverse benchmarks provide a comprehensive framework for evaluating the performance and capabilities of our proposed method in multimodal learning.

\textbf{MME:} MME\cite{fu2024mmecomprehensiveevaluationbenchmark}, short for Multimodal Evaluation
, is a comprehensive multimodal benchmark designed to evaluate the ability of models to understand and process information across multiple modalities, including vision, text, and audio. It provides a standardized framework to measure performance on tasks requiring cross-modal reasoning and understanding, making it an essential tool for assessing the generalization of multimodal large language models (MLLMs).


\textbf{MMBench} MMBench(Multimodal Benchmark)\cite{liu2025mmbench} is a task-driven benchmark that focuses on systematically evaluating multimodal models across diverse real-world application scenarios, such as visual question answering, image captioning, and video understanding. Its emphasis on practical use cases highlights its importance for assessing the practical utility of MLLMs.


\textbf{SEED:} SEED(Spatial and Entity-aware Evaluation Dataset)\cite{li2023seed} is a benchmark specifically designed to evaluate the spatial and entity reasoning capabilities of multimodal models. By incorporating complex spatial relationships and entity-based queries, SEED tests a model’s ability to perform fine-grained reasoning, which is critical for tasks such as scene understanding and object-oriented question answering.


\textbf{POPE:} POPE(Perceptual and Object-aware Performance Evaluation)\cite{li2023evaluating} focuses on evaluating the perceptual understanding and object-centric reasoning of multimodal models. It emphasizes tasks like object detection, recognition, and spatial awareness, making it a key benchmark for assessing models' performance in visually grounded tasks.


\textbf{AI2D:} AI2D(Allen Institute for AI Diagram Dataset)\cite{kembhavi2016diagram} is a dataset centered on diagram understanding, designed to evaluate models' abilities to process non-photographic visual content. It focuses on reasoning over diagrams and charts, making it vital for tasks requiring scientific and technical visual comprehension.


\textbf{MM-Vet:} MM-Vet(Multimodal Model Veterinary)\cite{yu2023mm} is a diagnostic benchmark aimed at identifying the strengths and weaknesses of multimodal models across various dimensions, such as robustness, interpretability, and cross-modal alignment. It is a critical tool for debugging and improving the reliability of MLLMs.


\textbf{MMMU:} MMMU(Multimodal Multitasking Understanding)\cite{yue2024mmmu} evaluates the multitasking capabilities of multimodal models by testing their performance on multiple simultaneous tasks across different modalities. This benchmark is essential for assessing the adaptability and efficiency of models in dynamic, multitask scenarios.


\textbf{MMTB:} MMTB(Multimodal Task Benchmark)\cite{ying2024mmt} is a broad benchmark designed to evaluate the performance of multimodal models on a wide range of tasks, including vision-and-language navigation, multimodal reasoning, and image captioning. Its diversity makes it a strong indicator of a model’s overall multimodal proficiency.


\textbf{OCRB:} OCRB (Optical Character Recognition Benchmark)\cite{mishra2019ocr} is a specialized benchmark for assessing a model's ability to recognize and interpret text in images. It focuses on OCR-related tasks, such as text detection, transcription, and contextual understanding, which are crucial for applications like document analysis and scene-text understanding.





\section{Detailed experiments.}
We present comprehensive ablation results derived from LLava-v1.5 to substantiate the experimental conclusions in the main text. Additionally, we performed ablation studies on the image loss function to demonstrate the simplicity and effectiveness of the L2 loss.

\begin{table}[ht]
\caption{Ablation study on the hyperparameter Data Ratio, which represents the proportion of different VDEP and LLava patterns in the pre-training stage.}
\label{data_ratio_all}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\scalebox{1.0}{%{%
\begin{tabular}{lcccccccc}
\toprule
\textbf{Data Ratio} & \textbf{AI2D} & \textbf{MM-Vet} & \textbf{MMMU} & \textbf{MMT} & \textbf{GQA} & \textbf{VizWizQA} & \textbf{VQA\textsuperscript{T}} & \textbf{SQA\textsuperscript{I}} \\
\midrule
\textbf{\emph{LLava-v1.5-VDEP-7B}} \\
\emph{w}/ 0.5   & 54.02 & 29.00 & 31.20 & 46.30 & 61.65 & 49.82 & 46.33 & 68.62 \\
\emph{w}/ 0.8   & 55.18 & 28.20 & \textbf{31.30} & 46.72 & 61.65 & 45.40 & 46.27 & \textbf{69.16} \\
\rowcolor[HTML]{ededed}
\emph{w}/ 1.0   & \textbf{56.57} & \textbf{30.60} & 30.80 & \textbf{48.00} & \textbf{62.50} & \textbf{50.37} & \textbf{46.76} & 68.36 \\
\bottomrule
\end{tabular}%
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[ht]
\caption{Ablation study on the hyperparameter $\alpha$, which represents the variation of the image loss weight.}
\label{alpha_study}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\scalebox{1.0}{%{%
\begin{tabular}{lcccccccc}
\toprule
\textbf{$\alpha$} & \textbf{AI2D} & \textbf{MM-Vet} & \textbf{MMMU} & \textbf{MMT} & \textbf{GQA} & \textbf{VizWizQA} & \textbf{VQA\textsuperscript{T}} & \textbf{SQA\textsuperscript{I}} \\
\midrule
\textbf{\emph{LLava-v1.5-VDEP-7B}} \\
\emph{w}/ 0.1   & 55.57 & 30.50 & 30.60 & 47.64 & 61.45 & 46.76 & 46.52 & 67.72 \\
\emph{w}/ 0.01  & \textbf{56.64} & \textbf{32.20} & \textbf{31.30} & \textbf{48.48} & \textbf{62.63} & \textbf{52.72} & 46.94 & 67.77 \\
\rowcolor[HTML]{ededed}
\emph{w}/ 0.001 & 56.57 & 30.60 & 30.80 & 48.00 & 62.50 & 50.37 & \textbf{46.76} & \textbf{68.36} \\
\bottomrule
\end{tabular}%
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%
\begin{table}[ht]
\caption{Ablation study on the hyperparameter Loss Function on MME.}
\label{loss_function_comparison}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\scalebox{0.9}{
\begin{tabular}{@{}cccccccc@{}}
\toprule
\multirow{2}{*}{\raggedright\textbf{Loss}} & \multirow{2}{*}{\textbf{Perception}}  & \multicolumn{1}{c}{\textbf{Commonsense QA}} & \multicolumn{4}{c}{\textbf{Coarse-grained Perception Tasks}} & \textbf{Total} \\
 \cmidrule(lr){4-7}
& & \textbf{(Reasoning)} & \textbf{Existence} & \textbf{Count} & \textbf{Position} & \textbf{Color} & \textbf{Scores} \\ 
\midrule
\textbf{\emph{LLava-v1.5-VDEP-7B}} & & & & & & & \\ 
\emph{1/L2}      & \textbf{1518.34} & 133.57 & 190.00 & \textbf{163.33} & 135.00 & \textbf{180.00} & \textbf{801.90} \\
\emph{Sigmoid(L2)}  & 1478.45 & 133.57 & 190.00 & 145.00 & \textbf{138.33} & 175.00 & 781.90 \\
\rowcolor[HTML]{ededed} % 浅灰背景
\emph{L2} & 1515.60 & \textbf{136.00} & 190.00 & 153.30 & 135.00 & \textbf{180.00} & 794.30 \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%
\begin{table}[ht]
\caption{Ablation study on the hyperparameter Loss Function on VQA.}
\label{loss_study}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\scalebox{1.0}{%{%
\begin{tabular}{ccccccc}
\toprule
\textbf{Loss} & \textbf{VQA\textsuperscript{ok}} & \textbf{GQA} & \textbf{VizWiz} & \textbf{VQA\textsuperscript{T}} & \textbf{RWQA} & \textbf{SQA\textsuperscript{I}} \\
\midrule
\textbf{\emph{LLava-v1.5-VDEP-7B}} \\
\emph{1/L2}      & 56.11 & 62.47 & \textbf{51.37} & 46.56 & 54.38 & \textbf{69.01} \\
\emph{Sigmoid(L2)}  & 57.37 & \textbf{62.95} & 49.87 & 46.67 & \textbf{57.90} & 68.32 \\
\rowcolor[HTML]{ededed}
\emph{L2} & \textbf{57.68} & 62.50 & 50.37 & \textbf{46.76} & 57.64 & 68.36 \\
\bottomrule
\end{tabular}%
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%
\begin{table}[ht]
\caption{Ablation study on the hyperparameter Loss Function on benchmarks for insruction-following LMMs.}
\label{loss_function_mmbench}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\scalebox{0.9}{
\begin{tabular}{@{}cccccccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Loss}} & \multicolumn{2}{c}{\textbf{MMBench}}  & \multirow{2}{*}{\textbf{AI2D}} & \multirow{2}{*}{\textbf{MM-Vet}} & \multirow{2}{*}{\textbf{MMMU}} & \multirow{2}{*}{\textbf{MMTB}} & \multirow{2}{*}{\textbf{OCRB}} & \multirow{2}{*}{\textbf{POPE}} \\ 
\cmidrule(lr){2-3}
& \textbf{en} & \textbf{cn} & & & & & & & \\ 
\midrule
\textbf{\emph{LLava-v1.5-VDEP-7B}} & & & & & & & \\ 
\emph{1/L2}      & 65.97 & \textbf{58.52} & \textbf{57.09} & 31.10 & \textbf{31.20} & 47.93 & 320 & 85.62 \\
\rowcolor[HTML]{ededed} % 浅灰背景
\emph{Sigmoid(L2)}  & 66.20 & 58.24 & 56.47 & \textbf{31.70} & 31.00 & \textbf{48.32} & \textbf{334} & \textbf{85.98} \\
\emph{L2} & \textbf{66.81} & 58.23 & 56.57 & 30.60 & 30.80 & 48.00 & 326 & 85.95 \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%
\section{Limitation.}
Although VDEP exhibits outstanding performance in improving image-text alignment, it relies on the hyperparameter $\alpha$. While we determine an appropriate range of $\alpha$ for models of varying scales, the optimal value for a given model size remains undetermined. Future work focuses on developing methods to adaptively determine the value of the hyperparameter based on model size and data characteristics. Alternatively, it proposes an effective strategy to eliminate the need for explicit hyperparameter tuning.
During pre-training, to improve the effectiveness of image-related tasks while ensuring no degradation in the performance of text-related tasks, we utilize a dataset with double the training samples of the original. As a result, the training time increases by around 3 hours.
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
