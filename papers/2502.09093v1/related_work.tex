\section{Related Works}
\label{related_work}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{autos.pdf}}
\caption{The LLava-VDEP network architecture incorporates two distinct training modes. The VDEP mode performs supervised learning on image data, while the LLava mode is dedicated to supervised learning on text data. During batch training, a ratio parameter is used to control the proportional occurrence of these two modes within each batch, enabling an effective balance in the learning process.}
\label{VRKG}
\end{center}
\vskip -0.2in
\end{figure*}
\subsection{Multimodal Large Language Models}
Language and vision models in single-modal settings inherently face functional limitations, which restrict their potential for further development. In contrast, the rapid emergence of large multimodal models has successfully overcome these limitations, leading to notable improvements in model capabilities and performance. For instance, CLIP\cite{radford2021learning} employs an efficient pre-training strategy based on contrastive learning; however, its similarity computation relies exclusively on basic dot-product operations. In comparison, VILT\cite{kim2021vilt} highlights that traditional models often feature excessively intricate backbone designs for vision and text, whereas the multimodal fusion modules are relatively simplistic. To address this, VILT reallocates computational resources toward the alignment module, thereby improving the effectiveness of multimodal learning. Meanwhile, ALBEF\cite{li2021align} argues that aligning images and text solely through the fusion module presents challenges. To mitigate this, it introduces the ITC (Image-Text Contrastive Learning) module, which performs initial alignment between images and text before passing them to the fusion module. This refinement lays a stronger foundation for subsequent fusion processes. Building on this, BLIP-v2\cite{li2023blip} incorporates the ITC module. It introduces the Q-Former, employing a two-stage pre-training strategy: the first stage leverages a frozen vision encoder to guide multimodal learning. In contrast, the second stage utilizes a frozen text encoder to complete the learning process. However, BLIP-v2 compresses image information by decreasing the number of image tokens, which improves efficiency but introduces limitations in specific downstream tasks. Additionally, LLava\cite{liu2024visual} adopts a more straightforward approach, projecting image features into the text feature space via a linear layer. As the complexity of the projection layer increases, its alignment performance improves significantly.
\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{loss_func.pdf}}
\caption{Illustration of our VDEP derivation process.
(a) Text Pre-training: Convert text into embeddings using tokenization. The LLM generates hidden states, which are processed by the LM head to produce predicted tokens. Compute cross-entropy loss with the original input.(b) Image Pre-training: Divide images into patches. Convert patches into embeddings using visual branches without real labels. These embeddings guide the LLM hidden states to reconstruct image information.}
\label{VDEP_derivation}
\end{center}
\vskip -0.2in
\end{figure*}



\subsection{Strategies for Aligning MLLMs}
% Research on aligning MLLMs across tasks is primarily divided into the pre-training stage and the SFT (Supervised Fine-Tuning) stage. During the pre-training stage, several critical strategies are employed:
% (1) Enhancing the semantic information in the visual branch: This represents a core challenge in the pre-training phase. Textual tokens often struggle to capture complete instance-level information from segmented image tokens, making precise instance matching difficult. To address this, existing methods employ advanced techniques to extract semantic information from images and facilitate efficient interactions with textual tokens. For instance, \cite{zhong2022regionclip} uses a pretrained Region Proposal Network (RPN) to generate bounding boxes, enabling region-level supervised fine-tuning of CLIP. Similarly, \cite{minderer2022simple} leverages region annotations from object detection datasets to achieve this process. Methods such as \cite{lai2024lisa, yang2023improved, aflalo2024fivl, zhao2023bubogpt} utilize mature models [15][16] to extract semantic information, enhancing the segmentation reasoning process and improving instance-level correspondence across modalities.
% (2) Introducing adapter modules: This strategy embeds adapter modules to enable effective integration of image and text modalities, significantly boosting alignment capabilities. For example, \cite{zhang2023llama} employs a zero-initialized attention mechanism to efficiently fine-tune the LLama model, showcasing the effectiveness of adapter modules within the CLIP framework. Meanwhile, \cite{gao2023llama} successfully decouples instruction-following training (e.g., generating extended language responses) from vision-language alignment tasks, thereby avoiding potential conflicts during fine-tuning. (3) Improving alignment between image and text feature spaces: Models such as LLava \cite{liu2024visual}, Shikra \cite{chen2023shikra}, and MiniGPT \cite{chen2023minigpt} map visual features to text feature spaces through linear projection adapters. Meanwhile, QWEN-VL \cite{bai2023qwen} and mPLUG-Owl \cite{ye2023mplug} utilize Perceiver-like architectures as adapters. Since BLIP-2 \cite{li2023blip} introduced the Q-Former, many multimodal models \cite{zhu2023minigpt, yu2023reformulating} have adopted it as a core adapter for modality alignment. Furthermore, models like \cite{wang2024visionllm, alayrac2022flamingo, gong2023multimodal, li2023ottermultimodalmodelincontext} enhance their understanding and interpretation of prompts using cross-attention mechanisms.
% (4) Incorporating additional tasks: For instance, \cite{tong2024metamorph} introduces visual generation tasks, which not only improve visual understanding but also strengthen image-text alignment. Additionally, \cite{fini2024multimodal} employs pixel-level generation tasks as autoregressive objectives for the visual branch, effectively enhancing text-vision alignment. During the SFT stage, the following strategies are widely adopted:
% (5) Developing new fine-tuning instruction sets: For example, LLava \cite{liu2024visual} and MiniGPT-4 \cite{zhu2023minigpt} incorporate image caption data into GPT-4/ChatGPT models to generate dialogues highly relevant to visual content. MultiInstruct \cite{xu2022multiinstruct} transforms various visual classification tasks into instruction-tuning formats. InstructBLIP \cite{NEURIPS2023_9a6a435e} collects data from 11 tasks across 26 datasets and converts them into instruction-tuning formats. Additionally, \cite{li2023m} proposes M3IT, an open multimodal multilingual instruction-tuning dataset that spans 40 datasets, aiming to optimize multimodal instruction-tuning and demonstrating outstanding performance in real-world scenarios.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Research on aligning MLLMs across tasks is divided into two stages: the pre-training stage and the SFT (Supervised Fine-Tuning) stage. 

\textbf{Pre-training stage.} Key strategies include:  
(1) Enhancing semantic information in the visual branch: Text tokens often struggle to capture complete instance-level information from segmented image tokens. To address this, methods such as \cite{zhong2022regionclip, minderer2022simple} use pre-trained region-based models or object detection datasets to extract semantic features. Other approaches \cite{lai2024lisa, yang2023improved, aflalo2024fivl, zhao2023bubogpt} leverage mature models \cite{radford2021learning} to improve cross-modal instance alignment.  
(2) Introducing adapter modules: Adapter modules integrate image and text modalities effectively. For example, \cite{zhang2023llama} uses zero-initialized attention for efficient fine-tuning, while \cite{gao2023llama} decouples instruction-following training from vision-language alignment to reduce interference.  
(3) Improving alignment between feature spaces: Models like LLava \cite{liu2024visual}, Shikra \cite{chen2023shikra}, and MiniGPT \cite{chen2023minigpt} map visual features to text spaces via linear projection adapters. Others, such as QWEN-VL \cite{bai2023qwen} and mPLUG-Owl \cite{ye2023mplug}, employ Perceiver-like architectures, while Q-Former \cite{li2023blip} has become a core adapter in many models \cite{zhu2023minigpt, yu2023reformulating}.  
(4) Incorporating additional tasks: Visual generation tasks \cite{tong2024metamorph} and pixel-level generation objectives \cite{fini2024multimodal} enhance visual understanding and strengthen image-text alignment.  

\textbf{SFT stage.} Key strategies include:  
(5) Developing new fine-tuning instruction sets: Models such as LLava \cite{liu2024visual} and MiniGPT-4 \cite{zhu2023minigpt} integrate image caption data for visually grounded dialogue. MultiInstruct \cite{xu2022multiinstruct} reformulates visual tasks into instruction-tuning formats, while InstructBLIP \cite{NEURIPS2023_9a6a435e} collects data from 11 tasks for instruction fine-tuning. 
M3IT \cite{li2023large} introduces a multimodal multilingual dataset spanning 40 datasets to optimize instruction tuning.