\section{Method}
\label{method}
% \begin{figure*}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.9\textwidth]{loss_func1.pdf}}
% \caption{Illustration of our VDEP derivation process.
% (a) Text Pre-training: Convert text into embeddings using tokenization. The LLM generates hidden states, which are processed by the LM head to produce predicted tokens. Compute cross-entropy loss with the original input.(b) Image Pre-training: Divide images into patches. Convert patches into embeddings using visual branches without real labels. These embeddings guide the LLM hidden states to reconstruct image information.}
% \label{VDEP_derivation}
% \end{center}
% \vskip -0.2in
% \end{figure*}
\subsection{Problem Formulation}


%Illustration of our VDEP derivation process. (a) The pre-training of LLMs reconstructs input text in an autoregressive manner. The text is tokenized into embeddings, fed into the LLM to generate hidden states, processed by the LM head, and decoded into predicted tokens. These tokens are compared with the input to calculate the loss.
%(b) Images are processed differently. Image patches are passed through a visual branch to produce embeddings, without ground truth tokens. Similar to text, embeddings from the visual branch guide the LLM’s hidden states to reconstruct image information.

In modality-aligned pre-training, MLLMs aim to establish semantic alignment between image representations ($X_i$) and text representations ($X_t$) by learning their mutual relationships. Specifically, an image $I$ is divided into a series of non-overlapping patches $P = [p_1, p_2, \cdots]$, which are processed using a Vision Transformer (ViT) followed by an MLP projection. The resulting representation forms the input embeddings of the image, denoted as $X_i = [x_{i1}, x_{i2}, \cdots]$. Correspondingly, the textual description of the image is represented as a sequence $X_t = [x_{t1}, x_{t2}, \cdots]$.
Typically, MLLMs employ alignment strategies to generate predicted text $X_t^{\text{p}}$ based on image inputs $X_i$. In an autoregressive training paradigm, this involves treating both $X_i$ and $X_t$ as inputs during training and minimizing the divergence between the predicted text $X_t^{\text{p}}$ and the target text $X_t$. The optimization objective can be expressed as:
\begin{equation}
L_t = - \sum_{t} P(X_t | X_i) \log P(X_t^{\text{p}} | X_i)
\end{equation}
This approach is inspired by the traditional pre-training framework of LLMs, where the model is trained to reconstruct input text by minimizing the difference between the input sequence $X_t$ and the predicted sequence $X_t^{\text{p}}$.
% \begin{equation} \textstyle
% L_t = \text{CrossEntropy}(X_t, X_t^{\text
% {p}})
% \end{equation}
\begin{equation}
L_t = - \sum_{t} P(X_t) \log P(X_t^{\text{p}})
\end{equation}
% In multimodal contexts, MLLMs extend this framework to jointly process image $X_i$ and text $X_t$ inputs. 
Within the context of multimodal tasks, MLLMs adopt a training paradigm similar to that of LLMs, prioritizing textual tokens over image data. Consequently, image information is often treated as static prior input, limiting its dynamic integration during the training process. This approach results in an imbalance, where text information dominates over image information during training.
As illustrated in Figure \ref{VDEP_derivation}.a, this inconsistency can be better understood through the lens of information theory. From an information-theoretic perspective, the goal of an autoregressive model is to maximize the mutual information $I(X_t;X_t^{\text{p}})$ between the input text $X_t$ and the predicted text $X_t^{\text{p}}$. This ensures that the model effectively captures and reconstructs the semantic content of the input text. This objective can be formulated as:
\begin{equation} 
\textstyle
I(X_t; X_t^{\text{p}}) = H(X_t) - H(X_t | X_t^{\text
{p}})
\label{LLM_mutaul_info_equ}
\end{equation}
During training, maximizing $I(X_t; X_t^{\text{p}})$ is equivalent to minimizing the conditional entropy $H(X_t | X_t^{\text{p}})$
. This relationship implies that as mutual information increases:
\begin{equation}
\begin{aligned}
H(X_t \mid X_t^{\text{p}}) &\to 0 \\
I(X_t; X_t^{\text{p}}) &\to H(X_t)
\end{aligned}
\end{equation}
Since the cross-entropy loss is inherently tied to $H(X_t | X_t^{\text{p}})$, minimizing the cross-entropy loss effectively reduces uncertainty, enhancing the model’s ability to reconstruct the input text $X_t$ from its predictions $X_t^{\text{p}}$. Analysis of the equation \ref{LLM_mutaul_info_equ} reveals that the visual information $X_i$ is not explicitly integrated into the optimization objective. This discrepancy in multimodal alignment between visual and textual representations leads to three critical challenges:
(1) The feature distribution of image information $X_i$ is not directly optimized within the training objective, which may limit the model's ability to fully leverage visual inputs;
(2) The optimization objective for text generation $X_t^{\text{p}}$ is primarily focused on reconstructing textual information $X_t$
, neglecting the need for effective cross-modal information fusion.
From an information-theoretic perspective, this inconsistency arises because the model fails to fully exploit the image input $X_i$ to maximize the mutual information $I(X_i; X_t^{\text{p}})$
, thereby limiting the overall effectiveness of multimodal alignment.

% Our model refines the LLava pre-training paradigm to enhance alignment with the visual modality. In MLLMs, an input image $M$ is divided into $I$ non-overlapping patches. Each patch is processed by a  (ViT) and then projected into the LLava hidden state space using a MLP. This process generates the image token embeddings:
% $
% Z_v = \{z^v_1, z^v_2, \dots, z^v_I\}.
% $
% Similarly, the input text sequence is tokenized into $T$ subwords:
% $
% X = \{x_1, x_2, \dots, x_T\},
% $
% which are transformed through an embedding layer to produce text token embeddings:
% $
% Z_t = \{z^t_1, z^t_2, \dots, z^t_T\}.
% $
% The concatenated embeddings $[Z_v; Z_t]$ serve as input for training. The model predicts subword tokens of a target text sequence:
% $
% \hat{X} = \{\hat{x}_1, \hat{x}_2, \dots, \hat{x}_T\}.
% $
% During the pre-training stage, the ViT and LLM parameters are frozen, while the MLP is optimized to map visual features into the text representation space. This ensures that the visual modality aligns with the existing text space without disrupting pretrained parameters. The alignment objective is defined as:
% \begin{equation}
% \begin{aligned}
%     \textstyle
%     \mathcal{L}_t = -\sum_{k=1}^{T} \log P(\hat{x}_k \mid \hat{x}_1, \dots, \hat{x}_{k-1}, Z_v, Z_t)
% \end{aligned}
% \end{equation}
% where $\mathcal{L}_t$ minimizes the discrepancy between the predicted and ground truth subwords. This objective drives the model to reconstruct input text accurately while accounting for visual input.
% % However, this approach underutilizes image information during pre-training, resulting in a bias toward the textual modality.
% However, this approach focuses exclusively on modeling textual information during the pre-training phase, without adequately incorporating visual information. Consequently, it neglects the critical role of visual task learning in achieving effective multimodal alignment.
% To address this imbalance, we introduce a new objective function, $\mathcal{L}$, which balances the contributions of both visual and textual modalities:
% \begin{equation}
% \begin{aligned}
% \textstyle
% \mathcal{L} = \alpha \mathcal{L}_t + (1 - \alpha) \mathcal{L}_v
% \end{aligned}
% \end{equation}
% where $\mathcal{L}_t$ and $\mathcal{L}_v$ denote the loss functions for text and image modalities, respectively. The parameter $\alpha \in [0, 1]$ controls the relative importance of each modality. By tuning $\alpha$, we can dynamically adjust the balance between modalities to optimize overall model performance.

\subsection{Dynamic Vision Autoregressive Training}

% As Figure \ref{VDEP_derivation}.a shown, we reexamine MLLMs through the lens of information theory. As shown in Equation \ref{Mutual_Information}, the goal of an autoregressive model is to minimize the mutual information $I(X; \hat{X})$ between the input text $X$ and the predicted text $\hat{X}$
% , aiming to reconstruct the information contained in the input.
% \begin{equation}
%     I(X; \hat{X}) = H(X) - H(X|\hat
% {X})
% \label{Mutual_Information}
% \end{equation}
% Here, $H(X)$ represents the entropy of the input text, while $H(X|\hat{X})$
%  denotes the conditional entropy of the input text given the predicted text.
% In practice, autoregressive training minimizes the conditional entropy $H(X|\hat{X})$, which quantifies the uncertainty of the input text $X$ given the predicted text $\hat{X}$. When $H(X|\hat{X})$ approaches zero, the mutual information $I(X; \hat{X})$ converges to $H(X)$, indicating that the predicted text effectively reconstructs the input text. Since the cross-entropy loss $\mathcal{L}_t$ is closely tied to $H(X|\hat{X})$, minimizing $\mathcal{L}_t$ reduces this uncertainty and ensures optimal information reconstruction.
% For textual data, minimizing mutual information provides a quantitative measure of similarity between predicted tokens and input tokens, facilitating the reconstruction of information. Extending this framework to images, we require a comparable metric to measure how well the model reconstructs input image information. Unlike text, where token-level cross-entropy can guide reconstruction, image tokens lack explicit semantic labels, making direct application of cross-entropy infeasible.
% As Figure \ref{VDEP_derivation}.b shown, in image-related tasks, token embeddings are first generated by a visual module, and the LLM subsequently produces hidden state embeddings for the images:
% \begin{equation}
%     H_v = (h_v^1, h_v^2, \cdots, h_v^
% K)
% \end{equation}
% However, this process does not explicitly incorporate semantic segmentation of the image. While patch-based operations divide the image into smaller regions, these patches lack semantic labels, making it unsuitable to directly apply cross-entropy for supervising image tokens.

% To address this limitation, we propose minimizing the mutual information, $I(H_v; Z_v)$, between image hidden state embeddings and image token embeddings. Direct computation of mutual information is intractable, so we approximate it by minimizing the L2 distance between the two embeddings. As shown in the appendix, this approach effectively increasing their informational similarity, aligning the semantic representations of input and output images.
% The objective function is defined as:
% \begin{equation}
%     \mathcal{L}_v = \sum_{j=1}^{K} \left\lVert h_v^j - z_v^j \right\rVert_2^2
% \end
% {equation}
% Here, $\left\lVert h_v^j - z_v^j \right\rVert_2^2$ represents the squared L2 distance between the $j$
% -th image hidden state and its corresponding token embedding. From an information-theoretic perspective, a smaller distance indicates that the semantic information encoded in the predicted image aligns more closely with the semantic information in the input image. This parallels the reconstruction process in text tasks, where the predicted text aims to recover the information of the ground-truth text.

As shown in Figure \ref{VDEP_derivation}, text data includes instance-level token labels, allowing the use of cross-entropy loss to quantify discrepancies between reconstructed outputs and input tokens. In contrast, image data lacks explicit instance-level labels, making the direct application of cross-entropy loss computationally prohibitive. To address this limitation, we propose a redesigned autoregressive training framework that incorporates image representations into the optimization objective:
\begin{equation}
\textstyle
I(X_I; X^{h}_I) = H(X_I) - H(X_I|X^{h}_I)
\end{equation}
Here, $X^{h}_I$ represents the hidden vector generated by the LLM at the corresponding position. This process is depicted in Figure \ref{VDEP_derivation}.b. In the absence of explicit token-level labels for images, we use embedding vectors to reconstruct the input image representations. This approach aligns the optimization process for image data with the token-level objectives used for text data. 
To maximize mutual information, we minimize the difference between $X_I$ and $X^{h}_I$. Considering that cross-entropy is not well-suited for the measurement of similarity between vectors, this study adopts the L2 norm to quantify the Euclidean distance-based difference between two vectors.
\begin{equation}
\textstyle
\mathcal{L}_i = \left\lVert X_I - X^{h}_I \right\rVert_2^2
\end{equation}
By minimizing the loss function $\mathcal{L}_i$, the hidden vector $X^{h}_I$ converges toward the original image embedding vector $X_I$, as described by:
\begin{equation}
\textstyle
\lim_{\mathcal{L}_i \to 0} X^{h}_I = X_I
\end{equation}
When this condition is satisfied, the conditional entropy $H(X_I | X^{h}_I)$ approaches zero, and the mutual information $I(X_I; X^{h}_I)$ becomes equal to the entropy $H(X_I)$.
\begin{equation}
\begin{aligned}
\textstyle
\lim_{X^{h}_I \to X_I} I(X_I; X^{h}_I) &= H(X_I)
\\
\lim_{X^{h}_I \to X_I} H(X_I | X^{h}_I) &= 0
\end{aligned}
\end{equation}
Therefore, the optimization objective is defined as:
\begin{equation}
\lim_{\mathcal{L}_i \to 0} \Big( X^{h}_I \to X_I, \, \, H(X_I | X^{h}_I) \to 0 \Big)
\end{equation}
This result shows that the hidden vector $X^{h}_I$ successfully captures and reconstructs the semantic information encoded in the original image embedding vector $X_I$.





\subsection{Hybrid Multimodal Alignment Training}
According to the above analysis, we introduce a new objective function, $\mathcal{L}$, which balances the contributions of both visual and textual modalities:
\begin{equation}
\begin{aligned}
\textstyle
\mathcal{L} =\mathcal{L}_t +  \alpha \mathcal{L}_i
\end{aligned}
\end{equation}
where $\mathcal{L}_t$ and $\mathcal{L}_v$ denote the loss functions for text and image modalities, respectively. The parameter $\alpha \in [0, 1]$ controls the relative importance of vision modality. By tuning $\alpha$, we can dynamically adjust the balance between modalities to optimize overall model performance.
The training process of LLava can be divided into two main stages: the pre-training stage and the supervised fine-tuning stage. As shown in Figure \ref{VRKG}, during the pre-training stage, we employed a hybrid multimodal alignment training strategy, combining the VDEP mode and the LLava mode. In the supervised fine-tuning stage, we consistently followed the original LLava framework.
In the pre-training stage, the process prioritizes optimizing the text space to align the image space with the text space. In MLLMs, the number of text tokens is significantly smaller than the number of image tokens. Training both modalities jointly without distinction can cause the model to overfit background information in images, hindering its ability to match the performance of text-only supervised training. Furthermore, it can introduce noise into the training process, potentially disrupting the alignment between image and text modalities.
To address this, we utilize the distribution derived from text data to align the image space with the text space. This requires the text distribution to first reach a stable state. Our mixed multimodal alignment strategy, which alternates the optimization of image and text losses during each forward and backward propagation, ensures balanced learning across modalities. Specifically, within each batch, the data is evenly divided, with one half (randomly selected) used to compute the image loss and the other half to compute the text loss. This
% table1
\begin
{table*}[t]
\caption
{Comparison of VDEP (Ours) and LLava series Across visual question-answering Datasets with Different Model Sizes.}
\label{main_vqa}
\vskip
 0.15in
\begin
{center}
\begin
{small}
\begin
{sc}
\scalebox
{1.0}{
\begin
{tabular}{lcccccc}
\toprule
\textbf{METHOD} & \textbf{VQA\textsuperscript{ok}} & \textbf{GQA} & \textbf{VIZWIZ} & \textbf{VQA\textsuperscript{T}} & \textbf{RWQA} & \textbf{SQA\textsuperscript{I}} \\
\midrule
\textbf{\emph{TinyLLava-3B}} & & & & & & \\ 
TinyLLava & \multicolumn{1}{c}{56.71} & \multicolumn{1}{c}{61.07} & \multicolumn{1}{c}{34.30} & \multicolumn{1}{c}{50.88} & \multicolumn{1}{c}{53.99} & \multicolumn{1}{c}{71.24} \\
\rowcolor[HTML]{ededed} % 浅灰背景
TinyLLava-VDEP (ours) & \multicolumn{1}{c}{\textbf{57.97}} & \multicolumn{1}{c}{\textbf{61.67}} & \multicolumn{1}{c}{\textbf{37.58}} & \multicolumn{1}{c}{\textbf{51.73}} & \multicolumn{1}{c}{\textbf{54.25}} & \multicolumn{1}{c}{\textbf{71.39}} \\
Change & \multicolumn{1}{c}{\textcolor{darkgreen}{+1.26}} & \multicolumn{1}{c}{\textcolor{darkgreen}{+0.60}} & \multicolumn{1}{c}{\textcolor{darkgreen}{+3.28}} & \multicolumn{1}{c}{\textcolor{darkgreen}{+0.85}} & \multicolumn{1}{c}{\textcolor{darkgreen}{+0.26}} & \multicolumn{1}{c}{\textcolor{darkgreen}{+0.15}} \\
\midrule
\textbf{\emph{LLava-v1.5-7B}} & & & & & & \\ 
LLava & \multicolumn{1}{c}{54.32} & \multicolumn{1}{c}{62.12} & \multicolumn{1}{c}{50.34} & \multicolumn{1}{c}{46.16} & \multicolumn{1}{c}{54.80} & \multicolumn{1}{c}{66.80} \\
\rowcolor[HTML]{ededed} % 浅灰背景
LLava-VDEP (Ours) & \multicolumn{1}{c}{\textbf{57.68}} & \multicolumn{1}{c}{\textbf{62.50}} & \multicolumn{1}{c}{\textbf{50.37}} & \multicolumn{1}{c}{\textbf{46.76}} & \multicolumn{1}{c}{\textbf{57.64}} & \multicolumn{1}{c}{\textbf{68.36}} \\
Change & \multicolumn{1}{c}{\textcolor{darkgreen}{+3.36}} & \multicolumn{1}{c}{\textcolor{darkgreen}{+0.38}} & \multicolumn{1}{c}{\textcolor{darkgreen}{+0.03}} & \multicolumn{1}{c}{\textcolor{darkgreen}{+0.60}} & \multicolumn{1}{c}{\textcolor{darkgreen}{+2.84}} & \multicolumn{1}{c}{\textcolor{darkgreen}{+1.56}} \\
\bottomrule
\end
{tabular}
}
\end
{sc}
\end
{small}
\end
{center}
\vskip
 -0.1in
\end
{table*}
\begin
{table*}[t]
\caption
{Comparison of VDEP (Ours) and LLava series on benchmarks for instruction-following LMMs with Different Model Sizes.}
\label{MLLM_Benchmark}
\vskip
 0.15in
\begin
{center}
\begin
{small}
\begin
{sc}
\scalebox
{0.9}{
\begin
{tabular}{@{}cccccccccc@{}}
\toprule
\multirow{2}{*}{\textbf{METHOD}} & \multirow{2}{*}{\textbf{POPE}} & \multirow{2}{*}{\textbf{SEEDB\textsuperscript{I}}} & \multirow{2}{*}{\textbf{AI2D}} & \multirow{2}{*}{\textbf{MM-Vet}} & \multirow{2}{*}{\textbf{MMMU}} & \multirow{2}{*}{\textbf{MMTB}} & \multirow{2}{*}{\textbf{OCRB}} & \multicolumn{2}{c}{\textbf{MMBench}} \\ 
\cmidrule
(lr){9-10}
& & & & & & & & \textbf{en} & \textbf{cn} \\ 
\midrule
\textbf{\emph{TinyLLava-3B}} & & & & & & & & & \\ 
TinyLLava & 85.93 & 68.54 & 59.75 & 33.00 & 33.80 & 48.93 & \textbf{345} & \textbf{67.88} & \textbf{45.07} \\
\rowcolor
[HTML]{EDEDED}
TinyLLava-VDEP (Ours) & \textbf{86.98} & \textbf{69.35} & \textbf{60.85} & \textbf{36.00} & \textbf{33.80} & \textbf{49.08} & 343 & 66.70 & 41.87 \\
Change & \textcolor{darkgreen}{+1.05} & \textcolor{darkgreen}{+0.81} & \textcolor{darkgreen}{+1.10} & \textcolor{darkgreen}{+3.00} & \textcolor{darkgreen}{+0.00} & \textcolor{darkgreen}{+0.15} & -2.00 & -1.18 & -3.20 \\
\midrule
\textbf{\emph{LLava-v1.5-7B}} & & & & & & & & & \\ 
LLava & 85.85 & 66.10 & 55.63 & 31.10 & 31.20 & 47.94 & 297 & 64.97 & 57.90 \\
\rowcolor
[HTML]{EDEDED}
LLava-VDEP (Ours) & \textbf{86.20} & \textbf{66.70} & \textbf{56.57} & 30.60 & 30.80 & \textbf{48.00} & \textbf{326} & \textbf{66.81} & \textbf{58.23} \\
Change & \textcolor{darkgreen}{+0.35} & \textcolor{darkgreen}{+0.60} & \textcolor{darkgreen}{+0.94} & -0.50 & -0.40 & \textcolor{darkgreen}{+0.06} & \textcolor{darkgreen}{+29} & \textcolor{darkgreen}{+1.84} & \textcolor{darkgreen}{+0.33} \\
% \midrule
% \textbf{\emph{LLM-Size-13B}} & & & & & & & & & \\ 
% LLava-v1.5 & \textbf{86.37} & 67.65 & 59.49 & \textbf{37.30} & \textbf{33.20} & \textbf{49.82} & \textbf{346} & 67.74 & 63.17 \\
% \rowcolor[HTML]{EDEDED}
% LLava-VDEP (Ours) & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
% Change & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
\bottomrule
\end
{tabular}
}
\end
{sc}
\end
{small}
\end
{center}
\vskip
 -0.1in
\end
{table*}
approach preserves the inherent characteristics of the text distribution while using it to align the image space.


