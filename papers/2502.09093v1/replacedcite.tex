\section{Related Works}
\label{related_work}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{autos.pdf}}
\caption{The LLava-VDEP network architecture incorporates two distinct training modes. The VDEP mode performs supervised learning on image data, while the LLava mode is dedicated to supervised learning on text data. During batch training, a ratio parameter is used to control the proportional occurrence of these two modes within each batch, enabling an effective balance in the learning process.}
\label{VRKG}
\end{center}
\vskip -0.2in
\end{figure*}
\subsection{Multimodal Large Language Models}
Language and vision models in single-modal settings inherently face functional limitations, which restrict their potential for further development. In contrast, the rapid emergence of large multimodal models has successfully overcome these limitations, leading to notable improvements in model capabilities and performance. For instance, CLIP____ employs an efficient pre-training strategy based on contrastive learning; however, its similarity computation relies exclusively on basic dot-product operations. In comparison, VILT____ highlights that traditional models often feature excessively intricate backbone designs for vision and text, whereas the multimodal fusion modules are relatively simplistic. To address this, VILT reallocates computational resources toward the alignment module, thereby improving the effectiveness of multimodal learning. Meanwhile, ALBEF____ argues that aligning images and text solely through the fusion module presents challenges. To mitigate this, it introduces the ITC (Image-Text Contrastive Learning) module, which performs initial alignment between images and text before passing them to the fusion module. This refinement lays a stronger foundation for subsequent fusion processes. Building on this, BLIP-v2____ incorporates the ITC module. It introduces the Q-Former, employing a two-stage pre-training strategy: the first stage leverages a frozen vision encoder to guide multimodal learning. In contrast, the second stage utilizes a frozen text encoder to complete the learning process. However, BLIP-v2 compresses image information by decreasing the number of image tokens, which improves efficiency but introduces limitations in specific downstream tasks. Additionally, LLava____ adopts a more straightforward approach, projecting image features into the text feature space via a linear layer. As the complexity of the projection layer increases, its alignment performance improves significantly.
\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{loss_func.pdf}}
\caption{Illustration of our VDEP derivation process.
(a) Text Pre-training: Convert text into embeddings using tokenization. The LLM generates hidden states, which are processed by the LM head to produce predicted tokens. Compute cross-entropy loss with the original input.(b) Image Pre-training: Divide images into patches. Convert patches into embeddings using visual branches without real labels. These embeddings guide the LLM hidden states to reconstruct image information.}
\label{VDEP_derivation}
\end{center}
\vskip -0.2in
\end{figure*}



\subsection{Strategies for Aligning MLLMs}
% Research on aligning MLLMs across tasks is primarily divided into the pre-training stage and the SFT (Supervised Fine-Tuning) stage. During the pre-training stage, several critical strategies are employed:
% (1) Enhancing the semantic information in the visual branch: This represents a core challenge in the pre-training phase. Textual tokens often struggle to capture complete instance-level information from segmented image tokens, making precise instance matching difficult. To address this, existing methods employ advanced techniques to extract semantic information from images and facilitate efficient interactions with textual tokens. For instance, ____ uses a pretrained Region Proposal Network (RPN) to generate bounding boxes, enabling region-level supervised fine-tuning of CLIP. Similarly, ____ leverages region annotations from object detection datasets to achieve this process. Methods such as ____ utilize mature models [15][16] to extract semantic information, enhancing the segmentation reasoning process and improving instance-level correspondence across modalities.
% (2) Introducing adapter modules: This strategy embeds adapter modules to enable effective integration of image and text modalities, significantly boosting alignment capabilities. For example, ____ employs a zero-initialized attention mechanism to efficiently fine-tune the LLama model, showcasing the effectiveness of adapter modules within the CLIP framework. Meanwhile, ____ successfully decouples instruction-following training (e.g., generating extended language responses) from vision-language alignment tasks, thereby avoiding potential conflicts during fine-tuning. (3) Improving alignment between image and text feature spaces: Models such as LLava ____, Shikra ____, and MiniGPT ____ map visual features to text feature spaces through linear projection adapters. Meanwhile, QWEN-VL ____ and mPLUG-Owl ____ utilize Perceiver-like architectures as adapters. Since BLIP-2 ____ introduced the Q-Former, many multimodal models ____ have adopted it as a core adapter for modality alignment. Furthermore, models like ____ enhance their understanding and interpretation of prompts using cross-attention mechanisms.
% (4) Incorporating additional tasks: For instance, ____ introduces visual generation tasks, which not only improve visual understanding but also strengthen image-text alignment. Additionally, ____ employs pixel-level generation tasks as autoregressive objectives for the visual branch, effectively enhancing text-vision alignment. During the SFT stage, the following strategies are widely adopted:
% (5) Developing new fine-tuning instruction sets: For example, LLava ____ and MiniGPT-4 ____ incorporate image caption data into GPT-4/ChatGPT models to generate dialogues highly relevant to visual content. MultiInstruct ____ transforms various visual classification tasks into instruction-tuning formats. InstructBLIP ____ collects data from 11 tasks across 26 datasets and converts them into instruction-tuning formats. Additionally, ____ proposes M3IT, an open multimodal multilingual instruction-tuning dataset that spans 40 datasets, aiming to optimize multimodal instruction-tuning and demonstrating outstanding performance in real-world scenarios.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Research on aligning MLLMs across tasks is divided into two stages: the pre-training stage and the SFT (Supervised Fine-Tuning) stage. 

\textbf{Pre-training stage.} Key strategies include:  
(1) Enhancing semantic information in the visual branch: Text tokens often struggle to capture complete instance-level information from segmented image tokens. To address this, methods such as ____ use pre-trained region-based models or object detection datasets to extract semantic features. Other approaches ____ leverage mature models ____ to improve cross-modal instance alignment.  
(2) Introducing adapter modules: Adapter modules integrate image and text modalities effectively. For example, ____ uses zero-initialized attention for efficient fine-tuning, while ____ decouples instruction-following training from vision-language alignment to reduce interference.  
(3) Improving alignment between feature spaces: Models like LLava ____, Shikra ____, and MiniGPT ____ map visual features to text spaces via linear projection adapters. Others, such as QWEN-VL ____ and mPLUG-Owl ____, employ Perceiver-like architectures, while Q-Former ____ has become a core adapter in many models ____.  
(4) Incorporating additional tasks: Visual generation tasks ____ and pixel-level generation objectives ____ enhance visual understanding and strengthen image-text alignment.  

\textbf{SFT stage.} Key strategies include:  
(5) Developing new fine-tuning instruction sets: Models such as LLava ____ and MiniGPT-4 ____ integrate image caption data for visually grounded dialogue. MultiInstruct ____ reformulates visual tasks into instruction-tuning formats, while InstructBLIP ____ collects data from 11 tasks for instruction fine-tuning. 
M3IT ____ introduces a multimodal multilingual dataset spanning 40 datasets to optimize instruction tuning.