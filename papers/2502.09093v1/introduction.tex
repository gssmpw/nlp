\section{Introduction}
\label{introduction}

The advent of large language models (LLMs) such as ChatGPT \cite{schulman2022chatgpt} has profoundly reshaped the trajectory of AGI development, showcasing exceptional zero-shot reasoning capabilities in addressing various NLP tasks via user-defined prompts or language instructions. 
Traditional vision foundation models typically follow a pre-training and fine-tuning paradigm \cite{wang2023internimage, chen2022vision, su2023towards, wang2023image, tao2023siamese}. While effective, this approach incurs significant marginal costs when adapting to diverse downstream tasks.
In contrast, LLMs open new opportunities for developing unified frameworks to address visual tasks. Multimodal large language models (MLLMs) extend this paradigm by employing vision pre-training models to project image features into a shared textual representation space, leveraging the language instruction capabilities of LLMs to adapt to a wide range of vision tasks. 
These models have achieved state-of-the-art results across multiple application domains, including information retrieval, video analysis, and open-world question answering, as evidenced by their superior benchmark scores and qualitative outputs. 
\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{lighthouse.pdf}}
\caption{Layer-wise attention visualization of visual-to-instruction information flow. The
example is derived from LLava-Bench \cite{liu2024improved} and the query is "Describe this photo in detail". The visualization results demonstrate that VDEP significantly enhances the model's ability to capture critical features in images, with particularly outstanding performance in identifying object boundaries.}
\label{lighthouse}
\end{center}
\vskip -0.2in
\end{figure*}
% Despite their demonstrated ability to handle diverse tasks effectively, multimodal systems remain constrained by current approaches to multimodal fusion.
% Current MLLMs are based on the architecture of LLMs and focus on learning textual ground truth, making the training process primarily text-driven. This leads to an imbalance in training, where the model tends to prioritize text annotations over image annotations.
% As a result, they face numerous challenges stemming from modality misalignment, such as hallucination phenomena \cite{wang2024mllm, wang2023amber, li2023evaluating}, insufficient comprehension of fine-grained semantic information in images \cite{wu2024towards, lai2024lisa, yang2023improved, aflalo2024fivl, zhao2023bubogpt}, performance degradation in vision and language tasks \cite{lin2024vila, zhao2025first, liu2024rar, barbany2024leveraging}, and limited ability to distinguish fine-grained contextual semantics in the input \cite{wang2023makes, li2023fine}.
% In this context, research has increasingly focused on improving the coordination of MLLMs to address the challenges above. Substantial efforts have been devoted to overcoming these issues during both the training process and downstream applications. During the pre-training stage, studies \cite{zhong2022regionclip, lai2024lisa} have aimed to alleviate the difficulty of associating text tokens with discrete image patches by capturing rich semantic features from images. Specifically, several works \cite{bai2023qwen, ye2023mplug, zhu2023minigpt, yu2023reformulating} have proposed advanced fusion modules—mechanisms designed to integrate visual and textual data more effectively. To achieve deeper alignment, some studies \cite{tong2024metamorph, fini2024multimodal} have incorporated autoregressive tasks into the image branch, enhancing the representation quality and interpretability of text tokens. In the SFT (supervised fine-tuning) stage, large-scale instruction-tuning datasets tailored for MLLMs have been developed \cite{li2023m, xu2022multiinstruct} to address the limitations of earlier multimodal alignment datasets.

However, MLLMs face significant challenges, particularly in multimodal fusion and alignment. Current MLLMs rely heavily on text-driven training objectives, prioritizing  textual annotations over image features. This imbalance often leads to modality misalignment, resulting in several limitations: hallucination phenomena \cite{wang2024mllm, wang2023amber}, inadequate comprehension of fine-grained semantic information in images \cite{wu2024towards, lai2024lisa}, and degraded performance in vision-language tasks \cite{lin2024vila}. Existing methods, such as advanced fusion modules \cite{bai2023qwen, ye2023mplug, zhu2023minigpt} or autoregressive tasks integrated into the image branch \cite{tong2024metamorph, fini2024multimodal}, show promise but often introduce significant architectural complexity or require extensive human intervention. These approaches fail to resolve training imbalances, where attention mechanisms favor text tokens over image tokens.



% Most existing methods require significant human intervention, making the process both operationally intricate and challenging. In contrast, LLava introduces a simple linear layer to bridge the image and text spaces, achieving notable performance improvements and surpassing the more sophisticated Q-Former structure \cite{li2022blip} in effectiveness. Prior studies suggest that Q-Former may inadvertently discard critical token-level information during extraction, whereas LLava preserves image information more comprehensively throughout the process.
% Subsequent enhancements led to the development of LLava-v1.5, which replaced the initial linear layer with a multilayer perceptron (MLP), further enhancing both the model's performance and its accuracy. However, recent research [32] reveals that LLava's attention mechanism disproportionately allocates weights to text tokens, often at the expense of image tokens. This imbalance in attention distribution \cite{huang2024opera} results in inadequate representation of image tokens in the model's final output, thereby constraining its capacity to fully leverage image information. Moreover, this insufficient focus on image tokens exacerbates the challenges associated with aligning the image and text feature spaces.

LLava offers a simpler alternative by introducing a lightweight linear layer to bridge the image and text feature spaces, achieving competitive performance and surpassing more complex architectures like Q-Former \cite{li2022blip}. Subsequent enhancements, LLava replaced the linear layer with a multilayer perceptron (MLP), further improving alignment and accuracy. However, recent studies \cite{zhang2024redundancy} reveal that LLava's attention mechanism still assigns insufficient weight to image tokens, limiting its ability to fully exploit visual information. This imbalance exacerbates alignment challenges and reduces the model’s effectiveness in cross-modal tasks.

% Although prior research has explored methods to improve the alignment of latent spaces by assigning higher importance to image tokens \cite{huang2024opera} or introducing additional encoders \cite{xing2024mitigating}, these approaches often involve complex architectural modifications or depend on external supervision signals. Such strategies substantially increase computational and architectural complexity while failing to address the fundamental issue of token imbalance during training. In MLLMs, focusing supervision exclusively on text introduces a significant disparity between image and text tokens in terms of training objectives.

To address these issues, some approaches have focused on improving image-text alignment by assigning greater importance to image tokens \cite{huang2024opera} or introducing additional encoders \cite{xing2024mitigating}. While these methods mitigate specific alignment issues, they often rely on complex architectural modifications or external supervision signals, increasing computational costs and operational challenges. Thus, achieving balanced and efficient alignment between image and text tokens remains an open problem in MLLMs.


From an information-theoretic perspective, the pre-training objective of MLLMs is to minimize mutual information between input and output textual representations, ensuring accurate text reconstruction. Extending this framework to image-related tasks, we show that L2 loss effectively approximates mutual information, enabling better alignment between image and text features. Building on this insight, we propose a method called Visual Dynamic Embedding-guided pre-training (VDEP). Without altering the architecture of the LLava model, VDEP achieves a significant improvement in image-text alignment.

Specifically, our approach leverages dynamic embeddings generated by the MLP following the visual encoder to supervise the hidden states of image representations. Simultaneously, image tokens are incorporated into the autoregressive training objective, allowing for the joint optimization of text and image during training. This mechanism enables the model to adjust the relative importance of image and text tokens dynamically, effectively balancing their attention distributions.

As illustrated in Figure \ref{lighthouse}, VDEP significantly enhances the model's ability to capture critical visual information, even demonstrating a nuanced understanding of contours within images. Across multiple benchmark datasets, our method delivers state-of-the-art performance and achieves an approximate 5.2\% improvement in RealWorldQA benchmark. Experimental results confirm that the autoregressive latent space alignment strategy mitigates hallucination issues and substantially boosts the model's effectiveness in cross-modal tasks.
To summarize, our key contributions are: 
\begin{itemize}
\item We redefine multimodal alignment as an information recovery process and propose VDEP, which integrates image information restoration into LLava's pre-training process without requiring additional data or architectural modifications.
\item We design a novel hybrid training strategy that dynamically alternates between VDEP mode and the original LLava mode, enhancing alignment performance during pre-training.
\item Experimental results demonstrate that VDEP consistently outperforms the baseline LLava across 13 benchmark datasets, including VQA, MME, and MMB.
\end{itemize}






