[
  {
    "index": 0,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kim2021vilt",
        "author": "Kim, Wonjae and Son, Bokyung and Kim, Ildoo",
        "title": "Vilt: Vision-and-language transformer without convolution or region supervision"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2021align",
        "author": "Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong",
        "title": "Align before fuse: Vision and language representation learning with momentum distillation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhong2022regionclip",
        "author": "Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and others",
        "title": "Regionclip: Region-based language-image pretraining"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "minderer2022simple",
        "author": "Minderer, Matthias and Gritsenko, Alexey and Stone, Austin and Neumann, Maxim and Weissenborn, Dirk and Dosovitskiy, Alexey and Mahendran, Aravindh and Arnab, Anurag and Dehghani, Mostafa and Shen, Zhuoran and others",
        "title": "Simple open-vocabulary object detection"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "lai2024lisa",
        "author": "Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya",
        "title": "Lisa: Reasoning segmentation via large language model"
      },
      {
        "key": "yang2023improved",
        "author": "Yang, Senqiao and Qu, Tianyuan and Lai, Xin and Tian, Zhuotao and Peng, Bohao and Liu, Shu and Jia, Jiaya",
        "title": "An improved baseline for reasoning segmentation with large language model"
      },
      {
        "key": "aflalo2024fivl",
        "author": "Aflalo, Estelle and Stan, Gabriela Ben Melech and Le, Tiep and Luo, Man and Rosenman, Shachar and Paul, Sayak and Tseng, Shao-Yen and Lal, Vasudev",
        "title": "FiVL: A Framework for Improved Vision-Language Alignment"
      },
      {
        "key": "zhao2023bubogpt",
        "author": "Zhao, Yang and Lin, Zhijie and Zhou, Daquan and Huang, Zilong and Feng, Jiashi and Kang, Bingyi",
        "title": "Bubogpt: Enabling visual grounding in multi-modal llms"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhang2023llama",
        "author": "Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu",
        "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "gao2023llama",
        "author": "Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others",
        "title": "Llama-adapter v2: Parameter-efficient visual instruction model"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chen2023shikra",
        "author": "Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui",
        "title": "Shikra: Unleashing multimodal llm's referential dialogue magic"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "chen2023minigpt",
        "author": "Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechun and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed",
        "title": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "ye2023mplug",
        "author": "Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others",
        "title": "mplug-owl: Modularization empowers large language models with multimodality"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
      },
      {
        "key": "yu2023reformulating",
        "author": "Yu, Tianyu and Hu, Jinyi and Yao, Yuan and Zhang, Haoye and Zhao, Yue and Wang, Chongyi and Wang, Shan and Pan, Yinxv and Xue, Jiao and Li, Dahai and others",
        "title": "Reformulating vision-language foundation models and datasets towards universal multimodal assistants"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "wang2024visionllm",
        "author": "Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and others",
        "title": "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks"
      },
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      },
      {
        "key": "gong2023multimodal",
        "author": "Gong, Tao and Lyu, Chengqi and Zhang, Shilong and Wang, Yudong and Zheng, Miao and Zhao, Qian and Liu, Kuikun and Zhang, Wenwei and Luo, Ping and Chen, Kai",
        "title": "Multimodal-gpt: A vision and language model for dialogue with humans"
      },
      {
        "key": "li2023ottermultimodalmodelincontext",
        "author": "Bo Li and Yuanhan Zhang and Liangyu Chen and Jinghao Wang and Jingkang Yang and Ziwei Liu",
        "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "tong2024metamorph",
        "author": "Tong, Shengbang and Fan, David and Zhu, Jiachen and Xiong, Yunyang and Chen, Xinlei and Sinha, Koustuv and Rabbat, Michael and LeCun, Yann and Xie, Saining and Liu, Zhuang",
        "title": "MetaMorph: Multimodal Understanding and Generation via Instruction Tuning"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "fini2024multimodal",
        "author": "Fini, Enrico and Shukor, Mustafa and Li, Xiujun and Dufter, Philipp and Klein, Michal and Haldimann, David and Aitharaju, Sai and da Costa, Victor Guilherme Turrisi and B{\\'e}thune, Louis and Gan, Zhe and others",
        "title": "Multimodal autoregressive pre-training of large vision encoders"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "xu2022multiinstruct",
        "author": "Xu, Zhiyang and Shen, Ying and Huang, Lifu",
        "title": "Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "NEURIPS2023_9a6a435e",
        "author": "Dai, Wenliang and Li, Junnan and LI, DONGXU and Tiong, Anthony and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven",
        "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "li2023m",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "zhong2022regionclip",
        "author": "Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and others",
        "title": "Regionclip: Region-based language-image pretraining"
      },
      {
        "key": "minderer2022simple",
        "author": "Minderer, Matthias and Gritsenko, Alexey and Stone, Austin and Neumann, Maxim and Weissenborn, Dirk and Dosovitskiy, Alexey and Mahendran, Aravindh and Arnab, Anurag and Dehghani, Mostafa and Shen, Zhuoran and others",
        "title": "Simple open-vocabulary object detection"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "lai2024lisa",
        "author": "Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya",
        "title": "Lisa: Reasoning segmentation via large language model"
      },
      {
        "key": "yang2023improved",
        "author": "Yang, Senqiao and Qu, Tianyuan and Lai, Xin and Tian, Zhuotao and Peng, Bohao and Liu, Shu and Jia, Jiaya",
        "title": "An improved baseline for reasoning segmentation with large language model"
      },
      {
        "key": "aflalo2024fivl",
        "author": "Aflalo, Estelle and Stan, Gabriela Ben Melech and Le, Tiep and Luo, Man and Rosenman, Shachar and Paul, Sayak and Tseng, Shao-Yen and Lal, Vasudev",
        "title": "FiVL: A Framework for Improved Vision-Language Alignment"
      },
      {
        "key": "zhao2023bubogpt",
        "author": "Zhao, Yang and Lin, Zhijie and Zhou, Daquan and Huang, Zilong and Feng, Jiashi and Kang, Bingyi",
        "title": "Bubogpt: Enabling visual grounding in multi-modal llms"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "zhang2023llama",
        "author": "Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu",
        "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "gao2023llama",
        "author": "Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others",
        "title": "Llama-adapter v2: Parameter-efficient visual instruction model"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "chen2023shikra",
        "author": "Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui",
        "title": "Shikra: Unleashing multimodal llm's referential dialogue magic"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "chen2023minigpt",
        "author": "Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechun and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed",
        "title": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "ye2023mplug",
        "author": "Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others",
        "title": "mplug-owl: Modularization empowers large language models with multimodality"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
      },
      {
        "key": "yu2023reformulating",
        "author": "Yu, Tianyu and Hu, Jinyi and Yao, Yuan and Zhang, Haoye and Zhao, Yue and Wang, Chongyi and Wang, Shan and Pan, Yinxv and Xue, Jiao and Li, Dahai and others",
        "title": "Reformulating vision-language foundation models and datasets towards universal multimodal assistants"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "tong2024metamorph",
        "author": "Tong, Shengbang and Fan, David and Zhu, Jiachen and Xiong, Yunyang and Chen, Xinlei and Sinha, Koustuv and Rabbat, Michael and LeCun, Yann and Xie, Saining and Liu, Zhuang",
        "title": "MetaMorph: Multimodal Understanding and Generation via Instruction Tuning"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "fini2024multimodal",
        "author": "Fini, Enrico and Shukor, Mustafa and Li, Xiujun and Dufter, Philipp and Klein, Michal and Haldimann, David and Aitharaju, Sai and da Costa, Victor Guilherme Turrisi and B{\\'e}thune, Louis and Gan, Zhe and others",
        "title": "Multimodal autoregressive pre-training of large vision encoders"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "xu2022multiinstruct",
        "author": "Xu, Zhiyang and Shen, Ying and Huang, Lifu",
        "title": "Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "NEURIPS2023_9a6a435e",
        "author": "Dai, Wenliang and Li, Junnan and LI, DONGXU and Tiong, Anthony and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven",
        "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"
      }
    ]
  },
  {
    "index": 43,
    "papers": [
      {
        "key": "li2023large",
        "author": "Li, Lei and Yin, Yuwei and Li, Shicheng and Chen, Liang and Wang, Peiyi and Ren, Shuhuai and Li, Mukai and Yang, Yazheng and Xu, Jingjing and Sun, Xu and others",
        "title": "A large-scale dataset towards multi-modal multilingual instruction tuning"
      }
    ]
  }
]