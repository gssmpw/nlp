\section{NVMe Flexible Data Placement (FDP)}
\label{sec:fdp}
\subsection{Overview}
The ratified NVMe Flexible Data Placement technical proposal~\cite{fdp_tp} represents an evolution in the space of SSD data placement based on lessons learned in the wild over the past decade. It is a merger of Google's SmartFTL~\cite{smartftl} and Meta's Direct Placement Mode proposals to enable data placement on Flash media without the high software engineering costs of explicit garbage collection of ZNS~\cite{bjorlingAHRMGA21} and low-level media control of Open-Channel SSD proposals~\cite{bjorling2017lightnvm}. It borrows elements from the multi-streamed SSD interface~\cite{kang2014multi} that was proposed a decade ago but did not really take off due to a lack of industry and academic interest. It has been designed with backward compatibility in mind so that applications can work unchanged with it. The choice of leveraging data placement and evaluating its costs and benefits has been left to the application. This enables investment of engineering effort in a pay-as-you-go fashion instead of an upfront cost.

\subsection{Physical Isolation in SSDs with FDP}
\subsubsection{FDP Architectural Concepts}
The Flexible Data Placement interface provides abstractions to the host to group data on the device with a similar expected lifetime (e.g., death time). The interface introduces the following concepts to expose the SSD physical architecture (see Figure \ref{fig:fdp-arch}).

\begin{figure}[!t]
  \centering
  \includegraphics[width=1.3 \linewidth]{svg_to_pdf/cns-fdpArch2.pdf}
  %\vspace{-3ex}
  \caption{Conventional SSD vs FDP SSD Architecture.} 
  \vspace{-2ex}
  \label{fig:fdp-arch}
\end{figure}

\minisec{Reclaim Unit (RU)} The NAND media is organized into a set of reclaim units where a reclaim unit consists of a set of blocks that can be written. A reclaim unit will typically consist of one or more erase blocks but no guarantees are made in the proposal towards this end. The size of an RU is decided by the SSD manufacturer. In this paper, our device has superblock-sized RUs where a superblock is a collection of erase blocks across the planes of dies in the SSD. If an SSD has 8 dies each with 2 planes and 2 erase blocks per plane, the superblock will consist of 32 erase blocks.

\minisec{Reclaim Group (RG)} A reclaim group is a set of reclaim units.

\minisec{Reclaim Unit Handles (RUH)} A reclaim unit handle is an abstraction in the device controller similar to a pointer that allows host software to point to the reclaim units in the device. Since a reclaim unit is not directly addressable by the host, the host software uses the reclaim unit handles to logically isolate data. The device manages the mapping of reclaim unit handles to a reclaim unit and has complete control over this mapping. The number of RUHs in the device determines the number of different logical locations in the NAND where the host software can concurrently place data.

\minisec{RUH Types} \move{
The FDP interface specifies two types of reclaim unit handles, each offering distinct data movement guarantees during garbage collection, along with their respective tradeoffs. During garbage collection, the RUH type is used to determine the source and destination RUs of data to be moved. FDP defines two RUH types namely,
\begin{enumerate}[leftmargin=*]
    \item \textbf{Initially Isolated} - All the reclaim units within a reclaim group pointed to by the RUHs of this type are candidates for data movement. For multiple RUHs of initially isolated type, data starts off being isolated from data written using another RUH of initially isolated type. However, upon garbage collection valid data written using these two handles can be intermixed. This type is the cheapest to implement on the SSD controller as it does not require explicit tracking of data written using RUHs and imposes the least constraints on data movement during garbage collection. 
    \item \textbf{Persistently Isolated} - All the reclaim units within a reclaim group that have been written utilizing the RUH are the only candidates for data movement upon garbage collection. This RUH type provides a stronger guarantee of data isolation but is expensive to implement on the controller as it requires explicit tracking of data written using RUHs and imposes more constraints on data movement during garbage collection.
\end{enumerate}
\minisec{Example} Consider a write pattern using two RUHs, RUH0 and RUH1 where RUH0 has written LBAs to RU0 and RU1 while RUH1 has written LBAs to RU2. For simplicity, let us assume that all the RUs belong to the same reclaim group. If RUH0 and RUH1 are of initially isolated type, then upon garbage collection valid data from RU0, RU1 and RU2 are candidates for movement and can be intermixed. If RUH0 and RUH1 are of persistently isolated type, then only data from RU0 and RU1 can be intermixed upon garbage collection while the data in RU2 is isolated from data in RU0 and RU1.
}
\begin{comment}
The FDP interface defines two types of reclaim unit handles that provide different data movement guarantees upon garbage collection with their associated tradeoffs. They are:\\
1. \textbf{Initially Isolated} - SSD is allowed to move the data written with an RUH to any Reclaim Units in the Reclaim Group. This means, upon garbage collection valid data written using different RUHs can be intermixed.\\
2. \textbf{Persistently Isolated} - SSD is only allowed to move the data written with an RUH to any Reclaim Units of the same RUH in the Reclaim Group. We cover more details and examples in Appendix~\ref{sec:appendix:fdp:details}. 
\end{comment}

\minisec{FDP Configurations}  An FDP configuration defines the RUHs, RUH type (Initially or Persistently Isolated), their association to RGs, and the RU size. \textit{The FDP configurations available on the device are predetermined by the manufacturer} and cannot be changed. This paper uses an SSD with a single FDP configuration of 8 Initially Isolated RUHs, 1RG and RU size of 6GB. A device can support multiple configurations that can be chosen by the host.

\begin{comment}
\minisec{Placement Identifier (PID)} 

\subsubsection{FDP Configurations} An FDP configuration defines the RUHs, their association to RGs, and the RU size. An SSD conforming to the FDP specification can support one or many FDP configurations that are available as a log page for selection by the host. At namespace creation, one or many RUHs can be associated to a namespace. \textit{The FDP configurations available on the device are pre-decided by the manufacturer}. If an FDP configuration is not enabled on the FDP SSD, it behaves as a conventional SSD.
\end{comment}
%\vspace{-3ex}

\subsubsection{Data Placement with RUHs}
In this section, we highlight important aspects of the FDP interface that influence data placement designs by the host.\\
\minisec{Physically Isolating Logical Blocks with RUHs}
The FDP storage interface does not introduce any new command sets to write to the device. Instead, a new data placement directive has been defined that allows each write command to specify a RUH. Thus, the host software can use the RUH to place a logical block in a RU utilizing the RUH. By allowing the host to dynamically associate a logical block with a RU, FDP enables flexible grouping of data based on varying temperature and death time (e.g., hot and cold data separation) or different data streams (e.g., large streams and small journals). This facilitates writing to different RUs in a physically isolated manner. By careful deallocation of all the data in a previously written RU, the host can achieve a DLWA of \textasciitilde1.

During namespace creation, the host software selects a list of RUHs that are accessible by the created namespace. Since FDP is backward compatible, a default RUH is chosen by the device for a namespace if it is not specified. Data is placed in this RUH in the absence of the placement directive from the host. Read operations remain unchanged as before. Writes in FDP can cross RU boundaries. If a write operation overfills an RU because the RU is written to its capacity, the device chooses a new RU and updates the mapping of the RUH to the new RU. Although this process is not visible to the host, the event is logged by the SSD in the device logs that the host can examine. 

\minisec{Managing Invalidations and Tracking RUs}
Since FDP does not focus on garbage collection but purely data placement, it does not introduce any new abstractions for erase operations. As in conventional SSDs, LBAs are invalidated or dealloacted in two ways, (1) by overwriting an LBA, (2) by explicitly using a trim operation over one or many LBAs. If all the data in a RU is invalidated, then the RU is erased for future writes and no logical blocks have to copied across RUs upon garbage collection. Since the host software can only access RUHs and not an RU, in order to perform fine-grained and targeted deallocation of RUs, the host software needs to track the LBAs that have been written to an RU together and deallocate those. The FDP specification also allows the host to query the available space in an RU which is currently referenced by the RUH.

\subsection{FDP Events and Statistics} FDP provides an elaborate set of events and garbage collection statistics for the host to track the FDP related events in the SSD. These help the host to be aware of device-level exceptions and make sure that both host and device are in sync regarding data placement.

\begin{comment}
\minisec{Reclaim Group (RG)} As shown in Figure \ref{fig:FDPARCH}, the FDP architecture allows the SSD to define a set of Reclaim Groups (RGs). Reclaim Groups enable a physical division of the NAND space. Reclaim Groups also permit the SSD to violate placement when warranty, space, or other unplanned in-drive activity demands it.  If this emergency Reclaim Group violation ever occurs within the SSD, the host software is able to discover it through querying the drive's logs.  \par
\end{comment}

\subsection{FDP and Other Major Data Placement Proposals}
\edit{
NVMe FDP technical proposal was conceived based on lessons learnt from integrating software stacks with the past data placement proposals. It has been designed to focus on data placement to allow host software stack to perform data segregation while leaving NAND media management and garbage collection to the SSD controller. In Table~\ref{tab:fdp-data-placement-tech}, we outline some of the key differences between the major data placement proposals of the past years. More details can be found in some of the recent industry presentations on FDP~\cite{sdc-fdp-dan, sdc-fdp-mike}.
}

\begin{table*}[!ht]
    \centering
    \edit{
    \begin{tabular}{|p{0.15\textwidth}|p{0.18\textwidth}|p{0.18\textwidth}|p{0.15\textwidth}|p{0.18\textwidth}|}
        \hline
    Characteristic & \textbf {Streams~\cite{kang2014multi}} & \textbf {Open-Channel~\cite{bjorling2017lightnvm}} & \textbf{ZNS~\cite{zns, bjorlingAHRMGA21}} & \textbf{FDP~\cite{fdp_tp}} \\ \hline
    Supported write patterns & Random, Sequential & Random, Sequential & Sequential & Random, Sequential \\ \hline
    Data placement primitive & Using stream identifiers & Using logical to physical address mapping by host & Using zones & Using reclaim unit handles \\ \hline
    Control of garbage collection & SSD-based without feedback to host & Host-based & Host-based & SSD-based with feedback through logs \\ \hline
    NAND media management by host & No & Yes & No & No \\ \hline
    Can run applications unchanged & Yes & No & No & Yes \\ \bottomrule
    \end{tabular}
    \caption{High-Level Comparison of Major Data Placement Proposals.}
    \label{tab:fdp-data-placement-tech}
    }
    \vspace{-2ex}
\end{table*}

\edit{
\subsection{Limitations}
\begin{enumerate}[leftmargin=*]
\item {\textbf{New and evolving technology.}} The FDP technical proposal was ratified at the end of 2022, and some devices from Samsung, such as the PM9D3a~\cite{pm9d3a} are emerging on the market with support for it, along with offerings from other vendors. Due to the relatively recent ratification, the proposal may undergo modifications over time to include extensions for desirable features.
\item {\textbf{Lack of host control over garbage collection.}} FDP was designed specifically for data placement while allowing hosts to perform random writes to LBAs, enabling the SSD to manage garbage collection. Consequently, the host has no control over the garbage collection process in the SSD, aside from invalidating LBAs by deallocating or overwriting them. Note that this limitation only applies in scenarios where the host can achieve greater performance gains by managing garbage collection more efficiently than the SSD, rather than focusing solely on data placement.
\item {\textbf{Requires device overprovisioning and mapping table in SSD.}} As in conventional SSDs today, FDP SSDs will also require a mapping table in DRAM to support transparent mapping of logical to physical addresses. Moreover, NAND overprovisioning in the device is required for acceptable performance in the absence of host-based garbage collection. This is a limitation when the proposal is viewed from the lenses of the cost of fabrication of FDP SSDs.
\end{enumerate}
}

