\section{Why FDP Matters for CacheLib and Hybrid Caches?}
\label{sec:observations}
In this section, we discuss the fit of FDP and the opportunities afforded by it for CacheLib and hybrid caches based on the analysis of CacheLib's Flash Cache architecture, web service caching deployments, and workloads.

\subsection{Insights and Observations}
\begin{figure}[!t]
  \centering
  \vspace{-3ex}
  \subfloat[a][SSD cross-section without FDP]{\includegraphics[width=0.9\linewidth]{svg_to_pdf/nand_crossection_non_fdp_drawio.pdf} \label{fig:non-fdp-cross}} \hfill \\
  \vspace{-2ex}  
  \subfloat[b][SSD cross-section without FDP]{\includegraphics[width=0.9\linewidth]{svg_to_pdf/nand_crossection_fdp_drawio.pdf} \label{fig:fdp-cross}} \hfill \\   
  \caption{SSD cross-section. {\Large \textcircled{\small 1a}} shows the intermixing of LOC’s sequential and cold data with SOC’s random and hot data in SSD blocks. {\Large \textcircled{\small 1b}} shows the inefficient use of device OP by both LOC and SOC data. {\Large \textcircled{\small 2a}} shows that with SOC data being segregated, invalidation of its data can result in free SSD blocks.  {\Large \textcircled{\small 2b}} shows that with FDP, LOC data which is written sequentially will not cause DLWA. {\Large \textcircled{\small 2c}} shows the efficient use of device OP exclusively by SOC data to cushion SOC DLWA.} 
  \label{fig:ssd-cross}
  \vspace{-2ex}
\end{figure}

\minisec{\textit{Insight 1: Intermixing of SOC and LOC data leads to high DLWA}}
Large cache items are written into the LOC in a log-structured fashion utilizing a FIFO or LRU eviction policy. This results in a sequential write pattern to the SSD. Small cache items are written into SOC buckets using a uniform hash function. Each item insert causes the entire SOC bucket (size is configurable but default is 4 KB) to be written to the SSD. Contrary to LOC, SOC writes generate a random write pattern to the SSD. 

For workloads with large working set sizes and key churn, the Flash cache layer receives writes due to evictions from the RAM cache~\cite{berg2020cachelib,mcallister2021kangaroo}. For workloads dominant in small object accesses, this segregation leads to an infrequent and cold data access pattern in the LOC together with a frequent and hot data access pattern in the SOC. This leads to the intermixing of LOC's sequential and cold data with SOC's random and hot data in a single SSD block (Figure \ref{fig:non-fdp-cross} {\Large \textcircled{\small 1a}}) causing high DLWA upon garbage collection. \\

\minisec{\textit{Insight 2: The use of host overprovisioning as a control measure for DLWA is inefficient}}
As explained in Section \ref{sec:background:flash-cache-cachelib}, \textit{CacheLib deployments utilize a host overprovisioning of almost 50\% of the SSD to limit DLWA to an acceptable value of $\sim$1.3}. This is inefficient from both cost and carbon efficiency perspectives. The LOC data due to its sequential and cold access pattern does not need any host or device overprovisioning for a DLWA of 1. Without host overprovisioning the only extra space available to help control DLWA is the device overprovisioning space. The random SOC data would benefit the most from the device overprovisioned space because it is small, hot and updated frequently. However, the intermixing of SOC and LOC results in an inefficient use of the device overprovisioning space (Figure \ref{fig:non-fdp-cross} {\Large \textcircled{\small 1b}}) as both the SOC and LOC data share it causing unnecessary data movement. \\

\minisec{\textit{Insight 3: High SOC invalidation and its small size can be harnessed to control DLWA}}
A smaller SOC size on devices leads to fewer buckets and a higher rate of key collisions. Since the entire SOC bucket of 4KB is written out, a larger SOC bucket invalidation rate is SSD-friendly because it leads to more SSD page invalidation. If only invalidated SOC data resided in an SSD erase block, this would result in the entire erase block freeing itself up and not needing movement of valid data. For workloads dominant in small object accesses, a high invalidation of SOC happens but the SOC data in erase blocks is intermixed with LOC data. This prevents the SSD from taking advantage of the updates occurring in the SOC buckets over a small LBA space. \\

\begin{comment}
More collisions would mean a faster invalidation rate for the same 4KB bucket. As the SOC size increases the collisions per bucket would reduce due to a large number of SOC buckets.  \\
A larger SOC bucket invalidation rate is SSD-friendly because this leads to more SSD page invalidation within the SSD block. If only the SOC data were to reside in an SSD block, this would result in SSD blocks freeing themselves up and not needing live data migrations. However, the LOC data residing in the same SSD block prevents this behaviour from occurring. \\
\end{comment}

\minisec{\textit{Insight 4: Data placement using FDP can help CacheLib control DLWA}}
FDP can be utilized by CacheLib to separate the SOC and LOC data in the SSD using different reclaim unit handles. This allows the LOC data and SOC data to reside in mutually exclusive SSD blocks (reclaim units). Such a design will have the following benefits,
\begin{enumerate}[noitemsep, topsep=1pt, partopsep=0pt,leftmargin=*]
    \item The SSD blocks containing LOC data get overwritten sequentially resulting in minimal data movement and DLWA (Figure \ref{fig:fdp-cross} {\Large \textcircled{\small 2b}}). If LOC data resides in separate reclaim units than SOC data, the device overprovisioning space can be used exclusively by SOC data.
    \item The ideal behaviour of SOC data invalidating only itself (Insight 3) can be realized by segregating it into separate reclaim units (Figure \ref{fig:fdp-cross} {\Large \textcircled{\small 2a}}). A smaller SOC size leads to a greater invalidation rate causing most of the SOC data in the SSD erase block being invalid. This leads to minimal live data movement and DLWA. As the SOC size increases we expect an increase in DLWA even with LOC and SOC segregation across reclaim units.
    \item The ideal utilization of device overprovisioning space (Insight 2) is possible with FDP (Figure \ref{fig:fdp-cross} {\Large \textcircled{\small 2c}}). SOC data can use the overprovisioned space to cushion DLWA. When the SOC size is smaller than the device overprovisioning space we expect a DLWA of $\sim$1 since there is at least one spare block available for each block of SOC data.
    \item The separation of LOC and SOC data in the SSD does not necessitate a change in the CacheLib architecture and API. Therefore, we expect no change in the application-level write amplification (ALWA).
\end{enumerate}
\vspace{1ex}
\begin{comment}
In conclusion, with FDP, the high DLWA from SOC's random writes can be controlled efficiently due to the inherent invalidation rate of SOC data and device overprovisioning space being exclusively available to cushion data migrations. \\
\end{comment}

\minisec{\textit{Insight 5: Initially Isolated FDP devices will suffice in controlling the DLWA in CacheLib}}
With the separation of LOC and SOC data within the SSD, the only live data movement will be due to SOC data. Irrespective of whether the SSD is initially isolated or persistently isolated only SOC data would reside in reclaim units used for garbage collection. Therefore, the isolation of LOC and SOC data would be preserved regardless. \\

\minisec{\textit{Insight 6: Data placement using FDP can help reduce carbon emissions in CacheLib}}
Embodied carbon emissions account for the major chunk of carbon emissions compared to operational carbon emissions. The DLWA gains from using FDP-enabled CacheLib leads to an improved device lifetime. This results in fewer device replacements during the system lifecycle leading to reduction in embodied carbon emissions.

Fewer garbage collection operations are the reason for the DLWA gains with FDP-enabled CacheLib. For a fixed number of host operations, fewer data migrations result in fewer total device operations. The reduction in total operations requires the device to spend fewer cycles in the active state leading to a lower SSD energy consumption and reduced operational carbon footprint~\cite{ssdenergy, ssdpower}.

\begin{comment}
\begin{itemize}[noitemsep, topsep=1pt, partopsep=0pt,leftmargin=*]
    \item The DLWA gains from using FDP-enabled CacheLib leads to an improved device lifetime. This results in fewer device replacements during the system lifecycle leading to reduction in embodied carbon emissions.
    \item Fewer garbage collection operations are the reason for the DLWA gains with FDP-enabled CacheLib. For a fixed number of host operations fewer data migrations equates to fewer total device operations. The reduction in total operations requires the device to spend fewer cycles in the active state leading to a lower SSD energy consumption~\cite{ssdenergy, ssdpower} and reduced operational carbon footprint.
\end{itemize}
\end{comment}

\subsection{Theoretical Analysis of FDP-enabled CacheLib DLWA and Carbon Emissions}
\label{theorem:DLWA}
We formulate a theoretical model of DLWA and carbon emissions for SOC and LOC data segregation in CacheLib using the insights of the previous section. We assume the DLWA of LOC data is $\sim$1. Additionally, we use the fact that only SOC data will use the device overprovisioning space and item insertions to the SOC buckets follow a uniform hash function. To simplify our analysis, we assume that the uniform hash function used in CacheLib is fairly well-behaved. Modelling DLWA by estimating live data movement for a uniform random workload has been used proposed before~\cite{DayanBB15}. We extend that methodology to model the SOC DLWA that translates to the DLWA for FDP-enabled CacheLib as the LOC does not contribute to DLWA. The derivation of the theorems in this section is available in Appendix \ref{ref:appendix:theoretical-model}.
%\vspace{-2ex}
\begin{theorem}
    The DLWA for FDP-enabled CacheLib using SOC and LOC data segregation is,
    \vspace{-0.75em}
    \begin{equation*}
        \text{DLWA} = \frac{1}{1- \delta}
    \end{equation*}
    \vspace{3ex}
    
    \noindent where $\delta$ denotes the average live SOC bucket migration due to garbage collection and is given by,
    \begin{equation*}
        \delta = - \frac{\text{S}_{\text{SOC}}}{\text{S}_{\text{P-SOC}}} \times \mathcal{W} (- \frac{\text{S}_{\text{P-SOC}}}{\text{S}_{\text{SOC}}} \times e^{- \frac{\text{S}_{\text{P-SOC}}}{\text{S}_{\text{SOC}}}} )
    \end{equation*}
    where $\text{S}_{\text{SOC}}$ is the total SOC size in bytes,  $\text{S}_{\text{P-SOC}}$ is the total physical space available for SOC data including device overprovisioning in bytes and $\mathcal{W}$ denotes the Lambert W function.
\end{theorem}


\subsubsection{Modelling CO2 emissions (CO2e) for FDP-enabled CacheLib}
The total carbon footprint is the sum of embodied and operational carbon emissions.
    \begin{equation*}
        \text{Total}_{\text{CO2e}} = \text{C}_{\text{embodied}} + \text{C}_{\text{operational}}
    \end{equation*}
\label{theorem:embodied-co2e}
\begin{theorem}
    The embodied carbon emissions from using CacheLib by accounting for SSD replacement during the system lifecycle of T years and rated SSD warranty of $\text{L}_{\text{dev}}$ years is,

    \begin{equation*}
        \text{C}_{\text{embodied}} = \text{DLWA} \times \text{Device}_{\text{cap}} \times \frac{T}{\text{L}_{\text{dev}}} \times \text{C}_{\text{SSD}}
    \end{equation*}
    where $\text{Device}_{\text{cap}}$ is the physical capacity of the device. \\
    $\text{Host}_{\text{cap}} = \text{Device}_{\text{cap}} \times (1 - \text{Total}_{\text{op}})$ denotes the SSD capacity used by the host system in GB, $\text{H}_{\text{op}} \text{ and } \text{D}_{\text{op}} \in [0,1) $ is the fraction of host overprovisioning and device overprovisioning and $\text{C}_{\text{SSD}}$ is the amount of CO2e (Kg) per GB of SSD manufactured.
    
\end{theorem}

Operational Energy can be converted to CO2 emission (CO2e) using the greenhouse equivalence calculator ~\cite{greenhouse_calc}. The operational energy consumed can be modelled by estimating the time spent in idle and active states ~\cite{ssdenergy}. The time spent in active states is proportional to the total number of device operations during the period in question,

\begin{theorem}
Operational energy is proportional to the total number of garbage collection events.
\begin{equation*} 
    \mathcal{E}_{\text{operational}} \propto \mathcal{E}(\text{Host}_{\text{operations}}) + \mathcal{E}(\text{Device}_{\text{migrations}})
    \end{equation*}
    where, $\text{Device}_{\text{migrations}}$ is the number of garbage collection operations triggered in the SSD. 
\end{theorem}
