\section{Design and Implementation}
\label{sec:implementation}
\edit{
In this section we outline the design principles, implementation details, and lessons learned while building FDP-aware SOC and LOC data segregation in CacheLib.

\subsection{Design Principles}
CacheLib is a popular building block for various caching services with diverse use cases. It is important to design a minimally invasive, adaptable, and maintainable solution for SOC and LOC data segregation. We applied the following design principles while building FDP-based data segregation support in CacheLib.

\minisec{\textit{1. Keep it simple~\cite{Lampson83:simple}}}
This guiding principle enabled us to merge our changes upstream given the diverse use cases of CacheLib and the need for stability and maintainability of the codebase.

\minisec{\textit{2. FDP is just another storage technology for CacheLib}}
The design should seamlessly support deployments and setups where FDP is not used, requiring minimal configuration changes to remain user-friendly. This ensures that CacheLib maintains backward compatibility for users who do not utilize FDP SSDs.

\minisec{\textit{3. Allow software extensibility for various data placement technologies}}
The design should be generic and extensible to allow existing and future modules in CacheLib to segregate data and  experiment with various data placement policies and decisions.

\minisec{\textit{4. Allow hardware extensibility for evolving data placement technologies}}
Since the FDP specification is new and expected to evolve over time, its use should be localized in the CacheLib architecture to minimize changes to the codebase over time as the hardware technology evolves.
}

\begin{comment}
\begin{itemize}[noitemsep, topsep=1pt, partopsep=0pt,leftmargin=*]
    \item \textbf{Keep it simple}~\cite{Lampson83:simple}. This guiding principle enabled us to merge our changes upstream given the varied use cases of CacheLib and the need for stability.
    \item \textbf{Backward Compatibility} An FDP-enabled CacheLib should be able to seamlessly support services where FDP is not used. Also FDP-enabled CacheLib should be able to work with FDP non-capable SSD devices.
    \item \textbf{Software Extensibility} The design should be generic and extensible to support more SSD-consuming modules and more complex use cases.
    \item \textbf{Hardware Extensibility} FDP spec is evolving and has the potential to add many more hardware features. The changes in device characteristics shouldn't affect how CacheLib's modules achieve data placement. So, abstracting the hardware layout and characteristics of FDP SSDs is essential.
\end{itemize}
\end{comment}

%\vspace{-2ex}

\subsection{Placement Handles}
We introduce the abstract concept of \texttt{Placement Handle} to CacheLib's SSD I/O path to achieve FDP-based data placement while preserving backward compatibility. \texttt{Placement handles} allow various consuming modules to segregate data. The set of available placement handles are allocated from a data placement aware device layer upon initialization. Such an abstraction hides the semantics of the underlying data placement technology e.g., FDP providing hardware extensibility. If FDP is not enabled on the underlying device, a default placement handle is used to indicate no placement preference.

\subsection{Placement Handle Allocator}
\begin{figure}[!t]
    \centering
    %\includegraphics[width=\linewidth]{svg_to_pdf/io_path_latest_drawio.pdf}\hfill
    \includegraphics[width=0.8\linewidth]{svg_to_pdf/eurosys_cachelib_io_path.pdf}\hfill
    \caption{CacheLib I/O Path. {\Large \textcircled{\small 1a}} denotes the placement handle allocator that is responsible for allocating placement handles that consume FDP.}  
    %\vspace{-1ex}
    \label{fig:cachelib-io-path}
\end{figure}
We implement a \texttt{Placement Handle Allocator} (Figure \ref{fig:cachelib-io-path} {\Large \textcircled{\small 1a}}) that is responsible for allocating placement handles to any module that wishes to use data placement. If FDP is enabled in CacheLib and the underlying SSD supports FDP, this module assigns an available <RUH, RG> pair, referred as a Placement Identifier (PID) in the FDP specification, to the placement handle it allocates. This abstracts out FDP semantics from the consumers of FDP. If the underlying SSD does not support FDP, the default handle will be allocated. This indicates that there is no placement preference. In CacheLib, SOC and LOC in each I/O engine pair get different allocation of placement handles during initialization. Modules that are minor consumers of SSD, e.g., the metadata, do not state their placement preferences, so the default reclaim unit handle is assigned to them. The introduction of the placement handle allocator provides software extensibility and flexibility.

\subsection{FDP Aware I/O Management}
The SOC and LOC instances tag their I/Os with unique placement handles. The FDP-aware device layer translates these handles to the corresponding FDP Placement Identifier (PID). The PIDs are further translated to the NVMe specification placement directive fields (DSPEC and DTYPE fields \cite{NVMeCS}), attached to the uring\_cmd \cite{uring-cmd} I/Os and then submitted. We use the I/O Passthru~\cite{io-passthru, JoshiG0KRGLA24:iopassthru} mechanism to send FDP-enabled I/Os to the Linux kernel. We use an io\_uring~\cite{axboe2019efficient} queue pair per worker thread so that I/Os can be sent to the kernel without synchronization or concurrency challenges in the submission and completion queues of io\_uring. The FDP aware I/O management layer provides hardware extensibility by abstracting the layout of an FDP-enabled SSD.

\subsection{Lessons Learned and Future Directions}
\edit{In this section, we outline a few lessons learnt over time as we integrated FDP features in CacheLib. Some of these lessons can be useful for future directions of work.\\

\minisec{\textit{1. FDP specialized LOC eviction policy in CacheLib did not provide much benefit}}
We explored the notion of a specialized FDP eviction policy for CacheLib by building reclaim unit size awareness in the region-based eviction policy of the LOC. The FDP specification provides the required semantics for the host to track writes to a reclaim unit. Utilizing it, we can track the LOC regions that belong to a reclaim unit and invalidate multiple regions in a reclaim unit together. This can be paired with a \texttt{TRIM} command to free an entire reclaim unit and aid the garbage collection process on the SSD. Early exploration of this policy showed minimal gains and was shelved. We speculate that such a policy could be beneficial in cases where reclaim units are smaller in size.}\\

\begin{comment}
This was based on the assumption that mapping objects to be aware reclaim unit boundaries so that they do not span them 


To introduce Reclaim Unit (RU) boundary awareness to Flash Caches, we explored using a RU size-aware region-based eviction policy of the LOC. The thesis was that mapping objects to RU boundaries can aid garbage collection in freeing an entire RU because objects mapped to an RU would be evicted together. The FDP spec provides the required semantics for the host to track writes into an RU. Using this to make the LOC's eviction policy RU size-aware would result in evicting multiple regions, equal to the size of an RU. This could further be paired with using the TRIM command to notify the device that an entire RU can be freed. Early exploration of this eviction policy by mapping Regions to an RU showed us minimal gains and was not adopted for the FDP-based segregation in CacheLib. However, such a policy would be beneficial in cases where RUs are smaller in size ($\sim$1GB), allowing the host better control over mapping its objects to an RU.
We explored the notion of reclaRU size aware region-based eviction policy of the LOC. 
\end{comment}

\begin{comment}
To introduce Reclaim Unit (RU) boundary awareness to Flash Caches, we explored using a RU size-aware region-based eviction policy of the LOC. The thesis was that mapping objects to RU boundaries can aid garbage collection in freeing an entire RU because objects mapped to an RU would be evicted together. The FDP spec provides the required semantics for the host to track writes into an RU. Using this to make the LOC's eviction policy RU size-aware would result in evicting multiple regions, equal to the size of an RU. This could further be paired with using the TRIM command to notify the device that an entire RU can be freed. Early exploration of this eviction policy by mapping Regions to an RU showed us minimal gains and was not adopted for the FDP-based segregation in CacheLib. However, such a policy would be beneficial in cases where RUs are smaller in size ($\sim$1GB), allowing the host better control over mapping its objects to an RU.  
\end{comment}

\edit{
\minisec{\textit{2. Dynamic and adaptive data placement is outperformed by simple static solutions}}
Using FDP event logs, the host can inform itself of garbage collection operations in the SSD. This allows host software to understand the impact of its data placement decisions in real time. A host software stack can utilize the logs to build a feedback loop to understand its placement decisions and adapt accordingly. We explored some dynamic data placement policies using various load balancing and data temperature techniques early in the project. However, we saw minimal gains compared to the engineering complexity for such an effort over a static predefined placement handle for segregating SOC and LOC data for the small object dominant hybrid workloads.
}

\begin{comment}
With FDP's Event Logs, the host can keep itself informed of the activities in the SSD, especially when it comes to garbage collection operations. This acts as a good feedback loop to the host which can aid in decision making and the process of placement handle allocation. The Placement Handle Allocator we described earlier can be dynamic and adaptive by monitoring the FDP events. We explored this option and realised we could engineer a simpler solution without dynamism to solve the DLWA problem in CacheLib. 
\end{comment}

